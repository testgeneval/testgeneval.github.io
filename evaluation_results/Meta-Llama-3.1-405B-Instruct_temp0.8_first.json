{"raw_generations": {"sample_0": ["def test_uncertainty_type_property(UncertClass):\n    \"\"\"Test that uncertainty type is correctly set for each class.\"\"\"\n    uncert = UncertClass()\n    assert uncert.uncertainty_type == {\n        FakeUncertainty: 'fake',\n        StdDevUncertainty: 'std',\n        VarianceUncertainty: 'var',\n        InverseVariance: 'ivar',\n        UnknownUncertainty: 'unknown'\n    }[UncertClass]\n", "def test_NDUncertainty_constructor_quantity(uncertainty_types_to_be_tested):\n    for UncertClass in uncertainty_types_to_be_tested:\n        # Passing a quantity with unit\n        uncert = UncertClass(Quantity([1, 2], unit='m'))\n        assert_array_equal(uncert.array, [1, 2])\n        assert uncert.unit == u.m\n\n        # Passing a quantity without unit\n        uncert = UncertClass(Quantity([1, 2]))\n        assert_array_equal(uncert.array, [1, 2])\n        assert uncert.unit is None\n\n        # Passing an array and a unit separately\n        uncert = UncertClass(array=[1, 2], unit='m')\n        assert_array_equal(uncert.array, [1, 2])\n        assert uncert.unit == u.m\n\n        # Passing an array without a unit\n        uncert = UncertClass(array=[1, 2])\n        assert_array_equal(uncert.array, [1, 2])\n        assert uncert.unit is None\n", "def test_uncertainty_unit_conversion(UncertClass):\n    if UncertClass in uncertainty_types_without_conversion_support:\n        with pytest.raises(TypeError):\n            uncert = UncertClass([1, 2, 3], unit='m')\n            new_uncert = uncert.represent_as(VarianceUncertainty)\n    elif UncertClass == VarianceUncertainty:\n        uncert = UncertClass([1, 4, 9], unit='m^2')\n        new_uncert = uncert.represent_as(StdDevUncertainty)\n        assert new_uncert.unit == u.m\n        assert_array_equal(new_uncert.array, np.sqrt(uncert.array))\n    else:\n        uncert = UncertClass([1, 2, 3], unit='m')\n        new_uncert = uncert.represent_as(VarianceUncertainty)\n        assert new_uncert.unit == u.m**2\n        assert_array_equal(new_uncert.array, uncert.array**2)\n", "def test_uncertainty_unit_conversion(uncertainties, data_unit):\n    for UncertClass in uncertainty_types_with_conversion_support:\n        uncert = UncertClass([1, 2, 3], unit='m^2')\n        ndd = NDData([1, 2, 3], unit=data_unit)\n        uncert.parent_nddata = ndd\n        assert uncert.unit == uncert._data_unit_to_uncertainty_unit(ndd.unit)\n", "def test_uncertainty_unit_conversion(UncertClass):\n    if UncertClass in uncertainty_types_with_conversion_support:\n        # Test conversion to and from variance\n        uncert = UncertClass([1, 2, 3], unit='m')\n        var_uncert = uncert._convert_to_variance()\n        assert isinstance(var_uncert, VarianceUncertainty)\n        assert_array_equal(var_uncert.array, uncert.array ** 2)\n        assert var_uncert.unit == 'm^2'\n\n        # Test conversion back to original uncertainty type\n        new_uncert = UncertClass._convert_from_variance(var_uncert)\n        assert isinstance(new_uncert, UncertClass)\n        assert_array_equal(new_uncert.array, uncert.array)\n        assert new_uncert.unit == uncert.unit\n\n    elif UncertClass in uncertainty_types_without_conversion_support:\n        # Test that conversion raises an error\n        uncert = UncertClass([1, 2, 3], unit='m')\n        with pytest.raises(TypeError):\n            uncert._convert_to_variance()\n\n        with pytest.raises(TypeError):\n            UncertClass._convert_from_variance(VarianceUncertainty([1, 2, 3]))\n"], "sample_3": ["def test_ecsv_mixin_column_roundtrip(tmpdir):\n    # Test round-trip writing and reading of mixin columns with ECSV format.\n    # This uses the mixin_cols fixture defined in mixin_columns.py\n    table = mixin_cols()\n\n    filename = str(tmpdir.join('test.ecsv'))\n    table.write(filename, format='ascii.ecsv')\n\n    read_table = Table.read(filename, format='ascii.ecsv')\n\n    # Check that mixin column attributes are preserved\n    for colname in table.colnames:\n        col1 = table[colname]\n        col2 = read_table[colname]\n        compare_attrs(col1, col2)\n", "def test_ecsv_mixin_columns(tmpdir):\n    # Test that mixin columns are properly serialized and deserialized.\n    filename = str(tmpdir.join('test.ecsv'))\n\n    table = Table(mixin_cols, meta={'comments': ['comment1', 'comment2']})\n\n    with nullcontext() if NUMPY_LT_1_19 else pytest.warns(UserWarning):\n        ascii.write(table, filename, format='ecsv')\n\n    read_table = ascii.read(filename, format='ecsv')\n\n    for colname in mixin_cols.colnames:\n        assert compare_attrs(getattr(table, colname),\n                             getattr(read_table, colname))\n", "def test_ecsv_roundtrip_mixin_columns(tmpdir):\n    # Test that mixin columns get round-tripped correctly in ECSV format.\n    table = Table(mixin_cols, meta={'comments': ['comment1', 'comment2']})\n    file_path = str(tmpdir.join('ecsv_test.txt'))\n    table.write(file_path, format='ascii.ecsv')\n\n    with open(file_path, 'r') as f:\n        lines = f.readlines()\n\n    # Check that mixin columns were written out as expected\n    assert 'datatype:' in lines[3]\n    assert 'subtype: object' in lines[4]\n\n    # Read back the table and check it is the same\n    table_read = Table.read(file_path, format='ascii.ecsv')\n    compare_attrs(table_read, table)\n\n    # Test with masked values\n    table['m'] = MaskedColumn([1, 2, 3], mask=[True, False, True])\n    file_path = str(tmpdir.join('ecsv_test_masked.txt'))\n    table.write(file_path, format='ascii.ecsv')\n\n    table_read = Table.read(file_path, format='ascii.ecsv')\n    compare_attrs(table_read, table)\n", "def test_ecsv_masked_column(tmpdir):\n    # Test that writing and reading a masked column works as expected\n    table = Table()\n    data = np.array([1, 2, 3], dtype='int64')\n    mask = np.array([False, True, False])\n    table['a'] = MaskedColumn(data, mask=mask)\n\n    filename = str(tmpdir.join('test.ecsv'))\n    table.write(filename, format='ascii.ecsv')\n\n    with open(filename, 'r') as f:\n        lines = f.readlines()\n\n    assert '# - {name: a, datatype: int64}' in lines[3]\n\n    table_read = ascii.read(filename, format='ecsv')\n\n    assert isinstance(table_read['a'], MaskedColumn)\n    assert np.all(table_read['a'].data == data)\n    assert np.all(table_read['a'].mask == mask)\n", "def test_ecsv_roundtrip_mixin_columns(tmpdir):\n    # Test round-trip of mixin columns via ECSV\n    for table, _ in mixin_cols:\n        file_path = tmpdir.join('test.ecsv')\n        with nullcontext() if NUMPY_LT_1_19 else pytest.warns(AstropyUserWarning):\n            table.write(file_path, format='ascii.ecsv', overwrite=True)\n        table_read = Table.read(file_path, format='ascii.ecsv')\n\n        assert isinstance(table_read, type(table))\n\n        # Compare info attributes\n        compare_attrs(table.info, table_read.info)\n\n        # Compare column attributes\n        for col, col_read in zip(table.columns.values(), table_read.columns.values()):\n            compare_attrs(col.info, col_read.info)\n\n        # Compare data\n        for col, col_read in zip(table.columns.values(), table_read.columns.values()):\n            if isinstance(col, QuantityInfo):\n                assert quantity_allclose(col, col_read)\n            else:\n                assert np.array_equal(col, col_read)\n"], "sample_1": ["def test_separable_compound_model_with_arithmetic_operator():\n    # Test separability with arithmetic operator\n    compound_model = sh1 + sh2\n    expected_is_separable = np.array([False])\n    expected_separability_matrix = np.array([[True]])\n    \n    is_separable_result = is_separable(compound_model)\n    separability_matrix_result = separability_matrix(compound_model)\n    \n    assert_allclose(is_separable_result, expected_is_separable)\n    assert_allclose(separability_matrix_result, expected_separability_matrix)\n", "def test_separable_raises_ModelDefinitionError():\n    with pytest.raises(ModelDefinitionError):\n        _arith_oper(models.Shift(1), models.Scale(2) & models.Scale(3))\n", "def test_arith_oper():\n    # Test that _arith_oper raises an error when models have different n_inputs or n_outputs\n    with pytest.raises(ModelDefinitionError):\n        _arith_oper(sh1, rot)\n\n    # Test that _arith_oper returns a non-separable output\n    result = _arith_oper(scl1, scl2)\n    assert np.all(result == 1)\n", "def test_separable_arithmetic_operator():\n    # Test that arithmetic operators are not separable\n    model = sh1 + sh2\n    assert not is_separable(model).any()\n    assert np.all(separability_matrix(model))\n", "def test_separable_with_arithmetic_operator():\n    # Test separability with arithmetic operator\n    model = sh1 + sh2\n    expected_is_separable = np.array([False])\n    expected_separability_matrix = np.array([[True, True]])\n    assert_allclose(is_separable(model), expected_is_separable)\n    assert_allclose(separability_matrix(model), expected_separability_matrix)\n\n    model = sh1 - sh2\n    expected_is_separable = np.array([False])\n    expected_separability_matrix = np.array([[True, True]])\n    assert_allclose(is_separable(model), expected_is_separable)\n    assert_allclose(separability_matrix(model), expected_separability_matrix)\n\n    model = sh1 * sh2\n    expected_is_separable = np.array([False])\n    expected_separability_matrix = np.array([[True, True]])\n    assert_allclose(is_separable(model), expected_is_separable)\n    assert_allclose(separability_matrix(model), expected_separability_matrix)\n\n    model = sh1 / sh2\n    expected_is_separable = np.array([False])\n    expected_separability_matrix = np.array([[True, True]])\n    assert_allclose(is_separable(model), expected_is_separable)\n    assert_allclose(separability_matrix(model), expected_separability_matrix)\n\n    model = sh1 ** sh2\n    expected_is_separable = np.array([False])\n    expected_separability_matrix = np.array([[True, True]])\n    assert_allclose(is_separable(model), expected_is_separable)\n    assert_allclose(separability_matrix(model), expected_separability_matrix)\n"], "sample_2": ["def test_ccddata_fits_writer_reader(tmpdir):\n    \"\"\"\n    Test writing a CCDData object to a FITS file and then reading it back in.\n    \"\"\"\n    ccd = create_ccd_data()\n    ccd.uncertainty = StdDevUncertainty(np.random.normal(size=ccd.shape))\n    ccd.mask = np.random.choice([True, False], size=ccd.shape)\n\n    filename = tmpdir.join('test_ccddata.fits').strpath\n    ccd.write(filename, overwrite=True)\n\n    ccd_new = CCDData.read(filename)\n    assert np.allclose(ccd.data, ccd_new.data)\n    assert np.allclose(ccd.uncertainty.array, ccd_new.uncertainty.array)\n    assert np.allclose(ccd.mask, ccd_new.mask)\n    assert ccd.unit == ccd_new.unit\n    assert ccd.header == ccd_new.header\n", "def test_ccddata_unit_from_bunit():\n    # Test that the unit from the BUNIT keyword in the FITS header is used\n    # when creating a CCDData object.\n    data = np.ones((10, 10))\n    header = fits.Header()\n    header['BUNIT'] = 'adu'\n    ccd = CCDData(data, meta=header)\n    assert ccd.unit == u.adu\n\n    # Test that the unit from the BUNIT keyword in the FITS header is used\n    # even if the unit argument is not provided.\n    ccd = CCDData(data, meta=header)\n    assert ccd.unit == u.adu\n\n    # Test that the unit argument overrides the unit from the BUNIT keyword\n    # in the FITS header.\n    ccd = CCDData(data, unit=u.electron, meta=header)\n    assert ccd.unit == u.electron\n\n    # Test that an invalid unit from the BUNIT keyword raises a ValueError.\n    header['BUNIT'] = ' invalid_unit'\n    with pytest.raises(ValueError):\n        CCDData(data, meta=header)\n", "def test_ccddata_reader_invalid_unit():\n    \"\"\"\n    Test that the CCDData reader raises a ValueError if the unit in the FITS\n    header is not a valid unit.\n    \"\"\"\n    # Create a temporary FITS file with an invalid unit\n    data = np.array([1, 2, 3])\n    hdu = fits.PrimaryHDU(data)\n    hdu.header['BUNIT'] = 'INVALID_UNIT'\n    with nd_testing.temp_file('temp.fits') as filename:\n        hdu.writeto(filename)\n\n        # Try to read the FITS file into a CCDData object\n        with pytest.raises(ValueError) as excinfo:\n            CCDData.read(filename)\n\n        # Check that the error message is as expected\n        assert \"The Header value for the key BUNIT (INVALID_UNIT) cannot be interpreted as valid unit.\" in str(excinfo.value)\n", "def test_ccddata_writer(tmpdir):\n    \"\"\"\n    Test that CCDData writer creates a valid FITS file with expected data.\n    \"\"\"\n    ccd = create_ccd_data()\n    filename = tmpdir.join('test.fits').strpath\n\n    # Add some uncertainty to the data\n    uncertainty = StdDevUncertainty(np.random.normal(size=ccd.shape))\n    ccd.uncertainty = uncertainty\n\n    # Write the data to a file\n    ccd.write(filename, hdu_uncertainty='UNCERT')\n\n    # Check that the file exists and is a valid FITS file\n    with fits.open(filename) as hdul:\n        assert len(hdul) == 2\n        assert hdul[0].header['BUNIT'] == 'adu'\n        assert np.allclose(hdul[0].data, ccd.data)\n        assert np.allclose(hdul[1].data, uncertainty.array)\n", "def test_ccddata_uncertainty_type(tmpdir):\n    \"\"\"\n    Test setting the uncertainty type in a CCDData object.\n    \"\"\"\n    ccd = create_ccd_data()\n    uncertainty = StdDevUncertainty(np.random.normal(size=ccd.shape))\n    ccd.uncertainty = uncertainty\n\n    # Check that uncertainty type is correctly stored in FITS header\n    hdu = ccd.to_hdu(key_uncertainty_type='UTYPE')\n    assert hdu[1].header['UTYPE'] == 'StdDevUncertainty'\n\n    # Check that uncertainty type is correctly loaded from FITS header\n    filename = str(tmpdir.join('test.fits'))\n    fits_ccddata_writer(ccd, filename)\n    with fits.open(filename) as hdus:\n        assert hdus[1].header['UTYPE'] == 'StdDevUncertainty'\n"], "sample_4": ["def test_read_html_table_format_mapping(self, cosmo, tmp_path):\n    \"\"\"\n    Test that ``read_html_table`` correctly maps column names to Cosmology inputs.\n    This tests both the default and a user-specified mapping.\n    \"\"\"\n    # Create table with columns in latex format\n    table = QTable({k: [v] for k, v in _FORMAT_TABLE.items()})\n    file = tmp_path / \"cosmology.html\"\n    table.write(file, format=\"ascii.html\")\n\n    # Test with default mapping\n    read_cosmo = read_html_table(file)\n    assert set(read_cosmo.__parameters__) == set(cosmo.__parameters__)\n\n    # Test with custom mapping\n    custom_format_table = {\"H0\": \"$$h_{0}$$\"}\n    table.rename_column(_FORMAT_TABLE[\"H0\"], custom_format_table[\"H0\"])\n    table.write(file, format=\"ascii.html\", overwrite=True)\n    with pytest.raises(ValueError):  # should raise since there's no mapping for h0\n        read_html_table(file)\n\n    # update the format table with the custom format\n    _FORMAT_TABLE.update(custom_format_table)\n    read_cosmo = read_html_table(file)\n    assert set(read_cosmo.__parameters__) == set(cosmo.__parameters__)\n", "def test_read_html_table_format_mapping(self, tmpdir):\n    \"\"\"Test '_FORMAT_TABLE' is used correctly in read_html_table.\"\"\"\n    # create table with some columns having LaTeX names\n    table = QTable({\"$$H_{0}$$\": [70], \"$$\\\\Omega_{m,0}$$\": [0.3]})\n    filename = str(tmpdir.join(\"table.html\"))\n    table.write(filename, format=\"ascii.html\", overwrite=True)\n\n    # read the table\n    cosmo = read_html_table(filename)\n\n    # check that the cosmology has the correct values\n    assert cosmo.H0.value == 70\n    assert cosmo.Om0.value == 0.3\n", "def test_read_html_table_format_mapping(self, cosmo):\n    \"\"\"\n    Test that ``read_html_table`` correctly maps the table's column names to Cosmology inputs (parameter names)\n    when ``latex_names=True``.\n    \"\"\"\n    # Create a table with latex column names\n    table = QTable(\n        {\n            _FORMAT_TABLE[\"H0\"]: [cosmo.H0.value],\n            _FORMAT_TABLE[\"Om0\"]: [cosmo.Om0.value],\n        }\n    )\n\n    # Write the table to an html file\n    write_html_table(cosmo, \"test.html\", latex_names=True)\n\n    # Read the html file and check that the column names are correctly mapped\n    read_cosmo = read_html_table(\"test.html\", index=0, cosmology=type(cosmo), latex_names=True)\n\n    assert read_cosmo.H0 == cosmo.H0\n    assert read_cosmo.Om0 == cosmo.Om0\n", "def test_read_write_html_format_table(cosmo):\n    \"\"\"Test reading and writing with '_FORMAT_TABLE'.\"\"\"\n    # Write to table using default _FORMAT_TABLE for column names.\n    file = \"cosmology.html\"\n    write_html_table(cosmo, file, latex_names=True, overwrite=True)\n\n    # Read back in the table. This should map the latex column names back to the\n    # correct cosmology parameter names.\n    got_cosmo = read_html_table(file)\n\n    assert cosmo == got_cosmo\n", "def test_read_html_table_format_mapping(self, cosmo):\n    \"\"\"Test `_FORMAT_TABLE` mapping when reading a HTML table.\"\"\"\n    # Write without latex names and read with\n    file = write_html_table(cosmo, None, latex_names=False)\n    got = read_html_table(file, latex_names=True)\n\n    assert got == cosmo\n\n    # Test with 'H0' AND '$$H_{0}$$'\n    table = QTable({\"H0\": [cosmo.H0], \"$$H_{0}$$\": [cosmo.H0]})\n    with pytest.raises(ValueError, match=\"column 'H0' already exists\"):\n        read_html_table(table, latex_names=True)\n"], "sample_5": ["def test_parameter_getter_setter():\n    # Test that the getter and setter functions are correctly applied to a parameter\n        return x * 2\n\n        return x / 2\n\n    param = Parameter(name='test', value=10, getter=getter, setter=setter)\n    assert param.value == 20  # The getter doubles the internal value\n\n    param.value = 40  # The setter halves the input value\n    assert param._internal_value == 20\n", "def test_parameter_std():\n    p = Parameter(name='test', value=1.0, std=0.1)\n    assert p.std == 0.1\n    p.std = 0.2\n    assert p.std == 0.2\n    assert isinstance(p.std, float)\n\n    with pytest.raises(TypeError):\n        p.std = 'not a number'\n", "def test_parameter_getter_setter():\n    # Test that getter and setter functions are correctly applied to parameter values\n\n        return value * 2\n\n        return value / 2\n\n    param = Parameter(name='test', default=5, getter=getter, setter=setter)\n    assert param.value == 10\n    param.value = 20\n    assert param._internal_value == 10\n\n    # Test that setting a Quantity updates the internal value correctly\n    param.quantity = 40 * u.m\n    assert param._internal_value == 20\n\n    # Test that getting the quantity returns the correct value\n    assert param.quantity == 40 * u.m\n", "def test_parameter_copy():\n    p = Parameter('test', 'description', default=42, unit='m',\n                  getter=None, setter=None, fixed=True, tied=False,\n                  min=0, max=100, bounds=None)\n    p_copy = p.copy()\n\n    assert p_copy.name == p.name\n    assert p_copy.description == p.description\n    assert p_copy.default == p.default\n    assert p_copy.unit == p.unit\n    assert p_copy.fixed == p.fixed\n    assert p_copy.tied == p.tied\n    assert p_copy.min == p.min\n    assert p_copy.max == p.max\n    assert p_copy.bounds == p.bounds\n\n    # Ensure that modifying the copy does not affect the original\n    p_copy.name = 'new_name'\n    p_copy.fixed = False\n    assert p_copy.name != p.name\n    assert p_copy.fixed != p.fixed\n", "def test_parameter_repr():\n    p = Parameter(name='test', default=1.0, description='Test parameter')\n    assert repr(p) == \"Parameter('test', value=1.0)\"\n    \n    p = Parameter(name='test', default=[1.0, 2.0], description='Test parameter')\n    assert repr(p) == \"Parameter('test', value=[1., 2.])\"\n    \n    p = Parameter(name='test', default=1.0, unit=u.m, description='Test parameter')\n    assert repr(p) == \"Parameter('test', value=1.0, unit=m)\"\n    \n    p = Parameter(name='test', default=1.0, fixed=True, description='Test parameter')\n    assert repr(p) == \"Parameter('test', value=1.0, fixed=True)\"\n    \n    p = Parameter(name='test', default=1.0, tied=lambda x: x, description='Test parameter')\n    assert repr(p).startswith(\"Parameter('test', value=1.0, tied=<function \")\n"], "sample_6": ["def test_angle_setitem():\n    # Create an Angle object\n    angle = Angle([10, 20, 30] * u.deg)\n    \n    # Test setting a single value\n    angle[0] = 40 * u.deg\n    assert_allclose(angle.degree, [40, 20, 30])\n    \n    # Test setting multiple values\n    angle[1:] = [50, 60] * u.deg\n    assert_allclose(angle.degree, [40, 50, 60])\n    \n    # Test setting with a Longitude object (should fail)\n    long = Longitude(70 * u.deg)\n    with pytest.raises(TypeError):\n        angle[0] = long\n    \n    # Test setting with a Latitude object (should work)\n    lat = Latitude(80 * u.deg)\n    angle[0] = lat\n    assert_allclose(angle.degree, [80, 50, 60])\n", "def test_angle_from_tuple():\n    with pytest.raises(u.UnitsError):\n        Angle((1, 2, 3), unit=u.arcsec)\n\n    a = Angle((1, 2, 3), unit=u.degree)\n    assert_allclose(a.degree, dms_to_degrees(1, 2, 3))\n\n    a = Angle((1, 2, 3), unit=u.hourangle)\n    assert_allclose(a.hour, hms_to_hours(1, 2, 3))\n", "def test_angle_invalid_input():\n    with pytest.raises(ValueError):\n        Angle('123d4m5s6e')  # too many fields\n\n    with pytest.raises(ValueError):\n        Angle('123h4m5s6e')  # too many fields\n\n    with pytest.raises(ValueError):\n        Angle('123.456.789d')  # too many decimal points\n\n    with pytest.raises(u.UnitsError):\n        Angle(10, unit='m')  # not an angular unit\n\n    with pytest.raises(u.UnitsError):\n        Angle(10 * u.m)  # not an angular unit\n\n    with pytest.raises(TypeError):\n        Angle(None)\n", "def test_longitude_wrap_angle():\n    # Test that Longitude correctly handles wrapping at different angles.\n    lon = Longitude(370*u.deg)\n    assert_allclose(lon.degree, 10)\n\n    lon = Longitude(370*u.deg, wrap_angle=180*u.deg)\n    assert_allclose(lon.degree, -350)\n\n    lon = Longitude(-190*u.deg)\n    assert_allclose(lon.degree, 170)\n\n    lon = Longitude(-190*u.deg, wrap_angle=0*u.deg)\n    assert_allclose(lon.degree, -190)\n\n    # Also check setting the wrap_angle attribute\n    lon = Longitude(370*u.deg)\n    lon.wrap_angle = 180*u.deg\n    assert_allclose(lon.degree, -350)\n", "def test_angle_to_string():\n    # Test that Angle.to_string() works correctly for different formats and precisions\n    angle = Angle(12.3456 * u.deg)\n\n    # Test decimal format\n    assert angle.to_string(decimal=True) == '12.3456'\n\n    # Test sexagesimal format with default precision\n    assert angle.to_string(decimal=False) == '12:20:44.16'\n\n    # Test sexagesimal format with specified precision\n    assert angle.to_string(decimal=False, precision=2) == '12:20:44.16'\n\n    # Test sexagesimal format with negative angle\n    angle = Angle(-12.3456 * u.deg)\n    assert angle.to_string(decimal=False) == '-12:20:44.16'\n\n    # Test latex format\n    angle = Angle(12.3456 * u.deg)\n    assert angle.to_string(format='latex') == r'$12^{\\circ}20{}^{\\prime}44.16{}^{\\prime\\prime}$'\n"], "sample_7": ["def test_column_copy():\n    # Test copying a column with different attributes\n    col = table.Column(data=[1, 2, 3], name='test', unit='m', format='%d',\n                       description='Test column', meta={'a': 1})\n    col_copy = col.copy()\n    \n    assert_array_equal(col_copy.data, col.data)\n    assert col_copy.name == col.name\n    assert col_copy.unit == col.unit\n    assert col_copy.format == col.format\n    assert col_copy.description == col.description\n    assert col_copy.meta == col.meta\n\n    # Test copying a column with indices\n    col.indices = [0, 1]\n    col_copy = col.copy()\n    assert col_copy.indices == col.indices\n\n    # Test copying a column with parent table\n    t = table.Table({'a': [1, 2, 3]})\n    col = t['a']\n    col_copy = col.copy()\n    assert col_copy.parent_table is None\n", "def test_column_insert():\n    # Test inserting into a Column\n    c = table.Column([1, 2, 3], name='a')\n    c1 = c.insert(0, 10)\n    assert_array_equal(c1, np.array([10, 1, 2, 3]))\n\n    # Test with out-of-bounds index\n    with pytest.raises(np.AxisError):\n        c.insert(10, 10)\n\n    # Test with mask\n    mc = table.MaskedColumn([1, 2, 3], mask=[True, False, True])\n    mc1 = mc.insert(0, 10)\n    assert_array_equal(mc1.data, np.array([10, 1, 2, 3]))\n    assert_array_equal(mc1.mask, np.array([False, True, False, True]))\n", "def test_column_unicode():\n    # Test that Column follows unicode guidelines (from astropy/astropy#1717)\n    assert_follows_unicode_guidelines(table.Column, repr_table_name='data')\n", "def test_column_string_truncation_warning():\n    # Test that a warning is emitted when assigning a string value to a Column\n    # that will be truncated.\n\n    with warnings.catch_warnings(record=True) as warns:\n        col = table.Column(name='col', dtype='S3', length=1)\n        col[0] = 'abcd'\n        assert len(warns) == 1\n        assert \"truncated\" in str(warns[0].message)\n\n    with warnings.catch_warnings(record=True) as warns:\n        col = table.Column(name='col', dtype='U3', length=1)\n        col[0] = 'abcd'\n        assert len(warns) == 1\n        assert \"truncated\" in str(warns[0].message)\n", "def test_column_quantity_to():\n    \"\"\"Test converting a Column with units to a Quantity\"\"\"\n    col = table.Column([1, 2, 3], unit='m')\n    with pytest.raises(u.UnitsError):\n        col.to('s')\n    assert_array_equal(col.to('cm').value, np.array([100, 200, 300]))\n    assert col.to('cm').unit == u.cm\n"], "sample_8": ["def test_array2string():\n    a = MaskedNDArray(np.array([1, 2, np.nan, 4]), mask=[False, True, False, False])\n    assert str(a) == '[1. -- nan 4.]'\n\n    # Structured array.\n    dt = np.dtype([('a', 'f8'), ('b', 'f8')])\n    a = MaskedNDArray(np.array([(1., 2.), (3., 4.)], dtype=dt),\n                      mask=np.array([(True, False), (False, True)], dtype=[('a', '?'), ('b', '?')]))\n    assert str(a) == \"[(--, 2.) (3., --)]\"\n", "def test_array2string():\n    a = MaskedNDArray(np.array([1, 2, np.nan, 4]), mask=[False, False, False, True])\n    assert str(a) == \"[ 1.  2. -- --]\"\n", "def test_masked_array_initialization_with_various_items():\n    for item in VARIOUS_ITEMS:\n        a = Masked(self.a, mask=self.mask_a)\n        b = a[item]\n        assert type(b) is Masked\n        assert isinstance(b.unmasked, self._data_cls)\n        assert type(b.mask) is np.ndarray\n\n        c = Masked(self.c, mask=self.mask_c)\n        d = c[item]\n        assert type(d) is Masked\n        assert isinstance(d.unmasked, self._data_cls)\n        assert type(d.mask) is np.ndarray\n\n        sa = Masked(self.sa, mask=self.mask_sa)\n        sb = sa[item]\n        assert type(sb) is Masked\n        assert isinstance(sb.unmasked, self._data_cls)\n        assert type(sb.mask) is np.ndarray\n\n        sc = Masked(self.sc, mask=self.mask_sc)\n        sd = sc[item]\n        assert type(sd) is Masked\n        assert isinstance(sd.unmasked, self._data_cls)\n        assert type(sd.mask) is np.ndarray\n", "def test_masked_array_get_data_and_masks():\n    data, masks = Masked._get_data_and_masks(self.a, self.b)\n    assert_array_equal(data[0], self.a.unmasked)\n    assert_array_equal(data[1], self.b.unmasked)\n    assert_array_equal(masks[0], self.mask_a)\n    assert_array_equal(masks[1], self.mask_b)\n\n    # Test with a single array\n    data, masks = Masked._get_data_and_masks(self.a)\n    assert_array_equal(data[0], self.a.unmasked)\n    assert_array_equal(masks[0], self.mask_a)\n\n    # Test with multiple arrays\n    data, masks = Masked._get_data_and_masks(self.a, self.b, self.c)\n    assert_array_equal(data[0], self.a.unmasked)\n    assert_array_equal(data[1], self.b.unmasked)\n    assert_array_equal(data[2], self.c.unmasked)\n    assert_array_equal(masks[0], self.mask_a)\n    assert_array_equal(masks[1], self.mask_b)\n    assert_array_equal(masks[2], self.mask_c)\n", "def test_masked_array_ufuncs():\n    # Test that ufuncs work properly with masked arrays\n    a = Masked(self.a, self.mask_a)\n    b = Masked(self.b, self.mask_b)\n\n    result = np.add(a, b)\n    assert isinstance(result, Masked)\n    assert_masked_equal(result, Masked(self.a + self.b, self.mask_a | self.mask_b))\n\n    result = np.multiply(a, b)\n    assert isinstance(result, Masked)\n    assert_masked_equal(result, Masked(self.a * self.b, self.mask_a | self.mask_b))\n\n    result = np.greater(a, b)\n    assert isinstance(result, Masked)\n    assert_masked_equal(result, Masked(self.a > self.b, self.mask_a | self.mask_b))\n"], "sample_9": ["def test_html_table_id():\n    # Test that the table_id parameter is working correctly\n    table = Table({'a': [1, 2], 'b': [3, 4]})\n    html_str = ascii.write(table, format='html', htmldict={'table_id': 'my_table'})\n\n    # Parse the HTML and check the id of the table\n    soup = BeautifulSoup(html_str[0], 'html.parser')\n    assert soup.find('table')['id'] == 'my_table'\n", "def test_html_reader_table_id():\n    \"\"\"\n    Test reading HTML tables with table_id parameter.\n    \"\"\"\n    table = Table({\n        'a': [1, 2, 3],\n        'b': [4, 5, 6]\n    })\n    html_string = ascii.write(table, format='html')\n\n    # Read the table with default table_id (first table)\n    read_table = ascii.read(html_string, format='html')\n    assert np.array_equal(read_table['a'], table['a'])\n    assert np.array_equal(read_table['b'], table['b'])\n\n    # Read the table with explicit table_id (string)\n    html_string = '<table id=\"mytable\">' + html_string.split('<table>')[1]\n    read_table = ascii.read(html_string, format='html', htmldict={'table_id': 'mytable'})\n    assert np.array_equal(read_table['a'], table['a'])\n    assert np.array_equal(read_table['b'], table['b'])\n\n    # Read the table with explicit table_id (integer)\n    html_string = '<table>' + html_string.split('<table>')[1] + '</table><table>' + html_string.split('<table>')[1]\n    read_table = ascii.read(html_string, format='html', htmldict={'table_id': 2})\n    assert np.array_equal(read_table['a'], table['a'])\n    assert np.array_equal(read_table['b'], table['b'])\n", "def test_html_table_id():\n    # Create a table with two identical tables\n    data = \"\"\"\n    <html>\n    <body>\n    <table id=\"table1\">\n        <tr><th>Col 1</th><th>Col 2</th></tr>\n        <tr><td>1.0</td><td>2.0</td></tr>\n        <tr><td>3.0</td><td>4.0</td></tr>\n    </table>\n    <table id=\"table2\">\n        <tr><th>Col 1</th><th>Col 2</th></tr>\n        <tr><td>5.0</td><td>6.0</td></tr>\n        <tr><td>7.0</td><td>8.0</td></tr>\n    </table>\n    </body>\n    </html>\n    \"\"\"\n\n    # Test reading the first table by id\n    table = ascii.read(StringIO(data), format='html', htmldict={'table_id': 'table1'})\n    assert np.all(table['Col 1'] == [1.0, 3.0])\n    assert np.all(table['Col 2'] == [2.0, 4.0])\n\n    # Test reading the second table by id\n    table = ascii.read(StringIO(data), format='html', htmldict={'table_id': 'table2'})\n    assert np.all(table['Col 1'] == [5.0, 7.0])\n    assert np.all(table['Col 2'] == [6.0, 8.0])\n\n    # Test reading the first table by index\n    table = ascii.read(StringIO(data), format='html', htmldict={'table_id': 1})\n    assert np.all(table['Col 1'] == [1.0, 3.0])\n    assert np.all(table['Col 2'] == [2.0, 4.0])\n\n    # Test reading the second table by index\n    table = ascii.read(StringIO(data), format='html', htmldict={'table_id': 2})\n    assert np.all(table['Col 1'] == [5.0, 7.0])\n    assert np.all(table['Col 2'] == [6.0, 8.0])\n", "def test_html_with_css():\n    # Test writing a table with custom CSS styles\n\n    # Create a simple table\n    table = Table({'a': [1, 2], 'b': [3, 4]})\n\n    # Define some custom CSS styles\n    css = \"\"\"\n        table {\n            border-collapse: collapse;\n        }\n        th, td {\n            border: 1px solid black;\n        }\n    \"\"\"\n\n    # Write the table to HTML with custom CSS\n    htmldict = {'css': css}\n    lines = ascii.write(table, format='html', htmldict=htmldict)\n\n    # Parse the HTML output\n    soup = BeautifulSoup(lines[0], 'html.parser')\n\n    # Check that the CSS styles are included in the HTML output\n    assert soup.find('style').text.strip() == css.strip()\n", "def test_html_header_start_line():\n    lines = [SoupString('<tr><th>Header</th></tr>'),\n             SoupString('<tr><td>Data</td></tr>')]\n    header = html.HTMLHeader()\n    assert header.start_line(lines) == 0\n    lines = [SoupString('<tr><td>Data</td></tr>'),\n             SoupString('<tr><th>Header</th></tr>')]\n    assert header.start_line(lines) == 1\n    lines = [SoupString('<tr><td>Data</td></tr>'),\n             SoupString('<tr><td>Data</td></tr>')]\n    assert header.start_line(lines) is None\n"], "sample_11": ["def test_sliced_low_level_wcs_world_axis_object_components():\n    slices = (slice(None), 10, slice(None))\n    sliced_wcs = SlicedLowLevelWCS(WCS_SPECTRAL_CUBE, slices)\n    assert_equal(sliced_wcs.world_axis_object_components,\n                 WCS_SPECTRAL_CUBE.world_axis_object_components[:1] +\n                 WCS_SPECTRAL_CUBE.world_axis_object_components[2:])\n", "def test_sanitize_slices():\n    # Test that sanitize_slices raises an error for invalid slices\n    with pytest.raises(ValueError):\n        sanitize_slices((1, 2, 3), 2)\n    with pytest.raises(IndexError):\n        sanitize_slices(([1, 2], 3), 2)\n    with pytest.raises(IndexError):\n        sanitize_slices((slice(1, 2, 3),), 1)\n\n    # Test that sanitize_slices handles Ellipsis correctly\n    assert_equal(sanitize_slices((Ellipsis,), 3), [slice(None), slice(None), slice(None)])\n    assert_equal(sanitize_slices((Ellipsis, 1), 3), [slice(None), slice(None), 1])\n\n    # Test that sanitize_slices handles slice objects correctly\n    assert_equal(sanitize_slices((slice(1, 2),), 1), [slice(1, 2)])\n    assert_equal(sanitize_slices((slice(1, 2), slice(3, 4)), 2), [slice(1, 2), slice(3, 4)])\n\n    # Test that sanitize_slices handles integer slices correctly\n    assert_equal(sanitize_slices((1,), 1), [1])\n    assert_equal(sanitize_slices((1, 2), 2), [1, 2])\n\n    # Test that sanitize_slices fills in missing dimensions with slice(None)\n    assert_equal(sanitize_slices((1,), 2), [1, slice(None)])\n    assert_equal(sanitize_slices((1, 2), 3), [1, 2, slice(None)])\n", "def test_sliced_low_level_wcs_pixel_to_world_values():\n    # Test pixel_to_world_values with different slices and input arrays\n    wcs = WCS_SPECTRAL_CUBE\n    sliced_wcs = SlicedLowLevelWCS(wcs, (slice(1, 5), slice(2, 6), 3))\n\n    # Test with a single pixel array\n    pixel_array = np.array([1, 2, 3])\n    world_coords = sliced_wcs.pixel_to_world_values(pixel_array)\n    assert_allclose(world_coords, wcs.pixel_to_world_values(pixel_array + 1, pixel_array + 2, 3))\n\n    # Test with multiple pixel arrays\n    pixel_arrays = [np.array([1, 2]), np.array([3, 4])]\n    world_coords = sliced_wcs.pixel_to_world_values(*pixel_arrays)\n    assert_allclose(world_coords, wcs.pixel_to_world_values(pixel_arrays[0] + 1, pixel_arrays[1] + 2, 3))\n", "def test_sliced_wcs_dropped_world_dimensions():\n    # Test that dropped_world_dimensions contains the correct information\n\n    wcs = WCS_SPECTRAL_CUBE\n    slices = (slice(1, 3), slice(4, 6), 8)\n\n    sliced_wcs = SlicedLowLevelWCS(wcs, slices)\n\n    # Check that dropped_world_dimensions contains the correct keys\n    expected_keys = ['value', 'world_axis_names', 'world_axis_physical_types', \n                     'world_axis_units', 'world_axis_object_components', \n                     'world_axis_object_classes', 'serialized_classes']\n\n    assert_equal(set(sliced_wcs.dropped_world_dimensions.keys()), set(expected_keys))\n\n    # Check that dropped_world_dimensions['value'] is correct\n    expected_value = wcs.pixel_to_world_values(*[0]*wcs.pixel_n_dim)[1]  # Frequency axis\n    assert_allclose(sliced_wcs.dropped_world_dimensions['value'][0], expected_value)\n", "def test_sliced_wcs_dropped_world_dimensions():\n    wcs = WCS_SPECTRAL_CUBE\n    slices = (slice(1, 3), slice(None), 5)\n    sliced_wcs = SlicedLowLevelWCS(wcs, slices)\n\n    dropped_world_dimensions = sliced_wcs.dropped_world_dimensions\n\n    assert_equal(dropped_world_dimensions['value'], [25])\n    assert_equal(dropped_world_dimensions['world_axis_names'], ['Longitude'])\n    assert_equal(dropped_world_dimensions['world_axis_physical_types'], ['pos.eq.ra'])\n    assert_equal(dropped_world_dimensions['world_axis_units'], ['deg'])\n\n    # Check that the serialized classes are present\n    assert 'serialized_classes' in dropped_world_dimensions\n"], "sample_12": ["def test_angle_setitem():\n    a = Angle([1, 2, 3] * u.deg)\n    a[0] = 10 * u.deg\n    assert_array_equal(a.degree, [10, 2, 3])\n    with pytest.raises(u.UnitsError):\n        a[1] = 4 * u.arcsec\n    a[:] = 20 * u.deg\n    assert_array_equal(a.degree, [20, 20, 20])\n    a[:] = Angle([30, 40, 50] * u.deg)\n    assert_array_equal(a.degree, [30, 40, 50])\n\n    # Test setting with Longitude and Latitude\n    lon = Longitude(10 * u.deg)\n    lat = Latitude(20 * u.deg)\n    a = Angle([1, 2, 3] * u.deg)\n\n    with pytest.raises(TypeError):\n        a[0] = lat\n    with pytest.raises(TypeError):\n        lat[()] = a\n\n    with pytest.raises(TypeError):\n        a[0] = lon\n    with pytest.raises(TypeError):\n        lon[()] = a\n", "def test_longitude_wrap_angle():\n    # Test Longitude with default wrap_angle (360 deg)\n    lon = Longitude(370 * u.deg)\n    assert_allclose(lon.degree, 10)\n\n    # Test Longitude with custom wrap_angle (180 deg)\n    lon = Longitude(190 * u.deg, wrap_angle=180 * u.deg)\n    assert_allclose(lon.degree, -170)\n\n    # Test Longitude with negative angle and custom wrap_angle\n    lon = Longitude(-190 * u.deg, wrap_angle=180 * u.deg)\n    assert_allclose(lon.degree, 170)\n\n    # Test setting wrap_angle after initialization\n    lon = Longitude(370 * u.deg)\n    lon.wrap_angle = 180 * u.deg\n    assert_allclose(lon.degree, -170)\n", "def test_angle_isclose():\n    a1 = Angle(10 * u.deg)\n    a2 = Angle(10.000001 * u.deg)\n    assert a1.isclose(a2)\n\n    a1 = Angle([10, 20] * u.deg)\n    a2 = Angle([10.000001, 19.999999] * u.deg)\n    assert np.all(a1.isclose(a2))\n\n    a1 = Angle([10, 20] * u.deg)\n    a2 = Angle([10.000001, 20.1] * u.deg)\n    assert not np.all(a1.isclose(a2))\n", "def test_angle_subclass_conversions():\n    # Check that Latitude and Longitude subclasses convert to Angle correctly\n    lat = Latitude(10 * u.deg)\n    lon = Longitude(20 * u.deg)\n\n    # Check that direct construction works\n    assert isinstance(lat, Latitude)\n    assert isinstance(lon, Longitude)\n\n    # Check that conversion back to Angle works\n    assert isinstance(lat.view(Angle), Angle)\n    assert isinstance(lon.view(Angle), Angle)\n\n    # Check that conversion from Latitude/Latitude to Angle happens during operations\n    result = lat + 10 * u.deg\n    assert isinstance(result, Angle)\n    assert not isinstance(result, Latitude)\n    result = lon - 10 * u.deg\n    assert isinstance(result, Angle)\n    assert not isinstance(result, Longitude)\n", "def test_angle_to_string_precision():\n    a = Angle(1.23456789, unit=u.deg)\n    assert a.to_string(decimal=True, precision=3) == '1.235'\n    assert a.to_string(decimal=True, precision=8) == '1.23456789'\n    assert a.to_string(decimal=False, precision=2) == '1:14:04.44'\n"], "sample_13": ["def test_angle_to_string():\n    # Test the to_string method of Angle objects\n\n    # Create an Angle object\n    a = Angle(10.2345, unit=u.deg)\n\n    # Test with default arguments\n    assert a.to_string() == '10.2345d'\n\n    # Test with decimal=True\n    assert a.to_string(decimal=True) == '10.2345'\n\n    # Test with precision argument\n    assert a.to_string(precision=2) == '10.23d'\n    assert a.to_string(decimal=True, precision=2) == '10.23'\n\n    # Test with sep argument\n    assert a.to_string(sep='hms') == '0h41m24.42s'\n\n    # Test with alwayssign argument\n    assert a.to_string(alwayssign=True) == '+10.2345d'\n\n    # Test with pad argument\n    assert a.to_string(pad=True) == '010.2345d'\n\n    # Test with fields argument\n    assert a.to_string(fields=2) == '10d14m'\n", "def test_angle_to_string_precision():\n    # Test that the precision argument of to_string works as expected\n    a = Angle(1.23456789 * u.deg)\n    assert a.to_string(decimal=True, precision=3) == '1.235'\n    assert a.to_string(decimal=True, precision=8) == '1.23456789'\n\n    # Test that it works for sexagesimal representation as well\n    assert a.to_string(precision=2) == '1:14:04.45'\n    assert a.to_string(precision=4) == '1:14:04.4436'\n\n    # Test that it raises an error for negative precision\n    with pytest.raises(ValueError):\n        a.to_string(precision=-1)\n\n    # Test that it does not round the underlying value\n    b = Angle(a.to_string(precision=3), unit=u.deg)\n    assert b != a\n", "def test_wrap_at():\n    # Regression test for https://github.com/astropy/astropy/issues/15067\n    angle = Angle([0, 180, 360] * u.deg)\n    wrapped_angle = angle.wrap_at(90 * u.deg)\n    assert_array_equal(wrapped_angle.degree, [-270., -90., 90.])\n", "def test_longitude_wrap_angle():\n    # Test setting and getting of wrap_angle attribute\n    lon = Longitude(10 * u.deg)\n    assert lon.wrap_angle == Angle(360 * u.deg)\n    \n    lon.wrap_angle = 180 * u.deg\n    assert lon.wrap_angle == Angle(180 * u.deg)\n    \n    # Test that changing wrap_angle re-wraps the angle values\n    lon = Longitude(190 * u.deg)\n    assert lon.degree == 190\n    \n    lon.wrap_angle = 180 * u.deg\n    assert lon.degree == -170\n    \n    # Test that wrap_angle is inherited when creating a new Longitude object\n    lon2 = Longitude(lon)\n    assert lon2.wrap_angle == lon.wrap_angle\n", "def test_longitude_wrap_at():\n    # Test that Longitude objects wrap at the correct angle.\n    lon = Longitude(350 * u.deg)\n    assert_allclose(lon.wrap_at(180 * u.deg).degree, -10)\n\n    # Test that the wrap_angle property is respected.\n    lon = Longitude(350 * u.deg, wrap_angle=180 * u.deg)\n    assert_allclose(lon.degree, -10)\n\n    # Test that changing the wrap_angle property rewraps the angles.\n    lon = Longitude(350 * u.deg)\n    lon.wrap_angle = 180 * u.deg\n    assert_allclose(lon.degree, -10)\n"], "sample_14": ["def test_angle_is_within_bounds():\n    a = Angle([-20, 150, 350] * u.deg)\n    assert a.is_within_bounds('0d', '360d') is False\n    assert a.is_within_bounds(None, '360d') is True\n    assert a.is_within_bounds(-30 * u.deg, None) is True\n", "def test_angle_array_wrap_at():\n    # Test that wrapping an Angle array in-place works\n    angles = Angle([350, 370, 10, -10]*u.deg)\n    angles.wrap_at('360d', inplace=True)\n    assert_allclose(angles.degree, [350, 10, 10, 350])\n\n    # Test that wrapping an Angle array not-in-place works\n    angles = Angle([350, 370, 10, -10]*u.deg)\n    wrapped_angles = angles.wrap_at('360d')\n    assert_allclose(wrapped_angles.degree, [350, 10, 10, 350])\n    assert_allclose(angles.degree, [350, 370, 10, -10])\n", "def test_angle_subclass_pickle():\n    # Regression test for gh-10969\n    lat = Latitude(10*u.deg)\n    lon = Longitude(20*u.deg)\n\n    with pytest.raises(TypeError):\n        pickle.loads(pickle.dumps(lat, protocol=0))\n\n    with pytest.raises(TypeError):\n        pickle.loads(pickle.dumps(lon, protocol=0))\n", "def test_angle_isclose():\n    # Test that isclose works for Angle objects\n    a1 = Angle(10 * u.deg)\n    a2 = Angle(10.000001 * u.deg)\n    assert np.isclose(a1, a2)\n\n    # Test that isclose broadcasting works for Angle objects\n    a1 = Angle([10, 20] * u.deg)\n    a2 = Angle([10.000001, 19.999999] * u.deg)\n    assert np.all(np.isclose(a1, a2))\n\n    # Test that isclose works for Latitude objects\n    a1 = Latitude(10 * u.deg)\n    a2 = Latitude(10.000001 * u.deg)\n    assert np.isclose(a1, a2)\n\n    # Test that isclose works for Longitude objects\n    a1 = Longitude(10 * u.deg)\n    a2 = Longitude(370.000001 * u.deg)\n    assert np.isclose(a1, a2)\n", "def test_angle_unary_ufuncs():\n    # Test that Angle and its subclasses behave correctly for unary ufuncs\n    a = Angle([1, 2, 3]*u.deg)\n    b = Latitude([1, 2, 3]*u.deg)\n    c = Longitude([1, 2, 3]*u.deg)\n\n    # Test with np.abs\n    assert isinstance(np.abs(a), Angle)\n    assert isinstance(np.abs(b), Angle)\n    assert isinstance(np.abs(c), Angle)\n\n    # Test with np.positive\n    assert isinstance(np.positive(a), Angle)\n    assert isinstance(np.positive(b), Angle)\n    assert isinstance(np.positive(c), Angle)\n\n    # Test with np.negative\n    assert isinstance(np.negative(a), Angle)\n    assert isinstance(np.negative(b), Angle)\n    assert isinstance(np.negative(c), Angle)\n"], "sample_15": ["def test_quantity_to_string_precision():\n    q = u.Quantity(1.123456, \"m\")\n    assert q.to_string(precision=3) == \"1.12 m\"\n    assert q.to_string(precision=None) == \"1.123456 m\"\n\n    # Test with array quantity\n    q = u.Quantity([1.123456, 2.123456], \"m\")\n    assert q.to_string(precision=3) == \"[1.12 2.12] m\"\n    assert q.to_string(precision=None) == \"[1.123456 2.123456] m\"\n", "def test_quantity_new_view():\n    # Test that Quantity._new_view correctly copies the unit of the input quantity.\n    q = u.Quantity(10, u.m)\n    new_q = q._new_view(q.value)\n    assert new_q.unit == q.unit\n\n    # Test that Quantity._new_view correctly handles units with powers.\n    q = u.Quantity(10, u.m ** 2)\n    new_q = q._new_view(q.value)\n    assert new_q.unit == q.unit\n\n    # Test that Quantity._new_view correctly handles dimensionless units.\n    q = u.Quantity(10, u.dimensionless_unscaled)\n    new_q = q._new_view(q.value)\n    assert new_q.unit == q.unit\n", "def test_quantity_str():\n    q = 10 * u.m\n    assert str(q) == \"10 m\"\n\n    q = 1.0e+10 * u.m\n    assert str(q) == \"1e+10 m\"\n\n    q = np.array([10, 20]) * u.m\n    assert str(q) == \"[10 20] m\"\n\n    q = np.array([10.5, 20.5]) * u.m\n    assert str(q) == \"[10.5 20.5] m\"\n", "def test_quantity_with_non_numeric_input():\n    with pytest.raises(TypeError) as exc:\n        u.Quantity(\"hello\", u.m)\n    assert \"The value must be a valid Python or Numpy numeric type.\" in str(exc.value)\n\n    with pytest.raises(TypeError) as exc:\n        u.Quantity([\"hello\"], u.m)\n    assert \"The value must be a valid Python or Numpy numeric type.\" in str(exc.value)\n\n    with pytest.raises(TypeError) as exc:\n        u.Quantity(object(), u.m)\n    assert \"The value must be a valid Python or Numpy numeric type.\" in str(exc.value)\n", "def test_quantity_isclose():\n    # Test that isclose works with quantities, and raises the right exceptions\n    q1 = np.array([1.0, 2.0, 3.0]) * u.m\n    q2 = np.array([1.000000001, 2.0, 3.0]) * u.m\n\n    assert np.all(isclose(q1, q2))\n    assert not np.any(isclose(q1, q2, atol=1e-12))\n\n    with pytest.raises(u.UnitsError):\n        isclose(q1, q2.value)\n\n    with pytest.raises(u.UnitsError):\n        isclose(q1.value, q2)\n\n    with pytest.raises(u.UnitsError):\n        isclose(q1, q2, atol=1*u.s)\n"], "sample_16": ["    def test_full_like(self):\n        self.check(np.full_like, fill_value=1 * u.m)\n        self.check(np.full_like, fill_value=1)\n", "    def test_diagonal(self):\n        self.check(np.diagonal)\n", "    def test_array_equal(self):\n        assert np.array_equal(self.q, self.q)\n", "    def test_atleast_1d(self):\n        self.check(np.atleast_1d)\n", "    def test_simple(self):\n        arr = np.array([(1, 2.5, 'hello'), (2, 3.5, 'world')],\n                       dtype=[('x', 'i4'), ('y', 'f4'), ('s', 'S10')])\n        unit = u.StructuredUnit((u.m, u.s, u.dimensionless_unscaled))\n        q = u.Quantity(arr, unit)\n        out = rfn.structured_to_unstructured(q)\n        expected = rfn.structured_to_unstructured(arr) * u.m\n        assert_array_equal(out, expected)\n"], "sample_17": ["    def test_atleast_1d(self):\n        self.check(np.atleast_1d)\n", "    def check(self, func, *args, **kwargs):\n        with pytest.raises(TypeError):\n            func(self.q, *args, **kwargs)\n", "    def test_sum(self):\n        self.check(np.sum)\n", "    def test_structured_to_unstructured(self):\n        dtype = np.dtype([(\"x\", float), (\"y\", float)])\n        q = np.array([(1, 2), (3, 4)], dtype=dtype) * u.m\n        expected = np.array([[1, 2], [3, 4]]) * u.m\n        assert_array_equal(np.lib.recfunctions.structured_to_unstructured(q), expected)\n", "    def test_reshape(self):\n        self.check(np.reshape, (2, -1))\n"], "sample_18": ["def test_info_copy(self):\n    q2 = copy.deepcopy(self.q)\n    assert_info_equal(self.q, q2)\n", "def test_info_propagation_copy(self):\n    q2 = copy.copy(self.q)\n    assert_info_equal(self.q, q2)\n", "def test_info_preserved_copy(self):\n    q2 = copy.copy(self.q)\n    assert_info_equal(self.q, q2)\n", "def test_quantity_info_copy(self):\n    q_copy = copy.deepcopy(self.q)\n    assert_info_equal(self.q, q_copy)\n", "def test_quantity_info_copy(self):\n    q2 = copy.deepcopy(self.q)\n    assert_info_equal(self.q, q2)\n    q3 = self.q.copy()\n    assert_info_equal(self.q, q3)\n"], "sample_20": ["def test_read_table_fits_memmap(self, tmpdir):\n    filename = str(tmpdir.join(\"test.fits\"))\n    fits.writeto(filename, np.array([1, 2, 3]), overwrite=True)\n    with fits.open(filename, mode=\"append\") as hdul:\n        table_hdu = table_to_hdu(Table({\"a\": [1, 2, 3]}))\n        hdul.append(table_hdu)\n\n    # Test that memmap=True does not raise an error\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", category=AstropyUserWarning)\n        t = Table.read(filename, hdu=1, memmap=True)\n\n    assert len(t) == 3\n    assert t[\"a\"][0] == 1\n    assert t[\"a\"][1] == 2\n    assert t[\"a\"][2] == 3\n", "def test_read_table_fits_with_astropy_native(self):\n    # Create a FITS table with a Time column\n    time = Time([1, 2, 3], format=\"cxcsec\")\n    table = Table({\"time\": time})\n    hdul = HDUList([PrimaryHDU(), table_to_hdu(table)])\n    hdul.writeto(\"test.fits\", overwrite=True)\n\n    # Read the FITS table with astropy_native=True\n    with fits.open(\"test.fits\") as hdul:\n        table_read = Table.read(hdul[1], astropy_native=True)\n\n    # Check that the Time column is read correctly\n    assert isinstance(table_read[\"time\"], Time)\n    assert_array_equal(table_read[\"time\"].value, time.value)\n", "def test_read_table_fits_with_memmap(self):\n    # Create a FITS file with a table extension\n    filename = \"test.fits\"\n    hdu = BinTableHDU.from_columns(\n        [fits.Column(name=\"a\", format=\"I\", array=[1, 2, 3, 4])],\n        name=\"TEST\",\n    )\n    hdu.writeto(filename)\n\n    # Read the table with memmap=True\n    table = Table.read(filename, format=\"fits\", memmap=True)\n\n    # Check that the data is correct\n    assert_array_equal(table[\"a\"], [1, 2, 3, 4])\n\n    # Check that the data is a memmap array\n    assert isinstance(table[\"a\"].data, np.memmap)\n", "def test_fits_round_trip():\n    # Test that a table can be written to FITS and read back in without losing data\n\n    # Create a table with some sample data\n    t = Table(\n        {\n            \"a\": [1, 2, 3],\n            \"b\": [\"x\", \"y\", \"z\"],\n            \"c\": [4.5, 5.6, 6.7],\n            \"d\": [True, False, True],\n            \"e\": [1 + 1j, 2 + 2j, 3 + 3j],\n        }\n    )\n\n    # Write the table to a FITS file\n    t.write(\"test.fits\", overwrite=True)\n\n    # Read the table back in from the FITS file\n    t2 = Table.read(\"test.fits\")\n\n    # Check that the tables are equal\n    assert equal_data(t, t2)\n", "def test_read_table_fits_memmap(self):\n    # Create a FITS file with a table\n    filename = \"test.fits\"\n    hdu = BinTableHDU.from_columns(\n        [fits.Column(name=\"a\", format=\"I\", array=[1, 2, 3])],\n        nrows=3,\n    )\n    hdu.writeto(filename)\n\n    # Read the table with memmap=True\n    t = Table.read(filename, format=\"fits\", memmap=True)\n\n    # Check that the data is correct\n    assert_array_equal(t[\"a\"], np.array([1, 2, 3]))\n\n    # Check that the data is a memmap array\n    assert isinstance(t[\"a\"].data, np.memmap)\n"], "sample_21": ["def test_write_table_qdp_err_specs():\n    # Create a simple table\n    t = Table({'a': [1, 2, 3], 'b': [4, 5, 6]})\n\n    # Add error columns\n    t['a_err'] = [0.1, 0.2, 0.3]\n    t['b_perr'] = [0.4, 0.5, 0.6]\n    t['b_nerr'] = [0.7, 0.8, 0.9]\n\n    # Write the table to a QDP file with error specs\n    lines = _write_table_qdp(t, err_specs={'serr': [1], 'terr': [2]})\n\n    # Check that the error specs are correctly written\n    assert 'READ SERR 1' in lines\n    assert 'READ TERR 2' in lines\n\n    # Check that the error columns are correctly written\n    assert '1 0.1 4 0.4 0.7' in ' '.join(lines)\n    assert '2 0.2 5 0.5 0.8' in ' '.join(lines)\n    assert '3 0.3 6 0.6 0.9' in ' '.join(lines)\n", "def test_get_lines_from_file(tmpdir):\n    # Create a temporary file with some content\n    filename = tmpdir.join('test.qdp')\n    with open(filename, 'w') as f:\n        f.write('Line 1\\nLine 2\\nLine 3')\n\n    # Test reading from a file\n    lines = _get_lines_from_file(str(filename))\n    assert lines == ['Line 1', 'Line 2', 'Line 3']\n\n    # Test reading from a string\n    lines = _get_lines_from_file('Line 1\\nLine 2\\nLine 3')\n    assert lines == ['Line 1', 'Line 2', 'Line 3']\n\n    # Test reading from an iterable\n    lines = _get_lines_from_file(['Line 1', 'Line 2', 'Line 3'])\n    assert lines == ['Line 1', 'Line 2', 'Line 3']\n\n    # Test invalid input\n    with pytest.raises(ValueError):\n        _get_lines_from_file(123)\n", "def test_write_table_qdp_err_specs():\n    # Test writing a table with error specs\n    table = Table(\n        [\n            Column([1, 2, 3], name=\"a\"),\n            Column([0.1, 0.2, 0.3], name=\"a_err\"),\n            Column([4, 5, 6], name=\"b\"),\n            Column([0.4, 0.5, 0.6], name=\"b_perr\"),\n            Column([0.7, 0.8, 0.9], name=\"b_nerr\"),\n        ]\n    )\n    err_specs = {\"serr\": [1], \"terr\": [2]}\n    lines = _write_table_qdp(table, err_specs=err_specs)\n    assert \"READ SERR 1\" in lines\n    assert \"READ TERR 2\" in lines\n", "def test_get_lines_from_file(tmpdir):\n    # Create a temporary QDP file\n    qdp_file = tmpdir.join('test.qdp')\n    qdp_file.write('! Comment line\\n'\n                   'READ TERR 1\\n'\n                   'READ SERR 3\\n'\n                   '53000.5   0.25  -0.5   1  1.5  3.5 2\\n'\n                   'NO NO NO NO NO\\n'\n                   '! Another comment line\\n'\n                   '54000.5   2.25  -2.5   NO  3.5  5.5 5\\n')\n\n    # Test reading lines from the file\n    lines = _get_lines_from_file(str(qdp_file))\n    assert len(lines) == 7\n\n    # Test reading lines from a string\n    lines = _get_lines_from_file(qdp_file.read())\n    assert len(lines) == 7\n\n    # Test reading lines from an iterable\n    lines = _get_lines_from_file(qdp_file.read().split('\\n'))\n    assert len(lines) == 7\n\n    # Test invalid input\n    with pytest.raises(ValueError):\n        _get_lines_from_file(123)\n", "def test_get_lines_from_file():\n    # Test with a string containing newline characters\n    qdp_file = \"Line 1\\nLine 2\\nLine 3\"\n    lines = _get_lines_from_file(qdp_file)\n    assert len(lines) == 3\n    assert lines[0] == \"Line 1\"\n    assert lines[1] == \"Line 2\"\n    assert lines[2] == \"Line 3\"\n\n    # Test with a filename\n    with open(\"test.qdp\", \"w\") as f:\n        f.write(\"Line 1\\nLine 2\\nLine 3\")\n\n    lines = _get_lines_from_file(\"test.qdp\")\n    assert len(lines) == 3\n    assert lines[0] == \"Line 1\"\n    assert lines[1] == \"Line 2\"\n    assert lines[2] == \"Line 3\"\n\n    # Test with an iterable of lines\n    lines = [\"Line 1\", \"Line 2\", \"Line 3\"]\n    result = _get_lines_from_file(lines)\n    assert len(result) == 3\n    assert result[0] == \"Line 1\"\n    assert result[1] == \"Line 2\"\n    assert result[2] == \"Line 3\"\n\n    # Test with invalid input\n    with pytest.raises(ValueError):\n        _get_lines_from_file(123)\n"], "sample_22": ["def test_rotation_matrix_x_axis():\n    # Test rotation matrix around x-axis\n    angle = np.radians(90)\n    matrix = rotation_matrix(angle, axis='x')\n    expected = np.array([[1, 0, 0], [0, 0, -1], [0, 1, 0]])\n    assert_allclose(matrix, expected)\n\n    # Test rotation matrix around x-axis with negative angle\n    angle = np.radians(-90)\n    matrix = rotation_matrix(angle, axis='x')\n    expected = np.array([[1, 0, 0], [0, 0, 1], [0, -1, 0]])\n    assert_allclose(matrix, expected)\n", "def test_rotation_matrix():\n    # Test that rotation_matrix gives the expected results for some known angles.\n    assert_allclose(rotation_matrix(0, axis=\"z\"), np.eye(3))\n    assert_allclose(rotation_matrix(90, axis=\"z\"), [[0, 1, 0], [-1, 0, 0], [0, 0, 1]])\n    assert_allclose(rotation_matrix(180, axis=\"z\"), [[-1, 0, 0], [0, -1, 0], [0, 0, 1]])\n\n    # Test that it also works with Quantity and Array inputs\n    assert_allclose(\n        rotation_matrix(0 * u.deg, axis=\"z\"),\n        np.eye(3),\n    )\n    assert_allclose(\n        rotation_matrix(np.array([0, 90, 180]) * u.deg, axis=\"z\"),\n        [\n            [1, 0, 0],\n            [0, 1, 0],\n            [0, 0, 1],\n        ],\n        atol=1e-10,\n    )\n\n    # Test with an array of angles\n    angles = np.array([0, 90, 180])\n    result = rotation_matrix(angles, axis=\"z\")\n    assert result.shape == (3, 3, 3)\n    for i in range(len(angles)):\n        assert_allclose(result[i], rotation_matrix(angles[i], axis=\"z\"))\n\n    # Test with a non-standard axis\n    assert_allclose(\n        rotation_matrix(90, axis=[1, 0, 0]),\n        [[1, 0, 0], [0, 0, -1], [0, 1, 0]],\n    )\n\n    # Test with an invalid axis specification\n    with pytest.raises(TypeError):\n        rotation_matrix(90, axis=1)\n", "def test_matrix_transpose():\n    # Create a 3x3 matrix\n    matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n\n    # Transpose the matrix using the function\n    transposed_matrix = matrix_transpose(matrix)\n\n    # Check if the transposed matrix is correct\n    assert_array_equal(transposed_matrix, np.transpose(matrix))\n\n    # Create a stack of 2x2 matrices\n    matrix_stack = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n\n    # Transpose each matrix in the stack using the function\n    transposed_matrix_stack = matrix_transpose(matrix_stack)\n\n    # Check if the transposed matrices are correct\n    assert_array_equal(transposed_matrix_stack, np.transpose(matrix_stack, (0, 2, 1)))\n", "def test_rotation_matrix_x_axis():\n    angle = 90 * u.deg\n    matrix = rotation_matrix(angle, axis=\"x\")\n    expected_matrix = np.array(\n        [[1.0, 0.0, 0.0], [0.0, 0.0, -1.0], [0.0, 1.0, 0.0]]\n    )\n    assert_allclose(matrix, expected_matrix)\n", "def test_rotation_matrix_xy():\n    # Test rotation matrices around x and y axes for 90 degrees.\n    angle = np.pi / 2\n\n    # Rotation matrix around x axis for 90 degrees counter-clockwise.\n    rot_x = np.array([[1, 0, 0], [0, 0, -1], [0, 1, 0]])\n    assert_allclose(rotation_matrix(angle, axis='x'), rot_x)\n\n    # Rotation matrix around y axis for 90 degrees counter-clockwise.\n    rot_y = np.array([[0, 0, 1], [0, 1, 0], [-1, 0, 0]])\n    assert_allclose(rotation_matrix(angle, axis='y'), rot_y)\n"], "sample_23": ["def test_longitude_wrap_angle():\n    # Test setting wrap_angle on Longitude\n    lon = Longitude(180 * u.deg)\n    assert lon.wrap_angle == Angle(360 * u.deg)\n    lon.wrap_angle = 180 * u.deg\n    assert lon.wrap_angle == Angle(180 * u.deg)\n\n    # Test that setting wrap_angle wraps the longitude value\n    lon = Longitude(190 * u.deg, wrap_angle=180 * u.deg)\n    assert_allclose(lon.degree, -170)\n\n    # Test that wrap_angle is preserved when copying Longitude\n    lon2 = lon.copy()\n    assert lon2.wrap_angle == lon.wrap_angle\n\n    # Test that Longitude raises error if set to Latitude\n    with pytest.raises(TypeError):\n        lon[0] = Latitude(20 * u.deg)\n", "def test_longitude_set_wrap_angle():\n    lon = Longitude(100 * u.deg)\n    assert lon.wrap_angle == Angle(360 * u.deg)\n\n    lon.wrap_angle = 180 * u.deg\n    assert lon.wrap_angle == Angle(180 * u.deg)\n\n    with pytest.raises(u.UnitsError):\n        lon.wrap_angle = 1 * u.second\n\n    lon.wrap_angle = Angle(200 * u.deg)\n    assert lon.wrap_angle == Angle(200 * u.deg)\n", "def test_longitude_wrap_angle():\n    # Test setting and getting of wrap_angle\n    lon = Longitude(10 * u.deg)\n    assert lon.wrap_angle == 360 * u.deg\n    lon.wrap_angle = 180 * u.deg\n    assert lon.wrap_angle == 180 * u.deg\n    \n    # Test wrapping at initialization\n    lon = Longitude(190 * u.deg, wrap_angle=180 * u.deg)\n    assert lon.degree == -170.\n    \n    # Test that wrap_angle is preserved when making a new Longitude\n    lon2 = Longitude(lon)\n    assert lon2.wrap_angle == lon.wrap_angle\n    \n    # Test that wrap_angle can be changed after initialization\n    lon2.wrap_angle = 0 * u.deg\n    assert lon2.degree == 190.\n    \n    # Test wrap_angle with array-valued Longitude\n    lon = Longitude([10, 20, 350] * u.deg, wrap_angle=180 * u.deg)\n    assert_array_equal(lon.degree, [10, 20, -10])\n", "def test_angle_array_creation():\n    # Test that an Angle array can be created from a sequence of strings\n    angles = Angle(['1d', '2d', '3d'])\n    assert_array_equal(angles.degree, np.array([1., 2., 3.]))\n\n    # Test that an Angle array can be created from a sequence of quantities\n    angles = Angle([1*u.deg, 2*u.deg, 3*u.deg])\n    assert_array_equal(angles.degree, np.array([1., 2., 3.]))\n\n    # Test that an Angle array can be created from a numpy array of quantities\n    angles = Angle(np.array([1*u.deg, 2*u.deg, 3*u.deg]))\n    assert_array_equal(angles.degree, np.array([1., 2., 3.]))\n", "def test_angle_arraycreation():\n    # Test creating an Angle array from a sequence of strings.\n    a = Angle(['1d', '2d', '3d'])\n    assert_array_equal(a.degree, np.array([1., 2., 3.]))\n\n    a = Angle(['1h', '2h', '3h'])\n    assert_array_equal(a.hour, np.array([1., 2., 3.]))\n\n    a = Angle(['1:00:00.0', '2:00:00', '3:00:00'], unit=u.degree)\n    assert_array_equal(a.degree, np.array([1., 2., 3.]))\n\n    a = Angle(['1:00:00.0', '2:00:00', '3:00:00'], unit=u.hourangle)\n    assert_array_equal(a.hour, np.array([1., 2., 3.]))\n"], "sample_24": ["    def test_inv(self):\n        self.check(np.linalg.inv)\n", "    def test_fft(self, func):\n        self.check(func)\n", "    def test_array_equal(self):\n        self.check(np.array_equal, self.mb)\n", "    def test_dispatched_functions(self, func):\n        if func in IGNORED_FUNCTIONS or func in UNSUPPORTED_FUNCTIONS:\n            pytest.skip(f\"{func.__name__} is not supported\")\n        elif func.__name__.startswith(\"matrix\"):\n            pytest.skip(f\"{func.__name__} requires matrix input\")\n\n        try:\n            self.check(func)\n        except Exception as e:\n            raise AssertionError(\n                f\"Failed to run {func.__name__} on MaskedArray: {e}\"\n            ) from e\n", "    def test_concatenate(self):\n        self.check(np.concatenate, ([1, 2],))\n        self.check2(np.concatenate, ([1, 2],))\n"], "sample_25": ["def test_card_fromstring_bytes():\n    # Test creating a Card from a bytes object\n    card = fits.Card.fromstring(b\"KEYWORD  = value\")\n    assert card.keyword == \"KEYWORD\"\n    assert card.value == \"value\"\n    assert card.comment == \"\"\n", "def test_card_equality():\n    c1 = fits.Card('NAXIS', 2)\n    c2 = fits.Card('NAXIS', 2)\n    assert c1 == c2\n\n    c3 = fits.Card('NAXIS', 3)\n    assert c1 != c3\n\n    c4 = fits.Card.fromstring(str(c1))\n    assert c1 == c4\n", "def test_card_keyword_length():\n    # Test that a Card with a keyword longer than 8 characters raises an error\n    with pytest.raises(ValueError):\n        fits.Card(keyword='TOOLONGKEY')\n", "def test_card_invalid_unparsable():\n    # Test that the Card class handles invalid/unparsable cards correctly\n    card = fits.Card.fromstring(\"FOO = invalid value\")\n    assert card._invalid\n\n    with pytest.raises(ValueError):\n        card.value = \"new value\"\n\n    with pytest.raises(ValueError):\n        card.comment = \"new comment\"\n\n    with pytest.raises(ValueError):\n        del card.value\n\n    with pytest.raises(ValueError):\n        del card.comment\n", "def test_long_string():\n    # Test that a long string is properly broken up into CONTINUE cards\n    long_value = \"a\" * 1000\n    c = fits.Card(\"LONGSTRING\", long_value)\n    assert len(c.image) > 80\n    # Make sure the value was preserved\n    assert ''.join(c.value.split('&')).replace(\"'\", '') == long_value\n    # Now make sure it can be read back in without issue\n    h = fits.Header([c])\n    new_c = h.cards[0]\n    assert new_c.keyword == 'LONGSTRING'\n    assert new_c.value == long_value\n"], "sample_26": ["def test_image_hdu_scale_back():\n    # Create an ImageHDU with scale_back=True\n    data = np.array([1, 2, 3], dtype=np.uint8)\n    hdu = fits.ImageHDU(data, scale_back=True)\n\n    # Scale the data and write to a file\n    hdu.scale(type='int16', bscale=2, bzero=10)\n    hdu.writeto('test.fits')\n\n    # Read the file back in and check that the data is unscaled\n    with fits.open('test.fits') as hdul:\n        assert_equal(hdul[0].data, data)\n    os.remove('test.fits')\n", "def test_image_section():\n    # Test ImageHDU section method with different indexing methods\n    data = np.arange(100).reshape((10, 10))\n    hdu = fits.PrimaryHDU(data)\n\n    # Slicing with integer indices\n    assert_equal(hdu.section[2, 3], data[2, 3])\n\n    # Slicing with slice indices\n    assert_equal(hdu.section[2:5, 3:6], data[2:5, 3:6])\n\n    # Slicing with iterable indices\n    assert_equal(hdu.section[[1, 3, 5], [2, 4, 6]], data[[1, 3, 5], [2, 4, 6]])\n\n    # Slicing with negative indices\n    assert_equal(hdu.section[-1, -2], data[-1, -2])\n\n    # Slicing with Ellipsis\n    assert_equal(hdu.section[..., 3:], data[..., 3:])\n", "def test_image_hdu_scale_back(tmpdir):\n    # Test that the scale_back feature works as expected\n    data = np.arange(100, dtype=np.uint16)\n    hdu = fits.PrimaryHDU(data, do_not_scale_image_data=True)\n    hdu.scale('int16', bzero=32768)\n\n    # Set scale_back to True and write to a new file\n    hdu.header['BZERO'] = 32768\n    hdu.header['BSCALE'] = 1\n    hdu.scale_back = True\n    filename = tmpdir.join('test.fits')\n    hdu.writeto(filename)\n\n    # Check that the data was scaled back correctly\n    with fits.open(filename) as hdul:\n        assert hdul[0].data.dtype == np.uint16\n        assert np.all(hdul[0].data == data)\n", "def test_image_hdu_init_scale_back():\n    # Test that initializing an ImageHDU with scale_back=True correctly\n    # preserves the original BSCALE and BZERO values when saving changes\n    data = np.array([1, 2, 3], dtype=np.int16)\n    header = fits.Header()\n    header[\"BSCALE\"] = 2.0\n    header[\"BZERO\"] = 10.0\n\n    hdu = fits.ImageHDU(data=data, header=header, scale_back=True)\n\n    # Make some changes to the data\n    hdu.data += 1\n\n    # Save the changes to a new HDU\n    new_hdu = fits.ImageHDU(hdu.data, header=hdu.header)\n\n    # Check that the original BSCALE and BZERO values are preserved\n    assert new_hdu.header[\"BSCALE\"] == 2.0\n    assert new_hdu.header[\"BZERO\"] == 10.0\n", "def test_image_hdu_scale_back(tmpdir):\n    # Create an unsigned int image\n    data = np.arange(256, dtype=np.uint8)\n    hdu = fits.PrimaryHDU(data)\n\n    # Scale the data to a float\n    hdu.scale('float32', bscale=1.0, bzero=128.0)\n\n    # Write the HDU to a file and read it back\n    filename = tmpdir.join('test.fits')\n    hdu.writeto(filename)\n    with fits.open(filename) as hdul:\n        hdu_read = hdul[0]\n\n    # Verify that the data is scaled back correctly\n    assert_equal(hdu_read.data, data)\n\n    # Now try with scale_back=False\n    hdu = fits.PrimaryHDU(data)\n    hdu.scale('float32', bscale=1.0, bzero=128.0)\n    hdu.writeto(filename, overwrite=True)\n    with fits.open(filename) as hdul:\n        hdu_read = hdul[0]\n    assert not np.allclose(hdu_read.data, data)\n"], "sample_27": ["def test_diff_keyword_values(self):\n    header1 = Header(cards=[(\"NAXIS\", 2, \"Number of axes\"), (\"CTYPE1\", \"RA---TAN\", \"Axis 1 type\")])\n    header2 = Header(cards=[(\"NAXIS\", 3, \"Number of axes\"), (\"CTYPE1\", \"DEC--TAN\", \"Axis 1 type\")])\n\n    diff = HeaderDiff(header1, header2)\n    assert not diff.identical\n\n    report = diff.report()\n    assert \"Keyword 'NAXIS' has different values:\" in report\n    assert \"Keyword 'CTYPE1' has different values:\" in report\n", "def test_rawdatadiff():\n    # Test diffing raw data\n    a = np.array([1, 2, 3], dtype=np.uint8)\n    b = np.array([1, 4, 3], dtype=np.uint8)\n    diff = ImageDataDiff(a, b)\n    assert not diff.identical\n    assert diff.diff_total == 1\n\n    a = np.array([1, 2, 3, 4], dtype=np.uint8)\n    b = np.array([1, 2, 3], dtype=np.uint8)\n    diff = RawDataDiff(a, b)\n    assert not diff.identical\n    assert diff.diff_dimensions == (4, 3)\n\n    a = np.array([], dtype=np.uint8)\n    b = np.array([], dtype=np.uint8)\n    diff = RawDataDiff(a, b)\n    assert diff.identical\n    assert diff.diff_dimensions == ()\n", "def test_raw_data_diff():\n    data1 = np.array([1, 2, 3], dtype=np.uint8)\n    data2 = np.array([1, 2, 4], dtype=np.uint8)\n\n    hdu1 = DummyNonstandardExtHDU(data=data1)\n    hdu2 = DummyNonstandardExtHDU(data=data2)\n\n    diff = RawDataDiff(hdu1.data, hdu2.data)\n    assert not diff.identical\n\n    report = diff.report()\n    assert \"Data differs at byte 3\" in report\n    assert \"1 different bytes found\" in report\n", "def test_header_diff_duplicate_keywords(self):\n    header_a = Header([(\"SIMPLE\", True), (\"HISTORY\", \"First\"), (\"HISTORY\", \"Second\")])\n    header_b = Header([(\"SIMPLE\", True), (\"HISTORY\", \"First\"), (\"HISTORY\", \"Third\")])\n\n    diff = HeaderDiff(header_a, header_b)\n\n    assert not diff.identical\n    assert diff.diff_duplicate_keywords == {}\n    assert \"HISTORY\" in diff.diff_keyword_values\n", "def test_raw_data_diff():\n    data1 = np.array([1, 2, 3, 4, 5], dtype=np.uint8)\n    data2 = np.array([1, 2, 3, 4, 6], dtype=np.uint8)\n\n    hdu1 = DummyNonstandardExtHDU(data=data1)\n    hdu2 = DummyNonstandardExtHDU(data=data2)\n\n    diff = FITSDiff(hdu1, hdu2)\n\n    assert not diff.identical\n    assert len(diff.diff_hdus) == 1\n    assert isinstance(diff.diff_hdus[0][1].diff_data, RawDataDiff)\n    assert diff.diff_hdus[0][1].diff_data.diff_bytes == [(4, (5, 6))]\n"], "sample_28": ["def test_card_from_bytes():\n    # Test creating a Card from a bytes object\n    image = b\"KEYWORD  = value / comment\"\n    card = fits.Card.fromstring(image)\n    assert card.keyword == \"KEYWORD\"\n    assert card.value == \"value\"\n    assert card.comment == \"comment\"\n\n    # Test creating a Card from a bytes object with non-ASCII characters\n    image = b\"KEYWORD  = valu\\xeb / comment\"\n    card = fits.Card.fromstring(image)\n    assert card.keyword == \"KEYWORD\"\n    assert card.value == \"valu\\xeb\"\n    assert card.comment == \"comment\"\n", "def test_card_image_roundtrip():\n    # Test that creating a Card object from its own image results in the same\n    # image\n    c = fits.Card('FOO', 'bar baz', 'qux')\n    assert c.image == fits.Card.fromstring(c.image).image\n", "def test_card_invalid_unparsable():\n    # Test that cards with invalid/unparsable syntax are handled correctly;\n    # unparsable cards should have a meaningful repr() and str()\n    card = fits.Card.fromstring(\"FOO = invalid...\")\n    assert repr(card) == \"Card(FOO, None, None)\"\n    assert str(card).startswith(\"FOO = \")\n\n    # An unparsable card's value should be None, and assigning to the value or\n    # comment should raise an exception\n    assert card.value is None\n    assert card.comment is None\n\n    with pytest.raises(ValueError):\n        card.value = 42\n\n    with pytest.raises(ValueError):\n        card.comment = \"Some comment\"\n\n    # An unparsable card should still be able to be used in a header (as long as\n    # it's verified/fixed first)\n    header = fits.Header([card])\n    with pytest.warns(VerifyWarning):\n        header.verify(\"fix+warn\")\n\n    header.tostring()\n", "def test_card_invalid_keyword_length():\n    with pytest.raises(ValueError):\n        fits.Card(keyword='TOOLONGKEYWORD', value='value')\n", "def test_card_image_from_constructor(self):\n    # Test that creating a Card object with a value and comment also\n    # properly populates the card image\n    c = fits.Card('TEST', 'Hello, World!', 'This is a test')\n    assert c.image == \"TEST     = 'Hello, World!' / This is a test                         \"\n"], "sample_29": ["def test_write_latex_format_table(self, cosmo):\n    \"\"\"Test that ``write_latex`` uses the _FORMAT_TABLE.\"\"\"\n    # create file-like\n    with open(\"test.tex\", \"w\") as f:\n        write_latex(cosmo, f)\n\n    # read the file and check the contents\n    with open(\"test.tex\", \"r\") as f:\n        lines = f.readlines()\n\n    for param_name in cosmo.__parameters__:\n        latex_name = _FORMAT_TABLE.get(param_name, param_name)\n        assert latex_name in \"\".join(lines)\n", "def test_format_table_coverage(self, cosmo):\n    \"\"\"Test that _FORMAT_TABLE covers all parameters\"\"\"\n    # Get the cosmology class's parameters\n    params = type(cosmo).__parameters__\n\n    # Check that all parameters are in _FORMAT_TABLE\n    for param in params:\n        assert param in _FORMAT_TABLE, f\"Parameter '{param}' not in _FORMAT_TABLE\"\n", "def test_format_table_contents(self, cosmo):\n    \"\"\"Test the contents of ``_FORMAT_TABLE``.\"\"\"\n    # Get the parameters for which there are latex representations\n    params = [p for p in cosmo.__parameters__ if p in _FORMAT_TABLE]\n\n    # Make sure all the params are in _FORMAT_TABLE\n    assert set(params).issubset(_FORMAT_TABLE)\n\n    # Check each parameter has a non-empty string representation\n    for param in params:\n        assert isinstance(_FORMAT_TABLE[param], str)\n        assert len(_FORMAT_TABLE[param]) > 0\n", "def test_format_table_contents(self, cosmo):\n    \"\"\"\n    Test that the contents of `_FORMAT_TABLE` are used when `latex_names=True`\n    and ignored when `latex_names=False`.\n    \"\"\"\n    # Make sure at least one parameter name will be changed by _FORMAT_TABLE\n    param_name = next((k for k in cosmo.__parameters__ if k in _FORMAT_TABLE), None)\n    assert param_name is not None  # param name that will be formatted\n\n    # Get table with latex_names=True (default) and check format of a parameter\n    table = QTable()\n    write_latex(cosmo, table, overwrite=True)\n    assert table.colnames[table.colnames.index(param_name)] == _FORMAT_TABLE[param_name]\n\n    # Now get table with latex_names=False and check format of same parameter\n    table = QTable()\n    write_latex(cosmo, table, latex_names=False, overwrite=True)\n    assert table.colnames[table.colnames.index(param_name)] == param_name\n", "def test_write_latex_format_table(cosmo):\n    \"\"\"Test ``write_latex`` correctly uses ``_FORMAT_TABLE``.\"\"\"\n    table = QTable()\n    write_latex(cosmo, table, format=\"latex\", cls=QTable)\n\n    # Check column names are LaTeX format\n    for name in cosmo.__parameters__:\n        latex_name = _FORMAT_TABLE.get(name, name)\n        assert latex_name in table.columns\n\n    # Check all columns from cosmology are in the table\n    assert len(table.columns) == len(cosmo.__parameters__)\n"], "sample_30": ["def test_votable_file_set_all_tables_format():\n    votable = parse_single_table(get_pkg_data_filename(\"data/nrao-1.1.xml\"))\n    votable.set_all_tables_format(\"binary\")\n    assert votable.get_first_table().format == \"binary\"\n    votable.set_all_tables_format(\"tabledata\")\n    assert votable.get_first_table().format == \"tabledata\"\n    votable.set_all_tables_format(\"binary2\")\n    assert votable.get_first_table().format == \"binary2\"\n    with pytest.raises(VOTableSpecError):\n        votable.set_all_tables_format(\"fits\")\n", "def test_votable_file_from_table():\n    from astropy.table import Table\n\n    # Create a simple table\n    t = Table({'a': [1, 2, 3], 'b': [4.0, 5.0, 6.0]})\n\n    # Convert to VOTableFile instance\n    votable = tree.VOTableFile.from_table(t)\n\n    # Check the resulting votable structure\n    assert len(votable.resources) == 1\n    assert len(votable.resources[0].tables) == 1\n    assert len(votable.resources[0].tables[0].fields) == 2\n\n    # Check the data\n    assert_array_equal(votable.resources[0].tables[0].array['a'], t['a'])\n    assert_array_equal(votable.resources[0].tables[0].array['b'], t['b'])\n", "def test_get_field_by_id():\n    votable = parse(get_pkg_data_filename(\"data/regression.xml\"))\n    field = votable.get_field_by_id(\"Dec\")\n    assert field.ID == \"Dec\"\n    assert field.name == \"Dec\"\n\n    with pytest.raises(KeyError):\n        votable.get_field_by_id(\"Nonexistent\")\n", "def test_votable_element():\n    votable = tree.VOTableFile()\n    assert votable.ID is None\n    votable.ID = \"test-id\"\n    assert votable.ID == \"test-id\"\n\n    # Test that ID is validated as an XML ID\n    with pytest.raises(VOTableSpecError):\n        votable.ID = \"test:id\"\n\n    # Test version attribute\n    assert votable.version == \"1.4\"\n    votable.version = \"1.3\"\n    assert votable.version == \"1.3\"\n\n    # Test that version is validated\n    with pytest.raises(ValueError):\n        votable.version = \"1.5\"\n", "def test_votable_file_round_trip(tmpdir):\n    # Create a VOTable file from scratch\n    votable = tree.VOTableFile()\n    resource = tree.Resource()\n    table = tree.Table(votable)\n    field1 = tree.Field(votable, ID=\"col1\", name=\"Column 1\", datatype=\"float\")\n    field2 = tree.Field(votable, ID=\"col2\", name=\"Column 2\", datatype=\"int\")\n    table.fields.extend([field1, field2])\n    resource.tables.append(table)\n    votable.resources.append(resource)\n\n    # Write the VOTable to a temporary file\n    filename = str(tmpdir.join(\"test_votable.xml\"))\n    votable.to_xml(filename)\n\n    # Read the VOTable back in and compare\n    votable_read = parse_single_table(filename)\n    assert_array_equal(votable_read.array, votable.get_first_table().array)\n\n    # Check that the metadata was preserved\n    for field_read, field_write in zip(votable_read.fields, votable.get_first_table().fields):\n        assert field_read.ID == field_write.ID\n        assert field_read.name == field_write.name\n        assert field_read.datatype == field_write.datatype\n"], "sample_31": ["def test_write_latex_format_table(self, cosmo):\n    \"\"\"Test LaTeX formatting of parameters.\"\"\"\n    # Test that all cosmology parameters are in _FORMAT_TABLE\n    for param in cosmo.__parameters__:\n        assert param in _FORMAT_TABLE\n\n    # Test write_latex uses _FORMAT_TABLE for column names\n    table = QTable({k: [v] for k, v in cosmo.to_format(\"mapping\").items()})\n    write_latex(cosmo, \"test.tex\", latex_names=True)\n    with open(\"test.tex\", \"r\") as f:\n        tex_content = f.read()\n    for param in cosmo.__parameters__:\n        latex_param = _FORMAT_TABLE[param]\n        assert latex_param in tex_content\n", "def test_write_latex_cls(self, cosmo, tmp_path):\n    \"\"\"\n    Test that the 'cls' argument is respected in write_latex.\n    \"\"\"\n    file = tmp_path / \"cosmology.tex\"\n\n    # Default is QTable\n    write_latex(cosmo, file)\n    with open(file, \"r\") as f:\n        contents = f.read()\n    assert \"QTable\" in contents\n\n    # Use Table instead of QTable\n    file.unlink()  # remove file to test that overwrite works\n    write_latex(cosmo, file, cls=Table)\n    with open(file, \"r\") as f:\n        contents = f.read()\n    assert \"Table\" in contents\n    assert \"QTable\" not in contents\n", "def test_write_latex_format_table(self, cosmo, tmpdir):\n    # Test that _FORMAT_TABLE is correctly used\n    table_file = tmpdir.join(\"table.tex\")\n    write_latex(cosmo, table_file, latex_names=True)\n\n    with open(table_file) as f:\n        table_contents = f.read()\n\n    for param_name, latex_name in _FORMAT_TABLE.items():\n        if hasattr(cosmo, param_name):\n            assert latex_name in table_contents\n", "def test_write_latex_format_table(cosmo):\n    \"\"\"Test `_FORMAT_TABLE` used in `write_latex`.\"\"\"\n    table = QTable({k: [getattr(cosmo, k)] for k in cosmo.__parameters__})\n    new_names = [_FORMAT_TABLE.get(k, k) for k in cosmo.__parameters__]\n    table.rename_columns(cosmo.__parameters__, new_names)\n\n    # Check that the correct formatting was applied to the columns\n    for k, v in _FORMAT_TABLE.items():\n        if k in cosmo.__parameters__:\n            assert v in table.columns\n", "def test_write_latex_format_table(self, cosmo, tmp_path):\n    \"\"\"\n    Test write_latex() with different format_table options.\n    \"\"\"\n    # Create a file to write the latex table\n    filename = tmp_path / \"cosmology.tex\"\n\n    # Write the cosmology to the file using the default format_table\n    write_latex(cosmo, filename)\n\n    # Check that the parameter names have been formatted correctly\n    with open(filename, \"r\") as f:\n        lines = f.readlines()\n\n    for param, latex_name in _FORMAT_TABLE.items():\n        assert latex_name in \"\".join(lines)\n\n    # Write the cosmology to the file without using latex_names\n    write_latex(cosmo, filename, latex_names=False)\n\n    # Check that the parameter names have not been formatted\n    with open(filename, \"r\") as f:\n        lines = f.readlines()\n\n    for param in _FORMAT_TABLE.keys():\n        assert param in \"\".join(lines)\n"], "sample_32": ["    def setup_class(self):\n        \"\"\"Setup for testing.\"\"\"\n        super().setup_class(self)\n        self._cls = w0wzCDM\n", "    def setup_class(self):\n        \"\"\"Setup for testing.\"\"\"\n        self.cls = w0wzCDM\n        self._cls_args = {\"H0\": 70, \"Om0\": 0.3, \"Ode0\": 0.7}\n        self._cls_kwargs = COMOVING_DISTANCE_EXAMPLE_KWARGS\n", "    def setup_class(self):\n        \"\"\"Setup for testing.\"\"\"\n        self._cls = w0wzCDM\n", "    def setup_class(self):\n        \"\"\"Setup for testing.\"\"\"\n        self._cls = w0wzCDM\n\n        self._cls_args = (\"70.0 km / (Mpc s)\", 0.3, 0.7)\n        self._cls_kwargs = {\"w0\": -0.9, \"wz\": 0.1}\n        self._cls_kwargs_noTcmb0 = {\"w0\": -0.9, \"wz\": 0.1, \"Tcmb0\": None}\n\n        super().setup_class()\n", "    def setup_class(self):\n        \"\"\"Setup for testing.\"\"\"\n        self._cls = w0wzCDM\n        self._cls_args = (\"H0\", \"Om0\", \"Ode0\")\n        self._cls_kwargs = {\"w0\": -1.0, \"wz\": 0.0}\n        self._func_kwargs = {\"Tcmb0\": 0.0}\n"], "sample_33": ["def test_dtype_bytes_or_chars():\n    # Test numeric type\n    dtype = np.dtype(np.float64)\n    assert misc.dtype_bytes_or_chars(dtype) == 8\n\n    # Test string type\n    dtype = np.dtype('U5')\n    assert misc.dtype_bytes_or_chars(dtype) == 5\n\n    # Test invalid input\n    with pytest.raises(AttributeError):\n        misc.dtype_bytes_or_chars('not a dtype')\n", "def test_set_locale():\n    orig_locale = locale.getlocale(locale.LC_ALL)\n    with misc.set_locale('C'):\n        assert locale.getlocale(locale.LC_ALL) == ('C', 'C')\n    assert locale.getlocale(locale.LC_ALL) == orig_locale\n", "def test_dtype_bytes_or_chars():\n    # Test numeric dtype\n    assert misc.dtype_bytes_or_chars(np.dtype('float64')) == 8\n\n    # Test string dtype\n    assert misc.dtype_bytes_or_chars(np.dtype('U10')) == 10\n\n    # Test invalid input\n    with pytest.raises(AttributeError):\n        misc.dtype_bytes_or_chars('invalid')\n", "def test_is_path_hidden(tmpdir):\n    hidden_file = tmpdir.join('.hidden_file')\n    hidden_file.write('')\n\n    assert misc.is_path_hidden(str(hidden_file))\n", "def test_ordered_descriptor_container():\n    class ExampleDescriptor(misc.OrderedDescriptor):\n        _class_attribute_ = '_examples_'\n        _name_attribute_ = 'name'\n\n    class Point2D(metaclass=misc.OrderedDescriptorContainer):\n        x = ExampleDescriptor()\n        y = ExampleDescriptor()\n\n    assert hasattr(Point2D, '_examples_')\n    assert isinstance(Point2D._examples_, dict)\n    assert 'x' in Point2D._examples_\n    assert 'y' in Point2D._examples_\n\n    class Point3D(Point2D):\n        z = ExampleDescriptor()\n\n    assert hasattr(Point3D, '_examples_')\n    assert isinstance(Point3D._examples_, dict)\n    assert 'x' not in Point3D._examples_\n    assert 'y' not in Point3D._examples_\n    assert 'z' in Point3D._examples_\n\n    class Point4D(Point2D):\n        _inherit_descriptors_ = (ExampleDescriptor,)\n        w = ExampleDescriptor()\n\n    assert hasattr(Point4D, '_examples_')\n    assert isinstance(Point4D._examples_, dict)\n    assert 'x' in Point4D._examples_\n    assert 'y' in Point4D._examples_\n    assert 'w' in Point4D._examples_\n"], "sample_34": ["def test_unit_namespace():\n    # Regression test for gh-13493\n    with catch_warnings() as w:\n        assert u.Unit('m').to_string('latex') == r'\\mathrm{m}'\n        assert len(w) == 0\n\n", "def test_unit_scale_error():\n    with raises(u.UnitScaleError):\n        u.m.scale = 2.\n", "def test_unit_scale():\n    assert u.m.scale == 1.0\n    assert (2*u.m).scale == 2.0\n    assert (u.m**2).scale == 1.0\n    assert (2*u.m**2).scale == 2.0\n", "def test_def_unit():\n    # Test defining a unit with an invalid name\n    with pytest.raises(ValueError):\n        u.def_unit('123abc')\n\n    # Test defining a unit with a name that is already taken\n    with pytest.raises(ValueError):\n        u.def_unit('m')\n\n    # Test defining a unit with a valid name and no represents\n    my_unit = u.def_unit('my_unit')\n    assert isinstance(my_unit, u.IrreducibleUnit)\n\n    # Test defining a unit with a valid name and a represents\n    my_other_unit = u.def_unit('my_other_unit', represents=u.m)\n    assert isinstance(my_other_unit, u.Unit)\n    assert my_other_unit.represents == u.m\n\n    # Test defining a unit with prefixes\n    my_prefixed_unit = u.def_unit('my_prefixed_unit', prefixes=True)\n    assert isinstance(my_prefixed_unit, u.IrreducibleUnit)\n    assert u.my_prefixed_unit == my_prefixed_unit\n    assert u.kmy_prefixed_unit == 1000 * my_prefixed_unit\n\n    # Test defining a unit with exclude_prefixes\n    my_excluded_unit = u.def_unit('my_excluded_unit', prefixes=True,\n                                  exclude_prefixes=['k'])\n    assert isinstance(my_excluded_unit, u.IrreducibleUnit)\n    assert u.my_excluded_unit == my_excluded_unit\n    with pytest.raises(AttributeError):\n        u.kmy_excluded_unit\n", "def test_composite_unit_decompose_cache():\n    # Test that the CompositeUnit decompose method correctly caches its result\n    unit = u.m * u.s ** -1\n    assert unit._decomposed_cache is None\n    decomposed_unit = unit.decompose()\n    assert unit._decomposed_cache is decomposed_unit\n    assert unit.decompose() is decomposed_unit\n"], "sample_35": ["def test_find_mod_objs_onlylocals():\n    localnames, fqnames, objs = find_mod_objs('astropy.utils.introspection', onlylocals=True)\n    assert all(fqn.startswith('astropy.utils.introspection') for fqn in fqnames)\n    localnames, fqnames, objs = find_mod_objs('astropy.utils.introspection', onlylocals=['astropy.utils'])\n    assert all(fqn.startswith('astropy.utils') for fqn in fqnames)\n", "def test_find_mod_objs():\n    localnames, fqnames, objs = find_mod_objs('astropy.utils.introspection')\n    assert 'find_mod_objs' in localnames\n    assert any(fqn.startswith('astropy.utils.introspection') for fqn in fqnames)\n    assert any(isinstance(obj, type(find_mod_objs)) for obj in objs)\n\n    # Test with onlylocals=True\n    localnames, fqnames, objs = find_mod_objs('astropy.utils.introspection', onlylocals=True)\n    assert 'find_mod_objs' in localnames\n    assert all(fqn.startswith('astropy.utils.introspection') for fqn in fqnames)\n    assert any(isinstance(obj, type(find_mod_objs)) for obj in objs)\n\n    # Test with onlylocals=['astropy']\n    localnames, fqnames, objs = find_mod_objs('astropy.utils.introspection', onlylocals=['astropy'])\n    assert 'find_mod_objs' in localnames\n    assert all(fqn.startswith('astropy') for fqn in fqnames)\n    assert any(isinstance(obj, type(find_mod_objs)) for obj in objs)\n", "def test_find_mod_objs():\n    # Create a test module with some attributes\n    mod = type('testmod', (), {\n        'local_var': 1,\n        'local_func': lambda: None,\n        '_private_var': 2,\n        '__all__': ['local_var', 'local_func']\n    })\n\n    localnames, fqnames, objs = find_mod_objs(mod.__name__)\n\n    assert set(localnames) == {'local_var', 'local_func'}\n    assert all(fqn.startswith(mod.__name__) for fqn in fqnames)\n    assert set(objs) == {mod.local_var, mod.local_func}\n", "def test_find_mod_objs_onlylocals():\n    localnames, fqnames, objs = find_mod_objs('astropy.utils.introspection', onlylocals=True)\n    for ln, fn in zip(localnames, fqnames):\n        assert fn.startswith('astropy.utils.introspection')\n    localnames, fqnames, objs = find_mod_objs('astropy.utils.introspection', onlylocals=['astropy'])\n    for ln, fn in zip(localnames, fqnames):\n        assert fn.startswith('astropy')\n", "def test_find_mod_objs():\n    # Create a test module with some attributes\n    test_mod = namedtuple('TestMod', ['a', 'b', '_c'])\n    test_mod.a = 1\n    test_mod.b = lambda x: x\n    test_mod._c = 3\n\n    localnames, fqnames, objs = find_mod_objs(__name__, onlylocals=True)\n\n    # Check that private attributes are excluded\n    assert '_c' not in localnames\n    # Check that public attributes are included\n    assert 'a' in localnames and 'b' in localnames\n    # Check that full qualified names are correct\n    assert fqnames[0].startswith(__name__)\n    # Check that objects are correct\n    assert isinstance(objs[0], int) and callable(objs[1])\n"], "sample_36": ["def test_biweight_midcorrelation():\n    # Test that biweight_midcorrelation returns the same result as\n    # manually computing it from biweight_midcovariance\n    with NumpyRNGContext(12345):\n        x = randn(1000)\n        y = randn(1000)\n        c = 9.0\n        M = None\n        modify_sample_size = False\n\n        bicorr = biweight_midcorrelation(x, y, c=c, M=M,\n                                        modify_sample_size=modify_sample_size)\n\n        bicov = biweight_midcovariance([x, y], c=c, M=M,\n                                       modify_sample_size=modify_sample_size)\n\n        expected_bicorr = bicov[0, 1] / (np.sqrt(bicov[0, 0] * bicov[1, 1]))\n\n        assert_allclose(bicorr, expected_bicorr)\n", "def test_biweight_location_constant_array():\n    # Test biweight location with a constant array\n    data = np.ones(100)\n    with catch_warnings() as w:\n        result = biweight_location(data)\n        assert len(w) == 0  # no warnings\n    assert_allclose(result, 1.0)\n", "def test_biweight_midcovariance():\n    # Test biweight_midcovariance with 2D array input\n    with NumpyRNGContext(12345):\n        x = normal(0, 1, 200)\n        y = normal(0, 3, 200)\n        data = np.array([x, y])\n        bicov = biweight_midcovariance(data)\n        assert bicov.shape == (2, 2)\n        assert_allclose(bicov[0, 0], biweight_midvariance(x))\n        assert_allclose(bicov[1, 1], biweight_midvariance(y))\n\n    # Test biweight_midcovariance with modify_sample_size=True\n    with NumpyRNGContext(12345):\n        x = normal(0, 1, 200)\n        y = normal(0, 3, 200)\n        data = np.array([x, y])\n        bicov = biweight_midcovariance(data, modify_sample_size=True)\n        assert bicov.shape == (2, 2)\n        assert_allclose(bicov[0, 0], biweight_midvariance(x, modify_sample_size=True))\n        assert_allclose(bicov[1, 1], biweight_midvariance(y, modify_sample_size=True))\n\n    # Test biweight_midcovariance with M=non-None\n    with NumpyRNGContext(12345):\n        x = normal(0, 1, 200)\n        y = normal(0, 3, 200)\n        data = np.array([x, y])\n        M = [np.mean(x), np.mean(y)]\n        bicov = biweight_midcovariance(data, M=M)\n        assert bicov.shape == (2, 2)\n        assert_allclose(bicov[0, 0], biweight_midvariance(x, M=np.mean(x)))\n        assert_allclose(bicov[1, 1], biweight_midvariance(y, M=np.mean(y)))\n", "def test_biweight_location_axis():\n    # Test biweight_location with axis argument\n    rng = np.random.RandomState(12345)\n    data = rng.randn(3, 4, 5)\n    with NumpyRNGContext(12345):\n        loc = biweight_location(data, axis=1)\n    assert loc.shape == (3, 5)\n\n    # Compare to manual calculation along each axis\n    for i in range(3):\n        for j in range(5):\n            loc_manual = biweight_location(data[i, :, j])\n            assert_allclose(loc[i, j], loc_manual)\n", "def test_biweight_midvariance_constant_array():\n    # Test biweight midvariance with a constant array\n    with NumpyRNGContext(1):\n        data = np.repeat(5., 100)\n        assert_allclose(biweight_midvariance(data), 0.)\n        assert_allclose(biweight_midvariance(data, modify_sample_size=True), 0.)\n"], "sample_37": ["def test_wcs_create_from_header_with_invalid_units(self):\n    # Test WCS creation from a header with invalid units\n    header = fits.Header.fromtextfile(get_pkg_data_filename('data/invalid_units.hdr'))\n    with catch_warnings() as w:\n        wcs.WCS(header)\n        assert len(w) == 1\n        assert \"UnitsNotFoundError\" in str(w[0].message)\n", "def test_wcs_init(self):\n    # Test initialization with different parameters\n    wcs1 = wcs.WCS(naxis=2)\n    assert wcs1.naxis == 2\n\n    header = fits.Header.fromtextfile(get_pkg_data_filename(\"data/3d_cd.hdr\"))\n    wcs2 = wcs.WCS(header, keysel=['image', 'binary'])\n    assert wcs2.wcs.naxis == 3\n\n    with pytest.raises(ValueError):\n        wcs.WCS(header, naxis=4)\n\n    with pytest.raises(TypeError):\n        wcs.WCS(header, naxis='a')\n", "def test_wcs_invalid_header(self):\n    # Test that creating a WCS object with an invalid header raises an error\n    with raises(_wcs.NoWcsKeywordsFoundError):\n        wcs.WCS(fits.Header())\n", "def test_wcs_invalid_units(self):\n    header = fits.Header.fromtextfile(get_pkg_data_filename('data/invalid_units.hdr'))\n    with pytest.warns(wcs.FITSFixedWarning, match='Invalid unit string'):\n        w = wcs.WCS(header)\n    assert w.wcs.cunit == ['deg', 'deg']\n", "def test_wcs_invalid_sip(self):\n    # Create a WCS object with invalid SIP distortion parameters\n    header = fits.Header()\n    header['CTYPE1'] = 'RA---TAN'\n    header['CTYPE2'] = 'DEC--TAN'\n    header['CRVAL1'] = 12.5\n    header['CRVAL2'] = 34.2\n    header['CRPIX1'] = 100\n    header['CRPIX2'] = 200\n    header['CDELT1'] = -0.01\n    header['CDELT2'] = 0.01\n    header['A_ORDER'] = 3\n    header['B_ORDER'] = 3\n    header['A_0_0'] = 1.0\n    header['A_1_1'] = 0.5\n    header['B_0_0'] = 1.0\n    header['B_1_1'] = 0.5\n\n    # Missing A_2_2 and B_2_2 keywords\n    wcs_obj = wcs.WCS(header)\n\n    # Try to convert pixel to world coordinates\n    pixcrd = np.array([[100, 200]])\n    with raises(wcs.NoConvergence):\n        wcs_obj.all_pix2world(pixcrd, 1)\n"], "sample_38": ["def test_to_header_relax(self):\n    # get the list of the hdr files that we want to test\n    header = fits.Header.fromtextfile(get_pkg_data_filename(\"data/3d_cd.hdr\"))\n    w = wcs.WCS(header)\n\n    # Test relax=False\n    with catch_warnings() as w:\n        warnings.simplefilter('ignore')\n        h1 = w.to_header(relax=False)\n    with catch_warnings() as w:\n        warnings.simplefilter('ignore')\n        h2 = w.to_header(relax=True)\n    assert set(h1.keys()) != set(h2.keys())\n\n    # Test relax=None\n    with catch_warnings() as w:\n        warnings.simplefilter('ignore')\n        h1 = w.to_header()\n    assert set(h1.keys()) == set(h2.keys())\n", "def test_wcs_to_header_roundtrip(self):\n    # Test that converting a WCS to a header and back results in the same WCS\n    for filename in self._file_list:\n        with fits.open(get_pkg_data_filename(filename)) as hdul:\n            wcs_in = wcs.WCS(hdul[0].header)\n            header_out = wcs_in.to_header()\n            wcs_out = wcs.WCS(header_out)\n            assert_array_almost_equal(wcs_in.wcs.crpix, wcs_out.wcs.crpix)\n            assert_array_almost_equal(wcs_in.wcs.crval, wcs_out.wcs.crval)\n            assert_array_almost_equal(wcs_in.wcs.cdelt, wcs_out.wcs.cdelt)\n            assert_array_almost_equal(wcs_in.wcs.pc, wcs_out.wcs.pc)\n", "def test_wcs_crpix_roundtrip(self):\n    # Test that we can set and get crpix to the same value without losing precision\n    wcsobj = wcs.WCS(naxis=2)\n    crpix = np.array([1000.5, 2000.25])\n    wcsobj.wcs.crpix = crpix\n    assert_array_almost_equal(wcsobj.wcs.crpix, crpix)\n", "def test_wcs_distortion_keywords(self):\n    # Test reading/writing WCS distortion keywords.\n    header = fits.Header.fromtextfile(get_pkg_data_filename('data/acs.hdr'))\n    with catch_warnings() as w:\n        warnings.simplefilter('ignore')\n        w = wcs.WCS(header)\n        assert w.sip is not None\n\n    # Check that we can write out the distortion coefficients and get\n    # them back in a new object.\n    header2 = w.to_header()\n    w2 = wcs.WCS(header2)\n    assert_array_almost_equal_nulp(w.sip.a, w2.sip.a, 10)\n    assert_array_almost_equal_nulp(w.sip.b, w2.sip.b, 10)\n\n    # Now check for case where there are no distortion coefficients\n    w3 = wcs.WCS(naxis=2)\n    assert w3.sip is None\n\n    header4 = w3.to_header()\n    assert 'A_1_1' not in header4\n\n    # Append distortion keywords to a header with no existing distortion\n    # keywords.\n    header5 = fits.Header()\n    header5['CTYPE1'] = 'RA---TAN'\n    header5['CTYPE2'] = 'DEC--TAN'\n    header5['CRVAL1'] = 0.0\n    header5['CRVAL2'] = 0.0\n    header5['CRPIX1'] = 0.0\n    header5['CRPIX2'] = 0.0\n    header5['CDELT1'] = 1.0\n    header5['CDELT2'] = 1.0\n    header5['A_1_1'] = 1.0\n    header5['A_2_2'] = 1.0\n    header5['B_1_1'] = 1.0\n    header5['B_2_2'] = 1.0\n    w6 = wcs.WCS(header5)\n    header7 = w6.to_header()\n    assert 'A_1_1' in header7\n    assert 'A_2_2' in header7\n    assert 'B_1_1' in header7\n    assert 'B_2_2' in header7\n", "def test_wcs_validation(self):\n    # Test WCS validation on a FITS file with valid WCS headers.\n    filename = get_pkg_data_filename(\"data/3d_cd.hdr\")\n    with fits.open(filename) as hdul:\n        results = wcs.validate(hdul)\n    assert len(results) == 1  # Only one HDU in the file\n    assert len(results[0]) == 1  # Only one WCS in that HDU\n    assert len(results[0][0]) == 0  # No issues found\n\n    # Test WCS validation on a FITS file with invalid WCS headers.\n    filename = get_pkg_data_filename(\"data/wcs_invalid.fits\")\n    with fits.open(filename) as hdul:\n        results = wcs.validate(hdul)\n    assert len(results) == 1  # Only one HDU in the file\n    assert len(results[0]) == 1  # Only one WCS in that HDU\n    assert len(results[0][0]) > 0  # Issues found\n"], "sample_39": ["def test_get_naxis(self):\n    # Create a WCS object with a header that has NAXIS keywords\n    header = fits.Header()\n    header['NAXIS'] = 3\n    header['NAXIS1'] = 100\n    header['NAXIS2'] = 200\n    header['NAXIS3'] = 300\n\n    wcs_obj = wcs.WCS(header)\n\n    # Check if the _naxis attribute is correctly set\n    assert_array_equal(wcs_obj._naxis, [100, 200, 300])\n\n    # Create a WCS object with a header that does not have NAXIS keywords\n    header = fits.Header()\n    wcs_obj = wcs.WCS(header)\n\n    # Check if the _naxis attribute is correctly set to [0, 0]\n    assert_array_equal(wcs_obj._naxis, [0, 0])\n", "def test_wcs_init_with_header(self):\n    # Test WCS initialization with a FITS header object\n    filename = get_pkg_data_filename(\"maps/1904-66_AZP.fits\")\n    with fits.open(filename) as hdul:\n        header = hdul[0].header\n\n    wcs_obj = wcs.WCS(header)\n    assert isinstance(wcs_obj, wcs.WCS)\n", "def test_read_sip_kw(self):\n    # Test reading SIP keywords from a FITS header.\n    # Create a sample FITS header with SIP keywords.\n    header = fits.Header()\n    header['CTYPE1'] = 'RA---TAN-SIP'\n    header['CTYPE2'] = 'DEC--TAN-SIP'\n    header['A_ORDER'] = 3\n    header['B_ORDER'] = 3\n    header['A_0_0'] = 1.0\n    header['A_1_0'] = 2.0\n    header['A_0_1'] = 3.0\n    header['B_0_0'] = 4.0\n    header['B_1_0'] = 5.0\n    header['B_0_1'] = 6.0\n\n    # Read the SIP keywords from the header.\n    sip = wcs.WCS(header).sip\n\n    # Check that the SIP coefficients are correctly read.\n    assert_array_almost_equal(sip.a, np.array([[1.0, 2.0], [3.0, 0.0]]))\n    assert_array_almost_equal(sip.b, np.array([[4.0, 5.0], [6.0, 0.0]]))\n", "def test_find_all_wcs(self):\n    # Create a FITS header with two WCS definitions\n    header = fits.Header()\n    header['CTYPE1'] = 'RA---TAN'\n    header['CTYPE2'] = 'DEC--TAN'\n    header['CRVAL1'] = 12.5\n    header['CRVAL2'] = 34.2\n    header['CRPIX1'] = 100\n    header['CRPIX2'] = 200\n    header['CDELT1'] = -0.01\n    header['CDELT2'] = 0.01\n    header['WCSNAME'] = 'main'\n\n    header['CTYPE1A'] = 'GLON-CAR'\n    header['CTYPE2A'] = 'GLAT-CAR'\n    header['CRVAL1A'] = 12.5\n    header['CRVAL2A'] = 34.2\n    header['CRPIX1A'] = 100\n    header['CRPIX2A'] = 200\n    header['CDELT1A'] = -0.01\n    header['CDELT2A'] = 0.01\n    header['WCSNAMEA'] = 'alt'\n\n    # Find all WCS definitions in the header\n    wcses = wcs.find_all_wcs(header, relax=True)\n\n    # Check that we found two WCS definitions\n    assert len(wcses) == 2\n\n    # Check that the WCS definitions are correct\n    for wcsobj in wcses:\n        if wcsobj.wcs.name == 'main':\n            assert wcsobj.wcs.ctype == ['RA---TAN', 'DEC--TAN']\n        elif wcsobj.wcs.name == 'alt':\n            assert wcsobj.wcs.ctype == ['GLON-CAR', 'GLAT-CAR']\n        else:\n            raise AssertionError(\"Unknown WCS name\")\n", "def test_wcs_init_with_distortion(self):\n    # Test WCS initialization with distortion corrections\n    header = fits.Header.fromtextfile(get_pkg_data_filename('data/distortion.hdr'))\n    with catch_warnings(AstropyUserWarning) as w:\n        wcs.WCS(header)\n        assert len(w) == 0\n"], "sample_40": ["def test_brightness_temperature():\n    freq = 30 * u.GHz\n    equiv = u.brightness_temperature(freq)\n    assert_quantity_allclose(1 * u.Jy / u.sr, (1 * u.Jy / u.sr).to(u.K, equivalencies=equiv).to(u.Jy / u.sr, equivalencies=equiv))\n    assert_quantity_allclose(1 * u.K, (1 * u.K).to(u.Jy / u.sr, equivalencies=equiv).to(u.K, equivalencies=equiv))\n", "def test_beam_angular_area():\n    # Check that beam_area works with different units and angles\n    for angle_unit in (u.arcsec, u.arcmin, u.deg):\n        for area_unit in (u.arcsec**2, u.arcmin**2, u.deg**2):\n            beam_area = 1*u.arcsec**2\n            equiv = u.beam_angular_area(beam_area)\n            value = 1*u.Jy/u.beam\n            result = value.to(u.Jy/area_unit, equivalencies=equiv)\n            assert_quantity_allclose(result, 1*u.Jy/(beam_area.to(area_unit)))\n", "def test_brightness_temperature():\n    freq = 30 * u.GHz\n    equiv = u.brightness_temperature(freq)\n    input_quantity = 1 * u.Jy / u.sr\n    output_quantity = input_quantity.to(u.K, equivalencies=equiv)\n    assert_quantity_allclose(output_quantity, 55.06388778 * u.K)\n", "def test_brightness_temperature():\n    freq = 30 * u.GHz\n    equiv = u.brightness_temperature(freq)\n    val = 1 * u.Jy / u.sr\n    assert_quantity_allclose(val.to(u.K, equivalencies=equiv), 4.144568368169899e-06 * u.K)\n\n    # Also test beam_area to get further into the function\n    beam_sigma = 50*u.arcsec\n    beam_area = 2*np.pi*(beam_sigma)**2\n    equiv = u.brightness_temperature(freq, beam_area=beam_area)\n    val = 1 * u.Jy / u.beam\n    assert_quantity_allclose(val.to(u.K, equivalencies=equiv), 3.526295144567176 * u.K)\n", "def test_pixel_scale():\n    # Test pixel scale equivalency for both pix->angle and angle->pix conversions\n    pixscale = 0.1 * u.arcsec / u.pix\n    assert_quantity_allclose(1*u.pix, 0.1*u.arcsec, equivalencies=u.pixel_scale(pixscale))\n    assert_quantity_allclose(0.1*u.arcsec, 1*u.pix, equivalencies=u.pixel_scale(pixscale))\n\n    pixscale = 10 * u.pix / u.arcmin\n    assert_quantity_allclose(1*u.arcmin, 10*u.pix, equivalencies=u.pixel_scale(pixscale))\n    assert_quantity_allclose(10*u.pix, 1*u.arcmin, equivalencies=u.pixel_scale(pixscale))\n"], "sample_41": ["def test_composite_unit_repr():\n    # Test that the repr of a CompositeUnit includes all of its bases and powers\n    u1 = u.Unit(\"m\")\n    u2 = u.Unit(\"s\")\n    cu = u.CompositeUnit(1, [u1, u2], [2, -3])\n    assert repr(cu) == \"Unit('m2 / s3')\"\n", "def test_unit_scale():\n    # Test setting scale with different numbers and quantities\n    q = u.Unit(1)\n    assert q.scale == 1\n\n    q = u.Unit(2.5)\n    assert q.scale == 2.5\n\n    q = u.Unit(u.Quantity(1))\n    assert q.scale == 1\n\n    q = u.Unit(u.Quantity(2.5))\n    assert q.scale == 2.5\n\n    with raises(TypeError):\n        u.Unit('string')\n", "def test_unit_scale_error():\n    with raises(UnitScaleError):\n        u.Unit('m', scale=2)\n", "def test_def_unit():\n    # Test basic functionality of def_unit\n    unit = u.def_unit('spam', 42 * u.kg, doc='A unit for spam.',\n                      format={'latex': r'\\rm{spam}'})\n    assert isinstance(unit, u.UnitBase)\n    assert unit.scale == 42.0\n    assert unit.decompose() == u.kg.decompose()\n    assert unit.doc == 'A unit for spam.'\n    assert unit.get_format_name('latex') == r'\\rm{spam}'\n\n    # Test def_unit with a custom namespace\n    namespace = {}\n    unit = u.def_unit('spam', 42 * u.kg, namespace=namespace)\n    assert namespace['spam'] is unit\n\n    # Test def_unit with SI prefixes\n    unit = u.def_unit('spam', 42 * u.kg, prefixes=True)\n    assert isinstance(unit, u.UnitBase)\n    assert unit.scale == 42.0\n    assert unit.decompose() == u.kg.decompose()\n\n    # Test def_unit with SI prefixes and custom namespace\n    namespace = {}\n    unit = u.def_unit('spam', 42 * u.kg, namespace=namespace, prefixes=True)\n    assert namespace['kspam'] is not None\n    assert namespace['Mspam'] is not None\n", "def test_def_unit():\n    with raises(ValueError):\n        u.def_unit('m', namespace={})\n\n    with raises(ValueError):\n        u.def_unit('pc', namespace={'pc': 1})\n\n    with raises(TypeError):\n        u.def_unit('foo', 'bar')\n\n    u.def_unit('foo', u.m, prefixes=False)\n    assert isinstance(u.foo, u.IrreducibleUnit)\n    assert u.foo.scale == 1.0\n\n    u.def_unit('bar', u.m**2, prefixes=True)\n    assert isinstance(u.bar, u.Unit)\n    assert u.bar.scale == 1.0\n\n    # Test exclude_prefixes\n    u.def_unit('a', u.s, prefixes=True, exclude_prefixes=['P'])\n    assert isinstance(u.da, u.PrefixUnit)\n    with raises(AttributeError):\n        u.Pa\n"], "sample_42": ["def test_with_H0():\n    # Test with_H0 equivalency\n    equiv = u.with_H0(H0=67*u.km/u.s/u.Mpc)\n    assert_quantity_allclose((1*u.littleh).to_value(u.dimensionless_unscaled, equivalencies=equiv), \n                             100/(67*u.km/u.s/u.Mpc).to_value(u.dimensionless_unscaled))\n    \n    # Test with_H0 using default cosmology\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        equiv = u.with_H0()\n        H0_def = cosmology.default_cosmology.get().H0\n        assert_quantity_allclose((1*u.littleh).to_value(u.dimensionless_unscaled, equivalencies=equiv), \n                                 100/H0_def.to_value((u.km/u.s)/u.Mpc))\n        \n    # Test invalid input\n    with pytest.raises(TypeError):\n        u.with_H0(H0='invalid')\n", "def test_beam_angular_area():\n    beam_area = 1 * u.sr\n    equiv = u.beam_angular_area(beam_area)\n    flux = 1 * u.Jy / u.beam\n    result = flux.to(u.Jy / u.sr, equivalencies=equiv)\n    assert_quantity_allclose(result, 1 * u.Jy / u.sr)\n", "def test_brightness_temperature():\n    # Test brightness temperature conversion for different frequencies\n    freqs = [1, 10, 100] * u.GHz\n    beam_area = 1 * u.sr\n\n    for freq in freqs:\n        equiv = u.brightness_temperature(freq)\n        flux = 1 * u.Jy / beam_area\n        temp = flux.to(u.K, equivalencies=equiv)\n        flux_back = temp.to(u.Jy / beam_area, equivalencies=equiv)\n        assert_quantity_allclose(flux, flux_back)\n", "def test_temperature_energy():\n    # Test temperature to energy conversion\n    temp = 1 * u.K\n    energy = temp.to(u.eV, equivalencies=u.temperature_energy())\n    assert_quantity_allclose(energy, (constants.k_B / constants.e).to(u.eV/u.K) * temp)\n\n    # Test energy to temperature conversion\n    energy = 1 * u.eV\n    temp = energy.to(u.K, equivalencies=u.temperature_energy())\n    assert_quantity_allclose(temp, (constants.e / constants.k_B).to(u.K/u.eV) * energy)\n", "def test_brightness_temperature():\n    freq = 30 * u.GHz\n    beam_area = 1 * u.sr\n    equiv = u.brightness_temperature(freq, beam_area=beam_area)\n    val = 1 * u.Jy / u.sr\n    assert_quantity_allclose(val.to(u.K, equivalencies=equiv), \n                             val.to_value(u.Jy/u.sr) * u.K * (2 * constants.k_B * freq**2 / constants.c**2).to_value(1/u.Jy))\n"], "sample_43": ["def test_bayesian_blocks_regular_events():\n    dt = 0.5\n    t = dt * np.arange(1000)\n    x = np.zeros(len(t))\n    x[np.random.randint(0, len(t), len(t) // 10)] = 1\n\n    edges = bayesian_blocks(t, x, fitness='regular_events', dt=dt)\n\n    # Check that the number of edges is reasonable\n    assert len(edges) > 2\n    assert len(edges) < len(t)\n\n    # Check that the edges are in ascending order\n    assert np.all(np.diff(edges) > 0)\n", "def test_bayesian_blocks_regular_events():\n    # Test Bayesian blocks with regular events\n    t = np.arange(100)\n    x = np.random.randint(0, 2, size=100)\n    edges = bayesian_blocks(t, x, fitness='regular_events', dt=1)\n    assert isinstance(edges, np.ndarray)\n    assert edges.ndim == 1\n    assert edges.size > 2  # There should be at least two change points\n", "def test_bayesian_blocks_events():\n    # Test bayesian blocks with event data\n    t = np.random.normal(size=100)\n    edges = bayesian_blocks(t, fitness='events', p0=0.01)\n    assert isinstance(edges, np.ndarray)\n    assert edges.ndim == 1\n    assert edges.size > 2  # at least two bins\n", "def test_bayesian_blocks_events():\n    # Test bayesian blocks with events fitness\n    t = np.random.normal(size=100)\n    edges = bayesian_blocks(t, fitness='events', p0=0.01)\n    assert len(edges) >= 2\n    assert edges[0] == t.min()\n    assert edges[-1] == t.max()\n", "def test_bayesian_blocks_measures():\n    np.random.seed(0)\n    t = 100 * np.random.rand(100)\n    x = np.exp(-0.5 * (t - 50) ** 2)\n    sigma = 0.1\n    x_obs = np.random.normal(x, sigma)\n    edges = bayesian_blocks(t, x_obs, sigma, fitness='measures')\n    assert np.all(edges[:-1] < edges[1:])\n"], "sample_44": ["    def test_function_quantity_creation(self):\n        for unit, lu_subclass, lq_subclass in zip(lu_units, lu_subclasses, lq_subclasses):\n            fq = lq_subclass(10, unit=unit)\n            assert isinstance(fq.unit, lu_subclass)\n            assert fq.value == 10\n", "    def test_init(self):\n        for lu in lu_subclasses:\n            for pu in pu_sample:\n                unit = lu(physical_unit=pu)\n                assert isinstance(unit, FunctionUnitBase)\n                assert unit.physical_unit == pu\n", "    def test_physical(self, unit):\n        fq = u.FunctionQuantity(1, unit=unit)\n        assert fq.physical.unit == unit.physical_unit\n", "    def test_comparison_with_self(self, unit):\n        fq1 = u.FunctionQuantity(10, unit=unit)\n        fq2 = u.FunctionQuantity(20, unit=unit)\n        assert fq1 < fq2\n        assert fq1 <= fq2\n        assert fq1 != fq2\n        assert fq2 > fq1\n        assert fq2 >= fq1\n", "    def test_function_quantity_comparison(self, lq_subclass, pu):\n        fq1 = lq_subclass(1, unit=lq_subclass(pu))\n        fq2 = lq_subclass(2, unit=lq_subclass(pu))\n\n        assert fq1 == fq1\n        assert fq1 != fq2\n        assert fq1 < fq2\n        assert fq1 <= fq2\n        assert fq2 > fq1\n        assert fq2 >= fq1\n"], "sample_45": ["    def test_trunc_year(self):\n        start_datetime = datetime(2015, 6, 15, 14, 30, tzinfo=pytz.UTC)\n        end_datetime = datetime(2018, 6, 15, 14, 30, tzinfo=pytz.UTC)\n        model = self.create_model(start_datetime, end_datetime)\n\n        self.assertEqual(\n            DTModel.objects.filter(start_datetime__year=start_datetime.year).count(),\n            1,\n        )\n\n        self.assertEqual(\n            DTModel.objects.annotate(year=TruncYear('start_datetime')).first().year,\n            truncate_to(start_datetime, 'year'),\n        )\n", "def test_trunc_time(self):\n    start_datetime = datetime(2015, 6, 15, 14, 30, 50, 321)\n    end_datetime = datetime(2018, 3, 1, 12, 5, 27, 123)\n    obj = self.create_model(start_datetime, end_datetime)\n\n    # Trunc to time\n    trunc_time_qs = DTModel.objects.annotate(\n        trunc_time=Trunc('start_datetime', 'time', output_field=TimeField()),\n    )\n    self.assertEqual(trunc_time_qs.get(pk=obj.pk).trunc_time, start_datetime.time())\n", "def test_trunc_functions(self):\n    start_datetime = datetime(2015, 6, 15, 14, 30, 50, 321)\n    end_datetime = datetime(2018, 3, 1, 12, 5, 10, 999)\n    model = self.create_model(start_datetime, end_datetime)\n\n    tests = [\n        (TruncYear, 'start_datetime', start_datetime.replace(month=1, day=1, hour=0, minute=0, second=0, microsecond=0)),\n        (TruncQuarter, 'start_datetime', start_datetime.replace(month=4, day=1, hour=0, minute=0, second=0, microsecond=0)),\n        (TruncMonth, 'start_datetime', start_datetime.replace(day=1, hour=0, minute=0, second=0, microsecond=0)),\n        (TruncWeek, 'start_datetime', (start_datetime - timedelta(days=start_datetime.weekday())).replace(hour=0, minute=0, second=0, microsecond=0)),\n        (TruncDate, 'start_datetime', start_datetime.date()),\n        (TruncTime, 'start_datetime', start_datetime.time()),\n        (TruncDay, 'start_datetime', start_datetime.replace(hour=0, minute=0, second=0, microsecond=0)),\n        (TruncHour, 'start_datetime', start_datetime.replace(minute=0, second=0, microsecond=0)),\n        (TruncMinute, 'start_datetime', start_datetime.replace(second=0, microsecond=0)),\n        (TruncSecond, 'start_datetime', start_datetime.replace(microsecond=0)),\n    ]\n\n    for trunc_class, field_name, expected_value in tests:\n        with self.subTest(trunc_class=trunc_class):\n            value = DTModel.objects.filter(pk=model.pk).values_list(trunc_class(field_name), flat=True)[0]\n            self.assertEqual(value, expected_value)\n", "def test_trunc_functions(self):\n    start_datetime = datetime(2015, 6, 15, 14, 30, 50, 321)\n    end_datetime = datetime(2018, 3, 1, 12, 5, 10)\n    obj = self.create_model(start_datetime, end_datetime)\n\n    tests = [\n        (TruncYear('start_datetime'), start_datetime.replace(month=1, day=1, hour=0, minute=0, second=0, microsecond=0)),\n        (TruncMonth('start_datetime'), start_datetime.replace(day=1, hour=0, minute=0, second=0, microsecond=0)),\n        (TruncDay('start_datetime'), start_datetime.replace(hour=0, minute=0, second=0, microsecond=0)),\n        (TruncHour('start_datetime'), start_datetime.replace(minute=0, second=0, microsecond=0)),\n        (TruncMinute('start_datetime'), start_datetime.replace(second=0, microsecond=0)),\n        (TruncSecond('start_datetime'), start_datetime.replace(microsecond=0)),\n        (TruncDate('start_datetime'), start_datetime.date()),\n        (TruncTime('start_datetime'), start_datetime.time()),\n    ]\n\n    for func, result in tests:\n        with self.subTest(func=func):\n            self.assertEqual(DTModel.objects.filter(id=obj.id).annotate(**{'result': func}).values_list('result', flat=True)[0], result)\n", "    def test_trunc_datetime_tz_aware(self):\n        start_datetime = datetime(2015, 6, 15, 14, 30, tzinfo=pytz.UTC)\n        end_datetime = datetime(2015, 6, 16, 14, 30, tzinfo=pytz.UTC)\n        self.create_model(start_datetime, end_datetime)\n\n        qs = DTModel.objects.annotate(\n            trunc_year=TruncYear('start_datetime'),\n            trunc_quarter=TruncQuarter('start_datetime'),\n            trunc_month=TruncMonth('start_datetime'),\n            trunc_week=TruncWeek('start_datetime'),\n            trunc_day=TruncDay('start_datetime'),\n            trunc_date=TruncDate('start_datetime'),\n            trunc_time=TruncTime('start_datetime'),\n            trunc_hour=TruncHour('start_datetime'),\n            trunc_minute=TruncMinute('start_datetime'),\n            trunc_second=TruncSecond('start_datetime'),\n        )\n        obj = qs.first()\n\n        self.assertEqual(obj.trunc_year, truncate_to(start_datetime, 'year'))\n        self.assertEqual(obj.trunc_quarter, truncate_to(start_datetime, 'quarter'))\n        self.assertEqual(obj.trunc_month, truncate_to(start_datetime, 'month'))\n        self.assertEqual(obj.trunc_week, truncate_to(start_datetime, 'week'))\n        self.assertEqual(obj.trunc_day, truncate_to(start_datetime, 'day'))\n        self.assertEqual(obj.trunc_date, start_datetime.date())\n        self.assertEqual(obj.trunc_time, start_datetime.time())\n        self.assertEqual(obj.trunc_hour, truncate_to(start_datetime, 'hour'))\n        self.assertEqual(obj.trunc_minute, truncate_to(start_datetime, 'minute'))\n        self.assertEqual(obj.trunc_second, truncate_to(start_datetime, 'second'))\n"], "sample_46": ["    def test_uuid_field_lookup(self):\n        instance = UUIDModel.objects.create(uuid=uuid.uuid4())\n        lookup_instance = UUIDModel.objects.get(uuid=instance.uuid)\n        self.assertEqual(instance, lookup_instance)\n", "    def setUpTestData(cls):\n        cls.obj1 = UUIDModel.objects.create(uuid_field='1234567890abcdef')\n        cls.obj2 = UUIDModel.objects.create(uuid_field='fedcba0987654321')\n", "    def setUpTestData(cls):\n        cls.obj = UUIDModel.objects.create(uuid=uuid.uuid4())\n", "    def test_exact_lookup(self):\n        obj = UUIDModel.objects.create(uuid_field=uuid.uuid4())\n        qs = UUIDModel.objects.filter(uuid_field=obj.uuid_field)\n        self.assertEqual(qs.count(), 1)\n", "    def test_exact_lookup(self):\n        obj = UUIDModel.objects.create(uuid=uuid.uuid4())\n        qs = UUIDModel.objects.filter(uuid=obj.uuid)\n        self.assertEqual(qs.count(), 1)\n"], "sample_47": ["    def test_cleanse_setting(self):\n        \"\"\" Test cleanse_setting function with sensitive and non-sensitive data\n        \"\"\"\n        self.assertEqual(cleanse_setting('SENSITIVE_SETTING', 'secret_value'), CLEANSED_SUBSTITUTE)\n        self.assertEqual(cleanse_setting('NON_SENSITIVE_SETTING', 'public_value'), 'public_value')\n", "    def test_cleanse_setting(self):\n        self.assertEqual(cleanse_setting('KEY', 'value'), CLEANSED_SUBSTITUTE)\n        self.assertEqual(cleanse_setting('OTHER', 'value'), 'value')\n        self.assertEqual(cleanse_setting('KEY', {'key': 'value'}), {'key': CLEANSED_SUBSTITUTE})\n        self.assertEqual(cleanse_setting('OTHER', {'key': 'value'}), {'key': 'value'})\n", "    def test_cleanse_setting(self):\n        self.assertEqual(cleanse_setting('API_KEY', 'secret'), CLEANSED_SUBSTITUTE)\n        self.assertEqual(cleanse_setting('NON_SENSITIVE_SETTING', 'value'), 'value')\n", "    def test_cleanse_setting(self):\n        self.assertEqual(cleanse_setting('API_KEY', '123'), CLEANSED_SUBSTITUTE)\n        self.assertEqual(cleanse_setting('DATABASES', {'ENGINE': 'django.db.backends.sqlite3'}), {'ENGINE': 'django.db.backends.sqlite3'})\n", "    def test_cleanse_setting(self):\n        self.assertEqual(cleanse_setting('API_KEY', 'secret_key'), CLEANSED_SUBSTITUTE)\n        self.assertEqual(cleanse_setting('OTHER_SETTING', 'other_value'), 'other_value')\n"], "sample_48": ["def test_aggregate_filter_clause(self):\n    with CaptureQueriesContext(connection) as queries:\n        Book.objects.aggregate(\n            Count('pk', filter=Q(rating__gt=4)),\n            Avg('price', filter=Q(pages__gt=500)),\n        )\n        query = queries[0]['sql']\n        self.assertIn('COUNT(CASE WHEN', query)\n        self.assertIn('AVG(CASE WHEN', query)\n\n    # Test that the filter clause is not used when the database doesn't support it.\n    with connection.cursor() as cursor:\n        original_features = connection.features\n        try:\n            connection.features = DatabaseFeatures(supports_aggregate_filter_clause=False)\n            Book.objects.aggregate(Count('pk', filter=Q(rating__gt=4)))\n            query = cursor.mogrify(*cursor.execute.call_args[0]).decode()\n            self.assertIn('CASE WHEN', query)\n        finally:\n            connection.features = original_features\n\n    # Test with a subquery in the filter.\n    with CaptureQueriesContext(connection) as queries:\n        Book.objects.aggregate(\n            Count('pk', filter=Q(pk__in=Book.objects.filter(pages__gt=500))),\n        )\n        query = queries[0]['sql']\n        self.assertIn('COUNT(CASE WHEN', query)\n        self.assertIn('IN (SELECT', query)\n", "def test_aggregate_with_filter(self):\n    # Create a filter to only consider books with a rating greater than 4.0\n    filter = Q(rating__gt=4.0)\n\n    # Use the filter with an aggregate function\n    max_rating = Book.objects.aggregate(Max('rating', filter=filter))['rating__max']\n\n    # Check that the result is correct\n    self.assertEqual(max_rating, 5.0)\n", "def test_aggregate_filter(self):\n    # Create a filter to only consider books with a rating greater than 4\n    filter = Q(rating__gt=4)\n    # Use the filter with an aggregate function (Count) on Book instances\n    count = Book.objects.aggregate(count=Count('id', filter=filter))['count']\n    self.assertEqual(count, 2)\n\n    # Now use the same filter with an aggregate function (Avg) on Book instances\n    avg_rating = Book.objects.aggregate(avg=Avg('rating', filter=filter))['avg']\n    self.assertAlmostEqual(avg_rating, 4.75)\n", "def test_aggregate_allow_distinct(self):\n    with self.assertRaises(TypeError):\n        Avg('age', distinct=True)\n\n    with self.assertRaises(TypeError):\n        Max('age', distinct=True)\n\n    with self.assertRaises(TypeError):\n        Min('age', distinct=True)\n\n    with self.assertRaises(TypeError):\n        StdDev('age', distinct=True)\n\n    with self.assertRaises(TypeError):\n        Sum('age', distinct=True)\n\n    with self.assertRaises(TypeError):\n        Variance('age', distinct=True)\n\n    Count('age', distinct=True)  # This should not raise an error\n", "def test_aggregate_filter(self):\n    # Test that filter is applied to aggregates.\n    count = Count('books', filter=Q(pages__gt=300))\n    self.assertEqual(Publisher.objects.aggregate(count=count)['count'], 3)\n\n    # Test that filter can be used with other aggregates.\n    avg = Avg('books__rating', filter=Q(books__pages__gt=300))\n    self.assertAlmostEqual(Publisher.objects.aggregate(avg=avg)['avg'], 4.0)\n"], "sample_49": ["def test_media_rendering(self):\n    class TestForm(Form):\n        field1 = CharField(widget=TextInput(attrs={'class': 'field1'}))\n        field2 = CharField(widget=TextInput(attrs={'class': 'field2'}))\n\n        class Media:\n            css = {\n                'all': ('path/to/css1.css', '/path/to/css2.css')\n            }\n            js = ('path/to/js1.js', 'path/to/js2.js')\n\n    form = TestForm()\n    media = form.media\n\n    self.assertHTMLEqual(\n        media.render(),\n        '<link href=\"http://media.example.com/static/path/to/css1.css\" type=\"text/css\" media=\"all\" rel=\"stylesheet\">'\n        '<link href=\"/path/to/css2.css\" type=\"text/css\" media=\"all\" rel=\"stylesheet\">'\n        '<script type=\"text/javascript\" src=\"http://media.example.com/static/path/to/js1.js\"></script>'\n        '<script type=\"text/javascript\" src=\"http://media.example.com/static/path/to/js2.js\"></script>'\n    )\n", "def test_media_absolute_path(self):\n    widget = TextInput()\n    media = Media(css={'all': ['path/to/file.css']}, js=['path/to/file.js'])\n    self.assertEqual(\n        widget.media.absolute_path('http://example.com/path/to/file.css'),\n        'http://example.com/path/to/file.css',\n    )\n    self.assertEqual(\n        widget.media.absolute_path('https://example.com/path/to/file.css'),\n        'https://example.com/path/to/file.css',\n    )\n    self.assertEqual(\n        widget.media.absolute_path('/path/to/file.css'),\n        '/path/to/file.css',\n    )\n    self.assertEqual(\n        widget.media.absolute_path('path/to/file.css'),\n        'http://media.example.com/static/path/to/file.css',\n    )\n", "def test_media_rendering(self):\n    class TestForm(Form):\n        field1 = CharField(max_length=10, widget=TextInput(attrs={'class': 'special'}))\n\n        class Media:\n            css = {\n                'all': ('path/to/css1.css', 'path/to/css2.css')\n            }\n            js = ('path/to/js1.js', 'path/to/js2.js')\n\n    form = TestForm()\n    rendered_form = Template('{% load static %}{{ form.media }}').render(Context({'form': form}))\n    expected_css = '<link href=\"http://media.example.com/static/path/to/css1.css\" type=\"text/css\" media=\"all\" rel=\"stylesheet\">\\n'\n    expected_css += '<link href=\"http://media.example.com/static/path/to/css2.css\" type=\"text/css\" media=\"all\" rel=\"stylesheet\">\\n'\n    expected_js = '<script type=\"text/javascript\" src=\"http://media.example.com/static/path/to/js1.js\"></script>\\n'\n    expected_js += '<script type=\"text/javascript\" src=\"http://media.example.com/static/path/to/js2.js\"></script>\\n'\n    self.assertHTMLEqual(rendered_form, expected_css + expected_js)\n", "def test_media_add(self):\n    class TestWidget(TextInput):\n            super().__init__(attrs)\n            self.media = Media(css={'all': ['test.css']}, js=['test.js'])\n\n    widget1 = TestWidget()\n    widget2 = TestWidget()\n    widget2.media = Media(css={'all': ['test2.css']}, js=['test2.js'])\n    combined = widget1.media + widget2.media\n    self.assertEqual(combined._css, {'all': ['test.css', 'test2.css']})\n    self.assertEqual(combined._js, ['test.js', 'test2.js'])\n", "def test_media_addition(self):\n    class TestForm(Form):\n        field1 = CharField(widget=TextInput(attrs={'class': 'special'}))\n        field2 = CharField()\n        field3 = CharField(widget=TextInput(attrs={'class': 'special'}))\n\n        class Media:\n            css = {'all': ('path/to/css1.css',)}\n            js = ('path/to/js1.js',)\n\n    form = TestForm()\n\n    # Simulate template rendering\n    context = Context({'form': form})\n    template = Template('{% for field in form %}{{ field }}{% endfor %}')\n    output = template.render(context)\n\n    self.assertContains(output, 'class=\"special\"')\n\n    # Check media is correctly handled\n    media = form.media\n    self.assertEqual(media._css, {'all': ['path/to/css1.css']})\n    self.assertEqual(media._js, ['path/to/js1.js'])\n"], "sample_50": ["def test_runshell_db_with_password(self):\n    dbinfo = {\n        'host': 'localhost',\n        'port': 5432,\n        'database': 'mydb',\n        'user': 'myuser',\n        'password': 'mypassword'\n    }\n    args, pgpassword = self._run_it(dbinfo)\n    self.assertEqual(args, ['psql', '-U', 'myuser', '-h', 'localhost', '-p', '5432', 'mydb'])\n    self.assertEqual(pgpassword, 'mypassword')\n", "def test_runshell_with_password(self):\n    dbinfo = {\n        'host': 'localhost',\n        'port': 5432,\n        'database': 'mydb',\n        'user': 'myuser',\n        'password': 'mypassword',\n    }\n    args, pgpassword = self._run_it(dbinfo)\n    self.assertEqual(args, ['psql', '-U', 'myuser', '-h', 'localhost', '-p', '5432', 'mydb'])\n    self.assertEqual(pgpassword, 'mypassword')\n", "def test_runshell_db_password(self):\n    dbinfo = {\n        'host': 'localhost',\n        'port': 5432,\n        'database': 'mydb',\n        'user': 'myuser',\n        'password': 'mypassword'\n    }\n    args, pgpassword = self._run_it(dbinfo)\n    self.assertEqual(args, ['psql', '-U', 'myuser', '-h', 'localhost', '-p', '5432', 'mydb'])\n    self.assertEqual(pgpassword, 'mypassword')\n", "def test_runshell_db_with_password(self):\n    dbinfo = {\n        'host': 'localhost',\n        'port': 5432,\n        'database': 'mydb',\n        'user': 'myuser',\n        'password': 'mypassword'\n    }\n    args, pgpassword = self._run_it(dbinfo)\n    self.assertEqual(args, ['psql', '-U', 'myuser', '-h', 'localhost', '-p', '5432', 'mydb'])\n    self.assertEqual(pgpassword, 'mypassword')\n", "def test_runshell_db_password(self):\n    dbinfo = {\n        'host': 'localhost',\n        'port': 5432,\n        'database': 'mydb',\n        'user': 'myuser',\n        'password': 'mypassword'\n    }\n    args, pgpassword = self._run_it(dbinfo)\n    self.assertEqual(pgpassword, 'mypassword')\n    self.assertEqual(args, ['psql', '-U', 'myuser', '-h', 'localhost', '-p', '5432', 'mydb'])\n"], "sample_51": ["def test_parse_date_valid(self):\n    self.assertEqual(parse_date('2012-02-21'), date(2012, 2, 21))\n    self.assertEqual(parse_date('2024-12-31'), date(2024, 12, 31))\n    self.assertEqual(parse_date('1999-01-01'), date(1999, 1, 1))\n", "def test_parse_date(self):\n    self.assertEqual(parse_date('2012-02-26'), date(2012, 2, 26))\n    self.assertIsNone(parse_date('2012-02-30'))  # Invalid date\n    self.assertIsNone(parse_date('not a date string'))  # Invalid format\n", "def test_parse_date(self):\n    self.assertEqual(parse_date('2012-02-21'), date(2012, 2, 21))\n    self.assertIsNone(parse_date('2012-13-32'))\n    self.assertIsNone(parse_date('foo'))\n    self.assertIsNone(parse_date('2012/02/21'))  # Invalid separator\n", "def test_parse_time(self):\n    self.assertEqual(parse_time('12:00'), time(12, 0))\n    self.assertEqual(parse_time('12:00:00'), time(12, 0, 0))\n    self.assertEqual(parse_time('12:00:00.123456'), time(12, 0, 0, 123456))\n    self.assertIsNone(parse_time('12:00:00.1234567'))  # more than 6 microseconds digits\n    self.assertIsNone(parse_time('12:00:00Z'))  # contains an offset\n    self.assertIsNone(parse_time('12:00:00+01:00'))  # contains an offset\n", "def test_parse_date(self):\n    self.assertEqual(parse_date('2013-07-13'), date(2013, 7, 13))\n    self.assertIsNone(parse_date('2013-07'))\n    self.assertIsNone(parse_date('2013'))\n    self.assertIsNone(parse_date('abc'))\n    self.assertEqual(parse_date('2024-02-29'), date(2024, 2, 29))\n    with self.assertRaises(ValueError):\n        parse_date('2013-02-30')\n"], "sample_52": ["def test_model_choice_field_choices(self):\n    field = forms.ModelChoiceField(Category.objects.all())\n    self.assertEqual(list(field.choices), [\n        ('', '---------'),\n        (self.c1.pk, 'Entertainment'),\n        (self.c2.pk, 'A test'),\n        (self.c3.pk, 'Third'),\n    ])\n    self.assertEqual(len(field.choices), 4)\n", "def test_model_choice_field_iterator(self):\n    field = forms.ModelChoiceField(Category.objects.all())\n    iterator = ModelChoiceIterator(field)\n    self.assertEqual(len(iterator), 3)\n    self.assertEqual(iterator[0], ('', '---------'))\n    self.assertEqual(iterator[1], (self.c1.pk, str(self.c1)))\n    self.assertEqual(iterator[2], (self.c2.pk, str(self.c2)))\n    self.assertEqual(iterator[3], (self.c3.pk, str(self.c3)))\n    with self.assertRaises(StopIteration):\n        iterator.__next__()\n", "def test_modelchoicefield_iterable_value(self):\n    field = forms.ModelChoiceField(Category.objects.all())\n    self.assertEqual(field.clean([self.c1.pk]), self.c1)\n    with self.assertRaises(ValidationError):\n        field.clean([self.c1.pk, self.c2.pk])\n", "def test_model_choice_field_iterator(self):\n    field = forms.ModelChoiceField(Category.objects.all())\n    iterator = ModelChoiceIterator(field)\n    self.assertEqual(len(iterator), 3)\n    self.assertEqual(bool(iterator), True)\n    choices = list(iterator)\n    self.assertEqual(choices[0][0], self.c1.pk)\n    self.assertEqual(choices[1][0], self.c2.pk)\n    self.assertEqual(choices[2][0], self.c3.pk)\n", "def test_model_choice_field_iterator(self):\n    field = forms.ModelChoiceField(Category.objects.all())\n    iterator = ModelChoiceIterator(field)\n    self.assertEqual(len(iterator), 3)\n    choices = list(iterator)\n    self.assertEqual(choices[0], ('', '---------'))\n    self.assertEqual(choices[1], (self.c1.pk, 'Entertainment'))\n    self.assertEqual(choices[2], (self.c2.pk, 'A test'))\n    self.assertEqual(choices[3], (self.c3.pk, 'Third'))\n"], "sample_54": ["def test_file_response_get_value(self):\n    file = ContentFile('Hello, World!', name='example.txt')\n    response = FileResponse(file)\n    self.assertEqual(response.getvalue(), b'Hello, World!')\n", "def test_file_response_set_headers_with_absolute_filename(self):\n    tmp = tempfile.NamedTemporaryFile(delete=False)\n    try:\n        response = FileResponse(tmp.name, as_attachment=True, filename='example.txt')\n        self.assertEqual(response['Content-Type'], 'text/plain')\n        self.assertEqual(response['Content-Disposition'], 'attachment; filename=\"example.txt\"')\n    finally:\n        os.remove(tmp.name)\n", "def test_file_response_set_headers(self):\n    # Create a temporary file for testing.\n    with tempfile.NamedTemporaryFile() as tmp_file:\n        response = FileResponse(tmp_file)\n        response.set_headers(tmp_file)\n\n        # Assert that the 'Content-Length' header is set.\n        self.assertEqual(response['Content-Length'], str(os.path.getsize(tmp_file.name)))\n\n        # Assert that the 'Content-Type' header is set.\n        self.assertEqual(response['Content-Type'], 'application/octet-stream')\n\n        # Assert that the 'Content-Disposition' header is set.\n        self.assertIn('attachment; filename=', response['Content-Disposition'])\n", "def test_file_response_with_filelike_object(self):\n    filelike = io.BytesIO(b'Hello, World!')\n    response = FileResponse(filelike)\n    self.assertEqual(response.getvalue(), b'Hello, World!')\n    self.assertEqual(response['Content-Type'], 'application/octet-stream')\n", "def test_file_response_set_headers(self):\n    # Create a temporary file for testing\n    with tempfile.NamedTemporaryFile() as temp_file:\n        response = FileResponse(temp_file)\n\n        # Check that the Content-Length header is set\n        self.assertEqual(response['Content-Length'], str(os.path.getsize(temp_file.name)))\n\n        # Check that the Content-Type header is set\n        self.assertEqual(response['Content-Type'], 'application/octet-stream')\n\n        # Check that the Content-Disposition header is set\n        self.assertIn('attachment', response['Content-Disposition'])\n        self.assertIn('filename', response['Content-Disposition'])\n\n        # Test with as_attachment=False\n        response = FileResponse(temp_file, as_attachment=False)\n        self.assertIn('inline', response['Content-Disposition'])\n        self.assertIn('filename', response['Content-Disposition'])\n"], "sample_53": ["def test_select_date_widget_render(self):\n    widget = forms.SelectDateWidget()\n    self.check_html(widget, 'date', '', html=(\n        '<select name=\"date_year\">'\n        '<option value=\"\">---</option>'\n        '<option value=\"2022\">2022</option>'\n        '<option value=\"2023\">2023</option>'\n        '</select>'\n        '<select name=\"date_month\">'\n        '<option value=\"\">---</option>'\n        '<option value=\"1\">January</option>'\n        '<option value=\"2\">February</option>'\n        '</select>'\n        '<select name=\"date_day\">'\n        '<option value=\"\">---</option>'\n        '<option value=\"1\">1</option>'\n        '<option value=\"2\">2</option>'\n        '</select>'\n    ))\n\n    self.check_html(widget, 'date', datetime.date(2022, 1, 1), html=(\n        '<select name=\"date_year\">'\n        '<option value=\"\">---</option>'\n        '<option value=\"2022\" selected>2022</option>'\n        '<option value=\"2023\">2023</option>'\n        '</select>'\n        '<select name=\"date_month\">'\n        '<option value=\"\">---</option>'\n        '<option value=\"1\" selected>January</option>'\n        '<option value=\"2\">February</option>'\n        '</select>'\n        '<select name=\"date_day\">'\n        '<option value=\"\">---</option>'\n        '<option value=\"1\" selected>1</option>'\n        '<option value=\"2\">2</option>'\n        '</select>'\n    ))\n", "def test_multiwidget_decompress(self):\n    widget = forms.MultiWidget(widgets=[forms.TextInput, forms.Select(choices=[('1', 'One'), ('2', 'Two')])])\n    self.assertEqual(widget.decompress(['value1', '1']), ['value1', '1'])\n    self.assertEqual(widget.decompress(['value1', '2']), ['value1', '2'])\n    self.assertEqual(widget.decompress(['value1', None]), ['value1', None])\n", "def test_date_input_widget_format(self):\n    widget = forms.DateInput(format='%Y-%m-%d')\n    self.assertEqual(widget.format_value('2022-01-01'), '2022-01-01')\n    self.assertEqual(widget.format_value('2022/01/01'), None)\n", "def test_autocomplete_select_render(self):\n    form = AlbumForm()\n    band_field = form.fields['band']\n    self.assertHTMLEqual(\n        '<select class=\"my-class\" data-ajax--cache-ttl=\"None\" data-ajax--delay=\"250\" '\n        'data-ajax--type=\"GET\" data-autocomplete-light-function=\"select2\" '\n        'data-autocomplete-light-url=\"/admin_widgets/autocomplete_band/\" id=\"id_band\" name=\"band\">'\n        '<option value=\"\" selected>---------</option></select>',\n        band_field.widget.render('band', None),\n    )\n", "def test_select_widget(self):\n    widget = forms.Select(choices=[('1', 'One'), ('2', 'Two')])\n    self.check_html(widget, 'num', '1', html='<select name=\"num\">\\n<option value=\"1\" selected>One</option>\\n<option value=\"2\">Two</option>\\n</select>')\n"], "sample_58": ["def test_form_add_prefix(self):\n    # Test that add_prefix works correctly in the case where\n    # there is no prefix, and the case where there is a prefix.\n    p = Person()\n    self.assertEqual(p.add_prefix('field_name'), 'field_name')\n\n    p = Person(prefix='person')\n    self.assertEqual(p.add_prefix('field_name'), 'person-field_name')\n\n    p = Person(prefix='person-prefix')\n    self.assertEqual(p.add_prefix('field_name'), 'person-prefix-field_name')\n", "def test_add_prefix(self):\n    p = Person(prefix='person')\n    self.assertEqual(p.add_prefix('first_name'), 'person-first_name')\n\n    p = Person(prefix='person')\n    p.prefix = None\n    self.assertEqual(p.add_prefix('first_name'), 'first_name')\n", "def test_add_prefix(self):\n    # Make sure that add_prefix adds a prefix to the field name, and also\n    # that it doesn't add a prefix if the field name already has one.\n    p = Person(prefix='person')\n    self.assertEqual(p.add_prefix('first_name'), 'person-first_name')\n    self.assertEqual(p.add_prefix('person-first_name'), 'person-first_name')\n\n    p = Person(prefix=None)\n    self.assertEqual(p.add_prefix('first_name'), 'first_name')\n", "def test_form_add_prefix(self):\n    p = Person()\n    self.assertEqual(p.add_prefix('first_name'), 'first_name')\n    p = Person(prefix='person')\n    self.assertEqual(p.add_prefix('first_name'), 'person-first_name')\n", "def test_form_has_changed(self):\n    # Test the has_changed() method.\n    class Person(Form):\n        name = CharField()\n        age = IntegerField()\n\n    p = Person({'name': 'John', 'age': '30'}, initial={'name': 'Jane', 'age': 25})\n    self.assertTrue(p.has_changed())\n    self.assertEqual(p.changed_data, ['name', 'age'])\n\n    p = Person({'name': 'Jane', 'age': '25'}, initial={'name': 'Jane', 'age': 25})\n    self.assertFalse(p.has_changed())\n    self.assertEqual(p.changed_data, [])\n\n    # Changed data should be available before the form is validated.\n    p = Person({'name': 'John', 'age': '30'}, initial={'name': 'Jane', 'age': 25})\n    self.assertEqual(p.changed_data, ['name', 'age'])\n"], "sample_56": ["def test_check_fields(self):\n    class MyModelAdmin(admin.ModelAdmin):\n        fields = ['non_existent_field']\n\n    errors = MyModelAdmin(Model, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'fields' refers to 'non_existent_field', which is not an attribute of 'admin_checks.Model'.\",\n            obj=MyModelAdmin,\n            id='admin.E108',\n        )\n    ]\n    self.assertEqual(errors, expected)\n", "def test_check_autocomplete_fields_item(self):\n    class MyModelAdmin(admin.ModelAdmin):\n        autocomplete_fields = ['author']\n\n    errors = ModelAdminChecks().check(MyModelAdmin, model=Book, obj=None)\n    self.assertEqual(len(errors), 1)\n    self.assertEqual(errors[0].id, 'admin.E039')\n\n    class AuthorAdmin(admin.ModelAdmin):\n        search_fields = ['name']\n\n    admin.site.register(Author, AuthorAdmin)\n    errors = ModelAdminChecks().check(MyModelAdmin, model=Book, obj=None)\n    self.assertEqual(errors, [])\n\n    class MyModelAdmin(admin.ModelAdmin):\n        autocomplete_fields = ['unknown_field']\n\n    errors = ModelAdminChecks().check(MyModelAdmin, model=Book, obj=None)\n    self.assertEqual(len(errors), 1)\n    self.assertEqual(errors[0].id, 'admin.E037')\n", "def test_check_list_editable(self):\n    class MyAdmin(admin.ModelAdmin):\n        list_display = ['title', 'artist']\n        list_editable = ['title']\n\n    errors = MyAdmin(Song, AdminSite()).check()\n    self.assertEqual(errors, [])\n\n    class MyAdmin(admin.ModelAdmin):\n        list_display = ['title', 'artist']\n        list_editable = ['title', 'nonexistent_field']\n\n    errors = MyAdmin(Song, AdminSite()).check()\n    self.assertEqual(len(errors), 1)\n    self.assertEqual(errors[0].id, 'admin.E121')\n\n    class MyAdmin(admin.ModelAdmin):\n        list_display = ['title', 'artist']\n        list_editable = ['artist']  # artist is a ForeignKey and thus non-editable\n\n    errors = MyAdmin(Song, AdminSite()).check()\n    self.assertEqual(len(errors), 1)\n    self.assertEqual(errors[0].id, 'admin.E125')\n", "def test_check_dependencies(self):\n    # Unregister all apps to ensure a clean slate.\n    for app in list(admin.site._registry.keys()):\n        admin.site.unregister(app)\n\n    with self.settings(\n        INSTALLED_APPS=[\n            'django.contrib.admin',\n            'django.contrib.auth',\n            'django.contrib.contenttypes',\n            'django.contrib.sessions',\n        ],\n        MIDDLEWARE=[\n            'django.contrib.auth.middleware.AuthenticationMiddleware',\n            'django.contrib.messages.middleware.MessageMiddleware',\n        ],\n    ):\n        errors = checks.run_checks()\n        self.assertEqual(errors, [])\n\n    with self.settings(\n        INSTALLED_APPS=[\n            'django.contrib.admin',\n            'django.contrib.auth',\n            'django.contrib.contenttypes',\n            'django.contrib.sessions',\n        ],\n        MIDDLEWARE=[\n            'django.contrib.auth.middleware.AuthenticationMiddleware',\n        ],\n    ):\n        errors = checks.run_checks()\n        self.assertEqual(len(errors), 1)\n        error = errors[0]\n        self.assertEqual(error.id, 'admin.E409')\n        self.assertEqual(\n            error.msg,\n            \"'django.contrib.messages.middleware.MessageMiddleware' must \"\n            \"be in MIDDLEWARE in order to use the admin application.\",\n        )\n\n    with self.settings(\n        INSTALLED_APPS=[\n            'django.contrib.admin',\n            'django.contrib.auth',\n            'django.contrib.contenttypes',\n            'django.contrib.sessions',\n        ],\n        MIDDLEWARE=[\n            'django.contrib.messages.middleware.MessageMiddleware',\n        ],\n    ):\n        errors = checks.run_checks()\n        self.assertEqual(len(errors), 1)\n        error = errors[0]\n        self.assertEqual(error.id, 'admin.E408')\n        self.assertEqual(\n            error.msg,\n            \"'django.contrib.auth.middleware.AuthenticationMiddleware' must \"\n            \"be in MIDDLEWARE in order to use the admin application.\",\n        )\n", "def test_check_exclude(self):\n    class MyAdmin(admin.ModelAdmin):\n        exclude = ('non_existent_field',)\n\n    errors = checks.run_checks(MyAdmin, 'admin.E014')\n    self.assertEqual(len(errors), 1)\n    self.assertEqual(errors[0].id, 'admin.E014')\n\n    class MyAdmin(admin.ModelAdmin):\n        exclude = ('id', 'id')\n\n    errors = checks.run_checks(MyAdmin, 'admin.E015')\n    self.assertEqual(len(errors), 1)\n    self.assertEqual(errors[0].id, 'admin.E015')\n"], "sample_57": ["    def test_username_field_autocapitalize(self):\n        form = UserChangeForm(instance=self.u1)\n        self.assertEqual(form.fields['username'].widget.attrs['autocapitalize'], 'none')\n", "    def test_widget_render(self):\n        field = ReadOnlyPasswordHashField()\n        output = field.widget.render('password', 'sha1$1234567890abcdef$1234567890abcdef1234567890abcdef')\n        self.assertHTMLEqual(output, \"\"\"\n            <div>\n                <h2>Password:</h2>\n                <ul><li><strong>algorithm:</strong> sha1</li></ul>\n            </div>\n        \"\"\")\n", "    def test_widget(self):\n        field = ReadOnlyPasswordHashField()\n        self.assertIsInstance(field.widget, ReadOnlyPasswordHashWidget)\n", "    def test_user_change_form(self):\n        form = UserChangeForm(instance=self.u1)\n        self.assertEqual(form.fields['username'].initial, self.u1.username)\n", "    def test_bound_data(self):\n        field = ReadOnlyPasswordHashField()\n        initial = 'initial'\n        data = 'data'\n        self.assertEqual(field.bound_data(data, initial), initial)\n"], "sample_59": ["    def test_init(self):\n        state = ModelState()\n        self.assertIsNone(state.db)\n        self.assertTrue(state.adding)\n        self.assertEqual(state.fields_cache, {})\n", "def test_deferred_model_unpickling(self):\n    # Create an instance of Model1 with deferred fields.\n    obj = Model1.objects.create(title='Hello')\n    obj.refresh_from_db()\n    deferred_obj = Model1.objects.defer('title').get(pk=obj.pk)\n\n    # Pickle and unpickle the deferred model instance.\n    pickled_obj = self._pickle_and_unpickle(deferred_obj)\n\n    # Assert that the unpickled object is still a Model1 instance with deferred fields.\n    self.assertIsInstance(pickled_obj, Model1)\n    self.assertTrue(hasattr(pickled_obj, '__dict__'))\n    self.assertIn('id', pickled_obj.__dict__)\n    self.assertNotIn('title', pickled_obj.__dict__)\n", "def test_deferred_fields(self):\n    obj = Model1.objects.create(a=1, b=2)\n    # Test that deferred fields are properly stored in the instance.\n    deferred_obj = Model1.objects.defer('a').get(pk=obj.pk)\n    self.assertIn(Model1._meta.get_field('a').attname, deferred_obj.get_deferred_fields())\n    self.assertNotIn(Model1._meta.get_field('b').attname, deferred_obj.get_deferred_fields())\n    # Test that deferred fields are loaded on attribute access.\n    self.assertEqual(deferred_obj.a, 1)\n    self.assertNotIn(Model1._meta.get_field('a').attname, deferred_obj.get_deferred_fields())\n    # Test that assigning to a deferred field doesn't cause it to be loaded.\n    deferred_obj.a = 3\n    self.assertIn(Model1._meta.get_field('a').attname, deferred_obj.get_deferred_fields())\n    # Test that a field deferred in only one of multiple models will be\n    # loaded correctly.\n    obj = Model1.objects.create(a=1, b=2)\n    obj2 = Model2.objects.create(a=1, b=2, c=3)\n    deferred_obj = Model1.objects.defer('a').select_related('model2').get(model2=obj2)\n    self.assertEqual(deferred_obj.model2.a, 1)\n", "    def test_refresh_from_db(self):\n        m1 = Model1.objects.create(field1='Initial value')\n        m1.field1 = 'Updated value'\n        self.assertEqual(m1.field1, 'Updated value')\n\n        m1.refresh_from_db()\n        self.assertEqual(m1.field1, 'Initial value')\n\n        m2 = Model1.objects.get(pk=m1.pk)\n        m2.field1 = 'New value'\n        m2.save()\n\n        m1.refresh_from_db()\n        self.assertEqual(m1.field1, 'New value')\n", "    def test_concrete_fields(self):\n        class MyModel(models.Model):\n            field1 = models.CharField(max_length=10)\n            field2 = models.CharField(max_length=10)\n\n        obj = MyModel(field1='value1', field2='value2')\n        self.assertEqual(\n            list(obj._meta.concrete_fields),\n            [obj._meta.get_field('field1'), obj._meta.get_field('field2')]\n        )\n"], "sample_60": ["def test_inline_model_admin_get_extra(self):\n    class MyInline(admin.TabularInline):\n        model = Media\n\n    my_inline = MyInline(Media, admin_site)\n    request = RequestFactory().get('/')\n\n    self.assertEqual(my_inline.get_extra(request), 3)\n\n    class MyInlineWithCustomExtra(admin.TabularInline):\n        model = Media\n        extra = 10\n\n    my_inline_with_custom_extra = MyInlineWithCustomExtra(Media, admin_site)\n    self.assertEqual(my_inline_with_custom_extra.get_extra(request), 10)\n", "def test_inline_formsets(self):\n    response = self.client.get(reverse('admin:generic_inline_admin_episode_change', args=(self.episode_pk,)))\n    formsets = response.context['inline_admin_formsets']\n    self.assertEqual(len(formsets), 1)\n    self.assertEqual(len(formsets[0].formset.forms), 2)\n\n    # Check the formset's extra forms are correct.\n    self.assertEqual(formsets[0].formset.extra, 1)\n    self.assertEqual(formsets[0].formset.max_num, DEFAULT_MAX_NUM)\n", "def test_inline_model_admin_get_queryset(self):\n    request = RequestFactory().get(reverse('admin:generic_inline_admin_episode_changelist'))\n    request.user = self.superuser\n\n    inline_admin = MediaInline(Episode, admin_site)\n    queryset = inline_admin.get_queryset(request)\n\n    self.assertIsInstance(queryset, ContentType.objects.__class__)\n    self.assertEqual(queryset.count(), 2)\n", "def test_inline_model_admin_get_queryset(self):\n    class TestInline(admin.TabularInline):\n        model = Media\n\n    inline = TestInline(admin_site)\n    request = RequestFactory().get(reverse('admin:index'))\n    request.user = self.superuser\n    queryset = inline.get_queryset(request)\n    self.assertEqual(queryset.model, Media)\n\n    # If the user has no view or change permission, the queryset should be empty.\n    with override_settings(AUTHENTICATION_BACKENDS=['django.contrib.auth.backends.ModelBackend']):\n        self.superuser.is_staff = False\n        self.superuser.save()\n        request.user = self.superuser\n        queryset = inline.get_queryset(request)\n        self.assertEqual(queryset.count(), 0)\n", "def test_generic_inline_formset_validation(self):\n    response = self.client.get(reverse('admin:generic_inline_admin_episode_change', args=(self.episode_pk,)))\n    formset_data = {\n        'media-TOTAL_FORMS': 2,\n        'media-INITIAL_FORMS': 2,\n        'media-MIN_NUM_FORMS': 0,\n        'media-MAX_NUM_FORMS': DEFAULT_MAX_NUM,\n        'media-0-id': self.mp3_media_pk,\n        'media-0-url': 'http://example.com/podcast.mp3',\n        'media-1-id': self.png_media_pk,\n        'media-1-url': '',  # Invalid form data: empty URL\n    }\n    response = self.client.post(reverse('admin:generic_inline_admin_episode_change', args=(self.episode_pk,)), formset_data)\n    self.assertContains(response, 'This field is required.', count=1)\n"], "sample_61": ["    def test_ascii_validator(self):\n        validator = ASCIIUsernameValidator()\n        self.assertIsNone(validator('test123'))\n        with self.assertRaises(ValidationError):\n            validator('test@#$')\n", "    def test_ascii_validator(self):\n        validator = ASCIIUsernameValidator()\n        self.assertRaises(ValidationError, validator, ' username')\n        self.assertRaises(ValidationError, validator, 'username ')\n        self.assertRaises(ValidationError, validator, 'us\u00e9rname')\n        validator('username')\n", "    def test_ascii_validator(self):\n        validator = ASCIIUsernameValidator()\n        self.assertRaises(ValidationError, validator, 'invalid@username\u00a3')\n        self.assertIsNone(validator('valid.username'))\n", "    def test_ascii_validator(self):\n        validator = ASCIIUsernameValidator()\n        self.assertTrue(validator('testuser'))\n        self.assertTrue(validator('testuser123'))\n        self.assertTrue(validator('test.user'))\n        self.assertFalse(validator('test user'))  # space is not allowed\n        self.assertFalse(validator('test$user'))  # dollar sign is not allowed\n", "    def test_ascii_validator(self):\n        validator = ASCIIUsernameValidator()\n        self.assertIsNone(validator('test_user'))\n        self.assertIsNone(validator('test_user123'))\n        self.assertIsNone(validator('test_user@domain.com'))\n        self.assertEqual(validator.message, str(ValidationError(validator.message)))\n"], "sample_62": ["def test_register_unregister(self):\n    self.site.register(Person, NameAdmin)\n    self.assertIn(Person, self.site._registry)\n    self.site.unregister(Person)\n    self.assertNotIn(Person, self.site._registry)\n\n    # Test trying to unregister a model that's not registered.\n    with self.assertRaises(admin.NotRegistered):\n        self.site.unregister(Person)\n\n    # Test trying to register a model that's already registered.\n    self.site.register(Person, NameAdmin)\n    with self.assertRaises(admin.AlreadyRegistered):\n        self.site.register(Person, NameAdmin)\n", "def test_register_model(self):\n    self.site.register(Person, NameAdmin)\n    self.assertEqual(len(self.site._registry), 1)\n    self.assertIn(Person, self.site._registry)\n    self.assertIs(self.site._registry[Person].__class__, NameAdmin)\n\n    # Test that the same model can't be registered twice.\n    with self.assertRaises(admin.sites.AlreadyRegistered):\n        self.site.register(Person)\n\n    # Test that a model can't be registered with an abstract base class.\n    with self.assertRaises(ImproperlyConfigured):\n        self.site.register(Location)\n", "def test_register_model_with_site(self):\n    self.site.register(Location)\n    self.assertIn(Location, self.site._registry)\n\n    # Test registering a model with a custom admin class\n    self.site.register(Person, NameAdmin)\n    self.assertIn(Person, self.site._registry)\n    self.assertIsInstance(self.site._registry[Person], NameAdmin)\n\n    # Test registering a model that's already registered\n    with self.assertRaises(admin.AlreadyRegistered):\n        self.site.register(Location)\n\n    # Test registering a model with a custom site\n    custom_site = CustomSite()\n    custom_site.register(Place)\n    self.assertIn(Place, custom_site._registry)\n    self.assertNotIn(Place, self.site._registry)\n", "def test_register_model_with_custom_admin_class(self):\n    self.site.register(Person, NameAdmin)\n    self.assertIn(Person, self.site._registry)\n    self.assertIsInstance(self.site._registry[Person], NameAdmin)\n", "def test_register_decorator(self):\n    @register(Person, site=self.site)\n    class PersonAdmin(admin.ModelAdmin):\n        pass\n\n    self.assertIsInstance(self.site._registry[Person], admin.ModelAdmin)\n\n    # Test trying to register a model that's already registered.\n    with self.assertRaises(admin.sites.AlreadyRegistered):\n        @register(Person, site=self.site)\n        class AnotherPersonAdmin(admin.ModelAdmin):\n            pass\n"], "sample_63": ["def test_render_to_string_template_list(self):\n    template = self.engine.render_to_string(['index.html', 'base.html'], {'name': 'world'})\n    self.assertEqual(template.strip(), '<html>Hello world!</html>')\n", "def test_get_template_builtins(self):\n    builtins = ['django.template.defaulttags', 'django.template.defaultfilters']\n    engine = Engine(builtins=builtins)\n    self.assertEqual(len(engine.template_builtins), len(builtins))\n    for builtin in builtins:\n        self.assertIn(import_string(builtin), engine.template_builtins)\n", "def test_render_to_string_template_list(self):\n    template_name_list = ['template1.html', 'template2.html']\n    with self.assertRaises(TemplateDoesNotExist):\n        self.engine.render_to_string(template_name_list)\n    with open(os.path.join(TEMPLATE_DIR, 'template1.html'), 'w') as f:\n        f.write('Hello {{ name }}!')\n    rendered = self.engine.render_to_string(template_name_list, {'name': 'world'})\n    self.assertEqual(rendered, 'Hello world!')\n", "def test_get_template_builtins(self):\n    builtins = ['django.template.defaulttags', 'django.template.defaultfilters']\n    engine = Engine(builtins=builtins)\n    self.assertEqual(len(engine.template_builtins), len(builtins))\n    for builtin in builtins:\n        self.assertIn(import_string(builtin), engine.template_builtins)\n", "def test_render_to_string_template_list(self):\n    template_name_list = ['template1.html', 'template2.html']\n    with self.assertRaises(TemplateDoesNotExist):\n        self.engine.render_to_string(template_name_list)\n\n    with open(os.path.join(TEMPLATE_DIR, 'template1.html'), 'w') as f:\n        f.write('Template 1 content')\n\n    result = self.engine.render_to_string(template_name_list)\n    self.assertEqual(result, 'Template 1 content')\n"], "sample_64": ["    def test_content_type(self):\n        response = HttpResponse()\n        self.assertEqual(response['Content-Type'], 'text/html; charset=utf-8')\n\n        response = HttpResponse(content_type='text/plain')\n        self.assertEqual(response['Content-Type'], 'text/plain; charset=utf-8')\n\n        response = HttpResponse(content_type='text/plain; charset=iso-8859-1')\n        self.assertEqual(response['Content-Type'], 'text/plain; charset=iso-8859-1')\n", "    def testHttpResponse(self):\n        response = HttpResponse()\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.reason_phrase, 'OK')\n        self.assertEqual(response.content, b'')\n        self.assertEqual(response.charset, settings.DEFAULT_CHARSET)\n        self.assertEqual(response.cookies, SimpleCookie())\n        self.assertTrue(isinstance(response.serialize_headers(), bytes))\n", "    def test_http_response(self):\n        response = HttpResponse('Hello, world!')\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.content, b'Hello, world!')\n", "    def test_default_charset(self):\n        response = HttpResponse()\n        self.assertEqual(response.charset, 'utf-8')\n", "    def test_headers_type(self):\n        response = HttpResponse()\n        self.assertIsInstance(response.headers, dict)\n"], "sample_65": ["    def test_javascript_catalog_view(self):\n        response = self.client.get(reverse('jsi18n'))\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response['Content-Type'], 'text/javascript; charset=\"utf-8\"')\n        # Basic sanity check of content:\n        self.assertContains(response, 'django.catalog = django.catalog || {};')\n        self.assertContains(response, 'django.gettext = function(msgid) {')\n", "    def test_json_catalog(self):\n        response = self.client.get(reverse('jsi18n'), HTTP_ACCEPT='application/json')\n        self.assertEqual(response.status_code, 200)\n        catalog = json.loads(response.content.decode('utf-8'))\n        self.assertIn('catalog', catalog)\n        self.assertIn('formats', catalog)\n        self.assertIn('plural', catalog)\n", "    def test_404_without_packages(self):\n        response = self.client.get(reverse('jsi18n'))\n        self.assertEqual(response.status_code, 404)\n", "    def test_javascript_catalog_view(self):\n        response = self.client.get(reverse('jsi18n'))\n        self.assertEqual(response.status_code, 200)\n        self.assertTemplateUsed(response, 'jsi18n.html')\n", "    def test_get_catalog(self):\n        catalog = JavaScriptCatalog()\n        catalog.translation = gettext.NullTranslations()\n        catalog.locale = 'en'\n        catalog.domain = 'djangojs'\n        self.assertEqual(catalog.get_catalog(), {})\n"], "sample_67": ["def test_modelform_defines_fields(self):\n    self.assertTrue(modelform_defines_fields(BookForm))\n    self.assertFalse(modelform_defines_fields(ModelFormWithMedia))\n", "def test_modelform_defines_fields(self):\n    self.assertTrue(modelform_defines_fields(ProductForm))\n    self.assertFalse(modelform_defines_fields(forms.ModelForm))\n", "def test_fields_for_model_with_field_classes(self):\n    field_classes = {\n        'name': forms.CharField,\n        'price': forms.IntegerField,\n    }\n    fields = fields_for_model(Product, field_classes=field_classes)\n    self.assertIsInstance(fields['name'], field_classes['name'])\n    self.assertIsInstance(fields['price'], field_classes['price'])\n", "def test_modelform_defines_fields(self):\n    self.assertTrue(modelform_defines_fields(BookForm))\n    self.assertFalse(modelform_defines_fields(ModelFormWithMedia))\n    class NoFieldsModelForm(forms.ModelForm):\n        pass\n    self.assertFalse(modelform_defines_fields(NoFieldsModelForm))\n", "def test_modelform_defines_fields(self):\n    class NoFieldsForm(forms.ModelForm):\n        pass\n\n    class FieldsForm(forms.ModelForm):\n        class Meta:\n            model = Writer\n            fields = ('name',)\n\n    class ExcludeForm(forms.ModelForm):\n        class Meta:\n            model = Writer\n            exclude = ('name',)\n\n    self.assertFalse(modelform_defines_fields(NoFieldsForm))\n    self.assertTrue(modelform_defines_fields(FieldsForm))\n    self.assertTrue(modelform_defines_fields(ExcludeForm))\n"], "sample_68": ["    def test_cleanse_setting(self):\n        self.assertEqual(cleanse_setting('API_KEY', '123456'), CLEANSED_SUBSTITUTE)\n        self.assertEqual(cleanse_setting('PUBLIC_VAR', 'public_value'), 'public_value')\n        self.assertEqual(cleanse_setting('TEMPLATE_DIR', '/path/to/templates'), '/path/to/templates')\n", "    def test_cleanse_setting_with_sensitive_info(self):\n        sensitive_value = 'super_secret_key'\n        cleansed_value = cleanse_setting('SECRET_KEY', sensitive_value)\n        self.assertEqual(cleansed_value, CLEANSED_SUBSTITUTE)\n", "    def test_cleanse_setting(self):\n        self.assertEqual(cleanse_setting('SECRET_KEY', '123'), CLEANSED_SUBSTITUTE)\n        self.assertEqual(cleanse_setting('PUBLIC_VAR', '123'), '123')\n        self.assertEqual(cleanse_setting('DATABASES', {'default': {'PASSWORD': '123'}}), {'default': {'PASSWORD': CLEANSED_SUBSTITUTE}})\n        self.assertEqual(cleanse_setting('ALLOWED_HOSTS', ['example.com']), ['example.com'])\n        self.assertEqual(cleanse_setting('API_TOKEN', SimpleLazyObject(lambda: '123')), CLEANSED_SUBSTITUTE)\n", "    def test_cleanse_setting(self):\n        self.assertEqual(cleanse_setting('API_KEY', 'secret'), CLEANSED_SUBSTITUTE)\n        self.assertEqual(cleanse_setting('NON_SENSITIVE_SETTING', 'public'), 'public')\n        self.assertEqual(cleanse_setting('SETTING_WITH_DICT_VALUE', {'key': 'value'}), {'key': 'value'})\n        self.assertEqual(cleanse_setting('SETTING_WITH_LIST_VALUE', ['item1', 'item2']), ['item1', 'item2'])\n", "    def test_cleanse_setting(self):\n        self.assertEqual(cleanse_setting('foo', 'bar'), 'bar')\n        self.assertEqual(cleanse_setting('API_KEY', 'secret'), CLEANSED_SUBSTITUTE)\n        self.assertEqual(cleanse_setting('DATABASES', {'default': {'PASSWORD': 'secret'}}), {'default': {'PASSWORD': CLEANSED_SUBSTITUTE}})\n        self.assertEqual(cleanse_setting('CALLABLE_SETTING', lambda: 'value'), CallableSettingWrapper(lambda: 'value'))\n"], "sample_69": ["def test_iter_modules_and_files_zip_import(self):\n    with tempfile.TemporaryDirectory() as temp_dir:\n        filename = Path(temp_dir) / 'test.zip'\n        with zipfile.ZipFile(str(filename), 'w') as zip_file:\n            zip_file.writestr('module.py', b'pass')\n\n        sys.path.insert(0, str(filename))\n        self.addCleanup(sys.path.pop, 0)\n\n        import module\n        self.addCleanup(lambda: sys.modules.pop('module', None))\n\n        resolved_filename = filename.resolve()\n        self.clear_autoreload_caches()\n\n        # Test that the zip file itself is found.\n        self.assertIn(resolved_filename, list(autoreload.iter_all_python_module_files()))\n\n        # The individual modules within the zip are not returned.\n        self.assertNotIn(Path('module.py'), list(autoreload.iter_all_python_module_files()))\n", "def test_iter_modules_and_files_zip_import(self):\n    # Create a temporary zip file with a Python module inside.\n    with tempfile.NamedTemporaryFile(suffix='.zip', delete=False) as tmp_file:\n        with zipfile.ZipFile(tmp_file.name, 'w') as zip_file:\n            zip_file.writestr('mymodule.py', b'def foo(): pass')\n        self.addCleanup(os.remove, tmp_file.name)\n\n        # Add the zip file to sys.path and import the module.\n        with extend_sys_path(tmp_file.name):\n            self.import_and_cleanup('mymodule')\n\n        # The module file inside the zip file should be found by iter_modules_and_files.\n        zip_file_path = Path(tmp_file.name)\n        module_path = zip_file_path / 'mymodule.py'\n        self.assertFileFound(module_path)\n", "    def test_no_common_roots(self):\n        paths = [Path('/path/to/file1'), Path('/other/path/to/file2')]\n        self.assertEqual(autoreload.common_roots(paths), ())\n", "    def test_common_roots(self):\n        paths = [\n            Path('/home/user/project/app1/models.py'),\n            Path('/home/user/project/app1/views.py'),\n            Path('/home/user/project/app2/models.py'),\n            Path('/home/user/project/app2/views.py'),\n        ]\n        common_roots = autoreload.common_roots(paths)\n        self.assertEqual(common_roots, (Path('/home/user/project'),))\n", "def test_iter_modules_and_files_zip_import(self):\n    with extend_sys_path():\n        filename = self.temporary_file('example.zip')\n        with zipfile.ZipFile(str(filename), 'w') as zip_file:\n            zip_file.writestr('example/__init__.py', '')\n            zip_file.writestr('example/example.py', '')\n        self.import_and_cleanup('example')\n        self.assertFileFound(filename)\n"], "sample_70": ["def test_protect_on_delete(self):\n    p = Parent.objects.create(name='Parent')\n    c = Child.objects.create(parent=p, name='Child')\n    with self.assertRaises(ProtectedError) as cm:\n        p.delete()\n    self.assertEqual(cm.exception.protected_objects, [c])\n", "def test_protected_error(self):\n    collector = Collector(using=connection.alias)\n    p = Parent.objects.create()\n    c = Child.objects.create(parent=p)\n\n    with self.assertRaises(ProtectedError) as cm:\n        collector.collect([p], source=None, nullable=False, collect_related=True)\n\n    self.assertEqual(cm.exception.protected_objects, [c])\n    self.assertEqual(\n        str(cm.exception),\n        \"Cannot delete some instances of model 'Parent' because they are \"\n        \"referenced through a protected foreign key: 'Child.parent'\",\n    )\n", "def test_protect(self):\n    collector = Collector(using='default')\n    p = Parent.objects.create(name='Parent1')\n    child = Child.objects.create(parent=p, name='Child1')\n    with self.assertRaises(ProtectedError) as e:\n        collector.collect([p], source=None, nullable=False)\n    self.assertEqual(e.exception.protected_objects, [child])\n", "def test_sort(self):\n    collector = Collector(using='default')\n    a1, a2 = A.objects.create(), A.objects.create()\n    b1, b2 = M.objects.create(a=a1), M.objects.create(a=a2)\n    collector.collect([a1, a2])\n    self.assertEqual(list(collector.data.keys()), [A, M])\n    # Introduce a dependency to require sorting.\n    collector.dependencies[A] = {M}\n    collector.sort()\n    self.assertEqual(list(collector.data.keys()), [M, A])\n", "def test_do_nothing_on_delete(self):\n    a = create_a()\n    b = a.b_set.create()\n    Collector(using='default').collect([a])\n    self.assertEqual(B.objects.count(), 1)\n    a.delete()\n    self.assertEqual(B.objects.count(), 1)\n"], "sample_71": ["def test_decimal_pos_rounding(self):\n    self.assertEqual(nformat(Decimal('0.123456789'), '.', 4), '0.1235')\n    self.assertEqual(nformat(Decimal('12345.6789'), '.', 2), '12345.68')\n    self.assertEqual(nformat(Decimal('12345.6789'), '.', 2, grouping=3, thousand_sep=','), '12,345.68')\n", "def test_numberformat_decimal_pos(self):\n    self.assertEqual(nformat(12345.6789, decimal_sep='.', decimal_pos=2), '12345.68')\n    self.assertEqual(nformat(12345.6789, decimal_sep=',', decimal_pos=3), '12345,679')\n    self.assertEqual(nformat(Decimal('12345.6789'), decimal_sep='.', decimal_pos=2), '12345.68')\n    self.assertEqual(nformat(Decimal('12345.6789'), decimal_sep=',', decimal_pos=3), '12345,679')\n", "def test_format_decimal_pos(self):\n    self.assertEqual(nformat(12345.6789, decimal_sep='.', decimal_pos=2), '12345.68')\n    self.assertEqual(nformat(12345.6789, decimal_sep='.', decimal_pos=4), '12345.6789')\n    self.assertEqual(nformat(12345, decimal_sep='.', decimal_pos=2), '12345.00')\n", "def test_decimal_pos(self):\n    self.assertEqual(nformat(123.456, decimal_sep='.', decimal_pos=2), '123.46')\n    self.assertEqual(nformat(123.456, decimal_sep=',', decimal_pos=1), '123,5')\n    self.assertEqual(nformat(123.456, decimal_sep='.', decimal_pos=0), '123')\n    self.assertEqual(nformat(Decimal('123.456'), decimal_sep='.', decimal_pos=2), '123.46')\n    self.assertEqual(nformat(Decimal('123.456'), decimal_sep=',', decimal_pos=1), '123,5')\n    self.assertEqual(nformat(Decimal('123.456'), decimal_sep='.', decimal_pos=0), '123')\n", "def test_decimal_pos_cutoff(self):\n    # Test that decimal positions are cutoff correctly\n    self.assertEqual(nformat(Decimal('0.0001'), '.', 2), '0.00')\n    self.assertEqual(nformat(Decimal('0.0001'), '.', 3), '0.001')\n    self.assertEqual(nformat(Decimal('0.0001'), '.', 4), '0.0001')\n"], "sample_72": ["    def test_float_serializer(self):\n        value = float('inf')\n        serializer = BaseSerializer.get_serializer(value)\n        self.assertIsInstance(serializer, FloatSerializer)\n        result, imports = serializer.serialize()\n        self.assertEqual(result, 'float(\"inf\")')\n        self.assertEqual(imports, set())\n", "    def test_float_serializer(self):\n        serializer = BaseSerializer(1.5)\n        self.assertEqual(serializer.serialize(), ('1.5', set()))\n\n        serializer = BaseSerializer(float('inf'))\n        self.assertEqual(serializer.serialize(), (\"float('inf')\", set()))\n\n        serializer = BaseSerializer(float('-inf'))\n        self.assertEqual(serializer.serialize(), (\"float('-inf')\", set()))\n\n        serializer = BaseSerializer(float('nan'))\n        self.assertEqual(serializer.serialize(), (\"float('nan')\", set()))\n", "    def test_float_serializer(self):\n        serializer = BaseSerializer(1.23)\n        result, imports = serializer.serialize()\n        self.assertEqual(result, '1.23')\n        self.assertEqual(imports, set())\n\n        serializer = BaseSerializer(float('inf'))\n        result, imports = serializer.serialize()\n        self.assertEqual(result, \"float('inf')\")\n        self.assertEqual(imports, set())\n\n        serializer = BaseSerializer(float('-inf'))\n        result, imports = serializer.serialize()\n        self.assertEqual(result, \"float('-inf')\")\n        self.assertEqual(imports, set())\n\n        serializer = BaseSerializer(float('nan'))\n        result, imports = serializer.serialize()\n        self.assertEqual(result, \"float('nan')\")\n        self.assertEqual(imports, set())\n", "    def test_float_serializer(self):\n        serializer = BaseSerializer(1.5)\n        self.assertEqual(serializer.serialize(), ('1.5', set()))\n\n        serializer = BaseSerializer(float('inf'))\n        self.assertEqual(serializer.serialize(), (\"float('inf')\", set()))\n\n        serializer = BaseSerializer(float('-inf'))\n        self.assertEqual(serializer.serialize(), (\"float('-inf')\", set()))\n\n        serializer = BaseSerializer(float('nan'))\n        self.assertEqual(serializer.serialize(), (\"float('nan')\", set()))\n", "    def test_serialize_float(self):\n        serializer = BaseSerializer(1.0)\n        result, imports = serializer.serialize()\n        self.assertEqual(result, '1.0')\n        self.assertEqual(imports, set())\n\n        serializer = BaseSerializer(float('inf'))\n        result, imports = serializer.serialize()\n        self.assertEqual(result, 'float(\"inf\")')\n        self.assertEqual(imports, set())\n\n        serializer = BaseSerializer(float('-inf'))\n        result, imports = serializer.serialize()\n        self.assertEqual(result, 'float(\"-inf\")')\n        self.assertEqual(imports, set())\n\n        serializer = BaseSerializer(float('nan'))\n        result, imports = serializer.serialize()\n        self.assertEqual(result, 'float(\"nan\")')\n        self.assertEqual(imports, set())\n"], "sample_73": ["    def test_post_process_passes(self):\n        \"\"\"\n        The post_process() method should call post_process() on each file,\n        updating their filenames to include a hash of their content.\n        \"\"\"\n        storage.staticfiles_storage.max_post_process_passes = 1\n        original_path = self.hashed_file_path('styles.css')\n        hashed_path = self.hashed_file_path('styles.55e7cbb9ba48.css')\n        with open(original_path, 'w') as f:\n            f.write('.foo { background: url(\"images/blah.png\"); }')\n        with open(hashed_path, 'w') as f:\n            f.write('.foo { background: url(\"images/blah.d41d8cd98f00.png\"); }')\n\n        storage.staticfiles_storage.post_process([original_path], dry_run=False)\n\n        self.assertEqual(storage.staticfiles_storage.hashed_files, {\n            storage.staticfiles_storage.hash_key('styles.css'): hashed_path,\n        })\n        self.assertPostCondition()\n", "    def test_manifest_is_written(self):\n        self.assertPostCondition()\n        storage.staticfiles_storage.save('test.txt', StringIO('Hello World!'))\n        self.assertTrue(storage.staticfiles_storage.exists('staticfiles.json'))\n", "    def test_simple_file_hash(self):\n        original_path = os.path.join(TEST_ROOT, 'project', 'documents', 'document.txt')\n        with open(original_path, 'r') as f:\n            file_hash = storage.staticfiles_storage.file_hash('documents/document.txt', f)\n        self.assertEqual(len(file_hash), 12)\n", "    def test_manifest_is_written(self):\n        self.render_template('css/stylesheet.css')\n        self.assertTrue(self.storage.exists('staticfiles.json'))\n", "    def setUp(self):\n        super().setUp()\n        self.storage = storage.ConfiguredStorage()\n"], "sample_75": ["def test_prefetch_one_to_many(self):\n    with CaptureQueriesContext(connection) as captured_queries:\n        authors = list(Author.objects.prefetch_related('books_written').all())\n        self.assertLess(len(captured_queries), 7)\n\n    # The first query is fetching Authors, so the last two are for prefetching.\n    self.assertIn('WHERE (\"tests_book\".\"author_id\" IN', captured_queries[-2]['sql'])\n    self.assertIn('ORDER BY \"tests_book\".\"id\" ASC', captured_queries[-2]['sql'])\n\n    self.assertEqual(authors[0].books_written.all()[0].title, 'Poems')\n", "def test_m2m_prefetching_with_to_attr(self):\n    # When using to_attr on a ManyToManyField, the attribute is populated with\n    # a list of related objects. We need to ensure that this list is also\n    # prefetched.\n\n    class ModelIterableSubclassTests(TestCase):\n            obj = ModelIterableSubclass.objects.create()\n            obj.m2m.add(Author.objects.create(name='Author 1'))\n            obj.m2m.add(Author.objects.create(name='Author 2'))\n\n            with self.assertNumQueries(2):\n                obj = ModelIterableSubclass.objects.prefetch_related('m2m').get()\n                self.assertEqual(len(obj.authors), 2)\n\n    obj = AuthorWithAge.objects.create(age=25)\n    obj.favorite_authors.add(self.author1, self.author2)\n\n    with self.assertNumQueries(2):\n        obj = AuthorWithAge.objects.prefetch_related('favorite_authors').get()\n        self.assertEqual(len(obj.authors), 2)\n", "def test_prefetch_related_with_to_attr(self):\n    with CaptureQueriesContext(connection) as captured_queries:\n        authors = Author.objects.prefetch_related('books').all()\n        for author in authors:\n            self.assertTrue(hasattr(author, 'books'))\n\n    queries = [q['sql'] for q in captured_queries.captured_queries]\n    self.assertEqual(len(queries), 2)\n    self.assertWhereContains(queries[1], 'book_authors.author_id')\n", "def test_prefetch_related_empty_queryset(self):\n    with CaptureQueriesContext(connection) as captured_queries:\n        Book.objects.none().prefetch_related('authors').all()\n        self.assertEqual(len(captured_queries), 0)\n", "def test_prefetch_related_with_custom_queryset(self):\n    class CustomQuerySet(QuerySet):\n            super()._fetch_all()\n            self._result_cache = list(reversed(self._result_cache))\n\n    qs = Book.objects.prefetch_related(Prefetch('authors', queryset=Author.objects.all(), to_attr='cached_authors'))\n    with CaptureQueriesContext(connection) as captured_queries:\n        books = list(qs)\n        for book in books:\n            with self.assertNumQueries(0):\n                authors = book.cached_authors\n\n    # Test that the CustomQuerySet was used.\n    self.assertEqual([author.name for author in authors], ['Emily', 'Anne', 'Charlotte'])\n    self.assertEqual(len(captured_queries), 2)\n    self.assertWhereContains(captured_queries[1]['sql'], 'author')\n"], "sample_74": ["def test_runshell_db_password(self):\n    dbinfo = {'host': 'localhost', 'port': 5432, 'database': 'mydb', 'user': 'myuser', 'password': 'mypassword'}\n    args, env = self._run_it(dbinfo)\n    self.assertIn('-U', args)\n    self.assertIn('myuser', args)\n    self.assertEqual(env['PGPASSWORD'], 'mypassword')\n", "def test_runshell_db_with_ssl_params(self):\n    dbinfo = {\n        'host': 'localhost',\n        'port': 5432,\n        'database': 'mydb',\n        'user': 'myuser',\n        'password': 'mypassword',\n        'sslmode': 'require',\n        'sslrootcert': '/path/to/root/cert',\n        'sslcert': '/path/to/cert',\n        'sslkey': '/path/to/key'\n    }\n    args, env = self._run_it(dbinfo)\n    self.assertEqual(args, ['psql', '-U', 'myuser', '-h', 'localhost', '-p', '5432', 'mydb'])\n    self.assertEqual(env, {\n        'PGPASSWORD': 'mypassword',\n        'PGSSLMODE': 'require',\n        'PGSSLROOTCERT': '/path/to/root/cert',\n        'PGSSLCERT': '/path/to/cert',\n        'PGSSLKEY': '/path/to/key'\n    })\n", "def test_runshell_with_password_and_ssl(self):\n    dbinfo = {\n        'host': 'localhost',\n        'port': 5432,\n        'database': 'mydb',\n        'user': 'myuser',\n        'password': 'mypassword',\n        'sslmode': 'require',\n        'sslrootcert': '/path/to/root/cert',\n        'sslcert': '/path/to/cert',\n        'sslkey': '/path/to/key'\n    }\n    args, env = self._run_it(dbinfo)\n    self.assertEqual(args, ['psql', '-U', 'myuser', '-h', 'localhost', '-p', '5432', 'mydb'])\n    self.assertEqual(env, {\n        'PGPASSWORD': 'mypassword',\n        'PGSSLMODE': 'require',\n        'PGSSLROOTCERT': '/path/to/root/cert',\n        'PGSSLCERT': '/path/to/cert',\n        'PGSSLKEY': '/path/to/key'\n    })\n", "def test_runshell_with_ssl_params(self):\n    dbinfo = {\n        'host': 'localhost',\n        'port': 5432,\n        'database': 'mydb',\n        'user': 'myuser',\n        'password': 'mypassword',\n        'sslmode': 'require',\n        'sslrootcert': '/path/to/root/cert',\n        'sslcert': '/path/to/cert',\n        'sslkey': '/path/to/key'\n    }\n    args, env = self._run_it(dbinfo)\n    self.assertEqual(args, ['psql', '-U', 'myuser', '-h', 'localhost', '-p', '5432', 'mydb'])\n    self.assertEqual(env, {\n        'PGPASSWORD': 'mypassword',\n        'PGSSLMODE': 'require',\n        'PGSSLROOTCERT': '/path/to/root/cert',\n        'PGSSLCERT': '/path/to/cert',\n        'PGSSLKEY': '/path/to/key'\n    })\n", "def test_runshell_ssl_params(self):\n    dbinfo = {\n        'host': 'localhost',\n        'port': 5432,\n        'database': 'mydb',\n        'user': 'myuser',\n        'password': 'mypassword',\n        'sslmode': 'verify-ca',\n        'sslrootcert': '/path/to/root/cert',\n        'sslcert': '/path/to/client/cert',\n        'sslkey': '/path/to/client/key'\n    }\n    args, env = self._run_it(dbinfo)\n    self.assertEqual(args, ['psql', '-U', 'myuser', '-h', 'localhost', '-p', '5432', 'mydb'])\n    self.assertEqual(env, {\n        'PGPASSWORD': 'mypassword',\n        'PGSSLMODE': 'verify-ca',\n        'PGSSLROOTCERT': '/path/to/root/cert',\n        'PGSSLCERT': '/path/to/client/cert',\n        'PGSSLKEY': '/path/to/client/key'\n    })\n"], "sample_76": ["def test_check_setting_language_code(self):\n    with self.settings(LANGUAGE_CODE='en'):\n        self.assertEqual(check_setting_language_code(None), [])\n\n    with self.settings(LANGUAGE_CODE=self.invalid_tags[0]):\n        self.assertEqual(len(check_setting_language_code(None)), 1)\n        self.assertIsInstance(check_setting_language_code(None)[0], Error)\n\n    for tag in self.invalid_tags[1:]:\n        with self.settings(LANGUAGE_CODE=tag):\n            self.assertEqual(len(check_setting_language_code(None)), 1)\n            self.assertIsInstance(check_setting_language_code(None)[0], Error)\n", "def test_check_setting_language_code(self):\n    with self.settings(LANGUAGE_CODE='en'):\n        self.assertEqual(check_setting_language_code(None), [])\n\n    with self.settings(LANGUAGE_CODE=self.invalid_tags[0]):\n        errors = check_setting_language_code(None)\n        self.assertEqual(len(errors), 1)\n        self.assertIsInstance(errors[0], Error)\n        self.assertEqual(errors[0].id, 'translation.E001')\n\n    for tag in self.valid_tags:\n        with self.settings(LANGUAGE_CODE=tag):\n            self.assertEqual(check_setting_language_code(None), [])\n\n    for tag in self.invalid_tags[1:]:\n        with self.settings(LANGUAGE_CODE=tag):\n            errors = check_setting_language_code(None)\n            self.assertEqual(len(errors), 1)\n            self.assertIsInstance(errors[0], Error)\n            self.assertEqual(errors[0].id, 'translation.E001')\n", "def test_check_setting_language_code(self):\n    with self.settings(LANGUAGE_CODE='en'):\n        errors = check_setting_language_code(None)\n        self.assertEqual(errors, [])\n\n    with self.settings(LANGUAGE_CODE=' invalid '):\n        errors = check_setting_language_code(None)\n        self.assertEqual(len(errors), 1)\n        self.assertIsInstance(errors[0], Error)\n        self.assertEqual(errors[0].id, 'translation.E001')\n\n    with self.settings(LANGUAGE_CODE=None):\n        errors = check_setting_language_code(None)\n        self.assertEqual(len(errors), 1)\n        self.assertIsInstance(errors[0], Error)\n        self.assertEqual(errors[0].id, 'translation.E001')\n", "def test_check_setting_language_code(self):\n    with self.settings(LANGUAGE_CODE='en'):\n        errors = check_setting_language_code(None)\n        self.assertEqual(errors, [])\n\n    with self.settings(LANGUAGE_CODE=None):\n        errors = check_setting_language_code(None)\n        self.assertEqual(len(errors), 1)\n        self.assertIsInstance(errors[0], Error)\n        self.assertEqual(errors[0].id, 'translation.E001')\n\n    with self.settings(LANGUAGE_CODE=' invalid '):\n        errors = check_setting_language_code(None)\n        self.assertEqual(len(errors), 1)\n        self.assertIsInstance(errors[0], Error)\n        self.assertEqual(errors[0].id, 'translation.E001')\n", "def test_check_setting_language_code(self):\n    with self.settings(LANGUAGE_CODE='en'):\n        errors = check_setting_language_code(None)\n        self.assertEqual(errors, [])\n\n    with self.settings(LANGUAGE_CODE=None):\n        errors = check_setting_language_code(None)\n        self.assertEqual(len(errors), 1)\n        self.assertIsInstance(errors[0], Error)\n        self.assertEqual(errors[0].id, 'translation.E001')\n\n    with self.settings(LANGUAGE_CODE=' invalid '):\n        errors = check_setting_language_code(None)\n        self.assertEqual(len(errors), 1)\n        self.assertIsInstance(errors[0], Error)\n        self.assertEqual(errors[0].id, 'translation.E001')\n"], "sample_77": ["def test_html_safe(self):\n    @html_safe\n    class MyClass:\n            self.value = value\n\n            return self.value\n\n    obj = MyClass('<script>alert(\"XSS\")</script>')\n    self.assertEqual(str(obj), '<script>alert(\"XSS\")</script>')\n    self.assertIsInstance(str(obj), str)\n    self.assertEqual(obj.__html__(), '<script>alert(\"XSS\")</script>')\n\n    with self.assertRaises(ValueError):\n        @html_safe\n        class MyClass:\n                self.value = value\n\n                return self.value\n\n                return self.value\n\n    with self.assertRaises(ValueError):\n        @html_safe\n        class MyClass:\n                self.value = value\n", "def test_escapejs(self):\n    # Make sure escapejs does not mangle safe strings\n    s = mark_safe('Hello, world!')\n    self.assertEqual(escapejs(s), 'Hello, world!')\n\n    # Check escapejs on unsafe strings\n    s = 'foo\\nbar\\r\\nbaz'\n    self.assertEqual(escapejs(s), 'foo\\\\u000Abar\\\\u000D\\\\u000Abaz')\n\n    # Check escapejs on lazy strings\n    from django.utils.functional import lazystr\n    s = lazystr(lambda: 'Hello, world!')\n    self.assertEqual(escapejs(s), 'Hello, world!')\n\n    # Check escapejs on SafeData instances\n    class TestSafeData:\n            self.value = value\n\n            return self.value\n\n            return self.value\n\n    s = TestSafeData('Hello, world!')\n    self.assertEqual(escapejs(s), 'Hello, world!')\n\n    # Check escapejs with non-ASCII characters\n    s = '\u00e0\u00e9\u00ef\u00f4\u00fc'\n    self.assertEqual(escapejs(s), '\\\\u00E0\\\\u00E9\\\\u00EF\\\\u00F4\\\\u00FC')\n", "def test_html_safe_decorator(self):\n    @html_safe\n    class MyClass:\n            return '<p>Hello, World!</p>'\n\n    obj = MyClass()\n    self.assertEqual(str(obj), '<p>Hello, World!</p>')\n    self.assertIsInstance(str(obj), str)\n    self.assertEqual(obj.__html__(), '<p>Hello, World!</p>')\n", "def test_escapejs(self):\n    # Test escapejs against values that need escaping and ones that don't\n    self.check_output(escapejs, 'testing')\n    self.check_output(escapejs, 'testing\\n', r'testing\\u000A')\n    self.check_output(escapejs, '<testing>', r'\\u003Ctesting\\u003E')\n", "def test_escapejs(self):\n    # Test escapejs against a list of known values\n    test_values = (\n        (\"'\", \"\\\\'\"),\n        ('\"', '\\\\\"'),\n        ('<', '\\\\u003C'),\n        ('>', '\\\\u003E'),\n        ('&', '\\\\u0026'),\n        ('=', '\\\\u003D'),\n        ('-', '\\\\u002D'),\n        (';', '\\\\u003B'),\n        ('`', '\\\\u0060'),\n        ('\\u2028', '\\\\u2028'),\n        ('\\u2029', '\\\\u2029'),\n    )\n    for value, expected in test_values:\n        self.check_output(escapejs, value, expected)\n"], "sample_78": ["def test_base_command_arguments(self):\n    class TestCommand(BaseCommand):\n            return options['test_arg']\n\n    command = TestCommand()\n    parser = command.create_parser('manage.py', 'test')\n    parser.add_argument('--test-arg', action='store_true')\n    options = parser.parse_args(['--test-arg'])\n    self.assertTrue(command.handle(**options))\n", "def test_command_parser_called_from_command_line(self):\n    class TestCommand(BaseCommand):\n            pass\n\n    command = TestCommand()\n    parser = command.create_parser('manage.py', 'test')\n    self.assertFalse(parser.called_from_command_line)\n\n    command._called_from_command_line = True\n    parser = command.create_parser('manage.py', 'test')\n    self.assertTrue(parser.called_from_command_line)\n", "def test_base_command_style(self):\n    # Test that the BaseCommand.style attribute is set correctly\n    cmd = BaseCommand()\n    self.assertIsInstance(cmd.style, type(color_style()))\n    cmd = BaseCommand(no_color=True)\n    self.assertIsInstance(cmd.style, type(no_style()))\n    cmd = BaseCommand(force_color=True)\n    self.assertIsInstance(cmd.style, type(color_style(force_color=True)))\n", "def test_base_command_style(self):\n    class TestCommand(BaseCommand):\n            self.stdout.write(self.style.SUCCESS('Success message'))\n            self.stderr.write(self.style.ERROR('Error message'))\n\n    command = TestCommand()\n    with captured_stderr() as stderr:\n        with mock.patch.object(command.stdout, 'write') as stdout_write:\n            command.handle()\n            self.assertIn('\\033[92mSuccess message\\033[0m', stdout_write.call_args[0][0])\n            self.assertIn('\\033[91mError message\\033[0m', stderr.getvalue())\n", "def test_command_style(self):\n    class TestCommand(BaseCommand):\n            self.stdout.write(self.style.SUCCESS('success message'))\n            self.stderr.write(self.style.ERROR('error message'))\n\n    out = StringIO()\n    err = StringIO()\n    command = TestCommand(stdout=out, stderr=err)\n    command.handle()\n    self.assertEqual(out.getvalue(), 'success message\\n')\n    self.assertEqual(err.getvalue(), 'error message\\n')\n"], "sample_79": ["    def test_add(self):\n        output = self.engine.render_to_string('t', {'value': 5})\n        self.assertEqual(output, '10')\n", "    def test_plural_with_argument(self):\n        self.check_values(\n            (1, 'candy'),\n            (2, 'candies'),\n            (3, 'candies'),\n            (0, 'candies'),\n        )\n", "    def check_values(self, *tests):\n        for value, suffix, expected in tests:\n            with self.subTest(value=value, suffix=suffix):\n                output = self.engine.render_to_string('t', {'value': value})\n                self.assertEqual(output, expected)\n", "    def check_values(self, *tests):\n        for value, arg, expected in tests:\n            with self.subTest(value=value, arg=arg):\n                output = self.engine.render_to_string('t', {'value': value})\n                self.assertEqual(output, expected)\n", "    def test_filesizeformat(self):\n        self.assertEqual(self.engine.render_to_string('t', {'value': 1024}), '1.0 KB')\n        self.assertEqual(self.engine.render_to_string('t', {'value': 1000000}), '976.6 KB')\n        self.assertEqual(self.engine.render_to_string('t', {'value': 1200000000}), '1.1 GB')\n        self.assertEqual(self.engine.render_to_string('t', {'value': 1400000000000}), '1.3 TB')\n        self.assertEqual(self.engine.render_to_string('t', {'value': 1600000000000000}), '1.5 PB')\n        self.assertEqual(self.engine.render_to_string('t', {'value': -150}), '-150 bytes')\n        self.assertEqual(self.engine.render_to_string('t', {'value': 0}), '0 bytes')\n        self.assertEqual(self.engine.render_to_string('t', {'value': 'not a number'}), '0 bytes')\n"], "sample_80": ["def test_is_nullable(self):\n    field = CharField()\n    query = Query(Author)\n    self.assertIs(query.is_nullable(field), False)\n\n    query.connection = type('FakeConnection', (), {'features': type('FakeFeatures', (), {'interprets_empty_strings_as_nulls': True})})\n    self.assertIs(query.is_nullable(field), True)\n", "def test_query_join_reuse(self):\n    qs = Query(Author)\n    join1 = qs.join(BaseTable(\"author\", \"author\"), None)\n    join2 = qs.join(BaseTable(\"author\", \"author\"), {join1: Join(qs.get_meta().db_table, join1, None, INNER)})\n    self.assertEqual(join1, join2)\n", "def test_add_annotation(self):\n    q = Query(Author)\n    annotation = Lower('name')\n    q.add_annotation(annotation, 'lower_name', is_summary=False)\n    self.assertIn('lower_name', q.annotations)\n    self.assertEqual(q.annotations['lower_name'], annotation)\n", "def test_resolve_expression_with_filter(self):\n    qs = Query(Author)\n    annotation = F('name')\n    resolved_annotation = annotation.resolve_expression(qs, allow_joins=True, reuse=None, summarize=False)\n    self.assertEqual(resolved_annotation, SimpleCol('name', Author._meta.get_field('name')))\n", "def test_try_transform(self):\n    query = Query(Item)\n    lookup_name = 'lower'\n    lhs = F('name')\n    with self.assertRaises(FieldError) as cm:\n        query.try_transform(lhs, lookup_name)\n    self.assertEqual(str(cm.exception), \"Unsupported lookup 'lower' for CharField or join on the field not permitted.\")\n\n    register_lookup(CharField, lookup_name, lambda lhs, rhs: lhs)\n    transformed_lhs = query.try_transform(lhs, lookup_name)\n    self.assertEqual(transformed_lhs, lhs)\n\n    unregister_lookup = lookup_registration.cache.pop(CharField, lookup_name)\n    unregister_lookup()\n"], "sample_82": ["def test_localized_select_date_widget(self):\n    class SelectDateForm(Form):\n        date = DateField(widget=SelectDateWidget(years=('2007', '2008')))\n\n    # January 3rd, 2008\n    data = {'date_year': '2008', 'date_month': '1', 'date_day': '3'}\n    form = SelectDateForm(data)\n    self.assertTrue(form.is_valid())\n    self.assertEqual(form.cleaned_data['date'], date(2008, 1, 3))\n", "def test_select_date_widget_formatting(self):\n    class SelectDateForm(Form):\n        date_field = DateField(widget=SelectDateWidget(years=('2007', '2008'), months=MONTHS_AP))\n\n    form = SelectDateForm()\n    self.check_html(form, 'date_field', '<select name=\"date_field_month\" id=\"id_date_field_month\">\\n'\n                                   '<option value=\"\">---------</option>\\n'\n                                   '<option value=\"8\">Apr</option>\\n'\n                                   '<option value=\"5\">Aug</option>\\n'\n                                   # ... all month options ...\n                                   '</select>', input_type='select')\n\n    form = SelectDateForm({'date_field_year': '2008', 'date_field_month': '12', 'date_field_day': '1'})\n    self.assertTrue(form.is_valid())\n    self.assertEqual(form.cleaned_data['date_field'], date(2008, 12, 1))\n", "def test_l10n_date_format(self):\n    translation.activate('de')\n    widget = SelectDateWidget()\n    self.assertEqual(widget.format_value(date(2015, 6, 15)), {'year': 2015, 'month': 6, 'day': 15})\n    self.assertEqual(widget._parse_date_fmt(), ['day', 'month', 'year'])\n    translation.deactivate()\n", "def test_localized_date(self):\n    widget = SelectDateWidget()\n    with translation.override('en_GB'):\n        self.check_html(widget, 'date', '2016-02-29', html=(\n            '<select name=\"date_year\">'\n            '<option value=\"\">---</option>'\n            '<option value=\"2016\" selected>2016</option>'\n            '</select><select name=\"date_month\">'\n            '<option value=\"\">---</option>'\n            '<option value=\"2\" selected>February</option>'\n            '</select><select name=\"date_day\">'\n            '<option value=\"\">---</option>'\n            '<option value=\"29\" selected>29</option>'\n            '</select>'\n        ))\n", "def test_use_required_attribute(self):\n    widget = SelectDateWidget(required=True)\n    self.assertTrue(widget.use_required_attribute(None))\n    self.assertFalse(widget.use_required_attribute(date.today()))\n\n    widget = SelectDateWidget(required=False)\n    self.assertFalse(widget.use_required_attribute(None))\n    self.assertFalse(widget.use_required_attribute(date.today()))\n"], "sample_81": ["def test_regex_pattern_match(self):\n    pattern = RegexPattern(r'^users/(?P<username>\\w+)/$')\n    match = pattern.match('/users/john/')\n    self.assertIsNotNone(match)\n    self.assertEqual(match[0], '')\n    self.assertEqual(match[1], ())\n    self.assertEqual(match[2], {'username': 'john'})\n", "def test_regex_pattern_match(self):\n    pattern = RegexPattern(r'^test/(?P<slug>[-\\w]+)/$')\n    match = pattern.match('/test/slug-test/')\n    self.assertIsNotNone(match)\n    self.assertEqual(match[0], '')\n    self.assertEqual(match[1], ())\n    self.assertEqual(match[2], {'slug': 'slug-test'})\n\n    match = pattern.match('/test/')\n    self.assertIsNone(match)\n\n    match = pattern.match('/test/slug-test')\n    self.assertIsNone(match)\n", "def test_route_pattern_lazy_translation(self):\n    pattern = RoutePattern(_('test/%(path)s/'))\n    self.assertIsInstance(pattern.regex, property)\n", "def test_regex_pattern_match(self):\n    pattern = RegexPattern(r'^test/(?P<arg1>\\w+)/(?P<arg2>\\d+)/$')\n    match = pattern.match('test/hello/123/')\n    self.assertIsNotNone(match)\n    self.assertEqual(match[0], '')\n    self.assertEqual(match[1], ())\n    self.assertEqual(match[2], {'arg1': 'hello', 'arg2': '123'})\n", "def test_regex_pattern_match(self):\n    pattern = RegexPattern(r'^articles/(?P<year>[0-9]{4})/$', name='articles')\n    match = pattern.match('/articles/2022/')\n    self.assertIsNotNone(match)\n    self.assertEqual(match[1], ())\n    self.assertEqual(match[2], {'year': '2022'})\n"], "sample_83": ["def test_filter_registration(self):\n        return value.upper()\n\n    self.library.filter('my_filter', my_filter)\n    self.assertIn('my_filter', self.library.filters)\n    self.assertEqual(self.library.filters['my_filter'], my_filter)\n", "def test_filter_registration_with_name(self):\n        return value.upper()\n    self.library.filter('my_filter', filter_func)\n    self.assertEqual(self.library.filters['my_filter'], filter_func)\n", "def test_filter_registration(self):\n        return value.upper()\n\n    self.library.filter('my_filter', filter_func)\n    self.assertIn('my_filter', self.library.filters)\n    self.assertEqual(self.library.filters['my_filter'], filter_func)\n", "def test_filter_registration(self):\n        return value.upper()\n\n    self.library.filter('my_filter', filter_func)\n    self.assertIn('my_filter', self.library.filters)\n    self.assertEqual(self.library.filters['my_filter'], filter_func)\n", "def test_filter_registration(self):\n        return value.upper()\n\n    self.library.filter('my_filter', filter_func)\n    self.assertIn('my_filter', self.library.filters)\n    self.assertEqual(self.library.filters['my_filter'], filter_func)\n"], "sample_85": ["def test_on_delete_set_null(self):\n    r = R.objects.create()\n    m = M.objects.create(r=r)\n    r.delete()\n    m.refresh_from_db()\n    self.assertIsNone(m.r)\n", "def test_on_delete_cascade(self):\n    r = R.objects.create()\n    m = M.objects.create(r=r)\n    self.DEFAULT.r = r\n    self.DEFAULT.save()\n    r.delete()\n    with self.assertRaises(M.DoesNotExist):\n        M.objects.get(pk=m.pk)\n    with self.assertRaises(R.DoesNotExist):\n        R.objects.get(pk=self.DEFAULT.pk)\n", "def test_on_delete_integrity_error(self):\n    # Create an R object that can't be deleted because it's referenced by an A object.\n    r = R.objects.create()\n    a = A.objects.create(r=r)\n\n    # Try deleting the R object. This should raise an IntegrityError.\n    with self.assertRaises(IntegrityError):\n        r.delete()\n", "def test_on_delete_cascade(self):\n    a = create_a()\n    r = get_default_r()\n    a.r = r\n    a.save()\n    self.assertEqual(R.objects.count(), 2)\n    r.delete()\n    self.assertEqual(R.objects.count(), 1)\n    with self.assertRaises(A.DoesNotExist):\n        A.objects.get(pk=a.pk)\n", "def test_cascade_deletion(self):\n    a1 = create_a(R=self.DEFAULT)\n    collector = Collector(using='default')\n    collector.collect([a1])\n    self.assertCountEqual(collector.instances_with_model(), [(a1, A)])\n    collector.delete()\n    with self.assertRaises(A.DoesNotExist):\n        A.objects.get(pk=a1.pk)\n"], "sample_84": ["def test_urlencode_none_values(self):\n    with self.assertRaisesMessage(TypeError, self.cannot_encode_none_msg):\n        urlencode({'key': None})\n    with self.assertRaisesMessage(TypeError, self.cannot_encode_none_msg):\n        urlencode(MultiValueDict({'key': [None]}))\n    with self.assertRaisesMessage(TypeError, self.cannot_encode_none_msg):\n        urlencode([('key', None)])\n", "def test_urlencode_with_non_string_values(self):\n    non_string_values = [1, 2.3, True, None]\n    query = {'a': 1, 'b': 2.3, 'c': True, 'd': None}\n    with self.assertRaises(TypeError) as cm:\n        urlencode(query)\n    self.assertEqual(str(cm.exception), self.cannot_encode_none_msg)\n    query['d'] = 'None'\n    for key, value in query.items():\n        self.assertIn(key, urlencode(query))\n        self.assertIn(str(value), urlencode(query))\n", "def test_urlencode(self):\n    self.assertEqual(urlencode({'a': 1, 'b': 2}), 'a=1&b=2')\n    self.assertEqual(urlencode([('a', 1), ('b', 2)]), 'a=1&b=2')\n    self.assertEqual(urlencode(MultiValueDict({'a': [1, 2], 'b': [3, 4]})), 'a=1&a=2&b=3&b=4')\n    with self.assertRaises(TypeError) as e:\n        urlencode({'a': None})\n    self.assertEqual(str(e.exception), self.cannot_encode_none_msg)\n    with self.assertRaises(TypeError) as e:\n        urlencode([('a', None)])\n    self.assertEqual(str(e.exception), self.cannot_encode_none_msg)\n    with self.assertRaises(TypeError) as e:\n        urlencode(MultiValueDict({'a': [None]}))\n    self.assertEqual(str(e.exception), self.cannot_encode_none_msg)\n", "    def test_urlsafe_base64_encode_decode(self):\n        bytestring = b'Hello, World!'\n        encoded = urlsafe_base64_encode(bytestring)\n        decoded = urlsafe_base64_decode(encoded)\n        self.assertEqual(decoded, bytestring)\n", "    def test_urlencode_with_none_value(self):\n        with self.assertRaisesMessage(TypeError, self.cannot_encode_none_msg):\n            urlencode({'key': None})\n"], "sample_86": ["def test_cached_property(self):\n    class TestClass:\n        @cached_property\n            return 'Hello, world!'\n\n    obj = TestClass()\n    self.assertEqual(obj.my_property, 'Hello, world!')\n    # The property should be cached now.\n    with mock.patch.object(TestClass, 'my_property', new_callable=mock.PropertyMock) as mock_my_property:\n        self.assertEqual(obj.my_property, 'Hello, world!')\n        mock_my_property.assert_not_called()\n", "def test_cached_property(self):\n    class TestClass:\n        @cached_property\n            return 'Cached attribute value'\n\n    obj = TestClass()\n    self.assertEqual(obj.cached_attr, 'Cached attribute value')\n    # Accessing the attribute again should not re-call the method.\n    with mock.patch.object(TestClass, 'cached_attr', side_effect=Exception('Should not be called')):\n        self.assertEqual(obj.cached_attr, 'Cached attribute value')\n", "def test_cached_property(self):\n    class Test:\n        @cached_property\n            return 42\n\n    t = Test()\n    self.assertEqual(t.attr, 42)\n    self.assertEqual(t.__dict__['attr'], 42)\n\n    # Subsequent access comes from __dict__, not re-calling the function.\n    with mock.patch.object(Test, 'attr', side_effect=Exception('should not be called')):\n        self.assertEqual(t.attr, 42)\n", "def test_lazy(self):\n    # Test that lazy works with various numbers of arguments and keyword arguments\n        return args, kwargs\n\n    lazy_func = lazy(func, tuple)\n    self.assertEqual(lazy_func(1, 2, foo='bar'), ((1, 2), {'foo': 'bar'}))\n    self.assertEqual(lazy_func(), ((), {}))\n\n    # Test that lazy works with methods\n    class Class:\n            return arg\n\n    instance = Class()\n    lazy_method = lazy(instance.method, str)\n    self.assertEqual(lazy_method('hello'), 'hello')\n", "def test_cached_property(self):\n    class Test:\n        @cached_property\n            return 42\n\n    t = Test()\n    self.assertEqual(t.cached, 42)\n    self.assertEqual(t.cached, 42)\n\n    # The property should only be called once.\n    with mock.patch.object(Test, 'cached', new_callable=mock.PropertyMock) as m:\n        m.fget.side_effect = [43]\n        self.assertEqual(t.cached, 42)\n        m.assert_not_called()\n"], "sample_88": ["def test_safe_mime_multipart(self):\n    msg = SafeMIMEMultipart()\n    msg['Subject'] = 'Test SafeMIMEMultipart'\n    msg.attach(MIMEText('Hello, world!'))\n    self.assertEqual(msg.as_string(), '''Content-Type: multipart/mixed; boundary=\"===============...==", "def test_sanitize_address(self):\n    \"\"\"\n    Test that sanitize_address correctly formats email addresses and handles\n    encoding non-ASCII characters in names.\n    \"\"\"\n    # Test basic ASCII address formatting\n    self.assertEqual(sanitize_address(\"John Doe <john@example.com>\", \"utf-8\"), \"John Doe <john@example.com>\")\n\n    # Test non-ASCII name with ASCII address\n    self.assertEqual(\n        sanitize_address(\"J\u00f6hn Doe <john@example.com>\", \"utf-8\"),\n        \"=?utf-8?q?J=C3=B6hn_Doe?= <john@example.com>\",\n    )\n\n    # Test non-ASCII name with non-ASCII address (IDN)\n    self.assertEqual(\n        sanitize_address(\"J\u00f6hn D\u00f6e <j\u00f6hn@ex\u00e4mple.com>\", \"utf-8\"),\n        \"=?utf-8?q?J=C3=B6hn_D=C3=B6e?= <xn--jhnp4c@xn--exmple-cua.com>\",\n    )\n", "def test_sanitize_address(self):\n    # Test that non-ASCII names are properly encoded in addresses\n    address = ('Name with non-ASCII chars ', 'test@example.com')\n    sanitized_address = sanitize_address(address, encoding='utf-8')\n    self.assertEqual(\n        parseaddr(sanitized_address),\n        ('Name with non-ASCII chars ', 'test@example.com'),\n    )\n    # Test that ASCII names with commas are properly escaped\n    address = ('Name, with comma', 'test@example.com')\n    sanitized_address = sanitize_address(address, encoding='utf-8')\n    self.assertEqual(\n        parseaddr(sanitized_address),\n        ('\"Name, with comma\"', 'test@example.com'),\n    )\n    # Test that non-ASCII domain names are properly encoded in addresses\n    address = ('', 'test@.example.com>')\n    sanitized_address = sanitize_address(address, encoding='utf-8')\n    self.assertEqual(\n        parseaddr(sanitized_address),\n        ('', 'test@xn--non-ascii-domain-n2g.example.com'),\n    )\n    # Test that non-ASCII local parts are properly encoded in addresses\n    address = ('', '')\n    sanitized_address = sanitize_address(address, encoding='utf-8')\n    self.assertEqual(\n        parseaddr(sanitized_address),\n        ('', 'xn--non-ascii-localpart-q9c@domain.example.com'),\n    )\n", "def test_sanitize_address(self):\n    \"\"\"\n    Test that sanitize_address correctly encodes non-ASCII characters in names,\n    and removes any non-ASCII characters from localparts.\n    \"\"\"\n    self.assertEqual(\n        sanitize_address(\"John Doe <john.doe@example.com>\", \"utf-8\"),\n        \"John Doe <john.doe@example.com>\",\n    )\n\n    self.assertEqual(\n        sanitize_address(\"J\u00f6hn Doe <j\u00f6hn.doe@example.com>\", \"utf-8\"),\n        \"=?utf-8?q?J=C3=B6hn_Doe?= <john.doe@example.com>\",\n    )\n\n    self.assertEqual(\n        sanitize_address(\"John D\u00f6e <john.d\u00f6e@example.com>\", \"utf-8\"),\n        \"=?utf-8?q?John_D=C3=B6e?= <john.doe@example.com>\",\n    )\n", "def test_sanitize_address(self):\n    \"\"\"\n    Test that sanitize_address correctly encodes addresses and handles \n    non-ASCII characters.\n    \"\"\"\n    # ASCII-only address\n    self.assertEqual(sanitize_address('test@example.com', 'utf-8'), 'test@example.com')\n\n    # Non-ASCII local part\n    self.assertEqual(sanitize_address(('Test K\u00fchl', 'test@example.com'), 'utf-8'), \n                     str(Address(Header('Test K\u00fchl', 'utf-8').encode(), addr_spec='test@example.com')))\n\n    # Non-ASCII domain\n    self.assertEqual(sanitize_address('test@k\u00fchl.example.com', 'utf-8'), \n                     str(Address('', addr_spec='test@xn--khl-uia.example.com')))\n"], "sample_87": ["def test_iter_modules_and_files_zip(self):\n    with extend_sys_path():\n        filename = 'zipmodule.zip'\n        module_name = 'zipped_module'\n        zip_file_path = self.temporary_file(filename)\n        with zipfile.ZipFile(str(zip_file_path), 'w') as zip_file:\n            zip_file.writestr(f'{module_name}.py', '')\n        sys.path.append(str(zip_file_path.parent))\n        self.import_and_cleanup(module_name)\n        self.assertFileFound(zip_file_path)\n        sys.path.remove(str(zip_file_path.parent))\n", "def test_iter_modules_and_files_zip_import(self):\n    with tempfile.TemporaryDirectory() as tmpdir:\n        with zipfile.ZipFile(Path(tmpdir) / 'example.zip', 'w') as zip_file:\n            zip_file.writestr('example/__init__.py', '')\n            zip_file.writestr('example/module.py', 'x = 1')\n        sys.path.append(str(Path(tmpdir) / 'example.zip'))\n        self.import_and_cleanup('example.module')\n        self.assertFileFound(Path(tmpdir) / 'example.zip')\n        sys.path.pop()\n", "    def test_common_roots(self):\n        temp_dir = tempfile.mkdtemp()\n        self.addCleanup(shutil.rmtree, temp_dir)\n        path1 = Path(temp_dir) / 'path1'\n        path2 = Path(temp_dir) / 'path2'\n        common_root = autoreload.common_roots([path1, path2])\n        self.assertEqual(common_root, (Path(temp_dir),))\n", "    def setUp(self):\n        self.tempdir = tempfile.mkdtemp()\n        self.addCleanup(shutil.rmtree, self.tempdir)\n", "    def test_common_roots(self):\n        paths = [\n            Path('/home/user/project/app1/models.py'),\n            Path('/home/user/project/app1/views.py'),\n            Path('/home/user/project/app2/models.py'),\n            Path('/home/user/project/settings.py'),\n        ]\n        common_roots = autoreload.common_roots(paths)\n        self.assertEqual(common_roots, (Path('/home/user/project'),))\n"], "sample_89": ["    def test_zip_file(self):\n        with tempfile.TemporaryDirectory() as tmpdir:\n            zip_filename = Path(tmpdir) / 'example.zip'\n            with zipfile.ZipFile(zip_filename, 'w') as zip_file:\n                zip_file.writestr('example/__init__.py', '')\n                zip_file.writestr('example/module.py', '')\n            with extend_sys_path(tmpdir):\n                import example.module\n                self.clear_autoreload_caches()\n                files = list(autoreload.iter_all_python_module_files())\n                self.assertIn(zip_filename.resolve(), [f.resolve() for f in files])\n", "    def test_common_roots(self):\n        paths = [\n            Path('/home/user/project/src/app.py'),\n            Path('/home/user/project/src/settings.py'),\n            Path('/home/user/project/templates/base.html'),\n        ]\n        roots = autoreload.common_roots(paths)\n        self.assertEqual(roots, (Path('/home/user/project'),))\n", "    def test_autoreload_started_signal(self):\n        reloader = autoreload.StatReloader()\n        signal_received = []\n\n            signal_received.append(sender)\n\n        autoreload.autoreload_started.connect(receiver)\n        reloader.run(threading.Thread(target=lambda: None))\n\n        self.assertEqual(signal_received, [reloader])\n", "def test_iter_modules_and_files_with_zip_file(self):\n    # Create a temporary zip file containing a Python module.\n    with tempfile.NamedTemporaryFile(suffix='.zip', delete=False) as tmp_file:\n        zip_filename = Path(tmp_file.name)\n        self.addCleanup(zip_filename.unlink)\n\n        with zipfile.ZipFile(tmp_file, 'w') as zip_file:\n            zip_file.writestr('module.py', b'')\n            zip_file.writestr('__init__.py', b'')\n\n        with extend_sys_path(str(zip_filename.parent)):\n            self.import_and_cleanup('module')\n\n            self.assertFileFound(zip_filename)\n", "def test_iter_modules_and_files_zip_import(self):\n    with tempfile.TemporaryDirectory() as tmpdir:\n        with open(Path(tmpdir) / 'test.py', 'w') as f:\n            f.write('x = 1')\n        zip_filename = Path(tmpdir) / 'test.zip'\n        with zipfile.ZipFile(str(zip_filename), 'w') as zip_file:\n            zip_file.write(str(Path(tmpdir) / 'test.py'), 'test.py')\n\n        # Insert the zip file into sys.path.\n        with extend_sys_path([str(zip_filename)]):\n            self.import_and_cleanup('test')\n            self.assertFileFound(zip_filename)\n"], "sample_90": ["def test_modelform_defines_fields(self):\n    class NoFieldsForm(forms.ModelForm):\n        pass\n\n    self.assertFalse(modelform_defines_fields(NoFieldsForm))\n\n    class FieldsForm(forms.ModelForm):\n        class Meta:\n            model = Article\n            fields = '__all__'\n\n    self.assertTrue(modelform_defines_fields(FieldsForm))\n\n    class ExcludeForm(forms.ModelForm):\n        class Meta:\n            model = Article\n            exclude = ['id']\n\n    self.assertTrue(modelform_defines_fields(ExcludeForm))\n", "def test_model_to_dict_with_foreign_key(self):\n    homepage = Homepage.objects.create(url='http://example.com')\n    author = Author.objects.create(name='Author', homepage=homepage)\n    data = model_to_dict(author)\n    self.assertEqual(data['homepage'], homepage.pk)\n", "def test_modelform_defines_fields(self):\n    self.assertTrue(modelform_defines_fields(BookForm))\n    self.assertTrue(modelform_defines_fields(DerivedBookForm))\n    self.assertFalse(modelform_defines_fields(forms.ModelForm))\n", "def test_modelform_defines_fields(self):\n    self.assertTrue(modelform_defines_fields(BookForm))\n    self.assertFalse(modelform_defines_fields(forms.ModelForm))\n", "def test_modelformset_factory_fields_and_exclude(self):\n    with self.assertRaises(ImproperlyConfigured):\n        modelformset_factory(\n            Book,\n            fields=('title',),\n            exclude=('author',)\n        )\n"], "sample_91": ["def test_page_not_found_view(self):\n    \"\"\"Test page_not_found view\"\"\"\n    request = self.request_factory.get('/nonexistent_url/')\n    response = page_not_found(request, Http404('Nonexistent URL'))\n    self.assertEqual(response.status_code, 404)\n    self.assertTemplateUsed(response, '404.html')\n    self.assertIn(b'Nonexistent URL', response.content)\n\n    # Test with a custom template name\n    response = page_not_found(request, Http404('Nonexistent URL'), template_name='custom_404.html')\n    self.assertEqual(response.status_code, 404)\n    self.assertTemplateUsed(response, 'custom_404.html')\n\n    # Test with a missing custom template\n    with self.assertRaises(TemplateDoesNotExist):\n        page_not_found(request, Http404('Nonexistent URL'), template_name='missing_custom_404.html')\n", "def test_page_not_found(self):\n    request = self.request_factory.get('/nonexistent_url/')\n    response = page_not_found(request, Http404('Nonexistent URL'))\n    self.assertEqual(response.status_code, 404)\n    self.assertTemplateUsed(response, '404.html')\n    self.assertIn(b'Nonexistent URL', response.content)\n", "def test_page_not_found(self):\n    \"\"\"Test page_not_found view\"\"\"\n    request = self.request_factory.get('/nonexistent_url/')\n    response = page_not_found(request, Http404('Nonexistent URL'))\n    self.assertEqual(response.status_code, 404)\n    self.assertTemplateUsed(response, '404.html')\n    self.assertIn(b'Nonexistent URL', response.content)\n\n    # Test that the exception message is used if it's a string\n    response = page_not_found(request, Http404('Another nonexistent URL'))\n    self.assertEqual(response.status_code, 404)\n    self.assertTemplateUsed(response, '404.html')\n    self.assertIn(b'Another nonexistent URL', response.content)\n\n    # Test that the exception class name is used if the message is not a string\n    response = page_not_found(request, Exception())\n    self.assertEqual(response.status_code, 404)\n    self.assertTemplateUsed(response, '404.html')\n    self.assertIn(b'Exception', response.content)\n", "def test_page_not_found_renders_correct_template(self):\n    \"\"\"Test that page_not_found renders the correct template\"\"\"\n    request = self.request_factory.get('/nonexistent_url/')\n    response = page_not_found(request, Http404('Page not found'))\n    self.assertEqual(response.status_code, 404)\n    self.assertTemplateUsed(response, '404.html')\n    self.assertIn(b'Not Found', response.content)\n    self.assertIn(b'The requested resource was not found on this server.', response.content)\n\n    # Test that a custom template is used if provided\n    with override_settings(ROOT_URLCONF='view_tests.urls'):\n        response = page_not_found(request, Http404('Page not found'), template_name='custom_404.html')\n        self.assertEqual(response.status_code, 404)\n        self.assertTemplateUsed(response, 'custom_404.html')\n", "def test_page_not_found(self):\n    request = self.request_factory.get('/nonexistent_url/')\n    response = page_not_found(request, Http404('Nonexistent URL'))\n    self.assertEqual(response.status_code, 404)\n    self.assertTemplateUsed(response, '404.html')\n    self.assertEqual(response.context['request_path'], '/nonexistent_url/')\n    self.assertEqual(response.context['exception'], 'Nonexistent URL')\n\n    # Test with a custom template name.\n    response = page_not_found(request, Http404('Nonexistent URL'), template_name='custom_404.html')\n    self.assertEqual(response.status_code, 404)\n    self.assertTemplateUsed(response, 'custom_404.html')\n\n    # Test when the custom template does not exist.\n    with self.assertRaises(TemplateDoesNotExist):\n        page_not_found(request, Http404('Nonexistent URL'), template_name='nonexistent_template.html')\n"], "sample_92": ["    def setUpTestData(cls):\n        cls.user = User.objects.create_user('test', 'test@example.com', 'test')\n        cls.superuser = User.objects.create_superuser('super', 'super@example.com', 'super')\n", "    def setUpTestData(cls):\n        cls.user = User.objects.create_user('test', 'test@example.com', 'test')\n        cls.superuser = User.objects.create_superuser('super', 'super@example.com', 'super')\n", "    def setUpTestData(cls):\n        cls.user = User.objects.create_user('test', 'test@example.com', 'test')\n        cls.superuser = User.objects.create_superuser('super', 'super@example.com', 'super')\n", "    def setUpTestData(cls):\n        cls.user = User.objects.create_user('test', 'test@example.com', 'test')\n        cls.superuser = User.objects.create_superuser('super', 'super@example.com', 'super')\n", "    def setUpTestData(cls):\n        cls.user = User.objects.create_user('test', 'test@example.com', 'test')\n        cls.perm = Permission.objects.get(codename='add_user')\n"], "sample_93": ["def test_expression_with_duration_field(self):\n    duration_expr = F('duration')\n    # Make sure the expression knows its type.\n    self.assertIsInstance(duration_expr.output_field, DurationField)\n    # Make sure it's not trying to convert its value to a database literal.\n    self.assertEqual(duration_expr.as_sql(None, connection), ('\"duration\"', []))\n", "def test_exists_subquery(self):\n    subquery = Book.objects.filter(author__pk=OuterRef('pk'))\n    authors_with_books = Author.objects.filter(Exists(subquery))\n    self.assertQuerysetEqual(authors_with_books.order_by('pk'), [1, 2, 3, 4, 5, 6, 7, 8], transform=lambda x: x.pk)\n", "def test_exists_subquery(self):\n    qs = Book.objects.filter(\n        Exists(Author.objects.filter(books=OuterRef('pk')))\n    ).values_list('pk', flat=True)\n    self.assertEqual(list(qs), [self.b1.pk, self.b2.pk, self.b3.pk, self.b4.pk, self.b5.pk, self.b6.pk])\n    qs = Book.objects.filter(\n        ~Exists(Author.objects.filter(books=OuterRef('pk')))\n    ).values_list('pk', flat=True)\n    self.assertEqual(list(qs), [])\n", "def test_expression_with_duration_field(self):\n    duration_expr = F('duration')\n    self.assertEqual(duration_expr.output_field.__class__, DurationField)\n    self.assertEqual(duration_expr.as_sql(connection), ('\"myapp_publisher\".\"duration\"', []))\n", "def test_expression_in_filter(self):\n    expr = F('age') + 10\n    authors = Author.objects.filter(age__gte=expr)\n    sql, params = authors.query.sql_with_params()\n    self.assertIn('(age + 10)', sql)\n    self.assertEqual(authors.count(), 0)\n\n    authors = Author.objects.filter(age__lte=expr)\n    sql, params = authors.query.sql_with_params()\n    self.assertIn('(age + 10)', sql)\n    self.assertEqual(authors.count(), 9)\n"], "sample_94": ["def test_create_superuser(self):\n    with mock.patch('sys.stdin', MockTTY()):\n        call_command('createsuperuser', verbosity=0)\n        self.assertEqual(User.objects.count(), 1)\n        user = User.objects.get(username='alice')\n        self.assertTrue(user.is_superuser)\n        self.assertEqual(user.email, 'alice@example.com')\n", "def test_create_superuser_password_validation_bypass(self):\n    # Ensure password validation bypass works when creating a superuser.\n    with mock.patch.object(sys, 'stdin', MockTTY()):\n        call_command('createsuperuser', verbosity=0)\n        self.assertEqual(User.objects.filter(username='testuser').count(), 1)\n        user = User.objects.get(username='testuser')\n        self.assertTrue(user.is_superuser)\n", "def test_create_superuser_password_validation_bypass(self):\n    with mock.patch('sys.stdin', MockTTY()):\n        call_command('createsuperuser')\n        self.assertEqual(User.objects.filter(username='alice').count(), 1)\n", "def test_createsuperuser_validation_error(self):\n    with mock.patch.object(sys, 'stdin', MockTTY()):\n        with self.assertRaises(CommandError):\n            call_command('createsuperuser', '--noinput')\n", "def test_superuser_creation_with_invalid_password(self):\n    @mock_inputs({\n        'username': 'testuser',\n        'password': 'invalid',  # invalid password (too short)\n        'bypass': 'y'\n    })\n        with mock.patch('sys.stdin', MockTTY()):\n            call_command('createsuperuser', interactive=True, verbosity=0)\n            self.assertEqual(User.objects.filter(username='testuser').count(), 1)\n\n    test_func()\n"], "sample_98": ["    def setUpClass(cls):\n        cls.server_class = WSGIServer\n        super().setUpClass()\n", "    def setUpClass(cls):\n        cls.server_class = WSGIServer\n        super().setUpClass()\n", "    def setUpClass(cls):\n        super().setUpClass()\n        cls.server = WSGIServer(('localhost', 0), QuietWSGIRequestHandler)\n        cls.server_thread = LiveServerThread(cls.server)\n        cls.server_thread.daemon = True\n        cls.server_thread.start()\n", "    def setUpClass(cls):\n        cls.server_thread = LiveServerThread(\n            'localhost', 0, QuietWSGIRequestHandler,\n            allow_reuse_address=False,\n        )\n        cls.server_thread.daemon = True\n        cls.server_thread.start()\n        cls.server_thread.is_ready.wait()\n", "    def setUpClass(cls):\n        super().setUpClass()\n        cls.addr = '[::1]'\n        cls.port = 8000\n"], "sample_96": ["    def test_check_autocomplete_fields_item_not_a_foreign_key(self):\n        class MyModelAdmin(ModelAdmin):\n            autocomplete_fields = ['field1']\n\n        model = type('MyModel', (Model,), {'field1': Field()})\n\n        self.assertIsInvalid(\n            MyModelAdmin, model,\n            \"The value of 'autocomplete_fields[0]' must be a foreign key or a many-to-many field.\",\n            id='admin.E038',\n        )\n", "    def test_filter_vertical(self):\n        class ModelAdminTest(ModelAdmin):\n            filter_vertical = 'invalid'  # should be a list or tuple\n\n        self.assertIsInvalid(\n            ModelAdminTest, ValidationTestModel,\n            \"The value of 'filter_vertical' must be a list or tuple.\",\n            id='admin.E017',\n        )\n", "    def test_raw_id_fields_as_list(self):\n        class RawIdModelAdmin(ModelAdmin):\n            raw_id_fields = [f.name for f in Band._meta.get_fields() if isinstance(f, Field)]\n\n        self.assertIsValid(RawIdModelAdmin, Band)\n", "    def test_check_autocomplete_fields_item(self):\n        class MyModelAdmin(ModelAdmin):\n            autocomplete_fields = ['band']\n\n        self.assertIsInvalid(\n            MyModelAdmin, Song,\n            \"The value of 'autocomplete_fields[0]' must be a foreign key or a many-to-many field.\",\n            id='admin.E038'\n        )\n", "    def test_list_filter_with_tuple(self):\n        class MyModelAdmin(ModelAdmin):\n            list_filter = (('field', BooleanFieldListFilter),)\n\n        self.assertIsValid(MyModelAdmin, ValidationTestModel)\n"], "sample_99": ["    def test_trunc_subquery(self):\n        start_datetime = timezone.now()\n        model = self.create_model(start_datetime, start_datetime + timedelta(days=1))\n        qs = DTModel.objects.annotate(\n            trunc_start=Trunc('start_datetime', 'day', output_field=DateTimeField()),\n            trunc_end=Trunc('end_datetime', 'day', output_field=DateTimeField()),\n        ).filter(trunc_start=OuterRef('trunc_start'))\n        subquery = qs.values_list('trunc_end', flat=True).order_by('trunc_start')\n        DTModel.objects.annotate(\n            trunc_end=Subquery(subquery),\n        ).get(id=model.id)\n", "def test_trunc(self):\n    start_datetime = datetime(2015, 6, 15, 14, 30, 50, 321)\n    end_datetime = datetime(2018, 3, 1, 12, 5, 10, 999999)\n    obj = self.create_model(start_datetime, end_datetime)\n\n    # Truncate dates to years.\n    self.assertEqual(\n        DTModel.objects.filter(start_date=Trunc('start_date', 'year', output_field=DateField())).get(),\n        obj,\n    )\n    self.assertEqual(\n        DTModel.objects.filter(end_date=Trunc('end_date', 'year', output_field=DateField())).get(),\n        obj,\n    )\n\n    # Truncate dates to quarters.\n    self.assertEqual(\n        DTModel.objects.filter(start_date=Trunc('start_date', 'quarter', output_field=DateField())).get(),\n        obj,\n    )\n    self.assertEqual(\n        DTModel.objects.filter(end_date=Trunc('end_date', 'quarter', output_field=DateField())).get(),\n        obj,\n    )\n\n    # Truncate dates to months.\n    self.assertEqual(\n        DTModel.objects.filter(start_date=Trunc('start_date', 'month', output_field=DateField())).get(),\n        obj,\n    )\n    self.assertEqual(\n        DTModel.objects.filter(end_date=Trunc('end_date', 'month', output_field=DateField())).get(),\n        obj,\n    )\n\n    # Truncate dates to weeks.\n    self.assertEqual(\n        DTModel.objects.filter(start_date=Trunc('start_date', 'week', output_field=DateField())).get(),\n        obj,\n    )\n    self.assertEqual(\n        DTModel.objects.filter(end_date=Trunc('end_date', 'week', output_field=DateField())).get(),\n        obj,\n    )\n\n    # Truncate dates to days.\n    self.assertEqual(\n        DTModel.objects.filter(start_date=Trunc('start_date', 'day', output_field=DateField())).get(),\n        obj,\n    )\n    self.assertEqual(\n        DTModel.objects.filter(end_date=Trunc('end_date', 'day', output_field=DateField())).get(),\n        obj,\n    )\n", "    def test_trunc_subquery(self):\n        start_datetime = timezone.now()\n        self.create_model(start_datetime, start_datetime + timedelta(days=1))\n        qs = DTModel.objects.annotate(\n            start_date_truncated=Trunc('start_datetime', 'day', output_field=DateField()),\n            start_datetime_truncated=Trunc('start_datetime', 'day', output_field=DateTimeField()),\n        ).filter(\n            start_date_truncated=Subquery(\n                DTModel.objects.filter(\n                    start_datetime=start_datetime,\n                ).values('start_datetime')[:1]\n            ),\n            start_datetime_truncated=Subquery(\n                DTModel.objects.filter(\n                    start_datetime=start_datetime,\n                ).values('start_datetime')[:1]\n            ),\n        )\n        self.assertEqual(qs.count(), 1)\n", "def test_trunc_subquery(self):\n    start_datetime = datetime(2015, 6, 15, 14, 30, tzinfo=datetime_timezone.utc)\n    model = self.create_model(start_datetime, None)\n\n    subquery = DTModel.objects.annotate(\n        truncated=Trunc('start_datetime', 'day', output_field=DateTimeField()),\n    ).values('truncated')[:1]\n    dt = DTModel.objects.filter(\n        start_datetime=Subquery(subquery),\n    ).get()\n    self.assertEqual(dt, model)\n", "def test_trunc_with_tzinfo(self):\n    start_datetime = datetime(2015, 6, 15, 14, 30, tzinfo=pytz.timezone('America/New_York'))\n    end_datetime = datetime(2015, 6, 16, 14, 30, tzinfo=pytz.timezone('America/New_York'))\n    model = self.create_model(start_datetime, end_datetime)\n\n    # Test Trunc with tzinfo\n    self.assertEqual(\n        DTModel.objects.filter(start_datetime=Trunc('start_datetime', 'day', tzinfo=pytz.timezone('America/New_York'))).get(),\n        model\n    )\n    self.assertEqual(\n        DTModel.objects.filter(end_datetime=Trunc('end_datetime', 'day', tzinfo=pytz.timezone('America/New_York'))).get(),\n        model\n    )\n\n    # Test Trunc with tzinfo and output_field\n    self.assertEqual(\n        DTModel.objects.annotate(\n            start_date=Trunc('start_datetime', 'day', tzinfo=pytz.timezone('America/New_York'), output_field=DateField())\n        ).get().start_date,\n        start_datetime.date()\n    )\n    self.assertEqual(\n        DTModel.objects.annotate(\n            end_date=Trunc('end_datetime', 'day', tzinfo=pytz.timezone('America/New_York'), output_field=DateField())\n        ).get().end_date,\n        end_datetime.date()\n    )\n"], "sample_97": ["    def test_common_roots(self):\n        paths = [\n            Path('/path/to/common/root/file1.py'),\n            Path('/path/to/common/root/file2.py'),\n            Path('/path/to/common/root/subdir/file3.py'),\n        ]\n        common_roots = autoreload.common_roots(paths)\n        self.assertEqual(common_roots, (Path('/path/to/common/root'),))\n", "def test_iter_modules_and_files_zip_file(self):\n    with tempfile.TemporaryDirectory() as tmpdir:\n        zip_filename = Path(tmpdir) / 'test.zip'\n        with zipfile.ZipFile(zip_filename, 'w') as zip_file:\n            zip_file.writestr('test.py', b'')\n        sys.path.append(str(zip_filename))\n        self.import_and_cleanup('test')\n        self.assertFileFound(zip_filename)\n        sys.path.remove(str(zip_filename))\n", "def test_iter_modules_and_files_with_zip_file(self):\n    with tempfile.TemporaryDirectory() as tmpdir:\n        with extend_sys_path(tmpdir):\n            # Create a zip file with a Python module.\n            zip_filename = Path(tmpdir) / 'example.zip'\n            with zipfile.ZipFile(str(zip_filename), 'w') as zip_file:\n                zip_file.writestr('example.py', '')\n            self.import_and_cleanup('example')\n\n            # Check that the zip file is watched.\n            self.assertFileFound(zip_filename)\n\n            # Check that the zip file's contents are not watched separately.\n            zip_contents = Path('example.py')\n            self.assertFileNotFound(zip_contents)\n", "def test_iter_modules_and_files_zipfile(self):\n    with tempfile.NamedTemporaryFile(suffix='.zip') as tmp_file:\n        with zipfile.ZipFile(tmp_file.name, 'w') as zip_file:\n            zip_file.writestr('test.py', b'')\n        with extend_sys_path(tmp_file.name):\n            self.import_and_cleanup('test')\n            self.assertFileFound(Path(tmp_file.name))\n", "    def test_common_roots(self):\n        temp_dir = tempfile.mkdtemp()\n        self.addCleanup(shutil.rmtree, temp_dir)\n\n        # Create some files and directories\n        dir1 = Path(temp_dir) / 'dir1'\n        dir1.mkdir()\n        file1 = dir1 / 'file1.py'\n        file1.touch()\n\n        dir2 = Path(temp_dir) / 'dir2'\n        dir2.mkdir()\n        file2 = dir2 / 'file2.py'\n        file2.touch()\n\n        common_dir = Path(temp_dir)\n        files = [file1, file2]\n        roots = autoreload.common_roots(files)\n        self.assertEqual(roots, (common_dir,))\n"], "sample_100": ["def test_iter_modules_and_files_with_zip_importer(self):\n    with tempfile.NamedTemporaryFile(suffix='.zip', delete=False) as tmp_file:\n        zip_filename = Path(tmp_file.name)\n        self.addCleanup(zip_filename.unlink)\n\n        # Create a zip file containing a Python module.\n        with zipfile.ZipFile(tmp_file, 'w') as zip_file:\n            zip_file.writestr('mymodule.py', b'content = \"Hello World\"')\n\n        # Add the zip file to sys.path and reimport modules.\n        with extend_sys_path(str(zip_filename.parent)):\n            import mymodule\n            self.import_and_cleanup('mymodule')\n            self.assertFileFound(zip_filename)\n", "    def test_common_roots(self):\n        roots = autoreload.common_roots([Path('/path/to/file1.py'), Path('/path/to/file2.py')])\n        self.assertEqual(roots, (Path('/path/to'),))\n\n        roots = autoreload.common_roots([Path('/path/to/file1.py'), Path('/path/to/subdir/file2.py')])\n        self.assertEqual(roots, (Path('/path/to'),))\n\n        roots = autoreload.common_roots([Path('/path/to/file1.py'), Path('/other/path/to/file2.py')])\n        self.assertEqual(roots, (Path('/path/to'), Path('/other/path/to')))\n\n        roots = autoreload.common_roots([Path('/path/to/file1.py')])\n        self.assertEqual(roots, (Path('/path/to'),))\n", "def test_iter_modules_and_files_zip_file(self):\n    tmp_dir = tempfile.mkdtemp()\n    self.addCleanup(shutil.rmtree, tmp_dir)\n    filename = Path(tmp_dir) / 'myutils.zip'\n    with zipfile.ZipFile(str(filename), 'w') as f:\n        f.writestr('mymodule.py', b'')\n        f.writestr('data.dat', b'')\n    sys.path.append(str(filename))\n    self.addCleanup(sys.path.remove, str(filename))\n\n    import_module('mymodule')\n    self.addCleanup(lambda: sys.modules.pop('mymodule', None))\n\n    self.clear_autoreload_caches()\n    self.assertIn(filename, list(autoreload.iter_all_python_module_files()))\n", "def test_iter_modules_and_files_zip_import(self):\n    with tempfile.NamedTemporaryFile(suffix='.zip') as tmp_file:\n        with zipfile.ZipFile(tmp_file.name, 'w') as zip_file:\n            zip_file.writestr('test_module.py', '')\n        with extend_sys_path(tmp_file.name):\n            self.import_and_cleanup('test_module')\n            self.assertFileFound(Path(tmp_file.name))\n", "def test_iter_modules_and_files_with_zip_import(self):\n    with tempfile.TemporaryDirectory() as tmp_dir:\n        zip_filename = Path(tmp_dir) / 'test.zip'\n        with zipfile.ZipFile(str(zip_filename), 'w') as zip_file:\n            zip_file.writestr('test.py', b'')\n        sys.path.insert(0, str(zip_filename))\n        self.addCleanup(lambda: sys.path.remove(str(zip_filename)))\n        module_name = 'test'\n        self.import_and_cleanup(module_name)\n        self.assertFileFound(zip_filename)\n"], "sample_102": ["def test_union(self):\n    qs1 = Number.objects.filter(num__lte=3)\n    qs2 = Number.objects.filter(num__gt=7)\n    union_qs = qs1.union(qs2)\n    self.assertNumbersEqual(union_qs, [0, 1, 2, 3, 8, 9], ordered=False)\n\n    # The second queryset can contain an overlapping row.\n    qs1 = Number.objects.filter(num__lte=3)\n    qs2 = Number.objects.filter(num__gte=2)\n    union_qs = qs1.union(qs2)\n    self.assertNumbersEqual(union_qs, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], ordered=False)\n\n    # The union operation may require reordering.\n    qs1 = Number.objects.filter(num__lte=3).order_by('num')\n    qs2 = Number.objects.filter(num__gt=7).order_by('-num')\n    union_qs = qs1.union(qs2)\n    self.assertNumbersEqual(union_qs, [0, 1, 2, 3, 9, 8], ordered=True)\n", "def test_union(self):\n    qs1 = Number.objects.filter(num__lte=5)\n    qs2 = Number.objects.filter(num__gt=5)\n    self.assertNumbersEqual(qs1.union(qs2), list(range(10)))\n", "def test_union(self):\n    qs1 = Number.objects.filter(num__lte=5)\n    qs2 = Number.objects.filter(num__gte=8)\n    union_qs = qs1.union(qs2)\n    self.assertNumbersEqual(union_qs, [0, 1, 2, 3, 4, 5, 8, 9], ordered=False)\n", "def test_union(self):\n    qs1 = Number.objects.filter(num__lte=3)\n    qs2 = Number.objects.filter(num__gt=7)\n\n    union_qs = qs1.union(qs2)\n    self.assertNumbersEqual(union_qs, [0, 1, 2, 3, 8, 9], ordered=False)\n\n    # The union operator is the only set operator that can take more than two\n    # QuerySets.\n    qs3 = Number.objects.filter(num=4)\n    union_qs = qs1.union(qs2, qs3)\n    self.assertNumbersEqual(union_qs, [0, 1, 2, 3, 4, 8, 9], ordered=False)\n", "def test_union(self):\n    qs1 = Number.objects.filter(num__lte=5)\n    qs2 = Number.objects.filter(num__gt=5)\n    union_qs = qs1.union(qs2)\n    self.assertNumbersEqual(union_qs, range(10))\n"], "sample_101": ["def test_wsgi_request_get_script_name(self):\n    environ = {\n        'SCRIPT_URL': '/path/to/script',\n        'PATH_INFO': '/path/to/info',\n    }\n    request = self.request_factory.generic('GET', '/', environ=environ)\n    self.assertEqual(request.path_info, '/path/to/info')\n    self.assertEqual(request.path, '/path/to/script/path/to/info')\n\n    # Test FORCE_SCRIPT_NAME setting\n    with override_settings(FORCE_SCRIPT_NAME='/forced/script/name'):\n        request = self.request_factory.generic('GET', '/', environ=environ)\n        self.assertEqual(request.path_info, '/path/to/info')\n        self.assertEqual(request.path, '/forced/script/name/path/to/info')\n", "def test_limited_stream(self):\n    stream = BytesIO(b'Hello, World!')\n    limited_stream = LimitedStream(stream, 5)\n    self.assertEqual(limited_stream.read(), b'Hello')\n    self.assertEqual(limited_stream.read(), b'')\n", "def test_limited_stream(self):\n    # Test that LimitedStream does not read more than its limit.\n    stream = BytesIO(b'Hello, World!')\n    limited_stream = LimitedStream(stream, 5)\n    self.assertEqual(limited_stream.read(), b'Hello')\n    self.assertEqual(limited_stream.read(), b'')\n", "def test_wsgi_request_script_name(self):\n    environ = {\n        'PATH_INFO': '/path/info/',\n        'SCRIPT_NAME': '/script/name/',\n        'REQUEST_METHOD': 'GET',\n        'wsgi.url_scheme': 'http',\n    }\n    request = get_internal_wsgi_application()(environ)\n    self.assertEqual(request.path_info, '/path/info/')\n    self.assertEqual(request.path, '/script/name/path/info/')\n\n    # Test FORCE_SCRIPT_NAME setting\n    with override_settings(FORCE_SCRIPT_NAME='/forced/script/name/'):\n        request = get_internal_wsgi_application()(environ)\n        self.assertEqual(request.path_info, '/path/info/')\n        self.assertEqual(request.path, '/forced/script/name/path/info/')\n", "def test_wsgi_request_path_info(self):\n    environ = {'PATH_INFO': '/test/path', 'SCRIPT_NAME': '/script'}\n    request = self.request_factory.generic('GET', '/', environ=environ)\n    self.assertEqual(request.path_info, '/test/path')\n    self.assertEqual(request.path, '/script/test/path')\n\n    # Test empty PATH_INFO\n    environ['PATH_INFO'] = ''\n    request = self.request_factory.generic('GET', '/', environ=environ)\n    self.assertEqual(request.path_info, '/')\n    self.assertEqual(request.path, '/script/')\n"], "sample_103": ["def test_aggregate_with_filter_clause(self):\n    # Test that aggregates with a filter clause are correctly generated.\n    books = Book.objects.annotate(\n        num_authors=Count('authors', filter=Q(authors__name='Adrian Holovaty'))\n    )\n    self.assertEqual(books[0].num_authors, 1)\n    self.assertEqual(books[1].num_authors, 0)\n\n    # Test that the filter clause is correctly applied when using an alias.\n    books = Book.objects.annotate(\n        num_authors=Count('authors', filter=Q(authors__name='Adrian Holovaty'))\n    ).filter(num_authors__gt=0)\n    self.assertEqual(len(books), 1)\n\n    # Test that the filter clause is correctly applied when using a subquery.\n    books = Book.objects.annotate(\n        num_authors=Count('authors', filter=Q(authors__in=Author.objects.filter(name='Adrian Holovaty')))\n    )\n    self.assertEqual(books[0].num_authors, 1)\n    self.assertEqual(books[1].num_authors, 0)\n", "def test_stddev(self):\n    # Create some sample data\n    Book.objects.create(pages=100, rating=3.0)\n    Book.objects.create(pages=200, rating=4.0)\n    Book.objects.create(pages=300, rating=5.0)\n\n    # Calculate the standard deviation of page counts and ratings\n    stddev_pages = Book.objects.aggregate(stddev_pages=StdDev('pages'))['stddev_pages']\n    stddev_rating = Book.objects.aggregate(stddev_rating=StdDev('rating'))['stddev_rating']\n\n    # Check that the results are correct (approximately)\n    self.assertAlmostEqual(stddev_pages, 81.64965809277261)\n    self.assertAlmostEqual(stddev_rating, 0.8164965809277261)\n\n    # Test with a filter\n    stddev_filtered = Book.objects.filter(pages__gt=150).aggregate(stddev_pages=StdDev('pages'))['stddev_pages']\n    self.assertAlmostEqual(stddev_filtered, 50.0)\n", "def test_stddev(self):\n    # Create some sample data with decimal values to ensure accurate calculations.\n    Book.objects.create(isbn='123456789', pages=300, rating=3.5, price=Decimal('20.00'))\n    Book.objects.create(isbn='987654321', pages=400, rating=4.2, price=Decimal('30.00'))\n    \n    # Calculate standard deviation using StdDev function and built-in list std dev calculation for comparison.\n    sample_std_dev = Book.objects.aggregate(std_dev_sample=StdDev('pages', sample=True))['std_dev_sample']\n    population_std_dev = Book.objects.aggregate(std_dev_population=StdDev('pages'))['std_dev_population']\n    pages = [book.pages for book in Book.objects.all()]\n    expected_sample_std_dev = (sum((x - sum(pages) / len(pages)) ** 2 for x in pages) / (len(pages) - 1)) ** 0.5\n    expected_population_std_dev = (sum((x - sum(pages) / len(pages)) ** 2 for x in pages) / len(pages)) ** 0.5\n    \n    # Use Approximate assertions due to potential floating point precision errors.\n    self.assertAlmostEqual(sample_std_dev, expected_sample_std_dev)\n    self.assertAlmostEqual(population_std_dev, expected_population_std_dev)\n", "def test_aggregate_filter_clause(self):\n    # Test that the filter clause is properly applied to the aggregate\n    # when the database supports it.\n    with CaptureQueriesContext(connection) as captured_queries:\n        Book.objects.aggregate(Sum('pages', filter=Q(rating__gt=4)))\n        query = captured_queries[0]['sql']\n        self.assertIn('FILTER (WHERE', query)\n        self.assertIn('rating > 4', query)\n\n    # Test that the filter clause is not applied when the database doesn't support it.\n    with connection.features.supports_aggregate_filter_clause.override(False):\n        with CaptureQueriesContext(connection) as captured_queries:\n            Book.objects.aggregate(Sum('pages', filter=Q(rating__gt=4)))\n            query = captured_queries[0]['sql']\n            self.assertNotIn('FILTER (WHERE', query)\n            self.assertIn('CASE WHEN', query)\n", "def test_aggregate_filter_clause(self):\n    with CaptureQueriesContext(connection) as captured_queries:\n        Book.objects.aggregate(\n            Count('id', filter=Q(rating__gt=3.0)),\n            Avg('price', filter=Q(pages__lt=500)),\n        )\n        query = captured_queries[0]['sql']\n        self.assertRegex(query, r'COUNT\\(\"id\"\\) FILTER \\(WHERE \"rating\" > 3\\.0\\) AS \"id__count\"')\n        self.assertRegex(query, r'AVG\\(\"price\"\\) FILTER \\(WHERE \"pages\" < 500\\) AS \"price__avg\"')\n"], "sample_104": ["    def test_url(self):\n        \"\"\"\n        Test that StaticFilesStorage doesn't raise an error when it encounters a\n        file that has been removed after the storage was initialized.\n        \"\"\"\n        self.addCleanup(shutil.rmtree, settings.STATIC_ROOT)\n        call_command('collectstatic', verbosity=0, interactive=False)\n\n            return storage.url(name)\n\n        storage = storage.staticfiles_storage\n        with mock.patch.object(storage, 'url', side_effect=get_url):\n            # Simulate a removed file\n            os.remove(os.path.join(settings.STATIC_ROOT, 'test/file.txt'))\n            try:\n                # Should not raise an error\n                storage.post_process(['test/file.txt'], dry_run=True)\n            except ValueError:\n                self.fail(\"post_process raised an exception unexpectedly.\")\n", "    def test_post_process(self):\n        # Make sure the manifest is empty before we start.\n        self.assertEqual(storage.staticfiles_storage.hashed_files, {})\n\n        # Run post_process on a test file.\n        test_file_path = os.path.join(TEST_ROOT, 'project', 'css', 'test.css')\n        storage.staticfiles_storage.post_process([test_file_path], dry_run=False)\n\n        # Check that the manifest contains the hashed file path.\n        hashed_path = hashed_file_path(self, test_file_path)\n        self.assertIn(hashed_path, storage.staticfiles_storage.hashed_files.values())\n        self.assertPostCondition()\n", "    def test_hashed_name(self):\n        original_path = 'path/to/file.txt'\n        hashed_path = storage.staticfiles_storage.hashed_name(original_path)\n        self.assertNotEqual(original_path, hashed_path)\n        self.assertTrue(hashed_path.startswith('path/to/'))\n        self.assertTrue(hashed_path.endswith('.txt'))\n", "    def test_post_process(self):\n        # Make sure that we don't try to process files that don't exist.\n        storage.staticfiles_storage.hashed_files.clear()\n        storage.staticfiles_storage.max_post_process_passes = 1\n\n        with self.assertRaises(ValueError):\n            storage.staticfiles_storage.post_process({\n                'nonexistentfile.js': ('storage', '/path/to/nonexistentfile.js'),\n            })\n\n        # Make sure we don't generate hashes for directories.\n        self.assertEqual(\n            list(storage.staticfiles_storage.post_process({\n                'subdir': ('storage', '/path/to/subdir'),\n            })),\n            [('subdir', None, False)],\n        )\n\n        # Make sure that we don't try to hash the same file more than once.\n        hashed_files = {\n            'subdir/example.txt': ('storage', '/path/to/subdir/example.txt'),\n        }\n        self.assertEqual(\n            list(storage.staticfiles_storage.post_process(hashed_files)),\n            [(key, value[0] + '/subdir/example.6c8d2eb1.txt', True) for key, value in hashed_files.items()],\n        )\n        self.assertEqual(\n            list(storage.staticfiles_storage.post_process(hashed_files)),\n            [(key, value[0] + '/subdir/example.6c8d2eb1.txt', False) for key, value in hashed_files.items()],\n        )\n", "    def test_hashed_name(self):\n        path = 'path/to/file.txt'\n        hashed_name = storage.staticfiles_storage.hashed_name(path)\n        self.assertEqual(hashed_name, path)\n\n        with tempfile.NamedTemporaryFile() as tmp_file:\n            tmp_file.write(b'Hello, world!')\n            tmp_file.flush()\n            hashed_name = storage.staticfiles_storage.hashed_name(path, content=tmp_file)\n            self.assertNotEqual(hashed_name, path)\n            self.assertTrue(hashed_name.startswith('path/to/'))\n            self.assertTrue(hashed_name.endswith('.txt'))\n"], "sample_107": ["    def test_get_post_parameters(self):\n        request = RequestFactory().post('/test/', {'foo': 'bar'})\n        filter = ExceptionReporterFilter()\n        self.assertEqual(filter.get_post_parameters(request), {'foo': 'bar'})\n", "    def test_cleanse_setting(self):\n        self.assertEqual(cleanse_setting('ALLOWED_HOSTS', ['test1', 'test2']), ['test1', 'test2'])\n        self.assertEqual(cleanse_setting('API_KEY', 'secret_key'), CLEANSED_SUBSTITUTE)\n        self.assertEqual(cleanse_setting('DATABASES', {'default': {'PASSWORD': 'secret'}}), {'default': {'PASSWORD': CLEANSED_SUBSTITUTE}})\n        self.assertEqual(cleanse_setting('SECRET_KEY', 'secret_key'), CLEANSED_SUBSTITUTE)\n        self.assertEqual(cleanse_setting('SOME_SETTING', 'some_value'), 'some_value')\n", "    def test_cleanse_setting(self):\n        self.assertEqual(cleanse_setting('API_KEY', 'secret'), CLEANSED_SUBSTITUTE)\n        self.assertEqual(cleanse_setting('ALLOWED_HOSTS', ['example.com']), ['example.com'])\n        self.assertEqual(cleanse_setting('DATABASES', {'default': {'ENGINE': 'django.db.backends.sqlite3'}}), {'default': {'ENGINE': 'django.db.backends.sqlite3'}})\n        self.assertEqual(cleanse_setting('SECRET_KEY', 'secret'), CLEANSED_SUBSTITUTE)\n        self.assertEqual(cleanse_setting('SOME_SETTING', 'public'), 'public')\n", "    def test_cleanse_setting(self):\n        self.assertEqual(cleanse_setting('ALLOWED_HOSTS', ['example.com']), ['example.com'])\n        self.assertEqual(cleanse_setting('DATABASES', {'default': {'PASSWORD': 'password'}}), {'default': {'PASSWORD': CLEANSED_SUBSTITUTE}})\n        self.assertEqual(cleanse_setting('SECRET_KEY', 'secret_key'), CLEANSED_SUBSTITUTE)\n        self.assertEqual(cleanse_setting('NON_SENSITIVE_SETTING', 'value'), 'value')\n", "    def test_get_post_parameters(self):\n        request = RequestFactory().post('/someurl/', {'foo': 'bar'})\n        filter = ExceptionReporterFilter()\n        self.assertEqual(filter.get_post_parameters(request), {'foo': 'bar'})\n"], "sample_106": ["    def test_patch_cache_control(self):\n        response = HttpResponse()\n        patch_cache_control(response, max_age=3600)\n        self.assertEqual(response['Cache-Control'], 'max-age=3600')\n", "    def test_patch_cache_control(self):\n        response = HttpResponse()\n        patch_cache_control(response, max_age=3600)\n        self.assertEqual(response['Cache-Control'], 'max-age=3600')\n", "    def test_patch_vary_headers(self):\n        response = HttpResponse()\n        patch_vary_headers(response, ['Accept-Language', 'Cookie'])\n        self.assertEqual(response['Vary'], 'Accept-Language, Cookie')\n", "    def test_generate_cache_key(self):\n        request = HttpRequest()\n        request.META['HTTP_ACCEPT_LANGUAGE'] = 'fr'\n        request.path = '/somepath/'\n        key_prefix = 'testkey'\n        cache_key = learn_cache_key(request, HttpResponse(), key_prefix=key_prefix)\n        self.assertIn(key_prefix, cache_key)\n        self.assertIn('views.decorators.cache.cache_page', cache_key)\n", "    def test_get_cache_key(self):\n        request = HttpRequest()\n        request.path = '/path/to/page'\n        request.method = 'GET'\n        key_prefix = 'test-prefix'\n\n        # Test that get_cache_key returns None if there's no headerlist stored\n        cache_key = get_cache_key(request, key_prefix=key_prefix)\n        self.assertIsNone(cache_key)\n\n        # Test that get_cache_key returns a cache key if there's a headerlist stored\n        cache.set(_generate_cache_header_key(key_prefix, request), ['HTTP_ACCEPT_LANGUAGE'])\n        cache_key = get_cache_key(request, key_prefix=key_prefix)\n        self.assertIsNotNone(cache_key)\n"], "sample_105": ["def test_options(self):\n    request = self.rf.options('/simple/')\n    response = SimpleView.as_view()(request)\n    self.assertEqual(response.status_code, 200)\n    self.assertIn('Allow', response)\n    self.assertEqual(response['Allow'], 'GET, HEAD, OPTIONS')\n", "def test_as_view_passes_kwargs_to_init(self):\n    class ViewWithInit(View):\n            self.init_kwargs = kwargs\n            super().__init__()\n\n            return HttpResponse('')\n\n    view = ViewWithInit.as_view(a='b', c='d')\n    response = view(self.rf.get('/'), a='overridden')\n    self.assertEqual(response.status_code, 200)\n    self.assertEqual(view.view_initkwargs, {'a': 'b', 'c': 'd'})\n", "def test_as_view(self):\n    view = View.as_view()\n    self.assertTrue(hasattr(view, 'view_class'))\n    self.assertEqual(view.view_class, View)\n    self.assertTrue(hasattr(view, 'view_initkwargs'))\n    self.assertEqual(view.view_initkwargs, {})\n\n    # Test that arguments are preserved\n    CustomizableView.parameter = {'test': True}\n    custom_view = CustomizableView.as_view(parameter={'test': True})\n    self.assertEqual(custom_view.view_initkwargs, {'parameter': {'test': True}})\n", "def test_redirect_view(self):\n    class MyRedirectView(RedirectView):\n        url = '/redirected/'\n\n    request = self.rf.get('/original/')\n    view = MyRedirectView.as_view()\n    response = view(request)\n    self.assertEqual(response.status_code, 302)\n    self.assertEqual(response.url, '/redirected/')\n\n    class MyPermanentRedirectView(RedirectView):\n        url = '/permanent/'\n        permanent = True\n\n    view = MyPermanentRedirectView.as_view()\n    response = view(request)\n    self.assertEqual(response.status_code, 301)\n    self.assertEqual(response.url, '/permanent/')\n\n    class MyPatternNameRedirectView(RedirectView):\n        pattern_name = 'redirected'\n\n    with override_settings(ROOT_URLCONF='django.urls'):\n        view = MyPatternNameRedirectView.as_view()\n        response = view(request)\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response.url, '/redirected/')\n\n    class MyQuerystringRedirectView(RedirectView):\n        url = '/redirected/'\n        query_string = True\n\n    request = self.rf.get('/original/', {'a': 'b'})\n    view = MyQuerystringRedirectView.as_view()\n    response = view(request)\n    self.assertEqual(response.status_code, 302)\n    self.assertEqual(response.url, '/redirected/?a=b')\n", "def test_view_as_view(self):\n    view = SimpleView.as_view()\n    self.assertEqual(view.__name__, 'SimpleView')\n    self.assertEqual(view.__doc__, \"A simple view with a docstring.\")\n    response = view(self.rf.get('/'), 42)\n    self.assertEqual(response.status_code, 200)\n    self.assertEqual(response.content, b'This is a simple view')\n    self.assertEqual(view.view_class, SimpleView)\n    self.assertEqual(view.view_initkwargs, {})\n"], "sample_108": ["def test_path_with_route_starting_with_slash(self):\n    msg = (\n        \"Your URL pattern '/test/' has a route beginning with a '/'. Remove this \"\n        \"slash as it is unnecessary. If this pattern is targeted in an \"\n        \"include(), ensure the include() pattern has a trailing '/'\"\n    )\n    with self.assertRaisesMessage(Warning, msg):\n        path('/test/', empty_view)\n", "def test_route_pattern(self):\n    route_pattern = RoutePattern(route='path/<int:pk>/', name='route-pattern', is_endpoint=True)\n    self.assertEqual(str(route_pattern), 'path/<int:pk>/')\n    self.assertEqual(route_pattern.regex.pattern, '^path/(?P<pk>[0-9]+)/$')\n    self.assertEqual(route_pattern.converters, {'pk': DynamicConverter})\n    self.assertEqual(route_pattern.match('/path/123/'), ('', (), {'pk': 123}))\n    self.assertIsNone(route_pattern.match('/path/not-a-number/'))\n", "def test_url_resolver_resolve_with_namespace(self):\n    resolver = get_resolver()\n    match = resolver.resolve('/namespace/url/')\n    self.assertEqual(match.func, empty_view)\n    self.assertEqual(match.namespace, 'namespace')\n    self.assertEqual(match.url_name, 'namespaced-url')\n", "def test_path_value_errors(self):\n    msg = \"URL route '%s' cannot contain whitespace.\"\n    with self.assertRaisesMessage(ImproperlyConfigured, msg % 'path /'):\n        path('path /', empty_view)\n    with self.assertRaisesMessage(ImproperlyConfigured, msg % '/path/ '):\n        path('/path/ ', empty_view)\n    with self.assertRaisesMessage(ImproperlyConfigured, msg % 'path /path/'):\n        path('path /path/', empty_view)\n    msg = \"URL route '%s' is invalid. It must be a string or an iterable of strings.\"\n    with self.assertRaisesMessage(ImproperlyConfigured, msg % 123):\n        path(123, empty_view)\n", "def test_route_to_regex(self):\n    route = 'users/<int:user_id>/posts/<slug:post_slug>/'\n    regex, converters = _route_to_regex(route)\n    self.assertEqual(regex, '^users/(?P<user_id>[0-9]+)/posts/(?P<post_slug>[-a-zA-Z0-9_]+)/')\n    self.assertEqual(converters, {'user_id': <class 'django.urls.converters.IntConverter'>, 'post_slug': <class 'django.urls.converters.SlugConverter'>})\n"], "sample_109": ["def test_autocomplete_select_widget_with_to_field(self):\n    widget = AutocompleteSelect(\n        Album._meta.get_field('band').remote_field,\n        admin.site,\n        attrs={'class': 'my-class'},\n    )\n    self.assertEqual(widget.url_name, 'admin:%s_%s_autocomplete')\n", "def test_autocomplete_mixin_build_attrs(self):\n    form = AlbumForm()\n    field_name = 'band'\n    widget = form.fields[field_name].widget\n    attrs = widget.build_attrs({'id': 'id_band'}, extra_attrs={'class': 'extra-class'})\n    self.assertEqual(attrs['data-ajax--cache'], 'true')\n    self.assertEqual(attrs['data-ajax--delay'], 250)\n    self.assertEqual(attrs['data-ajax--type'], 'GET')\n    self.assertIn('admin-autocomplete', attrs['class'])\n    self.assertIn('my-class', attrs['class'])\n    self.assertIn('extra-class', attrs['class'])\n    with translation.override('fr'):\n        attrs = widget.build_attrs({'id': 'id_band'})\n        self.assertEqual(attrs['data-theme'], 'admin-autocomplete')\n", "def test_autocomplete_mixin_build_attrs(self):\n    form = AlbumForm()\n    band_widget = form.fields['band'].widget\n    attrs = band_widget.build_attrs(base_attrs={'id': 'my-id'}, extra_attrs={'class': 'extra-class'})\n    self.assertEqual(attrs['data-ajax--cache'], 'true')\n    self.assertEqual(attrs['data-ajax--delay'], 250)\n    self.assertEqual(attrs['data-ajax--type'], 'GET')\n    self.assertEqual(attrs['data-theme'], 'admin-autocomplete')\n    self.assertEqual(attrs['data-allow-clear'], 'false')  # Required field.\n    self.assertEqual(attrs['data-placeholder'], '')\n    self.assertEqual(attrs['class'], 'my-class extra-class admin-autocomplete')\n", "def test_autocomplete_select_widget_required(self):\n    form = RequiredBandForm()\n    self.assertHTMLEqual(\n        form.as_p(),\n        '<p>Band: <select class=\"admin-autocomplete\" data-ajax--cache=\"true\" '\n        'data-ajax--delay=\"250\" data-ajax--type=\"GET\" data-ajax--url=\"/autocomplete/album/band/\" '\n        'data-allow-clear=\"false\" data-placeholder=\"\" data-theme=\"admin-autocomplete\" '\n        'name=\"band\" required>\\n'\n        + self.empty_option +\n        '\\n</select></p>'\n    )\n", "def test_autocomplete_select_render(self):\n    form = AlbumForm()\n    band_field = form.fields['band']\n    self.assertHTMLEqual(\n        band_field.widget.render('band', ''),\n        '<select data-ajax--cache=\"true\" data-ajax--delay=\"250\" data-ajax--type=\"GET\" '\n        'data-ajax--url=\"/autocomplete/albums/band/\" data-theme=\"admin-autocomplete\" '\n        'data-allow-clear=\"false\" data-placeholder=\"\" class=\"my-class admin-autocomplete\" '\n        'name=\"band\">\\n' + self.empty_option + '\\n</select>'\n    )\n"], "sample_111": ["def test_get_ordering_field(self):\n    request = self._mocked_authenticated_request('/admin/', self.superuser)\n    model_admin = BandAdmin(Band, custom_site)\n    changelist = ChangeList(request, Band, model_admin.list_display, model_admin.list_display_links,\n                            model_admin.list_filter, model_admin.date_hierarchy, model_admin.search_fields,\n                            model_admin.list_select_related, model_admin.list_per_page,\n                            model_admin.list_max_show_all, model_admin.list_editable, model_admin, None)\n\n    # Test with a field name\n    field_name = 'name'\n    self.assertEqual(changelist.get_ordering_field(field_name), field_name)\n\n    # Test with a callable\n        return obj.name\n\n    admin_order_field.admin_order_field = 'name'\n    self.assertEqual(changelist.get_ordering_field(admin_order_field), 'name')\n\n    # Test with a property\n    class FakeModel:\n        @property\n            return 'my_property'\n\n    fake_model = FakeModel()\n    fake_model.my_property.admin_order_field = 'name'\n    self.assertEqual(changelist.get_ordering_field(fake_model.my_property), 'name')\n", "def test_get_ordering_field_columns(self):\n    model_admin = BandAdmin(Band, custom_site)\n    request = self._mocked_authenticated_request('/admin/band/', self.superuser)\n    cl = ChangeList(request, Band, model_admin.list_display, model_admin.list_display_links,\n                    model_admin.list_filter, model_admin.date_hierarchy, model_admin.search_fields,\n                    model_admin.list_select_related, model_admin.list_per_page, model_admin.list_max_show_all,\n                    model_admin.list_editable, model_admin=model_admin, sortable_by=model_admin.sortable_by)\n    columns = cl.get_ordering_field_columns()\n    self.assertEqual(columns, {0: 'asc'})  # By default it's ordered by the first column\n    request.GET = {'o': '-1'}\n    cl = ChangeList(request, Band, model_admin.list_display, model_admin.list_display_links,\n                    model_admin.list_filter, model_admin.date_hierarchy, model_admin.search_fields,\n                    model_admin.list_select_related, model_admin.list_per_page, model_admin.list_max_show_all,\n                    model_admin.list_editable, model_admin=model_admin, sortable_by=model_admin.sortable_by)\n    columns = cl.get_ordering_field_columns()\n    self.assertEqual(columns, {1: 'desc'})\n", "def test_get_filters_params(self):\n    request = self._mocked_authenticated_request('/admin/auth/user/', self.superuser)\n    request.GET = {'o': '1', 'ot': 'asc', 'q': 'test', 'p': '1'}\n    changelist = ChangeList(request, User, ['username'], [], [], [], [], 100, 1000)\n    params = changelist.get_filters_params()\n    self.assertEqual(params, {})\n", "def test_get_ordering_field(self):\n    request = self._mocked_authenticated_request('/admin/', self.superuser)\n    changelist = ChangeList(request, Band, list_display=['name'], list_display_links=None,\n                            list_filter=None, date_hierarchy=None, search_fields=None,\n                            list_select_related=None, list_per_page=100, list_max_show_all=200,\n                            list_editable=None, model_admin=BandAdmin, sortable_by=None)\n\n    # Test with a field name.\n    self.assertEqual(changelist.get_ordering_field('name'), 'name')\n\n    # Test with a callable that has the admin_order_field attribute.\n        pass\n    my_callable.admin_order_field = 'name'\n    self.assertEqual(changelist.get_ordering_field(my_callable), 'name')\n\n    # Test with a property that has the admin_order_field attribute.\n    class MyModel(models.Model):\n        @property\n            pass\n        my_property.fget.admin_order_field = 'name'\n    self.assertEqual(changelist.get_ordering_field(MyModel.my_property.fget), 'name')\n", "def test_get_ordering_field(self):\n    model_admin = BandAdmin(Band, admin.site)\n    request = self._mocked_authenticated_request('/admin/bands/', self.superuser)\n    changelist = ChangeList(request, Band, [], [], [], [], [], [], [], model_admin=model_admin, sortable_by=[])\n\n    # Test with a field name\n    self.assertEqual(changelist.get_ordering_field('name'), 'name')\n\n    # Test with a callable\n        return obj.name\n    callable_field.admin_order_field = 'name'\n    self.assertEqual(changelist.get_ordering_field(callable_field), 'name')\n\n    # Test with a property\n    class BandProperty:\n        @property\n            return self.name\n        prop.fget.admin_order_field = 'name'\n\n    self.assertEqual(changelist.get_ordering_field(BandProperty.prop.fget), 'name')\n"], "sample_110": ["    def test_resolve_expression(self):\n        # Create a F expression\n        expr = F('name')\n\n        # Resolve the expression\n        resolved_expr = expr.resolve_expression(query=None)\n\n        self.assertEqual(resolved_expr, expr)\n", "    def setUpTestData(cls):\n        cls.h1 = Happening.objects.create(name='The Big Event')\n        cls.h2 = Happening.objects.create(name='The Small Event')\n", "    def setUpTestData(cls):\n        cls.e1 = Event.objects.create(name='Event 1', date=datetime.date(2015, 1, 1))\n        cls.e2 = Event.objects.create(name='Event 2', date=datetime.date(2015, 1, 2))\n", "    def test_add(self):\n        e1 = models.F('a')\n        e2 = models.F('b')\n        combined = e1 + e2\n        self.assertEqual(str(combined), \"F(a) + F(b)\")\n        self.assertIsInstance(combined, models.CombinedExpression)\n        self.assertEqual(combined.output_field, None)\n", "    def test_resolve_expression(self):\n        # Create a simple expression\n        expression = models.F('name')\n\n        # Resolve the expression\n        resolved_expression = expression.resolve_expression(None)\n\n        # Check that the resolved expression is the same as the original\n        self.assertEqual(resolved_expression, expression)\n"], "sample_112": ["def test_prepopulated_fields_js(self):\n    adminform = ModelAdmin(Article, site).get_form(request=self.request_factory.get('/'))()\n    inline_admin_formset = ModelAdmin(Question, site).get_inline_formsets(request=self.request_factory.get('/'), obj=None)[0](instance=None)\n    context = {\n        'adminform': adminform,\n        'inline_admin_formsets': [inline_admin_formset],\n    }\n    context = prepopulated_fields_js(context)\n    self.assertIn('prepopulated_fields_json', context)\n    self.assertIsInstance(context['prepopulated_fields_json'], str)\n    self.assertEqual(context['prepopulated_fields'], [])\n", "def test_prepopulated_fields_js(self):\n    adminform = ArticleAdmin(Article, site).get_form(request=self.request_factory.get('/'))()\n    context = {\n        'adminform': adminform,\n        'inline_admin_formsets': [],\n    }\n    context = prepopulated_fields_js(context)\n    self.assertEqual(context['prepopulated_fields'], [])\n    self.assertEqual(context['prepopulated_fields_json'], '[]')\n", "def test_prepopulated_fields_js(self):\n    # Create a mock admin form and inline admin formsets\n    admin_form = ModelAdmin(Article, site).get_form(None)\n    inline_admin_formset = ModelAdmin(Question, site).get_inline_formsets(None)[0]\n\n    # Create a context with the admin form and inline admin formset\n    context = {\n        'adminform': admin_form,\n        'inline_admin_formsets': [inline_admin_formset],\n    }\n\n    # Call the prepopulated_fields_js function\n    context = prepopulated_fields_js(context)\n\n    # Check that the context has been updated with prepopulated fields\n    self.assertIn('prepopulated_fields', context)\n    self.assertIn('prepopulated_fields_json', context)\n\n    # Check that the prepopulated fields JSON is valid\n    try:\n        json.loads(context['prepopulated_fields_json'])\n    except json.JSONDecodeError:\n        self.fail(\"Invalid JSON for prepopulated fields\")\n", "def test_prepopulated_fields_js(self):\n    # Set up a request and admin form\n    request = self.request_factory.get(reverse('admin:admin_test_article_add'))\n    admin_form = ArticleAdmin(Article, site).get_form(request)()\n\n    # Create a context with the admin form and inline admin formsets\n    context = {\n        'adminform': admin_form,\n        'inline_admin_formsets': [],\n    }\n\n    # Call the prepopulated_fields_js function\n    context = prepopulated_fields_js(context)\n\n    # Check that the prepopulated fields JSON is correctly generated\n    self.assertIn('prepopulated_fields_json', context)\n    self.assertIsInstance(context['prepopulated_fields_json'], str)\n", "def test_prepopulated_fields_js(self):\n    request = self.request_factory.get(reverse('admin:auth_user_add'))\n    adminform = UserAdmin(User, site).get_form(request)()\n    context = {\n        'adminform': adminform,\n        'add': True,\n        'change': False,\n        'is_popup': False,\n        'save_as': False,\n        'has_add_permission': True,\n        'has_change_permission': False,\n        'has_view_permission': True,\n        'has_delete_permission': False,\n        'has_editable_inline_admin_formsets': False,\n    }\n    context = prepopulated_fields_js(context)\n    self.assertEqual(context['prepopulated_fields'], [])\n    self.assertEqual(context['prepopulated_fields_json'], '[]')\n"], "sample_113": ["def test_replace_named_groups(self):\n    pattern = r'^(?P<a>\\w+)/b/(?P<c>\\w+)/$'\n    expected = '^<a>/b/<c>/$'\n    self.assertEqual(utils.replace_named_groups(pattern), expected)\n\n    pattern = r'^(?P<a>\\w+)/b/(\\w+)$'\n    expected = '^<a>/b/(\\w+)$'\n    self.assertEqual(utils.replace_named_groups(pattern), expected)\n", "def test_get_view_name(self):\n    view_func = views.ModelView.as_view()\n    self.assertEqual(utils.get_view_name(view_func), 'django.contrib.admindocs.views.ModelView')\n", "def test_get_view_name(self):\n    view_func = views.doc_index\n    self.assertEqual(utils.get_view_name(view_func), 'django.contrib.admindocs.views.doc_index')\n", "def test_parse_docstring(self):\n    # Test with a single-line docstring\n    docstring = \"This is a single-line docstring.\"\n    title, body, metadata = utils.parse_docstring(docstring)\n    self.assertEqual(title, \"This is a single-line docstring.\")\n    self.assertEqual(body, '')\n    self.assertEqual(metadata, {})\n\n    # Test with a multi-line docstring\n    docstring = \"\"\"This is a multi-line docstring.\n", "def test_replace_named_groups(self):\n    pattern = r'^(?P<a>\\w+)/b/(?P<c>\\w+)/$'\n    expected = '^<a>/b/<c>/$'\n    self.assertEqual(utils.replace_named_groups(pattern), expected)\n\n    pattern = r'^(?P<a>\\w+)/b/(\\w+)$'\n    expected = '^<a>/b/(\\\\w+)$'\n    self.assertEqual(utils.replace_named_groups(pattern), expected)\n"], "sample_114": ["def test_altered_foo_together_with_renamed_field(self):\n    changes = self.get_changes(\n        [self.book_foo_together],\n        [ModelState(\"otherapp\", \"Book\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"new_author\", models.ForeignKey(\"testapp.Author\", models.CASCADE)),\n            (\"title\", models.CharField(max_length=200)),\n        ], {\n            \"index_together\": {(\"new_author\", \"title\")},\n            \"unique_together\": {(\"new_author\", \"title\")},\n        })],\n    )\n    self.assertNumberMigrations(changes, 'otherapp', 1)\n    self.assertOperationTypes(changes, 'otherapp', 0, [\"RenameField\", \"AlterUniqueTogether\", \"AlterIndexTogether\"])\n    self.assertOperationAttributes(changes, 'otherapp', 0, 0, old_name=\"author\", new_name=\"new_author\")\n    self.assertOperationAttributes(changes, 'otherapp', 0, 1, name=\"book\", unique_together={(\"new_author\", \"title\")})\n    self.assertOperationAttributes(changes, 'otherapp', 0, 2, name=\"book\", index_together={(\"new_author\", \"title\")})\n", "def test_alter_field_renames(self):\n    # Renaming a field and then altering it in the same set of changes\n    # should result in only one AlterField operation.\n    model_state = ModelState(\"testapp\", \"Author\", [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"name\", models.CharField(max_length=200)),\n    ])\n    new_model_state = ModelState(\"testapp\", \"Author\", [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"full_name\", models.EmailField(max_length=200)),\n    ])\n\n    changes = self.get_changes([model_state], [new_model_state])\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AlterField\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, old_name='name', new_name='full_name')\n", "def test_create_altered_constraints(self):\n    \"\"\"Tests creation of altered constraints.\"\"\"\n    # Make state\n    before = self.make_project_state([self.author_name])\n    after = self.make_project_state([self.author_name_check_constraint])\n\n    # Create the autodetector\n    autodetector = MigrationAutodetector(before, after)\n\n    # Test the resulting operations\n    changes = autodetector._detect_changes()\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['AddConstraint'])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name='name_contains_bob')\n", "def test_generate_altered_unique_together(self):\n    \"\"\"Tests that unique_together changes are correctly detected.\"\"\"\n    # Make state\n    before = self.make_project_state([self.book_foo_together])\n    after = self.make_project_state([self.book_foo_together_2])\n\n    # Make a autodetector and get changes\n    changes = MigrationAutodetector(before, after)._detect_changes()\n\n    # Right number of migrations?\n    self.assertNumberMigrations(changes, 'otherapp', 1)\n\n    # Right operations?\n    self.assertOperationTypes(changes, 'otherapp', 0, ['AlterUniqueTogether'])\n    self.assertOperationAttributes(changes, 'otherapp', 0, 0, name='book', unique_together={('title', 'author')})\n", "def test_alter_index_together(self):\n    # Make state\n    before = self.make_project_state([self.book_foo_together])\n    after = self.make_project_state([self.book_foo_together_2])\n    autodetector = MigrationAutodetector(before, after)\n    changes = autodetector._detect_changes()\n    # Right number of migrations?\n    self.assertNumberMigrations(changes, 'otherapp', 1)\n    # Right operations?\n    self.assertOperationTypes(changes, 'otherapp', 0, [\"AlterIndexTogether\", \"AlterUniqueTogether\"])\n    self.assertOperationAttributes(changes, 'otherapp', 0, 0, name='book', index_together={('title', 'author')})\n    self.assertOperationAttributes(changes, 'otherapp', 0, 1, name='book', unique_together={('title', 'author')})\n"], "sample_115": ["    def test_cleanse_setting(self):\n        filter = SafeExceptionReporterFilter()\n        self.assertEqual(filter.cleanse_setting('ALLOWED_HOSTS', ['example.com']), ['example.com'])\n        self.assertEqual(filter.cleanse_setting('SECRET_KEY', 'secret'), '********************')\n        self.assertEqual(filter.cleanse_setting('DATABASES', {'default': {'PASSWORD': 'secret'}}), {'default': {'PASSWORD': '********************'}})\n", "    def test_cleansed_setting(self):\n        filter = SafeExceptionReporterFilter()\n        value = 'secret_key_here'\n        key = 'SECRET_KEY'\n        cleansed_value = filter.cleanse_setting(key, value)\n        self.assertEqual(cleansed_value, '********************')\n", "    def test_get_safe_request_meta(self):\n        filter = SafeExceptionReporterFilter()\n        request = RequestFactory().get('/test')\n        request.META['HTTP_AUTHORIZATION'] = 'secret'\n        safe_meta = filter.get_safe_request_meta(request)\n        self.assertNotIn('HTTP_AUTHORIZATION', safe_meta)\n", "    def test_get_safe_settings(self):\n        filter = SafeExceptionReporterFilter()\n        settings = filter.get_safe_settings()\n        self.assertIsInstance(settings, dict)\n        for key, value in settings.items():\n            if key == 'SECRET_KEY':\n                self.assertEqual(value, '********************')\n            elif key == 'DATABASES':\n                self.assertIsInstance(value, dict)\n                for db_key, db_value in value.items():\n                    self.assertIsInstance(db_value, dict)\n                    for db_setting_key, db_setting_value in db_value.items():\n                        if db_setting_key == 'PASSWORD':\n                            self.assertEqual(db_setting_value, '********************')\n                        else:\n                            self.assertNotEqual(db_setting_value, '********************')\n", "    def test_cleansing(self):\n        filter = SafeExceptionReporterFilter()\n        self.assertEqual(filter.cleansed_substitute, '********************')\n\n        key = 'foo'\n        value = 'bar'\n        self.assertEqual(filter.cleanse_setting(key, value), value)\n\n        key = 'DATABASE_PASSWORD'\n        value = 'mysecretpassword'\n        self.assertEqual(filter.cleanse_setting(key, value), filter.cleansed_substitute)\n"], "sample_116": ["def test_make_template_fragment_key(self):\n    fragment_name = 'test_fragment'\n    vary_on = ['arg1', 'arg2']\n    key = make_template_fragment_key(fragment_name, vary_on)\n    self.assertRegex(key, r'^template\\.cache\\.%s\\.[0-9a-f]{32}$' % fragment_name)\n\n    # Test with no vary_on\n    key = make_template_fragment_key(fragment_name)\n    self.assertRegex(key, r'^template\\.cache\\.%s\\.[0-9a-f]{32}$' % fragment_name)\n\n    # Test with non-string vary_on\n    vary_on = [42, 3.14]\n    key = make_template_fragment_key(fragment_name, vary_on)\n    self.assertRegex(key, r'^template\\.cache\\.%s\\.[0-9a-f]{32}$' % fragment_name)\n", "def test_make_template_fragment_key(self):\n    # Test with no vary_on\n    key = make_template_fragment_key('fragment_name')\n    self.assertRegex(key, r'template\\.cache\\.fragment_name\\.[a-f0-9]{32}')\n\n    # Test with single vary_on\n    key = make_template_fragment_key('fragment_name', ['hello'])\n    self.assertRegex(key, r'template\\.cache\\.fragment_name\\.[a-f0-9]{32}')\n\n    # Test with multiple vary_on\n    key = make_template_fragment_key('fragment_name', ['hello', 42, 'world'])\n    self.assertRegex(key, r'template\\.cache\\.fragment_name\\.[a-f0-9]{32}')\n\n    # Test that the same vary_on produces the same key\n    key1 = make_template_fragment_key('fragment_name', ['hello', 42])\n    key2 = make_template_fragment_key('fragment_name', ['hello', 42])\n    self.assertEqual(key1, key2)\n\n    # Test that different vary_on produces different keys\n    key1 = make_template_fragment_key('fragment_name', ['hello', 42])\n    key2 = make_template_fragment_key('fragment_name', ['goodbye', 42])\n    self.assertNotEqual(key1, key2)\n", "def test_make_template_fragment_key(self):\n    # Test with no vary_on\n    key = make_template_fragment_key('test_fragment')\n    self.assertRegex(key, r'template\\.cache\\.test_fragment\\.[0-9a-f]{32}$')\n\n    # Test with single vary_on\n    key = make_template_fragment_key('test_fragment', vary_on=['hello'])\n    self.assertRegex(key, r'template\\.cache\\.test_fragment\\.[0-9a-f]{32}$')\n\n    # Test with multiple vary_on\n    key = make_template_fragment_key('test_fragment', vary_on=['hello', 42])\n    self.assertRegex(key, r'template\\.cache\\.test_fragment\\.[0-9a-f]{32}$')\n\n    # Test that the same inputs produce the same output\n    key1 = make_template_fragment_key('test_fragment', vary_on=['hello', 42])\n    key2 = make_template_fragment_key('test_fragment', vary_on=['hello', 42])\n    self.assertEqual(key1, key2)\n\n    # Test that different inputs produce different outputs\n    key1 = make_template_fragment_key('test_fragment', vary_on=['hello'])\n    key2 = make_template_fragment_key('test_fragment', vary_on=['goodbye'])\n    self.assertNotEqual(key1, key2)\n", "def test_make_template_fragment_key(self):\n    # Test with no vary_on\n    key = make_template_fragment_key('fragment_name')\n    self.assertRegex(key, r'template\\.cache\\.fragment_name\\.[0-9a-f]{32}')\n\n    # Test with vary_on\n    key = make_template_fragment_key('fragment_name', vary_on=['arg1', 'arg2'])\n    self.assertRegex(key, r'template\\.cache\\.fragment_name\\.[0-9a-f]{32}')\n\n    # Test with same vary_on arguments in different order\n    key1 = make_template_fragment_key('fragment_name', vary_on=['arg1', 'arg2'])\n    key2 = make_template_fragment_key('fragment_name', vary_on=['arg2', 'arg1'])\n    self.assertNotEqual(key1, key2)\n\n    # Test with non-string vary_on argument\n    key = make_template_fragment_key('fragment_name', vary_on=[42])\n    self.assertRegex(key, r'template\\.cache\\.fragment_name\\.[0-9a-f]{32}')\n", "def test_make_template_fragment_key(self):\n    # Test with no vary_on\n    key = make_template_fragment_key('test_fragment')\n    self.assertRegex(key, r'template\\.cache\\.test_fragment\\.[a-f0-9]{32}')\n\n    # Test with vary_on\n    key = make_template_fragment_key('test_fragment', ['arg1', 'arg2'])\n    self.assertRegex(key, r'template\\.cache\\.test_fragment\\.[a-f0-9]{32}')\n\n    # Test with non-string vary_on\n    key = make_template_fragment_key('test_fragment', [123, 456])\n    self.assertRegex(key, r'template\\.cache\\.test_fragment\\.[a-f0-9]{32}')\n\n    # Test with multiple calls with the same vary_on\n    key1 = make_template_fragment_key('test_fragment', ['arg1', 'arg2'])\n    key2 = make_template_fragment_key('test_fragment', ['arg1', 'arg2'])\n    self.assertEqual(key1, key2)\n"], "sample_117": ["    def test_widget(self):\n        field = ReadOnlyPasswordHashField()\n        self.assertIsInstance(field.widget, ReadOnlyPasswordHashWidget)\n", "    def test_init(self):\n        form = UserChangeForm(self.u1)\n        self.assertEqual(form.fields['username'].initial, self.u1.username)\n        self.assertEqual(form.fields['email'].initial, self.u1.email)\n", "    def test_render(self):\n        widget = ReadOnlyPasswordHashWidget()\n        password = 'sha1$1234567890123456789012345678901234567890'\n        html = widget.render('password', password)\n        self.assertHTMLEqual(\n            '<div><strong>SHA-1</strong> <code>12345678901234567890</code></div>',\n            html,\n        )\n", "    def test_readonly_password_hash_widget(self):\n        widget = ReadOnlyPasswordHashWidget()\n        password_hash = 'pbkdf2_sha256$12000$random$salt$hash'\n        html = widget.render('password', password_hash)\n        self.assertHTMLEqual(html, '''\n            <div>\n                <h2>Password:</h2>\n                <ul>\n                    <li>algorithm: pbkdf2_sha256</li>\n                    <li>iterations: 12000</li>\n                    <li>salt: salt</li>\n                    <li>hash: hash</li>\n                </ul>\n            </div>\n        ''')\n", "    def test_render(self):\n        widget = ReadOnlyPasswordHashWidget()\n        html = widget.render('password', 'mysecretpassword')\n        self.assertHTMLEqual(html, '<div>*************</div>')\n"], "sample_118": ["def test_lookup_in_with_unhashable_items(self):\n    # Test that passing unhashable items to the 'in' lookup doesn't raise a\n    # TypeError.\n    Article.objects.filter(headline__in=[['Article 1'], ['Article 2']])\n", "def test_lookup_with_bilateral_transforms(self):\n    # Create a bilateral transform on the lookup.\n    class BilateralTransform(Transform):\n        bilateral = True\n\n    # Create a lookup with the bilateral transform.\n    class TestLookup(Lookup):\n            lhs_sql, params = self.process_lhs(compiler, connection)\n            rhs_sql, rhs_params = self.process_rhs(compiler, connection)\n            params.extend(rhs_params)\n            return '%s = %s' % (lhs_sql, rhs_sql), params\n\n            return [BilateralTransform]\n\n    # Apply the bilateral transform to a lookup.\n    lookup = TestLookup(lhs=Article._meta.get_field('headline'), rhs='Article 1')\n    lookup = lookup.apply_bilateral_transforms('Article 1')\n\n    # Check that the bilateral transform was applied correctly.\n    sql, params = lookup.as_sql(None, connection)\n    self.assertEqual(sql, '\"articles\".\"headline\" = %s')\n    self.assertEqual(params, ['Article 1'])\n", "def test_lookup_prepare_rhs_with_bilateral_transforms(self):\n    # Test that bilateral transformations are applied correctly when preparing\n    # the rhs of a lookup.\n    class BilateralTransform(Transform):\n        bilateral = True\n\n            sql, params = compiler.compile(self.lhs)\n            return 'UPPER(%s)' % sql, params\n\n    lookup = Exact(BilateralTransform(Article.objects.values_list('headline')), 'foo')\n    sql, params = lookup.process_rhs(compiler=Article.objects.query.compiler(connection), connection=connection)\n    self.assertEqual(sql, 'UPPER(%s)')\n    self.assertEqual(params, ['foo'])\n", "def test_lookup_with_bilateral_transform(self):\n    # Test that bilateral transformations are applied correctly.\n    bilateral_transforms = [lambda x: x.upper()]\n    lookup = Contains(lhs=Article.objects.get(id=self.a1.id).headline, rhs='article')\n    lookup.bilateral_transforms = bilateral_transforms\n    sql, params = lookup.as_sql(connection.connection)\n    self.assertEqual(sql, \"UPPER(%s) LIKE UPPER(%s)\")\n    self.assertEqual(params, ['Article 1', '%article%'])\n", "def test_lookup_in_with_large_list(self):\n    max_in_list_size = connection.ops.max_in_list_size()\n    authors = [Author.objects.create(name=f'Author {i}') for i in range(max_in_list_size + 10)]\n    ids = [author.id for author in authors]\n    lookup_ids = ids[:max_in_list_size + 5]\n\n    result = Author.objects.filter(id__in=lookup_ids).order_by('id').values_list('id', flat=True)\n    self.assertEqual(list(result), lookup_ids)\n"], "sample_119": ["def test_join_promoter_demotion(self):\n    query = Query(Author)\n    joinpromoter = JoinPromoter(AND, 2, False)\n    joinpromoter.add_votes(['alias1', 'alias2'])\n    query.alias_map['alias1'] = query.alias_map['alias2'] = 'LOUTER'\n    joinpromoter.update_join_types(query)\n    self.assertEqual(query.alias_map['alias1'], INNER)\n    self.assertEqual(query.alias_map['alias2'], INNER)\n", "def test_query_is_nullable(self):\n    q = Query(Author)\n    field = Author._meta.get_field('name')\n    self.assertFalse(q.is_nullable(field))\n\n    # Ensure is_nullable returns correct result when\n    # interprets_empty_strings_as_nulls is enabled.\n    with self.settings(DATABASES={\n        'default': {\n            'ENGINE': 'django.db.backends.sqlite3',\n            'OPTIONS': {'interprets_empty_strings_as_nulls': True}\n        }\n    }):\n        q = Query(Author)\n        field = Author._meta.get_field('name')\n        self.assertTrue(q.is_nullable(field))\n", "def test_query_deferred_to_data(self):\n    query = Query(Author)\n    query.deferred_loading = (frozenset(['name']), True)\n\n    data = {}\n    query.deferred_to_data(data, lambda target, model, fields: target.update({model: fields}))\n\n    self.assertEqual(data, {Author: [Author._meta.get_field('name')]})\n", "def test_query_build_filter_with_F_expression(self):\n    q = Query(Author)\n    lookup = ('name', F('first_name'))\n    clause, _ = q.build_filter(lookup, can_reuse=None, branch_negated=False, current_negated=False)\n    self.assertIsInstance(clause.children[0], Exact)\n    self.assertIsInstance(clause.children[0].rhs, F)\n", "def test_join_promoter_demote(self):\n    query = Query(Item)\n    join_promoter = JoinPromoter(AND, 2, False)\n    join_promoter.add_votes(['item__author'])\n    join_promoter.update_join_types(query)\n    self.assertEqual(join_promoter.votes, Counter({'item__author': 1}))\n    self.assertEqual(join_promoter.to_demote, {'item__author'})\n"], "sample_120": ["    def test_float_serializer(self):\n        serializer = BaseSerializer(1.0)\n        self.assertEqual(serializer.serialize(), ('1.0', set()))\n\n        serializer = BaseSerializer(float('inf'))\n        self.assertEqual(serializer.serialize(), (\"float('inf')\", set()))\n\n        serializer = BaseSerializer(float('-inf'))\n        self.assertEqual(serializer.serialize(), (\"float('-inf')\", set()))\n\n        serializer = BaseSerializer(float('nan'))\n        self.assertEqual(serializer.serialize(), (\"float('nan')\", set()))\n", "    def test_float_serializer_with_nan(self):\n        value = float('nan')\n        serializer = FloatSerializer(value)\n        serialized_value, imports = serializer.serialize()\n        self.assertEqual(serialized_value, 'float(\"nan\")')\n        self.assertEqual(imports, set())\n", "    def test_float_serializer(self):\n        float_value = 3.14\n        serializer = BaseSerializer.get_serializer(float_value)\n        self.assertIsInstance(serializer, FloatSerializer)\n        serialized_value, imports = serializer.serialize()\n        self.assertEqual(serialized_value, repr(float_value))\n        self.assertEqual(imports, set())\n", "    def test_serializer_factory(self):\n        # Test that serializer_factory can serialize various types of values\n        values = [\n            123,  # integer\n            'hello',  # string\n            [1, 2, 3],  # list\n            (1, 2, 3),  # tuple\n            {'a': 1, 'b': 2},  # dict\n            datetime.datetime(2022, 1, 1),  # datetime\n            decimal.Decimal('10.99'),  # decimal\n            True,  # boolean\n            None,  # NoneType\n            re.compile(r'^test$'),  # regex pattern\n            uuid.UUID('12345678-1234-1234-1234-123456789012'),  # UUID\n        ]\n        for value in values:\n            serializer = serializer_factory(value)\n            self.assertIsInstance(serializer, BaseSerializer)\n            serialized_value, imports = serializer.serialize()\n            self.assertIsNotNone(serialized_value)\n            self.assertIsInstance(imports, set)\n", "    def test_float_serializer(self):\n        value = float('inf')\n        serializer = FloatSerializer(value)\n        result, imports = serializer.serialize()\n        self.assertEqual(result, 'float(\"inf\")')\n        self.assertEqual(imports, set())\n\n        value = float('-inf')\n        serializer = FloatSerializer(value)\n        result, imports = serializer.serialize()\n        self.assertEqual(result, 'float(\"-inf\")')\n        self.assertEqual(imports, set())\n\n        value = float('nan')\n        serializer = FloatSerializer(value)\n        result, imports = serializer.serialize()\n        self.assertEqual(result, 'float(\"nan\")')\n        self.assertEqual(imports, set())\n"], "sample_121": ["    def test_model_name_db_lookup_clashes(self):\n        class Model(models.Model):\n            pass\n\n        Model.__name__ = '_Model'\n        errors = Model.check()\n        self.assertEqual(errors, [\n            Error(\n                \"The model name '_Model' cannot start or end with an underscore \"\n                \"as it collides with the query lookup syntax.\",\n                obj=Model,\n                id='models.E023',\n            ),\n        ])\n\n        Model.__name__ = 'my__model'\n        errors = Model.check()\n        self.assertEqual(errors, [\n            Error(\n                \"The model name 'my__model' cannot contain double underscores as \"\n                \"it collides with the query lookup syntax.\",\n                obj=Model,\n                id='models.E024',\n            ),\n        ])\n", "    def test_order_with_respect_to_checks(self):\n        class Model(models.Model):\n            name = models.CharField(max_length=20)\n            order = models.IntegerField()\n\n            class Meta:\n                order_with_respect_to = 'name'\n\n        errors = Model.check()\n        self.assertEqual(errors, [\n            Error(\n                \"'order_with_respect_to' refers to the nonexistent field 'name'.\",\n                obj=Model,\n                id='models.E015',\n            ),\n        ])\n", "    def test_order_with_respect_to_checks(self):\n        class Model(models.Model):\n            order = models.IntegerField()\n\n        class RelatedModel(models.Model):\n            model = models.ForeignKey(Model, on_delete=models.CASCADE)\n            order = models.IntegerField()\n\n            class Meta:\n                order_with_respect_to = 'model'\n\n        self.assertEqual(RelatedModel.check(), [])\n\n        class AbstractModel(models.Model):\n            class Meta:\n                abstract = True\n                order_with_respect_to = 'model'\n\n        self.assertEqual(AbstractModel.check(), [\n            Error(\n                \"'order_with_respect_to' is not supported on abstract models.\",\n                id='models.E021',\n            ),\n        ])\n\n        class Model(models.Model):\n            field1 = models.IntegerField()\n            field2 = models.IntegerField()\n\n            class Meta:\n                order_with_respect_to = 'field1'\n\n        self.assertEqual(Model.check(), [\n            Error(\n                \"'order_with_respect_to' refers to a local field 'field1'.\",\n                id='models.E022',\n            ),\n        ])\n\n        class Model(models.Model):\n            pass\n\n        class RelatedModel(models.Model):\n            model = models.ForeignKey(Model, on_delete=models.CASCADE)\n\n            class Meta:\n                order_with_respect_to = 'model'\n\n        self.assertEqual(RelatedModel.check(), [\n            Error(\n                \"'order_with_respect_to' refers to the first field in a ForeignKey (model).\",\n                id='models.E023',\n            ),\n        ])\n", "    def test_field_name_clashes_with_related_field_accessor(self):\n        class Model(models.Model):\n            clash = models.CharField(max_length=10)\n\n                super().__init__(*args, **kwargs)\n                self.clash = None\n\n        related_model = Model._meta.get_field('clash').remote_field.model\n\n        class RelatedModel(related_model):\n            clash = models.CharField(max_length=10)\n\n        errors = RelatedModel.check()\n        expected_errors = [\n            Error(\n                \"The property 'clash' clashes with a related field accessor.\",\n                obj=RelatedModel,\n                id='models.E025',\n            ),\n        ]\n        self.assertEqual(errors, expected_errors)\n", "    def test_index_name_length(self):\n        allowed_len, db_alias = get_max_column_name_length()\n        if allowed_len is None:\n            self.skipTest(\"The database doesn't have a column name length limit.\")\n\n        with override_settings(DATABASES={db_alias: {'ENGINE': 'django.db.backends.sqlite3', 'NAME': ':memory:'}}):\n            class AbstractModel(models.Model):\n                longfield = models.CharField(max_length=255)\n\n                class Meta:\n                    abstract = True\n                    indexes = [models.Index(fields=['longfield'], name='a' * (allowed_len + 1))]\n\n            model = AbstractModel()\n\n            errors = model.check()\n            expected_errors = [\n                Error(\n                    \"The index name '%s' cannot be longer than %d characters.\" % ('a' * (allowed_len + 1), allowed_len),\n                    obj=model.__class__,\n                    id='models.E034',\n                )\n            ]\n            self.assertEqual(errors, expected_errors)\n"], "sample_122": ["    def test_patch_cache_control(self):\n        response = HttpResponse()\n        patch_cache_control(response, private=True)\n        self.assertEqual(response['Cache-Control'], 'private')\n\n        patch_cache_control(response, max_age=3600)\n        self.assertEqual(response['Cache-Control'], 'private, max-age=3600')\n\n        patch_cache_control(response, public=True)\n        self.assertEqual(response['Cache-Control'], 'public, max-age=3600')\n", "    def test_patch_cache_control(self):\n        response = HttpResponse()\n        patch_cache_control(response, max_age=3600)\n        self.assertEqual(response['Cache-Control'], 'max-age=3600')\n", "def test_patch_cache_control_max_age(self):\n    response = HttpResponse()\n    patch_cache_control(response, max_age=3600)\n    self.assertEqual(response['Cache-Control'], 'max-age=3600')\n\n    response = HttpResponse()\n    response['Cache-Control'] = 'max-age=300'\n    patch_cache_control(response, max_age=3600)\n    self.assertEqual(response['Cache-Control'], 'max-age=300')\n\n    response = HttpResponse()\n    patch_cache_control(response, max_age=None)\n    self.assertNotIn('max-age', response['Cache-Control'])\n\n    response = HttpResponse()\n    patch_cache_control(response, max_age='invalid')\n    self.assertNotIn('max-age', response['Cache-Control'])\n", "    def test_patch_cache_control(self):\n        response = HttpResponse()\n        patch_cache_control(response, max_age=3600)\n        self.assertEqual(response['Cache-Control'], 'max-age=3600')\n", "    def test_patch_vary_headers(self):\n        response = HttpResponse()\n        patch_vary_headers(response, ['Accept-Language', 'Cookie'])\n        self.assertEqual(response['Vary'], 'Accept-Language, Cookie')\n"], "sample_123": ["    def test_urlsafe_base64_encode(self):\n        self.assertEqual(urlsafe_base64_encode(b'Hello, World!'), 'SGVsbG8sIFdvcmxkIQ')\n", "    def test_urlencode_none_value(self):\n        with self.assertRaisesMessage(TypeError, self.cannot_encode_none_msg):\n            urlencode({'a': None})\n        with self.assertRaisesMessage(TypeError, self.cannot_encode_none_msg):\n            urlencode([('a', None)])\n", "    def test_urlencode_with_non_string_values(self):\n        query = {'a': 1, 'b': True, 'c': None}\n        with self.assertRaises(TypeError) as cm:\n            urlencode(query)\n        self.assertEqual(str(cm.exception), self.cannot_encode_none_msg)\n\n        query = {'a': 1, 'b': True}\n        expected_output = 'a=1&b=True'\n        self.assertEqual(urlencode(query), expected_output)\n", "    def test_base36_to_int(self):\n        self.assertEqual(base36_to_int('1'), 1)\n        self.assertEqual(base36_to_int('z'), 35)\n        self.assertEqual(base36_to_int('10'), 36)\n", "def test_urlencode_with_non_string_values(self):\n    with self.assertRaises(TypeError) as cm:\n        urlencode({'a': None})\n    self.assertEqual(str(cm.exception), self.cannot_encode_none_msg)\n\n    with self.assertRaises(TypeError) as cm:\n        urlencode([('a', None)])\n    self.assertEqual(str(cm.exception), self.cannot_encode_none_msg)\n\n    with self.assertRaises(TypeError) as cm:\n        urlencode(MultiValueDict({'a': [None]}))\n    self.assertEqual(str(cm.exception), self.cannot_encode_none_msg)\n"], "sample_125": ["def test_set_cookie_max_age(self):\n    response = HttpResponse()\n    max_age = 3600  # 1 hour in seconds\n    expires = datetime.utcnow() + timedelta(seconds=max_age)\n    expires_str = http_date(expires.timestamp())\n    response.set_cookie('test', 'test', max_age=max_age)\n    self.assertEqual(response.cookies['test']['max-age'], max_age)\n    self.assertEqual(response.cookies['test']['expires'], expires_str)\n", "def test_set_cookie_max_age(self):\n    max_age = 3600\n    expires = datetime.utcnow() + timedelta(seconds=max_age)\n    with freeze_time(expires):\n        response = HttpResponse()\n        response.set_cookie('key', 'value', max_age=max_age)\n        self.assertEqual(response.cookies['key']['max-age'], max_age)\n        self.assertEqual(response.cookies['key']['expires'], http_date(expires.timestamp()))\n", "def test_set_cookie_max_age(self):\n    response = HttpResponse()\n    max_age = 3600\n    response.set_cookie('test', 'value', max_age=max_age)\n    self.assertEqual(response.cookies['test']['max-age'], max_age)\n    self.assertEqual(response.cookies['test']['expires'], http_date(time.time() + max_age))\n", "def test_set_cookie_expires(self):\n    response = HttpResponse()\n    expires = datetime(2023, 1, 1, tzinfo=utc)\n    response.set_cookie('test', 'value', expires=expires)\n    self.assertEqual(response.cookies['test']['expires'], http_date(expires.timestamp()))\n", "def test_set_cookie_max_age(self):\n    response = HttpResponse()\n    max_age = 3600\n    response.set_cookie('test', 'value', max_age=max_age)\n    self.assertEqual(response.cookies['test']['max-age'], max_age)\n    self.assertEqual(response.cookies['test']['expires'], http_date(time.time() + max_age))\n"], "sample_126": ["def test_create_altered_indexes(self):\n    # Make state\n    before = self.make_project_state([self.book])\n    after = self.make_project_state([self.book_indexes])\n    autodetector = MigrationAutodetector(before, after)\n    changes = autodetector._detect_changes()\n    # Right number of migrations?\n    self.assertNumberMigrations(changes, 'otherapp', 1)\n    # Right operations?\n    self.assertOperationTypes(changes, 'otherapp', 0, ['AddIndex'])\n    self.assertOperationAttributes(changes, 'otherapp', 0, 0, model_name='book')\n    self.assertEqual(changes['otherapp'][0].operations[0].index.name, 'book_title_author_idx')\n    self.assertEqual(len(changes['otherapp'][0].operations[0].index.fields), 2)\n", "def test_add_check_constraint(self):\n    \"\"\"Tests autodetection of new CheckConstraints.\"\"\"\n    changes = self.get_changes(\n        [self.author_name],\n        [self.author_name_check_constraint],\n    )\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['AddConstraint'])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, constraint_name='name_contains_bob')\n", "def test_alter_order_with_respect_to(self):\n    \"\"\"\n    Tests the autodetection of AlterOrderWithRespectTo.\n    \"\"\"\n    # Make state\n    before = self.make_project_state([self.author_with_book])\n    after = self.make_project_state([self.author_with_book_order_wrt])\n\n    # Create the autodetector\n    autodetector = MigrationAutodetector(before, after)\n\n    # Detect changes\n    changes = autodetector._detect_changes()\n\n    # Right number/types of operations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AlterOrderWithRespectTo\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name='author', order_with_respect_to='book')\n", "def test_add_check_constraint(self):\n    changes = self.get_changes(\n        [self.author_name],\n        [self.author_name_check_constraint],\n    )\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['AddConstraint'])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name='name_contains_bob')\n", "def test_generate_added_indexes(self):\n    \"\"\"\n    Tests that added indexes are correctly generated.\n    \"\"\"\n    # Make state\n    before_state = self.make_project_state([self.book])\n    after_state = self.make_project_state([self.book_indexes])\n    autodetector = MigrationAutodetector(before_state, after_state)\n    changes = autodetector._detect_changes()\n    # Right number of migrations?\n    self.assertNumberMigrations(changes, 'otherapp', 1)\n    # Right operations?\n    self.assertOperationTypes(changes, 'otherapp', 0, [\"AddIndex\"])\n    self.assertOperationAttributes(changes, 'otherapp', 0, 0, model_name=\"book\", index=Index(fields=[\"author\", \"title\"], name=\"book_title_author_idx\"))\n"], "sample_127": ["def test_bulk_create_empty(self):\n    self.assertEqual(Country.objects.bulk_create([]), [])\n    self.assertQuerysetEqual(Country.objects.all(), [])\n", "def test_bulk_create_nullable_fields(self):\n    data = [\n        NullableFields(name=\"Test1\"),\n        NullableFields(name=\"Test2\", country=\"USA\"),\n        NullableFields(name=\"Test3\")\n    ]\n    NullableFields.objects.bulk_create(data)\n    self.assertEqual(NullableFields.objects.count(), 3)\n", "def test_bulk_create_empty(self):\n    self.assertEqual(Country.objects.bulk_create([]), [])\n", "def test_bulk_create_empty(self):\n    self.assertEqual(Country.objects.bulk_create([]), [])\n    self.assertQuerysetEqual(Country.objects.all(), [])\n", "def test_bulk_create_with_nullable_fields(self):\n    data = [\n        NullableFields(name=\"Test 1\"),\n        NullableFields(name=\"Test 2\", country=self.data[0]),\n        NullableFields(name=\"Test 3\", country=self.data[1])\n    ]\n    created = NullableFields.objects.bulk_create(data)\n    self.assertEqual(len(created), len(data))\n    for obj in created:\n        self.assertIsInstance(obj, NullableFields)\n        self.assertIsNotNone(obj.pk)\n"], "sample_128": ["def test_index_name(self):\n    article = Article()\n    index = Index(fields=['headline', '-publications'])\n    index.set_name_with_model(article)\n    self.assertLessEqual(len(index.name), Index.max_name_length)\n    self.assertEqual(index.name[:3], 'ar_')  # ar from 'article'\n    self.assertEqual(index.name[-4:], '_idx')\n    self.assertEqual(len(index.name.split('_')), 4)\n", "def test_index_condition(self):\n    index = Index(fields=['title'], condition=Q(title='example'))\n    sql = index.create_sql(Article, connection.schema_editor(), using=connection.alias)\n    self.assertIn('WHERE', sql)\n    self.assertIn(\"title = 'example'\", sql)\n\n    # Test that an unquoted string is not allowed in the condition.\n    with self.assertRaises(ValueError):\n        Index(fields=['title'], condition='invalid condition')\n\n    # Test that a Q object with an unquoted string is not allowed in the condition.\n    with self.assertRaises(ValueError):\n        Index(fields=['title'], condition=Q(title__contains='example\"'))\n\n    # Test that a Q object with a quoted string is allowed in the condition.\n    index = Index(fields=['title'], condition=Q(title__contains='\"example\"'))\n    sql = index.create_sql(Article, connection.schema_editor(), using=connection.alias)\n    self.assertIn('WHERE', sql)\n    self.assertIn(\"title LIKE '%\\\"example\\\"%'\", sql)\n", "def test_index_deconstruction(self):\n    index = Index(fields=['field1', 'field2'], name='my_index')\n    path, args, kwargs = index.deconstruct()\n    self.assertEqual(path, 'django.db.models.Index')\n    self.assertEqual(args, ())\n    self.assertEqual(kwargs, {'fields': ['field1', 'field2'], 'name': 'my_index'})\n\n    # Test with condition\n    index = Index(fields=['field1', 'field2'], name='my_index', condition=Q(field1='value'))\n    path, args, kwargs = index.deconstruct()\n    self.assertEqual(path, 'django.db.models.Index')\n    self.assertEqual(args, ())\n    self.assertEqual(kwargs, {'fields': ['field1', 'field2'], 'name': 'my_index', 'condition': Q(field1='value')})\n\n    # Test with include\n    index = Index(fields=['field1', 'field2'], name='my_index', include=['field3'])\n    path, args, kwargs = index.deconstruct()\n    self.assertEqual(path, 'django.db.models.Index')\n    self.assertEqual(args, ())\n    self.assertEqual(kwargs, {'fields': ['field1', 'field2'], 'name': 'my_index', 'include': ['field3']})\n", "    def test_create_index(self):\n        index = Index(fields=['title'], name='article_title_idx')\n        sql = index.create_sql(Article, connection.schema_editor())\n        self.assertRegex(sql, r'CREATE INDEX article_title_idx ON .*articles \\(\"title\"\\)')\n", "def test_index_creation_with_condition(self):\n    index = Index(fields=['headline'], condition=Q(headline='Test'))\n    sql = index.create_sql(Article, connection.schema_editor(), using='default')\n    self.assertIn('WHERE', sql)\n    self.assertIn(\"headline = 'Test'\", sql)\n\n    index = Index(fields=['headline'])\n    sql = index.create_sql(Article, connection.schema_editor(), using='default')\n    self.assertNotIn('WHERE', sql)\n"], "sample_129": ["    def test_floatformat_with_arg(self):\n        output = self.engine.render_to_string('floatformat02', {'a': 34.23234, 'b': 34.00000})\n        self.assertEqual(output, '34.232 34.000')\n", "    def test_floatformat_negative(self):\n        output = self.engine.render_to_string('floatformat_negative', {'a': 34.23234})\n        self.assertEqual(output, '34.23')\n", "    def test_floatformat_with_negative_precision(self):\n        val = Decimal('123.456')\n        self.assertEqual(floatformat(val, '-2'), '123.46')\n        self.assertEqual(floatformat(val, '-3'), '123.456')\n", "    def test_floatformat_with_positive_arg(self):\n        out = self.engine.render_to_string('floatformat01', {'a': 34.23234, 'b': 34.00000})\n        self.assertEqual(out, '34.2 34')\n\n        out = self.engine.render_to_string('floatformat02', {'a': 34.23234, 'b': 34.00000}, {'floatformat_arg': 3})\n        self.assertEqual(out, '34.232 34.000')\n", "    def test_floatformat_decimal(self):\n        with localcontext() as context:\n            context.prec = 10\n            value = Decimal('1234567.89')\n            self.assertEqual(floatformat(value, arg=-3), '1,234,567.890')\n            self.assertEqual(floatformat(value, arg=2), '1,234,567.89')\n            self.assertEqual(floatformat(value, arg=5), '1,234,567.89000')\n"], "sample_130": ["def test_resolve_ref_allow_joins(self):\n    q = Query(Item)\n    q.add_annotation(Ranking('author__id'), alias='ranking')\n    with self.assertRaises(FieldError):\n        q.resolve_ref('author__name', allow_joins=False, reuse=None)\n    q.resolve_ref('author__name', allow_joins=True, reuse=None)\n", "def test_query_add_immediate_loading(self):\n    query = Query(Author)\n    query.add_immediate_loading(['name'])\n    self.assertEqual(query.deferred_loading, (frozenset(['name']), False))\n    query.add_immediate_loading(['name', 'age'])\n    self.assertEqual(query.deferred_loading, (frozenset(['name', 'age']), False))\n", "def test_build_lookup(self):\n    query = Query(Author)\n    lookup = query.build_lookup(['exact'], SimpleCol('name', 'author'), 'test')\n    self.assertIsInstance(lookup, Exact)\n    self.assertEqual(lookup.lhs.output_field, CharField())\n    self.assertEqual(lookup.rhs, 'test')\n\n    with self.assertRaises(FieldError):\n        query.build_lookup(['nonexistent'], SimpleCol('name', 'author'), 'test')\n\n    # Ensure that the lookup is built with the correct transform.\n    lookup = query.build_lookup(['lower__exact'], SimpleCol('name', 'author'), 'test')\n    self.assertIsInstance(lookup.lhs, Lower)\n    self.assertIsInstance(lookup, Exact)\n\n    # If a lookup doesn't exist, try to interpret it as a transform and do an\n    # exact lookup on that.\n    lookup = query.build_lookup(['lower'], SimpleCol('name', 'author'), 'test')\n    self.assertIsInstance(lookup.lhs, Lower)\n    self.assertIsInstance(lookup, Exact)\n\n    lookup = query.build_lookup(['isnull'], SimpleCol('name', 'author'), True)\n    self.assertIsInstance(lookup, IsNull)\n    self.assertTrue(lookup.rhs)\n\n    lookup = query.build_lookup(['isnull'], SimpleCol('name', 'author'), False)\n    self.assertIsInstance(lookup, IsNull)\n    self.assertFalse(lookup.rhs)\n", "def test_ref_alias(self):\n    q = Query(Author)\n    alias = 'my_table'\n    q.ref_alias(alias)\n    self.assertEqual(q.alias_refcount[alias], 1)\n    q.unref_alias(alias)\n    self.assertEqual(q.alias_refcount[alias], 0)\n    q.unref_alias(alias)\n    self.assertEqual(q.alias_refcount[alias], -1)\n", "def test_query_join_promotion(self):\n    qs = Query(Author)\n    query = Q(name='John') & Q(item__name='Book')\n    lookup = qs.build_filter(query.children[1], branch_negated=False, current_negated=False)\n    self.assertEqual(lookup[0].connector, AND)\n    join_promoter = JoinPromoter(AND, 2, False)\n    join_promoter.add_votes(['item'])\n    join_promoter.update_join_types(qs)\n    self.assertEqual(qs.alias_map['item'].join_type, INNER)\n"], "sample_131": ["def test_get_test_db_name(self):\n    test_connection = self.get_connection_copy()\n    test_connection.settings_dict['NAME'] = 'mydb'\n    test_connection.settings_dict['TEST']['NAME'] = ''\n\n    db_creation = BaseDatabaseCreation(test_connection)\n    test_db_name = db_creation._get_test_db_name()\n\n    self.assertEqual(test_db_name, TEST_DATABASE_PREFIX + 'mydb')\n", "    def test_create_test_db(self):\n        test_connection = self.get_connection_copy()\n        creation = BaseDatabaseCreation(test_connection)\n        with mock.patch.object(creation, '_create_test_db') as _create_test_db:\n            _create_test_db.return_value = 'test_database_name'\n            db_name = creation.create_test_db()\n            self.assertEqual(db_name, 'test_database_name')\n            self.assertEqual(test_connection.settings_dict['NAME'], 'test_database_name')\n", "    def test_test_db_name(self):\n        test_connection = self.get_connection_copy()\n        db_creation = BaseDatabaseCreation(test_connection)\n        test_db_name = db_creation._get_test_db_name()\n        self.assertEqual(test_db_name, TEST_DATABASE_PREFIX + test_connection.settings_dict['NAME'])\n", "    def test_create_test_db(self):\n        test_connection = self.get_connection_copy()\n        creation = BaseDatabaseCreation(test_connection)\n        with mock.patch.object(creation, '_create_test_db') as _create_test_db:\n            _create_test_db.return_value = 'test_database'\n            db_name = creation.create_test_db(verbosity=0)\n            self.assertEqual(db_name, 'test_database')\n            self.assertEqual(test_connection.settings_dict['NAME'], 'test_database')\n", "    def test_create_test_db(self):\n        # Create a connection to the default database.\n        test_connection = self.get_connection_copy()\n\n        # Set up the creation class\n        creation = BaseDatabaseCreation(test_connection)\n\n        # Mock out the execute SQL functions\n        with mock.patch.object(creation, '_execute_create_test_db'):\n            with mock.patch.object(creation, 'destroy_test_db'):\n                test_database_name = creation.create_test_db()\n\n        # Make sure the test db was created with the correct name.\n        self.assertEqual(test_database_name, TEST_DATABASE_PREFIX + test_connection.settings_dict['NAME'])\n"], "sample_132": ["    def test_cleansing_of_setting_values(self):\n        filter = SafeExceptionReporterFilter()\n        self.assertEqual(filter.cleanse_setting('ALLOWED_HOSTS', ['example.com']), ['example.com'])\n        self.assertEqual(filter.cleanse_setting('SECRET_KEY', 'my_secret_key'), filter.cleansed_substitute)\n        self.assertEqual(filter.cleanse_setting('DATABASES', {'default': {'ENGINE': 'django.db.backends.sqlite3', 'NAME': 'my_database'}}), {'default': {'ENGINE': 'django.db.backends.sqlite3', 'NAME': 'my_database'}})\n        self.assertEqual(filter.cleanse_setting('API_KEY', 'my_api_key'), filter.cleansed_substitute)\n", "    def test_cleanse_setting(self):\n        filter = SafeExceptionReporterFilter()\n        self.assertEqual(filter.cleanse_setting('ALLOWED_HOSTS', ['example.com']), ['example.com'])\n        self.assertEqual(filter.cleanse_setting('SECRET_KEY', 'secret'), '********************')\n        self.assertEqual(filter.cleanse_setting('DATABASES', {'default': {'ENGINE': 'django.db.backends.sqlite3'}}), {'default': {'ENGINE': 'django.db.backends.sqlite3'}})\n", "    def test_cleansed_substitute(self):\n        filter = SafeExceptionReporterFilter()\n        self.assertEqual(filter.cleansed_substitute, '********************')\n", "    def test_safe_exception_reporter_filter(self):\n        reporter_filter = SafeExceptionReporterFilter()\n        request = RequestFactory().get('/example')\n        self.assertTrue(reporter_filter.is_active(request))\n\n        # Make sure that settings values are properly cleansed\n        original_settings = {\n            'DATABASES': {\n                'default': {\n                    'ENGINE': 'django.db.backends.sqlite3',\n                    'NAME': '/path/to/db.sqlite3',\n                    'PASSWORD': 'mysecretpassword',\n                    'USER': 'mydatabaseuser',\n                }\n            },\n            'SECRET_KEY': 'mysecretkey',\n        }\n        cleansed_settings = reporter_filter.get_safe_settings()\n\n        for key, value in cleansed_settings.items():\n            if key in original_settings:\n                if key == 'DATABASES':\n                    self.assertNotEqual(value, original_settings[key])\n                    self.assertEqual(value['default']['PASSWORD'], '********************')\n                    self.assertEqual(value['default']['USER'], '********************')\n                elif key == 'SECRET_KEY':\n                    self.assertEqual(value, '********************')\n\n        # Make sure that request.META values are properly cleansed\n        original_meta = {\n            'HTTP_AUTHORIZATION': 'Token mysecrettoken',\n            'HTTP_COOKIE': 'sessionid=mysecretcookie',\n        }\n        request.META.update(original_meta)\n        cleansed_meta = reporter_filter.get_safe_request_meta(request)\n\n        for key, value in cleansed_meta.items():\n            if key in original_meta:\n                if key == 'HTTP_AUTHORIZATION':\n                    self.assertEqual(value, '********************')\n                elif key == 'HTTP_COOKIE':\n                    self.assertEqual(value, '********************')\n", "    def test_cleanse_setting(self):\n        filter = SafeExceptionReporterFilter()\n        self.assertEqual(filter.cleanse_setting('PUBLIC_SETTING', 'public_value'), 'public_value')\n        self.assertEqual(filter.cleanse_setting('SECRET_SETTING', 'secret_value'), '********************')\n"], "sample_133": ["    def test_catalog(self):\n        catalog = JavaScriptCatalog()\n        catalog.translation = gettext.NullTranslations()\n        catalog.request = RequestFactory().get('/')\n        context = catalog.get_context_data()\n        self.assertIn('catalog', context)\n        self.assertIn('formats', context)\n        self.assertIn('plural', context)\n", "    def test_catalog(self):\n        catalog = JavaScriptCatalog()\n        catalog.translation = gettext.NullTranslations()\n        catalog.translation._catalog = {'hello': 'hello'}\n        self.assertEqual(catalog.get_catalog(), {'hello': 'hello'})\n", "    def test_javascript_catalog(self):\n        response = self.client.get(reverse('jsi18n'))\n        self.assertEqual(response.status_code, 200)\n        self.assertTemplateUsed(response, 'jsi18n.html')\n        catalog_data = response.context['catalog']\n        formats_data = response.context['formats']\n        self.assertIsInstance(catalog_data, dict)\n        self.assertIsInstance(formats_data, dict)\n        self.assertContains(response, 'django.catalog = django.catalog || {};')\n        self.assertContains(response, 'django.formats =')\n", "    def test_json_catalog(self):\n        \"\"\"Test JSONCatalog view returns correct JSON.\"\"\"\n        catalog = JSONCatalog()\n        catalog.translation = gettext.NullTranslations()\n        response = catalog.get(RequestFactory().get('/jsi18n/'))\n        self.assertEqual(response.status_code, 200)\n        data = json.loads(response.content.decode('utf-8'))\n        self.assertEqual(data['catalog'], {})\n        self.assertIn('formats', data)\n        self.assertEqual(data['plural'], None)\n", "    def test_catalog(self):\n        catalog = JavaScriptCatalog()\n        catalog.translation = gettext.NullTranslations()\n        context = catalog.get_context_data()\n        self.assertIn('catalog', context)\n        self.assertIsInstance(context['catalog'], dict)\n"], "sample_135": ["def test_date_format(self):\n    d = date(2009, 5, 16)\n    self.assertEqual(dateformat.format(d, 'Y'), '2009')\n    self.assertEqual(dateformat.format(d, 'y'), '09')\n    self.assertEqual(dateformat.format(d, 'F'), 'May')\n    self.assertEqual(dateformat.format(d, 'E'), 'May')\n    self.assertEqual(dateformat.format(d, 'M'), 'May')\n    self.assertEqual(dateformat.format(d, 'b'), 'may')\n    self.assertEqual(dateformat.format(d, 'N'), 'May')\n    self.assertEqual(dateformat.format(d, 'm'), '05')\n    self.assertEqual(dateformat.format(d, 'n'), '5')\n    self.assertEqual(dateformat.format(d, 'd'), '16')\n    self.assertEqual(dateformat.format(d, 'j'), '16')\n    self.assertEqual(dateformat.format(d, 'D'), 'Sat')\n    self.assertEqual(dateformat.format(d, 'l'), 'Saturday')\n    self.assertEqual(dateformat.format(d, 'w'), '6')\n    self.assertEqual(dateformat.format(d, 'z'), '136')\n    self.assertEqual(dateformat.format(d, 'W'), '20')\n    self.assertEqual(dateformat.format(d, 't'), '31')\n    self.assertEqual(dateformat.format(d, 'L'), False)\n    self.assertEqual(dateformat.format(d, 'o'), '2009')\n", "def test_date_format(self):\n    my_birthday = date(1999, 6, 1)\n    formatted_date = format(my_birthday, 'F j, Y')\n    self.assertEqual(formatted_date, 'June 1, 1999')\n", "def test_date_format(self):\n    dt = date(2022, 9, 1)\n    self.assertEqual(format(dt, 'Y'), '2022')\n    self.assertEqual(format(dt, 'y'), '22')\n    self.assertEqual(format(dt, 'm'), '09')\n    self.assertEqual(format(dt, 'n'), '9')\n    self.assertEqual(format(dt, 'd'), '01')\n    self.assertEqual(format(dt, 'j'), '1')\n    self.assertEqual(format(dt, 'D'), 'Thu')\n    self.assertEqual(format(dt, 'l'), 'Thursday')\n    self.assertEqual(format(dt, 'N'), 'Sep')\n    self.assertEqual(format(dt, 'F'), 'September')\n    self.assertEqual(format(dt, 'M'), 'Sep')\n    self.assertEqual(format(dt, 'b'), 'sep')\n", "def test_date_format_b(self):\n    dt = date(2022, 1, 15)\n    self.assertEqual(dateformat.format(dt, 'b'), 'jan')\n", "def test_date_format(self):\n    d = date(2009, 2, 12)\n    self.assertEqual(dateformat.format(d, 'W'), '7')\n    self.assertEqual(dateformat.format(d, 'N'), 'February')\n    self.assertEqual(dateformat.format(d, 'j'), '12')\n    self.assertEqual(dateformat.format(d, 'd'), '12')\n    self.assertEqual(dateformat.format(d, 'D'), 'Thu')\n    self.assertEqual(dateformat.format(d, 'l'), 'Thursday')\n"], "sample_134": ["    def test_serializer_factory(self):\n        # Test that serializer_factory can handle a variety of types\n        value = 'a string'\n        serializer = serializer_factory(value)\n        self.assertIsInstance(serializer, BaseSerializer)\n\n        value = 123\n        serializer = serializer_factory(value)\n        self.assertIsInstance(serializer, BaseSerializer)\n\n        value = 123.45\n        serializer = serializer_factory(value)\n        self.assertIsInstance(serializer, FloatSerializer)\n\n        value = decimal.Decimal('123.45')\n        serializer = serializer_factory(value)\n        self.assertIsInstance(serializer, DecimalSerializer)\n\n        value = datetime.datetime(2022, 1, 1, tzinfo=utc)\n        serializer = serializer_factory(value)\n        self.assertIsInstance(serializer, DatetimeDatetimeSerializer)\n\n        value = re.compile(r'^test$')\n        serializer = serializer_factory(value)\n        self.assertIsInstance(serializer, RegexSerializer)\n\n        value = uuid.UUID('12345678-1234-1234-1234-123456789012')\n        serializer = serializer_factory(value)\n        self.assertIsInstance(serializer, UUIDSerializer)\n", "    def test_serialize_float(self):\n        serializer = serializer_factory(1.0)\n        string, imports = serializer.serialize()\n        self.assertEqual(string, '1.0')\n        self.assertEqual(imports, set())\n\n        serializer = serializer_factory(float('inf'))\n        string, imports = serializer.serialize()\n        self.assertEqual(string, \"float('inf')\")\n        self.assertEqual(imports, set())\n\n        serializer = serializer_factory(float('-inf'))\n        string, imports = serializer.serialize()\n        self.assertEqual(string, \"float('-inf')\")\n        self.assertEqual(imports, set())\n\n        serializer = serializer_factory(float('nan'))\n        string, imports = serializer.serialize()\n        self.assertEqual(string, \"float('nan')\")\n        self.assertEqual(imports, set())\n", "    def test_serialize_float(self):\n        serializer = serializer_factory(1.0)\n        self.assertEqual(serializer.serialize(), ('1.0', set()))\n\n        serializer = serializer_factory(float('inf'))\n        self.assertEqual(serializer.serialize(), (\"float('inf')\", set()))\n\n        serializer = serializer_factory(float('-inf'))\n        self.assertEqual(serializer.serialize(), (\"float('-inf')\", set()))\n\n        serializer = serializer_factory(float('nan'))\n        self.assertEqual(serializer.serialize(), (\"float('nan')\", set()))\n", "    def test_serializer_factory_with_lazy_object(self):\n        lazy_obj = SimpleLazyObject(lambda: 'lazy_value')\n        serializer = serializer_factory(lazy_obj)\n        self.assertIsInstance(serializer, BaseSimpleSerializer)\n        serialized_value, imports = serializer.serialize()\n        self.assertEqual(serialized_value, repr('lazy_value'))\n        self.assertEqual(imports, set())\n", "    def test_float_serializer(self):\n        serializer = serializer_factory(float('inf'))\n        self.assertEqual(serializer.serialize(), ('float(\"inf\")', set()))\n        serializer = serializer_factory(float('-inf'))\n        self.assertEqual(serializer.serialize(), ('float(\"-inf\")', set()))\n        serializer = serializer_factory(float('nan'))\n        self.assertEqual(serializer.serialize(), ('float(\"nan\")', set()))\n        serializer = serializer_factory(1.0)\n        self.assertEqual(serializer.serialize(), (repr(1.0), set()))\n"], "sample_139": ["def test_dynamic_list_filter(self):\n    model_admin = DynamicListFilterChildAdmin(Child, admin.site)\n    request = self._mocked_authenticated_request('/child/', self.superuser)\n    changelist = model_admin.get_changelist_instance(request)\n\n    # Test if get_list_filter() returns the expected filters\n    filters = model_admin.get_list_filter(request)\n    self.assertEqual(len(filters), 2)\n    self.assertIn('name', filters)\n    self.assertIn('age', filters)\n\n    # Test if the filters are applied correctly\n    filtered_queryset = changelist.get_queryset(request)\n    self.assertEqual(filtered_queryset.count(), Child.objects.filter(age__gt=10).count())\n", "def test_get_list_display(self):\n    model_admin = ModelAdmin(Band, custom_site)\n    request = self._mocked_authenticated_request('/band/', self.superuser)\n    self.assertEqual(model_admin.get_list_display(request), ('__str__',))\n\n    model_admin.list_display = ('name', 'genre')\n    self.assertEqual(model_admin.get_list_display(request), ('name', 'genre'))\n\n    model_admin.list_display = ('name', 'invalid_field')\n    with self.assertRaises(FieldError):\n        model_admin.get_list_display(request)\n", "def test_dynamic_list_display(self):\n    model_admin = DynamicListDisplayChildAdmin(Child, custom_site)\n    request = self._mocked_authenticated_request('/dynamic_list_display/', self.superuser)\n    changelist = model_admin.get_changelist_instance(request)\n    # Make sure the correct lookups are present in the stateful list display.\n    self.assertEqual(model_admin.get_list_display(request), ('name', 'parent'))\n    model_admin.dynamic_list_display = ('name',)\n    self.assertEqual(model_admin.get_list_display(request), ('name',))\n    model_admin.dynamic_list_display = ('parent',)\n    self.assertEqual(model_admin.get_list_display(request), ('parent',))\n    model_admin.dynamic_list_display = ('name', 'parent')\n    self.assertEqual(model_admin.get_list_display(request), ('name', 'parent'))\n    # Make sure all needed lookups are present in the stateful list display.\n    model_admin.dynamic_list_display = ('rank',)\n    self.assertEqual(model_admin.get_list_display(request), ('rank', 'name', 'parent'))\n", "def test_changelist_view_queryset_defer(self):\n    \"\"\"\n    Regression test for #11886: Ensure that the queryset returned by\n    ChangeList.get_queryset() properly defers loading model fields.\n    \"\"\"\n    modeladmin = ChildAdmin(Child, custom_site)\n    request = self._mocked_authenticated_request('/child/', self.superuser)\n    list_display = ['name', 'parent']\n    list_select_related = ['parent']\n    cl = modeladmin.get_changelist_instance(request)\n    cl.list_display = list_display\n    cl.list_select_related = list_select_related\n    queryset = cl.get_queryset(request)\n    self.assertEqual(queryset.deferred_loading[0], (None, [f.name for f in Child._meta.local_fields if f.name not in list_display]))\n", "def test_model_admin_get_search_results(self):\n    model_admin = BandAdmin(Band, custom_site)\n    request = self._mocked_authenticated_request('/bands/', self.superuser)\n\n    queryset = Band.objects.all()\n    search_term = 'test'\n    use_distinct = False\n\n    search_fields = model_admin.get_search_fields(request)\n    ModelAdmin = model_admin.__class__\n\n    # Ensure that a custom ModelAdmin class with no search_fields attribute\n    # doesn't raise an AttributeError.\n    class CustomModelAdmin(ModelAdmin):\n        pass\n\n    custom_model_admin = CustomModelAdmin(Band, custom_site)\n    try:\n        custom_model_admin.get_search_results(request, queryset, search_term)\n    except AttributeError:\n        self.fail('get_search_results() raised an AttributeError')\n\n    # Ensure that the returned queryset contains the expected items.\n    bands = [Band(name='Test Band'), Band(name='Another Test Band')]\n    Band.objects.bulk_create(bands)\n    search_results = model_admin.get_search_results(request, queryset, search_term)\n    self.assertEqual(len(search_results[0]), len(bands))\n    self.assertEqual(use_distinct, search_results[1])\n\n    # Ensure that a single search field is handled correctly.\n    class CustomModelAdmin(ModelAdmin):\n        search_fields = ('name',)\n\n    custom_model_admin = CustomModelAdmin(Band, custom_site)\n    search_results = custom_model_admin.get_search_results(request, queryset, search_term)\n    self.assertEqual(len(search_results[0]), len(bands))\n\n    # Ensure that multiple search fields are handled correctly.\n    class CustomModelAdmin(ModelAdmin):\n        search_fields = ('name', 'genre')\n\n    custom_model_admin = CustomModelAdmin(Band, custom_site)\n    search_results = custom_model_admin.get_search_results(request, queryset, search_term)\n    self.assertEqual(len(search_results[0]), len(bands))\n\n    # Ensure that the 'contains' lookup type is used when searching.\n    # This is done by checking the SQL generated for the query.\n    class CustomModelAdmin(ModelAdmin):\n        search_fields = ('name',)\n\n    custom_model_admin = CustomModelAdmin(Band, custom_site)\n    search_results = custom_model_admin.get_search_results(request, queryset, search_term)\n    sql = str(search_results[0].query)\n    self.assertIn('LIKE', sql)\n    self.assertIn('%{}%'.format(search_term), sql)\n"], "sample_137": ["def test_parse_docstring(self):\n    title, body, metadata = parse_docstring(self.docstring)\n    self.assertEqual(title, \"This __doc__ output is required for testing. I copied this example from `admindocs` documentation. (TITLE)\")\n    self.assertIn(\"Display an individual :model:`myapp.MyModel`.\", body)\n    self.assertIn(\"**Context**\", body)\n    self.assertIn(\":template:`myapp/my_template.html` (DESCRIPTION)\", body)\n    self.assertEqual(metadata, {\"some_metadata\": \"some data\"})\n", "def test_parse_docstring(self):\n    title, body, metadata = parse_docstring(self.docstring)\n    self.assertEqual(title, \"This __doc__ output is required for testing. I copied this example from `admindocs` documentation. (TITLE)\")\n    self.assertIn(\"Display an individual :model:`myapp.MyModel`.\", body)\n    self.assertIn(\"some_metadata: some data\", metadata)\n", "def test_parse_docstring(self):\n    title, body, metadata = parse_docstring(self.docstring)\n    self.assertEqual(title, \"This __doc__ output is required for testing. I copied this example from `admindocs` documentation. (TITLE)\")\n    self.assertIn(\"Display an individual :model:`myapp.MyModel`.\", body)\n    self.assertEqual(metadata, {\"some_metadata\": \"some data\"})\n", "def test_parse_docstring(self):\n    title, body, metadata = parse_docstring(self.docstring)\n    self.assertEqual(title, \"This __doc__ output is required for testing. I copied this example from `admindocs` documentation. (TITLE)\")\n    self.assertIn(\"Display an individual :model:`myapp.MyModel`.\", body)\n    self.assertIn(\":template:`myapp/my_template.html` (DESCRIPTION)\", body)\n    self.assertEqual(metadata, {\"some_metadata\": \"some data\"})\n", "def test_parse_docstring(self):\n    title, body, metadata = parse_docstring(self.docstring)\n    self.assertEqual(title, \"This __doc__ output is required for testing. I copied this example from `admindocs` documentation. (TITLE)\")\n    self.assertIn(\"Display an individual :model:`myapp.MyModel`.\", body)\n    self.assertEqual(metadata, {\"some_metadata\": \"some data\"})\n"], "sample_138": ["    def setUp(self):\n        super().setUp()\n        self.storage = storage.HashedFilesStorage(location=self.collection_root)\n        self.hashed_files = {}\n", "    def setUp(self):\n        super().setUp()\n        self.storage = storage.StaticFilesStorage(location=self.temp_dir)\n", "    def test_huge_files(self):\n        \"\"\"\n        Huge files (> 2GB) should be hashable.\n        \"\"\"\n        huge_file = Path(tempfile.gettempdir()) / 'huge_file.txt'\n        with huge_file.open('w') as f:\n            f.write('0' * (2**31))\n\n        storage.StaticFilesStorage().save('huge_file.txt', huge_file.open('rb'))\n\n        hashed_name = storage.staticfiles_storage.hashed_name('huge_file.txt')\n\n        self.assertEqual(\n            hashed_name,\n            storage.staticfiles_storage.hashed_name(hashed_name),\n        )\n", "    def test_post_process(self):\n        # Create a file and save it to the storage\n        filename = 'test.txt'\n        with tempfile.NamedTemporaryFile() as tmp:\n            tmp.write(b'Hello, World!')\n            tmp.flush()\n            self.storage.save(filename, tmp)\n\n        # Get the hashed filename\n        hashed_filename = storage.staticfiles_storage.hashed_name(filename)\n\n        # Check that the hashed file exists\n        self.assertTrue(storage.staticfiles_storage.exists(hashed_filename))\n\n        # Check that the original file still exists\n        self.assertTrue(storage.staticfiles_storage.exists(filename))\n", "    def test_post_process(self):\n        self.addCleanup(mock.patch.stopall)\n        with mock.patch('django.contrib.staticfiles.storage.settings', DEBUG=False):\n            hashed_files = storage.HashedFilesMixin()\n            files = {\n                'file.txt': ('storage', '/path/to/file.txt'),\n                'subdir/file.txt': ('storage', '/path/to/subdir/file.txt'),\n            }\n            processed_files = list(hashed_files.post_process(files))\n            self.assertEqual(len(processed_files), len(files))\n\n            # Check the files were \"post-processed\" and their paths updated.\n            for path, (storage, original_path) in files.items():\n                hashed_name = hashed_files.hashed_name(path)\n                self.assertEqual((path, hashed_name, True), processed_files.pop(0))\n"], "sample_140": ["    def test_sanitize_specified_variables(self):\n        @sensitive_variables('password', 'credit_card')\n            pass\n\n        wrapped = my_function.__wrapped__\n        self.assertEqual(wrapped.sensitive_variables, ('password', 'credit_card'))\n", "    def test_view_with_sensitive_variables(self):\n        @sensitive_variables('user', 'password')\n            self.assertIn('user', my_function.sensitive_variables)\n            self.assertIn('password', my_function.sensitive_variables)\n            return user, password\n\n        my_function('jacob', 'top_secret')\n", "    def test_explicit_variable_names(self):\n        @sensitive_variables('user', 'password')\n            self.assertEqual(my_function.sensitive_variables, ['user', 'password'])\n\n        my_function('jacob', 'secret')\n", "    def test_explicit_variable_names(self):\n        @sensitive_variables('user', 'password')\n            self.assertEqual(my_function.sensitive_variables, ('user', 'password'))\n        my_function('jacob', 'secret')\n", "    def test_specified_sensitive_variables(self):\n        @sensitive_variables('user', 'password')\n            return user, password\n\n        self.assertEqual(my_function.sensitive_variables, ('user', 'password'))\n"], "sample_141": ["    def test_progress_bar(self):\n        output = []\n        progress_bar = serializers.base.ProgressBar(output.append, 10)\n        for i in range(10):\n            progress_bar.update(i + 1)\n        self.assertEqual(len(output), 10)\n        self.assertIn('[..........' + ' ' * (serializers.base.ProgressBar.progress_width - 1) + ']', output[0])\n        self.assertIn('[' + '.' * serializers.base.ProgressBar.progress_width + ']\\n', output[-1])\n", "def test_json_serializer_handle_field(self):\n    class TestModel(models.Model):\n        name = models.CharField(max_length=100)\n        value = models.IntegerField()\n\n    obj = TestModel(name='Test', value=42)\n\n    class TestSerializer(serializers.base.Serializer):\n            self.stream.write(json.dumps({field.name: getattr(obj, field.name)}))\n\n    stream = StringIO()\n    serializer = TestSerializer()\n    serializer.serialize([obj], stream=stream)\n\n    self.assertEqual(json.loads(stream.getvalue()), {'name': 'Test', 'value': 42})\n", "    def test_deserializer_exception(self):\n        data = '[{\"pk\": 1, \"model\": \"serializers.category\", \"fields\": {\"name\": \"Reference\"}}]'\n        deserializer = serializers.get_deserializer('json')()\n        with self.assertRaises(DeserializationError):\n            deserializer(data, ignorenonexistent=True)\n", "    def test_update(self):\n        output = []\n        progress_bar = serializers.base.ProgressBar(output.append, 10)\n        for i in range(11):\n            progress_bar.update(i)\n        self.assertEqual(len(output), 10)\n        self.assertEqual(output[-1], '\\n')\n", "def test_deserialize_m2m_values(self):\n    # Create a model instance with a many-to-many field\n    article = self.article_model.objects.create(headline='Test Article')\n    category1 = self.category_model.objects.create(name='Category 1')\n    category2 = self.category_model.objects.create(name='Category 2')\n    article.categories.add(category1, category2)\n\n    # Serialize the model instance to JSON\n    serial_str = serializers.serialize('json', [article], use_natural_foreign_keys=True)\n\n    # Deserialize the JSON string back into a model instance\n    deserialized_objects = serializers.deserialize('json', serial_str, ignorenonexistent=True)\n\n    # Check that the many-to-many values were deserialized correctly\n    for deserialized_object in deserialized_objects:\n        self.assertEqual(deserialized_object.object.categories.count(), 2)\n        self.assertIn(category1, deserialized_object.object.categories.all())\n        self.assertIn(category2, deserialized_object.object.categories.all())\n"], "sample_142": ["def test_modeladmin_check_register_with_invalid_form(self):\n    class InvalidForm(forms.ModelForm):\n        class Meta:\n            model = Song\n            fields = ['invalid_field']\n\n    class MyAdmin(admin.ModelAdmin):\n        form = InvalidForm\n\n    admin_site = AdminSite()\n    errors = admin_site.check(admin_checks.models.Author)\n    self.assertEqual(len(errors), 0)\n\n    errors = admin_site.check(admin_checks.models.Song)\n    self.assertEqual(len(errors), 1)\n    error = errors[0]\n    self.assertEqual(error.id, 'admin.E016')\n    self.assertEqual(error.msg, \"The value of 'form' must inherit from 'BaseModelForm'.\")\n    self.assertEqual(error.href, 'https://docs.djangoproject.com/en/dev/ref/contrib/admin/#django.contrib.admin.ModelAdmin.form')\n    self.assertEqual(error.obj, MyAdmin)\n    self.assertEqual(error.id, 'admin.E016')\n", "def test_modelform_defines_fields(self):\n    class MyModelForm(forms.ModelForm):\n        class Meta:\n            model = Album\n            fields = ('name',)\n\n    self.assertTrue(modelform_defines_fields(MyModelForm))\n\n    class MyModelForm(forms.ModelForm):\n        class Meta:\n            model = Album\n            exclude = ('name',)\n\n    self.assertTrue(modelform_defines_fields(MyModelForm))\n\n    class MyModelForm(forms.ModelForm):\n        class Meta:\n            model = Album\n\n    self.assertFalse(modelform_defines_fields(MyModelForm))\n", "def test_modelform_defines_fields(self):\n    class MyModelForm(forms.ModelForm):\n        class Meta:\n            model = Album\n            fields = ('name',)\n\n    self.assertTrue(modelform_defines_fields(MyModelForm))\n\n    class MyModelForm(forms.ModelForm):\n        class Meta:\n            model = Album\n            exclude = ('name',)\n\n    self.assertTrue(modelform_defines_fields(MyModelForm))\n\n    class MyModelForm(forms.ModelForm):\n        class Meta:\n            model = Album\n\n    self.assertFalse(modelform_defines_fields(MyModelForm))\n", "def test_model_form_defines_fields(self):\n    class MyModelForm(forms.ModelForm):\n        class Meta:\n            model = Song\n            fields = ('title',)\n\n    self.assertTrue(modelform_defines_fields(MyModelForm))\n\n    class MyModelForm(forms.ModelForm):\n        class Meta:\n            model = Song\n            exclude = ('title',)\n\n    self.assertTrue(modelform_defines_fields(MyModelForm))\n\n    class MyModelForm(forms.ModelForm):\n        class Meta:\n            model = Song\n\n    self.assertFalse(modelform_defines_fields(MyModelForm))\n", "def test_model_form_checks(self):\n    class SongForm(forms.ModelForm):\n        pass\n\n    class InvalidFields(admin.ModelAdmin):\n        form = SongForm\n\n    class InvalidFieldsets(admin.ModelAdmin):\n            class ExtraFieldForm(SongForm):\n                name = forms.CharField(max_length=50)\n            return ExtraFieldForm\n\n        fieldsets = (\n            (None, {\n                'fields': ('name',),\n            }),\n        )\n\n    errors = checks.run_checks()\n    self.assertEqual(len(errors), 2)\n\n    invalid_fields_errors = errors[0]\n    self.assertEqual(invalid_fields_errors.id, 'admin.E016')\n    self.assertEqual(len(invalid_fields_errors.msg), 1)\n    self.assertIn(\"The value of 'fields' must be a list or tuple.\", invalid_fields_errors.msg[0])\n\n    invalid_fieldsets_errors = errors[1]\n    self.assertEqual(invalid_fieldsets_errors.id, 'admin.E017')\n    self.assertEqual(len(invalid_fieldsets_errors.msg), 1)\n    self.assertIn(\"The value of 'fieldsets' must contain tuples.\", invalid_fieldsets_errors.msg[0])\n"], "sample_143": ["def test_capfirst(self):\n    self.assertEqual(text.capfirst('hello world'), 'Hello world')\n    self.assertEqual(text.capfirst('HELLO WORLD'), 'HELLO WORLD')\n    self.assertEqual(text.capfirst(''), '')\n    self.assertEqual(text.capfirst('a'), 'A')\n", "def test_wrap(self):\n    self.assertEqual(text.wrap('this is a long string', 10), 'this is a \\nlong string')\n    self.assertEqual(text.wrap('this is a long string with spaces', 10), 'this is a \\nlong string\\nwith spaces')\n    self.assertEqual(text.wrap('short', 10), 'short')\n    self.assertEqual(text.wrap('', 10), '')\n    self.assertEqual(text.wrap('   ', 10), '   ')\n    self.assertEqual(text.wrap('a' * 100, 1), '\\n'.join(['a'] * 100))\n", "def test_unescape_string_literal(self):\n    self.assertEqual(text.unescape_string_literal('\"abc\"'), 'abc')\n    self.assertEqual(text.unescape_string_literal(\"'abc'\"), 'abc')\n    self.assertEqual(text.unescape_string_literal('\"a \\\\\"bc\\\\\"\"'), 'a \"bc\"')\n    self.assertEqual(text.unescape_string_literal(\"'\\\\'ab\\\\' c'\"), \"'ab' c\")\n\n    with self.assertRaises(ValueError):\n        text.unescape_string_literal('abc')\n\n    with self.assertRaises(ValueError):\n        text.unescape_string_literal('\"abc')\n", "def test_capfirst(self):\n    self.assertEqual(text.capfirst('hello'), 'Hello')\n    self.assertEqual(text.capfirst('HELLO'), 'HELLO')\n    self.assertEqual(text.capfirst('hello world'), 'Hello world')\n    self.assertEqual(text.capfirst(''), '')\n    self.assertEqual(text.capfirst(None), '')\n", "def test_capfirst(self):\n    self.assertEqual(text.capfirst(\"hello world\"), \"Hello world\")\n    self.assertEqual(text.capfirst(\"\"), \"\")\n    self.assertEqual(text.capfirst(\"h\"), \"H\")\n    self.assertEqual(text.capfirst(\"hello   world\"), \"Hello   world\")\n    self.assertEqual(text.capfirst(\"multiple   spaces   and   words\"), \"Multiple   spaces   and   words\")\n"], "sample_144": ["def test_model_inheritance_check_constraints(self):\n    class ModelWithConstraints(Model):\n        class Meta:\n            constraints = [\n                CheckConstraint(check=models.Q(age__gte=18), name='age_gte_18')\n            ]\n\n    errors = ModelWithConstraints.check()\n    self.assertEqual(len(errors), 1)\n    self.assertIsInstance(errors[0], checks.Warning)\n    self.assertEqual(errors[0].id, 'models.W027')\n", "    def testModelStateFieldCacheDescriptor(self):\n        # Create a new model instance to test ModelStateFieldCacheDescriptor\n        person = Person()\n\n        # Check if the fields_cache is initially empty\n        self.assertEqual(person.fields_cache, {})\n\n        # Set some attributes on the model instance\n        person.first_name = 'John'\n        person.last_name = 'Doe'\n\n        # Check if the fields_cache has been populated with the set attributes\n        self.assertEqual(person.fields_cache, {'first_name': 'John', 'last_name': 'Doe'})\n\n        # Modify an attribute and check if the fields_cache is updated\n        person.first_name = 'Jane'\n        self.assertEqual(person.fields_cache, {'first_name': 'Jane', 'last_name': 'Doe'})\n", "def test_model_inheritance_with_m2m_fields(self):\n    # Test that a model with an M2M field can be inherited from, and that the\n    # M2M field is correctly created on the child model.\n    child = Child.objects.create(name='Child 1')\n    m2m_child = M2MChild.objects.create(name='M2M Child 1')\n    child.m2m_children.add(m2m_child)\n    self.assertEqual(child.m2m_children.count(), 1)\n    self.assertEqual(child.m2m_children.all()[0].name, 'M2M Child 1')\n", "def test_deferred_fields(self):\n    # Check that deferred fields on a model work correctly when using model\n    # inheritance. See #13816.\n\n    # Create an instance of the Child model (which has deferred fields), and\n    # check that trying to access those fields results in a database query.\n    child = Child.objects.create(name='Child 1', age=12)\n    with self.assertNumQueries(1):\n        child.parent_field\n\n    # Now try accessing the deferred fields on the base class, and check that\n    # this also results in a database query.\n    parent = Parent.objects.get(id=child.id)\n    with self.assertNumQueries(1):\n        parent.parent_field\n", "def test_deferred_model_methods(self):\n    # Test that a model with deferred fields can still use its methods.\n    restaurant = Restaurant.objects.defer('name').get(id=1)\n    self.assertIsInstance(restaurant.get_absolute_url(), str)\n    self.assertIsInstance(str(restaurant), str)\n"], "sample_145": ["    def test_fieldset_valid(self):\n        class ModelAdmin(ModelAdmin):\n            fieldsets = [\n                (\"General\", {\"fields\": [\"name\"]}),\n                (None, {\"fields\": [\"status\"]}),\n            ]\n\n        self.assertIsValid(ModelAdmin, Band)\n", "    def test_field_spec_with_invalid_field_name(self):\n        class MyModelAdmin(ModelAdmin):\n            fields = ('non_existent_field',)\n\n        self.assertIsInvalid(\n            MyModelAdmin, Band,\n            \"The value of 'fields' refers to 'non_existent_field', which is not an attribute of 'admin.Band'.\",\n            id='admin.E108',\n        )\n", "    def test_raw_id_fields_valid(self):\n        class RawIdModelAdmin(ModelAdmin):\n            raw_id_fields = ('field1', 'field2')\n\n        self.assertIsValid(RawIdModelAdmin, ValidationTestModel)\n", "    def test_raw_id_fields(self):\n        class RawIdModelAdmin(ModelAdmin):\n            raw_id_fields = (1,)\n\n        self.assertIsInvalid(RawIdModelAdmin, Band, \"The value of 'raw_id_fields' must be a list or tuple.\", 'admin.E001')\n\n        class RawIdModelAdmin(ModelAdmin):\n            raw_id_fields = ('non_existent_field',)\n\n        self.assertIsInvalid(RawIdModelAdmin, Band, \"The value of 'raw_id_fields\\[0\\]' refers to 'non_existent_field', which is not an attribute of 'tests.Band'.\", 'admin.E002')\n\n        class RawIdModelAdmin(ModelAdmin):\n            raw_id_fields = ('name',)\n\n        self.assertIsInvalid(RawIdModelAdmin, Band, \"The value of 'raw_id_fields\\[0\\]' must be a foreign key or a many-to-many field.\", 'admin.E003')\n\n        class RawIdModelAdmin(ModelAdmin):\n            raw_id_fields = ('genres',)\n\n        self.isValid(RawIdModelAdmin, Band)\n", "    def test_raw_id_fields_valid(self):\n        class RawIdModelAdmin(ModelAdmin):\n            raw_id_fields = ('id',)\n\n        self.assertIsValid(RawIdModelAdmin, ValidationTestModel)\n"], "sample_146": ["def test_check_language_settings_consistent_valid(self):\n    self.assertEqual(check_language_settings_consistent(None), [])\n", "def test_check_language_settings_consistent(self):\n    with override_settings(LANGUAGE_CODE='en', LANGUAGES=[('fr', 'French')]):\n        errors = check_language_settings_consistent(None)\n        self.assertEqual(len(errors), 1)\n        self.assertIsInstance(errors[0], Error)\n        self.assertEqual(errors[0].id, 'translation.E004')\n\n    with override_settings(LANGUAGE_CODE='en', LANGUAGES=[('en', 'English')]):\n        errors = check_language_settings_consistent(None)\n        self.assertEqual(len(errors), 0)\n", "def test_check_setting_language_code(self):\n    with override_settings(LANGUAGE_CODE='en'):\n        errors = check_setting_language_code(None)\n        self.assertEqual(errors, [])\n\n    with override_settings(LANGUAGE_CODE=None):\n        errors = check_setting_language_code(None)\n        self.assertEqual(len(errors), 1)\n        self.assertIsInstance(errors[0], Error)\n        self.assertEqual(errors[0].id, 'translation.E001')\n\n    with override_settings(LANGUAGE_CODE=123):\n        errors = check_setting_language_code(None)\n        self.assertEqual(len(errors), 1)\n        self.assertIsInstance(errors[0], Error)\n        self.assertEqual(errors[0].id, 'translation.E001')\n\n    with override_settings(LANGUAGE_CODE=' invalid'):\n        errors = check_setting_language_code(None)\n        self.assertEqual(len(errors), 1)\n        self.assertIsInstance(errors[0], Error)\n        self.assertEqual(errors[0].id, 'translation.E001')\n", "def test_check_setting_language_code(self):\n    @override_settings(LANGUAGE_CODE='en')\n        self.assertEqual(check_setting_language_code(None), [])\n\n    test_valid()\n\n    @override_settings(LANGUAGE_CODE=None)\n        errors = check_setting_language_code(None)\n        self.assertEqual(len(errors), 1)\n        self.assertIsInstance(errors[0], Error)\n        self.assertEqual(errors[0].id, 'translation.E001')\n\n    test_invalid_type()\n\n    @override_settings(LANGUAGE_CODE='e\u00fc')\n        errors = check_setting_language_code(None)\n        self.assertEqual(len(errors), 1)\n        self.assertIsInstance(errors[0], Error)\n        self.assertEqual(errors[0].id, 'translation.E001')\n\n    test_invalid_value()\n", "def test_check_setting_language_code(self):\n    with override_settings(LANGUAGE_CODE='en'):\n        errors = check_setting_language_code(None)\n        self.assertEqual(errors, [])\n\n    with override_settings(LANGUAGE_CODE=None):\n        errors = check_setting_language_code(None)\n        self.assertEqual(len(errors), 1)\n        self.assertIsInstance(errors[0], Error)\n        self.assertEqual(errors[0].id, 'translation.E001')\n\n    with override_settings(LANGUAGE_CODE=123):\n        errors = check_setting_language_code(None)\n        self.assertEqual(len(errors), 1)\n        self.assertIsInstance(errors[0], Error)\n        self.assertEqual(errors[0].id, 'translation.E001')\n\n    with override_settings(LANGUAGE_CODE=' invalid '):\n        errors = check_setting_language_code(None)\n        self.assertEqual(len(errors), 1)\n        self.assertIsInstance(errors[0], Error)\n        self.assertEqual(errors[0].id, 'translation.E001')\n"], "sample_147": ["def test_union_with_empty_queryset(self):\n    qs1 = Number.objects.filter(num__lt=5)\n    qs2 = Number.objects.none()\n    union_qs = qs1.union(qs2)\n    self.assertNumbersEqual(union_qs, [0, 1, 2, 3, 4])\n", "def test_union_ordered(self):\n    qs1 = Number.objects.filter(num__lte=5).order_by('num')\n    qs2 = Number.objects.filter(num__gt=5).order_by('-num')\n    union_qs = qs1.union(qs2)\n    self.assertNumbersEqual(union_qs, [0, 1, 2, 3, 4, 5, 9, 8, 7, 6], ordered=True)\n", "def test_union(self):\n    qs1 = Number.objects.filter(num__lte=5)\n    qs2 = Number.objects.filter(num__gt=5)\n    self.assertNumbersEqual(qs1.union(qs2), range(10))\n\n    qs1 = Number.objects.filter(num__lte=5).values('num')\n    qs2 = Number.objects.filter(num__gt=5).values('num')\n    self.assertQuerysetEqual(qs1.union(qs2), range(10), lambda x: x['num'])\n", "def test_union(self):\n    qs1 = Number.objects.filter(num__lte=5)\n    qs2 = Number.objects.filter(num__gte=5)\n    self.assertNumbersEqual(qs1.union(qs2), list(range(10)), ordered=False)\n    self.assertNumbersEqual(qs1.union(qs2).union(qs1), list(range(10)), ordered=False)\n", "def test_union_with_values(self):\n    qs1 = Number.objects.filter(num__lt=5).values('num')\n    qs2 = Number.objects.filter(num__gt=5).values('num')\n    union_qs = qs1.union(qs2)\n    self.assertEqual(list(union_qs), [{'num': i} for i in range(10)])\n"], "sample_148": ["def test_nested_objects_protected(self):\n    self._connect(0, 1)\n    self._connect(2, 1)\n    self._collect(1)\n    try:\n        with transaction.atomic():\n            self.n.delete()\n    except models.ProtectedError as e:\n        self.assertEqual(set(e.protected_objects), set(self.objs[0:3]))\n    else:\n        self.fail('ProtectedError not raised')\n", "def test_reverse_field_path(self):\n    model = Car\n    path = \"site__location\"\n    reversed_model, reversed_path = reverse_field_path(model, path)\n    self.assertEqual(reversed_model, Location)\n    self.assertEqual(reversed_path, \"car__site\")\n", "def test_nested_objects_edge_cases(self):\n    # Test that NestedObjects handles edge cases correctly\n    self.n.collect([Count.objects.create(num=10)])\n    self.assertEqual(self.n.nested(lambda obj: obj.num), [[10]])\n\n    self.n.collect([])\n    self.assertEqual(self.n.nested(lambda obj: obj.num), [])\n\n    with self.assertRaises(ValueError):\n        self.n.collect(None)\n\n    with self.assertRaises(TypeError):\n        self.n.collect([1, 2, 3])\n", "def test_quote_and_unquote(self):\n    original = \"Hello, world!\"\n    quoted = quote(original)\n    self.assertNotEqual(original, quoted)\n    unquoted = unquote(quoted)\n    self.assertEqual(original, unquoted)\n", "def test_nested_objects_protected(self):\n    n = NestedObjects(using=DEFAULT_DB_ALIAS)\n    p = Count.objects.create(num=0)\n    c1 = Count.objects.create(num=1, parent=p)\n    c2 = Count.objects.create(num=2, parent=p)\n\n    with self.assertRaises(models.ProtectedError):\n        n.collect([p])\n    self.assertEqual(n.protected, {c1, c2})\n"], "sample_151": ["def test_generate_altered_indexes(self):\n    # Make state\n    before_state = self.make_project_state([self.book])\n    after_state = self.make_project_state([self.book_indexes])\n\n    # Create autodetector\n    autodetector = MigrationAutodetector(before_state, after_state)\n\n    # Detect changes\n    changes = autodetector._detect_changes()\n\n    # Assert correct changes\n    self.assertNumberMigrations(changes, 'otherapp', 1)\n    self.assertOperationTypes(changes, 'otherapp', 0, ['AddIndex'])\n    self.assertOperationAttributes(changes, 'otherapp', 0, 0, model_name='book', name='book_title_author_idx')\n", "def test_generate_added_constraints(self):\n    # Create model state with a constraint.\n    operations = [\n        migrations.CreateModel(\n            name='Author',\n            fields=[\n                ('id', models.AutoField(primary_key=True)),\n                ('name', models.CharField(max_length=100)),\n            ],\n            options={\n                'constraints': [\n                    models.CheckConstraint(check=models.Q(name__contains='Bob'), name='name_contains_bob'),\n                ]\n            },\n        )\n    ]\n\n    changes = self.get_changes([self.author_empty], [self.author_name_check_constraint])\n    # Assert there is one migration with two operations (create model, add constraint).\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['CreateModel', 'AddConstraint'])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name='author')\n    self.assertOperationAttributes(changes, 'testapp', 0, 1, model_name='author')\n", "def test_altered_indexes(self):\n    changes = self.get_changes(\n        [self.book],\n        [self.book_indexes],\n    )\n    self.assertNumberMigrations(changes, 'otherapp', 1)\n    self.assertOperationTypes(changes, 'otherapp', 0, ['AddIndex'])\n    self.assertOperationAttributes(changes, 'otherapp', 0, 0, name='book_title_author_idx', fields=['author', 'title'])\n", "def test_swappable_first_key(self):\n    \"\"\"\n    The swappable_first_key method should correctly identify models that are\n    swappable and place them first in lists of created models.\n    \"\"\"\n    # Create a model state with a swappable model\n    model_state = ModelState(\"testapp\", \"Author\", [\n        (\"id\", models.AutoField(primary_key=True)),\n    ], bases=(AbstractBaseUser,))\n    # Create another model state that isn't swappable\n    other_model_state = ModelState(\"testapp\", \"OtherModel\", [\n        (\"id\", models.AutoField(primary_key=True)),\n    ])\n\n    # Get the swappable first key for each model state\n    swappable_key = MigrationAutodetector.swappable_first_key((model_state.app_label, model_state.name))\n    other_key = MigrationAutodetector.swappable_first_key((other_model_state.app_label, other_model_state.name))\n\n    # The swappable model should come before the non-swappable model\n    self.assertLess(swappable_key, other_key)\n", "def test_add_constraints(self):\n    before = [self.author_empty]\n    after = [self.author_name_check_constraint]\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['CreateModel'])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name='author')\n    self.assertEqual(len(changes['testapp'][0].operations[0].options['constraints']), 1)\n    constraint = changes['testapp'][0].operations[0].options['constraints'][0]\n    self.assertIsInstance(constraint, models.CheckConstraint)\n    self.assertEqual(constraint.name, 'name_contains_bob')\n    self.assertEqual(str(constraint.check), \"name__contains='Bob'\")\n"], "sample_149": ["def test_check_user_model_required_fields_is_not_list(self):\n    @self.apps.model\n    class CustomUserNonListRequiredFields(AbstractBaseUser):\n        USERNAME_FIELD = 'username'\n        REQUIRED_FIELDS = 'non_list'\n\n        username = models.CharField(max_length=10, unique=True)\n        non_list = models.CharField(max_length=10)\n\n    errors = checks.run_checks()\n    expected_errors = [\n        checks.Error(\n            \"'REQUIRED_FIELDS' must be a list or tuple.\",\n            obj=CustomUserNonListRequiredFields,\n            id='auth.E001',\n        ),\n    ]\n    self.assertEqual(errors, expected_errors)\n", "def test_check_user_model_required_fields_is_not_list(self):\n    @self.apps.get_model('auth_tests', 'CustomUserNonListRequiredFields')\n    class CustomUserNonListRequiredFields(AbstractBaseUser):\n        REQUIRED_FIELDS = 'non_list'\n\n    errors = check_user_model(app_configs=self.apps.app_configs)\n    expected_errors = [\n        checks.Error(\n            \"'REQUIRED_FIELDS' must be a list or tuple.\",\n            obj=CustomUserNonListRequiredFields,\n            id='auth.E001',\n        ),\n    ]\n    self.assertEqual(errors, expected_errors)\n", "def test_check_user_model_required_fields_not_list(self):\n    @self.apps.register_model\n    class CustomUser(AbstractBaseUser):\n        USERNAME_FIELD = 'username'\n        REQUIRED_FIELDS = 'not a list'\n\n        username = models.CharField(max_length=10, unique=True)\n\n    errors = check_user_model(app_configs=self.apps.app_configs)\n    expected_errors = [\n        checks.Error(\n            \"'REQUIRED_FIELDS' must be a list or tuple.\",\n            obj=CustomUser,\n            id='auth.E001',\n        ),\n    ]\n    self.assertEqual(errors, expected_errors)\n", "def test_check_user_model_required_fields_is_not_list(self):\n    @self.apps.get_model\n    class CustomUserNonListRequiredFields(AbstractBaseUser):\n        REQUIRED_FIELDS = 'non_list'\n        USERNAME_FIELD = 'username'\n\n    errors = checks.run_checks()\n    expected_errors = [\n        checks.Error(\n            \"'REQUIRED_FIELDS' must be a list or tuple.\",\n            obj=CustomUserNonListRequiredFields,\n            id='auth.E001',\n        ),\n    ]\n    self.assertEqual(errors, expected_errors)\n", "def test_check_user_model_required_fields_not_list(self):\n    @self.apps.model\n    class CustomUserNonListRequiredFields(AbstractBaseUser):\n        REQUIRED_FIELDS = 'non_list'\n        USERNAME_FIELD = 'username'\n        username = models.CharField(max_length=10, unique=True)\n\n    errors = checks.run_checks()\n    self.assertEqual(len(errors), 1)\n    error = errors[0]\n    self.assertEqual(error.id, 'auth.E001')\n    self.assertEqual(str(error), \"'REQUIRED_FIELDS' must be a list or tuple.\")\n    self.assertEqual(error.obj, CustomUserNonListRequiredFields)\n"], "sample_152": ["def test_can_fast_delete(self):\n    a = create_a()\n    collector = Collector(using='default')\n    self.assertTrue(collector.can_fast_delete(a))\n    self.assertFalse(collector.can_fast_delete(B.objects.all()))\n", "def test_sort(self):\n    collector = Collector(using='default')\n    a1 = A.objects.create()\n    b1 = B.objects.create(a=a1)\n    c1 = Child.objects.create(b=b1)\n    collector.collect([a1])\n    self.assertEqual(list(collector.data), [A, B, Child])\n    collector.sort()\n    self.assertEqual(list(collector.data), [Child, B, A])\n", "def test_on_delete_do_nothing(self):\n    collector = Collector(using=connection.alias)\n    b = B.objects.create()\n    a = A.objects.create(b=b)\n\n    # Set on_delete to DO_NOTHING for A's foreign key to B\n    A._meta.get_field('b').remote_field.on_delete = models.DO_NOTHING\n\n    # Delete B, which should not delete A due to DO_NOTHING\n    collector.collect([b])\n    with connection.transaction.atomic():\n        collector.delete()\n\n    # Verify that A still exists\n    self.assertEqual(A.objects.count(), 1)\n    self.assertEqual(B.objects.count(), 0)\n", "def test_collector_add_field_update(self):\n    collector = Collector(using=\"default\")\n    model = A\n    field = model._meta.get_field(\"b\")\n    value = \"test_value\"\n    objs = [model.objects.create()]\n    collector.add_field_update(field, value, objs)\n    self.assertEqual(\n        collector.field_updates[model][(field, value)],\n        set(objs),\n    )\n", "def test_on_delete_protect(self):\n    a1 = create_a()\n    b1 = B.objects.create(a=a1)\n    with self.assertRaises(ProtectedError) as cm:\n        a1.delete()\n    self.assertEqual(list(cm.exception.protected_objects), [b1])\n"], "sample_150": ["    def test_command_parser(self):\n        parser = CommandParser()\n        self.assertIsInstance(parser, ArgumentParser)\n        self.assertIsInstance(parser._actions, list)\n", "    def test_output_transaction(self):\n        class TestCommand(BaseCommand):\n            output_transaction = True\n\n                return 'SQL'\n\n        cmd = TestCommand()\n        output = cmd.execute()\n        self.assertRegex(output, r'BEGIN;\\s+SQL\\s+COMMIT;')\n", "    def test_create_parser(self):\n        command = BaseCommand()\n        parser = command.create_parser('manage.py', 'test')\n        self.assertIsInstance(parser, CommandParser)\n        self.assertEqual(parser.prog, 'manage.py test')\n        self.assertEqual(parser.description, '')\n", "    def test_command_error(self):\n        error = CommandError(\"Test error message\")\n        self.assertEqual(str(error), \"Test error message\")\n", "    def test_base_command_has_no_output(self):\n        command = BaseCommand()\n        output = command.handle()\n        self.assertIsNone(output)\n"], "sample_153": ["def test_model_validation(self):\n    class TestModel(models.Model):\n        name = models.CharField(max_length=100)\n        age = models.IntegerField()\n\n        class Meta:\n            app_label = 'tests'\n            unique_together = (('name', 'age'),)\n\n    errors = TestModel.check()\n    self.assertEqual(errors, [])\n\n    # Test that unique_together validation is run correctly.\n    class TestModel(models.Model):\n        name = models.CharField(max_length=100)\n        age = models.IntegerField(unique=True)\n\n        class Meta:\n            app_label = 'tests'\n            unique_together = (('name', 'age'),)\n\n    errors = TestModel.check()\n    self.assertEqual(len(errors), 1)\n    self.assertEqual(errors[0].id, 'models.E012')\n", "def test_database_check_passes(self):\n    with mock.patch.object(connection, 'validation') as mock_validation:\n        mock_validation.check.return_value = []\n        errors = check_database_backends()\n        self.assertEqual(errors, [])\n", "    def test_model_checks_run(self):\n        # Ensure model validation is run as part of the database checks.\n        from django.core.checks.model_checks import check_all_models\n        with mock.patch('django.core.checks.model_checks.check_all_models') as mocked_check:\n            check_database_backends(connection)\n            self.assertEqual(mocked_check.call_count, 1)\n", "def test_check_database_backends_multi_db(self):\n    errors = check_database_backends(database='other')\n    self.assertEqual(errors, [])\n", "def test_check_database_backends(self):\n    @mock.patch('django.db.backends.base.validation.BaseDatabaseValidation.check')\n        mock_check.return_value = []\n        errors = check_database_backends('database.backends')\n        self.assertEqual(errors, [])\n\n    run_test()\n    with self.settings(DATABASES={'default': {'ENGINE': 'database.backends.dummy'}}):\n        run_test()\n"], "sample_154": ["def test_check_database_backends_no_databases(self):\n    with mock.patch('django.db.connections') as connections:\n        issues = check_database_backends()\n        self.assertEqual(issues, [])\n        connections.assert_not_called()\n", "    def test_check_database_backends_no_databases(self):\n        issues = check_database_backends(databases=None)\n        self.assertEqual(issues, [])\n", "    def test_check_database_backends_no_databases(self):\n        issues = check_database_backends()\n        self.assertEqual(issues, [])\n", "def test_check_database_backends_empty_databases(self):\n    issues = check_database_backends(databases=None)\n    self.assertEqual(issues, [])\n", "def test_check_database_backends_no_databases(self):\n    with mock.patch('django.db.connections') as connections_mock:\n        issues = check_database_backends()\n        self.assertEqual(issues, [])\n        connections_mock.assert_not_called()\n"], "sample_155": ["def test_file_response_with_as_attachment(self):\n    file = ContentFile(b'Hello, World!', name='example.txt')\n    response = FileResponse(file, as_attachment=True)\n    self.assertEqual(response['Content-Disposition'], 'attachment; filename=\"example.txt\"')\n", "def test_file_response_with_filename(self):\n    response = FileResponse(io.BytesIO(b'Hello, world!'), filename='example.txt')\n    self.assertEqual(response['Content-Disposition'], 'inline; filename=\"example.txt\"')\n    self.assertEqual(response['Content-Type'], 'text/plain')\n\n    response = FileResponse(io.BytesIO(b'Hello, world!'), filename='example.txt', as_attachment=True)\n    self.assertEqual(response['Content-Disposition'], 'attachment; filename=\"example.txt\"')\n", "def test_file_response_with_filename(self):\n    response = FileResponse(io.BytesIO(b'Hello, world!'), filename='example.txt')\n    self.assertEqual(response['Content-Disposition'], 'inline; filename=\"example.txt\"')\n    self.assertEqual(response['Content-Type'], 'text/plain')\n\n    # Test with a non-ASCII filename.\n    response = FileResponse(io.BytesIO(b'Hello, world!'), filename='ex@mple.txt')\n    self.assertEqual(response['Content-Disposition'], \"inline; filename*=utf-8''ex%40mple.txt\")\n\n    # Test with as_attachment=True.\n    response = FileResponse(io.BytesIO(b'Hello, world!'), as_attachment=True, filename='example.txt')\n    self.assertEqual(response['Content-Disposition'], 'attachment; filename=\"example.txt\"')\n", "def test_file_response_set_headers(self):\n    filelike = ContentFile(b'Hello, World!', name='example.txt')\n    response = FileResponse(filelike)\n    self.assertEqual(response['Content-Type'], 'text/plain')\n    self.assertEqual(response['Content-Length'], str(len(b'Hello, World!')))\n", "def test_file_response_with_non_ascii_filename(self):\n    filename = 'test_\u00f1.txt'\n    t = tempfile.NamedTemporaryFile(mode='wb', delete=False)\n    t.write(b'Hello, world!')\n    t.close()\n\n    response = FileResponse(open(t.name, 'rb'), as_attachment=True, filename=filename)\n\n    self.assertEqual(response['Content-Disposition'], f\"attachment; filename*=utf-8''{filename}\")\n    os.remove(t.name)\n"], "sample_156": ["def test_form_initial(self):\n    # Initial data is provided as a dictionary when the Form is instantiated.\n    p = Person(initial={'first_name': 'John', 'last_name': 'Doe'})\n    self.assertEqual(p.as_table(), '''<tr><th><label for=\"id_first_name\">First name:</label></th><td><input id=\"id_first_name\" name=\"first_name\" type=\"text\" value=\"John\"></td></tr>", "def test_base_form_has_changed(self):\n    # Test that has_changed works with various form data types.\n\n    class HasChangedForm(Form):\n        field1 = CharField()\n        field2 = IntegerField()\n        field3 = FloatField()\n\n    # Changed data\n    form1 = HasChangedForm({'field1': 'new', 'field2': 2, 'field3': 3.0}, initial={'field1': 'old', 'field2': 1, 'field3': 2.0})\n    self.assertTrue(form1.has_changed())\n\n    # No changed data\n    form2 = HasChangedForm({'field1': 'old', 'field2': 1, 'field3': 2.0}, initial={'field1': 'old', 'field2': 1, 'field3': 2.0})\n    self.assertFalse(form2.has_changed())\n\n    # No initial data\n    form3 = HasChangedForm({'field1': 'new', 'field2': 2, 'field3': 3.0})\n    self.assertTrue(form3.has_changed())\n\n    # No data (and no initial data)\n    form4 = HasChangedForm()\n    self.assertFalse(form4.has_changed())\n", "def test_form_add_prefix(self):\n    p = Person(prefix='person')\n    self.assertEqual(p.add_prefix('first_name'), 'person-first_name')\n    self.assertEqual(p.add_prefix('last_name'), 'person-last_name')\n    self.assertEqual(p.add_initial_prefix('first_name'), 'initial-person-first_name')\n\n    p = Person(prefix=None)\n    self.assertEqual(p.add_prefix('first_name'), 'first_name')\n    self.assertEqual(p.add_prefix('last_name'), 'last_name')\n    self.assertEqual(p.add_initial_prefix('first_name'), 'initial-first_name')\n", "def test_full_clean_with_empty_permitted_and_no_changes(self):\n    class TestForm(Form):\n        field = CharField(initial='initial')\n\n    data = {'field': 'initial'}\n    form = TestForm(data, empty_permitted=True)\n    self.assertEqual(form.full_clean(), None)\n    self.assertEqual(form.errors, {})\n", "def test_form_order_fields(self):\n    class TestForm(Form):\n        field1 = CharField()\n        field2 = CharField()\n        field3 = CharField()\n\n    form = TestForm()\n    self.assertEqual(list(form.fields.keys()), ['field1', 'field2', 'field3'])\n\n    form = TestForm(field_order=['field3', 'field2'])\n    self.assertEqual(list(form.fields.keys()), ['field3', 'field2', 'field1'])\n\n    form = TestForm(field_order=['field4', 'field2'])\n    self.assertEqual(list(form.fields.keys()), ['field2', 'field1', 'field3'])\n"], "sample_157": ["    def test_get_test_db_name(self):\n        connection = get_connection_copy()\n        creation = BaseDatabaseCreation(connection)\n\n        # If 'TEST'['NAME'] is not set, the name should be prefixed with 'test_'\n        self.assertEqual(\n            creation._get_test_db_name(),\n            TEST_DATABASE_PREFIX + connection.settings_dict['NAME']\n        )\n\n        # If 'TEST'['NAME'] is set, that name should be used\n        test_name = 'custom_test_name'\n        connection.settings_dict['TEST']['NAME'] = test_name\n        self.assertEqual(creation._get_test_db_name(), test_name)\n", "def test_create_test_db(self):\n    test_connection = get_connection_copy()\n    creation = BaseDatabaseCreation(test_connection)\n    with mock.patch.object(creation, '_create_test_db') as _create_test_db:\n        _create_test_db.return_value = 'test_database'\n        with mock.patch.object(creation, 'log') as log:\n            db_name = creation.create_test_db(verbosity=1, autoclobber=False, serialize=True, keepdb=False)\n            self.assertEqual(db_name, 'test_database')\n            self.assertEqual(log.call_count, 2)\n            log.assert_called_with('Creating test database for alias \\'default\\'...')\n", "def test_create_test_db(self):\n    test_connection = get_connection_copy()\n    creation = BaseDatabaseCreation(test_connection)\n\n    # Mock the _create_test_db method to avoid actual database creation.\n    with mock.patch.object(creation, '_create_test_db', return_value='test_database_name'):\n        with mock.patch.object(creation, 'log'):\n            test_database_name = creation.create_test_db()\n\n    self.assertEqual(test_database_name, 'test_database_name')\n    self.assertEqual(test_connection.settings_dict['NAME'], 'test_database_name')\n", "    def test_create_test_db(self):\n        test_connection = get_connection_copy()\n        creation = BaseDatabaseCreation(test_connection)\n        with mock.patch.object(creation, '_create_test_db') as mock_create_test_db:\n            test_db_name = creation.create_test_db(verbosity=0)\n            self.assertEqual(test_db_name, TEST_DATABASE_PREFIX + test_connection.settings_dict['NAME'])\n            mock_create_test_db.assert_called_once_with(verbosity=0, autoclobber=False, keepdb=False)\n", "    def test_create_test_db(self):\n        test_connection = get_connection_copy()\n        creation = BaseDatabaseCreation(test_connection)\n        with mock.patch.object(creation, '_create_test_db') as _create_test_db:\n            _create_test_db.return_value = 'test_database'\n            db_name = creation.create_test_db(verbosity=0, autoclobber=True)\n            self.assertEqual(db_name, 'test_database')\n            self.assertEqual(test_connection.settings_dict['NAME'], 'test_database')\n"], "sample_158": ["def test_check_field_name_clashes(self):\n    model = ModelState(\n        app_label='app',\n        name='Model',\n        fields=[\n            ('id', models.AutoField(primary_key=True)),\n            ('m2m', models.ManyToManyField('self')),\n            ('field', models.IntegerField()),\n        ],\n    )\n    field = model.get_field('m2m')\n    self.assertEqual(\n        field.check(),\n        [\n            Error(\n                \"Reverse accessor for 'Model.m2m' clashes with field name 'Model.field'.\",\n                hint=\"Rename field 'Model.field', or add/change a related_name argument to the definition for field 'Model.m2m'.\",\n                obj=field,\n                id='fields.E302',\n            ),\n        ],\n    )\n", "def test_many_to_many_field_checks(self):\n    class Model(models.Model):\n        m2m = models.ManyToManyField('self', through='Intermediate')\n\n    class Intermediate(models.Model):\n        src = models.ForeignKey(Model, on_delete=models.CASCADE)\n        dst = models.ForeignKey(Model, on_delete=models.CASCADE)\n\n    field = Model._meta.get_field('m2m')\n    errors = field.check(from_model=Model)\n    self.assertEqual(errors, [])\n\n    # Test missing through model.\n    class MissingThroughModel(models.Model):\n        m2m = models.ManyToManyField('self', through='MissingIntermediate')\n\n    field = MissingThroughModel._meta.get_field('m2m')\n    errors = field.check(from_model=MissingThroughModel)\n    self.assertEqual(len(errors), 1)\n    self.assertIsInstance(errors[0], Error)\n    self.assertEqual(errors[0].id, 'fields.E331')\n", "def test_field_name_length(self):\n    class Model(models.Model):\n        field = models.ForeignKey('self', on_delete=models.CASCADE)\n\n    field = Model._meta.get_field('field')\n    errors = field.check()\n    self.assertEqual(errors, [\n        Error(\n            'The name \\'field\\' is invalid related_name for field Model.field',\n            hint='Related name must be a valid Python identifier or end with a \\'+\\'',\n            obj=field,\n            id='fields.E306',\n        )\n    ])\n", "def test_related_name_lazy_references(self):\n    class Model(models.Model):\n        field = models.ForeignKey('self', on_delete=models.CASCADE, related_name='rel')\n\n    errors = Model.check()\n    self.assertEqual(errors, [])\n", "def test_invalid_related_name(self):\n    class Model(models.Model):\n        field = models.ForeignKey('self', related_name='invalid+')\n\n    errors = Model.check()\n    expected_errors = [\n        Error(\n            \"The name 'invalid+' is invalid related_name for field Model.field\",\n            hint=\"Related name must be a valid Python identifier or end with a '+'\",\n            obj=Model._meta.get_field('field'),\n            id='fields.E306',\n        )\n    ]\n    self.assertEqual(errors, expected_errors)\n"], "sample_159": ["def test_check_user_model_required_fields_is_list(self):\n    @self.apps.register_model\n    class CustomUser(AbstractBaseUser):\n        REQUIRED_FIELDS = 'not a list'\n\n    errors = check_user_model(app_configs=self.apps.app_configs)\n    expected_error = checks.Error(\n        \"'REQUIRED_FIELDS' must be a list or tuple.\",\n        obj=CustomUser,\n        id='auth.E001',\n    )\n    self.assertEqual(errors, [expected_error])\n", "def test_check_user_model_required_fields_not_list(self):\n    @self.apps.get_model('CustomUserNonListRequiredFields')\n    class CustomUserNonListRequiredFields(AbstractBaseUser):\n        USERNAME_FIELD = 'username'\n        REQUIRED_FIELDS = 'email'  # not a list\n\n        username = models.CharField(max_length=10, unique=True)\n        email = models.EmailField()\n\n    errors = checks.run_checks()\n    expected_errors = [\n        checks.Error(\n            \"'REQUIRED_FIELDS' must be a list or tuple.\",\n            obj=CustomUserNonListRequiredFields,\n            id='auth.E001',\n        ),\n    ]\n    self.assertEqual(errors, expected_errors)\n", "def test_user_model_required_fields_is_not_list(self):\n    errors = checks.run_checks()\n    self.assertEqual(len(errors), 1)\n    self.assertEqual(errors[0].id, 'auth.E001')\n    self.assertEqual(errors[0].msg, \"'REQUIRED_FIELDS' must be a list or tuple.\")\n", "def test_check_user_model_username_field_not_unique(self):\n    errors = check_user_model(apps=self.apps)\n    self.assertEqual(len(errors), 1)\n    self.assertEqual(errors[0].id, 'auth.W004')\n    self.assertIn(\"USERNAME_FIELD\", errors[0].msg)\n    self.assertIn(\"not unique\", errors[0].msg)\n", "def test_check_user_model_required_fields_not_list(self):\n    @self.apps.get_model\n    class CustomUser(AbstractBaseUser):\n        REQUIRED_FIELDS = 'non_list'\n\n    errors = check_user_model(apps=self.apps)\n    self.assertEqual(len(errors), 1)\n    self.assertEqual(errors[0].id, 'auth.E001')\n"], "sample_160": ["def test_decimal_pos_rounding(self):\n    self.assertEqual(nformat(123.456, decimal_sep='.', decimal_pos=2), '123.46')\n    self.assertEqual(nformat(123.456, decimal_sep='.', decimal_pos=1), '123.5')\n    self.assertEqual(nformat(123.456, decimal_sep='.', decimal_pos=0), '123')\n    self.assertEqual(nformat(Decimal('123.456'), decimal_sep='.', decimal_pos=2), '123.46')\n    self.assertEqual(nformat(Decimal('123.456'), decimal_sep='.', decimal_pos=1), '123.5')\n    self.assertEqual(nformat(Decimal('123.456'), decimal_sep='.', decimal_pos=0), '123')\n", "def test_decimal_pos_rounding(self):\n    self.assertEqual(nformat(123.456, '.', decimal_pos=2), '123.46')\n    self.assertEqual(nformat(123.456, '.', decimal_pos=1), '123.5')\n    self.assertEqual(nformat(Decimal('123.456'), '.', decimal_pos=2), '123.46')\n    self.assertEqual(nformat(Decimal('123.456'), '.', decimal_pos=1), '123.5')\n", "def test_decimal_pos_with_large_numbers(self):\n    # Test that decimal positions are handled correctly for large numbers\n    self.assertEqual(nformat(123456789012.12345, '.', 3), '123,456,789,012.123')\n    self.assertEqual(nformat(Decimal('123456789012.12345'), '.', 3), '123,456,789,012.123')\n    self.assertEqual(nformat(123456789012.12345, '.', 5), '123,456,789,012.12345')\n    self.assertEqual(nformat(Decimal('123456789012.12345'), '.', 5), '123,456,789,012.12345')\n", "def test_decimal_pos_with_large_numbers(self):\n    large_number = Decimal('12345678901234567890.12345')\n    result = nformat(large_number, decimal_sep='.', decimal_pos=3)\n    self.assertEqual(result, '12,345,678,901,234,567,890.123')\n", "def test_decimal_pos(self):\n    self.assertEqual(nformat(123.456, decimal_sep='.', decimal_pos=2), '123.46')\n    self.assertEqual(nformat(123.456, decimal_sep='.', decimal_pos=0), '123')\n    self.assertEqual(nformat(123.456, decimal_sep='.', decimal_pos=None), '123.456')\n    self.assertEqual(nformat(Decimal('123.456'), decimal_sep='.', decimal_pos=2), '123.46')\n    self.assertEqual(nformat(Decimal('123.456'), decimal_sep='.', decimal_pos=0), '123')\n    self.assertEqual(nformat(Decimal('123.456'), decimal_sep='.', decimal_pos=None), '123.456')\n"], "sample_161": ["def test_related_name_is_valid(self):\n    class Model(models.Model):\n        field = models.ForeignKey('self', on_delete=models.CASCADE, related_name='invalid+name')\n\n    errors = Model.check()\n    expected_errors = [\n        Error(\n            \"The name 'invalid+name' is invalid related_name for field Model.field\",\n            hint=\"Related name must be a valid Python identifier or end with a '+'\",\n            obj=Model.field,\n            id='fields.E306',\n        )\n    ]\n    self.assertEqual(errors, expected_errors)\n", "def test_check_related_name_is_valid(self):\n    class Model(models.Model):\n        field = models.ForeignKey('self', on_delete=models.CASCADE, related_name='invalid+name')\n\n    errors = Model.check()\n    expected_errors = [\n        Error(\n            \"The name 'invalid+name' is invalid related_name for field Model.field\",\n            hint=\"Related name must be a valid Python identifier or end with a '+'\",\n            obj=Model._meta.get_field('field'),\n            id='fields.E306',\n        )\n    ]\n    self.assertEqual(errors, expected_errors)\n", "def test_related_name_is_valid_python_identifier(self):\n    class Model(models.Model):\n        field = models.ForeignKey('self', related_name='123invalid+')\n\n    errors = Model.check()\n    expected_errors = [\n        Error(\n            \"The name '123invalid+' is invalid related_name for field Model.field\",\n            hint=\"Related name must be a valid Python identifier or end with a '+'\",\n            obj=Model.field,\n            id='fields.E306',\n        )\n    ]\n    self.assertEqual(errors, expected_errors)\n", "def test_recursive_foreign_key(self):\n    class Model(models.Model):\n        fk = models.ForeignKey('self', on_delete=models.CASCADE)\n\n    field = Model._meta.get_field('fk')\n    errors = field.check()\n    self.assertEqual(errors, [])\n\n    # A recursive foreign key with related_name='+' should not raise an error.\n    class Model(models.Model):\n        fk = models.ForeignKey('self', on_delete=models.CASCADE, related_name='+')\n\n    field = Model._meta.get_field('fk')\n    errors = field.check()\n    self.assertEqual(errors, [])\n\n    # A recursive foreign key with related_name should not raise an error.\n    class Model(models.Model):\n        fk = models.ForeignKey('self', on_delete=models.CASCADE, related_name='test')\n\n    field = Model._meta.get_field('fk')\n    errors = field.check()\n    self.assertEqual(errors, [])\n\n    # A recursive foreign key with the same related name as another should\n    # raise an error.\n    class Model(models.Model):\n        fk1 = models.ForeignKey('self', on_delete=models.CASCADE, related_name='test')\n        fk2 = models.ForeignKey('self', on_delete=models.CASCADE, related_name='test')\n\n    field = Model._meta.get_field('fk2')\n    errors = field.check()\n    expected_errors = [\n        Error(\n            \"Reverse accessor for 'Model.fk2' clashes with reverse accessor for 'Model.fk1'.\",\n            hint=(\"Add or change a related_name argument to the definition for \"\n                  \"'Model.fk2' or 'Model.fk1'.\"),\n            obj=field,\n            id='fields.E304',\n        )\n    ]\n    self.assertEqual(errors, expected_errors)\n", "def test_resolve_relation(self):\n    with mock.patch('django.db.models.options.Options.get_field') as get_field:\n        model = models.Model()\n        get_field.return_value.remote_field.model = 'app_label.ModelName'\n        self.assertEqual(resolve_relation(model, 'ModelName'), 'app_label.ModelName')\n        self.assertEqual(resolve_relation(model, 'self'), model)\n        self.assertEqual(resolve_relation(model, 'app_label.ModelName'), 'app_label.ModelName')\n"], "sample_162": ["    def test_write_po_file(self):\n        potfile = 'locale/django.pot'\n        locale = 'de'\n        with open(potfile, 'w') as f:\n            f.write('# SOME DESCRIPTIVE TITLE.\\n')\n            f.write('# Copyright (C) YEAR THE PACKAGE\\'S COPYRIGHT HOLDER\\n')\n            f.write('# This file is distributed under the same license as the PACKAGE package.\\n')\n            f.write('# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.\\n')\n            f.write('msgid \"\"\\n')\n            f.write('msgstr \"\"\\n')\n            f.write('\"Content-Type: text/plain; charset=UTF-8\\\\n\"\\n')\n            f.write('\"Content-Transfer-Encoding: 8bit\\\\n\"\\n')\n            f.write('\"Project-Id-Version: Django\\\\n\"\\n')\n            f.write('\"Language: \\\\n\"\\n')\n        MakeMessagesCommand().write_po_file(potfile, locale)\n        po_file = 'locale/de/LC_MESSAGES/django.po'\n        self.assertTrue(os.path.exists(po_file))\n        with open(po_file) as f:\n            contents = f.read()\n        self.assertIn('msgid \"\"', contents)\n        self.assertIn('msgstr \"\"', contents)\n", "def test_makemessages_pot_file_header(self):\n    # Make sure the pot file header contains the correct charset and plural forms\n    with captured_stdout() as stdout:\n        management.call_command('makemessages', locale=[LOCALE], verbosity=2)\n    output = stdout.getvalue()\n    self.assertTrue(os.path.exists(self.PO_FILE))\n    with open(self.PO_FILE) as fp:\n        po_contents = fp.read()\n    self.assertRegex(po_contents, r'Content-Type: text/plain; charset=UTF-8')\n    self.assertRegex(po_contents, r'\"Plural-Forms.*\"')\n", "    def test_write_pot_file(self):\n        pot_contents = \"\"\"\\", "def test_handle_no_locale(self):\n    with captured_stderr() as stderr, captured_stdout() as stdout:\n        management.call_command('makemessages', locale=[], verbosity=2)\n    self.assertIn('You must run this command with one of either the --locale, --exclude, or --all options.', stderr.getvalue())\n    self.assertEqual(stdout.getvalue(), '')\n", "def test_build_potfiles(self):\n    with mock.patch('django.core.management.commands.makemessages.find_files') as find_files:\n        find_files.return_value = [\n            MakeMessagesCommand.translatable_file_class('.', 'test.py', '.'),\n        ]\n        potfiles = MakeMessagesCommand().build_potfiles()\n        self.assertEqual(len(potfiles), 1)\n        self.assertTrue(os.path.exists(potfiles[0]))\n"], "sample_163": ["    def test_password_reset_confirm_view_valid(self):\n        # Create a password reset token\n        token = default_token_generator.make_token(self.u1)\n        uidb64 = urlsafe_base64_encode(str(self.u1.pk).encode())\n\n        # Get the password reset confirm view\n        response = self.client.get(\n            reverse(\"password_reset_confirm\", kwargs={\"uidb64\": uidb64, \"token\": token})\n        )\n\n        # Check that the view is rendered correctly\n        self.assertEqual(response.status_code, 200)\n        self.assertTemplateUsed(response, \"registration/password_reset_confirm.html\")\n\n        # Post a new password to the view\n        response = self.client.post(\n            reverse(\"password_reset_confirm\", kwargs={\"uidb64\": uidb64, \"token\": token}),\n            {\"new_password1\": \"newpassword\", \"new_password2\": \"newpassword\"},\n        )\n\n        # Check that the password has been changed\n        self.u1.refresh_from_db()\n        self.assertTrue(self.u1.check_password(\"newpassword\"))\n\n        # Check that the user is redirected to the login page\n        self.assertRedirects(response, reverse(\"login\"))\n", "def test_password_reset_complete_view(self):\n    response = self.client.get(reverse(\"password_reset_complete\"))\n    self.assertEqual(response.status_code, 200)\n    self.assertTemplateUsed(response, \"registration/password_reset_complete.html\")\n\n    # Test that the login URL is in the context.\n    self.assertIn(\"login_url\", response.context)\n    self.assertEqual(response.context[\"login_url\"], reverse(settings.LOGIN_URL))\n", "def test_password_reset_confirm_valid_token(self):\n    # Create a password reset token\n    token = default_token_generator.make_token(self.u1)\n\n    # Create a valid password reset link\n    uidb64 = urlsafe_base64_encode(str(self.u1.pk).encode())\n    link = reverse(\"password_reset_confirm\", args=[uidb64, token])\n\n    # Follow the password reset link\n    response = self.client.get(link)\n\n    # Check that we're on the password reset confirm page\n    self.assertEqual(response.status_code, 200)\n    self.assertTemplateUsed(response, \"registration/password_reset_confirm.html\")\n\n    # Check that the form is valid\n    form = response.context[\"form\"]\n    self.assertIsInstance(form, SetPasswordForm)\n\n    # Set a new password\n    data = {\"new_password1\": \"new_password\", \"new_password2\": \"new_password\"}\n    response = self.client.post(link, data)\n\n    # Check that we've been redirected to the password reset complete page\n    self.assertRedirects(response, reverse(\"password_reset_complete\"))\n", "def test_password_reset_complete_view(self):\n    \"\"\"\n    Test that the password reset complete view renders the correct template.\n    \"\"\"\n    url = reverse(\"password_reset_complete\")\n    response = self.client.get(url)\n    self.assertEqual(response.status_code, 200)\n    self.assertTemplateUsed(response, \"registration/password_reset_complete.html\")\n\n    # Test that the login URL is in the context.\n    self.assertIn(\"login_url\", response.context)\n    self.assertEqual(response.context[\"login_url\"], reverse(settings.LOGIN_URL))\n", "    def test_login_view_render(self):\n        response = self.client.get(reverse(\"login\"))\n        self.assertEqual(response.status_code, 200)\n        self.assertTemplateUsed(response, \"registration/login.html\")\n"], "sample_164": ["    def setUp(self):\n        self.logger = logging.getLogger('django.request')\n        self.handler = AdminEmailHandler()\n        self.logger.handlers = [self.handler]\n", "    def test_emit(self):\n        logger = logging.getLogger('test_admin_email_handler')\n        handler = AdminEmailHandler()\n        logger.handlers = [handler]\n\n        exception = Exception('Test exception')\n        logger.exception(exception)\n\n        self.assertEqual(len(mail.outbox), 1)\n        self.assertIn('Test exception', mail.outbox[0].body)\n", "    def test_emit_with_request(self):\n        handler = AdminEmailHandler()\n        record = logging.LogRecord('test', logging.ERROR, __file__, 1, 'Test message')\n        request = RequestFactory().get('/')\n        record.request = request\n        handler.emit(record)\n        self.assertEqual(len(mail.outbox), 1)\n", "    def test_filter_with_debug_true(self):\n        filter = RequireDebugFalse()\n        record = logging.makeLogRecord('name', logging.ERROR, 'path', 1, 'message', None, None)\n        self.assertFalse(filter.filter(record))\n", "    def test_filter_with_debug_true(self):\n        filter = RequireDebugFalse()\n        self.assertFalse(filter.filter(logging.makeLogRecord('django', logging.DEBUG, 'path/to/module.py', 42, 'message', None, None)))\n"], "sample_165": ["    def test_choices(self):\n        choices = [\n            ChoiceModel.objects.create(name='choice1'),\n            ChoiceModel.objects.create(name='choice2'),\n        ]\n        field = ModelChoiceField(queryset=ChoiceModel.objects.all())\n        self.assertEqual(list(field.choices), [\n            ('', '---------'),\n            (choices[0].pk, str(choices[0])),\n            (choices[1].pk, str(choices[1])),\n        ])\n", "    def test_model_choice_field(self):\n        class ModelChoiceForm(Form):\n            category = ModelChoiceField(ChoiceModel.objects.all())\n\n        # Create a ChoiceModel instance to be the selected value.\n        choice1 = ChoiceModel.objects.create(name='choice1')\n        choice2 = ChoiceModel.objects.create(name='choice2')\n\n        form = ModelChoiceForm({'category': choice1.id})\n        self.assertTrue(form.is_valid())\n        self.assertEqual(form.cleaned_data['category'], choice1)\n\n        # If we pass a string instead of an integer, it should still work.\n        form = ModelChoiceForm({'category': str(choice1.id)})\n        self.assertTrue(form.is_valid())\n        self.assertEqual(form.cleaned_data['category'], choice1)\n\n        # If anything other than a valid ID is passed in, the form should not validate.\n        invalid_values = ['non-existent-id', '1234567890', choice1.name]\n        for invalid_value in invalid_values:\n            form = ModelChoiceForm({'category': invalid_value})\n            self.assertFalse(form.is_valid())\n            self.assertFormErrors(['Select a valid choice. That choice is not one of the available choices.'], form.full_clean)\n", "    def test_model_choice_iterable(self):\n        choices = [obj.pk for obj in ChoiceModel.objects.all()]\n        w = ModelChoiceField(ChoiceModel.objects.all(), required=False)\n        self.assertEqual(list(w.choices), [(obj.pk, str(obj)) for obj in ChoiceModel.objects.all()])\n        w = ModelChoiceField(ChoiceModel.objects.none(), required=False)\n        self.assertEqual(list(w.choices), [])\n", "    def test_choices(self):\n        class TestForm(Form):\n            field1 = ModelChoiceField(ChoiceModel.objects.all())\n\n        form = TestForm()\n        self.assertEqual(list(form.fields['field1'].choices), [\n            ('', '---------'),\n            (1, 'Choice 1'),\n            (2, 'Choice 2'),\n            (3, 'Choice 3'),\n        ])\n", "    def setUp(self):\n        self.choice1 = ChoiceModel.objects.create(name='choice1')\n        self.choice2 = ChoiceModel.objects.create(name='choice2')\n"], "sample_166": ["def test_salted_hmac_invalid_algorithm(self):\n    msg = \"Invalid algorithm\"\n    with self.assertRaisesMessage(InvalidAlgorithm, msg):\n        salted_hmac(\"test_key_salt\", \"test_value\", algorithm=\" invalid\")\n", "def test_constant_time_compare(self):\n    self.assertTrue(constant_time_compare('hello', 'hello'))\n    self.assertFalse(constant_time_compare('hello', 'goodbye'))\n    self.assertFalse(constant_time_compare(b'hello', 'hello'))\n    self.assertFalse(constant_time_compare('hello', b'hello'))\n    self.assertTrue(constant_time_compare(b'hello', b'hello'))\n", "def test_salted_hmac_invalid_algorithm(self):\n    with self.assertRaises(InvalidAlgorithm):\n        salted_hmac('test_key', 'test_value', algorithm='invalid_algo')\n", "def test_constant_time_compare(self):\n    # Test that constant time compare returns True for equal strings\n    self.assertTrue(constant_time_compare('hello', 'hello'))\n\n    # Test that constant time compare returns False for unequal strings\n    self.assertFalse(constant_time_compare('hello', 'goodbye'))\n\n    # Test that constant time compare works with non-ASCII strings\n    self.assertTrue(constant_time_compare('h\u00e9llo', 'h\u00e9llo'))\n    self.assertFalse(constant_time_compare('h\u00e9llo', 'goodbye'))\n\n    # Test that constant time compare works with bytes\n    self.assertTrue(constant_time_compare(b'hello', b'hello'))\n    self.assertFalse(constant_time_compare(b'hello', b'goodbye'))\n", "def test_constant_time_compare(self):\n    self.assertTrue(constant_time_compare('hello', 'hello'))\n    self.assertFalse(constant_time_compare('hello', 'goodbye'))\n    # Test that it doesn't leak timing information\n    with self.assertNumQueries(0):\n        constant_time_compare('hello', 'goodbye')\n"], "sample_167": ["def test_ordinal(self):\n    self.assertEqual(humanize.ordinal(1), '1st')\n    self.assertEqual(humanize.ordinal(2), '2nd')\n    self.assertEqual(humanize.ordinal(3), '3rd')\n    self.assertEqual(humanize.ordinal(4), '4th')\n    self.assertEqual(humanize.ordinal(10), '10th')\n    self.assertEqual(humanize.ordinal(11), '11th')\n    self.assertEqual(humanize.ordinal(12), '12th')\n    self.assertEqual(humanize.ordinal(13), '13th')\n    self.assertEqual(humanize.ordinal(20), '20th')\n    self.assertEqual(humanize.ordinal(21), '21st')\n    self.assertEqual(humanize.ordinal(22), '22nd')\n    self.assertEqual(humanize.ordinal(23), '23rd')\n    self.assertEqual(humanize.ordinal(24), '24th')\n\n    with translation.override('fr'):\n        self.assertEqual(humanize.ordinal(1), '1er')\n        self.assertEqual(humanize.ordinal(2), '2e')\n\n    self.assertEqual(humanize.ordinal('a'), 'a')\n    self.assertEqual(humanize.ordinal('1a'), '1a')\n", "def test_ordinal(self):\n    self.assertEqual(humanize.ordinal(1), '1st')\n    self.assertEqual(humanize.ordinal(2), '2nd')\n    self.assertEqual(humanize.ordinal(3), '3rd')\n    self.assertEqual(humanize.ordinal(4), '4th')\n    self.assertEqual(humanize.ordinal(11), '11th')\n    self.assertEqual(humanize.ordinal(12), '12th')\n    self.assertEqual(humanize.ordinal(13), '13th')\n    self.assertEqual(humanize.ordinal(101), '101st')\n    with translation.override('de'):\n        self.assertEqual(humanize.ordinal(1), '1.')\n        self.assertEqual(humanize.ordinal(2), '2.')\n        self.assertEqual(humanize.ordinal(3), '3.')\n        self.assertEqual(humanize.ordinal(4), '4.')\n", "def test_ordinal(self):\n    self.assertEqual(humanize.ordinal(1), '1st')\n    self.assertEqual(humanize.ordinal(2), '2nd')\n    self.assertEqual(humanize.ordinal(3), '3rd')\n    self.assertEqual(humanize.ordinal(4), '4th')\n    self.assertEqual(humanize.ordinal(10), '10th')\n    self.assertEqual(humanize.ordinal(11), '11th')\n    self.assertEqual(humanize.ordinal(12), '12th')\n    self.assertEqual(humanize.ordinal(13), '13th')\n    self.assertEqual(humanize.ordinal(20), '20th')\n    self.assertEqual(humanize.ordinal(21), '21st')\n    self.assertEqual(humanize.ordinal(22), '22nd')\n    self.assertEqual(humanize.ordinal(23), '23rd')\n    self.assertEqual(humanize.ordinal(24), '24th')\n    with translation.override('es'):\n        self.assertEqual(humanize.ordinal(1), '1\u00ba')\n        self.assertEqual(humanize.ordinal(2), '2\u00ba')\n        self.assertEqual(humanize.ordinal(3), '3\u00ba')\n        self.assertEqual(humanize.ordinal(4), '4\u00ba')\n", "def test_ordinal(self):\n    self.assertEqual(humanize.ordinal(1), '1st')\n    self.assertEqual(humanize.ordinal(2), '2nd')\n    self.assertEqual(humanize.ordinal(3), '3rd')\n    self.assertEqual(humanize.ordinal(4), '4th')\n    self.assertEqual(humanize.ordinal(10), '10th')\n    self.assertEqual(humanize.ordinal(11), '11th')\n    self.assertEqual(humanize.ordinal(12), '12th')\n    self.assertEqual(humanize.ordinal(13), '13th')\n    self.assertEqual(humanize.ordinal(20), '20th')\n    self.assertEqual(humanize.ordinal(21), '21st')\n    self.assertEqual(humanize.ordinal(22), '22nd')\n    self.assertEqual(humanize.ordinal(23), '23rd')\n    self.assertEqual(humanize.ordinal(24), '24th')\n", "def test_ordinal(self):\n    self.assertEqual(humanize.ordinal(1), '1st')\n    self.assertEqual(humanize.ordinal(2), '2nd')\n    self.assertEqual(humanize.ordinal(3), '3rd')\n    self.assertEqual(humanize.ordinal(4), '4th')\n    self.assertEqual(humanize.ordinal(10), '10th')\n    self.assertEqual(humanize.ordinal(11), '11th')\n    self.assertEqual(humanize.ordinal(12), '12th')\n    self.assertEqual(humanize.ordinal(13), '13th')\n    self.assertEqual(humanize.ordinal(21), '21st')\n    self.assertEqual(humanize.ordinal(22), '22nd')\n    self.assertEqual(humanize.ordinal(23), '23rd')\n    self.assertEqual(humanize.ordinal(24), '24th')\n    with translation.override('fr'):\n        self.assertEqual(humanize.ordinal(1), '1er')\n"], "sample_168": ["def test_remove_stale_content_types_interactive(self):\n    with captured_stdout() as stdout:\n        call_command('remove_stale_content_types', interactive=True, verbosity=2)\n    self.assertIn(\"Some content types in your database are stale and can be deleted.\", stdout.getvalue())\n    self.assertEqual(ContentType.objects.count(), self.before_count + 1)\n\n    with mock.patch('builtins.input', return_value='yes'):\n        call_command('remove_stale_content_types', interactive=True, verbosity=2)\n    self.assertEqual(ContentType.objects.count(), self.before_count)\n", "def test_remove_stale_content_types_interactive_yes(self):\n    with captured_stdout() as stdout:\n        call_command('remove_stale_contenttypes', '--noinput=False', verbosity=2)\n        self.assertIn('Some content types in your database are stale and can be deleted.', stdout.getvalue())\n        self.assertEqual(ContentType.objects.count(), self.before_count - 1)\n\n    # Make sure the confirmation message is shown again when a new stale content type is added.\n    ContentType.objects.create(app_label='contenttypes_tests', model='AnotherFake')\n    with captured_stdout() as stdout:\n        call_command('remove_stale_contenttypes', '--noinput=False', verbosity=2)\n        self.assertIn('Some content types in your database are stale and can be deleted.', stdout.getvalue())\n        self.assertEqual(ContentType.objects.count(), self.before_count - 2)\n", "def test_remove_stale_content_types_interactive(self):\n    with captured_stdout() as stdout:\n        call_command('remove_stale_content_types', '--noinput', verbosity=2)\n    self.assertEqual(stdout.getvalue(), '')\n    with mock.patch('builtins.input', return_value='yes'):\n        with captured_stdout() as stdout:\n            call_command('remove_stale_content_types', interactive=True, verbosity=2)\n        self.assertIn('Deleting stale content type', stdout.getvalue())\n    self.assertEqual(ContentType.objects.count(), self.before_count)\n", "def test_remove_stale_content_types_interactive(self):\n    with captured_stdout() as stdout:\n        call_command('remove_stale_content_types', interactive=True, verbosity=0)\n    self.assertIn('Some content types in your database are stale and can be deleted.', stdout.getvalue())\n    self.assertEqual(ContentType.objects.count(), self.before_count + 1)\n\n    with mock.patch('builtins.input', return_value='yes'):\n        call_command('remove_stale_content_types', interactive=True, verbosity=0)\n    self.assertEqual(ContentType.objects.count(), self.before_count)\n", "def test_interactive_remove_stale_content_types(self):\n    with captured_stdout() as stdout:\n        call_command('remove_stale_contenttypes', interactive=True, verbosity=0)\n    self.assertIn(\"Some content types in your database are stale and can be deleted.\", stdout.getvalue())\n    self.assertIn(\"Are you sure you want to delete these content types?\", stdout.getvalue())\n\n    # Make sure the content type wasn't deleted.\n    self.assertEqual(ContentType.objects.count(), self.before_count + 1)\n\n    with mock.patch('builtins.input', return_value='yes'):\n        call_command('remove_stale_contenttypes', interactive=True, verbosity=0)\n\n    # Make sure the content type was deleted.\n    self.assertEqual(ContentType.objects.count(), self.before_count)\n"], "sample_169": ["def test_xml_serializer_with_json_field(self):\n    obj = JSONModel.objects.create(json_field={'foo': 'bar'})\n    data = serializers.serialize('xml', [obj])\n    self.assertIn('<field name=\"json_field\" type=\"JSONField\">{&quot;foo&quot;: &quot;bar&quot;}</field>', data)\n", "def test_serialize_json_field_with_defusedxml(self):\n    # Create a JSONModel instance with some data\n    obj = JSONModel.objects.create(json_field={'key': 'value'})\n\n    # Serialize the object to XML using DefusedExpatParser\n    xml = serializers.serialize('xml', [obj], indent=4)\n\n    # Check that the XML contains the expected data\n    self.assertContains(xml, '<field name=\"json_field\" type=\"JSONField\">')\n    self.assertContains(xml, '{\"key\": \"value\"}')\n\n    # Deserialize the XML back into a Python object\n    deserialized_obj = list(serializers.deserialize('xml', xml))[0].object\n\n    # Check that the deserialized object matches the original object\n    self.assertEqual(deserialized_obj.json_field, obj.json_field)\n", "def test_xml_serializer_with_json_field(self):\n    obj = JSONModel(json_field={'key': 'value'})\n    obj.save()\n\n    serializer = serializers.get_serializer('xml')()\n    stream = io.BytesIO()\n    serializer.serialize([obj], stream=stream)\n\n    # Check that the JSON field is serialized correctly\n    xml_string = stream.getvalue().decode()\n    self.assertContains(xml_string, '<field name=\"json_field\" type=\"JSONField\">{&quot;key&quot;: &quot;value&quot;}</field>')\n", "def test_defused_xml_parser(self):\n    # Test that the DefusedExpatParser correctly raises exceptions for DTDs,\n    # entity definitions, and external references.\n    parser = DefusedExpatParser()\n\n    # Test DTD\n    with self.assertRaises(DTDForbidden):\n        parser.feed('<?xml version=\"1.0\"?><!DOCTYPE note SYSTEM \"note.dtd\">')\n\n    # Test entity definition\n    with self.assertRaises(EntitiesForbidden):\n        parser.feed('<?xml version=\"1.0\"?><!ENTITY entity \"value\">')\n\n    # Test external reference\n    with self.assertRaises(ExternalReferenceForbidden):\n        parser.feed('<?xml version=\"1.0\"?><!ENTITY entity SYSTEM \"entity.dtd\">')\n", "def test_xml_serializer(self):\n    # Create a test instance of JSONModel\n    obj = JSONModel.objects.create(json_field={'a': 1, 'b': 2})\n\n    # Serialize the object to XML\n    xml = serializers.serialize('xml', [obj])\n\n    # Check that the XML contains the expected data\n    self.assertIn('<field name=\"json_field\" type=\"JSONField\">', xml)\n    self.assertIn('<field name=\"json_field\">{&quot;a&quot;: 1, &quot;b&quot;: 2}</field>', xml)\n\n    # Deserialize the XML back into an object\n    deserialized_objects = list(serializers.deserialize('xml', xml))\n\n    # Check that the deserialized object matches the original\n    self.assertEqual(len(deserialized_objects), 1)\n    self.assertIsInstance(deserialized_objects[0].object, JSONModel)\n    self.assertEqual(deserialized_objects[0].object.json_field, {'a': 1, 'b': 2})\n"], "sample_171": ["def test_migrate_command_check_consistent_history(self):\n    with mock.patch.object(MigrationExecutor, 'loader') as loader:\n        loader.check_consistent_history.side_effect = InconsistentMigrationHistory('Mock error')\n        with self.assertRaises(CommandError) as context:\n            call_command('migrate', verbosity=0)\n        self.assertIn('Mock error', str(context.exception))\n", "def test_migrate_with_conflicting_app_labels(self):\n    \"\"\"\n    Tests the migrate command with conflicting app labels.\n    \"\"\"\n    with self.settings(INSTALLED_APPS=[\"migrations.conflicting_app_labels\"]):\n        msg = (\n            \"Conflicting migrations detected; multiple leaf nodes in the \"\n            \"migration graph: (0001_initial in app1, 0001_initial in app2).\"\n        )\n        with self.assertRaisesMessage(CommandError, msg):\n            call_command(\"migrate\", verbosity=0)\n", "def test_migrate_fake_initial(self):\n    \"\"\"\n    Tests the --fake-initial option.\n    \"\"\"\n    with connection.schema_editor() as editor:\n        # Create the tables for app1, but not its migration history\n        for model in apps.get_app_config('app1').get_models():\n            editor.create_model(model)\n    \n    out = io.StringIO()\n    call_command('migrate', '0001', verbosity=0, stdout=out, fake_initial=True)\n    \n    self.assertEqual(out.getvalue(), '')\n    self.assertEqual(MigrationRecorder(connection).applied_migrations(), [('app1', '0001')])\n", "def test_migrate_app_label(self):\n    \"\"\"\n    Tests the --app-label option of the migrate command.\n    \"\"\"\n    with connection.schema_editor() as editor:\n        editor.create_model(UnicodeModel)\n\n    call_command(\"migrate\", \"migrations\", verbosity=0)\n    recorder = MigrationRecorder(connection)\n    applied = recorder.applied_migrations()\n    self.assertIn((\"migrations\", \"0001_initial\"), applied)\n\n    # Undo all migrations for the 'migrations' app.\n    call_command(\"migrate\", \"migrations\", \"zero\", verbosity=0)\n    applied = recorder.applied_migrations()\n    self.assertNotIn((\"migrations\", \"0001_initial\"), applied)\n", "def test_migrate_fake_initial(self):\n    \"\"\"\n    Tests that --fake-initial works correctly.\n    \"\"\"\n    app_label = 'migrations'\n    with connection.schema_editor() as editor:\n        # Create the table manually to simulate a pre-existing database.\n        editor.create_model(UnicodeModel)\n\n    call_command('migrate', app_label, verbosity=0, fake_initial=True)\n    applied_migrations = MigrationRecorder(connection).applied_migrations()\n    self.assertIn((app_label, '0001_initial'), applied_migrations)\n    self.assertIn((app_label, '0002_second'), applied_migrations)\n"], "sample_170": ["    def test_cleansed_multivaluedict(self):\n        request = RequestFactory().post('/some_url/', {'foo': 'bar', 'sensitive': 'data'})\n        request.sensitive_post_parameters = ['sensitive']\n        filter = SafeExceptionReporterFilter()\n        cleaned_dict = filter.get_cleansed_multivaluedict(request, request.POST)\n        self.assertEqual(cleaned_dict['foo'], 'bar')\n        self.assertEqual(cleaned_dict['sensitive'], SafeExceptionReporterFilter.cleansed_substitute)\n", "    def test_cleanse_setting(self):\n        filter = SafeExceptionReporterFilter()\n        self.assertEqual(filter.cleanse_setting('SETTING', 'VALUE'), 'VALUE')\n        self.assertEqual(filter.cleanse_setting('API_SETTING', 'VALUE'), '********************')\n", "    def test_get_safe_settings(self):\n        filter = SafeExceptionReporterFilter()\n        settings = filter.get_safe_settings()\n\n        # Make sure it doesn't raise an exception on a setting that's not regex-able\n        self.assertIn('SECRET_KEY', settings)\n        self.assertEqual(settings['SECRET_KEY'], '********************')\n\n        # Make sure we don't accidentally expose sensitive info\n        self.assertNotIn('secret_key', settings['DATABASES']['default'])\n\n        # Make sure we can handle unserializable objects\n        self.assertEqual(repr(settings['LOGGING']), repr(settings.LOGGING))\n", "    def test_get_safe_settings(self):\n        filter = SafeExceptionReporterFilter()\n        settings = filter.get_safe_settings()\n        self.assertIn('SECRET_KEY', settings)\n        self.assertNotEqual(settings['SECRET_KEY'], '********************')\n        with override_settings(SECRET_KEY='mysecret'):\n            settings = filter.get_safe_settings()\n            self.assertEqual(settings['SECRET_KEY'], '********************')\n", "    def test_cleansed_substitute(self):\n        filter = SafeExceptionReporterFilter()\n        self.assertEqual(filter.cleansed_substitute, '********************')\n"], "sample_172": ["    def test_charfield_with_textinput_override(self):\n        self.assertFormfield(Profile, 'name', forms.TextInput, formfield_overrides={CharField: {'widget': forms.TextInput}})\n", "    def test_custom_widget(self):\n        # Define a model with a field that uses a custom widget\n        class MyModel(models.Model):\n            my_field = CharField(max_length=10)\n\n        # Define a custom widget\n        class MyWidget(forms.TextInput):\n            pass\n\n        # Define a model admin that specifies the custom widget\n        class MyModelAdmin(admin.ModelAdmin):\n            formfield_overrides = {CharField: {'widget': MyWidget}}\n\n        # Construct the admin, and ask it for a formfield\n        ma = MyModelAdmin(MyModel, admin.site)\n        ff = ma.formfield_for_dbfield(MyModel._meta.get_field('my_field'), request=None)\n\n        # Verify that the returned formfield uses the custom widget\n        self.assertIsInstance(ff.widget, MyWidget)\n", "    def test_textarea_override(self):\n        # Define a model with a CharField\n        class MyModel(models.Model):\n            myfield = CharField(max_length=10)\n\n        # Override the CharField to use a Textarea widget\n        class MyModelAdmin(admin.ModelAdmin):\n            formfield_overrides = {\n                CharField: {'widget': forms.Textarea},\n            }\n\n        ma = MyModelAdmin(MyModel, admin.site)\n        ff = ma.formfield_for_dbfield(MyModel._meta.get_field('myfield'), request=None)\n        self.assertIsInstance(ff.widget, forms.Textarea)\n", "    def test_text_field_override(self):\n        class MyModelAdmin(admin.ModelAdmin):\n            formfield_overrides = {\n                CharField: {'widget': forms.Textarea},\n            }\n        ma = MyModelAdmin(Band, admin.site)\n        ff = ma.formfield_for_dbfield(Band._meta.get_field('name'), request=None)\n        self.assertIsInstance(ff.widget, forms.Textarea)\n", "    def test_textarea_override(self):\n        # Test that the textarea widget can be overridden.\n        model = Album\n        fieldname = 'name'\n        override = {'widget': forms.TextInput}\n        self.assertFormfield(model, fieldname, forms.TextInput, formfield_overrides={CharField: override})\n"], "sample_173": ["def test_adapt_decimalfield_value(self):\n    value = decimal.Decimal('3.14')\n    max_digits = 5\n    decimal_places = 2\n    adapted_value = self.ops.adapt_decimalfield_value(value, max_digits, decimal_places)\n    self.assertEqual(adapted_value, '3.14')\n\n    # Test without max_digits and decimal_places\n    adapted_value = self.ops.adapt_decimalfield_value(value)\n    self.assertEqual(adapted_value, '3.14')\n", "def test_adapt_decimalfield_value(self):\n    value = decimal.Decimal('12.345')\n    max_digits = 5\n    decimal_places = 2\n    self.assertEqual(\n        self.ops.adapt_decimalfield_value(value, max_digits, decimal_places),\n        '12.35'\n    )\n", "def test_adapt_unknown_value(self):\n    now = timezone.now()\n    self.assertEqual(self.ops.adapt_unknown_value(now), str(now))\n    self.assertIsNone(self.ops.adapt_unknown_value(None))\n    decimal_value = decimal.Decimal('1.23')\n    self.assertEqual(self.ops.adapt_unknown_value(decimal_value), str(decimal_value))\n", "def test_adapt_unknown_value(self):\n    self.assertIsNone(self.ops.adapt_unknown_value(None))\n    self.assertEqual(self.ops.adapt_unknown_value('hello'), 'hello')\n    self.assertEqual(self.ops.adapt_unknown_value(123), 123)\n    self.assertIsInstance(self.ops.adapt_unknown_value(decimal.Decimal('3.14')), str)\n    self.assertIsInstance(self.ops.adapt_unknown_value(timezone.now()), str)\n", "def test_adapt_unknown_value(self):\n    now = timezone.now()\n    today = now.date()\n    time = now.time()\n    decimal_value = decimal.Decimal('1.2')\n    tests = (\n        (now, self.ops.adapt_datetimefield_value(now)),\n        (today, self.ops.adapt_datefield_value(today)),\n        (time, self.ops.adapt_timefield_value(time)),\n        (decimal_value, self.ops.adapt_decimalfield_value(decimal_value)),\n        ('string', 'string'),\n        (1, 1),\n    )\n    for value, expected in tests:\n        with self.subTest(value=value):\n            self.assertEqual(self.ops.adapt_unknown_value(value), expected)\n"], "sample_174": ["def test_adapt_decimalfield_value(self):\n    value = decimal.Decimal('3.14')\n    self.assertEqual(self.ops.adapt_decimalfield_value(value), '3.14')\n", "def test_adapt_decimalfield_value(self):\n    value = decimal.Decimal('12.345')\n    self.assertEqual(self.ops.adapt_decimalfield_value(value), '12.345')\n\n    max_digits = 5\n    decimal_places = 2\n    self.assertEqual(\n        self.ops.adapt_decimalfield_value(value, max_digits, decimal_places),\n        '12.35'\n    )\n", "def test_adapt_decimalfield_value(self):\n    value = decimal.Decimal('12.345')\n    self.assertEqual(self.ops.adapt_decimalfield_value(value), '12.345')\n\n    max_digits = 5\n    decimal_places = 2\n    formatted_value = self.ops.adapt_decimalfield_value(value, max_digits, decimal_places)\n    self.assertEqual(formatted_value, '12.35')\n", "def test_adapt_unknown_value(self):\n    self.assertIsNone(self.ops.adapt_unknown_value(None))\n    self.assertEqual(self.ops.adapt_unknown_value('hello'), 'hello')\n    self.assertIsInstance(self.ops.adapt_unknown_value(decimal.Decimal('3.14')), str)\n    self.assertIsInstance(self.ops.adapt_unknown_value(timezone.now()), str)\n", "def test_adapt_unknown_value(self):\n    self.assertIsNone(self.ops.adapt_unknown_value(None))\n    self.assertIsInstance(self.ops.adapt_unknown_value(decimal.Decimal('1.2')), str)\n    self.assertIsInstance(self.ops.adapt_unknown_value(timezone.now()), str)\n    self.assertIsInstance(self.ops.adapt_unknown_value(timezone.now().date()), str)\n    self.assertIsInstance(self.ops.adapt_unknown_value(timezone.now().time()), str)\n"], "sample_175": ["def test_delete_collector_sorts_instances_by_pk(self):\n    collector = Collector(using='default')\n    a1 = A.objects.create()\n    a2 = A.objects.create()\n    b1 = B.objects.create(a=a1)\n    b2 = B.objects.create(a=a1)\n    c1 = Child.objects.create(b=b1)\n    c2 = Child.objects.create(b=b2)\n\n    collector.collect([a1, a2], collect_related=True)\n    instances = list(collector.data.values())\n    self.assertEqual(len(instances), 3)  # A, B, Child\n\n    for model_instances in instances:\n        pks = [instance.pk for instance in model_instances]\n        self.assertEqual(pks, sorted(pks))\n", "def test_fast_delete(self):\n    a = create_a()\n    collector = Collector(using='default')\n    collector.collect([a])\n    self.assertEqual(collector.fast_deletes, [A.objects.filter(pk=a.pk)])\n", "def test_sort_with_deferred_constraints(self):\n    collector = Collector(using='default')\n    a1 = create_a()\n    a2 = create_a()\n    collector.add([a1, a2])\n    # Simulate deferred constraints by adding a dependency from A to itself.\n    collector.add_dependency(A, A)\n    # The collector should be able to sort the instances despite the circular dependency.\n    collector.sort()\n    self.assertEqual(collector.data, {A: [a1, a2]})\n", "def test_protected_error(self):\n    a = create_a()\n    b = B.objects.create(a=a)\n    collector = Collector(using='default')\n    with self.assertRaises(ProtectedError) as cm:\n        collector.collect([a])\n    protected_objects = list(cm.exception.protected_objects)\n    self.assertEqual(len(protected_objects), 1)\n    self.assertEqual(protected_objects[0], b)\n", "def test_fast_delete(self):\n    a = create_a()\n    b = a.b_set.create()\n    c = b.c_set.create()\n    collector = Collector(using='default')\n    collector.collect([a])\n    self.assertEqual(len(collector.fast_deletes), 1)\n    self.assertEqual(collector.fast_deletes[0].model, A)\n    with connection.transaction.atomic():\n        collector.delete()\n    self.assertQuerysetEqual(A.objects.all(), [])\n    self.assertQuerysetEqual(B.objects.all(), [])\n    self.assertQuerysetEqual(C.objects.all(), [])\n"], "sample_176": ["def test_add_unique_constraint(self):\n    \"\"\"Test adding a unique constraint.\"\"\"\n    before = self.make_project_state([self.author_name])\n    after = self.make_project_state([\n        ModelState(\n            \"testapp\", \"Author\", [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n            ],\n            {\n                'constraints': [models.UniqueConstraint(fields=['name'], name='author_name_uniq')],\n            },\n        ),\n    ])\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['AddConstraint'])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, constraint=models.UniqueConstraint(fields=['name'], name='author_name_uniq'))\n", "def test_generate_altered_foo_together(self):\n    \"\"\"\n    Tests the addition of new index/unique_together constraints.\n    \"\"\"\n    # Make state\n    before = self.make_project_state([self.book])\n    after = self.make_project_state([self.book_foo_together_2])\n\n    # Work out what changed\n    changes = self.get_changes(before, after)\n\n    # Right number/types of migrations?\n    self.assertNumberMigrations(changes, 'otherapp', 1)\n    self.assertOperationTypes(changes, 'otherapp', 0, [\"AlterUniqueTogether\", \"AlterIndexTogether\"])\n    self.assertOperationAttributes(changes, 'otherapp', 0, 0, name='book', unique_together={(\"title\", \"author\")})\n    self.assertOperationAttributes(changes, 'otherapp', 0, 1, name='book', index_together={(\"title\", \"author\")})\n", "def test_check_dependency(self):\n    operation = operations.AddField(\n        model_name='model',\n        name='field',\n        field=models.IntegerField(),\n    )\n    dependency = ('app', 'model', 'field', True)\n    self.assertTrue(MigrationAutodetector.check_dependency(operation, dependency))\n\n    operation = operations.RemoveField(\n        model_name='model',\n        name='field',\n    )\n    dependency = ('app', 'model', 'field', False)\n    self.assertTrue(MigrationAutodetector.check_dependency(operation, dependency))\n\n    operation = operations.AlterField(\n        model_name='model',\n        name='field',\n        field=models.IntegerField(),\n    )\n    dependency = ('app', 'model', 'field', 'alter')\n    self.assertTrue(MigrationAutodetector.check_dependency(operation, dependency))\n\n    operation = operations.CreateModel(\n        name='model',\n        fields=[('field', models.IntegerField())],\n    )\n    dependency = ('app', 'model', None, True)\n    self.assertTrue(MigrationAutodetector.check_dependency(operation, dependency))\n\n    operation = operations.DeleteModel(\n        name='model',\n    )\n    dependency = ('app', 'model', None, False)\n    self.assertTrue(MigrationAutodetector.check_dependency(operation, dependency))\n\n    operation = operations.AddField(\n        model_name='model',\n        name='field',\n        field=models.IntegerField(),\n    )\n    dependency = ('app', 'model', 'other_field', True)\n    self.assertFalse(MigrationAutodetector.check_dependency(operation, dependency))\n", "def test_generate_altered_order_with_respect_to(self):\n    \"\"\"Tests the order_with_respect_to operation.\"\"\"\n    changes = self.get_changes(\n        [self.author_with_book],\n        [self.author_with_book_order_wrt],\n    )\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['AlterOrderWithRespectTo'])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name='author', order_with_respect_to='book')\n", "def test_generate_added_indexes(self):\n    \"\"\"Tests that added indexes are properly detected.\"\"\"\n    changes = self.get_changes([self.author_name], [self.author_name_with_index])\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['AddIndex'])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name='custom_index', fields=['name'])\n"], "sample_177": ["def test_get_related_models_recursive(self):\n    class ModelA(models.Model):\n        pass\n\n    class ModelB(models.Model):\n        a = models.ForeignKey(ModelA, on_delete=models.CASCADE)\n\n    class ModelC(models.Model):\n        b = models.ForeignKey(ModelB, on_delete=models.CASCADE)\n\n    related_models = get_related_models_recursive(ModelA)\n    self.assertIn((ModelB._meta.app_label, ModelB._meta.model_name), related_models)\n    self.assertIn((ModelC._meta.app_label, ModelC._meta.model_name), related_models)\n", "def test_model_state_clone(self):\n    \"\"\"Test that cloning a ModelState results in an exact copy.\"\"\"\n    model_state = ModelState(\n        app_label='testapp',\n        name='TestModel',\n        fields=[\n            ('id', models.AutoField(primary_key=True)),\n            ('name', models.CharField(max_length=255)),\n        ],\n        options={'verbose_name': 'Test Model'},\n        bases=(models.Model,),\n        managers=[('objects', models.Manager())],\n    )\n    cloned_state = model_state.clone()\n\n    self.assertEqual(model_state.app_label, cloned_state.app_label)\n    self.assertEqual(model_state.name, cloned_state.name)\n    self.assertEqual(list(model_state.fields.keys()), list(cloned_state.fields.keys()))\n    self.assertEqual(model_state.options, cloned_state.options)\n    self.assertEqual(model_state.bases, cloned_state.bases)\n    self.assertEqual(model_state.managers, cloned_state.managers)\n", "def test_model_state_render(self):\n    \"\"\"\n    ModelState.render() creates a Model object from the ModelState's current state.\n    \"\"\"\n    project_state = ProjectState()\n    app_label = 'myapp'\n    model_name = 'MyModel'\n\n    # Create a new ModelState\n    model_state = ModelState(\n        app_label,\n        model_name,\n        fields=[\n            ('id', models.AutoField(primary_key=True)),\n            ('name', models.CharField(max_length=255)),\n        ],\n    )\n\n    # Add the ModelState to the ProjectState\n    project_state.add_model(model_state)\n\n    # Render the ModelState into a Model object\n    model = model_state.render(project_state.apps)\n\n    # Check that the Model object was created correctly\n    self.assertEqual(model.__name__, model_name)\n    self.assertEqual(model._meta.app_label, app_label)\n    self.assertEqual(len(model._meta.fields), 2)\n", "def test_model_state_render(self):\n    \"\"\"\n    ModelState.render() properly handles recursive references.\n    \"\"\"\n    model_state = ModelState(\n        'testapp',\n        'TestModel',\n        [\n            ('id', models.AutoField(primary_key=True)),\n            ('parent', models.ForeignKey('testapp.TestModel', on_delete=models.CASCADE, null=True)),\n        ],\n    )\n    apps = Apps()\n    with isolate_apps('testapp', 'django.db.migrations.tests.test_state'):\n        model = model_state.render(apps)\n        self.assertEqual(model._meta.get_field('parent').related_model, model)\n", "def test_get_related_models_recursive(self):\n    class ModelA(models.Model):\n        pass\n\n    class ModelB(models.Model):\n        a = models.ForeignKey(ModelA, on_delete=models.CASCADE)\n\n    class ModelC(models.Model):\n        b = models.ForeignKey(ModelB, on_delete=models.CASCADE)\n\n    related_models = get_related_models_recursive(ModelC)\n    self.assertIn((ModelC._meta.app_label, ModelC._meta.model_name), related_models)\n    self.assertIn((ModelB._meta.app_label, ModelB._meta.model_name), related_models)\n    self.assertIn((ModelA._meta.app_label, ModelA._meta.model_name), related_models)\n"], "sample_178": ["def test_baseformset_total_form_count(self):\n    # Test that total_form_count returns the correct value when is_bound is True\n    formset = self.make_choiceformset(formset_data=[('Choice 1', 1), ('Choice 2', 2)])\n    self.assertEqual(formset.total_form_count(), 2)\n\n    # Test that total_form_count returns the correct value when is_bound is False\n    formset = self.make_choiceformset()\n    self.assertEqual(formset.total_form_count(), 1)  # default extra is 1\n\n    # Test that total_form_count returns the correct value when initial forms are present\n    formset = self.make_choiceformset(initial_forms=3)\n    self.assertEqual(formset.total_form_count(), 4)  # 3 initial + 1 extra\n\n    # Test that total_form_count returns the correct value when max_num is set\n    formset = self.make_choiceformset(max_num_forms=5, initial_forms=6)\n    self.assertEqual(formset.total_form_count(), 5)  # max_num takes precedence over initial\n", "def test_formset_management_form_validation(self):\n    # Test that the management form validates correctly\n    formset = self.make_choiceformset(formset_data=[('Choice 1', 10), ('Choice 2', 20)])\n    self.assertTrue(formset.management_form.is_valid())\n\n    # Test that an invalid TOTAL_FORMS value raises a ValidationError\n    data = {\n        'choices-TOTAL_FORMS': 'abc',\n        'choices-INITIAL_FORMS': '0',\n        'choices-MAX_NUM_FORMS': '0',\n        'choices-MIN_NUM_FORMS': '0',\n        'choices-0-choice': 'Choice 1',\n        'choices-0-votes': '10',\n        'choices-1-choice': 'Choice 2',\n        'choices-1-votes': '20',\n    }\n    formset = ChoiceFormSet(data, prefix='choices')\n    with self.assertRaises(ValidationError):\n        formset.management_form.clean()\n\n    # Test that an invalid INITIAL_FORMS value raises a ValidationError\n    data['choices-TOTAL_FORMS'] = '2'\n    data['choices-INITIAL_FORMS'] = 'abc'\n    formset = ChoiceFormSet(data, prefix='choices')\n    with self.assertRaises(ValidationError):\n        formset.management_form.clean()\n", "def test_formset_non_form_errors(self):\n    class FormSetWithNonFormErrors(BaseFormSet):\n            raise ValidationError('Non-form error')\n\n    FormSet = formset_factory(FavoriteDrinkForm, formset=FormSetWithNonFormErrors)\n    data = {\n        'form-TOTAL_FORMS': '1',\n        'form-INITIAL_FORMS': '0',\n        'form-MAX_NUM_FORMS': '',\n        'form-MIN_NUM_FORMS': '',\n        'form-0-name': 'Test',\n    }\n    formset = FormSet(data)\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(formset.non_form_errors(), ['Non-form error'])\n", "def test_baseformset_add_fields(self):\n    # Test that BaseFormSet.add_fields adds the ORDER and DELETE fields when can_order and can_delete are True\n    class TestForm(Form):\n        pass\n\n    formset = formset_factory(TestForm, can_order=True, can_delete=True)\n    form = formset.form()\n    self.assertIn('ORDER', form.fields)\n    self.assertIn('DELETE', form.fields)\n\n    # Test that BaseFormSet.add_fields does not add the ORDER field when can_order is False\n    formset = formset_factory(TestForm, can_order=False, can_delete=True)\n    form = formset.form()\n    self.assertNotIn('ORDER', form.fields)\n    self.assertIn('DELETE', form.fields)\n\n    # Test that BaseFormSet.add_fields does not add the DELETE field when can_delete is False\n    formset = formset_factory(TestForm, can_order=True, can_delete=False)\n    form = formset.form()\n    self.assertIn('ORDER', form.fields)\n    self.assertNotIn('DELETE', form.fields)\n", "def test_formset_management_form(self):\n    formset = self.make_choiceformset()\n    management_form = formset.management_form\n\n    # Check that the management form has the correct fields.\n    self.assertIsInstance(management_form.fields['TOTAL_FORMS'], IntegerField)\n    self.assertIsInstance(management_form.fields['INITIAL_FORMS'], IntegerField)\n    self.assertIsInstance(management_form.fields['MIN_NUM_FORMS'], IntegerField)\n    self.assertIsInstance(management_form.fields['MAX_NUM_FORMS'], IntegerField)\n\n    # Check that the management form is valid when the formset is valid.\n    data = {\n        'choices-TOTAL_FORMS': '2',\n        'choices-INITIAL_FORMS': '0',\n        'choices-MIN_NUM_FORMS': '',\n        'choices-MAX_NUM_FORMS': '',\n    }\n    formset = self.make_choiceformset([(u'Choice 1', u'10'), (u'Choice 2', u'20')], data=data)\n    self.assertTrue(formset.is_valid())\n    self.assertTrue(formset.management_form.is_valid())\n\n    # Check that the management form is invalid when the formset is invalid.\n    data = {\n        'choices-TOTAL_FORMS': '',  # This field is required.\n        'choices-INITIAL_FORMS': '0',\n        'choices-MIN_NUM_FORMS': '',\n        'choices-MAX_NUM_FORMS': '',\n    }\n    formset = self.make_choiceformset([(u'Choice 1', u'10'), (u'Choice 2', u'20')], data=data)\n    self.assertFalse(formset.is_valid())\n    self.assertFalse(formset.management_form.is_valid())\n"], "sample_180": ["    def test_field_name_clashes(self):\n        class Model(models.Model):\n            foo = models.CharField(max_length=10)\n            clean = models.CharField(max_length=10)\n\n        errors = Model.check()\n        self.assertEqual(len(errors), 1)\n        self.assertIsInstance(errors[0], Error)\n        self.assertEqual(errors[0].id, 'models.E006')\n", "    def test_unique_for_date(self):\n        class Model(models.Model):\n            title = models.CharField(max_length=255, unique_for_date='published')\n            published = models.DateField()\n\n        self.assertEqual(Model._perform_unique_checks([]), [])\n        self.assertEqual(Model._perform_date_checks([(Model, 'date', 'title', 'published')]), [])\n\n        # Create a model instance with values.\n        model = Model(title='Test', published='2022-01-01')\n\n        # Perform validation checks.\n        errors = model._perform_unique_checks([(Model, ('title',))])\n        self.assertEqual(len(errors), 0)\n\n        errors = model._perform_date_checks([(Model, 'date', 'title', 'published')])\n        self.assertEqual(len(errors), 0)\n", "    def test_construction(self):\n        state = ModelState()\n        self.assertIsNone(state.db)\n        self.assertTrue(state.adding)\n        self.assertEqual(state.fields_cache, {})\n", "    def test_check_order_with_respect_to(self):\n        class MyModel(models.Model):\n            order_with_respect_to = 'field'\n\n        errors = MyModel.check()\n        self.assertEqual(len(errors), 1)\n        self.assertIsInstance(errors[0], Error)\n        self.assertEqual(errors[0].id, 'models.E020')\n", "    def test_unique_error_message(self):\n        class MyModel(models.Model):\n            my_field = models.CharField(max_length=10, unique=True)\n\n        model_instance = MyModel(my_field='test')\n        model_instance.clean_fields()\n        try:\n            model_instance.validate_unique()\n        except ValidationError as e:\n            self.assertEqual(e.code, 'unique')\n            self.assertEqual(e.params['model'], model_instance)\n            self.assertEqual(e.params['model_class'], MyModel)\n            self.assertEqual(e.params['field_name'], 'my_field')\n"], "sample_179": ["    def test_deferred_model_validation(self):\n        class DeferredModel(models.Model):\n            name = models.CharField(max_length=255)\n\n        self.assertEqual(DeferredModel.check(), [])\n", "    def test_validate_unique_checks_instance_field_value(self):\n        class UniqueModel(models.Model):\n            unique_field = models.CharField(max_length=10, unique=True)\n\n        instance1 = UniqueModel(unique_field='value')\n        instance1.validate_unique()\n        instance1.save()\n\n        instance2 = UniqueModel(unique_field='value')\n        with self.assertRaises(ValidationError) as cm:\n            instance2.validate_unique()\n        self.assertEqual(cm.exception.code, 'unique')\n        self.assertEqual(cm.exception.message, 'UniqueModel with this Unique field already exists.')\n", "    def test_model_with_order_with_respect_to_and_ordering(self):\n        class Model(models.Model):\n            order_with_respect_to = 'field'\n            field = models.IntegerField()\n            class Meta:\n                ordering = ['field']\n\n        errors = Model.check()\n        self.assertEqual(len(errors), 1)\n        self.assertIsInstance(errors[0], Error)\n        self.assertEqual(errors[0].id, 'models.E021')\n", "    def test_deferred_model_fields_check(self):\n        class DeferredModel(models.Model):\n            field = models.CharField(max_length=10)\n\n                super().__init__(*args, **kwargs)\n                self.field = models.Deferred()\n\n        errors = DeferredModel.check()\n        self.assertEqual(errors, [\n            Error(\n                \"The field 'field' clashes with the field 'field' \"\n                \"from model 'invalid_models_tests.deferredmodel'.\",\n                obj=DeferredModel,\n                id='models.E006',\n            ),\n        ])\n", "    def test_field_name_clashes(self):\n        class Model(models.Model):\n            field1 = models.CharField(max_length=10)\n            field1 = models.CharField(max_length=10)\n\n        errors = Model.check()\n        self.assertEqual(len(errors), 1)\n        self.assertIsInstance(errors[0], Error)\n        self.assertEqual(errors[0].id, 'models.E006')\n"], "sample_182": ["def test_union(self):\n    qs1 = Number.objects.filter(num__lte=3)\n    qs2 = Number.objects.filter(num__gte=7)\n    union_qs = qs1.union(qs2)\n    self.assertNumbersEqual(union_qs, [0, 1, 2, 3, 7, 8, 9])\n", "def test_union_with_empty_queryset(self):\n    qs1 = Number.objects.filter(num__lte=5)\n    qs2 = Number.objects.none()\n    union_qs = qs1.union(qs2)\n    self.assertNumbersEqual(union_qs, [0, 1, 2, 3, 4, 5])\n", "def test_union(self):\n    qs1 = Number.objects.filter(num__lte=5)\n    qs2 = Number.objects.filter(num__gte=5)\n    union_qs = qs1.union(qs2)\n    self.assertNumbersEqual(union_qs, list(range(10)))\n", "def test_union(self):\n    qs1 = Number.objects.filter(num__lt=5)\n    qs2 = Number.objects.filter(num__gte=5)\n    union_qs = qs1.union(qs2)\n    self.assertNumbersEqual(union_qs, range(10))\n", "def test_union(self):\n    qs1 = Number.objects.filter(num__lte=5)\n    qs2 = Number.objects.filter(num__gte=8)\n    union_qs = qs1.union(qs2)\n    self.assertNumbersEqual(union_qs, [0, 1, 2, 3, 4, 5, 8, 9], ordered=False)\n"], "sample_181": ["def test_filtered_aggregate_subquery(self):\n    subquery = Book.objects.filter(pages__gt=500).values('publisher')\n    qs = Publisher.objects.annotate(\n        num_books=Count('book'),\n        num_long_books=Count('book', filter=Q(id__in=Subquery(subquery))),\n    )\n    self.assertEqual(qs.get(num_books=3).num_long_books, 1)\n", "def test_subquery_aggregation(self):\n    subquery = Book.objects.filter(publisher=OuterRef('pk')).order_by('-pages').values('pages')[:1]\n    publishers = Publisher.objects.annotate(max_pages=Subquery(subquery)).order_by('-max_pages')\n    self.assertEqual(publishers[0].max_pages, 600)\n    self.assertEqual(publishers[1].max_pages, None)\n\n    subquery = Book.objects.filter(publisher=OuterRef('pk')).order_by('-rating').values('rating')[:1]\n    publishers = Publisher.objects.annotate(max_rating=Subquery(subquery)).order_by('-max_rating')\n    self.assertAlmostEqual(publishers[0].max_rating, 4.5, places=1)\n    self.assertEqual(publishers[1].max_rating, None)\n\n    subquery = Book.objects.filter(publisher=OuterRef('pk')).order_by('-price').values('price')[:1]\n    publishers = Publisher.objects.annotate(max_price=Subquery(subquery)).order_by('-max_price')\n    self.assertAlmostEqual(publishers[0].max_price, Decimal('30.00'), places=2)\n    self.assertEqual(publishers[1].max_price, None)\n\n    subquery = Book.objects.filter(contact=OuterRef('pk')).order_by('-pages').values('pages')[:1]\n    authors = Author.objects.annotate(max_pages=Subquery(subquery)).order_by('-max_pages')\n    self.assertEqual(authors[0].max_pages, 600)\n    self.assertEqual(authors[1].max_pages, 528)\n    self.assertEqual(authors[2].max_pages, None)\n", "def test_exists(self):\n    qs = Book.objects.filter(publisher=OuterRef('pk'))\n    self.assertTrue(Publisher.objects.filter(Exists(qs)).exists())\n    self.assertFalse(Publisher.objects.filter(~Exists(qs)).exists())\n\n    # Test exists on annotation.\n    annotated = Publisher.objects.annotate(has_book=Exists(qs))\n    self.assertTrue(annotated.filter(has_book=True).exists())\n    self.assertFalse(annotated.filter(has_book=False).exists())\n\n    # Test with Subquery.\n    subquery = Book.objects.filter(publisher=OuterRef('pk')).values('publisher')\n    self.assertTrue(Publisher.objects.filter(id__in=Subquery(subquery)).exists())\n\n    # Test with conditional expressions.\n    condition = Case(\n        When(Exists(qs), then=True),\n        default=False,\n    )\n    annotated = Publisher.objects.annotate(has_book=condition)\n    self.assertTrue(annotated.filter(has_book=True).exists())\n    self.assertFalse(annotated.filter(has_book=False).exists())\n", "def test_filtered_aggregate_subquery(self):\n    b1_contact_age = Author.objects.filter(\n        id=self.b1.contact_id).values('age').first()['age']\n    b2_contact_age = Author.objects.filter(\n        id=self.b2.contact_id).values('age').first()['age']\n    b3_contact_age = Author.objects.filter(\n        id=self.b3.contact_id).values('age').first()['age']\n\n    subquery = Author.objects.filter(id=OuterRef('contact_id')).values('age')\n    max_contact_age = Book.objects.annotate(\n        contact_age=Subquery(subquery)).aggregate(Max('contact_age'))['contact_age__max']\n    self.assertEqual(max_contact_age, max(b1_contact_age, b2_contact_age, b3_contact_age))\n", "def test_exists_subquery(self):\n    subquery = Book.objects.filter(author=self.a1).values('publisher')\n    qs = Publisher.objects.filter(Exists(subquery))\n    self.assertEqual(qs.count(), 1)\n    self.assertEqual(qs[0], self.p1)\n\n    subquery = Book.objects.filter(publisher=OuterRef('pk')).values('author')\n    qs = Publisher.objects.filter(Exists(subquery))\n    self.assertEqual(qs.count(), 1)\n    self.assertEqual(qs[0], self.p1)\n"], "sample_183": ["def test_case_expression_with_multiple_conditions(self):\n    # Create a Case expression with multiple conditions.\n    case_expr = Case(\n        When(integer=1, then=Value('one')),\n        When(integer=2, then=Value('two')),\n        default=Value('other')\n    )\n\n    # Use the Case expression in a query.\n    results = list(CaseTestModel.objects.values_list('integer', case_expr))\n\n    # Check that the results are correct.\n    expected_results = [\n        (1, 'one'),\n        (2, 'two'),\n        (3, 'other'),\n        (2, 'two'),\n        (3, 'other'),\n        (3, 'other'),\n        (4, 'other'),\n    ]\n    self.assertEqual(results, expected_results)\n", "def test_case_with_empty_when_clauses(self):\n    with self.assertRaises(ValueError):\n        list(CaseTestModel.objects.annotate(\n            result=Case(\n                When(Q(integer__gt=10), then=Value('g')),\n                default=Value('l'),\n            )\n        ).values('result'))\n", "def test_case_expression_with_f(self):\n    case_test_models = CaseTestModel.objects.annotate(\n        result=Case(When(integer=1, then=Value('one')), default=Value('other'), output_field=CharField()),\n    ).filter(result=F('string'))\n\n    self.assertQuerysetEqual(case_test_models, ['<CaseTestModel: 1>'], attrgetter('integer'))\n", "def test_case_expression_with_value(self):\n    # Ensure that a Case expression with a Value as the condition or value\n    # doesn't crash.\n    case_test = CaseTestModel.objects.annotate(\n        result=Case(\n            When(integer=Value(1), then=Value('one')),\n            default=Value('other'),\n            output_field=CharField(),\n        )\n    ).get(integer=1)\n    self.assertEqual(case_test.result, 'one')\n", "def test_case_value(self):\n    case = Case(\n        When(integer=1, then=Value('one')),\n        When(integer=2, then=Value('two')),\n        default=Value('other'),\n        output_field=CharField(),\n    )\n    self.assertEqual(\n        list(CaseTestModel.objects.values_list(case)),\n        [('one',), ('two',), ('other',), ('two',), ('other',), ('other',), ('other',)],\n    )\n"], "sample_184": ["    def test_clash(self):\n        class Model(models.Model):\n            clash = models.CharField(max_length=10)\n\n            @property\n                return None\n\n        model = Model()\n        errors = model._meta.check(property_names=['clash'])\n        self.assertEqual(len(errors), 1)\n        error = errors[0]\n        self.assertIsInstance(error, Error)\n        self.assertEqual(error.id, 'models.E025')\n        self.assertEqual(\n            error.msg,\n            \"The property 'clash' clashes with a related field accessor.\",\n        )\n", "    def test_unique_error_message(self):\n        class Model(models.Model):\n            unique_field = models.CharField(max_length=10, unique=True)\n\n        with self.assertRaisesMessage(ValueError, 'Value must be a list or tuple.'):\n            Model._meta.get_field('unique_field').error_messages['unique'] = 'not a list'\n", "    def test_constructor(self):\n        state = ModelState()\n        self.assertIsNone(state.db)\n        self.assertTrue(state.adding)\n", "    def test_model_validation(self):\n        class TestModel(models.Model):\n            name = models.CharField(max_length=10)\n\n                if self.name == 'test':\n                    raise ValidationError('Test name is not allowed')\n\n        model = TestModel(name='test')\n        with self.assertRaises(ValidationError):\n            model.full_clean()\n", "    def test_check_swappable(self):\n        class SwappedModel(models.Model):\n            class Meta:\n                swappable = 'UNSUPPORTED'\n\n        errors = SwappedModel.check()\n        expected_errors = [\n            Error(\n                \"'UNSUPPORTED' is not of the form 'app_label.app_name'.\",\n                id='models.E001',\n            ),\n        ]\n        self.assertEqual(errors, expected_errors)\n"], "sample_185": ["def test_get_format_lazy(self):\n    with translation.override('de'):\n        self.assertEqual(get_format_lazy('DATE_FORMAT'), get_format('DATE_FORMAT'))\n    with translation.override('en'):\n        self.assertEqual(get_format_lazy('DATE_FORMAT'), get_format('DATE_FORMAT'))\n", "def test_get_format_lazy(self):\n    with translation.override('de'):\n        self.assertEqual(get_format_lazy('DATE_FORMAT'), get_format('DATE_FORMAT'))\n        self.assertIsInstance(get_format_lazy('DATE_FORMAT'), str)\n\n    with translation.override('en'):\n        self.assertEqual(get_format_lazy('DATE_FORMAT'), get_format('DATE_FORMAT'))\n        self.assertIsInstance(get_format_lazy('DATE_FORMAT'), str)\n", "def test_get_format_lazy(self):\n    with translation.override('de'):\n        self.assertEqual(get_format_lazy('DATE_FORMAT'), get_format('DATE_FORMAT'))\n        self.assertIsInstance(get_format_lazy('DATE_FORMAT'), str)\n", "def test_get_format_modules_reverse(self):\n    lang = 'de'\n    modules = get_format_modules(lang, reverse=True)\n    self.assertEqual(modules, list(reversed(get_format_modules(lang))))\n", "def test_get_format_modules(self):\n    # Ensure get_format_modules returns modules in the correct order.\n    with self.settings(FORMAT_MODULE_PATH=['django.conf.locale']):\n        modules = get_format_modules(lang='en')\n        self.assertEqual(len(modules), 2)\n        self.assertEqual(modules[0].__name__, 'django.conf.locale.en.formats')\n        self.assertEqual(modules[1].__name__, 'django.conf.locale.en.formats')\n\n    with self.settings(FORMAT_MODULE_PATH=['django.conf.locale', 'myapp.locale']):\n        modules = get_format_modules(lang='en')\n        self.assertEqual(len(modules), 3)\n        self.assertEqual(modules[0].__name__, 'myapp.locale.en.formats')\n        self.assertEqual(modules[1].__name__, 'django.conf.locale.en.formats')\n        self.assertEqual(modules[2].__name__, 'django.conf.locale.en.formats')\n\n    # Ensure get_format_modules uses the correct locale.\n    with self.settings(FORMAT_MODULE_PATH=['django.conf.locale']):\n        modules = get_format_modules(lang='fr-ca')\n        self.assertEqual(len(modules), 3)\n        self.assertEqual(modules[0].__name__, 'django.conf.locale.fr_ca.formats')\n        self.assertEqual(modules[1].__name__, 'django.conf.locale.fr.formats')\n        self.assertEqual(modules[2].__name__, 'django.conf.locale.en.formats')\n"], "sample_186": ["def test_check_dependencies(self):\n    class MyAdminSite(AdminSite):\n        pass\n\n    errors = check_dependencies()\n    self.assertEqual(errors, [])\n\n    with override_settings(INSTALLED_APPS=['django.contrib.admin']):\n        errors = check_dependencies()\n        self.assertEqual(len(errors), 3)\n\n    with override_settings(\n        AUTHENTICATION_BACKENDS=['django.contrib.auth.backends.ModelBackend'],\n        TEMPLATES=[{\n            'BACKEND': 'django.template.backends.django.DjangoTemplates',\n            'DIRS': [],\n            'APP_DIRS': True,\n            'OPTIONS': {\n                'context_processors': [\n                    'django.template.context_processors.debug',\n                    'django.template.context_processors.request',\n                    'django.contrib.auth.context_processors.auth',\n                ],\n            },\n        }],\n        MIDDLEWARE=[\n            'django.contrib.sessions.middleware.SessionMiddleware',\n            'django.contrib.messages.middleware.MessageMiddleware',\n        ]\n    ):\n        errors = check_dependencies()\n        self.assertEqual(errors, [])\n\n    with override_settings(\n        AUTHENTICATION_BACKENDS=['django.contrib.auth.backends.ModelBackend'],\n        TEMPLATES=[{\n            'BACKEND': 'django.template.backends.django.DjangoTemplates',\n            'DIRS': [],\n            'APP_DIRS': True,\n            'OPTIONS': {\n                'context_processors': [\n                    'django.template.context_processors.debug',\n                    'django.contrib.auth.context_processors.auth',\n                ],\n            },\n        }],\n        MIDDLEWARE=[\n            'django.contrib.sessions.middleware.SessionMiddleware',\n            'django.contrib.messages.middleware.MessageMiddleware',\n        ]\n    ):\n        errors = check_dependencies()\n        self.assertEqual(len(errors), 1)\n\n    with override_settings(\n        AUTHENTICATION_BACKENDS=['django.contrib.auth.backends.ModelBackend'],\n        TEMPLATES=[{\n            'BACKEND': 'django.template.backends.django.DjangoTemplates',\n            'DIRS': [],\n            'APP_DIRS': True,\n            'OPTIONS': {\n                'context_processors': [\n                    'django.template.context_processors.debug',\n                    'django.template.context_processors.request',\n                    'django.contrib.auth.context_processors.auth',\n                ],\n            },\n        }],\n        MIDDLEWARE=[\n            'django.contrib.messages.middleware.MessageMiddleware',\n        ]\n    ):\n        errors = check_dependencies()\n        self.assertEqual(len(errors), 1)\n", "def test_check_dependencies(self):\n    errors = checks.run_checks()\n    self.assertEqual(errors, [])\n\n    with override_settings(INSTALLED_APPS=['django.contrib.admin']):\n        errors = checks.run_checks()\n        self.assertEqual(len(errors), 3)\n        self.assertEqual(errors[0].id, 'admin.E401')\n        self.assertEqual(errors[1].id, 'admin.E402')\n        self.assertEqual(errors[2].id, 'admin.E403')\n\n    with override_settings(\n        INSTALLED_APPS=[\n            'django.contrib.admin',\n            'django.contrib.auth',\n            'django.contrib.contenttypes',\n        ],\n        MIDDLEWARE=[\n            'django.contrib.sessions.middleware.SessionMiddleware',\n            'django.contrib.messages.middleware.MessageMiddleware',\n        ],\n    ):\n        errors = checks.run_checks()\n        self.assertEqual(len(errors), 3)\n        self.assertEqual(errors[0].id, 'admin.E404')\n        self.assertEqual(errors[1].id, 'admin.E408')\n        self.assertEqual(errors[2].id, 'admin.E409')\n", "def test_checks_against_readonly_fields(self):\n    class ReadOnlyFieldsModelAdmin(admin.ModelAdmin):\n        readonly_fields = ('readonly',)\n\n    class TestModelAdmin(ReadOnlyFieldsModelAdmin, admin.ModelAdmin):\n        readonly_fields = ('readonly',)\n        fields = ('readonly',)\n\n    errors = TestModelAdmin(Book, AdminSite()).check()\n    self.assertEqual(errors, [])\n", "def test_check_list_display_links_item(self):\n    class MyModelAdmin(admin.ModelAdmin):\n        list_display = ('field1', 'field2')\n        list_display_links = ('field3',)\n\n    errors = checks.run_checks(MyModelAdmin)\n    expected_errors = [\n        checks.Error(\n            \"The value of 'list_display_links[0]' refers to 'field3', which is not defined in 'list_display'.\",\n            obj=MyModelAdmin,\n            id='admin.E111',\n        )\n    ]\n    self.assertEqual(errors, expected_errors)\n", "def test_check_list_filter_item(self):\n    class MyModelAdmin(admin.ModelAdmin):\n        list_filter = ('nonexistent_field',)\n\n    errors = MyModelAdmin(Model, AdminSite()).check()\n    expected_errors = [\n        checks.Error(\n            \"The value of 'list_filter[0]' refers to 'nonexistent_field', which does not refer to a Field.\",\n            obj=MyModelAdmin,\n            id='admin.E116',\n        ),\n    ]\n    self.assertEqual(errors, expected_errors)\n"], "sample_187": ["def test_compress_sequence(self):\n    sequence = [b'Hello, ', b'world!']\n    compressed = list(text.compress_sequence(sequence))\n    self.assertEqual(len(compressed), 3)  # header, data, footer\n    decompressed = bytearray()\n    with gzip.GzipFile(fileobj=BytesIO(b''.join(compressed)), mode='rb') as f:\n        while True:\n            chunk = f.read(8192)\n            if not chunk:\n                break\n            decompressed.extend(chunk)\n    self.assertEqual(bytes(decompressed), b'Hello, world!')\n", "def test_compress_sequence(self):\n    data = [b'Hello, ', b'world!']\n    compressed = list(text.compress_sequence(data))\n    self.assertEqual(len(compressed), 3)  # headers, data, and tail\n    decompressed = ''.join(gzip.GzipFile(fileobj=BytesIO(b''.join(compressed)), mode='rb').read().decode('utf-8') for _ in range(1))\n    self.assertEqual(decompressed, 'Hello, world!')\n", "def test_unescape_string_literal(self):\n    self.assertEqual(text.unescape_string_literal('\"abc\"'), 'abc')\n    self.assertEqual(text.unescape_string_literal(\"'abc'\"), 'abc')\n    self.assertEqual(text.unescape_string_literal('\"a \\\\\"bc\\\\\"\"'), 'a \"bc\"')\n    self.assertEqual(text.unescape_string_literal(\"'\\\\'ab\\\\' c'\"), \"'ab' c\")\n\n    with self.assertRaises(ValueError):\n        text.unescape_string_literal('abc')\n\n    with self.assertRaises(ValueError):\n        text.unescape_string_literal('\"abc')\n", "def test_wrap(self):\n    self.assertEqual(text.wrap('This is a long string', 10), 'This is a\\nlong string')\n    self.assertEqual(text.wrap('This is a long string with a line break\\nalready in it', 10), 'This is a\\nlong string\\nwith a line\\nbreak\\nalready in\\nit')\n    self.assertEqual(text.wrap('Short string', 10), 'Short string')\n    self.assertEqual(text.wrap('This is a long string that will be wrapped to multiple lines', 5), 'This\\nis a\\nlong\\nstring\\nthat\\nwill\\nbe\\nwrapped\\nto\\nmultiple\\nlines')\n", "def test_unescape_string_literal(self):\n    self.assertEqual(text.unescape_string_literal('\"abc\"'), 'abc')\n    self.assertEqual(text.unescape_string_literal(\"'abc'\"), 'abc')\n    self.assertEqual(text.unescape_string_literal('\"a \\\\\"bc\\\\\"\"'), 'a \"bc\"')\n    self.assertEqual(text.unescape_string_literal(\"'\\\\'ab\\\\' c'\"), \"'ab' c\")\n    with self.assertRaises(ValueError):\n        text.unescape_string_literal(\"not a string literal\")\n    with self.assertRaises(ValueError):\n        text.unescape_string_literal('\"no end quote')\n"], "sample_188": ["def test_expression_pickle(self):\n    expr = F(\"num_employees\") + 5\n    pickled_expr = pickle.dumps(expr)\n    unpickled_expr = pickle.loads(pickled_expr)\n    self.assertEqual(expr, unpickled_expr)\n\n    expr = Case(\n        When(num_employees__gt=1000, then=Value('Large')),\n        default=Value('Small')\n    )\n    pickled_expr = pickle.dumps(expr)\n    unpickled_expr = pickle.loads(pickled_expr)\n    self.assertEqual(expr, unpickled_expr)\n", "def test_expressions_deepcopy(self):\n    # Ensure expressions can be deepcopied properly\n    expr = F('name') + Value(1)\n    expr_copy = deepcopy(expr)\n    self.assertEqual(expr, expr_copy)\n", "def test_expression_equality(self):\n    expr1 = F('name')\n    expr2 = F('name')\n    self.assertEqual(expr1, expr2)\n\n    expr3 = F('num_employees')\n    self.assertNotEqual(expr1, expr3)\n\n    expr4 = ExpressionWrapper(F('name'), output_field=CharField())\n    expr5 = ExpressionWrapper(F('name'), output_field=CharField())\n    self.assertEqual(expr4, expr5)\n\n    expr6 = ExpressionWrapper(F('num_employees'), output_field=IntegerField())\n    self.assertNotEqual(expr4, expr6)\n", "def test_expression_pickle(self):\n    # Ensure F expressions can be pickled and unpickled correctly.\n    pickle_expr = pickle.loads(pickle.dumps(F('name')))\n    self.assertEqual(pickle_expr, F('name'))\n\n    # Ensure RawSQL expressions can be pickled and unpickled correctly.\n    pickle_expr = pickle.loads(pickle.dumps(RawSQL('1', ())))\n    self.assertEqual(pickle_expr.sql, '1')\n    self.assertEqual(pickle_expr.params, ())\n\n    # Ensure Case expressions can be pickled and unpickled correctly.\n    case_expr = Case(When(name='test', then='foo'), default='bar')\n    pickle_expr = pickle.loads(pickle.dumps(case_expr))\n    self.assertEqual(pickle_expr.cases[0].condition, case_expr.cases[0].condition)\n    self.assertEqual(pickle_expr.default, case_expr.default)\n\n    # Ensure When expressions can be pickled and unpickled correctly.\n    when_expr = When(name='test', then='foo')\n    pickle_expr = pickle.loads(pickle.dumps(when_expr))\n    self.assertEqual(pickle_expr.condition, when_expr.condition)\n    self.assertEqual(pickle_expr.result, when_expr.result)\n\n    # Ensure Exists expressions can be pickled and unpickled correctly.\n    exists_expr = Exists(Company.objects.all())\n    pickle_expr = pickle.loads(pickle.dumps(exists_expr))\n    self.assertEqual(pickle_expr.queryset.query.sql, exists_expr.queryset.query.sql)\n", "def test_expression_is_summary(self):\n    subquery = Company.objects.values('name').annotate(\n        employee_count=Count('employees'),\n    ).order_by('employee_count')\n    qs = Company.objects.filter(\n        employee_count__in=[subquery.first().employee_count],\n    )\n    self.assertTrue(qs.query.expressions[0].is_summary)\n"], "sample_189": ["def test_base_cache_key_warnings(self):\n    # Test that the BaseCache class warns about keys that would cause errors with memcached\n    cache = caches['default']\n    key_with_control_char = 'test\\ntest'\n    key_too_long = 'a' * (MEMCACHE_MAX_KEY_LENGTH + 1)\n\n    with warnings.catch_warnings(record=True) as w:\n        cache.validate_key(key_with_control_char)\n        self.assertEqual(len(w), 1)\n        self.assertIn(KEY_ERRORS_WITH_MEMCACHED_MSG % key_with_control_char, str(w[0].message))\n\n    with warnings.catch_warnings(record=True) as w:\n        cache.validate_key(key_too_long)\n        self.assertEqual(len(w), 1)\n        self.assertIn(KEY_ERRORS_WITH_MEMCACHED_MSG % key_too_long, str(w[0].message))\n", "    def test_default_key_func(self):\n        key = 'my_key'\n        key_prefix = 'my_prefix'\n        version = 1\n\n        expected_key = '%s:%s:%s' % (key_prefix, version, key)\n        self.assertEqual(default_key_func(key, key_prefix, version), expected_key)\n", "    def test_make_key(self):\n        cache = caches['default']\n        key = 'test-key'\n        self.assertEqual(cache.make_key(key), f'{cache.key_prefix}:{cache.version}:{key}')\n", "    def test_default_key_func(self):\n        key = 'my_key'\n        key_prefix = 'my_prefix'\n        version = 1\n        self.assertEqual(\n            default_key_func(key, key_prefix, version),\n            f'{key_prefix}:{version}:{key}'\n        )\n", "    def test_default_key_func(self):\n        key = 'test_key'\n        key_prefix = 'test_prefix'\n        version = 1\n\n        expected_key = '%s:%s:%s' % (key_prefix, version, key)\n        self.assertEqual(default_key_func(key, key_prefix, version), expected_key)\n"], "sample_190": ["def test_exact_lookup_with_bool(self):\n    # Test that an exact lookup with a boolean value doesn't generate a BOOL\n    # condition on the database (#29493).\n    Article.objects.filter(pub_date__isnull=False).all()\n    sql = connection.queries[-1]['sql']\n    self.assertIn('WHERE \"lookup_tests_article\".\"pub_date\" IS NOT NULL', sql)\n\n    Article.objects.filter(pub_date__isnull=True).all()\n    sql = connection.queries[-1]['sql']\n    self.assertIn('WHERE \"lookup_tests_article\".\"pub_date\" IS NULL', sql)\n", "def test_exact_lookup_with_bool_value(self):\n    # Create a model instance with a boolean field.\n    class MyModel(models.Model):\n        bool_field = models.BooleanField()\n\n    # Create a model instance with bool_field=True.\n    my_obj = MyModel.objects.create(bool_field=True)\n\n    # Use the exact lookup with a boolean value.\n    self.assertTrue(MyModel.objects.filter(bool_field__exact=True).exists())\n    self.assertFalse(MyModel.objects.filter(bool_field__exact=False).exists())\n\n    # Use the exact lookup with a non-boolean value.\n    with self.assertRaisesMessage(ValueError, 'The QuerySet value for an exact lookup must be limited to one result using slicing.'):\n        MyModel.objects.filter(bool_field__exact='True')\n\n    # Test that a RemovedInDjango40Warning is raised when passing a non-boolean value.\n    with self.assertWarnsMessage(RemovedInDjango40Warning, 'Using a non-boolean value for an exact lookup on a BooleanField is deprecated, use True or False instead.'):\n        MyModel.objects.filter(bool_field__exact='1')\n", "def test_in_lookup_value_is_iterable(self):\n    a8 = Article.objects.create(headline='Article 8', pub_date=datetime(2005, 7, 28))\n    a9 = Article.objects.create(headline='Article 9', pub_date=datetime(2005, 7, 29))\n    a10 = Article.objects.create(headline='Article 10', pub_date=datetime(2005, 7, 30))\n\n    articles = Article.objects.filter(id__in=[a8.id, a9.id, a10.id])\n    self.assertQuerysetEqual(articles, [a8, a9, a10], attrgetter('headline'))\n\n    # Test that an empty list works.\n    self.assertQuerysetEqual(Article.objects.filter(id__in=[]), [])\n", "def test_lookup_prepare_rhs(self):\n    lookup = Article.objects.filter(headline__exact='Article 1')\n    self.assertIsInstance(lookup.query.where.children[0].rhs, str)\n\n    lookup = Article.objects.filter(pub_date__exact=datetime(2005, 7, 26))\n    self.assertIsInstance(lookup.query.where.children[0].rhs, datetime)\n", "def test_exact_lookup_with_none(self):\n    # Ensure that an exact lookup with None returns no results.\n    with self.assertRaises(ValueError):\n        Article.objects.filter(headline__exact=None)\n"], "sample_191": ["    def test_empty(self):\n        self.assertEqual(autoreload.common_roots([]), ())\n", "def test_iter_modules_and_files_zipfile(self):\n    with tempfile.TemporaryDirectory() as tmpdir:\n        # Create a zip file containing a Python module.\n        zip_path = Path(tmpdir) / 'test.zip'\n        with zipfile.ZipFile(zip_path, 'w') as zip_file:\n            zip_file.writestr('test.py', b'pass')\n\n        # Add the zip file to sys.path and import the module.\n        with extend_sys_path(tmpdir):\n            self.import_and_cleanup('test')\n            self.assertFileFound(zip_path)\n", "    def test_zip_import(self):\n        tmpdir = tempfile.mkdtemp()\n        self.addCleanup(shutil.rmtree, tmpdir)\n        zip_filename = Path(tmpdir) / 'test.zip'\n        with zipfile.ZipFile(str(zip_filename), 'w') as z:\n            z.writestr('test/__init__.py', '')\n            z.writestr('test/module.py', '')\n        sys.path.append(str(zip_filename))\n        self.addCleanup(lambda: sys.path.remove(str(zip_filename)))\n        import_module('test.module')\n        self.addCleanup(lambda: sys.modules.pop('test.module', None))\n        self.clear_autoreload_caches()\n        self.assertIn(zip_filename, list(autoreload.iter_all_python_module_files()))\n", "def test_iter_modules_and_files_zip_file(self):\n    # Create a temporary zip file containing a Python module.\n    with tempfile.NamedTemporaryFile(suffix='.zip', delete=False) as tmp_file:\n        with zipfile.ZipFile(tmp_file.name, 'w') as zip_file:\n            zip_file.writestr('test_module.py', b'')\n        self.addCleanup(os.remove, tmp_file.name)\n\n        # Add the zip file to sys.path and import the module.\n        with extend_sys_path(tmp_file.name):\n            self.import_and_cleanup('test_module')\n\n            # The zip file itself should be in the output.\n            self.assertFileFound(Path(tmp_file.name))\n\n            # The individual module file inside the zip file should not be in the output.\n            self.assertFileNotFound(Path(tmp_file.name) / 'test_module.py')\n", "    def test_common_roots(self):\n        paths = [\n            Path('/home/user/project/app1/models.py'),\n            Path('/home/user/project/app1/views.py'),\n            Path('/home/user/project/app2/models.py'),\n            Path('/home/user/project/templates/base.html'),\n        ]\n        roots = autoreload.common_roots(paths)\n        self.assertEqual(roots, (Path('/home/user/project'),))\n"], "sample_192": ["def test_formset_non_form_errors(self):\n    class FormSet(BaseFormSet):\n            raise ValidationError('Error in formset')\n\n    Form = type('Form', (forms.Form,), {'field': forms.CharField()})\n    FormSet = formset_factory(Form, formset=FormSet)\n    formset = FormSet({'form-0-field': 'value'})\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(formset.non_form_errors(), ['Error in formset'])\n", "def test_formset_absolute_max(self):\n    # Test that absolute_max is enforced.\n    ChoiceFormSetAbsoluteMax = formset_factory(Choice, max_num=5, absolute_max=4)\n    formset = self.make_choiceformset([('1', '1'), ('2', '2'), ('3', '3'), ('4', '4')], ChoiceFormSetAbsoluteMax)\n    self.assertTrue(formset.is_valid())\n\n    # Test that exceeding absolute_max raises an error.\n    formset = self.make_choiceformset([('1', '1'), ('2', '2'), ('3', '3'), ('4', '4'), ('5', '5')], ChoiceFormSetAbsoluteMax)\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(len(formset.non_form_errors()), 1)\n    self.assertIn('Please submit 4 or fewer forms.', str(formset.non_form_errors()))\n", "def test_formset_absolute_max(self):\n    ChoiceFormSet = formset_factory(Choice, max_num=10, absolute_max=15)\n    formset = self.make_choiceformset([('1', '1')] * 16)\n    with self.assertRaises(ValidationError) as cm:\n        formset.is_valid()\n    self.assertEqual(cm.exception.code, 'too_many_forms')\n", "def test_formset_absolute_max(self):\n    ChoiceFormSet = formset_factory(Choice, max_num=10, absolute_max=15)\n    formset = self.make_choiceformset(formset_data=[('a', 1)] * 20, formset_class=ChoiceFormSet)\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(len(formset.forms), 15)  # Should be capped at absolute_max\n    self.assertEqual(formset.non_form_errors(), ['Please submit 10 or fewer forms.'])\n", "def test_formset_management_form_validation(self):\n    formset = self.make_choiceformset([('A', '1'), ('B', '2')])\n    with mock.patch('django.forms.formsets.ValidationError') as mock_error:\n        formset.management_form.cleaned_data['TOTAL_FORMS'] = 'a'\n        formset._management_form = None\n        formset.management_form.is_valid()\n        self.assertTrue(mock_error.called)\n        self.assertEqual(mock_error.call_count, 1)\n        mock_error.assert_called_once_with(\n            _('ManagementForm data is missing or has been tampered with'),\n            code='missing_management_form',\n        )\n"], "sample_193": ["    def test_related_field(self):\n        field = models.ForeignKey('app.Model', on_delete=models.CASCADE)\n        self.assertEqual(field.remote_field.model, 'app.Model')\n        self.assertEqual(field.many_to_many, False)\n        self.assertEqual(field.one_to_one, False)\n        self.assertEqual(field.many_to_one, True)\n        self.assertEqual(field.one_to_many, False)\n", "    def test_field_deconstruction(self):\n        field = models.ForeignKey('testapp.Model', on_delete=models.CASCADE)\n        name, path, args, kwargs = field.deconstruct()\n        new_field = field.__class__(*args, **kwargs)\n        self.assertEqual(field.remote_field.model, new_field.remote_field.model)\n", "    def test_fields_from_model(self):\n        \"\"\"\n        Fields are properly extracted from a model during ModelState creation.\n        \"\"\"\n        fields = ModelState('migrations', 'model', [\n            ('id', models.AutoField(primary_key=True)),\n            ('name', models.CharField(max_length=255)),\n            ('related', models.ForeignKey('self', on_delete=models.CASCADE)),\n            ('m2m', models.ManyToManyField('self')),\n        ]).fields\n\n        self.assertEqual(fields[0][1].primary_key, True)\n        self.assertEqual(fields[1][1].max_length, 255)\n        self.assertEqual(fields[2][1].remote_field.model, 'self')\n        self.assertEqual(fields[3][1].remote_field.model, 'self')\n", "    def test_related_field_deconstruction(self):\n        field = models.ForeignKey('testapp.Model', on_delete=models.CASCADE)\n        name, path, args, kwargs = field.deconstruct()\n        self.assertEqual(kwargs['to'], 'testapp.Model')\n", "    def test_related_field_deconstruction(self):\n        model = ModelState(\n            app_label='migrations',\n            name='model',\n            fields=[\n                ('id', models.AutoField(primary_key=True)),\n                ('other', models.ForeignKey('migrations.Model', models.CASCADE)),\n            ],\n        )\n        field = model.get_field_by_name('other')\n        name, path, args, kwargs = field.deconstruct()\n        new_field = field.__class__(*args, **kwargs)\n        self.assertEqual(field.remote_field.model, new_field.remote_field.model)\n        self.assertEqual(field.remote_field.related_name, new_field.remote_field.related_name)\n        self.assertEqual(field.remote_field.on_delete, new_field.remote_field.on_delete)\n"], "sample_194": ["    def test_create_check_constraint(self):\n        # Create a model with a check constraint.\n        class CheckModel(models.Model):\n            value = models.IntegerField()\n            class Meta:\n                constraints = [\n                    models.CheckConstraint(check=models.Q(value__gte=0), name='value_gte_0')\n                ]\n\n        # Ensure the check constraint was created.\n        with connection.schema_editor() as schema_editor:\n            schema_editor.create_model(CheckModel)\n        constraints = get_constraints(CheckModel._meta.db_table)\n        self.assertIn('value_gte_0', [c['name'] for c in constraints])\n        self.assertEqual(1, len([c for c in constraints if c['type'] == 'check']))\n", "    def test_check_constraint(self):\n        # Create a model with a check constraint.\n        class CheckConstraintModel(models.Model):\n            value = models.IntegerField()\n            class Meta:\n                constraints = [\n                    models.CheckConstraint(check=models.Q(value__gte=0), name='value_gte_0')\n                ]\n\n        # Ensure the constraint is enforced at the database level.\n        with self.assertRaises(IntegrityError):\n            CheckConstraintModel.objects.create(value=-1)\n", "    def test_check_constraint(self):\n        # Create a model with a check constraint.\n        class Product(models.Model):\n            price = models.DecimalField(max_digits=10, decimal_places=2)\n            class Meta:\n                constraints = [\n                    models.CheckConstraint(check=models.Q(price__gte=0), name='price_non_negative'),\n                ]\n\n        # Make sure the check constraint is created.\n        with connection.schema_editor() as schema_editor:\n            schema_editor.create_model(Product)\n\n        # Try to create an object that violates the check constraint.\n        with self.assertRaises(IntegrityError):\n            Product.objects.create(price=-1)\n\n        # Make sure the check constraint allows a valid value.\n        Product.objects.create(price=0)\n", "    def test_check_constraint(self):\n        class Model(models.Model):\n            value = models.IntegerField()\n\n            class Meta:\n                constraints = [\n                    models.CheckConstraint(check=models.Q(value__gte=0), name='value_gte_0'),\n                ]\n\n        with connection.schema_editor() as schema_editor:\n            schema_editor.create_model(Model)\n\n        try:\n            with atomic():\n                Model.objects.create(value=-1)\n                self.fail('Negative value should raise IntegrityError')\n        except IntegrityError:\n            pass\n\n        Model.objects.create(value=1)  # Should not raise an error.\n\n        constraint_sql = get_constraints(Model._meta.db_table)\n        self.assertEqual(len(constraint_sql), 1)\n        self.assertEqual(constraint_sql[0]['name'], 'value_gte_0')\n", "    def test_check_constraint(self):\n        # Create a model with a check constraint.\n        class Product(models.Model):\n            price = models.DecimalField(max_digits=10, decimal_places=2)\n            class Meta:\n                constraints = [\n                    models.CheckConstraint(check=models.Q(price__gte=0), name='price_gte_0'),\n                ]\n\n        # Create the table for the model.\n        with connection.schema_editor() as schema_editor:\n            schema_editor.create_model(Product)\n\n        # Verify that the constraint was created.\n        constraints = get_constraints(Product._meta.db_table)\n        self.assertIn(('price_gte_0', 'check'), [(c['name'], c['type']) for c in constraints])\n\n        # Try to insert an invalid value.\n        try:\n            Product.objects.create(price=-1)\n            self.fail('IntegrityError not raised')\n        except IntegrityError:\n            pass\n\n        # Try to insert a valid value.\n        Product.objects.create(price=1)\n\n        # Drop the table.\n        with connection.schema_editor() as schema_editor:\n            schema_editor.delete_model(Product)\n"], "sample_195": ["    def test_adapt_datetimefield_value(self):\n        ops = connection.ops\n        dt = timezone.make_aware(timezone.datetime(2022, 1, 1, 12, 0, 0), timezone.utc)\n        self.assertEqual(ops.adapt_datetimefield_value(dt), '2022-01-01 12:00:00')\n        self.assertIsNone(ops.adapt_datetimefield_value(None))\n", "    def test_bulk_batch_size(self):\n        ops = connection.ops\n        self.assertEqual(ops.bulk_batch_size(['field1', 'field2'], []), 0)\n        self.assertEqual(ops.bulk_batch_size(['field1'], [1, 2, 3]), 500)\n        self.assertGreaterEqual(ops.bulk_batch_size(['field1', 'field2'], [1, 2, 3]), 1)\n", "    def test_bulk_batch_size(self):\n        ops = connection.ops\n        self.assertEqual(ops.bulk_batch_size(['field1'], ['obj1', 'obj2']), 500)\n        self.assertGreater(ops.bulk_batch_size(['field1', 'field2'], ['obj1', 'obj2']), 0)\n        self.assertEqual(ops.bulk_batch_size([], ['obj1', 'obj2']), 2)\n", "    def test_bulk_batch_size(self):\n        ops = connection.ops\n        self.assertEqual(ops.bulk_batch_size(['field1'], []), 500)\n        self.assertEqual(ops.bulk_batch_size(['field1', 'field2'], []), \n                         ops.connection.features.max_query_params // 2)\n        self.assertEqual(ops.bulk_batch_size([], [1, 2, 3]), 3)\n", "    def test_adapt_datetimefield_value(self):\n        ops = connection.ops\n        dt = timezone.now()\n        self.assertEqual(ops.adapt_datetimefield_value(dt), str(timezone.make_naive(dt, connection.timezone)))\n        self.assertIsNone(ops.adapt_datetimefield_value(None))\n"], "sample_196": ["def test_adapt_unknown_value(self):\n    self.assertIsNone(self.ops.adapt_unknown_value(None))\n    self.assertEqual(self.ops.adapt_unknown_value(123), 123)\n    self.assertEqual(self.ops.adapt_unknown_value('hello'), 'hello')\n    self.assertIsInstance(self.ops.adapt_unknown_value(decimal.Decimal('12.34')), str)\n    self.assertIsInstance(self.ops.adapt_unknown_value(timezone.now()), str)\n", "def test_adapt_decimalfield_value(self):\n    value = decimal.Decimal('12.345')\n    self.assertEqual(self.ops.adapt_decimalfield_value(value), '12.345')\n", "def test_adapt_decimalfield_value(self):\n    value = decimal.Decimal('3.14')\n    max_digits = 5\n    decimal_places = 2\n    self.assertEqual(\n        self.ops.adapt_decimalfield_value(value, max_digits, decimal_places),\n        '3.14'\n    )\n", "def test_adapt_unknown_value(self):\n    values = [\n        (1, 1),\n        ('a', 'a'),\n        (decimal.Decimal('1.2'), decimal.Decimal('1.2')),\n        (timezone.now(), str(timezone.now())),\n        (timezone.make_naive(timezone.now()), str(timezone.make_naive(timezone.now()))),\n    ]\n    for value, expected in values:\n        with self.subTest(value=value):\n            self.assertEqual(self.ops.adapt_unknown_value(value), expected)\n", "def test_date_extract_sql(self):\n    with self.assertRaises(NotImplementedError) as cm:\n        self.ops.date_extract_sql('year', 'field_name')\n    self.assertEqual(str(cm.exception), self.may_require_msg % 'date_extract_sql')\n"], "sample_198": ["def test_expression_pickle(self):\n    expr = F('num_employees') + Value(5)\n    pickled_expr = pickle.loads(pickle.dumps(expr))\n    self.assertEqual(expr, pickled_expr)\n\n    output_field = IntegerField()\n    expr = ExpressionWrapper(F('num_employees'), output_field=output_field)\n    pickled_expr = pickle.loads(pickle.dumps(expr))\n    self.assertEqual(expr, pickled_expr)\n    self.assertEqual(expr.output_field, pickled_expr.output_field)\n\n    expr = Case(When(num_employees__gt=10, then=Value(1)), default=Value(0))\n    pickled_expr = pickle.loads(pickle.dumps(expr))\n    self.assertEqual(expr, pickled_expr)\n", "def test_expression_pickle(self):\n    expr = F('num_employees') + Value(10)\n    pickled_expr = pickle.dumps(expr)\n    unpickled_expr = pickle.loads(pickled_expr)\n    self.assertEqual(expr, unpickled_expr)\n\n    # Test with more complex expression.\n    expr = (F('num_employees') + Value(10)) * (F('num_chairs') - Value(1))\n    pickled_expr = pickle.dumps(expr)\n    unpickled_expr = pickle.loads(pickled_expr)\n    self.assertEqual(expr, unpickled_expr)\n", "def test_expression_pickle(self):\n    expr = F('num_employees') + Value(5)\n    pickled_expr = pickle.dumps(expr)\n    unpickled_expr = pickle.loads(pickled_expr)\n    self.assertEqual(unpickled_expr, expr)\n\n    # Check that the unpickled expression can be used in a query.\n    result = Company.objects.values('name').annotate(\n        num_employees_plus_five=unpickled_expr,\n    ).get(name='Example Inc.')\n    self.assertEqual(result['num_employees_plus_five'], 2305)\n", "def test_expression_pickle(self):\n    expr = F('name') + Value('test')\n    pickled_expr = pickle.loads(pickle.dumps(expr))\n    self.assertEqual(expr, pickled_expr)\n\n    # Test that a complex expression with multiple nodes can be pickled and unpickled correctly.\n    expr = (F('name') + Value('test')) * (F('num_employees') - Value(5))\n    pickled_expr = pickle.loads(pickle.dumps(expr))\n    self.assertEqual(expr, pickled_expr)\n", "def test_combinable(self):\n    combined = F('num_employees') + Value(5)\n    self.assertIsInstance(combined, Expression)\n    self.assertEqual(str(combined), \"F(num_employees) + Value(5)\")\n    combined = combined - F('num_chairs')\n    self.assertIsInstance(combined, Expression)\n    self.assertEqual(str(combined), \"(F(num_employees) + Value(5)) - F(num_chairs)\")\n    qs = Company.objects.values('name').annotate(new_value=combined).order_by('name')\n    self.assertQuerysetEqual(qs, [\n        {'name': 'Example Inc.', 'new_value': 2300},\n        {'name': 'Foobar Ltd.', 'new_value': 4},\n        {'name': 'Test GmbH', 'new_value': 36},\n    ])\n"], "sample_197": ["def test_timesince_depth(self):\n    with translation.override('en'):\n        self.assertEqual(timesince(self.t - self.oneweek, self.t, depth=1), '1 week')\n        self.assertEqual(timesince(self.t - self.oneweek - self.oneday, self.t, depth=2), '1 week, 1 day')\n        self.assertEqual(timesince(self.t - self.oneyear - self.onemonth, self.t, depth=3), '1 year, 1 month')\n        self.assertEqual(timesince(self.t - self.oneyear - self.onemonth - self.oneweek, self.t, depth=4), '1 year, 1 month, 1 week')\n", "def test_timesince_depth(self):\n    with translation.override('en'):\n        self.assertEqual(timesince(self.t - self.oneweek, depth=1), '1 week')\n        self.assertEqual(timesince(self.t - self.oneweek - self.oneday, depth=2), '1 week, 1 day')\n        self.assertEqual(timesince(self.t - self.oneweek - self.oneday, depth=1), '1 week')\n", "def test_timesince_depth(self):\n    with translation.override('en'):\n        self.assertEqual(timesince(self.t - self.oneday, self.t, depth=1), '1 day')\n        self.assertEqual(timesince(self.t - self.oneweek, self.t, depth=2), '1 week, 0 days')\n        self.assertEqual(timesince(self.t - self.onemonth, self.t, depth=3), '1 month, 0 weeks, 0 days')\n", "def test_timesince_depth(self):\n    self.assertEqual(timesince(self.t - self.oneweek, self.t, depth=1), '1 week')\n    self.assertEqual(timesince(self.t - self.oneweek - self.oneday, self.t, depth=2), '1 week, 1 day')\n    self.assertEqual(timesince(self.t - self.oneweek - self.oneday - self.onehour, self.t, depth=3), '1 week, 1 day, 1 hour')\n", "def test_timesince_depth(self):\n    with translation.override('en'):\n        self.assertEqual(timesince(self.t - self.oneweek, self.t, depth=1), '1 week')\n        self.assertEqual(timesince(self.t - self.oneweek - self.oneday, self.t, depth=2), '1 week, 1 day')\n        self.assertEqual(timesince(self.t - self.oneweek - self.oneday - self.onehour, self.t, depth=3), '1 week, 1 day, 1 hour')\n"], "sample_199": ["def test_expression_wrapper(self):\n    wrapper = ExpressionWrapper(Value(1), output_field=IntegerField())\n    self.assertEqual(wrapper.output_field.get_internal_type(), 'IntegerField')\n    self.assertIsInstance(wrapper.output_field, IntegerField)\n    self.assertEqual(wrapper.as_sql(connection), ('%s', [1]))\n", "def test_expression_in_f_group_by(self):\n    # Ensure that an expression can be used in a .values() clause with a F()\n    # expression in the same query.\n    books = Book.objects.annotate(extra=RawSQL(\"pages * rating\", ())).values('publisher__name', 'extra')\n    self.assertEqual(books.query.annotation_select, {'extra': RawSQL('pages * rating', ())})\n    self.assertEqual(books.query.values_select, ['publisher__name', 'extra'])\n    books = list(books)\n    self.assertEqual(len(books), 5)\n    for book in books:\n        self.assertIn(book['publisher__name'], ['Apress', 'Sams', 'Prentice Hall', 'Morgan Kaufmann'])\n        self.assertIn(book['extra'], [2008.0, 1584.0, 1196.0, 1300.0, 5304.0])\n", "def test_non_aggregate_annotation_raw_sql(self):\n    raw_sql = RawSQL(\"SELECT COUNT(*) FROM tests_author\", ())\n    authors = Author.objects.annotate(count=raw_sql).values_list('count', flat=True)\n    self.assertEqual(authors[0], Author.objects.count())\n", "def test_exists_subquery(self):\n    subquery = Book.objects.filter(contact=OuterRef('pk'))\n    authors = Author.objects.annotate(has_book=Exists(subquery)).filter(has_book=True)\n    self.assertEqual(authors.count(), 9)\n    self.assertQuerysetEqual(\n        authors.order_by('name'),\n        ['Adrian Holovaty', 'Brad Dayley', 'Jacob Kaplan-Moss', 'James Bennett',\n         'Jeffrey Forcier', 'Paul Bissex', 'Peter Norvig', 'Stuart Russell', 'Wesley J. Chun'],\n        lambda a: a.name,\n    )\n", "def test_expression_list(self):\n    authors = Author.objects.annotate(\n        num_friends=Count('friends'),\n        sum_friends_age=Sum('friends__age')\n    ).values_list(\n        'name',\n        ExpressionList(\n            F('num_friends'),\n            F('sum_friends_age'),\n            output_field=IntegerField()\n        )\n    )\n    for author in authors:\n        name, counts = author\n        with self.subTest(name=name):\n            self.assertIsInstance(counts, tuple)\n            self.assertEqual(len(counts), 2)\n"], "sample_200": ["def test_sanitize_address(self):\n    # Test that sanitize_address correctly handles various inputs.\n    self.assertEqual(sanitize_address('test@example.com', 'utf-8'), 'test@example.com')\n    self.assertEqual(sanitize_address(('Name', 'test@example.com'), 'utf-8'), 'Name <test@example.com>')\n    self.assertEqual(sanitize_address(('Name', 'test@invalid'), 'utf-8'), 'Name <test@invalid>')\n\n    # Test that sanitize_address raises an error on invalid input.\n    with self.assertRaises(ValueError):\n        sanitize_address('invalid', 'utf-8')\n    with self.assertRaises(ValueError):\n        sanitize_address(('Name', 'invalid'), 'utf-8')\n", "def test_sanitize_address(self):\n    # Test that sanitize_address correctly handles non-ASCII values and IDs\n    self.assertEqual(\n        sanitize_address('test@example.com', 'utf-8'),\n        'test@example.com',\n    )\n    self.assertEqual(\n        sanitize_address(('Test User', 'test@example.com'), 'utf-8'),\n        'Test User <test@example.com>',\n    )\n    self.assertEqual(\n        sanitize_address(('Test User', 'test@xN--mller-kva.com'), 'utf-8'),\n        'Test User <test@xn--mller-kva.com>',\n    )\n    self.assertEqual(\n        sanitize_address(('Test User', 'test@mllerkv\u00e6\u00f0i.example'), 'utf-8'),\n        '=?utf-8?q?Test_User?= <test@xn--mllerkvi-2g4c8c.example>',\n    )\n    with self.assertRaises(ValueError):\n        sanitize_address('\\n@example.com', 'utf-8')\n    with self.assertRaises(ValueError):\n        sanitize_address(('Test\\nUser', 'test@example.com'), 'utf-8')\n", "def test_sanitize_address(self):\n    self.assertEqual(sanitize_address('test@example.com', 'utf-8'), 'test@example.com')\n    self.assertEqual(sanitize_address(('Name', 'test@example.com'), 'utf-8'), 'Name <test@example.com>')\n    self.assertEqual(sanitize_address('Name <test@example.com>', 'utf-8'), 'Name <test@example.com>')\n\n    # Non-ASCII name\n    self.assertEqual(\n        sanitize_address(('Name\u00fc', 'test@example.com'), 'utf-8'),\n        '=?utf-8?q?Name=C3=BC?= <test@example.com>'\n    )\n\n    # Non-ASCII address\n    with self.assertRaises(ValueError):\n        sanitize_address('test@ex\u00e4mple.com', 'utf-8')\n\n    # Newline in address\n    with self.assertRaises(ValueError):\n        sanitize_address('test@exa\\nmple.com', 'utf-8')\n", "def test_sanitize_address(self):\n    # Test that sanitize_address handles empty address strings.\n    self.assertEqual(sanitize_address('', 'utf-8'), '')\n\n    # Test that sanitize_address handles single email addresses correctly.\n    self.assertEqual(sanitize_address('test@example.com', 'utf-8'), 'test@example.com')\n\n    # Test that sanitize_address handles name and email address pairs correctly.\n    self.assertEqual(sanitize_address(('John Doe', 'test@example.com'), 'utf-8'), 'John Doe <test@example.com>')\n\n    # Test that sanitize_address handles internationalized domain names correctly.\n    self.assertEqual(sanitize_address('test@xn--example.com', 'utf-8'), 'test@example.com')\n\n    # Test that sanitize_address raises a ValueError when the address is invalid.\n    with self.assertRaises(ValueError):\n        sanitize_address('invalid_email', 'utf-8')\n", "def test_sanitize_address(self):\n    # Test that sanitize_address handles invalid addresses correctly.\n    self.assertEqual(sanitize_address('example@example.com', 'utf-8'), 'example@example.com')\n    self.assertEqual(sanitize_address('Example Name <example@example.com>', 'utf-8'), 'Example Name <example@example.com>')\n    with self.assertRaises(ValueError):\n        sanitize_address('Invalid Address', 'utf-8')\n    with self.assertRaises(ValueError):\n        sanitize_address('example@example.com\\nCC: spammer@example.com', 'utf-8')\n    # Test that sanitize_address handles non-ASCII names correctly.\n    self.assertEqual(\n        sanitize_address('Example N\u00e4me <example@example.com>', 'utf-8'),\n        '=?utf-8?q?Example_N=C3=A4me?= <example@example.com>',\n    )\n"], "sample_201": ["def test_process_messages(self):\n    messages = [\n        Message(constants.INFO, 'Test message'),\n        Message(constants.ERROR, mark_safe('Error message')),\n        Message(constants.SUCCESS, 'Success message', extra_tags='extra-tag'),\n    ]\n    encoder = MessageEncoder()\n    encoded_messages = encoder.encode(messages)\n    decoder = MessageDecoder()\n    decoded_messages = decoder.process_messages(json.loads(encoded_messages))\n\n    self.assertEqual(len(decoded_messages), 3)\n    self.assertIsInstance(decoded_messages[0], Message)\n    self.assertIsInstance(decoded_messages[1], Message)\n    self.assertIsInstance(decoded_messages[2], Message)\n\n    self.assertEqual(decoded_messages[0].level, constants.INFO)\n    self.assertEqual(decoded_messages[0].message, 'Test message')\n\n    self.assertEqual(decoded_messages[1].level, constants.ERROR)\n    self.assertEqual(decoded_messages[1].message, mark_safe('Error message'))\n\n    self.assertEqual(decoded_messages[2].level, constants.SUCCESS)\n    self.assertEqual(decoded_messages[2].message, 'Success message')\n    self.assertEqual(decoded_messages[2].extra_tags, 'extra-tag')\n", "def test_legacy_hash_removal(self):\n    # Set up the storage with some messages and encode them using the legacy hash\n    storage = self.storage_class(FakeRequest())\n    messages = [Message(constants.INFO, 'Test message')]\n    encoded_data = storage._encode(messages)\n    bits = encoded_data.split('$', 1)\n    hash_, value = bits\n\n    # Decode the data using the legacy hash method\n    decoded_data = storage._legacy_decode(encoded_data)\n    self.assertEqual(decoded_data, value)\n\n    # Now decode the data again without the legacy hash method\n    try:\n        decoded_data = storage.signer.unsign(encoded_data)\n    except signing.BadSignature:\n        decoded_data = None\n    self.assertIsNone(decoded_data)\n\n    # Test that the new hash is used when encoding\n    new_encoded_data = storage._encode(messages)\n    try:\n        decoded_data = storage.signer.unsign(new_encoded_data)\n    except signing.BadSignature:\n        decoded_data = None\n    self.assertIsNotNone(decoded_data)\n", "def test_legacy_hash_removed_in_django40(self):\n    storage = self.storage_class(FakeRequest())\n    value = 'legacy_value'\n    legacy_hash = storage._legacy_hash(value)\n    data = f'{legacy_hash}${value}'\n\n    # Legacy hash should still be accepted in Django 3.1.\n    with warnings.catch_warnings(record=True) as w:\n        decoded = storage._decode(data)\n        self.assertEqual(decoded, value)\n\n    # Legacy hash will not be accepted in Django 4.0.\n    with override_settings(VERSION='4.0'):\n        with warnings.catch_warnings(record=True) as w:\n            decoded = storage._decode(data)\n            self.assertIsNone(decoded)\n            self.assertEqual(len(w), 1)\n            self.assertIs(w[0].category, RemovedInDjango40Warning)\n", "def test_legacy_decode(self):\n    storage = self.storage_class(FakeRequest())\n    messages = [Message(constants.INFO, 'Test')]\n    encoded_data = storage._encode(messages)\n    bits = encoded_data.split('$', 1)\n    value = bits[1]\n    hash_ = storage._legacy_hash(value)\n    legacy_encoded_data = f\"{hash_}${value}\"\n    self.assertEqual(storage._legacy_decode(legacy_encoded_data), value)\n    self.assertIsNone(storage._legacy_decode(' invalid data '))\n    self.assertIsNone(storage._legacy_decode(' invalid$format '))\n", "def test_legacy_hash(self):\n    storage = self.storage_class(FakeRequest())\n    value = 'test_value'\n    self.assertEqual(storage._legacy_hash(value), salted_hmac('django.contrib.messages', value).hexdigest())\n"], "sample_202": ["def test_max_cookie_size(self):\n    storage = self.storage_class(FakeRequest())\n    response = FakeResponse()\n    messages = [\n        Message(constants.INFO, 'Test message 1'),\n        Message(constants.INFO, 'Test message 2' * 1000),  # Large message\n    ]\n\n    unstored_messages = storage._store(messages, response)\n\n    self.assertEqual(len(unstored_messages), 1)\n    self.assertEqual(unstored_messages[0].message, 'Test message 2' * 1000)\n\n    cookie = response.cookies.get(storage.cookie_name)\n    self.assertLess(len(cookie.value), storage.max_cookie_size)\n", "def test_max_cookie_size(self):\n    storage = self.storage_class(FakeRequest())\n    response = FakeResponse()\n    messages = [Message(constants.INFO, 'message' + str(i)) for i in range(20)]\n    unstored_messages = storage._store(messages, response)\n\n    # max_cookie_size is 2048. We calculate the maximum number of messages that can be stored.\n    max_messages = (storage.max_cookie_size - len(storage.not_finished) - len(storage._hash(''))) // len(storage._encode([messages[0]]))\n    self.assertGreaterEqual(max_messages, 1)\n    self.assertLessEqual(max_messages, 19)\n\n    # Check that only max_messages are stored in the cookie.\n    stored_messages = storage._decode(response.cookies[storage.cookie_name].value)\n    self.assertEqual(len(stored_messages), max_messages)\n\n    # Check that the remaining messages are returned as unstored_messages.\n    self.assertEqual(len(unstored_messages), len(messages) - max_messages)\n", "def test_message_encoder_decoder(self):\n    messages = [\n        Message(constants.INFO, 'Test message'),\n        Message(constants.ERROR, mark_safe('<b>Error message</b>')),\n        Message(constants.SUCCESS, 'Success message', extra_tags='extra-tag'),\n    ]\n    encoded_messages = MessageEncoder(separators=(',', ':')).encode(messages)\n    decoded_messages = json.loads(encoded_messages, cls=MessageDecoder)\n\n    self.assertEqual(len(decoded_messages), 3)\n    for original, decoded in zip(messages, decoded_messages):\n        self.assertEqual(original.level, decoded.level)\n        self.assertEqual(original.message, decoded.message)\n        self.assertEqual(original.extra_tags, decoded.extra_tags)\n        if isinstance(original.message, SafeData):\n            self.assertIsInstance(decoded.message, SafeData)\n", "def test_process_messages(self):\n    decoder = MessageDecoder()\n    message = Message(constants.INFO, 'Test message')\n    data = [MessageEncoder.message_key, 0, constants.INFO, 'Test message']\n    self.assertEqual(decoder.process_messages(data), message)\n\n    # Test with SafeData\n    safe_message = Message(constants.INFO, mark_safe('Test message'))\n    data = [MessageEncoder.message_key, 1, constants.INFO, 'Test message']\n    self.assertEqual(decoder.process_messages(data), safe_message)\n\n    # Test with extra tags\n    message = Message(constants.INFO, 'Test message', 'extra_tag')\n    data = [MessageEncoder.message_key, 0, constants.INFO, 'Test message', 'extra_tag']\n    self.assertEqual(decoder.process_messages(data), message)\n", "def test_cookie_storage_max_cookie_size(self):\n    storage = self.storage_class(FakeRequest())\n    messages = [Message(constants.INFO, 'message %s' % i) for i in range(10)]\n    response = HttpResponse()\n\n    # Test that the max_cookie_size limit is applied.\n    with override_settings(MESSAGES_COOKIE_MAX_SIZE=50):\n        CookieStorage.max_cookie_size = 50\n        unstored_messages = storage._store(messages, response)\n        self.assertGreater(len(unstored_messages), 0)\n\n    # Test that the max_cookie_size limit doesn't affect messages storage when it's None.\n    with override_settings(MESSAGES_COOKIE_MAX_SIZE=None):\n        CookieStorage.max_cookie_size = None\n        unstored_messages = storage._store(messages, response)\n        self.assertEqual(len(unstored_messages), 0)\n"], "sample_203": ["def test_prohibit_null_characters_validator(self):\n    validator = validators.ProhibitNullCharactersValidator()\n    with self.assertRaises(ValidationError):\n        validator('Hello\\x00World')\n    # No ValidationError should be raised for strings without null characters.\n    validator('Hello World')\n", "def test_regex_validator(self):\n    v = validators.RegexValidator(r'^[a-zA-Z0-9]+$')\n    self.assertRaisesMessage(ValidationError, 'Enter a valid value.', v, '!')\n    self.assertIsNone(v('a'))\n    self.assertIsNone(v('123'))\n    self.assertIsNone(v('a123'))\n\n    # Make sure the regex is not matched as a substring\n    v = validators.RegexValidator(r'^[a-zA-Z0-9]+$')\n    self.assertRaisesMessage(ValidationError, 'Enter a valid value.', v, '!!a!!')\n\n    # Make sure the regex can be matched at the end of a string\n    v = validators.RegexValidator(r'[a-zA-Z0-9]+$')\n    self.assertRaisesMessage(ValidationError, 'Enter a valid value.', v, '!')\n    self.assertIsNone(v('a'))\n    self.assertIsNone(v('123'))\n    self.assertIsNone(v('!!a'))\n\n    # Make sure invalid regex pattern raises an exception\n    with self.assertRaises(re.error):\n        validators.RegexValidator(r'[')\n\n    # Test inverse_match.\n    v = validators.RegexValidator(r'^[a-zA-Z0-9]+$', inverse_match=True)\n    self.assertIsNone(v('!'))\n    self.assertRaisesMessage(ValidationError, 'Enter a valid value.', v, 'a')\n", "def test_regex_validator(self):\n    # Test RegexValidator with a string pattern.\n    validator = validators.RegexValidator(r'^[a-zA-Z0-9]+$')\n    self.assertEqual(validator('hello'), None)\n    with self.assertRaisesMessage(ValidationError, \"'hello!' didn't match regex\"):\n        validator('hello!')\n\n    # Test RegexValidator with a compiled pattern.\n    validator = validators.RegexValidator(re.compile(r'^[a-zA-Z0-9]+$'))\n    self.assertEqual(validator('hello'), None)\n    with self.assertRaisesMessage(ValidationError, \"'hello!' didn't match regex\"):\n        validator('hello!')\n\n    # Test RegexValidator with an invalid pattern.\n    with self.assertRaises(re.error):\n        validators.RegexValidator(r'^[a-zA-Z0-9')\n\n    # Test RegexValidator with a string pattern and flags.\n    validator = validators.RegexValidator(r'^[a-z0-9]+$', re.IGNORECASE)\n    self.assertEqual(validator('Hello'), None)\n    with self.assertRaisesMessage(ValidationError, \"'hello!' didn't match regex\"):\n        validator('hello!')\n\n    # Test RegexValidator with a compiled pattern and flags.\n    validator = validators.RegexValidator(re.compile(r'^[a-z0-9]+$'), re.IGNORECASE)\n    self.assertEqual(validator('Hello'), None)\n    with self.assertRaisesMessage(ValidationError, \"'hello!' didn't match regex\"):\n        validator('hello!')\n", "def test_regex_validator(self):\n    # Test RegexValidator with a regex that matches the string 'hello'\n    validator = validators.RegexValidator(re.compile('hello'))\n    validator('hello')  # Should not raise an exception\n\n    # Test RegexValidator with a regex that does not match the string 'hello'\n    validator = validators.RegexValidator(re.compile('goodbye'))\n    with self.assertRaises(ValidationError):\n        validator('hello')\n\n    # Test RegexValidator with a compiled regex and a custom error message\n    validator = validators.RegexValidator(\n        re.compile('hello'),\n        message='This value must contain \"hello\".',\n    )\n    with self.assertRaisesMessage(ValidationError, 'This value must contain \"hello\".'):\n        validator('goodbye')\n", "def test_regex_validator(self):\n    # Create a regex validator that only accepts strings containing 'abc'\n    validator = validators.RegexValidator(re.compile('abc'))\n\n    # Test that 'abc' is valid\n    validator('abc')\n\n    # Test that 'def' is not valid\n    with self.assertRaises(ValidationError):\n        validator('def')\n"], "sample_204": ["    def test_load(self):\n        \"\"\"\n        Makes sure the loader can load the migrations for an app from disk.\n        \"\"\"\n        loader = MigrationLoader(connection)\n        self.assertEqual(loader.get_migration('migrations.test_loader', '0001_initial').name, '0001_initial')\n        self.assertEqual(loader.get_migration('migrations.test_loader', '0002_second').name, '0002_second')\n", "    def test_load(self):\n        \"\"\"\n        Tests that the migration loader can load the initial Django\n        migrations.\n        \"\"\"\n        loader = MigrationLoader(connection)\n        self.assertEqual(loader.load_disk(), None)\n        self.assertGreater(len(loader.disk_migrations), 0)\n", "    def test_load(self):\n        \"\"\"\n        Makes sure the loader can load the migrations for an app from the disk.\n        \"\"\"\n        loader = MigrationLoader(connection)\n        self.addCleanup(loader.clear_cache)\n        migrations = loader.disk_migrations\n        self.assertIn(('migrations', '0001_initial'), migrations)\n        self.assertIsInstance(migrations[('migrations', '0001_initial')], type)\n        self.assertIn(('migrations', '0002_second'), migrations)\n        self.assertIsInstance(migrations[('migrations', '0002_second')], type)\n", "    def test_load(self):\n        \"\"\"\n        Makes sure the loader can load the migrations for an app from the\n        filesystem.\n        \"\"\"\n        loader = MigrationLoader(connection)\n        self.addCleanup(loader.build_graph)\n        loader.load_disk()\n        self.assertEqual(set(loader.disk_migrations.keys()), {\n            ('migrations', '0001_initial'),\n            ('migrations', '0002_second'),\n        })\n", "    def test_load(self):\n        \"\"\"\n        Makes sure the loader can load the test migrations.\n        \"\"\"\n        loader = MigrationLoader(connection)\n        self.assertEqual(loader.disk_migrations, {\n            ('migrations', '0001_initial'): loader.get_migration('migrations', '0001_initial'),\n            ('migrations', '0002_second': loader.get_migration('migrations', '0002_second'),\n        })\n"], "sample_205": ["def test_validation_error_dict(self):\n    error_dict = {'field1': ['error1', 'error2'], 'field2': ['error3']}\n    ve = ValidationError(error_dict)\n\n    self.assertEqual(ve.error_dict, error_dict)\n    self.assertEqual(ve.message_dict, error_dict)\n    self.assertEqual(list(ve), [('field1', ['error1', 'error2']), ('field2', ['error3'])])\n    self.assertEqual(repr(ve), repr(error_dict))\n", "def test_validation_error_with_dict(self):\n    error_dict = {'field1': ['Error 1', 'Error 2'], 'field2': ['Error 3']}\n    ve = ValidationError(error_dict)\n    self.assertEqual(ve.error_dict, error_dict)\n    self.assertEqual(ve.message_dict, error_dict)\n    self.assertEqual(ve.messages, ['Error 1', 'Error 2', 'Error 3'])\n    self.assertEqual(str(ve), repr(error_dict))\n", "def test_validation_error_equality(self):\n    error1 = ValidationError('Error message', code='error_code')\n    error2 = ValidationError('Error message', code='error_code')\n    self.assertEqual(error1, error2)\n\n    error3 = ValidationError('Different message', code='error_code')\n    self.assertNotEqual(error1, error3)\n\n    error4 = ValidationError('Error message', code='different_code')\n    self.assertNotEqual(error1, error4)\n", "def test_validation_error_init_dict(self):\n    message = {'field1': ['error1', 'error2'], 'field2': ['error3']}\n    error = ValidationError(message)\n    self.assertEqual(error.error_dict, {\n        'field1': [ValidationError('error1'), ValidationError('error2')],\n        'field2': [ValidationError('error3')],\n    })\n    self.assertEqual(error.message_dict, message)\n    self.assertEqual(list(error), [('field1', ['error1', 'error2']), ('field2', ['error3'])])\n", "def test_validation_error_dict(self):\n    error_dict = {'field1': ['Error 1', 'Error 2'], 'field2': ['Error 3']}\n    ve = ValidationError(error_dict)\n    self.assertEqual(ve.message_dict, error_dict)\n    self.assertEqual(ve.messages, ['Error 1', 'Error 2', 'Error 3'])\n    self.assertEqual(str(ve), repr(error_dict))\n    self.assertEqual(repr(ve), 'ValidationError(%s)' % repr(error_dict))\n"], "sample_206": ["def test_file_field_pickling(self):\n    # Create a model instance with a file attached\n    document = Document.objects.create(file=File(temp.NamedTemporaryFile()))\n    \n    # Pickle the model instance\n    pickled_document = pickle.dumps(document)\n    \n    # Unpickle the model instance\n    unpickled_document = pickle.loads(pickled_document)\n    \n    # Check that the unpickled instance still has the correct file\n    self.assertEqual(unpickled_document.file.name, document.file.name)\n", "def test_file_field_save(self):\n    # Test saving a file through the FileField\n    document = Document()\n    content = b'Hello, world!'\n    document.file.save('example.txt', ContentFile(content))\n    self.assertEqual(document.file.size, len(content))\n    with open(document.file.path, 'rb') as f:\n        self.assertEqual(f.read(), content)\n    document.file.delete()\n    self.assertFalse(os.path.exists(document.file.path))\n", "def test_filefield_pickling(self):\n    doc = Document(myfile='tests/test1.txt')\n    self.assertIsNone(doc.myfile.file)\n    dump = pickle.dumps(doc)\n    loaded_doc = pickle.loads(dump)\n    self.assertEqual(loaded_doc.myfile.name, 'tests/test1.txt')\n    self.assertIsNone(loaded_doc.myfile.file)\n", "def test_filefield_save(self):\n    doc = Document(myfile='example.txt')\n    content = b'Hello, world!'\n    with tempfile.TemporaryDirectory() as tmpdir:\n        file_path = os.path.join(tmpdir, 'example.txt')\n        with open(file_path, 'wb') as f:\n            f.write(content)\n\n        with open(file_path, 'rb') as f:\n            doc.myfile.save('example.txt', File(f))\n            self.assertEqual(doc.myfile.size, len(content))\n\n        doc.save()\n        self.assertEqual(Document.objects.get(pk=doc.pk).myfile.size, len(content))\n", "def test_fieldfile_pickle(self):\n    # Make sure FieldFile instances can be pickled\n    document = Document(myfile='something.txt')\n    f = document.myfile\n    self.assertIsNone(f.file)\n    dump = pickle.dumps(f)\n    f2 = pickle.loads(dump)\n    self.assertEqual(f.name, f2.name)\n    self.assertIsNone(f2.file)\n\n    # Now try it with an actual file open\n    document.save()\n    f = document.myfile\n    with f.open() as file:\n        dump = pickle.dumps(f)\n    f2 = pickle.loads(dump)\n    self.assertEqual(f.name, f2.name)\n    self.assertIsNone(f2.file)\n"], "sample_207": ["def test_jsonfield_default_value(self):\n    field = models.JSONField(default={'a': 1})\n    self.assertEqual(field.get_default(), {'a': 1})\n\n    field = models.JSONField(default=None)\n    self.assertIsNone(field.get_default())\n\n    field = models.JSONField(default=list)\n    self.assertEqual(field.get_default(), [])\n\n    class CustomJSONEncoder(json.JSONEncoder):\n            if isinstance(obj, uuid.UUID):\n                return str(obj)\n            return super().default(obj)\n\n    field = models.JSONField(encoder=CustomJSONEncoder, default={'uuid': uuid.uuid4()})\n    self.assertIsInstance(field.get_default(), dict)\n", "def test_json_field_key_transforms(self):\n    obj = JSONModel.objects.create(value={'a': {'b': 'c'}})\n    self.assertEqual(\n        JSONModel.objects.filter(value__a__b='c').get(),\n        obj,\n    )\n    self.assertEqual(\n        JSONModel.objects.filter(value__a__contains={'b': 'c'}).get(),\n        obj,\n    )\n    self.assertEqual(\n        JSONModel.objects.filter(value__has_keys=['a']).get(),\n        obj,\n    )\n    self.assertEqual(\n        JSONModel.objects.filter(value__has_any_keys=['a', 'd']).get(),\n        obj,\n    )\n    self.assertEqual(\n        JSONModel.objects.filter(value__keys='a').get(),\n        obj,\n    )\n    self.assertEqual(\n        JSONModel.objects.filter(value__values='c').get(),\n        obj,\n    )\n    self.assertEqual(\n        JSONModel.objects.filter(value__contains={'a': {'b': 'c'}}).get(),\n        obj,\n    )\n    self.assertEqual(\n        JSONModel.objects.filter(value__contained_by={'a': {'b': 'c'}}).get(),\n        obj,\n    )\n    self.assertEqual(\n        JSONModel.objects.filter(value__has_key='a').get(),\n        obj,\n    )\n", "def test_key_transform(self):\n    obj = JSONModel.objects.create(value={'foo': 'bar'})\n    self.assertEqual(\n        JSONModel.objects.get(id=obj.id).value['foo'],\n        KeyTransform('foo', JSONModel.value).as_postgresql(connection, None)[1][1],\n    )\n    self.assertEqual(KeyTransform('foo', JSONModel.value).as_postgresql(connection, None)[0], '(value -> %s)')\n    self.assertEqual(KeyTransformTextLookupMixin.lookup_name, '')\n    self.assertIsInstance(\n        KeyTransformTextLookupMixin(JSONModel.value),\n        Transform,\n    )\n    self.assertIsInstance(\n        KeyTransform(0, JSONModel.value),\n        KeyTransform,\n    )\n    with self.assertRaises(ValueError):\n        KeyTransform(None, JSONModel.value)\n    with self.assertRaises(TypeError):\n        KeyTransformTextLookupMixin(KeyTransform('foo', JSONModel.value), foo='bar')\n", "def test_key_transform_get_prep_lookup(self):\n    key_transform = KeyTransform('test')\n    lookup = HasKeys(key_transform, ['value'])\n    self.assertEqual(lookup.get_prep_lookup(), ['value'])\n", "def test_key_transform_exact_lookup_with_none_value(self):\n    obj = JSONModel.objects.create(json_field={'key': None})\n    self.assertSequenceEqual(JSONModel.objects.filter(json_field__key=None), [obj])\n    self.assertSequenceEqual(JSONModel.objects.filter(json_field__key__exact=None), [obj])\n    self.assertSequenceEqual(JSONModel.objects.filter(json_field__key='null'), [obj])\n    self.assertSequenceEqual(JSONModel.objects.filter(json_field__key__exact='null'), [obj])\n"], "sample_208": ["def test_swappable_foreign_key(self):\n    changes = self.get_changes(\n        [self.author_empty],\n        [self.author_with_user],\n    )\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['AddField'])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name='user')\n    self.assertOperationFieldAttributes(changes, 'testapp', 0, 0, to='auth.User')\n", "def test_alter_check_constraint(self):\n    # Make state\n    before = self.make_project_state([self.author_name])\n    after = self.make_project_state([self.author_name_check_constraint])\n    autodetector = MigrationAutodetector(before, after)\n    changes = autodetector._detect_changes()\n    # Right number of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    # Right operations in the right order?\n    self.assertOperationTypes(changes, 'testapp', 0, ['AddConstraint'])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name='name_contains_bob')\n", "def test_create_altered_constraints(self):\n    # Create initial tables.\n    before = self.make_project_state([self.author_empty, self.book])\n    after = self.make_project_state([\n        self.author_empty.clone(\n            options={\n                \"constraints\": [\n                    models.CheckConstraint(check=models.Q(id__gt=0), name='author_id_check'),\n                ]\n            }\n        ),\n        self.book,\n    ])\n    changes = self.get_changes(before, after)\n    # Right number of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    # Right operations in migration?\n    self.assertOperationTypes(changes, 'testapp', 0, ['AddConstraint'])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name='author_id_check')\n", "def test_swappable_first_key(self):\n    \"\"\"\n    The swappable_first_key method should return a key that places\n    swappable models first in lists of created models.\n    \"\"\"\n    # Define a model state with a swappable model\n    swappable_model = ModelState(\"testapp\", \"CustomUser\", [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"username\", models.CharField(max_length=255)),\n    ], bases=(AbstractBaseUser,))\n    \n    # Define a non-swappable model\n    non_swappable_model = ModelState(\"testapp\", \"Author\", [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"name\", models.CharField(max_length=200)),\n    ])\n    \n    # Create a MigrationAutodetector instance\n    autodetector = MigrationAutodetector(\n        ProjectState(),\n        self.make_project_state([swappable_model, non_swappable_model]),\n    )\n    \n    # Call the swappable_first_key method\n    key1 = autodetector.swappable_first_key((\"testapp\", \"CustomUser\"))\n    key2 = autodetector.swappable_first_key((\"testapp\", \"Author\"))\n    \n    # Assert the order of the keys\n    self.assertLess(key1, key2)\n", "def test_create_model_with_partial_indexes(self):\n    changes = self.get_changes([self.author_empty], [self.book_indexes])\n    # Right number of migrations?\n    self.assertNumberMigrations(changes, 'otherapp', 1)\n    self.assertNumberMigrations(changes, 'testapp', 0)\n    # Right number of operations?\n    self.assertOperationTypes(changes, 'otherapp', 0, ['CreateModel'])\n    self.assertOperationAttributes(changes, 'otherapp', 0, 0, name='book')\n    # Check for index on fields.\n    self.assertEqual(changes['otherapp'][0].operations[0].options['indexes'], [\n        models.Index(fields=['author', 'title'], name='book_title_author_idx'),\n    ])\n"], "sample_209": ["    def test_fields_cache_descriptor(self):\n        m = Model1()\n        self.assertEqual(m._state.fields_cache, {})\n        m._state.fields_cache['test'] = 'value'\n        self.assertEqual(m._state.fields_cache, {'test': 'value'})\n        del m._state.fields_cache\n        self.assertEqual(m._state.fields_cache, {})\n", "def test_refresh_from_db(self):\n    m = Model1(f1='orig', f2='orig')\n    m.save()\n    Model1.objects.filter(pk=m.pk).update(f1='new')\n    self.assertEqual(m.f1, 'orig')\n    m.refresh_from_db()\n    self.assertEqual(m.f1, 'new')\n", "    def test_fields_cache(self):\n        # Ensure ModelState.fields_cache is populated correctly when\n        # the model has deferred fields.\n        obj = Model1.objects.defer('field1').get()\n        self.assertEqual(obj._state.fields_cache, {'field1': Deferred})\n", "    def test_fields_cache(self):\n        m = Model1()\n        self.assertEqual(m._state.fields_cache, {})\n        # Set a value to see if the cache is populated.\n        m.f1 = 'foo'\n        self.assertEqual(m._state.fields_cache, {})\n        # Retrieve a value to see if the cache is populated.\n        m.f1\n        self.assertEqual(m._state.fields_cache, {'f1': 'foo'})\n        # Update the value and check if the cache is updated.\n        m.f1 = 'bar'\n        self.assertEqual(m._state.fields_cache, {'f1': 'bar'})\n", "    def test_model_state_fields_cache(self):\n        m = Model1(f1='a', f2='b')\n        self.assertEqual(m._state.fields_cache, {})\n        m.save()\n        self.assertEqual(m._state.fields_cache, {})\n        m.refresh_from_db()\n        self.assertEqual(m._state.fields_cache, {})\n        m.f1 = 'c'\n        self.assertEqual(m._state.fields_cache, {})\n"], "sample_210": ["def test_options(self):\n    request = self.rf.options('/simple/')\n    response = SimpleView.as_view()(request)\n    self.assertEqual(response.status_code, 200)\n    self.assertEqual(response['Allow'], 'GET, HEAD, OPTIONS')\n    self.assertEqual(response.content, b'')\n", "def test_view_as_view(self):\n    view = SimpleView.as_view()\n    request = self.rf.get('/')\n    response = view(request)\n    self._assert_simple(response)\n    self.assertEqual(view.view_class, SimpleView)\n    self.assertEqual(view.view_initkwargs, {})\n", "def test_view_as_view(self):\n    request = self.rf.get('/path/')\n    view = SimpleView.as_view()\n    response = view(request)\n    self._assert_simple(response)\n", "def test_template_view_get_context_data(self):\n    class TestTemplateView(TemplateView):\n        template_name = 'generic_views/about.html'\n\n    view = TestTemplateView()\n    view.object = 'test_object'\n    request = self.rf.get('/')\n\n    context = view.get_context_data(object='override_object')\n\n    self.assertEqual(context['object'], 'override_object')\n    self.assertEqual(context['view'], view)\n", "def test_template_view_get_context_data(self):\n    class TestTemplateView(TemplateView):\n        template_name = 'generic_views/about.html'\n\n    view = TestTemplateView()\n    request = self.rf.get('/about/')\n    view.setup(request)\n\n    context = view.get_context_data()\n    self.assertEqual(context['view'], view)\n"], "sample_211": ["def test_redirect_view_get(self):\n    class MyRedirectView(RedirectView):\n        url = '/foo'\n\n    request = self.rf.get('/bar')\n    response = MyRedirectView.as_view()(request)\n    self.assertIsInstance(response, HttpResponseRedirect)\n    self.assertEqual(response.status_code, 302)\n    self.assertEqual(response.url, '/foo')\n", "def test_view_as_view(self):\n    view = SimpleView.as_view()\n    self.assertEqual(view.__name__, 'SimpleView')\n    self.assertEqual(view.__doc__, \"A simple view with a docstring.\")\n    response = view(self.rf.get('/'), some_kwarg='some_value')\n    self._assert_simple(response)\n", "def test_view_as_view_method(self):\n    view = View.as_view()\n    self.assertIsInstance(view, type(View.dispatch))\n\n    # Test that as_view() does not accept arguments that are already attributes\n    # of the class.\n    with self.assertRaises(TypeError):\n        View.as_view(get='foo')\n\n    # Test that as_view() does not accept arguments that are not already\n    # attributes of the class.\n    with self.assertRaises(TypeError):\n        View.as_view(foo='bar')\n", "def test_view_as_view(self):\n    view = View.as_view()\n    self.assertTrue(callable(view))\n    self.assertIsInstance(view.view_class, type)\n    self.assertEqual(view.view_class, View)\n    self.assertEqual(view.view_initkwargs, {})\n\n    # Test that as_view() can handle keyword arguments.\n    view_with_kwargs = View.as_view(extra_arg='value')\n    self.assertTrue(callable(view_with_kwargs))\n    self.assertIsInstance(view_with_kwargs.view_class, type)\n    self.assertEqual(view_with_kwargs.view_class, View)\n    self.assertEqual(view_with_kwargs.view_initkwargs, {'extra_arg': 'value'})\n", "def test_class_based_view_as_view_passes_kwargs_to_init(self):\n    class TestView(View):\n            self.kwargs = kwargs\n\n            return HttpResponse('Test view')\n\n    view = TestView.as_view(test_kwarg='test_value')\n    response = view(self.rf.get('/'), test_kwarg='overridden_value')\n    self.assertEqual(response.status_code, 200)\n    self.assertEqual(view.view_initkwargs, {'test_kwarg': 'test_value'})\n"], "sample_213": ["    def test_file_field_upload(self):\n        # Create a model with a FileField\n        from .models import FileModel\n        file_model = FileModel()\n\n        # Upload a file to the FileField\n        file = ContentFile('Hello, world!')\n        file_model.file.save('example.txt', file)\n\n        # Check that the file is saved correctly\n        self.assertEqual(file_model.file.name, 'example.txt')\n        self.assertEqual(file_model.file.size, 13)\n\n        # Check that the file can be opened and read correctly\n        with file_model.file.open() as f:\n            self.assertEqual(f.read(), b'Hello, world!')\n\n        # Delete the file\n        file_model.file.delete()\n\n        # Check that the file is deleted correctly\n        self.assertFalse(file_model.file)\n        self.assertEqual(file_model.file.name, '')\n", "    def test_close_file(self):\n        # Make sure that files are properly closed after use\n        obj = Storage.objects.create(file='test.txt')\n        file_obj = obj.file\n        file_obj.open()\n        self.assertFalse(file_obj.closed)\n        file_obj.close()\n        self.assertTrue(file_obj.closed)\n", "    def test_file_field_get_size(self):\n        storage = FileSystemStorage(location=tempfile.mkdtemp())\n        field = FileField(storage=storage)\n        instance = field.model_instance = type('ModelInstance', (), {})\n        instance.field = field\n\n        # Create a file with some data\n        file_name = 'test.txt'\n        file_data = b'Hello, world!'\n        with open(os.path.join(storage.location, file_name), 'wb') as f:\n            f.write(file_data)\n\n        field_file = FieldFile(instance, field, file_name)\n        self.assertEqual(field_file.size, len(file_data))\n\n        # Test that file size is cached when file is closed\n        field_file.close()\n        self.assertEqual(field_file.size, len(file_data))\n", "    def test_field_file(self):\n        instance = Storage()\n        field = FileField(upload_to='tests')\n        file = FieldFile(instance, field, 'tests/example.txt')\n\n        self.assertEqual(file.instance, instance)\n        self.assertEqual(file.field, field)\n        self.assertEqual(file.name, 'tests/example.txt')\n        self.assertEqual(file.storage, default_storage)\n\n        # Make sure file can be opened and read.\n        with file.open() as f:\n            self.assertEqual(f.read(), b'example content')\n\n        # Make sure file can be deleted.\n        file.delete()\n        self.assertFalse(default_storage.exists('tests/example.txt'))\n\n        # Save the file again and make sure it's stored correctly.\n        file.save('example.txt', ContentFile(b'example content'))\n        self.assertTrue(default_storage.exists('tests/example.txt'))\n", "    def test_unicode(self):\n        storage = FileSystemStorage()\n        f = FileField(name='test', upload_to='test', storage=storage)\n        instance = object()\n        ff = f.attr_class(instance, f, 'test.txt')\n        self.assertEqual(str(ff), 'test.txt')\n"], "sample_212": ["    def test_session_middleware_process_request(self):\n        middleware = SessionMiddleware()\n        request = HttpRequest()\n        request.COOKIES[settings.SESSION_COOKIE_NAME] = 'session_key'\n        middleware.process_request(request)\n        self.assertIsInstance(request.session, middleware.SessionStore)\n", "    def test_session_middleware_process_request(self):\n        # Create a request object\n        request = HttpRequest()\n\n        # Set up the session middleware\n        middleware = SessionMiddleware(get_response=lambda req: HttpResponse())\n\n        # Call process_request to set up the session\n        middleware.process_request(request)\n\n        # Assert that the session attribute is set on the request\n        self.assertIsNotNone(request.session)\n", "    def test_process_request(self):\n        request = HttpRequest()\n        middleware = SessionMiddleware()\n        middleware.process_request(request)\n        self.assertIsInstance(request.session, middleware.SessionStore)\n", "    def test_session_middleware_init(self):\n        middleware = SessionMiddleware(get_response=None)\n        self.assertIsInstance(middleware.SessionStore, type(import_module(settings.SESSION_ENGINE).SessionStore))\n", "    def test_process_request(self):\n        request = HttpRequest()\n        middleware = SessionMiddleware()\n        middleware.process_request(request)\n        self.assertIsNotNone(request.session)\n"], "sample_214": ["def test_jsonfield_default_encoder_decoder(self):\n    # Test that JSONField properly uses the default encoder and decoder.\n    model = JSONModel.objects.create(json={\n        'uuid': uuid.uuid4(),\n        'date': '2022-01-01',\n    })\n    self.assertIsInstance(model.json, dict)\n    self.assertEqual(len(model.json), 2)\n    self.assertIsInstance(model.json['uuid'], str)\n    self.assertIsInstance(model.json['date'], str)\n\n    # Test that saving and loading a JSONField with a custom encoder works.\n    class CustomEncoder(DjangoJSONEncoder):\n            if isinstance(obj, uuid.UUID):\n                return obj.hex\n            return super().default(obj)\n\n    model = JSONModel.objects.create(json={\n        'uuid': uuid.uuid4(),\n    }, encoder=CustomEncoder())\n    model.refresh_from_db()\n    self.assertIsInstance(model.json, dict)\n    self.assertEqual(len(model.json), 1)\n    self.assertIsInstance(model.json['uuid'], str)\n\n    # Test that loading a JSONField with a custom decoder works.\n    class CustomDecoder(json.JSONDecoder):\n            kwargs['object_hook'] = self.dict_to_object\n            super().__init__(*args, **kwargs)\n\n            for key, value in dictionary.items():\n                if key == 'uuid':\n                    dictionary[key] = uuid.UUID(value)\n            return dictionary\n\n    model.refresh_from_db(decoder=CustomDecoder())\n    self.assertIsInstance(model.json, dict)\n    self.assertEqual(len(model.json), 1)\n    self.assertIsInstance(model.json['uuid'], uuid.UUID)\n", "def test_key_transform_json_path_compilation(self):\n    key_transforms = [\n        KeyTransform('key1', 'value'),\n        KeyTransform('key2', 'value'),\n        KeyTransform(3, 'value'),\n    ]\n    path = compile_json_path([kt.key_name for kt in key_transforms])\n    self.assertEqual(path, '$.key1.key2[3]')\n", "def test_key_transform_exact_lookup(self):\n    obj = JSONModel.objects.create(value={'key': 'value'})\n    self.assertEqual(JSONModel.objects.filter(value__key='value').count(), 1)\n    self.assertEqual(JSONModel.objects.filter(value__key='other_value').count(), 0)\n\n    # Test with a nested key\n    obj = JSONModel.objects.create(value={'key': {'nested_key': 'value'}})\n    self.assertEqual(JSONModel.objects.filter(value__key__nested_key='value').count(), 1)\n    self.assertEqual(JSONModel.objects.filter(value__key__nested_key='other_value').count(), 0)\n\n    # Test with a null value\n    obj = JSONModel.objects.create(value={'key': None})\n    self.assertEqual(JSONModel.objects.filter(value__key=None).count(), 1)\n    self.assertEqual(JSONModel.objects.filter(value__key='value').count(), 0)\n", "def test_key_transform_factory(self):\n    transform = KeyTransformFactory('key')\n    self.assertIsInstance(transform('value'), KeyTransform)\n    self.assertEqual(transform('value').key_name, 'key')\n", "def test_key_transform_factory(self):\n    factory = KeyTransformFactory('test_key')\n    key_transform = factory(JSONModel.json)\n    self.assertIsInstance(key_transform, KeyTransform)\n    self.assertEqual(key_transform.key_name, 'test_key')\n    self.assertEqual(key_transform.lhs.output_field, JSONModel.json.output_field)\n"], "sample_215": ["    def test_cleansing(self):\n        reporter_filter = SafeExceptionReporterFilter()\n        self.assertEqual(reporter_filter.cleansed_substitute, '********************')\n\n        cleansed = reporter_filter.cleanse_setting('key', 'value')\n        self.assertEqual(cleansed, 'value')\n\n        cleansed = reporter_filter.cleanse_setting('SECRET_KEY', 'my_secret_key')\n        self.assertEqual(cleansed, reporter_filter.cleansed_substitute)\n", "    def test_get_safe_settings(self):\n        filter = SafeExceptionReporterFilter()\n        settings = filter.get_safe_settings()\n        self.assertIsInstance(settings, dict)\n        for key in settings:\n            self.assertIsInstance(key, str)\n", "    def test_get_traceback_data(self):\n        request = RequestFactory().get('/')\n        exc_type = Exception\n        exc_value = Exception('Test exception')\n        tb = sys.exc_info()[2]\n\n        reporter = ExceptionReporter(request, exc_type, exc_value, tb)\n        data = reporter.get_traceback_data()\n\n        self.assertIn('frames', data)\n        self.assertIn('request', data)\n        self.assertIn('request_meta', data)\n        self.assertIn('filtered_POST_items', data)\n        self.assertIn('settings', data)\n        self.assertIn('sys_executable', data)\n        self.assertIn('sys_version_info', data)\n        self.assertIn('server_time', data)\n        self.assertIn('django_version_info', data)\n        self.assertIn('sys_path', data)\n        self.assertIn('template_info', data)\n        self.assertIn('postmortem', data)\n", "    def test_get_safe_settings(self):\n        filter = SafeExceptionReporterFilter()\n        settings = filter.get_safe_settings()\n        self.assertIsInstance(settings, dict)\n        for key in settings:\n            self.assertNotIn('SECRET', key)\n            self.assertNotIn('PASSWORD', key)\n", "    def test_get_safe_settings(self):\n        filter = SafeExceptionReporterFilter()\n        settings_dict = filter.get_safe_settings()\n        self.assertIn('DATABASES', settings_dict)\n        self.assertNotIn('DATABASES_PASSWORD', settings_dict)\n        self.assertNotIn('SECRET_KEY', settings_dict)\n"], "sample_216": ["def test_field_references(self):\n    field = models.ForeignKey(\"testapp.Author\", models.CASCADE)\n    model_tuple = (\"testapp\", \"Author\")\n    reference_model_tuple = (\"testapp\", \"Book\")\n\n    # Test no reference\n    self.assertFalse(field_references(model_tuple, field, reference_model_tuple))\n\n    # Test direct reference\n    field = models.ForeignKey(\"testapp.Book\", models.CASCADE)\n    self.assertTrue(field_references(model_tuple, field, reference_model_tuple))\n\n    # Test indirect reference through another model\n    field = models.ForeignKey(\"otherapp.Publisher\", models.CASCADE)\n    reference_model_tuple = (\"otherapp\", \"Publisher\")\n    self.assertTrue(field_references(model_tuple, field, reference_model_tuple))\n\n    # Test with a field name\n    field = models.ForeignKey(\"testapp.Book\", models.CASCADE)\n    reference_field_name = \"author\"\n    self.assertTrue(field_references(model_tuple, field, reference_model_tuple, reference_field_name))\n", "def test_field_is_referenced(self):\n    state = self.make_project_state([\n        self.author_with_m2m,\n        self.publisher,\n        self.contract,\n    ])\n    self.assertTrue(field_is_referenced(state, (\"testapp\", \"author\"), (\"id\",)))\n    self.assertFalse(field_is_referenced(state, (\"testapp\", \"publisher\"), (\"id\",)))\n", "def test_field_references_with_recursive_relationship(self):\n    model_tuple = (\"testapp\", \"Author\")\n    field = models.ForeignKey(\"self\", on_delete=models.CASCADE)\n    reference_model_tuple = (\"testapp\", \"Author\")\n\n    reference = field_references(model_tuple, field, reference_model_tuple)\n\n    self.assertIsNotFalse(reference)\n    self.assertEqual(reference.to, (field.remote_field, None))\n    self.assertIsNone(reference.through)\n", "def test_field_references(self):\n    field = models.ForeignKey(\"testapp.Author\", models.CASCADE)\n    reference_model_tuple = (\"testapp\", \"author\")\n    model_tuple = (\"otherapp\", \"book\")\n\n    # Test references to a specific field\n    reference_field_name = \"id\"\n    self.assertTrue(field_references(model_tuple, field, reference_model_tuple, reference_field_name))\n\n    # Test references to a non-existent field\n    reference_field_name = \"non_existent_field\"\n    self.assertFalse(field_references(model_tuple, field, reference_model_tuple, reference_field_name))\n\n    # Test references to any field\n    self.assertTrue(field_references(model_tuple, field, reference_model_tuple))\n\n    # Test with a different model tuple\n    model_tuple = (\"thirdapp\", \"edition\")\n    self.assertFalse(field_references(model_tuple, field, reference_model_tuple))\n", "def test_field_references_with_recursive_relationship(self):\n    model_tuple = (\"testapp\", \"Author\")\n    field = models.ForeignKey(\"self\", on_delete=models.CASCADE)\n    reference_model_tuple = (\"testapp\", \"Author\")\n    reference_field_name = None\n\n    result = field_references(model_tuple, field, reference_model_tuple, reference_field_name)\n\n    self.assertIsNotNone(result)\n    self.assertIsInstance(result, FieldReference)\n    self.assertEqual(result.to, (field.remote_field, None))\n    self.assertIsNone(result.through)\n"], "sample_217": ["def test_media_add(self):\n    widget1 = TextInput()\n    widget2 = TextInput()\n    media = Media(css={'all': ['test.css']}, js=['test.js'])\n    widget1.media += media\n    widget2.media += media\n    self.assertEqual(str(widget1.media + widget2.media), 'http://media.example.com/static/test.css\\nhttp://media.example.com/static/test.js')\n", "def test_media_addition(self):\n    class TestForm(Form):\n        field1 = CharField(widget=TextInput(media=Media(css={'all': ['test1.css']})))\n        field2 = CharField(widget=TextInput(media=Media(css={'all': ['test2.css']})))\n\n    form = TestForm()\n    self.assertHTMLEqual(\n        str(form.media),\n        '<link href=\"http://media.example.com/static/test1.css\" type=\"text/css\" media=\"all\" rel=\"stylesheet\">'\n        '<link href=\"http://media.example.com/static/test2.css\" type=\"text/css\" media=\"all\" rel=\"stylesheet\">'\n    )\n", "def test_media_add(self):\n    class TestForm(Form):\n        field1 = CharField(widget=TextInput(media=Media(js=['form.js'])))\n        field2 = CharField(widget=TextInput(media=Media(js=['field.js'])))\n\n    form = TestForm()\n    self.assertEqual(form.media.render(), (\n        '<script src=\"http://media.example.com/static/form.js\"></script>\\n'\n        '<script src=\"http://media.example.com/static/field.js\"></script>'\n    ))\n", "def test_media_add(self):\n    \"\"\"Adding two Media objects together yields a new Media object.\"\"\"\n    media1 = Media(css={'all': ['path/to/1.css']}, js=['path/to/1.js'])\n    media2 = Media(css={'all': ['path/to/2.css']}, js=['path/to/2.js'])\n    combined = media1 + media2\n\n    self.assertEqual(combined._css, {'all': ['path/to/1.css', 'path/to/2.css']})\n    self.assertEqual(combined._js, ['path/to/1.js', 'path/to/2.js'])\n", "def test_media_add(self):\n    \"\"\"Media objects can be added together\"\"\"\n    media1 = Media(css={'all': ['path/to/css']}, js=['path/to/js'])\n    media2 = Media(css={'print': ['path/to/print/css']}, js=['path/to/other/js'])\n    combined_media = media1 + media2\n\n    self.assertEqual(combined_media._css, {'all': ['path/to/css'], 'print': ['path/to/print/css']})\n    self.assertEqual(combined_media._js, ['path/to/js', 'path/to/other/js'])\n"], "sample_218": ["def test_trunc_functions_with_timezone(self):\n    start_datetime = datetime(2015, 6, 15, 14, 30, tzinfo=pytz.UTC)\n    model = self.create_model(start_datetime, None)\n\n    # Test Trunc functions with a specific timezone.\n    melbourne = pytz.timezone('Australia/Melbourne')\n    tests = [\n        (TruncYear, 'year', start_datetime),\n        (TruncMonth, 'month', start_datetime),\n        (TruncDay, 'day', start_datetime),\n        (TruncHour, 'hour', start_datetime),\n        (TruncMinute, 'minute', start_datetime),\n        (TruncSecond, 'second', start_datetime),\n    ]\n    for func, kind, value in tests:\n        with self.subTest(func=func):\n            truncated_value = truncate_to(value, kind, tzinfo=melbourne)\n            qs = DTModel.objects.annotate(\n                result=func('start_datetime', tzinfo=melbourne),\n            )\n            self.assertEqual(qs.get(pk=model.pk).result, truncated_value)\n", "def test_trunc_functions(self):\n    start_datetime = datetime(2015, 6, 15, 14, 30, 50, 321)\n    end_datetime = datetime(2018, 3, 1, 12, 0, 0)\n    model = self.create_model(start_datetime, end_datetime)\n\n    # Test Trunc functions\n    self.assertEqual(\n        DTModel.objects.filter(start_datetime=TruncYear(model.start_datetime)).get(),\n        model,\n    )\n    self.assertEqual(\n        DTModel.objects.filter(start_datetime=TruncMonth(model.start_datetime)).get(),\n        model,\n    )\n    self.assertEqual(\n        DTModel.objects.filter(start_datetime=TruncDay(model.start_datetime)).get(),\n        model,\n    )\n    self.assertEqual(\n        DTModel.objects.filter(start_datetime=TruncHour(model.start_datetime)).get(),\n        model,\n    )\n    self.assertEqual(\n        DTModel.objects.filter(start_datetime=TruncMinute(model.start_datetime)).get(),\n        model,\n    )\n    self.assertEqual(\n        DTModel.objects.filter(start_datetime=TruncSecond(model.start_datetime)).get(),\n        model,\n    )\n\n    # Also test with a different timezone\n    tzinfo = pytz.timezone('Europe/Berlin')\n    start_datetime_tz = start_datetime.replace(tzinfo=tzinfo)\n    model_tz = self.create_model(start_datetime_tz, end_datetime)\n\n    self.assertEqual(\n        DTModel.objects.filter(start_datetime=TruncYear(model_tz.start_datetime)).get(),\n        model_tz,\n    )\n    self.assertEqual(\n        DTModel.objects.filter(start_datetime=TruncMonth(model_tz.start_datetime)).get(),\n        model_tz,\n    )\n    self.assertEqual(\n        DTModel.objects.filter(start_datetime=TruncDay(model_tz.start_datetime)).get(),\n        model_tz,\n    )\n    self.assertEqual(\n        DTModel.objects.filter(start_datetime=TruncHour(model_tz.start_datetime)).get(),\n        model_tz,\n    )\n    self.assertEqual(\n        DTModel.objects.filter(start_datetime=TruncMinute(model_tz.start_datetime)).get(),\n        model_tz,\n    )\n    self.assertEqual(\n        DTModel.objects.filter(start_datetime=TruncSecond(model_tz.start_datetime)).get(),\n        model_tz,\n    )\n", "    def test_trunc_function_with_timezone(self):\n        start_datetime = datetime(2016, 1, 1, 12, 30, tzinfo=pytz.timezone('US/Pacific'))\n        model = self.create_model(start_datetime, None)\n        qs = DTModel.objects.filter(start_datetime=start_datetime)\n        with self.assertNumQueries(1):\n            result = list(qs.annotate(truncated_start_datetime=Trunc('start_datetime', 'hour')))\n        expected = truncate_to(start_datetime, 'hour', tzinfo=pytz.timezone(settings.TIME_ZONE))\n        self.assertEqual(result[0].truncated_start_datetime, expected)\n", "    def test_trunc_subquery(self):\n        start_datetime = datetime(2015, 6, 15, 14, 30, tzinfo=pytz.utc)\n        model = self.create_model(start_datetime, None)\n        qs = DTModel.objects.annotate(\n            start_datetime_truncated=Trunc(\n                'start_datetime',\n                'hour',\n                output_field=DateTimeField(),\n            ),\n        ).filter(start_datetime_truncated=Subquery(\n            DTModel.objects.filter(pk=model.pk).values('start_datetime')[:1]\n        ))\n        self.assertEqual(qs.count(), 1)\n", "def test_trunc_subquery(self):\n    start_datetime = datetime(2015, 6, 15, 14, 30, 50, 321)\n    self.create_model(start_datetime, None)\n\n    dt = DTModel.objects.annotate(\n        trunc_start=Trunc('start_datetime', 'hour'),\n        trunc_end=Trunc('end_datetime', 'hour'),\n    ).filter(trunc_start=start_datetime.replace(minute=0, second=0, microsecond=0))\n\n    subquery = dt.values('trunc_start')\n    self.assertQuerysetEqual(subquery, [start_datetime.replace(minute=0, second=0, microsecond=0)])\n\n    # Ensure that the subquery is evaluated correctly.\n    subquery = Subquery(dt.values('trunc_start')[:1])\n    dt = DTModel.objects.filter(start_datetime__hour=Subquery(subquery.values('hour')[:1]))\n    self.assertEqual(dt.get().start_datetime, start_datetime)\n"], "sample_219": ["def test_expression_pickle(self):\n    expr = F('num_employees') + Value(5)\n    pickled_expr = pickle.loads(pickle.dumps(expr))\n    self.assertEqual(expr, pickled_expr)\n\n    query = Company.objects.annotate(num_chairs_plus_5=expr)\n    with CaptureQueriesContext(connection) as captured_queries:\n        list(query)\n\n    query_pickled_expr = query._clone()\n    query_pickled_expr.query.annotations['num_chairs_plus_5'] = pickled_expr\n    with CaptureQueriesContext(connection) as captured_pickled_queries:\n        list(query_pickled_expr)\n\n    self.assertEqual(captured_queries.captured_queries[0]['sql'], captured_pickled_queries.captured_queries[0]['sql'])\n", "def test_combinable_expression(self):\n    combined_expr = F('num_employees') + F('num_chairs')\n    query = Company.objects.annotate(result=combined_expr).values('result')\n    result = list(query)\n    self.assertEqual(result, [{'result': 2305}, {'result': 7}, {'result': 33}])\n", "def test_expression_pickling(self):\n    company = Company.objects.create(name='Pickle Inc.')\n    # Test that expressions can be pickled and unpickled.\n    for expr in [\n        F('name'),\n        Q(name='Pickle Inc.'),\n        Case(When(name='Pickle Inc.', then=True), default=False),\n        Exists(Employee.objects.filter(company=company)),\n        OuterRef('name'),\n        Value(42),\n        RawSQL(\"SELECT 42\", ()),\n        Subquery(Employee.objects.values_list('pk', flat=True)),\n        Col('tests', 'Company', 'name'),\n        Random(),\n        Ref('name', StringAgg('name', ';')),\n    ]:\n        with self.subTest(expr=expr):\n            self.assertEqual(pickle.loads(pickle.dumps(expr)), expr)\n", "def test_expression_wrapper(self):\n    # Ensure that ExpressionWrapper can be used to provide an output_field\n    # for an expression.\n    expr = ExpressionWrapper(F('num_employees'), output_field=IntegerField())\n    self.assertIsInstance(expr.output_field, IntegerField)\n    self.assertEqual(expr.output_field.model_field, None)\n\n    # Ensure that ExpressionWrapper can be used with an aggregate expression.\n    expr = ExpressionWrapper(Sum('num_employees'), output_field=IntegerField())\n    self.assertIsInstance(expr.output_field, IntegerField)\n    self.assertEqual(expr.output_field.model_field, None)\n", "def test_expression_list(self):\n    # Test ExpressionList with a single expression\n    expression = ExpressionList(F('num_employees'))\n    self.assertEqual(expression.as_sql(None, connection), ('num_employees', []))\n\n    # Test ExpressionList with multiple expressions\n    expression = ExpressionList(F('num_employees'), F('num_chairs'))\n    self.assertEqual(\n        expression.as_sql(None, connection),\n        ('num_employees, num_chairs', [])\n    )\n"], "sample_220": ["def test_set_cookie_samesite(self):\n    response = HttpResponse()\n    response.set_cookie('samesite_test', 'test', samesite='lax')\n    self.assertIn('Set-Cookie: samesite_test=test; Samesite=Lax', str(response.cookies))\n    with self.assertRaises(ValueError):\n        response.set_cookie('samesite_test', 'test', samesite='invalid')\n", "def test_set_cookie_expires(self):\n    response = HttpResponse()\n    expires = datetime(2038, 1, 1, tzinfo=utc)\n    response.set_cookie('test', 'value', expires=expires)\n    self.assertEqual(response.cookies['test']['expires'], http_date(expires.timestamp()))\n", "def test_set_cookie_max_age(self):\n    response = HttpResponse()\n    max_age = 3600  # 1 hour\n    expires = datetime.utcnow() + timedelta(seconds=max_age)\n    response.set_cookie('test', 'value', max_age=max_age)\n    self.assertEqual(response.cookies['test']['max-age'], str(max_age))\n    self.assertEqual(response.cookies['test']['expires'], http_date(expires.timestamp()))\n", "def test_set_cookie_with_expires_as_string(self):\n    response = HttpResponse()\n    expires = http_date(time.time() + 60)\n    response.set_cookie('key', 'value', expires=expires)\n    self.assertEqual(response.cookies['key']['expires'], expires)\n", "def test_set_cookie_max_age(self):\n    response = HttpResponse()\n    max_age = 3600\n    response.set_cookie('test', 'value', max_age=max_age)\n    self.assertEqual(response.cookies['test']['max-age'], max_age)\n"], "sample_221": ["def test_queryset_pickle_with_deferred_fields(self):\n    qs = Happening.objects.defer('name')\n    self.assert_pickles(qs)\n", "def test_pickle_with_deferred_fields(self):\n    qs = Happening.objects.defer('name')\n    self.assert_pickles(qs)\n", "def test_queryset_pickle_with_filter(self):\n    self.assert_pickles(Happening.objects.filter(id=self.happening.id))\n", "def test_queryset_pickling_with_expressions(self):\n    qs = Event.objects.annotate(duration=models.F('end_date') - models.F('start_date'))\n    self.assert_pickles(qs)\n", "def test_queryset_pickling_with_options(self):\n    qs = MyEvent.objects.all()\n    self.assert_pickles(qs)\n\n    qs = MyEvent.objects.filter(name='foo')\n    self.assert_pickles(qs)\n\n    qs = MyEvent.objects.exclude(name='foo')\n    self.assert_pickles(qs)\n\n    qs = MyEvent.objects.order_by('id')\n    self.assert_pickles(qs)\n\n    qs = MyEvent.objects.select_related('happening')\n    self.assert_pickles(qs)\n\n    qs = MyEvent.objects.prefetch_related('groups')\n    self.assert_pickles(qs)\n"], "sample_222": ["def test_file_locking(self):\n    with tempfile.TemporaryFile() as f:\n        self.assertTrue(locks.lock(f, locks.LOCK_EX))\n        self.assertFalse(locks.lock(f, locks.LOCK_EX))  # File is already locked\n        locks.unlock(f)\n        self.assertTrue(locks.lock(f, locks.LOCK_EX))  # File can be locked again\n\n    with tempfile.TemporaryFile() as f:\n        self.assertTrue(locks.lock(f, locks.LOCK_SH))\n        self.assertTrue(locks.lock(f, locks.LOCK_SH))  # Shared lock can be acquired multiple times\n        locks.unlock(f)\n\n    with tempfile.TemporaryFile() as f:\n        self.assertTrue(locks.lock(f, locks.LOCK_EX | locks.LOCK_NB))\n        self.assertFalse(locks.lock(f, locks.LOCK_EX | locks.LOCK_NB))  # Non-blocking lock fails if file is locked\n        locks.unlock(f)\n", "def test_locks(self):\n    with tempfile.TemporaryFile() as f:\n        # Test exclusive lock\n        self.assertTrue(locks.lock(f, locks.LOCK_EX))\n        # Test shared lock\n        self.assertTrue(locks.lock(f, locks.LOCK_SH))\n        # Test non-blocking lock\n        self.assertTrue(locks.lock(f, locks.LOCK_NB))\n        # Test unlock\n        self.assertTrue(locks.unlock(f))\n        # Test dummy lock and unlock on unsupported platforms\n        with mock.patch('django.core.files.locks.LOCK_EX', 0):\n            self.assertFalse(locks.lock(f, locks.LOCK_EX))\n            self.assertTrue(locks.unlock(f))\n", "def test_file_locking(self):\n    with tempfile.TemporaryFile() as f:\n        # Test exclusive lock\n        self.assertTrue(locks.lock(f, locks.LOCK_EX))\n        # Test non-blocking lock\n        self.assertFalse(locks.lock(f, locks.LOCK_EX | locks.LOCK_NB))\n        locks.unlock(f)\n        # Test shared lock\n        self.assertTrue(locks.lock(f, locks.LOCK_SH))\n        # Test non-blocking shared lock\n        self.assertTrue(locks.lock(f, locks.LOCK_SH | locks.LOCK_NB))\n        locks.unlock(f)\n", "def test_locks(self):\n    with tempfile.TemporaryFile() as f:\n        # Test exclusive lock\n        self.assertTrue(locks.lock(f, locks.LOCK_EX))\n        self.assertFalse(locks.lock(f, locks.LOCK_EX))  # Lock is already held\n\n        # Test shared lock\n        locks.unlock(f)\n        self.assertTrue(locks.lock(f, locks.LOCK_SH))\n        self.assertTrue(locks.lock(f, locks.LOCK_SH))  # Shared lock can be acquired multiple times\n\n        # Test non-blocking lock\n        locks.unlock(f)\n        self.assertTrue(locks.lock(f, locks.LOCK_EX | locks.LOCK_NB))\n        self.assertFalse(locks.lock(f, locks.LOCK_EX | locks.LOCK_NB))  # Lock is already held\n\n        # Test unlock\n        locks.unlock(f)\n        self.assertTrue(locks.lock(f, locks.LOCK_EX))  # Lock can be re-acquired after unlocking\n", "def test_locks(self):\n    with tempfile.TemporaryFile() as f:\n        # Test exclusive lock\n        self.assertTrue(locks.lock(f, locks.LOCK_EX))\n        self.assertFalse(locks.lock(f, locks.LOCK_EX))  # Lock is already held\n\n        # Test shared lock\n        locks.unlock(f)\n        self.assertTrue(locks.lock(f, locks.LOCK_SH))\n        self.assertTrue(locks.lock(f, locks.LOCK_SH))  # Shared lock can be acquired multiple times\n\n        # Test non-blocking lock\n        locks.unlock(f)\n        self.assertTrue(locks.lock(f, locks.LOCK_EX | locks.LOCK_NB))\n        self.assertFalse(locks.lock(f, locks.LOCK_EX | locks.LOCK_NB))  # Lock is already held\n\n        # Test unlock\n        locks.unlock(f)\n        self.assertTrue(locks.lock(f, locks.LOCK_EX))  # Lock can be reacquired after unlock\n"], "sample_223": ["def test_query_empty_result_set(self):\n    \"\"\"\n    Test that a query with an empty result set returns correctly.\n    \"\"\"\n    # Create an empty QuerySet\n    qs = Tag.objects.filter(name='does-not-exist')\n    self.assertIsInstance(qs, EmptyQuerySet)\n    self.assertEqual(list(qs), [])\n    self.assertFalse(qs.exists())\n    self.assertEqual(qs.count(), 0)\n", "def test_ticket23605(self):\n    Ticket23605A.objects.create()\n    qs = Ticket23605A.objects.annotate(\n        foo=Exists(Ticket23605B.objects.filter(a_id=OuterRef('id'))),\n    ).filter(foo=True)\n    with self.assertNumQueries(1):\n        list(qs)\n\n    qs = Ticket23605A.objects.annotate(\n        foo=Exists(Ticket23605C.objects.filter(a_id=OuterRef('id'))),\n    ).filter(foo=True)\n    with self.assertNumQueries(1):\n        list(qs)\n", "def test_query_annotate(self):\n    # Create a QuerySet with an annotated value.\n    qs = Item.objects.annotate(creator_name=F('creator__name'))\n    self.assertEqual(qs.get(id=self.i1.id).creator_name, 'a1')\n    self.assertEqual(qs.get(id=self.i2.id).creator_name, 'a2')\n\n    # The annotation is included in values() results.\n    values_qs = qs.values('id', 'creator_name')\n    self.assertEqual(values_qs.get(id=self.i1.id)['creator_name'], 'a1')\n    self.assertEqual(values_qs.get(id=self.i2.id)['creator_name'], 'a2')\n\n    # Annotations can reference other annotations.\n    qs = Item.objects.annotate(\n        creator_name=F('creator__name'),\n        creator_name_upper=Upper(F('creator_name')),\n    )\n    self.assertEqual(qs.get(id=self.i1.id).creator_name_upper, 'A1')\n    self.assertEqual(qs.get(id=self.i2.id).creator_name_upper, 'A2')\n", "def test_prefetch_related_m2m_with_intermediate(self):\n    rel1 = Job.objects.create(name='rel1')\n    rel2 = Job.objects.create(name='rel2')\n    employee = Employee.objects.create(name='emp')\n    employee.jobs.add(rel1, rel2)\n\n    qs = Employee.objects.prefetch_related('jobs').order_by('-name')\n    with self.assertNumQueries(2):\n        employees = list(qs)\n    for emp in employees:\n        with self.assertNumQueries(0):\n            self.assertEqual(len(emp.jobs.all()), 2)\n", "def test_update_with_select_related(self):\n    # Test that update() with select_related() updates the correct model instance.\n    related_obj = RelatedObject.objects.create(name='related')\n    obj = ModelA.objects.create(a='a', related=related_obj)\n    ModelA.objects.select_related('related').filter(pk=obj.pk).update(a='b')\n    self.assertEqual(ModelA.objects.get(pk=obj.pk).a, 'b')\n"], "sample_224": ["def test_exists_subquery(self):\n    subquery = Book.objects.filter(rating__gt=F('publisher__num_awards'))\n    publishers = Publisher.objects.annotate(has_book_with_higher_rating=Exists(subquery)).filter(has_book_with_higher_rating=True)\n    self.assertEqual(publishers.count(), 2)\n", "def test_distinct_on(self):\n    # Test that distinct() can be used with order_by() to get the distinct values of one field.\n    books = Book.objects.order_by('publisher').distinct('publisher')\n    self.assertEqual(len(books), 4)\n    self.assertEqual(set(b.publisher for b in books), set(Publisher.objects.all()))\n", "def test_values_list_with_expression(self):\n    # Test that an expression is correctly handled in values_list()\n    books = Book.objects.values_list('publisher__name', flat=True).distinct()\n    publishers = Publisher.objects.values_list('name', flat=True)\n    self.assertEqual(list(books), list(publishers))\n", "def test_values_list_with_in_bulk(self):\n    books = Book.objects.values_list('publisher', flat=True)\n    publishers = Publisher.objects.in_bulk(books)\n    self.assertEqual(len(publishers), 4)\n    for publisher in publishers.values():\n        self.assertIsInstance(publisher, Publisher)\n", "def test_annotate_with_aggregate(self):\n    agg = Count('friends')\n    qs = Author.objects.annotate(friend_count=agg).order_by('friend_count')\n    self.assertQuerysetEqual(\n        qs, [\n            'Wesley J. Chun', 'James Bennett', 'Paul Bissex',\n            'Peter Norvig', 'Stuart Russell', 'Adrian Holovaty',\n            'Brad Dayley', 'Jacob Kaplan-Moss', 'Jeffrey Forcier'\n        ],\n        lambda x: x.name\n    )\n"], "sample_225": ["def test_each_context_has_permission(self):\n    request = self.request_factory.get(reverse('test_adminsite:index'))\n    request.user = self.u1\n    ctx = site.each_context(request)\n    self.assertTrue(ctx['has_permission'])\n\n    # Test with a non-superuser\n    u2 = User.objects.create_user(username='normal', password='secret')\n    request.user = u2\n    ctx = site.each_context(request)\n    self.assertFalse(ctx['has_permission'])\n", "def test_each_context_has_permission(self):\n    request = self.request_factory.get(reverse('test_adminsite:index'))\n    request.user = self.u1\n    ctx = site.each_context(request)\n    self.assertTrue(ctx['has_permission'])\n\n    # Test with a user that doesn't have permission\n    u2 = User.objects.create_user(username='nonstaff', password='secret', email='nonstaff@example.com')\n    request.user = u2\n    ctx = site.each_context(request)\n    self.assertFalse(ctx['has_permission'])\n", "    def test_register_without_model(self):\n        class MyAdmin(admin.ModelAdmin):\n            pass\n\n        with self.assertRaises(TypeError):\n            site.register(MyAdmin)\n", "def test_each_context_has_permission(self):\n    request = self.request_factory.get(reverse('test_adminsite:index'))\n    request.user = self.u1\n    ctx = site.each_context(request)\n    self.assertTrue(ctx['has_permission'])\n\n    request.user = User.objects.create_user(username='user', password='secret')\n    ctx = site.each_context(request)\n    self.assertFalse(ctx['has_permission'])\n", "    def setUpTestData(cls):\n        cls.u1 = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n        cls.u2 = User.objects.create_user(username='user', password='secret', email='user@example.com')\n"], "sample_226": ["    def test_create_test_db_name(self):\n        test_connection = get_connection_copy()\n        creation = BaseDatabaseCreation(test_connection)\n        test_database_name = creation._get_test_db_name()\n        self.assertEqual(\n            test_database_name,\n            TEST_DATABASE_PREFIX + test_connection.settings_dict['NAME']\n        )\n", "    def test_create_test_db(self):\n        test_connection = get_connection_copy()\n        db_creation = BaseDatabaseCreation(test_connection)\n        with mock.patch.object(db_creation, '_create_test_db') as _create_test_db:\n            _create_test_db.return_value = 'test_database_name'\n            db_name = db_creation.create_test_db(verbosity=0)\n            self.assertEqual(db_name, 'test_database_name')\n            self.assertEqual(test_connection.settings_dict['NAME'], 'test_database_name')\n            _create_test_db.assert_called_once_with(verbosity=0, autoclobber=False, keepdb=False)\n", "    def test_test_db_signature(self):\n        # Create a connection to the default database.\n        test_connection = get_connection_copy()\n\n        # Get the test database signature.\n        db_signature = BaseDatabaseCreation(test_connection).test_db_signature()\n\n        # Check that the signature matches the expected format.\n        self.assertIsInstance(db_signature, tuple)\n        self.assertEqual(len(db_signature), 4)\n        self.assertEqual(db_signature[0], test_connection.settings_dict['HOST'])\n        self.assertEqual(db_signature[1], test_connection.settings_dict['PORT'])\n        self.assertEqual(db_signature[2], test_connection.settings_dict['ENGINE'])\n        self.assertIn(TEST_DATABASE_PREFIX, db_signature[3])\n", "    def test_get_test_db_name(self):\n        test_connection = get_connection_copy()\n        creation = BaseDatabaseCreation(test_connection)\n        self.assertEqual(\n            creation._get_test_db_name(),\n            TEST_DATABASE_PREFIX + test_connection.settings_dict['NAME']\n        )\n\n        # Test that a custom test database name is used if provided.\n        test_connection.settings_dict['TEST'] = {'NAME': 'custom_test_db'}\n        creation = BaseDatabaseCreation(test_connection)\n        self.assertEqual(creation._get_test_db_name(), 'custom_test_db')\n", "    def test_create_test_db_naming(self):\n        test_connection = get_connection_copy()\n        db_creation = BaseDatabaseCreation(test_connection)\n        test_db_name = db_creation._get_test_db_name()\n        self.assertEqual(test_db_name, 'auto_named_test_database')\n"], "sample_227": ["    def test_generic_relation(self):\n        bookmark = Bookmark.objects.create(url='https://example.com')\n        tag = TaggedItem.objects.create(tag='test', content_object=bookmark)\n        self.assertEqual(bookmark.tags.all()[0], tag)\n        self.assertEqual(tag.content_object, bookmark)\n", "def test_generic_relation_with_empty_filter(self):\n    model_admin = BookmarkAdminGenericRelation(Bookmark, site)\n    request = self.request_factory.get('/bookmarks/', {'tags__tag': ''})\n    queryset = model_admin.get_queryset(request)\n    changelist = model_admin.get_changelist_instance(request)\n    filterspecs = changelist.get_filters(request)\n    for filter in filterspecs:\n        if filter.title == 'By tags':\n            self.assertEqual(filter.used_parameters, {'tags__tag': ''})\n            break\n    else:\n        self.fail('Could not find \"By tags\" filter')\n", "    def test_generic_foreign_key(self):\n        bookmark = Bookmark.objects.create(url='https://www.example.com')\n        tag = TaggedItem.objects.create(tag='example', content_object=bookmark)\n        self.assertEqual(tag.content_object, bookmark)\n", "    def test_generic_relation_reverse_manager(self):\n        bookmark = Bookmark.objects.create(url='https://www.example.com')\n        tag = TaggedItem.objects.create(tag='example', content_object=bookmark)\n        self.assertEqual(list(bookmark.tags.all()), [tag])\n        self.assertEqual(bookmark.tags.count(), 1)\n        self.assertEqual(bookmark.tags.filter(tag='example').count(), 1)\n", "def test_generic_relation_filter(self):\n    # Test if filtering by a generic relation works.\n    model_admin = BookmarkAdminGenericRelation(Bookmark, site)\n    request = self.request_factory.get('/bookmark/?tags__tag=foo')\n    changelist = model_admin.get_changelist_instance(request)\n    queryset = changelist.get_queryset(request)\n    self.assertIsInstance(queryset, models.QuerySet)\n"], "sample_228": ["def test_formset_management_form_error(self):\n    formset = self.make_choiceformset(formset_data=[('Calexico', '100')])\n    # Make the management form data inconsistent with the number of forms.\n    formset.data['choices-TOTAL_FORMS'] = '99'\n    with self.assertRaises(ValidationError):\n        formset.is_valid()\n", "def test_formset_absolute_max(self):\n    # Test that the absolute maximum number of forms is enforced.\n    ChoiceFormSet = formset_factory(Choice, max_num=1000, absolute_max=1000)\n    formset = self.make_choiceformset([('A', 1)] * 1001, ChoiceFormSet)\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(len(formset.non_form_errors()), 1)\n    self.assertIn('Please submit at most 1000 forms.', str(formset.non_form_errors()[0]))\n", "def test_formset_management_form_validation(self):\n    # Test that the ManagementForm is validated correctly\n    formset = self.make_choiceformset(formset_data=[('Choice 1', 10), ('Choice 2', 20)])\n    formset.management_form.cleaned_data['TOTAL_FORMS'] = 'abc'\n    with self.assertRaises(ValidationError):\n        formset.is_valid()\n    formset.management_form.cleaned_data['TOTAL_FORMS'] = '3'\n    self.assertTrue(formset.is_valid())\n", "def test_formset_management_form_validation(self):\n    formset = self.make_choiceformset(formset_data=[('A', '1'), ('B', '2')], total_forms=3)\n    # Tamper with the management form data to make it invalid\n    formset.data['choices-TOTAL_FORMS'] = 'abc'\n    with self.assertRaises(ValidationError) as cm:\n        formset.is_valid()\n    self.assertEqual(cm.exception.code, 'missing_management_form')\n", "def test_formset_management_form_validates(self):\n    data = {\n        'choices-TOTAL_FORMS': '1',\n        'choices-INITIAL_FORMS': '',\n        'choices-MIN_NUM_FORMS': '',\n        'choices-MAX_NUM_FORMS': '',\n    }\n    formset = self.make_choiceformset(data)\n    with self.assertRaises(ValidationError) as cm:\n        formset.management_form.is_valid()\n    self.assertEqual(cm.exception.code, 'missing_management_form')\n    self.assertEqual(cm.exception.message, 'ManagementForm data is missing or has been tampered with')\n"], "sample_229": ["def test_union(self):\n    qs1 = Number.objects.filter(num__lte=3)\n    qs2 = Number.objects.filter(num__gt=7)\n    self.assertNumbersEqual(qs1.union(qs2), [0, 1, 2, 3, 8, 9], ordered=False)\n", "def test_union(self):\n    qs1 = Number.objects.filter(num__lt=5)\n    qs2 = Number.objects.filter(num__gte=5)\n    union_qs = qs1.union(qs2)\n    self.assertNumbersEqual(union_qs, range(10))\n", "def test_union(self):\n    qs1 = Number.objects.filter(num__lte=3)\n    qs2 = Number.objects.filter(num__gt=7)\n    self.assertNumbersEqual(qs1.union(qs2), [0, 1, 2, 3, 8, 9])\n", "def test_union(self):\n    qs1 = Number.objects.filter(num__lte=3)\n    qs2 = Number.objects.filter(num__gte=7)\n    union_qs = qs1.union(qs2)\n    self.assertNumbersEqual(union_qs, [0, 1, 2, 3, 7, 8, 9])\n", "def test_union_with_values(self):\n    qs1 = Number.objects.filter(num__lt=5).values('num')\n    qs2 = Number.objects.filter(num__gte=5).values('num')\n    union_qs = qs1.union(qs2)\n    self.assertQuerysetEqual(union_qs, range(10), operator.itemgetter('num'))\n"], "sample_230": ["def test_disabled_json_field(self):\n    class TestForm(Form):\n        json = JSONField(disabled=True)\n\n    form = TestForm(data={'json': '{\"a\": 1}'})\n    self.assertIsNone(form.fields['json'].to_python('{\"a\": 1}'))\n    self.assertFalse(form.is_valid())\n    self.assertEqual(form.errors, {})\n    self.assertEqual(form.cleaned_data, {'json': None})\n\n    form = TestForm(data={'json': 'invalid'})\n    self.assertEqual(form.fields['json'].to_python('invalid'), 'invalid')\n    self.assertTrue(form.is_valid())\n    self.assertEqual(form.errors, {})\n    self.assertEqual(form.cleaned_data, {'json': 'invalid'})\n", "def test_json_field_custom_encoder_decoder(self):\n    data = {'test': 'value'}\n\n    class CustomEncoder(json.JSONEncoder):\n            if isinstance(obj, set):\n                return list(obj)\n            return super().default(obj)\n\n    class CustomDecoder(json.JSONDecoder):\n            super().__init__(object_hook=self.dict_to_set, *args, **kwargs)\n\n            if 'test' in data:\n                data['test'] = set(data['test'])\n            return data\n\n    field = JSONField(encoder=CustomEncoder, decoder=CustomDecoder)\n    self.assertEqual(field.to_python(json.dumps(data)), data)\n\n    # Test with a custom encoder that can't handle the value.\n    class CustomEncoder(json.JSONEncoder):\n            raise TypeError('Test')\n\n    field = JSONField(encoder=CustomEncoder)\n    with self.assertRaises(ValidationError):\n        field.to_python(json.dumps(data))\n", "def test_jsonfield_clean_validates_json(self):\n    class JSONForm(Form):\n        name = JSONField()\n\n    form = JSONForm({'name': '{\"foo\": \"bar\"}'})\n    self.assertTrue(form.is_valid())\n    self.assertEqual(form.cleaned_data['name'], {'foo': 'bar'})\n\n    form = JSONForm({'name': '[1, 2, 3]'})\n    self.assertTrue(form.is_valid())\n    self.assertEqual(form.cleaned_data['name'], [1, 2, 3])\n\n    form = JSONForm({'name': '\"foo\"'})\n    self.assertTrue(form.is_valid())\n    self.assertEqual(form.cleaned_data['name'], 'foo')\n\n    form = JSONForm({'name': '1'})\n    self.assertTrue(form.is_valid())\n    self.assertEqual(form.cleaned_data['name'], 1)\n\n    form = JSONForm({'name': 'true'})\n    self.assertTrue(form.is_valid())\n    self.assertEqual(form.cleaned_data['name'], True)\n", "def test_json_field_custom_encoder_decoder(self):\n    class CustomEncoder(DjangoJSONEncoder):\n            if isinstance(obj, uuid.UUID):\n                return str(obj)\n            return super().default(obj)\n\n    class CustomDecoder(json.JSONDecoder):\n            super().__init__(*args, object_hook=self.dict_to_uuid, **kwargs)\n\n            for k, v in d.items():\n                if isinstance(v, str) and len(v) == 36:\n                    try:\n                        d[k] = uuid.UUID(v)\n                    except ValueError:\n                        pass\n            return d\n\n    class JSONForm(Form):\n        data = JSONField(encoder=CustomEncoder, decoder=CustomDecoder)\n\n    form = JSONForm({'data': '{\"id\": \"%s\"}' % uuid.uuid4()})\n    self.assertTrue(form.is_valid())\n    self.assertIsInstance(form.cleaned_data['data']['id'], uuid.UUID)\n", "def test_jsonfield_raises_error_for_invalid_input(self):\n    class JSONForm(Form):\n        name = JSONField(encoder=DjangoJSONEncoder)\n\n    form = JSONForm({'name': 'Invalid JSON'})\n    with self.assertRaises(ValidationError) as e:\n        form.full_clean()\n    self.assertEqual(e.exception.code, 'invalid')\n"], "sample_231": ["    def test_cleanse_setting(self):\n        filter = SafeExceptionReporterFilter()\n        self.assertEqual(filter.cleanse_setting('SENSITIVE_SETTING', 'secret_value'), '********************')\n        self.assertEqual(filter.cleanse_setting('NON_SENSITIVE_SETTING', 'public_value'), 'public_value')\n", "    def test_cleanse_setting(self):\n        filter = SafeExceptionReporterFilter()\n        self.assertEqual(filter.cleanse_setting('ALLOWED_HOSTS', ['example.com']), ['example.com'])\n        self.assertEqual(filter.cleanse_setting('SECRET_KEY', 'my_secret_key'), '********************')\n        self.assertEqual(filter.cleanse_setting('DATABASES', {'default': {'ENGINE': 'django.db.backends.sqlite3'}}), {'default': {'ENGINE': 'django.db.backends.sqlite3'}})\n        self.assertEqual(filter.cleanse_setting('API_KEY', 'my_api_key'), '********************')\n", "    def test_cleanse_setting(self):\n        filter = SafeExceptionReporterFilter()\n        self.assertEqual(filter.cleanse_setting('API_KEY', 'my_secret_key'), '********************')\n        self.assertEqual(filter.cleanse_setting('OTHER_SETTING', 'other_value'), 'other_value')\n        self.assertIsInstance(filter.cleanse_setting('CALLABLE_SETTING', lambda: None), CallableSettingWrapper)\n", "    def test_cleansed_setting(self):\n        filter = SafeExceptionReporterFilter()\n        self.assertEqual(filter.cleanse_setting('SETTING', 'value'), 'value')\n        self.assertEqual(filter.cleanse_setting('API_KEY', 'secret'), '********************')\n", "    def test_get_safe_settings(self):\n        filter = SafeExceptionReporterFilter()\n        settings_dict = filter.get_safe_settings()\n        self.assertIsInstance(settings_dict, dict)\n        self.assertEqual(len(settings_dict), len([k for k in dir(settings) if k.isupper()]))\n        for k in settings_dict:\n            self.assertEqual(k, k.upper())\n"], "sample_232": ["def test_key_transform_text_lookup(self):\n    # Create a JSONModel instance with a JSON object\n    obj = JSONModel.objects.create(json_field={'key': 'value'})\n\n    # Test KeyTransformTextLookupMixin lookups\n    self.assertTrue(JSONModel.objects.filter(json_field__key__iexact='Value').exists())\n    self.assertTrue(JSONModel.objects.filter(json_field__key__icontains='alu').exists())\n    self.assertTrue(JSONModel.objects.filter(json_field__key__istartswith='va').exists())\n    self.assertTrue(JSONModel.objects.filter(json_field__key__iendswith='ue').exists())\n\n    # Test KeyTransformTextLookupMixin lookup with non-string value\n    obj.json_field['key'] = 123\n    obj.save()\n    self.assertTrue(JSONModel.objects.filter(json_field__key__iexact=123).exists())\n", "def test_jsonfield_key_transform_raw_sql(self):\n    # Test that using RawSQL with KeyTransform works correctly.\n    # This is a simplified version of ticket #31452.\n    qs = JSONModel.objects.annotate(\n        key=RawSQL(\"data->'key'\", (), output_field=models.CharField()),\n    ).values_list('key', flat=True)\n    self.assertEqual(list(qs), ['value'])\n", "def test_key_transform_nested(self):\n    obj = JSONModel.objects.create(json={\n        'a': {'b': {'c': 'd'}}\n    })\n    self.assertEqual(\n        JSONModel.objects.filter(json__a__b__c='d').count(),\n        1\n    )\n    self.assertEqual(\n        JSONModel.objects.filter(json__a__b__c='e').count(),\n        0\n    )\n    self.assertEqual(\n        JSONModel.objects.filter(json__a__b__contains={'c': 'd'}).count(),\n        1\n    )\n    self.assertEqual(\n        JSONModel.objects.filter(json__a__b__contained_by={'c': 'd', 'e': 'f'}).count(),\n        1\n    )\n    self.assertEqual(\n        JSONModel.objects.filter(json__a__b__keys='c').count(),\n        1\n    )\n", "def test_key_transform_factory(self):\n    key_name = 'test_key'\n    factory = KeyTransformFactory(key_name)\n    transform = factory(models.JSONField())\n    self.assertIsInstance(transform, KeyTransform)\n    self.assertEqual(transform.key_name, key_name)\n", "def test_key_transform_text_lookup(self):\n    # Test that a text lookup on a JSONField key works as expected.\n    obj = JSONModel.objects.create(value={'name': 'John'})\n    self.assertTrue(JSONModel.objects.filter(value__name='John').exists())\n    self.assertFalse(JSONModel.objects.filter(value__name='Jane').exists())\n\n    # Test that the lookup is case-sensitive.\n    obj = JSONModel.objects.create(value={'name': 'John'})\n    self.assertFalse(JSONModel.objects.filter(value__name='john').exists())\n\n    # Test that the lookup works with a KeyTransform.\n    obj = JSONModel.objects.create(value={'name': {'first': 'John'}})\n    self.assertTrue(JSONModel.objects.filter(value__name__first='John').exists())\n    self.assertFalse(JSONModel.objects.filter(value__name__first='Jane').exists())\n\n    # Test that the lookup works with a KeyTextTransform.\n    obj = JSONModel.objects.create(value={'name': {'first': 'John'}})\n    self.assertTrue(JSONModel.objects.filter(value__name__first='John').exists())\n    self.assertFalse(JSONModel.objects.filter(value__name__first='Jane').exists())\n"], "sample_233": ["def test_make_token(self):\n    user = User.objects.create_user('testuser', 'test@example.com', 'password')\n    token_generator = PasswordResetTokenGenerator()\n    token = token_generator.make_token(user)\n    self.assertIsNotNone(token)\n    # Token should have a valid base36 timestamp and a hash value\n    ts_b36, _ = token.split(\"-\")\n    try:\n        base36_to_int(ts_b36)\n    except ValueError:\n        self.fail(\"Invalid base36 timestamp in token\")\n", "def test_make_token_with_timestamp(self):\n    user = User.objects.create_user('testuser', 'test@example.com', 'testpassword')\n    token_generator = PasswordResetTokenGenerator()\n    timestamp = token_generator._num_seconds(datetime(2022, 1, 1))\n    token = token_generator._make_token_with_timestamp(user, timestamp)\n    ts_b36, hash_string = token.split('-')\n    self.assertEqual(int_to_base36(timestamp), ts_b36)\n    self.assertEqual(len(hash_string), 20)  # The hash string should be 20 characters long\n", "def test_token_generator_timeout(self):\n    # Create a user and a token generator with a mocked _now value\n    user = User.objects.create_user('testuser', 'test@example.com', 'testpassword')\n    now = datetime.now()\n    token_generator = MockedPasswordResetTokenGenerator(now)\n    \n    # Generate a token\n    token = token_generator.make_token(user)\n    \n    # Advance the mocked _now value beyond the password reset timeout\n    token_generator._now_val = now + timedelta(seconds=settings.PASSWORD_RESET_TIMEOUT + 1)\n    \n    # The token should be invalid due to the timeout\n    self.assertFalse(token_generator.check_token(user, token))\n", "def test_make_token_with_timestamp_legacy(self):\n    user = User.objects.create_user('testuser', 'test@example.com', 'testpassword')\n    generator = MockedPasswordResetTokenGenerator(datetime.now())\n    token = generator._make_token_with_timestamp(user, self._num_seconds(datetime.now()), legacy=True)\n    self.assertEqual(len(token.split('-')[1]), 20)  # SHA1 hash is 40 characters long, but we take every 2nd character\n", "def test_token_generator_timeout(self):\n    user = User.objects.create_user('testuser', 'test@example.com', 'testpassword')\n    token_generator = MockedPasswordResetTokenGenerator(datetime.now())\n    token = token_generator.make_token(user)\n    \n    # Test that the token is valid immediately after creation\n    self.assertTrue(token_generator.check_token(user, token))\n    \n    # Test that the token is invalid after the timeout period has expired\n    token_generator._now_val += timedelta(seconds=settings.PASSWORD_RESET_TIMEOUT + 1)\n    self.assertFalse(token_generator.check_token(user, token))\n"], "sample_234": ["def test_union_with_empty_queryset(self):\n    qs1 = Number.objects.filter(num__lte=5)\n    qs2 = Number.objects.none()\n    qs3 = Number.objects.filter(num__gt=5)\n\n    union_qs = qs1.union(qs2).union(qs3)\n    self.assertNumbersEqual(union_qs, range(10))\n", "def test_union(self):\n    qs1 = Number.objects.filter(num__lte=5)\n    qs2 = Number.objects.filter(num__gt=5)\n    union_qs = qs1.union(qs2)\n    self.assertNumbersEqual(union_qs, range(10))\n", "def test_union(self):\n    qs1 = Number.objects.filter(num__lt=5)\n    qs2 = Number.objects.filter(num__gte=5)\n    self.assertNumbersEqual(qs1.union(qs2), range(10))\n    self.assertNumbersEqual(qs1.union(qs2).order_by('-num'), range(9, -1, -1))\n    self.assertNumbersEqual(qs1.union(qs2).order_by('num', 'other_num'), range(10))\n", "def test_union(self):\n    qs1 = Number.objects.filter(num__lte=5)\n    qs2 = Number.objects.filter(num__gt=5)\n    union_qs = qs1.union(qs2)\n    self.assertNumbersEqual(union_qs, range(10))\n", "def test_union(self):\n    qs1 = Number.objects.filter(num__lte=5)\n    qs2 = Number.objects.filter(num__gt=5)\n    union_qs = qs1.union(qs2)\n    self.assertNumbersEqual(union_qs, range(10))\n"], "sample_235": ["def test_on_commit_nested(self):\n    \"\"\"Nested on_commit() calls should work correctly.\"\"\"\n        self.notify('outer')\n        transaction.on_commit(lambda: self.notify('inner'))\n\n    transaction.on_commit(outer_notify)\n    self.do(1)\n    self.assertDone([1])\n    self.assertNotified(['outer', 'inner'])\n", "def test_on_commit_inside_atomic_block(self):\n    with transaction.atomic():\n        self.do(1)\n        self.assertNotified([])\n        with transaction.atomic():\n            self.do(2)\n            self.assertNotified([])\n            transaction.commit()\n            self.assertNotified([])\n        self.assertNotified([])\n    self.assertDone([1, 2])\n", "def test_on_commit_after_transaction(self):\n    with transaction.atomic():\n        self.do(1)\n        with self.assertRaises(TransactionManagementError):\n            transaction.on_commit(lambda: self.notify(2))\n    self.assertDone([1])\n", "def test_on_commit_executes_immediately_when_autocommit_enabled(self):\n    # Ensure on_commit callback executes immediately when autocommit is enabled.\n    connection.set_autocommit(True)\n    transaction.on_commit(lambda: self.notify('autocommit'))\n    self.assertNotified(['autocommit'])\n", "def test_on_commit_nested(self):\n    with transaction.atomic():\n        self.do(1)\n        with transaction.atomic():\n            self.do(2)\n            self.do(3)\n        self.do(4)\n    self.assertDone([1, 2, 3, 4])\n"], "sample_236": ["def test_cascade_from_one_to_one_field(self):\n    m = M.objects.create()\n    r = R.objects.create(m=m)\n    collector = Collector(using='default')\n    collector.collect([m])\n    self.assertEqual(collector.data, {M: {m}, R: {r}})\n", "def test_sort_instances(self):\n    collector = Collector(using='default')\n    a1, a2 = A.objects.create(), A.objects.create()\n    b1, b2 = B.objects.create(a=a1), B.objects.create(a=a2)\n    c1, c2 = Child.objects.create(b=b1), Child.objects.create(b=b2)\n\n    collector.collect([a1, a2])\n    self.assertEqual(len(collector.data), 3)\n    self.assertIn(A, collector.data)\n    self.assertIn(B, collector.data)\n    self.assertIn(Child, collector.data)\n\n    collector.sort()\n    instances = list(collector.instances_with_model())\n    self.assertEqual(instances[0][0], Child)\n    self.assertEqual(instances[1][0], Child)\n    self.assertEqual(instances[2][0], B)\n    self.assertEqual(instances[3][0], B)\n    self.assertEqual(instances[4][0], A)\n    self.assertEqual(instances[5][0], A)\n", "def test_sort(self):\n    collector = Collector(using='default')\n    a = create_a()\n    b = a.b_set.create()\n    c = b.c_set.create()\n\n    collector.add([c])\n    collector.collect([b], source=c._meta.model, nullable=False)\n    collector.collect([a], source=b._meta.model, nullable=False)\n\n    collector.sort()\n\n    # Check that the models are in the correct order\n    self.assertEqual(list(collector.data.keys()), [A, B, C])\n\n    # Check that all instances are still present\n    self.assertEqual(len(collector.data[A]), 1)\n    self.assertEqual(len(collector.data[B]), 1)\n    self.assertEqual(len(collector.data[C]), 1)\n", "def test_do_nothing(self):\n    collector = Collector(using='default')\n    b1 = B1.objects.create()\n    b2 = B2.objects.create(b1=b1)\n    with self.assertNumQueries(0):\n        collector.collect([b1], B1._meta.get_field('id'))\n    self.assertEqual(collector.data, {})\n", "def test_sort_with_circular_dependency(self):\n    a = create_a()\n    collector = Collector(using=connection.alias)\n    collector.collect([a], keep_parents=True)\n\n    # Introduce circular dependency between A and R\n    collector.add_dependency(A, R)\n    collector.add_dependency(R, A)\n\n    # Attempt to sort with circular dependency should not raise an error\n    collector.sort()\n\n    # Check that all models are still present in the collector's data\n    self.assertEqual(len(collector.data), len(set(collector.data)))\n"], "sample_237": ["def test_user_model_required_fields_is_not_list(self):\n    errors = checks.run_checks()\n    self.assertEqual(len(errors), 1)\n    error = errors[0]\n    self.assertEqual(error.id, 'auth.E001')\n    self.assertEqual(error.msg, \"'REQUIRED_FIELDS' must be a list or tuple.\")\n    self.assertEqual(error.obj.__name__, 'CustomUserNonListRequiredFields')\n", "def test_check_user_model_required_fields_is_not_list(self):\n    @self.apps.model\n    class CustomUser(models.Model):\n        USERNAME_FIELD = 'username'\n        REQUIRED_FIELDS = 'string'\n\n        username = models.CharField(max_length=10)\n\n    errors = check_user_model(app_configs=self.apps.get_app_configs())\n    expected_errors = [\n        checks.Error(\n            \"'REQUIRED_FIELDS' must be a list or tuple.\",\n            obj=CustomUser,\n            id='auth.E001',\n        ),\n    ]\n    self.assertEqual(errors, expected_errors)\n", "def test_check_user_model_required_fields_is_not_list(self):\n    @self.apps.get_model('CustomUserNonListRequiredFields')\n    class CustomUserNonListRequiredFields(AbstractBaseUser):\n        USERNAME_FIELD = 'username'\n        REQUIRED_FIELDS = 'non_list'\n\n        username = models.CharField(max_length=10, unique=True)\n        non_list = models.CharField(max_length=10)\n\n    errors = check_user_model(app_configs=self.apps.app_configs)\n    self.assertEqual(len(errors), 1)\n    self.assertEqual(errors[0].id, 'auth.E001')\n", "def test_check_user_model_required_fields_not_list(self):\n    @self.apps.get_model\n    class CustomUser(AbstractBaseUser):\n        REQUIRED_FIELDS = 'not a list'\n\n    errors = check_user_model(app_configs=self.apps.app_configs)\n    expected_error = checks.Error(\n        \"'REQUIRED_FIELDS' must be a list or tuple.\",\n        obj=CustomUser,\n        id='auth.E001',\n    )\n    self.assertEqual(errors, [expected_error])\n", "def test_check_user_model_required_fields_is_not_list(self):\n    class CustomUserNonListRequiredFields(AbstractBaseUser):\n        USERNAME_FIELD = 'username'\n        REQUIRED_FIELDS = 'email'\n\n        username = models.CharField(max_length=10, unique=True)\n        email = models.EmailField()\n\n    errors = check_user_model(apps=self.apps)\n    self.assertEqual(len(errors), 1)\n    self.assertEqual(errors[0].id, 'auth.E001')\n    self.assertEqual(errors[0].msg, \"'REQUIRED_FIELDS' must be a list or tuple.\")\n    self.assertEqual(errors[0].obj, CustomUserNonListRequiredFields)\n"], "sample_239": ["def test_baseformset_can_delete(self):\n    # Create a formset with can_delete=True\n    ChoiceFormSet = formset_factory(Choice, can_delete=True)\n    formset = self.make_choiceformset([('A', '1'), ('B', '2')], formset_class=ChoiceFormSet)\n\n    # Make sure the deletion field is present on the forms\n    for form in formset.forms:\n        self.assertIn('DELETE', form.fields)\n\n    # Mark one of the forms for deletion\n    data = {\n        'choices-TOTAL_FORMS': '2',\n        'choices-INITIAL_FORMS': '2',\n        'choices-MIN_NUM_FORMS': '0',\n        'choices-MAX_NUM_FORMS': '1000',\n        'choices-0-choice': 'A',\n        'choices-0-votes': '1',\n        'choices-0-DELETE': 'on',\n        'choices-1-choice': 'B',\n        'choices-1-votes': '2',\n    }\n    formset = ChoiceFormSet(data)\n\n    # Check that the deletion field is processed correctly\n    self.assertTrue(formset.is_valid())\n    self.assertEqual(len(formset.deleted_forms), 1)\n    self.assertEqual(formset.deleted_forms[0].cleaned_data['choice'], 'A')\n", "def test_formset_management_form_required_fields(self):\n    formset = self.make_choiceformset()\n    management_form_data = {\n        'choices-TOTAL_FORMS': '1',\n        # Missing 'choices-INITIAL_FORMS'\n    }\n    with self.assertRaisesMessage(ValidationError, 'ManagementForm data is missing or has been tampered with'):\n        formset.management_form.clean(management_form_data)\n", "def test_formset_validate_min(self):\n    ChoiceFormSet = formset_factory(Choice, min_num=2, validate_min=True)\n    data = {\n        'choices-TOTAL_FORMS': '2',\n        'choices-INITIAL_FORMS': '0',\n        'choices-MIN_NUM_FORMS': '2',\n        'choices-MAX_NUM_FORMS': '2',\n        'choices-0-choice': 'Cale',\n        'choices-0-votes': '100',\n        'choices-1-choice': '',\n        'choices-1-votes': '',\n    }\n    formset = ChoiceFormSet(data)\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(formset.non_form_errors(), ['Please submit at least 2 forms.'])\n", "def test_management_form_required_fields(self):\n    # Create a FormSet with some data\n    formset = self.make_choiceformset([('Choice 1', 1), ('Choice 2', 2)])\n\n    # Remove the TOTAL_FORMS field from the data\n    del formset.data['choices-TOTAL_FORMS']\n\n    # Try to validate the FormSet\n    with self.assertRaises(ValidationError) as cm:\n        formset.is_valid()\n\n    # Check that the error is about the missing TOTAL_FORMS field\n    self.assertEqual(cm.exception.code, 'missing_management_form')\n    self.assertIn('TOTAL_FORMS', str(cm.exception))\n", "def test_formset_management_form(self):\n    ChoiceFormSet = formset_factory(Choice)\n    formset = self.make_choiceformset([('Calexico', '100')])\n    management_form = formset.management_form\n\n    self.assertIsInstance(management_form, Form)\n    self.assertIn('TOTAL_FORMS', management_form.fields)\n    self.assertIn('INITIAL_FORMS', management_form.fields)\n    self.assertIn('MIN_NUM_FORMS', management_form.fields)\n    self.assertIn('MAX_NUM_FORMS', management_form.fields)\n\n    # Check that TOTAL_FORMS is a hidden field.\n    self.assertIsInstance(management_form.fields['TOTAL_FORMS'].widget, HiddenInput)\n    # Check that the form is valid.\n    self.assertTrue(management_form.is_valid())\n    # Check that the total forms is correctly calculated.\n    self.assertEqual(formset.total_form_count(), 1)\n"], "sample_240": ["def test_token_generator_timeout(self):\n    user = User.objects.create_user('testuser', 'test@example.com', 'testpassword')\n    generator = MockedPasswordResetTokenGenerator(datetime.now())\n    token = generator.make_token(user)\n    self.assertTrue(generator.check_token(user, token))\n\n    # Move the clock forward past the timeout period\n    now = datetime.now() + timedelta(seconds=settings.PASSWORD_RESET_TIMEOUT + 1)\n    generator = MockedPasswordResetTokenGenerator(now)\n    self.assertFalse(generator.check_token(user, token))\n", "def test_token_generator_timeout(self):\n    # Create a user and get the token\n    user = User.objects.create_user('testuser', 'test@example.com', 'password')\n    generator = MockedPasswordResetTokenGenerator(datetime.now())\n    token = generator.make_token(user)\n\n    # Move the clock forward past the expiration time\n    now = datetime.now() + timedelta(seconds=settings.PASSWORD_RESET_TIMEOUT + 1)\n    generator = MockedPasswordResetTokenGenerator(now)\n\n    # The token should be invalid\n    self.assertFalse(generator.check_token(user, token))\n", "def test_token_generator_timeout(self):\n    user = User.objects.create_user('testuser', 'test@example.com', 'testpassword')\n    token_generator = PasswordResetTokenGenerator()\n    token = token_generator.make_token(user)\n\n    # Move the clock forward by the token timeout plus one second\n    now = datetime.now() + timedelta(seconds=settings.PASSWORD_RESET_TIMEOUT + 1)\n    mocked_token_generator = MockedPasswordResetTokenGenerator(now)\n\n    self.assertFalse(mocked_token_generator.check_token(user, token))\n", "def test_make_token_with_timestamp_legacy(self):\n    user = User.objects.create_user('testuser', 'test@example.com', 'testpassword')\n    token_generator = PasswordResetTokenGenerator()\n    timestamp = 123456\n    legacy_token = token_generator._make_token_with_timestamp(user, timestamp, legacy=True)\n    self.assertRegex(legacy_token, r'^[0-9a-z]{6}-[0-9a-f]{10}$')\n    # Check that the hash algorithm used is 'sha1' for legacy tokens.\n    with ignore_warnings(category=RemovedInDjango40Warning):\n        hash_string = salted_hmac(\n            token_generator.key_salt,\n            token_generator._make_hash_value(user, timestamp),\n            secret=settings.SECRET_KEY,\n            algorithm='sha1',\n        ).hexdigest()[::2]\n    self.assertEqual(legacy_token.split('-')[1], hash_string)\n", "def test_make_token_with_timestamp(self):\n    user = User.objects.create_user('testuser', 'test@example.com', 'testpassword')\n    token_generator = PasswordResetTokenGenerator()\n    timestamp = token_generator._num_seconds(datetime(2022, 1, 1))\n    token = token_generator._make_token_with_timestamp(user, timestamp)\n    self.assertRegex(token, r'^[0-9a-z]{6}-[0-9a-f]{13}$')\n    # Test legacy token generation\n    legacy_token = token_generator._make_token_with_timestamp(user, timestamp, legacy=True)\n    self.assertRegex(legacy_token, r'^[0-9a-z]{6}-[0-9a-f]{13}$')\n"], "sample_241": ["def test_query_add_filter(self):\n    query = Company.objects.all().query\n    lookup = ('num_employees', 2300)\n    query.add_filter(lookup)\n    self.assertEqual(query.where.children[0].lhs.target.column, 'num_employees')\n    self.assertEqual(query.where.children[0].rhs, 2300)\n", "def test_deferred_to_data(self):\n    # Create a query that defers some fields.\n    qs = Company.objects.defer('num_employees')\n    # Check that deferred_to_data() correctly identifies the fields to load.\n    collection = {}\n    qs.query.deferred_to_data(collection, lambda target, model, fields: target.setdefault(model, []).extend(fields))\n    self.assertEqual(collection, {Company: [Company._meta.get_field('id'), Company._meta.get_field('name'), Company._meta.get_field('num_chairs')]})\n", "def test_query_clone_with_filtered_relation(self):\n    Company.objects.create(name='Test GmbH', num_employees=32, num_chairs=1)\n    qs = Company.objects.filter(ceo__firstname='Max')\n    cloned_qs = qs.clone()\n    self.assertEqual(qs.query._filtered_relations, cloned_qs.query._filtered_relations)\n    self.assertIsNot(qs.query._filtered_relations, cloned_qs.query._filtered_relations)\n", "def test_expression_pickle(self):\n    expr = F('salary') + Value(10)\n    pickled_expr = pickle.dumps(expr)\n    unpickled_expr = pickle.loads(pickled_expr)\n    self.assertEqual(expr, unpickled_expr)\n\n    # Check that the unpickled expression can be used in a query.\n    employee = Employee.objects.create(firstname='John', lastname='Doe', salary=20)\n    result = Employee.objects.filter(id=employee.id).update(salary=unpickled_expr)\n    employee.refresh_from_db()\n    self.assertEqual(employee.salary, 30)\n", "def test_query_set_alias(self):\n    qs = Company.objects.alias(\n        employee_count=Count(\"employees\"),\n        chair_count=Count(\"num_chairs\")\n    ).filter(employee_count__gt=0, chair_count__gt=0)\n    self.assertEqual(qs.count(), 2)\n    self.assertEqual(qs.first().name, \"Example Inc.\")\n"], "sample_242": ["def test_get_source_expressions(self):\n    lookup = CustomLookup(lhs=Value(1), rhs=Value(2))\n    self.assertEqual(lookup.get_source_expressions(), [Value(1), Value(2)])\n    lookup = CustomLookup(lhs=Value(1), rhs=1)\n    self.assertEqual(lookup.get_source_expressions(), [Value(1)])\n", "def test_get_db_prep_lookup_value_is_iterable(self):\n    class CustomLookup(FieldGetDbPrepValueMixin, Lookup):\n        get_db_prep_lookup_value_is_iterable = True\n\n    lookup = CustomLookup(lhs=DateTimeField(), rhs=[1, 2, 3])\n    with mock.patch.object(DateTimeField, 'get_db_prep_value') as get_db_prep_value:\n        lookup.get_db_prep_lookup([1, 2, 3], connection=None)\n        self.assertEqual(get_db_prep_value.call_count, 3)\n", "def test_lookup_get_source_expressions(self):\n    lhs = mock.Mock()\n    rhs = mock.Mock()\n    lookup = CustomLookup(lhs, rhs)\n    with self.subTest(rhs_is_direct_value=False):\n        rhs.has_as_sql = True\n        self.assertEqual(lookup.get_source_expressions(), [lhs, rhs])\n    with self.subTest(rhs_is_direct_value=True):\n        rhs.has_as_sql = False\n        self.assertEqual(lookup.get_source_expressions(), [lhs])\n", "def test_lookup_get_source_expressions(self):\n    lookup = CustomLookup(lhs='field', rhs='value')\n    self.assertEqual(lookup.get_source_expressions(), ['field'])\n\n    lookup = CustomLookup(lhs='field', rhs=Value('value'))\n    self.assertEqual(lookup.get_source_expressions(), ['field', Value('value')])\n", "def test_lookup_get_source_expressions(self):\n    lookup = CustomLookup(lhs=Value(1), rhs=Value(2))\n    self.assertEqual(lookup.get_source_expressions(), [Value(1), Value(2)])\n    lookup = CustomLookup(lhs=Value(1), rhs=None)\n    self.assertEqual(lookup.get_source_expressions(), [Value(1)])\n"], "sample_243": ["def test_deferred_to_data(self):\n    query = Query(Author)\n    query.deferred_loading = (frozenset(['name']), True)\n    data = {}\n    query.deferred_to_data(data, lambda target, model, fields: target.setdefault(model, set()).update(fields))\n    self.assertEqual(data, {Author: {'id'}})\n", "def test_build_lookup(self):\n    query = Query(Author)\n    lookup = query.build_lookup(['exact'], F('name'), 'John')\n    self.assertIsInstance(lookup, Exact)\n    self.assertEqual(lookup.lhs.output_field, Author._meta.get_field('name'))\n    self.assertEqual(lookup.rhs, 'John')\n\n    lookup = query.build_lookup(['lower__exact'], F('name'), 'john')\n    self.assertIsInstance(lookup, Exact)\n    self.assertIsInstance(lookup.lhs, Lower)\n    self.assertEqual(lookup.lhs.arg.output_field, Author._meta.get_field('name'))\n    self.assertEqual(lookup.rhs, 'john')\n\n    with self.assertRaises(FieldError):\n        query.build_lookup(['invalid__lookup'], F('name'), 'value')\n", "def test_build_filter_on_annotated_col(self):\n    class MyFunc(Func):\n        function = 'TEST_FUNC'\n        arity = 1\n\n    query = Query(Author)\n    annotation = MyFunc('name')\n    annotation.output_field = CharField()\n    query.annotations['my_annotation'] = annotation\n    clause, joins = query.build_filter(('my_annotation__exact', 'test'))\n    self.assertEqual(clause.children[0].lhs.function, 'TEST_FUNC')\n    self.assertEqual(clause.children[0].rhs.value, 'test')\n", "def test_build_filter_lookup_in(self):\n    class MyLookup(Lookup):\n        lookup_name = 'mylookup'\n\n            return 'mylookup', []\n\n    register_lookup(CharField, MyLookup)\n\n    query = Query(Item)\n    filter_expr = ('name__mylookup', 'value')\n    clause, _ = query.build_filter(filter_expr)\n    self.assertIsInstance(clause.children[0], MyLookup)\n", "def test_build_lookup(self):\n    class DummyModel:\n        foo = CharField()\n\n    query = Query(DummyModel)\n    lookup = query.build_lookup(['exact'], Col('foo', 'bar'), 'baz')\n    self.assertIsInstance(lookup, Exact)\n    self.assertEqual(lookup.lhs.output_field.name, 'foo')\n\n    lookup = query.build_lookup(['isnull'], Col('foo', 'bar'), True)\n    self.assertIsInstance(lookup, IsNull)\n    self.assertEqual(lookup.lhs.output_field.name, 'foo')\n\n    with self.assertRaises(FieldError):\n        query.build_lookup(['nonexistent_lookup'], Col('foo', 'bar'), 'baz')\n"], "sample_244": ["def test_formset_absolute_max(self):\n    ChoiceFormSet = formset_factory(Choice, absolute_max=2)\n    formset_data = [('Choice 1', '10'), ('Choice 2', '12'), ('Choice 3', '8')]\n    formset = self.make_choiceformset(formset_data, ChoiceFormSet)\n\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(len(formset.non_form_errors()), 1)\n    self.assertEqual(formset.non_form_errors()[0].code, 'too_many_forms')\n", "def test_formset_validate_min(self):\n    # Create a formset that validates for a minimum of 2 forms\n    MinChoiceFormSet = formset_factory(Choice, min_num=2, validate_min=True)\n    formset = self.make_choiceformset(formset_class=MinChoiceFormSet, total_forms=1)\n\n    # With 1 form, the formset should be invalid\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(formset.non_form_errors(), [ValidationError('Please submit at least 2 forms.', code='too_few_forms')])\n\n    # With 2 forms, the formset should be valid\n    formset = self.make_choiceformset(formset_class=MinChoiceFormSet, total_forms=2)\n    self.assertTrue(formset.is_valid())\n", "def test_baseformset_absolute_max(self):\n    # Test that the absolute_max parameter is enforced.\n    formset = self.make_choiceformset(formset_data=[('Choice1', 10)] * 1001, total_forms=1001)\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(len(formset.non_form_errors()), 1)\n    self.assertIn('Please submit at most 1000 forms.', str(formset.non_form_errors()[0]))\n", "def test_formset_absolute_max(self):\n    ChoiceFormSetAbsoluteMax = formset_factory(Choice, max_num=10, absolute_max=5)\n    data = {\n        'choices-TOTAL_FORMS': 6,\n        'choices-INITIAL_FORMS': 0,\n        'choices-MIN_NUM_FORMS': 0,\n        'choices-MAX_NUM_FORMS': 10,\n        'choices-0-choice': 'choice1',\n        'choices-0-votes': 1,\n        'choices-1-choice': 'choice2',\n        'choices-1-votes': 2,\n        'choices-2-choice': 'choice3',\n        'choices-2-votes': 3,\n        'choices-3-choice': 'choice4',\n        'choices-3-votes': 4,\n        'choices-4-choice': 'choice5',\n        'choices-4-votes': 5,\n        'choices-5-choice': 'choice6',\n        'choices-5-votes': 6,\n    }\n    formset = ChoiceFormSetAbsoluteMax(data)\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(len(formset.non_form_errors()), 1)\n    self.assertIn('Please submit at most 5 forms.', str(formset.non_form_errors()))\n", "def test_formset_management_form_validation(self):\n    formset = self.make_choiceformset(formset_data=[('Choice 1', '1'), ('Choice 2', '2')])\n    formset.management_form.data['choices-TOTAL_FORMS'] = 'abc'  # Invalid data\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(len(formset.management_form.errors), 1)\n    self.assertIn('TOTAL_FORMS', formset.management_form.errors)\n"], "sample_245": ["    def test_build_file_init(self):\n        command = mock.Mock()\n        domain = 'django'\n        translatable = mock.Mock()\n        build_file = MakeMessagesCommand.build_file_class(command, domain, translatable)\n        self.assertEqual(build_file.command, command)\n        self.assertEqual(build_file.domain, domain)\n        self.assertEqual(build_file.translatable, translatable)\n", "def test_makemessages_command_output(self):\n    # Test that the makemessages command output is as expected\n    with captured_stdout() as stdout:\n        management.call_command('makemessages', locale=[LOCALE], verbosity=2)\n    self.assertIn('processing locale %s' % LOCALE, stdout.getvalue())\n    self.assertIn('processing file', stdout.getvalue())\n", "def test_write_pot_file(self):\n    pot_contents = \"\"\"msgid \"\"", "    def test_no_locale(self):\n        with self.assertRaises(CommandError):\n            management.call_command('makemessages', verbosity=0)\n", "    def test_requires_subcommand(self):\n        with captured_stderr() as stderr:\n            execute_from_command_line(['django-admin', 'makemessages'])\n        self.assertRegex(stderr.getvalue(), r'makemessages: error: the following arguments are required: --locale, --exclude, or --all')\n"], "sample_246": ["def test_find_files_ignores_media_and_static_roots(self):\n    media_root = os.path.join(self.test_dir, 'media')\n    static_root = os.path.join(self.test_dir, 'static')\n    os.mkdir(media_root)\n    os.mkdir(static_root)\n\n    file1 = os.path.join(media_root, 'file1.txt')\n    file2 = os.path.join(static_root, 'file2.txt')\n    file3 = os.path.join(self.test_dir, 'file3.txt')\n\n    with open(file1, 'w') as f:\n        f.write('Hello')\n    with open(file2, 'w') as f:\n        f.write('World')\n    with open(file3, 'w') as f:\n        f.write('Django')\n\n    files = MakeMessagesCommand().find_files(self.test_dir)\n\n    self.assertEqual(len(files), 1)\n    self.assertEqual(files[0].path, file3)\n", "def test_makemessages_with_locale_dir_outside_of_app(self):\n    app_dir = Path('app')\n    locale_dir = Path('locale')\n    os.mkdir(app_dir)\n    os.mkdir(locale_dir)\n    open(os.path.join(app_dir, '__init__.py'), 'w').close()\n    open(os.path.join(app_dir, 'models.py'), 'w').write('from django.utils.translation import gettext as _\\n_(\"Hello\")')\n\n    with captured_stdout():\n        management.call_command('makemessages', locale=[LOCALE], verbosity=0)\n\n    self.assertTrue(os.path.exists(os.path.join(locale_dir, LOCALE, 'LC_MESSAGES', 'django.po')))\n    self.assertMsgId(_('Hello'), open(os.path.join(locale_dir, LOCALE, 'LC_MESSAGES', 'django.po')).read())\n", "    def setUp(self):\n        super().setUp()\n        self.locale_dir = tempfile.mkdtemp()\n", "    def test_is_templatized(self):\n        build_file = MakeMessagesCommand.build_file_class(None, 'django', None)\n        build_file.domain = 'djangojs'\n        build_file.command = mock.Mock(gettext_version=(0, 18, 2))\n        self.assertTrue(build_file.is_templatized)\n\n        build_file.command.gettext_version = (0, 18, 3)\n        self.assertFalse(build_file.is_templatized)\n\n        build_file.domain = 'django'\n        build_file.translatable = mock.Mock(file='foo.html')\n        self.assertTrue(build_file.is_templatized)\n\n        build_file.translatable.file = 'foo.py'\n        self.assertFalse(build_file.is_templatized)\n", "    def test_handle_extensions(self):\n        # Test that the handle_extensions method correctly handles different formats\n        extensions = ['html,txt,py']\n        self.assertEqual(handle_extensions(extensions), {'.html', '.txt', '.py'})\n\n        extensions = ['js']\n        self.assertEqual(handle_extensions(extensions), {'.js'})\n\n        extensions = []\n        self.assertEqual(handle_extensions(extensions), set())\n\n        extensions = None\n        with self.assertRaises(TypeError):\n            handle_extensions(extensions)\n"], "sample_247": ["def test_exists_subquery_with_outer_ref_in_filter(self):\n    subquery = Book.objects.filter(pages=OuterRef('pages'))\n    books = Book.objects.annotate(has_equal_pages=Exists(subquery)).filter(has_equal_pages=True)\n    self.assertQuerysetEqual(books, ['<Book: Artificial Intelligence: A Modern Approach>'])\n", "def test_exists_subquery_with_outer_ref_in_filter(self):\n    authors = Author.objects.annotate(\n        has_book=Exists(Book.objects.filter(authors=OuterRef('pk'))),\n    ).filter(has_book=True)\n    self.assertQuerysetEqual(authors, [self.a1, self.a2, self.a3, self.a4, self.a5, self.a6, self.a7, self.a8, self.a9])\n", "def test_order_by_annotation(self):\n    books = Book.objects.annotate(total_authors=Count('authors')).order_by('-total_authors')\n    self.assertQuerysetEqual(\n        books,\n        [self.b4, self.b5, self.b1, self.b6, self.b2, self.b3],\n        lambda b: b.pk\n    )\n    books = books.order_by('total_authors')\n    self.assertQuerysetEqual(\n        books,\n        [self.b2, self.b3, self.b6, self.b1, self.b5, self.b4],\n        lambda b: b.pk\n    )\n", "def test_add_fields_with_non_string_field_names(self):\n    with self.assertRaisesMessage(TypeError, 'field_names must be a list of strings'):\n        Author.objects.add_fields([1])\n", "def test_resolve_expression(self):\n    # Ensure that resolve_expression() properly handles a simple F() expression.\n    expr = F('name')\n    resolved_expr = expr.resolve_expression(Book.objects.all().query)\n    self.assertEqual(resolved_expr, Col(Book._meta.get_field('name'), 'name'))\n"], "sample_248": ["def test_handle_command_option(self):\n    with captured_stdout() as stdout:\n        call_command('shell', '--command', self.script_globals)\n    self.assertEqual(stdout.getvalue().strip(), 'True')\n    with captured_stdout() as stdout:\n        call_command('shell', '--command', self.script_with_inline_function)\n    self.assertEqual(stdout.getvalue().strip(), __version__)\n", "def test_no_startup_option(self):\n    with captured_stdout() as stdout:\n        call_command('shell', '--no-startup')\n        self.assertEqual(stdout.getvalue(), '')\n", "def test_handle_command(self):\n    with captured_stdout() as stdout:\n        call_command('shell', command=self.script_globals)\n    self.assertEqual(stdout.getvalue().strip(), 'True')\n\n    with captured_stdout() as stdout:\n        call_command('shell', command=self.script_with_inline_function)\n    self.assertEqual(stdout.getvalue().strip(), __version__)\n", "def test_handle_command_option(self):\n    with captured_stdout() as stdout:\n        call_command('shell', '--command', self.script_globals)\n    self.assertEqual(stdout.getvalue().strip(), 'True')\n", "def test_handle_command_option(self):\n    with captured_stdout() as stdout:\n        call_command('shell', '--command', self.script_globals)\n    self.assertEqual(stdout.getvalue().strip(), 'True')\n"], "sample_249": ["    def test_create_test_db(self):\n        test_connection = get_connection_copy()\n        db_creation = BaseDatabaseCreation(test_connection)\n        test_db_name = db_creation.create_test_db(verbosity=0)\n        self.assertIn(TEST_DATABASE_PREFIX, test_db_name)\n        self.assertEqual(test_connection.settings_dict['NAME'], test_db_name)\n", "    def test_create_test_db(self):\n        # Ensure creating the test database doesn't modify the original settings.\n        test_connection = get_connection_copy()\n        db_settings = test_connection.settings_dict\n        db_name = db_settings['NAME']\n        test_db_name = TEST_DATABASE_PREFIX + db_name\n\n        creation = BaseDatabaseCreation(test_connection)\n        test_db_name_returned = creation.create_test_db(verbosity=0)\n\n        self.assertEqual(test_db_name_returned, test_db_name)\n        self.assertEqual(db_settings['NAME'], db_name)\n\n        # The database really was created.\n        c = test_connection.cursor()\n        c.execute('SELECT 1')\n        c.fetchone()\n", "    def test_create_test_db(self):\n        # Ensure we're not using the real database\n        db_settings = connection.settings_dict\n        db_settings['NAME'] = 'test_database'\n\n        # Mock out the execute SQL functions to avoid actually creating a DB\n        with mock.patch.object(connection, '_nodb_cursor') as mock_cursor:\n            with mock.patch.object(connection, 'close'):\n                creation = BaseDatabaseCreation(connection)\n                creation._create_test_db(verbosity=0, autoclobber=False, keepdb=False)\n\n        # Check that the mock cursor executed a CREATE DATABASE statement\n        mock_cursor.return_value.__enter__.return_value.execute.assert_called_once()\n        self.assertIn('CREATE DATABASE', mock_cursor.return_value.__enter__.return_value.execute.call_args[0][0])\n", "    def test_create_test_db(self):\n        test_connection = get_connection_copy()\n        creation = BaseDatabaseCreation(test_connection)\n        test_db_name = creation.create_test_db(verbosity=0, autoclobber=True)\n\n        # Ensure the test database was created with the correct prefix.\n        self.assertStartsWith(test_db_name, TEST_DATABASE_PREFIX)\n        self.assertNotEqual(test_db_name, test_connection.settings_dict['NAME'])\n", "    def test_get_test_db_name(self):\n        test_connection = get_connection_copy()\n        creation = BaseDatabaseCreation(test_connection)\n        test_database_name = creation._get_test_db_name()\n        self.assertEqual(\n            test_database_name,\n            TEST_DATABASE_PREFIX + test_connection.settings_dict['NAME']\n        )\n"], "sample_250": ["def test_day_of_week(self):\n    dt = date(2022, 6, 15)\n    self.assertEqual(format(dt, 'l'), 'Wednesday')\n    self.assertEqual(format(dt, 'D'), 'Wed')\n    with translation.override('fr'):\n        self.assertEqual(format(dt, 'l'), 'mercredi')\n        self.assertEqual(format(dt, 'D'), 'mer')\n", "def test_time_format(self):\n    dt = datetime(2022, 1, 1, 12, 0, 0)\n    self.assertEqual(dateformat.time_format(dt, 'a'), 'p.m.')\n    self.assertEqual(dateformat.time_format(dt, 'A'), 'PM')\n    self.assertEqual(dateformat.time_format(dt, 'f'), '12:00')\n    self.assertEqual(dateformat.time_format(dt, 'g'), '12')\n    self.assertEqual(dateformat.time_format(dt, 'G'), '12')\n    self.assertEqual(dateformat.time_format(dt, 'h'), '12')\n    self.assertEqual(dateformat.time_format(dt, 'H'), '12')\n    self.assertEqual(dateformat.time_format(dt, 'i'), '00')\n    self.assertEqual(dateformat.time_format(dt, 'O'), '+0100')\n    self.assertEqual(dateformat.time_format(dt, 'P'), 'noon')\n    self.assertEqual(dateformat.time_format(dt, 's'), '00')\n    self.assertEqual(dateformat.time_format(dt, 'T'), 'CET')\n    self.assertEqual(dateformat.time_format(dt, 'u'), '000000')\n    self.assertEqual(dateformat.time_format(dt, 'Z'), '3600')\n", "def test_date_format(self):\n    d = date(2009, 2, 12)\n    self.assertEqual(dateformat.format(d, 'Y'), '2009')\n    self.assertEqual(dateformat.format(d, 'y'), '09')\n    self.assertEqual(dateformat.format(d, 'm'), '02')\n    self.assertEqual(dateformat.format(d, 'n'), '2')\n    self.assertEqual(dateformat.format(d, 'd'), '12')\n    self.assertEqual(dateformat.format(d, 'j'), '12')\n    self.assertEqual(dateformat.format(d, 'l'), 'Thursday')\n    self.assertEqual(dateformat.format(d, 'D'), 'Thu')\n    self.assertEqual(dateformat.format(d, 'w'), '4')\n", "def test_date_format(self):\n    d = date(2009, 5, 16)\n    self.assertEqual(dateformat.format(d, 'Y'), '2009')\n    self.assertEqual(dateformat.format(d, 'y'), '09')\n    self.assertEqual(dateformat.format(d, 'F'), 'May')\n    self.assertEqual(dateformat.format(d, 'E'), 'May')\n    self.assertEqual(dateformat.format(d, 'M'), 'May')\n    self.assertEqual(dateformat.format(d, 'b'), 'may')\n    self.assertEqual(dateformat.format(d, 'N'), 'May')\n    self.assertEqual(dateformat.format(d, 'm'), '05')\n    self.assertEqual(dateformat.format(d, 'n'), '5')\n    self.assertEqual(dateformat.format(d, 'd'), '16')\n    self.assertEqual(dateformat.format(d, 'j'), '16')\n    self.assertEqual(dateformat.format(d, 'D'), 'Sat')\n    self.assertEqual(dateformat.format(d, 'l'), 'Saturday')\n    self.assertEqual(dateformat.format(d, 'S'), 'th')\n    self.assertEqual(dateformat.format(d, 'w'), '6')\n    self.assertEqual(dateformat.format(d, 'z'), '136')\n    self.assertEqual(dateformat.format(d, 'W'), '20')\n    self.assertEqual(dateformat.format(d, 't'), '31')\n    self.assertEqual(dateformat.format(d, 'L'), False)\n    self.assertEqual(dateformat.format(d, 'o'), '2009')\n", "def test_date_format(self):\n    d = date(2009, 2, 12)\n    self.assertEqual(dateformat.format(d, 'b'), 'Feb')\n    self.assertEqual(dateformat.format(d, 'c'), '2009-02-12')\n    self.assertEqual(dateformat.format(d, 'd'), '12')\n    self.assertEqual(dateformat.format(d, 'D'), 'Thu')\n    self.assertEqual(dateformat.format(d, 'e'), '')\n    self.assertEqual(dateformat.format(d, 'E'), 'February')\n    self.assertEqual(dateformat.format(d, 'F'), 'February')\n    self.assertEqual(dateformat.format(d, 'I'), '')\n    self.assertEqual(dateformat.format(d, 'j'), '12')\n    self.assertEqual(dateformat.format(d, 'l'), 'Thursday')\n    self.assertEqual(dateformat.format(d, 'L'), False)\n    self.assertEqual(dateformat.format(d, 'm'), '02')\n    self.assertEqual(dateformat.format(d, 'M'), 'Feb')\n    self.assertEqual(dateformat.format(d, 'n'), '2')\n    self.assertEqual(dateformat.format(d, 'N'), 'Feb.')\n    self.assertEqual(dateformat.format(d, 'o'), '2009')\n    self.assertEqual(dateformat.format(d, 'r'), '')\n    self.assertEqual(dateformat.format(d, 'S'), 'th')\n    self.assertEqual(dateformat.format(d, 't'), '28')\n    self.assertEqual(dateformat.format(d, 'U'), '1234567890')\n    self.assertEqual(dateformat.format(d, 'w'), '4')\n    self.assertEqual(dateformat.format(d, 'W'), '6')\n    self.assertEqual(dateformat.format(d, 'y'), '09')\n    self.assertEqual(dateformat.format(d, 'Y'), '2009')\n    self.assertEqual(dateformat.format(d, 'z'), '43')\n"], "sample_251": ["def test_flatten(self):\n    # Test that an expression can be flattened into its constituent parts.\n    expr = (F('a') + F('b')) * F('c')\n    parts = list(expr.flatten())\n    self.assertEqual(len(parts), 4)\n    self.assertIsInstance(parts[0], Expression)\n    self.assertIsInstance(parts[1], F)\n    self.assertIsInstance(parts[2], F)\n    self.assertIsInstance(parts[3], F)\n", "def test_expressionlist(self):\n    expr1 = F('field1')\n    expr2 = F('field2')\n    exprlist = ExpressionList(expr1, expr2)\n    self.assertEqual(len(exprlist.source_expressions), 2)\n    self.assertEqual(exprlist.source_expressions[0], expr1)\n    self.assertEqual(exprlist.source_expressions[1], expr2)\n    self.assertEqual(str(exprlist), 'field1, field2')\n", "def test_expression_output_field(self):\n    expression = ExpressionWrapper(F('age') + Value(10), output_field=IntegerField())\n    self.assertEqual(expression.output_field.get_internal_type(), 'IntegerField')\n", "def test_expression_wrapper(self):\n    wrapper = ExpressionWrapper(F('name'), output_field=CharField())\n    self.assertEqual(wrapper.output_field.__class__, CharField)\n    self.assertEqual(wrapper.expression.__class__, F)\n\n    qs = Author.objects.annotate(name_lower=wrapper).values_list('name_lower')\n    self.assertQuerysetEqual(\n        qs, [\n            ('Adrian Holovaty',),\n            ('Brad Dayley',),\n            ('Jacob Kaplan-Moss',),\n            ('James Bennett',),\n            ('Jeffrey Forcier',),\n            ('Paul Bissex',),\n            ('Peter Norvig',),\n            ('Stuart Russell',),\n            ('Wesley J. Chun',),\n        ],\n        transform=lambda x: (x[0].lower(),),\n        ordered=False,\n    )\n", "def test_expression_in_filter(self):\n    authors = Author.objects.annotate(age_as_float=ExpressionWrapper(F('age'), output_field=FloatField()))\n    self.assertEqual(authors.filter(age_as_float__gt=30).count(), 5)\n"], "sample_252": ["def test_key_transform_text_lookup_mixin(self):\n    mixin = KeyTransformTextLookupMixin()\n    key_transform = KeyTransform('test', 'test')\n    with self.assertRaises(TypeError):\n        mixin.__init__(key_transform, 'wrong_arg')\n", "def test_key_transform_text_lookup(self):\n    obj = JSONModel.objects.create(json={\"a\": \"foo\", \"b\": {\"c\": \"bar\"}})\n    self.assertSequenceEqual(\n        JSONModel.objects.filter(json__a__iexact=\"FOO\"),\n        [obj],\n    )\n    self.assertSequenceEqual(\n        JSONModel.objects.filter(json__b__c__contains=\"ba\"),\n        [obj],\n    )\n    self.assertSequenceEqual(\n        JSONModel.objects.filter(json__b__c__startswith=\"ba\"),\n        [obj],\n    )\n    self.assertSequenceEqual(\n        JSONModel.objects.filter(json__b__c__endswith=\"ar\"),\n        [obj],\n    )\n", "def test_key_transform_factory(self):\n    # Test that the KeyTransformFactory correctly returns a KeyTransform instance.\n    key_name = 'test_key'\n    factory = KeyTransformFactory(key_name)\n    transform = factory(models.JSONField())\n    self.assertIsInstance(transform, KeyTransform)\n    self.assertEqual(transform.key_name, key_name)\n", "def test_key_transform_exact_lookup_with_null(self):\n    obj = JSONModel.objects.create(value={'key': None})\n    self.assertTrue(JSONModel.objects.filter(value__key__exact=None).exists())\n    self.assertFalse(JSONModel.objects.filter(value__key='nonexistent').exists())\n", "def test_json_field_key_transform_text_lookup(self):\n    # Create a JSONModel instance with a JSON field containing a nested object\n    obj = JSONModel.objects.create(json={\"name\": \"John\", \"age\": 30, \"address\": {\"street\": \"123 Main St\"}})\n\n    # Use KeyTransformTextLookupMixin to query the JSON field\n    qs = JSONModel.objects.filter(json__address__street=\"123 Main St\")\n\n    # Assert that the query returns the created object\n    self.assertEqual(qs.get(), obj)\n"], "sample_253": ["def test_iter_modules_and_files_zip_imports(self):\n    with tempfile.TemporaryDirectory() as tmpdir:\n        # Create a zip file containing a Python module.\n        with zipfile.ZipFile(Path(tmpdir) / 'myzip.zip', 'w') as zip_file:\n            zip_file.writestr('mymodule.py', b'')\n        # Add the zip file to sys.path.\n        with extend_sys_path([Path(tmpdir) / 'myzip.zip']):\n            self.import_and_cleanup('mymodule')\n            self.assertFileFound(Path(tmpdir) / 'myzip.zip')\n", "def test_iter_modules_and_files_with_zip_file(self):\n    tmp_dir = tempfile.mkdtemp()\n    self.addCleanup(shutil.rmtree, tmp_dir)\n    with open(os.path.join(tmp_dir, 'test.zip'), 'wb') as f:\n        with zipfile.ZipFile(f, 'w') as zip_file:\n            zip_file.writestr('test.py', b'')\n    sys.path.insert(0, tmp_dir)\n    self.addCleanup(sys.path.remove, tmp_dir)\n    self.import_and_cleanup('test')\n    self.assertFileFound(Path(tmp_dir) / 'test.zip')\n", "    def test_common_roots(self):\n        paths = [\n            Path('/path/to/dir1/file1.py'),\n            Path('/path/to/dir1/file2.py'),\n            Path('/path/to/dir2/file3.py'),\n            Path('/path/to/dir2/subdir/file4.py'),\n        ]\n        expected_roots = (\n            Path('/path/to/dir1'),\n            Path('/path/to/dir2'),\n        )\n        self.assertEqual(autoreload.common_roots(paths), expected_roots)\n", "    def test_common_roots(self):\n        paths = [\n            Path('/path/to/directory/file1.py'),\n            Path('/path/to/directory/subdir/file2.py'),\n            Path('/path/to/directory/subdir/subsubdir/file3.py'),\n        ]\n        common_roots = autoreload.common_roots(paths)\n        self.assertEqual(common_roots, (Path('/path/to/directory'),))\n", "    def test_common_roots(self):\n        roots = autoreload.common_roots([\n            Path('/home/user/project/app1/models.py'),\n            Path('/home/user/project/app2/models.py'),\n            Path('/home/user/project/settings.py'),\n        ])\n        self.assertEqual(roots, (Path('/home/user/project'),))\n"], "sample_254": ["def test_get_inline_formsets(self):\n    request = self.factory.get('/')\n    request.user = self.superuser\n    admin = ModelAdmin(Holder, admin_site)\n    inline_instances = [InnerInline(Holder, admin_site)]\n    obj = Holder.objects.get(dummy=13)\n    formsets, inlines = admin._create_formsets(request, obj, change=True)\n    inline_admin_formsets = admin.get_inline_formsets(request, formsets, inline_instances, obj)\n    self.assertEqual(len(inline_admin_formsets), 1)\n", "def test_inline_admin_formset_view_permission(self):\n    request = self.factory.get('/admin/')\n    request.user = self.superuser\n\n    class TestInlineAdmin(TabularInline):\n        model = Inner\n\n    class TestAdmin(ModelAdmin):\n        inlines = [TestInlineAdmin]\n        has_change_permission = lambda self, request, obj=None: False\n\n    admin = TestAdmin(Holder, admin_site)\n\n    inline_admin_formset = admin.get_inline_formsets(request, Holder.objects.all(), [], obj=self.holder)[0]\n\n    self.assertFalse(inline_admin_formset.has_change_permission)\n    self.assertFalse(inline_admin_formset.has_add_permission)\n    self.assertFalse(inline_admin_formset.has_delete_permission)\n", "def test_inline_get_queryset(self):\n    request = self.factory.get(reverse('admin:admin_inlines_holder_changelist'))\n    request.user = self.superuser\n    inline = InnerInline(Holder, admin_site)\n    queryset = inline.get_queryset(request)\n    self.assertEqual(queryset.model, Inner)\n", "def test_inline_formset_validation(self):\n    request = self.factory.post('/admin/holder/add/')\n    admin = ModelAdmin(Holder, admin_site)\n    formset = admin.get_inline_formsets(request, Holder())[0]\n    self.assertIsInstance(formset.form().fields['dummy'], forms.IntegerField)\n\n    # Missing required field 'dummy' should raise ValidationError\n    data = {'holder': 1}\n    with self.assertRaises(forms.ValidationError):\n        formset.form(data).full_clean()\n", "def test_inline_get_queryset(self):\n    request = self.factory.get('/admin/')\n    request.user = self.superuser\n    inline = InnerInline(self.model, admin_site)\n    queryset = inline.get_queryset(request)\n    self.assertEqual(list(queryset), list(Inner.objects.all()))\n"], "sample_256": ["    def test_render(self):\n        widget = ReadOnlyPasswordHashWidget()\n        password_hash = 'pbkdf2_sha256$30000$Vo0VlMnkR4Bk$qEvtdyZRWTcOsCnI/oQ7fVOu1XAURIZYoOZ3iq8Dr4M='\n        html = widget.render('password', password_hash)\n        self.assertHTMLEqual(\n            html,\n            '<div><strong>algorithm</strong>: pbkdf2_sha256<br><strong>iterations</strong>: 30000</div>'\n        )\n", "    def test_constructor(self):\n        field = ReadOnlyPasswordHashField()\n        self.assertEqual(field.required, False)\n        self.assertEqual(field.disabled, True)\n", "    def test_user_creation_form(self):\n        # Test that UserCreationForm doesn't let users choose their own passwords\n        # when one is already set, and that it lets them choose a password when\n        # they don't have one.\n        form_for_user_with_password = UserCreationForm(instance=self.u1)\n        self.assertFalse(form_for_user_with_password.fields['password1'].required)\n\n        form_for_user_without_password = UserCreationForm(instance=self.u4)\n        self.assertTrue(form_for_user_without_password.fields['password1'].required)\n", "    def test_widget(self):\n        field = ReadOnlyPasswordHashField()\n        self.assertIsInstance(field.widget, ReadOnlyPasswordHashWidget)\n", "    def test_user_already_exists(self):\n        data = {\n            'username': 'testclient',\n            'password1': 'test123',\n            'password2': 'test123',\n        }\n        form = UserCreationForm(data)\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form['username'].errors, [_('A user with that username already exists.')])\n"], "sample_255": ["def test_handle_one_request(self):\n    request = self.request_factory.get('/test/')\n    handler = WSGIRequestHandler(request, Stub(), Stub())\n    handler.handle_one_request()\n    self.assertEqual(handler.close_connection, True)\n", "def test_get_environ_strips_headers_with_underscores(self):\n    request = self.request_factory.get('/', HTTP_ACCEPT_LANGUAGE='fr_FR')\n    request_handler = WSGIRequestHandler(Stub(), BytesIO(), BytesIO(), Stub())\n    request_handler.headers['HTTP_ACCEPT_LANGUAGE'] = request.META['HTTP_ACCEPT_LANGUAGE']\n    request_handler.headers['Custom_Header'] = 'Value'\n    environ = request_handler.get_environ()\n    self.assertIn('HTTP_ACCEPT_LANGUAGE', environ)\n    self.assertNotIn('Custom_Header', environ)\n", "def test_handle_one_request_close_connection(self):\n    request = self.request_factory.get('/test/')\n    handler = WSGIRequestHandler(Stub(), Stub(), Stub())\n    handler.close_connection = True\n    handler.handle_one_request()\n    self.assertTrue(handler.close_connection)\n\n    handler = WSGIRequestHandler(Stub(), Stub(), Stub())\n    handler.close_connection = False\n    handler.handle_one_request()\n    self.assertFalse(handler.close_connection)\n", "def test_get_environ(self):\n    request = self.request_factory.get('/path')\n    handler = WSGIRequestHandler(Stub(), Stub(), BytesIO())\n    handler.client_address = ('127.0.0.1', 12345)\n    handler.raw_requestline = b'GET /path HTTP/1.1'\n    handler.command = 'GET'\n    handler.path = '/path'\n    handler.request_version = 'HTTP/1.1'\n    handler.headers = {'Host': 'testserver'}\n    environ = handler.get_environ()\n    self.assertEqual(environ['wsgi.version'], (1, 0))\n    self.assertEqual(environ['wsgi.url_scheme'], 'http')\n    self.assertEqual(environ['wsgi.multithread'], False)\n    self.assertEqual(environ['wsgi.multiprocess'], True)\n    self.assertEqual(environ['wsgi.run_once'], False)\n    self.assertEqual(environ['SERVER_NAME'], 'testserver')\n    self.assertEqual(environ['SERVER_PORT'], '80')\n    self.assertEqual(environ['REQUEST_METHOD'], 'GET')\n    self.assertEqual(environ['SCRIPT_NAME'], '')\n    self.assertEqual(environ['PATH_INFO'], '/path')\n    self.assertEqual(environ['QUERY_STRING'], '')\n    self.assertEqual(environ['CONTENT_TYPE'], '')\n    self.assertEqual(environ['CONTENT_LENGTH'], '')\n    self.assertEqual(environ['HTTP_HOST'], 'testserver')\n    self.assertIsInstance(environ['wsgi.input'], BytesIO)\n    self.assertIsInstance(environ['wsgi.errors'], BytesIO)\n", "def test_get_environ(self):\n    request = self.request_factory.get('/path', HTTP_HOST='example.com')\n    handler = WSGIRequestHandler(request, Stub(), Stub())\n    handler.headers['HTTP_HOST'] = 'example.com'\n    handler.headers['HTTP_ACCEPT'] = 'text/html'\n\n    # Test that headers with underscores are stripped\n    handler.headers['HTTP_FOO_BAR'] = 'baz'\n    environ = handler.get_environ()\n\n    self.assertEqual(environ['SERVER_NAME'], 'example.com')\n    self.assertEqual(environ['HTTP_ACCEPT'], 'text/html')\n    self.assertNotIn('HTTP_FOO_BAR', environ)\n"], "sample_257": ["def test_key_transform_text_lookup_mixin(self):\n    mixin = KeyTransformTextLookupMixin()\n    key_transform = KeyTransform('key', 'value')\n    with self.assertRaises(TypeError):\n        mixin.__init__(object())\n    mixin.__init__(key_transform)\n    self.assertEqual(mixin.lhs, key_transform)\n", "def test_key_transform_nested_lookup(self):\n    obj = JSONModel.objects.create(json={'a': {'b': 'c'}})\n    self.assertEqual(JSONModel.objects.filter(json__a__b='c').count(), 1)\n\n    # Test with a subquery.\n    subquery = JSONModel.objects.filter(json__a__b=OuterRef('json__a__b'))\n    self.assertEqual(\n        JSONModel.objects.filter(json__a__b__in=Subquery(subquery.values('pk'))).count(),\n        1,\n    )\n\n    # Test with an expression.\n    expr = F('json__a__b')\n    self.assertEqual(JSONModel.objects.filter(json__a__b=expr).count(), 1)\n\n    # Test with a transform.\n    transform = KeyTransform('b', KeyTransform('a', JSONModel._meta.get_field('json')))\n    self.assertEqual(JSONModel.objects.filter(json__a__b=transform).count(), 1)\n", "def test_key_transform_text_lookup_mixin(self):\n    mixin = KeyTransformTextLookupMixin()\n    key_transform = KeyTransform('test', 'test')\n    with self.assertRaises(TypeError):\n        mixin.__init__(object())\n    mixin.__init__(key_transform)\n    self.assertEqual(mixin.lhs, key_transform)\n", "def test_key_transform_exact_lookup_with_none_value(self):\n    obj = JSONModel.objects.create(value={'key': None})\n    self.assertTrue(JSONModel.objects.filter(value__key=None).exists())\n    self.assertFalse(JSONModel.objects.filter(value__key='not none').exists())\n\n    # Test that None is treated as a null value, not the string 'null'.\n    obj.value['key'] = 'null'\n    obj.save()\n    self.assertFalse(JSONModel.objects.filter(value__key=None).exists())\n    self.assertTrue(JSONModel.objects.filter(value__key='null').exists())\n", "def test_key_transform_nested(self):\n    obj = JSONModel.objects.create(json={\"a\": {\"b\": {\"c\": \"value\"}}})\n    self.assertEqual(\n        JSONModel.objects.filter(json__a__b__c=\"value\").count(), 1\n    )\n    self.assertEqual(\n        JSONModel.objects.filter(json__a__b__contains={\"c\": \"value\"}).count(), 1\n    )\n    self.assertEqual(\n        JSONModel.objects.filter(json__a__contains={\"b\": {\"c\": \"value\"}}).count(), 1\n    )\n    self.assertEqual(\n        JSONModel.objects.filter(json__contains={\"a\": {\"b\": {\"c\": \"value\"}}}).count(), 1\n    )\n"], "sample_258": ["def test_connect_non_callable_receiver(self):\n    with self.assertRaisesMessage(ValueError, \"Signal receivers must be callable.\"):\n        a_signal.connect(\"non-callable receiver\")\n", "def test_connect_invalid_receivers(self):\n    msg = \"Signal receivers must be callable.\"\n    with self.assertRaisesMessage(ValueError, msg):\n        a_signal.connect('non-callable')\n\n    msg = \"Signal receivers must accept keyword arguments (**kwargs).\"\n    with self.assertRaisesMessage(ValueError, msg):\n        a_signal.connect(lambda x: x)\n", "def test_connect_disconnect(self):\n        pass\n\n    a_signal.connect(receiver)\n    self.assertTrue(a_signal.has_listeners())\n    self.assertEqual(len(a_signal.receivers), 1)\n\n    a_signal.disconnect(receiver)\n    self.assertFalse(a_signal.has_listeners())\n    self.assertEqual(len(a_signal.receivers), 0)\n", "def test_connect_invalid_receivers(self):\n    msg = \"Signal receivers must be callable.\"\n    with self.assertRaisesMessage(AssertionError, msg):\n        a_signal.connect(\"non-callable\")\n\n    msg = \"Signal receivers must accept keyword arguments (**kwargs).\"\n    with self.assertRaisesMessage(ValueError, msg):\n        a_signal.connect(lambda x: x)\n", "def test_connect_invalid_receivers(self):\n    msg = \"Signal receivers must be callable.\"\n    with self.assertRaisesMessage(ValueError, msg):\n        a_signal.connect(\"non-callable\")\n\n    msg = \"Signal receivers must accept keyword arguments (**kwargs).\"\n    with self.assertRaisesMessage(ValueError, msg):\n        a_signal.connect(lambda x: x)\n"], "sample_259": ["def test_prefetch_related_objects_m2m(self):\n    authors = list(Author.objects.all())\n    prefetch_related_objects(authors, 'books')\n    with self.assertNumQueries(0):\n        for author in authors:\n            self.assertEqual(len(author.books.all()), author.books.count())\n", "def test_prefetch_related_objects_with_custom_queryset(self):\n    authors = Author.objects.all()\n    prefetch_related_objects(authors, 'first_book', Book.objects.filter(title__startswith='P'))\n    with self.assertNumQueries(0):\n        for author in authors:\n            if author.first_book_id == self.book1.pk:\n                self.assertEqual(author.first_book.title, 'Poems')\n            else:\n                self.assertIsNone(author.first_book)\n", "def test_prefetch_related_objects_with_queryset(self):\n    qs = Author.objects.all()\n    prefetch_related_objects([self.book1, self.book2], 'authors', queryset=qs)\n    with self.assertNumQueries(0):\n        authors1 = list(self.book1.authors.all())\n        authors2 = list(self.book2.authors.all())\n    self.assertEqual(authors1, [self.author1, self.author2, self.author3])\n    self.assertEqual(authors2, [self.author1])\n", "def test_prefetch_related_objects_with_to_attr(self):\n    authors = list(Author.objects.all())\n    prefetch_related_objects(authors, Prefetch('books', to_attr='author_books'))\n    for author in authors:\n        with self.assertNumQueries(0):\n            self.assertEqual(hasattr(author, 'author_books'), True)\n            self.assertEqual(set(author.author_books), set(author.books.all()))\n", "def test_prefetch_related_objects_with_to_attr(self):\n    authors = list(Author.objects.all())\n    prefetch_related_objects(authors, Prefetch('books', to_attr='book_list'))\n    with self.assertNumQueries(0):\n        for author in authors:\n            self.assertEqual(hasattr(author, 'book_list'), True)\n            self.assertEqual(isinstance(author.book_list, list), True)\n            for book in author.book_list:\n                self.assertEqual(isinstance(book, Book), True)\n"], "sample_260": ["def test_alter_model_table_optimization(self):\n    operations = [\n        migrations.AlterModelTable(name='UnicodeModel', table='new_table_name'),\n        migrations.AlterModelTable(name='UnicodeModel', table='new_table_name_2'),\n    ]\n    expected = [\n        migrations.AlterModelTable(name='UnicodeModel', table='new_table_name_2'),\n    ]\n    self.assertOptimizesTo(operations, expected, exact=1)\n", "def test_optimizer_handles_model_rename_and_field_removal(self):\n    operations = [\n        migrations.RenameModel(\"OldModel\", \"NewModel\"),\n        migrations.RemoveField(\"NewModel\", \"field_name\"),\n    ]\n    expected = [\n        migrations.CreateModel(\n            name=\"NewModel\",\n            fields=[],\n            options={},\n        ),\n        migrations.DeleteModel(\"OldModel\"),\n    ]\n    self.assertOptimizesTo(operations, expected)\n", "def test_optimizer_remove_add_field(self):\n    operations = [\n        migrations.RemoveField(\n            model_name='unicodemodel',\n            name='charfield',\n        ),\n        migrations.AddField(\n            model_name='unicodemodel',\n            name='charfield',\n            field=models.CharField(max_length=100),\n        ),\n    ]\n    self.assertOptimizesTo(operations, [])\n", "def test_remove_index(self):\n    operations = [\n        migrations.AddIndex(\n            model_name='unicode',\n            index=models.Index(fields=['charfield'], name='my_index'),\n        ),\n        migrations.RemoveIndex(\n            model_name='unicode',\n            name='my_index',\n        ),\n    ]\n    self.assertOptimizesTo(operations, [], exact=1)\n", "def test_alter_model_options(self):\n    operations = [\n        migrations.AlterModelOptions(\n            name='UnicodeModel',\n            options={'verbose_name': 'New verbose name'},\n        ),\n        migrations.AlterModelOptions(\n            name='UnicodeModel',\n            options={'verbose_name': 'Even newer verbose name'},\n        ),\n    ]\n    expected = [\n        migrations.AlterModelOptions(\n            name='UnicodeModel',\n            options={'verbose_name': 'Even newer verbose name'},\n        ),\n    ]\n    self.assertOptimizesTo(operations, expected)\n"], "sample_261": ["def test_parse_date_valid_date(self):\n    self.assertEqual(parse_date('2022-07-25'), date(2022, 7, 25))\n", "def test_parse_date(self):\n    self.assertEqual(parse_date('2022-02-28'), date(2022, 2, 28))\n    self.assertIsNone(parse_date('2022-02'))  # incomplete date\n    self.assertIsNone(parse_date('abcdef'))  # invalid format\n    with self.assertRaises(ValueError):\n        parse_date('2022-02-30')  # invalid date (Feb only has 28/29 days)\n", "def test_parse_date(self):\n    self.assertEqual(parse_date('2022-02-28'), date(2022, 2, 28))\n    self.assertIsNone(parse_date('2022-02-30'))  # invalid day\n    self.assertIsNone(parse_date('2022-13-01'))  # invalid month\n    self.assertIsNone(parse_date('2022-02'))  # missing day\n    self.assertIsNone(parse_date('20220228'))  # no dashes\n", "def test_parse_date_valid(self):\n    self.assertEqual(parse_date('2012-02-21'), date(2012, 2, 21))\n    self.assertEqual(parse_date('2020-12-31'), date(2020, 12, 31))\n    self.assertEqual(parse_date('1999-01-01'), date(1999, 1, 1))\n", "def test_parse_date_valid(self):\n    self.assertEqual(parse_date('2022-01-01'), date(2022, 1, 1))\n    self.assertEqual(parse_date('1999-12-31'), date(1999, 12, 31))\n"], "sample_262": ["def test_cached_property(self):\n    class Test:\n        @cached_property\n            return 42\n\n    t = Test()\n    self.assertEqual(t.value, 42)\n    # Subsequent access comes from instance dict.\n    self.assertEqual(t.__dict__['value'], 42)\n    with mock.patch.object(Test, 'value', return_value=13) as f:\n        self.assertEqual(f(t), 13)\n    # Value is still cached even after patching.\n    self.assertEqual(t.value, 42)\n", "def test_lazy(self):\n        return x + y\n\n    lazy_func = lazy(my_function, int)\n    self.assertIsInstance(lazy_func(1, 2), int)\n    self.assertEqual(lazy_func(1, 2), 3)\n\n    # Test that the function is only evaluated once\n    func_mock = mock.Mock(side_effect=my_function)\n    lazy_func = lazy(func_mock, int)\n    self.assertEqual(lazy_func(1, 2), 3)\n    self.assertEqual(func_mock.call_count, 1)\n    self.assertEqual(lazy_func(1, 2), 3)\n    self.assertEqual(func_mock.call_count, 2)\n", "def test_lazy_object_eq(self):\n    class TestObject:\n            self.value = value\n\n            return self.value == other.value\n\n    lazy_obj1 = SimpleLazyObject(lambda: TestObject(42))\n    lazy_obj2 = SimpleLazyObject(lambda: TestObject(42))\n    lazy_obj3 = SimpleLazyObject(lambda: TestObject(24))\n\n    self.assertEqual(lazy_obj1, lazy_obj2)\n    self.assertNotEqual(lazy_obj1, lazy_obj3)\n", "def test_cached_property(self):\n    class TestClass:\n        @cached_property\n            return 'cached_value'\n\n    obj = TestClass()\n    self.assertEqual(obj.cached_attr, 'cached_value')\n    # Subsequent access should return the cached value, not re-call the method\n    with mock.patch.object(TestClass, 'cached_attr', wraps=TestClass.cached_attr) as mock_cached_attr:\n        self.assertEqual(obj.cached_attr, 'cached_value')\n        mock_cached_attr.fget.assert_not_called()\n", "def test_cached_property(self):\n    class TestClass:\n        @cached_property\n            return 'cached value'\n\n    obj = TestClass()\n    self.assertEqual(obj.cached_attr, 'cached value')\n    # Subsequent access should return the cached value.\n    self.assertEqual(obj.cached_attr, 'cached value')\n\n    # The cached value should be stored in the object's __dict__.\n    self.assertIn('cached_attr', obj.__dict__)\n"], "sample_263": ["    def test_dumpdata_with_proxy_model(self):\n        out = StringIO()\n        with warnings.catch_warnings(record=True) as w:\n            management.call_command('dumpdata', 'app.ProxySpy', stdout=out)\n            self.assertEqual(len(w), 1)\n            self.assertIsInstance(w[0].message, ProxyModelWarning)\n", "    def test_dumpdata_command_with_primary_keys(self):\n        # Create some objects to dump.\n        article1 = Article.objects.create(title='Article 1')\n        article2 = Article.objects.create(title='Article 2')\n\n        # Run the dumpdata command with primary keys.\n        output = StringIO()\n        management.call_command('dumpdata', 'fixtures.Article', '--pks=%s,%s' % (article1.pk, article2.pk), stdout=output)\n\n        # Check that only the specified objects are dumped.\n        self.assertIn(str(article1.pk), output.getvalue())\n        self.assertIn(str(article2.pk), output.getvalue())\n\n        # Check that no other objects are dumped.\n        self.assertEqual(output.getvalue().count('pk'), 2)\n", "    def test_dumpdata_format(self):\n        with NamedTemporaryFile(mode='w', delete=False) as f:\n            management.call_command('dumpdata', format='json', stdout=f)\n            with open(f.name, 'r') as f_read:\n                self.assertEqual(f_read.read().startswith('['), True)\n", "    def test_dumpdata_uses_natural_keys(self):\n        NaturalKeyThing.objects.create(name='test')\n        with NamedTemporaryFile(mode='w', delete=False) as f:\n            management.call_command('dumpdata', 'naturalkeyapp.NaturalKeyThing', stdout=f)\n        with open(f.name, 'r') as f:\n            self.assertIn('\"name\": \"test\"', f.read())\n        os.remove(f.name)\n", "    def test_dumpdata_proxy_model(self):\n        out = StringIO()\n        with mock.patch('sys.stdout', out):\n            management.call_command('dumpdata', 'fixtures.ProxySpy')\n        self.assertIn(\"ProxySpy is a proxy model and won't be serialized.\", out.getvalue())\n"], "sample_264": ["def test_cookie_storage_legacy_hash(self):\n    # Test that the legacy hash is correctly generated and validated.\n    storage = self.storage_class(FakeRequest())\n    messages = [Message(constants.INFO, 'message')]\n    encoded_data = storage._encode(messages)\n    bits = encoded_data.split('$', 1)\n    self.assertEqual(len(bits), 2)\n    hash_, value = bits\n    self.assertTrue(constant_time_compare(hash_, storage._legacy_hash(value)))\n", "def test_legacy_hash(self):\n    storage = self.storage_class(FakeRequest())\n    value = 'somevalue'\n    with ignore_warnings(category=RemovedInDjango40Warning):\n        self.assertEqual(storage._legacy_hash(value), salted_hmac('django.contrib.messages', value).hexdigest())\n", "def test_legacy_hash(self):\n    # RemovedInDjango40Warning: pre-Django 3.1 hashes will be invalid.\n    value = get_random_string(12)\n    storage = self.storage_class(FakeRequest())\n    hash_ = storage._legacy_hash(value)\n    self.assertEqual(len(hash_), 40)  # SHA-1 hash length\n\n    # Test that the hash is valid.\n    self.assertTrue(storage._legacy_decode(f'{hash_}${value}'))\n\n    # Test that the hash is not valid when the value changes.\n    self.assertIsNone(storage._legacy_decode(f'{hash_}${value + \"a\"}'))\n\n    # Test that the hash is not valid when the hash itself changes.\n    self.assertIsNone(storage._legacy_decode(f'{hash_[:-1]}${value}'))\n", "def test_cookie_storage_max_cookie_size(self):\n    storage = self.storage_class(FakeRequest())\n    response = FakeResponse()\n\n    # Set max_cookie_size to a small value for testing purposes.\n    storage.max_cookie_size = 50\n\n    # Create a list of messages that will exceed the max cookie size.\n    messages = [Message(constants.INFO, 'message %s' % i) for i in range(10)]\n\n    # Try to store the messages.\n    unstored_messages = storage._store(messages, response)\n\n    # Check that some messages were not stored due to the max cookie size.\n    self.assertGreater(len(unstored_messages), 0)\n    self.assertLessEqual(stored_cookie_messages_count(storage, response), len(messages) - len(unstored_messages))\n", "def test_message_encoder_decoder(self):\n    messages = [\n        Message(constants.INFO, 'Test message'),\n        Message(constants.ERROR, mark_safe('Error message'), 'extra_tag'),\n    ]\n    encoder = MessageEncoder()\n    encoded_messages = encoder.encode(messages)\n    decoder = MessageDecoder()\n    decoded_messages = decoder.decode(encoded_messages)\n\n    self.assertEqual(len(messages), len(decoded_messages))\n    for original, decoded in zip(messages, decoded_messages):\n        self.assertEqual(original.level, decoded.level)\n        self.assertEqual(original.message, decoded.message)\n        self.assertEqual(original.extra_tags, decoded.extra_tags)\n"], "sample_265": ["def test_get_installed_libraries(self):\n    libraries = get_installed_libraries()\n    self.assertIsInstance(libraries, dict)\n    for name, module in libraries.items():\n        self.assertIsInstance(name, str)\n        self.assertIsInstance(module, str)\n\n    # Test that a library can be added and is loaded correctly\n    with override_settings(INSTALLED_APPS=['test.templatetags.testlib']):\n        libraries = get_installed_libraries()\n        self.assertIn('testlib', libraries)\n\n    # Test that an exception is raised if a library has no 'register' attribute\n    with self.assertRaises(InvalidTemplateLibrary):\n        get_package_libraries(import_module('test.templatetags.invalidlib'))\n", "def test_get_templatetag_libraries(self):\n    # Test that get_templatetag_libraries returns a dictionary of template tag libraries\n    # from installed applications and the supplied custom_libraries argument.\n    engine = self.engine_class({\n        'DIRS': [],\n        'APP_DIRS': True,\n        'OPTIONS': {\n            'libraries': {'my_tags': 'path.to.my.tags'}\n        }\n    })\n    libraries = engine.get_templatetag_libraries({'custom_tags': 'path.to.custom.tags'})\n    self.assertIsInstance(libraries, dict)\n    self.assertIn('my_tags', libraries)\n    self.assertIn('custom_tags', libraries)\n\n    # Test that an InvalidTemplateLibrary is raised when a library cannot be imported\n    with override_settings(INSTALLED_APPS=['template_tests.test_invalid_templatetag_app']):\n        with self.assertRaises(InvalidTemplateLibrary):\n            engine.get_templatetag_libraries({})\n", "def test_get_templatetag_libraries(self):\n    # Test that get_templatetag_libraries returns a dictionary of libraries\n    # from installed applications and the supplied custom_libraries argument.\n    custom_libraries = {'my_library': 'path.to.my.library'}\n    backend = self.engine_class({\n        'DIRS': [],\n        'APP_DIRS': True,\n        'OPTIONS': {\n            'libraries': custom_libraries,\n        },\n    })\n    libraries = backend.get_templatetag_libraries(custom_libraries)\n    self.assertDictContainsSubset(custom_libraries, libraries)\n\n    # Test that an exception is raised if a library has an invalid register attribute.\n    with self.assertRaises(InvalidTemplateLibrary):\n        get_package_libraries(import_module('django.templatetags.invalid'))\n", "def test_get_templatetag_libraries(self):\n    custom_libraries = {\n        'my_tags': 'path.to.my_tags',\n    }\n    expected_libraries = get_installed_libraries()\n    expected_libraries.update(custom_libraries)\n\n    params = {'OPTIONS': {'libraries': custom_libraries}}\n    engine = self.engine_class(params)\n    libraries = engine.get_templatetag_libraries(custom_libraries)\n\n    self.assertEqual(libraries, expected_libraries)\n", "def test_get_installed_libraries(self):\n    # Ensure that the built-in template tag libraries are included\n    libraries = get_installed_libraries()\n    self.assertIn('i18n', libraries)\n    self.assertIn('static', libraries)\n\n    # Ensure that libraries from installed apps are included\n    with override_settings(INSTALLED_APPS=['template_tests.test_app']):\n        libraries = get_installed_libraries()\n        self.assertIn('template_tests.test_app.templatetags.test_tags', libraries)\n"], "sample_266": ["    def test_load_disk(self):\n        \"\"\"\n        load_disk() should load all migrations from disk, and ignore\n        replaced migrations.\n        \"\"\"\n        loader = MigrationLoader(connection)\n        loader.load_disk()\n        self.assertIn(('migrations', '0001_initial'), loader.disk_migrations)\n        self.assertNotIn(('migrations', '0002_second'), loader.disk_migrations)\n", "    def test_load(self):\n        \"\"\"\n        Makes sure the loader can load the migrations for an app from the \"migrations\"\n        module, and that it can load them even if they're in a \"migrations\" package\n        in an app with a dotted name.\n        \"\"\"\n        loader = MigrationLoader(connection)\n        self.assertEqual(loader.load_disk()[('testapp', '0001_initial')].name, '0001_initial')\n", "    def test_load(self):\n        \"\"\"\n        Makes sure the loader can load the migrations for an app from the disk.\n        \"\"\"\n        loader = MigrationLoader(connection)\n        self.addCleanup(loader.build_graph)\n        self.assertEqual(loader.disk_migrations, {\n            ('migrations', '0001_initial'): loader.get_migration('migrations', '0001_initial'),\n            ('migrations', '0002_second'): loader.get_migration('migrations', '0002_second'),\n        })\n", "    def test_load_disk(self):\n        \"\"\"\n        load_disk() should load all migrations from disk, and ignore the ones\n        not ending with '.py'.\n        \"\"\"\n        loader = MigrationLoader(connection)\n        loader.load_disk()\n        self.assertIn(('test_utils', '0001_initial'), loader.disk_migrations)\n        self.assertNotIn(('test_utils', 'not_a_migration'), loader.disk_migrations)\n", "    def test_load_migrations(self):\n        \"\"\"\n        load_disk() should load all migrations from disk, and they should be\n        correctly ordered.\n        \"\"\"\n        loader = MigrationLoader(connection)\n        self.addCleanup(loader.build_graph)  # Make sure we clean up the caches\n        loader.load_disk()\n        self.assertIn(('app_with_custom_name', '0001_initial'), loader.disk_migrations)\n        self.assertIn(('app_with_custom_name', '0002_second'), loader.disk_migrations)\n"], "sample_267": ["def test_sqlite_wrapper_thread_safety(self):\n    # Create a new SQLite connection\n    conn = dbapi2.connect(':memory:')\n    # Create a cursor object from the connection\n    cursor = conn.cursor()\n\n        # Execute a query using the cursor\n        cursor.execute('SELECT 1')\n\n    # Create multiple threads that attempt to execute queries concurrently\n    threads = [threading.Thread(target=execute_query) for _ in range(10)]\n    for thread in threads:\n        thread.start()\n    for thread in threads:\n        thread.join()\n\n    # Verify that no exceptions were raised during concurrent execution\n    self.assertEqual(cursor.fetchone(), (1,))\n", "def test_sqlite_database_wrapper_init(self):\n    wrapper = connection.__class__({})\n    self.assertIsNone(wrapper.client_class())\n    self.assertIsInstance(wrapper.creation, connection.creation_class)\n    self.assertIsInstance(wrapper.features, connection.features_class)\n    self.assertIsInstance(wrapper.introspection, connection.introspection_class)\n    self.assertIsInstance(wrapper.ops, connection.ops_class)\n    self.assertIsInstance(wrapper.schema_editor(), connection.SchemaEditorClass)\n", "def test_database_wrapper(self):\n    # Ensure we can create a DatabaseWrapper object without any issues.\n    db = connection.__class__({\n        'ENGINE': 'django.db.backends.sqlite3',\n        'NAME': ':memory:',\n    })\n    self.assertIsInstance(db, connection.__class__)\n\n    # Also check with a file-based database.\n    with tempfile.NamedTemporaryFile():\n        db = connection.__class__({\n            'ENGINE': 'django.db.backends.sqlite3',\n            'NAME': Path(tempfile.gettempdir()) / 'test.db',\n        })\n        self.assertIsInstance(db, connection.__class__)\n", "def test_sqlite_wrapper_thread_safety(self):\n    # SQLite's own connections are not thread-safe, but our wrapper should be.\n    with tempfile.NamedTemporaryFile(suffix='.db') as db_file:\n        settings = {\n            'ENGINE': 'django.db.backends.sqlite3',\n            'NAME': db_file.name,\n        }\n        handler = ConnectionHandler(settings)\n        connection = handler[settings['NAME']]\n        with connection.cursor() as cursor:\n            cursor.execute('CREATE TABLE foo (id INTEGER PRIMARY KEY)')\n\n            with connection.cursor() as cursor:\n                cursor.execute('SELECT * FROM foo')\n                return cursor.fetchall()\n\n        thread = threading.Thread(target=run_query)\n        thread.start()\n        thread.join()\n\n        # If the connection wasn't properly synchronized, this will raise an error.\n        with connection.cursor() as cursor:\n            cursor.execute('SELECT * FROM foo')\n            self.assertEqual(cursor.fetchall(), [])\n", "def test_sqlite_database_wrapper_init_connection_state(self):\n    # SQLite doesn't require any specific connection state initialization.\n    # This test is here to ensure that init_connection_state() doesn't do anything\n    # and thus doesn't raise any errors when called.\n    connection.init_connection_state()\n    self.assertIsNone(connection.connection)\n"], "sample_268": ["def test_iter_modules_and_files_with_zip_file(self):\n    with tempfile.TemporaryDirectory() as tmpdir:\n        with zipfile.ZipFile(Path(tmpdir) / 'test.zip', 'w') as zip_file:\n            zip_file.writestr('module.py', b'')\n        with extend_sys_path(tmpdir):\n            self.import_and_cleanup('module')\n            self.assertFileFound(Path(tmpdir) / 'test.zip')\n", "    def test_common_roots(self):\n        paths = [\n            Path('/path/to/file1.py'),\n            Path('/path/to/file2.py'),\n            Path('/path/to/subdir/file3.py'),\n        ]\n        roots = autoreload.common_roots(paths)\n        self.assertEqual(roots, (Path('/path/to'),))\n", "    def test_common_roots(self):\n        paths = [\n            Path('/home/user/project/app/models.py'),\n            Path('/home/user/project/app/views.py'),\n            Path('/home/user/project/other_app/models.py'),\n            Path('/home/user/project/other_app/views.py'),\n        ]\n        roots = autoreload.common_roots(paths)\n        self.assertEqual(roots, (Path('/home/user/project'),))\n", "    def test_no_common_roots(self):\n        paths = [Path('/path/to/file1.py'), Path('/other/path/to/file2.py')]\n        self.assertEqual(autoreload.common_roots(paths), ())\n", "def test_iter_modules_and_files_with_zip_file(self):\n    with tempfile.NamedTemporaryFile(suffix='.zip') as tmp_file:\n        with zipfile.ZipFile(tmp_file.name, 'w') as zip_file:\n            zip_file.writestr('module1.py', '')\n            zip_file.writestr('module2.py', '')\n\n        sys.path.insert(0, tmp_file.name)\n        self.addCleanup(lambda: sys.path.remove(tmp_file.name))\n\n        self.import_and_cleanup('module1')\n        self.import_and_cleanup('module2')\n\n        self.clear_autoreload_caches()\n        self.assertEqual(len(list(autoreload.iter_all_python_module_files())), 2)\n        self.clear_autoreload_caches()\n        self.assertEqual(len(list(autoreload.iter_all_python_module_files())), 2)\n\n        # Test that the cache is invalidated when a new module is imported.\n        self.import_and_cleanup('module3')\n        self.clear_autoreload_caches()\n        self.assertEqual(len(list(autoreload.iter_all_python_module_files())), 3)\n"], "sample_269": ["    def test_catalog(self):\n        catalog = JavaScriptCatalog()\n        catalog.translation = gettext.NullTranslations()\n        catalog.translation._catalog = {'': 'Metadata', 'hello': 'Hello'}\n        catalog.translation._fallback = None\n        self.assertEqual(catalog.get_catalog(), {'hello': 'Hello'})\n", "    def test_catalog(self):\n        catalog = JavaScriptCatalog()\n        catalog.translation = gettext.NullTranslations()\n        context = catalog.get_context_data()\n        self.assertIn('catalog', context)\n        self.assertIn('formats', context)\n        self.assertIn('plural', context)\n", "    def test_catalog(self):\n        catalog = JavaScriptCatalog()\n        catalog.translation = gettext.NullTranslations()\n        context = catalog.get_context_data()\n        self.assertIn('catalog', context)\n        self.assertIn('formats', context)\n        self.assertIn('plural', context)\n", "    def test_get_plural(self):\n        catalog = JavaScriptCatalog()\n        catalog._plural_string = 'nplurals=3; plural=n%10==1 && n%100!=11 ? 0 : n%10>=2 && n%10<=4 && (n%100<10 || n%100>=20) ? 1 : 2;'\n        self.assertEqual(catalog.get_plural(), 'n%10==1 && n%100!=11 ? 0 : n%10>=2 && n%10<=4 && (n%100<10 || n%100>=20) ? 1 : 2')\n", "    def test_catalog(self):\n        catalog = JavaScriptCatalog()\n        catalog.translation = gettext.NullTranslations()\n        catalog.locale = 'en'\n        context = catalog.get_context_data()\n        self.assertEqual(context['catalog'], {})\n        self.assertEqual(context['formats'], get_formats())\n        self.assertIsNone(context['plural'])\n"], "sample_270": ["    def test_model_str_repr(self):\n        class MyModel(models.Model):\n            pass\n\n        self.assertEqual(str(MyModel()), 'MyModel object (None)')\n        self.assertEqual(repr(MyModel()), '<MyModel: MyModel object (None)>')\n\n        class CustomStrModel(models.Model):\n                return 'Custom str'\n\n        self.assertEqual(str(CustomStrModel()), 'Custom str')\n        self.assertEqual(repr(CustomStrModel()), '<CustomStrModel: Custom str>')\n", "    def test_unique_for_date_year_month(self):\n        class Model(models.Model):\n            title = models.CharField(max_length=255)\n            date_field = models.DateField(unique_for_date='title', unique_for_year='title', unique_for_month='title')\n\n        errors = Model.check()\n        self.assertEqual(len(errors), 1)\n        self.assertIsInstance(errors[0], Error)\n        self.assertEqual(errors[0].id, 'models.E017')\n", "    def test_model_name_db_lookup_clashes(self):\n        class MyModel(models.Model):\n            pass\n\n        with self.assertRaisesMessage(\n            Error,\n            \"The model name 'My_Model' cannot start or end with an underscore \"\n            \"as it collides with the query lookup syntax.\",\n        ):\n            MyModel._check_model_name_db_lookup_clashes()\n\n        class MyModel(models.Model):\n            class Meta:\n                db_table = 'my__model'\n\n        with self.assertRaisesMessage(\n            Error,\n            \"The model name 'My__Model' cannot contain double underscores as \"\n            \"it collides with the query lookup syntax.\",\n        ):\n            MyModel._check_model_name_db_lookup_clashes()\n", "    def test_field_name_clashes(self):\n        class Model(models.Model):\n            foo = models.CharField(max_length=10)\n            clean = models.CharField(max_length=10)\n\n        errors = Model.check()\n        expected_errors = [\n            Error(\n                \"The field 'clean' clashes with the model method's attribute \"\n                \"'clean'.\",\n                obj=Model.clean.field,\n                id='models.E020',\n            ),\n        ]\n        self.assertEqual(errors, expected_errors)\n", "    def test_unique_error_message(self):\n        class UniqueModel(models.Model):\n            unique_field = models.CharField(max_length=10, unique=True)\n\n            class Meta:\n                app_label = 'invalid_models_tests'\n\n        error = UniqueModel._perform_unique_checks([(UniqueModel, ('unique_field',))])\n        self.assertEqual(len(error), 1)\n        self.assertEqual(list(error.keys()), ['unique_field'])\n        self.assertEqual(len(error['unique_field']), 1)\n        self.assertIsInstance(error['unique_field'][0], Error)\n        self.assertEqual(\n            error['unique_field'][0].args[0],\n            'Model %s with field %s %s already exists.' % (\n                UniqueModel.__name__, 'unique_field', 'unique_field',\n            ),\n        )\n"], "sample_271": ["def test_iter_modules_and_files_zipped_module(self):\n    with extend_sys_path():\n        zipped_file = self.temporary_file('zipped_module.zip')\n        with zipfile.ZipFile(zipped_file, 'w') as zip_file:\n            zip_file.writestr('zipped_module/__init__.py', '')\n            zip_file.writestr('zipped_module/zipped.py', 'print(\"Hello from zipped\")')\n        sys.path.insert(0, str(zipped_file))\n        self.import_and_cleanup('zipped_module.zipped')\n        self.assertFileFound(zipped_file)\n", "    def test_common_roots(self):\n        paths = [Path('/path/to/file1.py'), Path('/path/to/file2.py'), Path('/path/to/subdir/file3.py')]\n        common_roots = autoreload.common_roots(paths)\n        self.assertEqual(common_roots, (Path('/path/to'),))\n", "def test_iter_modules_and_files_with_zip_file(self):\n    tmp_dir = tempfile.mkdtemp()\n    self.addCleanup(shutil.rmtree, tmp_dir)\n\n    # Create a zip file containing a Python module.\n    zip_filename = Path(tmp_dir) / 'myapp.zip'\n    with zipfile.ZipFile(zip_filename, 'w') as zip_file:\n        zip_file.writestr('myapp/__init__.py', '')\n        zip_file.writestr('myapp/my_module.py', 'x = 1')\n\n    # Add the zip file to sys.path.\n    extend_sys_path(str(zip_filename.parent))\n\n    try:\n        self.import_and_cleanup('myapp.my_module')\n        self.assertFileFound(zip_filename)\n    finally:\n        sys.path.remove(str(zip_filename.parent))\n", "    def test_common_roots(self):\n        paths = [\n            Path('/home/user/project/src/app1/models.py'),\n            Path('/home/user/project/src/app2/models.py'),\n            Path('/home/user/project/templates/base.html'),\n        ]\n        common_roots = autoreload.common_roots(paths)\n        self.assertEqual(common_roots, (Path('/home/user/project'),))\n", "    def test_zip_imports(self):\n        # Create a temporary directory with a zip file containing a Python module.\n        tmp_dir = tempfile.mkdtemp()\n        self.addCleanup(shutil.rmtree, tmp_dir)\n        zip_file_path = Path(tmp_dir) / 'test.zip'\n        with zipfile.ZipFile(zip_file_path, 'w') as zip_file:\n            zip_file.writestr('test_module.py', b'def test_func(): pass')\n        with extend_sys_path(tmp_dir):\n            import_module('test_module')\n            self.clear_autoreload_caches()\n            self.assertIn(zip_file_path.resolve(strict=True), list(autoreload.iter_all_python_module_files()))\n"], "sample_272": ["def test_migration_plan_with_mixed_plan(self):\n    \"\"\"\n    Tests that the migration executor raises an error when a mixed plan\n    (both forwards and backwards migrations) is provided.\n    \"\"\"\n    recorder = MigrationRecorder(connection)\n    loader = migrations.loader.MigrationLoader(connection, load=False)\n    graph = loader.graph\n    executor = MigrationExecutor(connection)\n\n    # Create a mixed plan with both forwards and backwards migrations\n    plan = [\n        (loader.get_migration(\"migrations\", \"0001_initial\"), False),\n        (loader.get_migration(\"migrations\", \"0002_second\"), True),\n    ]\n\n    with self.assertRaises(InvalidMigrationPlan):\n        executor.migration_plan(targets=[\"migrations.0002_second\"], clean_start=False, plan=plan)\n", "def test_migration_plan(self):\n    executor = MigrationExecutor(connection)\n    targets = [(\"migrations\", \"0001_initial\")]\n    plan = executor.migration_plan(targets)\n    self.assertEqual(len(plan), 1)\n    self.assertEqual(plan[0][0].app_label, \"migrations\")\n    self.assertEqual(plan[0][0].name, \"0001_initial\")\n    self.assertFalse(plan[0][1])  # forwards\n\n    targets = [(\"migrations\", None)]\n    plan = executor.migration_plan(targets)\n    self.assertEqual(len(plan), 2)\n    self.assertEqual(plan[0][0].app_label, \"migrations\")\n    self.assertEqual(plan[0][0].name, \"0002_second\")\n    self.assertTrue(plan[0][1])  # backwards\n    self.assertEqual(plan[1][0].app_label, \"migrations\")\n    self.assertEqual(plan[1][0].name, \"0001_initial\")\n    self.assertTrue(plan[1][1])  # backwards\n", "def test_migration_plan_with_replacements(self):\n    \"\"\"\n    Test that migration plan correctly handles replacement migrations.\n    \"\"\"\n    executor = MigrationExecutor(connection)\n    recorder = MigrationRecorder(connection)\n\n    # Create a fake migration history with replacements\n    replacements = {\n        ('app1', '0002_second'): [\n            ('app1', '0001_initial'),\n            ('app1', '0003_third'),\n        ]\n    }\n    graph = MigrationGraph()\n    graph.add_node(('app1', '0001_initial'))\n    graph.add_node(('app1', '0002_second'))\n    graph.add_node(('app1', '0003_third'))\n    graph.add_dependency(('app1', '0002_second'), ('app1', '0001_initial'))\n    graph.add_dependency(('app1', '0003_third'), ('app1', '0002_second'))\n\n    with mock.patch.object(executor.loader, 'graph', graph), \\\n            mock.patch.object(executor.loader, 'replacements', replacements), \\\n            mock.patch.object(recorder, 'applied_migrations', return_value=[('app1', '0001_initial')]):\n        targets = [('app1', '0003_third')]\n        plan = executor.migration_plan(targets)\n\n    self.assertEqual(plan, [\n        (executor.loader.graph.nodes[('app1', '0002_second')], False),\n        (executor.loader.graph.nodes[('app1', '0003_third')], False),\n    ])\n", "def test_migration_plan_with_clean_start(self):\n    \"\"\"\n    Test migration plan with clean start.\n    \"\"\"\n    executor = MigrationExecutor(connection)\n    targets = [('migrations', '0001_initial')]\n    plan = executor.migration_plan(targets, clean_start=True)\n    self.assertEqual(len(plan), 1)\n    self.assertEqual(plan[0][0].app_label, 'migrations')\n    self.assertEqual(plan[0][0].name, '0001_initial')\n    self.assertFalse(plan[0][1])\n", "def test_migration_plan_with_replaced_migrations(self):\n    \"\"\"\n    Tests the migration plan with replaced migrations.\n    \"\"\"\n    # Create a mock migration graph with a replaced migration\n    graph = MigrationGraph()\n    graph.add_node((\"migrations\", \"0001_initial\"), None)\n    graph.add_node((\"migrations\", \"0002_second\"), [(\"migrations\", \"0001_initial\")])\n    graph.add_node((\"migrations\", \"0003_third\"), [(\"migrations\", \"0002_second\")])\n    graph.add_replacement((\"migrations\", \"0004_replacement\"), [(\"migrations\", \"0002_second\")])\n\n    # Create a mock migration recorder with applied migrations\n    recorder = MigrationRecorder(connection)\n    recorder.record_applied(\"migrations\", \"0001_initial\")\n    recorder.record_applied(\"migrations\", \"0002_second\")\n\n    # Create a migration executor\n    executor = MigrationExecutor(connection, progress_callback=None)\n\n    # Get the migration plan\n    plan = executor.migration_plan([(\"migrations\", \"0004_replacement\")])\n\n    # Assert that the replaced migration is not in the plan\n    self.assertNotIn((\"migrations\", \"0002_second\"), [m[0] for m in plan])\n    self.assertIn((\"migrations\", \"0004_replacement\"), [m[0] for m in plan])\n"], "sample_273": ["    def test_model_str_representation(self):\n        class MyModel(models.Model):\n            name = models.CharField(max_length=255)\n\n        model = MyModel(name='Test Model')\n        self.assertEqual(str(model), 'MyModel object (None)')\n", "    def test_model_checkAdded_with_app_label(self):\n        class Model(models.Model):\n            pass\n\n        model = Model()\n        model._meta.app_label = 'app_label'\n        errors = model.check()\n        self.assertEqual(errors, [])\n", "    def test_model_check_added_fields(self):\n        class TestModel(models.Model):\n            name = models.CharField(max_length=100)\n\n            class Meta:\n                app_label = 'check_framework'\n\n        errors = TestModel.check()\n        self.assertEqual(errors, [])\n\n        # Add a new field to the model\n        TestModel.new_field = models.CharField(max_length=100)\n\n        errors = TestModel.check()\n        self.assertEqual(len(errors), 1)\n        self.assertIsInstance(errors[0], Error)\n        self.assertEqual(errors[0].id, 'models.E006')\n", "    def test_model_with_abstract_and_proxy_raises_error(self):\n        class Model(models.Model):\n            pass\n\n        Model.Meta.abstract = True\n        Model.Meta.proxy = True\n\n        self.assertEqual(\n            checks.Error(\n                \"Model cannot be both abstract and proxy.\",\n                id='models.E021',\n                obj=Model,\n            ),\n            Model.check()[0],\n        )\n", "    def test_deferred_model_fields(self):\n        class MyModel(models.Model):\n            my_field = models.CharField(max_length=10)\n\n                super().__init__(*args, **kwargs)\n                self.my_field = models.Deferred()\n\n        model = MyModel()\n        errors = model.check()\n        self.assertEqual(errors, [])\n"], "sample_274": ["    def test_model_choice_field(self):\n        class ModelChoiceForm(Form):\n            category = ModelChoiceField(queryset=ChoiceModel.objects.all())\n\n        form = ModelChoiceForm()\n        self.assertEqual(list(form.fields['category'].choices), [(c.pk, str(c)) for c in ChoiceModel.objects.all()])\n\n        # Test ModelChoiceField with queryset of different model\n        class OtherModelChoiceForm(Form):\n            category = ModelChoiceField(queryset=User.objects.all())\n\n        form = OtherModelChoiceForm()\n        self.assertEqual(list(form.fields['category'].choices), [(u.pk, str(u)) for u in User.objects.all()])\n", "    def test_model_choice_field(self):\n        class ModelChoiceForm(Form):\n            category = ModelChoiceField(ChoiceModel.objects.all())\n\n        # Instantiate the form with a valid instance\n        form = ModelChoiceForm({'category': 1})\n        self.assertTrue(form.is_valid())\n        self.assertEqual(form.cleaned_data['category'].pk, 1)\n\n        # Instantiate the form with an invalid instance\n        form = ModelChoiceForm({'category': 100})\n        self.assertFalse(form.is_valid())\n        self.assertFormErrors(['Select a valid choice. That choice is not one of the available choices.'], form.full_clean)\n", "    def test_model_choice_field(self):\n        class TestForm(Form):\n            choice = ModelChoiceField(ChoiceModel.objects.all())\n\n        form = TestForm(data={'choice': 1})\n        self.assertFalse(form.is_valid())\n        self.assertFormErrors(['Select a valid choice. That choice is not one of the available choices.'], form.clean)\n", "    def setUp(self):\n        self.choice1 = ChoiceModel.objects.create(name='Choice 1')\n        self.choice2 = ChoiceModel.objects.create(name='Choice 2')\n", "    def test_model_choice_field(self):\n        class TestForm(Form):\n            choice = ModelChoiceField(queryset=ChoiceModel.objects.all())\n\n        # Instantiate the form with a valid choice\n        form = TestForm({'choice': 1})\n        self.assertTrue(form.is_valid())\n        self.assertEqual(form.cleaned_data['choice'].pk, 1)\n\n        # Instantiate the form with an invalid choice\n        form = TestForm({'choice': 100})\n        self.assertFalse(form.is_valid())\n        self.assertFormErrors(['Select a valid choice. That choice is not one of the available choices.'], form.full_clean)\n"], "sample_275": ["    def test_queryset_iterator(self):\n        # Create some items\n        for i in range(10):\n            Item.objects.create(name='Item %s' % i)\n\n        # Use the iterator() method to fetch them in chunks of 2\n        results = list(Item.objects.iterator(chunk_size=2))\n        self.assertEqual(len(results), 10)\n", "    def test_get_on_none_queryset(self):\n        with self.assertRaises(Person.DoesNotExist):\n            Person.objects.none().get()\n", "    def test_model_iterable(self):\n        qs = Person.objects.all()\n        self.assertIsInstance(qs._iterable_class, ModelIterable)\n        self.assertEqual(list(qs), list(Person.objects.all()))\n", "def test_query_set_earliest_latest(self):\n    p1 = Person.objects.create(name='Alice', age=25)\n    p2 = Person.objects.create(name='Bob', age=30)\n    self.assertEqual(Person.objects.earliest('age'), p1)\n    self.assertEqual(Person.objects.latest('age'), p2)\n    self.assertEqual(Person.objects.earliest('-age'), p2)\n    self.assertEqual(Person.objects.latest('-age'), p1)\n", "    def test_foreign_key(self):\n        book1 = Book.objects.create(title='Book 1')\n        book2 = Book.objects.create(title='Book 2')\n        author = Person.objects.create(name='Author 1')\n        book1.author = author\n        book1.save()\n        book2.author = author\n        book2.save()\n\n        books = Book.objects.prefetch_related('author').order_by('title')\n        with self.assertNumQueries(2):\n            authors = [book.author for book in books]\n        self.assertEqual(authors, [author, author])\n"], "sample_276": ["def test_bookmarklets_view(self):\n    response = self.client.get(reverse('django-admindocs-bookmarklets'))\n    self.assertEqual(response.status_code, 200)\n    self.assertTemplateUsed(response, 'admin_doc/bookmarklets.html')\n", "    def test_bookmarklets_view(self):\n        response = self.client.get(reverse('django-admindocs-bookmarklets'))\n        self.assertEqual(response.status_code, 200)\n        self.assertTemplateUsed(response, 'admin_doc/bookmarklets.html')\n", "def test_get_return_data_type(self):\n    self.assertEqual(get_return_data_type('get_test_list'), 'List')\n    self.assertEqual(get_return_data_type('get_test_count'), 'Integer')\n    self.assertEqual(get_return_data_type('test_function'), '')\n", "def test_bookmarklets_view(self):\n    response = self.client.get(reverse('django-admindocs-bookmarklets'))\n    self.assertEqual(response.status_code, 200)\n    self.assertTemplateUsed(response, 'admin_doc/bookmarklets.html')\n", "def test_simplify_regex(self):\n    self.assertEqual(simplify_regex(r'^(?P<sport_slug>\\w+)/athletes/(?P<athlete_slug>\\w+)/$'), '/<sport_slug>/athletes/<athlete_slug>/')\n    self.assertEqual(simplify_regex(r'^athletes/(?P<athlete_slug>\\w+)/$'), '/athletes/<athlete_slug>/')\n    self.assertEqual(simplify_regex(r'^(?P<sport_slug>\\w+)/athletes/$'), '/<sport_slug>/athletes/')\n"], "sample_277": ["def test_combine_empty_q(self):\n    q1 = Q()\n    q2 = Q(field='value')\n    combined = q1 & q2\n    self.assertEqual(combined.children, [q2])\n    self.assertEqual(combined.connector, Q.AND)\n    self.assertFalse(combined.negated)\n", "def test_combine_Q_objects(self):\n    q1 = Q(a=1)\n    q2 = Q(b=2)\n    combined = q1 & q2\n    self.assertEqual(combined.connector, Q.AND)\n    self.assertEqual(len(combined.children), 2)\n    self.assertIn(q1, combined.children)\n    self.assertIn(q2, combined.children)\n\n    combined = q1 | q2\n    self.assertEqual(combined.connector, Q.OR)\n    self.assertEqual(len(combined.children), 2)\n    self.assertIn(q1, combined.children)\n    self.assertIn(q2, combined.children)\n", "def test_combine_with_empty_Q(self):\n    q1 = Q(x=1)\n    q2 = Q()\n    combined = q1 & q2\n    self.assertEqual(combined.children, [(x, 1)])\n    combined = q1 | q2\n    self.assertEqual(combined.children, [(x, 1)])\n", "def test_combine_with_empty_q(self):\n    q1 = Q(a=1)\n    q2 = Q()\n    combined = q1 & q2\n    self.assertEqual(combined.children, [q1.children[0]])\n    combined = q1 | q2\n    self.assertEqual(combined.children, [q1.children[0]])\n", "def test_combine_with_empty_q(self):\n    q1 = Q(a=1)\n    q2 = Q()\n    combined = q1 & q2\n    self.assertEqual(combined.children, [q1])\n    combined = q1 | q2\n    self.assertEqual(combined.children, [q1])\n"], "sample_278": ["    def test_combine_with_conditional(self):\n        q1 = Q(foo='bar')\n        q2 = Q(baz='qux')\n\n        with mock.patch.object(q2, 'conditional', False):\n            msg = \"Q object is not conditional\"\n            with self.assertRaisesMessage(TypeError, msg):\n                q1 & q2\n", "def test_q_object_clone(self):\n    q = Q(name='test')\n    cloned_q = deepcopy(q)\n    self.assertEqual(q, cloned_q)\n    self.assertIsNot(q, cloned_q)\n\n    q_negated = ~Q(name='test')\n    cloned_q_negated = deepcopy(q_negated)\n    self.assertEqual(q_negated, cloned_q_negated)\n    self.assertIsNot(q_negated, cloned_q_negated)\n", "def test_q_combine(self):\n    q1 = Q(num_employees__gt=1000)\n    q2 = Q(based_in_eu=True)\n    combined_q = q1 & q2\n\n    self.assertEqual(combined_q.connector, Q.AND)\n    self.assertEqual(len(combined_q.children), 2)\n\n    # Test that combining with an empty Q() returns the original Q()\n    q3 = Q()\n    self.assertEqual((q1 & q3).connector, Q.AND)\n    self.assertEqual(len((q1 & q3).children), 1)\n\n    # Test that combining two empty Q() instances returns an empty Q()\n    q4 = Q()\n    self.assertEqual((q3 & q4).connector, Q.AND)\n    self.assertEqual(len((q3 & q4).children), 0)\n", "def test_deferred_attribute(self):\n    # Test that DeferredAttribute correctly raises an exception when trying to\n    # access a deferred field without actually loading it.\n    company = Company.objects.defer('name').get(id=self.example_inc.id)\n    with self.assertRaises(FieldError):\n        company.name\n\n    # Test that accessing a deferred field after loading it returns the correct value.\n    company.refresh_from_db(fields=['name'])\n    self.assertEqual(company.name, \"Example Inc.\")\n\n    # Test that DeferredAttribute can be pickled and unpickled.\n    deferred_attr = company._meta.get_field('name').get_deferred_value()\n    pickled_deferred_attr = pickle.dumps(deferred_attr)\n    unpickled_deferred_attr = pickle.loads(pickled_deferred_attr)\n    self.assertEqual(unpickled_deferred_attr, deferred_attr)\n", "    def test_q_combination(self):\n        q1 = Q(x=1)\n        q2 = Q(y=2)\n        combined = q1 & q2\n        self.assertEqual(combined.connector, Q.AND)\n        self.assertEqual(len(combined.children), 2)\n"], "sample_279": ["    def test_check_constraint(self):\n        # Create a model with a check constraint.\n        class TestModel(models.Model):\n            value = models.IntegerField()\n            check_constraint = models.CheckConstraint(\n                check=models.Q(value__gt=0),\n                name='test_check_constraint',\n            )\n\n            class Meta:\n                app_label = 'tests'\n\n        # Create the table.\n        with connection.schema_editor() as schema_editor:\n            schema_editor.create_model(TestModel)\n\n        # Try to insert an invalid value.\n        with self.assertRaises(IntegrityError):\n            TestModel.objects.create(value=-1)\n\n        # Try to insert a valid value.\n        TestModel.objects.create(value=1)\n\n        # Drop the table.\n        with connection.schema_editor() as schema_editor:\n            schema_editor.delete_model(TestModel)\n", "    def test_check_constraint(self):\n        # Create a model with a check constraint.\n        class CheckModel(models.Model):\n            value = models.IntegerField()\n            check = models.CheckConstraint(check=models.Q(value__gt=0), name='check_value')\n\n        # Ensure the table is created.\n        with connection.schema_editor() as schema_editor:\n            schema_editor.create_model(CheckModel)\n\n        # Try to insert an invalid value.\n        with self.assertRaises(IntegrityError):\n            CheckModel.objects.create(value=-1)\n\n        # Insert a valid value.\n        CheckModel.objects.create(value=1)\n\n        # Drop the table.\n        with connection.schema_editor() as schema_editor:\n            schema_editor.delete_model(CheckModel)\n", "    def test_check_constraint(self):\n        # Create a model with a check constraint.\n        class Product(models.Model):\n            price = models.DecimalField(max_digits=10, decimal_places=2)\n            class Meta:\n                constraints = [\n                    models.CheckConstraint(\n                        check=models.Q(price__gt=0),\n                        name='price_gt_zero',\n                    ),\n                ]\n\n        # Make sure the constraint is created.\n        with connection.schema_editor() as schema_editor:\n            schema_editor.create_model(Product)\n\n        # Try to create an instance that violates the constraint.\n        with self.assertRaises(IntegrityError):\n            Product.objects.create(price=-1)\n\n        # Make sure the constraint is enforced on existing instances.\n        product = Product.objects.create(price=1)\n        product.price = -1\n        with self.assertRaises(IntegrityError):\n            product.save()\n", "    def test_check_constraint(self):\n        # Create a model with a check constraint.\n        class Product(models.Model):\n            price = models.DecimalField(max_digits=10, decimal_places=2)\n            class Meta:\n                constraints = [\n                    models.CheckConstraint(check=models.Q(price__gt=0), name='price_gt_0'),\n                ]\n\n        # Create the table.\n        with atomic():\n            Product.objects.create(price=10)\n\n        # Ensure the constraint was created.\n        constraints = get_constraints(Product._meta.db_table)\n        self.assertIn('price_gt_0', [c['name'] for c in constraints])\n\n        # Ensure the constraint works.\n        with self.assertRaises(IntegrityError):\n            Product.objects.create(price=-1)\n", "    def test_check_constraint(self):\n        class Model(models.Model):\n            value = models.IntegerField()\n\n            class Meta:\n                constraints = [\n                    models.CheckConstraint(check=models.Q(value__gte=0), name='value_gte_0'),\n                ]\n\n        with atomic():\n            Model.objects.create(value=1)\n            with self.assertRaises(IntegrityError):\n                Model.objects.create(value=-1)\n\n        constraints = get_constraints(Model._meta.db_table)\n        self.assertIn('value_gte_0', [c['name'] for c in constraints])\n"], "sample_280": ["def test_aggregate_with_distinct(self):\n    # Ensure that an aggregate with a distinct argument works as expected.\n    count = Count('authors', distinct=True)\n    self.assertEqual(Book.objects.aggregate(count=count)['count'], 9)\n\n    # Ensure that an aggregate with a distinct argument and a filter works as expected.\n    count = Count('authors', filter=Q(rating__gt=3.5), distinct=True)\n    self.assertEqual(Book.objects.aggregate(count=count)['count'], 7)\n", "def test_aggregate_resolve_expression_with_filter(self):\n    max_price = Max('price', filter=Q(pages__gt=300))\n    resolved_max_price = max_price.resolve_expression(Book.objects.all().query)\n    self.assertIsInstance(resolved_max_price.filter, Q)\n    self.assertEqual(resolved_max_price.filter.children, [('pages__gt', 300)])\n", "def test_count_star_with_filter(self):\n    count = Book.objects.filter(rating__gt=4).aggregate(Count('*'))['*__count']\n    self.assertEqual(count, 1)\n", "def test_aggregate_resolve_expression(self):\n    # Test that resolving an aggregate expression with a filter sets the filter correctly\n    agg = Count('id', filter=Q(name='test'))\n    resolved_agg = agg.resolve_expression(None)\n    self.assertEqual(resolved_agg.filter, Q(name='test'))\n\n    # Test that resolving an aggregate expression with a default value sets the default correctly\n    agg = Count('id', default=0)\n    resolved_agg = agg.resolve_expression(None)\n    self.assertEqual(resolved_agg.default, 0)\n\n    # Test that resolving an aggregate expression with a filter and default value raises an error\n    agg = Count('id', filter=Q(name='test'), default=0)\n    with self.assertRaises(FieldError):\n        agg.resolve_expression(None)\n", "def test_aggregate_distinct_expression(self):\n    # Test that distinct works with an expression.\n    qs = Book.objects.annotate(\n        the_sum=Sum(Case(When(pages__gt=300, then='pages'), default=Value(0)), distinct=True)\n    )\n    result = qs.first().the_sum\n    expected = sum(set([b.pages if b.pages > 300 else 0 for b in Book.objects.all()]))\n    self.assertEqual(result, expected)\n"], "sample_281": ["def test_get_paginator(self):\n    request = self.factory.get(self.url, self.opts)\n    request.user = self.user\n    view = AutocompleteJsonView.as_view(**self.as_view_args)\n    response = view(request)\n    paginator = view.view_class().get_paginator(response.context_data['object_list'], PAGINATOR_SIZE)\n    self.assertEqual(paginator.count, Answer.objects.count())\n    self.assertEqual(paginator.num_pages, (Answer.objects.count() + PAGINATOR_SIZE - 1) // PAGINATOR_SIZE)\n", "def test_get_paginator(self):\n    request = self.factory.get(self.url, data=self.opts)\n    view = AutocompleteJsonView.as_view(**self.as_view_args)\n    response = view(request)\n    paginator = response.context_data['paginator']\n    self.assertEqual(paginator.num_pages, 1)\n\n    # Create enough objects to trigger pagination.\n    for _ in range(PAGINATOR_SIZE + 1):\n        Answer.objects.create(question=Question.objects.create())\n\n    response = view(request)\n    paginator = response.context_data['paginator']\n    self.assertGreater(paginator.num_pages, 1)\n", "def test_get_paginator(self):\n    request = self.factory.get(self.url, data=self.opts)\n    view = AutocompleteJsonView.as_view(**self.as_view_args)\n    response = view(request)\n    paginator = view.view_instance.get_paginator(response.context_data['object_list'], PAGINATOR_SIZE)\n    self.assertEqual(paginator.count, Answer.objects.count())\n    self.assertEqual(paginator.num_pages, (Answer.objects.count() + PAGINATOR_SIZE - 1) // PAGINATOR_SIZE)\n", "def test_get(self):\n    request = self.factory.get(self.url, data=self.opts)\n    request.user = self.user\n    response = AutocompleteJsonView.as_view()(request)\n    self.assertEqual(response.status_code, 200)\n    data = json.loads(response.content.decode('utf-8'))\n    self.assertEqual(data['results'], [])\n    self.assertFalse(data['pagination']['more'])\n", "def test_get_paginator(self):\n    request = self.factory.get(self.url, data=self.opts)\n    view = AutocompleteJsonView.as_view(**self.as_view_args)\n    response = view(request)\n    paginator = response.context_data['paginator']\n    self.assertEqual(paginator.num_pages, 1)\n    self.assertEqual(paginator.count, Answer.objects.count())\n"], "sample_282": ["def test_boundfield_widget_type(self):\n    form = ComplexFieldForm()\n    bound_field = form['field1']\n    self.assertEqual(bound_field.widget_type, 'complexmultiwidget')\n", "def test_boundfield_widget_type(self):\n    form = ComplexFieldForm()\n    bound_field = form['field1']\n    self.assertEqual(bound_field.widget_type, 'complexmultiwidget')\n", "def test_boundfield_widget_type(self):\n    form = ComplexFieldForm()\n    field1 = form['field1']\n    self.assertEqual(field1.widget_type, 'complexmultiwidget')\n", "def test_boundfield_str(self):\n    form = ComplexFieldForm({'field1': 'test,J,2022-01-01 12:00:00'})\n    bound_field = form['field1']\n    self.assertEqual(str(bound_field), bound_field.as_widget())\n    self.assertHTMLEqual(str(bound_field), bound_field.as_widget())\n", "def test_boundfield_as_widget(self):\n    form = ComplexFieldForm()\n    bound_field = form['field1']\n    widget = bound_field.as_widget()\n    self.assertHTMLEqual(\n        widget,\n        '<input type=\"text\" name=\"field1_0\"><select multiple name=\"field1_1\">'\n        '<option value=\"J\">John</option><option value=\"P\">Paul</option>'\n        '<option value=\"G\">George</option><option value=\"R\">Ringo</option>'\n        '</select><input type=\"date\" name=\"field1_2_0\"><input type=\"time\" name=\"field1_2_1\">',\n    )\n"], "sample_283": ["def test_settings_to_cmd_args_env_with_ssl(self):\n    settings_dict = {\n        'HOST': 'localhost',\n        'PORT': 5432,\n        'NAME': 'mydb',\n        'USER': 'myuser',\n        'PASSWORD': 'mypassword',\n        'OPTIONS': {\n            'sslmode': 'require',\n            'sslrootcert': '/path/to/root/cert',\n            'sslcert': '/path/to/cert',\n            'sslkey': '/path/to/key',\n        }\n    }\n    args, env = self.settings_to_cmd_args_env(settings_dict)\n    self.assertEqual(args, ['psql', '-h', 'localhost', '-p', '5432', '-U', 'myuser', 'mydb'])\n    self.assertEqual(env, {\n        'PGPASSWORD': 'mypassword',\n        'PGSSLMODE': 'require',\n        'PGSSLROOTCERT': '/path/to/root/cert',\n        'PGSSLCERT': '/path/to/cert',\n        'PGSSLKEY': '/path/to/key',\n    })\n", "def test_settings_to_cmd_args_env_with_ssl_settings(self):\n    settings_dict = {\n        'HOST': 'localhost',\n        'PORT': 5432,\n        'NAME': 'mydb',\n        'USER': 'myuser',\n        'PASSWORD': 'mypassword',\n        'OPTIONS': {\n            'sslmode': 'require',\n            'sslrootcert': '/path/to/root/cert',\n            'sslcert': '/path/to/client/cert',\n            'sslkey': '/path/to/client/key',\n        }\n    }\n    args, env = self.settings_to_cmd_args_env(settings_dict)\n    self.assertEqual(args, ['psql', '-U', 'myuser', '-h', 'localhost', '-p', '5432', 'mydb'])\n    self.assertEqual(env, {\n        'PGSSLMODE': 'require',\n        'PGSSLROOTCERT': '/path/to/root/cert',\n        'PGSSLCERT': '/path/to/client/cert',\n        'PGSSLKEY': '/path/to/client/key',\n    })\n", "def test_settings_to_cmd_args_env_service(self):\n    settings_dict = {\n        'OPTIONS': {'service': 'my_service'},\n    }\n    cmd_args, env = self.settings_to_cmd_args_env(settings_dict)\n    self.assertEqual(cmd_args, ['psql'])\n    self.assertEqual(env['PGSERVICE'], 'my_service')\n", "def test_settings_to_cmd_args_env_with_password(self):\n    settings_dict = {\n        'HOST': 'localhost',\n        'PORT': 5432,\n        'NAME': 'mydatabase',\n        'USER': 'myuser',\n        'PASSWORD': 'mypassword',\n    }\n    args, env = self.settings_to_cmd_args_env(settings_dict)\n    self.assertEqual(env['PGPASSWORD'], 'mypassword')\n    self.assertNotIn('-W', args)  # password should not be passed as a command line argument\n", "def test_settings_to_cmd_args_env_with_ssl(self):\n    settings_dict = {\n        'NAME': 'mydb',\n        'USER': 'myuser',\n        'PASSWORD': 'mypassword',\n        'HOST': 'localhost',\n        'PORT': 5432,\n        'OPTIONS': {\n            'sslmode': 'require',\n            'sslrootcert': '/path/to/ssl/root/cert',\n            'sslcert': '/path/to/ssl/cert',\n            'sslkey': '/path/to/ssl/key',\n        }\n    }\n    args, env = self.settings_to_cmd_args_env(settings_dict)\n    self.assertEqual(args, ['psql', '-U', 'myuser', '-h', 'localhost', '-p', '5432', 'mydb'])\n    self.assertEqual(env, {\n        'PGSSLMODE': 'require',\n        'PGSSLROOTCERT': '/path/to/ssl/root/cert',\n        'PGSSLCERT': '/path/to/ssl/cert',\n        'PGSSLKEY': '/path/to/ssl/key',\n        'PGPASSWORD': 'mypassword',\n    })\n"], "sample_284": ["    def test_post_process_passes(self):\n        \"\"\"\n        Test that the post processing is run multiple times to hash\n        files referencing other files.\n        \"\"\"\n        self.assertPostCondition = lambda: self.assertEqual(\n            storage.staticfiles_storage.max_post_process_passes, 5)\n\n        # Set max_post_process_passes to a lower value to avoid long tests\n        storage.staticfiles_storage.max_post_process_passes = 3\n\n        # Add a file with a reference to another file\n        relative_path = 'styles/main.css'\n        file_path = os.path.join(settings.STATIC_ROOT, relative_path)\n        Path(file_path).write_text('/* */\\n@import url(\"foo.css\");')\n\n        relative_path2 = 'styles/foo.css'\n        file_path2 = os.path.join(settings.STATIC_ROOT, relative_path2)\n        Path(file_path2).write_text('/* */')\n\n        call_command('collectstatic', verbosity=0, interactive=False)\n\n        hashed_relative_path = self.hashed_file_path(relative_path)\n        hashed_relative_path2 = self.hashed_file_path(relative_path2)\n\n        # Check the first file was hashed and references the second one correctly\n        with open(os.path.join(settings.STATIC_ROOT, hashed_relative_path)) as f:\n            content = f.read()\n            self.assertIn(hashed_relative_path2, content)\n\n        self.assertPostCondition()\n", "def test_hashed_name(self):\n    storage.staticfiles_storage.max_post_process_passes = 1\n    original_path = 'path/to/file.txt'\n    hashed_path = storage.staticfiles_storage.hashed_name(original_path)\n    self.assertNotEqual(hashed_path, original_path)\n    self.assertTrue(hashed_path.startswith('path/to/'))\n    self.assertTrue(hashed_path.endswith('.txt'))\n\n    # Test with query string.\n    original_path = 'path/to/file.txt?query=string'\n    hashed_path = storage.staticfiles_storage.hashed_name(original_path)\n    self.assertNotEqual(hashed_path, original_path)\n    self.assertTrue(hashed_path.startswith('path/to/'))\n    self.assertTrue(hashed_path.endswith('.txt?query=string'))\n\n    # Test with fragment.\n    original_path = 'path/to/file.txt#fragment'\n    hashed_path = storage.staticfiles_storage.hashed_name(original_path)\n    self.assertNotEqual(hashed_path, original_path)\n    self.assertTrue(hashed_path.startswith('path/to/'))\n    self.assertTrue(hashed_path.endswith('.txt#fragment'))\n", "    def test_custom_max_post_process_passes(self):\n        storage.staticfiles_storage.max_post_process_passes = 2\n        self.addCleanup(lambda: setattr(storage.staticfiles_storage, 'max_post_process_passes', self._max_post_process_passes))\n        self.assertPostCondition = lambda: self.assertEqual(storage.staticfiles_storage.max_post_process_passes, 2)\n        self.collection_processed({\n            'file.txt': 'file.txt',\n            'file2.txt': 'file2.txt',\n            'css/file.css': 'css/file.css',\n        }, {\n            'file.txt': 'file.txt',\n            'file2.txt': 'file2.txt',\n            'css/file.css': hashed_file_path(self, 'css/file.css'),\n        })\n        self.assertPostCondition()\n", "def test_hashed_files_storage(self):\n    hashed_storage = storage.ManifestStaticFilesStorage(location=settings.STATIC_ROOT)\n    hashed_storage.save('test.txt', StringIO('Hello, world!'))\n\n    # Check that the file was saved with a hash in its name\n    self.assertTrue(hashed_storage.exists('test.{}.txt'.format(hashed_storage.file_hash('test.txt', StringIO('Hello, world!')))))\n\n    # Check that the original file does not exist\n    self.assertFalse(hashed_storage.exists('test.txt'))\n", "    def test_post_process(self):\n        self.render_template(self.static_template_snippet('cached/css/stylesheet.css'))\n        self.assertTemplateRaises(ValueError, self.static_template_snippet('cached/css/stylesheet2.css'))\n\n        # Make sure the hashed files were stored\n        hashed_files = storage.staticfiles_storage.hashed_files\n        self.assertIn('cached/css/stylesheet.css', hashed_files)\n        self.assertNotIn('cached/css/stylesheet2.css', hashed_files)\n\n        # Check if we can access the hashed file via storage\n        self.assertTrue(storage.staticfiles_storage.exists(hashed_files['cached/css/stylesheet.css']))\n"], "sample_285": ["def test_finders_check_nonexistent_staticfiles_dirs(self):\n    errors = check_finders()\n    self.assertEqual(len(errors), 1)\n    self.assertIsInstance(errors[0], Warning)\n    self.assertEqual(errors[0].id, 'staticfiles.W004')\n", "    def test_find_all(self):\n        finder = BaseFinder()\n        with self.assertRaises(NotImplementedError):\n            finder.find('test', all=True)\n", "def test_finders_check_filesystemfinder(self):\n    finder = get_finder('django.contrib.staticfiles.finders.FileSystemFinder')\n    with mock.patch.object(finder, 'check', return_value=[]):\n        errors = check_finders()\n        self.assertEqual(errors, [])\n\n    with mock.patch.object(finder, 'check', return_value=[Error('foo')]):\n        errors = check_finders()\n        self.assertEqual(len(errors), 1)\n        self.assertIsInstance(errors[0], Error)\n        self.assertEqual(errors[0].msg, 'foo')\n\n    with mock.patch.object(finder, 'check', return_value=[Warning('bar')]):\n        errors = check_finders()\n        self.assertEqual(len(errors), 1)\n        self.assertIsInstance(errors[0], Warning)\n        self.assertEqual(errors[0].msg, 'bar')\n", "    def test_base_finder_raises_not_implemented(self):\n        finder = BaseFinder()\n        with self.assertRaises(NotImplementedError):\n            finder.find('path')\n        with self.assertRaises(NotImplementedError):\n            finder.list([])\n        with self.assertRaises(NotImplementedError):\n            finder.check()\n", "    def test_check(self):\n        finder = BaseFinder()\n        with self.assertRaises(NotImplementedError):\n            finder.check()\n"], "sample_286": ["    def testModelStateFieldsCacheDescriptor(self):\n        instance = Article()\n        self.assertEqual(instance._state.fields_cache, {})\n        instance._state.fields_cache['field'] = 'value'\n        self.assertEqual(instance._state.fields_cache, {'field': 'value'})\n", "    def test_deferred_fields(self):\n        instance = Article.objects.create(headline='Test')\n        self.assertEqual(instance.get_deferred_fields(), set())\n\n        instance = Article.objects.defer('headline').get(pk=instance.pk)\n        self.assertEqual(instance.get_deferred_fields(), {'headline'})\n\n        instance.refresh_from_db()\n        self.assertEqual(instance.get_deferred_fields(), set())\n", "    def test_refresh_from_db(self):\n        # Create an article\n        article = Article.objects.create(\n            headline='Original Headline',\n            pub_date=datetime(2022, 1, 1),\n        )\n\n        # Update the article outside of the ORM\n        with connections[DEFAULT_DB_ALIAS].cursor() as cursor:\n            cursor.execute(\"UPDATE myapp_article SET headline = 'Updated Headline' WHERE id = %s\", [article.pk])\n\n        # Refresh the article from the database\n        article.refresh_from_db()\n\n        # Assert that the article's headline has been updated\n        self.assertEqual(article.headline, 'Updated Headline')\n", "    def test_deferred_fields(self):\n        instance = Article.objects.create(headline='Test')\n        self.assertEqual(instance.get_deferred_fields(), set())\n        instance = Article.objects.defer('headline').get(pk=instance.pk)\n        self.assertEqual(instance.get_deferred_fields(), {'headline'})\n", "    def test_setting_primary_key_on_deferred_instance(self):\n        instance = Article()\n        self.assertIsNone(instance.pk)\n        self.assertTrue(instance._state.adding)\n\n        # Set the primary key and verify that _state.adding is updated.\n        instance.id = 1\n        self.assertFalse(instance._state.adding)\n"], "sample_287": ["def test_check_dependencies(self):\n    class MyAdminSite(AdminSite):\n        pass\n\n    site = MyAdminSite()\n    errors = check_dependencies()\n    self.assertEqual(errors, [])\n\n    with override_settings(INSTALLED_APPS=['django.contrib.admin']):\n        errors = check_dependencies()\n        self.assertEqual(len(errors), 3)\n\n    with override_settings(MIDDLEWARE=[\n        'django.contrib.sessions.middleware.SessionMiddlewareSubclass',\n        'django.contrib.auth.middleware.AuthenticationMiddlewareSubclass',\n        'django.contrib.messages.middleware.MessageMiddlewareSubclass',\n    ]):\n        errors = check_dependencies()\n        self.assertEqual(errors, [])\n\n    with override_settings(MIDDLEWARE=[\n        'django.contrib.auth.middleware.AuthenticationMiddleware',\n    ]):\n        errors = check_dependencies()\n        self.assertEqual(len(errors), 2)\n\n    with override_settings(AUTHENTICATION_BACKENDS=['django.contrib.auth.backends.ModelBackendSubclass']):\n        errors = check_dependencies()\n        self.assertEqual(len(errors), 1)\n", "def test_check_ordering_item(self):\n    class MyAdmin(admin.ModelAdmin):\n        ordering = ('nonexistent_field',)\n    errors = checks.run_checks()\n    self.assertEqual(len(errors), 1)\n    error = errors[0]\n    self.assertEqual(error.id, 'admin.E033')\n    self.assertEqual(str(error), \"The value of 'ordering' refers to 'nonexistent_field', which is not a field of 'None'.\")\n", "def test_check_admin_app(self):\n    class MyAdminSite(AdminSite):\n            return ['error from MyAdminSite']\n\n    site = MyAdminSite()\n    errors = check_admin_app([site])\n    self.assertEqual(errors, ['error from MyAdminSite'])\n", "def test_check_dependencies(self):\n    with self.settings(INSTALLED_APPS=['django.contrib.admin']):\n        errors = checks.run_checks()\n        self.assertEqual(errors, [\n            checks.Error(\n                \"'django.contrib.contenttypes' must be in INSTALLED_APPS in order to use the admin application.\",\n                id='admin.E401',\n            ),\n            checks.Error(\n                \"'django.contrib.auth' must be in INSTALLED_APPS in order to use the admin application.\",\n                id='admin.E405',\n            ),\n            checks.Error(\n                \"'django.contrib.messages' must be in INSTALLED_APPS in order to use the admin application.\",\n                id='admin.E406',\n            ),\n            checks.Error(\n                \"A 'django.template.backends.django.DjangoTemplates' instance must be configured in TEMPLATES in order to use the admin application.\",\n                id='admin.E403',\n            ),\n        ])\n", "def test_check_list_display_links(self):\n    class MyAdmin(admin.ModelAdmin):\n        list_display = ('title', 'author')\n        list_display_links = ('title',)\n\n    errors = MyAdmin(Song, AdminSite()).check()\n    self.assertEqual(errors, [])\n\n    class MyAdmin(admin.ModelAdmin):\n        list_display = ('title', 'author')\n        list_display_links = ('title', 'author', 'nonexistent')\n\n    errors = MyAdmin(Song, AdminSite()).check()\n    self.assertEqual(len(errors), 1)\n    self.assertEqual(errors[0].id, 'admin.E111')\n"], "sample_288": ["def test_key_transform_factory(self):\n    json_field = JSONModel._meta.get_field('value')\n    key_transform_factory = KeyTransformFactory('key')\n    key_transform = key_transform_factory(json_field)\n    self.assertIsInstance(key_transform, KeyTransform)\n    self.assertEqual(key_transform.key_name, 'key')\n    self.assertEqual(key_transform.lhs, json_field)\n", "def test_jsonfield_default_value(self):\n    model = JSONModel.objects.create()\n    self.assertEqual(model.field, {})\n    model.delete()\n\n    model = NullableJSONModel.objects.create()\n    self.assertIsNone(model.field)\n    model.delete()\n\n    model = JSONModel.objects.create(field={'key': 'value'})\n    self.assertEqual(model.field, {'key': 'value'})\n    model.delete()\n\n    with self.assertRaises(ValidationError):\n        JSONModel.objects.create(field='invalid')\n", "def test_key_transform_nested(self):\n    obj = JSONModel.objects.create(json={\"a\": {\"b\": {\"c\": \"value\"}}})\n    self.assertEqual(\n        JSONModel.objects.filter(json__a__b__c=\"value\").get(), obj\n    )\n    self.assertEqual(\n        JSONModel.objects.filter(json__a__b__contains={\"c\": \"value\"}).get(), obj\n    )\n", "def test_key_transform_text_lookup_mixin(self):\n    # Test that KeyTransformTextLookupMixin correctly adapts the lhs expression.\n    mixin = KeyTransformTextLookupMixin()\n    key_transform = KeyTransform('key', 'value')\n    adapted_lhs = mixin.process_lhs(key_transform, None, connection)\n    self.assertIsInstance(adapted_lhs, KeyTextTransform)\n    self.assertEqual(adapted_lhs.key_name, key_transform.key_name)\n    self.assertEqual(adapted_lhs.source_expressions, key_transform.source_expressions)\n", "def test_key_transform_exact_lookup_with_dict(self):\n    obj = JSONModel.objects.create(json_field={'key': {'nested_key': 'value'}})\n    self.assertSequenceEqual(\n        JSONModel.objects.filter(json_field__key__nested_key=Value('value')).values_list('id', flat=True),\n        [obj.id],\n    )\n    self.assertSequenceEqual(\n        JSONModel.objects.filter(json_field__key__nested_key=Value('other_value')).values_list('id', flat=True),\n        [],\n    )\n"], "sample_289": ["def test_ordered_set_init(self):\n    # Test initialization of OrderedSet with various inputs\n    self.assertEqual(OrderedSet().dict, {})\n    self.assertEqual(OrderedSet([]).dict, {})\n    self.assertEqual(OrderedSet([1, 2, 3]).dict, {1: None, 2: None, 3: None})\n    self.assertEqual(OrderedSet((1, 2, 3)).dict, {1: None, 2: None, 3: None})\n    self.assertEqual(OrderedSet('abc').dict, {'a': None, 'b': None, 'c': None})\n", "def test_add_remove(self):\n    s = OrderedSet([1, 2, 3])\n    self.assertEqual(len(s), 3)\n    s.add(4)\n    self.assertEqual(len(s), 4)\n    s.remove(2)\n    self.assertEqual(len(s), 3)\n    self.assertEqual(list(s), [1, 3, 4])\n    with self.assertRaises(KeyError):\n        s.remove(2)\n    s.discard(1)\n    self.assertEqual(len(s), 2)\n    self.assertEqual(list(s), [3, 4])\n    s.discard(1)  # should not raise an error\n", "def test_add_and_remove(self):\n    s = OrderedSet()\n    self.assertEqual(len(s), 0)\n    s.add(1)\n    self.assertEqual(len(s), 1)\n    self.assertIn(1, s)\n    s.remove(1)\n    self.assertEqual(len(s), 0)\n    self.assertNotIn(1, s)\n    with self.assertRaises(KeyError):\n        s.remove(1)\n", "def test_OrderedSet_remove(self):\n    s = OrderedSet([1, 2, 3])\n    self.assertEqual(len(s), 3)\n    s.remove(2)\n    self.assertEqual(len(s), 2)\n    self.assertNotIn(2, s)\n    with self.assertRaises(KeyError):\n        s.remove(2)\n", "def test_ordered_set_remove(self):\n    s = OrderedSet([1, 2, 3])\n    s.remove(2)\n    self.assertEqual(list(s), [1, 3])\n    with self.assertRaises(KeyError):\n        s.remove(4)\n"], "sample_290": ["def test_suggest_name(self):\n    # Test suggest_name with no operations\n    migration = Migration(\"testapp\", \"0001_initial\")\n    self.assertEqual(migration.suggest_name(), 'initial')\n\n    # Test suggest_name with a single operation\n    migration.operations = [migrations.CreateModel(name=\"Author\")]\n    self.assertEqual(migration.suggest_name(), 'create_author')\n\n    # Test suggest_name with multiple operations\n    migration.operations = [\n        migrations.CreateModel(name=\"Author\"),\n        migrations.CreateModel(name=\"Book\"),\n    ]\n    self.assertEqual(migration.suggest_name(), 'create_author_and_more')\n", "def test_suggest_name(self):\n    migration = Migration('auto_123', 'testapp')\n    migration.operations = [\n        migrations.CreateModel(\n            name='Author',\n            fields=[\n                ('id', models.AutoField(primary_key=True)),\n                ('name', models.CharField(max_length=200)),\n            ],\n        ),\n    ]\n    self.assertEqual(migration.suggest_name(), 'create_author')\n\n    migration.operations = [\n        migrations.CreateModel(\n            name='Author',\n            fields=[\n                ('id', models.AutoField(primary_key=True)),\n                ('name', models.CharField(max_length=200)),\n            ],\n        ),\n        migrations.CreateModel(\n            name='Book',\n            fields=[\n                ('id', models.AutoField(primary_key=True)),\n                ('author', models.ForeignKey('testapp.Author', models.CASCADE)),\n                ('title', models.CharField(max_length=200)),\n            ],\n        ),\n    ]\n    self.assertEqual(migration.suggest_name(), 'create_author_and_book')\n\n    migration.operations = [\n        migrations.CreateModel(\n            name='Author',\n            fields=[\n                ('id', models.AutoField(primary_key=True)),\n                ('name', models.CharField(max_length=200)),\n            ],\n        ),\n        migrations.CreateModel(\n            name='Book',\n            fields=[\n                ('id', models.AutoField(primary_key=True)),\n                ('author', models.ForeignKey('testapp.Author', models.CASCADE)),\n                ('title', models.CharField(max_length=200)),\n            ],\n        ),\n        migrations.CreateModel(\n            name='Publisher',\n            fields=[\n                ('id', models.AutoField(primary_key=True)),\n                ('name', models.CharField(max_length=100)),\n            ],\n        ),\n    ]\n    self.assertEqual(migration.suggest_name(), 'create_author_and_more')\n\n    migration.initial = True\n    self.assertEqual(migration.suggest_name(), 'initial')\n", "def test_suggest_name(self):\n    migration = migrations.Migration(\"testapp\", \"0001_initial\")\n    migration.operations = [\n        migrations.CreateModel(\n            name=\"Author\",\n            fields=[\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n            ],\n        ),\n    ]\n    self.assertEqual(migration.suggest_name(), \"initial\")\n\n    migration.operations = [\n        migrations.CreateModel(\n            name=\"Author\",\n            fields=[\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n            ],\n        ),\n        migrations.CreateModel(\n            name=\"Book\",\n            fields=[\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"author\", models.ForeignKey(\"testapp.Author\", models.CASCADE)),\n                (\"title\", models.CharField(max_length=200)),\n            ],\n        ),\n    ]\n    self.assertRegex(migration.suggest_name(), r\"^auto_\\d+\")\n\n    migration.operations = [\n        migrations.CreateModel(\n            name=\"Author\",\n            fields=[\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n            ],\n        ),\n        migrations.CreateModel(\n            name=\"Book\",\n            fields=[\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"author\", models.ForeignKey(\"testapp.Author\", models.CASCADE)),\n                (\"title\", models.CharField(max_length=200)),\n            ],\n        ),\n        migrations.CreateModel(\n            name=\"Contract\",\n            fields=[\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"author\", models.ForeignKey(\"testapp.Author\", models.CASCADE)),\n                (\"publisher\", models.ForeignKey(\"testapp.Publisher\", models.CASCADE)),\n            ],\n        ),\n    ]\n    self.assertRegex(migration.suggest_name(), r\"^auto_\\d+\")\n\n    migration.operations = [\n        migrations.AddField(\n            model_name=\"author\",\n            name=\"biography\",\n            field=models.TextField(),\n        ),\n    ]\n    self.assertEqual(migration.suggest_name(), \"add_biography_to_author\")\n", "def test_suggest_name(self):\n    migration = Migration(\"0001_initial\", \"testapp\")\n    migration.operations = [\n        migrations.CreateModel(\n            name=\"Author\",\n            fields=[\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n            ],\n        )\n    ]\n    self.assertEqual(migration.suggest_name(), \"initial\")\n\n    migration.operations = [\n        migrations.AddField(\n            model_name=\"author\",\n            name=\"last_name\",\n            field=models.CharField(max_length=200),\n        )\n    ]\n    self.assertRegex(migration.suggest_name(), r\"^auto_\\d{14}$\")\n\n    migration.operations = [\n        migrations.RenameField(\n            model_name=\"author\",\n            old_name=\"last_name\",\n            new_name=\"surname\",\n        )\n    ]\n    self.assertRegex(migration.suggest_name(), r\"^rename_last_name_to_surname$\")\n\n    migration.operations = [\n        migrations.RenameField(\n            model_name=\"author\",\n            old_name=\"last_name\",\n            new_name=\"surname\",\n        ),\n        migrations.RenameField(\n            model_name=\"author\",\n            old_name=\"first_name\",\n            new_name=\"given_name\",\n        ),\n    ]\n    self.assertRegex(migration.suggest_name(), r\"^rename_last_name_to_surname_and_more$\")\n", "def test_suggest_name(self):\n    migration = Migration(\"testapp\", \"0001_initial\")\n    migration.operations = [\n        migrations.CreateModel(\n            name='Author',\n            fields=[\n                ('id', models.AutoField(primary_key=True)),\n                ('name', models.CharField(max_length=200)),\n            ],\n        ),\n    ]\n    self.assertEqual(migration.suggest_name(), 'initial')\n\n    migration = Migration(\"testapp\", \"0002_auto\")\n    migration.operations = [\n        migrations.AddField(\n            model_name='author',\n            name='publisher',\n            field=models.ForeignKey(\"testapp.Publisher\", models.CASCADE),\n        ),\n    ]\n    timestamp = get_migration_name_timestamp()\n    self.assertRegex(migration.suggest_name(), r'^auto_%s$' % timestamp)\n\n    migration = Migration(\"testapp\", \"0003_auto\")\n    migration.operations = [\n        migrations.AddField(\n            model_name='author',\n            name='publisher',\n            field=models.ForeignKey(\"testapp.Publisher\", models.CASCADE),\n        ),\n        migrations.AddField(\n            model_name='author',\n            name='book',\n            field=models.ForeignKey(\"otherapp.Book\", models.CASCADE),\n        ),\n    ]\n    self.assertRegex(migration.suggest_name(), r'^add_book_publisher_%s$' % timestamp)\n"], "sample_291": ["def test_view_as_view_method(self):\n    view = View.as_view()\n    self.assertTrue(callable(view))\n    self.assertIsInstance(view, type(lambda: None))  # check that it's a function\n\n    wrapped_view = DecoratedDispatchView.as_view()\n    response = wrapped_view(self.rf.get('/'))\n    self.assertEqual(response.status_code, 200)\n    self.assertTrue(wrapped_view.is_decorated)  # check that decorator was applied\n", "def test_as_view_inheritance(self):\n    class InheritedView(SimpleView):\n            return HttpResponse('This is an inherited view')\n\n    response = InheritedView.as_view()(self.rf.get('/'))\n    self.assertEqual(response.status_code, 200)\n    self.assertEqual(response.content, b'This is an inherited view')\n", "def test_template_response_mixin_get_template_names_raises_error(self):\n    class TemplateViewWithoutTemplate(TemplateResponseMixin, View):\n        pass\n\n    view = TemplateViewWithoutTemplate()\n    request = self.rf.get('/test/')\n    view.setup(request)\n\n    with self.assertRaises(ImproperlyConfigured):\n        view.get_template_names()\n", "def test_options(self):\n    request = self.rf.options('/')\n    response = SimpleView().dispatch(request)\n    self.assertEqual(response.status_code, 200)\n    self.assertIn('Allow', response.headers)\n    self.assertEqual(response.headers['Allow'], 'GET, HEAD, OPTIONS')\n", "def test_as_view(self):\n    view = View.as_view()\n    self.assertEqual(view.view_class, View)\n    self.assertEqual(view.view_initkwargs, {})\n    self.assertIsNone(view.__self__)\n    self.assertEqual(view.__name__, 'View')\n    self.assertEqual(view.__module__, views.__name__)\n\n    # Test that `as_view()` method sets the correct attributes on the view function.\n    self.assertTrue(hasattr(view, '__annotations__'))\n    self.assertTrue(hasattr(view, '__doc__'))\n    self.assertTrue(hasattr(view, '__dict__'))\n\n    # Test that `as_view()` raises TypeError when keyword arguments are not valid.\n    with self.assertRaises(TypeError):\n        View.as_view(invalid_kwarg='value')\n"], "sample_292": ["def test_csrf_trusted_origins(self):\n    req = self._get_POST_request_with_token()\n    req.META['HTTP_ORIGIN'] = 'https://example.com'\n    with override_settings(CSRF_TRUSTED_ORIGINS=['https://example.com']):\n        response = CsrfViewMiddleware().process_view(req, None, [], {})\n        self.assertIsNone(response)\n\n    with override_settings(CSRF_TRUSTED_ORIGINS=['http://example.com']):\n        response = CsrfViewMiddleware().process_view(req, None, [], {})\n        self.assertEqual(response.status_code, 403)\n", "def test_csrf_trusted_origins_exact(self):\n    mw = CsrfViewMiddleware()\n    request = self._get_POST_request_with_token()\n    request.META['HTTP_ORIGIN'] = 'https://example.com'\n    with override_settings(CSRF_TRUSTED_ORIGINS=['https://example.com']):\n        response = mw.process_view(request, None, (), {})\n        self.assertIsNone(response)\n", "def test_csrf_view_middleware_process_response_csrf_cookie_needs_reset(self):\n    req = self._get_POST_request_with_token()\n    req.META['CSRF_COOKIE_USED'] = True\n    req.csrf_cookie_needs_reset = True\n\n    middleware = CsrfViewMiddleware()\n    response = HttpResponse()\n    response = middleware.process_response(req, response)\n\n    self.assertTrue(response.csrf_cookie_set)\n    self.assertIn(settings.CSRF_COOKIE_NAME, response.cookies)\n", "def test_csrf_token_length(self):\n    req = self._get_GET_no_csrf_cookie_request()\n    token = get_token(req)\n    self.assertEqual(len(token), CSRF_TOKEN_LENGTH)\n", "def test_csrf_token_length(self):\n    token = get_token(self._get_GET_no_csrf_cookie_request())\n    self.assertEqual(len(token), CSRF_TOKEN_LENGTH)\n"], "sample_293": ["def test_no_url_patterns(self):\n    resolver = get_resolver(None)\n    with self.assertRaises(Resolver404):\n        resolver.resolve('/example/')\n", "    def test_no_url_patterns(self):\n        resolver = get_resolver()\n        with self.assertRaisesMessage(Resolver404, 'No URL pattern matched'):\n            resolver.resolve('/any_url/')\n", "    def test_urlresolver_populate(self):\n        resolver = URLResolver(RegexPattern(r'^/'), 'urlpatterns_reverse.urls')\n        self.assertFalse(resolver._populated)\n        resolver._populate()\n        self.assertTrue(resolver._populated)\n        self.assertIsInstance(resolver.reverse_dict, dict)\n        self.assertIsInstance(resolver.namespace_dict, dict)\n        self.assertIsInstance(resolver.app_dict, dict)\n", "    def test_empty_urlconf(self):\n        resolver = get_resolver(None)\n        with self.assertRaises(Resolver404):\n            resolver.resolve('/test/')\n", "    def test_url_pattern_resolve(self):\n        resolver = get_resolver(None)\n        pattern = URLPattern(RegexPattern(r'^test/$'), views.empty_view, name='test')\n        match = pattern.resolve('/test/')\n        self.assertIsInstance(match, ResolverMatch)\n        self.assertEqual(match.view_name, 'test')\n        self.assertEqual(match.func, views.empty_view)\n"], "sample_294": ["def test_csrf_tokenrotation(self):\n    # Create a request with a valid CSRF token\n    req = self._get_POST_request_with_token()\n    get_token(req)\n\n    # Rotate the token\n    rotate_token(req)\n\n    # Check that the new token is different from the old one\n    self.assertNotEqual(req.META['CSRF_COOKIE'], self._csrf_id)\n", "def test_process_view_rotate_token(self):\n    req = self._get_POST_csrf_cookie_request()\n    CsrfViewMiddleware().process_view(req, None, [], {})\n    token1 = get_token(req)\n    rotate_token(req)\n    CsrfViewMiddleware().process_view(req, None, [], {})\n    token2 = get_token(req)\n    self.assertNotEqual(token1, token2)\n    self.assertTrue(equivalent_tokens(token2, req.META['CSRF_COOKIE']))\n", "def test_csrf_tokenrotation(self):\n    mw = CsrfViewMiddleware()\n    request = self._get_GET_csrf_cookie_request()\n    mw.process_request(request)\n    initial_csrf_token = request.META['CSRF_COOKIE']\n    rotate_token(request)\n    mw.process_response(request, HttpResponse())\n    self.assertNotEqual(initial_csrf_token, request.META['CSRF_COOKIE'])\n    self.assertTrue(equivalent_tokens(initial_csrf_token, request.META['CSRF_COOKIE']))\n", "def test_csrf_view_middleware_process_response(self):\n    req = self._get_GET_csrf_cookie_request()\n    req.META['CSRF_COOKIE_USED'] = True\n    middleware = CsrfViewMiddleware()\n\n    # Simulate a response from a view that did not set the CSRF cookie\n    response = HttpResponse()\n    response = middleware.process_response(req, response)\n\n    # Check that the CSRF cookie is set in the response\n    self.assertTrue(response.csrf_cookie_set)\n    self.assertEqual(response.cookies[settings.CSRF_COOKIE_NAME].value, req.META['CSRF_COOKIE'])\n", "def test_csrf_token_length(self):\n    token = get_token(HttpRequest())\n    self.assertEqual(len(token), CSRF_TOKEN_LENGTH)\n    self.assertTrue(re.match('^[a-zA-Z0-9]+$', token))\n"], "sample_295": ["def test_expression_pickle(self):\n    expr = F('name')\n    pickled_expr = pickle.dumps(expr)\n    unpickled_expr = pickle.loads(pickled_expr)\n    self.assertEqual(expr, unpickled_expr)\n\n    expr = F('name') + F('num_employees')\n    pickled_expr = pickle.dumps(expr)\n    unpickled_expr = pickle.loads(pickled_expr)\n    self.assertEqual(expr, unpickled_expr)\n", "def test_expression_pickle(self):\n    expr = F('num_employees') + Value(5)\n    pickled_expr = pickle.loads(pickle.dumps(expr))\n    self.assertEqual(expr, pickled_expr)\n\n    # Check that it still works after a copy\n    expr_copy = deepcopy(expr)\n    pickled_expr_copy = pickle.loads(pickle.dumps(expr_copy))\n    self.assertEqual(expr, pickled_expr_copy)\n", "def test_expression_pickle(self):\n    expr = F('num_employees') + Value(5)\n    pickled_expr = pickle.loads(pickle.dumps(expr))\n    self.assertEqual(expr, pickled_expr)\n    self.assertEqual(expr.as_sql(connection), pickled_expr.as_sql(connection))\n", "def test_expression_pickle(self):\n    expr = F('name') + Value('example')\n    pickled_expr = pickle.dumps(expr)\n    unpickled_expr = pickle.loads(pickled_expr)\n    self.assertEqual(expr, unpickled_expr)\n    self.assertEqual(unpickled_expr.resolve_expression().as_sql(connection), ('\"name\" + %s', ['example']))\n", "def test_expression_pickle(self):\n    expr = F('name') + Value('test')\n    pickled_expr = pickle.dumps(expr)\n    unpickled_expr = pickle.loads(pickled_expr)\n    self.assertEqual(expr, unpickled_expr)\n    with self.assertRaises(pickle.UnpicklingError):\n        pickle.loads(pickle.dumps(expr, protocol=0))\n"], "sample_296": ["def test_process_messages_with_non_list_data(self):\n    decoder = MessageDecoder()\n    data = {'message': 'Hello, world!'}\n    processed_data = decoder.process_messages(data)\n    self.assertEqual(processed_data, data)\n\n    data = 'Hello, world!'\n    processed_data = decoder.process_messages(data)\n    self.assertEqual(processed_data, data)\n\n    data = 123\n    processed_data = decoder.process_messages(data)\n    self.assertEqual(processed_data, data)\n", "def test_decode_invalid_json(self):\n    storage = self.storage_class(FakeRequest())\n    # Set a cookie with invalid JSON.\n    encoded_data = storage._encode([Message(constants.INFO, 'Test')])\n    # Corrupt the JSON by replacing a '{' with a '['.\n    encoded_data = encoded_data.replace(b'{', b'[')\n    set_cookie_data(storage, [], encode_empty=True)\n    storage.request.COOKIES = {CookieStorage.cookie_name: encoded_data}\n    messages, all_retrieved = storage._get()\n    self.assertIsNone(messages)\n    self.assertTrue(all_retrieved)\n", "def test_message_decoder_process_messages(self):\n    # Test that MessageDecoder.process_messages correctly decodes messages\n    message = Message(constants.INFO, 'Test message')\n    encoded_message = MessageEncoder().default(message)\n    decoded_message = MessageDecoder().process_messages(encoded_message)\n\n    self.assertIsInstance(decoded_message, Message)\n    self.assertEqual(decoded_message.level, constants.INFO)\n    self.assertEqual(decoded_message.message, 'Test message')\n\n    # Test that MessageDecoder.process_messages correctly handles SafeData\n    safe_message = Message(constants.INFO, mark_safe('Test <b>message</b>'))\n    encoded_safe_message = MessageEncoder().default(safe_message)\n    decoded_safe_message = MessageDecoder().process_messages(encoded_safe_message)\n\n    self.assertIsInstance(decoded_safe_message, Message)\n    self.assertEqual(decoded_safe_message.level, constants.INFO)\n    self.assertIsInstance(decoded_safe_message.message, SafeData)\n    self.assertEqual(decoded_safe_message.message, 'Test <b>message</b>')\n", "def test_message_decoder_process_messages(self):\n    message = Message(constants.INFO, 'message')\n    encoded_message = MessageEncoder().default(message)\n\n    # Test that the decoder correctly identifies and decodes messages\n    self.assertIsInstance(MessageDecoder().process_messages(encoded_message), Message)\n\n    # Test that the decoder leaves non-message data unchanged\n    data = ['not', 'a', 'message']\n    self.assertEqual(MessageDecoder().process_messages(data), data)\n\n    # Test that the decoder handles recursive data structures\n    recursive_data = [encoded_message, data]\n    decoded_recursive_data = MessageDecoder().process_messages(recursive_data)\n    self.assertIsInstance(decoded_recursive_data[0], Message)\n    self.assertEqual(decoded_recursive_data[1], data)\n", "def test_max_cookie_length(self):\n    storage = self.get_storage()\n    messages = []\n    for _ in range(100):\n        messages.append(Message(constants.INFO, 'A' * 50))\n    response = self.get_response()\n    unstored_messages = storage._store(messages, response)\n    self.assertTrue(unstored_messages)\n    self.assertLess(len(storage._encode(messages)), CookieStorage.max_cookie_size)\n    cookie = SimpleCookie()\n    self.assertLess(len(cookie.value_encode(storage._encode(messages))[1]), CookieStorage.max_cookie_size)\n"], "sample_298": ["def test_make_token_with_timestamp(self):\n    user = User.objects.create_user('testuser', 'test@example.com', 'password')\n    timestamp = 1643723900  # equivalent to a specific date in 2022\n    token_generator = PasswordResetTokenGenerator()\n    token = token_generator._make_token_with_timestamp(user, timestamp)\n    self.assertRegex(token, r'^[a-z0-9]{6}-[a-f0-9]{13}$')  # check token format\n", "def test_make_token_with_timestamp(self):\n    user = User.objects.create_user('testuser', 'test@example.com', 'password')\n    timestamp = 1643723900\n    token_generator = PasswordResetTokenGenerator()\n    token = token_generator._make_token_with_timestamp(user, timestamp)\n    ts_b36, hash_string = token.split('-')\n    self.assertEqual(base36_to_int(ts_b36), timestamp)\n    self.assertEqual(len(hash_string), 20)  # The length of the hash string is 20 characters.\n", "def test_make_token(self):\n    user = User.objects.create_user('testuser', 'test@example.com', 'password')\n    token_generator = PasswordResetTokenGenerator()\n    token = token_generator.make_token(user)\n    self.assertRegex(token, r'^[0-9a-z]{6}-[0-9a-f]{13}$')\n    self.assertTrue(token_generator.check_token(user, token))\n", "def test_make_token_with_timestamp(self):\n    user = User.objects.create_user('testuser', 'test@example.com', 'password')\n    timestamp = 1643723900  # arbitrary timestamp value\n    token_generator = PasswordResetTokenGenerator()\n    token = token_generator._make_token_with_timestamp(user, timestamp)\n    self.assertRegex(token, r'^[0-9a-z]{6}-[0-9a-f]{13}$')\n", "def test_token_generator_timeout(self):\n    user = User.objects.create_user('testuser', 'test@example.com', 'password')\n    token_generator = MockedPasswordResetTokenGenerator(datetime.now())\n    token = token_generator.make_token(user)\n\n    # Check that the token is valid within the timeout period\n    with override_settings(PASSWORD_RESET_TIMEOUT=60):\n        self.assertTrue(token_generator.check_token(user, token))\n\n    # Check that the token is not valid after the timeout period\n    with override_settings(PASSWORD_RESET_TIMEOUT=1):\n        token_generator._now_val += timedelta(seconds=2)\n        self.assertFalse(token_generator.check_token(user, token))\n"], "sample_299": ["def test_check_default_cache_is_configured(self):\n    with override_settings(CACHES=self.VALID_CACHES_CONFIGURATION):\n        self.assertEqual(check_default_cache_is_configured([]), [])\n\n    with override_settings(CACHES=self.INVALID_CACHES_CONFIGURATION):\n        self.assertEqual(check_default_cache_is_configured([]), [E001])\n", "def test_check_default_cache_is_configured(self):\n    with override_settings(CACHES=self.VALID_CACHES_CONFIGURATION):\n        self.assertEqual(check_default_cache_is_configured(None), [])\n\n    with override_settings(CACHES=self.INVALID_CACHES_CONFIGURATION):\n        self.assertEqual(\n            check_default_cache_is_configured(None),\n            [E001],\n        )\n", "def test_check_default_cache_is_configured_valid_configuration(self):\n    with override_settings(CACHES=self.VALID_CACHES_CONFIGURATION):\n        errors = check_default_cache_is_configured()\n        self.assertEqual(errors, [])\n", "def test_check_default_cache_is_configured(self):\n    with override_settings(CACHES=self.INVALID_CACHES_CONFIGURATION):\n        errors = check_default_cache_is_configured(None)\n        self.assertEqual(errors, [E001])\n", "def test_check_default_cache_is_configured(self):\n    with override_settings(CACHES=self.INVALID_CACHES_CONFIGURATION):\n        errors = check_default_cache_is_configured(None)\n        self.assertEqual(errors, [E001])\n    with override_settings(CACHES=self.VALID_CACHES_CONFIGURATION):\n        errors = check_default_cache_is_configured(None)\n        self.assertEqual(errors, [])\n"], "sample_300": ["def test_names_to_path(self):\n    query = Query(Item)\n    path, final_field, targets, rest = query.names_to_path(\n        ['created_by', 'name'], Item._meta, allow_many=True, fail_on_missing=True,\n    )\n    self.assertEqual(len(path), 1)\n    self.assertIsInstance(final_field, CharField)\n    self.assertEqual(targets, (Author._meta.get_field('name'),))\n    self.assertEqual(rest, ())\n", "def test_build_lookup(self):\n    query = Query(Author)\n    lookup = query.build_lookup(['exact'], Col('name', 'author'), 'John')\n    self.assertIsInstance(lookup, Exact)\n\n    with self.assertRaises(FieldError):\n        query.build_lookup(['invalid'], Col('name', 'author'), 'John')\n\n    # Ensure that the output_field of the lookup is correctly set.\n    lookup = query.build_lookup(['exact'], Col('name', 'author', output_field=CharField()), 'John')\n    self.assertEqual(lookup.output_field.__class__, CharField)\n\n    # Ensure that the lookup can handle F expressions as input.\n    lookup = query.build_lookup(['exact'], F('name'), 'John')\n    self.assertIsInstance(lookup, Exact)\n", "def test_build_lookup(self):\n    query = Query(Author)\n    with self.assertRaises(FieldError) as cm:\n        query.build_lookup(['invalid'], Col(F('name'), Author._meta.get_field('name')), 'value')\n    self.assertEqual(cm.exception.args[0], \"Unsupported lookup 'invalid' for CharField or join on the field not permitted.\")\n\n    with self.assertRaises(FieldError) as cm:\n        query.build_lookup(['isnull', 'invalid'], Col(F('name'), Author._meta.get_field('name')), True)\n    self.assertEqual(cm.exception.args[0], \"Unsupported lookup 'invalid' for CharField or join on the field not permitted.\")\n\n    lookup = query.build_lookup(['exact'], Col(F('name'), Author._meta.get_field('name')), 'value')\n    self.assertIsInstance(lookup, Exact)\n    self.assertEqual(lookup.lhs.output_field, Author._meta.get_field('name'))\n    self.assertEqual(lookup.rhs, 'value')\n", "def test_build_filter_on_col(self):\n    query = Query(Author)\n    lookup = query.build_filter(('name', 'test'))\n    self.assertIsInstance(lookup, Exact)\n    self.assertIsInstance(lookup.lhs, Col)\n    self.assertEqual(lookup.rhs, 'test')\n", "def test_build_lookup_with_func(self):\n    class TestFunc(Func):\n        function = 'TEST_FUNC'\n        arity = 1\n\n    lookup = Query(Author).build_lookup(['exact'], TestFunc('name', output_field=CharField()), 'John')\n    self.assertIsInstance(lookup, Exact)\n    self.assertIsInstance(lookup.lhs, Col)\n    self.assertEqual(lookup.rhs, 'John')\n"], "sample_301": ["    def test_get_child_arguments(self):\n        args = autoreload.get_child_arguments()\n        self.assertEqual(args[0], sys.executable)\n", "    def setUp(self):\n        self.temp_dir = tempfile.mkdtemp()\n        self.addCleanup(shutil.rmtree, self.temp_dir)\n\n        self.zip_file_path = Path(self.temp_dir) / 'myzip.zip'\n        with zipfile.ZipFile(self.zip_file_path, 'w') as zip_file:\n            zip_file.writestr('mymodule.py', b'print(\"Hello, World!\")')\n\n        sys.path.insert(0, str(self.zip_file_path))\n        self.addCleanup(lambda: sys.path.remove(str(self.zip_file_path)))\n\n        self.imported_module = import_module('mymodule')\n        self.addCleanup(lambda: sys.modules.pop('mymodule', None))\n", "def test_iter_modules_and_files_zip_import(self):\n    # Create a zip archive containing a Python module.\n    with tempfile.NamedTemporaryFile(suffix='.zip', delete=False) as tmp_file:\n        with zipfile.ZipFile(tmp_file, 'w') as zip_file:\n            zip_file.writestr('test_module/__init__.py', '')\n            zip_file.writestr('test_module/test_file.py', 'pass')\n        self.addCleanup(os.remove, tmp_file.name)\n\n        # Add the zip archive to sys.path and import the test module.\n        with extend_sys_path(tmp_file.name):\n            self.import_and_cleanup('test_module.test_file')\n\n        # Check that the zip archive is included in iter_modules_and_files().\n        self.clear_autoreload_caches()\n        modules = list(autoreload.iter_all_python_module_files())\n        self.assertIn(Path(tmp_file.name), [p.parent for p in modules])\n", "    def test_common_roots(self):\n        temp_dir = tempfile.mkdtemp()\n        self.addCleanup(shutil.rmtree, temp_dir)\n        dir1 = Path(temp_dir) / 'dir1'\n        dir2 = Path(temp_dir) / 'dir2'\n        dir1.mkdir()\n        dir2.mkdir()\n\n        file1 = dir1 / 'file1.py'\n        file2 = dir2 / 'file2.py'\n\n        file1.touch()\n        file2.touch()\n\n        roots = autoreload.common_roots([file1, file2])\n        self.assertEqual(roots, (Path(temp_dir),))\n", "    def test_common_roots(self):\n        paths = [\n            Path('/home/user/project/src/app.py'),\n            Path('/home/user/project/src/utils.py'),\n            Path('/home/user/project/tests/test_app.py'),\n            Path('/home/user/other_project/src/other_app.py'),\n        ]\n        roots = autoreload.common_roots(paths)\n        self.assertEqual(roots, (Path('/home/user/project'),))\n"], "sample_302": ["def test_settings_to_cmd_args_env(self):\n    settings_dict = {\n        'HOST': 'localhost',\n        'PORT': 5432,\n        'NAME': 'mydb',\n        'USER': 'myuser',\n        'PASSWORD': 'mypassword',\n        'OPTIONS': {\n            'passfile': '/path/to/passfile',\n            'service': 'myservice',\n            'sslmode': 'require',\n            'sslrootcert': '/path/to/sslrootcert',\n            'sslcert': '/path/to/sslcert',\n            'sslkey': '/path/to/sslkey',\n        }\n    }\n    parameters = ['--extra-param']\n\n    with mock.patch.object(DatabaseClient, 'executable_name', 'psql_mock'):\n        args, env = DatabaseClient.settings_to_cmd_args_env(settings_dict, parameters)\n\n    expected_args = [\n        'psql_mock',\n        '-U', 'myuser',\n        '-h', 'localhost',\n        '-p', '5432',\n        'mydb',\n        '--extra-param'\n    ]\n    expected_env = {\n        'PGPASSWORD': 'mypassword',\n        'PGSERVICE': 'myservice',\n        'PGSSLMODE': 'require',\n        'PGSSLROOTCERT': '/path/to/sslrootcert',\n        'PGSSLCERT': '/path/to/sslcert',\n        'PGSSLKEY': '/path/to/sslkey',\n        'PGPASSFILE': '/path/to/passfile',\n    }\n\n    self.assertEqual(args, expected_args)\n    self.assertEqual(env, expected_env)\n", "def test_settings_to_cmd_args_env(self):\n    settings_dict = {\n        'HOST': 'localhost',\n        'PORT': 5432,\n        'NAME': 'mydatabase',\n        'USER': 'myuser',\n        'PASSWORD': 'mypassword',\n        'OPTIONS': {\n            'passfile': '/path/to/passfile',\n            'service': 'myservice',\n            'sslmode': 'require',\n            'sslrootcert': '/path/to/sslrootcert',\n            'sslcert': '/path/to/sslcert',\n            'sslkey': '/path/to/sslkey',\n        }\n    }\n    parameters = ['--extra-param']\n    args, env = DatabaseClient.settings_to_cmd_args_env(settings_dict, parameters)\n\n    self.assertEqual(args, ['psql', '-U', 'myuser', '-h', 'localhost', '-p', '5432', 'mydatabase', '--extra-param'])\n    self.assertEqual(env, {\n        'PGPASSWORD': 'mypassword',\n        'PGSERVICE': 'myservice',\n        'PGSSLMODE': 'require',\n        'PGSSLROOTCERT': '/path/to/sslrootcert',\n        'PGSSLCERT': '/path/to/sslcert',\n        'PGSSLKEY': '/path/to/sslkey',\n        'PGPASSFILE': '/path/to/passfile',\n    })\n", "def test_settings_to_cmd_args_env(self):\n    settings_dict = {\n        'HOST': 'localhost',\n        'PORT': 5432,\n        'NAME': 'mydatabase',\n        'USER': 'myuser',\n        'PASSWORD': 'mypassword',\n        'OPTIONS': {\n            'passfile': '/path/to/passfile',\n            'service': 'myservice',\n            'sslmode': 'require',\n            'sslrootcert': '/path/to/sslrootcert',\n            'sslcert': '/path/to/sslcert',\n            'sslkey': '/path/to/sslkey',\n        },\n    }\n    parameters = ['--extra-param']\n    args, env = DatabaseClient.settings_to_cmd_args_env(settings_dict, parameters)\n    self.assertEqual(args, ['psql', '-U', 'myuser', '-h', 'localhost', '-p', '5432', 'mydatabase', '--extra-param'])\n    self.assertEqual(env, {\n        'PGPASSWORD': 'mypassword',\n        'PGSERVICE': 'myservice',\n        'PGSSLMODE': 'require',\n        'PGSSLROOTCERT': '/path/to/sslrootcert',\n        'PGSSLCERT': '/path/to/sslcert',\n        'PGSSLKEY': '/path/to/sslkey',\n        'PGPASSFILE': '/path/to/passfile',\n    })\n", "def test_settings_to_cmd_args_env(self):\n    settings_dict = {\n        'HOST': 'localhost',\n        'PORT': 5432,\n        'NAME': 'mydatabase',\n        'USER': 'myuser',\n        'PASSWORD': 'mypassword',\n        'OPTIONS': {\n            'passfile': '/path/to/passfile',\n            'service': 'myservice',\n            'sslmode': 'require',\n            'sslrootcert': '/path/to/sslrootcert',\n            'sslcert': '/path/to/sslcert',\n            'sslkey': '/path/to/sslkey',\n        }\n    }\n    parameters = ['--extra-param']\n\n    with mock.patch.object(DatabaseClient, 'executable_name', 'psql_mock'):\n        args, env = DatabaseClient.settings_to_cmd_args_env(settings_dict, parameters)\n\n    self.assertEqual(args, ['psql_mock', '-U', 'myuser', '-h', 'localhost', '-p', '5432', 'mydatabase', '--extra-param'])\n    self.assertEqual(env, {\n        'PGPASSWORD': 'mypassword',\n        'PGSERVICE': 'myservice',\n        'PGSSLMODE': 'require',\n        'PGSSLROOTCERT': '/path/to/sslrootcert',\n        'PGSSLCERT': '/path/to/sslcert',\n        'PGSSLKEY': '/path/to/sslkey',\n        'PGPASSFILE': '/path/to/passfile',\n    })\n", "def test_settings_to_cmd_args_env(self):\n    settings_dict = {\n        'HOST': 'localhost',\n        'PORT': 5432,\n        'NAME': 'mydb',\n        'USER': 'myuser',\n        'PASSWORD': 'mypassword',\n        'OPTIONS': {\n            'passfile': '/path/to/passfile',\n            'service': 'myservice',\n            'sslmode': 'require',\n            'sslrootcert': '/path/to/sslrootcert',\n            'sslcert': '/path/to/sslcert',\n            'sslkey': '/path/to/sslkey'\n        }\n    }\n    parameters = ['--extra-param']\n    args, env = DatabaseClient.settings_to_cmd_args_env(settings_dict, parameters)\n    self.assertEqual(args, ['psql', '-U', 'myuser', '-h', 'localhost', '-p', '5432', 'mydb', '--extra-param'])\n    self.assertEqual(env, {\n        'PGPASSWORD': 'mypassword',\n        'PGSERVICE': 'myservice',\n        'PGSSLMODE': 'require',\n        'PGSSLROOTCERT': '/path/to/sslrootcert',\n        'PGSSLCERT': '/path/to/sslcert',\n        'PGSSLKEY': '/path/to/sslkey',\n        'PGPASSFILE': '/path/to/passfile'\n    })\n"], "sample_303": ["def test_runshell_raises_when_settings_to_cmd_args_env_not_implemented(self):\n    with mock.patch.object(self.client, 'settings_to_cmd_args_env', side_effect=NotImplementedError):\n        with self.assertRaises(NotImplementedError):\n            self.client.runshell(parameters={})\n", "def test_settings_to_cmd_args_env_not_implemented(self):\n    with self.assertRaises(NotImplementedError):\n        self.client.settings_to_cmd_args_env(connection.settings_dict, parameters={})\n", "def test_settings_to_cmd_args_env_not_implemented(self):\n    with self.assertRaises(NotImplementedError):\n        self.client.settings_to_cmd_args_env({}, [])\n", "def test_runshell_raises_not_implemented_error(self):\n    with self.assertRaises(NotImplementedError):\n        self.client.runshell(parameters={})\n", "def test_runshell_raises_not_implemented_error(self):\n    with self.assertRaises(NotImplementedError):\n        self.client.runshell(parameters={})\n"], "sample_304": ["    def test_decimal_validator_trailing_zeros(self):\n        # Trailing zeros should not affect validation.\n        validator = DecimalValidator(max_digits=4, decimal_places=2)\n        self.assertIsNone(validator(Decimal('12.34')))\n        self.assertIsNone(validator(Decimal('12.3400')))\n        with self.assertRaisesMessage(ValidationError, 'Ensure that there are no more than 2 decimal places.'):\n            validator(Decimal('12.3401'))\n", "    def test_deconstructible(self):\n        # Make sure the validators are deconstructible.\n        # This is necessary for them to be serialized in migrations.\n\n        for cls in [\n            RegexValidator, URLValidator, EmailValidator,\n            MaxValueValidator, MinValueValidator, MaxLengthValidator,\n            MinLengthValidator, DecimalValidator, FileExtensionValidator,\n            ProhibitNullCharactersValidator,\n        ]:\n            instance = cls()\n            self.assertEqual(\n                instance.deconstruct(),\n                (f'django.core.validators.{cls.__name__}', (), {}),\n            )\n\n        class CustomRegexValidator(RegexValidator):\n            regex = re.compile('foo')\n            message = 'Custom message'\n            code = 'custom'\n\n        instance = CustomRegexValidator()\n        self.assertEqual(\n            instance.deconstruct(),\n            (\n                'django.core.validators.RegexValidator',\n                (),\n                {\n                    'regex': 'foo',\n                    'message': 'Custom message',\n                    'code': 'custom',\n                },\n            ),\n        )\n", "def test_url_validator_with_idn(self):\n    # Test with Internationalized Domain Names (IDN)\n    idn_url = 'http://xn--80ak6aa92e.com'\n    self.assertIsNone(URLValidator()(idn_url))\n\n    # Test with IDN and non-ASCII characters in path\n    idn_url_with_path = 'http://xn--80ak6aa92e.com/%D0%BF%D1%83%D1%82%D1%8C'\n    self.assertIsNone(URLValidator()(idn_url_with_path))\n", "def test_email_validator_domain_allowlist(self):\n    validator = EmailValidator(allowlist=['example.com'])\n    self.assertIsNone(validator('test@example.com'))\n    with self.assertRaisesMessage(ValidationError, 'Enter a valid email address.'):\n        validator('test@otherdomain.com')\n", "    def test_deconstructible(self):\n        # Test deconstructible validators\n        for validator in [\n            RegexValidator(),\n            URLValidator(),\n            EmailValidator(),\n            validate_integer,\n            validate_email,\n            validate_ipv4_address,\n            validate_ipv6_address,\n            validate_ipv46_address,\n            validate_slug,\n            validate_unicode_slug,\n            MaxValueValidator(10),\n            MinValueValidator(-10),\n            MaxLengthValidator(10),\n            MinLengthValidator(10),\n            DecimalValidator(max_digits=2, decimal_places=1),\n            FileExtensionValidator(['txt']),\n            ProhibitNullCharactersValidator(),\n        ]:\n            self.assertEqual(validator.__class__.deconstruct(validator), (\n                '%s.%s' % (validator.__class__.__module__, validator.__class__.__name__),\n                (),\n                {},\n            ))\n"], "sample_305": ["def test_lookup_inheritance(self):\n    # Ensure lookups can be inherited and overridden.\n    class MyLookup(BuiltinLookup):\n        lookup_name = 'exact'\n\n    class MyField(Field):\n        pass\n\n    with self.assertRaises(FieldError):\n        MyField.register_lookup(MyLookup)\n\n    class MyLookup2(MyLookup):\n        lookup_name = 'contains'\n\n    MyField.register_lookup(MyLookup2)\n    self.assertIn('contains', MyField.get_lookups())\n", "def test_lookup_inheritance(self):\n    # Test that lookups are inherited correctly.\n    qs = Book.objects.filter(publisher__in=[self.p1, self.p2])\n    self.assertQuerysetEqual(qs, [self.b1, self.b3, self.b2], attrgetter('name'))\n\n    qs = Publisher.objects.filter(book__in=[self.b1, self.b3])\n    self.assertQuerysetEqual(qs, [self.p1], attrgetter('name'))\n", "def test_annotation(self):\n    # Make sure annotation won't interfere with existing filters.\n    books = Book.objects.filter(pages__gt=300).annotate(total_authors=Count('authors'))\n    self.assertEqual(len(books), 4)\n    for book in books:\n        self.assertEqual(book.pages > 300, True)\n        self.assertEqual(book.total_authors, book.authors.count())\n", "def test_lookup_value_is_direct(self):\n    lookup = Book.objects.filter(pages__gt=200)\n    self.assertTrue(lookup.query.where.children[0].rhs_is_direct_value())\n\n    lookup = Book.objects.filter(pages__gt=F('pages'))\n    self.assertFalse(lookup.query.where.children[0].rhs_is_direct_value())\n", "def test_annotation_with_database_function_and_filter(self):\n    from django.db.models.functions import Upper\n\n    books = Book.objects.annotate(\n        upper_name=Upper('name')\n    ).filter(upper_name__contains='DJANGO').order_by('pk')\n\n    self.assertEqual(list(books), [self.b1, self.b2, self.b4])\n"], "sample_306": ["def test_parse_date(self):\n    self.assertEqual(parse_date('2014-06-23'), date(2014, 6, 23))\n    self.assertIsNone(parse_date('2014-06-32'))\n    self.assertIsNone(parse_date('2014-13-23'))\n    self.assertIsNone(parse_date('2014-ab-23'))\n    self.assertIsNone(parse_date('invalid'))\n", "def test_parse_date(self):\n    self.assertEqual(parse_date('2012-02-26'), date(2012, 2, 26))\n    self.assertIsNone(parse_date('2012-2-26'))  # zero-padding required\n    self.assertIsNone(parse_date('20120226'))   # dashes required\n    self.assertIsNone(parse_date('2012-13-26')) # invalid month\n    self.assertIsNone(parse_date('2012-02-30')) # invalid day\n", "def test_parse_time_microseconds(self):\n    self.assertEqual(parse_time('12:34:56.123456'), time(12, 34, 56, 123456))\n    self.assertEqual(parse_time('12:34:56.123'), time(12, 34, 56, 123000))\n    self.assertEqual(parse_time('12:34:56'), time(12, 34, 56))\n    self.assertIsNone(parse_time('12:34:56.1234567'))  # too many digits for microseconds\n", "def test_parse_date(self):\n    self.assertEqual(parse_date('2013-03-12'), date(2013, 3, 12))\n    self.assertIsNone(parse_date('2013-03'))\n    self.assertIsNone(parse_date('2013'))\n    self.assertIsNone(parse_date('abc'))\n    self.assertEqual(parse_date('2022-02-30'), None)  # invalid date\n    self.assertEqual(parse_date('2022-13-01'), None)  # invalid month\n    self.assertEqual(parse_date('2022-01-32'), None)  # invalid day\n", "def test_parse_time_microseconds(self):\n    self.assertEqual(parse_time('12:34:56.123'), time(12, 34, 56, 123000))\n    self.assertEqual(parse_time('12:34:56.123456'), time(12, 34, 56, 123456))\n    self.assertIsNone(parse_time('12:34:56.1234567'))  # More than 6 digits for microseconds\n"], "sample_307": ["def test_date_format(self):\n    d = date(2009, 2, 12)\n    self.assertEqual(dateformat.format(d, 'b'), 'Feb')\n    self.assertEqual(dateformat.format(d, 'c'), '2009-02-12')\n    self.assertEqual(dateformat.format(d, 'd'), '12')\n    self.assertEqual(dateformat.format(d, 'D'), 'Thu')\n    self.assertEqual(dateformat.format(d, 'E'), 'February')\n    self.assertEqual(dateformat.format(d, 'F'), 'February')\n    self.assertEqual(dateformat.format(d, 'I'), '')\n    self.assertEqual(dateformat.format(d, 'j'), '12')\n    self.assertEqual(dateformat.format(d, 'l'), 'Thursday')\n    self.assertEqual(dateformat.format(d, 'L'), False)\n    self.assertEqual(dateformat.format(d, 'm'), '02')\n    self.assertEqual(dateformat.format(d, 'M'), 'Feb')\n    self.assertEqual(dateformat.format(d, 'n'), '2')\n    self.assertEqual(dateformat.format(d, 'N'), 'Feb.')\n    self.assertEqual(dateformat.format(d, 'o'), '2009')\n    self.assertEqual(dateformat.format(d, 'r'), '')\n    self.assertEqual(dateformat.format(d, 'S'), 'th')\n    self.assertEqual(dateformat.format(d, 't'), '28')\n    self.assertEqual(dateformat.format(d, 'U'), '1234320000')\n    self.assertEqual(dateformat.format(d, 'w'), '4')\n    self.assertEqual(dateformat.format(d, 'W'), '7')\n    self.assertEqual(dateformat.format(d, 'y'), '09')\n    self.assertEqual(dateformat.format(d, 'Y'), '2009')\n    self.assertEqual(dateformat.format(d, 'z'), '43')\n    self.assertEqual(dateformat.format(d, 'Z'), '')\n", "def test_day_of_month(self):\n    my_birthday = date(1984, 6, 6)\n    formatted = format(my_birthday, 'd')\n    self.assertEqual(formatted, '06')\n\n    my_birthday = date(1984, 12, 25)\n    formatted = format(my_birthday, 'j')\n    self.assertEqual(formatted, '25')\n", "def test_date_format(self):\n    d = date(2009, 2, 12)\n    self.assertEqual(dateformat.format(d, 'b'), 'Feb')\n    self.assertEqual(dateformat.format(d, 'c'), '2009-02-12')\n    self.assertEqual(dateformat.format(d, 'd'), '12')\n    self.assertEqual(dateformat.format(d, 'D'), 'Thu')\n    self.assertEqual(dateformat.format(d, 'E'), 'February')\n    self.assertEqual(dateformat.format(d, 'F'), 'February')\n    self.assertEqual(dateformat.format(d, 'I'), '')\n    self.assertEqual(dateformat.format(d, 'j'), '12')\n    self.assertEqual(dateformat.format(d, 'l'), 'Thursday')\n    self.assertEqual(dateformat.format(d, 'L'), False)\n    self.assertEqual(dateformat.format(d, 'm'), '02')\n    self.assertEqual(dateformat.format(d, 'M'), 'Feb')\n    self.assertEqual(dateformat.format(d, 'n'), '2')\n    self.assertEqual(dateformat.format(d, 'N'), 'Feb.')\n    self.assertEqual(dateformat.format(d, 'o'), '2009')\n    self.assertEqual(dateformat.format(d, 'S'), 'th')\n    self.assertEqual(dateformat.format(d, 't'), '28')\n    self.assertEqual(dateformat.format(d, 'U'), '1234567890')\n    self.assertEqual(dateformat.format(d, 'w'), '4')\n    self.assertEqual(dateformat.format(d, 'W'), '7')\n    self.assertEqual(dateformat.format(d, 'y'), '09')\n    self.assertEqual(dateformat.format(d, 'Y'), '2009')\n    self.assertEqual(dateformat.format(d, 'z'), '43')\n", "def test_date_format(self):\n    d = date(2009, 2, 12)\n    self.assertEqual(dateformat.format(d, 'Y'), '2009')\n    self.assertEqual(dateformat.format(d, 'y'), '09')\n    self.assertEqual(dateformat.format(d, 'm'), '02')\n    self.assertEqual(dateformat.format(d, 'n'), '2')\n    self.assertEqual(dateformat.format(d, 'd'), '12')\n    self.assertEqual(dateformat.format(d, 'j'), '12')\n    self.assertEqual(dateformat.format(d, 'l'), 'Thursday')\n    self.assertEqual(dateformat.format(d, 'D'), 'Thu')\n    self.assertEqual(dateformat.format(d, 'F'), 'February')\n    self.assertEqual(dateformat.format(d, 'M'), 'Feb')\n    self.assertEqual(dateformat.format(d, 't'), '28')\n", "def test_date_format(self):\n    dt = date(2022, 9, 1)\n    self.assertEqual(format(dt, 'b'), 'Sep')\n    self.assertEqual(format(dt, 'd'), '01')\n    self.assertEqual(format(dt, 'D'), 'Thu')\n    self.assertEqual(format(dt, 'E'), 'Sept')\n    self.assertEqual(format(dt, 'F'), 'September')\n    self.assertEqual(format(dt, 'I'), '')\n    self.assertEqual(format(dt, 'j'), '1')\n    self.assertEqual(format(dt, 'l'), 'Thursday')\n    self.assertEqual(format(dt, 'L'), False)\n    self.assertEqual(format(dt, 'm'), '09')\n    self.assertEqual(format(dt, 'M'), 'Sep')\n    self.assertEqual(format(dt, 'n'), '9')\n    self.assertEqual(format(dt, 'N'), 'Sept.')\n    self.assertEqual(format(dt, 'o'), '2022')\n    self.assertEqual(format(dt, 'r'), 'Thu, 01 Sep 2022 00:00:00 +0200')\n    self.assertEqual(format(dt, 'S'), 'st')\n    self.assertEqual(format(dt, 't'), '30')\n    self.assertEqual(format(dt, 'U'), '1661990400')\n    self.assertEqual(format(dt, 'w'), '4')\n    self.assertEqual(format(dt, 'W'), '35')\n    self.assertEqual(format(dt, 'y'), '22')\n    self.assertEqual(format(dt, 'Y'), '2022')\n    self.assertEqual(format(dt, 'z'), '244')\n"], "sample_308": ["def test_b(self):\n    dt = date(2022, 9, 1)\n    self.assertEqual(format(dt, 'b'), 'sep')\n", "def test_date_format_b(self):\n    dt = date(2022, 9, 1)\n    self.assertEqual(dateformat.format(dt, 'b'), 'sep')\n    dt = date(2022, 12, 1)\n    self.assertEqual(dateformat.format(dt, 'b'), 'dec')\n", "def test_b(self):\n    dt = date(2022, 9, 1)\n    self.assertEqual(format(dt, 'b'), 'sep')\n", "def test_date_format(self):\n    dt = date(2022, 9, 1)\n    self.assertEqual(format(dt, 'b'), 'Sep')\n    self.assertEqual(format(dt, 'c'), '2022-09-01')\n    self.assertEqual(format(dt, 'd'), '01')\n    self.assertEqual(format(dt, 'D'), 'Thu')\n    self.assertEqual(format(dt, 'E'), 'September')\n    self.assertEqual(format(dt, 'F'), 'September')\n    self.assertEqual(format(dt, 'I'), '')\n    self.assertEqual(format(dt, 'j'), '1')\n    self.assertEqual(format(dt, 'l'), 'Thursday')\n    self.assertEqual(format(dt, 'L'), False)\n    self.assertEqual(format(dt, 'm'), '09')\n    self.assertEqual(format(dt, 'M'), 'Sep')\n    self.assertEqual(format(dt, 'n'), '9')\n    self.assertEqual(format(dt, 'N'), 'Sept.')\n    self.assertEqual(format(dt, 'o'), '2022')\n    self.assertEqual(format(dt, 'S'), 'st')\n    self.assertEqual(format(dt, 't'), '30')\n    self.assertEqual(format(dt, 'U'), '1661990400')\n    self.assertEqual(format(dt, 'w'), '4')\n    self.assertEqual(format(dt, 'W'), '35')\n    self.assertEqual(format(dt, 'y'), '22')\n    self.assertEqual(format(dt, 'Y'), '2022')\n    self.assertEqual(format(dt, 'z'), '244')\n", "def test_date_format(self):\n    d = date(2009, 2, 12)\n    self.assertEqual(dateformat.format(d, 'd'), '12')\n    self.assertEqual(dateformat.format(d, 'ds'), '12th')\n    self.assertEqual(dateformat.format(d, 'D'), 'Thu')\n    self.assertEqual(dateformat.format(d, 'E'), 'February')\n    self.assertEqual(dateformat.format(d, 'F'), 'February')\n    self.assertEqual(dateformat.format(d, 'I'), '')\n    self.assertEqual(dateformat.format(d, 'j'), '12')\n    self.assertEqual(dateformat.format(d, 'l'), 'Thursday')\n    self.assertEqual(dateformat.format(d, 'L'), False)\n    self.assertEqual(dateformat.format(d, 'm'), '02')\n    self.assertEqual(dateformat.format(d, 'M'), 'Feb')\n    self.assertEqual(dateformat.format(d, 'n'), '2')\n    self.assertEqual(dateformat.format(d, 'N'), 'Feb.')\n    self.assertEqual(dateformat.format(d, 'o'), '2009')\n    self.assertEqual(dateformat.format(d, 'r'), 'Thu, 12 Feb 2009 00:00:00 +0100')\n    self.assertEqual(dateformat.format(d, 'S'), 'th')\n    self.assertEqual(dateformat.format(d, 't'), '28')\n    self.assertEqual(dateformat.format(d, 'U'), '1234368000')\n    self.assertEqual(dateformat.format(d, 'w'), '4')\n    self.assertEqual(dateformat.format(d, 'W'), '7')\n    self.assertEqual(dateformat.format(d, 'y'), '09')\n    self.assertEqual(dateformat.format(d, 'Y'), '2009')\n    self.assertEqual(dateformat.format(d, 'z'), '43')\n"], "sample_309": ["def test_urlencode_with_multivalue_dict(self):\n    mvd = MultiValueDict({'a': ['1', '2'], 'b': ['3']})\n    self.assertEqual(urlencode(mvd), 'a=1&a=2&b=3')\n    self.assertEqual(urlencode(mvd, doseq=True), 'a=1&a=2&b=3')\n\n    mvd = MultiValueDict({'a': [1, 2], 'b': [3]})\n    self.assertEqual(urlencode(mvd), 'a=1&a=2&b=3')\n    self.assertEqual(urlencode(mvd, doseq=True), 'a=1&a=2&b=3')\n\n    mvd = MultiValueDict({'a': ['1', None], 'b': ['3']})\n    with self.assertRaisesMessage(TypeError, self.cannot_encode_none_msg):\n        urlencode(mvd)\n", "    def test_roundtrip(self):\n        bytestrings = [\n            b'',\n            b'f',\n            b'fo',\n            b'foo',\n            b'qux' * 50,\n            b'\\x00\\x01\\x02',\n            b'\\xfd\\xfe\\xff',\n        ]\n        for bytestring in bytestrings:\n            encoded = urlsafe_base64_encode(bytestring)\n            self.assertIsInstance(encoded, str)\n            decoded = urlsafe_base64_decode(encoded)\n            self.assertEqual(decoded, bytestring)\n", "    def test_roundtrip(self):\n        bytestrings = [b'Hello, World!', b'\\x00\\x01\\x02', b'']\n        for bytestr in bytestrings:\n            encoded = urlsafe_base64_encode(bytestr)\n            decoded = urlsafe_base64_decode(encoded)\n            self.assertEqual(bytestr, decoded)\n", "    def test_urlsafe_base64_encode_decode(self):\n        bytestring = b'Hello, World!'\n        encoded = urlsafe_base64_encode(bytestring)\n        self.assertEqual(urlsafe_base64_decode(encoded), bytestring)\n", "    def test_urlsafe_base64_encode(self):\n        s = b'Hello, World!'\n        encoded = urlsafe_base64_encode(s)\n        self.assertEqual(urlsafe_base64_decode(encoded), s)\n"], "sample_310": ["def test_model_detail_view(self):\n    model_name = Company._meta.object_name.lower()\n    url = reverse('django-admindocs:model-detail', args=(Company._meta.app_label, model_name))\n    response = self.client.get(url)\n    self.assertEqual(response.status_code, 200)\n    self.assertTemplateUsed(response, 'admin_doc/model_detail.html')\n    self.assertIn(model_name, response.context['name'])\n    self.assertIsInstance(response.context['fields'], list)\n    self.assertIsInstance(response.context['methods'], list)\n", "def test_get_return_data_type(self):\n    self.assertEqual(get_return_data_type('get_user_list'), 'List')\n    self.assertEqual(get_return_data_type('get_user_count'), 'Integer')\n    self.assertEqual(get_return_data_type('get_user'), '')\n", "def test_bookmarklets_view(self):\n    response = self.client.get(reverse('django-admindocs-bookmarklets'))\n    self.assertEqual(response.status_code, 200)\n    self.assertTemplateUsed(response, 'admin_doc/bookmarklets.html')\n", "def test_bookmarklets_view(self):\n    response = self.client.get(reverse('django-admindocs-bookmarklets'))\n    self.assertEqual(response.status_code, 200)\n    self.assertTemplateUsed(response, 'admin_doc/bookmarklets.html')\n", "def test_bookmarklets_view(self):\n    response = self.client.get(reverse('django-admindocs-bookmarklets'))\n    self.assertEqual(response.status_code, 200)\n    self.assertTemplateUsed(response, 'admin_doc/bookmarklets.html')\n"], "sample_312": ["def test_add_squash(self):\n    node3 = Node([('c', 3)])\n    result = self.node1.add(node3, 'DEFAULT')\n    self.assertEqual(result, node3)\n    self.assertEqual(len(self.node1.children), 3)\n    self.assertIn(('c', 3), self.node1.children)\n", "def test_add_squash(self):\n    node3 = Node([('c', 3), ('d', 4)])\n    result = self.node1.add(node3, 'DEFAULT', squash=True)\n    self.assertEqual(len(self.node1.children), 4)\n    self.assertEqual(result, node3)\n", "def test_add_with_squash(self):\n    node3 = Node([('c', 3)])\n    result = self.node1.add(node3, 'DEFAULT')\n    self.assertEqual(result, node3)\n    self.assertEqual(self.node1.children, self.node1_children + [('c', 3)])\n", "def test_node_str(self):\n    self.assertEqual(str(self.node1), '(DEFAULT: a, b)')\n    self.node1.negate()\n    self.assertEqual(str(self.node1), '(NOT (DEFAULT: a, b))')\n    self.node1.negate()\n    node2 = Node([('c', 3)], 'OR')\n    self.assertEqual(str(node2), '(OR: c)')\n", "def test_node_add(self):\n    node3_children = [('c', 3), ('d', 4)]\n    node3 = Node(node3_children)\n    self.node1.add(node3, 'DEFAULT')\n    self.assertEqual(len(self.node1.children), 4)\n    self.assertIn(('a', 1), self.node1.children)\n    self.assertIn(('b', 2), self.node1.children)\n    self.assertIn(('c', 3), self.node1.children)\n    self.assertIn(('d', 4), self.node1.children)\n"], "sample_313": ["def test_watch_for_template_changes(self, mock_get_template_directories):\n    autoreload_started.send(sender=None)\n    mock_get_template_directories.assert_called_once()\n    for directory in mock_get_template_directories.return_value:\n        self.assertTrue(autoreload.watched_files.watching(directory))\n", "def test_watch_for_template_changes(self, mock_get_template_directories):\n    mock_get_template_directories.return_value = [EXTRA_TEMPLATES_DIR]\n    sender = mock.Mock()\n    watch_for_template_changes(sender)\n    sender.watch_dir.assert_called_once_with(EXTRA_TEMPLATES_DIR, '**/*')\n", "def test_watch_for_template_changes(self):\n    with mock.patch('django.utils.autoreload.autoreload_started.send') as send_mock:\n        watch_for_template_changes(autoreload)\n        self.assertEqual(send_mock.call_count, 1)\n        args, kwargs = send_mock.call_args\n        self.assertEqual(args, (autoreload,))\n        self.assertEqual(kwargs['dispatch_uid'], 'template_loaders_watch_changes')\n        self.assertIn(EXTRA_TEMPLATES_DIR, kwargs['sender'].watched_dirs)\n", "    def test_get_template_directories(self):\n        dirs = autoreload.get_template_directories()\n        self.assertIn(EXTRA_TEMPLATES_DIR, dirs)\n        # Test that Django templates are filtered out\n        django_template_dir = Path(__file__).parent.absolute() / \"templates\"\n        self.assertNotIn(django_template_dir, dirs)\n", "def test_get_template_directories(self):\n    template_dirs = autoreload.get_template_directories()\n    self.assertIn(EXTRA_TEMPLATES_DIR, template_dirs)\n    self.assertIsInstance(template_dirs, set)\n\n    # Test that Django templates are filtered out\n    with mock.patch('django.utils._os.to_path', return_value='django/template'):\n        template_dirs = autoreload.get_template_directories()\n        self.assertNotIn(EXTRA_TEMPLATES_DIR, template_dirs)\n"], "sample_314": ["    def test_widget(self):\n        field = ReadOnlyPasswordHashField()\n        self.assertIsInstance(field.widget, ReadOnlyPasswordHashWidget)\n", "    def test_user_already_exists(self):\n        data = {'username': 'testclient', 'password1': 'test123', 'password2': 'test123'}\n        form = UserCreationForm(data)\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['username'], [_('A user with that username already exists.')])\n", "    def test_widget_rendering(self):\n        field = ReadOnlyPasswordHashField()\n        html = field.widget.render('password', 'sha1$1234567890abcdef$1234567890abcdef1234567890abcdef')\n        self.assertHTMLEqual(\n            '<div><strong>password:</strong> sha1$1234567890abcdef$1234567890abcdef1234567890abcdef</div>',\n            html\n        )\n", "    def test_username_max_length(self):\n        # The max_length for username in User model is 150.\n        form = UserChangeForm(data={'username': 'a' * 151})\n        self.assertFalse(form.is_valid())\n        self.assertEqual(len(form.errors), 1)\n        self.assertIn('username', form.errors)\n", "    def test_readonlypasswordhashfield_widget(self):\n        field = ReadOnlyPasswordHashField()\n        widget = field.widget\n        self.assertIsInstance(widget, ReadOnlyPasswordHashWidget)\n"], "sample_315": ["    def test_process_request(self):\n        request = RequestFactory().get('/path')\n        middleware = LocaleMiddleware()\n        middleware.process_request(request)\n        self.assertEqual(request.LANGUAGE_CODE, 'en-us')\n", "    def test_process_request(self):\n        # Create a request with a specific language in the session\n        request = RequestFactory().get('/path/')\n        request.session = {'django_language': 'pt-br'}\n        LocaleMiddleware().process_request(request)\n        self.assertEqual(translation.get_language(), 'pt-br')\n        self.assertEqual(request.LANGUAGE_CODE, 'pt-br')\n", "    def test_language_from_request(self):\n        rf = RequestFactory()\n        request = rf.get('/test/')\n        request.META['HTTP_ACCEPT_LANGUAGE'] = 'nl'\n        middleware = LocaleMiddleware()\n\n        middleware.process_request(request)\n\n        self.assertEqual(translation.get_language(), 'nl')\n", "    def test_process_request(self):\n        request = RequestFactory().get('/path')\n        middleware = LocaleMiddleware()\n        middleware.process_request(request)\n        self.assertEqual(translation.get_language(), 'en-us')\n", "    def test_accept_language(self):\n        # Set up a request factory with a non-English Accept-Language header\n        rf = RequestFactory(HTTP_ACCEPT_LANGUAGE='pt-br')\n\n        # Create a request to a URL that does not contain a language prefix\n        request = rf.get('/test/')\n\n        # Run the middleware\n        middleware = LocaleMiddleware()\n        middleware.process_request(request)\n\n        # Check if the correct language was activated\n        self.assertEqual(translation.get_language(), 'pt-br')\n"], "sample_316": ["def test_get_image_dimensions_with_corrupted_image(self):\n    # Create a corrupted image file\n    corrupted_image = BytesIO(b'not a valid image')\n\n    # Test that get_image_dimensions returns (None, None) for corrupted images\n    dimensions = images.get_image_dimensions(corrupted_image)\n    self.assertEqual(dimensions, (None, None))\n\n    # Test that the file is closed after attempting to read it\n    with mock.patch.object(corrupted_image, 'close') as close:\n        images.get_image_dimensions(corrupted_image, close=True)\n        close.assert_called_once()\n", "def test_get_image_dimensions_from_path(self):\n    with tempfile.NamedTemporaryFile(suffix='.jpg') as tmp_file:\n        # Create a simple JPEG image\n        img = Image.new('RGB', (100, 200))\n        img.save(tmp_file.name)\n        \n        # Get dimensions from path\n        dimensions = images.get_image_dimensions(tmp_file.name)\n        \n        self.assertEqual(dimensions, (100, 200))\n", "def test_get_image_dimensions_from_file(self):\n    if Image is None:\n        self.skipTest(\"PIL is not installed\")\n\n    with tempfile.NamedTemporaryFile(mode='wb', delete=False) as tmp_file:\n        # Create a small PNG image\n        img = Image.new('RGB', (10, 10))\n        img.save(tmp_file.name, 'PNG')\n\n        dimensions = images.get_image_dimensions(tmp_file.name)\n        self.assertEqual(dimensions, (10, 10))\n\n    os.remove(tmp_file.name)\n", "def test_get_image_dimensions_from_file_path(self):\n    # Create a temporary image file\n    with tempfile.NamedTemporaryFile(suffix='.png', delete=True) as tmp:\n        # Use PIL to create a small image and save it to the temporary file\n        img = Image.new('RGB', (10, 10))\n        img.save(tmp.name)\n\n        # Get the dimensions using the function being tested\n        width, height = images.get_image_dimensions(tmp.name)\n\n        # Verify the dimensions are correct\n        self.assertEqual(width, 10)\n        self.assertEqual(height, 10)\n", "def test_image_file_dimensions(self):\n    img_file = images.ImageFile(BytesIO(b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x08\\x02\\x00\\x00\\x00\\x90wS\\xde\\x00\\x00\\x00\\x01sRGB\\x00\\xae\\xce\\x1c\\xe9\\x00\\x00\\x00\\x04gAMA\\x00\\x00\\xb1\\x8f\\x0b\\xfca\\x05\\x00\\x00\\x00 cHRM\\x00\\x00z&\\x00\\x00\\x80\\x84\\x00\\x00\\xfa\\x00\\x00\\x00\\x80\\xe8\\x00\\x00u0\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00'))\n    self.assertEqual(img_file.width, 1)\n    self.assertEqual(img_file.height, 1)\n\n    # Test caching of dimensions\n    with mock.patch.object(img_file, '_get_image_dimensions') as get_dims:\n        img_file.width\n        img_file.height\n        get_dims.assert_called_once()\n"], "sample_317": ["def test_rss_feed(self):\n    feed = Rss201rev2Feed(\n        title='Test Feed', link='http://example.com/', description='A test feed'\n    )\n    feed.add_item(title='Item 1', link='http://example.com/item1/', description='The first item')\n    feed.add_item(title='Item 2', link='http://example.com/item2/', description='The second item')\n\n    xml = feed.writeString('utf-8')\n    doc = minidom.parseString(xml)\n    channel = doc.getElementsByTagName('channel')[0]\n\n    self.assertChildNodes(channel, ['title', 'link', 'description', 'item', 'item'])\n    self.assertChildNodeContent(channel, {\n        'title': 'Test Feed',\n        'link': 'http://example.com/',\n        'description': 'A test feed',\n    })\n\n    items = channel.getElementsByTagName('item')\n    self.assertEqual(len(items), 2)\n\n    self.assertChildNodes(items[0], ['title', 'link', 'description'])\n    self.assertChildNodeContent(items[0], {\n        'title': 'Item 1',\n        'link': 'http://example.com/item1/',\n        'description': 'The first item',\n    })\n\n    self.assertChildNodes(items[1], ['title', 'link', 'description'])\n    self.assertChildNodeContent(items[1], {\n        'title': 'Item 2',\n        'link': 'http://example.com/item2/',\n        'description': 'The second item',\n    })\n", "def test_rfc2822_date(self):\n    dt = datetime.datetime(2022, 1, 1, 12, 0, tzinfo=TZ)\n    self.assertEqual(\n        rfc2822_date(dt),\n        'Sat, 01 Jan 2022 12:00:00 +0000'\n    )\n\n    dt = datetime.date(2022, 1, 1)\n    self.assertEqual(\n        rfc2822_date(dt),\n        'Sat, 01 Jan 2022 00:00:00 -0000'\n    )\n", "def test_feedgenerator_rfc2822_date(self):\n    dt = datetime.datetime(2022, 1, 1, 12, 0, tzinfo=TZ)\n    expected = 'Sat, 01 Jan 2022 12:00:00 +0000'\n    self.assertEqual(rfc2822_date(dt), expected)\n\n    # Test with a naive datetime (no timezone info)\n    naive_dt = datetime.datetime(2022, 1, 1, 12, 0)\n    expected_naive = 'Sat, 01 Jan 2022 12:00:00 -0000'\n    self.assertEqual(rfc2822_date(naive_dt), expected_naive)\n\n    # Test with a date object\n    d = datetime.date(2022, 1, 1)\n    expected_date = 'Sat, 01 Jan 2022 00:00:00 -0000'\n    self.assertEqual(rfc2822_date(d), expected_date)\n", "def test_rfc2822_date(self):\n    dt = datetime.datetime(2022, 1, 1, 12, 0, tzinfo=TZ)\n    result = rfc2822_date(dt)\n    self.assertEqual(result, 'Sat, 01 Jan 2022 12:00:00 +0000')\n\n    dt = datetime.date(2022, 1, 1)\n    result = rfc2822_date(dt)\n    self.assertEqual(result, 'Sat, 01 Jan 2022 00:00:00 +0000')\n", "def test_rfc2822_date(self):\n    dt = datetime.datetime(2008, 1, 2, 12, 30)\n    self.assertEqual(rfc2822_date(dt), 'Wed, 02 Jan 2008 12:30:00 +0000')\n    dt = datetime.date(2008, 1, 2)\n    self.assertEqual(rfc2822_date(dt), 'Wed, 02 Jan 2008 00:00:00 +0000')\n"], "sample_318": ["    def test_resolve(self):\n        resolver = get_resolver(None)\n        for path, url_name, app_name, namespace, view_name, func, args, kwargs in resolve_test_data:\n            match = resolver.resolve(path)\n            self.assertEqual(match.url_name, url_name)\n            self.assertEqual(match.app_name, app_name)\n            self.assertEqual(match.namespace, namespace)\n            self.assertEqual(match.view_name, view_name)\n            self.assertEqual(match.func, func)\n            self.assertEqual(match.args, args)\n            self.assertEqual(match.kwargs, kwargs)\n", "def test_no_url_patterns(self):\n    resolver = get_resolver(None)\n    with self.assertRaises(Resolver404):\n        resolver.resolve('/example/')\n    with self.assertRaises(NoReverseMatch):\n        reverse('view-name')\n", "def test_no_urls(self):\n    resolver = get_resolver(None)\n    with self.assertRaises(Resolver404):\n        resolver.resolve('/example/')\n    with self.assertRaises(NoReverseMatch):\n        reverse('example')\n", "    def test_reverse(self):\n        for args in test_data:\n            url_name, expected, args, kwargs = args\n            try:\n                result = reverse(url_name, args=args, kwargs=kwargs)\n            except NoReverseMatch:\n                result = NoReverseMatch\n            self.assertEqual(result, expected)\n", "def test_resolver_match_repr(self):\n    match = ResolverMatch(views.empty_view, (), {}, 'some-url-name')\n    self.assertEqual(repr(match), (\n        \"ResolverMatch(func='urlpatterns_reverse.views.empty_view', args=(), \"\n        \"kwargs={}, url_name='some-url-name', app_names=[], namespaces=[], route=None)\"\n    ))\n"], "sample_321": ["def test_csrf_trusted_origins(self):\n    settings.CSRF_TRUSTED_ORIGINS = ['http://example.com', 'https://example.net']\n\n    req = self._get_POST_request_with_token()\n    req.META['HTTP_ORIGIN'] = 'http://example.com'\n    response = CsrfViewMiddleware().process_view(req, None, None, None)\n    self.assertIsNone(response)\n\n    req.META['HTTP_ORIGIN'] = 'https://example.net'\n    response = CsrfViewMiddleware().process_view(req, None, None, None)\n    self.assertIsNone(response)\n\n    req.META['HTTP_ORIGIN'] = 'http://not.example.com'\n    with self.assertRaisesMessage(RejectRequest, REASON_BAD_ORIGIN % 'http://not.example.com'):\n        CsrfViewMiddleware().process_view(req, None, None, None)\n", "def test_csrf_token_length(self):\n    token = get_token(self._get_GET_no_csrf_cookie_request())\n    self.assertEqual(len(token), CSRF_TOKEN_LENGTH)\n", "def test_csrf_view_middleware_process_response_with_csrf_cookie_used(self):\n    req = self._get_GET_csrf_cookie_request()\n    req.META['CSRF_COOKIE_USED'] = True\n    response = HttpResponse()\n    CsrfViewMiddleware().process_response(req, response)\n    self.assertTrue(hasattr(response, 'csrf_cookie_set'))\n    self.assertTrue(response.csrf_cookie_set)\n", "def test_csrf_token_length(self):\n    \"\"\"Check that the CSRF token has the correct length.\"\"\"\n    token = get_token(self._get_GET_no_csrf_cookie_request())\n    self.assertEqual(len(token), CSRF_TOKEN_LENGTH)\n", "def test_csrf_trusted_origins(self):\n    middleware = CsrfViewMiddleware()\n    request = self._get_POST_request_with_token()\n\n    # Test that a trusted origin in CSRF_TRUSTED_ORIGINS doesn't raise an error.\n    with override_settings(CSRF_TRUSTED_ORIGINS=['http://example.com']):\n        request.META['HTTP_ORIGIN'] = 'http://example.com'\n        response = middleware.process_view(request, token_view, (), {})\n        self.assertIsNone(response)\n\n    # Test that a trusted origin with a scheme and subdomain wildcard doesn't raise an error.\n    with override_settings(CSRF_TRUSTED_ORIGINS=['https://*.example.com']):\n        request.META['HTTP_ORIGIN'] = 'https://sub.example.com'\n        response = middleware.process_view(request, token_view, (), {})\n        self.assertIsNone(response)\n\n    # Test that an untrusted origin raises an error.\n    with override_settings(CSRF_TRUSTED_ORIGINS=['http://example.com']):\n        request.META['HTTP_ORIGIN'] = 'http://untrusted-site.com'\n        response = middleware.process_view(request, token_view, (), {})\n        self.assertEqual(response.status_code, 403)\n        self.assertEqual(response.reason_phrase, REASON_BAD_ORIGIN % 'http://untrusted-site.com')\n"], "sample_322": ["def test_migration_plan_with_replacements(self):\n    \"\"\"\n    Tests that replaced migrations are properly removed from the plan.\n    \"\"\"\n    executor = MigrationExecutor(connection)\n    recorder = MigrationRecorder(connection)\n    # Create a fake migration history where 0001 is applied and 0002 is not,\n    # but 0002 replaces 0001.\n    recorder.record_applied(\"migrations\", \"0001_initial\")\n    migration_graph = MigrationGraph()\n    migration_graph.add_node((\"migrations\", \"0001_initial\"), None)\n    migration_graph.add_node((\"migrations\", \"0002_second\"), [(\"migrations\", \"0001_initial\")])\n    migration_graph.add_replacement((\"migrations\", \"0002_second\"), (\"migrations\", \"0001_initial\"))\n    executor.loader.graph = migration_graph\n\n    plan = executor.migration_plan([(\"migrations\", \"0002_second\")])\n\n    self.assertEqual(len(plan), 1)\n    self.assertEqual(plan[0][0].name, \"0002_second\")\n", "def test_detect_soft_applied(self):\n    \"\"\"\n    Tests the MigrationExecutor's ability to detect soft applied migrations.\n    \"\"\"\n    # Create a mock connection that doesn't raise an error on execute\n    with mock.patch.object(connection, 'execute') as execute_mock:\n        execute_mock.return_value = None\n\n        # Create a mock migration that is marked as initial\n        class InitialMigration(migrations.Migration):\n            initial = True\n            dependencies = []\n            operations = [migrations.CreateModel(\n                name='TestModel',\n                fields=[('id', models.AutoField(auto_created=True, primary_key=True))]\n            )]\n\n        # Create a mock migration that is not marked as initial\n        class NonInitialMigration(migrations.Migration):\n            initial = False\n            dependencies = []\n            operations = [migrations.CreateModel(\n                name='TestModel2',\n                fields=[('id', models.AutoField(auto_created=True, primary_key=True))]\n            )]\n\n        # Create an executor and set the recorder to return the mock migration\n        executor = MigrationExecutor(connection)\n        with mock.patch.object(MigrationRecorder, 'migration_qs') as qs_mock:\n            qs_mock.filter.return_value = [InitialMigration(\"test_app\", \"0001_initial\")]\n\n            # Test that the initial migration is detected as soft applied\n            state = ProjectState()\n            applied, new_state = executor.detect_soft_applied(state, InitialMigration(\"test_app\", \"0001_initial\"))\n            self.assertTrue(applied)\n\n            # Test that the non-initial migration is not detected as soft applied\n            state = ProjectState()\n            applied, new_state = executor.detect_soft_applied(state, NonInitialMigration(\"test_app\", \"0002_second\"))\n            self.assertFalse(applied)\n", "def test_migration_plan_with_replaced_migrations(self):\n    \"\"\"\n    Test that replaced migrations are correctly handled in the migration plan.\n    \"\"\"\n    recorder = MigrationRecorder(connection)\n    loader = MigrationLoader(connection, load=False)\n    executor = MigrationExecutor(connection)\n\n    # Create a fake migration graph with a replaced migration\n    migration_graph = MigrationGraph()\n    app_label = 'migrations'\n    migration_name = '0001_initial'\n    replacement_migration_name = '0002_auto'\n    migration_graph.add_node((app_label, migration_name), None)\n    migration_graph.add_node((app_label, replacement_migration_name), None)\n    migration_graph.add_dependency((app_label, replacement_migration_name), (app_label, migration_name))\n    migration_graph.replace_node((app_label, migration_name), (app_label, replacement_migration_name))\n\n    # Create a fake project state\n    project_state = ProjectState()\n\n    # Set up the migration loader to use our fake migration graph\n    loader.graph = migration_graph\n\n    # Get the migration plan for the replacement migration\n    targets = [(app_label, replacement_migration_name)]\n    plan = executor.migration_plan(targets, clean_start=True)\n\n    # The plan should include the replacement migration, but not the original migration\n    self.assertEqual(len(plan), 1)\n    self.assertEqual(plan[0][0].name, replacement_migration_name)\n\n    # The original migration should be marked as applied\n    self.assertTrue(recorder.has_applied_migration((app_label, migration_name)))\n", "def test_migration_plan(self):\n    \"\"\"\n    Tests the migration plan (dependency order) is correctly generated.\n    \"\"\"\n    executor = MigrationExecutor(connection)\n    targets = [(\"migrations\", \"0001_initial\"), (\"migrations\", \"0002_second\")]\n    plan = executor.migration_plan(targets)\n\n    # Check initial migration is applied first\n    self.assertEqual(plan[0][0].name, \"0001_initial\")\n\n    # Check second migration is applied after initial\n    self.assertEqual(plan[1][0].name, \"0002_second\")\n", "def test_migration_plan_with_replacements(self):\n    \"\"\"\n    Test migration plan with replacements.\n    \"\"\"\n    executor = MigrationExecutor(connection)\n    recorder = MigrationRecorder(connection)\n    # Fake some applied migrations\n    recorder.record_applied(\"migrations\", \"0001_initial\")\n    recorder.record_applied(\"migrations\", \"0002_second\")\n\n    # Load the graph and get a plan to 0003_third\n    graph = MigrationGraph()\n    graph.add_node((\"migrations\", \"0001_initial\"), None)\n    graph.add_node((\"migrations\", \"0002_second\"), None)\n    graph.add_node((\"migrations\", \"0003_third\"), None)\n    graph.add_dependency((\"migrations\", \"0003_third\"), (\"migrations\", \"0002_second\"))\n    graph.add_dependency((\"migrations\", \"0002_second\"), (\"migrations\", \"0001_initial\"))\n\n    # Add a replacement\n    graph.add_replacement((\"migrations\", \"0003_third\"), ((\"migrations\", \"0002_second\"),))\n\n    # Get a plan from 0001_initial to 0003_third\n    plan = executor.migration_plan([(\"migrations\", \"0003_third\")])\n\n    # The plan should be: unapply 0002_second, apply 0003_third\n    self.assertEqual(plan, [\n        (graph.nodes[(\"migrations\", \"0002_second\")], True),\n        (graph.nodes[(\"migrations\", \"0003_third\")], False),\n    ])\n"], "sample_323": ["def test_migration_plan(self):\n    executor = MigrationExecutor(connection)\n    loader = executor.loader\n\n    # Make sure we have no applied migrations to start with\n    self.assertEqual(list(loader.applied_migrations), [])\n\n    # Add some fake applied migrations\n    recorder = MigrationRecorder(connection)\n    recorder.record_applied(\"migrations\", \"0001_initial\")\n    recorder.record_applied(\"migrations\", \"0002_second\")\n\n    # Now load up the migration plan\n    targets = [(\"migrations\", \"0003_third\")]\n    plan = executor.migration_plan(targets)\n\n    # Check the plan has three migrations: 0001, 0002 (reversed) and 0003\n    self.assertEqual(len(plan), 3)\n    self.assertEqual(plan[0][0].name, \"0001_initial\")\n    self.assertEqual(plan[1][0].name, \"0002_second\")\n    self.assertEqual(plan[2][0].name, \"0003_third\")\n    self.assertTrue(plan[0][1])  # Reversing 0001\n    self.assertTrue(plan[1][1])  # Reversing 0002\n    self.assertFalse(plan[2][1])  # Applying 0003\n", "def test_migration_plan_with_replacements(self):\n    \"\"\"\n    Tests the migration plan contains the correct replacements.\n    \"\"\"\n    executor = MigrationExecutor(connection)\n    recorder = MigrationRecorder(connection)\n    # Create a fake migration history\n    recorder.record_applied(\"migrations\", \"0001_initial\")\n    recorder.record_applied(\"migrations\", \"0002_second\")\n\n    # Load the graph with a replacement migration\n    graph = MigrationGraph()\n    graph.add_node((\"migrations\", \"0001_initial\"), None)\n    graph.add_node((\"migrations\", \"0002_second\"), [(\"migrations\", \"0001_initial\")])\n    graph.add_node((\"migrations\", \"0003_third\"), [(\"migrations\", \"0002_second\")])\n    graph.add_replacement((\"migrations\", \"0004_merge\"), ((\"migrations\", \"0003_third\"),))\n\n    plan = executor.migration_plan([(\"migrations\", \"0004_merge\")])\n    self.assertEqual(len(plan), 1)\n    self.assertEqual(plan[0][0].name, \"0004_merge\")\n    self.assertTrue(plan[0][1])  # Applied is True\n\n    # Make sure the replaced migration is marked as applied\n    applied_migrations = recorder.applied_migrations()\n    self.assertIn((\"migrations\", \"0003_third\"), applied_migrations)\n", "def test_migration_plan_with_replaced_migrations(self):\n    \"\"\"\n    Test that the migration plan handles replaced migrations correctly.\n    \"\"\"\n    # Create a mock migration graph with a replaced migration\n    graph = MigrationGraph()\n    graph.add_node((\"app\", \"0001_initial\"))\n    graph.add_node((\"app\", \"0002_second\"))\n    graph.add_node((\"app\", \"0003_third\"))\n    graph.add_dependency((\"app\", \"0002_second\"), (\"app\", \"0001_initial\"))\n    graph.add_dependency((\"app\", \"0003_third\"), (\"app\", \"0002_second\"))\n    graph.add_replacement((\"app\", \"0002_second\"), [(\"app\", \"0002_second_replacement\")])\n\n    # Create a mock migration recorder\n    recorder = MigrationRecorder(connection)\n\n    # Create a migration executor\n    executor = MigrationExecutor(connection, progress_callback=None)\n    executor.loader.graph = graph\n    executor.recorder = recorder\n\n    # Get the migration plan\n    targets = [(\"app\", \"0003_third\")]\n    plan = executor.migration_plan(targets)\n\n    # Check that the plan includes the replaced migration\n    self.assertEqual(len(plan), 3)\n    self.assertEqual(plan[0][0].name, \"0001_initial\")\n    self.assertEqual(plan[1][0].name, \"0002_second_replacement\")\n    self.assertEqual(plan[2][0].name, \"0003_third\")\n", "def test_migration_plan_with_replacements(self):\n    \"\"\"\n    Test that the migration plan takes into account replacement migrations.\n    \"\"\"\n    # Create a graph with a few migrations, including one that replaces another\n    graph = MigrationGraph()\n    graph.add_node((\"app\", \"0001\"), None)\n    graph.add_node((\"app\", \"0002\"), [(\"app\", \"0001\")])\n    graph.add_node((\"app\", \"0003\"), [(\"app\", \"0002\")])\n    graph.add_node((\"app\", \"0004\"), [(\"app\", \"0003\")])\n    graph.add_replacement((\"app\", \"0004\"), [(\"app\", \"0003\")])\n\n    # Create an executor and get the migration plan\n    executor = MigrationExecutor(connection)\n    plan = executor.migration_plan([(\"app\", \"0004\")])\n\n    # The plan should include the initial migration, but not the replaced one\n    self.assertEqual(len(plan), 3)\n    self.assertEqual(plan[0][0].name, \"0001\")\n    self.assertEqual(plan[1][0].name, \"0002\")\n    self.assertEqual(plan[2][0].name, \"0004\")\n", "def test_migration_plan_with_replacements(self):\n    \"\"\"\n    Tests the migration plan with replacements.\n    \"\"\"\n    executor = MigrationExecutor(connection)\n    app_label = \"migrations\"\n    recorder = MigrationRecorder(connection)\n\n    # Apply initial migration\n    migration1 = (app_label, \"0001_initial\")\n    executor.apply_migration(ProjectState(), executor.loader.graph.nodes[migration1], fake=False)\n\n    # Create a replacement migration\n    migration2 = (app_label, \"0002_second\")\n    migration3 = (app_label, \"0003_third\")\n    recorder.record_applied(app_label, migration2[1])\n    recorder.record_applied(app_label, migration3[1])\n\n    # Create a new migration that replaces migration2 and migration3\n    migration4 = (app_label, \"0004_fourth\")\n    recorder.record_applied(app_label, migration4[1])\n\n    # Test the migration plan\n    targets = [(app_label, \"0004_fourth\")]\n    plan = executor.migration_plan(targets)\n\n    self.assertEqual(len(plan), 1)\n    self.assertEqual(plan[0][0].app_label, app_label)\n    self.assertEqual(plan[0][0].name, migration4[1])\n    self.assertFalse(plan[0][1])  # not backwards\n"], "sample_324": ["def test_csrf_trusted_origins_allowed_origin_subdomains(self):\n    settings.CSRF_TRUSTED_ORIGINS = ['http://*.example.com', 'https://subdomain.example.org']\n    request = self._get_POST_request_with_token()\n    request.META['HTTP_ORIGIN'] = 'http://subdomain.example.com'\n    CsrfViewMiddleware().process_view(request, None, (), {})\n    with self.settings(CSRF_TRUSTED_ORIGINS=['http://example.com']):\n        request.META['HTTP_ORIGIN'] = 'http://example.com'\n        CsrfViewMiddleware().process_view(request, None, (), {})\n    with self.settings(CSRF_TRUSTED_ORIGINS=['http://*.example.com:8080']):\n        request.META['HTTP_ORIGIN'] = 'http://subdomain.example.com:8080'\n        CsrfViewMiddleware().process_view(request, None, (), {})\n", "def test_csrf_trusted_origins_header(self):\n    \"\"\"\n    Test that the 'Origin' header is checked against CSRF_TRUSTED_ORIGINS.\n    \"\"\"\n    req = self._get_POST_request_with_token()\n    req.META['HTTP_ORIGIN'] = 'http://example.com'\n    with override_settings(CSRF_TRUSTED_ORIGINS=['http://example2.com']):\n        with self.assertRaisesMessage(RejectRequest, REASON_BAD_ORIGIN % req.META['HTTP_ORIGIN']):\n            CsrfViewMiddleware().process_view(req, None, [], {})\n    with override_settings(CSRF_TRUSTED_ORIGINS=['http://example.com']):\n        CsrfViewMiddleware().process_view(req, None, [], {})\n", "def test_csrf_origin_header(self):\n    request = self._get_POST_request_with_token()\n    request.META['HTTP_ORIGIN'] = 'https://example.com'\n    with override_settings(CSRF_TRUSTED_ORIGINS=['https://example.com']):\n        CsrfViewMiddleware().process_view(request, None, [], {})\n    with override_settings(CSRF_TRUSTED_ORIGINS=['http://example.com']):\n        with self.assertRaisesMessage(RejectRequest, REASON_BAD_ORIGIN % 'https://example.com'):\n            CsrfViewMiddleware().process_view(request, None, [], {})\n", "def test_process_request_csrf_token_rotation(self):\n    req = self._get_GET_no_csrf_cookie_request()\n    CsrfViewMiddleware().process_request(req)\n    self.assertIn('CSRF_COOKIE', req.META)\n\n    # Simulate token rotation on login.\n    rotate_token(req)\n    self.assertNotEqual(req.META['CSRF_COOKIE'], CsrfViewMiddleware()._get_token(req))\n\n    # Check that the new token is used in subsequent requests.\n    response = HttpResponse()\n    CsrfViewMiddleware().process_response(req, response)\n    self.assertEqual(response.cookies[settings.CSRF_COOKIE_NAME].value, req.META['CSRF_COOKIE'])\n", "def test_csrf_view_middleware_process_response_with_reset(self):\n    req = self._get_GET_csrf_cookie_request()\n    req.META['CSRF_COOKIE_USED'] = True\n    req.csrf_cookie_needs_reset = True\n\n    middleware = CsrfViewMiddleware()\n    response = HttpResponse()\n\n    response = middleware.process_response(req, response)\n\n    self.assertTrue(response.csrf_cookie_set)\n    self.assertEqual(response.cookies[settings.CSRF_COOKIE_NAME].value, get_token(req))\n"], "sample_325": ["def test_boundfield(self):\n    # Test BoundField with different widget types\n    form = Person({'first_name': 'John', 'last_name': 'Doe', 'birthday': '1980-01-01'})\n    bound_field = form['first_name']\n    self.assertIsInstance(bound_field, BoundField)\n    self.assertEqual(bound_field.name, 'first_name')\n    self.assertEqual(bound_field.label, 'First name')\n    self.assertEqual(bound_field.value(), 'John')\n\n    bound_field = form['birthday']\n    self.assertIsInstance(bound_field, BoundField)\n    self.assertEqual(bound_field.name, 'birthday')\n    self.assertEqual(bound_field.label, 'Birthday')\n    self.assertEqual(bound_field.value(), datetime.date(1980, 1, 1))\n\n    # Test BoundField with initial value\n    form = Person(initial={'first_name': 'Jane'}, data={})\n    bound_field = form['first_name']\n    self.assertEqual(bound_field.initial, 'Jane')\n    self.assertEqual(bound_field.value(), 'Jane')\n\n    # Test BoundField with show_hidden_initial=True\n    class PersonForm(Person):\n        first_name = CharField(show_hidden_initial=True)\n\n    form = PersonForm({'first_name': 'John'})\n    bound_field = form['first_name']\n    self.assertEqual(bound_field.as_widget(), '<input type=\"text\" name=\"first_name\" value=\"John\" />')\n    self.assertEqual(bound_field.as_hidden(only_initial=True), '<input type=\"hidden\" name=\"initial-first_name\" value=\"John\" />')\n", "def test_boundfield_initial_value(self):\n    # Test that the initial value for a BoundField is correctly set\n    class MyForm(Form):\n        field1 = CharField(initial='initial value')\n\n    form = MyForm()\n    self.assertEqual(form['field1'].initial, 'initial value')\n\n    # Test that the initial value is not overridden by data\n    form = MyForm({'field1': 'new value'})\n    self.assertEqual(form['field1'].initial, 'initial value')\n\n    # Test that the initial value is used when the form is unbound\n    form = MyForm()\n    self.assertEqual(form['field1'].value(), 'initial value')\n", "def test_boundfield_str_with_show_hidden_initial(self):\n    class MyForm(Form):\n        field = CharField(show_hidden_initial=True)\n\n    form = MyForm()\n    bound_field = form['field']\n    self.assertHTMLEqual(\n        bound_field.__str__(),\n        '<input type=\"text\" name=\"field\" required id=\"id_field\"><input type=\"hidden\" name=\"initial-field\" value=\"\">'\n    )\n", "def test_bound_field(self):\n    # Test that a BoundField is created with the correct values\n    form = Person()\n    bound_field = form['first_name']\n    self.assertIsInstance(bound_field, forms.BoundField)\n    self.assertEqual(bound_field.name, 'first_name')\n    self.assertEqual(bound_field.label, 'First name')\n    self.assertEqual(bound_field.help_text, '')\n    self.assertEqual(bound_field.html_name, 'first_name')\n    self.assertEqual(bound_field.html_initial_name, 'initial-first_name')\n    self.assertEqual(bound_field.html_initial_id, 'initial-id_first_name')\n", "def test_boundfield_widget_type(self):\n    # Test that BoundField.widget_type returns the correct widget type.\n    p = Person()\n    self.assertEqual(p['first_name'].widget_type, 'textinput')\n    self.assertEqual(p['birthday'].widget_type, 'dateinput')\n\n    # Test that BoundField.widget_type handles MultiWidget instances correctly.\n    class SplitDateTimeForm(Form):\n        dt = SplitDateTimeField(widget=SplitHiddenDateTimeWidget())\n\n    sdt = SplitDateTimeForm()\n    self.assertEqual(sdt['dt'].widget_type, 'splithiddendatetimewidget')\n"], "sample_326": ["def test_conditional_escape(self):\n    self.check_output(conditional_escape, \"<h1>Hello World</h1>\", \"&lt;h1&gt;Hello World&lt;/h1&gt;\")\n    self.check_output(conditional_escape, mark_safe(\"<h1>Hello World</h1>\"), \"<h1>Hello World</h1>\")\n", "def test_escapejs(self):\n    self.check_output(escapejs, 'test@example.com', 'test\\\\u0040example.com')\n    self.check_output(escapejs, 'example.com', 'example.com')\n    self.check_output(escapejs, '<script>alert(\"XSS\")</script>', '\\\\u003Cscript\\\\u003Ealert\\\\u0028\\\\u0022XSS\\\\u0022\\\\u0029\\\\u003C/script\\\\u003E')\n    self.check_output(escapejs, '\"double quotes\" and \\'single quotes\\'', '\\\\u0022double quotes\\\\u0022 and \\\\u0027single quotes\\\\u0027')\n", "def test_escapejs(self):\n    # Make sure escapejs does not mangle SafeStrings\n    self.assertEqual(escapejs(mark_safe('foo & bar')), 'foo & bar')\n\n    # Test escapejs against values that need escaping\n    self.assertEqual(escapejs('\"some\" string'), '\\u0022some\\u0022 string')\n    self.assertEqual(escapejs(\"'some' string\"), '\\u0027some\\u0027 string')\n    self.assertEqual(escapejs('some < string'), 'some \\u003C string')\n    self.assertEqual(escapejs('some > string'), 'some \\u003E string')\n    self.assertEqual(escapejs('some & string'), 'some \\u0026 string')\n    self.assertEqual(escapejs('some = string'), 'some \\u003D string')\n    self.assertEqual(escapejs('some - string'), 'some \\u002D string')\n    self.assertEqual(escapejs('some ; string'), 'some \\u003B string')\n    self.assertEqual(escapejs('some ` string'), 'some \\u0060 string')\n", "def test_urlize(self):\n    # Test with no arguments\n    self.assertEqual(urlize('http://example.com'), '<a href=\"http://example.com\">http://example.com</a>')\n\n    # Test with trim_url_limit argument\n    self.assertEqual(urlize('http://example.com', trim_url_limit=10), '<a href=\"http://example.com\">http://exam\u2026</a>')\n\n    # Test with nofollow argument\n    self.assertEqual(urlize('http://example.com', nofollow=True), '<a href=\"http://example.com\" rel=\"nofollow\">http://example.com</a>')\n\n    # Test with autoescape argument\n    self.assertEqual(urlize('http://example.com', autoescape=True), '<a href=\"http://example.com\">http://example.com</a>')\n\n    # Test with non-ASCII URL\n    self.assertEqual(urlize('http://ex\u00e4mple.com'), '<a href=\"http://ex%C3%A4mple.com\">http://ex\u00e4mple.com</a>')\n", "def test_escapejs(self):\n    # Make sure escapejs works as expected on a string\n    self.check_output(escapejs, 'Testing \\r\\n testing', 'Testing \\\\u000D\\\\u000A testing')\n    self.check_output(escapejs, 'Testing < testing', 'Testing \\\\u003C testing')\n    self.check_output(escapejs, 'Testing > testing', 'Testing \\\\u003E testing')\n    self.check_output(escapejs, 'Testing & testing', 'Testing \\\\u0026 testing')\n    self.check_output(escapejs, 'Testing = testing', 'Testing \\\\u003D testing')\n    self.check_output(escapejs, 'Testing - testing', 'Testing \\\\u002D testing')\n    self.check_output(escapejs, 'Testing ; testing', 'Testing \\\\u003B testing')\n"], "sample_327": ["def test_jsonfield_prepare_value(self):\n    field = JSONField(encoder=DjangoJSONEncoder)\n    self.assertIsNone(field.prepare_value(None))\n    self.assertEqual(field.prepare_value('{\"a\": 1}'), '{\"a\": 1}')\n    obj = uuid.UUID('12345678-1234-1234-1234-1234567890ab')\n    self.assertEqual(field.prepare_value(obj), f'\"{obj}\"')\n", "def test_json_field_validates_valid_json(self):\n    class JSONForm(Form):\n        json_field = JSONField()\n\n    form = JSONForm(data={'json_field': '{\"foo\": \"bar\"}'})\n    self.assertTrue(form.is_valid())\n", "def test_jsonfield_exception(self):\n    class JSONForm(Form):\n        name = JSONField(encoder=DjangoJSONEncoder)\n\n    with self.assertRaises(ValidationError) as context:\n        JSONForm(data={'name': ' invalid'}).full_clean()\n\n    self.assertEqual(context.exception.message, 'Enter a valid JSON.')\n", "def test_jsonfield_accepts_string(self):\n    class JSONForm(Form):\n        name = JSONField(encoder=DjangoJSONEncoder)\n\n    form = JSONForm({'name': '\"test\"'})\n    self.assertIsValid(form)\n    self.assertEqual(form.cleaned_data['name'], 'test')\n", "def test_json_field_prepare_value(self):\n    field = JSONField(encoder=DjangoJSONEncoder)\n    self.assertEqual(field.prepare_value([1, 2, 3]), '[1, 2, 3]')\n    self.assertEqual(field.prepare_value({'a': 'b'}), '{\"a\": \"b\"}')\n    self.assertEqual(field.prepare_value('string'), '\"string\"')\n    self.assertEqual(field.prepare_value(123), '123')\n    self.assertEqual(field.prepare_value(45.67), '45.67')\n    self.assertEqual(field.prepare_value(True), 'true')\n    self.assertEqual(field.prepare_value(False), 'false')\n    self.assertEqual(field.prepare_value(None), 'null')\n    self.assertEqual(field.prepare_value(uuid.uuid4()), f'\"{uuid.uuid4()}\"')\n"], "sample_328": ["def test_bulk_update(self):\n    notes = self.notes\n    Note.objects.bulk_update(notes, ['note'])\n    for note in notes:\n        note.refresh_from_db()\n        self.assertEqual(note.note, str(note.pk))\n    with self.assertRaises(FieldDoesNotExist):\n        Note.objects.bulk_update(notes, ['nonexistent_field'])\n    with self.assertRaises(ValueError):\n        Note.objects.bulk_update(notes, ['pk'])\n", "def test_bulk_update(self):\n    Note.objects.bulk_update(self.notes, ['note', 'misc'])\n    for note in self.notes:\n        with self.subTest(note=note):\n            refreshed_note = Note.objects.get(pk=note.pk)\n            self.assertEqual(refreshed_note.note, note.note)\n            self.assertEqual(refreshed_note.misc, note.misc)\n", "def test_bulk_update(self):\n    notes = Note.objects.all()\n    Note.objects.bulk_update(notes, ['note', 'misc'])\n    self.assertEqual(Note.objects.count(), 10)\n    for i, note in enumerate(Note.objects.order_by('id')):\n        self.assertEqual(note.note, str(i))\n        self.assertEqual(note.misc, str(i))\n", "def test_bulk_update_fields(self):\n    Note.objects.bulk_update(self.notes, ['note', 'misc'])\n    notes = list(Note.objects.all())\n    for i, note in enumerate(notes):\n        self.assertEqual(note.note, str(i))\n        self.assertEqual(note.misc, str(i))\n    # update only one field\n    Note.objects.bulk_update(self.notes, ['note'])\n    notes = list(Note.objects.all())\n    for i, note in enumerate(notes):\n        self.assertEqual(note.note, str(i))\n        self.assertEqual(note.misc, str(i))\n", "def test_bulk_update(self):\n    Note.objects.bulk_update(self.notes, ['note'])\n    for note in self.notes:\n        note.refresh_from_db()\n        self.assertEqual(note.note, str(int(note.misc)))\n"], "sample_329": ["    def test_serializer_factory(self):\n        # Test that the serializer factory correctly handles a value of type 'float'\n        value = 3.14\n        serializer = serializer_factory(value)\n        self.assertIsInstance(serializer, FloatSerializer)\n        serialized_value, imports = serializer.serialize()\n        self.assertEqual(serialized_value, repr(value))\n        self.assertEqual(imports, set())\n\n        # Test that the serializer factory correctly handles a value of type 'decimal.Decimal'\n        value = decimal.Decimal('3.14')\n        serializer = serializer_factory(value)\n        self.assertIsInstance(serializer, DecimalSerializer)\n        serialized_value, imports = serializer.serialize()\n        self.assertEqual(serialized_value, repr(value))\n        self.assertEqual(imports, {\"from decimal import Decimal\"})\n\n        # Test that the serializer factory correctly handles a value of type 'datetime.datetime'\n        value = datetime.datetime(2022, 1, 1, tzinfo=utc)\n        serializer = serializer_factory(value)\n        self.assertIsInstance(serializer, DatetimeDatetimeSerializer)\n        serialized_value, imports = serializer.serialize()\n        self.assertEqual(serialized_value, repr(value).replace('<UTC>', 'utc'))\n        self.assertEqual(imports, {\"import datetime\", \"from django.utils.timezone import utc\"})\n\n        # Test that the serializer factory correctly handles a value of type 'enum.Enum'\n        value = TextEnum.A\n        serializer = serializer_factory(value)\n        self.assertIsInstance(serializer, EnumSerializer)\n        serialized_value, imports = serializer.serialize()\n        self.assertEqual(serialized_value, '%s.%s[%r]' % (value.__class__.__module__, value.__class__.__qualname__, value.name))\n        self.assertEqual(imports, {\"import %s\" % value.__class__.__module__})\n", "    def test_serialize_datetime_with_timezone(self):\n        dt = datetime.datetime(2022, 1, 1, 12, 0, tzinfo=get_fixed_timezone(-300))\n        serializer = DatetimeDatetimeSerializer(dt)\n        string, imports = serializer.serialize()\n        self.assertEqual(string, \"datetime.datetime(2022, 1, 1, 17, 0, tzinfo=utc)\")\n        self.assertEqual(imports, {\"import datetime\", \"from django.utils.timezone import utc\"})\n", "    def test_float_serializer(self):\n        float_value = 3.14\n        serializer = FloatSerializer(float_value)\n        serialized, imports = serializer.serialize()\n        self.assertEqual(serialized, repr(float_value))\n        self.assertEqual(imports, set())\n", "    def test_float_serializer(self):\n        serializer = FloatSerializer(float('inf'))\n        result, imports = serializer.serialize()\n        self.assertEqual(result, \"float('inf')\")\n        self.assertEqual(imports, set())\n\n        serializer = FloatSerializer(float('-inf'))\n        result, imports = serializer.serialize()\n        self.assertEqual(result, \"float('-inf')\")\n        self.assertEqual(imports, set())\n\n        serializer = FloatSerializer(float('nan'))\n        result, imports = serializer.serialize()\n        self.assertEqual(result, \"float('nan')\")\n        self.assertEqual(imports, set())\n", "    def test_serializer_factory(self):\n        self.assertIsInstance(serializer_factory(123), BaseSerializer)\n        self.assertIsInstance(serializer_factory(\"hello\"), BaseSerializer)\n        self.assertIsInstance(serializer_factory([1, 2, 3]), BaseSerializer)\n        self.assertIsInstance(serializer_factory({\"a\": 1, \"b\": 2}), BaseSerializer)\n        self.assertIsInstance(serializer_factory(datetime.datetime.now()), BaseSerializer)\n        self.assertIsInstance(serializer_factory(decimal.Decimal(\"123.45\")), BaseSerializer)\n        self.assertIsInstance(serializer_factory(enum.Enum(\"Color\", \"RED GREEN BLUE\")), BaseSerializer)\n        self.assertIsInstance(serializer_factory(functools.partial(int, base=2)), BaseSerializer)\n        self.assertIsInstance(serializer_factory(pathlib.Path(\"/path/to/file\")), BaseSerializer)\n        self.assertIsInstance(serializer_factory(re.compile(\"regex\")), BaseSerializer)\n        self.assertIsInstance(serializer_factory(uuid.uuid4()), BaseSerializer)\n"], "sample_330": ["    def test_typecast_date(self):\n        self.assertIsNone(typecast_date(None))\n        self.assertEqual(typecast_date('2022-01-01'), datetime.date(2022, 1, 1))\n", "    def test_typecast_date(self):\n        self.assertIsNone(typecast_date(None))\n        self.assertIsNone(typecast_date(''))\n        self.assertEqual(typecast_date('2022-01-01'), datetime.date(2022, 1, 1))\n", "    def test_typecast_date(self):\n        self.assertIsNone(typecast_date(''))\n        self.assertEqual(typecast_date('2022-01-01'), datetime.date(2022, 1, 1))\n", "    def test_typecast_date(self):\n        self.assertIsNone(typecast_date(None))\n        self.assertEqual(typecast_date('2022-01-01'), datetime.date(2022, 1, 1))\n", "    def test_typecast_date(self):\n        self.assertIsNone(typecast_date(''))\n        self.assertEqual(typecast_date('2022-01-01'), datetime.date(2022, 1, 1))\n"], "sample_331": ["def test_parse_time(self):\n    self.assertEqual(parse_time('12:00'), time(12, 0))\n    self.assertEqual(parse_time('12:00:00'), time(12, 0, 0))\n    self.assertEqual(parse_time('12:00:00.123'), time(12, 0, 0, 123000))\n    self.assertIsNone(parse_time('12:00:00 PM'))  # PM/AM not supported\n    self.assertIsNone(parse_time('12:00:00+02:00'))  # Time zone offset not supported\n    with self.assertRaises(ValueError):\n        parse_time('25:00')\n", "def test_parse_date(self):\n    self.assertEqual(parse_date('2022-01-01'), date(2022, 1, 1))\n    self.assertIsNone(parse_date('20220101'))  # not in YYYY-MM-DD format\n    self.assertIsNone(parse_date('2022-01'))  # incomplete date\n    self.assertIsNone(parse_date('2022-13-01'))  # invalid month\n    self.assertIsNone(parse_date('2022-01-32'))  # invalid day\n", "def test_parse_date(self):\n    self.assertEqual(parse_date('2022-01-01'), date(2022, 1, 1))\n    self.assertIsNone(parse_date('not a date'))\n    self.assertIsNone(parse_date('2022-02-30'))  # invalid date\n    self.assertEqual(parse_date('2022-12-31'), date(2022, 12, 31))\n", "def test_parse_date_valid(self):\n    self.assertEqual(parse_date('2022-02-28'), date(2022, 2, 28))\n    self.assertEqual(parse_date('2020-12-31'), date(2020, 12, 31))\n    self.assertEqual(parse_date('1999-01-01'), date(1999, 1, 1))\n", "def test_parse_date_valid(self):\n    self.assertEqual(parse_date('2022-01-01'), date(2022, 1, 1))\n    self.assertEqual(parse_date('2022-12-31'), date(2022, 12, 31))\n"], "sample_332": ["def test_formset_validate_max(self):\n    ChoiceFormSet = formset_factory(Choice, max_num=2, validate_max=True)\n    data = {\n        'choices-TOTAL_FORMS': '3',\n        'choices-INITIAL_FORMS': '0',\n        'choices-MAX_NUM_FORMS': '2',\n        'choices-0-choice': 'A',\n        'choices-0-votes': '1',\n        'choices-1-choice': 'B',\n        'choices-1-votes': '2',\n        'choices-2-choice': 'C',\n        'choices-2-votes': '3',\n    }\n    formset = ChoiceFormSet(data)\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(len(formset.non_form_errors()), 1)\n    self.assertIn('Please submit at most 2 forms.', str(formset.non_form_errors()[0]))\n", "def test_formset_absolute_max(self):\n    # Test that a formset with more forms than absolute_max raises an error\n    ChoiceFormSet = formset_factory(Choice, max_num=10, absolute_max=5)\n    data = {\n        'choices-TOTAL_FORMS': 6,\n        'choices-INITIAL_FORMS': 0,\n        'choices-MIN_NUM_FORMS': 0,\n        'choices-MAX_NUM_FORMS': 10,\n        'choices-0-choice': '1',\n        'choices-0-votes': 1,\n        'choices-1-choice': '2',\n        'choices-1-votes': 1,\n        'choices-2-choice': '3',\n        'choices-2-votes': 1,\n        'choices-3-choice': '4',\n        'choices-3-votes': 1,\n        'choices-4-choice': '5',\n        'choices-4-votes': 1,\n        'choices-5-choice': '6',\n        'choices-5-votes': 1,\n    }\n    formset = ChoiceFormSet(data)\n    self.assertEqual(formset.errors, [])\n    self.assertEqual(len(formset.non_form_errors()), 1)\n    self.assertIn('Please submit at most 5 forms.', str(formset.non_form_errors()[0]))\n", "def test_formset_absolute_max(self):\n    class TestForm(Form):\n        field = CharField()\n\n    TestFormSet = formset_factory(TestForm, max_num=10, absolute_max=5)\n\n    data = {\n        'form-TOTAL_FORMS': '6',\n        'form-INITIAL_FORMS': '0',\n        'form-MIN_NUM_FORMS': '',\n        'form-MAX_NUM_FORMS': '',\n        'form-0-field': 'field1',\n        'form-1-field': 'field2',\n        'form-2-field': 'field3',\n        'form-3-field': 'field4',\n        'form-4-field': 'field5',\n        'form-5-field': 'field6',\n    }\n\n    formset = TestFormSet(data)\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(len(formset.non_form_errors()), 1)\n    self.assertEqual(\n        formset.non_form_errors()[0],\n        'Please submit at most 5 forms.'\n    )\n", "def test_formset_management_form_validation(self):\n    formset = self.make_choiceformset(formset_data=[('choice1', '10'), ('choice2', '20')])\n    formset.management_form.cleaned_data['TOTAL_FORMS'] = 'abc'\n    with self.assertRaises(ValidationError):\n        formset.full_clean()\n", "def test_formset_absolute_max(self):\n    # Test that the absolute_max parameter is enforced.\n    ChoiceFormSetAbsoluteMax = formset_factory(Choice, max_num=10, absolute_max=10)\n    formset = self.make_choiceformset([('A', 1)] * 11, ChoiceFormSetAbsoluteMax)\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(len(formset.non_form_errors()), 1)\n    self.assertIn('Please submit at most 10 forms.', str(formset.non_form_errors()[0]))\n"], "sample_333": ["def test_form_rendering_with_empty_permitted(self):\n    class EmptyPermittedForm(Form):\n        field1 = CharField(required=False)\n        field2 = CharField(required=True)\n\n    f = EmptyPermittedForm({}, empty_permitted=True)\n    self.assertTrue(f.is_valid())\n    self.assertEqual(f.as_table(), '')\n", "def test_form_add_prefix(self):\n    # A form can be instantiated with a prefix to create unique field names.\n    # With this Form instance, we're using the prefix 'person'.\n    form = Person(prefix='person')\n    self.assertEqual(form.add_prefix('first_name'), 'person-first_name')\n    self.assertEqual(form.add_prefix('last_name'), 'person-last_name')\n\n    # These are used in form rendering.\n    self.assertHTMLEqual(str(form['first_name']), '<input id=\"id_person-first_name\" name=\"person-first_name\" type=\"text\" required>')\n    self.assertHTMLEqual(str(form['last_name']), '<input id=\"id_person-last_name\" name=\"person-last_name\" type=\"text\" required>')\n", "def test_add_prefix(self):\n    # Test that add_prefix is adding the prefix to the field name\n    f = Person(prefix='person')\n    self.assertEqual(f.add_prefix('first_name'), 'person-first_name')\n\n    # Test that it doesn't change names without a prefix\n    f = Person()\n    self.assertEqual(f.add_prefix('first_name'), 'first_name')\n", "def test_form_add_prefix(self):\n    # Test that the 'add_prefix' method is called when rendering a form.\n    class Person(Form):\n        first_name = CharField()\n\n            return field_name + '_prefix'\n\n    p = Person()\n    self.assertHTMLEqual(p.as_p(), '<p>First name: <input type=\"text\" name=\"first_name_prefix\"></p>')\n", "def test_form_init_with_use_required_attribute(self):\n    class TestForm(Form):\n        field = CharField(required=True)\n\n    form = TestForm(use_required_attribute=False)\n    self.assertFalse(form.use_required_attribute)\n\n    form = TestForm(use_required_attribute=True)\n    self.assertTrue(form.use_required_attribute)\n\n    with self.assertRaises(ValueError):\n        TestForm(empty_permitted=True, use_required_attribute=True)\n"], "sample_334": ["def test_form_has_changed(self):\n    # Test the has_changed method on a form.\n    class F(Form):\n        field1 = CharField()\n        field2 = CharField()\n\n    data = {'field1': 'new value', 'field2': 'new value'}\n    initial_data = {'field1': 'initial value', 'field2': 'initial value'}\n\n    f = F(data, initial=initial_data)\n    self.assertTrue(f.has_changed())\n\n    f = F(initial_data, initial=initial_data)\n    self.assertFalse(f.has_changed())\n\n    # Make sure it works for files as well.\n    class G(Form):\n        file1 = FileField()\n\n    file_data = {'file1': SimpleUploadedFile('file1.txt', b'content')}\n\n    g = G(file_data, initial={'file1': 'initial_file'})\n    self.assertTrue(g.has_changed())\n\n    g = G({}, initial={'file1': 'initial_file'})\n    self.assertFalse(g.has_changed())\n", "def test_form_initial_data(self):\n    # Test that initial data is processed correctly\n    class Person(Form):\n        first_name = CharField(initial='John')\n        last_name = CharField()\n        birthday = DateField(initial=datetime.date(1990, 1, 1))\n\n    p = Person()\n    self.assertEqual(p['first_name'].value(), 'John')\n    self.assertEqual(p['birthday'].value(), datetime.date(1990, 1, 1))\n    self.assertIsNone(p['last_name'].value())\n", "def test_form_add_prefix(self):\n    p = Person()\n    self.assertEqual(p.add_prefix('first_name'), 'first_name')\n    p = Person(prefix='person')\n    self.assertEqual(p.add_prefix('first_name'), 'person-first_name')\n", "def test_add_prefix(self):\n    p = Person()\n    self.assertEqual(p.add_prefix('first_name'), 'first_name')\n    p = Person(prefix='person')\n    self.assertEqual(p.add_prefix('first_name'), 'person-first_name')\n", "def test_form_add_prefix(self):\n    # Test prefix construction\n    form1 = Person(prefix='foo')\n    self.assertEqual(form1.add_prefix('bar'), 'foo-bar')\n\n    form2 = Person(prefix='foo.bar')\n    self.assertEqual(form2.add_prefix('baz'), 'foo.bar-baz')\n\n    form3 = Person()\n    self.assertEqual(form3.add_prefix('quux'), 'quux')\n"], "sample_335": ["def test_decimalfield_widget_attrs(self):\n    f = DecimalField(decimal_places=3, max_value=999.999, min_value=-999.999)\n    self.assertEqual(f.widget_attrs(NumberInput()), {'step': '0.001', 'min': '-999.999', 'max': '999.999'})\n", "    def test_decimalfield_uses_widget_attrs(self):\n        f = DecimalField(max_value=100, min_value=5)\n        self.assertIsInstance(f.widget, NumberInput)\n        self.assertEqual(f.widget.attrs, {'min': 5, 'max': 100})\n", "def test_decimalfield_uses_widget_attrs(self):\n    f = DecimalField(min_value=5, max_value=10)\n    self.assertEqual(f.widget_attrs(NumberInput()), {'min': '5', 'max': '10'})\n", "def test_decimalfield_uses_max_digits_and_decimal_places_in_widget(self):\n    f = DecimalField(max_digits=5, decimal_places=3)\n    self.assertEqual(f.widget.attrs, {'max': '999', 'step': '0.001'})\n", "def test_decimalfield_uses_widget_attrs(self):\n    f = DecimalField(max_value=999.99, min_value=0.01)\n    widget = f.widget\n    self.assertIsInstance(widget, NumberInput)\n    self.assertEqual(widget.attrs, {'max': '999.99', 'min': '0.01', 'step': 'any'})\n"], "sample_336": ["    def test_url_pattern_str(self):\n        pattern = URLPattern(RegexPattern(r'^foo/$'), empty_view, name='test')\n        self.assertEqual(str(pattern.pattern), '^foo/$')\n", "    def test_url_resolver_populate(self):\n        resolver = get_resolver()\n        self.assertFalse(resolver._populated)\n        resolver._populate()\n        self.assertTrue(resolver._populated)\n        self.assertIsInstance(resolver.reverse_dict, dict)\n        self.assertIsInstance(resolver.namespace_dict, dict)\n        self.assertIsInstance(resolver.app_dict, dict)\n", "def test_reverse_urlresolver_populate(self):\n    resolver = URLResolver(RegexPattern(r'^/'), 'urlpatterns_reverse.urls')\n    self.assertFalse(resolver._populated)\n    resolver.reverse('normal-view', args=[42, 37])\n    self.assertTrue(resolver._populated)\n", "def test_resolver_match_non_string_view_name(self):\n    resolver_match = ResolverMatch(None, None, None, url_name=object())\n    with self.assertRaises(TypeError):\n        repr(resolver_match)\n", "    def test_reverse_lazy(self):\n        lazy_url = reverse_lazy('normal-view', kwargs={'arg1': '42', 'arg2': '37'})\n        self.assertEqual(str(lazy_url), '/normal/42/37/')\n        self.assertEqual(reverse_lazy('unknown-view').__class__, ReverseLazy)\n"], "sample_337": ["def test_csrf_trusted_origins_hosts(self):\n    @override_settings(CSRF_TRUSTED_ORIGINS=['http://example.com', 'https://sub.example.com'])\n        csrf_middleware = CsrfViewMiddleware()\n        self.assertEqual(csrf_middleware.csrf_trusted_origins_hosts, ['example.com', 'sub.example.com'])\n\n    test_get_trusted_origins_hosts()\n\n    @override_settings(CSRF_TRUSTED_ORIGINS=['http://example.com:8080', 'https://sub.example.com'])\n        csrf_middleware = CsrfViewMiddleware()\n        self.assertEqual(csrf_middleware.csrf_trusted_origins_hosts, ['example.com:8080', 'sub.example.com'])\n\n    test_get_trusted_origins_hosts_with_port()\n", "def test_origin_header_verification(self):\n    request = HttpRequest()\n    request.META['HTTP_ORIGIN'] = 'https://example.com'\n    middleware = CsrfViewMiddleware()\n\n    with override_settings(CSRF_TRUSTED_ORIGINS=['https://example.com']):\n        self.assertTrue(middleware._origin_verified(request))\n\n    with override_settings(CSRF_TRUSTED_ORIGINS=['https://other.example.com']):\n        self.assertFalse(middleware._origin_verified(request))\n", "def test_csrf_trusted_origins(self):\n    # Test that CSRF checks are disabled when the Origin header matches a\n    # trusted origin.\n    request = HttpRequest()\n    request.method = 'POST'\n    request.META['HTTP_ORIGIN'] = 'https://example.com'\n    request.META['CSRF_COOKIE'] = _mask_cipher_secret(TEST_SECRET)\n    middleware = CsrfViewMiddleware()\n\n    with override_settings(CSRF_TRUSTED_ORIGINS=['https://example.com']):\n        response = middleware.process_view(request, None, (), {})\n        self.assertIsNone(response)\n\n    # Test that CSRF checks are not disabled when the Origin header does not\n    # match a trusted origin.\n    request.META['HTTP_ORIGIN'] = 'https://other.example.com'\n    with override_settings(CSRF_TRUSTED_ORIGINS=['https://example.com']):\n        with self.assertRaises(RejectRequest) as cm:\n            middleware.process_view(request, None, (), {})\n        self.assertEqual(cm.exception.reason, REASON_BAD_ORIGIN % request.META['HTTP_ORIGIN'])\n", "def test_get_token_rotates_if_csrf_cookie_is_old_format(self):\n    request = HttpRequest()\n    request.COOKIES[settings.CSRF_COOKIE_NAME] = TEST_SECRET  # Old format\n    token1 = get_token(request)\n    self.assertEqual(len(token1), CSRF_TOKEN_LENGTH)\n    token2 = get_token(request)\n    self.assertEqual(token2, token1)\n    self.assertTrue(equivalent_tokens(token1, _mask_cipher_secret(TEST_SECRET)))\n", "def test_compare_masked_tokens(self):\n    self.assertTrue(equivalent_tokens(MASKED_TEST_SECRET1, MASKED_TEST_SECRET2))\n    self.assertFalse(equivalent_tokens(MASKED_TEST_SECRET1, _mask_cipher_secret('n' * CSRF_TOKEN_LENGTH)))\n"], "sample_338": ["def test_generate_altered_db_table(self):\n    # Test the db_table alteration\n    changes = self.get_changes([self.author_with_db_table_options], [self.author_with_new_db_table_options])\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterModelTable\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"author\", table=\"author_two\")\n\n    # Test the db_table alteration when the model is renamed\n    changes = self.get_changes([self.author_with_db_table_options], [self.author_renamed_with_db_table_options])\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"RenameModel\"])\n\n    changes = self.get_changes([self.author_with_db_table_options], [self.author_renamed_with_new_db_table_options])\n    self.assertNumberMigrations(changes, \"testapp\", 2)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"RenameModel\"])\n    self.assertOperationTypes(changes, \"testapp\", 1, [\"AlterModelTable\"])\n    self.assertOperationAttributes(changes, \"testapp\", 1, 0, name=\"newauthor\", table=\"author_three\")\n", "def test_generate_added_field_with_deconstructible_default(self):\n    \"\"\"Test add field with deconstructible default.\"\"\"\n    changes = self.get_changes(\n        [self.author_empty],\n        [self.author_name_deconstructible_1],\n    )\n    # Right number of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    # Right operations in the migration?\n    self.assertOperationTypes(changes, 'testapp', 0, ['AddField'])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name='name')\n    self.assertEqual(\n        changes['testapp'][0].operations[0].field.default,\n        DeconstructibleObject(),\n    )\n", "def test_altered_unique_together_with_renamed_model(self):\n    # Renaming a model and changing unique_together at the same time.\n    before = self.make_project_state([self.author_with_book, self.book_foo_together])\n    after = self.make_project_state([self.author_renamed_with_book, self.book_with_author_renamed])\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertNumberMigrations(changes, 'otherapp', 1)\n    self.assertOperationTypes(changes, 'otherapp', 0, ['AlterUniqueTogether'])\n    self.assertOperationAttributes(changes, 'otherapp', 0, 0, name='book')\n", "def test_deep_deconstruct(self):\n    # test that deep_deconstruct handles recursive deconstruction correctly.\n    recursive_default = DeconstructibleObject(\n        DeconstructibleObject(1),\n        (DeconstructibleObject('t1'), DeconstructibleObject('t2'),),\n        a=DeconstructibleObject('A'),\n        b=DeconstructibleObject(B=DeconstructibleObject('c')),\n    )\n    recursive_field = models.CharField(max_length=200, default=recursive_default)\n\n    autodetector = MigrationAutodetector(\n        ProjectState(),\n        ProjectState(),\n    )\n\n    self.assertEqual(\n        autodetector.deep_deconstruct(recursive_field),\n        (\n            \"django.db.models.CharField\",\n            [],\n            {\n                \"max_length\": 200,\n                \"default\": (\n                    \"__main__.DeconstructibleObject\",\n                    (\n                        (\"__main__.DeconstructibleObject\", (1,), {}),\n                        (\n                            \"__main__.DeconstructibleObject\",\n                            ('t1',),\n                            {}\n                        ),\n                        (\n                            \"__main__.DeconstructibleObject\",\n                            ('t2',),\n                            {}\n                        ),\n                    ),\n                    {\n                        \"a\": (\"__main__.DeconstructibleObject\", ('A',), {}),\n                        \"b\": (\n                            \"__main__.DeconstructibleObject\",\n                            (),\n                            {\n                                \"B\": (\n                                    \"__main__.DeconstructibleObject\",\n                                    ('c',),\n                                    {}\n                                ),\n                            }\n                        ),\n                    },\n                ),\n            },\n        )\n    )\n", "def test_deep_deconstruct(self):\n    field = models.CharField(max_length=200, default=[DeconstructibleObject(), 123])\n    deconstructed = MigrationAutodetector.deep_deconstruct(field)\n    self.assertEqual(\n        deconstructed,\n        (\n            'django.db.models.CharField',\n            [],\n            {\n                'default': [\n                    ('migrations.test_autodetector', 'DeconstructibleObject', (), {}),\n                    123\n                ],\n                'max_length': 200\n            }\n        )\n    )\n"], "sample_339": ["def test_modelformset_factory_without_fields_or_exclude_raises(self):\n    msg = \"Calling modelformset_factory without defining 'fields' or 'exclude' explicitly is prohibited.\"\n    with self.assertRaisesMessage(ImproperlyConfigured, msg):\n        modelformset_factory(Book, form=forms.modelform_factory(Book))\n", "    def test_label_from_instance(self):\n        class CustomModelChoiceField(forms.ModelChoiceField):\n                return f\"Custom {obj.name}\"\n\n        field = CustomModelChoiceField(queryset=Author.objects.all())\n        self.assertEqual(\n            field.label_from_instance(Author(name=\"John\")),\n            \"Custom John\"\n        )\n", "def test_modelformset_factory(self):\n    # Create a model formset factory with a queryset\n    BookFormSet = modelformset_factory(Book, fields=('title',), queryset=Book.objects.all())\n    formset = BookFormSet()\n    self.assertEqual(len(formset.forms), Book.objects.count())\n\n    # Create a model formset factory without a queryset\n    BookFormSet = modelformset_factory(Book, fields=('title',))\n    formset = BookFormSet()\n    self.assertEqual(len(formset.forms), 1)  # extra form\n\n    # Create a model formset factory with a queryset and extra forms\n    BookFormSet = modelformset_factory(Book, fields=('title',), queryset=Book.objects.all(), extra=3)\n    formset = BookFormSet()\n    self.assertEqual(len(formset.forms), Book.objects.count() + 3)\n\n    # Create a model formset factory with a queryset and max_num\n    BookFormSet = modelformset_factory(Book, fields=('title',), queryset=Book.objects.all(), max_num=2)\n    formset = BookFormSet()\n    self.assertEqual(len(formset.forms), min(Book.objects.count(), 2))\n\n    # Create a model formset factory with a queryset, extra forms and max_num\n    BookFormSet = modelformset_factory(Book, fields=('title',), queryset=Book.objects.all(), extra=3, max_num=2)\n    formset = BookFormSet()\n    self.assertEqual(len(formset.forms), min(Book.objects.count() + 3, 2))\n", "    def test_formset_save_with_empty_queryset(self):\n        FormSet = modelformset_factory(Book, fields=['title'])\n        formset = FormSet(queryset=Book.objects.filter(title='nonexistent'))\n        self.assertEqual(formset.save(), [])\n", "def test_inlineformset_factory_model_with_to_field(self):\n    \"\"\"\n    Test that inlineformset_factory works correctly for a ForeignKey with\n    to_field set.\n    \"\"\"\n    LocationFormSet = inlineformset_factory(\n        Restaurant,\n        Location,\n        fields=('restaurant', 'street', 'city', 'state', 'zip_code'),\n    )\n    restaurant = Restaurant.objects.create(name='Example')\n    location1 = Location.objects.create(\n        restaurant=restaurant,\n        street='123 Main St',\n        city='Anytown',\n        state='US',\n        zip_code='12345',\n    )\n    location2 = Location.objects.create(\n        restaurant=restaurant,\n        street='456 Elm St',\n        city='Othertown',\n        state='US',\n        zip_code='67890',\n    )\n    data = {\n        'locations-TOTAL_FORMS': 2,\n        'locations-INITIAL_FORMS': 2,\n        'locations-MIN_NUM_FORMS': 0,\n        'locations-MAX_NUM_FORMS': 1000,\n        'locations-0-id': location1.id,\n        'locations-0-restaurant': restaurant.id,\n        'locations-0-street': '123 Main St',\n        'locations-0-city': 'Anytown',\n        'locations-0-state': 'US',\n        'locations-0-zip_code': '12345',\n        'locations-1-id': location2.id,\n        'locations-1-restaurant': restaurant.id,\n        'locations-1-street': '456 Elm St',\n        'locations-1-city': 'Othertown',\n        'locations-1-state': 'US',\n        'locations-1-zip_code': '67890',\n    }\n    formset = LocationFormSet(data, instance=restaurant)\n    self.assertTrue(formset.is_valid())\n    formset.save()\n"], "sample_340": ["    def test_load(self):\n        \"\"\"\n        Makes sure the loader can load the migrations for the 'migrations'\n        app.\n        \"\"\"\n        loader = MigrationLoader(connection)\n        self.addCleanup(loader.build_graph)\n        loader.load_disk()\n        self.assertIn(('migrations', '0001_initial'), loader.disk_migrations)\n        self.assertEqual(len(loader.disk_migrations), 6)\n", "    def test_load(self):\n        \"\"\"\n        Makes sure the loader can load the test migrations.\n        \"\"\"\n        loader = MigrationLoader(connection)\n        self.addCleanup(loader.clear_cache)\n        self.assertEqual(len(loader.disk_migrations), 6)\n", "    def test_load(self):\n        \"\"\"\n        Tests that migrations are properly loaded from disk.\n        \"\"\"\n        loader = MigrationLoader(connection)\n        self.addCleanup(loader.clear_cache)\n        loader.load_disk()\n        self.assertEqual(len(loader.disk_migrations), 2)\n        self.assertEqual(len(loader.graph.nodes), 2)\n", "    def test_load_migrations(self):\n        \"\"\"\n        Tests that the loader correctly loads migrations from disk.\n        \"\"\"\n        loader = MigrationLoader(connection)\n        self.assertEqual(loader.disk_migrations, {})\n        self.assertEqual(loader.applied_migrations, {})\n\n        # Make a fake migration and add it to the loader.\n        class FakeMigration:\n            pass\n\n        with self.settings(INSTALLED_APPS=['migrations']):\n            import_module('migrations')\n            compileall.compile_dir('migrations', force=True)\n\n            # Now the loader should have loaded our one migration.\n            loader = MigrationLoader(connection)\n            self.assertIn(('migrations', '0001_initial'), loader.disk_migrations)\n", "    def test_load(self):\n        \"\"\"\n        Makes sure the loader can load the migration tree correctly.\n        \"\"\"\n        loader = MigrationLoader(connection)\n        self.assertEqual(loader.load_disk()[0][0], ('migrations', '0001_initial'))\n        self.assertEqual(len(loader.load_disk()[0]), 2)\n"], "sample_341": ["def test_formset_management_form_invalid(self):\n    # Create a formset with invalid management form data\n    formset_data = [('choice1', 1), ('choice2', 2)]\n    formset = self.make_choiceformset(\n        formset_data,\n        total_forms='a',  # invalid TOTAL_FORMS value\n    )\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(len(formset.non_form_errors()), 1)\n    self.assertEqual(formset.non_form_errors()[0].code, 'missing_management_form')\n", "def test_formset_factory_absolute_max(self):\n    with self.assertRaises(ValueError):\n        formset_factory(FavoriteDrinkForm, max_num=1000, absolute_max=500)\n", "def test_formset_deletion_widget(self):\n    class DeletionForm(Form):\n        field = CharField()\n\n    DeletionFormSet = formset_factory(DeletionForm, can_delete=True)\n\n    formset = DeletionFormSet()\n    self.assertIsInstance(formset.forms[0].fields['DELETE'].widget, formsets.CheckboxInput)\n    self.assertEqual(formset.forms[0].fields['DELETE'].label, 'Delete')\n", "def test_formset_management_form_validation(self):\n    # Test that the management form is validated correctly\n    formset = self.make_choiceformset(formset_data=[('Choice 1', '10'), ('Choice 2', '20')])\n    formset.management_form.cleaned_data['TOTAL_FORMS'] = 'abc'\n    with self.assertRaises(ValidationError):\n        formset.is_valid()\n\n    # Test that the management form is not validated when the formset is not bound\n    formset = self.make_choiceformset()\n    formset.management_form.cleaned_data['TOTAL_FORMS'] = 'abc'\n    self.assertFalse(formset.is_valid())\n\n    # Test that the management form errors are propagated to the formset\n    formset = self.make_choiceformset(formset_data=[('Choice 1', '10'), ('Choice 2', '20')])\n    formset.management_form.cleaned_data['TOTAL_FORMS'] = 'abc'\n    try:\n        formset.is_valid()\n    except ValidationError as e:\n        self.assertEqual(len(e.error_list), 1)\n        self.assertEqual(str(e.error_list[0]), 'ManagementForm data is missing or has been tampered with. Missing fields: TOTAL_FORMS.')\n", "def test_formset_absolute_max(self):\n    ChoiceFormSet = formset_factory(Choice, max_num=10, absolute_max=15)\n    formset = self.make_choiceformset([('A', 1)] * 20, ChoiceFormSet)\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(len(formset.forms), 15)\n    self.assertEqual(formset.non_form_errors()[0].code, 'too_many_forms')\n"], "sample_342": ["def test_process_request(self):\n    request = self.factory.get(self.url, {\n        'term': 'test',\n        'app_label': Answer._meta.app_label,\n        'model_name': Answer._meta.model_name,\n        'field_name': 'question'\n    })\n    view = AutocompleteJsonView.as_view(**self.as_view_args)\n    with self.assertRaises(PermissionDenied):\n        view(request)\n\n    # Test that process_request raises Http404 if the target model admin is not\n    # configured properly with search_fields.\n    with model_admin(Question, admin.ModelAdmin):\n        with self.assertRaises(Http404):\n            view(request)\n\n    # Test that process_request raises PermissionDenied if the requested app,\n    # model or field are malformed.\n    bad_requests = [\n        self.factory.get(self.url, {\n            'term': 'test',\n            'app_label': 'bad_app_label',\n            'model_name': Answer._meta.model_name,\n            'field_name': 'question'\n        }),\n        self.factory.get(self.url, {\n            'term': 'test',\n            'app_label': Answer._meta.app_label,\n            'model_name': 'bad_model_name',\n            'field_name': 'question'\n        }),\n        self.factory.get(self.url, {\n            'term': 'test',\n            'app_label': Answer._meta.app_label,\n            'model_name': Answer._meta.model_name,\n            'field_name': 'bad_field_name'\n        }),\n    ]\n    for bad_request in bad_requests:\n        with self.assertRaises(PermissionDenied):\n            view(bad_request)\n", "def test_process_request(self):\n    request = self.factory.get(self.url, self.opts)\n    view = AutocompleteJsonView.as_view(**self.as_view_args)\n    response = view(request)\n    # Check that the response is not an error\n    self.assertEqual(response.status_code, 200)\n    # Check that process_request sets the correct attributes on the view\n    view_instance = AutocompleteJsonView(**self.as_view_args)\n    view_instance.request = request\n    term, model_admin, source_field, to_field_name = view_instance.process_request(request)\n    self.assertEqual(term, '')\n    self.assertIsInstance(model_admin, QuestionAdmin)\n    self.assertEqual(source_field.name, 'question')\n    self.assertEqual(to_field_name, 'id')\n", "def test_get_queryset(self):\n    with model_admin(Answer, AnswerAdmin, admin_site=site):\n        request = self.factory.get(self.url, data=self.opts)\n        request.user = self.user\n        view = AutocompleteJsonView.as_view(**self.as_view_args)\n        response = view(request)\n        queryset = view.view_class().get_queryset()\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(queryset.model, Question)\n\n        # Test that the queryset is filtered by the search term\n        request = self.factory.get(self.url, data={**self.opts, 'term': 'test'})\n        response = view(request)\n        queryset = view.view_class().get_queryset()\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(queryset.model, Question)\n", "def test_has_perm(self):\n    request = self.factory.get(self.url, self.opts)\n    request.user = self.user\n    view = AutocompleteJsonView.as_view(**self.as_view_args)\n    with model_admin(Answer, AnswerAdmin, admin_site=site):\n        response = view(request)\n        self.assertEqual(response.status_code, 200)\n\n    # Test without view permission\n    self.user.is_staff = False\n    self.user.save()\n    request.user = self.user\n    with model_admin(Answer, AnswerAdmin, admin_site=site):\n        with self.assertRaises(PermissionDenied):\n            view(request)\n", "def test_get_queryset(self):\n    request = self.factory.get(self.url, data=self.opts)\n    request.user = self.user\n    view = AutocompleteJsonView.as_view(**self.as_view_args)\n    response = view(request)\n    self.assertEqual(response.status_code, 200)\n    # Create some answers to check if they are in the queryset\n    Answer.objects.create(question=Question.objects.create(), answer='Answer 1')\n    Answer.objects.create(question=Question.objects.create(), answer='Answer 2')\n    response = view(request)\n    self.assertEqual(response.status_code, 200)\n    results = json.loads(response.content)['results']\n    self.assertEqual(len(results), 2)\n"], "sample_343": ["def test_generic_foreign_key_checks(self):\n    class InvalidModel(models.Model):\n        gfk = GenericForeignKey('content_type', 'object_id')\n\n    errors = InvalidModel.check()\n    self.assertEqual(len(errors), 2)\n    self.assertEqual(errors[0].id, 'fields.E001')\n    self.assertEqual(errors[1].id, 'contenttypes.E001')\n\n    class ValidModel(models.Model):\n        content_type = models.ForeignKey('contenttypes.ContentType', on_delete=models.CASCADE)\n        object_id = models.IntegerField()\n        gfk = GenericForeignKey()\n\n    errors = ValidModel.check()\n    self.assertEqual(errors, [])\n", "def test_generic_foreign_key_check(self):\n    class Model(models.Model):\n        content_type = models.ForeignKey('contenttypes.ContentType', on_delete=models.CASCADE)\n        object_id = models.PositiveIntegerField()\n        content_object = GenericForeignKey()\n\n    model = Model()\n    errors = model.content_object.check()\n    self.assertEqual(errors, [])\n\n    class Model(models.Model):\n        object_id = models.PositiveIntegerField()\n        content_object = GenericForeignKey(ct_field='content_type')\n\n    model = Model()\n    errors = model.content_object.check()\n    self.assertEqual(len(errors), 1)\n    self.assertEqual(errors[0].id, 'contenttypes.E001')\n", "def test_generic_foreign_key_checks(self):\n    class InvalidModel(models.Model):\n        gfk = GenericForeignKey('content_type', 'object_id', for_concrete_model=False)\n\n    errors = InvalidModel.check()\n    self.assertEqual(len(errors), 1)\n    self.assertEqual(errors[0].id, 'fields.E001')\n", "def test_generic_foreign_key_checks(self):\n    class InvalidModel(models.Model):\n        gfk = GenericForeignKey(ct_field='invalid_ct_field')\n\n    errors = InvalidModel.check()\n    self.assertEqual(len(errors), 1)\n    self.assertEqual(errors[0].id, 'contenttypes.E001')\n    self.assertEqual(\n        errors[0].msg,\n        \"The GenericForeignKey object ID references the nonexistent field 'object_id'.\",\n    )\n    self.assertEqual(errors[0].obj, InvalidModel._meta.get_field('gfk'))\n\n    class AnotherInvalidModel(models.Model):\n        gfk = GenericForeignKey(fk_field='invalid_fk_field')\n\n    errors = AnotherInvalidModel.check()\n    self.assertEqual(len(errors), 1)\n    self.assertEqual(errors[0].id, 'contenttypes.E002')\n    self.assertEqual(\n        errors[0].msg,\n        \"The GenericForeignKey content type references the nonexistent field 'content_type'.\",\n    )\n    self.assertEqual(errors[0].obj, AnotherInvalidModel._meta.get_field('gfk'))\n", "def test_generic_foreign_key_checks(self):\n    class ModelWithGenericForeignKey(models.Model):\n        content_type = models.ForeignKey('contenttypes.ContentType', on_delete=models.CASCADE)\n        object_id = models.IntegerField()\n        gfk = GenericForeignKey()\n\n    errors = ModelWithGenericForeignKey.content_type.check()\n    self.assertEqual(errors, [])\n\n    class ModelWithInvalidContentObjectField(models.Model):\n        content_object = models.CharField(max_length=255)\n        object_id = models.IntegerField()\n        gfk = GenericForeignKey(ct_field='content_object')\n\n    errors = ModelWithInvalidContentObjectField.gfk.check()\n    self.assertEqual(len(errors), 1)\n    self.assertEqual(errors[0].id, 'contenttypes.E003')\n"], "sample_344": ["def test_state_clone(self):\n    \"\"\"Tests that cloning a state results in an independent copy.\"\"\"\n    model_state = ModelState(\n        'app',\n        'Model',\n        fields=[\n            ('id', models.AutoField(primary_key=True)),\n        ],\n    )\n    project_state = ProjectState(models={'app.model': model_state})\n    project_state.apps  # Trigger cache population\n\n    cloned_state = project_state.clone()\n\n    # Change the original model state\n    model_state.fields['new_field'] = models.CharField(max_length=255)\n\n    # Rendering the cloned state should not include the new field\n    with self.assertRaises(FieldDoesNotExist):\n        cloned_state.apps.get_model('app', 'Model')._meta.get_field('new_field')\n", "def test_state_get_index_by_name(self):\n    project_state = ProjectState()\n    app_label = \"migrations\"\n    model_name = \"TestModel\"\n    index_name = \"my_index\"\n\n    # Create a model with an index.\n    model_state = ModelState(\n        app_label=app_label,\n        name=model_name,\n        fields=[],\n        options={\"indexes\": [models.Index(fields=[\"field1\"], name=index_name)]},\n    )\n    project_state.add_model(model_state)\n\n    # Get the index by name.\n    index = model_state.get_index_by_name(index_name)\n    self.assertEqual(index.name, index_name)\n", "def test_state_clone(self):\n    \"\"\"Cloned ProjectState has the same apps as the original\"\"\"\n    state = ProjectState()\n    with isolate_apps('app1', 'app2'):\n        state.apps = Apps(['app1', 'app2'], models={})\n        cloned_state = state.clone()\n        self.assertEqual(cloned_state.apps.all_models, state.apps.all_models)\n", "def test_render_model_with_custom_manager(self):\n    project_state = ProjectState()\n    app_label = 'testapp'\n\n    model_state = ModelState(\n        app_label,\n        'Food',\n        [\n            ('id', models.AutoField(primary_key=True)),\n        ],\n        {'verbose_name': 'food'},\n        managers=[\n            ('objects', FoodManager('a', 'b')),\n            ('no_migration_objects', NoMigrationFoodManager()),\n        ],\n    )\n    project_state.add_model(model_state)\n\n    model = project_state.apps.get_model(app_label, 'Food')\n    self.assertIsInstance(model.objects, FoodQuerySet)\n    self.assertEqual(model.objects.a, 'a')\n    self.assertEqual(model.objects.b, 'b')\n    self.assertNotIsInstance(model.no_migration_objects, FoodQuerySet)\n", "def test_state_clone(self):\n    \"\"\"\n    Cloning a ProjectState should clone all its ModelState instances and the\n    Apps registry.\n    \"\"\"\n    project_state = ProjectState()\n    model_state = ModelState(\n        app_label='app',\n        name='Model',\n        fields=[('id', models.AutoField(primary_key=True))],\n    )\n    project_state.add_model(model_state)\n    cloned_state = project_state.clone()\n\n    self.assertEqual(len(project_state.models), len(cloned_state.models))\n    for (app_label, model_name), cloned_model_state in cloned_state.models.items():\n        original_model_state = project_state.models[(app_label, model_name)]\n        self.assertIsNot(original_model_state, cloned_model_state)\n        self.assertEqual(original_model_state, cloned_model_state)\n\n    self.assertIsNot(project_state.apps, cloned_state.apps)\n    self.assertEqual(project_state.apps.models, cloned_state.apps.models)\n"], "sample_345": ["    def test_common_roots(self):\n        roots = autoreload.common_roots([\n            Path('/path/to/project/src/app1/file.py'),\n            Path('/path/to/project/src/app2/file.py'),\n            Path('/path/to/project/src/other/file.py'),\n        ])\n        self.assertEqual(roots, (Path('/path/to/project/src'),))\n", "def test_iter_modules_and_files_with_zip_importer(self):\n    with tempfile.TemporaryDirectory() as tmpdir:\n        filename = Path(tmpdir) / 'test.zip'\n        with zipfile.ZipFile(filename, 'w') as zip_file:\n            zip_file.writestr('test.py', b'def test(): pass')\n        sys.path.insert(0, str(filename))\n        self.import_and_cleanup('test')\n        self.assertFileFound(filename)\n        sys.path.remove(str(filename))\n", "    def test_empty(self):\n        self.assertEqual(autoreload.common_roots([]), ())\n", "def test_iter_modules_and_files_zip_file(self):\n    tmp_dir = tempfile.mkdtemp()\n    self.addCleanup(shutil.rmtree, tmp_dir)\n\n    with open(os.path.join(tmp_dir, 'example.zip'), 'wb') as f:\n        with zipfile.ZipFile(f, 'w') as zip_file:\n            zip_file.writestr('test.py', b'print(\"Hello, World!\")')\n\n    with extend_sys_path(tmp_dir):\n        self.import_and_cleanup('test')\n        self.assertFileFound(Path(tmp_dir) / 'example.zip')\n", "    def test_common_roots(self):\n        paths = [\n            Path('/home/user/project/app1/models.py'),\n            Path('/home/user/project/app1/views.py'),\n            Path('/home/user/project/app2/models.py'),\n            Path('/home/user/project/app2/views.py'),\n        ]\n        common_roots = autoreload.common_roots(paths)\n        self.assertEqual(common_roots, (Path('/home/user/project'),))\n"], "sample_346": ["def test_method_decorator(self):\n    class MyClass:\n            return \"hello\"\n\n    decorator = lambda x: x\n    decorated_class = method_decorator(decorator, name='my_method')(MyClass)\n    self.assertEqual(decorated_class().my_method(), \"hello\")\n", "def test_method_decorator(self):\n            return func(*args, **kwargs)\n        return wrapper\n\n    class MyClass:\n            return \"Hello, World!\"\n\n    method_decorator(my_decorator, name='my_method')(MyClass)\n\n    obj = MyClass()\n    self.assertEqual(obj.my_method(), \"Hello, World!\")\n", "def test_method_decorator(self):\n    class MyClass:\n            return \"Hello, World!\"\n\n    decorator = lambda x: x\n    decorated_class = method_decorator(decorator, name='my_method')(MyClass)\n    self.assertEqual(decorated_class().my_method(), \"Hello, World!\")\n\n    # Test that the decorator was applied correctly\n            return \"Decorated: \" + func(*args, **kwargs)\n        return wrapper\n\n    decorated_class = method_decorator(my_decorator, name='my_method')(MyClass)\n    self.assertEqual(decorated_class().my_method(), \"Decorated: Hello, World!\")\n", "def test_method_decorator(self):\n            return \"Hello, \" + func(*args, **kwargs)\n        return wrapper\n\n    class MyClass:\n        @method_decorator(my_decorator, name='my_method')\n            return \"world!\"\n\n    obj = MyClass()\n    self.assertEqual(obj.my_method(), \"Hello, world!\")\n\n    # Test that the decorator was applied correctly\n    self.assertTrue(hasattr(MyClass, 'my_method'))\n    self.assertIsInstance(MyClass.my_method, type(obj.my_method.__func__))\n    self.assertEqual(MyClass.my_method.__name__, 'my_method')\n\n    # Test that the original method is still callable\n    self.assertEqual(obj.my_method.__func__(obj), \"world!\")\n", "def test_method_decorator(self):\n    class TestClass:\n            return \"Expected result\"\n\n    decorator = lambda func: lambda self: \"Decorated result\"\n    decorated_class = method_decorator(decorator, name='test_method')(TestClass)\n    instance = decorated_class()\n    self.assertEqual(instance.test_method(), \"Decorated result\")\n\n    # Test that the original method is preserved\n    self.assertEqual(TestClass.test_method.__name__, \"test_method\")\n    self.assertEqual(decorated_class.test_method.__name__, \"test_method\")\n\n    # Test that the decorator is applied correctly when using a class-based decorator\n    class ClassDecorator:\n            self.func = func\n\n            return \"Decorated result\"\n\n    decorated_class = method_decorator(ClassDecorator, name='test_method')(TestClass)\n    instance = decorated_class()\n    self.assertEqual(instance.test_method(), \"Decorated result\")\n"], "sample_347": ["def test_make_aware(self):\n    naive_dt = datetime.datetime(2022, 6, 15, 14, 30)\n    for tz in PARIS_IMPLS:\n        with timezone.override(tz):\n            aware_dt = timezone.make_aware(naive_dt)\n            self.assertEqual(aware_dt.tzinfo, tz)\n            self.assertEqual(aware_dt.utcoffset(), tz.utcoffset(aware_dt))\n", "def test_make_aware(self):\n    naive_datetime = datetime.datetime(2022, 1, 1, 12, 0)\n    aware_datetime = timezone.make_aware(naive_datetime, timezone=EAT)\n\n    self.assertEqual(aware_datetime.tzinfo, EAT)\n    self.assertEqual(aware_datetime.utcoffset(), datetime.timedelta(hours=3))\n", "def test_get_default_timezone(self):\n    self.assertEqual(timezone.get_default_timezone(), pytz.timezone(settings.TIME_ZONE))\n    with override_settings(TIME_ZONE='America/New_York'):\n        self.assertEqual(timezone.get_default_timezone(), pytz.timezone('America/New_York'))\n", "def test_make_aware(self):\n    naive = datetime.datetime(2011, 9, 1, 13, 20, 30)\n    for tz in PARIS_IMPLS:\n        with timezone.override(tz):\n            aware = timezone.make_aware(naive)\n            self.assertEqual(aware.tzinfo, tz)\n            self.assertEqual(aware, tz.localize(naive))\n", "def test_get_default_timezone(self):\n    self.assertEqual(timezone.get_default_timezone(), pytz.timezone(settings.TIME_ZONE))\n    with override_settings(TIME_ZONE='Europe/London'):\n        self.assertEqual(timezone.get_default_timezone(), pytz.timezone('Europe/London'))\n"], "sample_348": ["    def test_modelformset_factory_requires_fields_or_exclude(self):\n        with self.assertRaisesMessage(ValueError, 'Calling modelformset_factory without defining \\'fields\\' or \\'exclude\\' explicitly is prohibited.'):\n            modelformset_factory(Band)\n", "    def test_modelform_defines_fields(self):\n        class TestForm(forms.ModelForm):\n            class Meta:\n                model = Band\n                fields = '__all__'\n\n        self.assertIs(True, modelform_defines_fields(TestForm))\n\n        class TestForm(forms.ModelForm):\n            class Meta:\n                model = Band\n                exclude = []\n\n        self.assertIs(True, modelform_defines_fields(TestForm))\n\n        class TestForm(forms.ModelForm):\n            class Meta:\n                model = Band\n\n        self.assertIs(False, modelform_defines_fields(TestForm))\n", "    def test_model_choice_field_with_queryset(self):\n        field = forms.ModelChoiceField(queryset=User.objects.all())\n        self.assertEqual(field.widget.choices, field.choices)\n        # Test deep copy\n        import copy\n        field_copy = copy.deepcopy(field)\n        self.assertEqual(field_copy.queryset.all()[0], field.queryset.all()[0])\n", "    def test_model_choice_field(self):\n        class BandAdmin(ModelAdmin):\n            raw_id_fields = ('best_album',)\n\n        self.assertIsValid(BandAdmin, Band)\n\n        class AlbumAdmin(ModelAdmin):\n            raw_id_fields = ('band',)\n\n        self.assertIsInvalid(AlbumAdmin, Song, 'The value of raw_id_fields must be a list or tuple.', id='admin.E001')\n", "    def test_modelformset_factory(self):\n        class MyModelAdmin(ModelAdmin):\n            formset = modelformset_factory(Band, fields=('name',))\n\n        self.assertIsValid(MyModelAdmin, Band)\n"], "sample_349": ["def test_autocomplete_select_render_required(self):\n    form = RequiredBandForm()\n    with translation.override('en'):\n        output = form.as_p()\n    self.assertIn('data-allow-clear=\"false\"', output)\n    self.assertNotIn(self.empty_option, output)\n\n    form = NotRequiredBandForm()\n    with translation.override('en'):\n        output = form.as_p()\n    self.assertIn('data-allow-clear=\"true\"', output)\n    self.assertIn(self.empty_option, output)\n", "def test_autocomplete_mixin_build_attrs(self):\n    form = AlbumForm()\n    field_name = 'band'\n    widget = form.fields[field_name].widget\n    attrs = widget.build_attrs({'id': 'id_band'}, name='band')\n    self.assertEqual(attrs['data-ajax--cache'], 'true')\n    self.assertEqual(attrs['data-ajax--delay'], 250)\n    self.assertEqual(attrs['data-ajax--type'], 'GET')\n    self.assertIn('data-ajax--url', attrs)\n    self.assertEqual(attrs['data-app-label'], 'admin_widgets')\n    self.assertEqual(attrs['data-model-name'], 'album')\n    self.assertEqual(attrs['data-field-name'], 'band')\n    self.assertEqual(attrs['data-theme'], 'admin-autocomplete')\n    self.assertEqual(attrs['data-allow-clear'], 'false')\n    self.assertEqual(attrs['data-placeholder'], '')\n    self.assertEqual(attrs['lang'], translation.get_language())\n    self.assertIn('class', attrs)\n    self.assertIn('my-class', attrs['class'])\n    self.assertIn('admin-autocomplete', attrs['class'])\n", "def test_autocomplete_select_widget_with_choices(self):\n    form = AlbumForm()\n    self.assertHTMLEqual(\n        str(form['band']),\n        '<select class=\"my-class admin-autocomplete\" '\n        'data-ajax--cache=\"true\" data-ajax--delay=\"250\" data-ajax--type=\"GET\" '\n        'data-ajax--url=\"/autocomplete/\" '\n        'data-app-label=\"albums\" data-model-name=\"album\" data-field-name=\"band\" '\n        'data-theme=\"admin-autocomplete\" data-allow-clear=\"false\" data-placeholder=\"\" '\n        'lang=\"en\" name=\"band\">'\n        '<option value=\"1\">The Doors</option><option value=\"2\">The Beatles</option></select>'\n    )\n", "def test_autocomplete_select(self):\n    form = AlbumForm()\n    band_field = form.fields['band']\n    widget = band_field.widget\n    attrs = widget.build_attrs({'id': 'id_band'}, name='band')\n    self.assertEqual(attrs['data-app-label'], 'admin_widgets')\n    self.assertEqual(attrs['data-model-name'], 'band')\n    self.assertEqual(attrs['data-field-name'], 'band')\n    self.assertEqual(attrs['data-ajax--url'], '/autocomplete/')\n    self.assertEqual(attrs['data-theme'], 'admin-autocomplete')\n    self.assertEqual(attrs['class'], 'my-class admin-autocomplete')\n", "def test_autocomplete_select_required_field(self):\n    form = RequiredBandForm()\n    output = form.as_p()\n    self.assertNotIn(self.empty_option, output)\n    self.assertIn('data-allow-clear=\"false\"', output)\n\n    form = RequiredBandForm(data={'band': ''})\n    self.assertFalse(form.is_valid())\n"], "sample_350": ["def test_union_with_empty_queryset(self):\n    # A union of a non-empty queryset and an empty queryset should produce the same results as the non-empty queryset.\n    qs1 = Number.objects.filter(num__in=[1, 2, 3])\n    qs2 = Number.objects.none()\n    union_qs = qs1.union(qs2)\n    self.assertNumbersEqual(union_qs, [1, 2, 3])\n", "def test_union_ordering(self):\n    qs1 = Number.objects.filter(num__lte=5).order_by('num')\n    qs2 = Number.objects.filter(num__gt=5).order_by('-num')\n    union_qs = qs1.union(qs2)\n    self.assertNumbersEqual(union_qs, [0, 1, 2, 3, 4, 5, 9, 8, 7, 6])\n", "def test_union_with_empty_queryset(self):\n    qs1 = Number.objects.filter(num__lte=5)\n    qs2 = Number.objects.none()\n    union_qs = qs1.union(qs2)\n    self.assertNumbersEqual(union_qs, [0, 1, 2, 3, 4, 5])\n", "def test_union(self):\n    qs1 = Number.objects.filter(num__lte=5)\n    qs2 = Number.objects.filter(num__gte=5)\n    union_qs = qs1.union(qs2)\n    self.assertNumbersEqual(union_qs, range(10))\n    self.assertNumbersEqual(union_qs.union(qs1), range(10))\n    self.assertNumbersEqual(qs1.union(union_qs), range(10))\n", "def test_union_with_values_list(self):\n    qs1 = Number.objects.filter(num__lte=5).values_list('num', flat=True)\n    qs2 = Number.objects.filter(num__gt=5).values_list('num', flat=True)\n    self.assertQuerysetEqual(qs1.union(qs2), range(10), ordered=False)\n"], "sample_351": ["def test_model_choice_field_iter_value_repr(self):\n    field = forms.ModelChoiceField(Category.objects.all())\n    iterator = ModelChoiceIterator(field)\n    choice = iterator.choice(self.c1)\n    self.assertEqual(repr(choice[0]), f\"ModelChoiceIteratorValue({self.c1.pk})\")\n", "def test_model_choice_field_iterator(self):\n    field = forms.ModelChoiceField(queryset=Category.objects.all())\n    iterator = ModelChoiceIterator(field)\n    self.assertEqual(len(iterator), 3)\n    choices = list(iterator)\n    self.assertEqual(choices[0][1], '---------')\n    self.assertEqual(choices[1][1], 'Entertainment')\n    self.assertEqual(choices[2][1], 'A test')\n    self.assertEqual(choices[3][1], 'Third')\n\n    # Test that the iterator doesn't include empty strings when no empty_label is provided\n    field = forms.ModelChoiceField(queryset=Category.objects.all(), empty_label=None)\n    iterator = ModelChoiceIterator(field)\n    self.assertEqual(len(iterator), 3)\n    choices = list(iterator)\n    self.assertEqual(choices[0][1], 'Entertainment')\n    self.assertEqual(choices[1][1], 'A test')\n    self.assertEqual(choices[2][1], 'Third')\n", "def test_model_choice_iterator_value(self):\n    category = Category.objects.get(slug='entertainment')\n    value = ModelChoiceIteratorValue(category.pk, category)\n    self.assertEqual(str(value), str(category.pk))\n    self.assertEqual(hash(value), hash(category.pk))\n    self.assertEqual(value, category.pk)\n    self.assertEqual(value, ModelChoiceIteratorValue(category.pk, category))\n    self.assertNotEqual(value, ModelChoiceIteratorValue(12345, None))\n", "def test_model_choice_iterator_value_repr(self):\n    value = ModelChoiceIteratorValue('test', 'instance')\n    self.assertEqual(repr(value), \"ModelChoiceIteratorValue('test', 'instance')\")\n", "def test_model_choice_field_choices_accepts_callables(self):\n    # Make sure the callable is actually called and not treated as a queryset.\n        return Category.objects.all()\n\n    field = forms.ModelChoiceField(queryset=get_queryset)\n    self.assertEqual(list(field.choices), [\n        ('', '---------'),\n        (self.c1.pk, 'Entertainment'),\n        (self.c2.pk, 'A test'),\n        (self.c3.pk, 'Third'),\n    ])\n\n    field = forms.ModelChoiceField(queryset=lambda: Category.objects.all())\n    self.assertEqual(list(field.choices), [\n        ('', '---------'),\n        (self.c1.pk, 'Entertainment'),\n        (self.c2.pk, 'A test'),\n        (self.c3.pk, 'Third'),\n    ])\n"], "sample_352": ["def test_where_node_split_having(self):\n    # Create a WhereNode with a mix of aggregate and non-aggregate children.\n    where_node = WhereNode([\n        Q(name='foo'),  # Non-aggregate\n        Count('name') > 1,  # Aggregate\n        Q(age=25),  # Non-aggregate\n        Max('age') > 30,  # Aggregate\n    ])\n\n    # Split the node into WHERE and HAVING parts.\n    where_part, having_part = where_node.split_having()\n\n    # Check that the non-aggregate conditions are in the WHERE part.\n    self.assertEqual(len(where_part.children), 2)\n    self.assertIsInstance(where_part.children[0], Q)\n    self.assertEqual(where_part.children[0].children, [('name', 'foo')])\n    self.assertIsInstance(where_part.children[1], Q)\n    self.assertEqual(where_part.children[1].children, [('age', 25)])\n\n    # Check that the aggregate conditions are in the HAVING part.\n    self.assertEqual(len(having_part.children), 2)\n    self.assertIsInstance(having_part.children[0], Q)\n    self.assertEqual(str(having_part.children[0]), '(COUNT(\"name\") > 1)')\n    self.assertIsInstance(having_part.children[1], Q)\n    self.assertEqual(str(having_part.children[1]), '(MAX(\"age\") > 30)')\n", "def test_where_node_split_having(self):\n    # Create a WhereNode with a mix of aggregate and non-aggregate children.\n    where_node = WhereNode([\n        Q(author=self.a1),  # Non-aggregate child\n        Q(num__gt=F('author__num')),  # Aggregate child\n        Q(extra__filterable=False),  # Non-aggregate child\n    ])\n\n    # Split the node into WHERE and HAVING parts.\n    where_part, having_part = where_node.split_having()\n\n    self.assertIsNotNone(where_part)\n    self.assertEqual(len(where_part.children), 2)  # Two non-aggregate children\n\n    self.assertIsNotNone(having_part)\n    self.assertEqual(len(having_part.children), 1)  # One aggregate child\n", "    def test_empty_node(self):\n        node = WhereNode(children=[])\n        with self.assertRaises(EmptyResultSet):\n            node.as_sql(None, None)\n", "    def test_empty(self):\n        node = WhereNode(children=[])\n        with self.assertRaises(EmptyResultSet):\n            node.as_sql(None, None)\n", "def test_where_node_clone(self):\n    q1 = Q(name='test')\n    q2 = Q(num=42)\n\n    n1 = WhereNode([q1, q2], connector=AND)\n    n2 = n1.clone()\n\n    self.assertEqual(n1.connector, n2.connector)\n    self.assertEqual(n1.negated, n2.negated)\n    self.assertEqual(len(n1.children), len(n2.children))\n\n    for c1, c2 in zip(n1.children, n2.children):\n        self.assertEqual(c1.connector, c2.connector)\n        self.assertEqual(c1.negated, c2.negated)\n        self.assertEqual(str(c1), str(c2))\n"], "sample_353": ["def test_create_superuser_password_validation_bypass(self):\n    call_command('createsuperuser', stdin=MockTTY())\n    self.assertEqual(User.objects.count(), 1)\n    user = User.objects.get(username='alice')\n    self.assertTrue(user.check_password('test1234'))\n", "def test_create_superuser_with_bypass_password_validation(self):\n    with mock.patch('sys.stdin', MockTTY()):\n        call_command('createsuperuser', verbosity=0)\n    self.assertEqual(User.objects.count(), 1)\n    user = User.objects.get(username='alice')\n    self.assertEqual(user.email, 'alice@example.com')\n    self.assertTrue(user.check_password('test123'))\n", "def test_create_superuser_password_validation_bypass(self):\n    call_command('createsuperuser', stdin=MockTTY())\n    user = User.objects.get(username='alice')\n    self.assertEqual(user.username, 'alice')\n    self.assertTrue(user.check_password('test1234'))\n", "def test_interactive_suppresses_validation_error(self):\n    # Ticket #28444 - Createsuperuser suppresses validation error for a user\n    # that cannot be created.\n    with self.assertRaises(CommandError), override_settings(AUTH_USER_MODEL='auth.CustomUserWithFK'):\n        call_command('createsuperuser', stdin=MockTTY(), verbosity=0)\n", "def test_createsuperuser_bypass_password_validation(self):\n    call_command('createsuperuser', stdin=MockTTY())\n    u = User.objects.get(username='alice')\n    self.assertEqual(u.username, 'alice')\n    self.assertTrue(u.is_superuser)\n    self.assertTrue(u.check_password('test123'))\n"], "sample_354": ["def test_create_superuser_empty_username(self):\n    with mock.patch('sys.stdin', MockTTY()):\n        with self.assertRaises(CommandError):\n            call_command('createsuperuser', verbosity=0)\n", "def test_create_superuser_password_validation_bypass(self):\n    with mock.patch('sys.stdin', MockTTY()):\n        call_command('createsuperuser', verbosity=0)\n        self.assertEqual(User.objects.filter(username='alice').count(), 1)\n", "def test_create_superuser_with_invalid_password(self):\n    # Test creating a superuser with an invalid password and bypassing validation.\n    call_command('createsuperuser', stdin=MockTTY(), verbosity=0)\n    user = User.objects.get(username='alice')\n    self.assertTrue(user.is_superuser)\n", "def test_non_interactive_mode_with_password_from_environment_variable(self):\n    os.environ['DJANGO_SUPERUSER_PASSWORD'] = 'testpassword'\n    with mock.patch('builtins.input', return_value='alice'):\n        call_command('createsuperuser', '--noinput', '--username=alice')\n    user = User.objects.get(username='alice')\n    self.assertEqual(user.username, 'alice')\n    self.assertTrue(user.check_password('testpassword'))\n    del os.environ['DJANGO_SUPERUSER_PASSWORD']\n", "def test_empty_username(self):\n    with self.assertRaises(CommandError):\n        call_command('createsuperuser', stdin=MockTTY())\n"], "sample_355": ["    def setUpTestData(cls):\n        cls.user = User.objects.create_user('test', 'test@example.com', 'test')\n        cls.group = Group.objects.create(name='Test Group')\n        cls.permission = Permission.objects.create(\n            name='Test Permission',\n            content_type=ContentType.objects.get_for_model(User),\n            codename='test_permission',\n        )\n        cls.group.permissions.add(cls.permission)\n        cls.user.groups.add(cls.group)\n", "    def setUpTestData(cls):\n        cls.permission = Permission.objects.create(\n            name='test_permission',\n            content_type=ContentType.objects.get_for_model(User),\n            codename='test_codename',\n        )\n", "    def setUpTestData(cls):\n        cls.user = User.objects.create_user('test', 'test@example.com', 'test')\n        cls.group = Group.objects.create(name='test_group')\n        cls.permission = Permission.objects.create(\n            codename='test_permission',\n            name='Test Permission',\n            content_type=ContentType.objects.get_for_model(User)\n        )\n        cls.group.permissions.add(cls.permission)\n        cls.user.groups.add(cls.group)\n", "    def setUpTestData(cls):\n        cls.user = User.objects.create_user('test', 'test@example.com', 'test')\n        cls.group = Group.objects.create(name='Test Group')\n        cls.permission = Permission.objects.create(\n            name='Test Permission',\n            content_type=ContentType.objects.get_for_model(User),\n            codename='test_permission',\n        )\n        cls.group.permissions.add(cls.permission)\n        cls.user.groups.add(cls.group)\n", "def test_get_all_permissions(self):\n    user = self.user\n    perms = user.get_all_permissions()\n    self.assertEqual(len(perms), 2)\n    self.assertIn('user_perm', perms)\n    self.assertIn('group_perm', perms)\n\n    # Test that get_all_permissions works with a superuser.\n    superuser = User.objects.create_superuser('super', 'super@example.com', 'super')\n    perms = superuser.get_all_permissions()\n    self.assertTrue(perms)  # superuser should have at least one permission\n\n    # Test that get_all_permissions works when there are no backends.\n    with override_settings(AUTHENTICATION_BACKENDS=[]):\n        perms = user.get_all_permissions()\n        self.assertEqual(len(perms), 0)\n"], "sample_356": ["def test_altered_unique_together_with_renamed_fields(self):\n    \"\"\"Tests that renamed fields are correctly handled in unique_together.\"\"\"\n    before = self.make_project_state([self.author_name, self.book_foo_together])\n    after = self.make_project_state([self.author_name_renamed, self.book_foo_together_2])\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertNumberMigrations(changes, 'otherapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"RenameField\"])\n    self.assertOperationTypes(changes, 'otherapp', 0, [\"AlterUniqueTogether\"])\n    self.assertOperationAttributes(changes, 'otherapp', 0, 0, name='book', unique_together={('title', 'author')})\n", "def test_alter_index_together(self):\n    \"\"\"Tests the addition and removal of index_together constraints.\"\"\"\n    changes = self.get_changes(\n        [self.book_foo_together],\n        [self.book_foo_together_2],\n    )\n    self.assertNumberMigrations(changes, \"otherapp\", 1)\n    self.assertOperationTypes(changes, \"otherapp\", 0, [\"AlterIndexTogether\"])\n    self.assertOperationAttributes(changes, \"otherapp\", 0, 0, name=\"book\", index_together={(\"title\", \"author\")})\n\n    changes = self.get_changes(\n        [self.book_foo_together],\n        [self.book_foo_together_3],\n    )\n    self.assertNumberMigrations(changes, \"otherapp\", 2)\n    self.assertOperationTypes(changes, \"otherapp\", 0, [\"AddField\"])\n    self.assertOperationTypes(changes, \"otherapp\", 1, [\"AlterIndexTogether\"])\n    self.assertOperationAttributes(changes, \"otherapp\", 1, 0, name=\"book\", index_together={(\"title\", \"newfield\")})\n\n    changes = self.get_changes(\n        [self.book_foo_together],\n        [self.book_foo_together_4],\n    )\n    self.assertNumberMigrations(changes, \"otherapp\", 2)\n    self.assertOperationTypes(changes, \"otherapp\", 0, [\"AddField\"])\n    self.assertOperationTypes(changes, \"otherapp\", 1, [\"AlterIndexTogether\"])\n    self.assertOperationAttributes(changes, \"otherapp\", 1, 0, name=\"book\", index_together={(\"title\", \"newfield2\")})\n", "def test_generate_added_field_with_db_column(self):\n    \"\"\"\n    Tests the addition of a field with db_column.\n    \"\"\"\n    changes = self.get_changes(\n        [self.author_name],\n        [ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n            (\"added_field\", models.IntegerField(db_column='custom_db_column')),\n        ])],\n    )\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AddField\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"added_field\")\n    self.assertOperationFieldAttributes(changes, 'testapp', 0, 0, db_column=\"custom_db_column\")\n", "def test_alter_model_options(self):\n    \"\"\"Tests that model options are correctly altered.\"\"\"\n    # Create the initial state, with a model.\n    before_state = self.make_project_state([self.author_empty])\n    after_state = self.make_project_state([ModelState(\"testapp\", \"Author\", [(\"id\", models.AutoField(primary_key=True))], {\n        \"permissions\": [('can_hire', 'Can hire')],\n        \"verbose_name\": \"Authi\",\n    })])\n\n    changes = self.get_changes(before_state, after_state)\n\n    # Right number of migrations?\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    # Right operations in there?\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterModelOptions\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, options={\n        \"permissions\": [('can_hire', 'Can hire')],\n        \"verbose_name\": \"Authi\",\n    })\n", "def test_alter_model_options(self):\n    \"\"\"\n    Tests the autodetector correctly handles changes to model options.\n    \"\"\"\n    # Test adding a verbose name\n    changes = self.get_changes(\n        [self.author_empty],\n        [ModelState(\"testapp\", \"Author\", [(\"id\", models.AutoField(primary_key=True))], {\"verbose_name\": \"Authi\"})],\n    )\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['AlterModelOptions'])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, options={'verbose_name': 'Authi'})\n\n    # Test changing a verbose name\n    changes = self.get_changes(\n        [ModelState(\"testapp\", \"Author\", [(\"id\", models.AutoField(primary_key=True))], {\"verbose_name\": \"Authi\"})],\n        [ModelState(\"testapp\", \"Author\", [(\"id\", models.AutoField(primary_key=True))], {\"verbose_name\": \"Author\"})],\n    )\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['AlterModelOptions'])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, options={'verbose_name': 'Author'})\n"], "sample_357": ["def test_alter_db_table(self):\n    \"\"\"\n    Tests the db_table alteration.\n    \"\"\"\n    changes = self.get_changes([self.author_with_db_table_options], [self.author_with_new_db_table_options])\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['AlterModelTable'])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name='author', table='author_two')\n\n    # Test altering db_table on a model rename\n    changes = self.get_changes([self.author_with_db_table_options], [self.author_renamed_with_new_db_table_options])\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['RenameModel', 'AlterModelTable'])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, old_name='Author', new_name='NewAuthor')\n    self.assertOperationAttributes(changes, 'testapp', 0, 1, name='newauthor', table='author_three')\n", "def test_create_altered_constraints(self):\n    \"\"\"Tests the create_altered_constraints method.\"\"\"\n    operation_name = operations.AddConstraint.option_name\n    before = self.make_project_state([self.author_empty])\n    after = self.make_project_state([\n        ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n        ], options={operation_name: [\n            models.CheckConstraint(check=models.Q(id__gt=0), name='check_id'),\n        ]}),\n    ])\n    changes = MigrationAutodetector(before, after)._detect_changes()\n    # Right number of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    # Right number of AddConstraint operations?\n    migration = changes['testapp'][0]\n    add_constraint_ops = [op for op in migration.operations if isinstance(op, operations.AddConstraint)]\n    self.assertEqual(len(add_constraint_ops), 1)\n", "def test_altered_index_together(self):\n    \"\"\"Tests that altered index_together are correctly detected.\"\"\"\n    changes = self.get_changes(\n        [self.book_foo_together],\n        [self.book_foo_together_2],\n    )\n    self.assertNumberMigrations(changes, 'otherapp', 1)\n    self.assertOperationTypes(changes, 'otherapp', 0, ['AlterIndexTogether'])\n    self.assertOperationAttributes(changes, 'otherapp', 0, 0, name='book', index_together={('title', 'author')})\n", "def test_alter_index_together_add_and_remove(self):\n    before = self.make_project_state([self.book_foo_together])\n    after = self.make_project_state([self.book_foo_together_2])\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, 'otherapp', 1)\n    self.assertOperationTypes(changes, 'otherapp', 0, ['AlterIndexTogether'])\n    self.assertOperationAttributes(changes, 'otherapp', 0, 0, name='book', index_together={('title', 'author')})\n", "def test_create_altered_constraints(self):\n    # Create the initial model state.\n    before_state = self.make_project_state([self.author_name])\n\n    # Alter the model by adding a constraint.\n    after_state = self.make_project_state([\n        ModelState(\n            \"testapp\",\n            \"Author\",\n            [(\"id\", models.AutoField(primary_key=True)), (\"name\", models.CharField(max_length=200))],\n            {\n                \"constraints\": [\n                    models.CheckConstraint(check=models.Q(name__contains='Bob'), name='name_contains_bob')\n                ]\n            },\n        )\n    ])\n\n    # Construct the autodetector.\n    changes = self.get_changes(before_state, after_state)\n\n    # Assert the correct operations are generated.\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AddConstraint\"])\n    self.assertOperationAttributes(\n        changes,\n        \"testapp\",\n        0,\n        0,\n        constraint=models.CheckConstraint(check=models.Q(name__contains='Bob'), name='name_contains_bob'),\n    )\n"], "sample_358": ["    def setUp(self):\n        self.reference = TableColumns('table', ['column1', 'column2'])\n", "    def setUp(self):\n        self.reference = TableColumns('table', ['column1', 'column2'])\n", "    def setUp(self):\n        self.reference = TableColumns('table', ['column1', 'column2'])\n", "    def test_references_column(self):\n        reference = TableColumns('table', ['column1', 'column2'])\n        self.assertTrue(reference.references_column('table', 'column1'))\n        self.assertTrue(reference.references_column('table', 'column2'))\n        self.assertFalse(reference.references_column('table', 'column3'))\n        self.assertFalse(reference.references_column('other_table', 'column1'))\n", "    def setUp(self):\n        self.reference = Columns('table', ['column1', 'column2'], lambda column: column.upper())\n"], "sample_359": ["def test_create_model(self):\n    operation = migrations.CreateModel(\n        name='Pony',\n        fields=[\n            ('id', models.AutoField(primary_key=True)),\n            ('pink', models.IntegerField()),\n        ],\n    )\n    # Test the state alteration\n    project_state = ProjectState()\n    new_state = project_state.clone()\n    operation.state_forwards('testapp', new_state)\n    self.assertEqual(new_state.models['testapp', 'pony'].fields, [\n        ('id', models.AutoField(primary_key=True)),\n        ('pink', models.IntegerField()),\n    ])\n    # Test the database creation\n    with connection.schema_editor() as editor:\n        operation.database_forwards('testapp', editor, project_state, new_state)\n    self.assertTableExists('testapp_pony')\n    # And test reversal\n    with connection.schema_editor() as editor:\n        operation.database_backwards('testapp', editor, new_state, project_state)\n    self.assertTableNotExists('testapp_pony')\n", "    def test_create_model(self):\n        operation = migrations.CreateModel(\n            name='TestModel',\n            fields=[\n                ('id', models.AutoField(primary_key=True)),\n                ('name', models.CharField(max_length=255)),\n            ],\n        )\n        self.apply_operations(operation)\n        project_state = self.set_up_test_model('testapp', 'TestModel')\n        self.assertTableExists('testapp_TestModel')\n        self.assertColumnExists('testapp_TestModel', 'id')\n        self.assertColumnExists('testapp_TestModel', 'name')\n        self.unapply_operations(operation)\n        self.assertTableNotExists('testapp_TestModel')\n", "    def test_create_model(self):\n        operation = migrations.CreateModel(\n            \"Pony\",\n            [\n                (\"pink\", models.IntegerField(primary_key=True)),\n            ],\n        )\n        self.apply_operations([operation])\n        project_state = self.set_up_test_model(\"testapp\", \"pony\")\n        apps = project_state.apps\n        Pony = apps.get_model(\"testapp\", \"Pony\")\n        pony = Pony.objects.create(pink=1)\n        self.assertEqual(pony.pink, 1)\n", "    def test_create_model(self):\n        # Test the state alteration\n        operation = migrations.CreateModel(\n            name='Pony',\n            fields=[\n                ('id', models.AutoField(primary_key=True)),\n            ],\n        )\n        self.assertEqual(operation.state_forwards('tests', ProjectState()), [\n            ModelState('tests', 'Pony', [\n                ('id', models.AutoField(primary_key=True)),\n            ]),\n        ])\n        # Test the database alteration\n        self.apply_operations('test_crmo', [operation])\n        self.assertTableExists('test_crmo_pony')\n        with connection.schema_editor() as schema_editor:\n            self.assertEqual(schema_editor.get_table_description('test_crmo_pony'), [\n                {'name': 'id', 'type': connection.features.introspected_field_type(models.AutoField), 'nullable': False, 'default': None, 'primary_key': True},\n            ])\n        # And test reversal\n        self.unapply_operations('test_crmo', [operation])\n        self.assertTableNotExists('test_crmo_pony')\n", "    def test_create_model(self):\n        # Test the state alteration\n        operation = migrations.CreateModel(\n            \"Pony\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"pink\", models.IntegerField(default=1)),\n            ],\n        )\n        # Test the database alteration\n        self.apply_operations(operation)\n        with connection.cursor() as cursor:\n            cursor.execute(\"\"\"\n                SELECT sql\n                FROM sqlite_master\n                WHERE type='table' AND name='pony';\n            \"\"\")\n            self.assertEqual(cursor.fetchall(), [(\"CREATE TABLE pony (id INTEGER PRIMARY KEY AUTOINCREMENT, pink integer NOT NULL DEFAULT 1);\",)])\n"], "sample_360": ["    def test_cache_middleware(self):\n        factory = RequestFactory()\n        request = factory.get('/foo')\n        middleware = CacheMiddleware(get_response=lambda req: HttpResponse('Hello, world!'))\n        response = middleware(request)\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.content, b'Hello, world!')\n        # Check that the response was cached\n        cache_key = get_cache_key(request)\n        self.assertIsNotNone(cache_key)\n        cached_response = cache.get(cache_key)\n        self.assertIsNotNone(cached_response)\n        self.assertEqual(cached_response.status_code, 200)\n        self.assertEqual(cached_response.content, b'Hello, world!')\n", "    def test_cache_middleware(self):\n        # Set up a cache middleware\n        cache_middleware = CacheMiddleware(get_response=lambda req: HttpResponse('Hello, world!'))\n\n        # Create a request and response\n        request = HttpRequest()\n        request.method = 'GET'\n        request.path = '/test/'\n\n        # Process the request through the middleware\n        response = cache_middleware.process_request(request)\n\n        # If there's no cached response, process_response should cache it\n        if response is None:\n            response = cache_middleware.process_response(request, HttpResponse('Hello, world!'))\n\n        # Check that the response was cached\n        self.assertIsNotNone(cache_middleware.cache.get(get_cache_key(request)))\n\n        # Make another request to check that the response is fetched from the cache\n        request2 = HttpRequest()\n        request2.method = 'GET'\n        request2.path = '/test/'\n        response2 = cache_middleware.process_request(request2)\n\n        # Check that the second response is the same as the first one\n        self.assertEqual(response.content, response2.content)\n", "    def setUp(self):\n        self.factory = RequestFactory()\n        self.middleware = UpdateCacheMiddleware(get_response=lambda req: HttpResponse())\n", "    def setUp(self):\n        self.factory = RequestFactory()\n        self.cache_middleware = CacheMiddleware(get_response=lambda req: HttpResponse())\n", "    def setUp(self):\n        self.factory = RequestFactory()\n        self.middleware = UpdateCacheMiddleware(get_response=lambda req: HttpResponse())\n"], "sample_361": ["def test_escapejs(self):\n    # Test escapejs against a list of known values\n    test_values = [\n        (\"\", \"\"),\n        (\"a\", \"a\"),\n        (\"\\\\\", \"\\\\\\\\u005C\"),\n        (\"'\", \"\\\\u0027\"),\n        ('\"', '\\\\u0022'),\n        (\">\", \"\\\\u003E\"),\n        (\"<\", \"\\\\u003C\"),\n        (\"&\", \"\\\\u0026\"),\n        (\"=\", \"\\\\u003D\"),\n        (\"-\", \"\\\\u002D\"),\n        (\";\", \"\\\\u003B\"),\n        (\"`\", \"\\\\u0060\"),\n        (\"\\u2028\", \"\\\\u2028\"),\n        (\"\\u2029\", \"\\\\u2029\"),\n    ]\n    for value, expected in test_values:\n        self.check_output(escapejs, value, expected)\n", "def test_escapejs(self):\n    self.check_output(escapejs, 'testing', 'testing')\n    self.check_output(escapejs, 'te\\fsting', 'te\\\\u0009sting')\n    self.check_output(escapejs, 'te\\nsting', 'te\\\\nstuffing')\n    self.check_output(escapejs, 'te\\rsting', 'te\\\\rsting')\n    self.check_output(escapejs, 'te\\tsting', 'te\\\\tsting')\n    self.check_output(escapejs, '<a>test</a>', '\\\\u003Ca\\\\u003Etest\\\\u003C/a\\\\u003E')\n    self.check_output(escapejs, '\"double quotes\"', '\\\\\"double quotes\\\\\"')\n    self.check_output(escapejs, \"'single quotes'\", \"\\\\'single quotes\\\\'\")\n", "def test_escapejs(self):\n    # escapejs() escapes ASCII characters with a value less than 32.\n    self.check_output(escapejs, 'a\\rb', 'a\\\\u000D\\\\u0008b')\n    # escapejs() escapes special chars.\n    self.check_output(escapejs, 'a\\'\"b&c\\\\d<e>f/g=h;i`j', 'a\\\\u0027\\\\u0022b\\\\u0026c\\\\\\\\d\\\\u003Ce\\\\u003Ef/g=h;i\\\\u0060j')\n", "def test_escapejs(self):\n    self.check_output(escapejs, \"Hello, World!\")\n    self.check_output(escapejs, 'var name = \"John\";')\n    self.check_output(escapejs, \"foo\\nbar\")\n    self.check_output(escapejs, \"foo\\rbar\")\n    self.check_output(escapejs, \"foo\\tbar\")\n    self.check_output(escapejs, \"foo\\u2028bar\")\n    self.check_output(escapejs, \"foo\\u2029bar\")\n    self.check_output(escapejs, lazystr(\"Hello, World!\"))\n", "def test_escapejs(self):\n    # Make sure escapejs does the right thing\n    self.assertEqual(escapejs('foo'), 'foo')\n    self.assertEqual(escapejs('foo\\nbar'), 'foo\\\\nbar')\n    self.assertEqual(escapejs('foo\\r\\nbar'), 'foo\\\\r\\\\nbar')\n    self.assertEqual(escapejs('\"foo\"'), '\\\\u0022foo\\\\u0022')\n    self.assertEqual(escapejs(\"'foo'\"), '\\\\u0027foo\\\\u0027')\n"], "sample_362": ["def test_generate_added_indexes(self):\n    # Create model state with index.\n    before_state = self.make_project_state([self.author_name])\n    after_state = self.make_project_state([\n        ModelState(\n            \"testapp\", \"Author\", [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n            ],\n            options={\n                \"indexes\": [models.Index(fields=[\"name\"], name=\"author_name_idx\")]\n            }\n        )\n    ])\n\n    changes = self.get_changes(before_state, after_state)\n    # Right number of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n\n    # Right operations and options?\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AddIndex\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, model_name='author')\n", "def test_alter_unique_together_with_renamed_fields(self):\n    # Issue #26619\n    changes = self.get_changes(\n        [\n            self.author_name,\n            ModelState(\"testapp\", \"Book\", [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"title\", models.CharField(max_length=200)),\n                (\"newfield\", models.IntegerField()),\n            ], {\n                \"unique_together\": {(\"title\", \"newfield\")},\n            }),\n        ],\n        [\n            self.author_name,\n            ModelState(\"testapp\", \"Book\", [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"title\", models.CharField(max_length=200)),\n                (\"oldfield\", models.IntegerField()),\n            ], {\n                \"unique_together\": {(\"title\", \"oldfield\")},\n            }),\n        ],\n    )\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['RenameField'])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, old_name='newfield', new_name='oldfield')\n", "def test_altered_foo_together(self):\n    # Make the state\n    before = self.make_project_state([self.book_foo_together])\n    after = self.make_project_state([self.book_foo_together_2])\n    autodetector = MigrationAutodetector(before, after)\n    changes = autodetector._detect_changes()\n    # Right number of migrations?\n    self.assertNumberMigrations(changes, 'otherapp', 1)\n    # Right operations?\n    migration = changes['otherapp'][0]\n    self.assertEqual(len(migration.operations), 3)\n    self.assertIsInstance(migration.operations[0], migrations.AlterUniqueTogether)\n    self.assertIsInstance(migration.operations[1], migrations.AlterIndexTogether)\n    self.assertIsInstance(migration.operations[2], migrations.RenameField)\n", "def test_altered_db_table_options(self):\n    changes = self.get_changes(\n        [self.author_with_db_table_options],\n        [self.author_with_new_db_table_options],\n    )\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['AlterModelTable'])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name='author', table='author_two')\n\n    changes = self.get_changes(\n        [self.author_with_db_table_options],\n        [self.author_renamed_with_db_table_options],\n    )\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['RenameModel'])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, old_name='Author', new_name='NewAuthor')\n\n    changes = self.get_changes(\n        [self.author_with_db_table_options],\n        [self.author_renamed_with_new_db_table_options],\n    )\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['RenameModel', 'AlterModelTable'])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, old_name='Author', new_name='NewAuthor')\n    self.assertOperationAttributes(changes, 'testapp', 0, 1, name='newauthor', table='author_three')\n", "def test_generate_altered_db_table(self):\n    \"\"\"\n    Tests the autodetector can rename a model and change its db table in one go.\n    \"\"\"\n    # Test the state changes\n    before = self.make_project_state([self.author_with_db_table_options])\n    after = self.make_project_state([self.author_renamed_with_new_db_table_options])\n    changes = self.get_changes(before, after)\n    # Right number of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    # Right operations in the right order?\n    self.assertOperationTypes(changes, 'testapp', 0, [\"RenameModel\", \"AlterModelTable\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, old_name=\"Author\", new_name=\"NewAuthor\")\n    self.assertOperationAttributes(changes, 'testapp', 0, 1, name=\"NewAuthor\", table=\"author_three\")\n"], "sample_363": ["    def test_autocomplete_select_widget(self):\n        class MyModelAdmin(admin.ModelAdmin):\n            autocomplete_fields = ['owner']\n\n        ma = MyModelAdmin(Car, admin.site)\n        ff = ma.formfield_for_dbfield(Car._meta.get_field('owner'), request=None)\n\n        self.assertIsInstance(ff.widget, widgets.AutocompleteSelect)\n", "    def test_filtered_select_multiple(self):\n        widget = widgets.FilteredSelectMultiple('verbose_name', is_stacked=False)\n        self.assertEqual(widget.verbose_name, 'verbose_name')\n        self.assertFalse(widget.is_stacked)\n\n        context = widget.get_context('name', ['value1', 'value2'], {'id': 'id'})\n        self.assertIn('widget', context)\n        self.assertIn('attrs', context['widget'])\n        self.assertEqual(context['widget']['attrs']['class'], 'selectfilter')\n\n        widget = widgets.FilteredSelectMultiple('verbose_name', is_stacked=True)\n        context = widget.get_context('name', ['value1', 'value2'], {'id': 'id'})\n        self.assertEqual(context['widget']['attrs']['class'], 'selectfilter stacked')\n", "    def test_filtered_select_multiple_widget(self):\n        widget = widgets.FilteredSelectMultiple('test', False)\n        self.assertEqual(widget.get_context('test', [1, 2], {})['widget']['attrs']['class'], 'selectfilter')\n        self.assertIsInstance(widget.media, forms.Media)\n", "    def test_filteredselectmultiple(self):\n        widget = widgets.FilteredSelectMultiple('verbose_name', is_stacked=False)\n        self.assertEqual(widget.media.js, [\n            'admin/js/core.js',\n            'admin/js/SelectBox.js',\n            'admin/js/SelectFilter2.js',\n        ])\n", "    def test_filteredselectmultiple(self):\n        widget = widgets.FilteredSelectMultiple('test', False)\n        self.assertEqual(widget.media.js, [\n            'admin/js/core.js',\n            'admin/js/SelectBox.js',\n            'admin/js/SelectFilter2.js',\n        ])\n        self.assertEqual(widget.get_context('test', [], {})['widget']['attrs']['class'], 'selectfilter')\n"], "sample_364": ["def test_include_with_namespace(self):\n    # Test that including a urlconf with a namespace works.\n    resolver = path('inc/', include('urlpatterns.included_urls', namespace='included'))\n    inc_resolver = resolver.resolve('inc/test/')\n    self.assertEqual(inc_resolver.func, empty_view)\n    self.assertEqual(inc_resolver.namespace, 'included')\n    with self.assertRaises(ImproperlyConfigured):\n        path('inc/', include('urlpatterns.included_urls', namespace='included2'))\n", "def test_include_with_namespace(self):\n    with self.assertRaises(ImproperlyConfigured):\n        include(('urlpatterns.path_urls', 'app_name'), namespace='overridden_namespace')\n\n    with self.assertRaises(ImproperlyConfigured):\n        include('urlpatterns.path_urls', namespace='namespace_without_app_name')\n", "def test_include_with_namespace_hint(self):\n    # Test that include() with a tuple (urlconf_module, app_name) works.\n    # Also test that the namespace hint is correctly propagated to the resolver.\n    resolver = resolve('/namespace-hint-test/')\n    self.assertEqual(resolver.namespace, 'namespace-hint-test')\n    self.assertEqual(resolver.app_name, 'namespace-hint-test')\n\n    # Test that providing an explicit namespace argument overrides the hint.\n    resolver = resolve('/namespace-hint-override-test/')\n    self.assertEqual(resolver.namespace, 'overridden-namespace')\n    self.assertEqual(resolver.app_name, 'namespace-hint-test')\n", "def test_include_with_namespace(self):\n    with self.assertRaises(ImproperlyConfigured):\n        include(('someapp.urls', 'someapp'), namespace='overridden-namespace')\n", "def test_include_with_namespace(self):\n    with self.assertRaises(ImproperlyConfigured):\n        include(('urlpatterns', 'app_name'), namespace='overridden_namespace')\n\n    with self.assertRaises(ImproperlyConfigured):\n        include('urlpatterns', namespace='namespace_without_app_name')\n"], "sample_365": ["def test_cached_property(self):\n    class Test:\n        @cached_property\n            return 'test'\n\n    t = Test()\n    self.assertEqual(t.my_property, 'test')\n    with mock.patch.object(t, 'my_property', new_callable=mock.PropertyMock) as m:\n        m.fget.return_value = 'new value'\n        self.assertEqual(t.my_property, 'test')\n\n    # Test that __set_name__ sets the attribute name correctly\n    class Test2:\n            self.value = value\n\n        @cached_property\n            return self.value\n\n    t2 = Test2('initial')\n    self.assertEqual(t2.my_property, 'initial')\n    t2.value = 'changed'\n    self.assertEqual(t2.my_property, 'initial')\n", "def test_cached_property_docstring(self):\n    class Test:\n        @cached_property\n            \"\"\"This is a docstring.\"\"\"\n            return None\n\n    self.assertEqual(Test.docstring.__doc__, \"This is a docstring.\")\n", "def test_lazy_object_eq(self):\n    lazy_obj1 = SimpleLazyObject(lambda: 5)\n    lazy_obj2 = SimpleLazyObject(lambda: 5)\n    self.assertEqual(lazy_obj1, lazy_obj2)\n\n    lazy_obj3 = SimpleLazyObject(lambda: 'test')\n    self.assertNotEqual(lazy_obj1, lazy_obj3)\n\n    lazy_obj4 = SimpleLazyObject(lambda: 5)\n    self.assertEqual(lazy_obj1, lazy_obj4)\n    self.assertEqual(lazy_obj4, lazy_obj1)\n\n    # Test with a non-lazy object\n    self.assertEqual(lazy_obj1, 5)\n    self.assertEqual(5, lazy_obj1)\n", "def test_cached_property(self):\n    class cached:\n            self.value = value\n\n        @cached_property\n            return self.value\n\n    c = cached(\"test\")\n    with self.assertNumQueries(0):\n        self.assertEqual(c.expensive, \"test\")\n\n    # make sure it doesn't call the method again\n    with mock.patch.object(c, 'expensive.fget') as mock_get:\n        self.assertEqual(c.expensive, \"test\")\n        mock_get.assert_not_called()\n", "def test_cached_property(self):\n    class CachedPropertyTest:\n            self.value = value\n\n        @cached_property\n            return self.value * 2\n\n    obj = CachedPropertyTest(5)\n    self.assertEqual(obj.cached_value, 10)\n    self.assertEqual(obj.cached_value, 10)  # second access should return the cached result\n\n    # Test that changing the underlying value doesn't change the cached value\n    obj.value = 10\n    self.assertEqual(obj.cached_value, 10)  # should still be the original cached value\n\n    # Test that deleting the cached value allows it to be recomputed\n    del obj.cached_value\n    self.assertEqual(obj.cached_value, 20)  # should be recomputed with the new value\n"], "sample_366": ["def test_parse_date(self):\n    self.assertEqual(parse_date('2022-02-28'), date(2022, 2, 28))\n    self.assertIsNone(parse_date('2022-02-30'))  # Invalid day\n    self.assertIsNone(parse_date('2022-13-01'))  # Invalid month\n    self.assertIsNone(parse_date('2022-02'))  # Incomplete date\n    self.assertIsNone(parse_date('abcdefg'))  # Invalid format\n", "def test_parse_date_isoformat(self):\n    self.assertEqual(parse_date('2022-07-25'), date(2022, 7, 25))\n    with self.assertRaises(ValueError):\n        parse_date('2022-07-32')\n    self.assertIsNone(parse_date('invalid-date'))\n", "def test_parse_date_valid_date(self):\n    self.assertEqual(parse_date('2022-01-01'), date(2022, 1, 1))\n    self.assertEqual(parse_date('1999-12-31'), date(1999, 12, 31))\n", "def test_parse_date_valid(self):\n    self.assertEqual(parse_date('2022-01-01'), date(2022, 1, 1))\n    self.assertEqual(parse_date('2022-12-31'), date(2022, 12, 31))\n", "def test_parse_date(self):\n    self.assertIsNone(parse_date(''))\n    self.assertIsNone(parse_date('not a date'))\n    self.assertEqual(parse_date('2022-01-01'), date(2022, 1, 1))\n    self.assertEqual(parse_date('2022-12-31'), date(2022, 12, 31))\n    with self.assertRaises(ValueError):\n        parse_date('2022-02-30')\n"], "sample_367": ["def test_cache_control_decorator(self):\n    @cache_control(max_age=3600)\n        return HttpResponse('Hello, world!')\n\n    request = HttpRequest()\n    response = my_view(request)\n    self.assertEqual(response.get('Cache-Control'), 'max-age=3600')\n\n    @cache_control(public=True)\n        return HttpResponse('Hello, public world!')\n\n    response = my_public_view(request)\n    self.assertEqual(response.get('Cache-Control'), 'public')\n\n    @cache_control(must_revalidate=True)\n        return HttpResponse('Hello, revalidate world!')\n\n    response = my_must_revalidate_view(request)\n    self.assertEqual(response.get('Cache-Control'), 'must-revalidate')\n", "def test_cache_control_decorator(self):\n    @cache_control(private=True, max_age=3600)\n        return HttpResponse()\n\n    request = HttpRequest()\n    response = view(request)\n\n    self.assertEqual(response['Cache-Control'], 'private, max-age=3600')\n", "def test_cache_control_decorator(self):\n    @cache_control(max_age=3600)\n        return HttpResponse()\n\n    request = HttpRequest()\n    response = view_func(request)\n\n    self.assertEqual(response.get('Cache-Control'), 'max-age=3600')\n", "def test_cache_page_decorator(self):\n    request = HttpRequest()\n    request.method = 'GET'\n    request.path = '/path/'\n\n    # Test cache_page decorator with timeout and cache arguments\n    @cache_page(60, cache='my_cache', key_prefix='my_key')\n        return HttpResponse('Hello, World!')\n\n    response = view(request)\n    self.assertEqual(response.status_code, 200)\n    self.assertEqual(response.content, b'Hello, World!')\n\n    # Test cache_page decorator with only timeout argument\n    @cache_page(60)\n        return HttpResponse('Hello, World!')\n\n    response = view(request)\n    self.assertEqual(response.status_code, 200)\n    self.assertEqual(response.content, b'Hello, World!')\n", "def test_cache_page(self):\n    request = HttpRequest()\n    request.method = 'GET'\n    request.path = '/path/'\n\n    # Test cache_page with a timeout of 60 seconds\n    @cache_page(60)\n        return HttpResponse('Hello, world!')\n\n    response = view(request)\n    self.assertEqual(response.status_code, 200)\n    self.assertEqual(response.content, b'Hello, world!')\n\n    # Test cache_page with a cache alias and key prefix\n    @cache_page(60, cache='my_cache', key_prefix='my_prefix')\n        return HttpResponse('Hello, world!')\n\n    response = view_with_cache_and_prefix(request)\n    self.assertEqual(response.status_code, 200)\n    self.assertEqual(response.content, b'Hello, world!')\n"], "sample_368": ["def test_migration_plan_with_replacements(self):\n    \"\"\"\n    Test that the migration plan includes replacement migrations.\n    \"\"\"\n    executor = MigrationExecutor(connection)\n    recorder = MigrationRecorder(connection)\n\n    # Create a fake migration history with a replaced migration\n    recorder.record_applied(\"migrations\", \"0001_initial\")\n    recorder.record_applied(\"migrations\", \"0002_second\")\n\n    # Load the graph with the replaced migration\n    graph = MigrationGraph()\n    graph.add_node((\"migrations\", \"0001_initial\"), None)\n    graph.add_node((\"migrations\", \"0002_second\"), [(\"migrations\", \"0001_initial\")])\n    graph.add_node((\"migrations\", \"0003_third\"), [(\"migrations\", \"0002_second\")])\n    graph.add_replacement((\"migrations\", \"0002_second\"), [(\"migrations\", \"0002_2_second\")])\n\n    # Get the migration plan\n    plan = executor.migration_plan([(\"migrations\", \"0003_third\")])\n\n    # Check that the plan includes the replacement migration\n    self.assertEqual(len(plan), 2)\n    self.assertEqual(plan[0][0].name, \"0002_2_second\")\n    self.assertEqual(plan[1][0].name, \"0003_third\")\n", "def test_migration_plan_with_replaced_migrations(self):\n    \"\"\"\n    Test that replaced migrations are properly removed from the plan.\n    \"\"\"\n    executor = MigrationExecutor(connection)\n    recorder = MigrationRecorder(connection)\n    # Create a fake migration history\n    recorder.record_applied(\"migrations\", \"0001_initial\")\n    recorder.record_applied(\"migrations\", \"0002_second\")\n    recorder.record_applied(\"migrations\", \"0003_third\")\n\n    # Load the graph with a replacement migration\n    graph = MigrationGraph()\n    graph.add_node((\"migrations\", \"0001_initial\"), None)\n    graph.add_node((\"migrations\", \"0002_second\"), [(\"migrations\", \"0001_initial\")])\n    graph.add_node((\"migrations\", \"0003_third\"), [(\"migrations\", \"0002_second\")])\n    graph.add_node((\"migrations\", \"0004_replacement\"), None, replaces=[(\"migrations\", \"0002_second\"), (\"migrations\", \"0003_third\")])\n\n    executor.loader.graph = graph\n\n    plan = executor.migration_plan([(\"migrations\", \"0004_replacement\")])\n\n    self.assertEqual(plan, [\n        (graph.nodes[(\"migrations\", \"0001_initial\")], False),\n        (graph.nodes[(\"migrations\", \"0004_replacement\")], False),\n    ])\n", "def test_migration_plan_with_replacement(self):\n    \"\"\"\n    Test migration plan with replacement migrations.\n    \"\"\"\n    # Create a mock MigrationRecorder to return the applied migrations\n    recorder = mock.MagicMock(spec=MigrationRecorder)\n    recorder.applied_migrations.return_value = {\n        ('migrations', '0001_initial'),\n        ('migrations', '0002_second'),\n    }\n    executor = MigrationExecutor(connection, progress_callback=None)\n    executor.recorder = recorder\n\n    # Create a mock MigrationLoader to return the graph and nodes\n    loader = mock.MagicMock(spec=executor.loader)\n    loader.graph = mock.MagicMock(spec=MigrationGraph)\n    loader.graph.leaf_nodes.return_value = [\n        ('migrations', '0003_third'),\n    ]\n    loader.graph.node_map = {\n        ('migrations', '0001_initial'): mock.MagicMock(),\n        ('migrations', '0002_second'): mock.MagicMock(),\n        ('migrations', '0003_third'): mock.MagicMock(replaces=[('migrations', '0001_initial'), ('migrations', '0002_second')]),\n    }\n    executor.loader = loader\n\n    targets = [('migrations', '0003_third')]\n    plan = executor.migration_plan(targets)\n\n    self.assertEqual(plan, [\n        (loader.graph.nodes[('migrations', '0003_third')], False),\n    ])\n", "def test_migration_plan_with_replacement_migrations(self):\n    executor = MigrationExecutor(connection)\n    executor.loader.build_graph()\n    targets = [('migrations', '0002_second_migration')]\n    plan = executor.migration_plan(targets)\n\n    # Test that the replacement migration is properly expanded\n    self.assertEqual(len(plan), 3)\n    self.assertEqual(plan[0][0].name, '0001_initial')\n    self.assertEqual(plan[1][0].name, '0002_second_migration')\n    self.assertEqual(plan[2][0].name, '0003_third_migration')\n\n    # Test that the backwards plan correctly unapplies the replacement migration\n    targets = [('migrations', '0001_initial')]\n    plan = executor.migration_plan(targets, clean_start=True)\n    self.assertEqual(len(plan), 3)\n    self.assertEqual(plan[0][0].name, '0003_third_migration')\n    self.assertEqual(plan[1][0].name, '0002_second_migration')\n    self.assertEqual(plan[2][0].name, '0001_initial')\n", "def test_migration_plan(self):\n    executor = MigrationExecutor(connection)\n    targets = [(\"migrations\", \"0001_initial\")]\n    plan = executor.migration_plan(targets)\n    self.assertEqual(len(plan), 1)\n    self.assertEqual(plan[0][0].name, \"0001_initial\")\n    self.assertFalse(plan[0][1])\n"], "sample_369": ["def test_rename_model_with_db_table(self):\n    changes = self.get_changes(\n        [self.author_with_db_table_options],\n        [self.author_renamed_with_new_db_table_options],\n    )\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['RenameModel', 'AlterModelTable'])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, old_name='Author', new_name='NewAuthor')\n    self.assertOperationAttributes(changes, 'testapp', 0, 1, name='NewAuthor', table='author_three')\n", "def test_create_model_with_manager(self):\n    \"\"\"\n    Tests that a model with a manager is correctly created.\n    \"\"\"\n    changes = self.get_changes(\n        [],\n        [self.other_pony_food],\n    )\n    self.assertNumberMigrations(changes, 'otherapp', 1)\n    self.assertOperationTypes(changes, 'otherapp', 0, ['CreateModel'])\n    self.assertOperationAttributes(changes, 'otherapp', 0, 0, name='Pony')\n    self.assertEqual(len(changes['otherapp'][0].operations[0].managers), 3)\n    self.assertIsInstance(changes['otherapp'][0].operations[0].managers[0][1], models.Manager)\n    self.assertEqual(changes['otherapp'][0].operations[0].managers[0][0], 'food_qs')\n    self.assertIsInstance(changes['otherapp'][0].operations[0].managers[1][1], FoodManager)\n    self.assertEqual(changes['otherapp'][0].operations[0].managers[1][0], 'food_mgr')\n    self.assertEqual(changes['otherapp'][0].operations[0].managers[1][1].args, ('a', 'b'))\n    self.assertIsInstance(changes['otherapp'][0].operations[0].managers[2][1], FoodManager)\n    self.assertEqual(changes['otherapp'][0].operations[0].managers[2][0], 'food_mgr_kwargs')\n    self.assertEqual(changes['otherapp'][0].operations[0].managers[2][1].args, ('x', 'y', 3, 4))\n", "def test_autodetector_add_remove_indexes(self):\n    changes = self.get_changes(\n        [self.book],\n        [self.book_indexes],\n    )\n    self.assertNumberMigrations(changes, 'otherapp', 1)\n    self.assertOperationTypes(changes, 'otherapp', 0, ['AddIndex'])\n    self.assertOperationAttributes(changes, 'otherapp', 0, 0, name='book_title_author_idx', fields=['author', 'title'])\n\n    changes = self.get_changes(\n        [self.book_indexes],\n        [self.book_unordered_indexes],\n    )\n    self.assertNumberMigrations(changes, 'otherapp', 1)\n    self.assertOperationTypes(changes, 'otherapp', 0, ['RemoveIndex', 'AddIndex'])\n    self.assertOperationAttributes(changes, 'otherapp', 0, 0, name='book_title_author_idx', fields=['author', 'title'])\n    self.assertOperationAttributes(changes, 'otherapp', 0, 1, name='book_author_title_idx', fields=['title', 'author'])\n\n    changes = self.get_changes(\n        [self.book_indexes],\n        [self.book],\n    )\n    self.assertNumberMigrations(changes, 'otherapp', 1)\n    self.assertOperationTypes(changes, 'otherapp', 0, ['RemoveIndex'])\n    self.assertOperationAttributes(changes, 'otherapp', 0, 0, name='book_title_author_idx', fields=['author', 'title'])\n", "def test_alter_model_table(self):\n    changes = self.get_changes(\n        [self.author_with_db_table_options],\n        [self.author_with_new_db_table_options],\n    )\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['AlterModelTable'])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name='author', table='author_two')\n", "def test_alter_index_together(self):\n    \"\"\"Tests that AlterIndexTogether is correctly generated.\"\"\"\n    changes = self.get_changes(\n        [self.book_foo_together],\n        [self.book_foo_together_2],\n    )\n    self.assertNumberMigrations(changes, 'otherapp', 1)\n    self.assertOperationTypes(changes, 'otherapp', 0, ['AlterIndexTogether', 'AlterUniqueTogether'])\n    self.assertOperationAttributes(changes, 'otherapp', 0, 0, name='book', option_value={('title', 'author')})\n    self.assertOperationAttributes(changes, 'otherapp', 0, 1, name='book', option_value={('title', 'author')})\n"], "sample_370": ["def test_foreign_key_prefetch(self):\n    with CaptureQueriesContext(connection) as captured_queries:\n        authors = list(Author.objects.prefetch_related('first_book'))\n\n    self.assertEqual(len(captured_queries), 2)\n\n    query_sql = captured_queries[1]['sql']\n    self.assertWhereContains(query_sql, 'author.id IN')\n", "def test_prefetch_one_to_one(self):\n    with CaptureQueriesContext(connection) as captured_queries:\n        authors = list(Author.objects.prefetch_related('first_book'))\n        self.assertLess(len(captured_queries), len(authors))\n        for author in authors:\n            with self.assertNumQueries(0):\n                self.assertIsNotNone(author.first_book)\n", "def test_prefetch_related_with_to_attr(self):\n    with CaptureQueriesContext(connection) as captured_queries:\n        authors = list(Author.objects.prefetch_related(Prefetch('books', to_attr='book_list')))\n        self.assertWhereContains(captured_queries[-1]['sql'], 'book_id IN')\n\n    # Check that the attribute is set\n    for author in authors:\n        with self.subTest(author=author):\n            self.assertTrue(hasattr(author, 'book_list'))\n            self.assertIsInstance(author.book_list, list)\n            self.assertEqual(len(author.book_list), author.books.count())\n", "def test_forward_many_to_many_prefetch_related_objects(self):\n    with CaptureQueriesContext(connection) as captured_queries:\n        books = list(Book.objects.prefetch_related('authors'))\n        self.assertLessEqual(len(captured_queries), 3)\n        for book in books:\n            with self.assertNumQueries(0):\n                list(book.authors.all())\n", "def test_reverse_one_to_one_descriptor(self):\n    reader = Reader.objects.create(name='John')\n    bookmark = Bookmark.objects.create(reader=reader, book=self.book1)\n    self.assertEqual(bookmark.reader, reader)\n\n    # Accessing the reverse one-to-one descriptor should fetch the related object\n    bookmark.refresh_from_db()\n    with self.assertNumQueries(1):\n        self.assertEqual(bookmark.reader, reader)\n\n    # Accessing the reverse one-to-one descriptor again should not fetch the related object again\n    with self.assertNumQueries(0):\n        self.assertEqual(bookmark.reader, reader)\n"], "sample_371": ["    def test_cleansed_substitute(self):\n        filter = SafeExceptionReporterFilter()\n        self.assertEqual(filter.cleansed_substitute, '********************')\n", "    def test_cleansed_substitute(self):\n        reporter = SafeExceptionReporterFilter()\n        self.assertEqual(reporter.cleansed_substitute, '********************')\n", "    def test_cleanse_setting(self):\n        filter = SafeExceptionReporterFilter()\n        self.assertEqual(filter.cleanse_setting('ALLOWED_HOSTS', ['example.com']), ['example.com'])\n        self.assertEqual(filter.cleanse_setting('DATABASES', {'default': {'PASSWORD': 'secret'}}), {'default': {'PASSWORD': '********************'}})\n        self.assertEqual(filter.cleanse_setting('SECRET_KEY', 'secret'), '********************')\n        self.assertEqual(filter.cleanse_setting('OTHER_SETTING', 'value'), 'value')\n", "    def test_get_traceback_frames(self):\n        request = RequestFactory().get('/test/')\n        reporter = ExceptionReporter(request, *sys.exc_info())\n        frames = reporter.get_traceback_frames()\n        self.assertIsInstance(frames, list)\n", "    def test_cleanse_setting(self):\n        filter = SafeExceptionReporterFilter()\n        self.assertEqual(filter.cleanse_setting('SETTING', 'value'), 'value')\n        self.assertEqual(filter.cleanse_setting('SECRET_SETTING', 'secret_value'), '********************')\n"], "sample_372": ["def test_resolver_match_getitem(self):\n    resolver_match = ResolverMatch(None, (1, 2), {'a': 3, 'b': 4})\n    self.assertEqual(resolver_match[0], None)\n    self.assertEqual(resolver_match[1], (1, 2))\n    self.assertEqual(resolver_match[2], {'a': 3, 'b': 4})\n    with self.assertRaises(IndexError):\n        resolver_match[3]\n", "    def test_no_url_patterns(self):\n        resolver = get_resolver()\n        with self.assertRaises(Resolver404) as cm:\n            resolver.resolve('/test/')\n        self.assertEqual(cm.exception.args[0]['path'], '/test/')\n        self.assertEqual(cm.exception.args[0]['tried'], [])\n", "def test_resolver_match_with_callable(self):\n        pass\n\n    pattern = URLPattern(RegexPattern(r'^test/$'), view, name='test')\n    match = ResolverMatch(view, (), {}, route=str(pattern.pattern))\n    self.assertEqual(match.view_name, 'urlpatterns_reverse.no_urls.test')\n", "def test_get_resolver_empty_urlconf(self):\n    msg = \"The included URLconf 'urlpatterns_reverse.empty' does not appear to have any patterns in it.\"\n    with self.assertRaisesMessage(ImproperlyConfigured, msg):\n        get_resolver('urlpatterns_reverse.empty')\n", "    def test_resolver_repr(self):\n        resolver = get_resolver(None)\n        self.assertRegex(repr(resolver), r'^<URLResolver .*\\(None:None\\) .*>$')\n"], "sample_373": ["def test_get_return_data_type(self):\n    self.assertEqual(get_return_data_type('get_test_list'), 'List')\n    self.assertEqual(get_return_data_type('get_test_count'), 'Integer')\n    self.assertEqual(get_return_data_type('test_method'), '')\n", "def test_get_return_data_type(self):\n    self.assertEqual(get_return_data_type('get_something_list'), 'List')\n    self.assertEqual(get_return_data_type('get_something_count'), 'Integer')\n    self.assertEqual(get_return_data_type('something_else'), '')\n", "def test_get_return_data_type(self):\n    self.assertEqual(get_return_data_type('get_something_list'), 'List')\n    self.assertEqual(get_return_data_type('get_something_count'), 'Integer')\n    self.assertEqual(get_return_data_type('something_else'), '')\n", "def test_get_return_data_type(self):\n    self.assertEqual(get_return_data_type('get_something_list'), 'List')\n    self.assertEqual(get_return_data_type('get_something_count'), 'Integer')\n    self.assertEqual(get_return_data_type('get_something_else'), '')\n", "def test_model_detail_view(self):\n    response = self.client.get(reverse('django-admindocs-model-detail', args=['sites', 'site']))\n    self.assertEqual(response.status_code, 200)\n    self.assertTemplateUsed(response, 'admin_doc/model_detail.html')\n    self.assertEqual(response.context['name'], 'Site')\n"], "sample_374": ["def test_prefetch_related_objects_with_empty_result_cache(self):\n    with CaptureQueriesContext(connection) as captured_queries:\n        books = Book.objects.prefetch_related('authors')\n        prefetch_related_objects([book for book in books if book.pk == 1], 'authors')\n        self.assertEqual(len(captured_queries), 2)\n        self.assertWhereContains(captured_queries[1]['sql'], 'WHERE \"books\".\"id\" IN (1)')\n", "def test_prefetch_related_objects_with_to_attr(self):\n    qs = Book.objects.all()\n    prefetch_related_objects(qs, Prefetch('authors', to_attr='custom_authors'))\n    with self.assertNumQueries(0):\n        for book in qs:\n            with self.subTest(book=book):\n                self.assertTrue(hasattr(book, 'custom_authors'))\n                self.assertEqual(set(book.authors.all()), set(book.custom_authors))\n", "def test_prefetch_related_objects_with_to_attr(self):\n    qs = Book.objects.prefetch_related(Prefetch('authors', to_attr='the_authors'))\n    with self.assertNumQueries(2):\n        books = list(qs)\n        for book in books:\n            self.assertTrue(hasattr(book, 'the_authors'))\n            self.assertEqual(book.the_authors, list(book.authors.all()))\n", "def test_prefetch_related_objects_with_custom_to_attr(self):\n    qs = Book.objects.prefetch_related(Prefetch('authors', to_attr='custom_authors'))\n    with self.assertNumQueries(2):\n        books = list(qs)\n    for book in books:\n        with self.assertNumQueries(0):\n            self.assertEqual(hasattr(book, 'custom_authors'), True)\n            self.assertEqual(getattr(book, 'custom_authors', None) is None, False)\n", "def test_prefetch_related_with_to_attr(self):\n    related_objects = list(Author.objects.prefetch_related(Prefetch('friends__name', to_attr='friend_names')).all())\n    self.assertEqual(related_objects[0].friend_names, ['John'])\n"], "sample_375": ["def test_resolve_fields_and_relations(self):\n    \"\"\"\n    Tests that resolve_fields_and_relations properly resolves fields and relations.\n    \"\"\"\n    project_state = ProjectState()\n    app_label = 'myapp'\n\n    # Define a model with a foreign key to another model\n    model_state = ModelState(\n        app_label,\n        'MyModel',\n        [\n            ('id', models.AutoField(primary_key=True)),\n            ('my_field', models.ForeignKey('MyOtherModel')),\n        ],\n    )\n    project_state.models[(app_label, model_state.name_lower)] = model_state\n\n    # Define the other model\n    other_model_state = ModelState(\n        app_label,\n        'MyOtherModel',\n        [\n            ('id', models.AutoField(primary_key=True)),\n        ],\n    )\n    project_state.models[(app_label, other_model_state.name_lower)] = other_model_state\n\n    # Resolve fields and relations\n    project_state.resolve_fields_and_relations()\n\n    # Check that the relations are properly resolved\n    self.assertEqual(len(project_state._relations), 2)\n    self.assertIn((app_label, model_state.name_lower), project_state._relations)\n    self.assertIn((app_label, other_model_state.name_lower), project_state._relations)\n    self.assertEqual(len(project_state._relations[(app_label, model_state.name_lower)]), 1)\n    self.assertIn((app_label, other_model_state.name_lower), project_state._relations[(app_label, model_state.name_lower)])\n    self.assertEqual(len(project_state._relations[(app_label, other_model_state.name_lower)]), 0)\n", "def test_get_related_models_recursive(self):\n    state = ProjectState()\n    model_state = ModelState(\"app\", \"Model\", [\n        (\"id\", models.AutoField(primary_key=True)),\n    ])\n    state.add_model(model_state)\n    related_models = get_related_models_recursive(state.apps.get_model(\"app\", \"Model\"))\n    self.assertEqual(related_models, set())\n    # Add a model with an FK to Model.\n    model_state_2 = ModelState(\"app\", \"Model2\", [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"fk\", models.ForeignKey(\"app.Model\", models.CASCADE)),\n    ])\n    state.add_model(model_state_2)\n    related_models = get_related_models_recursive(state.apps.get_model(\"app\", \"Model\"))\n    self.assertEqual(related_models, {(\"app\", \"model2\")})\n    # Add a model with an M2M to Model.\n    model_state_3 = ModelState(\"app\", \"Model3\", [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"m2m\", models.ManyToManyField(\"app.Model\")),\n    ])\n    state.add_model(model_state_3)\n    related_models = get_related_models_recursive(state.apps.get_model(\"app\", \"Model\"))\n    self.assertEqual(related_models, {(\"app\", \"model2\"), (\"app\", \"model3\")})\n", "def test_get_related_models_recursive(self):\n    class ModelA(models.Model):\n        pass\n\n    class ModelB(models.Model):\n        a = models.ForeignKey(ModelA, on_delete=models.CASCADE)\n\n    class ModelC(models.Model):\n        b = models.ForeignKey(ModelB, on_delete=models.CASCADE)\n\n    with isolate_apps('myapp', 'modela', 'modelb', 'modelc'):\n        related_models = get_related_models_recursive(ModelC)\n        self.assertEqual(related_models, {('myapp', 'modela'), ('myapp', 'modelb')})\n", "def test_project_state_from_apps(self):\n    \"\"\"\n    Test that ProjectState.from_apps() properly constructs a ProjectState from\n    the given Apps.\n    \"\"\"\n    with isolate_apps('django.db.migrations.tests', kwarg_app='tests'):\n        apps = Apps()\n        project_state = ProjectState.from_apps(apps)\n        self.assertEqual(len(project_state.models), len(apps.get_models()))\n        for model in apps.get_models():\n            model_state = ModelState.from_model(model)\n            self.assertIn((model_state.app_label, model_state.name_lower), project_state.models)\n", "def test_get_related_models_recursive(self):\n    \"\"\"\n    get_related_models_recursive() returns a set of (app_label, model_name)\n    tuples for all related models to the given model.\n    \"\"\"\n    model_state = ModelState(\n        'myapp',\n        'MyModel',\n        [\n            ('id', models.AutoField(primary_key=True)),\n            ('fk', models.ForeignKey('self', on_delete=models.CASCADE)),\n            ('m2m', models.ManyToManyField('OtherModel')),\n        ],\n        {},\n        bases=(models.Model,),\n    )\n    other_model_state = ModelState(\n        'myapp',\n        'OtherModel',\n        [\n            ('id', models.AutoField(primary_key=True)),\n        ],\n        {},\n        bases=(models.Model,),\n    )\n    project_state = ProjectState()\n    project_state.add_model(model_state)\n    project_state.add_model(other_model_state)\n\n    related_models = get_related_models_recursive(project_state.apps.get_model('myapp', 'MyModel'))\n    self.assertEqual(related_models, {('myapp', 'othermodel')})\n"], "sample_376": ["def test_message_encoder_decoder(self):\n    message = Message(constants.DEBUG, 'Test message')\n    encoder = MessageEncoder()\n    decoder = MessageDecoder()\n\n    encoded_message = encoder.default(message)\n    self.assertIsInstance(encoded_message, list)\n    self.assertEqual(len(encoded_message), 4)\n\n    decoded_message = decoder.process_messages(encoded_message)\n    self.assertIsInstance(decoded_message, Message)\n    self.assertEqual(decoded_message.level, constants.DEBUG)\n    self.assertEqual(decoded_message.message, 'Test message')\n\n    # Test encoding and decoding with extra tags\n    message.extra_tags = 'tag1 tag2'\n    encoded_message = encoder.default(message)\n    self.assertIsInstance(encoded_message, list)\n    self.assertEqual(len(encoded_message), 5)\n\n    decoded_message = decoder.process_messages(encoded_message)\n    self.assertIsInstance(decoded_message, Message)\n    self.assertEqual(decoded_message.level, constants.DEBUG)\n    self.assertEqual(decoded_message.message, 'Test message')\n    self.assertEqual(decoded_message.extra_tags, 'tag1 tag2')\n\n    # Test encoding and decoding with safe data\n    message.message = mark_safe('Safe <b>message</b>')\n    encoded_message = encoder.default(message)\n    self.assertIsInstance(encoded_message, list)\n    self.assertEqual(len(encoded_message), 5)\n\n    decoded_message = decoder.process_messages(encoded_message)\n    self.assertIsInstance(decoded_message, Message)\n    self.assertEqual(decoded_message.level, constants.DEBUG)\n    self.assertEqual(str(decoded_message.message), 'Safe <b>message</b>')\n    self.assertIsInstance(decoded_message.message, SafeData)\n", "def test_message_encoder(self):\n    message = Message(constants.DEBUG, 'Test message', extra_tags='tag1,tag2')\n    encoder = MessageEncoder()\n    encoded_message = encoder.default(message)\n    self.assertEqual(len(encoded_message), 5)\n    self.assertEqual(encoded_message[0], MessageEncoder.message_key)\n    self.assertEqual(encoded_message[1], 0)  # Not SafeData\n    self.assertEqual(encoded_message[2], constants.DEBUG)\n    self.assertEqual(encoded_message[3], 'Test message')\n    self.assertEqual(encoded_message[4], 'tag1,tag2')\n\n    safe_message = Message(constants.DEBUG, mark_safe('Test message'), extra_tags='tag1,tag2')\n    encoded_safe_message = encoder.default(safe_message)\n    self.assertEqual(len(encoded_safe_message), 5)\n    self.assertEqual(encoded_safe_message[0], MessageEncoder.message_key)\n    self.assertEqual(encoded_safe_message[1], 1)  # Is SafeData\n    self.assertEqual(encoded_safe_message[2], constants.DEBUG)\n    self.assertEqual(encoded_safe_message[3], 'Test message')\n    self.assertEqual(encoded_safe_message[4], 'tag1,tag2')\n", "def test_process_messages(self):\n    decoder = MessageDecoder()\n\n    # Test processing of a single message\n    data = [MessageEncoder.message_key, 1, constants.DEBUG, 'Test message']\n    self.assertIsInstance(decoder.process_messages(data), Message)\n\n    # Test processing of a list of messages\n    data = [\n        [MessageEncoder.message_key, 1, constants.DEBUG, 'Test message 1'],\n        [MessageEncoder.message_key, 0, constants.INFO, 'Test message 2'],\n    ]\n    processed_data = decoder.process_messages(data)\n    self.assertEqual(len(processed_data), 2)\n    self.assertIsInstance(processed_data[0], Message)\n    self.assertIsInstance(processed_data[1], Message)\n\n    # Test processing of a dictionary containing messages\n    data = {\n        'messages': [\n            [MessageEncoder.message_key, 1, constants.DEBUG, 'Test message 1'],\n            [MessageEncoder.message_key, 0, constants.INFO, 'Test message 2'],\n        ],\n    }\n    processed_data = decoder.process_messages(data)\n    self.assertIn('messages', processed_data)\n    self.assertEqual(len(processed_data['messages']), 2)\n    self.assertIsInstance(processed_data['messages'][0], Message)\n    self.assertIsInstance(processed_data['messages'][1], Message)\n\n    # Test processing of a SafeData instance\n    safe_message = mark_safe('Test <b>safe</b> message')\n    data = [MessageEncoder.message_key, 1, constants.DEBUG, safe_message]\n    processed_message = decoder.process_messages(data)\n    self.assertIsInstance(processed_message.message, SafeData)\n", "def test_process_messages_with_non_message_list(self):\n    decoder = MessageDecoder()\n    data = [1, 2, 3]\n    self.assertEqual(decoder.process_messages(data), data)\n", "def test_message_decoder_process_messages(self):\n    decoder = MessageDecoder()\n    messages = [\n        [MessageEncoder.message_key, 1, constants.DEBUG, 'Test message'],\n        [MessageEncoder.message_key, 0, constants.INFO, 'Another message'],\n    ]\n    processed_messages = decoder.process_messages(messages)\n    self.assertIsInstance(processed_messages[0], Message)\n    self.assertIsInstance(processed_messages[1], Message)\n    self.assertEqual(processed_messages[0].message, mark_safe('Test message'))\n    self.assertEqual(processed_messages[1].message, 'Another message')\n\n    # Test that non-message objects are left unchanged\n    mixed_list = ['hello', [MessageEncoder.message_key, 1, constants.DEBUG, 'Test message']]\n    processed_list = decoder.process_messages(mixed_list)\n    self.assertEqual(processed_list[0], 'hello')\n    self.assertIsInstance(processed_list[1], Message)\n\n    # Test that dictionaries are processed recursively\n    nested_dict = {'key': [MessageEncoder.message_key, 1, constants.DEBUG, 'Test message']}\n    processed_dict = decoder.process_messages(nested_dict)\n    self.assertIsInstance(processed_dict['key'], Message)\n"], "sample_377": ["    def test_safe_exception_reporter_filter_get_safe_settings(self):\n        filter = SafeExceptionReporterFilter()\n        settings = filter.get_safe_settings()\n        self.assertIn(\"DATABASES\", settings)\n        self.assertNotIn(\"SECRET_KEY\", settings)\n", "    def test_get_traceback_data(self):\n        request = RequestFactory().get('/test/')\n        reporter = ExceptionReporter(request, *sys.exc_info())\n        data = reporter.get_traceback_data()\n        self.assertIn('request', data)\n        self.assertIn('request_meta', data)\n        self.assertIn('request_COOKIES_items', data)\n        self.assertIn('user_str', data)\n        self.assertIn('filtered_POST_items', data)\n        self.assertIn('settings', data)\n        self.assertIn('sys_executable', data)\n        self.assertIn('sys_version_info', data)\n        self.assertIn('server_time', data)\n        self.assertIn('django_version_info', data)\n        self.assertIn('sys_path', data)\n", "    def test_cleanse_setting(self):\n        filter = SafeExceptionReporterFilter()\n        self.assertEqual(filter.cleanse_setting(\"ALLOWED_HOSTS\", [\"example.com\"]), [\"example.com\"])\n        self.assertEqual(filter.cleanse_setting(\"DATABASES\", {\"default\": {\"PASSWORD\": \"secret\"}}), {\"default\": {\"PASSWORD\": \"********************\"}})\n        self.assertEqual(filter.cleanse_setting(\"SECRET_KEY\", \"secret\"), \"********************\")\n", "    def test_cleanse_setting(self):\n        filter = SafeExceptionReporterFilter()\n        self.assertEqual(filter.cleanse_setting(\"ALLOWED_HOSTS\", [\"example.com\"]), [\"example.com\"])\n        self.assertEqual(filter.cleanse_setting(\"SECRET_KEY\", \"my_secret_key\"), \"********************\")\n        self.assertEqual(filter.cleanse_setting(\"DATABASES\", {\"default\": {\"ENGINE\": \"django.db.backends.sqlite3\"}}), {\"default\": {\"ENGINE\": \"django.db.backends.sqlite3\"}})\n", "    def test_get_safe_settings(self):\n        filter = SafeExceptionReporterFilter()\n        settings_dict = filter.get_safe_settings()\n        self.assertIn(\"DATABASES\", settings_dict)\n        self.assertNotIn(\"DATABASES\", filter.hidden_settings)\n\n        # Check that sensitive settings are hidden\n        sensitive_settings = [\"SECRET_KEY\", \"API_KEY\"]\n        for setting in sensitive_settings:\n            if hasattr(settings, setting):\n                self.assertEqual(settings_dict[setting], \"********************\")\n"], "sample_378": ["def test_bulk_update(self):\n    Note.objects.bulk_update(self.notes, ['note'])\n    notes = Note.objects.all()\n    for i, note in enumerate(notes):\n        self.assertEqual(note.note, str(i))\n", "def test_bulk_update(self):\n    notes = Note.objects.all()\n    self.assertEqual(notes.count(), 10)\n    Note.objects.bulk_update(notes, ['note'])\n    self.assertEqual(Note.objects.count(), 10)\n    Note.objects.bulk_update(notes, ['misc'], batch_size=5)\n    self.assertEqual(Note.objects.count(), 10)\n", "def test_bulk_update(self):\n    Note.objects.bulk_update(self.notes, ['note'])\n    for i, note in enumerate(self.notes):\n        self.assertEqual(Note.objects.get(pk=note.pk).note, str(i))\n        self.assertEqual(Note.objects.get(pk=note.pk).misc, str(i))\n\n    # Test update of one field on instances with different pk values.\n    notes = list(Note.objects.all())\n    for note in notes:\n        note.note = 'x'\n    Note.objects.bulk_update(notes, ['note'])\n    self.assertEqual(Note.objects.filter(note='x').count(), 10)\n\n    # Test update of one field on instances with the same pk value.\n    notes = list(Note.objects.all())\n    for note in notes:\n        note.note = 'same'\n        note.pk = notes[0].pk\n    with self.assertRaises(ValueError):\n        Note.objects.bulk_update(notes, ['note'])\n", "def test_bulk_update(self):\n    notes = Note.objects.all()\n    notes.bulk_update([{'id': note.id, 'note': 'updated'} for note in notes])\n    self.assertEqual(Note.objects.filter(note='updated').count(), 10)\n", "def test_bulk_update(self):\n    Note.objects.bulk_update(self.notes, ['note'])\n    self.assertEqual(Note.objects.get(pk=self.notes[0].pk).note, '0')\n    self.assertEqual(Note.objects.get(pk=self.notes[9].pk).note, '9')\n\n    # update with F expression\n    Note.objects.bulk_update([Note(note=F('note') + ' updated') for note in self.notes], ['note'])\n    self.assertEqual(Note.objects.get(pk=self.notes[0].pk).note, '0 updated')\n    self.assertEqual(Note.objects.get(pk=self.notes[9].pk).note, '9 updated')\n"], "sample_379": ["def test_mark_safe_decorator(self):\n    @mark_safe\n        return '<p>Hello, World!</p>'\n\n    self.assertIsInstance(my_safe_string(), SafeString)\n    self.assertEqual(my_safe_string(), '<p>Hello, World!</p>')\n\n    # Test that mark_safe decorator doesn't escape already escaped strings\n    @mark_safe\n        return '&lt;p&gt;Hello, World!&lt;/p&gt;'\n\n    self.assertIsInstance(my_escaped_string(), SafeString)\n    self.assertEqual(my_escaped_string(), '&lt;p&gt;Hello, World!&lt;/p&gt;')\n", "def test_safe_string_addition(self):\n    safe_str = SafeString('<p>Hello</p>')\n    regular_str = '<span>World!</span>'\n    other_safe_str = SafeString(regular_str)\n\n    # Test adding a safe string and a regular string\n    result = safe_str + regular_str\n    self.assertIsInstance(result, str)\n    self.assertNotIsInstance(result, SafeString)\n\n    # Test adding two safe strings\n    result = safe_str + other_safe_str\n    self.assertIsInstance(result, SafeString)\n    self.assertEqual(result, '<p>Hello</p><span>World!</span>')\n", "def test_safe_string_addition(self):\n    safe_str = SafeString('<p>Hello</p>')\n    other_safe_str = SafeString('<span>World</span>')\n    unsafe_str = '<script>alert(\"XSS\")</script>'\n\n    self.assertIsInstance(safe_str + other_safe_str, SafeString)\n    self.assertNotIsInstance(safe_str + unsafe_str, SafeString)\n\n    result = safe_str + other_safe_str\n    self.assertEqual(result, '<p>Hello</p><span>World</span>')\n\n    result = safe_str + unsafe_str\n    self.assertEqual(result, '<p>Hello</p><script>alert(\"XSS\")</script>')\n", "def test_mark_safe_decorator(self):\n    @mark_safe\n        return '<p>Hello, World!</p>'\n\n    self.assertIsInstance(my_safe_string(), SafeString)\n    self.assertEqual(my_safe_string().__html__(), '<p>Hello, World!</p>')\n", "def test_safe_string_addition(self):\n    safe_str = SafeString('<p>Hello</p>')\n    other_safe_str = SafeString('<p>World</p>')\n    unsafe_str = '<script>alert(\"XSS\")</script>'\n\n    self.assertIsInstance(safe_str + other_safe_str, SafeString)\n    self.assertNotIsInstance(safe_str + unsafe_str, SafeString)\n\n    self.assertEqual((safe_str + other_safe_str).__html__(), '<p>Hello</p><p>World</p>')\n    self.assertEqual((safe_str + unsafe_str).__html__(), None)\n"], "sample_380": ["def test_aggregate_with_filter(self):\n    # Test that an aggregate with a filter clause is applied correctly.\n    count = Book.objects.aggregate(c=Count('id', filter=Q(pages__gt=400)))['c']\n    self.assertEqual(count, 3)\n", "def test_aggregate_filter(self):\n    # Testing filter on aggregate expression.\n    authors = Author.objects.annotate(\n        age_max=Max('age', filter=Q(name__startswith='A')),\n        age_avg=Avg('age', filter=Q(name__startswith='J')),\n    )\n    self.assertEqual(authors.get(name='Adrian Holovaty').age_max, 34)\n    self.assertIsNone(authors.get(name='Adrian Holovaty').age_avg)\n\n    authors = Author.objects.annotate(\n        age_avg=Avg('age', filter=Q(name__startswith='J')),\n        age_sum=Sum('age', filter=Q(name__startswith='A')),\n    ).filter(age_sum__isnull=False)\n    self.assertEqual(authors.count(), 1)\n    self.assertEqual(authors[0].name, 'Adrian Holovaty')\n", "def test_aggregate_with_filter(self):\n    # Test aggregate with filter argument\n    agg = Count('id', filter=Q(age__gt=30))\n    result = Author.objects.aggregate(count=agg)['count']\n    self.assertEqual(result, 6)\n\n    # Test aggregate with filter argument and distinct=True\n    agg = Count('id', filter=Q(age__gt=30), distinct=True)\n    result = Author.objects.aggregate(count=agg)['count']\n    self.assertEqual(result, 6)\n\n    # Test aggregate with filter argument on a related field\n    agg = Count('friends__id', filter=Q(friends__age__gt=30))\n    result = Author.objects.aggregate(count=agg)['count']\n    self.assertEqual(result, 4)\n", "def test_aggregate_resolve_expression_with_filter(self):\n    # Test that the filter expression is resolved correctly\n    publisher = Publisher.objects.annotate(\n        num_books=Count('book', filter=Q(book__rating__gt=3))\n    ).first()\n    query = publisher._meta.model.objects.filter(pk=publisher.pk)\n    resolved_expression = Count('book', filter=Q(book__rating__gt=3)).resolve_expression(query)\n    self.assertIsInstance(resolved_expression, Coalesce)\n    self.assertEqual(resolved_expression.output_field, IntegerField())\n", "def test_aggregate_with_filter_clause(self):\n    # Test that aggregates with filter clauses are properly generated.\n    agg = Count('id', filter=Q(rating__gt=3.0))\n    self.assertEqual(agg.as_sql(connection), ('COUNT(CASE WHEN rating > 3.0 THEN id ELSE NULL END)', []))\n\n    # Test that filter clause is properly resolved.\n    self.assertSequenceEqual(\n        list(Book.objects.annotate(count=Count('id', filter=Q(rating__gt=3.0))).values_list('count')),\n        [(4,), (0,), (1,), (1,), (1,), (1,)]\n    )\n\n    # Test that the filter clause is applied correctly.\n    self.assertEqual(\n        Book.objects.aggregate(count=Count('id', filter=Q(rating__gt=3.0))),\n        {'count': 6}\n    )\n"], "sample_381": ["def test_generate_altered_foo_together(self):\n    \"\"\"Tests the _generate_altered_foo_together method.\"\"\"\n    # Make a model state\n    before = self.make_project_state([self.book_foo_together])\n    after = self.make_project_state([self.book_foo_together_2])\n\n    # We don't need to test the entire migration, just the relevant operations\n    autodetector = MigrationAutodetector(before, after)\n    changes = autodetector._detect_changes()\n    alters = [op for op in changes['otherapp'][0].operations if isinstance(op, migrations.AlterUniqueTogether) or isinstance(op, migrations.AlterIndexTogether)]\n    self.assertEqual(len(alters), 2)\n\n    self.assertOperationAttributes(changes, \"otherapp\", 0, 0, name='book', unique_together={('title', 'author')})\n    self.assertOperationAttributes(changes, \"otherapp\", 0, 1, name='book', index_together={('title', 'author')})\n", "def test_generate_added_constraints(self):\n    \"\"\"\n    Added constraints should be correctly added in a single operation.\n    \"\"\"\n    before = [\n        self.author_empty,\n    ]\n    after = [\n        ModelState(\"testapp\", \"Author\", [(\"id\", models.AutoField(primary_key=True))], {\n            \"constraints\": [models.CheckConstraint(check=models.Q(id__gt=0), name='author_id_gt_0')],\n        }),\n    ]\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['CreateModel'])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, options={'constraints': [\n        models.CheckConstraint(check=models.Q(id__gt=0), name='author_id_gt_0'),\n    ]})\n", "def test_rename_model_with_m2m(self):\n    \"\"\"Test rename model with m2m field.\"\"\"\n    before = self.make_project_state([self.author_with_m2m, self.publisher])\n    after = self.make_project_state([self.author_renamed_with_book, self.publisher])\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['RenameModel'])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, old_name='Author', new_name='Writer')\n", "def test_generate_added_indexes(self):\n    # Create model state and project state for testing\n    before_state = self.make_project_state([self.book])\n    after_state = self.make_project_state([self.book_indexes])\n\n    # Get changes from autodetector\n    changes = self.get_changes(before_state, after_state)\n\n    # Assert correct number of migrations and operations\n    self.assertNumberMigrations(changes, \"otherapp\", 1)\n    self.assertOperationTypes(changes, \"otherapp\", 0, [\"AddIndex\"])\n\n    # Assert index is created with correct fields\n    self.assertOperationAttributes(\n        changes,\n        \"otherapp\",\n        0,\n        0,\n        name=\"book_title_author_idx\",\n        fields=[\"author\", \"title\"],\n    )\n", "def test_alter_constraints(self):\n    \"\"\"Tests that altering constraints works.\"\"\"\n    changes = self.get_changes(\n        [self.author_name],\n        [self.author_name_check_constraint],\n    )\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['AddConstraint'])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name='name_contains_bob')\n"], "sample_382": ["    def test_get_template_directories(self):\n        dirs = autoreload.get_template_directories()\n        self.assertIn(EXTRA_TEMPLATES_DIR, dirs)\n        for dir in dirs:\n            self.assertIsInstance(dir, Path)\n", "    def test_watch_for_template_changes(self, mock_get_template_directories):\n        mock_get_template_directories.return_value = [EXTRA_TEMPLATES_DIR]\n        sender = mock.Mock()\n        watch_for_template_changes(sender)\n        sender.watch_dir.assert_called_once_with(EXTRA_TEMPLATES_DIR, '**/*')\n", "def test_reset_loaders_called_on_file_change(self, mock_get_template_directories):\n    mock_get_template_directories.return_value = [EXTRA_TEMPLATES_DIR]\n    file_path = EXTRA_TEMPLATES_DIR / 'test.txt'\n    with mock.patch('django.template.autoreload.reset_loaders') as mock_reset_loaders:\n        autoreload.template_changed(None, file_path)\n        mock_reset_loaders.assert_called_once()\n", "    def test_template_changed_calls_reset_loaders(self, mock_reset_loaders):\n        file_path = EXTRA_TEMPLATES_DIR / 'test_template.html'\n        with self.settings(TEMPLATES=[{\n            'BACKEND': 'django.template.backends.django.DjangoTemplates',\n            'DIRS': [EXTRA_TEMPLATES_DIR],\n        }]):\n            autoreload.template_changed(None, file_path)\n            mock_reset_loaders.assert_called_once()\n", "    def test_template_changed_with_py_file(self, mock_reset_loaders):\n        file_path = Path('path/to/template.py')\n        result = autoreload.template_changed(None, file_path)\n        self.assertIsNone(result)\n        mock_reset_loaders.assert_not_called()\n"], "sample_383": ["    def test_related_exact_lookup(self):\n        qs = Item.objects.filter(creator__extra=self.a1.extra)\n        self.assertEqual(qs.count(), 1)\n", "    def setUpTestData(cls):\n        cls.t1 = Tag.objects.create(name=\"t1\")\n        cls.t2 = Tag.objects.create(name=\"t2\", parent=cls.t1)\n        cls.n1 = Note.objects.create(note=\"n1\", misc=\"foo\")\n        cls.ann1 = Annotation.objects.create(name=\"a1\", tag=cls.t1)\n        cls.ann1.notes.add(cls.n1)\n", "    def setUpTestData(cls):\n        cls.a1 = Author.objects.create(name=\"a1\", num=1001)\n        cls.e1 = ExtraInfo.objects.create(info=\"e1\", note=None, value=42)\n        cls.a2 = Author.objects.create(name=\"a2\", num=2002, extra=cls.e1)\n", "    def test_related_exact_lookup(self):\n        qs = Item.objects.filter(tags__exact=self.t1)\n        self.assertEqual(qs.count(), 2)\n", "    def setUpTestData(cls):\n        cls.n1 = Note.objects.create(note=\"n1\", misc=\"foo\", id=1)\n        cls.n2 = Note.objects.create(note=\"n2\", misc=\"bar\", id=2)\n        cls.e1 = ExtraInfo.objects.create(info=\"e1\", note=cls.n1, value=42)\n        cls.e2 = ExtraInfo.objects.create(\n            info=\"e2\", note=cls.n2, value=41, filterable=False\n        )\n"], "sample_384": ["def test_bulk_update(self):\n    notes = Note.objects.all()\n    Note.objects.bulk_update(notes, ['note'])\n    self.assertEqual(Note.objects.filter(note=F('misc')).count(), 10)\n\n    with self.assertRaises(ValueError):\n        Note.objects.bulk_update(notes, ['nonexistent_field'])\n\n    with self.assertRaises(ValueError):\n        Note.objects.bulk_update(notes, ['id'])  # primary key\n\n    with self.assertRaises(ValueError):\n        Note.objects.bulk_update([Note()], ['note'])  # non-persisted object\n\n    with self.settings(DATABASE_ROUTERS=[WriteToOtherRouter()]):\n        with self.assertRaises(ValueError):\n            Note.objects.bulk_update(notes, ['note'])\n", "def test_bulk_update(self):\n    Note.objects.bulk_update(self.notes, ['note', 'misc'])\n    for note in self.notes:\n        with self.subTest(note=note):\n            refreshed_note = Note.objects.get(pk=note.pk)\n            self.assertEqual(refreshed_note.note, note.note)\n            self.assertEqual(refreshed_note.misc, note.misc)\n", "def test_bulk_update_fields(self):\n    with self.assertNumQueries(1):\n        Note.objects.bulk_update(self.notes, ['note'])\n    for note in self.notes:\n        note.refresh_from_db()\n        self.assertEqual(note.note, str(note.pk))\n    # Update again to ensure that save() after bulk update doesn't cause errors.\n    with self.assertNumQueries(1):\n        Note.objects.bulk_update(self.notes, ['note'])\n", "def test_bulk_update(self):\n    Note.objects.bulk_update(self.notes, ['note', 'misc'])\n    for i, note in enumerate(Note.objects.all()):\n        self.assertEqual(note.note, str(i))\n        self.assertEqual(note.misc, str(i))\n", "def test_bulk_update(self):\n    notes = Note.objects.all()\n    Note.objects.bulk_update(notes, fields=[\"note\"])\n    self.assertEqual(Note.objects.filter(note=F(\"misc\")).count(), 10)\n"], "sample_385": ["def test_autocomplete_select_with_to_field_name(self):\n    class BandForm(forms.ModelForm):\n        class Meta:\n            model = Album\n            fields = [\"band\"]\n            widgets = {\n                \"band\": AutocompleteSelect(\n                    Album._meta.get_field(\"band\"),\n                    admin.site,\n                ),\n            }\n\n    form = BandForm()\n    self.assertContains(form.as_p(), 'data-field-name=\"band\"')\n", "def test_autocomplete_select_rendering(self):\n    band = Band.objects.create(name=\"Test Band\")\n    album = Album.objects.create(band=band, name=\"Test Album\")\n    form = AlbumForm(instance=album)\n    output = form.as_p()\n    self.assertHTMLEqual(\n        output,\n        \"\"\"\n        <p>\n            <label for=\"id_band\">Band:</label>\n            <select data-ajax--cache=\"true\" data-ajax--delay=\"250\" data-ajax--type=\"GET\"\n                    data-ajax--url=\"/autocomplete/\" data-app-label=\"admin_widgets\"\n                    data-model-name=\"band\" data-field-name=\"band\"\n                    data-theme=\"admin-autocomplete\" data-allow-clear=\"false\"\n                    data-placeholder=\"\" lang=\"en\" class=\"my-class admin-autocomplete\">\n                {}\n                <option value=\"{}\" selected>Test Band</option>\n            </select>\n        </p>\n        <p>\n            <label for=\"id_featuring\">Featuring:</label>\n            <select data-ajax--cache=\"true\" data-ajax--delay=\"250\" data-ajax--type=\"GET\"\n                    data-ajax--url=\"/autocomplete/\" data-app-label=\"admin_widgets\"\n                    data-model-name=\"band\" data-field-name=\"featuring\"\n                    data-theme=\"admin-autocomplete\" data-allow-clear=\"false\"\n                    data-placeholder=\"\" lang=\"en\" class=\"admin-autocomplete\">\n                {}\n            </select>\n        </p>\n        \"\"\".format(\n            self.empty_option, band.pk, self.empty_option\n        ),\n    )\n", "def test_autocomplete_select_required_field(self):\n    form = RequiredBandForm()\n    widget = form.fields[\"band\"].widget\n    html = widget.render(\"band\", \"\")\n    self.assertNotIn(self.empty_option, html)\n    self.assertIn('data-allow-clear=\"false\"', html)\n\n    form = NotRequiredBandForm()\n    widget = form.fields[\"band\"].widget\n    html = widget.render(\"band\", \"\")\n    self.assertIn(self.empty_option, html)\n    self.assertIn('data-allow-clear=\"true\"', html)\n", "def test_autocomplete_select_render(self):\n    album = Album.objects.create(band=\"The Doors\", name=\"L.A. Woman\")\n    band = Band.objects.create(name=\"The Doors\")\n    form = AlbumForm(instance=album)\n    output = form.as_p()\n    self.assertHTMLEqual(\n        output,\n        \"\"\"\n        <p>\n            <label for=\"id_band\">Band:</label>\n            <select data-ajax--cache=\"true\" data-ajax--delay=\"250\" data-ajax--type=\"GET\"\n                    data-ajax--url=\"/autocomplete/\" data-app-label=\"admin_widgets\" data-field-name=\"band\"\n                    data-model-name=\"album\" data-theme=\"admin-autocomplete\" data-allow-clear=\"false\"\n                    data-placeholder=\"\" id=\"id_band\" name=\"band\" class=\"my-class admin-autocomplete\">\n                {}\n                <option value=\"{}\" selected>The Doors</option>\n            </select>\n        </p>\n        <p>\n            <label for=\"id_featuring\">Featuring:</label>\n            <select data-ajax--cache=\"true\" data-ajax--delay=\"250\" data-ajax--type=\"GET\"\n                    data-ajax--url=\"/autocomplete/\" data-app-label=\"admin_widgets\" data-field-name=\"featuring\"\n                    data-model-name=\"album\" data-theme=\"admin-autocomplete\" data-allow-clear=\"false\"\n                    data-placeholder=\"\" id=\"id_featuring\" name=\"featuring\" class=\"admin-autocomplete\">\n                {}\n            </select>\n        </p>\n        \"\"\".format(\n            self.empty_option, band.pk, self.empty_option\n        ),\n    )\n", "def test_autocomplete_select_with_to_field(self):\n    class ToFieldBandForm(forms.Form):\n        band = ModelChoiceField(\n            queryset=Band.objects.all(),\n            widget=AutocompleteSelect(\n                Album._meta.get_field(\"band\").remote_field, admin.site\n            ),\n        )\n\n    form = ToFieldBandForm()\n    self.assertContains(form.as_p(), 'data-field-name=\"band\"')\n    self.assertContains(form.as_p(), 'data-app-label=\"admin_widgets\"')\n    self.assertContains(form.as_p(), 'data-model-name=\"band\"')\n    self.assertContains(form.as_p(), 'data-ajax--url=\"/autocomplete/\"')\n"], "sample_386": ["def test_safe_string_addition(self):\n    safe_str = SafeString(\"<p>Hello</p>\")\n    self.assertIsInstance(safe_str + SafeString(\" World\"), SafeString)\n    self.assertNotIsInstance(safe_str + \" World\", SafeString)\n\n    # Test adding SafeString with custom class implementing __html__\n    custom_str = customescape(\"<script>\")\n    self.assertIsInstance(safe_str + custom_str, str)\n    self.assertEqual((safe_str + custom_str).__html__(), \"<p>Hello</p><<script>>\")\n", "def test_mark_safe_decorator(self):\n    @mark_safe\n        return \"<p>Hello, World!</p>\"\n\n    self.assertIsInstance(decorated_function(), SafeString)\n    self.assertEqual(str(decorated_function()), \"<p>Hello, World!</p>\")\n", "def test_mark_safe_decorator(self):\n    @mark_safe\n        return \"<p>Hello, World!</p>\"\n\n    self.assertIsInstance(decorated_function(), SafeString)\n    self.assertEqual(str(decorated_function()), \"<p>Hello, World!</p>\")\n", "def test_safe_string_addition(self):\n    safe_str = SafeString(\"<p>Hello</p>\")\n    regular_str = \"<p>World</p>\"\n\n    # Adding two SafeStrings should result in another SafeString\n    self.assertIsInstance(safe_str + safe_str, SafeString)\n\n    # Adding a SafeString and a regular string should result in a regular string\n    self.assertNotIsInstance(safe_str + regular_str, SafeString)\n\n    # Test the actual output of the addition\n    self.assertEqual(str(safe_str + safe_str), \"<p>Hello</p><p>Hello</p>\")\n    self.assertEqual(str(safe_str + regular_str), \"<p>Hello</p><p>World</p>\")\n", "def test_safestring_addition(self):\n    s1 = SafeString(\"<p>Hello</p>\")\n    s2 = SafeString(\"<p>World</p>\")\n    result = s1 + s2\n    self.assertIsInstance(result, SafeString)\n    self.assertEqual(str(result), \"<p>Hello</p><p>World</p>\")\n\n    s3 = \" unsafe string \"\n    result = s1 + s3\n    self.assertNotIsInstance(result, SafeString)\n    self.assertEqual(str(result), \"<p>Hello</p> unsafe string \")\n"], "sample_388": ["def test_unknown_user_created(self):\n    \"\"\"\n    An unknown user is created when the REMOTE_USER is not found in the database.\n    \"\"\"\n    with self.settings(AUTHENTICATION_BACKENDS=[self.backend]):\n        username = \"newuser\"\n        request = self.client.request()\n        request.META[self.header] = username\n\n        # Simulate middleware\n        RemoteUserMiddleware().process_request(request)\n\n        # Check that a new user was created\n        self.assertEqual(User.objects.filter(username=username).count(), 1)\n\n        # Check that the user can authenticate\n        user = authenticate(remote_user=username)\n        self.assertIsNotNone(user)\n        self.assertEqual(user.username, username)\n", "def test_unknown_user(self):\n    unknown_user = \"newuser\"\n    user_count = User.objects.count()\n\n    # Create a new user by authenticating with a new username\n    response = self.client.get(\n        \"/remote_user/\", HTTP_REMOTE_USER=unknown_user, follow=True\n    )\n    self.assertEqual(response.status_code, 200)\n\n    # Check that a new user was created\n    self.assertEqual(User.objects.count(), user_count + 1)\n\n    # Check that the new user has a valid username and no email or password\n    new_user = User.objects.get(username=unknown_user)\n    self.assertEqual(new_user.username, unknown_user)\n    self.assertEqual(new_user.email, \"\")\n    self.assertFalse(new_user.has_usable_password())\n", "def test_remote_user_backend_create_unknown_user(self):\n    backend = RemoteUserBackend()\n    user = backend.authenticate(request=None, remote_user=\"newuser\")\n    self.assertIsNotNone(user)\n    self.assertEqual(user.username, \"newuser\")\n\n    # Test that create_unknown_user=False doesn't create a new user\n    backend.create_unknown_user = False\n    user = backend.authenticate(request=None, remote_user=\"newuser2\")\n    self.assertIsNone(user)\n", "def test_remote_user_middleware_create_unknown_user(self):\n    # Test that the RemoteUserMiddleware creates a user when\n    # create_unknown_user is True.\n\n    class CustomRemoteUserBackend(RemoteUserBackend):\n        create_unknown_user = True\n\n    with modify_settings(\n        AUTHENTICATION_BACKENDS={\"append\": \"auth_tests.test_backends.CustomRemoteUserBackend\"}\n    ):\n        client = Client(**{self.header: \"newuser\"})\n        response = client.get(\"/remote_user/\")\n        self.assertEqual(response.status_code, 200)\n        user = User.objects.get(username=\"newuser\")\n        self.assertIsNotNone(user)\n\n    # Now test with create_unknown_user set to False.\n\n    class CustomRemoteUserBackend(RemoteUserBackend):\n        create_unknown_user = False\n\n    with modify_settings(\n        AUTHENTICATION_BACKENDS={\"append\": \"auth_tests.test_backends.CustomRemoteUserBackend\"}\n    ):\n        client = Client(**{self.header: \"newuser2\"})\n        response = client.get(\"/remote_user/\")\n        self.assertEqual(response.status_code, 200)\n        with self.assertRaises(User.DoesNotExist):\n            User.objects.get(username=\"newuser2\")\n", "def test_remote_user_created(self):\n    # Create a new user using the RemoteUserBackend\n    remote_user = \"newuser\"\n    request = self.client.request()\n    request.META[self.header] = remote_user\n    user = authenticate(request, remote_user=remote_user)\n\n    # Check that the user was created with the correct username\n    self.assertEqual(user.username, remote_user)\n\n    # Check that the user has no permissions\n    self.assertFalse(user.has_perm(\"auth.test_permission\"))\n\n    # Check that the user is active\n    self.assertTrue(user.is_active)\n"], "sample_390": ["def test_serve(self):\n    \"\"\"Test serving a static file\"\"\"\n    response = self.client.get(f\"/{self.prefix}/testfile.txt\")\n    self.assertEqual(response.status_code, 200)\n    self.assertIsInstance(response, FileResponse)\n    self.assertEqual(response.headers[\"Content-Type\"], \"text/plain\")\n    with open(path.join(media_dir, \"testfile.txt\"), \"rb\") as f:\n        self.assertEqual(response.content, f.read())\n", "def test_serve_directory_index(self):\n    \"\"\"\n    Test serving a directory index.\n    \"\"\"\n    response = self.client.get('/%s/directory/' % self.prefix)\n    self.assertEqual(response.status_code, 200)\n    self.assertTemplateUsed(response, 'static/directory_index.html')\n\n    # Check for invalid paths\n    response = self.client.get('/%s/./' % self.prefix)\n    self.assertEqual(response.status_code, 404)\n\n    response = self.client.get('/%s/../' % self.prefix)\n    self.assertEqual(response.status_code, 404)\n", "def test_serve_directory_index(self):\n    \"\"\"\n    Test serving a directory index works and uses the correct template.\n    \"\"\"\n    response = self.client.get(\"/\" + self.prefix + \"/\")\n    self.assertEqual(response.status_code, 200)\n    self.assertTemplateUsed(response, \"static/directory_index.html\")\n\n    # Check that a custom template can be used.\n    with override_settings(TEMPLATES=[{\"BACKEND\": \"django.template.backends.django.DjangoTemplates\", \"DIRS\": [path.join(path.dirname(__file__), \"templates\")]}]):\n        response = self.client.get(\"/\" + self.prefix + \"/\")\n        self.assertEqual(response.status_code, 200)\n        self.assertTemplateUsed(response, \"static/directory_index.html\")\n\n    # Check that the default template is used when no custom template is available.\n    with override_settings(TEMPLATES=[{\"BACKEND\": \"django.template.backends.django.DjangoTemplates\", \"DIRS\": []}]):\n        response = self.client.get(\"/\" + self.prefix + \"/\")\n        self.assertEqual(response.status_code, 200)\n        self.assertContains(response, \"<h1>Index of /</h1>\")\n", "def test_serve(self):\n    \"\"\"Test the serve view\"\"\"\n    response = self.client.get(f\"/{self.prefix}/test.txt\")\n    self.assertEqual(response.status_code, 200)\n    self.assertIsInstance(response, FileResponse)\n    self.assertEqual(\n        response.headers[\"Content-Type\"], mimetypes.types_map[\".txt\"]\n    )\n    self.assertEqual(response.headers[\"Content-Disposition\"], 'inline; filename=\"test.txt\"')\n\n    # Test If-Modified-Since header\n    mtime = path.getmtime(media_dir / \"test.txt\")\n    response = self.client.get(\n        f\"/{self.prefix}/test.txt\", HTTP_IF_MODIFIED_SINCE=http_date(mtime)\n    )\n    self.assertEqual(response.status_code, 304)\n    self.assertIsInstance(response, HttpResponseNotModified)\n", "def test_serve_directory_index(self):\n    \"\"\"Test serving a directory index\"\"\"\n    with override_settings(ALLOWED_HOSTS=[\"testserver\"]):\n        response = self.client.get(\"/site_media/\")\n        self.assertEqual(response.status_code, 200)\n        self.assertTemplateUsed(response, \"static/directory_index.html\")\n        self.assertContains(response, \"<h1>Index of /</h1>\")\n        self.assertContains(response, \"<li><a href='test.txt'>test.txt</a></li>\")\n"], "sample_391": ["def test_optimizer_remove_index(self):\n    operations = [\n        operations.AddIndex(\n            model_name=\"testmodel\",\n            index=models.Index(fields=[\"field1\"], name=\"my_index\"),\n        ),\n        operations.RemoveIndex(model_name=\"testmodel\", name=\"my_index\"),\n    ]\n    self.assertOptimizesTo(operations, [])\n", "def test_create_model_with_options(self):\n    operations = [\n        migrations.CreateModel(\n            \"MyModel\",\n            fields=[(\"id\", models.AutoField(primary_key=True))],\n            options={\"verbose_name\": \"My Model\"},\n        ),\n        migrations.AlterModelOptions(\"MyModel\", {\"verbose_name\": \"My Model\"}),\n    ]\n    self.assertOptimizesTo(operations, [operations[0]])\n", "def test_alter_model_options(self):\n    operations = [\n        migrations.AlterModelOptions(\n            name=\"UnicodeModel\", options={\"verbose_name\": \"New verbose name\"}\n        ),\n        migrations.AlterModelOptions(\n            name=\"UnicodeModel\", options={\"verbose_name_plural\": \"New plural name\"}\n        ),\n    ]\n    expected = [\n        migrations.AlterModelOptions(\n            name=\"UnicodeModel\",\n            options={\"verbose_name\": \"New verbose name\", \"verbose_name_plural\": \"New plural name\"},\n        )\n    ]\n    self.assertOptimizesTo(operations, expected)\n", "def test_optimizer_remove_index(self):\n    operations = [\n        migrations.AddIndex(\n            model_name=\"unicode\",\n            index=models.Index(fields=[\"char_field\"], name=\"my_index\"),\n        ),\n        migrations.RemoveIndex(model_name=\"unicode\", name=\"my_index\"),\n    ]\n    self.assertOptimizesTo(operations, [])\n", "def test_optimizer_create_and_delete_model(self):\n    operations = [\n        migrations.CreateModel(\n            name=\"TestModel\",\n            fields=[\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=255)),\n            ],\n        ),\n        migrations.DeleteModel(name=\"TestModel\"),\n    ]\n    self.assertOptimizesTo(operations, [])\n"], "sample_392": ["def test_key_transform_numeric_lookup(self):\n    obj = JSONModel.objects.create(json={\"key\": 5})\n    # Test that the lookup is performed as expected.\n    self.assertTrue(JSONModel.objects.filter(json__key__gt=4).exists())\n    self.assertFalse(JSONModel.objects.filter(json__key__gt=5).exists())\n    # Test that an error is raised if the value is not a number.\n    with self.assertRaises(ValidationError):\n        JSONModel.objects.get(json__key__gt=\"a\")\n    # Test that the lookup works correctly with decimals.\n    obj.json[\"key\"] = 5.5\n    obj.save()\n    self.assertTrue(JSONModel.objects.filter(json__key__gt=5).exists())\n    self.assertFalse(JSONModel.objects.filter(json__key__gt=6).exists())\n", "def test_key_transform_factory(self):\n    key_name = \"test_key\"\n    factory = KeyTransformFactory(key_name)\n    transform = factory(models.JSONField())\n    self.assertIsInstance(transform, KeyTransform)\n    self.assertEqual(transform.key_name, key_name)\n", "def test_key_transform_text_lookup_mixin(self):\n    mixin = KeyTransformTextLookupMixin()\n    key_transform = KeyTransform(\"test\", \"test\")\n    with self.assertRaises(TypeError):\n        mixin.__init__(object)\n    mixin.__init__(key_transform)\n    self.assertIsInstance(mixin.lhs, KeyTextTransform)\n", "def test_key_transform_text_lookup_mixin(self):\n    mixin = KeyTransformTextLookupMixin()\n    key_transform = KeyTransform(\"test\", \"test\")\n    with self.assertRaises(TypeError):\n        mixin.__init__(key_transform, \"test\", \"test\")\n    key_text_transform = KeyTextTransform(\"test\", \"test\")\n    mixin.__init__(key_text_transform, \"test\", \"test\")\n    self.assertEqual(mixin.lhs, key_text_transform)\n", "def test_key_transform_in_lookup(self):\n    obj = JSONModel.objects.create(json_field={\"a\": 1, \"b\": 2})\n    self.assertQuerysetEqual(\n        JSONModel.objects.filter(json_field__a__in=[1]), [obj.pk], lambda x: x.pk\n    )\n    self.assertQuerysetEqual(\n        JSONModel.objects.filter(json_field__a__in=[2]), [], lambda x: x.pk\n    )\n    self.assertQuerysetEqual(\n        JSONModel.objects.filter(json_field__a__in=[1, 2]), [obj.pk], lambda x: x.pk\n    )\n    self.assertQuerysetEqual(\n        JSONModel.objects.filter(json_field__a__in=[]), [], lambda x: x.pk\n    )\n"], "sample_393": ["def test_build_potfiles(self):\n    with mock.patch.object(MakeMessagesCommand, \"find_files\") as mock_find_files:\n        mock_find_files.return_value = [\n            MakeMessagesCommand.translatable_file_class(\n                \"/path/to/file\", \"example.py\", \"/locale\"\n            )\n        ]\n        with mock.patch.object(MakeMessagesCommand, \"process_files\") as mock_process_files:\n            command = MakeMessagesCommand()\n            command.build_potfiles()\n            self.assertEqual(mock_process_files.call_count, 1)\n            self.assertEqual(len(command.locale_paths), 1)\n", "def test_process_files_with_excluded_paths(self):\n    os.makedirs(\"ignored\")\n    with open(os.path.join(\"ignored\", \"example.html\"), \"w\") as fp:\n        fp.write('<p>Translate this: {% trans \"Hello\" %}</p>')\n    output, po_contents = self._run_makemessages(ignore_patterns=[\"ignored\"])\n    self.assertNotMsgId('Hello', po_contents)\n", "def test_makemessages_with_domain(self):\n    self.make_project()\n    management.call_command(\"makemessages\", locale=[LOCALE], domain=\"django\")\n    self.assertTrue(os.path.exists(self.PO_FILE))\n\n    # Test that the domain is correct in the generated PO file.\n    with open(self.PO_FILE, \"r\") as f:\n        po_contents = f.read()\n    self.assertRegex(po_contents, r\"domain \\\"django\\\"\")\n\n    # Test that the POT file was created.\n    pot_file = os.path.join(self.project_dir, \"locale\", \"django.pot\")\n    self.assertTrue(os.path.exists(pot_file))\n", "def test_copy_plural_forms(self):\n    with tempfile.NamedTemporaryFile(mode=\"w\", encoding=\"utf-8\") as po_file:\n        po_file.write(\n            \"\"\"\\", "def test_makemessages_with_domain(self):\n    with captured_stdout() as stdout, captured_stderr() as stderr:\n        management.call_command(\n            \"makemessages\",\n            locale=[LOCALE],\n            domain=\"djangojs\",\n            verbosity=2,\n        )\n    self.assertEqual(stderr.getvalue(), \"\")\n    self.assertTrue(os.path.exists(\"locale/%s/LC_MESSAGES/djangojs.po\" % LOCALE))\n    output = stdout.getvalue()\n    self.assertIn(\"processing locale %s\" % LOCALE, output)\n    self.assertIn(\"processing file\", output)\n"], "sample_395": ["def test_template_changed_calls_reset_loaders_when_file_path_in_template_dir(\n    self, mock_get_template_directories", "    def test_get_template_directories(self):\n        dirs = autoreload.get_template_directories()\n        self.assertIn(EXTRA_TEMPLATES_DIR, dirs)\n        self.assertNotIn(ROOT, dirs)\n\n        # Test that only DjangoTemplates backends are considered.\n        with override_settings(\n            TEMPLATES=[\n                {\n                    \"BACKEND\": \"django.template.backends.dummy.TemplateStrings\",\n                    \"DIRS\": [ROOT],\n                },\n                {\n                    \"BACKEND\": \"django.template.backends.django.DjangoTemplates\",\n                    \"DIRS\": [EXTRA_TEMPLATES_DIR],\n                },\n            ]\n        ):\n            dirs = autoreload.get_template_directories()\n            self.assertIn(EXTRA_TEMPLATES_DIR, dirs)\n            self.assertNotIn(ROOT, dirs)\n", "def test_reset_loaders_called_on_template_change(self):\n    # Mock the reset_loaders function to check if it's called\n    with mock.patch(\"django.template.autoreload.reset_loaders\") as mock_reset:\n        # Create a file path that appears to be in the template directory\n        file_path = EXTRA_TEMPLATES_DIR / \"test_template.html\"\n        # Call the template_changed function with the mock file path\n        autoreload.template_changed(file_path)\n        # Check if reset_loaders was called\n        self.assertTrue(mock_reset.called)\n", "def test_reset_loaders_called_for_template_change_in_watched_directory(self):\n    template_dir = EXTRA_TEMPLATES_DIR\n    file_path = template_dir / \"test_template.txt\"\n\n    with mock.patch(\"django.template.autoreload.reset_loaders\") as mock_reset_loaders:\n        autoreload.template_changed(file_path)\n        self.assertTrue(mock_reset_loaders.called)\n\n    with mock.patch(\"django.template.autoreload.reset_loaders\") as mock_reset_loaders:\n        autoreload.template_changed(Path(\"/path/to/other/file.txt\"))\n        self.assertFalse(mock_reset_loaders.called)\n", "    def test_reset_loaders_called_when_template_changed(self, mock_reset_loaders):\n        file_path = EXTRA_TEMPLATES_DIR / \"test_template.html\"\n        with mock.patch(\"django.utils._os.to_path\", return_value=file_path):\n            autoreload.template_changed(None, file_path)\n            mock_reset_loaders.assert_called_once()\n"], "sample_397": ["def test_engine_repr(self):\n    engine = self.engine_class({\n        'DIRS': ['/path/to/templates'],\n        'APP_DIRS': True,\n        'CONTEXT_PROCESSORS': ['path.to.processor'],\n        'DEBUG': True,\n        'LOADERS': [('django.template.loaders.cached.Loader', ['django.template.loaders.filesystem.Loader'])],\n        'STRING_IF_INVALID': 'INVALID',\n        'FILE_CHARSET': 'utf-16',\n        'LIBRARIES': {'custom': 'path.to.custom'},\n        'BUILTINS': ['path.to.builtin'],\n        'AUTOESCAPE': False,\n    })\n\n    expected_repr = (\n        \"<DjangoTemplates: dirs=['/path/to/templates'] app_dirs=True context_processors=['path.to.processor'] \"\n        \"debug=True loaders=[('django.template.loaders.cached.Loader', ['django.template.loaders.filesystem.Loader'])] \"\n        \"string_if_invalid='INVALID' file_charset='utf-16' libraries={'custom': 'path.to.custom'} \"\n        \"builtins=['path.to.builtin'] autoescape=False>\"\n    )\n    self.assertEqual(repr(engine), expected_repr)\n", "def test_engine_get_default(self):\n    engine = Engine.get_default()\n    self.assertIsInstance(engine, Engine)\n    # Test that the default engine is properly configured.\n    self.assertTrue(engine.debug)\n    self.assertEqual(engine.string_if_invalid, '')\n    self.assertEqual(engine.file_charset, 'utf-8')\n    self.assertTrue(engine.autoescape)\n", "def test_engine_get_default(self):\n    engine = Engine.get_default()\n    self.assertIsInstance(engine, Engine)\n    # Test that the default engine is properly configured\n    self.assertTrue(engine.debug)\n    self.assertEqual(engine.string_if_invalid, \"\")\n    self.assertEqual(engine.file_charset, \"utf-8\")\n    self.assertTrue(engine.autoescape)\n", "def test_engine_repr(self):\n    engine = self.engine_class({\n        'DIRS': ['test_dir'],\n        'APP_DIRS': True,\n        'CONTEXT_PROCESSORS': ['test_processor'],\n        'DEBUG': True,\n        'LOADERS': [('test_loader', 'test.loader')],\n        'STRING_IF_INVALID': 'test_string',\n        'FILE_CHARSET': 'utf-16',\n        'LIBRARIES': {'test_library': 'test.library'},\n        'BUILTINS': ['test_builtin'],\n        'AUTOESCAPE': False,\n    })\n\n    expected_repr = (\n        \"<DjangoTemplates: dirs=['test_dir'] app_dirs=True \"\n        \"context_processors=['test_processor'] debug=True \"\n        \"loaders=[('test_loader', 'test.loader')] \"\n        \"string_if_invalid='test_string' file_charset='utf-16' \"\n        \"libraries={'test_library': 'test.library'} builtins=['test_builtin'] \"\n        \"autoescape=False>\"\n    )\n\n    self.assertEqual(repr(engine), expected_repr)\n", "def test_engine_repr(self):\n    engine = self.engine_class({\n        'DIRS': ['/path/to/templates'],\n        'APP_DIRS': True,\n        'CONTEXT_PROCESSORS': ['processor1', 'processor2'],\n        'DEBUG': True,\n        'LOADERS': ['loader1', 'loader2'],\n        'STRING_IF_INVALID': 'Invalid',\n        'FILE_CHARSET': 'utf-8',\n        'LIBRARIES': {'lib1': 'path.to.lib1'},\n        'BUILTINS': ['builtin1', 'builtin2'],\n        'AUTOESCAPE': True,\n    })\n\n    expected_repr = (\n        \"<DjangoTemplates: dirs=['/path/to/templates'] app_dirs=True \"\n        \"context_processors=['processor1', 'processor2'] debug=True \"\n        \"loaders=['loader1', 'loader2'] string_if_invalid='Invalid' \"\n        \"file_charset='utf-8' libraries={'lib1': 'path.to.lib1'} \"\n        \"builtins=['builtin1', 'builtin2'] autoescape=True>\"\n    )\n\n    self.assertEqual(repr(engine), expected_repr)\n"], "sample_398": ["    def test_login_view_get(self):\n        response = self.client.get(\"/login/\")\n        self.assertEqual(response.status_code, 200)\n        self.assertTemplateUsed(response, \"registration/login.html\")\n", "def test_password_reset_confirm_view_invalid_token(self):\n    # Create a password reset token for the user\n    token = default_token_generator.make_token(self.u1)\n    uidb64 = urlsafe_base64_encode(str(self.u1.pk).encode())\n\n    # Try to access the password reset confirm view with an invalid token\n    response = self.client.get(reverse(\"password_reset_confirm\", args=[uidb64, \"invalid-token\"]))\n\n    # Check that the view renders the \"Password reset unsuccessful\" page\n    self.assertEqual(response.status_code, 200)\n    self.assertTemplateUsed(response, \"registration/password_reset_confirm.html\")\n    self.assertEqual(response.context[\"validlink\"], False)\n", "    def test_login_view(self):\n        # Test that the login view works as expected\n        response = self.client.get(\"/login/\")\n        self.assertEqual(response.status_code, 200)\n        self.assertTemplateUsed(response, \"registration/login.html\")\n\n        # Test that the login form is displayed\n        self.assertIn(\"form\", response.context)\n        self.assertIsInstance(response.context[\"form\"], AuthenticationForm)\n\n        # Test that the user can log in successfully\n        self.login()\n        self.assertEqual(self.client.session[SESSION_KEY], self.u1.pk)\n\n        # Test that the user is redirected to the correct URL after logging in\n        response = self.client.post(\n            \"/login/\", {\"username\": \"testclient\", \"password\": \"password\"}\n        )\n        self.assertRedirects(response, settings.LOGIN_REDIRECT_URL)\n", "    def test_login_view_redirects(self):\n        response = self.client.post(\n            \"/login/\", {\"username\": \"testclient\", \"password\": \"password\"}\n        )\n        self.assertRedirects(response, \"/\")\n", "def test_password_reset_complete_view(self):\n    \"\"\"\n    Test that the password reset complete view renders the correct template.\n    \"\"\"\n    url = reverse(\"password_reset_complete\")\n    response = self.client.get(url)\n    self.assertEqual(response.status_code, 200)\n    self.assertTemplateUsed(response, \"registration/password_reset_complete.html\")\n\n    # Test that the login URL is in the context\n    self.assertIn(\"login_url\", response.context)\n    self.assertEqual(response.context[\"login_url\"], reverse(settings.LOGIN_URL))\n"], "sample_399": ["def test_exists_with_subquery_and_limit(self):\n    subquery = Book.objects.filter(contact=self.a1).values(\"contact\")[:1]\n    qs = Author.objects.filter(Exists(subquery))\n    self.assertQuerysetEqual(qs, [self.a1])\n", "def test_empty(self):\n    Book.objects.filter(isbn=\"nonexistent\").delete()\n    self.assertEqual(Book.objects.count(), 6)\n    Book.objects.filter(isbn=\"nonexistent\").set_empty()\n    self.assertEqual(Book.objects.count(), 0)\n", "def test_build_lookup(self):\n    # Test lookup parts parsing.\n    lookup_parts, field_parts, _ = Book.objects._query.solve_lookup_type(\n        \"publisher__num_awards__gt\"\n    )\n    self.assertEqual(lookup_parts, (\"gt\",))\n    self.assertEqual(field_parts, (\"publisher\", \"num_awards\"))\n\n    lookup_parts, field_parts, _ = Book.objects._query.solve_lookup_type(\"rating\")\n    self.assertEqual(lookup_parts, ())\n    self.assertEqual(field_parts, (\"rating\",))\n\n    # Test resolving lookup parts into a concrete lookup class.\n    lookup = Book.objects._query.build_lookup(\n        [\"exact\"], Book.objects._query.get_meta().get_field(\"rating\"), 5\n    )\n    self.assertIsInstance(lookup, Exact)\n", "def test_resolve_expression(self):\n    expression = F(\"pages\") + 10\n    resolved_expression = expression.resolve_expression(Book.objects.all().query)\n    self.assertIsInstance(resolved_expression, Col)\n    self.assertEqual(resolved_expression.target.column, \"pages\")\n    self.assertEqual(resolved_expression.output_field, IntegerField())\n\n    annotation = Book.objects.annotate(total_pages=F(\"pages\") + 10)\n    resolved_annotation = annotation.query.annotations[\"total_pages\"].resolve_expression(\n        annotation.query\n    )\n    self.assertIsInstance(resolved_annotation, Col)\n    self.assertEqual(resolved_annotation.target.column, \"pages\")\n    self.assertEqual(resolved_annotation.output_field, IntegerField())\n", "def test_exists_subquery_with_outer_ref(self):\n    subquery = Book.objects.filter(\n        publisher=OuterRef(\"pk\"), pages__gt=F(\"publisher__num_awards\")\n    ).values_list(\"pk\", flat=True)[:1]\n    publishers = Publisher.objects.annotate(has_book=Exists(subquery)).filter(has_book=True)\n    self.assertQuerysetEqual(\n        publishers, [\"<Publisher: Apress>\", \"<Publisher: Prentice Hall>\"], ordered=False\n    )\n"], "sample_401": ["def test_management_form_is_invalid_if_total_forms_exceeds_absolute_max(self):\n    formset = self.make_choiceformset(\n        total_forms=1001,\n        initial_forms=0,\n        max_num_forms=1000,\n        min_num_forms=0,\n    )\n    self.assertFalse(formset.management_form.is_valid())\n    self.assertEqual(\n        formset.management_form.errors,\n        {\n            TOTAL_FORM_COUNT: [\n                \"Ensure this value is less than or equal to 1000.\"\n            ]\n        },\n    )\n", "def test_formset_add_fields_hook(self):\n    class HookedForm(Form):\n        pass\n\n    class BaseHookedFormSet(BaseFormSet):\n            super().add_fields(form, index)\n            form.fields[\"hooked_field\"] = CharField()\n\n    HookedFormSet = formset_factory(HookedForm, formset=BaseHookedFormSet)\n\n    formset = self.make_choiceformset(\n        formset_data=[(\"Choice1\", 1), (\"Choice2\", 2)], formset_class=HookedFormSet\n    )\n    for form in formset.forms:\n        self.assertIn(\"hooked_field\", form.fields)\n", "def test_formset_validate_max(self):\n    ChoiceFormSet = formset_factory(Choice, validate_max=True, max_num=2)\n    formset = self.make_choiceformset([\n        ('Choice 1', '10'),\n        ('Choice 2', '20'),\n        ('Choice 3', '30'),\n    ], formset_class=ChoiceFormSet)\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(len(formset.non_form_errors()), 1)\n    self.assertIn('Please submit at most 2 forms.', str(formset.non_form_errors()[0]))\n", "def test_formset_default_renderer(self):\n    # Test that the default renderer for a FormSet is used when not specified\n    formset = ChoiceFormSet()\n    self.assertIsInstance(formset.renderer, TemplatesSetting)\n", "def test_formset_with_multipart(self):\n    class MultipartForm(Form):\n        file = FileField()\n\n    MultipartFormSet = formset_factory(MultipartForm)\n    formset = MultipartFormSet()\n    self.assertTrue(formset.is_multipart())\n\n    formset = MultipartFormSet(data={\"form-0-file\": \"file.txt\"}, files={})\n    self.assertTrue(formset.is_multipart())\n"], "sample_402": ["    def test_append_slash_with_query_string(self):\n        request = self.rf.get(\"/test\", {\"query\": \"string\"})\n        request.urlconf = None\n        middleware = CommonMiddleware(get_response_empty)\n\n        response = middleware.process_request(request)\n\n        self.assertEqual(response.status_code, 301)\n        self.assertEqual(response[\"Location\"], \"/test/?query=string\")\n", "    def test_append_slash_with_custom_response_class(self):\n        class CustomRedirect(HttpResponsePermanentRedirect):\n            pass\n\n        class CustomCommonMiddleware(CommonMiddleware):\n            response_redirect_class = CustomRedirect\n\n        request = self.rf.get(\"/test\", HTTP_HOST=\"example.com\")\n        middleware = CustomCommonMiddleware(get_response_404)\n\n        response = middleware.process_request(request)\n\n        self.assertIsInstance(response, CustomRedirect)\n        self.assertEqual(response.status_code, 301)\n        self.assertEqual(response.url, \"/test/\")\n", "    def test_append_slash_with_query_string(self):\n        request = self.rf.get(\"/test/?param=value\")\n        middleware = CommonMiddleware(get_response_404)\n        response = middleware.process_request(request)\n        self.assertIsInstance(response, HttpResponsePermanentRedirect)\n        self.assertEqual(response.url, \"/test/?param=value/\")\n", "def test_append_slash_with_query_string(self):\n    request = self.rf.get(\"/test\", {\"a\": \"1\"})\n    request.path_info = \"/test\"\n    request.urlconf = None\n    middleware = CommonMiddleware(get_response_404)\n    response = middleware.process_request(request)\n    self.assertIsInstance(response, HttpResponsePermanentRedirect)\n    self.assertEqual(response.url, \"/test/?a=1\")\n", "    def test_append_slash_with_query_string(self):\n        request = self.rf.get(\"/test\", {\"query\": \"string\"})\n        request.urlconf = \"middleware.urls\"\n        middleware = CommonMiddleware(get_response_404)\n        response = middleware.process_request(request)\n        self.assertIsInstance(response, HttpResponsePermanentRedirect)\n        self.assertEqual(response.status_code, 301)\n        self.assertEqual(response[\"Location\"], \"/test/?query=string\")\n"], "sample_403": ["    def test_create_model(self):\n        operation = migrations.CreateModel(\n            \"Pony\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"pink\", models.IntegerField(default=1)),\n            ],\n        )\n        self.apply_operations([operation])\n        self.assertTableExists(\"pony\")\n        self.assertColumnExists(\"pony\", \"id\")\n        self.assertColumnExists(\"pony\", \"pink\")\n", "    def test_create_model(self):\n        operation = migrations.CreateModel(\n            name=\"Pony\",\n            fields=[\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"pink\", models.IntegerField(default=1)),\n            ],\n        )\n        # Test the state alteration\n        project_state = ProjectState()\n        new_state = project_state.clone()\n        operation.state_forwards(\"testapp\", new_state)\n        self.assertEqual(len(new_state.models[\"testapp\", \"pony\"].fields), 2)\n        # Test the database alteration\n        self.apply_operations(operation)\n        self.assertTableExists(\"testapp_pony\")\n        with connection.cursor() as cursor:\n            cursor.execute(\"PRAGMA table_info(testapp_pony)\")\n            self.assertEqual(\n                len([c for c in cursor.fetchall() if c[1] in [\"id\", \"pink\"]]), 2\n            )\n", "    def test_create_model(self):\n        operation = migrations.CreateModel(\n            \"Pony\", [(\"name\", models.CharField(max_length=300))]\n        )\n        self.apply_operations(operation)\n        Pony = self apps.get_model(\"testapp\", \"Pony\")\n        pony = Pony.objects.create(name=\"Sparkles\")\n        self.assertEqual(pony.name, \"Sparkles\")\n        self.assertTableExists(\"testapp_pony\")\n        with connection.schema_editor() as schema_editor:\n            constraints_with_names = [\n                constraint\n                for constraint in schema_editor.constraints\n                if constraint.name is not None\n            ]\n            self.assertEqual(len(constraints_with_names), 1)\n", "    def test_create_model(self):\n        operation = migrations.CreateModel(\n            name=\"Pony\",\n            fields=[\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"pink\", models.IntegerField()),\n            ],\n        )\n        self.assertEqual(operation.name, \"Pony\")\n        self.assertEqual(len(operation.fields), 2)\n        self.assertEqual(operation.options, {})\n\n        project_state = ProjectState()\n        new_state = project_state.clone()\n        operation.state_forwards(\"testapp\", new_state)\n        model_state = new_state.models[\"testapp\", \"pony\"]\n        self.assertEqual(model_state.name, \"Pony\")\n        self.assertEqual(len(model_state.fields), 2)\n        self.assertEqual(model_state.options, {})\n\n        with connection.schema_editor() as schema_editor:\n            operation.database_forwards(\"testapp\", schema_editor, project_state, new_state)\n            self.assertTableExists(\"testapp_pony\")\n\n        with connection.schema_editor() as schema_editor:\n            operation.database_backwards(\"testapp\", schema_editor, new_state, project_state)\n            self.assertTableNotExists(\"testapp_pony\")\n", "    def test_create_model(self):\n        operation = migrations.CreateModel(\n            name=\"Pony\",\n            fields=[\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=100)),\n            ],\n        )\n        # Test the state alteration\n        project_state = ProjectState()\n        new_state = project_state.clone()\n        operation.state_forwards(\"testapp\", new_state)\n        self.assertEqual(len(new_state.models[\"testapp\"]), 1)\n        self.assertEqual(\n            list(new_state.models[\"testapp\"][\"pony\"].fields.keys()), [\"id\", \"name\"]\n        )\n\n        # Make sure we can use the model\n        Pony = new_state.apps.get_model(\"testapp\", \"pony\")\n        pony = Pony.objects.create(name=\"Sparkles\")\n        self.assertEqual(pony.id, 1)\n\n        # Now go backwards\n        operation.database_backwards(\"testapp\", None, new_state, project_state)\n        self.assertTableDoesNotExist(\"testapp_pony\")\n\n        # And check it's all gone from the state\n        self.assertEqual(len(project_state.models[\"testapp\"]), 0)\n"], "sample_404": ["    def test_lexers(self):\n        template_string = '{% if foo %}Bar{% endif %}'\n        lexer = Lexer(template_string)\n        tokens = lexer.tokenize()\n        self.assertEqual(len(tokens), 5)\n        self.assertEqual(tokens[0].token_type, Lexer.TOKEN_BLOCK)\n        self.assertEqual(tokens[1].token_type, Lexer.TOKEN_TEXT)\n        self.assertEqual(tokens[2].token_type, Lexer.TOKEN_VAR)\n        self.assertEqual(tokens[3].token_type, Lexer.TOKEN_TEXT)\n        self.assertEqual(tokens[4].token_type, Lexer.TOKEN_BLOCK)\n", "    def test_tokenize(self):\n        lexer = self._engine().lexer\n        template_string = \"Hello, {{ name }}!\"\n        tokens = lexer.tokenize(template_string)\n        self.assertEqual(len(tokens), 3)\n        self.assertEqual(tokens[0].token_type, 0)  # TokenType.TEXT\n        self.assertEqual(tokens[1].token_type, 1)  # TokenType.VAR\n        self.assertEqual(tokens[2].token_type, 0)  # TokenType.TEXT\n", "    def test_tokenize(self):\n        source = \"{% if foo %}{{ bar }}{% endif %}\"\n        lexer = DebugLexer(source)\n        tokens = lexer.tokenize()\n        self.assertEqual(len(tokens), 5)\n        self.assertEqual(tokens[0].token_type, 2)  # BLOCK\n        self.assertEqual(tokens[1].token_type, 1)  # VAR\n        self.assertEqual(tokens[2].token_type, 2)  # BLOCK\n", "    def test_tokenization(self):\n        source = \"{% if foo %}bar{% endif %}\"\n        lexer = DebugLexer(source)\n        tokens = lexer.tokenize()\n        self.assertEqual(len(tokens), 5)\n        self.assertEqual(tokens[0].token_type, TokenType.BLOCK)\n        self.assertEqual(tokens[1].token_type, TokenType.TEXT)\n        self.assertEqual(tokens[2].token_type, TokenType.BLOCK)\n", "    def test_create_token(self):\n        lexer = self._engine().lexer\n        token = lexer.create_token(\"hello\", None, 1, False)\n        self.assertEqual(token.token_type, lexer.TokenType.TEXT)\n        self.assertEqual(token.contents, \"hello\")\n        self.assertIsNone(token.position)\n        self.assertEqual(token.lineno, 1)\n"], "sample_406": ["    def test_manager_descriptor(self):\n        # Test that the ManagerDescriptor returns the correct manager instance\n        manager = Manager()\n        descriptor = ManagerDescriptor(manager)\n        self.assertIs(descriptor.__get__(None, Article), manager)\n", "    def test_manager_construction(self):\n        manager = BaseManager()\n        self.assertEqual(manager._constructor_args, ((), {}))\n        self.assertIsNone(manager.model)\n        self.assertIsNone(manager.name)\n        self.assertIsNone(manager._db)\n        self.assertEqual(manager._hints, {})\n", "    def test_base_manager_from_queryset(self):\n        class MyQuerySet(models.QuerySet):\n                return \"custom method called\"\n\n        MyManager = BaseManager.from_queryset(MyQuerySet)\n        manager = MyManager()\n        self.assertEqual(manager.custom_method(), \"custom method called\")\n        self.assertIs(manager._queryset_class, MyQuerySet)\n", "    def test_manager_descriptor(self):\n        manager = BaseManager()\n        manager.name = 'test_manager'\n        model = mock.Mock(_meta=mock.Mock(managers_map={'test_manager': manager}))\n        descriptor = ManagerDescriptor(manager)\n        self.assertEqual(descriptor.__get__(None, model), manager)\n", "    def test_construction(self):\n        manager = BaseManager()\n        self.assertIsNone(manager.model)\n        self.assertIsNone(manager.name)\n        self.assertIsNone(manager._db)\n"], "sample_407": ["def test_foreign_key_on_model_with_custom_primary_key(self):\n    school = School.objects.create(name=\"Test School\")\n    student1 = Student.objects.create(school=school, student_id=1)\n    student2 = Student.objects.create(school=school, student_id=2)\n\n    self.assertEqual(student1.school, school)\n    self.assertEqual(student2.school, school)\n\n    # Ensure that foreign key is correctly set when retrieving model instance\n    retrieved_student1 = Student.objects.get(student_id=1)\n    self.assertEqual(retrieved_student1.school, school)\n", "def test_multiple_objects_returned(self):\n    # Create another Article with the same headline and reporter as the first article.\n    a2 = Article(headline=\"This is a test\", pub_date=datetime.date(2005, 7, 27), reporter=self.r)\n    a2.save()\n\n    # Try to get an article by its headline. This should raise MultipleObjectsReturned.\n    with self.assertRaises(MultipleObjectsReturned):\n        Article.objects.get(headline=\"This is a test\")\n", "def test_delete_model_instance(self):\n    self.assertEqual(Article.objects.count(), 1)\n    self.a.delete()\n    self.assertEqual(Article.objects.count(), 0)\n\n    # Deleting a non-existent object should not raise an error.\n    self.a.delete()\n    self.assertEqual(Article.objects.count(), 0)\n", "def test_foreign_key_onetoonefield_unsaved(self):\n    # Accessing an unsaved forward OneToOneField or ForeignKey should raise a ValueError.\n    p = Parent(name=\"fred\")\n    self.assertRaises(ValueError, lambda: Child(parent=p))\n    self.assertRaises(ValueError, lambda: ToFieldChild(parent=p))\n\n    c = Child(name=\"barney\")\n    self.assertRaises(ValueError, lambda: Parent(child=c))\n\n    # Accessing an unsaved reverse OneToOneField or ForeignKey should also raise a ValueError.\n    self.assertRaises(ValueError, lambda: p.child)\n    self.assertRaises(ValueError, lambda: p.to_field_child)\n\n    self.assertRaises(ValueError, lambda: c.parent)\n", "def test_related_object_cache(self):\n    # Create an article to use in checking the related-object cache\n    article = Article.objects.create(\n        headline=\"Test\", pub_date=datetime.date(2005, 7, 28), reporter=self.r\n    )\n\n    # Check that we do have attribute access working properly.\n    self.assertEqual(article.reporter.id, self.r.id)\n\n    # Now wipe out the reporter info from the article; this will not\n    # delete the related object instance from memory (since we still have a\n    # reference to it).\n    del article.reporter\n\n    # We can reload the related info from the database by using refresh_from_db()\n    article.refresh_from_db()\n\n    # We haven't specified any values for article.reporter, so accessing it\n    # should load the data from the database (which is reporter r2).\n    self.assertEqual(article.reporter.id, self.r.id)\n"], "sample_409": ["def test_blocktrans_with_filter(self):\n    output = self.engine.render_to_string(\"i18n04\", {\"anton\": \"Anton\"})\n    self.assertEqual(output, \"ANTON\")\n", "def test_blocktrans_with(self):\n    \"\"\"The {% blocktrans with %} template tag works.\"\"\"\n    output = self.engine.render_to_string(\"i18n04\")\n    self.assertEqual(output, \"bar\")\n", "def test_i18n04(self):\n    \"\"\"Test the {% blocktranslate with %} syntax\"\"\"\n    output = self.engine.render_to_string(\"i18n04\", {\"anton\": \"Anton\", \"foo\": \"bar\"})\n    self.assertEqual(output, \"Anton BAR\")\n", "    def test_blocktrans_context(self, tag_name):\n        template = self.engine.get_template(\"i18n04\")\n        rendered = template.render(Context({\"foo\": \"bar\"}))\n        trans_real.pgettext(\"greeting\", \"bar\")\n        self.assertEqual(rendered.strip(), \"bar\")\n", "def test_blocktrans_tag_with_variable(self, tag_name):\n    output = self.engine.render_to_string(\"i18n05\", {\"foo\": \"not bar\"})\n    self.assertEqual(output, \"bar\\nbar\")\n"], "sample_410": ["    def test_normalize_email(self):\n        email = \"Test@Example.com\"\n        normalized_email = BaseUserManager.normalize_email(email)\n        self.assertEqual(normalized_email, \"Test@example.com\")\n", "    def test_normalize_email(self):\n        email = \"TestEmail@example.com\"\n        normalized_email = BaseUserManager.normalize_email(email)\n        self.assertEqual(normalized_email, \"TestEmail@example.com\")\n\n        email = \"testemail@EXAMPLE.COM\"\n        normalized_email = BaseUserManager.normalize_email(email)\n        self.assertEqual(normalized_email, \"testemail@example.com\")\n\n        email = \"\"\n        normalized_email = BaseUserManager.normalize_email(email)\n        self.assertEqual(normalized_email, \"\")\n\n        email = None\n        normalized_email = BaseUserManager.normalize_email(email)\n        self.assertEqual(normalized_email, \"\")\n", "    def test_normalize_email(self):\n        email = \"Test@Example.com\"\n        normalized_email = BaseUserManager.normalize_email(email)\n        self.assertEqual(normalized_email, \"Test@example.com\")\n", "    def test_normalize_email(self):\n        email = \"test@EXAMPLE.COM\"\n        normalized_email = BaseUserManager.normalize_email(email)\n        self.assertEqual(normalized_email, \"test@example.com\")\n", "    def test_normalize_email(self):\n        email = \"TestUser@EXAMPLE.com\"\n        normalized_email = BaseUserManager.normalize_email(email)\n        self.assertEqual(normalized_email, \"TestUser@example.com\")\n"], "sample_411": ["def test_base_command_with_missing_args_message(self):\n    class TestCommand(BaseCommand):\n        missing_args_message = \"You need to specify some arguments.\"\n\n    command = TestCommand()\n    parser = command.create_parser(\"manage.py\", \"test\")\n    with self.assertRaises(CommandError) as context:\n        parser.parse_args([])\n    self.assertEqual(context.exception.args[0], \"You need to specify some arguments.\")\n", "def test_base_command_style(self):\n    class TestCommand(BaseCommand):\n            self.stdout.write(self.style.SUCCESS(\"Success\"))\n            self.stderr.write(self.style.ERROR(\"Error\"))\n\n    with captured_stderr() as stderr:\n        with mock.patch.object(TestCommand, \"stdout\", new=StringIO()):\n            command = TestCommand()\n            command.handle()\n            self.assertIn(\"\\033[92mSuccess\\033[0m\", command.stdout.getvalue())\n            self.assertIn(\"\\033[91mError\\033[0m\", stderr.getvalue())\n", "def test_base_command_options(self):\n    class TestCommand(BaseCommand):\n            pass\n\n    # Ensure the default options are present.\n    command = TestCommand()\n    parser = command.create_parser(\"manage.py\", \"test\")\n    self.assertEqual(parser.get_default(\"verbosity\"), 1)\n    self.assertEqual(parser.get_default(\"settings\"), None)\n    self.assertEqual(parser.get_default(\"pythonpath\"), None)\n    self.assertEqual(parser.get_default(\"traceback\"), False)\n\n    # Ensure the default options can be overridden.\n    parser = command.create_parser(\"manage.py\", \"test\")\n    options = parser.parse_args([\"--verbosity\", \"2\", \"--settings\", \"test_settings\"])\n    self.assertEqual(options.verbosity, 2)\n    self.assertEqual(options.settings, \"test_settings\")\n\n    # Ensure that unknown options raise an error.\n    with self.assertRaises(SystemExit):\n        parser.parse_args([\"--unknown-option\"])\n\n    # Ensure the --no-color and --force-color options are mutually exclusive.\n    with self.assertRaises(CommandError):\n        parser.parse_args([\"--no-color\", \"--force-color\"])\n", "def test_base_command_arguments(self):\n    class Command(BaseCommand):\n            self.stdout.write(\", \".join(sorted(options.keys())))\n\n    out = StringIO()\n    command = Command(stdout=out)\n    parser = command.create_parser(\"manage.py\", \"command\")\n    options = parser.parse_args([\"--verbosity\", \"2\", \"--no-color\"])\n    command.execute(**vars(options))\n    self.assertEqual(\n        set(out.getvalue().strip().split(\", \")),\n        {\n            \"force_color\",\n            \"no_color\",\n            \"pythonpath\",\n            \"settings\",\n            \"skip_checks\",\n            \"stderr\",\n            \"stdout\",\n            \"traceback\",\n            \"verbosity\",\n        },\n    )\n", "def test_base_command_style(self):\n    class TestCommand(BaseCommand):\n            self.stdout.write(self.style.SUCCESS(\"Success message\"))\n            self.stderr.write(self.style.ERROR(\"Error message\"))\n\n    out = StringIO()\n    err = StringIO()\n    command = TestCommand(stdout=out, stderr=err)\n    command.handle()\n\n    self.assertIn(\"\\033[92mSuccess message\\033[0m\", out.getvalue())\n    self.assertIn(\"\\033[91mError message\\033[0m\", err.getvalue())\n"], "sample_412": ["def test_escapejs(self):\n    self.check_output(escapejs, \"testing\\\\x20javascript\\\\x20escape\",\n                      'testing\\\\u005Cx20javascript\\\\u005Cx20escape')\n    self.check_output(escapejs, \"testing\\x20javascript\\x20escape\",\n                      'testing\\\\u0020javascript\\\\u0020escape')\n    self.check_output(escapejs, \"testing\\u2028javascript\\u2029escape\",\n                      'testing\\\\u2028javascript\\\\u2029escape')\n    self.check_output(escapejs, \"testing'javascript\\\"escape\",\n                      'testing\\\\u0027javascript\\\\u0022escape')\n", "def test_escapejs(self):\n    self.check_output(escapejs, \"hello world\")\n    self.check_output(escapejs, \"<script>alert('hello')</script>\")\n    self.check_output(escapejs, \"this & that\")\n    self.check_output(escapejs, '\"hello\"')\n    self.check_output(escapejs, \"'hello'\")\n    self.check_output(escapejs, \"\\u2028\\u2029\")\n", "def test_escapejs(self):\n    self.check_output(escapejs, \"Hello, World!\")\n    self.check_output(escapejs, \"Hello,\\nWorld!\")\n    self.check_output(escapejs, \"<script>alert('XSS')</script>\")\n    self.check_output(escapejs, '\"double quotes\"')\n    self.check_output(escapejs, \"'single quotes'\")\n", "def test_escapejs(self):\n    self.check_output(escapejs, \"Hello, World!\")\n    self.check_output(escapejs, \"<script>alert('XSS')</script>\")\n    self.check_output(escapejs, 'var x = \"Hello, World!\";')\n", "def test_conditional_escape(self):\n    self.check_output(conditional_escape, \"Hello, World!\")\n    self.check_output(conditional_escape, \"<script>alert('XSS')</script>\", \"&lt;script&gt;alert(&#x27;XSS&#x27;)&lt;/script&gt;\")\n    self.check_output(conditional_escape, mark_safe(\"<p>Hello, World!</p>\"), \"<p>Hello, World!</p>\")\n    self.check_output(conditional_escape, lazystr(\"Hello, World!\"), \"Hello, World!\")\n"], "sample_413": ["    def test_string_if_invalid_is_not_string(self):\n        errors = check_string_if_invalid_is_string(None)\n        self.assertEqual(len(errors), 1)\n        self.assertEqual(errors[0].id, E002.id)\n        self.assertIn(\"must be a string but got: 123 (int)\", errors[0].msg)\n", "    def test_string_if_invalid_not_a_string(self):\n        errors = check_string_if_invalid_is_string(None)\n        self.assertEqual(len(errors), 1)\n        error = errors[0]\n        self.assertIsInstance(error, Error)\n        self.assertEqual(error.id, E002.id)\n        self.assertIn(\"must be a string but got: 123 (int)\", error.msg)\n", "    def test_string_if_invalid_not_a_string(self):\n        errors = check_string_if_invalid_is_string(None)\n        self.assertEqual(len(errors), 1)\n        error = errors[0]\n        self.assertEqual(error.id, E002.id)\n        self.assertIn(\"must be a string but got: 123 (int)\", error.msg)\n", "    def test_string_if_invalid_is_not_string(self):\n        errors = check_string_if_invalid_is_string(None)\n        self.assertEqual(len(errors), 1)\n        self.assertEqual(errors[0].id, E002.id)\n        self.assertIn(\"must be a string but got: 123 (int)\", errors[0].msg)\n", "    def test_string_if_invalid_is_not_string(self):\n        errors = check_string_if_invalid_is_string(None)\n        self.assertEqual(len(errors), 1)\n        error = errors[0]\n        self.assertIsInstance(error, Error)\n        self.assertEqual(error.id, E002.id)\n"], "sample_414": ["    def test_charfield_with_form_class_override(self):\n        model = Album\n        fieldname = \"name\"\n        widgetclass = forms.TextInput\n\n        class MyModelAdmin(admin.ModelAdmin):\n            formfield_overrides = {\n                CharField: {\"widget\": forms.Textarea},\n            }\n\n        self.assertFormfield(model, fieldname, widgetclass, formfield_overrides=MyModelAdmin.formfield_overrides)\n", "    def test_datefield_with_widget_override(self):\n        model = Event\n        fieldname = \"date\"\n        widgetclass = widgets.AdminDateWidget\n\n        class MyModelAdmin(admin.ModelAdmin):\n            formfield_overrides = {\n                DateField: {\"widget\": widgetclass},\n            }\n\n        ma = MyModelAdmin(model, admin.site)\n        ff = ma.formfield_for_dbfield(model._meta.get_field(fieldname), request=None)\n\n        self.assertIsInstance(ff.widget, widgetclass)\n", "    def test_datefield_with_widget_overrides(self):\n        class MyModel(models.Model):\n            my_field = DateField()\n\n        self.assertFormfield(\n            MyModel,\n            \"my_field\",\n            admin.widgets.AdminDateWidget,\n            formfield_overrides={DateField: {\"widget\": admin.widgets.AdminDateWidget}},\n        )\n", "    def test_charfield_with_textareawidget_override(self):\n        self.assertFormfield(\n            Event, \"description\", widgets.AdminTextareaWidget, formfield_overrides={CharField: {\"widget\": widgets.AdminTextareaWidget}}\n        )\n", "    def test_charfield_with_textareawidget_override(self):\n        # Define a model with a CharField\n        class MyModel(models.Model):\n            f = models.CharField(max_length=10)\n\n        # Override the CharField widget to be a Textarea\n        class MyModelAdmin(admin.ModelAdmin):\n            formfield_overrides = {models.CharField: {\"widget\": widgets.AdminTextareaWidget}}\n\n        ma = MyModelAdmin(MyModel, admin.site)\n        ff = ma.formfield_for_dbfield(MyModel._meta.get_field(\"f\"), request=None)\n        self.assertIsInstance(ff.widget, widgets.AdminTextareaWidget)\n"], "sample_415": ["    def test_create_check_constraint(self):\n        constraint = models.CheckConstraint(\n            check=models.Q(age__gte=18), name=\"age_gte_18\"\n        )\n        with atomic():\n            with connection.schema_editor() as schema_editor:\n                schema_editor.add_constraint(Product, constraint)\n                constraints = get_constraints(Product._meta.db_table)\n                self.assertIn(\"age_gte_18\", [c[\"name\"] for c in constraints])\n", "    def test_create_check_constraint(self):\n        # Create a model with a check constraint\n        model = Product\n        constraint = CheckConstraint(check=Q(price__gt=0), name='price_check')\n        model.add_to_class('Meta', type('Meta', (), {'constraints': [constraint]}))\n\n        # Make sure the check constraint is created\n        with atomic():\n            model.objects.create(name='Test Product', price=10)\n            constraints = get_constraints(model._meta.db_table)\n            self.assertIn(constraint.name, [c['name'] for c in constraints])\n", "    def test_create_check_constraint(self):\n        # Create a model with a check constraint.\n        class CheckConstraintModel(models.Model):\n            value = models.IntegerField()\n            check_constraint = models.CheckConstraint(\n                check=models.Q(value__gte=0), name=\"check_value_gte_0\"\n            )\n\n        # Create the table in the database.\n        with connection.schema_editor() as schema_editor:\n            schema_editor.create_model(CheckConstraintModel)\n\n        # Check that the constraint was created.\n        constraints = get_constraints(CheckConstraintModel._meta.db_table)\n        self.assertIn(\"check_value_gte_0\", [c[\"name\"] for c in constraints])\n", "    def test_create_check_constraint(self):\n        # Create a model with a check constraint.\n        class Model(models.Model):\n            value = models.IntegerField()\n            check_constraint = models.CheckConstraint(\n                check=models.Q(value__gt=0), name='check_value'\n            )\n\n        # Create the table.\n        with connection.schema_editor() as schema_editor:\n            schema_editor.create_model(Model)\n\n        # Ensure the check constraint exists.\n        constraints = get_constraints(Model._meta.db_table)\n        self.assertIn('check_value', [c['name'] for c in constraints])\n", "    def test_create_check_constraint(self):\n        class Model(models.Model):\n            value = models.IntegerField()\n\n            class Meta:\n                constraints = [\n                    models.CheckConstraint(check=models.Q(value__gte=0), name='value_gte_0'),\n                ]\n\n        with connection.schema_editor() as schema_editor:\n            schema_editor.create_model(Model)\n            constraints = get_constraints(Model._meta.db_table)\n            self.assertIn('value_gte_0', [c['name'] for c in constraints])\n"], "sample_416": ["def test_settings_to_cmd_args_env_with_ssl_options(self):\n    settings_dict = {\n        \"HOST\": \"localhost\",\n        \"PORT\": 5432,\n        \"NAME\": \"mydb\",\n        \"USER\": \"myuser\",\n        \"PASSWORD\": \"mypassword\",\n        \"OPTIONS\": {\n            \"sslmode\": \"require\",\n            \"sslrootcert\": \"/path/to/root/cert\",\n            \"sslcert\": \"/path/to/cert\",\n            \"sslkey\": \"/path/to/key\",\n        },\n    }\n    args, env = self.settings_to_cmd_args_env(settings_dict)\n    self.assertEqual(args, [\"psql\", \"-U\", \"myuser\", \"-h\", \"localhost\", \"-p\", \"5432\", \"mydb\"])\n    self.assertEqual(env, {\n        \"PGPASSWORD\": \"mypassword\",\n        \"PGSSLMODE\": \"require\",\n        \"PGSSLROOTCERT\": \"/path/to/root/cert\",\n        \"PGSSLCERT\": \"/path/to/cert\",\n        \"PGSSLKEY\": \"/path/to/key\",\n    })\n", "def test_settings_to_cmd_args_env_with_ssl(self):\n    settings_dict = {\n        'HOST': 'localhost',\n        'PORT': 5432,\n        'NAME': 'mydb',\n        'USER': 'myuser',\n        'PASSWORD': 'mypassword',\n        'OPTIONS': {\n            'sslmode': 'require',\n            'sslrootcert': '/path/to/root/cert',\n            'sslcert': '/path/to/client/cert',\n            'sslkey': '/path/to/client/key',\n        }\n    }\n    args, env = self.settings_to_cmd_args_env(settings_dict)\n    self.assertEqual(args, ['psql', '-h', 'localhost', '-p', '5432', '-U', 'myuser', 'mydb'])\n    self.assertEqual(env, {\n        'PGPASSWORD': 'mypassword',\n        'PGSSLMODE': 'require',\n        'PGSSLROOTCERT': '/path/to/root/cert',\n        'PGSSLCERT': '/path/to/client/cert',\n        'PGSSLKEY': '/path/to/client/key',\n    })\n", "def test_runshell_signal_handling(self):\n    @mock.patch('signal.signal')\n    @mock.patch.object(DatabaseClient, 'runshell', side_effect=subprocess.CalledProcessError(1, 'psql'))\n        # Test that the original SIGINT handler is restored after runshell.\n        DatabaseClient().runshell([])\n        mock_signal.assert_called_with(signal.SIGINT, mock.ANY)\n        self.assertEqual(mock_signal.call_count, 2)\n\n    test_call()\n", "def test_settings_to_cmd_args_env_with_password(self):\n    settings_dict = {\n        \"NAME\": \"mydatabase\",\n        \"USER\": \"myuser\",\n        \"PASSWORD\": \"mypassword\",\n        \"HOST\": \"localhost\",\n        \"PORT\": 5432,\n    }\n    cmd_args, env = self.settings_to_cmd_args_env(settings_dict)\n    self.assertEqual(env[\"PGPASSWORD\"], \"mypassword\")\n    self.assertIn(\"-U\", cmd_args)\n    self.assertIn(\"myuser\", cmd_args)\n    self.assertIn(\"-h\", cmd_args)\n    self.assertIn(\"localhost\", cmd_args)\n    self.assertIn(\"-p\", cmd_args)\n    self.assertIn(\"5432\", cmd_args)\n    self.assertIn(\"mydatabase\", cmd_args)\n", "def test_runshell(self):\n    @mock.patch('django.db.backends.postgresql.client.DatabaseClient.runshell')\n    @mock.patch('signal.signal')\n        client = DatabaseClient(connection)\n        client.runshell()\n        mock_signal.assert_called_with(signal.SIGINT, signal.SIG_IGN)\n        mock_runshell.assert_called_once()\n        mock_signal.assert_called_with(signal.SIGINT, mock.ANY)\n\n    inner()\n"], "sample_417": ["    def test_floatformat_decimal(self):\n        with localcontext() as ctx:\n            ctx.prec = 3\n            self.assertEqual(floatformat(Decimal(\"123.456\")), \"123.5\")\n            self.assertEqual(floatformat(Decimal(\"123.456\"), 2), \"123.46\")\n            self.assertEqual(floatformat(Decimal(\"123.456\"), -2), \"123.46\")\n            self.assertEqual(floatformat(Decimal(\"123.456\"), \"2g\"), \"123.46\")\n            self.assertEqual(floatformat(Decimal(\"123.456\"), \"2u\"), \"123.46\")\n", "    def test_floatformat_arg(self):\n        output = self.engine.render_to_string(\n            \"floatformat02\", {\"a\": 34.23234, \"b\": 34.00000, \"c\": 34.26000}\n        )\n        self.assertEqual(output, \"34.232 34.000 34.260\")\n", "    def test_floatformat_precision(self):\n        out = self.engine.render_to_string(\n            \"{% autoescape off %}{{ a|floatformat:3 }} {{ b|floatformat:4 }}{% endautoescape %}\",\n            {\"a\": 34.23234, \"b\": 34.00000},\n        )\n        self.assertEqual(out, \"34.232 34.0000\")\n", "    def test_floatformat_negative(self):\n        output = self.engine.render_to_string(\n            \"floatformat_negative01\", {\"a\": 34.23234, \"b\": 34.00000}\n        )\n        self.assertEqual(output, \"34.232 34\")\n", "    def test_addslashes(self):\n        output = self.engine.render_to_string(\"addslashes01\", {\"a\": \"Hello, world!\", \"b\": \"Hello\\\\world!\"})\n        self.assertEqual(output, \"Hello, world! Hello\\\\\\\\world!\")\n"], "sample_418": ["    def test_addslashes(self):\n        output = self.engine.render_to_string(\"stringfilter01\")\n        self.assertEqual(output, \"Hello, World\\\\!\")\n", "    def test_add(self):\n        output = self.engine.render_to_string(\"add01\", {\"num1\": 5, \"num2\": 7})\n        self.assertEqual(output, \"12\")\n", "    def test_addslashes(self):\n        output = self.engine.render_to_string(\"addslashes01\", {\"a\": \"a 'b' c\"})\n        self.assertEqual(output, \"a \\\\'b\\\\' c\")\n", "    def test_floatformat_without_arguments(self):\n        output = self.engine.render_to_string(\"stringformat01\", {\"s\": 123.4567})\n        self.assertEqual(output, \"123.5\")\n", "    def test_lower(self):\n        output = self.engine.render_to_string(\"lower01\", {\"a\": \"HELLO\"})\n        self.assertEqual(output, \"hello\")\n"], "sample_419": ["def test_formset_get_context(self):\n    ChoiceFormSet = formset_factory(Choice)\n    formset = ChoiceFormSet()\n    context = formset.get_context()\n    self.assertEqual(context, {\"formset\": formset})\n", "def test_formset_initial_data(self):\n    initial = [{\"choice\": \"Cale\", \"votes\": 100}, {\"choice\": \"Sizemore\", \"votes\": 1}]\n    formset = self.make_choiceformset(initial=initial)\n    self.assertEqual(formset.initial_form_count(), len(initial))\n    for form, data in zip(formset.forms, initial):\n        self.assertEqual(form.initial, data)\n", "def test_formset_management_form_is_valid_with_no_data(self):\n    formset = self.make_choiceformset(formset_data=None)\n    management_form = formset.management_form\n    self.assertTrue(management_form.is_valid())\n    self.assertEqual(management_form.cleaned_data[TOTAL_FORM_COUNT], 1)\n    self.assertEqual(management_form.cleaned_data[INITIAL_FORM_COUNT], 0)\n    self.assertEqual(management_form.cleaned_data[MIN_NUM_FORM_COUNT], 0)\n    self.assertEqual(management_form.cleaned_data[MAX_NUM_FORM_COUNT], 0)\n", "def test_management_form_clean_total_forms_required(self):\n    class TestFormSet(BaseFormSet):\n            super().__init__(*args, **kwargs)\n            self.management_form.cleaned_data = {\n                TOTAL_FORM_COUNT: None,\n                INITIAL_FORM_COUNT: 0,\n            }\n\n    formset = TestFormSet()\n    with self.assertRaises(ValidationError):\n        formset.management_form.clean()\n\n    formset.management_form.cleaned_data = {TOTAL_FORM_COUNT: 5}\n    self.assertEqual(formset.management_form.clean(), {TOTAL_FORM_COUNT: 5, INITIAL_FORM_COUNT: 0})\n", "def test_formset_factory_renderer(self):\n    # Test that the renderer argument is correctly passed to the FormSet.\n    renderer = TemplatesSetting()\n    FormSet = formset_factory(FavoriteDrinkForm, renderer=renderer)\n    self.assertIs(FormSet.renderer, renderer)\n\n    # Test that the default renderer is used if no renderer is provided.\n    FormSet = formset_factory(FavoriteDrinkForm)\n    self.assertIs(FormSet.renderer, get_default_renderer())\n"], "sample_420": ["def test_modelform_factory_all_fields(self):\n    # When using modelform_factory, passing fields='__all__' should result in\n    # a form with all fields.\n    Form = modelform_factory(Article, fields=\"__all__\")\n    expected_fields = [\"headline\", \"pub_date\", \"categories\"]\n    self.assertEqual(set(Form.base_fields), set(expected_fields))\n", "def test_fields_for_model_with_to_field(self):\n    class CustomModelForm(forms.ModelForm):\n        class Meta:\n            model = Writer\n            fields = \"__all__\"\n            widgets = {\"name\": forms.TextInput}\n\n    fields = fields_for_model(\n        Writer, fields=[\"name\"], widgets={\"name\": forms.Textarea}\n    )\n    self.assertEqual(fields[\"name\"].widget.__class__, forms.Textarea)\n\n    # Make sure fields_for_model doesn't modify the form's widgets.\n    fields = fields_for_model(Writer, fields=[\"name\"], form=CustomModelForm)\n    self.assertEqual(fields[\"name\"].widget.__class__, forms.TextInput)\n", "def test_fields_for_model_empty_fields(self):\n    fields = fields_for_model(Writer, fields=())\n    self.assertEqual(fields, {})\n", "def test_fields_for_model_with_custom_widget(self):\n    class CustomForm(forms.ModelForm):\n        class Meta:\n            model = Article\n            fields = \"__all__\"\n            widgets = {\"headline\": forms.Textarea}\n\n    fields = fields_for_model(Article, widgets={\"headline\": forms.Textarea})\n    self.assertIsInstance(fields[\"headline\"].widget, forms.Textarea)\n\n    fields = fields_for_model(Article)\n    self.assertIsInstance(fields[\"headline\"].widget, forms.TextInput)\n\n    CustomForm.fields = fields\n    form = CustomForm()\n    self.assertIsInstance(form.fields[\"headline\"].widget, forms.Textarea)\n", "def test_modelform_factory_with_fields_and_exclude(self):\n    # When both 'fields' and 'exclude' are passed to modelform_factory(),\n    # 'exclude' should be ignored.\n\n    form_class = modelform_factory(Article, fields=(\"headline\",), exclude=(\"pub_date\",))\n    form = form_class()\n    self.assertEqual(list(form.fields.keys()), [\"headline\"])\n"], "sample_421": ["def test_case_expression_with_multiple_conditions(self):\n    # Create a Case expression with multiple When conditions.\n    case_expr = Case(\n        When(integer=1, then=Value('one')),\n        When(integer=2, then=Value('two')),\n        default=Value('other')\n    )\n\n    # Use the Case expression in a query.\n    results = list(CaseTestModel.objects.values_list('integer', case_expr))\n\n    # Expected results.\n    expected_results = [\n        (1, 'one'),\n        (2, 'two'),\n        (2, 'two'),\n        (3, 'other'),\n        (3, 'other'),\n        (3, 'other'),\n        (4, 'other'),\n    ]\n\n    # Assert that the results match the expected results.\n    self.assertEqual(results, expected_results)\n", "def test_case_expression_with_empty_when_clauses(self):\n    with self.assertRaises(ValueError):\n        Case(When(Q(integer=1), then=Value('1'))).resolve_expression()\n", "def test_case_when_boolean_values(self):\n    CaseTestModel.objects.create(boolean=True)\n    CaseTestModel.objects.create(boolean=False)\n    # Test that boolean values in When conditions are handled correctly.\n    qs = CaseTestModel.objects.annotate(\n        then_value=Case(When(boolean=True, then=Value(\"True\")), default=Value(\"False\"))\n    )\n    self.assertEqual(qs.filter(then_value=\"True\").count(), 1)\n    self.assertEqual(qs.filter(then_value=\"False\").count(), 1)\n", "def test_case_expression_with_empty_when_clauses(self):\n    with self.assertRaises(ValueError):\n        Case(When(), default=Value(\"default\"))\n\n    with self.assertRaises(ValueError):\n        Case(When(Q()), default=Value(\"default\"))\n", "def test_case_expression_with_multiple_conditions(self):\n    case = Case(\n        When(integer=1, then=Value(\"one\")),\n        When(integer=2, then=Value(\"two\")),\n        default=Value(\"other\"),\n        output_field=CharField(),\n    )\n    results = list(CaseTestModel.objects.values(\"integer\").annotate(result=case))\n    expected = [\n        {\"integer\": 1, \"result\": \"one\"},\n        {\"integer\": 2, \"result\": \"two\"},\n        {\"integer\": 2, \"result\": \"two\"},\n        {\"integer\": 3, \"result\": \"other\"},\n        {\"integer\": 3, \"result\": \"other\"},\n        {\"integer\": 3, \"result\": \"other\"},\n        {\"integer\": 4, \"result\": \"other\"},\n    ]\n    self.assertEqual(results, expected)\n"], "sample_422": ["def test_forward_many_to_one_descriptor_get_prefetch_queryset_with_nullable_foreign_key(\n    self,", "def test_prefetch_related_get_prefetch_queryset_uses_provided_queryset(self):\n    # Ensure get_prefetch_queryset() uses the provided queryset when passed.\n    with CaptureQueriesContext(connection) as captured_queries:\n        qs = Book.objects.prefetch_related(Prefetch(\"authors\", queryset=Author.objects.filter(name=\"Charlotte\")))\n        list(qs)\n        queries = [q[\"sql\"] for q in captured_queries.captured_queries]\n        self.assertEqual(len(queries), 2)\n        # Check the IN clause of the query to ensure it's using the filter.\n        self.assertWhereContains(queries[1], \"%s IN ('Charlotte')\")\n", "def test_prefetch_related_through_model(self):\n    with CaptureQueriesContext(connection) as captured_queries:\n        authors = Author.objects.prefetch_related(\"book_set__read_by\").all()\n        for author in authors:\n            list(author.book_set.all())\n    self.assertEqual(len(captured_queries), 3)\n    self.assertWhereContains(captured_queries[1][\"sql\"], \"author_id IN\")\n    self.assertWhereContains(captured_queries[2][\"sql\"], \"book_id IN\")\n", "def test_forward_many_to_one_descriptor_get(self):\n    with self.assertNumQueries(1):\n        author = self.author1.first_book.authors.get(name=\"Charlotte\")\n    self.assertEqual(author, self.author1)\n    # The cached related object is used.\n    with self.assertNumQueries(0):\n        self.assertEqual(self.author1.first_book.authors.get(name=\"Charlotte\"), author)\n\n    # Accessing a different author hits the database again.\n    with self.assertNumQueries(1):\n        self.assertEqual(self.author2.first_book.authors.get(name=\"Anne\"), self.author2)\n", "def test_prefetch_many_to_many(self):\n    with self.assertNumQueries(2):\n        authors = Author.objects.prefetch_related(\"books\").all()\n        books = [list(author.books.all()) for author in authors]\n\n    # Check that the prefetched results are correct.\n    self.assertEqual(\n        [list(author.books.values_list(\"pk\", flat=True)) for author in authors],\n        [\n            [self.book1.pk, self.book2.pk],\n            [self.book1.pk],\n            [self.book1.pk, self.book3.pk],\n            [self.book4.pk],\n        ],\n    )\n\n    # Check that the results didn't change after accessing them again.\n    self.assertEqual(\n        [list(author.books.values_list(\"pk\", flat=True)) for author in authors],\n        [\n            [self.book1.pk, self.book2.pk],\n            [self.book1.pk],\n            [self.book1.pk, self.book3.pk],\n            [self.book4.pk],\n        ],\n    )\n"], "sample_425": ["    def test_uuid_serialization(self):\n        value = uuid.uuid4()\n        serializer = BaseSerializer(value)\n        serialized_value, imports = serializer_factory(value).serialize()\n        self.assertEqual(serialized_value, \"uuid.%s\" % repr(value))\n        self.assertEqual(imports, {\"import uuid\"})\n", "    def test_serializer_factory_with_deconstructable_instance(self):\n        instance = DeconstructibleInstances()\n        serializer = serializer_factory(instance)\n        self.assertIsInstance(serializer, DeconstructableSerializer)\n", "def test_serialize_float(self):\n    serializer = FloatSerializer(1.0)\n    self.assertEqual(serializer.serialize(), (\"1.0\", set()))\n\n    serializer = FloatSerializer(float(\"inf\"))\n    self.assertEqual(serializer.serialize(), ('float(\"inf\")', set()))\n\n    serializer = FloatSerializer(float(\"-inf\"))\n    self.assertEqual(serializer.serialize(), ('float(\"-inf\")', set()))\n\n    serializer = FloatSerializer(float(\"nan\"))\n    self.assertEqual(serializer.serialize(), ('float(\"nan\")', set()))\n", "def test_serialize_lazy_object(self):\n    lazy_obj = SimpleLazyObject(lambda: \"hello\")\n    serializer = serializer_factory(lazy_obj)\n    self.assertEqual(serializer.serialize(), (\"'hello'\", set()))\n", "    def test_serialize_float(self):\n        value = float(\"nan\")\n        serializer = FloatSerializer(value)\n        result, imports = serializer.serialize()\n        self.assertEqual(result, \"float('nan')\")\n        self.assertEqual(imports, set())\n"], "sample_426": ["def test_timesince_depth(self):\n    with translation.override('en'):\n        self.assertEqual(timesince(self.t, depth=1), '11 years')\n        self.assertEqual(timesince(self.t, depth=2), '11 years, 5 months')\n        self.assertEqual(timesince(self.t + self.oneweek, depth=1), '1 week')\n        self.assertEqual(timesince(self.t + self.oneweek, depth=2), '1 week, 0 days')\n", "def test_timesince_depth(self):\n    with translation.override('en'):\n        self.assertEqual(timesince(self.t - self.oneweek, self.t, depth=1), '1 week')\n        self.assertEqual(timesince(self.t - self.oneweek - self.oneday, self.t, depth=2), '1 week, 1 day')\n        self.assertEqual(timesince(self.t - self.oneweek - self.oneday - self.onehour, self.t, depth=3), '1 week, 1 day, 1 hour')\n", "def test_timesince_depth(self):\n    with translation.override('en'):\n        self.assertEqual(timesince(self.t - self.oneday, depth=1), '1 day')\n        self.assertEqual(timesince(self.t - self.oneday - self.onehour, depth=2), '1 day, 1 hour')\n        self.assertEqual(timesince(self.t - self.oneweek - self.oneday - self.onehour, depth=3), '1 week, 1 day, 1 hour')\n", "def test_timesince_years_and_months(self):\n    t1 = datetime.datetime(2013, 2, 10)\n    t2 = datetime.datetime(2014, 3, 10)\n    self.assertEqual(timesince(t1, t2), '1 year, 1 month')\n\n    t1 = datetime.datetime(2007, 8, 10)\n    t2 = datetime.datetime(2008, 9, 10)\n    self.assertEqual(timesince(t1, t2), '1 year, 1 month')\n", "def test_timesince_depth(self):\n    with translation.override('en'):\n        self.assertEqual(timesince(self.t, depth=1), '11 years')\n        self.assertEqual(timesince(self.t - self.oneweek, self.t, depth=2), '1 week, 0 days')\n        self.assertEqual(timesince(self.t - self.onemonth, self.t, depth=3), '1 month, 0 weeks, 0 days')\n        self.assertEqual(timesince(self.t - self.oneyear, self.t, depth=4), '1 year, 0 months, 0 weeks, 0 days')\n"], "sample_427": ["def test_management_form_validates_non_numeric_input(self):\n    management_form = ManagementForm(\n        {\n            TOTAL_FORM_COUNT: \"abc\",\n            INITIAL_FORM_COUNT: \"def\",\n            MAX_NUM_FORM_COUNT: \"ghi\",\n            MIN_NUM_FORM_COUNT: \"jkl\",\n        }\n    )\n    self.assertFalse(management_form.is_valid())\n    self.assertEqual(len(management_form.errors), 4)\n    for field_name in (\n        TOTAL_FORM_COUNT,\n        INITIAL_FORM_COUNT,\n        MAX_NUM_FORM_COUNT,\n        MIN_NUM_FORM_COUNT,\n    ):\n        self.assertIn(field_name, management_form.errors)\n        self.assertEqual(\n            management_form.errors[field_name],\n            [\"Enter a whole number.\"],\n        )\n", "def test_formset_non_form_error(self):\n    \"\"\"Test non-form error in formset.\"\"\"\n    data = {\n        \"choices-TOTAL_FORMS\": \"1\",\n        \"choices-INITIAL_FORMS\": \"0\",\n        \"choices-MIN_NUM_FORMS\": \"0\",\n        \"choices-MAX_NUM_FORMS\": \"0\",\n        \"choices-0-choice\": \"Test\",\n        \"choices-0-votes\": \"1\",\n    }\n    formset = ChoiceFormsetWithNonFormError(data)\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(len(formset.non_form_errors()), 1)\n    self.assertEqual(formset.non_form_errors()[0], \"non-form error\")\n", "def test_management_form_with_empty_data(self):\n    management_form = ManagementForm(\n        data={\n            TOTAL_FORM_COUNT: \"\",\n            INITIAL_FORM_COUNT: \"\",\n            MIN_NUM_FORM_COUNT: \"\",\n            MAX_NUM_FORM_COUNT: \"\",\n        },\n        prefix=\"test\",\n    )\n    self.assertFalse(management_form.is_valid())\n    self.assertEqual(management_form.errors, {\n        TOTAL_FORM_COUNT: [\"This field is required.\"],\n        INITIAL_FORM_COUNT: [\"This field is required.\"],\n    })\n", "def test_formset_non_form_error(self):\n    formset = ChoiceFormsetWithNonFormError(\n        data={\n            \"choices-TOTAL_FORMS\": \"1\",\n            \"choices-INITIAL_FORMS\": \"0\",\n            \"choices-MAX_NUM_FORMS\": \"\",\n            \"choices-MIN_NUM_FORMS\": \"\",\n            \"choices-0-choice\": \"Foo\",\n            \"choices-0-votes\": \"1\",\n        }\n    )\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(len(formset.non_form_errors()), 1)\n    self.assertEqual(formset.non_form_errors()[0], \"non-form error\")\n", "def test_baseformset_add_fields(self):\n    class TestForm(Form):\n        field1 = CharField()\n\n    formset = BaseFormSet(form=TestForm, data=None)\n    form = formset.empty_form\n\n    self.assertEqual(len(form.fields), 2)  # field1 and ORDERING_FIELD_NAME\n\n    formset = BaseFormSet(form=TestForm, data=None, can_delete=True)\n    form = formset.empty_form\n\n    self.assertEqual(len(form.fields), 3)  # field1, ORDERING_FIELD_NAME and DELETION_FIELD_NAME\n"], "sample_428": ["def test_decimal_pos_truncation(self):\n    num = Decimal('123.456')\n    formatted_num = nformat(num, decimal_sep='.', decimal_pos=2)\n    self.assertEqual(formatted_num, '123.46')\n\n    num = Decimal('123.456789')\n    formatted_num = nformat(num, decimal_sep='.', decimal_pos=4)\n    self.assertEqual(formatted_num, '123.4568')\n", "def test_decimal_pos(self):\n    self.assertEqual(nformat(12345.6789, decimal_sep='.', decimal_pos=2), '12345.68')\n    self.assertEqual(nformat(12345.6789, decimal_sep=',', decimal_pos=3), '12345,679')\n    self.assertEqual(nformat(12345, decimal_sep='.', decimal_pos=2), '12345.00')\n    self.assertEqual(nformat(Decimal('12345.6789'), decimal_sep='.', decimal_pos=2), '12345.68')\n", "def test_decimal_pos_rounding(self):\n    self.assertEqual(nformat(1.23456789, \".\", 4), '1.2346')\n    self.assertEqual(nformat(12345.6789, \".\", 2), '12345.68')\n    self.assertEqual(nformat(Decimal('12345.6789'), \".\", 2), '12345.68')\n    self.assertEqual(nformat(12345.6789, \".\", 0), '12346')\n", "def test_decimal_pos_rounding(self):\n    self.assertEqual(nformat(123.456, \".\", 2), \"123.46\")\n    self.assertEqual(nformat(123.456, \".\", 0), \"123\")\n    self.assertEqual(nformat(123.456, \".\", 4), \"123.4560\")\n    self.assertEqual(nformat(Decimal(\"123.456\"), \".\", 2), \"123.46\")\n    self.assertEqual(nformat(Decimal(\"123.456\"), \".\", 0), \"123\")\n    self.assertEqual(nformat(Decimal(\"123.456\"), \".\", 4), \"123.4560\")\n", "def test_decimal_pos(self):\n    self.assertEqual(nformat(123.456, \".\", 2), \"123.46\")\n    self.assertEqual(nformat(123.456, \".\", 4), \"123.4560\")\n    self.assertEqual(nformat(123.456, \".\", 0), \"123\")\n    self.assertEqual(nformat(Decimal('123.456'), \".\", 2), \"123.46\")\n    self.assertEqual(nformat(Decimal('123.456'), \".\", 4), \"123.4560\")\n    self.assertEqual(nformat(Decimal('123.456'), \".\", 0), \"123\")\n"], "sample_429": ["    def test_prohibit_null_characters_validator(self):\n        validator = ProhibitNullCharactersValidator()\n        with self.assertRaisesMessage(\n            ValidationError, \"Null characters are not allowed.\"\n        ):\n            validator(\"\\x00something\")\n        self.assertIsNone(validator(\"something\"))\n        self.assertIsNone(validator(None))\n        self.assertIsNone(validator(\"\"))\n        self.assertIsNone(validator(\"null\"))\n", "def test_prohibit_null_characters_validator(self):\n    validator = ProhibitNullCharactersValidator()\n    with self.assertRaisesMessage(\n        ValidationError, validator.message\n    ):\n        validator(\"\\x00something\")\n    # Test that the error message can be overridden.\n    validator = ProhibitNullCharactersValidator(message=\"Custom error message.\")\n    with self.assertRaisesMessage(\n        ValidationError, \"Custom error message.\"\n    ):\n        validator(\"\\x00something\")\n    # Test that the code can be overridden.\n    validator = ProhibitNullCharactersValidator(code=\"custom_code\")\n    with self.assertRaises(ValidationError) as cm:\n        validator(\"\\x00something\")\n    self.assertEqual(cm.exception.code, \"custom_code\")\n    # Test that the error message can be overridden using the `__init__` method.\n    class CustomProhibitNullCharactersValidator(ProhibitNullCharactersValidator):\n        message = \"Custom error message from __init__.\"\n    validator = CustomProhibitNullCharactersValidator()\n    with self.assertRaisesMessage(\n        ValidationError, \"Custom error message from __init__.\"\n    ):\n        validator(\"\\x00something\")\n", "def test_prohibit_null_characters_validator_with_non_string_input(self):\n    # Test ProhibitNullCharactersValidator with non-string input\n    validator = ProhibitNullCharactersValidator()\n    self.assertIsNone(validator(123))\n    self.assertIsNone(validator(None))\n    self.assertIsNone(validator([1, 2, 3]))\n    self.assertIsNone(validator({'a': 1}))\n", "def test_regex_validator_flags(self):\n    # Test that RegexValidator's flags parameter is applied correctly.\n    # We'll use the IGNORECASE flag for this test.\n    validator = RegexValidator(\"a\", flags=re.IGNORECASE)\n    self.assertIsNone(validator(\"A\"))\n    self.assertIsNone(validator(\"a\"))\n\n    validator = RegexValidator(\"a\")\n    with self.assertRaisesMessage(ValidationError, \"Enter a valid value.\"):\n        validator(\"A\")\n", "    def test_regex_validator_with_custom_message(self):\n        message = \"Custom error message\"\n        validator = RegexValidator(regex=\"^[a-z]+$\", message=message)\n        with self.assertRaisesMessage(ValidationError, message):\n            validator(\"Invalid\")\n"], "sample_431": ["    def test_model_validation(self):\n        # Create a model with a CharField that has max_length=10\n        class TestModel(models.Model):\n            char_field = models.CharField(max_length=10)\n\n        # Try to create an instance of the model with a value that exceeds the max_length\n        try:\n            TestModel.objects.create(char_field='a' * 11)\n            self.fail(\"Expected ValidationError\")\n        except models.ValidationError as e:\n            self.assertEqual(e.code, 'max_length')\n            self.assertEqual(e.message, 'Ensure this value has at most 10 characters (it has 11).')\n\n        # Try to create an instance of the model with a valid value\n        TestModel.objects.create(char_field='a' * 10)\n\n        # Try to update an instance of the model with a value that exceeds the max_length\n        obj = TestModel.objects.get()\n        obj.char_field = 'a' * 11\n        try:\n            obj.full_clean()\n            self.fail(\"Expected ValidationError\")\n        except models.ValidationError as e:\n            self.assertEqual(e.code, 'max_length')\n            self.assertEqual(e.message, 'Ensure this value has at most 10 characters (it has 11).')\n", "    def test_get_deferred_fields(self):\n        model = Article.objects.create(title=\"Test\")\n        self.assertEqual(model.get_deferred_fields(), set())\n\n        model = Article.objects.defer(\"title\").get(pk=model.pk)\n        self.assertEqual(model.get_deferred_fields(), {\"title\"})\n", "    def test_model_state(self):\n        instance = Article()\n        self.assertTrue(instance._state.adding)\n        self.assertIsNone(instance._state.db)\n\n        instance.save()\n        self.assertFalse(instance._state.adding)\n        self.assertEqual(instance._state.db, DEFAULT_DB_ALIAS)\n\n        instance._state.db = None\n        instance.save(using=\"other\")\n        self.assertFalse(instance._state.adding)\n        self.assertEqual(instance._state.db, \"other\")\n\n        instance.refresh_from_db()\n        self.assertFalse(instance._state.adding)\n        self.assertEqual(instance._state.db, \"other\")\n", "    def test_fields_cache(self):\n        instance = Article()\n        self.assertEqual(instance._state.fields_cache, {})\n        instance._state.fields_cache[\"field\"] = \"value\"\n        self.assertEqual(instance._state.fields_cache, {\"field\": \"value\"})\n", "def test_model_instance_state(self):\n    article = Article()\n    self.assertTrue(article._state.adding)\n    self.assertIsNone(article._state.db)\n\n    article.save()\n    self.assertFalse(article._state.adding)\n    self.assertEqual(article._state.db, DEFAULT_DB_ALIAS)\n"], "sample_432": ["def test_get_deleted_objects(self):\n    band = Band.objects.create(name=\"Test Band\")\n    concert1 = Concert.objects.create(band=band, name=\"Concert 1\")\n    concert2 = Concert.objects.create(band=band, name=\"Concert 2\")\n\n    deleted_objects, model_count, perms_needed, protected = (\n        BandAdmin(Band, admin.site).get_deleted_objects([band], self._mocked_authenticated_request(\"/admin/band/\", self.superuser))\n    )\n\n    self.assertEqual(model_count[\"admin_changelist.Band\"], 1)\n    self.assertEqual(model_count[\"admin_changelist.Concert\"], 2)\n    self.assertIn(band, deleted_objects)\n    self.assertIn(concert1, deleted_objects)\n    self.assertIn(concert2, deleted_objects)\n", "def test_model_admin_get_inline_formsets_with_view_permission(self):\n    model_admin = ChildAdmin(Child, admin.site)\n    request = self._mocked_authenticated_request(\"/\", self.superuser)\n    obj = None\n\n    with mock.patch.object(model_admin, \"has_view_permission\", return_value=True):\n        inline_formsets = model_admin.get_inline_formsets(\n            request, formsets=[], inline_instances=[], obj=obj\n        )\n\n    for inline_formset in inline_formsets:\n        self.assertTrue(inline_formset.has_view_permission)\n", "def test_changelist_view_pagination(self):\n    model_admin = BandAdmin(Band, custom_site)\n    request = self._mocked_authenticated_request(\"/band/\", self.superuser)\n    model_admin.get_queryset = mock.Mock(return_value=Band.objects.all())\n\n    # Test that pagination works correctly on a changelist page with no GET parameters\n    response = model_admin.changelist_view(request)\n    self.assertEqual(response.status_code, 200)\n\n    # Test that pagination works correctly on a changelist page with GET parameters\n    request = self._mocked_authenticated_request(\"/band/?page=1\", self.superuser)\n    response = model_admin.changelist_view(request)\n    self.assertEqual(response.status_code, 200)\n", "def test_changelist_view_search(self):\n    model_admin = BandAdmin(Band, custom_site)\n    request = self._mocked_authenticated_request(\"/bands/\", self.superuser)\n    request.GET = {\"q\": \"some_band\"}\n    with mock.patch.object(model_admin, \"changelist_view\") as mock_changelist_view:\n        response = model_admin.changelist_view(request)\n        mock_changelist_view.assert_called_once()\n        self.assertEqual(response.status_code, 200)\n", "def test_get_queryset(self):\n    model_admin = ModelAdmin(Band, admin.site)\n    request = self._mocked_authenticated_request(\"/admin/band/\", self.superuser)\n\n    # Test that get_queryset returns all objects by default.\n    queryset = model_admin.get_queryset(request)\n    self.assertEqual(list(queryset), list(Band.objects.all()))\n\n    # Test that get_queryset can be overridden to return a filtered queryset.\n    class FilteredBandAdmin(ModelAdmin):\n            return super().get_queryset(request).filter(name__startswith=\"T\")\n\n    model_admin = FilteredBandAdmin(Band, admin.site)\n    queryset = model_admin.get_queryset(request)\n    self.assertEqual(list(queryset), list(Band.objects.filter(name__startswith=\"T\")))\n"], "sample_433": ["    def test_migration_name_fragment(self):\n        # Create a migration with multiple operations.\n        migration = Migration(\"testapp\", \"0001_initial\")\n        migration.operations = [\n            migrations.CreateModel(\n                name=\"Author\",\n                fields=[\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"name\", models.CharField(max_length=200)),\n                ],\n            ),\n            migrations.CreateModel(\n                name=\"Book\",\n                fields=[\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"title\", models.CharField(max_length=200)),\n                ],\n            ),\n        ]\n\n        # Suggest a name for the migration based on its operations.\n        suggested_name = migration.suggest_name()\n\n        # Assert that the suggested name includes fragments from all operations.\n        self.assertIn(\"author\", suggested_name.lower())\n        self.assertIn(\"book\", suggested_name.lower())\n", "def test_suggest_name_with_long_fragment(self):\n    migration = Migration(\"test\", \"testapp\")\n    migration.operations = [\n        migrations.CreateModel(\n            name=\"VeryLongModelNameThatWillBeTruncated\",\n            fields=[(\"id\", models.AutoField(primary_key=True))],\n        )\n    ]\n    suggested_name = migration.suggest_name()\n    self.assertLessEqual(len(suggested_name), 52)\n    self.assertEqual(suggested_name, \"create_verylongmodelnamethatwil_and_more\")\n", "def test_migration_suggest_name(self):\n    migration = Migration(\"test\", \"app_label\")\n    migration.operations = [\n        migrations.CreateModel(\n            name=\"Author\",\n            fields=[(\"id\", models.AutoField(primary_key=True))],\n        )\n    ]\n    self.assertEqual(migration.suggest_name(), \"create_author\")\n\n    migration.operations = [\n        migrations.AddField(model_name=\"author\", name=\"name\", field=models.CharField(max_length=200)),\n        migrations.RemoveField(model_name=\"author\", name=\"old_field\"),\n    ]\n    self.assertRegex(migration.suggest_name(), r\"^auto_\\d{14}$\")\n\n    migration.operations = [\n        migrations.RenameField(model_name=\"author\", old_name=\"old_field\", new_name=\"new_field\")\n    ]\n    self.assertEqual(migration.suggest_name(), \"rename_old_field_to_new_field\")\n\n    migration.operations = [\n        migrations.AlterField(model_name=\"author\", name=\"field\", field=models.CharField(max_length=200))\n    ]\n    self.assertEqual(migration.suggest_name(), \"alter_field\")\n", "def test_migration_suggest_name(self):\n    migration = Migration(\"testapp\", \"0001_initial\")\n    migration.operations = [\n        migrations.CreateModel(\n            name=\"Author\",\n            fields=[\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n            ],\n        ),\n        migrations.CreateModel(\n            name=\"Book\",\n            fields=[\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"author\", models.ForeignKey(\"testapp.Author\", models.CASCADE)),\n                (\"title\", models.CharField(max_length=200)),\n            ],\n        ),\n    ]\n    self.assertEqual(migration.suggest_name(), \"initial\")\n\n    migration = Migration(\"testapp\", \"0002_auto\")\n    migration.operations = [\n        migrations.AddField(\n            model_name=\"author\",\n            name=\"biography\",\n            field=models.TextField(),\n        ),\n    ]\n    self.assertRegex(migration.suggest_name(), r\"^auto_\\d{14}$\")\n\n    migration = Migration(\"testapp\", \"0003_auto\")\n    migration.operations = [\n        migrations.RenameField(\n            model_name=\"author\",\n            old_name=\"biography\",\n            new_name=\"description\",\n        ),\n    ]\n    self.assertRegex(migration.suggest_name(), r\"^auto_\\d{14}$\")\n", "def test_suggest_name(self):\n    migration = Migration(\"testapp\", \"0001_initial\")\n    migration.operations = [\n        migrations.CreateModel(\n            name=\"Author\",\n            fields=[\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n            ],\n        ),\n        migrations.CreateModel(\n            name=\"Book\",\n            fields=[\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"author\", models.ForeignKey(\"testapp.Author\", models.CASCADE)),\n                (\"title\", models.CharField(max_length=200)),\n            ],\n        ),\n    ]\n    self.assertEqual(migration.suggest_name(), \"initial\")\n\n    migration = Migration(\"testapp\", \"0002_auto_<timestamp>\")\n    migration.operations = [\n        migrations.AddField(\n            model_name=\"author\",\n            name=\"biography\",\n            field=models.TextField(),\n        ),\n    ]\n    self.assertRegex(migration.suggest_name(), r\"^auto_\\d+\")\n"], "sample_434": ["    def test_view_is_async(self):\n        class AsyncView(View):\n            async def get(self, request):\n                return HttpResponse(\"Async\")\n\n        class SyncView(View):\n                return HttpResponse(\"Sync\")\n\n        self.assertTrue(AsyncView.view_is_async)\n        self.assertFalse(SyncView.view_is_async)\n", "    def test_view_is_async(self):\n        class AsyncView(View):\n            async def get(self, request):\n                pass\n\n        class SyncView(View):\n                pass\n\n        self.assertTrue(AsyncView.view_is_async)\n        self.assertFalse(SyncView.view_is_async)\n", "    def test_view_dispatch(self):\n        class TestView(View):\n                return HttpResponse(\"Hello, World!\")\n\n        view = TestView.as_view()\n        request = RequestFactory().get(\"/\")\n        response = view(request)\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.content, b\"Hello, World!\")\n", "    def test_view_is_async(self):\n        class AsyncView(View):\n            async def get(self, request):\n                return HttpResponse(\"Hello\")\n\n        self.assertTrue(AsyncView.view_is_async)\n\n        class SyncView(View):\n                return HttpResponse(\"Hello\")\n\n        self.assertFalse(SyncView.view_is_async)\n\n        class MixedView(View):\n            async def get(self, request):\n                return HttpResponse(\"Hello\")\n\n                return HttpResponse(\"Hello\")\n\n        with self.assertRaises(ImproperlyConfigured):\n            MixedView.view_is_async\n", "    def test_view_dispatch(self):\n        class TestView(View):\n                return HttpResponse(\"Hello, world!\")\n\n        view = TestView()\n        request = RequestFactory().get(\"/test/\")\n        response = view.dispatch(request)\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.content, b\"Hello, world!\")\n"], "sample_435": ["    def test_init(self):\n        form = UserChangeForm(self.u1)\n        self.assertEqual(form.fields[\"username\"].initial, self.u1.username)\n", "    def test_user_change_form(self):\n        form = UserChangeForm(instance=self.u1)\n        self.assertEqual(form.initial[\"username\"], \"testclient\")\n        self.assertEqual(form.initial[\"email\"], \"testclient@example.com\")\n", "    def test_readonly_password_hash_widget(self):\n        user = User.objects.create_user(username=\"testclient\", password=\"password\")\n        widget = ReadOnlyPasswordHashWidget()\n        value = user.password\n        output = widget.render(\"password\", value)\n        self.assertHTMLEqual(\n            output,\n            '<div class=\"readonly\">{}</div>'.format(\n                _(\"No password set.\") if not value else _(\"Invalid password format or unknown hashing algorithm.\")\n            ),\n        )\n", "    def test_widget(self):\n        field = ReadOnlyPasswordHashField()\n        self.assertIsInstance(field.widget, ReadOnlyPasswordHashWidget)\n", "    def test_widget(self):\n        field = ReadOnlyPasswordHashField()\n        self.assertIsInstance(field.widget, ReadOnlyPasswordHashWidget)\n"], "sample_436": ["    def test_runserver_command_with_empty_allowed_hosts(self):\n        with captured_stderr() as stderr:\n            with self.assertRaises(CommandError) as e:\n                call_command(\"runserver\")\n            self.assertEqual(\n                str(e.exception),\n                \"You must set settings.ALLOWED_HOSTS if DEBUG is False.\",\n            )\n", "    def test_naiveip_re(self):\n        valid_ips = [\n            \"127.0.0.1:8000\",\n            \"[::1]:8000\",\n            \"localhost:8000\",\n        ]\n        for ip in valid_ips:\n            self.assertTrue(RunserverCommand.naiveip_re.match(ip))\n\n        invalid_ips = [\n            \"256.1.1.1:8000\",  # Invalid IPv4 address\n            \"example.com:8000:8000\",  # Invalid port number\n            \"2001:0db8:85a3:0000:0000:8a2e:0370:7334\",  # Invalid IPv6 address (missing brackets)\n        ]\n        for ip in invalid_ips:\n            self.assertIsNone(RunserverCommand.naiveip_re.match(ip))\n", "    def test_runserver_command_with_ipv6_address(self):\n        cmd = RunserverCommand()\n        options = {\n            \"addrport\": \"::1:8000\",\n            \"use_ipv6\": True,\n            \"use_reloader\": False,\n            \"use_threading\": True,\n            \"skip_checks\": True,\n        }\n        with captured_stdout() as stdout, captured_stderr() as stderr:\n            cmd.handle(*[], **options)\n        self.assertIn(\"Starting development server at http://[::1]:8000/\", stdout.getvalue())\n        self.assertEqual(stderr.getvalue(), \"\")\n", "    def test_runserver_ipv6_address(self):\n        # Ensure the correct address is passed to WSGIServer when using IPv6\n        with mock.patch(\"django.core.management.commands.runserver.WSGIServer\") as mock_server:\n            call_command(\"runserver\", addrport=\"::1:8000\")\n            mock_server.assert_called_once_with(\n                '::1', 8000, mock.ANY, ipv6=True, threading=True\n            )\n", "    def test_runserver_handles_import_error(self, mock_inner_run):\n        mock_inner_run.side_effect = ImportError(\"Test import error\")\n        with captured_stderr() as stderr:\n            call_command('runserver', '--skip-checks')\n        self.assertIn(\"Error:\", stderr.getvalue())\n"], "sample_437": ["def test_database_wrapper_validate_thread_sharing(self):\n    db = BaseDatabaseWrapper({}, alias=DEFAULT_DB_ALIAS)\n    db.inc_thread_sharing()\n    self.assertTrue(db.allow_thread_sharing)\n\n    db.dec_thread_sharing()\n    self.assertFalse(db.allow_thread_sharing)\n\n    with self.assertRaises(RuntimeError):\n        db.dec_thread_sharing()\n\n    db.inc_thread_sharing()\n    db._thread_ident = \"fake-thread-id\"\n    with self.assertRaises(DatabaseError):\n        db.validate_thread_sharing()\n", "def test_database_wrapper_run_on_commit(self):\n    wrapper = BaseDatabaseWrapper({}, DEFAULT_DB_ALIAS)\n    mock_func = MagicMock()\n    wrapper.on_commit(mock_func, robust=True)\n\n    wrapper.run_and_clear_commit_hooks()\n\n    mock_func.assert_called_once()\n", "def test_database_wrapper_init(self):\n    settings_dict = {\n        \"NAME\": \"test\",\n        \"USER\": \"test\",\n        \"PASSWORD\": \"test\",\n        \"HOST\": \"test\",\n        \"PORT\": \"test\",\n        \"TIME_ZONE\": None,\n    }\n    wrapper = BaseDatabaseWrapper(settings_dict, DEFAULT_DB_ALIAS)\n    self.assertEqual(wrapper.alias, DEFAULT_DB_ALIAS)\n    self.assertEqual(wrapper.settings_dict, settings_dict)\n    self.assertFalse(wrapper.queries_logged)\n    self.assertIsNone(wrapper.connection)\n    self.assertTrue(wrapper.autocommit)\n    self.assertEqual(wrapper.savepoint_state, 0)\n    self.assertEqual(wrapper.run_on_commit, [])\n", "def test_run_on_commit(self):\n    run_on_commit = MagicMock()\n    connection.on_commit(run_on_commit)\n    connection.run_and_clear_commit_hooks()\n    run_on_commit.assert_called_once_with()\n", "    def test_init_connection_params(self):\n        settings_dict = {\n            \"ENGINE\": \"django.db.backends.dummy\",\n            \"NAME\": \"test_name\",\n            \"USER\": \"test_user\",\n            \"PASSWORD\": \"test_password\",\n            \"HOST\": \"test_host\",\n            \"PORT\": \"test_port\",\n        }\n        wrapper = BaseDatabaseWrapper(settings_dict, alias=DEFAULT_DB_ALIAS)\n        self.assertEqual(wrapper.settings_dict, settings_dict)\n        self.assertEqual(wrapper.alias, DEFAULT_DB_ALIAS)\n        self.assertIsNone(wrapper.connection)\n        self.assertFalse(wrapper.autocommit)\n        self.assertFalse(wrapper.in_atomic_block)\n"], "sample_438": ["def test_model_state_fields_cache_descriptor(self):\n    post = Post()\n    self.assertEqual(post._state.fields_cache, {})\n    post._state.fields_cache[\"test\"] = \"value\"\n    self.assertEqual(post._state.fields_cache, {\"test\": \"value\"})\n", "    def test_deferred_fields(self):\n        class MyModel(models.Model):\n            field1 = models.IntegerField()\n            field2 = models.IntegerField()\n\n        obj = MyModel(field1=1)\n        self.assertEqual(obj.get_deferred_fields(), {\"field2\"})\n", "def test_deferred_model_unpickle(self):\n    model = Question()\n    model.save()\n    model_pickled = json.dumps(model.__reduce__())\n    model_unpickled = json.loads(model_pickled)\n    self.assertEqual(model.__class__, model_unpickled[0][0][1])\n", "def test_model_repr(self):\n    p = Post.objects.create(title=\"Django\", content=\"Django is awesome\")\n    self.assertEqual(repr(p), f\"<Post: {p.title}>\")\n    q = Question.objects.create(question=\"What is Django?\")\n    self.assertEqual(repr(q), f\"<Question: {q.question}>\")\n    a = Answer.objects.create(question=q, answer=\"A web framework.\")\n    self.assertEqual(repr(a), f\"<Answer: {a.answer}>\")\n", "    def test_constructor(self):\n        state = ModelState()\n        self.assertIsNone(state.db)\n        self.assertTrue(state.adding)\n        self.assertEqual(state.fields_cache, {})\n"], "sample_440": ["def test_bulk_create_conflict_update_fields(self):\n    Country.objects.create(name=\"France\", iso_two_letter=\"FR\")\n    countries = [\n        Country(name=\"Germany\", iso_two_letter=\"DE\"),\n        Country(name=\"France\", iso_two_letter=\"EN\"),\n    ]\n    with self.assertRaises(ValueError):\n        Country.objects.bulk_create(countries, update_conflicts=True)\n    with self.assertRaises(ValueError):\n        Country.objects.bulk_create(\n            countries, update_conflicts=True, update_fields=(\"iso_two_letter\",)\n        )\n    Country.objects.bulk_create(\n        countries,\n        update_conflicts=True,\n        update_fields=(\"name\",),\n        unique_fields=(\"iso_two_letter\",),\n    )\n    country = Country.objects.get(iso_two_letter=\"FR\")\n    self.assertEqual(country.name, \"France\")\n    country = Country.objects.get(iso_two_letter=\"DE\")\n    self.assertEqual(country.name, \"Germany\")\n", "def test_bulk_create_empty(self):\n    self.assertEqual(Country.objects.bulk_create([]), [])\n", "def test_bulk_create_conflict_update_fields(self):\n    country = Country.objects.create(name=\"United States of America\", iso_two_letter=\"US\")\n    countries = [\n        Country(name=\"The Netherlands\", iso_two_letter=\"US\"),\n        Country(name=\"Germany\", iso_two_letter=\"DE\"),\n    ]\n    with self.assertRaises(IntegrityError):\n        Country.objects.bulk_create(countries, update_conflicts=True)\n    Country.objects.bulk_create(\n        countries, update_conflicts=True, update_fields=[\"name\"], unique_fields=[\"iso_two_letter\"]\n    )\n    country.refresh_from_db()\n    self.assertEqual(country.name, \"The Netherlands\")\n", "def test_bulk_create_with_conflict_update_fields(self):\n    country = Country.objects.create(name=\"United States of America\", iso_two_letter=\"US\")\n    new_country = Country(name=\"USA\", iso_two_letter=\"US\")\n    try:\n        Country.objects.bulk_create([new_country], update_fields=[\"name\"], update_conflicts=True)\n    except NotSupportedError as e:\n        self.fail(\"Database should support update on conflict, error: {}\".format(e))\n    else:\n        country.refresh_from_db()\n        self.assertEqual(country.name, \"USA\")\n", "def test_bulk_create_large_batch(self):\n    # Create a large number of objects to ensure that the bulk creation is done\n    # in batches (due to SQLite's parameter limit)\n    countries = [\n        Country(name=f\"Country {i}\", iso_two_letter=f\"C{i}\") for i in range(1000)\n    ]\n    Country.objects.bulk_create(countries)\n    self.assertEqual(Country.objects.count(), 1000)\n"], "sample_441": ["    def test_readonlypasswordhashwidget_render(self):\n        widget = ReadOnlyPasswordHashWidget()\n        password_hash = \"pbkdf2_sha256$100000$1234567890abcdefghijklmnopqrstuvwxyz\"\n        rendered_widget = widget.render(\"password\", password_hash)\n        self.assertTemplateUsed(rendered_widget, \"auth/widgets/read_only_password_hash.html\")\n        self.assertContains(rendered_widget, \"algorithm\")\n        self.assertContains(rendered_widget, \"iterations\")\n        self.assertContains(rendered_widget, \"salt\")\n        self.assertContains(rendered_widget, \"hash\")\n", "    def test_bound_field(self):\n        field = ReadOnlyPasswordHashField()\n        bound_field = field.get_bound_field(None, \"password\")\n        self.assertEqual(bound_field.as_widget(), '<div class=\"readonly\">*******</div>')\n", "    def test_readonlypasswordhashfield_widget_attrs(self):\n        field = ReadOnlyPasswordHashField()\n        widget = field.widget\n        self.assertIsInstance(widget, ReadOnlyPasswordHashWidget)\n        attrs = widget.get_context(name=\"password\", value=\"test\", attrs={})[\"widget\"][\"attrs\"]\n        self.assertEqual(attrs.get(\"readonly\"), \"readonly\")\n", "    def test_readonlypasswordhashfield_widget(self):\n        field = ReadOnlyPasswordHashField()\n        self.assertIsInstance(field.widget, ReadOnlyPasswordHashWidget)\n        self.assertTrue(field.disabled)\n", "    def test_init(self):\n        form = UserChangeForm(self.u1)\n        self.assertEqual(form.fields[\"username\"].initial, \"testclient\")\n        self.assertEqual(form.fields[\"email\"].initial, \"testclient@example.com\")\n"], "sample_442": ["def test_signer_unsign_object_with_compression(self):\n    signer = signing.Signer()\n    obj = {'foo': 'bar'}\n    signed_obj = signer.sign_object(obj, compress=True)\n    self.assertTrue(signed_obj.startswith('.'))\n    unsigned_obj = signer.unsign_object(signed_obj)\n    self.assertEqual(unsigned_obj, obj)\n", "def test_unsign_object_with_bad_signature(self):\n    signer = signing.Signer()\n    signed_obj = signer.sign_object({\"foo\": \"bar\"})\n    # Tamper with the signature\n    signed_obj = signed_obj[:-1] + \"X\" if signed_obj[-1] != \"X\" else signed_obj[:-1] + \"Y\"\n    with self.assertRaises(signing.BadSignature):\n        signer.unsign_object(signed_obj)\n", "def test_signature(self):\n    signer = signing.Signer()\n    value = \"hello\"\n    signature = signer.signature(value)\n    self.assertNotEqual(signature, signer.signature(\"goodbye\"))\n    with override_settings(USE_TZ=True):\n        self.assertEqual(signer.signature(value), signature)\n", "def test_signer_SEP_UNSAFE(self):\n    with self.assertRaises(ValueError):\n        signing.Signer(sep='unsafe')\n    with self.assertRaises(ValueError):\n        signing.Signer(sep='')\n    with self.assertRaises(ValueError):\n        signing.Signer(sep='A-z0-9-_=')\n", "def test_unsign_object_with_compression(self):\n    data = {'foo': 'bar'}\n    signer = signing.TimestampSigner()\n    signed_obj = signer.sign_object(data, compress=True)\n    self.assertEqual(data, signer.unsign_object(signed_obj))\n"], "sample_443": ["    def test_filebased_cache(self):\n        cache = caches[\"default\"]\n        cache.set(\"key1\", \"value1\")\n        self.assertEqual(cache.get(\"key1\"), \"value1\")\n", "    def test_filebased_cache(self):\n        cache = caches[\"default\"]\n        cache_key = \"test_key\"\n        cache_value = \"test_value\"\n\n        # Set a value in the cache\n        cache.set(cache_key, cache_value)\n\n        # Check that the value is stored in a file\n        cache_dir = cache._dir\n        cache_files = os.listdir(cache_dir)\n        self.assertEqual(len(cache_files), 1)\n\n        # Get the value from the cache\n        cached_value = cache.get(cache_key)\n        self.assertEqual(cached_value, cache_value)\n\n        # Delete the value from the cache\n        cache.delete(cache_key)\n\n        # Check that the file has been deleted\n        cache_files = os.listdir(cache_dir)\n        self.assertEqual(len(cache_files), 0)\n", "    def test_file_based_cache_write_read(self):\n        cache = caches[\"default\"]\n        key = \"test-key\"\n        value = \"Hello, World!\"\n        cache.set(key, value)\n\n        # Check that the file has been created\n        cache_dir = cache._dir\n        cache_files = os.listdir(cache_dir)\n        self.assertEqual(len(cache_files), 1)\n\n        # Check that the value can be read from the cache\n        cached_value = cache.get(key)\n        self.assertEqual(cached_value, value)\n", "    def test_filebased_cache_write_read(self):\n        cache_key = \"test_key\"\n        cache_value = \"Hello, World!\"\n        caches[\"default\"].set(cache_key, cache_value)\n        self.assertEqual(caches[\"default\"].get(cache_key), cache_value)\n", "    def setUp(self):\n        self.cache_dir = tempfile.mkdtemp()\n        self.cache = FileBasedCache(self.cache_dir, {})\n"], "sample_444": ["    def test_url(self):\n        rel_path = \"test.txt\"\n        storage.staticfiles_storage.save(rel_path, StringIO(\"Test file\"))\n        actual_url = storage.staticfiles_storage.url(rel_path)\n        expected_url = f\"{settings.STATIC_URL}{rel_path}\"\n        self.assertEqual(actual_url, expected_url)\n", "    def test_post_process(self):\n        \"\"\"\n        Test post processing of files.\n        \"\"\"\n        self.addCleanup(storage.staticfiles_storage.hashed_files.clear)\n        hashed_file_path = self.hashed_file_path\n\n        # Make sure that modification times are different for all files to avoid\n        # overwriting files that were just processed in the same second.\n        import time\n\n        original_mtime = int(time.time()) - 10\n\n        # Add a file that will be post-processed.\n        original_name = \"test.txt\"\n        original_path = os.path.join(settings.STATIC_ROOT, original_name)\n        with open(original_path, \"w\") as f:\n            f.write(\"Hello, world!\")\n        os.utime(original_path, (original_mtime, original_mtime))\n\n        storage.staticfiles_storage.post_process(\n            {\n                original_name: (\n                    storage.staticfiles_storage,\n                    original_path,\n                )\n            }\n        )\n\n        # Check the output file.\n        output_path = os.path.join(settings.STATIC_ROOT, hashed_file_path(original_name))\n        self.assertTrue(os.path.exists(output_path))\n        with open(output_path) as f:\n            self.assertEqual(f.read(), \"Hello, world!\")\n\n        # Make sure we can handle multiple identical files with different paths.\n        original_name2 = \"sub/test.txt\"\n        original_path2 = os.path.join(settings.STATIC_ROOT, original_name2)\n        os.makedirs(os.path.dirname(original_path2))\n        with open(original_path2, \"w\") as f:\n            f.write(\"Hello, world!\")\n        os.utime(original_path2, (original_mtime - 1, original_mtime - 1))\n        storage.staticfiles_storage.post_process(\n            {\n                original_name2: (\n                    storage.staticfiles_storage,\n                    original_path2,\n                )\n            }\n        )\n\n        output_path2 = os.path.join(settings.STATIC_ROOT, hashed_file_path(original_name2))\n        self.assertTrue(os.path.exists(output_path2))\n        with open(output_path2) as f:\n            self.assertEqual(f.read(), \"Hello, world!\")\n        self.assertPostCondition()\n", "    def setUp(self):\n        super().setUp()\n        self.storage = storage.HashedFilesMixin()\n", "    def setUp(self):\n        super().setUp()\n        self.storage = storage.HashedFilesMixin()\n", "    def test_post_process(self):\n        \"\"\"\n        Test post_process modifies files as expected.\n        \"\"\"\n        storage.staticfiles_storage = storage.HashedFilesMixin()\n        original_file_path = os.path.join(TEST_ROOT, \"project\", \"static\", \"css\", \"styles.css\")\n        hashed_file_path = self.hashed_file_path(\"css/styles.css\")\n\n        # Collect static files\n        call_command(\"collectstatic\", verbosity=0)\n\n        # Post process should hash the file name and update content\n        storage.staticfiles_storage.post_process({original_file_path: (storage.staticfiles_storage, original_file_path)})\n\n        # Check that the hashed file exists and its content has been updated\n        hashed_file = storage.staticfiles_storage.open(hashed_file_path)\n        self.assertIn(hashed_file_path, hashed_file.read().decode())\n"], "sample_445": ["def test_timesince_year(self):\n    t1 = self.t + self.oneyear\n    self.assertEqual(timesince(self.t, t1), '1 year')\n    with translation.override('en'):\n        t2 = self.t + self.oneyear + self.onemonth\n        self.assertEqual(timesince(self.t, t2), '1 year, 1 month')\n", "def test_timesince_depth(self):\n    with translation.override('en'):\n        self.assertEqual(timesince(self.t, depth=1), '11 years')\n        self.assertEqual(timesince(self.t + self.oneweek, depth=2), '11 years, 1 week')\n        self.assertEqual(timesince(self.t + self.oneweek + self.oneday, depth=3), '11 years, 1 week, 1 day')\n", "def test_timesince_depth(self):\n    with translation.override('en'):\n        self.assertEqual(timesince(self.t, depth=1), '10 years')\n        self.assertEqual(timesince(self.t + self.oneweek, depth=2), '10 years, 1 week')\n        self.assertEqual(timesince(self.t + self.onemonth, depth=3), '10 years, 1 month, 2 weeks')\n", "def test_timesince_depth(self):\n    with translation.override('en'):\n        self.assertEqual(timesince(self.t, depth=1), '10 years')\n        self.assertEqual(timesince(self.t + self.oneyear, depth=2), '1 year, 11 months')\n        self.assertEqual(timesince(self.t + self.oneweek, depth=3), '1 week, 0 days, 0 hours')\n", "def test_timesince_depth(self):\n    with translation.override('en'):\n        self.assertEqual(timesince(self.t - self.oneday, self.t, depth=1), '1 day')\n        self.assertEqual(timesince(self.t - self.oneweek, self.t, depth=1), '1 week')\n        self.assertEqual(timesince(self.t - self.onemonth, self.t, depth=2), '1 month, 1 day')\n        self.assertEqual(timesince(self.t - self.oneyear, self.t, depth=3), '1 year, 1 month, 1 day')\n"], "sample_446": ["    def test_addslashes(self):\n        output = self.engine.render_to_string(\n            \"addslashes01\", {\"a\": \"a\" * 10, \"b\": \"a\" + '\"' + \"a\" + \"'\" + \"a\"}\n        )\n        self.assertEqual(output, \"aaaaaaaaaa a\\\\\\\"a\\\\'a\")\n", "    def test_floatformat_with_argument(self):\n        with translation.override(\"en\"):\n            out = self.engine.render(\n                \"{% autoescape off %}{{ a|floatformat:3 }} {{ b|floatformat:2 }}{% endautoescape %}\",\n                {\"a\": 12345.6789, \"b\": 98765},\n            )\n        self.assertEqual(out, \"12,345.679 98,765.00\")\n", "    def test_floatformat_with_arg(self):\n        with translation.override(\"en\"):\n            with localcontext() as ctx:\n                ctx.rounding = Decimal(\"ROUND_HALF_UP\")\n                rendered = self.engine.render(\n                    \"{% autoescape off %}{{ a|floatformat:3 }} {{ b|floatformat:3 }}\"\n                    \"{% endautoescape %}\",\n                    {\"a\": Decimal(\"34.23234\"), \"b\": Decimal(\"34.26000\")},\n                )\n                self.assertEqual(rendered, \"34.232 34.260\")\n", "    def test_floatformat_with_argument(self):\n        output = self.engine.render_to_string(\n            \"floatformat02\", {\"a\": 34.23234, \"b\": 34.00000}\n        )\n        self.assertEqual(output, \"34.233 34.00\")\n", "    def test_floatformat_with_arg(self):\n        val = Decimal(\"123.4567\")\n        self.assertEqual(floatformat(val, 2), \"123.46\")\n        self.assertEqual(floatformat(val, 3), \"123.457\")\n        self.assertEqual(floatformat(val, 4), \"123.4567\")\n        self.assertEqual(floatformat(val, 0), \"123\")\n"], "sample_447": ["def test_expression_with_duration_field(self):\n    duration = datetime.timedelta(days=1)\n    expr = F(\"value\") + Value(duration)\n    self.assertIsInstance(expr.output_field, DateTimeField)\n\n    duration = datetime.timedelta(hours=1)\n    expr = F(\"value\") - Value(duration)\n    self.assertIsInstance(expr.output_field, DateTimeField)\n\n    duration = datetime.timedelta(minutes=1)\n    expr = Value(duration) + F(\"value\")\n    self.assertIsInstance(expr.output_field, DateTimeField)\n\n    duration = datetime.timedelta(seconds=1)\n    expr = Value(duration) - F(\"value\")\n    self.assertIsInstance(expr.output_field, DateTimeField)\n", "def test_expression_type_casting(self):\n    tests = [\n        (Value(1), IntegerField()),\n        (Value(\"hello\"), CharField()),\n        (Value(True), BooleanField()),\n        (Value(datetime.date.today()), DateTimeField()),\n        (Value(Decimal(\"3.14\")), DecimalField()),\n        (Value(3.14), FloatField()),\n    ]\n    for value, expected_output_field in tests:\n        self.assertEqual(value.output_field.__class__, expected_output_field.__class__)\n", "def test_expression_node_precedence(self):\n    # Create expressions with various precidences and check that they are\n    # correctly ordered in the resulting SQL.\n    F1 = F(\"field1\")\n    F2 = F(\"field2\")\n\n    # AND\n    q1 = Q(field1=True) & Q(field2=False)\n    self.assertEqual(str(q1), \"(AND: ('field1', True), ('field2', False))\")\n\n    # OR\n    q2 = Q(field1=True) | Q(field2=False)\n    self.assertEqual(str(q2), \"(OR: ('field1', True), ('field2', False))\")\n\n    # AND and OR\n    q3 = q1 & q2\n    self.assertEqual(\n        str(q3),\n        \"(AND: (AND: ('field1', True), ('field2', False)), \"\n        \"(OR: ('field1', True), ('field2', False)))\",\n    )\n\n    # AND, OR and addition\n    q4 = q1 & q2 & Q(field3=F1 + F2)\n    self.assertEqual(\n        str(q4),\n        \"(AND: (AND: ('field1', True), ('field2', False)), \"\n        \"(OR: ('field1', True), ('field2', False)), ('field3', F(field1) + F(field2)))\",\n    )\n", "def test_resolve_expression_on_mixed_type_expression(self):\n    mixed_expr = F('pages') + Value(10)\n    resolved_expr = mixed_expr.resolve_expression(Book._meta.get_field('pages').get_queryset())\n    self.assertIsInstance(resolved_expr, CombinedExpression)\n    self.assertEqual(resolved_expr.output_field.__class__, IntegerField)\n", "def test_order_by_length_of_column(self):\n    \"\"\"\n    OrderBy Length() of a column should work correctly.\n    \"\"\"\n    short_title = Book.objects.create(title=\"Short\")\n    long_title = Book.objects.create(title=\"Longer title\")\n\n    books = list(Book.objects.order_by(Length(\"title\")))\n    self.assertEqual([short_title, long_title], books)\n\n    books = list(Book.objects.order_by(Length(\"title\").desc()))\n    self.assertEqual([long_title, short_title], books)\n"], "sample_448": ["    def test_create_check_constraint(self):\n        # Create a model with a CheckConstraint.\n        class Model(models.Model):\n            value = models.IntegerField()\n            check_value = models.CheckConstraint(check=models.Q(value__gt=0), name='check_value')\n\n        # Create the table.\n        with connection.schema_editor() as schema_editor:\n            schema_editor.create_model(Model)\n\n        # Ensure the constraint is created.\n        constraints = get_constraints(Model._meta.db_table)\n        self.assertIn('check_value', [c['name'] for c in constraints])\n\n        # Drop the table.\n        with connection.schema_editor() as schema_editor:\n            schema_editor.delete_model(Model)\n", "    def test_check_constraint(self):\n        class TestModel(models.Model):\n            value = models.IntegerField()\n\n            class Meta:\n                constraints = [\n                    models.CheckConstraint(check=models.Q(value__gt=0), name='value_gt_0'),\n                ]\n\n        with atomic():\n            TestModel.objects.create(value=1)\n            with self.assertRaises(IntegrityError):\n                TestModel.objects.create(value=-1)\n", "    def test_check_constraint(self):\n        class Model(models.Model):\n            value = models.IntegerField()\n\n            class Meta:\n                constraints = [\n                    models.CheckConstraint(check=models.Q(value__gte=0), name='value_gte_0')\n                ]\n\n        with atomic():\n            Model.objects.create(value=1)\n            with self.assertRaises(IntegrityError):\n                Model.objects.create(value=-1)\n", "    def test_create_check_constraint(self):\n        # Create a model with a check constraint.\n        class TestModel(models.Model):\n            value = models.IntegerField()\n            class Meta:\n                constraints = [\n                    models.CheckConstraint(check=models.Q(value__gte=0), name='value_gte_0')\n                ]\n\n        # Create the table.\n        with connection.schema_editor() as schema_editor:\n            schema_editor.create_model(TestModel)\n\n        # Ensure the constraint is created.\n        constraints = get_constraints(TestModel._meta.db_table)\n        self.assertIn('value_gte_0', [c['name'] for c in constraints])\n\n        # Drop the table.\n        with connection.schema_editor() as schema_editor:\n            schema_editor.delete_model(TestModel)\n", "    def test_check_constraint(self):\n        class Model(models.Model):\n            value = models.IntegerField()\n\n            class Meta:\n                constraints = [\n                    models.CheckConstraint(check=models.Q(value__gte=0), name='value_gte_0')\n                ]\n\n        with self.assertRaises(IntegrityError):\n            Model.objects.create(value=-1)\n\n        Model.objects.create(value=0)\n        Model.objects.create(value=1)\n\n        constraint = Model._meta.constraints[0]\n        self.assertEqual(str(constraint), '<CheckConstraint: check=(AND: (\\'value__gte\\', 0)) name=value_gte_0>')\n"], "sample_449": ["def test_get_environ_strip_underscores(self):\n    request = self.request_factory.get('/path', HTTP_HOST='example.com', HTTP_FOO_BAR='baz')\n    handler = WSGIRequestHandler(Stub(), Stub(), Stub())\n    handler.client_address = ('192.168.1.100', 1234)\n    handler.raw_requestline = b'GET /path HTTP/1.1'\n    handler.parse_request()\n    environ = handler.get_environ()\n    self.assertEqual(environ.get('HTTP_FOO_BAR'), None)\n", "def test_get_internal_wsgi_application(self):\n    # Test that get_internal_wsgi_application returns the correct application\n    # when settings.WSGI_APPLICATION is set.\n    from django.conf import settings\n\n    original_value = getattr(settings, \"WSGI_APPLICATION\", None)\n    settings.WSGI_APPLICATION = \"test.wsgi.application\"\n\n    wsgi_app = get_internal_wsgi_application()\n    self.assertEqual(wsgi_app.__module__, \"test.wsgi\")\n    self.assertEqual(wsgi_app.__name__, \"application\")\n\n    # Restore the original value\n    if original_value is None:\n        del settings.WSGI_APPLICATION\n    else:\n        settings.WSGI_APPLICATION = original_value\n\n    # Test that get_internal_wsgi_application returns the default application\n    # when settings.WSGI_APPLICATION is not set.\n    wsgi_app = get_internal_wsgi_application()\n    self.assertEqual(wsgi_app.__module__, \"django.core.wsgi\")\n    self.assertEqual(wsgi_app.__name__, \"get_wsgi_application\")\n", "def test_handle_one_request(self):\n    request = self.request_factory.get('/test/')\n    environ = request.environ\n\n    handler = WSGIRequestHandler(Stub(), UnclosableBytesIO(), BytesIO())\n    handler.get_environ = lambda: environ\n\n        response = app(environ, lambda x, y: None)\n        return response\n\n    handler.server = Stub(get_app=lambda: run_app)\n\n    handler.handle_one_request()\n\n    # Check that the request was processed correctly.\n    self.assertEqual(handler.close_connection, True)\n    self.assertEqual(handler.requestline, '')\n", "def test_handle_one_request(self):\n    environ = self.request_factory.generic('GET', '/').environ\n    request_handler = WSGIRequestHandler(Stub(), Stub(), BytesIO())\n    request_handler.get_environ = lambda: environ\n\n    # Set up a dummy server handler to prevent actual HTTP output\n    class DummyServerHandler:\n            pass\n\n    request_handler.handle_one_request()\n    self.assertTrue(request_handler.close_connection)\n", "def test_handle_one_request(self):\n    request = self.request_factory.get('/test')\n    server = Stub(\n        server_address=('127.0.0.1', 8000),\n        RequestHandlerClass=WSGIRequestHandler,\n        get_app=lambda: lambda environ, start_response: start_response('200 OK', []),\n    )\n    handler = WSGIRequestHandler(request, '127.0.0.1', server)\n    handler.rfile = BytesIO(b'GET /test HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n')\n    handler.wfile = UnclosableBytesIO()\n    handler.handle_one_request()\n    response = handler.wfile.getvalue()\n    self.assertRegex(response, b'HTTP/1.1 200 OK')\n"], "sample_450": ["def test_get_admin_log_template_tag(self):\n    # Create a few more log entries to test pagination\n    content_type_pk = ContentType.objects.get_for_model(Article).pk\n    LogEntry.objects.log_action(\n        self.user.pk,\n        content_type_pk,\n        self.a1.pk,\n        repr(self.a1),\n        CHANGE,\n        change_message=\"Changed something else\",\n    )\n    LogEntry.objects.log_action(\n        self.user.pk,\n        content_type_pk,\n        self.a1.pk,\n        repr(self.a1),\n        CHANGE,\n        change_message=\"Changed something again\",\n    )\n\n    # Test the template tag with a limit\n    response = self.client.get(reverse(\"admin:index\"))\n    context = {\"log_entries\": LogEntry.objects.all()}\n    template = template.Template('{% load admin_utils %}{% get_admin_log 2 as admin_log %}{{ admin_log|length }}')\n    rendered = template.render(context)\n    self.assertEqual(rendered, '2')\n\n    # Test the template tag without a user specified\n    template = template.Template('{% load admin_utils %}{% get_admin_log 2 as admin_log %}{{ admin_log|length }}')\n    rendered = template.render(context)\n    self.assertEqual(rendered, '2')\n\n    # Test the template tag with a user specified\n    template = template.Template('{% load admin_utils %}{% get_admin_log 2 as admin_log for_user user %}{{ admin_log|length }}')\n    context['user'] = self.user\n    rendered = template.render(context)\n    self.assertEqual(rendered, '2')\n\n    # Test the template tag with an invalid user\n    template = template.Template('{% load admin_utils %}{% get_admin_log 2 as admin_log for_user \"invalid\" %}{{ admin_log|length }}')\n    with self.assertRaises(template.TemplateSyntaxError):\n        template.render(context)\n", "def test_get_admin_log_template_tag(self):\n    # Create another log entry for a different user\n    user2 = User.objects.create_user(\n        username=\"user2\", password=\"secret\", email=\"user2@example.com\"\n    )\n    content_type_pk = ContentType.objects.get_for_model(Article).pk\n    LogEntry.objects.log_action(\n        user2.pk,\n        content_type_pk,\n        self.a1.pk,\n        repr(self.a1),\n        CHANGE,\n        change_message=\"Changed something else\",\n    )\n\n    # Test the template tag with a limit and user\n    context = {\n        \"log_entries\": LogEntry.objects.all(),\n        \"user\": self.user,\n    }\n    template = Template('{% load admin_utils %}{% get_admin_log 1 as admin_log for_user user %}{{ admin_log|length }}')\n    rendered = template.render(Context(context))\n    self.assertEqual(rendered, \"1\")\n\n    # Test the template tag with a limit and no user\n    template = Template('{% load admin_utils %}{% get_admin_log 1 as admin_log %}{{ admin_log|length }}')\n    rendered = template.render(Context({\"log_entries\": LogEntry.objects.all()}))\n    self.assertEqual(rendered, \"2\")\n\n    # Test the template tag with an invalid user\n    template = Template('{% load admin_utils %}{% get_admin_log 1 as admin_log for_user invalid_user %}{{ admin_log|length }}')\n    with self.assertRaises(KeyError):\n        template.render(Context({\"log_entries\": LogEntry.objects.all()}))\n", "def test_get_admin_log_tag(self):\n    template = \"\"\"\n        {% load admin_utils %}\n        {% get_admin_log 1 as log_entries for_user user %}\n        {{ log_entries|length }}\n    \"\"\"\n    t = self.client.get(reverse('admin:index'))\n    request = t.wsgi_request\n    context = {'user': self.user, 'request': request}\n    content_type_pk = ContentType.objects.get_for_model(Article).pk\n    LogEntry.objects.log_action(\n        self.user.pk,\n        content_type_pk,\n        self.a1.pk,\n        repr(self.a1),\n        CHANGE,\n        change_message=\"Changed something\",\n    )\n    rendered = self.client.render(template, context)\n    self.assertEqual(rendered.strip(), '1')\n\n    # Test with hard-coded integer user ID\n    template = \"\"\"\n        {% load admin_utils %}\n        {% get_admin_log 1 as log_entries for_user %d %}\n        {{ log_entries|length }}\n    \"\"\" % self.user.pk\n    rendered = self.client.render(template, context)\n    self.assertEqual(rendered.strip(), '1')\n", "def test_get_admin_log_template_tag(self):\n    response = self.client.get(reverse(\"admin:index\"))\n    self.assertTemplateUsed(response, \"admin/index.html\")\n\n    # Test template tag with user ID\n    response = self.client.get(reverse(\"admin:index\") + \"?user_id={}\".format(self.user.pk))\n    self.assertContains(response, escape(repr(self.a1)))\n\n    # Test template tag with user object\n    response = self.client.get(reverse(\"admin:index\") + \"?user={}\".format(self.user.username))\n    self.assertContains(response, escape(repr(self.a1)))\n\n    # Test template tag without user\n    response = self.client.get(reverse(\"admin:index\"))\n    self.assertContains(response, escape(repr(self.a1)))\n\n    # Test template tag with invalid user ID\n    response = self.client.get(reverse(\"admin:index\") + \"?user_id=999\")\n    self.assertNotContains(response, escape(repr(self.a1)))\n", "def test_get_admin_log_template_tag(self):\n    template = \"\"\"\n        {% load admin_utils %}\n        {% get_admin_log 10 as admin_log %}\n        {% for entry in admin_log %}\n            {{ entry.object_repr }}\n        {% endfor %}\n    \"\"\"\n    context = {\n        \"log_entries\": LogEntry.objects.all(),\n    }\n    output = self.render_template(template, context)\n    self.assertContains(output, \"Title\")\n\n    template = \"\"\"\n        {% load admin_utils %}\n        {% get_admin_log 10 as admin_log for_user user %}\n        {% for entry in admin_log %}\n            {{ entry.object_repr }}\n        {% endfor %}\n    \"\"\"\n    context = {\n        \"log_entries\": LogEntry.objects.all(),\n        \"user\": self.user,\n    }\n    output = self.render_template(template, context)\n    self.assertContains(output, \"Title\")\n\n    template = \"\"\"\n        {% load admin_utils %}\n        {% get_admin_log 10 as admin_log for_user 999 %}\n        {% for entry in admin_log %}\n            {{ entry.object_repr }}\n        {% endfor %}\n    \"\"\"\n    context = {\n        \"log_entries\": LogEntry.objects.all(),\n    }\n    output = self.render_template(template, context)\n    self.assertNotContains(output, \"Title\")\n"], "sample_451": ["def test_parse_docstring(self):\n    title, body, metadata = parse_docstring(self.docstring)\n    self.assertEqual(title, \"This __doc__ output is required for testing. I copied this example from `admindocs` documentation. (TITLE)\")\n    self.assertIn(\"Display an individual :model:`myapp.MyModel`.\", body)\n    self.assertIn(\"**Context**\", body)\n    self.assertIn(\"**Template:**\", body)\n    self.assertEqual(metadata, {\"some_metadata\": \"some data\"})\n", "def test_parse_docstring(self):\n    title, body, metadata = parse_docstring(self.docstring)\n    self.assertEqual(title, \"This __doc__ output is required for testing. I copied this example from `admindocs` documentation. (TITLE)\")\n    self.assertIn(\"**Context**\", body)\n    self.assertIn(\":template:`myapp/my_template.html` (DESCRIPTION)\", body)\n    self.assertEqual(metadata, {\"some_metadata\": \"some data\"})\n", "def test_parse_docstring(self):\n    title, body, metadata = parse_docstring(self.docstring)\n    self.assertEqual(title, \"This __doc__ output is required for testing. I copied this example from `admindocs` documentation. (TITLE)\")\n    self.assertIn(\"**Context**\", body)\n    self.assertIn(\":template:`myapp/my_template.html` (DESCRIPTION)\", body)\n    self.assertEqual(metadata, {\"some_metadata\": \"some data\"})\n", "def test_parse_docstring(self):\n    title, body, metadata = parse_docstring(self.docstring)\n    self.assertEqual(title, \"This __doc__ output is required for testing. I copied this example from `admindocs` documentation. (TITLE)\")\n    self.assertIn(\"Display an individual :model:`myapp.MyModel`.\", body)\n    self.assertIn(\"**Context**\", body)\n    self.assertIn(\":template:`myapp/my_template.html` (DESCRIPTION)\", body)\n    self.assertEqual(metadata, {\"some_metadata\": \"some data\"})\n", "def test_parse_docstring(self):\n    title, body, metadata = parse_docstring(self.docstring)\n    self.assertEqual(title, \"This __doc__ output is required for testing. I copied this example from `admindocs` documentation. (TITLE)\")\n    self.assertIn(\"Display an individual :model:`myapp.MyModel`.\", body)\n    self.assertEqual(metadata, {\"some_metadata\": \"some data\"})\n"], "sample_453": ["def test_prepopulated_fields_js(self):\n    # Setup an admin form for a model with prepopulated fields\n    request = self.request_factory.get(reverse('admin:myapp_article_add'))\n    article_admin = ArticleAdmin(Article, site)\n    form = article_admin.get_form(request)(instance=None)\n\n    # Define prepopulated fields\n    article_admin.prepopulated_fields = {\n        'slug': ('title',)\n    }\n\n    # Create a context with the admin form and its prepopulated fields\n    context = {\n        'adminform': form,\n        'add': True,\n        'change': False,\n        'has_add_permission': True,\n        'has_change_permission': False,\n    }\n\n    # Call the template tag\n    updated_context = prepopulated_fields_js(context)\n\n    # Check that the prepopulated fields are in the context\n    self.assertIn('prepopulated_fields', updated_context)\n    self.assertEqual(len(updated_context['prepopulated_fields']), 1)\n\n    # Check the JSON output\n    prepopulated_fields_json = json.loads(updated_context['prepopulated_fields_json'])\n    self.assertEqual(len(prepopulated_fields_json), 1)\n    self.assertEqual(prepopulated_fields_json[0]['name'], 'slug')\n    self.assertEqual(prepopulated_fields_json[0]['dependency_ids'], ['#id_title'])\n", "def test_prepopulated_fields_js(self):\n    request = self.request_factory.get(reverse('admin:auth_user_add'))\n    adminform = UserAdmin(User, site).get_form(request)\n    form = adminform()\n    context = {\n        'adminform': adminform,\n        'add': True,\n        'change': False,\n        'is_popup': False,\n        'save_as': False,\n        'has_add_permission': True,\n        'has_change_permission': True,\n        'has_view_permission': True,\n        'has_editable_inline_admin_formsets': False,\n    }\n    context = prepopulated_fields_js(context)\n    self.assertIn('prepopulated_fields_json', context)\n    self.assertIsInstance(context['prepopulated_fields_json'], str)\n", "def test_prepopulated_fields_js(self):\n    request = self.request_factory.get(reverse('admin:auth_user_add'))\n    adminform = UserAdmin(User, site).get_form(request)\n    field = adminform.base_fields['username']\n    prepopulated_fields = [{'field': field, 'dependencies': [field]}]\n    context = {'adminform': adminform, 'prepopulated_fields': prepopulated_fields}\n    context = prepopulated_fields_js(context)\n    self.assertIn('prepopulated_fields_json', context)\n    self.assertIsInstance(context['prepopulated_fields_json'], str)\n    self.assertEqual(len(context['prepopulated_fields']), 1)\n", "def test_prepopulated_fields_js(self):\n    request = self.request_factory.get(reverse('admin:auth_user_add'))\n    adminform = UserAdmin(User, site).get_form(request)()\n    context = {'adminform': adminform}\n    output_context = prepopulated_fields_js(context)\n    self.assertIn('prepopulated_fields_json', output_context)\n    self.assertIsInstance(output_context['prepopulated_fields_json'], str)\n    self.assertEqual(output_context['prepopulated_fields'], [])\n", "def test_prepopulated_fields_js(self):\n    request = self.request_factory.get(reverse('admin:myapp_article_add'))\n    adminform = ArticleAdmin(Article, site).get_form(request)()\n    context = {\n        'adminform': adminform,\n        'has_change_permission': True,\n    }\n    context = prepopulated_fields_js(context)\n    self.assertIn('prepopulated_fields', context)\n    self.assertIn('prepopulated_fields_json', context)\n    self.assertIsInstance(context['prepopulated_fields'], list)\n    self.assertIsInstance(context['prepopulated_fields_json'], str)\n"], "sample_455": ["    def test_check_constraint_validate(self):\n        class CheckModel(models.Model):\n            value = models.IntegerField()\n            check_constraint = models.CheckConstraint(\n                check=models.Q(value__gte=0), name=\"value_gte_0\"\n            )\n\n        instance = CheckModel(value=-1)\n        with self.assertRaises(ValidationError):\n            instance.full_clean()\n\n        instance.value = 1\n        instance.full_clean()  # No ValidationError raised\n", "    def test_check_constraint_validate(self):\n        class Model(models.Model):\n            value = models.IntegerField()\n\n            class Meta:\n                constraints = [\n                    models.CheckConstraint(check=models.Q(value__gte=0), name='value_gte_0')\n                ]\n\n        model = Model(value=-1)\n        with self.assertRaises(ValidationError):\n            model.full_clean()\n", "    def test_check_constraint(self):\n        class Product(models.Model):\n            name = models.CharField(max_length=255)\n            price = models.DecimalField(max_digits=10, decimal_places=2)\n\n            class Meta:\n                constraints = [\n                    models.CheckConstraint(check=models.Q(price__gt=0), name='price_gt_0')\n                ]\n\n        with atomic():\n            try:\n                Product.objects.create(name='Product 1', price=-10)\n                self.fail('Negative price should raise IntegrityError')\n            except IntegrityError:\n                pass\n\n        product = Product.objects.create(name='Product 2', price=10)\n        try:\n            product.price = -10\n            product.save()\n            self.fail('Negative price should raise IntegrityError')\n        except IntegrityError:\n            pass\n", "    def test_check_constraint(self):\n        # Create a model with a check constraint\n        class Model(models.Model):\n            value = models.IntegerField()\n            class Meta:\n                constraints = [\n                    models.CheckConstraint(check=models.Q(value__gte=0), name='value_gte_0'),\n                ]\n\n        # Try to create an instance that violates the constraint\n        with self.assertRaises(IntegrityError):\n            Model.objects.create(value=-1)\n\n        # Create an instance that satisfies the constraint\n        Model.objects.create(value=1)\n", "    def test_check_constraint(self):\n        class Product(models.Model):\n            name = models.CharField(max_length=255)\n            price = models.DecimalField(max_digits=10, decimal_places=2)\n\n            class Meta:\n                constraints = [\n                    models.CheckConstraint(check=models.Q(price__gt=0), name='price_gt_0'),\n                ]\n\n        with self.assertRaises(IntegrityError):\n            Product.objects.create(name='Test', price=-1)\n"], "sample_456": ["def test_formset_default_renderer(self):\n    formset = ChoiceFormSet()\n    self.assertIsInstance(formset.renderer, TemplatesSetting)\n    self.assertEqual(formset.template_name, \"django/forms/formsets/default.html\")\n", "def test_management_form_clean_handles_missing_fields(self):\n    form = ManagementForm(\n        {\n            TOTAL_FORM_COUNT: 1,\n        },\n        prefix=\"test\",\n    )\n    form.full_clean()\n    self.assertEqual(form.cleaned_data[TOTAL_FORM_COUNT], 1)\n    self.assertEqual(form.cleaned_data[INITIAL_FORM_COUNT], 0)\n", "def test_management_form_validation_error(self):\n    formset = self.make_choiceformset(\n        [(\"Choice 1\", 1), (\"Choice 2\", 2)], total_forms=3, initial_forms=2\n    )\n    # Make the management form invalid by changing the TOTAL_FORMS value.\n    data = formset.data.copy()\n    data[\"choices-TOTAL_FORMS\"] = \"abc\"\n    formset = self.make_choiceformset(\n        [(\"Choice 1\", 1), (\"Choice 2\", 2)], data=data, total_forms=3, initial_forms=2\n    )\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(len(formset.non_form_errors()), 1)\n    error = formset.non_form_errors()[0]\n    self.assertEqual(error.code, \"missing_management_form\")\n", "def test_formset_validate_min(self):\n    ChoiceFormSet = formset_factory(Choice, min_num=2, validate_min=True)\n    formset = self.make_choiceformset(formset_data=[(\"Choice 1\", 1)], formset_class=ChoiceFormSet)\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(formset.non_form_errors(), [\"Please submit at least 2 forms.\"])\n", "def test_formset_custom_get_form_kwargs(self):\n    class CustomFormSet(BaseFormSet):\n            kwargs = super().get_form_kwargs(index)\n            kwargs[\"custom_kwarg\"] = \"value\"\n            return kwargs\n\n    FormSet = formset_factory(CustomKwargForm, formset=CustomFormSet)\n    formset = FormSet()\n    for form in formset.forms:\n        self.assertEqual(form.custom_kwarg, \"value\")\n"], "sample_457": ["    def test_check_constraint(self):\n        class CheckModel(models.Model):\n            num = models.IntegerField()\n\n            class Meta:\n                constraints = [\n                    models.CheckConstraint(check=models.Q(num__gte=0), name='num_gte_0')\n                ]\n\n        with connection.schema_editor() as schema_editor:\n            schema_editor.create_model(CheckModel)\n\n        try:\n            CheckModel.objects.create(num=-1)\n            self.fail('Creating a model with a negative number should raise an IntegrityError.')\n        except IntegrityError:\n            pass\n\n        CheckModel.objects.create(num=0)\n        CheckModel.objects.create(num=1)\n", "    def test_check_constraint(self):\n        class Product(models.Model):\n            name = models.CharField(max_length=255)\n            price = models.DecimalField(max_digits=5, decimal_places=2)\n\n            class Meta:\n                constraints = [\n                    models.CheckConstraint(\n                        check=models.Q(price__gte=0), name=\"price_gte_0\"\n                    )\n                ]\n\n        with self.settings(DATABASES={\"default\": {\"ENGINE\": \"django.db.backends.sqlite3\"}}):\n            Product.objects.create(name=\"Test\", price=-1)\n            with self.assertRaises(IntegrityError):\n                with atomic():\n                    Product.objects.create(name=\"Test\", price=-1)\n", "    def test_create_check_constraint(self):\n        # Create a model with a check constraint.\n        class Product(models.Model):\n            price = models.DecimalField(max_digits=10, decimal_places=2)\n            class Meta:\n                constraints = [\n                    models.CheckConstraint(\n                        check=models.Q(price__gt=0),\n                        name='price_gt_zero',\n                    ),\n                ]\n\n        # Make sure the check constraint is created.\n        with connection.schema_editor() as schema_editor:\n            schema_editor.create_model(Product)\n\n        constraints = get_constraints(Product._meta.db_table)\n        self.assertIn('price_gt_zero', [c['name'] for c in constraints])\n", "    def test_check_constraint(self):\n        class Model(models.Model):\n            field = models.IntegerField()\n\n            class Meta:\n                constraints = [\n                    models.CheckConstraint(check=models.Q(field__gt=0), name='field_gt_0')\n                ]\n\n        with self.assertRaises(ValidationError):\n            Model(field=-1).full_clean()\n\n        Model(field=1).full_clean()  # Should not raise an error\n\n        # Test that the constraint is also enforced at the database level.\n        with atomic():\n            with self.assertRaises(IntegrityError):\n                Model.objects.create(field=-1)\n\n        # Test that the constraint is properly serialized.\n        constraints = get_constraints(Model._meta.db_table)\n        self.assertIn('field_gt_0', [c['name'] for c in constraints])\n        constraint = next(c for c in constraints if c['name'] == 'field_gt_0')\n        self.assertEqual(constraint['type'], 'check')\n        self.assertIn('field > 0', constraint['check'])\n", "    def test_check_constraint(self):\n        # Create a model with a check constraint.\n        class Product(models.Model):\n            price = models.DecimalField(max_digits=10, decimal_places=2)\n            class Meta:\n                constraints = [\n                    models.CheckConstraint(\n                        check=models.Q(price__gte=0), name=\"price_positive\"\n                    ),\n                ]\n\n        # Try to create an instance that violates the constraint.\n        product = Product(price=-1)\n        with self.assertRaises(ValidationError):\n            product.full_clean()\n\n        # Create an instance that satisfies the constraint.\n        product = Product(price=1)\n        product.full_clean()  # Should not raise an exception.\n\n        # Drop the table to prevent it from being left behind after this test.\n        with connection.schema_editor() as schema_editor:\n            schema_editor.delete_model(Product)\n"], "sample_458": ["    def test_addslashes(self):\n        output = self.engine.render_to_string(\"addslashes01\", {\"a\": \"a\\'b\"})\n        self.assertEqual(output, \"a\\\\'b\")\n", "    def test_floatformat_with_negative_precision(self):\n        with localcontext() as ctx:\n            ctx.prec = 3\n            result = floatformat(123.456, arg=\"-2\")\n            self.assertEqual(result, \"123.46\")\n", "    def test_floatformat_arg(self):\n        with translation.override(\"en\"):\n            output = self.engine.render_to_string(\"floatformat_arg_int\")\n            self.assertEqual(output, \"1.235\")\n\n            output = self.engine.render_to_string(\"floatformat_arg_str\")\n            self.assertEqual(output, \"6.790\")\n\n            output = self.engine.render_to_string(\"floatformat_arg_g\")\n            self.assertEqual(output, \"12,345\")\n\n            output = self.engine.render_to_string(\"floatformat_arg_u\")\n            self.assertEqual(output, \"98765\")\n\n            output = self.engine.render_to_string(\"floatformat_arg_neg\")\n            self.assertEqual(output, \"123,456\")\n", "    def test_floatformat_with_positive_arg(self):\n        out = self.engine.render_to_string(\n            \"floatformat01\", {\"a\": 34.23234, \"b\": 34.00000}\n        )\n        self.assertEqual(out, \"34.2 34\")\n", "    def test_floatformat_arg(self):\n        with localcontext() as ctx:\n            ctx.prec = 3\n            rendered = self.engine.render(\n                \"{% autoescape off %}{{ a|floatformat:2 }} {{ b|floatformat:3 }}{% endautoescape %}\",\n                {\"a\": Decimal(\"1.12345\"), \"b\": Decimal(\"2.12345\")},\n            )\n            self.assertEqual(rendered, \"1.12 2.123\")\n"], "sample_459": ["def test_integer_field_get_db_prep_value(self):\n    field = self.model._meta.get_field(\"value\")\n    value = 12345\n    prep_value = field.get_db_prep_value(value, connection)\n    self.assertEqual(prep_value, value)\n", "    def testBigIntegerFieldCoversIntegerFieldRange(self):\n        # Test that BigIntegerField can handle values within the IntegerField range.\n        self.model.objects.create(value=self.documented_range[0])\n        self.model.objects.create(value=self.documented_range[1])\n\n        # Check that the values are correctly stored and retrieved.\n        self.assertEqual(\n            self.model.objects.filter(value=self.documented_range[0]).count(), 1\n        )\n        self.assertEqual(\n            self.model.objects.filter(value=self.documented_range[1]).count(), 1\n        )\n", "def test_integer_field_lookups(self):\n    instance1 = self.model.objects.create(value=123)\n    instance2 = self.model.objects.create(value=456)\n\n    # Test exact lookup\n    self.assertEqual(\n        list(self.model.objects.filter(value__exact=123)), [instance1]\n    )\n\n    # Test gt lookup\n    self.assertEqual(\n        list(self.model.objects.filter(value__gt=123)), [instance2]\n    )\n\n    # Test gte lookup\n    self.assertEqual(\n        list(self.model.objects.filter(value__gte=123)), [instance1, instance2]\n    )\n\n    # Test lt lookup\n    self.assertEqual(\n        list(self.model.objects.filter(value__lt=456)), [instance1]\n    )\n\n    # Test lte lookup\n    self.assertEqual(\n        list(self.model.objects.filter(value__lte=456)), [instance1, instance2]\n    )\n", "def test_integer_field_get_prep_value(self):\n    field = self.model._meta.get_field(\"value\")\n    self.assertEqual(field.get_prep_value(123), 123)\n    self.assertEqual(field.get_prep_value(\"123\"), 123)\n    with self.assertRaises(ValidationError):\n        field.get_prep_value(\"abc\")\n", "def test_integer_field_max_length(self):\n    field = self.model._meta.get_field(\"value\")\n    self.assertIsNone(field.max_length)\n"], "sample_461": ["def test_urlfield_assume_scheme(self):\n    f = URLField(assume_scheme='https')\n    with self.assertRaisesMessage(ValidationError, 'Enter a valid URL.'):\n        f.clean('http://www.example.com')\n\n    f = URLField(assume_scheme='http')\n    self.assertEqual(f.clean('www.example.com'), 'http://www.example.com')\n", "def test_urlfield_with_scheme(self):\n    f = URLField(assume_scheme='https')\n    self.assertEqual(f.clean('example.com'), 'https://example.com')\n    with self.assertRaisesMessage(ValidationError, 'Enter a valid URL.'):\n        f.clean('http://example.com')\n    f = URLField(assume_scheme='http')\n    self.assertEqual(f.clean('example.com'), 'http://example.com')\n", "def test_urlfield_assume_scheme(self):\n    f = URLField(assume_scheme='https')\n    with self.assertRaisesMessage(ValidationError, 'Enter a valid URL.'):\n        f.clean('http://example.com')\n\n    f = URLField(assume_scheme='http')\n    self.assertEqual(f.clean('example.com'), 'http://example.com')\n\n    f = URLField()\n    with self.assertRaisesMessage(ValidationError, 'Enter a valid URL.'):\n        f.clean('example.com')\n", "    def test_typedchoicefield_coerce(self):\n        f = TypedChoiceField(coerce=int, choices=[(1, \"1\"), (2, \"2\")])\n        with self.assertRaisesMessage(ValidationError, \"'3' is not one of the available choices.\"):\n            f.clean('3')\n", "def test_urlfield_uses_https_scheme_by_default(self):\n    f = URLField()\n    self.assertEqual(f.clean('example.com'), 'https://example.com')\n"], "sample_462": ["def test_choicefield_1(self):\n    f = ChoiceField(choices=[('1', 'One'), ('2', 'Two')])\n    self.assertWidgetRendersTo(f, '<select name=\"f\"><option value=\"1\">One</option><option value=\"2\">Two</option></select>')\n    with self.assertRaisesMessage(ValidationError, \"'Select a valid choice. 3 is not one of the available choices.'\"):\n        f.clean('3')\n", "    def test_choicefield_1(self):\n        f = ChoiceField(choices=[('1', '1'), ('2', '2')])\n        self.assertEqual('2', f.clean('2'))\n        msg = \"Select a valid choice. 3 is not one of the available choices.\"\n        with self.assertRaisesMessage(ValidationError, msg):\n            f.clean('3')\n", "    def test_choices_property(self):\n        choices = [\n            (1, \"Option 1\"),\n            (2, \"Option 2\"),\n            (3, \"Option 3\"),\n        ]\n\n        field = ChoiceField(choices=choices)\n        self.assertEqual(field.choices, choices)\n\n        # Test that setting the choices attribute updates the widget's choices\n        new_choices = [\n            (4, \"Option 4\"),\n            (5, \"Option 5\"),\n            (6, \"Option 6\"),\n        ]\n        field.choices = new_choices\n        self.assertEqual(field.widget.choices, new_choices)\n", "    def test_choicefield_1(self):\n        f = ChoiceField(choices=[(\"a\", \"A\"), (\"b\", \"B\")])\n        self.assertFormErrors(\n            {\"f\": f},\n            {\"f\": \"not_a\"},\n            [\"Select a valid choice. not_a is not one of the available choices.\"],\n        )\n        with self.assertRaisesMessage(ValidationError, \"invalid_choice\"):\n            f.clean(\"not_a\")\n", "def test_choicefield_uses_choices_from_model(self):\n    class Model(models.Model):\n        field = models.CharField(max_length=10, choices=[('a', 'A'), ('b', 'B')])\n\n    class MyForm(Form):\n        field = ChoiceField(model=Model, initial='a')\n\n    form = MyForm()\n    self.assertEqual(form.fields['field'].choices, [('a', 'A'), ('b', 'B')])\n    self.assertEqual(form.initial['field'], 'a')\n    self.assertEqual(form.as_p(), '<p>Field: <select name=\"field\">\\n<option value=\"a\" selected>A</option>\\n<option value=\"b\">B</option>\\n</select><br></p>')\n"], "sample_464": ["def test_file_response_with_unseekable_file(self):\n    file = UnseekableBytesIO(b'Hello, world!')\n    response = FileResponse(file)\n    self.assertEqual(response.status_code, 200)\n    self.assertEqual(response['Content-Type'], 'application/octet-stream')\n    self.assertEqual(response['Content-Length'], '')\n", "def test_file_response_non_existent_file(self):\n    filename = 'non_existent_file.txt'\n    response = FileResponse(filename)\n    self.assertEqual(response.status_code, 200)\n    with self.assertRaises(FileNotFoundError):\n        response._set_streaming_content(filename)\n", "def test_file_response_non_string_filename(self):\n    f = ContentFile(b\"Hello, World!\", name=123)\n    response = FileResponse(f)\n    self.assertEqual(response.filename, \"\")\n", "def test_file_response_set_headers_with_filename(self):\n    response = FileResponse(io.BytesIO(b'Hello, world!'), filename='example.txt')\n    self.assertEqual(response.headers['Content-Type'], 'text/plain')\n    self.assertEqual(response.headers['Content-Disposition'], 'attachment; filename=\"example.txt\"')\n", "def test_file_response_with_as_attachment_and_filename(self):\n    file = ContentFile(b\"Hello, World!\", name=\"example.txt\")\n    response = FileResponse(file, as_attachment=True, filename=\"downloaded.txt\")\n\n    self.assertEqual(response[\"Content-Disposition\"], 'attachment; filename=\"downloaded.txt\"')\n    self.assertEqual(response[\"Content-Type\"], \"text/plain\")\n    self.assertEqual(response[\"Content-Length\"], str(len(b\"Hello, World!\")))\n"], "sample_465": ["def test_get_inline_instances(self):\n    class Inline(TabularInline):\n        model = Song\n\n    class ModelAdminTest(ModelAdmin):\n        inlines = [Inline]\n\n    ma = ModelAdminTest(Band, self.site)\n    request = MockRequest()\n    inline_instances = ma.get_inline_instances(request)\n\n    self.assertEqual(len(inline_instances), 1)\n    self.assertIsInstance(inline_instances[0], Inline)\n", "def test_get_content_type_for_model(self):\n    content_type = get_content_type_for_model(Band)\n    self.assertEqual(content_type.model_class(), Band)\n    self.assertEqual(content_type.app_label, \"admin_views\")\n", "def test_get_inline_instances(self):\n    class SongInline(TabularInline):\n        model = Song\n\n    class BandAdmin(ModelAdmin):\n        inlines = [SongInline]\n\n    band_admin = BandAdmin(Band, self.site)\n\n    request = MockRequest()\n    request.user = MockSuperUser()\n\n    inline_instances = band_admin.get_inline_instances(request)\n    self.assertEqual(len(inline_instances), 1)\n    self.assertIsInstance(inline_instances[0], SongInline)\n", "def test_get_formset(self):\n    class ConcertInline(TabularInline):\n        model = Concert\n\n    ma = ModelAdmin(Band, self.site)\n    formset = ma.get_inline_instances(request)[0].get_formset(request)\n    self.assertIsInstance(formset, forms.BaseFormSet)\n", "def test_get_changelist_instance(self):\n    ma = ModelAdmin(Band, self.site)\n    request = MockRequest()\n    cl = ma.get_changelist_instance(request)\n    self.assertIsInstance(cl, ma.get_changelist(request))\n"], "sample_466": ["def test_operation_writer_serialize_list(self):\n    operation = migrations.RunPython(\n        code=[lambda apps, schema_editor: None, lambda apps, schema_editor: None],\n        reverse_code=[lambda apps, schema_editor: None],\n    )\n    writer = OperationWriter(operation)\n    output, imports = writer.serialize()\n    self.assertEqual(\n        output,\n        \"migrations.RunPython(\\n\"\n        \"    code=[\\n\"\n        \"        lambda apps, schema_editor: None,\\n\"\n        \"        lambda apps, schema_editor: None,\\n\"\n        \"    ],\\n\"\n        \"    reverse_code=[\\n\"\n        \"        lambda apps, schema_editor: None,\\n\"\n        \"    ],\\n\"\n        \"),\",\n    )\n    self.assertEqual(imports, set())\n", "def test_operation_writer_serialize_list_of_dicts(self):\n    operation = migrations.RunSQL(\n        sql=\"SELECT * FROM table\",\n        reverse_sql=\"DROP TABLE table\",\n        state_operations=[\n            migrations.CreateModel(\n                name='Model',\n                fields=[\n                    ('id', models.AutoField(auto_created=True, primary_key=True)),\n                ],\n            ),\n        ],\n        hints={'tables': ['table']},\n    )\n    writer = OperationWriter(operation)\n    output, imports = writer.serialize()\n    self.assertIn(\"state_operations=[\", output)\n    self.assertIn(\"{'name': 'Model',\", output)\n    self.assertIn(\"fields=[\", output)\n    self.assertIn(\"{'id': models.AutoField(\", output)\n    self.assertIn(\"hints={'tables': ['table']}\", output)\n", "def test_operation_writer_serialize_with_list(self):\n    operation = migrations.RunPython(\n        code=[lambda apps, schema_editor: None],\n        reverse_code=[lambda apps, schema_editor: None],\n    )\n    writer = OperationWriter(operation)\n    output, imports = writer.serialize()\n    self.assertEqual(output.splitlines()[0], \"migrations.RunPython(\")\n    self.assertIn(\"code=[\", output)\n    self.assertIn(\"reverse_code=[\", output)\n    self.assertEqual(imports, {\"import django.db.migrations\"})\n", "def test_operation_writer_serialize_with_nested_dict(self):\n    operation = migrations.RunPython(\n        code=lambda apps, schema_editor: None,\n        reverse_code=lambda apps, schema_editor: None,\n        hints={\"models\": [\"model1\", \"model2\"]},\n    )\n    writer = OperationWriter(operation)\n    output, imports = writer.serialize()\n    self.assertIn(\"hints={'models': ['model1', 'model2']}\", output)\n    self.assertEqual(imports, {\"import django.db.models\"})\n", "def test_operation_writer_serialize_custom_operations(self):\n    custom_operation = custom_migration_operations.operations.CustomOperation()\n    writer = OperationWriter(custom_operation)\n    output, imports = writer.serialize()\n    self.assertIn(\"custom_migration_operations.operations\", imports)\n    self.assertEqual(\n        output,\n        \"custom_migration_operations.operations.CustomOperation(),\",\n    )\n"], "sample_467": ["def test_use_required_attribute(self):\n    widget = SelectDateWidget(years=(\"2007\", \"2008\"))\n    self.assertTrue(widget.use_required_attribute(None))\n    self.assertFalse(widget.use_required_attribute(date(2007, 1, 1)))\n", "def test_use_required_attribute(self):\n    widget = SelectDateWidget(required=True)\n    self.assertTrue(widget.use_required_attribute(None))\n    self.assertFalse(widget.use_required_attribute(date.today()))\n", "def test_use_required_attribute(self):\n    widget = SelectDateWidget(years=(\"2007\", \"2008\"), required=True)\n    self.assertTrue(widget.use_required_attribute(None))\n    self.assertFalse(widget.use_required_attribute(date(2007, 1, 1)))\n", "def test_required(self):\n    class SelectDateForm(Form):\n        date = DateField(widget=SelectDateWidget, required=True)\n\n    form = SelectDateForm({})\n    self.assertFalse(form.is_valid())\n    self.assertEqual(\n        form.errors[\"date\"], [\"This field is required.\"],\n    )\n", "def test_use_required_attribute(self):\n    widget = SelectDateWidget()\n    self.assertTrue(widget.use_required_attribute(None))\n    self.assertFalse(widget.use_required_attribute(date.today()))\n"], "sample_469": ["def test_names_to_path_with_m2m(self):\n    with self.assertRaises(MultiJoin):\n        list(Author.objects.filter(friends__friends__name=\"John\"))\n", "def test_get_field_names_from_opts(self):\n    opts = Author._meta\n    field_names = get_field_names_from_opts(opts)\n    self.assertIn(\"name\", field_names)\n    self.assertIn(\"age\", field_names)\n    self.assertIn(\"friends\", field_names)\n    self.assertIn(\"friend\", field_names)\n\n    # Test that proxy models return the correct field names.\n    opts = Store._meta\n    field_names = get_field_names_from_opts(opts)\n    self.assertIn(\"name\", field_names)\n    self.assertIn(\"original_opening\", field_names)\n    self.assertIn(\"friday_night_closing\", field_names)\n    self.assertIn(\"books\", field_names)\n", "def test_get_field_names_from_opts(self):\n    field_names = get_field_names_from_opts(Author._meta)\n    self.assertIn(\"name\", field_names)\n    self.assertIn(\"age\", field_names)\n    self.assertIn(\"friends\", field_names)\n\n    # Check that the method doesn't return a field from an abstract base class.\n    self.assertNotIn(\"extra_field\", field_names)\n\n    # Check that the method returns fields inherited from concrete base classes.\n    self.assertIn(\"publisher_ptr\", get_field_names_from_opts(Book._meta))\n\n    # Check that the method returns fields inherited from concrete base classes\n    # when multi-table inheritance is used.\n    self.assertIn(\"company_ptr\", get_field_names_from_opts(Store._meta))\n    self.assertIn(\"original_opening\", get_field_names_from_opts(Store._meta))\n", "def test_get_field_names_from_opts(self):\n    field_names = get_field_names_from_opts(Book._meta)\n    self.assertEqual(field_names, {\"contact\", \"id\", \"isbn\", \"name\", \"pages\", \"price\", \"pubdate\", \"publisher\", \"rating\"})\n    # Check that non-concrete fields are included.\n    field_names = get_field_names_from_opts(Book._meta, include_non_concrete_fields=True)\n    self.assertEqual(\n        field_names,\n        {\n            \"contact\",\n            \"id\",\n            \"isbn\",\n            \"name\",\n            \"pages\",\n            \"price\",\n            \"pubdate\",\n            \"publisher\",\n            \"rating\",\n            \"authors\",\n            \"stores\",\n        },\n    )\n", "def test_get_field_names_from_opts(self):\n    field_names = get_field_names_from_opts(Book._meta)\n    self.assertIn(\"isbn\", field_names)\n    self.assertIn(\"name\", field_names)\n    self.assertIn(\"pages\", field_names)\n    self.assertIn(\"rating\", field_names)\n    self.assertIn(\"price\", field_names)\n    self.assertIn(\"contact_id\", field_names)\n    self.assertIn(\"publisher_id\", field_names)\n    self.assertIn(\"pubdate\", field_names)\n\n    field_names = get_field_names_from_opts(Author._meta)\n    self.assertIn(\"id\", field_names)\n    self.assertIn(\"name\", field_names)\n    self.assertIn(\"age\", field_names)\n"], "sample_468": ["def test_contextdict_exit(self):\n    context = Context()\n    with context.push({'a': 1}) as ctx:\n        self.assertIn(ctx, context.dicts)\n    self.assertNotIn(ctx, context.dicts)\n", "def test_context_pop_exception(self):\n    context = Context()\n    with self.assertRaises(ContextPopException):\n        context.pop()\n", "def test_context_flatten(self):\n    context = Context({'a': 1, 'b': 2})\n    context.push({'b': 3, 'c': 4})\n    self.assertEqual(context.flatten(), {'True': True, 'False': False, 'None': None, 'a': 1, 'b': 3, 'c': 4})\n", "def test_contextdict_enter_exit(self):\n    context = Context()\n    with context.push({'key': 'value'}) as ctx_dict:\n        self.assertIn(ctx_dict, context.dicts)\n    self.assertNotIn(ctx_dict, context.dicts)\n", "def test_context_setdefault(self):\n    context = Context({'a': 1})\n    self.assertEqual(context.setdefault('a', 2), 1)\n    self.assertEqual(context['a'], 1)\n    self.assertEqual(context.setdefault('b', 3), 3)\n    self.assertEqual(context['b'], 3)\n"], "sample_470": ["def test_lazy(self):\n    # Test that lazy works with different types of return values.\n    self.assertEqual(lazy(lambda: 1)(), 1)\n    self.assertEqual(lazy(lambda: \"hello\")(), \"hello\")\n    self.assertEqual(lazy(lambda: True)(), True)\n\n    # Test that lazy objects can be used as function arguments.\n        return x\n\n    self.assertEqual(takes_int(lazy(lambda: 1)()), 1)\n\n    # Test that lazy objects are hashable.\n    self.assertEqual(hash(lazy(lambda: 1)()), hash(1))\n\n    # Test that lazy objects can be compared for equality.\n    self.assertEqual(lazy(lambda: 1)(), lazy(lambda: 1)())\n    self.assertNotEqual(lazy(lambda: 1)(), lazy(lambda: 2)())\n\n    # Test that lazy objects support boolean operations.\n    self.assertTrue(bool(lazy(lambda: True)()))\n    self.assertFalse(bool(lazy(lambda: False)()))\n\n    # Test that lazy objects can be iterated over (when they represent an iterable).\n        yield 1\n        yield 2\n        yield 3\n\n    self.assertEqual(list(lazy(yields_123)()), [1, 2, 3])\n\n    # Test that lazy objects can be sliced (when they represent a sequence).\n        return [1, 2, 3]\n\n    self.assertEqual(lazy(returns_list)()[1:], [2, 3])\n", "def test_cached_property(self):\n    class TestClass:\n            self.value = value\n\n        @cached_property\n            return self.value * 2\n\n    obj = TestClass(5)\n    self.assertEqual(obj.cached_value, 10)\n\n    # Check that the property is only computed once\n    with mock.patch.object(TestClass, 'cached_value', new_callable=mock.PropertyMock) as mock_cached_value:\n        mock_cached_value.fget.side_effect = Exception('This should not be called')\n        self.assertEqual(obj.cached_value, 10)\n", "def test_cached_property(self):\n    class CachedPropertyTest:\n        @cached_property\n            return 42\n\n    obj = CachedPropertyTest()\n    self.assertEqual(obj.cached_value, 42)\n    # Subsequent access should return the cached value.\n    self.assertEqual(obj.cached_value, 42)\n    # The cached value should be stored in the instance's __dict__.\n    self.assertIn('cached_value', obj.__dict__)\n    self.assertEqual(obj.__dict__['cached_value'], 42)\n", "def test_lazy(self):\n    func = mock.Mock(return_value='Hello')\n    lazy_func = lazy(func, str)\n    self.assertIsInstance(lazy_func(), str)\n    func.assert_called_once()\n    # Subsequent calls don't call the function again.\n    self.assertEqual(lazy_func(), 'Hello')\n    self.assertEqual(func.call_count, 1)\n", "def test_lazy(self):\n    func = mock.Mock(return_value='Hello')\n    lazy_func = lazy(func, str)\n    self.assertEqual(lazy_func(), 'Hello')\n    self.assertEqual(func.call_count, 1)\n\n    # Calling the lazy function again should re-evaluate the function.\n    func.return_value = 'World'\n    self.assertEqual(lazy_func(), 'World')\n    self.assertEqual(func.call_count, 2)\n"], "sample_471": ["    def test_floatfield_required(self):\n        f = FloatField(required=True)\n        with self.assertRaisesMessage(ValidationError, \"This field is required.\"):\n            f.clean(\"\")\n        with self.assertRaisesMessage(ValidationError, \"This field is required.\"):\n            f.clean(None)\n", "    def test_floatfield_1(self):\n        f = FloatField()\n        self.assertWidgetRendersTo(f, '<input step=\"any\" type=\"number\" name=\"f\" required id=\"id_f\">')\n        with self.assertRaisesMessage(ValidationError, \"Enter a number.\"):\n            f.clean(\"abc\")\n        self.assertEqual(f.clean(1), 1.0)\n        self.assertEqual(f.clean(3.14), 3.14)\n        self.assertEqual(f.clean(\"3.14\"), 3.14)\n", "    def test_charfield_widget(self):\n        field = CharField()\n        self.assertIsInstance(field.widget, Textarea)\n", "    def test_floatfield_1(self):\n        f = FloatField()\n        self.assertWidgetRendersTo(f, 'number')\n        with self.assertRaisesMessage(ValidationError, \"'This is not a number' value must be a float.\"):\n            f.clean('This is not a number')\n", "    def test_charfield_1(self):\n        f = CharField(max_length=20)\n        self.assertWidgetRendersTo(f, '<input type=\"text\" name=\"f\" maxlength=\"20\" required>')\n        with self.assertRaisesMessage(ValidationError, \"'moo' value must be at most 5 characters (it has 3).\"):\n            f.clean('m' * 21)\n"], "sample_472": ["def test_validate_number(self):\n    paginator = Paginator([1, 2, 3], 2)\n    with self.assertRaises(PageNotAnInteger):\n        paginator.validate_number(\"a\")\n    with self.assertRaises(EmptyPage):\n        paginator.validate_number(-1)\n    with self.assertRaises(EmptyPage):\n        paginator.validate_number(paginator.num_pages + 1)\n    self.assertEqual(paginator.validate_number(1), 1)\n    self.assertEqual(paginator.validate_number(2), 2)\n", "def test_get_elided_page_range(self):\n    \"\"\"\n    Test the get_elided_page_range method.\n    \"\"\"\n    paginator = Paginator(range(100), 5)\n    self.assertEqual(\n        list(paginator.get_elided_page_range(number=10)),\n        [1, 2, '\u2026', 8, 9, 10, 11, 12, '\u2026', 19, 20]\n    )\n    self.assertEqual(\n        list(paginator.get_elided_page_range(number=20)),\n        [1, 2, '\u2026', 18, 19, 20, 21, 22, '\u2026', 19, 20]\n    )\n    self.assertEqual(\n        list(paginator.get_elided_page_range(number=1)),\n        [1, 2, 3, 4, 5, '\u2026', 19, 20]\n    )\n    self.assertEqual(\n        list(paginator.get_elided_page_range(number=20, on_each_side=1)),\n        [1, '\u2026', 19, 20]\n    )\n    self.assertEqual(\n        list(paginator.get_elided_page_range(number=1, on_ends=0)),\n        [1, 2, 3, '\u2026', 20]\n    )\n", "def test_validate_number(self):\n    paginator = Paginator([1, 2, 3, 4, 5], 2)\n    with self.assertRaises(PageNotAnInteger):\n        paginator.validate_number(\"not an integer\")\n    with self.assertRaises(EmptyPage):\n        paginator.validate_number(-1)\n    with self.assertRaises(EmptyPage):\n        paginator.validate_number(paginator.num_pages + 1)\n    self.assertEqual(paginator.validate_number(1), 1)\n    self.assertEqual(paginator.validate_number(paginator.num_pages), paginator.num_pages)\n", "def test_paginator_constructor(self):\n    \"\"\"\n    Tests that the Paginator constructor correctly handles its arguments.\n    \"\"\"\n    # Test with an empty list\n    paginator = Paginator([], 10)\n    self.assertEqual(paginator.count, 0)\n    self.assertEqual(paginator.num_pages, 0)\n    self.assertEqual(list(paginator.page_range), [])\n\n    # Test with a non-empty list\n    paginator = Paginator([1, 2, 3, 4, 5], 2)\n    self.assertEqual(paginator.count, 5)\n    self.assertEqual(paginator.num_pages, 3)\n    self.assertEqual(list(paginator.page_range), [1, 2, 3])\n\n    # Test with an invalid per_page value\n    with self.assertRaises(ValueError):\n        Paginator([1, 2, 3], -1)\n\n    # Test with a non-integer per_page value\n    with self.assertRaises(TypeError):\n        Paginator([1, 2, 3], \"a\")\n", "def test_paginator_iter(self):\n    \"\"\"\n    Test that Paginator.__iter__() correctly returns Page objects.\n    \"\"\"\n    objects = [1, 2, 3, 4, 5]\n    paginator = Paginator(objects, 2)\n    pages = list(paginator)\n    self.assertEqual(len(pages), 3)\n    for i, page in enumerate(pages, start=1):\n        self.assertIsInstance(page, paginator.page_class)\n        self.assertEqual(page.number, i)\n        self.assertEqual(page.paginator, paginator)\n"], "sample_473": ["compilation error", "compilation error", "def test_asgi_handler_request_finished_signal(self):\n    async def send(message):\n        if message[\"type\"] == \"http.response.start\":\n            self.assertEqual(message[\"status\"], 200)\n        elif message[\"type\"] == \"http.response.body\":\n            self.assertFalse(message.get(\"more_body\", False))\n\n    async def receive():\n        return {\"type\": \"http.request\", \"body\": b\"\", \"more_body\": False}\n\n    request_finished.connect(self._request_finished)\n    handler = ASGIHandler()\n    scope = {\"type\": \"http\", \"method\": \"GET\", \"path\": \"/\"}\n    with override_settings(ROOT_URLCONF=\"django.urls\"):\n        with self.settings(ALLOWED_HOSTS=[\"testserver\"]):\n            async with AsyncRequestFactory() as request_factory:\n                await handler(scope, receive, send)\n    self.assertTrue(self._request_finished_called)\n    request_finished.disconnect(self._request_finished)\n", "compilation error", "compilation error"], "sample_474": ["    def setUpTestData(cls):\n        cls.author1 = Author.objects.create(name=\"John\", alias=\"Doe\")\n        cls.author2 = Author.objects.create(name=\"Jane\", alias=\"Smith\")\n", "    def setUpTestData(cls):\n        cls.author = Author.objects.create(name=\"John Smith\", alias=\"smithj\")\n", "    def setUpTestData(cls):\n        cls.author = Author.objects.create(name=\"John Smith\", alias=\"smithj\")\n", "    def setUpTestData(cls):\n        cls.john = Author.objects.create(name=\"John Smith\", alias=\"smithj\")\n", "    def setUpTestData(cls):\n        cls.john = Author.objects.create(name=\"John Smith\", alias=\"smithj\")\n"], "sample_475": ["    def test_readonly_fields_item(self):\n        class MyModelAdmin(admin.ModelAdmin):\n            readonly_fields = (\"non_existent_field\",)\n\n        self.assertIsInvalid(\n            MyModelAdmin,\n            ValidationTestModel,\n            \"The value of 'readonly_fields[0]' is not a callable, an attribute of \"\n            \"'MyModelAdmin', or an attribute of 'tests.ValidationTestModel'.\",\n            id=\"admin.E035\",\n        )\n", "    def test_list_display_must_be_a_list_or_tuple(self):\n        class MyModelAdmin(ModelAdmin):\n            list_display = \"foo\"\n\n        self.assertIsInvalid(\n            MyModelAdmin,\n            Band,\n            \"The value of 'list_display' must be a list or tuple.\",\n            id=\"admin.E107\",\n        )\n", "    def test_filter_vertical_must_be_a_list_or_tuple(self):\n        class MyModelAdmin(ModelAdmin):\n            filter_vertical = \"field\"\n\n        self.assertIsInvalid(\n            MyModelAdmin, Band, \"must be a list or tuple\", id=\"admin.E017\"\n        )\n", "    def test_check_relation(self):\n        class NoFKModel(Model):\n            pass\n\n        class InvalidInline(TabularInline):\n            model = NoFKModel\n\n        self.assertIsInvalid(\n            InvalidInline,\n            ValidationTestModel,\n            \"No ForeignKey to 'ValidationTestModel' found on 'NoFKModel'\",\n            id=\"admin.E202\",\n        )\n", "    def test_filter_vertical_must_be_a_list_or_tuple(self):\n        class MyModelAdmin(ModelAdmin):\n            filter_vertical = \"not a list or tuple\"\n\n        self.assertIsInvalid(\n            MyModelAdmin,\n            ValidationTestModel,\n            \"The value of 'filter_vertical' must be a list or tuple.\",\n            id=\"admin.E017\",\n        )\n"], "sample_476": ["def test_image_field_update_dimension_fields(self):\n    \"\"\"\n    Tests that the update_dimension_fields method of ImageField is correctly\n    called after a model instance is initialized and when the image file is\n    changed. Verifies that it updates the height and width fields of the model\n    if they are defined.\n    \"\"\"\n    person = self.PersonModel(mugshot=self.file1)\n    # Dimensions should not be updated yet.\n    self.assertIsNone(person.mugshot_width)\n    self.assertIsNone(person.mugshot_height)\n\n    # Save the person to trigger dimension caching.\n    person.save()\n    self.check_dimensions(person, 4, 8)\n\n    # Change the mugshot image to a new one.\n    person.mugshot = self.file2\n    person.save()\n    self.check_dimensions(person, 8, 4)\n\n    # Make sure that updating an image field to None will clear the\n    # dimensions fields.\n    person.mugshot = None\n    person.save()\n    self.check_dimensions(person, None, None)\n", "def test_image_field_update_dimension_fields(self):\n    \"\"\"\n    Tests that the update_dimension_fields method of ImageField correctly\n    updates the height and width fields of the model instance when an image\n    is assigned to the field.\n\n    This test uses a Person model with a mugshot ImageField and height/width\n    fields defined.\n    \"\"\"\n    # Create a new Person instance and save it.\n    person = self.PersonModel()\n    person.save()\n\n    # Check that the height/width fields are empty.\n    self.assertIsNone(person.mugshot_height)\n    self.assertIsNone(person.mugshot_width)\n\n    # Assign an image file to the mugshot field and check that the\n    # height/width fields are updated correctly.\n    person.mugshot = self.file1\n    person.save()\n    self.check_dimensions(person, 4, 8)\n\n    # Clear the mugshot field and check that the height/width fields are\n    # cleared.\n    person.mugshot.delete(save=True)\n    self.check_dimensions(person, None, None)\n", "def test_generate_filename(self):\n    \"\"\"\n    Tests that ImageField's generate_filename method correctly uses the\n    upload_to callable or string to generate the filename for the image file.\n    \"\"\"\n    instance = self.PersonModel()\n    field = instance._meta.get_field('mugshot')\n\n    # Test with a string upload_to\n    field.upload_to = 'test'\n    filename = field.generate_filename(instance, 'image.png')\n    self.assertEqual(filename, 'test/image.png')\n\n    # Test with a callable upload_to\n        return 'test/' + filename\n    field.upload_to = upload_to\n    filename = field.generate_filename(instance, 'image.png')\n    self.assertEqual(filename, 'test/image.png')\n\n    # Test with a datetime-based upload_to\n    field.upload_to = '%Y/%m/%d'\n    filename = field.generate_filename(instance, 'image.png')\n    date_path = datetime.datetime.now().strftime('%Y/%m/%d')\n    self.assertEqual(filename, date_path + '/image.png')\n", "def test_update_dimension_fields(self):\n    \"\"\"\n    Test that the update_dimension_fields method updates the dimension fields\n    correctly when an image is assigned to the ImageField.\n    \"\"\"\n    p = self.PersonModel()\n    self.assertIsNone(p.mugshot_width)\n    self.assertIsNone(p.mugshot_height)\n\n    p.mugshot = self.file1\n    p.save()\n\n    self.check_dimensions(p, 4, 8)\n\n    # Check that dimensions are not updated if the image is not changed\n    p.save()\n    self.check_dimensions(p, 4, 8)\n\n    # Check that dimensions are updated if the image is changed\n    p.mugshot = self.file2\n    p.save()\n    self.check_dimensions(p, 8, 4)\n\n    # Check that dimensions are cleared if the image is deleted\n    p.mugshot.delete()\n    p.save()\n    self.check_dimensions(p, None, None)\n", "def test_update_dimension_fields(self):\n    \"\"\"\n    Tests that the update_dimension_fields method updates the width and height\n    fields of the model instance when an image is assigned to the ImageField.\n    \"\"\"\n    instance = self.PersonModel()\n    instance.mugshot = self.file1\n    self.check_dimensions(instance, 4, 8)\n\n    # Check that dimensions are updated when a new image is assigned.\n    instance.mugshot = self.file2\n    self.check_dimensions(instance, 8, 4)\n\n    # Check that dimensions are cleared when no image is assigned.\n    instance.mugshot = None\n    self.check_dimensions(instance, None, None)\n"], "sample_477": ["    def test_stringformat_float(self):\n        output = self.engine.render_to_string(\"stringformat01\", {\"a\": 3.14})\n        self.assertEqual(output, \"3.140000E+00\")\n", "    def test_stringformat_float(self):\n        output = self.engine.render_to_string(\"stringformat01\", {\"a\": 3.14})\n        self.assertEqual(output, \"3.140000E+00\")\n", "    def test_add_filter(self):\n        output = self.engine.render_to_string(\"add01\", {\"i\": 5, \"f\": 4.0})\n        self.assertEqual(output, \"6 5.0\")\n", "    def test_stringformat_float(self):\n        output = self.engine.render_to_string(\"stringformat01\", {\"a\": 3.14})\n        self.assertEqual(output, \"3.140000E+00\")\n", "    def test_addslashes(self):\n        output = self.engine.render_to_string(\"string_filter\", {\"a\": \"foo\\'bar\"})\n        self.assertEqual(output, \"foo\\\\'bar\")\n"], "sample_478": ["    def test_filter_must_be_list_or_tuple(self):\n        class MyModelAdmin(ModelAdmin):\n            list_filter = \"bad\"\n\n        self.assertIsInvalid(\n            MyModelAdmin,\n            ValidationTestModel,\n            \"The value of 'list_filter' must be a list or tuple.\",\n            id=\"admin.E112\",\n        )\n", "    def test_autocomplete_fields_must_be_a_list_or_tuple(self):\n        class MyModelAdmin(ModelAdmin):\n            autocomplete_fields = \"not a list or tuple\"\n\n        self.assertIsInvalid(\n            MyModelAdmin,\n            ValidationTestModel,\n            \"The value of 'autocomplete_fields' must be a list or tuple.\",\n            id=\"admin.E036\",\n        )\n", "    def test_list_filter_must_be_a_list_or_tuple(self):\n        class MyModelAdmin(ModelAdmin):\n            list_filter = \"not a list or tuple\"\n\n        self.assertIsInvalid(\n            MyModelAdmin,\n            ValidationTestModel,\n            \"The value of 'list_filter' must be a list or tuple.\",\n            id=\"admin.E112\",\n        )\n", "    def test_raw_id_fields_as_list(self):\n        class RawIdFieldsAdmin(ModelAdmin):\n            raw_id_fields = [\"field1\", \"field2\"]\n\n        self.assertIsValid(RawIdFieldsAdmin, ValidationTestModel)\n", "    def test_fieldset_fields_can_refer_to_non_model_fields(self):\n        class ModelAdmin(ModelAdmin):\n            fieldsets = [(None, {\"fields\": (\"non_model_field\",)})]\n\n        self.assertIsValid(ModelAdmin, Band)\n"], "sample_479": ["def test_optimizer_remove_index(self):\n    operations = [\n        migrations.AddIndex(\n            model_name=\"model\",\n            index=models.Index(fields=[\"field\"], name=\"index_name\"),\n        ),\n        migrations.RemoveIndex(model_name=\"model\", name=\"index_name\"),\n    ]\n    self.assertOptimizesTo(operations, [])\n", "def test_optimizer_remove_index(self):\n    operations = [\n        migrations.CreateModel(\n            name=\"TestModel\",\n            fields=[(\"id\", models.AutoField(primary_key=True))],\n            options={\"indexes\": [models.Index(fields=[\"id\"], name=\"test_index\")]},\n        ),\n        migrations.RemoveIndex(model_name=\"testmodel\", name=\"test_index\"),\n    ]\n    expected = [\n        migrations.CreateModel(\n            name=\"TestModel\",\n            fields=[(\"id\", models.AutoField(primary_key=True))],\n        )\n    ]\n    self.assertOptimizesTo(operations, expected)\n", "def test_optimizer_remove_index(self):\n    operations = [\n        migrations.AddIndex(\n            model_name=\"mymodel\",\n            index=models.Index(fields=[\"field1\"], name=\"myindex\"),\n        ),\n        migrations.RemoveIndex(model_name=\"mymodel\", name=\"myindex\"),\n    ]\n    expected = []\n    self.assertOptimizesTo(operations, expected)\n", "def test_rename_index(self):\n    operations = [\n        migrations.RenameIndex(\n            model_name=\"UnicodeModel\",\n            new_name=\"new_name\",\n            old_name=\"old_name\",\n        ),\n        migrations.RenameIndex(\n            model_name=\"UnicodeModel\",\n            new_name=\"another_new_name\",\n            old_name=\"new_name\",\n        ),\n    ]\n    expected = [\n        migrations.RenameIndex(\n            model_name=\"UnicodeModel\",\n            new_name=\"another_new_name\",\n            old_name=\"old_name\",\n        ),\n    ]\n    self.assertOptimizesTo(operations, expected)\n", "def test_remove_index(self):\n    operations = [\n        migrations.CreateModel(\n            name=\"TestModel\",\n            fields=[\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"field\", models.IntegerField()),\n            ],\n            options={\"indexes\": [models.Index(fields=[\"field\"], name=\"my_index\")]},\n        ),\n        migrations.RemoveIndex(model_name=\"testmodel\", name=\"my_index\"),\n    ]\n    expected = [\n        migrations.CreateModel(\n            name=\"TestModel\",\n            fields=[\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"field\", models.IntegerField()),\n            ],\n        ),\n    ]\n    self.assertOptimizesTo(operations, expected)\n"], "sample_480": ["def test_key_transform_text_lookup_mixin(self):\n    mixin = KeyTransformTextLookupMixin()\n    with self.assertRaises(TypeError):\n        mixin.__init__(KeyTransform(\"test\", \"test\"), \"test\")\n    key_text_transform = KeyTextTransform(\"test\", \"test\")\n    mixin.__init__(key_text_transform, \"test\")\n    self.assertEqual(mixin.lhs, key_text_transform)\n", "def test_key_transform_text_lookup_mixin(self):\n    mixin = KeyTransformTextLookupMixin()\n    key_transform = KeyTransform(\"test\", \"test\")\n    with self.assertRaises(TypeError):\n        mixin.__init__(key_transform, \"test\", \"test\")\n\n    class TestKeyTransformTextLookup(KeyTransformTextLookupMixin, lookups.Exact):\n        pass\n\n    lookup = TestKeyTransformTextLookup(key_transform, \"test\")\n    sql, params = lookup.process_lhs(mock.Mock(), connection)\n    self.assertEqual(sql, \"(%s ->> %s)\" % (connection.ops.quote_name(\"test\"), \"%s\"))\n    self.assertEqual(params, [\"test\"])\n", "def test_key_transform_factory(self):\n    factory = KeyTransformFactory('test')\n    transform = factory(JSONField())\n    self.assertIsInstance(transform, KeyTransform)\n    self.assertEqual(transform.key_name, 'test')\n    self.assertIsInstance(transform.output_field, JSONField)\n\n    transform_text = factory(JSONField(), output_field=models.TextField())\n    self.assertIsInstance(transform_text, KeyTransform)\n    self.assertIsInstance(transform_text.output_field, models.TextField)\n", "def test_key_transform_text_lookup_mixin(self):\n    mixin = KeyTransformTextLookupMixin()\n    key_transform = KeyTransform(\"key\", \"value\")\n    with self.assertRaises(TypeError):\n        mixin.__init__(key_transform, \"rhs\")\n    with mock.patch.object(mixin, \"__class__\") as cls:\n        cls.__name__ = \"TestKeyTransformTextLookupMixin\"\n        mixin.__init__(key_transform, \"rhs\")\n        self.assertIsInstance(mixin.lhs, KeyTextTransform)\n        self.assertEqual(mixin.rhs, \"rhs\")\n", "def test_key_transform_text_lookup_mixin(self):\n    mixin = KeyTransformTextLookupMixin()\n    key_transform = KeyTransform(\"test\", \"test\")\n    with self.assertRaises(TypeError):\n        mixin.__init__(key_transform)\n"], "sample_481": ["    def test_stringfilter_addslashes(self):\n        output = self.engine.render_to_string(\"stringfilter01\", {\"a\": \"Hello, world!\"})\n        self.assertEqual(output, \"Hello\\\\, world\\\\!\")\n", "    def test_add_filter(self):\n        output = self.engine.render_to_string(\"add01\", {\"i\": 5})\n        self.assertEqual(output, \"10\")\n", "    def test_length(self):\n        output = self.engine.render_to_string(\"length01\", {\"a\": \"hello\"})\n        self.assertEqual(output, \"5\")\n", "    def test_length(self):\n        output = self.engine.render_to_string(\"length01\", {\"a\": \"hello\"})\n        self.assertEqual(output, \"5\")\n", "    def test_length(self):\n        output = self.engine.render_to_string(\"length01\", {\"a\": [1, 2, 3]})\n        self.assertEqual(output, \"3\")\n"], "sample_482": ["    def test_stringformat(self):\n        output = self.engine.render_to_string(\n            'stringformat_basic', {\"value\": 1}\n        )\n        self.assertEqual(output, \"01\")\n", "    def test_stringformat(self):\n        output = self.engine.render_to_string(\"stringformat_float\")\n        self.assertEqual(output, \"3.140\")\n\n        output = self.engine.render_to_string(\"stringformat_exp\")\n        self.assertEqual(output, \"1.234500e+04\")\n\n        output = self.engine.render_to_string(\"stringformat_hex\")\n        self.assertEqual(output, \"0xff\")\n", "    def test_addslashes(self):\n        output = self.engine.render_to_string(\"addslashes_basic\", {\"a\": 'Hello, \"World\"'})\n        self.assertEqual(output, 'Hello, \\\\\"World\\\\\"')\n", "    def test_stringformat(self):\n        output = self.engine.render_to_string(\n            'stringformat', {'value': 123.456}\n        )\n        self.assertEqual(output, '123.456')\n        ", "    def test_stringformat(self):\n        output = self.engine.render_to_string(\"stringformat_valid\", {\"a\": 1})\n        self.assertEqual(output, \"01\")\n\n        output = self.engine.render_to_string(\"stringformat_invalid\", {\"a\": 1})\n        self.assertEqual(output, \"\")\n\n        output = self.engine.render_to_string(\"stringformat_empty\", {\"a\": 1})\n        self.assertEqual(output, \"\")\n"], "sample_483": ["def test_check_admin_app(self):\n    class MyAdminSite(AdminSite):\n        @property\n            return [\"error!\"]\n\n    self.assertEqual(check_admin_app([MyAdminSite()]), [\"error!\"])\n", "def test_inline_model_admin_checks(self):\n    class MyInline(admin.TabularInline):\n        model = Song\n\n    errors = MyInline(Book, AdminSite()).check()\n    self.assertEqual(errors, [])\n\n    class MyInline(admin.TabularInline):\n        model = Song\n        extra = \"not an integer\"\n\n    errors = MyInline(Book, AdminSite()).check()\n    self.assertEqual(len(errors), 1)\n    self.assertEqual(errors[0].id, \"admin.E203\")\n\n    class MyInline(admin.TabularInline):\n        model = Song\n        max_num = \"not an integer\"\n\n    errors = MyInline(Book, AdminSite()).check()\n    self.assertEqual(len(errors), 1)\n    self.assertEqual(errors[0].id, \"admin.E204\")\n\n    class MyInline(admin.TabularInline):\n        model = Song\n        min_num = \"not an integer\"\n\n    errors = MyInline(Book, AdminSite()).check()\n    self.assertEqual(len(errors), 1)\n    self.assertEqual(errors[0].id, \"admin.E205\")\n", "def test_admin_checks_base_model_admin_checks_check_filter_horizontal(self):\n    class MyAdmin(admin.ModelAdmin):\n        filter_horizontal = \"test\"\n\n    errors = BaseModelAdminChecks().check(MyAdmin, obj=MyAdmin)\n    expected_errors = [\n        checks.Error(\n            \"The value of 'filter_horizontal' must be a list or tuple.\",\n            obj=MyAdmin.__class__,\n            id=\"admin.E018\",\n        )\n    ]\n    self.assertEqual(errors, expected_errors)\n\n    class MyAdmin(admin.ModelAdmin):\n        filter_horizontal = (\"test\",)\n\n    errors = BaseModelAdminChecks().check(MyAdmin, obj=MyAdmin)\n    expected_errors = [\n        checks.Error(\n            \"The value of 'filter_horizontal[0]' refers to 'test', which is not a field of 'None'.\",\n            obj=MyAdmin.__class__,\n            id=\"admin.E019\",\n        )\n    ]\n    self.assertEqual(errors, expected_errors)\n", "def test_check_inline_model_admin_checks(self):\n    class MyInline(admin.TabularInline):\n        model = Song\n\n    errors = MyInline(Album, AdminSite()).check()\n    self.assertEqual(errors, [])\n\n    class MyInline(admin.TabularInline):\n        model = Song\n        extra = \"not an integer\"\n\n    errors = MyInline(Album, AdminSite()).check()\n    self.assertEqual(len(errors), 1)\n    self.assertEqual(errors[0].id, \"admin.E203\")\n\n    class MyInline(admin.TabularInline):\n        model = Song\n        max_num = \"not an integer\"\n\n    errors = MyInline(Album, AdminSite()).check()\n    self.assertEqual(len(errors), 1)\n    self.assertEqual(errors[0].id, \"admin.E204\")\n\n    class MyInline(admin.TabularInline):\n        model = Song\n        min_num = \"not an integer\"\n\n    errors = MyInline(Album, AdminSite()).check()\n    self.assertEqual(len(errors), 1)\n    self.assertEqual(errors[0].id, \"admin.E205\")\n", "def test_check_prepopulated_fields_value(self):\n    class MyAdmin(admin.ModelAdmin):\n        prepopulated_fields = {\"slug\": (\"title\", \"non_existent_field\")}\n\n    errors = MyAdmin(Book, AdminSite()).check()\n    expected = [\n        checks.Error(\n            \"The value of 'prepopulated_fields[\\\"slug\\\"][1]' refers to 'non_existent_field', \"\n            \"which is not a field of 'admin_checks.Book'.\",\n            obj=MyAdmin,\n            id=\"admin.E030\",\n        ),\n    ]\n    self.assertEqual(errors, expected)\n"], "sample_484": ["    def setUpTestData(cls):\n        Author.objects.create(name=\"John Smith\", alias=\"smithj\")\n", "    def setUpTestData(cls):\n        Author.objects.create(name=\"John Smith\", alias=\"smithj\")\n", "    def setUpTestData(cls):\n        Author.objects.create(name=\"John Smith\", alias=\"smithj\")\n", "    def setUpTestData(cls):\n        Author.objects.create(name=\"John Smith\", alias=\"smithj\")\n", "    def test_length(self):\n        author = Author.objects.create(name=\"John Smith\")\n        length = Author.objects.filter(name=author.name).annotate(\n            name_length=Length(\"name\")\n        )\n        self.assertEqual(length.get().name_length, len(author.name))\n"], "sample_485": ["def test_escape(self):\n    self.check_output(escape, \"<script>alert('test')</script>\", \"&lt;script&gt;alert(&#x27;test&#x27;)&lt;/script&gt;\")\n    self.check_output(escape, \"Hello, world!\", \"Hello, world!\")\n    self.check_output(escape, \"<p>Hello, world!</p>\", \"&lt;p&gt;Hello, world!&lt;/p&gt;\")\n", "def test_escapejs(self):\n    # Test escapejs against values that need escaping and ones that don't\n    self.check_output(escapejs, \"hello world\", \"hello\\\\u0020world\")\n    self.check_output(escapejs, 'hello \"world\"', 'hello\\\\u0022world\\\\u0022')\n    self.check_output(escapejs, \"line\\nbreak\", \"line\\\\u000Abreak\")\n    self.check_output(escapejs, \"line\\r\\nbreak\", \"line\\\\u000D\\\\u000Abreak\")\n    self.check_output(escapejs, \"hello\\\\world\", \"hello\\\\u005Cworld\")\n", "def test_escapejs(self):\n    # Make sure escapejs works as expected\n    self.check_output(escapejs, 'testing\\\\x20spaces')\n    self.check_output(escapejs, 'testing\\nlinebreaks')\n    self.check_output(escapejs, 'testing\\r\\nWindows linebreaks')\n    self.check_output(escapejs, 'testing\\ttabs')\n    self.check_output(escapejs, 'testing \"double quotes\" and \\'single quotes\\'')\n", "def test_escapejs(self):\n    # Make sure escapejs works correctly for simple strings\n    self.check_output(escapejs, \"Hello World\", \"Hello World\")\n    self.check_output(escapejs, \"<script>alert('Hello')</script>\", \"\\\\u003cscript\\\\u003ealert(\\\\u0027Hello\\\\u0027)\\\\u003c/script\\\\u003e\")\n\n    # Make sure escapejs works correctly for strings with special characters\n    self.check_output(escapejs, \"\\n\\r\\t\\b\\f\\v\\a\", \"\\\\u000a\\\\u000d\\\\u0009\\\\u0008\\\\u000c\\\\u000b\\\\u0007\")\n", "def test_escapejs(self):\n    # Test escapejs against values that need escaping and ones that don't\n    self.check_output(escapejs, \"hello world\")\n    self.check_output(escapejs, \"hello world\\n\")\n    self.check_output(escapejs, '<script>alert(\"XSS\")</script>')\n    self.check_output(escapejs, '\"double quotes\"')\n    self.check_output(escapejs, \"'single quotes'\")\n"], "sample_486": ["def test_inline_formset_factory_with_uuid_pk(self):\n    ChildFormSet = inlineformset_factory(UUIDPKParent, UUIDPKChild, fields=(\"name\",))\n    parent = UUIDPKParent.objects.create()\n    formset = ChildFormSet(instance=parent)\n    self.assertEqual(formset.fk.name, \"parent\")\n    self.assertIsInstance(formset.forms[0].fields[\"parent\"], InlineForeignKeyField)\n", "def test_inline_formset_with_model_field_overrides(self):\n    ChildFormSet = inlineformset_factory(\n        AutoPKParent,\n        UUIDPKChild,\n        fields=(\"id\", \"name\"),\n        formfield_callback=lambda f, **kwargs: f.formfield(**kwargs) if f.name != \"name\" else f.formfield(max_length=10),\n    )\n    formset = ChildFormSet()\n    self.assertEqual(formset.forms[0].fields[\"name\"].max_length, 10)\n", "def test_inline_formset_factory(self):\n    # Create an inline formset factory with a model that has a foreign key to another model\n    InlineFormSet = inlineformset_factory(\n        ParentWithUUIDAlternateKey, ChildRelatedViaAK, fields=\"__all__\"\n    )\n\n    # Create a parent instance\n    parent = ParentWithUUIDAlternateKey.objects.create()\n\n    # Create a child instance related to the parent\n    child = ChildRelatedViaAK.objects.create(parent=parent)\n\n    # Get the queryset of the inline formset\n    queryset = InlineFormSet.get_queryset()\n\n    # Check that the queryset contains the child instance\n    self.assertIn(child, queryset)\n", "def test_inline_formset_factory_with_uuid_pk(self):\n    ChildFormset = inlineformset_factory(\n        UUIDPKParent, UUIDPKChild, fields=(\"name\",)\n    )\n    parent = UUIDPKParent.objects.create()\n    formset = ChildFormset(instance=parent)\n    self.assertEqual(formset.model, UUIDPKChild)\n    self.assertEqual(formset.fk, UUIDPKChild._meta.get_field(\"parent\"))\n", "def test_inlineformset_factory_validates_max_num(self):\n    ChildFormSet = inlineformset_factory(UUIDPKParent, UUIDPKChild, fields=(\"name\",), max_num=1)\n    parent = UUIDPKParent.objects.create()\n    formset = ChildFormSet(instance=parent, data={\n        \"form-0-name\": \"child1\",\n        \"form-1-name\": \"child2\",\n    })\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(len(formset.errors), 1)\n    self.assertIn(\"form-1\", formset.errors)\n    self.assertEqual(len(formset.errors[\"form-1\"]), 1)\n    self.assertEqual(formset.errors[\"form-1\"][0], \"The inline value did not match the parent instance.\")\n"], "sample_487": ["    def test_raw_id_fields_valid(self):\n        class RawIdModelAdmin(ModelAdmin):\n            raw_id_fields = (\"raw_id_fk\",)\n\n        self.assertIsValid(RawIdModelAdmin, ValidationTestModel)\n", "    def test_autocomplete_fields_valid(self):\n        class MyModelAdmin(ModelAdmin):\n            autocomplete_fields = ['band']\n\n        self.assertIsValid(MyModelAdmin, Song)\n", "    def test_must_be_a_list_or_tuple(self):\n        class MyModelAdmin(ModelAdmin):\n            list_filter = \"test\"\n\n        self.assertIsInvalid(\n            MyModelAdmin,\n            ValidationTestModel,\n            \"The value of 'list_filter' must be a list or tuple.\",\n            id=\"admin.E112\",\n        )\n", "    def test_raw_id_fields_invalid_type(self):\n        class RawIdModelAdmin(ModelAdmin):\n            raw_id_fields = \"invalid\"\n\n        self.assertIsInvalid(\n            RawIdModelAdmin, ValidationTestModel, \"must be a list or tuple\", id=\"admin.E001\"\n        )\n", "    def test_autocomplete_fields(self):\n        class MyModelAdmin(ModelAdmin):\n            autocomplete_fields = (\"band\",)\n\n        self.assertIsValid(MyModelAdmin, Song)\n"], "sample_488": ["def test_lazy_model_operation(self):\n    apps.clear_cache()\n    self.assertEqual(apps._pending_operations, defaultdict(list))\n\n        self.assertIsInstance(model1, models.Model)\n        self.assertIsInstance(model2, models.Model)\n\n    apps.lazy_model_operation(lazy_function, (\"admin\", \"logentry\"), (\"contenttypes\", \"contenttype\"))\n\n    self.assertIn((\"admin\", \"logentry\"), apps._pending_operations)\n    self.assertIn((\"contenttypes\", \"contenttype\"), apps._pending_operations)\n\n    log_entry_model = apps.get_registered_model(\"admin\", \"logentry\")\n    content_type_model = apps.get_registered_model(\"contenttypes\", \"contenttype\")\n\n    apps.do_pending_operations(log_entry_model)\n    apps.do_pending_operations(content_type_model)\n\n    self.assertEqual(apps._pending_operations, defaultdict(list))\n", "def test_get_containing_app_config(self):\n    \"\"\"\n    Apps.get_containing_app_config() returns the AppConfig instance for the\n    app that contains a given object, or None if no such app exists.\n    \"\"\"\n    class SomeModel(models.Model):\n        pass\n\n    with isolate_apps(\"apps\"):\n        new_apps.apps.models_module = __name__\n        self.assertIsNone(apps.get_containing_app_config(SomeModel.__module__))\n        apps.register_model(\"apps\", SomeModel)\n        self.assertEqual(\n            apps.get_containing_app_config(SomeModel.__module__).label,\n            \"apps\",\n        )\n        self.assertIsNone(apps.get_containing_app_config(__name__ + \".nonexistent\"))\n", "def test_lazy_model_operation(self):\n    apps.clear_cache()\n\n    # Simulate a pending operation that should be executed when the model is registered.\n        self.assertEqual(model.__name__, 'LogEntry')\n        nonlocal operation_executed\n        operation_executed = True\n\n    operation_executed = False\n    apps.lazy_model_operation(my_pending_operation, ('admin', 'logentry'))\n\n    # Registering the LogEntry model will trigger the execution of the pending operation.\n    apps.register_model('admin', LogEntry)\n    self.assertTrue(operation_executed)\n", "def test_lazy_model_operation(self):\n    apps.clear_cache()\n\n    calls = []\n\n        self.assertEqual(Model1, LogEntry)\n        self.assertEqual(Model2, TotallyNormal)\n        calls.append(1)\n\n    apps.lazy_model_operation(callback, (\"admin\", \"logentry\"), (\"apps\", \"totallynormal\"))\n\n    # It's not called immediately.\n    self.assertEqual(calls, [])\n\n    # But it is after the model is retrieved.\n    apps.get_model(\"admin\", \"LogEntry\")\n    apps.get_model(\"apps\", \"TotallyNormal\")\n    self.assertEqual(calls, [1])\n", "def test_get_swappable_settings_name(self):\n    \"\"\"\n    Apps.get_swappable_settings_name() returns the setting name for a swappable\n    model.\n    \"\"\"\n    new_apps = new_apps()\n    self.assertEqual(new_apps.get_swappable_settings_name(\"auth.User\"), \"AUTH_USER_MODEL\")\n    self.assertIsNone(new_apps.get_swappable_settings_name(\"contenttypes.ContentType\"))\n"], "sample_489": ["def test_bulk_create_on_model_with_auto_field(self):\n    country = Country.objects.create(name=\"Australia\", iso_two_letter=\"AU\")\n    countries = [\n        Restaurant(country=country, name=\"Restaurant 1\"),\n        Restaurant(country=country, name=\"Restaurant 2\"),\n    ]\n    Restaurant.objects.bulk_create(countries)\n    self.assertEqual(Restaurant.objects.count(), 2)\n", "def test_bulk_create_empty(self):\n    self.assertEqual(Country.objects.bulk_create([]), [])\n", "def test_bulk_create_on_model_with_default_pk(self):\n    instance = BigAutoFieldModel.objects.create()\n    self.assertIsNotNone(instance.pk)\n    new_instances = [\n        BigAutoFieldModel(),\n        BigAutoFieldModel(),\n    ]\n    BigAutoFieldModel.objects.bulk_create(new_instances)\n    self.assertIsNotNone(new_instances[0].pk)\n    self.assertIsNotNone(new_instances[1].pk)\n    self.assertNotEqual(new_instances[0].pk, new_instances[1].pk)\n    self.assertNotEqual(instance.pk, new_instances[0].pk)\n    self.assertNotEqual(instance.pk, new_instances[1].pk)\n", "def test_bulk_create_with_database_function(self):\n    obj = Country.objects.create(name='Test', iso_two_letter='XY')\n    countries = [\n        Country(name=Lower(Value('New')), iso_two_letter='NW'),\n        Country(name=Lower(Value('Other')), iso_two_letter='OT'),\n    ]\n    Country.objects.bulk_create(countries)\n    self.assertEqual(Country.objects.count(), 3)\n    self.assertEqual(Country.objects.filter(name__in=['new', 'other']).count(), 2)\n", "def test_bulk_create_conflict_database_features(self):\n    if connection.features.supports_ignore_conflicts:\n        with self.assertRaises(IntegrityError):\n            Country.objects.bulk_create([Country(iso_two_letter=\"US\")], ignore_conflicts=False)\n        Country.objects.bulk_create([Country(iso_two_letter=\"US\")], ignore_conflicts=True)\n\n    if connection.features.supports_update_conflicts:\n        with self.assertRaises(IntegrityError):\n            Country.objects.bulk_create(\n                [Country(iso_two_letter=\"US\")],\n                update_conflicts=True,\n                update_fields=[\"name\"],\n                unique_fields=[\"iso_two_letter\"],\n            )\n        Country.objects.bulk_create(\n            [Country(iso_two_letter=\"US\", name=\"United States of America\")],\n            update_conflicts=True,\n            update_fields=[\"name\"],\n            unique_fields=[\"iso_two_letter\"],\n        )\n"], "sample_490": ["    def test_check_constraint_violation_error_message(self):\n        class Model(models.Model):\n            value = models.IntegerField()\n\n            class Meta:\n                constraints = [\n                    models.CheckConstraint(\n                        check=models.Q(value__gte=0),\n                        name=\"value_gte_0\",\n                        violation_error_message=\"Value must be greater than or equal to 0.\",\n                    ),\n                ]\n\n        with self.assertRaisesMessage(ValidationError, \"Value must be greater than or equal to 0.\"):\n            Model(value=-1).full_clean()\n", "    def test_check_constraint(self):\n        class Model(models.Model):\n            value = models.IntegerField()\n\n            class Meta:\n                constraints = [\n                    models.CheckConstraint(\n                        check=models.Q(value__gte=0), name=\"value_gte_0\"\n                    )\n                ]\n\n        with atomic():\n            Model.objects.create(value=1)\n            with self.assertRaises(IntegrityError):\n                Model.objects.create(value=-1)\n", "    def test_check_constraint(self):\n        # Create a model with a check constraint.\n        class CheckConstraintModel(models.Model):\n            value = models.IntegerField()\n            check = models.CheckConstraint(check=models.Q(value__gte=0), name='check_value_gte_0')\n\n        # Validate that the check constraint is enforced.\n        with self.assertRaises(ValidationError):\n            CheckConstraintModel(value=-1).full_clean()\n\n        # Validate that valid values pass the check constraint.\n        try:\n            CheckConstraintModel(value=0).full_clean()\n        except ValidationError:\n            self.fail(\"CheckConstraintModel with value=0 should be valid\")\n\n        try:\n            CheckConstraintModel(value=1).full_clean()\n        except ValidationError:\n            self.fail(\"CheckConstraintModel with value=1 should be valid\")\n", "    def test_create_check_constraint(self):\n        # Create a model with a check constraint.\n        class CheckModel(models.Model):\n            value = models.IntegerField()\n            class Meta:\n                constraints = [\n                    models.CheckConstraint(\n                        check=models.Q(value__gte=0), name='value_gte_0'\n                    ),\n                ]\n\n        # Create the table.\n        with connection.schema_editor() as schema_editor:\n            schema_editor.create_model(CheckModel)\n\n        # Ensure the check constraint exists.\n        constraints = get_constraints(CheckModel._meta.db_table)\n        self.assertIn(('value_gte_0', 'CHECK'), [(c['name'], c['type']) for c in constraints])\n\n        # Drop the table.\n        with connection.schema_editor() as schema_editor:\n            schema_editor.delete_model(CheckModel)\n", "    def test_check_constraint(self):\n        class Model(models.Model):\n            field = models.IntegerField()\n\n            class Meta:\n                constraints = [\n                    models.CheckConstraint(\n                        check=models.Q(field__gte=0), name=\"field_gte_0\"\n                    )\n                ]\n\n        with self.assertRaises(IntegrityError):\n            Model.objects.create(field=-1)\n\n        model = Model.objects.create(field=1)\n        model.field = -1\n        with self.assertRaises(IntegrityError):\n            model.save()\n"], "sample_492": ["def test_operation_writer_serialize_with_custom_migration_operations(self):\n    operation = custom_migration_operations.operations.CreateModel(\n        name='MyModel',\n        fields=[\n            ('id', models.AutoField(auto_created=True, primary_key=True)),\n        ],\n    )\n    writer = OperationWriter(operation)\n    output, imports = writer.serialize()\n    self.assertIn(\"custom_migration_operations.operations\", imports)\n    self.assertEqual(output.strip(), \"custom_migration_operations.operations.CreateModel(\\n        name='MyModel',\\n        fields=[\\n            ('id', models.AutoField(auto_created=True, primary_key=True)),\\n        ],\\n    )\")\n", "def test_operation_writer_serialize(self):\n    # Test serializing an operation with various types of arguments.\n    operation = migrations.CreateModel(\n        name='TestModel',\n        fields=[\n            ('id', models.AutoField(primary_key=True)),\n            ('name', models.CharField(max_length=255)),\n            ('price', models.DecimalField(max_digits=10, decimal_places=2)),\n            ('created_at', models.DateTimeField(auto_now_add=True)),\n        ],\n        options={'verbose_name': 'Test Model', 'verbose_name_plural': 'Test Models'},\n    )\n    writer = OperationWriter(operation)\n    output, imports = writer.serialize()\n    self.assertIn(\"name='TestModel'\", output)\n    self.assertIn(\"fields=[\", output)\n    self.assertIn(\"options={'verbose_name': 'Test Model', 'verbose_name_plural': 'Test Models'}\", output)\n    self.assertEqual(imports, {'import django.db.models.deletion', 'from django.db import migrations, models'})\n", "    def test_serialize_with_swappable_dependency(self):\n        operation = migrations.swappable_dependency(\"settings.AUTH_USER_MODEL\")\n        writer = OperationWriter(operation)\n        output, imports = writer.serialize()\n        self.assertEqual(\n            output,\n            \"migrations.swappable_dependency(settings.AUTH_USER_MODEL),\",\n        )\n        self.assertIn(\"from django.conf import settings\", imports)\n", "    def test_operation_writer_serialize_with_args(self):\n        operation = migrations.CreateModel(\n            name=\"MyModel\",\n            fields=[\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=255)),\n            ],\n        )\n        writer = OperationWriter(operation)\n        output, imports = writer.serialize()\n        self.assertEqual(\n            output,\n            \"migrations.CreateModel(\\n\"\n            \"    name='MyModel',\\n\"\n            \"    fields=[\\n\"\n            \"        ('id', models.AutoField(primary_key=True)),\\n\"\n            \"        ('name', models.CharField(max_length=255)),\\n\"\n            \"    ],\\n\"\n            \"),\",\n        )\n        self.assertEqual(imports, {\"from django.db import migrations, models\"})\n", "def test_operation_writer_serialize_with_manual_porting(self):\n    operation = migrations.RunPython(\n        code=custom_migration_operations.operations.test_function\n    )\n    writer = OperationWriter(operation)\n    output, imports = writer.serialize()\n    self.assertIn(\"# Functions from the following migrations need manual copying\", imports)\n    self.assertIn(\n        \"Move them and any dependencies into this file, then update the\",\n        imports,\n    )\n    self.assertIn(\n        \"# RunPython operations to refer to the local versions:\", imports\n    )\n    self.assertIn(\"test_function\", imports)\n    self.assertEqual(output.strip(), \"migrations.RunPython(code=test_function),\")\n"], "sample_493": ["def test_resolve_ref(self):\n    with self.assertRaises(FieldError):\n        Author.objects.annotate(name=F(\"name\")).filter(name=\"Adrian\").resolve_ref(\n            \"name\", allow_joins=False\n        )\n    self.assertEqual(\n        Author.objects.annotate(name=F(\"name\")).filter(name=\"Adrian\").resolve_ref(\n            \"name\"\n        ),\n        F(\"name\"),\n    )\n    self.assertEqual(\n        Author.objects.annotate(name=F(\"name\")).filter(name=\"Adrian\").resolve_ref(\n            \"name__lower\"\n        ),\n        Lower(F(\"name\")),\n    )\n", "def test_exists_subquery(self):\n    subquery = Book.objects.filter(pages__gt=300)\n    qs = Author.objects.filter(Exists(subquery.filter(authors=OuterRef(\"pk\"))))\n    self.assertEqual(len(qs), 6)\n    self.assertQuerysetEqual(\n        qs, [self.a1, self.a2, self.a3, self.a5, self.a6, self.a8], ordered=False\n    )\n", "def test_subqueries_with_datetime_functions(self):\n    books = Book.objects.annotate(\n        publication_start=TruncDate(\"pubdate\"),\n        publication_end=TruncDate(Now()),\n    )\n    self.assertQuerysetEqual(\n        books.filter(publication_start__lte=F(\"publication_end\")), [\n            self.b1.pk,\n            self.b2.pk,\n            self.b3.pk,\n            self.b4.pk,\n            self.b5.pk,\n            self.b6.pk,\n        ],\n        lambda x: x.pk,\n        ordered=False,\n    )\n\n    books = Book.objects.annotate(\n        publication_start=TruncHour(\"pubdate\"),\n        publication_end=TruncHour(Now()),\n    )\n    self.assertQuerysetEqual(\n        books.filter(publication_start__lte=F(\"publication_end\")), [\n            self.b1.pk,\n            self.b2.pk,\n            self.b3.pk,\n            self.b4.pk,\n            self.b5.pk,\n            self.b6.pk,\n        ],\n        lambda x: x.pk,\n        ordered=False,\n    )\n", "def test_exists_subquery_annotation(self):\n    subquery = Book.objects.filter(rating__gt=3).values(\"publisher\")\n    qs = Publisher.objects.annotate(has_good_books=Exists(subquery)).filter(\n        has_good_books=True\n    )\n    self.assertEqual(qs.count(), 3)\n    self.assertEqual(set(qs.values_list(\"name\", flat=True)), {\"Apress\", \"Prentice Hall\", \"Morgan Kaufmann\"})\n", "def test_outerref_in_subquery(self):\n    subquery = Book.objects.filter(pages=OuterRef(\"pages\"))\n    books = Book.objects.annotate(has_same_pages=Exists(subquery)).filter(has_same_pages=True)\n    self.assertEqual(books.count(), 6)\n"], "sample_494": ["def test_serializer_float(self):\n    float_val = 1.23\n    serializer = FloatSerializer(float_val)\n    result, imports = serializer.serialize()\n    self.assertEqual(result, repr(float_val))\n    self.assertEqual(imports, set())\n\n    # Test special float values\n    nan_val = float(\"nan\")\n    serializer = FloatSerializer(nan_val)\n    result, imports = serializer.serialize()\n    self.assertEqual(result, \"float('nan')\")\n    self.assertEqual(imports, set())\n\n    inf_val = float(\"inf\")\n    serializer = FloatSerializer(inf_val)\n    result, imports = serializer.serialize()\n    self.assertEqual(result, \"float('inf')\")\n    self.assertEqual(imports, set())\n", "    def test_serializer_factory(self):\n        value = TestModel1()\n        with self.assertRaises(ValueError) as context:\n            serializer_factory(value)\n        self.assertEqual(\n            str(context.exception),\n            \"Cannot serialize: %r\\nThere are some values Django cannot serialize into \"\n            \"migration files.\\nFor more, see https://docs.djangoproject.com/en/%s/\"\n            \"topics/migrations/#migration-serializing\"\n            % (value, get_version().split(\".\")[0]),\n        )\n", "    def test_regex_serializer(self):\n        regex = re.compile(r\"test\")\n        serializer = RegexSerializer(regex)\n        string, imports = serializer.serialize()\n        self.assertEqual(string, \"re.compile('test')\")\n        self.assertEqual(imports, {\"import re\"})\n", "def test_serialize_float(self):\n    float_value = 1.23456789\n    serializer = BaseSerializer(float_value)\n    serialized, imports = serializer.serialize()\n    self.assertEqual(serialized, repr(float_value))\n    self.assertEqual(imports, set())\n\n    nan_value = float(\"nan\")\n    serializer = FloatSerializer(nan_value)\n    serialized, imports = serializer.serialize()\n    self.assertEqual(serialized, 'float(\"nan\")')\n    self.assertEqual(imports, set())\n\n    inf_value = float(\"inf\")\n    serializer = FloatSerializer(inf_value)\n    serialized, imports = serializer.serialize()\n    self.assertEqual(serialized, 'float(\"inf\")')\n    self.assertEqual(imports, set())\n", "def test_serializer_float(self):\n    serializer = serializer_factory(1.0)\n    self.assertEqual(serializer.serialize(), ('1.0', set()))\n\n    serializer = serializer_factory(float('inf'))\n    self.assertEqual(serializer.serialize(), (\"float('inf')\", set()))\n\n    serializer = serializer_factory(float('-inf'))\n    self.assertEqual(serializer.serialize(), (\"float('-inf')\", set()))\n\n    serializer = serializer_factory(float('nan'))\n    self.assertEqual(serializer.serialize(), (\"float('nan')\", set()))\n"], "sample_495": ["def test_validate_number(self):\n    \"\"\"\n    Test that validate_number() raises exceptions for invalid inputs and\n    returns the page number for valid inputs.\n    \"\"\"\n    paginator = Paginator([1, 2, 3], 2)\n    with self.assertRaises(PageNotAnInteger):\n        paginator.validate_number('a')\n    with self.assertRaises(EmptyPage):\n        paginator.validate_number(-1)\n    with self.assertRaises(EmptyPage):\n        paginator.validate_number(paginator.num_pages + 1)\n    self.assertEqual(paginator.validate_number(1), 1)\n    self.assertEqual(paginator.validate_number(2), 2)\n", "def test_page_range(self):\n    \"\"\"\n    Test that the page_range property returns the correct range of page numbers.\n    \"\"\"\n    paginator = Paginator([1, 2, 3, 4, 5], 2)\n    self.assertEqual(list(paginator.page_range), [1, 2, 3])\n    paginator = Paginator([], 2)\n    self.assertEqual(list(paginator.page_range), [])\n", "def test_validate_number(self):\n    paginator = Paginator([1, 2, 3, 4, 5], 2)\n    with self.assertRaises(PageNotAnInteger):\n        paginator.validate_number('a')\n    with self.assertRaises(EmptyPage):\n        paginator.validate_number(0)\n    with self.assertRaises(EmptyPage):\n        paginator.validate_number(-1)\n    with self.assertRaises(EmptyPage):\n        paginator.validate_number(paginator.num_pages + 1)\n    self.assertEqual(paginator.validate_number(1), 1)\n    self.assertEqual(paginator.validate_number(2), 2)\n    self.assertEqual(paginator.validate_number(3), 3)\n", "def test_validate_number(self):\n    paginator = Paginator([1, 2, 3, 4, 5], 2)\n    with self.assertRaises(PageNotAnInteger):\n        paginator.validate_number('a')\n    with self.assertRaises(EmptyPage):\n        paginator.validate_number(-1)\n    with self.assertRaises(EmptyPage):\n        paginator.validate_number(paginator.num_pages + 1)\n    self.assertEqual(paginator.validate_number(1), 1)\n    self.assertEqual(paginator.validate_number(2), 2)\n    self.assertEqual(paginator.validate_number(3), 3)\n", "def test_validate_number(self):\n    paginator = Paginator([1, 2, 3], 1)\n    with self.assertRaises(PageNotAnInteger):\n        paginator.validate_number('a')\n    with self.assertRaises(EmptyPage):\n        paginator.validate_number(-1)\n    with self.assertRaises(EmptyPage):\n        paginator.validate_number(paginator.num_pages + 1)\n    self.assertEqual(paginator.validate_number(1), 1)\n    self.assertEqual(paginator.validate_number(float(1)), 1)\n"], "sample_496": ["def test_command_parser(self):\n    class TestCommand(BaseCommand):\n            parser.add_argument('--option1', action='store_true')\n\n    cmd = TestCommand()\n    parser = cmd.create_parser('manage.py', 'test')\n    options = parser.parse_args(['--option1'])\n    self.assertTrue(options.option1)\n\n    with self.assertRaises(CommandError) as context:\n        parser.parse_args(['--invalid-option'])\n\n    self.assertIn('unrecognized arguments: --invalid-option', str(context.exception))\n", "    def test_parse_args(self):\n        class Command(BaseCommand):\n            missing_args_message = 'missing args'\n\n        parser = Command.create_parser('prog', 'subcommand')\n        cmd = Command()\n        parser.cmd = cmd\n\n        with self.assertRaises(CommandError) as ctx:\n            parser.parse_args([])\n        self.assertEqual(str(ctx.exception), 'Error: missing args')\n\n        # Positional argument\n        parser.add_argument('arg')\n        with self.assertRaises(CommandError) as ctx:\n            parser.parse_args([])\n        self.assertEqual(str(ctx.exception), 'Error: the following arguments are required: arg')\n\n        # Optional argument\n        parser = Command.create_parser('prog', 'subcommand')\n        cmd = Command()\n        parser.cmd = cmd\n        parser.add_argument('--opt')\n        opts = parser.parse_args(['--opt', 'value'])\n        self.assertEqual(opts.opt, 'value')\n", "    def test_base_command_style(self):\n        command = BaseCommand()\n        self.assertEqual(command.style.ERROR('Error'), '\\x1b[31;1mError\\x1b[0m')\n        self.assertEqual(command.style.SUCCESS('Success'), '\\x1b[32;1mSuccess\\x1b[0m')\n        self.assertEqual(command.style.NOTICE('Notice'), '\\x1b[34;1mNotice\\x1b[0m')\n        self.assertEqual(command.style.SQL_FIELD('SQL Field'), '\\x1b[35;1mSQL Field\\x1b[0m')\n        self.assertEqual(command.style.SQL_COLTYPE('SQL Coltype'), '\\x1b[36;1mSQL Coltype\\x1b[0m')\n        self.assertEqual(command.style.SQL_KEYWORD('SQL Keyword'), '\\x1b[37;1mSQL Keyword\\x1b[0m')\n        self.assertEqual(command.style.SQL_TABLE('SQL Table'), '\\x1b[33;1mSQL Table\\x1b[0m')\n", "def test_call_command_with_system_exit(self):\n    \"\"\"\n    Test that call_command propagates SystemExit exceptions.\n    \"\"\"\n        raise SystemExit('Test')\n\n    with mock.patch.object(RunserverCommand, 'handle', raise_system_exit):\n        with self.assertRaises(SystemExit):\n            call_command('runserver', verbosity=0)\n", "    def handle(self, *args, **options):\n        self.stdout.write(\"Test command executed\")\n"], "sample_497": ["def test_ticklabels_empty_list():\n    fig, ax = plt.subplots()\n    ax.set_xticks([1, 2, 3])\n    ax.set_xticklabels([])\n    assert len(ax.get_xticklabels()) == 3\n    for label in ax.get_xticklabels():\n        assert label.get_text() == ''\n", "def test_axis_get_ticks_position():\n    fig, ax = plt.subplots()\n    ax.set_xticks([0.1, 0.5, 0.9])\n    assert ax.xaxis.get_ticks_position() == \"bottom\"\n    ax.tick_top()\n    assert ax.xaxis.get_ticks_position() == \"top\"\n    ax.tick_bottom()\n    assert ax.xaxis.get_ticks_position() == \"bottom\"\n    ax.tick_params(axis='x', which='both', top=True, bottom=True)\n    assert ax.xaxis.get_ticks_position() == \"default\"\n    ax.tick_params(axis='x', which='both', top=False, bottom=False)\n    assert ax.xaxis.get_ticks_position() == \"unknown\"\n\n    ax.set_yticks([0.1, 0.5, 0.9])\n    assert ax.yaxis.get_ticks_position() == \"left\"\n    ax.tick_right()\n    assert ax.yaxis.get_ticks_position() == \"right\"\n    ax.tick_left()\n    assert ax.yaxis.get_ticks_position() == \"left\"\n    ax.tick_params(axis='y', which='both', left=True, right=True)\n    assert ax.yaxis.get_ticks_position() == \"default\"\n    ax.tick_params(axis='y', which='both', left=False, right=False)\n    assert ax.yaxis.get_ticks_position() == \"unknown\"\n", "def test_axis_get_ticks_position():\n    fig, ax = plt.subplots()\n    ax.xaxis.set_ticks_position('bottom')\n    assert ax.xaxis.get_ticks_position() == 'bottom'\n    ax.xaxis.set_ticks_position('top')\n    assert ax.xaxis.get_ticks_position() == 'top'\n    ax.xaxis.set_ticks_position('both')\n    assert ax.xaxis.get_ticks_position() == 'default'\n\n    ax.yaxis.set_ticks_position('left')\n    assert ax.yaxis.get_ticks_position() == 'left'\n    ax.yaxis.set_ticks_position('right')\n    assert ax.yaxis.get_ticks_position() == 'right'\n    ax.yaxis.set_ticks_position('both')\n    assert ax.yaxis.get_ticks_position() == 'default'\n", "def test_major_locator_view_limits():\n    fig, ax = plt.subplots()\n    ax.set_xlim(0, 10)\n    locator = mticker.MaxNLocator(nbins=5)\n    ax.xaxis.set_major_locator(locator)\n    ticks = ax.get_xticks()\n    assert_array_equal(ticks, np.array([0., 2.5, 5., 7.5, 10.]))\n    ax.set_xlim(-1, 11)\n    ticks = ax.get_xticks()\n    assert_array_equal(ticks, np.array([-1., 2., 5., 8., 11.]))\n", "def test_axis_get_tick_space():\n    fig, ax = plt.subplots()\n    ax.set_xlim(0, 10)\n    ax.set_ylim(0, 10)\n\n    # Test tick space calculation for x-axis\n    xaxis = ax.xaxis\n    assert isinstance(xaxis.get_tick_space(), int)\n\n    # Test tick space calculation for y-axis\n    yaxis = ax.yaxis\n    assert isinstance(yaxis.get_tick_space(), int)\n\n    plt.close(fig)\n"], "sample_498": ["def test_legend_draggable():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3], label='Test')\n    legend = ax.legend()\n    legend.set_draggable(True)\n    with mock.patch.object(mpl.backend_bases.FigureCanvasBase,\n                           'button_press_event') as bpe:\n        bpe.xdata = 0.5\n        bpe.ydata = 0.5\n        bpe.button = 1\n        legend._draggable.button_press_event(bpe)\n    with mock.patch.object(mpl.backend_bases.FigureCanvasBase,\n                           'motion_notify_event') as mme:\n        mme.xdata = 0.7\n        mme.ydata = 0.7\n        legend._draggable.motion_notify_event(mme)\n    with mock.patch.object(mpl.backend_bases.FigureCanvasBase,\n                           'button_release_event') as bre:\n        bre.xdata = 0.8\n        bre.ydata = 0.8\n        legend._draggable.button_release_event(bre)\n    return fig\n", "def test_legend_alignment(kwargs, expected_alignment):\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3], label=\"test\")\n    legend = ax.legend(**kwargs)\n    assert legend.get_alignment() == expected_alignment\n", "def test_legend_with_invalid_label():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2], label=None)\n    with pytest.raises(ValueError):\n        ax.legend()\n    ax.plot([1, 2], label='')\n    with pytest.raises(ValueError):\n        ax.legend()\n    ax.plot([1, 2], label='  ')\n    with pytest.raises(ValueError):\n        ax.legend()\n    ax.plot([1, 2], label='\\n')\n    with pytest.raises(ValueError):\n        ax.legend()\n", "def test_legend_alignment(alignment):\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3], label='test')\n    legend = ax.legend(alignment=alignment)\n    assert legend.get_alignment() == alignment\n", "def test_legend_alignment(alignment):\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3], label=\"Test\")\n    legend = ax.legend(alignment=alignment)\n    assert legend.get_alignment() == alignment\n    plt.close(fig)\n"], "sample_499": ["def test_legend_set_bbox_to_anchor():\n    # Test setting bbox_to_anchor using different methods\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3], label='Line')\n    legend = ax.legend()\n\n    # Set bbox_to_anchor using a tuple of (left, bottom)\n    legend.set_bbox_to_anchor((0.5, 0.5))\n    assert legend.get_bbox_to_anchor().x0 == 0.5\n    assert legend.get_bbox_to_anchor().y0 == 0.5\n\n    # Set bbox_to_anchor using a tuple of (left, bottom, width, height)\n    legend.set_bbox_to_anchor((0.5, 0.5, 0.2, 0.2))\n    assert legend.get_bbox_to_anchor().x0 == 0.5\n    assert legend.get_bbox_to_anchor().y0 == 0.5\n    assert legend.get_bbox_to_anchor().width == 0.2\n    assert legend.get_bbox_to_anchor().height == 0.2\n\n    # Set bbox_to_anchor using a BboxBase instance\n    bbox = mtransforms.Bbox.from_bounds(0.5, 0.5, 0.2, 0.2)\n    legend.set_bbox_to_anchor(bbox)\n    assert legend.get_bbox_to_anchor().x0 == 0.5\n    assert legend.get_bbox_to_anchor().y0 == 0.5\n    assert legend.get_bbox_to_anchor().width == 0.2\n    assert legend.get_bbox_to_anchor().height == 0.2\n\n    # Set bbox_to_anchor to None\n    legend.set_bbox_to_anchor(None)\n    assert legend.get_bbox_to_anchor() is None\n", "def test_legend_labelcolor():\n    fig, ax = plt.subplots()\n    line1, = ax.plot([1, 2], [1, 2], label='Line 1')\n    line2, = ax.plot([1, 2], [2, 1], label='Line 2')\n\n    legend = ax.legend(labelcolor='linecolor')\n    assert legend.texts[0].get_color() == line1.get_color()\n    assert legend.texts[1].get_color() == line2.get_color()\n\n    legend = ax.legend(labelcolor='markerfacecolor')\n    assert legend.texts[0].get_color() == line1.get_markerfacecolor()\n    assert legend.texts[1].get_color() == line2.get_markerfacecolor()\n\n    legend = ax.legend(labelcolor=['red', 'green'])\n    assert legend.texts[0].get_color() == 'red'\n    assert legend.texts[1].get_color() == 'green'\n\n    legend = ax.legend(labelcolor='none')\n    assert legend.texts[0].get_color() == 'none'\n    assert legend.texts[1].get_color() == 'none'\n", "def test_legend_with_fancybox_and_no_frame():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2], label='test')\n    legend = ax.legend(fancybox=True, frameon=False)\n    assert isinstance(legend.get_frame(), mpl.patches.FancyBboxPatch)\n    assert not legend.get_frame().get_visible()\n", "def test_legend_get_texts():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3], label='a')\n    ax.plot([4, 5, 6], label='b')\n    legend = ax.legend()\n    assert len(legend.get_texts()) == 2\n    plt.close(fig)\n", "def test_legend_loc(kwargs, expected_loc):\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3], label=\"test\")\n    legend = ax.legend(**kwargs)\n    assert legend._loc == expected_loc\n"], "sample_500": ["def test_colorbar_with_lines_at_custom_locations():\n    fig, ax = plt.subplots()\n    pcm = ax.pcolormesh(np.random.random((20, 20)))\n    cbar = fig.colorbar(pcm, ax=ax)\n    line_locations = [0.2, 0.5, 0.8]\n    cbar.add_lines(line_locations, ['r', 'g', 'b'], [1, 2, 3])\n    return fig\n", "def test_colorbar_with_negative_boundaries():\n    fig = plt.figure()\n    ax1 = fig.add_subplot(121)\n    ax2 = fig.add_subplot(122)\n\n    # Create a colormap with boundaries that include negative numbers.\n    cmap = cm.get_cmap(\"RdBu\", lut=5)\n    clevs = [-10, -5., -2.5, -.5, .5, 1.5, 3.5]\n    norm = BoundaryNorm(clevs, len(clevs) - 1)\n\n    # Generate the colorbar.\n    Colorbar(ax1, cmap=cmap, norm=norm,\n             boundaries=clevs, values=clevs[:-1],\n             extend='both', orientation='horizontal')\n\n    # Create another colorbar with the same colormap but without negative boundaries.\n    clevs = [0, 2.5, 5, 7.5, 10]\n    norm = BoundaryNorm(clevs, len(clevs) - 1)\n    Colorbar(ax2, cmap=cmap, norm=norm,\n             boundaries=clevs, values=clevs[:-1],\n             extend='both', orientation='horizontal')\n", "def test_colorbar_extension_length():\n    fig1 = _colorbar_extension_length('uniform')\n    fig2 = _colorbar_extension_length('proportional')\n    return fig1, fig2\n", "def test_colorbar_extension_length():\n    \"\"\"\n    Test the extension length of colorbars with uniform and proportional spacing.\n    \"\"\"\n    fig1 = _colorbar_extension_length('uniform')\n    fig2 = _colorbar_extension_length('proportional')\n    return fig1, fig2\n\n", "def test_colorbar_extension_length():\n    \"\"\"\n    Test colorbars with variable length extensions.\n    \"\"\"\n    fig_uniform = _colorbar_extension_length('uniform')\n    fig_proportional = _colorbar_extension_length('proportional')\n    return [fig_uniform, fig_proportional]\n"], "sample_501": ["def test_legend_frame_alpha():\n    fig, ax = plt.subplots()\n    line1, = ax.plot([1, 2, 3], label='Line 1')\n    line2, = ax.plot([2, 4, 6], label='Line 2')\n    legend = ax.legend(handles=[line1, line2], framealpha=0.5)\n    assert legend.get_frame().get_alpha() == 0.5\n    plt.close(fig)\n", "def test_legend_handler_map(tmpdir):\n    # Test that custom handler map is used correctly.\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3], label='Line')\n    ax.scatter([1, 2, 3], [1, 2, 3], label='Scatter')\n\n    class CustomHandler(HandlerTuple):\n                           x, y, width, height, fontsize,\n                           trans):\n            return [plt.Line2D([x], [y])]\n\n    handler_map = {mlines.Line2D: CustomHandler()}\n    with mock.patch.object(mlegend.Legend, 'get_legend_handler_map',\n                           return_value=handler_map):\n        leg = ax.legend()\n\n    assert isinstance(leg.legendHandles[0], mlines.Line2D)\n    assert not isinstance(leg.legendHandles[1], mlines.Line2D)\n\n    # Check that the default handler map is still used for other artists.\n    assert isinstance(leg.legendHandles[1], mcollections.PathCollection)\n", "def test_legend_get_default_handler_map():\n    # Test that the default handler map is correctly returned\n    handler_map = mlegend.Legend.get_default_handler_map()\n    assert isinstance(handler_map, dict)\n    # Test a few known handlers are in the map\n    assert mlines.Line2D in handler_map\n    assert mcollections.PathCollection in handler_map\n", "def test_legend_get_texts():\n    fig, ax = plt.subplots()\n    line1, = ax.plot([1, 2, 3], label='Line 1')\n    line2, = ax.plot([4, 5, 6], label='Line 2')\n    legend = ax.legend(handles=[line1, line2])\n    assert len(legend.get_texts()) == 2\n    for text in legend.get_texts():\n        assert isinstance(text, mpl.text.Text)\n    plt.close(fig)\n", "def test_legend_handles():\n    fig, ax = plt.subplots()\n    line1, = ax.plot([1, 2, 3], label='Line 1')\n    line2, = ax.plot([3, 2, 1], label='Line 2')\n\n    # Test that legend handles are correctly returned\n    handles = list(mlegend._get_legend_handles([ax]))\n    assert len(handles) == 2\n    assert handles[0] == line1\n    assert handles[1] == line2\n\n    # Test that labels are correctly returned\n    labels = [h.get_label() for h in handles]\n    assert labels == ['Line 1', 'Line 2']\n\n    # Test with multiple axes\n    fig, (ax1, ax2) = plt.subplots(2)\n    line1, = ax1.plot([1, 2, 3], label='Line 1')\n    line2, = ax2.plot([3, 2, 1], label='Line 2')\n\n    handles = list(mlegend._get_legend_handles([ax1, ax2]))\n    assert len(handles) == 2\n    assert handles[0] == line1\n    assert handles[1] == line2\n\n    plt.close('all')\n"], "sample_502": ["def test_switch_backend():\n    original_backend = mpl.get_backend()\n    plt.switch_backend('pdf')\n    assert mpl.get_backend() == 'pdf'\n    plt.switch_backend(original_backend)\n    assert mpl.get_backend() == original_backend\n", "def test_subplots():\n    # Create a figure and a set of subplots\n    fig, axs = plt.subplots(2, 2)\n    \n    # Check that the returned axes are stored in a 2D array\n    assert isinstance(axs, np.ndarray)\n    assert axs.shape == (2, 2)\n    \n    # Check that each axes object is an instance of Axes\n    for ax in axs.flat:\n        assert isinstance(ax, plt.Axes)\n        \n    # Close the figure to prevent resource leak\n    plt.close(fig)\n", "def test_pie():\n    # Create a pie chart and check the return value\n    fig, ax = plt.subplots()\n    wedges, texts, autotexts = ax.pie([1, 2, 3])\n    assert len(wedges) == 3\n    assert len(texts) == 3\n    assert len(autotexts) == 3\n    plt.close(fig)\n", "def test_xkcd():\n    with plt.xkcd():\n        fig, ax = plt.subplots()\n        assert fig.get_facecolor() == 'white'\n        assert ax.spines['bottom'].get_color() == 'black'\n        assert ax.spines['top'].get_color() == 'black'\n        assert ax.spines['left'].get_color() == 'black'\n        assert ax.spines['right'].get_color() == 'black'\n        assert ax.get_xticks()[0] == 0\n        assert ax.get_yticks()[0] == 0\n\n    # Test if the xkcd context manager properly restores the original settings.\n    fig, ax = plt.subplots()\n    original_facecolor = fig.get_facecolor()\n    original_bottom_spine_color = ax.spines['bottom'].get_color()\n    original_top_spine_color = ax.spines['top'].get_color()\n    original_left_spine_color = ax.spines['left'].get_color()\n    original_right_spine_color = ax.spines['right'].get_color()\n\n    with plt.xkcd():\n        pass  # Do nothing in this block.\n\n    assert fig.get_facecolor() == original_facecolor\n    assert ax.spines['bottom'].get_color() == original_bottom_spine_color\n    assert ax.spines['top'].get_color() == original_top_spine_color\n    assert ax.spines['left'].get_color() == original_left_spine_color\n    assert ax.spines['right'].get_color() == original_right_spine_color\n", "def test_subplots():\n    # Simple test of pyplot.subplots()\n    fig, axs = plt.subplots(2, 2)\n    assert len(axs) == 2\n    assert len(axs[0]) == 2\n\n    # Test with sharex and sharey\n    fig, axs = plt.subplots(2, 2, sharex=True, sharey=True)\n    ax = axs[0, 0]\n    assert ax._sharex == axs[1, 0]\n    assert ax._sharey == axs[0, 1]\n\n    # Test with squeeze=False\n    fig, axs = plt.subplots(1, 1, squeeze=False)\n    assert isinstance(axs, np.ndarray)\n    assert axs.shape == (1, 1)\n\n    plt.close('all')\n"], "sample_503": ["def test_Line2D_get_transformed_path():\n    line = mlines.Line2D([0, 1], [0, 1])\n    path = line._get_transformed_path()\n    assert isinstance(path, mtransforms.TransformedPath)\n    assert path.get_transform() == line.get_transform()\n", "def test_line_set_markevery():\n    line = mlines.Line2D([0, 1], [0, 1])\n    markevery = (0, 2)\n    line.set_markevery(markevery)\n    assert line.get_markevery() == markevery\n\n    markevery = 3\n    line.set_markevery(markevery)\n    assert line.get_markevery() == markevery\n\n    markevery = slice(1, 10, 2)\n    line.set_markevery(markevery)\n    assert line.get_markevery() == markevery\n\n    markevery = [1, 2, 3]\n    line.set_markevery(markevery)\n    assert line.get_markevery() == markevery\n\n    markevery = np.array([1, 2, 3])\n    line.set_markevery(markevery)\n    assert_array_equal(line.get_markevery(), markevery)\n\n    markevery = 0.1\n    line.set_markevery(markevery)\n    assert line.get_markevery() == markevery\n\n    markevery = (0.5, 0.1)\n    line.set_markevery(markevery)\n    assert line.get_markevery() == markevery\n\n    # Check that invalid values raise an error\n    with pytest.raises(ValueError):\n        line.set_markevery('invalid')\n", "def test_markevery_relative():\n    fig, ax = plt.subplots()\n    x = np.arange(10)\n    y = np.arange(10)\n    line, = ax.plot(x, y, markevery=(0.5, 0.1))\n    assert_array_equal(line.get_markevery(), (0.5, 0.1))\n    line.remove()\n    line, = ax.plot(x, y, markevery=0.1)\n    assert_array_equal(line.get_markevery(), (0, 0.1))\n", "def test_set_markevery():\n    line = mlines.Line2D([1, 2, 3], [1, 2, 3])\n    line.set_markevery((0, 1))\n    assert line.get_markevery() == (0, 1)\n\n    line.set_markevery(2)\n    assert line.get_markevery() == 2\n\n    line.set_markevery(None)\n    assert line.get_markevery() is None\n\n    with pytest.raises(ValueError):\n        line.set_markevery(\"invalid\")\n", "def test_line_set_data():\n    # Test setting data with scalar inputs.\n    line = mlines.Line2D(0, 1)\n    assert_array_equal(line.get_xdata(), np.array([0]))\n    assert_array_equal(line.get_ydata(), np.array([1]))\n\n    # Test setting data with list inputs.\n    line.set_data([0, 1], [2, 3])\n    assert_array_equal(line.get_xdata(), np.array([0, 1]))\n    assert_array_equal(line.get_ydata(), np.array([2, 3]))\n\n    # Test setting data with numpy array inputs.\n    line.set_data(np.array([4, 5]), np.array([6, 7]))\n    assert_array_equal(line.get_xdata(), np.array([4, 5]))\n    assert_array_equal(line.get_ydata(), np.array([6, 7]))\n\n    # Test setting data with numpy arrays of different lengths.\n    with pytest.raises(ValueError):\n        line.set_data(np.array([8, 9]), np.array([10]))\n"], "sample_504": ["def test_colorbar_extension_length():\n    \"\"\"\n    Produce 12 colorbars with variable length extensions for both uniform and\n    proportional spacing.\n\n    This test ensures that the extension lengths are correctly set in both cases.\n    \"\"\"\n    fig1 = _colorbar_extension_length('uniform')\n    fig2 = _colorbar_extension_length('proportional')\n    return fig1, fig2\n", "def test_colorbar_alpha():\n    fig, ax = plt.subplots()\n    cmap = cm.get_cmap(\"RdBu\")\n    norm = Normalize(vmin=0, vmax=1)\n    cbar = Colorbar(ax, cmap=cmap, norm=norm, alpha=0.5)\n    assert cbar.alpha == 0.5\n\n    # Test setting alpha with an array\n    alpha_array = np.array([0.2, 0.3, 0.4])\n    cbar.set_alpha(alpha_array)\n    assert cbar.alpha is None  # Alpha should be None when using an array\n\n    # Test setting alpha with a scalar value again\n    cbar.set_alpha(0.6)\n    assert cbar.alpha == 0.6\n", "def test_colorbar_extension_length():\n    \"\"\"\n    Test colorbar extension length for uniform and proportional spacing.\n\n    Produces 2 figures with 12 subplots each. Each figure represents a\n    different spacing (uniform or proportional). Within each figure, there\n    are four rows of three subplots each. The four rows represent different\n    extension types (neither, min, max, both), and the three columns\n    represent different extension lengths (default, 'auto', and a fraction).\n    \"\"\"\n    # Create figures with uniform and proportional spacing.\n    fig_uniform = _colorbar_extension_length('uniform')\n    fig_proportional = _colorbar_extension_length('proportional')\n    return [fig_uniform, fig_proportional]\n", "def test_colorbar_orientation():\n    \"\"\"\n    Test colorbar orientation.\n\n    Colorbars can be oriented either vertically or horizontally.\n    This test generates a vertical and a horizontal colorbar.\n    \"\"\"\n    fig = plt.figure()\n    ax1 = fig.add_subplot(2, 1, 1)\n    ax2 = fig.add_subplot(2, 1, 2)\n    cmap = cm.get_cmap(\"RdBu\")\n    norm = Normalize(vmin=-10, vmax=10)\n\n    Colorbar(ax1, cmap=cmap, norm=norm, orientation='vertical')\n    ax1.tick_params(left=False, labelleft=False,\n                    bottom=False, labelbottom=False)\n    Colorbar(ax2, cmap=cmap, norm=norm, orientation='horizontal')\n    ax2.tick_params(left=False, labelleft=False,\n                    bottom=False, labelbottom=False)\n", "def test_colorbar_extension_triangles():\n    \"\"\"\n    Test colorbars with triangular extensions for uniform and proportional\n    spacing.\n    \"\"\"\n    fig1 = _colorbar_extension_shape(spacing='uniform')\n    fig2 = _colorbar_extension_shape(spacing='proportional')\n    return fig1, fig2\n"], "sample_505": ["def test_date_converter(converter_cls):\n    # Create a DateConverter instance\n    converter = converter_cls()\n\n    # Test axisinfo method\n    axis_info = converter.axisinfo(None, None)\n    assert isinstance(axis_info, mticker.AxisInfo)\n    assert isinstance(axis_info.majloc, mdates.AutoDateLocator)\n    assert isinstance(axis_info.majfmt, (mdates.AutoDateFormatter, mdates.ConciseDateFormatter))\n\n    # Test default_units method\n    dt = datetime.datetime(2022, 1, 1)\n    assert converter.default_units(dt, None) is None\n\n    # Test convert method\n    assert converter.convert(dt, None, None) == mdates.date2num(dt)\n", "def test_date_converter_axisinfo(converter_cls):\n    converter = converter_cls()\n    axis = plt.gca().xaxis\n    unit = None\n\n    info = converter.axisinfo(unit, axis)\n\n    assert isinstance(info, mticker.AxisInfo)\n    assert isinstance(info.majloc, mdates.AutoDateLocator)\n    assert isinstance(info.majfmt, (mdates.AutoDateFormatter, mdates.ConciseDateFormatter))\n", "def test_date_ticker_factory():\n    # test that date_ticker_factory returns the expected tickers\n    locator, formatter = mdates.date_ticker_factory(365, tz=None)\n    assert isinstance(locator, mdates.YearLocator)\n    assert isinstance(formatter, mdates.DateFormatter)\n\n    locator, formatter = mdates.date_ticker_factory(30, tz=None)\n    assert isinstance(locator, mdates.MonthLocator)\n    assert isinstance(formatter, mdates.DateFormatter)\n\n    locator, formatter = mdates.date_ticker_factory(7, tz=None)\n    assert isinstance(locator, mdates.WeekdayLocator)\n    assert isinstance(formatter, mdates.DateFormatter)\n\n    locator, formatter = mdates.date_ticker_factory(1, tz=None)\n    assert isinstance(locator, mdates.DayLocator)\n    assert isinstance(formatter, mdates.DateFormatter)\n\n    locator, formatter = mdates.date_ticker_factory(1/24, tz=None)\n    assert isinstance(locator, mdates.HourLocator)\n    assert isinstance(formatter, mdates.DateFormatter)\n\n    locator, formatter = mdates.date_ticker_factory(1/(24*60), tz=None)\n    assert isinstance(locator, mdates.MinuteLocator)\n    assert isinstance(formatter, mdates.DateFormatter)\n", "def test_AutoDateLocator_single_tick():\n    # Test that AutoDateLocator returns at least two ticks.\n    locator = mdates.AutoDateLocator()\n    with rc_context({'date.autoformatter.year': '%Y'}):\n        fig, ax = plt.subplots()\n        ax.xaxis.set_major_locator(locator)\n        ax.plot([datetime.date(2020, 1, 1)])\n        ticks = ax.get_xticks()\n        assert len(ticks) > 1\n        plt.close(fig)\n", "def test_RRuleLocator_get_unit_generic(freq):\n    assert isinstance(mdates.RRuleLocator.get_unit_generic(freq), (int, float))\n"], "sample_506": ["def test_spines_set_position(fig_test, fig_ref):\n    fig_test.subplots()\n    fig_ref.subplots()\n\n    fig_test.axes[0].spines.top.set_position((\"axes\", 0.5))\n    fig_test.axes[0].spines.bottom.set_position((\"data\", 1))\n\n    fig_ref.axes[0].spines.top.set_position((\"axes\", 0.5))\n    fig_ref.axes[0].spines.bottom.set_position((\"data\", 1))\n\n    assert fig_test.axes[0].spines.top.get_position() == (\"axes\", 0.5)\n    assert fig_test.axes[0].spines.bottom.get_position() == (\"data\", 1)\n\n    # Check that the spines are drawn in the correct position\n    fig_test.canvas.draw()\n    fig_ref.canvas.draw()\n", "def test_spine_position():\n    fig, ax = plt.subplots()\n    spine = ax.spines['left']\n    spine.set_position(('outward', 10))\n    assert spine.get_position() == ('outward', 10)\n    spine.set_position(('axes', 0.5))\n    assert spine.get_position() == ('axes', 0.5)\n    spine.set_position(('data', 1.0))\n    assert spine.get_position() == ('data', 1.0)\n    spine.set_position('center')\n    assert spine.get_position() == ('axes', 0.5)\n    spine.set_position('zero')\n    assert spine.get_position() == ('data', 0)\n", "def test_spines_set_bounds(fig_test, fig_ref):\n    fig_test.subplots()\n    ax = fig_test.axes[0]\n    ax.spines['left'].set_bounds(0, 1)\n    ax.spines['bottom'].set_bounds(0, 1)\n\n    fig_ref.subplots()\n    ax = fig_ref.axes[0]\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n", "def test_spines_proxy():\n    fig, ax = plt.subplots()\n    spines = Spines.from_dict(ax.spines)\n    proxy = spines[['top', 'right']]\n\n    # Test setting a valid attribute\n    proxy.set_visible(False)\n    assert not ax.spines['top'].get_visible()\n    assert not ax.spines['right'].get_visible()\n\n    # Test setting an invalid attribute\n    with pytest.raises(AttributeError):\n        proxy.set_invalid_attribute(True)\n\n    # Test that the proxy doesn't have any other methods\n    with pytest.raises(AttributeError):\n        proxy.get_visible()\n\n    plt.close(fig)\n", "def test_spine_position():\n    fig, ax = plt.subplots()\n    spine = ax.spines['left']\n\n    # Test default position\n    assert spine.get_position() == ('outward', 0)\n\n    # Test setting position with tuple\n    spine.set_position(('axes', 0.5))\n    assert spine.get_position() == ('axes', 0.5)\n\n    # Test setting position with string\n    spine.set_position('center')\n    assert spine.get_position() == ('axes', 0.5)\n\n    # Test setting position with invalid string\n    with pytest.raises(ValueError):\n        spine.set_position('invalid')\n\n    # Test setting position with invalid tuple\n    with pytest.raises(ValueError):\n        spine.set_position(('invalid', 0.5))\n"], "sample_507": ["def test_update_check_isinstance(self):\n    unit_data = cat.UnitData()\n    with pytest.raises(TypeError):\n        unit_data.update([1, 2, 3])\n", "def test_update_with_convertible_strings():\n    unit_data = cat.UnitData()\n    data = [\"1.2\", \"3.4\", \"5.6\"]\n    unit_data.update(data)\n    assert len(unit_data._mapping) == 3\n    assert all(key in unit_data._mapping for key in data)\n    # Check that convertible strings are properly mapped to integers.\n    for i, key in enumerate(unit_data._mapping):\n        assert unit_data._mapping[key] == i\n", "def test_update_with_convertible_values(self):\n    unit_data = cat.UnitData()\n    values = [\"1.0\", \"2.0\", \"3.0\"]\n    with pytest.warns(UserWarning) as record:\n        unit_data.update(values)\n    assert len(record) == 1\n    assert \"Using categorical units to plot a list of strings\" in str(record[0].message)\n    assert len(unit_data._mapping) == 3\n", "def test_unitdata_update():\n    unit = cat.UnitData()\n    data1 = ['A', 'B', 'C']\n    data2 = ['D', 'E', 'F']\n\n    unit.update(data1)\n    assert len(unit._mapping) == 3\n\n    unit.update(data2)\n    assert len(unit._mapping) == 6\n\n    # Check that updating with the same data doesn't change anything\n    unit.update(data1)\n    assert len(unit._mapping) == 6\n\n    # Check that updating with non-string data raises an error\n    with pytest.raises(TypeError):\n        unit.update([1, 2, 3])\n", "def test_unit_data_update():\n    unit_data = cat.UnitData()\n    unit_data.update(['A', 'B', 'C'])\n    assert list(unit_data._mapping.keys()) == ['A', 'B', 'C']\n    assert list(unit_data._mapping.values()) == [0, 1, 2]\n\n    unit_data.update(['D', 'E', 'A'])\n    assert list(unit_data._mapping.keys()) == ['A', 'B', 'C', 'D', 'E']\n    assert list(unit_data._mapping.values()) == [0, 1, 2, 3, 4]\n"], "sample_508": ["def test_artist_aliases():\n    fig, ax = plt.subplots()\n    line, = ax.plot([1, 2, 3])\n    aliases = {\n        'lw': 'linewidth',\n        'ls': 'linestyle',\n        'c': 'color',\n        'fc': 'facecolor',\n        'ec': 'edgecolor',\n        'mew': 'markeredgewidth',\n        'mec': 'markeredgecolor',\n        'mfc': 'markerfacecolor',\n    }\n    for alias, prop in aliases.items():\n        assert (getattr(line, f'get_{alias}')()\n                == getattr(line, f'get_{prop}')())\n        val = 'red' if alias in ['c', 'fc', 'ec', 'mec', 'mfc'] else 2\n        setattr(line, alias, val)\n        assert (getattr(line, f'get_{alias}')()\n                == getattr(line, f'get_{prop}')())\n", "def test_artist_setp():\n    fig, ax = plt.subplots()\n    line, = ax.plot([1, 2, 3])\n    martist.setp(line, 'linewidth', 2, 'color', 'r')\n    assert line.get_linewidth() == 2\n    assert line.get_color() == 'r'\n\n    # Test MATLAB style string/value pairs.\n    martist.setp(line, 'linestyle', '--', 'marker', 'o')\n    assert line.get_linestyle() == '--'\n    assert line.get_marker() == 'o'\n\n    # Test setting multiple properties at once with keyword arguments.\n    martist.setp(line, linewidth=4, color='g')\n    assert line.get_linewidth() == 4\n    assert line.get_color() == 'g'\n", "def test_artist_setp():\n    fig, ax = plt.subplots()\n    line, = ax.plot([1, 2, 3])\n\n    # Test setting a property with setp\n    martist.setp(line, linewidth=5)\n    assert line.get_linewidth() == 5\n\n    # Test getting a property with getp\n    assert martist.getp(line, 'linewidth') == 5\n\n    # Test listing all properties with getp\n    output = io.StringIO()\n    martist.getp(line, file=output)\n    assert 'linewidth' in output.getvalue()\n\n    # Test setting multiple properties at once\n    martist.setp(line, color='red', linestyle='--')\n    assert line.get_color() == 'red'\n    assert line.get_linestyle() == '--'\n\n    # Test MATLAB style string/value pairs\n    martist.setp(line, 'marker', 'o', 'markersize', 10)\n    assert line.get_marker() == 'o'\n    assert line.get_markersize() == 10\n", "def test_artist_setp():\n    fig, ax = plt.subplots()\n    line = mlines.Line2D([0, 1], [0, 1])\n    ax.add_line(line)\n\n    # Test setting a single property using MATLAB style.\n    martist.setp(line, 'linewidth', 2)\n    assert line.get_linewidth() == 2\n\n    # Test setting multiple properties using Python style.\n    martist.setp(line, linewidth=3, color='r')\n    assert line.get_linewidth() == 3\n    assert line.get_color() == 'r'\n\n    # Test listing allowed values for a property.\n    with io.StringIO() as file:\n        martist.setp(line, 'linestyle', file=file)\n        file.seek(0)\n        output = file.read()\n    assert 'linestyle:' in output\n\n    # Test listing all properties that can be set.\n    with io.StringIO() as file:\n        martist.setp(line, file=file)\n        file.seek(0)\n        output = file.read()\n    assert 'agg_filter:' in output\n", "def test_artist_setp_invalid_arguments():\n    fig, ax = plt.subplots()\n    line, = ax.plot([1, 2, 3])\n\n    with pytest.raises(ValueError):\n        martist.setp(line, 'non_existent_property', 42)\n\n    with pytest.raises(TypeError):\n        martist.setp(line, 123)\n"], "sample_509": ["def test_date_ticker_factory():\n    span = 10\n    tz = dateutil.tz.gettz('UTC')\n    locator, formatter = mdates.date_ticker_factory(span, tz=tz)\n    assert isinstance(locator, mdates.DayLocator)\n    assert isinstance(formatter, mdates.DateFormatter)\n", "def test_date_converter(converter):\n    rc_context = {'date.converter': converter}\n    with plt.style.context(rc_context):\n        fig, ax = plt.subplots()\n        dates = [datetime.datetime(2020, 1, 1), datetime.datetime(2020, 1, 2)]\n        ax.plot(dates, [1, 2])\n        ax.xaxis.set_major_locator(mdates.AutoDateLocator())\n        ax.xaxis.set_major_formatter(mdates.AutoDateFormatter(ax.xaxis.get_major_locator()))\n        plt.draw()\n        assert len(ax.xaxis.get_ticklabels()) > 0\n        for tick in ax.xaxis.get_ticklabels():\n            assert tick.get_text() != \"\"\n", "def test_date_ticker_factory(tz):\n    span = 365  # days\n    locator, formatter = mdates.date_ticker_factory(span, tz=tz)\n    assert isinstance(locator, mdates.YearLocator)\n    assert isinstance(formatter, mdates.DateFormatter)\n\n    span = 30  # days\n    locator, formatter = mdates.date_ticker_factory(span, tz=tz)\n    assert isinstance(locator, mdates.MonthLocator)\n    assert isinstance(formatter, mdates.DateFormatter)\n\n    span = 7  # days\n    locator, formatter = mdates.date_ticker_factory(span, tz=tz)\n    assert isinstance(locator, mdates.WeekdayLocator)\n    assert isinstance(formatter, mdates.DateFormatter)\n\n    span = 1  # day\n    locator, formatter = mdates.date_ticker_factory(span, tz=tz)\n    assert isinstance(locator, mdates.DayLocator)\n    assert isinstance(formatter, mdates.DateFormatter)\n\n    span = 1 / 24  # hour\n    locator, formatter = mdates.date_ticker_factory(span, tz=tz)\n    assert isinstance(locator, mdates.HourLocator)\n    assert isinstance(formatter, mdates.DateFormatter)\n\n    span = 1 / (24 * 60)  # minute\n    locator, formatter = mdates.date_ticker_factory(span, tz=tz)\n    assert isinstance(locator, mdates.MinuteLocator)\n    assert isinstance(formatter, mdates.DateFormatter)\n", "def test_date_ticker_factory():\n    with rc_context({'timezone': 'UTC'}):\n        fig, ax = plt.subplots()\n        ax.xaxis.set_major_locator(mdates.AutoDateLocator())\n        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n\n        # Simulate some data\n        dates = np.array([\n            datetime.datetime(2022, 1, 1),\n            datetime.datetime(2022, 6, 15),\n            datetime.datetime(2023, 1, 1)\n        ])\n        ax.plot(dates, [1, 2, 3])\n\n        # Set the x-axis limits to the simulated data range\n        ax.set_xlim(dates[0], dates[-1])\n\n        # Test date_ticker_factory with numticks=5\n        locator, formatter = mdates.date_ticker_factory(\n            (dates[-1] - dates[0]).days, tz=dateutil.tz.UTC, numticks=5\n        )\n        ax.xaxis.set_major_locator(locator)\n        ax.xaxis.set_major_formatter(formatter)\n\n        # Check if the tick labels are as expected\n        ticks = ax.get_xticks()\n        labels = [formatter(x, pos=None) for x in ticks]\n        assert len(labels) == 5\n        assert labels[0] == '2022'\n        assert labels[-1] == '2023'\n\n        # Clean up\n        plt.close(fig)\n", "def test_date_ticker_factory():\n    span = 366  # days\n    loc, fmt = mdates.date_ticker_factory(span)\n    assert isinstance(loc, mdates.YearLocator)\n    assert fmt == '%Y'\n"], "sample_510": ["def test_xkcd():\n    fig, ax = plt.subplots()\n    with pytest.warns(MatplotlibDeprecationWarning):\n        plt.xkcd()\n    assert fig.get_figwidth() == 6.0\n    assert ax.get_xlim() == (0.0, 1.0)\n", "def test_subplots_kwargs():\n    fig, ax = plt.subplots(1, 2, sharex=True)\n    assert ax[0].get_shared_x_axes().joined(ax[0], ax[1])\n    fig, ax = plt.subplots(2, 1, sharey=True)\n    assert ax[0].get_shared_y_axes().joined(ax[0], ax[1])\n    fig, ax = plt.subplots(2, 2, sharex='col', sharey='row')\n    for a in ax:\n        assert a[0].get_shared_y_axes().joined(a[0], a[1])\n    for a in zip(*ax):\n        assert a[0].get_shared_x_axes().joined(a[0], a[1])\n", "def test_tight_layout():\n    # Test that using tight_layout still works with pyplot\n    fig, axs = plt.subplots(2, 2)\n    for ax in axs.flat:\n        ax.plot([1, 2, 3])\n    fig.tight_layout()\n    # Check that there are no overlaps\n    fig.canvas.draw()\n    assert not any(ax.bbox.fully_overlaps(other.bbox)\n                   for ax in axs.flat for other in axs.flat if ax is not other)\n", "def test_switch_backend():\n    # This test should be skipped if there is no display, otherwise it will fail.\n    pytest.importorskip(\"matplotlib.backends.backend_gtk3agg\")\n    orig_backend = plt.get_backend()\n    plt.switch_backend('gtk3agg')\n    assert plt.get_backend() == 'gtk3agg'\n    plt.switch_backend(orig_backend)\n    assert plt.get_backend() == orig_backend\n", "def test_subplot_mosaic():\n    fig, axd = plt.subplot_mosaic([['left', 'right'], ['bottom', 'bottom']],\n                                  empty_sentinel='.')\n    assert len(axd) == 3\n    assert isinstance(axd['left'], mpl.axes.Axes)\n    assert isinstance(axd['right'], mpl.axes.Axes)\n    assert isinstance(axd['bottom'], mpl.axes.Axes)\n    plt.close(fig)\n"], "sample_511": ["def test_figure():\n    # Create a figure with a specified size\n    fig = plt.figure(figsize=(8, 6))\n\n    # Check that the figure has the correct size\n    assert fig.get_size_inches() == (8, 6)\n\n    # Create a figure with a specified dpi\n    fig = plt.figure(dpi=100)\n\n    # Check that the figure has the correct dpi\n    assert fig.dpi == 100\n\n    # Create a figure with a specified facecolor\n    fig = plt.figure(facecolor='red')\n\n    # Check that the figure has the correct facecolor\n    assert fig.get_facecolor() == (1.0, 0.0, 0.0, 1.0)\n\n    # Create a figure with a specified edgecolor\n    fig = plt.figure(edgecolor='blue')\n\n    # Check that the figure has the correct edgecolor\n    assert fig.get_edgecolor() == (0.0, 0.0, 1.0, 1.0)\n", "def test_switch_backend():\n    old_backend = mpl.get_backend()\n    plt.switch_backend('pdf')\n    assert mpl.get_backend() == 'pdf'\n    plt.switch_backend(old_backend)\n    assert mpl.get_backend() == old_backend\n", "def test_switch_backend_interactive_framework():\n    # Ensure that the right exception is raised if we try to switch to an\n    # interactive backend from a non-main thread.\n    import threading\n    import matplotlib.backends.backend_agg\n\n        with pytest.raises(ImportError):\n            plt.switch_backend(\"Qt5Agg\")\n\n    thread = threading.Thread(target=try_switch_backend)\n    thread.start()\n    thread.join()\n\n    # The following should not raise, even though we're not in the main thread,\n    # because 'agg' is a non-interactive backend.\n    plt.switch_backend(\"agg\")\n    assert isinstance(matplotlib.get_backend(), matplotlib.backends.backend_agg.FigureCanvasAgg)\n", "def test_pyplot_using_subplots_context_manager():\n    fig, axs = plt.subplots(2)\n    with axs[0]:\n        plt.plot([1, 2])\n    assert len(axs[0].lines) == 1\n    assert len(axs[1].lines) == 0\n    with axs[1]:\n        plt.plot([3, 4])\n    assert len(axs[0].lines) == 1\n    assert len(axs[1].lines) == 1\n    with axs[0]:\n        plt.plot([5, 6])\n    assert len(axs[0].lines) == 2\n    assert len(axs[1].lines) == 1\n", "def test_pyplot_connect():\n    fig, ax = plt.subplots()\n        pass\n    cid = plt.connect('button_press_event', on_press)\n    assert isinstance(cid, int)\n    plt.disconnect(cid)\n    with pytest.raises(Exception):\n        plt.disconnect(cid)\n    plt.close(fig)\n"], "sample_512": ["def test_switch_backend_interactive_framework(monkeypatch):\n    monkeypatch.setattr(mpl, '_get_running_interactive_framework', lambda: 'qt')\n    with pytest.raises(ImportError):\n        plt.switch_backend('gtk3agg')\n    monkeypatch.setattr(mpl, '_get_running_interactive_framework', lambda: None)\n    plt.switch_backend('gtk3agg')  # should not raise\n", "def test_set_cmap():\n    plt.set_cmap('viridis')\n    assert plt.rcParams['image.cmap'] == 'viridis'\n    plt.close('all')\n", "def test_subplots_sharex():\n    fig, axs = plt.subplots(2, 1, sharex=True)\n    axs[0].plot([1, 2, 3])\n    axs[1].plot([1, 2, 3])\n    assert axs[0].get_xaxis().majorticklabels == []\n    assert axs[1].get_xaxis().majorticklabels != []\n    plt.close(fig)\n", "def test_set_cmap():\n    # Test setting colormap with name\n    plt.set_cmap('viridis')\n    assert mpl.rcParams['image.cmap'] == 'viridis'\n\n    # Test setting colormap with object\n    cmap = mpl.cm.get_cmap('plasma')\n    plt.set_cmap(cmap)\n    assert mpl.rcParams['image.cmap'] == 'plasma'\n", "def test_pyplot_with_statement():\n    fig, ax = plt.subplots()\n    assert plt.gcf() is fig\n    with plt.ioff():\n        assert not plt.isinteractive()\n        new_fig = plt.figure()\n        assert plt.gcf() is new_fig\n    assert plt.gcf() is new_fig\n    plt.close('all')\n"], "sample_513": ["def test_legend_location(loc):\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3], label='test')\n    ax.legend(loc=loc)\n    assert ax.get_legend()._loc == loc\n    plt.close(fig)\n", "def test_legend_auto1():\n    'Test automatic legend placement'\n    fig, ax = plt.subplots()\n    x = np.arange(100)\n    for i in range(100):\n        ax.plot(x, i + x)\n\n    ax.legend(loc='best')\n    return fig\n", "def test_legend_labelcolor():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2], [1, 2], label='test', color='red')\n    leg = ax.legend(labelcolor='linecolor')\n    assert leg.texts[0].get_color() == 'red'\n    plt.close(fig)\n\n    fig, ax = plt.subplots()\n    ax.plot([1, 2], [1, 2], label='test', color='red')\n    leg = ax.legend(labelcolor='mfc')\n    assert leg.texts[0].get_color() == 'red'\n    plt.close(fig)\n\n    fig, ax = plt.subplots()\n    ax.plot([1, 2], [1, 2], label='test', color='red')\n    leg = ax.legend(labelcolor='none')\n    assert leg.texts[0].get_color() == 'none'\n    plt.close(fig)\n", "def test_legend_with_fancybox():\n    # Test legend with fancybox=True\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3], label='Line 1')\n    ax.plot([2, 4, 6], label='Line 2')\n    ax.legend(fancybox=True)\n    assert ax.get_legend().get_frame().get_boxstyle().pad == 0\n", "def test_legend_alignment(alignment, expected):\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3], label=\"test\")\n    legend = ax.legend(alignment=alignment)\n    assert legend.get_alignment() == expected\n"], "sample_514": ["def test_colorbar_with_log_norm():\n    \"\"\"\n    Test colorbar with a logarithmic norm.\n    \"\"\"\n    fig, ax = plt.subplots()\n    cmap = cm.get_cmap(\"RdBu\")\n    norm = LogNorm(vmin=1e-3, vmax=1e3)\n    Colorbar(ax, cmap=cmap, norm=norm, orientation='horizontal')\n    assert ax.get_xaxis().get_scale() == 'log'\n", "def test_colorbar_scale(fig_test, fig_ref):\n    data = np.linspace(1, 1000, 100)\n    fig_test.subplots()\n    fig_ref.subplots()\n\n    norm = LogNorm()\n    Colorbar(fig_test.axes[0], cmap='viridis', norm=norm)\n    fig_test.axes[0].set_yscale('log')\n\n    norm = PowerNorm(0.5)\n    Colorbar(fig_ref.axes[0], cmap='viridis', norm=norm)\n    fig_ref.axes[0].set_yscale('power', exponent=0.5)\n\n    # Check that setting the scale via set_yscale has the same effect as\n    # passing a norm with a _scale attribute.\n    Colorbar(fig_test.axes[0], cmap='viridis')\n    fig_test.axes[0].set_yscale('log')\n", "def test_colorbar_extension_fraction():\n    \"\"\"\n    Test that the extension fraction is correctly calculated.\n    \"\"\"\n    fig, ax = plt.subplots()\n    cmap = cm.get_cmap(\"RdBu\", lut=5)\n    clevs = [-5., -2.5, -.5, .5, 1.5, 3.5]\n    norm = BoundaryNorm(clevs, len(clevs) - 1)\n    boundaries = values = norm.boundaries\n    values = values[:-1]\n\n    # Test with extendfrac=None (default)\n    cbar = Colorbar(ax, cmap=cmap, norm=norm,\n                    boundaries=boundaries, values=values,\n                    extend='both', orientation='horizontal')\n    assert cbar.extendfrac == [0.05, 0.05]\n\n    # Test with extendfrac='auto'\n    cbar = Colorbar(ax, cmap=cmap, norm=norm,\n                    boundaries=boundaries, values=values,\n                    extend='both', extendfrac='auto',\n                    orientation='horizontal')\n    assert cbar.extendfrac == [0.1111111111111111, 0.1111111111111111]\n\n    # Test with extendfrac as a single value\n    cbar = Colorbar(ax, cmap=cmap, norm=norm,\n                    boundaries=boundaries, values=values,\n                    extend='both', extendfrac=0.1,\n                    orientation='horizontal')\n    assert cbar.extendfrac == [0.1, 0.1]\n\n    # Test with extendfrac as a list of two values\n    cbar = Colorbar(ax, cmap=cmap, norm=norm,\n                    boundaries=boundaries, values=values,\n                    extend='both', extendfrac=[0.1, 0.2],\n                    orientation='horizontal')\n    assert cbar.extendfrac == [0.1, 0.2]\n\n    plt.close(fig)\n", "def test_colorbar_extension_length():\n    \"\"\"\n    Test colorbar extension length with uniform and proportional spacing.\n\n    Produces 24 subplots, each containing a horizontal colorbar with a\n    different combination of extensions and extension lengths.\n    \"\"\"\n    fig1 = _colorbar_extension_length('uniform')\n    fig2 = _colorbar_extension_length('proportional')\n    return [fig1, fig2]\n", "def test_colorbar_extension_length():\n    fig1 = _colorbar_extension_length('uniform')\n    fig2 = _colorbar_extension_length('proportional')\n    return fig1, fig2\n"], "sample_515": ["def test_colorbar_ticklabels_rotation(fig_test, fig_ref):\n    ax_test = fig_test.add_subplot(1, 1, 1)\n    ax_ref = fig_ref.add_subplot(1, 1, 1)\n\n    cmap = mpl.colormaps[\"RdBu\"]\n    norm = Normalize(vmin=0, vmax=10)\n\n    Colorbar(ax_test, cmap=cmap, norm=norm, orientation='horizontal')\n    ax_test.tick_params(axis='x', labelrotation=45)\n\n    Colorbar(ax_ref, cmap=cmap, norm=norm, orientation='horizontal')\n    ax_ref.set_xticklabels(ax_ref.get_xticks(), rotation=45)\n", "def test_colorbar_remove(fig_test, fig_ref):\n    # Create a figure and axis object.\n    ax = fig_test.add_subplot(111)\n    im = ax.pcolormesh(np.random.rand(10, 10))\n    cbar = fig_test.colorbar(im)\n    assert cbar.ax in fig_test.axes\n    cbar.remove()\n    assert cbar.ax not in fig_test.axes\n\n    # Reference figure should have no colorbar.\n    ax = fig_ref.add_subplot(111)\n    im = ax.pcolormesh(np.random.rand(10, 10))\n", "def test_colorbar_lognorm():\n    fig, ax = plt.subplots()\n    data = np.random.rand(10, 10)\n    norm = LogNorm(vmin=0.1, vmax=10)\n    cax = ax.imshow(data, norm=norm)\n    fig.colorbar(cax, ax=ax, orientation='vertical')\n", "def test_colorbar_extension_fraction(fig_test, fig_ref):\n    \"\"\"\n    Test colorbars with variable extension fraction.\n    \"\"\"\n    # Create a colormap and specify the levels it represents.\n    cmap = mpl.colormaps[\"RdBu\"].resampled(5)\n    clevs = [-5., -2.5, -.5, .5, 1.5, 3.5]\n    # Define a norm for the colormaps.\n    norm = BoundaryNorm([-10] + clevs[1:-1] + [10], len(clevs) - 1)\n    boundaries = values = norm.boundaries\n    values = values[:-1]\n\n    # Create a subplot.\n    ax_test = fig_test.add_subplot(1, 1, 1)\n    ax_ref = fig_ref.add_subplot(1, 1, 1)\n\n    # Generate the colorbar.\n    Colorbar(ax_test, cmap=cmap, norm=norm,\n             boundaries=boundaries, values=values,\n             extend='both', extendfrac=[0.05, 0.1],\n             orientation='horizontal')\n\n    # Generate a reference colorbar with equal extensions.\n    Colorbar(ax_ref, cmap=cmap, norm=norm,\n             boundaries=boundaries, values=values,\n             extend='both', extendfrac=0.075,\n             orientation='horizontal')\n", "def test_colorbar_alpha(fig_test, fig_ref):\n    \"\"\"\n    Test that the colorbar's alpha is properly set.\n    \"\"\"\n    fig_test.subplots_adjust(bottom=0.25)\n    ax1 = fig_test.add_subplot(221)\n    ax2 = fig_test.add_subplot(222)\n    ax3 = fig_test.add_subplot(223)\n    ax4 = fig_test.add_subplot(224)\n\n    data = np.random.rand(30, 40)\n\n    norm = Normalize(vmin=0., vmax=1.)\n\n    cmap = cm.get_cmap(\"RdYlGn\")\n    im1 = ax1.imshow(data, cmap=cmap, norm=norm)\n    im2 = ax2.imshow(data, cmap=cmap, norm=norm)\n    im3 = ax3.imshow(data, cmap=cmap, norm=norm)\n    im4 = ax4.imshow(data, cmap=cmap, norm=norm)\n\n    fig_test.colorbar(im1, ax=ax1)\n    fig_test.colorbar(im2, ax=ax2)\n    fig_test.colorbar(im3, ax=ax3).set_alpha(0.5)\n    fig_test.colorbar(im4, ax=ax4).set_alpha([0.5]*len(data))\n\n    # Create a reference figure with expected alpha value\n    fig_ref.subplots_adjust(bottom=0.25)\n    ax1_ref = fig_ref.add_subplot(221)\n    ax2_ref = fig_ref.add_subplot(222)\n    ax3_ref = fig_ref.add_subplot(223)\n    ax4_ref = fig_ref.add_subplot(224)\n\n    im1_ref = ax1_ref.imshow(data, cmap=cmap, norm=norm)\n    im2_ref = ax2_ref.imshow(data, cmap=cmap, norm=norm)\n    im3_ref = ax3_ref.imshow(data, cmap=cmap, norm=norm)\n    im4_ref = ax4_ref.imshow(data, cmap=cmap, norm=norm)\n\n    fig_ref.colorbar(im1_ref, ax=ax1_ref)\n    fig_ref.colorbar(im2_ref, ax=ax2_ref)\n    fig_ref.colorbar(im3_ref, ax=ax3_ref).set_alpha(0.5)\n    fig_ref.colorbar(im4_ref, ax=ax4_ref).set_alpha([0.5]*len(data))\n"], "sample_516": ["def test_pdf_pages_metadata():\n    metadata = {\n        'Title': 'My plot',\n        'Author': 'Me',\n        'Subject': 'Nothing interesting',\n        'Keywords': 'foo, bar',\n        'Creator': None,\n    }\n    with NamedTemporaryFile(suffix='.pdf') as tmp:\n        with PdfPages(tmp.name, metadata=metadata) as pdf:\n            fig, ax = plt.subplots()\n            ax.plot([1, 2, 3])\n            pdf.savefig(fig)\n        # Use PyPDF2 to check the generated PDF.\n        import PyPDF2\n        with open(tmp.name, 'rb') as f:\n            pdf_reader = PyPDF2.PdfFileReader(f)\n            info = pdf_reader.getDocumentInfo()\n            assert info['/Title'] == metadata['Title']\n            assert info['/Author'] == metadata['Author']\n            assert info['/Subject'] == metadata['Subject']\n            assert info['/Keywords'] == metadata['Keywords']\n            assert info['/Creator'] != metadata['Creator']\n", "def test_create_pdf_info_dict():\n    metadata = {\n        'Title': 'Test',\n        'Author': 'Me',\n        'Subject': 'Nothing',\n        'Keywords': 'test,me,nothing',\n        'Creator': None,\n        'Producer': None,\n        'CreationDate': datetime.datetime(2022, 1, 1),\n        'ModDate': datetime.datetime(2022, 1, 2),\n        'Trapped': 'True'\n    }\n    info = PdfPages.create_pdf_info_dict('pdf', metadata)\n    assert info['Creator'] == f'Matplotlib v{mpl.__version__}, https://matplotlib.org'\n    assert info['Producer'] == f'Matplotlib pdf backend v{mpl.__version__}'\n    assert isinstance(info['CreationDate'], str)\n    assert info['Trapped'] == Name('True')\n", "def test_get_subsetted_psname():\n    font_path = findfont(\"DejaVu Sans\", fontext='ttf')\n    font = FT2Font(font_path)\n    charmap = font.get_charmap()\n    ps_name = PdfFile._get_subsetted_psname(font.postscript_name, charmap)\n    assert len(ps_name) > 0\n    assert isinstance(ps_name, str)\n", "def test_pdf_pages_metadata(tmpdir):\n    filename = str(tmpdir.join('metadata.pdf'))\n    with PdfPages(filename, metadata={'Author': 'Me'}) as pdf:\n        fig, ax = plt.subplots()\n        ax.plot([1, 2, 3])\n        pdf.savefig(fig)\n    with open(filename, 'rb') as f:\n        contents = f.read().decode('latin-1')\n    assert '/Author (Me)' in contents\n", "def test_pdf_pages_metadata():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3])\n\n    metadata = {\n        'Title': 'Test PDF',\n        'Author': 'Matplotlib',\n        'Subject': 'PDF Metadata Test',\n        'Keywords': 'Matplotlib, PDF',\n        'CreationDate': datetime.datetime(2022, 1, 1),\n        'ModDate': datetime.datetime(2022, 1, 2),\n    }\n\n    with NamedTemporaryFile(suffix='.pdf') as tmpfile:\n        with PdfPages(tmpfile.name, metadata=metadata) as pdf:\n            pdf.savefig(fig)\n\n        # Check if the metadata is correctly written to the PDF file\n        with open(tmpfile.name, 'rb') as f:\n            pdf_content = f.read()\n\n        # Check for the metadata in the PDF content\n        for key, value in metadata.items():\n            if isinstance(value, datetime.datetime):\n                value = value.strftime('%Y%m%d%H%M%S')\n            assert str(value).encode() in pdf_content\n"], "sample_517": ["def test_text_get_position():\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, 'test')\n    assert_almost_equal(text.get_position(), (0.5, 0.5))\n    text.set_position((0.7, 0.2))\n    assert_almost_equal(text.get_position(), (0.7, 0.2))\n    plt.close(fig)\n", "def test_annotation_wrap():\n    fig, ax = plt.subplots()\n    ann = ax.annotate(\"This is a very long text that should wrap\",\n                      xy=(0.5, 0.5), xycoords='data',\n                      xytext=(0.5, 0.7), textcoords='data',\n                      arrowprops=dict(facecolor='black', shrink=0.05),\n                      wrap=True)\n    fig.canvas.draw()\n    assert ann.get_window_extent().height > 20\n    plt.close('all')\n", "def test_annotation_set_position():\n    fig, ax = plt.subplots()\n    ann = ax.annotate('test', xy=(0.5, 0.5), xycoords='axes fraction')\n    ann.set_position((0.7, 0.7))\n    assert_almost_equal(ann.xyann, (0.7, 0.7))\n    ann.set_position((0.8, 0.8))\n    assert_almost_equal(ann.xyann, (0.8, 0.8))\n    plt.close(fig)\n", "def test_text_get_window_extent_with_display_space():\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, 'Test', transform=ax.transAxes)\n    renderer = fig.canvas.get_renderer()\n    window_extent = text.get_window_extent(renderer)\n    assert isinstance(window_extent, mtransforms.Bbox)\n", "def test_text_set_fontfamily():\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, 'Hello')\n    font_family = ['serif', 'sans-serif']\n    text.set_fontfamily(font_family)\n    assert text.get_fontfamily() == font_family\n"], "sample_518": ["def test_FancyArrowPatch():\n    fig, ax = plt.subplots()\n    arrow = FancyArrowPatch((0.3, 0.2), (0.5, 0.8),\n                            mutation_scale=100,\n                            connectionstyle=\"arc3,rad=.1\",\n                            arrowstyle=\"Fancy,head_width=10,head_length=20\")\n    ax.add_patch(arrow)\n    assert isinstance(arrow.get_path(), mpath.Path)\n    assert isinstance(arrow.get_connectionstyle(), BoxStyle._Base)\n    assert isinstance(arrow.get_arrowstyle(), FancyArrow._Base)\n    plt.close(fig)\n", "def test_connectionpatch_xy_coords():\n    fig, ax = plt.subplots()\n    xyA = (0.1, 0.2)\n    xyB = (0.3, 0.4)\n    con = mpatches.ConnectionPatch(xyA, xyB, coordsA='data', coordsB='data',\n                                   axesA=ax, axesB=ax)\n    assert_array_equal(con._get_xy(xyA, 'data'), xyA)\n    assert_array_equal(con._get_xy(xyB, 'data'), xyB)\n    con.set_positions((xyB, xyA))\n    assert_array_equal(con.xy1, xyB)\n    assert_array_equal(con.xy2, xyA)\n    con.set_annotation_clip(True)\n    assert con.get_annotation_clip()\n    con.set_annotation_clip(False)\n    assert not con.get_annotation_clip()\n", "def test_fancyarrowpatch_connectionstyle():\n    # Test that FancyArrowPatch connection styles work correctly.\n    fig, ax = plt.subplots()\n\n    # Define two points for the arrow\n    posA = (0.3, 0.2)\n    posB = (0.7, 0.8)\n\n    # Create a FancyArrowPatch with different connection styles\n    arrow1 = FancyArrowPatch(posA, posB, connectionstyle='arc3')\n    arrow2 = FancyArrowPatch(posA, posB, connectionstyle='angle3')\n    arrow3 = FancyArrowPatch(posA, posB, connectionstyle='angle')\n\n    # Add the arrows to the axes\n    ax.add_patch(arrow1)\n    ax.add_patch(arrow2)\n    ax.add_patch(arrow3)\n\n    # Set the aspect ratio of the plot to be equal so the arrows are not distorted\n    ax.set_aspect('equal')\n\n    # Check that the plot contains the arrows\n    assert len(ax.patches) == 3\n\n    # Close the figure to free up resources\n    plt.close(fig)\n", "def test_ArrowStyle():\n    fig, ax = plt.subplots()\n    ax.set_xlim(-1.5, 1.5)\n    ax.set_ylim(-1.5, 1.5)\n\n    arrow = FancyArrowPatch(posA=(0, 0), posB=(1, 1),\n                            connectionstyle=\"arc3\", arrowstyle='simple')\n    ax.add_patch(arrow)\n\n    arrow = FancyArrowPatch(posA=(0, 0), posB=(1, -1),\n                            connectionstyle=\"arc3\", arrowstyle='-|>')\n    ax.add_patch(arrow)\n\n    arrow = FancyArrowPatch(posA=(0, 0), posB=(-1, 1),\n                            connectionstyle=\"arc3\", arrowstyle='<|-')\n    ax.add_patch(arrow)\n\n    arrow = FancyArrowPatch(posA=(0, 0), posB=(-1, -1),\n                            connectionstyle=\"arc3\", arrowstyle='<->')\n    ax.add_patch(arrow)\n\n    # Check that the arrow styles are being set correctly.\n    assert arrow.arrowstyle == '<->'\n\n    with pytest.raises(ValueError):\n        FancyArrowPatch(posA=(0, 0), posB=(-1, -1),\n                        connectionstyle=\"arc3\", arrowstyle=' invalid')\n", "def test_fancyarrowpatch_connectionstyle():\n    fig, ax = plt.subplots()\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n\n    arrow = FancyArrowPatch((0.2, 0.2), (0.8, 0.8),\n                            connectionstyle=\"arc3,rad=.2\",\n                            mutation_scale=20,\n                            edgecolor='black',\n                            facecolor='none')\n    ax.add_patch(arrow)\n\n    arrow = FancyArrowPatch((0.2, 0.8), (0.8, 0.2),\n                            connectionstyle=\"angle3,angleA=90,angleB=0\",\n                            mutation_scale=20,\n                            edgecolor='black',\n                            facecolor='none')\n    ax.add_patch(arrow)\n\n    plt.draw()\n    assert len(ax.patches) == 2\n"], "sample_519": ["def test_figure_layout_engine():\n    # Test setting and getting of layout engine\n    fig = Figure()\n    assert fig.get_layout_engine() is None\n\n    fig.set_layout_engine('constrained')\n    assert isinstance(fig.get_layout_engine(), ConstrainedLayoutEngine)\n\n    fig.set_layout_engine('tight')\n    assert isinstance(fig.get_layout_engine(), TightLayoutEngine)\n\n    with pytest.raises(ValueError):\n        fig.set_layout_engine('invalid')\n\n    # Test that layout engine is properly reset when setting to None\n    fig.set_layout_engine(None)\n    assert fig.get_layout_engine() is None\n", "def test_figure_align_labels():\n    fig, axs = plt.subplots(2, 2)\n    for ax in axs.flat:\n        ax.set(xticks=[0.1, 0.5, 0.9], yticks=[0.1, 0.5, 0.9])\n    fig.align_xlabels(axs[:, 0])\n    fig.align_ylabels(axs[0, :])\n    fig.canvas.draw()\n", "def test_constrained_layout_with_colorbar():\n    # Test constrained layout with a colorbar.\n    fig, ax = plt.subplots(constrained_layout=True)\n    im = ax.imshow(np.arange(100).reshape((10, 10)))\n    fig.colorbar(im)\n    assert isinstance(fig.get_layout_engine(), ConstrainedLayoutEngine)\n", "def test_figure_layout_engine():\n    fig = Figure()\n    assert fig.get_layout_engine() is None\n\n    fig.set_layout_engine('constrained')\n    assert isinstance(fig.get_layout_engine(), ConstrainedLayoutEngine)\n\n    fig.set_layout_engine('tight')\n    assert isinstance(fig.get_layout_engine(), TightLayoutEngine)\n\n    with pytest.raises(ValueError):\n        fig.set_layout_engine('invalid')\n\n    fig.set_layout_engine(None)\n    assert fig.get_layout_engine() is None\n", "def test_constrained_layout1():\n    # Simple figure with constrained layout.\n    fig, axs = plt.subplots(2, 2, figsize=(3, 3), constrained_layout=True)\n    for ax in axs.flat:\n        example_plot(ax)\n"], "sample_520": ["def test_Poly3DCollection_set_facecolor(setup_test_data):\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    poly = Poly3DCollection([[[0, 0, 0], [1, 0, 0], [1, 1, 0]]])\n    poly.set_facecolor('red')\n    ax.add_collection3d(poly)\n    assert np.allclose(poly.get_facecolor(), [1, 0, 0, 1])\n", "def test_art3d_line_collection_2d_to_3d():\n    fig = plt.figure()\n    ax = Axes3D(fig)\n\n    # create a LineCollection in 2D\n    x = np.array([0, 1, 2])\n    y = np.array([0, 1, 2])\n    line = LineCollection([np.column_stack([x, y])])\n\n    # convert to 3D\n    art3d.line_collection_2d_to_3d(line, zs=1)\n\n    # check that the LineCollection has been correctly converted to 3D\n    assert isinstance(line, art3d.Line3DCollection)\n    assert line.get_zs() == [1]\n", "def test_poly_collection_2d_to_3d():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    poly = PolyCollection([[(0, 0), (1, 0), (1, 1), (0, 1)]])\n    art3d.poly_collection_2d_to_3d(poly, zs=5)\n    ax.add_collection3d(poly)\n    assert isinstance(poly, art3d.Poly3DCollection)\n    assert poly.get_zsort() == 'average'\n", "def test_line3d_collection_get_set_zsort():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    xs = [1, 2, 3]\n    ys = [4, 5, 6]\n    zs = [7, 8, 9]\n    line = Line3DCollection([list(zip(xs, ys, zs))])\n    ax.add_collection(line)\n    assert line.get_sort_zpos() is None\n    line.set_sort_zpos(10)\n    assert line.get_sort_zpos() == 10\n", "def test_3d_text_zdir():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    text = ax.text(0.5, 0.5, 0.5, \"text\", zdir='x')\n    assert isinstance(text, art3d.Text3D)\n    assert text._dir_vec == np.array([1, 0, 0])\n"], "sample_521": ["def test_art3d_line_collection_2d_to_3d():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    xs = np.random.rand(10)\n    ys = np.random.rand(10)\n    zs = np.random.rand(10)\n    line_collection = LineCollection([np.column_stack([xs, ys])])\n    art3d.line_collection_2d_to_3d(line_collection, zs=zs, zdir='z')\n    ax.add_collection(line_collection)\n    ax.set_xlim3d(-1, 1)\n    ax.set_ylim3d(-1, 1)\n    ax.set_zlim3d(-1, 1)\n", "def test_art3d_line_collection_2d_to_3d():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n\n    # Create a LineCollection with 2D coordinates\n    lc = LineCollection([[(0, 0), (1, 1)], [(1, 0), (0, 1)]])\n\n    # Convert the LineCollection to a Line3DCollection\n    art3d.line_collection_2d_to_3d(lc, zs=0, zdir='z')\n\n    # Add the Line3DCollection to the axes\n    ax.add_collection(lc)\n\n    # Set axis limits to ensure the lines are visible\n    ax.set_xlim(-1, 2)\n    ax.set_ylim(-1, 2)\n    ax.set_zlim(-1, 2)\n", "def test_art3d_line_collection_2d_to_3d():\n    fig = plt.figure()\n    ax = Axes3D(fig)\n\n    # Create a 2D line collection\n    x = np.array([1, 2, 3])\n    y = np.array([1, 2, 3])\n    line_collection = LineCollection([np.column_stack((x, y))])\n\n    # Convert the line collection to 3D\n    art3d.line_collection_2d_to_3d(line_collection, zs=5, zdir='z')\n\n    # Add the 3D line collection to the axes\n    ax.add_collection3d(line_collection)\n\n    # Set axis limits to ensure the line is visible\n    ax.set_xlim(0, 4)\n    ax.set_ylim(0, 4)\n    ax.set_zlim(0, 10)\n", "def test_text3d_properties():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n\n    text = art3d.Text3D(0.5, 0.5, 0.5, 'text', zdir='z')\n    ax.add_artist(text)\n\n    assert text.get_position_3d() == (0.5, 0.5, 0.5)\n    text.set_position_3d((0.6, 0.6, 0.6))\n    assert text.get_position_3d() == (0.6, 0.6, 0.6)\n\n    assert text._z == 0.6\n    text.set_z(0.7)\n    assert text._z == 0.7\n\n    assert text._dir_vec.tolist() == [0, 0, 1]\n    text.set_3d_properties(zdir='y')\n    assert text._dir_vec.tolist() == [0, 1, 0]\n", "def test_art3d_get_dir_vector():\n    # Test get_dir_vector function\n    assert np.array_equal(art3d.get_dir_vector('x'), np.array([1, 0, 0]))\n    assert np.array_equal(art3d.get_dir_vector('y'), np.array([0, 1, 0]))\n    assert np.array_equal(art3d.get_dir_vector('z'), np.array([0, 0, 1]))\n    assert np.array_equal(art3d.get_dir_vector(None), np.array([0, 0, 0]))\n    assert np.array_equal(art3d.get_dir_vector([1, 2, 3]), np.array([1, 2, 3]))\n\n    with pytest.raises(ValueError):\n        art3d.get_dir_vector('w')\n    with pytest.raises(ValueError):\n        art3d.get_dir_vector([1, 2])\n"], "sample_522": ["def test_colorbar_extension_length():\n    \"\"\"\n    Test rectangular colorbar extensions with variable lengths.\n\n    This is a visual comparison test.\n    \"\"\"\n    fig1 = _colorbar_extension_length('uniform')\n    fig2 = _colorbar_extension_length('proportional')\n    return [fig1, fig2]\n", "def test_colorbar_extension_length():\n    \"\"\"\n    Test that colorbars with variable length extensions are correctly sized\n    for uniform and proportional spacing.\n    \"\"\"\n    fig_uniform = _colorbar_extension_length('uniform')\n    fig_proportional = _colorbar_extension_length('proportional')\n    return fig_uniform, fig_proportional\n", "def test_colorbar_extension_length():\n    fig1 = _colorbar_extension_length('uniform')\n    fig2 = _colorbar_extension_length('proportional')\n    return fig1, fig2\n\n", "def test_colorbar_extension_length():\n    fig_uniform = _colorbar_extension_length('uniform')\n    fig_proportional = _colorbar_extension_length('proportional')\n    return [fig_uniform, fig_proportional]\n", "def test_colorbar_extension_length():\n    \"\"\"\n    Test rectangular colorbar extensions with different lengths.\n\n    Produces 24 subplots (4 extension types, 3 extendfrac settings each,\n    uniform and proportional spacing) with different length extensions.\n    \"\"\"\n    fig_uniform = _colorbar_extension_length('uniform')\n    fig_proportional = _colorbar_extension_length('proportional')\n    return [fig_uniform, fig_proportional]\n"], "sample_523": ["def test_legend_alignment(fig_test, fig_ref):\n    fig_test.subplots()\n    fig_ref.subplots()\n\n    fig_test.legend(['A', 'B', 'C'], loc='upper right', alignment='left')\n    fig_ref.legend(['A', 'B', 'C'], loc='upper right', alignment='left')\n\n    # Update alignment\n    fig_test.legends[0].set_alignment('center')\n    fig_ref.legends[0].set_alignment('right')\n\n    # Check if the alignment is updated correctly\n    assert fig_test.legends[0].get_alignment() == 'center'\n    assert fig_ref.legends[0].get_alignment() == 'right'\n", "def test_legend_with_fancybboxpatch():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3], label='Line')\n    legend = ax.legend(borderaxespad=0.5, fancybox=True)\n    assert isinstance(legend.legendPatch, mpl.patches.FancyBboxPatch)\n    assert legend.legendPatch.get_boxstyle().pad == 0\n\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3], label='Line')\n    legend = ax.legend(borderaxespad=0.5, fancybox=False)\n    assert isinstance(legend.legendPatch, mpl.patches.FancyBboxPatch)\n    assert legend.legendPatch.get_boxstyle().pad == 0\n", "def test_legend_alignment(fig_test, fig_ref):\n    fig_test.subplots()\n    fig_ref.subplots()\n\n    labels = [\"short\", \"medium size label\"]\n    fig_test.legend(labels, loc=\"center\", alignment=\"left\")\n    fig_ref.legend(labels, loc=\"center\", alignment=\"left\")\n\n    # Update alignment to 'right'\n    fig_test.legends[0].set_alignment(\"right\")\n    fig_ref.legends[0].set_alignment(\"right\")\n", "def test_get_legend_handles_labels():\n    fig, axs = plt.subplots(2)\n    line1 = axs[0].plot([1, 2, 3], label='Line 1')\n    line2 = axs[1].plot([4, 5, 6], label='Line 2')\n    handles, labels = mlegend._get_legend_handles_labels([axs[0], axs[1]])\n    assert len(handles) == 2\n    assert len(labels) == 2\n    assert labels == ['Line 1', 'Line 2']\n", "def test_legend_alignment(alignment):\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3], label='test')\n    legend = ax.legend(alignment=alignment)\n    assert legend.get_alignment() == alignment\n"], "sample_524": ["def test_colorbar_extension_shape_uniform():\n    \"\"\"Test uniform spacing colorbars with rectangular extensions.\"\"\"\n    return _colorbar_extension_shape('uniform')\n", "def test_colorbar_extension_length():\n    fig1 = _colorbar_extension_length('uniform')\n    fig2 = _colorbar_extension_length('proportional')\n    return fig1, fig2\n", "def test_colorbar_extension_length():\n    fig1 = _colorbar_extension_length('uniform')\n    fig2 = _colorbar_extension_length('proportional')\n    return [fig1, fig2]\n", "def test_set_size_inches_forward_false():\n    # Create a figure with a specified size.\n    fig = plt.figure(figsize=(8, 6))\n    \n    # Change the size, but don't update the canvas.\n    fig.set_size_inches((10, 8), forward=False)\n    \n    # Check that the figure's internal size has changed, but the\n    # canvas size is still the original size.\n    assert fig.get_size_inches() == (10, 8)\n    assert fig.canvas.get_width_height() == (800, 600)  # assuming 100 dpi\n    \n    # Now update the canvas and check that the sizes match.\n    fig.canvas.draw()\n    assert fig.canvas.get_width_height() == (1000, 800)  # assuming 100 dpi\n", "def test_figure_savefig_tight_layout(fig_test, fig_ref):\n    fig_test, ax = plt.subplots()\n    ax.plot([1, 2, 3])\n    ax.set_title(\"Test Figure\")\n    fig_test.tight_layout()\n\n    fig_ref, ax = plt.subplots()\n    ax.plot([1, 2, 3])\n    ax.set_title(\"Test Figure\")\n    fig_ref.tight_layout()\n    fig_ref.savefig(\"test.png\", bbox_inches=\"tight\")\n\n    fig_test.savefig(\"test.png\")\n"], "sample_525": ["def test_subfigure_layout():\n    fig = plt.figure()\n    sfigs = fig.subfigures(1, 2)\n    axsL = sfigs[0].subplots(1, 2)\n    axsR = sfigs[1].subplots(2, 1)\n    for ax in axsL + axsR:\n        ax.plot([1, 2, 3])\n", "def test_figure_constrained_layout(fig_test, fig_ref):\n    gs = gridspec.GridSpec(2, 2, figure=fig_test)\n    ax1 = fig_test.add_subplot(gs[0, 0])\n    ax2 = fig_test.add_subplot(gs[0, 1])\n    ax3 = fig_test.add_subplot(gs[1, :])\n\n    ax1.plot([1, 2, 3])\n    ax2.plot([2, 4, 6])\n    ax3.plot([3, 5, 7])\n\n    fig_test.set_layout_engine(\"constrained\")\n\n    gs = gridspec.GridSpec(2, 2, figure=fig_ref)\n    ax1 = fig_ref.add_subplot(gs[0, 0])\n    ax2 = fig_ref.add_subplot(gs[0, 1])\n    ax3 = fig_ref.add_subplot(gs[1, :])\n\n    ax1.plot([1, 2, 3])\n    ax2.plot([2, 4, 6])\n    ax3.plot([3, 5, 7])\n\n    fig_ref.set_layout_engine(ConstrainedLayoutEngine())\n", "def test_figure_base_layout_engine():\n    fig = FigureBase()\n    assert isinstance(fig.get_layout_engine(), PlaceHolderLayoutEngine)\n\n    fig.set_layout_engine('constrained')\n    assert isinstance(fig.get_layout_engine(), ConstrainedLayoutEngine)\n\n    fig.set_layout_engine('tight')\n    assert isinstance(fig.get_layout_engine(), TightLayoutEngine)\n", "def test_figure_pickle():\n    fig = Figure()\n    buf = io.BytesIO()\n    pickle.dump(fig, buf)\n    buf.seek(0)\n    loaded_fig = pickle.load(buf)\n    assert isinstance(loaded_fig, Figure)\n    assert loaded_fig.axes == []\n", "def test_figure_constrained_layout():\n    fig, axs = plt.subplots(2, 2, figsize=(3, 3), layout='constrained')\n    for ax in axs.flat:\n        ax.plot([1, 2, 3])\n    with pytest.raises(ValueError):\n        fig.execute_constrained_layout()\n"], "sample_526": ["def test_rrule_locator():\n    rrule = dateutil.rrule.rrule(dateutil.rrule.YEARLY, bymonth=1, byday=1,\n                                 dtstart=datetime.date(2010, 1, 1))\n    locator = mdates.RRuleLocator(rrule)\n    tz = dateutil.tz.gettz('US/Pacific')\n    dates = [datetime.datetime(2009, 12, 31, tzinfo=tz),\n             datetime.datetime(2010, 1, 1, tzinfo=tz),\n             datetime.datetime(2011, 1, 1, tzinfo=tz),\n             datetime.datetime(2012, 1, 1, tzinfo=tz)]\n    nums = [mdates.date2num(d) for d in dates]\n    ticks = locator.tick_values(nums[0], nums[-1])\n    assert np.all(ticks == nums[1:])\n", "def test_RRuleLocator_get_unit_generic():\n    # test that RRuleLocator.get_unit_generic returns the correct units\n    freq = dateutil.rrule.YEARLY\n    assert mdates.RRuleLocator.get_unit_generic(freq) == 365\n\n    freq = dateutil.rrule.MONTHLY\n    assert mdates.RRuleLocator.get_unit_generic(freq) == 30\n\n    freq = dateutil.rrule.WEEKLY\n    assert mdates.RRuleLocator.get_unit_generic(freq) == 7\n\n    freq = dateutil.rrule.DAILY\n    assert mdates.RRuleLocator.get_unit_generic(freq) == 1\n\n    freq = dateutil.rrule.HOURLY\n    assert mdates.RRuleLocator.get_unit_generic(freq) == 1/24\n\n    freq = dateutil.rrule.MINUTELY\n    assert mdates.RRuleLocator.get_unit_generic(freq) == 1/(24*60)\n\n    freq = dateutil.rrule.SECONDLY\n    assert mdates.RRuleLocator.get_unit_generic(freq) == 1/(24*60*60)\n", "def test_date2num_datetime64_out_of_bounds():\n    # date2num should raise an error for np.datetime64 outside of valid range\n    with pytest.raises(ValueError):\n        mdates.date2num(np.datetime64('0001-01-01T00:00:00'))\n    with pytest.raises(ValueError):\n        mdates.date2num(np.datetime64('10000-01-01T00:00:00'))\n", "def test_date2num_tzinfo():\n    tz = dateutil.tz.gettz('US/Eastern')\n    dt = datetime.datetime(2020, 1, 1, tzinfo=tz)\n    num = mdates.date2num(dt)\n    assert isinstance(num, float)\n    dt_back = mdates.num2date(num, tz=tz)\n    assert dt == dt_back\n\n    # Test with numpy array of datetime objects\n    dts = np.array([dt, dt])\n    nums = mdates.date2num(dts)\n    assert isinstance(nums, np.ndarray)\n    assert nums.shape == (2,)\n    dts_back = mdates.num2date(nums, tz=tz)\n    assert np.all(dts == dts_back)\n", "def test_date_formatter(fmt):\n    dates = [datetime.datetime(2022, 1, 1), datetime.datetime(2022, 1, 15)]\n    formatter = mdates.DateFormatter(fmt)\n    formatted_dates = [formatter(x) for x in mdates.date2num(dates)]\n    assert all(date.strftime(fmt) == formatted_date for date, formatted_date in zip(dates, formatted_dates))\n"], "sample_527": ["def test_figure_size(figsize):\n    fig = Figure(figsize=figsize)\n    assert np.allclose(fig.get_size_inches(), figsize)\n", "def test_set_size_inches():\n    fig = Figure()\n    assert fig.get_size_inches() == np.array([6.4, 4.8])\n\n    # Set with one argument.\n    fig.set_size_inches(np.array([5, 3]))\n    assert np.all(fig.get_size_inches() == np.array([5., 3.]))\n\n    # Set with two arguments.\n    fig.set_size_inches(4, 6)\n    assert np.all(fig.get_size_inches() == np.array([4., 6.]))\n\n    # Setting the size with strings should raise an error.\n    with pytest.raises(TypeError):\n        fig.set_size_inches('5', '3')\n", "def test_figure_add_subplot():\n    fig = Figure()\n    ax1 = fig.add_subplot(2, 2, 1)\n    assert ax1.get_geometry() == (2, 2, 1)\n    ax2 = fig.add_subplot(2, 2, 2, sharex=ax1)\n    assert ax2.get_geometry() == (2, 2, 2)\n    assert ax2.get_shared_x_axes().joined(ax1, ax2)\n", "def test_figure_repr():\n    fig = Figure()\n    repr_string = repr(fig)\n    assert \"Figure\" in repr_string\n    assert re.match(r\"<Figure size \\d+\\.\\d+x\\d+\\.\\d+ with 0 Axes>\", repr_string)\n\n    ax = fig.add_subplot(111)\n    repr_string = repr(fig)\n    assert \"Figure\" in repr_string\n    assert re.match(r\"<Figure size \\d+\\.\\d+x\\d+\\.\\d+ with 1 Axes>\", repr_string)\n", "def test_figure_get_default_bbox_extra_artists():\n    fig, ax = plt.subplots()\n    assert len(fig.get_default_bbox_extra_artists()) > 0\n\n    # Test that all artists in the figure are returned if they are visible and\n    # have in_layout=True.\n    for artist in fig.get_children():\n        if artist.get_visible() and artist.get_in_layout():\n            assert artist in fig.get_default_bbox_extra_artists()\n\n    # Test that artists with in_layout=False are not included.\n    artist = plt.Line2D([0, 1], [0, 1])\n    artist.set_visible(True)\n    artist.set_in_layout(False)\n    fig.add_artist(artist)\n    assert artist not in fig.get_default_bbox_extra_artists()\n\n    # Test that artists with visible=False are not included.\n    artist = plt.Line2D([0, 1], [0, 1])\n    artist.set_visible(False)\n    artist.set_in_layout(True)\n    fig.add_artist(artist)\n    assert artist not in fig.get_default_bbox_extra_artists()\n"], "sample_528": ["def test_reload_library():\n    \"\"\"Test that reload_library() clears and updates the style library.\"\"\"\n    original_library = style.library.copy()\n    with temp_style('test_reload', DUMMY_SETTINGS):\n        assert 'test_reload' in style.library\n    style.reload_library()\n    assert style.library == original_library\n", "def test_reload_library():\n    \"\"\"Test that reload_library re-reads the style library.\"\"\"\n    original_lib = style.library.copy()\n    with temp_style('test_reload', {'lines.linewidth': 10}):\n        # This should update the library.\n        style.reload_library()\n        assert 'test_reload' in style.library\n        assert style.library['test_reload']['lines.linewidth'] == 10\n\n    # And this should reset it back to the original state.\n    style.reload_library()\n    assert style.library == original_lib\n", "def test_use_with_list_of_styles():\n    style1 = 'style1'\n    style2 = 'style2'\n    with temp_style(style1, {'lines.linewidth': 10}):\n        with temp_style(style2, {'lines.color': 'red'}):\n            style.use([style1, style2])\n            assert mpl.rcParams['lines.linewidth'] == 10\n            assert mpl.rcParams['lines.color'] == 'red'\n", "def test_use_with_path():\n    \"\"\"Test using a style file specified by path.\"\"\"\n    with temp_style('temp') as style_file:\n        style.use(Path(USER_LIBRARY_PATHS[-1]) / 'temp.mplstyle')\n        assert mpl.rcParams[PARAM] == VALUE\n\n", "def test_use_with_list_of_styles():\n    with temp_style('style1', DUMMY_SETTINGS):\n        with temp_style('style2', {PARAM: 'blue'}):\n            style.use(['style1', 'style2'])\n            assert mpl.rcParams[PARAM] == 'blue'\n            style.use(['style2', 'style1'])\n            assert mpl.rcParams[PARAM] == 'pink'\n"], "sample_529": ["def test_legend_handles():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2], label='Line')\n    ax.scatter([1, 2], label='Scatter')\n\n    # Test if handles are correctly identified\n    handles, _ = mlegend._get_legend_handles_labels([ax])\n    assert len(handles) == 2\n    assert isinstance(handles[0], mlines.Line2D)\n    assert isinstance(handles[1], mcollections.PathCollection)\n\n    # Test if handles are correctly filtered by label\n    ax.plot([1, 2], label='_nolegend_')\n    handles, _ = mlegend._get_legend_handles_labels([ax])\n    assert len(handles) == 2\n\n    # Test if handles are correctly ordered\n    ax.plot([1, 2], label='Another line')\n    handles, _ = mlegend._get_legend_handles_labels([ax])\n    assert len(handles) == 3\n    assert isinstance(handles[0], mlines.Line2D)\n    assert isinstance(handles[1], mcollections.PathCollection)\n    assert isinstance(handles[2], mlines.Line2D)\n", "def test_legend_alignment(fig_test, fig_ref):\n    fig_test.subplots_adjust(left=0.2)\n    fig_ref.subplots_adjust(left=0.2)\n\n    ax_test = fig_test.add_subplot(111)\n    ax_ref = fig_ref.add_subplot(111)\n\n    p1 = ax_test.bar([1], [1], color='red', label='Red Bar')\n    p2 = ax_test.bar([2], [2], color='blue', label='Blue Bar')\n\n    ax_ref.bar([1], [1], color='red', label='Red Bar')\n    ax_ref.bar([2], [2], color='blue', label='Blue Bar')\n\n    ax_test.legend(handles=[p1, p2], loc='upper left', bbox_to_anchor=(0, 1),\n                   alignment='left')\n    ax_ref.legend(loc='upper left', bbox_to_anchor=(0, 1), alignment='left')\n\n    ax_test.set_xlim(0, 3)\n    ax_test.set_ylim(0, 3)\n    ax_ref.set_xlim(0, 3)\n    ax_ref.set_ylim(0, 3)\n", "def test_legend_handles_labels():\n    fig, ax = plt.subplots()\n    line1, = ax.plot([1, 2, 3], label='Line 1')\n    line2, = ax.plot([2, 4, 6], label='Line 2')\n\n    handles, labels = mlegend._get_legend_handles_labels([ax])\n    assert len(handles) == 2\n    assert len(labels) == 2\n    assert handles[0] == line1\n    assert labels[0] == 'Line 1'\n    assert handles[1] == line2\n    assert labels[1] == 'Line 2'\n", "def test_legend_set_bbox_to_anchor(loc, expected_bbox_to_anchor):\n    fig, ax = plt.subplots()\n    ax.plot([1, 2], label=\"label\")\n    legend = ax.legend(loc=loc)\n\n    assert_allclose(legend.get_bbox_to_anchor().bounds[:2], expected_bbox_to_anchor)\n", "def test_legend_get_children():\n    fig, ax = plt.subplots()\n    line1, = ax.plot([1, 2], label='Line 1')\n    line2, = ax.plot([2, 3], label='Line 2')\n    legend = ax.legend(handles=[line1, line2])\n\n    children = legend.get_children()\n\n    assert len(children) == 2\n    assert all(isinstance(child, mlegend.Legend) for child in children)\n    assert children[0] == legend._legend_box\n    assert children[1] == legend.legendPatch\n\n    plt.close(fig)\n"], "sample_530": ["def test_offsetbox_no_clipping(fig, renderer):\n    # Test that OffsetBox does not clip its children by default.\n    box = OffsetBox()\n    fig.add_artist(box)\n\n    # Create a child rectangle with negative coordinates.\n    rect = mpatches.Rectangle((-10, -10), 20, 20, fill=True)\n    box._children = [rect]\n\n    fig.canvas.draw()\n    assert len(renderer._drawn_artists) == 2  # box and rect\n", "def test_offsetbox_contains():\n    fig, ax = plt.subplots()\n    box = OffsetBox()\n    box.set_visible(True)\n    ax.add_artist(box)\n\n    event1 = MouseEvent(\n        \"button_press_event\", fig.canvas, 0.5, 0.5, MouseButton.LEFT)\n    assert not box.contains(event1)[0]\n\n    box._children.append(mpatches.Rectangle((0.2, 0.2), 0.4, 0.4))\n    assert box.contains(event1)[0]\n", "def test_DraggableOffsetBox():\n    fig, ax = plt.subplots()\n    text = TextArea(\"Test\")\n    offsetbox = AnchoredOffsetbox(child=text, frameon=False, loc='lower left')\n    ax.add_artist(offsetbox)\n    draggable = DraggableOffsetBox(ref_artist=ax, offsetbox=offsetbox)\n    event = MouseEvent('button_press_event', fig.canvas, 0.5, 0.5, MouseButton.LEFT)\n    draggable.on_pick(event)\n    assert draggable.got_artist\n    assert draggable.offsetbox == offsetbox\n\n    # Test dragging\n    event = MouseEvent('motion_notify_event', fig.canvas, 0.6, 0.6)\n    draggable.on_motion(event)\n    assert_allclose(draggable.offsetbox.get_offset(), (60, 60))\n\n    # Test release\n    event = MouseEvent('button_release_event', fig.canvas, 0.6, 0.6, MouseButton.LEFT)\n    draggable.on_release(event)\n    assert not draggable.got_artist\n", "def test_AnnotationBbox():\n    fig, ax = plt.subplots()\n    offsetbox = TextArea(\"Test\")\n    ab = AnnotationBbox(offsetbox, (0.5, 0.5),\n                        xycoords='data',\n                        boxcoords=\"offset points\",\n                        frameon=True,\n                        pad=0.4)\n    ax.add_artist(ab)\n    assert isinstance(ab.arrow_patch, mpatches.FancyArrowPatch)\n    ab.arrowprops = None\n    assert ab.arrow_patch is None\n\n    # test update_positions\n    renderer = fig.canvas.get_renderer()\n    ab.update_positions(renderer)\n    assert_allclose(ab.patch.get_xy(), (-24.875, -16.455))\n    assert_allclose(ab.patch.get_window_extent().bounds, (-24.875, -16.455, 49.75, 32.91))\n", "def test_offsetbox_container():\n    fig, ax = plt.subplots()\n\n    box1 = TextArea('Text1')\n    box2 = TextArea('Text2')\n\n    container = OffsetBox()\n    container.add_artist(box1)\n    container.add_artist(box2)\n\n    ax.add_artist(container)\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n\n    assert len(container.get_children()) == 2\n    assert container.get_visible_children()[0] == box1\n    assert container.get_visible_children()[1] == box2\n\n    plt.close(fig)\n"], "sample_531": ["def test_subfigure(fig_test, fig_ref):\n    fig_test.subplots(1, 2)\n    fig_ref.add_subplot(121)\n    fig_ref.add_subplot(122)\n", "def test_figure_subfigure():\n    fig = Figure()\n    sfig1 = fig.add_subfigure(gridspec.GridSpec(1, 1)[0])\n    assert isinstance(sfig1, FigureBase)\n    assert sfig1.get_layout_engine() is None\n\n    sfig2 = fig.add_subfigure(gridspec.GridSpec(1, 1)[0], layout='constrained')\n    assert isinstance(sfig2, FigureBase)\n    assert isinstance(sfig2.get_layout_engine(), ConstrainedLayoutEngine)\n\n    sfig3 = fig.add_subfigure(gridspec.GridSpec(1, 1)[0], layout='tight')\n    assert isinstance(sfig3, FigureBase)\n    assert isinstance(sfig3.get_layout_engine(), TightLayoutEngine)\n", "def test_figure_pickle():\n    fig = Figure()\n    ax = fig.add_subplot(111)\n    ax.plot([1, 2, 3])\n    buf = io.BytesIO()\n    pickle.dump(fig, buf)\n    buf.seek(0)\n    fig2 = pickle.load(buf)\n    assert isinstance(fig2, FigureBase)\n    assert len(fig2.axes) == 1\n    assert isinstance(fig2.axes[0], Axes)\n", "def test_figure_add_subplot():\n    fig = Figure()\n    ax1 = fig.add_subplot(121)\n    ax2 = fig.add_subplot(122, sharex=ax1)\n    assert ax1.get_subplotspec().get_geometry() == (1, 2, 1)\n    assert ax2.get_subplotspec().get_geometry() == (1, 2, 2)\n    assert ax1 is not ax2\n    assert ax1.get_shared_x_axes().joined(ax1, ax2)\n", "def test_figure_subplots():\n    fig = Figure()\n    axs = fig.subplots(2, 2)\n    assert len(axs) == 2\n    assert len(axs[0]) == 2\n    for ax in axs.flat:\n        assert isinstance(ax, Axes)\n"], "sample_532": ["def test_contour_corner_mask(corner_mask):\n    x = np.arange(10)\n    y = np.arange(10)\n    X, Y = np.meshgrid(x, y)\n    Z = np.sin(X) + np.cos(Y)\n\n    fig, ax = plt.subplots()\n    cs = ax.contour(X, Y, Z, corner_mask=corner_mask)\n    assert len(cs.collections) > 0\n\n    # Check that corner masking is applied correctly.\n    for collection in cs.collections:\n        paths = collection.get_paths()\n        for path in paths:\n            vertices = path.vertices\n            assert np.all(vertices == vertices[0]) or not corner_mask\n", "def test_contour_corner_mask():\n    # Test that corner masking works as expected.\n    z = np.array([[1, 2], [3, 4]])\n\n    # Without corner masking, all four points should be part of the contour.\n    cs = plt.contour(z, levels=[2.5], corner_mask=False)\n    assert len(cs.allsegs[0][0]) == 4\n\n    # With corner masking, only two points should be part of the contour.\n    cs = plt.contour(z, levels=[2.5], corner_mask=True)\n    assert len(cs.allsegs[0][0]) == 2\n", "def test_contour_extend_levels(extend, expected):\n    fig, ax = plt.subplots()\n    x = np.linspace(0, 10, 100)\n    y = np.linspace(0, 10, 100)\n    X, Y = np.meshgrid(x, y)\n    Z = X + Y\n    cs = ax.contour(X, Y, Z, levels=[1.0, 2.0, 3.0, 4.0, 5.0], extend=extend)\n    assert_array_almost_equal(cs.levels, expected)\n", "def test_contourf_lognorm(algorithm):\n    # Test that contourf with a log norm works correctly (issue #7267)\n    x = np.linspace(1, 100, 100)\n    y = np.linspace(1, 100, 100)\n    X, Y = np.meshgrid(x, y)\n    Z = X * Y\n\n    fig, ax = plt.subplots()\n    locator = ticker.LogLocator(base=10, subs='all')\n    cs = ax.contourf(X, Y, Z, locator=locator, norm=LogNorm(), algorithm=algorithm)\n\n    # Check that all colors are finite.\n    colors = [c.get_facecolor()[0] for c in cs.collections]\n    assert all(np.all(np.isfinite(c)) for c in colors)\n", "def test_contour_label_inline_spacing():\n    fig, ax = plt.subplots()\n    x = np.linspace(-3, 5, 100)\n    y = np.linspace(-3, 5, 100)\n    X, Y = np.meshgrid(x, y)\n    Z = np.sin(X) + np.cos(Y)\n\n    # Create contour plot with labels\n    cs = ax.contour(X, Y, Z, levels=10)\n    ax.clabel(cs, inline=True, inline_spacing=5)\n\n    # Get the text objects\n    texts = cs.labelTexts\n\n    # Check that the spacing between the text and the contour is correct\n    for text in texts:\n        # Get the position of the text object\n        text_x, text_y = text.get_position()\n\n        # Find the closest point on the contour\n        conmin, segmin, imin, xmin, ymin, _ = cs.find_nearest_contour(\n            text_x, text_y, indices=[0], pixel=False)\n\n        # Calculate the distance between the text and the contour\n        distance = np.hypot(text_x - xmin, text_y - ymin)\n\n        # Check that the distance is approximately equal to the inline spacing\n        assert_array_almost_equal_nulp(distance, 5, nulp=5)\n"], "sample_533": ["def test_contour_corner_mask():\n    # Test that corner masking works correctly.\n    x = np.arange(10)\n    y = np.arange(10)\n    X, Y = np.meshgrid(x, y)\n    Z = np.random.rand(10, 10)\n\n    fig, ax = plt.subplots()\n    with rc_context({'contour.corner_mask': True}):\n        cs1 = ax.contour(X, Y, Z)\n    with rc_context({'contour.corner_mask': False}):\n        cs2 = ax.contour(X, Y, Z)\n\n    assert len(cs1.allsegs) != len(cs2.allsegs)\n", "def test_contour_corner_mask():\n    # Test that corner masking works as expected.\n    x = np.arange(10)\n    y = np.arange(10)\n    X, Y = np.meshgrid(x, y)\n    Z = X**2 + Y**2\n\n    fig, ax = plt.subplots()\n    with rc_context({'contour.corner_mask': True}):\n        cs = ax.contour(X, Y, Z, levels=5)\n    assert_array_almost_equal(cs.levels, [1.5, 5.5, 12.5, 22.5, 35.5])\n\n    fig, ax = plt.subplots()\n    with rc_context({'contour.corner_mask': False}):\n        cs = ax.contour(X, Y, Z, levels=5)\n    assert_array_almost_equal(cs.levels, [0.5, 3.5, 9.5, 17.5, 29.5])\n", "def test_contourf_lognorm(algorithm):\n    z = np.array([[10, 20], [30, 40]])\n    with rc_context({\"contour.algorithm\": algorithm}):\n        fig, ax = plt.subplots()\n        cs = ax.contourf(z, norm=LogNorm())\n        assert_array_almost_equal(cs.levels, np.logspace(1, 2, 6))\n", "def test_corner_mask(corner_mask, expected_n_segs):\n    x = np.arange(3)\n    y = np.arange(3)\n    z = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    contour = plt.contour(x, y, z, levels=[5], corner_mask=corner_mask)\n    n_segs = sum(len(collections.get_paths()) for collections in contour.collections)\n    assert n_segs == expected_n_segs\n", "def test_contour_corner_mask_False():\n    x = np.linspace(0, 10, 50)\n    y = np.linspace(-9, 9, 50)\n    X, Y = np.meshgrid(x, y)\n    Z = np.sin(np.sqrt(X**2 + Y**2))\n\n    # Mask out the negative values of Z\n    mask = Z < 0\n    Z_masked = np.ma.masked_array(Z, mask)\n\n    fig, ax = plt.subplots()\n    cs = ax.contourf(X, Y, Z_masked, levels=20, corner_mask=False)\n    ax.set_title('corner_mask=False')\n    fig.colorbar(cs)\n"], "sample_534": ["def test_contourf_extends():\n    # Test that contourf correctly handles extend options.\n    z = np.linspace(0, 10, 100).reshape((10, 10))\n    levels = [2, 5, 8]\n    for extend in ['neither', 'min', 'max', 'both']:\n        with rc_context({'image.cmap': 'viridis'}):\n            fig, ax = plt.subplots()\n            cs = ax.contourf(z, levels=levels, extend=extend)\n            cmap = cs.get_cmap()\n            if extend == 'neither':\n                assert_array_almost_equal_nulp(cmap(cs.norm.vmin), cmap(cs.levels[0]))\n                assert_array_almost_equal_nulp(cmap(cs.norm.vmax), cmap(cs.levels[-1]))\n            elif extend == 'min':\n                assert_array_almost_equal_nulp(cmap(cs.norm.vmin), cmap(cs.levels[0]))\n                assert_array_almost_equal_nulp(cmap(cs.norm.vmax), cmap(cs.levels[-1]))\n                assert same_color(cs.cmap(cs.norm.vmin), cs.cmap(cs.norm.vmin))\n            elif extend == 'max':\n                assert_array_almost_equal_nulp(cmap(cs.norm.vmin), cmap(cs.levels[0]))\n                assert_array_almost_equal_nulp(cmap(cs.norm.vmax), cmap(cs.levels[-1]))\n                assert same_color(cs.cmap(cs.norm.vmax), cs.cmap(cs.norm.vmax))\n            elif extend == 'both':\n                assert same_color(cs.cmap(cs.norm.vmin), cs.cmap(cs.norm.vmin))\n                assert same_color(cs.cmap(cs.norm.vmax), cs.cmap(cs.norm.vmax))\n            plt.close(fig)\n", "def test_contour_corner_mask(corner_mask):\n    # Create a 2D array with a masked value in the corner\n    z = np.ma.array([[1, 2], [3, 4]], mask=[[True, False], [False, False]])\n\n    fig, ax = plt.subplots()\n    ax.contour(z, corner_mask=corner_mask)\n\n    # Check if the contour lines are correct\n    lines = ax.collections[0].get_paths()[0].vertices\n    expected_lines = np.array([[0.5, 0.5], [0.5, 1.5], [1.5, 1.5]])\n    assert_array_almost_equal(lines, expected_lines)\n", "def test_contour_empty(contour_func, filled):\n    # Test that an empty contour plot does not cause any errors.\n    fig, ax = plt.subplots()\n    try:\n        cs = contour_func(ax=ax, levels=[])\n    except Exception as e:\n        pytest.fail(f\"Unexpected error: {e}\")\n    assert len(cs.collections) == 0\n    assert len(cs.levels) == 0\n    if filled:\n        assert len(cs.layers) == 0\n", "def test_contour_corner_mask(corner_mask):\n    z = np.ma.masked_array([[0, 1], [2, 3]], [[False, True], [False, False]])\n    fig, ax = plt.subplots()\n    cs = ax.contour(z, corner_mask=corner_mask)\n    assert len(cs.collections) == 3\n    plt.close(fig)\n", "def test_contourf_log():\n    z = np.random.rand(20, 30)\n    fig, ax = plt.subplots()\n    ax.contourf(z, levels=np.logspace(-2, 0, 10), locator=None)\n    ax.set_title('contourf with logarithmic levels')\n"], "sample_535": ["def test_table_edges():\n    fig, ax = plt.subplots()\n    ax.axis('off')\n    table = Table(ax, loc='center')\n    table.edges = 'open'\n    for i in range(3):\n        for j in range(3):\n            table.add_cell(i, j, width=0.5, height=0.2, text=f'Cell {i,j}')\n    ax.add_table(table)\n", "def test_table_edges():\n    fig, ax = plt.subplots()\n    ax.axis('off')\n\n    cellText = [['1', '2'], ['3', '4']]\n    table = Table(ax, cellText=cellText, edges='open')\n    ax.add_table(table)\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n", "def test_table_auto_set_column_width():\n    fig, ax = plt.subplots()\n    table = Table(ax)\n    for i in range(5):\n        table.add_cell(0, i, width=0.1, height=0.1, text=f'Column {i}')\n    table.auto_set_column_width(2)\n    ax.add_table(table)\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    return fig\n", "def test_table():\n    fig, ax = plt.subplots()\n    ax.axis('off')\n    ax.set_xlim(0, 10)\n    ax.set_ylim(0, 10)\n\n    cell_text = [['1', '2'], ['3', '4']]\n    table = Table(ax, cellText=cell_text, loc='center')\n\n    ax.add_table(table)\n    return fig\n", "def test_table_edges():\n    fig, ax = plt.subplots()\n    ax.axis('off')\n    cellText = [['1', '2'], ['3', '4']]\n    table = Table(ax, cellText=cellText, loc='center', edges='open')\n    ax.add_table(table)\n    return fig\n"], "sample_537": ["def test_stride_windows(self):\n    x = np.arange(10)\n    NFFT = 5\n    noverlap = 2\n\n    # Test axis=0\n    result = mlab.stride_windows(x, NFFT, noverlap, axis=0)\n    target = self.calc_window_target(x, NFFT, noverlap, axis=0)\n    assert_array_almost_equal_nulp(result, target)\n\n    # Test axis=1\n    result = mlab.stride_windows(x, NFFT, noverlap, axis=1)\n    target = self.calc_window_target(x, NFFT, noverlap, axis=1)\n    assert_array_almost_equal_nulp(result, target)\n\n    # Test with zero padding\n    NFFT = 10\n    noverlap = 5\n    result = mlab.stride_windows(x, NFFT, noverlap, axis=0)\n    target = self.calc_window_target(x, NFFT, noverlap, axis=0)\n    assert_array_almost_equal_nulp(result, target)\n\n    # Test with large noverlap\n    NFFT = 5\n    noverlap = 4\n    result = mlab.stride_windows(x, NFFT, noverlap, axis=0)\n    target = self.calc_window_target(x, NFFT, noverlap, axis=0)\n    assert_array_almost_equal_nulp(result, target)\n", "def test_stride_windows_real_data(self):\n    x = np.arange(10)\n    NFFT = 5\n    noverlap = 3\n\n    result = mlab._stride_windows(x, NFFT, noverlap)\n    target = self.calc_window_target(x, NFFT, noverlap)\n\n    assert_array_almost_equal_nulp(result, target, 5)\n", "def test_stride_windows(self):\n    x = np.arange(10)\n    NFFT = 5\n    noverlap = 3\n\n    # Test with axis=0 (default)\n    result = mlab.stride_windows(x, NFFT, noverlap=noverlap)\n    target = self.calc_window_target(x, NFFT, noverlap=noverlap)\n    assert_array_equal(result, target)\n\n    # Test with axis=1\n    result = mlab.stride_windows(x, NFFT, noverlap=noverlap, axis=1)\n    target = self.calc_window_target(x, NFFT, noverlap=noverlap, axis=1)\n    assert_array_equal(result, target)\n\n    # Test with noverlap=0\n    result = mlab.stride_windows(x, NFFT, noverlap=0)\n    target = self.calc_window_target(x, NFFT, noverlap=0)\n    assert_array_equal(result, target)\n\n    # Test with noverlap=NFFT-1\n    result = mlab.stride_windows(x, NFFT, noverlap=NFFT-1)\n    target = self.calc_window_target(x, NFFT, noverlap=NFFT-1)\n    assert_array_equal(result, target)\n", "def test_stride_windows(self):\n    x = np.arange(10)\n    NFFT = 3\n    noverlap = 1\n\n    result = mlab.stride_windows(x, NFFT, noverlap=noverlap)\n    target = self.calc_window_target(x, NFFT, noverlap=noverlap)\n\n    assert_array_equal(result, target)\n    assert self.get_base(result) is not x\n", "def test_stride_windows(self):\n    x = np.arange(10)\n    NFFT = 5\n    noverlap = 3\n\n    # Test axis=0\n    result = mlab.stride_windows(x, NFFT, noverlap)\n    target = self.calc_window_target(x, NFFT, noverlap)\n    assert_array_equal(result, target)\n\n    # Test axis=1\n    result = mlab.stride_windows(x, NFFT, noverlap, axis=1)\n    target = self.calc_window_target(x, NFFT, noverlap, axis=1)\n    assert_array_equal(result, target)\n\n    # Test with noverlap greater than NFFT\n    with pytest.raises(ValueError):\n        mlab.stride_windows(x, NFFT, noverlap=NFFT+1)\n\n    # Test with NFFT larger than the array\n    with pytest.raises(ValueError):\n        mlab.stride_windows(x, NFFT=len(x)+1)\n"], "sample_538": ["def test_transformed_bbox():\n    bbox = mtransforms.Bbox([[1, 2], [3, 4]])\n    trans = mtransforms.Affine2D().scale(2)\n    tbbox = mtransforms.TransformedBbox(bbox, trans)\n    assert_array_almost_equal(tbbox.get_points(),\n                              np.array([[2, 4], [6, 8]]))\n    assert_almost_equal(tbbox.width, 4)\n    assert_almost_equal(tbbox.height, 4)\n    assert_almost_equal(tbbox.x0, 2)\n    assert_almost_equal(tbbox.y0, 4)\n    assert_almost_equal(tbbox.x1, 6)\n    assert_almost_equal(tbbox.y1, 8)\n    assert_almost_equal(tbbox.p0, [2, 4])\n    assert_almost_equal(tbbox.p1, [6, 8])\n", "def test_transformed_bbox():\n    # Create a sample bounding box and transform\n    bbox = mtransforms.Bbox([[0, 0], [1, 1]])\n    trans = mtransforms.Affine2D().rotate(np.pi / 4)\n\n    # Apply the transform to the bounding box\n    transformed_bbox = mtransforms.TransformedBbox(bbox, trans)\n\n    # Check that the transformed bounding box is correct\n    assert_almost_equal(transformed_bbox.get_points(),\n                        np.array([[-1 / np.sqrt(2), -1 / np.sqrt(2)],\n                                  [1 / np.sqrt(2), 1 / np.sqrt(2)]]))\n\n    # Check that the transformed bounding box has the correct type\n    assert isinstance(transformed_bbox, mtransforms.TransformedBbox)\n", "def test_transformed_bbox():\n    # Test Transform.transform_bbox()\n    bbox = mtransforms.Bbox([[0, 0], [1, 1]])\n    trans = mtransforms.Affine2D().scale(2).translate(3, 4)\n    result = trans.transform_bbox(bbox)\n    assert_array_equal(result.extents, np.array([3, 4, 5, 6]))\n", "def test_transformed_bbox():\n    # Create a sample bbox and transform.\n    bbox = mtransforms.Bbox([[0, 0], [1, 1]])\n    transform = mtransforms.Affine2D().translate(1, 1)\n\n    # Create a TransformedBbox and check its points.\n    tbbox = mtransforms.TransformedBbox(bbox, transform)\n    assert_array_almost_equal(tbbox.get_points(), [[1, 1], [2, 2]])\n\n    # Invalidate the transform and check that the TransformedBbox updates.\n    transform.invalidate()\n    assert_array_almost_equal(tbbox.get_points(), [[1, 1], [2, 2]])\n\n    # Check that the TransformedBbox is pickleable.\n    tbbox2 = copy.deepcopy(tbbox)\n    assert_array_almost_equal(tbbox2.get_points(), [[1, 1], [2, 2]])\n", "def test_transformed_bbox():\n    bbox = mtransforms.Bbox([[0, 0], [1, 1]])\n    trans = mtransforms.Affine2D().scale(2).translate(3, 4)\n    tbbox = mtransforms.TransformedBbox(bbox, trans)\n    assert_array_almost_equal(tbbox.get_points(),\n                              np.array([[3., 4.], [5., 6.]]))\n\n    # Check that the transform is applied to points outside of get_points.\n    assert_array_almost_equal(tbbox.corners(),\n                              np.array([[3., 4.], [3., 6.], [5., 4.], [5., 6.]]))\n    assert_almost_equal(tbbox.width, 2)\n    assert_almost_equal(tbbox.height, 2)\n\n    # Check that the transform is re-applied if it changes.\n    trans.scale(0.5)\n    assert_array_almost_equal(tbbox.get_points(),\n                              np.array([[3., 4.], [4., 5.]]))\n"], "sample_540": ["def test_artist_animation_save(tmpdir):\n    fig, ax = plt.subplots()\n    line1, = ax.plot([], [], lw=2)\n    line2, = ax.plot([], [], lw=2)\n\n        line1.set_data([], [])\n        line2.set_data([], [])\n        return line1, line2\n\n        x = np.linspace(0, 10, 100)\n        y1 = np.sin(x + i)\n        y2 = np.cos(x + i)\n        line1.set_data(x, y1)\n        line2.set_data(x, y2)\n        return line1, line2\n\n    ani = animation.ArtistAnimation(fig, [[line1], [line2]], init_func=init)\n\n    writer = NullMovieWriter()\n    ani.save(str(tmpdir / 'test.mp4'), writer=writer)\n    assert writer.fig is fig\n    assert writer.outfile == str(tmpdir / 'test.mp4')\n    assert writer.dpi == fig.dpi\n    assert writer._count == 2\n", "def test_func_animation_save(tmpdir):\n    fig, ax = plt.subplots()\n    animation.FuncAnimation(fig, lambda frame: ax.plot([]))\n    path = str(tmpdir.join('test.gif'))\n    fig.savefig(path)\n    assert Path(path).is_file()\n", "def test_artist_animation(anim):\n    # Test that ArtistAnimation actually works\n    artists = [plt.Line2D([0, 1], [0, 1]), plt.Line2D([0, 1], [1, 0])]\n    anim = animation.ArtistAnimation(anim._fig, [(artists[0],), (artists[1],)])\n    assert len(anim.new_frame_seq()) == 2\n\n    # Test ArtistAnimation.__init__\n    with pytest.raises(TypeError):\n        animation.ArtistAnimation(None, artists)\n", "def test_animation_save_without_renderer(anim):\n    \"\"\"Test saving an animation without a renderer.\"\"\"\n    writer = NullMovieWriter()\n    anim.save('test.mp4', writer=writer)\n    assert writer.fig is anim._fig\n    assert writer.outfile == 'test.mp4'\n    assert writer.dpi == mpl.rcParams['savefig.dpi']\n    assert writer._count == anim.save_count\n", "def test_cache_frame_data(anim):\n    # Set cache_frame_data to False and check if frame data is not cached.\n    anim._cache_frame_data = False\n    assert anim.new_saved_frame_seq() is not anim._save_seq\n\n    # Set cache_frame_data to True and check if frame data is cached.\n    anim._cache_frame_data = True\n    assert anim.new_saved_frame_seq() is anim._save_seq\n"], "sample_542": ["def test_text_set_position():\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, 'test')\n    text.set_position((0.6, 0.6))\n    assert_almost_equal(text.get_position(), (0.6, 0.6))\n    text.set_x(0.7)\n    text.set_y(0.7)\n    assert_almost_equal(text.get_position(), (0.7, 0.7))\n", "def test_text_contains():\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, \"test\")\n    event = MouseEvent(\"button_press_event\", fig.canvas, 100, 100)\n    assert not text.contains(event)[0]\n    event = MouseEvent(\"button_press_event\", fig.canvas, 200, 200)\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    fig.canvas.draw()\n    assert text.contains(event)[0]\n", "def test_text_set_position():\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, 'Test')\n    assert text.get_position() == (0.5, 0.5)\n    text.set_position((0.6, 0.6))\n    assert text.get_position() == (0.6, 0.6)\n    text.set_x(0.7)\n    text.set_y(0.7)\n    assert text.get_position() == (0.7, 0.7)\n", "def test_text_contains():\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, 'Hello')\n    event = MouseEvent('button_press_event', fig.canvas, 0.5, 0.5)\n    contains, _ = text.contains(event)\n    assert contains\n    event = MouseEvent('button_press_event', fig.canvas, 1.0, 1.0)\n    contains, _ = text.contains(event)\n    assert not contains\n", "def test_text_get_window_extent():\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, 'test')\n    renderer = fig.canvas.get_renderer()\n    extent = text.get_window_extent(renderer)\n    assert isinstance(extent, mpl.transforms.Bbox)\n    assert extent.width > 0\n    assert extent.height > 0\n"], "sample_544": ["def test_composite_images():\n    # Create a test image with different alpha values\n    img1 = np.zeros((10, 10, 4), dtype=np.uint8)\n    img1[:, :, :3] = 255  # white\n    img1[:, :, 3] = 128  # 50% alpha\n\n    img2 = np.zeros((10, 10, 4), dtype=np.uint8)\n    img2[:, :, :3] = 255  # white\n    img2[:, :, 3] = 64  # 25% alpha\n\n    # Create a figure and axis\n    fig, ax = plt.subplots()\n\n    # Create two images with different alpha values\n    im1 = AxesImage(ax, data=img1)\n    im2 = AxesImage(ax, data=img2)\n\n    # Composite the images\n    composite_img, offset_x, offset_y = mimage.composite_images([im1, im2], ax.figure.canvas.renderer)\n\n    # Check that the composite image has the correct shape and alpha values\n    assert composite_img.shape == (10, 10, 4)\n    assert composite_img[:, :, 3].max() == 192  # 75% alpha (50% + 25%)\n", "def test_pcolorimage():\n    fig, ax = plt.subplots()\n    x = np.linspace(0, 10, 11)\n    y = np.linspace(0, 10, 11)\n    A = np.random.rand(10, 10)\n    im = PcolorImage(ax, x, y, A)\n    ax.add_image(im)\n    ax.set_xlim(0, 10)\n    ax.set_ylim(0, 10)\n    ax.set_aspect('equal')\n", "def test_image_set_data():\n    fig, ax = plt.subplots()\n    im = ax.imshow(np.random.rand(20, 20))\n\n    # Update the image data with a new array.\n    new_data = np.random.rand(20, 20)\n    im.set_data(new_data)\n\n    # Ensure that the data is updated correctly.\n    assert_array_equal(im.get_array(), new_data)\n", "def test_thumbnail(tmpdir):\n    # Create a temporary image file.\n    img = Image.new('RGB', (100, 100))\n    img.save(str(tmpdir.join('image.png')))\n\n    # Create a thumbnail.\n    mimage.thumbnail(str(tmpdir.join('image.png')), str(tmpdir.join('thumb.png')))\n\n    # Check that the thumbnail is smaller.\n    thumb_img = Image.open(str(tmpdir.join('thumb.png')))\n    assert thumb_img.size[0] < img.size[0]\n    assert thumb_img.size[1] < img.size[1]\n", "def test_pil_png_to_float_array():\n    img_data = np.random.randint(0, 256, size=(10, 10, 3), dtype=np.uint8)\n    img = Image.fromarray(img_data, 'RGB')\n    float_array = mimage._pil_png_to_float_array(img)\n    assert float_array.dtype == np.float32\n    assert np.allclose(float_array, img_data / 255.0)\n"], "sample_545": ["def test_subfigure_add_subplot():\n    fig = Figure()\n    subfigs = fig.subfigures(1, 2)\n    ax1 = subfigs[0].add_subplot(111)\n    ax2 = subfigs[1].add_subplot(111)\n\n    assert isinstance(ax1, Axes)\n    assert isinstance(ax2, Axes)\n    assert ax1.figure is fig\n    assert ax2.figure is fig\n    assert ax1 not in fig.axes\n    assert ax2 not in fig.axes\n    assert ax1 in subfigs[0].axes\n    assert ax2 in subfigs[1].axes\n", "def test_figure_subfigures():\n    fig = Figure()\n    sfigs = fig.subfigures(1, 2)\n    assert len(sfigs) == 2\n    assert isinstance(sfigs[0], FigureBase)\n    assert isinstance(sfigs[1], FigureBase)\n    axsL = sfigs[0].subplots(1, 2)\n    axsR = sfigs[1].subplots(2, 1)\n    assert len(axsL) == 2\n    assert len(axsR) == 2\n    assert all(isinstance(ax, Axes) for ax in axsL + axsR)\n", "def test_subfigure_add_subplot():\n    fig = Figure()\n    subfig = fig.subfigures(1)[0]\n    ax = subfig.add_subplot(111)\n    assert isinstance(ax, Axes)\n    assert ax.get_figure() is fig\n    assert ax.get_subplotspec().get_topmost_subplotspec() is subfig\n", "def test_subfigure_repr():\n    fig = Figure()\n    subfig = fig.subfigures(1)[0]\n    assert repr(subfig).startswith(\"<SubFigure\")\n    assert str(subfig).startswith(\"SubFigure\")\n", "def test_subfigure_align_labels():\n    fig = plt.figure()\n    sfigs = fig.subfigures(1, 2)\n    axsL = sfigs[0].subplots(1, 2)\n    axsR = sfigs[1].subplots(2, 1)\n    for ax in axsL + axsR:\n        ax.set_ylabel('y-label')\n        ax.set_xlabel('x-label')\n    sfigs[0].align_ylabels(axsL)\n    sfigs[0].align_xlabels(axsL)\n    sfigs[1].align_ylabels(axsR)\n    sfigs[1].align_xlabels(axsR)\n    return fig\n"], "sample_546": ["def test_set_layout_engine():\n    fig = Figure()\n    # Test that default is None\n    assert fig.get_layout_engine() is None\n\n    # Test setting to 'tight'\n    fig.set_layout_engine('tight')\n    assert isinstance(fig.get_layout_engine(), type(fig.get_layout_engine()))\n\n    # Test setting to 'constrained'\n    fig.set_layout_engine('constrained')\n    assert isinstance(fig.get_layout_engine(), type(fig.get_layout_engine()))\n\n    # Test setting to 'none'\n    fig.set_layout_engine('none')\n    assert fig.get_layout_engine() is None\n", "def test_set_dpi(dpi, forward):\n    fig = Figure()\n    fig.set_dpi(dpi)\n    assert fig.dpi == dpi\n    fig._set_dpi(dpi, forward=forward)\n    assert fig.dpi == dpi\n", "def test_figure_repr(figsize, dpi):\n    fig = Figure(figsize=figsize, dpi=dpi)\n    expected_repr = (f\"<Figure size {figsize[0]:g}x{figsize[1]:g} with 0 Axes>\")\n    assert repr(fig) == expected_repr\n", "def test_subplots_adjust():\n    fig = Figure()\n    fig.subplots_adjust(left=0.1, right=0.9, bottom=0.2, top=0.8,\n                        wspace=0.5, hspace=0.5)\n    assert fig.subplotpars.left == 0.1\n    assert fig.subplotpars.right == 0.9\n    assert fig.subplotpars.bottom == 0.2\n    assert fig.subplotpars.top == 0.8\n    assert fig.subplotpars.wspace == 0.5\n    assert fig.subplotpars.hspace == 0.5\n\n    # Check that the setter raises an error for invalid values\n    with pytest.raises(ValueError):\n        fig.subplots_adjust(left=1.1)\n\n    with pytest.raises(ValueError):\n        fig.subplots_adjust(right=-0.1)\n\n    with pytest.raises(ValueError):\n        fig.subplots_adjust(bottom=1.2)\n\n    with pytest.raises(ValueError):\n        fig.subplots_adjust(top=-0.2)\n", "def test_figure_set_size_inches():\n    fig = Figure()\n    assert fig.get_size_inches() == (6.4, 4.8)\n    fig.set_size_inches(8, 10)\n    assert np.allclose(fig.get_size_inches(), (8, 10))\n    fig.set_size_inches((8, 10))\n    assert np.allclose(fig.get_size_inches(), (8, 10))\n"], "sample_547": ["def test_AnchoredOffsetbox_clip_children():\n    fig, ax = plt.subplots()\n    da = DrawingArea(100, 100, clip=True)\n    ab = AnchoredOffsetbox('upper left', pad=0.4, borderpad=0.5,\n                           child=da, frameon=False)\n    ax.add_artist(ab)\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    assert not da.clip_children\n    ab.clip_children = True\n    assert da.clip_children\n", "def test_AnnotationBbox_update_positions():\n    fig, ax = plt.subplots()\n    annotation = AnnotationBbox(\n        OffsetBox(), (0.5, 0.5), xycoords='data', frameon=False)\n    ax.add_artist(annotation)\n    fig.canvas.draw()\n    renderer = fig._get_renderer()\n\n    # Check that the update_position method doesn't crash when no arrow is\n    # drawn.\n    annotation.update_positions(renderer)\n\n    # Now check that the offset box has been correctly positioned.\n    assert_allclose(annotation.offsetbox.get_offset(), (80., 52.))\n", "def test_AnchoredOffsetbox():\n    fig, ax = plt.subplots()\n    box = AnchoredOffsetbox(\n        loc='upper right',\n        child=mpatches.Rectangle((0, 0), 1, 1, facecolor='red'),\n        frameon=False,\n        pad=0.4,\n        borderpad=0.5,\n    )\n    ax.add_artist(box)\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    assert isinstance(box.get_bbox_to_anchor(), mpatches.BboxBase)\n    box.set_bbox_to_anchor((0.5, 0.5))\n    assert isinstance(box.get_bbox_to_anchor(), mpatches.BboxBase)\n", "def test_offsetbox_bbox():\n    fig, ax = plt.subplots()\n    box = OffsetBox()\n    box._children = [mpatches.Rectangle((0, 0), 1, 1)]\n    ax.add_artist(box)\n    box.set_offset((10, 10))\n    assert_allclose(box.get_window_extent().bounds, (10, 10, 1, 1))\n", "def test_anchored_text():\n    fig, ax = plt.subplots()\n    text = AnchoredText(\"Test\", loc=\"upper left\", frameon=False)\n    ax.add_artist(text)\n    assert text.txt.get_text() == \"Test\"\n    assert text.loc == 2\n    text.set_text(\"New Text\")\n    assert text.txt.get_text() == \"New Text\"\n    text.set_loc(\"lower right\")\n    assert text.loc == 4\n"], "sample_548": ["def test_colorbar_empty_mappable():\n    # Test that creating a colorbar with an empty mappable doesn't fail.\n    fig, ax = plt.subplots()\n    cmap = mpl.colormaps[\"RdBu\"]\n    norm = Normalize(vmin=0, vmax=1)\n    cbar = Colorbar(ax, cmap=cmap, norm=norm)\n    assert cbar.mappable is None\n    assert cbar.cmap == cmap\n    assert cbar.norm == norm\n", "def test_colorbar_set_alpha():\n    # Create a colorbar and check its initial alpha value.\n    fig, ax = plt.subplots()\n    mappable = cm.ScalarMappable(cmap='viridis')\n    cbar = fig.colorbar(mappable, ax=ax)\n    assert cbar.alpha is None\n\n    # Set the alpha value to a scalar and check it is updated correctly.\n    cbar.set_alpha(0.5)\n    assert cbar.alpha == 0.5\n\n    # Set the alpha value to an array and check it is updated correctly.\n    cbar.set_alpha(np.array([0.1, 0.2, 0.3]))\n    assert cbar.alpha is None\n\n    # Remove the figure to avoid displaying it.\n    plt.close(fig)\n", "def test_colorbar_remove(fig_test, fig_ref):\n    \"\"\"\n    Test that removing a colorbar clears its axes.\n    \"\"\"\n    fig_test.subplots()\n    cax = fig_test.add_axes([0.85, 0.1, 0.075, 0.8])\n    Colorbar(cax, cmap=\"RdYlGn\").remove()\n    fig_test.canvas.draw()\n    assert not fig_test.axes[1].lines\n    assert not fig_test.axes[1].collections\n    assert not fig_test.axes[1].patches\n    assert not fig_test.axes[1].texts\n\n    fig_ref.subplots()\n    fig_ref.add_axes([0.85, 0.1, 0.075, 0.8])\n    fig_ref.canvas.draw()\n", "def test_colorbar_no_norm():\n    \"\"\"\n    Test colorbar with NoNorm.\n    \"\"\"\n    fig, ax = plt.subplots()\n    cmap = mpl.colormaps[\"RdBu\"].resampled(5)\n    norm = mcolors.NoNorm()\n    boundaries = np.arange(5)\n    values = np.arange(4)\n    Colorbar(ax, cmap=cmap, norm=norm,\n             boundaries=boundaries, values=values,\n             orientation='horizontal')\n    assert not ax.yaxis.get_visible()\n    assert not ax.xaxis.get_major_ticks()[0].label1.get_visible()\n", "def test_colorbar_inverted_scale():\n    # Test colorbar with inverted scale\n    fig, ax = plt.subplots()\n    norm = mcolors.Normalize(vmin=0, vmax=1)\n    cmap = mpl.colormaps['viridis']\n    cbar = Colorbar(ax, cmap=cmap, norm=norm, orientation='vertical')\n    cbar.set_label('Colorbar')\n    ax.invert_yaxis()\n    assert cbar._y[0] > cbar._y[-1]\n    cbar.update_ticks()\n    fig.canvas.draw()\n    assert np.all(cbar._y >= 0) and np.all(cbar._y <= 1)\n"], "sample_549": ["def test_safe_first_element():\n    assert cbook.safe_first_element([1, 2, 3]) == 1\n    assert cbook.safe_first_element((1, 2, 3)) == 1\n    assert cbook.safe_first_element(np.array([1, 2, 3])) == 1\n    assert cbook.safe_first_element(iter([1, 2, 3])) == 1\n    with pytest.raises(RuntimeError):\n        cbook.safe_first_element((x for x in [1, 2, 3]))\n", "def test__reshape_2D():\n    x = np.array([1, 2, 3, 4])\n    y = cbook._reshape_2D(x, \"x\")\n    assert_array_equal(y[0], x)\n\n    x = np.array([[1, 2], [3, 4]])\n    y = cbook._reshape_2D(x, \"x\")\n    assert_array_equal(y[0], x[:, 0])\n    assert_array_equal(y[1], x[:, 1])\n\n    x = [[1, 2], [3, 4]]\n    y = cbook._reshape_2D(x, \"x\")\n    assert_array_equal(y[0], np.array([1, 3]))\n    assert_array_equal(y[1], np.array([2, 4]))\n\n    with pytest.raises(ValueError):\n        cbook._reshape_2D(np.array([[[1]]]), \"x\")\n\n    with pytest.raises(ValueError):\n        cbook._reshape_2D([1, [2]], \"x\")\n", "def test_safe_masked_invalid():\n    a = np.array([1, 2, np.nan, 4])\n    b = cbook.safe_masked_invalid(a)\n    assert_array_equal(b.mask, [False, False, True, False])\n    assert_array_equal(b.data, a)\n\n    # Check that non-native byte order arrays are handled correctly.\n    a_non_native = a.byteswap().newbyteorder()\n    b_non_native = cbook.safe_masked_invalid(a_non_native)\n    assert_array_equal(b_non_native.mask, b.mask)\n    assert_array_equal(b_non_native.data, b.data)\n\n    # Check that safe_masked_invalid works with non-float arrays.\n    a_int = np.array([1, 2, 3, 4], dtype=int)\n    b_int = cbook.safe_masked_invalid(a_int)\n    assert not np.ma.is_masked(b_int)\n\n    # Check that it returns the original array for object type.\n    arr_object = np.array([1, 'two', np.nan, None], dtype='object')\n    assert cbook.safe_masked_invalid(arr_object) is arr_object\n", "def test__check_1d():\n    # Test _check_1d with different types of input.\n    assert isinstance(cbook._check_1d([1, 2, 3]), np.ndarray)\n    assert isinstance(cbook._check_1d(np.array([1, 2, 3])), np.ndarray)\n    assert isinstance(cbook._check_1d(5), np.ndarray)  # scalar\n    with pytest.raises(ValueError):\n        cbook._check_1d(\"hello\")  # non-numeric\n", "def test_safe_masked_invalid():\n    a = np.array([1, 2, np.nan, 4])\n    b = cbook.safe_masked_invalid(a)\n    assert_array_equal(b.mask, [False, False, True, False])\n    assert_array_equal(b.data, a)\n"], "sample_550": ["def test_set_axisbelow(axis):\n    fig, ax = plt.subplots()\n    ax.plot([0, 1])\n    assert ax.get_axisbelow() is True\n    ax.set_axisbelow(False)\n    assert ax.get_axisbelow() is False\n    ax.grid(True, axis=axis)\n    fig.canvas.draw()\n    renderer = fig.canvas.renderer\n    gridline_zorder = [line.get_zorder() for line in ax.lines if line.get_gid() == f\"{axis}grid\"]\n    tickline_zorder = [line.get_zorder() for line in ax.lines if line.get_gid() == f\"{axis}ticklines\"]\n    if axis == \"x\":\n        spine_zorder = [ax.spines[\"bottom\"].get_zorder()]\n    else:\n        spine_zorder = [ax.spines[\"left\"].get_zorder()]\n    assert all(zorder == 2.5 for zorder in gridline_zorder + tickline_zorder + spine_zorder)\n", "def test_axes_rasterization_zorder():\n    fig, ax = plt.subplots()\n    ax.set_rasterization_zorder(0)\n    ax.plot([1, 2, 3], [1, 2, 3], zorder=-1)\n    ax.plot([1, 2, 3], [2, 3, 4], zorder=1)\n\n    renderer = fig.canvas.get_renderer()\n    assert len(renderer._rasterizing_artists) == 1\n\n    ax.set_rasterization_zorder(None)\n    renderer = fig.canvas.get_renderer()\n    assert len(renderer._rasterizing_artists) == 0\n", "def test_axes_stale():\n    fig, ax = plt.subplots()\n    ax.stale = False\n    assert not ax.stale\n\n    # changing limits should set stale to True\n    ax.set_xlim(1, 2)\n    assert ax.stale\n\n    # draw should reset stale to False\n    ax.draw(RendererBase())\n    assert not ax.stale\n\n    # setting artist properties should also set stale to True\n    line, = ax.plot([1, 2, 3])\n    ax.stale = False\n    line.set_color('red')\n    assert ax.stale\n", "def test_axes_add_artist():\n    fig, ax = plt.subplots()\n    artist = plt.Line2D([0, 1], [0, 1])\n    ax.add_artist(artist)\n    assert artist in ax.artists\n", "def test_axes_set_bound(axis_name):\n    fig, ax = plt.subplots()\n    setter = getattr(ax, f\"set_{axis_name}bound\")\n    getter = getattr(ax, f\"get_{axis_name}bound\")\n\n    setter(-1, 10)\n    assert getter() == (-1, 10)\n\n    setter(lower=-5)\n    assert getter() == (-5, 10)\n\n    setter(upper=5)\n    assert getter() == (-5, 5)\n\n    setter((-10, 0))\n    assert getter() == (-10, 0)\n"], "sample_551": ["def test_zalpha():\n    colors = np.array([[1, 0, 0, 1], [0, 1, 0, 1]])\n    zs = np.array([0.5, 1])\n    rgba = art3d._zalpha(colors, zs)\n    expected_rgba = np.array([[1, 0, 0, 0.65], [0, 1, 0, 0.3]])\n    assert np.allclose(rgba, expected_rgba)\n", "def test_text3d():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n\n    # Test basic text\n    art3d.Text3D(.1, .1, .1, \"Basic Text\", zdir='x', size=1000)\n    ax.set_xlim(-1, 1)\n    ax.set_ylim(-1, 1)\n    ax.set_zlim(-1, 1)\n\n    # Test text with different direction vectors\n    art3d.Text3D(.5, .5, .5, \"Direction Vector (1,1,1)\", zdir=(1, 1, 1), size=1000)\n    art3d.Text3D(.8, .8, .8, \"Direction Vector (-1,-1,-1)\", zdir=(-1, -1, -1), size=1000)\n", "def test_text_2d_to_3d():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    text = ax.text(0.5, 0.5, 'text')\n    art3d.text_2d_to_3d(text, z=0.5)\n    plt.draw()\n", "def test_poly3dcollection_set_verts():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    xs = np.random.rand(5)\n    ys = np.random.rand(5)\n    zs = np.random.rand(5)\n    verts = [np.column_stack([xs, ys, zs])]\n    col = art3d.Poly3DCollection(verts)\n    ax.add_collection(col)\n    ax.set_xlim(-1, 2)\n    ax.set_ylim(-1, 2)\n    ax.set_zlim(-1, 2)\n    new_verts = [np.column_stack([xs*2, ys*2, zs*2])]\n    col.set_verts(new_verts)\n    assert np.array_equal(col.get_verts()[0], new_verts[0])\n    plt.draw()\n", "def test_text3d():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n\n    # Create a Text3D object\n    text = art3d.Text3D(0.5, 0.5, 0.5, 'Hello', zdir='z')\n\n    # Add the Text3D object to the axes\n    ax.add_artist(text)\n\n    # Check that the text is correctly positioned\n    assert text.get_position_3d() == (0.5, 0.5, 0.5)\n    assert text.get_text() == 'Hello'\n    assert text._dir_vec == np.array([0, 0, 1])\n"], "sample_552": ["def test_subfigure_properties():\n    # Test that subfigure properties are passed to the figure patch.\n    fig = Figure()\n    sfig = fig.subfigures(1)[0]\n    assert sfig.patch.get_facecolor() == \"none\"\n    sfig2 = fig.add_subfigure(gridspec.GridSpec(1, 1)[0],\n                              facecolor=\"red\", edgecolor=\"green\",\n                              linewidth=5, frameon=False)\n    assert sfig2.patch.get_facecolor() == \"red\"\n    assert sfig2.patch.get_edgecolor() == \"green\"\n    assert sfig2.patch.get_linewidth() == 5\n    assert not sfig2.patch.get_visible()\n", "def test_figure_suppressComposite():\n    fig = Figure()\n    ax = fig.add_subplot(111)\n    ax.imshow(np.random.rand(20, 20))\n    ax.imshow(np.random.rand(20, 20))\n    fig.suppressComposite = True\n    return fig\n", "def test_set_size_inches(fig):\n    fig.set_size_inches([5, 4])\n    assert np.allclose(fig.get_size_inches(), [5., 4.])\n\n    fig.set_size_inches(3)\n    assert np.allclose(fig.get_size_inches(), [3., 4.])\n\n    with pytest.raises(ValueError):\n        fig.set_size_inches([1, 0, 3])\n", "def test_subfigure():\n    fig = plt.figure()\n    subfigs = fig.subfigures(2, 1)\n    axs0 = subfigs[0].subplots(2, 1)\n    axs1 = subfigs[1].subplots(2, 1)\n    for ax in axs0:\n        ax.plot([1, 2, 3])\n    for ax in axs1:\n        ax.plot([4, 5, 6])\n    fig.canvas.draw()\n    # Check if all the axes are in the figure's axes list\n    assert len(fig.axes) == 4\n    assert all(ax in fig.axes for ax in axs0 + axs1)\n", "def test_layout_engine():\n    fig = Figure()\n    assert isinstance(fig.get_layout_engine(), PlaceHolderLayoutEngine)\n\n    fig.set_layout_engine('constrained')\n    assert isinstance(fig.get_layout_engine(), ConstrainedLayoutEngine)\n\n    fig.set_layout_engine('tight')\n    assert isinstance(fig.get_layout_engine(), TightLayoutEngine)\n\n    fig.set_layout_engine('compressed')\n    assert isinstance(fig.get_layout_engine(), ConstrainedLayoutEngine)\n    assert fig.get_layout_engine().compress\n\n    with pytest.raises(ValueError):\n        fig.set_layout_engine('invalid')\n\n    # Test setting layout engine with parameters\n    fig.set_layout_engine('constrained', w_pad=2, h_pad=3)\n    assert fig.get_layout_engine().w_pad == 2\n    assert fig.get_layout_engine().h_pad == 3\n"], "sample_553": ["def test_animation_save_to_file(tmpdir):\n    fig, ax = plt.subplots()\n    line, = ax.plot([], [])\n\n        line.set_data([], [])\n        return line,\n\n        x = np.linspace(0, 10, 100)\n        y = np.sin(x + i)\n        line.set_data(x, y)\n        return line,\n\n    ani = animation.FuncAnimation(fig, animate, frames=5, init_func=init)\n\n    writer = NullMovieWriter()\n    ani.save(str(tmpdir / \"test\"), writer=writer)\n\n    assert writer.fig is fig\n    assert writer.outfile == str(tmpdir / \"test\")\n    assert writer.dpi == fig.dpi\n    assert writer.args == ()\n    assert writer.savefig_kwargs == {}\n    assert writer._count == 5\n", "def test_validate_grabframe_kwargs():\n    from matplotlib.animation import _validate_grabframe_kwargs\n\n    # Test that _validate_grabframe_kwargs raises an error when 'dpi', \n    # 'bbox_inches' or 'format' are present in savefig_kwargs\n    with pytest.raises(TypeError):\n        _validate_grabframe_kwargs({'dpi': 100})\n    with pytest.raises(TypeError):\n        _validate_grabframe_kwargs({'bbox_inches': 'tight'})\n    with pytest.raises(TypeError):\n        _validate_grabframe_kwargs({'format': 'png'})\n\n    # Test that _validate_grabframe_kwargs raises an error when \n    # rcParams['savefig.bbox'] is 'tight'\n    mpl.rcParams['savefig.bbox'] = 'tight'\n    with pytest.raises(ValueError):\n        _validate_grabframe_kwargs({})\n", "def test_animation_init_with_invalid_blit():\n    fig, ax = plt.subplots()\n    with pytest.raises(ValueError):\n        animation.Animation(fig, blit='invalid')\n    with pytest.raises(TypeError):\n        animation.Animation(fig, blit=123)\n", "def test_animation_invalid_grabframe_kwargs(tmpdir):\n    writer = NullMovieWriter()\n    fig = plt.figure()\n    writer.setup(fig, str(tmpdir / 'test'), 100)\n    \n    with pytest.raises(ValueError):\n        writer.grab_frame(bbox_inches='tight')\n\n    with pytest.raises(TypeError):\n        writer.grab_frame(dpi=100)\n\n    with pytest.raises(TypeError):\n        writer.grab_frame(format='png')\n", "def test_grab_frame_kwargs_validation():\n    fig, ax = plt.subplots()\n    writer = NullMovieWriter()\n    writer.setup(fig, 'dummy', 100)\n\n        writer.grab_frame(**kwargs)\n\n    # Test that dpi and bbox_inches are disallowed\n    with pytest.raises(TypeError):\n        grab_frame(dpi=50)\n    with pytest.raises(TypeError):\n        grab_frame(bbox_inches='tight')\n\n    # Test that savefig.bbox='tight' raises an error\n    with mpl.rc_context({'savefig.bbox': 'tight'}):\n        with pytest.raises(ValueError):\n            grab_frame()\n"], "sample_554": ["def test_annotation_multiline_text(fig_test, fig_ref):\n    fig_test.subplots()\n    Annotation(\"Line 1\\nLine 2\", (0.5, 0.5), xycoords='axes fraction',\n               textcoords=\"offset points\", xytext=(0, 10),\n               arrowprops=dict(arrowstyle=\"->\"), figure=fig_test)\n    fig_ref.subplots()\n    Annotation(\"Line 1\\nLine 2\", (0.5, 0.5), xycoords='axes fraction',\n               textcoords=\"offset points\", xytext=(0, 10),\n               arrowprops=dict(arrowstyle=\"->\"), figure=fig_ref)\n    assert len(fig_test.artists) == len(fig_ref.artists)\n", "def test_annotation_negative_shrink():\n    fig, ax = plt.subplots()\n    ann = ax.annotate(\"Test\", (0.5, 0.5), xytext=(0.1, 0.1),\n                      arrowprops=dict(shrink=-10))\n    fig.canvas.draw()\n    assert ann.arrow_patch.shrinkA == 0\n    assert ann.arrow_patch.shrinkB == 0\n", "def test_text_set_bbox():\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, 'Hello')\n    text.set_bbox(dict(facecolor='red', alpha=0.5))\n    assert isinstance(text.get_bbox_patch(), mpatches.FancyBboxPatch)\n    plt.close(fig)\n", "def test_text_get_window_extent():\n    fig, ax = plt.subplots()\n    text = Text(0.5, 0.5, 'test')\n    ax.add_artist(text)\n    fig.canvas.draw()\n    extent = text.get_window_extent()\n    assert isinstance(extent, mtransforms.Bbox)\n    assert extent.width > 0\n    assert extent.height > 0\n", "def test_text_contains():\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, 'hello')\n    event = MouseEvent('button_press_event', fig.canvas, 100, 100)\n    contains, _ = text.contains(event)\n    assert not contains\n\n    # Test with out of bounds coordinates\n    event = MouseEvent('button_press_event', fig.canvas, -1, -1)\n    contains, _ = text.contains(event)\n    assert not contains\n"], "sample_555": ["def test_annulus_angles():\n    # Test that Annulus's theta1 and theta2 are applied correctly.\n    annulus = Annulus((0, 0), 5, 1)\n    assert annulus.theta1 == 0\n    assert annulus.theta2 == 360\n\n    annulus = Annulus((0, 0), 5, 1, theta1=90, theta2=180)\n    assert annulus.theta1 == 90\n    assert annulus.theta2 == 180\n\n    # Test that the angles are validated correctly.\n    with pytest.raises(ValueError):\n        Annulus((0, 0), 5, 1, theta1=-1)\n\n    with pytest.raises(ValueError):\n        Annulus((0, 0), 5, 1, theta2=361)\n\n    with pytest.raises(ValueError):\n        Annulus((0, 0), 5, 1, theta1=180, theta2=90)\n", "def test_rectangle_get_bbox(kwargs, expected):\n    rectangle = Rectangle(**kwargs)\n    assert_array_equal(rectangle.get_bbox().bounds, expected.bounds)\n", "def test_fancyarrow_patch():\n    fig, ax = plt.subplots()\n\n    # Test with two points.\n    xy1 = [0.2, 0.2]\n    xy2 = [0.8, 0.8]\n    arrow = FancyArrowPatch(posA=xy1, posB=xy2,\n                            connectionstyle=\"arc3,rad=.2\",\n                            arrowstyle=\"Fancy,head_width=10,head_length=20\",\n                            lw=2, edgecolor='red', facecolor='blue')\n    ax.add_patch(arrow)\n\n    # Test with a path.\n    path = mpath.Path([(0.1, 0.1), (0.3, 0.7), (0.5, 0.3), (0.9, 0.9)])\n    arrow = FancyArrowPatch(path=path,\n                            arrowstyle=\"simple,head_width=10,head_length=20\",\n                            lw=2, edgecolor='red', facecolor='blue')\n    ax.add_patch(arrow)\n\n    ax.set_xlim([0, 1])\n    ax.set_ylim([0, 1])\n\n    assert arrow.get_window_extent(ax.get_renderer()) != Bbox([[0, 0], [0, 0]])\n", "def test_fancyarrowpatch_connectionstyle():\n    fig, ax = plt.subplots()\n    ax.set_aspect('equal')\n\n    # Define two points in data coordinates.\n    posA = (0.1, 0.3)\n    posB = (0.4, 0.8)\n\n    # Create a FancyArrowPatch with the specified positions and connection style.\n    arrow = FancyArrowPatch(posA=posA, posB=posB,\n                            connectionstyle=\"arc3,rad=.2\")\n\n    # Add the arrow to the axes.\n    ax.add_patch(arrow)\n\n    # Check if the arrow is correctly added to the axes.\n    assert arrow in ax.patches\n\n    # Check if the arrow has the correct connection style.\n    assert isinstance(arrow.get_connectionstyle(), mpatches.ConnectionStyle._Base)\n\n    plt.close(fig)\n", "def test_patch_alpha():\n    # Test that we can set the alpha of a patch and it gets passed to the\n    # underlying colors.\n    p = Patch(facecolor='r', edgecolor='b')\n    assert p.get_facecolor()[0, 3] == 1\n    assert p.get_edgecolor()[0, 3] == 1\n\n    p.set_alpha(0.5)\n    assert p.get_facecolor()[0, 3] == 0.5\n    assert p.get_edgecolor()[0, 3] == 0.5\n\n    p.set_facecolor('g')\n    assert p.get_facecolor()[0, 3] == 0.5\n\n    p.set_edgecolor('y')\n    assert p.get_edgecolor()[0, 3] == 0.5\n"], "sample_556": ["def test_figure_constrained_layout_pads():\n    fig = Figure()\n    fig.set_layout_engine('constrained')\n    assert fig.get_constrained_layout_pads() == (None, None, None, None)\n    fig.set_constrained_layout_pads(w_pad=1, h_pad=2)\n    assert fig.get_constrained_layout_pads() == (1, 2, None, None)\n    fig.set_constrained_layout_pads(wspace=0.1, hspace=0.2)\n    assert fig.get_constrained_layout_pads() == (1, 2, 0.1, 0.2)\n", "def test_figure_repr():\n    fig = Figure()\n    repr_fig = repr(fig)\n    assert 'Figure' in repr_fig\n    assert 'size' in repr_fig\n    assert 'Axes' in repr_fig\n    assert str(id(fig)) not in repr_fig  # check that no memory address is included\n\n    fig2 = Figure()\n    fig2.add_subplot(111)\n    repr_fig2 = repr(fig2)\n    assert 'Figure' in repr_fig2\n    assert 'size' in repr_fig2\n    assert '1 Axes' in repr_fig2\n", "def test_layout_engine_change():\n    fig = Figure()\n    assert isinstance(fig.get_layout_engine(), PlaceHolderLayoutEngine)\n    fig.set_layout_engine('constrained')\n    assert isinstance(fig.get_layout_engine(), ConstrainedLayoutEngine)\n    fig.set_layout_engine('tight')\n    assert isinstance(fig.get_layout_engine(), TightLayoutEngine)\n    fig.set_layout_engine(None)\n    assert fig.get_layout_engine() is None\n", "def test_figure_add_subplot():\n    fig = Figure()\n    ax = fig.add_subplot(111)\n    assert isinstance(ax, Axes)\n    assert len(fig.axes) == 1\n\n    # Test that adding a subplot with the same arguments returns the same axes\n    ax2 = fig.add_subplot(111)\n    assert ax is ax2\n    assert len(fig.axes) == 1\n\n    # Test that adding a subplot with different arguments creates a new axes\n    ax3 = fig.add_subplot(122)\n    assert ax3 is not ax\n    assert len(fig.axes) == 2\n", "def test_subfigure_properties():\n    fig = Figure()\n    sfig1 = fig.subfigures(1, 2)[0]\n    sfig2 = sfig1.subfigures(2, 1)[0]\n\n    # Check inherited properties\n    assert sfig1.dpi == fig.dpi\n    assert sfig2.dpi == fig.dpi\n\n    # Check setting properties\n    sfig1.set_facecolor('red')\n    sfig1.set_edgecolor('blue')\n    assert sfig1.get_facecolor() == (1.0, 0.0, 0.0, 1.0)\n    assert sfig1.get_edgecolor() == (0.0, 0.0, 1.0, 1.0)\n\n    # Check that subfigure's axes have correct figure\n    ax = sfig1.subplots()\n    assert ax[0].figure == fig\n"], "sample_557": ["def test_subfigure_add_subplot():\n    fig = Figure()\n    subfigs = fig.subfigures(1, 2)\n    ax1 = subfigs[0].add_subplot(121)\n    ax2 = subfigs[0].add_subplot(122)\n    ax3 = subfigs[1].add_subplot(111)\n\n    assert len(fig.axes) == 3\n    assert len(subfigs[0].axes) == 2\n    assert len(subfigs[1].axes) == 1\n\n    assert ax1.figure is fig\n    assert ax2.figure is fig\n    assert ax3.figure is fig\n", "def test_figure_constrained_layout_pads():\n    fig = Figure(constrained_layout=True)\n    fig.set_constrained_layout_pads(w_pad=0.1, h_pad=0.2, wspace=0.3, hspace=0.4)\n\n    # Check if the layout engine is correctly set\n    assert isinstance(fig.get_layout_engine(), ConstrainedLayoutEngine)\n\n    # Check if the pads are correctly set\n    expected_pads = {'w_pad': 0.1, 'h_pad': 0.2, 'wspace': 0.3, 'hspace': 0.4}\n    actual_pads = fig.get_constrained_layout_pads()\n    assert actual_pads == expected_pads\n\n    # Check if the figure can be pickled and unpickled with the correct pads\n    fig_pickled = pickle.dumps(fig)\n    fig_unpickled = pickle.loads(fig_pickled)\n    assert isinstance(fig_unpickled.get_layout_engine(), ConstrainedLayoutEngine)\n    actual_pads_unpickled = fig_unpickled.get_constrained_layout_pads()\n    assert actual_pads_unpickled == expected_pads\n", "def test_figure_suptitle():\n    fig = Figure()\n    assert fig.get_suptitle() == \"\"\n\n    fig.suptitle(\"test\")\n    assert fig.get_suptitle() == \"test\"\n\n    fig.suptitle(None)\n    assert fig.get_suptitle() == \"\"\n", "def test_get_tightbbox():\n    fig = Figure()\n    ax = fig.subplots()\n    ax.plot([1, 2, 3])\n    bbox = fig.get_tightbbox()\n    assert isinstance(bbox, mpl.transforms.BboxBase)\n", "def test_figure_too_many_subplots():\n    fig = Figure()\n    with pytest.warns(UserWarning, match=\"More than 20 subplots\"):\n        fig.subplots(25)\n"], "sample_558": ["def test_grid_share_axes(direction, share_all):\n    fig = plt.figure()\n    grid = Grid(fig, 111, (2, 2), direction=direction, share_all=share_all)\n    \n    if share_all:\n        assert grid.axes_llc is grid.axes_all[0]\n        for ax in grid.axes_all[1:]:\n            assert ax.sharex is grid.axes_all[0]\n            assert ax.sharey is grid.axes_all[0]\n    else:\n        assert grid.axes_llc is grid.axes_all[-1]\n        for i, ax in enumerate(grid.axes_all):\n            if direction == \"row\":\n                assert ax.sharex is grid.axes_all[i // 2 * 2]\n                assert ax.sharey is None\n            else:\n                assert ax.sharex is None\n                assert ax.sharey is grid.axes_all[i % 2]\n", "def test_grid_get_col_row(nrows, ncols, direction):\n    grid = Grid(plt.figure(), (0.1, 0.1, 0.8, 0.8), (nrows, ncols), direction=direction)\n    for i in range(nrows * ncols):\n        col, row = grid._get_col_row(i)\n        assert 0 <= col < ncols\n        assert 0 <= row < nrows\n", "def test_imagegrid_cbar_mode():\n    fig = plt.figure()\n    grid = ImageGrid(fig, 111, (2, 2), cbar_mode='each')\n    assert len(grid.cbar_axes) == 4\n\n    grid = ImageGrid(fig, 111, (2, 2), cbar_mode='single')\n    assert len(grid.cbar_axes) == 1\n\n    grid = ImageGrid(fig, 111, (2, 2), cbar_mode='edge')\n    assert len(grid.cbar_axes) == 2\n", "def test_imagegrid(direction, share_all, aspect):\n    fig = plt.figure()\n    grid = ImageGrid(fig, 111, (2, 2), direction=direction, share_all=share_all, aspect=aspect)\n    assert len(grid.axes_all) == 4\n    for ax in grid.axes_all:\n        assert ax.get_aspect() == 'equal' if aspect else 'auto'\n    plt.close(fig)\n", "def test_imagegrid_cbar_mode():\n    fig = plt.figure()\n    grid = ImageGrid(fig, 111, (2, 2), cbar_mode='each')\n    assert len(grid.cbar_axes) == 4\n\n    grid = ImageGrid(fig, 111, (2, 2), cbar_mode='single')\n    assert len(grid.cbar_axes) == 1\n\n    grid = ImageGrid(fig, 111, (2, 2), cbar_mode='edge')\n    assert len(grid.cbar_axes) == 2\n\n    grid = ImageGrid(fig, 111, (2, 2), cbar_mode=None)\n    assert len(grid.cbar_axes) == 4\n    for ax in grid.cbar_axes:\n        assert not ax.get_visible()\n"], "sample_560": ["def test_legend_title_fontproperties(fig_test, fig_ref):\n    fig_test.subplots()\n    fig_ref.subplots()\n\n    prop = FontProperties(size=20, weight='bold')\n    fig_test.legend(['Test'], title='Title', title_fontproperties=prop)\n    fig_ref.legend(['Test'], title='Title', title_fontproperties=prop)\n\n    # Checking if title_fontsize is set correctly in the legend\n    assert fig_test.legends[0].get_title().get_fontsize() == 20\n\n    # Checking if title_fontproperties are set correctly in the legend\n    assert fig_test.legends[0].get_title().get_fontproperties().get_weight() == 'bold'\n", "def test_legend_labelcolor():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3], label='test')\n    with rc_context({'text.color': 'red', 'legend.labelcolor': None}):\n        leg = ax.legend()\n        assert leg.texts[0].get_color() == 'red'\n\n    with rc_context({'text.color': 'blue', 'legend.labelcolor': 'green'}):\n        leg = ax.legend()\n        assert leg.texts[0].get_color() == 'green'\n\n    ax.plot([1, 2, 3], label='test2', color='yellow')\n    leg = ax.legend(labelcolor='linecolor')\n    assert leg.texts[1].get_color() == 'yellow'\n\n    ax.plot([1, 2, 3], label='test3', color='pink')\n    leg = ax.legend(labelcolor=['green', 'red', 'yellow'])\n    assert leg.texts[2].get_color() == 'yellow'\n", "def test_legend_auto():\n    fig, ax = plt.subplots()\n    x = np.linspace(0, 10, 100)\n    for i in range(5):\n        ax.plot(x, x**i, label=f'x^{i}')\n\n    ax.legend(loc='best')\n    return fig\n", "def test_legend_title_visibility():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2], label='test')\n    legend = ax.legend(title='Title')\n    assert legend.get_title().get_visible()\n\n    legend.set_title('')\n    assert not legend.get_title().get_visible()\n\n    legend.set_title('New title')\n    assert legend.get_title().get_visible()\n", "def test_legend_get_texts():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3], label='a')\n    ax.plot([4, 5, 6], label='b')\n    legend = ax.legend()\n\n    # Test that get_texts returns a silent list of Text instances.\n    assert isinstance(legend.get_texts(), mlegend.silent_list)\n    for text in legend.get_texts():\n        assert isinstance(text, mpl.text.Text)\n"], "sample_561": ["def test_default_fillstyle(marker, expected_fillstyle):\n    marker_style = markers.MarkerStyle(marker)\n    assert marker_style.get_fillstyle() == expected_fillstyle\n", "def test_marker_paths(marker, expected_path_type):\n    marker_style = markers.MarkerStyle(marker)\n    assert isinstance(marker_style.get_path(), expected_path_type)\n", "def test_marker_fillstyle(fig_test, fig_ref):\n    fig_test.subplots()\n    fig_ref.subplots()\n\n    marker = markers.MarkerStyle(marker='o', fillstyle='top')\n    fig_test.gca().plot([1, 2, 3], marker=marker, linestyle='None')\n\n    fig_ref.gca().plot([1, 2, 3], marker='o', fillstyle='top', linestyle='None')\n", "def test_marker_fillstyle(marker, fillstyle):\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3], marker=marker, markersize=20, fillstyle=fillstyle)\n    assert len(ax.lines) == 1\n    line = ax.lines[0]\n    assert isinstance(line.get_marker(), markers.MarkerStyle)\n    assert line.get_marker().get_fillstyle() == fillstyle\n    plt.close(fig)\n", "def test_custom_marker(fig_test, fig_ref):\n    fig_test.subplots()\n    marker = Path.unit_circle()\n    fig_test.gca().plot([1, 2, 3], marker=marker)\n    fig_test.gca().set_xlim(0, 4)\n    fig_test.gca().set_ylim(-1, 1)\n\n    fig_ref.subplots()\n    fig_ref.gca().plot([1, 2, 3], marker='o')\n    fig_ref.gca().set_xlim(0, 4)\n    fig_ref.gca().set_ylim(-1, 1)\n"], "sample_562": ["def test_axline_transform():\n    fig, ax = plt.subplots()\n    line = mlines.AxLine((0.1, 0.2), (0.8, 0.7), slope=None)\n    ax.add_line(line)\n\n    # Force the transform to be calculated.\n    line.get_transform()\n\n    assert isinstance(line.get_transform(), mtransforms.BboxTransformTo)\n    fig.canvas.draw()\n\n    # Check that points outside of the viewLim are clipped correctly.\n    line = mlines.AxLine((-10, 0), (10, 0))\n    ax.add_line(line)\n    fig.canvas.draw()\n", "def test_line2d_markevery():\n    fig, ax = plt.subplots()\n    x = np.arange(10)\n    y = np.arange(10)\n    line, = ax.plot(x, y, 'o-', markevery=3)\n\n    expected_markers = np.array([[0, 0], [3, 3], [6, 6], [9, 9]])\n    assert_array_equal(line.get_markevery(), 3)\n    assert_array_equal(line._marker._path.vertices, expected_markers)\n", "def test_line2d_get_dash_pattern(style, expected):\n    offset, dashes = mlines._get_dash_pattern(style)\n    if expected == 'solid':\n        assert dashes is None\n    elif expected == 'draw_nothing':\n        assert dashes is None and offset == 0\n    else:\n        assert dashes == tuple(mpl.rcParams[f'lines.{expected}_pattern'])\n", "def test_line2d_contains():\n    fig, ax = plt.subplots()\n    line, = ax.plot([0, 1], [0, 1])\n\n    event = SimpleNamespace(\n        x=0.5,\n        y=0.5,\n        xdata=None,\n        ydata=None,\n        inaxes=ax\n    )\n\n    contains, _ = line.contains(event)\n    assert contains\n\n    event.x = 10\n    event.y = 10\n    contains, _ = line.contains(event)\n    assert not contains\n", "def test_Line2D_set_gapcolor():\n    fig, ax = plt.subplots()\n    line, = ax.plot([1, 2, 3])\n    line.set_linestyle('--')\n    line.set_gapcolor('red')\n    assert line.get_gapcolor() == 'red'\n    line.set_gapcolor(None)\n    assert line.get_gapcolor() is None\n    plt.close(fig)\n"], "sample_563": ["def test_OffsetBox():\n    # Create a simple OffsetBox with a single child\n    child = mpatches.Rectangle((0, 0), 1, 1)\n    box = OffsetBox()\n    box._children = [child]\n\n    # Test get_visible_children method\n    assert box.get_visible_children() == [child]\n\n    # Test get_children method\n    assert box.get_children() == [child]\n\n    # Test set_figure method\n    fig = plt.figure()\n    box.set_figure(fig)\n    assert child.get_figure() == fig\n\n    # Test set_axes method\n    ax = fig.add_subplot(111)\n    box.axes = ax\n    assert child.axes == ax\n", "def test_offsetbox_set_offset():\n    fig, ax = plt.subplots()\n    box = OffsetBox()\n    box.set_offset((10, 20))\n    assert box.get_offset() == (10, 20)\n    box.set_offset(lambda x, y, w, h, renderer: (30, 40))\n    assert callable(box.get_offset())\n", "def test_AnnotationBbox_update_positions():\n    fig, ax = plt.subplots()\n    annotation = AnnotationBbox(\n        OffsetBox(),\n        (0.5, 0.5),\n        xycoords='data',\n        boxcoords='offset points',\n        pad=0.4,\n        arrowprops=dict(arrowstyle='->')\n    )\n    ax.add_artist(annotation)\n\n    # Test update_positions without renderer\n    with pytest.raises(AttributeError):\n        annotation.update_positions(None)\n\n    # Test update_positions with renderer\n    renderer = fig.canvas.get_renderer()\n    annotation.update_positions(renderer)\n", "def test_offsetbox_children():\n    # Test that children are correctly added and removed from an OffsetBox\n    box = OffsetBox()\n    child1 = TextArea(\"Hello\")\n    child2 = TextArea(\"World\")\n\n    box._children = [child1]\n    assert box.get_children() == [child1]\n\n    box.add_artist(child2)\n    assert box.get_children() == [child1, child2]\n\n    box._children = []\n    assert box.get_children() == []\n", "def test_AnchoredOffsetbox():\n    fig, ax = plt.subplots()\n    box = TextArea(\"Test\")\n    anchored_box = AnchoredOffsetbox(loc=1, child=box)\n    ax.add_artist(anchored_box)\n    fig.canvas.draw()\n\n    assert anchored_box.contains(MouseEvent(\n        name='test_event', canvas=fig.canvas, button=MouseButton.LEFT,\n        x=anchored_box.get_window_extent().x0 + 10,\n        y=anchored_box.get_window_extent().y0 + 10, xdata=None, ydata=None))\n"], "sample_565": ["def test_inset_locator():\n    fig, ax = plt.subplots()\n    inset_ax = inset_axes(ax, \"30%\", \"40%\")\n    assert isinstance(inset_ax, mpl.axes.Axes)\n    assert inset_ax.get_position().width == 0.3\n    assert inset_ax.get_position().height == 0.4\n\n    inset_ax = zoomed_inset_axes(ax, 2)\n    assert isinstance(inset_ax, mpl.axes.Axes)\n    assert inset_ax.get_xlim() == (0, 1)\n    assert inset_ax.get_ylim() == (0, 1)\n\n    pp, p1, p2 = mark_inset(ax, inset_ax, loc1=1, loc2=3)\n    assert isinstance(pp, mpl.patches.Patch)\n    assert isinstance(p1, mpl.patches.Patch)\n    assert isinstance(p2, mpl.patches.Patch)\n", "def test_inset_axes():\n    fig, ax = plt.subplots()\n    inset_ax = inset_axes(ax, width=\"30%\", height=\"40%\")\n    assert isinstance(inset_ax, mpl.axes.Axes)\n    assert inset_ax.get_position().width == 0.3\n    assert inset_ax.get_position().height == 0.4\n\n    # Test inset axes with different loc and bbox_to_anchor\n    inset_ax = inset_axes(ax, width=\"30%\", height=\"40%\", loc=\"lower left\")\n    assert inset_ax.get_position().x0 < ax.get_position().x0\n    assert inset_ax.get_position().y0 < ax.get_position().y0\n\n    inset_ax = inset_axes(ax, width=\"30%\", height=\"40%\", loc=\"upper right\",\n                          bbox_to_anchor=(0.5, 0.5))\n    assert inset_ax.get_position().x0 > ax.get_position().x0\n    assert inset_ax.get_position().y0 > ax.get_position().y0\n\n    # Test inset axes with borderpad\n    inset_ax = inset_axes(ax, width=\"30%\", height=\"40%\", borderpad=1)\n    assert inset_ax.get_position().width < 0.3\n    assert inset_ax.get_position().height < 0.4\n", "def test_inset_axes():\n    fig, ax = plt.subplots()\n    inset_ax = inset_axes(ax, \"30%\", \"40%\")\n    assert isinstance(inset_ax, mpl_toolkits.axes_grid1.mpl_axes.HostAxes)\n    assert inset_ax.get_position().width > 0\n    assert inset_ax.get_position().height > 0\n\n    # Test with bbox_to_anchor and bbox_transform\n    inset_ax2 = inset_axes(ax, \"30%\", \"40%\", bbox_to_anchor=(0.5, 0.5),\n                           bbox_transform=ax.transAxes)\n    assert isinstance(inset_ax2, mpl_toolkits.axes_grid1.mpl_axes.HostAxes)\n    assert inset_ax2.get_position().width > 0\n    assert inset_ax2.get_position().height > 0\n", "def test_inset_axes():\n    fig, ax = plt.subplots()\n    inset_ax = inset_axes(ax, width=\"30%\", height=\"40%\", loc=\"lower left\")\n    assert isinstance(inset_ax, mpl_toolkits.axes_grid1.mpl_axes.Axes)\n    assert inset_ax.get_position().x0 < 0.5\n    assert inset_ax.get_position().y0 < 0.5\n\n    # Test that the inset axes is correctly sized\n    fig.canvas.draw()\n    assert np.isclose(inset_ax.get_window_extent().width / fig.dpi,\n                      0.3 * ax.get_window_extent().width / fig.dpi, atol=1)\n\n    # Test that the inset axes is correctly placed\n    assert np.isclose(inset_ax.get_window_extent().x0 / fig.dpi,\n                      0.05 * ax.get_window_extent().width / fig.dpi, atol=1)\n", "def test_inset_axes_locator():\n    fig, ax = plt.subplots()\n    inset_ax = inset_axes(ax, \"30%\", \"40%\")\n    assert isinstance(inset_ax.axes_locator, AnchoredSizeLocator)\n    assert inset_ax.axes_locator.x_size.get_size(inset_ax.figure.canvas.renderer) == (0.3, 0)\n\n    inset_ax = zoomed_inset_axes(ax, 2)\n    assert isinstance(inset_ax.axes_locator, AnchoredZoomLocator)\n    assert inset_ax.axes_locator.zoom == 2\n\n    inset_ax = inset_axes(ax, \"30%\", \"40%\", loc='lower left')\n    assert inset_ax.axes_locator.loc == 'lower left'\n\n    plt.close(fig)\n"], "sample_566": ["def test_figure_repr():\n    fig = Figure()\n    repr_fig = repr(fig)\n    assert 'Figure' in repr_fig\n    assert 'size' in repr_fig\n\n    fig2 = plt.figure()\n    repr_fig2 = repr(fig2)\n    assert 'Figure' in repr_fig2\n    assert 'size' in repr_fig2\n", "def test_figure_suptitle_getter():\n    fig = Figure()\n    assert fig.get_suptitle() == \"\"\n    fig.suptitle(\"Hello\")\n    assert fig.get_suptitle() == \"Hello\"\n", "def test_suptitle(fig_test, fig_ref):\n    fig_test.suptitle('Test Title')\n    fig_ref.text(0.5, 0.98, 'Test Title', ha='center', va='top',\n                 size=mpl.rcParams['figure.titlesize'])\n", "def test_subfigure_repr():\n    fig = Figure()\n    subfig = fig.subfigures(1)[0]\n    assert isinstance(repr(subfig), str)\n    assert isinstance(str(subfig), str)\n", "def test_figure_repr():\n    fig = Figure()\n    repr_fig = repr(fig)\n    assert \"Figure\" in repr_fig\n    assert \"size\" in repr_fig\n    assert \"x\" in repr_fig\n    assert str(len(fig.axes)) in repr_fig\n"], "sample_567": ["def test_annotation_update_positions():\n    fig, ax = plt.subplots()\n    ann = Annotation(\"Test\", (0.5, 0.5), xycoords='axes fraction')\n    ax.add_artist(ann)\n    renderer = fig.canvas.get_renderer()\n\n    # Force the update of positions\n    with warnings.catch_warnings(record=True):\n        ann.update_positions(renderer)\n\n    # Assert arrow_patch is None when not set\n    assert ann.arrow_patch is None\n\n    # Set arrowprops and check that update_positions updates it correctly\n    ann.arrowprops = dict(arrowstyle=\"->\")\n    with warnings.catch_warnings(record=True):\n        ann.update_positions(renderer)\n    assert ann.arrow_patch is not None\n    assert ann.arrow_patch.arrowstyle == \"->\"\n\n    # Check that the mutation scale is set correctly\n    assert_almost_equal(ann.arrow_patch.mutation_scale, ann.get_size())\n", "def test_annotation_clip():\n    fig, ax = plt.subplots()\n    annot = Annotation(\"Test\", (0.5, 0.5), xycoords='axes fraction')\n    annot.set_annotation_clip(False)\n    ax.add_artist(annot)\n    assert annot.get_annotation_clip() is False\n    annot.set_annotation_clip(True)\n    assert annot.get_annotation_clip() is True\n    annot.set_annotation_clip(None)\n    assert annot.get_annotation_clip() is None\n", "def test_annotation_with_fancy_arrow():\n    fig, ax = plt.subplots()\n    ann = Annotation(\"Test\", (0.5, 0.5), xycoords=\"axes fraction\",\n                     arrowprops=dict(arrowstyle=\"fancy\"))\n    ax.add_artist(ann)\n    fig.canvas.draw_idle()\n\n    # Check that the arrow is correctly positioned.\n    assert ann.arrow_patch is not None\n    assert_almost_equal(ann.arrow_patch.get_positions(),\n                        np.array([[0.5, 0.5], [0.5, 0.5]]))\n", "def test_text_contains():\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, 'test')\n    event = MouseEvent('button_press_event', fig.canvas, 0.5, 0.5)\n    contains, _ = text.contains(event)\n    assert contains\n\n    event = MouseEvent('button_press_event', fig.canvas, 0.1, 0.1)\n    contains, _ = text.contains(event)\n    assert not contains\n", "def test_text_contains():\n    fig, ax = plt.subplots()\n    text = Text(0.5, 0.5, 'test')\n    ax.add_artist(text)\n    event = MouseEvent('button_press_event', fig.canvas, 0.5, 0.5, button=1)\n    assert text.contains(event)[0]\n    event = MouseEvent('button_press_event', fig.canvas, 0.7, 0.7, button=1)\n    assert not text.contains(event)[0]\n"], "sample_568": ["def test_text3d_properties():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    text = ax.text(0, 0, 0, 'Hello World', zdir='x')\n\n    # Check initial properties\n    assert text._x == 0\n    assert text._y == 0\n    assert text._z == 0\n    assert np.all(text._dir_vec == [1, 0, 0])\n\n    # Update properties\n    text.set_position_3d([1, 2, 3], 'y')\n    assert text._x == 1\n    assert text._y == 2\n    assert text._z == 3\n    assert np.all(text._dir_vec == [0, 1, 0])\n", "def test_Poly3DCollection_shading():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n\n    vertices = [np.array([[0, 0, 0], [1, 0, 0], [1, 1, 0]])]\n    poly = art3d.Poly3DCollection(vertices, color='b', shade=True)\n\n    ax.add_collection3d(poly)\n    ax.set_xlim(-1, 2)\n    ax.set_ylim(-1, 2)\n    ax.set_zlim(-1, 2)\n", "def test_art3d_line_collection_2d_to_3d():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n\n    # create a 2D line collection\n    lc = LineCollection([[[0, 0], [1, 1]], [[0, 1], [1, 0]]])\n    art3d.line_collection_2d_to_3d(lc, zs=1)\n\n    ax.add_collection3d(lc)\n\n    assert isinstance(lc, art3d.Line3DCollection)\n", "def test_Poly3DCollection_shading():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n\n    # Create a Poly3DCollection\n    vertices = [np.array([[0, 0, 0], [1, 0, 0], [1, 1, 0], [0, 1, 0]]),\n                np.array([[0, 0, 1], [1, 0, 1], [1, 1, 1], [0, 1, 1]])]\n    poly = art3d.Poly3DCollection(vertices, facecolors='r', edgecolors='k')\n    ax.add_collection3d(poly)\n\n    # Test that shading is applied correctly\n    ax.set_xlim(-1, 2)\n    ax.set_ylim(-1, 2)\n    ax.set_zlim(-1, 2)\n\n    # Check that the collection is shaded\n    assert poly.get_depthshade()\n", "def test_text3d_zdir():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    text = art3d.Text3D(0.5, 0.5, 0.5, \"Text\", zdir='x')\n    ax.add_artist(text)\n    ax.set_xlim(-1, 1)\n    ax.set_ylim(-1, 1)\n    ax.set_zlim(-1, 1)\n\n    fig2 = plt.figure()\n    ax2 = fig2.add_subplot(111, projection='3d')\n    text2 = art3d.Text3D(0.5, 0.5, 0.5, \"Text\", zdir='y')\n    ax2.add_artist(text2)\n    ax2.set_xlim(-1, 1)\n    ax2.set_ylim(-1, 1)\n    ax2.set_zlim(-1, 1)\n\n    @check_figures_equal(extensions=[\"png\"])\n        return fig, fig2\n"], "sample_569": ["def test_regressor_partial():\n    fig, ax = plt.subplots()\n    df = self.df.copy()\n    lm.regplot(\"x\", \"y\", data=df, x_partial=\"d\", ax=ax)\n    assert len(ax.lines) == 1\n    assert len(ax.collections) == 1\n    fig, ax = plt.subplots()\n    lm.regplot(\"x\", \"y\", data=df, y_partial=\"d\", ax=ax)\n    assert len(ax.lines) == 1\n    assert len(ax.collections) == 1\n    fig, ax = plt.subplots()\n    lm.regplot(\"x\", \"y\", data=df, x_partial=\"d\", y_partial=\"s\", ax=ax)\n    assert len(ax.lines) == 1\n    assert len(ax.collections) == 1\n", "def test_regression_plotter_dropna(self):\n\n    p = lm._RegressionPlotter(\"x\", \"y_na\", data=self.df, dropna=True)\n    npt.assert_equal(p.x.shape, (57,))\n    npt.assert_equal(p.y.shape, (57,))\n\n    p = lm._RegressionPlotter(\"x\", \"y_na\", data=self.df, dropna=False)\n    npt.assert_equal(p.x.shape, (60,))\n    npt.assert_equal(p.y.shape, (60,))\n", "def test_regress_out(self):\n    df = self.df.copy()\n    res_w_na = lm._RegressionPlotter(x=\"x\", y=\"y_na\", data=df)\n    res_no_na = lm._RegressionPlotter(x=\"x\", y=\"y\", data=df)\n    res_w_na.regress_out(\"x\", \"d\")\n    res_no_na.regress_out(\"x\", \"d\")\n    npt.assert_array_equal(res_w_na.x, res_no_na.x)\n", "def test_regress_out(self):\n    x = self.rs.randn(100)\n    y = 3 * x + self.rs.randn(100) / 1.5\n    z = 2 * x + self.rs.randn(100) / 1.5\n\n    x_regressed = lm._RegressionPlotter.regress_out(x, z)\n    y_regressed = lm._RegressionPlotter.regress_out(y, z)\n\n    npt.assert_almost_equal(np.corrcoef(x_regressed, z)[0, 1], 0, decimal=1)\n    npt.assert_almost_equal(np.corrcoef(y_regressed, z)[0, 1], 0, decimal=1)\n    npt.assert_almost_equal(np.corrcoef(x_regressed, y_regressed)[0, 1],\n                            np.corrcoef(x, y)[0, 1], decimal=2)\n", "def test_regression_plotter_dropna(self):\n    p = lm._RegressionPlotter(\"x\", \"y_na\", data=self.df)\n    assert len(p.x) == len(self.df)\n    p.dropna(\"x\", \"y_na\")\n    npt.assert_array_equal(p.x, self.df.x[~self.df.y_na.isnull()])\n    npt.assert_array_equal(p.y, self.df.y_na[~self.df.y_na.isnull()])\n\n    p = lm._RegressionPlotter(\"x\", \"y_na\", data=self.df, dropna=True)\n    npt.assert_array_equal(p.x, self.df.x[~self.df.y_na.isnull()])\n    npt.assert_array_equal(p.y, self.df.y_na[~self.df.y_na.isnull()])\n"], "sample_570": ["def test_kde_univariate(self, x):\n    kde = KDE()\n    density, support = kde(x)\n    assert np.all(density >= 0)\n    assert_array_almost_equal(self.integrate(density, support), 1)\n\n    # Check that we get the same result with weights of ones\n    weights = np.ones_like(x)\n    density_weighted, support_weighted = kde(x, weights=weights)\n    assert_array_almost_equal(density, density_weighted)\n    assert_array_almost_equal(support, support_weighted)\n\n    # Smoke test for cumulative\n    kde = KDE(cumulative=True)\n    density, support = kde(x)\n    assert np.all(density >= 0)\n    assert np.all(density <= 1)\n", "def test_kde_univariate(self, x):\n    kde = KDE()\n    density, support = kde(x)\n    assert np.all(density >= 0)\n    assert np.isclose(self.integrate(density, support), 1)\n", "def test_kde_univariate(self, x):\n    kde = KDE()\n    density, support = kde(x)\n    assert_array_almost_equal(self.integrate(density, support), 1, 2)\n", "def test_kde_univariate(self, x):\n    kde = KDE()\n    density, support = kde(x)\n    assert np.all(density >= 0)\n    assert np.isclose(self.integrate(density, support), 1)\n    assert support.shape == (kde.gridsize,)\n", "def test_kde_univariate(self, x):\n    kde = KDE()\n    density, support = kde(x)\n    assert np.all(density >= 0)\n    assert_array_almost_equal(self.integrate(density, support), 1)\n"], "sample_571": ["def test_regression_plotter_units(self):\n\n    p = lm._RegressionPlotter(\"x\", \"y\", data=self.df, units=\"s\")\n    npt.assert_array_equal(p.x, self.df[\"x\"])\n    npt.assert_array_equal(p.y, self.df[\"y\"])\n\n    p = lm._RegressionPlotter(\"x\", \"y\", data=self.df)\n    npt.assert_array_equal(p.x, self.df[\"x\"])\n    npt.assert_array_equal(p.y, self.df[\"y\"])\n", "def test_regression_plotterDropNA(self):\n    p = lm._RegressionPlotter(\"x\", \"y_na\", data=self.df, dropna=True)\n    assert not pd.isnull(p.y).any()\n\n    p = lm._RegressionPlotter(\"x\", \"y_na\", data=self.df, dropna=False)\n    npt.assert_array_equal(pd.isnull(p.y), pd.isnull(self.df[\"y_na\"]))\n", "def test_regress_out():\n    df = pd.DataFrame({\"x\": [1, 2, 3, 4, 5], \"y\": [2, 3, 5, 7, 11]})\n    plotter = lm._RegressionPlotter(\"x\", \"y\", data=df)\n    plotter.regress_out(plotter.x, plotter.y)\n    assert plotter.x.shape == (5,)\n", "def test_lmplot_color(self):\n    fig = lm.lmplot(x=\"x\", y=\"y\", data=self.df, hue=\"s\", palette=\"Set1\")\n    axes = fig.axes\n    colors = color_palette(\"Set1\", len(axes))\n    for ax, c in zip(axes, colors):\n        npt.assert_equal(ax.get_lines()[0].get_color(), mpl.colors.rgb2hex(c))\n", "def test_regression_plotterdropna(self):\n    p = lm._RegressionPlotter(\"x\", \"y_na\", data=self.df, dropna=False)\n    npt.assert_equal(p.x.shape[0], self.df.shape[0])\n    npt.assert_equal(p.y.shape[0], self.df.shape[0])\n\n    p = lm._RegressionPlotter(\"x\", \"y_na\", data=self.df, dropna=True)\n    npt.assert_equal(p.x.shape[0], self.df.dropna().shape[0])\n    npt.assert_equal(p.y.shape[0], self.df.dropna().shape[0])\n"], "sample_572": ["def test_KDE_cumulative(self, x):\n    kde = KDE(cumulative=True)\n    density, support = kde(x)\n    assert np.allclose(density[-1], 1)\n    assert np.allclose(density[0], 0)\n\n    # Ensure that the density sums to 1 when we integrate over support\n    integral, _ = self.integrate(density, support)\n    assert np.allclose(integral, 1, atol=1e-3)\n", "def test_KDE_default(self, x):\n    kde = KDE()\n    density, support = kde(x)\n    assert np.all(density >= 0)\n    assert np.isclose(self.integrate(density, support), 1)\n", "def test_KDE_defaults(self, x):\n    kde = KDE()\n    y, support = kde(x)\n    assert_array_almost_equal(self.integrate(y, support), 1)\n", "def test_kde_default(self, x):\n    kde = KDE()\n    density, support = kde(x)\n    assert_array_almost_equal(self.integrate(density, support), 1)\n", "def test_KDE_define_support_univariate(self, x, weights):\n    kde = KDE()\n    support = kde.define_support(x, weights=weights)\n    assert isinstance(support, np.ndarray)\n    assert support.ndim == 1\n    assert len(support) == kde.gridsize\n"], "sample_573": ["def test_polyfit_output(self, df):\n\n    groupby = GroupBy(\"color\")\n    stat = PolyFit()\n\n    result = stat(df, groupby, \"x\", None)\n\n    assert isinstance(result, pd.DataFrame)\n    assert result.shape == (stat.gridsize * df[\"color\"].nunique(), 2)\n    assert_array_equal(result.columns, [\"x\", \"y\"])\n", "def test_polyfit_output(self, df):\n\n    groupby = GroupBy([\"color\"])\n    stat = PolyFit()\n\n    result = stat(df, groupby, orient=\"x\", scales=None)\n\n    assert isinstance(result, pd.DataFrame)\n    assert result.columns.tolist() == [\"x\", \"y\"]\n    assert len(result) == stat.gridsize * df[\"color\"].nunique()\n", "def test_polyfit_simple(self, df):\n\n    poly_fit = PolyFit()\n    groupby = GroupBy([\"group\"])\n    result = poly_fit(df, groupby, orient=None, scales=None)\n\n    assert isinstance(result, pd.DataFrame)\n    assert \"x\" in result.columns\n    assert \"y\" in result.columns\n", "def test_polyfit_smoke(self, df):\n\n    gb = GroupBy(\"color\")\n    stat = PolyFit()\n    result = stat(df, gb, \"x\", None)\n\n    assert isinstance(result, pd.DataFrame)\n    assert \"x\" in result.columns\n    assert \"y\" in result.columns\n", "def test_polyfit_single_group(self, df):\n\n    df = df.drop(columns=[\"color\", \"group\"])\n    groupby = GroupBy([\"groupby\"])\n    stat = PolyFit()\n    result = stat(df, groupby, orient=\"x\", scales=None)\n\n    assert isinstance(result, pd.DataFrame)\n    assert result.shape == (stat.gridsize, 2)\n    assert_array_almost_equal(result[\"x\"].min(), df[\"x\"].min())\n    assert_array_almost_equal(result[\"x\"].max(), df[\"x\"].max())\n"], "sample_574": ["def test_setup_ticks_with_every_and_between(self, x):\n\n    a = self.setup_ticks(x, every=2, between=(1, 9))\n    ticks = a.major.locator()\n\n    assert_array_equal(ticks, [1, 3, 5, 7, 9])\n", "def test_tickMinorLocator(self, x):\n\n    a = self.setup_ticks(x, minor=2)\n    assert isinstance(a.minor.locator, mpl.ticker.AutoMinorLocator)\n    assert a.minor.locator.n == 3\n\n    a = self.setup_ticks(x, every=.5, minor=1)\n    assert isinstance(a.minor.locator, mpl.ticker.MultipleLocator)\n    assert a.minor.locator.base == .25\n", "def test_continuous_label_formatter(self, x):\n\n    a, locs = self.setup_labels(x, like=\".1f\")\n    labels = a.major.formatter.format_ticks(locs)\n    assert all(re.match(r\"^\\d\\.\\d$\", label) for label in labels)\n\n    a, locs = self.setup_labels(x, like=lambda x, pos: f\"${x:.2f}\")\n    labels = a.major.formatter.format_ticks(locs)\n    assert all(re.match(r\"^\\$\\d\\.\\d\\d$\", label) for label in labels)\n\n    a, locs = self.setup_labels(x, base=2)\n    labels = a.major.formatter.format_ticks(locs)\n    assert all(re.match(r\"^2\\^.*$\", label) for label in labels)\n\n    a, locs = self.setup_labels(x, unit=\"g\")\n    labels = a.major.formatter.format_ticks(locs)\n    assert all(\"g\" in label or \"kg\" in label for label in labels)\n", "def test_tick_at(self, x):\n    a = self.setup_ticks(x, at=[1, 5])\n    assert_array_equal(a.major.locator(), [1, 5])\n", "def test_continuous_label_formatter(self, x):\n\n    a, locs = self.setup_labels(x)\n    labels = a.major.formatter.format_ticks(locs)\n\n    assert len(labels) == len(locs)\n\n    a, locs = self.setup_labels(x, like=\".2f\")\n    labels = a.major.formatter.format_ticks(locs)\n\n    assert all(re.match(r\"^\\d+\\.\\d\\d$\", label) for label in labels)\n\n    a, locs = self.setup_labels(x, like=lambda x, pos: f\"${x:.2f}\")\n    labels = a.major.formatter.format_ticks(locs)\n\n    assert all(label.startswith(\"$\") for label in labels)\n"], "sample_575": ["def test_continuous_scale_label_formatter(self, x):\n\n    a, locs = self.setup_labels(x)\n    assert_array_equal(locs, [1, 5])\n\n    a, locs = self.setup_labels(x, like=\".0f\")\n    labels = a.major.formatter.format_ticks(locs)\n    assert labels == [\"1\", \"5\"]\n\n    a, locs = self.setup_labels(x, unit=\"g\")\n    labels = a.major.formatter.format_ticks(locs)\n    assert labels == [\"1 g\", \"5 g\"]\n\n    a, locs = self.setup_labels(x, base=2)\n    labels = a.major.formatter.format_ticks(locs)\n    expected = [f\"2^{i:.0f}\" for i in np.log2(locs)]\n    assert labels == expected\n\n        return f\"${x:.0f}\"\n\n    a, locs = self.setup_labels(x, like=fmt)\n    labels = a.major.formatter.format_ticks(locs)\n    assert labels == [\"$1\", \"$5\"]\n", "def test_continuous_label_formatter(self, x):\n\n    # Test that a provided formatter is used\n    fmt = mpl.ticker.FuncFormatter(lambda x, pos: f\"${x:.2f}\")\n    a, locs = self.setup_labels(x, formatter=fmt)\n    assert_array_equal([f\"${loc:.2f}\" for loc in locs], a.major.formatter.format_ticks(locs))\n\n    # Test that the \"like\" string is used\n    a, locs = self.setup_labels(x, like=\".2f\")\n    assert_array_equal([f\"{loc:.2f}\" for loc in locs], a.major.formatter.format_ticks(locs))\n\n    # Test that the \"like\" string with fields is used\n    a, locs = self.setup_labels(x, like=\"${x:.2f}\")\n    assert_array_equal([f\"${loc:.2f}\" for loc in locs], a.major.formatter.format_ticks(locs))\n\n    # Test that the \"like\" function is used\n        return f\"${x:.2f}\"\n    a, locs = self.setup_labels(x, like=fmt)\n    assert_array_equal([f\"${loc:.2f}\" for loc in locs], a.major.formatter.format_ticks(locs))\n", "def test_continuous_label(self, x):\n\n    # Test with a custom formatter\n    a, locs = self.setup_labels(x, like=\".1f\")\n    labels = a.major.formatter.format_ticks(locs)\n    assert all(re.match(r\"^\\d+\\.\\d$\", label) for label in labels)\n\n    # Test with a custom formatter function\n        return str(int(x))\n    a, locs = self.setup_labels(x, like=fmt)\n    labels = a.major.formatter.format_ticks(locs)\n    assert all(label.isdigit() for label in labels)\n\n    # Test with a log transform and base argument\n    a, locs = self.setup_labels(x, trans=\"log\", base=2)\n    labels = a.major.formatter.format_ticks(locs)\n    assert any(\"\u00d7\" in label for label in labels)\n\n    # Test with an SI prefix\n    a, locs = self.setup_labels(x, unit=\"g\")\n    labels = a.major.formatter.format_ticks(locs)\n    assert any(\"k\" in label for label in labels)\n", "def test_setup_continuous_scale_with_norm(self, x):\n\n    s = Continuous(norm=(1, 9))._setup(x, Coordinate())\n    a = PseudoAxis(s._matplotlib_scale)\n\n    assert_array_equal(a.get_view_interval(), (0, 1))\n    assert_array_equal(a.major.locator(), [0., 0.5, 1.])\n    assert_array_equal(a.major.formatter.format_ticks([0., 0.5, 1.]), ['1', '5', '9'])\n", "def test_continuous_label(self, x):\n\n    a, locs = self.setup_labels(x)\n    assert_array_equal(locs, [1, 3, 9])\n\n    a, locs = self.setup_labels(x, like=\".0f\")\n    labels = a.major.formatter.format_ticks(locs)\n    assert labels == [\"1\", \"3\", \"9\"]\n\n    a, locs = self.setup_labels(x, unit=\"x\")\n    labels = a.major.formatter.format_ticks(locs)\n    assert labels == [\"1 x\", \"3 x\", \"9 x\"]\n\n    a, locs = self.setup_labels(x, unit=(\"\", \"x\"))\n    labels = a.major.formatter.format_ticks(locs)\n    assert labels == [\"1x\", \"3x\", \"9x\"]\n"], "sample_576": ["def test_plot_init_positional_args():\n\n    # Test that passing data as first positional arg works\n    p = Plot(pd.DataFrame({\"x\": [1, 2, 3]}), x=\"x\")\n    assert_frame_equal(p._data.frame, pd.DataFrame({\"x\": [1, 2, 3]}))\n\n    # Test that passing vector(s) as positional args works\n    p = Plot([1, 2, 3], [4, 5, 6])\n    assert_frame_equal(p._data.frame, pd.DataFrame({\"x\": [1, 2, 3], \"y\": [4, 5, 6]}))\n\n    # Test that trying to pass too many positional args raises an error\n    with pytest.raises(TypeError):\n        Plot([1, 2, 3], [4, 5, 6], [7, 8, 9])\n\n    # Test that passing both data and x / y as positional args raises an error\n    with pytest.raises(TypeError):\n        Plot(pd.DataFrame({\"x\": [1, 2, 3]}), [4, 5, 6])\n", "def test_plot_init_empty_data():\n    p = Plot()\n    assert isinstance(p._data, PlotData)\n    assert p._data.frame.empty\n    assert not p._layers\n    assert not p._scales\n    assert not p._shares\n    assert not p._limits\n    assert not p._labels\n    assert not p._theme\n    assert not p._facet_spec\n    assert not p._pair_spec\n    assert not p._figure_spec\n    assert not p._subplot_spec\n    assert not p._layout_spec\n", "    def test_plotter_methods(self):\n\n        p = Plot(x=[1, 2, 3], y=[1, 2, 3]).add(MockMark())\n        plotter = p.plot()\n\n        assert hasattr(plotter.save, \"__call__\")\n        assert hasattr(plotter.show, \"__call__\")\n        with pytest.raises(AttributeError):\n            plotter.not_a_method()\n", "    def test_plot_target(self):\n\n        fig, ax = plt.subplots()\n        p = Plot().on(ax)\n        assert p._target is ax\n\n        with pytest.raises(TypeError):\n            Plot().on(\"not an axes\")\n\n        if hasattr(mpl.figure, \"SubFigure\"):\n            fig = mpl.figure.Figure()\n            sf = fig.subfigures(1)[0]\n            p = Plot().on(sf)\n            assert p._target is sf\n        else:\n            with pytest.raises(TypeError):\n                Plot().on(\"not a figure either\")\n", "def test_plot_init_positional_args():\n\n    # Positional data arg with keyword x, y\n    p = Plot(pd.DataFrame(), x=\"x\", y=\"y\")\n    assert_frame_equal(p._data.frame, pd.DataFrame())\n\n    # Positional x arg with keyword y\n    p = Plot(\"x\", y=\"y\")\n    assert p._data.frame.empty\n\n    # Positional x, y args\n    p = Plot(\"x\", \"y\")\n    assert p._data.frame.empty\n\n    # Positional data, x, y args\n    p = Plot(pd.DataFrame(), \"x\", \"y\")\n    assert_frame_equal(p._data.frame, pd.DataFrame())\n\n    # Too many positional args\n    with pytest.raises(TypeError):\n        Plot(1, 2, 3, 4)\n\n    # Non-data, non-string as positional arg\n    with pytest.raises(TypeError):\n        Plot(123)\n"], "sample_577": ["    def test_plot_init_smoke(self):\n\n        Plot()\n", "    def test_plot_calls_setup_methods(self, long_df):\n\n        class MockPlotter(Plot._Plotter):\n                self.called = []\n                super().__init__(*args, **kwargs)\n\n                self.called.append(\"_setup_figure\")\n                return super()._setup_figure(*args, **kwargs)\n\n                self.called.append(\"_setup_scales\")\n                return super()._setup_scales(*args, **kwargs)\n\n                self.called.append(\"_plot_layers\")\n                return super()._plot_layers(*args, **kwargs)\n\n                self.called.append(\"_setup_legend\")\n                return super()._setup_legend(*args, **kwargs)\n\n        plot = Plot(long_df, x=\"x\", y=\"y\").add(MockMark())\n        plot._Plotter = MockPlotter\n        plot.compile()\n        assert plot._Plotter().called == [\n            \"_setup_figure\",\n            \"_setup_scales\",\n            \"_plot_layers\",\n            \"_setup_legend\",\n        ]\n", "    def test_pair_without_cross(self):\n\n        p = Plot().pair(x=[\"a\", \"b\"], y=[\"1\", \"2\"], cross=False)\n        assert p._pair_spec[\"structure\"] == {\"x\": [\"a\", \"b\"], \"y\": [\"1\", \"2\"]}\n        assert p._pair_spec[\"cross\"] is False\n", "    def test_init_smoke(self):\n\n        Plot()\n", "def test_plot_init_positional_args():\n    # Test that passing data and/or variables positionally works as expected\n\n    data = pd.DataFrame({\"x\": [1, 2, 3], \"y\": [4, 5, 6]})\n    x, y = [1, 2, 3], [4, 5, 6]\n\n    p1 = Plot(data)\n    assert_frame_equal(p1._data.frame, data)\n\n    p2 = Plot(x, y)\n    assert_vector_equal(p2._data.frame[\"x\"], x)\n    assert_vector_equal(p2._data.frame[\"y\"], y)\n\n    p3 = Plot(data, x, y)\n    assert_frame_equal(p3._data.frame, data)\n    assert_vector_equal(p3._data.frame[\"x\"], x)\n    assert_vector_equal(p3._data.frame[\"y\"], y)\n\n    with pytest.raises(TypeError):\n        Plot(1, 2, 3, 4)\n\n    with pytest.raises(TypeError):\n        Plot(data, data)\n"], "sample_578": ["def test_bar_width(self):\n\n    x = np.arange(5)\n    y = np.random.rand(5)\n\n    bars = self.plot_bars(\n        {\"x\": x, \"y\": y}, {}, {}\n    )\n\n    for bar, xi in zip(bars, x):\n        self.check_bar(bar, xi - .4, 0, .8, y[xi])\n", "def test_bar_color_resolution(self):\n\n    x = [1, 2, 3]\n    y = [1, 2, 3]\n    color = [\"red\", \"green\", \"blue\"]\n\n    p = Plot(x=x, y=y, color=color).add(Bar()).plot()\n    ax = p._figure.axes[0]\n    bars = [bar for barlist in ax.containers for bar in barlist]\n\n    expected_colors = to_rgba_array(color)\n    for i, bar in enumerate(bars):\n        assert_array_equal(to_rgba(bar.get_facecolor()), expected_colors[i])\n", "def test_bar_colors(self):\n\n    x = [1, 2, 3]\n    y = [2, 4, 6]\n    df = pd.DataFrame({\"x\": x, \"y\": y})\n\n    bar_kws = {\"color\": to_rgba(\"C0\"), \"alpha\": .8}\n    bars = self.plot_bars({\"x\": \"x\", \"y\": \"y\"}, bar_kws, {\"data\": df})\n\n    expected_color = np.array([to_rgba_array(bar_kws[\"color\"])])\n    assert_array_equal([bar.get_facecolor() for bar in bars], expected_color)\n\n    bar_kws = {\"edgecolor\": to_rgba(\"C1\"), \"edgealpha\": .8}\n    bars = self.plot_bars({\"x\": \"x\", \"y\": \"y\"}, bar_kws, {\"data\": df})\n\n    expected_edgecolor = np.array([to_rgba_array(bar_kws[\"edgecolor\"])])\n    assert_array_equal([bar.get_edgecolor() for bar in bars], expected_edgecolor)\n", "def test_bars_baseline(self):\n\n    x = [1, 2, 3]\n    y = [10, 20, 30]\n    baseline = 5\n\n    mark_kws = {\"baseline\": baseline}\n    bars = self.plot_bars({\"x\": x, \"y\": y}, mark_kws, {})\n\n    for bar, (xi, yi) in zip(bars, zip(x, y)):\n        self.check_bar(bar, xi - 0.4, baseline, 0.8, yi - baseline)\n", "def test_bars_baseline(self):\n\n    x = np.arange(3)\n    y1 = np.array([1, 2, 3])\n    y2 = np.array([4, 5, 6])\n\n    p = Plot(x=x, y=y1).add(Bars(baseline=2), x=x, y=y2).plot()\n    ax = p._figure.axes[0]\n    bars = [bar for barlist in ax.containers for bar in barlist]\n\n    assert len(bars) == 6\n    for i, bar in enumerate(bars[:3]):\n        self.check_bar(bar, i - .4, 0, .8, y1[i])\n    for i, bar in enumerate(bars[3:]):\n        self.check_bar(bar, i - .4, 2, .8, y2[i])\n"], "sample_579": ["def test_heatmap_formatting(self):\n\n    ax = mat.heatmap(self.df_norm, annot=True, fmt=\".0f\")\n    for text in ax.texts:\n        assert text.get_text().count(\".\") == 0\n\n    ax = mat.heatmap(self.df_norm, annot=True, fmt=\".2f\")\n    for text in ax.texts:\n        assert text.get_text().count(\".\") == 1\n\n    ax = mat.heatmap(self.df_norm, annot=True, fmt=\"s\")\n    for text in ax.texts:\n        assert isinstance(text.get_text(), str)\n\n    ax = mat.heatmap(self.df_norm, annot=True, fmt=lambda x: \"hi\")\n    for text in ax.texts:\n        assert text.get_text() == \"hi\"\n", "def test_heatmap_dataframe_index_names(self):\n    kws = self.default_kws.copy()\n    kws[\"xticklabels\"] = True\n    kws[\"yticklabels\"] = True\n    ax = mat.heatmap(self.df_norm, **kws)\n    assert ax.get_xlabel() == self.df_norm.columns.name\n    assert ax.get_ylabel() == self.df_norm.index.name\n", "def test_heatmap_dataframe_index_name(self):\n\n    ax = mat.heatmap(self.df_norm, xticklabels=True, yticklabels=True)\n    assert ax.get_ylabel() == self.df_norm.index.name\n\n    ax = mat.heatmap(self.df_unif, xticklabels=True, yticklabels=True)\n    assert ax.get_ylabel() is not None\n", "def test_heatmap_mask(self):\n    data = self.df_norm.copy()\n    mask = np.random.choice([True, False], data.shape)\n    mask[0, 0] = True\n    kws = copy.deepcopy(self.default_kws)\n    kws['mask'] = mask\n\n    p = mat._HeatMapper(data, **kws)\n    npt.assert_array_equal(p.plot_data.mask, mask)\n\n    fig = plt.figure(figsize=(2, 2))\n    ax = fig.add_subplot(111)\n    p.plot(ax, ax, {})\n    for i in range(mask.shape[0]):\n        for j in range(mask.shape[1]):\n            if mask[i, j]:\n                assert not p.plot_data[i, j]\n", "def test_heatmap_dataframe_with_indexname(self):\n    p = mat._HeatMapper(self.df_norm, **self.default_kws)\n    npt.assert_equal(p.xlabel, self.df_norm.columns.name)\n    npt.assert_equal(p.ylabel, self.df_norm.index.name)\n"], "sample_580": ["def test_variable_type_categorical_dtype():\n    s = pd.Series([\"a\", \"b\", \"c\"], dtype=\"category\")\n    assert variable_type(s) == VarType(\"categorical\")\n", "def test_variable_type_categorical_dtype():\n    vector = pd.Series([1, 2, 3], dtype=\"category\")\n    assert variable_type(vector) == VarType(\"categorical\")\n", "def test_variable_type_numeric():\n    a = pd.Series([1, 2, 3])\n    assert variable_type(a) == VarType(\"numeric\")\n    b = pd.Series([1.0, 2.0, 3.0])\n    assert variable_type(b) == VarType(\"numeric\")\n    c = pd.Series([1, np.nan, 3])\n    assert variable_type(c) == VarType(\"numeric\")\n", "def test_variable_type_boolean_strict():\n    s = pd.Series([True, False])\n    assert variable_type(s, strict_boolean=True) == VarType(\"boolean\")\n    s = pd.Series([0, 1], dtype=\"Int64\")\n    assert variable_type(s, strict_boolean=True) != VarType(\"boolean\")\n    s = pd.Series([0, 1], dtype=\"bool\")\n    assert variable_type(s, strict_boolean=True) == VarType(\"boolean\")\n", "def test_variable_type_numeric():\n    a = pd.Series([1, 2, 3, np.nan])\n    assert variable_type(a) == VarType(\"numeric\")\n\n    b = pd.Series([1.0, 2.0, 3.0, np.nan])\n    assert variable_type(b) == VarType(\"numeric\")\n\n    c = pd.Series([\"1\", \"2\", \"3\"], dtype=\"category\")\n    assert variable_type(c) != VarType(\"numeric\")\n\n    d = pd.Series([True, False, np.nan])\n    assert variable_type(d, boolean_type=\"numeric\") == VarType(\"numeric\")\n"], "sample_581": ["def test_blueprint_setupstate():\n    app = flask.Flask(__name__)\n    blueprint = flask.Blueprint(\"test\", __name__)\n\n        state.app.config[\"SETUP_STATE\"] = True\n\n    blueprint.record(setup)\n    assert \"SETUP_STATE\" not in app.config\n    app.register_blueprint(blueprint)\n    assert \"SETUP_STATE\" in app.config\n", "def test_blueprint_before_app_request():\n    app = flask.Flask(__name__)\n    bp = flask.Blueprint(\"bp\", __name__)\n\n    called = []\n\n    @bp.before_app_request\n        called.append(True)\n\n    @app.route(\"/\")\n        return \"index\"\n\n    bp.register(app)\n\n    c = app.test_client()\n    c.get(\"/\")\n    assert len(called) == 1\n", "def test_blueprint_template_filter():\n    app = flask.Flask(__name__)\n    bp = flask.Blueprint(\"test\", __name__)\n\n    @bp.app_template_filter()\n        return s.upper()\n\n    @app.route(\"/\")\n        return flask.render_template_string(\"{{ 'hello' | uppercase }}\")\n\n    bp.register(app)\n    client = app.test_client()\n    rv = client.get(\"/\")\n    assert rv.data == b\"HELLO\"\n", "def test_blueprint_setupstate():\n    app = flask.Flask(__name__)\n    blueprint = flask.Blueprint(\"test\", __name__)\n    state = blueprint.make_setup_state(app, {\"test\": \"test\"}, first_registration=True)\n    assert state.app == app\n    assert state.blueprint == blueprint\n    assert state.options == {\"test\": \"test\"}\n    assert state.first_registration is True\n", "def test_blueprint_setupstate_url_prefix():\n    app = flask.Flask(__name__)\n    bp = flask.Blueprint(\"test\", __name__, url_prefix=\"/foo\")\n    state = bp.make_setup_state(app, {})\n    assert state.url_prefix == \"/foo\"\n    state = bp.make_setup_state(app, {\"url_prefix\": \"/bar\"})\n    assert state.url_prefix == \"/bar\"\n"], "sample_582": ["def test_find_best_app(monkeypatch):\n        if name == \"test_app\":\n            return types.ModuleType(\"test_app\")\n        elif name == \"test_app.app\":\n            app = Flask(\"test_app\")\n            return types.ModuleType(\"test_app.app\", \"test_app.app\", {\"app\": app})\n        else:\n            raise ImportError(name)\n\n    monkeypatch.setattr(\"importlib.import_module\", mock_import)\n\n    # Test finding an app in the root module\n    module = importlib.import_module(\"test_app\")\n    app = find_best_app(module)\n    assert app is not None\n\n    # Test finding an app in a submodule\n    module = importlib.import_module(\"test_app.app\")\n    app = find_best_app(module)\n    assert app is not None\n\n    # Test not finding an app\n    with pytest.raises(NoAppException):\n        find_best_app(types.ModuleType(\"empty_module\"))\n", "def test_cli_with_app_ctx():\n    app = Flask(__name__)\n\n    @app.cli.command()\n    @with_appcontext\n        click.echo(current_app.name)\n\n    result = CliRunner().invoke(app.cli, [\"hello\"])\n    assert result.exit_code == 0\n    assert result.output.strip() == app.name\n", "def test_find_best_app():\n    with pytest.raises(NoAppException):\n        find_best_app(types.ModuleType(\"fake_module\"))\n\n    app = Flask(__name__)\n    module = types.ModuleType(\"test\")\n    module.app = app\n    assert find_best_app(module) == app\n\n    del module.app\n    module.application = app\n    assert find_best_app(module) == app\n\n    del module.application\n    module.create_app = lambda: app\n    assert find_best_app(module) == app\n\n    del module.create_app\n    module.make_app = lambda: app\n    assert find_best_app(module) == app\n\n    del module.make_app\n    module.app_factory = lambda: app\n    with pytest.raises(NoAppException):\n        find_best_app(module)\n\n    module.app2 = app\n    module.app3 = Flask(__name__)\n    with pytest.raises(NoAppException):\n        find_best_app(module)\n", "def test_cli_with_appctx(runner):\n    app = Flask(__name__)\n\n    @app.cli.command()\n    @with_appcontext\n        click.echo(current_app.name)\n\n    result = runner.invoke(app.cli, [\"hello\"])\n    assert result.exit_code == 0\n    assert result.output.strip() == app.name\n", "def test_find_best_app():\n    with pytest.raises(NoAppException):\n        find_best_app(types.ModuleType(\"dummy\"))\n\n    app = Flask(__name__)\n    module = types.ModuleType(\"dummy\")\n    module.app = app\n    assert find_best_app(module) == app\n\n    module.application = app\n    assert find_best_app(module) == app\n\n    del module.application\n    module.app = None\n    module.create_app = lambda: app\n    assert find_best_app(module) == app\n\n    del module.create_app\n    module.make_app = lambda: app\n    assert find_best_app(module) == app\n\n    del module.make_app\n    module.app = app\n    module.application = \"not an app\"\n    assert find_best_app(module) == app\n"], "sample_583": ["def test_implicit_to_explicit(self):\n    x = np.random.rand(10, 20, 30)\n    i = indexing.ImplicitToExplicitIndexingAdapter(x)\n\n    # test slicing\n    assert_array_equal(x[:5], i[:5])\n    assert_array_equal(x[:, :5], i[:, :5])\n    assert_array_equal(x[..., :5], i[..., :5])\n\n    # test array indexing\n    indices = np.array([1, 3, 4])\n    assert_array_equal(x[indices], i[indices])\n    assert_array_equal(x[:, indices], i[:, indices])\n    assert_array_equal(x[..., indices], i[..., indices])\n\n    # test mixed indexing\n    assert_array_equal(x[:5, indices], i[:5, indices])\n    assert_array_equal(x[..., :5, indices], i[..., :5, indices])\n", "def test_orthogonal_indexer(self):\n    x = np.arange(24).reshape((3, 2, 4))\n    i = indexing.OuterIndexer((0, slice(None), slice(None)))\n    assert_array_equal(x[i], x[0])\n    i = indexing.OuterIndexer((slice(None), 0, slice(None)))\n    assert_array_equal(x[i], x[:, 0])\n    i = indexing.OuterIndexer((slice(None), slice(None), 0))\n    assert_array_equal(x[i], x[..., 0])\n\n    with raises_regex(TypeError, 'unexpected key type'):\n        indexing.OuterIndexer((0, [0, 1], slice(None)))\n\n    i = indexing.OuterIndexer(([0, 1], slice(None), slice(None)))\n    assert_array_equal(x[i], x[[0, 1]])\n    i = indexing.OuterIndexer((slice(None), [0, 1], slice(None)))\n    assert_array_equal(x[i], x[:, [0, 1]])\n    i = indexing.OuterIndexer((slice(None), slice(None), [0, 1]))\n    assert_array_equal(x[i], x[..., [0, 1]])\n\n    i = indexing.OuterIndexer(([0, 1], [0, 1], slice(None)))\n    assert_array_equal(x[i], x[[0, 1]][:, [0, 1]])\n    i = indexing.OuterIndexer(([0, 1], slice(None), [0, 1]))\n    assert_array_equal(x[i], x[[0, 1]][..., [0, 1]])\n    i = indexing.OuterIndexer((slice(None), [0, 1], [0, 1]))\n    assert_array_equal(x[i], x[:, [0, 1]][..., [0, 1]])\n\n    i = indexing.OuterIndexer(([0, 1], [0, 1], [0, 1]))\n    assert_array_equal(x[i], x[[0, 1]][:, [0, 1]][..., [0, 1]])\n", "def test_posify_mask_indexer(self):\n    # Test that posify_mask_indexer correctly converts masked values (-1) in an\n    # indexer to nearest unmasked values.\n    key = indexing.VectorizedIndexer((np.array([-1, 2, 3]),))\n    result = indexing.posify_mask_indexer(key)\n    expected = indexing.VectorizedIndexer((np.array([0, 2, 3]),))\n    assert_array_equal(result.tuple, expected.tuple)\n\n    key = indexing.OuterIndexer((np.array([-1, 2, 3]),))\n    result = indexing.posify_mask_indexer(key)\n    expected = indexing.OuterIndexer((np.array([0, 2, 3]),))\n    assert_array_equal(result.tuple, expected.tuple)\n", "def test_implicit_to_explicit(self):\n    x = np.arange(10)\n    i = indexing.ImplicitToExplicitIndexingAdapter(x, indexing.BasicIndexer)\n    assert_array_equal(i[:5], x[:5])\n    i = indexing.ImplicitToExplicitIndexingAdapter(x, indexing.OuterIndexer)\n    assert_array_equal(i[:5], x[:5])\n    i = indexing.ImplicitToExplicitIndexingAdapter(x, indexing.VectorizedIndexer)\n    assert_array_equal(i[:5], x[:5])\n", "def test_vectorized_indexer(self):\n    index = pd.Index([1, 2, 3])\n    label = np.array([1, 3])\n    indexer, new_index = indexing.convert_label_indexer(index, label, 'foo')\n    assert_array_equal(indexer, np.array([0, 2]))\n    assert new_index is None\n\n    # vectorized indexing with a list\n    label = [1, 3]\n    indexer, new_index = indexing.convert_label_indexer(index, label, 'foo')\n    assert_array_equal(indexer, np.array([0, 2]))\n    assert new_index is None\n"], "sample_584": ["def test_infer_concat_order_from_positions():\n    # Create a nested list of datasets with varying dimensions\n    datasets = [\n        [Dataset(), Dataset()],\n        [Dataset(), Dataset()],\n        [Dataset()]\n    ]\n\n    # Manually define the expected tile IDs\n    expected_tile_ids = OrderedDict([\n        ((0, 0), datasets[0][0]),\n        ((0, 1), datasets[0][1]),\n        ((1, 0), datasets[1][0]),\n        ((1, 1), datasets[1][1]),\n        ((2, 0), datasets[2][0])\n    ])\n\n    # Infer the concat order from positions\n    inferred_tile_ids = _infer_concat_order_from_positions(datasets)\n\n    # Assert that the inferred tile IDs match the expected tile IDs\n    assert_combined_tile_ids_equal(inferred_tile_ids, expected_tile_ids)\n", "def test_infer_concat_order_from_coords_edge_cases():\n    # Test that _infer_concat_order_from_coords raises a ValueError when it\n    # encounters inconsistent coordinate values.\n    ds1 = Dataset()\n    ds1['x'] = DataArray([1, 2, 3], dims='x')\n    ds2 = Dataset()\n    ds2['x'] = DataArray([4, 5, 6], dims='x')\n    ds3 = Dataset()\n    ds3['x'] = DataArray([1, 2, 4], dims='x')\n\n    with pytest.raises(ValueError):\n        _infer_concat_order_from_coords([ds1, ds2, ds3])\n\n    # Test that _infer_concat_order_from_coords can handle empty datasets.\n    ds1 = Dataset()\n    ds1['x'] = DataArray([], dims='x')\n    ds2 = Dataset()\n    ds2['x'] = DataArray([], dims='x')\n\n    with pytest.raises(ValueError):\n        _infer_concat_order_from_coords([ds1, ds2])\n", "def test_infer_concat_order_from_positions():\n    # Create a nested list of datasets with different dimensions\n    datasets = [\n        Dataset(),\n        [Dataset(), Dataset()],\n        [[Dataset(), Dataset()], [Dataset(), Dataset()]]\n    ]\n\n    # Manually specify the expected tile IDs\n    expected_tile_ids = OrderedDict({\n        (): datasets[0],\n        (0,): datasets[1][0],\n        (1,): datasets[1][1],\n        (0, 0): datasets[2][0][0],\n        (0, 1): datasets[2][0][1],\n        (1, 0): datasets[2][1][0],\n        (1, 1): datasets[2][1][1]\n    })\n\n    # Use _infer_concat_order_from_positions to get the tile IDs\n    combined_ids = _infer_concat_order_from_positions(datasets)\n\n    # Check if the expected and actual tile IDs are equal\n    assert_combined_tile_ids_equal(expected_tile_ids, combined_ids)\n", "def test_infer_concat_order_from_coords_edge_cases():\n    # no dimension coordinates\n    ds1 = Dataset()\n    ds2 = Dataset()\n    with pytest.raises(ValueError):\n        _infer_concat_order_from_coords([ds1, ds2])\n\n    # inconsistent dimension coordinates\n    ds1 = Dataset(coords={'x': [1, 2]})\n    ds2 = Dataset(coords={'x': [3, 4, 5]})\n    with pytest.raises(ValueError):\n        _infer_concat_order_from_coords([ds1, ds2])\n\n    # duplicate dimension coordinates\n    ds1 = Dataset(coords={'x': [1, 2]})\n    ds2 = Dataset(coords={'x': [1, 2]})\n    with pytest.raises(ValueError):\n        _infer_concat_order_from_coords([ds1, ds2])\n\n    # dimension coordinates are not monotonic\n    ds1 = Dataset(coords={'x': [1, 3]})\n    ds2 = Dataset(coords={'x': [2, 4]})\n    with pytest.raises(ValueError):\n        _infer_concat_order_from_coords([ds1, ds2])\n", "def test_infer_concat_order_from_coords():\n    # Create sample datasets with dimension coordinates\n    ds1 = Dataset()\n    ds1['x'] = ('x', [1, 2, 3])\n    ds1['y'] = ('y', [4, 5, 6])\n\n    ds2 = Dataset()\n    ds2['x'] = ('x', [7, 8, 9])\n    ds2['y'] = ('y', [10, 11, 12])\n\n    ds3 = Dataset()\n    ds3['x'] = ('x', [13, 14, 15])\n    ds3['y'] = ('y', [16, 17, 18])\n\n    # Infer concat order from coords\n    combined_ids, concat_dims = _infer_concat_order_from_coords([ds1, ds2, ds3])\n\n    # Expected result\n    expected_combined_ids = OrderedDict([\n        ((0,), ds1),\n        ((1,), ds2),\n        ((2,), ds3)\n    ])\n    expected_concat_dims = ['x']\n\n    # Assert results\n    assert_combined_tile_ids_equal(combined_ids, expected_combined_ids)\n    assert expected_concat_dims == concat_dims\n"], "sample_585": ["def test_consolidate_slices():\n    slices = [slice(0, 2), slice(2, 4), slice(4, 6)]\n    expected = slices\n    assert _consolidate_slices(slices) == expected\n\n    slices = [slice(0, 2), slice(2, 3), slice(3, 4)]\n    expected = [slice(0, 4)]\n    assert _consolidate_slices(slices) == expected\n\n    slices = [slice(0, 1), slice(2, 3), slice(4, 5)]\n    expected = slices\n    assert _consolidate_slices(slices) == expected\n\n    with pytest.raises(ValueError):\n        _consolidate_slices([slice(0, 2), 3])\n\n    with pytest.raises(ValueError):\n        _consolidate_slices([slice(0, 2), 'foo'])\n", "def test_consolidate_slices():\n    slices = [slice(0, 2), slice(2, 4)]\n    expected = [slice(0, 4)]\n    assert _consolidate_slices(slices) == expected\n\n    slices = [slice(0, 2), slice(3, 4)]\n    expected = [slice(0, 2), slice(3, 4)]\n    assert _consolidate_slices(slices) == expected\n\n    slices = [slice(0, 2), slice(2, 3), slice(3, 4)]\n    expected = [slice(0, 4)]\n    assert _consolidate_slices(slices) == expected\n", "def test_consolidate_slices():\n    slices = [slice(0, 2), slice(2, 3), slice(3, 5)]\n    expected = [slice(0, 5)]\n    assert _consolidate_slices(slices) == expected\n\n    slices = [slice(0, 2), slice(3, 4), slice(5, 6)]\n    expected = [slice(0, 2), slice(3, 4), slice(5, 6)]\n    assert _consolidate_slices(slices) == expected\n\n    slices = [slice(0, 1), slice(1, 2), slice(2, 3)]\n    expected = [slice(0, 3)]\n    assert _consolidate_slices(slices) == expected\n", "def test_consolidate_slices():\n    slices = [slice(0, 1), slice(1, 2), slice(2, 3)]\n    expected = slices\n    actual = _consolidate_slices(slices)\n    assert actual == expected\n\n    slices = [slice(0, 1), slice(1, 2), slice(2, 3), slice(3, 4)]\n    expected = slices\n    actual = _consolidate_slices(slices)\n    assert actual == expected\n\n    slices = [slice(0, 1), slice(2, 3), slice(3, 4)]\n    expected = [slice(0, 1), slice(2, 4)]\n    actual = _consolidate_slices(slices)\n    assert actual == expected\n\n    slices = [slice(0, 1), slice(1, 3), slice(3, 4)]\n    expected = [slice(0, 4)]\n    actual = _consolidate_slices(slices)\n    assert actual == expected\n", "def test_consolidate_slices():\n    slices = [slice(1, 3), slice(4, 6), slice(7, 8)]\n    expected = slices\n    assert _consolidate_slices(slices) == expected\n\n    slices = [slice(1, 3), slice(3, 5), slice(6, 8)]\n    expected = [slice(1, 5), slice(6, 8)]\n    assert _consolidate_slices(slices) == expected\n\n    slices = [slice(1, 3, 2), slice(3, 5), slice(6, 8)]\n    with pytest.raises(ValueError):\n        _consolidate_slices(slices)\n"], "sample_586": ["def test_concat_dataset_coords_data_vars_overlap():\n    # Regression test for GH2613\n    ds1 = Dataset()\n    ds1[\"x\"] = (\"x\", [1, 2])\n    ds1[\"y\"] = (\"y\", [1, 2])\n    ds1[\"data\"] = ((\"x\", \"y\"), [[1, 2], [3, 4]])\n\n    ds2 = Dataset()\n    ds2[\"x\"] = (\"x\", [3, 4])\n    ds2[\"y\"] = (\"y\", [1, 2])\n    ds2[\"data\"] = ((\"x\", \"y\"), [[5, 6], [7, 8]])\n\n    with pytest.raises(ValueError):\n        concat([ds1, ds2], dim=\"x\", coords=\"minimal\", data_vars=[\"x\"])\n", "def test_concat_coords_data_vars_mismatch():\n    # Check that an error is raised when concatenating along a dimension that\n    # is present in both coordinates and data variables, but not all datasets\n    # have it as a coordinate or data variable.\n\n    ds1 = Dataset()\n    ds1['x'] = Variable('x', [1, 2])\n    ds1['y'] = Variable('y', [3, 4])\n\n    ds2 = Dataset()\n    ds2['x'] = Variable('x', [5, 6])\n    ds2['y'] = Variable(('x',), [7, 8])\n\n    with raises_regex(ValueError, \"is a coordinate in some datasets but not others\"):\n        concat([ds1, ds2], dim='new_dim')\n", "def test_concat_dataarray_compat():\n    # Test that concat fails when combining a DataArray with\n    # a different name if compat='identical'\n    arr1 = DataArray([1, 2], dims=\"x\", name=\"a\")\n    arr2 = DataArray([3, 4], dims=\"x\", name=\"b\")\n\n    with raises_regex(ValueError, \"array names not identical\"):\n        concat([arr1, arr2], dim=\"y\", compat=\"identical\")\n\n    # Test that concat succeeds when combining a DataArray with\n    # a different name if compat is not 'identical'\n    result = concat([arr1, arr2], dim=\"y\", compat=\"equals\")\n    expected = DataArray(np.array([[1, 2], [3, 4]]), dims=[\"y\", \"x\"], name=\"a\")\n    assert_identical(result, expected)\n", "def test_concat_dataset_variables_align():\n    # Test that variables are aligned during concat\n    data1 = create_test_data()\n    data2 = create_test_data()\n    data2[\"var1\"] += 1\n    data2[\"var3\"] += 1\n\n    # remove some coords to make sure alignment works\n    del data2.coords[\"time\"]\n    del data2.coords[\"lat\"]\n\n    # check that non-aligned variables raise an error\n    with pytest.raises(ValueError):\n        concat([data1, data2], dim=\"x\")\n\n    # check that aligned variables work correctly\n    data2_aligned = data2.assign_coords(time=data1.time, lat=data1.lat)\n    concatenated = concat([data1, data2_aligned], dim=\"x\")\n    assert_equal(concatenated[\"var1\"].values, np.concatenate([data1[\"var1\"].values, data2[\"var1\"].values]))\n", "def test_concat_dim_coord():\n    # Test that the coordinate of the concatenated dimension is created\n    # correctly\n    dim = \"new_dim\"\n    coord = pd.Index([1, 2, 3], name=dim)\n    datasets = [Dataset() for _ in range(3)]\n\n    result = concat(datasets, dim=coord, data_vars=\"all\", coords=\"all\")\n    expected = DataArray(coord, dims=[dim], name=dim).variable\n    assert_identical(result[dim].variable, expected)\n"], "sample_587": ["def test_merge_variables_priority():\n    # Test that priority argument works as expected with merge_variables\n    variables1 = {\"a\": xr.Variable((\"x\",), [1, 2, 3])}\n    variables2 = {\"a\": xr.Variable((\"x\",), [4, 5, 6])}\n    priority_vars = {\"a\": xr.Variable((\"x\",), [7, 8, 9])}\n\n    merged = merge.merge_variables([variables1, variables2], priority_vars)\n\n    assert np.all(merged[\"a\"].values == priority_vars[\"a\"].values)\n", "def test_merge_data_and_coords():\n    data = create_test_data()\n    coords = data.coords\n    expected = data\n\n    actual = merge.merge_data_and_coords(data, coords)\n    assert actual.equals(expected)\n\n    # Test that we can safely modify the resulting Dataset's variables\n    # without modifying the original inputs.\n    actual[\"foo\"] += 1\n\n    # Make sure the new variable was created, but the original wasn't\n    # modified.\n    assert \"foo\" in actual.data_vars\n    assert \"foo\" not in data.data_vars\n", "def test_merge_variables():\n    # Create variables with different names and dimensions\n    var1 = xr.Variable(('x',), np.array([1, 2, 3]))\n    var2 = xr.Variable(('y',), np.array([4, 5, 6]))\n    var3 = xr.Variable(('x', 'y'), np.array([[7, 8], [9, 10]]))\n\n    # Merge variables with different names and dimensions\n    merged_vars = merge.merge_variables(\n        [{'var1': var1}, {'var2': var2}, {'var3': var3}],\n        priority_vars={'var1': var1}\n    )\n\n    # Check that the merged variables have the correct dimensions\n    assert merged_vars['var1'].dims == ('x',)\n    assert merged_vars['var2'].dims == ('y',)\n    assert merged_vars['var3'].dims == ('x', 'y')\n", "def test_merge_variables():\n    # Create variables\n    v1 = xr.Variable((\"x\",), [1, 2, 3], {\"units\": \"m\"})\n    v2 = xr.Variable((\"y\",), [4, 5, 6], {\"units\": \"m\"})\n\n    # Merge with identical variables\n    merged = merge.merge_variables([{\"v1\": v1}, {\"v1\": v1}], compat=\"identical\")\n    assert len(merged) == 1\n    assert \"v1\" in merged\n\n    # Merge with equal variables\n    merged = merge.merge_variables([{\"v1\": v1}, {\"v1\": v1}], compat=\"equals\")\n    assert len(merged) == 1\n    assert \"v1\" in merged\n\n    # Merge with broadcast equals variables\n    merged = merge.merge_variables([{\"v1\": v1}, {\"v1\": v1}], compat=\"broadcast_equals\")\n    assert len(merged) == 1\n    assert \"v1\" in merged\n\n    # Merge with no conflicts\n    merged = merge.merge_variables([{\"v1\": v1}, {\"v2\": v2}], compat=\"no_conflicts\")\n    assert len(merged) == 2\n    assert \"v1\" in merged and \"v2\" in merged\n\n    # Merge with override\n    merged = merge.merge_variables([{\"v1\": v1}, {\"v1\": v2}], compat=\"override\")\n    assert len(merged) == 1\n    assert \"v1\" in merged\n", "def test_merge_variables_broadcast_equals():\n    # Test merging variables with different dimensions, but broadcastable\n    var1 = xr.Variable((\"x\",), np.array([1, 2]))\n    var2 = xr.Variable((\"y\",), np.array([3, 4]))\n    var3 = xr.Variable((\"x\", \"y\"), np.array([[5, 6], [7, 8]]))\n\n    variables = {\"var1\": var1, \"var2\": var2, \"var3\": var3}\n    merged_vars = merge.merge_variables([variables], compat=\"broadcast_equals\")\n\n    expected_var3 = xr.Variable((\"x\", \"y\"), np.array([[5, 6], [7, 8]]))\n    assert merged_vars[\"var3\"].equals(expected_var3)\n"], "sample_588": ["def test_check_shape_tile_ids():\n    # Create a dictionary with inconsistent tile IDs\n    tile_ids = {(0, 1): Dataset(), (0, 2): Dataset(), (1, 1): Dataset()}\n    with pytest.raises(ValueError):\n        _check_shape_tile_ids(tile_ids)\n\n    # Create a dictionary with consistent tile IDs\n    tile_ids = {(0, 0): Dataset(), (0, 1): Dataset(), (1, 0): Dataset(), (1, 1): Dataset()}\n    _check_shape_tile_ids(tile_ids)\n", "def test_infer_concat_order_from_coords():\n    # create 4 datasets with different temperature and precipitation values\n    # but same coordinates\n    temp1 = DataArray(np.random.rand(2, 2), dims=[\"x\", \"y\"])\n    precip1 = DataArray(np.random.rand(2, 2), dims=[\"x\", \"y\"])\n    ds1 = Dataset({\"temperature\": temp1, \"precipitation\": precip1})\n\n    temp2 = DataArray(np.random.rand(2, 2), dims=[\"x\", \"y\"])\n    precip2 = DataArray(np.random.rand(2, 2), dims=[\"x\", \"y\"])\n    ds2 = Dataset({\"temperature\": temp2, \"precipitation\": precip2})\n\n    temp3 = DataArray(np.random.rand(2, 2), dims=[\"x\", \"y\"])\n    precip3 = DataArray(np.random.rand(2, 2), dims=[\"x\", \"y\"])\n    ds3 = Dataset({\"temperature\": temp3, \"precipitation\": precip3})\n\n    temp4 = DataArray(np.random.rand(2, 2), dims=[\"x\", \"y\"])\n    precip4 = DataArray(np.random.rand(2, 2), dims=[\"x\", \"y\"])\n    ds4 = Dataset({\"temperature\": temp4, \"precipitation\": precip4})\n\n    datasets = [ds1, ds2, ds3, ds4]\n\n    # add global coordinates to the datasets\n    for i, ds in enumerate(datasets):\n        ds.coords[\"time\"] = DataArray([i], dims=[\"time\"])\n\n    # infer concat order from coords\n    combined_ids, concat_dims = _infer_concat_order_from_coords(datasets)\n\n    # check that the combined ids are correct\n    expected_combined_ids = OrderedDict([(i,) for i in range(4)])\n    for i, ds in enumerate(datasets):\n        expected_combined_ids[(i,)] = ds\n\n    assert_combined_tile_ids_equal(combined_ids, expected_combined_ids)\n\n    # check that the concat dims are correct\n    expected_concat_dims = [\"time\"]\n    assert concat_dims == expected_concat_dims\n", "def test_infer_concat_order_from_coords():\n    # Create 4 datasets with different temperature and precipitation values\n    temp1 = DataArray([1, 2, 3], dims=[\"x\"])\n    precip1 = DataArray([4, 5, 6], dims=[\"x\"])\n    ds1 = Dataset({\"temperature\": temp1, \"precipitation\": precip1})\n\n    temp2 = DataArray([7, 8, 9], dims=[\"x\"])\n    precip2 = DataArray([10, 11, 12], dims=[\"x\"])\n    ds2 = Dataset({\"temperature\": temp2, \"precipitation\": precip2})\n\n    temp3 = DataArray([13, 14, 15], dims=[\"x\"])\n    precip3 = DataArray([16, 17, 18], dims=[\"x\"])\n    ds3 = Dataset({\"temperature\": temp3, \"precipitation\": precip3})\n\n    temp4 = DataArray([19, 20, 21], dims=[\"x\"])\n    precip4 = DataArray([22, 23, 24], dims=[\"x\"])\n    ds4 = Dataset({\"temperature\": temp4, \"precipitation\": precip4})\n\n    # Create coordinate variables\n    ds1.coords[\"x\"] = DataArray([1, 2, 3], dims=[\"x\"])\n    ds2.coords[\"x\"] = DataArray([4, 5, 6], dims=[\"x\"])\n    ds3.coords[\"x\"] = DataArray([7, 8, 9], dims=[\"x\"])\n    ds4.coords[\"x\"] = DataArray([10, 11, 12], dims=[\"x\"])\n\n    # Test that the datasets are concatenated in the correct order\n    combined_ids, concat_dims = _infer_concat_order_from_coords([ds1, ds2, ds3, ds4])\n    expected_combined_ids = OrderedDict([(0, ds1), (1, ds2), (2, ds3), (3, ds4)])\n    assert_combined_tile_ids_equal(combined_ids, expected_combined_ids)\n    assert concat_dims == [\"x\"]\n", "def test_infer_concat_order_from_positions():\n    # Create a nested list of datasets with different variables\n    ds1 = Dataset()\n    ds1[\"a\"] = DataArray([1, 2], dims=\"x\")\n    ds2 = Dataset()\n    ds2[\"b\"] = DataArray([3, 4], dims=\"x\")\n    ds3 = Dataset()\n    ds3[\"c\"] = DataArray([5, 6], dims=\"x\")\n    nested_list = [[ds1, ds2], [ds3]]\n\n    # Infer the concat order from positions\n    combined_ids = _infer_concat_order_from_positions(nested_list)\n\n    # Check the result\n    expected = OrderedDict([((0, 0), ds1), ((0, 1), ds2), ((1, 0), ds3)])\n    assert_combined_tile_ids_equal(combined_ids, expected)\n", "def test_infer_concat_order_from_coords():\n    # Create 4 datasets with coordinates that can be used for concatenation\n    data = create_test_data()\n    ds00 = data.isel(x=slice(0, 2), y=slice(0, 2))\n    ds01 = data.isel(x=slice(0, 2), y=slice(2, 4))\n    ds10 = data.isel(x=slice(2, 4), y=slice(0, 2))\n    ds11 = data.isel(x=slice(2, 4), y=slice(2, 4))\n\n    datasets = [ds00, ds01, ds10, ds11]\n    combined_ids, concat_dims = _infer_concat_order_from_coords(datasets)\n    expected_combined_ids = {(0, 0): ds00, (0, 1): ds01, (1, 0): ds10, (1, 1): ds11}\n    assert_combined_tile_ids_equal(combined_ids, expected_combined_ids)\n    assert concat_dims == [\"x\", \"y\"]\n"], "sample_589": ["def test_interp_func_1d():\n    # Test 1D interpolation with scipy interp1d\n    x = np.array([1, 2, 3, 4, 5])\n    y = np.array([2, 3, 5, 7, 11])\n    new_x = np.array([1.5, 2.5, 3.5, 4.5])\n\n    expected = np.interp(new_x, x, y)\n    actual = _interpnd(y, (x,), (new_x,), scipy.interpolate.interp1d, {})\n\n    assert_array_equal(expected, actual)\n", "def test_get_clean_interp_index_non_monotonic():\n    arr = xr.DataArray([1, 2, 3], dims=\"x\", coords={\"x\": [3, 2, 1]})\n    with raises_regex(ValueError, \"Index x must be monotonically increasing\"):\n        get_clean_interp_index(arr, \"x\")\n", "def test_interp_like():\n    da = xr.DataArray([0, 1, 2], dims=\"x\", coords={\"x\": [0, 1, 2]})\n    other = xr.DataArray([np.nan, np.nan, np.nan], dims=\"x\", coords={\"x\": [0.5, 1.5, 2.5]})\n\n    actual = da.interp_like(other)\n    expected = xr.DataArray([0.5, 1.5, 2.5], dims=\"x\", coords={\"x\": [0.5, 1.5, 2.5]})\n    assert_equal(actual, expected)\n", "def test_get_clean_interp_index():\n    # Test uniform spacing\n    da = xr.DataArray([1, 2, 3], dims=\"x\", coords={\"x\": [1, 2, 3]})\n    index = get_clean_interp_index(da, \"x\")\n    assert_array_equal(index, np.array([1.0, 2.0, 3.0]))\n\n    # Test non-uniform spacing\n    da = xr.DataArray([1, 2, 3], dims=\"x\", coords={\"x\": [1, 4, 9]})\n    index = get_clean_interp_index(da, \"x\")\n    assert_array_equal(index, np.array([1.0, 4.0, 9.0]))\n\n    # Test with duplicate coordinates\n    da = xr.DataArray([1, 2, 3], dims=\"x\", coords={\"x\": [1, 2, 2]})\n    with pytest.raises(ValueError):\n        get_clean_interp_index(da, \"x\")\n\n    # Test with non-monotonic coordinates\n    da = xr.DataArray([1, 2, 3], dims=\"x\", coords={\"x\": [1, 3, 2]})\n    with pytest.raises(ValueError):\n        get_clean_interp_index(da, \"x\")\n\n    # Test with non-numeric coordinates\n    da = xr.DataArray([1, 2, 3], dims=\"x\", coords={\"x\": [\"a\", \"b\", \"c\"]})\n    with pytest.raises(TypeError):\n        get_clean_interp_index(da, \"x\")\n", "def test_interp_na_limit(da):\n    actual = da.interp_na(dim=\"time\", limit=1)\n    expected = xr.DataArray(\n        [0, 0.5, 1, 2, 2.5, 3, 4, 5, 5.5, 6, 7], dims=\"time\"\n    )\n    assert_array_equal(actual, expected)\n\n    actual = da.interp_na(dim=\"time\", limit=2)\n    expected = xr.DataArray(\n        [0, 0.5, 1, 2, 2.5, 3, 4, 5, 5.5, 6, 7], dims=\"time\"\n    )\n    assert_array_equal(actual, expected)\n\n    actual = da.interp_na(dim=\"time\", limit=0)\n    expected = xr.DataArray(\n        [0, np.nan, 1, 2, np.nan, 3, 4, 5, np.nan, 6, 7], dims=\"time\"\n    )\n    assert_array_equal(actual, expected)\n"], "sample_590": ["def test_concat_dataset_dim_order():\n    # Test that concat preserves dimension order\n    ds1 = Dataset()\n    ds1['x'] = ('x', [1, 2])\n    ds1['y'] = ('y', [3, 4])\n    ds1['z'] = ('x', [5, 6])\n\n    ds2 = Dataset()\n    ds2['x'] = ('x', [7, 8])\n    ds2['y'] = ('y', [9, 10])\n    ds2['z'] = ('x', [11, 12])\n\n    concatenated = concat([ds1, ds2], dim='new_dim')\n    assert concatenated.dims == {'new_dim': 2, 'x': 2, 'y': 2}\n", "def test_concat_dim_coord():\n    # Test that the coordinate of the concatenated dimension is set properly\n    dim = \"new_dim\"\n    coord = DataArray([1, 2], dims=[dim], name=dim)\n    ds1 = Dataset()\n    ds2 = Dataset()\n    result = concat([ds1, ds2], dim=coord)\n    assert_identical(result.coords[dim], coord)\n", "def test_concat_dataset_dim_order():\n    # Make sure that the order of dimensions is preserved with concat.\n    ds1 = Dataset()\n    ds1[\"x\"] = (\"y\", \"z\", \"t\"), np.random.rand(2, 3, 4)\n    ds2 = Dataset()\n    ds2[\"x\"] = (\"y\", \"z\", \"t\"), np.random.rand(2, 3, 4)\n\n    assert_identical(\n        concat((ds1, ds2), dim=\"concat\"),\n        concat((ds1, ds2), dim=\"concat\").transpose(),\n    )\n", "def test_concat_fill_value():\n    # Test that fill_value works with Dataset concat\n    ds1 = Dataset()\n    ds1[\"x\"] = (\"x\", [1, 2, 3])\n    ds2 = Dataset()\n    ds2[\"x\"] = (\"x\", [4, 5, 6])\n\n    result = concat([ds1, ds2], dim=\"y\", fill_value=-999)\n\n    expected = Dataset()\n    expected[\"x\"] = ((\"y\", \"x\"), [[1, 2, 3], [4, 5, 6]])\n\n    assert_identical(result, expected)\n", "def test_concat_dataset_with_positions():\n    # Test concatenating datasets with positions\n    data = create_test_data()\n    datasets = [data, data]\n    positions = [[0], [1]]\n    expected = Dataset()\n    for var in data.data_vars:\n        expected[var] = Variable([\"time\", \"x\", \"y\"], np.concatenate([data[var].values, data[var].values], axis=0))\n    result = concat(datasets, dim=\"time\", positions=positions)\n    assert_identical(result, expected)\n"], "sample_592": ["def test_pretty_print():\n    max_name_length = 80\n    col_width = max_name_length + 6\n\n    # Test with a short name\n    name = \"short_name\"\n    expected = pretty_print(f\"  {name} \", col_width)\n    assert len(expected) == col_width\n\n    # Test with a long name\n    name = \"a\" * (max_name_length + 1)\n    expected = pretty_print(f\"  {name} \", col_width)\n    assert len(expected) == col_width\n    assert expected.endswith(\"...\")\n\n    # Test with an even longer name\n    name = \"a\" * (col_width + 1)\n    expected = pretty_print(f\"  {name} \", col_width)\n    assert len(expected) == col_width\n    assert expected.endswith(\"...\")\n", "def test_format_item_timedelta64():\n    assert formatting.format_item(np.timedelta64(1, \"D\"), timedelta_format=\"date\") == \"1 days\"\n    assert formatting.format_item(np.timedelta64(1, \"D\"), timedelta_format=\"time\") == \"00:00:00\"\n    assert formatting.format_item(np.timedelta64(1, \"D\")) == \"1 days 00:00:00\"\n", "def test_pretty_print():\n    max_name_length = 40\n    names = [\n        \"short\",\n        \"very_long_variable_name_that_exceeds_max_length\",\n        (\"short\", \"very_long_tuple_item_name\"),\n        (\"short\", \"short\"),\n        (\"very_long_tuple_item_name\", \"short\"),\n    ]\n    expected = [\n        \"short\",\n        \"very_long_variable_name_that_e\",\n        \"('short', 'very_long_tuple_item_name')\",\n        \"('short', 'short')\",\n        \"('very_long_tuple_item_name', 'short')\",\n    ]\n    for name, exp in zip(names, expected):\n        act = formatting.pretty_print(name, max_name_length)\n        assert act == exp\n", "def test_inline_variable_array_repr():\n    # Test that inline_variable_array_repr works as expected for a simple case\n    var = xr.Variable((\"x\",), np.array([1, 2, 3]))\n    expected = \"1 2 3\"\n    assert formatting.inline_variable_array_repr(var, max_width=10) == expected\n\n    # Test that inline_variable_array_repr truncates arrays that are too long\n    var = xr.Variable((\"x\",), np.arange(1000))\n    expected = \"0 1 2 ... 997 998 999\"\n    assert formatting.inline_variable_array_repr(var, max_width=20) == expected\n", "def test_inline_variable_array_repr():\n    array = xr.DataArray([1, 2, 3], dims=\"x\")\n    assert formatting.inline_variable_array_repr(array, max_width=80) == \"[1 2 3]\"\n    array = xr.DataArray([1, 2, 3] * 1000, dims=\"x\")\n    assert len(formatting.inline_variable_array_repr(array, max_width=80)) <= 80\n"], "sample_593": ["def test_array_repr_html():\n    arr = xr.DataArray(np.random.RandomState(0).randn(4, 6), dims=[\"x\", \"y\"])\n    html = fh.array_repr(arr)\n    assert \"xarray.DataArray\" in html\n    assert \"Dimensions: x: 4, y: 6\" in html\n    assert \"Attributes:\" not in html\n\n    arr.attrs[\"foo\"] = \"bar\"\n    html = fh.array_repr(arr)\n    assert \"Attributes:\" in html\n    assert \"foo: bar\" in html\n", "def test_short_data_repr_html(dataarray):\n    text = fh.short_data_repr(dataarray)\n    html = fh.short_data_repr_html(dataarray)\n\n    # check that text and html reprs are consistent\n    assert text in html\n\n    # check that html repr contains expected markup\n    assert html.startswith(\"<pre>\")\n    assert html.endswith(\"</pre>\")\n\n    # check that html repr handles multi-dimensional data\n    dataarray_3d = xr.DataArray(np.random.RandomState(0).randn(2, 3, 4))\n    html_3d = fh.short_data_repr_html(dataarray_3d)\n    assert \"...\" in html_3d  # indicates truncation\n", "def test_dataset_repr_multiindex(multiindex):\n    html = fh.dataset_repr(multiindex)\n    assert \"Coordinates\" in html\n    assert \"(level_1, level_2)\" in html  # multi-index repr\n    assert \"x\" in html  # coord name\n", "def test_array_section_with_empty_dataarray():\n    arr = xr.DataArray(np.empty((0, 0)))\n    section = fh.array_section(arr)\n    expected = (\n        \"<div class='xr-array-wrap'>\"\n        \"<input id='section-\" + str(uuid.uuid4()) + \"' class='xr-array-in' type='checkbox' checked>\"\n        \"<label for='section-\" + str(uuid.uuid4()) + \"' title='Show/hide data repr'><svg \"\n        \"class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label>\"\n        \"<div class='xr-array-preview xr-preview'><span></span></div>\"\n        \"<div class='xr-array-data'><pre></pre></div>\"\n        \"</div>\"\n    )\n    assert section == expected\n", "def test_array_repr_multiindex(dataarray):\n    dataarray = dataarray.assign_coords(\n        level_1=(\"x\", [\"a\"] * 4), level_2=(\"x\", [1, 2, 3, 4])\n    ).set_index(x=[\"level_1\", \"level_2\"])\n    html = fh.array_repr(dataarray)\n    assert \"MultiIndex\" in html\n    assert \"level_1\" in html\n    assert \"level_2\" in html\n"], "sample_594": ["def test_inline_variable_array_repr_ndarray():\n    v = xr.Variable((\"x\",), np.array([1, 2, 3], dtype=\"int64\"))\n    expected = \"[1 2 3]\"\n    assert formatting.inline_variable_array_repr(v, 80) == expected\n\n    v = xr.Variable((\"x\", \"y\"), np.array([[1, 2], [3, 4]], dtype=\"int64\"))\n    expected = \"[[1 2]\\n [3 4]]\"\n    assert formatting.inline_variable_array_repr(v, 80) == expected\n", "def test_pretty_print():\n    assert formatting.pretty_print(\"test\", 10) == \"test\" + \" \" * 6\n    assert formatting.pretty_print(\"verylongstring\", 10) == \"verylo...\"\n", "def test_inline_variable_array_repr():\n    data = np.random.rand(10, 20)\n    v = xr.Variable((\"x\", \"y\"), data)\n    assert formatting.inline_variable_array_repr(v, 80) == formatting.format_array_flat(v, 80)\n\n    data = np.random.rand(1000, 2000)\n    v = xr.Variable((\"x\", \"y\"), data)\n    assert formatting.inline_variable_array_repr(v, 80) == formatting.inline_dask_repr(v.data)\n", "def test_formatting_short_numpy_repr():\n    array = np.random.rand(1000)\n    expected_repr = formatting.short_numpy_repr(array)\n    assert len(expected_repr) <= 80\n\n    array = np.random.rand(1000, 1000)\n    expected_repr = formatting.short_numpy_repr(array)\n    assert len(expected_repr) <= 80\n\n    array = np.random.rand(10)\n    expected_repr = formatting.short_numpy_repr(array)\n    assert len(expected_repr) > 50\n", "def test_pretty_print():\n    max_name_length = 10\n    col_width = formatting._calculate_col_width([\"short\", \"long_variable_name\"])\n\n    assert col_width == max_name_length + 6\n\n    short_repr = formatting.pretty_print(\"short\", col_width)\n    long_repr = formatting.pretty_print(\"long_variable_name\", col_width)\n\n    assert len(short_repr) == col_width\n    assert len(long_repr) == col_width\n"], "sample_595": ["def test_str_accessor_slice(dtype):\n    values = np.array([\"hello\", \"world\"], dtype=dtype)\n    da = xr.DataArray(values)\n\n    expected = xr.DataArray(np.array([\"he\", \"wo\"], dtype=dtype))\n    result = da.str.slice(0, 2)\n    assert_equal(result, expected)\n\n    expected = xr.DataArray(np.array([\"lo\", \"ld\"], dtype=dtype))\n    result = da.str.slice(-2)\n    assert_equal(result, expected)\n\n    expected = xr.DataArray(np.array([\"l\", \"r\"], dtype=dtype))\n    result = da.str.slice(2, -2)\n    assert_equal(result, expected)\n", "def test_str_accessor_replace(dtype):\n    values = xr.DataArray([\"foo\", \"fuz\", \"bar\"], dtype=dtype)\n    result = values.str.replace(\"f\", \"b\")\n    expected = xr.DataArray([\"boo\", \"buz\", \"bar\"], dtype=dtype)\n    assert_equal(result, expected)\n\n    # test with regex=False\n    result = values.str.replace(\"f\", \"b\", regex=False)\n    assert_equal(result, expected)\n", "def test_str_accessor_decode(dtype):\n    values = np.array([\"a\", \"b\", \"c\"]).astype(dtype)\n    da = xr.DataArray(values)\n\n    decoded = da.str.decode(\"utf-8\")\n    expected = xr.DataArray(np.array([\"a\", \"b\", \"c\"], dtype=np.str_))\n    assert_equal(decoded, expected)\n", "def test_str_accessor_replace(dtype):\n    values = xr.DataArray([\"foo\", \"fuz\", \"bar\"], dtype=dtype)\n    expected = xr.DataArray([\"hoo\", \"huz\", \"har\"], dtype=dtype)\n\n    result = values.str.replace(\"f\", \"h\")\n    assert_equal(result, expected)\n", "def test_str_accessor_count(dtype):\n    values = xr.DataArray([\"foo\", \"foobar\", \"bar\", \"baz\"], dtype=dtype)\n    result = values.str.count(\"o\")\n    expected = xr.DataArray([2, 2, 0, 0], dtype=\"int64\")\n    assert_equal(result, expected)\n\n    result = values.str.count(\"o\", flags=re.IGNORECASE)\n    expected = xr.DataArray([2, 2, 0, 0], dtype=\"int64\")\n    assert_equal(result, expected)\n"], "sample_596": ["def test_concat_dataarray_with_new_dim():\n    # Test concatenating DataArray objects along a new dimension\n    da1 = DataArray([1, 2], dims=[\"x\"])\n    da2 = DataArray([3, 4], dims=[\"x\"])\n\n    expected = DataArray([[1, 2], [3, 4]], dims=[\"concat_dim\", \"x\"])\n\n    result = concat([da1, da2], dim=\"concat_dim\")\n    assert_identical(result, expected)\n\n    # Test concatenating with the new dimension as a DataArray\n    dim_da = DataArray([0, 1], dims=[\"concat_dim\"])\n    result = concat([da1, da2], dim=dim_da)\n    assert_identical(result, expected)\n\n    # Test concatenating with the new dimension as a pandas Index\n    dim_index = pd.Index([0, 1], name=\"concat_dim\")\n    result = concat([da1, da2], dim=dim_index)\n    assert_identical(result, expected)\n", "def test_concat_dataset_dim_order():\n    # Test that the order of dimensions in the resulting dataset is correct.\n    ds1 = Dataset()\n    ds1['x'] = ('x', [1])\n    ds1['y'] = ('y', [2])\n    ds1['z'] = ('x', [3])\n\n    ds2 = Dataset()\n    ds2['x'] = ('x', [4])\n    ds2['y'] = ('y', [5])\n    ds2['z'] = ('x', [6])\n\n    result = concat([ds1, ds2], dim='new')\n    assert list(result.dims) == ['new', 'x', 'y']\n", "def test_concat_coords_data_vars_kwarg():\n    # Test that the coords and data_vars kwargs are respected.\n    # These were added to ensure that only DataArrays in which the dimension already appears are concatenated.\n\n    # Create some test data\n    da1 = DataArray([1, 2], dims='x', coords={'x': [10, 20]})\n    da2 = DataArray([3, 4], dims='x', coords={'x': [30, 40]})\n\n    # Concatenate along a new dimension 'y'\n    result_minimal = concat([da1, da2], dim='y', coords='minimal')\n    assert list(result_minimal.coords) == ['x']\n\n    result_all = concat([da1, da2], dim='y', coords='all')\n    assert list(result_all.coords) == ['x', 'y']\n\n    result_different = concat([da1, da2], dim='y', coords='different')\n    assert list(result_different.coords) == ['x', 'y']\n\n    # Test with data_vars kwarg\n    result_minimal = concat([da1, da2], dim='y', data_vars='minimal')\n    assert list(result_minimal.data_vars) == []\n\n    result_all = concat([da1, da2], dim='y', data_vars='all')\n    assert list(result_all.data_vars) == [None]\n\n    result_different = concat([da1, da2], dim='y', data_vars='different')\n    assert list(result_different.data_vars) == [None]\n", "def test_concat_dataset_coords_compat():\n    # Test that concat with coords='different' or 'all' works as expected\n    ds1 = Dataset()\n    ds1['x'] = ('x', [1, 2])\n    ds1.coords['y'] = ('y', [3, 4])\n\n    ds2 = Dataset()\n    ds2['x'] = ('x', [5, 6])\n    ds2.coords['y'] = ('y', [7, 8])\n\n    expected = Dataset()\n    expected['x'] = ('x', np.concatenate([ds1['x'].values, ds2['x'].values]))\n    expected.coords['y'] = ('y', np.concatenate([ds1['y'].values, ds2['y'].values]))\n\n    actual_all = concat((ds1, ds2), dim='new_dim', coords='all')\n    assert_identical(actual_all, expected)\n\n    actual_different = concat((ds1, ds2), dim='new_dim', coords='different')\n    assert_identical(actual_different, expected)\n", "def test_concat_dim_coord_name():\n    # Test that the name of the concatenated dimension is used as the\n    # name of the resulting coordinate variable.\n    da1 = DataArray(np.array([1, 2]), dims=[\"x\"], coords={\"x\": [0, 1]})\n    da2 = DataArray(np.array([3, 4]), dims=[\"x\"], coords={\"x\": [2, 3]})\n\n    da_concat = concat([da1, da2], dim=\"new_x\")\n\n    assert da_concat.dims == (\"new_x\",)\n    assert \"new_x\" in da_concat.coords\n    assert_array_equal(da_concat.coords[\"new_x\"], [0, 1, 2, 3])\n"], "sample_597": ["def test_merge_collected():\n    collected = {\n        \"x\": [(xr.Variable(), None), (xr.Variable(), None)],\n        \"y\": [(xr.Variable(), pd.Index([1, 2]))],\n    }\n    prioritized = {\"x\": (xr.Variable(), None)}\n\n    merged, indexes = merge.merge_collected(collected, prioritized)\n    assert len(merged) == 2\n    assert len(indexes) == 1\n    assert isinstance(merged[\"x\"], xr.Variable)\n    assert isinstance(indexes[\"y\"], pd.Index)\n", "def test_merge_collected():\n    # Test that merge_collected handles a variety of inputs correctly.\n    collected = {\n        \"var1\": [(xr.Variable(), None), (xr.Variable(), None)],\n        \"var2\": [(xr.Variable(), pd.Index([1, 2, 3])), (xr.Variable(), pd.Index([1, 2, 3]))],\n    }\n    result_vars, result_indexes = merge.merge_collected(collected)\n\n    assert len(result_vars) == 2\n    assert len(result_indexes) == 1\n\n    with pytest.raises(MergeError):\n        collected[\"var2\"] = [(xr.Variable(), pd.Index([1, 2, 3])), (xr.Variable(), pd.Index([4, 5, 6]))]\n        merge.merge_collected(collected)\n", "def test_merge_collected():\n    collected = {\n        \"x\": [(xr.Variable(), None), (xr.Variable(), None)],\n        \"y\": [(xr.Variable(), pd.Index([1, 2, 3])), (xr.Variable(), pd.Index([1, 2, 3]))],\n    }\n    prioritized = {\"z\": (xr.Variable(), pd.Index([4, 5, 6]))}\n\n    merged_vars, merged_indexes = merge.merge_collected(collected, prioritized)\n\n    assert len(merged_vars) == 3\n    assert len(merged_indexes) == 2\n    assert isinstance(merged_vars[\"x\"], xr.Variable)\n    assert isinstance(merged_vars[\"y\"], xr.Variable)\n    assert isinstance(merged_vars[\"z\"], xr.Variable)\n    assert isinstance(merged_indexes[\"y\"], pd.Index)\n    assert isinstance(merged_indexes[\"z\"], pd.Index)\n", "def test_merge_core_with_priority_arg():\n    # Test that merge_core respects the priority_arg argument.\n    # This should prioritize the second dataset when merging.\n\n    ds1 = xr.Dataset()\n    ds1[\"a\"] = (\"x\", [1, 2])\n    ds1[\"b\"] = (\"x\", [3, 4])\n\n    ds2 = xr.Dataset()\n    ds2[\"a\"] = (\"x\", [5, 6])\n    ds2[\"c\"] = (\"x\", [7, 8])\n\n    expected = xr.Dataset()\n    expected[\"a\"] = (\"x\", [5, 6])\n    expected[\"b\"] = (\"x\", [3, 4])\n    expected[\"c\"] = (\"x\", [7, 8])\n\n    result = merge.merge_core([ds1, ds2], priority_arg=1)\n    assert_identical(result, expected)\n", "def test_merge_collected_indexes():\n    # Test that merge_collected prioritizes indexes over variables with the same name.\n    index1 = xr.IndexVariable(\"x\", [1, 2, 3])\n    index2 = xr.IndexVariable(\"x\", [4, 5, 6])\n    var1 = xr.Variable((\"x\",), [7, 8, 9])\n    collected = {\"x\": [(var1, None), (index1, index1.to_index()), (index2, index2.to_index())]}\n    result_variables, result_indexes = merge.merge_collected(collected)\n    assert_identical(result_variables[\"x\"], index1)\n    assert_identical(result_indexes[\"x\"], index1.to_index())\n"], "sample_598": ["def test_pretty_print():\n    max_name_length = 10\n    col_width = formatting._calculate_col_width([\"short\", \"very_long_variable_name\"])\n    assert col_width == max(max_name_length, 7) + 6\n\n    short_name = \"short\"\n    long_name = \"very_long_variable_name\"\n\n    # short name should be padded to the calculated width\n    assert len(formatting.pretty_print(short_name, col_width)) == col_width\n\n    # long name should be truncated and padded to the calculated width\n    assert len(formatting.pretty_print(long_name, col_width)) == col_width\n", "def test_inline_variable_array_repr_datetime64():\n    var = xr.Variable(\"x\", np.array([np.datetime64(\"2000-01-01\")]))\n    expected = \"2000-01-01\"\n    assert formatting.inline_variable_array_repr(var, 10) == expected\n\n    var = xr.Variable(\"x\", np.array([np.datetime64(\"2000-01-01T12:00:00\")]))\n    expected = \"2000-01-01T12:00:00\"\n    assert formatting.inline_variable_array_repr(var, 20) == expected\n", "def test_inline_variable_array_repr_datetime64():\n    var = xr.Variable(\n        dims=(\"time\",),\n        values=np.array([np.datetime64(\"2000-01-01T12:00:00\")], dtype=\"datetime64[ns]\"),\n    )\n    expected = \"2000-01-01T12:00:00\"\n    assert formatting.inline_variable_array_repr(var, 20) == expected\n", "def test_pretty_print():\n    max_name_length = 10\n    col_width = formatting._calculate_col_width([\"short\", \"very_long_variable_name\"])\n    assert col_width == max_name_length + 6\n\n    short_name = \"short\"\n    long_name = \"very_long_variable_name\"\n\n    # short name should be padded to the calculated width\n    assert len(formatting.pretty_print(short_name, col_width)) == col_width\n\n    # long name should be truncated and ellipsized if necessary\n    assert len(formatting.pretty_print(long_name, col_width)) == col_width\n", "def test_format_timestamp():\n    assert formatting.format_timestamp(pd.Timestamp(\"2000-01-01\")) == \"2000-01-01\"\n    assert formatting.format_timestamp(np.datetime64(\"2000-01-01\")) == \"2000-01-01\"\n    assert formatting.format_timestamp(pd.Timestamp(\"2000-01-01 12:00:00\")) == \"2000-01-01T12:00:00\"\n    assert formatting.format_timestamp(np.datetime64(\"2000-01-01T12:00:00\")) == \"2000-01-01T12:00:00\"\n"], "sample_599": ["def test_CFScaleOffsetCoder_encode_decode():\n    coder = variables.CFScaleOffsetCoder()\n    scale_factor = 2.0\n    add_offset = 3.0\n    encoding = {\"scale_factor\": scale_factor, \"add_offset\": add_offset}\n    attrs = {}\n    data = np.array([1, 2, 3], dtype=np.float32)\n    variable = xr.Variable((\"x\",), data, attrs, encoding=encoding)\n\n    encoded_var = coder.encode(variable)\n    assert_allclose(encoded_var.data, (data - add_offset) / scale_factor)\n\n    decoded_var = coder.decode(encoded_var)\n    assert_identical(decoded_var, variable)\n", "def test_scale_offset_encoding():\n    dtype = \"float32\"\n    scale_factor = 10.0\n    add_offset = 2.0\n    encoding = {\"dtype\": dtype, \"scale_factor\": scale_factor, \"add_offset\": add_offset}\n    coder = variables.CFScaleOffsetCoder()\n    data = np.array([1.0, 2.0, 3.0], dtype=dtype)\n    variable = xr.Variable((\"x\",), data, encoding=encoding)\n\n    encoded_var = coder.encode(variable)\n    assert_allclose(encoded_var.data, (data - add_offset) / scale_factor)\n", "def test_cf_scale_offsetCoder():\n    coder = variables.CFScaleOffsetCoder()\n\n    # Test encode with scale_factor and add_offset\n    data = np.array([1, 2, 3], dtype=np.float32)\n    encoding = {\"scale_factor\": 2.0, \"add_offset\": 3.0}\n    attrs = {}\n    variable = xr.Variable([], data, attrs, encoding)\n    encoded_var = coder.encode(variable)\n    assert_allclose(encoded_var.data, (data - 3) / 2)\n\n    # Test decode with scale_factor and add_offset\n    decoded_var = coder.decode(encoded_var)\n    assert_allclose(decoded_var.data, data)\n\n    # Test encode with only scale_factor\n    data = np.array([1, 2, 3], dtype=np.float32)\n    encoding = {\"scale_factor\": 2.0}\n    attrs = {}\n    variable = xr.Variable([], data, attrs, encoding)\n    encoded_var = coder.encode(variable)\n    assert_allclose(encoded_var.data, data / 2)\n\n    # Test decode with only scale_factor\n    decoded_var = coder.decode(encoded_var)\n    assert_allclose(decoded_var.data, data)\n\n    # Test encode with only add_offset\n    data = np.array([1, 2, 3], dtype=np.float32)\n    encoding = {\"add_offset\": 3.0}\n    attrs = {}\n    variable = xr.Variable([], data, attrs, encoding)\n    encoded_var = coder.encode(variable)\n    assert_allclose(encoded_var.data, data - 3)\n\n    # Test decode with only add_offset\n    decoded_var = coder.decode(encoded_var)\n    assert_allclose(decoded_var.data, data)\n", "def test_UnsignedIntegerCoder_encode():\n    coder = variables.UnsignedIntegerCoder()\n    unsigned_var = xr.Variable(\n        (\"x\",), np.array([1, 2, 3], dtype=\"uint32\"), attrs={\"_Unsigned\": \"true\"}\n    )\n    signed_var = coder.encode(unsigned_var)\n    assert signed_var.dtype == \"int32\"\n    assert \"_Unsigned\" not in signed_var.attrs\n    assert_identical(unsigned_var, coder.decode(signed_var))\n", "def test_CFMaskCoder_encode():\n    data = np.array([1, 2, np.nan, 4])\n    variable = xr.Variable((\"x\",), data, attrs={\"_FillValue\": 999})\n    coder = variables.CFMaskCoder()\n    encoded_variable = coder.encode(variable)\n\n    assert_identical(\n        encoded_variable,\n        xr.Variable(\n            (\"x\",), np.array([1, 2, 999, 4]), attrs={\"_FillValue\": 999}\n        ),\n    )\n"], "sample_600": ["def test_CFMaskCoder_encode_decode():\n    coder = variables.CFMaskCoder()\n\n    # Test with _FillValue\n    var = xr.Variable(\n        dims=(\"x\",),\n        data=np.array([1, 2, np.nan, 4]),\n        attrs={\"_FillValue\": 3},\n        encoding={\"dtype\": \"int32\"},\n    )\n    encoded_var = coder.encode(var)\n    assert encoded_var.dtype == \"int32\"\n    assert_equal(encoded_var.data, np.array([1, 2, 3, 4]))\n\n    decoded_var = coder.decode(encoded_var)\n    assert_identical(decoded_var, var)\n\n    # Test with missing_value\n    var = xr.Variable(\n        dims=(\"x\",),\n        data=np.array([1, 2, np.nan, 4]),\n        attrs={\"missing_value\": 3},\n        encoding={\"dtype\": \"int32\"},\n    )\n    encoded_var = coder.encode(var)\n    assert encoded_var.dtype == \"int32\"\n    assert_equal(encoded_var.data, np.array([1, 2, 3, 4]))\n\n    decoded_var = coder.decode(encoded_var)\n    assert_identical(decoded_var, var)\n", "def test_unsigned_integer_coder():\n    coder = variables.UnsignedIntegerCoder()\n    dims, data, attrs, encoding = (\"x\",), np.array([1, 2, 3], dtype=\"uint16\"), {}, {\"_Unsigned\": \"true\"}\n    encoded_var = coder.encode(xr.Variable(dims, data, attrs, encoding))\n    assert encoded_var.dtype == \"int16\"\n    assert \"_Unsigned\" not in encoded_var.attrs\n    assert \"_Unsigned\" not in encoded_var.encoding\n\n    decoded_var = coder.decode(encoded_var)\n    assert decoded_var.dtype == \"uint16\"\n    assert \"_Unsigned\" in decoded_var.encoding\n    assert decoded_var.encoding[\"_Unsigned\"] == \"true\"\n", "def test_unsigned_integer_coder():\n    coder = variables.UnsignedIntegerCoder()\n\n    # Test encoding\n    var = xr.Variable(\"x\", np.array([1, 2, 3], dtype=\"uint8\"))\n    var.encoding[\"_Unsigned\"] = \"true\"\n    encoded_var = coder.encode(var)\n    assert encoded_var.dtype == \"int8\"\n\n    # Test decoding\n    decoded_var = coder.decode(encoded_var)\n    assert decoded_var.dtype == \"uint8\"\n\n    # Test encoding and decoding with fill value\n    var = xr.Variable(\"x\", np.array([1, 2, 3], dtype=\"uint8\"))\n    var.encoding[\"_Unsigned\"] = \"true\"\n    var.encoding[\"_FillValue\"] = 255\n    encoded_var = coder.encode(var)\n    decoded_var = coder.decode(encoded_var)\n    assert decoded_var.dtype == \"uint8\"\n    assert decoded_var.encoding[\"_FillValue\"] == 255\n", "def test_UnsignedIntegerCoder():\n    coder = variables.UnsignedIntegerCoder()\n\n    # Test encode with _Unsigned = \"true\"\n    variable = xr.Variable(\n        dims=(\"x\",),\n        data=np.array([1, 2, 3], dtype=\"uint8\"),\n        attrs={\"_FillValue\": 255},\n        encoding={\"_Unsigned\": \"true\"},\n    )\n    encoded = coder.encode(variable)\n    assert encoded.dtype == \"int8\"\n    assert encoded.attrs[\"_FillValue\"] == -1\n\n    # Test decode with _Unsigned = \"true\"\n    decoded = coder.decode(encoded)\n    assert decoded.dtype == \"uint8\"\n    assert decoded.attrs[\"_FillValue\"] == 255\n\n    # Test encode with _Unsigned = \"false\"\n    variable = xr.Variable(\n        dims=(\"x\",),\n        data=np.array([1, 2, 3], dtype=\"int8\"),\n        attrs={\"_FillValue\": -1},\n        encoding={\"_Unsigned\": \"false\"},\n    )\n    encoded = coder.encode(variable)\n    assert encoded.dtype == \"int8\"\n    assert encoded.attrs[\"_FillValue\"] == -1\n\n    # Test decode with _Unsigned = \"false\"\n    decoded = coder.decode(encoded)\n    assert decoded.dtype == \"int8\"\n    assert decoded.attrs[\"_FillValue\"] == -1\n\n    # Test warning on non-integer type\n    variable = xr.Variable(\n        dims=(\"x\",),\n        data=np.array([1.0, 2.0, 3.0], dtype=\"float32\"),\n        attrs={\"_FillValue\": -1},\n        encoding={\"_Unsigned\": \"true\"},\n    )\n    with pytest.warns(variables.SerializationWarning):\n        coder.decode(variable)\n", "compilation error"], "sample_601": ["def test_dt_accessor_season(self):\n    seasons = self.times_data.dt.season\n    expected = np.array(\n        [_season_from_months(month) for month in self.times_arr.month]\n    )\n    assert_array_equal(seasons.values, expected)\n", "def test_season(self):\n    seasons = self.data.time.dt.season\n    assert_array_equal(\n        seasons.values,\n        np.array([_season_from_months(m) for m in self.times.month]),\n    )\n\n    seasons = self.times_data.dt.season\n    assert_array_equal(\n        seasons.values,\n        np.array([_season_from_months(m) for m in self.times_arr.flat]).reshape(\n            self.times_arr.shape\n        ),\n    )\n", "def test_dt_strftime(self):\n    assert_array_equal(\n        self.data.time.dt.strftime(\"%Y-%m-%d\"), self.times.strftime(\"%Y-%m-%d\")\n    )\n    assert_array_equal(\n        self.times_data.dt.strftime(\"%Y-%m-%d\"),\n        np.vectorize(lambda x: x.strftime(\"%Y-%m-%d\"))(self.times_arr),\n    )\n", "def test_strftime(self):\n    assert_array_equal(\n        self.data.time.dt.strftime(\"%Y-%m-%d\"), self.times.strftime(\"%Y-%m-%d\")\n    )\n    assert_array_equal(\n        self.times_data.dt.strftime(\"%Y-%m-%d\"),\n        np.vectorize(lambda x: x.strftime(\"%Y-%m-%d\"))(self.times_arr),\n    )\n", "def test_dt_accessor(self):\n    assert_equal(self.data.time.dt.year, self.times.year)\n    assert_equal(self.data.time.dt.month, self.times.month)\n    assert_equal(self.data.time.dt.day, self.times.day)\n    assert_equal(self.data.time.dt.hour, self.times.hour)\n    assert_equal(self.data.time.dt.minute, self.times.minute)\n    assert_equal(self.data.time.dt.second, self.times.second)\n    assert_equal(self.data.time.dt.microsecond, self.times.microsecond)\n    assert_equal(self.data.time.dt.nanosecond, self.times.nanosecond)\n\n    assert_identical(self.data.time.dt.season, xr.DataArray(_season_from_months(self.times.month), dims=[\"time\"]))\n"], "sample_602": ["def test_open_dataset_default_engine():\n    # Make sure we get the default engine for a netCDF file\n    engine = _get_default_engine(\"example.nc\")\n    assert engine == \"netcdf4\"\n\n    # Make sure we get the default engine for a gzipped netCDF file\n    engine = _get_default_engine(\"example.nc.gz\")\n    assert engine == \"scipy\"\n", "def test_open_dataset_with_netcdf4():\n    engine = _get_default_engine(\"example.nc\")\n    assert engine == \"netcdf4\"\n    with xr.Dataset() as ds:\n        ds.to_netcdf(\"example.nc\", engine=engine)\n        with xr.open_dataset(\"example.nc\", engine=engine) as loaded_ds:\n            assert_identical(ds, loaded_ds)\n", "def test_open_dataset_invalid_netcdf():\n    with xr.Dataset() as ds:\n        with pytest.raises(ValueError):\n            xr.backends.api._validate_attrs(ds, invalid_netcdf=True)\n", "def test_get_default_engine_remote_uri():\n    engine = _get_default_engine(\"http://example.com/file.nc\", allow_remote=True)\n    assert engine == \"netcdf4\"\n", "def test_to_netcdf_invalid_engine():\n    ds = xr.Dataset()\n    with pytest.raises(ValueError):\n        ds.to_netcdf(\"test.nc\", engine=\"invalid\")\n"], "sample_603": ["def test_summarize_attrs():\n    attrs = {\"a\": 1, \"b\": \"hello\", \"c\": np.array([1, 2, 3])}\n    expected = \"<dl class='xr-attrs'><dt><span>a :</span></dt><dd>1</dd><dt><span>b :</span></dt><dd>hello</dd><dt><span>c :</span></dt><dd>[1 2 3]</dd></dl>\"\n    assert fh.summarize_attrs(attrs) == expected\n", "def test_array_repr_multiindex(dataarray):\n    dataarray = dataarray.assign_coords(\n        level_1=(\"x\", [\"a\"] * 4), level_2=(\"x\", [1, 2, 3, 4])\n    ).set_index(x=[\"level_1\", \"level_2\"])\n    html = fh.array_repr(dataarray)\n    assert \"xr-has-index\" in html  # coord is an index\n    assert \"<div class='xr-array-name'>\" in html  # array name\n    assert \"Coordinates\" in html  # coord section\n    assert \"MultiIndex\" in html  # MultiIndex type\n    assert \"<th>level_1</th>\" in html  # level 1 header\n    assert \"<th>level_2</th>\" in html  # level 2 header\n", "def test_array_repr_multiindex(dataarray):\n    mindex = pd.MultiIndex.from_product(\n        [[\"a\", \"b\"], [1, 2]], names=(\"level_1\", \"level_2\")\n    )\n    dataarray.coords[\"x\"] = (\"x\", mindex)\n    html = fh.array_repr(dataarray)\n    assert \"Coordinates\" in html\n    assert \"level_1\" in html\n    assert \"level_2\" in html\n    assert \"(a, 1)\" in html\n", "def test_array_repr_multiindex(dataarray):\n    mindex = pd.MultiIndex.from_product(\n        [[\"a\", \"b\"], [1, 2]], names=(\"level_1\", \"level_2\")\n    )\n    dataarray = dataarray.assign_coords(x=mindex)\n    html = fh.array_repr(dataarray)\n    assert \"Coordinates\" in html\n    assert \"MultiIndex\" in html\n    assert \"level_1\" in html\n    assert \"level_2\" in html\n", "def test_coord_section_with_multiindex(multiindex):\n    coord_section_html = fh.coord_section(multiindex.coords)\n    assert \"level_1\" in coord_section_html\n    assert \"level_2\" in coord_section_html\n    assert \"(a, 1)\" in coord_section_html\n    assert \"(b, 2)\" in coord_section_html\n"], "sample_604": ["def test_pretty_print():\n    max_name_length = 20\n    cases = [\n        (\"short\", \"short\"),\n        (\"very_long_variable_name\", \"very_long_variab...\"),\n        ((\"short\", \"longer\"), (\"short\", \"longer\")),\n        ((\"some_long_name\", \"even_longer_name\"), (\"some_long_name\", \"even_longer_n...\")),\n    ]\n    for item, expected in cases:\n        result = formatting.pretty_print(item, max_name_length)\n        assert result == expected\n", "def test_format_item_timedelta():\n    assert formatting.format_item(pd.Timedelta(hours=1), timedelta_format=\"date\") == \"0 days\"\n    assert formatting.format_item(pd.Timedelta(hours=1), timedelta_format=\"time\") == \"01:00:00\"\n    assert formatting.format_item(pd.Timedelta(hours=1)) == \"0 days 01:00:00\"\n", "def test_inline_variable_array_repr():\n    # Test inline_variable_array_repr with different types of arrays\n    array = xr.Variable((2, 3), np.random.rand(2, 3))\n    assert formatting.inline_variable_array_repr(array, 80) != \"\"\n\n    array = xr.DataArray(np.random.rand(2, 3), dims=(\"x\", \"y\"))\n    assert formatting.inline_variable_array_repr(array, 80) != \"\"\n\n    array = xr.DataArray(\n        np.random.rand(2, 3),\n        dims=(\"x\", \"y\"),\n        coords={\"x\": [1, 2], \"y\": [3, 4, 5]},\n    )\n    assert formatting.inline_variable_array_repr(array, 80) != \"\"\n", "def test_format_timestamp_out_of_bounds():\n    timestamp = pd.Timestamp(\"1677-01-01\")  # before valid range\n    assert formatting.format_timestamp(timestamp) == \"1677-01-01\"\n\n    timestamp = pd.Timestamp(\"2263-01-01\")  # after valid range\n    assert formatting.format_timestamp(timestamp) == \"2263-01-01\"\n", "def test_inline_dask_repr():\n    # Create a dask array\n    import dask.array as da\n    dask_array = da.from_array(np.random.random((10, 10)), chunks=(5, 5))\n\n    # Create an xarray variable with the dask array\n    var = xr.Variable((\"x\", \"y\"), dask_array)\n\n    # Test the inline dask repr function\n    repr_str = formatting.inline_dask_repr(var.data)\n\n    # Check if the repr string contains the expected information\n    assert \"dask.array\" in repr_str\n    assert \"chunksize=(5, 5)\" in repr_str\n"], "sample_605": ["def test_groupby_dataset_map(dataset):\n        return x\n\n    expected = dataset\n    result = dataset.groupby(\"x\").map(identity)\n    assert_identical(result, expected)\n", "def test_groupby_concat_dim_order(array):\n    # GH3415\n    expected = array.groupby(\"x\").mean()\n    result = array.groupby(\"x\").map(lambda x: x.mean(\"y\"))\n    assert_identical(result, expected)\n", "def test_groupby_consolidate_slices():\n    slices = [slice(1, 3), slice(3, 5), slice(5, 7)]\n    expected = [slice(1, 7)]\n    assert _consolidate_slices(slices) == expected\n\n    slices = [slice(1, 3), slice(4, 5), slice(5, 7)]\n    expected = [slice(1, 3), slice(4, 7)]\n    assert _consolidate_slices(slices) == expected\n\n    slices = [slice(1, 2), slice(2, 3), slice(3, 4)]\n    expected = [slice(1, 4)]\n    assert _consolidate_slices(slices) == expected\n\n    slices = [slice(None)]\n    expected = [slice(None)]\n    assert _consolidate_slices(slices) == expected\n\n    slices = [slice(1, 3), slice(3, 5, 2)]\n    with pytest.raises(ValueError):\n        _consolidate_slices(slices)\n", "def test_groupby_reduce_dim_order(dataset):\n    # regression test for GH3695\n    ds = dataset\n    reduced = ds.groupby(\"x\").reduce(np.mean, dim=\"y\")\n    assert reduced.dims == (\"x\", \"z\")\n", "def test_groupby_consolidate_slices():\n    slices = [slice(0, 1), slice(1, 3), slice(3, 5)]\n    expected = [slice(0, 1), slice(1, 5)]\n    actual = _consolidate_slices(slices)\n    assert_equal(actual, expected)\n\n    slices = [slice(0, 1), slice(2, 3), slice(3, 4)]\n    expected = [slice(0, 1), slice(2, 4)]\n    actual = _consolidate_slices(slices)\n    assert_equal(actual, expected)\n"], "sample_607": ["def test_remove_duplicates(dummy_duplicated_entrypoints):\n    unique_eps = plugins.remove_duplicates(dummy_duplicated_entrypoints)\n    assert len(unique_eps) == 2\n    assert unique_eps[0].name == \"engine1\"\n    assert unique_eps[1].name == \"engine2\"\n", "def test_remove_duplicates(dummy_duplicated_entrypoints):\n    entrypoints = plugins.remove_duplicates(dummy_duplicated_entrypoints)\n    assert len(entrypoints) == 2\n    names = [ep.name for ep in entrypoints]\n    assert names == [\"engine1\", \"engine2\"]\n", "def test_remove_duplicates(dummy_duplicated_entrypoints):\n    unique_eps = plugins.remove_duplicates(dummy_duplicated_entrypoints)\n    assert len(unique_eps) == 2\n    names = [ep.name for ep in unique_eps]\n    assert names == [\"engine1\", \"engine2\"]\n", "def test_remove_duplicates(dummy_duplicated_entrypoints):\n    unique_eps = plugins.remove_duplicates(dummy_duplicated_entrypoints)\n    assert len(unique_eps) == 2\n    assert unique_eps[0].name == \"engine1\"\n    assert unique_eps[1].name == \"engine2\"\n", "def test_remove_duplicates(dummy_duplicated_entrypoints):\n    unique_eps = plugins.remove_duplicates(dummy_duplicated_entrypoints)\n    assert len(unique_eps) == 2\n    names = [ep.name for ep in unique_eps]\n    assert names == [\"engine1\", \"engine2\"]\n"], "sample_608": ["def test_format_timestamp_with_time():\n    timestamp = pd.Timestamp(\"2022-01-01 12:00:00\")\n    expected = \"2022-01-01T12:00:00\"\n    assert formatting.format_timestamp(timestamp) == expected\n", "def test_inline_variable_array_repr():\n    # Test that inline_variable_array_repr returns a string\n    # that can be used in the __repr__ method of a Variable\n    var = xr.Variable((\"x\",), np.array([1, 2, 3]))\n    assert isinstance(formatting.inline_variable_array_repr(var, 80), str)\n\n    # Test with a longer array that should be truncated\n    var = xr.Variable((\"x\",), np.arange(100))\n    assert len(formatting.inline_variable_array_repr(var, 80)) <= 80\n\n    # Test with an empty array\n    var = xr.Variable((\"x\",), np.array([]))\n    assert formatting.inline_variable_array_repr(var, 80) == \"...\"\n\n    # Test with a numpy array that has more than one dimension\n    var = xr.Variable((\"x\", \"y\"), np.array([[1, 2], [3, 4]]))\n    assert isinstance(formatting.inline_variable_array_repr(var, 80), str)\n", "def test_format_timestamp():\n    # Test that NaT is formatted correctly\n    assert formatting.format_timestamp(pd.NaT) == \"NaT\"\n\n    # Test that a numpy.datetime64 object is formatted correctly\n    dt = np.datetime64(\"2022-01-01\")\n    assert formatting.format_timestamp(dt) == \"2022-01-01\"\n\n    # Test that a pandas.Timestamp object is formatted correctly\n    ts = pd.Timestamp(\"2022-01-01 12:00:00\")\n    assert formatting.format_timestamp(ts) == \"2022-01-01T12:00:00\"\n", "def test_pretty_print():\n    max_width = 20\n    long_str = \"this is a very long string that should be truncated\"\n    expected = \"this is a very lo...\"\n\n    result = formatting.pretty_print(long_str, max_width)\n    assert result == expected\n", "def test_inline_variable_array_repr_datetime64_with_NaT():\n    var = xr.Variable(\n        dims=(\"x\",),\n        data=np.array([np.datetime64(\"2000-01-01\"), np.datetime64(\"NaT\")]),\n    )\n    expected = \"['2000-01-01', NaT]\"\n    assert formatting.inline_variable_array_repr(var, 50) == expected\n"], "sample_609": ["def test_unified_dim_sizes():\n    # Create two DataArray objects with different dimensions\n    da1 = xr.DataArray(np.random.rand(2, 3), dims=(\"x\", \"y\"))\n    da2 = xr.DataArray(np.random.rand(2, 4), dims=(\"x\", \"z\"))\n\n    # Test that unified_dim_sizes raises an error for mismatched dimensions\n    with pytest.raises(ValueError):\n        unified_dim_sizes([da1.variable, da2.variable])\n\n    # Test that unified_dim_sizes works when dimensions are aligned\n    da3 = xr.DataArray(np.random.rand(2, 3), dims=(\"x\", \"y\"))\n    dim_sizes = unified_dim_sizes([da1.variable, da3.variable])\n    assert dim_sizes == {\"x\": 2, \"y\": 3}\n", "def test_apply_ufunc_input_core_dims():\n    # 1d -> 1d\n        return x ** 2\n\n    array = xr.DataArray([1, 2, 3], dims=\"x\")\n    expected = xr.DataArray([1, 4, 9], dims=\"x\")\n    result = apply_ufunc(square, array, input_core_dims=[[\"x\"]])\n    assert_identical(result, expected)\n\n    # 2d -> 2d\n        return x * y\n\n    array = xr.DataArray([[1, 2], [3, 4]], dims=(\"x\", \"y\"))\n    other = xr.DataArray([[5, 6], [7, 8]], dims=(\"x\", \"y\"))\n    expected = xr.DataArray([[5, 12], [21, 32]], dims=(\"x\", \"y\"))\n    result = apply_ufunc(mul, array, other, input_core_dims=[[\"x\", \"y\"], [\"x\", \"y\"]])\n    assert_identical(result, expected)\n\n    # 2d -> 1d with non-core dimension\n    array = xr.DataArray([[1, 2], [3, 4]], dims=(\"x\", \"y\"))\n    expected = xr.DataArray([3, 7], dims=\"x\")\n    result = apply_ufunc(\n        lambda x: x.sum(axis=-1), array, input_core_dims=[[\"x\", \"y\"]]\n    )\n    assert_identical(result, expected)\n", "def test_apply_ufunc_kwargs():\n        return x + y\n\n    array = xr.DataArray([1, 2], dims=\"x\")\n    expected = xr.DataArray([3, 4], dims=\"x\")\n\n    result = apply_ufunc(func, array, array, kwargs={\"extra_arg\": 1})\n    assert_identical(result, expected)\n", "def test_unified_dim_sizes():\n    # Create a few variables with different dimensions and sizes\n    var1 = xr.Variable((\"x\", \"y\"), np.random.randn(3, 4))\n    var2 = xr.Variable((\"x\", \"z\"), np.random.randn(3, 5))\n    var3 = xr.Variable((\"y\", \"z\"), np.random.randn(4, 5))\n\n    # Test that unified_dim_sizes returns the correct size for each dimension\n    dim_sizes = unified_dim_sizes((var1, var2, var3))\n    assert dim_sizes == {\"x\": 3, \"y\": 4, \"z\": 5}\n\n    # Test that it raises an error when variables have conflicting sizes\n    var4 = xr.Variable((\"x\",), np.random.randn(4))  # conflicting size for x\n    with pytest.raises(ValueError):\n        unified_dim_sizes((var1, var2, var4))\n", "def test_apply_ufunc_signature_error():\n    with pytest.raises(TypeError):\n        apply_ufunc(lambda x: x, xr.DataArray([1]), input_core_dims=[[\"x\", \"y\"]])\n"], "sample_610": ["def test_parse_iso8601_like_invalid_string():\n    with pytest.raises(ValueError):\n        parse_iso8601_like(\"invalid_string\")\n", "def test_parse_iso8601_like_raises():\n    with pytest.raises(ValueError):\n        parse_iso8601_like(\"1999-01-99\")\n    with pytest.raises(ValueError):\n        parse_iso8601_like(\"1999-13-01\")\n    with pytest.raises(ValueError):\n        parse_iso8601_like(\"1999-01-01T25\")\n    with pytest.raises(ValueError):\n        parse_iso8601_like(\"1999-01-01T12:60\")\n    with pytest.raises(ValueError):\n        parse_iso8601_like(\"1999-01-01T12:30:60\")\n", "def test_parse_iso8601_with_reso():\n    date_type = cftime.DatetimeNoLeap\n    default = date_type(1, 1, 1)\n\n    for string, expected in ISO8601_LIKE_STRING_TESTS.values():\n        result, resolution = _parse_iso8601_with_reso(date_type, string)\n        assert isinstance(result, dict)\n        assert result == expected\n\n        # Verify that the resolution is correct.\n        if \"second\" in result:\n            assert resolution == \"second\"\n        elif \"minute\" in result:\n            assert resolution == \"minute\"\n        elif \"hour\" in result:\n            assert resolution == \"hour\"\n        elif \"day\" in result:\n            assert resolution == \"day\"\n        elif \"month\" in result:\n            assert resolution == \"month\"\n        else:\n            assert resolution == \"year\"\n\n        # Verify that the default is replaced with the correct values.\n        replaced = default.replace(**result)\n        assert replaced.year == int(expected[\"year\"])\n        assert replaced.month == int(expected.get(\"month\", 1))\n        assert replaced.day == int(expected.get(\"day\", 1))\n        assert replaced.hour == int(expected.get(\"hour\", 0))\n        assert replaced.minute == int(expected.get(\"minute\", 0))\n        assert replaced.second == int(expected.get(\"second\", 0))\n", "def test_parse_iso8601_with_reso():\n    date_type = _all_cftime_date_types[standard_or_gregorian]\n    default = date_type(1, 1, 1)\n\n    expected_result = (\n        default.replace(year=1999, month=1, day=1, hour=12),\n        \"hour\",\n    )\n    assert _parse_iso8601_with_reso(date_type, \"1999-01-01T12\") == expected_result\n\n    expected_result = (\n        default.replace(year=1999, month=1, day=1, hour=12, minute=34),\n        \"minute\",\n    )\n    assert _parse_iso8601_with_reso(date_type, \"1999-01-01T12:34\") == expected_result\n\n    expected_result = (\n        default.replace(\n            year=1999, month=1, day=1, hour=12, minute=34, second=56\n        ),\n        \"second\",\n    )\n    assert _parse_iso8601_with_reso(date_type, \"1999-01-01T12:34:56\") == expected_result\n", "def test_parse_iso8601_like_errors():\n    with pytest.raises(ValueError):\n        parse_iso8601_like(\"abcdefg\")\n\n    with pytest.raises(ValueError):\n        parse_iso8601_like(\"1999-02-30\")\n"], "sample_611": ["def test_to_offset(calendar):\n    offset = to_offset(\"10D\")\n    assert isinstance(offset, Day)\n    assert offset.n == 10\n\n    offset = to_offset(\"2MS\")\n    assert isinstance(offset, MonthBegin)\n    assert offset.n == 2\n\n    offset = to_offset(\"3M\")\n    assert isinstance(offset, MonthEnd)\n    assert offset.n == 3\n\n    offset = to_offset(\"A-JUL\")\n    assert isinstance(offset, YearEnd)\n    assert offset.month == 7\n\n    offset = to_offset(\"AS-OCT\")\n    assert isinstance(offset, YearBegin)\n    assert offset.month == 10\n\n    offset = to_offset(\"QS-DEC\")\n    assert isinstance(offset, QuarterBegin)\n    assert offset.month == 12\n\n    offset = to_offset(\"Q-FEB\")\n    assert isinstance(offset, QuarterEnd)\n    assert offset.month == 2\n", "def test_to_offset(freq_str, expected_offset):\n    offset = to_offset(freq_str)\n    assert isinstance(offset, type(expected_offset))\n    assert offset.n == expected_offset.n\n    if hasattr(offset, \"month\"):\n        assert offset.month == expected_offset.month\n", "def test_rule_code(offset, freq_str):\n    assert offset.rule_code() == freq_str\n", "def test_to_offset(offset, freq_str):\n    assert to_offset(freq_str) == offset\n", "def test_to_offset(offset, freq_str):\n    assert to_offset(freq_str) == offset\n"], "sample_612": ["def test_groupby_concat_dim_order(array):\n    # GH3485\n    array = array.stack(s=[\"x\", \"y\"])\n    groupby = array.groupby(\"z\")\n    expected = array.mean(dim=\"s\")\n    result = groupby.map(lambda x: x.mean(dim=\"s\"))\n    assert_identical(result, expected)\n", "def test_groupby_reduce_dim_order(dataset):\n    ds = dataset\n    grouped = ds.stack(space=(\"x\", \"y\")).groupby(\"space\")\n    reduced = grouped.mean()\n    assert_identical(reduced, ds.mean((\"x\", \"y\")))\n", "def test_groupby_reduce_dim_order(dataset):\n    ds = dataset\n    expected = ds.mean(\"y\")\n    actual = ds.groupby(\"x\").reduce(np.mean, dim=\"y\")\n    assert_identical(actual, expected)\n", "def test_groupby_map_no_restore_dim_order(array):\n    # regression test for GH4324\n        return x.mean(dim=[\"x\", \"y\"])\n\n    expected = func(array)\n    result = array.groupby(\"z\").map(func)\n    assert_identical(result, expected)\n", "def test_groupby_consolidate_slices():\n    slices = [slice(0, 1), slice(1, 2), slice(2, 3)]\n    expected = [slice(0, 3)]\n    assert _consolidate_slices(slices) == expected\n\n    slices = [slice(0, 1), slice(2, 3), slice(1, 2)]\n    expected = [slice(0, 1), slice(1, 3)]\n    assert _consolidate_slices(slices) == expected\n\n    slices = [slice(0, 1, 2), slice(1, 2, 2), slice(2, 3, 2)]\n    with pytest.raises(ValueError):\n        _consolidate_slices(slices)\n"], "sample_613": ["def test_groupby_map_func(dataset):\n    # Test that using a function with map returns the correct result\n        return x ** 2\n\n    expected = (dataset[\"foo\"] ** 2).mean(\"x\")\n    result = dataset.groupby(\"y\").map(square).mean(\"x\")\n    assert_identical(result, expected)\n", "def test_groupby_concat_dim_order(dataset):\n    # Test that groupby operations with a concat_dim return an array with the correct order of dimensions\n    expected_dim_order = (\"x\", \"y\", \"z\")\n    result = dataset.groupby(\"x\").map(lambda x: x)\n    assert result.dims == expected_dim_order\n", "def test_groupby_consolidate_slices():\n    slices = [slice(0, 5), slice(5, 10), slice(10, 15)]\n    expected = [slice(0, 15)]\n    assert _consolidate_slices(slices) == expected\n\n    slices = [slice(0, 5), slice(6, 10), slice(11, 15)]\n    expected = slices\n    assert _consolidate_slices(slices) == expected\n\n    slices = [slice(0, 5, 2), slice(6, 10, 3), slice(11, 15, 4)]\n    with pytest.raises(ValueError):\n        _consolidate_slices(slices)\n", "def test_groupby_consolidate_slices():\n    slices = [slice(0, 3), slice(3, 5), slice(5, 7)]\n    consolidated = _consolidate_slices(slices)\n    expected = [slice(0, 7)]\n    assert consolidated == expected\n\n    slices = [slice(0, 3), slice(4, 5), slice(5, 7)]\n    consolidated = _consolidate_slices(slices)\n    expected = [slice(0, 3), slice(4, 7)]\n    assert consolidated == expected\n\n    slices = [slice(0, 3), slice(3, 5), slice(6, 7)]\n    consolidated = _consolidate_slices(slices)\n    expected = [slice(0, 5), slice(6, 7)]\n    assert consolidated == expected\n", "def test_groupby_bins(dataset):\n    bins = [1, 3, 5]\n    grouped = dataset.groupby_bins(\"y\", bins=bins)\n    expected_coords = pd.cut(dataset.coords[\"y\"].values, bins=bins)\n    assert_array_equal(grouped._group_indices, [0, 0, 1, 1])\n    assert_identical(grouped._unique_coord, Variable(expected_coords))\n"], "sample_614": ["def test_format_timestamp():\n    # Test a simple datetime object\n    dt = pd.Timestamp(\"2022-01-01 12:00:00\")\n    expected = \"2022-01-01T12:00:00\"\n    assert formatting.format_timestamp(dt) == expected\n\n    # Test a NaT object\n    nat = pd.NaT\n    expected = \"NaT\"\n    assert formatting.format_timestamp(nat) == expected\n\n    # Test an OutOfBoundsDatetime object\n    oob_dt = pd.Timestamp.max + pd.Timedelta(days=1)\n    expected = str(oob_dt)\n    assert formatting.format_timestamp(oob_dt) == expected\n", "def test_inline_dask_repr():\n    # Create a simple dask array\n    da = xr.DataArray([1, 2, 3], chunks=2)\n\n    # Test inline dask repr with default settings\n    expected = \"dask.array<chunksize=(2,), meta=np.ndarray>\"\n    assert formatting.inline_dask_repr(da.data) == expected\n\n    # Test inline dask repr with custom meta\n    class CustomMeta:\n        pass\n\n    da.data._meta = CustomMeta()\n    expected = \"dask.array<chunksize=(2,), meta=CustomMeta>\"\n    assert formatting.inline_dask_repr(da.data) == expected\n", "def test_inline_dask_repr():\n    import dask.array as da\n\n    # Test that inline dask repr does not error and returns expected string\n    array = da.random.random((10, 10), chunks=(5, 5))\n    assert formatting.inline_dask_repr(array) == \"dask.array<chunksize=(5, 5), meta='ndarray'>\"\n", "def test_pretty_print():\n    # Test with a short string\n    assert formatting.pretty_print(\"test\", 10) == \"test\" + \" \" * 6\n    \n    # Test with a longer string that needs truncation\n    long_string = \"this is a very long string\"\n    expected = long_string[:7] + \"...\"\n    assert formatting.pretty_print(long_string, 10) == expected\n    \n    # Test with a string that exactly fits the available space\n    assert formatting.pretty_print(\"test\" * 2, 8) == \"testtest\"\n", "def test_pretty_print():\n    assert formatting.pretty_print(\"test\", 10) == \"test\" + \" \" * 6\n    assert formatting.pretty_print(\"longer string than width\", 10) == \"longer...\"\n"], "sample_615": ["def test_broadcast_compat_data():\n    data = np.random.randn(2, 3)\n    dims = (\"x\", \"y\")\n\n    # simplest case: no change needed\n    var = xr.Variable(dims, data)\n    assert_identical(\n        broadcast_compat_data(var, dims, dims), var.data, strict=False\n    )\n\n    # add one dimension\n    new_dims = (\"z\",) + dims\n    expected = data[np.newaxis]\n    assert_array_equal(broadcast_compat_data(var, dims, new_dims), expected)\n\n    # remove one dimension\n    new_dims = dims[1:]\n    with pytest.raises(ValueError):\n        broadcast_compat_data(var, dims, new_dims)\n\n    # swap dimensions\n    new_dims = dims[::-1]\n    expected = data.transpose()\n    assert_array_equal(broadcast_compat_data(var, dims, new_dims), expected)\n\n    # same as first example, but with DataArray\n    da = xr.DataArray(data, dims=dims)\n    assert_identical(\n        broadcast_compat_data(da, dims, dims), var.data, strict=False\n    )\n", "def test_apply_ufunc_signature_validation():\n    with pytest.raises(ValueError):\n        apply_ufunc(\n            np.mean, xr.DataArray([1]), input_core_dims=[[\"x\"]], output_core_dims=[]\n        )\n\n    with pytest.raises(ValueError):\n        apply_ufunc(\n            np.mean,\n            xr.DataArray([1]),\n            input_core_dims=[(\"x\",)],\n            output_core_dims=[(\"y\",)],\n        )\n\n    with pytest.raises(ValueError):\n        apply_ufunc(\n            np.mean,\n            xr.DataArray([1]),\n            input_core_dims=[(\"x\",)],\n            output_core_dims=[(\"y\", \"z\")],\n        )\n", "def test_ordered_set_union():\n    list1 = [\"a\", \"b\", \"c\"]\n    list2 = [\"c\", \"d\", \"e\"]\n    result = ordered_set_union([list1, list2])\n    expected = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n    assert list(result) == expected\n", "def test_apply_ufunc_with_exclude_dims():\n    # Test apply_ufunc with exclude_dims\n    da = xr.DataArray(np.random.randn(3, 4), dims=[\"x\", \"y\"])\n    result = apply_ufunc(lambda x: x.mean(axis=-1), da, exclude_dims={\"y\"})\n    assert result.dims == (\"x\",)\n", "def test_broadcast_compat_data():\n    # Test broadcast_compat_data function with different input scenarios\n    data = xr.DataArray(np.random.rand(2, 3), dims=[\"x\", \"y\"])\n    var = xr.Variable(data.dims, data.data)\n\n    # Test when broadcast_dims is empty and core_dims are the same as var.dims\n    result = broadcast_compat_data(var, (), (\"x\", \"y\"))\n    assert_identical(result, data.data)\n\n    # Test when broadcast_dims is not empty and core_dims are a subset of var.dims\n    result = broadcast_compat_data(var, (\"x\",), (\"y\",))\n    assert_identical(result, data.data)\n\n    # Test when broadcast_dims is not empty and core_dims are not a subset of var.dims\n    with pytest.raises(ValueError):\n        broadcast_compat_data(var, (\"z\",), (\"y\",))\n\n    # Test when broadcast_dims is empty and core_dims are not a subset of var.dims\n    with pytest.raises(ValueError):\n        broadcast_compat_data(var, (), (\"z\",))\n"], "sample_616": ["def test_apply_ufunc_with_non_xarray_inputs():\n    # Test apply_ufunc with non-xarray inputs\n        return a + b\n\n    result = apply_ufunc(add, 1, 2)\n    assert result == 3\n\n    result = apply_ufunc(add, np.array([1, 2]), np.array([3, 4]))\n    assert_array_equal(result, np.array([4, 6]))\n", "def test_apply_ufunc_vectorize():\n    # Test that apply_ufunc with vectorize=True gives the same result as applying\n    # the ufunc directly.\n    array = xr.DataArray([1, 2, 3], dims=\"x\")\n    expected = np.square(array)\n    result = apply_ufunc(np.square, array, vectorize=True)\n    assert_identical(result, expected)\n", "def test_broadcast_compat_data():\n    data = np.arange(3 * 4 * 5).reshape(3, 4, 5)\n    var = xr.Variable((\"x\", \"y\", \"z\"), data)\n\n    # simplest case: same dimensions, no change needed\n    broadcast_dims = (\"x\", \"y\", \"z\")\n    core_dims = ()\n    result = broadcast_compat_data(var, broadcast_dims, core_dims)\n    assert_array_equal(result, data)\n\n    # swap dimensions\n    broadcast_dims = (\"y\", \"x\", \"z\")\n    core_dims = ()\n    result = broadcast_compat_data(var, broadcast_dims, core_dims)\n    assert_array_equal(result, var.transpose(\"y\", \"x\", \"z\").data)\n\n    # add broadcast dimension\n    broadcast_dims = (\"w\", \"x\", \"y\", \"z\")\n    core_dims = (\"x\", \"y\", \"z\")\n    result = broadcast_compat_data(var, broadcast_dims, core_dims)\n    assert_array_equal(result, var.data[np.newaxis])\n", "def test_apply_ufunc_signature_error():\n        return x\n\n    with pytest.raises(TypeError):\n        apply_ufunc(\n            func,\n            xr.DataArray([1, 2]),\n            input_core_dims=[(\"x\", \"y\")],\n            output_core_dims=[(\"x\", \"y\")],\n        )\n", "def test_broadcast_compat_data_3d_to_4d():\n    data = np.random.rand(2, 3, 4)\n    dims = (\"x\", \"y\", \"z\")\n    broadcast_dims = (\"w\", \"x\", \"y\")\n    core_dims = (\"z\",)\n\n    expected = data[np.newaxis, ...]\n\n    result = broadcast_compat_data(\n        xr.Variable(dims, data), broadcast_dims, core_dims\n    )\n    assert_array_equal(result, expected)\n"], "sample_618": ["def test_apply_ufunc_dataarray_groupby():\n    array = xr.DataArray([1, 2, 3], coords=[(\"x\", [0.1, 0.2, 0.3])])\n    groupby = array.groupby(\"x\")\n    result = apply_ufunc(lambda x: x.sum(), groupby)\n    assert_identical(result, array)\n", "def test_unified_dim_sizes():\n    # Create some test variables with different dimensions\n    v1 = xr.Variable((\"x\", \"y\"), np.random.rand(2, 3))\n    v2 = xr.Variable((\"x\",), np.random.rand(2))\n    v3 = xr.Variable((\"y\", \"z\"), np.random.rand(3, 4))\n\n    # Test that the function returns the correct unified dimension sizes\n    dim_sizes = unified_dim_sizes([v1, v2, v3])\n    assert dim_sizes == {\"x\": 2, \"y\": 3, \"z\": 4}\n\n    # Test that the function raises an error if there are conflicting sizes\n    v4 = xr.Variable((\"x\",), np.random.rand(5))\n    with pytest.raises(ValueError):\n        unified_dim_sizes([v1, v4])\n", "def test_apply_ufunc_signature():\n    # Test _UFuncSignature creation and equality\n    sig1 = _UFuncSignature([(\"a\", \"b\")], [(\"c\", \"d\")])\n    sig2 = _UFuncSignature([(\"a\", \"b\")], [(\"c\", \"d\")])\n    sig3 = _UFuncSignature([(\"a\",)], [(\"c\", \"d\")])\n\n    assert sig1 == sig2\n    assert sig1 != sig3\n\n    # Test None in input_core_dims\n    sig4 = _UFuncSignature([(None, \"b\")], [(\"c\", \"d\")])\n    assert sig4.input_core_dims == [(None, \"b\")]\n    assert sig4.all_input_core_dims == {\"b\"}\n", "def test_apply_ufunc_dtype():\n    # regression test for GH2117\n    da = xr.DataArray(np.array([1, 2, 3], dtype=np.float32))\n    result = apply_ufunc(lambda x: x, da)\n    assert result.dtype == np.float32\n", "def test_unified_dim_sizes():\n    # Test case 1: variables with the same dimensions\n    var1 = xr.Variable((\"x\", \"y\"), np.array([[1, 2], [3, 4]]))\n    var2 = xr.Variable((\"x\", \"y\"), np.array([[5, 6], [7, 8]]))\n    dim_sizes = unified_dim_sizes([var1, var2])\n    assert dim_sizes == {\"x\": 2, \"y\": 2}\n\n    # Test case 2: variables with different dimensions\n    var3 = xr.Variable((\"x\", \"z\"), np.array([[1, 2], [3, 4]]))\n    var4 = xr.Variable((\"y\", \"z\"), np.array([[5, 6], [7, 8]]))\n    dim_sizes = unified_dim_sizes([var3, var4])\n    assert dim_sizes == {\"z\": 2}\n\n    # Test case 3: variables with duplicate dimensions\n    var5 = xr.Variable((\"x\", \"x\"), np.array([[1, 2], [3, 4]]))\n    with pytest.raises(ValueError):\n        unified_dim_sizes([var5])\n\n    # Test case 4: exclude_dims argument\n    var6 = xr.Variable((\"x\", \"y\"), np.array([[1, 2], [3, 4]]))\n    var7 = xr.Variable((\"x\", \"z\"), np.array([[5, 6], [7, 8]]))\n    dim_sizes = unified_dim_sizes([var6, var7], exclude_dims={\"x\"})\n    assert dim_sizes == {}\n"], "sample_619": ["def test_encode_cf_datetime_cftime():\n    import cftime\n\n    # Test encoding with non-standard calendar\n    dates = cftime_range(\"2000\", periods=5, calendar=\"noleap\")\n    encoded, units, calendar = encode_cf_datetime(dates)\n    assert calendar == \"noleap\"\n\n    # Test encoding with standard calendar\n    dates = pd.date_range(\"2000\", periods=5)\n    encoded, units, calendar = encode_cf_datetime(dates)\n    assert calendar == \"proleptic_gregorian\"\n", "def test_encode_cf_datetime_cftime_dtype():\n    import cftime\n\n    date_type = cftime.DatetimeJulian\n    dates = np.array([date_type(1, 1, 1), date_type(2, 2, 2)])\n    encoded, units, calendar = encode_cf_datetime(dates)\n\n    assert calendar == \"julian\"\n    assert units == \"days since 0001-01-01 00:00:00\"\n    assert_array_equal(encoded, [0, 397])\n", "def test_encode_cf_datetime_overflow():\n    import cftime\n\n    # Create a datetime object that will overflow when encoded to a float64\n    dt = cftime.DatetimeGregorian(59632, 12, 31)\n\n    # Try encoding it with encode_cf_datetime\n    num, units, calendar = coding.times.encode_cf_datetime(dt)\n\n    # Check that the resulting number is not finite (i.e., it overflowed)\n    assert not np.isfinite(num)\n\n    # Check that the units and calendar are correct\n    assert units == \"days since 1970-01-01\"\n    assert calendar == \"gregorian\"\n\n    # Try encoding it with _encode_datetime_with_cftime\n    num_cftime = coding.times._encode_datetime_with_cftime(dt, units, calendar)\n\n    # Check that the resulting number is not finite (i.e., it overflowed)\n    assert not np.isfinite(num_cftime)\n", "def test_encode_cf_datetime_overflow():\n    # GH 2272\n    dates = pd.date_range(\"1678-01-01\", periods=10)\n    units = \"days since 1677-12-31\"\n    encoded, _, _ = encode_cf_datetime(dates, units)\n    assert_array_equal(encoded, np.arange(1, 11))\n", "def test_encode_cf_datetime_overflow_error():\n    # Test encoding datetime values that would cause an OverflowError when\n    # trying to represent as a timedelta64[ns] array.\n    dates = pd.date_range(start=\"1800-01-01\", periods=100, freq=\"D\")\n    units = \"days since 1900-01-01\"\n    calendar = \"standard\"\n\n    with pytest.raises(OverflowError):\n        encode_cf_datetime(dates, units=units, calendar=calendar)\n"], "sample_620": ["def test_concat_dim_is_coordinate():\n    # Test that when the concat dimension is an existing coordinate,\n    # the resulting coordinate is correct.\n    da1 = DataArray([1, 2], coords={\"x\": [10, 20]}, dims=[\"x\"])\n    da2 = DataArray([3, 4], coords={\"x\": [30, 40]}, dims=[\"x\"])\n    result = concat((da1, da2), dim=\"x\")\n    expected = DataArray(\n        [1, 2, 3, 4], coords={\"x\": [10, 20, 30, 40]}, dims=[\"x\"]\n    )\n    assert_identical(result, expected)\n", "def test_concat_dataset_dim_order():\n    # Test that concat preserves dimension order\n    ds1 = Dataset()\n    ds1[\"a\"] = (\"x\", [1])\n    ds1[\"b\"] = (\"y\", [2])\n\n    ds2 = Dataset()\n    ds2[\"a\"] = (\"x\", [3])\n    ds2[\"b\"] = (\"y\", [4])\n\n    result = concat([ds1, ds2], \"new_dim\")\n    assert result.dims == {\"new_dim\": 2, \"x\": 1, \"y\": 1}\n", "def test_concat_dataarray_coords():\n    # Test that DataArray.coords are updated correctly during concat\n    da1 = DataArray([1, 2], coords={\"x\": [0, 1]}, dims=\"x\")\n    da2 = DataArray([3, 4], coords={\"x\": [2, 3]}, dims=\"x\")\n\n    # Check that coords are updated when concatenating along an existing dimension\n    result = concat((da1, da2), dim=\"x\")\n    expected = DataArray([1, 2, 3, 4], coords={\"x\": [0, 1, 2, 3]}, dims=\"x\")\n    assert_identical(result, expected)\n\n    # Check that coords are updated when concatenating along a new dimension\n    da1 = DataArray([1, 2], dims=\"x\")\n    da2 = DataArray([3, 4], dims=\"x\")\n    result = concat((da1, da2), dim=\"y\")\n    expected = DataArray([[1, 2], [3, 4]], coords={\"y\": [0, 1]}, dims=(\"y\", \"x\"))\n    assert_identical(result, expected)\n", "def test_concat_join_exact():\n    # Test that concat fails when indexes are not equal and join=\"exact\"\n    data = create_test_data()\n    data1 = data.copy(deep=True)\n    data2 = data.copy(deep=True)\n    data2[\"dim2\"] = (\"dim2\", [3, 4, 5])\n\n    with pytest.raises(ValueError):\n        concat((data1, data2), dim=\"new_dim\", join=\"exact\")\n", "def test_concat_dataset_dim_coords():\n    # Test concatenating datasets with different dimension coordinates.\n    ds1 = Dataset(\n        coords={\"x\": (\"x\", [0, 1])}, data_vars={\"a\": ((\"x\"), [0, 1])}\n    )\n    ds2 = Dataset(\n        coords={\"x\": (\"x\", [2, 3])}, data_vars={\"a\": ((\"x\"), [2, 3])}\n    )\n    result = concat([ds1, ds2], dim=\"x\")\n    expected = Dataset(\n        coords={\"x\": (\"x\", [0, 1, 2, 3])}, data_vars={\"a\": ((\"x\"), [0, 1, 2, 3])}\n    )\n    assert_identical(result, expected)\n"], "sample_621": ["def test_create_variables_multiindex() -> None:\n    index = pd.MultiIndex.from_product([[\"a\", \"b\"], [1, 2]], names=(\"level1\", \"level2\"))\n    dim = \"dim\"\n    level_coords_dtype = {\"level1\": object, \"level2\": int}\n\n    multi_index = PandasMultiIndex(index, dim, level_coords_dtype=level_coords_dtype)\n\n    variables: dict[Any, Variable] = {}\n    new_variables = multi_index.create_variables(variables)\n    assert set(new_variables) == {dim, \"level1\", \"level2\"}\n\n    for name in new_variables:\n        var = new_variables[name]\n        if name == dim:\n            assert var.dims == (dim,)\n        else:\n            assert var.dtype == level_coords_dtype[name]\n", "def test_create_variables_multi_index_levels():\n    # Create a multi-index with two levels\n    index = pd.MultiIndex.from_product([[\"a\", \"b\"], [1, 2]], names=[\"level1\", \"level2\"])\n    pmindex = PandasMultiIndex(index, \"dim\")\n\n    # Create variables for the multi-index levels\n    variables = pmindex.create_variables()\n\n    # Check that the variables are created correctly\n    assert len(variables) == 3\n    assert \"dim\" in variables\n    assert \"level1\" in variables\n    assert \"level2\" in variables\n\n    # Check that the variables have the correct dimensions and data\n    assert variables[\"dim\"].dims == (\"dim\",)\n    assert variables[\"level1\"].dims == (\"dim\",)\n    assert variables[\"level2\"].dims == (\"dim\",)\n\n    assert np.array_equal(variables[\"dim\"].data, np.arange(4))\n    assert np.array_equal(variables[\"level1\"].data, [\"a\", \"a\", \"b\", \"b\"])\n    assert np.array_equal(variables[\"level2\"].data, [1, 2, 1, 2])\n", "def test_pandas_index_equals():\n    # create two identical PandasIndex objects\n    index1 = PandasIndex(pd.Index([1, 2, 3]), \"x\")\n    index2 = PandasIndex(pd.Index([1, 2, 3]), \"x\")\n\n    # check if they are equal\n    assert index1.equals(index2)\n\n    # create a different PandasIndex object\n    index3 = PandasIndex(pd.Index([4, 5, 6]), \"x\")\n    assert not index1.equals(index3)\n\n    # create a PandasIndex object with a different dimension\n    index4 = PandasIndex(pd.Index([1, 2, 3]), \"y\")\n    assert not index1.equals(index4)\n", "def test_asarray_tuplesafe() -> None:\n    res = _asarray_tuplesafe((1, 2, 3))\n    assert isinstance(res, np.ndarray)\n    assert res.shape == (3,)\n    assert res.dtype.kind == \"O\"\n\n    res = _asarray_tuplesafe([(1, 2), (3, 4)])\n    assert isinstance(res, np.ndarray)\n    assert res.shape == (2,)\n    assert res.dtype.kind == \"O\"\n", "def test_indexes_equals():\n    # Create indexes for testing\n    index1 = PandasIndex(pd.Index([1, 2, 3]), \"x\")\n    index2 = PandasIndex(pd.Index([1, 2, 3]), \"x\")\n    index3 = PandasIndex(pd.Index([4, 5, 6]), \"y\")\n\n    var1 = IndexVariable(\"x\", [1, 2, 3])\n    var2 = IndexVariable(\"x\", [1, 2, 3])\n    var3 = IndexVariable(\"y\", [4, 5, 6])\n\n    # Test equals method for identical indexes\n    assert indexes_equal(index1, index2, var1, var2)\n\n    # Test equals method for non-identical indexes\n    assert not indexes_equal(index1, index3, var1, var3)\n\n    # Test equals method with cache\n    cache: dict[tuple[int, int], bool | None] = {}\n    assert indexes_equal(index1, index2, var1, var2, cache)\n    assert cache[(id(index1), id(index2))] is True\n\n    # Test equals method with missing implementation\n    class CustomIndex(Index):\n            self.array = array\n            self.dim = dim\n\n    custom_index1 = CustomIndex([1, 2, 3], \"x\")\n    custom_index2 = CustomIndex([1, 2, 3], \"x\")\n\n    assert indexes_equal(custom_index1, custom_index2, var1, var2)\n"], "sample_622": ["def test_encode_cf_variable_unsigned_int():\n    var = Variable([1, 2, 3], dtype=\"uint32\")\n    encoded_var = conventions.encode_cf_variable(var)\n    assert_array_equal(encoded_var.data, np.array([1, 2, 3], dtype=np.int32))\n    assert encoded_var.attrs == {\"_Unsigned\": \"true\"}\n", "def test_decode_cf_variables_nonstandard_calendar():\n    original = Dataset()\n    times = cftime_range(\n        \"2000-01-01\", periods=20, freq=\"D\", calendar=\"noleap\", start_cftime=True\n    )\n    original[\"time\"] = (\"time\", times)\n    original.time.attrs[\"units\"] = \"days since 0001-01-01\"\n    original.time.attrs[\"calendar\"] = \"noleap\"\n\n    encoded_times = conventions.encode_cf_variable(original.time)\n\n    decoded_times = conventions.decode_cf_variable(\n        encoded_times.name,\n        encoded_times,\n        decode_times=True,\n        use_cftime=True,\n    )\n\n    assert_identical(decoded_times, original.time)\n", "    def decode_coords(self, request):\n        return request.param\n", "def test_decode_cf_variables_bounds_encoding():\n    original = Dataset()\n    original.coords[\"time\"] = (\"time\", pd.date_range(\"2000-01-01\", periods=3))\n    original.coords[\"time_bnds\"] = (\n        (\"time\", \"bnds\"),\n        np.array(\n            [\n                [np.datetime64(\"1999-12-31\"), np.datetime64(\"2000-01-01\")],\n                [np.datetime64(\"2000-01-01\"), np.datetime64(\"2000-01-02\")],\n                [np.datetime64(\"2000-01-02\"), np.datetime64(\"2000-01-03\")],\n            ]\n        ),\n    )\n    original.coords[\"time\"].attrs[\"bounds\"] = \"time_bnds\"\n\n    expected = original.copy(deep=True)\n\n    encoded = conventions.encode_dataset_coordinates(original)\n    decoded_vars, decoded_attrs, _ = conventions.decode_cf_variables(\n        encoded[0], encoded[1]\n    )\n\n    result = Dataset(decoded_vars, attrs=decoded_attrs)\n\n    assert_identical(result, expected)\n", "    def test_to_native_endianness(self):\n        arr = np.arange(5, dtype=\">i2\")\n        wrapped_arr = conventions.NativeEndiannessArray(arr)\n        assert wrapped_arr.dtype == \"int16\"\n"], "sample_623": ["def test_get_default_engine_remote_uri():\n    remote_uri = \"http://example.com/data.nc\"\n    engine = _get_default_engine(remote_uri, allow_remote=True)\n    assert engine in [\"netcdf4\", \"pydap\"]\n", "def test_open_dataset_invalid_netcdf_file() -> None:\n    # Create a temporary invalid netCDF file\n    with open(\"invalid.nc\", \"wb\") as f:\n        f.write(b\"Invalid netCDF file\")\n\n    # Try to open the invalid netCDF file\n    with pytest.raises(ValueError):\n        xr.open_dataset(\"invalid.nc\")\n\n    # Remove the temporary file\n    import os\n\n    os.remove(\"invalid.nc\")\n", "def test_open_dataset_invalid_netcdf():\n    with pytest.raises(ValueError) as excinfo:\n        xr.open_dataset(\"nonexistent.nc\", engine=\"invalid\")\n    assert \"unrecognized engine\" in str(excinfo.value)\n\n    with pytest.raises(ValueError) as excinfo:\n        xr.open_dataset(\"nonexistent.nc\", engine=\"netcdf4\", format=\"invalid\")\n    assert \"invalid format\" in str(excinfo.value)\n", "def test_to_netcdf_no_engine():\n    # Test that _get_default_engine() is called when no engine is provided.\n    with pytest.mock.patch(\"xarray.backends.api._get_default_engine\") as mock:\n        mock.return_value = \"netcdf4\"\n        ds = xr.Dataset()\n        ds.to_netcdf(\"test.nc\")\n        mock.assert_called_once_with(\"test.nc\", allow_remote=False)\n", "def test_to_netcdf_invalid_netcdf():\n    # regression test for GH1401\n    with pytest.raises(ValueError):\n        xr.Dataset().to_netcdf(engine=\"h5netcdf\", invalid_netcdf=True)\n"], "sample_624": ["def test_pretty_print():\n    max_name_length = 10\n    col_width = formatting._calculate_col_width([\"short\", \"very_long_variable_name\"])\n\n    assert col_width == max_name_length + 6\n\n    short_repr = formatting.pretty_print(\"short\", col_width)\n    long_repr = formatting.pretty_print(\"very_long_variable_name\", col_width)\n\n    expected_short_repr = \"short\" + \" \" * (col_width - len(\"short\"))\n    expected_long_repr = \"very_long_variable_name\"\n\n    assert short_repr == expected_short_repr\n    assert long_repr == expected_long_repr\n", "def test_format_timestamp():\n    # Test that format_timestamp returns a string in the correct format\n    timestamp = pd.Timestamp(\"2022-01-01 12:00:00\")\n    expected = \"2022-01-01T12:00:00\"\n    assert formatting.format_timestamp(timestamp) == expected\n\n    # Test that format_timestamp handles NaT correctly\n    nat = pd.NaT\n    expected = \"NaT\"\n    assert formatting.format_timestamp(nat) == expected\n\n    # Test that format_timestamp handles out-of-bounds dates correctly\n    oob_date = pd.Timestamp(\"1000-01-01 12:00:00\")\n    expected = \"1000-01-01 12:00:00\"\n    assert formatting.format_timestamp(oob_date) == expected\n", "def test_format_timestamp_out_of_bounds_datetime():\n    # Test that formatting an out-of-bounds datetime results in the original string.\n    assert formatting.format_timestamp(\"1677-01-01T00:00:00\") == \"1677-01-01T00:00:00\"\n", "def test_inline_index_repr():\n    index = pd.Index([1, 2, 3], name=\"foo\")\n    expected = \"Index('foo', [1, 2, 3])\"\n    assert formatting.inline_index_repr(index) == expected\n\n    index = xr.IndexVariable(\"x\", [1, 2, 3])\n    expected = \"<xarray.IndexVariable 'x' (x: 3)>\"\n    assert formatting.inline_index_repr(index) == expected\n", "def test_format_timestamp():\n    # Test formatting of numpy.datetime64 and pandas.Timestamp objects\n    timestamp = np.datetime64(\"2022-01-01T12:00:00\")\n    assert formatting.format_timestamp(timestamp) == \"2022-01-01T12:00:00\"\n\n    timestamp = pd.Timestamp(\"2022-01-01T12:00:00\")\n    assert formatting.format_timestamp(timestamp) == \"2022-01-01T12:00:00\"\n\n    # Test formatting of datetime objects outside the valid range for Timestamp\n    with pytest.warns(UserWarning):\n        dt = datetime(1600, 1, 1)\n        assert formatting.format_timestamp(dt) == str(dt)\n\n    # Test NaT\n    nat = pd.NaT\n    assert formatting.format_timestamp(nat) == \"NaT\"\n"], "sample_625": ["def test_apply_ufunc_skipna():\n    array = xr.DataArray([1, 2, np.nan, 4], dims=\"x\")\n    expected = xr.DataArray([10, 20, np.nan, 40], dims=\"x\")\n    result = apply_ufunc(lambda x: x * 10, array, skipna=True)\n    assert_identical(result, expected)\n", "def test_apply_ufunc():\n    # simple example\n    array = xr.DataArray([1, 2, 3], dims=\"x\")\n    actual = apply_ufunc(np.mean, array)\n    expected = np.mean(array.data)\n    assert_identical(actual, expected)\n\n    # more complex example with multiple args\n        return a + b\n\n    array_a = xr.DataArray([1, 2, 3], dims=\"x\")\n    array_b = xr.DataArray([4, 5, 6], dims=\"x\")\n    actual = apply_ufunc(add, array_a, array_b)\n    expected = array_a + array_b\n    assert_identical(actual, expected)\n", "def test_broadcast_compat_data():\n    data = np.arange(3 * 4).reshape(3, 4)\n    broadcast_dims = (\"x\", \"y\")\n    core_dims = (\"z\",)\n\n    with pytest.raises(ValueError):\n        broadcast_compat_data(xr.Variable((\"a\", \"b\"), data), broadcast_dims, core_dims)\n\n    with pytest.raises(ValueError):\n        broadcast_compat_data(xr.Variable((\"a\", \"b\", \"c\"), data), broadcast_dims, core_dims)\n\n    var = xr.Variable((\"x\", \"y\", \"z\"), np.arange(3 * 4 * 5).reshape(3, 4, 5))\n    result = broadcast_compat_data(var, broadcast_dims, core_dims)\n    assert_array_equal(result.shape, (3, 4, 5))\n\n    # Test with transposed data\n    var = xr.Variable((\"x\", \"z\", \"y\"), np.arange(3 * 4 * 5).reshape(3, 5, 4))\n    result = broadcast_compat_data(var, broadcast_dims, core_dims)\n    assert_array_equal(result.shape, (3, 4, 5))\n", "def test_apply_ufunc_dtype():\n    da = xr.DataArray(np.array([1, 2, 3], dtype=\"int64\"))\n    result = apply_ufunc(lambda x: x, da, dask=\"forbidden\", output_dtypes=[\"float64\"])\n    assert result.dtype == \"float64\"\n", "def test_unified_dim_sizes() -> None:\n    a = xr.Variable((\"x\", \"y\"), np.random.randn(2, 3))\n    b = xr.Variable((\"x\",), np.random.randn(2))\n    c = xr.Variable((\"y\",), np.random.randn(3))\n\n    dims = unified_dim_sizes([a, b, c])\n    assert dims == {\"x\": 2, \"y\": 3}\n\n    with pytest.raises(ValueError):\n        unified_dim_sizes([a, xr.Variable((\"x\",), np.random.randn(3))])\n"], "sample_626": ["def test_explicit_indexing_adapter() -> None:\n    array = np.arange(10)\n    adapter = xr.core.indexing.ExplicitIndexingAdapter(array)\n\n    # BasicIndexer\n    key = xr.core.indexing.BasicIndexer((slice(1, 3),))\n    assert_array_equal(adapter[key], array[1:3])\n\n    # OuterIndexer\n    key = xr.core.indexing.OuterIndexer((np.array([1, 2]),))\n    assert_array_equal(adapter[key], array[[1, 2]])\n\n    # VectorizedIndexer\n    key = xr.core.indexing.VectorizedIndexer((np.array([1, 2]),))\n    assert_array_equal(adapter[key], array[[1, 2]])\n", "def test_as_indexable_datetime64_array():\n    array = np.array([np.datetime64(\"2000-01-01\")], dtype=\"datetime64[ns]\")\n    result = as_indexable(array)\n    assert isinstance(result, NumpyIndexingAdapter)\n    assert_array_equal(result, array)\n", "def test_as_indexable() -> None:\n    array = np.array([1, 2, 3])\n    actual = as_indexable(array)\n    assert isinstance(actual, NumpyIndexingAdapter)\n\n    pandas_index = pd.Index([1, 2, 3])\n    actual = as_indexable(pandas_index)\n    assert isinstance(actual, PandasIndexingAdapter)\n\n    with pytest.raises(TypeError):\n        as_indexable(\"not array like\")  # type: ignore\n", "def test_safe_cast_to_index_already_index():\n    index = pd.Index([1, 2, 3])\n    actual = safe_cast_to_index(index)\n    assert actual is index\n\n", "def test_posify_mask_subindexer() -> None:\n    # Test case where all values are masked\n    index = np.array([-1, -1, -1])\n    expected_result = np.array([0, 0, 0])\n    assert_array_equal(_posify_mask_subindexer(index), expected_result)\n\n    # Test case where no values are masked\n    index = np.array([0, 1, 2])\n    expected_result = np.array([0, 1, 2])\n    assert_array_equal(_posify_mask_subindexer(index), expected_result)\n\n    # Test case where some values are masked\n    index = np.array([-1, 1, -1, 3])\n    expected_result = np.array([0, 1, 1, 3])\n    assert_array_equal(_posify_mask_subindexer(index), expected_result)\n"], "sample_627": ["def test_concat_dataset_along_new_dim_with_positions():\n    # Test concatenating along a new dimension with positions.\n    datasets = create_concat_datasets(num_datasets=3, include_day=False)\n    result = concat(datasets, dim=\"time\", positions=[[0], [1], [2]])\n    expected = Dataset()\n    for i, ds in enumerate(datasets):\n        expected = merge([expected, ds.expand_dims(\"time\", axis=0).assign_coords(time=[i])])\n    assert_identical(result, expected)\n\n    # Test concatenating along a new dimension with positions and different lengths.\n    datasets = create_concat_datasets(num_datasets=3, include_day=False)\n    datasets[0] = datasets[0].isel(x=slice(0, 2))\n    result = concat(datasets, dim=\"time\", positions=[[0], [1, 2], [3]])\n    expected = Dataset()\n    for i, ds in enumerate(datasets):\n        if i == 0:\n            expected = merge([expected, ds.expand_dims(\"time\", axis=0).assign_coords(time=[i])])\n        else:\n            expected = merge([expected, ds.assign_coords(time=[i] * len(ds.x))])\n    assert_identical(result, expected)\n", "def test_concat_coords_mismatched_length():\n    # Test that concat fails when coordinate lengths are mismatched\n    ds1 = Dataset()\n    ds1.coords[\"x\"] = (\"x\", [1, 2, 3])\n    ds2 = Dataset()\n    ds2.coords[\"x\"] = (\"x\", [4, 5])\n\n    with pytest.raises(ValueError):\n        concat([ds1, ds2], dim=\"x\")\n", "def test_concat_dataset_with_mismatched_coord_names(\n    join: JoinOptions = \"outer\", combine_attrs: CombineAttrsOptions = \"override\"", "def test_concat_join_exact():\n    # Test that concat with join=\"exact\" aligns DataArrays with\n    # different dimension lengths\n    da1 = DataArray([1, 2, 3], dims=\"x\", coords={\"x\": [1, 2, 3]})\n    da2 = DataArray([4, 5], dims=\"x\", coords={\"x\": [4, 5]})\n\n    with pytest.raises(ValueError):\n        concat([da1, da2], dim=\"new_dim\", join=\"exact\")\n", "def test_concat_join_exact():\n    # Test that concat fails when join='exact' and indexes are not equal\n    ds1 = Dataset()\n    ds1[\"x\"] = (\"x\", [1, 2, 3])\n    ds2 = Dataset()\n    ds2[\"x\"] = (\"x\", [1, 2, 4])\n\n    with pytest.raises(ValueError):\n        concat([ds1, ds2], dim=\"new_dim\", join=\"exact\")\n\n    # Test that concat succeeds when join='exact' and indexes are equal\n    ds1 = Dataset()\n    ds1[\"x\"] = (\"x\", [1, 2, 3])\n    ds2 = Dataset()\n    ds2[\"x\"] = (\"x\", [1, 2, 3])\n\n    result = concat([ds1, ds2], dim=\"new_dim\", join=\"exact\")\n    assert_equal(result.x, ds1.x)\n"], "sample_628": ["def test_check_spelling_in_comment_with_directive(self):\n    self.checker.process_tokens(\n        _tokenize_str(\n            \"\"\"", "    def test_spelling_comment_directives(self):\n        \"\"\"Test that directives at the beginning of comments are ignored.\"\"\"\n        node = astroid.extract_node(\"\"\"\n            # fmt: off\n            # This is a commment with a spelling eror.\n            \"\"\")\n        with self.assertAddsMessages(\n            Message(\n                msg_id='wrong-spelling-in-comment',\n                line=3,\n                args=(\"eror\", \"# This is a commment with a spelling eror.\", \"               ^^^^\", self._get_msg_suggestions(\"eror\")),\n            ),\n        ):\n            self.checker.visit_module(node)\n        self.checker.process_tokens(_tokenize_str(node.as_string()))\n", "    def test_ignore_comment_directives(self):\n        # Test that ignore-comment-directives are correctly ignored\n        code = \"\"\"", "def test_spelling_ignore_comment_directives(self):\n    \"\"\"Test that ignore-comment-directives option works.\"\"\"\n    comment = \"# fmt: on  # pylint: disable=C0321,C0103  # noqa  # myvar: int\"\n    with self.assertNoMessages():\n        self.checker.process_tokens(_tokenize_str(comment))\n", "    def test_spellchecking_ignore_words(self):\n        self.checker.ignore_list = [\"nonexistantword\"]\n        self.checker.process_tokens(\n            _tokenize_str(\n                \"\"\"\n            # This is a nonexistantword in a comment\n            \"\"\"\n            )\n        )\n        with pytest.raises(AssertionError):\n            self.assert_add_message(\"wrong-spelling-in-comment\")\n"], "sample_629": ["def test_get_python_path():\n    # Create a temporary directory with an __init__.py file\n    tmp_dir = Path(\"tmp\")\n    tmp_dir.mkdir()\n    init_file = tmp_dir / \"__init__.py\"\n    init_file.touch()\n\n    # Test get_python_path with the temporary directory\n    assert get_python_path(str(tmp_dir)) == str(tmp_dir.parent)\n\n    # Remove the temporary directory\n    init_file.unlink()\n    tmp_dir.rmdir()\n", "def test_get_python_path():\n    # Create a temporary directory with an __init__.py file\n    temp_dir = Path(\"temp\")\n    temp_dir.mkdir()\n    init_file = temp_dir / \"__init__.py\"\n    init_file.touch()\n\n    # Test that get_python_path returns the correct directory\n    assert get_python_path(str(init_file)) == str(temp_dir.parent)\n\n    # Remove the temporary directory\n    import shutil\n    shutil.rmtree(temp_dir)\n", "def test_get_python_path():\n    filepath = Path(__file__).resolve().parent / \"test_file.py\"\n    assert get_python_path(str(filepath)) == str(filepath.parent)\n\n    dirpath = Path(__file__).resolve().parent\n    assert get_python_path(str(dirpath)) == str(dirpath)\n", "def test_get_python_path(tmp_path):\n    \"\"\"Test get_python_path function\"\"\"\n    some_file = tmp_path / \"some_file.py\"\n    some_file.touch()\n    assert get_python_path(str(some_file)) == str(tmp_path)\n\n    some_dir = tmp_path / \"some_dir\"\n    some_dir.mkdir()\n    init_file = some_dir / \"__init__.py\"\n    init_file.touch()\n    assert get_python_path(str(some_dir)) == str(tmp_path)\n", "def test_get_python_path(tmp_path):\n    # Create a temporary directory with an __init__.py file\n    package_dir = tmp_path / \"package\"\n    package_dir.mkdir()\n    init_file = package_dir / \"__init__.py\"\n    init_file.touch()\n\n    # Create a subdirectory without an __init__.py file\n    sub_dir = package_dir / \"subdir\"\n    sub_dir.mkdir()\n\n    # Test that get_python_path returns the correct path for the package\n    assert get_python_path(str(package_dir)) == str(tmp_path)\n\n    # Test that get_python_path returns the correct path for the subdirectory\n    assert get_python_path(str(sub_dir)) == str(package_dir)\n"], "sample_630": ["def test_vcg_writer(setup, tmpdir):\n    project = get_project(os.path.join(os.path.dirname(__file__), \"data\"))\n    linker = Linker(project)\n    CONFIG = Config()\n    CONFIG.output_format = \"vcg\"\n    handler = DiadefsHandler(CONFIG)\n    dd = DefaultDiadefGenerator(linker, handler).visit(project)\n    for diagram in dd:\n        diagram.extract_relationships()\n    writer = VCGWriter(CONFIG)\n    writer.write(dd)\n    vcg_files = [\"packages_No_Name.vcg\", \"classes_No_Name.vcg\"]\n    for fname in vcg_files:\n        assert os.path.exists(fname)\n        with open(fname) as f:\n            contents = f.read()\n            assert contents.startswith(\"graph:\")\n            assert \"title:\" in contents\n        os.remove(fname)\n", "def test_vcg_writer(setup, tmpdir):\n    \"\"\"Test VCGWriter\"\"\"\n    project = get_project(os.path.join(os.path.dirname(__file__), \"data\"))\n    linker = Linker(project)\n    CONFIG = Config()\n    CONFIG.output_format = \"vcg\"\n    handler = DiadefsHandler(CONFIG)\n    dd = DefaultDiadefGenerator(linker, handler).visit(project)\n    for diagram in dd:\n        diagram.extract_relationships()\n    writer = VCGWriter(CONFIG)\n    output_file = str(tmpdir / \"output.vcg\")\n    with patch.object(writer, \"graph_file\", open(output_file, \"w+\")):\n        writer.write(dd)\n    assert os.path.exists(output_file)\n    with open(output_file) as f:\n        content = f.read()\n    assert \"title=No Name\" in content\n    assert \"layoutalgorithm=dfs\" in content\n    assert \"late_edge_labels=yes\" in content\n    assert \"port_sharing=no\" in content\n    assert \"manhattan_edges=yes\" in content\n", "def test_vcg_writer(setup, tmpdir):\n    \"\"\"test vcg writer\"\"\"\n    project = get_project(os.path.join(os.path.dirname(__file__), \"data\"))\n    linker = Linker(project)\n    CONFIG = Config()\n    CONFIG.output_format = \"vcg\"\n    CONFIG.output_directory = str(tmpdir)\n    handler = DiadefsHandler(CONFIG)\n    dd = DefaultDiadefGenerator(linker, handler).visit(project)\n    for diagram in dd:\n        diagram.extract_relationships()\n    writer = VCGWriter(CONFIG)\n    writer.write(dd)\n    assert len(tmpdir.listdir()) == 2\n    for file in tmpdir.listdir():\n        assert file.basename.startswith(\"classes_No_Name\") or file.basename.startswith(\n            \"packages_No_Name\"\n        )\n        assert file.ext == \".vcg\"\n", "def test_vcg_writer(tmpdir):\n    \"\"\"Test VCGWriter\"\"\"\n    project = get_project(os.path.join(os.path.dirname(__file__), \"data\"))\n    linker = Linker(project)\n    CONFIG = Config()\n    CONFIG.output_format = \"vcg\"\n    handler = DiadefsHandler(CONFIG)\n    dd = DefaultDiadefGenerator(linker, handler).visit(project)\n    for diagram in dd:\n        diagram.extract_relationships()\n    writer = VCGWriter(CONFIG)\n    output_dir = str(tmpdir)\n    CONFIG.output_directory = output_dir\n    writer.write(dd)\n    for fname in [\"packages_No_Name.vcg\", \"classes_No_Name.vcg\"]:\n        assert os.path.exists(os.path.join(output_dir, fname))\n", "def test_diagram_writer(setup):\n    \"\"\"Test DiagramWriter class\"\"\"\n    config = Config()\n    writer = DotWriter(config)\n    diagram = astroid.Diagram(\"Test Diagram\")\n    diagram.TYPE = \"class\"\n    obj1 = astroid.ClassDef(\"Class1\", None)\n    obj2 = astroid.ClassDef(\"Class2\", None)\n    diagram.objects = [obj1, obj2]\n    writer.write([diagram])\n    assert os.path.exists(\"classes_Test_Diagram.dot\")\n    with open(\"classes_Test_Diagram.dot\", \"r\") as f:\n        content = f.read()\n        assert \"Class1\" in content\n        assert \"Class2\" in content\n    os.remove(\"classes_Test_Diagram.dot\")\n"], "sample_631": ["    def test_undefined_variable_in_lambda(self):\n        code = \"\"\"\n        x = 10\n        y = 5\n        a = lambda: lambda z=x+y: z\n        print(a()())\n        \"\"\"\n\n        with self.assertNoMessages():\n            self.checker.process_module(astroid.parse(code))\n", "    def test_unused_variable_in_loop(self):\n        \"\"\"Test that unused variables are detected in loops\"\"\"\n        config = self.linter.config\n        config.allow_global_unused_variables = False\n\n        code = \"\"\"\n            for i in range(10):\n                j = i + 1\n                print(j)\n        \"\"\"\n\n        with self.assertAddsMessages(\n            Message(\n                \"unused-variable\",\n                line=3,\n                args=(\"i\",),\n                confidence=variables.HIGH,\n            )\n        ):\n            self.checker.visit_module(astroid.parse(code))\n            self.checker.leave_module(astroid.parse(code))\n", "    def test_undefined_variable_in_annotation(self):\n        code = \"\"\"\n            from typing import Union\n\n                pass\n\n            bar: str = 'bar'\n            baz = bar + x\n            \"\"\"\n        messages = [\n            Message(msg_id='undefined-variable', line=7, args=('x',)),\n        ]\n        self.checker.visit_module(astroid.parse(code))\n        with self.assertAddsMessages(messages):\n            self.walk(code)\n", "    def test_self_cls_assign(self):\n        assign = astroid.extract_node(\n            \"\"\"\n            class MyClass:\n                    self = 42  #@\n            \"\"\"\n        )\n        with self.assertAddsMessages(\n            Message(\"self-cls-assignment\", node=assign, args=(\"self\",))\n        ):\n            self.checker.visit_assign(assign)\n\n        assign = astroid.extract_node(\n            \"\"\"\n            class Meta(type):\n                    cls = 42  #@\n            \"\"\"\n        )\n        with self.assertAddsMessages(\n            Message(\"self-cls-assignment\", node=assign, args=(\"cls\",))\n        ):\n            self.checker.visit_assign(assign)\n", "    def test_undefined_variable_with_nested_loops(self):\n        code = \"\"\""], "sample_632": ["def test_similar_ignore_imports():\n    reporter = Reporter()\n    linter = PyLinter(reporter=reporter)\n    checker = similar.SimilarChecker(linter)\n    with open(HIDE_CODE_WITH_IMPORTS) as f:\n        lines = f.readlines()\n    checker.append_stream(\"test.py\", lines)\n    checker.close()\n    expected = [\n        \"Similar lines in 1 files\",\n        \"==test.py:2\",\n        \"    # pylint: disable=import-error\",\n        \"TOTAL lines=10 duplicates=0 percent=0.00\",\n    ]\n    output = StringIO()\n    with redirect_stdout(output):\n        checker.run()\n    assert output.getvalue().strip().splitlines() == expected\n", "def test_get_similarity_report():\n    sim = similar.Similar()\n    sim.append_stream(\"file1\", StringIO(\"a\\nb\\nc\\na\\nb\\nc\"))\n    sim.append_stream(\"file2\", StringIO(\"a\\nb\\nc\"))\n    similarities = [(3, {(\"file1\", 0), (\"file2\", 0)})]\n    report = sim._get_similarity_report(similarities)\n    expected_report = (\n        \"\\n3 similar lines in 2 files\\n==file1:0\\n==file2:0\\n   a\\n   b\\n   c\\n\"\n        \"TOTAL lines=7 duplicates=3 percent=42.86\\n\"\n    )\n    assert report == expected_report\n", "def test_similar_with_ignore_docstrings():\n    sim = similar.Similar(ignore_docstrings=True)\n    with open(SIMILAR1) as stream:\n        sim.append_stream(SIMILAR1, stream)\n    with open(SIMILAR2) as stream:\n        sim.append_stream(SIMILAR2, stream)\n    output = StringIO()\n    with redirect_stdout(output):\n        sim.run()\n    assert \"TOTAL lines\" in output.getvalue()\n", "def test_similar_with_ignore_imports():\n    with open(SIMILAR1) as f1, open(SIMILAR2) as f2:\n        sim = similar.Similar(ignore_imports=True)\n        sim.append_stream(\"similar1\", f1)\n        sim.append_stream(\"similar2\", f2)\n        with redirect_stdout(StringIO()) as stdout:\n            sim.run()\n        output = stdout.getvalue()\n        assert \"TOTAL lines=\" in output\n        assert \"duplicates=\" in output\n        assert \"percent=\" in output\n", "def test_similar_ignore_signatures():\n    with open(SIMILAR4) as f1, open(SIMILAR5) as f2:\n        sim = similar.Similar(min_lines=4, ignore_comments=True, ignore_docstrings=True, ignore_imports=False, ignore_signatures=True)\n        sim.append_stream(\"similar4\", f1)\n        sim.append_stream(\"similar5\", f2)\n        out = StringIO()\n        with redirect_stdout(out):\n            sim.run()\n        output = out.getvalue()\n        assert \"TOTAL lines=\" in output\n        assert \"duplicates=\" in output\n        assert \"percent=\" in output\n"], "sample_633": ["def test_filter_noncode_lines():\n    lineset1 = similar.LineSet(\n        \"test_file\", [\"def test():\\n\", \"    pass\\n\", \"\\n\", \"# comment\\n\"]\n    )\n    lineset2 = similar.LineSet(\n        \"test_file\", [\"def test():\\n\", \"    pass\\n\", \"\\n\", \"# comment\\n\"]\n    )\n    assert (\n        similar.filter_noncode_lines(\n            lineset1, 0, lineset2, 0, common_lines_nb=4\n        )\n        == 2\n    )\n", "def test_ignore_imports():\n    sim = similar.Similar(ignore_imports=True)\n    with open(HIDE_CODE_WITH_IMPORTS, encoding=\"utf-8\") as stream:\n        sim.append_stream(HIDE_CODE_WITH_IMPORTS, stream)\n    output = StringIO()\n    with redirect_stdout(output):\n        sim.run()\n    assert not output.getvalue()\n", "def test_ignore_imports():\n    \"\"\"Test that ignore-imports option works.\"\"\"\n    output = StringIO()\n    with redirect_stdout(output):\n        similar.Run([f\"--ignore-imports\", str(HIDE_CODE_WITH_IMPORTS)])\n    assert \"1 similar lines in 1 files\" not in output.getvalue()\n    assert \"TOTAL lines=\" in output.getvalue()\n", "def test_get_similarity_report(similar_checker):\n    \"\"\"Test the _get_similarity_report method.\"\"\"\n    # Create a fake Similar instance with some test data\n    similar_checker.linesets = [\n        similar.LineSet(\"file1\", [\"line1\", \"line2\", \"line3\"]),\n        similar.LineSet(\"file2\", [\"line1\", \"line2\", \"line3\"]),\n    ]\n    similarities = [(2, {(similar_checker.linesets[0], 0, 2), (similar_checker.linesets[1], 0, 2)})]\n\n    report = similar_checker._get_similarity_report(similarities)\n    assert \"2 similar lines in 2 files\" in report\n    assert \"==file1:[0:2]\" in report\n    assert \"==file2:[0:2]\" in report\n    assert \"   line1\" in report\n    assert \"   line2\" in report\n    assert \"TOTAL lines=6 duplicates=2 percent=33.33\" in report\n", "def test_filter_noncode_lines():\n    lines1 = [\"import os\", \"print('Hello World')\", \"def test():\", \"    pass\"]\n    lineset1 = similar.LineSet(\"test1.py\", lines1)\n    lines2 = [\"import os\", \"print('Hello World')\", \"def test():\", \"    pass\"]\n    lineset2 = similar.LineSet(\"test2.py\", lines2)\n\n    common_lines_nb = 4\n    stindex_1 = similar.Index(0)\n    stindex_2 = similar.Index(0)\n\n    filtered_lines = similar.filter_noncode_lines(\n        lineset1, stindex_1, lineset2, stindex_2, common_lines_nb\n    )\n\n    assert filtered_lines == 3\n"], "sample_634": ["def test_expand_modules_with_ignore_list_paths_re(tmp_path):\n    \"\"\"Test expand_modules with ignore_list_paths_re\"\"\"\n    tmp_file = tmp_path / \"file.py\"\n    tmp_file.write_text(\"\")\n    ignore_list_paths_re = [re.compile(str(tmp_file.parent) + \"/.*\")]\n    result, _ = expand_modules([str(tmp_file)], [], [], ignore_list_paths_re)\n    assert not result\n", "def test_expand_modules_with_namespace_packages():\n    files_or_modules = [\"mynamespace.mypackage\"]\n    ignore_list = []\n    ignore_list_re = [re.compile(r\"^__init__.py$\")]\n    ignore_list_paths_re = []\n\n    module_path = str(Path(__file__).parent / \"data\" / \"namespace_package\")\n    result, errors = expand_modules(\n        files_or_modules,\n        ignore_list,\n        ignore_list_re,\n        ignore_list_paths_re,\n    )\n\n    expected_result = [\n        {\n            \"path\": str(Path(module_path) / \"__init__.py\"),\n            \"name\": \"mynamespace.mypackage\",\n            \"isarg\": True,\n            \"basepath\": str(Path(module_path) / \"__init__.py\"),\n            \"basename\": \"mynamespace.mypackage\",\n        },\n        {\n            \"path\": str(Path(module_path) / \"submodule.py\"),\n            \"name\": \"mynamespace.mypackage.submodule\",\n            \"isarg\": False,\n            \"basepath\": str(Path(module_path) / \"__init__.py\"),\n            \"basename\": \"mynamespace.mypackage\",\n        },\n    ]\n    assert result == expected_result\n    assert errors == []\n", "def test_get_python_path():\n    \"\"\"Test that get_python_path returns the correct python path.\"\"\"\n    filepath = \"/path/to/module.py\"\n    assert get_python_path(filepath) == \"/path/to\"\n\n    filepath = \"/path/to/package/__init__.py\"\n    assert get_python_path(filepath) == \"/path/to\"\n\n    filepath = \"/path/to/package/submodule.py\"\n    assert get_python_path(filepath) == \"/path/to/package\"\n\n    filepath = \"/path/to/namespace/__init__.py\"\n    assert get_python_path(filepath) == \"/path/to\"\n\n    filepath = \"/path/to/namespace/submodule.py\"\n    assert get_python_path(filepath) == \"/path/to/namespace\"\n", "def test_get_python_path():\n    \"\"\"Test that get_python_path returns the correct python path\"\"\"\n    filepath = \"/path/to/module.py\"\n    assert get_python_path(filepath) == \"/path/to\"\n\n    filepath = \"/path/to/package/__init__.py\"\n    assert get_python_path(filepath) == \"/path/to\"\n\n    filepath = \"/path/to/package/submodule.py\"\n    assert get_python_path(filepath) == \"/path/to/package\"\n", "def test_get_python_path(tmpdir):\n    \"\"\"Test that get_python_path returns the correct path\"\"\"\n    # Create a temporary directory with an __init__.py file\n    tmp_dir = tmpdir.mkdir(\"test_dir\")\n    init_file = tmp_dir.join(\"__init__.py\")\n    init_file.write(\"\")\n    \n    # Get the python path for the temporary directory\n    python_path = get_python_path(str(tmp_dir))\n    \n    # Assert that the python path is the parent of the temporary directory\n    assert python_path == str(tmp_dir.dirpath())\n"], "sample_635": ["def test_sphinx_param_documentation(self):\n    \"\"\"Test sphinx-style parameter documentation\"\"\"\n    module = astroid.parse(\n        \"\"\"", "    def test_property_returns_with_type(self):\n        \"\"\"Test that a property with a return type is correctly parsed.\"\"\"\n        node = astroid.extract_node(\n            \"\"\"\n            class MyClass:\n                @property\n                    '''Some description.'''\n                    return 42\n            \"\"\"\n        )\n        checker = self.checker\n        checker.visit_functiondef(node)\n        with pytest.raises(AssertionError):\n            self.assertAddsMessages(checker, [])\n", "    def test_google_style(self):\n        \"\"\"Test that Google-style docstrings are parsed correctly\"\"\"\n        node = astroid.extract_node(\n            \"\"\"\n                \\\"\\\"\\\"\n                Args:\n                    param1 (int): The first parameter.\n                    param2 (str): The second parameter.\n\n                Returns:\n                    str: A string result.\n                \\\"\\\"\\\"\n            \"\"\"\n        )\n        with set_config_directly(self.checker, {\"docparams_style\": \"google\"}):\n            self.walk(node)\n        assert len(self.linter.release_messages()) == 0\n", "def test_sphinx_param_documentation(self):\n    \"\"\"Test Sphinx-style parameter documentation.\"\"\"\n    node = astroid.extract_node(\"\"\"\n            \"\"\"\n            :param foo: Foo description.\n            :type foo: str\n\n            :param bar: Bar description.\n            \"\"\"\n            pass\n    \"\"\")\n    with set_config_directly(self.checker, {\"docparams_style\": \"sphinx\"}):\n        self.checker.visit_functiondef(node)\n        assert len(self.linter.release_messages()) == 0\n", "def test_get_setters_property_name(self):\n    node = astroid.extract_node(\"\"\"\n        @my_property.setter\n            pass\n    \"\"\")\n    assert get_setters_property_name(node) == \"my_property\"\n\n    node = astroid.extract_node(\"\"\"\n            pass\n    \"\"\")\n    assert get_setters_property_name(node) is None\n"], "sample_636": ["def test_similar_lines_ignore_docstrings() -> None:\n    \"\"\"Test similar lines with ignore-docstrings option.\"\"\"\n    with _patch_streams(StringIO()) as out, warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        Run([join(DATA, \"docstrings.py\"), \"--disable=all\", \"--enable=similarities\", \"--ignore-docstrings\"])\n    assert \"Similar lines in 1 files\" not in out.getvalue()\n", "def test_similar_code_checker_ignore_docstrings():\n    \"\"\"Test that the similar code checker ignores docstrings.\"\"\"\n    with open(join(DATA, \"ignore_docstrings.py\"), encoding=\"utf-8\") as f:\n        with _patch_streams(StringIO()) as out:\n            Run([f.name], exit_zero=1)\n    output = out.getvalue()\n    assert \"Similar lines in 2 files\" not in output\n", "def test_filter_noncode_lines() -> None:\n    lines1 = [\n        LineSpecifs(text=\"def some_function():\", line_number=LineNumber(0)),\n        LineSpecifs(text=\"    print('Hello')\", line_number=LineNumber(1)),\n        LineSpecifs(text=\"\", line_number=LineNumber(2)),\n        LineSpecifs(text=\"    # Some comment\", line_number=LineNumber(3)),\n        LineSpecifs(text=\"    some_other_call()\", line_number=LineNumber(4)),\n    ]\n    lineset1 = LineSet(\"some_file.py\", [\"\"] * len(lines1))\n    lineset1._stripped_lines = lines1\n\n    lines2 = [\n        LineSpecifs(text=\"def some_function():\", line_number=LineNumber(0)),\n        LineSpecifs(text=\"    print('Hello')\", line_number=LineNumber(1)),\n        LineSpecifs(text=\"\", line_number=LineNumber(2)),\n        LineSpecifs(text=\"    # Some other comment\", line_number=LineNumber(3)),\n        LineSpecifs(text=\"    some_other_call()\", line_number=LineNumber(4)),\n    ]\n    lineset2 = LineSet(\"some_other_file.py\", [\"\"] * len(lines2))\n    lineset2._stripped_lines = lines2\n\n    assert (\n        filter_noncode_lines(\n            lineset1, Index(0), lineset2, Index(0), common_lines_nb=5\n        )\n        == 3\n    )\n", "def test_filter_noncode_lines() -> None:\n    lineset1 = LineSet(\n        \"file1\",\n        [\n            \"def test_function():\\n\",\n            \"    pass\\n\",\n            \"\\n\",\n            \"    # this is a comment\\n\",\n            \"    variable = 5  # and another comment\\n\",\n            '    \"\"\"docstring\"\"\"',\n        ],\n        ignore_comments=True,\n        ignore_docstrings=True,\n    )\n    lineset2 = LineSet(\n        \"file2\",\n        [\n            \"def test_function():\\n\",\n            \"    pass\\n\",\n            \"\\n\",\n            \"    # this is another comment\\n\",\n            \"    variable = 5  # and yet another comment\\n\",\n            '    \"\"\"another docstring\"\"\"',\n        ],\n        ignore_comments=True,\n        ignore_docstrings=True,\n    )\n\n    common_lines_nb = 4\n    start_index_1 = Index(0)\n    start_index_2 = Index(0)\n\n    result = filter_noncode_lines(\n        lineset1, start_index_1, lineset2, start_index_2, common_lines_nb\n    )\n\n    assert result == 2\n", "    def test_similarities_ignore_comments(self):\n        with _patch_streams(StringIO()) as out:\n            Run([\"--ignore-comments\", join(DATA, \"file1.py\"), join(DATA, \"file2.py\")])\n            output = out.getvalue()\n            assert \"Similar lines in 2 files\" in output\n            assert \"TOTAL lines=14 duplicates=2 percent=14.29\" in output\n"], "sample_637": ["    def test_use_symbolic_message_instead(self):\n        code = \"\"\"", "    def test_use_symbolic_message_instead(self):\n        enable = \"# pylint: enable=C0301\"\n        disable = \"# pylint: disable=C0301\"\n        node = _tokenize_str(enable + \"\\n\" + disable)\n        self.checker._get_by_id_managed_msgs = lambda: [\n            (\"module_name\", \"C0301\", \"line-too-long\", 1, False),\n            (\"module_name\", \"C0301\", \"line-too-long\", 2, True),\n        ]\n        with set_config(node, self.checker):\n            self.checker.process_module(node)\n        expected_messages = [\n            MessageTest(\n                msg_id=\"use-symbolic-message-instead\",\n                line=1,\n                args=\"'C0301' is cryptic: use '# pylint: enable=line-too-long' instead\",\n            ),\n            MessageTest(\n                msg_id=\"use-symbolic-message-instead\",\n                line=2,\n                args=\"'C0301' is cryptic: use '# pylint: disable=line-too-long' instead\",\n            ),\n        ]\n        self.assert_adds_messages(expected_messages)\n", "    def test_use_symbolic_message_instead(self):\n        # this needs to be in a file because add_message now enforces that it's only called from a module\n        with self.create_file(\n            \"\"\"# pylint: disable-by-id=I0022\n            \"\"\"\n        ) as module:\n            linter = self.linter\n            linter._by_id_managed_msgs.append((\"test\", \"I0022\", \"test-message\", 1, True))\n            self.checker.process_module(module)\n            expected = [MessageTest(\"use-symbolic-message-instead\", line=1)]\n            self.assert_add_message(*expected)\n", "    def test_by_id_managed_messages(self):\n        node = self.checker.linter.module\n        node.name = 'test_module'\n        self.checker.linter._by_id_managed_msgs.append(('test_module', '1234', 'message-symbol', 1, True))\n        with self.assertAddsMessages(\n            MessageTest(msg_id='use-symbolic-message-instead', line=1, args=\"'1234' is cryptic: use '# pylint: disable=message-symbol' instead\")\n        ):\n            self.checker.process_module(node)\n", "    def test_encoding_problem(self):\n        node = self.ast_node(\"\"\"\n            # -*- coding: invalid -*-\n            pass\n        \"\"\")\n        with self.assertAddsMessages(\n            MessageTest(misc.EncodingChecker, 'syntax-error', line=1)\n        ):\n            self.checker.process_module(node)\n"], "sample_638": ["def test_run_class_diagram(mock_subprocess):\n    \"\"\"Test Run class diagram generation.\"\"\"\n    with mock.patch(\"pylint.pyreverse.main.Run.load_command_line_configuration\") as mock_load:\n        mock_load.return_value = [\"--classes\", \"class1\", \"--output-format\", \"dot\"]\n        run = main.Run([\"--classes\", \"class1\"])\n        assert run.config.classes == [\"class1\"]\n        assert run.config.output_format == \"dot\"\n", "def test_run_with_invalid_format(mock_subprocess, mock_graphviz, capsys):\n    with pytest.raises(SystemExit) as pytest_exit:\n        main.Run([\"--output-format=invalid\", \"pylint/pyreverse/__init__.py\"])\n    assert pytest_exit.value.code == 1\n    captured = capsys.readouterr()\n    assert \"Format invalid is not supported natively.\" in captured.out\n    assert 'Format: \"invalid\" not recognized.' in captured.out\n", "def test_run_main(mock_graphviz, mock_subprocess, setup_path):\n    \"\"\"Test running the main function.\"\"\"\n    with mock.patch(\"pylint.pyreverse.main.Run.run\") as mock_run:\n        mock_run.return_value = 0\n        with mock.patch(\"sys.argv\", [\"pyreverse\", \"test_package\"]):\n            main.Run(sys.argv[1:])\n            mock_run.assert_called_once()\n", "def test_run_main_without_args(mock_subprocess, capsys):\n    with pytest.raises(SystemExit) as pytest_exit:\n        main.Run([])\n    assert pytest_exit.value.code == 1\n    captured = capsys.readouterr()\n    assert \"Usage:\" in captured.out\n", "def test_run_main(mock_graphviz, mock_subprocess):\n    with mock.patch(\"pylint.pyreverse.main.Run.run\") as mock_run:\n        mock_run.return_value = 0\n        sys.argv = [\"pyreverse\", \"test_module\"]\n        main.Run(sys.argv[1:])\n    mock_run.assert_called_once()\n    mock_subprocess.run.assert_not_called()\n"], "sample_639": ["def test_checker_equality():\n    checker1 = OtherBasicChecker()\n    checker2 = OtherBasicChecker()\n    assert checker1 == checker2\n\n    checker3 = DifferentBasicChecker()\n    assert checker1 != checker3\n\n    checker4 = LessBasicChecker()\n    assert checker1 != checker4  # Because of different options\n\n    # Check that hash works correctly for equal checkers\n    assert hash(checker1) == hash(checker2)\n\n    # Check that hash works correctly for unequal checkers\n    assert hash(checker1) != hash(checker3)\n    assert hash(checker1) != hash(checker4)\n", "def test_create_message_definition_from_tuple():\n    checker = BaseChecker()\n    msgid = \"W1234\"\n    msg_tuple = (\n        \"message\",\n        \"message-symbol\",\n        \"Message description with detail.\",\n        {\"scope\": \"node\"},\n    )\n    message_definition = checker.create_message_definition_from_tuple(msgid, msg_tuple)\n    assert message_definition.msgid == msgid\n    assert message_definition.symbol == msg_tuple[1]\n    assert message_definition.msg == msg_tuple[0]\n    assert message_definition.descr == msg_tuple[2]\n    assert message_definition.scope == \"node\"\n\n    # Test with default scope\n    del msg_tuple[3][\"scope\"]\n    message_definition = checker.create_message_definition_from_tuple(msgid, msg_tuple)\n    assert message_definition.scope == WarningScope.NODE\n\n    # Test with invalid message tuple\n    msg_tuple = (\"message\",)\n    with pytest.raises(InvalidMessageError):\n        checker.create_message_definition_from_tuple(msgid, msg_tuple)\n", "def test_checker_equality():\n    \"\"\"Test equality of checkers.\"\"\"\n    checker1 = OtherBasicChecker()\n    checker2 = OtherBasicChecker()\n    assert checker1 == checker2\n\n    checker3 = DifferentBasicChecker()\n    assert checker1 != checker3\n\n    checker4 = LessBasicChecker()\n    assert checker1 != checker4\n", "    def test_checker_ordering(self):\n        checker1 = OtherBasicChecker()\n        checker2 = DifferentBasicChecker()\n\n        assert checker1 > checker2\n        assert checker1 != checker2\n", "def test_create_message_definition_from_tuple():\n    \"\"\"Test BaseChecker.create_message_definition_from_tuple.\"\"\"\n    checker = BaseChecker()\n    msgid = \"W1234\"\n    msg_tuple = (\n        \"message\",\n        \"message-symbol\",\n        \"Message description with detail.\",\n    )\n    message_definition = checker.create_message_definition_from_tuple(msgid, msg_tuple)\n    assert message_definition.msgid == msgid\n    assert message_definition.symbol == msg_tuple[1]\n    assert message_definition.msg == msg_tuple[0]\n    assert message_definition.help == msg_tuple[2]\n\n    # Test that an InvalidMessageError is raised when the tuple has less than 3 elements.\n    msg_tuple = (\"message\", \"message-symbol\")\n    try:\n        checker.create_message_definition_from_tuple(msgid, msg_tuple)\n        assert False, \"Expected InvalidMessageError\"\n    except InvalidMessageError as e:\n        assert str(e).startswith(\"Messages should have a msgid and a symbol.\")\n\n    # Test that an InvalidMessageError is not raised when the tuple has more than 3 elements.\n    msg_tuple = (\n        \"message\",\n        \"message-symbol\",\n        \"Message description with detail.\",\n        {\"scope\": \"line\"},\n    )\n    checker.create_message_definition_from_tuple(msgid, msg_tuple)\n"], "sample_640": ["def test_is_classdef_type(node_type, expected):\n    node = node_type()\n    assert utils.is_classdef_type(node) == expected\n", "def test_is_builtin_object() -> None:\n    builtin_node = astroid.extract_node(\n        \"\"\"\n        len([1, 2, 3])\n        \"\"\"\n    )\n    assert utils.is_builtin_object(builtin_node.func)\n    not_builtin_node = astroid.extract_node(\n        \"\"\"\n        open = 'I like to mess with builtins'\n        open(\"file.txt\", \"r\")\n        \"\"\"\n    )\n    assert not utils.is_builtin_object(not_builtin_node.func)\n", "def test_is_overload_stub() -> None:\n    overload_stub = astroid.extract_node(\n        \"\"\"\n        from typing import overload\n\n        @overload\n            ...\n\n            return x\n        \"\"\"\n    )\n    assert utils.is_overload_stub(overload_stub)\n\n    not_overload_stub = astroid.extract_node(\n        \"\"\"\n            return x\n        \"\"\"\n    )\n    assert not utils.is_overload_stub(not_overload_stub)\n", "def test_is_builtin_object() -> None:\n    module = astroid.parse(\n        \"\"\"\n        import builtins\n\n            a = len\n            b = builtins.len\n        \"\"\"\n    )\n    node_a = module.body[1].body[0].targets[0]\n    node_b = module.body[1].body[1].targets[0]\n\n    assert utils.is_builtin_object(node_a) is False\n    assert utils.is_builtin_object(node_b) is True\n", "def test_is_comprehension(node_type, expected):\n    node = node_type()\n    assert utils.is_comprehension(node) == expected\n"], "sample_641": ["def test_load_results(tmp_path):\n    linter_stats = LinterStats()\n    linter_stats.bad_names = [BadNames(\"bad_name\", 1)]\n    save_results(linter_stats, tmp_path / \"base\")\n    loaded_results = load_results(tmp_path / \"base\", pylint_home=tmp_path)\n    assert loaded_results == linter_stats\n    assert loaded_results is not linter_stats  # Check that it's a different instance\n\n    # Test with invalid data in the cache file\n    with open(_get_pdata_path(tmp_path / \"base\", 1, tmp_path), \"wb\") as stream:\n        pickle.dump(\"Invalid data\", stream)\n    loaded_results = load_results(tmp_path / \"base\", pylint_home=tmp_path)\n    assert loaded_results is None\n\n    # Test with no cache file\n    assert load_results(tmp_path / \"non_existent_base\", pylint_home=tmp_path) is None\n", "def test_load_results(tmp_path):\n    linter_stats = LinterStats()\n    linter_stats.bad_names = [BadNames(\"bad_name\", 1)]\n    base = \"test_base\"\n    pylint_home = tmp_path / \"pylint_home\"\n    save_results(linter_stats, base, pylint_home)\n\n    loaded_stats = load_results(base, pylint_home)\n    assert loaded_stats == linter_stats\n\n    # Test that a warning is raised when loading old cache with invalid data\n    invalid_data_file = _get_pdata_path(Path(base), 1, pylint_home)\n    with open(invalid_data_file, \"wb\") as stream:\n        pickle.dump(\"invalid data\", stream)\n    with pytest.warns(UserWarning):\n        load_results(base, pylint_home)\n", "def test_load_results(tmp_path, monkeypatch):\n    # Create a temporary file with some data\n    data_file = tmp_path / \"data.stats\"\n    with open(data_file, \"wb\") as stream:\n        pickle.dump(LinterStats(), stream)\n\n    # Monkeypatch _get_pdata_path to return the temporary file\n        return data_file\n\n    monkeypatch.setattr(\"pylint.lint.caching._get_pdata_path\", mock_get_pdata_path)\n\n    # Test load_results\n    assert load_results(\"\") is not None\n\n    # Test load_results with invalid data\n    with open(data_file, \"wb\") as stream:\n        pickle.dump(object(), stream)\n    with pytest.warns(UserWarning):\n        assert load_results(\"\") is None\n\n    # Test load_results with non-existent file\n    data_file.unlink()\n    assert load_results(\"\") is None\n", "def test_load_results(tmp_path, monkeypatch):\n    pylint_home = tmp_path / \"pylint_home\"\n    monkeypatch.setattr(\"pylint.constants.PYLINT_HOME\", str(pylint_home))\n\n    # Create a fake stats file\n    data_file = _get_pdata_path(Path(\"base\"), 1, pylint_home)\n    data_file.parent.mkdir(parents=True, exist_ok=True)\n    with open(data_file, \"wb\") as stream:\n        pickle.dump(LinterStats(), stream)\n\n    # Load the stats\n    loaded_stats = load_results(\"base\")\n\n    assert isinstance(loaded_stats, LinterStats)\n\n    # Try loading from a non-existent file\n    assert load_results(\"non_existent_base\") is None\n\n    # Try loading from an invalid cache file\n    with open(data_file, \"wb\") as stream:\n        pickle.dump(\"Invalid data\", stream)\n    with pytest.warns(UserWarning):\n        assert load_results(\"base\") is None\n", "def test_load_results(tmp_path):\n    # Create a fake pylint home and data file\n    pylint_home = tmp_path / \"pylint_home\"\n    pylint_home.mkdir()\n    data_file = _get_pdata_path(Path(\"base\"), 1, pylint_home)\n    data_file.touch()\n\n    # Load results with existing data file\n    with open(data_file, \"wb\") as stream:\n        pickle.dump(LinterStats(), stream)\n    assert isinstance(load_results(\"base\", pylint_home), LinterStats)\n\n    # Load results without existing data file\n    non_existent_data_file = _get_pdata_path(Path(\"non_existent_base\"), 1, pylint_home)\n    assert load_results(\"non_existent_base\", pylint_home) is None\n\n    # Load results with invalid data\n    with open(data_file, \"wb\") as stream:\n        pickle.dump(\"Invalid data\", stream)\n    with pytest.warns(UserWarning):\n        assert load_results(\"base\", pylint_home) is None\n"], "sample_642": ["def test__preprocess_options() -> None:\n    \"\"\"Test _preprocess_options function.\"\"\"\n    run = Run([], exit=False)\n    args = [\"--init-hook=import os\", \"--rcfile=pylintrc\", \"--load-plugins=plugin\"]\n    processed_args = config._preprocess_options(run, args)\n    assert not processed_args\n\n    # Test unknown option\n    args = [\"--unknown-option\"]\n    processed_args = config._preprocess_options(run, args)\n    assert processed_args == [\"--unknown-option\"]\n\n    # Test option with value\n    args = [\"--output=output.txt\"]\n    processed_args = config._preprocess_options(run, args)\n    assert not processed_args\n    assert run._output == \"output.txt\"\n\n    # Test option without value\n    args = [\"--verbose\"]\n    processed_args = config._preprocess_options(run, args)\n    assert not processed_args\n    assert run.verbose\n", "def test_preprocess_options() -> None:\n    \"\"\"Test that preprocess options work as expected.\"\"\"\n    run = Run([], exit_zero=False)\n    args = [\"--init-hook=import os\", \"--rcfile=some_file.rc\", \"--load-plugins=some_plugin\"]\n    processed_args = config._preprocess_options(run, args)\n    assert not processed_args\n\n    assert run._rcfile == \"some_file.rc\"\n    assert run._plugins == [\"some_plugin\"]\n\n    # Check that an error is raised when an option expects a value but none is provided\n    with pytest.raises(config.ArgumentPreprocessingError):\n        config._preprocess_options(run, [\"--init-hook\"])\n\n    # Check that an error is raised when an option does not expect a value but one is provided\n    with pytest.raises(config.ArgumentPreprocessingError):\n        config._preprocess_options(run, [\"--verbose=some_value\"])\n", "def test__convert_option_to_argument_store_true() -> None:\n    \"\"\"Test _convert_option_to_argument with store_true action.\"\"\"\n    optdict = {\"action\": \"store_true\", \"help\": \"Help message\"}\n    argument = _convert_option_to_argument(\"option\", optdict)\n    assert isinstance(argument, _StoreTrueArgument)\n    assert argument.flags == [\"--option\"]\n    assert argument.default is True\n    assert argument.arg_help == \"Help message\"\n    assert argument.hide_help is False\n    assert argument.section is None\n", "def test_preprocess_options() -> None:\n    \"\"\"Test _preprocess_options function.\"\"\"\n    run = Run([])\n    args = [\"--init-hook=import os\", \"--rcfile=pylintrc\", \"--output=output.txt\"]\n    processed_args = config._preprocess_options(run, args)\n    assert processed_args == []\n    assert run._rcfile == \"pylintrc\"\n    assert run._output == \"output.txt\"\n\n    # Test with abbreviated options\n    args = [\"--init-h=import os\", \"--rcf=pylintrc\", \"--out=output.txt\"]\n    processed_args = config._preprocess_options(run, args)\n    assert processed_args == []\n    assert run._rcfile == \"pylintrc\"\n    assert run._output == \"output.txt\"\n\n    # Test with invalid options\n    args = [\"--invalid-option\"]\n    with pytest.raises(config.ArgumentPreprocessingError):\n        config._preprocess_options(run, args)\n\n    # Test with option that doesn't expect a value\n    args = [\"--verbose=value\"]\n    with pytest.raises(config.ArgumentPreprocessingError):\n        config._preprocess_options(run, args)\n", "def test_preprocess_options() -> None:\n    \"\"\"Test preprocessing of options.\"\"\"\n    run = Run([\"--init-hook=import os\", \"--rcfile=some_file.rc\"])\n    processed_args = config._preprocess_options(run, [\"--init-hook=import os\", \"--rcfile=some_file.rc\"])\n    assert not processed_args\n\n    run = Run([\"-v\", \"--verbose\"])\n    with pytest.raises(config.ArgumentPreprocessingError):\n        config._preprocess_options(run, [\"-v\", \"--verbose\", \"value\"])\n\n    run = Run([\"--enable-all-extensions\"])\n    processed_args = config._preprocess_options(run, [\"--enable-all-extensions\"])\n    assert not processed_args\n\n    run = Run([\"--load-plugins=plugin1,plugin2\"])\n    processed_args = config._preprocess_options(run, [\"--load-plugins=plugin1,plugin2\"])\n    assert not processed_args\n"], "sample_643": ["def test_colorized_text_reporter_output(capsys):\n    reporter = ColorizedTextReporter()\n    message = Message(\n        msg_id=\"W1234\",\n        symbol=\"warning-message\",\n        location=MessageLocationTuple(\"file.py\", 1, 0),\n        msg=\"This is a warning message\",\n        confidence=HIGH,\n    )\n    reporter.handle_message(message)\n\n    captured = capsys.readouterr()\n    assert \"\\x1b[\" in captured.out  # Check for ANSI escape codes\n    assert \"file.py:1:0: W1234: This is a warning message\" in captured.out\n", "def test_text_reporter_colorize_ansi():\n    reporter = TextReporter()\n    message = \"Test message\"\n    msg_style = MessageStyle(\"red\", (\"bold\",))\n    colored_message = colorize_ansi(message, msg_style)\n    assert colored_message == f\"\\033[1;31m{message}\\033[0m\"\n\n    # Test with invalid color\n    msg_style = MessageStyle(\"invalid_color\")\n    colored_message = colorize_ansi(message, msg_style)\n    assert colored_message == message\n\n    # Test with no color or style\n    msg_style = MessageStyle(None)\n    colored_message = colorize_ansi(message, msg_style)\n    assert colored_message == message\n", "def test_colorized_text_reporter_color_mapping_deprecation(\n    capsys: pytest.CaptureFixture[str], reporter: TextReporter", "def test_text_reporter_line_format(capsys, reporter):\n    \"\"\"Test that TextReporter's line format is customizable.\"\"\"\n    # Instantiate a TextReporter with a custom line format\n    rep = reporter(line_format=\"{path}:{line}: {msg} ({symbol})\")\n\n    # Create a dummy Message object\n    msg = Message(\n        \"W1234\",\n        \"message\",\n        \"module\",\n        \"obj\",\n        1,\n        0,\n        HIGH,\n        checkers.BaseChecker(),\n    )\n\n    # Call the write_message method of the TextReporter instance\n    with redirect_stdout(StringIO()) as stdout:\n        rep.write_message(msg)\n\n    # Verify that the message was formatted according to the custom line format\n    assert stdout.getvalue().strip() == \"module:1: message (W1234)\"\n", "def test_colorize_ansi_deprecation_warnings():\n    with warnings.catch_warnings(record=True) as w:\n        # Cause all warnings to always be triggered.\n        warnings.simplefilter(\"always\")\n        # Trigger a warning.\n        colorize_ansi(msg=\"message\", msg_style=\"color\", style=\"\")\n        # Verify some things\n        assert len(w) == 1\n        assert issubclass(w[-1].category, DeprecationWarning)\n        assert \"In pylint 3.0\" in str(w[-1].message)\n"], "sample_644": ["    def test_relative_import_from_same_package(self):\n        code = \"\"\"\n            from . import module\n            from .module import function\n        \"\"\"\n        node = astroid.parse(code)\n        with self.assertAddsMessages(\n            MessageTest(\n                msg_id=\"relative-beyond-top-level\",\n                node=node.body[0],\n                line=2,\n                col_offset=0,\n                end_line=2,\n                end_col_offset=len(\"from . import module\"),\n            ),\n            ignore_position=True,\n        ):\n            self.walk(node)\n", "def test_deprecated_modules(self):\n    \"\"\"Test that deprecated-modules option is taken into account.\"\"\"\n    test_pkg = astroid.MANAGER.ast_from_module_name(\"test_pkg\")\n    checker = imports.ImportsChecker(self.linter)\n    checker.linter.config.deprecated_modules = (\"os\",)\n\n    with self.assertAddsMessages(\n        MessageTest(\"deprecated-module\", node=test_pkg.body[0], args=(\"os\",), line=1)\n    ):\n        checker.visit_import(test_pkg.body[0])\n", "    def test_import_inside_toplevel(self):\n        \"\"\"Check imports inside toplevel are correctly detected.\"\"\"\n        code = \"\"\"\n                import os  # should be at the top level\n            \"\"\"\n        with self.assertAddsMessages(\n            MessageTest(msg_id=\"import-outside-toplevel\", node=astroid.Import())\n        ):\n            self.checker.visit_functiondef(astroid.parse(code))\n", "    def test_relative_imports(self):\n        \"\"\"Test relative imports.\"\"\"\n        code = \"\"\"\n            from . import module1\n            from .. import module2\n            from .module3 import function\n            from ..module4 import function\n        \"\"\"\n        node = astroid.parse(code)\n        with self.assertAddsMessages(\n            MessageTest(\n                msg_id=\"relative-beyond-top-level\",\n                line=2,\n                col_offset=0,\n                end_line=2,\n                end_col_offset=len(\"from .. import module2\"),\n            )\n        ):\n            self.walk(node)\n", "def test_wildcard_import_allowed_with_all(self):\n    code = \"\"\"\n    from module_with_all import *\n    \"\"\"\n    node = astroid.extract_node(code)\n    imported_module = astroid.Module(name=\"module_with_all\", doc=\"\")\n    imported_module.locals[\"__all__\"] = [astroid.Const(\"some_value\")]\n    checker = self.checker\n    checker.linter.config.allow_wildcard_with_all = True\n    with self.assertNoMessages():\n        checker._check_wildcard_imports(node, imported_module)\n"], "sample_645": ["def test_get_log_level_for_setting(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n\n            parser.addini(\"log_cli_level\", type=\"string\")\n    \"\"\"\n    )\n    config = pytester.parseconfigure()\n    assert get_log_level_for_setting(config, \"log_cli_level\") is None\n\n    pytester.makeini(\"[pytest]\\nlog_cli_level = DEBUG\")\n    config = pytester.parseconfigure()\n    assert get_log_level_for_setting(config, \"log_cli_level\") == logging.DEBUG\n\n    pytester.makeini(\"[pytest]\\nlog_cli_level = 10\")\n    config = pytester.parseconfigure()\n    assert get_log_level_for_setting(config, \"log_cli_level\") == logging.DEBUG\n\n    with pytest.raises(UsageError):\n        pytester.makeini(\"[pytest]\\nlog_cli_level = invalid\")\n        config = pytester.parseconfigure()\n        get_log_level_for_setting(config, \"log_cli_level\")\n", "def test_caplog_record_tuples(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n\n            logging.getLogger(__name__).info(\"bar\")\n            assert caplog.record_tuples == [(\"test_foo\", 20, \"bar\")]\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)\n", "def test_caplog_set_level(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n        import pytest\n\n            caplog.set_level(logging.INFO, logger=\"foo\")\n            logging.getLogger(\"foo\").info(\"info message\")\n            assert len(caplog.records) == 1\n            assert caplog.records[0].levelname == \"INFO\"\n\n            caplog.set_level(logging.INFO)\n            logging.info(\"info message\")\n            assert len(caplog.records) == 1\n            assert caplog.records[0].levelname == \"INFO\"\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    assert result.ret == 0\n", "def test_caplog_set_level(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n        import pytest\n\n            caplog.set_level(logging.INFO, logger=\"root\")\n            logging.getLogger(\"root\").info(\"simple\")\n            assert len(caplog.records) == 1\n            caplog.clear()\n            caplog.set_level(logging.INFO)\n            logging.info(\"simple\")\n            assert len(caplog.records) == 1\n    \"\"\"\n    )\n    result = pytester.runpytest(\"-p\", \"no:pytest-logging\")\n    assert result.ret == 0\n", "def test_caplog_set_level_with_logger(caplog):\n    caplog.set_level(logging.INFO, logger=\"foo\")\n    logger = logging.getLogger(\"foo\")\n    logger.debug(\"not shown\")\n    logger.info(\"shown\")\n    assert \"not shown\" not in caplog.text\n    assert \"shown\" in caplog.text\n\n    caplog.set_level(logging.DEBUG, logger=\"foo\")\n    logger.debug(\"now shown\")\n    assert \"now shown\" in caplog.text\n"], "sample_646": ["def test_unittest_skip_test(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        class MyTest(unittest.TestCase):\n            @unittest.skip(\"skipped\")\n                assert 0\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    assert result.ret == ExitCode.OK\n    result.stdout.fnmatch_lines([\"*1 skipped in *\"])\n", "def test_unittest_skip(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        class TestFoo(unittest.TestCase):\n            @unittest.skip(\"skipped\")\n                assert 0\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    result.assert_outcomes(skipped=1)\n    assert result.ret == ExitCode.OK\n", "def test_trial_runtest_protocol(pytester: Pytester, monkeypatch: MonkeyPatch) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n        from twisted.trial import unittest as twisted_unittest\n\n        class Test(twisted_unittest.TestCase):\n                pass\n    \"\"\"\n    )\n\n        return True\n\n    monkeypatch.setattr(\n        \"pytest.junitxml.check_testcase_implements_trial_reporter\",\n        check_testcase_implements_trial_reporter_mock,\n    )\n\n    result = pytester.runpytest(\"--junit-xml=path\")\n    assert result.ret == ExitCode.OK\n", "def test_unittest_skip_reason(\n    pytester: Pytester, monkeypatch: MonkeyPatch", "def test_unittest_skip_at_class_level(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        @unittest.skip(\"skip reason\")\n        class TestFoo(unittest.TestCase):\n                pass\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    assert result.ret == ExitCode.OK\n    result.stdout.fnmatch_lines([\"*1 skipped in *\"])\n"], "sample_647": ["def test_unformatted_warning_formatting():\n    unformatted_warning = warning_types.UnformattedWarning(\n        category=warning_types.PytestWarning, template=\"This is a {adjective} warning\"\n    )\n    formatted_warning = unformatted_warning.format(adjective=\"test\")\n    assert isinstance(formatted_warning, warning_types.PytestWarning)\n    assert str(formatted_warning) == \"This is a test warning\"\n", "def test_warn_explicit_for(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import warnings\n        from _pytest import warning_types\n\n            pass\n\n        warning_types.warn_explicit_for(my_function, warning_types.PytestWarning(\"my message\"))\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    assert result.ret == 0\n    assert \"my message\" in result.stdout.str()\n    assert \"at test_warn_explicit_for.py:5\" in result.stdout.str()\n", "def test_warn_explicit_for(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n            pass\n\n        pytest.warn_explicit_for(my_function, pytest.PytestWarning(\"some warning\"))\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines(\"*some warning*\")\n", "def test_unformatted_warning_formatting(warning_class):\n    template = \"Something went wrong with {variable}\"\n    unformatted_warning = warning_types.UnformattedWarning(warning_class, template)\n    formatted_warning = unformatted_warning.format(variable=\"test\")\n    assert isinstance(formatted_warning, warning_class)\n    assert str(formatted_warning) == template.format(variable=\"test\")\n", "def test_warn_explicit_for(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import warnings\n        from _pytest.warning_types import warn_explicit_for, PytestWarning\n\n            pass\n\n        warn_explicit_for(foo, PytestWarning(\"some message\"))\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    assert result.ret == 0\n    assert \"some message\" in result.stdout.str()\n    assert \"at test_warn_explicit_for.py:5\" in result.stdout.str()\n"], "sample_648": ["def test_mark_decorator_with_args(self) -> None:\n    mark = MarkDecorator(Mark(\"test_mark\", (), {}, _ispytest=True), _ispytest=True)\n    new_mark = mark.with_args(\"arg1\", \"arg2\", kwarg1=\"value1\")\n    assert new_mark.name == \"test_mark\"\n    assert new_mark.args == (\"arg1\", \"arg2\")\n    assert new_mark.kwargs == {\"kwarg1\": \"value1\"}\n", "def test_mark_with_parametrize(self, pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.parametrize(\"arg\", [\"a\", \"b\"])\n            pass\n        \"\"\"\n    )\n    result = pytester.runpytest(\"--collect-only\")\n    assert result.ret == ExitCode.OK\n    result.stdout.fnmatch_lines([\"*<Function test_func[a]>*\", \"*<Function test_func[b]>\"])\n", "def test_mark_init_bad_name(self, pytester: Pytester) -> None:\n    mark = MarkGenerator()\n    with pytest.raises(AttributeError):\n        getattr(mark, \"_ LeadingUnderscore\")\n\n    with pytest.raises(AttributeError):\n        mark.__getattr__(\"_ LeadingUnderscore\")\n", "    def test_mark_decorator_with_args(self):\n        \"\"\"Test MarkDecorator with arguments.\"\"\"\n        mg = MarkGenerator()\n        mark = mg.foo(\"bar\")\n        assert mark.name == \"foo\"\n        assert mark.args == (\"bar\",)\n        assert mark.kwargs == {}\n", "def test_mark_decorator_combined_with(self) -> None:\n    mark1 = MarkGenerator().mark1(\"arg1\", kwarg1=\"value1\")\n    mark2 = MarkGenerator().mark1(\"arg2\", kwarg2=\"value2\")\n    combined_mark = mark1.combined_with(mark2)\n    assert combined_mark.name == \"mark1\"\n    assert combined_mark.args == (\"arg1\", \"arg2\")\n    assert combined_mark.kwargs == {\"kwarg1\": \"value1\", \"kwarg2\": \"value2\"}\n"], "sample_649": ["def test_logging_plugin_init(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n\n            logging.warning(\"bar\")\n    \"\"\"\n    )\n    result = pytester.runpytest(\"--log-cli-level=WARNING\")\n    assert result.ret == ExitCode.OK\n    result.stdout.fnmatch_lines([\"*bar*\"])\n\n    # Test that the log format is used.\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        log_format = %(levelname)s %(message)s\n        \"\"\"\n    )\n    result = pytester.runpytest(\"--log-cli-level=WARNING\")\n    assert result.ret == ExitCode.OK\n    result.stdout.fnmatch_lines([\"WARNING bar\"])\n\n    # Test that the log date format is used.\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        log_date_format = %Y-%m-%d\n        \"\"\"\n    )\n    result = pytester.runpytest(\"--log-cli-level=WARNING\")\n    assert result.ret == ExitCode.OK\n    result.stdout.fnmatch_lines([\"*YYYY-MM-DD*\"])\n", "def test_caplog_set_level(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n            caplog.set_level(logging.INFO, logger='root')\n            logging.getLogger('root').info('hello')\n            assert 'hello' in caplog.text\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)\n", "def test_logging_plugin_set_log_path(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n\n            logging.info(\"info\")\n    \"\"\"\n    )\n    log_file = \"test.log\"\n    pytester.mkdir(\"logs\")\n    result = pytester.runpytest(f\"--log-file={os.path.join('logs', log_file)}\")\n    assert result.ret == 0\n    with open(os.path.join(pytester.path, \"logs\", log_file)) as f:\n        log_content = f.read()\n    assert \"info\" in log_content\n", "def test_log_cli_enabled(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n\n            logging.info(\"info\")\n            logging.warning(\"warning\")\n            logging.error(\"error\")\n        \"\"\"\n    )\n    result = pytester.runpytest(\"--log-cli-level=INFO\")\n    assert \"info\" in result.stdout.str()\n    assert \"warning\" in result.stdout.str()\n    assert \"error\" in result.stdout.str()\n\n    result = pytester.runpytest(\"--log-cli-level=WARNING\")\n    assert \"info\" not in result.stdout.str()\n    assert \"warning\" in result.stdout.str()\n    assert \"error\" in result.stdout.str()\n", "def test_catching_logs(caplog: pytest.LogCaptureFixture) -> None:\n    \"\"\"Test the catching_logs context manager.\"\"\"\n    log_message = \"Log message\"\n    with catching_logs(caplog.handler, level=logging.INFO):\n        logging.info(log_message)\n    assert len(caplog.records) == 1\n    assert caplog.records[0].getMessage() == log_message\n"], "sample_650": ["def test_catch_logs_handler_level(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n\n            caplog.set_level(logging.INFO, logger='root')\n            logging.getLogger('root').info('Info message')\n            assert len(caplog.records) == 1\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    assert result.ret == ExitCode.OK\n", "def test_catch_logs_with_invalid_level(capsys, caplog):\n    with caplog.at_level(\" invalid_level\"):\n        logging.getLogger().info(\"should not be captured\")\n    assert \"should not be captured\" not in capsys.readouterr().out\n    assert caplog.records == []\n", "def test_catching_logs_with_non_standard_level_string(capsys):\n    class CustomLogger(logging.Logger):\n            super().__init__(name)\n            self.messages = []\n\n            self.messages.append(msg)\n\n    logger = CustomLogger(\"custom\")\n    with catching_logs(LogCaptureHandler(), level=\"CUSTOM\") as handler:\n        logger.log(logging.INFO, \"Info message\")\n\n    out, err = capsys.readouterr()\n    assert not out\n    assert not err\n    assert len(handler.records) == 1\n", "def test_caplog_captures_log_records(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n\n            logging.getLogger().info(\"hello\")\n            assert len(caplog.records) == 1\n            assert caplog.records[0].getMessage() == \"hello\"\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)\n", "def test_log_cli_enabled(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n\n            logging.warning(\"WARNING\")\n            assert \"WARNING\" in caplog.text\n    \"\"\"\n    )\n    result = pytester.runpytest(\"--log-cli-level=INFO\")\n    assert result.ret == ExitCode.OK\n    assert \"WARNING\" in result.stdout.str()\n"], "sample_651": ["def test_warnings_checker_matches(pytester: Pytester) -> None:\n    with WarningsRecorder() as recorder:\n        pytester.makepyfile(\n            \"\"\"\n            import warnings\n            warnings.warn(\"some warning\", UserWarning)\n            warnings.warn(\"another warning\", DeprecationWarning)\n            \"\"\"\n        )\n        pytester.runpytest()\n    \n    checker = WarningsChecker((UserWarning, DeprecationWarning))\n    assert checker.matches(recorder.list[0])\n    assert not checker.matches(recorder.list[1])\n\n    checker = WarningsChecker(UserWarning)\n    assert checker.matches(recorder.list[0])\n    assert not checker.matches(recorder.list[1])\n", "def test_warnings_recorder_list(\n    pytester: Pytester, recwarn: WarningsRecorder", "def test_warns_message_repr(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import warnings\n        import pytest\n\n            warnings.warn(\"foo\")\n\n            with pytest.warns(UserWarning, match=\"bar\"):\n                pass\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines(\"*E       Failed: DID NOT WARN. No warnings of type *\")\n", "def test_recwarn_list(tmp_path):\n    \"\"\"Test that recwarn returns a list of warnings.WarningMessage objects\"\"\"\n    pytester = Pytester(tmp_path)\n    pytester.makepyfile(\n        \"\"\"\n        import warnings\n            warnings.warn(\"some warning\")\n            assert len(recwarn) == 1\n            assert isinstance(recwarn[0], warnings.WarningMessage)\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)\n", "def test_warns_context_manager_match_re(recwarn: WarningsRecorder) -> None:\n    with pytest.warns(UserWarning, match=r\"must be \\d+$\"):\n        warnings.warn(\"value must be 42\", UserWarning)\n    assert len(recwarn.list) == 1\n    warning = recwarn.pop(UserWarning)\n    assert str(warning.message) == \"value must be 42\"\n"], "sample_652": ["def test_show_fixtures_per_test(pytester):\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture\n            return \"hello\"\n\n            assert hello == \"hello\"\n\n            assert hello == \"hello\"\n    \"\"\"\n    )\n    result = pytester.runpytest(\"--fixtures-per-test\")\n    assert result.ret == 0\n    result.stdout.fnmatch_lines([\"*fixtures used by test_1*\", \"*fixtures used by test_2*\"])\n", "def test_show_fixtures_per_test(pytester):\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture\n            \"\"\"fix1 doc\"\"\"\n    \"\"\"\n    )\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture\n            \"\"\"fix2 doc\"\"\"\n\n            pass\n    \"\"\"\n    )\n    result = pytester.runpytest(\"--fixtures-per-test\")\n    assert result.ret == 0\n    result.stdout.fnmatch_lines([\"*fix1 -- conftest.py*\", \"*fix2 -- test_foo.py*\"])\n    assert \"no docstring available\" not in result.stdout.str()\n", "def test_pytest_generate_tests_issue1261(testdir):\n    # issue 1261: metafunc.parametrize() should not raise an error when the\n    # parametrized function is already a generator.\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n            metafunc.parametrize('arg', [1, 2])\n\n            yield", "def test_show_fixtures_per_test(pytester):\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture\n            return \"hello\"\n\n            pass\n\n            pass\n    \"\"\"\n    )\n    result = pytester.runpytest(\"--fixtures-per-test\")\n    assert result.ret == 0\n    output = result.stdout.str()\n    assert \"fixtures used by test_1\" in output\n    assert \"fixtures used by test_2\" in output\n    assert \"no fixtures found\" not in output\n    assert \"hello\" in output\n", "def test_pyobj_has_docstring():\n    \"\"\"Test that PyobjMixin classes have a docstring (#2667)\"\"\"\n    assert PyobjMixin.__doc__ is not None\n    assert PyCollector.__doc__ is not None\n    assert Module.__doc__ is not None\n    assert Class.__doc__ is not None\n    assert Instance.__doc__ is not None\n    assert FunctionMixin.__doc__ is not None\n    assert Function.__doc__ is not None\n"], "sample_653": ["def test_log_capture_fixture_records():\n    class TestItem:\n            self.catch_log_handler = None\n\n    item = TestItem()\n    fixture = LogCaptureFixture(item)\n    handler = LogCaptureHandler()\n    item.catch_log_handler = handler\n\n    record1 = logging.LogRecord(\"name\", logging.INFO, \"path\", 10, \"message1\", None, None)\n    record2 = logging.LogRecord(\"name\", logging.INFO, \"path\", 10, \"message2\", None, None)\n    handler.handle(record1)\n    handler.handle(record2)\n\n    assert len(fixture.records) == 2\n    assert fixture.records[0] == record1\n    assert fixture.records[1] == record2\n", "def test_catching_logs_context_handler_without_level_and_formatter():\n    handler = logging.StreamHandler()\n    original_level = handler.level\n    original_formatter = handler.formatter\n\n    with catching_logs(handler) as result_handler:\n        assert result_handler is handler\n        assert handler.level == original_level\n        assert handler.formatter == original_formatter\n", "def test_get_option_ini(config):\n    config.option.log_level = None\n    assert get_option_ini(config, \"log_level\") is None\n\n    config.option.log_level = \"INFO\"\n    assert get_option_ini(config, \"log_level\") == \"INFO\"\n\n    del config.option.log_level\n    config.inicfg[\"log_level\"] = \"DEBUG\"\n    assert get_option_ini(config, \"log_level\") == \"DEBUG\"\n", "def test_catching_logs_context_manager(tmpdir):\n    handler = logging.StreamHandler()\n    formatter = logging.Formatter()\n    log_file = tmpdir.join(\"log.txt\")\n\n    with catching_logs(handler, formatter=formatter) as handler:\n        logging.getLogger().error(\"Error message\")\n        handler.flush()\n\n    assert len(handler.records) == 1\n\n    with open(log_file, \"w\") as f:\n        with catching_logs(handler, formatter=formatter) as handler:\n            logging.getLogger().error(\"Error message\")\n            handler.stream = f\n            handler.flush()\n\n    with open(log_file, \"r\") as f:\n        contents = f.read()\n\n    assert len(handler.records) == 1\n    assert \"Error message\" in contents\n", "def test_catching_logs_context_handler_level():\n    handler = logging.StreamHandler()\n    with catching_logs(handler, level=logging.INFO) as handler:\n        assert handler.level == logging.INFO\n"], "sample_654": ["def test_fixture_request_ordering(testdir):\n    testdir.makeconftest(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture(scope='session', autouse=True)\n            pass\n\n        @pytest.fixture(scope='module', autouse=True)\n            pass\n\n        @pytest.fixture(scope='function', autouse=True)\n            pass\n    \"\"\"\n    )\n\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n            assert list(request._fixturemanager._arg2fixturedefs['s1']) == [\n                request._fixture_defs['s1'][0]\n            ]\n            assert list(request._fixturemanager._arg2fixturedefs['m1']) == [\n                request._fixture_defs['m1'][0]\n            ]\n            assert list(request._fixturemanager._arg2fixturedefs['f1']) == [\n                request._fixture_defs['f1'][0]\n            ]\n    \"\"\"\n    )\n\n    result = testdir.runpytest()\n    result.assert_outcomes(passed=1)\n", "def test_fixture_lookup_error_repr(monkeypatch):\n        raise FixtureLookupError(argname, request)\n\n    class DummyFixtureRequest:\n        _pyfuncitem = None\n\n    monkeypatch.setattr(fixtures, 'FixtureLookupError', raise_fixture_lookup_error)\n    lookup_err = raise_fixture_lookup_error('argname', DummyFixtureRequest())\n    expected_repr = \"\"\"<FixtureLookupError argname='argname' request=<DummyFixtureRequest>>\"\"\"\n    assert repr(lookup_err) == expected_repr\n", "def test_fixture_lookup_error_repr(monkeypatch):\n    class DummyFixtureRequest:\n        _pyfuncitem = None\n\n            self._fixture_defs = {}\n            self.fixturename = \"some_fixture\"\n            self.argname = \"arg\"\n\n    class DummyFixtureDef:\n            self.argname = argname\n            self.func = lambda: None\n\n    fr = DummyFixtureRequest()\n    fd = DummyFixtureDef(\"arg\")\n\n    monkeypatch.setattr(fixtures, \"_getlocation\", lambda func: \"somewhere\")\n    fe = FixtureLookupError(fr, fd)\n    expected = \"FixtureLookupError: \\nsome_fixture (fixtures used: some_fixture)\"\n    assert repr(fe) == expected\n", "def test_fixture_function_not_called_directly(testdir):\n    testdir.makeconftest(\"\"\"\n        import pytest\n\n        @pytest.fixture\n            assert False, \"should not be called directly\"\n    \"\"\")\n\n    testdir.makepyfile(\"\"\"\n        from conftest import my_fixture\n\n            pass\n    \"\"\")\n\n    result = testdir.runpytest()\n    assert result.ret == 0\n", "def test_fixture_scope_mismatch_error_message():\n    testdir = pytester.Testdir()\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture(scope='session')\n            return 1\n\n            assert foo == 1\n    \"\"\"\n    )\n    result = testdir.runpytest(\"--fixtures-per-test\")\n    result.stdout.fnmatch_lines(\"E       ScopeMismatch: You tried to access the 'session' scoped fixture 'foo' with a 'function' scoped request object, involved factories\")\n\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture(scope='package')\n            return 1\n\n            assert foo == 1\n    \"\"\"\n    )\n    result = testdir.runpytest(\"--fixtures-per-test\")\n    result.stdout.fnmatch_lines(\"E       ScopeMismatch: You tried to access the 'package' scoped fixture 'foo' with a 'function' scoped request object, involved factories\")\n\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture(scope='module')\n            return 1\n\n            assert foo == 1\n    \"\"\"\n    )\n    result = testdir.runpytest(\"--fixtures-per-test\")\n    result.stdout.fnmatch_lines(\"E       ScopeMismatch: You tried to access the 'module' scoped fixture 'foo' with a 'function' scoped request object, involved factories\")\n\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture(scope='class')\n            return 1\n\n            assert foo == 1\n    \"\"\"\n    )\n    result = testdir.runpytest(\"--fixtures-per-test\")\n    result.stdout.fnmatch_lines(\"E       ScopeMismatch: You tried to access the 'class' scoped fixture 'foo' with a 'function' scoped request object, involved factories\")\n"], "sample_655": ["def test_capturing_disabled(caplog):\n    capman = CaptureManager(\"no\")\n    assert not capman.is_globally_capturing()\n    assert capman._global_capturing is None\n    capman.start_global_capturing()\n    assert capman._global_capturing is not None\n    print(\"hello\")\n    out, err = capman.suspend_global_capture(in_=False)\n    assert out == \"\"\n    assert err == \"\"\n    assert \"hello\" in caplog.text\n", "def test_capturing_enabled_by_default(monkeypatch, pytestconfig):\n    class DummyCollector(pytest.Collector):\n            super(DummyCollector, self).__init__(\"dummy\")\n\n    capman = CaptureManager(\"fd\")\n    capman._global_capturing = True\n    monkeypatch.setattr(pytestconfig.pluginmanager, \"getplugin\", lambda x: capman)\n    collector = DummyCollector()\n    out = io.BytesIO()\n    err = io.BytesIO()\n    with StdCaptureFD(out=False, err=False):\n        with StdCapture(out=True, err=True) as capture:\n            capman.pytest_make_collect_report(collector)\n    assert capture.out.getvalue() == \"\"\n    assert capture.err.getvalue() == \"\"\n    assert not out.getvalue()\n    assert not err.getvalue()\n", "def test_capturing_resume_suspend(capsys):\n    capture_manager = CaptureManager(\"fd\")\n    capture_manager.start_global_capturing()\n    print(\"Hello World\")\n    capture_manager.suspend_global_capture()\n    assert capsys.readouterr().out == \"Hello World\\n\"\n    capture_manager.resume_global_capture()\n    print(\"Resume Capturing\")\n    capture_manager.stop_global_capturing()\n    assert capsys.readouterr().out == \"Resume Capturing\\n\"\n", "def test_capture_manager_fixture_setup_teardown(capsys):\n    \"\"\"Test that the capturemanager fixture is properly reset during teardown.\"\"\"\n    capmanager = CaptureManager(\"fd\")\n    try:\n        with capsys.disabled():\n            sys.stdout.write(\"out\\n\")\n            sys.stderr.write(\"err\\n\")\n        out, err = capmanager.read_global_capture()\n        assert out == \"out\\n\"\n        assert err == \"err\\n\"\n    finally:\n        capmanager.stop_global_capturing()\n", "def test_captured_fd_restore_cwd(tmpdir):\n    \"\"\"Test that captured FDs are restored when the current working directory changes (#6657).\"\"\"\n    capture_manager = CaptureManager(\"fd\")\n    with StdCaptureFD() as capture:\n        sys.stdout.write(\"hello\")\n        assert os.getcwd() == str(py.path.local())\n        tmpdir.chdir()\n        try:\n            assert os.getcwd() == str(tmpdir)\n            capture_manager.suspend_global_capture(in_=False)\n            assert os.read(1, 10) == b\"hello\"\n        finally:\n            capture_manager.resume_global_capture()\n"], "sample_656": ["def test_capture_manager_fixture_setup_and_teardown(capsys):\n    capman = CaptureManager(\"fd\")\n    with capsys.disabled():\n        capman.start_global_capturing()\n        item = pytest.Item(\"test_func\", None)\n        capman._current_item = item\n        capman.activate_fixture(item)\n\n    out, err = capsys.readouterr()\n    assert not out\n    assert not err\n\n    # Ensure capture is active.\n    print(\"Test output\")\n    sys.stderr.write(\"Test error\")\n\n    with capsys.disabled():\n        capman.deactivate_fixture(item)\n        capman.suspend_global_capture(in_=False)\n        out, err = capman.read_global_capture()\n\n    assert out == \"Test output\\n\"\n    assert err == \"Test error\"\n", "def test_capture_manager_resume_global_capture():\n    capman = CaptureManager(\"fd\")\n    capman.start_global_capturing()\n    capman.suspend_global_capture()\n    assert capman._global_capturing._state == \"suspended\"\n    capman.resume_global_capture()\n    assert capman._global_capturing._state == \"resumed\"\n", "def test_capturing_call_suspended_before_resume(capsys):\n    capmanager = CaptureManager(\"fd\")\n    capmanager.start_global_capturing()\n    print(\"before\")\n    capmanager.suspend_global_capture(in_=True)\n    capsys.readouterr()  # should not raise\n    capmanager.resume_global_capture()\n    print(\"after\")\n    out, err = capmanager.read_global_capture()\n    assert out == \"before\\nafter\\n\"\n", "def test_capture_manager_fixture_control(capsys):\n    \"\"\"Test that the CaptureManager controls the fixture's capturing correctly.\"\"\"\n    capman = CaptureManager(\"fd\")\n    capsys._start()\n    out, err = capsys.readouterr()\n    assert out == \"\"\n    assert err == \"\"\n    capman.suspend_fixture(capsys)\n    print(\"Hello, stdout!\")\n    print(\"Hello, stderr!\", file=sys.stderr)\n    out, err = capsys.readouterr()\n    assert out == \"\"\n    assert err == \"\"\n    capman.resume_fixture(capsys)\n    out, err = capsys.readouterr()\n    assert out == \"Hello, stdout!\\n\"\n    assert err == \"Hello, stderr!\\n\"\n    capsys.close()\n", "def test_capture_manager_suspend_resume(capsys):\n    capman = CaptureManager(\"fd\")\n    capman.start_global_capturing()\n    print(\"Hello\")\n    capman.suspend_global_capture(in_=True)\n    print(\"World\", file=sys.stderr)\n    capman.resume_global_capture()\n    print(\"after\")\n    out, err = capman.read_global_capture()\n    assert out == \"Hello\\nafter\\n\"\n    assert err == \"World\\n\"\n"], "sample_657": ["def test_mark_decorator():\n    mark_name = \"test_mark\"\n    mark_decorator = MarkDecorator(Mark(mark_name, (), {}))\n\n        pass\n\n    marked_test_func = mark_decorator(test_func)\n    assert marked_test_func is test_func\n\n    mark_info = get_unpacked_marks(test_func)[0]\n    assert mark_info.name == mark_name\n    assert mark_info.args == ()\n    assert mark_info.kwargs == {}\n", "def test_mark_decorator_with_single_callable_argument():\n    mark_decorator = MarkDecorator(Mark(\"test\", (), {}))\n\n        pass\n\n    marked_test_function = mark_decorator(test_function)\n\n    assert marked_test_function is test_function\n    assert get_unpacked_marks(test_function) == [Mark(\"test\", (), {})]\n", "def test_mark_decorator_with_single_callable_argument():\n    mark = MarkDecorator(Mark(\"test\", (), {}))\n        pass\n    marked_test_func = mark(test_func)\n    assert marked_test_func is test_func\n    assert get_unpacked_marks(test_func) == [Mark(\"test\", (), {})]\n", "    def test_mark_decorator_with_args(self):\n        mark = MarkDecorator(Mark(\"test\", (), {}))\n        new_mark = mark.with_args(1, 2, three=3)\n        assert new_mark.mark.name == \"test\"\n        assert new_mark.mark.args == (1, 2)\n        assert new_mark.mark.kwargs == {\"three\": 3}\n", "def test_mark_with_args(self):\n    mark = Mark().with_args(\"arg1\", \"arg2\", kwarg=\"value\")\n    assert mark.mark.args == (\"arg1\", \"arg2\")\n    assert mark.mark.kwargs == {\"kwarg\": \"value\"}\n"], "sample_658": ["def test_doctest_unwrap_mock_aware():\n    with _patch_unwrap_mock_aware():\n        assert inspect.unwrap(lambda x: x) == lambda x: x\n\n    mock_obj = object()\n    setattr(mock_obj, \"pytest_mock_example_attribute_that_shouldnt_exist\", True)\n    assert _is_mocked(mock_obj)\n\n        pass\n\n    orig_unwrap = inspect.unwrap\n    inspect.unwrap = lambda x: wrapped_func\n    try:\n        assert _patch_unwrap_mock_aware().__enter__() is None\n        assert inspect.unwrap(mock_obj) == mock_obj\n    finally:\n        inspect.unwrap = orig_unwrap\n", "def test_get_optionflags():\n    class MockConfig:\n            if name == \"doctest_optionflags\":\n                return [\"ELLIPSIS\", \"IGNORE_EXCEPTION_DETAIL\"]\n            raise MODULE_NOT_FOUND_ERROR\n\n    parent = MockConfig()\n    flags = DoctestModule.get_optionflags(parent)\n    import doctest\n    assert flags & doctest.ELLIPSIS\n    assert flags & doctest.IGNORE_EXCEPTION_DETAIL\n", "def test_get_optionflags():\n    class Config:\n            if name == \"doctest_optionflags\":\n                return [\"ELLIPSIS\", \"NORMALIZE_WHITESPACE\"]\n            return None\n\n    parent = type(\"Parent\", (), {\"config\": Config()})\n    flags = DoctestModule.get_optionflags(parent)\n    import doctest\n\n    assert flags & doctest.ELLIPSIS\n    assert flags & doctest.NORMALIZE_WHITESPACE\n", "def test_get_runner():\n    runner1 = _get_runner()\n    runner2 = _get_runner()\n    assert runner1 is not runner2\n    assert isinstance(runner1, _init_runner_class())\n", "def test_doctest_module_collects_from_module_with_mocks(monkeypatch):\n    class Mock:\n            pass\n\n    monkeypatch.setattr(inspect, \"unwrap\", lambda obj: obj)\n    doctest_module = DoctestModule(\"path/to/module.py\")\n    with _patch_unwrap_mock_aware():\n        items = list(doctest_module.collect())\n    assert len(items) > 0\n    for item in items:\n        assert isinstance(item, DoctestItem)\n"], "sample_659": ["def test_getrawcode_ignores_builtins_and_non_python_functions():\n    import builtins\n\n    assert getrawcode(builtins.len) is None\n    assert getrawcode(1) is None\n    assert getrawcode(None) is None\n", "def test_reprcompare_not_important_for_traceback_entry():\n    entry = TracebackEntry(None)\n    assert repr(entry) == \"<TracebackEntry ????:?>\"\n", "def test_reprcompare_not_same_instance():\n    x = ExceptionInfo.from_current(None)\n    y = ExceptionInfo.from_current(None)\n    assert repr(x) != repr(y)\n", "def test_raises_match():\n    with pytest.raises(ValueError) as excinfo:\n        raise ValueError(\"text\")\n    assert excinfo.match(\"text\")\n    assert not excinfo.match(\"other text\")\n\n    with pytest.raises(AssertionError):\n        excinfo.match(\"other text\")\n", "def test_getrawcode():\n        pass\n\n    assert getrawcode(foo) is foo.__code__\n    assert getrawcode(type(\"bar\", (object,), {\"__call__\": lambda: None})) is not None\n"], "sample_660": ["def test_record_property(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n            record_property(\"example_key\", 1)\n    \"\"\"\n    )\n    result, dom = runandparse(testdir)\n    properties_node = (\n        dom.find_first_by_tag(\"testsuite\")\n        .find_first_by_tag(\"testcase\")\n        .find_first_by_tag(\"properties\")\n    )\n    assert properties_node is not None\n    property_node = properties_node.find_first_by_tag(\"property\")\n    assert property_node is not None\n    assert_attr(property_node, name=\"example_key\", value=\"1\")\n", "def test_add_global_property(testdir):\n    resultpath = testdir.tmpdir.join(\"junit.xml\")\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n            record_testsuite_property(\"ARCH\", \"PPC\")\n            record_testsuite_property(\"STORAGE_TYPE\", \"CEPH\")\n    \"\"\"\n    )\n    result = testdir.runpytest(\"--junitxml=%s\" % resultpath)\n    assert result.ret == 0\n    xmldoc = minidom.parse(str(resultpath))\n    node = DomNode(xmldoc)\n    properties_node = node.find_first_by_tag(\"properties\")\n    assert properties_node is not None\n    properties = properties_node.children\n    assert len(properties) == 2\n    assert_attr(properties[0], name=\"ARCH\", value=\"PPC\")\n    assert_attr(properties[1], name=\"STORAGE_TYPE\", value=\"CEPH\")\n", "def test_mangle_test_address():\n    address = \"path/to/test.py::TestClass::test_function[param1]\"\n    expected = [\"path.to.test\", \"TestClass\", \"test_function[param1]\"]\n    assert mangle_test_address(address) == expected\n\n    address = \"path/to/test.py::test_function\"\n    expected = [\"path.to.test\", \"test_function\"]\n    assert mangle_test_address(address) == expected\n\n    address = \"path/to/test.py::TestClass::()::test_function\"\n    expected = [\"path.to.test\", \"TestClass\", \"test_function\"]\n    assert mangle_test_address(address) == expected\n", "def test_junit_family_default(testdir):\n    \"\"\"Test that junit_family is xunit1 by default.\"\"\"\n    resultpath = testdir.tmpdir.join(\"junit.xml\")\n    testdir.runpytest(\"--junitxml=%s\" % resultpath)\n    xmldoc = minidom.parse(str(resultpath))\n    node = DomNode(xmldoc).find_first_by_tag(\"testsuite\")\n    assert_attr(node, name=\"pytest\")  # xunit1 suite has a 'name' attribute\n", "def test_node_reporter_testcase_property(testdir):\n    class MyTest(BaseReport):\n            super().__init__(*args, **kwargs)\n            self.user_properties = [(\"testcase_property\", \"some_value\")]\n\n    xml = LogXML(\"path\", None, \"testsuite\")\n    reporter = xml._NodeReporter(\"nodeid\", xml)\n\n    my_test = MyTest(\"nodeid\", None, None, None, None, None, None)\n    reporter.record_testreport(my_test)\n    reporter.add_property(\"node_property\", \"node_value\")\n\n    testcase_node = reporter.to_xml()\n    properties_node = testcase_node.find_first_by_tag(\"properties\")\n    assert len(properties_node.children) == 2\n\n    node_property = properties_node.children[0]\n    assert node_property[\"name\"] == \"node_property\"\n    assert node_property[\"value\"] == \"node_value\"\n\n    testcase_property = properties_node.children[1]\n    assert testcase_property[\"name\"] == \"testcase_property\"\n    assert testcase_property[\"value\"] == \"some_value\"\n"], "sample_661": ["def test_mangle_test_address():\n    address = \"path/to/test.py::TestClass::test_method[params]\"\n    expected = [\"path.to.test\", \"TestClass\", \"test_method[params]\"]\n    assert mangle_test_address(address) == expected\n\n    address = \"path/to/test.py::test_function\"\n    expected = [\"path.to.test\", \"test_function\"]\n    assert mangle_test_address(address) == expected\n\n    address = \"path/to/test.py::TestClass::()::test_method\"\n    expected = [\"path.to.test\", \"TestClass\", \"test_method\"]\n    assert mangle_test_address(address) == expected\n", "def test_junitxml_property(testdir):\n    testdir.makepyfile(\n        \"\"\"\n            record_property(\"example_key\", 1)\n    \"\"\"\n    )\n    result, dom = runandparse(testdir)\n    node = dom.find_first_by_tag(\"testcase\")\n    props = node.find_first_by_tag(\"properties\")\n    assert props is not None\n    prop_node = props.find_first_by_tag(\"property\")\n    assert_attr(prop_node, name=\"example_key\", value=\"1\")\n", "def test_pytest_internalerror(testdir):\n    resultpath = testdir.tmpdir.join(\"junit.xml\")\n    testdir.makepyfile(\n        \"\"\"\n            raise Exception(\"Something went wrong\")\n        \"\"\"\n    )\n    result = testdir.runpytest(\"--junitxml=%s\" % resultpath)\n    assert result.ret == 1\n\n    xmldoc = minidom.parse(str(resultpath))\n    suite_node = DomNode(xmldoc).find_first_by_tag(\"testsuite\")\n    assert suite_node is not None\n\n    testcase_nodes = suite_node.find_by_tag(\"testcase\")\n    assert len(testcase_nodes) == 1\n\n    testcase_node = testcase_nodes[0]\n    assert testcase_node.find_first_by_tag(\"error\") is not None\n", "def test_pytest_internalerror(testdir):\n    resultpath = testdir.tmpdir.join(\"junit.xml\")\n    result = testdir.runpytest(\"--junitxml=%s\" % resultpath, \"pytest_internalerror_test.py\")\n    xmldoc = minidom.parse(str(resultpath))\n    node = DomNode(xmldoc).find_first_by_tag(\"testsuite\").get_unique_child\n    assert node.tag == \"testcase\"\n    assert node[\"name\"] == \"internal\"\n    assert node[\"classname\"] == \"pytest\"\n    failure_node = node.find_first_by_tag(\"error\")\n    assert failure_node is not None\n    assert failure_node[\"message\"] == \"internal error\"\n", "def test_internalerror(testdir):\n    resultpath = testdir.tmpdir.join(\"junit.xml\")\n    testdir.makepyfile(\"def test_foo(): 0/0\")\n    result = testdir.runpytest(\"--junitxml=%s\" % resultpath)\n    assert result.ret == 1\n\n    xmldoc = minidom.parse(str(resultpath))\n    node = DomNode(xmldoc)\n\n    suite_node = node.find_first_by_tag(\"testsuite\")\n    assert_attr(\n        suite_node,\n        name=\"pytest\",\n        errors=\"1\",\n        failures=\"0\",\n        skips=\"0\",\n        tests=\"1\",\n    )\n\n    internal_error_node = suite_node.find_first_by_tag(\"testcase\")\n    assert internal_error_node is not None\n    assert internal_error_node[\"name\"] == \"internal\"\n    error_node = internal_error_node.find_first_by_tag(\"error\")\n    assert error_node is not None\n    assert error_node[\"message\"] == \"internal error\"\n"], "sample_662": ["def test_collect_report_serialization():\n    report = CollectReport(\n        nodeid=\"node1\",\n        outcome=\"passed\",\n        longrepr=None,\n        result=[\"item1\", \"item2\"],\n        sections=[(\"section1\", \"content1\"), (\"section2\", \"content2\")],\n    )\n    data = report._to_json()\n    new_report = CollectReport._from_json(data)\n    assert new_report.nodeid == report.nodeid\n    assert new_report.outcome == report.outcome\n    assert new_report.longrepr == report.longrepr\n    assert new_report.result == report.result\n    assert new_report.sections == report.sections\n", "def test_deserialize_repr_entry_native():\n    reportdict = {\n        \"longrepr\": {\n            \"reprcrash\": {\"path\": \"foo.py\", \"lineno\": 10, \"message\": \"Error\"},\n            \"reprtraceback\": {\n                \"reprentries\": [{\"type\": \"ReprEntryNative\", \"data\": {\"lines\": [\"line1\", \"line2\"]}}],\n            },\n        }\n    }\n    kwargs = _report_kwargs_from_json(reportdict)\n    assert isinstance(kwargs[\"longrepr\"].reprtraceback.reprentries[0], ReprEntryNative)\n    assert kwargs[\"longrepr\"].reprtraceback.reprentries[0].lines == [\"line1\", \"line2\"]\n", "def test_report_serialization_with_exception_chain_repr(tmp_path):\n    # Create a temporary file for testing\n    p = tmp_path / \"test_file.py\"\n    p.write_text(\"def test_foo():\\n  raise Exception('Test exception')\")\n\n    # Create an ExceptionChainRepr instance\n    chain_repr = ExceptionChainRepr(\n        [(ReprTraceback([]), ReprFileLocation(str(p), 2, \"\"), \"Test description\")]\n    )\n\n    # Create a TestReport instance with the ExceptionChainRepr instance\n    report = TestReport(\n        nodeid=\"test_foo\",\n        location=(str(p), 1, \"\"),\n        keywords={},\n        outcome=\"failed\",\n        longrepr=chain_repr,\n        when=\"call\",\n        sections=[],\n        duration=0.5,\n    )\n\n    # Serialize and deserialize the TestReport instance\n    serialized_report = _report_to_json(report)\n    deserialized_report = TestReport._from_json(serialized_report)\n\n    # Check if the deserialized report matches the original report\n    assert deserialized_report.longrepr.chain == report.longrepr.chain\n    assert deserialized_report.nodeid == report.nodeid\n    assert deserialized_report.location == report.location\n    assert deserialized_report.keywords == report.keywords\n    assert deserialized_report.outcome == report.outcome\n    assert deserialized_report.when == report.when\n    assert deserialized_report.sections == report.sections\n    assert deserialized_report.duration == report.duration\n", "def test_report_to_json_with_exception_chain_repr():\n    report = TestReport(\n        nodeid=\"test_node\",\n        location=(\"test_file.py\", 10, \"test_function\"),\n        keywords=[\"test_keyword\"],\n        outcome=\"failed\",\n        longrepr=ExceptionChainRepr([\n            (ReprTraceback([]), ReprFileLocation(\"test_file.py\", 10), \"Test message\")\n        ]),\n        when=\"call\",\n        sections=[],\n        duration=0.5,\n        user_properties=[],\n    )\n    data = report._to_json()\n    assert data[\"$report_type\"] == \"TestReport\"\n    assert data[\"longrepr\"][\"chain\"] is not None\n    assert len(data[\"longrepr\"][\"chain\"]) == 1\n", "def test_report_unserialization_failure():\n    reportdict = {\"longrepr\": {\"type\": \"UnknownType\"}}\n    with pytest.raises(RuntimeError):\n        _report_kwargs_from_json(reportdict)\n"], "sample_663": ["def test_collect_ignore_collection_path_configured_absolute(tmp_path, pytester):\n    ignore_path = tmp_path / \"ignore_me\"\n    ignore_path.mkdir()\n\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        addopts = --ignore={}\n        \"\"\".format(\n            ignore_path\n        )\n    )\n\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n            pass\n        \"\"\"\n    )\n\n    pytester.copy_example(str(ignore_path))\n\n    result = pytester.runpytest()\n    assert result.ret == ExitCode.OK\n    assert len(result.outlines) == 2\n", "def test_collect_ignore_collect_paths(tmp_path, monkeypatch):\n    \"\"\"Test that pytest_ignore_collect is called for all paths.\"\"\"\n    collector = Session(pytest.config)\n    ignore_paths = [tmp_path / \"ignore1\", tmp_path / \"ignore2\"]\n    monkeypatch.setattr(collector.config, \"_getconftest_pathlist\", lambda *args: ignore_paths)\n\n        return path in ignore_paths\n\n    monkeypatch.setattr(collector.config.hook, \"pytest_ignore_collect\", pytest_ignore_collect)\n    assert collector.pytest_ignore_collect(ignore_paths[0], collector.config) is True\n    assert collector.pytest_ignore_collect(tmp_path / \"not_ignored\", collector.config) is False\n", "def test_collect_ignore_path(testdir):\n    p = testdir.makepyfile(\"test_1.py\", \"def test_1(): pass\")\n    ignore_path = p.dirpath().join(\"ignore_path\")\n    ignore_path.mkdir()\n    ignore_file = ignore_path.join(\"test_ignore.py\")\n    ignore_file.write(\"def test_ignore(): pass\")\n\n    testdir.makeconftest(\n        \"\"\"\n            config.addinivalue_line(\"paths\", \"ignore_path\")\n    \"\"\"\n    )\n\n    result = testdir.runpytest()\n    assert result.ret == ExitCode.OK\n    result.assert_collected(\"test_1.py::test_1\")\n    result.assert_not_collected(\"ignore_path/test_ignore.py::test_ignore\")\n", "def test_collect_ignore_collect(tmpdir, pytestconfig):\n    a_dir = tmpdir.mkdir(\"a\")\n    b_dir = tmpdir.mkdir(\"b\")\n    c_file = tmpdir.join(\"c.py\")\n\n        return path == a_dir\n\n    pytestconfig._getconftest_pathlist = lambda *args, **kwargs: [collect_ignore]\n\n    session = Session(pytestconfig)\n    items, hook = pytestconfig.pluginmanager.hook.pytest_collectreport.call_with_report(\n        pytestconfig.pluginmanager.hook.pytest_collectsession\n    )\n    assert not any(str(a_dir) in str(item.fspath) for item in items)\n    assert any(str(b_dir) in str(item.fspath) for item in items)\n    assert any(str(c_file) in str(item.fspath) for item in items)\n", "def test_collect_ignore_empty_files(tmpdir, request):\n    a_dir = tmpdir.mkdir(\"a\")\n    b_dir = tmpdir.mkdir(\"b\")\n    a_file = a_dir.join(\"test_a.py\")\n    a_file.write(\"def test_a(): pass\")\n    b_file = b_dir.join(\"test_b.py\")\n    b_file.write(\"\")\n    config = request.config\n    session = Session(config)\n    items, reprec, errors = session._collect([tmpdir], genitems=False)\n    assert len(items) == 1\n    assert items[0].fspath == a_file\n    assert not reprec\n    assert not errors\n"], "sample_664": ["def test_deprecated_external_plugins():\n    assert \"pytest_catchlog\" in deprecated.DEPRECATED_EXTERNAL_PLUGINS\n    assert \"pytest_capturelog\" in deprecated.DEPRECATED_EXTERNAL_PLUGINS\n    assert \"pytest_faulthandler\" in deprecated.DEPRECATED_EXTERNAL_PLUGINS\n", "def test_deprecated_external_plugins():\n    assert \"pytest_catchlog\" in deprecated.DEPRECATED_EXTERNAL_PLUGINS\n    assert \"pytest_capturelog\" in deprecated.DEPRECATED_EXTERNAL_PLUGINS\n    assert \"pytest_faulthandler\" in deprecated.DEPRECATED_EXTERNAL_PLUGINS\n", "def test_deprecated_external_plugins():\n    assert isinstance(deprecated.DEPRECATED_EXTERNAL_PLUGINS, set)\n    assert len(deprecated.DEPRECATED_EXTERNAL_PLUGINS) == 3\n    assert \"pytest_catchlog\" in deprecated.DEPRECATED_EXTERNAL_PLUGINS\n    assert \"pytest_capturelog\" in deprecated.DEPRECATED_EXTERNAL_PLUGINS\n    assert \"pytest_faulthandler\" in deprecated.DEPRECATED_EXTERNAL_PLUGINS\n", "def test_deprecated_external_plugins():\n    assert \"pytest_catchlog\" in deprecated.DEPRECATED_EXTERNAL_PLUGINS\n    assert \"pytest_capturelog\" in deprecated.DEPRECATED_EXTERNAL_PLUGINS\n    assert \"pytest_faulthandler\" in deprecated.DEPRECATED_EXTERNAL_PLUGINS\n", "def test_deprecated_external_plugins():\n    assert isinstance(deprecated.DEPRECATED_EXTERNAL_PLUGINS, set)\n    assert len(deprecated.DEPRECATED_EXTERNAL_PLUGINS) == 3\n    assert \"pytest_catchlog\" in deprecated.DEPRECATED_EXTERNAL_PLUGINS\n    assert \"pytest_capturelog\" in deprecated.DEPRECATED_EXTERNAL_PLUGINS\n    assert \"pytest_faulthandler\" in deprecated.DEPRECATED_EXTERNAL_PLUGINS\n"], "sample_665": ["def test_pytest_collect_file_check_if_not_a_file(tmpdir):\n    \"\"\"Test that pytest_collect_file raises an error if the given path is not a file\"\"\"\n    path = tmpdir.mkdir(\"dir\")\n    with pytest.raises(pytest.UsageError) as excinfo:\n        pytest.collect.File.from_parent(parent=None, path=path).collect()\n    assert str(excinfo.value).startswith(\"Check files for 'dir' exists and is not a directory\")\n", "def test_collectfile_uses_list_setdefault_to_create_lists_of_nodes(pytester):\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n            return [parent.path]\n\n            assert isinstance(report.result, list)\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    assert result.ret == 0\n", "compilation error", "def test_collect_report_info():\n    \"\"\"Test that collector report info is properly generated.\"\"\"\n    class DummyCollector(pytest.Collector):\n            return [pytest.Item(\"dummy\", self)]\n\n    session = Session()\n    collector = DummyCollector()\n    item = collector.collect()[0]\n\n    assert item.reportinfo() == (collector.fspath, 0, \"dummy\")\n", "def test_collect_file_check_fspath(self, tmpdir):\n    \"\"\"Test that the collect_file method checks if the given path is a file.\"\"\"\n    p = tmpdir.mkdir(\"test\")\n    collector = pytest.PyCollector(fspath=p)\n    with pytest.raises(pytest.UsageError):\n        collector.collect_file(p)\n"], "sample_666": ["def test_catching_logs_context_manager(tmpdir):\n    log_file = tmpdir.join(\"log_file.log\")\n    logger = logging.getLogger()\n    handler = logging.FileHandler(str(log_file))\n    logger.addHandler(handler)\n\n    try:\n        with catching_logs(handler, level=logging.INFO):\n            logging.info(\"info message\")\n            assert log_file.read() == \"INFO:root:info message\\n\"\n    finally:\n        logger.removeHandler(handler)\n", "def test_catch_log_handler():\n    handler = LogCaptureHandler()\n    record = logging.LogRecord(\"name\", logging.INFO, \"pathname\", 1, \"message\", None, None)\n    handler.emit(record)\n    assert len(handler.records) == 1\n    assert handler.records[0] is record\n    assert handler.stream.getvalue() == _remove_ansi_escape_sequences(\n        handler.formatter.format(record) + \"\\n\"\n    )\n", "def test_catch_log_handler_reset():\n    log_handler = LogCaptureHandler()\n    log_handler.stream.write(\"Test log message\")\n    log_handler.records.append(logging.LogRecord(\"test\", logging.INFO, \"\", 0, \"Test log message\", None, None))\n    assert log_handler.records\n    assert log_handler.stream.getvalue()\n\n    log_handler.reset()\n    assert not log_handler.records\n    assert not log_handler.stream.getvalue()\n", "def test_catch_log_handler(capsys):\n    log = logging.getLogger()\n    handler = LogCaptureHandler()\n    handler.setLevel(logging.INFO)\n    log.addHandler(handler)\n    log.info(\"Info message\")\n    log.warning(\"Warning message\")\n    captured = capsys.readouterr()\n    assert \"Info message\" in handler.stream.getvalue()\n    assert \"Warning message\" in handler.stream.getvalue()\n    assert not captured.out\n    assert not captured.err\n", "def test_catching_logs_context_manager():\n    log_handler = logging.StreamHandler()\n    formatter = logging.Formatter(\"%(levelname)s:%(message)s\")\n    level = logging.INFO\n\n    with catching_logs(log_handler, formatter=formatter, level=level) as handler:\n        assert handler is log_handler\n        assert handler.formatter is formatter\n        assert handler.level == level\n\n        logging.info(\"Test message\")\n        assert len(handler.records) == 1\n        record = handler.records[0]\n        assert record.levelname == \"INFO\"\n        assert record.message == \"Test message\"\n\n    # After the context manager exits, the log handler should be removed from the root logger\n    assert log_handler not in logging.getLogger().handlers\n"], "sample_667": ["def test_temp_path_factory_basetemp(tmp_path_factory: pathlib.TempPathFactory) -> None:\n    \"\"\"Test that the TempPathFactory returns the correct basetemp directory.\"\"\"\n    assert tmp_path_factory.getbasetemp().is_dir()\n    assert tmp_path_factory._given_basetemp is None  # type: ignore\n    assert tmp_path_factory._basetemp is not None  # type: ignore\n", "def test_tmp_path_factory_basetemp(tmp_path_factory, monkeypatch):\n    basetemp = tmp_path_factory.getbasetemp()\n    assert basetemp.exists()\n    monkeypatch.setenv(\"PYTEST_DEBUG_TEMPROOT\", str(basetemp))\n    new_factory = pathlib.TempPathFactory.from_config(pytest.config)\n    assert new_factory.getbasetemp() == basetemp\n", "def test_mktemp_uses_basetemp_if_given(tmp_path_factory, monkeypatch):\n    basetemp = tmp_path_factory.getbasetemp()\n    new_basetemp = basetemp / \"new\"\n    monkeypatch.setattr(tmp_path_factory, \"_given_basetemp\", new_basetemp)\n    assert tmp_path_factory.mktemp(\"foo\").parent == new_basetemp\n", "def test_tmp_path_factory_basetemp(tmp_path_factory: pathlib.TempPathFactory) -> None:\n    \"\"\"Test that the tmp_path_factory fixture returns a TempPathFactory with the correct basetemp.\"\"\"\n    assert isinstance(tmp_path_factory, pathlib.TempPathFactory)\n    assert isinstance(tmp_path_factory.getbasetemp(), Path)\n    assert tmp_path_factory.getbasetemp().is_dir()\n", "def test_get_user(monkeypatch):\n    import getpass\n\n    @attr.s\n    class FakeGetPass:\n        user = attr.ib()\n\n            if self.user is None:\n                raise KeyError(\"Mock failure\")\n            return self.user\n\n    fake_getpass = FakeGetPass(\"fakeuser\")\n    monkeypatch.setattr(getpass, \"getuser\", fake_getpass.getuser)\n    assert pathlib.get_user() == \"fakeuser\"\n\n    fake_getpass.user = None\n    assert pathlib.get_user() is None\n"], "sample_668": ["def test_NODE_USE_FROM_PARENT_warning_message():\n    warning = deprecated.NODE_USE_FROM_PARENT\n    assert issubclass(warning, deprecated.UnformattedWarning)\n    assert warning._category == pytest.PytestDeprecationWarning\n    expected_message = (\n        \"Direct construction of {name} has been deprecated, please use {name}.from_parent.\\n\"\n        \"See https://docs.pytest.org/en/latest/deprecations.html#node-construction-changed-to-node-from-parent \"\n        \"for more details.\"\n    )\n    assert warning._message == expected_message\n    # Test formatting of the message.\n    formatted_message = warning._format(\"SomeNode\")\n    assert formatted_message == expected_message.format(name=\"SomeNode\")\n", "def test_deprecated_external_plugins():\n    assert \"pytest_catchlog\" in deprecated.DEPRECATED_EXTERNAL_PLUGINS\n    assert \"pytest_capturelog\" in deprecated.DEPRECATED_EXTERNAL_PLUGINS\n    assert \"pytest_faulthandler\" in deprecated.DEPRECATED_EXTERNAL_PLUGINS\n", "def test_deprecated_external_plugins():\n    assert isinstance(deprecated.DEPRECATED_EXTERNAL_PLUGINS, set)\n    assert len(deprecated.DEPRECATED_EXTERNAL_PLUGINS) > 0\n    for plugin in deprecated.DEPRECATED_EXTERNAL_PLUGINS:\n        assert isinstance(plugin, str)\n", "def test_deprecated_external_plugins():\n    assert \"pytest_catchlog\" in deprecated.DEPRECATED_EXTERNAL_PLUGINS\n    assert \"pytest_capturelog\" in deprecated.DEPRECATED_EXTERNAL_PLUGINS\n    assert \"pytest_faulthandler\" in deprecated.DEPRECATED_EXTERNAL_PLUGINS\n", "def test_deprecated_external_plugins():\n    assert isinstance(deprecated.DEPRECATED_EXTERNAL_PLUGINS, set)\n    assert len(deprecated.DEPRECATED_EXTERNAL_PLUGINS) > 0\n    for plugin in deprecated.DEPRECATED_EXTERNAL_PLUGINS:\n        assert isinstance(plugin, str)\n"], "sample_669": ["    def test_capturing_methods(self, tmpdir):\n        capman = CaptureManager(\"fd\")\n        assert isinstance(capman._get_multicapture(), MultiCapture)\n        assert isinstance(capman._get_multicapture().out, capture.FDCapture)\n\n        capman = CaptureManager(\"sys\")\n        assert isinstance(capman._get_multicapture(), MultiCapture)\n        assert isinstance(capman._get_multicapture().out, capture.SysCapture)\n\n        capman = CaptureManager(\"no\")\n        assert isinstance(capman._get_multicapture(), MultiCapture)\n        assert capman._get_multicapture().out is None\n\n        capman = CaptureManager(\"tee-sys\")\n        assert isinstance(capman._get_multicapture(), MultiCapture)\n        assert isinstance(capman._get_multicapture().out, capture.TeeSysCapture)\n", "def test_capturing_resume_and_suspend_multiple_times(capfd):\n    with capfd.disabled():\n        print(\"Not captured\")\n    print(\"Captured 1\")\n    capfd.suspend()\n    print(\"Not captured 2\")\n    capfd.resume()\n    print(\"Captured 2\")\n    capfd.suspend()\n    print(\"Not captured 3\")\n    capfd.resume()\n    print(\"Captured 3\")\n    out, err = capfd.readouterr()\n    assert out == \"Captured 1\\nCaptured 2\\nCaptured 3\\n\"\n", "def test_capturing_methods(capsys, capfd):\n        print(\"Hello, stdout!\")\n\n        import sys\n        print(\"Hello, stderr!\", file=sys.stderr)\n\n    methods = [\"sys\", \"fd\"]\n    messages = [\"Hello, stdout!\\n\", \"Hello, stderr!\\n\"]\n\n    for method in methods:\n        capture_manager = CaptureManager(method)\n        capture_manager.start_global_capturing()\n\n        if method == \"sys\":\n            print_to_stdout()\n            out, err = capsys.readouterr()\n        else:\n            print_to_stderr()\n            out, err = capfd.readouterr()\n\n        assert (out, err) == (messages[0], messages[1])\n        capture_manager.stop_global_capturing()\n", "    def test_capturing_methods(self, tmp_path):\n        \"\"\"Test that the different capturing methods are used correctly.\"\"\"\n        capture_manager = CaptureManager(\"fd\")\n        assert isinstance(capture_manager._get_multicapture(), StdCaptureFD)\n\n        capture_manager = CaptureManager(\"sys\")\n        assert isinstance(capture_manager._get_multicapture(), StdCapture)\n\n        capture_manager = CaptureManager(\"tee-sys\")\n        assert isinstance(capture_manager._get_multicapture(), TeeStdCapture)\n\n        capture_manager = CaptureManager(\"no\")\n        assert capture_manager._get_multicapture() is None\n\n        # Test that an unknown method raises a ValueError\n        with pytest.raises(ValueError):\n            CaptureManager(\"unknown\")\n", "def test_capturing_enabled_at_request_time(pytestconfig, monkeypatch):\n    capman = CaptureManager(\"fd\")\n    request = pytest.FixtureRequest(None, _pyfuncitem=None)\n    monkeypatch.setattr(request.config, \"pluginmanager\", pytestconfig.pluginmanager)\n\n    @contextlib.contextmanager\n        yield None\n\n    monkeypatch.setattr(capman, \"_capturing_for_request\", _capturing_for_request)\n    assert capman._capture_fixture is None\n    capman._capturing_for_request(request)\n    assert capman._capture_fixture is not None\n"], "sample_670": ["def test_evaluate_invalid_input():\n    with pytest.raises(ParseError):\n        evaluate(\"invalid(input\", lambda x: True)\n", "def test_empty_expression():\n    assert not evaluate(\"\", lambda x: True)\n", "def test_evaluate_empty_expression():\n        assert False, \"Matcher should not be called for empty expression\"\n    assert not evaluate(\"\", matcher)\n", "def test_empty_expression():\n    assert not evaluate(\"\", lambda x: True)\n", "def test_simple_expression():\n        return ident == \"foo\"\n\n    assert evaluate(\"foo\", matcher)\n    assert not evaluate(\"bar\", matcher)\n    assert evaluate(\"foo or bar\", matcher)\n    assert evaluate(\"foo and foo\", matcher)\n    assert not evaluate(\"foo and bar\", matcher)\n"], "sample_671": ["def test_xfail_strict_with_runxfail(pytestconfig):\n    pytestconfig.option.runxfail = True\n    item = pytest.Item(\"test_func\", parent=None)\n    item._store[evalxfail_key] = MarkEvaluator(item, \"xfail\")\n    item._store[evalxfail_key]._mark = pytest.mark.xfail(strict=True)\n    rep = runtestprotocol(item, log=False)[0]\n    assert rep.outcome == \"passed\"\n    assert not hasattr(rep, \"wasxfail\")\n", "def test_xfail_strict(pytestconfig):\n    pytestconfig.setinivalue(\"xfail_strict\", True)\n    item = pytest.Function(\"test_func\", pytestconfig.rootpath, nodeid=\"nodeid\")\n    item.add_marker(pytest.mark.xfail(reason=\"some reason\"))\n    pytest_runtest_setup(item)\n    evalxfail = item._store[evalxfail_key]\n    assert evalxfail.get(\"strict\") is True\n", "def test_xfail_strict_no_run(item_with_xfail_strict):\n    item = item_with_xfail_strict\n    pytest_runtest_setup(item)\n    runtestprotocol(item, nextitem=None)\n    assert item._store[evalxfail_key].get(\"run\") is False\n    assert item._store[evalxfail_key].get(\"strict\") is True\n", "    def test_xfail_strict(self, pytestconfig):\n        item = pytest.Function(\"test_func\", pytestconfig.rootpath)\n        item.add_marker(pytest.mark.xfail(strict=True))\n        item.config = pytestconfig\n        pytest_runtest_setup(item)\n        rep = runtestprotocol(item, nextitem=None)\n        assert rep.outcome == \"failed\"\n        assert \"[XPASS(strict)]\" in rep.longreprtext\n", "def test_evaluator_istrue_with_condition_true():\n    item = pytest.Item(\"test_func\", parent=None)\n    item.keywords[\"skipif\"] = MarkEvaluator(item, \"skipif\")\n    item.keywords[\"skipif\"].mark.args = (\"True\",)\n    assert item.keywords[\"skipif\"].istrue()\n"], "sample_672": ["def test_saferepr_dispatches_to_pformat():\n    obj = {\"foo\": \"bar\", \"baz\": [1, 2, 3]}\n    expected = _pformat_dispatch(obj)\n    assert saferepr(obj) == expected\n", "def test_saferepr_instance_with_exception():\n    class BadRepr:\n            raise Exception(\"bad repr\")\n\n    obj = BadRepr()\n    result = saferepr(obj)\n    assert \"Exception('bad repr')\" in result\n    assert \"BadRepr object at\" in result\n", "def test_saferepr_dispatching_pretty_printer():\n    obj = {\"foo\": \"bar\", \"baz\": [\"qux\", \"quux\"]}\n    expected = \"\"\"{'baz': ['qux', 'quux'], 'foo': 'bar'}\"\"\"\n    assert _pformat_dispatch(obj) == expected\n    assert saferepr(obj) == expected\n\n    class BadRepr:\n            raise Exception(\"Bad repr!\")\n\n    bad_repr_obj = BadRepr()\n    assert saferepr(bad_repr_obj).startswith(\"<[Exception\")\n", "def test_saferepr_instance_repr_failure():\n    class BadRepr:\n            raise Exception(\"bad repr\")\n\n    obj = BadRepr()\n    rep = saferepr(obj)\n    assert \"raised in repr()\" in rep\n    assert type(obj).__name__ in rep\n    assert hex(id(obj)) in rep\n", "def test_saferepr_recursive():\n    class RecursiveRepr:\n            return saferepr(self)\n\n    obj = RecursiveRepr()\n    assert \"unpresentable exception\" in saferepr(obj)\n"], "sample_673": ["def test_get_checker():\n    checker = _get_checker()\n    assert isinstance(checker, type)\n    assert \"ALLOW_UNICODE\" in [name for name, value in checker.__dict__.items() if isinstance(value, int)]\n    assert \"ALLOW_BYTES\" in [name for name, value in checker.__dict__.items() if isinstance(value, int)]\n    assert \"NUMBER\" in [name for name, value in checker.__dict__.items() if isinstance(value, int)]\n", "def test_get_checker():\n    checker = _get_checker()\n    assert isinstance(checker, type)\n    assert issubclass(checker, inspect.getmodule(checker).OutputChecker)\n", "def test_doctest_item_repr_failure(monkeypatch):\n    import doctest\n\n        return \"fake repr failure\"\n\n    monkeypatch.setattr(DoctestItem, \"repr_failure\", fake_repr_failure)\n\n    item = DoctestItem.from_parent(\n        parent=None,\n        name=\"test\",\n        runner=None,\n        dtest=doctest.DocTest(),\n    )\n\n    excinfo = pytest.raises(doctest.DocTestFailure)\n    assert item.repr_failure(excinfo) == \"fake repr failure\"\n", "def test_doctest_item_repr_failure(monkeypatch):\n    checker = _get_checker()\n    report_choice = 1\n\n        return textwrap.dedent(\n            \"\"\"\n            expected\n            -------\n                foo\n            but got\n            --------\n                bar\n            \"\"\"\n        )\n\n    monkeypatch.setattr(checker, \"output_difference\", output_difference)\n\n    class DummyDoctest:\n            self.examples = [DummyExample()]\n            self.lineno = 10\n            self.filename = \"example.txt\"\n            self.docstring = \"This is a docstring.\"\n\n    class DummyExample:\n            self.source = \"This is an example.\"\n            self.lineno = 5\n            self.want = \"foo\"\n\n    item = DoctestItem(\"example\", parent=None, runner=None, dtest=DummyDoctest())\n    excinfo = pytest.raises(AssertionError)\n    excinfo.value = \"bar\"\n    assert (\n        item.repr_failure(excinfo).toterminal(pytest.config.create_terminal_writer())\n        == textwrap.dedent(\n            \"\"\"\n            example.txt:16: This is an example.\n            This is a docstring.\n              >>> This is an example.\n              This is an example.\n            expected\n            -------\n                foo\n            but got\n            --------\n                bar\n            \"\"\"\n        )\n    )\n", "def test_doctest_namespace():\n    \"\"\"Test that doctest_namespace fixture is used.\"\"\"\n    config = pytest.config\n    session = pytest.Session(config)\n    doctest_module = DoctestModule.from_parent(\n        parent=session, path=pytest.path.local(\"example.py\")\n    )\n    item = DoctestItem.from_parent(\n        parent=doctest_module,\n        name=\"example\",\n        runner=None,\n        dtest=None,\n    )\n\n    namespace_fixture_value = {\"myvar\": \"value\"}\n    item.fixture_request = pytest.FixtureRequest(item)\n    item.fixture_request._fixturemanager._arg2fixturedefs[\"doctest_namespace\"] = [\n        pytest.fixture(lambda: namespace_fixture_value)\n    ]\n\n    item.setup()\n\n    assert item.dtest.globs[\"myvar\"] == \"value\"\n"], "sample_674": ["def test_node_keywords_mark():\n    node = nodes.Node.from_parent(parent=None, name=\"test_node\")\n    assert not node.keywords\n\n    mark = pytest.mark.foo\n    node.add_marker(mark)\n\n    assert \"foo\" in node.keywords\n    assert node.get_closest_marker(\"foo\") == mark\n\n    node2 = nodes.Node.from_parent(parent=node, name=\"test_node2\")\n    assert \"foo\" in node2.keywords\n    assert node2.get_closest_marker(\"foo\") == mark\n", "def test_node_repr_failure(pytestconfig):\n    excinfo = pytest.raises(AssertionError, lambda: 0 / 0)\n    node = nodes.Node.from_parent(parent=None, config=pytestconfig, session=None)\n    assert \"AssertionError\" in str(node.repr_failure(excinfo))\n", "def test_node_repr():\n    node = nodes.Node(name=\"test_node\", nodeid=\"test_node_id\")\n    assert repr(node) == \"<Node test_node>\"\n", "def test_get_fslocation_from_item(tmpdir):\n    \"\"\"Test that get_fslocation_from_item returns the correct location.\"\"\"\n    item = nodes.Item(\"test_item\", parent=None)\n    item.fspath = tmpdir.join(\"test_file.py\")\n    item.location = (str(item.fspath), 10, \"test_function\")\n\n    location = nodes.get_fslocation_from_item(item)\n    assert location == (str(item.fspath), 10)\n\n    item.obj = lambda: None\n    item.obj.__code__ = py.code.Code()\n    item.obj.__code__.co_filename = str(item.fspath)\n    item.obj.__code__.co_firstlineno = 20\n\n    location = nodes.get_fslocation_from_item(item)\n    assert location == (str(item.fspath), 20)\n", "def test_node_repr(tmp_path):\n    fspath = tmp_path / \"test_file.py\"\n    node = nodes.File.from_parent(parent=None, fspath=fspath)\n    assert repr(node) == f\"<File {fspath.name}>\"\n"], "sample_675": ["def test_log_cli_enabled(pytestconfig, capsys):\n    \"\"\"Test that live logging is enabled when the log_cli or log_cli_level options are used.\"\"\"\n    pytestconfig.option.log_cli = True\n    plugin = LoggingPlugin(pytestconfig)\n    assert plugin._log_cli_enabled()\n\n    pytestconfig.option.log_cli = False\n    pytestconfig.option.log_cli_level = \"INFO\"\n    plugin = LoggingPlugin(pytestconfig)\n    assert plugin._log_cli_enabled()\n\n    pytestconfig.option.log_cli = False\n    pytestconfig.option.log_cli_level = None\n    plugin = LoggingPlugin(pytestconfig)\n    assert not plugin._log_cli_enabled()\n", "def test_catchlog_records(testdir):\n    \"\"\"Test that catchlog records are properly stored and retrieved.\"\"\"\n    p = testdir.makepyfile(\n        \"\"\"\n        import logging\n            logging.getLogger().info('hello')\n            assert len(caplog.records) == 1\n            assert caplog.records[0].getMessage() == 'hello'\n    \"\"\"\n    )\n    result = testdir.runpytest(p)\n    result.assert_outcomes(passed=1)\n", "def test_logging_plugin_sets_handler_levels(pytester: Testdir) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n            logging.getLogger('foo').info('info')\n        \"\"\"\n    )\n    result = pytester.runpytest(\"--log-cli-level=INFO\", \"--log-level=DEBUG\")\n    assert result.ret == 0\n    result.stdout.fnmatch_lines([\"*info*\"])\n\n    result = pytester.runpytest(\"--log-cli-level=DEBUG\", \"--log-level=INFO\")\n    assert result.ret == 0\n    result.stdout.fnmatch_lines([\"*info*\"])\n", "def test_catch_log_handler_key(tmpdir):\n    \"\"\"Verify that catch_log_handler_key is a StoreKey instance\"\"\"\n    assert isinstance(catch_log_handler_key, pytest.StoreKey)\n    assert catch_log_handler_key.name == \"catch_log_handler\"\n", "def test_catching_logs_with_level(testdir):\n    \"\"\"Test that catching_logs context manager sets the level of the handler and root logger correctly.\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n        import pytest\n\n            with pytest.helpers.catching_logs(level=logging.INFO) as log_handler:\n                logging.getLogger().info(\"info message\")\n                assert len(log_handler.records) == 1\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    assert result.ret == 0\n"], "sample_676": ["def test__plugin_nameversions(option):\n    plugininfo = [\n        (\"test-plugin\", DistInfo(project_name=\"test\", version=1)),\n        (\"pytest-test\", DistInfo(project_name=\"pytest-test\", version=2)),\n        (\"test-plugin\", DistInfo(project_name=\"test\", version=1)),\n    ]\n    expected = [\"test-1\", \"test-2\"]\n    assert _plugin_nameversions(plugininfo) == expected\n", "def test_getreportopt(input, expected):\n    config = pytest.config\n    config.option.reportchars = \"\".join(input)\n    result = getreportopt(config)\n    assert result == expected\n", "def test__plugin_nameversions():\n    plugininfo = [\n        (\"plugin1\", DistInfo(project_name=\"test\", version=1)),\n        (\"plugin2\", DistInfo(project_name=\"pytest-test\", version=1)),\n        (\"plugin3\", DistInfo(project_name=\"test\", version=1)),\n    ]\n    expected = [\"test-1\"]\n    assert _plugin_nameversions(plugininfo) == expected\n", "def test_getreportopt(input, expected):\n    config = pytest.config\n    config.option.reportchars = input\n    config.option.disable_warnings = False\n    assert getreportopt(config) == expected\n", "def test__plugin_nameversions():\n    plugininfo = [\n        (\"plugin1\", DistInfo(project_name=\"test\", version=1)),\n        (\"plugin2\", DistInfo(project_name=\"pytest-test\", version=2)),\n        (\"plugin3\", DistInfo(project_name=\"test\", version=1)),\n    ]\n    expected = [\"test-1\", \"test-2\"]\n    assert _plugin_nameversions(plugininfo) == expected\n"], "sample_677": ["def test_empty_expression() -> None:\n    assert not evaluate(\"\", lambda x: True)\n", "def test_compile_empty() -> None:\n    with pytest.raises(ParseError) as excinfo:\n        Expression.compile(\"\")\n    assert str(excinfo.value) == \"at column 1: expected identifier; got end of input\"\n", "def test_expression_parser_error():\n    with pytest.raises(ParseError) as excinfo:\n        Expression.compile(\"foo bar\")\n    assert str(excinfo.value).startswith(\"at column \")\n    assert \"unexpected character\" in str(excinfo.value)\n", "def test_compile_empty() -> None:\n    assert not evaluate(\"\", lambda x: True)\n", "def test_syntax_error():\n    with pytest.raises(ParseError) as excinfo:\n        Expression.compile(\"foo bar\")\n    assert str(excinfo.value).startswith(\"at column 4: \")\n"], "sample_678": ["def test_get_extended_length_path_str():\n    path = \"C:\\\\path\\\\that\\\\is\\\\long\\\\enough\"\n    assert get_extended_length_path_str(path) == \"\\\\\\\\?\\\\C:\\\\path\\\\that\\\\is\\\\long\\\\enough\"\n    unc_path = \"\\\\\\\\server\\\\share\\\\path\"\n    assert get_extended_length_path_str(unc_path) == \"\\\\\\\\?\\\\UNC\\\\server\\\\share\\\\path\"\n    long_path = \"\\\\\\\\?\\\\C:\\\\path\\\\that\\\\is\\\\already\\\\long\\\\enough\"\n    assert get_extended_length_path_str(long_path) == long_path\n", "def test_get_lock_path(tmp_path: Path) -> None:\n    p = tmp_path / \"foo\"\n    lock_path = get_lock_path(p)\n    assert lock_path.name == \".lock\"\n    assert lock_path.parent == p\n\n    lock_path2 = get_lock_path(lock_path)\n    assert lock_path2.name == \".lock\"\n    assert lock_path2.parent == lock_path\n", "def test_get_lock_path(tmp_path):\n    path = tmp_path / \"test_dir\"\n    path.mkdir()\n    lock_path = get_lock_path(path)\n    assert lock_path == path / \".lock\"\n    assert not lock_path.exists()\n", "def test_get_lock_path(tmp_path):\n    path = tmp_path / \"test\"\n    lock_path = get_lock_path(path)\n    assert lock_path == path / \".lock\"\n    assert not lock_path.exists()\n\n    # Test with a path that already has an extension\n    path = tmp_path / \"test.txt\"\n    lock_path = get_lock_path(path)\n    assert lock_path == path / \".lock\"\n    assert not lock_path.exists()\n", "def test_get_lock_path(tmp_path):\n    path = tmp_path / \"test_dir\"\n    path.mkdir()\n    lock_path = get_lock_path(path)\n    assert lock_path == path / \".lock\"\n"], "sample_679": ["def test_mark_evaluator_compiled_eval(self):\n    item = mock.MagicMock(spec=Node)\n    item.obj = mock.MagicMock()\n    item.obj.__globals__ = {\"foo\": \"bar\"}\n\n    expr = \"os.name == 'posix'\"\n    evaluator = MarkEvaluator(item, \"skipif\")\n    with mock.patch(\"os.name\", \"posix\"):\n        result = compiled_eval(expr, evaluator._getglobals())\n        assert result is True\n\n    expr = \"foo == 'bar'\"\n    result = compiled_eval(expr, evaluator._getglobals())\n    assert result is True\n", "def test_mark_evaluator_compiled_eval(self, tmpdir):\n    item = mock.Mock()\n    item.obj = mock.Mock(__globals__={\"foo\": \"bar\"})\n    item.config = mock.Mock()\n    expr = \"foo == 'bar'\"\n    evaluator = MarkEvaluator(item, \"my_mark\")\n    assert evaluator._istrue() is False  # no marks defined\n\n    mark = Mark(\"my_mark\", (\"condition\", expr))\n    item.iter_markers.return_value = [mark]\n    assert evaluator._istrue() is True\n    assert evaluator.expr == expr\n\n    # Test with SyntaxError in the expression\n    expr = \" invalid syntax\"\n    mark = Mark(\"my_mark\", (\"condition\", expr))\n    item.iter_markers.return_value = [mark]\n    with pytest.raises(TEST_OUTCOME):\n        evaluator._istrue()\n\n    # Test with condition not met\n    expr = \"foo != 'bar'\"\n    mark = Mark(\"my_mark\", (\"condition\", expr))\n    item.iter_markers.return_value = [mark]\n    assert evaluator._istrue() is False\n", "    def test_mark_evaluator_istrue(self):\n        item = mock.Mock(spec=Node, obj=None)\n        mark_evaluator = MarkEvaluator(item, \"my_mark\")\n\n        # Mock _get_marks to return a list of marks with a condition\n        mark_evaluator._get_marks = mock.Mock(return_value=[Mark(\"my_mark\", (), {\"condition\": \"True\"})])\n\n        assert mark_evaluator.istrue() is True\n\n        # Mock _get_marks to return a list of marks without a condition\n        mark_evaluator._get_marks = mock.Mock(return_value=[Mark(\"my_mark\", (True,), {})])\n\n        assert mark_evaluator.istrue() is True\n\n        # Mock _get_marks to return an empty list\n        mark_evaluator._get_marks = mock.Mock(return_value=[])\n\n        assert mark_evaluator.istrue() is False\n", "def test_MarkEvaluator_compiled_eval(self):\n    item = mock.Mock(obj=mock.Mock(__globals__={\"foo\": \"bar\"}))\n    item.config = mock.Mock()\n    expr = \"os.name == 'posix'\"\n    evaluator = MarkEvaluator(item, \"some_mark\")\n    result = compiled_eval(expr, evaluator._getglobals())\n    assert isinstance(result, bool)\n", "def test_MarkEvaluator_istrue_with_condition(self, pytestconfig):\n    item = Node.from_parent(pytestconfig, name=\"item\")\n    item.obj = None\n    mark_evaluator = MarkEvaluator(item, \"skipif\")\n    mark_evaluator._marks = [Mark(name=\"skipif\", args=(\"True\",), kwargs={})]\n    assert mark_evaluator.istrue() is True\n"], "sample_680": ["def test_evaluate_xfail_marks_strict(item):\n    item.config._inicache[\"xfail_strict\"] = True\n    mark = pytest.mark.xfail(reason=\"foo\")\n    item.add_marker(mark)\n    result = evaluate_xfail_marks(item)\n    assert result.strict is True\n\n    item.config._inicache[\"xfail_strict\"] = False\n    result = evaluate_xfail_marks(item)\n    assert result.strict is False\n\n    mark = pytest.mark.xfail(reason=\"foo\", strict=True)\n    item.add_marker(mark)\n    result = evaluate_xfail_marks(item)\n    assert result.strict is True\n\n    mark = pytest.mark.xfail(reason=\"foo\", strict=False)\n    item.add_marker(mark)\n    result = evaluate_xfail_marks(item)\n    assert result.strict is False\n", "def test_evaluate_condition_with_string_condition():\n    item = pytest.Item(\"test\", None)\n    mark = pytest.Mark(\"xfail\", (\"condition\" == \"True\"), {})\n    condition = \"os.name == 'posix'\"\n    result, reason = evaluate_condition(item, mark, condition)\n    assert result == (sys.platform != 'win32')\n    assert reason == condition\n", "def test_evaluate_xfail_mark_strict(self):\n    item = pytest.Item(\"test_func\", self.parent)\n    mark = pytest.mark.xfail(strict=True)\n    item.add_marker(mark)\n\n    result = evaluate_xfail_marks(item)\n    assert result is not None\n    assert result.strict is True\n\n    item = pytest.Item(\"test_func\", self.parent)\n    mark = pytest.mark.xfail(strict=False)\n    item.add_marker(mark)\n\n    result = evaluate_xfail_marks(item)\n    assert result is not None\n    assert result.strict is False\n", "def test_xfail_mark_strict(self, item):\n    xfail_mark = pytest.mark.xfail(reason=\"some reason\", strict=True)\n    item.add_marker(xfail_mark)\n    result = evaluate_xfail_marks(item)\n    assert result is not None\n    assert result.strict is True\n    assert result.reason == \"some reason\"\n    assert result.run is True\n    assert result.raises is None\n", "def test_evaluate_xfail_mark_strict(self, item):\n    item.config.getini.return_value = True  # xfail_strict\n    mark = pytest.mark.xfail(reason=\"foo\", strict=True)\n    item.add_marker(mark)\n    result = evaluate_xfail_marks(item)\n    assert result.strict is True\n"], "sample_681": ["def test_log_cli_enabled(pytestconfig, capsys):\n    \"\"\"Test that live logging is enabled when the log_cli option is set.\"\"\"\n    pytestconfig.option.log_cli = True\n    with pytest.raises(SystemExit) as excinfo:\n        pytestconfig.pluginmanager.hook.pytest_sessionstart()\n    assert excinfo.value.code == ExitCode.OK\n    captured = capsys.readouterr()\n    assert \"Live Log\" in captured.out\n", "def test_catch_log_message_levels(testdir: Testdir) -> None:\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n\n            logging.debug(\"debug message\")\n            logging.info(\"info message\")\n            logging.warning(\"warning message\")\n            logging.error(\"error message\")\n            logging.critical(\"critical message\")\n\n            assert \"debug message\" in caplog.text\n            assert \"info message\" in caplog.text\n            assert \"warning message\" in caplog.text\n            assert \"error message\" in caplog.text\n            assert \"critical message\" in caplog.text\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.assert_outcomes(passed=1)\n", "def test_live_logging_custom_formatting(testdir):\n    \"\"\"Test live logging with custom formatting (#3397).\"\"\"\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_format = %(asctime)s %(message)s\n        log_date_format = %Y-%m-%dT%H:%M:%S\n        \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n        import pytest\n\n            logging.info('obar')\n        \"\"\"\n    )\n    result = testdir.runpytest(\"-v\", \"--log-cli-level=INFO\")\n    assert result.ret == ExitCode.OK\n    assert re.search(r\"\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2} obar\", result.outlines[-1])\n", "def test_log_cli_handler_emit(testdir, capsys):\n    \"\"\"Test that _LiveLoggingStreamHandler emits logs correctly.\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n            logging.warning(\"hello\")\n    \"\"\"\n    )\n    result = testdir.runpytest(\"--log-cli-level=WARNING\")\n    assert result.ret == ExitCode.OK\n    captured = capsys.readouterr()\n    assert \"hello\" in captured.out\n", "def test_log_cli_enabled(pytestconfig):\n    class DummyPluginManager:\n            if name == \"terminalreporter\":\n                return object()\n            return None\n\n    config = pytestconfig\n    config.pluginmanager = DummyPluginManager()\n\n    assert not LoggingPlugin(config)._log_cli_enabled()\n\n    class DummyPluginManager:\n            if name == \"terminalreporter\":\n                return None\n            return object()\n\n    config = pytestconfig\n    config.pluginmanager = DummyPluginManager()\n\n    assert not LoggingPlugin(config)._log_cli_enabled()\n\n    class DummyPluginManager:\n            return object()\n\n    config = pytestconfig\n    config.pluginmanager = DummyPluginManager()\n\n    assert not LoggingPlugin(config)._log_cli_enabled()\n\n    config.option.log_cli_level = True\n\n    assert LoggingPlugin(config)._log_cli_enabled()\n\n    del config.option.log_cli_level\n\n    config.option.log_cli = True\n\n    assert LoggingPlugin(config)._log_cli_enabled()\n"], "sample_682": ["def test_xfail_mark_with_condition(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.xfail(condition=True, reason=\"some reason\")\n            pass\n    \"\"\"\n    )\n\n    result = testdir.runpytest(\"--verbose\")\n    result.stdout.fnmatch_lines([\"*XFAIL test_func.py::test_func*some reason\"])\n", "def test_evaluate_condition_unexpected_exception(testdir):\n    item = testdir.getitem(\"def test_func(): pass\")\n    mark = pytest.mark.skipif(sys.platform == \"unknown\", reason=\"Unknown platform\")\n\n    with pytest.raises(pytest.fail.Exception) as excinfo:\n        evaluate_condition(item, mark, lambda: 1 / 0)\n\n    assert \"Error evaluating 'skipif' condition\" in str(excinfo.value)\n    assert \"ZeroDivisionError\" in str(excinfo.value)\n", "def test_xfail_mark_strict(self, testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n            pytest.xfail(\"strict\")\n    \"\"\"\n    )\n    result = testdir.runpytest(\"--runxfail\")\n    assert \"XPASS(strict)\" not in result.stdout.str()\n    result = testdir.runpytest()\n    assert \"XPASS(strict)\" in result.stdout.str()\n", "def test_xfail_mark_evaluation_with_condition_true(testdir):\n    item = testdir.getitem(\"\"\"\n        import pytest\n\n        @pytest.mark.xfail(condition=True, reason=\"xfail reason\")\n            pass\n    \"\"\")\n    xfailed = evaluate_xfail_marks(item)\n    assert xfailed is not None\n    assert xfailed.reason == \"xfail reason\"\n    assert xfailed.run is True\n    assert xfailed.strict is False\n", "def test_xfail_mark_strict(pytestconfig):\n    item = pytest.Function(\"func\", pytestconfig)\n    mark = pytest.mark.xfail(reason=\"some_reason\", strict=True)\n    item.add_marker(mark)\n    xfailed = evaluate_xfail_marks(item)\n    assert xfailed is not None\n    assert xfailed.strict is True\n    assert xfailed.reason == \"some_reason\"\n"], "sample_683": ["def test_capture_manager_resume_fixture(capsys, capsysbinary, capfd, capfdbinary):\n    \"\"\"Test that the capture manager resumes fixtures correctly\"\"\"\n    capman = CaptureManager(\"sys\")\n    for fixture in [capsys, capsysbinary, capfd, capfdbinary]:\n        capman.set_fixture(fixture)\n        capman.suspend_fixture()\n        assert not fixture._capture.is_capturing()\n        capman.resume_fixture()\n        assert fixture._capture.is_capturing()\n        capman.unset_fixture()\n", "def test_capture_manager_is_globally_capturing():\n    capman = CaptureManager(\"fd\")\n    assert capman.is_globally_capturing() is True\n\n    capman = CaptureManager(\"no\")\n    assert capman.is_globally_capturing() is False\n", "    def test_capture_manager_fixture_control(self, capsys):\n        \"\"\"Test that the capture manager properly controls the fixture capturing.\"\"\"\n        capman = CaptureManager(\"sys\")\n        capture_fixture = capsys\n        capman.set_fixture(capture_fixture)\n\n        # Ensure that the fixture is active\n        assert capman._capture_fixture == capture_fixture\n\n        # Suspend and resume capturing\n        capman.suspend_fixture()\n        assert capture_fixture._capture._state == \"suspended\"\n        capman.resume_fixture()\n        assert capture_fixture._capture._state == \"started\"\n\n        # Deactivate and reactivate the fixture\n        capman.deactivate_fixture()\n        assert capture_fixture._capture._state == \"stopped\"\n        capman.activate_fixture()\n        assert capture_fixture._capture._state == \"started\"\n\n        # Unset the fixture\n        capman.unset_fixture()\n        assert capman._capture_fixture is None\n", "def test_capturemanager_fixture_control(capsys, capsysbinary, capfd, capfdbinary):\n    \"\"\"Test that the CaptureManager correctly controls the fixture capturing.\"\"\"\n    capman = cast(CaptureManager, capsys.request.config.pluginmanager.getplugin(\"capturemanager\"))\n\n    # Check that the global capture is suspended when a fixture is activated\n    assert capman.is_globally_capturing()\n    capsys.readouterr()  # Activate the fixture\n    assert not capman.is_globally_capturing()\n\n    # Check that the global capture is resumed when the fixture is deactivated\n    capsys.close()\n    assert capman.is_globally_capturing()\n\n    # Repeat with the other fixtures\n    capsysbinary.readouterr()\n    assert not capman.is_globally_capturing()\n    capsysbinary.close()\n    assert capman.is_globally_capturing()\n\n    capfd.readouterr()\n    assert not capman.is_globally_capturing()\n    capfd.close()\n    assert capman.is_globally_capturing()\n\n    capfdbinary.readouterr()\n    assert not capman.is_globally_capturing()\n    capfdbinary.close()\n    assert capman.is_globally_capturing()\n", "def test_capturing_methods(capsys, capsysbinary, capfd, capfdbinary):\n        print(\"Hello\")\n        sys.stdout.write(\"World\\n\")\n        sys.stderr.write(\"Error\\n\")\n\n    capsys.readouterr()\n    capsysbinary.readouterr()\n    capfd.readouterr()\n    capfdbinary.readouterr()\n\n    print_output()\n\n    out, err = capsys.readouterr()\n    assert out == \"Hello\\nWorld\\n\"\n    assert err == \"Error\\n\"\n\n    out, err = capsysbinary.readouterr()\n    assert out == b\"\"\n    assert err == b\"\"\n\n    out, err = capfd.readouterr()\n    assert out == \"Hello\\nWorld\\n\"\n    assert err == \"Error\\n\"\n\n    out, err = capfdbinary.readouterr()\n    assert out == b\"\"\n    assert err == b\"\"\n"], "sample_684": ["def test_reprfuncargs() -> None:\n    args = [(\"foo\", \"bar\"), (\"a\", 1)]\n    repr_func_args = ReprFuncArgs(args)\n    tw_mock = mock.Mock()\n    repr_func_args.toterminal(tw_mock)\n    assert tw_mock.line.call_count == 2\n    first_line = tw_mock.line.call_args_list[0][1][\"s\"]\n    second_line = tw_mock.line.call_args_list[1][1][\"s\"]\n    assert re.match(r\"foo = 'bar', a = 1$\", first_line) or re.match(\n        r\"a = 1, foo = 'bar'$\", first_line\n    )\n    assert second_line == \"\"\n", "def test_getfslineno_with_real_func():\n        pass\n\n    mock_obj = mock.Mock()\n    mock_obj.place_as = real_func\n    fspath, lineno = getfslineno(mock_obj)\n    assert fspath == py.path.local(__file__)\n    assert lineno == 0\n", "def test_frame_getargs() -> None:\n        frame = Frame(sys._getframe())\n        assert frame.getargs(var=False) == [(\"x\", 1), (\"y\", 2)]\n        assert frame.getargs(var=True) == [\n            (\"x\", 1),\n            (\"y\", 2),\n            (\"varargs\", (3, 4)),\n            (\"varkwargs\", {\"a\": 1, \"b\": 2}),\n        ]\n\n    func(1, 2, 3, 4, a=1, b=2)\n", "def test_reprfuncargs():\n    args = [(\"a\", \"1\"), (\"b\", \"2\")]\n    reprfuncargs = ReprFuncArgs(args)\n    assert reprfuncargs.args == args\n    tw = mock.Mock()\n    reprfuncargs.toterminal(tw)\n    assert tw.line.call_count == 2\n    expected_calls = [mock.call(\"a = 1, b = 2\"), mock.call(\"\")]\n    tw.line.assert_has_calls(expected_calls)\n", "def test_traceback_entry_repr():\n    entry = TracebackEntry(mock.Mock(), excinfo=None)\n    entry.lineno = 10\n    entry.frame = Frame(mock.Mock(spec=FrameType))\n    entry.frame.code.path = \"foo.py\"\n    assert re.match(r\"^  File 'foo\\.py', line 11\", str(entry))\n"], "sample_685": ["def test_caplog_records(tmpdir, caplog):\n    records = []\n    logger.addHandler(logging.StreamHandler())\n    logger.setLevel(\"DEBUG\")\n    sublogger.setLevel(\"DEBUG\")\n\n        logger.debug(\"simple\")\n        sublogger.info(\"simple2\")\n        logger.warning(\"next\")\n        logger.critical(\"more\")\n        logger.error(\"error\")\n        logger.info(\"info\")\n        logger.debug(\"debug\")\n        logger.log(42, \"custom\")\n\n    record_msg()\n\n    assert len(caplog.records) == 7\n    for (index, rec) in enumerate(caplog.records):\n        if index == 0:\n            assert rec.name == __name__\n            assert rec.msg == \"simple\"\n            assert rec.levelname == \"DEBUG\"\n        elif index == 1:\n            assert rec.name == sublogger.name\n            assert rec.msg == \"simple2\"\n            assert rec.levelname == \"INFO\"\n        elif index == 2:\n            assert rec.name == __name__\n            assert rec.msg == \"next\"\n            assert rec.levelname == \"WARNING\"\n        elif index == 3:\n            assert rec.name == __name__\n            assert rec.msg == \"more\"\n            assert rec.levelname == \"CRITICAL\"\n        elif index == 4:\n            assert rec.name == __name__\n            assert rec.msg == \"error\"\n            assert rec.levelname == \"ERROR\"\n        elif index == 5:\n            assert rec.name == __name__\n            assert rec.msg == \"info\"\n            assert rec.levelname == \"INFO\"\n        elif index == 6:\n            assert rec.name == __name__\n            assert rec.msg == \"custom\"\n            assert rec.levelname == \"LEVEL 42\"\n\n    # test that clear() works as expected\n    caplog.clear()\n    assert len(caplog.records) == 0\n\n    # and test get_records(when) with different 'when' values\n    record_msg()\n    assert len(caplog.get_records(\"setup\")) == 0\n    assert len(caplog.get_records(\"call\")) == 7\n    assert len(caplog.get_records(\"teardown\")) == 0\n", "def test_caplog_record_tuples(caplog):\n    caplog.set_level(logging.INFO)\n    logger.info(\"boo %s\", \"arg\")\n    assert caplog.record_tuples == [(\"test_logging.py\", logging.INFO, \"boo arg\")]\n", "def test_get_log_level_for_setting(pytestconfig):\n    pytestconfig.addinivalue_line(\"log_cli_level\", \"INFO\")\n    assert get_log_level_for_setting(pytestconfig, \"log_cli_level\") == logging.INFO\n\n    pytestconfig.addinivalue_line(\"log_cli_level\", \"42\")\n    assert get_log_level_for_setting(pytestconfig, \"log_cli_level\") == 42\n\n    pytestconfig.addinivalue_line(\"log_cli_level\", \"UNKNOWN\")\n    with pytest.raises(pytest.UsageError):\n        get_log_level_for_setting(pytestconfig, \"log_cli_level\")\n", "def test_log_cli_enabled(pytestconfig, capsys):\n    class DummyTerminalReporter:\n            pass\n\n    pytestconfig.pluginmanager.get_plugin.return_value = DummyTerminalReporter()\n    logging_plugin = LoggingPlugin(pytestconfig)\n    assert logging_plugin._log_cli_enabled()\n    pytestconfig.option.log_cli_level = None\n    pytestconfig.getini.return_value = False\n    assert not logging_plugin._log_cli_enabled()\n", "def test_get_log_level_for_setting(testdir: Testdir) -> None:\n    \"\"\"Test that get_log_level_for_setting handles valid and invalid input correctly.\"\"\"\n    config = testdir.parseconfig(\n        \"\"\"\n        [tool:pytest]\n        log_level = DEBUG\n        \"\"\"\n    )\n    assert get_log_level_for_setting(config, \"log_level\") == logging.DEBUG\n\n    config = testdir.parseconfig(\n        \"\"\"\n        [tool:pytest]\n        log_level = INVALID\n        \"\"\"\n    )\n    with pytest.raises(pytest.UsageError):\n        get_log_level_for_setting(config, \"log_level\")\n\n    config = testdir.parseconfig(\n        \"\"\"\n        [tool:pytest]\n        log_level = 10\n        \"\"\"\n    )\n    assert get_log_level_for_setting(config, \"log_level\") == 10\n"], "sample_686": ["def test_NODE_USE_FROM_PARENT():\n    name = \"TestNode\"\n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter(\"always\")\n        deprecated.NODE_USE_FROM_PARENT.format(name=name)\n        assert len(w) == 1\n        assert issubclass(w[0].category, deprecated.PytestDeprecationWarning)\n        assert str(w[0].message) == (\n            f\"Direct construction of {name} has been deprecated, \"\n            f\"please use {name}.from_parent.\\nSee \"\n            \"https://docs.pytest.org/en/stable/deprecations.html#node-construction-changed-to-node-from-parent \"\n            \"for more details.\"\n        )\n", "def test_funcargnames_attr_is_deprecated():\n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter(\"always\")\n        deprecated.FUNCARGNAMES\n        assert len(w) == 1\n        assert issubclass(w[0].category, deprecated.PytestDeprecationWarning)\n        assert str(w[0].message) == deprecated.FUNCARGNAMES.args[0]\n", "def test_deprecated_external_plugins():\n    config = Config()\n    with mock.patch.object(config, \"getoption\", return_value=True):\n        for plugin in deprecated.DEPRECATED_EXTERNAL_PLUGINS:\n            with pytest.warns(pytest.PytestDeprecationWarning):\n                config.pluginmanager.load_setuptools_entrypoints(plugin)\n", "def test_deprecated_external_plugins():\n    config = Config()\n    for plugin in deprecated.DEPRECATED_EXTERNAL_PLUGINS:\n        with pytest.warns(pytest.PytestDeprecationWarning):\n            config.pluginmanager.loadplugin(plugin)\n", "def test_deprecated_external_plugins():\n    config = Config()\n    for plugin in deprecated.DEPRECATED_EXTERNAL_PLUGINS:\n        with pytest.warns(pytest.PytestDeprecationWarning):\n            config.pluginmanager.loadplugin(plugin)\n"], "sample_687": ["def test_caplog_messages(caplog):\n    caplog.set_level(logging.INFO)\n    logger.info(\"message 1\")\n    logger.info(\"message 2\")\n    assert caplog.messages == [\"message 1\", \"message 2\"]\n", "def test_caplog_resets_log_level_of_loggers(tmpdir, caplog):\n    logger.setLevel(logging.INFO)\n    sublogger.setLevel(logging.INFO)\n\n        assert logger.level == logging.NOTSET\n        assert sublogger.level == logging.NOTSET\n        logger.info(\"1\")\n        sublogger.info(\"2\")\n\n    with tmpdir.mkdir(\"tests\").join(\"test_logging.py\").open(\"w\") as f:\n        f.write(\"\"\"\n            import logging\n            import pytest\n            logger = logging.getLogger('%s')\n            sublogger = logging.getLogger('%s')\n                logger.setLevel(logging.INFO)\n                sublogger.setLevel(logging.INFO)\n            \"\"\" % (logger.name, sublogger.name))\n\n    caplog._finalize()\n    caplog._item = pytest.Item(\"test_foo\", None)\n    test_foo()\n    assert [x.message for x in caplog.records] == [\"1\", \"2\"]\n", "def test_log_format_with_auto_indent(testdir):\n    testdir.makeini(\"[pytest]\\nlog_auto_indent = True\\n\")\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n\n            logging.info(\"foo\\\\nbar\")\n            assert caplog.text == \"INFO     foo\\\\n    bar\\\\n\"\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.assert_outcomes(passed=1)\n", "def test_caplog_set_level(testdir):\n    p = testdir.makepyfile(\n        \"\"\"\n            import logging\n            import pytest\n\n                caplog.set_level(logging.INFO, logger=\"root\")\n                logging.getLogger(\"root\").info(\"1\")\n                assert len(caplog.records) == 1\n                caplog.set_level(logging.WARNING, logger=\"root\")\n                logging.getLogger(\"root\").info(\"2\")\n                assert len(caplog.records) == 1\n                caplog.set_level(logging.INFO)\n                logging.getLogger(\"root\").info(\"3\")\n                assert len(caplog.records) == 2\n    \"\"\"\n    )\n    result = testdir.runpytest(p)\n    result.assert_outcomes(passed=1)\n", "def test_caplog_captures_messages_from_nested_loggers(caplog):\n    caplog.set_level(logging.INFO)\n    logger.info(\"parent\")\n    sublogger.info(\"child\")\n\n    assert len(caplog.records) == 2\n    assert \"parent\" in caplog.text\n    assert \"child\" in caplog.text\n"], "sample_688": ["def test_import_path_mode_importlib(tmp_path: Path) -> None:\n    \"\"\"Test import_path with mode='importlib'\"\"\"\n    # Create a package with an __init__.py file and a module inside it\n    package_dir = tmp_path / \"my_package\"\n    package_dir.mkdir()\n    init_file = package_dir / \"__init__.py\"\n    init_file.touch()\n    module_file = package_dir / \"my_module.py\"\n    module_file.write_text(\"def my_function(): pass\")\n\n    # Import the module using import_path with mode='importlib'\n    mod = import_path(module_file, mode=ImportMode.importlib)\n\n    # Check that the module was imported correctly\n    assert hasattr(mod, \"my_function\")\n", "def test_ensure_reset_dir(tmp_path: Path) -> None:\n    dir = tmp_path / \"dir\"\n    dir.mkdir()\n    file = dir / \"file.txt\"\n    file.touch()\n    ensure_reset_dir(dir)\n    assert not file.exists()\n    assert dir.exists() and dir.is_dir()\n", "def test_absolutepath(tmp_path: Path) -> None:\n    rel_path = tmp_path.relative_to(\".\")\n    assert not rel_path.is_absolute()\n    abs_path = absolutepath(rel_path)\n    assert abs_path.is_absolute()\n    assert abs_path == tmp_path.resolve()\n\n    # Ensure that the function also works with strings.\n    abs_path_str = absolutepath(str(rel_path))\n    assert abs_path_str.is_absolute()\n    assert abs_path_str == tmp_path.resolve()\n", "def test_ensure_extended_length_path(tmpdir):\n    \"\"\"Test ensure_extended_length_path on Windows.\"\"\"\n    if not sys.platform.startswith(\"win32\"):\n        pytest.skip(\"Windows-specific test\")\n\n    # Create a path longer than MAX_PATH\n    long_path = tmpdir.join(\"a\" * 260)\n\n    # ensure_extended_length_path should convert it to an extended-length path\n    extended_path = ensure_extended_length_path(Path(str(long_path)))\n\n    assert str(extended_path).startswith(\"\\\\\\\\?\\\\\")\n", "def test_make_numbered_dir(tmpdir):\n    \"\"\"Test that make_numbered_dir creates a directory with an increased number as suffix.\"\"\"\n    root = Path(tmpdir)\n    prefix = \"prefix\"\n    p = make_numbered_dir(root, prefix)\n    assert p.exists()\n    assert p.name.startswith(prefix)\n    assert p.name[len(prefix):].isdigit()\n\n    # Create another directory to check the increment.\n    p2 = make_numbered_dir(root, prefix)\n    assert p2.exists()\n    assert p2.name.startswith(prefix)\n    assert int(p2.name[len(prefix):]) == int(p.name[len(prefix):]) + 1\n"], "sample_689": ["def test_deprecated_external_plugins():\n    \"\"\"Test that the DEPRECATED_EXTERNAL_PLUGINS set is correctly defined.\"\"\"\n    assert isinstance(deprecated.DEPRECATED_EXTERNAL_PLUGINS, set)\n    assert len(deprecated.DEPRECATED_EXTERNAL_PLUGINS) == 3\n    for plugin in deprecated.DEPRECATED_EXTERNAL_PLUGINS:\n        assert isinstance(plugin, str)\n", "def test_deprecation_warnings(warning, expected_message):\n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter(\"always\")\n        warnings.warn(warning)\n        assert len(w) == 1\n        assert re.search(expected_message, str(w[0].message))\n", "def test_deprecated_external_plugins(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n            config.pluginmanager.register(\"pytest_catchlog\", \"catchlog\")\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    assert result.ret == 0\n    warning = result.parseoutcomes()[\"warnings\"][0]\n    assert \"pytest_catchlog\" in warning.message\n", "def test_deprecated_warnings(warning_type, expected_message):\n    with pytest.warns(warning_type) as record:\n        warnings.warn(\"\", warning_type)\n    assert re.search(expected_message, str(record[0].message))\n", "def test_deprecated_external_plugins():\n    assert isinstance(deprecated.DEPRECATED_EXTERNAL_PLUGINS, set)\n    assert len(deprecated.DEPRECATED_EXTERNAL_PLUGINS) > 0\n    for plugin in deprecated.DEPRECATED_EXTERNAL_PLUGINS:\n        assert isinstance(plugin, str)\n"], "sample_690": ["def test_skip_mark_with_empty_reason(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.skip()\n            pass\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines([\"SKIP [1] *test_skip.py:3: unconditional skip\"])\n", "def test_xfail_strict(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.xfail(strict=True)\n            pass\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=0, failed=1, skipped=0)\n    assert \"XPASS(strict)\" in result.stdout.str()\n", "def test_xfail_strict(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.xfail(strict=True)\n            pass\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    assert result.ret == 1\n    result.stdout.fnmatch_lines([\"*XPASS(strict)*\"])\n", "def test_skip_marks_with_condition(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        textwrap.dedent(\n            \"\"\"\\\n            import pytest\n\n            @pytest.mark.skipif(sys.platform == 'win32')\n                assert 1 + 1 == 2\n\n            @pytest.mark.skipif(sys.platform != 'win32')\n                assert 1 + 1 == 2\n            \"\"\"\n        )\n    )\n    result = pytester.runpytest()\n    assert \"skipped\" in result.stdout.fnmatch_lines(\"test_skip_windows*\")\n    assert \"skipped\" not in result.stdout.fnmatch_lines(\"test_skip_not_windows*\")\n\n    # On Windows, the first test should be skipped and the second test should run.\n    # On non-Windows, the first test should run and the second test should be skipped.\n    if sys.platform == \"win32\":\n        assert result.ret == 0\n    else:\n        assert result.ret == 0\n", "def test_skip_mark_with_condition(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.skipif(False, reason=\"condition is false\")\n            assert False\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)\n    assert \"skipped\" not in result.outlines\n"], "sample_691": ["def test_faulthandler_hooks_enabled(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import faulthandler\n        import pytest\n\n            import time\n            while True:\n                time.sleep(1)\n        \"\"\"\n    )\n    result = pytester.runpytest(\"--faulthandler-timeout=1\")\n    assert result.ret == 0\n    assert \"Fault handler activated\" in result.stdout.str()\n    assert \"Current thread\" in result.stdout.str()\n", "def test_faulthandler_timeout(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import time\n\n            time.sleep(1)\n        \"\"\"\n    )\n    result = pytester.runpytest(\"--faulthandler-timeout=0.1\")\n    assert \"Dumping traceback to stderr due to timeout\" in result.stdout.str()\n    assert result.ret == 0\n", "def test_fault_handler_stderr_key(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import faulthandler\n        import pytest\n\n            assert isinstance(pytest.config._store[fault_handler_stderr_key], io.TextIOWrapper)\n    \"\"\"\n    )\n    result = pytester.runpytest(\"--faulthandler-timeout=1.0\")\n    result.assert_outcomes(passed=1)\n", "def test_fault_handler_timeout(pytester: Pytester) -> None:\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        faulthandler_timeout = 0.1\n    \"\"\"\n    )\n\n    pytester.makepyfile(\n        \"\"\"\n        import time\n\n            time.sleep(0.2)\n    \"\"\"\n    )\n\n    result = pytester.runpytest()\n    assert result.ret == 0\n    assert \"Timeout\" not in result.stdout.str()\n    assert \"Traceback\" in result.stdout.str()\n", "def test_fault_handler_stderr_key(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n        import sys\n        from _pytest.config import Config\n        from _pytest.store import StoreKey\n\n        fault_handler_stderr_key = StoreKey[object]()\n            sys.stderr = object()\n            config._store[fault_handler_stderr_key] = sys.stderr\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    assert result.ret == 0\n"], "sample_692": ["def test_tmp_path_factory_cleanup_lock(pytestconfig, tmp_path_factory: TempPathFactory) -> None:\n    \"\"\"Test that the cleanup lock is created and removed correctly.\"\"\"\n    basetemp = tmp_path_factory.getbasetemp()\n    lockfile = create_cleanup_lock(basetemp)\n    assert lockfile.exists()\n\n    # Simulate a new pytest session\n    new_config = Config.fromdictargs(pytestconfig.known_args_namespace, {\"basetemp\": basetemp})\n    new_factory = TempPathFactory.from_config(new_config)\n\n    # The lock file should still exist\n    assert lockfile.exists()\n\n    # Clean up the basetemp directory\n    rm_rf(basetemp)\n\n    # The lock file should be gone\n    assert not lockfile.exists()\n", "def test_get_user(monkeypatch: MonkeyPatch) -> None:\n    import getpass\n\n        raise ImportError()\n\n    monkeypatch.setattr(getpass, \"getuser\", raise_error)\n    assert get_user() is None\n\n        return \"test_user\"\n\n    monkeypatch.setattr(getpass, \"getuser\", return_value)\n    assert get_user() == \"test_user\"\n", "def test_tmpdir_factory_basetemp(tmp_path_factory: TempPathFactory, monkeypatch: MonkeyPatch) -> None:\n    \"\"\"Test that the tmpdir_factory fixture uses the correct basetemp directory (#4427).\"\"\"\n    basetemp = Path(\"my_basetemp\")\n    monkeypatch.setattr(tmp_path_factory, \"_given_basetemp\", basetemp)\n    assert tmp_path_factory.getbasetemp() == basetemp.resolve()\n", "def test_tmp_path_factory_basetemp(tmp_path_factory: TempPathFactory, tmpdir: py.path.local) -> None:\n    \"\"\"Test that tmp_path_factory returns a Path object and uses the correct basetemp (#4427).\"\"\"\n    # Use os.path.abspath() to get absolute path instead of resolve() as it does not work the same in all platforms.\n    assert str(tmp_path_factory.getbasetemp()) == os.path.abspath(str(tmpdir))\n    assert isinstance(tmp_path_factory.mktemp(\"foo\"), Path)\n", "def test_get_user(monkeypatch: MonkeyPatch) -> None:\n    import getpass\n\n        raise ImportError()\n\n    monkeypatch.setattr(getpass, \"getuser\", raise_error)\n    assert get_user() is None\n\n        return \"invalid-user\"\n\n    monkeypatch.setattr(getpass, \"getuser\", return_invalid_user)\n    assert get_user() == \"invalid-user\"\n"], "sample_693": ["def test_unittest_skip_reason(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        class MyTest(unittest.TestCase):\n            @unittest.skip(\"some reason\")\n                pass\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines([\"*Skipped: some reason*\"])\n", "def test_trial_reporter_protocol(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n        from twisted.trial import unittest as trial_unittest\n\n        class Test(trial_unittest.TestCase):\n                self.assertEqual(1, 1)\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    assert result.ret == ExitCode.OK\n", "def test_trial_reporter_protocol(pytester: Pytester, monkeypatch: MonkeyPatch) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n        from twisted.trial.unittest import TestCase\n\n        class TestTrial(TestCase):\n                pass\n    \"\"\"\n    )\n\n        return True\n\n    monkeypatch.setattr(\n        \"pytest._pytest.unittest.check_testcase_implements_trial_reporter\",\n        check_testcase_implements_trial_reporter_mock,\n    )\n    result = pytester.runpytest()\n    assert result.ret == ExitCode.OK\n    result.stdout.fnmatch_lines([\"*1 passed*\"])\n", "def test_unittest_skip_reason(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        class TestFoo(unittest.TestCase):\n            @unittest.skip(\"because\")\n                pass\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    assert result.ret == ExitCode.OK\n    result.stdout.fnmatch_lines([\"*SKIP because*\"])\n", "def test_unittest_skip_test(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        class TestSkippedTest(unittest.TestCase):\n            @unittest.skip(\"skipped\")\n                assert 0\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    result.assert_outcomes(skipped=1)\n    assert result.ret == ExitCode.OK\n"], "sample_694": ["def test_nose_support_method(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        class TestSomething:\n                self.setup_called = False\n                self.teardown_called = False\n\n                self.setup_called = True\n\n                self.teardown_called = True\n\n                assert self.setup_called\n                assert not self.teardown_called\n    \"\"\"\n    )\n    result = pytester.runpytest(\"-p\", \"no:pytest_nose\")\n    result.assert_outcomes(passed=1, warnings=2)\n\n    warnings_list = [\n        warning.message.args[0]\n        for warning in result.parseoutcomes()[\"warnings\"]\n        if issubclass(warning.category, PytestDeprecationWarning)\n    ]\n\n    assert len(warnings_list) == 2\n    assert all(\"Support for nose tests\" in str(warning) for warning in warnings_list)\n    assert any(\"setup(self)\" in str(warning) for warning in warnings_list)\n    assert any(\"teardown(self)\" in str(warning) for warning in warnings_list)\n", "def test_nose_support_method(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        class TestNoseMethod:\n                pass\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines(\n        \"*PytestRemovedIn8Warning: Support for nose tests is deprecated*\"\n    )\n\n", "def test_nose_support_methods(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n        class Test:\n                pass\n\n                pass\n\n                pass\n\n                pass\n\n            pass\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*Support for nose tests is deprecated and will be removed in a future release.*\",\n            \"*uses nose-specific method: `setup(self)`*\",\n            \"*uses nose-specific method: `teardown(self)`*\",\n        ]\n    )\n    result.assert_outcomes(passed=1, warnings=4)\n", "def test_hook_legacy_path_arg_warning(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n            pass\n        \"\"\"\n    )\n    pytester.makepyfile(\"def test_pass(): pass\")\n    result = pytester.runpytest(\"-vv\", \"-p\", \"no:pytester\")\n    assert result.ret == 0\n    result.stdout.fnmatch_lines(\"*deprecated*py.path.local argument is deprecated*\")\n", "def test_deprecated_nose_support_method():\n    pytester = Pytester()\n    p = pytester.makepyfile(\n        \"\"\"\n        import pytest\n        class Test:\n                pass\n        \"\"\"\n    )\n    result = pytester.runpytest(p, \"-p\", \"nose\")\n    assert result.ret == 0\n    assert \"Support for nose tests is deprecated\" in result.stdout.str()\n    assert re.search(r\"test_method\\(self\\)\", result.stdout.str())\n    assert re.search(r\"See docs: https://docs\\.pytest\\.org/en/stable/deprecations\\.html#support-for-tests-written-for-nose\", result.stdout.str())\n"], "sample_695": ["def test_node_repr_with_name() -> None:\n    node = nodes.Node(name=\"test_node\", nodeid=\"node_id\")\n    assert repr(node) == \"<Node test_node>\"\n", "def test_iterparentnodeids(nodeid, expected):\n    assert list(nodes.iterparentnodeids(nodeid)) == expected\n", "def test_iterparentnodeids_edge_cases() -> None:\n    assert list(nodes.iterparentnodeids(\"::\")) == [\"\", \"::\"]\n    assert list(nodes.iterparentnodeids(\"a::\")) == [\"\", \"a\", \"a::\"]\n    assert list(nodes.iterparentnodeids(\"::a\")) == [\"\", \"::a\"]\n    assert list(nodes.iterparentnodeids(\"a::b::\")) == [\"\", \"a\", \"a::b\", \"a::b::\"]\n", "def test_get_fslocation_from_item(tmp_path):\n    path = tmp_path / \"foo.py\"\n    node = nodes.Item.from_parent(parent=nodes.Collector(), name=\"foo\", nodeid=\"foo\")\n    node.location = (str(path), 10, \"\")\n    assert get_fslocation_from_item(node) == (path, 10)\n\n    node2 = nodes.Item.from_parent(parent=nodes.Collector(), name=\"foo\", nodeid=\"foo\")\n    node2.obj = object()\n    assert get_fslocation_from_item(node2) == (\"unknown location\", -1)\n", "def test_get_fslocation_from_item(tmp_path: Path) -> None:\n    fspath = tmp_path / \"foo.py\"\n    fspath.touch()\n\n    class MockNode(nodes.Node):\n            super().__init__(\"MockNode\", parent=None)\n            self.fspath = legacy_path(fspath)\n            self.location = location\n\n    # Test that get_fslocation_from_item returns the location of a node if set.\n    node = MockNode(fspath, (\"path/to/foo.py\", 10, \"\"))\n    assert nodes.get_fslocation_from_item(node) == (str(fspath), 10)\n\n    # Test that get_fslocation_from_item returns the fspath of a node if no location is set.\n    node = MockNode(fspath, None)\n    assert nodes.get_fslocation_from_item(node) == (str(fspath), -1)\n\n    # Test that get_fslocation_from_item returns the location from getfslineno for an object.\n    obj = object()\n    getfslineno_mock = cast(Type[nodes.Node], type(\"MockNode\", (nodes.Node,), {}))\n    getfslineno_mock.getfslineno = lambda x: (\"path/from/getfslineno\", 20)\n    node = MockNode(fspath, None)\n    node.obj = obj\n    assert nodes.get_fslocation_from_item(node) == (\"path/from/getfslineno\", 20)\n"], "sample_696": ["def test_check_ispytest():\n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter(\"always\")\n        deprecated.check_ispytest(False)\n        assert len(w) == 1\n        assert issubclass(w[-1].category, PytestDeprecationWarning)\n        assert \"A private pytest class or function was used\" in str(w[-1].message)\n\n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter(\"always\")\n        deprecated.check_ispytest(True)\n        assert not w\n", "def test_warnings(warning):\n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter(\"always\")\n        warnings.warn(warning)\n        assert len(w) == 1\n        assert issubclass(w[0].category, PytestDeprecationWarning)\n        assert str(w[0].message) == str(warning)\n", "def test_hook_legacy_path_arg_warns(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n\n            pass\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines(\"PytestDeprecationWarning:*The (pylib_path_arg: py.path.local)*argument is deprecated*\")\n", "def test_hook_legacy_path_arg():\n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter(\"always\")\n        deprecated.HOOK_LEGACY_PATH_ARG.warn(\n            pylib_path_arg=\"pylib_path_arg\", pathlib_path_arg=\"pathlib_path_arg\"\n        )\n        assert len(w) == 1\n        assert issubclass(w[-1].category, PytestDeprecationWarning)\n        assert \"The (pylib_path_arg: py.path.local)\" in str(w[-1].message)\n        assert \"(pathlib_path_arg: pathlib.Path)\" in str(w[-1].message)\n", "def test_hook_legacy_path_arg(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n\n        @pytest.hookimpl\n            pass\n    \"\"\"\n    )\n    with pytest.raises(PytestDeprecationWarning, match=r\"The \\(path: py\\.path\\.local\\) argument is deprecated\"):\n        pytester.runpytest()\n"], "sample_697": ["def test_tmp_path_factory_basetemp(tmp_path_factory: TempPathFactory, tmpdir: Path) -> None:\n    \"\"\"Test that the basetemp is correctly created and cleaned up.\"\"\"\n    basetemp = tmp_path_factory.getbasetemp()\n    assert basetemp.exists()\n    assert basetemp.is_dir()\n\n    # Create a file in the basetemp directory.\n    file = basetemp / \"test_file.txt\"\n    file.touch()\n\n    # Make sure the file exists.\n    assert file.exists()\n\n    # Clean up the basetemp directory.\n    rm_rf(basetemp)\n\n    # Make sure the file no longer exists.\n    assert not file.exists()\n", "def test_get_user(monkeypatch: MonkeyPatch) -> None:\n    import getpass\n\n        raise ImportError(\"Mocked ImportError\")\n\n    monkeypatch.setattr(getpass, \"getuser\", raise_error)\n    assert get_user() is None\n\n        raise KeyError(\"Mocked KeyError\")\n\n    monkeypatch.setattr(getpass, \"getuser\", raise_key_error)\n    assert get_user() is None\n\n        return \"test_user\"\n\n    monkeypatch.setattr(getpass, \"getuser\", return_user)\n    assert get_user() == \"test_user\"\n", "def test_tmp_path_factory_basetemp_with_numbered_suffix(tmp_path_factory: TempPathFactory, monkeypatch: MonkeyPatch) -> None:\n    \"\"\"Test that tmp_path_factory creates basetemp with a numbered suffix (#4427).\"\"\"\n    # Arrange.\n    basetemp_parent = Path(\"basetemp-parent\")\n    basetemp_parent.mkdir()\n    try:\n        # Act.\n        basetemp = tmp_path_factory.getbasetemp()\n\n        # Assert.\n        assert basetemp.parent == basetemp_parent\n        assert basetemp.name.startswith(\"pytest-\")\n\n        # Act.\n        numbered_basetemp = tmp_path_factory.mktemp(\"foo-\", numbered=True)\n\n        # Assert.\n        assert numbered_basetemp.parent == basetemp\n        assert numbered_basetemp.name.startswith(\"foo-\")\n    finally:\n        rm_rf(basetemp_parent)\n", "def test_tmp_path_factory_basetemp(tmp_path_factory: TempPathFactory, monkeypatch: MonkeyPatch) -> None:\n    \"\"\"Test that the tmp_path_factory's basetemp is created correctly (#4427).\"\"\"\n    basetemp = Path(\"/path/to/basetemp\")\n    monkeypatch.setattr(tmp_path_factory, \"_given_basetemp\", basetemp)\n    assert tmp_path_factory.getbasetemp() == basetemp.resolve()\n    assert tmp_path_factory.getbasetemp().exists()\n", "def test_tmp_path_factory_creates_unique_directory_names(tmp_path_factory: TempPathFactory) -> None:\n    \"\"\"Test that the temporary directories created by TempPathFactory have unique names.\"\"\"\n    path1 = tmp_path_factory.mktemp(\"test_dir\", numbered=True)\n    path2 = tmp_path_factory.mktemp(\"test_dir\", numbered=True)\n    assert path1 != path2\n    assert path1.parent == path2.parent\n"], "sample_698": ["def test_colored_level_formatter_create_terminal_writer(config) -> None:\n    terminal_writer = TerminalWriter(config)\n    formatter = ColoredLevelFormatter(terminal_writer, \"%(levelname)s:%(message)s\")\n    assert formatter._level_to_fmt_mapping == {}\n", "def test_colored_level_formatter(tmpdir):\n    formatter = ColoredLevelFormatter(\n        create_terminal_writer(config=None), \"%(levelname)s:%(message)s\"\n    )\n    record = logging.LogRecord(\"test\", logging.INFO, \"test.py\", 1, \"message\", None, None)\n    output = formatter.format(record)\n    assert \"\\033[32m\" in output  # Green color code for INFO level\n    assert \"INFO:message\" in _remove_ansi_escape_sequences(output)\n", "def test_colored_level_formatter(tmpdir, capsys, request):\n    \"\"\"Test that ColoredLevelFormatter preserves the color of the level name.\"\"\"\n    log_file = tmpdir.join(\"log.txt\")\n    formatter = ColoredLevelFormatter(\n        create_terminal_writer(request.config), \"%(levelname)s:%(message)s\"\n    )\n    record = logging.makeLogRecord(\n        \"name\", logging.INFO, \"path\", 1, \"message\", None, None\n    )\n\n    # Act\n    formatted = formatter.format(record)\n\n    # Assert\n    assert \"\\x1b[32mINFO\\x1b[0m:message\" == formatted\n\n    # Act\n    with open(log_file, \"w\") as f:\n        f.write(formatted)\n\n    # Assert\n    with open(log_file) as f:\n        assert f.read() == \"\\x1b[32mINFO\\x1b[0m:message\"\n", "def test_colored_level_formatter_create_terminal_writer(monkeypatch, mocker):\n    class MockTerminalWriter:\n            pass\n\n    monkeypatch.setattr(\"_pytest._io.TerminalWriter\", MockTerminalWriter)\n    formatter = ColoredLevelFormatter.create_terminal_writer()\n    assert isinstance(formatter, TerminalWriter)\n\n    mock_get_option = mocker.patch(\"pytest.config.getoption\")\n    mock_get_option.return_value = \"no\"\n    formatter = ColoredLevelFormatter(\n        create_terminal_writer(), \"%(levelname)s:%(message)s\"\n    )\n    assert not isinstance(formatter, ColoredLevelFormatter)\n    assert isinstance(formatter, logging.Formatter)\n", "def test_colored_level_formatter(tmpdir):\n    formatter = ColoredLevelFormatter(\n        TerminalWriter(f\"terminal-{tmpdir}\"), \"%(levelname)s:%(name)s:%(message)s\"\n    )\n    record = logging.LogRecord(\"name\", logging.INFO, \"pathname\", 10, \"message\", None, None)\n    formatted = formatter.format(record)\n    assert \"\\x1b[\" in formatted\n"], "sample_699": ["def test_doctest_namespace_fixture(pytester: Pytester) -> None:\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        addopts = --doctest-modules\n        \"\"\"\n    )\n    pytester.makepyfile(\n        \"\"\"\n            '''\n            >>> foo()\n            'bar'\n            '''\n            return 'bar'\n\n            assert foo() == 'bar'\n        \"\"\"\n    )\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture(autouse=True)\n            return {'foo': lambda: 'baz'}\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=2)\n", "def test_get_checker() -> None:\n    checker = _get_checker()\n    assert hasattr(checker, \"check_output\")\n    assert inspect.ismethod(checker.check_output)\n    assert checker.check_output(\"foo\", \"foo\", 0) is True\n", "def test_get_checker():\n    checker = _get_checker()\n    assert hasattr(checker, \"check_output\")\n    assert inspect.isclass(type(checker))\n", "def test_doctest_namespace_fixture(pytester: Pytester) -> None:\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        doctest_optionflags = ALLOW_UNICODE ALLOW_BYTES\n    \"\"\"\n    )\n    pytester.makepyfile(\n        \"\"\"\n            return \"external\"\n\n        class Test:\n                self.value = \"value\"\n\n            return Test()\n\n            return test.value\n\n            '''\n            >>> external()\n            'external'\n            >>> get_test().value\n            'value'\n            >>> internal(get_test())\n            'value'\n            '''\n    \"\"\"\n    )\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture(scope=\"session\", autouse=True)\n            return {\"external\": external, \"get_test\": get_test, \"internal\": internal}\n    \"\"\"\n    )\n    result = pytester.runpytest(\"--doctest-modules\")\n    result.assert_outcomes(passed=1)\n", "def test_doctest_namespace_fixture(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture(autouse=True)\n            doctest_namespace[\"myvar\"] = 42\n    \"\"\"\n    )\n    p = pytester.makepyfile(\n        \"\"\"\n            '''\n            >>> add(myvar, 1)\n            43\n            '''\n            return a + b\n    \"\"\"\n    )\n    result = pytester.runpytest(\"--doctest-modules\", p)\n    assert result.ret == 0\n"], "sample_700": ["def test_parametrize_id_functions(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        textwrap.dedent(\n            \"\"\"\n            import pytest\n\n                if isinstance(val, str):\n                    return val\n                else:\n                    raise ValueError(\"idfn failed\")\n\n            @pytest.mark.parametrize('x', [1, 'a'])\n                pass\n            \"\"\"\n        )\n    )\n    result = pytester.runpytest(\"--collect-only\")\n    assert result.ret == 0\n", "def test_skip_mark_evaluation(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.skip(reason=\"skipped_test\")\n            assert 1 == 0\n    \"\"\"\n    )\n\n    item = pytester.getitem(\"test_skip\")\n    assert evaluate_skip_marks(item) is True\n    assert item.keywords[\"skip\"].kwargs[\"reason\"] == \"skipped_test\"\n", "def test_pytest_pyfunc_call_sets_request_params_withMarks(\n    pytester: Pytester,", "def test_evaluate_skip_marks_with_condition_string(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        textwrap.dedent(\n            \"\"\"\n            import pytest\n\n            @pytest.mark.skip(\"reason\")\n                pass\n\n            @pytest.mark.skip(reason=\"reason\")\n                pass\n\n            @pytest.mark.skip(condition=\"1 + 1 == 2\", reason=\"reason\")\n                pass\n            \"\"\"\n        )\n    )\n\n    items = pytester.getitems(\"test_*.py\")\n    for item in items:\n        if item.name == \"test_skip\":\n            assert evaluate_skip_marks(item) is True\n        elif item.name == \"test_skip_reason\":\n            assert evaluate_skip_marks(item) is True\n        elif item.name == \"test_skip_condition\":\n            assert evaluate_skip_marks(item) is True\n", "def test_parametrize_eval_skips(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.parametrize(\"arg\", [1, 2])\n            if arg == 1:\n                pytest.skip(\"skip reason\")\n            assert arg == 2\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.assert_outcomes(skipped=1, passed=1)\n"], "sample_701": ["def test_hook_legacy_path_arg(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n\n            pass\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines(\n        \"*The (pylib_path_arg: py.path.local) argument is deprecated*\"\n    )\n", "def test_hook_legacy_path_arg_warning():\n    pytester = Pytester()\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n            pass\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines(\n        \"*The (pylib_path_arg: py.path.local) argument is deprecated*\"\n    )\n", "def test_check_ispytest():\n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter(\"always\")\n        deprecated.check_ispytest(False)\n        assert len(w) == 1\n        assert issubclass(w[0].category, PytestDeprecationWarning)\n        assert \"A private pytest class or function was used\" in str(w[0].message)\n\n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter(\"always\")\n        deprecated.check_ispytest(True)\n        assert len(w) == 0\n", "def test_deprecation_messages(warning, message):\n    with pytest.warns(PytestDeprecationWarning, match=message):\n        warnings.warn(warning, stacklevel=2)\n", "def test_hook_legacy_path_arg_deprecation(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n            pass\n        \"\"\"\n    )\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n            pass\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines(\n        \"*The (pylib_path_arg: py.path.local) argument is deprecated*\"\n    )\n"], "sample_702": ["def test_pytester_runpytest_args(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n            pass\n            assert 0\n    \"\"\"\n    )\n    result = pytester.runpytest(\"-k\", \"test_pass\")\n    assert result.ret == ExitCode.OK\n    result = pytester.runpytest(\"-k\", \"test_fail\")\n    assert result.ret == ExitCode.TESTS_FAILED\n", "def test_syspathinsert(tmp_path: Path, pytester: Pytester) -> None:\n    pytester.makepyfile(\"import sys; print(sys.path)\")\n    pytester.syspathinsert(tmp_path)\n    result = pytester.runpython(\"test_syspathinsert.py\")\n    assert str(tmp_path) in result.stdout.lines[0]\n", "def test_runpytest_subprocess_timeout(pytester: Pytester) -> None:\n    pytester.makepyfile(\"import time; time.sleep(10)\")\n    result = pytester.runpytest_subprocess(timeout=1)\n    assert isinstance(result, pytester_mod.RunResult)\n    assert result.ret == ExitCode.INTERRUPTED\n", "def test_make_hook_recorder(pytester: Pytester) -> None:\n    pm = PytestPluginManager()\n    hook_recorder = pytester.make_hook_recorder(pm)\n    assert isinstance(hook_recorder, HookRecorder)\n    assert hook_recorder._pluginmanager is pm\n", "def test_runpytest_subprocess_timeout(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import time\n            time.sleep(10)\n    \"\"\"\n    )\n    result = pytester.runpytest_subprocess(\"--runpytest\", \"subprocess\", timeout=1)\n    assert result.ret == ExitCode.INTERRUPTED\n"], "sample_703": ["def test_empty_expression() -> None:\n    assert not evaluate(\"\", lambda x: True)\n", "def test_syntax_error_reporting() -> None:\n    with pytest.raises(ParseError) as excinfo:\n        Expression.compile(\"foo bar\")\n    assert str(excinfo.value).startswith(\"at column 4: \")\n", "def test_syntax_error():\n    with pytest.raises(ParseError) as excinfo:\n        Expression.compile(\"foo bar\")\n    assert \"expected end of input; got identifier\" in str(excinfo.value)\n", "def test_empty_expression() -> None:\n    assert not evaluate(\"\", lambda x: True)\n    assert not evaluate(\"   \", lambda x: True)\n", "def test_empty_expression() -> None:\n    with pytest.raises(ParseError):\n        Expression.compile(\"\")\n"], "sample_704": ["def test_iterparentnodeids_after_first_colon(nodeid, expected):\n    \"\"\"Test that iterparentnodeids() returns the correct parent node IDs after the first ::\"\"\"\n    assert list(nodes.iterparentnodeids(nodeid))[1:] == expected\n", "def test_iterparentnodeids_edge_cases():\n    assert list(nodes.iterparentnodeids(\"::\")) == [\"\", \"::\"]\n    assert list(nodes.iterparentnodeids(\"/::\")) == [\"\", \"/::\"]\n    assert list(nodes.iterparentnodeids(\"/a::b\")) == [\"\", \"a\", \"/a::b\"]\n", "def test_node_keywords_marked() -> None:\n    node = nodes.Node.from_parent(\n        parent=nodes.Node.from_parent(parent=None, name=\"parent\", nodeid=\"parent\"),\n        name=\"node\",\n        nodeid=\"parent::node\",\n    )\n    mark1 = pytest.mark.foo\n    mark2 = pytest.mark.bar\n    node.add_marker(mark1)\n    node.add_marker(mark2)\n\n    assert set(node.keywords) == {\"foo\", \"bar\"}\n    assert len(node.own_markers) == 2\n    assert node.own_markers[0].name == \"foo\"\n    assert node.own_markers[1].name == \"bar\"\n", "def test_iterparentnodeids_edge_cases():\n    assert list(nodes.iterparentnodeids(\"::\")) == [\"\", \"::\"]\n    assert list(nodes.iterparentnodeids(\"a::\")) == [\"\", \"a\", \"a::\"]\n    assert list(nodes.iterparentnodeids(\"a::b::\")) == [\"\", \"a\", \"a::b\", \"a::b::\"]\n    assert list(nodes.iterparentnodeids(\"a/b::c::d::\")) == [\n        \"\",\n        \"a\",\n        \"a/b\",\n        \"a/b::c\",\n        \"a/b::c::d\",\n        \"a/b::c::d::\",\n    ]\n", "def test_node_repr_with_name() -> None:\n    node = nodes.Node(name=\"test_name\", nodeid=\"node_id\")\n    assert repr(node) == \"<Node test_name>\"\n"], "sample_705": ["def test_runpytest_inprocess_timeout(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import time\n            time.sleep(10)\n        \"\"\"\n    )\n    result = pytester.runpytest_inprocess(\"--runpytest\", \"inprocess\", timeout=1.0)\n    assert result.ret == ExitCode.INTERRUPTED\n", "def test_runpytest_inprocess(pytester: Pytester) -> None:\n    pytester.makepyfile(\"def test_pass(): pass\")\n    result = pytester.runpytest_inprocess()\n    assert result.ret == ExitCode.OK\n", "def test_make_hook_recorder(pytester: Pytester) -> None:\n    class DummyPluginManager(PytestPluginManager):\n            self._hooks = []\n\n            self._hooks.append((before, after))\n\n    pluginmanager = DummyPluginManager()\n    hook_recorder = pytester.make_hook_recorder(pluginmanager)\n    assert isinstance(hook_recorder, HookRecorder)\n    assert len(pluginmanager._hooks) == 1\n", "def test_monkeypatch_preservation(pytester: Pytester) -> None:\n    pytester.makepyfile(\"def test_foo(): pass\")\n    monkeypatch = MonkeyPatch()\n    old_cwd = os.getcwd()\n    with monkeypatch.context() as m:\n        m.chdir(pytester.path)\n        result = pytester.runpytest_inprocess()\n    assert result.ret == ExitCode.OK\n    assert os.getcwd() == old_cwd\n", "def test_inline_run(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n            pass\n    \"\"\"\n    )\n    reprec = pytester.inline_run()\n    assert reprec.ret == 0\n    calls = reprec.getcalls(\"pytest_runtest_logreport\")\n    assert len(calls) == 1\n    call = calls[0]\n    report = call.report\n    assert report.passed\n    assert report.failed == False\n"], "sample_706": ["def test_expression_empty() -> None:\n    class Matcher:\n            return False\n\n    with pytest.raises(ParseError):\n        evaluate(\"\", Matcher())\n", "def test_syntax_error():\n    with pytest.raises(ParseError) as excinfo:\n        evaluate(\"foo or\", lambda x: True)\n    assert str(excinfo.value).startswith(\"at column\")\n", "def test_simple_expressions() -> None:\n        return True\n\n    assert evaluate(\"True\", match_always)\n    assert not evaluate(\"False\", match_always)\n    assert evaluate(\"not False\", match_always)\n    assert not evaluate(\"not True\", match_always)\n    assert evaluate(\"True and True\", match_always)\n    assert not evaluate(\"True and False\", match_always)\n    assert evaluate(\"True or False\", match_always)\n    assert not evaluate(\"False or False\", match_always)\n", "def test_empty_expression() -> None:\n    assert not evaluate(\"\", lambda _: True)\n    assert not evaluate(\"   \", lambda _: True)\n", "def test_compile_and_evaluate() -> None:\n        return ident == \"foo\"\n\n    assert evaluate(\"foo\", matcher)\n    assert not evaluate(\"bar\", matcher)\n    assert evaluate(\"foo or bar\", matcher)\n    assert evaluate(\"foo and foo\", matcher)\n    assert not evaluate(\"foo and bar\", matcher)\n    assert evaluate(\"(foo or bar) and foo\", matcher)\n"], "sample_707": ["def test_iterparentnodeids_edge_cases() -> None:\n    assert list(nodes.iterparentnodeids(\"\")) == [\"\"]\n    assert list(nodes.iterparentnodeids(\"::\")) == [\"\", \"::\"]\n    assert list(nodes.iterparentnodeids(\"a/b/c::\")) == [\"\", \"a\", \"a/b\", \"a/b/c\", \"a/b/c::\"]\n    assert list(nodes.iterparentnodeids(\"a/b/c::D:\")) == [\n        \"\",\n        \"a\",\n        \"a/b\",\n        \"a/b/c\",\n        \"a/b/c::D:\",\n    ]\n", "def test_get_fslocation_from_item(tmp_path: Path) -> None:\n    path = tmp_path / \"test_file.py\"\n    path.touch()\n    node = nodes.Item.from_parent(parent=nodes.File(path=path), name=\"test_function\")\n    assert get_fslocation_from_item(node) == (path, None)\n\n    class MockNode(nodes.Node):\n            self.location = (path, lineno, \"\")\n\n    node = MockNode(path, 10)\n    assert get_fslocation_from_item(node) == (path, 10)\n\n    node = nodes.Item.from_parent(\n        parent=nodes.File(path=path), name=\"test_function\", nodeid=\"test_node_id\"\n    )\n    assert get_fslocation_from_item(node) == (path, None)\n", "def test_get_fslocation_from_item(tmp_path: Path) -> None:\n    path = tmp_path / \"test.txt\"\n    path.touch()\n    node = nodes.Item.from_parent(parent=nodes.Node.from_parent(parent=None), name=\"test\", nodeid=str(path))\n    assert get_fslocation_from_item(node) == (str(path), None)\n", "def test_iterparentnodeids_on_item_with_square_bracket_in_name():\n    node = nodes.Item(\n        name=\"test_name\",\n        parent=nodes.Collector(\"collector\", nodeid=\"a/b::c/d\"),\n        nodeid=\"a/b::c/d::e[/test]\",\n    )\n    expected = [\"\", \"a\", \"a/b\", \"a/b::c/d\", \"a/b::c/d::e[/test]\"]\n    assert list(node.iterparentnodeids()) == expected\n", "def test_get_fslocation_from_item(tmp_path: Path) -> None:\n    path = tmp_path / \"example.py\"\n    path.touch()\n\n    class DummyNode(nodes.Node):\n            self.path = path\n            self.obj = object()\n\n    node = DummyNode(path)\n    assert get_fslocation_from_item(node) == (str(path), None)\n\n    node.location = (\"other_path\", 10, \"\")\n    assert get_fslocation_from_item(node) == (\"other_path\", 10)\n\n    del node.location\n    del node.obj\n    assert get_fslocation_from_item(node) == (str(path), -1)\n"], "sample_708": ["def test_source_getstatementrange_multiline_string():\n    source = Source(\n        \"\"\"\n            x = '''hello\n            world'''\n            return x\n\n            pass\n    \"\"\"\n    )\n    start, end = source.getstatementrange(2)\n    assert start == 1\n    assert end == 4\n", "def test_Source_getstatement():\n    src = Source(\n        \"\"\"\n            pass\n\n            pass\n    \"\"\"\n    )\n    assert str(src.getstatement(0)) == \"def f(x):\\n    pass\"\n    assert str(src.getstatement(2)) == \"def g(x):\\n    pass\"\n\n    src = Source(\n        \"\"\"\n        x = 1\n        y = 2\n    \"\"\"\n    )\n    assert str(src.getstatement(0)) == \"x = 1\"\n    assert str(src.getstatement(1)) == \"y = 2\"\n", "def test_source_getstatement_multiline():\n    source = Source(\n        \"\"\"\n            pass\n\n            pass\n        \"\"\"\n    )\n    statement = source.getstatement(0)\n    assert len(statement) == 2\n    assert statement[0] == \"def foo():\"\n    assert statement[1] == \"            pass\"\n\n    statement = source.getstatement(3)\n    assert len(statement) == 2\n    assert statement[0] == \"def bar():\"\n    assert statement[1] == \"            pass\"\n", "def test_source_getstatement():\n    src = Source(\n        \"\"\"\n            pass\n\n            pass\n    \"\"\"\n    )\n    statement = src.getstatement(1)\n    assert len(statement) == 2\n    assert \"def foo\" in statement[0]\n    assert \"pass\" in statement[1]\n\n    statement = src.getstatement(4)\n    assert len(statement) == 2\n    assert \"def bar\" in statement[0]\n    assert \"pass\" in statement[1]\n", "def test_getstatementrange_ast_with_decorators():\n    source = Source(\n        \"\"\"\n        @decorator1\n        @decorator2\n            pass\n    \"\"\"\n    )\n    astnode, start, end = getstatementrange_ast(1, source)\n    assert start == 0\n    assert end == 4\n"], "sample_709": ["def test_hook_recorder_pluginmanager_finalization(pytester: Pytester) -> None:\n    class Plugin:\n            pass\n\n    plugin_manager = PytestPluginManager()\n    plugin_manager.add_plugin(Plugin())\n\n    hook_recorder = HookRecorder(plugin_manager)\n    pytester.make_hook_recorder(plugin_manager)\n\n    plugin_manager._ensure_teardown()\n    assert not hook_recorder._pluginmanager\n", "def test_runpytest_inprocess_timeout(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import time\n            time.sleep(10)\n    \"\"\"\n    )\n    result = pytester.runpytest_inprocess(\"--runpytest\", \"inprocess\", timeout=1)\n    assert result.ret == ExitCode.INTERRUPTED\n    assert \"KeyboardInterrupt\" in result.stdout.fnmatch_lines(\"*\")\n", "def test_runpytest_subprocess_timeout(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import time\n            time.sleep(10)\n    \"\"\"\n    )\n    result = pytester.runpytest_subprocess(timeout=1)\n    assert isinstance(result, pytester_mod.RunResult)\n    assert result.ret == ExitCode.INTERRUPTED\n", "def test_runpytest_subprocess_timeout(pytester: Pytester) -> None:\n    pytester.makepyfile(\"def test_sleep(): import time; time.sleep(10)\")\n    result = pytester.runpytest_subprocess(\"--runpytest\", \"subprocess\", timeout=1)\n    assert isinstance(result, pytester_mod.RunResult)\n    assert result.ret == ExitCode.INTERRUPTED\n", "def test_pytester_instance(pytester: Pytester) -> None:\n    assert isinstance(pytester.path, Path)\n    assert pytester.plugins == []\n    assert pytester._method in (\"inprocess\", \"subprocess\")\n    assert isinstance(pytester._monkeypatch, MonkeyPatch)\n    assert isinstance(pytester._cwd_snapshot, CwdSnapshot)\n    assert isinstance(pytester._sys_path_snapshot, SysPathsSnapshot)\n    assert isinstance(pytester._sys_modules_snapshot, SysModulesSnapshot)\n"], "sample_710": ["def test_unittest_skip(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        class TestSkipped(unittest.TestCase):\n            @unittest.skip(\"reason\")\n                pass\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    assert result.ret == ExitCode.OK\n    result.stdout.fnmatch_lines([\"*1 skipped*\"])\n", "def test_unittest_skip(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        @unittest.skip(\"skipped\")\n        class TestSkipped(unittest.TestCase):\n                assert 1 == 1\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    result.assert_outcomes(skipped=1)\n    assert result.ret == ExitCode.OK\n", "def test_trial_unittests(pytester: Pytester, monkeypatch: MonkeyPatch) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n        from twisted.trial import unittest as trial_unittest\n\n        class MyTestCase(trial_unittest.TestCase):\n                pass\n\n                assert False\n\n            @unittest.skip(\"skipped\")\n                pass\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1, failed=1, skipped=1)\n", "def test_unittest_skip_reason(\n    pytester: Pytester, monkeypatch: MonkeyPatch", "def test_unittest_skip_reason(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        class TestFoo(unittest.TestCase):\n            @unittest.skip(\"some reason\")\n                pass\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    assert result.ret == ExitCode.OK\n    assert \"some reason\" in result.stdout.str()\n"], "sample_711": ["def test_iterparentnodeids_corner_cases(nodeid, expected):\n    assert list(nodes.iterparentnodeids(nodeid)) == expected\n", "def test_node_init_pathlib_path() -> None:\n    node = nodes.Node.from_parent(parent=None, path=Path(\"/test/path\"), name=\"test\")\n    assert node.path == Path(\"/test/path\")\n    assert isinstance(node.fspath, legacy_path)\n    assert str(node.fspath) == \"/test/path\"\n", "def test_iterparentnodeids_trailing_sep() -> None:\n    nodeid = \"a/b/c::D/e::f/\"\n    expected = [\"\", \"a\", \"a/b\", \"a/b/c\", \"a/b/c::D/e\", \"a/b/c::D/e::f\"]\n    assert list(nodes.iterparentnodeids(nodeid)) == expected\n", "def test_iterparentnodeids_edge_cases():\n    assert list(nodes.iterparentnodeids(\"::\")) == [\"\", \"::\"]\n    assert list(nodes.iterparentnodeids(\"a::\")) == [\"\", \"a\", \"a::\"]\n    assert list(nodes.iterparentnodeids(\"::a\")) == [\"\", \"::a\"]\n    assert list(nodes.iterparentnodeids(\"a/b::c::d\")) == [\n        \"\",\n        \"a\",\n        \"a/b\",\n        \"a/b::c\",\n        \"a/b::c::d\",\n    ]\n", "def test_get_fslocation_from_item(tmp_path: Path) -> None:\n    class DummyNode(nodes.Node):\n            super().__init__(\"dummy\", nodeid=\"dummy\", session=None)\n            self.path = path\n            self.obj = obj\n\n    # Test with location attribute.\n    node = DummyNode(tmp_path / \"test_file.py\")\n    node.location = (\"test_file.py\", 10, \"TestFunction\")\n    assert nodes.get_fslocation_from_item(node) == (\"test_file.py\", 10)\n\n    # Test with obj attribute.\n    node = DummyNode(tmp_path / \"test_file.py\", obj=lambda: None)\n    fslocation = nodes.get_fslocation_from_item(node)\n    assert fslocation[0] == str(tmp_path / \"test_file.py\")\n    assert fslocation[1] is not None\n\n    # Test with fspath attribute.\n    node = DummyNode(tmp_path / \"test_file.py\")\n    assert nodes.get_fslocation_from_item(node) == (str(tmp_path / \"test_file.py\"), -1)\n\n    # Test with no relevant attributes.\n    node = DummyNode(tmp_path / \"test_file.py\")\n    node.fspath = None\n    assert nodes.get_fslocation_from_item(node) == (\"unknown location\", -1)\n"], "sample_712": ["def test_one_hot_encoder_inverse_transform():\n    # Test inverse transform for OneHotEncoder\n\n    X = np.array([[1, 2], [2, 0]])\n\n    ohe = OneHotEncoder()\n    X_ohe = ohe.fit_transform(X)\n    X_inv = ohe.inverse_transform(X_ohe)\n\n    assert_array_equal(toarray(X_inv), X.astype(object))\n", "def test_one_hot_encoder_categories():\n    # Test that categories are correctly set\n    X = np.array([[1, 2], [3, 4]])\n    encoder = OneHotEncoder()\n    encoder.fit(X)\n    assert_array_equal(encoder.categories_, [[1, 3], [2, 4]])\n\n    # Test that categories can be manually set\n    categories = [[1, 3, 5], [2, 4]]\n    encoder = OneHotEncoder(categories=categories)\n    encoder.fit(X)\n    assert_array_equal(encoder.categories_, categories)\n\n    # Test that an error is raised if categories are not a list of lists\n    categories = [1, 2, 3]\n    encoder = OneHotEncoder(categories=categories)\n    assert_raises_regex(ValueError, \"The parameter 'categories' must be a list of lists.\",\n                        encoder.fit, X)\n\n    # Test that an error is raised if categories have different lengths\n    categories = [[1, 2], [3]]\n    encoder = OneHotEncoder(categories=categories)\n    assert_raises_regex(ValueError, \"All the arrays of categories must have the same length.\",\n                        encoder.fit, X)\n", "def test_one_hot_encoder_inverse_transform():\n    # create an instance of OneHotEncoder\n    encoder = OneHotEncoder()\n\n    # fit the encoder with some data\n    X = [['Male', 1], ['Female', 3], ['Female', 2]]\n    encoder.fit(X)\n\n    # transform the data\n    transformed_X = encoder.transform(X)\n\n    # inverse transform the data\n    inverse_X = encoder.inverse_transform(transformed_X)\n\n    # assert that the original and inverse transformed data are equal\n    assert_array_equal(X, inverse_X)\n", "def test_one_hot_encoder_inverse_transform():\n    # Test inverse transform with a mix of categorical and numerical features\n    X = np.array([[1, 'a'], [2, 'b'], [3, 'a']])\n    ohe = OneHotEncoder(sparse=False)\n    ohe.fit(X)\n\n    X_tr = ohe.transform([[1, 'a'], [4, 'c']])\n    X_inv = ohe.inverse_transform(X_tr)\n\n    # Check that unknown values are replaced by None\n    assert_array_equal(X_inv[0], [1, 'a'])\n    assert_array_equal(X_inv[1], [None, None])\n", "def test_one_hot_encoder_categories():\n    # Test that categories are correctly set\n    X = [['Male', 1], ['Female', 3], ['Female', 2]]\n    enc = OneHotEncoder()\n    enc.fit(X)\n    assert_array_equal(enc.categories_, [np.array(['Female', 'Male']), np.array([1, 2, 3])])\n"], "sample_713": ["def test_ridge_multilabel():\n    # Test that Ridge can handle multilabel targets\n    X, y = make_multilabel_classification(n_samples=10, n_features=5,\n                                         n_classes=3, n_labels=3)\n    clf = Ridge()\n    clf.fit(X, y)\n    assert_equal(clf.coef_.shape[0], 3)\n    assert_equal(clf.score(X, y).shape, ())\n", "def test_ridgecv_scorer():\n    # Test RidgeCV with a custom scorer\n    X, y = make_regression(n_samples=100, n_features=10, random_state=0)\n    scorer = make_scorer(mean_squared_error, greater_is_better=False)\n    ridge_cv = RidgeCV(scoring=scorer)\n    ridge_cv.fit(X, y)\n    assert_true(isinstance(ridge_cv.scorer_, get_scorer(scoring=scorer)))\n", "def test_ridge_regression_sample_weight():\n    # Test that Ridge regression can handle sample weights\n\n    rng = np.random.RandomState(0)\n    X = rng.rand(100, 10)\n    y = rng.rand(100)\n\n    sample_weight = rng.rand(100)\n\n    ridge = Ridge()\n    ridge.fit(X, y, sample_weight=sample_weight)\n\n    # Check that the weighted sum of the residuals is close to zero\n    residuals = y - ridge.predict(X)\n    weighted_residuals = residuals * sample_weight\n    assert_almost_equal(np.sum(weighted_residuals), 0)\n", "def test_ridgecv_normalize():\n    # Test that RidgeCV is the same with and without normalize\n    X, y = make_regression(n_samples=100, n_features=10, random_state=0)\n\n    ridgecv = RidgeCV(normalize=False)\n    ridgecv.fit(X, y)\n    score_no_normalize = ridgecv.score(X, y)\n\n    X = StandardScaler().fit_transform(X)\n    ridgecv = RidgeCV(normalize=True)\n    ridgecv.fit(X, y)\n    score_normalize = ridgecv.score(X, y)\n\n    assert_almost_equal(score_no_normalize, score_normalize)\n", "def test_ridge_regression_sample_weight():\n    # Test that the sample weight is taken into account for Ridge regression\n    rng = np.random.RandomState(0)\n    X = rng.randn(5, 3)\n    y = rng.randn(5)\n    sw = rng.randn(5)\n    sw /= sw.sum()  # normalize so that it sums to one\n\n    solver = 'cholesky'\n    tol = 1e-6\n\n    # Compare the result with and without sample weights\n    res_sw = ridge_regression(X, y, alpha=1.0, sample_weight=sw,\n                              solver=solver, tol=tol)\n    res_no_sw = ridge_regression(X, y, alpha=1.0, solver=solver, tol=tol)\n\n    assert_true(not np.allclose(res_sw, res_no_sw))\n\n    # Compare with the naive implementation\n    X_sw = X * np.sqrt(sw[:, np.newaxis])\n    y_sw = y * np.sqrt(sw)\n    res_sw_naive = ridge_regression(X_sw, y_sw, alpha=1.0,\n                                    solver=solver, tol=tol)\n\n    assert_array_almost_equal(res_sw, res_sw_naive)\n"], "sample_714": ["def test_brier_score_loss():\n    y_true, y_pred, _ = make_prediction(binary=True)\n    y_prob = np.array([0.9 if yt == 1 else 0.2 for yt in y_true])\n\n    # Check manually computed brier_score\n    brier_ref = np.mean((y_true - y_prob) ** 2)\n    assert_almost_equal(brier_score_loss(y_true, y_prob), brier_ref)\n\n    # Check that y_prob of the correct shape is passed through\n    y_prob = np.array([[p, 1-p] for p in y_prob])\n    assert_almost_equal(brier_score_loss(y_true, y_prob[:, 1]), brier_ref)\n\n    # Check for invalid input\n    assert_raise_message(ValueError, \"y_true contains only one label\",\n                         brier_score_loss, [1, 1, 1], [0.4, 0.5, 0.6])\n    assert_raise_message(ValueError, \"y_prob contains values greater than 1.\",\n                         brier_score_loss, [0, 1, 0], [0.5, 0.6, 1.2])\n    assert_raise_message(ValueError, \"y_prob contains values less than 0.\",\n                         brier_score_loss, [0, 1, 0], [0.5, -0.6, 0.3])\n", "def test_brier_score_loss():\n    y_true = np.array([0, 1, 1, 0])\n    y_prob = np.array([0.1, 0.9, 0.8, 0.3])\n    assert_almost_equal(brier_score_loss(y_true, y_prob), 0.037)\n    assert_almost_equal(brier_score_loss(y_true, 1-y_prob, pos_label=0), 0.037)\n    assert_almost_equal(brier_score_loss(1-y_true, y_prob, pos_label=0), 0.037)\n    assert_almost_equal(brier_score_loss(y_true, np.array(y_prob) > 0.5), 0.0)\n", "def test_precision_recall_fscore_support_errors():\n    y_true, y_pred, _ = make_prediction(binary=True)\n\n    # Test that labels not present in y_true or y_pred don't lead to errors\n    labels = [0, 1, 2]\n    assert_warns(UndefinedMetricWarning, precision_recall_fscore_support,\n                 y_true, y_pred, labels=labels, average=\"macro\")\n    assert_warns(UndefinedMetricWarning, precision_recall_fscore_support,\n                 y_true, y_pred, labels=labels, average=\"weighted\")\n\n    # Test that average options work\n    for average in [\"macro\", \"weighted\", \"binary\"]:\n        p, r, f, s = precision_recall_fscore_support(y_true, y_pred,\n                                                     average=average)\n        assert_array_almost_equal(p, r)\n        assert_array_almost_equal(f, 2 * (p * r) / (p + r))\n", "def test_precision_recall_fscore_support_errors():\n    y_true, y_pred, _ = make_prediction(binary=True)\n\n    # Bad beta\n    assert_raises(ValueError, precision_recall_fscore_support,\n                  y_true, y_pred, beta=0.0)\n\n    # Bad average option\n    assert_raises(ValueError, precision_recall_fscore_support,\n                  y_true, y_pred, average=\"invalid\")\n\n    # Bad pos_label\n    assert_raises(ValueError, precision_recall_fscore_support,\n                  y_true, y_pred, average='binary', pos_label=2)\n", "def test_precision_recall_fscore_support_errors():\n    y_true, y_pred, _ = make_prediction(binary=True)\n\n    # Test with invalid beta\n    assert_raises(ValueError, precision_recall_fscore_support,\n                  y_true, y_pred, beta=0.0)\n    assert_raises(ValueError, fbeta_score, y_true, y_pred, beta=0.0)\n\n    # Test with invalid average\n    assert_raises(ValueError, precision_recall_fscore_support,\n                  y_true, y_pred, average=\"invalid\")\n    assert_raises(ValueError, precision_score, y_true, y_pred, average=\"invalid\")\n    assert_raises(ValueError, recall_score, y_true, y_pred, average=\"invalid\")\n    assert_raises(ValueError, f1_score, y_true, y_pred, average=\"invalid\")\n    assert_raises(ValueError, fbeta_score, y_true, y_pred, beta=1.0,\n                  average=\"invalid\")\n\n    # Test with invalid pos_label\n    assert_raises(ValueError, precision_recall_fscore_support,\n                  y_true, y_pred, pos_label=\"invalid\", average='binary')\n    assert_raises(ValueError, precision_score, y_true, y_pred,\n                  pos_label=\"invalid\", average='binary')\n    assert_raises(ValueError, recall_score, y_true, y_pred,\n                  pos_label=\"invalid\", average='binary')\n    assert_raises(ValueError, f1_score, y_true, y_pred,\n                  pos_label=\"invalid\", average='binary')\n    assert_raises(ValueError, fbeta_score, y_true, y_pred, beta=1.0,\n                  pos_label=\"invalid\", average='binary')\n"], "sample_715": ["def test_cross_validate_multiple_metrics():\n    # Test that cross_validate works with multiple metrics\n    X, y = make_classification(n_samples=100, n_features=20, random_state=0)\n    estimator = LogisticRegression()\n    scoring = ['accuracy', 'precision', 'recall']\n    scores = cross_validate(estimator, X, y, cv=5, scoring=scoring)\n    assert len(scores) == 7  # 3 metrics + fit_time + score_time + 2 train scores\n    for metric in scoring:\n        assert metric in scores\n        assert 'test_' + metric in scores\n        assert 'train_' + metric in scores\n", "def test_cross_validate_scorers_list():\n    X, y = make_classification(n_samples=20, n_features=5, random_state=0)\n    estimator = MockClassifier()\n    scoring = ['accuracy', 'precision']\n    cv = KFold(n_splits=2, shuffle=True, random_state=0)\n    scores = cross_validate(estimator, X, y, scoring=scoring, cv=cv)\n    assert_array_equal(scores['test_accuracy'], [1., 1.])\n    assert_array_equal(scores['test_precision'], [1., 1.])\n    assert_array_equal(scores['fit_time'], scores['score_time'])\n", "def test_cross_val_score_with_sample_weight():\n    # Test that cross_val_score allows passing sample weights\n    X = np.array([[1], [2], [3], [4]])\n    y = np.array([1, 1, 2, 2])\n    weights = np.array([0.5, 0.5, 0.5, 0.5])\n\n    estimator = MockClassifier()\n    score = cross_val_score(estimator, X, y, cv=2, fit_params={'sample_weight': weights})\n\n    assert_equal(len(score), 2)\n    assert_greater(score[0], 0)\n    assert_greater(score[1], 0)\n", "def test_cross_val_score_with_multioutput_scoring():\n    # Test cross_val_score with multioutput regression estimator and\n    # multioutput scoring function\n    X, y = make_regression(n_samples=100, n_features=10, n_targets=3,\n                           random_state=42)\n    estimator = Ridge()\n    scorer = make_scorer(explained_variance_score, multioutput='raw_values')\n    scores = cross_val_score(estimator, X, y, cv=5, scoring=scorer)\n    assert_array_equal(scores.shape, (5, 3))\n    scorer = make_scorer(explained_variance_score, multioutput='uniform_average')\n    scores = cross_val_score(estimator, X, y, cv=5, scoring=scorer)\n    assert_array_equal(scores.shape, (5,))\n", "def test_cross_validate_score_with_fit_params():\n    # Check that cross_validate works with fit_params\n\n    class MockEstimator(BaseEstimator):\n            self.fit_called = False\n\n            assert 'dummy_param' in fit_params\n            assert fit_params['dummy_param'] == 'some_value'\n            self.fit_called = True\n            return self\n\n            return 1.0\n\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([0, 1])\n\n    estimator = MockEstimator()\n    scores = cross_validate(estimator, X, y, cv=2, fit_params={'dummy_param': 'some_value'})\n\n    assert estimator.fit_called\n    assert scores['test_score'].shape == (2,)\n"], "sample_716": ["def test_ridgecv_scorer():\n    # Test that RidgeCV works with various scorers\n    X, y = make_regression(n_samples=100, n_features=5, random_state=0)\n\n    for scorer in ['r2', 'neg_mean_squared_error']:\n        ridge = RidgeCV(scoring=scorer)\n        ridge.fit(X, y)\n        assert_greater(ridge.score(X, y), 0.5)\n", "def test_ridgecv_store_cv_values():\n    # Test if cv_values are stored correctly\n    X, y = make_regression(n_samples=10, n_features=10, random_state=0)\n    ridgecv = RidgeCV(store_cv_values=True).fit(X, y)\n    assert_array_equal(ridgecv.cv_values_.shape, (10, len(ridgecv.alphas)))\n", "def test_ridgecv_scorer():\n    # Test that RidgeCV can handle different scoring functions\n\n    X, y = make_regression(n_samples=100, n_features=10, random_state=0)\n    alphas = [0.1, 1.0, 10.0]\n\n    for scorer_name in ['mean_squared_error', 'r2']:\n        scorer = get_scorer(scorer_name)\n        ridgecv = RidgeCV(alphas=alphas, scoring=scorer)\n        ridgecv.fit(X, y)\n\n        assert_greater(ridgecv.score(X, y), -1e6)\n", "def test_ridge_regression_sample_weights():\n    # Test that Ridge regression with sample weights gives the same result as\n    # reweighting the samples and using no weights.\n    X, y = make_regression(n_samples=10, n_features=5, random_state=0)\n    sample_weight = np.random.rand(10)\n\n    X_weighted = X * np.sqrt(sample_weight[:, np.newaxis])\n    y_weighted = y * np.sqrt(sample_weight)\n\n    coef_weighted = ridge_regression(X_weighted, y_weighted, 1.0)\n    coef_sample_weight = ridge_regression(X, y, 1.0, sample_weight=sample_weight)\n\n    assert_array_almost_equal(coef_weighted, coef_sample_weight)\n", "def test_ridgecv_scorer():\n    # Test that RidgeCV can work with different scoring functions.\n    X, y = make_regression(n_samples=100, n_features=10, random_state=0)\n    scorer = make_scorer(mean_squared_error, greater_is_better=False)\n    model = RidgeCV(alphas=[1e-2, 1e-1, 1], scoring=scorer)\n    model.fit(X, y)\n    assert_true(model.coef_ is not None)\n\n    # Test that RidgeCV fails with an invalid scorer.\n    scorer = 'invalid'\n    model = RidgeCV(alphas=[1e-2, 1e-1, 1], scoring=scorer)\n    assert_raises_regex(ValueError, \"Unknown scorer.*\", model.fit, X, y)\n"], "sample_717": ["def test_fetch_lfw_people():\n    # Smoke test to check if all the different optional parameters of the\n    # function.\n    fetch_lfw_people(data_home=SCIKIT_LEARN_DATA, color=True,\n                     resize=1.0, min_faces_per_person=2, slice_=None)\n\n    # Test that the data structure is correct and the shape is as expected\n    lfw_people = fetch_lfw_people(data_home=SCIKIT_LEARN_DATA, funneled=False)\n    assert_equal(lfw_people.images.shape, (62, 62, 3))\n    assert_equal(lfw_people.data.shape, (62, 62 * 3))\n    assert_array_equal(lfw_people.target_names, np.unique(FAKE_NAMES))\n\n    # Test the return type\n    assert isinstance(lfw_people.data, np.ndarray)\n    assert isinstance(lfw_people.images, np.ndarray)\n    assert isinstance(lfw_people.target, np.ndarray)\n\n    # Test that an error is raised when there are less than the required number\n    # of faces per person\n    assert_raises(ValueError, fetch_lfw_people, data_home=SCIKIT_LEARN_EMPTY_DATA,\n                  min_faces_per_person=100)\n\n    # Test that the dataset is shuffled\n    assert not np.array_equal(fetch_lfw_people(data_home=SCIKIT_LEARN_DATA).target,\n                              fetch_lfw_people(data_home=SCIKIT_LEARN_DATA).target)\n\n    # Check that the Bunch returned has the right keys\n    assert set(lfw_people.keys()) == set(['data', 'images', 'target',\n                                         'target_names', 'DESCR'])\n", "def test_fetch_lfw_pairs_10_folds():\n    \"\"\"Check that the 10 folds subset is loaded correctly\"\"\"\n    lfw_pairs = fetch_lfw_pairs(subset='10_folds', data_home=SCIKIT_LEARN_DATA)\n    assert_equal(len(lfw_pairs.target), 600)\n    assert_array_equal(np.unique(lfw_pairs.target), np.array([0, 1]))\n    assert_equal(lfw_pairs.pairs.shape, (600, 2, 62, 47))\n    assert_equal(lfw_pairs.data.shape, (600, 5828))\n", "def test_fetch_lfw_people():\n    \"\"\"Test that the people dataset is loaded correctly\"\"\"\n    lfw_people = fetch_lfw_people(data_home=SCIKIT_LEARN_DATA, min_faces_per_person=1)\n    assert_equal(lfw_people.images.shape[0], 22)\n    assert_equal(lfw_people.target_names.shape[0], 7)\n\n    # test that the min_faces_per_person parameter works as expected\n    lfw_people = fetch_lfw_people(data_home=SCIKIT_LEARN_DATA, min_faces_per_person=3)\n    assert_equal(lfw_people.images.shape[0], 12)\n    assert_equal(lfw_people.target_names.shape[0], 4)\n", "def test_fetch_lfw_pairs_subset():\n    # Check that the different subsets are loaded correctly\n    data_home = SCIKIT_LEARN_DATA\n    for subset in ['train', 'test', '10_folds']:\n        lfw_people = fetch_lfw_pairs(subset=subset, data_home=data_home)\n        assert_equal(lfw_people.target.shape[0], lfw_people.pairs.shape[0])\n        assert_equal(lfw_people.target_names.shape[0], 2)\n\n    # Check that a wrong subset raises an error\n    assert_raises(ValueError, fetch_lfw_pairs, subset='unknown',\n                  data_home=data_home)\n", "def test_fetch_lfw_pairs_wrong_subset():\n    # Test that an invalid subset raises a ValueError\n    assert_raises(ValueError, fetch_lfw_pairs, subset='invalid')\n"], "sample_718": ["def test_check_estimator_unfitted():\n    msg = \"non-public estimator classes should not be checked\"\n\n    # check public estimator\n    class PublicEstimator:\n            self.fitted_ = True\n\n    check_estimators_unfitted(\"PublicEstimator\", PublicEstimator())\n\n    # check non-public estimator\n    class _NonPublicEstimator:\n            self.fitted_ = True\n\n    assert_raises_regex(ValueError, msg, check_estimators_unfitted,\n                        \"_NonPublicEstimator\", _NonPublicEstimator())\n", "    def test_set_checking_parameters(self):\n        estimator = SGDClassifier(max_iter=1000, tol=None)\n        set_checking_parameters(estimator)\n        self.assertEqual(estimator.max_iter, 5)\n\n        estimator = LinearRegression()\n        set_checking_parameters(estimator)\n        self.assertTrue(isinstance(estimator, LinearRegression))\n", "    def test_check_estimator_unfitted(self):\n        # check if estimator is unfitted\n        msg = \"The estimator should raise a NotFittedError when it is unfitted.\"\n        assert_raises_regex(ValueError, msg, check_estimators_unfitted, \n                            CorrectNotFittedErrorClassifier())\n", "    def test_check_estimator_unfitted(self):\n        # check that predict raises an exception in an unfitted estimator.\n        estimator = LinearRegression()\n        X = np.array([[1, 2], [3, 4]])\n        y = np.array([5, 6])\n        msg = \"fit\"\n        assert_raise_message((AttributeError, ValueError), msg,\n                             estimator.predict, X)\n        estimator.fit(X, y)\n        estimator.predict(X)\n\n        # Same with 'transform' instead of 'predict'\n        estimator = NMF(n_components=1)\n        X = np.array([[1, 2], [3, 4]])\n        msg = \"fit\"\n        assert_raise_message((AttributeError, ValueError), msg,\n                             estimator.transform, X)\n        estimator.fit(X)\n        estimator.transform(X)\n\n        # Same with 'predict_proba' instead of 'predict'\n        estimator = SGDClassifier(max_iter=1000)\n        X = np.array([[1, 2], [3, 4]])\n        y = np.array([0, 1])\n        msg = \"fit\"\n        assert_raise_message((AttributeError, ValueError), msg,\n                             estimator.predict_proba, X)\n        estimator.fit(X, y)\n        estimator.predict_proba(X)\n", "    def test_check_methods_subset_invariance(self):\n        X, y = make_blobs(n_samples=30, centers=[[0, 0, 0], [1, 1, 1]],\n                          random_state=0, n_features=2, cluster_std=0.1)\n        X -= X.min()\n        estimator = GaussianMixture(n_components=3, random_state=0)\n\n        # Ensure estimator methods subset invariance holds for predict and\n        # predict_proba\n        result_full, result_by_batch = _apply_func(estimator.predict, X)\n        assert_array_equal(result_full, result_by_batch)\n\n        result_full, result_by_batch = _apply_func(estimator.predict_proba, X)\n        assert_array_equal(result_full, result_by_batch)\n\n        # Ensure estimator methods subset invariance does not hold for\n        # score_samples\n        estimator.fit(X)\n        result_full, result_by_batch = _apply_func(estimator.score_samples, X)\n        assert_raises(AssertionError, assert_array_equal, result_full,\n                      result_by_batch)\n"], "sample_719": ["def test_tfidf_vectorizer_max_features():\n    # Test that the max_features parameter works correctly\n\n    vec = TfidfVectorizer(max_features=4)\n    X = vec.fit_transform(ALL_FOOD_DOCS)\n\n    assert_equal(X.shape[1], 4)\n    assert_equal(len(vec.vocabulary_), 4)\n    assert_equal(len(vec.idf_), 4)\n    assert_equal(vec.stop_words_, set())\n", "def test_tfidf_vectorizer_idf_log_ratio():\n    # Test that the idf is a log ratio\n    v = TfidfVectorizer()\n    X = v.fit_transform(ALL_FOOD_DOCS)\n    idf = v.idf_\n    df = _document_frequency(X)\n\n    n_samples = len(ALL_FOOD_DOCS)\n    expected_idf = np.log((n_samples + 1) / (df + 1)) + 1\n\n    assert_array_almost_equal(idf, expected_idf)\n", "def test_vectorizer_pipeline_pickling():\n    X_train, X_test, y_train, y_test = train_test_split(ALL_FOOD_DOCS,\n                                                        [1] * len(JUNK_FOOD_DOCS) + [0] * len(NOTJUNK_FOOD_DOCS),\n                                                        test_size=0.5, random_state=42)\n\n    pipeline = Pipeline([\n        ('vectorizer', TfidfVectorizer()),\n        ('clf', LinearSVC())\n    ])\n\n    pipeline.fit(X_train, y_train)\n    score_before = pipeline.score(X_test, y_test)\n\n    pickled_pipeline = pickle.dumps(pipeline)\n    unpickled_pipeline = pickle.loads(pickled_pipeline)\n\n    score_after = unpickled_pipeline.score(X_test, y_test)\n\n    assert_array_almost_equal(score_before, score_after)\n", "def test_tfidf_vectorizer_with_norm():\n    # Test that TfidfVectorizer's norm is applied correctly\n\n    # Check Euclidean norm (default)\n    v = TfidfVectorizer(norm='l2')\n    X = v.fit_transform(JUNK_FOOD_DOCS)\n    assert_array_almost_equal(X.power(2).sum(axis=1), np.ones(X.shape[0]))\n\n    # Check L1 norm\n    v = TfidfVectorizer(norm='l1')\n    X = v.fit_transform(JUNK_FOOD_DOCS)\n    assert_array_almost_equal(X.sum(axis=1), np.ones(X.shape[0]))\n\n    # Check max norm\n    v = TfidfVectorizer(norm='max')\n    X = v.fit_transform(JUNK_FOOD_DOCS)\n    assert_array_almost_equal(X.max(axis=1).toarray(), np.ones((X.shape[0], 1)))\n\n    # Check non-normalized output\n    v = TfidfVectorizer(norm=None)\n    X = v.fit_transform(JUNK_FOOD_DOCS)\n    assert_not_equal(X.sum(axis=1), np.ones(X.shape[0]))\n", "def test_tfidfvectorizer_get_feature_names():\n    v = TfidfVectorizer()\n    X = v.fit_transform(JUNK_FOOD_DOCS)\n    feature_names = v.get_feature_names()\n    assert_equal(len(feature_names), X.shape[1])\n    assert_equal(len(set(feature_names)), X.shape[1])\n    assert_true(isinstance(feature_names[0], str))\n"], "sample_721": ["def test_check_is_fitted():\n    class Estimator:\n            self.fitted_ = True\n\n    estimator = Estimator()\n\n    # Not fitted yet\n    assert_raises_regex(NotFittedError, \"not fitted yet\",\n                        check_is_fitted, estimator, [\"fitted_\"])\n\n    # Now it's fitted\n    estimator.fit()\n    check_is_fitted(estimator, [\"fitted_\"])\n\n    # Check with multiple attributes\n    class MultiEstimator:\n            self.fitted_ = True\n            self.coef_ = None\n\n    multi_estimator = MultiEstimator()\n\n    # Not fitted yet\n    assert_raises_regex(NotFittedError, \"not fitted yet\",\n                        check_is_fitted, multi_estimator, [\"fitted_\", \"coef_\"])\n\n    # Now it's fitted\n    multi_estimator.fit()\n    check_is_fitted(multi_estimator, [\"fitted_\", \"coef_\"])\n\n    # Check with `all_or_any` parameter\n    class HalfEstimator:\n            self.fitted_ = True\n\n    half_estimator = HalfEstimator()\n\n    # Not fitted yet\n    assert_raises_regex(NotFittedError, \"not fitted yet\",\n                        check_is_fitted, half_estimator, [\"fitted_\", \"coef_\"],\n                        all_or_any=all)\n\n    # Now it's fitted, but only one attribute is set\n    half_estimator.fit()\n    assert_raises_regex(NotFittedError, \"not fitted yet\",\n                        check_is_fitted, half_estimator, [\"fitted_\", \"coef_\"],\n                        all_or_any=all)\n\n    # Now it's fitted and we check for any attribute\n    check_is_fitted(half_estimator, [\"fitted_\", \"coef_\"], all_or_any=any)\n", "def test_check_is_fitted():\n    class Estimator:\n            self.coef_ = None\n\n            self.coef_ = 1\n\n    estimator = Estimator()\n\n    # Not fitted yet\n    with pytest.raises(NotFittedError):\n        check_is_fitted(estimator, \"coef_\")\n\n    # Now it is fitted\n    estimator.fit()\n    check_is_fitted(estimator, \"coef_\")\n\n    # Check that we can pass multiple attributes\n    check_is_fitted(estimator, [\"coef_\", \"n_features_in_\"], all_or_any=any)\n    with pytest.raises(NotFittedError):\n        check_is_fitted(estimator, [\"coef_\", \"n_features_in_\"])\n", "def test_check_is_fitted():\n    class Estimator:\n            self.coef_ = None\n\n    estimator = Estimator()\n    assert_raises(NotFittedError, check_is_fitted, estimator, \"coef_\")\n    estimator.fit()\n    check_is_fitted(estimator, \"coef_\")\n\n    # check multiple attributes\n    assert_raises(NotFittedError, check_is_fitted, estimator, [\"coef_\", \"n_features_in_\"])\n    estimator.n_features_in_ = None\n    check_is_fitted(estimator, [\"coef_\", \"n_features_in_\"])\n\n    # check any of the attributes\n    assert_raises(NotFittedError, check_is_fitted, estimator, [\"n_samples_in_\", \"n_features_in_\"], all_or_any=any)\n    estimator.n_samples_in_ = None\n    check_is_fitted(estimator, [\"n_samples_in_\", \"n_features_in_\"], all_or_any=any)\n\n    # check with custom message\n    msg = \"Estimator must be fitted before use\"\n    assert_raise_message(NotFittedError, msg, check_is_fitted, estimator, \"dcoef_\", msg=msg)\n", "def test_check_is_fitted():\n    class Estimator:\n            self.estimator_ = None\n\n            self.estimator_ = 'fitted'\n\n    estimator = Estimator()\n    assert_raises_regex(NotFittedError,\n                        \"This Estimator instance is not fitted yet\",\n                        check_is_fitted, estimator, 'estimator_')\n\n    estimator.fit()\n    check_is_fitted(estimator, 'estimator_')\n\n    # test with multiple attributes\n    class MultiEstimator:\n            self.estimator1_ = None\n            self.estimator2_ = None\n\n            self.estimator1_ = 'fitted'\n            self.estimator2_ = 'fitted'\n\n    multi_estimator = MultiEstimator()\n    assert_raises_regex(NotFittedError,\n                        \"This MultiEstimator instance is not fitted yet\",\n                        check_is_fitted, multi_estimator, ['estimator1_', 'estimator2_'])\n\n    multi_estimator.fit()\n    check_is_fitted(multi_estimator, ['estimator1_', 'estimator2_'])\n", "def test_check_array_force_all_finite():\n    X = np.array([[1, 2], [3, np.inf]])\n    with pytest.raises(ValueError):\n        check_array(X, force_all_finite=True)\n    with pytest.raises(ValueError):\n        check_array(X, force_all_finite='allow-nan')\n    with ignore_warnings(category=FutureWarning):\n        X_checked = check_array(X, force_all_finite=False)\n    assert_array_equal(X_checked, X)\n"], "sample_722": ["def test_kmeans_convergence_warning():\n    # Check that KMeans raises a warning when the number of iterations is too low\n    # to converge.\n    X, _ = make_blobs(n_samples=100, centers=3, n_features=2, random_state=0,\n                      cluster_std=0.8)\n    km = KMeans(n_clusters=3, max_iter=1, random_state=0)\n    assert_warns_message(ConvergenceWarning, \"Number of iterations\", km.fit, X)\n", "def test_kmeans_init():\n    # Check that the kmeans initialization works as expected\n    # We use a small number of clusters to ensure the initialization is not too random\n    n_clusters = 2\n    rng = np.random.RandomState(0)\n    X = rng.rand(10, 3)\n    init_centers = np.array([[0.5, 0.5, 0.5], [0.8, 0.8, 0.8]])\n    \n    kmeans = KMeans(n_clusters=n_clusters, init=init_centers, n_init=1,\n                    random_state=rng)\n    kmeans.fit(X)\n    \n    assert_array_almost_equal(kmeans.cluster_centers_, init_centers)\n", "def test_kmeans_relocation():\n    # Test whether the relocation of a cluster center to its nearest neighbor\n    # when no samples are assigned to it is correct.\n    # To do this, we create a dataset with 3 clusters and one sample that is\n    # very far away from the others. We set n_clusters to 4 so that one of the\n    # clusters has no samples assigned to it initially.\n    X = np.array([\n        [0.0, 0.0],\n        [0.1, 0.1],\n        [0.2, 0.2],\n        [10.0, 10.0],\n    ])\n    kmeans = KMeans(n_clusters=4, random_state=42)\n    kmeans.fit(X)\n\n    # The last cluster should be assigned to the nearest neighbor of the\n    # cluster center, which is the point [10.0, 10.0].\n    assert_array_almost_equal(kmeans.cluster_centers_[-1], [10.0, 10.0])\n", "def test_kmeans_labels_assignment_and_inertia():\n    # pure numpy implementation as easily auditable reference gold\n    # standard\n        n_samples = X.shape[0]\n        k = centers.shape[0]\n        labels = np.empty(n_samples, dtype=np.int32)\n        mindists = np.empty(n_samples)\n        for i in range(n_samples):\n            dist = np.linalg.norm(centers - X[i], axis=1)\n            labels[i] = np.argmin(dist)\n            mindists[i] = dist[labels[i]]\n        inertia = mindists.sum()\n        return labels, inertia\n\n    random_state = 42\n    rng = np.random.RandomState(random_state)\n    X = make_blobs(n_samples=200, n_features=2, random_state=random_state)[0]\n    X_csr = sp.csr_matrix(X)\n\n    centers = _init_centroids(X, 3, init=\"random\", random_state=rng)\n\n    labels_ref, inertia_ref = _labels_inertia(X, centers)\n\n    labels_new, inertia_new = _labels_inertia(X, centers)\n    assert_array_equal(labels_new, labels_ref)\n    assert_almost_equal(inertia_new, inertia_ref)\n\n    labels_new, inertia_new = _labels_inertia(X_csr, centers)\n    assert_array_equal(labels_new, labels_ref)\n    assert_almost_equal(inertia_new, inertia_ref)\n", "def test_kmeans_init():\n    # Check that the KMeans initialization is correct\n    X, _ = make_blobs(n_samples=100, centers=3, cluster_std=0.8,\n                      random_state=42)\n\n    kmeans = KMeans(n_clusters=3, init='k-means++', n_init=1,\n                    random_state=42)\n    kmeans.fit(X)\n    init_centers = kmeans.cluster_centers_\n\n    kmeans = KMeans(n_clusters=3, init='random', n_init=1,\n                    random_state=42)\n    kmeans.fit(X)\n    random_centers = kmeans.cluster_centers_\n\n    assert_true(not np.array_equal(init_centers, random_centers))\n\n    kmeans = KMeans(n_clusters=3, init=X[:3], n_init=1, random_state=42)\n    kmeans.fit(X)\n    assert_array_equal(kmeans.cluster_centers_, X[:3])\n"], "sample_723": ["def test_imputer_copy():\n    # Check that copy was done correctly\n\n    X = np.array([[1, 2], [np.nan, 3], [7, 6]])\n\n    for sparse in [True, False]:\n        X_test = sparse_random_matrix(5, 5, random_state=0) if sparse else X.copy()\n\n        imputer = SimpleImputer(missing_values=np.nan, strategy='mean', axis=0,\n                                copy=True)\n        imputed_data = imputer.fit_transform(X_test)\n\n        imputer = SimpleImputer(missing_values=np.nan, strategy='mean', axis=0,\n                                copy=False)\n        imputed_data_no_copy = imputer.fit_transform(X_test)\n\n        input_address = X_test.__array_interface__['data'][0]\n\n        imputed_data_address = (imputed_data.__array_interface__['data'][0]\n                                if not sparse else imputed_data.data.__array_interface__['data'][0])\n\n        imputed_data_no_copy_address = (imputed_data_no_copy.__array_interface__['data'][0]\n                                        if not sparse else imputed_data_no_copy.data.__array_interface__['data'][0])\n\n        assert_false(input_address == imputed_data_address,\n                     \"Copy was not done (got same array address)\")\n\n        assert_equal(input_address, imputed_data_no_copy_address,\n                     \"Copy was done (got different array address)\")\n", "def test_imputer_copy():\n    # Check that copy was done correctly\n    X = np.array([[1, 2], [np.nan, 3]])\n\n    for sparse in [True, False]:\n        for axis in [0, 1]:\n            imputer = SimpleImputer(strategy='mean', axis=axis, copy=True)\n            if sparse:\n                X = sparse.csc_matrix(X)\n\n            X_trans = imputer.fit_transform(X)\n            X_trans[0, 0] = -1\n\n            if sparse:\n                assert_false(np.allclose(X.toarray(), X_trans.toarray()))\n            else:\n                assert_false(np.allclose(X, X_trans))\n", "def test_imputer_copy():\n    # Test that copy was done correctly\n\n    X = np.array([[1, 2], [np.nan, 3], [7, 6]])\n\n    for strategy in ['mean', 'median', 'most_frequent']:\n        imputer = SimpleImputer(strategy=strategy)\n\n        X_trans = imputer.fit_transform(X.copy())\n        X_trans_copy = imputer.fit_transform(X)\n\n        assert_array_equal(X_trans, X_trans_copy)\n        assert_false(np.may_share_memory(X, X_trans_copy))\n", "def test_imputer_copy():\n    # Check that copy=True creates a copy of X on transform\n    X = np.array([[1, 2], [np.nan, 3]])\n    imputer = SimpleImputer(strategy='mean', copy=True)\n\n    X_imputed = imputer.fit_transform(X)\n    X_imputed[0, 0] = np.nan\n\n    assert_false(np.isnan(X).any())\n\n    # Check that copy=False modifies X on transform\n    X = np.array([[1, 2], [np.nan, 3]])\n    imputer = SimpleImputer(strategy='mean', copy=False)\n\n    X_imputed = imputer.fit_transform(X)\n    X_imputed[0, 0] = np.nan\n\n    assert_array_equal(X, X_imputed)\n", "def test_imputer_pipeline_grid_search():\n    \"\"\"Test imputer in a pipeline with GridSearchCV\"\"\"\n    X = np.array([[1, 2], [np.nan, 3], [7, 6]])\n    y = np.array([0, 1, 0])\n    pipeline = Pipeline([('imputer', SimpleImputer()), ('tree', tree.DecisionTreeClassifier())])\n    params = {'imputer__strategy': ['mean', 'median']}\n    grid_search = GridSearchCV(pipeline, params, cv=2)\n    grid_search.fit(X, y)\n    assert_equal(grid_search.best_params_['imputer__strategy'], 'mean')\n"], "sample_724": ["def test_imputer_copy():\n    # Test that copy was made.\n    X = np.array([[1, 2], [np.nan, 3]])\n\n    imputer = Imputer(strategy='mean', copy=True)\n    X_trans = imputer.fit_transform(X)\n\n    X[0, 0] = np.nan\n\n    assert_false(np.isnan(X_trans[0, 0]))\n\n    # Test that copy was not made.\n    X = np.array([[1, 2], [np.nan, 3]])\n\n    imputer = Imputer(strategy='mean', copy=False)\n    X_trans = imputer.fit_transform(X)\n\n    X[0, 0] = np.nan\n\n    assert_equal(X_trans[0, 0], np.nan)\n", "def test_imputer_copy():\n    # Test that copy was done correctly\n    X = np.array([[1, 2], [np.nan, 3]])\n\n    for sparse in [True, False]:\n        for axis in [0, 1]:\n            imputer = Imputer(strategy='mean', axis=axis, copy=True)\n\n            if sparse:\n                X_fit = sparse.csc_matrix(X)\n            else:\n                X_fit = X\n\n            imputer.fit(X_fit)\n\n            if sparse:\n                X_trans = sparse.csc_matrix(X)\n            else:\n                X_trans = X\n\n            X_original = X_trans.copy()\n\n            imputer.transform(X_trans)\n\n            assert_false(np.array_equal(X_original.data if sparse else X_original,\n                                        X_trans.data if sparse else X_trans))\n\n    # Test that fit and transform were done without copy\n    X = np.array([[1, 2], [np.nan, 3]])\n\n    for sparse in [True, False]:\n        for axis in [0, 1]:\n            imputer = Imputer(strategy='mean', axis=axis, copy=False)\n\n            if sparse:\n                X_fit = sparse.csc_matrix(X)\n            else:\n                X_fit = X\n\n            imputer.fit(X_fit)\n\n            if sparse:\n                X_trans = sparse.csc_matrix(X)\n            else:\n                X_trans = X\n\n            X_original = X_trans.copy()\n\n            imputer.transform(X_trans)\n\n            assert_array_equal(X_original.data if sparse else X_original,\n                               X_trans.data if sparse else X_trans)\n", "def test_imputer_sparse_copy():\n    # Test that imputation on sparse matrices with copy=True returns a new\n    # sparse matrix.\n\n    X = sparse.csc_matrix(np.array([[1, 2], [np.nan, 3]]))\n    imputer = Imputer(strategy='mean')\n\n    X_imputed = imputer.fit_transform(X)\n    assert_false(X is X_imputed)\n    assert_equal(X.shape, X_imputed.shape)\n    assert_equal(X.nnz + 1, X_imputed.nnz)\n\n    # compare the data and indices of the original and imputed matrices\n    assert_array_almost_equal(X.data, X_imputed.data[:-1])\n    assert_array_equal(X.indices, X_imputed.indices[:-1])\n    assert_equal(X_imputed.data[-1], imputer.statistics_[0])\n    assert_equal(X_imputed.indices[-1], 0)\n", "def test_imputer_sparse_copy():\n    # Test that imputation on sparse matrix does not modify the original data\n    X = sparse_random_matrix(5, 5, random_state=0)\n    X_missing = X.copy()\n    X_missing.data[0] = np.nan\n\n    imputer = Imputer(strategy='mean', copy=True)\n    X_imputed = imputer.fit_transform(X_missing)\n\n    assert_false(np.isnan(X_missing.data).any())\n    assert_array_equal(X.toarray(), X_missing.toarray())\n    assert_false(np.isnan(X_imputed.toarray()).any())\n", "def test_imputer_sparse_copy():\n    # Test that transform returns a copy when input is sparse and copy=True\n    X = sparse.csc_matrix(np.array([[1, 0], [0, 2]]))\n    imputer = Imputer(missing_values=0, strategy=\"mean\", axis=0)\n    imputer.fit(X)\n\n    X_trans = imputer.transform(X, copy=True)\n    X_trans.data[0] = 10\n\n    assert_false(np.allclose(X.toarray(), X_trans.toarray()))\n"], "sample_725": ["def test_check_is_fitted():\n    class Estimator:\n            self.coef_ = None\n\n    estimator = Estimator()\n    with pytest.raises(NotFittedError):\n        check_is_fitted(estimator, [\"coef_\"])\n\n    estimator.fit()\n    check_is_fitted(estimator, [\"coef_\"])\n\n    # test with multiple attributes\n    class MultiEstimator:\n            self.coef_ = None\n            self.intercept_ = None\n\n    multi_estimator = MultiEstimator()\n    with pytest.raises(NotFittedError):\n        check_is_fitted(multi_estimator, [\"coef_\", \"intercept_\"])\n\n    multi_estimator.fit()\n    check_is_fitted(multi_estimator, [\"coef_\", \"intercept_\"])\n\n    # test with custom message\n    with pytest.raises(NotFittedError, match=\"Custom error message\"):\n        check_is_fitted(estimator, [\"coef_\"], msg=\"Custom error message\")\n", "def test_check_is_fitted():\n    class Estimator:\n            self.fitted_ = False\n\n            self.fitted_ = True\n\n    estimator = Estimator()\n    with pytest.raises(NotFittedError):\n        check_is_fitted(estimator, ['fitted_'])\n\n    estimator.fit(None)\n    assert_true(check_is_fitted(estimator, ['fitted_']) is None)\n\n    class Estimator2:\n            pass\n\n    estimator2 = Estimator2()\n    with pytest.raises(TypeError):\n        check_is_fitted(estimator2, ['fitted_'])\n", "def test_check_is_fitted():\n    class Estimator:\n            self.estimator_ = None\n\n            self.estimator_ = 'fitted'\n\n    estimator = Estimator()\n    assert_raises(NotFittedError, check_is_fitted, estimator, ['estimator_'])\n\n    estimator.fit(None, None)\n    check_is_fitted(estimator, ['estimator_'])\n\n    assert_raises_regex(TypeError, \"is not an estimator instance\", \n                        check_is_fitted, object(), ['estimator_'])\n\n    # check for multiple attributes\n    class MultiEstimator:\n            self.estimator1_ = None\n            self.estimator2_ = None\n\n            self.estimator1_ = 'fitted'\n            self.estimator2_ = 'fitted'\n\n    multi_estimator = MultiEstimator()\n    assert_raises(NotFittedError, check_is_fitted, multi_estimator, \n                  ['estimator1_', 'estimator2_'])\n\n    multi_estimator.fit(None, None)\n    check_is_fitted(multi_estimator, ['estimator1_', 'estimator2_'])\n\n    # check with all_or_any parameter set to any\n    assert_raises(NotFittedError, check_is_fitted, multi_estimator, \n                  ['estimator1_', 'estimator3_'])\n    check_is_fitted(multi_estimator, ['estimator1_', 'estimator3_'], \n                    all_or_any=any)\n", "def test_check_is_fitted():\n    class Estimator:\n            self.fitted_ = False\n\n            self.fitted_ = True\n\n    estimator = Estimator()\n    assert_raises(NotFittedError, check_is_fitted, estimator, 'fitted_')\n\n    estimator.fit()\n    check_is_fitted(estimator, 'fitted_')\n\n    assert_raises_regex(TypeError, \"is not an estimator instance\",\n                        check_is_fitted, object(), 'fitted_')\n", "def test_check_is_fitted():\n    # Test with estimator that has not been fitted yet\n    estimator = ARDRegression()\n    assert_raises(NotFittedError, check_is_fitted, estimator, \"coef_\")\n\n    # Test with estimator that has been fitted\n    X, y = make_blobs(n_samples=10, n_features=5)\n    estimator.fit(X, y)\n    assert_no_warnings(check_is_fitted, estimator, \"coef_\")\n\n    # Test with custom message\n    estimator = ARDRegression()\n    msg = \"Estimator must be fitted before use\"\n    assert_raise_message(NotFittedError, msg,\n                         check_is_fitted, estimator, \"coef_\", msg=msg)\n\n    # Test with multiple attributes\n    estimator = RandomForestRegressor()\n    assert_raises(NotFittedError, check_is_fitted, estimator, [\"estimators_\", \"n_features_in_\"])\n\n    # Test with all_or_any parameter\n    estimator = SVR()\n    assert_raises(NotFittedError, check_is_fitted, estimator, [\"dual_coef_\", \"_sparse_fit\"])\n    assert_no_warnings(check_is_fitted, estimator, [\"dual_coef_\", \"_sparse_fit\"], all_or_any=any)\n"], "sample_726": ["def test_label_binarizer_set_y_to_be_list_of_list():\n    # Test the set of y to be list of list instead of array\n    lb = LabelBinarizer()\n    y = [[1], [2], [6], [4], [2]]\n    lb.fit(y)\n    assert_array_equal(lb.classes_, [1, 2, 4, 6])\n\n    y_test = [[1], [6]]\n    y_trans = lb.transform(y_test)\n    assert_array_equal(toarray(y_trans), [[1, 0, 0, 0],\n                                           [0, 0, 0, 1]])\n", "def test_label_binarizer_fit_transform_multiclass():\n    # Test LabelBinarizer's fit_transform on multiclass problem\n    X = np.array([1, 2, 3, 4, 5])\n    lb = LabelBinarizer()\n    Xt = lb.fit_transform(X)\n    assert_array_equal(lb.classes_, np.array([1, 2, 3, 4, 5]))\n    assert_equal(Xt.shape[0], len(X))\n    assert_equal(Xt.shape[1], len(np.unique(X)))\n", "def test_label_binarize_with_unseen_labels():\n    # Test label_binarize function with unseen labels\n    y = [1, 2, 3]\n    classes = [1, 2, 3, 4]\n    Y_1 = label_binarize(y, classes)\n    assert_array_equal(toarray(Y_1), [[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0]])\n\n    lb = LabelBinarizer()\n    lb.fit(y)\n    Y_2 = lb.transform([1, 2, 3, 4])\n    assert_raises(ValueError, lb.transform, [1, 2, 3, 5])\n\n    lb = LabelBinarizer()\n    Y_3 = lb.fit_transform([1, 2, 3, 4])\n    assert_array_equal(toarray(Y_3), [[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]])\n", "def test_label_binarizer_fit_transform_multiclass():\n    # Test LabelBinarizer's fit_transform on multiclass data\n    lb = LabelBinarizer()\n    X = np.array([1, 2, 3, 4, 5])\n    Xt = lb.fit_transform(X)\n\n    assert_equal(lb.classes_.shape[0], 5)\n    assert_equal(Xt.shape[0], 5)\n    assert_equal(Xt.shape[1], 5)\n\n    # Check inverse transform\n    Xinv = lb.inverse_transform(Xt)\n    assert_array_equal(X, Xinv)\n", "def test_label_binarize_sparse_input():\n    # Test label_binarize with sparse input\n    y_sparse = csr_matrix(np.array([[0, 1, 0], [1, 0, 0]]))\n    classes = np.array([0, 1, 2])\n    Y = label_binarize(y_sparse, classes)\n    assert_true(issparse(Y))\n    assert_array_equal(toarray(Y), [[1, 0, 0], [0, 1, 0]])\n"], "sample_727": ["def test_imputer_copy():\n    \"\"\"Test that imputer returns a copy of the original array when copy=True.\"\"\"\n    X = np.array([[1, 2], [3, 4]])\n    imputer = Imputer(strategy=\"mean\", copy=True)\n\n    X_trans = imputer.fit_transform(X)\n    X_trans[0, 0] = -1\n\n    assert_false(np.allclose(X, X_trans))\n", "def test_imputer_axis1_warning():\n    X = np.array([[1, 2], [np.nan, 3]])\n\n    imputer = Imputer(strategy='mean', axis=1)\n    assert_warns_message(DeprecationWarning,\n                         \"Parameter 'axis' has been deprecated in 0.20 and \"\n                         \"will be removed in 0.22. Future (and default) \"\n                         \"behavior is equivalent to 'axis=0' (impute along \"\n                         \"columns). Row-wise imputation can be performed \"\n                         \"with FunctionTransformer.\",\n                         imputer.fit, X)\n", "def test_imputer_axis0_empty_matrix():\n    \"\"\"Test imputation on empty matrices with axis=0\"\"\"\n    X = np.array([[]])\n    X_sparse = sparse.csc_matrix(X)\n\n    imputer = Imputer(strategy='mean', axis=0)\n    assert_raises(ValueError, imputer.fit, X)\n    assert_raises(ValueError, imputer.fit, X_sparse)\n\n    imputer = Imputer(strategy='median', axis=0)\n    assert_raises(ValueError, imputer.fit, X)\n    assert_raises(ValueError, imputer.fit, X_sparse)\n\n    imputer = Imputer(strategy='most_frequent', axis=0)\n    assert_raises(ValueError, imputer.fit, X)\n    assert_raises(ValueError, imputer.fit, X_sparse)\n", "def test_imputer_sparse_copy():\n    # Test that transform returns a copy when X is sparse\n    X = sparse.csc_matrix(np.array([[1, 0], [0, 2]]))\n    imputer = Imputer(strategy='mean', missing_values=0)\n\n    X_fit_trans = imputer.fit_transform(X)\n    X_trans = imputer.transform(X)\n\n    assert_false(X_fit_trans is X_trans)\n    assert_array_equal(X_fit_trans.toarray(), X_trans.toarray())\n", "def test_imputer_copy():\n    # Test that copy was done correctly\n    X = np.array([[1, 2], [np.nan, 3], [7, 6]])\n\n    for strategy in ['mean', 'median', 'most_frequent']:\n        imputer = Imputer(strategy=strategy, axis=0, copy=True)\n        X_imputed = imputer.fit_transform(X)\n\n        # modify original data\n        X[0, 0] = np.nan\n\n        # check that imputed data was not modified\n        assert_false(np.isnan(X_imputed[0, 0]))\n\n    for strategy in ['mean', 'median', 'most_frequent']:\n        imputer = Imputer(strategy=strategy, axis=0, copy=False)\n        X_imputed = imputer.fit_transform(X)\n\n        # modify original data\n        X[0, 0] = np.nan\n\n        # check that imputed data was modified\n        assert np.isnan(X_imputed[0, 0])\n"], "sample_728": ["def test_make_sparse_spd_matrix():\n    # Check that the matrix is symmetric and positive definite\n    X = make_sparse_spd_matrix(10, alpha=0.5, norm_diag=True,\n                               smallest_coef=.1, largest_coef=.9)\n    assert_array_almost_equal(X, X.T)\n    eigenvalues = np.linalg.eigvals(X)\n    assert_true(np.all(eigenvalues > 0))\n\n    # Check that the resulting matrix has the correct shape\n    X = make_sparse_spd_matrix(10, alpha=0.5, norm_diag=False,\n                               smallest_coef=.1, largest_coef=.9)\n    assert_equal(X.shape, (10, 10))\n", "def test_make_classification():\n    X, y = make_classification(n_samples=100, n_features=20, n_informative=5,\n                               n_redundant=3, n_repeated=2, n_classes=3,\n                               n_clusters_per_class=1, random_state=0)\n\n    assert_array_equal(np.unique(y), np.array([0, 1, 2]))\n\n    # Test for n_features < n_informative + n_redundant + n_repeated\n    with pytest.raises(ValueError):\n        make_classification(n_samples=100, n_features=10, n_informative=5,\n                            n_redundant=3, n_repeated=3, n_classes=2,\n                            random_state=0)\n", "def test_make_sparse_coded_signal():\n    # Test that the generated signal is indeed sparse encoded\n    X, D, code = make_sparse_coded_signal(n_samples=10, n_components=10,\n                                          n_features=20, n_nonzero_coefs=3)\n    assert_equal(code.shape, (10, 10))\n    assert_less(np.mean(np.sum(code != 0, axis=0)), 4)\n    assert_array_almost_equal(X, np.dot(code, D.T))\n\n    # Test with custom random state\n    X_2, _, _ = make_sparse_coded_signal(n_samples=10, n_components=10,\n                                         n_features=20, n_nonzero_coefs=3,\n                                         random_state=42)\n    X_3, _, _ = make_sparse_coded_signal(n_samples=10, n_components=10,\n                                         n_features=20, n_nonzero_coefs=3,\n                                         random_state=42)\n    assert_array_equal(X_2, X_3)\n\n    # Test for correct error messages\n    assert_raises(ValueError, make_sparse_coded_signal, n_components=0)\n    assert_raises(ValueError, make_sparse_coded_signal, n_features=0)\n    assert_raises(ValueError, make_sparse_coded_signal, n_nonzero_coefs=0)\n", "def test_make_sparse_coded_signal():\n    X, D, code = make_sparse_coded_signal(\n        n_samples=10, n_components=5, n_features=20, n_nonzero_coefs=3,\n        random_state=0)\n    assert_equal(X.shape, (10, 20))\n    assert_equal(D.shape, (20, 5))\n    assert_equal(code.shape, (5, 10))\n    assert_array_almost_equal(X, np.dot(code.T, D.T))\n    for i in range(10):\n        assert_less(np.sum(code[:, i] != 0), 4)\n", "def test_make_gaussian_quantiles():\n    X, y = make_gaussian_quantiles(n_samples=100, n_features=2,\n                                   n_classes=3, random_state=0)\n\n    assert_equal(X.shape, (100, 2))\n    assert_array_equal(np.unique(y), np.arange(3))\n\n    # Test that the quantiles are correct\n    for i in range(3):\n        assert_true(np.all((X[y == i] ** 2).sum(axis=1) <\n                           (X[y == (i + 1) % 3] ** 2).sum(axis=1)))\n"], "sample_729": ["def test_enet_path_precompute_gram():\n    # Test that precomputing Gram gives same result with enet_path\n    X, y = load_boston(return_X_y=True)\n    X = check_array(X, dtype='float64', order='F')\n    y = check_array(y, dtype='float64')\n    max_iter = 1000\n\n    alphas = np.logspace(-4, -2, 10)\n\n    # compute gram and precompute Gram\n    Gram = np.dot(X.T, X)\n    path_params = dict(eps=1e-3, n_alphas=None, alphas=alphas,\n                       precompute=Gram, Xy=np.dot(X.T, y),\n                       fit_intercept=False, normalize=False,\n                       copy_X=True, verbose=False, tol=1e-7,\n                       positive=False, max_iter=max_iter,\n                       selection='cyclic')\n\n    coef_1 = enet_path(X, y, l1_ratio=0.5, **path_params)[1]\n\n    # without precomputing Gram\n    del path_params['precompute']\n    coef_2 = enet_path(X, y, l1_ratio=0.5, **path_params)[1]\n\n    assert_array_almost_equal(coef_1, coef_2, decimal=2)\n", "def test_lasso_cv_positive_constraint():\n    # Test that the positive constraint works correctly in LassoCV\n    X, y = load_boston(return_X_y=True)\n    X = check_array(X, order='F')\n    model = LassoCV(positive=True, fit_intercept=False, cv=2)\n    model.fit(X, y)\n    assert_true(np.all(model.coef_ >= 0))\n", "def test_lasso_path_positive():\n    # Test that lasso_path can handle positive parameter correctly\n\n    X, y = load_boston(return_X_y=True)\n\n    alphas, coefs, _ = lasso_path(X, y, positive=True)\n\n    # All coefficients should be non-negative\n    assert_true(np.all(coefs >= 0))\n\n    # Check that the solution is not trivial (all coefficients are zero)\n    assert_true(np.any(coefs != 0))\n", "def test_multi_task_lasso_and_enet():\n    # Test MultiTaskLasso and ElasticNet for different values of alpha\n\n    X, y = load_boston(return_X_y=True)\n    X = np.array([X, X]).transpose((1, 0, 2))\n    y = np.hstack((y, y))\n\n    # test for different values of alpha\n    for alpha in [0.1, 0.5, 1.0]:\n        clf_multi_task_lasso = MultiTaskLasso(alpha=alpha, fit_intercept=False,\n                                              max_iter=1000)\n        assert_warns(ConvergenceWarning, clf_multi_task_lasso.fit, X, y)\n\n        clf_elastic_net = MultiTaskElasticNet(alpha=alpha, fit_intercept=False,\n                                              max_iter=1000)\n        assert_warns(ConvergenceWarning, clf_elastic_net.fit, X, y)\n\n        # Check if the objective function is minimized for both fits\n        # (this is not a mathematical proof but it seems that the\n        # minimization works)\n        for clf in [clf_multi_task_lasso, clf_elastic_net]:\n            new_coef = clf.coef_.copy() + np.random.normal(0, 0.1,\n                                                           clf.coef_.shape)\n            new_residual = y - clf._decision_function(X, new_coef)\n            old_residual = y - clf._decision_function(X, clf.coef_)\n            assert_greater(np.mean(new_residual ** 2), np.mean(old_residual ** 2))\n", "def test_lasso_path_positive():\n    X, y = load_boston(return_X_y=True)\n    alphas, coefs, _ = lasso_path(X, y, positive=True)\n    assert_array_almost_equal(coefs[coefs < 0], np.zeros_like(coefs[coefs < 0]))\n    alpha_max = (np.sqrt(np.sum(X.T ** 2, axis=1)).max() /\n                 (X.shape[0] * 1.0))\n    assert_greater(alphas[0], alpha_max)\n\n    # Check that the coefficients are feasible (i.e. do not exceed max_alpha)\n    assert_array_almost_equal(coefs[alphas > alpha_max], np.zeros_like(\n        coefs[alphas > alpha_max]))\n\n    # check error on wrong precompute\n    with pytest.raises(ValueError):\n        lasso_path(X, y, precompute='idontexist')\n\n    # check full output of lasso path\n    alphas, coefs, dual_gaps = lasso_path(X, y, return_n_iter=False,\n                                          positive=True)\n    assert len(dual_gaps) == len(alphas)\n    for gap in dual_gaps:\n        assert gap >= 0\n\n    # Check bug LARS vs coordinate descent\n    rng = np.random.RandomState(42)\n    X = rng.randn(10, 3)\n    y = rng.randn(10)\n    alpha = 1.\n    _, coefs_cd, dual_gaps_cd, _ = lasso_path(\n        X, y, alpha_min=alpha, positive=True, return_n_iter=True)\n    coefs_lars = lars_path(X, y, method='lasso', positive=True,\n                           alphas=[alpha])[2]\n    assert_array_almost_equal(coefs_cd[:, -1], coefs_lars[:, -1])\n\n    # check that the returned coefs is for the correct alpha\n    _, coefs_cd, _, _ = lasso_path(X, y, alphas=[alpha], positive=True)\n    assert_array_almost_equal(coefs_cd[:, -1], coefs_lars[:, -1])\n"], "sample_730": ["def test_coordinate_descent_convergence():\n    # Test convergence of all coordinate descent solvers\n\n    X, y = load_boston(return_X_y=True)\n\n    # Lasso\n    clf = Lasso(max_iter=1000)\n    ignore_warnings(clf.fit)(X, y)\n    assert_array_almost_equal(clf.coef_, clf.path(X, y, max_iter=1000)[1][:, -1])\n\n    # ElasticNet\n    clf = ElasticNet(max_iter=1000)\n    ignore_warnings(clf.fit)(X, y)\n    assert_array_almost_equal(clf.coef_, clf.path(X, y, max_iter=1000)[1][:, -1])\n\n    # MultiTaskLasso\n    clf = MultiTaskLasso(max_iter=1000)\n    ignore_warnings(clf.fit)(X, np.array([y, y]).T)\n    assert_array_almost_equal(clf.coef_, clf.path(X, np.array([y, y]).T, max_iter=1000)[1][:, :, -1])\n\n    # MultiTaskElasticNet\n    clf = MultiTaskElasticNet(max_iter=1000)\n    ignore_warnings(clf.fit)(X, np.array([y, y]).T)\n    assert_array_almost_equal(clf.coef_, clf.path(X, np.array([y, y]).T, max_iter=1000)[1][:, :, -1])\n", "def test_lasso_path_return_n_iter():\n    # Test that lasso_path and enet_path return the correct n_iter\n    X, y = load_boston(return_X_y=True)\n    max_iter = 100\n    _, _, _, n_iter = lasso_path(X, y, max_iter=max_iter, return_n_iter=True)\n    assert_equal(len(n_iter), max_iter)\n\n    _, _, _, n_iter = enet_path(X, y, max_iter=max_iter, return_n_iter=True)\n    assert_equal(len(n_iter), max_iter)\n", "def test_multitask_lasso_and_elasticnet_convergence():\n    # Test that multitask Lasso and ElasticNet models converge\n    X, y = load_boston(return_X_y=True)\n    y = np.column_stack((y, y))\n\n    model = MultiTaskLasso(max_iter=10000, tol=1e-10)\n    ignore_warnings(assert_warns_message)(ConvergenceWarning,\n                                          model.fit(X, y))\n    assert_greater(model.n_iter_, 0)\n\n    model = MultiTaskElasticNet(max_iter=10000, tol=1e-10)\n    ignore_warnings(assert_warns_message)(ConvergenceWarning,\n                                          model.fit(X, y))\n    assert_greater(model.n_iter_, 0)\n", "def test_multitasklasso_and_elasticnet_with_sparse_X_and_copy_X_False():\n    # Check that _preprocess_data correctly handles X and y when X is sparse\n    # and copy_X=False.\n    n_samples, n_features = 10, 5\n    n_tasks = 2\n    X_sparse = sparse.csr_matrix(np.random.randn(n_samples, n_features))\n    y = np.random.randn(n_samples, n_tasks)\n\n    multi_task_lasso = MultiTaskLasso(copy_X=False)\n    multi_task_elasticnet = MultiTaskElasticNet(copy_X=False)\n\n    multi_task_lasso.fit(X_sparse, y)\n    multi_task_elasticnet.fit(X_sparse, y)\n\n    assert_array_equal(multi_task_lasso.coef_.shape, (n_tasks, n_features))\n    assert_array_equal(multi_task_elasticnet.coef_.shape, (n_tasks, n_features))\n", "def test_multitasklasso_and_enet_on_sparse_data():\n    n_targets = 3\n    n_samples, n_features = 50, 100\n    np.random.seed(0)\n    X = sparse.csr_matrix(np.random.randn(n_samples, n_features))\n    y = np.random.randn(n_samples, n_targets)\n\n    model = MultiTaskLasso()\n    model.fit(X, y)\n\n    model_enet = MultiTaskElasticNet()\n    model_enet.fit(X, y)\n\n    assert_array_equal(model.coef_.shape, (n_targets, n_features))\n    assert_array_equal(model_enet.coef_.shape, (n_targets, n_features))\n\n    # check the shape of predict()\n    y_pred = model.predict(X)\n    y_pred_enet = model_enet.predict(X)\n    assert_array_equal(y_pred.shape, (n_samples, n_targets))\n    assert_array_equal(y_pred_enet.shape, (n_samples, n_targets))\n"], "sample_731": ["def test_california_housing_fetch():\n    data = fetch()\n    assert data.data.shape == (20640, 8)\n    assert data.target.shape == (20640,)\n    assert len(data.feature_names) == 8\n    assert isinstance(data.DESCR, str)\n", "def test_california_housing_fetch():\n    data = fetch()\n    assert data.data.shape == (20640, 8)\n    assert data.target.shape == (20640,)\n    assert len(data.feature_names) == 8\n    assert isinstance(data.DESCR, str)\n", "def test_california_housing_fetch():\n    data = fetch()\n    assert isinstance(data, dict)\n    assert 'data' in data\n    assert 'target' in data\n    assert 'feature_names' in data\n    assert 'DESCR' in data\n    assert len(data['data']) == 20640\n    assert len(data['target']) == 20640\n    assert len(data['feature_names']) == 8\n", "def test_california_housing_fetch():\n    data = fetch()\n    assert isinstance(data, dict)\n    assert 'data' in data\n    assert 'target' in data\n    assert 'feature_names' in data\n    assert 'DESCR' in data\n\n    # Check shapes and types\n    assert data['data'].shape == (20640, 8)\n    assert data['target'].shape == (20640,)\n    assert isinstance(data['feature_names'], list)\n    assert len(data['feature_names']) == 8\n    assert isinstance(data['DESCR'], str)\n", "def test_california_housing_fetch_return_X_y():\n    # smoke test with return_X_y=True\n    fetch_func = partial(fetch, return_X_y=True)\n    check_return_X_y(fetch_func)\n"], "sample_732": ["def test_fetch_kddcup99_return_X_y():\n    fetch_func = partial(fetch_kddcup99, percent10=True)\n    check_return_X_y(fetch_func, subset='SA')\n", "def test_fetch_kddcup99_shuffle():\n    try:\n        dataset = fetch_kddcup99(shuffle=True, random_state=0)\n    except IOError:\n        raise SkipTest(\"KDDCup 99 dataset can not be loaded\")\n\n    assert_equal(dataset.data.shape[0], 494021)\n    assert_equal(dataset.target.shape[0], 494021)\n\n    # check shuffling\n    data_1 = dataset.data\n    target_1 = dataset.target\n\n    dataset = fetch_kddcup99(shuffle=True, random_state=0)\n    data_2 = dataset.data\n    target_2 = dataset.target\n\n    assert_equal(data_1, data_2)\n    assert_equal(target_1, target_2)\n", "def test_kddcup99_subset():\n    # Smoke test to check if the subsets are loaded correctly\n    data = fetch_kddcup99(subset='SA', percent10=True)\n    assert_equal(data.data.shape, (976158, 41))\n    assert_equal(data.target.shape, (976158,))\n\n    data = fetch_kddcup99(subset='SF', percent10=True)\n    assert_equal(data.data.shape, (699691, 4))\n    assert_equal(data.target.shape, (699691,))\n\n    data = fetch_kddcup99(subset='http', percent10=True)\n    assert_equal(data.data.shape, (619052, 3))\n    assert_equal(data.target.shape, (619052,))\n\n    data = fetch_kddcup99(subset='smtp', percent10=True)\n    assert_equal(data.data.shape, (95373, 3))\n    assert_equal(data.target.shape, (95373,))\n", "def test_fetch_kddcup99_subset():\n    # fetch_kddcup99 with subset='SA', 'SF', 'http', 'smtp'\n    data = fetch_kddcup99(subset='SA', percent10=True)\n    assert_equal(data.data.shape, (976158, 41))\n    assert_equal(data.target.shape, (976158,))\n\n    data = fetch_kddcup99(subset='SF', percent10=True)\n    assert_equal(data.data.shape, (699691, 4))\n    assert_equal(data.target.shape, (699691,))\n\n    data = fetch_kddcup99(subset='http', percent10=True)\n    assert_equal(data.data.shape, (619052, 3))\n    assert_equal(data.target.shape, (619052,))\n\n    data = fetch_kddcup99(subset='smtp', percent10=True)\n    assert_equal(data.data.shape, (95373, 3))\n    assert_equal(data.target.shape, (95373,))\n", "def test_fetch_kddcup99_shuffle(random_state=0):\n    \"\"\"Test that kddcup99 dataset can be shuffled\"\"\"\n    try:\n        data = fetch_kddcup99(shuffle=True, random_state=random_state)\n    except IOError:\n        raise SkipTest(\"KDDCup99 dataset can not be loaded.\")\n    data2 = fetch_kddcup99(shuffle=True, random_state=random_state)\n    assert_equal(data.data.tolist(), data2.data.tolist())\n    data3 = fetch_kddcup99(shuffle=True, random_state=random_state + 1)\n    assert not (data.data.tolist() == data3.data.tolist())\n"], "sample_733": ["def test_tfidf_vectorizer_idf_smoothing():\n    # Test that idf smoothing is on by default\n    v = TfidfVectorizer()\n    X = v.fit_transform(ALL_FOOD_DOCS)\n    assert_equal(v._tfidf.idf_[0], 1 + np.log((len(ALL_FOOD_DOCS) + 1) / (2 + 1)))\n\n    # Test that no idf smoothing doesn't apply the +1\n    v = TfidfVectorizer(smooth_idf=False)\n    X = v.fit_transform(ALL_FOOD_DOCS)\n    assert_equal(v._tfidf.idf_[0], 1 + np.log(len(ALL_FOOD_DOCS) / 2))\n\n    # Test that idf smoothing is applied when the document frequency is 0\n    v = TfidfVectorizer(smooth_idf=True)\n    X = v.fit_transform([\"the\", \"the\", \"the\"])\n    assert_greater(v._tfidf.idf_[0], 1)\n\n    # Test that no idf smoothing will result in a NaN when doc freq is 0\n    v = TfidfVectorizer(smooth_idf=False)\n    X = v.fit_transform([\"the\", \"the\", \"the\"])\n    assert_true(np.isnan(v._tfidf.idf_[0]))\n", "def test_tfidf_vectorizer_max_features():\n    # Test that the max_features parameter works as expected\n    vectorizer = TfidfVectorizer(max_features=4)\n    X = vectorizer.fit_transform(ALL_FOOD_DOCS)\n    assert_equal(X.shape[1], 4)\n    assert_equal(len(vectorizer.vocabulary_), 4)\n    # Ensure that the most common words are selected\n    stop_words = set(['the', 'copyright'])\n    assert_equal(stop_words, set(vectorizer.get_feature_names()))\n", "def test_tfidf_vectorizer_max_features():\n    # Test that max_features parameter is handled correctly.\n    vectorizer = TfidfVectorizer(max_features=4)\n    X = vectorizer.fit_transform(ALL_FOOD_DOCS)\n\n    assert_equal(X.shape[1], 4)\n    assert_equal(len(vectorizer.vocabulary_), 4)\n    assert_equal(len(vectorizer.stop_words_), 10)\n\n    vectorizer = TfidfVectorizer(max_features=4, stop_words='english')\n    X = vectorizer.fit_transform(ALL_FOOD_DOCS)\n\n    assert_equal(X.shape[1], 4)\n    assert_equal(len(vectorizer.vocabulary_), 4)\n    assert_true('the' not in vectorizer.vocabulary_)\n", "def test_vectorizer_pipeline_grid_selection():\n    # raw documents as an iterable\n    pipeline = Pipeline([\n        ('vectorizer', CountVectorizer()),\n        ('classifier', LinearSVC())\n    ])\n\n    parameters = {\n        'vectorizer__max_df': (0.5, 1.0),\n        'vectorizer__ngram_range': [(1, 1), (1, 2)],\n        'classifier__C': (0.1, 1),\n    }\n\n    X_train, X_test, y_train, y_test = train_test_split(ALL_FOOD_DOCS,\n                                                        [1] * len(JUNK_FOOD_DOCS) + [0] * len(NOTJUNK_FOOD_DOCS),\n                                                        test_size=0.5, random_state=0)\n\n    grid_search = GridSearchCV(pipeline, parameters, cv=3)\n    grid_search.fit(X_train, y_train)\n\n    assert_true(grid_search.best_score_ > 0.8)\n    assert_equal(grid_search.best_estimator_.named_steps['vectorizer'].max_df, 0.5)\n", "def test_tfidfvectorizer_get_feature_names():\n    v = TfidfVectorizer()\n    v.fit(JUNK_FOOD_DOCS)\n    feature_names = v.get_feature_names()\n    assert_equal(len(feature_names), len(v.vocabulary_))\n    assert_equal(len(feature_names), v.idf_.shape[0])\n    assert_in(\"pizza\", feature_names)\n    assert_in(\"burger\", feature_names)\n    assert_in(\"beer\", feature_names)\n\n    # check for duplicates in feature names\n    assert_equal(len(feature_names), len(set(feature_names)))\n\n    v2 = TfidfVectorizer(norm=None)\n    v2.fit(JUNK_FOOD_DOCS)\n    assert_array_equal(v2.get_feature_names(), feature_names)\n"], "sample_734": ["def test_fowlkes_mallows_score():\n    labels_a = np.array([0, 0, 0, 1, 1, 1])\n    labels_b = np.array([0, 0, 1, 1, 1, 1])\n    score = fowlkes_mallows_score(labels_a, labels_b)\n    assert_almost_equal(score, 0.816496580927726)\n\n    labels_a = np.array([0, 0, 0, 0, 0, 0])\n    labels_b = np.array([0, 0, 0, 0, 0, 0])\n    score = fowlkes_mallows_score(labels_a, labels_b)\n    assert_almost_equal(score, 1.0)\n\n    labels_a = np.array([0, 0, 0, 0, 0, 0])\n    labels_b = np.array([0, 1, 2, 3, 4, 5])\n    score = fowlkes_mallows_score(labels_a, labels_b)\n    assert_almost_equal(score, 0.0)\n", "def test_entropy():\n    # Test that the entropy function returns expected values for a given labeling\n    labels = np.array([1, 1, 1, 2, 2, 3])\n    expected_entropy = -((3 / 6) * np.log(3 / 6) + (2 / 6) * np.log(2 / 6) + (1 / 6) * np.log(1 / 6))\n    assert_almost_equal(entropy(labels), expected_entropy)\n\n    # Test that the entropy function returns 0 when all labels are the same\n    labels = np.array([1, 1, 1, 1, 1, 1])\n    assert_almost_equal(entropy(labels), 0.0)\n\n    # Test that the entropy function returns 1 when all labels are different\n    labels = np.array([1, 2, 3, 4, 5, 6])\n    assert_almost_equal(entropy(labels), np.log(6))\n\n    # Test that the entropy function handles empty label arrays\n    labels = np.array([])\n    assert_almost_equal(entropy(labels), 1.0)\n", "def test_fowlkes_mallows_score():\n    # Perfect match\n    assert_almost_equal(fowlkes_mallows_score([0, 0, 1, 1], [0, 0, 1, 1]), 1.0)\n\n    # No match\n    assert_almost_equal(fowlkes_mallows_score([0, 0, 0, 0], [0, 1, 2, 3]), 0.0)\n\n    # Some match\n    assert_almost_equal(\n        fowlkes_mallows_score([0, 0, 1, 1, 2, 2], [0, 0, 1, 2, 2, 2]),\n        0.47140452079103168,\n    )\n", "def test_entropy():\n    # Test the entropy function\n    labels = np.array([1, 1, 2, 2, 3, 3, 3])\n    assert_almost_equal(entropy(labels), 1.3709580831467377)\n    labels = np.array([1, 1, 1, 1])\n    assert_almost_equal(entropy(labels), 0.0)\n    labels = np.array([])\n    assert_almost_equal(entropy(labels), 1.0)\n", "def test_fowlkes_mallows_score():\n    # Test with perfect labeling\n    labels_true = [0, 0, 1, 1]\n    labels_pred = [0, 0, 1, 1]\n    assert_almost_equal(fowlkes_mallows_score(labels_true, labels_pred), 1.0)\n\n    # Test with completely random labeling\n    labels_true = [0, 0, 0, 0]\n    labels_pred = [0, 1, 2, 3]\n    assert_almost_equal(fowlkes_mallows_score(labels_true, labels_pred), 0.0)\n\n    # Test with non-integer labels\n    labels_true = [0.5, 0.5, 1.5, 1.5]\n    labels_pred = [0.5, 0.5, 1.5, 1.5]\n    assert_almost_equal(fowlkes_mallows_score(labels_true, labels_pred), 1.0)\n"], "sample_735": ["def test_bic_1():\n    # Initialize and fit a GaussianMixture with 2 components on a dataset\n    # that is known to be easily clusterable into two groups.\n    X, y = np.random.rand(100, 10), np.random.randint(0, 2, size=100)\n    gm = GaussianMixture(n_components=2, random_state=42).fit(X)\n\n    # Calculate the bic value using the fitted model's score method\n    bic = gm.bic(X)\n\n    # Assert that bic is not NaN or infinite\n    assert_true(np.isfinite(bic))\n", "def test_gaussian_mixture_estimate_log_prob():\n    # Test log probability estimation for GaussianMixture.\n    rng = np.random.RandomState(0)\n    n_samples, n_features = 50, 2\n    X = rng.rand(n_samples, n_features)\n\n    gmm_full = GaussianMixture(covariance_type='full', random_state=rng,\n                               n_components=2)\n    gmm_tied = GaussianMixture(covariance_type='tied', random_state=rng,\n                               n_components=2)\n    gmm_diag = GaussianMixture(covariance_type='diag', random_state=rng,\n                               n_components=2)\n    gmm_spher = GaussianMixture(covariance_type='spherical', random_state=rng,\n                                n_components=2)\n\n    gmm_full.fit(X)\n    gmm_tied.fit(X)\n    gmm_diag.fit(X)\n    gmm_spher.fit(X)\n\n    assert_true(np.allclose(gmm_full.score(X), gmm_full._estimate_log_prob(X)))\n    assert_true(np.allclose(gmm_tied.score(X), gmm_tied._estimate_log_prob(X)))\n    assert_true(np.allclose(gmm_diag.score(X), gmm_diag._estimate_log_prob(X)))\n    assert_true(np.allclose(gmm_spher.score(X), gmm_spher._estimate_log_prob(X)))\n", "def test_gaussian_mixture_covariance_type():\n    # Check covariance type is correctly set\n    rng = np.random.RandomState(0)\n    n_samples, n_features, n_components = 10, 2, 2\n    X = rng.rand(n_samples, n_features)\n\n    for covar_type in COVARIANCE_TYPE:\n        gmm = GaussianMixture(n_components=n_components,\n                              covariance_type=covar_type,\n                              random_state=rng).fit(X)\n        assert_equal(gmm.covariance_type, covar_type)\n        if covar_type == 'full':\n            assert_equal(gmm.covariances_.shape, (n_components, n_features,\n                                                  n_features))\n            assert_equal(gmm.precisions_.shape, (n_components, n_features,\n                                                 n_features))\n            assert_equal(gmm.precisions_cholesky_.shape, (n_components,\n                                                          n_features,\n                                                          n_features))\n        elif covar_type == 'tied':\n            assert_equal(gmm.covariances_.shape, (n_features, n_features))\n            assert_equal(gmm.precisions_.shape, (n_features, n_features))\n            assert_equal(gmm.precisions_cholesky_.shape, (n_features,\n                                                          n_features))\n        elif covar_type == 'diag':\n            assert_equal(gmm.covariances_.shape, (n_components, n_features))\n            assert_equal(gmm.precisions_.shape, (n_components, n_features))\n            assert_equal(gmm.precisions_cholesky_.shape, (n_components,\n                                                          n_features))\n        elif covar_type == 'spherical':\n            assert_equal(gmm.covariances_.shape, (n_components,))\n            assert_equal(gmm.precisions_.shape, (n_components,))\n            assert_equal(gmm.precisions_cholesky_.shape, (n_components,))\n", "def test_bic():\n    # Create sample data\n    rng = np.random.RandomState(0)\n    n_samples, n_features, n_components = 50, 2, 2\n    X = rng.rand(n_samples, n_features)\n\n    # Initialize GMM with n_components\n    gmm = GaussianMixture(n_components=n_components, random_state=rng)\n    gmm.fit(X)\n\n    # Check BIC scoring function\n    bic = gmm.bic(X)\n    assert isinstance(bic, float)\n\n    # Check BIC is decreasing with more components\n    gmm_more_components = GaussianMixture(n_components=n_components + 1,\n                                          random_state=rng)\n    gmm_more_components.fit(X)\n    bic_more_components = gmm_more_components.bic(X)\n    assert bic_more_components <= bic\n", "def test_gaussian_mixture_estimate_log_gaussian_prob():\n    rng = np.random.RandomState(0)\n    n_samples, n_features = 10, 2\n    X = rng.rand(n_samples, n_features)\n\n    # Initialize a GaussianMixture with two components\n    gm = GaussianMixture(n_components=2, random_state=rng)\n\n    # Set the parameters of the GaussianMixture manually\n    gm.means_ = np.array([[0.5, 0.5], [0.8, 0.8]])\n    gm.precisions_cholesky_ = np.array([[[1.0, 0.0], [0.0, 1.0]], [[1.0, 0.0], [0.0, 1.0]]])\n    gm.covariance_type = 'full'\n\n    # Compute log probabilities\n    log_prob = _estimate_log_gaussian_prob(X, gm.means_, gm.precisions_cholesky_, gm.covariance_type)\n\n    # Check shape\n    assert_equal(log_prob.shape, (n_samples, 2))\n\n    # Check that probabilities are non-negative\n    assert_greater_equal(np.exp(log_prob), 0).all()\n"], "sample_736": ["def test_logistic_regression_solvers():\n    \"\"\"Test that the different solvers give similar results\"\"\"\n    X, y = make_classification(n_samples=100, n_features=10,\n                               random_state=42, n_informative=5)\n    lr_liblinear = LogisticRegression(solver='liblinear', random_state=0)\n    lr_lbfgs = LogisticRegression(solver='lbfgs', random_state=0)\n    lr_newton_cg = LogisticRegression(solver='newton-cg', random_state=0)\n    lr_sag = LogisticRegression(solver='sag', random_state=0, max_iter=1000)\n    lr_saga = LogisticRegression(solver='saga', random_state=0, max_iter=1000)\n\n    for lr in [lr_liblinear, lr_lbfgs, lr_newton_cg, lr_sag, lr_saga]:\n        check_predictions(lr, X, y)\n\n    # Compare solutions\n    assert_array_almost_equal(lr_liblinear.coef_, lr_lbfgs.coef_)\n    assert_array_almost_equal(lr_liblinear.coef_, lr_newton_cg.coef_)\n    assert_array_almost_equal(lr_liblinear.coef_, lr_sag.coef_)\n    assert_array_almost_equal(lr_liblinear.coef_, lr_saga.coef_)\n", "def test_logistic_regression_solvers():\n    \"\"\"Check that the different solvers give the same result on iris.\"\"\"\n    X, y = iris.data, iris.target\n    n_samples, n_features = X.shape\n\n    for multi_class in ('ovr', 'multinomial'):\n        # Test for all relevant solvers\n        solvers = ['liblinear', 'lbfgs', 'newton-cg']\n        if multi_class == 'multinomial':\n            solvers = ['lbfgs', 'newton-cg']\n\n        results = {}\n        for solver in solvers:\n            clf = LogisticRegression(solver=solver, max_iter=1000,\n                                     multi_class=multi_class)\n            clf.fit(X, y)\n            results[solver] = clf.score(X, y)\n\n        # Compare scores across solvers\n        scores = list(results.values())\n        for score in scores:\n            assert_almost_equal(score, scores[0], decimal=2)\n\n    # Similar test for the sparse case\n    X_sp = sp.csr_matrix(X)\n    for multi_class in ('ovr', 'multinomial'):\n        solvers = ['liblinear', 'lbfgs', 'newton-cg']\n        if multi_class == 'multinomial':\n            solvers = ['lbfgs', 'newton-cg']\n\n        results = {}\n        for solver in solvers:\n            clf = LogisticRegression(solver=solver, max_iter=1000,\n                                     multi_class=multi_class)\n            clf.fit(X_sp, y)\n            results[solver] = clf.score(X_sp, y)\n\n        scores = list(results.values())\n        for score in scores:\n            assert_almost_equal(score, scores[0], decimal=2)\n", "def test_logistic_regression_multinomial_grad_hess():\n    # Test that the gradient and Hessian of the log loss are computed correctly\n    # for the multinomial case\n\n    X = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n    y = np.array([0, 1, 2])\n    Y_multi = LabelEncoder().fit_transform(y)\n\n    w = np.array([[0.5, 0.3, 0.2], [0.7, 0.4, 0.9], [0.1, 0.8, 0.6]])\n\n    grad, hessp = _multinomial_grad_hess(w.ravel(), X, Y_multi, 0.1,\n                                         sample_weight=np.array([1, 1, 1]))\n\n    # Compute the numerical gradient and Hessian using finite differences\n    eps = 1e-6\n    num_grad = np.zeros_like(grad)\n    num_hessp = np.zeros_like(hessp(np.ones_like(grad)))\n\n    for i in range(grad.shape[0]):\n        w_eps = w.ravel().copy()\n        w_eps[i] += eps\n        loss_plus_eps = _multinomial_loss(w_eps, X, Y_multi, 0.1,\n                                          sample_weight=np.array([1, 1, 1]))[0]\n        w_eps[i] -= 2 * eps\n        loss_minus_eps = _multinomial_loss(w_eps, X, Y_multi, 0.1,\n                                           sample_weight=np.array([1, 1, 1]))[0]\n        num_grad[i] = (loss_plus_eps - loss_minus_eps) / (2 * eps)\n        num_hessp[i] = (loss_plus_eps - 2 * _multinomial_loss(\n            w.ravel(), X, Y_multi, 0.1, sample_weight=np.array([1, 1, 1]))[0] +\n                        loss_minus_eps) / eps**2\n\n    assert_array_almost_equal(grad, num_grad, decimal=6)\n    assert_array_almost_equal(hessp(np.ones_like(grad)), num_hessp, decimal=6)\n", "def test_logistic_regression_solvers():\n    \"\"\"Test that the different solvers give similar results\"\"\"\n    X, y = make_classification(n_samples=1000, n_features=20,\n                               n_informative=10, n_redundant=0,\n                               random_state=42)\n\n    lr_newton_cg = LogisticRegression(solver='newton-cg', max_iter=1000)\n    lr_lbfgs = LogisticRegression(solver='lbfgs', max_iter=1000)\n    lr_liblinear = LogisticRegression(solver='liblinear')\n    lr_sag = LogisticRegression(solver='sag', max_iter=1000, tol=1e-6)\n    lr_saga = LogisticRegression(solver='saga', max_iter=1000, tol=1e-6)\n\n    for lr in [lr_newton_cg, lr_lbfgs, lr_liblinear, lr_sag, lr_saga]:\n        check_predictions(lr, X, y)\n\n    # Compare solutions (we need to fit again to have the same number of iter)\n    lr_newton_cg.fit(X, y)\n    lr_lbfgs.fit(X, y)\n    lr_liblinear.fit(X, y)\n    lr_sag.fit(X, y)\n    lr_saga.fit(X, y)\n\n    assert_array_almost_equal(lr_newton_cg.coef_, lr_lbfgs.coef_, decimal=2)\n    assert_array_almost_equal(lr_newton_cg.coef_, lr_liblinear.coef_,\n                              decimal=2)\n    assert_array_almost_equal(lr_newton_cg.coef_, lr_sag.coef_, decimal=2)\n    assert_array_almost_equal(lr_newton_cg.coef_, lr_saga.coef_, decimal=2)\n", "def test_logistic_regression_multinomial():\n    \"\"\"Test logistic regression on a multi-class problem.\"\"\"\n    X, y = make_classification(n_samples=100, n_features=20, n_informative=10,\n                               n_redundant=0, n_repeated=0, n_classes=3,\n                               random_state=0)\n\n    clf = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n    check_predictions(clf, X, y)\n\n    # Test with sample weights\n    sample_weight = np.random.RandomState(0).uniform(size=len(y))\n    clf.fit(X, y, sample_weight=sample_weight)\n    assert_array_equal(clf.classes_, np.unique(y))\n\n    # Test with non-uniform class weights\n    class_weight = {0: 1.0, 1: 2.0, 2: 3.0}\n    clf.set_params(class_weight=class_weight)\n    clf.fit(X, y)\n    assert_array_equal(clf.classes_, np.unique(y))\n"], "sample_737": ["def test_tfidfvectorizer_pickle_idf():\n    # Regression test for issue #11446\n    vect = TfidfVectorizer()\n    X = vect.fit_transform(JUNK_FOOD_DOCS)\n    idf_before = vect.idf_.copy()\n    pickle.dump(vect, open(\"vect.pkl\", \"wb\"))\n    vect_loaded = pickle.load(open(\"vect.pkl\", \"rb\"))\n    assert_array_almost_equal(idf_before, vect_loaded.idf_)\n    import os\n    os.remove(\"vect.pkl\")\n", "def test_tfidf_vectorizer_transform_with_non_fitted():\n    v = TfidfVectorizer()\n    assert_raises(ValueError, v.transform, [\"some text\"])\n", "def test_tfidf_vectorizer_transform_with_non_fitted():\n    tfidf = TfidfVectorizer()\n    assert_raises(ValueError, tfidf.transform, [\"some text\"])\n", "def test_tfidf_vectorizer_get_feature_names():\n    v = TfidfVectorizer()\n    X = v.fit_transform(ALL_FOOD_DOCS)\n    feature_names = v.get_feature_names()\n    assert_equal(len(feature_names), X.shape[1])\n    assert_equal(feature_names[0], 'beer')\n    assert_in('burger', feature_names)\n    assert_in('celeri', feature_names)\n    assert_in('coke', feature_names)\n    assert_in('pizza', feature_names)\n    assert_in('salad', feature_names)\n    assert_in('tomato', feature_names)\n    assert_in('water', feature_names)\n\n    # compare with CountVectorizer\n    cv = CountVectorizer()\n    cv.fit(ALL_FOOD_DOCS)\n    assert_array_equal(cv.get_feature_names(), v.get_feature_names())\n", "def test_vectorizer_max_features():\n    # Test that the max_features parameter is handled correctly\n\n    vectorizer = CountVectorizer(max_features=2)\n    X = vectorizer.fit_transform(JUNK_FOOD_DOCS)\n\n    # max_features should be <= n_features\n    assert_less_equal(vectorizer.max_features, X.shape[1])\n\n    # the number of features in the transformed data should be equal to max_features\n    assert_equal(X.shape[1], vectorizer.max_features)\n"], "sample_738": ["def test_tfidf_vectorizer_with_strip_accents():\n    # Test that using strip_accents with TfidfVectorizer works as expected\n    v = TfidfVectorizer(strip_accents='unicode')\n    X = v.fit_transform([\"caf\u00e9\", \"cafe\"])\n    assert_equal(v.vocabulary_[\"cafe\"], 0)\n    assert_equal(X.shape, (2, 1))\n\n    v = TfidfVectorizer(strip_accents='ascii')\n    X = v.fit_transform([\"caf\u00e9\", \"cafe\"])\n    assert_equal(v.vocabulary_[\"cafe\"], 0)\n    assert_equal(X.shape, (2, 1))\n", "def test_tfidf_vectorizer_decode_error():\n    # Test that the decode_error parameter is propagated to CountVectorizer\n\n    # Create a mock dataset with one document containing a non-ASCII character\n    docs = [\"the caf\u00e9\"]\n\n    # Create a TfidfVectorizer with decode_error='strict' (default)\n    vectorizer = TfidfVectorizer(decode_error='strict')\n\n    # Attempt to fit the vectorizer on the dataset\n    with pytest.raises(UnicodeDecodeError):\n        vectorizer.fit(docs)\n\n    # Create a TfidfVectorizer with decode_error='ignore'\n    vectorizer = TfidfVectorizer(decode_error='ignore')\n\n    # Fit the vectorizer on the dataset without raising an error\n    vectorizer.fit(docs)\n", "def test_tfidf_vectorizer_max_features():\n    # Test the max_features parameter of the TfidfVectorizer\n\n    vec = TfidfVectorizer(max_features=5)\n    X = vec.fit_transform(ALL_FOOD_DOCS)\n\n    assert_equal(X.shape[1], 5)\n    assert_equal(len(vec.vocabulary_), 5)\n\n    # The most frequent terms are selected\n    most_frequent_terms = set(['the', 'beer', 'burger', 'salad', 'copyright'])\n    assert_equal(set(vec.vocabulary_.keys()), most_frequent_terms)\n\n    # With a different max_features, the terms are different\n    vec = TfidfVectorizer(max_features=10)\n    X = vec.fit_transform(ALL_FOOD_DOCS)\n    assert_equal(X.shape[1], 10)\n    assert_equal(len(vec.vocabulary_), 10)\n\n    # The most frequent terms are selected\n    most_frequent_terms = set(['the', 'beer', 'burger', 'salad', 'copyright',\n                               'coke', 'pizza', 'tomato', 'celeri', 'water'])\n    assert_equal(set(vec.vocabulary_.keys()), most_frequent_terms)\n", "def test_tfidf_vectorizer_max_features():\n    # Test that the max_features parameter works as expected\n    vectorizer = TfidfVectorizer(max_features=10)\n    X = vectorizer.fit_transform(ALL_FOOD_DOCS)\n    assert_equal(X.shape[1], 10)\n\n    vectorizer = TfidfVectorizer(max_features=None)\n    X = vectorizer.fit_transform(ALL_FOOD_DOCS)\n    assert_greater(X.shape[1], 10)\n", "def test_tfidf_vectorizer_preprocessor():\n    # Test that the preprocessor parameter works as expected\n\n    # Define a custom preprocessor\n        return doc.upper()\n\n    # Initialize the vectorizer with the custom preprocessor\n    vectorizer = TfidfVectorizer(preprocessor=my_preprocessor)\n\n    # Fit and transform the data\n    X = vectorizer.fit_transform(JUNK_FOOD_DOCS)\n\n    # Get the feature names\n    feature_names = vectorizer.get_feature_names()\n\n    # Assert that the feature names are in uppercase\n    assert_true(all(feature_name.isupper() for feature_name in feature_names))\n"], "sample_739": ["def test_label_binarizer_multilabel():\n    # Test LabelBinarizer with multilabel data\n    y = [[0, 1], [1, 2]]\n    lb = LabelBinarizer()\n    Y = lb.fit_transform(y)\n    assert_array_equal(toarray(Y), [[1, 1, 0], [0, 1, 1]])\n    y_inv = lb.inverse_transform(Y)\n    assert_array_equal(y_inv, y)\n    y_inv = lb.inverse_transform(toarray(Y))\n    assert_array_equal(y_inv, y)\n\n    # Check that inverse_transform works with array-like input\n    y_inv = lb.inverse_transform(Y.toarray())\n    assert_array_equal(y_inv, y)\n\n    # Test that fit_transform works with different types of input\n    y = [(0, 1), (1, 2)]\n    lb = LabelBinarizer()\n    Y = lb.fit_transform(y)\n    assert_array_equal(toarray(Y), [[1, 1, 0], [0, 1, 1]])\n\n    y = np.array([[0, 1], [1, 2]])\n    lb = LabelBinarizer()\n    Y = lb.fit_transform(y)\n    assert_array_equal(toarray(Y), [[1, 1, 0], [0, 1, 1]])\n\n    y = csr_matrix([[0, 1], [1, 2]])\n    lb = LabelBinarizer()\n    Y = lb.fit_transform(y)\n    assert_array_equal(toarray(Y), [[1, 1, 0], [0, 1, 1]])\n", "def test_label_binarizer_fit_transform_y_type_binary():\n    # Test that fit_transform works with binary target data\n    lb = LabelBinarizer()\n    y = np.array([0, 1, 0, 1])\n    Y = lb.fit_transform(y)\n    assert_array_equal(Y, np.array([[1], [2], [1], [2]]))\n    assert_equal(lb.classes_.shape[0], 2)\n    assert_equal(type_of_target(Y), 'binary')\n", "def test_label_binarizer_fit_transform():\n    # Test fit_transform with different types of targets\n    lb = LabelBinarizer()\n\n    # Binary classification\n    y = [0, 1, 0, 1]\n    Y = lb.fit_transform(y)\n    assert_array_equal(toarray(Y), np.array([[1, 0], [0, 1], [1, 0], [0, 1]]))\n\n    # Multiclass classification\n    y = [0, 1, 2, 0, 1, 2]\n    Y = lb.fit_transform(y)\n    assert_array_equal(toarray(Y), np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1],\n                                             [1, 0, 0], [0, 1, 0], [0, 0, 1]]))\n\n    # Multilabel classification\n    y = [[0, 1], [1, 2], [0, 2]]\n    Y = lb.fit_transform(y)\n    assert_array_equal(toarray(Y), np.array([[1, 1, 0], [0, 1, 1], [1, 0, 1]]))\n", "def test_label_binarize_sparsetarget():\n    # Test label_binarize with sparse target\n    n_samples, n_classes = 5, 3\n    rng = np.random.RandomState(0)\n    y_sparse = csr_matrix((np.ones(n_samples), rng.randint(0, n_classes,\n                                                           n_samples)),\n                          shape=(n_samples, n_classes))\n    y_dense = toarray(y_sparse)\n\n    # check that both sparse and dense targets lead to the same output\n    for y in [y_sparse, y_dense]:\n        Y = label_binarize(y, classes=np.arange(n_classes))\n        assert_array_equal(Y, y_dense)\n\n    # check that output is sparse when requested\n    Y_sparse = label_binarize(y_sparse, classes=np.arange(n_classes),\n                              sparse_output=True)\n    assert_true(issparse(Y_sparse))\n\n    # check that label_binarize preserves input sparsity if possible\n    Y_sparse = label_binarize(y_sparse, classes=np.arange(n_classes))\n    assert_true(issparse(Y_sparse))\n", "def test_label_binarizer_unseen_labels():\n    # Test that LabelBinarizer raises an error when transforming unseen labels\n    lb = LabelBinarizer()\n    lb.fit([1, 2, 3])\n    assert_raises(ValueError, lb.transform, [4])\n    assert_raises(ValueError, lb.transform, [1, 2, 4])\n"], "sample_740": ["def test_check_is_fitted():\n    class Estimator:\n            self.fitted_ = True\n\n    estimator = Estimator()\n    assert_raises_regex(NotFittedError, \"Estimator\", check_is_fitted,\n                        estimator, 'fitted_')\n\n    estimator.fit()\n    assert_no_warnings(check_is_fitted, estimator, 'fitted_')\n\n    # check for attribute which does not exist\n    assert_raises_regex(NotFittedError, \"Estimator\", check_is_fitted,\n                        estimator, 'non_existing_attribute_')\n\n    # check with multiple attributes\n    assert_raises_regex(NotFittedError, \"Estimator\", check_is_fitted,\n                        estimator, ['fitted_', 'non_existing_attribute_'])\n\n    # check with custom message\n    assert_raises_regex(NotFittedError, \"Custom message Estimator\",\n                        check_is_fitted, estimator,\n                        'non_existing_attribute_', msg=\"Custom message %s\")\n", "def test_check_is_fitted():\n    class Estimator:\n            self.fitted_ = False\n\n            self.fitted_ = True\n            return self\n\n    estimator = Estimator()\n    assert_raises(NotFittedError, check_is_fitted, estimator, 'fitted_')\n    estimator.fit()\n    check_is_fitted(estimator, 'fitted_')\n\n    # Test with multiple attributes\n    estimator.coef_ = np.array([1, 2, 3])\n    estimator.n_features_in_ = 3\n    check_is_fitted(estimator, ['coef_', 'n_features_in_'])\n\n    # Test with custom message\n    msg = \"Estimator needs to be fitted before calling this method\"\n    assert_raise_message(NotFittedError, msg, check_is_fitted,\n                         estimator, ['non_existent_attr'], msg=msg)\n", "def test_check_is_fitted():\n    class Estimator:\n            self.fitted_ = True\n\n    estimator = Estimator()\n    assert_raises_regex(NotFittedError,\n                        \"This Estimator instance is not fitted yet\",\n                        check_is_fitted, estimator, 'fitted_')\n\n    estimator.fit(None)\n    assert_no_warnings(check_is_fitted, estimator, 'fitted_')\n\n    # checking multiple attributes\n    assert_raises_regex(NotFittedError,\n                        \"This Estimator instance is not fitted yet\",\n                        check_is_fitted, estimator, ['fitted_', 'other_'])\n\n    class MultiAttrEstimator:\n            self.fitted_ = True\n            self.other_ = True\n\n    multi_estimator = MultiAttrEstimator()\n    multi_estimator.fit(None)\n    assert_no_warnings(check_is_fitted, multi_estimator, ['fitted_', 'other_'])\n", "def test_check_is_fitted():\n    class Estimator:\n            self.fitted_ = False\n\n            self.fitted_ = True\n            return self\n\n    estimator = Estimator()\n    assert_raises(NotFittedError, check_is_fitted, estimator, \"fitted_\")\n\n    estimator.fit(None)\n    assert_no_warnings(check_is_fitted, estimator, \"fitted_\")\n\n    # Test with multiple attributes\n    class MultiAttrEstimator:\n            self.attr1_ = None\n            self.attr2_ = None\n\n            self.attr1_ = True\n            self.attr2_ = True\n            return self\n\n    multi_attr_estimator = MultiAttrEstimator()\n    assert_raises(NotFittedError, check_is_fitted, multi_attr_estimator,\n                  [\"attr1_\", \"attr2_\"])\n\n    multi_attr_estimator.fit(None)\n    assert_no_warnings(check_is_fitted, multi_attr_estimator, [\"attr1_\", \"attr2_\"])\n", "def test_check_is_fitted():\n    class Estimator:\n            self.fit_called_ = False\n\n            self.fit_called_ = True\n            self.coef_ = None\n\n    estimator = Estimator()\n    assert_false(check_is_fitted(estimator, \"coef_\"))\n    estimator.fit()\n    assert_true(check_is_fitted(estimator, \"fit_called_\"))\n\n    # check with multiple attributes\n    assert_false(check_is_fitted(estimator, [\"coef_\", \"fit_called_\"]))\n    assert_false(check_is_fitted(estimator, (\"coef_\", \"fit_called_\")))\n\n    # error when estimator does not have a fit method\n    del Estimator.fit\n    estimator = Estimator()\n    assert_raises(TypeError, check_is_fitted, estimator, \"coef_\")\n\n    # custom message\n    msg = \"Estimator must be fitted before use\"\n    assert_raise_message(NotFittedError, msg, check_is_fitted, estimator, \"coef_\", msg=msg)\n"], "sample_741": ["def test_grid_search_cv_error_score():\n    # Test error handling in GridSearchCV with a failing estimator\n    X, y = make_classification(n_samples=20, n_features=10, random_state=0)\n\n    class FailingEstimator(BaseEstimator):\n            raise ValueError(\"Failing estimator\")\n\n    param_grid = {'C': [1]}\n\n    gs = GridSearchCV(FailingEstimator(), param_grid, cv=2, error_score='raise')\n    assert_raises(ValueError, gs.fit, X, y)\n\n    gs = GridSearchCV(FailingEstimator(), param_grid, cv=2, error_score=np.nan)\n    gs.fit(X, y)\n    assert np.isnan(gs.cv_results_['mean_test_score']).all()\n\n    gs = GridSearchCV(FailingEstimator(), param_grid, cv=2, error_score=-1)\n    gs.fit(X, y)\n    assert (gs.cv_results_['mean_test_score'] == -1).all()\n", "def test_parameter_grid_iter_and_getitem():\n    param_grid = {'a': [1, 2], 'b': [3, 4]}\n    grid = ParameterGrid(param_grid)\n    assert_grid_iter_equals_getitem(grid)\n\n    param_grid = [{'a': [1, 2]}, {'b': [3, 4]}]\n    grid = ParameterGrid(param_grid)\n    assert_grid_iter_equals_getitem(grid)\n\n    param_grid = [{'a': [1, 2], 'b': [3, 4]}, {'c': [5, 6]}]\n    grid = ParameterGrid(param_grid)\n    assert_grid_iter_equals_getitem(grid)\n", "def test_grid_search_cv_validate_scoring_parameter():\n    # Check that the scoring parameter is validated in GridSearchCV\n    X, y = make_classification(n_samples=100, n_features=10, random_state=0)\n\n    # Custom scorer with a private method\n    class CustomScorer:\n            pass\n\n            return 1.0\n\n    gs = GridSearchCV(DecisionTreeClassifier(random_state=0), {}, cv=2)\n    assert_raise_message(ValueError, \"The score function should be callable\",\n                         gs.fit, X, y, scoring=CustomScorer())\n\n    # Custom scorer with a public method but requires parameters\n    class CustomScorer:\n            pass\n\n            return 1.0\n\n    gs = GridSearchCV(DecisionTreeClassifier(random_state=0), {}, cv=2)\n    assert_raise_message(ValueError,\n                         (\"The score function should be callable with \"\n                          \"parameters (estimator, X, y)\"),\n                         gs.fit, X, y, scoring=CustomScorer().public_method)\n\n    # Custom scorer with fit method but no score method\n    class CustomScorer:\n            pass\n\n            return self\n\n    gs = GridSearchCV(DecisionTreeClassifier(random_state=0), {}, cv=2)\n    assert_raise_message(ValueError, \"The score function should be callable\",\n                         gs.fit, X, y, scoring=CustomScorer())\n", "def test_parameter_grid_iter_and_getitem():\n    # Test ParameterGrid iteration and getitem against known values\n    param_grid = {'a': [1, 2], 'b': [True, False]}\n    grid = ParameterGrid(param_grid)\n    expected = [{'a': 1, 'b': True}, {'a': 1, 'b': False},\n                {'a': 2, 'b': True}, {'a': 2, 'b': False}]\n    assert_grid_iter_equals_getitem(grid)\n    assert_equal(list(grid), expected)\n\n    param_grid = [{'kernel': ['linear']}, {'kernel': ['rbf'], 'gamma': [1, 10]}]\n    grid = ParameterGrid(param_grid)\n    expected = [{'kernel': 'linear'},\n                {'kernel': 'rbf', 'gamma': 1}, {'kernel': 'rbf', 'gamma': 10}]\n    assert_grid_iter_equals_getitem(grid)\n    assert_equal(list(grid), expected)\n", "def test_parameter_grid():\n    # Test basic properties of ParameterGrid.\n    param_grid = {'a': [1, 2], 'b': [3, 4]}\n    grid = ParameterGrid(param_grid)\n    assert_equal(len(grid), 4)\n    assert_grid_iter_equals_getitem(grid)\n\n    param_grid = {'a': [1, 2], 'b': [3, 4], 'c': [5, 6]}\n    grid = ParameterGrid(param_grid)\n    assert_equal(len(grid), 8)\n    assert_grid_iter_equals_getitem(grid)\n\n    param_grid = [{'a': [1, 2]}, {'b': [3, 4]}]\n    grid = ParameterGrid(param_grid)\n    assert_equal(len(grid), 2)\n    assert_grid_iter_equals_getitem(grid)\n\n    param_grid = [{'a': [1, 2], 'b': [3, 4]}, {'b': [5, 6], 'c': [7, 8]}]\n    grid = ParameterGrid(param_grid)\n    assert_equal(len(grid), 4)\n    assert_grid_iter_equals_getitem(grid)\n"], "sample_742": ["def test_logistic_regression_multiclass():\n    \"\"\"Test logistic regression with multi-class classification\"\"\"\n    X, y = make_classification(n_samples=100, n_features=20, n_informative=10,\n                               n_redundant=0, n_repeated=0, n_classes=3,\n                               random_state=42)\n\n    clf = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n    check_predictions(clf, X, y)\n\n    clf = LogisticRegression(multi_class='ovr', solver='lbfgs')\n    check_predictions(clf, X, y)\n", "def test_logistic_regression_saga_sparse_binary():\n    # Test sparse matrix input for saga solver on binary classification problem\n    X, y = make_classification(n_samples=20, n_features=10, random_state=0)\n    X_sparse = sp.csr_matrix(X)\n\n    clf_dense = LogisticRegression(solver='saga', max_iter=1000,\n                                   random_state=42).fit(X, y)\n    clf_sparse = LogisticRegression(solver='saga', max_iter=1000,\n                                    random_state=42).fit(X_sparse, y)\n\n    assert_array_almost_equal(clf_dense.coef_, clf_sparse.coef_)\n    assert_array_almost_equal(clf_dense.intercept_, clf_sparse.intercept_)\n", "def test_logistic_regression_path_multinomial():\n    # Test logistic regression path with multinomial option\n\n    # Create a multi-class problem\n    X, y = make_classification(n_samples=100, n_features=20, n_informative=10,\n                               n_redundant=0, n_repeated=0, n_classes=3,\n                               random_state=42)\n\n    Cs = np.logspace(-4, 4, 10)\n\n    # Check that the path function return the coefs for all classes\n    coefs, Cs, n_iter = logistic_regression_path(X, y, Cs=Cs,\n                                                 multi_class='multinomial',\n                                                 max_iter=1000, tol=1e-6)\n\n    assert_equal(len(coefs), len(Cs))\n    for c in coefs:\n        assert_equal(c.shape, (3, X.shape[1]))\n\n    # Check that the predict_proba function returns a correct shape\n    clf = LogisticRegression(multi_class='multinomial')\n    clf.coef_ = coefs[-1]\n    clf.intercept_ = np.zeros(3)\n    y_pred = clf.predict_proba(X)\n    assert_equal(y_pred.shape, (X.shape[0], 3))\n", "def test_logistic_regression_solvers():\n    \"\"\"Check that the different solvers give similar results\"\"\"\n    X, y = make_classification(n_samples=100, n_features=20, n_informative=10,\n                               n_redundant=0, random_state=42)\n    lr_newton = LogisticRegression(solver='newton-cg', max_iter=1000)\n    lr_lbfgs = LogisticRegression(solver='lbfgs', max_iter=1000)\n    lr_liblinear = LogisticRegression(solver='liblinear', max_iter=1000)\n    lr_sag = LogisticRegression(solver='sag', max_iter=1000)\n    lr_saga = LogisticRegression(solver='saga', max_iter=1000)\n\n    for lr in [lr_newton, lr_lbfgs, lr_liblinear, lr_sag, lr_saga]:\n        check_predictions(lr, X, y)\n\n    # Compare solutions\n    lr_newton.fit(X, y)\n    lr_lbfgs.fit(X, y)\n    lr_liblinear.fit(X, y)\n    lr_sag.fit(X, y)\n    lr_saga.fit(X, y)\n    assert_allclose(lr_newton.coef_, lr_lbfgs.coef_, atol=1e-6)\n    assert_allclose(lr_newton.coef_, lr_liblinear.coef_, atol=1e-6)\n    assert_allclose(lr_newton.coef_, lr_sag.coef_, atol=1e-6)\n    assert_allclose(lr_newton.coef_, lr_saga.coef_, atol=1e-6)\n", "def test_logistic_regression_solvers():\n    \"\"\"Test that the different solvers give similar results\"\"\"\n    X, y = make_classification(n_samples=1000, n_features=20,\n                               n_informative=10, n_redundant=0,\n                               n_classes=2, random_state=0)\n    X_sparse = sp.csr_matrix(X)\n\n    solvers = ['liblinear', 'lbfgs', 'newton-cg', 'sag', 'saga']\n\n    results = {}\n    for solver in solvers:\n        clf = LogisticRegression(solver=solver, max_iter=1000, random_state=0)\n        if solver == 'liblinear':\n            # liblinear fails on sparse data with non-convergence\n            # so we only test it on dense data\n            clf.fit(X, y)\n        else:\n            clf.fit(X_sparse, y)\n        results[solver] = clf.score(X_sparse, y)\n\n    # Compare scores across all pairs of solvers\n    for solver1 in solvers:\n        for solver2 in solvers:\n            assert_greater(results[solver1], 0.8)\n            assert_almost_equal(results[solver1], results[solver2], places=3)\n"], "sample_743": ["def test_neighbors_base_algorithm_metric():\n    # Test that algorithm and metric are set correctly in NeighborsBase\n    X = iris.data\n\n    # Test ball_tree\n    nbrs = neighbors.NearestNeighbors(algorithm='ball_tree', metric='euclidean')\n    nbrs.fit(X)\n    assert_equal(nbrs._fit_method, 'ball_tree')\n    assert_equal(nbrs.effective_metric_, 'euclidean')\n\n    # Test kd_tree\n    nbrs = neighbors.NearestNeighbors(algorithm='kd_tree', metric='euclidean')\n    nbrs.fit(X)\n    assert_equal(nbrs._fit_method, 'kd_tree')\n    assert_equal(nbrs.effective_metric_, 'euclidean')\n\n    # Test brute\n    nbrs = neighbors.NearestNeighbors(algorithm='brute', metric='euclidean')\n    nbrs.fit(X)\n    assert_equal(nbrs._fit_method, 'brute')\n    assert_equal(nbrs.effective_metric_, 'euclidean')\n\n    # Test auto\n    nbrs = neighbors.NearestNeighbors(algorithm='auto', metric='euclidean')\n    nbrs.fit(X)\n    assert_in(nbrs._fit_method, ['ball_tree', 'kd_tree'])\n    assert_equal(nbrs.effective_metric_, 'euclidean')\n\n    # Test with callable metric\n    nbrs = neighbors.NearestNeighbors(algorithm='auto', metric=lambda x, y: x - y)\n    nbrs.fit(X)\n    assert_equal(nbrs._fit_method, 'ball_tree')\n    assert_equal(nbrs.effective_metric_, nbrs.metric)\n\n    # Test invalid algorithm\n    assert_raises_regex(ValueError, \"unrecognized algorithm\",\n                        neighbors.NearestNeighbors, algorithm='foo', metric='euclidean')\n", "def test_neighbors_base_algorithm():\n    # Test that algorithm auto-selection chooses ball_tree for small datasets\n    X = rng.rand(10, 5)\n    nbrs = neighbors.NearestNeighbors(n_neighbors=3)\n    nbrs.fit(X)\n\n    assert_equal(nbrs._fit_method, 'ball_tree')\n\n    # For larger datasets, it should choose 'brute' for metrics not in BallTree\n    X = rng.rand(1000, 5)\n    nbrs = neighbors.NearestNeighbors(n_neighbors=3, metric='cosine')\n    nbrs.fit(X)\n\n    assert_equal(nbrs._fit_method, 'brute')\n\n    # If n_neighbors is close to the number of samples, it should choose brute\n    X = rng.rand(200, 5)\n    nbrs = neighbors.NearestNeighbors(n_neighbors=100)\n    nbrs.fit(X)\n\n    assert_equal(nbrs._fit_method, 'brute')\n", "def test_neighbors_base_metric_params():\n    # Test to check if metric_params is passed to the underlying BallTree or KDTree\n\n        return np.sqrt(((x - y) * scale) ** 2).sum()\n\n    X = iris.data\n    n_neighbors = 5\n\n    # Check for BallTree\n    nn = neighbors.NearestNeighbors(n_neighbors=n_neighbors,\n                                    algorithm='ball_tree',\n                                    metric=custom_distance,\n                                    metric_params={'scale': 2.0})\n    nn.fit(X)\n\n    # Check for KDTree\n    nn = neighbors.NearestNeighbors(n_neighbors=n_neighbors,\n                                    algorithm='kd_tree',\n                                    metric='minkowski',\n                                    metric_params={'p': 3})\n    nn.fit(X)\n", "def test_kneighbors_radius_getitem():\n    # Test kneighbors and radius with getitem\n    X = rng.random_sample((10, 3))\n    d = neighbors.NearestNeighbors(radius=1)\n    d.fit(X)\n\n    dist, ind = d.radius_neighbors(X)\n\n    for i in range(len(dist)):\n        assert_array_almost_equal(dist[i], dist[i])\n        assert_array_equal(ind[i], ind[i])\n", "def test_neighbors_base_check_algorithm_metric():\n    # Test that invalid algorithms and metrics raise the correct errors\n    X = rng.rand(10, 5)\n\n    # Test invalid algorithm\n    knn = neighbors.NeighborsBase(algorithm='invalid')\n    assert_raises_regex(ValueError, \"unrecognized algorithm\", knn._fit, X)\n\n    # Test invalid metric\n    knn = neighbors.NeighborsBase(metric='invalid')\n    assert_raises_regex(ValueError, \"Metric 'invalid' not valid for algorithm\",\n                         knn._fit, X)\n\n    # Test callable metric is not accepted for kd_tree algorithm\n    knn = neighbors.NeighborsBase(algorithm='kd_tree', metric=lambda x, y: x - y)\n    assert_raises_regex(ValueError, \"kd_tree algorithm does not support callable\",\n                         knn._fit, X)\n"], "sample_745": ["def test_function_transformer_kw_args():\n    X = np.array([[1, 2], [3, 4]])\n    args_store = []\n    kwargs_store = {}\n\n        args_store.append(X)\n        args_store.extend(args)\n        kwargs_store.update(kwargs)\n        return X\n\n    func = FunctionTransformer(func=_func, kw_args={'a': 5, 'b': 10})\n    func.fit(X)\n\n    assert_equal(len(args_store), 1)\n    assert_array_equal(args_store[0], X)\n    assert_equal(kwargs_store, {'a': 5, 'b': 10})\n", "def test_function_transformer_check_inverse():\n    X = np.random.rand(10, 5)\n\n    # Define a simple function and its inverse\n        return X * 2\n\n        return X / 2\n\n    transformer = FunctionTransformer(func=func, inverse_func=inverse_func)\n    assert_no_warnings(transformer.fit, X)\n\n    # Define a function without an inverse\n        return X ** 2\n\n    transformer = FunctionTransformer(func=func, check_inverse=True)\n    assert_warns_message(UserWarning, \"The provided functions are not strictly\"\n                         \" inverse of each other.\",\n                         transformer.fit, X)\n\n    # Define a function with an inverse but check_inverse=False\n    transformer = FunctionTransformer(func=func, inverse_func=lambda X: X,\n                                      check_inverse=False)\n    assert_no_warnings(transformer.fit, X)\n", "def test_function_transformer_check_inverse():\n    # Test that FunctionTransformer checks if func and inverse_func are inverses.\n    X = np.random.rand(10, 10)\n\n        return 2 * X\n\n        return X / 2\n\n    # Inverse functions\n    ft = FunctionTransformer(func=double, inverse_func=halve)\n    assert_no_warnings(ft.fit, X)\n\n    # Non-inverse functions\n    ft = FunctionTransformer(func=double, inverse_func=lambda X: X)\n    assert_warns_message(UserWarning, \"The provided functions are not strictly\"\n                         \" inverse of each other.\",\n                         ft.fit, X)\n", "def test_function_transformer_inverse_transform():\n    X = np.array([[1, 2], [3, 4]])\n\n    # Define a simple function and its inverse\n        return X * 2\n\n        return X / 2\n\n    transformer = FunctionTransformer(func=func, inverse_func=inverse_func)\n\n    # Fit and transform\n    transformer.fit(X)\n    X_transformed = transformer.transform(X)\n\n    # Check that the inverse transform recovers the original data\n    assert_allclose_dense_sparse(X, transformer.inverse_transform(X_transformed))\n", "def test_function_transformer_check_inverse():\n    X = np.array([[1, 2], [3, 4]])\n\n    # Define a function and its inverse that don't quite round-trip\n        return X * 2\n\n        return X / 2.01\n\n    # Check that the FunctionTransformer warns when check_inverse=True\n    with pytest.warns(UserWarning):\n        FunctionTransformer(func=func, inverse_func=inverse_func,\n                            check_inverse=True).fit(X)\n\n    # Check that no warning is raised when check_inverse=False\n    with assert_no_warnings():\n        FunctionTransformer(func=func, inverse_func=inverse_func,\n                            check_inverse=False).fit(X)\n"], "sample_746": ["def test_classification_report_multiclass_with_labels():\n    # Test classification report with labels and multiclass classification\n    y_true = np.array([0, 1, 2, 2, 0, 1, 2, 0, 1])\n    y_pred = np.array([0, 2, 1, 1, 0, 2, 2, 0, 2])\n\n    # Test with labels as list\n    labels = [0, 1, 2]\n    report = classification_report(y_true, y_pred, labels=labels)\n    assert_equal(len(report.split('\\n')), 7)\n\n    # Test with labels as numpy array\n    labels = np.array([0, 1, 2])\n    report = classification_report(y_true, y_pred, labels=labels)\n    assert_equal(len(report.split('\\n')), 7)\n\n    # Test with labels having only a subset of classes\n    labels = np.array([0, 1])\n    report = classification_report(y_true, y_pred, labels=labels)\n    assert_equal(len(report.split('\\n')), 6)\n\n    # Test with labels not present in data\n    labels = np.array([3, 4])\n    report = classification_report(y_true, y_pred, labels=labels)\n    assert_equal(len(report.split('\\n')), 5)\n", "def test_f1_score_multiclass_unchanged():\n    # Test that F1 score is unchanged if the true and predicted labels are the same\n    y_true = np.array([0, 1, 2, 0, 1, 2])\n    y_pred = np.array([0, 1, 2, 0, 1, 2])\n\n    f1 = f1_score(y_true, y_pred, average='macro')\n    assert_almost_equal(f1, 1.0)\n\n    f1 = f1_score(y_true, y_pred, average='weighted')\n    assert_almost_equal(f1, 1.0)\n\n    f1 = f1_score(y_true, y_pred, average='micro')\n    assert_almost_equal(f1, 1.0)\n", "def test_precision_recall_fscore_support_errors():\n    y_true, y_pred, _ = make_prediction(binary=True)\n\n    # Test that labels not present in y_true or y_pred don't lead to errors\n    labels = np.arange(-1, 3)\n    assert_no_warnings(precision_recall_fscore_support, y_true, y_pred,\n                       labels=labels, average=\"macro\")\n\n    # Test that if only some labels are present in y_true, others in labels\n    # don't lead to an error\n    labels = np.arange(2)\n    assert_no_warnings(precision_recall_fscore_support, y_true, y_pred,\n                       labels=labels, average=\"macro\")\n\n    # Test that multilabel or multioutput input gets an appropriate error\n    y_true_multi = np.array([[1, 0], [0, 1]])\n    y_pred_multi = np.array([[0.5, 0.5], [0.5, 0.5]])\n\n    for y_true, y_pred in [(y_true_multi, y_pred),\n                           (y_true, y_pred_multi)]:\n        with pytest.raises(ValueError):\n            precision_recall_fscore_support(y_true, y_pred, average=\"macro\")\n", "def test_classification_report_multiclass_with_labels():\n    # Test classification report with labels and multiclass classification\n    y_true, y_pred, _ = make_prediction(binary=False)\n    labels = np.unique(y_true)\n    report = classification_report(y_true, y_pred, labels=labels)\n    assert_not_equal(report, \"\")\n", "def test_f1_score_binary_single_class_input():\n    # Test that f1_score function handles binary classification case with a\n    # single class in the input. This test is needed because _check_targets\n    # raises an error when y_true and y_pred contain a single class.\n    y_true = np.array([0, 0, 0, 0])\n    y_pred = np.array([0, 0, 0, 1])\n\n    # Calculate F1-score for the single class case\n    score = f1_score(y_true, y_pred, average='binary')\n\n    assert_almost_equal(score, 0.75)\n"], "sample_748": ["def test_parameter_grid():\n    # Test basic properties of ParameterGrid.\n    param_grid = {'a': [1, 2], 'b': [True, False]}\n    grid = ParameterGrid(param_grid)\n    assert_equal(len(grid), 4)\n    assert_grid_iter_equals_getitem(grid)\n\n    param_grid = [{'a': [1, 2]}, {'b': [True, False]}]\n    grid = ParameterGrid(param_grid)\n    assert_equal(len(grid), 3)\n    assert_grid_iter_equals_getitem(grid)\n\n    param_grid = {'a': [[1, 2]], 'b': [True, False]}\n    grid = ParameterGrid(param_grid)\n    assert_equal(len(grid), 2)\n    assert_grid_iter_equals_getitem(grid)\n", "def test_parameter_grid():\n    # Test basic properties of ParameterGrid.\n    param_grid = {'foo': [1, 2, 3], 'bar': ['a', 'b']}\n    grid = ParameterGrid(param_grid)\n    assert_true(isinstance(grid, Iterable))\n    assert_true(isinstance(grid, Sized))\n    assert_equal(len(grid), 6)\n\n    # smoke test the repr:\n    repr(grid)\n\n    assert_grid_iter_equals_getitem(grid)\n\n    # test that invalid arguments raise appropriate errors\n    assert_raises(TypeError, ParameterGrid, 'foo')\n    assert_raises(ValueError, ParameterGrid, {'foo': 1})\n    assert_raises(ValueError, ParameterGrid, {'foo': []})\n    assert_raises(TypeError, ParameterGrid, {'foo': [1, 2], 'bar': 'baz'})\n\n    # test that empty dict is handled correctly\n    grid = ParameterGrid({})\n    assert_equal(list(grid), [{}])\n", "def test_parameter_grid():\n    param_grid = {'kernel': ('linear', 'rbf'), 'C': [1, 10]}\n    grid = ParameterGrid(param_grid)\n    assert_equal(len(grid), 4)\n    assert_grid_iter_equals_getitem(grid)\n\n    param_grid = {'C': [1, 10]}\n    grid = ParameterGrid(param_grid)\n    assert_equal(len(grid), 2)\n    assert_grid_iter_equals_getitem(grid)\n\n    param_grid = {'kernel': ('linear', 'rbf')}\n    grid = ParameterGrid(param_grid)\n    assert_equal(len(grid), 2)\n    assert_grid_iter_equals_getitem(grid)\n\n    param_grid = {'kernel': ('linear', 'rbf'), 'C': [1, 10], 'gamma': [0.1, 1]}\n    grid = ParameterGrid(param_grid)\n    assert_equal(len(grid), 8)\n    assert_grid_iter_equals_getitem(grid)\n\n    param_grid = [{'kernel': ['linear']}, {'kernel': ['rbf'], 'gamma': [1, 10]}]\n    grid = ParameterGrid(param_grid)\n    assert_equal(len(grid), 3)\n    assert_grid_iter_equals_getitem(grid)\n", "def test_parameter_grid_error():\n    param_grid = {'foo': np.array([[0, 1], [2, 3]])}\n    assert_raises(ValueError, ParameterGrid, param_grid)\n\n    param_grid = {'foo': []}\n    assert_raises(ValueError, ParameterGrid, param_grid)\n\n    param_grid = {'foo': 'bar'}\n    assert_raises(ValueError, ParameterGrid, param_grid)\n", "def test_parameter_grid_empty():\n    # Test that an empty parameter grid raises a ValueError\n    param_grid = {}\n    with assert_raises(ValueError):\n        ParameterGrid(param_grid)\n\n    param_grid = {'foo': []}\n    with assert_raises(ValueError):\n        ParameterGrid(param_grid)\n"], "sample_749": ["def test_column_transformer_remainder_passthrough_sparse():\n    X_array = np.array([[1, 2, 3], [4, 5, 6]])\n    X_sparse = sparse.csr_matrix(X_array)\n    transformer = ColumnTransformer(\n        [(\"trans\", Trans(), [0])],\n        remainder=\"passthrough\"\n    )\n    transformer.fit(X_sparse)\n\n    X_trans_array = transformer.transform(X_array)\n    X_trans_sparse = transformer.transform(X_sparse)\n\n    assert_allclose_dense_sparse(X_trans_array, X_trans_sparse)\n", "def test_column_transformer_get_feature_names():\n    # get feature names with transformers that have get_feature_names method\n    X = np.array([[1, 2], [3, 4]])\n    transformer = ColumnTransformer(\n        [('trans1', StandardScaler(), [0]),\n         ('trans2', Normalizer(), [1])])\n    transformer.fit(X)\n    assert_equal(transformer.get_feature_names(),\n                 ['trans1__x0', 'trans2__x0'])\n\n    # get feature names with transformers that do not have get_feature_names\n    # method\n    transformer = ColumnTransformer(\n        [('trans1', Trans(), [0]),\n         ('trans2', Trans(), [1])])\n    transformer.fit(X)\n    assert_raises(AttributeError, transformer.get_feature_names)\n\n    # get feature names with transformers that are 'passthrough'\n    transformer = ColumnTransformer(\n        [('trans1', 'passthrough', [0]),\n         ('trans2', StandardScaler(), [1])])\n    transformer.fit(X)\n    assert_raises(NotImplementedError, transformer.get_feature_names)\n", "def test_column_transformer_get_feature_names():\n    X = np.array([[1, 2], [3, 4]])\n\n    # single transformer case\n    ct = ColumnTransformer([('trans', Trans(), [0])])\n    ct.fit(X)\n    assert_equal(ct.get_feature_names(), ['trans__0'])\n\n    # multiple transformers case\n    ct = ColumnTransformer([('trans1', Trans(), [0]), ('trans2', Trans(), [1])])\n    ct.fit(X)\n    assert_equal(ct.get_feature_names(), ['trans1__0', 'trans2__0'])\n\n    # test with pandas dataframe\n    import pandas as pd\n    df = pd.DataFrame(X, columns=['col1', 'col2'])\n    ct = ColumnTransformer([('trans1', Trans(), ['col1']), ('trans2', Trans(), ['col2'])])\n    ct.fit(df)\n    assert_equal(ct.get_feature_names(), ['trans1__col1', 'trans2__col2'])\n\n    # passthrough case\n    ct = ColumnTransformer([('trans1', 'passthrough', [0])])\n    with pytest.raises(NotImplementedError):\n        ct.fit(X)\n        ct.get_feature_names()\n\n    # drop case\n    ct = ColumnTransformer([('trans1', 'drop', [0])])\n    ct.fit(X)\n    assert_equal(ct.get_feature_names(), [])\n", "def test_column_transformer_get_feature_names():\n    # Test get_feature_names method for ColumnTransformer\n    X = np.array([[1, 2, 3], [4, 5, 6]])\n\n    # Single transformer case\n    transformer = ColumnTransformer(\n        [('trans', StandardScaler(), [0, 1])],\n        remainder='passthrough'\n    )\n    transformer.fit(X)\n    feature_names = transformer.get_feature_names()\n    assert_array_equal(feature_names, ['trans__x0', 'trans__x1', 'x2'])\n\n    # Multiple transformers case\n    transformer = ColumnTransformer(\n        [('trans1', StandardScaler(), [0]),\n         ('trans2', Normalizer(), [1])],\n        remainder='passthrough'\n    )\n    transformer.fit(X)\n    feature_names = transformer.get_feature_names()\n    assert_array_equal(feature_names, ['trans1__x0', 'trans2__x1', 'x2'])\n", "def test_column_transformer_get_feature_names():\n    # get feature names from a ColumnTransformer\n    X = np.array([[1, 2, 3], [4, 5, 6]])\n    ct = ColumnTransformer([(\"trans1\", Trans(), [0]),\n                            (\"trans2\", Trans(), [1, 2])])\n    ct.fit(X)\n    \n    assert_equal(ct.get_feature_names(), ['trans1__0', 'trans2__1', 'trans2__2'])\n    \n    # get feature names with remainder\n    ct = ColumnTransformer([(\"trans1\", Trans(), [0])],\n                           remainder=\"passthrough\")\n    ct.fit(X)\n    \n    assert_raise_message(NotImplementedError,\n                         \"get_feature_names is not yet supported when using \"\n                         \"a 'passthrough' transformer.\",\n                         ct.get_feature_names)\n    \n    # get feature names when last transformer has no get_feature_names method\n    class NoGetFeatureNamesTrans(BaseEstimator):\n            return self\n\n            return X\n    \n    ct = ColumnTransformer([(\"trans1\", Trans(), [0]),\n                            (\"trans2\", NoGetFeatureNamesTrans(), [1, 2])])\n    ct.fit(X)\n    \n    assert_raise_message(AttributeError,\n                         \"Transformer trans2 (type NoGetFeatureNamesTrans) does \"\n                         \"not provide get_feature_names.\",\n                         ct.get_feature_names)\n"], "sample_750": ["def test_orthogonal_mp_convergence():\n    # Ensure the stopping criterion is reached, and the shapes are correct\n    for fit_intercept in (True, False):\n        omp = OrthogonalMatchingPursuit(n_nonzero_coefs=n_nonzero_coefs,\n                                        fit_intercept=fit_intercept)\n        omp.fit(X, y)\n\n        assert_equal(omp.coef_.shape, (n_features, n_targets))\n        if fit_intercept:\n            assert_equal(omp.intercept_.shape, (n_targets,))\n        else:\n            assert_equal(omp.intercept_, 0)\n        assert_equal(omp.n_iter_, n_targets)\n", "def test_omp_reaches_least_squares():\n    # Check that OMP finds the same solution as least squares when n_nonzero_coefs\n    # is large enough.\n    omp = OrthogonalMatchingPursuit(n_nonzero_coefs=n_features)\n    omp.fit(X, y)\n    ls = LinearRegression()\n    ls.fit(X, y)\n    assert_array_almost_equal(omp.coef_, ls.coef_)\n", "def test_omp_tolerances():\n    # Ensure that OMP with different tolerances give different results\n    tol1 = 0.5\n    tol2 = 1e-6\n\n    omp1 = OrthogonalMatchingPursuit(tol=tol1)\n    omp1.fit(X, y)\n    coef1 = omp1.coef_\n\n    omp2 = OrthogonalMatchingPursuit(tol=tol2)\n    omp2.fit(X, y)\n    coef2 = omp2.coef_\n\n    assert_true(not np.array_equal(coef1, coef2))\n", "def test_omp_path():\n    # Test that the path computed by OMP has the correct shape\n    rng = check_random_state(0)\n    X = rng.randn(n_samples, n_features)\n    y = rng.randn(n_samples, n_targets)\n    coefs = orthogonal_mp(X, y, n_nonzero_coefs=10, return_path=True)\n    assert_equal(coefs.shape[1], n_targets)\n    assert_equal(coefs.shape[2], 10)\n    assert_array_almost_equal(coefs[:, :, -1], orthogonal_mp(X, y, n_nonzero_coefs=10))\n", "def test_omp_path_return_path():\n    # Test that the returned coefficients are correct when return_path=True\n    n_nonzero_coefs = 5\n    tol = None\n    coef = orthogonal_mp(X, y[:, 0], n_nonzero_coefs=n_nonzero_coefs,\n                         tol=tol, return_path=True)\n    assert_equal(coef.shape[1], n_features)\n    assert_equal(coef.shape[0], n_features)\n    # Check that the number of non-zero coefficients in each iteration is\n    # increasing by at most one.\n    for i in range(1, n_features):\n        assert_true(np.count_nonzero(coef[:, i]) -\n                    np.count_nonzero(coef[:, i - 1]) <= 1)\n"], "sample_751": ["def test_random_trees_embedding_sparse():\n    # Test on sparse data\n    X_sparse = csc_matrix(X)\n    rt_sparse = RandomTreesEmbedding()\n    rt_sparse.fit(X_sparse)\n\n    rt_dense = RandomTreesEmbedding()\n    rt_dense.fit(X)\n\n    assert_array_equal(rt_sparse.apply(X_sparse).toarray(),\n                       rt_dense.apply(X))\n", "def test_random_forest_classifier_oob_score():\n    # Test that oob_score is the same as computed manually\n    X, y = iris.data, iris.target\n    clf = RandomForestClassifier(n_estimators=10, bootstrap=True, oob_score=True)\n    clf.fit(X, y)\n\n    # Compute oob_score manually\n    predicted = np.array([tree.predict(X) for tree in clf.estimators_]).T\n    oob_score_manual = np.mean(y == predicted)\n\n    assert_almost_equal(clf.oob_score_, oob_score_manual)\n", "def test_base_forest_transform():\n    # Test whether the transform method of BaseForest works correctly.\n    X, y = iris.data, iris.target\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1,\n                                                        random_state=42)\n\n    clf = RandomForestClassifier(n_estimators=10, random_state=42)\n    clf.fit(X_train, y_train)\n    transformed_X = clf.transform(X_test)\n\n    # The number of features should be equal to the number of estimators\n    assert_equal(transformed_X.shape[1], clf.n_estimators)\n", "def test_random_trees_embedding():\n    # Test the random trees embedding on a dataset\n    X, y = datasets.make_classification(n_samples=1000, n_features=10,\n                                        n_informative=3, n_redundant=0,\n                                        n_repeated=0, random_state=rng)\n\n    # Compute the embedding\n    embedding = RandomTreesEmbedding(n_estimators=10, random_state=rng)\n    X_transformed = embedding.fit_transform(X)\n\n    # Assert the shape of the transformed data\n    assert_equal(X_transformed.shape[0], X.shape[0])\n\n    # Check if the transformation is learned independently of the target\n    X_transformed_again = embedding.transform(X)\n    assert_array_equal(X_transformed, X_transformed_again)\n", "def test_random_trees_embedding_sparse_input():\n    # Test RandomTreesEmbedding with sparse input.\n    X = csc_matrix(np.array([[0, 1], [1, 0]]))\n    rt = RandomTreesEmbedding(n_estimators=5, random_state=0)\n    rt.fit(X)\n\n    assert_array_equal(rt.apply(X).shape, (2, rt.n_estimators))\n"], "sample_752": ["def test_iforest_check_contamination():\n    \"\"\"Check if the 'contamination' parameter is correctly handled\"\"\"\n    X = iris.data\n\n    # check string input for contamination parameter\n    assert_warns_message(FutureWarning, \"default contamination parameter 0.1\"\n                         \" will change in version 0.22 to \\\"auto\\\"\",\n                         IsolationForest(contamination=\"legacy\").fit, X)\n\n    # check float input for contamination parameter\n    assert_warns_message(ValueError, \"when 'contamination' is set to a float,\"\n                         \" it must be in the range [0, 0.5]\",\n                         IsolationForest(contamination=0.6).fit, X)\n\n    # check int input for contamination parameter\n    assert_warns_message(TypeError, \"contamination must be \\\"auto\\\", \\\"legacy\\\"\"\n                         \" or a float\",\n                         IsolationForest(contamination=1).fit, X)\n\n    # check list input for contamination parameter\n    assert_warns_message(TypeError, \"contamination must be \\\"auto\\\", \\\"legacy\\\"\"\n                         \" or a float\",\n                         IsolationForest(contamination=[0.1]).fit, X)\n\n    # check contamination value and threshold_\n    X_inliers = np.random.normal(size=(100, 3), loc=0, scale=0.1)\n    X_outliers = np.random.uniform(size=(10, 3), low=4, high=5)\n    X_test = np.vstack((X_inliers, X_outliers))\n\n    model = IsolationForest(n_estimators=100, contamination=0.1,\n                            random_state=rng).fit(X_test)\n    assert_almost_equal(model.offset_, -model.score_samples(X_test)[10])\n\n    model = IsolationForest(n_estimators=100, contamination='auto',\n                            random_state=rng).fit(X_test)\n    assert_almost_equal(model.offset_, -0.5)\n", "def test_iforest_check_contamination():\n    # Check if the warn message is raised when the contamination parameter is\n    # not in the range [0, 0.5] for an IsolationForest instance\n    assert_warns_message(FutureWarning,\n                         \"default contamination parameter 0.1 will change \"\n                         \"in version 0.22 to \\\"auto\\\". This will change the \"\n                         \"predict method behavior.\",\n                         IsolationForest(contamination=\"legacy\"))\n    \n    # Check if the error is raised when the contamination parameter is\n    # not a float or 'auto' for an IsolationForest instance\n    assert_raises(ValueError,\n                  IsolationForest,\n                  contamination=\"something\")\n    \n    # Check if the error is raised when the contamination parameter is\n    # out of the range [0, 0.5] for an IsolationForest instance\n    assert_raises(ValueError,\n                  IsolationForest,\n                  contamination=0.6)\n", "def test_iforest_average_path_length():\n    # Check _average_path_length function\n    # This function is used within iforest.score_samples method\n\n    # Test n_samples_leaf as integer\n    assert_almost_equal(_average_path_length(1), 1.)\n    assert_almost_equal(_average_path_length(2), 2 * np.log(1) + 2 * euler_gamma - 2)\n    \n    # Test n_samples_leaf as array-like\n    n_samples_leaf = np.array([1, 2])\n    average_path_length = np.zeros_like(n_samples_leaf, dtype=float)\n    average_path_length[0] = 1.\n    average_path_length[1] = 2 * np.log(1) + 2 * euler_gamma - 2\n    assert_array_almost_equal(_average_path_length(n_samples_leaf), average_path_length)\n\n    # Test n_samples_leaf with non-integer value\n    with assert_raises(TypeError):\n        _average_path_length(1.5)\n\n    # Test n_samples_leaf with non-positive value\n    with assert.raises(ValueError):\n        _average_path_length(-1)\n", "def test_iforest_average_path_length():\n    # Test the average path length function\n    assert_almost_equal(_average_path_length(1), 1.)\n    assert_almost_equal(_average_path_length(np.array([1])), 1.)\n    assert_almost_equal(_average_path_length(np.array([1, 1])), 1.)\n\n    n_samples_leaf = np.array([2, 3, 4])\n    average_path_length = _average_path_length(n_samples_leaf)\n    expected_result = 2. * (np.log(n_samples_leaf - 1.) + euler_gamma) - 2. * (\n        n_samples_leaf - 1.) / n_samples_leaf\n    assert_array_almost_equal(average_path_length, expected_result)\n\n    # Check for a larger array of values\n    n_samples_leaf = np.random.randint(1, 100, size=10)\n    average_path_length = _average_path_length(n_samples_leaf)\n    expected_result = np.zeros(n_samples_leaf.shape)\n    mask = (n_samples_leaf <= 1)\n    not_mask = np.logical_not(mask)\n    expected_result[mask] = 1.\n    expected_result[not_mask] = 2. * (\n        np.log(n_samples_leaf[not_mask] - 1.) + euler_gamma) - 2. * (\n        n_samples_leaf[not_mask] - 1.) / n_samples_leaf[not_mask]\n    assert_array_almost_equal(average_path_length, expected_result)\n", "def test_iforest_check_contamination():\n    # Test that the 'contamination' parameter in IsolationForest is handled correctly\n\n    # Test that a contamination value of 0.5 raises an error\n    assert_raises(ValueError, IsolationForest, contamination=0.5)\n\n    # Test that a contamination value of 'auto' sets the threshold_ attribute to -0.5\n    clf = IsolationForest(contamination='auto')\n    iris_data = iris.data\n    clf.fit(iris_data)\n    assert_almost_equal(clf.threshold_, -0.5)\n\n    # Test that a custom contamination value is accepted and used to set the threshold_\n    clf = IsolationForest(contamination=0.1)\n    clf.fit(iris_data)\n    assert_greater(clf.threshold_, -0.5)\n\n    # Test that the contamination parameter is deprecated when using 'legacy'\n    clf = IsolationForest(contamination='legacy')\n    with ignore_warnings(category=FutureWarning):\n        clf.fit(iris_data)\n    assert_almost_equal(clf._contamination, 0.1)\n"], "sample_753": ["def test_logistic_regression_solvers_convergence():\n    \"\"\"Check that all solvers converge to similar solutions\"\"\"\n    X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n    lr_params = {'C': 1.0, 'max_iter': 1000, 'tol': 1e-6}\n\n    # Remove previous warnings about newton-cg\n    with ignore_warnings(category=FutureWarning):\n        # Use a common random seed for all solvers\n        rng = np.random.RandomState(42)\n\n        # Fit the different solvers and store the coefficients\n        coefs = {}\n        for solver in ('liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga'):\n            if solver == 'liblinear':\n                # liblinear solver does not support sparse coef_\n                lr = LogisticRegression(solver=solver, random_state=rng,\n                                        **lr_params)\n            else:\n                lr = LogisticRegression(solver=solver, random_state=rng,\n                                        fit_intercept=False, **lr_params)\n            lr.fit(X, y)\n            coefs[solver] = lr.coef_\n\n    # Check that all solvers converged to a similar solution\n    all_coefs = [coefs[solver] for solver in coefs]\n    for i in range(len(all_coefs)):\n        for j in range(i + 1, len(all_coefs)):\n            assert_allclose(all_coefs[i], all_coefs[j], atol=1e-3)\n", "def test_logistic_regression_solvers():\n    \"\"\"Test that all solvers give similar results on iris dataset\"\"\"\n    X = iris.data\n    y = iris.target\n\n    # Test for ovr (one-vs-rest)\n    for solver in ['liblinear', 'sag', 'lbfgs']:\n        clf = LogisticRegression(solver=solver, max_iter=1000)\n        clf.fit(X, y)\n        assert_almost_equal(clf.score(X, y), 0.96, 2)\n\n    # Test for multinomial\n    for solver in ['newton-cg', 'lbfgs', 'sag']:\n        clf = LogisticRegression(solver=solver, multi_class='multinomial',\n                                 max_iter=1000)\n        clf.fit(X, y)\n        assert_almost_equal(clf.score(X, y), 0.96, 2)\n", "def test_logistic_regression_saga_sparse_binary():\n    # Test the SAGA solver on a binary classification problem with sparse data\n    X = sp.csr_matrix([[0, 1], [1, 0], [1, 1]])\n    y = np.array([0, 1, 1])\n\n    clf = LogisticRegression(solver='saga', random_state=0)\n    check_predictions(clf, X, y)\n\n    # Check that the solver converges to the optimal solution\n    # by comparing the coefficients with those obtained using liblinear\n    clf_liblinear = LogisticRegression(solver='liblinear', random_state=0)\n    clf_liblinear.fit(X, y)\n\n    assert_allclose(clf.coef_, clf_liblinear.coef_, atol=1e-3)\n    assert_allclose(clf.intercept_, clf_liblinear.intercept_, atol=1e-3)\n", "def test_logistic_regression_path():\n    # Test that the path gives the same result as a single fit\n    X = iris.data\n    y = iris.target\n    Cs = [1e-2, 1e-1, 1]\n    max_iter = 1000\n    tol = 1e-7\n    scoring = 'accuracy'\n    solver = 'lbfgs'\n\n    coefs, Cs, scores, n_iter_ = _log_reg_scoring_path(\n        X, y, Cs=Cs, fit_intercept=True, max_iter=max_iter, tol=tol,\n        scoring=scoring, solver=solver)\n\n    clf = LogisticRegression(solver=solver, max_iter=max_iter, tol=tol)\n    for C, coef in zip(Cs, coefs):\n        clf.C = C\n        clf.fit(X, y)\n        assert_array_almost_equal(coef, clf.coef_)\n", "def test_logistic_regression_solvers():\n    \"\"\"Check that different solvers give similar results\"\"\"\n    X, y = make_classification(n_samples=100, n_features=10, n_informative=5,\n                               n_redundant=3, n_repeated=2, random_state=0)\n\n    # Compare newton-cg, lbfgs, sag and saga with liblinear\n    for solver in ['newton-cg', 'lbfgs', 'sag', 'saga']:\n        clf = LogisticRegression(solver=solver, max_iter=200,\n                                 random_state=42).fit(X, y)\n        clf_liblinear = LogisticRegression(solver='liblinear',\n                                           random_state=42).fit(X, y)\n        assert_allclose(clf.coef_, clf_liblinear.coef_, atol=1e-3)\n        assert_allclose(clf.intercept_, clf_liblinear.intercept_, atol=1e-2)\n"], "sample_754": ["def test_sparse_pca_init_parameters():\n    n_components, alpha, ridge_alpha = 5, 0.1, 0.01\n    max_iter, tol = 100, 1e-6\n    method, n_jobs = 'lars', -1\n    U_init, V_init = np.random.rand(10, 5), np.random.rand(5, 20)\n    verbose, random_state = 1, 42\n    normalize_components = True\n\n    # Test SparsePCA init parameters\n    spca = SparsePCA(n_components=n_components, alpha=alpha,\n                     ridge_alpha=ridge_alpha, max_iter=max_iter, tol=tol,\n                     method=method, n_jobs=n_jobs, U_init=U_init, V_init=V_init,\n                     verbose=verbose, random_state=random_state,\n                     normalize_components=normalize_components)\n\n    assert_equal(spca.n_components, n_components)\n    assert_equal(spca.alpha, alpha)\n    assert_equal(spca.ridge_alpha, ridge_alpha)\n    assert_equal(spca.max_iter, max_iter)\n    assert_equal(spca.tol, tol)\n    assert_equal(spca.method, method)\n    assert_equal(spca.n_jobs, n_jobs)\n    assert_array_almost_equal(spca.U_init, U_init)\n    assert_array_almost_equal(spca.V_init, V_init)\n    assert_equal(spca.verbose, verbose)\n    assert_equal(spca.random_state, random_state)\n    assert_equal(spca.normalize_components, normalize_components)\n\n    # Test MiniBatchSparsePCA init parameters\n    mbspca = MiniBatchSparsePCA(n_components=n_components, alpha=alpha,\n                                ridge_alpha=ridge_alpha, n_iter=max_iter,\n                                batch_size=5, callback=None, shuffle=True,\n                                verbose=verbose, random_state=random_state,\n                                normalize_components=normalize_components)\n\n    assert_equal(mbspca.n_components, n_components)\n    assert_equal(mbspca.alpha, alpha)\n    assert_equal(mbspca.ridge_alpha, ridge_alpha)\n    assert_equal(mbspca.n_iter, max_iter)\n    assert_equal(mbspca.batch_size, 5)\n    assert_equal(mbspca.callback, None)\n    assert_equal(mbspca.shuffle, True)\n    assert_equal(mbspca.verbose, verbose)\n    assert_equal(mbspca.random_state, random_state)\n    assert_equal(mbspca.normalize_components, normalize_components)\n", "def test_sparse_pca_verbose(norm_comp):\n    n_components, n_samples, image_size = 3, 100, (10, 10)\n    Y, _, _ = generate_toy_data(n_components, n_samples, image_size)\n\n    # test if verbose outputs the correct number of iterations\n    import io\n    import sys\n    stdout = sys.stdout\n    try:\n        sys.stdout = io.StringIO()\n        spca = SparsePCA(n_components=n_components, max_iter=50,\n                         normalize_components=norm_comp, verbose=True)\n        spca.fit(Y)\n        captured_stdout = sys.stdout.getvalue()\n        assert 'n_iter=' in captured_stdout\n        assert 'Converged at iteration' in captured_stdout\n    finally:\n        sys.stdout = stdout\n\n    # test if verbosity doesn't affect output\n    spca_verbose = SparsePCA(n_components=n_components, max_iter=50,\n                             normalize_components=norm_comp, verbose=True)\n    spca_verbose.fit(Y)\n    spca_not_verbose = SparsePCA(n_components=n_components, max_iter=50,\n                                 normalize_components=norm_comp, verbose=False)\n    spca_not_verbose.fit(Y)\n    assert_array_almost_equal(spca_verbose.components_, spca_not_verbose.components_)\n", "def test_sparse_pca_init():\n    # Test that SparsePCA and MiniBatchSparsePCA initialize with the correct\n    # attributes.\n    spca = SparsePCA(n_components=5, alpha=0.1, ridge_alpha=0.01,\n                     max_iter=1000, tol=1e-8, method='lars', n_jobs=1,\n                     U_init=None, V_init=None, verbose=False,\n                     random_state=42, normalize_components=True)\n\n    assert_equal(spca.n_components, 5)\n    assert_equal(spca.alpha, 0.1)\n    assert_equal(spca.ridge_alpha, 0.01)\n    assert_equal(spca.max_iter, 1000)\n    assert_equal(spca.tol, 1e-8)\n    assert_equal(spca.method, 'lars')\n    assert_equal(spca.n_jobs, 1)\n    assert_equal(spca.U_init, None)\n    assert_equal(spca.V_init, None)\n    assert_equal(spca.verbose, False)\n    assert_equal(spca.random_state, 42)\n    assert_equal(spca.normalize_components, True)\n\n    mb_spca = MiniBatchSparsePCA(n_components=5, alpha=0.1, ridge_alpha=0.01,\n                                  n_iter=100, callback=None, batch_size=3,\n                                  verbose=False, shuffle=True, n_jobs=1,\n                                  method='lars', random_state=42,\n                                  normalize_components=True)\n\n    assert_equal(mb_spca.n_components, 5)\n    assert_equal(mb_spca.alpha, 0.1)\n    assert_equal(mb_spca.ridge_alpha, 0.01)\n    assert_equal(mb_spca.n_iter, 100)\n    assert_equal(mb_spca.callback, None)\n    assert_equal(mb_spca.batch_size, 3)\n    assert_equal(mb_spca.verbose, False)\n    assert_equal(mb_spca.shuffle, True)\n    assert_equal(mb_spca.n_jobs, 1)\n    assert_equal(mb_spca.method, 'lars')\n    assert_equal(mb_spca.random_state, 42)\n    assert_equal(mb_spca.normalize_components, True)\n", "def test_sparse_pca_init_parameters():\n    # Test that the init parameters are correctly set\n    n_components, alpha, ridge_alpha = 5, 0.1, 0.01\n    max_iter, tol = 1000, 1e-8\n    method, n_jobs, verbose = 'lars', 2, True\n    U_init, V_init = np.random.rand(10, 5), np.random.rand(5, 10)\n    random_state, normalize_components = 42, True\n\n    spca = SparsePCA(n_components=n_components, alpha=alpha,\n                     ridge_alpha=ridge_alpha, max_iter=max_iter, tol=tol,\n                     method=method, n_jobs=n_jobs, verbose=verbose,\n                     U_init=U_init, V_init=V_init, random_state=random_state,\n                     normalize_components=normalize_components)\n\n    assert_equal(spca.n_components, n_components)\n    assert_equal(spca.alpha, alpha)\n    assert_equal(spca.ridge_alpha, ridge_alpha)\n    assert_equal(spca.max_iter, max_iter)\n    assert_equal(spca.tol, tol)\n    assert_equal(spca.method, method)\n    assert_equal(spca.n_jobs, n_jobs)\n    assert_equal(spca.verbose, verbose)\n    assert_array_almost_equal(spca.U_init, U_init)\n    assert_array_almost_equal(spca.V_init, V_init)\n    assert_equal(spca.random_state, random_state)\n    assert_equal(spca.normalize_components, normalize_components)\n", "def test_sparse_pca_parameter_checking(norm_comp):\n    X, _, _ = generate_toy_data(3, 10, (8, 8), random_state=0)\n    pca = SparsePCA(n_components=3, normalize_components=norm_comp)\n\n    with pytest.raises(ValueError):\n        pca.set_params(alpha=-1)\n    with pytest.raises(ValueError):\n        pca.set_params(ridge_alpha=-1)\n    with pytest.raises(ValueError):\n        pca.set_params(n_components=-1)\n    with pytest.raises(ValueError):\n        pca.set_params(max_iter=-1)\n    with pytest.raises(ValueError):\n        pca.set_params(tol=-1)\n\n    # Test that a warning is raised when normalize_components=False\n    if not norm_comp:\n        assert_warns_message(DeprecationWarning, \"normalize_components=False\",\n                             pca.fit, X)\n"], "sample_755": ["def test_silhouette_score_precomputed():\n    # Test silhouette score using precomputed distances\n    X = np.array([[0, 0], [0, 1], [1, 0], [2, 2]])\n    D = pairwise_distances(X)\n    labels = np.array([0, 0, 1, 1])\n    score = silhouette_score(D, labels, metric='precomputed')\n    assert score > 0.5\n\n    # Test with sparse matrix\n    D_sparse = csr_matrix(D)\n    score_sparse = silhouette_score(D_sparse, labels, metric='precomputed')\n    assert_array_equal(score, score_sparse)\n", "def test_silhouette_score_non_zero_diagonal():\n    # Test that silhouette_score raises an error when the precomputed distance matrix contains non-zero diagonal elements\n    X = np.array([[0, 1], [1, 0]])\n    labels = np.array([0, 0])\n    with pytest.raises(ValueError):\n        silhouette_score(X, labels, metric='precomputed')\n", "def test_silhouette_score_nonzero_diagonal():\n    # Test that silhouette_score raises an error when the precomputed\n    # distance matrix has non-zero diagonal entries.\n    X = np.array([[0, 1, 2], [1, 0, 3], [2, 3, 0]])\n    labels = np.array([0, 0, 1])\n    with pytest.raises(ValueError):\n        silhouette_score(X, labels, metric='precomputed')\n", "def test_silhouette_score_perfect_labeling():\n    # Test silhouette score for perfect labeling\n    X, y = datasets.make_blobs(n_samples=50, n_features=2, centers=3,\n                               cluster_std=0.6, random_state=0)\n    score = silhouette_score(X, y)\n    assert np.isclose(score, 1, atol=0.05)\n", "def test_silhouette_score_precomputed():\n    # Test silhouette score with precomputed distance matrix\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    labels = np.array([0, 0, 1])\n    dist_matrix = pairwise_distances(X)\n    score = silhouette_score(dist_matrix, labels, metric='precomputed')\n    assert isinstance(score, float)\n\n    # Test that the score is the same as when using a non-precomputed metric\n    score_non_precomputed = silhouette_score(X, labels, metric='euclidean')\n    assert np.isclose(score, score_non_precomputed)\n"], "sample_756": ["def test_optics_fit_predict():\n    optics = OPTICS(min_samples=10, xi=.05, min_cluster_size=.05)\n    labels = optics.fit_predict(X)\n    assert_equal(optics.labels_.shape[0], X.shape[0])\n    assert_array_equal(labels, optics.labels_)\n", "def test_optics_extract_dbscan():\n    # Ensure DBSCAN extraction equivalence\n    X, y = make_blobs(random_state=42)\n    optics = OPTICS(max_eps=2, min_samples=5)\n    optics.fit(X)\n\n    dbscan = DBSCAN(eps=0.5, min_samples=5)\n    dbscan.fit(X)\n\n    labels_optics = optics.extract_dbscan(eps=0.5)[1]\n    labels_dbscan = dbscan.labels_\n\n    contingency = contingency_matrix(labels_optics, labels_dbscan)\n    n_agreements = np.sum(np.max(contingency, axis=0))\n    assert_allclose(n_agreements / len(X), 1, atol=0.05)\n", "def test_optics_blobs():\n    # Generate sample data\n    centers = [[1, 1], [-1, -1], [1, -1]]\n    X, labels_true = make_blobs(n_samples=750, centers=centers, cluster_std=0.4,\n                                random_state=0)\n\n    # Compute OPTICS\n    clust = OPTICS(min_samples=20, xi=.05, min_cluster_size=.05)\n\n    # Run the clustering algorithm\n    clust.fit(X)\n\n    # Check that all labels where assigned\n    assert_equal(np.sum(clust.labels_ == -1), 17)\n\n    # Check that the estimated number of clusters is correct\n    n_clusters_ = (len(set(clust.labels_)) -\n                   (1 if -1 in clust.labels_ else 0))\n    assert_equal(n_clusters_, 3)\n", "def test_optics_reachability_plot():\n    # Test that the reachability plot is correctly computed\n    optics = OPTICS(min_samples=5, max_eps=np.inf)\n    optics.fit(X)\n    reachability_plot = optics.reachability_[optics.ordering_]\n    assert_equal(reachability_plot.shape, (X.shape[0],))\n    assert np.all(reachability_plot >= 0)\n", "def test_optics_extract_dbscan():\n    # Test OPTICS extract_dbscan against DBSCAN for equivalence\n    X, y = make_blobs(n_samples=50, random_state=0)\n    eps = 0.5\n\n    optics = OPTICS(max_eps=eps * 5.0).fit(X)\n    optics_labels = optics.extract_dbscan(eps)\n\n    dbscan = DBSCAN(eps=eps).fit(X)\n    assert_array_equal(optics_labels[1], dbscan.labels_)\n"], "sample_757": ["def test_ordinal_encoder_inverse_transform():\n    # Test inverse transform of OrdinalEncoder\n    X = [['Male', 1], ['Female', 3], ['Female', 2]]\n    enc = OrdinalEncoder()\n    X_tr = enc.fit_transform(X)\n    \n    # Check that inverse transform works correctly\n    X_inv = enc.inverse_transform(X_tr)\n    assert_array_equal(toarray(X_inv), np.array(X, dtype=object))\n", "def test_one_hot_encoder_inverse_transform():\n    # Test that inverse transform is correct for both dense and sparse inputs.\n    X = np.array([[0, 1], [1, 0]])\n    enc = OneHotEncoder()\n    X_tr = enc.fit_transform(X)\n\n    assert_array_equal(enc.inverse_transform(toarray(X_tr)), X)\n    assert_array_equal(enc.inverse_transform(X_tr), X)\n    assert_array_equal(enc.inverse_transform(sparse.csr_matrix(toarray(X_tr))), X)\n", "def test_one_hot_encoder_handle_unknown():\n    X = np.array([[1, 2], [3, 4]])\n    ohe = OneHotEncoder(handle_unknown='error')\n    ohe.fit(X)\n    \n    # Test that an error is raised when an unknown category is encountered\n    X_test = np.array([[1, 5]])\n    assert_raises(ValueError, ohe.transform, X_test)\n\n    # Test that no error is raised when handle_unknown is set to 'ignore'\n    ohe = OneHotEncoder(handle_unknown='ignore')\n    ohe.fit(X)\n    X_test = np.array([[1, 5]])\n    ohe.transform(X_test)\n", "def test_ordinal_encoder_inverse_transform():\n    # Test that inverse transform works as expected\n    enc = OrdinalEncoder()\n    X = [['Male', 1], ['Female', 3], ['Female', 2]]\n    enc.fit(X)\n    transformed_X = enc.transform(X)\n    inverse_X = enc.inverse_transform(transformed_X)\n    assert_array_equal(inverse_X, X)\n\n    # Test that inverse transform raises an error when input has invalid shape\n    with assert_raises(ValueError):\n        enc.inverse_transform([[0], [1]])\n", "def test_one_hot_encoder_categories():\n    # Test the new categories parameter of OneHotEncoder\n    X = [['Male', 1], ['Female', 3], ['Female', 2]]\n    encoder = OneHotEncoder(categories=[['Male', 'Female'], [1, 2, 3]])\n\n    # fit and transform should work as expected\n    Xt = encoder.fit_transform(X)\n    assert_array_equal(toarray(Xt), [[0., 1., 0., 0., 1.],\n                                     [1., 0., 0., 0., 0.],\n                                     [1., 0., 0., 1., 0.]])\n    assert_array_equal(encoder.categories_, [['Male', 'Female'], [1, 2, 3]])\n\n    # Check that an error is raised if there are unknown categories during fit\n    with pytest.raises(ValueError):\n        encoder = OneHotEncoder(categories=[['Male'], [1, 2, 3]])\n        encoder.fit(X)\n\n    # Check that an error is raised if there are unknown categories during transform\n    with pytest.raises(ValueError):\n        encoder = OneHotEncoder(categories=[['Male', 'Female'], [1, 2, 3]])\n        encoder.fit([['Male', 1]])\n        encoder.transform([['Female', 4]])\n\n    # Check that handle_unknown works as expected\n    encoder = OneHotEncoder(categories=[['Male', 'Female'], [1, 2, 3]],\n                            handle_unknown='ignore')\n    Xt = encoder.fit_transform([['Male', 1], ['Female', 4]])\n    assert_array_equal(toarray(Xt), [[0., 1., 0., 0., 1.],\n                                     [1., 0., 0., 0., 0.]])\n"], "sample_758": ["def test_check_is_fitted():\n    class Estimator:\n            self.estimator_ = None\n\n            self.estimator_ = np.mean(X, axis=0)\n\n    estimator = Estimator()\n\n    # Not fitted\n    assert_raises(NotFittedError, check_is_fitted, estimator, \"estimator_\")\n\n    # Fitted\n    estimator.fit([[1, 2], [3, 4]])\n    assert_no_warnings(check_is_fitted, estimator, \"estimator_\")\n\n    # Custom message\n    msg = \"Estimator needs to be fitted before use\"\n    with pytest.raises(NotFittedError, match=msg):\n        check_is_fitted(estimator, \"nonexistent_attribute_\", msg=msg)\n", "def test_check_is_fitted():\n    class Estimator:\n            self.fitted_ = True\n\n    estimator = Estimator()\n    assert_raises(NotFittedError, check_is_fitted, estimator, 'fitted_')\n\n    estimator.fit(None)\n    check_is_fitted(estimator, 'fitted_')\n\n    # Check that it also works with attributes that are not ending with an\n    # underscore.\n    check_is_fitted(estimator, 'fit')\n", "def test_check_is_fitted():\n    class Estimator:\n            self.coef_ = None\n\n    estimator = Estimator()\n    assert_raises_regex(NotFittedError,\n                        \"This Estimator instance is not fitted yet\",\n                        check_is_fitted, estimator, \"coef_\")\n\n    estimator.fit()\n    check_is_fitted(estimator, \"coef_\")\n\n    # check if the message is correctly formatted\n    estimator = Estimator()\n    msg = (\"Estimator %s must be fitted before use\")\n    assert_raise_message(NotFittedError, msg % type(estimator).__name__,\n                         check_is_fitted, estimator, \"coef_\", msg=msg)\n", "def test_check_is_fitted():\n    class Estimator:\n            self.fitted_ = False\n\n            self.fitted_ = True\n\n    estimator = Estimator()\n    assert_raises(NotFittedError, check_is_fitted, estimator, 'fitted_')\n\n    estimator.fit(None)\n    check_is_fitted(estimator, 'fitted_')\n\n    # check multiple attributes\n    assert_raises(NotFittedError, check_is_fitted, estimator, ['fitted_', 'not_fitted_'])\n\n    estimator.not_fitted_ = True\n    check_is_fitted(estimator, ['fitted_', 'not_fitted_'])\n\n    # check with custom message\n    msg = \"Estimator must be fitted before use\"\n    assert_raise_message(NotFittedError, msg, check_is_fitted, estimator, 'not existing', msg=msg)\n", "def test_check_is_fitted():\n    class Estimator:\n            self.fitted_ = True\n\n    estimator = Estimator()\n    assert_raises(NotFittedError, check_is_fitted, estimator, 'fitted_')\n\n    estimator.fit(None)\n    check_is_fitted(estimator, 'fitted_')\n\n    # Check multiple attributes\n    class MultiAttrEstimator:\n            self.attr1_ = True\n            self.attr2_ = True\n\n    multi_attr_estimator = MultiAttrEstimator()\n\n    # Before fitting\n    assert_raises(NotFittedError, check_is_fitted, multi_attr_estimator,\n                  ['attr1_', 'attr2_'])\n\n    multi_attr_estimator.fit(None)\n    check_is_fitted(multi_attr_estimator, ['attr1_', 'attr2_'])\n    check_is_fitted(multi_attr_estimator, ('attr1_', 'attr2_'))\n\n    # Check with any\n    class PartialFitEstimator:\n            self.attr1_ = None\n\n            self.attr1_ = True\n\n    partial_fit_estimator = PartialFitEstimator()\n    check_is_fitted(partial_fit_estimator, ['attr1_', 'attr2_'], all_or_any=any)\n    assert_raises(NotFittedError, check_is_fitted, partial_fit_estimator,\n                  ['attr1_', 'attr2_'], all_or_any=all)\n\n    partial_fit_estimator.partial_fit(None)\n    check_is_fitted(partial_fit_estimator, ['attr1_', 'attr2_'], all_or_any=any)\n    check_is_fitted(partial_fit_estimator, ['attr1_', 'attr2_'], all_or_any=all)\n"], "sample_759": ["def test_one_hot_encoder_handle_unknown():\n    # Test the handle_unknown parameter of OneHotEncoder\n    X = [['Male', 1], ['Female', 3], ['Female', 2]]\n    enc = OneHotEncoder(handle_unknown='error')\n    enc.fit(X)\n\n    # Test that an error is raised when an unknown category is encountered\n    with pytest.raises(ValueError):\n        enc.transform([['Unknown', 3]])\n\n    # Test that no error is raised and the resulting array has all zeros in\n    # the corresponding row when handle_unknown='ignore'\n    enc = OneHotEncoder(handle_unknown='ignore')\n    enc.fit(X)\n    X_tr = enc.transform([['Unknown', 3]])\n    assert_array_equal(toarray(X_tr)[0, :], np.zeros(X_tr.shape[1]))\n", "def test_one_hot_encoder_inverse_transform():\n    # Test inverse transform for one-hot encoder.\n    X = [['Male', 1], ['Female', 3], ['Female', 2]]\n    enc = OneHotEncoder(handle_unknown='ignore')\n    X_tr = enc.fit_transform(X)\n    assert_array_equal(enc.inverse_transform(toarray(X_tr)), X)\n", "def test_one_hot_encoder_inverse_transform():\n    # Test inverse transform of OneHotEncoder for all possible cases\n    X = [['Male', 1], ['Female', 3], ['Female', 2]]\n    enc = OneHotEncoder(handle_unknown='ignore')\n    X_tr = enc.fit_transform(X)\n\n    # check for correct output shape\n    assert_array_equal(toarray(X_tr).shape, (3, 5))\n\n    # check for correct number of features in output\n    assert_equal(toarray(X_tr).shape[1], 5)\n\n    # check for correct inverse transform\n    X_inv = enc.inverse_transform(X_tr)\n    assert_array_equal(X_inv, [['Male', 1], ['Female', 3], ['Female', 2]])\n\n    # check for correct inverse transform when unknown categories are ignored\n    X_tr[0, :] = 0\n    X_inv = enc.inverse_transform(X_tr)\n    assert_array_equal(X_inv, [[None, 1], ['Female', 3], ['Female', 2]])\n", "def test_ordinal_encoder_inverse_transform():\n    # Test inverse_transform after fitting with categorical data\n    enc = OrdinalEncoder()\n    X = [['Male', 1], ['Female', 3], ['Female', 2]]\n    enc.fit(X)\n\n    transformed_X = enc.transform([['Female', 3], ['Male', 1]])\n    inverse_transformed_X = enc.inverse_transform(transformed_X)\n    assert_array_equal(inverse_transformed_X, [['Female', 3], ['Male', 1]])\n", "def test_one_hot_encoder_inverse_transform():\n    # create an instance of OneHotEncoder\n    enc = OneHotEncoder()\n\n    # fit the encoder with a simple dataset\n    X = np.array([[1, 2], [3, 4]])\n    enc.fit(X)\n\n    # transform a new dataset\n    X_new = np.array([[1, 4], [3, 2]])\n    X_transformed = enc.transform(X_new)\n\n    # inverse transform and check if we get back the original data\n    X_inverse = enc.inverse_transform(toarray(X_transformed))\n    assert_array_equal(X_inverse, X_new)\n"], "sample_760": ["def test_make_scorer_with_kwargs():\n    # Test that make_scorer captures the kwargs correctly\n        return kwargs['foo']\n\n    scorer = make_scorer(score_func, greater_is_better=True, needs_proba=False,\n                         foo='bar')\n    assert_almost_equal(scorer(EstimatorWithFitAndPredict(), [1], [1]), 'bar')\n\n    # Smoke test that refit works with kwargs\n    GridSearchCV(estimator=DecisionTreeClassifier(random_state=0),\n                 param_grid={'max_depth': [5]}, scoring=scorer).fit(X_mm, y_mm)\n", "def test_get_scorer():\n    # Test that get_scorer returns the correct scorer\n    scorer = get_scorer('accuracy')\n    assert isinstance(scorer, _PredictScorer)\n    assert scorer._score_func.__name__ == 'accuracy_score'\n\n    # Test that get_scorer raises an error for unknown scorers\n    with pytest.raises(ValueError):\n        get_scorer('unknown_scorer')\n\n    # Test that get_scorer returns a callable when given a callable\n    scorer = get_scorer(accuracy_score)\n    assert callable(scorer)\n", "def test_make_scorer_with_kwargs():\n    # Test that kwargs are correctly passed to the score function\n        return np.mean(y_true == y_pred) if foo is None else foo\n\n    scorer = make_scorer(score_func, greater_is_better=True, foo=0.5)\n    score = scorer(EstimatorWithFitAndPredict(), np.array([1, 2, 3]), np.array([1, 2, 3]))\n    assert_almost_equal(score, 0.5)\n\n    scorer = make_scorer(score_func, greater_is_better=True)\n    score = scorer(EstimatorWithFitAndPredict(), np.array([1, 2, 3]), np.array([1, 2, 3]))\n    assert_almost_equal(score, 1.0)\n", "def test_make_scorer_needs_proba():\n    # Test that needs_proba is properly passed to the scorer\n    scoring = make_scorer(log_loss, greater_is_better=False, needs_proba=True)\n    assert_true(isinstance(scoring, _ProbaScorer))\n\n    scoring = make_scorer(accuracy_score, greater_is_better=True,\n                          needs_proba=False)\n    assert_true(isinstance(scoring, _PredictScorer))\n\n    # Test that an error is raised when needs_proba and needs_threshold are both\n    # True\n    with assert_raises(ValueError):\n        make_scorer(log_loss, greater_is_better=False, needs_proba=True,\n                    needs_threshold=True)\n", "def test_make_scorer_parsing():\n    # Test that make_scorer is able to parse 'accuracy' and\n    # 'neg_log_loss' to accuracy_score and log_loss respectively.\n    accuracy = make_scorer('accuracy')\n    assert_true(isinstance(accuracy, _PredictScorer))\n    assert_equal(accuracy._score_func, accuracy_score)\n\n    neg_log_loss = make_scorer('neg_log_loss')\n    assert_true(isinstance(neg_log_loss, _ProbaScorer))\n    assert_equal(neg_log_loss._score_func, log_loss)\n"], "sample_761": ["def test_iterative_imputer_check_statistics():\n    # Check the statistics computation\n    rng = np.random.RandomState(0)\n    X = sparse_random_matrix(100, 10, random_state=rng).toarray()\n    missing_values = X.min() - 1\n    X.flat[rng.sample(X.size, 20)] = missing_values\n\n    imputer = IterativeImputer(missing_values=missing_values,\n                               initial_strategy='mean',\n                               n_nearest_features=5)\n    imputer.fit(X)\n\n    mean_init = SimpleImputer(strategy='mean').fit_transform(X)\n    assert_allclose(imputer.initial_imputer_.statistics_,\n                    mean_init.mean(axis=0))\n", "def test_iterative_imputer_sparse():\n    # Test IterativeImputer on sparse data\n    X = sparse_random_matrix(10, 5, density=0.5, random_state=0)\n    X_missing = X.copy()\n    X_missing.data[0:10] = np.nan\n\n    imputer = IterativeImputer(max_iter=10, random_state=0)\n\n    with pytest.raises(ValueError):\n        imputer.fit_transform(X_missing)\n", "def test_iterative_imputer_sparse_inputs():\n    # Test that IterativeImputer supports sparse inputs\n    rng = np.random.RandomState(0)\n    X = sparse_random_matrix(100, 10, density=0.1, random_state=rng).toarray()\n    missing_values = X.max() + 1\n    X_missing = X.copy()\n    X_missing[~np.isnan(X_missing)] = missing_values\n\n    imputer = IterativeImputer(missing_values=missing_values,\n                               estimator=RidgeCV(),\n                               n_nearest_features=5)\n\n    X_imputed = imputer.fit_transform(sparse.csr_matrix(X_missing))\n    assert_allclose_dense_sparse(X_imputed, X, atol=10)\n", "def test_iterative_imputer_get_neighbor_idx():\n    # Test the _get_neighbor_feat_idx function\n\n    rng = np.random.RandomState(0)\n    X = sparse_random_matrix(10, 5, random_state=rng).toarray()\n    imputer = IterativeImputer(n_nearest_features=2, random_state=rng)\n\n    # create a corr matrix with known structure\n    X[:, 0] = X[:, 1] + X[:, 2]\n    X[:, 3] = -X[:, 4]\n\n    abs_corr_mat = imputer._get_abs_corr_mat(X)\n    np.testing.assert_array_equal(abs_corr_mat[0, 1:3], np.array([1., 1.]))\n\n    for i in range(5):\n        neighbor_idx = imputer._get_neighbor_feat_idx(5, i, abs_corr_mat)\n        assert len(neighbor_idx) == 2\n", "def test_iterative_imputer_check_fit_and_transform():\n    # Test that fit and transform can be called on different data\n    X_fit = np.array([[1, 4], [2, 5], [3, 6]])\n    X_trans = np.array([[1, 4], [np.nan, 5]])\n\n    imputer = IterativeImputer(random_state=0)\n    imputer.fit(X_fit)\n\n    Xt = imputer.transform(X_trans)\n    assert_array_equal(Xt, np.array([[1., 4.], [2., 5.]]))\n"], "sample_762": ["def test_clone_empty_array():\n    # Test that clone works with numpy arrays as parameters\n    estimator = MyEstimator(empty=np.array([]))\n    cloned_estimator = clone(estimator)\n    assert_array_equal(cloned_estimator.empty, np.array([]))\n", "def test_clone_modify_init_params():\n    # Test that clone doesn't fail with deprecated behavior\n    # of estimators modifying parameters in __init__\n    estimator = ModifyInitParams()\n    clone(estimator)\n", "def test_clone_modify_init_params():\n    # Test that clone works with estimators that modify their init params\n    estimator = ModifyInitParams()\n    cloned_estimator = clone(estimator)\n    assert_array_equal(cloned_estimator.a, estimator.a)\n\n    estimator = ModifyInitParams(a=np.array([1, 2, 3]))\n    cloned_estimator = clone(estimator)\n    assert_array_equal(cloned_estimator.a, estimator.a)\n", "def test_base_estimator_repr():\n    # Test that the repr of a BaseEstimator contains its parameters.\n    estimator = MyEstimator(l1=0, empty=None)\n    assert_equal(repr(estimator), \"MyEstimator(empty=None, l1=0)\")\n", "def test_clone_with_nones():\n    # Test that clone handles None values correctly\n    estimator = MyEstimator(l1=0, empty=None)\n    cloned_estimator = clone(estimator)\n    assert_equal(cloned_estimator.l1, 0)\n    assert_equal(cloned_estimator.empty, None)\n"], "sample_763": ["def test_check_is_fitted():\n    class Estimator:\n            self.coef_ = None\n\n            self.coef_ = np.array([1])\n\n    estimator = Estimator()\n    with pytest.raises(NotFittedError):\n        check_is_fitted(estimator, ['coef_'])\n\n    estimator.fit(None, None)\n    assert_no_warnings(check_is_fitted, estimator, ['coef_'])\n\n    class EstimatorWithFitAttribute:\n            self.coef_ = None\n            self.fit_ = None\n\n            self.coef_ = np.array([1])\n            self.fit_ = True\n\n    estimator_with_fit_attribute = EstimatorWithFitAttribute()\n    with pytest.raises(NotFittedError):\n        check_is_fitted(estimator_with_fit_attribute, ['coef_', 'fit_'])\n\n    estimator_with_fit_attribute.fit(None, None)\n    assert_no_warnings(check_is_fitted, estimator_with_fit_attribute, ['coef_', 'fit_'])\n", "def test_check_is_fitted():\n    class Estimator:\n            self.fitted_ = False\n\n            self.fitted_ = True\n\n    estimator = Estimator()\n    assert_raises_regex(NotFittedError, \"Estimator instance is not fitted yet.\",\n                        check_is_fitted, estimator, 'fitted_')\n    estimator.fit(None)\n    assert_no_warnings(check_is_fitted, estimator, 'fitted_')\n\n    class Estimator2:\n            pass\n\n            pass\n\n    estimator = Estimator2()\n    assert_raises_regex(NotFittedError, \"Estimator2 instance is not fitted yet.\",\n                        check_is_fitted, estimator, ['fitted_'])\n    assert_raises_regex(TypeError, \"is not an estimator instance\",\n                        check_is_fitted, object(), 'fitted_')\n", "def test_check_is_fitted():\n    class Estimator:\n            self.estimator_ = None\n\n            self.estimator_ = 'fitted'\n\n    estimator = Estimator()\n\n    # not fitted\n    with pytest.raises(NotFittedError):\n        check_is_fitted(estimator, 'estimator_')\n\n    # now it's fitted\n    estimator.fit()\n    assert_no_warnings(check_is_fitted, estimator, 'estimator_')\n    assert_no_warnings(check_is_fitted, estimator, ['estimator_'])\n", "def test_check_is_fitted():\n    class Estimator:\n            self.estimator_ = None\n\n            self.estimator_ = np.array([1., 2., 3.])\n\n    estimator = Estimator()\n    with pytest.raises(NotFittedError):\n        check_is_fitted(estimator, 'estimator_')\n\n    estimator.fit()\n    assert_no_warnings(check_is_fitted, estimator, 'estimator_')\n\n    # checking multiple attributes\n    with pytest.raises(NotFittedError):\n        check_is_fitted(Estimator(), ['estimator_', 'coef_'])\n\n    estimator.fit()\n    estimator.coef_ = np.array([1., 2., 3.])\n    assert_no_warnings(check_is_fitted, estimator, ['estimator_', 'coef_'])\n", "def test_check_is_fitted():\n    class Estimator:\n            self.coef_ = None\n\n    estimator = Estimator()\n    assert_raises_regex(NotFittedError,\n                        \"Estimator needs to be fit before `check_is_fitted` can be used\",\n                        check_is_fitted, estimator, 'coef_')\n    estimator.fit()\n    assert_no_warnings(check_is_fitted, estimator, 'coef_')\n\n    class Estimator2:\n            self.n_features_in_ = 1\n\n    estimator2 = Estimator2()\n    # This should not raise an error since we have n_features_in_\n    assert_no_warnings(check_is_fitted, estimator2, 'n_features_in_')\n\n    class Estimator3:\n            self.n_features_in_ = 1\n            self.n_features_out_ = 1\n\n    estimator3 = Estimator3()\n    # This should not raise an error since we have n_features_in_ and n_features_out_\n    assert_no_warnings(check_is_fitted, estimator3, ['n_features_in_', 'n_features_out_'])\n"], "sample_764": ["def test_column_transformer_get_feature_names():\n    X = np.array([[1, 2], [3, 4]])\n    columns = ['col1', 'col2']\n    transformer = ColumnTransformer(\n        [('trans1', Trans(), ['col1']), ('trans2', Trans(), ['col2'])]\n    )\n    transformer.fit(X, columns)\n\n    feature_names = transformer.get_feature_names()\n    assert_array_equal(feature_names, ['trans1__col1', 'trans2__col2'])\n", "def test_column_transformer_remainder_pandas():\n    # Test that the 'remainder' parameter works with pandas DataFrames\n    X_array = np.array([[1, 2, 3], [4, 5, 6]])\n    X_df = pd.DataFrame(X_array, columns=['a', 'b', 'c'])\n\n    # Use ColumnTransformer with 'remainder' and a pandas DataFrame\n    ct = ColumnTransformer(\n        [('trans1', Trans(), ['a'])],\n        remainder='passthrough'\n    )\n    result_df = ct.fit_transform(X_df)\n    assert_array_equal(result_df, np.array([[1, 2, 3], [4, 5, 6]]))\n\n    # Use ColumnTransformer with 'remainder' and a numpy array\n    ct = ColumnTransformer(\n        [('trans1', Trans(), [0])],\n        remainder='passthrough'\n    )\n    result_array = ct.fit_transform(X_array)\n    assert_array_equal(result_array, np.array([[1, 2, 3], [4, 5, 6]]))\n", "def test_column_transformer_get_feature_names():\n    X = np.array([[1, 2, 3], [4, 5, 6]])\n\n    # Single transformer\n    ct = ColumnTransformer([(\"trans\", Trans(), [0])])\n    ct.fit(X)\n    assert_equal(ct.get_feature_names(), [\"trans__x0\"])\n\n    # Multiple transformers\n    ct = ColumnTransformer([(\"trans1\", Trans(), [0]), (\"trans2\", Trans(), [1])])\n    ct.fit(X)\n    assert_equal(ct.get_feature_names(), [\"trans1__x0\", \"trans2__x1\"])\n\n    # Transformer with multiple columns\n    ct = ColumnTransformer([(\"trans\", Trans(), [0, 1])])\n    ct.fit(X)\n    assert_equal(ct.get_feature_names(), [\"trans__x0\", \"trans__x1\"])\n\n    # 'drop' transformer\n    ct = ColumnTransformer([(\"drop\", \"drop\", [0]), (\"trans\", Trans(), [1])])\n    ct.fit(X)\n    assert_equal(ct.get_feature_names(), [\"trans__x1\"])\n\n    # 'passthrough' transformer\n    ct = ColumnTransformer([(\"pass\", \"passthrough\", [0]), (\"trans\", Trans(), [1])])\n    ct.fit(X)\n    with pytest.raises(NotImplementedError):\n        ct.get_feature_names()\n", "def test_column_transformer_remainder_pandas():\n    # Test pandas DataFrame in remainder\n    X_array = np.array([[1, 2, 3], [4, 5, 6]])\n    X_df = pd.DataFrame(X_array, columns=['a', 'b', 'c'])\n\n    # All columns selected\n    ct_all = ColumnTransformer([('trans', Trans(), ['a', 'b', 'c'])],\n                               remainder='passthrough')\n    assert_array_equal(ct_all.fit_transform(X_df), X_array)\n\n    # All columns selected with passthrough\n    ct_passthrough = ColumnTransformer([('trans', Trans(), ['a', 'b'])],\n                                       remainder='passthrough')\n    assert_array_equal(ct_passthrough.fit_transform(X_df), X_array)\n\n    # No columns selected and passthrough\n    ct_no_trans = ColumnTransformer([], remainder='passthrough')\n    assert_array_equal(ct_no_trans.fit_transform(X_df), X_array)\n", "def test_column_transformer_get_feature_names():\n    X = np.array([[1, 2, 3], [4, 5, 6]])\n\n    # single transformer case\n    ct = ColumnTransformer([('trans', Trans(), [0, 1])])\n    ct.fit(X)\n    assert_equal(ct.get_feature_names(), ['trans__x0', 'trans__x1'])\n\n    # multiple transformer case\n    ct = ColumnTransformer([('trans1', Trans(), [0]),\n                            ('trans2', Trans(), [1, 2])])\n    ct.fit(X)\n    assert_equal(ct.get_feature_names(), ['trans1__x0', 'trans2__x0', 'trans2__x1'])\n\n    # not fitted case\n    ct = ColumnTransformer([('trans', Trans(), [0, 1])])\n    assert_raises(NotFittedError, ct.get_feature_names)\n\n    # passthrough case\n    ct = ColumnTransformer([('passthrough', 'passthrough', [0])])\n    ct.fit(X)\n    with pytest.raises(NotImplementedError):\n        ct.get_feature_names()\n"], "sample_765": ["def test_precision_recall_fscore_support_errors():\n    y_true, y_pred, _ = make_prediction(binary=True)\n\n    # Test that labels not present in y_true or y_pred don't lead to a ZeroDivisionError\n    assert_warns_message(UndefinedMetricWarning,\n                         \"Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\",\n                         precision_recall_fscore_support, y_true, y_pred,\n                         labels=[0, 1, 2], average=None)\n\n    # Test that labels not present in y_true or y_pred don't lead to a ZeroDivisionError\n    assert_warns_message(UndefinedMetricWarning,\n                         \"Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\",\n                         precision_recall_fscore_support, y_true, y_pred,\n                         labels=[0, 1, 2], average=None)\n\n    # Test that if there are no true samples, recall is still calculated\n    assert_warns_message(UndefinedMetricWarning,\n                         \"Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\",\n                         precision_recall_fscore_support, [0, 0], [0, 1],\n                         average=None)\n", "def test_multilabel_confusion_matrix_binary():\n    # Test multilabel confusion matrix for binary classification problems\n    y_true = np.array([[0, 1], [1, 0]])\n    y_pred = np.array([[0, 1], [0, 1]])\n\n    # Multilabel-indicator case\n    cm = multilabel_confusion_matrix(y_true, y_pred)\n    assert_array_equal(cm, np.array([[[1, 0], [0, 1]], [[1, 0], [1, 0]]]))\n\n    # Label indicator array / sparse matrix case\n    y_true_sparse = csr_matrix(y_true)\n    y_pred_sparse = csr_matrix(y_pred)\n    cm_sparse = multilabel_confusion_matrix(y_true_sparse, y_pred_sparse)\n    assert_array_equal(cm_sparse, cm)\n\n    # 1d array-like of shape (n_samples,) case\n    y_true_1d = np.array([1, 0])\n    y_pred_1d = np.array([1, 1])\n    cm_1d = multilabel_confusion_matrix(y_true_1d, y_pred_1d)\n    assert_array_equal(cm_1d, np.array([[[0, 1], [0, 1]]]))\n", "def test_precision_recall_fscore_support_errors():\n    y_true, y_pred, _ = make_prediction(binary=True)\n\n    # Bad beta\n    assert_raises(ValueError, precision_recall_fscore_support,\n                  y_true, y_pred, beta=0.0)\n\n    # Bad average option\n    assert_raises(ValueError, precision_recall_fscore_support,\n                  [1, 2, 3], [1, 2, 3], average='mega')\n", "def test_precision_recall_fscore_support_errors():\n    y_true, y_pred, _ = make_prediction(binary=True)\n\n    # Raise ValueError if pos_label is not present in y_true\n    assert_raise_message(ValueError, \"pos_label=3 is not a valid label\",\n                         precision_recall_fscore_support, y_true, y_pred,\n                         pos_label=3)\n\n    # Raise ValueError if there are more than 2 unique labels when pos_label\n    # is provided\n    y_true = np.array([0, 1, 2])\n    y_pred = np.array([0, 1, 2])\n    assert_raise_message(ValueError, \"More than one label\",\n                         precision_recall_fscore_support, y_true, y_pred,\n                         pos_label=0)\n", "def test_multilabel_confusion_matrix_binary():\n    # Test multilabel_confusion_matrix on a binary classification problem.\n    y_true = [0, 1, 0, 1]\n    y_pred = [0, 1, 1, 0]\n\n    # Binary classification as multi-label classification\n    y_true_ml = [[y_true[i]] for i in range(len(y_true))]\n    y_pred_ml = [[y_pred[i]] for i in range(len(y_pred))]\n\n    cm = multilabel_confusion_matrix(y_true_ml, y_pred_ml)\n\n    assert_array_equal(cm, [[[2, 1], [1, 2]]])\n"], "sample_766": ["def test_dict_learning_online_overcomplete():\n    n_components = 12\n    dlo = MiniBatchDictionaryLearning(n_components=n_components, random_state=0)\n    X = rng_global.randn(10, 8)\n    dlo.fit(X)\n    assert_equal(dlo.components_.shape, (n_components, X.shape[1]))\n", "def test_dict_learning_online_partial_fit():\n    n_components = 5\n    dico = MiniBatchDictionaryLearning(n_components=n_components,\n                                       batch_size=10, random_state=0)\n    X_batch1 = rng_global.randn(20, n_features)\n    X_batch2 = rng_global.randn(30, n_features)\n\n    # Initialisation\n    assert not hasattr(dico, 'components_')\n\n    # First fit with 20 samples\n    dico.partial_fit(X_batch1)\n    assert dico.components_.shape == (n_components, n_features)\n\n    # Second fit with 30 samples\n    components_batch_1_2 = dico.components_.copy()\n    dico.partial_fit(X_batch2)\n    components_batch_1_2_new = dico.components_\n\n    # Fit with all the data at once\n    X = np.vstack((X_batch1, X_batch2))\n    dico_full = MiniBatchDictionaryLearning(n_components=n_components,\n                                            batch_size=50, random_state=0)\n    dico_full.fit(X)\n    components_full = dico_full.components_\n\n    # Test if the results of one fit with 50 samples and two fits with\n    # 20 and 30 samples are close\n    assert_array_almost_equal(components_batch_1_2, components_full)\n    assert_array_almost_equal(components_batch_1_2_new, components_full)\n", "def test_dict_learning_online_partial_fit():\n    n_components = 5\n    dlo = MiniBatchDictionaryLearning(n_components=n_components, n_iter=1)\n    X_batch1 = rng_global.randn(10, n_features)\n    X_batch2 = rng_global.randn(10, n_features)\n\n    # Initial dictionary is random\n    dict_init = rng_global.randn(n_components, n_features)\n    dlo.dict_init = dict_init\n    dlo.partial_fit(X_batch1)\n\n    # Get the inner stats of the first iteration\n    A_batch1, B_batch1 = dlo.inner_stats_\n\n    # Compute second iteration manually\n    dlo2 = MiniBatchDictionaryLearning(n_components=n_components, n_iter=1)\n    dlo2.dict_init = dlo.components_\n    dlo2.inner_stats_ = (A_batch1, B_batch1)\n    dlo2.iter_offset_ = 1\n    dlo2.partial_fit(X_batch2)\n\n    # Compare with a single call to partial_fit\n    dlo_single = MiniBatchDictionaryLearning(n_components=n_components, n_iter=1)\n    dlo_single.dict_init = dict_init\n    dlo_single.partial_fit(np.vstack((X_batch1, X_batch2)))\n\n    assert_array_almost_equal(dlo2.components_, dlo_single.components_)\n", "def test_dict_learning_online_partial_fit():\n    # Test that dict_learning_online partial_fit is equivalent to fit\n    n_components = 5\n    alpha = 1\n    batch_size = 3\n    n_iter = 10\n    dict_init = rng_global.randn(n_components, n_features)\n\n    dlo = MiniBatchDictionaryLearning(\n        n_components=n_components, alpha=alpha, batch_size=batch_size,\n        n_iter=n_iter, dict_init=dict_init, random_state=rng_global)\n    X_batch = X[:batch_size]\n    dlo.partial_fit(X_batch)\n\n    # Compute the code for the rest of the data using online dict learning\n    for i in range(batch_size, len(X), batch_size):\n        X_batch = X[i:i + batch_size]\n        dlo.partial_fit(X_batch)\n\n    # Fit a second object on the entire data\n    dlo_2 = MiniBatchDictionaryLearning(\n        n_components=n_components, alpha=alpha, batch_size=batch_size,\n        n_iter=n_iter, dict_init=dict_init, random_state=rng_global)\n    dlo_2.fit(X)\n\n    assert_array_almost_equal(dlo.components_, dlo_2.components_)\n", "def test_sparse_encode_positive_code():\n    n_samples, n_features = 10, 8\n    X = rng_global.randn(n_samples, n_features)\n    n_components = 5\n    V = rng_global.randn(n_components, n_features)\n\n    code = sparse_encode(X, V, algorithm='lasso_lars', positive=True)\n\n    assert_array_equal(np.sign(code[code != 0]), np.ones_like(code[code != 0]))\n"], "sample_767": ["def test_column_transformer_sparse_threshold():\n    X_array = np.array([[1, 2, 3], [4, 5, 6]])\n\n    # sparse transformer\n    sparse_trans = SparseMatrixTrans()\n\n    # create column transformer with sparse transformer and threshold\n    ct = ColumnTransformer(\n        [('sparse', sparse_trans, [0])],\n        remainder='passthrough',\n        sparse_threshold=0.5\n    )\n\n    # fit-transform the column transformer\n    X_ct = ct.fit_transform(X_array)\n\n    # check output is sparse\n    assert sparse.issparse(X_ct)\n\n    # create column transformer with higher threshold\n    ct = ColumnTransformer(\n        [('sparse', sparse_trans, [0])],\n        remainder='passthrough',\n        sparse_threshold=1.0\n    )\n\n    # fit-transform the column transformer\n    X_ct = ct.fit_transform(X_array)\n\n    # check output is dense\n    assert not sparse.issparse(X_ct)\n", "def test_column_transformer_get_feature_names():\n    columns = ['a', 'b', 'c']\n    X = pd.DataFrame({\n        'a': [1, 2],\n        'b': [3, 4],\n        'c': [5, 6]\n    })\n\n    ohe = OneHotEncoder()\n    ct = ColumnTransformer([(\"ohe\", ohe, ['a'])], remainder='passthrough')\n\n    with pytest.raises(NotImplementedError):\n        ct.fit(X).get_feature_names()\n", "def test_column_transformer_get_params():\n    # check if we can get the parameters of a ColumnTransformer\n    columns = ['col1', 'col2']\n    transformers = [('trans1', Trans(), columns)]\n    column_transformer = ColumnTransformer(transformers)\n\n    params = column_transformer.get_params()\n    assert 'transformers' in params\n    assert len(params['transformers']) == 1\n    assert params['transformers'][0][0] == 'trans1'\n    assert isinstance(params['transformers'][0][1], Trans)\n    assert params['transformers'][0][2] == columns\n", "def test_column_transformer_feature_names():\n    # Test that feature names are correctly obtained from transformers\n    X = np.array([[1, 2, 3], [4, 5, 6]])\n\n    trans1 = StandardScaler()\n    trans2 = OneHotEncoder()\n\n    ct = ColumnTransformer([\n        ('trans1', trans1, [0, 1]),\n        ('trans2', trans2, [2])\n    ])\n\n    ct.fit(X)\n\n    assert ct.get_feature_names() == ['trans1__x0', 'trans1__x1', 'trans2__x0_3']\n", "def test_column_transformer_remainder_passthrough_sparse():\n    X_array = np.array([[1, 2], [3, 4]])\n    X_sparse = sparse.csr_matrix(X_array)\n    X_list = [[1, 2], [3, 4]]\n    transformer = ColumnTransformer(\n        [(\"trans\", Trans(), [0])],\n        remainder=\"passthrough\"\n    )\n    \n    X_array_transformed = transformer.fit_transform(X_array)\n    X_sparse_transformed = transformer.fit_transform(X_sparse)\n    X_list_transformed = transformer.fit_transform(X_list)\n    \n    assert_allclose_dense_sparse(X_array_transformed, X_sparse_transformed)\n    assert_array_equal(X_array_transformed, X_list_transformed)\n"], "sample_768": ["def test_validate_shuffle_split_init():\n    # Test that _validate_shuffle_split_init raises error for invalid input\n    assert_raises(ValueError, _validate_shuffle_split_init, 0.6, 0.5)\n    assert_raises(ValueError, _validate_shuffle_split_init, 0.4, 1.1)\n    assert_raises(ValueError, _validate_shuffle_split_init, -0.2, None)\n    assert_raises(ValueError, _validate_shuffle_split_init, 1.2, None)\n    assert_raises(ValueError, _validate_shuffle_split_init, 'a', None)\n    assert_raises(ValueError, _validate_shuffle_split_init, None, 'b')\n", "def test_validate_shuffle_split_init():\n    with assert_raises_regexp(ValueError, 'test_size and train_size can not '\n                                     'both be None'):\n        _validate_shuffle_split_init(test_size=None, train_size=None)\n\n    with assert_raises_regexp(ValueError, 'test_size=0 should be either '\n                                     'positive and smaller than the number '\n                                     'of samples 10 or a float in the '\n                                     '(0,1) range'):\n        _validate_shuffle_split_init(test_size=0, train_size=None)\n\n    with assert_raises_regexp(ValueError, 'Invalid value for test_size: '\n                                     \"'abc'\"):\n        _validate_shuffle_split_init(test_size='abc', train_size=None)\n\n    with assert_raises_regexp(ValueError, 'Invalid value for train_size: '\n                                     \"'abc'\"):\n        _validate_shuffle_split_init(test_size=None, train_size='abc')\n\n    with assert_raises_regexp(ValueError, 'train_size=0 should be either '\n                                     'positive and smaller than the number '\n                                     'of samples 10 or a float in the '\n                                     '(0,1) range'):\n        _validate_shuffle_split_init(test_size=None, train_size=0)\n\n    with assert_raises_regexp(ValueError, 'The sum of test_size and '\n                                     'train_size = 1.2, should be in the '\n                                     '(0, 1) range. Reduce test_size '\n                                     'and/or train_size'):\n        _validate_shuffle_split_init(test_size=0.7, train_size=0.5)\n", "def test_cross_validator_repr():\n    # Test the repr of the cross-validator object\n    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n    assert_equal(_build_repr(cv),\n                 \"KFold(n_splits=5, random_state=42, shuffle=True)\")\n", "def test_shuffle_split_init():\n    # Check that the parameters are set correctly\n    ss = ShuffleSplit(n_splits=5, test_size=0.3, train_size=0.6, random_state=42)\n    assert ss.n_splits == 5\n    assert ss.test_size == 0.3\n    assert ss.train_size == 0.6\n    assert ss.random_state == 42\n\n    # Check that the parameters are set correctly when using default values\n    ss = ShuffleSplit(n_splits=5)\n    assert ss.n_splits == 5\n    assert ss.test_size == 'default'\n    assert ss.train_size is None\n    assert ss.random_state is None\n\n    # Check that an error is raised when test_size and train_size are both None\n    with pytest.raises(ValueError):\n        ShuffleSplit(n_splits=5, test_size=None, train_size=None)\n\n    # Check that an error is raised when test_size is not a float or int\n    with pytest.raises(ValueError):\n        ShuffleSplit(n_splits=5, test_size='invalid')\n\n    # Check that an error is raised when train_size is not a float or int\n    with pytest.raises(ValueError):\n        ShuffleSplit(n_splits=5, train_size='invalid')\n", "def test_train_test_split_no_shuffle():\n    # Test that train_test_split with shuffle=False preserves the order of\n    # indices when splitting.\n    X = np.arange(10)\n    y = np.arange(10)\n\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42, shuffle=False)\n\n    assert_array_equal(X_train, [0, 1, 2, 3, 4, 5, 6, 7])\n    assert_array_equal(X_test, [8, 9])\n    assert_array_equal(y_train, [0, 1, 2, 3, 4, 5, 6, 7])\n    assert_array_equal(y_test, [8, 9])\n"], "sample_769": ["def test_precision_recall_fscore_support_errors():\n    y_true, y_pred, _ = make_prediction(binary=True)\n\n    # Bad beta\n    assert_raises(ValueError, precision_recall_fscore_support,\n                  y_true, y_pred, beta=0.0)\n\n    # Bad average option\n    assert_raises(ValueError, precision_recall_fscore_support,\n                  [1, 2, 2, 2], [1, 1, 2, 2], average=\"invalid\")\n\n    # Labels are not consecutive integers; no error should occur\n    assert_no_warnings(precision_recall_fscore_support,\n                       [1, 3, 4, 5], [1, 1, 4, 4])\n", "def test_matthews_corrcoef():\n    # Test for matthews_corrcoef\n    y_true = np.array([0, 1, 0, 1, 0, 1])\n    y_pred = np.array([0, 1, 1, 0, 0, 1])\n    assert_almost_equal(matthews_corrcoef(y_true, y_pred), 0.258198889747161)\n", "def test_classification_report_multiclass():\n    y_true = [0, 1, 2, 2, 0]\n    y_pred = [0, 0, 2, 1, 0]\n    report = classification_report(y_true, y_pred)\n    assert_equal(len(report.split('\\n')), 7)\n", "def test_precision_recall_fscore_support_multiclass():\n    y_true = np.array([0, 1, 2, 0, 1, 2])\n    y_pred = np.array([0, 2, 1, 0, 0, 1])\n\n    # compute scores with default labels introspection\n    p, r, f, s = precision_recall_fscore_support(y_true, y_pred)\n\n    # compute scores with explicit labels\n    labels = np.unique(y_true)\n    p_explicit, r_explicit, f_explicit, s_explicit = precision_recall_fscore_support(\n        y_true, y_pred, labels=labels)\n\n    assert_array_equal(p, p_explicit)\n    assert_array_equal(r, r_explicit)\n    assert_array_equal(f, f_explicit)\n    assert_array_equal(s, s_explicit)\n", "def test_precision_recall_fscore_support_binary_averages():\n    # Test averages for precision, recall, F-score and support in the binary\n    # case.\n    y_true = np.array([0, 1, 1, 0, 1, 0, 1, 1, 0, 1])\n    y_pred = np.array([0, 1, 1, 0, 0, 1, 1, 1, 0, 1])\n\n    # reference values for precision, recall and F-score\n    p, r, f, s = precision_recall_fscore_support(y_true, y_pred,\n                                                 average=None)\n\n    # compute weighted averages with different sample weights\n    weights = [0.3, 0.3, 0.4]  # sum = 1\n    p_w, r_w, f_w, s_w = precision_recall_fscore_support(\n        y_true, y_pred, average='weighted', sample_weight=np.repeat(weights,\n                                                                    [4, 3, 3]))\n\n    assert_array_almost_equal([p[1], r[1], f[1]], [p_w, r_w, f_w], 2)\n    assert_almost_equal(s[1], s_w)\n\n    weights = [0.6, 0.4]  # sum = 1\n    p_w, r_w, f_w, s_w = precision_recall_fscore_support(\n        y_true, y_pred, average='weighted', sample_weight=np.repeat(weights,\n                                                                    [5, 5]))\n\n    assert_array_almost_equal([p[1], r[1], f[1]], [p_w, r_w, f_w], 2)\n    assert_almost_equal(s[1], s_w)\n"], "sample_770": ["def test_silhouette_score_sparse_input():\n    # Test that silhouette score works with sparse input\n    X, y = datasets.make_blobs(n_samples=50, n_features=2, centers=3,\n                               random_state=0)\n    X_sparse = csr_matrix(X)\n    score_dense = silhouette_score(X, y)\n    score_sparse = silhouette_score(X_sparse, y)\n    assert_equal(score_dense, score_sparse)\n", "def test_davies_bouldin_score_perfect_labeling():\n    # Test that the Davies-Bouldin score is zero for a perfect labeling\n    X = np.array([[1, 2], [1, 4], [1, 0], [10, 2], [10, 4], [10, 0]])\n    labels = np.array([1, 1, 1, 2, 2, 2])\n    assert_equal(davies_bouldin_score(X, labels), 0.0)\n", "def test_silhouette_score_precomputed():\n    # Test silhouette score using precomputed distances\n    X, y = datasets.make_blobs(n_samples=50, n_features=2,\n                               centers=[[0.5, 0.5], [0.9, 0.9]], cluster_std=0.5,\n                               random_state=42)\n    D = pairwise_distances(X)\n    score = silhouette_score(D, y, metric='precomputed')\n    assert_greater(score, 0.5)\n    assert_less(score, 1.0)\n", "def test_silhouette_score_non_compact_clusters():\n    # Test that the silhouette score is close to 0 when clusters are not compact.\n    # This happens when the distance between clusters is small compared to the\n    # average distance within a cluster.\n\n    # Create a dataset where clusters are very close to each other\n    np.random.seed(0)\n    n_samples = 100\n    n_features = 2\n    centers = [[0, 0], [0.5, 0.5]]\n    X, labels = datasets.make_blobs(n_samples=n_samples,\n                                    centers=centers,\n                                    n_features=n_features,\n                                    cluster_std=1.0,\n                                    random_state=42)\n\n    score = silhouette_score(X, labels)\n\n    # Assert that the silhouette score is close to 0\n    assert_greater(-1, score)\n    assert_greater(score, -0.2)\n", "def test_silhouette_score_non_finite_values():\n    # Test silhouette_score with non-finite values\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    labels = np.array([0, 0, 1])\n    \n    # Introduce a NaN value in the data\n    X[1, 0] = np.nan\n    \n    assert_warns_message(UserWarning, \"Array 'X' contains non-finite values.\",\n                         silhouette_score, X, labels)\n    \n    # Introduce an infinite value in the data\n    X[1, 0] = np.inf\n    \n    assert_warns_message(UserWarning, \"Array 'X' contains non-finite values.\",\n                         silhouette_score, X, labels)\n"], "sample_771": ["def test_power_transformer_lambdas():\n    # Test that the lambda values are correctly computed and applied.\n\n    X = np.array([1, 2, 3, 4, 5])\n\n    pt = PowerTransformer(method='yeo-johnson')\n    pt.fit(X)\n\n    lmbda = pt.lambdas_[0]\n    assert lmbda > 0\n\n    # Apply the transformation with the fitted lambda\n    X_trans = pt.transform(X)\n\n    # Apply the inverse transformation to get back the original data\n    X_inv = pt.inverse_transform(X_trans)\n\n    assert_array_almost_equal(X, X_inv)\n", "def test_power_transformer_zeros():\n    # Test that PowerTransformer handles zero values\n    X = np.array([[1, 2], [3, 4], [0, 0]])\n    pt = PowerTransformer()\n    X_trans = pt.fit_transform(X)\n    assert_array_less(np.abs(X_trans[:, 0].mean()), 1e-10)\n    assert_array_less(np.abs(X_trans[:, 1].mean()), 1e-10)\n    assert_array_equal(pt.lambdas_, [0, 0])\n", "def test_power_transformer_check_lambdas():\n    # Check that the lambda values are within the expected range\n    pt = PowerTransformer(method='yeo-johnson')\n    X = np.array([[1, 2], [3, 2], [4, 5]])\n    pt.fit(X)\n    assert_array_less(pt.lambdas_, 2)\n    assert_array_greater_equal(pt.lambdas_, -2)\n\n    pt = PowerTransformer(method='box-cox')\n    X = np.array([[1, 2], [3, 2], [4, 5]])\n    pt.fit(X)\n    assert_array_less(pt.lambdas_, 2)\n    assert_array_greater_equal(pt.lambdas_, -2)\n", "def test_power_transformer():\n    # Test PowerTransformer on a simple dataset\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    pt = PowerTransformer(method='yeo-johnson', standardize=False)\n    Xt = pt.fit_transform(X)\n\n    assert_equal(pt.lambdas_.shape[0], X.shape[1])\n    assert_array_almost_equal(pt.inverse_transform(Xt), X)\n\n    pt = PowerTransformer(method='box-cox', standardize=False)\n    Xt = pt.fit_transform(X + 10)  # shift to ensure all values are positive\n\n    assert_equal(pt.lambdas_.shape[0], X.shape[1])\n    assert_array_almost_equal(pt.inverse_transform(Xt), X + 10)\n", "def test_power_transformer_zero_variance():\n    # Test that PowerTransformer handles zero-variance features correctly\n\n    X = np.random.RandomState(0).normal(size=(100, 3))\n    X[:, 1] = 1  # one feature with zero variance\n\n    pt = PowerTransformer()\n    X_trans = pt.fit_transform(X)\n\n    assert_array_equal(pt.lambdas_, np.array([1., np.nan, 1.]))\n\n    # non-regression test for #12001\n    assert not np.isnan(X_trans).any()\n"], "sample_772": ["def check_classificationiris(name):\n    \"\"\"Check classification on the iris dataset.\"\"\"\n    ForestClassifier = FOREST_CLASSIFIERS[name]\n\n    # Test permutation invariance\n    clf = ForestClassifier(n_estimators=100, random_state=0)\n    clf.fit(iris.data, iris.target)\n    assert_array_equal(clf.predict(iris.data), iris.target)\n\n    # Test feature importances\n    clf = ForestClassifier(n_estimators=100, random_state=0)\n    clf.fit(X_large, y_large)\n    assert_almost_equal(sum(clf.feature_importances_), 1)\n\n    # Check on a random subset of the iris set that we can get 100% accuracy\n    # with more estimators\n    rng = np.random.RandomState(0)\n    idx = rng.choice(range(iris.data.shape[0]), size=25, replace=False)\n    X_subset = iris.data[idx]\n    y_subset = iris.target[idx]\n    clf = ForestClassifier(n_estimators=500, random_state=0)\n    clf.fit(X_subset, y_subset)\n    assert_array_equal(clf.predict(X_subset), y_subset)\n\n    # Check that oob_score=True gives good results\n    clf = ForestClassifier(n_estimators=100, oob_score=True, random_state=0)\n    clf.fit(iris.data, iris.target)\n    assert_greater(clf.oob_score_, 0.8)\n", "def check_parallel_classification(name):\n    \"\"\"Check classification on a dataset with multiple cores.\"\"\"\n    ForestClassifier = FOREST_CLASSIFIERS[name]\n\n    X, y = datasets.make_classification(n_samples=100, n_features=10,\n                                        n_informative=3, n_redundant=0,\n                                        random_state=42)\n\n    clf = ForestClassifier(n_estimators=10, n_jobs=-1, random_state=42)\n    clf.fit(X, y)\n    assert_array_equal(clf.predict(X), clf.predict(X))\n\n    clf = ForestClassifier(n_estimators=10, n_jobs=2, random_state=42)\n    clf.fit(X, y)\n    assert_array_equal(clf.predict(X), clf.predict(X))\n", "def check_parallel_pickle(name):\n    \"\"\"Check parallel training and pickling.\"\"\"\n    ForestEstimator = FOREST_ESTIMATORS[name]\n\n    # Create an estimator and fit it\n    est = ForestEstimator(n_estimators=10, n_jobs=2, random_state=1)\n    if name in FOREST_TRANSFORMERS:\n        est.fit(X)\n    elif name in FOREST_CLASSIFIERS:\n        est.fit(X, y)\n    else:\n        est.fit(X, np.array([1.0] * len(X)))\n\n    # Make sure estimators are stored in public estimators_ attribute\n    assert hasattr(est, 'estimators_')\n\n    # Check for memory bloat during training by making sure that\n    # the estimator size does not increase after a second fit.\n    initial_size = sys.getsizeof(est)\n    est.fit(X, y)\n    assert sys.getsizeof(est) <= 3 * initial_size\n\n    # pickle and unpickle\n    est_pickled = pickle.loads(pickle.dumps(est))\n\n    # Check for deterministic oob_score\n    if name in FOREST_CLASSIFIERS_REGRESSORS:\n        est2 = ForestEstimator(n_estimators=10, n_jobs=2, random_state=1,\n                               oob_score=True)\n        if name in FOREST_CLASSIFIERS:\n            est2.fit(X, y)\n        else:\n            est2.fit(X, np.array([1.0] * len(X)))\n        oob_error_before = est2.oob_score_\n        est2_pickled = pickle.loads(pickle.dumps(est2))\n        oob_error_after = est2_pickled.oob_score_\n        assert_almost_equal(oob_error_before, oob_error_after)\n\n    # Check that the port do not change after pickling\n    if LooseVersion(__joblib_version__) >= LooseVersion('0.12'):\n        for i, (e1, e2) in enumerate(zip(est.estimators_, est_pickled.estimators_)):\n            assert e1.n_features_ == e2.n_features_, (\n                \"Forest was not correctly pickled: mismatch at estimator {} \"\n                \"(before: {}, after: {})\".format(i, e1.n_features_,\n                                                 e2.n_features_))\n\n    # Check that prediction works on the unpickled object\n    assert_array_equal(est.predict(T), est_pickled.predict(T))\n", "def check_parallel(name):\n    \"\"\"Check parallel computations in classification and regression\"\"\"\n    ForestEstimator = FOREST_ESTIMATORS[name]\n\n    if name in FOREST_CLASSIFIERS:\n        estimator = ForestClassifier(n_estimators=10, n_jobs=-1, random_state=0)\n        X, y = iris.data, iris.target\n    elif name in FOREST_REGRESSORS:\n        estimator = ForestRegressor(n_estimators=10, n_jobs=-1, random_state=0)\n        X, y = boston.data, boston.target\n    else:\n        return\n\n    with ignore_warnings():\n        # Ignore joblib's DeprecationWarning about unpickling workers\n        estimator.fit(X, y)\n\n    # Check that fitting completed correctly by making predictions\n    estimator.predict(X)\n\n    # Checking for feature importances with parallelism\n    estimator.feature_importances_\n\n    # Checking for oob_score with parallelism\n    if name not in FOREST_TRANSFORMERS:\n        estimator.set_params(oob_score=True)\n        estimator.fit(X, y)\n        estimator.oob_score_\n        if name in FOREST_CLASSIFIERS:\n            estimator.oob_decision_function_\n", "def check_classification_probability(name):\n    \"\"\"Check classification probabilities on a toy dataset.\"\"\"\n    ForestClassifier = FOREST_CLASSIFIERS[name]\n\n    clf = ForestClassifier(n_estimators=10, random_state=1)\n    clf.fit(X, y)\n\n    # Check that predict_proba and predict agree.\n    y_pred = clf.predict(X)\n    y_pred_proba = clf.predict_proba(X)\n    assert_array_equal(np.argmax(y_pred_proba, axis=1), y_pred)\n\n    # Check that predict_proba for all classes for one sample is close to 1.\n    assert_almost_equal(np.sum(y_pred_proba, axis=1), np.ones(len(X)))\n\n    # Check that predict_proba works with sparse input\n    if SPARSE_SPLITTERS:\n        y_pred_proba_sparse = clf.predict_proba(csr_matrix(X))\n        assert_array_almost_equal(y_pred_proba, y_pred_proba_sparse)\n"], "sample_773": ["def test_multinomial_grad_hess():\n    # Test that the gradient and Hessian of the multinomial log loss is\n    # correct.\n    X = np.array([[1, 2], [3, 4]])\n    Y = np.array([[0, 1], [1, 0]])  # one-hot encoding\n    w = np.array([1, 2])  # coefficients\n\n    grad, hessp = _multinomial_grad_hess(w, X, Y, alpha=0.5)\n\n    # compute expected gradient and Hessian using finite differences\n    eps = 1e-6\n    grad_approx = np.zeros_like(grad)\n    for i in range(len(w)):\n        w_plus_eps = w.copy()\n        w_plus_eps[i] += eps\n        w_minus_eps = w.copy()\n        w_minus_eps[i] -= eps\n        loss_plus_eps, _, _ = _multinomial_loss(w_plus_eps, X, Y, alpha=0.5)\n        loss_minus_eps, _, _ = _multinomial_loss(w_minus_eps, X, Y, alpha=0.5)\n        grad_approx[i] = (loss_plus_eps - loss_minus_eps) / (2 * eps)\n\n    assert_array_almost_equal(grad, grad_approx)\n\n    # check Hessian by applying it to a random vector\n    v = np.random.rand(len(w))\n    hessp_v = hessp(v)\n    hessp_approx = np.zeros_like(hessp_v)\n    for i in range(len(w)):\n        w_plus_eps = w.copy()\n        w_plus_eps[i] += eps\n        w_minus_eps = w.copy()\n        w_minus_eps[i] -= eps\n        _, grad_plus_eps, _ = _multinomial_loss_and_grad(w_plus_eps, X, Y, alpha=0.5)\n        _, grad_minus_eps, _ = _multinomial_loss_and_grad(w_minus_eps, X, Y, alpha=0.5)\n        hessp_approx[i] = (grad_plus_eps[i] - grad_minus_eps[i]) / (2 * eps) * v[i]\n\n    assert_array_almost_equal(hessp_v, hessp_approx)\n", "def test_logistic_regression_solvers():\n    \"\"\"Check that all solvers converge to similar solutions\"\"\"\n    X, y = make_classification(n_samples=100, n_features=5, n_informative=3,\n                               random_state=0)\n    X = scale(X)\n\n    # LBFGS is the most stable solver and has a good convergence rate, use it\n    # as a reference.\n    clf_ref = LogisticRegression(solver='lbfgs', max_iter=1000, tol=1e-6,\n                                 random_state=42, fit_intercept=False)\n    clf_ref.fit(X, y)\n    coef_ref = clf_ref.coef_\n\n    for solver in ['newton-cg', 'liblinear', 'sag', 'saga']:\n        clf = LogisticRegression(solver=solver, max_iter=1000, tol=1e-6,\n                                 random_state=42, fit_intercept=False)\n        with ignore_warnings(category=ConvergenceWarning):\n            clf.fit(X, y)\n        assert_allclose(clf.coef_, coef_ref, atol=1e-4, rtol=1e-3)\n", "def test_logistic_regression_cv_saga_solver():\n    # Test that the 'saga' solver is chosen when the number of samples\n    # exceeds the number of features in the dataset.\n    X, y = make_classification(n_samples=20, n_features=10, random_state=0)\n    clf = LogisticRegressionCV(solver='lbfgs', max_iter=1000)\n    assert_warns_message(ConvergenceWarning,\n                         \"lbfgs failed to converge\",\n                         clf.fit, X, y)\n\n    # With the 'saga' solver, there should be no convergence warning.\n    clf = LogisticRegressionCV(solver='saga', max_iter=1000)\n    assert_no_warnings(clf.fit, X, y)\n", "def test_multiclass_logistic_regression_path():\n    \"\"\"Check that logistic_regression_path gives the same result as\n    LogisticRegression with multi_class='multinomial'\"\"\"\n    X, y = load_iris(return_X_y=True)\n    Cs = [0.1, 1]\n    multiclass = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n    coefs, Cs, n_iter = _logistic_regression_path(X, y, Cs=Cs,\n                                                  multi_class='multinomial',\n                                                  solver='lbfgs')\n    for C, n in zip(Cs, n_iter):\n        multiclass.set_params(C=C)\n        multiclass.fit(X, y)\n        assert_array_almost_equal(coefs[:, n], multiclass.coef_.ravel())\n", "def test_multinomial_loss_object():\n    # Test multinomial loss object function with 3 classes.\n    X = [[-1, 0], [0, 1], [1, 1]]\n    Y = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\n\n    # First check with a classifier that has coef_ and intercept_\n    clf = LogisticRegression(fit_intercept=True, multi_class='multinomial',\n                             solver='lbfgs')\n    clf.fit(X, np.argmax(Y, axis=1))\n    loss, p, w = _multinomial_loss(clf.coef_.ravel(), X, Y, 0.5)\n    assert_almost_equal(loss, log_loss(Y, p))\n"], "sample_774": ["def test_ordinal_encoder_inverse_transform():\n    # Test inverse_transform method of OrdinalEncoder\n    X = [['Male', 1], ['Female', 3], ['Female', 2]]\n    enc = OrdinalEncoder()\n    X_trans = enc.fit_transform(X)\n    assert_array_equal(enc.inverse_transform(X_trans), X)\n\n    # Test inverse_transform with a single feature\n    X = ['Male', 'Female', 'Female']\n    enc = OrdinalEncoder()\n    X_trans = enc.fit_transform(np.array(X).reshape(-1, 1))\n    assert_array_equal(enc.inverse_transform(X_trans), np.array(X).reshape(-1, 1))\n", "def test_ordinal_encoder_handle_unknown():\n    # Test OrdinalEncoder's handle_unknown parameter\n\n    X = np.array([[1, 'a'], [2, 'b'], [3, 'c']])\n    enc = OrdinalEncoder(categories=[[1, 2], ['a', 'b', 'c']])\n\n    # Test that an error is raised when an unknown value is encountered\n    with pytest.raises(ValueError):\n        enc.fit_transform(np.array([[4, 'd']]))\n\n    # Test that no error is raised when handle_unknown='ignore'\n    enc.handle_unknown = 'ignore'\n    transformed = enc.fit_transform(np.array([[4, 'd']]))\n    assert_array_equal(toarray(transformed), np.array([[-1, -1]]))\n", "def test_one_hot_encoder_handle_unknown():\n    # Test the handle_unknown parameter of OneHotEncoder\n    X = np.array([[1, 2], [3, 4]])\n    ohe = OneHotEncoder(handle_unknown='error')\n    ohe.fit(X)\n\n    # Test that an error is raised when an unknown category is encountered\n    X_test = np.array([[5, 6]])\n    assert_raises(ValueError, ohe.transform, X_test)\n\n    # Test that no error is raised when an unknown category is encountered\n    # and handle_unknown is set to 'ignore'\n    ohe = OneHotEncoder(handle_unknown='ignore')\n    ohe.fit(X)\n    X_test = np.array([[5, 6]])\n    X_transformed = ohe.transform(X_test)\n    assert_array_equal(toarray(X_transformed), np.zeros((1, 4)))\n", "def test_one_hot_encoder_inverse_transform():\n    # Test that OneHotEncoder.inverse_transform works with dropped categories\n    X = np.array([[1, 2], [3, 4]])\n    enc = OneHotEncoder(drop='first')\n    X_enc = enc.fit_transform(X)\n    assert_array_equal(enc.inverse_transform(toarray(X_enc)), X)\n\n    # Test that OneHotEncoder.inverse_transform works with multiple dropped categories\n    X = np.array([[1, 2, 3], [4, 5, 6]])\n    drop = np.array([0, 1, 0])\n    enc = OneHotEncoder(drop=drop)\n    X_enc = enc.fit_transform(X)\n    assert_array_equal(enc.inverse_transform(toarray(X_enc)), X)\n", "def test_one_hot_encoder_handle_unknown():\n    # Test OneHotEncoder with handle_unknown='error' and 'ignore'\n    X = [['Male', 1], ['Female', 3], ['Female', 2]]\n    enc = OneHotEncoder(handle_unknown='error')\n    enc.fit(X)\n\n    # Test with unknown value in existing feature\n    X_test = [['Unknown', 1]]\n    assert_raises(ValueError, enc.transform, X_test)\n\n    # Test with unknown feature\n    X_test = [['Male', 4]]\n    assert_raises(ValueError, enc.transform, X_test)\n\n    # Test with handle_unknown='ignore'\n    enc = OneHotEncoder(handle_unknown='ignore')\n    enc.fit(X)\n    X_test = [['Unknown', 1]]\n    result = toarray(enc.transform(X_test))\n    expected = np.array([[0., 0., 0., 0.]])\n    assert_array_equal(result, expected)\n"], "sample_775": ["def test_estimator_repr_n_max_elements_to_show():\n    # Test the n_max_elements_to_show parameter of _EstimatorPrettyPrinter\n    data = [[1, 2], [3, 4], [5, 6]]\n    estimator = LogisticRegression(max_iter=1000)\n    selector = RFE(estimator, 1)\n    pipeline = make_pipeline(StandardScaler(), PCA(n_components=2), selector)\n\n    # Set n_max_elements_to_show to a small value\n    set_config(print_changed_only=False)\n    printer = _EstimatorPrettyPrinter(n_max_elements_to_show=2)\n    repr_str = printer.pformat(pipeline)\n    assert '...' in repr_str\n\n    # Set n_max_elements_to_show to a large value\n    printer = _EstimatorPrettyPrinter(n_max_elements_to_show=10)\n    repr_str = printer.pformat(pipeline)\n    assert '...' not in repr_str\n\n    set_config(print_changed_only=True)\n    printer = _EstimatorPrettyPrinter(n_max_elements_to_show=2)\n    repr_str = printer.pformat(pipeline)\n    assert '...' in repr_str\n", "def test_estimator_repr_n_max_elements_to_show():\n    # Test that the n_max_elements_to_show parameter is taken into account\n    data = [[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]\n    pca = PCA(n_components=2)\n    pca.fit(data)\n\n    # default: show all components\n    repr_pca = _EstimatorPrettyPrinter(n_max_elements_to_show=None).pformat(pca)\n    assert \"components=\" in repr_pca\n\n    # show only 1 component\n    repr_pca = _EstimatorPrettyPrinter(n_max_elements_to_show=1).pformat(pca)\n    assert \"components=[[...\" in repr_pca\n", "def test_changed_only():\n    # Test that the 'changed_only' parameter works as expected\n    estimator = LogisticRegression(max_iter=1000)\n    printer = _EstimatorPrettyPrinter(changed_only=True)\n    representation = printer.pformat(estimator)\n    assert \"max_iter=1000\" in representation\n\n    # Default max_iter is 100, so it shouldn't be included\n    estimator = LogisticRegression()\n    representation = printer.pformat(estimator)\n    assert \"max_iter\" not in representation\n", "def test_n_max_elements_to_show():\n    # Test that the n_max_elements_to_show parameter is taken into account\n    data = [1, 2, 3, 4, 5]\n    expected_repr = \"[1, 2, 3, ...]\"\n    \n    printer = _EstimatorPrettyPrinter(n_max_elements_to_show=3)\n    assert printer.pformat(data) == expected_repr\n\n    printer = _EstimatorPrettyPrinter(n_max_elements_to_show=None)\n    assert printer.pformat(data) == \"[1, 2, 3, 4, 5]\"\n\n    # Also test for dictionaries\n    data = {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5}\n    expected_repr = \"{'a': 1, 'b': 2, 'c': 3, ...}\"\n    \n    printer = _EstimatorPrettyPrinter(n_max_elements_to_show=3)\n    assert printer.pformat(data) == expected_repr\n\n    printer = _EstimatorPrettyPrinter(n_max_elements_to_show=None)\n    assert printer.pformat(data) == \"{'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5}\"\n", "def test_pprint_estimator():\n    # Test that _EstimatorPrettyPrinter formats estimators as expected\n\n    printer = _EstimatorPrettyPrinter()\n    estimator = LogisticRegression(penalty='l2', C=1.0)\n    expected = re.escape(\"LogisticRegression(C=1.0, penalty='l2')\")\n    assert re.match(expected, printer.pformat(estimator))\n"], "sample_776": ["def test_lars_path_positive():\n    # Test that Lars with positive=True gives a solution where\n    # all non-zero coefficients are positive.\n\n    # Create a random dataset with two features and 1000 samples\n    X = np.random.rand(1000, 2)\n    y = np.random.rand(1000)\n\n    _, active, coefs = lars_path(X, y, method='lasso', max_iter=10,\n                                 positive=True)\n\n    # Check if all coefficients are positive or zero\n    assert np.all(coefs >= 0)\n", "def test_lars_path_positive():\n    # Test that the positive LARS path gives the same result as the normal LARS\n    # path when all coefficients are positive.\n    X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    y = np.array([10, 20, 30])\n\n    _, _, coefs = linear_model.lars_path(X, y, method='lasso', positive=False)\n    _, _, coefs_positive = linear_model.lars_path(X, y, method='lasso',\n                                                  positive=True)\n\n    assert_array_almost_equal(coefs, coefs_positive)\n", "def test_lars_path_positive():\n    # Test that Lars with positive=True gives the same result as\n    # Lars with positive=False when all coefficients are positive.\n    np.random.seed(0)\n    X = np.random.rand(10, 5)\n    y = np.random.rand(10)\n\n    alpha_max = np.max(np.abs(np.dot(X.T, y))) / 10.\n\n    _, _, coefs_positive = linear_model.lars_path(X, y, method='lasso',\n                                                  alpha_min=alpha_max,\n                                                  positive=True)\n    _, _, coefs = linear_model.lars_path(X, y, method='lasso',\n                                         alpha_min=alpha_max)\n\n    # Coefficients can be different in the last few iterations due to\n    # numerical instabilities.\n    for i in range(len(coefs) - 5):\n        assert_array_almost_equal(coefs_positive[:, i], coefs[:, i])\n", "def test_lars_path_nonuniform_grid():\n    # Test that the grid of alphas returned by lars_path is not uniform.\n    X, y = diabetes.data, diabetes.target\n    _, alphas, _ = linear_model.lars_path(X, y)\n    assert_less(np.std(np.diff(alphas)), np.std(alphas))\n", "def test_lars_larscv_non_float_y():\n    # check that LARS and LarsCV accept non-float y\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([0, 1])\n    for model in [linear_model.LassoLars(), linear_model.LarsCV()]:\n        model.fit(X, y)\n        assert_equal(model.coef_.shape, (X.shape[1],))\n"], "sample_777": ["def check_regression_toy(presort, loss):\n    # Check regression on a toy dataset.\n    clf = GradientBoostingRegressor(loss=loss, n_estimators=10,\n                                    random_state=1, presort=presort)\n\n    assert_raises(ValueError, clf.predict, T)\n\n    clf.fit(X, np.array(y).astype(np.float64))\n    pred = clf.predict(T)\n    assert_equal(pred.shape, (3,))\n    assert_equal(10, len(clf.estimators_))\n\n    deviance_decrease = (clf.train_score_[:-1] - clf.train_score_[1:])\n    if loss == 'huber':\n        # Huber loss is not necessarily decreasing\n        assert np.any(deviance_decrease >= 0.0)\n    else:\n        assert np.all(deviance_decrease >= 0.0)\n\n    leaves = clf.apply(X)\n    assert_equal(leaves.shape, (6, 10))\n", "def test_init_estimators():\n    # Test that the init estimators are set correctly\n    X, y = make_classification(n_samples=100, n_features=10)\n    clf = GradientBoostingClassifier(init='zero')\n    clf.fit(X, y)\n    assert isinstance(clf.init_, ZeroEstimator)\n\n    clf = GradientBoostingClassifier(init='dummy')\n    with pytest.raises(ValueError):\n        clf.fit(X, y)\n\n    dummy = DummyClassifier()\n    clf = GradientBoostingClassifier(init=dummy)\n    clf.fit(X, y)\n    assert clf.init_ is dummy\n\n    reg = GradientBoostingRegressor(init='zero')\n    reg.fit(X, y)\n    assert isinstance(reg.init_, ZeroEstimator)\n\n    reg = GradientBoostingRegressor(init='dummy')\n    with pytest.raises(ValueError):\n        reg.fit(X, y)\n\n    dummy = DummyRegressor()\n    reg = GradientBoostingRegressor(init=dummy)\n    reg.fit(X, y)\n    assert reg.init_ is dummy\n", "def test_classification_loss_functions():\n    # Check if loss functions return the correct value.\n    X, y = make_classification(n_samples=50, n_features=5,\n                               n_informative=3, n_redundant=0,\n                               random_state=1)\n\n    clf = GradientBoostingClassifier(n_estimators=100, random_state=1)\n\n    # compute reference score with sklearn's log loss metric\n    from sklearn.metrics import log_loss\n    probas = clf.fit(X, y).predict_proba(X)\n    reference_score = log_loss(y, probas)\n\n    # compute score with deviance loss function\n    score = clf.loss_(y, clf.decision_function(X))\n    assert_almost_equal(-score, reference_score)\n\n    # exponential loss should be exp(-margin) for classification\n    exp_loss = np.exp(-(2 * y - 1) * clf.decision_function(X)).mean()\n    clf = GradientBoostingClassifier(n_estimators=100, loss='exponential',\n                                     random_state=1)\n    score = clf.loss_(y, clf.decision_function(X))\n    assert_almost_equal(score, exp_loss)\n", "def test_gradient_boosting_regression():\n    # Check regression with GradientBoostingRegressor.\n    for presort in ['auto', True, False]:\n        clf = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1,\n                                        max_depth=3, min_samples_split=2,\n                                        min_samples_leaf=1, subsample=1.0,\n                                        max_features=None, random_state=1,\n                                        presort=presort)\n        clf.fit(boston.data, boston.target)\n        y_pred = clf.predict(boston.data)\n        mse = mean_squared_error(boston.target, y_pred)\n        assert_less(mse, 10)\n\n        leaves = clf.apply(boston.data)\n        assert_equal(leaves.shape, (506, 100))\n", "def test_classification_with_init_estimator(presort):\n    # Check classification with an init estimator.\n    clf = GradientBoostingClassifier(n_estimators=10, random_state=1,\n                                     presort=presort)\n    clf.fit(X, y)\n\n    # predict before fit used to throw an error\n    y_pred_before_fit = clf.predict(np.array([[-2, -1]]))\n    assert np.isnan(y_pred_before_fit).all()\n\n    # check that the init estimator is used for predictions if n_estimators=0\n    clf = GradientBoostingClassifier(n_estimators=0, random_state=1,\n                                     presort=presort)\n    clf.fit(X, y)\n    y_pred = clf.predict(np.array([[-2, -1]]))\n    assert np.allclose(y_pred, np.mean(y))\n\n    # check that the init estimator is used for feature importances\n    # if n_estimators=0\n    assert_array_equal(clf.feature_importances_, np.zeros(2))\n"], "sample_778": ["def test_nmf_fit_transform():\n    # Test that the fit_transform method is equivalent to fit + transform\n    X = np.abs(np.random.rand(10, 10))\n    nmf = NMF(n_components=5)\n    \n    W1 = nmf.fit_transform(X)\n    W2 = nmf.fit(X).transform(X)\n    \n    assert_array_almost_equal(W1, W2)\n", "def test_nmf_multiplicative_update():\n    # Test that the multiplicative update solver is working as expected\n    # by comparing its results with the CD solver for a simple case.\n    rng = np.random.RandomState(0)\n    X = rng.rand(10, 5)\n    W, H, _ = non_negative_factorization(X, n_components=2, solver='cd',\n                                         random_state=0)\n    W_mu, H_mu, _ = non_negative_factorization(X, n_components=2, solver='mu',\n                                               random_state=0)\n\n    assert_array_almost_equal(W, W_mu, decimal=1)\n    assert_array_almost_equal(H, H_mu, decimal=1)\n", "def test_nmf_solver_parameter():\n    X = np.abs(np.random.rand(10, 10))\n    W_ref, H_ref, _ = non_negative_factorization(X, solver='cd', random_state=0)\n    W_mul, H_mul, _ = non_negative_factorization(X, solver='mu', random_state=0)\n\n    assert_not_equal(W_ref, W_mul)\n    assert_not_equal(H_ref, H_mul)\n\n    with pytest.raises(ValueError):\n        non_negative_factorization(X, solver='unknown')\n", "def test_nmf_multiplicative_update_sparse():\n    # Test that NMF multiplicative update works with sparse data\n    X = csc_matrix(np.array([[1, 0, 0], [0, 1, 0]]))\n    W, H, _ = non_negative_factorization(X, n_components=2, solver='mu')\n    assert_array_almost_equal(X.toarray(), np.dot(W, H))\n", "def test_nmf_fit_transform_limit_iterations():\n    # Test that the fitting process stops after a limited number of iterations.\n    X = np.array([[1, 0, 0, 1, 0],\n                  [1, 1, 0, 0, 0],\n                  [0, 1, 1, 0, 1],\n                  [0, 0, 1, 1, 0]])\n\n    nmf = NMF(n_components=2, init='random', random_state=0, max_iter=10)\n    W1 = nmf.fit_transform(X)\n\n    nmf.max_iter = 5\n    W2 = nmf.fit_transform(X)\n\n    assert_less(squared_norm(W1), squared_norm(W2))\n"], "sample_779": ["def test_check_estimator_idempotent():\n    class DummyEstimator(BaseEstimator):\n            self.n_features_in_ = X.shape[1]\n            return self\n\n            return np.ones(X.shape[0])\n\n    estimator = DummyEstimator()\n    X_train = np.array([[1, 2], [3, 4]])\n    y_train = np.array([0, 1])\n    X_test = np.array([[5, 6]])\n\n    # First fit\n    estimator.fit(X_train, y_train)\n    y_pred_1 = estimator.predict(X_test)\n\n    # Second fit\n    estimator.fit(X_train, y_train)\n    y_pred_2 = estimator.predict(X_test)\n\n    assert_array_equal(y_pred_1, y_pred_2)\n", "def test_check_estimator_idempotent():\n    # Check that check_estimator is idempotent, i.e. running it twice\n    # doesn't change the result\n    class Estimator(BaseEstimator):\n            self.fitted_ = True\n            return self\n\n            check_is_fitted(self)\n            return np.zeros(X.shape[0])\n\n    estimator = Estimator()\n    check_estimator(estimator)\n    check_estimator(estimator)\n", "def test_check_fit_score_takes_y_list():\n    X = np.array([[1, 2], [3, 4]])\n    y = [0, 1]\n    estimator = LinearRegression()\n    set_checking_parameters(estimator)\n    check_fit_score_takes_y(\"LinearRegression\", estimator)\n    assert estimator.fit(X, y) is not None\n    assert estimator.score(X, y) is not None\n", "def test_check_fit_idempotent():\n    # Test that check_fit_idempotent raises no exception on good estimator.\n    X, y = make_blobs(n_samples=20, random_state=0)\n    check_fit_idempotent(\"test\", SGDClassifier(random_state=0), X, y)\n\n    # Test that check_fit_idempotent raises an exception on a bad estimator.\n    class BadEstimator(BaseEstimator):\n            self.coef_ = np.random.rand(X.shape[1])\n            return self\n\n            return np.dot(X, self.coef_)\n\n    assert_raises_regex(AssertionError,\n                        \"Estimator test should be deterministic\",\n                        check_fit_idempotent, \"test\", BadEstimator(), X, y)\n", "def test_check_estimator_sparse_data():\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([0, 1])\n    estimator = LinearRegression()\n    set_random_state(estimator)\n    check_estimator_sparse_data(\"LinearRegression\", estimator)\n    # Check with a classifier\n    estimator = SGDClassifier(max_iter=1000, tol=1e-3)\n    set_random_state(estimator)\n    check_estimator_sparse_data(\"SGDClassifier\", estimator)\n"], "sample_780": ["def test_latent_dirichlet_allocation_fit_transform():\n    # Test that fit and transform give the same result as fit_transform\n    n_components, X = _build_sparse_mtx()\n    lda = LatentDirichletAllocation(n_components=n_components,\n                                    max_iter=1, random_state=0)\n\n    X_fit_transform = lda.fit_transform(X)\n    X_fit = lda.fit(X).transform(X)\n\n    assert_array_almost_equal(X_fit_transform, X_fit)\n", "def test_latent_dirichlet_allocation_fit_transform():\n    # Test that fit_transform works and returns the same result as fit + transform\n    n_components, X = _build_sparse_mtx()\n    lda = LatentDirichletAllocation(n_components=n_components)\n    \n    # Fit and transform in two steps\n    lda.fit(X)\n    doc_topic_distr_two_steps = lda.transform(X)\n    \n    # Fit and transform in one step\n    lda_one_step = LatentDirichletAllocation(n_components=n_components)\n    doc_topic_distr_one_step = lda_one_step.fit_transform(X)\n    \n    # Check that the results are the same\n    assert_allclose(doc_topic_distr_two_steps, doc_topic_distr_one_step)\n", "def test_latent_dirichlet_allocation_fit_transform():\n    # Test fit and transform methods of LatentDirichletAllocation\n    n_components, X = _build_sparse_mtx()\n    lda = LatentDirichletAllocation(n_components=n_components, random_state=0)\n    X_transformed = lda.fit_transform(X)\n\n    assert_equal(X_transformed.shape[1], n_components)\n    assert_greater_equal(X_transformed.sum(axis=1).min(), 0.99)\n    assert_array_almost_equal(X_transformed.sum(axis=1), np.ones(X.shape[0]))\n", "def test_lda_partial_fit():\n    n_components, X = _build_sparse_mtx()\n    lda = LatentDirichletAllocation(n_components=n_components,\n                                    learning_method='online',\n                                    random_state=0)\n    # Fit on 2 samples\n    lda.partial_fit(X[:2])\n    # Continue fitting on the next 2 samples\n    lda.partial_fit(X[2:4])\n    assert_equal(lda.n_batch_iter_, 4)  # 2 batches of 2 samples each\n", "def test_lda_fit_transform():\n    # Test that fit_transform works and returns the correct shape\n    n_components, X = _build_sparse_mtx()\n    lda = LatentDirichletAllocation(n_components=n_components)\n    doc_topic_distr = lda.fit_transform(X)\n    assert_equal(doc_topic_distr.shape, (X.shape[0], n_components))\n"], "sample_781": ["def check_classification_max_features(name):\n    \"\"\"Check classification with different max features.\"\"\"\n    ForestClassifier = FOREST_CLASSIFIERS[name]\n\n    # Test that an error is raised if max_features is not in (0, n_features]\n    clf = ForestClassifier(max_features=0)\n    assert_raises(ValueError, clf.fit, X, y)\n\n    # Test that an error is raised if max_features is not an int or a float\n    clf = ForestClassifier(max_features='not_an_int')\n    assert_raises(ValueError, clf.fit, X, y)\n\n    # Test with max_features as an int\n    clf = ForestClassifier(max_features=1)\n    clf.fit(X, y)\n    assert_array_equal(clf.predict(T), true_result)\n\n    # Test with max_features as a float\n    clf = ForestClassifier(max_features=0.5)\n    clf.fit(X, y)\n    assert_array_equal(clf.predict(T), true_result)\n\n    # Test with max_features as 'auto'\n    clf = ForestClassifier(max_features='auto')\n    clf.fit(X, y)\n    assert_array_equal(clf.predict(T), true_result)\n\n    # Test with max_features as 'sqrt'\n    clf = ForestClassifier(max_features='sqrt')\n    clf.fit(X, y)\n    assert_array_equal(clf.predict(T), true_result)\n\n    # Test with max_features as 'log2'\n    clf = ForestClassifier(max_features='log2')\n    clf.fit(X, y)\n    assert_array_equal(clf.predict(T), true_result)\n\n    # Test with max_features as None\n    clf = ForestClassifier(max_features=None)\n    clf.fit(X, y)\n    assert_array_equal(clf.predict(T), true_result)\n", "def check_parallel_pickle(name):\n    \"\"\"Check pickling of forests in parallel.\"\"\"\n    ForestEstimator = FOREST_ESTIMATORS[name]\n\n    with joblib.parallel_backend('threading', n_jobs=2):\n        est = ForestEstimator(n_estimators=5, random_state=0)\n        est.fit(iris.data, iris.target)\n\n        # check that the forest is operational after pickling\n        est_pickled = pickle.loads(pickle.dumps(est))\n        assert_array_equal(est.predict(iris.data), est_pickled.predict(iris.data))\n\n        # check that the forest can be pickled again\n        pickle.dumps(est_pickled)\n\n    # Check that we can still pickle even if the backend changes\n    if name == \"RandomForestClassifier\":\n        with joblib.parallel_backend('loky', n_jobs=2):\n            est = ForestEstimator(n_estimators=5, random_state=0)\n            est.fit(iris.data, iris.target)\n            pickle.dumps(est)\n", "def check_classification_sparse(name):\n    \"\"\"Check classification on a sparse dataset.\"\"\"\n    ForestClassifier = FOREST_CLASSIFIERS[name]\n\n    # Create a sparse matrix with the iris dataset\n    iris_sparse = csr_matrix(iris.data)\n\n    clf = ForestClassifier(n_estimators=10, random_state=1)\n    clf.fit(iris_sparse, iris.target)\n\n    # Check that the classifier can predict on both sparse and dense inputs\n    assert_array_equal(clf.predict(iris_sparse), clf.predict(iris.data))\n\n    # Check that feature importances have been computed correctly\n    assert_array_almost_equal(\n        np.sum(clf.feature_importances_), 1.0, decimal=3)\n", "def check_importances(name):\n    \"\"\"Check feature importances.\"\"\"\n    ForestEstimator = FOREST_ESTIMATORS[name]\n\n    X, y = datasets.make_classification(\n        n_samples=1000, n_features=10, n_informative=3, n_redundant=2,\n        n_repeated=0, shuffle=False, random_state=0)\n\n    # Check importance on a bigger dataset.\n    est = ForestEstimator(n_estimators=10, random_state=0)\n    est.fit(X, y)\n    importances = est.feature_importances_\n    assert_array_almost_equal(importances.sum(), 1)\n\n    # Check importance using oob_score and compare with PermutationImportance.\n    from sklearn.inspection import permutation_importance\n\n    if issubclass(ForestEstimator, RandomForestClassifier):\n        est = ForestEstimator(n_estimators=5, random_state=0, oob_score=True)\n    else:\n        est = ForestEstimator(n_estimators=5, random_state=0)\n\n    est.fit(X, y)\n\n    if hasattr(est, 'oob_score_'):\n        assert_greater(est.oob_score_, 0.6)\n\n    result = permutation_importance(est, X, y, n_repeats=10, random_state=0)\n    importances_permutation = result.importances_mean\n\n    # permutation importance might not be exactly equal to the RF importance\n    # but they should be correlated.\n    assert_greater(np.corrcoef(importances, importances_permutation)[0, 1], 0.9)\n", "def test_parallel_train_with_no_valid_features(name):\n    \"\"\"Check that no exception is thrown when training in parallel with no valid features.\"\"\"\n    ForestEstimator = FOREST_ESTIMATORS[name]\n\n    X = [[1, 1], [1, 1], [1, 1]]\n    y = [0, 0, 0]\n\n    estimator = ForestEstimator(n_estimators=10, n_jobs=2)\n    with ignore_warnings(category=UserWarning):\n        estimator.fit(X, y)\n"], "sample_782": ["def test_column_transformer_get_feature_names():\n    X = np.array([[1, 2, 3], [4, 5, 6]])\n\n    # Single transformer\n    ct = ColumnTransformer([('trans', Trans(), [0, 1])])\n    ct.fit(X)\n    assert_equal(ct.get_feature_names(), ['trans__x0', 'trans__x1'])\n\n    # Multiple transformers\n    ct = ColumnTransformer([('trans1', Trans(), [0]), ('trans2', Trans(), [1, 2])])\n    ct.fit(X)\n    assert_equal(ct.get_feature_names(), ['trans1__x0', 'trans2__x0', 'trans2__x1'])\n\n    # 'drop' transformer\n    ct = ColumnTransformer([('trans1', Trans(), [0]), ('drop', 'drop', [1])])\n    ct.fit(X)\n    assert_equal(ct.get_feature_names(), ['trans1__x0'])\n\n    # 'passthrough' transformer\n    with pytest.raises(NotImplementedError):\n        ct = ColumnTransformer([('trans1', Trans(), [0]), ('pass', 'passthrough', [1])])\n        ct.fit(X)\n        ct.get_feature_names()\n", "def test_column_transformer_remainder_passthrough_sparse():\n    X_array = np.array([[1, 2, 3], [4, 5, 6]])\n    X_sparse = sparse.csr_matrix(X_array)\n    X_list = [[1, 2, 3], [4, 5, 6]]\n    transformer = ColumnTransformer(\n        [(\"trans\", StandardScaler(), [0])],\n        remainder=\"passthrough\",\n        sparse_threshold=0.3,\n    )\n\n    for X in [X_array, X_sparse, X_list]:\n        X_trans = transformer.fit_transform(X)\n        assert_allclose_dense_sparse(X_trans[:, 1:], X[:, 1:])\n", "def test_column_transformer_get_feature_names():\n    X = np.array([[1, 2], [3, 4]])\n    columns = ['col1', 'col2']\n\n    scaler = StandardScaler()\n    ohe = OneHotEncoder()\n\n    ct = ColumnTransformer([('scaler', scaler, ['col1']),\n                            ('ohe', ohe, ['col2'])])\n\n    X_df = pd.DataFrame(X, columns=columns)\n    ct.fit(X_df)\n\n    feature_names = ct.get_feature_names()\n    assert_array_equal(feature_names, ['scaler__col1', 'ohe__col2_2.0', 'ohe__col2_4.0'])\n", "def test_column_transformer_remainder_passthrough_sparse_threshold():\n    X_array = np.array([[1, 2, 3], [4, 5, 6]])\n    transformer = ColumnTransformer(\n        [(\"trans\", StandardScaler(), [0, 1])],\n        remainder=\"passthrough\",\n        sparse_threshold=0.5,\n    )\n\n    X_trans = transformer.fit_transform(X_array)\n    assert isinstance(X_trans, np.ndarray)\n    assert X_trans.shape == (2, 3)\n\n    transformer = ColumnTransformer(\n        [(\"trans\", OneHotEncoder(), [0])],\n        remainder=\"passthrough\",\n        sparse_threshold=0.5,\n    )\n    X_trans = transformer.fit_transform(X_array)\n    assert sparse.issparse(X_trans)\n    assert X_trans.shape == (2, 4)\n", "def test_column_transformer_remainder_pandas():\n    # Test that the ColumnTransformer works with a pandas dataframe and\n    # the 'remainder' parameter set to 'passthrough'.\n    import pandas as pd\n\n    X_array = np.array([[1, 2, 3], [4, 5, 6]])\n    X_df = pd.DataFrame(X_array, columns=['a', 'b', 'c'])\n\n    trans = ColumnTransformer([('trans1', StandardScaler(), ['a', 'b'])],\n                              remainder='passthrough')\n\n    X_trans = trans.fit_transform(X_df)\n    assert_equal(X_trans.shape, (2, 3))\n    assert_allclose_dense_sparse(X_trans[:, :2], StandardScaler().fit_transform(X_array[:, :2]))\n    assert_array_equal(X_trans[:, 2], X_array[:, 2])\n"], "sample_783": ["def test_simple_imputer_copy():\n    # Test that copy=True creates a copy of X on fit\n    X = sparse_random_matrix(5, 5, random_state=0).toarray()\n    original_X = X.copy()\n\n    imputer = SimpleImputer(missing_values=0, strategy='mean', copy=True)\n    imputer.fit(X)\n\n    X[0, 0] = 1\n\n    assert_array_equal(imputer.statistics_, np.mean(original_X, axis=0))\n", "def test_simple_imputer_copy():\n    # Check that copy is True by default\n    X = np.array([[1, 2], [np.nan, 3]])\n    imputer = SimpleImputer()\n    X_trans = imputer.fit_transform(X)\n    assert X_trans is not X\n\n    # Check that copy works when it's explicitly set to True\n    imputer = SimpleImputer(copy=True)\n    X_trans = imputer.fit_transform(X)\n    assert X_trans is not X\n\n    # Check that copy works when it's explicitly set to False\n    imputer = SimpleImputer(copy=False)\n    X_trans = imputer.fit_transform(X)\n    assert X_trans is X\n", "def test_imputer_copy():\n    # Test that copy was done correctly on the imputer\n    X = sparse_random_matrix(5, 5, random_state=0).toarray()\n    X.flat[2] = np.nan\n\n    imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n\n    # copy and modify data\n    X_imputed_copy = imputer.fit_transform(X.copy())\n    X_imputed_inplace = imputer.transform(X)\n\n    assert not np.array_equal(X_imputed_copy, X_imputed_inplace)\n", "def test_simple_imputer_copy():\n    # Check that copy_X=True creates a copy of X\n    X = np.array([[1, 2], [np.nan, 3]])\n    imputer = SimpleImputer(copy=True)\n    imputer.fit(X)\n\n    X_trans = imputer.transform(X)\n    X[0, 0] = np.nan\n\n    assert not np.isnan(X_trans[0, 0])\n\n    # Check that copy_X=False modifies X in place\n    X = np.array([[1, 2], [np.nan, 3]])\n    imputer = SimpleImputer(copy=False)\n    imputer.fit(X)\n\n    X_trans = imputer.transform(X)\n    assert X_trans is X\n", "def test_imputer_sparse_with_explicit_zero():\n    # Check that imputation works with explicit zeros in sparse data\n    X = sparse.csc_matrix(np.array([[1, 0, 0], [0, 2, 0], [0, 0, 3]]))\n    X_missing = sparse.csc_matrix(np.array([[1, 0, np.nan], [np.nan, 2, 0], [0, np.nan, 3]]))\n    imputer = SimpleImputer(strategy=\"mean\")\n    imputer.fit(X_missing)\n    X_trans = imputer.transform(X_missing)\n    assert_allclose_dense_sparse(X, X_trans)\n"], "sample_784": ["def test_calibration_curve():\n    # Generate data\n    X, y = make_classification(n_samples=100, n_features=20, random_state=42)\n\n    # Train a classifier\n    clf = RandomForestClassifier(random_state=42)\n    clf.fit(X, y)\n    y_pred = clf.predict_proba(X)[:, 1]\n\n    # Compute calibration curve\n    prob_true, prob_pred = calibration_curve(y, y_pred, n_bins=5)\n\n    # Check shape and boundaries\n    assert_equal(len(prob_true), len(prob_pred))\n    assert_greater_equal(prob_true.min(), 0)\n    assert_less_equal(prob_true.max(), 1)\n    assert_greater_equal(prob_pred.min(), 0)\n    assert_less_equal(prob_pred.max(), 1)\n\n    # Check with different strategy\n    prob_true_uniform, prob_pred_uniform = calibration_curve(y, y_pred, n_bins=5, strategy='uniform')\n    assert_array_not_equal(prob_true, prob_true_uniform)\n    assert_array_not_equal(prob_pred, prob_pred_uniform)\n", "def test_calibration_curve():\n    # Test calibration_curve function\n    y_true, y_pred = make_classification(n_samples=100, n_features=10,\n                                         random_state=42)\n    prob_true, prob_pred = calibration_curve(y_true, y_pred)\n\n    assert_equal(prob_true.shape, prob_pred.shape)\n    assert_greater_equal(prob_true.min(), 0)\n    assert_greater_equal(prob_pred.min(), 0)\n    assert_less_equal(prob_true.max(), 1)\n    assert_less_equal(prob_pred.max(), 1)\n\n    # Test with strategy='quantile'\n    prob_true, prob_pred = calibration_curve(y_true, y_pred, strategy='quantile')\n    assert_equal(prob_true.shape, prob_pred.shape)\n    assert_greater_equal(prob_true.min(), 0)\n    assert_greater_equal(prob_pred.min(), 0)\n    assert_less_equal(prob_true.max(), 1)\n    assert_less_equal(prob_pred.max(), 1)\n\n    # Test with normalize=True\n    prob_true, prob_pred = calibration_curve(y_true, y_pred, normalize=True)\n    assert_equal(prob_true.shape, prob_pred.shape)\n    assert_greater_equal(prob_true.min(), 0)\n    assert_greater_equal(prob_pred.min(), 0)\n    assert_less_equal(prob_true.max(), 1)\n    assert_less_equal(prob_pred.max(), 1)\n", "def test_calibration_curve():\n    # Test calibration_curve function\n    y_true, y_pred = make_classification(n_samples=100, n_features=20,\n                                         n_informative=10, n_redundant=5,\n                                         random_state=42)\n    prob_true, prob_pred = calibration_curve(y_true, y_pred, normalize=True)\n\n    assert_equal(prob_true.shape, prob_pred.shape)\n    assert_greater_equal(prob_true.min(), 0)\n    assert_less_equal(prob_true.max(), 1)\n    assert_greater_equal(prob_pred.min(), 0)\n    assert_less_equal(prob_pred.max(), 1)\n\n    prob_true, prob_pred = calibration_curve(y_true, y_pred, normalize=False)\n\n    assert_equal(prob_true.shape, prob_pred.shape)\n    assert_greater_equal(prob_true.min(), 0)\n    assert_less_equal(prob_true.max(), 1)\n    assert_greater_equal(prob_pred.min(), 0)\n    assert_less_equal(prob_pred.max(), 1)\n\n    with pytest.raises(ValueError):\n        calibration_curve(y_true, y_pred, strategy='unknown')\n", "def test_calibration_curve():\n    # Test calibration_curve function\n    y_true, y_pred = make_classification(n_samples=100, n_features=10,\n                                         n_informative=5, n_redundant=3,\n                                         n_repeated=2, random_state=42)\n    prob_true, prob_pred = calibration_curve(y_true, y_pred, normalize=True)\n\n    assert_greater_equal(prob_true.min(), 0)\n    assert_less_equal(prob_true.max(), 1)\n    assert_greater_equal(prob_pred.min(), 0)\n    assert_less_equal(prob_pred.max(), 1)\n\n    # Check for the case where y_true only contains one class label\n    y_true = np.zeros(100)\n    y_pred = np.random.rand(100)\n    with pytest.raises(ValueError):\n        calibration_curve(y_true, y_pred)\n", "def test_calibration_curve():\n    # Generate data\n    X, y = make_classification(n_samples=100, n_features=2, n_informative=2,\n                               n_redundant=0, random_state=42)\n\n    # Train a classifier\n    clf = RandomForestClassifier(random_state=42)\n    clf.fit(X, y)\n    y_pred = clf.predict_proba(X)[:, 1]\n\n    # Test calibration_curve function\n    prob_true, prob_pred = calibration_curve(y, y_pred, n_bins=5)\n    assert_greater(len(prob_true), 0)\n    assert_greater(len(prob_pred), 0)\n    assert_array_almost_equal(np.unique(y), np.array([0, 1]))\n"], "sample_785": ["def test_build_repr():\n    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n    repr = _build_repr(cv)\n    assert \"KFold\" in repr\n    assert \"n_splits=5\" in repr\n    assert \"shuffle=True\" in repr\n    assert \"random_state=42\" in repr\n", "def test_check_cv():\n    cv = 5\n    y = np.array([1, 2, 3, 4, 5])\n    classifier = True\n    # Should return StratifiedKFold\n    assert isinstance(check_cv(cv, y, classifier), StratifiedKFold)\n\n    y = np.array([1, 1, 1, 1, 1])\n    # Should return KFold\n    assert isinstance(check_cv(cv, y, classifier), KFold)\n\n    cv = 'warn'\n    y = np.array([1, 2, 3, 4, 5])\n    # Should raise a FutureWarning and return StratifiedKFold\n    with pytest.warns(FutureWarning):\n        assert isinstance(check_cv(cv, y, classifier), StratifiedKFold)\n", "def test_train_test_split_allow_nd():\n    # Test that train_test_split allows to return nd arrays if the classifier\n    # supports it.\n    X = np.random.randn(10, 2, 3)\n    y = np.random.randint(0, 2, 10)\n\n    with assert_raises(ValueError):\n        train_test_split(X, y, random_state=42)\n\n    splitter = MockClassifier(allow_nd=True)\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, random_state=42)\n\n    assert_equal(X_train.ndim, 3)\n    assert_equal(X_test.ndim, 3)\n\n    splitter.fit(X_train, y_train)\n    pred = splitter.predict(X_test)\n\n    assert_array_equal(pred, X_test[:, 0])\n", "def test_build_repr():\n    cv = KFold(n_splits=5, shuffle=True, random_state=0)\n    expected = 'KFold(n_splits=5, random_state=0, shuffle=True)'\n    assert _build_repr(cv) == expected\n\n    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n    expected = 'StratifiedKFold(n_splits=5, random_state=0, shuffle=True)'\n    assert _build_repr(cv) == expected\n\n    cv = GroupKFold(n_splits=5)\n    expected = 'GroupKFold(n_splits=5)'\n    assert _build_repr(cv) == expected\n", "def test_check_cv():\n    # Check integer\n    cv = check_cv(5)\n    assert isinstance(cv, KFold)\n    assert_equal(cv.n_splits, 5)\n\n    # Check None (default)\n    cv = check_cv(None)\n    assert isinstance(cv, KFold)\n    assert_equal(cv.n_splits, 5)\n\n    # Check old style cv object\n    cv_old = KFold(n_splits=5, shuffle=True, random_state=42)\n    cv = check_cv(cv_old)\n    assert isinstance(cv, KFold)\n    assert_equal(cv.n_splits, 5)\n    assert cv.shuffle\n    assert_equal(cv.random_state, 42)\n\n    # Check new style cv object\n    cv_new = KFold(n_splits=5, shuffle=True, random_state=42)\n    cv = check_cv(cv_new)\n    assert isinstance(cv, KFold)\n    assert_equal(cv.n_splits, 5)\n    assert cv.shuffle\n    assert_equal(cv.random_state, 42)\n\n    y_binary = np.array([0, 0, 1, 1, 1])\n    # Check with y and classifier\n    cv = check_cv(5, y=y_binary, classifier=True)\n    assert isinstance(cv, StratifiedKFold)\n    assert_equal(cv.n_splits, 5)\n\n    # Check with y and non-classifier\n    cv = check_cv(5, y=y_binary, classifier=False)\n    assert isinstance(cv, KFold)\n    assert_equal(cv.n_splits, 5)\n"], "sample_786": ["def test_kbins_discretizer_invalid_n_bins():\n    # Test that an invalid value for n_bins raises a ValueError\n    kbd = KBinsDiscretizer(n_bins='invalid')\n    with pytest.raises(ValueError):\n        kbd.fit(X)\n    with pytest.raises(ValueError):\n        kbd.fit_transform(X)\n\n    # Test that a non-integer value for n_bins raises a ValueError\n    kbd = KBinsDiscretizer(n_bins=3.5)\n    with pytest.raises(ValueError):\n        kbd.fit(X)\n    with pytest.raises(ValueError):\n        kbd.fit_transform(X)\n\n    # Test that a non-positive integer value for n_bins raises a ValueError\n    kbd = KBinsDiscretizer(n_bins=0)\n    with pytest.raises(ValueError):\n        kbd.fit(X)\n    with pytest.raises(ValueError):\n        kbd.fit_transform(X)\n", "def test_kbins_discretizer_invalid_encode_parameter():\n    # Test that an error is raised when an invalid encode parameter is passed\n    kbd = KBinsDiscretizer(n_bins=3, encode='invalid')\n    with pytest.raises(ValueError) as excinfo:\n        kbd.fit(X)\n    assert 'Valid options for \\'encode\\' are' in str(excinfo.value)\n", "def test_encode_parameter(encode, expected_type):\n    est = KBinsDiscretizer(n_bins=3, encode=encode)\n    Xt = est.fit_transform(X)\n    assert isinstance(Xt, expected_type)\n", "def test_kbins_discretizer_encode_onehot(encode):\n    est = KBinsDiscretizer(n_bins=3, encode=encode, strategy='uniform')\n    Xt = est.fit_transform(X)\n    if encode == 'onehot':\n        assert isinstance(Xt, sp.csr_matrix)\n    else:\n        assert isinstance(Xt, np.ndarray)\n    assert_array_equal(est.inverse_transform(Xt), est.inverse_transform(est.transform(X)))\n", "def test_invalid_n_bins():\n    # Test that invalid n_bins values raise errors\n    X = np.array([[1, 2], [3, 4]])\n    \n    # n_bins is not an integer\n    kbd = KBinsDiscretizer(n_bins=2.5)\n    assert_raises(ValueError, kbd.fit, X)\n    \n    # n_bins is less than 2\n    kbd = KBinsDiscretizer(n_bins=1)\n    assert_raises(ValueError, kbd.fit, X)\n    \n    # n_bins is an array with a value less than 2\n    kbd = KBinsDiscretizer(n_bins=[2, 1])\n    assert_raises(ValueError, kbd.fit, X)\n    \n    # n_bins is an array with a non-integer value\n    kbd = KBinsDiscretizer(n_bins=[2, 2.5])\n    assert_raises(ValueError, kbd.fit, X)\n"], "sample_787": ["def test_precision_recall_fscore_support_errors():\n    y_true, y_pred, _ = make_prediction(binary=True)\n\n    # Test that labels not present in y_true or y_pred don't raise an error\n    assert_no_warnings(precision_recall_fscore_support, y_true, y_pred,\n                       labels=np.arange(-1, 3))\n\n    # Test that if only some labels are present in y_pred, an UserWarning is\n    # thrown but the computation is correct.\n    y_pred = np.zeros_like(y_true)\n    with pytest.warns(UserWarning):\n        precision_recall_fscore_support(y_true, y_pred, average=None)\n", "def test_precision_recall_fscore_support_errors():\n    y_true, y_pred, _ = make_prediction(binary=True)\n\n    # Test with invalid beta\n    assert_raises(ValueError, precision_recall_fscore_support,\n                  y_true, y_pred, beta=0.0)\n    assert_raises(ValueError, precision_recall_fscore_support,\n                  y_true, y_pred, beta=-1.0)\n\n    # Test with non-averaging but not all labels in y_pred\n    assert_raises(ValueError, precision_recall_fscore_support,\n                  y_true, y_pred, average=None, labels=[0, 2])\n", "def test_multilabel_confusion_matrix():\n    # Test multilabel confusion matrix function\n    y_true = np.array([[1, 0, 1], [0, 1, 0]])\n    y_pred = np.array([[1, 0, 0], [0, 1, 1]])\n\n    # Get confusion matrix for each label\n    conf_mat = multilabel_confusion_matrix(y_true, y_pred)\n\n    # Check shape of confusion matrix\n    assert_equal(conf_mat.shape, (3, 2, 2))\n\n    # Check values in confusion matrix\n    expected_conf_mat = np.array([[[1, 0], [0, 1]], [[1, 0], [0, 1]], [[0, 1], [1, 0]]])\n    assert_array_equal(conf_mat, expected_conf_mat)\n", "def test_f1_score_binary_averaging():\n    # Test f1 score for different averaging methods in the binary case\n    y_true = np.array([0, 0, 1, 1])\n    y_pred = np.array([0, 1, 1, 0])\n\n    # Calculate f1 score for each label and their weighted average\n    f1_label_0 = f1_score(y_true, y_pred, pos_label=0)\n    f1_label_1 = f1_score(y_true, y_pred, pos_label=1)\n\n    # Calculate weighted average of the f1 scores\n    f1_weighted = (f1_label_0 + f1_label_1) / 2\n\n    # Assert that the macro average is the same as the weighted average\n    assert_almost_equal(f1_score(y_true, y_pred, average='macro'), f1_weighted)\n\n    # Assert that the weighted average is the same as the weighted average\n    assert_almost_equal(f1_score(y_true, y_pred, average='weighted'), f1_weighted)\n\n    # Assert that the micro average is the same as the weighted average\n    assert_almost_equal(f1_score(y_true, y_pred, average='micro'), f1_weighted)\n", "def test_multilabel_confusion_matrix():\n    # Test multilabel confusion matrix with different formats and averaging\n    y_true = np.array([[1, 0, 1], [0, 1, 0]])\n    y_pred = np.array([[1, 0, 0], [0, 1, 1]])\n\n    # Multilabel-indicator format\n    cm = multilabel_confusion_matrix(y_true, y_pred)\n    assert_array_equal(cm, np.array([[[1, 0], [0, 1]], [[1, 0], [0, 1]], [[0, 1], [1, 0]]]))\n\n    # Label format\n    y_true_lbl = [np.where(x)[0] for x in y_true]\n    y_pred_lbl = [np.where(x)[0] for x in y_pred]\n    cm = multilabel_confusion_matrix(y_true_lbl, y_pred_lbl)\n    assert_array_equal(cm, np.array([[[1, 0], [0, 1]], [[1, 0], [0, 1]], [[0, 1], [1, 0]]]))\n\n    # Samplewise\n    cm = multilabel_confusion_matrix(y_true, y_pred, samplewise=True)\n    assert_array_equal(cm, np.array([[[1, 1], [1, 1]], [[1, 1], [1, 1]]]))\n"], "sample_788": ["def test_kbins_discretizer_encode():\n    # Test the 'ordinal', 'onehot' and 'onehot-dense' encoding strategies.\n    discretizer = KBinsDiscretizer(n_bins=3, encode='ordinal')\n    X_trans_ord = discretizer.fit_transform(X)\n    assert_array_equal(X_trans_ord.shape, (4, 4))\n\n    discretizer = KBinsDiscretizer(n_bins=3, encode='onehot')\n    X_trans_onehot = discretizer.fit_transform(X)\n    assert isinstance(X_trans_onehot, sp.csr_matrix)\n    assert_array_equal(X_trans_onehot.shape, (4, 12))\n\n    discretizer = KBinsDiscretizer(n_bins=3, encode='onehot-dense')\n    X_trans_onehot_dense = discretizer.fit_transform(X)\n    assert not sp.issparse(X_trans_onehot_dense)\n    assert_array_equal(X_trans_onehot_dense.shape, (4, 12))\n", "def test_kbins_discretizer_encode():\n    est = KBinsDiscretizer(n_bins=3, encode='onehot-dense')\n    Xt = est.fit_transform(X)\n    assert_array_equal(Xt.shape, (4, 12))\n\n    est = KBinsDiscretizer(n_bins=3, encode='onehot')\n    Xt = est.fit_transform(X)\n    assert isinstance(Xt, sp.csr_matrix)\n    assert_array_equal(Xt.shape, (4, 12))\n", "def test_kbins_discretizer_inverse_transform():\n    est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n    Xt = est.fit_transform(X)\n    Xinv = est.inverse_transform(Xt)\n\n    # Check that the inverse transformed data is within the bin edges\n    for jj in range(X.shape[1]):\n        bin_edges = est.bin_edges_[jj]\n        assert_array_less_equal(bin_edges[:-1], Xinv[:, jj])\n        assert_array_less_equal(Xinv[:, jj], bin_edges[1:])\n", "def test_kbins_discretizer_encode(encode):\n    est = KBinsDiscretizer(n_bins=3, encode=encode, strategy='uniform')\n    Xt = est.fit_transform(X)\n    if encode == \"onehot\":\n        assert isinstance(Xt, sp.csr_matrix)\n    else:\n        assert isinstance(Xt, np.ndarray)\n", "def test_kbins_discretizer_encode_ordinal():\n    est = KBinsDiscretizer(n_bins=3, encode='ordinal')\n    Xt = est.fit_transform(X)\n    assert_array_equal(Xt, [[0, 0, 0, 0], [1, 1, 1, 0], [2, 2, 2, 1], [2, 2, 2, 2]])\n    Xinv = est.inverse_transform(Xt)\n    assert_array_almost_equal(Xinv, [[-1.5, 2., -3.5, -0.75],\n                                     [-0.5, 3., -2.5, -0.25],\n                                     [0.5, 4., -1.5, 0.75],\n                                     [0.5, 4., -1.5, 1.25]])\n"], "sample_789": ["def test_adaboost_multiclass_samme_r():\n    # Test the SAMME.R algorithm on a multi-class dataset\n    X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target,\n                                                        test_size=0.5,\n                                                        random_state=rng)\n    clf_samme = AdaBoostClassifier(algorithm='SAMME')\n    clf_samme_r = AdaBoostClassifier(algorithm='SAMME.R')\n    clf_samme.fit(X_train, y_train)\n    clf_samme_r.fit(X_train, y_train)\n    assert_greater(clf_samme.score(X_test, y_test), 0.9)\n    assert_greater(clf_samme_r.score(X_test, y_test), 0.9)\n", "def test_ada_boost_regressor_with_sparse():\n    # Test AdaBoostRegressor with sparse data\n    X_sparse = csr_matrix(X)\n    ada = AdaBoostRegressor()\n    ada.fit(X_sparse, y_regr)\n    assert_array_equal(ada.predict(T), ada.predict(csr_matrix(T)))\n    assert_array_equal(ada.predict(X_sparse), ada.predict(X))\n\n    # Check if we can fit with X and predict with X_sparse\n    ada.fit(X, y_regr)\n    assert_array_equal(ada.predict(X_sparse), ada.predict(X))\n", "def test_adaboost_multiclass():\n    # Test on a multi-class problem\n    X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target,\n                                                        random_state=rng)\n\n    clf = AdaBoostClassifier()\n    clf.fit(X_train, y_train)\n    assert_array_equal(clf.classes_, np.unique(y_train))\n    assert_equal(clf.n_classes_, len(np.unique(y_train)))\n\n    y_pred = clf.predict(X_test)\n    assert_array_equal(y_pred.shape, (y_test.shape[0],))\n\n    y_pred_proba = clf.predict_proba(X_test)\n    assert_array_equal(y_pred_proba.shape, (y_test.shape[0], np.unique(y_train).size))\n", "def test_adaboost_regressor_with_sparse_data():\n    # Test AdaBoostRegressor with sparse data\n    X_sparse = csc_matrix(X)\n    y_sparse = np.array(y_regr)\n\n    clf_sparse = AdaBoostRegressor(random_state=0)\n    clf_sparse.fit(X_sparse, y_sparse)\n\n    assert_array_almost_equal(clf_sparse.predict(T),\n                              clf_sparse.predict(csr_matrix(T)))\n\n    # Check if predict works on sparse and dense inputs\n    for sparse_format in [csc_matrix, csr_matrix, coo_matrix, dok_matrix,\n                          lil_matrix]:\n        sparse_T = sparse_format(T)\n        assert_array_almost_equal(clf_sparse.predict(sparse_T),\n                                  clf_sparse.predict(T))\n\n    # Train on sparse data and test on sparse data\n    clf_sparse = AdaBoostRegressor(random_state=0)\n    clf_sparse.fit(X_sparse, y_sparse)\n    assert_array_almost_equal(clf_sparse.predict(X_sparse), y_sparse)\n", "def test_adaboost_sparse_matrices():\n    # Check that AdaBoostClassifier and AdaBoostRegressor support sparse matrices\n\n    # Define a simple sparse matrix\n    X_sparse = csr_matrix(np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]]))\n\n    # Create a simple classification problem\n    y_class = np.array([0, 1, 1])\n\n    # Train an AdaBoostClassifier on the sparse data\n    clf = AdaBoostClassifier(n_estimators=10)\n    clf.fit(X_sparse, y_class)\n\n    # Train an AdaBoostRegressor on the sparse data\n    y_regr = np.array([0.0, 1.0, 1.0])\n    reg = AdaBoostRegressor(n_estimators=10)\n    reg.fit(X_sparse, y_regr)\n\n    # Check that the estimators trained on sparse data can predict\n    assert_array_equal(clf.predict(X_sparse), y_class)\n    assert_array_almost_equal(reg.predict(X_sparse), y_regr)\n"], "sample_790": ["def test_kernel_pca_sparse():\n    # Test that KernelPCA works with sparse matrices\n    X = sp.rand(100, 5, format='csr')\n    kpca = KernelPCA(n_components=2, kernel='linear')\n    X_trans = kpca.fit_transform(X)\n    assert X_trans.shape == (100, 2)\n", "def test_kernel_pca_sparse():\n    # Test that KernelPCA works with sparse data\n    rng = np.random.RandomState(0)\n    X = rng.rand(100, 5)\n    X_sparse = sp.csr_matrix(X)\n\n    kpca_dense = KernelPCA(n_components=3, kernel='linear', random_state=0)\n    kpca_sparse = KernelPCA(n_components=3, kernel='linear', random_state=0)\n\n    X_trans_dense = kpca_dense.fit_transform(X)\n    X_trans_sparse = kpca_sparse.fit_transform(X_sparse)\n\n    assert_array_almost_equal(X_trans_dense, X_trans_sparse)\n", "def test_kernel_pca_precomputed():\n    # Test that KernelPCA works with a precomputed kernel matrix\n    X, y = make_circles(n_samples=400, factor=.3, noise=.05)\n    K = rbf_kernel(X)\n\n    kpca = KernelPCA(kernel='precomputed', n_components=2)\n    X_kpca = kpca.fit_transform(K)\n\n    assert_equal(X_kpca.shape[1], 2)\n    assert_array_almost_equal(kpca.alphas_.shape, (K.shape[0], 2))\n    assert_array_almost_equal(kpca.lambdas_.shape, (2,))\n", "def test_kernel_pca_sparse():\n    \"\"\"Test KernelPCA on sparse data.\"\"\"\n    X, _ = make_circles(n_samples=400, factor=.2, noise=.05, random_state=0)\n    X_sparse = sp.csr_matrix(X)\n\n    kpca = KernelPCA(kernel=\"rbf\", gamma=10, n_components=2, fit_inverse_transform=True)\n    X_kpca_sparse = kpca.fit_transform(X_sparse)\n\n    assert_array_almost_equal(X_kpca_sparse, kpca.transform(X))\n", "def test_kernel_pca_sparse():\n    # Test KernelPCA on sparse data\n    rng = np.random.RandomState(0)\n    X = sp.csr_matrix(rng.rand(10, 5))\n    kpca = KernelPCA(n_components=3, kernel='linear')\n    X_trans = kpca.fit_transform(X)\n    assert_equal(X_trans.shape[1], 3)\n    assert_array_almost_equal(kpca.inverse_transform(X_trans), X.toarray())\n"], "sample_791": ["def test_one_hot_encoder_inverse_transform():\n    # create an instance of OneHotEncoder\n    enc = OneHotEncoder(categories='auto')\n\n    # fit the encoder to some data\n    X = [['Male', 1], ['Female', 3], ['Female', 2]]\n    enc.fit(X)\n\n    # transform some new data\n    X_new = [['Male', 2], ['Female', 1]]\n    X_new_transformed = enc.transform(X_new)\n\n    # inverse transform and check that we get the original data back\n    X_new_inverse_transformed = enc.inverse_transform(X_new_transformed)\n    assert_array_equal(X_new, X_new_inverse_transformed)\n", "def test_ordinal_encoder_inverse_transform():\n    # Test that inverse_transform works as expected\n    enc = OrdinalEncoder()\n    X = [['Male', 1], ['Female', 3], ['Female', 2]]\n    enc.fit(X)\n    transformed_X = enc.transform(X)\n    inverse_X = enc.inverse_transform(transformed_X)\n    assert_array_equal(inverse_X, X)\n\n    # Test with different data\n    X2 = [['Male', 4], ['Female', 1]]\n    transformed_X2 = enc.transform(X2)\n    inverse_X2 = enc.inverse_transform(transformed_X2)\n    assert_array_equal(inverse_X2, X2)\n", "def test_one_hot_encoder_handle_unknown():\n    # Test the handle_unknown parameter of the OneHotEncoder\n\n    X = np.array([[1, 2], [3, 4]])\n\n    # Test when handle_unknown is set to 'error'\n    ohe = OneHotEncoder(handle_unknown='error')\n    ohe.fit(X)\n\n    # This should raise a ValueError because 5 and 6 are not in the training data\n    assert_raises(ValueError, ohe.transform, np.array([[5, 6]]))\n\n    # Test when handle_unknown is set to 'ignore'\n    ohe = OneHotEncoder(handle_unknown='ignore')\n    ohe.fit(X)\n\n    # This should not raise an error and should return an array with all zeros\n    result = ohe.transform(np.array([[5, 6]]))\n    expected_result = sparse.csr_matrix(np.zeros((1, 4)))\n    assert_array_equal(toarray(result), toarray(expected_result))\n", "def test_one_hot_encoder_inverse_transform_handle_unknown():\n    # create an array with categorical values\n    X = np.array([['a', 'b'], ['c', 'd']])\n\n    # create a OneHotEncoder object and fit it to the data\n    ohe = OneHotEncoder(handle_unknown='ignore')\n    ohe.fit(X)\n\n    # transform the data\n    X_trans = ohe.transform(X)\n\n    # inverse transform the data\n    X_inv = ohe.inverse_transform(X_trans)\n\n    # check that the original and inverse transformed arrays are equal\n    assert_array_equal(toarray(X), toarray(X_inv))\n\n    # check that the inverse transform of an unknown category is None\n    X_new = np.array([['e', 'f']])\n    X_new_trans = ohe.transform(X_new)\n    X_new_inv = ohe.inverse_transform(X_new_trans)\n    assert_array_equal(toarray(X_new_inv), np.array([[None, None]]))\n", "def test_ordinal_encoder_handle_unknown():\n    # Test that OrdinalEncoder handles unknown values correctly\n\n    X = np.array([[1, 'a'], [2, 'b'], [3, 'c']])\n    enc = OrdinalEncoder()\n    enc.fit(X)\n\n    # Test that an error is raised when an unknown value is encountered\n    X_test = np.array([[4, 'd']])\n    with pytest.raises(ValueError, match=\"Found unknown categories\"):\n        enc.transform(X_test)\n"], "sample_792": ["def test_complementnb_predict_proba():\n    # Test ComplementNB predict_proba method\n    clf = ComplementNB()\n    clf.fit(X2, y2)\n    y_pred_proba = clf.predict_proba(X2)\n    assert_array_almost_equal(y_pred_proba.sum(axis=1), np.ones(len(X2)))\n    assert_array_equal(np.argmax(y_pred_proba, axis=1) + 1, y2)\n\n    # Test predict_proba with alpha=0\n    clf = ComplementNB(alpha=0)\n    clf.fit(X2, y2)\n    y_pred_proba = clf.predict_proba(X2)\n    assert_array_almost_equal(y_pred_proba.sum(axis=1), np.ones(len(X2)))\n    assert_array_equal(np.argmax(y_pred_proba, axis=1) + 1, y2)\n", "def test_complementnb_predict_proba():\n    # Test ComplementNB predict_proba method\n    clf = ComplementNB()\n    clf.fit(X2, y2)\n    y_pred_proba = clf.predict_proba(X2)\n    assert_array_almost_equal(y_pred_proba.sum(axis=1), np.ones(len(X2)))\n    assert_equal(y_pred_proba.shape, (len(X2), len(np.unique(y2))))\n", "def test_complementnb_alpha():\n    # Test if the alpha parameter is working as expected\n    clf = ComplementNB(alpha=0.5)\n    X_train, X_test, y_train, y_test = train_test_split(X2, y2, test_size=0.2)\n    clf.fit(X_train, y_train)\n    assert_greater(cross_val_score(clf, X2, y2, cv=3).mean(), 0.5)\n", "def test_complementnb():\n    # Some basic checks on ComplementNB.\n    clf = ComplementNB()\n    assert_raises(ValueError, clf.predict, X)\n\n    # Test if we can do basic fitting and prediction with ComplementNB.\n    clf.fit(X2, y2)\n    y_pred = clf.predict(X2)\n    assert_array_equal(y_pred, y2)\n\n    # Test for correct class prior.\n    clf = ComplementNB(class_prior=[0.5, 0.3, 0.2])\n    clf.fit(X2, y2)\n    assert_almost_equal(clf.class_log_prior_, np.log(np.array([0.5, 0.3, 0.2])))\n\n    # Test alpha.\n    clf = ComplementNB(alpha=0.01)\n    clf.fit(X2, y2)\n    y_pred = clf.predict(X2)\n    assert_array_equal(y_pred, y2)\n\n    # Test a few edge cases.\n    clf = ComplementNB(alpha=0)\n    clf.fit(X2, y2)\n    assert_array_equal(y2, clf.predict(X2))\n\n    # Check that an appropriate error is raised on linearly\n    # inseparable data.\n    assert_raises(ValueError, clf.fit, X, y)\n", "def test_gaussian_nb_partial_fit():\n    # Test GaussianNB's partial fit method\n    gnb = GaussianNB()\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n    gnb.partial_fit(X_train, y_train, classes=np.unique(y))\n    y_pred = gnb.predict(X_test)\n    assert_array_equal(y_pred, y_test)\n\n    # Check that the class counts are updated correctly\n    assert_array_equal(gnb.class_count_, np.array([3, 3]))\n\n    # Check that the theta and sigma attributes are updated correctly\n    assert gnb.theta_.shape == (2, 2)\n    assert gnb.sigma_.shape == (2, 2)\n"], "sample_793": ["def test_iforest_check_contamination():\n    # Test that contamination parameter is checked and validated correctly\n\n    X = iris.data\n\n    # contamination should be a float in (0, 0.5]\n    assert_raises_regex(ValueError, \"contamination must be in \\(0, 0.5\\]\",\n                        IsolationForest, contamination=0.6)\n\n    # contamination should not be negative\n    assert_raises_regex(ValueError, \"contamination must be in \\(0, 0.5\\]\",\n                        IsolationForest, contamination=-0.1)\n\n    # check for default value of contamination\n    iso = IsolationForest()\n    assert iso.contamination == \"legacy\"\n\n    # check when behaviour='new'\n    iso = IsolationForest(behaviour='new')\n    iso.fit(X)\n    assert iso.contamination == \"auto\"\n\n    # check that we raise an error if contamination='auto' with behaviour='old'\n    assert_raises_regex(ValueError, \"contamination parameter cannot be set to \"\n                                    \"'auto' when behaviour == 'old'.\",\n                        IsolationForest, behaviour='old', contamination='auto')\n", "def test_iforest_check_contamination():\n    # Check if the contamination parameter is set correctly\n    X = iris.data\n\n    # Test that an error is raised when contamination is not in (0, 0.5]\n    assert_raises_regex(ValueError,\n                        \"contamination must be in \\(0, 0.5\\]\",\n                        IsolationForest(contamination=0).fit, X)\n    assert_raises_regex(ValueError,\n                        \"contamination must be in \\(0, 0.5\\]\",\n                        IsolationForest(contamination=0.6).fit, X)\n\n    # Test that an error is raised when contamination is 'auto' and behaviour='old'\n    assert_raises_regex(ValueError,\n                        \"contamination parameter cannot be set to 'auto' when behaviour == 'old'\",\n                        IsolationForest(contamination='auto', behaviour='old').fit, X)\n", "def test_iforest_sparse_input():\n    # Isolation Forest should accept and handle sparse input correctly\n    X_train, X_test, y_train, y_test = train_test_split(boston.data,\n                                                        boston.target,\n                                                        test_size=0.5,\n                                                        random_state=rng)\n    X_train_sparse = csc_matrix(X_train)\n    X_test_sparse = csr_matrix(X_test)\n\n    # Fit on sparse data and predict on sparse data\n    iforest_sparse = IsolationForest(random_state=rng)\n    iforest_sparse.fit(X_train_sparse)\n    y_pred_sparse_sparse = iforest_sparse.predict(X_test_sparse)\n\n    # Fit on dense data and predict on sparse data\n    iforest_dense = IsolationForest(random_state=rng)\n    iforest_dense.fit(X_train)\n    y_pred_dense_sparse = iforest_dense.predict(X_test_sparse)\n\n    # Fit on sparse data and predict on dense data\n    y_pred_sparse_dense = iforest_sparse.predict(X_test)\n\n    # All three predictions should be the same\n    assert_array_equal(y_pred_sparse_sparse, y_pred_dense_sparse)\n    assert_array_equal(y_pred_sparse_sparse, y_pred_sparse_dense)\n", "def test_iforest_sparse_input():\n    # Test that IsolationForest works with sparse input\n    X_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target, random_state=rng)\n    X_train_sparse = csc_matrix(X_train)\n    X_test_sparse = csr_matrix(X_test)\n\n    iforest = IsolationForest(contamination=0.1, random_state=rng)\n    iforest.fit(X_train_sparse)\n\n    y_pred = iforest.predict(X_test_sparse)\n    assert_array_equal(y_pred.shape, (y_test.shape[0],))\n\n    # Check that predict works on both sparse and dense input\n    y_pred_dense = iforest.predict(X_test)\n    assert_array_equal(y_pred, y_pred_dense)\n", "def test_isolation_forest_predict():\n    # Test IsolationForest predict method\n    X_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target, test_size=0.5, random_state=rng)\n    \n    # make sure there are outliers in the test set\n    X_test_outliers = np.vstack((X_test, 10 * rng.rand(10, X_test.shape[1])))\n    \n    for contamination in (0.1, 0.2):\n        iso = IsolationForest(contamination=contamination, random_state=rng)\n        iso.fit(X_train)\n        \n        y_pred = iso.predict(X_test_outliers)\n        assert_greater(np.sum(y_pred == -1), 0)  # check that some outliers are detected\n        assert_greater(np.sum(y_pred == 1), 0)   # check that some inliers are detected\n        \n        # check that the decision function is working as expected\n        scores = iso.decision_function(X_test_outliers)\n        assert_array_equal((scores > 0).astype(int), y_pred)\n"], "sample_794": ["def test_ridgecv_multitarget():\n    # Create a multitarget dataset\n    X, y = make_regression(n_samples=100, n_features=10, n_targets=3,\n                           random_state=42)\n\n    # Create a RidgeCV instance with multiple alphas\n    ridgecv = RidgeCV(alphas=[0.1, 1.0, 10.0])\n\n    # Fit the model\n    ridgecv.fit(X, y)\n\n    # Check that the coefficients are of the correct shape\n    assert_equal(ridgecv.coef_.shape, (3, 10))\n\n    # Check that the intercepts are of the correct shape\n    assert_equal(ridgecv.intercept_.shape, (3,))\n", "def test_ridge_cv_store_cv_values():\n    # Test that RidgeCV can store CV values\n    X, y = make_regression(n_samples=10, n_features=5, random_state=0)\n    ridge_cv = RidgeCV(alphas=[0.1, 1.0], store_cv_values=True)\n    ridge_cv.fit(X, y)\n    assert hasattr(ridge_cv, 'cv_values_')\n    assert ridge_cv.cv_values_.shape == (10, 2)\n", "def test_ridge_cv_store_cv_values():\n    # Test if cv_values are stored when store_cv_values is True\n    X, y = make_regression(n_samples=100, n_features=10, random_state=0)\n    ridge_cv = RidgeCV(alphas=[1e-3, 1e-2, 1e-1], store_cv_values=True)\n    ridge_cv.fit(X, y)\n    assert hasattr(ridge_cv, 'cv_values_')\n    assert ridge_cv.cv_values_.shape == (100, 3)\n\n    # Test if cv_values are not stored when store_cv_values is False\n    ridge_cv = RidgeCV(alphas=[1e-3, 1e-2, 1e-1], store_cv_values=False)\n    ridge_cv.fit(X, y)\n    assert not hasattr(ridge_cv, 'cv_values_')\n", "def test_ridgecv_scorer():\n    # Test that RidgeCV works with a custom scorer\n    # Also checks the shape of the coefs_ attribute\n\n    X, y = make_regression(n_samples=100, n_features=10, random_state=42)\n\n        y_pred = estimator.predict(X)\n        return np.mean(np.abs(y - y_pred))\n\n    ridge_cv = RidgeCV(scoring=custom_scorer)\n    ridge_cv.fit(X, y)\n    assert hasattr(ridge_cv, 'coef_')\n    assert ridge_cv.coef_.shape == (X.shape[1], )\n", "def test_ridgecv_scorer_gets_coefficient():\n    # Test if the scorer function passed to RidgeCV gets the correct\n    # coefficients.\n        return np.mean(estimator.coef_)\n    \n    X, y = make_regression(n_samples=200, random_state=42)\n    alphas = [0.1, 1, 10]\n    model = RidgeCV(alphas=alphas, scoring=test_scorer)\n    model.fit(X, y)\n\n    assert_almost_equal(model.alpha_, alphas[0])\n"], "sample_795": ["def test_check_estimator_sparse_data():\n    class SparseDataEstimator(BaseEstimator):\n            return self\n\n            return np.ones(X.shape[0])\n\n    estimator = SparseDataEstimator()\n    check_estimator_sparse_data(\"SparseDataEstimator\", estimator)\n    msg = (\"Estimator SparseDataEstimator doesn't seem to fail gracefully on \"\n           \"sparse data: it should raise a TypeError if sparse input is not\"\n           \" supported.\")\n    with assert_raises_regex(TypeError, msg):\n        check_estimator_sparse_data(\"SparseDataEstimator\", estimator)\n", "def test_check_estimator_idempotent():\n    # Test that check_estimator_idempotent raises no error\n    estimator = LinearRegression()\n    set_random_state(estimator)\n    check_fit_idempotent('LinearRegression', estimator)\n", "def test_check_estimator_idempotent():\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([0, 0, 1])\n\n    estimator = LinearRegression()\n    check_fit_idempotent('LinearRegression', estimator)\n", "def test_check_estimator_idempotent():\n    class NotIdempotentEstimator(BaseEstimator):\n            self.fitted_ = True\n            return self\n\n            if not hasattr(self, 'fitted_'):\n                raise CorrectNotFittedError(\"estimator is not fitted yet\")\n            return np.ones(X.shape[0])\n\n    msg = \"The estimator NotIdempotentEstimator should be fitted\"\n    assert_raises_regex(CorrectNotFittedError, msg,\n                        check_fit_idempotent, \"NotIdempotentEstimator\",\n                        NotIdempotentEstimator())\n", "def test_check_estimator_with_tags():\n    class DummyEstimator(BaseEstimator):\n            return self\n\n    # should not raise anything if estimator has no tags\n    check_estimator(DummyEstimator())\n\n    # should not raise anything if estimator has tags\n    class DummyEstimatorWithTags(BaseEstimator):\n            return self\n\n            return {\"requires_positive_X\": True}\n\n    check_estimator(DummyEstimatorWithTags())\n\n    # should raise a warning if estimator has a tag that is not recognized\n    class DummyEstimatorWithUnknownTag(BaseEstimator):\n            return self\n\n            return {\"unknown_tag\": True}\n\n    with assert_warns_message(FutureWarning, \"Unknown tag unknown_tag\"):\n        check_estimator(DummyEstimatorWithUnknownTag())\n"], "sample_796": ["def test_huber_regression_with_sample_weight():\n    # Test that the HuberRegressor works with sample weights.\n    X, y = make_regression_with_outliers(n_samples=50, n_features=20)\n    sample_weight = np.ones_like(y)\n    sample_weight[:10] = 2.0  # give more weight to the first 10 samples\n\n    huber = HuberRegressor()\n    huber.fit(X, y, sample_weight=sample_weight)\n\n    assert_array_almost_equal(huber.coef_.shape, (X.shape[1],))\n    assert_greater(huber.score(X, y), 0.5)\n", "def test_huber_regression_with_sparse_data():\n    # Test that huber regression works with sparse data.\n    X, y = make_regression_with_outliers(n_samples=50, n_features=20)\n    X_sparse = sparse.csr_matrix(X)\n\n    huber_dense = HuberRegressor()\n    huber_sparse = HuberRegressor()\n\n    huber_dense.fit(X, y)\n    huber_sparse.fit(X_sparse, y)\n\n    assert_array_almost_equal(huber_dense.coef_, huber_sparse.coef_)\n    assert_almost_equal(huber_dense.intercept_, huber_sparse.intercept_)\n    assert_almost_equal(huber_dense.scale_, huber_sparse.scale_)\n", "def test_huber_regression_convergence_failure():\n    # Test that a HuberRegressor convergence failure raises a ValueError.\n    X, y = make_regression_with_outliers(n_samples=50, n_features=20)\n    huber = HuberRegressor(max_iter=1, tol=1e-5)\n    with pytest.raises(ValueError):\n        huber.fit(X, y)\n", "def test_huber_regression_check_epsilon():\n    # Make sure that epsilon should be greater than 1.0.\n    X, y = make_regression_with_outliers()\n    huber = HuberRegressor(epsilon=0.5)\n    with pytest.raises(ValueError):\n        huber.fit(X, y)\n\n    # Make sure that epsilon can be equal to 1.0.\n    X, y = make_regression_with_outliers()\n    huber = HuberRegressor(epsilon=1.0)\n    huber.fit(X, y)\n", "def test_huber_regression_sample_weights():\n    # Test that HuberRegressor handles sample weights correctly.\n    X, y = make_regression_with_outliers(n_samples=50, n_features=20)\n    sample_weight = np.ones_like(y)\n    sample_weight[:10] = 2.0  # assign more weight to the first 10 samples\n\n    huber = HuberRegressor(max_iter=1000)\n    huber.fit(X, y, sample_weight=sample_weight)\n\n    # Check that the outliers are identified correctly.\n    residual = np.abs(y - huber.predict(X))\n    outliers = residual > huber.scale_ * huber.epsilon\n    assert_array_equal(outliers[:10], False)  # first 10 samples should not be outliers\n"], "sample_797": ["def test_power_transformer_zero_variance():\n    # Check that PowerTransformer works with zero-variance features.\n    X = np.array([[1, 2, 3], [1, 2, 4], [1, 2, 5]])\n    pt = PowerTransformer()\n    Xt = pt.fit_transform(X)\n    assert_array_equal(Xt[:, 0], np.zeros(X.shape[0]))\n    assert_array_equal(Xt[:, 1], np.zeros(X.shape[0]))\n    assert np.var(Xt[:, 2]) > 0\n\n    X_inv = pt.inverse_transform(Xt)\n    assert_allclose(X, X_inv)\n", "def test_power_transformer_ignore_nan():\n    X = np.array([[1, 2], [np.nan, 4], [5, 6]])\n    pt = PowerTransformer()\n    X_trans = pt.fit_transform(X)\n    assert_array_equal(np.isnan(X_trans), np.isnan(X))\n", "def test_power_transformer_inverse():\n    # Test that the inverse method of PowerTransformer leads to the original data\n    pt = PowerTransformer(method='yeo-johnson', standardize=True)\n    X = np.array([[1, 2], [3, 2], [4, 5]])\n    X_trans = pt.fit_transform(X)\n    X_inv = pt.inverse_transform(X_trans)\n    assert_array_almost_equal(X, X_inv)\n", "def test_power_transformer_ignores_zeros():\n    # check that power transformer ignores zeros when finding lambda\n    X = np.array([0, 0, 0, 1, 2, 3, 4, 5]).reshape(-1, 1)\n    pt = PowerTransformer()\n    pt.fit(X)\n    assert_equal(pt.lambdas_, np.array([1]))  # lambda for identity transform\n", "def test_power_transformer_on_constant_features():\n    # Test that the PowerTransformer does not raise an error when a constant\n    # feature is present in the data.\n    X = np.array([[1, 2], [1, 3], [1, 4]])\n    pt = PowerTransformer()\n    X_trans = pt.fit_transform(X)\n    assert_array_equal(X_trans[:, 0], np.zeros(X.shape[0]))\n"], "sample_798": ["def test_ridge_cv_store_cv_values():\n    # Test if cv_values are stored when store_cv_values is True\n    ridge_cv = RidgeCV(alphas=[0.1, 1.0, 10.0], store_cv_values=True)\n    ridge_cv.fit(X_diabetes, y_diabetes)\n    assert hasattr(ridge_cv, 'cv_values_')\n    assert ridge_cv.cv_values_.shape == (len(y_diabetes), len(ridge_cv.alphas))\n\n    # Test if cv_values are not stored when store_cv_values is False\n    ridge_cv = RidgeCV(alphas=[0.1, 1.0, 10.0], store_cv_values=False)\n    ridge_cv.fit(X_diabetes, y_diabetes)\n    assert not hasattr(ridge_cv, 'cv_values_')\n", "def test_ridge_regression_sample_weight():\n    # Test sample_weight in _RidgeGCV\n    X, y = make_regression(n_samples=10, n_features=10, random_state=0)\n    sample_weight = np.ones(10)\n    sample_weight[0] = 0.5\n\n    # check that the sample weights are taken into account in _RidgeGCV\n    ridge_gcv = _RidgeGCV(alphas=[1], fit_intercept=False)\n    ridge_gcv.fit(X, y, sample_weight=sample_weight)\n\n    X_weighted = X * np.sqrt(sample_weight[:, np.newaxis])\n    y_weighted = y * np.sqrt(sample_weight)\n    ridge = Ridge(alpha=1, fit_intercept=False)\n    ridge.fit(X_weighted, y_weighted)\n\n    assert_allclose(ridge.coef_, ridge_gcv.coef_)\n", "def test_ridge_cv_store_cv_values():\n    # Test that cv_values are stored correctly\n    X, y = make_regression(n_samples=10, n_features=5)\n    ridge_cv = RidgeCV(alphas=[0.1, 1.0, 10.0], store_cv_values=True)\n    ridge_cv.fit(X, y)\n    assert_array_equal(ridge_cv.cv_values_.shape, (10, 3))\n", "def test_ridgecv_store_cv_values():\n    # Test if cv_values are stored correctly\n    X, y = make_regression(n_samples=10, n_features=10, random_state=0)\n    ridge_cv = RidgeCV(alphas=[1.0, 2.0], store_cv_values=True, cv=None)\n    ridge_cv.fit(X, y)\n    assert hasattr(ridge_cv, 'cv_values_')\n    assert ridge_cv.cv_values_.shape == (10, 2)\n", "def test_ridge_regression_sample_weights():\n    # Test sample weights in Ridge regression\n    X, y = make_regression(n_samples=100, n_features=10, noise=0.1,\n                           random_state=0)\n    sample_weight = np.random.rand(100)\n\n    # Naive weighted least squares\n    X_sw = X * np.sqrt(sample_weight)[:, np.newaxis]\n    y_sw = y * np.sqrt(sample_weight)\n    coef_naive = _solve_cholesky(X_sw, y_sw, alpha=1)\n\n    # Ridge with sample weights through rescaling\n    ridge = Ridge(alpha=1, fit_intercept=False)\n    ridge.fit(X, y, sample_weight=sample_weight)\n    assert_array_almost_equal(coef_naive, ridge.coef_)\n"], "sample_799": ["def test_cross_val_score_with_fit_params():\n    # Check cross_val_score accepting fit_params and passing them to the\n    # estimator's fit function.\n    X, y = make_classification(n_samples=100, n_features=20, n_informative=5,\n                               n_redundant=3, random_state=1)\n    param_grid = {'max_depth': [1, 2, 3]}\n    clf = GridSearchCV(Pipeline([('clf', SGDClassifier())]), param_grid)\n\n    scores = cross_val_score(clf, X, y, cv=3, fit_params={'clf__sample_weight': np.ones(X.shape[0])})\n    assert len(scores) == 3\n", "def test_cross_validate_verbose():\n    # Test that cross_validate with return_train_score=False and verbose=1\n    # doesn't print the train scores\n    X, y = make_classification(n_samples=100, n_features=20, random_state=42)\n    estimator = LogisticRegression()\n    scoring = 'accuracy'\n\n    old_stdout = sys.stdout\n    try:\n        sys.stdout = StringIO()\n        cross_validate(estimator, X, y, cv=3, scoring=scoring,\n                       return_train_score=False, verbose=1)\n        output = sys.stdout.getvalue()\n        assert 'train_accuracy' not in output\n    finally:\n        sys.stdout = old_stdout\n", "def test_cross_validate_with_callback():\n    # Test that cross_validate can accept a callback function.\n        assert isinstance(estimator, MockClassifier)\n        callback.called = True\n\n    callback.called = False\n    cv = KFold(n_splits=2)\n    mock_classifier = MockClassifier()\n    cross_validate(mock_classifier, X, y, cv=cv, fit_params={'callback': callback})\n    assert callback.called\n", "def test_cross_validate_return_estimator():\n    # Test that cross_validate returns fitted estimators\n    X, y = make_classification(n_samples=20, n_features=5, random_state=0)\n    cv = KFold(n_splits=2, shuffle=True, random_state=0)\n\n    est = MockClassifier()\n    scores = cross_validate(est, X, y, cv=cv, return_estimator=True)\n\n    assert 'estimator' in scores\n    for est in scores['estimator']:\n        assert hasattr(est, 'X_subset')\n", "def test_cross_val_score_with_single_fit_call_allowed():\n    # Test that cross_val_score does not call fit method multiple times\n    X, y = make_classification(n_samples=20, n_features=5, random_state=0)\n    estimator = MockEstimatorWithSingleFitCallAllowed()\n    scores = cross_val_score(estimator, X, y, cv=2)\n    assert len(scores) == 2\n"], "sample_800": ["def test_check_estimator_idempotent():\n    # Test that check_estimator does not modify the estimator parameters during\n    # testing.\n    X = np.array([[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1],\n                  [-1, -2], [2, 2], [-2, -2]])\n    y = [1, 1, 1, 2, 2, 2, 3, 3, 3]\n    estimator = LogisticRegression()\n    original_params = estimator.get_params(deep=True)\n    check_estimator(estimator, generate_only=False)\n    assert estimator.get_params(deep=True) == original_params\n    estimator.fit(X, y)\n    y_pred_original = estimator.predict(X)\n    check_estimator(estimator, generate_only=False)\n    y_pred_after_check = estimator.predict(X)\n    assert_array_equal(y_pred_original, y_pred_after_check)\n", "def test_check_estimator_idempotent():\n    # Test that check_estimator is idempotent. If it is, calling\n    # check_estimator(estimator) multiple times should not raise any new\n    # errors.\n    estimator = TaggedBinaryClassifier()\n    for _ in range(5):\n        check_estimator(estimator)\n", "def test_check_fit_idempotent():\n    # Test that check_fit_idempotent raises no warnings\n    estimator = LogisticRegression(random_state=0)\n    check_fit_idempotent(\"LogisticRegression\", estimator)\n", "def test_check_fit_score_takes_y():\n    # check if estimator checks for y in fit and score methods\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([0, 1])\n\n    class Estimator(BaseEstimator):\n            return self\n\n            return 0.0\n\n    estimator = Estimator()\n    msg = \"Estimator does not validate y in fit or score method.\"\n    with assert_raises_regex(ValueError, msg):\n        check_fit_score_takes_y(\"test\", estimator)\n\n    class Estimator(BaseEstimator):\n            return self\n\n            return 0.0\n\n    estimator = Estimator()\n    check_fit_score_takes_y(\"test\", estimator)\n", "def test_check_estimator_with_missing_tags():\n    # Test that check_estimator raises an informative error message when an\n    # estimator is missing required tags.\n    class MissingTagsEstimator(BaseEstimator):\n            return self\n\n    msg = \"can't test estimator.*MissingTagsEstimator.*tags are not set\"\n    assert_raises_regex(SkipTest, msg, check_estimator, MissingTagsEstimator())\n"], "sample_801": ["def test_clone():\n    # Tests that clone creates a new estimator with the same parameters\n    estimator = LogisticRegression(penalty='l2', dual=False, tol=1e-4, C=1.0,\n                                   fit_intercept=True, intercept_scaling=1,\n                                   class_weight=None, random_state=None,\n                                   solver='warn', max_iter=100, multi_class='warn',\n                                   verbose=0, warm_start=False, n_jobs=None,\n                                   l1_ratio=None)\n\n    clone_estimator = clone(estimator)\n\n    assert estimator is not clone_estimator\n    assert estimator.get_params() == clone_estimator.get_params()\n\n    # Test that clone raises an error when input is not an estimator\n    try:\n        clone(\"not an estimator\")\n        assert False\n    except TypeError as e:\n        assert \"Cannot clone object\" in str(e)\n", "def test_clone_pipeline_with_named_steps():\n    # Test cloning a pipeline with named steps\n    pipeline = make_pipeline(StandardScaler(), LogisticRegression())\n    cloned_pipeline = clone(pipeline)\n    assert cloned_pipeline.named_steps['standardscaler'] is not pipeline.named_steps['standardscaler']\n    assert cloned_pipeline.named_steps['logisticregression'] is not pipeline.named_steps['logisticregression']\n", "def test__more_tags():\n    # Test that the _more_tags method is called and its results are included\n    # in the estimator's tags.\n    class MyEstimator(BaseEstimator):\n            return {'my_tag': True}\n\n    est = MyEstimator()\n    tags = est._get_tags()\n    assert 'my_tag' in tags\n    assert tags['my_tag'] is True\n", "def test_clone_with_n_jobs():\n    # Test that clone preserves the n_jobs parameter\n    estimator = LogisticRegression(n_jobs=4)\n    cloned_estimator = clone(estimator)\n\n    assert cloned_estimator.n_jobs == 4\n\n    # Test that clone propagates n_jobs to sub-estimators\n    estimator = Pipeline([\n        ('selector', SelectKBest(score_func=chi2, k=5)),\n        ('clf', LogisticRegressionCV(n_jobs=3))\n    ])\n\n    cloned_estimator = clone(estimator)\n    assert cloned_estimator.steps[-1][1].n_jobs == 3\n", "def test_repr_multilines():\n    # Test estimator repr with multiline param value\n    estimator = LogisticRegression(multi_class=[\"multiclass\", \"ovr\"])\n    repr_ = repr(estimator)\n    assert \"multi_class=['multiclass', 'ovr']\" in repr_\n    assert len(repr_.splitlines()) == 2\n"], "sample_802": ["def test_pipeline_predict_with_fit_params():\n    # Test that pipeline passes fit_params to the final estimator\n    # in predict method\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([0, 1])\n\n    fit_params_estimator = FitParamT()\n    pipe = Pipeline([('mult', Mult()), ('fit_params_estimator', fit_params_estimator)])\n\n    pipe.fit(X, y)\n    pipe.predict(X, should_succeed=True)\n    assert fit_params_estimator.successful\n\n    fit_params_estimator.successful = False\n    pipe.predict(X)\n    assert not fit_params_estimator.successful\n", "def test_pipeline_score_samples_attribute_error():\n    # Test that pipeline raises AttributeError if final estimator does not have\n    # a score_samples method.\n\n    # Create a pipeline with an estimator that does not have a score_samples method\n    pipeline = Pipeline([('mult', Mult()), ('fit_param_t', FitParamT())])\n\n    # Try to call the score_samples method of the pipeline\n    X = np.array([[1], [2]])\n    assert_raises_regex(AttributeError, \"'FitParamT' object has no attribute 'score_samples'\", \n                        pipeline.score_samples, X)\n", "def test_pipeline_score_samples_attribute_error():\n    # Test that pipeline.score raises an AttributeError when the last estimator\n    # does not have a score method.\n    X = np.array([[1, 2]])\n    y = np.array([1])\n    pipe = Pipeline([('transf', Transf()), ('clf', NoInvTransf())])\n    assert_raises_regex(AttributeError,\n                        \"'NoInvTransf' object has no attribute 'score'\",\n                        pipe.score, X, y)\n", "def test_pipeline_transformer_weights():\n    # Test that transformer weights are taken into account in FeatureUnion\n    X = np.array([[1, 2], [3, 4]])\n    transformers = [('trans1', Mult(mult=2)), ('trans2', Mult(mult=3))]\n    weights = {'trans1': 0.5, 'trans2': 2}\n    fu = FeatureUnion(transformer_list=transformers,\n                      transformer_weights=weights)\n    assert_array_equal(fu.transform(X), np.array([[1, 6], [3, 12]]))\n", "def test_pipeline_get_params():\n    # Test that get_params returns all parameters of the estimators\n    pipe = make_pipeline(LogisticRegression(), StandardScaler())\n    params = pipe.get_params()\n    assert 'logisticregression__C' in params\n    assert 'standardscaler__with_mean' in params\n\n    # Test that set_params updates the parameters of the estimators\n    pipe.set_params(logisticregression__C=10, standardscaler__with_mean=False)\n    assert pipe.steps[0][1].C == 10\n    assert pipe.steps[1][1].with_mean is False\n"], "sample_803": ["def test_roc_curve_drop_intermediate():\n    # Test that drop_intermediate drops the correct thresholds\n    y_true = np.array([0, 0, 0, 1, 1, 1])\n    y_score = np.array([0.1, 0.4, 0.35, 0.8, 0.9, 0.95])\n    fpr, tpr, thresh = roc_curve(y_true, y_score, drop_intermediate=False)\n    fpr_dropped, tpr_dropped, thresh_dropped = roc_curve(y_true, y_score,\n                                                         drop_intermediate=True)\n\n    assert len(fpr_dropped) == 4\n    assert_array_almost_equal(fpr_dropped, [0., 0., 0.5, 1.])\n    assert_array_almost_equal(tpr_dropped, [0., 1., 1., 1.])\n    assert_array_almost_equal(thresh_dropped, [1.9, 0.8, 0.4, 0.1])\n", "def test_roc_auc_score_multi_class():\n    # Test roc_auc_score for multi-class classification problems\n    rng = np.random.RandomState(0)\n    y_pred = rng.rand(20, 3)\n    y_true = rng.randint(0, 3, size=20)\n\n    with pytest.raises(ValueError):\n        roc_auc_score(y_true, y_pred, average=\"macro\", multi_class=None)\n\n    with pytest.raises(ValueError):\n        roc_auc_score(y_true, y_pred, average=\"weighted\", multi_class=None)\n\n    with pytest.raises(ValueError):\n        roc_auc_score(y_true, y_pred, average=\"samples\", multi_class=None)\n\n    with pytest.raises(ValueError):\n        roc_auc_score(y_true, y_pred, average=\"micro\", multi_class=\"ovr\")\n\n    with pytest.raises(ValueError):\n        roc_auc_score(y_true, y_pred, average=\"macro\", multi_class=\"ovo\")\n\n    score_ovr = roc_auc_score(y_true, y_pred, multi_class=\"ovr\")\n    assert_array_equal(score_ovr.shape, (3,))\n    assert np.allclose(score_ovr, [0.93333333, 0.77777778, 0.91666667])\n\n    score_ovo = roc_auc_score(y_true, y_pred, multi_class=\"ovo\")\n    assert_array_equal(score_ovo.shape, (3,))\n    assert np.allclose(score_ovo, [0.8, 0.70833333, 0.875])\n", "def test_roc_auc_score_multiclass():\n    \"\"\"Test roc_auc_score for multi-class classification\"\"\"\n    y_true, y_pred, _ = make_prediction(dataset=None, binary=False)\n\n    # one-vs-one multi-class ROC AUC\n    with pytest.raises(ValueError):\n        roc_auc_score(y_true, y_pred, average=\"macro\")\n\n    # one-vs-rest multi-class ROC AUC\n    with pytest.raises(ValueError):\n        roc_auc_score(y_true, y_pred, average=\"weighted\")\n", "def test_label_ranking_average_precision_score():\n    # Test label_ranking_average_precision_score function\n\n    # Example 1: normal case\n    y_true = np.array([[1, 0, 0], [0, 0, 1]])\n    y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]])\n    assert_almost_equal(label_ranking_average_precision_score(y_true, y_score), \n                        0.41666666666666673)\n\n    # Example 2: all labels are relevant or unrelevant\n    y_true = np.array([[1, 1, 1], [0, 0, 0]])\n    y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]])\n    assert_almost_equal(label_ranking_average_precision_score(y_true, y_score), \n                        1.)\n\n    # Example 3: multilabel-indicator with only one label\n    y_true = np.array([[1], [0]])\n    y_score = np.array([[0.9], [0.1]])\n    assert_almost_equal(label_ranking_average_precision_score(y_true, y_score), \n                        1.)\n\n    # Example 4: binary classification problem\n    y_true = np.array([1, 0])\n    y_score = np.array([0.9, 0.1])\n    assert_raises(ValueError, label_ranking_average_precision_score, y_true, \n                  y_score)\n", "def test_auc_score_non_consecutive_labels():\n    # Test whether auc_score function correctly handles\n    # non-consecutive labels\n    y_true = np.array([0, 1, 2, 3])\n    y_pred = np.array([0.1, 0.8, 0.4, 0.9])\n    expected_auc = _auc(y_true, y_pred)\n    assert_almost_equal(auc(y_true, y_pred), expected_auc)\n"], "sample_804": ["def test_ordinal_encoder_inverse_transform():\n    # Test that OrdinalEncoder.inverse_transform works as expected\n    enc = OrdinalEncoder(categories=[['a', 'b', 'c'], [1, 2, 3]])\n    X = [['a', 1], ['b', 2]]\n    X_enc = enc.fit_transform(X)\n    X_inv = enc.inverse_transform(X_enc)\n    assert_array_equal(X_inv, X)\n", "def test_one_hot_encoder_inverse_transform():\n    # create an instance of OneHotEncoder with categorical features having different numbers of categories\n    ohe = OneHotEncoder(sparse=False)\n    \n    # fit the encoder to a dataset and transform it\n    X = np.array([[1, 2], [3, 1]])\n    ohe.fit(X)\n    transformed_X = ohe.transform(X)\n    \n    # inverse transform and compare with original data\n    inverse_transformed_X = ohe.inverse_transform(transformed_X)\n    assert_array_equal(toarray(inverse_transformed_X), X)\n\n    # create another instance of OneHotEncoder with drop parameter\n    ohe_drop = OneHotEncoder(drop='first', sparse=False)\n    \n    # fit the encoder to a dataset and transform it\n    X = np.array([[1, 2], [3, 1]])\n    ohe_drop.fit(X)\n    transformed_X = ohe_drop.transform(X)\n    \n    # inverse transform and compare with original data\n    inverse_transformed_X = ohe_drop.inverse_transform(transformed_X)\n    assert_array_equal(toarray(inverse_transformed_X), X)\n", "def test_one_hot_encoder_categories():\n    # Test that categories are correctly set\n    X = np.array([[1, 2], [3, 4]])\n    encoder = OneHotEncoder(categories='auto')\n    encoder.fit(X)\n    assert_array_equal(encoder.categories_, [[1, 3], [2, 4]])\n\n    # Test that categories can be manually set\n    X = np.array([[1, 2], [3, 4]])\n    encoder = OneHotEncoder(categories=[[1, 3], [2, 4]])\n    encoder.fit(X)\n    assert_array_equal(encoder.categories_, [[1, 3], [2, 4]])\n\n    # Test that categories must be the correct length\n    X = np.array([[1, 2], [3, 4]])\n    encoder = OneHotEncoder(categories=[[1, 3]])\n    with pytest.raises(ValueError):\n        encoder.fit(X)\n", "def test_one_hot_encoder_inverse_transform_missing_category():\n    # Test inverse transform with a missing category in the input data\n    X = np.array([['cat1', 1], ['cat2', 2]])\n    enc = OneHotEncoder(handle_unknown='ignore')\n    enc.fit(X)\n\n    # Create a new array with a missing category\n    X_new = np.array([[0, 0, 0], [1, 0, 0]])\n    X_inv = enc.inverse_transform(X_new)\n    assert_array_equal(X_inv, np.array([['cat1', None], ['cat2', None]]))\n", "def test_one_hot_encoder_handle_unknown():\n    # Test the 'handle_unknown' parameter of OneHotEncoder\n    X = [['Male', 1], ['Female', 3], ['Female', 2]]\n    ohe = OneHotEncoder(handle_unknown='error')\n    ohe.fit(X)\n\n    # Test that an error is raised when an unknown category is encountered\n    X_test = [['Unknown', 1]]\n    assert_raises(ValueError, ohe.transform, X_test)\n\n    # Test that no error is raised when handle_unknown is set to 'ignore'\n    ohe = OneHotEncoder(handle_unknown='ignore')\n    ohe.fit(X)\n    X_test = [['Unknown', 1]]\n    X_test_transformed = ohe.transform(X_test)\n    assert_array_equal(toarray(X_test_transformed), np.array([[0., 0., 0., 0., 0.]]))\n"], "sample_805": ["def test_mean_tweedie_deviance():\n    # check for correct output shape\n    y_true = np.array([1, 2, 3, 4])\n    y_pred = np.array([1.1, 2.1, 3.1, 4.1])\n    score = mean_tweedie_deviance(y_true, y_pred, p=1)\n    assert isinstance(score, float)\n\n    # check for correct output with sample_weight\n    sample_weight = np.array([0.5, 0.5, 0.5, 0.5])\n    score = mean_tweedie_deviance(y_true, y_pred, sample_weight=sample_weight, p=1)\n    assert isinstance(score, float)\n\n    # check for correct output with different power parameters\n    for p in [0, 1, 2]:\n        score = mean_tweedie_deviance(y_true, y_pred, p=p)\n        assert isinstance(score, float)\n\n    # check for invalid power parameter\n    with pytest.raises(ValueError):\n        mean_tweedie_deviance(y_true, y_pred, p=3.5)\n\n    # check for non-positive y_true or y_pred when p=1 or p=2\n    y_true = np.array([-1, 2, 3, 4])\n    y_pred = np.array([1.1, 2.1, 3.1, 4.1])\n    with pytest.raises(ValueError):\n        mean_tweedie_deviance(y_true, y_pred, p=1)\n    with pytest.raises(ValueError):\n        mean_tweedie_deviance(y_true, y_pred, p=2)\n\n    y_true = np.array([1, 2, 3, 4])\n    y_pred = np.array([0, 2.1, 3.1, 4.1])\n    with pytest.raises(ValueError):\n        mean_tweedie_deviance(y_true, y_pred, p=1)\n    with pytest.raises(ValueError):\n        mean_tweedie_deviance(y_true, y_pred, p=2)\n", "def test_mean_tweedie_deviance():\n    y_true = np.array([2, 0, 1, 4])\n    y_pred = np.array([0.5, 0.5, 2., 2.])\n\n    # check for power parameter p=1 (poisson)\n    assert_almost_equal(mean_tweedie_deviance(y_true, y_pred, p=1),\n                        mean_poisson_deviance(y_true, y_pred))\n\n    # check for power parameter p=2 (gamma)\n    assert_almost_equal(mean_tweedie_deviance(y_true, y_pred, p=2),\n                        mean_gamma_deviance(y_true, y_pred))\n\n    # check for invalid power parameter\n    with pytest.raises(ValueError):\n        mean_tweedie_deviance(y_true, y_pred, p=-0.5)\n\n    # check for non-numeric inputs\n    with pytest.raises(TypeError):\n        mean_tweedie_deviance(y_true, list(y_pred), p=1)\n\n    # check for invalid y_true and y_pred\n    with pytest.raises(ValueError):\n        mean_tweedie_deviance(np.array([-1, 0, 1]), y_pred, p=1)\n\n    with pytest.raises(ValueError):\n        mean_tweedie_deviance(y_true, np.array([0, 0, 0, 0]), p=1)\n", "def test_mean_tweedie_deviance():\n    y_true = np.array([2, 0, 1, 4])\n    y_pred = np.array([0.5, 0.5, 2., 2.])\n\n    # check power parameter\n    assert_raises_regex(ValueError, \"Tweedie deviance is only defined for \"\n                                    \"p<=0 and p>=1.\",\n                        mean_tweedie_deviance, y_true, y_pred, p=0.5)\n\n    # check y_true and y_pred non-negative\n    assert_raises_regex(ValueError, \"Mean Tweedie deviance error with \"\n                                    \"p=1 can only be used on strictly \"\n                                    \"positive y_pred.\",\n                        mean_tweedie_deviance, y_true, -y_pred, p=1)\n    assert_raises_regex(ValueError, \"Mean Tweedie deviance error with \"\n                                    \"p=2 can only be used on strictly \"\n                                    \"positive y_true and y_pred.\",\n                        mean_tweedie_deviance, y_true, y_pred, p=2)\n    assert_raises_regex(ValueError, \"Mean Tweedie deviance error with \"\n                                    \"p=1 can only be used on non-negative \"\n                                    \"y_true and strictly positive y_pred.\",\n                        mean_tweedie_deviance, -y_true, y_pred, p=1)\n\n    # check result\n    score = mean_tweedie_deviance(y_true, y_pred, p=1)\n    assert_almost_equal(score, 1.4260254037844386)\n\n    # check result for power=2 (Gamma distribution)\n    score = mean_tweedie_deviance(y_true[1:], y_pred[1:], p=2)\n    assert_almost_equal(score, 0.8647540865898748)\n", "def test_mean_tweedie_deviance():\n    # Check for non-positive y_true and y_pred\n    y_true = np.array([1, 2, 3])\n    y_pred = np.array([0, 0, 0])\n    assert_raises_regex(ValueError, \"Mean Tweedie deviance error with p=1 can only be used on \"\n                                    \"non-negative y_true and strictly positive y_pred.\",\n                        mean_tweedie_deviance, y_true, y_pred, p=1)\n\n    y_true = np.array([0, 0, 0])\n    y_pred = np.array([1, 2, 3])\n    assert_raises_regex(ValueError, \"Mean Tweedie deviance error with p=1 can only be used on \"\n                                    \"non-negative y_true and strictly positive y_pred.\",\n                        mean_tweedie_deviance, y_true, y_pred, p=1)\n\n    # Check for invalid power parameter\n    y_true = np.array([1, 2, 3])\n    y_pred = np.array([1, 2, 3])\n    assert_raises_regex(ValueError, \"Tweedie deviance is only defined for p<=0 and p>=1.\",\n                        mean_tweedie_deviance, y_true, y_pred, p=0.5)\n", "def test__check_reg_targets():\n    # Check that _check_reg_targets returns the correct type and shape of targets\n    y_true = np.array([1, 2, 3])\n    y_pred = np.array([1.0, 2.0, 3.0])\n\n    y_type, y_true_checked, y_pred_checked, _ = _check_reg_targets(y_true, y_pred, None)\n\n    assert y_type == 'continuous'\n    assert_array_equal(y_true_checked, y_true.reshape(-1, 1))\n    assert_array_equal(y_pred_checked, y_pred.reshape(-1, 1))\n\n    # Check that _check_reg_targets raises an error for inconsistent lengths\n    y_true = np.array([1, 2, 3])\n    y_pred = np.array([1.0, 2.0])\n\n    with pytest.raises(ValueError):\n        _check_reg_targets(y_true, y_pred, None)\n"], "sample_806": ["def test_gradient_boosting_classifier_init_estimators():\n    # Test that the init estimator is set correctly.\n    X, y = make_classification(n_samples=100, n_features=10, n_informative=5,\n                               n_redundant=0, random_state=1)\n\n    clf = GradientBoostingClassifier(init='zero', n_estimators=10,\n                                     random_state=1)\n    assert isinstance(clf.init_, ZeroEstimator)\n\n    clf = GradientBoostingClassifier(init='dummy', n_estimators=10,\n                                     random_state=1)\n    assert isinstance(clf.init_, DummyClassifier)\n\n    with assert_raises(ValueError):\n        GradientBoostingClassifier(init='unknown', n_estimators=10,\n                                   random_state=1)\n\n    # Test that using a classifier as init works\n    clf = GradientBoostingClassifier(init=DummyClassifier(), n_estimators=10,\n                                     random_state=1)\n    clf.fit(X, y)\n\n    # Test that using a regressor as init raises an error\n    with assert_raises(ValueError):\n        GradientBoostingClassifier(init=DummyRegressor(), n_estimators=10,\n                                   random_state=1).fit(X, y)\n", "def check_regression_boston(presort, loss):\n    # Check regression on boston dataset.\n    clf = GradientBoostingRegressor(loss=loss, n_estimators=100,\n                                    random_state=1, presort=presort)\n\n    assert_raises(ValueError, clf.predict, boston.data)\n\n    clf.fit(boston.data, boston.target)\n    y_pred = clf.predict(boston.data)\n    mse = mean_squared_error(boston.target, y_pred)\n    assert_less(mse, 10)\n\n    leaves = clf.apply(boston.data)\n    assert_equal(leaves.shape, (506, 100))\n", "def check_regression_toy(presort, loss):\n    # Check regression on a toy dataset.\n    clf = GradientBoostingRegressor(loss=loss, n_estimators=10,\n                                    random_state=1, presort=presort)\n\n    assert_raises(ValueError, clf.predict, T)\n\n    clf.fit(X, y)\n    assert_almost_equal(clf.predict(T), true_result)\n    assert_equal(10, len(clf.estimators_))\n\n    leaves = clf.apply(X)\n    assert_equal(leaves.shape, (6, 10))\n", "def test_gradient_boosting_regression_boston_housing_init_estimator():\n    # Check Gradient Boosting with different init estimators on the boston dataset.\n    X, y = fetch_california_housing(return_X_y=True)\n\n    # Linear regression as init estimator\n    linear_regressor = LinearRegression()\n    clf = GradientBoostingRegressor(n_estimators=100, random_state=1,\n                                    init=linear_regressor)\n    clf.fit(X, y)\n\n    # Dummy regressor as init estimator\n    dummy_regressor = DummyRegressor()\n    clf_dummy_init = GradientBoostingRegressor(n_estimators=100, random_state=1,\n                                               init=dummy_regressor)\n    clf_dummy_init.fit(X, y)\n\n    # Zero estimator as init estimator\n    zero_regressor = ZeroEstimator()\n    clf_zero_init = GradientBoostingRegressor(n_estimators=100, random_state=1,\n                                              init=zero_regressor)\n    clf_zero_init.fit(X, y)\n\n    assert_less(mean_squared_error(y, clf.predict(X)),\n                mean_squared_error(y, clf_dummy_init.predict(X)))\n    assert_less(mean_squared_error(y, clf.predict(X)),\n                mean_squared_error(y, clf_zero_init.predict(X)))\n", "def test_init_estimator():\n    # Test that the init estimator is fit correctly\n    X, y = make_classification(n_samples=100, n_features=10, random_state=42)\n    clf = GradientBoostingClassifier(init='zero')\n    clf.fit(X, y)\n    assert isinstance(clf.init_, ZeroEstimator)\n\n    clf = GradientBoostingClassifier(init='dummy')\n    with pytest.raises(ValueError):\n        clf.fit(X, y)\n\n    clf = GradientBoostingClassifier(init=DummyClassifier())\n    clf.fit(X, y)\n    assert isinstance(clf.init_, DummyClassifier)\n"], "sample_807": ["def test_calibration_curve():\n    # Test calibration_curve function\n    np.random.seed(0)\n    y_true = np.array([0, 1, 0, 1, 0, 1])\n    y_pred = np.array([0.2, 0.8, 0.4, 0.6, 0.1, 0.9])\n\n    prob_true, prob_pred = calibration_curve(y_true, y_pred, n_bins=2)\n\n    assert_array_almost_equal(prob_true, [0., 1.])\n    assert_array_almost_equal(prob_pred, [0.16666667, 0.76666667])\n", "def test_calibration_curve():\n    # Generate data\n    X, y = make_classification(n_samples=100, n_features=20, random_state=42)\n\n    # Train model\n    clf = RandomForestClassifier(random_state=42)\n    clf.fit(X, y)\n\n    # Predict probabilities\n    y_prob = clf.predict_proba(X)[:, 1]\n\n    # Compute calibration curve\n    prob_true, prob_pred = calibration_curve(y, y_prob, normalize=False,\n                                             n_bins=5, strategy='uniform')\n\n    # Assert shapes\n    assert_equal(prob_true.shape, (len(np.unique(prob_true)),))\n    assert_equal(prob_pred.shape, (len(np.unique(prob_pred)),))\n\n    # Assert values\n    assert_greater_equal(prob_true.min(), 0)\n    assert_less_equal(prob_true.max(), 1)\n    assert_greater_equal(prob_pred.min(), 0)\n    assert_less_equal(prob_pred.max(), 1)\n", "def test_calibration_curve():\n    # Test calibration_curve function\n    np.random.seed(0)\n    y_true = np.random.randint(2, size=100)\n    y_pred = np.random.rand(100)\n\n    prob_true, prob_pred = calibration_curve(y_true, y_pred, n_bins=5,\n                                             strategy='uniform')\n    assert_greater(len(prob_true), 0)\n    assert_greater(len(prob_pred), 0)\n\n    with pytest.raises(ValueError):\n        calibration_curve(y_true, y_pred + 1, normalize=False)\n\n    with pytest.raises(ValueError):\n        calibration_curve(y_true, y_pred, strategy='invalid')\n\n    prob_true, prob_pred = calibration_curve(y_true, y_pred, n_bins=5,\n                                             strategy='quantile')\n    assert_greater(len(prob_true), 0)\n    assert_greater(len(prob_pred), 0)\n", "def test_calibration_curve():\n    # Generate data\n    X, y = make_classification(n_samples=100, n_features=2,\n                               n_informative=2, n_redundant=0,\n                               random_state=42)\n\n    clf = LinearSVC()\n    clf.fit(X, y)\n    y_pred = clf.decision_function(X)\n\n    prob_true, prob_pred = calibration_curve(y, y_pred, normalize=True,\n                                             n_bins=5, strategy='uniform')\n\n    assert_equal(len(prob_true), 5)\n    assert_equal(len(prob_pred), 5)\n    assert_greater_equal(prob_true, 0)\n    assert_greater_equal(prob_pred, 0)\n    assert_greater_equal(1, prob_true)\n    assert_greater_equal(1, prob_pred)\n\n    prob_true, prob_pred = calibration_curve(y, y_pred, normalize=False,\n                                             n_bins=3, strategy='quantile')\n\n    assert_equal(len(prob_true), 3)\n    assert_equal(len(prob_pred), 3)\n    assert_greater_equal(prob_true, 0)\n    assert_greater_equal(prob_pred, 0)\n    assert_greater_equal(1, prob_true)\n    assert_greater_equal(1, prob_pred)\n", "def test_calibration_curve():\n    # Test calibration_curve function\n    y_true, y_pred = make_classification(n_samples=100, n_features=20,\n                                         n_informative=10, n_redundant=0,\n                                         random_state=42)\n    prob_true, prob_pred = calibration_curve(y_true, y_pred)\n    assert_array_equal(prob_true.shape, prob_pred.shape)\n    assert_greater_equal(prob_true.min(), 0)\n    assert_greater_equal(prob_pred.min(), 0)\n    assert_greater_equal(1, prob_true.max())\n    assert_greater_equal(1, prob_pred.max())\n\n    # Test with normalize option\n    y_pred_unnormalized = 2 * y_pred - 1  # values in [-1, 1]\n    prob_true, prob_pred = calibration_curve(y_true, y_pred_unnormalized,\n                                             normalize=True)\n    assert_array_equal(prob_true.shape, prob_pred.shape)\n    assert_greater_equal(prob_true.min(), 0)\n    assert_greater_equal(prob_pred.min(), 0)\n    assert_greater_equal(1, prob_true.max())\n    assert_greater_equal(1, prob_pred.max())\n\n    # Test with different number of bins\n    prob_true_3bins, prob_pred_3bins = calibration_curve(y_true, y_pred,\n                                                         n_bins=3)\n    assert_array_equal(prob_true_3bins.shape, prob_pred_3bins.shape)\n    assert_array_equal(prob_true_3bins.shape, (3,))\n    assert_greater_equal(prob_true_3bins.min(), 0)\n    assert_greater_equal(prob_pred_3bins.min(), 0)\n    assert_greater_equal(1, prob_true_3bins.max())\n    assert_greater_equal(1, prob_pred_3bins.max())\n\n    # Test with quantile strategy\n    prob_true_quantile, prob_pred_quantile = calibration_curve(\n        y_true, y_pred, strategy='quantile')\n    assert_array_equal(prob_true_quantile.shape, prob_pred_quantile.shape)\n    assert_greater_equal(prob_true_quantile.min(), 0)\n    assert_greater_equal(prob_pred_quantile.min(), 0)\n    assert_greater_equal(1, prob_true_quantile.max())\n    assert_greater_equal(1, prob_pred_quantile.max())\n"], "sample_808": ["def test_iforest_sparse_input():\n    # Isolation Forest should accept and handle sparse input correctly\n\n    X_train, X_test, y_train, y_test = train_test_split(boston.data,\n                                                        boston.target,\n                                                        test_size=0.5,\n                                                        random_state=rng)\n\n    sparse_X_train = csc_matrix(X_train)\n    sparse_X_test = csr_matrix(X_test)\n\n    iforest = IsolationForest(n_estimators=10, random_state=rng)\n    iforest.fit(sparse_X_train)\n\n    # The scores from dense and sparse input should be identical\n    assert_array_almost_equal(iforest.decision_function(X_test),\n                              iforest.decision_function(sparse_X_test))\n", "def test_iforest_average_path_length():\n    # Check _average_path_length function\n    n_samples_leaf = np.array([1, 2, 3, 4, 5])\n    average_path_length = _average_path_length(n_samples_leaf)\n\n    # Check the output shape\n    assert_equal(average_path_length.shape, (5,))\n\n    # Check the values\n    expected_values = np.array([0., 1., 2 * np.log(2) + np.euler_gamma - 2,\n                                2 * np.log(3) + np.euler_gamma - 3,\n                                2 * np.log(4) + np.euler_gamma - 4])\n    assert_allclose(average_path_length, expected_values)\n", "def test_iforest_average_path_length():\n    # check average path length computation\n    n_samples_leaf = np.array([1, 2, 3, 4, 5])\n    average_path_length = _average_path_length(n_samples_leaf)\n\n    # check for NaNs\n    assert not np.any(np.isnan(average_path_length))\n\n    # check for non regression\n    expected_result = np.array([0., 1., 1.25331414, 1.41168669, 1.50958631])\n    assert_array_almost_equal(average_path_length, expected_result)\n", "def test_iforest_average_path_length():\n    # Test the average path length function\n    n_samples_leaf = np.array([1, 2, 3, 4, 5])\n    average_path_lengths = _average_path_length(n_samples_leaf)\n\n    # Check for expected values\n    assert_array_almost_equal(average_path_lengths,\n                              np.array([0., 1., 1.84544818, 2.4150375, 2.88861118]))\n\n    # Test edge cases\n    assert_array_almost_equal(_average_path_length(np.array([0])), np.array([0.]))\n    assert_array_almost_equal(_average_path_length(np.array([-1])), np.array([np.nan]))\n", "def test_iforest_sparse_inputs():\n    # Test that sparse inputs are handled correctly\n    X = csc_matrix(iris.data)\n    y = iris.target\n\n    iforest = IsolationForest(contamination=0.1, random_state=rng)\n    iforest.fit(X)\n\n    # Check that fit() didn't convert the data to dense\n    assert isinstance(iforest.estimators_[0].tree_.feature, np.ndarray)\n\n    y_pred = iforest.predict(X)\n    assert_array_equal(y_pred.shape, (X.shape[0],))\n\n    scores = iforest.decision_function(X)\n    assert_array_equal(scores.shape, (X.shape[0],))\n"], "sample_809": ["def test_mutual_info_classif_discrete_features():\n    # Test mutual information between discrete features and a target variable.\n    X = np.array([[0, 1, 0], [1, 0, 1], [0, 1, 1], [1, 0, 0]])\n    y = np.array([0, 0, 1, 1])\n    mi = mutual_info_classif(X, y, discrete_features=True)\n    assert_array_equal(mi, np.array([0., 0., 0.69314718]))\n", "def test_mutual_info_regression_with_discrete_features():\n    # Test mutual information regression with discrete features.\n    X = np.array([[0, 1], [1, 0], [0, 1], [1, 0]])\n    y = np.array([0.5, 0.5, 1.5, 1.5])\n    discrete_features = [True, True]\n    mi = mutual_info_regression(X, y, discrete_features=discrete_features)\n    assert_array_equal(mi, np.array([0., 0.]))\n\n    X_sparse = csr_matrix(X)\n    mi_sparse = mutual_info_regression(X_sparse, y, discrete_features=discrete_features)\n    assert_array_equal(mi_sparse, np.array([0., 0.]))\n\n    # Test that mutual information is non-negative.\n    assert_greater(mutual_info_regression(X, y), 0.)\n", "def test_mutual_info_classif_discrete_features():\n    # Test mutual information between discrete features and a target variable.\n    X = np.array([[0, 1, 0],\n                  [1, 0, 1],\n                  [0, 1, 0],\n                  [1, 0, 1]])\n    y = np.array([0, 1, 0, 1])\n\n    mi = mutual_info_classif(X, y, discrete_features=True)\n    assert_array_equal(mi, np.array([1., 1., 1.]))\n\n    # Test with sparse matrix\n    X_sparse = csr_matrix(X)\n    mi_sparse = mutual_info_classif(X_sparse, y, discrete_features=True)\n    assert_array_equal(mi_sparse, mi)\n\n    # Test with a single feature\n    mi_single = mutual_info_classif(X[:, 0].reshape(-1, 1), y,\n                                    discrete_features=True)\n    assert_array_equal(mi_single, np.array([1.]))\n", "def test_mutual_info_classif_discrete_features():\n    # Test mutual information between discrete features and a target variable\n    X = np.array([[0, 1, 0], [1, 0, 1], [0, 1, 1], [1, 0, 0]])\n    y = np.array([0, 1, 1, 0])\n    discrete_features = [True, True, True]\n\n    mi = mutual_info_classif(X, y, discrete_features=discrete_features)\n    assert_array_equal(mi, np.array([0.5, 0.5, 0.5]))\n\n    # Test with sparse matrix\n    X_sparse = csr_matrix(X)\n    mi_sparse = mutual_info_classif(X_sparse, y, discrete_features=discrete_features)\n    assert_array_equal(mi_sparse, mi)\n\n    # Test with random state\n    rng = check_random_state(42)\n    mi_random_state = mutual_info_classif(X, y, discrete_features=discrete_features,\n                                          random_state=rng)\n    assert_array_equal(mi_random_state, mi)\n", "def test_mutual_info_classif_discrete_features():\n    # Test mutual information between discrete features and a target variable\n    X = np.array([[0, 1, 0], [1, 0, 1], [0, 1, 1], [1, 0, 0]])\n    y = np.array([0, 0, 1, 1])\n    mi = mutual_info_classif(X, y, discrete_features=True)\n    assert_array_equal(mi, np.array([0., 0., 0.69314718]))\n"], "sample_810": ["def test_pipeline_memory_cache_fit():\n    # Test that Pipeline uses the memory cache if provided during fit\n\n    X = np.array([[1], [2], [3]])\n    pipe = Pipeline([('transf', DummyTransf()), ('clf', DummyRegressor())],\n                    memory=Memory(location=mkdtemp(), verbose=10))\n\n    # Check that second call to fit does not call fit again on the transformer\n    pipe.fit(X, None)\n    timestamp_first_fit = pipe.named_steps['transf'].timestamp_\n    pipe.fit(X, None)\n    assert_equal(pipe.named_steps['transf'].timestamp_, timestamp_first_fit)\n\n    # Check that the cache is cleared when the estimator is modified\n    pipe.set_params(transf__a=1)\n    pipe.fit(X, None)\n    assert not (pipe.named_steps['transf'].timestamp_ == timestamp_first_fit)\n\n    # Check that cache is cleared on pipeline clone\n    cloned_pipe = clone(pipe)\n    cloned_pipe.fit(X, None)\n    assert not (cloned_pipe.named_steps['transf'].timestamp_ ==\n                pipe.named_steps['transf'].timestamp_)\n", "def test_pipeline_memory():\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n\n    # Create a pipeline with caching\n    cachedir = mkdtemp()\n    try:\n        memory = Memory(location=cachedir, verbose=10)\n        pipe = Pipeline([('transf', DummyTransf()), ('clf', LogisticRegression())],\n                        memory=memory)\n\n        # Check if the cache is used\n        pipe.fit(X, y)\n        first_timestamp = pipe.named_steps['transf'].timestamp_\n        pipe.fit(X, y)\n        assert_equal(pipe.named_steps['transf'].timestamp_, first_timestamp)\n\n        # Check if the cache is not used when the memory is None\n        pipe.memory = None\n        pipe.fit(X, y)\n        assert_greater(pipe.named_steps['transf'].timestamp_, first_timestamp)\n\n        # Check if the cache is not used when the data is different\n        pipe.memory = memory\n        pipe.fit(X, y + 1)\n        assert_greater(pipe.named_steps['transf'].timestamp_, first_timestamp)\n\n        # Check if the cache is not used when the transformer is different\n        pipe.memory = memory\n        pipe.named_steps['transf'].a = 1\n        pipe.fit(X, y)\n        assert_greater(pipe.named_steps['transf'].timestamp_, first_timestamp)\n\n    finally:\n        shutil.rmtree(cachedir)\n", "def test_pipeline_predict_params():\n    # Test that predict methods are called with the correct parameters\n\n    class DummyEstimatorParams(BaseEstimator):\n        \"\"\"Mock classifier that takes params on predict\"\"\"\n\n            return self\n\n            self.got_attribute = got_attribute\n            return self\n\n    estimator = DummyEstimatorParams()\n    pipe = make_pipeline(estimator)\n\n    pipe.fit([[1], [2]], [1, 2])\n    pipe.predict([[1], [2]], got_attribute=True)\n    assert hasattr(estimator, 'got_attribute')\n    assert estimator.got_attribute\n", "def test_pipeline_slicing():\n    # Test that slicing a pipeline gives the expected results\n    pipeline = make_pipeline(StandardScaler(), PCA(n_components=2), LogisticRegression())\n    sliced_pipeline = pipeline[:-1]\n\n    assert isinstance(sliced_pipeline, Pipeline)\n    assert len(sliced_pipeline.steps) == 2\n    assert isinstance(sliced_pipeline.steps[0][1], StandardScaler)\n    assert isinstance(sliced_pipeline.steps[1][1], PCA)\n\n    sliced_pipeline = pipeline[-1:]\n    assert isinstance(sliced_pipeline, Pipeline)\n    assert len(sliced_pipeline.steps) == 1\n    assert isinstance(sliced_pipeline.steps[0][1], LogisticRegression)\n", "def test_pipeline_inverse_transform():\n    # Test that inverse_transform works on pipelines with more than one\n    # transformer\n    X = np.array([[1, 2], [3, 4]])\n    p = make_pipeline(Mult(mult=2), Transf())\n    p.fit(X)\n    X_tr = p.transform(X)\n    assert_array_equal(p.inverse_transform(X_tr), X)\n"], "sample_811": ["def test_pairwise_distances_argmin_min():\n    # Test pairwise_distances_argmin_min and its alias pairwise_distances_argmin\n    X = np.random.randn(10, 5)\n    Y = np.random.randn(7, 5)\n\n    dist = pairwise_distances(X, Y)\n    argmin = dist.argmin(axis=1)\n    min_dist = dist[np.arange(len(argmin)), argmin]\n\n    argmin_2, min_dist_2 = pairwise_distances_argmin_min(X, Y)\n    assert_array_equal(argmin, argmin_2)\n    assert_array_almost_equal(min_dist, min_dist_2)\n\n    argmin_3 = pairwise_distances_argmin(X, Y)\n    assert_array_equal(argmin, argmin_3)\n", "def test_pairwise_distances_argmin_min():\n    # Test that pairwise_distances_argmin_min returns the correct minimum\n    # distances and indices for a given distance metric.\n    X = np.array([[1, 2], [3, 4]])\n    Y = np.array([[5, 6], [7, 8]])\n\n    # Test with metric='euclidean'\n    dist_metric = 'euclidean'\n    argmin, min_dist = pairwise_distances_argmin_min(X, Y, metric=dist_metric)\n    assert_array_almost_equal(argmin, np.array([0, 0]))\n    assert_array_almost_equal(min_dist, np.array([5.38516481, 5.38516481]))\n\n    # Test with metric='manhattan'\n    dist_metric = 'manhattan'\n    argmin, min_dist = pairwise_distances_argmin_min(X, Y, metric=dist_metric)\n    assert_array_almost_equal(argmin, np.array([0, 0]))\n    assert_array_almost_equal(min_dist, np.array([8., 8.]))\n\n    # Test with metric='cosine'\n    dist_metric = 'cosine'\n    argmin, min_dist = pairwise_distances_argmin_min(X, Y, metric=dist_metric)\n    assert_array_almost_equal(argmin, np.array([0, 0]))\n    assert_array_almost_equal(min_dist, np.array([0.00588235, 0.00588235]))\n", "def test_pairwise_distances_chunked_precomputed():\n    # Test that pairwise_distances_chunked works with precomputed distances\n    X = np.random.RandomState(0).rand(5, 3)\n    D = pairwise_distances(X, metric='euclidean')\n    gen = pairwise_distances_chunked(D, metric='precomputed')\n    D_chunked = next(gen)\n\n    assert_array_almost_equal(D, D_chunked)\n", "def test_pairwise_distances_argmin_min():\n    # Test that pairwise_distances_argmin_min is equivalent to calling\n    # argmin and min on the result of pairwise_distances, but more efficient.\n    X = np.random.RandomState(0).randn(100, 5)\n    Y = np.random.RandomState(1).randn(50, 5)\n\n    dist_mat = pairwise_distances(X, Y)\n    argmin_full = dist_mat.argmin(axis=1)\n    min_full = dist_mat.min(axis=1)\n\n    argmin_fast, min_fast = pairwise_distances_argmin_min(X, Y)\n\n    assert_array_equal(argmin_full, argmin_fast)\n    assert_array_almost_equal(min_full, min_fast)\n", "def test_pairwise_distances_chunked_reduce_func():\n    # Test pairwise_distances_chunked with a reduce function that takes in\n    # a distance matrix chunk and returns the indices of the k nearest neighbors\n\n    X = np.random.RandomState(0).rand(10, 5)\n    Y = np.random.RandomState(0).rand(7, 5)\n\n        k = 3\n        neigh = np.argsort(D_chunk, axis=1)[:, :k]\n        return neigh\n\n    gen = pairwise_distances_chunked(X, Y, reduce_func=reduce_func)\n    chunks = list(gen)\n\n    assert len(chunks) > 1  # Ensure we have more than one chunk\n\n    neigh = np.concatenate(chunks)\n    assert neigh.shape == (X.shape[0], 3)\n\n    # Compare with the result from pairwise_distances without chunking\n    D = pairwise_distances(X, Y)\n    neigh_full = np.argsort(D, axis=1)[:, :3]\n\n    assert_array_equal(neigh, neigh_full)\n"], "sample_812": ["def test_pprint_estimator_max_elements():\n    # Test that the `n_max_elements_to_show` parameter is taken into account\n    data = np.random.rand(100, 10)\n    estimator = LogisticRegressionCV(max_iter=10000)\n    estimator.fit(data, np.random.randint(0, 2, size=100))\n\n    # Get the repr with all elements shown\n    set_config(print_changed_only=False)\n    repr_all = _EstimatorPrettyPrinter(n_max_elements_to_show=None).pprint(\n        estimator)\n\n    # Get the repr with a limited number of elements shown\n    set_config(print_changed_only=False)\n    repr_limited = _EstimatorPrettyPrinter(n_max_elements_to_show=5).pprint(\n        estimator)\n\n    assert len(repr_limited) < len(repr_all)\n    assert '...' in repr_limited\n", "def test_long_dict_compact():\n    data = {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6}\n    printer = _EstimatorPrettyPrinter(compact=True,\n                                      n_max_elements_to_show=3)\n    expected = (\"{'a': 1, 'b': 2, 'c': 3, ...}\", True, False)\n    assert printer._format_dict_items(data.items(), \"\", 0, 100, {}, 0) == expected\n", "def test_pprint_estimator_max_iter():\n    # Test that the max_iter parameter is displayed when it's set to a value\n    # other than the default\n    estimator = LogisticRegression(max_iter=500)\n    repr_ = _EstimatorPrettyPrinter().pformat(estimator)\n    assert re.search(r'max_iter=500', repr_)\n", "def test_changed_only():\n    # Test that only parameters that were explicitly passed are shown when\n    # print_changed_only=True\n\n    class Estimator(BaseEstimator):\n            self.a = a\n            self.b = b\n            self.c = c\n\n    estimator = Estimator(a=1)\n    set_config(print_changed_only=True)\n\n    pp = _EstimatorPrettyPrinter()\n    repr_ = pp.pformat(estimator)\n    assert \"Estimator(a=1)\" in repr_\n    assert \"b\" not in repr_\n    assert \"c\" not in repr_\n\n    set_config(print_changed_only=False)\n    pp = _EstimatorPrettyPrinter()\n    repr_ = pp.pformat(estimator)\n    assert \"Estimator(a=1, b=None, c=None)\" in repr_\n", "def test_pprint_estimator_with_empty_params():\n    # Test that an estimator with no parameters is correctly pretty-printed.\n    class EmptyEstimator(BaseEstimator):\n            pass\n\n    estimator = EmptyEstimator()\n    expected_repr = \"EmptyEstimator()\"\n    assert repr(estimator) == expected_repr\n"], "sample_813": ["def test_bayesian_ridge_init_parameters():\n    X, y = diabetes.data, diabetes.target\n    n_samples, n_features = X.shape\n\n    # Check that the default initialization of the parameters is correct\n    br = BayesianRidge()\n    assert_almost_equal(br.lambda_init_, None)\n    assert_almost_equal(br.alpha_init_, None)\n\n    # Check that the initialization of the parameters with given values is correct\n    lambda_init = 1.0\n    alpha_init = 2.0\n    br = BayesianRidge(lambda_init=lambda_init, alpha_init=alpha_init)\n    assert_almost_equal(br.lambda_init_, lambda_init)\n    assert_almost_equal(br.alpha_init_, alpha_init)\n\n    # Check that the parameters are updated correctly during fit\n    br.fit(X, y)\n    assert_almost_equal(br.lambda_, br.lambda_)\n    assert_almost_equal(br.alpha_, br.alpha_)\n", "def test_bayesian_ridge_log_marginal_likelihood_value():\n    # Test log marginal likelihood value for BayesianRidge\n    X = diabetes.data\n    y = diabetes.target\n    n_samples, n_features = X.shape\n\n    # Initialize BayesianRidge with a large number of iterations\n    br = BayesianRidge(n_iter=1000)\n    br.fit(X, y)\n\n    # Compute the log marginal likelihood using the _log_marginal_likelihood\n    # function from the BayesianRidge class\n    log_marginal_likelihood = br._log_marginal_likelihood(\n        n_samples, n_features, np.linalg.eigvalsh(np.dot(X.T, X)),\n        br.alpha_, br.lambda_, br.coef_, np.sum((y - np.dot(X, br.coef_)) ** 2))\n\n    # Compute the log marginal likelihood using the formula\n    # (see Bishop book, page 167)\n    sigma = pinvh(np.dot(X.T, X) / br.lambda_ + np.eye(n_features))\n    log_marginal_likelihood_formula = (\n        0.5 * (-n_samples * log(2 * np.pi) - n_samples * log(br.alpha_)\n               - n_samples * log(1 + br.lambda_ / (br.alpha_ * n_samples))\n               - np.sum(np.log(np.linalg.eigvalsh(sigma))) +\n               np.dot(np.dot(y.T, X), np.dot(sigma, np.dot(X.T, y))) /\n               (n_samples * br.alpha_)))\n\n    assert_almost_equal(log_marginal_likelihood,\n                        log_marginal_likelihood_formula, decimal=3)\n", "def test_bayesian_ridge_log_marginal_likelihood_value():\n    # Test that the log marginal likelihood of BayesianRidge is correct.\n    X, y = diabetes.data, diabetes.target\n    n_samples, n_features = X.shape\n\n    # Fit a BayesianRidge model and get the log marginal likelihood.\n    br = BayesianRidge(compute_score=True)\n    br.fit(X, y)\n    score_br = br.scores_[-1]\n\n    # Compute the log marginal likelihood using the formula in the docstring.\n    alpha_1 = 1.e-6\n    alpha_2 = 1.e-6\n    lambda_1 = 1.e-6\n    lambda_2 = 1.e-6\n    alpha_ = br.alpha_\n    lambda_ = br.lambda_\n    coef_ = br.coef_\n    eigen_vals_ = np.linalg.eigvalsh(np.dot(X.T, X))\n    score manual = (lambda_1 * log(lambda_) - lambda_2 * lambda_ +\n                    alpha_1 * log(alpha_) - alpha_2 * alpha_ +\n                    0.5 * (n_features * log(lambda_) +\n                           n_samples * log(alpha_) -\n                           alpha_ * np.sum((y - np.dot(X, coef_)) ** 2) -\n                           lambda_ * np.sum(coef_ ** 2) +\n                           fast_logdet(np.linalg.inv(np.dot(X.T, X) / alpha_ + np.eye(n_features) / lambda_)) -\n                           n_samples * log(2 * np.pi)))\n\n    assert_almost_equal(score_br, score_manual, decimal=10)\n", "def test_bayesianridge_log_marginal_likelihood_value():\n    # Test log marginal likelihood value for BayesianRidge\n    X, y = diabetes.data, diabetes.target\n    n_samples, n_features = X.shape\n\n    # compute with Ridge and log marginal likelihood equation\n    alpha = 0.1\n    lambda_ = 0.1\n    ridge = Ridge(alpha=lambda_, fit_intercept=False)\n    ridge.fit(X, y)\n    K = np.dot(X.T, X) / alpha + lambda_ * np.eye(n_features)\n    L = fast_logdet(K)\n    log_marginal_likelihood = - 0.5 * (L + n_samples * log(2 * np.pi * alpha) +\n                                       np.sum((y - ridge.predict(X)) ** 2) / alpha)\n\n    # compute with BayesianRidge\n    bayesridge = BayesianRidge(fit_intercept=False, n_iter=100,\n                               alpha_1=1.e-6, alpha_2=1.e-6,\n                               lambda_1=1.e-6, lambda_2=1.e-6)\n    bayesridge.alpha_ = alpha\n    bayesridge.lambda_ = lambda_\n    bayesridge.fit(X, y)\n\n    assert_almost_equal(log_marginal_likelihood, bayesridge.scores_[-1], 1)\n", "def test_bayesianridge_log_marginal_likelihood_value():\n    # Test log marginal likelihood value\n    X = np.array([[1], [2], [3]])\n    y = np.array([1, 2, 3])\n    X_offset_ = np.mean(X, axis=0)\n    X_scale_ = np.ones(X.shape[1])\n    X_test = (X - X_offset_) / X_scale_\n    eigen_vals_ = np.linalg.eigvalsh(np.dot(X_test.T, X_test))\n    n_samples, n_features = X_test.shape\n    alpha_ = 1.0\n    lambda_ = 1.0\n    coef_ = np.dot(pinvh(np.eye(n_features) + alpha_ * np.dot(X_test.T, X_test)), \n                   np.dot(X_test.T, y))\n    rmse_ = np.sum((y - np.dot(X_test, coef_)) ** 2)\n    score = -lambda_ * fast_logdet(np.eye(n_features) + alpha_ * np.dot(X_test.T, X_test)) \\\n            - n_samples * log(2 * np.pi) - rmse_ / alpha_ - np.sum(lambda_ * coef_ ** 2)\n    br = BayesianRidge(compute_score=True)\n    br.fit(X, y)\n    assert_almost_equal(score, br.scores_[-1], decimal=4)\n"], "sample_814": ["def check_regression_toy(presort, loss):\n    # Check regression on a toy dataset.\n    clf = GradientBoostingRegressor(loss=loss, n_estimators=10,\n                                    random_state=1, presort=presort)\n\n    assert_raises(ValueError, clf.predict, T)\n\n    clf.fit(X, np.array(y))\n    assert_almost_equal(clf.predict(T), true_result)\n    assert_equal(10, len(clf.estimators_))\n\n    deviance_decrease = (clf.train_score_[:-1] - clf.train_score_[1:])\n    assert np.any(deviance_decrease >= 0.0)\n\n    leaves = clf.apply(X)\n    assert_equal(leaves.shape, (6, 10))\n", "def test_check_classification_init_estimators():\n    # Test that the init estimator is set correctly for classification.\n    X, y = make_classification(n_samples=100, n_features=10, random_state=42)\n    \n    clf = GradientBoostingClassifier(loss='deviance', n_estimators=10, \n                                     init='zero')\n    clf.fit(X, y)\n    assert isinstance(clf.init_, ZeroEstimator)\n    \n    clf = GradientBoostingClassifier(loss='deviance', n_estimators=10, \n                                     init=None)\n    clf.fit(X, y)\n    assert isinstance(clf.init_, DummyClassifier)\n    \n    clf = GradientBoostingClassifier(loss='deviance', n_estimators=10, \n                                     init='invalid')\n    with pytest.raises(ValueError):\n        clf.fit(X, y)\n", "def test_gradient_boosting_regressor_oob_improvement():\n    # Test the oob improvement calculation in GradientBoostingRegressor\n    X, y = make_regression(n_samples=100, n_features=10, random_state=0)\n    clf = GradientBoostingRegressor(n_estimators=10, random_state=0,\n                                    subsample=0.5)\n    clf.fit(X, y)\n\n    oob_improvement = clf.oob_improvement_\n    train_score = clf.train_score_\n\n    # Check that oob_improvement is of shape (n_estimators,)\n    assert_equal(oob_improvement.shape, (clf.n_estimators_,))\n\n    # Check for non-negative oob improvement values\n    assert np.all(oob_improvement >= 0)\n\n    # Check that oob_improvement is calculated correctly\n    for i in range(1, clf.n_estimators_):\n        assert_almost_equal(oob_improvement[i],\n                            train_score[i - 1] - train_score[i])\n", "def test_check_classification_init_estimator(presort):\n    # Check classification using different init estimators.\n    X, y = make_classification(n_samples=100, n_features=10,\n                               n_informative=3, n_redundant=0,\n                               random_state=1)\n\n    # check default ZeroEstimator\n    clf = GradientBoostingClassifier(n_estimators=5, random_state=1,\n                                     presort=presort)\n    clf.fit(X, y)\n\n    # check marginal class prior (dummy classifier)\n    clf.init = DummyClassifier(strategy='prior')\n    clf.fit(X, y)\n\n    # check first-per-class predictions of a linear svc\n    base_estimator = make_pipeline(OneHotEncoder(), LinearSVC(random_state=1))\n    base_estimator.fit(X, y)\n    predictions = np.zeros_like(y)\n    for c in np.unique(y):\n        predictions[y == c] = base_estimator.decision_function(X[y == c]).max()\n    clf.init = DummyRegressor(strategy='constant', constant=predictions)\n    clf.fit(X, y)\n\n", "def test_gradient_boosting_oob_switch():\n    # Test if oob predictions are the same regardless of oob_improvement_\n    # being used for early stopping or not.\n    X, y = make_classification(n_samples=1000)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n    gbt = GradientBoostingClassifier(n_estimators=50, subsample=0.5,\n                                     random_state=0)\n    gbt.fit(X_train, y_train)\n    y_pred1 = gbt.predict_proba(X_test)\n\n    gbt = GradientBoostingClassifier(n_estimators=50, subsample=0.5,\n                                     random_state=0, n_iter_no_change=5)\n    gbt.fit(X_train, y_train)\n    y_pred2 = gbt.predict_proba(X_test)\n\n    assert_array_almost_equal(y_pred1, y_pred2)\n"], "sample_815": ["def test_precision_recall_fscore_support_errors():\n    y_true, y_pred, _ = make_prediction(binary=True)\n\n    # Test with invalid beta\n    assert_raises(ValueError, precision_recall_fscore_support,\n                  y_true, y_pred, beta=0.0)\n    assert_raises(ValueError, precision_recall_fscore_support,\n                  y_true, y_pred, beta=-1.0)\n\n    # Test with invalid average\n    assert_raises(ValueError, precision_recall_fscore_support,\n                  y_true, y_pred, average='invalid')\n\n    # Test with invalid pos_label\n    assert_raises(ValueError, precision_recall_fscore_support,\n                  y_true, y_pred, pos_label='invalid')\n    assert_raises(ValueError, precision_recall_fscore_support,\n                  y_true, y_pred, pos_label=None)\n", "def test_precision_recall_fscore_support_errors():\n    y_true, y_pred, _ = make_prediction(binary=True)\n\n    # Test with invalid beta\n    assert_raises(ValueError, precision_recall_fscore_support,\n                  y_true, y_pred, beta=0.0)\n\n    # Test with not supported average\n    assert_raises(ValueError, precision_recall_fscore_support,\n                  y_true, y_pred, average=\"macroo\")\n\n    # Test with multilabel-indicator but without labels\n    y_true = np.random.randint(0, 2, size=(10, 3))\n    y_pred = np.random.randint(0, 2, size=(10, 3))\n    assert_raises(ValueError, precision_recall_fscore_support,\n                  y_true, y_pred)\n", "def test_multilabel_confusion_matrix():\n    # Test multilabel confusion matrix function with different inputs\n    y_true = np.array([[0, 1, 1], [1, 0, 0]])\n    y_pred = np.array([[1, 1, 0], [0, 0, 1]])\n\n    # Test the shape and content of the confusion matrix\n    assert_array_equal(multilabel_confusion_matrix(y_true, y_pred),\n                       np.array([[[0, 1], [0, 1]], [[1, 0], [1, 0]], [[0, 1], [1, 0]]]))\n\n    # Test the shape and content of the confusion matrix with samplewise=True\n    assert_array_equal(multilabel_confusion_matrix(y_true, y_pred, samplewise=True),\n                       np.array([[[1, 1], [1, 1]], [[0, 0], [0, 0]]]))\n\n    # Test that an error is raised when input arrays have different shapes\n    y_true_diff_shape = np.array([[0, 1, 1], [1, 0, 0], [1, 1, 0]])\n    assert_raises(ValueError, multilabel_confusion_matrix, y_true_diff_shape, y_pred)\n\n    # Test that an error is raised when input arrays are not binary\n    y_true_non_binary = np.array([[2, 1, 1], [1, 0, 0]])\n    assert_raises(ValueError, multilabel_confusion_matrix, y_true_non_binary, y_pred)\n", "def test_precision_recall_fscore_support_errors():\n    y_true, y_pred, _ = make_prediction(binary=True)\n\n    # Bad beta\n    assert_raises(ValueError, precision_recall_fscore_support,\n                  y_true, y_pred, beta=0.0)\n\n    # Bad average option\n    assert_raises(ValueError, precision_recall_fscore_support,\n                  [1, 2, 3], [1, 2, 3], average=\"wrong\")\n", "def test_f1_score_binary():\n    y_true = np.array([0, 1, 0, 1, 0, 1])\n    y_pred = np.array([0, 1, 0, 0, 1, 1])\n\n    # true F1-score\n    f1 = f1_score(y_true, y_pred, average='binary')\n\n    # F1-score by hand\n    tp = np.sum(y_true[y_pred == 1] == 1)\n    fp = np.sum(y_true[y_pred == 1] == 0)\n    fn = np.sum(y_true[y_pred == 0] == 1)\n    manual_f1 = 2. * tp / (2 * tp + fp + fn)\n\n    assert_almost_equal(f1, manual_f1)\n    assert_equal(tp, 2)\n    assert_equal(fp, 1)\n    assert_equal(fn, 1)\n"], "sample_816": ["def test_tfidf_vectorizer_with_preprocessor():\n    # Test that the preprocessor is called before tokenization\n    preprocessed_docs = [uppercase(doc) for doc in ALL_FOOD_DOCS]\n    vectorizer = TfidfVectorizer(preprocessor=uppercase).fit(ALL_FOOD_DOCS)\n    vectorized_docs = vectorizer.transform(ALL_FOOD_DOCS)\n    vectorized_preprocessed_docs = vectorizer.transform(preprocessed_docs)\n\n    # The results should be identical since the preprocessor was applied before\n    # tokenization.\n    assert_array_almost_equal(vectorized_docs.toarray(),\n                              vectorized_preprocessed_docs.toarray())\n", "def test_tfidf_vectorizer_transform_with_preprocessor():\n    # Test the transform method with a custom preprocessor.\n    vectorizer = TfidfVectorizer(preprocessor=uppercase)\n    X = vectorizer.fit_transform(JUNK_FOOD_DOCS)\n    assert_equal(X.shape[0], len(JUNK_FOOD_DOCS))\n\n    # Call transform with the same data to check that it gives the same result\n    X2 = vectorizer.transform(JUNK_FOOD_DOCS)\n    assert_array_equal(X.toarray(), X2.toarray())\n\n    # Check with a different type of iterable\n    X3 = vectorizer.transform(np.array(JUNK_FOOD_DOCS))\n    assert_array_equal(X.toarray(), X3.toarray())\n", "def test_tfidfvectorizer_pickle_idf():\n    vec = TfidfVectorizer()\n    X = vec.fit_transform(ALL_FOOD_DOCS)\n\n    # Set idf attr\n    vec.idf_ = np.array([0.1, 0.2, 0.3])\n\n    # Pickle and unpickle\n    vec_pickled = pickle.loads(pickle.dumps(vec))\n\n    # Check idf is preserved\n    assert_array_almost_equal(vec.idf_, vec_pickled.idf_)\n", "def test_vectorizer_stop_words_consistency():\n    stop_words = ['the', 'and']\n    v = CountVectorizer(stop_words=stop_words)\n    v.fit(['this is a test'])\n\n    # Ensure the stop words are correctly set\n    assert_equal(v.get_stop_words(), set(stop_words))\n\n    # Ensure the stop words are correctly used\n    X = v.transform(['the and test'])\n    assert_equal(X.sum(), 1)\n\n    # Ensure the stop words are consistent when tokenizing\n    analyze = v.build_analyzer()\n    tokens = analyze('the and test')\n    assert_equal(tokens, ['test'])\n\n    # Ensure the stop words are consistent when filtering\n    vocabulary = v.vocabulary_\n    terms = [term for term in tokens if term not in stop_words]\n    assert_equal(terms, ['test'])\n\n    # Ensure the stop words are correctly ignored when building vocabulary\n    v2 = CountVectorizer(stop_words=stop_words)\n    v2.fit(['the and test'])\n    assert_not_equal(v2.vocabulary_, {})\n", "def test_tfidf_vectorizer_empty_vocabulary():\n    v = TfidfVectorizer(vocabulary=[])\n    with pytest.raises(ValueError):\n        v.fit([\"hello world\"])\n"], "sample_817": ["def test_variance_threshold_sparse():\n    X = csr_matrix(data)\n    assert_raises(ValueError, VarianceThreshold(threshold=10).fit, X)\n\n    selector = VarianceThreshold()\n    X_trans = selector.fit_transform(X)\n    assert_equal(X_trans.shape[1], 4)\n\n    selector = VarianceThreshold(threshold=0.4)\n    X_trans = selector.fit_transform(X)\n    assert_equal(X_trans.shape[1], 3)\n", "def test_variance_threshold_sparse():\n    X = csc_matrix(data)\n    sel = VarianceThreshold()\n    sel.fit(X)\n    assert_array_equal(sel.variances_, np.var(X.toarray(), axis=0))\n    X_trans = sel.transform(X)\n    assert_equal(X_trans.shape[1], X.shape[1])\n", "def test_variance_threshold_sparse():\n    X = csr_matrix(data)\n    selector = VarianceThreshold()\n    X_trans = selector.fit_transform(X)\n\n    # get the expected transformed data ignoring the zero-variance features\n    expected = np.array([[1, 2, 4],\n                         [2, 2, 5],\n                         [1, 2, 0]])\n\n    assert_array_equal(X_trans.toarray(), expected)\n", "def test_variance_threshold_sparse():\n    X = csr_matrix(data)\n    selector = VarianceThreshold()\n    selector.fit(X)\n\n    # check variances\n    assert_array_equal(selector.variances_, np.var(data, axis=0))\n\n    # check transform\n    X_transformed = selector.transform(X)\n    assert_equal(X_transformed.shape[1], 4)  # one feature has been removed\n\n    # check fit_transform\n    X_transformed2 = selector.fit_transform(X)\n    assert_array_equal(X_transformed.toarray(), X_transformed2.toarray())\n", "def test_variance_threshold_sparse():\n    X = csr_matrix(data)\n    selector = VarianceThreshold()\n    selector.fit(X)\n    assert_array_equal(selector.variances_, np.var(data, axis=0))\n    X_transformed = selector.transform(X)\n    assert_equal(X_transformed.shape[1], 4)  # one feature has zero variance\n\n    # Check get_support and get_support_mask\n    support = selector.get_support()\n    support_mask = selector.get_support_mask()\n    assert_array_equal(support, [True, True, False, True, True])\n    assert_array_equal(support_mask, [True, True, False, True, True])\n"], "sample_818": ["def test_spectral_clustering_sparse():\n    # Test spectral clustering on a sparse matrix\n    S = np.array([[1, 1, 0, 0],\n                  [1, 1, 1, 0],\n                  [0, 1, 1, 1],\n                  [0, 0, 1, 1]])\n    S = sparse.csr_matrix(S)\n    labels = spectral_clustering(S, n_clusters=2, eigen_solver='arpack',\n                                 random_state=0, assign_labels='kmeans')\n    expected_labels = np.array([1, 1, 0, 0])\n    assert_array_equal(labels, expected_labels)\n", "def test_spectral_clustering_sparse_matrix():\n    # Test spectral clustering on sparse matrix\n    S = np.array([[1, 1, 0, 0],\n                  [1, 1, 0, 0],\n                  [0, 0, 1, 1],\n                  [0, 0, 1, 1]])\n    S = sparse.csr_matrix(S)\n    labels = spectral_clustering(S, n_clusters=2, eigen_solver='arpack',\n                                 random_state=0)\n    assert_array_equal(labels, np.array([0, 0, 1, 1]))\n", "def test_spectral_clustering_sparse_matrix():\n    # Test spectral clustering with a sparse matrix as input\n    S = sparse.csr_matrix(np.array([[1, 1, 0, 0],\n                                    [1, 1, 0, 0],\n                                    [0, 0, 1, 1],\n                                    [0, 0, 1, 1]]))\n    labels = spectral_clustering(S, n_clusters=2, eigen_solver='arpack',\n                                 random_state=0)\n    assert_array_equal(labels, np.array([0, 0, 1, 1]))\n", "def test_spectral_clustering_sparse_matrix():\n    # Test that spectral clustering works with sparse matrices\n    rng = check_random_state(0)\n    X = rng.rand(10, 5)\n    S = pairwise_distances(X, metric='euclidean')\n    S = sparse.csr_matrix(np.maximum(S, 0.1))\n\n    labels = spectral_clustering(S, n_clusters=3, eigen_solver='arpack',\n                                 random_state=rng)\n\n    assert_equal(labels.shape, (10,))\n    assert_equal(np.unique(labels).shape, (3,))\n", "def test_spectral_clustering_n_components():\n    # Test that n_components is correctly handled in SpectralClustering\n    X, _ = make_blobs(n_samples=10, n_features=2, centers=2, random_state=0)\n    clustering = SpectralClustering(n_clusters=2, n_components=5,\n                                    assign_labels='discretize', random_state=0)\n    with assert_warns_message(UserWarning, \"n_components should be an integer\"):\n        clustering.fit(X)\n\n    clustering = SpectralClustering(n_clusters=2, n_components='invalid',\n                                    assign_labels='discretize', random_state=0)\n    with assert.raises(ValueError):\n        clustering.fit(X)\n"], "sample_819": ["def test_voting_classifier_predict_proba():\n    # Check predict_proba for proper dimensions and values\n    clf1 = LogisticRegression(random_state=1)\n    clf2 = RandomForestClassifier(random_state=1)\n    eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2)], voting='soft')\n    eclf.fit(X, y)\n\n    proba = eclf.predict_proba(X)\n    assert_array_equal(proba.shape, (len(X), len(np.unique(y))))\n    assert_array_almost_equal(np.sum(proba, axis=1), np.ones(len(X)))\n", "def test_voting_regressor_weights():\n    # Test VotingRegressor with different weights\n    estimators = [('lr', LinearRegression()), ('rf', RandomForestRegressor(n_estimators=10))]\n    weights = [0.5, 0.5]\n    voting_regressor = VotingRegressor(estimators, weights)\n    voting_regressor.fit(X_r, y_r)\n\n    assert_array_almost_equal(\n        voting_regressor.predict(X_r),\n        (estimators[0][1].fit(X_r, y_r).predict(X_r) +\n         estimators[1][1].fit(X_r, y_r).predict(X_r)) / 2.\n    )\n\n    weights = [0.8, 0.2]\n    voting_regressor = VotingRegressor(estimators, weights)\n    voting_regressor.fit(X_r, y_r)\n\n    assert_array_almost_equal(\n        voting_regressor.predict(X_r),\n        (0.8 * estimators[0][1].fit(X_r, y_r).predict(X_r) +\n         0.2 * estimators[1][1].fit(X_r, y_r).predict(X_r))\n    )\n", "def test_voting_regressor_weights():\n    # Test weights with VotingRegressor\n    r1 = LinearRegression()\n    r2 = RandomForestRegressor(n_estimators=10, random_state=0)\n    X = np.array([[1], [2], [3]])\n    y = np.array([1, 2, 3])\n\n    eclf = VotingRegressor(estimators=[('lr', r1), ('rf', r2)],\n                           weights=[1, 2])\n    eclf.fit(X, y)\n\n    assert_array_almost_equal(eclf.predict(X),\n                              (eclf.named_estimators_['lr'].predict(X) +\n                               2 * eclf.named_estimators_['rf'].predict(X)) / 3)\n", "def test_voting_classifier_gridsearch():\n    clf1 = LogisticRegression(random_state=1)\n    clf2 = RandomForestClassifier(random_state=1)\n    clf3 = GaussianNB()\n    eclf = VotingClassifier(estimators=[\n        ('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='soft')\n\n    param_grid = {'lr__C': [1, 100], 'rf__n_estimators': [20, 200]}\n    grid = GridSearchCV(estimator=eclf, param_grid=param_grid, cv=5, refit=False)\n    grid.fit(X, y)\n\n    assert hasattr(grid, 'best_params_')\n    assert hasattr(grid, 'best_score_')\n", "def test_voting_regressor_weights():\n    # Test weights with VotingRegressor\n    X, y = datasets.make_regression(n_samples=100, n_features=5,\n                                    n_informative=3, random_state=0)\n    estimator1 = DummyRegressor(strategy='mean')\n    estimator2 = DummyRegressor(strategy='median')\n    voting = VotingRegressor(estimators=[('mean', estimator1), ('median', estimator2)],\n                             weights=[0.8, 0.2])\n    voting.fit(X, y)\n    assert_almost_equal(voting.predict(X), 0.8 * estimator1.predict(X) +\n                        0.2 * estimator2.predict(X))\n"], "sample_820": ["def test_voting_regressor_weights():\n    # Test that the weights parameter is used correctly in VotingRegressor\n    X, y = datasets.make_regression(n_samples=100, n_features=5, random_state=0)\n    estimator1 = DummyRegressor(strategy='mean')\n    estimator2 = DummyRegressor(strategy='median')\n    voting_regressor = VotingRegressor(estimators=[('mean', estimator1), ('median', estimator2)],\n                                       weights=[0.7, 0.3])\n    voting_regressor.fit(X, y)\n    predictions = voting_regressor.predict(X)\n    assert_almost_equal(predictions, (0.7 * estimator1.predict(X) + 0.3 * estimator2.predict(X)))\n", "def test_voting_classifier_get_params():\n    clf1 = LogisticRegression(random_state=1)\n    clf2 = RandomForestClassifier(random_state=1)\n    clf3 = GaussianNB()\n    eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)])\n\n    params = eclf.get_params()\n    assert 'lr' in params\n    assert 'rf' in params\n    assert 'gnb' in params\n    assert params['lr'] is clf1\n    assert params['rf'] is clf2\n    assert params['gnb'] is clf3\n", "def test_voting_classifier_gridsearch():\n    # Check that VotingClassifier works with GridSearchCV\n    clf1 = LogisticRegression(solver='lbfgs', multi_class='multinomial', random_state=1)\n    clf2 = RandomForestClassifier(n_estimators=50, random_state=1)\n    eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2)])\n\n    params = {'lr__C': [1, 2], 'rf__n_estimators': [10, 20]}\n    grid = GridSearchCV(estimator=eclf, param_grid=params, cv=5)\n    grid.fit(X, y)\n\n    assert hasattr(grid, 'best_params_')\n    assert hasattr(grid, 'best_score_')\n    assert hasattr(grid, 'best_estimator_')\n", "def test_voting_classifier_predict_proba():\n    # Test predict_proba method for VotingClassifier\n    clf1 = LogisticRegression(solver='lbfgs', multi_class='multinomial', random_state=1)\n    clf2 = RandomForestClassifier(n_estimators=10, random_state=1)\n    clf3 = GaussianNB()\n    eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='soft')\n    eclf.fit(X, y)\n\n    # check if predict_proba works\n    y_proba = eclf.predict_proba(X)\n    assert_array_almost_equal(np.sum(y_proba, axis=1), np.ones(len(X)))\n", "def test_voting_regressor_weights():\n    # Test that weights are taken into account for VotingRegressor\n    X, y = datasets.make_regression(n_samples=100, n_features=5)\n    r1 = LinearRegression()\n    r2 = RandomForestRegressor(n_estimators=10, random_state=0)\n    vr = VotingRegressor([('lr', r1), ('rf', r2)], weights=[2, 1])\n    vr.fit(X, y)\n    y_pred = vr.predict(X)\n    y_pred_weighted = (2 * r1.predict(X) + r2.predict(X)) / 3\n    assert_almost_equal(y_pred, y_pred_weighted)\n"], "sample_821": ["def test_affinity_propagation_equal_similarities_and_preferences():\n    # Test affinity propagation with equal similarities and preferences\n    S = np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]])\n    preference = np.array([1, 1, 1])\n\n    assert _equal_similarities_and_preferences(S, preference)\n\n    cluster_centers_indices, labels = affinity_propagation(S, preference)\n    assert_array_equal(cluster_centers_indices, np.arange(3))\n    assert_array_equal(labels, np.arange(3))\n", "def test_affinity_propagation_equal_similarities_and_preferences():\n    # Test the case when all similarities and preferences are equal\n    S = np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]])\n    preference = np.array([1, 1, 1])\n\n    assert _equal_similarities_and_preferences(S, preference)\n\n    # Test the case when all similarities are equal but preferences are not\n    S = np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]])\n    preference = np.array([1, 2, 3])\n\n    assert not _equal_similarities_and_preferences(S, preference)\n\n    # Test the case when all preferences are equal but similarities are not\n    S = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    preference = np.array([1, 1, 1])\n\n    assert not _equal_similarities_and_preferences(S, preference)\n", "def test_affinity_propagation_equal_similarities_and_preferences():\n    # Test the case when all similarities and preferences are equal\n    S = np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]])\n    preference = np.array([1, 1, 1])\n\n    assert _equal_similarities_and_preferences(S, preference)\n\n    cluster_centers_indices, labels = affinity_propagation(S, preference)\n    assert_array_equal(cluster_centers_indices, np.arange(3))\n    assert_array_equal(labels, np.arange(3))\n\n    # Test the case when all similarities are equal but preferences are not\n    preference = np.array([0.5, 0.5, 0.5])\n    cluster_centers_indices, labels = affinity_propagation(S, preference)\n    assert_array_equal(cluster_centers_indices, np.array([0]))\n    assert_array_equal(labels, np.array([0, 0, 0]))\n", "def test_affinity_propagation_equal_similarities_and_preferences():\n    # Test that affinity_propagation behaves as expected when all similarities\n    # and preferences are equal.\n\n    # Generate a matrix of equal similarities\n    S = np.ones((10, 10))\n\n    # Set the preference to be equal to the similarities\n    preference = np.ones(10)\n\n    # Run affinity propagation\n    cluster_centers_indices, labels = affinity_propagation(S, preference)\n\n    # In this case, we expect all points to be assigned to the same cluster\n    assert_array_equal(labels, np.zeros(10))\n", "def test_affinity_propagation_equal_similarities_and_preferences():\n    # Test the case where all similarities and preferences are equal.\n    S = np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]])\n    preference = np.array([1, 1, 1])\n\n    assert _equal_similarities_and_preferences(S, preference)\n    cluster_centers_indices, labels = affinity_propagation(S, preference)\n    assert_array_equal(cluster_centers_indices, np.arange(3))\n    assert_array_equal(labels, np.arange(3))\n\n    # Test the case where all similarities are equal but preferences are not.\n    preference = np.array([0.5, 0.5, 0.5])\n    cluster_centers_indices, labels = affinity_propagation(S, preference)\n    assert_array_equal(cluster_centers_indices, np.array([0]))\n    assert_array_equal(labels, np.array([0, 0, 0]))\n"], "sample_822": ["def test_pairwise_distances_chunked_sparse():\n    # Test that pairwise_distances_chunked works with sparse matrices\n    X = csr_matrix(np.random.rand(10, 5))\n    Y = csr_matrix(np.random.rand(10, 5))\n\n        return (D_chunk.sum(axis=1),)\n\n    gen = pairwise_distances_chunked(X, Y, reduce_func=reduce_func)\n    result = next(gen)\n\n    assert isinstance(result, tuple)\n    assert len(result) == 1\n    assert result[0].shape == (X.shape[0],)\n", "def test_pairwise_distances_argmin_min_dtype():\n    # Test the return type of pairwise_distances_argmin_min\n    X = np.random.randn(10, 5).astype(np.float32)\n    Y = np.random.randn(8, 5).astype(np.float32)\n\n    dist_min_indices, dist_min_values = pairwise_distances_argmin_min(X, Y)\n\n    assert dist_min_indices.dtype == np.int64\n    assert dist_min_values.dtype == np.float32\n\n    X = np.random.randn(10, 5).astype(np.float64)\n    Y = np.random.randn(8, 5).astype(np.float64)\n\n    dist_min_indices, dist_min_values = pairwise_distances_argmin_min(X, Y)\n\n    assert dist_min_indices.dtype == np.int64\n    assert dist_min_values.dtype == np.float64\n", "def test_pairwise_distances_chunked_reduce_on_slice():\n    # Test pairwise_distances_chunked with a reduction function that operates\n    # on slices of the distance matrix\n    X = np.random.RandomState(0).rand(5, 3)\n        return D_chunk.sum(axis=1)\n    gen = pairwise_distances_chunked(X, reduce_func=reduce_func)\n    result = next(gen)\n    assert_array_almost_equal(result, euclidean_distances(X).sum(axis=1))\n", "def test_pairwise_distances_chunked_reduce_on_2d_array():\n    # Test that the reduce_func is applied to 2D arrays when using\n    # pairwise_distances_chunked.\n\n        return D_chunk.sum(axis=1)\n\n    X = np.random.RandomState(0).rand(5, 3)\n    gen = pairwise_distances_chunked(X, reduce_func=reduce_func)\n    result = next(gen)\n\n    assert_array_almost_equal(result, euclidean_distances(X).sum(axis=1))\n", "def test_pairwise_distances_chunked_reduce_func():\n    # Test pairwise_distances_chunked with a reduce function\n    X = np.random.RandomState(0).rand(5, 3)\n    D_chunk = next(pairwise_distances_chunked(X, reduce_func=lambda x, y: x))\n    assert_array_almost_equal(D_chunk, euclidean_distances(X))\n"], "sample_823": ["def test_pairwise_distances_reduce_func_not_called():\n    # Test that pairwise_distances_chunked does not call the reduce function\n    # when Y is None and X is a single row.\n    X = np.random.rand(1, 10)\n        assert False, \"reduce_func should not be called\"\n    list(pairwise_distances_chunked(X, reduce_func=reduce_func))\n", "def test_pairwise_distances_chunked_reduce_on_slice():\n    # Test that pairwise_distances_chunked with a reduce function can handle\n    # slices of the distance matrix.\n    X = np.random.RandomState(0).rand(5, 3)\n    D = pairwise_distances(X)\n\n        return (D_chunk.sum(axis=1), [start + i for i in range(D_chunk.shape[0])])\n\n    gen = pairwise_distances_chunked(X, reduce_func=reduce_func)\n    sums, indices = next(gen)\n\n    assert_array_almost_equal(sums, D.sum(axis=1))\n    assert_array_equal(indices, np.arange(D.shape[0]))\n", "def test_pairwise_distances_chunked_non_negative():\n    # Test that pairwise_distances_chunked returns a non-negative distance\n    # matrix when metric is 'euclidean' or 'manhattan'.\n    X = np.random.RandomState(0).rand(5, 3)\n    Y = np.random.RandomState(1).rand(4, 3)\n\n    for metric in ['euclidean', 'manhattan']:\n        gen = pairwise_distances_chunked(X, Y, metric=metric,\n                                         working_memory=0)\n        for chunk in gen:\n            assert_array_equal(chunk >= 0, np.ones_like(chunk, dtype='bool'))\n", "def test_pairwise_distances_chunked_reduce_on_2d():\n    # Test that reduce_func can return 2D arrays\n    X = np.random.RandomState(0).rand(5, 3)\n        assert D_chunk.shape[1] == 5\n        return D_chunk[:2], D_chunk[2:]\n    gen = pairwise_distances_chunked(X, reduce_func=reduce_func)\n    D1, D2 = next(gen)\n    assert D1.shape == (2, 5)\n    assert D2.shape == (3, 5)\n", "def test_pairwise_distances_chunked_reduce_on_slice():\n    # Check that the reduce function is applied to a slice of the distance\n    # matrix when using pairwise_distances_chunked with a reduce function.\n\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    Y = np.array([[7, 8], [9, 10]])\n\n        return D_chunk[:1]  # Return only the first row of each chunk\n\n    gen = pairwise_distances_chunked(X, Y, reduce_func=reduce_func)\n    result = next(gen)\n\n    assert_equal(result.shape, (1, Y.shape[0]))\n"], "sample_824": ["def test_pairwise_distances_reduction():\n    # Test pairwise_distances with different reduction functions\n    X = np.random.rand(5, 3)\n    Y = np.random.rand(4, 3)\n\n        return D_chunk.sum(axis=1)\n\n    D_sum = pairwise_distances_chunked(X, Y, reduce_func=reduction_func)\n    D_sum_expected = pairwise_distances(X, Y).sum(axis=1)\n    assert_array_almost_equal(np.concatenate(list(D_sum)), D_sum_expected)\n", "def test_pairwise_distances_argmin_min():\n    # Test that pairwise_distances_argmin_min returns the argmins and the\n    # corresponding minimum distances.\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    Y = np.array([[1, 2], [3, 4], [5, 6]])\n    argmin, min_dist = pairwise_distances_argmin_min(X, Y)\n    assert_array_equal(argmin, [0, 1, 2])\n    assert_array_almost_equal(min_dist, [0., 0., 0.])\n\n    # Check with a random matrix\n    np.random.seed(0)\n    X = np.random.rand(10, 5)\n    Y = np.random.rand(8, 5)\n    argmin, min_dist = pairwise_distances_argmin_min(X, Y)\n    dist_mat = pairwise_distances(X, Y)\n    assert_array_equal(argmin, np.argmin(dist_mat, axis=1))\n    assert_array_almost_equal(min_dist, np.min(dist_mat, axis=1))\n", "def test_pairwise_distances_chunked_reduce_func():\n    # Test that pairwise_distances_chunked returns correct results with a\n    # reduce_func.\n    X = np.random.RandomState(0).randn(5, 3)\n    Y = np.random.RandomState(1).randn(4, 3)\n\n        return (D_chunk.sum(axis=1), D_chunk.mean(axis=1))\n\n    gen = pairwise_distances_chunked(X, Y, reduce_func=reduce_func)\n    sums, means = next(gen)\n\n    D = pairwise_distances(X, Y)\n    assert_array_almost_equal(sums, D.sum(axis=1))\n    assert_array_almost_equal(means, D.mean(axis=1))\n", "def test_pairwise_distances_chunked_reduce_func():\n    # Test pairwise_distances_chunked with a reduce function\n    X = np.random.RandomState(0).rand(5, 3)\n    D_chunk = next(pairwise_distances_chunked(X, reduce_func=lambda x, y: x.sum(axis=1)))\n    assert_array_almost_equal(D_chunk, euclidean_distances(X).sum(axis=1))\n", "def test_pairwise_distances_argmin_min():\n    # Test that pairwise_distances_argmin_min returns the correct indices\n    # and distances for a given distance matrix.\n    X = np.array([[1, 2], [3, 4]])\n    Y = np.array([[5, 6], [7, 8]])\n    D = euclidean_distances(X, Y)\n    argmin, min_dist = pairwise_distances_argmin_min(X, Y)\n    assert_array_equal(argmin, D.argmin(axis=1))\n    assert_array_almost_equal(min_dist, D.min(axis=1))\n"], "sample_825": ["def test_pls_regressor_scale():\n    # Test that PLSRegressor works with and without scaling\n    X, Y = load_linnerud(return_X_y=True)\n    pls = pls_.PLSRegression(n_components=2, scale=False)\n    pls.fit(X, Y)\n    assert_warns(UserWarning, pls.fit, X, Y)\n\n    X_std = StandardScaler().fit_transform(X)\n    Y_std = StandardScaler().fit_transform(Y)\n    pls = pls_.PLSRegression(n_components=2, scale=False)\n    pls.fit(X_std, Y_std)\n\n    X_std2, Y_std2 = pls_.PLSRegression(scale=True).fit_transform(X, Y)\n    assert_array_almost_equal(X_std2, X_std)\n    assert_array_almost_equal(Y_std2, Y_std)\n", "def test_pls_regression():\n    # Test PLS regression on a simple dataset\n    X = np.array([[1., 2.], [3., 4.], [5., 6.]])\n    Y = np.array([1., 2., 3.])\n\n    pls = pls_.PLSRegression(n_components=1)\n    pls.fit(X, Y)\n\n    # Check the shape of the weights and loadings\n    assert_equal(pls.x_weights_.shape, (X.shape[1], 1))\n    assert_equal(pls.y_weights_.shape, (1,))\n    assert_equal(pls.x_loadings_.shape, (X.shape[1], 1))\n\n    # Check the predictions\n    Y_pred = pls.predict(X)\n    assert_array_almost_equal(Y_pred, Y)\n", "def test_pls_regression():\n    # Test PLS regression\n    X, Y = load_linnerud(return_X_y=True)\n    pls = pls_.PLSRegression(n_components=2)\n    pls.fit(X, Y)\n    assert_equal(pls.x_weights_.shape[1], 2)\n    Y_pred = pls.predict(X)\n    assert_array_almost_equal(Y, Y_pred, decimal=1)\n", "def test_pls_scaling():\n    # Test that PLS transforms the data correctly when scaling is applied\n\n    # Generate some random data\n    rng = check_random_state(0)\n    X = rng.rand(100, 5)\n    Y = rng.rand(100, 3)\n\n    # Create a PLS object with and without scaling\n    pls_scaled = pls_.PLSRegression(scale=True)\n    pls_unscaled = pls_.PLSRegression(scale=False)\n\n    # Fit both objects to the data\n    pls_scaled.fit(X, Y)\n    pls_unscaled.fit(X, Y)\n\n    # Check that the x_mean_ and x_std_ attributes are set when scaling is True\n    assert_array_equal(pls_scaled.x_mean_, np.mean(X, axis=0))\n    assert_array_equal(pls_scaled.x_std_, np.std(X, axis=0, ddof=1))\n\n    # Check that the x_mean_ and x_std_ attributes are not set when scaling is False\n    assert not hasattr(pls_unscaled, 'x_mean_')\n    assert not hasattr(pls_unscaled, 'x_std_')\n\n    # Check that the transformed data has zero mean and unit variance when scaling is True\n    X_scaled = pls_scaled.transform(X)\n    assert_array_almost_equal(np.mean(X_scaled, axis=0), np.zeros(X.shape[1]))\n    assert_array_almost_equal(np.var(X_scaled, axis=0, ddof=1), np.ones(X.shape[1]))\n\n    # Check that the transformed data does not have zero mean or unit variance when scaling is False\n    X_unscaled = pls_unscaled.transform(X)\n    assert not np.allclose(np.mean(X_unscaled, axis=0), np.zeros(X.shape[1]))\n    assert not np.allclose(np.var(X_unscaled, axis=0, ddof=1), np.ones(X.shape[1]))\n", "def test_pls_canonical():\n    # Test PLS canonical on a simple dataset\n    X = np.array([[1., 0., 1.], [0., 1., 1.], [1., 1., 0.],\n                  [1., 0., 0.], [0., 1., 0.]])\n    Y = np.array([[1., 0., 1.], [0., 1., 1.], [1., 1., 0.],\n                  [1., 0., 0.], [0., 1., 0.]])\n    pls_ca = PLSCanonical(n_components=3)\n    X_r, Y_r = pls_ca.fit_transform(X, Y)\n\n    # check shape of fitted matrices\n    assert_equal(pls_ca.x_weights_.shape, (3, 3))\n    assert_equal(pls_ca.y_weights_.shape, (3, 3))\n    assert_equal(pls_ca.x_scores_.shape, (5, 3))\n    assert_equal(pls_ca.y_scores_.shape, (5, 3))\n\n    # check that first component of X scores is correlated with Y scores\n    assert_approx_equal(np.corrcoef(X_r[:, 0], Y_r[:, 0])[0, 1], 1., 10)\n"], "sample_826": ["def test_one_hot_encoder_handle_unknown():\n    # Test the handle_unknown parameter of OneHotEncoder\n    X = [['Male', 1], ['Female', 3], ['Female', 2]]\n    ohe = OneHotEncoder(handle_unknown='ignore')\n    ohe.fit(X)\n\n    # Try encoding a new value\n    X_new = [['Unknown', 1]]\n\n    # With handle_unknown='ignore', unknown values should be encoded as all zeros\n    expected_output = sparse.csr_matrix(np.array([[0., 0., 1., 0., 0.]]))\n    assert_array_equal(toarray(ohe.transform(X_new)), toarray(expected_output))\n\n    # Try encoding a new value with handle_unknown='error'\n    ohe = OneHotEncoder(handle_unknown='error')\n    ohe.fit(X)\n    assert_raises(ValueError, ohe.transform, X_new)\n", "def test_ordinal_encoder_categories():\n    # Test that categories are correctly set after fitting\n    enc = OrdinalEncoder(categories='auto')\n    X = [['Male', 1], ['Female', 3], ['Female', 2]]\n    enc.fit(X)\n    assert_array_equal(enc.categories_, [np.array(['Female', 'Male'], dtype=object), np.array([1, 2, 3], dtype=object)])\n\n    # Test that categories can be set manually\n    enc = OrdinalEncoder(categories=[['Female', 'Male'], [1, 2, 3]])\n    enc.fit(X)\n    assert_array_equal(enc.categories_, [np.array(['Female', 'Male'], dtype=object), np.array([1, 2, 3], dtype=object)])\n", "def test_one_hot_encoder_handle_unknown():\n    # Test the handle_unknown parameter\n\n    X = np.array([[0, 1], [1, 0]])\n\n    ohe = OneHotEncoder(handle_unknown='error')\n    ohe.fit(X)\n\n    # Test that an error is raised when encountering an unknown category\n    X_test = np.array([[2, 1]])\n    assert_raises(ValueError, ohe.transform, X_test)\n\n    ohe = OneHotEncoder(handle_unknown='ignore')\n    ohe.fit(X)\n\n    # Test that unknown categories are ignored\n    X_test = np.array([[2, 1]])\n    X_test_transformed = ohe.transform(X_test)\n    assert_array_equal(toarray(X_test_transformed), [[0., 0., 0.]])\n", "def test_one_hot_encoder_inverse_transform():\n    # Test inverse transform with an array and a sparse matrix\n    X = [['Male', 1], ['Female', 3], ['Female', 2]]\n    enc = OneHotEncoder()\n    X_tr = enc.fit_transform(X)\n\n    # Inverse transform from array\n    X_inv = enc.inverse_transform(toarray(X_tr))\n    assert_array_equal(X, X_inv)\n\n    # Inverse transform from sparse matrix\n    X_inv = enc.inverse_transform(X_tr)\n    assert_array_equal(X, X_inv)\n", "def test_one_hot_encoder_inverse_transform():\n    # Test the inverse transform method of OneHotEncoder\n    X = [['Male', 1], ['Female', 3], ['Female', 2]]\n    enc = OneHotEncoder()\n    X_tr = enc.fit_transform(X)\n    assert_array_equal(enc.inverse_transform(toarray(X_tr)), X)\n\n    # test inverse transform with dropped category\n    enc = OneHotEncoder(drop='first')\n    X_tr = enc.fit_transform(X)\n    assert_array_equal(enc.inverse_transform(toarray(X_tr)),\n                       [['Female', 1], ['Male', 2], ['Male', 3]])\n"], "sample_827": ["def test_inplace_swap_column():\n    # Test swapping two columns of a CSR matrix in-place\n    X = sp.csr_matrix(np.array([[1, 2, 3], [4, 5, 6]]))\n    inplace_swap_column(X, 0, 2)\n    expected_X = sp.csr_matrix(np.array([[3, 2, 1], [6, 5, 4]]))\n    assert_array_equal(X.toarray(), expected_X.toarray())\n\n    # Test swapping two columns of a CSC matrix in-place\n    X = sp.csc_matrix(np.array([[1, 2, 3], [4, 5, 6]]))\n    inplace_swap_column(X, 0, 2)\n    expected_X = sp.csc_matrix(np.array([[3, 2, 1], [6, 5, 4]]))\n    assert_array_equal(X.toarray(), expected_X.toarray())\n", "def test_inplace_swap_column():\n    # Test swapping two columns of a CSR matrix\n    X = sp.csr_matrix(np.array([[1, 2, 3], [4, 5, 6]]))\n    inplace_swap_column(X, 0, 2)\n    expected = np.array([[3, 2, 1], [6, 5, 4]])\n    assert_array_equal(X.toarray(), expected)\n\n    # Test swapping two columns of a CSC matrix\n    X = sp.csc_matrix(np.array([[1, 2, 3], [4, 5, 6]]))\n    inplace_swap_column(X, 0, 2)\n    expected = np.array([[3, 2, 1], [6, 5, 4]])\n    assert_array_equal(X.toarray(), expected)\n", "def test_inplace_swap_row_csc():\n    # Test inplace swapping of rows in CSC matrix\n    X = sp.csc_matrix(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]))\n    inplace_swap_row_csc(X, 0, 2)\n    expected = np.array([[7, 8, 9], [4, 5, 6], [1, 2, 3]])\n    assert_array_equal(X.toarray(), expected)\n    inplace_swap_row_csc(X, 1, 2)\n    expected = np.array([[7, 8, 9], [1, 2, 3], [4, 5, 6]])\n    assert_array_equal(X.toarray(), expected)\n", "def test_inplace_swap_row_csc():\n    X = sp.csc_matrix(np.array([[1, 2], [3, 4]]))\n    inplace_swap_row_csc(X, 0, 1)\n    assert_array_equal(X.toarray(), np.array([[3, 4], [1, 2]]))\n", "def test_inplace_swap_row_csr():\n    # Test swapping two rows of a CSR matrix in-place.\n    X = sp.csr_matrix(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]))\n    inplace_swap_row_csr(X, 0, 2)\n    expected_X = sp.csr_matrix(np.array([[7, 8, 9], [4, 5, 6], [1, 2, 3]]))\n    assert_array_equal(X.toarray(), expected_X.toarray())\n"], "sample_828": ["def test_pairwise_distances_upcast():\n    # Test that _euclidean_distances_upcast computes the same result as\n    # euclidean_distances for both dense and sparse inputs, while using less\n    # memory.\n\n    X = np.random.RandomState(0).randn(100, 100)\n    Y = np.random.RandomState(1).randn(100, 100)\n\n    X_sparse = csr_matrix(X)\n    Y_sparse = csr_matrix(Y)\n\n    for X_, Y_ in [(X, Y), (X_sparse, Y_sparse)]:\n        distances_dense = euclidean_distances(X_, Y_)\n        distances_upcast = _euclidean_distances_upcast(X_, Y_)\n\n        assert_array_almost_equal(distances_upcast, distances_dense)\n\n    # Check that upcasting uses less memory\n    mem_dense = (sys.getsizeof(distances_dense) +\n                 sys.getsizeof(X_) + sys.getsizeof(Y_))\n\n    # peak memory usage of upcast function\n    mem_upcast = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n\n    assert_greater(mem_dense, mem_upcast)\n", "def test_pairwise_distances_chunked_non_sparse():\n    # Check that pairwise_distances_chunked works correctly on dense inputs.\n    # Also check for correct behavior with reduce_func.\n\n    X = np.random.RandomState(0).rand(5, 3)\n    Y = np.random.RandomState(1).rand(4, 3)\n\n        return (D_chunk + 1,\n                D_chunk.sum(axis=1),\n                [x for x in D_chunk])\n\n    gen = pairwise_distances_chunked(X, Y, reduce_func=reduce_func)\n    outputs = list(gen)\n\n    D = pairwise_distances(X, Y)\n\n    assert len(outputs) == 1\n    chunk = outputs[0]\n    assert isinstance(chunk, tuple)\n    assert len(chunk) == 3\n    D_chunk, sum_D_chunk, D_chunk_list = chunk\n\n    assert_array_almost_equal(D, D_chunk - 1)\n    assert_array_almost_equal(D.sum(axis=1), sum_D_chunk)\n    assert len(D_chunk_list) == 4\n    for i, row in enumerate(D):\n        assert_array_almost_equal(row, D_chunk_list[i])\n", "def test_pairwise_distances_chunked_reduce_func():\n    # Test pairwise_distances_chunked with a reduce function\n    X = np.random.RandomState(0).rand(5, 3)\n    D_chunk = list(pairwise_distances_chunked(X, reduce_func=lambda x, y: x.sum(axis=1)))[0]\n    assert_array_almost_equal(D_chunk, euclidean_distances(X).sum(axis=1))\n", "def test_pairwise_distances_chunked_reduce_on_slice():\n    # Test that pairwise_distances_chunked can apply a reduction function\n    # on each chunk of the distance matrix, and that it returns the expected\n    # results.\n    X = np.random.RandomState(0).rand(5, 3)\n        return D_chunk.sum(axis=1)\n\n    gen = pairwise_distances_chunked(X, reduce_func=reduce_func)\n    result = next(gen)\n    assert_array_almost_equal(result, euclidean_distances(X).sum(axis=1))\n", "def test_pairwise_distances_upcast():\n    # Test that _euclidean_distances_upcast returns the correct result\n    # when X and Y have different dtypes.\n    X = np.array([[1, 2], [3, 4]], dtype=np.float32)\n    Y = np.array([[5, 6], [7, 8]], dtype=np.float64)\n\n    distances = _euclidean_distances_upcast(X, None, Y, None)\n    expected_distances = euclidean_distances(X, Y)\n\n    assert_array_almost_equal(distances, expected_distances)\n"], "sample_829": ["def test_incremental_pca_sparse():\n    # Test that IncrementalPCA works on sparse matrices\n    X = sparse.csr_matrix(iris.data)\n    ipca = IncrementalPCA(n_components=2, batch_size=10)\n    X_trans = ipca.fit_transform(X)\n    assert X_trans.shape == (iris.data.shape[0], 2)\n    assert_allclose_dense_sparse(ipca.transform(X), X_trans)\n", "def test_incremental_pca_batch_size():\n    # Test that incremental PCA works with different batch sizes\n    X = iris.data\n    ipca = IncrementalPCA(n_components=2)\n\n    # Test with batch size equal to the number of samples\n    ipca.fit(X)\n    assert ipca.components_.shape == (2, 4)\n\n    # Test with batch size less than the number of samples\n    ipca = IncrementalPCA(n_components=2, batch_size=50)\n    ipca.fit(X)\n    assert ipca.components_.shape == (2, 4)\n\n    # Test with batch size greater than the number of samples\n    ipca = IncrementalPCA(n_components=2, batch_size=200)\n    ipca.fit(X)\n    assert ipca.components_.shape == (2, 4)\n", "def test_incremental_pca_batch_size():\n    X = iris.data\n    ipca = IncrementalPCA(n_components=2, batch_size=20)\n    ipca.fit(X)\n    assert ipca.components_.shape == (2, X.shape[1])\n", "def test_incremental_pca_inverse_transform():\n    \"\"\"Test that IncrementalPCA.inverse_transform works correctly\"\"\"\n    X = iris.data\n    ipca = IncrementalPCA(n_components=2)\n    X_reduced = ipca.fit_transform(X)\n    X_original = ipca.inverse_transform(X_reduced)\n    assert_array_almost_equal(X, X_original)\n", "def test_incremental_pca_batch_size():\n    # Test that the batch size is taken into account\n    X = iris.data\n    ipca = IncrementalPCA(n_components=2, batch_size=50)\n    ipca.fit(X)\n    assert ipca.components_.shape == (2, X.shape[1])\n    assert ipca.explained_variance_ratio_.shape == (2,)\n"], "sample_830": ["def test_get_blas_info():\n    blas_info = _get_blas_info()\n    assert isinstance(blas_info, dict)\n    assert 'macros' in blas_info\n    assert 'lib_dirs' in blas_info\n    assert 'cblas_libs' in blas_info\n", "def test_get_deps_info():\n    deps_info = _get_deps_info()\n    assert isinstance(deps_info, dict)\n    for dep in [\"pip\", \"setuptools\", \"sklearn\", \"numpy\", \"scipy\", \"Cython\", \"pandas\", \"matplotlib\"]:\n        assert dep in deps_info\n        if deps_info[dep] is not None:\n            assert isinstance(deps_info[dep], str)\n", "def test_get_sys_info():\n    sys_info = _get_sys_info()\n    assert 'python' in sys_info\n    assert 'executable' in sys_info\n    assert 'machine' in sys_info\n\n    # Check that the values are not empty\n    assert sys_info['python']\n    assert sys_info['executable']\n    assert sys_info['machine']\n", "def test_get_sys_info():\n    sys_info = _get_sys_info()\n    assert 'python' in sys_info\n    assert 'executable' in sys_info\n    assert 'machine' in sys_info\n    assert len(sys_info) == 3\n", "def test_get_blas_info():\n    # Test that _get_blas_info returns a dictionary with expected keys\n    blas_info = _get_blas_info()\n    expected_keys = ['macros', 'lib_dirs', 'cblas_libs']\n    assert all(key in blas_info for key in expected_keys)\n    assert all(isinstance(val, str) for val in blas_info.values())\n"], "sample_831": ["def test_export_text_feature_names():\n    # Check that export_text works with a list of feature names\n    clf = DecisionTreeClassifier(random_state=0, max_depth=2)\n    clf.fit(X, y)\n\n    # Export tree with feature names\n    report = export_text(clf, feature_names=['feature_0', 'feature_1'])\n\n    # Check the report contains the feature names\n    assert_in('feature_0', report)\n    assert_in('feature_1', report)\n\n    # Export tree without feature names\n    report = export_text(clf)\n\n    # Check the report contains the generic feature names\n    assert_in('feature_0', report)\n    assert_in('feature_1', report)\n", "def test_export_text_max_depth():\n    clf = DecisionTreeClassifier(random_state=0, max_depth=5)\n    clf.fit(X, y)\n\n    # Check max_depth parameter and leaf values\n    report = export_text(clf, max_depth=3)\n    leaves = [line for line in report.split('\\n') if 'class:' in line]\n    assert_less_equal(len(leaves), 8)  # Each leaf has two lines\n", "def test_export_text():\n    # Check export_text function\n    clf = DecisionTreeClassifier(random_state=0, max_depth=3)\n    clf.fit(X, y)\n    report = export_text(clf, feature_names=[\"feature_0\", \"feature_1\"])\n    assert_in(\"feature_0 <= -1.00\", report)\n    assert_in(\"class: -1\", report)\n\n    # Check truncation\n    report = export_text(clf, max_depth=1)\n    assert_in(\"truncated branch of depth 2\", report)\n\n    # Check export_text with multi-output\n    clf = DecisionTreeRegressor(random_state=0, max_depth=3)\n    clf.fit(X, y2)\n    report = export_text(clf, feature_names=[\"feature_0\", \"feature_1\"])\n    assert_in(\"value: [1.00, 2.00]\", report)\n\n    # Check export_text with classification and show_weights\n    clf = DecisionTreeClassifier(random_state=0, max_depth=3)\n    clf.fit(X, y)\n    report = export_text(clf, feature_names=[\"feature_0\", \"feature_1\"],\n                         show_weights=True)\n    assert_in(\"weights: [1.00, 0.00]\", report)\n", "def test_export_text():\n    # Check export_text on a simple tree\n    clf = DecisionTreeClassifier(max_depth=3, random_state=0)\n    clf.fit(X, y)\n    report = export_text(clf, feature_names=[\"feature0\", \"feature1\"])\n    assert_in(\"feature0\", report)\n    assert_in(\"feature1\", report)\n    assert_in(\"class: 0\", report)\n    assert_in(\"class: 1\", report)\n\n    # Check export_text with max_depth\n    report = export_text(clf, feature_names=[\"feature0\", \"feature1\"], max_depth=0)\n    assert_not_in(\"feature0\", report)\n    assert_not_in(\"feature1\", report)\n    assert_not_in(\"class: 0\", report)\n    assert_not_in(\"class: 1\", report)\n\n    # Check export_text with decimals\n    report = export_text(clf, feature_names=[\"feature0\", \"feature1\"], decimals=5)\n    assert_in(\"feature0\", report)\n    assert_in(\"feature1\", report)\n    assert_in(\"class: 0\", report)\n    assert_in(\"class: 1\", report)\n\n    # Check export_text with show_weights\n    report = export_text(clf, feature_names=[\"feature0\", \"feature1\"], show_weights=True)\n    assert_in(\"feature0\", report)\n    assert_in(\"feature1\", report)\n    assert_in(\"weights:\", report)\n", "def test_export_text():\n    # Check export_text on a simple tree\n    clf = DecisionTreeClassifier(random_state=0, max_depth=2)\n    clf.fit(X, y)\n\n    text = export_text(clf, feature_names=['feature0', 'feature1'])\n    assert_in('--- feature0 <= 0.00', text)\n    assert_in('class: -1', text)\n    assert_in('--- feature1 <= 0.00', text)\n    assert_in('class: 1', text)\n\n    # Test with a regressor\n    reg = DecisionTreeRegressor(random_state=0, max_depth=2)\n    reg.fit(X, [1, 2, 3, 4, 5, 6])\n    text = export_text(reg, feature_names=['feature0', 'feature1'])\n    assert_in('--- feature0 <= 0.00', text)\n    assert_in('value: ', text)\n\n    # Test max_depth param\n    clf = DecisionTreeClassifier(random_state=0, max_depth=3)\n    clf.fit(X, y)\n    text = export_text(clf, max_depth=0, feature_names=['feature0', 'feature1'])\n    assert_equal(text.count('truncated branch'), 2)\n"], "sample_832": ["def test_bayesian_ridge_log_marginal_likelihood_value():\n    # Test the value of the log marginal likelihood for BayesianRidge\n\n    # Generate some data\n    rng = check_random_state(0)\n    n_samples, n_features = 10, 5\n    X = rng.rand(n_samples, n_features)\n    y = rng.rand(n_samples)\n\n    # Fit a BayesianRidge model and compute the log marginal likelihood\n    br = BayesianRidge(compute_score=True)\n    br.fit(X, y)\n    score = br.scores_[-1]\n\n    # Compute the log marginal likelihood manually\n    alpha_1 = br.alpha_1\n    alpha_2 = br.alpha_2\n    lambda_1 = br.lambda_1\n    lambda_2 = br.lambda_2\n    alpha_ = br.alpha_\n    lambda_ = br.lambda_\n    sigma_ = br.sigma_\n\n    # compute the log of the determinant of the posterior covariance.\n    logdet_sigma = fast_logdet(sigma_)\n    log_marginal_likelihood = (lambda_1 * log(lambda_) - lambda_2 * lambda_ +\n                               alpha_1 * log(alpha_) - alpha_2 * alpha_ +\n                               0.5 * (logdet_sigma + n_samples * log(alpha_)))\n\n    assert_almost_equal(score, log_marginal_likelihood)\n", "def test_bayesianridge_log_marginal_likelihood():\n    # Test that the log marginal likelihood of Bayesian Ridge is correct\n    X = diabetes.data\n    y = diabetes.target\n    n_samples, n_features = X.shape\n\n    # Fit BayesianRidge with a small number of iterations\n    br = BayesianRidge(n_iter=10, compute_score=True)\n    br.fit(X, y)\n\n    # Compute the log marginal likelihood manually\n    alpha_1 = br.alpha_1\n    alpha_2 = br.alpha_2\n    lambda_1 = br.lambda_1\n    lambda_2 = br.lambda_2\n    alpha_ = br.alpha_\n    lambda_ = br.lambda_\n    coef_ = br.coef_\n    rmse_ = np.sum((y - np.dot(X, coef_)) ** 2)\n\n    # Use Woodbury matrix identity to compute logdet\n    sigma_ = pinvh(np.eye(n_features) / lambda_ +\n                   np.dot(X.T, X) / alpha_)\n    logdet_sigma = fast_logdet(sigma_)\n\n    score = (lambda_1 * log(lambda_) - lambda_2 * lambda_ +\n             alpha_1 * log(alpha_) - alpha_2 * alpha_ +\n             0.5 * (logdet_sigma + n_samples * log(alpha_) -\n                    alpha_ * rmse_ - lambda_ * np.sum(coef_ ** 2)))\n\n    # Compare with the log marginal likelihood computed by the class\n    assert_almost_equal(score, br.scores_[-1], decimal=3)\n", "def test_bayesianridge_log_marginal_likelihood():\n    # Test that the log marginal likelihood is computed correctly for BayesianRidge.\n    X, y = diabetes.data, diabetes.target\n    n_samples, n_features = X.shape\n\n    # Fit the model and compute the log marginal likelihood\n    br = BayesianRidge(compute_score=True)\n    br.fit(X, y)\n    ll1 = br.scores_[-1]\n\n    # Compute the log marginal likelihood manually\n    alpha_1 = br.alpha_1\n    alpha_2 = br.alpha_2\n    lambda_1 = br.lambda_1\n    lambda_2 = br.lambda_2\n    alpha_ = br.alpha_\n    lambda_ = br.lambda_\n    sigma_ = br.sigma_\n    eigen_vals_ = np.linalg.eigvalsh(np.dot(X.T, X))\n\n        n = n_samples\n        d = n_features\n        logdet_sigma = -np.sum(np.log(lambda_ + alpha_ * eigen_vals_))\n        ll = (lambda_1 * log(lambda_) - lambda_2 * lambda_ +\n              alpha_1 * log(alpha_) - alpha_2 * alpha_ +\n              0.5 * (n * log(alpha_) +\n                     d * log(lambda_) +\n                     logdet_sigma -\n                     n * log(2 * np.pi)))\n        return ll\n\n    ll2 = log_marginal_likelihood(alpha_, lambda_, sigma_, eigen_vals_)\n    assert_almost_equal(ll1, ll2)\n", "def test_bayesian_ridge_log_marginal_likelihood_value():\n    # Test that the log marginal likelihood of BayesianRidge is correct\n    X, y = diabetes.data, diabetes.target\n    n_samples, n_features = X.shape\n\n    # Fit BayesianRidge with a large number of iterations to reach convergence\n    br = BayesianRidge(n_iter=1000)\n    br.fit(X, y)\n\n    # Compute the log marginal likelihood manually\n    alpha_1 = br.alpha_1\n    alpha_2 = br.alpha_2\n    lambda_1 = br.lambda_1\n    lambda_2 = br.lambda_2\n    alpha_ = br.alpha_\n    lambda_ = br.lambda_\n    sigma_ = br.sigma_\n    coef_ = br.coef_\n    rmse_ = np.sum((y - np.dot(X, coef_)) ** 2)\n\n    s = (lambda_1 * np.log(lambda_) - lambda_2 * lambda_)\n    s += alpha_1 * log(alpha_) - alpha_2 * alpha_\n    s += 0.5 * (fast_logdet(sigma_) + n_samples * log(alpha_) +\n                np.sum(np.log(lambda_)))\n    s -= 0.5 * (alpha_ * rmse_ + (lambda_ * coef_ ** 2).sum())\n\n    # Compare with the log marginal likelihood computed by BayesianRidge\n    assert_almost_equal(s, br.scores_[-1])\n", "def test_bayesian_ridge_convergence():\n    X, y = diabetes.data, diabetes.target\n    n_samples, n_features = X.shape\n\n    # Make sure that the BayesianRidge converges and does not overfit.\n    tol = 1e-3\n    bayes = BayesianRidge(tol=tol, fit_intercept=False)\n    bayes.fit(X, y)\n\n    # The posterior weights should be closer to the ones obtained by Ridge.\n    ridge = Ridge(alpha=1.0 / (n_samples * bayes.lambda_), fit_intercept=False)\n    ridge.fit(X, y)\n    assert_array_almost_equal(bayes.coef_, ridge.coef_, decimal=2)\n"], "sample_833": ["def test_logistic_regression_cv():\n    # Test that the cross-validation object works as expected\n    X, y = make_classification(n_samples=100, n_features=10, n_informative=5,\n                               n_redundant=3, n_repeated=2, random_state=42)\n\n    # Test l1 and l2 regularization\n    for penalty in ['l1', 'l2']:\n        # Test that an error is thrown if Cs is not a list of positive values\n        assert_raises(ValueError, LogisticRegressionCV, Cs=[-1, 0, 1],\n                      penalty=penalty)\n\n        # Test that an error is thrown if solver is not one of the allowed solvers\n        assert_raises(ValueError, LogisticRegressionCV, solver='random',\n                      penalty=penalty)\n\n        # Test that the best C and score are returned\n        cv = LogisticRegressionCV(Cs=[1, 10], penalty=penalty)\n        cv.fit(X, y)\n        best_C = cv.C_[0]\n        assert_greater(best_C, 0)\n        assert_greater(cv.scores_[0].max(), 0)\n\n        # Test that refit works as expected\n        cv = LogisticRegressionCV(Cs=[1, 10], penalty=penalty, refit=False)\n        cv.fit(X, y)\n        assert_array_equal(cv.coef_, np.zeros((1, X.shape[1])))\n", "def test_logistic_regression_cv_penalty():\n    # Test that LogisticRegressionCV works with different penalty options\n\n    X, y = make_classification(n_samples=100, n_features=10, n_informative=5,\n                               n_redundant=3, n_repeated=2, n_classes=3,\n                               random_state=0)\n\n    for penalty in ['l1', 'l2', 'elasticnet']:\n        if penalty == 'elasticnet':\n            solver = 'saga'\n            l1_ratios = [0.1, 0.5, 0.9]\n        else:\n            solver = 'liblinear'\n            l1_ratios = None\n\n        clf = LogisticRegressionCV(Cs=[0.1, 1, 10], cv=5, penalty=penalty,\n                                   solver=solver, l1_ratios=l1_ratios)\n        clf.fit(X, y)\n\n        assert_array_equal(np.unique(y), clf.classes_)\n        assert clf.coef_.shape == (3, 10)\n        assert clf.intercept_.shape == (3,)\n", "def test_logistic_regression_solvers():\n    \"\"\"Test logistic regression solvers on a given dataset\"\"\"\n    X, y = make_classification(n_samples=100, n_features=10, random_state=0)\n    X = scale(X)\n\n    for solver in ['liblinear', 'lbfgs', 'newton-cg', 'sag', 'saga']:\n        clf = LogisticRegression(solver=solver, max_iter=1000)\n        clf.fit(X, y)\n        assert_almost_equal(clf.score(X, y), 1.0, decimal=2)\n\n    # Test that an error is raised for invalid solver\n    clf = LogisticRegression(solver='invalid')\n    with pytest.raises(ValueError):\n        clf.fit(X, y)\n", "def test_logistic_regression_solvers():\n    \"\"\"Test that all solvers give the same result on iris dataset\"\"\"\n    X, y = load_iris(return_X_y=True)\n    n_samples, n_features = X.shape\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n                                                        random_state=42)\n\n    solver_names = ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']\n\n    for multi_class in ['ovr', 'multinomial']:\n        for solver_name in solver_names:\n            if multi_class == 'multinomial' and solver_name == 'liblinear':\n                continue\n\n            lr = LogisticRegression(solver=solver_name, max_iter=10000,\n                                    multi_class=multi_class, random_state=42)\n            lr.fit(X_train, y_train)\n\n            assert_greater(lr.score(X_test, y_test), 0.7)\n\n            # check learned parameters are not corrupted\n            assert_array_equal(lr.coef_.shape, (3, n_features))\n            assert_array_equal(lr.intercept_.shape, (3,))\n", "def test_logistic_regression_saga_sparse():\n    # Make sure that saga is working with sparse data\n\n    X, y = make_classification(n_samples=100, n_features=10,\n                               random_state=0)\n\n    X_sparse = sp.csr_matrix(X)\n    X_sparse = sp.dia_matrix(X)\n\n    lr_dense = LogisticRegression(solver='saga', max_iter=1000)\n    lr_sparse = LogisticRegression(solver='saga', max_iter=1000)\n\n    lr_dense.fit(X, y)\n    lr_sparse.fit(X_sparse, y)\n\n    assert_array_almost_equal(lr_dense.coef_, lr_sparse.coef_)\n"], "sample_834": ["def test_nca_init():\n    # Test that the initialization of the transformation works as expected\n    X, y = iris_data, iris_target\n\n    # Test 'auto' init with n_components <= n_classes - 1\n    nca = NeighborhoodComponentsAnalysis(n_components=2, init='auto')\n    nca.fit(X, y)\n    assert_equal(nca.components_.shape, (2, X.shape[1]))\n\n    # Test 'auto' init with n_components > n_classes - 1\n    nca = NeighborhoodComponentsAnalysis(n_components=5, init='auto')\n    nca.fit(X, y)\n    assert_equal(nca.components_.shape, (5, X.shape[1]))\n\n    # Test 'pca' init\n    nca = NeighborhoodComponentsAnalysis(n_components=2, init='pca')\n    nca.fit(X, y)\n    assert_equal(nca.components_.shape, (2, X.shape[1]))\n\n    # Test 'lda' init\n    nca = NeighborhoodComponentsAnalysis(n_components=2, init='lda')\n    nca.fit(X, y)\n    assert_equal(nca.components_.shape, (2, X.shape[1]))\n\n    # Test 'identity' init\n    nca = NeighborhoodComponentsAnalysis(n_components=2, init='identity')\n    nca.fit(X, y)\n    assert_array_almost_equal(nca.components_, np.eye(2, X.shape[1]))\n\n    # Test 'random' init\n    nca = NeighborhoodComponentsAnalysis(n_components=2, init='random',\n                                        random_state=rng)\n    nca.fit(X, y)\n    assert_equal(nca.components_.shape, (2, X.shape[1]))\n\n    # Test init with a numpy array\n    init = rng.randn(2, X.shape[1])\n    nca = NeighborhoodComponentsAnalysis(n_components=2, init=init)\n    nca.fit(X, y)\n    assert_array_almost_equal(nca.components_, init)\n\n    # Test invalid init\n    nca = NeighborhoodComponentsAnalysis(n_components=2, init='invalid')\n    assert_raises(ValueError, nca.fit, X, y)\n", "def test_nca_init():\n    # Test that the initialization of NCA is correct\n    n_components = 2\n    n_features = 4\n    X = np.random.rand(10, n_features)\n\n    # Test 'auto' init\n    nca = NeighborhoodComponentsAnalysis(n_components=n_components,\n                                         init='auto')\n    nca.fit(X, np.random.randint(0, 2, size=X.shape[0]))\n    assert_equal(nca.components_.shape, (n_components, n_features))\n\n    # Test 'pca' init\n    nca = NeighborhoodComponentsAnalysis(n_components=n_components,\n                                         init='pca')\n    nca.fit(X, np.random.randint(0, 2, size=X.shape[0]))\n    assert_equal(nca.components_.shape, (n_components, n_features))\n\n    # Test 'lda' init\n    nca = NeighborhoodComponentsAnalysis(n_components=n_components,\n                                         init='lda')\n    nca.fit(X, np.random.randint(0, 2, size=X.shape[0]))\n    assert_equal(nca.components_.shape, (n_components, n_features))\n\n    # Test 'identity' init\n    nca = NeighborhoodComponentsAnalysis(n_components=n_components,\n                                         init='identity')\n    nca.fit(X, np.random.randint(0, 2, size=X.shape[0]))\n    assert_array_equal(nca.components_, np.eye(n_components, n_features))\n\n    # Test 'random' init\n    nca = NeighborhoodComponentsAnalysis(n_components=n_components,\n                                         init='random', random_state=0)\n    nca.fit(X, np.random.randint(0, 2, size=X.shape[0]))\n    assert_equal(nca.components_.shape, (n_components, n_features))\n", "def test_nca_init_transformation():\n    # Test that the initialization of the transformation matrix is correct\n    n_samples, n_features = 100, 5\n    X, y = make_classification(n_samples=n_samples, n_features=n_features,\n                               n_informative=3, random_state=0)\n    \n    n_components = 3\n    init_pca = NeighborhoodComponentsAnalysis(n_components=n_components,\n                                              init='pca', random_state=0)\n    init_lda = NeighborhoodComponentsAnalysis(n_components=n_components,\n                                              init='lda', random_state=0)\n    init_identity = NeighborhoodComponentsAnalysis(n_components=n_components,\n                                                  init='identity',\n                                                  random_state=0)\n    init_random = NeighborhoodComponentsAnalysis(n_components=n_components,\n                                                init='random', random_state=0)\n    \n    init_pca.fit(X, y)\n    init_lda.fit(X, y)\n    init_identity.fit(X, y)\n    init_random.fit(X, y)\n    \n    assert_array_equal(init_pca.components_.shape, (n_components, n_features))\n    assert_array_equal(init_lda.components_.shape, (n_components, n_features))\n    assert_array_equal(init_identity.components_.shape, (n_components, n_features))\n    assert_array_equal(init_random.components_.shape, (n_components, n_features))\n", "def test_nca_init_auto():\n    # Test that the 'auto' initialization of NCA leads to the expected\n    # initialization, depending on the number of components and classes.\n    X, y = make_classification(n_samples=50, n_features=10, n_informative=5,\n                               n_redundant=3, n_classes=3, random_state=0)\n    n_components = 2\n\n    nca = NeighborhoodComponentsAnalysis(n_components=n_components, init='auto',\n                                         random_state=0)\n    nca.fit(X, y)\n    assert_array_equal(nca.components_.shape, (n_components, X.shape[1]))\n\n    # With two classes and n_components greater than n_classes-1 (=1), init\n    # should be set to pca\n    X, y = make_classification(n_samples=50, n_features=10, n_informative=5,\n                               n_redundant=3, n_classes=2, random_state=0)\n    n_components = 2\n\n    nca = NeighborhoodComponentsAnalysis(n_components=n_components, init='auto',\n                                         random_state=0)\n    nca.fit(X, y)\n    assert_array_equal(nca.components_.shape, (n_components, X.shape[1]))\n\n    # With two classes and n_components equal to n_classes-1 (=1), init should\n    # be set to lda\n    X, y = make_classification(n_samples=50, n_features=10, n_informative=5,\n                               n_redundant=3, n_classes=2, random_state=0)\n    n_components = 1\n\n    nca = NeighborhoodComponentsAnalysis(n_components=n_components, init='auto',\n                                         random_state=0)\n    nca.fit(X, y)\n    assert_array_equal(nca.components_.shape, (n_components, X.shape[1]))\n", "def test_nca_init():\n    # Test that the initialization of the transformation is correct\n    X, y = iris_data, iris_target\n    n_samples, n_features = X.shape\n    n_components = 2\n\n    # 'auto' init\n    nca = NeighborhoodComponentsAnalysis(n_components=n_components, init='auto')\n    nca.fit(X, y)\n    assert_equal(nca.components_.shape, (n_components, n_features))\n\n    # 'pca' init\n    nca = NeighborhoodComponentsAnalysis(n_components=n_components, init='pca')\n    nca.fit(X, y)\n    assert_equal(nca.components_.shape, (n_components, n_features))\n\n    # 'lda' init\n    nca = NeighborhoodComponentsAnalysis(n_components=n_components, init='lda')\n    nca.fit(X, y)\n    assert_equal(nca.components_.shape, (n_components, n_features))\n\n    # 'identity' init\n    nca = NeighborhoodComponentsAnalysis(n_components=n_components, init='identity')\n    nca.fit(X, y)\n    assert_array_equal(nca.components_, np.eye(n_components, n_features))\n\n    # 'random' init\n    nca = NeighborhoodComponentsAnalysis(n_components=n_components, init='random', random_state=0)\n    nca.fit(X, y)\n    assert_equal(nca.components_.shape, (n_components, n_features))\n\n    # numpy array init\n    init = np.random.rand(n_components, n_features)\n    nca = NeighborhoodComponentsAnalysis(n_components=n_components, init=init)\n    nca.fit(X, y)\n    assert_array_equal(nca.components_, init)\n\n    # invalid init\n    nca = NeighborhoodComponentsAnalysis(n_components=n_components, init='invalid')\n    with pytest.raises(ValueError):\n        nca.fit(X, y)\n"], "sample_835": ["def test_adaboost_regressor_with_svr():\n    # Test that AdaBoostRegressor works with SVR as base estimator\n    X, y = boston.data, boston.target\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5,\n                                                        random_state=rng)\n    svr = SVR(kernel='linear')\n    adaboost_regressor = AdaBoostRegressor(base_estimator=svr, n_estimators=10,\n                                           random_state=rng)\n    adaboost_regressor.fit(X_train, y_train)\n    score = adaboost_regressor.score(X_test, y_test)\n    assert score > 0.1\n", "def test_adaboost_regressor_with_sparse_mats():\n    # Test Adaboost with sparse matrix inputs for decision_tree regressor\n    classif = AdaBoostRegressor(base_estimator=DecisionTreeRegressor(max_depth=1),\n                                n_estimators=5, random_state=0)\n    matrices = [csc_matrix(X), csr_matrix(X), coo_matrix(X), dok_matrix(X),\n                lil_matrix(X)]\n    for mat in matrices:\n        classif.fit(mat, y_regr)\n        assert_array_equal(classif.predict(T), [0.5751191291288824, 0.5751191291288824, 0.5751191291288824])\n", "def test_adaboost_regressor_with_sparse_matrices():\n    # Test AdaBoostRegressor with different types of sparse matrices\n    X_sparse = csc_matrix(X)\n    y_sparse = np.array(y_regr)\n\n    # Test with different types of sparse matrices\n    sparse_matrices = [csc_matrix, csr_matrix, coo_matrix, dok_matrix, lil_matrix]\n\n    for sparse_matrix in sparse_matrices:\n        X_sparse = sparse_matrix(X)\n        adaboost = AdaBoostRegressor()\n        adaboost.fit(X_sparse, y_sparse)\n\n        assert_array_equal(adaboost.predict(T), [0.78472222, 1.0, 1.0])\n\n    # Check that we cannot pass sparse matrices to predict\n    with assert_raises(TypeError):\n        adaboost.predict(csc_matrix(T))\n", "def test_adaboost_regressor_with_none_estimator():\n    # Test AdaBoostRegressor with None as estimator\n    X, y = boston.data, boston.target\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\n    adaboost = AdaBoostRegressor(base_estimator=None, n_estimators=10)\n    adaboost.fit(X_train, y_train)\n    assert isinstance(adaboost.base_estimator_, DecisionTreeRegressor)\n    assert adaboost.score(X_test, y_test) > 0.5\n", "def test_adaboost_regressor_with_sparse():\n    # Test AdaBoostRegressor with sparse data\n    X_sparse = csr_matrix(X)\n    y_sparse = np.array(y_regr)\n\n    ada = AdaBoostRegressor(random_state=0)\n    ada.fit(X_sparse, y_sparse)\n    assert_array_almost_equal(ada.predict(T), [-1., 1., 1.])\n\n    ada = AdaBoostRegressor(random_state=0)\n    ada.fit(X, y_sparse)\n    assert_array_almost_equal(ada.predict(csr_matrix(T)), [-1., 1., 1.])\n"], "sample_836": ["def test_class_distribution():\n    # Test with dense arrays\n    y = np.array([[1, 2], [2, 1]])\n    classes, n_classes, class_prior = class_distribution(y)\n    assert_array_equal(classes, [[1, 2], [1, 2]])\n    assert_array_equal(n_classes, [2, 2])\n    assert_array_almost_equal(class_prior, [[0.5, 0.5], [0.5, 0.5]])\n\n    # Test with sparse arrays\n    y_sparse = sp.csr_matrix(y)\n    classes, n_classes, class_prior = class_distribution(y_sparse)\n    assert_array_equal(classes, [[1, 2], [1, 2]])\n    assert_array_equal(n_classes, [2, 2])\n    assert_array_almost_equal(class_prior, [[0.5, 0.5], [0.5, 0.5]])\n\n    # Test with sample weights\n    sample_weight = np.array([0.5, 1.5])\n    classes, n_classes, class_prior = class_distribution(y, sample_weight)\n    assert_array_equal(classes, [[1, 2], [1, 2]])\n    assert_array_equal(n_classes, [2, 2])\n    assert_array_almost_equal(class_prior, [[0.25, 0.75], [0.75, 0.25]])\n", "def test_class_distribution():\n    # Test class_distribution for multiclass and multilabel problems\n    y = np.array([[1, 2], [1, 3], [2, 1]])\n    classes, n_classes, class_prior = class_distribution(y)\n    assert_array_equal(classes, [[1, 2], [1, 2, 3]])\n    assert_array_equal(n_classes, [2, 3])\n    assert_allclose(class_prior, [[0.5, 0.5], [2/3, 1/3, 0]])\n\n    y = np.array([[1, 2, 3], [1, 3, 4], [2, 1, 3]])\n    classes, n_classes, class_prior = class_distribution(y)\n    assert_array_equal(classes, [[1, 2], [1, 2, 3], [3, 4]])\n    assert_array_equal(n_classes, [2, 3, 2])\n    assert_allclose(class_prior, [[0.5, 0.5], [2/3, 1/3, 0], [0.5, 0.5]])\n\n    y_sparse = csr_matrix(y)\n    classes, n_classes, class_prior = class_distribution(y_sparse)\n    assert_array_equal(classes, [[1, 2], [1, 2, 3], [3, 4]])\n    assert_array_equal(n_classes, [2, 3, 2])\n    assert_allclose(class_prior, [[0.5, 0.5], [2/3, 1/3, 0], [0.5, 0.5]])\n", "def test_type_of_target_unknown():\n    # Test that type_of_target correctly identifies unknown types of data\n    for example in EXAMPLES['unknown']:\n        assert type_of_target(example) == 'unknown'\n\n    for example in NON_ARRAY_LIKE_EXAMPLES:\n        assert type_of_target(example) == 'unknown'\n\n    for example in MULTILABEL_SEQUENCES:\n        assert type_of_target(example) == 'multilabel-sequences'\n", "def test_class_distribution_multilabel_indicator():\n    # Test class distribution for multilabel-indicator format\n    y = np.array([[0, 1, 1], [1, 0, 1]])\n    classes, n_classes, class_prior = class_distribution(y)\n    \n    assert_array_equal(classes, [np.array([0, 1])]*3)\n    assert_array_equal(n_classes, [2]*3)\n    assert_allclose(class_prior, [[0.5, 0.5]]*3)\n    \n    # Test with sample weights\n    sample_weight = np.array([0.5, 1.5])\n    classes, n_classes, class_prior = class_distribution(y, sample_weight)\n    \n    assert_array_equal(classes, [np.array([0, 1])]*3)\n    assert_array_equal(n_classes, [2]*3)\n    assert_allclose(class_prior, [[0.25, 0.75]]*3)\n", "def test_class_distribution():\n    # Test with multiclass target\n    y = np.array([[1, 2], [2, 3], [3, 1]])\n    classes, n_classes, class_prior = class_distribution(y)\n    assert_array_equal(classes, [[1, 2, 3], [1, 2, 3]])\n    assert_array_equal(n_classes, [3, 3])\n    assert_allclose(class_prior, [[1/3, 1/3, 1/3], [1/3, 1/3, 1/3]])\n\n    # Test with multilabel-indicator target\n    y = np.array([[0, 1, 1], [1, 0, 1], [1, 1, 0]])\n    classes, n_classes, class_prior = class_distribution(y)\n    assert_array_equal(classes, [[0, 1], [0, 1], [0, 1]])\n    assert_array_equal(n_classes, [2, 2, 2])\n    assert_allclose(class_prior, [[1/2, 1/2], [1/2, 1/2], [1/2, 1/2]])\n\n    # Test with sparse target\n    y = csr_matrix(np.array([[0, 1, 1], [1, 0, 1], [1, 1, 0]]))\n    classes, n_classes, class_prior = class_distribution(y)\n    assert_array_equal(classes, [[0, 1], [0, 1], [0, 1]])\n    assert_array_equal(n_classes, [2, 2, 2])\n    assert_allclose(class_prior, [[1/2, 1/2], [1/2, 1/2], [1/2, 1/2]])\n"], "sample_837": ["def test_get_sys_info():\n    sys_info = _get_sys_info()\n    assert 'python' in sys_info\n    assert 'executable' in sys_info\n    assert 'machine' in sys_info\n    assert len(sys_info) == 3\n", "def test_get_sys_info():\n    sys_info = _get_sys_info()\n    assert 'python' in sys_info\n    assert 'executable' in sys_info\n    assert 'machine' in sys_info\n    assert isinstance(sys_info['python'], str)\n    assert isinstance(sys_info['executable'], str)\n    assert isinstance(sys_info['machine'], str)\n", "def test_get_blas_info():\n    # Test that _get_blas_info returns a dictionary with the expected keys\n    blas_info = _get_blas_info()\n    expected_keys = ['macros', 'lib_dirs', 'cblas_libs']\n    assert set(blas_info.keys()) == set(expected_keys)\n", "def test_get_sys_info():\n    sys_info = _get_sys_info()\n    assert 'python' in sys_info\n    assert 'executable' in sys_info\n    assert 'machine' in sys_info\n    assert len(sys_info) == 3\n", "def test_get_sys_info():\n    sys_info = _get_sys_info()\n    assert 'python' in sys_info\n    assert 'executable' in sys_info\n    assert 'machine' in sys_info\n    assert len(sys_info) == 3\n"], "sample_838": ["def test_column_transformer_get_feature_names():\n    X = np.array([[1, 2], [3, 4]])\n\n    ohe = OneHotEncoder()\n    scaler = StandardScaler()\n\n    ct = ColumnTransformer(\n        transformers=[('ohe', ohe, [0]), ('scaler', scaler, [1])])\n\n    ct.fit(X)\n\n    assert_array_equal(ct.get_feature_names(), ['ohe__x0_1', 'ohe__x0_2', 'scaler__x1'])\n\n    with pytest.raises(NotImplementedError):\n        make_column_transformer((StandardScaler(), [0]), (OneHotEncoder(), [1]),\n                                remainder='passthrough').fit(X).get_feature_names()\n\n    with pytest.raises(AttributeError):\n        make_column_transformer((Trans(), [0]), (StandardScaler(), [1])).fit(X).get_feature_names()\n", "def test_column_transformer_remainder_pandas_dataframe():\n    # Test that the ColumnTransformer works with a pandas DataFrame as input\n    # and the 'remainder' parameter is set to 'passthrough'\n\n    import pandas as pd\n\n    X_array = np.array([[1, 2, 3], [4, 5, 6]])\n    X_df = pd.DataFrame(X_array, columns=['a', 'b', 'c'])\n\n    ct = ColumnTransformer([('trans1', Trans(), ['a'])],\n                           remainder='passthrough')\n\n    X_array_trans = ct.fit_transform(X_array)\n    X_df_trans = ct.fit_transform(X_df)\n\n    assert_array_equal(X_array_trans, X_df_trans)\n", "def test_column_transformer_feature_names():\n    X = np.array([[1, 2], [3, 4]])\n    transformer = ColumnTransformer(\n        [('trans1', StandardScaler(), [0]),\n         ('trans2', Normalizer(norm='l1'), [1])])\n    transformer.fit(X)\n    feature_names = transformer.get_feature_names()\n    assert_array_equal(feature_names, ['trans1__x0', 'trans2__x1'])\n\n    # test that we raise an error if a transformer does not have get_feature_names\n    class DummyTrans(BaseEstimator):\n            return self\n\n            return X\n\n    transformer = ColumnTransformer([('trans1', DummyTrans(), [0])])\n    transformer.fit(X)\n    with pytest.raises(AttributeError):\n        transformer.get_feature_names()\n", "def test_column_transformer_remainder_passthrough_sparse():\n    X_array = np.array([[1, 2, 3], [4, 5, 6]])\n    X_sparse = sparse.csr_matrix(X_array)\n    transformer = ColumnTransformer(\n        [('trans', Trans(), [0])],\n        remainder='passthrough'\n    )\n    X_trans_array = transformer.fit_transform(X_array)\n    X_trans_sparse = transformer.fit_transform(X_sparse)\n    assert_allclose_dense_sparse(X_trans_array, X_trans_sparse)\n", "def test_column_transformer_feature_names():\n    X = np.array([[1, 2, 3], [4, 5, 6]])\n\n    # Test with transformers that have get_feature_names\n    ct = ColumnTransformer([\n        ('scale', StandardScaler(), [0]),\n        ('norm', Normalizer(norm='l1'), [1, 2])\n    ])\n    ct.fit(X)\n    feature_names = ct.get_feature_names()\n    assert_array_equal(feature_names, ['scale__x0', 'norm__x1', 'norm__x2'])\n\n    # Test with transformers that don't have get_feature_names\n    ct = ColumnTransformer([\n        ('trans', Trans(), [0, 1]),\n        ('norm', Normalizer(norm='l1'), [2])\n    ])\n    ct.fit(X)\n    with pytest.raises(AttributeError):\n        ct.get_feature_names()\n\n    # Test with a passthrough transformer\n    ct = ColumnTransformer([\n        ('pass', 'passthrough', [0]),\n        ('norm', Normalizer(norm='l1'), [1, 2])\n    ])\n    ct.fit(X)\n    with pytest.raises(NotImplementedError):\n        ct.get_feature_names()\n"], "sample_839": ["def test_tfidf_vectorizer_max_features():\n    # Test the max_features parameter of the TfidfVectorizer\n    vectorizer = TfidfVectorizer(max_features=4)\n    X = vectorizer.fit_transform(ALL_FOOD_DOCS)\n    assert X.shape[1] == 4\n\n    vectorizer = TfidfVectorizer(max_features=None)\n    X = vectorizer.fit_transform(ALL_FOOD_DOCS)\n    assert X.shape[1] > 4\n", "def test_tfidfvectorizer_use_idf_false():\n    # Test TfidfVectorizer with use_idf=False\n    v = TfidfVectorizer(use_idf=False)\n    X = v.fit_transform(JUNK_FOOD_DOCS)\n    assert_array_equal(X.toarray().sum(axis=0), v.transform(JUNK_FOOD_DOCS).toarray().sum(axis=0))\n    assert_array_almost_equal(v.idf_, np.ones(len(v.vocabulary_)))\n", "def test_tfidfvectorizer_with_preprocessor():\n    # Test that TfidfVectorizer works with a custom preprocessor\n    preprocessor = lambda x: x.upper()\n    vect = TfidfVectorizer(preprocessor=preprocessor)\n    X = vect.fit_transform(JUNK_FOOD_DOCS)\n    assert X.shape[0] == len(JUNK_FOOD_DOCS)\n    assert X.shape[1] > 0\n\n    # Check that the preprocessor is applied correctly\n    assert 'PIZZA' in vect.vocabulary_\n    assert 'burger' not in vect.vocabulary_\n", "def test_tfidf_vectorizer_max_features():\n    # Test that the max_features parameter of TfidfVectorizer works as expected\n    vectorizer = TfidfVectorizer(max_features=4)\n    X = vectorizer.fit_transform(JUNK_FOOD_DOCS)\n    assert X.shape[1] == 4\n\n    vectorizer = TfidfVectorizer(max_features=None)\n    X = vectorizer.fit_transform(JUNK_FOOD_DOCS)\n    assert X.shape[1] > 4\n\n    vectorizer = TfidfVectorizer(max_features=0.5)\n    with pytest.raises(ValueError):\n        X = vectorizer.fit_transform(JUNK_FOOD_DOCS)\n", "def test_tfidf_vectorizer_get_feature_names():\n    v = TfidfVectorizer()\n    v.fit(JUNK_FOOD_DOCS)\n    assert_array_equal(v.get_feature_names(), sorted(v.vocabulary_))\n"], "sample_840": ["def test_pls_regression():\n    # Test PLS regression\n    X, Y = load_linnerud(return_X_y=True)\n    pls = pls_.PLSRegression(n_components=2)\n    pls.fit(X, Y)\n    assert hasattr(pls, 'x_weights_')\n    assert hasattr(pls, 'y_weights_')\n    assert hasattr(pls, 'x_loadings_')\n    assert hasattr(pls, 'y_loadings_')\n    assert hasattr(pls, 'x_scores_')\n    assert hasattr(pls, 'y_scores_')\n    assert hasattr(pls, 'x_rotations_')\n    assert hasattr(pls, 'y_rotations_')\n    assert hasattr(pls, 'coef_')\n    assert hasattr(pls, 'n_iter_')\n", "def test_pls_regressor_attributes():\n    # Test that PLSRegressor has the correct attributes after fitting\n    X, Y = load_linnerud(return_X_y=True)\n    pls = pls_.PLSRegression(n_components=2)\n    pls.fit(X, Y)\n\n    assert hasattr(pls, 'x_weights_')\n    assert hasattr(pls, 'y_weights_')\n    assert hasattr(pls, 'x_loadings_')\n    assert hasattr(pls, 'y_loadings_')\n    assert hasattr(pls, 'x_scores_')\n    assert hasattr(pls, 'y_scores_')\n    assert hasattr(pls, 'x_rotations_')\n    assert hasattr(pls, 'y_rotations_')\n    assert hasattr(pls, 'coef_')\n    assert hasattr(pls, 'n_iter_')\n", "def test_pls_regression():\n    # Test PLS regression on a simple dataset\n    X = np.array([[0., 0., 1.], [1., 0., 0.], [2., 2., 2.], [2., 5., 4.]])\n    Y = np.array([[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]])\n\n    pls = pls_.PLSRegression(n_components=2)\n    pls.fit(X, Y)\n\n    # Test that the model was fitted correctly\n    assert_array_almost_equal(pls.coef_, np.array([[1.05, 1.04], [-0.47, -0.45], [0.55, 0.54]]), decimal=2)\n\n    # Test that the prediction is correct\n    Y_pred = pls.predict(X)\n    assert_array_almost_equal(Y_pred, np.array([[0.26, 0.14], [1.26, 1.16], [6.21, 5.81], [11.74, 11.98]]), decimal=2)\n", "compilation error", "def test_pls_standardized():\n    # Test that PLS with and without standardization by hand gives the same\n    # results. Also checks that the 'scale' option works.\n    X = np.random.rand(10, 3)\n    Y = np.random.rand(10, 2)\n    pls1 = PLSRegression(scale=True)\n    pls1.fit(X, Y)\n\n    scaler = StandardScaler()\n    X_std = scaler.fit_transform(X)\n    Y_std = scaler.fit_transform(Y)\n    pls2 = PLSRegression(scale=False)\n    pls2.fit(X_std, Y_std)\n\n    assert_array_almost_equal(pls1.x_weights_, pls2.x_weights_)\n    assert_array_almost_equal(pls1.y_weights_, pls2.y_weights_)\n"], "sample_841": ["def test_ridge_gcv_mode():\n    # Test that the 'gcv_mode' parameter is correctly handled\n    X, y = make_regression(n_samples=100, n_features=100, random_state=0)\n\n    # Check that gcv_mode=None and gcv_mode='auto' are equivalent\n    ridge1 = _RidgeGCV(gcv_mode=None)\n    ridge2 = _RidgeGCV(gcv_mode='auto')\n    assert_array_almost_equal(ridge1.fit(X, y).coef_, ridge2.fit(X, y).coef_)\n\n    # Check that gcv_mode='svd' raises an error when X is sparse\n    X_sparse = sp.csr_matrix(X)\n    ridge = _RidgeGCV(gcv_mode='svd')\n    with pytest.raises(ValueError):\n        ridge.fit(X_sparse, y)\n\n    # Check that gcv_mode='eigen' works correctly with sparse X\n    ridge = _RidgeGCV(gcv_mode='eigen')\n    ridge.fit(X_sparse, y)\n\n    # Check that gcv_mode='auto' chooses 'eigen' when X is sparse\n    ridge = _RidgeGCV(gcv_mode='auto')\n    ridge.fit(X_sparse, y)\n    assert ridge._decompose == ridge._eigen_decompose_covariance\n", "def test_ridgecv_scorer_nans():\n    # Test that RidgeCV with custom scorer handles NaNs\n    X, y = make_regression(n_samples=10, n_features=5)\n    y_true = np.array([np.nan, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n        y_pred = est.predict(X)\n        return np.mean((y_pred - y) ** 2)\n\n    ridgecv = RidgeCV(scoring=make_scorer(nan_scorer))\n    with ignore_warnings(category=UserWarning):\n        ridgecv.fit(X, y_true)\n\n    assert_warns(UserWarning, ridgecv.score, X, y_true)\n", "def test_ridge_loo(self):\n    # Test that leave-one-out cross-validation works correctly for a given\n    # value of alpha\n    alphas = [0.1]\n    ridge_gcv = _RidgeGCV(alphas=alphas)\n    ridge_cv = RidgeCV(alphas=alphas, cv=None)\n    \n    X, y = make_regression(n_samples=20, n_features=10, random_state=0)\n    \n    # Compute LOO predictions using the _RidgeGCV class\n    ridge_gcv.fit(X, y)\n    y_pred_gcv = ridge_gcv.predict(X)\n    \n    # Compute LOO predictions using the RidgeCV class with cv=None\n    ridge_cv.fit(X, y)\n    y_pred_cv = ridge_cv.predict(X)\n    \n    # Check that the two sets of predictions are equal\n    assert_array_almost_equal(y_pred_gcv, y_pred_cv)\n", "def test_ridge_multilabel():\n    # Test that Ridge can handle multilabel targets\n    X, y = make_multilabel_classification(n_samples=10, n_features=10, n_classes=3,\n                                          n_labels=3, random_state=0)\n    clf = Ridge().fit(X, y)\n    assert clf.coef_.shape == (3, 10)\n    y_pred = clf.predict(X)\n    assert y_pred.shape == (10, 3)\n", "def test_ridgecv_scorer():\n    # Test that using a scorer in RidgeCV works as expected\n    X, y = make_regression(n_samples=100, n_features=10)\n    alphas = [0.1, 1.0, 10.0]\n    ridge_cv = RidgeCV(alphas=alphas, scoring='neg_mean_squared_error')\n    ridge_cv.fit(X, y)\n    assert_almost_equal(ridge_cv.alpha_, alphas[np.argmin(\n        [mean_squared_error(y, Ridge(alpha=a).fit(X, y).predict(X))\n         for a in alphas])])\n"], "sample_842": ["def test_clone_kernel():\n    # Test that kernel can be cloned\n    for kernel in kernels:\n        clone(kernel)\n", "def test_kernel_clone():\n    # Test that the kernel can be cloned\n    for kernel in kernels:\n        kernel_cloned = clone(kernel)\n        assert type(kernel_cloned) == type(kernel)\n        if hasattr(kernel, 'get_params'):\n            assert kernel.get_params() == kernel_cloned.get_params()\n", "def test_kernel_clone():\n    # Test that kernel can be cloned\n    for kernel in kernels:\n        kernel_cloned = clone(kernel)\n        assert kernel.get_params() == kernel_cloned.get_params()\n", "def test_clone_kernel(kernel):\n    # Test that kernel can be cloned\n    kernel_cloned = clone(kernel)\n    assert isinstance(kernel_cloned, Kernel)\n    assert kernel_cloned.__dict__ == kernel.__dict__\n    if isinstance(kernel, KernelOperator):\n        if isinstance(kernel, Exponentiation):\n            assert kernel_cloned.kernel is not kernel.kernel\n            assert kernel_cloned.kernel.__dict__ == kernel.kernel.__dict__\n        elif isinstance(kernel, Sum) or isinstance(kernel, Product):\n            for k1, k2 in zip(kernel_cloned.kernels, kernel.kernels):\n                assert k1 is not k2\n                assert k1.__dict__ == k2.__dict__\n", "def test_clone():\n    # Test that clone preserves all parameters and gives the same results\n    for kernel in kernels:\n        kernel2 = clone(kernel)\n        assert type(kernel) == type(kernel2)\n        if isinstance(kernel, KernelOperator):\n            assert type(kernel.kernels[0]) == type(kernel2.kernels[0])\n        else:\n            for attr in kernel.__dict__:\n                if not attr.startswith('__'):\n                    assert getattr(kernel, attr) == getattr(kernel2, attr)\n\n        X = np.random.RandomState(0).normal(0, 1, (5, 2))\n        Y = np.random.RandomState(0).normal(0, 1, (6, 2))\n\n        K1 = kernel(X, Y)\n        K2 = kernel2(X, Y)\n        assert_array_almost_equal(K1, K2)\n"], "sample_843": ["def test_kernel_gradient():\n    # Compare analytic and numeric gradient of kernels\n    # Note that the kernel gradients are tested only for a few kernels,\n    # because many kernels inherit their gradient from others.\n    kernels_to_test = [RBF(length_scale=1.0),\n                       ConstantKernel(constant_value=2.0),\n                       Matern(length_scale=1.0, nu=0.5),\n                       Matern(length_scale=1.0, nu=1.5),\n                       Matern(length_scale=1.0, nu=2.5),\n                       RationalQuadratic(length_scale=1.0, alpha=2.0),\n                       ExpSineSquared(length_scale=1.0, periodicity=2.0)]\n\n        kernel = clone(kernel_to_test)\n        kernel.theta = theta\n        return kernel(X)\n\n    for kernel_to_test in kernels_to_test:\n        theta = kernel_to_test.theta\n        X = np.random.RandomState(0).normal(0, 1, (10, 3))\n        K, K_grad = kernel_to_test(X, eval_gradient=True)\n        K_grad_approx = _approx_fprime(theta, f, 1e-10, args=(X,))\n        assert_almost_equal(K_grad, K_grad_approx, 4)\n", "def test_kernel_gradient():\n    # Compare analytic and numeric gradient of kernels\n    # Note that the PairwiseKernel currently only supports isotropic distances\n    kernel = RBF(length_scale=1.0)\n    X = np.array([[1.0, 2.0], [3.0, 4.0]])\n    K, K_grad = kernel(X, eval_gradient=True)\n\n        kernel_clone = clone(kernel)\n        kernel_clone.theta = theta\n        return kernel_clone(X)\n\n    K_grad_approx = _approx_fprime(kernel.theta, f, 1e-10)\n\n    assert_almost_equal(K_grad, K_grad_approx, 4)\n", "def test_kernel_gradient():\n    # Compare analytic and numeric gradient of kernels\n        K, K_grad = k(X, eval_gradient=True)\n        assert K_grad.shape == (len(X), len(X), k.n_dims)\n\n        # compare with numeric gradient\n            return clone(k).set_params(**k.hyperparameter_dict[theta])(X)\n\n        K_grad_approx = _approx_fprime(k.theta, f, 1e-10)\n        assert_almost_equal(K_grad, K_grad_approx, 4)\n\n    for kernel in kernels:\n        if not isinstance(kernel, KernelOperator) or \\\n                isinstance(kernel, Exponentiation):\n            test_kernel(kernel)\n", "def test_kernel_gradient(kernel):\n    # Compare analytic and numeric gradient of kernel\n        kernel_clone = clone(kernel)\n        kernel_clone.theta = theta\n        return kernel_clone(X)\n\n    K = f(kernel.theta)\n    K_gradient = kernel(X, eval_gradient=True)[1]\n\n    approx_K_gradient = _approx_fprime(kernel.theta, f, 1e-10)\n    assert_array_almost_equal(K_gradient, approx_K_gradient)\n", "def test_kernel_get_params():\n    kernel1 = RBF(length_scale=2.0) + WhiteKernel(noise_level=3.0)\n    params1 = kernel1.get_params()\n    assert 'k1' in params1\n    assert 'k2' in params1\n    assert 'length_scale' in params1['k1']\n    assert 'noise_level' in params1['k2']\n\n    kernel2 = 2.0 * RBF(length_scale=0.33, length_scale_bounds=\"fixed\")\n    params2 = kernel2.get_params()\n    assert 'k1__constant_value' in params2\n    assert 'k2__length_scale' in params2\n\n    kernel3 = PairwiseKernel(gamma=1.0, metric='rbf')\n    params3 = kernel3.get_params()\n    assert 'gamma' in params3\n    assert 'metric' in params3\n"], "sample_844": ["def test_xi_cluster():\n    # Create a simple reachability plot with some steep upward and downward regions\n    reachability_plot = np.array([10, 8.9, 8.8, 8.7, 7, 6, np.inf])\n    predecessor_plot = np.array([-1, 0, 1, 2, 3, 4, 5])\n    ordering = np.array([0, 1, 2, 3, 4, 5, 6])\n    xi = 0.05\n    min_samples = 2\n    min_cluster_size = 2\n\n    clusters = _xi_cluster(reachability_plot, predecessor_plot, ordering, xi,\n                           min_samples, min_cluster_size, predecessor_correction=True)\n\n    expected_clusters = np.array([[0, 5]])\n\n    assert_array_equal(clusters, expected_clusters)\n", "def test_optics_xi_predecessor_correction():\n    # Create a specific dataset to test predecessor correction\n    X = np.array([[0, 0], [1, 0], [2, 0], [3, 0], [4, 0], [5, 0]])\n    optics = OPTICS(min_samples=2, xi=0.05, predecessor_correction=True)\n    optics.fit(X)\n\n    # Check that predecessor correction is applied correctly\n    assert_array_equal(optics.labels_, [0, 0, 0, 0, 0, 0])\n", "def test_optics_xi():\n    # test OPTICS with xi clustering method\n    optics = OPTICS(min_samples=5, xi=.05, min_cluster_size=.05)\n    optics.fit(X)\n    labels = optics.labels_\n    # test if all points have been assigned to a cluster or noise\n    assert np.all(labels >= -1)\n    # test if at least one cluster has been found\n    assert len(np.unique(labels)) > 1\n    # test if more than one cluster has been found\n    assert len(np.unique(labels)) > 2\n    # check that all large enough clusters are not noise\n    for c in np.unique(labels):\n        if c != -1:\n            assert np.sum(labels == c) >= optics.min_samples\n", "def test_optics_xi_predecessor_correction():\n    # Create a simple dataset where predecessor correction is necessary\n    X = np.array([[0, 0], [1, 1], [2, 2], [3, 3], [4, 4]])\n    optics = OPTICS(min_samples=2, xi=0.05, predecessor_correction=True)\n    optics.fit(X)\n\n    # Check that the predecessor correction was applied correctly\n    assert_array_equal(optics.labels_, [0, 0, 0, 0, 0])\n", "def test_optics_xi_predecessor_correction():\n    # Test that predecessor correction works correctly\n    optics = OPTICS(cluster_method='xi', predecessor_correction=True)\n    X = np.array([[0, 0], [1, 1], [2, 2], [3, 3], [4, 4]])\n    optics.fit(X)\n    assert_array_equal(optics.labels_, [0, 0, 0, 0, 0])\n\n    optics = OPTICS(cluster_method='xi', predecessor_correction=False)\n    optics.fit(X)\n    assert_array_equal(optics.labels_, [0, 0, 0, 0, -1])\n"], "sample_845": ["def test_tfidf_vectorizer_sublinear_tf():\n    v = TfidfVectorizer(sublinear_tf=True)\n    tfidf = v.fit_transform([\"hello world\", \"hello world world\"])\n    assert tfidf.shape == (2, 2)\n    assert_array_almost_equal(tfidf.toarray(), [[1., 1.], [1., 1. + np.log(2)]])\n", "def test_tfidf_vectorizer_empty_vocabulary():\n    v = TfidfVectorizer(vocabulary=[])\n    with pytest.raises(ValueError):\n        v.fit(JUNK_FOOD_DOCS)\n", "def test_tfidf_vectorizer_deprecated_copy():\n    X = [\"This is a sample.\", \"Another example.\"]\n    vect = TfidfVectorizer()\n    with pytest.warns(DeprecationWarning):\n        vect.transform(X, copy=True)\n    with pytest.warns(DeprecationWarning):\n        vect.transform(X, copy=False)\n    with pytest.raises(ValueError):\n        vect.transform(X, copy='foo')\n    assert_no_warnings(vect.transform, X)\n", "def test_tfidf_vectorizer_get_feature_names_out():\n    v = TfidfVectorizer()\n    with pytest.raises(AttributeError):\n        v.get_feature_names_out()\n\n    X = v.fit_transform([\"hello world\"])\n    names = v.get_feature_names_out()\n    assert_array_equal(names, np.array(['hello', 'world'], dtype=object))\n", "def test_tfidf_vectorizer_transform_with_empty_strings():\n    # Test that empty strings are handled correctly in transform method\n    v = TfidfVectorizer()\n    X = v.fit_transform([\"hello world\", \"\"])\n    assert_array_equal(v.transform([\"hello\", \"world\", \"\"]).toarray(),\n                       [[1., 0.], [0., 1.], [0., 0.]])\n    assert_array_equal(v.transform([\"\", \"\", \"\"]).toarray(),\n                       [[0., 0.], [0., 0.], [0., 0.]])\n"], "sample_846": ["def test_column_transformer_get_feature_names():\n    # get feature names with a pipeline in ColumnTransformer\n    columns = ['a', 'b', 'c']\n    X = np.array([[1, 2, 3], [4, 5, 6]])\n\n    ohe = OneHotEncoder(sparse=False)\n    pipe = make_pipeline(ohe)\n\n    ct = ColumnTransformer([('pipe', pipe, ['a'])], remainder='passthrough')\n    ct.fit(X, columns=columns)\n\n    assert_raises(NotImplementedError, ct.get_feature_names)\n", "def test_column_transformer_get_feature_names():\n    X = np.array([[1, 2, 3], [4, 5, 6]])\n\n    # Single transformer case\n    ct = ColumnTransformer([(\"trans\", Trans(), [0])])\n    ct.fit(X)\n    assert_raises(NotImplementedError, ct.get_feature_names)\n\n    # Multiple transformers case\n    ct = ColumnTransformer([(\"trans1\", Trans(), [0]), (\"trans2\", Trans(), [1])])\n    ct.fit(X)\n    assert_raises(NotImplementedError, ct.get_feature_names)\n\n    # Test with OneHotEncoder\n    ct = ColumnTransformer([(\"ohe\", OneHotEncoder(), [0])])\n    ct.fit(X)\n    feature_names = ct.get_feature_names()\n    assert_array_equal(feature_names, ['ohe__x0_1', 'ohe__x0_2', 'ohe__x0_3', 'ohe__x0_4'])\n\n    # Test with FunctionTransformer\n        return X\n\n    ct = ColumnTransformer([(\"ft\", FunctionTransformer(func), [0])])\n    ct.fit(X)\n    assert_raises(NotImplementedError, ct.get_feature_names)\n", "def test_column_transformer_feature_names_out():\n    X = np.array([[1, 2, 3], [4, 5, 6]])\n\n    # Single transformer with 2 columns\n    ct = ColumnTransformer([('trans', Trans(), [0, 1])])\n    ct.fit(X)\n    assert ct.get_feature_names_out() == ['trans__x0', 'trans__x1']\n\n    # Two transformers, one for each column\n    ct = ColumnTransformer([('trans1', Trans(), [0]), ('trans2', Trans(), [1])])\n    ct.fit(X)\n    assert ct.get_feature_names_out() == ['trans1__x0', 'trans2__x0']\n\n    # Using 'passthrough' should raise an error\n    ct = ColumnTransformer([('trans', 'passthrough', [0])])\n    ct.fit(X)\n    with pytest.raises(NotImplementedError):\n        ct.get_feature_names_out()\n\n    # Using 'drop' should not include the dropped columns in feature names\n    ct = ColumnTransformer([('trans', 'drop', [0]), ('trans2', Trans(), [1])])\n    ct.fit(X)\n    assert ct.get_feature_names_out() == ['trans2__x0']\n", "def test_column_transformer_get_params():\n    # check if we can get the parameters of a ColumnTransformer\n    params = make_column_transformer(\n        (StandardScaler(), [0]),\n        (Normalizer(), slice(1, 3)),\n        remainder='passthrough'\n    ).get_params()\n\n    assert 'transformers' in params\n    assert 'remainder' in params\n    assert 'sparse_threshold' in params\n\n    # check the structure of the transformers param\n    transformers_param = params['transformers']\n    assert isinstance(transformers_param, list)\n    assert len(transformers_param) == 2\n\n    # check that the transformers themselves have the correct params\n    scaler_params = transformers_param[0][1].get_params()\n    normalizer_params = transformers_param[1][1].get_params()\n\n    assert 'with_mean' in scaler_params\n    assert 'norm' in normalizer_params\n", "def test_column_transformer_get_feature_names():\n    X = np.array([[1, 2, 3], [4, 5, 6]])\n    transformer = ColumnTransformer(\n        [(\"trans1\", Trans(), [0]),\n         (\"trans2\", Trans(), [1, 2])])\n    transformer.fit(X)\n    \n    feature_names = transformer.get_feature_names()\n    assert_array_equal(feature_names, [\"trans1__x0\", \"trans2__x1\", \"trans2__x2\"])\n\n    # test with 'passthrough' transformer\n    with pytest.raises(NotImplementedError):\n        ColumnTransformer([(\"trans1\", 'passthrough', [0])]).fit(X).get_feature_names()\n\n    # test with 'drop' transformer\n    transformer = ColumnTransformer([(\"trans1\", 'drop', [0])])\n    transformer.fit(X)\n    assert_array_equal(transformer.get_feature_names(), [])\n\n    # test with missing get_feature_names method on transformer\n    class NoFeatureNamesTrans(BaseEstimator):\n            return self\n\n            return X\n\n    transformer = ColumnTransformer([(\"trans1\", NoFeatureNamesTrans(), [0])])\n    transformer.fit(X)\n    with pytest.raises(AttributeError):\n        transformer.get_feature_names()\n"], "sample_847": ["def test_enet_path_return_n_iter():\n    # Make sure that return_n_iter returns the correct number of iterations\n    # performed for each alpha value.\n    X, y = load_boston(return_X_y=True)\n    alphas = [0.1, 0.01, 0.001]\n    result = enet_path(X, y, l1_ratio=0.5, alphas=alphas,\n                       return_n_iter=True)\n\n    # Check if the returned n_iter is a list with the same length as alphas.\n    assert len(result[3]) == len(alphas)\n\n    # Check if all elements in the n_iter list are integers greater than 0.\n    assert all(isinstance(n, int) and n > 0 for n in result[3])\n", "def test_lasso_path_return_n_iter():\n    X = [[1, 2], [3, 4], [5, 6]]\n    y = [1, 2, 3]\n    return_n_iter = True\n    alphas, coefs, dual_gaps, n_iters = lasso_path(X, y, return_n_iter=return_n_iter)\n    assert len(alphas) == len(n_iters)\n    assert all(isinstance(x, int) for x in n_iters)\n\n    # Test that the n_iters are non-increasing\n    assert all(n_iters[i] >= n_iters[i+1] for i in range(len(n_iters)-1))\n\n    # Test with return_n_iter=False\n    alphas, coefs, dual_gaps = lasso_path(X, y)\n    assert len(alphas) == len(dual_gaps)\n", "def test_multitask_lasso_and_enet():\n    # Test MultiTaskLasso and ElasticNet for different parameters\n    X = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])\n    y = np.array([[13, 14], [15, 16], [17, 18]])\n\n    # Set regularization parameters\n    alpha = 0.1\n    l1_ratio = 0.5\n\n    # Create instances of MultiTaskLasso and MultiTaskElasticNet\n    mtl = MultiTaskLasso(alpha=alpha)\n    mten = MultiTaskElasticNet(alpha=alpha, l1_ratio=l1_ratio)\n\n    # Fit models\n    mtl.fit(X, y)\n    mten.fit(X, y)\n\n    # Check if coefficients are not all zero\n    assert_array_ne(mtl.coef_, 0)\n    assert_array_ne(mten.coef_, 0)\n\n    # Check shape of coefficients\n    assert_equal(mtl.coef_.shape, (2, 4))\n    assert_equal(mten.coef_.shape, (2, 4))\n", "def test_enet_path_precompute_gram():\n    # Test that passing precomputed Gram to enet_path yields same result\n    # as computing it during execution.\n    X, y = load_boston(return_X_y=True)\n    params = dict(l1_ratio=0.5, eps=1e-3, n_alphas=10, alphas=None,\n                  precompute='auto', Xy=None, copy_X=True, coef_init=None,\n                  verbose=False, return_n_iter=False, positive=False,\n                  check_input=True)\n\n    coefs1 = enet_path(X, y, **params)[1]\n\n    gram = X.T.dot(X)\n    Xy = X.T.dot(y)\n    params['precompute'] = gram\n    params['Xy'] = Xy\n    coefs2 = enet_path(X, y, **params)[1]\n\n    assert_array_almost_equal(coefs1, coefs2)\n", "def test_multi_task_lasso_and_elastic_netcv_return_support():\n    # Test that MultiTaskLassoCV and MultiTaskElasticNetCV return the correct\n    # support array after a fit.\n    X, y = load_boston(return_X_y=True)\n    X = np.hstack([X, X])\n    y = np.hstack([y, y]).reshape(-1, 2)\n\n    mtl = MultiTaskLassoCV()\n    mtl.fit(X, y)\n    assert_array_equal(mtl.support_, mtl.coef_ != 0)\n\n    mten = MultiTaskElasticNetCV()\n    mten.fit(X, y)\n    assert_array_equal(mten.support_, mten.coef_ != 0)\n"], "sample_848": ["def test_multioutput_estimator_predict_proba():\n    # Test predict_proba for MultiOutputClassifier\n    X, y = make_classification(n_samples=100, n_features=10, n_informative=5,\n                               n_redundant=3, n_classes=3, random_state=0)\n    X, y = shuffle(X, y, random_state=0)\n\n    est = LogisticRegression(random_state=0)\n    multi_est = MultiOutputClassifier(est).fit(X, np.hstack((y[:, None], y[:, None])))\n\n    y_pred = multi_est.predict_proba(X)\n    assert len(y_pred) == 2\n    assert_array_almost_equal(y_pred[0], y_pred[1])\n", "def test_regressor_chain_partial_fit():\n    # Test partial fit with RegressorChain\n    X, y = make_classification(n_samples=100, n_features=5, n_informative=3,\n                               n_targets=2, random_state=0)\n    y = np.random.rand(y.shape[0], 2)  # regression targets\n\n    est = RegressorChain(base_estimator=Ridge(random_state=0), order=[0, 1])\n    est.partial_fit(X, y)\n\n    # check that all estimators are fitted\n    for e in est.estimators_:\n        assert hasattr(e, 'coef_')\n\n    # check that the chain is correctly ordered\n    assert_array_equal(est.order_, [0, 1])\n\n    # check predictions\n    y_pred = est.predict(X)\n    assert y_pred.shape == y.shape\n", "def test_regressor_chain_cross_val_predict():\n    # Test that RegressorChain with cv parameter set to an integer or a\n    # cross-validation object uses cross-validated predictions for the results\n    # of previous estimators in the chain.\n    X, y = datasets.make_regression(n_samples=100, n_features=5, n_targets=3,\n                                    random_state=42)\n    base_estimator = GradientBoostingRegressor()\n    chain = RegressorChain(base_estimator, order=[0, 1, 2], cv=5)\n\n    y_pred = chain.fit(X, y).predict(X)\n\n    # Check that y_pred has the correct shape\n    assert_array_equal(y_pred.shape, (100, 3))\n\n    # Check that mean squared error is lower when using cross-validated\n    # predictions\n    mse_without_cv = mean_squared_error(y, MultiOutputRegressor(base_estimator).fit(X, y).predict(X))\n    mse_with_cv = mean_squared_error(y, y_pred)\n    assert_almost_equal(mse_with_cv, mse_without_cv, decimal=2)\n", "def test_multi_output_regressor_partial_fit():\n    # Test if multi-output regressor's partial fit works as expected\n    X, y = datasets.make_regression(n_samples=100, n_features=10, n_targets=3)\n    estimator = MultiOutputRegressor(Ridge())\n    \n    # Partial fit with a portion of the data\n    estimator.partial_fit(X[:50], y[:50])\n    \n    # Check if the estimator is fitted\n    assert hasattr(estimator, 'estimators_')\n    \n    # Predict on the remaining data\n    y_pred = estimator.predict(X[50:])\n    \n    # Check if the prediction shape is correct\n    assert y_pred.shape == (50, 3)\n    \n    # Check if the prediction is not empty\n    assert np.any(y_pred != 0)\n", "def test_multi_output_regressor_partial_fit():\n    # Test partial fit with different numbers of samples and features\n    X1, y1 = datasets.make_regression(n_samples=100, n_features=10, n_targets=3)\n    X2, y2 = datasets.make_regression(n_samples=50, n_features=10, n_targets=3)\n\n    est = MultiOutputRegressor(Ridge())\n    est.partial_fit(X1, y1)\n    est.partial_fit(X2, y2)\n\n    # Check that the model can make predictions\n    y_pred = est.predict(X1)\n    assert_array_almost_equal(y_pred.shape, (100, 3))\n"], "sample_849": ["def test_stratified_split():\n    # Check that stratified split preserves the proportion of each class in\n    # the train and test set.\n    n_samples = 1000\n    X, y = make_classification(n_samples=n_samples, n_features=5,\n                               n_informative=3, n_redundant=2,\n                               n_classes=3, random_state=0)\n    sss = StratifiedShuffleSplit(test_size=0.2, random_state=0)\n    train, test = next(sss.split(X, y))\n\n    # Compute the proportion of each class in the train and test set\n    train_prop = np.bincount(y[train]) / len(train)\n    test_prop = np.bincount(y[test]) / len(test)\n\n    # The proportion of each class should be preserved up to a tolerance\n    assert_allclose(train_prop, test_prop, atol=0.05)\n", "def test_build_repr():\n    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n    repr = _build_repr(cv)\n    assert repr == \"KFold(n_splits=5, random_state=42, shuffle=True)\"\n\n    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    repr = _build_repr(cv)\n    assert repr == \"StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\"\n", "def test_build_repr():\n    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n    repr = _build_repr(cv)\n    assert repr == \"KFold(n_splits=5, random_state=42, shuffle=True)\"\n\n    cv = StratifiedKFold(n_splits=3, shuffle=False, random_state=None)\n    repr = _build_repr(cv)\n    assert repr == \"StratifiedKFold(n_splits=3, random_state=None, shuffle=False)\"\n", "def test_repeated_stratified_kfold_splits():\n    X = np.ones(10)\n    y = np.arange(10) // 2\n\n    rkf = RepeatedStratifiedKFold(n_splits=2, n_repeats=3, random_state=0)\n    for train_index, test_index in rkf.split(X, y):\n        assert_array_equal(np.unique(y[test_index]), np.unique(y))\n        assert train_index.shape[0] + test_index.shape[0] == X.shape[0]\n        assert np.intersect1d(train_index, test_index).size == 0\n", "def test_validate_shuffle_split():\n    # Test if shuffle split validation raises errors for invalid inputs\n    with assert_raises_regexp(ValueError, 'test_size should be either positive and smaller'):\n        _validate_shuffle_split(10, test_size=0)\n    with assert_raises_regexp(ValueError, 'train_size should be either positive and smaller'):\n        _validate_shuffle_split(10, train_size=11)\n    with assert_raises_regexp(ValueError, 'train_size should be either positive and smaller'):\n        _validate_shuffle_split(10, train_size=-1)\n    with assert_raises_regexp(ValueError, 'train_size should be a float or int'):\n        _validate_shuffle_split(10, train_size='a')\n    with assert_raises_regexp(ValueError, 'test_size should be a float or int'):\n        _validate_shuffle_split(10, test_size='a')\n    with assert_raises_regexp(ValueError, 'The sum of test_size and train_size = 11, should be smaller'):\n        _validate_shuffle_split(10, test_size=5, train_size=6)\n    with assert_raises_regexp(ValueError, 'With n_samples=10, test_size=0.2 and train_size=0.9, the '\n                                         'resulting train set will be empty'):\n        _validate_shuffle_split(10, test_size=0.2, train_size=0.9)\n"], "sample_850": ["def test_nystroem_approximation():\n    n_components = 100\n    gamma = 0.2\n    nystrom = Nystroem(gamma=gamma, n_components=n_components)\n    X_reduced = nystrom.fit_transform(X)\n\n    # compute exact kernel map\n    K_exact = rbf_kernel(X, gamma=gamma)\n\n    # compute approximate kernel map using Nystroem\n    K_approx = np.dot(X_reduced, X_reduced.T)\n\n    assert_array_almost_equal(K_exact, K_approx, decimal=1)\n", "def test_nystroem_kernel_params():\n    # Test that kernel parameters are passed correctly to the pairwise_kernels\n    # function\n    X = np.random.rand(10, 5)\n    nystrom = Nystroem(kernel='polynomial', degree=3, coef0=1, gamma=None,\n                       kernel_params={'gamma': 2})\n    with pytest.raises(ValueError):\n        nystrom.fit(X)\n\n    nystrom = Nystroem(kernel='rbf', gamma=2, kernel_params={'gamma': 1})\n    with pytest.raises(ValueError):\n        nystrom.fit(X)\n", "def test_nystroem_approximation():\n    n_components = 100\n    gamma = 0.2\n    nystrom = Nystroem(gamma=gamma, n_components=n_components)\n    X_reduced = nystrom.fit_transform(X)\n\n    # compute exact kernel map\n    K_exact = rbf_kernel(X, gamma=gamma)\n\n    # compute approximate kernel map using Nystroem\n    K_approx = np.dot(X_reduced, X_reduced.T)\n\n    assert_array_almost_equal(K_exact, K_approx, decimal=1)\n", "def test_nystroem_kernel_params():\n    # Test that Nystroem can handle various kernel parameters\n    X = np.random.RandomState(0).random_sample(size=(300, 50))\n\n    for kernel in kernel_metrics:\n        nystrom = Nystroem(kernel=kernel, gamma=0.5, coef0=1, degree=2,\n                           n_components=100)\n        X_transformed = nystrom.fit_transform(X)\n\n        assert X_transformed.shape == (X.shape[0], 100)\n\n    # test with callable kernel\n        return np.dot(x, y.T)\n\n    nystrom = Nystroem(kernel=custom_kernel, n_components=100)\n    X_transformed = nystrom.fit_transform(X)\n\n    assert X_transformed.shape == (X.shape[0], 100)\n", "def test_nystroem_fit_transform():\n    n_components = 50\n    gamma = 0.2\n    nystrom = Nystroem(gamma=gamma, n_components=n_components)\n    X_reduced = nystrom.fit_transform(X)\n\n    # compute exact kernel map\n    K_exact = rbf_kernel(X, gamma=gamma)\n\n    # compute approximate kernel map using Nystrom\n    K_approx = np.dot(X_reduced, X_reduced.T)\n\n    assert_array_almost_equal(K_exact, K_approx, decimal=1)\n"], "sample_851": ["def test_mean_tweedie_deviance():\n    y_true = np.array([1, 2, 3, 4])\n    y_pred = np.array([1.1, 1.9, 3.2, 4.1])\n\n    # test for power = 0 (Normal distribution)\n    assert_almost_equal(mean_tweedie_deviance(y_true, y_pred, power=0),\n                        mean_squared_error(y_true, y_pred))\n\n    # test for power = 1 (Poisson distribution)\n    assert_almost_equal(mean_tweedie_deviance(y_true, y_pred, power=1),\n                        mean_poisson_deviance(y_true, y_pred))\n\n    # test for power = 2 (Gamma distribution)\n    assert_almost_equal(mean_tweedie_deviance(y_true, y_pred, power=2),\n                        mean_gamma_deviance(y_true, y_pred))\n\n    # test for invalid power value\n    with pytest.raises(ValueError):\n        mean_tweedie_deviance(y_true, y_pred, power=-1)\n\n    # test for y_true and y_pred having different lengths\n    with pytest.raises(ValueError):\n        mean_tweedie_deviance(y_true, np.array([1, 2, 3]), power=0)\n", "def test__check_reg_targets():\n    # Test with a 1D array-like y_true and y_pred\n    y_true = [3, -0.5, 2, 7]\n    y_pred = [2.5, 0.0, 2, 8]\n    y_type, y_true, y_pred, _ = _check_reg_targets(y_true, y_pred, None)\n    assert_array_equal(y_true, np.array([[3], [-0.5], [2], [7]]))\n    assert_array_equal(y_pred, np.array([[2.5], [0.0], [2], [8]]))\n    assert y_type == 'continuous'\n\n    # Test with a 2D array-like y_true and y_pred\n    y_true = [[0.5, 1], [-1, 1], [7, -6]]\n    y_pred = [[0, 2], [-1, 2], [8, -5]]\n    y_type, y_true, y_pred, _ = _check_reg_targets(y_true, y_pred, None)\n    assert_array_equal(y_true, np.array([[0.5, 1], [-1, 1], [7, -6]]))\n    assert_array_equal(y_pred, np.array([[0, 2], [-1, 2], [8, -5]]))\n    assert y_type == 'continuous-multioutput'\n\n    # Test that an error is raised when y_true and y_pred are of different lengths\n    y_true = [3, -0.5, 2, 7]\n    y_pred = [2.5, 0.0, 2]\n    with pytest.raises(ValueError):\n        _check_reg_targets(y_true, y_pred, None)\n\n    # Test that an error is raised when y_true and y_pred have different numbers of outputs\n    y_true = [[0.5, 1], [-1, 1], [7, -6]]\n    y_pred = [[0, 2, 3], [-1, 2, 4], [8, -5, 6]]\n    with pytest.raises(ValueError):\n        _check_reg_targets(y_true, y_pred, None)\n", "def test_mean_tweedie_deviance():\n    # Check for power parameter in (0, 1)\n    y_true = np.array([2, 0, 1, 4])\n    y_pred = np.array([0.5, 0.5, 2., 2.])\n\n    with pytest.raises(ValueError):\n        mean_tweedie_deviance(y_true, y_pred, power=0.5)\n\n    # Check for negative y_pred for power < 0\n    y_true = np.array([2, 0, 1, 4])\n    y_pred = np.array([-0.5, 0.5, 2., 2.])\n\n    with pytest.raises(ValueError):\n        mean_tweedie_deviance(y_true, y_pred, power=-1)\n\n    # Check for non-positive y_true for power >= 1\n    y_true = np.array([2, 0, -1, 4])\n    y_pred = np.array([0.5, 0.5, 2., 2.])\n\n    with pytest.raises(ValueError):\n        mean_tweedie_deviance(y_true, y_pred, power=1)\n\n    # Check for non-positive y_pred for power >= 1\n    y_true = np.array([2, 0, 1, 4])\n    y_pred = np.array([0.5, 0.5, 0., 2.])\n\n    with pytest.raises(ValueError):\n        mean_tweedie_deviance(y_true, y_pred, power=1)\n", "def test_mean_tweedie_deviance():\n    # check power parameter\n    y_true = np.array([1.0, 2.0, 3.0, 4.0])\n    y_pred = np.array([1.0, 2.0, 3.0, 4.0])\n\n    assert_almost_equal(mean_tweedie_deviance(y_true, y_pred, power=0),\n                        mean_squared_error(y_true, y_pred))\n\n    # test for power > 0 (or not a number) raises ValueError\n    with pytest.raises(ValueError):\n        mean_tweedie_deviance(y_true, y_pred, power=-1)\n\n    with pytest.raises(ValueError):\n        mean_tweedie_deviance(y_true, y_pred, power='a')\n\n    with pytest.raises(ValueError):\n        mean_tweedie_deviance(y_true, y_pred, power=0.5)\n", "def test_mean_tweedie_deviance():\n    y_true = np.array([2, 0, 1, 4])\n    y_pred = np.array([0.5, 0.5, 2., 2.])\n\n    # check power = 0, should be equivalent to mean squared error\n    assert_almost_equal(mean_tweedie_deviance(y_true, y_pred, power=0),\n                        mean_squared_error(y_true, y_pred))\n\n    # check power = 1, should be equivalent to mean poisson deviance\n    assert_almost_equal(mean_tweedie_deviance(y_true, y_pred, power=1),\n                        mean_tweedie_deviance(y_true, y_pred, power=1))\n\n    # check power = 2, should be equivalent to mean gamma deviance\n    assert_almost_equal(mean_tweedie_deviance(y_true, y_pred, power=2),\n                        mean_tweedie_deviance(y_true, y_pred, power=2))\n\n    # check for negative y_pred\n    with pytest.raises(ValueError):\n        mean_tweedie_deviance(y_true, -y_pred, power=2)\n\n    # check for non-positive y_true\n    y_true = np.array([2, 0, 1, 4])\n    y_pred = np.array([0.5, 0.5, 2., 2.])\n    with pytest.raises(ValueError):\n        mean_tweedie_deviance(y_true, y_pred, power=2)\n"], "sample_852": ["def test_make_classification_n_classes():\n    # Test that make_classification produces the correct number of classes\n    X, y = make_classification(n_samples=100, n_features=20, n_informative=10,\n                               n_redundant=0, n_repeated=0, n_classes=3)\n\n    assert len(np.unique(y)) == 3\n\n    X, y = make_classification(n_samples=100, n_features=20, n_informative=10,\n                               n_redundant=0, n_repeated=0, n_classes=5)\n\n    assert len(np.unique(y)) == 5\n", "def test_make_classification_return_Xy():\n    X, y = make_classification(n_samples=200, n_features=20, random_state=0)\n    assert X.shape == (200, 20)\n    assert y.shape == (200,)\n    assert_array_equal(np.unique(y), [0, 1])\n", "def test_make_classification_weights():\n    X, y = make_classification(n_samples=100, n_features=20, weights=[0.3, 0.7])\n    assert_array_equal(np.unique(y), np.array([0, 1]))\n    assert_almost_equal(np.sum(y == 0) / 100, 0.3, decimal=2)\n\n    # test that the user-specified weights are normalized correctly\n    X, y = make_classification(n_samples=100, n_features=20, weights=[10, 30])\n    assert_array_equal(np.unique(y), np.array([0, 1]))\n    assert_almost_equal(np.sum(y == 0) / 100, 10 / (10 + 30), decimal=2)\n\n    # test that an error is raised when the weights array has length different\n    # from the number of classes.\n    with pytest.raises(ValueError):\n        make_classification(n_samples=100, n_features=20, n_informative=2,\n                            n_redundant=2, n_repeated=0, n_classes=3,\n                            weights=[0.2, 0.8])\n\n    # test that the weights are used with n_classes=1\n    X, y = make_classification(n_samples=100, n_features=20, n_informative=2,\n                               n_redundant=2, n_repeated=0, n_classes=1,\n                               weights=[1.0])\n    assert_array_equal(np.unique(y), np.array([0]))\n    assert_almost_equal(np.sum(y == 0) / 100, 1.0, decimal=2)\n", "def test_make_sparse_spd_matrix():\n    # Test the function with different parameters\n    X = make_sparse_spd_matrix(dim=5, alpha=0.5, norm_diag=False)\n    assert_all_finite(X)\n    assert np.allclose(np.diag(X), 1)\n\n    X = make_sparse_spd_matrix(dim=10, alpha=0.9, norm_diag=True)\n    assert_all_finite(X)\n    assert np.allclose(np.diag(X), 1)\n", "def test_make_sparse_coded_signal():\n    # Test that the generated signal is indeed sparse\n    y, D, X = make_sparse_coded_signal(n_samples=10, n_components=5,\n                                       n_features=20, n_nonzero_coefs=3)\n    assert_array_equal(X[X != 0].shape[0], 30)\n\n    # Test that the generated dictionary has unit norm columns\n    assert_array_almost_equal(np.linalg.norm(D, axis=0), np.ones(5))\n\n    # Smoke test for various parameters\n    make_sparse_coded_signal(n_samples=10, n_components=5, n_features=20,\n                             n_nonzero_coefs=3, random_state=0)\n    make_sparse_coded_signal(n_samples=10, n_components=10, n_features=20,\n                             n_nonzero_coefs=5, random_state=0)\n"], "sample_853": ["def test_transformed_target_regressor_with_function_transformer():\n    # Create a sample dataset\n    X, y = friedman\n\n    # Define a simple function and its inverse\n        return x ** 2\n\n        return np.sqrt(x)\n\n    # Create a FunctionTransformer using the function and its inverse\n    transformer = FunctionTransformer(func=func, inverse_func=inverse_func)\n\n    # Create a TransformedTargetRegressor with the transformer and a regressor\n    regr = TransformedTargetRegressor(transformer=transformer,\n                                      regressor=LinearRegression())\n\n    # Fit the model and make predictions\n    regr.fit(X, y)\n    y_pred = regr.predict(X)\n\n    # Check that the predictions are correct\n    assert_allclose(y_pred, y, atol=1e-5)\n", "def test_transformed_target_regressor_check_inverse():\n    X, y = friedman\n\n    # Define a transformer with no inverse\n    class NoInverseTransformer(TransformerMixin, BaseEstimator):\n            return self\n\n            return X ** 2\n\n    # Check that an error is raised when check_inverse=True\n    ttr = TransformedTargetRegressor(transformer=NoInverseTransformer(),\n                                     regressor=DummyRegressor(),\n                                     check_inverse=True)\n    with pytest.raises(ValueError):\n        ttr.fit(X, y)\n\n    # Check that no error is raised when check_inverse=False\n    ttr = TransformedTargetRegressor(transformer=NoInverseTransformer(),\n                                     regressor=DummyRegressor(),\n                                     check_inverse=False)\n    assert_no_warnings(ttr.fit, X, y)\n", "def test_transformed_target_regressor_with_sample_weight():\n    # Test that the TransformedTargetRegressor works with sample weights\n    X, y = friedman\n    sample_weight = np.ones_like(y)\n    sample_weight[:10] = 0.5\n\n    regr = TransformedTargetRegressor(regressor=Lasso(),\n                                      func=np.log, inverse_func=np.exp)\n    regr.fit(X, y, sample_weight=sample_weight)\n\n    assert hasattr(regr, 'regressor_')\n    assert isinstance(regr.regressor_, Lasso)\n", "def test_transformed_target_regressor_check_inverse():\n    X, y = friedman\n\n    # Define a transformer with no inverse\n    class NoInverseTransformer(TransformerMixin, BaseEstimator):\n            return self\n\n            return X ** 2\n\n            return X  # This will not be the exact inverse\n\n    # Create a TransformedTargetRegressor with check_inverse=True\n    regr = TransformedTargetRegressor(\n        regressor=LinearRegression(),\n        transformer=NoInverseTransformer(),\n        check_inverse=True,\n    )\n\n    # Fit the regressor and check that a warning is raised\n    assert_warns_message(UserWarning, \"The provided functions or transformer are not strictly inverse of each other.\",\n                         regr.fit, X, y)\n\n    # Create a TransformedTargetRegressor with check_inverse=False\n    regr = TransformedTargetRegressor(\n        regressor=LinearRegression(),\n        transformer=NoInverseTransformer(),\n        check_inverse=False,\n    )\n\n    # Fit the regressor and check that no warning is raised\n    assert_no_warnings(regr.fit, X, y)\n", "def test_function_transformer_check_inverse():\n    X, y = friedman\n    y = np.vstack([y, y]).T  # make y 2D\n\n        return y / 10\n\n        return y * 5  # not the actual inverse\n\n    regr = TransformedTargetRegressor(\n        regressor=LinearRegression(),\n        func=func,\n        inverse_func=inverse_func,\n        check_inverse=True)\n\n    assert_warns_message(UserWarning,\n                         \"The provided functions or transformer are\"\n                         \" not strictly inverse of each other.\",\n                         regr.fit, X, y)\n"], "sample_854": ["def test_libsvm_sparse_predict_proba():\n    # Test that predict_proba with sparse input gives the same result as dense\n    X, y = make_classification(n_samples=100, n_features=10, n_informative=5,\n                               n_redundant=3, n_repeated=2, random_state=42)\n    X_sparse = sparse.csr_matrix(X)\n\n    clf_sparse = svm.SVC(probability=True, random_state=42)\n    clf_dense = svm.SVC(probability=True, random_state=42)\n\n    clf_sparse.fit(X_sparse, y)\n    clf_dense.fit(X, y)\n\n    assert_array_almost_equal(clf_sparse.predict_proba(X_sparse),\n                              clf_dense.predict_proba(X))\n", "def test_svc_max_iter():\n    # Test that the max_iter parameter is used\n    X, y = make_classification(n_samples=20, n_features=10, random_state=0)\n    clf = svm.SVC(kernel='linear', max_iter=1, random_state=0)\n    with ignore_warnings(category=ConvergenceWarning):\n        clf.fit(X, y)\n\n    assert_warns(ConvergenceWarning, clf.fit, X, y)\n", "def test_libsvm_sparse_predict_proba():\n    # Test that predict_proba works with sparse input\n    X, y = make_classification(n_samples=50, n_features=10, random_state=0)\n    X_sparse = sparse.csr_matrix(X)\n\n    clf_dense = svm.SVC(probability=True, random_state=0)\n    clf_sparse = svm.SVC(probability=True, random_state=0)\n\n    clf_dense.fit(X, y)\n    clf_sparse.fit(X_sparse, y)\n\n    assert_array_almost_equal(clf_dense.predict_proba(X),\n                              clf_sparse.predict_proba(X_sparse))\n", "def test_libsvm_sparse_predict_proba():\n    # Test that sparse predict_proba gives the same result as dense predict_proba\n    X, y = make_classification(n_samples=100, n_features=20, random_state=0)\n    X_sparse = sparse.csr_matrix(X)\n\n    clf_dense = svm.SVC(probability=True, random_state=0)\n    clf_sparse = svm.SVC(probability=True, random_state=0)\n    clf_dense.fit(X, y)\n    clf_sparse.fit(X_sparse, y)\n\n    assert_array_almost_equal(clf_dense.predict_proba(X),\n                              clf_sparse.predict_proba(X_sparse))\n", "def test_libsvm_sparse_predict_proba():\n    # Test libsvm's sparse predict_proba implementation\n    X, y = make_classification(n_samples=50, n_features=20, random_state=0)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n    clf = svm.SVC(probability=True)\n    clf.fit(X_train, y_train)\n\n    X_test_sparse = sparse.csr_matrix(X_test)\n    y_proba_dense = clf.predict_proba(X_test)\n    y_proba_sparse = clf.predict_proba(X_test_sparse)\n\n    assert_array_almost_equal(y_proba_dense, y_proba_sparse)\n"], "sample_855": ["def test_dummy_classifier_predict_sparse():\n    # Test that predict returns sparse array when y is sparse\n    X = np.array([[0], [0], [0], [0]])  # ignored\n    y = sp.csr_matrix(np.array([[1, 0], [2, 0], [1, 0], [1, 3]]))\n    clf = DummyClassifier(strategy='most_frequent')\n    clf.fit(X, y)\n    y_pred = clf.predict(X)\n    assert sp.issparse(y_pred)\n    assert_array_equal(y_pred.toarray(), np.tile([1, 0], (4, 1)))\n", "def test_dummy_classifier_predict_sparse():\n    X = np.array([[0], [0], [0]])\n    y = sp.csr_matrix(np.array([[1, 0], [2, 0], [1, 0]]))\n\n    clf = DummyClassifier(strategy=\"most_frequent\")\n    clf.fit(X, y)\n    assert sp.issparse(clf.predict(X))\n\n    clf = DummyClassifier(strategy=\"stratified\")\n    clf.fit(X, y)\n    assert sp.issparse(clf.predict(X))\n", "def test_dummy_classifier_sparse_target():\n    # Test that the classifier handles sparse targets correctly\n    X = np.array([[0], [0], [0], [0]])  # ignored\n    y = sp.csr_matrix(np.array([1, 2, 1, 1]))\n    clf = DummyClassifier(strategy=\"stratified\")\n    clf.fit(X, y)\n    y_pred = clf.predict(X)\n    assert y.shape == y_pred.shape\n    assert sp.issparse(y_pred)\n", "def test_dummy_regressor_quantile():\n    X = np.array([[0], [0], [0], [0]])  # ignored\n    y = np.array([1, 2, 3, 4])\n\n    # Test quantile=0.5 (median)\n    est = DummyRegressor(strategy='quantile', quantile=0.5)\n    est.fit(X, y)\n    y_pred = est.predict(X)\n    assert_array_equal(y_pred, np.tile(np.median(y), (X.shape[0],)))\n\n    # Test quantile=0.25\n    est = DummyRegressor(strategy='quantile', quantile=0.25)\n    est.fit(X, y)\n    y_pred = est.predict(X)\n    assert_array_equal(y_pred, np.tile(np.percentile(y, 25), (X.shape[0],)))\n\n    # Test quantile=0.75\n    est = DummyRegressor(strategy='quantile', quantile=0.75)\n    est.fit(X, y)\n    y_pred = est.predict(X)\n    assert_array_equal(y_pred, np.tile(np.percentile(y, 75), (X.shape[0],)))\n", "def test_dummy_classifier_constant_strategy():\n    # Test that the constant strategy of DummyClassifier works correctly\n    X = np.array([[0], [0], [0], [0]])  # ignored\n    y = np.array([1, 2, 1, 1])\n\n    clf = DummyClassifier(strategy='constant', constant=2)\n    clf.fit(X, y)\n\n    assert_array_equal(clf.predict(X), np.array([2, 2, 2, 2]))\n\n    # Test with multiple outputs\n    y = np.array([[1, 0],\n                  [2, 0],\n                  [1, 0],\n                  [1, 3]])\n    clf = DummyClassifier(strategy='constant', constant=[2, 4])\n    clf.fit(X, y)\n\n    assert_array_equal(clf.predict(X), np.array([[2, 4],\n                                                  [2, 4],\n                                                  [2, 4],\n                                                  [2, 4]]))\n"], "sample_856": ["def test_split_numpy_array_last_axis():\n    # Check that the last axis is not lost during train_test_split\n    X = np.random.randint(0, 10, size=(4, 4, 4))\n    y = np.random.randint(0, 2, size=(4,))\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5,\n                                                        random_state=42)\n    assert X_train.shape == (2, 4, 4)\n", "def test_cross_validator_repr():\n    # Test that repr works for all cross validators\n    cv = [\n        KFold(),\n        StratifiedKFold(),\n        GroupKFold(),\n        TimeSeriesSplit(),\n        LeaveOneOut(),\n        LeaveOneGroupOut(),\n        LeavePOut(),\n        LeavePGroupsOut(),\n        ShuffleSplit(),\n        GroupShuffleSplit(),\n        StratifiedShuffleSplit(),\n        PredefinedSplit(np.array([1, 1, 2, 2])),\n        RepeatedKFold(),\n        RepeatedStratifiedKFold()\n    ]\n\n    for cv_instance in cv:\n        repr(cv_instance)\n", "def test_build_repr():\n    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n    repr = _build_repr(cv)\n    assert repr == \"KFold(n_splits=5, random_state=42, shuffle=True)\"\n\n    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    repr = _build_repr(cv)\n    assert repr == \"StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\"\n", "def test_build_repr():\n    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n    repr = _build_repr(cv)\n    assert repr == \"KFold(n_splits=5, random_state=42, shuffle=True)\"\n\n    cv = StratifiedKFold(n_splits=3)\n    repr = _build_repr(cv)\n    assert repr == \"StratifiedKFold(n_splits=3, random_state=None, shuffle=False)\"\n", "def test_build_repr():\n    cv = KFold(n_splits=5, shuffle=True, random_state=0)\n    repr = _build_repr(cv)\n    assert repr == \"KFold(n_splits=5, random_state=0, shuffle=True)\"\n\n    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n    repr = _build_repr(cv)\n    assert repr == \"StratifiedKFold(n_splits=5, random_state=0, shuffle=True)\"\n"], "sample_857": ["def test_tree_regressor_multioutput():\n    # Check estimators on multi-output problems.\n    X = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1]]\n    y = [[-1, 0], [-1, 0], [-1, 0], [1, 1], [1, 1], [1, 1]]\n\n    clf = DecisionTreeRegressor()\n    y_pred = clf.fit(X, y).predict(X)\n    assert_array_almost_equal(y, y_pred)\n\n    clf = ExtraTreeRegressor()\n    y_pred = clf.fit(X, y).predict(X)\n    assert_array_almost_equal(y, y_pred)\n", "def test_criterion_init():\n    # Test that all criteria can be instantiated and have the correct\n    # number of parameters.\n    for name, criterion_cls in CRITERIA_CLF.items():\n        criterion = criterion_cls(n_outputs=1, n_classes=3)\n        assert hasattr(criterion, \"n_outputs\")\n        assert hasattr(criterion, \"n_classes\")\n\n    for name, criterion_cls in CRITERIA_REG.items():\n        criterion = criterion_cls(n_outputs=2, n_samples=10)\n        assert hasattr(criterion, \"n_outputs\")\n        assert hasattr(criterion, \"n_samples\")\n", "def test_min_impurity_decrease():\n    # Test if min_impurity_decrease is working as expected\n    X = [[1, 2], [3, 4], [5, 6], [7, 8]]\n    y = [0, 0, 1, 1]\n\n    clf = DecisionTreeClassifier(min_impurity_decrease=1.0)\n    clf.fit(X, y)\n\n    # With min_impurity_decrease=1.0, the tree should not split\n    assert clf.tree_.node_count == 1\n\n    clf = DecisionTreeClassifier(min_impurity_decrease=0.0)\n    clf.fit(X, y)\n\n    # With min_impurity_decrease=0.0, the tree should split\n    assert clf.tree_.node_count > 1\n", "def test_tree_value_dtype():\n    # Check that the dtype of tree.value is consistent with the type of y.\n    # See https://github.com/scikit-learn/scikit-learn/issues/17755\n\n    # Classification with integer labels\n    clf = DecisionTreeClassifier(random_state=0)\n    clf.fit(X, y)\n    assert clf.tree_.value.dtype == np.float64\n\n    # Classification with string labels\n    y_str = [str(label) for label in y]\n    clf.fit(X, y_str)\n    assert clf.tree_.value.dtype == object\n\n    # Regression with continuous targets\n    reg = DecisionTreeRegressor(random_state=0)\n    reg.fit(X, np.random.rand(len(X)))\n    assert reg.tree_.value.dtype == np.float64\n\n    # Regression with integer targets\n    reg.fit(X, np.random.randint(0, 10, size=len(X)))\n    assert reg.tree_.value.dtype == np.float64\n", "def test_prune_tree():\n    # Test that pruning a tree with ccp_alpha works as expected\n    X, y = DATASETS[\"clf_small\"][\"X\"], DATASETS[\"clf_small\"][\"y\"]\n    clf = DecisionTreeClassifier(random_state=0)\n    clf.fit(X, y)\n\n    # Get the original number of nodes\n    original_node_count = clf.tree_.node_count\n\n    # Prune the tree\n    clf.prune_tree(ccp_alpha=0.1)\n\n    # Check that the number of nodes has decreased\n    assert clf.tree_.node_count < original_node_count\n\n    # Check that the pruned tree still gives the same predictions\n    y_pred_original = DecisionTreeClassifier(random_state=0).fit(X, y).predict(X)\n    y_pred_pruned = clf.predict(X)\n    assert_array_equal(y_pred_original, y_pred_pruned)\n"], "sample_858": ["def test_voting_regressor_weights():\n    # Test that weights work for VotingRegressor\n    X, y = datasets.make_regression(n_samples=100, n_features=10)\n    r1 = LinearRegression()\n    r2 = RandomForestRegressor()\n    vr = VotingRegressor([('lr', r1), ('rf', r2)], weights=[0.5, 0.5])\n    vr.fit(X, y)\n    y_pred = vr.predict(X)\n\n    # Test that output is a weighted average of the two regressors\n    y_pred_r1 = r1.fit(X, y).predict(X)\n    y_pred_r2 = r2.fit(X, y).predict(X)\n    y_pred_weighted = (y_pred_r1 * 0.5 + y_pred_r2 * 0.5)\n    assert_almost_equal(y_pred, y_pred_weighted)\n\n    # Test that weights are normalized if they don't add up to 1\n    vr = VotingRegressor([('lr', r1), ('rf', r2)], weights=[1, 2])\n    vr.fit(X, y)\n    y_pred = vr.predict(X)\n    y_pred_weighted = (y_pred_r1 * 1/3 + y_pred_r2 * 2/3)\n    assert_almost_equal(y_pred, y_pred_weighted)\n", "def test_voting_classifier_predict_proba():\n    # Test predict_proba for VotingClassifier\n    clf1 = LogisticRegression(random_state=0)\n    clf2 = RandomForestClassifier(random_state=0)\n    clf3 = GaussianNB()\n    eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='soft')\n    eclf.fit(X, y)\n    assert_array_almost_equal(eclf.predict_proba(X).sum(axis=1), np.ones(len(X)))\n", "def test_voting_classifier_predict_proba():\n    # Test that predict_proba returns the expected probabilities\n    clf1 = LogisticRegression(random_state=1)\n    clf2 = RandomForestClassifier(n_estimators=10, random_state=1)\n    clf3 = GaussianNB()\n    eclf = VotingClassifier(estimators=[\n        ('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='soft')\n    eclf.fit(X, y)\n    probas = eclf.predict_proba(X)\n    assert np.allclose(probas.sum(axis=1), 1)\n    assert_array_almost_equal(eclf.predict_proba(X),\n                              (eclf._collect_probas(X).mean(axis=0)))\n", "def test_voting_classifier_predict_proba():\n    # Test predict_proba for VotingClassifier\n    clf1 = LogisticRegression(random_state=0)\n    clf2 = RandomForestClassifier(random_state=0)\n    eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2)], voting='soft')\n    eclf.fit(X, y)\n    proba = eclf.predict_proba(X)\n    assert np.allclose(proba.sum(axis=1), 1)\n    assert_array_almost_equal(eclf.predict_proba(X), (eclf._collect_probas(X)).mean(axis=0))\n", "def test_voting_regressor_weights():\n    # Test weights with VotingRegressor\n    r1 = LinearRegression()\n    r2 = RandomForestRegressor(n_estimators=10, random_state=0)\n    vr = VotingRegressor([('lr', r1), ('rf', r2)], weights=[0.5, 0.5])\n    vr.fit(X_r, y_r)\n    assert_almost_equal(vr.predict(X_r), (r1.fit(X_r, y_r).predict(X_r) +\n                                          r2.fit(X_r, y_r).predict(X_r)) / 2)\n\n    vr = VotingRegressor([('lr', r1), ('rf', r2)], weights=[0.8, 0.2])\n    vr.fit(X_r, y_r)\n    assert_almost_equal(vr.predict(X_r), (0.8 * r1.fit(X_r, y_r).predict(X_r) +\n                                          0.2 * r2.fit(X_r, y_r).predict(X_r)))\n"], "sample_859": ["def test_lasso_path_return_n_iter():\n    # Test that lasso_path return correct n_iter\n    X, y = load_boston(return_X_y=True)\n    max_iter = 1000\n\n    alphas, coefs, dual_gaps, n_iters = lasso_path(\n        X, y, max_iter=max_iter, return_n_iter=True)\n\n    for n_iter in n_iters:\n        assert n_iter <= max_iter\n", "def test_lasso_path_return_n_iter():\n    # Test that lasso_path and enet_path return the number of iterations\n    X, y = load_boston(return_X_y=True)\n    result = lasso_path(X, y, return_n_iter=True)\n    assert len(result) == 4\n    _, coefs, dual_gaps, n_iters = result\n    assert isinstance(n_iters, list)\n\n    result = enet_path(X, y, return_n_iter=True)\n    assert len(result) == 4\n    _, coefs, dual_gaps, n_iters = result\n    assert isinstance(n_iters, list)\n", "def test_enet_path_precompute():\n    # Test that precomputing Gram makes a difference when iterating over\n    # alphas with warm_start\n    X, y = load_boston(return_X_y=True)\n    n_samples, n_features = X.shape\n\n        return enet_path(X, y, alphas=alphas, **kwargs)[1]\n\n    alphas = np.logspace(-2, 0, 30)\n    precompute = np.dot(X.T, X)\n    path_1 = my_enet_path(X, y, alphas, precompute=precompute)\n    path_2 = my_enet_path(X, y, alphas, precompute=False)\n    assert_array_almost_equal(path_1, path_2)\n\n    # With warm_start=False, passing in precompute should still be faster\n    with ignore_warnings(category=ConvergenceWarning):\n        path_1 = my_enet_path(X, y, alphas, precompute=precompute,\n                              warm_start=False)\n    with ignore_warnings(category=ConvergenceWarning):\n        path_2 = my_enet_path(X, y, alphas, precompute=False, warm_start=False)\n    assert_array_almost_equal(path_1, path_2)\n", "def test_lasso_positive_constraint():\n    # Test that the positive constraint works correctly\n    X, y = load_boston(return_X_y=True)\n    X = X[:, [0, 1, 2]]\n\n    lasso = Lasso(positive=True)\n    lasso.fit(X, y)\n    assert np.all(lasso.coef_ >= 0)\n\n    # Check that the coefficients are non-negative when all coefficients are\n    # positive and that the solution is almost identical to the one obtained\n    # without the positivity constraint\n    lasso = Lasso(positive=False)\n    lasso.fit(X, y)\n    lasso_pos = Lasso(positive=True)\n    lasso_pos.fit(X, y)\n    assert_array_almost_equal(lasso.coef_, lasso_pos.coef_)\n", "def test_multi_task_lasso_and_elastic_net():\n    X, y = load_boston(return_X_y=True)\n    X = X[:10, :]\n    y = np.column_stack((y[:10], y[:10]))\n\n    # L1 regularization\n    multi_task_lasso = MultiTaskLasso(alpha=1e-3, fit_intercept=False,\n                                      tol=1e-6)\n    multi_task_lasso.fit(X, y)\n\n    # Check that L2 norm of each row of coefs is less than or equal to alpha\n    for i in range(multi_task_lasso.coef_.shape[0]):\n        assert np.linalg.norm(multi_task_lasso.coef_[i]) <= 1e-3\n\n    # Elastic net regularization\n    multi_task_enet = MultiTaskElasticNet(alpha=1e-3, l1_ratio=0.5,\n                                          fit_intercept=False, tol=1e-6)\n    multi_task_enet.fit(X, y)\n\n    # Check that L2 norm of each row of coefs is less than or equal to alpha * l1_ratio\n    for i in range(multi_task_enet.coef_.shape[0]):\n        assert np.linalg.norm(multi_task_enet.coef_[i]) <= 1e-3 * 0.5\n"], "sample_860": ["def test_check_scalar():\n    # Test that check_scalar raises the correct errors\n    with assert_raises_regex(TypeError, \"`param` must be an instance of \"):\n        check_scalar(\"test\", \"param\", int)\n\n    with assert_raises_regex(ValueError, \"`param`= -1, must be >= 0.\"):\n        check_scalar(-1, \"param\", int, min_val=0)\n\n    with assert_raises_regex(ValueError, \"`param`= 2, must be <= 1.\"):\n        check_scalar(2, \"param\", int, max_val=1)\n\n    # Test that check_scalar does not raise any errors when input is valid\n    check_scalar(1, \"param\", int)\n    check_scalar(1.0, \"param\", float)\n    check_scalar(1, \"param\", int, min_val=0, max_val=2)\n", "def test_check_sample_weight():\n    # Test that sample weights are checked correctly\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([0, 1])\n\n    # Check that passing None for sample_weight returns an array of ones\n    sw = _check_sample_weight(None, X)\n    assert_array_equal(sw, np.ones(X.shape[0]))\n\n    # Check that passing a Number for sample_weight returns an array filled with that number\n    sw = _check_sample_weight(0.5, X)\n    assert_array_equal(sw, np.full(X.shape[0], 0.5))\n\n    # Check that passing an array for sample_weight returns the array\n    sw = np.array([0.5, 0.5])\n    sw_checked = _check_sample_weight(sw, X)\n    assert_array_equal(sw, sw_checked)\n\n    # Check that passing an array with wrong shape raises a ValueError\n    sw = np.array([0.5])\n    with pytest.raises(ValueError):\n        _check_sample_weight(sw, X)\n\n    # Check that passing an array with non-float dtype raises a TypeError\n    sw = np.array([0.5, 0.5], dtype='int')\n    with pytest.raises(TypeError):\n        _check_sample_weight(sw, X)\n", "def test_check_scalar():\n    # Test with integer\n    check_scalar(1, 'test_int', int)\n    assert_raises(TypeError, check_scalar, 1, 'test_int', str)\n\n    # Test with float\n    check_scalar(1.0, 'test_float', float)\n    assert_raises(TypeError, check_scalar, 1.0, 'test_float', str)\n\n    # Test with min and max values\n    check_scalar(5, 'test_int', int, min_val=0, max_val=10)\n    assert_raises(ValueError, check_scalar, -1, 'test_int', int, min_val=0)\n    assert_raises(ValueError, check_scalar, 11, 'test_int', int, max_val=10)\n", "def test_check_symmetric_sparse():\n    # Check that a sparse matrix is symmetrized correctly\n    X = sp.csr_matrix(np.array([[1, 2], [3, 4]]))\n    X_sym = check_symmetric(X)\n    assert_allclose_dense_sparse(X_sym.toarray(), np.array([[1, 2.5], [2.5, 4]]))\n\n    # Check that a sparse matrix with a different format is symmetrized correctly\n    X = sp.coo_matrix(np.array([[1, 2], [3, 4]]))\n    X_sym = check_symmetric(X)\n    assert_allclose_dense_sparse(X_sym.toarray(), np.array([[1, 2.5], [2.5, 4]]))\n", "def test__check_sample_weight():\n    # Test that _check_sample_weight returns an array of ones when sample_weight=None\n    X = np.array([[1, 2], [3, 4]])\n    sample_weight = None\n    result = _check_sample_weight(sample_weight, X)\n    assert_array_equal(result, np.ones(2))\n\n    # Test that _check_sample_weight returns a numpy array\n    sample_weight = [1, 2]\n    result = _check_sample_weight(sample_weight, X)\n    assert isinstance(result, np.ndarray)\n\n    # Test that _check_sample_weight raises an error when sample_weight has the wrong shape\n    sample_weight = [1, 2, 3]\n    with pytest.raises(ValueError):\n        _check_sample_weight(sample_weight, X)\n\n    # Test that _check_sample_weight raises an error when sample_weight is not a number or numpy array\n    sample_weight = 'a'\n    with pytest.raises(ValueError):\n        _check_sample_weight(sample_weight, X)\n"], "sample_861": ["def test_parameter_grid_with_iterable_values():\n    param_grid = {'a': [1, 2], 'b': [(3, 4), (5, 6)]}\n    grid = ParameterGrid(param_grid)\n    assert len(grid) == 4\n    assert list(grid) == [{'a': 1, 'b': (3, 4)}, {'a': 1, 'b': (5, 6)},\n                         {'a': 2, 'b': (3, 4)}, {'a': 2, 'b': (5, 6)}]\n    assert_grid_iter_equals_getitem(grid)\n", "def test_base_search_cv_inheritance():\n    # Test that inheritance works with a custom scorer and no default scorer.\n    class MySVC(LinearSVCNoScore):\n        pass\n\n    class MySearchCV(BaseSearchCV):\n            evaluate_candidates([{'C': 1}])\n\n    X, y = make_classification(n_samples=100, n_features=10, n_informative=5)\n    my_svc = MySVC()\n    my_search = MySearchCV(my_svc, scoring='accuracy', cv=5)\n    my_search.fit(X, y)\n\n    assert hasattr(my_search, 'best_estimator_')\n    assert isinstance(my_search.best_estimator_, MySVC)\n", "def test_parameter_grid_iter_and_getitem_equivalence():\n    param_grid = {'a': [1, 2], 'b': [True, False]}\n    grid = ParameterGrid(param_grid)\n    assert_grid_iter_equals_getitem(grid)\n\n    param_grid = [{'kernel': ['linear']}, {'kernel': ['rbf'], 'gamma': [1, 10]}]\n    grid = ParameterGrid(param_grid)\n    assert_grid_iter_equals_getitem(grid)\n\n    sampler = ParameterSampler(param_grid, n_iter=10)\n    assert_grid_iter_equals_getitem(sampler)\n", "def test_parameter_input(klass, input, error_type, error_message):\n    with pytest.raises(error_type, match=error_message):\n        klass(input)\n", "def test_parameter_grid_iter_and_getitem():\n    param_grid = {'a': [1, 2], 'b': [3, 4]}\n    grid = ParameterGrid(param_grid)\n\n    # Test iter\n    params_list = list(grid)\n    assert len(params_list) == 4\n\n    # Test getitem\n    for i in range(len(grid)):\n        assert isinstance(grid[i], dict)\n\n    # Test that iter and getitem return the same elements\n    assert_grid_iter_equals_getitem(grid)\n"], "sample_862": ["def test_tfidf_vectorizer_max_features():\n    # Test that the max_features parameter is used correctly in TfidfVectorizer\n    vectorizer = TfidfVectorizer(max_features=5)\n    X = vectorizer.fit_transform(JUNK_FOOD_DOCS)\n\n    # The number of features should be 5\n    assert X.shape[1] == 5\n\n    # The feature names should be the most frequent words\n    feature_names = vectorizer.get_feature_names()\n    assert 'the' in feature_names\n    assert 'pizza' in feature_names\n    assert 'beer' in feature_names\n    assert 'burger' in feature_names\n    assert 'copyright' in feature_names\n", "def test_tfidf_vectorizer_empty_vocabulary():\n    # Test that TfidfVectorizer works with empty vocabulary.\n    X = [\"foo\", \"bar\", \"baz\"]\n    v = TfidfVectorizer(vocabulary={})\n    assert_array_equal(v.fit_transform(X).toarray(), np.zeros((3, 0)))\n    assert_no_warnings(v.fit_transform, X)\n", "def test_tfidf_vectorizer_stop_words():\n    # Ensure that stop words are properly ignored\n\n    v1 = TfidfVectorizer(stop_words='english')\n    v2 = TfidfVectorizer()\n\n    X1 = v1.fit_transform(ALL_FOOD_DOCS)\n    X2 = v2.fit_transform(ALL_FOOD_DOCS)\n\n    assert X1.shape[1] < X2.shape[1]\n    assert_array_equal(v1.stop_words_, ENGLISH_STOP_WORDS)\n\n    v3 = TfidfVectorizer(stop_words=['the', 'salad'])\n    X3 = v3.fit_transform(ALL_FOOD_DOCS)\n    assert X3.shape[1] < X2.shape[1]\n\n    v4 = TfidfVectorizer(stop_words=lambda x: x == 'the')\n    X4 = v4.fit_transform(ALL_FOOD_DOCS)\n    assert X4.shape[1] < X2.shape[1]\n", "def test_tfidfvectorizer_get_stop_words():\n    v = TfidfVectorizer(stop_words='english')\n    stop = v.get_stop_words()\n    assert isinstance(stop, frozenset)\n    assert 'a' in stop\n    assert len(stop) == len(ENGLISH_STOP_WORDS)\n\n    v = TfidfVectorizer(stop_words=None)\n    assert v.get_stop_words() is None\n\n    stop_list = ['and', 'the']\n    v = TfidfVectorizer(stop_words=stop_list)\n    assert_array_equal(sorted(v.get_stop_words()), sorted(stop_list))\n", "def test_tfidf_vectorizer_pickle_idf():\n    # Check for an issue with pickling TF-IDF vectorizers\n    v = TfidfVectorizer()\n    v.fit(ALL_FOOD_DOCS)\n    assert hasattr(v, 'idf_')\n    pickle.dumps(v)\n\n    # Check that the attributes are still there after reloading\n    v_reloaded = pickle.loads(pickle.dumps(v))\n    assert hasattr(v_reloaded, 'idf_')\n"], "sample_863": ["def test_pipeline_with_sample_weight():\n    # Test that pipeline passes sample_weight to estimators that support it\n\n    X = iris.data\n    y = iris.target\n    sample_weight = np.random.rand(len(X))\n\n    # Estimator with support for sample_weight in fit\n    estimator = LogisticRegression()\n    pipeline = make_pipeline(estimator)\n\n    pipeline.fit(X, y, logisticregression__sample_weight=sample_weight)\n    assert_array_equal(estimator.sample_weight, sample_weight)\n\n    # Estimator with support for sample_weight in fit and score\n    estimator = FitParamT()\n    pipeline = make_pipeline(estimator)\n\n    pipeline.fit(X, y, fitparamt__should_succeed=True)\n    assert estimator.successful\n\n    pipeline.score(X, y, fitparamt__sample_weight=sample_weight)\n    assert_array_equal(estimator.X, X * sample_weight)\n", "def test_pipeline_transformer_weights():\n    # Test if transformer weights are taken into account in FeatureUnion\n    X = np.array([[1, 2], [3, 4]])\n    tf1 = Transf()\n    tf2 = Transf()\n\n    fu = FeatureUnion([(\"tf1\", tf1), (\"tf2\", tf2)], transformer_weights={\"tf1\": 10})\n    result = fu.fit_transform(X)\n    expected = np.hstack([X * 10, X])\n    assert_array_equal(result, expected)\n\n    fu.set_params(transformer_weights={\"tf2\": 10})\n    result = fu.fit_transform(X)\n    expected = np.hstack([X, X * 10])\n    assert_array_equal(result, expected)\n\n    fu = make_union(tf1, tf2, tf1, tf2, transformer_weights=[10, 5, 0, 20])\n    result = fu.fit_transform(X)\n    expected = np.hstack([X * 10, X * 5, X * 0, X * 20])\n    assert_array_equal(result, expected)\n", "def test_pipeline_fit_predict_on_pipeline():\n    # Test that fit_predict works on a pipeline.\n    X = np.array([[1], [2]])\n    y = np.array([1, 2])\n\n    pipe = make_pipeline(Mult(), FitParamT())\n    result = assert_warns_message(UserWarning,\n                                  \"The 'should_succeed' parameter is not \"\n                                  \"functional in fit_predict\",\n                                  pipe.fit_predict, X, y,\n                                  should_succeed=True)\n    assert result == [True, True]\n", "def test_pipeline_memory():\n    # Test pipeline with caching of transformers\n    cachedir = mkdtemp()\n    try:\n        # Create a pipeline of two simple transformers and a regressor\n        pipeline = Pipeline([\n            ('trans1', DummyTransf()),\n            ('trans2', DummyTransf()),\n            ('regressor', DummyRegressor())\n        ], memory=joblib.Memory(location=cachedir, verbose=0))\n\n        # Fit the pipeline once\n        X = np.random.randn(10, 3)\n        y = np.random.randn(10)\n        pipeline.fit(X, y)\n\n        # Check that the transformers have been cached\n        trans1_timestamp = pipeline.named_steps['trans1'].timestamp_\n        trans2_timestamp = pipeline.named_steps['trans2'].timestamp_\n\n        # Fit the pipeline again with the same data\n        pipeline.fit(X, y)\n\n        # Check that the transformers have not been refitted\n        assert pipeline.named_steps['trans1'].timestamp_ == trans1_timestamp\n        assert pipeline.named_steps['trans2'].timestamp_ == trans2_timestamp\n\n    finally:\n        shutil.rmtree(cachedir)\n", "def test_pipeline_transformers_timestamp():\n    # Test that timestamps are different when fitting\n    # a pipeline multiple times.\n    transf = DummyTransf()\n    pipe = Pipeline([('transf', transf)])\n\n    pipe.fit(iris.data)\n    ts1 = transf.timestamp_\n    time.sleep(0.1)\n\n    pipe.fit(iris.data)\n    ts2 = transf.timestamp_\n\n    assert ts1 != ts2\n\n    # Check that the timestamp doesn't change when calling\n    # fit again with the same data.\n    pipe = Pipeline([('transf', DummyTransf())], memory=joblib.Memory(cachedir='.'))\n\n    pipe.fit(iris.data)\n    ts1 = pipe.named_steps['transf'].timestamp_\n    time.sleep(0.1)\n\n    pipe.fit(iris.data)\n    ts2 = pipe.named_steps['transf'].timestamp_\n\n    assert ts1 == ts2\n"], "sample_864": ["def test_estimate_bandwidth():\n    # Test that the estimated bandwidth is approximately equal to the manually\n    # computed value for a small dataset.\n    np.random.seed(0)\n    X = np.random.rand(100, 2)\n    bandwidth = estimate_bandwidth(X, quantile=0.5, n_samples=None,\n                                   random_state=42)\n    distances = np.linalg.norm(X[:, None] - X[None, :], axis=2)\n    distances = distances[np.triu_indices(distances.shape[0], k=1)]\n    median_distance = np.median(distances)\n    assert_array_almost_equal(bandwidth, median_distance, decimal=2)\n\n    # Test that the estimated bandwidth is the same when using parallel\n    # computation.\n    bandwidth_parallel = estimate_bandwidth(X, quantile=0.5, n_samples=None,\n                                            random_state=42, n_jobs=-1)\n    assert_array_almost_equal(bandwidth, bandwidth_parallel)\n", "def test_mean_shift_predict():\n    ms = MeanShift()\n    ms.fit(X)\n    labels1 = ms.labels_\n    labels2 = ms.predict(X)\n    assert_array_equal(labels1, labels2)\n", "def test_estimate_bandwidth():\n    # Test that the estimate_bandwidth function returns a positive value\n    bandwidth = estimate_bandwidth(X)\n    assert bandwidth > 0\n\n    # Test that the estimate_bandwidth function raises a warning when no neighbors are found\n    with warnings.catch_warnings(record=True) as w:\n        estimate_bandwidth(X, quantile=0.001)\n        assert len(w) == 1\n        assert issubclass(w[-1].category, UserWarning)\n\n    # Test that the estimate_bandwidth function works with a random state\n    bandwidth1 = estimate_bandwidth(X, random_state=42)\n    bandwidth2 = estimate_bandwidth(X, random_state=42)\n    assert bandwidth1 == bandwidth2\n", "def test_estimate_bandwidth():\n    # Test estimate_bandwidth with different quantiles\n    for quantile in [0.1, 0.3, 0.5]:\n        bw = estimate_bandwidth(X, quantile=quantile)\n        assert bw > 0\n\n    # Test estimate_bandwidth with random_state\n    bw1 = estimate_bandwidth(X, random_state=42)\n    bw2 = estimate_bandwidth(X, random_state=42)\n    assert bw1 == bw2\n\n    # Test estimate_bandwidth with n_samples\n    bw = estimate_bandwidth(X, n_samples=100)\n    assert bw > 0\n", "def test_estimate_bandwidth():\n    # Test estimate_bandwidth with different quantiles\n    bandwidth = estimate_bandwidth(X, quantile=0.5)\n    assert bandwidth > 0\n\n    bandwidth = estimate_bandwidth(X, quantile=0.8)\n    assert bandwidth > 0\n\n    # Test estimate_bandwidth with n_samples\n    bandwidth = estimate_bandwidth(X, n_samples=100)\n    assert bandwidth > 0\n\n    # Test estimate_bandwidth with random_state\n    bandwidth1 = estimate_bandwidth(X, random_state=42)\n    bandwidth2 = estimate_bandwidth(X, random_state=42)\n    assert bandwidth1 == bandwidth2\n\n    # Test estimate_bandwidth with n_jobs\n    bandwidth = estimate_bandwidth(X, n_jobs=-1)\n    assert bandwidth > 0\n"], "sample_865": ["def test_tree_feature_importances():\n    # Test that feature importances are computed correctly and add up to 1.\n    X, y = datasets.make_classification(n_samples=1000, n_features=10,\n                                       n_informative=3, n_redundant=2,\n                                       n_repeated=0, random_state=42)\n\n    for Tree in [DecisionTreeClassifier, DecisionTreeRegressor]:\n        clf = Tree(random_state=42)\n        clf.fit(X, y)\n        feature_importances = clf.feature_importances_\n        assert_array_almost_equal(feature_importances.sum(), 1,\n                                  err_msg=\"Feature importances do not sum to 1\")\n\n    # Check feature importances with max_features < n_features\n    clf = DecisionTreeClassifier(max_features=5, random_state=42)\n    clf.fit(X, y)\n    feature_importances = clf.feature_importances_\n    assert_array_almost_equal(feature_importances.sum(), 1,\n                              err_msg=\"Feature importances do not sum to 1\")\n", "def test_pruning():\n    # Test pruning of a tree with two features and 3 classes.\n    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n    y = np.array([0, 1, 2, 0])\n\n    clf = DecisionTreeClassifier(random_state=0)\n    clf.fit(X, y)\n\n    path = clf.cost_complexity_pruning_path(X, y)\n    ccp_alphas, impurities = path.ccp_alphas, path.impurities\n\n    # Assert that there are as many alphas as there are leaves in the tree\n    # minus one (the root node).\n    assert len(ccp_alphas) == clf.tree_.n_leaves - 1\n\n    # Assert that the alpha values are non-decreasing.\n    assert np.all(np.diff(ccp_alphas) >= 0)\n\n    # Assert that the impurities are non-increasing.\n    assert np.all(np.diff(impurities) <= 0)\n\n    # Check if the last tree is a single node\n    last_tree = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alphas[-1])\n    last_tree.fit(X, y)\n    assert last_tree.tree_.n_leaves == 1\n", "def test_tree_value_dtype():\n    # Test the dtype of tree_.value for different types of target variables.\n    X = np.array([[1, 2], [3, 4]])\n    y_int = np.array([1, 2])\n    y_float = np.array([1.0, 2.0])\n\n    clf = DecisionTreeClassifier(random_state=0)\n    clf.fit(X, y_int)\n    assert clf.tree_.value.dtype == np.int64\n\n    reg = DecisionTreeRegressor(random_state=0)\n    reg.fit(X, y_float)\n    assert reg.tree_.value.dtype == np.float64\n\n    clf = DecisionTreeClassifier(random_state=0)\n    clf.fit(X, y_float)\n    assert clf.tree_.value.dtype == np.float64\n\n    y_multiclass = np.array([[1, 0, 0], [0, 1, 0]])\n    clf = DecisionTreeClassifier(random_state=0)\n    clf.fit(X, y_multiclass)\n    assert clf.tree_.value.dtype == np.int64\n\n    y_multilabel = np.array([[1, 1], [0, 1]])\n    clf = DecisionTreeClassifier(random_state=0)\n    clf.fit(X, y_multilabel)\n    assert clf.tree_.value.dtype == np.int64\n", "def test_feature_importances_with_zero_impurity():\n    # Test that feature importances are calculated correctly when the tree has\n    # zero impurity (i.e., all samples belong to the same class).\n\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([0, 0])\n\n    clf = DecisionTreeClassifier(random_state=0)\n    clf.fit(X, y)\n\n    assert_array_equal(clf.feature_importances_, np.array([0.5, 0.5]))\n", "def test_tree_min_impurity_decrease():\n    # Test if min_impurity_decrease is taken into account when building trees.\n    X = [[0, 0], [0, 1], [0, 2], [1, 0], [1, 1], [1, 2]]\n    y = [0, 0, 0, 1, 1, 1]\n\n    clf = DecisionTreeClassifier(random_state=0, min_impurity_decrease=1.0)\n    clf.fit(X, y)\n\n    assert clf.tree_.node_count == 3  # only root and two leaves\n\n    clf = DecisionTreeClassifier(random_state=0, min_impurity_decrease=0.0)\n    clf.fit(X, y)\n\n    assert clf.tree_.node_count > 3  # should be a larger tree\n\n    reg = DecisionTreeRegressor(random_state=0, min_impurity_decrease=1.0)\n    reg.fit(X, y)\n\n    assert reg.tree_.node_count == 3  # only root and two leaves\n\n    reg = DecisionTreeRegressor(random_state=0, min_impurity_decrease=0.0)\n    reg.fit(X, y)\n\n    assert reg.tree_.node_count > 3  # should be a larger tree\n"], "sample_866": ["def test_affinity_propagation_equal_similarities_and_preferences():\n    # Test the case when all training samples have equal similarities and\n    # equal preferences.\n\n    # Generate a matrix with equal similarities\n    S = np.full((10, 10), 1)\n\n    # Set preferences to a value smaller than the similarities\n    preference = 0.5\n\n    # Check that all samples are assigned to a single cluster\n    cluster_centers_indices, labels = affinity_propagation(S, preference)\n    assert len(cluster_centers_indices) == 1\n    assert_array_equal(labels, np.zeros(10))\n\n    # Set preferences to a value larger than the similarities\n    preference = 2\n\n    # Check that every sample becomes its own cluster center\n    cluster_centers_indices, labels = affinity_propagation(S, preference)\n    assert len(cluster_centers_indices) == 10\n    assert_array_equal(labels, np.arange(10))\n", "def test_affinity_propagation_equal_similarities():\n    # Test affinity propagation with equal similarities and preferences\n    S = np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]])\n    preference = np.array([1, 1, 1])\n\n    assert _equal_similarities_and_preferences(S, preference)\n\n    cluster_centers_indices, labels = affinity_propagation(S, preference)\n    assert_array_equal(cluster_centers_indices, np.arange(3))\n    assert_array_equal(labels, np.arange(3))\n\n    # Test with a single sample\n    S = np.array([[1]])\n    preference = np.array([1])\n    cluster_centers_indices, labels = affinity_propagation(S, preference)\n    assert_array_equal(cluster_centers_indices, np.array([0]))\n    assert_array_equal(labels, np.array([0]))\n", "def test_affinity_propagation_equal_similarities_and_preferences():\n    # Test the case where all similarities and preferences are equal.\n    S = np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]])\n    preference = np.array([1, 1, 1])\n    \n    assert _equal_similarities_and_preferences(S, preference)\n    \n    # Test the case where all similarities are equal but preferences are not.\n    S = np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]])\n    preference = np.array([1, 2, 3])\n    \n    assert not _equal_similarities_and_preferences(S, preference)\n    \n    # Test the case where all preferences are equal but similarities are not.\n    S = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    preference = np.array([1, 1, 1])\n    \n    assert not _equal_similarities_and_preferences(S, preference)\n", "def test_affinity_propagation_equal_similarities_and_preferences():\n    # Test the case where all similarities and preferences are equal.\n    S = np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]])\n    preference = np.array([1, 1, 1])\n    \n    assert _equal_similarities_and_preferences(S, preference) == True\n    \n    # Test the case where not all similarities and preferences are equal.\n    S = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    preference = np.array([1, 2, 3])\n    \n    assert _equal_similarities_and_preferences(S, preference) == False\n", "def test_affinity_propagation_equal_similarities():\n    # Test affinity propagation with equal similarities and preferences\n    S = np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]])\n    preference = np.array([1, 1, 1])\n\n    assert _equal_similarities_and_preferences(S, preference)\n    cluster_centers_indices, labels = affinity_propagation(S, preference)\n\n    # With equal similarities and preferences, all points should be exemplars\n    assert_array_equal(cluster_centers_indices, np.arange(3))\n    assert_array_equal(labels, np.arange(3))\n\n    # Test with a smaller preference value\n    preference = np.array([0.5, 0.5, 0.5])\n    cluster_centers_indices, labels = affinity_propagation(S, preference)\n\n    # All points should be assigned to a single cluster\n    assert len(cluster_centers_indices) == 1\n    assert_array_equal(labels, np.zeros(3))\n"], "sample_867": ["def test_parameter_grid_iter_and_getitem_equivalence():\n    param_grid = {'a': [1, 2], 'b': [True, False]}\n    grid = ParameterGrid(param_grid)\n    assert_grid_iter_equals_getitem(grid)\n\n    param_grid = [{'kernel': ['linear']}, {'kernel': ['rbf'], 'gamma': [1, 10]}]\n    grid = ParameterGrid(param_grid)\n    assert_grid_iter_equals_getitem(grid)\n", "def test_param_input_validation(klass, input, error_type, error_message):\n    with pytest.raises(error_type, match=error_message):\n        klass(input)\n", "def test_validate_parameter_input(klass, input, error_type, error_message):\n    with pytest.raises(error_type, match=error_message):\n        klass(input)\n", "def test_parameter_grid_repr():\n    param_grid = ParameterGrid({'a': [1, 2], 'b': [True, False]})\n    assert repr(param_grid) == \"ParameterGrid({'a': [1, 2], 'b': [True, False]})\"\n", "def test_parameter_grid_list_of_lists():\n    param_grid = [{'a': [1, 2], 'b': [3, 4]}, {'a': [5, 6], 'c': [7, 8]}]\n    grid = ParameterGrid(param_grid)\n\n    assert len(grid) == 4\n    assert sorted(grid.param_grid) == sorted(param_grid)\n    assert_array_equal([p for p in grid],\n                       [{'a': 1, 'b': 3}, {'a': 1, 'b': 4},\n                        {'a': 2, 'b': 3}, {'a': 2, 'b': 4},\n                        {'a': 5, 'c': 7}, {'a': 5, 'c': 8},\n                        {'a': 6, 'c': 7}, {'a': 6, 'c': 8}])\n"], "sample_868": ["def test_perfect_labelings():\n    # Perfect labelings are both homogeneous and complete, hence have\n    # score 1.0 for all supervised metrics\n    for name, metric in SUPERVISED_METRICS.items():\n        assert_allclose(metric(y1, y1), 1.0, atol=1e-10)\n", "def test_supervised_metrics_on_empty_arrays():\n    # Test that supervised metrics raise an error on empty arrays\n    for name, metric in SUPERVISED_METRICS.items():\n        with pytest.raises(ValueError):\n            metric([], [])\n", "def test_metrics_given_empty_arrays():\n    # Given empty arrays, metrics should return the appropriate value.\n    # For some metrics, it is 1.0, and for others, it's 0.0.\n\n    empty_labels_true = np.array([])\n    empty_labels_pred = np.array([])\n\n    assert_allclose(adjusted_mutual_info_score(empty_labels_true, empty_labels_pred), 1.0)\n    assert_allclose(adjusted_rand_score(empty_labels_true, empty_labels_pred), 1.0)\n    assert_allclose(completeness_score(empty_labels_true, empty_labels_pred), 1.0)\n    assert_allclose(homogeneity_score(empty_labels_true, empty_labels_pred), 1.0)\n    assert_allclose(mutual_info_score(empty_labels_true, empty_labels_pred), 0.0)\n    assert_allclose(normalized_mutual_info_score(empty_labels_true, empty_labels_pred), 1.0)\n    assert_allclose(v_measure_score(empty_labels_true, empty_labels_pred), 1.0)\n    assert_allclose(fowlkes_mallows_score(empty_labels_true, empty_labels_pred), 1.0)\n", "def test_perfect_labelings():\n    # Perfect labelings are homogeneous and complete, hence have score 1.0\n    for metric_name in NORMALIZED_METRICS:\n        assert_allclose(SUPERVISED_METRICS[metric_name](y1, y1), 1.0)\n        assert_allclose(SUPERVISED_METRICS[metric_name](y2, y2), 1.0)\n", "def test_entropy():\n    # Test that entropy calculation is correct\n    labels = np.array([0, 0, 0, 1, 1, 1, 2, 2, 2])\n    assert_allclose(entropy(labels), 1.0986122886681098)\n\n    # Test that entropy of a single cluster is zero\n    labels = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0])\n    assert_allclose(entropy(labels), 0.0)\n\n    # Test that entropy of an empty array is 1.0\n    labels = np.array([])\n    assert_allclose(entropy(labels), 1.0)\n"], "sample_870": ["def test_gaussian_process_regression_multi_target(\n    kernel=non_fixed_kernels[0], n_targets=2", "def test_gpr_normalize_y_multi_output():\n    # Test normalization of multi-output y\n    kernel = RBF(length_scale=1.0)\n    gpr = GaussianProcessRegressor(kernel=kernel, normalize_y=True)\n\n    X_train = np.array([[1, 2], [3, 4]])\n    y_train = np.array([[10, 20], [30, 40]])\n\n    gpr.fit(X_train, y_train)\n\n    assert_almost_equal(gpr._y_train_mean, np.mean(y_train, axis=0))\n    assert_almost_equal(gpr._y_train_std, np.std(y_train, axis=0))\n", "def test_gpr_normalize_y_multioutput():\n    # Test normalization of multi-output y in GaussianProcessRegressor\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([[1, 2], [3, 4]])\n    gpr = GaussianProcessRegressor(normalize_y=True)\n    gpr.fit(X, y)\n\n    np.testing.assert_array_equal(gpr._y_train_mean, np.mean(y, axis=0))\n    np.testing.assert_array_equal(gpr._y_train_std, np.std(y, axis=0))\n\n    y_mean, _ = gpr.predict(X, return_std=True)\n    np.testing.assert_allclose(y_mean, y, atol=1e-10)\n", "def test_gpr_log_marginal_likelihood_value():\n    # Test that log_marginal_likelihood_value is computed correctly and that\n    # log_marginal_likelihood returns the same value\n    gpr = GaussianProcessRegressor(kernel=RBF(), alpha=1e-5, n_restarts_optimizer=0)\n    gpr.fit(X, y)\n\n    # Compute log marginal likelihood using the log_marginal_likelihood method\n    theta = gpr.kernel_.theta\n    lml_method = gpr.log_marginal_likelihood(theta)\n\n    # Compute log marginal likelihood manually\n    K = gpr.kernel_(gpr.X_train_)\n    K[np.diag_indices_from(K)] += 1e-5\n    L = np.linalg.cholesky(K)\n    alpha = np.linalg.solve(L.T, np.linalg.solve(L, gpr.y_train_))\n    lml_manual = -0.5 * gpr.y_train_.dot(alpha) - np.log(np.diag(L)).sum() - (\n        gpr.X_train_.shape[0] / 2 * np.log(2 * np.pi)\n    )\n\n    assert_almost_equal(lml_method, lml_manual)\n    assert_almost_equal(gpr.log_marginal_likelihood_value_, lml_method)\n", "def test_gpr_log_marginal_likelihood_value(kernel):\n    gpr = GaussianProcessRegressor(kernel=kernel, optimizer=None)\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        gpr.fit(X, y)\n    theta = gpr.kernel_.theta\n    profile_ll = gpr.log_marginal_likelihood_value_\n    assert_almost_equal(\n        profile_ll, gpr.log_marginal_likelihood(theta), 10, err_msg=\"Profile log-likelihood does not match self.log_marginal_likelihood\"\n    )\n"], "sample_871": ["def test_silhouette_score_random_state():\n    # Test that the silhouette score is the same for a given random state\n    X, y = datasets.make_blobs(n_samples=50, n_features=2, centers=3, random_state=0)\n    score1 = silhouette_score(X, y, sample_size=25, random_state=42)\n    score2 = silhouette_score(X, y, sample_size=25, random_state=42)\n    assert_allclose(score1, score2)\n\n    # Test that the silhouette score is different for different random states\n    score1 = silhouette_score(X, y, sample_size=25, random_state=42)\n    score2 = silhouette_score(X, y, sample_size=25, random_state=13)\n    assert not np.allclose(score1, score2)\n", "def test_silhouette_score_sparse():\n    # Test silhouette score with sparse matrices\n    X, y = datasets.make_blobs(n_samples=50, n_features=3, centers=2,\n                               cluster_std=1.0, random_state=42)\n\n    # Convert to sparse matrix\n    sparse_X = csr_matrix(X)\n\n    score_dense = silhouette_score(X, y)\n    score_sparse = silhouette_score(sparse_X, y)\n\n    assert_allclose(score_dense, score_sparse)\n", "def test_silhouette_score_sparse_input():\n    # Test that sparse input matrices are accepted for silhouette score\n    X, y = datasets.make_blobs(n_samples=20, n_features=10, random_state=0)\n    X_sparse = csr_matrix(X)\n\n    score_dense = silhouette_score(X, y, metric=\"euclidean\")\n    score_sparse = silhouette_score(X_sparse, y, metric=\"euclidean\")\n\n    assert_allclose(score_dense, score_sparse)\n", "def test_silhouette_score_precomputed_sparse():\n    # Test silhouette score with precomputed sparse matrix\n    X = csr_matrix(np.array([[0, 1, 2], [1, 0, 3], [2, 3, 0]]))\n    labels = np.array([0, 1, 1])\n    score = silhouette_score(X, labels, metric=\"precomputed\")\n    assert isinstance(score, float)\n", "def test_silhouette_score_sparse_input():\n    # Test that sparse input to silhouette_score works as expected\n    X, y = datasets.make_moons(n_samples=100, noise=0.05)\n    X_sparse = csr_matrix(X)\n\n    score_dense = silhouette_score(X, y)\n    score_sparse = silhouette_score(X_sparse, y)\n\n    assert_allclose(score_dense, score_sparse)\n"], "sample_872": ["def test_top_k_accuracy_score():\n    # Test top-k accuracy for multi-class classification\n    y_true = np.array([0, 1, 2, 2])\n    y_score = np.array([[0.5, 0.2, 0.2], [0.3, 0.4, 0.2], [0.2, 0.4, 0.3], [0.7, 0.2, 0.1]])\n    assert_almost_equal(top_k_accuracy_score(y_true, y_score, k=2), 0.75)\n    assert_almost_equal(top_k_accuracy_score(y_true, y_score, k=2, normalize=False), 3)\n\n    # Test top-k accuracy for binary classification\n    y_true = np.array([0, 1, 1, 0])\n    y_score = np.array([0.5, 0.6, 0.7, 0.4])\n    assert_almost_equal(top_k_accuracy_score(y_true, y_score, k=1), 0.75)\n    assert_almost_equal(top_k_accuracy_score(y_true, y_score, k=1, normalize=False), 3)\n\n    # Test top-k accuracy with ties in predictions\n    y_true = np.array([0, 1, 2, 2])\n    y_score = np.array([[1, 0, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0]])\n    assert_almost_equal(top_k_accuracy_score(y_true, y_score, k=1), 0.5)\n    assert_almost_equal(top_k_accuracy_score(y_true, y_score, k=1, normalize=False), 2)\n", "def test_roc_curve_fpr_tpr_thresholds():\n    # Test whether the outputs of `roc_curve` have the same length and are valid\n    y_true, _, y_score = make_prediction(binary=True)\n    fpr, tpr, thresholds = roc_curve(y_true, y_score)\n\n    assert len(fpr) == len(tpr) == len(thresholds)\n    assert np.all(fpr >= 0) and np.all(fpr <= 1)\n    assert np.all(tpr >= 0) and np.all(tpr <= 1)\n    assert np.all(thresholds >= 0) and np.all(thresholds <= 1)\n", "def test_average_precision_score_at_k():\n    # Test average precision with k parameter\n    y_true = np.array([1, 0, 1, 0, 1])\n    y_score = np.array([0.9, 0.8, 0.7, 0.6, 0.5])\n\n    # k is not supported in average_precision_score, it should raise an error\n    with pytest.raises(TypeError):\n        average_precision_score(y_true, y_score, k=2)\n", "def test_roc_auc_score_binary():\n    # Test area under ROC curve with binary data\n    y_true, _, y_score = make_prediction(binary=True)\n    auc_score = roc_auc_score(y_true, y_score)\n    assert_array_almost_equal(auc_score, _auc(y_true, y_score), 10)\n\n    assert roc_auc_score(y_true, y_score) == roc_auc_score(1 - y_true, 1 - y_score)\n    assert_almost_equal(roc_auc_score([0, 1], [0.5, 0.5]), 0.5)\n", "def test_roc_auc_score_average_parameter():\n    # Test the average parameter\n    y_true = np.array([0, 1, 1, 0, 1])\n    y_score = np.array([0.05, 0.95, 0.95, 0.05, 0.95])\n\n    # macro averaging\n    score_macro = roc_auc_score(y_true, y_score, average='macro')\n    assert_almost_equal(score_macro, 1.0)\n\n    # weighted averaging\n    score_weighted = roc_auc_score(y_true, y_score, average='weighted')\n    assert_almost_equal(score_weighted, 1.0)\n\n    # samples averaging\n    score_samples = roc_auc_score(y_true, y_score, average='samples')\n    assert_almost_equal(score_samples, 1.0)\n\n    # micro averaging\n    score_micro = roc_auc_score(y_true, y_score, average='micro')\n    assert_almost_equal(score_micro, 1.0)\n"], "sample_873": ["def test_selector_get_support():\n    # Test get_support with mask and indices\n    selector = StepSelector(step=2)\n    selector.fit(X, y)\n\n    support_mask = selector.get_support(indices=False)\n    assert_array_equal(support_mask, support)\n\n    support_indices = selector.get_support(indices=True)\n    assert_array_equal(support_indices, support_inds)\n", "def test_transform_inverse_transform_equal():\n    # Check that transform and inverse_transform return the original array\n    selector = StepSelector(step=2)\n    selector.fit(X, y)\n    X_selected = selector.transform(X)\n    X_original = selector.inverse_transform(X_selected)\n    assert_array_equal(X_original, Xinv)\n", "def test_transform_inverse_transform():\n    # Test that inverse_transform is an involution\n    sel = StepSelector(step=2)\n    sel.fit(X, y)\n    Xt = sel.transform(X)\n    Xinv = sel.inverse_transform(Xt)\n    assert_array_equal(Xinv, X)\n\n    # Check for sparse matrices\n    X_sp = sp.csr_matrix(X)\n    Xt_sp = sel.transform(X_sp)\n    Xinv_sp = sel.inverse_transform(Xt_sp)\n    assert_array_equal(Xinv_sp.toarray(), X)\n", "def test_selectorMixin_get_support():\n    # Test _get_support_mask with different steps\n    selector = StepSelector(step=2)\n    selector.fit(X, y)\n    assert_array_equal(selector._get_support_mask(), support)\n\n    selector = StepSelector(step=3)\n    selector.fit(X, y)\n    assert_array_equal(selector._get_support_mask(), [True, False, False] * 3 + [True])\n\n    selector = StepSelector(step=1)\n    selector.fit(X, y)\n    assert_array_equal(selector._get_support_mask(), np.ones(10, dtype=bool))\n\n    selector = StepSelector(step=0)\n    selector.fit(X, y)\n    assert_array_equal(selector._get_support_mask(), np.zeros(10, dtype=bool))\n", "def test_transform_pandas_dataframe():\n    import pandas as pd\n\n    # create a pandas dataframe\n    df = pd.DataFrame(X, columns=feature_names)\n\n    # create a selector and fit it\n    selector = StepSelector(step=2)\n    selector.fit(df)\n\n    # transform the dataframe\n    df_t = selector.transform(df)\n\n    # check that the result is a pandas dataframe\n    assert isinstance(df_t, pd.DataFrame)\n\n    # check that the columns are correct\n    assert_array_equal(df_t.columns, feature_names_t)\n"], "sample_874": ["def test_clone():\n    # Test that clone creates a new estimator\n    selector = StepSelector(step=2)\n    selector_fitted = selector.fit(X, y)\n\n    cloned_selector = clone(selector)\n    assert_array_equal(cloned_selector.get_params(), selector.get_params())\n\n    cloned_selector_fitted = clone(selector_fitted)\n    assert_array_equal(cloned_selector_fitted.get_params(), selector_fitted.get_params())\n    assert not hasattr(cloned_selector_fitted, \"n_features_in_\")\n", "def test_selector_get_support_names():\n    selector = StepSelector(step=2)\n    selector.fit(X, y)\n\n    assert_array_equal(selector.get_support(names=feature_names), feature_names_t)\n    assert_array_equal(selector.get_support(names=np.array(feature_names)), feature_names_t)\n\n    with pytest.raises(ValueError):\n        selector.get_support(names=[1, 2, 3])\n", "def test_validate_params():\n    # Define a simple estimator with a parameter\n    class SimpleEstimator(BaseEstimator):\n        _parameter_constraints = {\"param\": [\"array-like\"]}\n\n            self.param = param\n\n    # Create an instance of the estimator\n    estimator = SimpleEstimator(param=[1, 2, 3])\n\n    # Call _validate_params and check that no error is raised\n    estimator._validate_params()\n\n    # Create another instance with an invalid parameter\n    estimator = SimpleEstimator(param=\"invalid\")\n\n    # Check that a ValueError is raised when calling _validate_params\n    with pytest.raises(ValueError):\n        estimator._validate_params()\n", "def test_clone():\n    # Tests that clone creates a deep copy.\n    selector = StepSelector(step=3)\n    selector.fit(X, y)\n\n    cloned_selector = clone(selector)\n    assert_array_equal(selector.support_, cloned_selector.support_)\n    assert_array_equal(selector.feature_names_in_, cloned_selector.feature_names_in_)\n\n    # Modify original selector to ensure no references are shared\n    selector.support_[0] = False\n    selector.feature_names_in_[0] = \"giraffe\"\n    assert not np.array_equal(selector.support_, cloned_selector.support_)\n    assert not np.array_equal(selector.feature_names_in_, cloned_selector.feature_names_in_)\n", "        def fit(self, X, y=None):\n            self.n_features_in_ = X.shape[1]\n            return self\n"], "sample_876": ["def test_mlp_multilabel():\n    # Test the multilabel classification case.\n    X, y = make_multilabel_classification(n_samples=50, n_features=20, random_state=0)\n    mlp = MLPClassifier(hidden_layer_sizes=(15,))\n    mlp.fit(X, y)\n    assert_array_equal(mlp.classes_, np.arange(y.shape[1]))\n    y_pred = mlp.predict(X)\n    assert y_pred.shape == y.shape\n\n    # Test _predict_proba for multi-label classification\n    proba = mlp._predict_proba(X)\n    assert proba.shape == (X.shape[0], y.shape[1])\n\n    # Test predict_proba for multi-label classification\n    proba = mlp.predict_proba(X)\n    assert proba.shape == (X.shape[0], 2 * y.shape[1])\n", "def test_mlp_multilabel():\n    X, y = make_multilabel_classification(\n        n_samples=50, n_features=20, n_labels=3, random_state=0\n    )\n    clf = MLPClassifier(random_state=0)\n    clf.fit(X, y)\n    assert_array_equal(clf.classes_, np.arange(y.shape[1]))\n    y_pred = clf.predict(X)\n    assert_array_equal(y_pred.shape, y.shape)\n", "def test_mlp_classifier_partial_fit():\n    # Test partial_fit on different data.\n    X, y = make_multilabel_classification(\n        n_samples=50, n_features=20, n_classes=3, n_labels=2, random_state=0\n    )\n    X1, X2, y1, y2 = train_test_split(X, y, test_size=0.5, random_state=42)\n    clf = MLPClassifier(max_iter=10, random_state=0)\n\n    clf.partial_fit(X1, y1, classes=np.unique(y))\n    assert clf.score(X2, y2) > 0.4\n\n    clf.partial_fit(X2, y2)\n    assert clf.score(X1, y1) > 0.4\n\n    # check that predict works after only one call to partial_fit\n    X, y = make_classification(n_samples=100, n_features=10, n_informative=5)\n    clf = MLPClassifier(random_state=0)\n    clf.partial_fit(X[:50], y[:50], classes=np.unique(y))\n    assert clf.predict(X[50:]).shape == (50,)\n", "def test_mlpclassifier_multilabel():\n    # Test the multi-label classification case\n    X, y = make_multilabel_classification(\n        n_samples=100, n_features=10, n_classes=3, n_labels=2, random_state=0\n    )\n    clf = MLPClassifier(random_state=0, max_iter=1000)\n    with ignore_warnings(category=ConvergenceWarning):\n        clf.fit(X, y)\n    assert clf.score(X, y) > 0.5\n    assert clf.predict(X).shape == y.shape\n    assert clf.predict_proba(X).shape == (y.shape[0], y.shape[1])\n", "def test_mlp_classifier_multilabel():\n    # Test MLPClassifier on multi-label classification problems.\n    X, y = make_multilabel_classification(\n        n_samples=100, n_features=10, n_classes=3, n_labels=2, random_state=0\n    )\n    clf = MLPClassifier(hidden_layer_sizes=(10,), max_iter=1000)\n    clf.fit(X, y)\n\n    assert clf.score(X, y) > 0.7\n    assert clf.predict(X).shape == (X.shape[0], y.shape[1])\n    assert clf.predict_proba(X).shape == (X.shape[0], y.shape[1])\n\n    # Check warning when y has a single class\n    with pytest.warns(UserWarning):\n        clf.fit(X, np.zeros_like(y))\n\n    # Check the shape of the output when predict_proba is called\n    # with a single sample\n    proba = clf.predict_proba(X[0].reshape(1, -1))\n    assert proba.shape == (1, y.shape[1])\n"], "sample_877": ["def test_isotonic_regression_ties():\n    # Test isotonic regression with ties\n    y = np.array([1, 2, 3, 3, 4, 5])\n    sample_weight = np.array([1, 1, 1, 1, 1, 1])\n    result = isotonic_regression(y, sample_weight=sample_weight)\n    assert_array_almost_equal(result, [1., 2., 3., 3., 4., 5.])\n\n    y = np.array([1, 2, 3, 3, 3, 4, 5])\n    sample_weight = np.array([1, 1, 1, 1, 1, 1, 1])\n    result = isotonic_regression(y, sample_weight=sample_weight)\n    assert_array_almost_equal(result, [1., 2., 3., 3., 3., 4., 5.])\n", "def test_isotonic_regression_model_complexity():\n    # Check that the model complexity (number of unique values) is reduced\n    # when sample weights are used.\n    X, y = make_regression(n_samples=100, n_features=1, random_state=42)\n    X = X.flatten()\n    sample_weight = np.ones_like(y)\n\n    # Introduce duplicate values with different weights\n    X[10] = X[11]\n    y[10] = 0.5\n    y[11] = 1.0\n    sample_weight[10] = 2.0\n\n    iso_reg = IsotonicRegression(increasing=True, out_of_bounds=\"clip\")\n    iso_reg.fit(X, y, sample_weight=sample_weight)\n\n    assert len(iso_reg.X_thresholds_) < len(np.unique(X))\n", "def test_isotonic_regression_ties():\n    # Test isotonic regression with ties in y\n    X = np.array([1, 2, 3, 4, 5])\n    y = np.array([1, 1, 2, 2, 2])\n\n    # Check that ties are preserved\n    y_ = isotonic_regression(y)\n    assert_array_equal(y_, y)\n\n    # Check that ties are broken correctly when increasing=False\n    y_ = isotonic_regression(y, increasing=False)\n    assert_array_equal(y_, y[::-1])\n", "def test_isotonic_regression_X_1d():\n    # Test that X can be 1d when calling fit.\n    X, y = make_regression(n_samples=10, n_features=1, random_state=42)\n    X = X.squeeze()  # Make X 1d\n\n    iso_reg = IsotonicRegression()\n    iso_reg.fit(X, y)\n\n    assert hasattr(iso_reg, \"X_thresholds_\")\n    assert hasattr(iso_reg, \"y_thresholds_\")\n", "def test_isotonic_regression_bounds():\n    # Test isotonic regression with bounds\n    y = np.array([3, 7, 5, 9])\n    y_min = 4\n    y_max = 8\n\n    # Check that the isotonic regression respects the bounds\n    y_bound = isotonic_regression(y, y_min=y_min, y_max=y_max)\n    assert_array_equal(y_bound, [4, 6, 6, 8])\n\n    # Check that isotonic_regression raises an error when bounds are invalid\n    with pytest.raises(ValueError):\n        isotonic_regression(y, y_min=10, y_max=0)\n\n    # Check that the bounds have no effect when they are outside the data range\n    y_no_bounds = isotonic_regression(y, y_min=-10, y_max=20)\n    y_no_bounds_2 = isotonic_regression(y)\n    assert_array_almost_equal(y_no_bounds, y_no_bounds_2)\n"], "sample_878": ["def test_column_transformer_make_column_selector():\n    # Test make_column_selector with pattern\n    selector = make_column_selector(pattern=\"^A\")\n    df = pd.DataFrame({\"A1\": [1, 2], \"A2\": [3, 4], \"B1\": [5, 6]})\n    assert_array_equal(selector(df), [\"A1\", \"A2\"])\n\n    # Test make_column_selector with dtype_include\n    selector = make_column_selector(dtype_include=\"int64\")\n    df = pd.DataFrame({\"A\": [1, 2], \"B\": [3.0, 4.0], \"C\": [\"a\", \"b\"]})\n    assert_array_equal(selector(df), [\"A\"])\n\n    # Test make_column_selector with dtype_exclude\n    selector = make_column_selector(dtype_exclude=\"int64\")\n    df = pd.DataFrame({\"A\": [1, 2], \"B\": [3.0, 4.0], \"C\": [\"a\", \"b\"]})\n    assert_array_equal(selector(df), [\"B\", \"C\"])\n\n    # Test make_column_selector with both pattern and dtype_include\n    selector = make_column_selector(pattern=\"^A\", dtype_include=\"int64\")\n    df = pd.DataFrame({\"A1\": [1, 2], \"A2\": [3.0, 4.0], \"B1\": [5, 6]})\n    assert_array_equal(selector(df), [\"A1\"])\n", "def test_column_transformer_get_feature_names_out():\n    # Test that the ColumnTransformer returns the correct feature names\n\n    X = np.array([[1, 2, 3], [4, 5, 6]])\n\n    # Define transformers with get_feature_names_out method\n    trans1 = StandardScaler()\n    trans2 = OneHotEncoder()\n\n    # Define ColumnTransformer with transformers and column names\n    ct = ColumnTransformer(\n        [\n            (\"trans1\", trans1, [0]),\n            (\"trans2\", trans2, [1]),\n        ],\n        remainder=\"passthrough\",\n    )\n\n    # Fit the ColumnTransformer\n    ct.fit(X)\n\n    # Get the feature names out\n    feature_names_out = ct.get_feature_names_out()\n\n    # Check that the feature names are as expected\n    assert_array_equal(feature_names_out, [\"trans1__x0\", \"trans2__x1_2.0\", \"trans2__x1_5.0\", \"x2\"])\n", "def test_column_transformer_get_feature_names_out():\n    X = np.array([[1, 2, 3], [4, 5, 6]])\n\n    # Single transformer with get_feature_names_out\n    ct = ColumnTransformer([(\"trans\", StandardScaler(), [0, 1])])\n    ct.fit(X)\n    assert_array_equal(ct.get_feature_names_out(), [\"trans__x0\", \"trans__x1\"])\n\n    # Multiple transformers with get_feature_names_out\n    ct = ColumnTransformer(\n        [(\"trans1\", StandardScaler(), [0]), (\"trans2\", OneHotEncoder(), [1])]\n    )\n    ct.fit(X)\n    assert_array_equal(ct.get_feature_names_out(), [\"trans1__x0\", \"trans2__x1_2.0\"])\n\n    # Transformer without get_feature_names_out\n    ct = ColumnTransformer([(\"trans\", TransNo2D(), [0, 1])])\n    ct.fit(X)\n    with pytest.raises(AttributeError):\n        ct.get_feature_names_out()\n\n    # 'drop' transformer\n    ct = ColumnTransformer([(\"drop\", \"drop\", [0])])\n    ct.fit(X)\n    assert_array_equal(ct.get_feature_names_out(), [])\n\n    # 'passthrough' transformer\n    ct = ColumnTransformer([(\"passthrough\", \"passthrough\", [0])])\n    ct.fit(X)\n    assert_array_equal(ct.get_feature_names_out(), [\"x0\"])\n", "def test_column_transformer_feature_names_out_pandas():\n    # Test that the feature names out are correctly generated when using pandas\n    X_array = np.array([[1, 2], [3, 4]])\n    X_df = pd.DataFrame(X_array, columns=[\"a\", \"b\"])\n\n    # Use DataFrame with ColumnTransformer\n    ct = ColumnTransformer([(\"trans1\", Trans(), [\"a\"]), (\"trans2\", Trans(), [\"b\"])])\n    ct.fit(X_df)\n    feature_names_out = ct.get_feature_names_out()\n    assert_array_equal(feature_names_out, [\"trans1__a\", \"trans2__b\"])\n\n    # Use array with ColumnTransformer\n    ct = ColumnTransformer([(\"trans1\", Trans(), [0]), (\"trans2\", Trans(), [1])])\n    ct.fit(X_array)\n    feature_names_out = ct.get_feature_names_out()\n    assert_array_equal(feature_names_out, [\"trans1__x0\", \"trans2__x1\"])\n", "def test_column_transformer_get_feature_names_out():\n    X = pd.DataFrame({\"A\": [1, 2], \"B\": [3, 4]})\n    ct = ColumnTransformer([(\"trans\", Trans(), [\"A\"])])\n    ct.fit(X)\n\n    feature_names = ct.get_feature_names_out()\n    assert_array_equal(feature_names, [\"trans__A\"])\n\n    # with verbose_feature_names_out=False\n    ct.verbose_feature_names_out = False\n    feature_names = ct.get_feature_names_out()\n    assert_array_equal(feature_names, [\"A\"])\n"], "sample_879": ["def test_one_hot_encoder_handle_unknown_if_binary():\n    # Test the encoding when handle_unknown is 'infrequent_if_exist'\n    # and drop is 'if_binary'.\n    X = np.array([[\"a\"], [\"b\"] * 9]).T\n\n    ohe = OneHotEncoder(handle_unknown=\"infrequent_if_exist\", drop=\"if_binary\")\n    with pytest.raises(ValueError, match=\"No valid categories\"):\n        ohe.fit(X)\n", "def test_one_hot_encoder_inverse_transform_handle_unknown():\n    # Test the inverse transform of OneHotEncoder with handle_unknown\n    X = np.array([[\"Male\", 1], [\"Female\", 3], [\"Female\", 2]])\n    ohe = OneHotEncoder(handle_unknown=\"ignore\")\n    ohe.fit(X)\n\n    # Create a matrix with unknown categories\n    X_test = sparse.csr_matrix([[0, 0, 0, 0, 1]])\n\n    # Transform and inverse transform\n    X_trans = ohe.inverse_transform(X_test)\n    assert_array_equal(X_trans, np.array([[\"Male\", None]]))\n", "def test_one_hot_encoder_get_feature_names_out():\n    enc = OneHotEncoder()\n    X = np.array([[1, 'a'], [2, 'b']])\n    enc.fit(X)\n    feature_names = enc.get_feature_names_out(['feature1', 'feature2'])\n    assert_array_equal(feature_names, ['feature1_1', 'feature1_2', 'feature2_a', 'feature2_b'])\n", "def test_one_hot_encoder_handle_unknown_if_binary():\n    # Test that the 'if_binary' parameter of OneHotEncoder works as expected.\n    # In particular, when an unknown category is encountered in a binary\n    # feature, it should be treated as if handle_unknown='ignore'.\n\n    X = np.array([[\"a\", 1], [\"b\", 2]])\n    X_test = np.array([[\"c\", 3]])\n\n    ohe = OneHotEncoder(handle_unknown=\"if_binary\")\n    with pytest.raises(ValueError):\n        ohe.fit(X)\n\n    ohe = OneHotEncoder(handle_unknown=\"if_binary\", drop=\"if_binary\")\n    ohe.fit(X)\n    Xt = ohe.transform(X_test)\n    assert sparse.issparse(Xt)\n    assert_array_equal(Xt.toarray(), [[0, 0]])\n\n    ohe = OneHotEncoder(handle_unknown=\"if_binary\", drop=\"first\")\n    ohe.fit(X)\n    Xt = ohe.transform(X_test)\n    assert sparse.issparse(Xt)\n    assert_array_equal(Xt.toarray(), [[0, 0]])\n\n    ohe = OneHotEncoder(handle_unknown=\"error\", drop=\"if_binary\")\n    ohe.fit(X)\n    with pytest.raises(ValueError):\n        ohe.transform(X_test)\n\n    ohe = OneHotEncoder(handle_unknown=\"ignore\", drop=\"if_binary\")\n    ohe.fit(X)\n    Xt = ohe.transform(X_test)\n    assert sparse.issparse(Xt)\n    assert_array_equal(Xt.toarray(), [[0, 0]])\n", "def test_one_hot_encoder_handle_unknown_frame():\n    # Test OneHotEncoder with handle_unknown='error' and 'ignore'\n    X = np.array([[1, 2], [3, 4]])\n    enc = OneHotEncoder(handle_unknown=\"error\")\n    with pytest.raises(ValueError):\n        enc.fit(X).transform(np.array([[5, 6]]))\n\n    enc = OneHotEncoder(handle_unknown=\"ignore\")\n    transformed_X = enc.fit(X).transform(np.array([[5, 6]]))\n    assert_array_equal(transformed_X.toarray(), np.array([[0, 0, 0, 0]]))\n\n    enc = OneHotEncoder(handle_unknown=\"infrequent_if_exist\")\n    transformed_X = enc.fit(X).transform(np.array([[5, 6]]))\n    assert_array_equal(transformed_X.toarray(), np.array([[0, 0, 0, 0]]))\n"], "sample_880": ["def test_class_distribution():\n    # Test with dense array\n    y = np.array([[0, 1], [1, 0]])\n    classes, n_classes, class_prior = class_distribution(y)\n    assert_array_equal(classes, [np.array([0, 1]), np.array([0, 1])])\n    assert_array_equal(n_classes, [2, 2])\n    assert_allclose(class_prior, [[0.5, 0.5], [0.5, 0.5]])\n\n    # Test with sparse array\n    y_sparse = csr_matrix(y)\n    classes, n_classes, class_prior = class_distribution(y_sparse)\n    assert_array_equal(classes, [np.array([0, 1]), np.array([0, 1])])\n    assert_array_equal(n_classes, [2, 2])\n    assert_allclose(class_prior, [[0.5, 0.5], [0.5, 0.5]])\n\n    # Test with sample weights\n    sample_weight = np.array([0.5, 1.5])\n    classes, n_classes, class_prior = class_distribution(y, sample_weight)\n    assert_array_equal(classes, [np.array([0, 1]), np.array([0, 1])])\n    assert_array_equal(n_classes, [2, 2])\n    assert_allclose(class_prior, [[0.25, 0.75], [0.75, 0.25]])\n\n    # Test with sparse array and sample weights\n    classes, n_classes, class_prior = class_distribution(y_sparse, sample_weight)\n    assert_array_equal(classes, [np.array([0, 1]), np.array([0, 1])])\n    assert_array_equal(n_classes, [2, 2])\n    assert_allclose(class_prior, [[0.25, 0.75], [0.75, 0.25]])\n", "def test_type_of_target_sequence_of_sequences_deprecated():\n    # Test that sequences of sequences (deprecated) are correctly identified\n    # as multilabel-sequences, and raise a deprecation warning\n\n    # Construct some example inputs\n    sequence_of_sequences_inputs = [\n        [[1], [2], [0, 1]],\n        [(), (2), (0, 1)],\n        np.array([[], [1, 2]], dtype=\"object\"),\n    ]\n\n    for input_array in sequence_of_sequences_inputs:\n        with pytest.warns(DeprecationWarning):\n            result = type_of_target(input_array)\n        assert result == \"multilabel-sequences\"\n", "def test_class_distribution():\n    # Test class_distribution function with dense and sparse inputs\n    y = np.array([[0, 1], [1, 0]])\n    classes, n_classes, class_prior = class_distribution(y)\n    assert_array_equal(classes, [[0, 1], [0, 1]])\n    assert_array_equal(n_classes, [2, 2])\n    assert_allclose(class_prior, [[0.5, 0.5], [0.5, 0.5]])\n\n    y_sparse = csc_matrix(y)\n    classes, n_classes, class_prior = class_distribution(y_sparse)\n    assert_array_equal(classes, [[0, 1], [0, 1]])\n    assert_array_equal(n_classes, [2, 2])\n    assert_allclose(class_prior, [[0.5, 0.5], [0.5, 0.5]])\n\n    # Test with weights\n    sample_weight = np.array([0.5, 1.5])\n    classes, n_classes, class_prior = class_distribution(y, sample_weight)\n    assert_array_equal(classes, [[0, 1], [0, 1]])\n    assert_array_equal(n_classes, [2, 2])\n    assert_allclose(class_prior, [[0.25, 0.75], [0.75, 0.25]])\n\n    # Test with zero weight\n    sample_weight = np.array([0, 1])\n    classes, n_classes, class_prior = class_distribution(y, sample_weight)\n    assert_array_equal(classes, [[0, 1], [0, 1]])\n    assert_array_equal(n_classes, [2, 2])\n    assert_allclose(class_prior, [[0, 1], [1, 0]])\n", "def test_class_distribution_multilabel_indicator_sparse():\n    # Test class_distribution with multilabel-indicator sparse matrix\n    y = sp.csr_matrix(np.array([[1, 0, 1], [0, 1, 0]]))\n    classes, n_classes, class_prior = class_distribution(y)\n\n    assert len(classes) == 3\n    assert np.all(n_classes == [2, 2, 2])\n    assert np.allclose(class_prior, [[0.5, 0.5], [0.5, 0.5], [0.5, 0.5]])\n\n    # Test class_distribution with multilabel-indicator sparse matrix and sample_weight\n    sample_weight = np.array([0.5, 1.0])\n    classes, n_classes, class_prior = class_distribution(y, sample_weight)\n\n    assert len(classes) == 3\n    assert np.all(n_classes == [2, 2, 2])\n    assert np.allclose(class_prior, [[0.33333333, 0.66666667], [0.33333333, 0.66666667], [0.33333333, 0.66666667]])\n", "def test_class_distribution():\n    # Test class_distribution function\n    y = np.array([[0, 1, 2], [1, 2, 0]])\n    classes, n_classes, class_prior = class_distribution(y)\n    \n    assert_array_equal(classes, [np.array([0, 1, 2]), np.array([0, 1, 2]), np.array([0, 1, 2])])\n    assert_array_equal(n_classes, [3, 3, 3])\n    assert_array_almost_equal(class_prior, [[1/3, 1/3, 1/3], [1/3, 1/3, 1/3], [1/3, 1/3, 1/3]])\n\n    y_sparse = csr_matrix(y)\n    classes_sparse, n_classes_sparse, class_prior_sparse = class_distribution(y_sparse)\n    \n    assert_array_equal(classes_sparse, classes)\n    assert_array_equal(n_classes_sparse, n_classes)\n    assert_array_almost_equal(class_prior_sparse, class_prior)\n\n    y_with_zero_weight = np.array([[0, 1, 2], [1, 2, 0]])\n    sample_weight = np.array([0, 1])\n    classes_with_zero_weight, _, class_prior_with_zero_weight = class_distribution(y_with_zero_weight, sample_weight)\n    \n    assert_array_equal(classes_with_zero_weight, classes)\n    assert_array_almost_equal(class_prior_with_zero_weight, [[0, 1/3, 2/3], [1/3, 1/3, 1/3], [2/3, 1/3, 0]])\n\n    y_with_zero_weight_sparse = csr_matrix(y_with_zero_weight)\n    classes_with_zero_weight_sparse, _, class_prior_with_zero_weight_sparse = class_distribution(y_with_zero_weight_sparse, sample_weight)\n    \n    assert_array_equal(classes_with_zero_weight_sparse, classes)\n    assert_array_almost_equal(class_prior_with_zero_weight_sparse, class_prior_with_zero_weight)\n"], "sample_881": ["def test_top_k_accuracy_score_tied_results():\n    # Test that top_k_accuracy_score handles tied results correctly\n    y_true = np.array([0, 1, 2])\n    y_score = np.array([[0.5, 0.5, 0], [0, 0.5, 0.5], [0.5, 0, 0.5]])\n    assert_almost_equal(top_k_accuracy_score(y_true, y_score, k=2), 1.0)\n    assert_array_equal(top_k_accuracy_score(y_true, y_score, k=2, normalize=False), [1, 1, 1])\n", "def test_top_k_accuracy_score_ties():\n    # Test top_k_accuracy_score with ties in the predictions\n    y_true = np.array([0, 1, 2])\n    y_score = np.array([[0.5, 0.5, 0], [0, 0.5, 0.5], [0.5, 0, 0.5]])\n    assert_almost_equal(top_k_accuracy_score(y_true, y_score, k=2), 1)\n", "def test_top_k_accuracy_score_multiclass():\n    # Test top-k accuracy score for multiclass classification\n\n    y_true = np.array([0, 1, 2, 2])\n    y_score = np.array([[0.5, 0.2, 0.2], [0.3, 0.4, 0.2], [0.2, 0.4, 0.3], [0.7, 0.2, 0.1]])\n    k = 2\n\n    score = top_k_accuracy_score(y_true, y_score, k=k)\n    assert_almost_equal(score, 0.75)\n\n    # Test with different values of k\n    for k in range(1, 4):\n        score = top_k_accuracy_score(y_true, y_score, k=k)\n        assert isinstance(score, float)\n        assert 0 <= score <= 1\n\n    # Test with invalid input\n    with pytest.raises(ValueError):\n        top_k_accuracy_score(y_true, y_score, k=0)\n\n    with pytest.raises(ValueError):\n        top_k_accuracy_score(y_true, y_score, k=-1)\n\n    with pytest.raises(ValueError):\n        top_k_accuracy_score(y_true, y_score, labels=[0, 1])\n\n    with pytest.raises(ValueError):\n        top_k_accuracy_score(y_true, y_score[:, :2], labels=[0, 1, 2])\n", "def test_roc_auc_score_average_parameter():\n    # Test that roc_auc_score function handles the average parameter correctly\n    y_true = np.array([0, 1, 1, 0, 1])\n    y_pred = np.array([0.4, 0.8, 0.3, 0.6, 0.7])\n\n    # Test 'macro' average\n    score_macro = roc_auc_score(y_true, y_pred, average='macro')\n    assert_almost_equal(score_macro, _auc(y_true, y_pred))\n\n    # Test 'weighted' average\n    score_weighted = roc_auc_score(y_true, y_pred, average='weighted')\n    assert_almost_equal(score_weighted, _auc(y_true, y_pred))\n\n    # Test 'samples' average\n    score_samples = roc_auc_score(y_true, y_pred, average='samples')\n    assert_almost_equal(score_samples, _auc(y_true, y_pred))\n\n    # Test None average\n    score_none = roc_auc_score(y_true, y_pred, average=None)\n    assert_array_equal(score_none.shape, (1,))\n", "def test_roc_auc_score_binary_class():\n    # Test Area under Receiver Operating Characteristic Curve (ROC AUC)\n    # with binary classification\n    y_true, _, y_score = make_prediction(binary=True)\n\n    # Binary classification\n    auc_score = roc_auc_score(y_true, y_score)\n    assert auc_score > 0.5\n\n"], "sample_882": ["def test_mlp_regressor_early_stopping():\n    X, y = make_regression(\n        n_samples=200, n_features=10, bias=20.0, noise=100.0, random_state=7\n    )\n    mlp = MLPRegressor(\n        hidden_layer_sizes=(10,), max_iter=5, early_stopping=True, random_state=7\n    )\n    mlp.fit(X, y)\n    assert hasattr(mlp, \"validation_scores_\")\n    assert len(mlp.validation_scores_) == mlp.n_iter_\n    assert hasattr(mlp, \"best_validation_score_\")\n    assert mlp.best_validation_score_ is not None\n    assert hasattr(mlp, \"best_loss_\")\n    assert mlp.best_loss_ is None\n", "def test_mlp_classifier_partial_fit_classes():\n    # Test if classes are correctly set in partial_fit\n    X, y = make_multilabel_classification(n_samples=100, n_features=20,\n                                         n_classes=3, n_labels=2, random_state=0)\n    clf = MLPClassifier(random_state=0)\n\n    # The following should raise a ValueError since the classes were not set\n    msg = \"classes must be passed on the first call to partial_fit\"\n    with pytest.raises(ValueError, match=msg):\n        clf.partial_fit(X, y)\n\n    # Now set the classes explicitly and it should work\n    clf.partial_fit(X, y, classes=np.unique(y))\n", "def test_mlp_classifier_partial_fit(hidden_layer_sizes, activation, solver):\n    X, y = X_digits_binary, y_digits_binary\n\n    clf = MLPClassifier(\n        hidden_layer_sizes=hidden_layer_sizes,\n        activation=activation,\n        solver=solver,\n        max_iter=1,\n    )\n    with ignore_warnings(category=ConvergenceWarning):\n        for _ in range(10):\n            clf.partial_fit(X, y)\n\n    assert clf.score(X, y) > 0.7\n", "def test_mlp_sparse_input():\n    X, y = make_multilabel_classification(\n        n_samples=50, n_features=20, n_classes=3, random_state=0\n    )\n    X_sparse = csr_matrix(X)\n\n    mlp = MLPClassifier(random_state=42)\n    with ignore_warnings(category=ConvergenceWarning):\n        mlp.fit(X_sparse, y)\n    assert_array_equal(mlp.classes_, np.arange(3))\n", "def test_mlp_regressor_early_stopping():\n    # Test that early stopping is working by setting a low number of maximum no\n    # improvement epochs and checking that the actual number of epochs is less\n    mlp = MLPRegressor(\n        max_iter=100,\n        n_iter_no_change=5,\n        early_stopping=True,\n        validation_fraction=0.2,\n    )\n    mlp.fit(*make_regression())\n    assert mlp.n_iter_ < mlp.max_iter\n"], "sample_883": ["def test_bayesianridge_log_marginal_likelihood():\n    # Test log marginal likelihood for BayesianRidge\n    X, y = diabetes.data, diabetes.target\n    n_samples, n_features = X.shape\n\n    # compute the log marginal likelihood using _log_marginal_likelihood\n    # function from the BayesianRidge class\n    br = BayesianRidge(compute_score=True)\n    br.fit(X, y)\n\n        score = 0.5 * (\n            n_features * log(lambda_)\n            + n_samples * log(alpha_)\n            - alpha_ * rmse\n            - lambda_ * np.sum(coef**2)\n            + fast_logdet(np.linalg.inv(np.diag(eigen_vals) + lambda_/alpha_ * np.eye(n_features)))\n            - n_samples * log(2 * np.pi)\n        )\n        return score\n\n    eigen_vals = np.linalg.eigvalsh(np.dot(X.T, X))\n    score_manual = _log_marginal_likelihood(n_samples, n_features, eigen_vals, br.alpha_, br.lambda_, br.coef_, np.sum((y - np.dot(X, br.coef_)) ** 2))\n\n    assert_almost_equal(score_manual, br.scores_[-1])\n", "def test_bayesianridge_log_marginal_likelihood():\n    # Test the log marginal likelihood of BayesianRidge\n    X, y = diabetes.data, diabetes.target\n    n_samples, n_features = X.shape\n\n    # Initialize the values of the parameters\n    alpha_1 = 1.0e-6\n    alpha_2 = 1.0e-6\n    lambda_1 = 1.0e-6\n    lambda_2 = 1.0e-6\n    alpha_init = 1.0 / np.var(y)\n    lambda_init = 1.0\n\n    # Compute eigenvalues of X^T X\n    eigen_vals_ = np.linalg.eigvalsh(np.dot(X.T, X))\n\n    # Compute the log marginal likelihood\n    br = BayesianRidge(\n        fit_intercept=False,\n        alpha_1=alpha_1,\n        alpha_2=alpha_2,\n        lambda_1=lambda_1,\n        lambda_2=lambda_2,\n    )\n    br.alpha_ = alpha_init\n    br.lambda_ = lambda_init\n    coef_ = np.linalg.lstsq(X, y, rcond=None)[0]\n    rmse_ = np.sum((y - np.dot(X, coef_)) ** 2)\n\n    score_0 = br._log_marginal_likelihood(\n        n_samples, n_features, eigen_vals_, alpha_init, lambda_init, coef_, rmse_\n    )\n\n    # Increase alpha and lambda to check that the log marginal likelihood increases\n    alpha_init *= 10\n    lambda_init *= 10\n    score_1 = br._log_marginal_likelihood(\n        n_samples, n_features, eigen_vals_, alpha_init, lambda_init, coef_, rmse_\n    )\n    assert score_0 < score_1\n\n    # Check that a constant is added to the log marginal likelihood when compute_score=True\n    br.compute_score = True\n    br.fit(X, y)\n    assert len(br.scores_) == br.n_iter_ + 1\n", "def test_bayesianridge_hyperparameters():\n    # Test that hyperparameters are well updated\n\n    X, y = diabetes.data, diabetes.target\n\n    # Initial value of log(lambda_2) and log(alpha_2)\n    h1 = -6.0\n    h2 = -6.0\n\n    # Fitting the Bayesian Ridge regression model with the initial values\n    br = BayesianRidge(\n        lambda_1=1.0e-6,\n        lambda_2=1.0e-6,\n        alpha_1=1.0e-6,\n        alpha_2=1.0e-6,\n        fit_intercept=False,\n        normalize=False,\n        copy_X=True,\n    )\n    br.fit(X, y)\n\n    # Checking whether the values are updated correctly\n    assert_almost_equal(log(br.lambda_2), h1, decimal=5)\n    assert_almost_equal(log(br.alpha_2), h2, decimal=5)\n\n    # Checking whether the values are still the same after fitting\n    assert_almost_equal(br.lambda_1, 1.0e-6, decimal=10)\n    assert_almost_equal(br.alpha_1, 1.0e-6, decimal=10)\n", "def test_bayesian_ridge_parameter_update():\n    # Test that the parameter update in BayesianRidge is correct\n    X, y = diabetes.data, diabetes.target\n    n_samples, n_features = X.shape\n\n    # Initialize parameters\n    alpha_1 = 1.0e-6\n    alpha_2 = 1.0e-6\n    lambda_1 = 1.0e-6\n    lambda_2 = 1.0e-6\n    alpha_init = 1.0 / np.var(y)\n    lambda_init = 1.0\n\n    # Initialize model\n    br = BayesianRidge(alpha_1=alpha_1, alpha_2=alpha_2, lambda_1=lambda_1,\n                       lambda_2=lambda_2, alpha_init=alpha_init,\n                       lambda_init=lambda_init)\n\n    # Fit the model\n    br.fit(X, y)\n\n    # Compute gamma and new parameters\n    gamma_ = np.sum((br.alpha_ * np.linalg.eigvalsh(np.dot(X.T, X))) /\n                    (br.lambda_ + br.alpha_ * np.linalg.eigvalsh(np.dot(X.T, X))))\n    lambda_new = (gamma_ + 2 * br.lambda_1) / (np.sum(br.coef_**2) + 2 * br.lambda_2)\n    alpha_new = (n_samples - gamma_ + 2 * br.alpha_1) / (np.sum((y - np.dot(X, br.coef_))**2) +\n                                                        2 * br.alpha_2)\n\n    assert_almost_equal(lambda_new, br.lambda_)\n    assert_almost_equal(alpha_new, br.alpha_)\n", "def test_bayesian_ridge_log_marginal_likelihood():\n    # Test that the log marginal likelihood of the BayesianRidge is close to\n    # its definition\n    X, y = diabetes.data, diabetes.target\n    n_samples, n_features = X.shape\n\n    rng = check_random_state(0)\n    alpha_ = 1.0 / (np.var(y) + 1e-8)\n    lambda_ = 1.0\n    coef_ = rng.normal(size=n_features)\n\n        n_features = coef_.shape[0]\n        return (\n            0.5 * (-n_samples * np.log(2 * np.pi * (1.0 / alpha_)))\n            - 0.5 * (alpha_ * rmse_)\n            - 0.5 * (lambda_ * np.sum(coef_**2))\n            + 0.5 * (fast_logdet(np.eye(n_features) / lambda_))\n        )\n\n    br = BayesianRidge(compute_score=True)\n    br.fit(X, y)\n    assert_almost_equal(\n        br.scores_[-1], _log_marginal_likelihood(br.alpha_, br.lambda_, br.coef_, np.sum((y - np.dot(X, br.coef_)) ** 2)), 5\n    )\n"], "sample_884": ["def test_deprecated_class_with_overridden_new():\n    # Test that the deprecation warning is raised for a class that overrides __new__\n    with pytest.warns(FutureWarning, match=\"Class MockClass6 is deprecated; a message\"):\n        MockClass6(1)\n\n    # Test that the deprecation warning is not raised when calling _is_deprecated\n    assert _is_deprecated(MockClass6.__new__)\n\n    # Test that the original __new__ method is preserved\n    assert MockClass6.__new__.deprecated_original == object.__new__\n", "def test_deprecated_property():\n    # Test that the deprecation warning is raised when accessing a deprecated property.\n    obj = MockClass2()\n    with pytest.warns(FutureWarning, match=\"n_features_ is deprecated\"):\n        assert obj.n_features_ == 10\n\n    # Test that the property still works as expected after being decorated.\n    assert isinstance(MockClass2.n_features_, property)\n    assert MockClass2.n_features_.fget.__name__ == \"n_features_\"\n", "def test_deprecated_class_with_overridden_new():\n    # Test that the deprecation warning is raised when instantiating a class\n    # that overrides __new__ and is decorated with @deprecated.\n    with pytest.warns(FutureWarning, match=\"Class MockClass6 is deprecated; a message\"):\n        MockClass6(1)\n\n    # Test that the deprecation warning is not raised when calling the original\n    # __new__ method directly.\n    with pytest.warns(None) as record:\n        MockClass6.__new__.deprecated_original(MockClass6, 1)\n    assert len(record) == 0\n", "def test_deprecated_decorator_pickle():\n    # Test that the deprecated decorator is pickleable\n    @deprecated(\"test\")\n        return 10\n\n    pickled_func = pickle.dumps(func)\n    unpickled_func = pickle.loads(pickled_func)\n\n    assert _is_deprecated(unpickled_func)\n    with pytest.warns(FutureWarning):\n        unpickled_func()\n", "def test_deprecated_pickle():\n    # Test that deprecation warnings are preserved when pickling\n    pickle.loads(pickle.dumps(mock_function))\n    with pytest.warns(FutureWarning, match=\"Function mock_function is deprecated\"):\n        pickle.loads(pickle.dumps(mock_function))()\n"], "sample_885": ["def test_constraint_repr(constraint, params):\n    assert str(make_constraint(constraint)) == f\"an instance of {params[0]}\"\n", "def test_interval_is_satisfied_by(constraint, invalid_val):\n    assert not constraint.is_satisfied_by(invalid_val)\n", "def test_generate_invalid_param_val():\n    # Test for the different constraint types\n    constraints = [\n        _ArrayLikes(),\n        _SparseMatrices(),\n        _RandomStates(),\n        _Callables(),\n        _NoneConstraint(),\n        _InstancesOf(int),\n        _Booleans(),\n        _VerboseHelper(),\n        _MissingValues(),\n        HasMethods([\"__call__\"]),\n        _IterablesNotString(),\n        _CVObjects(),\n        Options(int, [1, 2, 3]),\n        StrOptions([\"a\", \"b\", \"c\"]),\n        Interval(Integral, 0, 5, closed=\"both\"),\n        Interval(Real, 0, 5, closed=\"both\"),\n        Interval(Integral, 0, None, closed=\"left\"),\n        Interval(Integral, None, 5, closed=\"right\"),\n        Interval(Real, 0, None, closed=\"left\"),\n        Interval(Real, None, 5, closed=\"right\"),\n    ]\n\n    for constraint in constraints:\n        try:\n            invalid_value = generate_invalid_param_val(constraint)\n            assert not constraint.is_satisfied_by(invalid_value)\n        except NotImplementedError:\n            continue\n\n    # Test for Interval with multiple constraints\n    constraint = Interval(Integral, 0, 5, closed=\"both\")\n    constraints = [constraint, Interval(Real, 0, 5, closed=\"both\")]\n    invalid_value = generate_invalid_param_val(constraint, constraints)\n    assert not any(c.is_satisfied_by(invalid_value) for c in constraints)\n", "def test_interval_is_satisfied_by(constraint, invalid_val):\n    \"\"\"Test that Interval constraint correctly identifies invalid values.\"\"\"\n    assert not constraint.is_satisfied_by(invalid_val)\n", "def test_interval_constraint(constraint, params, invalid_params):\n    for param in params:\n        assert constraint.is_satisfied_by(param)\n    for param in invalid_params:\n        assert not constraint.is_satisfied_by(param)\n"], "sample_886": ["def test_wrap_in_pandas_container_sparse():\n    # Test that wrapping sparse data raises an error\n    sparse_data = csr_matrix(np.array([[1, 2], [3, 4]]))\n    with pytest.raises(ValueError):\n        _wrap_in_pandas_container(sparse_data, columns=[\"col1\", \"col2\"])\n", "def test_wrap_in_pandas_container():\n    # Test wrapping a ndarray in a pandas DataFrame\n    data_to_wrap = np.array([[1, 2], [3, 4]])\n    columns = [\"col1\", \"col2\"]\n    index = [0, 1]\n    wrapped_data = _wrap_in_pandas_container(data_to_wrap, columns=columns, index=index)\n    assert_array_equal(wrapped_data.columns, columns)\n    assert_array_equal(wrapped_data.index, index)\n\n    # Test wrapping a sparse matrix raises an error\n    sparse_data = csr_matrix(np.array([[1, 2], [3, 4]]))\n    with pytest.raises(ValueError):\n        _wrap_in_pandas_container(sparse_data, columns=columns, index=index)\n\n    # Test wrapping a DataFrame with new columns and index\n    import pandas as pd\n    df = pd.DataFrame(np.array([[1, 2], [3, 4]]), columns=[\"old_col1\", \"old_col2\"])\n    wrapped_df = _wrap_in_pandas_container(df, columns=columns, index=index)\n    assert_array_equal(wrapped_df.columns, columns)\n    assert_array_equal(wrapped_df.index, index)\n", "def test_wrap_in_pandas_container():\n    # Test that _wrap_in_pandas_container returns a pandas DataFrame\n    import pandas as pd\n\n    data = np.array([[1, 2], [3, 4]])\n    columns = [\"col1\", \"col2\"]\n    index = [0, 1]\n\n    result = _wrap_in_pandas_container(data, columns=columns, index=index)\n    assert isinstance(result, pd.DataFrame)\n    assert_array_equal(result.values, data)\n    assert result.columns.tolist() == columns\n    assert result.index.tolist() == index\n\n    # Test that _wrap_in_pandas_container raises an error for sparse data\n    sparse_data = csr_matrix(data)\n    with pytest.raises(ValueError):\n        _wrap_in_pandas_container(sparse_data, columns=columns, index=index)\n\n    # Test that _wrap_in_pandas_container handles callable columns\n        return columns\n\n    result = _wrap_in_pandas_container(data, columns=get_columns, index=index)\n    assert isinstance(result, pd.DataFrame)\n    assert_array_equal(result.values, data)\n    assert result.columns.tolist() == columns\n    assert result.index.tolist() == index\n\n    # Test that _wrap_in_pandas_container handles None columns\n    result = _wrap_in_pandas_container(data, columns=None, index=index)\n    assert isinstance(result, pd.DataFrame)\n    assert_array_equal(result.values, data)\n    assert result.columns.tolist() == [0, 1]\n    assert result.index.tolist() == index\n", "def test__wrap_in_pandas_container_sparse_data():\n    # Test that _wrap_in_pandas_container raises an error for sparse data\n    sparse_data = csr_matrix(np.array([[1, 2], [3, 4]]))\n    with pytest.raises(ValueError, match=\"Pandas output does not support sparse data.\"):\n        _wrap_in_pandas_container(sparse_data, columns=None)\n", "def test__wrap_in_pandas_container_sparse():\n    sparse_data = csr_matrix(np.array([[1, 0, 2], [0, 3, 0]]))\n    with pytest.raises(ValueError):\n        _wrap_in_pandas_container(sparse_data, columns=None)\n"], "sample_887": ["def test_calibration_display_attributes(data):\n    X, y = data\n    clf = LogisticRegression()\n    clf.fit(X, y)\n    y_prob = clf.predict_proba(X)[:, 1]\n\n    display = CalibrationDisplay.from_estimator(clf, X, y)\n\n    assert hasattr(display, \"line_\")\n    assert hasattr(display, \"ax_\")\n    assert hasattr(display, \"figure_\")\n\n    assert hasattr(display, \"prob_true\")\n    assert hasattr(display, \"prob_pred\")\n    assert hasattr(display, \"y_prob\")\n    assert hasattr(display, \"estimator_name\")\n", "def test_calibration_display_from_estimator(data):\n    X, y = data\n    clf = LogisticRegression(random_state=0)\n    clf.fit(X, y)\n\n    display = CalibrationDisplay.from_estimator(clf, X, y)\n    assert hasattr(display, \"ax_\")\n    assert hasattr(display, \"figure_\")\n    assert hasattr(display, \"line_\")\n", "def test_calibration_curve_multiclass_warns(data):\n    X, y = make_classification(\n        n_samples=100, n_features=6, n_informative=3, n_redundant=0, n_classes=3, random_state=42\n    )\n    clf = DecisionTreeClassifier(random_state=42)\n    y_pred = clf.fit(X, y).predict_proba(X)[:, 1]\n    with pytest.warns(UserWarning, match=\"Only binary classification is supported\"):\n        calibration_curve(y, y_pred)\n", "def test_calibrated_classifier_cv_prefit(data):\n    X, y = data\n    X_train, X_calib, y_train, y_calib = train_test_split(\n        X, y, random_state=42, test_size=0.8\n    )\n    base_clf = LogisticRegression(random_state=42)\n    base_clf.fit(X_train, y_train)\n\n    calibrated_clf = CalibratedClassifierCV(base_clf, cv=\"prefit\")\n    calibrated_clf.fit(X_calib, y_calib)\n\n    assert len(calibrated_clf.calibrated_classifiers_) == 1\n\n    y_pred = calibrated_clf.predict_proba(X_calib)\n    assert_array_almost_equal(np.sum(y_pred, axis=1), np.ones(N_SAMPLES * 0.8))\n", "def test_calibrated_classifier_cv_sample_weight(data):\n    X, y = data\n    sample_weight = np.random.rand(N_SAMPLES)\n    clf = CalibratedClassifierCV(\n        base_estimator=LinearSVC(random_state=0), cv=5, method=\"sigmoid\"\n    )\n    with pytest.warns(UserWarning, match=\"sample_weight\"):\n        clf.fit(X, y, sample_weight=sample_weight)\n\n    y_pred = clf.predict(X)\n    assert_array_equal(np.unique(y_pred), np.array([0, 1]))\n"], "sample_888": ["def test_iforest_average_path_length():\n    # Test _average_path_length function\n\n    n_samples_leaf = np.array([1, 2, 3, 4, 5])\n    average_path_length_computed = _average_path_length(n_samples_leaf)\n\n    # Check for NaNs\n    assert not np.isnan(average_path_length_computed).any()\n\n    # Check the shape of the output\n    assert average_path_length_computed.shape == (n_samples_leaf.shape[0],)\n\n    # Check values for edge cases (n_samples_leaf <= 2)\n    assert_array_equal(average_path_length_computed[:2], [0.0, 1.0])\n\n    # Check non-edge case values using pre-computed values\n    # These values were computed manually using the formula in the original paper\n    expected_values = np.array(\n        [\n            0.0,\n            1.0,\n            2 * (np.log(2) + np.euler_gamma) - 2 * (2 / 3),\n            2 * (np.log(3) + np.euler_gamma) - 2 * (3 / 4),\n            2 * (np.log(4) + np.euler_gamma) - 2 * (4 / 5),\n        ]\n    )\n    assert_allclose(average_path_length_computed, expected_values, atol=1e-10)\n", "def test_iforest_sparse():\n    # Test that sparse input data is handled correctly\n    X = csc_matrix(np.random.rand(10, 5))\n    y = np.random.randint(0, 2, size=10)\n\n    clf = IsolationForest()\n    with ignore_warnings(category=FutureWarning):\n        clf.fit(X, y)\n\n    assert clf.predict(X).shape == (10,)\n    assert clf.decision_function(X).shape == (10,)\n\n    X_csr = csr_matrix(X)\n    assert_array_equal(clf.predict(X_csr), clf.predict(X))\n    assert_array_almost_equal(clf.decision_function(X_csr), clf.decision_function(X))\n", "def test_iforest_sparse():\n    # Test that sparse input data is handled correctly\n\n    X, y = make_classification(n_samples=100, n_features=10, random_state=0)\n    X_sparse = csc_matrix(X)\n\n    iforest = IsolationForest(random_state=0)\n    iforest.fit(X_sparse)\n\n    # Check that the fitted model can be pickled and unpickled\n    import pickle\n    iforest_pickled = pickle.loads(pickle.dumps(iforest))\n\n    assert_array_equal(iforest.predict(X_sparse), iforest_pickled.predict(X_sparse))\n", "def test_iforest_sparse_input():\n    # Test Isolation Forest on sparse input\n    X = csc_matrix(diabetes.data)\n    rng = check_random_state(0)\n    iforest = IsolationForest(n_estimators=10, random_state=rng)\n    iforest.fit(X)\n\n    # Check that fitting with sparse data works\n    assert hasattr(iforest, 'estimators_')\n    assert len(iforest.estimators_) == 10\n\n    # Check that predict works\n    y_pred = iforest.predict(X)\n    assert_array_equal(y_pred.shape, (X.shape[0],))\n", "def test_iforest_average_path_length():\n    # Test the _average_path_length function\n\n    # Check for a single sample\n    assert _average_path_length([1]) == 0.0\n\n    # Check for two samples\n    assert _average_path_length([2]) == 1.0\n\n    # Check for more than two samples\n    n_samples_leaf = np.array([3, 5, 10])\n    average_path_length = _average_path_length(n_samples_leaf)\n    expected_result = (\n        2.0 * (np.log(n_samples_leaf - 1.0) + np.euler_gamma)\n        - 2.0 * (n_samples_leaf - 1.0) / n_samples_leaf\n    )\n    assert_array_almost_equal(average_path_length, expected_result)\n\n    # Check for input with shape (n_samples,)\n    n_samples_leaf = np.array([[3], [5], [10]])\n    average_path_length = _average_path_length(n_samples_leaf)\n    assert_array_almost_equal(average_path_length, expected_result)\n\n    # Check for non-integer input\n    n_samples_leaf = np.array([3.0, 5.0, 10.0])\n    with pytest.raises(ValueError):\n        _average_path_length(n_samples_leaf)\n\n    # Check for negative input\n    n_samples_leaf = np.array([-3, -5, -10])\n    with pytest.raises(ValueError):\n        _average_path_length(n_samples_leaf)\n\n    # Check for zero input\n    n_samples_leaf = np.array([0, 0, 0])\n    with pytest.raises(ValueError):\n        _average_path_length(n_samples_leaf)\n"], "sample_889": ["def test_calibrated_classifier_cv_pos_label(data):\n    X, y = data\n    y[y == 0] = -1  # Use -1 as the negative class label\n\n    # Check that CalibratedClassifierCV works with pos_label='1'\n    clf = CalibratedClassifierCV(\n        base_estimator=LogisticRegression(), cv=2, method=\"sigmoid\", pos_label=1\n    )\n    clf.fit(X, y)\n\n    # Check that the calibration display also works with pos_label='1'\n    CalibrationDisplay.from_estimator(clf, X, y, pos_label=1)\n", "def test_calibration_display_from_predictions(data):\n    X, y = data\n    clf = LogisticRegression()\n    clf.fit(X, y)\n    y_prob = clf.predict_proba(X)[:, 1]\n\n    display = CalibrationDisplay.from_predictions(y, y_prob)\n    assert hasattr(display, \"line_\")\n    assert hasattr(display, \"ax_\")\n    assert hasattr(display, \"figure_\")\n\n    # Check that the plot displays the calibration curve\n    line_data = display.line_.get_xydata()\n    assert_array_almost_equal(line_data[:, 0], display.prob_pred)\n    assert_array_almost_equal(line_data[:, 1], display.prob_true)\n", "def test_calibration_display(data):\n    X, y = data\n    clf = LogisticRegression()\n    clf.fit(X, y)\n    y_prob = clf.predict_proba(X)[:, 1]\n\n    display = CalibrationDisplay.from_estimator(clf, X, y)\n    assert isinstance(display, CalibrationDisplay)\n\n    display = CalibrationDisplay.from_predictions(y, y_prob)\n    assert isinstance(display, CalibrationDisplay)\n\n    # check that the plot method returns the display instance\n    assert display.plot() is display\n", "def test_calibrated_classifier_cv_sample_weight(data):\n    X, y = data\n    sample_weight = np.random.rand(N_SAMPLES)\n    clf = CalibratedClassifierCV(RandomForestClassifier(random_state=42), cv=3)\n    clf.fit(X, y, sample_weight=sample_weight)\n\n    # Check that the weights are used in the calibration process\n    assert hasattr(clf.calibrated_classifiers_[0], \"sample_weight\")\n", "def test_calibration_display_attributes(data):\n    X, y = data\n    clf = LogisticRegression()\n    clf.fit(X, y)\n    y_prob = clf.predict_proba(X)[:, 1]\n    prob_true, prob_pred = calibration_curve(y, y_prob)\n    disp = CalibrationDisplay(prob_true, prob_pred, y_prob)\n\n    assert hasattr(disp, \"line_\")\n    assert hasattr(disp, \"ax_\")\n    assert hasattr(disp, \"figure_\")\n\n    disp.plot()\n    assert disp.line_ is not None\n    assert disp.ax_ is not None\n    assert disp.figure_ is not None\n"], "sample_890": ["def test_sequential_feature_selector_forward_with_auto_and_tol():\n    # Test that SequentialFeatureSelector with forward direction, 'auto' and tol\n    # stops adding features when the score improvement is less than tol\n    X, y = make_regression(n_samples=100, n_features=10, random_state=0)\n    estimator = LinearRegression()\n    sfs = SequentialFeatureSelector(estimator, direction='forward', \n                                    n_features_to_select='auto', tol=0.01)\n    sfs.fit(X, y)\n    assert sfs.n_features_to_select_ < X.shape[1]\n", "def test_sequential_feature_selector_with_auto_and_tol():\n    # Test that SequentialFeatureSelector with 'auto' and tol works as expected\n    X, y = make_regression(n_samples=100, n_features=10, random_state=0)\n    estimator = LinearRegression()\n    selector = SequentialFeatureSelector(estimator, n_features_to_select='auto', tol=0.01)\n    selector.fit(X, y)\n    assert selector.n_features_to_select_ > 0\n    assert selector.support_.sum() > 0\n", "def test_sequential_feature_selector_with_auto_n_features_to_select_and_tol():\n    # Test SequentialFeatureSelector with auto n_features_to_select and tol\n    X, y = make_regression(n_samples=50, n_features=10, random_state=0)\n    estimator = LinearRegression()\n    selector = SequentialFeatureSelector(estimator, n_features_to_select=\"auto\", tol=0.01)\n    selector.fit(X, y)\n    assert selector.n_features_to_select_ > 0\n    assert selector.support_.sum() == selector.n_features_to_select_\n", "def test_sequential_feature_selector_tol_parameter():\n    # Test that the tol parameter is working as expected\n    X, y = make_regression(n_samples=100, n_features=10, random_state=42)\n    estimator = LinearRegression()\n    selector = SequentialFeatureSelector(estimator, n_features_to_select=\"auto\", tol=0.01)\n\n    selector.fit(X, y)\n    assert selector.n_features_to_select_ < X.shape[1]\n\n    # Test that a negative tol raises an error when doing forward selection\n    with pytest.raises(ValueError):\n        SequentialFeatureSelector(estimator, n_features_to_select=\"auto\", tol=-0.01, direction=\"forward\")\n\n    # Test that a negative tol does not raise an error when doing backward selection\n    selector = SequentialFeatureSelector(estimator, n_features_to_select=\"auto\", tol=-0.01, direction=\"backward\")\n    selector.fit(X, y)\n", "def test_sequential_feature_selector_with_auto_n_features_to_select_and_tol():\n    # Test that the SequentialFeatureSelector works with 'auto' n_features_to_select and tol\n    X, y = make_regression(n_samples=50, n_features=10, random_state=0)\n    estimator = LinearRegression()\n    selector = SequentialFeatureSelector(estimator, n_features_to_select='auto', tol=0.1, cv=5)\n    selector.fit(X, y)\n    assert selector.n_features_to_select_ > 0\n    assert selector.support_.sum() == selector.n_features_to_select_\n"], "sample_891": ["def test_top_k_accuracy_score_tied_scores():\n    y_true = np.array([0, 1, 2])\n    y_score = np.array([[1, 1, 0.5], [0.7, 1, 0.4], [0.3, 0.4, 1]])\n    # with tied scores and k=1, the top-k accuracy will be lower than 1\n    assert_almost_equal(top_k_accuracy_score(y_true, y_score, k=1), 0.33, decimal=2)\n    # with tied scores and k=2, the top-k accuracy should be 1\n    assert_almost_equal(top_k_accuracy_score(y_true, y_score, k=2), 1.0, decimal=2)\n", "def test_roc_curve_one_class():\n    # Test roc_curve when only one class is present in y_true\n    y_true = np.array([1, 1, 1, 1])\n    y_score = np.array([0.5, 0.6, 0.7, 0.8])\n    with pytest.warns(UndefinedMetricWarning):\n        fpr, tpr, _ = roc_curve(y_true, y_score)\n    assert_array_almost_equal(fpr, [np.nan])\n    assert_array_almost_equal(tpr, [np.nan])\n", "def test_top_k_accuracy_score():\n    y_true = np.array([0, 1, 2, 2])\n    y_score = np.array([[0.5, 0.2, 0.2], [0.3, 0.4, 0.2], [0.2, 0.4, 0.3], [0.7, 0.2, 0.1]])\n\n    # Should get 100% accuracy for k=3\n    assert_almost_equal(top_k_accuracy_score(y_true, y_score, k=3), 1.00)\n\n    # Should get 75% accuracy for k=2\n    assert_almost_equal(top_k_accuracy_score(y_true, y_score, k=2), 0.75)\n\n    # Test normalize=False\n    assert_array_equal(top_k_accuracy_score(y_true, y_score, k=2, normalize=False), 3)\n\n    # Should raise an error for arrays of different dimensions\n    with pytest.raises(ValueError):\n        top_k_accuracy_score(y_true, y_score[:, :2])\n\n    # Should raise an error for arrays of different lengths\n    with pytest.raises(ValueError):\n        top_k_accuracy_score(y_true[:-1], y_score)\n", "def test_top_k_accuracy_score_binary():\n    # Test top-k accuracy score for binary classification\n    y_true = np.array([0, 1, 1, 0])\n    y_score = np.array([[0.8, 0.2], [0.4, 0.6], [0.3, 0.7], [0.9, 0.1]])\n    assert_almost_equal(top_k_accuracy_score(y_true, y_score, k=1), 0.75)\n    assert_almost_equal(top_k_accuracy_score(y_true, y_score[:, 1], k=1), 0.75)\n\n    # Check with `normalize` set to False\n    assert_array_equal(top_k_accuracy_score(y_true, y_score, k=1, normalize=False), 3)\n\n    # Check that `labels` parameter is not used in binary case\n    assert_almost_equal(\n        top_k_accuracy_score(y_true, y_score, k=1, labels=[0, 1]), 0.75\n    )\n", "def test_roc_curve_drop_intermediate():\n    # Test that drop_intermediate drops the correct thresholds\n    y_true = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1, 1])\n    y_score = np.array(\n        [0.02058, 0.08598, 0.55154, 0.65265, 0.05778, 0.12551, 0.41418, 0.88738, 0.82976, 0.96403]\n    )\n    fpr, tpr, thresh = roc_curve(y_true, y_score, drop_intermediate=False)\n\n    # The 3rd and 4th sample are dropped because they do not add any extra\n    # information to the curve.\n    assert_array_almost_equal(fpr, [0.0, 0.25, 0.5, 0.75, 1.0])\n    assert_array_almost_equal(tpr, [0.833, 0.833, 0.667, 0.5, 0.0])\n\n    fpr, tpr, thresh = roc_curve(y_true, y_score, drop_intermediate=True)\n    assert_array_almost_equal(fpr, [0.0, 0.5, 0.75, 1.0])\n    assert_array_almost_equal(tpr, [0.833, 0.667, 0.5, 0.0])\n"], "sample_892": ["def test_adaboost_regressor_with_sparse_matrix():\n    # Test AdaBoostRegressor with sparse matrix\n    X_sparse = csr_matrix(X)\n    ada = AdaBoostRegressor(random_state=0, n_estimators=10)\n    ada.fit(X_sparse, y_regr)\n    assert_array_almost_equal(ada.predict(T), ada.predict(csr_matrix(T)))\n\n    # Test with different types of sparse matrices\n    for sparse_matrix_type in [csc_matrix, coo_matrix, dok_matrix, lil_matrix]:\n        X_sparse = sparse_matrix_type(X)\n        ada = AdaBoostRegressor(random_state=0, n_estimators=10)\n        ada.fit(X_sparse, y_regr)\n        assert_array_almost_equal(ada.predict(T), ada.predict(sparse_matrix_type(T)))\n", "def test_adaboost_regressor_with_svr_base_estimator():\n    # Test AdaBoostRegressor with SVR as base estimator\n    X, y = diabetes.data, diabetes.target\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=rng)\n    svr = SVR(kernel='linear')\n    adaboost = AdaBoostRegressor(base_estimator=svr, n_estimators=10, random_state=rng)\n    adaboost.fit(X_train, y_train)\n    assert_array_less(adaboost.score(X_test, y_test), 1.0)\n", "def test_adaboost_regressor_loss():\n    # Test the loss function for AdaBoostRegressor\n    X, y = diabetes.data, diabetes.target\n\n    # linear loss\n    regr_linear = AdaBoostRegressor(loss=\"linear\")\n    regr_linear.fit(X, y)\n    y_pred_linear = regr_linear.predict(X)\n\n    # square loss\n    regr_square = AdaBoostRegressor(loss=\"square\")\n    regr_square.fit(X, y)\n    y_pred_square = regr_square.predict(X)\n\n    # exponential loss\n    regr_exponential = AdaBoostRegressor(loss=\"exponential\")\n    regr_exponential.fit(X, y)\n    y_pred_exponential = regr_exponential.predict(X)\n\n    assert_array_less(\n        np.abs(y - y_pred_linear), np.abs(y - np.median(y)) * 0.9\n    )\n    assert_array_less(\n        np.abs(y - y_pred_square), np.abs(y - np.median(y)) * 0.9\n    )\n    assert_array_less(\n        np.abs(y - y_pred_exponential), np.abs(y - np.median(y)) * 0.9\n    )\n\n    # Check if the predict method raises an error when loss is not one of the\n    # expected values\n    regr_invalid_loss = AdaBoostRegressor(loss=\"invalid\")\n    with pytest.raises(ValueError):\n        regr_invalid_loss.fit(X, y)\n", "def test_ensemble_weights():\n    # Test that ensemble weights are correctly computed.\n    X, y = iris.data, iris.target\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\n    clf = AdaBoostClassifier(n_estimators=10, random_state=0)\n    clf.fit(X_train, y_train)\n\n    # Check that ensemble weights sum to 1.\n    assert_array_almost_equal(np.sum(clf.estimator_weights_), 1.0)\n\n    # Check that ensemble weights are non-negative.\n    assert_array_less(0, clf.estimator_weights_)\n", "def test_adaboost_regressor_with_svr():\n    # Test that AdaBoostRegressor works with SVR as base estimator.\n    # This is a non-trivial test since SVR does not expose a `coef_`\n    # attribute, which makes it incompatible with many of the other\n    # estimators.\n    X, y = diabetes.data, diabetes.target\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    svr = SVR(gamma=\"scale\")\n    adaboost_svr = AdaBoostRegressor(base_estimator=svr, n_estimators=10, random_state=42)\n    adaboost_svr.fit(X_train, y_train)\n    assert adaboost_svr.score(X_test, y_test) > 0.1\n"], "sample_893": ["def test_export_text():\n    # Check export_text on a simple tree\n    clf = DecisionTreeClassifier(random_state=0, max_depth=3)\n    clf.fit(X, y)\n    report = export_text(clf)\n    assert isinstance(report, str)\n\n    # Check export_text with feature_names\n    report = export_text(clf, feature_names=[\"feature0\", \"feature1\"])\n    assert \"feature0\" in report\n    assert \"feature1\" in report\n\n    # Check export_text with class_names\n    report = export_text(clf, class_names=[\"class0\", \"class1\"])\n    assert \"class: class0\" in report or \"class: class1\" in report\n\n    # Check export_text with max_depth\n    report = export_text(clf, max_depth=2)\n    assert \"...\" not in report\n    report = export_text(clf, max_depth=1)\n    assert \"...\" in report\n\n    # Check export_text with show_weights\n    report = export_text(clf, show_weights=True)\n    assert \"weights: [\" in report\n", "def test_export_text_classifier():\n    clf = DecisionTreeClassifier(random_state=0, max_depth=2)\n    clf.fit(X, y)\n\n    report = export_text(clf, feature_names=[\"feature_0\", \"feature_1\"])\n    lines = report.split(\"\\n\")\n\n    assert \"feature_0\" in lines[0]\n    assert \"class: 0\" in lines[1]\n\n    # Check for correct number of lines\n    assert len(lines) == 6\n\n    # Check for correct indentation\n    assert lines[0].startswith(\"|---\")\n    assert lines[1].startswith(\"|   |---\")\n    assert lines[2].startswith(\"|---\")\n    assert lines[3].startswith(\"|   |---\")\n    assert lines[4].startswith(\"|   |   |---\")\n    assert lines[5].startswith(\"|   |   |---\")\n\n    # Check that thresholds are displayed with the correct precision\n    assert \".00\" in lines[0]\n    assert \".75\" in lines[2]\n", "def test_export_text_classification_weights():\n    clf = DecisionTreeClassifier(random_state=0, max_depth=2)\n    clf.fit(X, y)\n    report = export_text(clf, feature_names=[\"feature_0\", \"feature_1\"], show_weights=True)\n\n    # Check the classification weights are included in the report\n    assert \"weights: [\" in report\n\n    # Check that the classification weights are correctly formatted\n    lines_with_weights = [line for line in report.split(\"\\n\") if \"weights:\" in line]\n    for line in lines_with_weights:\n        # Extract the weights from the line\n        start_idx = line.find(\"[\")\n        end_idx = line.find(\"]\")\n        weights_str = line[start_idx + 1:end_idx]\n        weights = [float(w) for w in weights_str.split(\", \")]\n\n        # Check that the weights sum to the total number of samples\n        assert np.isclose(sum(weights), len(y))\n", "def test_export_text_feature_names_deprecated():\n    clf = DecisionTreeClassifier(random_state=0)\n    clf.fit(X, y)\n\n    msg = \"feature_names is deprecated\"\n    with pytest.warns_message(FutureWarning, match=msg):\n        export_text(clf, feature_names=[\"a\", \"b\"])\n", "def test_export_text_feature_names():\n    clf = DecisionTreeClassifier(random_state=0, max_depth=2)\n    clf.fit(X, y)\n\n    report = export_text(clf, feature_names=[\"feature_0\", \"feature_1\"])\n    assert \"feature_0\" in report and \"feature_1\" in report\n\n    # Check that feature names are displayed when they are provided\n    feature_names = [\"petal width (cm)\", \"petal length (cm)\"]\n    report = export_text(clf, feature_names=feature_names)\n    for feature_name in feature_names:\n        assert feature_name in report\n"], "sample_894": ["def check_parallel(name):\n    \"\"\"Check parallel computations in classification and regression.\"\"\"\n    ForestEstimator = FOREST_ESTIMATORS[name]\n\n    X, y = datasets.make_classification(\n        n_samples=200, n_features=10, n_informative=5, random_state=1\n    )\n\n    estimator = ForestEstimator(n_estimators=20, n_jobs=-1, random_state=0)\n\n    if name in FOREST_CLASSIFIERS_REGRESSORS:\n        with ignore_warnings(message=\"F-score is ill-defined\"):\n            scores = cross_val_score(estimator, X, y, cv=5)\n        assert scores.mean() > 0.9\n\n    else:  # RandomTreesEmbedding\n        estimator.fit(X, y)\n        assert estimator.transform(X).shape[0] == X.shape[0]\n", "def test_forest_classifier_predict_proba(name):\n    \"\"\"Check classification predict_proba on a toy dataset.\"\"\"\n    ForestClassifier = FOREST_CLASSIFIERS[name]\n\n    clf = ForestClassifier(n_estimators=10, random_state=1)\n    clf.fit(X, y)\n\n    # check that predict_proba works\n    y_prob = clf.predict_proba(X)\n    assert_array_almost_equal(y_prob.sum(axis=1), np.ones(len(X)))\n\n    # check that predict_proba works with n_outputs > 1\n    X_multi, y_multi = make_classification(\n        n_samples=100, n_features=10, n_informative=3, n_redundant=0,\n        n_classes=2, n_outputs=3, random_state=1\n    )\n    clf.fit(X_multi, y_multi)\n    y_prob = clf.predict_proba(X_multi)\n    assert len(y_prob) == 3\n    for y_prob_i in y_prob:\n        assert_array_almost_equal(y_prob_i.sum(axis=1), np.ones(len(X_multi)))\n", "def check_parallel_pickle(name):\n    \"\"\"Check parallel pickling.\"\"\"\n    ForestEstimator = FOREST_ESTIMATORS[name]\n    # check that we can pickle and unpickle the estimator without exception\n    clf = ForestEstimator(n_estimators=10, n_jobs=2)\n    clf.fit(X_large, y_large)\n\n    # check that we can re-use the pickled estimator for prediction\n    y_pred = clf.predict(X_large)\n    assert_array_almost_equal(y_pred, clf.predict(X_large))\n\n    # check that we get an almost identical output after pickling and\n    # unpickling\n    with ignore_warnings():\n        pickled_clf = pickle.loads(pickle.dumps(clf))\n    y_pred_pickled = pickled_clf.predict(X_large)\n    assert_array_almost_equal(y_pred, y_pred_pickled)\n", "def test_classification_sparse_input():\n    \"\"\"Check classification with sparse input.\"\"\"\n    for name in FOREST_CLASSIFIERS:\n        ForestClassifier = FOREST_CLASSIFIERS[name]\n        X_sparse = csr_matrix(X)\n        clf = ForestClassifier(n_estimators=10, random_state=1)\n        clf.fit(X_sparse, y)\n        assert_array_equal(clf.predict(T), true_result)\n        assert 10 == len(clf)\n\n        # Check predict_proba with sparse input\n        if name != \"ExtraTreesClassifier\":\n            # ExtraTreesClassifier does not support sparse input for predict_proba\n            proba_sparse = clf.predict_proba(X_sparse)\n            proba_dense = clf.predict_proba(np.array(X))\n            assert_array_almost_equal(proba_sparse, proba_dense)\n\n        # Check apply with sparse input\n        leaf_indices = clf.apply(X_sparse)\n        assert leaf_indices.shape == (len(X), clf.n_estimators)\n", "def test_forest_regressor_multilabel_output():\n    \"\"\"Check that forest regressors can predict multilabel outputs.\"\"\"\n    X, y = datasets.make_regression(n_samples=10, n_features=5, n_targets=3)\n\n    for name in FOREST_REGRESSORS:\n        ForestRegressor = FOREST_REGRESSORS[name]\n        reg = ForestRegressor(n_estimators=5)\n        y_pred = reg.fit(X, y).predict(X)\n\n        assert y_pred.shape == (10, 3)\n        assert_array_equal(y_pred.shape, reg.predict(X).shape)\n"], "sample_895": ["def test_column_transformer_feature_names_out():\n    X = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\n\n    # Test that feature names out is correct when using verbose_feature_names_out=True\n    ct = ColumnTransformer(\n        [(\"trans1\", Trans(), [\"A\"]), (\"trans2\", Trans(), [\"B\"])],\n        verbose_feature_names_out=True,\n    )\n    ct.fit(X)\n    assert ct.get_feature_names_out() == [\"trans1__A\", \"trans2__B\"]\n\n    # Test that feature names out is correct when using verbose_feature_names_out=False\n    ct = ColumnTransformer(\n        [(\"trans1\", Trans(), [\"A\"]), (\"trans2\", Trans(), [\"B\"])],\n        verbose_feature_names_out=False,\n    )\n    ct.fit(X)\n    assert ct.get_feature_names_out() == [\"A\", \"B\"]\n\n    # Test that an error is raised when feature names are not unique and\n    # verbose_feature_names_out=False\n    ct = ColumnTransformer(\n        [(\"trans1\", Trans(), [\"A\"]), (\"trans2\", Trans(), [\"A\"])],\n        verbose_feature_names_out=False,\n    )\n    ct.fit(X)\n    with pytest.raises(ValueError):\n        ct.get_feature_names_out()\n", "def test_column_transformer_get_feature_names_out():\n    pandas = pytest.importorskip(\"pandas\")\n    X_array = np.array([[1, 2], [3, 4]])\n    X_df = pandas.DataFrame(X_array, columns=[\"col1\", \"col2\"])\n\n    # Basic case with numpy array\n    ct = ColumnTransformer([(\"trans1\", Trans(), [0]), (\"trans2\", Trans(), [1])])\n    ct.fit(X_array)\n    assert_array_equal(ct.get_feature_names_out(), [\"trans1__x0\", \"trans2__x1\"])\n\n    # Case with pandas dataframe\n    ct = ColumnTransformer(\n        [(\"trans1\", Trans(), [\"col1\"]), (\"trans2\", Trans(), [\"col2\"])]\n    )\n    ct.fit(X_df)\n    assert_array_equal(ct.get_feature_names_out(), [\"trans1__col1\", \"trans2__col2\"])\n\n    # Case with verbose_feature_names_out=False\n    ct = ColumnTransformer(\n        [(\"trans1\", Trans(), [0]), (\"trans2\", Trans(), [1])],\n        verbose_feature_names_out=False,\n    )\n    ct.fit(X_array)\n    assert_array_equal(ct.get_feature_names_out(), [\"x0\", \"x1\"])\n\n    # Case with non-string columns\n    ct = ColumnTransformer([(\"trans1\", Trans(), [True, False]), (\"trans2\", Trans(), [1])])\n    ct.fit(X_array)\n    assert_array_equal(ct.get_feature_names_out(), [\"trans1__x0\", \"trans2__x1\"])\n\n    # Case with empty selection\n    ct = ColumnTransformer([(\"trans1\", Trans(), [])])\n    ct.fit(X_array)\n    assert_array_equal(ct.get_feature_names_out(), [])\n\n    # Case with remainder='passthrough'\n    ct = ColumnTransformer(\n        [(\"trans1\", Trans(), [0])], remainder=\"passthrough\", verbose_feature_names_out=False\n    )\n    ct.fit(X_array)\n    assert_array_equal(ct.get_feature_names_out(), [\"x0\", \"x1\"])\n", "def test_column_transformer_feature_names_out_pandas():\n    X = pd.DataFrame({\"A\": [1, 2], \"B\": [3, 4]})\n    ct = ColumnTransformer(\n        [\n            (\"trans1\", Trans(), [\"A\"]),\n            (\"trans2\", Trans(), [\"B\"]),\n        ],\n        verbose_feature_names_out=False,\n    )\n    ct.fit(X)\n    assert_array_equal(ct.get_feature_names_out(), [\"A\", \"B\"])\n", "def test_column_transformer_feature_names_out_pandas():\n    X_array = np.array([[1, 2], [3, 4]])\n    X_df = pd.DataFrame(X_array, columns=[\"col1\", \"col2\"])\n\n    # single transformer with pandas output\n    transformer = ColumnTransformer(\n        [(\"trans1\", StandardScaler(), [\"col1\"]), (\"trans2\", StandardScaler(), [\"col2\"])]\n    )\n    transformer.fit(X_df)\n\n    feature_names_out_explicit = [\"trans1__col1\", \"trans2__col2\"]\n    feature_names_out_implicit = [\"col1\", \"col2\"]\n\n    assert_array_equal(\n        transformer.get_feature_names_out(),\n        feature_names_out_explicit,\n    )\n\n    # multiple transformers with pandas output\n    transformer = ColumnTransformer(\n        [\n            (\"trans1\", StandardScaler(), [\"col1\"]),\n            (\"trans2\", OneHotEncoder(), [\"col2\"]),\n            (\"trans3\", StandardScaler(), [\"col1\"]),\n        ]\n    )\n    transformer.fit(X_df)\n\n    feature_names_out_explicit = [\"trans1__col1\", \"trans2__col2_1\", \"trans2__col2_2\", \"trans3__col1\"]\n    feature_names_out_implicit = [\"col1\", \"col2_1\", \"col2_2\", \"col1\"]\n\n    assert_array_equal(\n        transformer.get_feature_names_out(),\n        feature_names_out_explicit,\n    )\n\n    # test verbose_feature_names_out=False\n    transformer.verbose_feature_names_out = False\n\n    assert_array_equal(\n        transformer.get_feature_names_out(),\n        feature_names_out_implicit,\n    )\n", "def test_column_transformer_feature_names_out_pandas():\n    # Test that feature names out are correctly prefixed for pandas dataframe output\n    X = pd.DataFrame({\"A\": [1, 2], \"B\": [3, 4]})\n    transformer = ColumnTransformer(\n        [(\"trans1\", Trans(), [\"A\"]), (\"trans2\", Trans(), [\"B\"])],\n        verbose_feature_names_out=True,\n    )\n    Xt = transformer.fit_transform(X)\n    assert isinstance(Xt, pd.DataFrame)\n    assert_array_equal(Xt.columns, [\"trans1__A\", \"trans2__B\"])\n"], "sample_896": ["def test_nmf_deprecation_warning_init_param():\n    with pytest.warns(FutureWarning) as records:\n        NMF(init=\"random\")\n    message = records[0].message.args[0]\n    assert \"The default value of `init` will change from 'random' to None\" in message\n\n    with pytest.warns(FutureWarning) as records:\n        MiniBatchNMF(init=\"random\")\n    message = records[0].message.args[0]\n    assert \"The default value of `init` will change from 'random' to None\" in message\n", "def test_nmf_multiplicative_update_sparse():\n    # Test that NMF with multiplicative update gives same result for dense and sparse matrices\n    random_state = np.random.RandomState(0)\n    X = random_state.rand(10, 5)\n    X[X < 0.8] = 0\n\n    W_ref, H_ref = non_negative_factorization(\n        X,\n        n_components=3,\n        solver=\"mu\",\n        init=\"random\",\n        random_state=random_state,\n    )\n\n    X_sparse = csc_matrix(X)\n    W, H = non_negative_factorization(\n        X_sparse,\n        n_components=3,\n        solver=\"mu\",\n        init=\"random\",\n        random_state=random_state,\n    )\n\n    assert_array_almost_equal(W, W_ref)\n    assert_array_almost_equal(H, H_ref)\n", "def test_nmf_initialize_nn_output():\n    # Test that NMF._initialize_nmf returns non-negative values.\n    rng = np.random.mtrand.RandomState(0)\n    X = np.abs(rng.randn(10, 5))\n    W, H = nmf._initialize_nmf(X, 3, init=\"random\", random_state=0)\n\n    assert not (W < 0).any()\n    assert not (H < 0).any()\n", "def test_nmf_verbose():\n    A = np.random.rand(10, 10)\n    nmf = NMF(n_components=5, max_iter=1, verbose=10)\n    old_stdout = sys.stdout\n    captured_output = StringIO()\n    sys.stdout = captured_output\n    nmf.fit(A)\n    sys.stdout = old_stdout\n    assert len(captured_output.getvalue()) > 0\n\n    nmf = MiniBatchNMF(n_components=5, max_iter=1, verbose=True)\n    old_stdout = sys.stdout\n    captured_output = StringIO()\n    sys.stdout = captured_output\n    nmf.fit(A)\n    sys.stdout = old_stdout\n    assert len(captured_output.getvalue()) > 0\n", "def test_nmf_solver_parameter():\n    # Test that an error is raised if an invalid solver is passed.\n    X = np.abs(np.random.rand(10, 10))\n\n    msg = \"Invalid solver parameter 'invalid'\"\n\n    with pytest.raises(ValueError, match=msg):\n        NMF(solver=\"invalid\").fit(X)\n"], "sample_897": ["def test_PartialDependenceDisplay_categorical_feature(clf_diabetes):\n    # Test PartialDependenceDisplay on categorical features\n\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n\n    clf = GradientBoostingClassifier(n_estimators=10, random_state=1)\n    clf.fit(X, y)\n\n    categorical_features = [0]  # we treat the first feature as categorical\n    feature_names = [\"categorical\", \"feature1\", \"feature2\", \"feature3\"]\n    pdp = PartialDependenceDisplay.from_estimator(\n        clf,\n        X,\n        [0],\n        kind=\"average\",\n        categorical_features=categorical_features,\n        feature_names=feature_names,\n    )\n\n    assert len(pdp.bars_) == 1\n    assert pdp.lines_.size == 0  # no lines for categorical features\n", "def test_PartialDependenceDisplay_from_estimator_categorical_features(\n    clf_diabetes, diabetes", "def test_PartialDependenceDisplay_categorical_feature(clf_diabetes, diabetes):\n    # Check that categorical features are plotted as bar plots\n    categorical_features = [1]\n    feature_names = [\"feature_{}\".format(i) for i in range(diabetes.data.shape[1])]\n    PartialDependenceDisplay.from_estimator(\n        clf_diabetes,\n        diabetes.data,\n        features=[1],\n        categorical_features=categorical_features,\n        feature_names=feature_names,\n    )\n", "def test_plot_partial_dependence_subsample(subsample, kind, clf_diabetes, diabetes):\n    # check that subsample is properly passed to the internal functions\n    PartialDependenceDisplay.from_estimator(\n        clf_diabetes, diabetes.data, features=[0], percentiles=(0.05, 0.95), kind=kind, subsample=subsample\n    )\n", "def test_plot_partial_dependence_categorical_feature(diabetes):\n    # make a categorical feature\n    data = np.copy(diabetes.data)\n    data[:, 0] = np.random.choice([0, 1], size=len(data))\n\n    clf = GradientBoostingRegressor(n_estimators=10, random_state=1)\n    clf.fit(data, diabetes.target)\n\n    # check with categorical_features as integer array\n    plot = PartialDependenceDisplay.from_estimator(\n        clf,\n        data,\n        [0],\n        categorical_features=[0],\n        feature_names=diabetes.feature_names,\n        kind=\"average\",\n    )\n\n    # check with categorical_features as boolean array\n    plot = PartialDependenceDisplay.from_estimator(\n        clf,\n        data,\n        [0],\n        categorical_features=[True] + [False] * (data.shape[1] - 1),\n        feature_names=diabetes.feature_names,\n        kind=\"average\",\n    )\n\n    # check with categorical_features as string array\n    plot = PartialDependenceDisplay.from_estimator(\n        clf,\n        data,\n        [\"sex\"],\n        categorical_features=[\"sex\"],\n        feature_names=diabetes.feature_names,\n        kind=\"average\",\n    )\n"], "sample_898": ["def test_auc_score_non_proba():\n    # Test AUC score for non-probability predictions\n    y_true = np.array([0, 1, 1, 0, 1, 0, 1, 1, 0, 1])\n    y_pred = np.array([0.1, 0.8, 0.4, 0.3, 0.9, 0.2, 0.7, 0.5, 0.1, 0.6])\n\n    auc = roc_auc_score(y_true, y_pred)\n    assert_greater(auc, 0.5)\n    assert_less_equal(auc, 1.)\n\n    # same result as using roc_curve\n    fpr, tpr, _ = roc_curve(y_true, y_pred)\n    expected_auc = auc(fpr, tpr)\n    assert_almost_equal(auc, expected_auc)\n", "def test_roc_auc_score_average_parameter():\n    # Test that roc_auc_score function handles correctly the average parameter.\n    y_true = np.array([0, 1, 1, 0, 1])\n    y_score = np.array([0.2, 0.8, 0.4, 0.3, 0.9])\n\n    # Test 'macro' and 'weighted' average for binary classification\n    scores = [\n        ('macro', roc_auc_score(y_true, y_score, average='macro')),\n        ('weighted', roc_auc_score(y_true, y_score, average='weighted'))\n    ]\n\n    for avg, score in scores:\n        assert_almost_equal(score, roc_auc_score(y_true, y_score, average='binary'))\n\n    # Test 'samples' average for binary classification\n    y_true = np.array([[0, 1], [1, 0]])\n    y_score = np.array([[0.2, 0.8], [0.7, 0.3]])\n\n    assert_almost_equal(roc_auc_score(y_true, y_score, average='samples'),\n                        1)\n\n    # Test 'micro' average for multi-class classification\n    y_true = np.array([0, 1, 2])\n    y_score = np.array([[0.9, 0.05, 0.05], [0.05, 0.9, 0.05], [0.05, 0.05, 0.9]])\n\n    assert_almost_equal(roc_auc_score(y_true, y_score, average='micro'),\n                        roc_auc_score(y_true, y_score, average='weighted'))\n", "def test_auc_score_non_increasing():\n    # Test whether auc correctly raises an error when x is non-increasing.\n    # Construct an example where the ROC curve has a decreasing x value.\n    y_true = np.array([0, 0, 1, 1])\n    y_score = np.array([0.2, 0.4, 0.3, 0.8])\n\n    assert_raise_message(ValueError,\n                         \"x is neither increasing nor decreasing : [0.2 0.3].\",\n                         roc_auc_score, y_true, y_score)\n", "def test_auc():\n    # Test Area Under Curve (AUC) computation\n    x = np.array([0, 0.5, 1.0])\n    y1 = np.array([0, 0.5, 1.0])\n    y2 = np.array([0, 0.75, 1.0])\n    auc1 = auc(x, y1)\n    auc2 = auc(x, y2)\n    assert_almost_equal(auc1, 0.5)\n    assert_almost_equal(auc2, 0.6875)\n\n    # Test AUC with decreasing x\n    x = np.array([1.0, 0.5, 0])\n    y1 = np.array([0, 0.5, 1.0])\n    y2 = np.array([0, 0.75, 1.0])\n    auc1 = auc(x, y1)\n    auc2 = auc(x, y2)\n    assert_almost_equal(auc1, 0.5)\n    assert_almost_equal(auc2, 0.6875)\n\n    # Test AUC with non-operating point\n    x = np.array([0, 0.5, 0.5, 1.0])\n    y1 = np.array([0, 0.25, 0.75, 1.0])\n    y2 = np.array([0, 0.5, 0.5, 1.0])\n    auc1 = auc(x, y1)\n    auc2 = auc(x, y2)\n    assert_almost_equal(auc1, 0.5625)\n    assert_almost_equal(auc2, 0.5)\n", "def test_average_precision_score_at_limiting_cases():\n    # Test average_precision_score for limiting cases\n    # average_precision_score should return 1 when all samples are true\n    y_true = np.array([1, 1, 1, 1])\n    y_score = np.array([1, 1, 1, 1])\n    assert_almost_equal(average_precision_score(y_true, y_score), 1.0)\n\n    # average_precision_score should return 0 when all samples are false\n    y_true = np.array([0, 0, 0, 0])\n    y_score = np.array([0, 0, 0, 0])\n    assert_almost_equal(average_precision_score(y_true, y_score), 0.0)\n"], "sample_899": ["def test_check_estimator_with_missing_fit_transform():\n    class EstimatorWithoutFitTransform(BaseEstimator):\n            return self\n\n            return X\n\n    msg = \"Estimator.* should implement a 'fit_transform' method\"\n    assert_raises_regex(ValueError, msg, check_estimator,\n                        EstimatorWithoutFitTransform())\n", "def test_check_estimators_overwrite_params():\n    # Test that estimator checks raise appropriate messages\n    # for overwrite of 'init' parameters.\n\n    class ModifyInitParams(BaseEstimator):\n            self.key = key\n\n            self.key = 'new_value'\n            return self\n\n    msg = \"Estimator ModifyInitParams should not change or mutate  the parameter\"\n    assert_raises_regex(AssertionError, msg,\n                        check_estimators_overwrite_params,\n                        \"ModifyInitParams\", ModifyInitParams())\n", "def test_check_estimator_sparse_data():\n    # Test that check_estimator works with sparse data and raises the correct\n    # errors for estimators that do not support sparse input.\n\n    class SparseEstimator(BaseEstimator):\n            return self\n\n            return np.zeros(X.shape[0])\n\n    class DenseEstimator(BaseEstimator):\n            if sp.issparse(X):\n                raise ValueError(\"Estimator does not support sparse input\")\n            return self\n\n            return np.zeros(X.shape[0])\n\n    sparse_X = sp.csr_matrix(np.random.rand(10, 5))\n    y = np.random.randint(2, size=10)\n\n    # Check that SparseEstimator passes the test\n    with ignore_warnings(category=DeprecationWarning):\n        check_estimator(SparseEstimator())\n\n    # Check that DenseEstimator raises the correct error\n    msg = (\"Estimator should raise an exception on sparse data, \"\n           \"but raised None instead\")\n    with assert_raises_regex(AssertionError, msg):\n        with ignore_warnings(category=DeprecationWarning):\n            check_estimator(DenseEstimator())\n", "def test_check_estimator_unfitted():\n    # test that we don't get a false negative\n    class Estimator:\n            return np.ones(X.shape[0])\n\n    msg = \"The estimator should raise an exception when not fitted\"\n    assert_raise_message((AttributeError, ValueError), msg,\n                         check_estimators_unfitted, \"test\", Estimator())\n", "def test_check_estimator_sparse_data():\n    # check that estimators raise an error on sparse data if\n    # not supported\n    class EstimatorWithoutSparseSupport(BaseEstimator):\n            return self\n\n            return np.zeros(X.shape[0])\n\n    msg = \"Estimator EstimatorWithoutSparseSupport should raise an error\"\n    assert_raises_regex(TypeError, msg, check_estimator_sparse_data,\n                        \"EstimatorWithoutSparseSupport\",\n                        EstimatorWithoutSparseSupport())\n"], "sample_900": ["def test_mlp_classifier_partial_fit():\n    # Test partial_fit on different datasets\n    for X, y in classification_datasets:\n        X_train, y_train = X[:150], y[:150]\n        X_test, y_test = X[150:], y[150:]\n\n        mlp = MLPClassifier(max_iter=1)\n        mlp.partial_fit(X_train, y_train, classes=np.unique(y))\n        assert mlp.score(X_test, y_test) > 0.5\n\n        mlp2 = MLPClassifier(max_iter=100)\n        mlp2.fit(X_train, y_train)\n        assert mlp2.score(X_test, y_test) > 0.8\n\n        # Compare the results of fit and partial_fit\n        mlp3 = MLPClassifier(max_iter=1)\n        for _ in range(100):\n            mlp3.partial_fit(X_train, y_train)\n        assert_almost_equal(mlp2.score(X_test, y_test), mlp3.score(X_test, y_test), decimal=2)\n", "def test_mlp_classifier_partial_fit():\n    # Test if partial_fit works as expected\n    X, y = classification_datasets[0]\n    clf = MLPClassifier(max_iter=1)\n    for _ in range(10):\n        clf.partial_fit(X, y, classes=np.unique(y))\n    y_pred = clf.predict(X)\n    assert np.mean(y_pred == y) >= 0.9\n", "def test_mlpclassifier_partial_fit():\n    # Test if partial_fit is working correctly by comparing the result\n    # with the model trained using fit method.\n\n    X, y = classification_datasets[0]\n\n    clf1 = MLPClassifier(max_iter=10, random_state=42)\n    clf1.fit(X, y)\n\n    clf2 = MLPClassifier(max_iter=5, warm_start=True, random_state=42)\n    for _ in range(2):\n        clf2.partial_fit(X, y)\n\n    assert_array_equal(clf1.coefs_, clf2.coefs_)\n    assert_array_equal(clf1.intercepts_, clf2.intercepts_)\n", "def test_mlp_classifier_partial_fit():\n    # Test if partial_fit gives the same result as fit for a small dataset\n    X, y = classification_datasets[0]\n    mlp = MLPClassifier(max_iter=1, warm_start=True, random_state=42)\n    mlp.fit(X, y)\n\n    mlp_partial_fit = MLPClassifier(max_iter=1, warm_start=True, random_state=42)\n    for _ in range(10):\n        mlp_partial_fit.partial_fit(X, y, classes=np.unique(y))\n\n    assert_almost_equal(mlp.predict_proba(X), mlp_partial_fit.predict_proba(X))\n", "def test_mlp_multiclass_partial_fit():\n    # Test partial fit with multi-class classification\n    X, y = make_multilabel_classification(n_samples=100, n_features=10,\n                                         n_classes=3, n_labels=2)\n    clf = MLPClassifier(max_iter=1)\n\n    with pytest.raises(ValueError):\n        clf.partial_fit(X, y, classes=None)\n\n    clf.partial_fit(X, y, classes=np.unique(y))\n    assert clf.classes_.shape == (3,)\n"], "sample_901": ["def test_kmeans_init_sparsity():\n    # check that kmeans initialization can handle sparse data properly\n    centers = np.array([\n        [0.0, 5.0, 0.0, 0.0, 0.0],\n        [1.0, 1.0, 4.0, 0.0, 0.0],\n        [1.0, 0.0, 0.0, 5.0, 1.0],\n    ])\n    X_csr = sp.csr_matrix(X)\n    km = KMeans(n_clusters=3, n_init=1, random_state=42, init='k-means++')\n    km.fit(X_csr)\n    assert_array_almost_equal(km.cluster_centers_, centers, decimal=2)\n", "def test_kmeans_init():\n    # Check that the initialization of KMeans is correct\n    rng = np.random.RandomState(42)\n    X = make_blobs(n_samples=100, centers=3, n_features=2,\n                   cluster_std=1., random_state=rng)[0]\n\n    km = KMeans(init='random', n_clusters=3, n_init=1, random_state=rng)\n    assert_array_almost_equal(km.fit(X).cluster_centers_, km.cluster_centers_)\n    km = KMeans(init='k-means++', n_clusters=3, n_init=1, random_state=rng)\n    assert_array_almost_equal(km.fit(X).cluster_centers_, km.cluster_centers_)\n    km = KMeans(init=X[:3], n_clusters=3, n_init=1, random_state=rng)\n    assert_array_almost_equal(km.fit(X).cluster_centers_, km.cluster_centers_)\n", "def test_kmeans_init_centers():\n    # Check if the number of clusters is less than or equal to the number of samples\n    assert_raises(ValueError, KMeans(n_clusters=n_samples + 1).fit, X)\n\n    # Check if the initial centers are correctly set when using init='random'\n    kmeans = KMeans(n_clusters=n_clusters, init='random', random_state=0)\n    kmeans.fit(X)\n    assert_array_equal(kmeans.cluster_centers_.shape, (n_clusters, n_features))\n\n    # Check if the initial centers are correctly set when using init=np.array\n    init_centers = np.random.rand(n_clusters, n_features)\n    kmeans = KMeans(n_clusters=n_clusters, init=init_centers)\n    kmeans.fit(X)\n    assert_array_equal(kmeans.cluster_centers_.shape, (n_clusters, n_features))\n", "def test_kmeans_init():\n    # Check that the init method of KMeans returns an instance of KMeans with\n    # the correct parameters set.\n    kmeans = KMeans(n_clusters=5, init='k-means++', random_state=0)\n    kmeans_init = kmeans._init_centroids(X, 5, 'k-means++', random_state=0)\n\n    assert_array_equal(kmeans_init.shape, (5, X.shape[1]))\n    assert_raises(ValueError, kmeans._init_centroids, X, 5, 'invalid_init')\n", "def test_kmeans_init_methods():\n    # Check that all init methods converge to the same solution\n    km_full = KMeans(init='k-means++', n_clusters=n_clusters,\n                     random_state=42).fit(X)\n\n    km_sample = KMeans(init='random', n_clusters=n_clusters,\n                       random_state=42).fit(X)\n\n    km_ndarray = KMeans(init=km_full.cluster_centers_, n_clusters=n_clusters,\n                        random_state=42).fit(X)\n\n    assert_array_almost_equal(km_full.cluster_centers_, km_sample.cluster_centers_)\n    assert_array_almost_equal(km_full.cluster_centers_, km_ndarray.cluster_centers_)\n"], "sample_902": ["def test_pipeline_raise_set_params_nested_estimator():\n    # test that setting parameter on nested estimator raises error\n    pipe = Pipeline([('svc', SVC())])\n    assert_raises_regex(ValueError, \"Pipeline\",\n                        pipe.set_params, svc__estimator='boo')\n", "def test_pipeline_fit_params():\n    # Test that Pipeline passes fit_params to the final estimator\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([0, 1])\n\n    fit_params = {'should_succeed': True}\n    pipe = make_pipeline(NoTransf(), FitParamT())\n    pipe.fit(X, y, **fit_params)\n    assert_true(pipe.steps[-1][-1].successful)\n\n    fit_params = {'should_succeed': False}\n    pipe = make_pipeline(NoTransf(), FitParamT())\n    pipe.fit(X, y, **fit_params)\n    assert_false(pipe.steps[-1][-1].successful)\n", "def test_pipeline_raise_set_params():\n    # Test pipeline raises error message when setting invalid parameters\n    pipe = make_pipeline(LogisticRegression())\n\n    # invalid parameters should raise an error message\n    assert_raises_regex(ValueError, \"Invalid parameter.*\", pipe.set_params,\n                        fake_param=True)\n    # non-existent step should raise an error message\n    assert_raises_regex(ValueError, \"Invalid parameter.*\", pipe.set_params,\n                        fake_step__param=True)\n\n    # valid step with invalid parameters should raise an error message\n    assert_raises_regex(ValueError, \"Invalid parameter.*\", pipe.set_params,\n                        logisticregression__fake_param=True)\n", "def test_pipeline_methods_raise_message():\n    # Test that methods raise the correct message when not supported by the\n    # last estimator\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([0, 1])\n\n    pipeline = make_pipeline(StandardScaler(), LogisticRegression())\n    pipeline.fit(X, y)\n\n    for method in ['predict_proba', 'decision_function']:\n        msg = (\"'{}' is not a valid method for this Pipeline object.\"\n               .format(method))\n        assert_raise_message(AttributeError, msg, getattr(pipeline, method), X)\n", "def test_pipeline_transform_inverse_transform():\n    # Test that piping two transformers and transforming data twice\n    # is equivalent to fitting a single pipeline with two transformers\n    X = np.array([[1, 2], [3, 4]])\n    p1 = Pipeline([('t1', Mult()), ('t2', Mult())])\n    p1.fit(X)\n    X_p1 = p1.transform(X)\n\n    t1 = Mult()\n    X_t1 = t1.fit_transform(X)\n    t2 = Mult()\n    X_t2 = t2.fit_transform(X_t1)\n\n    assert_array_equal(X_p1, X_t2)\n\n    # Test that inverse_transform works correctly\n    X_inv_p1 = p1.inverse_transform(X_p1)\n    X_inv_t2 = t2.inverse_transform(X_t2)\n    X_inv_t1 = t1.inverse_transform(X_inv_t2)\n    assert_array_equal(X_inv_p1, X_inv_t1)\n"], "sample_903": ["def test_tsne_trustworthiness():\n    # Create 2D dataset with 5 clusters\n    n_samples = 500\n    random_state = 0\n    np.random.seed(random_state)\n    centers = [[1, 1], [-1, -1], [1, -1], [-1, 1], [0, 0]]\n    X, _ = make_blobs(n_samples=n_samples, centers=centers, cluster_std=0.7,\n                      random_state=random_state)\n\n    # Compute trustworthiness for different n_components\n    for n_components in range(1, 6):\n        tsne = TSNE(n_components=n_components, random_state=random_state)\n        X_embedded = tsne.fit_transform(X)\n        trust = trustworthiness(X, X_embedded, n_neighbors=10)\n        assert_greater(trust, 0.85, \"Trustworthiness score should be greater \"\n                                     \"than 0.85 ({})\".format(trust))\n", "def test_kl_divergence_bh_angle():\n    # Test the KL divergence with Barnes-Hut approximation and varying angles.\n    n_samples = 100\n    n_features = 2\n    X = np.random.rand(n_samples, n_features)\n    P = pairwise_distances(X, squared=True)\n    P /= P.sum()\n    Y = np.random.rand(n_samples, n_features)\n\n    # Test that the KL divergence is approximated correctly for different angles.\n    for angle in [0.1, 0.5, 0.9]:\n        kl_div_approx = _kl_divergence_bh(Y.ravel(), P, 1, n_samples,\n                                          n_features, angle=angle,\n                                          skip_num_points=0, verbose=False,\n                                          compute_error=True)[0]\n        assert_greater(kl_div_approx, 0)\n", "def test_trustworthiness():\n    # Similar points should be neighbors\n    X = np.random.RandomState(0).randn(25, 2)\n    X[15:] += 5\n    X_tsne = TSNE(n_components=2, random_state=0).fit_transform(X)\n    assert_greater(trustworthiness(X, X_tsne), 0.7)\n    # Not similar points should not be neighbors\n    X_tsne = TSNE(n_components=2, random_state=0).fit_transform(X)\n    X_tsne[:10] += 10\n    assert_less(trustworthiness(X, X_tsne), 0.4)\n", "def test_trustworthiness():\n    # Create a simple dataset that can be embedded in 2D\n    X, _ = make_blobs(n_samples=100, n_features=3, centers=3,\n                      cluster_std=0.6, random_state=0)\n    \n    # Compute the trustworthiness for the original data\n    trust_original = trustworthiness(X, X, n_neighbors=5)\n\n    # Embed the data in 2D using t-SNE\n    tsne = TSNE(n_components=2, random_state=0)\n    X_embedded = tsne.fit_transform(X)\n\n    # Compute the trustworthiness for the embedded data\n    trust_embedded = trustworthiness(X, X_embedded, n_neighbors=5)\n\n    # The trustworthiness of the embedded data should be lower than the\n    # trustworthiness of the original data\n    assert_less(trust_embedded, trust_original)\n", "def test_early_exaggeration_too_low():\n    # Test that an error is thrown when early exaggeration is < 1.0\n    X, y = make_blobs(n_samples=100, random_state=42)\n    tsne = TSNE(early_exaggeration=0.99)\n    assert_raises(ValueError, tsne.fit_transform, X)\n"], "sample_904": ["def test_std_domain_term_role_xref(app):\n    app.add_object_type('confval', 'confval')\n    mock_inliner = mock.Mock()\n    result = StandardDomain._resolve_term_xref(\n        app.env, 'test', app.builder, 'term', 'test-term', pending_xref(), mock_inliner)\n    assert result is None\n\n    app.env.domains['std'].objects['term'] = {'test-term': ('test', 'term-test-term')}\n    result = StandardDomain._resolve_term_xref(\n        app.env, 'test', app.builder, 'term', 'test-term', pending_xref(), mock_inliner)\n    assert_node(result, addnodes.pending_xref)\n\n    # case insensitive test\n    result = StandardDomain._resolve_term_xref(\n        app.env, 'test', app.builder, 'term', 'TEST-TERM', pending_xref(), mock_inliner)\n    assert_node(result, addnodes.pending_xref)\n", "def test_resolve_xref_label():\n    app = mock.Mock()\n    env = mock.Mock()\n    env.domains = {'std': StandardDomain(env)}\n    builder = mock.Mock()\n    builder.name = 'html'\n    fromdocname = 'test'\n    docname = 'test2'\n    labelid = 'label1'\n    sectname = 'Section 1'\n    env.domains['std'].labels['label1'] = (docname, labelid, sectname)\n\n    node = pending_xref()\n    node['refexplicit'] = True\n    contnode = nodes.inline()\n    contnode['reftitle'] = 'Link title'\n\n    result = env.domains['std'].resolve_xref(env, fromdocname, builder, 'ref', 'label1', node, contnode)\n    assert_node(result, addnodes.pending_xref)\n    assert result.astext() == 'Link title'\n", "def test_resolve_xref_label(app):\n    env = app.builder.env\n    dom = StandardDomain(env)\n    dom.anonlabels['foo'] = ('bar', 'baz')\n    node = pending_xref('foo', reftype='ref', refdomain='std')\n    result = dom.resolve_xref(env, 'qux', app.builder, 'ref', 'foo', node, None)\n    assert isinstance(result, nodes.reference)\n    assert result.get('refuri') == 'bar.html#baz'\n", "def test_process_doc_with_anonymous_target(app):\n    \"\"\"\n    Test process_doc method with an anonymous target node.\n    \"\"\"\n    domain = StandardDomain(app.env)\n    docname = 'test'\n    document = nodes.document()\n    target_node = nodes.target(ids=['anonymous'])\n    document.append(target_node)\n\n    app.env.titles[docname] = 'Test Document'\n    app.env.docnames.add(docname)\n\n    with mock.patch.object(app.env, 'get_doctree') as get_doctree:\n        get_doctree.return_value = document\n        domain.process_doc(app.env, docname, document)\n\n    assert 'anonymous' in domain.anonlabels\n    assert domain.anonlabels['anonymous'] == (docname, 'anonymous')\n", "def test_build_reference_node():\n    env = mock.Mock()\n    builder = mock.Mock()\n    fromdocname = \"test\"\n    docname = \"test_doc\"\n    labelid = \"test_label\"\n    rolename = \"test_role\"\n\n    node = StandardDomain(env).build_reference_node(fromdocname, builder,\n                                                    docname, labelid, \"Test Title\", rolename)\n\n    assert isinstance(node, nodes.reference)\n    assert node[\"refuri\"] == builder.get_relative_uri.return_value + \"#\" + labelid\n    assert len(node) == 1\n    assert isinstance(node[0], nodes.inline)\n    assert node[0].astext() == \"Test Title\"\n    assert \"std\" in node[0].get(\"classes\")\n    assert f\"std-{rolename}\" in node[0].get(\"classes\")\n"], "sample_905": ["def test_signature_from_str():\n    sig = inspect.signature_from_str('(a, b, *, c, d=None)')\n    assert len(sig.parameters) == 4\n    assert sig.parameters['a'].kind == Parameter.POSITIONAL_OR_KEYWORD\n    assert sig.parameters['b'].kind == Parameter.POSITIONAL_OR_KEYWORD\n    assert sig.parameters['c'].kind == Parameter.KEYWORD_ONLY\n    assert sig.parameters['d'].kind == Parameter.KEYWORD_ONLY\n    assert sig.parameters['d'].default == 'None'\n", "def test_getorigbases():\n    class A:\n        pass\n\n    class B(A):\n        pass\n\n    assert inspect.getorigbases(A) is None\n    if sys.version_info >= (3, 7):\n        # __orig_bases__ is available since py37\n        assert inspect.getorigbases(B) == (A,)\n    else:\n        assert inspect.getorigbases(B) is None\n", "def test_isenumclass():\n    class EnumClass(enum.Enum):\n        pass\n\n    assert inspect.isenumclass(EnumClass)\n    assert not inspect.isenumclass(int)\n", "def test_safe_getattr():\n    class MockObject:\n            raise Exception(\"Attribute error\")\n\n            self.__dict__[\"__doc__\"] = \"Docstring\"\n\n    obj = MockObject()\n    assert inspect.safe_getattr(obj, \"__doc__\") == \"Docstring\"\n    with pytest.raises(AttributeError):\n        inspect.safe_getattr(obj, \"nonexistent\")\n", "def test_is_builtin_class_method():\n    assert inspect.is_builtin_class_method(int, '__init__')\n    assert not inspect.is_builtin_class_method(list, 'append')\n    assert inspect.is_builtin_class_method(datetime.datetime, '__init__')\n\n    class MyList(list):\n        pass\n\n    assert not inspect.is_builtin_class_method(MyList, '__init__')\n"], "sample_908": ["def test_unparse_arguments(source, expected):\n    node = ast.parse(source).body[0].args\n    assert ast.unparse_arguments(node) == expected\n", "def test_unparse_arguments(source, expected):\n    node = ast.parse(source).body[0].args\n    assert ast.unparse_arguments(node) == expected\n", "def test_unparse_arguments(source, expected):\n    node = ast.parse(source).body[0].args\n    assert ast.unparse_arguments(node) == expected\n", "def test_unparse_arguments(source, expected):\n    node = ast.parse(source).body[0].args\n    assert ast.unparse_arguments(node) == expected\n", "def test_unparse_arguments(source, expected):\n    node = ast.parse(source).body[0].args\n    assert ast.unparse_arguments(node) == expected\n"], "sample_909": ["    def test_attributes_section(self):\n        docstring = \"\"\"\\", "    def test_parse_numpydoc_see_also_section(self):\n        lines = [\n            \"func_name : Descriptive text\",\n            \"    continued text\",\n            \"another_func_name : Descriptive text\",\n            \"func_name1, func_name2, :meth:`func_name`, func_name3\"\n        ]\n        expected_lines = [\n            \"\",\n            \":obj:`func_name`\",\n            \"    Descriptive text continued text\",\n            \"\",\n            \":obj:`another_func_name`\",\n            \"    Descriptive text\",\n            \"\",\n            \":obj:`func_name1` , :obj:`func_name2` , :meth:`func_name` , :obj:`func_name3`\",\n            \"\"\n        ]\n        docstring = NumpyDocstring(lines, Config())\n        with mock.patch.object(docstring, '_consume_to_next_section', return_value=lines):\n            self.assertEqual(docstring._parse_see_also_section(\"See Also\"), \n                             docstring._format_admonition('seealso', expected_lines))\n", "    def test_docstring_with_multiple_colon_args(self):\n        docstring = cleandoc(\"\"\"\n                \\\"\\\"\\\"\n                Function description.\n\n                Args:\n                    arg1: Some number.\n                    arg2: Some string.\n\n                Returns:\n                    None\n                \\\"\\\"\\\"\n        \"\"\")\n        config = Config(napoleon_use_param=True)\n        actual = str(GoogleDocstring(docstring, config))\n        expected = cleandoc(\"\"\"\n            Function description.\n\n            :param arg1: Some number.\n            :type arg1: int\n            :param arg2: Some string.\n            :type arg2: str\n\n            :returns: None\n            :rtype: None\n        \"\"\")\n        self.assertEqual(actual, expected)\n", "    def test_attributes_with_class_reference(self):\n        lines = cleandoc(\"\"\"\n            Attributes\n            ----------\n            attr1 : int\n                Description of `attr1`\n            attr2 : str\n                Description of `attr2`\n            \"\"\")\n        config = Config(napoleon_use_ivar=True)\n        docstring = GoogleDocstring(lines, config=config)\n        expected = dedent(\"\"\"\n            :ivar ~NamedtupleSubclass.attr1: Description of `attr1`\n            :vartype ~NamedtupleSubclass.attr1: int\n\n            :ivar ~NamedtupleSubclass.attr2: Description of `attr2`\n            :vartype ~NamedtupleSubclass.attr2: str\n            \"\"\")\n        self.assertEqual(expected, str(docstring))\n", "    def test_numpydoc_see_also_section(self):\n        docstring = \"\"\"\\"], "sample_910": ["def test_SphinxLogRecordTranslator(app):\n    translator = logging.SphinxLogRecordTranslator(app)\n    record = logging.logging.LogRecord('name', logging.INFO, 'pathname', 1, 'message', None, None)\n    record.location = ('docname', 10)\n\n    assert translator.filter(record) is True\n    assert record.location == '%s:%s' % (app.env.doc2path('docname'), 10)\n", "def test_SphinxLogRecordTranslator():\n    class MockApp:\n            self.env = MockEnvironment()\n\n    class MockEnvironment:\n            return \"/path/to/\" + docname\n\n    app = MockApp()\n    translator = logging.SphinxLogRecordTranslator(app)\n\n    record = logging.LogRecord(\"name\", 10, \"/path/to/file.py\", 1, \"message\", None, None)\n    record.location = (\"docname\", 10)\n\n    translator.filter(record)\n    assert record.location == \"/path/to/docname:10\"\n\n    record = logging.LogRecord(\"name\", 10, \"/path/to/file.py\", 1, \"message\", None, None)\n    record.location = nodes.Node()\n\n    with pytest.raises(AttributeError):\n        translator.filter(record)\n", "def test_warningiserror_filter(app):\n    logger = logging.getLogger('sphinx')\n    filter_ = logging.WarningIsErrorFilter(app)\n    app.warningiserror = True\n\n    record = logging.LogRecord('name', logging.WARNING, 'pathname', 1, 'message', None, None)\n    with pytest.raises(SphinxWarning):\n        filter_.filter(record)\n\n    app.warningiserror = False\n    assert filter_.filter(record) is True\n", "def test_get_node_location():\n    node = nodes.Text(\"text\")\n    node.source = \"source\"\n    node.line = 10\n\n    location = logging.get_node_location(node)\n    assert location == \"source:10\"\n\n    node.line = None\n    location = logging.get_node_location(node)\n    assert location == \"source:\"\n\n    node.source = None\n    node.line = 10\n    location = logging.get_node_location(node)\n    assert location == \"<unknown>:10\"\n\n    node.line = None\n    location = logging.get_node_location(node)\n    assert location is None\n", "def test_SphinxLogRecordTranslator(tmpdir):\n    app = object()\n    translator = logging.SphinxLogRecordTranslator(app)\n\n    record = logging.SphinxWarningLogRecord('name', 10, 'pathname', 1, 'message', None, None)\n    record.location = ('docname', 10)\n    translator.filter(record)\n    assert record.location == '%s:%s' % (os.path.join(tmpdir.strpath, 'docname'), 10)\n\n    record = logging.SphinxWarningLogRecord('name', 10, 'pathname', 1, 'message', None, None)\n    record.location = nodes.Text()\n    translator.filter(record)\n    assert record.location is None\n\n    record = logging.SphinxWarningLogRecord('name', 10, 'pathname', 1, 'message', None, None)\n    record.location = 'docname'\n    translator.filter(record)\n    assert record.location == os.path.join(tmpdir.strpath, 'docname')\n"], "sample_238": ["    def test_abs(self):\n        result = Book.objects.annotate(abs_price=Abs('price')).get(isbn='159059725').abs_price\n        self.assertEqual(result, Decimal('30.00'))\n", "    def test_abs(self):\n        self.assertEqual(Book.objects.filter(rating__abs=4.5).count(), 1)\n", "def test_database_functions(self):\n    # Test that database functions are correctly registered and can be used in queries.\n    # We use a simple query to test each function, without checking the result.\n    from .functions import (\n        Abs, ACos, ASin, ATan, ATan2, Ceil, Cos, Cot, Degrees, Exp, Floor, Ln,\n        Log, Mod, Pi, Power, Radians, Random, Round, Sign, Sin, Sqrt, Tan,\n    )\n    for func in [\n        Abs(F('price')), ACos(F('rating')), ASin(F('rating')), ATan(F('rating')),\n        ATan2(F('price'), F('rating')), Ceil(F('price')), Cos(F('rating')),\n        Cot(F('rating')), Degrees(F('rating')), Exp(F('price')), Floor(F('price')),\n        Ln(F('price')), Log(F('price'), 2), Mod(F('price'), 3), Pi(),\n        Power(F('price'), 2), Radians(F('rating')), Random(), Round(F('price')),\n        Sign(F('price')), Sin(F('rating')), Sqrt(F('price')), Tan(F('rating')),\n    ]:\n        Book.objects.annotate(result=func).first()\n", "    def test_abs(self):\n        self.assertEqual(Book.objects.annotate(val=Abs(F('rating'))).get(id=self.b1.id).val, 4.5)\n        self.assertEqual(Book.objects.annotate(val=Abs(F('rating'))).get(id=self.b2.id).val, 3.0)\n", "    def test_abs(self):\n        self.assertEqual(Author.objects.filter(age__abs=-self.a1.age).count(), 1)\n"], "sample_912": ["def test_parse_annotation():\n    annotation = \"Union[int, str]\"\n    expected = [\n        addnodes.desc_sig_name(text='', fullname='Union'),\n        addnodes.desc_sig_punctuation(text='['),\n        addnodes.desc_sig_name(text='int', fullname='int'),\n        addnodes.desc_sig_punctuation(text=', '),\n        addnodes.desc_sig_name(text='str', fullname='str'),\n        addnodes.desc_sig_punctuation(text=']')\n    ]\n    result = _parse_annotation(annotation)\n    for i, node in enumerate(result):\n        assert_node(node, expected[i])\n", "def test_parse_annotation():\n    # Test simple annotation\n    annotation = \"str\"\n    expected = [addnodes.desc_sig_name(annotation, annotation)]\n    result = _parse_annotation(annotation)\n    assert_node(result, expected)\n\n    # Test annotation with dot notation\n    annotation = \"package.module.Class\"\n    expected = [addnodes.desc_sig_name(annotation, annotation)]\n    result = _parse_annotation(annotation)\n    assert_node(result, expected)\n\n    # Test complex annotation (e.g. List[int])\n    annotation = \"List[int]\"\n    expected = [\n        addnodes.desc_sig_name(\"List\", \"List\"),\n        addnodes.desc_sig_punctuation(\"[\", \"[\"),\n        addnodes.desc_sig_name(\"int\", \"int\"),\n        addnodes.desc_sig_punctuation(\"]\", \"]\"),\n    ]\n    result = _parse_annotation(annotation)\n    assert_node(result, expected)\n\n    # Test union type annotation (e.g. Union[str, int])\n    annotation = \"Union[str, int]\"\n    expected = [\n        addnodes.desc_sig_name(\"Union\", \"Union\"),\n        addnodes.desc_sig_punctuation(\"[\", \"[\"),\n        addnodes.desc_sig_name(\"str\", \"str\"),\n        addnodes.desc_sig_punctuation(\",\", \",\"),\n        addnodes.desc_sig_name(\"int\", \"int\"),\n        addnodes.desc_sig_punctuation(\"]\", \"]\"),\n    ]\n    result = _parse_annotation(annotation)\n    assert_node(result, expected)\n", "def test_parse_annotation():\n    annotation = \"Union[int, str]\"\n    expected = [\n        addnodes.desc_sig_name(annotation, annotation,\n                               addnodes.pending_xref(annotation, annotation,\n                                                     reftype='class',\n                                                     refdomain='py',\n                                                     reftarget=annotation)),\n    ]\n    result = _parse_annotation(annotation)\n    assert_node(result, expected)\n", "def test_parse_annotation():\n    # Test simple annotation\n    annotation = \"str\"\n    expected = [addnodes.desc_sig_name(annotation, annotation)]\n    result = _parse_annotation(annotation)\n    assert_node(result, expected)\n\n    # Test annotation with dot notation\n    annotation = \"package.module.Class\"\n    expected = [addnodes.desc_sig_name(annotation, annotation)]\n    result = _parse_annotation(annotation)\n    assert_node(result, expected)\n\n    # Test complex annotation (e.g. List[int])\n    annotation = \"List[int]\"\n    expected = [\n        addnodes.desc_sig_name(\"List\", \"List\"),\n        addnodes.desc_sig_punctuation(\"[\", \"[\"),\n        addnodes.desc_sig_name(\"int\", \"int\"),\n        addnodes.desc_sig_punctuation(\"]\", \"]\"),\n    ]\n    result = _parse_annotation(annotation)\n    assert_node(result, expected)\n\n    # Test union type annotation (e.g. Union[str, int])\n    annotation = \"Union[str, int]\"\n    expected = [\n        addnodes.desc_sig_name(\"Union\", \"Union\"),\n        addnodes.desc_sig_punctuation(\"[\", \"[\"),\n        addnodes.desc_sig_name(\"str\", \"str\"),\n        addnodes.desc_sig_punctuation(\",\", \",\"),\n        addnodes.desc_sig_name(\"int\", \"int\"),\n        addnodes.desc_sig_punctuation(\"]\", \"]\"),\n    ]\n    result = _parse_annotation(annotation)\n    assert_node(result, expected)\n", "def test_parse_annotation():\n    annotation = \"Union[int, str]\"\n    result = _parse_annotation(annotation)\n    assert len(result) == 1\n    assert_node(result[0], addnodes.pending_xref,\n                refdomain='py', reftype='class', reftarget='Union')\n    assert_node(result[0][0], nodes.Text, text='Union')\n    assert_node(result[0][1], addnodes.desc_sig_punctuation, text='[')\n    assert_node(result[0][2], addnodes.pending_xref,\n                refdomain='py', reftype='class', reftarget='int')\n    assert_node(result[0][2][0], nodes.Text, text='int')\n    assert_node(result[0][3], addnodes.desc_sig_punctuation, text=', ')\n    assert_node(result[0][4], addnodes.pending_xref,\n                refdomain='py', reftype='class', reftarget='str')\n    assert_node(result[0][4][0], nodes.Text, text='str')\n    assert_node(result[0][5], addnodes.desc_sig_punctuation, text=']')\n"], "sample_914": ["def test_unparse_arguments(source, expected):\n    node = ast.parse(source).body[0].args\n    assert ast.unparse_arguments(node) == expected\n", "def test_unparse_arguments(source, expected):\n    node = ast.parse(source).body[0].value  # type: ignore\n    assert ast.unparse_arguments(node.args) == expected\n", "def test_unparse_arguments(source, expected):\n    node = ast.parse(source).body[0].args\n    assert ast.unparse_arguments(node) == expected\n", "def test_unparse_arguments(source, expected):\n    node = ast.parse(source).body[0].args\n    assert ast.unparse_arguments(node) == expected\n", "def test_unparse_arguments(source, expected):\n    node = ast.parse(source).body[0].args\n    assert ast.unparse_arguments(node) == expected\n"], "sample_915": ["def test_is_builtin_class_method():\n    assert inspect.is_builtin_class_method(int, '__init__')\n    assert not inspect.is_builtin_class_method(datetime.datetime, '__init__')\n", "def test_is_builtin_class_method():\n    assert inspect.is_builtin_class_method(int, '__init__')\n    assert inspect.is_builtin_class_method(str, '__init__')\n    assert not inspect.is_builtin_class_method(datetime.datetime, '__init__')\n", "def test_isabstractmethod():\n    class A(metaclass=types.new_class('Meta', (type,), {'__abstractmethods__': frozenset()})):\n        pass\n\n    assert inspect.isabstractmethod(A)\n    assert not inspect.isabstractmethod(datetime.datetime)\n", "def test_getfullargspec():\n        pass\n\n    argspec = inspect.getfullargspec(foo)\n    assert argspec.args == ['a', 'b', 'c']\n    assert argspec.varargs == 'args'\n    assert argspec.varkw == 'kwargs'\n    assert argspec.defaults == (None,)\n    assert argspec.kwonlyargs == []\n    assert argspec.kwonlydefaults is None\n    assert argspec.annotations == {}\n\n    # Test partial function\n    partial_foo = functools.partial(foo, 1, 2)\n    argspec = inspect.getfullargspec(partial_foo)\n    assert argspec.args == ['c']\n    assert argspec.varargs == 'args'\n    assert argspec.varkw == 'kwargs'\n    assert argspec.defaults == (None,)\n    assert argspec.kwonlyargs == []\n    assert argspec.kwonlydefaults is None\n    assert argspec.annotations == {}\n", "def test_getfullargspec():\n    # Test that getfullargspec works for functions with annotations and default values\n        pass\n\n    spec = inspect.getfullargspec(test_func)\n    assert spec.args == ['a', 'b']\n    assert spec.varargs is None\n    assert spec.varkw is None\n    assert spec.defaults == ('hello',)\n    assert spec.kwonlyargs == []\n    assert spec.kwonlydefaults is None\n    assert spec.annotations == {'a': int, 'b': str, 'return': type(None)}\n"], "sample_916": ["def test_id_attributes():\n    input = \"void myFunction() id_attr;\"\n    ast = parse('function', input)\n    assert len(ast.declaration.attributes) == 1\n    assert ast.declaration.attributes[0].name == \"id_attr\"\n    assert ast.declaration.attributes[0].args is None\n\n    input = \"void myFunction() paren_attr(x, y);\"\n    ast = parse('function', input)\n    assert len(ast.declaration.attributes) == 1\n    assert ast.declaration.attributes[0].name == \"paren_attr\"\n    assert ast.declaration.attributes[0].args is not None\n    assert str(ast.declaration.attributes[0].args) == \"(x, y)\"\n", "def test_fundamental_type():\n    check(\"type\", \"int\", {1: \"cpp-integer\", 2: \"cpp-integer-7\"})\n    check(\"type\", \"unsigned int\", {1: \"cpp-unsigned-integer\",\n                                   2: \"cpp-unsigned-integer-7\"})\n    check(\"type\", \"__int64\", {1: \"cpp-__int64\", 2: \"cpp-__int64-7\"})\n\n    # We need to support 'signed' and 'unsigned'\n    # as stand-alone types for backwards compatibility.\n    check(\"type\", \"signed\", {1: \"cpp-signed-integer\",\n                             2: \"cpp-signed-integer-7\"})\n    check(\"type\", \"unsigned\", {1: \"cpp-unsigned-integer\",\n                               2: \"cpp-unsigned-integer-7\"})\n    # But this shouldn't work in combination with other types.\n    with pytest.raises(DefinitionError):\n        parse(\"type\", \"unsigned foo\")\n    with pytest.raises(DefinitionError):\n        parse(\"type\", \"signed foo\")\n", "def test_nested_name():\n    # check without template args\n    input = \"A::B::C\"\n    ast = cppDomain.ASTNestedName([cppDomain.ASTIdentifier(\"A\"),\n                                   cppDomain.ASTIdentifier(\"B\"),\n                                   cppDomain.ASTIdentifier(\"C\")],\n                                  rooted=False)\n    parser = DefinitionParser(input, location=None, config=None)\n    res = parser._parse_nested_name()\n    assert str(res) == input\n\n    # check with template args\n    input = \"::A<B>::C<D>::E\"\n    ast = cppDomain.ASTNestedName([cppDomain.ASTIdentifier(\"A\"),\n                                   cppDomain.ASTIdentifier(\"C\"),\n                                   cppDomain.ASTIdentifier(\"E\")],\n                                  rooted=True)\n    parser = DefinitionParser(input, location=None, config=None)\n    res = parser._parse_nested_name()\n    assert str(res) == input.replace(\"<B>\", \"\").replace(\"<D>\", \"\")\n", "def test_anon_enum():\n    check(\"enum\", \"enum { A }\", {\n        1: 'cpp-enumeration-TestDoc-',\n        2: 'cpp-enumeration-TestDoc-'\n    }, \"enum { A }\")\n", "def test_type_with_trailing():\n    check(\"type\",\n          \"int i\",\n          {1: \"cpp-type\",\n           2: \"cpp-type::i\"},\n          \"int i\")\n\n    check(\"type\",\n          \"int & i\",\n          {1: \"cpp-type\",\n           2: \"cpp-type::i\"})\n\n    check(\"type\",\n          \"int && i\",\n          {1: \"cpp-type\",\n           2: \"cpp-type::i\"})\n\n    check(\"type\",\n          \"int* i\",\n          {1: \"cpp-type\",\n           2: \"cpp-type::i\"})\n\n    check(\"type\",\n          \"const int i\",\n          {1: \"cpp-type\",\n           2: \"cpp-type::i\"})\n\n    check(\"type\",\n          \"int const i\",\n          {1: \"cpp-type\",\n           2: \"cpp-type::i\"})\n\n    check(\"type\",\n          \"int volatile i\",\n          {1: \"cpp-type\",\n           2: \"cpp-type::i\"})\n\n    check(\"type\",\n          \"int* volatile i\",\n          {1: \"cpp-type\",\n           2: \"cpp-type::i\"})\n\n    check(\"type\",\n          \"int* const i\",\n          {1: \"cpp-type\",\n           2: \"cpp-type::i\"})\n\n    check(\"type\",\n          \"int* const volatile i\",\n          {1: \"cpp-type\",\n           2: \"cpp-type::i\"})\n"], "sample_918": ["def test_parse_annotation():\n    annotation = \"Union[str, bytes]\"\n    expected = [\n        addnodes.desc_sig_name('', 'Union'),\n        addnodes.desc_sig_punctuation('', '['),\n        addnodes.desc_sig_name('', 'str'),\n        addnodes.desc_sig_punctuation('', ', '),\n        addnodes.desc_sig_name('', 'bytes'),\n        addnodes.desc_sig_punctuation('', ']')\n    ]\n    result = _parse_annotation(annotation)\n    assert_node(result, expected)\n\n    annotation = \"Optional[str]\"\n    expected = [\n        addnodes.desc_sig_name('', 'Optional'),\n        addnodes.desc_sig_punctuation('', '['),\n        addnodes.desc_sig_name('', 'str'),\n        addnodes.desc_sig_punctuation('', ']')\n    ]\n    result = _parse_annotation(annotation)\n    assert_node(result, expected)\n\n    annotation = \"Tuple[str, ...]\"\n    expected = [\n        addnodes.desc_sig_name('', 'Tuple'),\n        addnodes.desc_sig_punctuation('', '['),\n        addnodes.desc_sig_name('', 'str'),\n        addnodes.desc_sig_punctuation('', ', '),\n        addnodes.desc_sig_punctuation('', '...'),\n        addnodes.desc_sig_punctuation('', ']')\n    ]\n    result = _parse_annotation(annotation)\n    assert_node(result, expected)\n", "def test_parse_annotation():\n    # Test parsing of type annotations\n    annotation = \"Union[str, int]\"\n    parsed_annotation = _parse_annotation(annotation)\n    assert len(parsed_annotation) == 1\n    assert isinstance(parsed_annotation[0], addnodes.pending_xref)\n    assert parsed_annotation[0].astext() == \"Union\"\n\n    annotation = \"List[Tuple[str, int]]\"\n    parsed_annotation = _parse_annotation(annotation)\n    assert len(parsed_annotation) == 3\n    assert isinstance(parsed_annotation[0], addnodes.pending_xref)\n    assert parsed_annotation[0].astext() == \"List\"\n    assert isinstance(parsed_annotation[1], nodes.Text)\n    assert parsed_annotation[1].astext() == \"[\"\n    assert isinstance(parsed_annotation[2], addnodes.desc_sig_name)\n    assert parsed_annotation[2].astext() == \"Tuple[str, int]\"\n", "def test_parse_annotation():\n    # test parsing a simple annotation\n    annotation = \"int\"\n    result = _parse_annotation(annotation)\n    assert len(result) == 1\n    assert isinstance(result[0], nodes.Text)\n    assert result[0].astext() == annotation\n\n    # test parsing an annotation with module name\n    annotation = \"sphinx.addnodes.Node\"\n    result = _parse_annotation(annotation)\n    assert len(result) == 1\n    assert isinstance(result[0], pending_xref)\n    assert result[0].astext() == annotation\n\n    # test parsing an annotation with a type from the typing module\n    annotation = \"typing.Union[int, str]\"\n    result = _parse_annotation(annotation)\n    assert len(result) == 7\n    assert isinstance(result[0], nodes.Text)\n    assert result[0].astext() == \"typing.\"\n    assert isinstance(result[1], pending_xref)\n    assert result[1].astext() == \"Union\"\n    assert isinstance(result[2], desc_sig_punctuation)\n    assert result[2].astext() == \"[\"\n    assert isinstance(result[3], pending_xref)\n    assert result[3].astext() == \"int\"\n    assert isinstance(result[4], desc_sig_punctuation)\n    assert result[4].astext() == \",\"\n    assert isinstance(result[5], pending_xref)\n    assert result[5].astext() == \"str\"\n    assert isinstance(result[6], desc_sig_punctuation)\n    assert result[6].astext() == \"]\"\n\n    # test parsing an annotation with multiple types\n    annotation = \"int or str\"\n    result = _parse_annotation(annotation)\n    assert len(result) == 5\n    assert isinstance(result[0], nodes.Text)\n    assert result[0].astext() == \"int\"\n    assert isinstance(result[1], nodes.Text)\n    assert result[1].astext() == \" \"\n    assert isinstance(result[2], nodes.Text)\n    assert result[2].astext() == \"or\"\n    assert isinstance(result[3], nodes.Text)\n    assert result[3].astext() == \" \"\n    assert isinstance(result[4], nodes.Text)\n    assert result[4].astext() == \"str\"\n", "def test_parse_annotation():\n    # simple annotation\n    text = 'int'\n    expected = [addnodes.desc_sig_name('', 'int')]\n    result = _parse_annotation(text)\n    assert_node(result, expected)\n\n    # more complex annotation\n    text = 'Union[int, str]'\n    expected = [\n        addnodes.desc_sig_name('', 'Union'),\n        addnodes.desc_sig_punctuation('', '['),\n        addnodes.desc_sig_name('', 'int'),\n        addnodes.desc_sig_punctuation('', ', '),\n        addnodes.desc_sig_name('', 'str'),\n        addnodes.desc_sig_punctuation('', ']')\n    ]\n    result = _parse_annotation(text)\n    assert_node(result, expected)\n\n    # annotation with spaces\n    text = ' List[ int ,  str ] '\n    expected = [\n        addnodes.desc_sig_name('', 'List'),\n        addnodes.desc_sig_punctuation('', '['),\n        addnodes.desc_sig_name('', 'int'),\n        addnodes.desc_sig_punctuation('', ', '),\n        addnodes.desc_sig_name('', 'str'),\n        addnodes.desc_sig_punctuation('', ']')\n    ]\n    result = _parse_annotation(text)\n    assert_node(result, expected)\n\n    # invalid annotation\n    text = ' invalid ( annotation ) '\n    with pytest.raises(SyntaxError):\n        _parse_annotation(text)\n", "def test_parse_annotation():\n    annotation = \"Union[str, int]\"\n    expected = [addnodes.desc_sig_name(annotation, annotation)]\n    result = _parse_annotation(annotation)\n    assert len(result) == 1\n    assert_node(result[0], expected[0])\n"], "sample_920": ["    def test_attributes_section(self):\n        config = Config(napoleon_use_ivar=True)\n        docstring = \"\"\"\\", "    def test_attributes_with_class_reference(self):\n        lines = cleandoc(\"\"\"\n            Attributes:\n                attr1 (samplemodule.SampleClass): Description of `attr1`\n        \"\"\")\n        expected = dedent(\"\"\"\n            .. attribute:: attr1\n\n               Description of `attr1`\n               :type: samplemodule.SampleClass\n        \"\"\")\n        config = Config()\n        actual = str(GoogleDocstring(lines, config))\n        self.assertEqual(expected.rstrip(), actual)\n", "    def test_numpydoc_see_also_section(self):\n        docstring = \"\"\"\\", "    def test_attributes_with_trailing_colon(self):\n        config = Config(napoleon_use_ivar=True)\n        docstring = \"\"\"\\", "    def test_google_docstring(self):\n        docstring = \"\"\"\\"], "sample_921": ["def test_signature_from_str():\n    sig = inspect.signature_from_str(\"(a: int, b: str) -> None\")\n    assert isinstance(sig, inspect.Signature)\n    assert len(sig.parameters) == 2\n    assert sig.parameters[\"a\"].annotation == int\n    assert sig.parameters[\"b\"].annotation == str\n    assert sig.return_annotation == type(None)\n\n    sig = inspect.signature_from_str(\"(a, b: str = 'hello') -> int\")\n    assert isinstance(sig, inspect.Signature)\n    assert len(sig.parameters) == 2\n    assert sig.parameters[\"a\"].annotation == Parameter.empty\n    assert sig.parameters[\"b\"].annotation == str\n    assert sig.parameters[\"b\"].default == \"hello\"\n    assert sig.return_annotation == int\n\n    sig = inspect.signature_from_str(\"(a, *args, **kwargs) -> None\")\n    assert isinstance(sig, inspect.Signature)\n    assert len(sig.parameters) == 3\n    assert sig.parameters[\"a\"].annotation == Parameter.empty\n    assert sig.parameters[\"args\"].kind == Parameter.VAR_POSITIONAL\n    assert sig.parameters[\"kwargs\"].kind == Parameter.VAR_KEYWORD\n    assert sig.return_annotation == type(None)\n", "def test_signature_from_str():\n    sig = inspect.signature_from_str(\"(a, b, *, c, d=None) -> int\")\n    assert len(sig.parameters) == 4\n    assert sig.return_annotation == int\n    assert sig.parameters[\"a\"].kind == Parameter.POSITIONAL_OR_KEYWORD\n    assert sig.parameters[\"b\"].kind == Parameter.POSITIONAL_OR_KEYWORD\n    assert sig.parameters[\"c\"].kind == Parameter.KEYWORD_ONLY\n    assert sig.parameters[\"d\"].kind == Parameter.KEYWORD_ONLY\n    assert sig.parameters[\"d\"].default == None\n\n    # Test with no return annotation\n    sig = inspect.signature_from_str(\"(a, b, *, c, d=None)\")\n    assert sig.return_annotation == Parameter.empty\n\n    # Test with posonlyargs\n    sig = inspect.signature_from_str(\"(a, b, /, c, d=None) -> int\")\n    assert len(sig.parameters) == 4\n    assert sig.return_annotation == int\n    assert sig.parameters[\"a\"].kind == Parameter.POSITIONAL_ONLY\n    assert sig.parameters[\"b\"].kind == Parameter.POSITIONAL_ONLY\n    assert sig.parameters[\"c\"].kind == Parameter.POSITIONAL_OR_KEYWORD\n    assert sig.parameters[\"d\"].kind == Parameter.POSITIONAL_OR_KEYWORD\n    assert sig.parameters[\"d\"].default == None\n", "def test_is_builtin_class_method():\n    assert is_builtin_class_method(int, '__init__') is True\n    assert is_builtin_class_method(str, '__init__') is True\n    assert is_builtin_class_method(datetime.datetime, '__init__') is False\n", "def test_getargspec_bound_method():\n    class MyClass:\n            pass\n\n    obj = MyClass()\n    method = obj.my_method\n\n    argspec = inspect.getargspec(method)\n    assert argspec.args == ['arg1', 'arg2']\n    assert argspec.varargs is None\n    assert argspec.varkw is None\n    assert argspec.defaults is None\n    assert argspec.kwonlyargs == []\n    assert argspec.kwdefaults is None\n    assert argspec.annotations == {}\n", "def test_is_builtin_class_method():\n    assert is_builtin_class_method(int, '__init__')\n    assert is_builtin_class_method(str, '__init__')\n    assert not is_builtin_class_method(datetime.datetime, '__init__')\n"], "sample_922": ["def test_parse_annotation():\n    annotation = \"Union[int, str]\"\n    expected = [\n        addnodes.desc_sig_name('', 'Union'),\n        addnodes.desc_sig_punctuation('', '['),\n        addnodes.desc_sig_name('', 'int'),\n        addnodes.desc_sig_punctuation('', ', '),\n        addnodes.desc_sig_name('', 'str'),\n        addnodes.desc_sig_punctuation('', ']')\n    ]\n\n    result = _parse_annotation(annotation)\n    assert_node(result, expected)\n", "def test_PythonDomain_find_obj():\n    env = Mock()\n    domain = PythonDomain(env)\n    domain.objects = {\n        'module.function': ('module', 'function'),\n        'module.Class': ('module', 'Class'),\n        'module.Class.method': ('module', 'method'),\n        'function': ('', 'function'),\n        'Class': ('', 'Class'),\n        'Class.method': ('', 'method'),\n    }\n\n    # Test searching for a function\n    matches = domain.find_obj(env, None, None, 'function', None, 1)\n    assert len(matches) == 2\n    assert matches[0][0] == 'function'\n    assert matches[1][0] == 'module.function'\n\n    # Test searching for a class\n    matches = domain.find_obj(env, None, None, 'Class', None, 1)\n    assert len(matches) == 2\n    assert matches[0][0] == 'Class'\n    assert matches[1][0] == 'module.Class'\n\n    # Test searching for a method\n    matches = domain.find_obj(env, None, None, 'method', None, 1)\n    assert len(matches) == 2\n    assert matches[0][0] == 'Class.method'\n    assert matches[1][0] == 'module.Class.method'\n", "def test_parse_annotation():\n    # type: () -> None\n    ann = _parse_annotation(\"Union[int, str]\")\n    assert len(ann) == 5\n    assert_node(ann[0], addnodes.desc_sig_name, 'Union')\n    assert_node(ann[1], addnodes.desc_sig_punctuation, '[')\n    assert_node(ann[2], addnodes.pending_xref, '[int]')\n    assert_node(ann[3], addnodes.pending_xref, '[str]')\n    assert_node(ann[4], addnodes.desc_sig_punctuation, ']')\n\n    ann = _parse_annotation(\"Tuple[Union[int, str], ...]\")\n    assert len(ann) == 8\n    assert_node(ann[0], addnodes.desc_sig_name, 'Tuple')\n    assert_node(ann[1], addnodes.desc_sig_punctuation, '[')\n    assert_node(ann[2], addnodes.desc_sig_name, 'Union')\n    assert_node(ann[3], addnodes.desc_sig_punctuation, '[')\n    assert_node(ann[4], addnodes.pending_xref, '[int]')\n    assert_node(ann[5], addnodes.pending_xref, '[str]')\n    assert_node(ann[6], addnodes.desc_sig_punctuation, '], ...')\n    assert_node(ann[7], addnodes.desc_sig_punctuation, ']')\n\n    ann = _parse_annotation(\"Dict[str, List[int]]\")\n    assert len(ann) == 7\n    assert_node(ann[0], addnodes.desc_sig_name, 'Dict')\n    assert_node(ann[1], addnodes.desc_sig_punctuation, '[')\n    assert_node(ann[2], addnodes.pending_xref, '[str]')\n    assert_node(ann[3], addnodes.desc_sig_punctuation, ', ')\n    assert_node(ann[4], addnodes.desc_sig_name, 'List')\n    assert_node(ann[5], addnodes.pending_xref, '[int]')\n    assert_node(ann[6], addnodes.desc_sig_punctuation, ']')\n", "def test_parse_annotation():\n    annotations = [\n        ('int', ['int']),\n        ('a.b.c', ['a', '.', 'b', '.', 'c']),\n        ('a[b]', ['a', '[', 'b', ']']),\n        ('a[b, c]', ['a', '[', 'b', ', ', 'c', ']']),\n        ('a(b)', ['a', '(', 'b', ')']),\n        ('a(b, c)', ['a', '(', 'b', ', ', 'c', ')']),\n        ('a[b](c, d)', ['a', '[', 'b', ']', '(', 'c', ', ', 'd', ')']),\n        ('a[b, c](d, e)', ['a', '[', 'b', ', ', 'c', ']', '(', 'd', ', ', 'e', ')']),\n        ('None', ['None']),\n    ]\n\n    for annotation, expected in annotations:\n        result = _parse_annotation(annotation)\n        assert len(result) == len(expected)\n        for node, text in zip(result, expected):\n            if isinstance(node, nodes.Text):\n                assert node.astext() == text\n            else:\n                assert isinstance(node, addnodes.desc_sig_punctuation)\n                assert node.astext() == text\n", "def test_parse_annotation():\n    annotation = \"Union[str, int]\"\n    expected = [\n        addnodes.desc_sig_name('', 'Union'),\n        addnodes.desc_sig_punctuation('', '['),\n        addnodes.desc_sig_name('', 'str'),\n        addnodes.desc_sig_punctuation('', ', '),\n        addnodes.desc_sig_name('', 'int'),\n        addnodes.desc_sig_punctuation('', ']'),\n    ]\n    result = _parse_annotation(annotation)\n    for i, node in enumerate(result):\n        assert_node(node, expected[i])\n"], "sample_925": ["def test_mock_object_repr():\n    mock_obj = _MockObject()\n    assert repr(mock_obj) == '_MockObject'\n    subclass = _make_subclass('SubClass', '_MockObject', _MockObject)\n    subclass_obj = subclass()\n    assert repr(subclass_obj) == '_MockObject.SubClass'\n", "def test_mock_module():\n    with mock(['test.module']):\n        module = import_module('test.module')\n        assert isinstance(module, _MockModule)\n        assert module.__name__ == 'test.module'\n        assert module.__all__ == []\n        assert module.__path__ == []\n\n        # Test getattr\n        submodule = module.submodule\n        assert isinstance(submodule, _MockObject)\n        assert submodule.__display_name__ == 'test.module.submodule'\n", "def test_mock_object_repr():\n    mock_obj = _MockObject()\n    assert repr(mock_obj) == _MockObject.__display_name__\n\n    subclass_mock_obj = _make_subclass('Subclass', 'module', superclass=_MockObject)\n    assert repr(subclass_mock_obj()) == 'module.Subclass'\n", "def test_MockObject_repr():\n    obj = _MockObject()\n    assert repr(obj) == '_MockObject'\n", "def test_mock_module():\n    with mock(['sphinx']):\n        sphinx = import_module('sphinx')\n        assert isinstance(sphinx, _MockModule)\n        assert sphinx.__name__ == 'sphinx'\n        assert sphinx.__all__ == []\n        assert sphinx.__path__ == []\n\n        # Test getattr on mock module\n        mock_object = getattr(sphinx, 'mock_object')\n        assert isinstance(mock_object, _MockObject)\n        assert mock_object.__display_name__ == 'sphinx.mock_object'\n\n        # Test repr on mock module\n        assert repr(sphinx) == 'sphinx'\n"], "sample_928": ["def test_textwidth():\n    assert textwidth('Hello, World!', 'WF') == 13\n    assert textwidth('', 'WF') == 2  # East Asian wide character\n    assert textwidth('', 'WFA') == 2  # Ambiguous character in Japanese\n", "def test_default_role():\n    with default_role(\"test_doc\", \"test_role\"):\n        pass  # default role is registered and then unregistered\n    # check that the role is no longer registered\n    from sphinx.util.docutils import unregister_role\n    try:\n        unregister_role('')\n        assert False, \"default role should be already unregistered\"\n    except KeyError:\n        pass\n\n    # test with unknown role\n    logger = logging.getLogger(__name__)\n    with logging.captureLogs(logger) as logs:\n        with default_role(\"test_doc\", \"unknown_role\"):\n            pass\n        assert len(logs) == 1\n        assert \"WARNING\" in logs[0]\n        assert \"default role unknown_role not found\" in logs[0]\n", "def test_escape():\n    assert escape(\"Hello World\") == \"Hello World\"\n    assert escape(\"Hello, World!\") == r\"Hello\\, World\\!\"\n    assert escape(\".Hello World\") == r\"\\.Hello World\"\n    assert escape(\"Hello... World\") == r\"Hello\\.\\.\\. World\"\n", "def test_default_role():\n    docname = 'test'\n    name = 'test_role'\n\n    with default_role(docname, name) as _:\n        pass  # default role is registered here\n\n    # check if the role is unregistered after context exit\n    assert docutils.unregister_role('') is None\n", "def test_default_role():\n    with default_role('test', 'default') as _:\n        pass\n    # check if the role is registered and then unregistered\n    assert docutils.roles._roles.get('') is None\n\n    # check warning for unknown role\n    logger = logging.getLogger(__name__)\n    with logging.captureLogs(logger) as logs:\n        with default_role('test', 'unknown'):\n            pass\n    assert len(logs) == 1\n    assert logs[0].levelname == 'WARNING'\n    assert 'default role unknown not found' in logs[0].getMessage()\n"], "sample_929": ["def test_parse_annotation():\n    env = Mock()\n    result = _parse_annotation(\"int\", env)\n    assert len(result) == 1\n    assert isinstance(result[0], pending_xref)\n\n    result = _parse_annotation(\"Union[int, str]\", env)\n    assert len(result) == 3\n    assert isinstance(result[0], nodes.Text)\n    assert isinstance(result[1], pending_xref)\n    assert isinstance(result[2], pending_xref)\n\n    result = _parse_annotation(\"Dict[str, List[int]]\", env)\n    assert len(result) == 4\n    assert isinstance(result[0], pending_xref)\n    assert isinstance(result[1], pending_xref)\n    assert isinstance(result[2], pending_xref)\n    assert isinstance(result[3], pending_xref)\n", "def test_parse_annotation():\n    env = Mock()\n    env.ref_context = {}\n\n    # Simple annotation\n    annotation = \"int\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 1\n    assert isinstance(result[0], pending_xref)\n\n    # Annotation with module name\n    annotation = \"typing.Union[int, str]\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 1\n    assert isinstance(result[0], pending_xref)\n\n    # Annotation with nested types\n    annotation = \"typing.List[typing.Tuple[int, str]]\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) > 1\n    for node in result:\n        if isinstance(node, pending_xref):\n            assert node.astext() in ['List', 'Tuple', 'int', 'str']\n        else:\n            assert isinstance(node, nodes.Text)\n            assert node.astext() in ['[', ']', ', ', '(', ')']\n\n    # Test that invalid annotations are handled correctly\n    annotation = \"(\"  # Invalid syntax\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 1\n    assert isinstance(result[0], type_to_xref(annotation, env))\n", "def test_parse_annotation():\n    env = Mock()\n    env.ref_context = {}\n\n    # Test simple annotation\n    annotation = \"int\"\n    expected = [addnodes.pending_xref('', nodes.Text(\"int\"), refdomain='py', reftype='class', reftarget='int')]\n    assert _parse_annotation(annotation, env) == expected\n\n    # Test complex annotation with nested types\n    annotation = \"List[Tuple[int, str]]\"\n    expected = [\n        addnodes.pending_xref('', nodes.Text(\"List\"), refdomain='py', reftype='class', reftarget='List'),\n        addnodes.desc_sig_punctuation('', '['),\n        addnodes.pending_xref('', nodes.Text(\"Tuple\"), refdomain='py', reftype='class', reftarget='Tuple'),\n        addnodes.desc_sig_punctuation('', '['),\n        addnodes.pending_xref('', nodes.Text(\"int\"), refdomain='py', reftype='class', reftarget='int'),\n        addnodes.desc_sig_punctuation('', ', '),\n        addnodes.pending_xref('', nodes.Text(\"str\"), refdomain='py', reftype='class', reftarget='str'),\n        addnodes.desc_sig_punctuation('', ']'),\n        addnodes.desc_sig_punctuation('', ']')\n    ]\n    assert _parse_annotation(annotation, env) == expected\n", "def test_parse_annotation():\n    env = Mock()\n    annotation = 'Union[str, int]'\n    parsed_annotation = _parse_annotation(annotation, env)\n    assert len(parsed_annotation) == 1\n    assert isinstance(parsed_annotation[0], pending_xref)\n    assert_node(parsed_annotation[0],\n                pending_xref,\n                refdomain='py',\n                reftype='class',\n                reftarget='Union')\n", "def test_parse_annotation():\n    env = Mock()\n    env.ref_context = {}\n\n    # Test parsing of simple annotation\n    annotation = \"str\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 1\n    assert isinstance(result[0], pending_xref)\n    assert result[0].astext() == \"str\"\n\n    # Test parsing of complex annotation\n    annotation = \"Union[str, int]\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 3\n    assert isinstance(result[0], pending_xref)\n    assert result[0].astext() == \"Union\"\n    assert isinstance(result[1], desc_sig_punctuation)\n    assert result[1].astext() == \"[\"\n    assert isinstance(result[2], nodes.Text)\n    assert result[2].astext() == \"str, int\"\n\n    # Test parsing of string literal\n    annotation = \"'hello'\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 1\n    assert isinstance(result[0], nodes.Text)\n    assert result[0].astext() == \"'hello'\"\n\n    # Test parsing of nested annotation\n    annotation = \"List[Tuple[str, int]]\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 5\n    assert isinstance(result[0], pending_xref)\n    assert result[0].astext() == \"List\"\n    assert isinstance(result[1], desc_sig_punctuation)\n    assert result[1].astext() == \"[\"\n    assert isinstance(result[2], pending_xref)\n    assert result[2].astext() == \"Tuple\"\n    assert isinstance(result[3], desc_sig_punctuation)\n    assert result[3].astext() == \"[\"\n    assert isinstance(result[4], nodes.Text)\n    assert result[4].astext() == \"str, int\"\n"], "sample_930": ["def test_create_index(app, status, warning):\n    indexentries = IndexEntries(app.env)\n    builder = app.builder\n\n    # create some index entries\n    domain = app.env.get_domain('index')\n    domain.entries['file1'] = [\n        ('single', 'entry1', 'tid1', 'main1', None),\n        ('pair', 'entry2;entry3', 'tid2', 'main2', None),\n        ('triple', 'entry4;entry5;entry6', 'tid3', 'main3', None),\n    ]\n\n    # create the index\n    index = indexentries.create_index(builder)\n\n    # check the result\n    assert len(index) == 3\n    assert index[0][0] == 'E'\n    assert len(index[0][1]) == 3\n    assert index[0][1][0][0] == 'entry1'\n", "def test_create_index(app, status, warning):\n    \"\"\"\n    Test :meth:`IndexEntries.create_index` method.\n    \"\"\"\n    app.add_object_type('test', 'test', objname='pair', indextemplate='pair: %s; %s')\n    app.add_directive('test', restructuredtext.TestDirective)\n    app.add_node(restructuredtext.test_node,\n                 html=restructuredtext.TestNodeHTML,\n                 latex=restructuredtext.TestNodeLatex,\n                 text=restructuredtext.TestNodeText)\n\n    # create document\n    src = \"\"\"\n    .. test:: foo bar\n\n    .. test:: baz qux", "def test_create_index(app):\n    indexentries = IndexEntries(app.env)\n    builder = app.builder\n    domain = app.env.get_domain('index')\n\n    # Add some index entries\n    domain.entries['test'] = [\n        ('single', 'entry1', 'tid1', 'main1', None),\n        ('pair', 'entry2;subentry2', 'tid2', 'main2', None),\n        ('triple', 'entry3;subentry3;subsubentry3', 'tid3', 'main3', None),\n        ('see', 'entry4;other4', 'tid4', None, None),\n        ('seealso', 'entry5;other5', 'tid5', None, None),\n    ]\n\n    # Create the index\n    index = indexentries.create_index(builder)\n\n    # Check the index structure\n    assert len(index) > 0\n    for key, group in index:\n        assert isinstance(key, str)\n        assert isinstance(group, list)\n        for entry in group:\n            assert isinstance(entry, tuple)\n            assert len(entry) == 2\n            assert isinstance(entry[0], str)\n            assert isinstance(entry[1], list)\n", "def test_create_index(app, status, capsys):\n    app.add_object_type('index', 'pair', objname='index pair',\n                        indextemplate='pair: %s; %s')\n    restructuredtext.app = app\n\n    # Create a test index with various types of entries\n    entries = [\n        ('single', 'test', 'test', 'main', None),\n        ('pair', 'test1 test2', 'test1 test2', 'main', None),\n        ('triple', 'test1 test2 test3', 'test1 test2 test3', 'main', None),\n        ('see', 'test see', 'test see', None, None),\n        ('seealso', 'test seealso', 'test seealso', None, None)\n    ]\n\n    # Build the index\n    index = IndexEntries(app.env)\n    builder = app.builder\n    domain = app.env.get_domain('index')\n    domain.entries['test'] = entries\n    result = index.create_index(builder)\n\n    # Check that the index has the correct structure\n    assert len(result) == 1\n    key, values = result[0]\n    assert key == 'T'\n    assert len(values) == 6\n", "def test_create_index(app, status, warning):\n    app.add_object_type('myindex', 'myindex', 'pair')\n    restructuredtext.app = app\n    app.add_source('''\n    .. myindex:: pair1\n       pair2\n    .. myindex:: single1\n    .. myindex:: triple1; triple2; triple3\n    ''')\n\n    env = app.builder.env\n    indexentries = IndexEntries(env)\n    builder = app.builder\n\n    result = indexentries.create_index(builder)\n\n    # Assert the result is a list of tuples with the expected structure\n    assert isinstance(result, list)\n    for key, entries in result:\n        assert isinstance(key, str)\n        assert isinstance(entries, list)\n        for entry in entries:\n            assert isinstance(entry, tuple)\n            assert len(entry) == 2\n            assert isinstance(entry[0], str)\n            assert isinstance(entry[1], list)\n"], "sample_931": ["def test_parse_annotation():\n    env = Mock()\n    env.ref_context = {'py:module': 'module', 'py:class': 'class'}\n    annotation = \"Union[int, str]\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 1\n    assert isinstance(result[0], pending_xref)\n    assert result[0].astext() == \"Union[int, str]\"\n\n    annotation = \"module.class\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 1\n    assert isinstance(result[0], pending_xref)\n    assert result[0].astext() == \"module.class\"\n\n    annotation = \"~class\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 1\n    assert isinstance(result[0], pending_xref)\n    assert result[0].astext() == \"class\"\n", "def test_parse_annotation():\n    env = Mock()\n    env.ref_context = {}\n\n    annotation = \"Union[int, str]\"\n    expected_nodes = [\n        addnodes.pending_xref(\"\", nodes.Text(\"Union\"), refdomain=\"py\", reftype=\"class\", reftarget=\"Union\"),\n        addnodes.pending_xref(\"\", nodes.Text(\"int\"), refdomain=\"py\", reftype=\"class\", reftarget=\"int\"),\n        nodes.Text(\", \"),\n        addnodes.pending_xref(\"\", nodes.Text(\"str\"), refdomain=\"py\", reftype=\"class\", reftarget=\"str\"),\n    ]\n\n    result = _parse_annotation(annotation, env)\n    assert_node(result, expected_nodes)\n\n    annotation = \"Optional[str]\"\n    expected_nodes = [\n        addnodes.pending_xref(\"\", nodes.Text(\"Optional\"), refdomain=\"py\", reftype=\"class\", reftarget=\"Optional\"),\n        addnodes.pending_xref(\"\", nodes.Text(\"str\"), refdomain=\"py\", reftype=\"class\", reftarget=\"str\"),\n    ]\n\n    result = _parse_annotation(annotation, env)\n    assert_node(result, expected_nodes)\n", "def test_parse_annotation():\n    # type: () -> None\n    env = Mock()\n    annotation = \"Union[str, int]\"\n    parsed = _parse_annotation(annotation, env)\n    assert len(parsed) == 2\n    assert isinstance(parsed[0], addnodes.pending_xref)\n    assert parsed[0].astext() == \"str\"\n    assert isinstance(parsed[1], nodes.Text)\n    assert parsed[1].astext() == \", \"\n    assert isinstance(parsed[2], addnodes.pending_xref)\n    assert parsed[2].astext() == \"int\"\n\n    annotation = \"None\"\n    parsed = _parse_annotation(annotation, env)\n    assert len(parsed) == 1\n    assert isinstance(parsed[0], addnodes.pending_xref)\n    assert parsed[0].astext() == \"None\"\n", "def test_parse_annotation():\n    env = Mock()\n    annotations = _parse_annotation(\"List[int]\", env)\n    assert len(annotations) == 1\n    assert isinstance(annotations[0], pending_xref)\n    assert annotations[0].astext() == \"List[int]\"\n\n    annotations = _parse_annotation(\"Union[str, int]\", env)\n    assert len(annotations) == 1\n    assert isinstance(annotations[0], pending_xref)\n    assert annotations[0].astext() == \"Union[str, int]\"\n\n    annotations = _parse_annotation(\"Optional[str]\", env)\n    assert len(annotations) == 1\n    assert isinstance(annotations[0], pending_xref)\n    assert annotations[0].astext() == \"Optional[str]\"\n", "def test_parse_annotation():\n    env = Mock()\n    annotation = \"Union[int, str]\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 1\n    assert isinstance(result[0], pending_xref)\n    assert result[0].astext() == annotation\n\n    annotation = \"Dict[str, List[int]]\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 1\n    assert isinstance(result[0], pending_xref)\n    assert result[0].astext() == annotation\n\n    annotation = \"Any\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 1\n    assert isinstance(result[0], pending_xref)\n    assert result[0].astext() == annotation\n"], "sample_933": ["def test_gettext_additional_targets(app):\n    app.build()\n    pot_contents = []\n    with open(os.path.join(app.outdir, 'index.pot'), 'r', encoding='utf-8') as f:\n        pot_contents = f.read().splitlines()\n    # Extract translatable messages from index entries are enabled by default,\n    # but for other nodes like toctree, they should not be extracted by default.\n    assert '#: templates/index.html:3' in pot_contents\n    assert '#: templates/index.html:5' not in pot_contents\n\n    app.config.gettext_additional_targets = ['toctree']\n    app.build()\n    pot_contents = []\n    with open(os.path.join(app.outdir, 'index.pot'), 'r', encoding='utf-8') as f:\n        pot_contents = f.read().splitlines()\n    assert '#: templates/index.html:5' in pot_contents\n", "def test_gettext_additional_targets(app, status, warning):\n    app.builder.build_all()\n    assert (app.outdir / 'targets.pot').exists()\n    with open(app.outdir / 'targets.pot', encoding='utf-8') as f:\n        content = f.read()\n    assert 'This is an index entry.' in content\n    assert 'This is another index entry.' in content\n", "def test_gettext_write_pot(app, status, warning):\n    app.build()\n    assert (app.outdir / 'sphinx.pot').exists()\n\n    with open(app.outdir / 'sphinx.pot', encoding='utf-8') as f:\n        content = f.read()\n\n    assert re.search(r'^\"Project-Id-Version: Test Project 1\\.0$', content, re.MULTILINE)\n    assert re.search(r'^\"POT-Creation-Date: ', content, re.MULTILINE)\n\n    # Check messages\n    messages = [line for line in content.splitlines() if line.startswith('msgid \"')]\n    assert len(messages) == 3\n    assert 'msgid \"\"' in messages\n    assert 'msgid \"Test File\"' in messages\n    assert 'msgid \"Hello World\"' in messages\n", "def test_gettext_location(tmpdir):\n    srcdir = tmpdir.join(\"src\")\n    srcdir.mkdir()\n    outdir = tmpdir.join(\"out\")\n    outdir.mkdir()\n\n    # Create a simple conf.py file\n    conf_file = srcdir.join(\"conf.py\")\n    conf_file.write(\"copyright = '2022, Test Author'\\n\")\n    conf_file.write(\"version = '1.0'\\n\")\n    conf_file.write(\"language = 'en'\\n\")\n    conf_file.write(\"gettext_compact = False\\n\")\n    conf_file.write(\"gettext_location = True\\n\")\n\n    # Create a simple index.rst file\n    index_file = srcdir.join(\"index.rst\")\n    index_file.write(\"Welcome to my documentation!\\n\")\n    index_file.write(\"=============================\\n\")\n\n    # Run sphinx-build with gettext builder\n    with cd(srcdir):\n        try:\n            output = subprocess.check_output(\n                [\"sphinx-build\", \"-b\", \"gettext\", \"-D\", \"gettext_additional_targets=index\", str(srcdir), str(outdir)],\n                stderr=PIPE,\n            ).decode(\"utf-8\")\n        except CalledProcessError as e:\n            print(e.output.decode(\"utf-8\"))\n            raise\n\n    # Check if the pot file contains the correct location information\n    pot_file = outdir.join(\"index.pot\")\n    assert pot_file.exists()\n    with open(str(pot_file), encoding=\"utf-8\") as f:\n        content = f.read()\n    expected = r'#: .*index\\.rst:\\d+$'\n    assert re.search(expected, content, re.MULTILINE)\n", "def test_gettext_additional_targets(app, status, warning):\n    app.builder.build_all()\n    warning_message = warning.getvalue().strip()\n    assert 'extracting messages from index entries...' in status.getvalue()\n    assert not warning_message\n"], "sample_936": ["def test_stringify_broken_type():\n    with pytest.raises(TypeError):\n        stringify(BrokenType())\n", "def test_stringify_py36_builtin_types():\n    assert stringify(int) == 'int'\n    assert stringify(str) == 'str'\n    assert stringify(list) == 'list'\n    assert stringify(dict) == 'dict'\n    assert stringify(tuple) == 'tuple'\n", "def test_stringify_py37_optional():\n    if sys.version_info < (3, 7):\n        pytest.skip('This test is for Python 3.7 and later')\n    assert stringify(Optional[int]) == 'Optional[int]'\n    assert stringify(Union[int, None]) == 'Optional[int]'\n    assert stringify(Union[int, str, None]) == 'Optional[Union[int, str]]'\n", "def test_stringify_forwardref():\n    if sys.version_info > (3, 7):\n        from typing import ForwardRef\n    else:\n        from sphinx.util.typing import ForwardRef\n\n    ref = ForwardRef('MyClass1')\n    assert stringify(ref) == 'MyClass1'\n    ref = ForwardRef('MyClass2')\n    assert stringify(ref) == '<MyClass2>'\n", "def test_stringify_py37():\n    # test for py37+\n    if sys.version_info >= (3, 7):\n        from typing import ForwardRef\n        annotation = ForwardRef('MyClass1')\n        assert stringify(annotation) == 'MyClass1'\n\n        annotation = Dict[str, int]\n        assert stringify(annotation) == 'Dict[str, int]'\n\n        annotation = Union[int, str, None]\n        assert stringify(annotation) == 'Optional[Union[int, str]]'\n\n        annotation = Callable[[int, str], bool]\n        assert stringify(annotation) == 'Callable[[int, str], bool]'\n\n        annotation = Generator[int, str, bool]\n        assert stringify(annotation) == 'Generator[int, str, bool]'\n"], "sample_937": ["def test_unparse_arguments(source, expected):\n    node = ast.parse(source).body[0].args\n    assert ast.unparse(node) == expected\n", "def test_unparse_arguments(source, expected):\n    tree = ast.parse(source)\n    func_def = tree.body[0]\n    assert ast.unparse(func_def.args) == expected\n", "def test_unparse_arguments(source, expected):\n    tree = ast.parse(source)\n    func_def = tree.body[0]\n    assert ast.unparse(func_def.args) == expected\n", "def test_unparse_arguments(source, expected):\n    node = ast.parse(source).body[0].args  # type: ignore\n    assert ast.unparse(node) == expected\n", "def test_unparse_arguments(source, expected):\n    tree = ast.parse(source)\n    func_def = tree.body[0]\n    assert ast.unparse(func_def.args) == expected\n"], "sample_10": ["def test_table_replace_column_meta_copy(table_types):\n    # Test that Table.replace_column() correctly handles copying the column\n    # metadata when the table is initialized with `copy=False`.  In the\n    # process this also tests the `copy` argument of Column.__init__().\n    for copy in (True, False):\n        t = table_types.Table([[1, 2], [3, 4]], names=('a', 'b'), copy=copy)\n        ta = t['a']\n        orig_meta = {'foo': 'bar'}\n        ta.info.meta = orig_meta\n\n        # Now replace column 'a' with new column.  This should generate a copy\n        # of the column so that the original column and new column do not share\n        # the same meta object.\n        t['a'] = table_types.Column([5, 6], name='a')\n\n        # Assert that the meta was copied correctly.\n        assert t['a'].info.meta == orig_meta\n\n        # Assert that the original column and new column do not share the meta\n        # object.\n        t['a'].info.meta['foo'] = 'baz'\n        assert t['a'].info.meta != ta.info.meta\n", "def test_setitem_replace_column_with_masked_array():\n    t = Table([[1, 2, 3], [4, 5, 6]], names=('a', 'b'))\n    t['a'] = np.ma.MaskedArray([1, 2, 3], mask=[True, False, True])\n    assert_array_equal(t['a'].data, [1, 2, 3])\n    assert_array_equal(t['a'].mask, [True, False, True])\n", "def test_set_item_masked_column():\n    # Check that setting a masked column with a masked array works as expected\n    t = Table()\n    t['a'] = np.ma.array([1, 2, 3], mask=[True, False, True])\n    assert_array_equal(t['a'].data, [1, 2, 3])\n    assert_array_equal(t['a'].mask, [True, False, True])\n    # Also check that the mask is a separate array\n    t['a'].data[1] = 100\n    assert_array_equal(t['a'].data, [1, 100, 3])\n    assert_array_equal(t['a'].mask, [True, False, True])\n    # And check that setting an unmasked column with a masked array works as expected\n    t['b'] = np.ma.array([1, 2, 3], mask=[True, False, True])\n    assert_array_equal(t['b'].data, [1, 2, 3])\n    assert_array_equal(t['b'].mask, [True, False, True])\n    # And finally check that setting a masked column with an unmasked array works as expected\n    t['a'] = np.array([1, 2, 3])\n    assert_array_equal(t['a'].data, [1, 2, 3])\n    assert_array_equal(t['a'].mask, [False, False, False])\n", "def test_set_table_row():\n    t = Table([[1, 2, 3], [4, 5, 6]], names=('a', 'b'))\n    t[0] = (10, 40)\n    assert_array_equal(t['a'], [10, 2, 3])\n    assert_array_equal(t['b'], [40, 5, 6])\n", "def test_pandas_conversion(table_types):\n    from pandas import DataFrame\n    from pandas.testing import assert_frame_equal\n\n    SetupData._setup(table_types)\n\n    # Use MaskedColumn to ensure that conversion works properly\n    # with missing values.\n    t = table_types.Table([[1, 2, 3], [4, 5, 6]],\n                          names=('a', 'b'), masked=True)\n    t['a'].mask = [True, False, True]\n\n    df_from_table = t.to_pandas()\n    assert_frame_equal(df_from_table, DataFrame({'a': np.array([np.nan, 2, np.nan], dtype=np.float64),\n                                                 'b': np.array([4, 5, 6], dtype=np.int64)}))\n\n    # Test round-trip\n    t2 = table_types.Table.from_pandas(df_from_table)\n    assert_array_equal(t2['a'].mask, t['a'].mask)\n    assert_array_equal(t2['a'].data, t['a'].data)\n    assert_array_equal(t2['b'], t['b'])\n\n    # Test conversion with plain Column\n    t3 = table_types.Table([[1, 2, 3], [4, 5, 6]], names=('a', 'b'))\n    df_from_table3 = t3.to_pandas()\n    assert_frame_equal(df_from_table3, DataFrame({'a': np.array([1, 2, 3], dtype=np.int64),\n                                                  'b': np.array([4, 5, 6], dtype=np.int64)}))\n\n    # Test column names with special characters\n    t4 = table_types.Table([[1, 2, 3], [4, 5, 6]], names=('abc.def', 'ghi'))\n    df_from_table4 = t4.to_pandas()\n    assert_frame_equal(df_from_table4, DataFrame({'abc.def': np.array([1, 2, 3], dtype=np.int64),\n                                                  'ghi': np.array([4, 5, 6], dtype=np.int64)}))\n\n    # Test setting DataFrame index from Table\n    t5 = table_types.Table({'a': [1, 2, 3], 'b': [4, 5, 6]}, meta={'index': 'a'})\n    df_from_table5 = t5.to_pandas(index=True)\n    assert_frame_equal(df_from_table5, DataFrame({'b': np"], "sample_19": ["def test_wcs_to_header_equality_with_wcslib(self, tmpdir):\n    # Check if to_header() returns the same header as WCSLIB's wcsprm_to_header.\n    header = fits.Header.fromtextfile(get_pkg_data_filename(\"data/maps/1904-66_AZP.fits.hdr\"))\n    with warnings.catch_warnings(record=True):\n        wcs = wcs.WCS(header)\n\n    # WCSLIB's WCS to header\n    keys, values, comments = _wcs.wcsprm_to_header(wcs.wcs)\n    wcslib_header = fits.Header.fromtextfile(io.StringIO(\"\\n\".join([f\"{k} = {v} / {c}\" for k, v, c in zip(keys, values, comments)])))\n    \n    # Astropy's WCS to_header\n    astropy_header = wcs.to_header(relax=True)\n\n    # Compare the two headers\n    assert set(wcslib_header) == set(astropy_header)\n    for k in wcslib_header:\n        assert wcslib_header[k] == astropy_header[k]\n\n    # Create a FITS file using the Astropy WCS header\n    hdu = fits.PrimaryHDU(header=astropy_header)\n    tmp_filename = tmpdir.join(\"temp.fits\")\n    hdu.writeto(tmp_filename, overwrite=True)\n\n    # Try to open it and create a WCS object\n    with fits.open(tmp_filename, mode='readonly') as hdul:\n        header = hdul[0].header\n        wcs.WCS(header)\n", "def test_read_distortion_kw(self):\n    # Create a fake FITS header with some distortion keywords\n    header = fits.Header()\n    header[\"CPDIS1\"] = \"LOOKUP\"\n    header[\"DP1.EXTVER\"] = 1\n    header[\"DP1.NAXES\"] = 2\n    header[\"DP1.AXIS.1\"] = 1\n    header[\"DP1.AXIS.2\"] = 2\n    header[\"CPDIS2\"] = \"LOOKUP\"\n    header[\"DP2.EXTVER\"] = 1\n    header[\"DP2.NAXES\"] = 2\n    header[\"DP2.AXIS.1\"] = 1\n    header[\"DP2.AXIS.2\"] = 2\n\n    # Create a fake FITS HDUList\n    hdu = fits.PrimaryHDU(header)\n    hdulist = fits.HDUList([hdu])\n    image = fits.ImageHDU(np.zeros((10, 10)))\n    image.header[\"EXTNAME\"] = \"WCSDVARR\"\n    image.header[\"EXTVER\"] = 1\n    hdulist.append(image)\n\n    # Create a WCS object\n    wcs_obj = wcs.WCS(header, hdulist)\n\n    # Check that the distortion keywords were read correctly\n    assert wcs_obj.cpdis1 is not None\n    assert wcs_obj.cpdis2 is not None\n", "def test_sip_image_scaling():\n    # Check that SIP distortions are correctly scaled when image is scaled.\n    hdr = fits.Header.fromtextfile(\n        get_pkg_data_filename(\"data/acs.hdr\"),\n        endcard=False,\n        padding=False,\n    )\n    # Make sure we have a WCSDVARR keyword written out\n    hdr[\"CPDIS1\"] = \"LOOKUP\"\n    hdr[\"DP1.NAXES\"] = 2\n    hdr[\"DP1.AXIS.1\"] = 1\n    hdr[\"DP1.AXIS.2\"] = 2\n    hdr[\"DP1.EXTVER\"] = 1\n\n    w = wcs.WCS(hdr, fobj=fits.HDUList([fits.PrimaryHDU(header=hdr)]))\n\n    scale = 0.5\n    w.wcs.cd *= scale\n\n    assert_array_almost_equal_nulp(w.sip.a, scale * w.sip.a_0)\n    assert_array_almost_equal_nulp(w.sip.b, scale * w.sip.b_0)\n\n    w.wcs.set()\n    assert_array_almost_equal_nulp(w.sip.a, scale * w.sip.a_0)\n    assert_array_almost_equal_nulp(w.sip.b, scale * w.sip.b_0)\n", "def test_footprint_contains(self):\n    # Create a WCS object using a sample FITS file\n    filename = get_pkg_data_filename(\"data/j94f05bgq_flt.fits\")\n    hdulist = fits.open(filename)\n    w = wcs.WCS(hdulist[(\"sci\", 1)].header)\n    hdulist.close()\n\n    # Convert the WCS footprint to a SkyCoord\n    footprint = w.calc_footprint()\n    footprint_coord = SkyCoord.from_pixel(footprint[:, 0], footprint[:, 1], w)\n\n    # Create a random SkyCoord within the footprint\n    with NumpyRNGContext(123456789):\n        random_ind = np.random.randint(0, 4)\n        random_offset = np.random.uniform(-0.1, 0.1, 2)\n    random_coord = SkyCoord(\n        footprint_coord.ra.deg[random_ind] + random_offset[0],\n        footprint_coord.dec.deg[random_ind] + random_offset[1],\n        unit=\"deg\",\n        frame=\"icrs\",\n    )\n\n    # Check that the footprint contains the random coordinate\n    assert w.footprint_contains(random_coord)\n\n    # Create a random SkyCoord outside the footprint\n    with NumpyRNGContext(123456789):\n        random_ind = np.random.randint(0, 4)\n        random_offset = np.random.uniform(5, 10, 2)\n    random_coord = SkyCoord(\n        footprint_coord.ra.deg[random_ind] + random_offset[0],\n        footprint_coord.dec.deg[random_ind] + random_offset[1],\n        unit=\"deg\",\n        frame=\"icrs\",\n    )\n\n    # Check that the footprint does not contain the random coordinate\n    assert not w.footprint_contains(random_coord)\n", "    def test_wcs_get_axis_types(self):\n        # Test that wcs.get_axis_types() returns the correct axis types\n        # for a simple 2D WCS with RA and DEC axes.\n        wcs_2d = wcs.WCS(naxis=2)\n        wcs_2d.wcs.ctype = [\"RA---TAN\", \"DEC--TAN\"]\n        wcs_2d.wcs.crval = [0.0, 0.0]\n        wcs_2d.wcs.crpix = [1.0, 1.0]\n        wcs_2d.wcs.cdelt = [1.0, 1.0]\n        wcs_2d.wcs.set()\n\n        axis_types = wcs_2d.get_axis_types()\n        assert len(axis_types) == 2\n        assert axis_types[0][\"coordinate_type\"] == \"celestial\"\n        assert axis_types[0][\"scale\"] == \"linear\"\n        assert axis_types[0][\"group\"] == 0\n        assert axis_types[0][\"number\"] == 0\n        assert axis_types[1][\"coordinate_type\"] == \"celestial\"\n        assert axis_types[1][\"scale\"] == \"linear\"\n        assert axis_types[1][\"group\"] == 0\n        assert axis_types[1][\"number\"] == 1\n"], "sample_55": ["def test_admin_readonly_field(self):\n    \"\"\"\n    Test that readonly fields are correctly rendered in the admin.\n    \"\"\"\n    response = self.client.get(reverse('admin:admin_views_article_change', args=(self.a1.pk,)))\n    readonly_field = self.get_admin_readonly_field(response, 'section')\n    self.assertIsNotNone(readonly_field)\n    self.assertEqual(readonly_field.field['name'], 'section')\n    self.assertEqual(readonly_field.field['label'], 'Section')\n    self.assertEqual(readonly_field.contents(), str(self.s1))\n", "    def setUp(self):\n        self.superuser = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n        self.client.force_login(self.superuser)\n", "def test_adminform_errors(self):\n    class TestForm(forms.Form):\n        field1 = forms.CharField()\n        field2 = forms.CharField()\n\n    form = TestForm(data={'field1': 'value'})\n    admin_form = AdminForm(form, [(None, {'fields': ('field1', 'field2')})], {}, [])\n\n    self.assertEqual(len(admin_form.errors), 1)\n    self.assertEqual(len(admin_form.non_field_errors), 0)\n    self.assertEqual(len(admin_form[0].errors()), 1)\n    self.assertEqual(admin_form[0].errors(), '<ul class=\"errorlist\"><li>This field is required.</li></ul>')\n\n    # Test that AdminErrorList can handle all error types\n    class TestForm(forms.Form):\n        field1 = forms.CharField()\n        field2 = forms.CharField()\n\n    form = TestForm(data={'field1': 'value'})\n    form.add_error(None, 'Non field error')\n    form.add_error('field1', 'Field error')\n    admin_form = AdminForm(form, [(None, {'fields': ('field1', 'field2')})], {}, [])\n\n    self.assertEqual(len(admin_form.errors), 2)\n    self.assertEqual(len(admin_form.non_field_errors), 1)\n", "def test_admin_readonly_field(self):\n    \"\"\"\n    Test that readonly fields are correctly rendered.\n    \"\"\"\n    response = self.client.get(reverse('admin:admin_views_article_add'))\n    admin_readonly_fields = self.get_admin_readonly_fields(response)\n    self.assertEqual(len(admin_readonly_fields), 0)\n\n    # Make 'section' a readonly field\n    with mock.patch('django.contrib.admin.options.ModelAdmin.get_readonly_fields', return_value=('section',)):\n        response = self.client.get(reverse('admin:admin_views_article_add'))\n        admin_readonly_fields = self.get_admin_readonly_fields(response)\n        self.assertEqual(len(admin_readonly_fields), 1)\n        self.assertEqual(admin_readonly_fields[0].field['name'], 'section')\n\n    # Make 'title' and 'section' readonly fields\n    with mock.patch('django.contrib.admin.options.ModelAdmin.get_readonly_fields', return_value=('title', 'section')):\n        response = self.client.get(reverse('admin:admin_views_article_add'))\n        admin_readonly_fields = self.get_admin_readonly_fields(response)\n        self.assertEqual(len(admin_readonly_fields), 2)\n        self.assertEqual(admin_readonly_fields[0].field['name'], 'title')\n        self.assertEqual(admin_readonly_fields[1].field['name'], 'section')\n", "def test_inline_admin_formset_fields(self):\n    inline_formset = InlineAdminFormSet(\n        inline=Article,\n        formset=forms.formset_factory(forms.ModelForm),\n        fieldsets=(\n            (None, {'fields': ('title', 'content')}),\n        ),\n        prepopulated_fields=None,\n        readonly_fields=None,\n        model_admin=None,\n    )\n\n    fields = list(inline_formset.fields())\n    self.assertEqual(len(fields), 2)\n    self.assertEqual(fields[0]['name'], 'title')\n    self.assertEqual(fields[0]['label'], 'title')\n    self.assertEqual(fields[1]['name'], 'content')\n    self.assertEqual(fields[1]['label'], 'content')\n"], "sample_66": ["def test_http_headers(self):\n    environ = {\n        'HTTP_ACCEPT': 'text/html',\n        'HTTP_ACCEPT_LANGUAGE': 'en-us',\n        'HTTP_CACHE_CONTROL': 'max-age=0',\n        'HTTP_CONNECTION': 'keep-alive',\n        'HTTP_USER_AGENT': 'Mozilla/5.0',\n        'CONTENT_TYPE': 'text/plain',\n        'CONTENT_LENGTH': '42',\n    }\n    headers = HttpHeaders(environ)\n\n    # Test direct access\n    self.assertEqual(headers['Accept'], 'text/html')\n    self.assertEqual(headers['Content-Type'], 'text/plain')\n\n    # Test case-insensitive access\n    self.assertEqual(headers['accept'], 'text/html')\n    self.assertEqual(headers['content-type'], 'text/plain')\n    self.assertEqual(headers['USER_AGENT'], 'Mozilla/5.0')\n\n    # Test KeyError for missing header\n    with self.assertRaises(KeyError):\n        headers['Missing-Header']\n", "def test_request_get_host_with_empty_allowed_hosts(self):\n    request = HttpRequest()\n    request.META = {\n        'HTTP_HOST': 'example.com',\n        'SERVER_NAME': 'example.com',\n        'SERVER_PORT': '80',\n    }\n    with override_settings(ALLOWED_HOSTS=[]):\n        with self.settings(DEBUG=True):\n            self.assertEqual(request.get_host(), 'example.com')\n        with self.settings(DEBUG=False):\n            with self.assertRaises(DisallowedHost):\n                request.get_host()\n", "def test_request_get_host(self):\n    request = HttpRequest()\n    request.META = {\n        'HTTP_HOST': 'example.com',\n        'SERVER_NAME': 'server_name',\n        'SERVER_PORT': '80',\n    }\n    self.assertEqual(request.get_host(), 'example.com')\n\n    # Test case when SERVER_PORT is not 80 or 443\n    request.META['SERVER_PORT'] = '8080'\n    self.assertEqual(request.get_host(), 'example.com:8080')\n\n    # Test case when HTTP_HOST is not provided\n    del request.META['HTTP_HOST']\n    self.assertEqual(request.get_host(), 'server_name:8080')\n\n    # Test case when X_FORWARDED_HOST is provided\n    request.META['HTTP_X_FORWARDED_HOST'] = 'forwarded_host'\n    with override_settings(USE_X_FORWARDED_HOST=True):\n        self.assertEqual(request.get_host(), 'forwarded_host')\n", "def test_request_get_host_with_empty_allowed_hosts(self):\n    request = HttpRequest()\n    request.META['HTTP_HOST'] = 'example.com'\n    with override_settings(ALLOWED_HOSTS=[]):\n        with self.assertRaises(DisallowedHost):\n            request.get_host()\n\n    with override_settings(ALLOWED_HOSTS=['*']):\n        self.assertEqual('example.com', request.get_host())\n\n    with override_settings(ALLOWED_HOSTS=['example.com']):\n        self.assertEqual('example.com', request.get_host())\n", "def test_request_get_port(self):\n    request = HttpRequest()\n    request.META = {\n        'SERVER_PORT': '8080',\n    }\n    self.assertEqual(request.get_port(), '8080')\n\n    request = HttpRequest()\n    request.META = {\n        'HTTP_X_FORWARDED_PORT': '8080',\n    }\n    with override_settings(USE_X_FORWARDED_PORT=True):\n        self.assertEqual(request.get_port(), '8080')\n"], "sample_95": ["def test_patch_cache_control(self):\n    response = HttpResponse('content')\n    patch_cache_control(response, max_age=3600, public=True)\n    self.assertEqual(response['Cache-Control'], 'max-age=3600, public')\n    patch_cache_control(response, max_age=1800)\n    self.assertEqual(response['Cache-Control'], 'max-age=1800, public')\n    patch_cache_control(response, must_revalidate=True)\n    self.assertEqual(response['Cache-Control'], 'max-age=1800, public, must-revalidate')\n    patch_cache_control(response, max_age=0, no_cache=True, no_store=True, must_revalidate=True, private=True)\n    self.assertEqual(response['Cache-Control'], 'max-age=0, no-cache, no-store, must-revalidate, private')\n", "def test_patch_cache_control(self):\n    response = HttpResponse()\n    patch_cache_control(response, private=True, max_age=3600)\n    self.assertEqual(response['Cache-Control'], 'private, max-age=3600')\n\n    response = HttpResponse()\n    response['Cache-Control'] = 'public'\n    patch_cache_control(response, private=True, max_age=3600)\n    self.assertEqual(response['Cache-Control'], 'private, max-age=3600')\n\n    response = HttpResponse()\n    patch_cache_control(response, private=True, max_age=None)\n    self.assertEqual(response['Cache-Control'], 'private')\n\n    response = HttpResponse()\n    patch_cache_control(response, private=None, max_age=3600)\n    self.assertEqual(response['Cache-Control'], 'max-age=3600')\n\n    response = HttpResponse()\n    patch_cache_control(response, private=True, max_age=3600, no_cache=True)\n    self.assertEqual(response['Cache-Control'], 'private, max-age=3600, no-cache')\n", "def test_patch_cache_control(self):\n    response = HttpResponse()\n    patch_cache_control(response, public=True, max_age=300)\n    self.assertEqual(response['Cache-Control'], 'public, max-age=300')\n\n    patch_cache_control(response, private=True)\n    self.assertEqual(response['Cache-Control'], 'private, max-age=300')\n\n    patch_cache_control(response, max_age=60)\n    self.assertEqual(response['Cache-Control'], 'private, max-age=60')\n\n    patch_cache_control(response, must_revalidate=True)\n    self.assertEqual(response['Cache-Control'], 'private, max-age=60, must-revalidate')\n", "def test_patch_cache_control(self):\n    response = HttpResponse()\n    patch_cache_control(response, max_age=3600, private=True)\n    self.assertEqual(response['Cache-Control'], 'max-age=3600, private')\n\n    # Ensure the decorator adds new values correctly.\n    patch_cache_control(response, public=True, must_revalidate=True)\n    self.assertEqual(response['Cache-Control'], 'max-age=3600, public, must-revalidate')\n\n    # Ensure the decorator overwrites existing values correctly.\n    patch_cache_control(response, max_age=7200)\n    self.assertEqual(response['Cache-Control'], 'max-age=7200, public, must-revalidate')\n\n    # Ensure the decorator can handle multiple values.\n    patch_cache_control(response, 'no-cache', 'no-store')\n    self.assertEqual(response['Cache-Control'], 'max-age=7200, public, must-revalidate, no-cache, no-store')\n\n    # Ensure the decorator can handle values with arguments.\n    patch_cache_control(response, max_age=3600, stale_while_revalidate=300)\n    self.assertEqual(response['Cache-Control'], 'max-age=3600, public, must-revalidate, stale-while-revalidate=300, no-cache, no-store')\n", "def test_patch_cache_control(self):\n    response = HttpResponse()\n    patch_cache_control(response, max_age=3600, public=True)\n    self.assertEqual(response['Cache-Control'], 'max-age=3600, public')\n\n    response = HttpResponse()\n    response['Cache-Control'] = 'max-age=300'\n    patch_cache_control(response, max_age=3600, public=True)\n    self.assertEqual(response['Cache-Control'], 'max-age=3600, public')\n\n    response = HttpResponse()\n    patch_cache_control(response, private=True)\n    self.assertEqual(response['Cache-Control'], 'private')\n\n    response = HttpResponse()\n    patch_cache_control(response, no_cache=True, no_store=True, must_revalidate=True)\n    self.assertEqual(response['Cache-Control'], 'no-cache, no-store, must-revalidate')\n"], "sample_124": ["def test_field_initial_default(self):\n    # Initial values can be specified on the Field, and will be used if no initial\n    # value is provided in the Form.\n\n    class F(Form):\n        field1 = CharField(initial='field1', required=False)\n        field2 = CharField(initial='field2', required=True)\n\n    form = F()\n    self.assertEqual(form['field1'].initial, 'field1')\n    self.assertEqual(form['field2'].initial, 'field2')\n\n    self.assertEqual(form['field1'].value(), 'field1')\n    self.assertEqual(form['field2'].value(), 'field2')\n\n    form = F({'field1': 'new value'})\n    self.assertEqual(form['field1'].initial, 'field1')\n    self.assertEqual(form['field1'].value(), 'new value')\n    self.assertEqual(form['field2'].value(), 'field2')\n    self.assertEqual(form.initial['field1'], 'field1')\n    self.assertEqual(form.initial['field2'], 'field2')\n", "def test_split_datetime_field(self):\n    class SplitDateTimeForm(Form):\n        dt = SplitDateTimeField()\n\n    f = SplitDateTimeForm({'dt_0': '2006-01-10', 'dt_1': '07:30'})\n    self.assertTrue(f.is_valid())\n    dt = f.cleaned_data['dt']\n    self.assertEqual(dt.year, 2006)\n    self.assertEqual(dt.month, 1)\n    self.assertEqual(dt.day, 10)\n    self.assertEqual(dt.hour, 7)\n    self.assertEqual(dt.minute, 30)\n\n    # Test the special hidden widget for SplitDateTimeField\n    class SplitDateTimeForm(Form):\n        dt = SplitDateTimeField(widget=SplitHiddenDateTimeWidget)\n\n    f = SplitDateTimeForm({'dt_0': '2006-01-10', 'dt_1': '07:30'})\n    self.assertTrue(f.is_valid())\n    dt = f.cleaned_data['dt']\n    self.assertEqual(dt.year, 2006)\n    self.assertEqual(dt.month, 1)\n    self.assertEqual(dt.day, 10)\n    self.assertEqual(dt.hour, 7)\n    self.assertEqual(dt.minute, 30)\n", "def test_forms_with_initial_data(self):\n    class Person(Form):\n        first_name = CharField(initial='John', widget=TextInput(attrs={'id': 'first_name_id'}))\n        last_name = CharField()\n\n    f = Person(auto_id=False)\n    self.assertHTMLEqual(\n        f.as_p(),\n        '<p>First name: <input type=\"text\" name=\"first_name\" value=\"John\" id=\"first_name_id\"></p>'\n        '\\n<p>Last name: <input type=\"text\" name=\"last_name\"></p>',\n    )\n\n    f = Person({'last_name': 'Doe'}, auto_id=False)\n    self.assertHTMLEqual(\n        f.as_p(),\n        '<p>First name: <input type=\"text\" name=\"first_name\" value=\"John\" id=\"first_name_id\"></p>'\n        '\\n<p>Last name: <input type=\"text\" name=\"last_name\" value=\"Doe\"></p>',\n    )\n", "def test_forms_error_class(self):\n    # A Form's error class can be modified to use a different kind of\n    # ErrorList.  For example, the error messages can be displayed with\n    # formatting markers to indicate what kind of error they are (required,\n    # list, etc.)\n    class UserForm(forms.Form):\n        username = CharField(max_length=10)\n        email = CharField()\n\n    # Test the errors when they are not formatted\n    form = UserForm({}, error_class=ErrorList)\n    self.assertEqual(form.errors['username'][0], 'This field is required.')\n    self.assertEqual(form.errors['email'][0], 'This field is required.')\n\n    # Test the errors when they are formatted\n    class ErrorList(ErrorList):\n            return self.as_text()\n\n            return '\\n'.join('* %s' % e for e in self)\n\n    form = UserForm({}, error_class=ErrorList)\n    self.assertEqual(str(form.errors['username']), '* This field is required.')\n    self.assertEqual(str(form.errors['email']), '* This field is required.')\n", "def test_default_field_values(self):\n    # Every Field declares storage for a \"default\" value (in this case, None).\n    # Each Field also hardcodes an \"initial\" value in its __init__().\n    f = Person()\n    self.assertIsNone(f.fields['first_name'].initial)\n    self.assertIsNone(f.fields['last_name'].initial)\n    self.assertIsNone(f.fields['birthday'].initial)\n\n    # In this case, we're not passing any data to the Form's __init__(), so it will use\n    # the default values for fields.\n    self.assertEqual(f.as_table(), \"\"\"<tr><th><label for=\"id_first_name\">First name:</label></th><td><input type=\"text\" name=\"first_name\" id=\"id_first_name\" required /></td></tr>"], "sample_297": ["def test_exists_annotation(self):\n    qs = Note.objects.annotate(has_annotation=Exists(Annotation.notes.through.objects.filter(note=OuterRef('pk'))))\n    annotated_notes = list(qs)\n    self.assertEqual(len(annotated_notes), Note.objects.count())\n    self.assertTrue(annotated_notes[0].has_annotation)\n    self.assertTrue(annotated_notes[1].has_annotation)\n    self.assertTrue(annotated_notes[2].has_annotation)\n", "def test_split_exclude(self):\n    # Test that split_exclude() generates correct subqueries for exclude()\n    # filters that span ManyToMany relations.\n    # model exclude filter -> SQL\n    qs = Tag.objects.exclude(item__name='one').order_by('name')\n    self.assertQuerysetEqual(\n        qs,\n        [\"<Tag: t1>\", \"<Tag: t2>\", \"<Tag: t3>\", \"<Tag: t4>\", \"<Tag: t5>\"],\n        ordered=True\n    )\n", "def test_bump_prefix(self):\n    q1 = Item.objects.filter(name='one')\n    q2 = q1._clone()\n    self.assertIs(q1.alias_prefix, q2.alias_prefix)\n    q2.bump_prefix(q1)\n    self.assertNotEqual(q1.alias_prefix, q2.alias_prefix)\n\n    q3 = q1._clone()\n    self.assertIs(q1.alias_prefix, q3.alias_prefix)\n    q1.bump_prefix(q2)\n    self.assertNotEqual(q3.alias_prefix, q1.alias_prefix)\n    self.assertNotEqual(q1.alias_prefix, q2.alias_prefix)\n", "def test_build_where(self):\n    q = Query(self.Item)\n    where_clause = q.build_where(('name', 'test'))\n    self.assertIsInstance(where_clause, WhereNode)\n    self.assertEqual(len(where_clause.children), 1)\n\n    lookup = where_clause.children[0]\n    self.assertEqual(lookup.lhs.target.column, 'name')\n    self.assertEqual(lookup.rhs, 'test')\n", "    def test_foreign_key(self):\n        lookup_parts, field_parts, is_relation = Item._default_manager.solve_lookup_type('creator__name')\n        self.assertEqual(lookup_parts, ['exact'])\n        self.assertEqual(field_parts, ['creator', 'name'])\n        self.assertFalse(is_relation)\n"], "sample_136": ["def test_http_headers(self):\n    # Test HttpHeaders class\n    environ = {\n        'HTTP_ACCEPT': 'text/html',\n        'HTTP_ACCEPT_LANGUAGE': 'en-us',\n        'HTTP_HOST': 'example.com',\n        'CONTENT_TYPE': 'text/plain',\n        'CONTENT_LENGTH': '0',\n    }\n    headers = HttpHeaders(environ)\n    self.assertEqual(headers['Accept'], 'text/html')\n    self.assertEqual(headers['Accept-Language'], 'en-us')\n    self.assertEqual(headers['Host'], 'example.com')\n    self.assertEqual(headers['Content-Type'], 'text/plain')\n    self.assertEqual(headers['Content-Length'], '0')\n    # Test that HttpHeaders is case-insensitive\n    self.assertEqual(headers['accept'], 'text/html')\n    self.assertEqual(headers['ACCEPT'], 'text/html')\n    self.assertEqual(headers['Accept-Language'], 'en-us')\n    self.assertEqual(headers['accept-language'], 'en-us')\n", "def test_request_headers(self):\n    request = HttpRequest()\n    request.META = {\n        'HTTP_ACCEPT': 'text/html',\n        'HTTP_ACCEPT_CHARSET': 'utf-8',\n        'HTTP_HOST': 'example.com',\n        'CONTENT_TYPE': 'text/plain',\n        'CONTENT_LENGTH': '123',\n    }\n    headers = request.headers\n\n    self.assertEqual(headers['Accept'], 'text/html')\n    self.assertEqual(headers['Accept-Charset'], 'utf-8')\n    self.assertEqual(headers['Host'], 'example.com')\n    self.assertEqual(headers['Content-Type'], 'text/plain')\n    self.assertEqual(headers['Content-Length'], '123')\n\n    # Accessing headers with underscores works too.\n    self.assertEqual(headers['accept_charset'], 'utf-8')\n    self.assertEqual(headers['content_length'], '123')\n", "def test_request_header_case_insensitivity(self):\n    request = HttpRequest()\n    request.META = {'HTTP_FOO_BAR': 'baz'}\n    headers = HttpHeaders(request.META)\n    self.assertEqual(headers['foo-bar'], 'baz')\n    self.assertEqual(headers['FOO-BAR'], 'baz')\n    self.assertEqual(headers['Foo-Bar'], 'baz')\n", "def test_request_get_full_path_info(self):\n    # Regression test for #17315\n    request = HttpRequest()\n    request.path_info = '/somepath/'\n    self.assertEqual(request.get_full_path_info(), '/somepath/')\n    self.assertEqual(request.get_full_path_info(force_append_slash=True), '/somepath/')\n    request.path_info = '/somepath'\n    self.assertEqual(request.get_full_path_info(), '/somepath')\n    self.assertEqual(request.get_full_path_info(force_append_slash=True), '/somepath/')\n", "def test_request_get_host(self):\n    factory = RequestFactory()\n    request = factory.get('/path', HTTP_HOST='example.com')\n    self.assertEqual(request.get_host(), 'example.com')\n\n    # Test case when ALLOWED_HOSTS is empty and DEBUG=True\n    with override_settings(ALLOWED_HOSTS=[], DEBUG=True):\n        request = factory.get('/path', HTTP_HOST='localhost')\n        self.assertEqual(request.get_host(), 'localhost')\n\n    # Test case when ALLOWED_HOSTS is set and domain matches\n    with override_settings(ALLOWED_HOSTS=['example.com']):\n        request = factory.get('/path', HTTP_HOST='example.com')\n        self.assertEqual(request.get_host(), 'example.com')\n\n    # Test case when ALLOWED_HOSTS is set and domain doesn't match\n    with override_settings(ALLOWED_HOSTS=['example2.com']):\n        request = factory.get('/path', HTTP_HOST='example.com')\n        with self.assertRaises(DisallowedHost):\n            request.get_host()\n"], "sample_311": ["    def test_admin_site(self):\n        admin = AdminSite()\n        self.assertEqual(admin.site_title, 'Django site admin')\n        self.assertEqual(admin.site_header, 'Django administration')\n        self.assertEqual(admin.index_title, 'Site administration')\n        self.assertEqual(admin.site_url, '/')\n", "def test_check_dependencies(self):\n        return [Error('Test error message', obj='TestModel')]\n\n        return [Error('Test error message', obj='TestModel')]\n\n    model_admin = ModelAdmin(models.Model, AdminSite())\n    with mock.patch.object(model_admin, 'check_dependencies', side_effect=mocked_check_dependencies):\n        errors = AdminSite().check(None)\n        self.assertEqual(len(errors), 1)\n        self.assertEqual(errors[0].msg, 'Test error message')\n\n    with mock.patch.object(ModelAdmin, 'checks_class', side_effect=mocked_checks_class):\n        errors = AdminSite().check(None)\n        self.assertEqual(len(errors), 1)\n        self.assertEqual(errors[0].msg, 'Test error message')\n\n    # Test that the check method doesn't raise an exception when\n    # a model is registered without a ModelAdmin instance.\n    admin_site = AdminSite()\n    admin_site._registry = {models.Model: None}\n    errors = admin_site.check(None)\n    self.assertEqual(errors, [])\n", "    def test_register_before_model_is_defined(self):\n        site = AdminSite()\n        site.register(Book)\n\n        # Registering the model before it's defined should raise AppRegistryNotReady.\n        with self.assertRaises(AppRegistryNotReady):\n            site.check(models.Book)\n", "def test_admin_site_view_on_site_for_model_with_provided_view_on_site(self):\n    \"\"\"\n    If view_on_site is defined on a model, the view on site link on the admin site\n    should point to the defined URL.\n    \"\"\"\n    p = Pizza.objects.create(name='Quattro Formaggi')\n\n    response = self.client.get(reverse('admin:admin_views_pizza_change', args=(p.pk,)))\n    self.assertContains(response, '/view-on-site-url/')\n\n    # Make sure the link is correct even when the model has a custom primary key.\n    m = ModelWithStringPrimaryKey.objects.create(key='123', name='Test')\n    response = self.client.get(reverse('admin:admin_views_modelwithstringprimarykey_change', args=(m.pk,)))\n    self.assertContains(response, '/view-on-site-url/')\n", "    def test_custom_admin_site(self):\n        custom_site = AdminSite(name='custom')\n        self.assertEqual(custom_site.name, 'custom')\n"], "sample_319": ["def test_generate_altered_db_table_comment(self):\n    changes = self.get_changes(\n        [self.author_with_db_table_comment],\n        [ModelState(\"testapp\", \"Author\", [(\"id\", models.AutoField(primary_key=True))])],\n    )\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterModelTableComment\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, table_comment=None)\n", "def test_alter_model_table_comment(self):\n    before = self.make_project_state([self.author_empty])\n    after = self.make_project_state([self.author_with_db_table_comment])\n    autodetector = MigrationAutodetector(before, after)\n    changes = autodetector._detect_changes()\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['AlterModelTableComment'])\n    self.assertOperationAttributes(\n        changes, 'testapp', 0, 0, name='Author', table_comment='Table comment'\n    )\n", "def test_generate_added_indexes(self):\n    \"\"\"\n    Test added indexes on existing models and fields (new and existing)\n    \"\"\"\n    before = self.make_project_state(\n        [\n            self.author_name,\n            self.book,\n        ]\n    )\n    after = self.make_project_state(\n        [\n            self.author_name,\n            ModelState(\n                \"otherapp\",\n                \"Book\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"author\", models.ForeignKey(\"testapp.Author\", models.CASCADE)),\n                    (\"title\", models.CharField(max_length=200)),\n                    (\"newfield\", models.IntegerField()),\n                ],\n                {\n                    \"indexes\": [\n                        models.Index(fields=[\"author\", \"title\"], name=\"book_title_idx\")\n                    ],\n                },\n            ),\n        ]\n    )\n    autodetector = MigrationAutodetector(before, after)\n    changes = autodetector._detect_changes()\n    # Right number of migrations?\n    self.assertNumberMigrations(changes, \"otherapp\", 1)\n    # Right operations?\n    self.assertOperationTypes(changes, \"otherapp\", 0, [\"AddField\", \"AddIndex\"])\n    self.assertOperationAttributes(changes, \"otherapp\", 0, 0, name=\"newfield\")\n    self.assertOperationAttributes(\n        changes,\n        \"otherapp\",\n        0,\n        1,\n        name=\"book_title_idx\",\n        fields=[\"author\", \"title\"],\n    )\n", "def test_autodetector_renamed_indexes(self):\n    # Create an index, rename it and make sure the new index is created and\n    # the old one removed.\n    operation = migrations.AddIndex(\n        name=\"my_index\",\n        index=models.Index(fields=[\"field1\", \"field2\"], name=\"my_index\"),\n    )\n    indexes = [operation.index]\n    project_state = self.make_project_state(\n        [\n            ModelState(\n                \"testapp\",\n                \"MyModel\",\n                [(\"id\", models.AutoField(primary_key=True))],\n                {\n                    \"indexes\": indexes,\n                },\n            )\n        ]\n    )\n    new_state = self.make_project_state(\n        [\n            ModelState(\n                \"testapp\",\n                \"MyModel\",\n                [(\"id\", models.AutoField(primary_key=True))],\n                {\n                    \"indexes\": [\n                        models.Index(fields=[\"field1\", \"field2\"], name=\"my_new_index\")\n                    ],\n                },\n            )\n        ]\n    )\n    changes = self.get_changes(project_state, new_state)\n    # Right number of migrations?\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    # Right number of AddIndex and RemoveIndex operations?\n    migration = changes[\"testapp\"][0]\n    self.assertEqual(len(migration.operations), 2)\n    self.assertEqual(migration.operations[0][0], \"RenameIndex\")\n    self.assertEqual(migration.operations[1][0], \"RenameIndex\")\n    self.assertEqual(migration.operations[0].old_name, \"my_index\")\n    self.assertEqual(migration.operations[0].new_name, \"my_new_index\")\n", "def test_generate_added_indexes(self):\n    \"\"\"Tests that adding an index generates the correct operations.\"\"\"\n    before = self.make_project_state([self.book])\n    after = self.make_project_state([self.book_indexes])\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, 'otherapp', 1)\n    self.assertOperationTypes(changes, 'otherapp', 0, [\"AddIndex\"])\n    self.assertOperationAttributes(changes, 'otherapp', 0, 0, name=\"book_title_author_idx\")\n"], "sample_320": ["    def test_simple(self):\n        operation = migrations.CreateModel(\n            \"Pony\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"pink\", models.BooleanField()),\n            ],\n        )\n        self.apply_operations(operation)\n\n        # Test state\n        model_state = self apps.get_model_state(\"testapp.Pony\")\n        self.assertEqual(model_state.fields[0][0], \"id\")\n        self.assertEqual(model_state.fields[1][0], \"pink\")\n        self.assertEqual(len(model_state.options), 0)\n\n        # Test database\n        with connection.cursor() as cursor:\n            cursor.execute(\"PRAGMA table_info(testapp_pony)\")\n            results = cursor.fetchall()\n            self.assertEqual(len(results), 2)\n            self.assertEqual(results[0][1], \"id\")\n            self.assertEqual(results[1][1], \"pink\")\n\n            cursor.execute(\"SELECT COUNT(*) FROM testapp_pony\")\n            self.assertEqual(cursor.fetchone()[0], 0)\n\n        # Make some changes and re-run\n        with connection.cursor() as cursor:\n            cursor.execute(\"INSERT INTO testapp_pony (pink) VALUES (1)\")\n            cursor.execute(\"INSERT INTO testapp_pony (pink) VALUES (0)\")\n\n        self.apply_operations(operation)\n\n        with connection.cursor() as cursor:\n            cursor.execute(\"SELECT COUNT(*) FROM testapp_pony\")\n            self.assertEqual(cursor.fetchone()[0], 2)\n", "    def test_create_model(self):\n        operation = migrations.CreateModel(\n            \"Pony\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=50)),\n            ],\n        )\n        self.apply_operations(operation)\n        self.assertModelExists(\"Pony\")\n        self.assertTableExists(\"Pony\")\n", "    def test_create_model(self):\n        operation = migrations.CreateModel(\n            name=\"Pony\",\n            fields=[\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"pink\", models.IntegerField(default=3)),\n            ],\n        )\n        # Test the state alteration\n        self.assertEqual(operation.state_forwards(\"tests\", ProjectState()), [\n            ModelState(\n                app_label=\"tests\",\n                name=\"Pony\",\n                fields=[\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"pink\", models.IntegerField(default=3)),\n                ],\n            )\n        ])\n        # Test the database alteration\n        self.apply_operations(operation)\n        self.assertTableExists(\"tests_pony\")\n        self.assertColumnExists(\"tests_pony\", \"pink\")\n        # And test reversal\n        self.unapply_operations(operation)\n        self.assertTableNotExists(\"tests_pony\")\n", "    def test_create_model(self):\n        project_state = self.set_up_test_model(\"test_model\")\n\n        # Create a new model\n        operation = migrations.CreateModel(\n            \"NewModel\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"field\", models.CharField(max_length=255)),\n            ],\n        )\n        new_state = project_state.clone()\n        operation.state_forwards(\"test_app\", new_state)\n\n        self.assertEqual(len(new_state.models), 2)\n        self.assertEqual(list(new_state.models[\"test_app\", \"newmodel\"].fields), [\"id\", \"field\"])\n\n        # Test the database\n        self.assertTableExists(\"test_app_newmodel\")\n        self.assertColumnExists(\"test_app_newmodel\", \"field\")\n\n        # Make sure the new model is in the app registry\n        self.apps.clear_cache()\n        self.assertTrue(self.apps.is_model_registered(\"test_app\", \"NewModel\"))\n        Model = self.apps.get_model(\"test_app\", \"NewModel\")\n        self.assertEqual(Model._meta.app_label, \"test_app\")\n\n        # Revert\n        self.unapply_all([operation])\n\n        self.assertEqual(len(project_state.models), 1)\n        self.assertNotIn((\"test_app\", \"newmodel\"), project_state.models)\n        self.assertTableNotExists(\"test_app_newmodel\")\n        self.assertFalse(self.apps.is_model_registered(\"test_app\", \"NewModel\"))\n", "    def test_rename_table(self):\n        # Test the state alteration\n        operation = migrations.AlterModelTable(\"migrations_book\", \"new_name\")\n        project_state = self.set_up_test_model(\"migrations\", \"book\")\n        new_state = project_state.clone()\n        operation.state_forwards(\"migrations\", new_state)\n        self.assertEqual(\n            new_state.models[\"migrations\", \"book\"].options[\"db_table\"], \"new_name\"\n        )\n\n        # Test the database alteration\n        self.apply_operations(operation)\n        self.assertEqual(self.get_table_name(\"migrations_book\"), \"new_name\")\n        self.assertTableColumns(\n            \"new_name\",\n            [\n                (\"id\", \"integer\"),\n                (\"author\", \"varchar(200)\"),\n                (\"title\", \"varchar(200)\"),\n            ],\n        )\n        self.unapply_operations(operation)\n        self.assertEqual(self.get_table_name(\"migrations_book\"), \"migrations_book\")\n        self.assertTableColumns(\n            \"migrations_book\",\n            [\n                (\"id\", \"integer\"),\n                (\"author\", \"varchar(200)\"),\n                (\"title\", \"varchar(200)\"),\n            ],\n        )\n"], "sample_387": ["    def test_charfield_with_textareawidget_override(self):\n        # Create a model with a CharField\n        class MyModel(models.Model):\n            myfield = models.CharField(max_length=10)\n\n        # Override the widget for the CharField to be a Textarea\n        overrides = {\n            models.CharField: {\"widget\": widgets.AdminTextareaWidget()},\n        }\n\n        # Check the formfield is correct\n        self.assertFormfield(MyModel, \"myfield\", widgets.AdminTextareaWidget, formfield_overrides=overrides)\n", "    def test_charfield_with_textinput_override(self):\n        model = Individual\n        fieldname = \"name\"\n        self.assertFormfield(\n            model,\n            fieldname,\n            widgets.AdminTextInputWidget,\n            formfield_overrides={CharField: {\"widget\": widgets.AdminTextInputWidget}},\n        )\n", "    def test_charfield_override(self):\n        model = Album\n        fieldname = \"name\"\n        widgetclass = admin.widgets.AdminTextInputWidget\n        override = {\"formfield_overrides\": {CharField: {\"widget\": widgetclass}}}\n\n        self.assertFormfield(model, fieldname, widgetclass, **override)\n", "def test_formfield_for_dbfield_with_raw_id_fields(self):\n    class MyModelAdmin(admin.ModelAdmin):\n        raw_id_fields = ('owner',)\n\n    class MyCarModelAdmin(admin.ModelAdmin):\n        raw_id_fields = ('make',)\n\n    class MyModel(models.Model):\n        owner = ForeignKey(User, on_delete=models.CASCADE)\n\n    class MyCarModel(models.Model):\n        make = ForeignKey(Car, on_delete=models.CASCADE)\n\n    ma = MyModelAdmin(MyModel, admin.site)\n    ff = ma.formfield_for_dbfield(MyModel._meta.get_field('owner'), request=None)\n    self.assertIsInstance(ff.widget, widgets.ForeignKeyRawIdWidget)\n\n    ma = MyCarModelAdmin(MyCarModel, admin.site)\n    ff = ma.formfield_for_dbfield(MyCarModel._meta.get_field('make'), request=None)\n    self.assertIsInstance(ff.widget, widgets.ForeignKeyRawIdWidget)\n", "    def test_charfield_with_textarea_override(self):\n        model = Album\n        fieldname = \"name\"\n        widgetclass = widgets.AdminTextareaWidget\n        admin_overrides = {\"formfield_overrides\": {CharField: {\"widget\": widgetclass}}}\n        self.assertFormfield(model, fieldname, widgetclass, **admin_overrides)\n"], "sample_389": ["    def test_http_request_repr(self):\n        request = HttpRequest()\n        request.method = 'GET'\n        request.path = '/path/'\n        self.assertEqual(repr(request), \"<HttpRequest: GET '/path/'>\")\n        ", "    def test_http_request_headers(self):\n        request = HttpRequest()\n        request.META = {\n            \"HTTP_HOST\": \"example.com\",\n            \"HTTP_ACCEPT\": \"text/html\",\n            \"HTTP_USER_AGENT\": \"Mozilla/5.0\",\n        }\n        self.assertEqual(request.headers[\"Host\"], \"example.com\")\n        self.assertEqual(request.headers[\"Accept\"], \"text/html\")\n        self.assertEqual(request.headers[\"User-Agent\"], \"Mozilla/5.0\")\n", "    def test_http_request_get_host(self):\n        request = HttpRequest()\n        request.META = {\n            'HTTP_HOST': 'example.com',\n            'SERVER_NAME': 'server_name',\n            'SERVER_PORT': '80',\n        }\n        self.assertEqual(request.get_host(), 'example.com')\n\n        request.META = {\n            'HTTP_HOST': '',\n            'SERVER_NAME': 'server_name',\n            'SERVER_PORT': '80',\n        }\n        self.assertEqual(request.get_host(), 'server_name')\n\n        request.META = {\n            'HTTP_HOST': '',\n            'SERVER_NAME': '',\n            'SERVER_PORT': '80',\n        }\n        with self.assertRaises(DisallowedHost):\n            request.get_host()\n", "    def test_http_request_repr(self):\n        request = HttpRequest()\n        request.method = \"GET\"\n        request.path = \"/path\"\n        self.assertEqual(repr(request), \"<HttpRequest: GET '/path'>\")\n", "    def test_http_request_get_host(self):\n        request = HttpRequest()\n        request.META = {\n            \"HTTP_X_FORWARDED_HOST\": \"example.com\",\n            \"HTTP_HOST\": \"other.com\",\n            \"SERVER_NAME\": \"server.com\",\n            \"SERVER_PORT\": \"80\",\n        }\n        with override_settings(USE_X_FORWARDED_HOST=True):\n            self.assertEqual(request.get_host(), \"example.com\")\n        with override_settings(USE_X_FORWARDED_HOST=False):\n            self.assertEqual(request.get_host(), \"other.com\")\n"], "sample_396": ["    def test_deferred_related_models(self):\n        # Test that related models are properly deferred when only() is used\n\n        related_field = (\n            RelatedObject.objects.select_related(\"related\")\n            .defer(\"related__data\")\n            .get()\n        )\n\n        with self.assertNumQueries(1):\n            self.assertIsNotNone(related_field.related_id)\n            with self.assertRaises(self.model_instance.DoesNotExist):\n                related_field.related.data\n", "def test_filtering_subqueries_with_outerref(self):\n    # Ticket 28767\n    Fan.objects.create(name=\"fan1\")\n    Fan.objects.create(name=\"fan1\")\n    Celeb.objects.create(name=\"celeb1\", greatest_fan_id=1)\n    sub = Fan.objects.filter(OuterRef(\"greatest_fan_id\")).order_by().values(\"id\")\n    celebs = Celeb.objects.annotate(has_greatest_fan=Exists(sub))\n    self.assertEqual(celebs.filter(has_greatest_fan=True).count(), 1)\n    self.assertEqual(celebs.filter(has_greatest_fan=False).count(), 0)\n", "def test_can_filter(self):\n    query = Item.objects.all().query\n    self.assertIs(query.can_filter(), True)\n\n    query = Item.objects.all()[:10].query\n    self.assertIs(query.can_filter(), False)\n\n    query = Item.objects.all()[5:10].query\n    self.assertIs(query.can_filter(), False)\n", "    def test_ticket_29882(self):\n        # A subquery on a model with a filtered relation should not crash.\n        Author.objects.filter(extra__note__misc=\"foo\").values(\"name\").annotate(\n            Count(\"id\")\n        )\n        with self.assertNumQueries(1):\n            list(Author.objects.filter(extra__note__misc=\"foo\").values(\"name\"))\n", "def test_distinct_on_with_annotate(self):\n    Author.objects.create(name=\"John\", num=1)\n    Author.objects.create(name=\"John\", num=2)\n    Author.objects.create(name=\"Jane\", num=3)\n\n    qs = Author.objects.annotate(alias_num=F(\"num\")).distinct(\"name\").order_by(\n        \"name\", \"-alias_num\"\n    )\n    self.assertEqual(qs.count(), 2)\n    self.assertEqual(qs[0].name, \"Jane\")\n    self.assertEqual(qs[0].num, 3)\n    self.assertEqual(qs[1].name, \"John\")\n    self.assertEqual(qs[1].num, 2)\n"], "sample_394": ["def test_model_admin_get_exclude(self):\n    class MyModelAdmin(admin.ModelAdmin):\n        exclude = (\"title\",)\n\n    ma = MyModelAdmin(Article, admin.site)\n    request = self.factory.get(\"/\")\n\n    self.assertEqual(ma.get_exclude(request), (\"title\",))\n\n    class MyModelAdmin(admin.ModelAdmin):\n            return (\"title\",)\n\n    ma = MyModelAdmin(Article, admin.site)\n    request = self.factory.get(\"/\")\n\n    self.assertEqual(ma.get_exclude(request), (\"title\",))\n", "def test_changelist_view_has_media(self):\n    response = self.client.get(reverse(\"admin:admin_views_section_changelist\"))\n    self.assertIsInstance(response.context[\"media\"], forms.Media)\n    self.assertTemplateUsed(response, \"admin/change_list.html\")\n\n    # check that the media required for the changelist is present\n    # in the response content\n    static_files = [\n        \"css/changelists.css\",\n        \"js/core.js\",\n        \"js/admin/RelatedObjectLookups.js\",\n        \"js/jquery.init.js\",\n        \"js/jquery/selector.js\",\n        \"js/jquery/jquery.js\",\n        \"js/actions.js\",\n    ]\n    for file in static_files:\n        self.assertContains(response, file)\n", "    def test_get_changelist_instance_with_invalid_lookup_parameters(self):\n        request = self.client.request().wsgi_request\n        request.GET = {\"o\": \"invalid\"}\n        with self.assertRaises(IncorrectLookupParameters):\n            self.s1._get_admin_instance().get_changelist_instance(request)\n", "def test_get_inline_formsets(self):\n    request = self.client.get(\"/admin/admin_views/section/1/change/\")\n    admin = site._registry[Section]\n    obj = Section.objects.get(pk=1)\n    formsets, inline_instances = admin.get_inline_formsets(request, None, inline_instances=admin.get_inline_instances(request, obj), obj=obj)\n    self.assertEqual(len(formsets), 1)\n    self.assertEqual(len(inline_instances), 1)\n    self.assertIsInstance(formsets[0], forms.BaseInlineFormSet)\n    self.assertIsInstance(inline_instances[0], admin.inlines[0])\n", "def test_changelist_view_uses_correct_template(self):\n    response = self.client.get(reverse(\"admin:admin_views_article_changelist\"))\n    self.assertEqual(response.status_code, 200)\n    self.assertTemplateUsed(response, \"admin/change_list.html\")\n\n    # Check that custom ChangeList templates are used, if provided.\n    custom_site = AdminSite(name=\"custom-admin\")\n    custom_site.register(Article)\n    request = self.factory.get(reverse(\"custom-admin:admin_views_article_changelist\"))\n    request.user = self.superuser\n    response = custom_site.admin_view(ArticleAdmin(CustomArticle, custom_site).changelist_view)(request)\n    self.assertEqual(response.status_code, 200)\n    self.assertTemplateUsed(response, \"admin/admin_views/custom_change_list.html\")\n"], "sample_400": ["def test_generate_altered_db_table(self):\n    # Make state\n    before = self.make_project_state([self.author_empty])\n    after = self.make_project_state([self.author_with_db_table_options])\n    autodetector = MigrationAutodetector(before, after)\n    changes = autodetector._detect_changes()\n    # Right number/types of operations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AlterModelTable\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"author\", table=\"author_one\")\n\n    before = self.make_project_state([self.author_with_db_table_options])\n    after = self.make_project_state([self.author_with_new_db_table_options])\n    autodetector = MigrationAutodetector(before, after)\n    changes = autodetector._detect_changes()\n    # Right number/types of operations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AlterModelTable\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"author\", table=\"author_two\")\n\n    before = self.make_project_state([self.author_with_db_table_options])\n    after = self.make_project_state([self.author_renamed_with_db_table_options])\n    autodetector = MigrationAutodetector(before, after)\n    changes = autodetector._detect_changes()\n    # Right number/types of operations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"RenameModel\"])\n\n    before = self.make_project_state([self.author_with_db_table_options])\n    after = self.make_project_state([self.author_renamed_with_new_db_table_options])\n    autodetector = MigrationAutodetector(before, after)\n    changes = autodetector._detect_changes()\n    # Right number/types of operations?\n    self.assertNumberMigrations(changes, 'testapp', 2)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"RenameModel\"])\n    self.assertOperationTypes(changes, 'testapp', 1, [\"AlterModelTable\"])\n    self.assertOperationAttributes(changes, 'testapp', 1, 0, name=\"newauthor\", table=\"author_three\")\n", "def test_rename_with_db_table(self):\n    # Test renaming with db_table on model\n    changes = self.get_changes(\n        [self.author_with_db_table_options],\n        [self.author_renamed_with_db_table_options],\n    )\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"RenameModel\"])\n    self.assertOperationAttributes(\n        changes, \"testapp\", 0, 0, old_name=\"Author\", new_name=\"NewAuthor\"\n    )\n\n    changes = self.get_changes(\n        [self.author_with_db_table_options],\n        [self.author_renamed_with_new_db_table_options],\n    )\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"RenameModel\"])\n    self.assertOperationAttributes(\n        changes, \"testapp\", 0, 0, old_name=\"Author\", new_name=\"NewAuthor\"\n    )\n\n    changes = self.get_changes(\n        [self.author_with_new_db_table_options],\n        [self.author_renamed_with_db_table_options],\n    )\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"RenameModel\"])\n    self.assertOperationAttributes(\n        changes, \"testapp\", 0, 0, old_name=\"Author\", new_name=\"NewAuthor\"\n    )\n", "def test_deep_deconstruct(self):\n    # Test basic deconstruction\n    field = models.IntegerField(\n        null=True, validators=[validate_slug, RegexValidator(regex=r\"^foo$\")]\n    )\n    deconstruction = MigrationAutodetector.deep_deconstruct(field)\n    self.assertEqual(\n        deconstruction,\n        (\n            \"django.db.models.IntegerField\",\n            [],\n            {\n                \"null\": True,\n                \"validators\": [\n                    (\n                        \"django.core.validators.validate_slug\",\n                        (),\n                        {},\n                    ),\n                    (\n                        \"django.core.validators.RegexValidator\",\n                        (),\n                        {\"regex\": re.compile(\"^foo$\")},\n                    ),\n                ],\n            },\n        ),\n    )\n\n    # Test deconstruction with lambdas.\n    field = models.IntegerField(default=lambda: 42)\n    with self.assertRaises(ValueError):\n        MigrationAutodetector.deep_deconstruct(field)\n", "def test_generate_added_indexes(self):\n    \"\"\"Test generating AddIndex operations.\"\"\"\n    model_state = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n        ],\n        {\n            \"indexes\": [\n                models.Index(fields=[\"name\"], name=\"author_name_idx\"),\n            ],\n        },\n    )\n    before_state = self.make_project_state([])\n    after_state = self.make_project_state([model_state])\n    changes = MigrationAutodetector(before_state, after_state)._detect_changes()\n    # Right number of migrations\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    # Right number of AddField operations\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"CreateModel\", \"AddIndex\"])\n    # Check AddIndex operation\n    create_model_operation, add_index_operation = changes[\"testapp\"][0].operations\n    self.assertEqual(create_model_operation.name, \"author\")\n    self.assertEqual(add_index_operation.name, \"author\")\n    self.assertEqual(\n        add_index_operation.index.name,\n        model_state.options[\"indexes\"][0].name,\n    )\n    self.assertEqual(\n        add_index_operation.index.fields,\n        model_state.options[\"indexes\"][0].fields,\n    )\n", "def test_alter_field_default_with_callable(self):\n    \"\"\"\n    Altering a field with a callable default should work.\n    \"\"\"\n    model_state = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200, default=lambda: \"Ada Lovelace\")),\n        ],\n    )\n    new_state = model_state.clone()\n    new_state.get_field(\"name\").default = lambda: \"Charles Babbage\"\n    changes = self.get_changes([model_state], [new_state])\n    self.assertEqual(len(changes[\"testapp\"]), 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterField\"])\n    self.assertOperationFieldAttributes(\n        changes, \"testapp\", 0, 0, default=lambda: \"Charles Babbage\"\n    )\n"], "sample_405": ["    def test_create_model(self):\n        operation = migrations.CreateModel(\n            name=\"Pony\",\n            fields=[\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"pink\", models.IntegerField()),\n            ],\n        )\n        project_state, apps = self.apply_operations([operation])\n        self.assertEqual(len(project_state.models), 1)\n        Pony = project_state.models[\"tests\", \"pony\"]\n        self.assertEqual(len(Pony.fields), 2)\n        id_field = Pony.fields[\"id\"]\n        self.assertEqual(id_field.atts[\"primary_key\"], True)\n        self.assertEqual(id_field.atts[\"name\"], \"id\")\n        pink_field = Pony.fields[\"pink\"]\n        self.assertEqual(pink_field.atts[\"name\"], \"pink\")\n        self.assertModelExists(apps, \"tests\", \"Pony\")\n        self.assertTableExists(\"tests_pony\")\n        self.assertColumnExists(\"tests_pony\", \"pink\")\n", "    def test_create_model(self):\n        operation = migrations.CreateModel(\n            \"Pony\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"pink\", models.IntegerField()),\n            ],\n        )\n        self.assertEqual(operation.model_name, \"Pony\")\n        self.assertEqual(operation.name, \"Pony\")\n        self.assertEqual(operation.fields, [(\"id\", models.AutoField(primary_key=True)), (\"pink\", models.IntegerField())])\n\n        project_state = self.set_up_test_model(\"test_app\")\n        new_state = project_state.clone()\n        operation.state_forwards(\"test_app\", new_state)\n        self.assertEqual(len(new_state.models[\"test_app\", \"pony\"].fields), 2)\n\n        self.assertTableExists(\"test_app_pony\")\n        self.assertTableNotExists(\"test_app_pony2\")\n\n        with connection.schema_editor() as editor:\n            operation.database_forwards(\"test_app\", editor, project_state, new_state)\n        self.assertTableExists(\"test_app_pony\")\n        self.assertTableExists(\"test_app_pony2\")\n\n        with connection.schema_editor() as editor:\n            operation.database_backwards(\"test_app\", editor, new_state, project_state)\n        self.assertTableExists(\"test_app_pony\")\n        self.assertTableNotExists(\"test_app_pony2\")\n", "    def test_create_model(self):\n        operation = migrations.CreateModel(\n            \"Pony\",\n            [\n                (\"pink\", models.IntegerField(primary_key=True)),\n                (\"weight\", models.FloatField(null=True)),\n            ],\n        )\n        # Test the state alteration\n        project_state = ProjectState()\n        new_state = project_state.clone()\n        operation.state_forwards(\"testapp\", new_state)\n        self.assertEqual(len(new_state.models[\"testapp\", \"pony\"].fields), 2)\n        # Test the database creation\n        self.apply_operations(\"testapp\", [operation])\n        with connection.cursor() as cursor:\n            cursor.execute(\n                \"SELECT name, sql FROM sqlite_master \"\n                \"WHERE type='table' AND name='testapp_pony'\"\n            )\n            row = cursor.fetchone()\n            self.assertIsNotNone(row)\n            cursor.execute(\n                \"PRAGMA table_info(testapp_pony)\"\n            )\n            rows = cursor.fetchall()\n            self.assertEqual(len(rows), 2)\n            self.assertEqual(rows[0][1], \"pink\")\n            self.assertEqual(rows[0][2], \"INTEGER\")\n            self.assertEqual(rows[1][1], \"weight\")\n            self.assertEqual(rows[1][2], \"REAL\")\n        # And test reversal\n        self.apply_operations(\"testapp\", [operation], reverse=True)\n        with connection.cursor() as cursor:\n            cursor.execute(\n                \"SELECT name, sql FROM sqlite_master \"\n                \"WHERE type='table' AND name='testapp_pony'\"\n            )\n            self.assertIsNone(cursor.fetchone())\n", "    def test_create_model(self):\n        operation = migrations.CreateModel(\n            name=\"Pony\",\n            fields=[\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"pink\", models.IntegerField()),\n            ],\n        )\n        # Test the state alteration\n        project_state = ProjectState()\n        new_state = project_state.clone()\n        operation.state_forwards(\"testapp\", new_state)\n        self.assertEqual(len(new_state.models[\"testapp\", \"pony\"].fields), 2)\n        # Test the database creation\n        self.apply_operations(operation)\n        self.assertEqual(\n            self.get_table_description(\"testapp_pony\"),\n            [\n                {\"name\": \"id\", \"type\": \"integer\", \"null\": False, \"unique\": True},\n                {\"name\": \"pink\", \"type\": \"integer\", \"null\": False, \"unique\": False},\n            ],\n        )\n", "    def test_create_model(self):\n        operation = migrations.CreateModel(\n            \"Pony\", [(\"name\", models.CharField(max_length=20))]\n        )\n        project_state, new_state = self.apply_operations(\n            ProjectState(), [operation], \"myapp\"\n        )\n        self.assertEqual(len(new_state.models[\"myapp\", \"pony\"].fields), 1)\n        self.assertEqual(\n            new_state.models[\"myapp\", \"pony\"].fields[0][0], \"id\"\n        )  # Auto-add of id\n\n        # Test the state alteration\n        self.assertEqual(len(project_state.models), 0)\n        self.assertEqual(len(new_state.models), 1)\n        self.assertEqual(new_state.models[\"myapp\", \"pony\"].name, \"Pony\")\n        self.assertEqual(len(new_state.models[\"myapp\", \"pony\"].fields), 2)\n        self.assertEqual(new_state.models[\"myapp\", \"pony\"].fields[0][0], \"id\")\n        self.assertEqual(new_state.models[\"myapp\", \"pony\"].fields[1][0], \"name\")\n\n        # Test the database creation\n        self.assertTableExists(\"myapp_pony\")\n        self.assertColumnExists(\"myapp_pony\", \"id\")\n        self.assertColumnExists(\"myapp_pony\", \"name\")\n"], "sample_408": ["def test_add_foreign_key_to_unmanaged_model(self):\n    # Create an unmanaged model with a foreign key to another unmanaged model.\n    app_label = \"testapp\"\n    model_name = \"unmanaged\"\n    field_name = \"fk_field\"\n\n    before_state = self.make_project_state([])\n    after_state = self.make_project_state(\n        [\n            ModelState(\n                app_label,\n                model_name,\n                fields=[\n                    (\n                        field_name,\n                        models.ForeignKey(\"testapp.AuthorUnmanaged\", models.CASCADE),\n                    ),\n                ],\n                options={\"managed\": False},\n            ),\n            self.author_unmanaged_default_pk,\n        ]\n    )\n    changes = self.get_changes(before_state, after_state)\n\n    # Assert that no migration is created for the unmanaged model.\n    self.assertNumberMigrations(changes, app_label, 0)\n", "def test_alter_index_together(self):\n    \"\"\"Tests index_together changes.\"\"\"\n    # Add index_together\n    changes = self.get_changes(\n        [self.author_name, self.book],\n        [self.author_name, self.book_foo_together],\n    )\n    self.assertNumberMigrations(changes, \"otherapp\", 1)\n    self.assertOperationTypes(changes, \"otherapp\", 0, [\"AlterIndexTogether\"])\n    self.assertOperationAttributes(\n        changes,\n        \"otherapp\",\n        0,\n        0,\n        name=\"book\",\n        index_together={(\"author\", \"title\")},\n    )\n\n    # Remove index_together\n    changes = self.get_changes(\n        [self.author_name, self.book_foo_together],\n        [self.author_name, self.book],\n    )\n    self.assertNumberMigrations(changes, \"otherapp\", 1)\n    self.assertOperationTypes(changes, \"otherapp\", 0, [\"AlterIndexTogether\"])\n    self.assertOperationAttributes(changes, \"otherapp\", 0, 0, name=\"book\", index_together=None)\n\n    # Alter index_together\n    changes = self.get_changes(\n        [self.author_name, self.book_foo_together],\n        [self.author_name, self.book_foo_together_2],\n    )\n    self.assertNumberMigrations(changes, \"otherapp\", 1)\n    self.assertOperationTypes(changes, \"otherapp\", 0, [\"AlterIndexTogether\"])\n    self.assertOperationAttributes(\n        changes,\n        \"otherapp\",\n        0,\n        0,\n        name=\"book\",\n        index_together={(\"title\", \"author\")},\n    )\n\n    # No index_together changes\n    changes = self.get_changes(\n        [self.author_name, self.book],\n        [self.author_name, self.book],\n    )\n    self.assertNumberMigrations(changes, \"otherapp\", 0)\n", "def test_create_rename_field(self):\n    \"\"\"Test creating a field and then renaming it.\"\"\"\n    before = self.make_project_state([self.author_empty])\n    after = self.make_project_state([self.author_name_renamed])\n    changes = self.get_changes(before, after)\n    # Right number of migrations\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    # Right number of AddField and RenameField operations\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AddField\", \"RenameField\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"names\")\n    self.assertOperationAttributes(\n        changes, \"testapp\", 0, 1, old_name=\"names\", new_name=\"name\"\n    )\n", "def test_alter_model_options_with_custom_permissions(self):\n    before = self.make_project_state([self.author_empty])\n    after = self.make_project_state(\n        [\n            ModelState(\n                \"testapp\",\n                \"Author\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                ],\n                {\n                    \"permissions\": [(\"can_hire\", \"Can hire\"), (\"can_fire\", \"Can fire\")],\n                },\n            )\n        ]\n    )\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertMigrationDependencies(changes, \"testapp\", 0, [])\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterModelOptions\"])\n    self.assertOperationAttributes(\n        changes,\n        \"testapp\",\n        0,\n        0,\n        options={\n            \"permissions\": [(\"can_hire\", \"Can hire\"), (\"can_fire\", \"Can fire\")],\n        },\n    )\n", "def test_deep_deconstruct_with_nested_deconstructible(self):\n    field = models.CharField(\n        max_length=200,\n        default=DeconstructibleObject(\n            DeconstructibleObject(1),\n            (\n                DeconstructibleObject(\"t1\"),\n                DeconstructibleObject(\"t2\"),\n            ),\n            a=DeconstructibleObject(\"A\"),\n            b=DeconstructibleObject(B=DeconstructibleObject(\"c\")),\n        ),\n    )\n    deconstructed = MigrationAutodetector.deep_deconstruct(field)\n    self.assertEqual(\n        deconstructed,\n        (\n            \"django.db.models.CharField\",\n            [],\n            {\n                \"max_length\": 200,\n                \"default\": (\n                    \"__main__.DeconstructibleObject\",\n                    (\n                        (\"__main__.DeconstructibleObject\", (1,), {}),\n                        (\n                            \"__main__.DeconstructibleObject\",\n                            (\"t1\",),\n                            {},\n                        ),\n                        (\n                            \"__main__.DeconstructibleObject\",\n                            (\"t2\",),\n                            {},\n                        ),\n                    ),\n                    {\n                        \"a\": (\"__main__.DeconstructibleObject\", (\"A\",), {}),\n                        \"b\": (\n                            \"__main__.DeconstructibleObject\",\n                            (),\n                            {\n                                \"B\": (\n                                    \"__main__.DeconstructibleObject\",\n                                    (\"c\",),\n                                    {},\n                                ),\n                            },\n                        ),\n                    },\n                ),\n            },\n        ),\n    )\n"], "sample_423": ["def test_alter_unique_together_with_field_rename(self):\n    changes = self.get_changes(\n        [\n            self.author_name,\n            self.book_unique_together,\n        ],\n        [\n            self.author_name_renamed,\n            self.book_unique_together,\n        ],\n    )\n    # The unique_together change should come after the field rename\n    self.assertOperationTypes(changes, \"otherapp\", 0, [\"AlterUniqueTogether\"])\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"RenameField\"])\n    self.assertOperationTypes(changes, \"otherapp\", 1, [\"AlterUniqueTogether\"])\n", "def test_alter_index_reorder(self):\n    changes = self.get_changes(\n        [self.book_indexes],\n        [self.book_unordered_indexes],\n    )\n    self.assertNumberMigrations(changes, \"otherapp\", 1)\n    self.assertOperationTypes(changes, \"otherapp\", 0, [\"AlterIndex\"])\n    self.assertOperationAttributes(\n        changes, \"otherapp\", 0, 0, name=\"book_title_author_idx\", fields=[\"title\", \"author\"]\n    )\n", "def test_alter_m2m_add_through(self):\n    \"\"\"\n    Changing the through model on a ManyToManyField should create a new\n    model and then the ManyToManyField through model should be altered.\n    \"\"\"\n    # Make state\n    model_state = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\n                \"publishers\",\n                models.ManyToManyField(\n                    \"testapp.Publisher\", through=\"testapp.Contract\"\n                ),\n            ),\n        ],\n    )\n    new_state = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\n                \"publishers\",\n                models.ManyToManyField(\n                    \"testapp.Publisher\", through=\"testapp.Deal\"\n                ),\n            ),\n        ],\n    )\n    changes = self.get_changes([model_state], [new_state])\n    # Right number/type of operations?\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"RenameModel\", \"AlterField\"])\n    self.assertOperationAttributes(\n        changes, \"testapp\", 0, 0, old_name=\"Contract\", new_name=\"Deal\"\n    )\n    self.assertOperationAttributes(\n        changes,\n        \"testapp\",\n        0,\n        1,\n        model_name=\"Author\",\n        name=\"publishers\",\n        field=models.ManyToManyField(\"testapp.Publisher\", through=\"testapp.Deal\"),\n    )\n", "def test_alter_unique_together_unchanged(self):\n    \"Since #24307, unique_together detection is disabled when it's unchanged\"\n    # Make a model with an unchanged unique_together\n    model_state = ModelState(\n        \"testapp\",\n        \"Thing\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=100)),\n        ],\n        {\n            \"unique_together\": {(\"name\",)},\n        },\n    )\n    new_state = project_state = self.make_project_state([model_state])\n    # Evolve it\n    new_state = self.apply_operations(\n        new_state,\n        operations=[\n            migrations.AlterUniqueTogether(\n                name=\"Thing\",\n                unique_together={(\"name\",)},\n            ),\n        ],\n    )\n    # Check that no operation is made\n    changes = self.get_changes(project_state, new_state)\n    self.assertEqual(changes, {})\n", "def test_generate_altered_indexes(self):\n    # Create model state\n    model_state = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n        ],\n        {\n            \"indexes\": [\n                models.Index(fields=[\"name\"], name=\"author_name_idx\"),\n            ],\n        },\n    )\n    # Altered model state\n    altered_model_state = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n        ],\n        {\n            \"indexes\": [\n                models.Index(fields=[\"name\"], name=\"author_name_idx_altered\"),\n            ],\n        },\n    )\n    # Create project state\n    project_state = self.make_project_state([model_state])\n    # Create altered project state\n    altered_project_state = self.make_project_state([altered_model_state])\n    # Autodetect changes\n    changes = self.get_changes(project_state, altered_project_state)\n    # Assert changes\n    self.assertEqual(len(changes[\"testapp\"]), 1)\n    migration = changes[\"testapp\"][0]\n    self.assertEqual(len(migration.operations), 2)\n    self.assertIsInstance(migration.operations[0], operations.RemoveIndex)\n    self.assertEqual(migration.operations[0].name, \"author_name_idx\")\n    self.assertIsInstance(migration.operations[1], operations.AddIndex)\n    self.assertEqual(migration.operations[1].index.name, \"author_name_idx_altered\")\n"], "sample_424": ["    def test_create_model(self):\n        operation = migrations.CreateModel(\n            name=\"Pony\",\n            fields=[\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"pink\", models.IntegerField()),\n            ],\n        )\n        project_state, apps = self.apply_operations([operation])\n        self.assertEqual(apps.get_model(\"testapp\", \"Pony\")._meta.get_field(\"pink\").db_column, \"pink\")\n\n        # Test reversal\n        self.unapply_operations([operation])\n        self.assertEqual(len(apps.get_app_config(\"testapp\").models), 0)\n", "def test_create_model_m2m_with_custom_through(self):\n    \"\"\"\n    Creating a model with an m2m field with a custom through model works.\n    \"\"\"\n    model_name = \"test_model\"\n    operation = migrations.CreateModel(\n        model_name,\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\n                \"m2m\",\n                models.ManyToManyField(\"auth.User\", through=\"testapp.CustomThroughModel\"),\n            ),\n        ],\n    )\n    self.apply_operations([operation])\n    self.assertModelExists(model_name)\n    model = self/apps.get_model(\"testapp\", model_name)\n    self.assertEqual(model._meta.get_field(\"m2m\").through._meta.object_name, \"CustomThroughModel\")\n", "def test_add_index(self):\n    operation = migrations.AddIndex(\n        model_name=\"unicode\",\n        index=models.Index(fields=[\"charfield\"], name=\"my_index\"),\n    )\n    self.apply_operations([operation])\n    with connection.cursor() as cursor:\n        cursor.execute(\n            \"SELECT INDEX_NAME FROM INFORMATION_SCHEMA.STATISTICS \"\n            \"WHERE TABLE_NAME = %s AND INDEX_NAME = %s\",\n            [self.table_name, \"my_index\"],\n        )\n        self.assertEqual(cursor.fetchone()[0], \"my_index\")\n    self.unapply_operations([operation])\n    with connection.cursor() as cursor:\n        cursor.execute(\n            \"SELECT INDEX_NAME FROM INFORMATION_SCHEMA.STATISTICS \"\n            \"WHERE TABLE_NAME = %s AND INDEX_NAME = %s\",\n            [self.table_name, \"my_index\"],\n        )\n        self.assertIsNone(cursor.fetchone())\n", "    def test_rename_model(self):\n        project_state = self.set_up_test_model(\"test_model\")\n        new_state = project_state.clone()\n        operation = migrations.RenameModel(\"test_model\", \"test_model_renamed\")\n        operation.state_forwards(\"test_app\", new_state)\n        self.assertEqual(\n            new_state.models[\"test_app\", \"test_model_renamed\"].name, \"TestModel\"\n        )\n        self.assertColumnExists(\n            new_state.apps.get_model(\"test_app\", \"test_model_renamed\"), \"id\"\n        )\n\n        self.assertEqual(\n            len(\n                new_state.models[\"test_app\", \"test_model_renamed\"].fields\n            ),\n            2,\n        )\n\n        # Test the state alteration\n        self.assertModelExists(new_state, \"test_app\", \"test_model_renamed\")\n        self.assertModelDoesNotExist(new_state, \"test_app\", \"test_model\")\n\n        # Make the schema edits\n        with connection.schema_editor() as editor:\n            operation.database_forwards(\"test_app\", editor, project_state, new_state)\n\n        # Test the database schema\n        self.assertTableExists(\"test_app_test_model_renamed\")\n        self.assertTableDoesNotExist(\"test_app_test_model\")\n        self.assertColumnExists(\n            new_state.apps.get_model(\"test_app\", \"test_model_renamed\"), \"id\"\n        )\n\n        with self.assertRaises(FieldDoesNotExist):\n            self.assertColumnExists(\n                new_state.apps.get_model(\"test_app\", \"test_model\"), \"id\"\n            )\n\n        # Make the schema edits to go back to the previous model\n        with connection.schema_editor() as editor:\n            operation.database_backwards(\n                \"test_app\", editor, from_state=new_state, to_state=project_state\n            )\n\n        # Test the database schema\n        self.assertTableDoesNotExist(\"test_app_test_model_renamed\")\n        self.assertTableExists(\"test_app_test_model\")\n        self.assertColumnExists(\n            new_state.apps.get_model(\"test_app\", \"test_model\"), \"id\"\n        )\n\n        with self.assertRaises(FieldDoesNotExist):\n            self.assertColumnExists(\n                new_state.apps.get_model(\"test_app\", \"test_model_renamed\"), \"id\"\n            )\n", "    def test_create_model(self):\n        operation = migrations.CreateModel(\n            \"Pony\",\n            [\n                (\"pink\", models.IntegerField()),\n            ],\n        )\n        self.apply_operations([operation])\n        Pony = self.apps.get_model(\"testapp\", \"Pony\")\n        pony = Pony.objects.create(pink=1)\n        self.assertEqual(pony.pink, 1)\n"], "sample_430": ["    def test_generate_added_indexes(self):\n        project_state = self.make_project_state(\n            [\n                self.author_name,\n                self.publisher,\n                self.book_unique_together,\n            ]\n        )\n        new_state = self.make_project_state(\n            [\n                self.author_name,\n                self.publisher,\n                self.book_unique_together_2,\n                self.book_indexes,\n            ]\n        )\n        autodetector = MigrationAutodetector(\n            project_state,\n            new_state,\n        )\n        changes = autodetector._detect_changes()\n        # Right number of migrations?\n        self.assertNumberMigrations(changes, 'otherapp', 1)\n        # Right number of AddIndex operations?\n        self.assertOperationTypes(changes, 'otherapp', 0, [\"AddIndex\"])\n        # Right fields on AddIndex operation?\n        add_index = changes['otherapp'][0].operations[0]\n        self.assertEqual(\n            add_index.index.name, \"book_title_author_idx\"\n        )\n        self.assertEqual(\n            [x for x, _ in add_index.index.fields], [\"author\", \"title\"]\n        )\n", "def test_alter_model_options(self):\n    \"\"\"Tests that altering model options results in an AlterModelOptions operation.\"\"\"\n    # Test altering permissions.\n    changes = self.get_changes(\n        [self.author_empty],\n        [ModelState(\"testapp\", \"Author\", [], {\"permissions\": [(\"can_hire\", \"Can hire\")]})],\n    )\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterModelOptions\"])\n    self.assertOperationAttributes(\n        changes,\n        \"testapp\",\n        0,\n        0,\n        options={\"permissions\": [(\"can_hire\", \"Can hire\")]},\n    )\n\n    # Test altering a model's verbose name.\n    changes = self.get_changes(\n        [self.author_empty],\n        [ModelState(\"testapp\", \"Author\", [], {\"verbose_name\": \"Authi\"})],\n    )\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterModelOptions\"])\n    self.assertOperationAttributes(\n        changes,\n        \"testapp\",\n        0,\n        0,\n        options={\"verbose_name\": \"Authi\"},\n    )\n\n    # Test altering multiple model options at the same time.\n    changes = self.get_changes([self.author_empty], [self.author_with_options])\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterModelOptions\"])\n    self.assertOperationAttributes(\n        changes,\n        \"testapp\",\n        0,\n        0,\n        options={\"permissions\": [(\"can_hire\", \"Can hire\")], \"verbose_name\": \"Authi\"},\n    )\n", "def test_generate_added_index_together(self):\n    # Test addding an index_together\n    changes = self.get_changes([self.book], [self.book_unique_together])\n    self.assertNumberMigrations(changes, \"otherapp\", 1)\n    self.assertOperationTypes(changes, \"otherapp\", 0, [\"AlterUniqueTogether\"])\n    self.assertOperationAttributes(\n        changes,\n        \"otherapp\",\n        0,\n        0,\n        name=\"book\",\n        unique_together={(\"author\", \"title\")},\n    )\n\n    # Test reversing order of the same index_together\n    changes = self.get_changes([self.book_unique_together], [self.book_unique_together_2])\n    self.assertNumberMigrations(changes, \"otherapp\", 1)\n    self.assertOperationTypes(changes, \"otherapp\", 0, [\"AlterUniqueTogether\"])\n    self.assertOperationAttributes(\n        changes,\n        \"otherapp\",\n        0,\n        0,\n        name=\"book\",\n        unique_together={(\"title\", \"author\")},\n    )\n\n    # Test swapping one index_together for another\n    changes = self.get_changes([self.book_unique_together_2], [self.book_unique_together_3])\n    self.assertNumberMigrations(changes, \"otherapp\", 1)\n    self.assertOperationTypes(changes, \"otherapp\", 0, [\"AlterUniqueTogether\"])\n    self.assertOperationAttributes(\n        changes,\n        \"otherapp\",\n        0,\n        0,\n        name=\"book\",\n        unique_together={(\"title\", \"newfield\")},\n    )\n\n    # Test removing an index_together\n    changes = self.get_changes([self.book_unique_together_3], [self.book])\n    self.assertNumberMigrations(changes, \"otherapp\", 1)\n    self.assertOperationTypes(changes, \"otherapp\", 0, [\"AlterUniqueTogether\"])\n    self.assertOperationAttributes(changes, \"otherapp\", 0, 0, name=\"book\")\n\n    # Test adding index_together() to a model with a unique_together\n    changes = self.get_changes([self.book_unique_together], [self.book_indexes])\n    self.assertNumberMigrations(changes, \"otherapp\", 1)\n    self.assertOperationTypes(changes, \"otherapp\", 0, [\"AddIndex\"])\n    self.assertOperationAttributes(\n        changes,\n        \"otherapp\",\n        0,\n        0,\n        name=\"book\",\n    )\n", "def test_generate_added_indexes(self):\n    \"\"\"\n    Tests the addition of indexes.\n    \"\"\"\n    changes = self.get_changes(\n        [self.author_name],\n        [ModelState(\"testapp\", \"Author\", [(\"id\", models.AutoField(primary_key=True))], {\n            \"indexes\": [\n                models.Index(fields=[\"name\"], name=\"author_name_idx\"),\n            ],\n        })],\n    )\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AddIndex\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"author_name_idx\")\n", "def test_altered_db_table_comment(self):\n    project_state = self.make_project_state(\n        [\n            self.author_with_db_table_comment,\n        ]\n    )\n    after_state = self.make_project_state(\n        [\n            ModelState(\n                \"testapp\",\n                \"Author\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                ],\n                options={\n                    \"db_table_comment\": \"New table comment\",\n                },\n            ),\n        ]\n    )\n    changes = self.get_changes(project_state, after_state)\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterModelTableComment\"])\n    self.assertOperationAttributes(\n        changes,\n        \"testapp\",\n        0,\n        0,\n        name=\"author\",\n        table_comment=\"New table comment\",\n    )\n"], "sample_439": ["def test_base_form_add_prefix(self):\n    # Test that BaseForm.add_prefix doesn't modify the field name when no prefix is given\n    form = Person()\n    self.assertEqual(form.add_prefix(\"first_name\"), \"first_name\")\n\n    # Test that BaseForm.add_prefix adds the prefix to the field name\n    form = Person(prefix=\"person\")\n    self.assertEqual(form.add_prefix(\"first_name\"), \"person-first_name\")\n\n    # Test that BaseForm.add_prefix works with field names that already have a prefix\n    form = Person(prefix=\"person\")\n    self.assertEqual(form.add_prefix(\"person-first_name\"), \"person-first_name\")\n", "def test_form_add_prefix(self):\n    # Test prefix construction\n    form1 = FrameworkForm(prefix=\"test\")\n    self.assertEqual(form1.add_prefix(\"name\"), \"test-name\")\n\n    form2 = FrameworkForm(prefix=None)\n    self.assertEqual(form2.add_prefix(\"name\"), \"name\")\n\n    form3 = FrameworkForm(prefix=\"\")\n    self.assertEqual(form3.add_prefix(\"name\"), \"name\")\n\n    form4 = FrameworkForm()\n    self.assertEqual(form4.add_prefix(\"name\"), \"name\")\n\n    form5 = FrameworkForm(prefix=\"test\")\n    self.assertEqual(form5.add_initial_prefix(\"name\"), \"initial-test-name\")\n\n    form6 = FrameworkForm(prefix=None)\n    self.assertEqual(form6.add_initial_prefix(\"name\"), \"initial-name\")\n\n    form7 = FrameworkForm(prefix=\"\")\n    self.assertEqual(form7.add_initial_prefix(\"name\"), \"initial-name\")\n\n    form8 = FrameworkForm()\n    self.assertEqual(form8.add_initial_prefix(\"name\"), \"initial-name\")\n", "def test_form_initial(self):\n    # Initial data is passed to the Field's initial argument\n    form = Person(initial={\"first_name\": \"John\", \"birthday\": datetime.date(1980, 4, 12)})\n    self.assertEqual(form.fields[\"first_name\"].initial, \"John\")\n    self.assertEqual(form.fields[\"birthday\"].initial, datetime.date(1980, 4, 12))\n\n    # If the initial data is callable, it's called when needed\n        return \"Jane\"\n\n    form = Person(initial={\"first_name\": get_initial})\n    self.assertEqual(form.fields[\"first_name\"].initial, \"Jane\")\n\n    # Initial data is also used to set the initial values for widgets\n    form = PersonNew(initial={\"first_name\": \"John\", \"last_name\": \"Doe\"})\n    self.assertEqual(form[\"first_name\"].value(), \"John\")\n    self.assertEqual(form[\"last_name\"].value(), \"Doe\")\n    self.assertEqual(form[\"birthday\"].value(), \"\")\n\n    # Initial data is used to generate HTML\n    form = Person(initial={\"first_name\": \"John\"})\n    self.assertHTMLEqual(\n        form.as_ul(),\n        \"\"\"\n        <li><label for=\"id_first_name\">First name:</label> <input id=\"id_first_name\" name=\"first_name\" type=\"text\" value=\"John\" /></li>\n        <li><label for=\"id_last_name\">Last name:</label> <input id=\"id_last_name\" name=\"last_name\" type=\"text\" /></li>\n        <li><label for=\"id_birthday\">Birthday:</label> <input id=\"id_birthday\" name=\"birthday\" type=\"date\" /></li>\n        \"\"\",\n    )\n", "def test_form_add_prefix(self):\n    # Test prefix construction\n    form1 = FrameworkForm(prefix='framework')\n    self.assertEqual(form1.add_prefix('name'), 'framework-name')\n    self.assertEqual(form1.add_prefix('language'), 'framework-language')\n\n    form2 = FrameworkForm(prefix='test-form')\n    self.assertEqual(form2.add_prefix('name'), 'test-form-name')\n    self.assertEqual(form2.add_prefix('language'), 'test-form-language')\n\n    form3 = FrameworkForm()\n    self.assertEqual(form3.add_prefix('name'), 'name')\n    self.assertEqual(form3.add_prefix('language'), 'language')\n", "def test_form_add_prefix(self):\n    # Test that Form.add_prefix() method works properly\n    form = Person(prefix=\"person\")\n    self.assertEqual(form.add_prefix(\"first_name\"), \"person-first_name\")\n\n    form = Person(prefix=None)\n    self.assertEqual(form.add_prefix(\"first_name\"), \"first_name\")\n"], "sample_452": ["    def test_create_model(self):\n        operation = migrations.CreateModel(\n            \"Pony\",\n            [\n                (\"pink\", models.IntegerField(default=1)),\n                (\"weight\", models.FloatField(null=True)),\n            ],\n        )\n        self.apply_operations(operation)\n        Pony = selfapps.get_model(\"Pony\")\n        self.assertEqual(Pony._meta.get_field(\"pink\").default, 1)\n        self.assertEqual(Pony._meta.get_field(\"weight\").null, True)\n", "    def test_create_model(self):\n        operation = migrations.CreateModel(\n            name=\"Pony\",\n            fields=[\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"pink\", models.IntegerField()),\n            ],\n        )\n        self.apply_operations([operation])\n        self.assertTableExists(\"pony\")\n        self.assertColumnExists(\"pony\", \"pink\")\n        self.assertColumnNotNull(\"pony\", \"pink\")\n", "    def test_create_model(self):\n        # Test the state alteration\n        operation = migrations.CreateModel(\n            \"Pony\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"pink\", models.IntegerField(default=1)),\n            ],\n        )\n        project_state, _ = self.apply_operations(\"testapp\", [operation])\n        self.assertEqual(\n            project_state.models[\"testapp\", \"pony\"].fields,\n            [\n                ModelState.Field(\n                    app_label=\"testapp\",\n                    name=\"id\",\n                    max_length=None,\n                    is_relation=False,\n                    relation_to=None,\n                    related_model=None,\n                    related_name=None,\n                    related_object=None,\n                ),\n                ModelState.Field(\n                    app_label=\"testapp\",\n                    name=\"pink\",\n                    max_length=None,\n                    is_relation=False,\n                    relation_to=None,\n                    related_model=None,\n                    related_name=None,\n                    related_object=None,\n                ),\n            ],\n        )\n        # Now for the database tests\n        self.assertTableExists(\"testapp_pony\")\n        with connection.cursor() as cursor:\n            cursor.execute(\n                self.connection.ops.select_columns(\"testapp_pony\", [\"id\", \"pink\"])\n            )\n            rows = cursor.fetchall()\n            self.assertEqual(rows, [])\n        # And backwards\n        self.unapply_operations(\"testapp\", [operation])\n        self.assertTableNotExists(\"testapp_pony\")\n", "    def test_create_model(self):\n        operation = migrations.CreateModel(\n            \"Pony\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"pink\", models.IntegerField(default=42)),\n            ],\n        )\n        self.apply_operations(operation)\n        Pony = self.apps.get_model(\"testapp\", \"Pony\")\n        pony = Pony.objects.create()\n        self.assertEqual(pony.id, 1)\n        self.assertEqual(pony.pink, 42)\n", "    def test_create_model(self):\n        operation = migrations.CreateModel(\n            \"Pony\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"pink\", models.IntegerField(default=1)),\n            ],\n        )\n        self.apply_operations([operation])\n        self.assertTableExists(\"pony\")\n        self.assertColumnExists(\"pony\", \"pink\")\n\n        # Test reversing\n        self.unapply_operations([operation])\n        self.assertTableNotExists(\"pony\")\n"], "sample_454": ["    def test_create_exclusion_constraint(self):\n        constraint = ExclusionConstraint(\n            name=\"test_constraint\",\n            expressions=[(\"field1\", \"=\"), (\"field2\", \"<>\")]\n        )\n        sql = constraint.create_sql(Product, connection.schema_editor())\n        self.assertIn(\"CONSTRAINT test_constraint EXCLUDE USING GIST\", sql)\n        self.assertIn(\"(field1 WITH =\", sql)\n        self.assertIn(\", field2 WITH <>\", sql)\n", "    def test_create_exclusion_constraint(self):\n        class ExclusionModel(models.Model):\n            name = models.CharField(max_length=10)\n            class Meta:\n                constraints = [\n                    ExclusionConstraint(\n                        name='%(app_label)s_%(class)s_name_exclusion',\n                        expressions=[('name', '&&')],\n                    ),\n                ]\n\n        with connection.schema_editor() as schema_editor:\n            schema_editor.create_model(ExclusionModel)\n\n        constraints = get_constraints(ExclusionModel._meta.db_table)\n        self.assertIn('name_exclusion', [c['name'] for c in constraints])\n", "    def test_create_exclusion_constraint(self):\n        constraint = ExclusionConstraint(\n            name=\"exclude_overlapping_schedule\",\n            expressions=[(\"start\", \"=\"), (\"end\", \"=\")],\n        )\n        model = models.Model()\n        model._meta.db_table = \"my_table\"\n        schema_editor = connection.schema_editor()\n        create_sql = constraint.create_sql(model, schema_editor)\n        self.assertEqual(\n            str(create_sql),\n            'ALTER TABLE \"my_table\" ADD CONSTRAINT \"exclude_overlapping_schedule\" EXCLUDE USING GIST (start WITH =, end WITH =)',\n        )\n", "    def test_create_exclusion_constraint(self):\n        model = UniqueConstraintProduct\n        constraint = ExclusionConstraint(\n            name=\"exclude_test\",\n            expressions=[(\"name\", \"=\"), (\"price\", \"=\")],\n        )\n        with connection.schema_editor() as schema_editor:\n            schema_editor.add_constraint(model, constraint)\n        constraints = get_constraints(model._meta.db_table)\n        self.assertIn(\"exclude_test\", [c[\"name\"] for c in constraints])\n        self.assertEqual(\n            constraints[\"exclude_test\"][\"type\"], ExclusionConstraint.constraint_type\n        )\n", "    def test_create_exclusion_constraint(self):\n        with atomic():\n            class ExclusionConstraintModel(models.Model):\n                field1 = models.IntegerField()\n                field2 = models.IntegerField()\n\n                class Meta:\n                    constraints = [\n                        ExclusionConstraint(\n                            name=\"exclusion_constraint\",\n                            expressions=[(\"field1\", \"=\"), (\"field2\", \"=\")],\n                        )\n                    ]\n\n            ExclusionConstraintModel._meta.db_table = \"exclusion_constraint_tests\"\n\n            with connection.schema_editor() as schema_editor:\n                schema_editor.create_model(ExclusionConstraintModel)\n\n            constraints = get_constraints(ExclusionConstraintModel._meta.db_table)\n            self.assertIn(\n                \"exclusion_constraint\",\n                [constraint.name for constraint in constraints],\n            )\n\n            with self.assertRaises(IntegrityError):\n                ExclusionConstraintModel.objects.create(field1=1, field2=1)\n                ExclusionConstraintModel.objects.create(field1=1, field2=1)\n"], "sample_460": ["    def test_index_view(self):\n        response = self.client.get(reverse(\"admin:index\"))\n        self.assertEqual(response.status_code, 200)\n        self.assertTemplateUsed(response, \"admin/index.html\")\n", "def test_get_urls(self):\n    \"\"\"Test get_urls method of AdminSite.\"\"\"\n    site = AdminSite()\n    urls = site.get_urls()\n    self.assertTrue(isinstance(urls, list))\n    self.assertTrue(all(isinstance(url, tuple) or hasattr(url, 'resolve') for url in urls))\n    self.assertTrue(any(url.name == 'index' for url in urls if hasattr(url, 'name')))\n    self.assertTrue(any(url.name == 'login' for url in urls if hasattr(url, 'name')))\n", "    def setUp(self):\n        self.site = AdminSite()\n", "    def test_admin_urls(self):\n        admin = site\n\n        # Make sure the admin_base_url is valid.\n        try:\n            admin_urls = admin.urls\n        except NoReverseMatch:\n            self.fail(\"admin_base_url is not a valid URL.\")\n\n        # Check that the admin base url is in the urls.\n        admin_base_url = admin.urls[0]\n        self.assertIn(admin_base_url, admin.urls)\n\n        # Check that each model in the admin has a valid url.\n        for model, model_admin in admin._registry.items():\n            try:\n                model_admin.urls\n            except NoReverseMatch:\n                self.fail(f\"Model '{model}' admin has an invalid URL.\")\n", "    def setUp(self):\n        self.site = AdminSite()\n"], "sample_463": ["def test_alter_model_table_with_rename(self):\n    # Test altering a model with db_table and renaming\n    changes = self.get_changes(\n        [\n            self.author_with_db_table_options,\n        ],\n        [\n            self.author_renamed_with_db_table_options,\n        ],\n    )\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"RenameModel\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, new_name=\"NewAuthor\")\n", "def test_swappableicine(self):\n    changes = self.get_changes(\n        [\n            self.author_with_user,\n            self.custom_user\n        ],\n        [\n            self.author_with_custom_user,\n            self.custom_user\n        ],\n        MigrationQuestioner({\"ask_rename\": True}),\n    )\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AlterField\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name='user')\n", "def test_alter_index(self):\n    # Make state\n    before = ModelState(\"migrations\", \"Author\", [\n        (\"id\", models.AutoField(primary_key=True)),\n    ], {\n        \"indexes\": [models.Index(fields=[\"id\"], name=\"author_id_idx\")]\n    })\n    after = ModelState(\"migrations\", \"Author\", [\n        (\"id\", models.AutoField(primary_key=True)),\n    ], {\n        \"indexes\": [models.Index(fields=[\"id\"], name=\"author_new_idx\")]\n    })\n    # Test\n    changes = self.get_changes([before], [after])\n    # Assert\n    self.assertNumberMigrations(changes, \"migrations\", 1)\n    self.assertOperationTypes(changes, \"migrations\", 0, [\"RenameIndex\"])\n    self.assertOperationAttributes(changes, \"migrations\", 0, 0, new_name=\"author_new_idx\", old_name=\"author_id_idx\")\n", "def test_alter_unique_together(self):\n    before = self.make_project_state(\n        [\n            self.author_name,\n            self.book_unique_together,\n        ]\n    )\n    after = self.make_project_state(\n        [\n            self.author_name,\n            self.book_unique_together_2,\n        ]\n    )\n    autodetector = MigrationAutodetector(before, after)\n    changes = autodetector._detect_changes()\n    self.assertEqual(len(changes[\"otherapp\"]), 1)\n    self.assertEqual(changes[\"otherapp\"][0].operations, [])\n    self.assertOperationTypes(changes, \"otherapp\", 0, [])\n", "def test_detect_foreign_key_to_swappable_model(self):\n    changes = self.get_changes(\n        [\n            self.author_empty,\n            self.book_with_author,\n            self.custom_user_no_inherit,\n        ],\n        [\n            self.author_empty,\n            self.book_with_author,\n            self.custom_user,\n        ],\n    )\n    self.assertNumberMigrations(changes, \"thirdapp\", 1)\n    self.assertOperationTypes(changes, \"thirdapp\", 0, [\"AlterBasesOptions\"])\n    self.assertOperationAttributes(\n        changes,\n        \"thirdapp\",\n        0,\n        0,\n        bases=(AbstractBaseUser,),\n    )\n"], "sample_491": ["def test_boundfield_auto_id(self):\n    class MyForm(Form):\n        field1 = CharField()\n        field2 = CharField()\n\n    f = MyForm(auto_id=True)\n    self.assertEqual(f[\"field1\"].auto_id, \"id_field1\")\n    self.assertEqual(f[\"field2\"].auto_id, \"id_field2\")\n\n    f = MyForm(auto_id=\"my_%s\")\n    self.assertEqual(f[\"field1\"].auto_id, \"my_field1\")\n    self.assertEqual(f[\"field2\"].auto_id, \"my_field2\")\n\n    f = MyForm(auto_id=False)\n    self.assertEqual(f[\"field1\"].auto_id, \"\")\n    self.assertEqual(f[\"field2\"].auto_id, \"\")\n", "def test_boundfield_initial_value(self):\n    class MyForm(Form):\n        field1 = CharField(initial=\"initial1\")\n        field2 = CharField()\n\n    form = MyForm()\n    self.assertEqual(form[\"field1\"].initial, \"initial1\")\n    self.assertIsNone(form[\"field2\"].initial)\n\n    form = MyForm({\"field1\": \"new_value\"})\n    self.assertEqual(form[\"field1\"].initial, \"initial1\")\n    self.assertIsNone(form[\"field2\"].initial)\n", "def test_boundfield_auto_id(self):\n    # Create a test form\n    class TestForm(Form):\n        name = CharField()\n        email = CharField()\n\n    # Create a bound form\n    form = TestForm(auto_id=True)\n\n    # Check that auto_id is correctly set for each field\n    self.assertEqual(form[\"name\"].auto_id, \"id_name\")\n    self.assertEqual(form[\"email\"].auto_id, \"id_email\")\n\n    # Create a bound form with custom auto_id\n    form = TestForm(auto_id=\"my_id_%s\")\n\n    # Check that auto_id is correctly set for each field\n    self.assertEqual(form[\"name\"].auto_id, \"my_id_name\")\n    self.assertEqual(form[\"email\"].auto_id, \"my_id_email\")\n", "def test_boundfield_initial(self):\n    class TestForm(Form):\n        field1 = CharField(initial=\"initial1\")\n        field2 = CharField()\n\n    form = TestForm()\n    self.assertEqual(form[\"field1\"].initial, \"initial1\")\n    self.assertIsNone(form[\"field2\"].initial)\n\n    form = TestForm({\"field1\": \"new value\"})\n    self.assertEqual(form[\"field1\"].initial, \"initial1\")\n    self.assertIsNone(form[\"field2\"].initial)\n\n    class TestForm2(Form):\n        field1 = CharField()\n\n    form = TestForm2()\n    self.assertIsNone(form[\"field1\"].initial)\n\n    form = TestForm2({\"field1\": \"new value\"})\n    self.assertIsNone(form[\"field1\"].initial)\n\n    class TestForm3(Form):\n        field1 = CharField()\n\n    form = TestForm3(initial={\"field1\": \"initial value\"})\n    self.assertEqual(form[\"field1\"].initial, \"initial value\")\n\n    form = TestForm3({\"field1\": \"new value\"}, initial={\"field1\": \"initial value\"})\n    self.assertEqual(form[\"field1\"].initial, \"initial value\")\n", "    def test_label_tag_with_custom_attrs(self):\n        class TestForm(Form):\n            field = CharField()\n\n        form = TestForm()\n        bound_field = form[\"field\"]\n        label_tag = bound_field.label_tag(attrs={\"class\": \"my-class\"})\n        self.assertHTMLEqual(\n            label_tag,\n            '<label for=\"id_field\" class=\"my-class\">Field:</label>',\n        )\n"], "sample_536": ["def test_ellipse_selector():\n    fig, ax = plt.subplots()\n    ax.set_xlim(0, 10)\n    ax.set_ylim(0, 10)\n        pass\n    tool = widgets.EllipseSelector(ax, onselect, interactive=True)\n    tool.extents = (1, 5, 2, 6)\n    assert_allclose(tool.geometry, np.array([[1, 5, 5, 1, 1],\n                                             [2, 2, 6, 6, 2]]))\n", "def test_check_buttons_get_status(ax):\n    check_buttons = widgets.CheckButtons(ax, [\"Test 1\", \"Test 2\", \"Test 3\"])\n    check_buttons.set_active(0)\n    assert check_buttons.get_status() == [True, False, False]\n    check_buttons.set_active(1)\n    assert check_buttons.get_status() == [True, True, False]\n    check_buttons.set_active(1)\n    assert check_buttons.get_status() == [True, False, False]\n    check_buttons.set_active(2)\n    assert check_buttons.get_status() == [True, False, True]\n", "def test_LockDraw():\n    lock = widgets.LockDraw()\n\n    # Test locking and unlocking\n    lock(lock)\n    assert lock.locked()\n    lock.release(lock)\n    assert not lock.locked()\n\n    # Test attempting to lock when already locked\n    lock(lock)\n    with pytest.raises(ValueError):\n        lock(lock)\n\n    # Test attempting to release when not locked\n    lock.release(lock)\n    with pytest.raises(ValueError):\n        lock.release(lock)\n\n    # Test checking if locked\n    lock(lock)\n    assert lock.isowner(lock)\n    lock.release(lock)\n    assert not lock.isowner(lock)\n\n    # Test checking if available\n    lock(lock)\n    assert not lock.available(lock)\n    lock.release(lock)\n    assert lock.available(lock)\n", "def test_LockDraw():\n    lock = widgets.LockDraw()\n    assert lock.available(\"widget\") is True\n    lock(\"widget\")\n    assert lock.available(\"widget\") is False\n    lock.release(\"widget\")\n    assert lock.available(\"widget\") is True\n\n    with pytest.raises(ValueError):\n        lock(\"widget\")\n        lock(\"widget2\")\n\n    lock.release(\"widget\")\n    assert lock.available(\"widget\") is True\n    lock(\"widget\")\n    assert lock.available(\"widget2\") is False\n", "def test_button_press_event(ax):\n    button = widgets.Button(ax, \"Test\")\n    callback = mock.MagicMock()\n    button.on_clicked(callback)\n    assert callback.call_count == 0\n    event = mock_event(ax, button, xdata=0.5, ydata=0.5)\n    button._click(event)\n    assert callback.call_count == 1\n"], "sample_539": ["def test_rectangle_selector_move(ax):\n    onselect = mock.Mock()\n    tool = widgets.RectangleSelector(ax, onselect, interactive=True)\n    assert tool._active_handle is None\n\n    # Simulate a click on the rectangle.\n    event = mock_event(tool, 100, 100, button=1)\n    tool._press(event)\n    assert tool._active_handle == 'C'\n\n    # Move the rectangle.\n    event = mock_event(tool, 120, 120, button=1, key='shift')\n    tool._onmove(event)\n    assert_allclose(tool.extents, (20, 140, 20, 140))\n\n    # Check that onselect was called.\n    onselect.assert_called_once()\n", "def test_rectangle_selector(ax):\n    onselect = mock.Mock(spec=noop)\n    tool = widgets.RectangleSelector(ax, onselect, interactive=True)\n    assert not tool._selection_completed\n\n    # Start selection\n    click_and_drag(tool, start=(100, 100), end=(150, 150))\n    assert tool._selection_completed\n    onselect.assert_called_once()\n\n    # Move selection\n    click_and_drag(tool, start=(125, 125), end=(175, 175))\n    assert tool._selection_completed\n    assert onselect.call_count == 2\n", "def test_rectangle_selector_default_props(ax):\n    # Test that default properties are set.\n    onselect = mock.Mock(spec=noop)\n    widget = widgets.RectangleSelector(ax, onselect)\n    assert widget._selection_artist.get_facecolor() == 'red'\n    assert widget._selection_artist.get_edgecolor() == 'black'\n    assert widget._selection_artist.get_alpha() == 0.2\n    assert widget._selection_artist.get_fill() is True\n", "def test_rectangle_selector_square_aspect(ax):\n        assert_allclose((erelease.xdata - eclick.xdata) / (eclick.ydata - erelease.ydata), 1)\n\n    selector = widgets.RectangleSelector(ax, select_callback, 'box', useblit=True,\n                                         button=[1], minspanx=0, minspany=0,\n                                         spancoords='pixels', interactive=True,\n                                         state_modifier_keys=dict(square='control'))\n    with mock.patch('matplotlib.backend_bases.FigureCanvasBase.mpl_connect'):\n        selector.connect_default_events()\n\n    selector.extents = (1, 2, 3, 4)\n    click_and_drag(selector, start=(1.5, 3.5), end=(1.5, 3.1),\n                   key='control', button=1)\n", "def test_rectangle_selector(ax):\n    ax.plot([1, 2, 3], [4, 5, 6])\n    ax.set_xlim(0, 5)\n    ax.set_ylim(0, 10)\n    RS = widgets.RectangleSelector(ax, noop, drawtype='box',\n                                   interactive=True)\n    click_and_drag(ax, start=(1, 4), end=(3, 6))\n    RS.set_active(False)\n    RS.extents = 1, 3, 4, 6\n"], "sample_541": ["def test_button_set_active():\n    fig, ax = plt.subplots()\n    button = widgets.Button(ax, label='Test')\n    button.set_active(False)\n    assert not button.active\n    button.set_active(True)\n    assert button.active\n    button.ax.remove()\n", "def test_LockDraw():\n    lock = widgets.LockDraw()\n    assert lock.available(None) is True\n\n    lock(None)\n    assert lock.available(None) is False\n\n    with pytest.raises(ValueError):\n        lock(None)\n\n    lock.release(None)\n    assert lock.available(None) is True\n\n    lock(None)\n    assert lock.available('another') is False\n    assert lock.isowner(None) is True\n    assert lock.isowner('another') is False\n    assert lock.locked() is True\n", "def test_widgets_LockDraw():\n    draw = widgets.LockDraw()\n    assert draw.available(None)\n    assert not draw.locked()\n    draw(None)\n    assert not draw.available(None)\n    assert draw.locked()\n    draw.release(None)\n    assert draw.available(None)\n    assert not draw.locked()\n    with pytest.raises(ValueError):\n        draw(None)\n        draw(None)\n    with pytest.raises(ValueError):\n        draw.release(None)\n        draw.release(None)\n", "def test_Slider_reset(ax):\n    slider = widgets.Slider(ax, 'somename', 0, 1)\n    slider.set_val(0.5)\n    slider.reset()\n    assert_allclose(slider.val, 0)\n", "def test_SpanSelector(ax):\n        pass\n    widgets.SpanSelector(ax, onselect, 'horizontal', useblit=True,\n                         props=dict(facecolor='red', alpha=0.5))\n    ax.set_xlim(0, 1)\n"], "sample_543": ["def test_button_image():\n    fig, ax = plt.subplots()\n    image = np.random.rand(10, 10, 3)\n    ax_button = plt.axes([0.7, 0.05, 0.2, 0.075])\n    widgets.Button(ax_button, 'Button', image=image)\n    ax.set_xlim(0, 10)\n    ax.set_ylim(0, 10)\n", "def test_button():\n    fig, ax = plt.subplots()\n    ax_button = plt.axes([0.7, 0.05, 0.2, 0.075])\n    button = widgets.Button(ax_button, 'Click me')\n    called = [False]\n\n        called[0] = True\n\n    button.on_clicked(on_click)\n    click_and_drag(ax_button, (0.5, 0.5), 1)\n    assert called[0]\n", "def test_RadioButtons(fig_test, fig_ref):\n    fig, ax = plt.subplots()\n    fig.subplots_adjust(left=0.3)\n    rax = fig.add_axes([0.025, 0.5, 0.15, 0.15])\n    radio = widgets.RadioButtons(rax, ('Radio 1', 'Radio 2', 'Radio 3'))\n    ax.plot([1, 2, 3])\n\n    # Set visibility of the last radio and check it's not visible in output.\n    radio.circles[-1].set_visible(False)\n    radio.labels[-1].set_visible(False)\n\n    fig_ref, ax_ref = plt.subplots()\n    fig_ref.subplots_adjust(left=0.3)\n    rax_ref = fig_ref.add_axes([0.025, 0.5, 0.15, 0.15])\n    radio_ref = widgets.RadioButtons(rax_ref, ('Radio 1', 'Radio 2'))\n    ax_ref.plot([1, 2, 3])\n", "def test_rectangle_selector_get_geometry(ax):\n        pass\n\n    tool = widgets.RectangleSelector(ax, onselect, interactive=True)\n    x1, y1, x2, y2 = 100, 150, 200, 250\n    click_and_drag(tool, start=(x1, y1), end=(x2, y2))\n    assert_allclose(tool.geometry[0], [x1, x2, x2, x1, x1])\n    assert_allclose(tool.geometry[1], [y1, y1, y2, y2, y1])\n", "def test_SpanSelector_compress():\n    fig, ax = plt.subplots()\n    span_selector = widgets.SpanSelector(\n        ax, lambda x, y: None, \"horizontal\", useblit=True,\n        rectprops=dict(facecolor='b'))\n    # Simulate zoom.\n    with mock.patch('matplotlib.transforms.BboxBase.contains',\n                    return_value=False):\n        span_selector._onmove(mock_event(xdata=0.2, ydata=0.5))\n        span_selector._onmove(mock_event(xdata=0.8, ydata=0.5))\n    assert span_selector.extents == (0.2, 0.8)\n    # Simulate pan.\n    with mock.patch('matplotlib.transforms.BboxBase.contains',\n                    return_value=False):\n        span_selector._onmove(mock_event(xdata=0.4, ydata=0.5))\n        span_selector._onmove(mock_event(xdata=0.6, ydata=0.5))\n    assert span_selector.extents == (0.2, 0.8)\n"], "sample_559": ["def test_Axes_matshow():\n    fig, ax = plt.subplots()\n    Z = np.random.rand(20, 30)\n    ret = ax.matshow(Z)\n    assert ret.get_array().shape == Z.shape\n    assert ret.get_extent() == (-0.5, 29.5, -0.5, 19.5)\n    assert ax.get_xlim() == (-0.5, 29.5)\n    assert ax.get_ylim() == (-0.5, 19.5)\n    assert ax.get_xscale() == 'linear'\n    assert ax.get_yscale() == 'linear'\n    assert ax.get_xticks().size == 7\n    assert ax.get_yticks().size == 6\n    plt.close('all')\n", "def test_plot_non_uniform_data():\n    fig, ax = plt.subplots()\n    x = np.array([1, 2, 3, 4, 5])\n    y1 = np.array([1, 2, 3, 4, 5])\n    y2 = np.array([2, 3, 5, 7, 11])\n    ax.plot(x, y1)\n    ax.plot(x, y2)\n    assert_array_equal(ax.lines[0].get_xdata(), x)\n    assert_array_equal(ax.lines[0].get_ydata(), y1)\n    assert_array_equal(ax.lines[1].get_xdata(), x)\n    assert_array_equal(ax.lines[1].get_ydata(), y2)\n    plt.close(fig)\n", "def test_axes_plot_shape_kwargs():\n    fig, ax = plt.subplots()\n    x = np.linspace(0, 2*np.pi, 100)\n    y = np.sin(x)\n    ax.plot(x, y, 'o-', mfc='red', mec='green', ms=10)\n    plt.close(fig)\n", "def test_axes_divider_append_axes():\n    fig, ax = plt.subplots()\n    divider = make_axes_locatable(ax)\n    ax2 = divider.append_axes(\"top\", size=\"20%\", pad=\"15%\")\n    fig.canvas.draw()\n\n    assert ax2.get_position().height == ax.get_position().height * 0.2\n    assert ax2.get_position().y0 == ax.get_position().y1 + ax.get_position().height * 0.15\n", "def test_axes_divider_append_axes():\n    fig, ax = plt.subplots()\n    divider = make_axes_locatable(ax)\n    ax2 = divider.append_axes(\"right\", size=\"10%\", pad=\"10%\")\n    fig.canvas.draw()\n\n    assert ax2.get_position().x0 > ax.get_position().x1\n"], "sample_564": ["def test_axes3d_set_xlim3d_single_arg():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    assert ax.get_xlim() == (-1, 1)\n    ax.set_xlim3d(0)\n    assert ax.get_xlim() == (0, 0)\n    plt.close(fig)\n", "def test_3d_stem(remove_text=False):\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    theta = np.linspace(0, 2*np.pi)\n    x = np.cos(theta)\n    y = np.sin(theta)\n    z = np.cos(4*theta)\n    ax.stem(x, y, z)\n    ax.set_xlim(-1.1, 1.1)\n    ax.set_ylim(-1.1, 1.1)\n    ax.set_zlim(-1.1, 1.1)\n", "def test_voxels_facecolors_3d_array():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    x, y, z = np.indices((3, 3, 3))\n    vox = ax.voxels(x, y, z, np.abs(x) + np.abs(y) + np.abs(z) <= 1,\n                    facecolors=np.random.rand(3, 3, 3, 4))\n    ax.set_xlim(-1, 2)\n    ax.set_ylim(-1, 2)\n    ax.set_zlim(-1, 2)\n    return fig\n", "def test_axes3d_init():\n    fig = plt.figure()\n    ax1 = fig.add_subplot(111, projection='3d')\n    ax2 = fig.add_subplot(111, projection='3d')\n    ax2.view_init(elev=30, azim=60, roll=45)\n    ax1._shareview = ax2\n    ax1.view_init(elev=30, azim=60, roll=45, share=True)\n    assert ax1.elev == ax2.elev\n    assert ax1.azim == ax2.azim\n    assert ax1.roll == ax2.roll\n", "def test_3d_projection_text_update():\n    ax = axes3d.Axes3D(plt.figure())\n    text = ax.text(0, 0, 0, 'text')\n    ax.set_xlim(-1, 1)\n    ax.set_ylim(-1, 1)\n    ax.set_zlim(-1, 1)\n    ax.get_proj()\n    text.update({'position': (0, 0, 0)})\n    ax.get_proj()\n"], "sample_591": ["def test_assert_unique_multiindex_level_names_success():\n    variables = {\n        \"a\": xr.Variable((\"x\", \"y\"), np.zeros((2, 2)), {\"coordinates\": \"x y\"}),\n        \"x\": xr.IndexVariable(\"x\", [0, 1]),\n        \"y\": xr.IndexVariable(\"y\", [0, 1]),\n    }\n    merge.assert_unique_multiindex_level_names(variables)  # should not raise\n", "def test_assert_unique_multiindex_level_names() -> None:\n    variables: xr.core.utils.Frozen = xr.core.utils.Frozen(\n        {\n            \"x\": xr.Variable(),\n            \"y\": xr.Variable(),\n            \"z\": xr.Variable(),\n            \"x_level\": xr.IndexVariable(),\n            \"y_level\": xr.IndexVariable(),\n        }\n    )\n    with pytest.raises_regex(ValueError, r\"conflicting multi-index level name\"):\n        xr.core.merge.assert_unique_multiindex_level_names(variables)\n", "def test_merge_variables_errors():\n    # regression test for GH1684\n    var = xr.Variable((\"x\",), np.array([1, 2]))\n    with raises_regex(ValueError, \"replacement dimension\"):\n        merge.merge_variables(\n            {\"x\": var}, {\"x\": var}, compat=\"identical\", join=\"exact\"\n        )\n", "def test_merge_data_and_coords():\n    data1 = create_test_data()\n    data2 = create_test_data()\n    data2[\"var1\"].data = np.random.randn(*data2[\"var1\"].shape)\n    data2 = data2.drop_vars(\"var4\")\n    data2.coords[\"z\"] = data2[\"var1\"]\n\n    merged_dataset = merge.merge_data_and_coords([data1, data2])\n\n    expected = merge.merge_datasets([data1, data2], compat=\"broadcast_equals\")\n    assert_identical(merged_dataset, expected)\n", "def test_broadcast_coordinates():\n    # found in test_merge_overlapping_data_vars\n    datasets = [\n        xr.Dataset(),\n        xr.Dataset(),\n        xr.Dataset(coords={\"x\": 1}),\n        xr.Dataset(coords={\"x\": (\"y\", [1, 2])}),\n    ]\n    result = merge.broadcast_coordinates(*[ds.coords for ds in datasets])\n    expected = xr.Dataset(coords={\"x\": (\"y\", [1, 2])})\n    assert_identical(result, expected.coords.to_dataset())\n"], "sample_617": ["def test_apply_ufunc_input_core_dims():\n    # example with two non-broadcasting dimensions\n        return np.stack([x, y], axis=-1)\n\n    a = xr.DataArray([1, 2, 3])\n    b = xr.DataArray([10, 20, 30])\n\n    result = apply_ufunc(func, a, b, input_core_dims=[[\"dim_0\"], [\"dim_0\"]])\n    expected = xr.DataArray(\n        np.array([[1, 10], [2, 20], [3, 30]]), dims=[\"dim_0\", \"dim_1\"]\n    )\n    assert_identical(result, expected)\n\n    result = apply_ufunc(\n        func, a, b, input_core_dims=[[\"dim_0\"], [\"dim_0\"]], output_core_dims=[[\"dim_1\"]]\n    )\n    assert_identical(result, expected)\n\n    # example with two broadcasting dimensions\n        return x + y\n\n    a = xr.DataArray([1, 2, 3])\n    b = xr.DataArray([10])\n\n    result = apply_ufunc(func, a, b, input_core_dims=[[], []])\n    expected = a + b\n    assert_identical(result, expected)\n", "def test_apply_ufunc_signature_validation():\n        pass\n\n    with pytest.raises(ValueError):\n        apply_ufunc(\n            func, xr.DataArray([1]), xr.DataArray([2]), xr.DataArray([3]), input_core_dims=None\n        )\n\n    with pytest.raises(ValueError):\n        apply_ufunc(\n            func,\n            xr.DataArray([1]),\n            xr.DataArray([2]),\n            xr.DataArray([3]),\n            input_core_dims=([[\"x\"]], [[\"y\"]]),\n        )\n\n    with pytest.raises(ValueError):\n        apply_ufunc(\n            func,\n            xr.DataArray([1]),\n            xr.DataArray([2]),\n            xr.DataArray([3]),\n            input_core_dims=([[\"x\"]], [[\"y\"]], [[\"z\"], [\"z\"]]),\n        )\n", "def test_join_dict_keys():\n    objs = [{\"a\": 1, \"b\": 2}, {\"b\": 3, \"c\": 4}, {\"c\": 5}]\n    expected = [\"b\"]\n    actual = join_dict_keys(objs, how=\"inner\")\n    assert actual == expected\n\n    expected = [\"a\", \"b\", \"c\"]\n    actual = join_dict_keys(objs, how=\"outer\")\n    assert actual == expected\n\n    expected = [\"a\", \"b\"]\n    actual = join_dict_keys(objs, how=\"left\")\n    assert actual == expected\n\n    expected = [\"c\"]\n    actual = join_dict_keys(objs, how=\"right\")\n    assert actual == expected\n\n    with pytest.raises(ValueError):\n        join_dict_keys(objs, how=\"exact\")\n", "def test_unified_dim_sizes():\n    # Create sample data\n    da1 = xr.DataArray(np.random.rand(2, 3), dims=[\"x\", \"y\"])\n    da2 = xr.DataArray(np.random.rand(2, 3), dims=[\"x\", \"y\"])\n    da3 = xr.DataArray(np.random.rand(4, 5), dims=[\"a\", \"b\"])\n\n    # Test with no exclude dimensions\n    expected = {\"x\": 2, \"y\": 3}\n    actual = unified_dim_sizes([da1.variable, da2.variable])\n    assert actual == expected\n\n    # Test with exclude dimensions\n    expected = {\"x\": 2}\n    actual = unified_dim_sizes([da1.variable, da2.variable], exclude_dims={\"y\"})\n    assert actual == expected\n\n    # Test with mismatched dimensions\n    with pytest.raises(ValueError):\n        unified_dim_sizes([da1.variable, da3.variable])\n", "def test_collect_dict_values() -> None:\n    dict1 = {\"a\": 1, \"b\": 2}\n    dict2 = {\"a\": 3, \"b\": 4, \"c\": 5}\n    keys = [\"a\", \"b\"]\n\n    result = collect_dict_values([dict1, dict2], keys)\n\n    expected = [[1, 3], [2, 4]]\n    assert result == expected\n"], "sample_606": ["def test_apply_ufunc_with_excluded_dims():\n    # Test apply_ufunc with excluded dims\n    da = xr.DataArray(\n        np.arange(6).reshape(2, 3),\n        dims=[\"x\", \"y\"],\n        coords={\"x\": [0, 1], \"y\": [10, 20, 30]},\n    )\n\n        return x.sum(\"x\")\n\n    result = apply_ufunc(\n        func,\n        da,\n        input_core_dims=[[\"x\"]],\n        exclude_dims={\"x\"},\n    )\n    expected = xr.DataArray(\n        np.sum(da.values, axis=0),\n        dims=[\"y\"],\n        coords={\"y\": [10, 20, 30]},\n    )\n    assert_identical(result, expected)\n", "def test_apply_ufunc_dtype():\n    arr = xr.DataArray(np.array([1, 2, 3]), dims=[\"x\"])\n    result = apply_ufunc(np.round, arr, dtype=\"int32\")\n    assert result.dtype == \"int32\"\n", "def test_join_dict_keys():\n    dict1 = {\"a\": 1, \"b\": 2, \"c\": 3}\n    dict2 = {\"b\": 20, \"c\": 30, \"d\": 40}\n    dict3 = {\"c\": 300, \"d\": 400, \"e\": 500}\n    expected_union = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n    expected_intersection = [\"b\", \"c\"]\n\n    assert join_dict_keys([dict1, dict2, dict3], how=\"outer\") == expected_union\n    assert join_dict_keys([dict1, dict2, dict3], how=\"inner\") == expected_intersection\n\n    with pytest.raises(ValueError):\n        join_dict_keys([dict1, dict2, dict3], how=\"exact\")\n\n    assert join_dict_keys([dict1, dict1, dict1], how=\"exact\") == [\"a\", \"b\", \"c\"]\n", "def test_unified_dim_sizes():\n    # create some test arrays with different dims\n    da1 = xr.DataArray(np.random.rand(4, 3), dims=[\"x\", \"y\"])\n    da2 = xr.DataArray(np.random.rand(4, 3, 2), dims=[\"x\", \"y\", \"z\"])\n    da3 = xr.DataArray(np.random.rand(5), dims=[\"x\"])\n\n    # test with all arrays having the same dims\n    dims = unified_dim_sizes([da1, da2[:, :, 0]])\n    assert dims == {\"x\": 4, \"y\": 3}\n\n    # test with arrays having different dims\n    dims = unified_dim_sizes([da1, da3])\n    assert dims == {\"x\": 4}  # size of 'x' in da1\n\n    # test with arrays having different sizes for the same dim\n    with pytest.raises(ValueError):\n        unified_dim_sizes([da1, da2[:, :2, :]])\n", "def test_unified_dim_sizes():\n    # Create two variables with different dimensions\n    var1 = xr.Variable((\"x\", \"y\"), np.random.rand(2, 3))\n    var2 = xr.Variable((\"x\", \"z\"), np.random.rand(2, 4))\n\n    # Test that unified_dim_sizes raises an error for mismatched lengths\n    with pytest.raises(ValueError):\n        unified_dim_sizes((var1, var2))\n\n    # Test that unified_dim_sizes returns the correct sizes for matching lengths\n    var3 = xr.Variable((\"x\", \"y\"), np.random.rand(2, 3))\n    sizes = unified_dim_sizes((var1, var3))\n    assert sizes == {\"x\": 2, \"y\": 3}\n\n    # Test that unified_dim_sizes returns the correct sizes for exclude_dims\n    sizes = unified_dim_sizes((var1, var3), exclude_dims={\"y\"})\n    assert sizes == {\"x\": 2}\n"], "sample_720": ["def test_quantile_transformer_1d():\n    X = np.random.randn(100)\n    qt = QuantileTransformer()\n    X_trans = qt.fit_transform(X)\n    assert_equal(X_trans.shape, X.shape)\n    X_trans_inv = qt.inverse_transform(X_trans)\n    assert_allclose(X, X_trans_inv, rtol=0.5)\n", "def test_binarizer_copy():\n    X = np.array([[1., -1., 2.],\n                  [1., 0., 3.],\n                  [0., 1., 4.]])\n\n    expected_result = np.array([[1., 0., 1.],\n                                [1., 0., 1.],\n                                [0., 1., 1.]])\n\n    binarizer = Binarizer().fit(X)\n    binarizer_transform = binarizer.transform(X)\n\n    assert_array_equal(binarizer_transform, expected_result)\n\n    # Test binarizer with copy\n    binarizer = Binarizer(copy=True).fit(X)\n    binarizer_transform = binarizer.transform(X)\n\n    assert_array_equal(binarizer_transform, expected_result)\n    assert_false(np.may_share_memory(X, binarizer_transform))\n", "def test_power_transformer_method():\n    X = np.array([[1, 2], [3, 2], [4, 5]])\n\n    # box-cox\n    pt_boxcox = PowerTransformer(method='box-cox')\n    X_boxcox = pt_boxcox.fit_transform(X)\n\n    # yeo-johnson not implemented yet, should raise an error\n    pt_yeojohnson = PowerTransformer(method='yeo-johnson')\n    assert_raises_regex(ValueError, \"'method' must be one of {'box-cox'}\",\n                        pt_yeojohnson.fit, X)\n\n    # invalid method should raise an error\n    pt_invalid = PowerTransformer(method='invalid')\n    assert_raises_regex(ValueError, \"'method' must be one of {'box-cox'}\",\n                        pt_invalid.fit, X)\n", "def test_quantile_transformer_sparse():\n    # check that quantile_transformer works correctly with sparse matrices\n    rng = np.random.RandomState(0)\n    X = rng.randn(100, 3)\n    X[X < 0] = 0\n    X_sparse = sparse.csr_matrix(X)\n\n    qt_dense = QuantileTransformer(n_quantiles=10)\n    qt_sparse = QuantileTransformer(n_quantiles=10)\n\n    X_dense = qt_dense.fit_transform(X)\n    X_sparse = qt_sparse.fit_transform(X_sparse)\n\n    assert_array_almost_equal(X_dense, X_sparse.toarray())\n", "def test_transform_selected():\n    X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n\n    # select first and third column (0 and 2)\n    transformer = lambda x: x**2\n    X_selected = _transform_selected(X, transformer, [0, 2])\n\n    # test if the second column remains unchanged\n    assert_array_equal(X_selected[:, 1], X[:, 1])\n\n    # test if the first and third column are transformed correctly\n    assert_array_equal(X_selected[:, 0], X[:, 0]**2)\n    assert_array_equal(X_selected[:, 2], X[:, 2]**2)\n\n    # test if selection by boolean mask works\n    mask = [True, False, True]\n    X_selected = _transform_selected(X, transformer, mask)\n    assert_array_equal(X_selected[:, 0], X[:, 0]**2)\n    assert_array_equal(X_selected[:, 1], X[:, 1])\n    assert_array_equal(X_selected[:, 2], X[:, 2]**2)\n\n    # test if the transform function is applied to the correct features when\n    # using a numpy array of bools as the selector\n    selector = np.array([True, False, True], dtype=np.bool_)\n    X_selected = _transform_selected(X, transformer, selector)\n    assert_array_equal(X_selected[:, 0], X[:, 0]**2)\n    assert_array_equal(X_selected[:, 1], X[:, 1])\n    assert_array_equal(X_selected[:, 2], X[:, 2]**2)\n\n    # test if setting copy=False returns the result in the original array\n    X_copy = X.copy()\n    X_selected = _transform_selected(X_copy, transformer, [0, 2], copy=False)\n    assert_true(X_selected is X_copy)\n"], "sample_744": ["def test_quantile_transformer_ordinal_sparse():\n    # Smoke test for ordinal encoding with sparse data\n    X = sparse.csr_matrix(np.array([[1, 0, 3], [2, 0, 0]]))\n    qt = QuantileTransformer(n_quantiles=3, output_distribution='normal')\n    qt.fit(X)\n    assert_array_less(qt.transform(X).toarray(), np.array([[0.674, -0.674, 0.674], [0.0, -0.674, -0.674]]))\n", "def test_transform_selected():\n    # Test on that _transform_selected(X, transform, select, copy) applies\n    # the transform function to the selected features of X.\n    arr = np.array([[1, 2, 3], [4, 5, 6]])\n\n    # transform all features\n    transformed_arr = _transform_selected(arr, np.sqrt, [0, 1, 2])\n    assert_array_almost_equal(transformed_arr, np.sqrt(arr))\n\n    # transform a subset of features\n    transformed_arr = _transform_selected(arr, np.sqrt, [0, 2])\n    expected_arr = np.array([[1, 2, np.sqrt(3)], [2, 5, np.sqrt(6)]])\n    assert_array_almost_equal(transformed_arr, expected_arr)\n\n    # transform with a callable that takes X and feature_indices\n    transformed_arr = _transform_selected(arr, lambda X, indices: X[:, indices] ** 2, [0, 2])\n    expected_arr = np.array([[1, 2, 9], [16, 5, 36]])\n    assert_array_almost_equal(transformed_arr, expected_arr)\n", "def test_power_transformer_handle_zeros():\n    # Test that PowerTransformer handles zeros correctly.\n    X = np.array([[1, 2], [3, 2], [4, 5]])\n    X[0, 0] = 0\n\n    pt = PowerTransformer()\n    with pytest.raises(ValueError):\n        pt.fit(X)\n\n    pt = PowerTransformer(method='box-cox')\n    with pytest.raises(ValueError):\n        pt.fit(X)\n", "def test_maxabs_scaler_sparse():\n    X = sparse.csr_matrix([[-3, 0, 5], [-1, 0, 0], [0, 1, 0]])\n    scaler = MaxAbsScaler(copy=True)\n    X_scaled = scaler.fit_transform(X)\n    assert_equal(scaler.n_samples_seen_, 3)\n    assert_array_equal(scaler.max_abs_, np.array([3, 1, 5]))\n    assert_array_equal(X_scaled.data, np.array([[-1., 0., 1.], [-1/3, 0., 0.], [0., 1., 0.]]).ravel())\n", "def test_power_transformer_inverse():\n    # Test that the inverse method of PowerTransformer returns the original data\n    X = np.array([[1, 2], [3, 2], [4, 5]])\n    pt = PowerTransformer()\n    X_trans = pt.fit_transform(X)\n    X_inv = pt.inverse_transform(X_trans)\n    assert_array_almost_equal(X, X_inv)\n"], "sample_747": ["def test_power_transformer():\n    # test that PowerTransformer can work with positive data\n    X = np.abs(np.random.RandomState(0).randn(100, 3))\n    pt = PowerTransformer()\n    X_trans = pt.fit_transform(X)\n\n    # compute mean and std on original scale for comparison\n    X_trans_mean = pt.inverse_transform(np.mean(X_trans, axis=0))\n    X_trans_std = pt.inverse_transform(np.std(X_trans, axis=0))\n    assert_array_almost_equal(X_trans_mean, np.mean(X, axis=0))\n    assert_array_almost_equal(X_trans_std, np.std(X, axis=0))\n\n    # test that PowerTransformer can work with zero values\n    X = np.vstack((np.abs(np.random.RandomState(0).randn(100, 3)),\n                   np.zeros((10, 3))))\n    pt = PowerTransformer()\n    X_trans = pt.fit_transform(X)\n    X_trans_mean = pt.inverse_transform(np.mean(X_trans, axis=0))\n    X_trans_std = pt.inverse_transform(np.std(X_trans, axis=0))\n    assert_array_almost_equal(X_trans_mean, np.mean(X, axis=0))\n    assert_array_almost_equal(X_trans_std, np.std(X, axis=0))\n", "def test_quantile_transformer_ignore_implicit_zeros():\n    # Create a sparse matrix with some explicit zeros and implicit zeros.\n    X_sparse = sparse.csr_matrix([[0, 1, 0], [0, 0, 2], [0, 3, 0]])\n\n    # Since ignore_implicit_zeros is False by default, we expect both explicit\n    # and implicit zeros to be taken into account when computing the quantiles.\n    qt = QuantileTransformer(n_quantiles=3)\n    qt.fit(X_sparse)\n    X_trans = qt.transform(X_sparse)\n\n    # Now, set ignore_implicit_zeros to True and repeat the process.\n    qt_ignore_zeros = QuantileTransformer(n_quantiles=3,\n                                          ignore_implicit_zeros=True)\n    qt_ignore_zeros.fit(X_sparse)\n    X_trans_ignore_zeros = qt_ignore_zeros.transform(X_sparse)\n\n    # We expect the two transformed matrices to be different, since one took\n    # into account implicit zeros and the other did not.\n    assert_false(np.allclose(toarray(X_trans), toarray(X_trans_ignore_zeros)))\n", "def test_quantile_transformer_sparse():\n    # Test that QuantileTransformer works with sparse data\n    X = sparse.csc_matrix(np.array([[0, 1, 1], [2, 0, 1], [1, 2, 0]]))\n    qT = QuantileTransformer()\n    Xt = qT.fit_transform(X)\n    assert_array_almost_equal(Xt.min(axis=0), 0)\n    assert_array_almost_equal(np.array(Xt.max(axis=0)), 1)\n", "def test_power_transformer_inverse():\n    # Test that inverse_transform brings the data back to original values\n    X = np.array([[1, 2], [3, 2], [4, 5]])\n    pt = PowerTransformer()\n    X_trans = pt.fit_transform(X)\n    X_inv = pt.inverse_transform(X_trans)\n    assert_allclose(X, X_inv)\n", "def test_quantile_transformer_sparse():\n    # check that quantile transformer works with sparse data\n    rng = np.random.RandomState(0)\n    X = rng.randn(100, 3)\n    X[X < 0.1] = 0\n    X_sparse = sparse.csr_matrix(X)\n\n    q = QuantileTransformer(n_quantiles=10, random_state=0)\n    q.fit(X_sparse)\n    assert_array_almost_equal(q.quantiles_, np.sort(X, axis=0))\n\n    q = QuantileTransformer(n_quantiles=10, random_state=0,\n                            output_distribution='normal')\n    q.fit(X_sparse)\n    assert_array_almost_equal(q.quantiles_, np.sort(X, axis=0))\n"], "sample_869": ["def test_precision_recall_fscore_support_input():\n    # Check that errors are raised when bad inputs are passed\n    y_true = [0, 1, 0, 1]\n    y_pred = [0, 1, 1, 1]\n\n    # Bad beta\n    with pytest.raises(ValueError):\n        precision_recall_fscore_support(y_true, y_pred, beta=-1)\n    with pytest.raises(ValueError):\n        precision_recall_fscore_support(y_true, y_pred, beta=0)\n\n    # Bad average param\n    with pytest.raises(ValueError):\n        precision_recall_fscore_support(y_true, y_pred, average=\"bad\")\n\n    # Bad zero_division param\n    with pytest.raises(ValueError):\n        precision_recall_fscore_support(y_true, y_pred, zero_division=\"bad\")\n", "def test_precision_recall_fscore_support_errors():\n    y_true, y_pred, _ = make_prediction(binary=True)\n\n    # Bad beta\n    with pytest.raises(ValueError):\n        precision_recall_fscore_support(y_true, y_pred, beta=-1.0)\n\n    # Bad average option\n    with pytest.raises(ValueError):\n        precision_recall_fscore_support(y_true, y_pred, average='bad')\n\n    # average='weighted' is not possible for binary classification\n    labels = np.unique(y_true)\n    with pytest.raises(ValueError):\n        precision_recall_fscore_support(y_true, y_pred, labels=labels,\n                                        average='weighted')\n\n    # average='samples' is not possible for binary classification\n    with pytest.raises(ValueError):\n        precision_recall_fscore_support(y_true, y_pred, labels=labels,\n                                        average='samples')\n", "def test_classification_report_multiclass_with_labels():\n    y_true = [0, 1, 2, 2, 0]\n    y_pred = [0, 0, 2, 2, 1]\n\n    report = classification_report(y_true, y_pred, labels=[0, 1, 2])\n    report_dict = classification_report(y_true, y_pred, labels=[0, 1, 2], output_dict=True)\n\n    assert re.match(r\".*precision.*recall.*f1-score.*support.*\", report)\n    assert_array_almost_equal([p[\"precision\"] for p in report_dict.values()], [0.5, 0.0, 1.0], 2)\n    assert_array_almost_equal([p[\"recall\"] for p in report_dict.values()], [0.5, 0.0, 1.0], 2)\n    assert_array_almost_equal([p[\"f1-score\"] for p in report_dict.values()], [0.5, 0.0, 1.0], 2)\n    assert_array_equal([p[\"support\"] for p in report_dict.values()], [2, 1, 2])\n\n    report = classification_report(y_true, y_pred, labels=[0, 1, 2], zero_division=0)\n    report_dict = classification_report(y_true, y_pred, labels=[0, 1, 2], output_dict=True, zero_division=0)\n\n    assert_array_almost_equal([p[\"precision\"] for p in report_dict.values()], [0.5, 0.0, 1.0], 2)\n    assert_array_almost_equal([p[\"recall\"] for p in report_dict.values()], [0.5, 0.0, 1.0], 2)\n    assert_array_almost_equal([p[\"f1-score\"] for p in report_dict.values()], [0.5, 0.0, 1.0], 2)\n    assert_array_equal([p[\"support\"] for p in report_dict.values()], [2, 1, 2])\n", "def test_multilabel_confusion_matrix():\n    # Test multilabel_confusion_matrix function\n    y_true = np.array([[1, 0, 1], [0, 1, 0]])\n    y_pred = np.array([[1, 0, 0], [0, 1, 1]])\n\n    # Compute multilabel confusion matrix with default parameters\n    mcm = multilabel_confusion_matrix(y_true, y_pred)\n\n    # Expected result\n    expected_mcm = np.array([[[1, 0], [0, 1]], [[1, 0], [0, 1]], [[0, 1], [1, 0]]])\n\n    assert_array_equal(mcm, expected_mcm)\n\n    # Compute multilabel confusion matrix with samplewise=True\n    mcm_samplewise = multilabel_confusion_matrix(y_true, y_pred, samplewise=True)\n\n    # Expected result\n    expected_mcm_samplewise = np.array([[[1, 0], [0, 1]], [[1, 1], [0, 0]]])\n\n    assert_array_equal(mcm_samplewise, expected_mcm_samplewise)\n\n    # Compute multilabel confusion matrix with labels\n    labels = [0, 2]\n    mcm_labels = multilabel_confusion_matrix(y_true, y_pred, labels=labels)\n\n    # Expected result\n    expected_mcm_labels = np.array([[[1, 0], [0, 1]], [[0, 1], [1, 0]]])\n\n    assert_array_equal(mcm_labels, expected_mcm_labels)\n", "def test_classification_report_multiclass_with_zero_division():\n    # Test classification_report with zero division in multiclass classification\n    y_true = np.array([0, 0, 0, 0, 1, 1, 1, 1, 2])\n    y_pred = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1])\n\n    # Test with zero_division=0\n    report = classification_report(y_true, y_pred, zero_division=0)\n    report_dict = classification_report(y_true, y_pred, zero_division=0, output_dict=True)\n\n    # Test with zero_division=1\n    report_1 = classification_report(y_true, y_pred, zero_division=1)\n    report_dict_1 = classification_report(y_true, y_pred, zero_division=1, output_dict=True)\n\n    # Test with zero_division=\"warn\"\n    with pytest.warns(UndefinedMetricWarning):\n        report_warn = classification_report(y_true, y_pred, zero_division=\"warn\")\n        report_dict_warn = classification_report(y_true, y_pred, zero_division=\"warn\", output_dict=True)\n"], "sample_875": ["def test_precision_recall_fscore_support_errors():\n    y_true, y_pred, _ = make_prediction(binary=True)\n\n    # Bad beta\n    y_true, y_pred, _ = make_prediction(binary=True)\n    with pytest.raises(ValueError):\n        precision_recall_fscore_support(y_true, y_pred, beta=-1.0)\n\n    # Bad average option\n    with pytest.raises(ValueError):\n        precision_recall_fscore_support(y_true, y_pred, average=\"mega\")\n", "def test_precision_recall_fscore_support_binary_class_ignoring_labels():\n    y_true = np.array([0, 1, 1, 0])\n    y_pred = np.array([0, 1, 0, 0])\n\n    # labels argument can be used to ignore one of the classes in the\n    # precision, recall, fscore calculation\n    p, r, f, s = precision_recall_fscore_support(y_true, y_pred,\n                                                 labels=[0])\n    assert_array_almost_equal(p, [1.], 2)\n    assert_array_almost_equal(r, [1.], 2)\n    assert_array_almost_equal(f, [1.], 2)\n    assert_array_equal(s, [3])\n\n    p, r, f, s = precision_recall_fscore_support(y_true, y_pred,\n                                                 labels=[1])\n    assert_array_almost_equal(p, [1.], 2)\n    assert_array_almost_equal(r, [0.5], 2)\n    assert_array_almost_equal(f, [0.66], 2)\n    assert_array_equal(s, [1])\n", "def test_precision_recall_fbeta(y_true, y_pred, labels, zero_division):\n    # Test precision, recall and F1 with different zero division values\n    with ignore_warnings(category=UndefinedMetricWarning):\n        p, r, f, s = precision_recall_fscore_support(\n            y_true, y_pred, labels=labels, zero_division=zero_division\n        )\n        assert_array_almost_equal(p, [1.0, 0.0, 0.0], zero_division)\n        assert_array_almost_equal(r, [1.0, 0.0, 0.0], zero_division)\n        assert_array_almost_equal(f, [1.0, 0.0, 0.0], zero_division)\n        assert_array_equal(s, [1, 2, 3])\n", "def test_classification_report_multiclass_with_zero_division():\n    # Test classification_report with zero division in multiclass\n    y_true = np.array([1, 1, 1, 1, 1, 2, 2, 2, 2, 2])\n    y_pred = np.array([1, 1, 1, 2, 2, 1, 1, 1, 2, 2])\n\n    with pytest.warns(UndefinedMetricWarning) as record:\n        classification_report(y_true, y_pred, zero_division=0)\n\n    assert \"Ill-defined metric warning\" in record[0].message.args[0]\n    assert record[0].category == UndefinedMetricWarning\n", "def test_precision_recall_fscore_support_labels_list():\n    y_true = [0, 1, 2, 0, 1, 2]\n    y_pred = [0, 2, 1, 0, 0, 1]\n\n    labels = [0, 2]\n    p, r, f, s = precision_recall_fscore_support(y_true, y_pred, labels=labels)\n    assert_array_equal(p, [1.0, 0.0])\n    assert_array_equal(r, [1.0, 0.0])\n    assert_array_equal(f, [1.0, 0.0])\n    assert_array_equal(s, [2, 2])\n"], "sample_906": ["def test_get_objects():\n    env = {\"root_symbol\": Symbol(None, None, None, None, None, None, None)}\n    symbol1 = env[\"root_symbol\"].add_name(\"foo\")\n    env[\"root_symbol\"].add_declaration(\n        parse(\"function\", \"void foo(int arg)\"), docname=\"doc1\", line=42)\n    symbol2 = env[\"root_symbol\"].add_name(\"bar\")\n    env[\"root_symbol\"].add_declaration(\n        parse(\"function\", \"void bar(double arg)\"), docname=\"doc2\", line=42)\n    domain = cppDomain.CppDomain(env)\n    objects = list(domain.get_objects())\n    assert len(objects) == 2\n    for name, dispname, type, docname, anchor, priority in objects:\n        assert type == \"function\"\n        assert priority == 1\n        assert docname == \"doc1\" or docname == \"doc2\"\n        if docname == \"doc1\":\n            assert name == \"foo\" and dispname == \"foo\"\n        else:\n            assert name == \"bar\" and dispname == \"bar\"\n", "def test_stripped_name():\n    ast = parse('function', 'char foo(void);')\n    assert ast.name.get_display_string() == 'foo'\n    assert ast.name.get_display_string(True) == 'foo'\n    ast = parse('function', 'char foo(void) noexcept;')\n    assert ast.name.get_display_string() == 'foo'\n    assert ast.name.get_display_string(True) == 'foo noexcept'\n    ast = parse('function', 'char foo(void) const;')\n    assert ast.name.get_display_string() == 'foo'\n    assert ast.name.get_display_string(True) == 'foo const'\n    ast = parse('function', 'char foo(void) &&;')\n    assert ast.name.get_display_string() == 'foo'\n    assert ast.name.get_display_string(True) == 'foo &&'\n    ast = parse('function', 'char foo(void) &;')\n    assert ast.name.get_display_string() == 'foo'\n    assert ast.name.get_display_string(True) == 'foo &'\n    ast = parse('function', 'char foo(void) *;')\n    assert ast.name.get_display_string() == 'foo'\n    assert ast.name.get_display_string(True) == 'foo *'\n    ast = parse('function', 'char foo(void) const &;')\n    assert ast.name.get_display_string() == 'foo'\n    assert ast.name.get_display_string(True) == 'foo const &'\n", "def test_error_if_missing_type():\n    input = \"f();\"\n    with pytest.raises(DefinitionError):\n        check(\"function\", input, {1: \"c:function:f()\"})\n", "def test_type_alias():\n    check(\"type\", \"int MyInt\", {_max_id: \"cpp-type-MyInt\"}, \"int MyInt\", \"type\", \"MyInt\")\n", "def test_all_declarations():\n    # This test is designed to get as much coverage as possible\n    # by trying all kinds of different declarations.\n    # TODO: Add more test cases here to cover even more cases.\n\n    # Ordinary functions\n    check('function', 'void foo(int bar, float baz)', {1: 'foo', 2: 'foo'}, key=\"function\")\n    check('function', 'void foo(int bar, float baz) const', {1: 'foo', 2: 'foo'}, key=\"function\")\n    check('function', 'void foo(int bar, float baz) volatile', {1: 'foo', 2: 'foo'}, key=\"function\")\n    check('function', 'void foo(int bar, float baz) const volatile', {1: 'foo', 2: 'foo'}, key=\"function\")\n\n    # Variadic functions\n    check('function', 'void foo(int bar, float baz, ...)', {1: 'foo', 2: 'foo'}, key=\"function\")\n\n    # Function pointers\n    check('type', 'void (*foo)(int bar, float baz)', {1: 'foo', 2: 'foo'}, key=\"type\")\n    check('type', 'void (*foo)(int bar, float baz) const', {1: 'foo', 2: 'foo'}, key=\"type\")\n    check('type', 'void (*foo)(int bar, float baz) volatile', {1: 'foo', 2: 'foo'}, key=\"type\")\n    check('type', 'void (*foo)(int bar, float baz) const volatile', {1: 'foo', 2: 'foo'}, key=\"type\")\n\n    # Function pointer arrays\n    check('type', 'void (*foo[])(int bar, float baz)', {1: 'foo', 2: 'foo'}, key=\"type\")\n\n    # Member functions\n    check('function', 'void Foo::foo(int bar, float baz)', {1: 'Foo::foo', 2: 'Foo::foo'}, key=\"function\")\n    check('function', 'void Foo::foo(int bar, float baz) const', {1: 'Foo::foo', 2: 'Foo::foo'}, key=\"function\")\n    check('function', 'void Foo::foo(int bar, float baz) volatile', {1: 'Foo::foo', 2: 'Foo::foo'}, key=\""], "sample_907": ["def test_lookup_with_template_introduction():\n    parser = DefinitionParser('myFunction', location=None, config=None)\n    parser.allowFallbackExpressionParsing = False\n\n    root = Symbol(None, None, None, None, None, None, None)\n    astTemplate = parser.parse_declaration(\"function\", \"function\")\n    symbolTemplate = root.add_declaration(astTemplate, \"TestDoc\", 42)\n\n    astIntroduction = parser.parse_declaration(\"function\", \"concept C{...} \")\n    symbolIntroduction = root.add_declaration(astIntroduction, \"TestDoc\", 42)\n\n    astParam = parser.parse_declaration(\"function\", \"void param()\")\n    symbolParam = root.add_declaration(astParam, \"TestDoc\", 42)\n\n    # Check lookup with template params\n    lookupResult = root._symbol_lookup(\n        nestedName=parser._parse_nested_name(\"myFunction<int>\"),\n        templateDecls=[],\n        onMissingQualifiedSymbol=None,\n        strictTemplateParamArgLists=True,\n        ancestorLookupType=None,\n        templateShorthand=False,\n        matchSelf=False,\n        recurseInAnon=False,\n        correctPrimaryTemplateArgs=False,\n        searchInSiblings=False)\n\n    assert lookupResult.parentSymbol == symbolTemplate\n\n    # Check lookup with template introduction\n    lookupResult = root._symbol_lookup(\n        nestedName=parser._parse_nested_name(\"C{int}\"),\n        templateDecls=[],\n        onMissingQualifiedSymbol=None,\n        strictTemplateParamArgLists=True,\n        ancestorLookupType=None,\n        templateShorthand=False,\n        matchSelf=False,\n        recurseInAnon=False,\n        correctPrimaryTemplateArgs=False,\n        searchInSiblings=False)\n\n    assert lookupResult.parentSymbol == symbolIntroduction\n\n    # Check lookup without template params\n    lookupResult = root._symbol_lookup(\n        nestedName=parser._parse_nested_name(\"param\"),\n        templateDecls=[],\n        onMissingQualifiedSymbol=None,\n        strictTemplateParamArgLists=True,\n        ancestorLookupType=None,\n        templateShorthand=False,\n        matchSelf=False,\n        recurseInAnon=False,\n        correctPrimaryTemplateArgs=False,\n        searchInSiblings=False)\n\n    assert lookupResult.parentSymbol == symbolParam\n", "def test_add_member_with_same_name():\n    # Check that adding a member with the same name as an existing one correctly handles it.\n    root = Symbol(None, None, None, None, None, None, None)\n    parent = root.add_nested_name(\"Parent\")\n    child = parent.add_nested_name(\"Child\")\n    assert child.parent == parent\n\n    # Now try to add another Child.\n    child2 = parent.add_nested_name(\"Child\")\n    assert child2.parent == parent\n    assert child2 is child\n", "def test_template_operator():\n    input = \"template<typename T> operator T();\"\n    output = \"template<typename T> operator T()\"\n    idDict = {\n        1: 'opT',\n        2: 'cvT',\n    }\n    check(\"function\", input, idDict, output)\n    input = \"template<typename U> friend operator U();\"\n    output = \"template<typename U> friend operator U()\"\n    idDict = {\n        1: 'frdopU',\n        2: 'frdcvU',\n    }\n    check(\"function\", input, idDict, output)\n    input = \"template<typename U> friend operator const U();\"\n    output = \"template<typename U> friend operator const U()\"\n    idDict = {\n        1: 'frdopcU',\n        2: 'frdcvKU',\n    }\n    check(\"function\", input, idDict, output)\n", "def test_template_parameter_pack():\n    input = \"class X<T...>\"\n    output = \"class X<T...>\"\n    idDict = {1: 'X<T>', 2: '1IvE', 3: '1IvE', 4: '1IvE'}\n    check(\"class\", input, idDict, output)\n", "def test_template_parameter_parsing():\n    parser = DefinitionParser(\"template <typename T, int i>\",\n                              location=None, config=None)\n    parser.allowFallbackExpressionParsing = False\n    ast = parser._parse_template_parameter_list()\n    parser.assert_end()\n    assert len(ast.params) == 2\n    assert isinstance(ast.params[0], cppDomain.ASTTemplateParamType)\n    assert isinstance(ast.params[1], cppDomain.ASTTemplateParamNonType)\n"], "sample_911": ["def test_templateParams_with_nested_params():\n    ast = parse(\"function\", \"void f(A<T>::U x)\")\n    astParam = ast.function_params[0]\n    assert str(astParam.get_id(version=2)) == \"_j\"\n    # also test the name is not None\n    assert astParam.arg.name is not None\n", "def test_template_param_pack():\n    check(\"type\", r\"mytype<Types...>\", {3: \"mytype<Types@2>\"},\n          \"mytype<Types...>\")\n", "def test_member_function():\n    check(\"function\", \"void foo();\", {\n        1: \"TestDoc_foo\",\n        2: \"foo\",\n        3: \"foo\",\n        4: \"foo\"\n    }, \"void foo()\")\n", "def test_lookup_key():\n    root = Symbol(None, None, None, None, None, None)\n    rootSymbol = root.add_name(ASTNestedName([ASTNestedNameElement(ASTIdentifier(\"Test\"), None)], [False], rooted=False))\n    decl = parse(\"class\", \"class Test\")\n    symbol = rootSymbol.add_declaration(decl, docname=\"TestDoc\")\n    assert symbol.get_lookup_key().data == [\n        (ASTNestedNameElement(ASTIdentifier(\"Test\"), None), None, symbol.declaration.get_newest_id())\n    ]\n", "def test_template_declaration_prefix():\n    ast = parse(\"function\", \"template<typename T, U> void foo()\")\n    assert str(ast.templatePrefix.templates[0].params[0].get_identifier()) == \"T\"\n    assert ast.templatePrefix.templates[0].params[0].parameterPack is False\n    assert ast.templatePrefix.templates[0].params[0].isPack is False\n    assert ast.templatePrefix.templates[0].params[1].get_identifier() is None\n    assert ast.templatePrefix.templates[0].params[1].parameterPack is False\n    assert ast.templatePrefix.templates[0].params[1].isPack is False\n\n    with pytest.raises(DefinitionError):\n        parse(\"function\", \"template<template<typename T> typename U> void foo()\")\n\n    with pytest.raises(DefinitionError):\n        parse(\"function\", \"template<template<typename... T> typename... U> void foo()\")\n\n    with pytest.raises(DefinitionError):\n        parse(\"function\", \"template<template<template<typename T> typename U> typename V> void foo()\")\n"], "sample_913": ["def test_parse_annotation():\n    annotation = 'Union[str, int, float]'\n    children = _parse_annotation(annotation)\n    assert len(children) == 1\n    assert isinstance(children[0], addnodes.pending_xref)\n    assert children[0].astext() == 'Union[str, int, float]'\n\n    annotation = 'None'\n    children = _parse_annotation(annotation)\n    assert len(children) == 1\n    assert isinstance(children[0], addnodes.pending_xref)\n    assert children[0].astext() == 'None'\n", "def test_parse_annotation():\n    annotations = [\n        ('int', ['int']),\n        ('Union[int, str]', ['Union', '[', 'int', ',', ' ', 'str', ']']),\n        ('Tuple[int, ...]', ['Tuple', '[', 'int', ', ', '...']),\n        ('Optional[List[str]]', ['Optional', '[', 'List', '[', 'str', ']', ']']),\n        ('Any', ['Any']),\n        ('None', ['None']),\n        ('Tuple[Union[str, bytes, None], ...]', ['Tuple', '[', 'Union', '[', 'str', ',', ' ', 'bytes', ',', ' ', 'None', ']', ',', ' ', '...'])\n    ]\n    for annotation, expected_output in annotations:\n        node = _parse_annotation(annotation)\n        output = [n.astext() for n in node]\n        assert output == expected_output\n", "def test_parse_annotation():\n    annotation = \"Union[str, int]\"\n    nodes = _parse_annotation(annotation)\n    assert len(nodes) == 1\n    assert isinstance(nodes[0], pending_xref)\n    assert nodes[0].astext() == annotation\n\n    annotation = \"Optional[str]\"\n    nodes = _parse_annotation(annotation)\n    assert len(nodes) == 1\n    assert isinstance(nodes[0], pending_xref)\n    assert nodes[0].astext() == annotation\n\n    annotation = \"Tuple[str, int, float]\"\n    nodes = _parse_annotation(annotation)\n    assert len(nodes) == 1\n    assert isinstance(nodes[0], pending_xref)\n    assert nodes[0].astext() == annotation\n\n    annotation = \"Dict[str, int]\"\n    nodes = _parse_annotation(annotation)\n    assert len(nodes) == 1\n    assert isinstance(nodes[0], pending_xref)\n    assert nodes[0].astext() == annotation\n\n    annotation = \"List[str]\"\n    nodes = _parse_annotation(annotation)\n    assert len(nodes) == 1\n    assert isinstance(nodes[0], pending_xref)\n    assert nodes[0].astext() == annotation\n\n    annotation = \"bool\"\n    nodes = _parse_annotation(annotation)\n    assert len(nodes) == 1\n    assert isinstance(nodes[0], pending_xref)\n    assert nodes[0].astext() == annotation\n\n    annotation = \"None\"\n    nodes = _parse_annotation(annotation)\n    assert len(nodes) == 1\n    assert isinstance(nodes[0], pending_xref)\n    assert nodes[0].astext() == annotation\n", "def test_parse_annotation():\n    annotation = \"Union[int, str]\"\n    result = _parse_annotation(annotation)\n    assert len(result) == 3\n    assert isinstance(result[0], addnodes.pending_xref)\n    assert result[1].astext() == \"[\"\n    assert result[2].astext() == \"]\"\n\n    annotation = \"List[Tuple[int, str]]\"\n    result = _parse_annotation(annotation)\n    assert len(result) == 3\n    assert isinstance(result[0], addnodes.pending_xref)\n    assert result[1].astext() == \"[\"\n    assert result[2].astext() == \"]\"\n\n    annotation = \"None\"\n    result = _parse_annotation(annotation)\n    assert len(result) == 1\n    assert isinstance(result[0], addnodes.pending_xref)\n", "def test_parse_annotation():\n    annotation = \"Union[int, str]\"\n    result = _parse_annotation(annotation)\n    assert len(result) == 8\n    assert result[0].astext() == 'Union'\n    assert result[1].astext() == '['\n    assert result[2].astext() == 'int'\n    assert result[3].astext() == ', '\n    assert result[4].astext() == 'str'\n    assert result[5].astext() == ']'\n"], "sample_917": ["compilation error", "def test_template_type_parameter():\n    # Check template type parameter parsing\n    check(\"templateParam\", \"typename T\", {1: None, 2: \"0\", 3: \"0\", 4: \"0\"})\n    check(\"templateParam\", \"class T\", {1: None, 2: \"0\", 3: \"0\", 4: \"0\"})\n    check(\"templateParam\", \"typename... Ts\", {1: None, 2: \"Dp\", 3: \"Dp\", 4: \"Dp\"})\n    check(\"templateParam\", \"class... Ts\", {1: None, 2: \"Dp\", 3: \"Dp\", 4: \"Dp\"})\n    check(\"templateParam\", \"int N\", {1: None, 2: \"_0\", 3: \"_0\", 4: \"_0\"})\n    check(\"templateParam\", \"int... N\", {1: None, 2: \"_Dp\", 3: \"_Dp\", 4: \"_Dp\"})\n    check(\"templateParam\", \"bool b\", {1: None, 2: \"_0\", 3: \"_0\", 4: \"_0\"})\n    check(\"templateParam\", \"bool... b\", {1: None, 2: \"_Dp\", 3: \"_Dp\", 4: \"_Dp\"})\n", "def test_alias_transform():\n    class TestNode(nodes.Element):\n            super().__init__()\n            self.sig = sig\n            if env is not None:\n                if 'cpp:parent_symbol' not in env.temp_data:\n                    root = env.domaindata['cpp']['root_symbol']\n                    env.temp_data['cpp:parent_symbol'] = root\n                self.parentKey = env.temp_data['cpp:parent_symbol'].get_lookup_key()\n            else:\n                self.parentKey = None\n\n            return self.__class__(self.sig, env=None)\n\n    app = docutils.DocutilsTestApp()\n    env = app.create_environ()\n    env.prepare_settings()\n    app.create_doctree(env)\n    env.app.verbosity = 0\n    config = env.config\n    config.cpp_index_common_prefix = []\n    config.cpp_id_attributes = []\n    config.cpp_paren_attributes = []\n    env.domaindata['cpp'] = {\n        'root_symbol': Symbol(None, None, None, None, None, None),\n        'names': {}\n    }\n    env.ref_context = {}\n    env.temp_data = {}\n    transform = cppDomain.AliasTransform(app.env)\n\n    # Test that an error during parsing processes the node and continues.\n    # Subclassing DocutilsTestApp sets verbosity to 1, so the error should be printed.\n    node = TestNode(\"Not a valid C++ expression\")\n    app.do_parse(node)\n    transform.apply()\n    print(node.pformat())\n    assert node.children\n    assert len(node.children) == 1\n    assert isinstance(node.children[0], nodes.literal)\n    assert node.children[0].astext() == node.sig\n\n    # Test that an unparseable non-shorthand is processed.\n    node = TestNode(\"const int&\")\n    app.do_parse(node)\n    transform.apply()\n    print(node.pformat())\n    assert node.children\n    assert len(node.children) == 1\n    assert isinstance(node.children[0], addnodes.desc_signature)\n    assert node.children[0].astext() == \"const int&\"\n\n    # Test that an unparseable shorthand is processed.\n    node = TestNode(\"std::vector<int>\")\n    app.do_parse(node)\n    transform.apply()\n    print(node.pformat())\n    assert node.children\n    assert len(node.children) == 1\n    assert isinstance(node.children[0], addnodes.desc_signature)\n   ", "def test_template_declaration_prefix():\n    ast = parse('class', \"template <typename T> class A { }\")\n    assert ast.templatePrefix is not None\n    templates = ast.templatePrefix.templates\n    assert templates is not None\n    assert len(templates) == 1\n    assert isinstance(templates[0], cppDomain.ASTTemplateParams)\n    assert len(templates[0].params) == 1\n    param = templates[0].params[0]\n    assert isinstance(param, cppDomain.ASTTemplateParamType)\n    data = param.data\n    assert data.key == 'typename'\n    identifier = data.identifier\n    assert identifier is not None\n    assert identifier.identifier == 'T'\n    assert data.parameterPack is False\n    assert data.default is None\n\n    ast = parse('class', \"template <typename T, typename U> class A { }\")\n    assert ast.templatePrefix is not None\n    templates = ast.templatePrefix.templates\n    assert templates is not None\n    assert len(templates) == 1\n    assert isinstance(templates[0], cppDomain.ASTTemplateParams)\n    assert len(templates[0].params) == 2\n    param = templates[0].params[0]\n    assert isinstance(param, cppDomain.ASTTemplateParamType)\n    data = param.data\n    assert data.key == 'typename'\n    identifier = data.identifier\n    assert identifier is not None\n    assert identifier.identifier == 'T'\n    assert data.parameterPack is False\n    assert data.default is None\n    param = templates[0].params[1]\n    assert isinstance(param, cppDomain.ASTTemplateParamType)\n    data = param.data\n    assert data.key == 'typename'\n    identifier = data.identifier\n    assert identifier is not None\n    assert identifier.identifier == 'U'\n    assert data.parameterPack is False\n    assert data.default is None\n\n    ast = parse('class', \"template <typename T, typename... U> class A { }\")\n    assert ast.templatePrefix is not None\n    templates = ast.templatePrefix.templates\n    assert templates is not None\n    assert len(templates) == 1\n    assert isinstance(templates[0], cppDomain.ASTTemplateParams)\n    assert len(templates[0].params) == 2\n    param = templates[0].params[0]\n    assert isinstance(param, cppDomain.ASTTemplateParamType)\n    data = param.data\n   ", "def test_template_parameter_pack():\n    check('type',\n          r'template <class ... Ts> class Test;',\n          {1: 'Test',\n           2: '9class0E...E',\n           3: 'Vt0Dp0class0E...E',\n           4: 'Ut9Vt0Dp0class0E...E'})\n"], "sample_923": ["def test_type_with_id():\n    input = \"int myType id_attr\"\n    idDict = {1: \"cpp-type-myType\", 2: \"cpp-type-myType\", 3: \"cpp-type-myType\"}\n    check(\"type\", input, idDict)\n", "def test_type_specifier_enum():\n    input = \"enum TestEnum { TEST_penalty }\"\n    idDict = {1: \"c.type.TestEnum\", 2: \"c.enum.TestEnum\"}\n    check(\"type\", input, idDict)\n", "def test_nested_names():\n    input = 'A::B::C a'\n    ast = parse('var', input)\n    assert str(ast.name) == 'A::B::C'\n    assert ast.name.get_display_string() == 'A::B::C'\n    assert ast.name.get_id(version=2) == 'A::B::C'\n    assert ast.name.get_id(version=1) == 'A::B::C'\n\n    input = '::A::B::C a'\n    ast = parse('var', input)\n    assert str(ast.name) == '::A::B::C'\n    assert ast.name.get_display_string() == '::A::B::C'\n    assert ast.name.get_id(version=2) == '::A::B::C'\n    assert ast.name.get_id(version=1) == '::A::B::C'\n", "def test_member():\n    check(\"member\", \"int * x\", {1: \"int* x\", 2: \"int *x\"},\n          asTextOutput=\"int * TestDoc::x\")\n    check(\"member\", \"const int & x\", {1: \"int x\", 2: \"const int& x\"},\n          asTextOutput=\"const int & TestDoc::x\")\n    check(\"member\", \"int const * x\", {1: \"int x\", 2: \"int const* x\"},\n          asTextOutput=\"int const * TestDoc::x\")\n    check(\"member\", \"int const & x\", {1: \"int x\", 2: \"int const& x\"},\n          asTextOutput=\"int const & TestDoc::x\")\n    check(\"member\", \"const int const & x\", {1: \"int x\", 2: \"const int const& x\"},\n          asTextOutput=\"const int const & TestDoc::x\")\n    check(\"member\", \"int * const x\", {1: \"int x\", 2: \"int* const x\"},\n          asTextOutput=\"int * const TestDoc::x\")\n    check(\"member\", \"int x\", {1: \"int x\", 2: \"int x\"},\n          asTextOutput=\"int TestDoc::x\")\n    check(\"member\", \"int const x\", {1: \"int x\", 2: \"int const x\"},\n          asTextOutput=\"int const TestDoc::x\")\n    check(\"member\", \"const int x\", {1: \"int x\", 2: \"const int x\"},\n          asTextOutput=\"const int TestDoc::x\")\n    check(\"member\", \"int const * const x\", {1: \"int x\", 2: \"int const* const x\"},\n          asTextOutput=\"int const * const TestDoc::x\")\n    check(\"member\", \"int * * const x\", {1: \"int x\", 2: \"int** const x\"},\n          asTextOutput=\"int * * const TestDoc::x\")\n    check(\"member\", \"int const * * const x\", {1: \"int x\", 2: \"int const** const x\"},\n          asTextOutput=\"int const * * const TestDoc::x\")\n    check(\"member\", \"int * * x\", {1: \"int x\", 2: \"int** x\"},\n", "def test_macro():\n    check('macro', 'MACRO', {1: 'MACRO'}, 'MACRO')\n    check('macro', 'MACRO(a, b, c)', {1: 'MACRO'}, 'MACRO(a, b, c)')\n    check('macro', 'MACRO(a, b, ...)', {1: 'MACRO'}, 'MACRO(a, b, ...)')\n    check('macro', 'MACRO(', {1: 'MACRO'}, 'MACRO(', asTextOutput='MACRO (')\n    check('macro', 'MACRO()', {1: 'MACRO'}, 'MACRO()')\n    check('macro', 'MACRO, ignored', {1: 'MACRO'}, 'MACRO')\n    check('macro', 'MACRO(a, b, c, ignored)', {1: 'MACRO'}, 'MACRO(a, b, c)')\n    check('macro', 'MACRO(a, b, ..., ignored)', {1: 'MACRO'}, 'MACRO(a, b, ...)')\n"], "sample_919": ["def test_template_params() -> None:\n    ast = parse(\"function\", \"template<typename A, typename B> void foo(A, B);\")\n    assert ast.templatePrefix\n    assert len(ast.templatePrefix.templates) == 1\n    assert len(ast.templatePrefix.templates[0].params) == 2\n    param1 = ast.templatePrefix.templates[0].params[0]\n    assert param1.get_identifier().identifier == \"A\"\n    param2 = ast.templatePrefix.templates[0].params[1]\n    assert param2.get_identifier().identifier == \"B\"\n\n    ast = parse(\"function\", \"template<typename A, typename B, typename... Ts> void foo(A, B, Ts...);\")\n    assert ast.templatePrefix\n    assert len(ast.templatePrefix.templates) == 1\n    assert len(ast.templatePrefix.templates[0].params) == 3\n    param1 = ast.templatePrefix.templates[0].params[0]\n    assert param1.get_identifier().identifier == \"A\"\n    param2 = ast.templatePrefix.templates[0].params[1]\n    assert param2.get_identifier().identifier == \"B\"\n    param3 = ast.templatePrefix.templates[0].params[2]\n    assert param3.get_identifier().identifier == \"Ts\"\n    assert param3.isPack\n", "def test_template_decl_params():\n    name = 'templateParam'\n    input = 'template <typename T, int N, bool = true, typename... Ts> T'\n\n    idDict = {\n        1: None,\n        2: \"cT:A0\",\n        3: \"cT:A0\",\n        4: \"cT:A0TcDp\"\n    }\n    output = 'template <typename T, int N, bool = true, typename... Ts> T'\n\n    check(name, input, idDict, output)\n\n", "def check_lookup(name, decl, scope, expected):\n    rootSymbol = Symbol(None, None, None, None, None, None)\n    symbol = rootSymbol.add_declaration(parse(name, decl), docname=\"TestDoc\")\n    scopeSymbol = rootSymbol\n    for s in scope:\n        scopeSymbol = scopeSymbol.add_name(s)\n    result = scopeSymbol.find_name(ASTNestedName([ASTNestedNameElement(ASTIdentifier(\"x\"), None)],\n                                                 [False], rooted=False), [], 'any', True, True, True, True)[0]\n    if result is not None:\n        result = result.get_full_nested_name().get_id(5)\n    if result != expected:\n        print(\"Result:   \", result)\n        print(\"Expected: \", expected)\n        print(rootSymbol.dump(0))\n        raise DefinitionError(\"\")\n", "def test_template_parameter_number_mismatch():\n    # Regression test for #10437\n    input = \"template<typename T, typename U> void f()\"\n    with pytest.raises(DefinitionError):\n        parse(\"function\", input)\n", "def test_template_parameter_pack_data():\n    check(\n        'function',\n        'void f(Types...) ;',\n        {1: None, 2: 'T', 3: 'Dp', 4: 'Dp'},\n        'void f(Types...) ;',\n        key='function')\n    check(\n        'function',\n        'void f(Types...Ts) ;',\n        {1: None, 2: 'T', 3: 'Dp', 4: 'Dp'},\n        'void f(Types...Ts) ;',\n        key='function')\n    check(\n        'function',\n        'void f(Types...) ;',\n        {1: None, 2: '0', 3: 'Dp', 4: 'Dp'},\n        'void f(Types...) ;',\n        key='function templateParam')\n    check(\n        'function',\n        'void f(Types...Ts) ;',\n        {1: None, 2: '0', 3: 'Dp', 4: 'Dp'},\n        'void f(Types...Ts) ;',\n        key='function templateParam')\n"], "sample_924": ["def test_template_introduction():\n    input = \"concept C<T...> { };\"\n    output = \"concept C<T...> { };\"\n    idDict = {\n        1: None,\n        2: \"Ic@IcC#E#T#...#E#E\",\n        3: \"Ic@IcC#E#T#...#E#E\",\n        4: \"Ic@IcC#E#T#...#E#E\"\n    }\n    check('concept', input, idDict, output, key=None, asTextOutput=None)\n", "def test_template_with_anon_params():\n    check(\n        'function',\n        'void foo(int, int)',\n        {1: 'foo(int,int)', 2: 'foo#U0l#U0l#U', 3: 'foo#U0l#U0l#U', 4: 'foo#U0l#U0l#UE'},\n        key='function',\n        asTextOutput='void foo(int, int)'\n    )\n", "def test_template_parameter_pack():\n    check('function',\n          'void f(int..., long)',\n          {1: None, 2: 'FvvE', 3: 'FvvE', 4: 'FvvE'},\n          'void f(int..., long)')\n", "def test_lookup_fails():\n    rootSymbol = cppDomain.Symbol(None, None, None, None, None, None)\n    symbol = rootSymbol.add_name(cppDomain.ASTNestedName([cppDomain.ASTNestedNameElement(cppDomain.ASTIdentifier(\"Type\"), None)], [False], rooted=False))\n    lookupResult = rootSymbol._symbol_lookup(symbol.declaration.name, [], None, strictTemplateParamArgLists=False, ancestorLookupType=\"class\", templateShorthand=False, matchSelf=False, recurseInAnon=False, correctPrimaryTemplateArgs=False, searchInSiblings=False)\n    assert lookupResult is None\n", "def test_simple_declarator():\n    check(\n        \"type\",\n        \"t\",\n        {1: \"t\", 2: \"_CPPv2t\", 3: \"_CPPv3t\", 4: \"_CPPv4t\"},\n        key=None\n    )\n"], "sample_927": ["def test_template_parameter_with_nested_template():\n    input = \"template <template <typename U> class T> class Test;\"\n    key = \"class \"\n    output = \"template <template <typename U> class T> class Test\"\n    idDict = {\n        1: None,\n        2: \"IvE\",\n        3: \"IT_IvEE\",\n        4: \"IT_IvEE\"\n    }\n    check(\"class\", input, idDict, output, key)\n", "def test_template_parameter_pack():\n    check(\"function\", \"void foo(Ts... ts)\", {\n        1: None,\n        2: \"foo#U1\",\n        3: \"foo#U1\",\n        4: \"foo#U1\",\n    }, \"void foo(Ts... ts)\")\n", "def test_type_with_nested_name():\n    check('type',\n          \"const int& (MyClass::*Type)\",  # input\n          {1: None, 2: \"c.v.5MyClass.3a3a3a4Type\", 3: \"DKc.v.5MyClass.3a3a3a4Type\", 4: \"DKc.v.5MyClass.3a3a3a4Type\"},\n          \"const int& MyClass::* Type\")\n", "def test_lookup_direct():\n    rootSymbol = Symbol(None, None, None, None, None, None)\n    names = [ASTNestedName([ASTNestedNameElement(ASTIdentifier(\"A\"), None)],\n                           [False], rooted=False),\n             ASTNestedName([ASTNestedNameElement(ASTIdentifier(\"B\"), None)],\n                           [False], rooted=False),\n             ASTNestedName([ASTNestedNameElement(ASTIdentifier(\"C\"), None)],\n                           [False], rooted=False),\n             ASTNestedName([ASTNestedNameElement(ASTIdentifier(\"D\"), None)],\n                           [False], rooted=False)]\n    symbols = [rootSymbol.add_name(name, None) for name in names]\n    key = LookupKey([(names[0].names[0], None, None),\n                     (names[1].names[0], None, None),\n                     (names[2].names[0], None, None),\n                     (names[3].names[0], None, None)])\n    assert rootSymbol.direct_lookup(key) == symbols[3]\n", "def test_new_in_200():\n    input = \"std::new Foo\"\n    check(\"function\", input, {\n        1: \"std::newFoo\",\n        2: \"std::nw__0FooE\",\n        3: \"std::nw__0FooE\",\n        4: \"std::nw__0FooE\",\n    })\n"], "sample_926": ["def test_alias():\n    env = docutils.DocutilsTestSettings(env=docutils.DocutilsTestSettings())\n    env.temp_data['c:parent_symbol'] = Symbol(None, None, None, None, None, None)\n    app = restructuredtext.AppTestBuilder(env).create_app()\n    restructuredtext.parse('.. c:alias:: std::vector', app=app)\n    assert_node(app.doctree, aliasnode)\n    assert_node(aliasnode, addnodes.desc, docutils.nodes.Text, text='std::vector')\n", "def test_all_declarations(context):\n    \"\"\"\n    test all declarations\n    \"\"\"\n    snippet = \"\"\"", "def test_function_fallback_expression():\n    input = \"void f() = delete\"\n    with pytest.raises(DefinitionError):\n        parse('function', input)\n    parser = DefinitionParser(input, location=None, config=None)\n    parser.allowFallbackExpressionParsing = False\n    with pytest.raises(DefinitionError):\n        ast = parser.parse_declaration(\"function\", \"function\")\n    parser = DefinitionParser(input, location=None, config=None)\n    parser.allowFallbackExpressionParsing = True\n    ast = parser.parse_declaration(\"function\", \"function\")\n    assert isinstance(ast.declaration, cppDomain.ASTFallbackExpr)\n", "    def test_CObject(self, test_input, expected_output):\n        app = restructuredtext.app_test()\n        app.builder.add_object_type('func', 'func', objname='function',\n                                    indextemplate='pair: %s; function')\n        node = docutils.nodes.paragraph()\n        node.source = 'test.rst'\n        node.line = 1\n        app.parser.parse(\".. c:function:: %s\" % test_input, node)\n        doc = app.doctree\n        node = doc[0]\n        assert isinstance(node, desc)\n        signode = node[0]\n        assert isinstance(signode, addnodes.desc_signature)\n        assert_node(signode.astext(), expected_output)\n", "def test_function_has_no_name():\n    input = \"void foo1(int arg1, int arg2)\"\n    idDict = {\n        4: \"cpp:function:foo1(int,int)\",\n        5: \"cpp:function:foo1(int,arg2)\",\n    }\n    check(\"function\", input, idDict)\n"], "sample_934": ["def test_invalid_type():\n    with pytest.raises(DefinitionError):\n        parse('type', 'void f()')\n", "def test_class_constructor():\n    check(\"function\", \"class TestClass { public: TestClass(); }; TestClass::TestClass()\",\n          {2: \"c.TestClass.TestClass\", 3: \"c.TestClass.TestClass\"},\n          \"class TestClass {   public: TestClass(); }; TestClass::TestClass()\")\n", "def test_type_with_paren():\n    check(\n        \"type\",\n        \"typedef void (*pFunc)(int arg1, int arg2) pFunc;\",\n        {\n            1: \"f0a65a400230a7a651502d2f0a1a4d9c\",\n            2: \"36a45de5c3815d4b1aa93a27346ba8e5\",\n            3: \"55ebfbb6a64df789e7e9b0b3c6d16793\"\n        },\n        key=\"typedef \"\n    )\n", "def test_add_function_params():\n    rootSymbol = Symbol(None, None, None, None, None, None)\n    ast = parse(\"function\", \"void foo(int x)\")\n    symbol = rootSymbol.add_declaration(ast, docname=\"TestDoc\")\n    assert len(symbol._children) == 1\n    paramSymbol = symbol._children[0]\n    assert paramSymbol.parent == symbol\n    assert paramSymbol.declaration == ast.function_params[0]\n    assert paramSymbol.docname == \"TestDoc\"\n", "def test_redeclare_as_function():\n    input = \"int foo\"\n    idDict = {1: \"c.intfoo\", 2: \"c.foo\"}\n    check(\"function\", input, idDict, asTextOutput=\"int foo\")\n"], "sample_938": ["def test_default_man_pages(monkeypatch):\n    class TestConfig(Config):\n        project = 'Test Project'\n        release = '1.0'\n        author = 'Test Author'\n        master_doc = 'index'\n\n    config = TestConfig()\n    result = default_man_pages(config)\n\n    assert result == [('index', 'test-project', 'Test Project 1.0', ['Test Author'], 1)]\n", "def test_default_man_pages():\n    config = Config(\n        project='Sample Project',\n        release='1.0',\n        author='Author Name',\n        master_doc='index',\n    )\n    pages = default_man_pages(config)\n    assert len(pages) == 1\n    page = pages[0]\n    assert page == ('index', 'sample-project', 'Sample Project 1.0', ['Author Name'], 1)\n", "def test_default_man_pages():\n    config = Config({\n        'project': 'Test Project',\n        'release': '1.0',\n        'author': 'John Doe',\n        'master_doc': 'index'\n    })\n\n    man_pages = default_man_pages(config)\n\n    assert len(man_pages) == 1\n    assert man_pages[0] == ('index', 'test-project', 'Test Project 1.0', ['John Doe'], 1)\n", "def test_default_man_pages():\n    config = Config({\n        'project': 'Sample Project',\n        'release': '1.0',\n        'author': 'John Doe',\n        'master_doc': 'index'\n    })\n    pages = default_man_pages(config)\n    assert len(pages) == 1\n    page = pages[0]\n    assert page[0] == 'index'\n    assert page[1] == 'sample-project'\n    assert page[2] == 'Sample Project 1.0'\n    assert page[3] == ['John Doe']\n    assert page[4] == 1\n", "def test_default_man_pages():\n    config = Config(master_doc='index', project='My Project', release='1.0', author='John Doe')\n    pages = default_man_pages(config)\n    assert len(pages) == 1\n    assert pages[0] == ('index', 'my-project', 'My Project 1.0', ['John Doe'], 1)\n"], "sample_932": ["def test_template_parameter_pack():\n    ast = parse(\"function\",\n                \"void foo(myType<Args...>);\")\n    assert ast.templatePrefix.templates is not None\n    assert len(ast.templatePrefix.templates) == 1\n    templateParams = ast.templatePrefix.templates[0]\n    assert len(templateParams.params) == 1\n    param = templateParams.params[0]\n    assert param.isPack\n    assert param.name is not None\n    assert param.name.get_display_string() == \"Args\"\n    assert param.default is None\n\n    ast = parse(\"function\",\n                \"void foo(myType<Args..., int>);\")\n    assert ast.templatePrefix.templates is not None\n    assert len(ast.templatePrefix.templates) == 1\n    templateParams = ast.templatePrefix.templates[0]\n    assert len(templateParams.params) == 2\n    param = templateParams.params[0]\n    assert param.isPack\n    assert param.name is not None\n    assert param.name.get_display_string() == \"Args\"\n    assert param.default is None\n    param = templateParams.params[1]\n    assert not param.isPack\n    assert param.name is None\n    assert param.default is None\n", "def test_class_template_with_args():\n    check(\n        \"class\",\n        \"class myClass<int>\",\n        {\n            1: \"myClass-1\",\n            2: \"S1IiE\",\n            3: \"S1IiE\",\n            4: \"S1IiE\"\n        },\n        key=\"class\",\n        asTextOutput=\"class myClass<int>;\"\n    )\n", "def test_template_PARAM_with_id_attributes():\n    ast = parse('function',\n                'void foo(int arg0 [[id_attr=\"value0\"]], int arg1 [[id_attr=\"value1\"]])')\n    args = ast.function_params\n    assert len(args) == 2\n    assert args[0].get_id(1) == 'value0'\n    assert args[1].get_id(1) == 'value1'\n", "def test_class_scoped():\n    check(\n        'class',\n        'namespace Test { class Test { } } class Test { }',\n        {1: 'Test::Test', 2: 'N3Test3TestE', 3: 'N3Test3TestE', 4: 'N3Test3TestE'},\n        'namespace Test { class Test { }; } class Test { }',\n        key='class '\n    )\n", "def test_class_template():\n    input = \"class my::Class\"\n    idDict = {\n        1: \"my::Class\",\n        2: \"N3my5ClassE\",\n        3: \"N3my5ClassE\",\n        4: \"N3my5ClassE\",\n    }\n    check('class', input, idDict)\n"], "sample_935": ["def test_template_parameter_pack():\n    check(\"function\", \"void foo(Ts... ts)\", {\n        1: None,\n        2: \"foo#Ts#\",\n        3: \"foo#DpTs#\",\n        4: \"foo#DpTs#\"\n    }, \"void foo(Ts... ts)\", key=\"fn\")\n", "def test_template_param_pack():\n    check(\"function\", \"void f(param...)\", {4: \"Dp\"},\n          \"void f(param...)\", \"function \")\n    check(\"function\", \"void f(int...)\", {4: \"Di\"},\n          \"void f(int...)\", \"function \")\n    check(\"function\", \"void f(int...) const\", {4: \"DcDKi\"},\n          \"void f(int...) const\", \"function \")\n    check(\"function\", \"void f(const int...) const\", {4: \"DcDKi\"},\n          \"void f(const int...) const\", \"function \")\n    check(\"function\", \"void f(const int...) const volatile\", {4: \"DcDVKi\"},\n          \"void f(const int...) const volatile\", \"function \")\n    check(\"function\", \"void f(const int...) &\", {4: \"DcDRKi\"},\n          \"void f(const int...) &\", \"function \")\n    check(\"function\", \"void f(const int...) &&\", {4: \"DcDOKi\"},\n          \"void f(const int...) &&\", \"function \")\n    check(\"function\", \"void f(const int...) noexcept\", {4: \"DnDKi\"},\n          \"void f(const int...) noexcept\", \"function \")\n", "def test_class():\n    check('class', \"class Test\", {1: \"Test\", 2: \"3Test\", 3: \"_ZN4TestE\", 4: \"_ZN4TestE\"},\n          key=None)\n", "def test_alias():\n    # Test parsing an alias and the rendered xref.\n    env = docutils.DocutilsBuildEnvironment(app=None)\n    env.prepare_settings('dummy')\n    env.update_settings({'cpp_index_common_prefix': [], 'cpp_paren_attributes': []})\n    cppDomain.setup(app=None)\n    env.app.add_domain(cppDomain.CPPDomain)\n    env.app.add_directive('cpp:alias', cppDomain.CPPAliasObject)\n    restructuredtext.parse_directive(env=env,\n                                     contents='.. cpp:alias:: std::vector<int>', # NOQA\n                                     title='',\n                                     parent=document.Document('root'),\n                                     linenos=docutils.utils.lines(0, len_dt_common_prefix: 0))\n    refNodes = list(document.traverse(addnodes.pending_xref))\n    assert len(refNodes) == 1\n    refNode = refNodes[0]\n    key = refNode['reftarget']\n    assert key == 'std::vector<int>'\n    parentKey = refNode['cpp:parent_key']\n    assert parentKey\n    parser = DefinitionParser(key, location=None, config=None)\n    parser.allowFallbackExpressionParsing = False\n    ast = parser.parse_xref_object()\n    assert isinstance(ast, cppDomain.ASTDeclaration)\n    assert ast.objectType == 'type'\n    assert ast.name.get_display_string() == 'std::vector<int>'\n", "def test_xref_operator_parentheses():\n    env = docutils.build_environ(settings={'cpp_index_common_prefix': ['std::']})\n    config = env.config\n    config.cpp_index_common_prefix = ['std::']\n\n    ast = parse(\"function\", \"std::string foo()\")\n    rootSymbol = Symbol(None, None, None, None, None, None)\n    symbol = rootSymbol.add_declaration(ast, docname=\"TestDoc\")\n    symbol.docstring = \"Dummy docstring\"\n    env.domaindata['cpp']['root_symbol'].merge_with(rootSymbol, [], env)\n\n    node = addnodes.pending_xref('foo', 'foo', refdomain='cpp', reftype='func')\n    with pytest.raises(NoUri):\n        env.get_domain('cpp').resolve_xref(env, \"test\", \"builder\", \"func\", \"foo\",\n                                            node, node)\n    node = addnodes.pending_xref('foo()', 'foo', refdomain='cpp', reftype='func')\n    xref = env.get_domain('cpp').resolve_xref(env, \"test\", \"builder\", \"func\", \"foo()\",\n                                               node, node)\n    assert xref != None\n    assert xref.astext() == \"foo()\"\n"], "sample_940": ["def test_isdescriptor():\n    class RaisingDescriptor:\n            raise ZeroDivisionError\n\n            pass\n\n    assert inspect.isdescriptor(RaisingDescriptor())\n", "def test_issubclass():\n    assert inspect.isenumclass(datetime.datetime) is False\n    assert inspect.isenumclass(_testcapi.Enum) is True\n\n", "def test_isdescriptor():\n    class DummyDescr:\n            return instance\n\n    class Dummy:\n        attr = DummyDescr()\n\n    assert inspect.isdescriptor(DummyDescr())\n    assert inspect.isdescriptor(Dummy.__dict__[\"attr\"])\n    assert not inspect.isdescriptor(123)\n    assert not inspect.isdescriptor(\"string\")\n    assert not inspect.isdescriptor(Dummy())\n", "def test_signature_from_ast():\n    # Create a simple AST node for a function\n    node = ast.parse(\"def func(a: int, b: str) -> None: pass\").body[0]\n\n    # Create a Signature object from the AST node\n    sig = inspect.signature_from_ast(node)\n\n    # Check the parameters and return annotation of the Signature object\n    assert len(sig.parameters) == 2\n    assert sig.parameters['a'].annotation is int\n    assert sig.parameters['b'].annotation is str\n    assert sig.return_annotation is type(None)\n", "def test_signature_from_str__without_return_annotation():\n    sig = inspect.signature_from_str('(a, b, c)')\n    assert list(sig.parameters.keys()) == ['a', 'b', 'c']\n    assert sig.return_annotation is Parameter.empty\n"], "sample_941": ["def test_restify_broken_type():\n    assert restify(BrokenType) == ':class:`BrokenType`'\n", "def test_restify_forward_ref():\n    # ForwardRef is a special case, as it doesn't have __origin__ and __args__\n    fr = __import__(\"typing\")._ForwardRef('int')\n    assert restify(fr) == ':class:`int`'\n    assert restify(fr) != ':class:`str`'\n    if sys.version_info >= (3, 7):\n        from typing import ForwardRef as ForwardRef_37\n        fr = ForwardRef_37('int')\n        assert restify(fr) == ':class:`int`'\n        assert restify(fr) != ':class:`str`'\n", "def test_restify_forward_ref():\n    assert restify(ForwardRef('MyClass1')) == ':class:`MyClass1`'\n    assert restify(ForwardRef('MyClass2')) == ':class:`MyClass2`'\n    assert restify(ForwardRef('UnknownClass')) == ':class:`UnknownClass`'\n", "def test_restify_special_cases():\n    assert restify(None) == ':obj:`None`'\n    assert restify(Ellipsis) == '...'\n    assert restify(Struct) == ':class:`struct.Struct`'\n", "def test_restify_broken_type():\n    with pytest.warns(UserWarning):\n        assert restify(BrokenType) == ':class:`BrokenType`'\n"], "sample_939": ["def test_unparse_arguments(source, expected):\n    node = ast.parse(source, mode='exec').body[0].args\n    assert ast.unparse(node) == expected\n", "def test_unparse_arguments(source, expected):\n    tree = ast.parse(source)\n    func_def = tree.body[0]\n    assert ast.unparse(func_def.args) == expected\n", "def test_parse_arguments(source, expected):\n    node = ast.parse(source).body[0].args  # type: ignore\n    assert ast.unparse(node) == expected\n", "def test_unparse_arguments(source, expected):\n    node = ast.parse(source).body[0].args\n    assert ast.unparse(node) == expected\n", "def test_unparse_arguments(source, expected):\n    node = ast.parse(source).body[0].args  # type: ignore\n    result = ast.unparse(node)\n    assert result == expected\n"], "sample_944": ["def test_restify_builtin_classes_with_invalid_module():\n    assert restify(Struct) == ':class:`struct.Struct`'\n    assert restify(TracebackType) == ':class:`types.TracebackType`'\n", "def test_stringify_broken_type():\n    assert stringify(BrokenType()) == repr(BrokenType())\n", "def test_restify_with_invalid_builtins():\n    assert restify(Struct) == ':class:`struct.Struct`'\n    assert restify(TracebackType) == ':class:`types.TracebackType`'\n", "def test_restify_forward_ref():\n    class ForwardRefClass:\n        pass\n\n    forward_ref = ForwardRef('ForwardRefClass')\n    assert restify(forward_ref) == ':class:`ForwardRefClass`'\n    if sys.version_info < (3, 7):\n        forward_ref = typing._ForwardRef('ForwardRefClass')\n        assert restify(forward_ref) == ':class:`ForwardRefClass`'\n", "def test_restify_forward_ref():\n    ref = typing.ForwardRef('MyClass1')\n    assert restify(ref) == ':class:`MyClass1`'\n\n    # Check _ForwardRef for Python 3.6\n    if sys.version_info < (3, 7):\n        from typing import _ForwardRef  # type: ignore\n        ref = _ForwardRef('MyClass1')\n        assert restify(ref) == ':class:`MyClass1`'\n"], "sample_943": ["def test_implicit_namespaces(apidoc):\n    # Test that implicit namespaces are handled correctly\n    # by checking that the output directory contains the expected files\n    assert (apidoc.outdir / 'test_root.rst').exists()\n    assert (apidoc.outdir / 'test_root' / '__init__.rst').exists()\n    assert (apidoc.outdir / 'test_root' / 'submodule.rst').exists()\n", "def test_excludes(apidoc):\n    assert not (apidoc.outdir / 'subdir').exists()\n    assert (apidoc.outdir / 'not_excluded.rst').exists()\n    assert (apidoc.outdir / 'subdir_with_sub.rst').exists()\n    assert (apidoc.outdir / 'subdir_with_sub' / 'sub.rst').exists()\n", "def test_apidoc_private_modules(apidoc):\n    \"\"\"Test that private modules are included when -P option is used.\"\"\"\n    assert (apidoc.outdir / 'test-root.rst').exists()\n    assert (apidoc.outdir / 'test-root._private_module.rst').exists()\n", "def test_apidoc_separate_modules(apidoc):\n    assert (apidoc.outdir / 'test-root.rst').exists()\n    assert (apidoc.outdir / 'test-root' / 'module.rst').exists()\n", "def test_apidoc_dry_run(apidoc):\n    \"\"\"Test dry run mode of apidoc.\"\"\"\n    assert (apidoc.outdir / 'modules.rst').exists() is False\n"], "sample_942": ["def test_parse_annotation():\n    env = Mock()\n    env.ref_context = {}\n\n    annotation = \"Union[int, str]\"\n    expected = (\n        [addnodes.desc_sig_name(annotation, annotation)],\n        [addnodes.desc_sig_name(\"Union\", \"Union\")],\n        [addnodes.desc_sig_name(\"int\", \"int\"), addnodes.desc_sig_punctuation(\"\", \", \"), addnodes.desc_sig_name(\"str\", \"str\")],\n    )\n\n    result = _parse_annotation(annotation, env)\n    assert isinstance(result[0], addnodes.desc_sig_name)\n    assert result[0].astext() == expected[0][0].astext()\n    assert len(result) == 1\n\n    annotation = \"Optional[Union[int, str]]\"\n    expected = (\n        [addnodes.desc_sig_name(annotation, annotation)],\n        [addnodes.desc_sig_name(\"Optional\", \"Optional\")],\n        [addnodes.desc_sig_name(\"Union\", \"Union\")],\n        [addnodes.desc_sig_name(\"int\", \"int\"), addnodes.desc_sig_punctuation(\"\", \", \"), addnodes.desc_sig_name(\"str\", \"str\")],\n    )\n\n    result = _parse_annotation(annotation, env)\n    assert isinstance(result[0], addnodes.desc_sig_name)\n    assert result[0].astext() == expected[0][0].astext()\n    assert len(result) == 1\n\n    annotation = \"Union[Dict[str, int], List[str]]\"\n    expected = (\n        [addnodes.desc_sig_name(annotation, annotation)],\n        [addnodes.desc_sig_name(\"Union\", \"Union\")],\n        [\n            addnodes.desc_sig_name(\"Dict\", \"Dict\"),\n            addnodes.desc_sig_punctuation(\"\", \"[\"),\n            addnodes.desc_sig_name(\"str\", \"str\"),\n            addnodes.desc_sig_punctuation(\"\", \", \"),\n            addnodes.desc_sig_name(\"int\", \"int\"),\n            addnodes.desc_sig_punctuation(\"\", \"]\"),\n            addnodes.desc_sig_punctuation(\"\", \", \"),\n            addnodes.desc_sig_name(\"List\", \"List\"),\n            addnodes.desc_sig_punctuation(\"\", \"[\"),\n            addnodes.desc_sig_name(\"str\", \"str\"),\n            addnodes.desc_sig_punctuation(\"\", \"]\"),\n        ],\n    )\n\n    result = _parse_annotation(annotation, env)\n    assert isinstance(result[0], addnodes.desc_sig_name)\n    assert result[0].astext() == expected[0][0].astext()\n    assert len(result) == 1\n", "def test_parse_annotation():\n    env = Mock()\n    annotation = \"List[str]\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 1\n    assert isinstance(result[0], pending_xref)\n    assert result[0].astext() == \"List[str]\"\n\n    annotation = \"Tuple[List[str], int, Dict[str, str]]\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 1\n    assert isinstance(result[0], pending_xref)\n    assert result[0].astext() == \"Tuple[List[str], int, Dict[str, str]]\"\n", "def test_parse_annotation():\n    env = Mock()\n    env.ref_context = {'py:module': 'module', 'py:class': 'class'}\n    annotation = \"Union[None, List[Tuple[int, str]]]\"\n    result = _parse_annotation(annotation, env)\n    assert isinstance(result[0], pending_xref)\n    assert isinstance(result[1], nodes.Text)\n    assert result[1].astext() == '['\n    assert isinstance(result[2], pending_xref)\n    assert isinstance(result[3], nodes.Text)\n    assert result[3].astext() == ', '\n    assert isinstance(result[4], pending_xref)\n    assert isinstance(result[5], nodes.Text)\n    assert result[5].astext() == '['\n    assert isinstance(result[6], pending_xref)\n    assert isinstance(result[7], nodes.Text)\n    assert result[7].astext() == ', '\n    assert isinstance(result[8], pending_xref)\n    assert isinstance(result[9], nodes.Text)\n    assert result[9].astext() == ']'\n    assert isinstance(result[10], nodes.Text)\n    assert result[10].astext() == ']'\n", "def test_parse_annotation():\n    env = Mock()\n    env.ref_context = {}\n\n    # Test parsing a simple type annotation\n    annotation = \"int\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 1\n    assert result[0].astext() == annotation\n\n    # Test parsing a more complex type annotation\n    annotation = \"List[Tuple[int, str]]\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) > 1\n    assert result[0].astext() == \"List[\"\n    assert result[1].astext() == \"Tuple[\"\n    assert result[2].astext() == \"int\"\n    assert result[3].astext() == \", \"\n    assert result[4].astext() == \"str\"\n    assert result[5].astext() == \"]\"\n    assert result[6].astext() == \"]\"\n\n    # Test parsing a type annotation with a module prefix\n    annotation = \"my_module.MyClass\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 1\n    assert result[0].astext() == annotation\n", "def test_parse_annotation():\n    env = Mock()\n    annotation = \"Union[None, Tuple[bool, bool]]\"\n    expected = (\n        [addnodes.desc_sig_name('', \"Union\"),\n         addnodes.desc_sig_punctuation('', '['),\n         addnodes.desc_sig_name('', \"None\"),\n         addnodes.desc_sig_punctuation('', ', '),\n         addnodes.desc_sig_name('', \"Tuple\"),\n         addnodes.desc_sig_punctuation('', '['),\n         addnodes.desc_sig_name('', \"bool\"),\n         addnodes.desc_sig_punctuation('', ', '),\n         addnodes.desc_sig_name('', \"bool\"),\n         addnodes.desc_sig_punctuation('', ']'),\n         addnodes.desc_sig_punctuation('', ']')]\n    )\n    result = _parse_annotation(annotation, env)\n    assert_node(result, expected)\n"], "sample_945": ["def test_parse_annotation():\n    env = Mock(config=Mock/python_use_unqualified_type_names=False))\n\n    # Simple annotation\n    node = _parse_annotation('int', env)\n    assert len(node) == 1\n    assert isinstance(node[0], pending_xref)\n    assert node[0].astext() == 'int'\n\n    # Not a type (not a class or a type alias)\n    node = _parse_annotation('42', env)\n    assert len(node) == 1\n    assert isinstance(node[0], nodes.Text)\n    assert node[0].astext() == '42'\n\n    # Parentheses\n    node = _parse_annotation('(int)', env)\n    assert len(node) == 3\n    assert isinstance(node[0], desc_sig_punctuation)\n    assert node[0].astext() == '('\n    assert isinstance(node[1], pending_xref)\n    assert node[1].astext() == 'int'\n    assert isinstance(node[2], desc_sig_punctuation)\n    assert node[2].astext() == ')'\n\n    # Union\n    node = _parse_annotation('int | str', env)\n    assert len(node) == 3\n    assert isinstance(node[0], pending_xref)\n    assert node[0].astext() == 'int'\n    assert isinstance(node[1], desc_sig_punctuation)\n    assert node[1].astext() == '|'\n    assert isinstance(node[2], pending_xref)\n    assert node[2].astext() == 'str'\n\n    # Unqualified type name\n    env.config.python_use_unqualified_type_names = True\n    node = _parse_annotation('builtins.int', env)\n    assert len(node) == 2\n    assert isinstance(node[0], pending_xref_condition)\n    assert node[0].astext() == 'int'\n    assert isinstance(node[1], pending_xref_condition)\n    assert node[1].astext() == 'builtins.int'\n", "def test_parse_annotation():\n    env = Mock()\n    env.config.python_use_unqualified_type_names = False\n\n    # simple annotation\n    annotation = \"int\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 1\n    assert isinstance(result[0], pending_xref)\n\n    # annotation with module\n    annotation = \"typing.List[int]\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 5\n    assert isinstance(result[0], pending_xref)\n    assert isinstance(result[1], nodes.Text)\n    assert result[1].astext() == \".\"\n    assert isinstance(result[2], pending_xref)\n    assert isinstance(result[3], desc_sig_punctuation)\n    assert result[3].astext() == \"[\"\n    assert isinstance(result[4], pending_xref)\n\n    # annotation with module and unqualified type names\n    env.config.python_use_unqualified_type_names = True\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 5\n    assert isinstance(result[0], pending_xref)\n    assert isinstance(result[1], nodes.Text)\n    assert result[1].astext() == \".\"\n    assert isinstance(result[2], pending_xref)\n    assert isinstance(result[3], desc_sig_punctuation)\n    assert result[3].astext() == \"[\"\n    assert isinstance(result[4], pending_xref)\n", "def test_parse_annotation():\n    env = Mock()\n    env.ref_context = {}\n\n    annotation = \"Union[int, str]\"\n    result = _parse_annotation(annotation, env)\n\n    assert len(result) == 5\n    assert_node(result[0], addnodes.pending_xref, uri='Union')\n    assert_node(result[1], nodes.Text, text='[', rawsource='[')\n    assert_node(result[2], addnodes.pending_xref, uri='int')\n    assert_node(result[3], nodes.Text, text=', ', rawsource=', ')\n    assert_node(result[4], addnodes.pending_xref, uri='str')\n", "def test_parse_annotation():\n    env = Mock()\n    annotations = _parse_annotation(\"Union[int, str, None]\", env)\n    assert len(annotations) == 7\n    assert_node(annotations[0], addnodes.pending_xref, reftype='class', reftarget='Union', support_smartquotes=False)\n    assert_node(annotations[1], nodes.Text, text='[', support_smartquotes=False)\n    assert_node(annotations[2], addnodes.pending_xref, reftype='class', reftarget='int', support_smartquotes=False)\n    assert_node(annotations[3], addnodes.pending_xref, reftype='class', reftarget='str', support_smartquotes=False)\n    assert_node(annotations[4], addnodes.pending_xref, reftype='class', reftarget='None', support_smartquotes=False)\n    assert_node(annotations[5], nodes.Text, text=']', support_smartquotes=False)\n    assert_node(annotations[6], nodes.Text, text='', support_smartquotes=False)\n", "def test_parse_annotation():\n    env = Mock()\n    annotation = \"str | bytes\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 7\n    assert_node(result[0], addnodes.desc_sig_punctuation, '|')\n    assert_node(result[1], pending_xref, reftype='class', reftarget='str')\n    assert_node(result[2], nodes.Text, ' | ')\n    assert_node(result[3], addnodes.desc_sig_punctuation, '|')\n    assert_node(result[4], nodes.Text, ' ')\n    assert_node(result[5], pending_xref, reftype='class', reftarget='bytes')\n    assert_node(result[6], addnodes.desc_sig_punctuation, '')\n"], "sample_947": ["def test_macro():\n    check(\"macro\", \"MY_MACRO\",\n          {1: \"MY_MACRO\", 2: \"MY_MACRO\"},\n          )\n    check(\"macro\", \"MY_MACRO()\",\n          {1: \"MY_MACRO\", 2: \"MY_MACRO\"},\n          )\n    check(\"macro\", \"MY_MACRO(int arg)\",\n          {1: \"MY_MACRO\", 2: \"MY_MACRO::int_arg\"},\n          )\n    check(\"macro\", \"MY_MACRO(int arg1, double arg2)\",\n          {1: \"MY_MACRO\", 2: \"MY_MACRO::int_arg1(double_arg2)\"},\n          )\n    check(\"macro\", \"MY_MACRO(int arg1, double arg2, ...)\",\n          {1: \"MY_MACRO\", 2: \"MY_MACRO::int_arg1(double_arg2__VA_ARGS__)\"},\n          )\n    check(\"macro\", \"MY_MACRO(int arg1, double arg2, ... , ...)\",\n          {1: \"MY_MACRO\", 2: \"MY_MACRO::int_arg1(double_arg2__VA_ARGS__)\"},\n          )\n", "def test_c_domain_add_enumerator_to_parent():\n    rootSymbol = Symbol(None, None, None, None, None)\n    # Create AST declarations for an enum and an enumerator\n    enumDecl = parse('enum', 'enum EnumName')\n    enumeratorDecl = parse('enumerator', 'identifier1')\n    \n    # Create a symbol for the enum\n    enumSymbol = rootSymbol.add_declaration(enumDecl, docname=\"TestDoc\", line=42)\n    \n    # Create a symbol for the enumerator, with the enum as parent\n    enumeratorSymbol = enumSymbol.add_declaration(enumeratorDecl, docname=\"TestDoc\", line=42)\n    \n    # Call the _add_enumerator_to_parent method\n    enumeratorDecl.enumeratorScopedSymbol = enumeratorSymbol\n    enumDecl.objectType = 'enum'\n    CObject._add_enumerator_to_parent(enumeratorDecl)\n    \n    # Check that the enumerator was added to the enum's children\n    assert len(enumSymbol._children) == 1\n    assert enumSymbol._children[0] is enumeratorSymbol\n", "def test_type_with_function_pointer():\n    check('type', 'int (*var)(void)', {1: 'c.var', 2: 'c.var'}, key='typedef')\n", "compilation error", "def testdepartments_function(fixtures):\n    input = \"int myFunction(const int arg1, int arg2)\"\n    idDict = {\n        1: \"myFunction\",\n        2: \"myFunction(const int, int)\",\n    }\n    check('function', input, idDict)\n    input = \"void foo(t_gcd\u043a\u043b\u044e\u0447, the_key, typeGcD\u043a\u043b\u044e\u0447)\"\n    idDict = {\n        1: \"foo\",\n        2: \"foo(t_gcd\u043a\u043b\u044e\u0447, the_key, typeGcD\u043a\u043b\u044e\u0447)\",\n    }\n    check('function', input, idDict)\n"], "sample_946": ["def test_parse_annotation():\n    # type: () -> None\n    anno = _parse_annotation('List[str, int]', env=Mock(ref_context={}))\n    assert_node(\n        anno,\n        [addnodes.pending_xref(condition=\"resolved\", refdomain=\"py\", reftype=\"class\", reftarget=\"List[str, int]\",\n                                pyclass=None, pymodule=None, refspecific=False,\n                                ids=[], dupnames=[], classes=[], format=\"\",\n                                warn=False, refwarn=False, child=None, support_smartquotes=False)],\n    )\n    anno = _parse_annotation('Union[str, int]', env=Mock(ref_context={}))\n    assert_node(\n        anno,\n        [addnodes.pending_xref(condition=\"resolved\", refdomain=\"py\", reftype=\"class\", reftarget=\"Union[str, int]\",\n                                pyclass=None, pymodule=None, refspecific=False,\n                                ids=[], dupnames=[], classes=[], format=\"\",\n                                warn=False, refwarn=False, child=None, support_smartquotes=False)],\n    )\n    anno = _parse_annotation('Tuple[str, int]', env=Mock(ref_context={}))\n    assert_node(\n        anno,\n        [addnodes.pending_xref(condition=\"resolved\", refdomain=\"py\", reftype=\"class\", reftarget=\"Tuple[str, int]\",\n                                pyclass=None, pymodule=None, refspecific=False,\n                                ids=[], dupnames=[], classes=[], format=\"\",\n                                warn=False, refwarn=False, child=None, support_smartquotes=False)],\n    )\n    anno = _parse_annotation('Dict[str, int]', env=Mock(ref_context={}))\n    assert_node(\n        anno,\n        [addnodes.pending_xref(condition=\"resolved\", refdomain=\"py\", reftype=\"class\", reftarget=\"Dict[str, int]\",\n                                pyclass=None, pymodule=None, refspecific=False,\n                                ids=[], dupnames=[], classes=[], format=\"\",\n                                warn=False, refwarn=False, child=None, support_smartquotes=False)],\n    )\n", "compilation error", "def test_parse_annotation():\n    env = Mock()\n    env.ref_context = {}\n\n    # simple annotation\n    annotation = \"int\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 1\n    assert isinstance(result[0], addnodes.pending_xref)\n    assert result[0].astext() == annotation\n\n    # annotation with module\n    annotation = \"module.Class\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 1\n    assert isinstance(result[0], addnodes.pending_xref)\n    assert result[0].astext() == annotation\n\n    # Union annotation\n    annotation = \"Union[int, str]\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 7\n    assert isinstance(result[0], addnodes.desc_sig_punctuation)\n    assert result[0].astext() == \"(\"\n    assert isinstance(result[1], addnodes.pending_xref)\n    assert result[1].astext() == \"int\"\n    assert isinstance(result[2], nodes.Text)\n    assert result[2].astext() == \", \"\n    assert isinstance(result[3], addnodes.pending_xref)\n    assert result[3].astext() == \"str\"\n    assert isinstance(result[4], addnodes.desc_sig_punctuation)\n    assert result[4].astext() == \")\"\n\n    # Optional annotation\n    annotation = \"Optional[str]\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 5\n    assert isinstance(result[0], addnodes.desc_sig_punctuation)\n    assert result[0].astext() == \"(\"\n    assert isinstance(result[1], addnodes.pending_xref)\n    assert result[1].astext() == \"str\"\n    assert isinstance(result[2], addnodes.desc_sig_punctuation)\n    assert result[2].astext() == \")\"\n    assert isinstance(result[3], nodes.Text)\n    assert result[3].astext() == \", \"\n    assert isinstance(result[4], addnodes.desc_sig_name)\n    assert result[4].astext() == \"None\"\n\n    # Invalid annotation\n    annotation = \"Invalid[.Annotation]\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 1\n    assert isinstance(result[0], addnodes.pending_xref)\n    assert result[0].astext() == annotation\n", "def test_domain_python_parse_annotation():\n    env = Mock()\n    annotations = _parse_annotation('str', env)\n    assert len(annotations) == 1\n    assert isinstance(annotations[0], addnodes.pending_xref)\n    assert annotations[0].astext() == 'str'\n\n    annotations = _parse_annotation('\"str\"', env)\n    assert len(annotations) == 1\n    assert isinstance(annotations[0], nodes.Text)\n    assert annotations[0].astext() == '\"str\"'\n\n    annotations = _parse_annotation('List[str]', env)\n    assert len(annotations) == 5\n    assert isinstance(annotations[0], addnodes.pending_xref)\n    assert annotations[0].astext() == 'List'\n    assert isinstance(annotations[1], desc_sig_punctuation)\n    assert annotations[1].astext() == '['\n    assert isinstance(annotations[2], addnodes.pending_xref)\n    assert annotations[2].astext() == 'str'\n    assert isinstance(annotations[3], desc_sig_punctuation)\n    assert annotations[3].astext() == ']'\n\n    annotations = _parse_annotation('Dict[str, int]', env)\n    assert len(annotations) == 7\n    assert isinstance(annotations[0], addnodes.pending_xref)\n    assert annotations[0].astext() == 'Dict'\n    assert isinstance(annotations[1], desc_sig_punctuation)\n    assert annotations[1].astext() == '['\n    assert isinstance(annotations[2], addnodes.pending_xref)\n    assert annotations[2].astext() == 'str'\n    assert isinstance(annotations[3], desc_sig_punctuation)\n    assert annotations[3].astext() == ', '\n    assert isinstance(annotations[4], addnodes.pending_xref)\n    assert annotations[4].astext() == 'int'\n    assert isinstance(annotations[5], desc_sig_punctuation)\n    assert annotations[5].astext() == ']'\n", "def test_python_domain_resolve_xref():\n    app = Mock()\n    env = Mock()\n    builder = Mock()\n    fromdocname = 'testdoc'\n\n    # Create a PythonDomain instance\n    domain = PythonDomain(env)\n\n    # Add an object to the domain\n    domain.objects['testobj'] = ('testdoc', 'testnode', 'function')\n\n    # Create a pending_xref node\n    node = pending_xref('', refdomain='py', reftype='func', reftarget='testobj')\n\n    # Resolve the xref\n    result = domain.resolve_xref(env, fromdocname, builder, 'func', 'testobj', node, None)\n\n    # Check the result\n    assert isinstance(result, nodes.reference)\n    assert result.astext() == 'testobj'\n\n    # Create a pending_xref node with a different reftype\n    node = pending_xref('', refdomain='py', reftype='data', reftarget='testobj')\n\n    # Resolve the xref\n    result = domain.resolve_xref(env, fromdocname, builder, 'data', 'testobj', node, None)\n\n    # Check the result\n    assert result is None\n"], "sample_949": ["def test_default_man_pages(config):\n    \"\"\"Test the default man_pages settings.\"\"\"\n    pages = default_man_pages(config)\n    assert len(pages) == 1\n    assert pages[0][0] == config.root_doc\n    assert pages[0][2] == f'{config.project} {config.release}'\n    assert pages[0][3] == [config.author]\n    assert pages[0][4] == 1\n", "def test_default_man_pages_with_config():\n    class TestConfig(Config):\n        project = 'Test Project'\n        release = '1.0'\n        root_doc = 'index'\n        author = 'Test Author'\n\n    config = TestConfig()\n    pages = default_man_pages(config)\n    assert len(pages) == 1\n    assert pages[0][0] == 'index'\n    assert pages[0][1] == 'test-project'\n    assert pages[0][2] == 'Test Project 1.0'\n    assert pages[0][3] == ['Test Author']\n    assert pages[0][4] == 1\n", "def test_default_man_pages():\n    \"\"\"Test default man_pages settings.\"\"\"\n    class MockConfig(Config):\n        project = 'Test project'\n        release = '1.0'\n        root_doc = 'index'\n        author = 'John Doe'\n\n    config = MockConfig()\n    pages = default_man_pages(config)\n    assert pages == [('index', 'test-project', 'Test project 1.0', ['John Doe'], 1)]\n", "def test_default_man_pages():\n    class TestConfig(Config):\n        project = 'Test Project'\n        release = '1.0'\n        root_doc = 'index'\n        author = 'Test Author'\n\n    config = TestConfig()\n    pages = default_man_pages(config)\n    assert len(pages) == 1\n    assert pages[0][0] == config.root_doc\n    assert pages[0][1] == 'test-project'\n    assert pages[0][2] == config.project + ' ' + config.release\n    assert pages[0][3] == [config.author]\n    assert pages[0][4] == 1\n", "def test_default_man_pages():\n    class DummyConfig(Config):\n        project = 'Project'\n        release = '1.0'\n        author = 'Author'\n        root_doc = 'index'\n\n    config = DummyConfig()\n    pages = default_man_pages(config)\n    assert len(pages) == 1\n    assert pages[0] == ('index', 'project', 'Project 1.0', ['Author'], 1)\n"], "sample_950": ["def test_parse_annotation():\n    env = Mock()\n    env.ref_context = {'py:module': 'module', 'py:class': 'class'}\n\n    # Test simple annotation\n    annotation = 'str'\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 1\n    assert isinstance(result[0], addnodes.pending_xref)\n    assert result[0].get('reftarget') == 'str'\n\n    # Test annotation with dotted name\n    annotation = 'module.Class'\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 1\n    assert isinstance(result[0], addnodes.pending_xref)\n    assert result[0].get('reftarget') == 'module.Class'\n\n    # Test annotation with parameterized type\n    annotation = 'List[str]'\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 3\n    assert isinstance(result[0], addnodes.desc_sig_name)\n    assert result[0].astext() == 'List'\n    assert isinstance(result[1], addnodes.desc_sig_punctuation)\n    assert result[1].astext() == '['\n    assert isinstance(result[2], addnodes.pending_xref)\n    assert result[2].get('reftarget') == 'str'\n\n    # Test annotation with union type\n    annotation = 'Union[int, str]'\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 7\n    assert isinstance(result[0], addnodes.desc_sig_name)\n    assert result[0].astext() == 'Union'\n    assert isinstance(result[1], addnodes.desc_sig_punctuation)\n    assert result[1].astext() == '['\n    assert isinstance(result[2], addnodes.pending_xref)\n    assert result[2].get('reftarget') == 'int'\n    assert isinstance(result[3], addnodes.desc_sig_punctuation)\n    assert result[3].astext() == ','\n    assert isinstance(result[4], addnodes.pending_xref)\n    assert result[4].get('reftarget') == 'str'\n    assert isinstance(result[5], addnodes.desc_sig_punctuation)\n    assert result[5].astext() == ']'\n", "def test_parse_annotation():\n    env = Mock()\n    annotation = \"Union[List[str], Tuple[str, ...]]\"\n    parsed_annotation = _parse_annotation(annotation, env)\n    assert len(parsed_annotation) == 13\n\n    # Verify parsed nodes\n    assert_node(parsed_annotation[0], addnodes.pending_xref, reftype='class', reftarget='Union')\n    assert_node(parsed_annotation[1], desc_sig_punctuation, '[')\n    assert_node(parsed_annotation[2], addnodes.pending_xref, reftype='class', reftarget='List')\n    assert_node(parsed_annotation[3], desc_sig_punctuation, '[')\n    assert_node(parsed_annotation[4], addnodes.pending_xref, reftype='class', reftarget='str')\n    assert_node(parsed_annotation[5], desc_sig_punctuation, ']')\n    assert_node(parsed_annotation[6], desc_sig_punctuation, ',')\n    assert_node(parsed_annotation[7], addnodes.pending_xref, reftype='class', reftarget='Tuple')\n    assert_node(parsed_annotation[8], desc_sig_punctuation, '[')\n    assert_node(parsed_annotation[9], addnodes.pending_xref, reftype='class', reftarget='str')\n    assert_node(parsed_annotation[10], desc_sig_punctuation, ',')\n    assert_node(parsed_annotation[11], desc_sig_punctuation, '...')\n    assert_node(parsed_annotation[12], desc_sig_punctuation, ']')\n", "def test_parse_annotation():\n    env = Mock()\n    annotations = _parse_annotation(\"str\", env)\n    assert len(annotations) == 1\n    assert isinstance(annotations[0], pending_xref)\n    assert_node(annotations[0].children[0], addnodes.desc_sig_name, 'str')\n\n    annotations = _parse_annotation(\"typing.List[str]\", env)\n    assert len(annotations) == 2\n    assert_node(annotations[0], nodes.Text, 'typing.List[')\n    assert_node(annotations[1].children[0], addnodes.desc_sig_name, 'str')\n\n    annotations = _parse_annotation(\"typing.List[typing.Tuple[str, int]]\", env)\n    assert len(annotations) == 4\n    assert_node(annotations[0], nodes.Text, 'typing.List[')\n    assert_node(annotations[1], nodes.Text, 'typing.Tuple[')\n    assert_node(annotations[2].children[0], addnodes.desc_sig_name, 'str')\n    assert_node(annotations[3].children[0], addnodes.desc_sig_name, 'int')\n", "def test_parse_annotation():\n    env = Mock()\n    env.ref_context = {}\n\n    annotation = \"List[int]\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 1\n    assert isinstance(result[0], pending_xref)\n    assert result[0].astext() == \"List[int]\"\n\n    annotation = \"Union[str, bytes]\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 1\n    assert isinstance(result[0], pending_xref)\n    assert result[0].astext() == \"Union[str, bytes]\"\n", "def test_parse_annotation():\n    env = Mock()\n    env.ref_context = {}\n\n    # test simple annotation\n    annotation = 'int'\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 1\n    assert isinstance(result[0], pending_xref)\n\n    # test complex annotation\n    annotation = 'List[str]'\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 4\n    assert isinstance(result[0], pending_xref)\n    assert isinstance(result[1], nodes.Text)\n    assert isinstance(result[2], pending_xref)\n    assert isinstance(result[3], nodes.Text)\n\n    # test annotation with spaces\n    annotation = ' List [ str ] '\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 4\n    assert isinstance(result[0], pending_xref)\n    assert isinstance(result[1], nodes.Text)\n    assert isinstance(result[2], pending_xref)\n    assert isinstance(result[3], nodes.Text)\n"], "sample_948": ["def test_alias_node():\n    node = cppDomain.AliasNode(\"TestAlias\", {\"maxdepth\": 3}, None)\n    data = zlib.compress(node.__getstate__())\n    loadedNode = cppDomain.AliasNode(None, None)\n    loadedNode.__setstate__(zlib.decompress(data))\n    assert loadedNode.sig == node.sig\n    assert loadedNode.aliasOptions == node.aliasOptions\n    assert loadedNode.parentKey == node.parentKey\n", "def test_template_parse_function():\n    input = \"template < template < typename A, typename... B > typename... C > void f();\"\n    parser = DefinitionParser(input, location=None, config=None)\n    ast = parser.parse_declaration(\"function\", \"function\")\n    parser.assert_end()\n    assert ast.templatePrefix.templates is not None\n    templates = ast.templatePrefix.templates\n    assert len(templates) == 1\n    templateParams = templates[0]\n    assert len(templateParams.params) == 1\n    param = templateParams.params[0]\n    assert isinstance(param, cppDomain.ASTTemplateParamTemplateType)\n    nestedParams = param.nestedParams\n    assert len(nestedParams.params) == 2\n    assert nestedParams.params[0].parameterPack is False\n    assert nestedParams.params[1].parameterPack is True\n    assert param.data.parameterPack is True\n", "def test_lookup_key():\n    rootSymbol = Symbol(None, None, None, None, None, None, None)\n    ast = parse(\"class\", \"MyClass\")\n    symbol = rootSymbol.add_declaration(ast, docname=\"TestDoc\", line=42)\n    lookupKey = symbol.get_lookup_key()\n    assert str(lookupKey) == \"[((MyClass, None, N:MyClassE))]\"\n", "def test_cpp_class_template_parameter_pack():\n    idDict = {1: 'R1::Test<Types&&...>::Test', 2: 'NTestISt16remove_referenceITypesEE1DpEE', 3: 'NTestISt16remove_referenceITypesEE1DpEE', 4: 'NTestIKNSt16remove_referenceITypesEEE1DpEE'}\n    check('class',\n          'template<template<typename Types> class...> class Test;',\n          idDict)\n", "def test_new_expr():\n    check(\"member\", \"int i = new int(42)\", {1: None, 2: \"pi1_aE\", 3: \"pi1_aE\", 4: \"pi1_aE\"},\n          None, None, \"int i = new int(42)\")\n    check(\"member\", \"int i = new int[42]\", {1: None, 2: \"pi1_A_42\", 3: \"pi1_A_42\", 4: \"pi1_A_42\"},\n          None, None, \"int i = new int[42]\")\n    check(\"member\", \"int i = new (int*)\", {1: None, 2: \"pi1_PiE\", 3: \"pi1_PiE\", 4: \"pi1_PiE\"},\n          None, None, \"int i = new(int*)\")\n"], "sample_951": ["def test_signature_from_ast():\n    code = \"def foo(a: int, b: str = 'default', *args: list, **kwargs: dict) -> None: pass\"\n    node = ast.parse(code).body[0]\n    sig = inspect.signature_from_ast(node, code)\n    assert sig.return_annotation == Parameter.empty\n    assert len(sig.parameters) == 4\n    assert sig.parameters['a'].annotation == int\n    assert sig.parameters['b'].annotation == str\n    assert sig.parameters['b'].default == 'default'\n    assert sig.parameters['args'].annotation == list\n    assert sig.parameters['kwargs'].annotation == dict\n", "def test_signature_from_ast():\n    # Test basic function signature\n    code = \"def func(a: int, b: str) -> None: pass\"\n    tree = ast.parse(code)\n    func_def = tree.body[0]\n    sig = inspect.signature_from_ast(func_def, code)\n    expected = \"(a: int, b: str) -> None\"\n    assert inspect.stringify_signature(sig) == expected\n\n    # Test function signature with default values\n    code = \"def func(a: int, b: str = 'hello') -> None: pass\"\n    tree = ast.parse(code)\n    func_def = tree.body[0]\n    sig = inspect.signature_from_ast(func_def, code)\n    expected = \"(a: int, b: str = 'hello') -> None\"\n    assert inspect.stringify_signature(sig) == expected\n\n    # Test function signature with *args and **kwargs\n    code = \"def func(a: int, *args: int, **kwargs: str) -> None: pass\"\n    tree = ast.parse(code)\n    func_def = tree.body[0]\n    sig = inspect.signature_from_ast(func_def, code)\n    expected = \"(a: int, *args: int, **kwargs: str) -> None\"\n    assert inspect.stringify_signature(sig) == expected\n", "def test_signature_from_str():\n    # simple function\n    sig = inspect.signature_from_str(\"(a, b)\")\n    assert str(sig) == \"(a, b)\"\n\n    # function with default values\n    sig = inspect.signature_from_str(\"(a, b=1, c='hello')\")\n    assert str(sig) == \"(a, b=1, c='hello')\"\n\n    # function with *args and **kwargs\n    sig = inspect.signature_from_str(\"(*args, **kwargs)\")\n    assert str(sig) == \"(*args, **kwargs)\"\n\n    # function with type annotations\n    sig = inspect.signature_from_str(\"(a: int, b: str) -> bool\")\n    assert str(sig) == \"(a: int, b: str) -> bool\"\n\n    # function with positional only arguments\n    if sys.version_info >= (3, 8):\n        sig = inspect.signature_from_str(\"(a, b, /, c, d)\")\n        assert str(sig) == \"(a, b, /, c, d)\"\n", "def test_stringify_signature():\n    func = types.FunctionType(lambda: None, globals(), \"func\")\n    func.__annotations__ = {\"x\": \"int\", \"return\": \"str\"}\n    sig = inspect.signature(func)\n    sig = sig.replace(parameters=[Parameter(\"x\", Parameter.POSITIONAL_ONLY)])\n    assert stringify_signature(sig) == \"(x: int) -> str\"\n    assert stringify_signature(sig, show_annotation=False) == \"(x) -> str\"\n    assert stringify_signature(sig, show_return_annotation=False) == \"(x: int)\"\n    assert stringify_signature(sig, show_annotation=False, show_return_annotation=False) == \"(x)\"\n", "def test_stringify_signature():\n    # Create a signature with various parameters\n    sig = inspect.Signature(\n        parameters=[\n            Parameter('a', Parameter.POSITIONAL_OR_KEYWORD),\n            Parameter('b', Parameter.POSITIONAL_OR_KEYWORD, default=42),\n            Parameter('c', Parameter.KEYWORD_ONLY),\n            Parameter('d', Parameter.KEYWORD_ONLY, default=None),\n            Parameter('e', Parameter.VAR_POSITIONAL),\n            Parameter('f', Parameter.VAR_KEYWORD),\n        ],\n        return_annotation='return_annotation',\n    )\n\n    # Test with show_annotation=True and show_return_annotation=True\n    expected = '(a, b=42, *, c, d=None, *e, **f) -> return_annotation'\n    assert stringify_signature(sig) == expected\n\n    # Test with show_annotation=False and show_return_annotation=True\n    expected = '(a, b=42, *, c, d=None, *e, **f) -> return_annotation'\n    assert stringify_signature(sig, show_annotation=False) == expected\n\n    # Test with show_annotation=True and show_return_annotation=False\n    expected = '(a, b=42, *, c, d=None, *e, **f)'\n    assert stringify_signature(sig, show_return_annotation=False) == expected\n\n    # Test with show_annotation=False and show_return_annotation=False\n    expected = '(a, b=42, *, c, d=None, *e, **f)'\n    assert stringify_signature(sig, show_annotation=False, show_return_annotation=False) == expected\n"], "sample_952": ["def test_signature_from_str():\n    sig = inspect.signature_from_str(\"(a: int, b: str = 'hello') -> None\")\n    assert sig.parameters[\"a\"].annotation == int\n    assert sig.parameters[\"b\"].annotation == str\n    assert sig.parameters[\"b\"].default == 'hello'\n    assert sig.return_annotation == type(None)\n\n    sig = inspect.signature_from_str(\"(a: int, *args, b: str = 'hello', **kwargs) -> None\")\n    assert sig.parameters[\"a\"].annotation == int\n    assert sig.parameters[\"args\"].kind == Parameter.VAR_POSITIONAL\n    assert sig.parameters[\"b\"].annotation == str\n    assert sig.parameters[\"b\"].default == 'hello'\n    assert sig.parameters[\"kwargs\"].kind == Parameter.VAR_KEYWORD\n    assert sig.return_annotation == type(None)\n", "def test_getall():\n    class TestModule:\n        __all__ = ['test1', 'test2']\n\n    assert inspect.getall(TestModule) == ['test1', 'test2']\n\n    class TestModule2:\n        pass\n\n    assert inspect.getall(TestModule2) is None\n\n    class TestModule3:\n        __all__ = 'string'\n\n    with pytest.raises(ValueError):\n        inspect.getall(TestModule3)\n", "def test_signature_from_ast():\n    # Simple function\n    node = ast.parse('def foo(a, b, c=1, *d, e=2, **f): pass').body[0]\n    sig = inspect.signature_from_ast(node)\n    assert stringify_signature(sig) == '(a, b, c=1, *d, e=2, **f)'\n\n    # Function with annotations\n    node = ast.parse('def foo(a: int, b: str, c: float = 1, *d: tuple, e: list = 2, **f: dict) -> None: pass').body[0]\n    sig = inspect.signature_from_ast(node)\n    assert stringify_signature(sig) == '(a: int, b: str, c: float = 1, *d: tuple, e: list = 2, **f: dict) -> None'\n\n    # Function with default values\n    node = ast.parse('def foo(a=1, b=2, c=3): pass').body[0]\n    sig = inspect.signature_from_ast(node)\n    assert stringify_signature(sig) == '(a=1, b=2, c=3)'\n\n    # Function with positional-only parameters\n    node = ast.parse('def foo(a, b, /, c=1, *d, e=2, **f): pass').body[0]\n    sig = inspect.signature_from_ast(node)\n    assert stringify_signature(sig) == '(a, b, /, c=1, *d, e=2, **f)'\n", "def test_signature_from_ast():\n    # Create AST node for 'def func(a: int, b: str = \"default\") -> None:'\n    code = 'def func(a: int, b: str = \"default\") -> None: pass'\n    module = ast.parse(code)\n    function = module.body[0]\n\n    # Create signature from AST node\n    sig = inspect.signature_from_ast(function, code)\n\n    # Check parameters\n    assert len(sig.parameters) == 2\n    param_a = sig.parameters['a']\n    assert param_a.annotation == int\n    assert param_a.default == Parameter.empty\n    param_b = sig.parameters['b']\n    assert param_b.annotation == str\n    assert isinstance(param_b.default, inspect.DefaultValue)\n    assert param_b.default.value == '\"default\"'\n\n    # Check return annotation\n    assert sig.return_annotation == 'None'\n", "def test_signature_from_ast():\n    node = ast.parse('def func(a, b, /, c, *, d, e=None, **kwargs) -> int: pass').body[0]\n    sig = inspect.signature_from_ast(node)\n    assert len(sig.parameters) == 5\n    assert sig.parameters['a'].kind == Parameter.POSITIONAL_ONLY\n    assert sig.parameters['b'].kind == Parameter.POSITIONAL_ONLY\n    assert sig.parameters['c'].kind == Parameter.POSITIONAL_OR_KEYWORD\n    assert sig.parameters['d'].kind == Parameter.KEYWORD_ONLY\n    assert sig.parameters['e'].kind == Parameter.KEYWORD_ONLY\n    assert sig.parameters['e'].default == 'None'\n    assert sig.parameters['kwargs'].kind == Parameter.VAR_KEYWORD\n    assert sig.return_annotation == 'int'\n"], "sample_953": ["def test_ask_user_with_defaults(monkeypatch):\n    defaults = {\n        'path': '.',\n        'sep': True,\n        'dot': '_',\n        'project': 'My Project',\n        'author': 'John Doe',\n        'version': '',\n        'release': '',\n        'language': 'en',\n        'suffix': '.rst',\n        'master': 'index',\n        'extensions': [],\n        'makefile': True,\n        'batchfile': True,\n    }\n\n        for key, value in defaults.items():\n            if prompt.startswith(qs.PROMPT_PREFIX + key):\n                return str(value)\n        return ''\n\n    monkeypatch.setattr(qs, 'term_input', mock_input)\n    d = {}\n    qs.ask_user(d)\n    assert d == defaults\n", "def test_get_parser():\n    parser = qs.get_parser()\n    assert parser.usage == '%(prog)s [OPTIONS] <PROJECT_DIR>'\n    assert parser.prog == 'sphinx-quickstart'\n    args = parser.parse_args(['--version'])\n    assert args.show_version\n    args = parser.parse_args(['--quiet', 'path/to/project'])\n    assert args.quiet\n    assert args.path == 'path/to/project'\n", "def test_get_parser():\n    parser = qs.get_parser()\n    args = parser.parse_args(['--help'])\n    assert args.help is True\n\n    with pytest.raises(SystemExit):\n        parser.parse_args(['--version'])\n", "def test_valid_dir(tmp_path):\n    d = {'path': str(tmp_path), 'sep': True, 'dot': '_', 'master': 'index', 'suffix': '.rst'}\n    assert qs.valid_dir(d)\n\n    # test existing conf.py\n    conf_py = tmp_path / 'conf.py'\n    conf_py.touch()\n    assert not qs.valid_dir(d)\n    conf_py.unlink()\n\n    # test existing Makefile\n    makefile = tmp_path / 'Makefile'\n    makefile.touch()\n    assert not qs.valid_dir(d)\n    makefile.unlink()\n\n    # test existing master document\n    master_doc = tmp_path / 'index.rst'\n    master_doc.touch()\n    assert not qs.valid_dir(d)\n    master_doc.unlink()\n\n    # test existing build directory\n    build_dir = tmp_path / '_build'\n    build_dir.mkdir()\n    assert not qs.valid_dir(d)\n    build_dir.rmdir()\n\n    # test existing source directory\n    src_dir = tmp_path / 'source'\n    src_dir.mkdir()\n    assert qs.valid_dir(d)\n    src_dir.rmdir()\n", "def test_ask_user_with_defaults(monkeypatch):\n    monkeypatch.setattr(qs, 'term_input', mock_input({\n        'Root path for the documentation': '.',\n        'Project name': 'My Project',\n        'Author name(s)': 'My Name',\n    }, needanswer=True))\n\n    d = {}\n    qs.ask_user(d)\n\n    assert d['path'] == '.'\n    assert d['project'] == 'My Project'\n    assert d['author'] == 'My Name'\n    assert d['version'] == ''\n    assert d['release'] == ''\n    assert d['suffix'] == '.rst'\n    assert d['master'] == 'index'\n    assert d['makefile'] is True\n    assert d['batchfile'] is True\n"], "sample_954": ["def test_manpage_translator_no_authors():\n    config = Config({\n        'man_pages': default_man_pages,\n        'authors': None,\n    })\n\n    translator = ManualPageTranslator(None, None, config)\n    assert translator._docinfo['title'] == ''\n    assert translator._docinfo['subtitle'] == ''\n    assert 'author' not in translator._docinfo\n", "def test_manpage_writer_header():\n    builder = object()  # Mock builder object\n    writer = ManualPageWriter(builder)\n    writer.translator_class = ManualPageTranslator\n    document = nodes.document(title='Test Title', subtitle='Test Subtitle')\n    document.settings = Config(\n        title='Test Title',\n        subtitle='Test Subtitle',\n        authors=['Test Author'],\n        section='1',\n        today='2022-01-01',\n        today_fmt='%Y-%m-%d',\n        language='en',\n        project='Test Project'\n    )\n    writer.translate()\n    assert writer.output.startswith(\n        '.TH \"TEST TITLE\" \"1\" \"2022-01-01\" \"\" \"Test Project\"\\n'\n        '.SH NAME\\n'\n        'Test Title \\\\- Test Subtitle\\n'\n    )\n", "def test_manpage_translator():\n    # Create a mock config object\n    config = Config(None, None, None, None, None)\n\n    # Create a mock document object\n    class Document:\n        settings = config\n        metadata = {'title': 'Test Document'}\n        author = 'Test Author'\n\n    document = Document()\n\n    # Create a mock builder object\n    class Builder:\n        config = config\n            return ManualPageTranslator(document, builder)\n\n    builder = Builder()\n\n    # Create the translator\n    translator = ManualPageTranslator(document, builder)\n\n    # Test the translator's methods\n    assert translator.header() == '.TH \"TEST DOCUMENT\" \"\" \"\" \"\" \"\"\\n.SH NAME\\nTest Document \\\\- \\n'\n\n    # Test visit_desc\n    desc = nodes.Element()\n    translator.visit_desc(desc)\n    assert translator.body == ['.TP\\n']\n\n    # Test depart_desc\n    translator.depart_desc(desc)\n    assert translator.body == ['.TP\\n', '\\n']\n\n    # Test visit_desc_signature\n    signature = nodes.Element()\n    translator.visit_desc_signature(signature)\n    assert translator.body == ['.TP\\n', '\\n', '.TP\\n']\n", "def test_manpage_translator_header():\n    translator = ManualPageTranslator(document=None, builder=None)\n    translator._docinfo = {\n        'title_upper': 'test_title',\n        'manual_section': 'test_section',\n        'date': 'test_date',\n        'version': 'test_version',\n        'manual_group': 'test_group'\n    }\n    expected_header = (\".TH \\\"test_title\\\" \\\"test_section\\\"\"\n                       \" \\\"test_date\\\" \\\"test_version\\\" \\\"test_group\\\"\\n\"\n                       \".SH NAME\\n\")\n    assert translator.header().startswith(expected_header)\n", "def test_manpage_translator(tmpdir, app):\n    manpage = app.builder.translator_class(app.builder.document, app.builder)\n    # test header method\n    docinfo = {\n        'title_upper': 'SPHINX-TEST',\n        'manual_section': '1',\n        'date': '2021',\n        'version': '1.0',\n        'manual_group': 'Sphinx-test'\n    }\n    manpage._docinfo = docinfo\n    expected_header = '.TH \"SPHINX-TEST\" \"1\" \"2021\" \"1.0\" \"Sphinx-test\"\\n'\n    assert manpage.header() == expected_header\n    # test visit_caption method\n    node = app.builder.document.nodes[0].children[0]\n    manpage.visit_caption(node)\n    assert manpage.body[-1] == '.sp\\n'\n"], "sample_955": ["def test_unparse_arguments(source, expected):\n    tree = ast.parse(source)\n    args = tree.body[0].args  # type: ignore\n    assert ast.unparse(args) == expected\n", "def test_unparse_arguments(source, expected):\n    tree = ast.parse(source)\n    arg_node = tree.body[0].args  # type: ignore\n    assert ast.unparse(arg_node) == expected\n", "def test_unparse_arguments(source, expected):\n    node = ast.parse(source).body[0].args  # type: ignore\n    assert ast.unparse(node) == expected\n", "def test_unparse_arguments(source, expected):\n    node = ast.parse(source).body[0].args\n    assert ast.unparse(node) == expected\n", "def test_unparse_arguments(source, expected):\n    tree = ast.parse(source)\n    result = ast.unparse(tree.body[0].args)\n    assert result == expected\n"], "sample_957": ["def test_stringify(value, expected):\n    assert stringify(value) == expected\n", "def test_stringify_builtins():\n    assert stringify(int) == 'int'\n    assert stringify(list) == 'list'\n    assert stringify(type(None)) == 'None'\n    assert stringify(Ellipsis) == '...'\n    assert stringify(Struct) == 'struct.Struct'\n    assert stringify(TracebackType) == 'types.TracebackType'\n", "def test_restify_py37():\n    class MyClass3:\n        __module__ = 'mymodule'\n\n    assert restify(MyClass3) == ':class:`mymodule.MyClass3`'\n    assert restify(List[int]) == ':obj:`~typing.List`\\\\ [int]'\n    assert restify(Union[int, str]) == ':obj:`~typing.Union`\\\\ [int, str]'\n    assert restify(Optional[int]) == ':obj:`~typing.Optional`\\\\ [int]'\n    assert restify(Callable[[int], str]) == ':obj:`~typing.Callable`\\\\ [[int], str]'\n    assert restify(MyList[int]) == ':class:`MyList`\\\\ [int]'\n\n    # SpecialForm\n    assert restify(Any) == ':obj:`~typing.Any`'\n    assert restify(T) == ':obj:`~T`'\n\n    # ForwardRef\n    fr = sys.version_info > (3, 7) and type(T.__forward_arg__) or None\n    if fr:\n        fr_instance = fr('T')\n        assert restify(fr_instance) == ':class:`T`'\n\n    # broken class\n    assert restify(BrokenType) == repr(BrokenType)\n", "def test_restify_py37():\n    if sys.version_info >= (3, 7):\n        assert restify(Union[MyClass1, MyClass2]) == ':obj:`~typing.Union`\\\\ [:class:`__main__.MyClass1`, :class:`__main__.MyClass2`]'\n        assert restify(Callable[[int], str]) == ':class:`~typing.Callable`\\\\ [[:class:`int`], :class:`str`]'\n        assert restify(Dict[int, str]) == ':class:`~typing.Dict`\\\\ [:class:`int`, :class:`str`]'\n        assert restify(Generator[int, str, bool]) == ':class:`~typing.Generator`\\\\ [:class:`int`, :class:`str`, :class:`bool`]'\n        assert restify(Union[int, str, None]) == ':obj:`~typing.Optional`\\\\ [:obj:`~typing.Union`\\\\ [:class:`int`, :class:`str`]]'\n        assert restify(Union[int, None]) == ':obj:`~typing.Optional`\\\\ [:class:`int`]'\n", "def test_restify_py37():\n    assert restify(MyClass1) == ':class:`__main__.MyClass1`'\n    assert restify(MyClass2) == ':class:`__main__.MyClass2`'\n    assert restify(None) == ':obj:`None`'\n    assert restify(Struct) == ':class:`struct.Struct`'\n    assert restify(TracebackType) == ':class:`types.TracebackType`'\n    assert restify(Union[int, str]) == ':obj:`~typing.Union`\\\\ [:class:`int`, :class:`str`]'\n    assert restify(Optional[int]) == ':obj:`~typing.Optional`\\\\ [:class:`int`]'\n    assert restify(Callable[[int, str], bool]) == ':obj:`~typing.Callable`\\\\ [[ :class:`int`, :class:`str`], :class:`bool`]'\n    assert restify(MyInt) == ':class:`MyInt`'\n    assert restify(MyList[int]) == ':class:`__main__.MyList`\\\\ [:class:`int`]'\n\n    # ForwardRef\n    assert restify('List[int]') == ':class:`List[int]`'\n\n    # SpecialForm\n    assert restify(Union) == ':obj:`~typing.Union`'\n\n    # TypeVar\n    assert restify(T) == ':obj:`~__main__.T`'\n"], "sample_956": ["def test_resolve_reference_detect_inventory():\n    app = mock.Mock()\n    env = mock.Mock()\n    env.domains = {}\n    env.config = mock.Mock()\n    env.config.intersphinx_disabled_reftypes = []\n\n    node, contnode = fake_node('std', 'any', 'project:target', '')\n    InventoryAdapter(env).named_inventory = {'project': {'std:any': {'target': ('project', '1.0', 'uri', '-')}}}\n\n    result = resolve_reference_detect_inventory(env, node, contnode)\n    assert result is not None\n    assert result.attributes['refuri'] == 'uri'\n", "def test_resolve_reference_in_inventory():\n    app = mock.Mock()\n    env = mock.Mock()\n    env.domains = {}\n    env.config = mock.Mock()\n    env.config.intersphinx_disabled_reftypes = []\n    domain = mock.Mock()\n    domain.name = 'std'\n    domain.object_types = ['std:term']\n    env.domains['std'] = domain\n    inv_name = 'python'\n    target = 'os'\n    uri = 'https://docs.python.org/3/objects.inv'\n    inventory = {\n        'std:term': {\n            'os': ('Python', '3', uri, '-')\n        }\n    }\n    InventoryAdapter(env).named_inventory[inv_name] = inventory\n\n    node, contnode = fake_node('std', 'term', target, 'os')\n    result = resolve_reference_in_inventory(env, inv_name, node, contnode)\n    assert result is not None\n    assert result['refuri'] == uri\n", "def test_missing_reference_with_inv_name(app, monkeypatch):\n    set_config(app, {\n        'local': ('http://example.com', None),\n        'other': ('http://example.org', None),\n    })\n\n        if uri == 'http://example.com':\n            return {\n                'std:label': {\n                    'foo': ('example', '1.0', 'foo.html', '-'),\n                },\n            }\n        elif uri == 'http://example.org':\n            return {\n                'std:label': {\n                    'bar': ('example', '1.0', 'bar.html', '-'),\n                },\n            }\n        else:\n            raise AssertionError\n\n    monkeypatch.setattr('sphinx.ext.intersphinx.fetch_inventory', mock_fetch_inventory)\n\n    node, contnode = fake_node('std', 'ref', 'other:bar', 'bar')\n    result = missing_reference(app, app.env, node, contnode)\n    assert result is not None\n    assert result['refuri'] == 'http://example.org/bar.html'\n", "def test_resolve_reference_detect_inventory(app):\n    # Create a fake inventory file\n    inventory = {'std:class': {'MyClass': ('MyProject', '1.0', 'myclass.html', '-')},\n                 'std:function': {'my_function': ('MyProject', '1.0', 'my_function.html', '-')}}\n\n    # Patch the inventory adapter to return the fake inventory\n    with mock.patch.object(InventoryAdapter, 'main_inventory', return_value=inventory):\n        # Create a fake node for a reference\n        node, contnode = fake_node('std', 'class', 'MyClass', 'MyClass')\n\n        # Resolve the reference\n        result = resolve_reference_detect_inventory(app.env, node, contnode)\n\n        # Check the result\n        assert result is not None\n        assert isinstance(result, nodes.reference)\n        assert result['refuri'] == 'myclass.html'\n\n    # Create a fake inventory file with an inventory name\n    named_inventory = {'myinv': {'std:class': {'MyClass': ('MyProject', '1.0', 'myclass.html', '-')},\n                                 'std:function': {'my_function': ('MyProject', '1.0', 'my_function.html', '-'})}}\n\n    # Patch the inventory adapter to return the fake named inventory\n    with mock.patch.object(InventoryAdapter, 'named_inventory', return_value=named_inventory):\n        # Create a fake node for a reference with an inventory name\n        node, contnode = fake_node('std', 'class', 'myinv:MyClass', 'MyClass')\n\n        # Resolve the reference\n        result = resolve_reference_detect_inventory(app.env, node, contnode)\n\n        # Check the result\n        assert result is not None\n        assert isinstance(result, nodes.reference)\n        assert result['refuri'] == 'myclass.html'\n", "def test_resolve_reference_detect_inventory(app, monkeypatch):\n    class MockInventoryAdapter:\n            self.env = env\n            self.named_inventory = {'inv_name': {'std:function': {'target': ('proj', 'version', 'uri', 'dispname')}}}\n\n    monkeypatch.setattr('sphinx.ext.intersphinx.InventoryAdapter', MockInventoryAdapter)\n\n    node, contnode = fake_node('std', 'function', 'inv_name:target', 'content')\n    result = resolve_reference_detect_inventory(app.env, node, contnode)\n\n    assert result is not None\n    assert isinstance(result, nodes.reference)\n    assert result['refuri'] == 'uri'\n"], "sample_958": ["def test_id_prefix():\n    # Check the id prefix for different versions\n    expected_prefix = {\n        1: 'c.',\n        2: 'Cv2.',\n    }\n    for i in range(1, _max_id + 1):\n        assert _id_prefix[i] == expected_prefix.get(i, _id_prefix[_max_id])\n", "def test_enumerator():\n    check('enumerator', 'Const', {1: 'Const', 2: 'Const', 3: 'Const'}, key='enum')\n    check('enumerator', 'Const = 42', {1: 'Const', 2: 'Const', 3: 'Const'}, key='enum')\n    check('enumerator', 'Const = 42u', {1: 'Const', 2: 'Const', 3: 'Const'}, key='enum')\n    check('enumerator', 'Const = 0x42', {1: 'Const', 2: 'Const', 3: 'Const'}, key='enum')\n    check('enumerator', 'Const = 0x42u', {1: 'Const', 2: 'Const', 3: 'Const'}, key='enum')\n    check('enumerator', 'Const = -42', {1: 'Const', 2: 'Const', 3: 'Const'}, key='enum')\n    check('enumerator', 'Const = ~42', {1: 'Const', 2: 'Const', 3: 'Const'}, key='enum')\n", "def test_type_with_redundant_sized_array():\n    check(\n        \"type\",\n        \"int arr[5];\",\n        {1: \"cpp-type-arr\", 2: \"cpp-type-arr-5\"},\n        key=\"cpp-type\"\n    )\n", "def test_nested_template_params():\n    # Check if nested template parameters are handled correctly.\n    input = \"void f(A< B< C, D>, E>);\"\n    output = \"void f(A<B<C, D>, E>)\"\n    idDict = {\n        1: \"cpp:function::f(A<B<C,D>,E>)void\",\n        2: \"cpp:function::f(A<B<C,D>,E>)void\",\n        3: \"cpp:function::f(A<B<C,D>,E>)void\",\n    }\n    check(\"function\", input, idDict, output)\n", "def test_class_template_id():\n    check(\n        \"class\",\n        \"class my::name::Name<int>\",\n        {1: \"cpp-class\", 2: \"cpp-class\"},\n        \"class my::name::Name<int>\")\n"], "sample_960": ["def test_parse_annotation():\n    env = Mock()\n    annotation = \"Union[str, int]\"\n    expected = [\"Union\", \"[\", \"str\", \", \", \"int\", \"]\"]\n    result = _parse_annotation(annotation, env)\n    assert [node.astext() for node in result] == expected\n\n    annotation = \"Optional[Dict[str, List[int]]]\"\n    expected = [\"Optional\", \"[\", \"Dict\", \"[\", \"str\", \", \", \"List\", \"[\", \"int\", \"]\", \"]\", \"]\"]\n    result = _parse_annotation(annotation, env)\n    assert [node.astext() for node in result] == expected\n\n    annotation = \"Callable[..., Any]\"\n    expected = [\"Callable\", \"[\", \"...\", \", \", \"Any\", \"]\"]\n    result = _parse_annotation(annotation, env)\n    assert [node.astext() for node in result] == expected\n", "def test_parse_annotation():\n    env = Mock()\n    annotation = \"Union[str, int]\"\n    expected_nodes = [\n        addnodes.desc_sig_name('', 'Union'),\n        addnodes.desc_sig_punctuation('', '['),\n        addnodes.desc_sig_name('', 'str'),\n        addnodes.desc_sig_punctuation('', ', '),\n        addnodes.desc_sig_name('', 'int'),\n        addnodes.desc_sig_punctuation('', ']')\n    ]\n    result = _parse_annotation(annotation, env)\n    for exp, res in zip(expected_nodes, result):\n        assert_node(exp, res)\n", "def test_parse_annotation():\n    env = Mock()\n    env.ref_context = {}\n\n    # Simple annotation\n    annotation = \"int\"\n    expected = [pending_xref(\"\", \"\", reftype=\"class\", refdomain=\"py\", reftarget=\"int\")]\n    result = _parse_annotation(annotation, env)\n    assert_node(expected, result)\n\n    # Annotation with module\n    annotation = \"typing.Union[int, str]\"\n    expected = [\n        pending_xref(\n            \"\",\n            \"\",\n            reftype=\"class\",\n            refdomain=\"py\",\n            reftarget=\"typing.Union[int, str]\",\n        )\n    ]\n    result = _parse_annotation(annotation, env)\n    assert_node(expected, result)\n\n    # Complex annotation\n    annotation = \" typing.Union[ typing.Callable[ [int, str], typing.Tuple[int, str]], None]\"\n    expected = [\n        pending_xref(\n            \"\",\n            \"\",\n            reftype=\"class\",\n            refdomain=\"py\",\n            reftarget=\"typing.Union[typing.Callable[[int, str], typing.Tuple[int, str]]\",\n        ),\n        desc_sig_punctuation(\"\", \",\"),\n        pending_xref(\n            \"\",\n            \"\",\n            reftype=\"class\",\n            refdomain=\"py\",\n            reftarget=\"None\",\n        ),\n    ]\n    result = _parse_annotation(annotation, env)\n    assert_node(expected, result)\n", "def test_parse_annotation():\n    env = Mock()\n    env.ref_context = {}\n\n    # Simple annotation\n    node = _parse_annotation(\"int\", env)\n    assert len(node) == 1\n    assert isinstance(node[0], nodes.Text)\n    assert node[0].astext() == \"int\"\n\n    # Complex annotation\n    node = _parse_annotation(\"List[int]\", env)\n    assert len(node) == 3\n    assert isinstance(node[0], nodes.Text)\n    assert node[0].astext() == \"List[\"\n    assert isinstance(node[1], pending_xref)\n    assert node[1].astext() == \"int\"\n    assert isinstance(node[2], nodes.Text)\n    assert node[2].astext() == \"]\"\n\n    # Complex annotation with nested brackets\n    node = _parse_annotation(\"Dict[str, List[int]]\", env)\n    assert len(node) == 5\n    assert isinstance(node[0], nodes.Text)\n    assert node[0].astext() == \"Dict[\"\n    assert isinstance(node[1], pending_xref)\n    assert node[1].astext() == \"str\"\n    assert isinstance(node[2], nodes.Text)\n    assert node[2].astext() == \", List[\"\n    assert isinstance(node[3], pending_xref)\n    assert node[3].astext() == \"int\"\n    assert isinstance(node[4], nodes.Text)\n    assert node[4].astext() == \"]]\"\n", "def test_parse_annotation():\n    env = Mock()\n    annotation = \"Union[int, str]\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 1\n    assert isinstance(result[0], addnodes.pending_xref)\n    assert result[0].astext() == annotation\n\n    annotation = \"List['module.Class']\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 1\n    assert isinstance(result[0], addnodes.pending_xref)\n    assert result[0].astext() == annotation\n\n    annotation = \"Dict[str, 'module.Class']\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 1\n    assert isinstance(result[0], addnodes.pending_xref)\n    assert result[0].astext() == annotation\n\n    annotation = \"Union['package.module.Class1', 'package.module.Class2']\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 1\n    assert isinstance(result[0], addnodes.pending_xref)\n    assert result[0].astext() == annotation\n\n    annotation = \"Union['package.module.Class1', str]\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 1\n    assert isinstance(result[0], addnodes.pending_xref)\n    assert result[0].astext() == annotation\n"], "sample_959": ["def test_destructor_lookup():\n    parser = DefinitionParser(\"MyClass::~MyClass()\", location=None, config=None)\n    ast, _ = parser.parse_xref_object()\n    assert isinstance(ast, cppDomain.ASTDeclaration)\n\n    rootSymbol = cppDomain.Symbol(None, None, None, None, None, None, None)\n    parentSymbol = rootSymbol.add_name(\"MyClass\", None)\n\n    symbols, failReason = parentSymbol.find_name(\n        ast.name, [], 'function',\n        templateShorthand=True,\n        matchSelf=True, recurseInAnon=True,\n        searchInSiblings=False)\n    assert symbols is not None\n", "def test_template_parameter_pack():\n    check(\n        \"function\",\n        \"void f(Args... args);\",\n        {1: None, 2: \"f\", 3: \"f\", 4: \"f\"},\n        \"void f(Args... args)\"\n    )\n", "def test_issue_2666():\n    parser = DefinitionParser('Test', location=None, config=None)\n    parser.allowFallbackExpressionParsing = False\n\n        parser.parse_string(input)\n        parser.assert_end()\n    # This should pass\n    parse_and_check_input('[2]')\n    # This should fail\n    with pytest.raises(DefinitionError):\n        parse_and_check_input('[2][3]')\n", "def test_lookup_key():\n    root = Symbol(None, None, None, None, None, None, None)\n    s1 = root.add_name(ASTNestedName([ASTNestedNameElement(ASTIdentifier(\"s1\"), None)], [False], rooted=False))\n    decl = ASTDeclaration('function', None, None, None, None, None, None)\n    s1._fill_empty(decl, \"TestDoc\", 42)\n    s2 = s1.add_name(ASTNestedName([ASTNestedNameElement(ASTIdentifier(\"s2\"), None)], [False], rooted=False))\n    s3 = s2.add_name(ASTNestedName([ASTNestedNameElement(ASTIdentifier(\"s3\"), None)], [False], rooted=False))\n\n    key = s3.get_lookup_key()\n    assert len(key.data) == 3\n    assert key.data[0][2] is not None\n    assert key.data[1][2] is None\n    assert key.data[2][2] is None\n\n    s1b = root.direct_lookup(key)\n    assert s1b is s1\n", "def test_type_alias():\n    check(\n        'type',\n        'using StringList = std::vector<std::string>',\n        {1: None, 2: None, 3: 'StringList', 4: 'StringList'},\n        'using StringList = std::vector<std::string>'\n    )\n"], "sample_963": ["def test_restify_py37():\n    # test restify for Union\n    assert restify(Union[int, str]) == ':py:obj:`~typing.Union`\\\\ [:py:class:`int`, :py:class:`str`]'\n    assert restify(Union[int, None]) == ':py:obj:`~typing.Optional`\\\\ [:py:class:`int`]'\n\n    # test restify for typing.Callable\n    assert restify(Callable[[int, str], bool]) == ':py:class:`~typing.Callable`\\\\ [[[:py:class:`int`, :py:class:`str`], :py:class:`bool`]]'\n\n    # test restify for typing.Tuple\n    assert restify(Tuple[int, str]) == ':py:class:`~typing.Tuple`\\\\ [:py:class:`int`, :py:class:`str`]'\n", "def test_restify_newtype_py37():\n    if sys.version_info >= (3, 10):\n        assert restify(MyInt) == ':py:class:`sphinx.util.typing_test.MyInt`'\n    else:\n        assert restify(MyInt) == ':py:class:`MyInt`'\n", "def test_restify_builtins():\n    assert restify(int) == ':py:class:`int`'\n    assert restify(str) == ':py:class:`str`'\n    assert restify(MyClass1) == ':py:class:`__main__.MyClass1`'\n    assert restify(MyClass2) == ':py:class:`__main__.MyClass2`'\n    assert restify(Struct) == ':py:class:`struct.Struct`'\n    assert restify(TracebackType) == ':py:class:`types.TracebackType`'\n    assert restify(Generator) == ':py:class:`typing.Generator`'\n    assert restify(List) == ':py:class:`typing.List`'\n    assert restify(Tuple) == ':py:class:`typing.Tuple`'\n    assert restify(Dict) == ':py:class:`typing.Dict`'\n    assert restify(Callable) == ':py:class:`typing.Callable`'\n    assert restify(Optional) == ':py:class:`typing.Optional`'\n    assert restify(Union) == ':py:class:`typing.Union`'\n    assert restify(Any) == ':py:class:`typing.Any`'\n    assert restify(None) == ':py:obj:`None`'\n", "def test_restify_builtin_classes() -> None:\n    assert restify(Struct) == ':py:class:`struct.Struct`'\n    assert restify(TracebackType) == ':py:class:`types.TracebackType`'\n", "def test_restify_forwards_refAnnotation():\n    class A:\n        pass\n    assert restify(ForwardRef('A')) == ':py:class:`A`'\n    assert restify(ForwardRef('List[int]')) == ':py:obj:`List[int]`'\n"], "sample_962": ["def test_autodoc_mock_object():\n    with mock(['sphinx.test']):\n        import sphinx.test\n\n        # Test MockObject properties\n        mock_object = sphinx.test.MockObject()\n        assert mock_object.__display_name__ == '_MockObject'\n        assert mock_object.__name__ == ''\n        assert mock_object.__sphinx_mock__ is True\n        assert mock_object.__sphinx_decorator_args__ == ()\n\n        # Test MockObject methods\n        assert len(mock_object) == 0\n        assert 'test' not in mock_object\n        assert list(iter(mock_object)) == []\n        assert isinstance(mock_object.__mro_entries__(()), tuple)\n        assert isinstance(mock_object['test'], sphinx.test._MockObject)\n        assert isinstance(getattr(mock_object, 'test'), sphinx.test._MockObject)\n        assert callable(mock_object)\n        assert isinstance(mock_object(), sphinx.test._MockObject)\n\n        # Test _make_subclass\n        subclass = sphinx.test._make_subclass('Subclass', 'module')\n        assert issubclass(subclass, sphinx.test._MockObject)\n        assert subclass.__module__ == 'module'\n        assert subclass.__display_name__ == 'module.Subclass'\n", "def test_ismock():\n    with mock(['my.mock.module']):\n        import my.mock.module\n        assert mock.ismock(my.mock.module)\n        assert mock.ismock(my.mock.module.MyClass)\n        assert not mock.ismock(MyClass1)\n", "def test_autodoc_mock():\n    with mock(['module1', 'module2']):\n        import module1\n        import module2\n\n        assert mock.ismock(module1)\n        assert mock.ismock(module2)\n\n        class_ = module1.MyClass\n        assert mock.ismock(class_)\n        instance = class_()\n        assert mock.ismock(instance)\n\n        undecorated = mock.undecorate(instance)\n        assert undecorated is None\n\n        # test mock module has no attributes by default\n        with pytest.raises(AttributeError):\n            module1.attribute\n\n        # test mock object has no attributes by default\n        with pytest.raises(AttributeError):\n            instance.attribute\n\n        # test mock object can be called\n        result = instance()\n        assert mock.ismock(result)\n", "def test_autodoc_mock():\n    with mock(['target.module.name']) as _:\n        import target.module.name\n        assert isinstance(target.module.name, mock._MockModule)\n\n    import sys\n    assert 'target.module.name' not in sys.modules\n", "def test_ismock():\n    with mock(['sphinx.ext.autodoc.mock.MockObject']):\n        from sphinx.ext.autodoc.mock import MockObject\n        mock_object = MockObject()\n        assert ismock(mock_object) is True\n\n    assert ismock(object()) is False\n    assert ismock(MockFinder([])) is False\n\n"], "sample_961": ["def test_parse_annotation():\n    env = Mock()\n    env.ref_context = {}\n\n    # Simple annotation\n    ann = _parse_annotation('int', env)\n    assert len(ann) == 1\n    assert_node(ann[0], addnodes.pending_xref, reftype='class', reftarget='int')\n\n    # Annotation with module\n    ann = _parse_annotation('a.b.c', env)\n    assert len(ann) == 1\n    assert_node(ann[0], addnodes.pending_xref, reftype='class', reftarget='a.b.c')\n\n    # Annotation with complex type\n    ann = _parse_annotation('Dict[int, str]', env)\n    assert len(ann) == 3\n    assert_node(ann[0], addnodes.pending_xref, reftype='class', reftarget='Dict')\n    assert_node(ann[1], desc_sig_punctuation, '.')\n    assert_node(ann[2], desc_sig_punctuation, '.')\n", "def test_python_parse_annotation():\n    class MockConfig:\n        python_use_unqualified_type_names = False\n\n    class MockEnv:\n        config = MockConfig()\n        ref_context = {}\n\n    env = MockEnv()\n\n    # Simple type annotation\n    annotation = \"str\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 1\n    assert isinstance(result[0], pending_xref)\n\n    # Complex type annotation\n    annotation = \"Dict[str, List[int]]\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) > 1\n    assert isinstance(result[0], pending_xref)\n\n    # Test that unqualified type names are not used when python_use_unqualified_type_names is False\n    annotation = \"List[str]\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) > 1\n    assert isinstance(result[0], pending_xref)\n    assert result[0].astext() == \"List\"\n\n    # Test that unqualified type names are used when python_use_unqualified_type_names is True\n    env.config.python_use_unqualified_type_names = True\n    annotation = \"List[str]\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) > 1\n    assert isinstance(result[0], pending_xref)\n    assert result[0].astext() == \"list\"\n", "def test_parse_annotation():\n    env = Mock()\n    env.ref_context = {}\n\n    # simple annotation\n    annotation = 'int'\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 1\n    assert isinstance(result[0], pending_xref)\n    assert result[0].astext() == annotation\n\n    # complex annotation\n    annotation = 'List[int]'\n    result = _parse_annotation(annotation, env)\n    assert len(result) > 1\n    assert isinstance(result[0], pending_xref)\n    assert result[0].astext() == 'List'\n    assert isinstance(result[2], pending_xref)\n    assert result[2].astext() == 'int'\n\n    # annotation with dot\n    annotation = 'package.module.Class'\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 1\n    assert isinstance(result[0], pending_xref)\n    assert result[0].astext() == annotation\n", "def test_parse_annotation():\n    env = Mock()\n    # simple annotation\n    annotation = \"int\"\n    nodes = _parse_annotation(annotation, env)\n    assert len(nodes) == 1\n    assert isinstance(nodes[0], pending_xref)\n    assert nodes[0].astext() == \"int\"\n\n    # complex annotation\n    annotation = \"List[str]\"\n    nodes = _parse_annotation(annotation, env)\n    assert len(nodes) > 1\n    assert isinstance(nodes[0], pending_xref)\n    assert nodes[0].astext() == \"List\"\n\n    # parse error\n    annotation = \"invalid i\u00e7eren annotation\"\n    nodes = _parse_annotation(annotation, env)\n    assert len(nodes) == 1\n    assert isinstance(nodes[0], pending_xref)\n    assert nodes[0].astext() == annotation\n", "def test_parse_annotation():\n    env_mock = Mock()\n    annotation = 'str'\n    parsed_annotation = _parse_annotation(annotation, env=env_mock)\n    assert len(parsed_annotation) == 1\n    assert_node(parsed_annotation[0], addnodes.pending_xref, refdomain='py', reftype='class', reftarget='str')\n\n    annotation = 'List[str]'\n    parsed_annotation = _parse_annotation(annotation, env=env_mock)\n    assert len(parsed_annotation) == 5\n    assert_node(parsed_annotation[0], addnodes.pending_xref, refdomain='py', reftype='class', reftarget='List')\n    assert_node(parsed_annotation[1], nodes.Text, text='[')\n    assert_node(parsed_annotation[2], addnodes.pending_xref, refdomain='py', reftype='class', reftarget='str')\n    assert_node(parsed_annotation[3], nodes.Text, text=']')\n    assert_node(parsed_annotation[4], nodes.Text, text='')\n"], "sample_965": ["def test_stringify_signature_multiple_defaults():\n    # Create a function with multiple default values\n        pass\n\n    # Get the signature of the function\n    sig = inspect.signature(func)\n\n    # Stringify the signature\n    sig_str = stringify_signature(sig)\n\n    # Check that the default values are correctly represented\n    assert sig_str == \"(a, b=1, c=2)\"\n", "def test_getfullargspec():\n        return None\n\n    argspec = inspect.getfullargspec(func)\n    assert argspec.args == ['a', 'b', 'c', 'd']\n    assert argspec.varargs is None\n    assert argspec.varkw is None\n    assert argspec.defaults == (None,)\n    assert argspec.kwonlyargs == []\n    assert argspec.kwonlydefaults is None\n    assert argspec.annotations == {}\n", "def test_signature_from_ast():\n    code = 'def func(a: int, b: str, *, c: float) -> None: pass'\n    module = ast.parse(code)\n    function = module.body[0]\n\n    sig = inspect.signature_from_ast(function, code)\n    assert str(sig) == '(a: int, b: str, *, c: float) -> None'\n", "def test_isdescriptor():\n    class Descriptor:\n            return instance\n\n    class NotDescriptor:\n            return None\n\n    assert inspect.isdescriptor(Descriptor())\n    assert not inspect.isdescriptor(NotDescriptor())\n    assert not inspect.isdescriptor(lambda x: x)\n    assert not inspect.isdescriptor(1)\n", "def test_getmro():\n    class A:\n        pass\n\n    class B(A):\n        pass\n\n    assert inspect.getmro(A) == (A, object)\n    assert inspect.getmro(B) == (B, A, object)\n"], "sample_966": ["def test_parse_annotation():\n    env = Mock()\n    annotation = \"Union[int, str]\"\n    parsed_annotation = _parse_annotation(annotation, env)\n    assert len(parsed_annotation) == 5\n    assert_node(parsed_annotation[0], addnodes.desc_sig_name, annotation)\n    assert_node(parsed_annotation[1], addnodes.desc_sig_punctuation, '[')\n    assert_node(parsed_annotation[2], pending_xref, reftarget='int')\n    assert_node(parsed_annotation[3], addnodes.desc_sig_punctuation, ',')\n    assert_node(parsed_annotation[4], pending_xref, reftarget='str')\n", "def test_parse_annotation_basic():\n    # simple annotation\n    annotation = \"str\"\n    expected = [addnodes.desc_sig_name(annotation, annotation)]\n    result = _parse_annotation(annotation)\n    assert len(result) == 1\n    assert_node(result[0], expected[0])\n\n    # list of annotations\n    annotation = \"[str, int]\"\n    expected = [\n        addnodes.desc_sig_punctuation('', '['),\n        addnodes.desc_sig_name('str', 'str'),\n        addnodes.desc_sig_punctuation('', ', '),\n        addnodes.desc_sig_name('int', 'int'),\n        addnodes.desc_sig_punctuation('', ']')\n    ]\n    result = _parse_annotation(annotation)\n    for expected_node, result_node in zip(expected, result):\n        assert_node(result_node, expected_node)\n\n    # annotation with dot\n    annotation = \"package.module.Class\"\n    expected = [addnodes.desc_sig_name(annotation, annotation)]\n    result = _parse_annotation(annotation)\n    assert len(result) == 1\n    assert_node(result[0], expected[0])\n", "def test_parse_annotation():\n    env = Mock()\n    env.ref_context = {}\n\n    # Simple annotation\n    annotation = \"int\"\n    parsed_annotation = _parse_annotation(annotation, env)\n    assert len(parsed_annotation) == 1\n    assert isinstance(parsed_annotation[0], pending_xref)\n\n    # Complex annotation\n    annotation = \"list[str, int]\"\n    parsed_annotation = _parse_annotation(annotation, env)\n    assert len(parsed_annotation) == 7\n    assert isinstance(parsed_annotation[0], desc_sig_name)\n    assert isinstance(parsed_annotation[2], desc_sig_punctuation)\n    assert isinstance(parsed_annotation[3], pending_xref)\n    assert isinstance(parsed_annotation[5], pending_xref)\n\n    # Test annotation with spaces\n    annotation = \" typing.Union[str, int] \"\n    parsed_annotation = _parse_annotation(annotation, env)\n    assert len(parsed_annotation) == 1\n    assert isinstance(parsed_annotation[0], pending_xref)\n", "def test_parse_annotation():\n    env = Mock()\n    annotations = _parse_annotation('Union[int, str]', env)\n    assert len(annotations) == 7\n    assert isinstance(annotations[0], nodes.Text)\n    assert annotations[0].astext() == 'Union'\n    assert isinstance(annotations[1], pending_xref)\n    assert annotations[1].astext() == 'int'\n    assert isinstance(annotations[2], nodes.Text)\n    assert annotations[2].astext() == ','\n    assert isinstance(annotations[3], addnodes.desc_sig_space)\n    assert isinstance(annotations[4], pending_xref)\n    assert annotations[4].astext() == 'str'\n    assert isinstance(annotations[5], addnodes.desc_sig_punctuation)\n    assert annotations[5].astext() == '['\n    assert isinstance(annotations[6], addnodes.desc_sig_punctuation)\n    assert annotations[6].astext() == ']'\n\n    # Test with built-in type\n    annotations = _parse_annotation('int', env)\n    assert len(annotations) == 1\n    assert isinstance(annotations[0], pending_xref)\n    assert annotations[0].astext() == 'int'\n\n    # Test with None type\n    annotations = _parse_annotation('None', env)\n    assert len(annotations) == 1\n    assert isinstance(annotations[0], pending_xref)\n    assert annotations[0].astext() == 'None'\n\n    # Test with complex annotation\n    annotations = _parse_annotation('Dict[str, List[int]]', env)\n    assert len(annotations) == 13\n    assert isinstance(annotations[0], nodes.Text)\n    assert annotations[0].astext() == 'Dict'\n    assert isinstance(annotations[1], addnodes.desc_sig_punctuation)\n    assert annotations[1].astext() == '['\n    assert isinstance(annotations[2], pending_xref)\n    assert annotations[2].astext() == 'str'\n    assert isinstance(annotations[3], addnodes.desc_sig_punctuation)\n    assert annotations[3].astext() == ','\n    assert isinstance(annotations[4], addnodes.desc_sig_space)\n    assert isinstance(annotations[5], nodes.Text)\n    assert annotations[5].astext() == 'List'\n    assert isinstance(annotations[6], addnodes.desc_sig_punctuation)\n    assert annotations[6].astext() == '['\n    assert", "def test_parse_annotation():\n    env = Mock()\n    annotation = \"Union[str, int]\"\n    parsed = _parse_annotation(annotation, env)\n    assert len(parsed) == 7\n    assert_node(parsed[0], addnodes.desc_sig_name, annotation, annotation)\n    assert_node(parsed[1], addnodes.desc_sig_punctuation, '[', '[', 0)\n    assert_node(parsed[2], pending_xref, reftype='class', reftarget='str', modname=None)\n    assert_node(parsed[3], addnodes.desc_sig_punctuation, ',', ',', 0)\n    assert_node(parsed[4], addnodes.desc_sig_space, '', '', 0)\n    assert_node(parsed[5], pending_xref, reftype='class', reftarget='int', modname=None)\n    assert_node(parsed[6], addnodes.desc_sig_punctuation, ']', ']', 0)\n"], "sample_964": ["def test_parse_annotation():\n    env = Mock()\n    env.ref_context = {}\n\n    # simple annotation\n    result = _parse_annotation(\"int\", env)\n    assert len(result) == 1\n    assert isinstance(result[0], addnodes.pending_xref)\n\n    # annotation with module name\n    result = _parse_annotation(\"sphinx.addnodes.Node\", env)\n    assert len(result) == 1\n    assert isinstance(result[0], addnodes.pending_xref)\n\n    # None annotation\n    result = _parse_annotation(\"None\", env)\n    assert len(result) == 1\n    assert isinstance(result[0], addnodes.pending_xref)\n\n    # complex annotation\n    result = _parse_annotation(\"List[Tuple[int, str]]\", env)\n    assert len(result) == 10\n    assert isinstance(result[0], nodes.Text)\n    assert isinstance(result[1], addnodes.desc_sig_punctuation)  # '['\n    assert isinstance(result[2], addnodes.pending_xref)  # List\n    assert isinstance(result[3], addnodes.desc_sig_punctuation)  # '['\n    assert isinstance(result[4], addnodes.pending_xref)  # Tuple\n    assert isinstance(result[5], addnodes.desc_sig_punctuation)  # '['\n    assert isinstance(result[6], addnodes.pending_xref)  # int\n    assert isinstance(result[7], addnodes.desc_sig_punctuation)  # ','\n    assert isinstance(result[8], addnodes.desc_sig_space)\n    assert isinstance(result[9], addnodes.pending_xref)  # str\n", "def test_parse_annotation():\n    env = Mock()\n    env.ref_context = {}\n\n    # test type\n    annotation = \"int\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 1\n    assert isinstance(result[0], pending_xref)\n\n    # test complex annotation\n    annotation = \"Union[int, str]\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 6  # Union[ , int, , str, ]\n    assert isinstance(result[0], nodes.Text)\n    assert result[0].astext() == 'Union'\n    assert isinstance(result[2], pending_xref)\n    assert result[2].astext() == 'int'\n    assert isinstance(result[4], pending_xref)\n    assert result[4].astext() == 'str'\n\n    # test unparseable annotation\n    annotation = \"int, str\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 1\n    assert isinstance(result[0], pending_xref)\n    assert result[0].astext() == annotation\n\n    # test string literal annotation\n    annotation = \"'hello'\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 1\n    assert isinstance(result[0], desc_sig_literal_string)\n    assert result[0].astext() == annotation\n\n    # test tuple annotation\n    annotation = \"(int, str)\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 4  # ( , int, str , )\n    assert isinstance(result[1], pending_xref)\n    assert result[1].astext() == 'int'\n    assert isinstance(result[2], pending_xref)\n    assert result[2].astext() == 'str'\n\n    # test list annotation\n    annotation = \"[int, str]\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 4  # [ , int, str , ]\n    assert isinstance(result[1], pending_xref)\n    assert result[1].astext() == 'int'\n    assert isinstance(result[2], pending_xref)\n    assert result[2].astext() == 'str'\n\n    # test None annotation\n    annotation = \"None\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 1\n    assert isinstance(result[0], pending_xref)\n    assert result", "def test_py_pseudo_parse_arglist():\n    assert parse('foo()') == 'foo()'\n    assert parse('foo(a)') == 'foo(a)'\n    assert parse('foo(a, b)') == 'foo(a, b)'\n    assert parse('foo(a, b, c)') == 'foo(a, b, c)'\n    assert parse('foo(a, b=[1, 2, 3])') == 'foo(a, b=[1, 2, 3])'\n    assert parse('foo(a, b=(1, 2, 3))') == 'foo(a, b=(1, 2, 3))'\n    assert parse('foo(a, b={\"key\": \"value\"})') == 'foo(a, b={\"key\": \"value\"})'\n    assert parse('foo(a, b={1, 2, 3})') == 'foo(a, b={1, 2, 3})'\n    assert parse('foo(a, b=\"string\")') == 'foo(a, b=\"string\")'\n    assert parse('foo(a, b=\\'string\\')') == 'foo(a, b=\\'string\\')'\n    assert parse('foo(a, b=string)') == 'foo(a, b=string)'\n    assert parse('foo(a, b=123)') == 'foo(a, b=123)'\n    assert parse('foo(a, b=None)') == 'foo(a, b=None)'\n    assert parse('foo(a, b=True)') == 'foo(a, b=True)'\n    assert parse('foo(a, b=False)') == 'foo(a, b=False)'\n", "def test_parse_annotation():\n    # annotation with module name\n    annotation = \"typing.Union[None, str]\"\n    result = _parse_annotation(annotation)\n    assert len(result) == 4\n    assert isinstance(result[0], desc_sig_name)\n    assert isinstance(result[1], desc_sig_punctuation)\n    assert isinstance(result[2], desc_sig_name)\n    assert isinstance(result[3], desc_sig_punctuation)\n\n    # annotation without module name\n    annotation = \"Union[None, str]\"\n    result = _parse_annotation(annotation)\n    assert len(result) == 4\n    assert isinstance(result[0], desc_sig_name)\n    assert isinstance(result[1], desc_sig_punctuation)\n    assert isinstance(result[2], desc_sig_name)\n    assert isinstance(result[3], desc_sig_punctuation)\n\n    # annotation with nested types\n    annotation = \"Union[Dict[str, int], List[int]]\"\n    result = _parse_annotation(annotation)\n    assert len(result) == 14\n    assert isinstance(result[0], desc_sig_name)\n    assert isinstance(result[1], desc_sig_punctuation)\n    assert isinstance(result[2], desc_sig_name)\n    assert isinstance(result[3], desc_sig_punctuation)\n    assert isinstance(result[4], desc_sig_name)\n    assert isinstance(result[5], desc_sig_punctuation)\n    assert isinstance(result[6], desc_sig_name)\n    assert isinstance(result[7], desc_sig_punctuation)\n    assert isinstance(result[8], desc_sig_name)\n    assert isinstance(result[9], desc_sig_punctuation)\n    assert isinstance(result[10], desc_sig_name)\n    assert isinstance(result[11], desc_sig_punctuation)\n    assert isinstance(result[12], desc_sig_name)\n    assert isinstance(result[13], desc_sig_punctuation)\n", "def test_domain_py_parse_annotation():\n    env = Mock()\n    annotation = \"typing.Union[int, str]\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 1\n    node = result[0]\n    assert isinstance(node, pending_xref)\n    assert node.astext() == \"Union[int, str]\"\n    assert node.attributes['reftarget'] == \"typing.Union\"\n\n    annotation = \"int | str\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 1\n    node = result[0]\n    assert node.astext() == \"int | str\"\n\n    annotation = \"(int | str) = None\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 1\n    node = result[0]\n    assert node.astext() == \"(int | str) = None\"\n\n    annotation = \"None\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 1\n    node = result[0]\n    assert isinstance(node, pending_xref)\n    assert node.astext() == \"None\"\n    assert node.attributes['reftarget'] == \"None\"\n"], "sample_967": ["def test_mathjax_config(app, status, warning):\n    app.builder.build_all()\n    html = (app.outdir / 'index.html').read_text()\n    assert MATHJAX_URL in html\n    assert re.search(r'MathJax.Hub.Config', html) is None\n    assert re.search(r'window.MathJax', html) is None\n", "def test_mathjax_configuration(app, status, warning):\n    app.builder.build_all()\n    assert MATHJAX_URL in app.builder.html_context['script_files'][-1]\n    assert 'text/x-mathjax-config' not in [t for f, t in app.builder.html_context['script_files']]\n    assert app.builder.html_context['script_files'][-1][1] == {'defer': 'defer'}\n", "def test_mathjax_config(app, status, warning):\n    app.build()\n    html = (app.outdir / 'index.html').read_text()\n    assert 'window.MathJax' in html\n    assert 'tex: {inlineMath: [[\"$', '$'], [\"\\\\(\", \"\\\\)\"]]' in html\n    assert 'svg: {scale: 1.5}' in html\n", "def test_mathjax_output(app, status, warning):\n    app.build()\n    assert_node(app.env.docnames, 'index', nodes.math, r'\\frac{1}{2}')\n    assert_node(app.env.docnames, 'index', nodes.math_block, r'\\frac{3}{4}')\n    mathjax_script = '<script src=\"{}\" defer=\"defer\"></script>'.format(MATHJAX_URL)\n    assert mathjax_script in (app.outdir / 'index.html').read_text()\n", "def test_mathjax_html(app, status, warning):\n    app.builder.build_all()\n    html = (app.outdir / 'index.html').read_text()\n    assert MATHJAX_URL in html\n    assert r'\\(' in html\n    assert r'\\)' in html\n    assert r'\\[' in html\n    assert r'\\]' in html\n"], "sample_970": ["def test_is_builtin_class_method():\n    assert inspect.is_builtin_class_method(int, '__init__')\n    assert inspect.is_builtin_class_method(list, '__init__')\n    assert not inspect.is_builtin_class_method(datetime.datetime, '__init__')\n", "def test_signature_from_str():\n    sig = inspect.signature_from_str(\"(a, b: int) -> str\")\n    assert str(sig) == \"(a, b: int) -> str\"\n\n    sig = inspect.signature_from_str(\"(a: str = 'hello', b: int = 42) -> None\")\n    assert str(sig) == \"(a: str = 'hello', b: int = 42) -> None\"\n\n    sig = inspect.signature_from_str(\"(a: str, b: int, *, c: float = 3.14) -> bytes\")\n    assert str(sig) == \"(a: str, b: int, *, c: float = 3.14) -> bytes\"\n\n    sig = inspect.signature_from_str(\"(a: str, b: int, *args, **kwargs) -> tuple\")\n    assert str(sig) == \"(a: str, b: int, *args, **kwargs) -> tuple\"\n\n    sig = inspect.signature_from_str(\"(a: str, b: int, *, c: float, d: str = 'world') -> None\")\n    assert str(sig) == \"(a: str, b: int, *, c: float, d: str = 'world') -> None\"\n", "def test_isdescriptor():\n    class NotDescriptor:\n        pass\n\n    class Descriptor:\n            return self\n\n    class DataDescriptor:\n            pass\n\n    not_descriptor = NotDescriptor()\n    descriptor = Descriptor()\n    data_descriptor = DataDescriptor()\n\n    assert inspect.isdescriptor(descriptor) is True\n    assert inspect.isdescriptor(data_descriptor) is True\n    assert inspect.isdescriptor(not_descriptor) is False\n", "def test_signature_from_str():\n    signature_str = \"(a: int, b: str) -> None\"\n    sig = inspect.signature_from_str(signature_str)\n    assert sig.parameters[\"a\"].annotation == int\n    assert sig.parameters[\"b\"].annotation == str\n    assert sig.return_annotation == type(None)\n\n    # Test with no return annotation\n    signature_str = \"(a: int, b: str)\"\n    sig = inspect.signature_from_str(signature_str)\n    assert sig.parameters[\"a\"].annotation == int\n    assert sig.parameters[\"b\"].annotation == str\n    assert sig.return_annotation == Parameter.empty\n", "def test_stringify_signature():\n        pass\n\n    sig = inspect.signature(func)\n    assert stringify_signature(sig) == \"(a, b, *, c, d=None)\"\n\n        pass\n\n    sig = inspect.signature(func)\n    assert stringify_signature(sig) == \"(a, b, *, c, d=None) -> str\"\n\n        pass\n\n    sig = inspect.signature(func)\n    assert stringify_signature(sig, show_annotation=False) == \"(a, b, *, c, d=None)\"\n\n        pass\n\n    sig = inspect.signature(func)\n    assert stringify_signature(sig, show_return_annotation=False) == \"(a, b, *, c, d=None)\"\n"], "sample_969": ["def test_restify_py37():\n    assert restify(Union[int, str]) == ':py:obj:`~typing.Union`\\\\ [:py:obj:`~int`, :py:obj:`~str`]'\n    assert restify(Optional[int]) == ':py:obj:`~typing.Optional`\\\\ [:py:obj:`~int`]'\n    assert restify(MyList[int]) == ':py:class:`~MyList`\\\\ [:py:obj:`~int`]'\n    assert restify(Callable[[int], str]) == ':py:obj:`~typing.Callable`\\\\ [[:py:obj:`~int`], :py:obj:`~str`]'\n    assert restify(Generator[int, str, None]) == ':py:class:`~typing.Generator`\\\\ [:py:obj:`~int`, :py:obj:`~str`, :py:obj:`None`]'\n", "def test_restify_py37():\n    # Test built-in types\n    assert restify(int) == ':py:class:`int`'\n    assert restify(list) == ':py:class:`list`'\n    assert restify(tuple) == ':py:class:`tuple`'\n\n    # Test custom types\n    assert restify(MyClass1) == ':py:class:`__main__.MyClass1`'\n    assert restify(MyClass2) == ':py:class:`__main__.MyClass2`'\n\n    # Test generic types\n    assert restify(List[int]) == ':py:class:`~typing.List`\\\\ [:py:class:`int`]'\n    assert restify(MyList[int]) == ':py:class:`__main__.MyList`\\\\ [:py:class:`int`]'\n    assert restify(Dict[str, int]) == ':py:class:`~typing.Dict`\\\\ [:py:class:`str`, :py:class:`int`]'\n    assert restify(Union[int, str]) == ':py:obj:`~typing.Union`\\\\ [:py:class:`int`, :py:class:`str`]'\n    assert restify(Union[int, None]) == ':py:obj:`~typing.Optional`\\\\ [:py:class:`int`]'\n    assert restify(Callable[[int], str]) == ':py:obj:`~typing.Callable`\\\\ [[ :py:class:`int` ] ,  :py:class:`str` ]'\n\n    # Test newtype\n    assert restify(MyInt) == ':py:class:`__main__.MyInt`'\n\n    # Test broken type\n    assert restify(BrokenType) == ':py:obj:`__main__.BrokenType`'\n", "def test_restify_ForwardRef():\n    if sys.version_info > (3, 7):\n        from typing import ForwardRef\n    else:\n        from sphinx.util.typing import ForwardRef\n\n    ref = ForwardRef('MyClass1')\n    assert restify(ref) == ':py:class:`MyClass1`'\n", "def test_restify_union():\n    assert restify(Union[int, str]) == ':py:obj:`~typing.Union`\\\\ [:py:class:`int`, :py:class:`str`]'\n    assert restify(Optional[int]) == ':py:obj:`~typing.Optional`\\\\ [:py:class:`int`]'\n    assert restify(Union[int, None]) == ':py:obj:`~typing.Optional`\\\\ [:py:class:`int`]'\n    assert restify(Union[int, str, None]) == ':py:obj:`~typing.Optional`\\\\ [:py:obj:`~typing.Union`\\\\ [:py:class:`int`, :py:class:`str`]]'\n", "def test_restify_py37_builtins():\n    # Test builtins with __qualname__\n    assert restify(type(None)) == ':py:obj:`None`'\n    assert restify(Any) == ':py:obj:`~typing.Any`'\n    assert restify(list) == ':py:class:`list`'\n    assert restify(dict) == ':py:class:`dict`'\n    assert restify(object) == ':py:class:`object`'\n    # Test exception class\n    assert restify(Exception) == ':py:exc:`Exception`'\n    # Test generic alias from typing\n    assert restify(List[int]) == ':py:class:`~typing.List`\\\\ [:py:class:`int`]'\n    assert restify(Dict[str, int]) == ':py:class:`~typing.Dict`\\\\ [:py:class:`str`, :py:class:`int`]'\n    assert restify(Tuple[str, int]) == ':py:class:`~typing.Tuple`\\\\ [:py:class:`str`, :py:class:`int`]'\n    assert restify(Callable[[str, int], None]) == ':py:class:`~typing.Callable`\\\\ [[[:py:class:`str`, :py:class:`int`]], :py:obj:`None`]'\n"], "sample_971": ["def test_is_suppressed_warning():\n    assert is_suppressed_warning('warning.type', 'subtype', ['warning.type'])\n    assert is_suppressed_warning('warning.type', 'subtype', ['warning.type.subtype'])\n    assert is_suppressed_warning('warning.type', 'subtype', ['warning.type.*'])\n    assert not is_suppressed_warning('warning.type', 'subtype', ['other.warning.type'])\n    assert not is_suppressed_warning('warning.type', 'subtype', ['warning.type.other'])\n", "def test_is_suppressed_warning():\n    assert is_suppressed_warning('warning_type', None, []) is False\n    assert is_suppressed_warning('warning_type', None, ['warning_type']) is True\n    assert is_suppressed_warning('warning_type', 'subtype', ['warning_type.subtype']) is True\n    assert is_suppressed_warning('warning_type', 'subtype', ['warning_type.another_subtype']) is False\n    assert is_suppressed_warning('warning_type', 'subtype', ['warning_type.*']) is True\n    assert is_suppressed_warning(None, None, []) is False\n", "def test_get_node_location():\n    node = nodes.Text('source text', source='path/to/source', line=42)\n    assert logging.get_node_location(node) == 'path/to/source:42'\n    node = nodes.Text('source text', source='path/to/source')\n    assert logging.get_node_location(node) == 'path/to/source:'\n    node = nodes.Text('source text', line=42)\n    assert logging.get_node_location(node) == '<unknown>:42'\n    node = nodes.Text('source text')\n    assert logging.get_node_location(node) is None\n", "def test_sphinx_logger_adapter():\n    logger = logging.getLogger('sphinx')\n    adapter = logging.SphinxLoggerAdapter(logger, {})\n\n    # Test log method with integer level\n    adapter.log(logging.INFO, 'Info message')\n    adapter.log(15, 'Verbose message')\n\n    # Test log method with string level\n    adapter.log('INFO', 'Info message')\n    adapter.log('VERBOSE', 'Verbose message')\n\n    # Test verbose method\n    adapter.verbose('Verbose message')\n\n    # Test process method\n    record = logging.LogRecord('name', logging.INFO, 'pathname', 1, 'message', None, None)\n    record = adapter.process(record.msg, record.__dict__)\n    assert record == ('message', record.__dict__)\n", "def test_get_node_location():\n    node = nodes.Element()\n    node.source = \"test_file.rst\"\n    node.line = 10\n\n    location = logging.get_node_location(node)\n    assert location == \"test_file.rst:10\"\n\n    node.line = None\n    location = logging.get_node_location(node)\n    assert location == \"test_file.rst:\"\n\n    node.source = None\n    location = logging.get_node_location(node)\n    assert location is None\n\n    node.line = 20\n    location = logging.get_node_location(node)\n    assert location == \"<unknown>:20\"\n"], "sample_968": ["def test_parse_annotation():\n    env = Mock()\n    env.ref_context = {}\n\n    # simple annotation\n    text = \"str\"\n    expected = [\n        addnodes.desc_sig_name(text, text),\n    ]\n    assert _parse_annotation(text, env) == expected\n\n    # complex annotation\n    text = \"Union[str, int]\"\n    expected = [\n        addnodes.desc_sig_name(\"Union\", \"Union\"),\n        addnodes.desc_sig_punctuation('[', '['),\n        addnodes.desc_sig_name(\"str\", \"str\"),\n        addnodes.desc_sig_punctuation(',', ','),\n        addnodes.desc_sig_space(),\n        addnodes.desc_sig_name(\"int\", \"int\"),\n        addnodes.desc_sig_punctuation(']', ']'),\n    ]\n    result = _parse_annotation(text, env)\n    assert len(result) == len(expected)\n    for actual, expected in zip(result, expected):\n        assert_node(actual, expected)\n\n    # annotation with spaces\n    text = \"Dict[str, List[int]]\"\n    expected = [\n        addnodes.desc_sig_name(\"Dict\", \"Dict\"),\n        addnodes.desc_sig_punctuation('[', '['),\n        addnodes.desc_sig_name(\"str\", \"str\"),\n        addnodes.desc_sig_punctuation(',', ','),\n        addnodes.desc_sig_space(),\n        addnodes.desc_sig_name(\"List\", \"List\"),\n        addnodes.desc_sig_punctuation('[', '['),\n        addnodes.desc_sig_name(\"int\", \"int\"),\n        addnodes.desc_sig_punctuation(']', ']'),\n        addnodes.desc_sig_punctuation(']', ']'),\n    ]\n    result = _parse_annotation(text, env)\n    assert len(result) == len(expected)\n    for actual, expected in zip(result, expected):\n        assert_node(actual, expected)\n", "def test__parse_annotation():\n    env = Mock()\n    env.config.python_use_unqualified_type_names = False\n\n    # simple annotation\n    ann = _parse_annotation('int', env)\n    assert_node(ann, [addnodes.pending_xref])\n\n    # complex annotation\n    ann = _parse_annotation('List[int]', env)\n    assert_node(ann, [addnodes.pending_xref])\n    assert_node(ann[0].children, [nodes.Text('List['), addnodes.pending_xref, nodes.Text(']')])\n\n    # annotation with module name\n    ann = _parse_annotation('typing.List[int]', env)\n    assert_node(ann, [addnodes.pending_xref])\n    assert_node(ann[0].children, [nodes.Text('typing.List['), addnodes.pending_xref, nodes.Text(']')])\n\n    # annotation with attribute\n    ann = _parse_annotation('typing.List.__getitem__', env)\n    assert_node(ann, [addnodes.pending_xref])\n    assert_node(ann[0].children, [nodes.Text('typing.List.__getitem__')])\n\n    # annotation with constant\n    ann = _parse_annotation('None', env)\n    assert_node(ann, [addnodes.pending_xref])\n    assert_node(ann[0].children, [nodes.Text('None')])\n\n    # annotation with string literal\n    ann = _parse_annotation('\\'annotation\\'', env)\n    assert_node(ann, [addnodes.desc_sig_literal_string])\n\n    # annotation with tuple\n    ann = _parse_annotation('(int, str)', env)\n    assert_node(ann, [addnodes.pending_xref, addnodes.pending_xref])\n    assert_node(ann[0].children, [nodes.Text('('), addnodes.pending_xref, nodes.Text(',')])\n    assert_node(ann[1].children, [addnodes.desc_sig_space, addnodes.pending_xref, nodes.Text(')')])\n\n    # annotation with union\n    ann = _parse_annotation('Union[int, str]', env)\n    assert_node(ann, [addnodes.pending_xref])\n    assert_node(ann[0].children, [nodes.Text('Union['), addnodes.pending_xref, nodes.Text(',')])\n\n    # annotation with unqualified type names\n    env.config.python_use_unqualified_type_names = True\n    ann = _parse_annotation('List[int]', env)\n    assert_node(ann, [addnodes.pending_xref_condition, add", "def test_parse_annotation():\n    env = Mock()\n    annotations = _parse_annotation('int', env)\n    assert_node(annotations[0], addnodes.pending_xref,\n                reftype='class', reftarget='int', refspecific=True)\n\n    annotations = _parse_annotation('\"int\"', env)\n    assert_node(annotations[0], nodes.literal, text='\"int\"')\n\n    annotations = _parse_annotation('typing.List[int]', env)\n    assert_node(annotations[0], addnodes.pending_xref,\n                reftype='class', reftarget='typing.List', refspecific=True)\n    assert_node(annotations[1], addnodes.pending_xref,\n                reftype='class', reftarget='int', refspecific=True)\n\n    annotations = _parse_annotation('List[int]', env)\n    assert_node(annotations[0], nodes.Text, text='List')\n    assert_node(annotations[1], addnodes.pending_xref,\n                reftype='class', reftarget='int', refspecific=True)\n\n    annotations = _parse_annotation('Union[str, int]', env)\n    assert_node(annotations[0], addnodes.pending_xref,\n                reftype='class', reftarget='Union', refspecific=True)\n    assert_node(annotations[2], addnodes.pending_xref,\n                reftype='class', reftarget='str', refspecific=True)\n    assert_node(annotations[4], addnodes.pending_xref,\n                reftype='class', reftarget='int', refspecific=True)\n", "def test_parse_annotation():\n    env = Mock()\n    env.ref_context = {'py:module': 'module', 'py:class': 'class'}\n    annotation = \"Union[str, int]\"\n    children = _parse_annotation(annotation, env)\n    assert len(children) == 1\n    assert isinstance(children[0], pending_xref)\n    assert children[0].astext() == 'Union'\n    assert len(children[0].children) == 2\n    assert isinstance(children[0].children[0], nodes.Text)\n    assert children[0].children[0].astext() == 'Union'\n    assert isinstance(children[0].children[1], pending_xref_condition)\n", "def test_parse_annotation():\n    env = Mock()\n    env.ref_context = {}\n\n    # Basic type\n    text = 'int'\n    expected = [addnodes.pending_xref('', refdomain='py', reftype='class', reftarget='int')]\n    result = _parse_annotation(text, env)\n    assert_node(result, expected)\n\n    # Complex type\n    text = 'List[int]'\n    expected = [\n        addnodes.desc_sig_name('', 'List', *[\n            addnodes.pending_xref('', refdomain='py', reftype='class', reftarget='List')\n        ]),\n        addnodes.desc_sig_punctuation('', '['),\n        addnodes.pending_xref('', refdomain='py', reftype='class', reftarget='int'),\n        addnodes.desc_sig_punctuation('', ']')\n    ]\n    result = _parse_annotation(text, env)\n    assert_node(result, expected)\n\n    # Union type\n    text = 'int | str'\n    expected = [\n        addnodes.pending_xref('', refdomain='py', reftype='class', reftarget='int'),\n        addnodes.desc_sig_space(),\n        addnodes.desc_sig_punctuation('', '|'),\n        addnodes.desc_sig_space(),\n        addnodes.pending_xref('', refdomain='py', reftype='class', reftarget='str')\n    ]\n    result = _parse_annotation(text, env)\n    assert_node(result, expected)\n\n    # Literal\n    text = 'Literal[\"a\", 1]'\n    expected = [\n        addnodes.desc_sig_name('', 'Literal', *[\n            addnodes.pending_xref('', refdomain='py', reftype='class', reftarget='Literal')\n        ]),\n        addnodes.desc_sig_punctuation('', '['),\n        addnodes.literal('', 'a'),\n        addnodes.desc_sig_punctuation('', ','),\n        addnodes.desc_sig_space(),\n        addnodes.literal('', '1'),\n        addnodes.desc_sig_punctuation('', ']')\n    ]\n    result = _parse_annotation(text, env)\n    assert_node(result, expected)\n"], "sample_972": ["def test_stringify_broken_type():\n    with pytest.warns(UserWarning):\n        assert stringify(BrokenType()) == repr(BrokenType())\n", "def test_restify_broken_type():\n    assert restify(BrokenType) == ':py:obj:`BrokenType`'\n", "def test_restify_broken_type():\n    with pytest.raises(AttributeError):\n        restify(BrokenType)\n", "def test_restify_forwardref():\n    ref = ForwardRef('MyClass1')\n    assert restify(ref) == ':py:class:`MyClass1`'\n", "def test_restify():\n    # test restify() for builtins\n    assert restify(int) == ':py:class:`int`'\n    assert restify(str) == ':py:class:`str`'\n\n    # test restify() for typing module\n    assert restify(Union) == ':py:obj:`~typing.Union`'\n    assert restify(List) == ':py:class:`~typing.List`'\n    assert restify(Optional) == ':py:obj:`~typing.Optional`'\n\n    # test restify() for NewType\n    assert restify(MyInt) == ':py:class:`MyInt`'\n\n    # test restify() for types defined in typing module\n    assert restify(Tuple[int, str]) == ':py:class:`~typing.Tuple`\\\\ [int, str]'\n    assert restify(List[int]) == ':py:class:`~typing.List`\\\\ [int]'\n    assert restify(Generator[int, str, None]) == ':py:class:`~typing.Generator`\\\\ [int, str, NoneType]'\n\n    # test restify() for types defined in other modules\n    assert restify(MyList) == ':py:class:`__main__.MyList`'\n    assert restify(MyClass1) == ':py:class:`__main__.MyClass1`'\n\n    # test restify() for ForwardRef\n    if sys.version_info > (3, 7):\n        from typing import ForwardRef\n        ref = ForwardRef('MyClass1')\n        assert restify(ref) == ':py:class:`MyClass1`'\n    else:\n        from sphinx.util.typing import ForwardRef\n        ref = ForwardRef('MyClass1')\n        assert restify(ref) == ':py:class:`MyClass1`'\n"], "sample_974": ["def test_ccode_sinc():\n    expr = sinc(x)\n    result = ccode(expr)\n    assert \"((x > 0) - (x < 0))\" not in result\n    assert result == '((sin(x)/x) > 0 ? sin(x)/x : 1)'\n", "def test_ccode_exceptions():\n    raises(ValueError, lambda: ccode(Piecewise((x + 1, x > 0))))\n", "def test_ccode_reserved_words():\n    x, y = symbols('x, y')\n    f = implemented_function('int', Lambda(x, x**2))\n    g = implemented_function('while', Lambda(x, x**2))\n    assert ccode(f(x)) == \"int(x)\"\n    assert ccode(g(x), error_on_reserved=False) == \"while(x)\"\n    assert ccode(g(x), error_on_reserved=True) is None\n    assert ccode(g(x)) is None\n    raises(ValueError, lambda: ccode(g(x), error_on_reserved=True))\n", "def test_ccode_user_functions():\n    x = symbols('x')\n    p = CCodePrinter()\n\n    # Single user function\n    p.known_functions = {\"func\": \"f\"}\n    assert p._print_Function(func(x)) == \"f(x)\"\n\n    # Multiple user functions\n    p.known_functions = {\"func\": \"f\", \"sin\": \"my_sin\"}\n    assert p._print_Function(func(x)) == \"f(x)\"\n    assert p._print_Function(sin(x)) == \"my_sin(x)\"\n\n    # User function with multiple return types\n    p.known_functions = {\"Abs\": [(lambda x: not x.is_integer, \"fabs\"),\n                                 (lambda x: x.is_integer, \"ABS\")]}\n    assert p._print_Function(Abs(x)) == \"fabs(x)\"\n    assert p._print_Function(Abs(Integer(1))) == \"ABS(1)\"\n\n    # Test with function name as string\n    p.known_functions = {\"func\": \"f\"}\n    assert ccode(func(x), user_functions={\"func\": \"f\"}) == \"f(x)\"\n", "def test_print_sinc():\n    expr = implemented_function(\"sinc\", lambda x: sin(x)/x)\n    string = ccode(expr(x))\n    assert string == '((sin(x) > 0) - (sin(x) < 0))*((1 - cos(x))/x)'\n"], "sample_973": ["def test_signature_from_str():\n    sig = inspect.signature_from_str(\"(a: int, b: str) -> None\")\n    assert sig.parameters[\"a\"].annotation == int\n    assert sig.parameters[\"b\"].annotation == str\n    assert sig.return_annotation == type(None)\n", "def test_stringify_signature_unqualified_typehints():\n    # https://github.com/sphinx-doc/sphinx/issues/10065\n    import io\n    from typing import Optional\n\n        pass\n\n    sig = inspect.signature(func)\n    result = stringify_signature(sig, unqualified_typehints=True)\n    assert result == '(x: Optional[StringIO]) -> None'\n", "def test_signature_from_str():\n    # test simple function\n    sig = inspect.signature_from_str(\"(a, b)\")\n    assert len(sig.parameters) == 2\n    assert 'a' in sig.parameters\n    assert 'b' in sig.parameters\n\n    # test function with annotation\n    sig = inspect.signature_from_str(\"(a: int, b: str)\")\n    assert len(sig.parameters) == 2\n    assert 'a' in sig.parameters\n    assert 'b' in sig.parameters\n    assert sig.parameters['a'].annotation == int\n    assert sig.parameters['b'].annotation == str\n\n    # test function with return annotation\n    sig = inspect.signature_from_str(\"(a: int, b: str) -> None\")\n    assert len(sig.parameters) == 2\n    assert 'a' in sig.parameters\n    assert 'b' in sig.parameters\n    assert sig.parameters['a'].annotation == int\n    assert sig.parameters['b'].annotation == str\n    assert sig.return_annotation == type(None)\n\n    # test function with varargs\n    sig = inspect.signature_from_str(\"(a, *args)\")\n    assert len(sig.parameters) == 2\n    assert 'a' in sig.parameters\n    assert 'args' in sig.parameters\n    assert sig.parameters['args'].kind == Parameter.VAR_POSITIONAL\n\n    # test function with keyword-only argument\n    sig = inspect.signature_from_str(\"(a, *, b)\")\n    assert len(sig.parameters) == 2\n    assert 'a' in sig.parameters\n    assert 'b' in sig.parameters\n    assert sig.parameters['b'].kind == Parameter.KEYWORD_ONLY\n\n    # test function with default value\n    sig = inspect.signature_from_str(\"(a=1, b=2)\")\n    assert len(sig.parameters) == 2\n    assert 'a' in sig.parameters\n    assert 'b' in sig.parameters\n    assert sig.parameters['a'].default == \"1\"  # this returns string not int\n    assert sig.parameters['b'].default == \"2\"  # this returns string not int\n\n    # test function with TypeAlias\n    namespace = TypeAliasNamespace({\"int\": \"int\"})\n    sig = inspect.signature_from_str(\"(a: int)\", localns=namespace)\n    assert len(sig.parameters) == 1\n    assert 'a' in sig.parameters\n    assert sig.parameters['a'].annotation == \"int\"  # this returns string not int\n", "def test_signature_from_ast():\n    code = 'def func(a: int, b: str = \"hello\") -> None: pass'\n    module = ast.parse(code)\n    function = module.body[0]\n\n    sig = inspect.signature_from_ast(function, code)\n    assert str(sig) == '(a: int, b: str = \\'hello\\') -> None'\n\n    code = 'def func(a: int, *, b: str = \"hello\") -> None: pass'\n    module = ast.parse(code)\n    function = module.body[0]\n\n    sig = inspect.signature_from_ast(function, code)\n    assert str(sig) == '(a: int, *, b: str = \\'hello\\') -> None'\n\n    code = 'def func(a: int, /, b: str = \"hello\") -> None: pass'\n    module = ast.parse(code)\n    function = module.body[0]\n\n    sig = inspect.signature_from_ast(function, code)\n    assert str(sig) == '(a: int, /, b: str = \\'hello\\') -> None'\n", "def test_signature_from_ast():\n    code = \"\"\""], "sample_975": ["def test_nsolve_rational():\n    x = symbols('x')\n    assert nsolve(1/x - 1, 1) == 1\n", "def test_nsolve_no_method():\n    x = Symbol('x')\n    f = x**2 - 2\n    raises(ValueError, lambda: nsolve(f, x, 1, method='foo'))\n", "def test_nsolve_linear_system():\n    x, y = symbols('x y')\n    system = Matrix([\n        [1, 2],\n        [3, 4]\n    ])*Matrix([[x], [y]]) - Matrix([[2], [3]])\n\n    # check that the input is not modified\n    system = Matrix(system)\n    assert nsolve(system, [x, y], [0, 0]) == Matrix([\n        [-1],\n        [3/2]\n    ])\n    assert system == Matrix([\n        [x + 2*y - 2],\n        [3*x + 4*y - 3]\n    ])\n\n    assert nsolve(system, [x, y], [0, 0]) == Matrix([\n        [-1],\n        [3/2]\n    ])\n\n    assert nsolve(system, [x, y], [0, -10]) == Matrix([\n        [-1],\n        [3/2]\n    ])\n\n    assert nsolve(system, [x, y], [x, 0]) == Matrix([\n        [-1],\n        [3/2]\n    ])\n\n    assert nsolve(system, [x, y], [x, -10]) == Matrix([\n        [-1],\n        [3/2]\n    ])\n\n    assert nsolve(system, [x, y], [10, 0]) == Matrix([\n        [-1],\n        [3/2]\n    ])\n", "def test_nsolve_discrete():\n    assert nsolve(Eq(2**x, 3), x, 1) == mpf('1.58496250072115618145373894394781')\n    assert nsolve(sin(x), x, 1) == mpf('0.0')\n    assert nsolve(Eq(x, 1), x, 1) == mpf('1')\n    assert nsolve(2 - 2*sin(x)**2, x, 0.1) == mpf('0.52359877559829887307710723054658')\n", "def test_nsolve_lambda():\n    f = lambdify('x', sin('x'))\n    assert abs(nsolve(f, 1.9) - pi/2) < 1e-6\n"], "sample_977": ["def test_power():\n    assert mcode(x**2) == 'x^2'\n    assert mcode(x**-2) == 'x^(-2)'\n    assert mcode(1/x) == 'x^(-1)'\n    assert mcode(x**(1/2)) == 'x^(1/2)'\n    assert mcode(x**(y/2)) == 'x^(y/2)'\n    assert mcode(x**y) == 'x^y'\n", "def test_print_Mul():\n    assert mcode(x*y) == \"x*y\"\n    assert mcode(x*y*z) == \"x*y*z\"\n    assert mcode(x/y) == \"x/y\"\n    assert mcode(x/(y*z)) == \"x/(y*z)\"\n    assert mcode(1/(x*y)) == \"1/(x*y)\"\n", "def test_Pow():\n    assert mcode(x**3) == 'x^3'\n    assert mcode(x**(y**3)) == 'x^(y^3)'\n    assert mcode((x**y)**3) == '(x^y)^3'\n", "def test_Mathematica_code_Pow():\n    assert mcode(x**3) == 'x^3'\n    assert mcode(x**(1/2)) == 'x^(1/2)'\n    assert mcode(x**(3/2)) == 'x^(3/2)'\n", "def test_Pow():\n    assert mcode(x**2) == 'x^2'\n    assert mcode(1/x) == '1/x'\n    assert mcode(x**-2) == 'x^(-2)'\n    assert mcode(x**(1/2)) == 'x^(1/2)'\n    assert mcode(x**(3/2)) == 'x^(3/2)'\n    assert mcode((x**2)**(3/2)) == '(x^2)^(3/2)'\n"], "sample_976": ["def test_wild_properties():\n    # Test that Wild properties are properly applied\n    x = Symbol('x')\n    a = Wild('a', properties=[lambda x: x.is_integer])\n    assert a.matches(5) is not None\n    assert a.matches(x) is None\n    assert a.matches(pi) is None\n", "def test_var():\n    raises(ValueError, lambda: var(''))\n    x = Symbol('x')\n    assert var(x) == x\n    assert var('x') == x\n    assert var('x,y') == (x, Symbol('y'))\n    assert var('x,y', real=True) == (Symbol('x', real=True), Symbol('y', real=True))\n    a = Symbol('a')\n    b = Symbol('b')\n    assert var('a,b') == (a, b)\n    raises(ValueError, lambda: var('1'))\n", "def test_symbols_iterator():\n    x, y, z = symbols('x y z')\n    assert list(symbols('x:z')) == [x, y, z]\n    assert list(symbols('x(:z)')) == [Symbol('xa'), Symbol('xb'), Symbol('xc')]\n    assert list(symbols(':z')) == [Symbol('a'), Symbol('b'), Symbol('c')]\n    assert list(symbols('x:2(1:3)')) == [Symbol('x01'), Symbol('x02'), Symbol('x11'), Symbol('x12')]\n", "def test_Wild_properties():\n    a = Wild('a', properties=[lambda x: x.is_integer, lambda x: x.is_real])\n    assert a.matches(5)\n    assert a.matches(Rational(3, 2))\n    assert not a.matches(I)\n", "def test_symbolsIndexed():\n    # check symbols with ranges\n    a = symbols('a:5')\n    assert len(a) == 5\n    assert set(str(i) for i in a) == {'a0', 'a1', 'a2', 'a3', 'a4'}\n\n    b = symbols('b:6(2:4)')\n    assert len(b) == 6\n    assert set(str(i) for i in b) == {'b20', 'b21', 'b22', 'b30', 'b31', 'b32'}\n\n    # check typesetting\n    assert symbols('beta:5')[0] == Symbol('beta0')\n    assert symbols('x:2(1:3)')[0] == Symbol('x11')\n"], "sample_979": ["def test_MatrixExpr_as_coeff_mmul():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', m, l)\n    assert A.as_coeff_mmul() == (1, A)\n    expr = 2 * A * B\n    assert expr.as_coeff_mmul() == (2, MatMul(A, B))\n", "def test_matrix_element():\n    x = MatrixSymbol('x', 3, 3)\n    assert isinstance(x[0, 1], MatrixElement)\n    assert x[0, 1].parent == x\n    assert x[0, 1].i == 0\n    assert x[0, 1].j == 1\n\n    assert MatrixElement(x, 1, 1).doit() == x[1, 1]\n    assert MatrixElement(x, 1, 1).doit(deep=False) == x[1, 1]\n", "def test_MatrixElement():\n    i, j = symbols('i j', integer=True)\n    assert MatrixElement(A, i, j).parent == A\n    assert MatrixElement(A, i, j).i == i\n    assert MatrixElement(A, i, j).j == j\n    assert MatrixElement(A, 0, 0).subs(A, B) == MatrixElement(B, 0, 0)\n    raises(TypeError, lambda: MatrixElement(x, 0, 0))\n    raises(TypeError, lambda: MatrixElement(A, x, 0))\n    raises(TypeError, lambda: MatrixElement(A, 0, x))\n", "def test_MatrixElement():\n    # Test that MatrixElement is a commutative op\n    A = MatrixSymbol('A', 2, 2)\n    x, y = symbols('x y')\n    assert A[x, y] * x == x * A[x, y]\n\n    # Test that MatrixElement's parent has correct shape\n    assert A[x, y].parent.shape == (2, 2)\n\n    # Test that MatrixElement can get its value from a matrix\n    A_val = Matrix([[1, 2], [3, 4]])\n    assert A[0, 0].subs(A, A_val) == A_val[0, 0]\n    assert A[1, 1].subs(A, A_val) == A_val[1, 1]\n\n    # Test that MatrixElement can also represent a value from a matrix slice\n    assert A[0, 0].subs(A, A_val) == A_val[0, 0]\n    assert A[1, 1].subs(A, A_val) == A_val[1, 1]\n\n    # Test that MatrixElement will throw a TypeError when called with non-integer arguments\n    raises(TypeError, lambda: A[0.0, 0])\n    raises(TypeError, lambda: A[0, 0.0])\n\n    # Test that MatrixElement will throw an IndexError when indices are out of bounds\n    raises(IndexError, lambda: A[2, 0])\n    raises(IndexError, lambda: A[0, 2])\n", "def test_MatrixElement():\n    A = MatrixSymbol('A', 2, 2)\n    me = MatrixElement(A, 1, 0)\n    assert me.parent == A\n    assert me.i == 1\n    assert me.j == 0\n    assert me.diff(me) == 1\n    assert me.diff(A[1, 1]) == 0\n    assert me.diff(A[1, 0]) == 1\n"], "sample_978": ["def test_bspline_basis():\n    d = 0\n    knots = [1, 2, 3, 4, 5]\n    n = 0\n    x = symbols('x')\n    b = bspline_basis(d, knots, n, x)\n    assert b == Piecewise((1, And(x >= 1, x <= 2)), (0, True))\n", "def test_bspline_basis():\n    d = 0\n    knots = range(5)\n    b0 = bspline_basis(d, knots, 0, x)\n    assert b0 == Piecewise((1, Interval(0, 1).contains(x)), (0, True))\n\n    d = 3\n    b0 = bspline_basis(d, knots, 0, x)\n    assert b0 == Piecewise(\n        (x**3/6, Interval(0, 1).contains(x)),\n        (-x**3/2 + 2*x**2 - 2*x + 2/3, Interval(1, 2).contains(x)),\n        (x**3/2 - 4*x**2 + 10*x - 22/3, Interval(2, 3).contains(x)),\n        (-x**3/6 + 2*x**2 - 8*x + 32/3, Interval(3, 4).contains(x)),\n        (0, True))\n\n    knots = [0, 0, 2, 3, 4]\n    d = 1\n    b0 = bspline_basis(d, knots, 0, x)\n    assert b0 == Piecewise((-x/2 + 1, Interval(0, 2).contains(x)), (0, True))\n", "def test_bspline_basis():\n    d = 0\n    knots = [0, 1, 2, 3]\n    n = 0\n    x_val = 1\n    b0 = bspline_basis(d, knots, n, x)\n    assert b0.subs(x, x_val) == 0\n\n    d = 1\n    knots = [0, 0, 1, 2]\n    n = 0\n    x_val = 0.5\n    b0 = bspline_basis(d, knots, n, x)\n    assert b0.subs(x, x_val) == Rational(1, 2)\n\n    d = 3\n    knots = [0, 0, 0, 0, 1, 2, 3]\n    n = 0\n    x_val = 0.5\n    b0 = bspline_basis(d, knots, n, x)\n    assert b0.subs(x, x_val) == Rational(1, 6)\n", "def test_bspline_basis():\n    d = 0\n    knots = range(5)\n    n = 0\n    result = bspline_basis(d, knots, n, x)\n    expected = Piecewise((1, And(x >= 0, x <= 1)), (0, True))\n    assert result == expected\n\n    d = 1\n    knots = [0, 0, 2, 3, 4]\n    n = 0\n    result = bspline_basis(d, knots, n, x)\n    expected = Piecewise((-x/2 + 1, And(x >= 0, x <= 2)), (0, True))\n    assert result == expected\n\n    d = 3\n    knots = range(5)\n    n = 0\n    result = bspline_basis(d, knots, n, x)\n    expected = Piecewise((x**3/6, And(x >= 0, x <= 1)),\n                         (-x**3/2 + 2*x**2 - 2*x + 2/3, And(x >= 1, x <= 2)),\n                         (x**3/2 - 4*x**2 + 10*x - 22/3, And(x >= 2, x <= 3)),\n                         (-x**3/6 + 2*x**2 - 8*x + 32/3, And(x >= 3, x <= 4)),\n                         (0, True))\n    assert result == expected\n", "def test_bspline_basis():\n    d = 0\n    knots = [0, 1, 2, 3, 4, 5]\n    n = 2\n    spline = bspline_basis(d, knots, n, x)\n    assert spline == Piecewise((1, Interval(2, 3).contains(x)), (0, True))\n"], "sample_980": ["def test__af_parity():\n    p = Permutation([0, 1, 2, 3])\n    assert _af_parity(p.array_form) == 0\n    p = Permutation([3, 2, 0, 1])\n    assert _af_parity(p.array_form) == 1\n    p = Permutation([0, 1, 2])\n    assert _af_parity(p.array_form) == 0\n", "def test_cycle_init():\n    raises(ValueError, lambda: Cycle(-1))\n    raises(ValueError, lambda: Cycle(1, 1))\n    raises(ValueError, lambda: Cycle([1, 1]))\n    raises(ValueError, lambda: Cycle(1, 2)(2, 3, 4, 2))\n    assert Cycle(1, 2).array_form == [0, 2, 1]\n    assert Cycle(5).array_form == [0, 1, 2, 3, 4, 5]\n    assert Cycle(0).array_form == [0]\n    assert Cycle().array_form == []\n", "def test_cycle_init():\n    # Test singletons\n    assert Cycle(1).list() == [0, 1]\n    assert Cycle(1, 2).list(4) == [0, 2, 1, 3]\n    assert Cycle(1, 4, 10).list() == [0, 4, 1, 2, 3, 10, 5, 6, 7, 8, 9]\n    # Empty cycle\n    assert Cycle().list() == []\n    raises(ValueError, lambda: Cycle(-1, 1))\n    raises(ValueError, lambda: Cycle(1, -2))\n    raises(ValueError, lambda: Cycle(1, 1))\n", "def test_cycle():\n    # test the Cycle class\n    assert Cycle(1, 2, 3).list() == [0, 2, 3, 1]\n    assert Cycle(1, 2, 3).list(5) == [0, 2, 3, 1, 4]\n    assert Cycle(1, 2, 3).size == 4\n    assert Cycle(1, 4, 3, 2).list() == [0, 4, 2, 3, 1]\n    assert Cycle(1, 2, 3).full_cyclic_form == [[1, 2, 3]]\n    assert Cycle(1, 2, 3).cyclic_form == [[1, 2, 3]]\n    assert Cycle(1, 2, 3).array_form == [0, 2, 3, 1]\n    assert Cycle(1, 2, 3).support() == [1, 2, 3]\n    assert Cycle(1, 2, 3).length() == 3\n    assert Cycle(1, 2, 3).min() == 1\n    assert Cycle(1, 2, 3).max() == 3\n    assert Cycle(1, 2, 3).ascents() == []\n    assert Cycle(1, 2, 3).descents() == [0, 1]\n    assert Cycle(1, 2, 3).inversions() == 3\n    assert Cycle(1, 2, 3).signature() == -1\n    assert Cycle(1, 2, 3).rank() == 5\n", "def test_cycle_new():\n    # Test creating Cycle instances\n    assert Cycle(1, 2).list() == [0, 2, 1]\n    assert Cycle(4)(1, 2).list() == [0, 2, 1, 3, 4]\n    assert Cycle((1, 2)).list() == [0, 2, 1]\n\n    # Test Cycle instance with list\n    p = Permutation([0, 2, 1])\n    assert Cycle(p).list() == [0, 2, 1]\n\n    # Test Cycle instance with singletons\n    assert Cycle(4)(1, 2).list() == [0, 2, 1, 3, 4]\n    assert Cycle(1, 2)(3).list() == [0, 2, 1, 3]\n\n    # Test Cycle instance with invalid input\n    raises(ValueError, lambda: Cycle(1, 1))\n    raises(ValueError, lambda: Cycle(1, -2))\n    raises(ValueError, lambda: Cycle(-1, 2))\n"], "sample_981": ["def test__af_rmul():\n    # test rmul\n    assert _af_rmul([1, 0, 2], [2, 0, 1]) == [1, 2, 0]\n    assert _af_rmul([1, 2, 0], [2, 0, 1]) == [2, 0, 1]\n    # test rmuln\n    assert _af_rmuln([1, 0, 2], [2, 0, 1]) == _af_rmul([1, 0, 2], [2, 0, 1])\n    assert _af_rmuln([1, 2, 0], [2, 0, 1]) == _af_rmul([1, 2, 0], [2, 0, 1])\n    assert _af_rmuln([1, 2, 0], [2, 0, 1], [0, 1, 2]) == _af_rmul(_af_rmul([1, 2, 0], [2, 0, 1]), [0, 1, 2])\n", "def test_af_invert():\n    # _af_invert docstring\n    assert _af_invert([1, 2, 0, 3]) == [2, 0, 1, 3]\n    # other\n    assert _af_invert([]) == []\n    assert _af_invert([0, 1]) == [0, 1]\n    assert _af_invert([1, 0]) == [1, 0]\n    assert _af_invert([0, 1, 2]) == [0, 1, 2]\n    assert _af_invert([0, 2, 1]) == [0, 2, 1]\n    assert _af_invert([1, 0, 2]) == [1, 0, 2]\n    assert _af_invert([1, 2, 0]) == [2, 0, 1]\n    assert _af_invert([2, 0, 1]) == [1, 2, 0]\n    assert _af_invert([2, 1, 0]) == [2, 1, 0]\n", "compilation error", "compilation error", "def test_cycle_init():\n    c = Cycle()\n    assert c.size == 0\n    c = Cycle(1, 2)\n    assert c.size == 3\n    raises(ValueError, lambda: Cycle(-1, 2))\n    raises(ValueError, lambda: Cycle(2, 2))\n"], "sample_982": ["def test_factorint_smooth():\n    # 16843009 has smoothness 1787\n    assert factorint(16843009*1009, limit=1800) == {257: 1, 1009: 1, 17**2*13: 1}\n    # check if we can find 1787 although the limit is near the smoothness\n    assert factorint(16843009*1009, limit=1786) == {257: 1, 1009: 1, 17**2*13*1787: 1}\n    assert factorint(16843009*1009, limit=1785) == {257: 1, 1009: 1, 17**2*13*1787: 1}\n", "def test_smoothness():\n    assert smoothness(2**7*3**2) == (3, 128)\n    assert smoothness(2**4*13) == (13, 16)\n    assert smoothness(2) == (2, 2)\n    assert smoothness(1) == (1, 1)\n    assert smoothness(4) == (2, 4)\n    assert smoothness(9) == (3, 9)\n    assert smoothness(15) == (5, 15)\n    assert smoothness(16) == (2, 16)\n    assert smoothness(25) == (5, 25)\n    assert smoothness(27) == (3, 27)\n", "def test_core():\n    assert core(24, 2) == 6\n    assert core(9424, 3) == 1178\n    assert core(379238) == 379238\n    assert core(15**11, 10) == 15\n    raises(ValueError, lambda: core(-4))\n    raises(ValueError, lambda: core(4, 1))\n", "def test_small_trailing():\n    for i in range(256):\n        if i:\n            assert small_trailing[i] == max(int(not i % 2**j) and j for j in range(1, 8))\n        else:\n            assert small_trailing[i] == 0\n", "def test_factorint_visual():\n    assert factorint(10, visual=True) == 2*5\n    assert factorint(2**2*3**3, visual=True) == 2**2 * 3**3\n    assert factorint(2**2*3**3*5, visual=True) == 2**2 * 3**3 * 5\n    assert factorint(2**2*3**3*5, limit=2, visual=True) == 2**2 * 3**3 * 5\n"], "sample_984": ["def test_print_Symbol():\n    assert sstr(x) == 'x'\n    assert sstr(x, sympy_integers=True) == 'x'\n    assert sstr(Symbol('x', integer=True)) == 'x'\n    assert sstr(Symbol('x', integer=True), sympy_integers=True) == 'x'\n    assert sstr(MatrixSymbol('x', 2, 2)) == 'x'\n", "def test_StrPrinter_print_Derivative():\n    p = StrPrinter()\n    f = Function('f')\n    g = Function('g')\n    assert p._print(Derivative(f(x), x)) == \"Derivative(f(x), x)\"\n    assert p._print(Derivative(f(x, y), x, 2, y)) == \"Derivative(f(x, y), x, x, y)\"\n    assert p._print(Derivative(g(x)*f(x), x)) == \"Derivative(f(x)*g(x), x)\"\n", "def test_print_ComplexRootOf():\n    assert sstr(rootof(x**3 + 2*x + 1, 0)) == \"CRootOf(x**3 + 2*x + 1, 0)\"\n    assert sstr(rootof(x**3 + 2*x + 1, 1)) == \"CRootOf(x**3 + 2*x + 1, 1)\"\n    assert sstr(rootof(x**3 + 2*x + 1, 2)) == \"CRootOf(x**3 + 2*x + 1, 2)\"\n", "def test_print_Sample():\n    from sympy.statistics.distributions import Sample\n    sp = StrPrinter({'order': 'lex'})\n    assert sp._print(Sample([x, y, z])) == \"Sample([x, y, z])\"\n", "def test_sstrrepr():\n    assert sstrrepr(\"x\") == \"'x'\"\n    assert sstrrepr(x) == \"x\"\n    assert sstrrepr(x**2) == \"x**2\"\n    assert sstrrepr(1.0) == \"1.0\"\n    assert sstrrepr([1, 2, 3]) == \"[1, 2, 3]\"\n    assert sstrrepr((1, 2, 3)) == \"(1, 2, 3)\"\n    assert sstrrepr({1: 2, 3: 4}) == \"{1: 2, 3: 4}\"\n"], "sample_983": ["def test_sparse_copy():\n    M = SparseMatrix(3, 3, {(0, 1): 1, (1, 2): 2, (0, 0): -1})\n    N = M.copy()\n    assert M == N\n    N[0, 0] = 1\n    assert M != N\n    assert M.is_SparseMatrix\n    assert N.is_SparseMatrix\n", "def test_sparse_row_list():\n    a = SparseMatrix(((1, 2), (3, 4)))\n    assert a.row_list() == [(0, 0, 1), (0, 1, 2), (1, 0, 3), (1, 1, 4)]\n    assert a.CL == [(0, 0, 1), (1, 0, 3), (0, 1, 2), (1, 1, 4)]\n    a = SparseMatrix(3, 3, {})\n    assert a.row_list() == []\n    assert a.CL == []\n", "def test_sparse_matrix_row_list():\n    S = SparseMatrix([\n        [1, 0, 3, 2],\n        [0, 0, 1, 0],\n        [4, 0, 0, 5],\n        [0, 6, 7, 0]])\n    assert S.row_list() == [\n        (0, 0, 1),\n        (0, 2, 3),\n        (0, 3, 2),\n        (1, 2, 1),\n        (2, 0, 4),\n        (2, 3, 5),\n        (3, 1, 6),\n        (3, 2, 7)]\n", "def test_sparse_cholesky_solve():\n    A = SparseMatrix([[4, 12, -16], [12, 37, -43], [-16, -43, 98]])\n    b = SparseMatrix([[10], [25], [35]])\n    x = A._cholesky_solve(b)\n\n    assert A*x == b\n", "def test_sparse_matrix_col_op():\n    M = SparseMatrix(3, 3, {(0, 0): 1, (1, 1): 2, (2, 2): 3})\n    M.col_op(0, lambda v, i: v + 1)\n    assert M == SparseMatrix(3, 3, {(0, 0): 2, (1, 1): 2, (2, 2): 3})\n"], "sample_985": ["def test_max_min():\n    x, y = symbols('x y')\n    assert Max(x, y) == Max(y, x)\n    assert Min(x, y) == Min(y, x)\n    assert Max(x, -2).subs(x, 3) == 3\n    assert Min(x, -2).subs(x, 3) == -2\n    assert Max(1, x, oo) == oo\n    assert Min(1, x, -oo) == -oo\n    p = symbols('p', positive=True)\n    n = symbols('n', negative=True)\n    assert Max(p, -3) == p\n    assert Min(n, -7) == n\n    raises(ValueError, lambda: Max(x, I))\n    raises(ValueError, lambda: Min(x, I))\n", "def test_MinMaxBase():\n    class MockMinMaxBase(MinMaxBase):\n        zero = S.Infinity\n        identity = S.NegativeInfinity\n\n    assert MockMinMaxBase._is_connected(1, 2) == Max\n    assert MockMinMaxBase._is_connected(-1, 1) == True\n    assert MockMinMaxBase._is_connected(-1, 0) == True\n    assert MockMinMaxBase._is_connected(1, 1) == True\n    assert MockMinMaxBase._is_connected(1, -1) == Min\n\n    args = [1, 2, 3]\n    assert MockMinMaxBase._new_args_filter(args) == [1, 2, 3]\n\n    args = [1, MockMinMaxBase(2, 3), 4]\n    assert list(MockMinMaxBase._new_args_filter(args)) == [1, 2, 3, 4]\n\n    args = [1, 2, 3]\n    assert MockMinMaxBase._find_localzeros(args) == {1, 2, 3}\n\n    args = [1, 1, 1]\n    assert MockMinMaxBase._find_localzeros(args) == {1}\n\n    args = [1, MockMinMaxBase(1, 1), 1]\n    assert MockMinMaxBase._find_localzeros(args) == {1}\n", "def test_minmax():\n    x, y = symbols('x y')\n    assert Max(x, y) == Max(y, x)\n    assert Min(x, y) == Min(y, x)\n    assert Max(x, -2).subs(x, 3) == 3\n    assert Min(x, -2).subs(x, 3) == -2\n    assert Max(x, y).fdiff(1) == Piecewise((1, y < x), (0, True))\n    assert Min(x, y).fdiff(1) == Piecewise((1, x < y), (0, True))\n    assert Max(sin(x), cos(x)).rewrite(Heaviside) == sin(x)*Heaviside(sin(x) - cos(x)) + cos(x)*Heaviside(-sin(x) + cos(x))\n    assert Min(sin(x), cos(x)).rewrite(Heaviside) == sin(x)*Heaviside(cos(x) - sin(x)) + cos(x)*Heaviside(sin(x) - cos(x))\n    assert Max(sin(x), cos(x)).rewrite(Piecewise) == Piecewise((sin(x), sin(x) >= cos(x)), (cos(x), True))\n    assert Min(sin(x), cos(x)).rewrite(Piecewise) == Piecewise((sin(x), sin(x) <= cos(x)), (cos(x), True))\n", "def test_MinMaxBase():\n    # test for issue 14134\n    assert Max(I, -I) == Max(I, -I)\n    assert Min(I, -I) == Min(I, -I)\n    assert Max(-I, I) == Max(-I, I)\n    assert Min(-I, I) == Min(-I, I)\n\n    # test for issue 14089\n    assert Max(-oo, I).is_real is False\n    assert Max(-oo, -I).is_real is False\n    assert Min(oo, I).is_real is False\n    assert Min(oo, -I).is_real is False\n\n    assert Max(1, I).is_real is False\n    assert Max(1, -I).is_real is False\n    assert Min(1, I).is_real is False\n    assert Min(1, -I).is_real is False\n\n    assert Max(1, I, -I) == 1\n    assert Min(1, I, -I) == 1\n", "def test_MinMax():\n    x, y, z = symbols('x y z')\n    assert Max(x, y) == Max(y, x)\n    assert Min(x, y) == Min(y, x)\n    assert Max(x, Max(y, z)) == Max(x, y, z)\n    assert Min(x, Min(y, z)) == Min(x, y, z)\n    assert Max(x, -2).subs(x, 3) == 3\n    assert Min(x, -2).subs(x, 3) == -2\n    p = symbols('p', positive=True)\n    n = symbols('n', negative=True)\n    assert Max(p, -2) == p\n    assert Min(n, -2) == n\n    assert Max(x, y).fdiff(1) == Heaviside(x - y)\n    assert Min(x, y).fdiff(1) == Heaviside(y - x)\n"], "sample_986": ["def test_scaled_zero():\n    assert scaled_zero(100) == (([0], 1, 100, 1), -1)\n    assert scaled_zero(-100) == (([1], 1, 100, 1), -1)\n    assert scaled_zero(scaled_zero(100)) == (0, 1, 100, 1)\n    assert scaled_zero(scaled_zero(-100)) == (1, 1, 100, 1)\n    raises(ValueError, lambda: scaled_zero(3.14))\n", "def test_scaled_zero():\n    z, p = scaled_zero(100)\n    assert z[0][0] == 0\n    z, p = scaled_zero(100, -1)\n    assert z[0][0] == 1\n    z, p = scaled_zero(z)\n    assert z[0] == 1\n    raises(ValueError, lambda: scaled_zero(1.0))\n    raises(ValueError, lambda: scaled_zero(z, 2))\n", "def test_scaled_zero():\n    assert scaled_zero(100) == (([0], 1, 100, 1), -1)\n    assert scaled_zero(scaled_zero(100)[0]) == (0, 1, 100, 1)\n    assert scaled_zero(scaled_zero(100)[0], -1) == (0, -1, 100, 1)\n    assert scaled_zero(scaled_zero(100, -1)[0]) == (0, -1, 100, 1)\n    raises(ValueError, lambda: scaled_zero(1.0))\n", "def test_evalf_add():\n    assert NS(1 + 2) == '3.0'\n    assert NS(S(1)/7 + S(1)/14) == '0.238095238095238'\n    assert NS(Rational(1, 7) + Rational(1, 14)) == '0.238095238095238'\n    assert NS(S(1)/7 + x/14, subs={x: 2}) == '0.238095238095238'\n    assert NS(S(1)/7 + x/14, subs={x: S(2)}) == '0.238095238095238'\n    assert NS(S(1)/7 + x/14, subs={x: Rational(2, 1)}) == '0.238095238095238'\n", "def test_evalf_hypsum():\n    from sympy.abc import n\n    # test simple hypergeometric sums\n    assert NS(Sum(factorial(n)/n**(n+1), (n, 1, oo))) == NS(exp(1))\n    assert NS(Sum(1/factorial(n), (n, 1, oo))) == NS(exp(1)-1)\n    assert NS(Sum(1/factorial(2*n), (n, 1, oo))) == NS(exp(1)/2-1/2)\n    assert NS(Sum(factorial(n)/factorial(2*n), (n, 1, oo))) == NS(sqrt(exp(1)/2)*exp(1)/2 - sqrt(exp(1)/2)/2)\n\n    # test geometric series\n    assert NS(Sum(Rational(1, 2)**n, (n, 1, oo))) == NS(Rational(1, 1-1/2)-1)\n    assert NS(Sum(2**n, (n, 1, oo))) == 'zoo'\n    assert NS(Sum(.5**n, (n, 1, oo))) == NS(1/(1-0.5)-1)\n\n    # test a divergent alternating series\n    assert NS(Sum((-1)**n * n, (n, 1, oo))) == 'zoo'\n\n    # test a simple alternating series\n    assert NS(Sum((-1)**n / n, (n, 1, oo))) == NS(-log(2))\n\n    # test a series with non-integer powers\n    assert NS(Sum(1/(n**n + n**(n+S(1)/2)), (n, 2, oo))) == '0.75765'\n\n    # test a series that should fail because no closed-form expression\n    # is known\n    raises(NotImplementedError, lambda: NS(Sum(1/(n**n + n**(n+S(2)/3)), (n, 2, oo))))\n\n    raises(NotImplementedError, lambda: NS(Sum(1/(n**2 + n*S(1)/2), (n, 1, oo))))\n"], "sample_987": ["def test_scaled_zero():\n    assert scaled_zero(100) == (([0], 1, 100, 1), -1)\n    assert scaled_zero(-100) == (([1], 1, 100, 1), -1)\n    assert scaled_zero(scaled_zero(100)[0]) == (0, 1, 100, 1)\n    assert scaled_zero(scaled_zero(-100)[0]) == (1, 1, 100, 1)\n    raises(ValueError, lambda: scaled_zero(0.5))\n", "def test_evalf_get_complex_part():\n    a, b = symbols('a b', real=True)\n    assert get_complex_part(a + b*I, 0, 10, {}) == (a, None, 10, None)\n    assert get_complex_part(a + b*I, 1, 10, {}) == (b, None, 10, None)\n", "def test_evalf_get_integer_part():\n    assert get_integer_part(2.7, 1, {}, return_ints=True) == (2, 0)\n    assert get_integer_part(-2.7, 1, {}, return_ints=True) == (-3, 0)\n    assert get_integer_part(2.7 + 3*I, 1, {}, return_ints=True) == (2, 3)\n    assert get_integer_part(-2.7 - 3*I, 1, {}, return_ints=True) == (-3, -3)\n    assert get_integer_part(E, 1, {}) == (2, None, None, None)\n    assert get_integer_part(E + I, 1, {}) == (2, 1, None, None)\n", "def test_evalf_mul():\n    assert NS(pi * E * Rational(1,3)) == NS(pi * E / 3)\n    assert NS(pi * E * Rational(-1,3)) == NS(-pi * E / 3)\n    assert NS(pi * E * Rational(1,3), 10) == NS(pi * E / 3, 10)\n    assert NS(pi * E * Rational(-1,3), 10) == NS(-pi * E / 3, 10)\n", "def test_evalf_get_integer_part():\n    assert get_integer_part(123, 1, {}, return_ints=True) == (123, 0)\n    assert get_integer_part(123, -1, {}, return_ints=True) == (123, 0)\n    assert get_integer_part(123.456, 1, {}, return_ints=True) == (123, 0)\n    assert get_integer_part(123.456, -1, {}, return_ints=True) == (123, 0)\n    assert get_integer_part(123.456 + 123.456*I, 1, {}, return_ints=True) == (123, 123)\n    assert get_integer_part(123.456 + 123.456*I, -1, {}, return_ints=True) == (123, 123)\n"], "sample_988": ["def test_rewriting():\n    # Issue 21286\n    assert S.Exp1.rewrite(sin) == sin(I + S.Pi/2) - I*sin(I)\n    assert S.Exp1.rewrite(cos) == cos(I) + I*cos(I + S.Pi/2)\n", "def test_number_update_hash():\n    # Test integer hash\n    assert hash(S.One) != 0\n    assert hash(S.Two) != 0\n    assert hash(S.One) != hash(S.Two)\n\n    # Test float hash\n    assert hash(Float(1.1)) != 0\n    assert hash(Float(2.2)) != 0\n    assert hash(Float(1.1)) != hash(Float(2.2))\n\n    # Test rational hash\n    assert hash(Rational(1, 2)) != 0\n    assert hash(Rational(2, 3)) != 0\n    assert hash(Rational(1, 2)) != hash(Rational(2, 3))\n", "def test_issue_16428():\n    a = Float('1.0')\n    b = Float('nan')\n    assert comp(a, b) is False\n    assert comp(b, a) is False\n    assert comp(b, b) is False\n    assert comp(b, 3) is False\n    assert comp(3, b) is False\n", "def test_eval_power():\n    x = Symbol('x')\n    assert (1/S.Zero)**3 is S.ComplexInfinity\n    assert (1/S.Zero)**-3 is S.Zero\n    assert (1/S.Zero)**x is S.ComplexInfinity\n    assert S.Zero**3 is S.Zero\n    assert S.Zero**-3 is S.ComplexInfinity\n    assert S.Zero**x is S.Zero\n    assert S.One**3 is S.One\n    assert S.One**-3 is S.One\n    assert S.One**x is S.One\n    assert (-S.One)**3 is S.NegativeOne\n    assert (-S.One)**-3 is S.NegativeOne\n    assert (-S.One)**x is (-S.One)**x\n", "def test_number_construction():\n    assert S(1) == 1\n    assert S(1.0) == 1.0\n    assert S(1e1000) == Float('1e1000')\n    assert S(1 + 2j) == 1 + 2*I\n    assert S(1, evaluate=False) == Rational(1, 1)\n    assert S(1.0, evaluate=False) == Rational(10, 10)\n    assert S(1e1000, evaluate=False) == Rational(10**1000, 1)\n    assert S(1 + 2j, evaluate=False) == Rational(1, 1) + Rational(2, 1)*I\n\n    assert Number(1) == 1\n    assert Number(1.0) == 1.0\n    assert Number(1e1000) == Float('1e1000')\n    assert Number(1 + 2j) == 1 + 2*I\n\n    assert Rational(1, 2) == Rational(1, 2)\n    assert Rational(1, 2, evaluate=False) == Rational(1, 2)\n    assert Integer(1) == 1\n    assert Integer(1, evaluate=False) == 1\n"], "sample_989": ["def test_mpf_norm():\n    assert mpf_norm((0, 1, 0, 1), 53) == (0, 1, 0, 1)\n    assert mpf_norm((1, 1, 1, 1), 53) == (1, 1, 1, 1)\n    assert mpf_norm((0, 2**50, -50, 53), 53) == (0, 1, 0, 3)\n", "def test_mpf_norm():\n    assert mpf_norm((1, 10000000, -9, 30), 10) == (1, 10, 0, 1)\n    assert mpf_norm((1, 125, -6, 37), 10) == (1, 5, -4, 4)\n    assert mpf_norm((1, 0, 0, 10), 10) == (0, 0, 0, 0)\n    assert mpf_norm((1, 0, 0, 10), 20) == (0, 0, 0, 0)\n", "def test_Rational_caching():\n    # ensure that the Rational caching mechanism doesn't cause\n    # unexpected behavior when modifying a Rational instance\n    r1 = Rational(1, 2)\n    r2 = Rational(1, 2)\n    assert r1 is r2\n    r1.p = 3\n    assert r1 == Rational(3, 2)\n    assert r2 == Rational(1, 2)\n", "def test_mod_inverse():\n    assert mod_inverse(3, 11) == 4\n    assert mod_inverse(-3, 11) == -4\n    assert mod_inverse(3, -11) == 4\n    assert mod_inverse(2, 4) is None\n    assert mod_inverse(S(2)/7, S(5)/2) == S(7)/2\n    assert mod_inverse(3, 11) == 4\n    assert mod_inverse(2, 3) == 2\n    assert mod_inverse(3, 2) is None\n    assert mod_inverse(1, 2) == 1\n    assert mod_inverse(0, 2) is None\n    raises(ValueError, lambda: mod_inverse(2, 0))\n    raises(TypeError, lambda: mod_inverse(2, t))\n", "def test_Float_round():\n    assert Float(3.456).round(1) == 3.5\n    assert Float(3.456).round(2) == 3.46\n    assert Float(3.456).round(0) == 3\n    assert Float(3.456).round() == 3\n    assert Float(3.456).round(-1) == 0\n"], "sample_990": ["def test_issue_18771():\n    # test if hyperbolic functions can handle large inputs\n    x = Symbol('x')\n    assert sinh(10000).evalf() == sinh(10000).evalf()\n    assert cosh(10000).evalf() == cosh(10000).evalf()\n    assert tanh(10000).evalf() == tanh(10000).evalf()\n    assert coth(10000).evalf() == coth(10000).evalf()\n    assert asinh(10000).evalf() == log(10000 + sqrt(10000**2 + 1)).evalf()\n    assert acosh(10000).evalf() == log(10000 + sqrt(10000 - 1)*sqrt(10000 + 1)).evalf()\n    assert atanh(0.999999).evalf() == log(1999999/999999).evalf()/2\n    assert acoth(10000).evalf() == log(10001/9999).evalf()/2\n    assert asech(0.00001).evalf() == log(2/0.00001 + sqrt(1/0.00001 - 1)*sqrt(1/0.00001 + 1)).evalf()\n    assert acsch(10000).evalf() == log(1/10000 + sqrt(1/10000**2 + 1)).evalf()\n    assert sech(10000).evalf() == sech(10000).evalf()\n    assert csch(10000).evalf() == csch(10000).evalf()\n", "def test_hyperbolic_functions():\n    x = Symbol('x')\n    y = Symbol('y')\n    z = Symbol('z')\n\n    assert sinh(x).diff(x) == cosh(x)\n    assert cosh(x).diff(x) == sinh(x)\n    assert tanh(x).diff(x) == 1 - tanh(x)**2\n    assert coth(x).diff(x) == -1/sinh(x)**2\n\n    assert asinh(x).diff(x) == 1/sqrt(1 + x**2)\n    assert acosh(x).diff(x) == 1/sqrt(x - 1)/sqrt(x + 1)\n    assert atanh(x).diff(x) == 1/(1 - x**2)\n    assert acoth(x).diff(x) == 1/(1 - x**2)\n\n    assert asech(x).diff(x) == -1/x/sqrt(1 - x**2)\n    assert acsch(x).diff(x) == -1/x**2/sqrt(1 + 1/x**2)\n", "def test_issue_23115():\n    x = Symbol('x')\n    assert sinh(x).fdiff(x) == cosh(x)\n    assert cosh(x).fdiff(x) == sinh(x)\n    assert tanh(x).fdiff(x) == -tanh(x)**2 + 1\n    assert coth(x).fdiff(x) == -1/sinh(x)**2\n    assert asinh(x).fdiff(x) == 1/sqrt(x**2 + 1)\n    assert acosh(x).fdiff(x) == 1/sqrt(x**2 - 1)\n    assert atanh(x).fdiff(x) == 1/(1 - x**2)\n    assert acoth(x).fdiff(x) == 1/(1 - x**2)\n    assert asech(x).fdiff(x) == -1/(x*sqrt(1 - x**2))\n    assert acsch(x).fdiff(x) == -1/(x**2*sqrt(1 + x**(-2)))\n", "def test_hyperbolic_eval():\n    x = Symbol('x')\n    assert sinh(-x) == -sinh(x)\n    assert cosh(-x) == cosh(x)\n    assert tanh(-x) == -tanh(x)\n    assert coth(-x) == -coth(x)\n    assert sech(-x) == sech(x)\n    assert csch(-x) == -csch(x)\n    assert asinh(-x) == -asinh(x)\n    assert acosh(-x) == acosh(x) + I*pi\n    assert atanh(-x) == -atanh(x)\n    assert acoth(-x) == -acoth(x)\n    assert asech(-x) == asech(x) + I*pi\n    assert acsch(-x) == -acsch(x)\n", "def test_issue_18762():\n    x = symbols('x')\n    assert csch(x).diff(x) == -coth(x)*csch(x)\n    assert asech(x).diff(x) == -1/(x*sqrt(-x**2 + 1))\n    assert acsch(x).diff(x) == -1/(x**2*sqrt(1 + x**(-2)))\n"], "sample_992": ["def test_PythonCodePrinter_print_Piecewise():\n    p = PythonCodePrinter()\n    expr = Piecewise((x, x < 1), (x**2, x >= 1))\n    result = p._print_Piecewise(expr)\n    assert result == '((x) if (x < 1) else (x**2))'\n", "def test_SciPyPrinter_SparseMatrix():\n    M = SparseMatrix(4, 4, {(1, 1): x, (3, 3): y, (2, 1): z})\n    assert SciPyPrinter().doprint(M) == 'scipy.sparse.coo_matrix([x, z, y], ([1, 2, 3], [1, 1, 3]), shape=(4, 4))'\n", "def test_PYTHON_print_Piecewise_with_numbers():\n    expr = Piecewise(\n        (1, x > 0),\n        (2, x < 0),\n        (3, True)\n    )\n    assert PythonCodePrinter().doprint(expr) == \"\"\"((1) if (x > 0) else ((2) if (x < 0) else (3)))\"\"\"\n", "def test_PythonCodePrinter_print_Piecewise():\n    expr = Expr._from_meth('Piecewise')((x, x < y), (y, True))\n    assert pycode(expr) == '((y if (x < y) else (x)) if (y < x) else (y))'\n    assert PythonCodePrinter().doprint(expr) == '((y if (x < y) else (x)) if (y < x) else (y))'\n", "def test_python_codePrinter_reserved_words():\n    F = PythonCodePrinter({'fully_qualified_modules': False})\n    assert F.doprint(abs(x)) == 'abs(x)'\n    assert F.doprint(acos(x)) == 'math.acos(x)'\n    assert F.doprint(pi) == 'math.pi'\n"], "sample_991": ["def test_Product_reverse_order():\n    P = Product(x, (x, a, b))\n    Pr = P.reverse_order(x)\n    assert Pr == Product(1/x, (x, b + 1, a - 1))\n    Pr = Pr.doit()\n    assert simplify(Pr) == simplify(1/rf(b + 1, a - b - 1))\n", "def test_product_empty():\n    assert Product(k, (k, n, n-1)).doit() == 1\n    assert Product(k, (k, n, n)).doit() == n\n    assert Product(k, (k, n+1, n)).doit() == 1/n\n", "def test_product_change_limits():\n    # Test reversing limits\n    i = Symbol('i', integer=True)\n    a = Symbol('a', integer=True)\n    b = Symbol('b', integer=True)\n    p = Product(i, (i, a, b))\n    assert p.reverse_order(i) == Product(1/i, (i, b + 1, a - 1))\n    # Test index counting notation\n    x = Symbol('x', integer=True)\n    s = Sum(x, (x, a, b))\n    assert s.reverse_order(0) == Sum(-x, (x, b + 1, a - 1))\n", "def test_product_convergence():\n    assert Product(n/(n + 1), (n, 1, oo)).is_convergent() is False\n    assert Product(1/n**2, (n, 1, oo)).is_convergent() is False\n    assert Product(cos(pi/n), (n, 1, oo)).is_convergent() is True\n    assert Product(exp(-n**2), (n, 1, oo)).is_convergent() is False\n    raises(NotImplementedError, lambda: Product(f(n), (n, 1, oo)).is_convergent())\n", "def test_Product_doit():\n    assert Product(k, (k, 1, n)).doit() == factorial(n)\n    assert Product(k**2, (k, 1, n)).doit() == factorial(n)**2\n    assert Product(2*k/(2*k - 1) * 2*k/(2*k + 1), (k, 1, n)).doit() != 0  # cannot compute\n    assert Product(1/k, (k, 6, 1)).doit() == 120\n    assert Product(k, (k, 2, 5)).doit() == 120\n    assert Product(k, (k, n, n-1)).doit() == 1\n"], "sample_993": ["def test_FreeGroupElement_array_form():\n    assert x.array_form == ((x, 1),)\n    assert (x**2*y).array_form == ((x, 2), (y, 1))\n    assert (x**-2*y**-1).array_form == ((x, -2), (y, -1))\n", "def test_free_group_element_len():\n    assert len(x) == 1\n    assert len(y) == 1\n    assert len(z) == 1\n    assert len(x**3) == 3\n    assert len(x*y*z) == 3\n    assert len((x**3*y**2*z).inverse()) == 6\n    assert len(F.identity) == 0\n", "def test_free_group_element_power_of():\n    assert ((x*y)**2).power_of(x*y)\n    assert (x**-3*y**-2*x**3).power_of(x**-3*y*x**3)\n    assert (x**4).power_of(x)\n    assert not (x**3*y**2).power_of(x*y)\n", "def test_order():\n    assert F.order() == oo\n    assert free_group(\"\")[0].order() == 1\n", "def test_contains_generators():\n    w = x**2*y**-1\n    assert w.contains_generators() == {x, y}\n    w = x**3*z\n    assert w.contains_generators() == {x, z}\n"], "sample_994": ["def test_Float_mpf_norm():\n    # Test external mpf tuples are normalized correctly\n    assert Float((1, 100000001, -1, 3)) == Float('-1.0e-1')\n    assert Float((1, 100000001, -1, 3))._mpf_ == (1, 100000001, -1, 5)\n    assert Float((1, 101, 0, 3)) == Float('-101')\n    assert Float((1, 101, 0, 3))._mpf_ == (1, 101, 0, 7)\n    assert Float((0, 1, 1, 2)) == Float('0.5')\n    assert Float((0, 1, 1, 2))._mpf_ == (0, 1, 1, 1)\n    assert Float((1, 1, 1, 2)) == Float('-0.5')\n    assert Float((1, 1, 1, 2))._mpf_ == (1, 1, 1, 1)\n    assert Float((0, 10, -1, 2)) == Float('5')\n    assert Float((0, 10, -1, 2))._mpf_ == (0, 5, 0, 2)\n    assert Float((1, 10, -1, 2)) == Float('-5')\n    assert Float((1, 10, -1, 2))._mpf_ == (1, 5, 0, 2)\n", "def test_float_abs():\n    assert abs(1.1) == 1.1\n    assert abs(-1.1) == 1.1\n    assert abs(-0.0) == 0.0\n    assert abs(Float(1.1)) == 1.1\n    assert abs(Float(-1.1)) == 1.1\n    assert abs(Float(-0.0)) == 0.0\n", "def test_integer_nthroot():\n    assert integer_nthroot(64, 2) == (8, True)\n    assert integer_nthroot(65, 2) == (8, False)\n    assert integer_nthroot(729, 3) == (9, True)\n    assert integer_nthroot(730, 3) == (9, False)\n    assert integer_nthroot(4096, 4) == (8, True)\n    assert integer_nthroot(4097, 4) == (8, False)\n    assert integer_nthroot(32, 5) == (2, True)\n    assert integer_nthroot(33, 5) == (2, False)\n    assert integer_nthroot(1, 2) == (1, True)\n    assert integer_nthroot(0, 2) == (0, True)\n    assert integer_nthroot(-64, 2) == (8, False)\n    assert integer_nthroot(-64, 3) == (-4, True)\n", "def test_comp():\n    assert comp(1, 1) is True\n    assert comp(1, 2) is True\n    assert comp(1, 1e-500) is True\n    assert comp(1, 1.1) is False\n    assert comp(1e-500, 1e-500) is True\n    assert comp(1e-500, 2e-500) is True\n    assert comp(1e-500, 1.1e-500) is False\n    assert comp(1, 1, 1) is True\n    assert comp(1, 2, 1) is False\n    assert comp(1, 1e-500, 1) is False\n    assert comp(1, 1.1, 1) is False\n    assert comp(1e-500, 1e-500, 1e-501) is True\n    assert comp(1e-500, 2e-500, 1e-501) is False\n    assert comp(1e-500, 1.1e-500, 1e-501) is False\n    assert comp(1, '1') is True\n    assert comp(1, '1.0') is True\n    assert comp(1, '1e-500') is True\n    assert comp(1, '1.1') is False\n    assert comp(1, Rational(1, 1)) is True\n    assert comp(1, Rational(2, 1)) is True\n    assert comp(1, Rational(1, 10**500)) is True\n    assert comp(1, Rational(11, 10)) is False\n", "def test_Rational_copy():\n    x = Rational(2, 3)\n    assert x.copy() is x\n    assert x.copy(deep=True) is x\n"], "sample_995": ["def test_float_int_addition():\n    assert same_and_same_prec(Float(2) + 1, Float(3))\n    assert same_and_same_prec(Float(1) + (-2), Float(-1))\n    assert same_and_same_prec(1 + Float(-2), Float(-1))\n    assert same_and_same_prec(1 + Float(2), Float(3))\n    assert same_and_same_prec(Float(1) + 2, Float(3))\n    assert same_and_same_prec(2 + Float(1), Float(3))\n", "def test_mpf_norm():\n    a = mpf_norm((1, 5, 0, 3), 100)\n    b = Float._new(a, 100)\n    assert b == 5\n    assert b._prec == 100\n", "def test_mod_inverse():\n    raises(ValueError, lambda: mod_inverse(0, 2))\n    raises(ValueError, lambda: mod_inverse(2, 0))\n    raises(ValueError, lambda: mod_inverse(2, 4))\n    assert mod_inverse(1, 2) == 1\n    assert mod_inverse(2, 3) == 2\n    assert mod_inverse(5, 7) == 3\n    assert mod_inverse(3, 11) == 4\n    assert mod_inverse(23, 24) == 23\n    assert mod_inverse(1, -2) == 1\n    assert mod_inverse(2, -3) == -1\n    assert mod_inverse(5, -7) == -2\n", "def test_comp():\n    assert comp(0.1, 0.1)\n    assert not comp(0.1, 0.11)\n    assert comp(0.1, 0.11, 1e-1)\n    assert not comp(0.1, 0.11, 1e-2)\n    assert comp('0.1', '0.1')\n    assert not comp('0.1', '0.11')\n    assert comp(Integer(1), Integer(1))\n    assert not comp(Integer(1), Integer(2))\n    assert not comp(0.1, 0.11, '')\n", "def test_Float():\n    assert same_and_same_prec(Float(1.1, 3), Float(1100000000, 12))\n    assert same_and_same_prec(Float(1.1, 3), Float(11, 5))\n    assert same_and_same_prec(Float(1.1, 3), Float(1100000000000000000000, 22))\n"], "sample_996": ["def test_product_is_convergent():\n    assert not Product(n/(n + 1), (n, 1, oo)).is_convergent()\n    assert not Product(1/n**2, (n, 1, oo)).is_convergent()\n    assert Product(cos(pi/n), (n, 1, oo)).is_convergent()\n    assert not Product(exp(-n**2), (n, 1, oo)).is_convergent()\n", "def test_product_log():\n    # Test that the log of a product works as expected\n    i = Symbol('i', integer=True)\n    assert log(product(i, (i, 1, n))).simplify() == Sum(log(i), (i, 1, n)).simplify()\n    assert log(product(i**2, (i, 1, n))).simplify() == Sum(log(i**2), (i, 1, n)).simplify()\n", "def test_product_is_convergent():\n    assert Product(n/(n + 1), (n, 1, oo)).is_convergent() is False\n    assert Product(1/n**2, (n, 1, oo)).is_convergent() is False\n    assert Product(cos(pi/n), (n, 1, oo)).is_convergent() is True\n    assert Product(exp(-n**2), (n, 1, oo)).is_convergent() is False\n    raises(NotImplementedError, lambda: Product(exp(n), (n, 1, oo)).is_convergent())\n", "def test_product_doit():\n    # Test that doit() on Product objects is properly handled\n    expr = Product(k**2, (k, 1, n))\n    result = expr.doit()\n    assert result == factorial(n)**2\n\n    # Test that doit() handles infinite products\n    expr = Product(1/k**2, (k, 1, oo))\n    result = expr.doit()\n    assert result == S.Pi**2 / 6\n\n    # Test that doit() handles symbolic limits\n    expr = Product(k, (k, a, n))\n    result = expr.doit()\n    assert result == rf(a, n - a + 1)\n\n    # Test that doit() handles products with multiple symbols\n    expr = Product(k*m, (k, 1, n), (m, 1, n))\n    result = expr.doit()\n    assert result == factorial(n)**2\n", "def test_product_is_convergent():\n    assert Product(n/(n + 1), (n, 1, oo)).is_convergent() is False\n    assert Product(1/n**2, (n, 1, oo)).is_convergent() is False\n    assert Product(cos(pi/n), (n, 1, oo)).is_convergent() is True\n    assert Product(exp(-n**2), (n, 1, oo)).is_convergent() is False\n    raises(NotImplementedError, lambda: Product(Sum(n/(n + 1), (n, 1, k)), (k, 1, oo)).is_convergent())\n"], "sample_997": ["def test_convert_xor():\n    transformations = standard_transformations + (convert_xor,)\n    assert parse_expr('2^3', transformations=transformations) == 2**3\n    assert parse_expr('2 ^3', transformations=transformations) == 2**3\n    assert parse_expr('2^ 3', transformations=transformations) == 2**3\n    assert parse_expr('2 ^ 3', transformations=transformations) == 2**3\n    assert parse_expr('(2^ 3)^2', transformations=transformations) == ((2**3)**2)\n", "def test_convert_xor():\n    transformations = standard_transformations + (convert_xor,)\n    assert parse_expr('a^b', transformations=transformations) == Symbol('a')**Symbol('b')\n", "def test_factorial_notation():\n    transformations = standard_transformations + (factorial_notation,)\n    assert parse_expr('3!', transformations=transformations) == factorial(3)\n    assert parse_expr('3*2!', transformations=transformations) == 3*factorial(2)\n    assert parse_expr('2!*(2+3)!', transformations=transformations) == factorial(2)*(2+3)!\n    assert parse_expr('3!!', transformations=transformations) == factorial2(3)\n", "def test_factorial_notation():\n    assert parse_expr('3!') == factorial(3)\n    assert parse_expr('factorial(3)') == factorial(3)\n    assert parse_expr('3! + 4!') == factorial(3) + factorial(4)\n    assert parse_expr('factorial2(3)') == factorial2(3)\n    assert parse_expr('3!!') == factorial2(3)\n", "def test_factorial_notation():\n    assert parse_expr('3!') == Integer(6)\n    assert parse_expr('4!') == Integer(24)\n    assert parse_expr('5!') == Integer(120)\n    assert parse_expr('x!') == factorial(Symbol('x'))\n    assert parse_expr('(x + 2)!') == factorial(Symbol('x') + 2)\n    assert parse_expr('x!!') == factorial2(Symbol('x'))\n    raises(TokenError, lambda: parse_expr('x!!!'))\n"], "sample_998": ["def test_latex_tensor_product():\n    a, b, c = symbols('a b c')\n    assert latex(tensorproduct(a, b)) == 'a \\\\otimes b'\n    assert latex(tensorproduct(a, b, c)) == 'a \\\\otimes b \\\\otimes c'\n", "def test_LatexPrinter_with_Config():\n    l = LatexPrinter({'mode': 'equation', 'itex': True})\n    assert l.doprint(1 + 2) == r\"$$1 + 2$$\"\n", "def test_latex_tensorarray():\n    # A random rank 3 tensor of size 2 x 2 x 3\n    array = ImmutableDenseNDimArray(range(12), (2, 2, 3))\n    assert latex(array) == r'\\left[\\begin{matrix}\\left[\\begin{matrix}0 & 1 & 2\\\\3 & 4 & 5\\end{matrix}\\right]\\\\\\left[\\begin{matrix}6 & 7 & 8\\\\9 & 10 & 11\\end{matrix}\\right]\\end{matrix}\\right]'\n    array = ImmutableSparseNDimArray(range(12), (2, 2, 3))\n    assert latex(array) == r'\\left[\\begin{matrix}\\left[\\begin{matrix}0 & 1 & 2\\\\3 & 4 & 5\\end{matrix}\\right]\\\\\\left[\\begin{matrix}6 & 7 & 8\\\\9 & 10 & 11\\end{matrix}\\right]\\end{matrix}\\right]'\n    array = MutableDenseNDimArray(range(12), (2, 2, 3))\n    assert latex(array) == r'\\left[\\begin{matrix}\\left[\\begin{matrix}0 & 1 & 2\\\\3 & 4 & 5\\end{matrix}\\right]\\\\\\left[\\begin{matrix}6 & 7 & 8\\\\9 & 10 & 11\\end{matrix}\\right]\\end{matrix}\\right]'\n    array = MutableSparseNDimArray(range(12), (2, 2, 3))\n    assert latex(array) == r'\\left[\\begin{matrix}\\left[\\begin{matrix}0 & 1 & 2\\\\3 & 4 & 5\\end{matrix}\\right]\\\\\\left[\\begin{matrix}6 & 7 & 8\\\\9 & 10 & 11\\end{matrix}\\right]\\end{matrix}\\right]'\n", "def test_latex_Interval():\n    assert latex(Interval(0, 1)) == r\"\\left[0, 1\\right]\"\n    assert latex(Interval(0, 1, left_open=True)) == r\"\\left(0, 1\\right]\"\n    assert latex(Interval(0, 1, right_open=True)) == r\"\\left[0, 1\\right)\"\n    assert latex(Interval(0, 1, left_open=True, right_open=True)) == r\"\\left(0, 1\\right)\"\n    assert latex(Interval(-oo, 1)) == r\"\\left(- \\infty, 1\\right]\"\n    assert latex(Interval(-oo, 1, left_open=True)) == r\"\\left(- \\infty, 1\\right]\"\n    assert latex(Interval(-oo, 1, right_open=True)) == r\"\\left(- \\infty, 1\\right)\"\n    assert latex(Interval(0, oo)) == r\"\\left[0, \\infty\\right)\"\n    assert latex(Interval(0, oo, left_open=True)) == r\"\\left(0, \\infty\\right)\"\n    assert latex(Interval(0, oo, right_open=True)) == r\"\\left[0, \\infty\\right)\"\n", "def test_MatPow():\n    M = Matrix([[1, 2], [3, 4]])\n    assert latex(M**2) == r'\\left(\\begin{matrix}1 & 2\\\\3 & 4\\end{matrix}\\right)^{2}'\n    M = MatrixSymbol('A', 2, 2)\n    assert latex(M**2) == 'A^{2}'\n"], "sample_999": ["def test_latex_transformations():\n    x = Symbol(\"x\")\n    assert latex(FourierTransform(x, x, t)) == r\"\\mathcal{F}_{x}\\left[x\\right]\\left(t\\right)\"\n    assert latex(InverseFourierTransform(x, t, x)) == r\"\\mathcal{F}^{-1}_{t}\\left[x\\right]\\left(x\\right)\"\n    assert latex(LaplaceTransform(x, t, s)) == r\"\\mathcal{L}_{t}\\left[x\\right]\\left(s\\right)\"\n    assert latex(InverseLaplaceTransform(x, s, t)) == r\"\\mathcal{L}^{-1}_{s}\\left[x\\right]\\left(t\\right)\"\n    assert latex(MellinTransform(x, t, s)) == r\"\\mathcal{M}_{t}\\left[x\\right]\\left(s\\right)\"\n    assert latex(InverseMellinTransform(x, s, t)) == r\"\\mathcal{M}^{-1}_{s}\\left[x\\right]\\left(t\\right)\"\n    assert latex(SineTransform(x, x, t)) == r\"\\mathcal{SIN}_{x}\\left[x\\right]\\left(t\\right)\"\n    assert latex(InverseSineTransform(x, t, x)) == r\"\\mathcal{SIN}^{-1}_{t}\\left[x\\right]\\left(x\\right)\"\n    assert latex(CosineTransform(x, x, t)) == r\"\\mathcal{COS}_{x}\\left[x\\right]\\left(t\\right)\"\n    assert latex(InverseCosineTransform(x, t, x)) == r\"\\mathcal{COS}^{-1}_{t}\\left[x\\right]\\left(x\\right)\"\n", "def test_latex():\n    # Test that latex() returns something non-empty for these classes\n    classes_to_test = [\n        Abs, Chi, Ci, CosineTransform, Dict, Ei, Eq, FallingFactorial,\n        FiniteSet, Float, FourierTransform, Function, IndexedBase, Integral,\n        Interval, InverseCosineTransform, InverseFourierTransform,\n        InverseLaplaceTransform, InverseMellinTransform, InverseSineTransform,\n        Lambda, LaplaceTransform, Limit, Matrix, Max, MellinTransform, Min, Mul,\n        Order, Piecewise, Poly, Pow, Product, Range, Rational, RisingFactorial,\n        rootof, RootSum, S, Shi, Si, SineTransform, Subs, Sum, Symbol, Tuple,\n        Union, Ynm, Znm, arg, asin, assoc_laguerre, assoc_legendre, beta,\n        binomial, catalan, ceiling, chebyshevt, chebyshevu, conjugate, cot,\n        coth, diff, dirichlet_eta, euler, exp, expint, factorial, factorial2,\n        floor, gamma, gegenbauer, hermite, hyper, im, jacobi, laguerre,\n        legendre, lerchphi, log, lowergamma, meijerg, polar_lift, polylog, re,\n        root, sin, sqrt, subfactorial, totient, uppergamma, zeta, And, Or,\n        Xor, Implies, Not, Equivalent, Mod, elliptic_k, elliptic_f, elliptic_e,\n        elliptic_pi, cos, tan, Commutator, Operator, Quaternion,\n        ImmutableDenseNDimArray, ImmutableSparseNDimArray, MutableSparseNDimArray,\n        MutableDenseNDimArray, DiracDelta, Heaviside, KroneckerDelta, LeviCivita,\n        Permutation, Cycle, CoordSys3D, Cross, Curl, Dot, Divergence, Gradient,\n        udivisor_sigma\n    ]\n    for cls in classes_to_test:\n        x = cls(*[S(i) for i in range(3)])\n        assert latex(x)\n\n    # Test some specific output\n    assert latex(2*x**2) == \"2 x^{2}\"\n    assert latex(x**-1) == \"\\\\frac{1}{x}\"\n    assert latex(x**-2) == \"\\\\frac{1", "def test_LatexPrinter_settings():\n    # Test that global settings are passed to the LatexPrinter\n    assert latex(sqrt(x), fold_frac_powers=True) == \"x^{1/2}\"\n    assert latex(sqrt(x), fold_frac_powers=False) == \"x^{\\frac{1}{2}}\"\n    assert latex(2*tau, mul_symbol=\"times\") == \"2 \\\\times \\\\tau\"\n", "def test_print_numpyarray():\n    import numpy as np\n\n    A = np.array([[1, 2], [3, 4]])\n    assert latex(A) == r'\\left[\\begin{matrix}1 & 2\\\\3 & 4\\end{matrix}\\right]'\n\n    A = np.array([1, 2, 3])\n    assert latex(A) == r'\\left[\\begin{matrix}1\\\\2\\\\3\\end{matrix}\\right]'\n", "def testTranslator():\n    x = Symbol('x')\n    assert translate(s=\"x\") == \"x\"\n    assert translate(s=\"xhat\") == \"{\\\\hat{x}}\"\n    assert translate(s=\"xhatdot\") == \"{\\\\dot{\\\\hat{x}}}\"\n    assert translate(s=\"alphahatdotprime\") == \"{\\\\dot{\\\\hat{\\\\alpha}}}'\"\n    assert translate(s=\"xtilde\") == \"{\\\\tilde{x}}\"\n    assert translate(s=\"xbartilde\") == \"{\\\\tilde{\\\\bar{x}}}\"\n    assert translate(s=\"xcheck\") == \"{\\\\check{x}}\"\n    assert translate(s=\"xbreve\") == \"{\\\\breve{x}}\"\n    assert translate(s=\"xacute\") == \"{\\\\acute{x}}\"\n    assert translate(s=\"xgrave\") == \"{\\\\grave{x}}\"\n    assert translate(s=\"xvec\") == \"{\\\\vec{x}}\"\n    assert translate(s=\"xprm\") == \"{x}'\"\n    assert translate(s=\"xbar\") == \"{\\\\bar{x}}\"\n    assert translate(s=\"xi\") == \"\\\\xi\"\n    assert translate(s=\"alpha\") == \"\\\\alpha\"\n"], "sample_1000": ["def test_octave_code_lambda_functions():\n    f = implemented_function(Function('f'), Lambda(x, x**2))\n    g = implemented_function(Function('g'), Lambda(x, y, x*y))\n    assert octave_code(f(x)) == \"f(x)\"\n    assert octave_code(g(x, y)) == \"g(x, y)\"\n", "def test_octave_code_Indexed():\n    from sympy.tensor import IndexedBase, Idx\n    len_y = 5\n    y = IndexedBase('y', shape=(len_y,))\n    t = IndexedBase('t', shape=(len_y,))\n    Dy = IndexedBase('Dy', shape=(len_y-1,))\n    i = Idx('i', len_y-1)\n    e = (y[i+1]-y[i])/(t[i+1]-t[i])\n    assert octave_code(e, assign_to=Dy[i]) == \\\n        'Dy(i) = (y(i + 1) - y(i))./(t(i + 1) - t(i));'\n", "def test_octave_piecewise():\n    pw = Piecewise((x + 1, x > 0), (x, True))\n    with_linebreaks = mcode(pw, assign_to=y, inline=False)\n    assert with_linebreaks == (\n        \"if (x > 0)\\n\"\n        \"  y = x + 1;\\n\"\n        \"else\\n\"\n        \"  y = x;\\n\"\n        \"end\")\n    assert mcode(pw, assign_to=y) == (\n        \"y = ((x > 0).*(x + 1) + (~(x > 0)).*(x));\")\n", "def test_octave_code_Pow():\n    assert octave_code(x**3) == \"x.^3\"\n    assert octave_code(x**-3) == \"1./x.^3\"\n    assert octave_code(x**Rational(1, 3)) == \"x.^(1/3)\"\n    assert octave_code(2**x) == \"2.^x\"\n    assert octave_code(x**y) == \"x.^y\"\n    assert octave_code(S(-1)**x) == \"(-1).^x\"\n    assert octave_code(S.Exp1**x) == \"exp(1).^x\"\n    assert octave_code(S.Pi**x) == \"pi.^x\"\n    assert octave_code(S.Infinity**x) == \"inf.^x\"\n    assert octave_code(S.NegativeInfinity**x) == \"-inf.^x\"\n", "def test_octave_piecewise():\n    pw = Piecewise((x + 1, x > 0), (x, True))\n    assert octave_code(pw) == '((x > 0).*(x + 1) + (~(x > 0)).*(x))'\n    assert octave_code(pw, inline=False) == (\n            \"if (x > 0)\\n\"\n            \"  x + 1\\n\"\n            \"else\\n\"\n            \"  x\\n\"\n            \"end\")\n    pw = Piecewise((x**2, x > 0), (x, x <= 0))\n    assert octave_code(pw, inline=False) == (\n            \"if (x > 0)\\n\"\n            \"  x.^2\\n\"\n            \"elseif (x <= 0)\\n\"\n            \"  x\\n\"\n            \"end\")\n"], "sample_1003": ["def test_Options__init_dependencies_order():\n    # Make sure all options are registered correctly\n    assert len(Options.__order__) == len(Options.__options__)\n    assert set(Options.__order__) == set(Options.__options__.keys())\n\n    # Check for cyclic dependencies\n    for option in Options.__order__:\n        assert option not in Options.__options__[option].after\n", "def test_Composite():\n    assert Composite.default() is None\n    assert Composite.preprocess(True) is True\n    assert Composite.preprocess(False) is False\n    raises(OptionError, lambda: Options((x, y), {'composite': 1}))\n    raises(OptionError, lambda: Composite.postprocess({'domain': ZZ, 'composite': True}))\n    raises(OptionError, lambda: Composite.postprocess({'split': True, 'composite': True}))\n    raises(OptionError, lambda: Composite.postprocess({'gaussian': True, 'composite': True}))\n    raises(OptionError, lambda: Composite.postprocess({'extension': set([I]), 'composite': True}))\n    raises(OptionError, lambda: Composite.postprocess({'modulus': 2, 'composite': True}))\n    raises(OptionError, lambda: Composite.postprocess({'symmetric': True, 'composite': True}))\n", "def test_Options_clone():\n    options = Options((x, y), {'domain': 'ZZ'})\n    clone = options.clone({'domain': 'QQ'})\n\n    assert options['domain'] == ZZ\n    assert clone['domain'] == QQ\n", "def test_BooleanOption_preprocess():\n    assert BooleanOption.preprocess(True) == True\n    assert BooleanOption.preprocess(False) == False\n    raises(OptionError, lambda: BooleanOption.preprocess('True'))\n", "def test_options():\n    opts = Options((x, y, z), {'domain': 'ZZ'})\n\n    assert opts.args == {'domain': ZZ, 'gens': (x, y, z)}\n    assert opts.options == {\n        'auto': False, 'domain': ZZ, 'expand': True, 'extension': None,\n        'field': False, 'formal': False, 'frac': False, 'gaussian': False,\n        'gens': (x, y, z), 'greedy': False, 'modulus': None, 'order': lex,\n        'polys': False, 'sort': [], 'split': False, 'strict': True,\n        'symmetric': True, 'wrt': [], 'include': False, 'all': False, 'gen': 0\n    }\n\n    raises(OptionError, lambda: opts.__init__(((x, y, z), {'domain': 'ZZ', 'gens': (x, y, z)})))\n"], "sample_1001": ["def test_latex_canonicalize_names_with_subscripts():\n    x = Symbol('x_0')\n    assert latex(x) == 'x_{0}'\n", "def test_latex_seq():\n    s1 = SeqPer((1, 2, 3), (0, 2))\n    s2 = SeqFormula(x**2, (0, 4))\n    s3 = SeqAdd(SeqMul((1, 2), (3, 4)), SeqPer((2, 3), (4, 5)))\n    assert latex(s1) == r'\\left(1, \\  2, \\  3\\right)'\n    assert latex(s2) == r'\\left(x^{2}, \\  x^{2}, \\  x^{2}, \\  x^{2}, \\  x^{2}\\right)'\n    assert latex(s3) == r'\\left(3, \\  4, \\  8, \\  10, \\  6, \\  9\\right)'\n    assert latex(fps(1/(1-x))) == r'1 + x + x^{2} + x^{3} + \\ldots'\n", "def test_latex_matrix():\n    M = Matrix([[1, 2], [3, 4]])\n    assert latex(M) == r\"\\left[\\begin{matrix}1 & 2\\\\3 & 4\\end{matrix}\\right]\"\n    assert latex(M.T) == r\"\\left[\\begin{matrix}1 & 3\\\\2 & 4\\end{matrix}\\right]\"\n    assert latex(M.inv()) == r\"\\left[\\begin{matrix}-2 & 1\\\\ \\frac{3}{2} & - \\frac{1}{2}\\end{matrix}\\right]\"\n\n    M = Matrix([[1, 2, 3], [4, 5, 6]])\n    assert latex(M) == r\"\\left[\\begin{matrix}1 & 2 & 3\\\\4 & 5 & 6\\end{matrix}\\right]\"\n    assert latex(M.T) == r\"\\left[\\begin{matrix}1 & 4\\\\2 & 5\\\\3 & 6\\end{matrix}\\right]\"\n    assert latex(M.inv()) == \"Not defined\"\n", "def test_LatexPrinter_settings():\n    # Test that settings are preserved\n    lp = LatexPrinter({'mode': 'inline', 'itex': True})\n    assert lp._settings['mode'] == 'inline'\n    assert lp._settings['itex'] is True\n\n    # Test that new LatexPrinter instances have the default settings\n    new_lp = LatexPrinter()\n    assert new_lp._settings['mode'] == 'plain'\n    assert new_lp._settings['itex'] is False\n", "def test_latex_history():\n    x = Symbol('x')\n    ts = Symbol('_ts')\n    expr = sin(x).subs(x, 2)\n    assert latex(expr, mode='inline') == r'$\\sin{\\left (2 \\right )}$'\n\n    # Will raise an error if this is wrong\n    latex(expr, mode='inline', _ts=ts)\n"], "sample_1002": ["def test_mpf_norm():\n    assert mpf_norm((1, 5, 0, 3), 10) == mpf_norm((1, 5, 0, 3), 10)\n    assert mpf_norm((1, 5, 0, 3), 10) != mpf_norm((1, 5, 0, 2), 10)\n    assert mpf_norm((0, 0, 0, 10), 10) == (0, 0, 0, 0)\n    assert mpf_norm((0, 0, -5, 2), 10) == (0, 0, -5, 0)\n", "def test_integer_nthroot():\n    # Test integer_nthroot for various cases\n    assert integer_nthroot(0, 3) == (0, True)\n    assert integer_nthroot(1, 5) == (1, True)\n    assert integer_nthroot(2**3, 3) == (2, True)\n    assert integer_nthroot(2**3 + 1, 3) == (2, False)\n    assert integer_nthroot(2**53, 53) == (2, True)\n    assert integer_nthroot(2**53 + 1, 53) == (2, False)\n    assert integer_nthroot(3**4, 2) == (9, True)\n    assert integer_nthroot(3**4 + 1, 2) == (9, False)\n    assert integer_nthroot(999999999999999, 3) == (999999999, False)\n    assert integer_nthroot(999999999999999, 4) == (31622, False)\n    assert integer_nthroot(999999999999999, 5) == (2519, False)\n", "def test_is_integer():\n    assert Integer(0).is_integer\n    assert Integer(1).is_integer\n    assert Integer(-1).is_integer\n    assert not Rational(1, 2).is_integer\n    assert not Float(1.2).is_integer\n    assert not pi.is_integer\n", "def test_float_half():\n    assert Float(0.5) != Rational(1, 2)\n    assert Float(0.5) != Integer(1)/2\n    assert Float(0.5) != Integer(1)/Integer(2)\n    assert Float(1)/2 == Float(0.5)\n    assert Rational(1, 2).evalf() == Float(0.5)\n    assert (Rational(1, 2) + 1).evalf() == Float(1.5)\n    assert Float(0.5) + 1 == Float(1.5)\n", "def test_mpf_norm():\n    assert mpf_norm((0, 0, -1, 0), 10) == (0, 0, -1, 0)\n    assert mpf_norm((1, 2, 3, 4), 5) == mpf_norm((1, 2, 3, 4), 5)\n"], "sample_1004": ["def test_ConditionSet_subs_dummy():\n    c = ConditionSet(x, x < 1, {x, z})\n    assert c.subs(x, y) == ConditionSet(x, x < 1, {y, z})\n    assert c.subs(x, y).subs(x, y) == ConditionSet(y, y < 1, {y, z})\n    assert ConditionSet(y, y < 1, {y, z}).subs(y, 1) == ConditionSet(y, y < 1, {z})\n    raises(ValueError, lambda: ConditionSet(x, x < 1).subs(x, 1))\n", "def test_ConditionSet_contains():\n    # Test with an interval\n    cs = ConditionSet(x, x**2 > 4, Interval(2, 4))\n    assert cs.contains(3) == False\n    assert cs.contains(5) == False\n\n    # Test with a finite set\n    cs = ConditionSet(x, x**2 > 4, FiniteSet(1, 2, 3, 4, 5))\n    assert cs.contains(3) == False\n    assert cs.contains(5) == True\n\n    # Test with a union\n    cs = ConditionSet(x, x**2 > 4, Union(Interval(1, 3), Interval(4, 6)))\n    assert cs.contains(2) == False\n    assert cs.contains(5) == True\n\n    # Test with an intersection\n    cs = ConditionSet(x, x**2 > 4, Intersection(Interval(1, 3), Interval(2, 4)))\n    assert cs.contains(2) == False\n    assert cs.contains(5) == False\n", "def test_conditionset_subs():\n    c = ConditionSet(x, x < 1, {x, z})\n    assert c.subs(x, y) == ConditionSet(x, x < 1, {y, z})\n    assert c.subs(x, y).subs(x, y) == ConditionSet(y, y < 1, {y, z})\n    assert ConditionSet(y, y < 1, {y, z}).subs(y, 1) == ConditionSet(y, y < 1, {z})\n    assert ConditionSet(y, y < 1, {y, z}).subs(y, 1).subs(y, 1) == ConditionSet(y, y < 1, {z})\n", "def test_ConditionSet_dummy_symbol():\n    c = ConditionSet(x, x < 1, Interval(0, 2))\n    assert c(sym(Symbol('y'))) == ConditionSet(Symbol('y'), Symbol('y') < 1, Interval(0, 2))\n    assert c(sym(x)) == c\n    raises(ValueError, lambda: ConditionSet(x + 1, x < 1, Interval(0, 2)))\n", "def test_condition_set_free_symbols():\n    # No free symbols\n    assert ConditionSet(x, x < 1, S.Integers).free_symbols == set()\n    # Free symbol in the condition\n    assert ConditionSet(x, x < y, S.Integers).free_symbols == {y}\n    # Free symbol in the base set\n    assert ConditionSet(x, x < 1, Interval(0, y)).free_symbols == {y}\n    # Free symbols in both the condition and the base set\n    assert ConditionSet(x, x < y, Interval(0, z)).free_symbols == {y, z}\n"], "sample_1005": ["def test_latex_Transpose():\n    assert latex(Transpose(x)) == \"x^T\"\n    assert latex(Transpose(Matrix([[1, 2], [3, 4]]))) == \\\n        r\"\\left(\\begin{matrix}1 & 2\\\\3 & 4\\end{matrix}\\right)^T\"\n", "def test_latex_ConditionSet():\n    from sympy import Interval\n    p = ConditionSet((x, y), Eq(x, y), Interval(0, 1))\n    assert latex(p) == r\"\\left\\{x, y \\mid x = y \\wedge x, y \\in \\left[ 0, \\  1\\right] \\right\\}\"\n", "def test_LatexPrinter_settings():\n    x = Symbol('x')\n    lp = LatexPrinter({'order': 'lex'})\n    assert lp._print(x**2 + x) == 'x + x^{2}'\n    lp = LatexPrinter({'order': 'old'})\n    assert lp._print(x**2 + x) == 'x^{2} + x'\n    lp = LatexPrinter({'mode': 'inline'})\n    assert lp._print(x**2 + x) == '$x^{2} + x$'\n    lp = LatexPrinter({'fold_func_brackets': True})\n    assert lp._print(sin(x**2 + x)) == r'\\sin x^{2} + x'\n    lp = LatexPrinter({'inv_trig_style': 'full'})\n    assert lp._print(asin(x)) == r'\\arcsin x'\n    lp = LatexPrinter({'inv_trig_style': 'power'})\n    assert lp._print(asin(x)) == r'\\sin^{-1} x'\n", "def test_LatexPrinter_settings():\n    x = symbols('x')\n    # Test if settings work with nested print commands\n    assert latex(exp(x), ln_notation=True) == r'e^{x}'\n    assert LatexPrinter({'ln_notation': True}).doprint(exp(x)) == r'e^{x}'\n    assert LatexPrinter({'ln_notation': True}).doprint(exp(x), ln_notation=False) == r'e^{x}'\n\n    assert latex(log(x), ln_notation=True) == r'\\ln x'\n    assert LatexPrinter({'ln_notation': True}).doprint(log(x)) == r'\\ln x'\n    assert LatexPrinter({'ln_notation': True}).doprint(log(x), ln_notation=False) == r'\\log x'\n", "def test_latex_real_field():\n    assert latex(Reals) == r\"\\mathbb{R}\"\n"], "sample_1006": ["def test_factorial2():\n    assert factorial2(-1) == 1\n    assert factorial2(0) == 1\n    assert factorial2(5) == 15\n    assert factorial2(6) == 48\n    assert factorial2(-5) == 1/3\n    assert factorial2(-6) == 1/15\n    assert factorial2(Symbol('n', integer=True)) == factorial2(Symbol('n', integer=True))\n    assert factorial2(Poly(x**2, x), 2) == Poly(x**4 - 2*x**3 + x**2, x, domain='ZZ')\n    assert factorial2(oo) == oo\n", "def test_binomial_is_nonnegative():\n    x, y = symbols('x y', integer=True)\n    assert binomial(5, 2).is_nonnegative\n    assert binomial(x, 2).is_nonnegative is None\n    assert binomial(5, y).is_nonnegative is None\n    assert binomial(-5, y).is_nonnegative is None\n    assert binomial(x, y).is_nonnegative is None\n", "def test_factorial2():\n    x = Symbol('x')\n    assert factorial2(0) == 1\n    assert factorial2(1) == 1\n    assert factorial2(2) == 2\n    assert factorial2(3) == 3\n    assert factorial2(4) == 8\n    assert factorial2(5) == 15\n    assert factorial2(6) == 48\n    assert factorial2(-1) == 1\n    assert factorial2(-3) == 1\n    assert factorial2(-5) == 1/3\n    raises(ValueError, lambda: factorial2(-2))\n    raises(ValueError, lambda: factorial2(-4))\n    raises(ValueError, lambda: factorial2(x))\n", "def test_tractable_binomial():\n    n, k = symbols('n k')\n    assert binomial(n, k).rewrite('tractable') == gamma(n + 1) / (gamma(k + 1) * gamma(n - k + 1))\n    assert binomial(n, 2).rewrite('tractable') == (n * (n - 1)) / 2\n    assert binomial(10, 5).rewrite('tractable') == 252\n", "def test_factorial():\n    assert factorial(-1) == zoo\n    assert factorial(0) == 1\n    assert factorial(1) == 1\n    assert factorial(5) == 120\n    assert factorial(S(1)/2) == factorial(S(1)/2)\n    assert factorial(nan) == nan\n    assert factorial(oo) == oo\n    x = symbols('x')\n    assert factorial(x) == factorial(x)\n"], "sample_1007": ["def test_factorial():\n    assert factorial(-1) == zoo\n    assert factorial(0) == 1\n    assert factorial(1) == 1\n    assert factorial(5) == 120\n    assert factorial(nan) == nan\n    assert factorial(oo) == oo\n    x = symbols('x', real=True)\n    assert simplify(factorial(x)) == factorial(x)\n\n    # Check if factorial of non-integer raises error\n    k = symbols('k', integer=False)\n    assert factorial(k) == factorial(k)\n\n    # Check for numerical evaluation of factorial with floating point input\n    assert factorial(3.0) == 6\n    assert factorial(-4.0) == zoo\n\n    # Check for factorial2\n    assert factorial2(1) == 1\n    assert factorial2(5) == 15\n    assert factorial2(-1) == 1\n    assert factorial2(-3) == 1\n    assert factorial2(-5) == 1/3\n\n    # Check for subfactorial\n    assert subfactorial(0) == 1\n    assert subfactorial(1) == 0\n    assert subfactorial(5) == 44\n    assert subfactorial(-1) == S.NaN\n    assert subfactorial(oo) == oo\n    n = symbols('n', integer=True, nonnegative=True)\n    assert subfactorial(n).rewrite(uppergamma) == uppergamma(n+1, -1)/S.Exp1\n", "def test_factorial_is_nonnegative():\n    x = symbols('x', integer=True, nonnegative=True)\n    assert factorial(x).is_nonnegative\n    assert factorial(0).is_nonnegative\n    assert factorial(5).is_nonnegative\n", "def test_factorial_eval():\n    assert factorial(0) == 1\n    assert factorial(1) == 1\n    assert factorial(2) == 2\n    assert factorial(3) == 6\n    assert factorial(S(3)/2) == factorial(S(3)/2)\n    assert factorial(-1) == zoo\n    assert factorial(-2) == zoo\n    assert factorial(nan) == nan\n    assert factorial(oo) == oo\n", "def test_factorial():\n    assert factorial(-1) == zoo\n    assert factorial(0) == 1\n    assert factorial(1) == 1\n    assert factorial(2) == 2\n    assert factorial(3) == 6\n    assert factorial(4) == 24\n    assert factorial(5) == 120\n    assert factorial(n) == factorial(n)\n    assert factorial(2*n) == factorial(2*n)\n    assert factorial(S(1)/2) == factorial(S(1)/2)\n    assert factorial(-S(1)/2) == zoo\n    assert factorial(S(3)/2) == (S(1)/2)*factorial(S(1)/2)\n    assert factorial(S(5)/2) == (S(3)/2)*(S(1)/2)*factorial(S(1)/2)\n    assert factorial(-S(3)/2) == zoo\n    assert factorial(-S(5)/2) == zoo\n", "def test_subfactorial():\n    x = symbols('x')\n    assert subfactorial(0) == 1\n    assert subfactorial(1) == 0\n    assert subfactorial(2) == 1\n    assert subfactorial(3) == 2\n    assert subfactorial(4) == 9\n    assert subfactorial(5) == 44\n    assert subfactorial(x).rewrite(uppergamma) == uppergamma(x + 1, -1)/S.Exp1\n    assert subfactorial(nan) == nan\n    assert subfactorial(oo) == oo\n    assert subfactorial(-1) == zoo\n    assert subfactorial(-5) == zoo\n"], "sample_1008": ["def test_coordinate_sym():\n    N = ReferenceFrame('N')\n    a_x, a_y, a_z = N.varlist\n    assert isinstance(a_x, CoordinateSym)\n    assert a_x.frame == N\n    assert a_x == N[0]\n    assert a_x != N[1]\n    assert N[1] == a_y\n    assert hash(a_x) == hash(N[0])\n    assert a_x != 'a_x'\n    assert N[0] == N.varlist[0]\n    assert N[2] == N.varlist[2]\n    assert N[0] == a_x\n    assert N[1] == a_y\n    assert N[2] == a_z\n", "def test_coordinate_sym():\n    N = ReferenceFrame('N')\n    a, b, c = symbols('a b c')\n    vec = a * N.x + b * N.y + c * N.z\n    assert(N[0] == N.x)\n    assert(N[1] == N.y)\n    assert(N[2] == N.z)\n    assert(N.x == N[0])\n    assert(N.y == N[1])\n    assert(N.z == N[2])\n    assert(isinstance(N[0], CoordinateSym))\n    assert(isinstance(N[1], CoordinateSym))\n    assert(isinstance(N[2], CoordinateSym))\n    assert(N.x != N.y)\n    assert(N.y != N.z)\n    assert(N.z != N.x)\n    assert(N.x != vec)\n    assert(N.y != vec)\n    assert(N.z != vec)\n    assert(N.x == N.x)\n    assert(N.y == N.y)\n    assert(N.z == N.z)\n", "def test_coordinate_sym():\n    N = ReferenceFrame('N')\n    q = dynamicsymbols('q')\n    A = N.orientnew('A', 'Axis', [q, N.x])\n    assert A[0] == A.x\n    assert A[1] == A.y\n    assert A[2] == A.z\n    assert isinstance(A[0], CoordinateSym)\n    assert isinstance(A[1], CoordinateSym)\n    assert isinstance(A[2], CoordinateSym)\n    assert A[0] == A.varlist[0]\n    assert A[1] == A.varlist[1]\n    assert A[2] == A.varlist[2]\n    assert A[0].frame == A\n    assert A[1].frame == A\n    assert A[2].frame == A\n    assert A[0] == A[0]\n    assert A[1] == A[1]\n    assert A[2] == A[2]\n    assert A[0] != A[1]\n    assert A[1] != A[2]\n    assert A[0] != A[2]\n    assert hash(A[0]) == hash(A[0])\n    assert hash(A[1]) == hash(A[1])\n    assert hash(A[2]) == hash(A[2])\n    assert hash(A[0]) != hash(A[1])\n    assert hash(A[1]) != hash(A[2])\n    assert hash(A[0]) != hash(A[2])\n", "def test_coordinate_sym():\n    A = ReferenceFrame('A')\n    q1, q2, q3 = dynamicsymbols('q1 q2 q3')\n    B = A.orientnew('B', 'Body', [q1, q2, q3], '123')\n    assert isinstance(A.x, CoordinateSym)\n    assert isinstance(B.x, CoordinateSym)\n    assert A.x != B.x\n    assert A.x != A.y\n    assert A.x == A[0]\n    assert B.x == B[0]\n    assert hash(A.x) != hash(B.x)\n    assert hash(A.x) != hash(A.y)\n    mapping = A.variable_map(B)\n    assert mapping[A.x] == B.x*cos(q2)*cos(q3) - B.y*cos(q2)*sin(q3) + B.z*sin(q2)\n    assert mapping[A.y] == B.x*(sin(q1)*sin(q2)*cos(q3) - cos(q1)*sin(q3)) + B.y*(sin(q1)*sin(q2)*sin(q3) + cos(q1)*cos(q3)) - B.z*sin(q1)*cos(q2)\n    assert mapping[A.z] == B.x*(cos(q1)*sin(q2)*cos(q3) + sin(q1)*sin(q3)) + B.y*(cos(q1)*sin(q2)*sin(q3) - sin(q1)*cos(q3)) + B.z*cos(q1)*cos(q2)\n", "def test_reference_frame_variables():\n    N = ReferenceFrame('N')\n    x, y, z = dynamicsymbols('x y z')\n    n = ReferenceFrame('n', variables=(x, y, z))\n    assert n.varlist == (x, y, z)\n    assert n.variable_map(N) == {x: N.x, y: N.y, z: N.z}\n"], "sample_1009": ["def test_vector_diff():\n    u1 = dynamicsymbols('u1')\n    v = u1 * A.x\n    assert v.diff(u1, A) == A.x\n    assert v.diff(u1, A).diff(u1, A) == Vector(0)\n    assert v.diff(A.x, A) == Vector(0)\n", "def test_vector_magnitude():\n    v1 = A.x * 3 + A.y * 4\n    assert v1.magnitude() == 5\n    v2 = Vector(0)\n    assert v2.magnitude() == 0\n    v3 = A.x * symbols('a') + A.y * symbols('b')\n    assert v3.magnitude() == (symbols('a')**2 + symbols('b')**2)**0.5\n", "def test_vector_diff1():\n    # Test Vector.diff method for simple vector\n    q1 = dynamicsymbols('q1')\n    q1d = dynamicsymbols('q1', 1)\n    B = A.orientnew('B', 'Axis', [q1, A.x])\n    v1 = q1 * B.y\n    v1_dt = v1.diff(q1d, A)\n    assert v1_dt == q1d * B.y\n", "def test_vector_subs():\n    a, b, c, d = symbols('a b c d')\n    v = a * A.x + b * A.y + c * A.z\n    v_sub = v.subs({a: d})\n    assert v_sub == d * A.x + b * A.y + c * A.z\n    v_sub = v.subs({b: d})\n    assert v_sub == a * A.x + d * A.y + c * A.z\n    v_sub = v.subs({c: d})\n    assert v_sub == a * A.x + b * A.y + d * A.z\n", "def test_vector_diff():\n    q, u = dynamicsymbols('q u')\n    qd, ud = dynamicsymbols('q u', 1)\n    B = A.orientnew('B', 'Axis', [q, A.x])\n    v = u * B.x\n    assert v.diff(q, A) == u * B.y\n    assert v.diff(u, A) == B.x\n    assert v.diff(qd, A) == 0\n    assert v.diff(ud, A) == B.x\n"], "sample_1011": ["def test_octave_code_loggamma():\n    assert octave_code(loggamma(x), assign_to='l') == 'l = gammaln(x);'\n", "def test_octave_code_piecewise():\n    pw = Piecewise((x + 1, x > 0), (x, True))\n    with lowersettings(octave=\"1.5\"):\n        assert octave_code(pw, assign_to=\"tau\") == \"tau = ((x > 0).*(x + 1) + (~(x > 0)).*(x));\"\n    assert octave_code(pw, assign_to=\"tau\", inline=False) == \"if (x > 0)\\n  tau = x + 1;\\nelse\\n  tau = x;\\nend\"\n", "def test_matrix_element():\n    A = MatrixSymbol('A', 3, 3)\n    assert mcode(A[1, 2]) == 'A(2, 3)'\n", "def test_octave_code_Assignment():\n    A = MatrixSymbol('A', 3, 3)\n    B = MatrixSymbol('B', 3, 3)\n    v = MatrixSymbol('v', 3, 1)\n    expr = Eq(A*v + B*v, v)\n    assert octave_code(expr, assign_to='v', contract=False) == 'v = (A + B)*v;'\n", "def test_octave_code_user_functions():\n    f = Function('f')\n    g = Function('g')\n    h = Function('h')\n\n    # Simple function redefinition\n    custom_functions = {f: \"existing_octave_fcn\"}\n    assert octave_code(f(x), user_functions=custom_functions) == \"existing_octave_fcn(x)\"\n\n    # Function redefinition with multiple return types\n    custom_functions = {\n        g: [(lambda x: x.is_Matrix, \"my_mat_fcn\"),\n            (lambda x: not x.is_Matrix, \"my_fcn\")]\n    }\n    mat = Matrix([[1, x]])\n    assert octave_code(g(x), user_functions=custom_functions) == \"my_fcn(x)\"\n    assert octave_code(g(mat), user_functions=custom_functions) == \"my_mat_fcn([1 x])\"\n\n    # Test that undefined functions types get printed as-is\n    assert octave_code(h(x), user_functions=custom_functions) == \"h(x)\"\n"], "sample_1012": ["def test_printing_relational_operators():\n    assert NumPyPrinter().doprint(Eq(x, y)) == 'numpy.equal(x, y)'\n    assert NumPyPrinter().doprint(Le(x, y)) == 'numpy.less_equal(x, y)'\n    assert NumPyPrinter().doprint(Gt(x, y)) == 'numpy.greater(x, y)'\n", "def test_SymPyPrinter():\n    p = SymPyPrinter()\n    assert p.doprint(x**2) == 'x**2'\n    assert p.doprint(x**2.5) == 'x**2.5'\n    assert p.doprint(sign(x)) == 'sympy.sign(x)'\n    assert p.doprint(acos(x)) == 'sympy.acos(x)'\n", "def test_PythonCodePrinter_print_While():\n    whl = While(x < 5, AugAssign(x, '+', 1))\n    assert PythonCodePrinter().doprint(whl) == \"while x < 5:\\n    x += 1\"\n", "def test_PythonCodePrinter_print_ComplexInfinity():\n    assert PythonCodePrinter().doprint(Expr()) == 'None'\n    assert PythonCodePrinter().doprint(zoo) == 'float(\\'nan\\')'\n", "def test_pycode_printing_relational():\n    assert pycode(Eq(x, y), fully_qualified_modules=False) == '(x == y)'\n    assert pycode(Le(x, y), fully_qualified_modules=False) == '(x <= y)'\n    assert pycode(Gt(x, y), fully_qualified_modules=False) == '(x > y)'\n    assert pycode(Eq(x, y), fully_qualified_modules=False) == '(x == y)'\n    assert pycode(x > y, fully_qualified_modules=False) == '(x > y)'\n    assert pycode(x < y, fully_qualified_modules=False) == '(x < y)'\n    assert pycode(x >= y, fully_qualified_modules=False) == '(x >= y)'\n    assert pycode(x <= y, fully_qualified_modules=False) == '(x <= y)'\n"], "sample_1010": ["def test_LatexPrinter_settings():\n    # test if latex() respects the global settings\n    # issue 5177\n    x = Symbol('x')\n    with evaluate(False):\n        expr = pi * x * 2 / 3 * 4 / x\n    assert latex(expr) == r\"2 \\pi\"  # Add & Mul are in LatexPrinter._default_settings\n    assert latex(expr, order='none') == r\"\\frac{8 \\pi}{3}\"\n    assert latex(expr, order='lex') == r\"2 \\pi\" \n    assert latex(expr, order='old') == r\"2 \\pi\" \n", "def test_symbol_names():\n    assert latex(x**2, symbol_names={x:'x_i'}) == 'x_i^{2}'\n    assert latex(x**2 + y**2, symbol_names={x:'x_i', y:'y_j'}) == 'x_i^{2} + y_j^{2}'\n    assert latex(x**2 + y**2, symbol_names={x:r'\\hat{x}', y:'y'}) == r'\\hat{x}^{2} + y^{2}'\n", "def test_NDimArray():\n    # Immutable Dense NDimArray\n    array1 = ImmutableDenseNDimArray([1, 2, 3, 4, 5, 6], shape=(2, 3))\n    assert latex(array1) == r\"\\begin{matrix}1 & 2 & 3\\\\4 & 5 & 6\\end{matrix}\"\n    \n    # Immutable Sparse NDimArray\n    array2 = ImmutableSparseNDimArray([1, 2, 3, 4, 5, 6], shape=(2, 3))\n    assert latex(array2) == r\"\\begin{matrix}1 & 2 & 3\\\\4 & 5 & 6\\end{matrix}\"\n    \n    # Mutable Dense NDimArray\n    array3 = MutableDenseNDimArray([1, 2, 3, 4, 5, 6], shape=(2, 3))\n    assert latex(array3) == r\"\\begin{matrix}1 & 2 & 3\\\\4 & 5 & 6\\end{matrix}\"\n    \n    # Mutable Sparse NDimArray\n    array4 = MutableSparseNDimArray([1, 2, 3, 4, 5, 6], shape=(2, 3))\n    assert latex(array4) == r\"\\begin{matrix}1 & 2 & 3\\\\4 & 5 & 6\\end{matrix}\"\n    \n    # Rank 0 NDimArray\n    array5 = ImmutableDenseNDimArray([1], shape=())\n    assert latex(array5) == \"1\"\n    \n    # High rank NDimArray\n    array6 = ImmutableDenseNDimArray([1, 2, 3, 4, 5, 6, 7, 8], shape=(2, 2, 2))\n    expected = r\"\\begin{matrix}\\begin{matrix}1 & 2\\\\3 & 4\\end{matrix} & \\begin{matrix}5 & 6\\\\7 & 8\\end{matrix}\\end{matrix}\"\n    assert latex(array6) == expected\n", "def test_latex_matrix():\n    M = Matrix([[1, 2], [3, 4]])\n    assert latex(M) == r'\\left(\\begin{matrix}1 & 2\\\\3 & 4\\end{matrix}\\right)'\n    assert latex(M, mat_str='array') == r'\\left(\\begin{array}{cc}1 & 2\\\\3 & 4\\end{array}\\right)'\n    assert latex(M, mat_str='bmatrix') == r'\\begin{bmatrix}1 & 2\\\\3 & 4\\end{bmatrix}'\n    assert latex(M, mat_delim='(') == r'\\left(\\begin{matrix}1 & 2\\\\3 & 4\\end{matrix}\\right)'\n    assert latex(M, mat_delim='') == r'\\begin{matrix}1 & 2\\\\3 & 4\\end{matrix}'\n", "def test_LatexPrinter_settings():\n    x = Symbol('x')\n    assert latex(x**2, symbol_names={x: 'x_i'}) == 'x_i^{2}'\n    assert latex(x**2, ln_notation=True) == 'x^{2}'\n    assert latex(log(x), ln_notation=True) == r'\\ln{\\left(x \\right)}'\n    assert latex(log(x), ln_notation=False) == r'\\log{\\left(x \\right)}'\n    assert latex(exp(x), ln_notation=True) == 'e^{x}'\n"], "sample_1013": ["def test_lambdify_module_mpmath():\n    # For computing the modulus of a sympy expression we use the builtin abs\n    # function, instead of the previously used fabs function for all\n    # translation modules. This is because the fabs function in the math\n    # module does not accept complex valued arguments.\n    # The only exception, where we don't use the builtin abs function is the\n    # mpmath translation module, because mpmath.fabs returns mpf objects in\n    # contrast to abs().\n    f = lambdify(x, Abs(x), 'mpmath')\n    assert str(f(1.0)) == '1.0'\n", "def test_lambdify_with_implemented_function():\n    # Test if a sympy function with an implementation can be used with lambdify.\n    f = implemented_function('f', lambda x: x+1)\n    g = implemented_function('g', lambda x: x*10)\n\n    fgx = f(g(x))\n    lmbda = lambdify(x, fgx)\n    assert lmbda(2) == 21\n", "def test_lambdify_module_imports():\n    # Test if module imports work correctly in lambdify\n    f = lambdify(x, sin(x), modules='math')\n    assert math.sin(1) == f(1)\n\n    f = lambdify(x, sin(x), modules='mpmath')\n    assert mpmath.sin(1) == f(1)\n\n    if numpy:\n        f = lambdify(x, sin(x), modules='numpy')\n        assert numpy.sin(1) == f(1)\n\n    if numexpr:\n        f = lambdify(x, sin(x), modules='numexpr')\n        assert numpy.sin(1) == f(1)\n\n    f = lambdify(x, sin(x), modules={'sin': math.sin})\n    assert math.sin(1) == f(1)\n", "def test_lambdify_with_lambdaprinter():\n    # Test that lambdify works with a LambdaPrinter instance.\n    class CustomPrinter(LambdaPrinter):\n            return '({})**({})'.format(self._print(expr.base), self._print(expr.exp))\n\n    f = lambdify(x, x**2, printer=CustomPrinter())\n    assert f(1) == 1\n", "def test_lambdify_use_imps():\n    # Test that lambdify handles implemented functions correctly\n    f = implemented_function(Function('f'), lambda x: x+1)\n    g = implemented_function(Function('g'), lambda x: x*10)\n    h = lambdify(x, f(g(x)), use_imps=True)\n    assert h(2) == 21\n\n    # Test that it fails without use_imps=True\n    h = lambdify(x, f(g(x)), use_imps=False)\n    raises(NameError, lambda: h(2))\n"], "sample_1015": ["def test_ccode_dereference():\n    x = symbols('x', real=True)\n    expr = x + 1\n    printer = C89CodePrinter({'dereference': [x]})\n    assert printer.doprint(expr) == '(*x) + 1'\n", "compilation error", "compilation error", "def test_ccode_reserved_words():\n    x, y = symbols('x y')\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", SymPyDeprecationWarning)\n        assert ccode(Min(x, y), standard='c99') == 'fmin(x, y)'\n        assert ccode(Max(x, y), standard='c99') == 'fmax(x, y)'\n", "def test_CCodePrinter_dereference():\n    d = Dummy('d')\n    v = Variable(d, type='int')\n    ptr = Pointer(v, attrs=(value_const, pointer_const))\n    decl = Declaration(ptr)\n    deref = Declaration(Variable(d, type='int', attrs=(value_const,)))\n    assert C89CodePrinter().doprint(decl) == 'const int * const d;'\n    assert C89CodePrinter(dereference=[d]).doprint(decl) == 'const int * const d;'\n    assert C89CodePrinter().doprint(deref) == 'const int d;'\n    assert C89CodePrinter(dereference=[d]).doprint(deref) == 'const int *d;'\n"], "sample_1014": ["def test_tomatrix_conversion():\n    array = ImmutableDenseNDimArray([1 for i in range(9)], (3, 3))\n    matrix = array.tomatrix()\n    assert isinstance(matrix, Matrix)\n    assert matrix.shape == (3, 3)\n    raises(ValueError, lambda: ImmutableDenseNDimArray([1 for i in range(27)], (3, 3, 3)).tomatrix())\n", "def test_immutable_dense_ndim_array_as_mutable():\n    a = ImmutableDenseNDimArray([1, 2, 3, 4], (2, 2))\n    assert a.as_mutable().is_MutableDenseNDimArray\n    b = a.as_mutable()\n    b[0, 0] = 5\n    assert b[0, 0] == 5\n    assert a[0, 0] == 1\n", "def test_reshape():\n    array = MutableDenseNDimArray([1, 2, 3, 4, 5, 6], (2, 3))\n    new_array = array.reshape(3, 2)\n    assert new_array.shape == (3, 2)\n    assert new_array._array == [1, 2, 3, 4, 5, 6]\n    raises(ValueError, lambda: array.reshape(4, 2))\n    raises(ValueError, lambda: array.reshape(1, 5))\n", "def test_DenseNDimArray_methods():\n    array = ImmutableDenseNDimArray([1, 2, 3, 4], (2, 2))\n    assert array.tomatrix() == Matrix([[1, 2], [3, 4]])\n    assert array.rank() == 2\n    assert array.shape == (2, 2)\n    assert list(array) == [1, 2, 3, 4]\n\n    new_array = array.reshape(4)\n    assert new_array.shape == (4,)\n    assert list(new_array) == [1, 2, 3, 4]\n\n    new_array = array.reshape(1, 4)\n    assert new_array.shape == (1, 4)\n    assert list(new_array) == [1, 2, 3, 4]\n\n    # Test __getitem__ method\n    assert array[0, 0] == 1\n    assert array[1, 1] == 4\n    assert array[(0, 0)] == 1\n    assert array[(1, 1)] == 4\n\n    # Test __getitem__ method with slice\n    assert list(array[0, :]) == [1, 2]\n    assert list(array[1, :]) == [3, 4]\n    assert list(array[:, 0]) == [1, 3]\n    assert list(array[:, 1]) == [2, 4]\n", "def test_tomatrix_conversion():\n    array = ImmutableDenseNDimArray([1, 2, 3, 4], (2, 2))\n    matrix = array.tomatrix()\n    assert matrix == Matrix([[1, 2], [3, 4]])\n\n    raises(ValueError, lambda: ImmutableDenseNDimArray([1, 2, 3, 4, 5], (2, 3)).tomatrix())\n\n    array_3d = ImmutableDenseNDimArray([1, 2, 3, 4, 5, 6, 7, 8], (2, 2, 2))\n    raises(ValueError, lambda: array_3d.tomatrix())\n"], "sample_1017": ["def test_as_boolean():\n    assert as_Boolean(1) is true\n    assert as_Boolean(1) is not True\n    assert as_Boolean(x) == x\n    raises(TypeError, lambda: as_Boolean(2))\n", "def test_as_Boolean():\n    assert as_Boolean(True) is true\n    assert as_Boolean(False) is false\n    assert as_Boolean(1) is true\n    assert as_Boolean(0) is false\n    assert as_Boolean(S.true) is true\n    assert as_Boolean(S.false) is false\n    raises(TypeError, lambda: as_Boolean(2))\n    raises(TypeError, lambda: as_Boolean(oo))\n    raises(TypeError, lambda: as_Boolean(x))\n", "def test_as_Boolean():\n    assert as_Boolean(True) is true\n    assert as_Boolean(False) is false\n    assert as_Boolean(1) is true\n    raises(TypeError, lambda: as_Boolean(2))\n    raises(TypeError, lambda: as_Boolean(x))\n    assert as_Boolean(true) is true\n    assert as_Boolean(false) is false\n", "def test_as_boolean():\n    assert as_Boolean(True) is true\n    assert as_Boolean(False) is false\n    assert as_Boolean(x) is x\n    raises(TypeError, lambda: as_Boolean(2))\n", "def test_as_Boolean():\n    assert as_Boolean(True) is true\n    assert as_Boolean(False) is false\n    assert as_Boolean(1) is true\n    raises(TypeError, lambda: as_Boolean(2))\n    raises(TypeError, lambda: as_Boolean(None))\n    assert as_Boolean(x) == x\n    assert as_Boolean(~x) == ~x\n    assert as_Boolean(x & y) == x & y\n"], "sample_1016": ["def test_octave_code_Rational():\n    # this should give 1/2, but currently gives 0.5\n    assert octave_code(Rational(1, 2)) == '1/2'\n    assert octave_code(Rational(3, 2)) == '3/2'\n    assert octave_code(Rational(-3, 2)) == '-3/2'\n", "def test_special_matrices():\n    A = MatrixSymbol('A', 3, 3)\n    assert octave_code(eye(3)) == \"eye(3)\"\n    assert octave_code(Identity(3)) == \"eye(3)\"\n    assert octave_code(SparseMatrix(5, 5)) == 'sparse(5, 5)'\n    m = SparseMatrix(5, 5, {(1, 1): 2, (2, 2): 3})\n    assert octave_code(m) == \"sparse([2 3 1], [2 3 1], [2 3 0], 5, 5)\"\n", "def test_octave_code_Piecewise():\n    pw = Piecewise((x + 1, x > 0), (x, True))\n    assert octave_code(pw) == '((x > 0).*(x + 1) + (~(x > 0)).*(x))'\n    pw = Piecewise((x + 1, x > 1), (x**2, x > 0), (x, True))\n    assert octave_code(pw) == ('((x > 1).*(x + 1) + (~(x > 1) & (x > 0)).*(x.^2)) + '\n                              '(~((x > 1) | (x > 0))).*(x))')\n    pw = Piecewise((x + 1, x > 1), (x**2, x > 0), (x**3, x > -1), (x, True))\n    assert octave_code(pw) == ('(((x > 1).*(x + 1) + (~(x > 1) & (x > 0)).*(x.^2)) '\n                              '+ (~((x > 1) | (x > 0)) & (x > -1)).*(x.^3)) + '\n                              '(~(((x > 1) | (x > 0)) | (x > -1))).*(x))')\n    pw = Piecewise((x + 1, x > 0), (x, True))\n    assert octave_code(pw, assign_to='y') == 'y = ((x > 0).*(x + 1) + (~(x > 0)).*(x));'\n", "def test_octave_code_sinc():\n    expr = sinc(x)\n    result = octave_code(expr)\n    assert result == \"sinc(x/pi)\"\n", "def test_octave_code_Indexed():\n    len_y = 5\n    y = IndexedBase('y', shape=(len_y,))\n    t = IndexedBase('t', shape=(len_y,))\n    Dy = IndexedBase('Dy', shape=(len_y-1,))\n    i = symbols('i', integer=True)\n    e = Dy[i] + (y[i+1]-y[i])/(t[i+1]-t[i])\n    result = octave_code(e, assign_to=Dy[i], contract=False)\n    assert result == 'Dy(i) = Dy(i) + (y(i + 1) - y(i))./(t(i + 1) - t(i));'\n"], "sample_1018": ["def test_printing_of_SubroutineCall():\n    n = symbols('n', integer=True)\n    x = symbols('x', real=True)\n    s = symbols('s', real=True)\n    pas = symbols('Pas', real=True)\n    f = Function('f')\n    call = f(n, x, s, pas)\n    assert fcode(call) == 'call f(n, x, s, Pas)'\n", "compilation error", "def test_fcode_elemental():\n    x = symbols('x')\n    expr = 2*x\n    f = implemented_function('f', Lambda(x, expr))\n    result = fcode(f(x), standard=95, assign_to='res')\n    assert 'elemental' in result\n", "def test_printing_Declaration_with_Bool():\n    var = Variable('x', type='logical', attrs=[value_const])\n    decl = Declaration(var)\n    assert fcode(decl) == 'logical, parameter :: x'\n", "def test_printing_CodePrinter():\n    f = FCodePrinter({})\n\n    # Test that global_name_mapping is called\n    assert f._print_Indexed(IndexedBase('a', shape=(2,))[1]) == 'a(2)'\n    assert f._print_Indexed(IndexedBase('abc123', shape=(2,))[1]) == 'abc123_(2)'\n"], "sample_1020": ["def test_print_Mul():\n    assert mcode(x*y) == 'x*y'\n    assert mcode(2*x) == '2*x'\n    assert mcode(x*y*z) == 'x*y*z'\n    assert mcode(2*x*y) == '2*x*y'\n    assert mcode(x*(y+z)) == 'x*(y + z)'\n", "def test_Pow():\n    assert mcode(x**2) == 'x^2'\n    assert mcode(x**(y**2)) == 'x^(y^2)'\n    assert mcode((x**y)**2) == '(x^y)^2'\n", "def test_Pow():\n    assert mcode(x**2) == 'x^2'\n    assert mcode(x**(1/2)) == 'x^(1/2)'\n    assert mcode(x**-1) == 'x^(-1)'\n    assert mcode(1/x) == 'x^(-1)'\n    assert mcode(x**pi) == 'x^Pi'\n", "def test_mathematica_code_Pow():\n    assert mcode(x**2) == 'x^2'\n    assert mcode(x**(-2)) == 'x^(-2)'\n    assert mcode(x**(2/3)) == 'x^(2/3)'\n", "def test_mcode_list_tuple():\n    assert mcode([1, 2, 3]) == \"{1, 2, 3}\"\n    assert mcode((1, 2, 3)) == \"{1, 2, 3}\"\n    assert mcode(Tuple(1, 2, 3)) == \"{1, 2, 3}\"\n"], "sample_1021": ["def test_rotate_point():\n    q = Quaternion(cos(x/2), 0, 0, sin(x/2))\n    assert trigsimp(Quaternion.rotate_point((1, 1, 1), q)) == (sqrt(2)*cos(x + pi/4), sqrt(2)*sin(x + pi/4), 1)\n    (axis, angle) = q.to_axis_angle()\n    assert trigsimp(Quaternion.rotate_point((1, 1, 1), (axis, angle))) == (sqrt(2)*cos(x + pi/4), sqrt(2)*sin(x + pi/4), 1)\n", "def test_quaternion_diff():\n    q = Quaternion(x, y, z, w)\n    assert diff(q, x) == Quaternion(1, 0, 0, 0)\n    assert diff(q, y) == Quaternion(0, 1, 0, 0)\n    assert diff(q, z) == Quaternion(0, 0, 1, 0)\n    assert diff(q, w) == Quaternion(0, 0, 0, 1)\n    assert diff(q, x).diff(x) == Quaternion(0, 0, 0, 0)\n    assert diff(q.pow(2), x) == Quaternion(2*x, 2*y, 2*z, 2*w)\n    assert diff(q.pow(2), y) == Quaternion(0, 2*x, 0, 0) + Quaternion(0, 0, 2*w, -2*z)\n    assert diff(q.pow(2), z) == Quaternion(0, 0, 2*x, 0) + Quaternion(0, -2*w, 0, 2*y)\n    assert diff(q.pow(2), w) == Quaternion(0, 0, 0, 2*x) + Quaternion(0, 2*z, -2*y, 0)\n", "def test_rotate_point():\n    q = Quaternion(cos(x/2), 0, 0, sin(x/2))\n    assert trigsimp(Quaternion.rotate_point((1, 1, 1), q)) == (sqrt(2)*cos(x + pi/4), sqrt(2)*sin(x + pi/4), 1)\n    axis, angle = q.to_axis_angle()\n    assert trigsimp(Quaternion.rotate_point((1, 1, 1), (axis, angle))) == (sqrt(2)*cos(x + pi/4), sqrt(2)*sin(x + pi/4), 1)\n    assert Quaternion.rotate_point((1, 0, 0), Quaternion(1, 0, 0, 0)) == (1, 0, 0)\n", "def test_quaternion_pow():\n    q = Quaternion(x, y, z, w)\n    assert q.pow(0) == Quaternion(1, 0, 0, 0)\n    assert q.pow(1) == q\n    assert q.pow(-1) == q.inverse()\n    assert q.pow(2) == q * q\n    raises(ValueError, lambda: q.pow(x))\n", "def test_rotate_point():\n    q = Quaternion(cos(x/2), 0, 0, sin(x/2))\n    assert trigsimp(Quaternion.rotate_point((1, 1, 1), q)) == (sqrt(2)*cos(x + pi/4), sqrt(2)*sin(x + pi/4), 1)\n    axis, angle = q.to_axis_angle()\n    assert trigsimp(Quaternion.rotate_point((1, 1, 1), (axis, angle))) == (sqrt(2)*cos(x + pi/4), sqrt(2)*sin(x + pi/4), 1)\n"], "sample_1022": ["def test_factorial():\n    assert parse_expr('6!', transformations=standard_transformations) == 720\n    assert parse_expr('factorial(6)', transformations=standard_transformations) == 720\n    assert parse_expr('2*factorial(5)', transformations=standard_transformations) == 240\n", "def test_convert_xor():\n    transformations = standard_transformations + (convert_xor,)\n    assert parse_expr('x^2', transformations=transformations) == sympy.Symbol('x')**2\n", "def test_factorial_notation():\n    # Test that factorial notation works correctly.\n    expr = parse_expr('2*x!', transformations=standard_transformations)\n    assert expr == 2 * sympy.factorial(sympy.symbols('x'))\n", "def test_parse_expr_evaluate_false():\n    # Use evaluate=False to preserve the order of the arguments\n    expr = parse_expr('1 + x', evaluate=False)\n    assert expr.args == (1, sympy.Symbol('x'))\n    expr = parse_expr('x + 1', evaluate=False)\n    assert expr.args == (sympy.Symbol('x'), 1)\n\n    # Use evaluate=False to suppress automatic simplification\n    expr = parse_expr('2**3', evaluate=False)\n    assert expr != 8\n", "def test_split_symbols_custom():\n    # Test that split_symbols_custom can split symbols\n    # even when _token_splittable returns False\n        return True\n\n    transformation = split_symbols_custom(always_true)\n    expr = \"abcd\"\n    result = parse_expr(expr, transformations=(standard_transformations +\n                                               (transformation,)))\n    expected = sympy.symbols('a b c d')\n    assert result == sympy.prod(expected)\n\n    # Test that split_symbols_custom doesn't split symbols\n    # when the custom predicate returns False\n        return False\n\n    transformation = split_symbols_custom(always_false)\n    expr = \"abcd\"\n    result = parse_expr(expr, transformations=(standard_transformations +\n                                               (transformation,)))\n    assert result == sympy.symbols('abcd')\n"], "sample_1019": ["def test_factor_nc():\n    A, B, C = symbols('A B C', commutative=False)\n    assert factor_nc((x**2 + 2*A*x + A**2).expand()) == (x + A)**2\n    assert factor_nc(((x + A)*(x + B)).expand()) == (x + A)*(x + B)\n    assert factor_nc(A*(x + B)*(x + C) + B*(x + A)*(x + C)) == (A + B)*(x + A)*(x + B)*(x + C)\n    assert factor_nc((x + A)*(x + B) + (x + B)*(x + C)) == (x + B)*(2*x + A + C)\n", "def test_monotonic_sign():\n    assert _monotonic_sign(x) is None\n    assert _monotonic_sign(-x) is None\n    assert _monotonic_sign(S(2)) == S(2)\n    assert _monotonic_sign(S(-2)) == S(-2)\n    assert _monotonic_sign(S(3)) == S(3)\n    assert _monotonic_sign(S(-3)) == S(-3)\n    assert _monotonic_sign(x**2 + 2*x + 1) is None\n    assert _monotonic_sign(x**2 + 2*x + 1.5) is None\n    assert _monotonic_sign(x**2 + 1.5*x + 1) is None\n    assert _monotonic_sign(x + 2) is None\n    assert _monotonic_sign(x**2 + 2*x + 1, half_integers=True) is None\n\n    nn = Dummy(integer=True, nonnegative=True)\n    assert _monotonic_sign(nn + 1) == 1\n    assert _monotonic_sign(nn + 2) == 2\n    assert _monotonic_sign(nn - 2) == -2\n    assert _monotonic_sign(nn - 1) is None\n    assert _monotonic_sign(2*nn + 3) == 3\n", "def test_monotonic_sign():\n    assert _monotonic_sign(2*x) == 2*x\n    assert _monotonic_sign(x**3) is None\n    assert _monotonic_sign(-x**3) is None\n    assert _monotonic_sign(x**2) == x**2\n    assert _monotonic_sign(-x**2) is None\n    assert _monotonic_sign(2*x**2) == 2*x**2\n    assert _monotonic_sign(-2*x**2) is None\n    assert _monotonic_sign(2*x**2 + 3*x) is None\n    assert _monotonic_sign(x**2 + 2*x + 1) == 1\n    assert _monotonic_sign(x**2 + 2*x + 2) == 2\n    assert _monotonic_sign(-x**2 - 2*x - 1) == -1\n    assert _monotonic_sign(x + y) is None\n    assert _monotonic_sign(x - y) is None\n    assert _monotonic_sign(x*y) is None\n", "def test_monotonic_sign():\n    assert _monotonic_sign(x) is None\n    assert _monotonic_sign(-x) is None\n    nn = Dummy(integer=True, nonnegative=True)\n    assert _monotonic_sign(nn + 1) == 1\n    assert _monotonic_sign(nn - 1) == 'pos'\n    p = Dummy(integer=True, positive=True)\n    assert _monotonic_sign(p - 1) == 'pos'\n    assert _monotonic_sign(p + 1) == 2\n    assert _monotonic_sign(nn*p + 1) == 1\n    assert _monotonic_sign(p - S.Half) == 'pos'\n    assert _monotonic_sign(p + S.Half) == 'pos'\n    assert _monotonic_sign(-nn) == 'npos'\n    assert _monotonic_sign(-p) == 'neg'\n    assert _monotonic_sign(-nn*p) == 'neg'\n    assert _monotonic_sign(nn/2) == 'pos'\n    assert _monotonic_sign(nn + S.Half) == 'pos'\n    assert _monotonic_sign(p/2) == 'pos'\n    assert _monotonic_sign(p + S.Half) == 'pos'\n    assert _monotonic_sign((nn + 1)/2) == 'pos'\n    assert _monotonic_sign((p + 1)/2) == 'pos'\n    assert _monotonic_sign((p + 2)/2) == 1\n    assert _monotonic_sign(-nn/2) == 'npos'\n    assert _monotonic_sign(-p/2) == 'neg'\n    assert _monotonic_sign(-(nn + 1)/2) == 'neg'\n    assert _monotonic_sign(-(p + 1)/2) == 'neg'\n    assert _monotonic_sign((nn - 1)/2) == 'npos'\n    assert _monotonic_sign((p - 1)/2) == 'npos'\n    assert _monotonic_sign((p - 2)/2) == -1\n", "def test_gcd_terms():\n    # two terms with a common factor\n    assert gcd_terms(x + 2*x) == x * (1 + 2)\n    # two terms with a common factor, fraction=True\n    assert gcd_terms(x + 2*x, fraction=True) == x * (1 + 2)\n    # two terms with a common factor, clear=False\n    assert gcd_terms(x + 2*x, clear=False) == x * (1 + 2)\n    # two terms with a common factor, fraction=False, clear=False\n    assert gcd_terms(x + 2*x, fraction=False, clear=False) == x * (1 + 2)\n\n    # two terms without a common factor\n    assert gcd_terms(x + y) == x + y\n    # two terms without a common factor, fraction=True\n    assert gcd_terms(x + y, fraction=True) == x + y\n    # two terms without a common factor, clear=False\n    assert gcd_terms(x + y, clear=False) == x + y\n    # two terms without a common factor, fraction=False, clear=False\n    assert gcd_terms(x + y, fraction=False, clear=False) == x + y\n\n    # one term\n    assert gcd_terms(x) == x\n    # one term, fraction=True\n    assert gcd_terms(x, fraction=True) == x\n    # one term, clear=False\n    assert gcd_terms(x, clear=False) == x\n    # one term, fraction=False, clear=False\n    assert gcd_terms(x, fraction=False, clear=False) == x\n\n    # no terms\n    assert gcd_terms(0) == 0\n    # no terms, fraction=True\n    assert gcd_terms(0, fraction=True) == 0\n    # no terms, clear=False\n    assert gcd_terms(0, clear=False) == 0\n    # no terms, fraction=False, clear=False\n    assert gcd_terms(0, fraction=False, clear=False) == 0\n\n    # non-commutative symbols\n    A, B = symbols('A B', commutative=False)\n    assert gcd_terms(A*x + 2*A*x) == A*x * (1 + 2)\n    assert gcd_terms(A*x + B*x) == x * (A + B)\n"], "sample_1023": ["def test_cycle_length():\n    f = lambda i: (i**2 + 1) % 51\n    assert next(cycle_length(f, 4)) == (6, 2)\n    assert list(cycle_length(f, 4, values=True)) == [17, 35, 2, 5, 26, 14, 44, 50, 2, 5, 26, 14]\n    assert next(cycle_length(f, 4, nmax=4)) == (4, None)\n    assert list(cycle_length(f, 4, nmax=4, values=True)) == [17, 35, 2, 5]\n", "def test_primorial():\n    assert primorial(1) == 2\n    assert primorial(2) == 2 * 3\n    assert primorial(3) == 2 * 3 * 5\n    assert primorial(1, False) == 1\n    assert primorial(2, False) == 2\n    assert primorial(4, False) == 2 * 3\n    assert primorial(6, False) == 2 * 3 * 5\n    n = 15\n    assert primorial(n) == primorial(n, nth=False)\n", "def test_totientrange():\n    # Test totientrange function for small inputs\n    assert list(sieve.totientrange(1, 10)) == [1, 1, 2, 2, 4, 2, 6, 4, 6, 4]\n    assert list(sieve.totientrange(10, 20)) == [4, 10, 4, 12, 6, 8, 8, 16, 6, 18]\n    assert list(sieve.totientrange(100, 110)) == [40, 100, 4, 12, 6, 8, 8, 106, 6, 108]\n", "def test_primorial():\n    assert primorial(1) == 2\n    assert primorial(2) == 6\n    assert primorial(3) == 30\n    assert primorial(4) == 210\n    assert primorial(5, nth=False) == 30  # primes less than or equal to 5 are 2, 3, 5\n    assert primorial(10, nth=False) == 210 * 7 * 11 * 13 * 17 * 19 * 23 * 29  # primes less than or equal to 10\n    assert primorial(100, nth=False) > 210 * 7 * 11 * 13 * 17 * 19 * 23 * 29\n", "def test_cycle_length():\n    # test cycle_length with a sequence that has a cycle\n    f = lambda i: (i**2 + 1) % 51\n    assert next(cycle_length(f, 4)) == (6, 2)\n\n    # test cycle_length with values=True\n    n = cycle_length(f, 4, values=True)\n    assert list(ni for ni in n) == [17, 35, 2, 5, 26, 14, 44, 50, 2, 5, 26, 14]\n\n    # test cycle_length with nmax\n    assert next(cycle_length(f, 4, nmax=4)) == (4, None)\n    assert [ni for ni in cycle_length(f, 4, nmax=4, values=True)] == [17, 35, 2, 5]\n"], "sample_1025": ["def test_relatational():\n    assert NumPyPrinter()._print_Relational(Eq(x, 1)) == \"numpy.equal(x, 1)\"\n    assert NumPyPrinter()._print_Relational(Le(x, 1)) == \"numpy.less_equal(x, 1)\"\n    assert NumPyPrinter()._print_Relational(Gt(x, 1)) == \"numpy.greater(x, 1)\"\n", "def test_PythonCodePrinter_print_ImaginaryUnit():\n    assert PythonCodePrinter().doprint(1j) == '1j'\n    assert PythonCodePrinter().doprint(-1j) == '-1j'\n", "def test_PythonCodePrinter_print_Rational():\n    assert PythonCodePrinter().doprint(Rational(1, 2)) == '(1/2)'\n    assert PythonCodePrinter().doprint(Rational(3, 4)) == '(3/4)'\n    assert PythonCodePrinter().doprint(Rational(5, 1)) == '5'\n", "def test_PythonCodePrinter_print_Piecewise_without_default():\n    printer = PythonCodePrinter()\n    piecewise = Piecewise((x, x>0), (y, y>0))\n    assert printer._print(piecewise) == '((x) if (x > 0) else ((y) if (y > 0) else None))'\n", "def test_PythonCodePrinter_print_Pow():\n    # Test if Pow can be printed as an integer power, for constant bases and integer exponents\n    assert PythonCodePrinter().doprint(2**3) == \"2**3\"\n    # Test if Pow can be printed as a floating point power, for constant bases and non-integer exponents\n    assert PythonCodePrinter().doprint(2**3.5) == \"2**3.5\"\n    # Test if Pow can be printed as a general power, for non-constant bases or exponents\n    assert PythonCodePrinter().doprint(x**y) == \"x**y\"\n"], "sample_1024": ["def test_Pow_number():\n    # Test cases where the base is a number (not a Symbol) and the exponent is\n    # a Symbol or a number.  All such cases should be handled by the Number\n    # class methods (in this case Rational, Integer, Float, etc.).\n    assert Pow(2, 3, evaluate=False) == 8\n    assert Pow(2, -3, evaluate=False) == Rational(1, 8)\n    assert Pow(-2, -3, evaluate=False) == Rational(-1, 8)\n    assert Pow(oo, 3, evaluate=False) == oo\n    assert Pow(oo, -3, evaluate=False) == 0\n    assert Pow(-oo, 3, evaluate=False) == -oo\n    assert Pow(-oo, -3, evaluate=False) == 0\n    assert Pow(nan, 3, evaluate=False) == nan\n    assert Pow(nan, -3, evaluate=False) == nan\n    assert Pow(2, SSymbol('x', integer=True), evaluate=False) == 2**SSymbol('x', integer=True)\n    assert Pow(2, SSymbol('x', even=True), evaluate=False) == 2**SSymbol('x', even=True)\n", "def test_gcdex():\n    a, b = 5, 13\n    g, x, y = igcdex(a, b)\n    assert a*x + b*y == g\n    assert igcdex(10, 12) == (-1, 1, 1)\n    assert igcdex(100, 2004) == (-20, 1, 4)\n", "def test_mpf_norm():\n    # test that mpf_norm returns the right tuple and that it is\n    # comparable to the input\n    assert mpf_norm((1, 2, 3, 4), 5) == (1, 2, 3, 4)\n    a = mpf_norm((1, 2, 3, 4), 5)\n    b = mpf_norm((1, 2, 3, 4), 30)\n    assert comp(a, b)\n", "def test_Float_integer_nthroot():\n    assert same_and_same_prec(Float(2.5, 3).integer_nthroot(5),\n                              Float(1.15, 3))\n    assert same_and_same_prec(Float(2.5, 12).integer_nthroot(5),\n                              Float(1.148698092, 12))\n    assert same_and_same_prec(Float(2.5, 4).integer_nthroot(-5),\n                              Float(0.8695652, 4))\n    assert same_and_same_prec(Float(2.5, 6).integer_nthroot(-5),\n                              Float(0.869565, 6))\n", "def test_mpf_norm():\n    assert mpf_norm((1, 5, -2, 3), 30) == (1, 5, -2, 3)\n    assert mpf_norm((-1, 5, -2, 3), 30) == (-1, 5, -2, 3)\n    assert mpf_norm((0, 0, 0, 0), 30) == (0, 0, 0, 0)\n    assert mpf_norm((-1, 0, 0, 1), 30) == (-1, 0, 0, 1)\n"], "sample_1026": ["def test_lambdify_matrix_vector_multiplication():\n    M = MatrixSymbol('M', 2, 2)\n    v = MatrixSymbol('v', 2, 1)\n    l = lambdify((M, v), M*v, 'numpy')\n    M_val = numpy.array([[1, 2], [3, 4]])\n    v_val = numpy.array([5, 6])\n    assert numpy.array_equal(l(M_val, v_val), numpy.dot(M_val, v_val))\n", "def test_lambdify_matrix_with_integer_scalar():\n    M = Matrix([[1, 2], [3, 4]])\n    l = lambdify(x, M*x, 'numpy')\n    assert numpy.array_equal(l(2), M*2)\n", "def test_lambdify_matrix_with_custom_function():\n    f = implemented_function(Function('f'), lambda x: x+1)\n    M = Matrix([[f(x), x], [x, f(x)]])\n    l = lambdify(x, M, modules=[{'ImmutableDenseMatrix': numpy.array}, 'numpy'])\n    assert numpy.array_equal(l(1), numpy.array([[2, 1], [1, 2]]))\n", "def test_lambdastr():\n    # Issue #16230: Ensure lambdastr handles arguments that are not valid Python identifiers\n    assert lambdastr((x, (y, z)), x + y) == 'lambda _0,_1: (lambda x,y,z: (x + y))(_0,_1[0],_1[1])'\n    assert lambdastr(x, x**2) == 'lambda x: (x**2)'\n    assert lambdastr((x, y, z), [z, y, x]) == 'lambda x,y,z: ([z, y, x])'\n    assert lambdastr(x, sqrt(x)) == 'lambda x: (sqrt(x))'\n", "def test_lambdify_with_dict():\n    a, b, c = symbols('a b c')\n    d = {'a': a, 'b': b, 'c': c}\n    f = lambdify((a, b, c), a + b + c, 'math')\n    g = lambdify(d, a + b + c, 'math')\n    assert f(1, 2, 3) == g({'a': 1, 'b': 2, 'c': 3})\n"], "sample_1029": ["def test_srepr_Predicate():\n    sT(Q.is_integer, \"Predicate('is_integer')\")\n    sT(Q.is_real, \"Predicate('is_real')\")\n", "def test_srepr_AppliedPredicate():\n    P = Predicate('P')\n    expr = P(x)\n    string = \"AppliedPredicate(Predicate('P'), Symbol('x'))\"\n    assert srepr(expr) == string\n    assert eval(string, ENV) == expr\n    assert isinstance(eval(string, ENV), AppliedPredicate)\n    assert eval(string, ENV).func == P\n    assert eval(string, ENV).arg == x\n", "def test_srepr_poly_ring():\n    R = ring('x,y,z', ZZ, lex)\n    sT(R, \"PolyRing((x, y, z), ZZ, lex)\")\n    R = ring('x,y,z', QQ, grlex)\n    sT(R, \"PolyRing((x, y, z), QQ, grlex)\")\n", "def test_srepr_AppliedPredicate():\n    p = Predicate(\"P\")\n    expr = p(x)\n    sT(expr, 'AppliedPredicate(Predicate(\"P\"), Symbol(\"x\"))')\n", "def test_srepr_PolyRing():\n    x, y = symbols('x,y')\n    poly_ring = ring(x, y, ZZ, lex)\n    sT(poly_ring, \"PolyRing((x, y), ZZ, lex)\")\n"], "sample_1028": ["def test_Mod_eval():\n    # test for when both p and q are numbers\n    assert Mod(5, 2) == 1\n    assert Mod(-1, 4) == 3\n    assert Mod(2, 3).evalf() == 2\n    assert Mod(S.Zero, 2) == 0\n    assert Mod(2, S.One) == 0\n    assert Mod(10, 3).evalf() == 1\n    assert Mod(S.Infinity, 4) == nan\n    assert Mod(S.NegativeInfinity, 4) == nan\n    assert Mod(nan, 4) == nan\n    assert Mod(2, nan) == nan\n\n    # test for when p is Add\n    assert Mod(x+5, 3) == Mod(x+2, 3)\n    assert Mod(x+5, 3).subs(x, 4) == 1\n\n    # test for when p is Mul\n    assert Mod(2*x, 4) == 2*Mod(x, 2)\n    assert Mod(2*x, 4).subs(x, 5) == 2\n\n    # test for when q is positive and 2|q| < p < 2|q|\n    assert Mod(x, 4).subs(x, 9) == 1\n    assert Mod(x, 4).subs(x, -9) == 3\n\n    # test for when p is a simple rational multiple of q\n    assert Mod(2*x, 4*x) == 0\n\n    # test for when p and q have a gcd\n    assert Mod(6*x, 8*x) == 2*x*Mod(3, 4)\n\n    # test for some edge cases\n    assert Mod(S.Zero, S.Zero) == nan\n    assert Mod(S.Infinity, S.Infinity) == nan\n    assert Mod(S.NegativeInfinity, S.Infinity) == nan\n    assert Mod(S.Infinity, S.NegativeInfinity) == nan\n", "def test_Mod_eval():\n    from sympy.abc import x, y\n    assert Mod(x, 1) == 0\n    assert Mod(x, -1) == 0\n    assert Mod(x, 0) is nan  # raises ZeroDivisionError?\n    assert Mod(2, 3) == 2\n    assert Mod(-2, 3) == 1\n    assert Mod(5*x, 3) == Mod(2*x, 3)\n    assert Mod(x, x) == 0\n    assert Mod(0, x) == 0\n    assert Mod(x, y) == Mod(x, y)\n    assert Mod(x, x+y) == Mod(x, x+y)\n    assert Mod(x, 2).subs(x, 1) == 1\n    assert Mod(x, 2*y).subs(x, 1) == Mod(1, 2*y)\n    assert Mod(x, y) != Mod(x, y+1)\n", "def test_Mod_eval():\n    # test for Mod.eval with various inputs\n    assert Mod(x**2, y).subs({x: 5, y: 6}) == 1\n    assert Mod(x**2, y).subs({x: 6, y: 5}) == 1\n    assert Mod(x**2, y).subs({x: 5.5, y: 6}) == 0.25\n    assert Mod(x**2, y).subs({x: 6, y: 5.5}) == 0.25\n    assert Mod(x**2, 6).subs({x: 5}) == 1\n    assert Mod(x**2, 6).subs({x: 6}) == 0\n    assert Mod(5, 6) == 5\n    assert Mod(6, 5) == 1\n    assert Mod(5.5, 6) == 5.5\n    assert Mod(6, 5.5) == 0.5\n    assert Mod(1, 1) == 0\n    assert Mod(0, 1) == 0\n    assert Mod(1, 0) == nan  # Modulo by zero\n    assert Mod(nan, 1) == nan  # Modulo with nan\n    assert Mod(1, nan) == nan  # Modulo with nan\n", "def test_Mod_eval():\n    # test for reduction of Mod\n    assert Mod(5, 3) == 2\n    assert Mod(5, -3) == -1\n    assert Mod(-5, 3) == 1\n    assert Mod(-5, -3) == -2\n    assert Mod(x, 1) == 0\n    assert Mod(x, -1) == 0\n    assert Mod(x, 0) is nan\n\n    # test for handling of even/odd inputs\n    assert Mod(3*x, 2) == x % 2\n    assert Mod(6*x, 2) == 0\n    assert Mod(6*x + 1, 2) == x % 2\n    assert Mod(7*x, 3) == (7*x) % 3\n\n    # test for handling of coefficients\n    assert Mod(2*x, 2) == 0\n    assert Mod(2*x + 1, 2) == 1\n    assert Mod(3*x, 3) == 0\n    assert Mod(3*x + 1, 3) == x % 3 + 1\n\n    # test for denesting of Mod\n    assert Mod(Mod(x, 3) + 1, 3) == (x + 1) % 3\n    assert Mod(Mod(x, 3) + 3, 3) == x % 3\n    assert Mod(Mod(x, 3), 3) == x % 3\n", "def test_Mod_eval():\n    assert Mod(5, 3) == 2\n    assert Mod(-5, 3) == 1\n    assert Mod(5, -3) == -1\n    assert Mod(-5, -3) == -2\n    assert Mod(0, 3) == 0\n    assert Mod(3, 3) == 0\n    assert Mod(-3, 3) == 0\n    assert Mod(3, -3) == 0\n    assert Mod(-3, -3) == 0\n    assert Mod(5, 1) == 0\n    assert Mod(-5, 1) == 0\n    assert Mod(5, -1) == 0\n    assert Mod(-5, -1) == 0\n    assert Mod(0, 0) == nan\n    assert Mod(5, 0) == nan\n    assert Mod(-5, 0) == nan\n"], "sample_1027": ["def test_Poly_free_symbols_in_domain():\n    f = Poly(x**2 + y*x + 1, x)\n    assert f.free_symbols_in_domain == {y}\n    f = Poly(x**2 + 1, x)\n    assert f.free_symbols_in_domain == set()\n    f = Poly(x**2 + y, x)\n    assert f.free_symbols_in_domain == {y}\n", "def test_poly_Poly():\n    assert Poly(Poly(x, x), x) == Poly(x, x)\n    assert Poly(Poly(x**2 + 1, x), x) == Poly(x**2 + 1, x)\n    assert Poly(Poly(x*y + 2, x, y), x) == Poly(x*y + 2, x, y)\n", "def test_Poly_from_expr_noisy_term():\n    assert Poly(x + (1e-10, 1e-10)*y, x, y).as_expr() == x\n", "def test_Poly_neg():\n    assert _strict_eq(-Poly(x**2 + 2*x - 1, x), Poly(-x**2 - 2*x + 1, x))\n", "def test_Poly_map_coeffs():\n    p = Poly(x**3 + 2*x**2 + 3*x + 4, x)\n    assert p.map_coeffs(lambda c: 2*c) == Poly(2*x**3 + 4*x**2 + 6*x + 8, x)\n    assert p.map_coeffs(lambda c: c + 1) == Poly(x**3 + 3*x**2 + 4*x + 5, x)\n"], "sample_1031": ["def test_Quantity_scale_factor():\n    q1 = Quantity('q1')\n    q1.set_dimension(length)\n    q1.set_scale_factor(2 * m)\n\n    assert q1.scale_factor == 2\n    assert q1.dimension == length\n\n    q2 = Quantity('q2')\n    q2.set_dimension(mass)\n    q2.set_scale_factor(kg / 2)\n\n    assert q2.scale_factor == Rational(1, 2)\n    assert q2.dimension == mass\n", "def test_quantity_scale_factors():\n    # Test that scale factors are correctly set for Quantity instances\n    assert c.scale_factor == 299792458\n    assert kg.scale_factor == 1\n    assert m.scale_factor == 1\n    assert s.scale_factor == 1\n\n    # Test that scale factors are correctly calculated for derived units\n    newton = Quantity(\"newton\")\n    newton.set_dimension(mass*length/time**2)\n    newton.set_scale_factor(kg*m/s**2)\n    assert newton.scale_factor == 1\n", "def test_quantity_scale_factors():\n    # Check that scale factors are correctly set\n    assert c.scale_factor == 299792458\n    assert kg.scale_factor == 1\n    assert m.scale_factor == 1\n    assert s.scale_factor == 1\n\n    # Check that scale factors are correctly converted to SI base units\n    assert Quantity.get_dimensional_expr(c.scale_factor) == 1\n    assert Quantity.get_dimensional_expr(kg.scale_factor) == 1\n    assert Quantity.get_dimensional_expr(m.scale_factor) == 1\n    assert Quantity.get_dimensional_expr(s.scale_factor) == 1\n", "def test_quantity_dimension_check():\n    # Test that scale factors are checked for dimension consistency\n    with raises(ValueError):\n        Quantity(\"invalid\", abbrev=\"inv\")\n        Quantity.SI_quantity_scale_factors[\"inv\"] = m\n        Quantity.SI_quantity_dimension_map[\"inv\"] = Dimension(current)\n        Quantity._check_consistency()\n", "def test_UnitSystem():\n    # make sure units are added to the unit system\n    u = UnitSystem(\"u\", (m, s, kg))\n    assert u.base_units == (m, s, kg)\n\n    # make sure dimensions are correctly deduced\n    assert u.get_dimension(m) == length\n    assert u.get_dimension(s) == time\n    assert u.get_dimension(kg) == mass\n    assert u.get_dimension(c) == velocity\n\n    # check unit system with derived units\n    u2 = UnitSystem(\"u2\", (m, s, kg, c))\n    assert u2.base_units == (m, s, kg)\n    assert u2.derived_units == (c,)\n    assert u2.get_dimension(c) == velocity\n\n    # check on Quantities\n    q1 = Quantity(\"q1\")\n    q1.set_dimension(action)\n    q1.set_scale_factor(Rational(1, 2)*m**2/s)\n    q2 = Quantity(\"q2\")\n    q2.set_dimension(current)\n    q2.set_scale_factor(Rational(1, 2)*s/m)\n\n    u3 = UnitSystem(\"u3\", (m, s, q1, q2))\n    assert u3.base_units == (m, s)\n    assert u3.derived_units == (q1, q2)\n\n    dim = u3.get_dimension(q1)\n    assert dim == action\n    assert dim == u3.get_dimension(m**2/s)\n    assert dim == u3.get_dimension(m/s * m)\n\n    dim = u3.get_dimension(q2)\n    assert dim == current\n    assert dim == u3.get_dimension(1/m * s)\n    assert dim == u3.get_dimension(s/m)\n\n    with raises(ValueError):\n        UnitSystem(\"u4\", (m, s, kg, kg**2))\n"], "sample_1030": ["def test_idiff():\n    x = Symbol('x')\n    y = Symbol('y')\n    eq = x**2 + y**2 - 4\n    assert idiff(eq, y, x) == -x/y\n    assert idiff(eq, y, x, 2) == -(x**2 + y**2)/y**3\n    a = Symbol('a')\n    eq = x + a + y\n    assert idiff(eq, y, x) == -1\n    assert idiff(eq, [y, a], x) == -Derivative(a, x) - 1\n", "def test_util_centroid():\n    p1, p2, p3 = Point(0, 0), Point(10, 0), Point(10, 10)\n    p = Polygon(p1, p2, p3)\n    assert centroid(p) == Point(20/3, 10/3)\n\n    p1, p2, p3 = Point2D(0, 0), Point2D(10, 0), Point2D(10, 10)\n    p = Polygon(p1, p2, p3)\n    assert centroid(p) == Point(20/3, 10/3)\n\n    q = p.translate(0, 20)\n    assert centroid(p, q) == Point(20/3, 40/3)\n", "def test_convex_hull():\n    p1 = Point(0, 0)\n    p2 = Point(1, 1)\n    p3 = Point(2, 0)\n    p4 = Point(1, -1)\n    p5 = Point(-1, 1)\n    p6 = Point(-1, -1)\n\n    # test convex hull for a collection of points\n    assert convex_hull(p1, p2, p3, p4, p5, p6) == Polygon(p6, p1, p5, p2, p3, p4)\n\n    # test convex hull for a collection of 2D entities\n    s1 = Segment(p1, p2)\n    s2 = Segment(p2, p3)\n    s3 = Segment(p3, p1)\n    assert convex_hull(s1, s2, s3) == Polygon(p1, p2, p3)\n\n    # test convex hull for a collection of 2D entities and points\n    assert convex_hull(s1, s2, p3, p4) == Polygon(p1, p2, p3, p4)\n\n    # test convex hull for a collection of overlapping entities\n    assert convex_hull(Polygon(p1, p2, p3), Polygon(p2, p3, p4)) == Polygon(p1, p2, p3, p4)\n", "def test_idiff():\n    x = Symbol('x')\n    y = Symbol('y')\n    eq = x**2 + y**2 - 4\n    assert idiff(eq, y, x) == -x/y\n    assert idiff(eq, y, x, 2).simplify() == -(x**2 + y**2)/y**3\n\n    a = Symbol('a')\n    eq = x + a + y\n    assert idiff(eq, y, x) == -1\n    assert idiff(eq, [y, a], x) == -Derivative(a, x) - 1\n", "def test_convex_hull():\n    # a square\n    p = [(0, 0), (0, 1), (1, 1), (1, 0)]\n    assert convex_hull(*p) == Polygon(Point2D(0, 0), Point2D(0, 1), Point2D(1, 1), Point2D(1, 0))\n\n    # a square with an interior point\n    p = [(0, 0), (0, 1), (1, 1), (1, 0), (0.5, 0.5)]\n    assert convex_hull(*p) == Polygon(Point2D(0, 0), Point2D(0, 1), Point2D(1, 1), Point2D(1, 0))\n\n    # a non-convex shape\n    p = [(0, 0), (0, 1), (0.5, 0.5), (1, 1), (1, 0)]\n    assert convex_hull(*p) == Polygon(Point2D(0, 0), Point2D(0, 1), Point2D(1, 1), Point2D(1, 0))\n"], "sample_1032": ["def test_real_root():\n    assert real_root(4, 2) == 2\n    assert real_root(-4, 2) == -2\n    assert real_root(-4, 3) == real_root(-4, 3)\n    assert real_root(-4, 3, evaluate=False) == real_root(-4, 3)\n    assert real_root(4, 2, evaluate=False) == 2\n\n    x = Symbol('x')\n    assert real_root(x**2, 2) == abs(x)\n    assert real_root(x**3, 3) == real_root(x**3, 3)\n    assert real_root(x**4, 4) == abs(x)\n\n    # check that we don't accidentally pick up other Abs\n    assert (real_root(1 + x**2, 2) + real_root(1 + x**3, 3)).subs(x, -1) == \\\n        (real_root(2, 2) + real_root(-0, 3))\n", "def test_real_root():\n    assert real_root(-8, 3) == -2\n    assert real_root(8, 3) == 2\n    assert real_root(-2, 2) == I*sqrt(2)\n    assert real_root(2, 2) == sqrt(2)\n    assert real_root(-1, 5) == root(-1, 5, 5//2)\n    assert real_root(1, 5) == 1\n    assert real_root(-32, 5) == -2\n    assert real_root(32, 5) == 2\n    assert real_root(-2, 3) == root(-2, 3, 3//2)\n    assert real_root(2, 3) == cbrt(2)\n\n    x = Symbol('x')\n    assert real_root(x, 3) == Pow(x, Rational(1, 3))\n    assert real_root(x, 5) == root(x, 5)\n    assert real_root(-x, 5) == root(-x, 5, 5//2)\n\n    assert real_root(1 + 2*I, 2) == sqrt(1 + 2*I)\n    assert real_root(-2 + 2*I, 2) == sqrt(-2 + 2*I)\n", "def test_real_root():\n    assert real_root(-8, 3) == -2\n    assert real_root(8, 3) == 2\n    assert real_root(-8, 4) == 2*I\n    x = Symbol('x', real=True)\n    assert real_root(x**3, 3) == x\n    assert real_root((x+1)**3, 3) == x + 1\n    assert real_root(2, 3) == Pow(2, Rational(1, 3), evaluate=False)\n    assert real_root(-2, 3) == Pow(-2, Rational(1, 3), evaluate=False)\n    y = Symbol('y', positive=True)\n    assert real_root(-16*y, 4) == -2*y**(S(1)/4)\n    assert real_root(-16*y, 4) != 2*I*y**(S(1)/4)\n", "def test_root():\n    x = Symbol('x')\n    n = Symbol('n')\n    assert sqrt(x) == root(x, 2)\n    assert cbrt(x) == root(x, 3)\n    assert root(x, -n) == 1/root(x, n)\n    assert root(x, n, 2) == -(-root(x, n))\n    assert root(x, n, 3) == -(-(-root(x, n)))\n    assert root(x, 1, 2) == -x\n    assert root(x, 1, 1) == x\n    assert root(x, -1, 1) == 1/x\n    assert root(4, 2) == 2\n    assert root(-4, 2) == 2*I\n    assert root(-4, 2, 1) == -2*I\n    assert root(8, 3) == 2\n    assert root(-8, 3) == 2*(-1)**(1/3)\n    assert root(-8, 3, 1) == 2*(-1)**(1/3)*(-1)\n", "def test_MinMaxBase_is_connected():\n    x, y = Symbol('x', real=True), Symbol('y', real=True)\n    assert MinMaxBase._is_connected(x + 2, x + 3) is True\n    assert MinMaxBase._is_connected(x + 2, x - 2) is None\n    assert MinMaxBase._is_connected(x + 2, x + y) is None\n    assert MinMaxBase._is_connected(x - 2, x - 3) is True\n    assert MinMaxBase._is_connected(x - 2, x + 2) is None\n    assert MinMaxBase._is_connected(x - 2, x - y) is None\n"], "sample_1033": ["def test_Add_evalf():\n    assert same_and_same_prec(Add(1, 2).evalf(), 3)\n    assert same_and_same_prec(Add(x, 2).evalf(subs={x: 1}), 3)\n    assert same_and_same_prec(Add(x, 2, evaluate=False).evalf(subs={x: 1}), 3)\n    assert same_and_same_prec(Add(1, Float('1.1')).evalf(), Float('2.1'))\n    assert same_and_same_prec(Add(1, x).evalf(subs={x: Float('1.1')}), Float('2.1'))\n    assert same_and_same_prec(Add(1, Float('1.1', 3)).evalf(), Float('2.1', 3))\n    assert same_and_same_prec(Add(1, x).evalf(subs={x: Float('1.1', 3)}), Float('2.1', 3))\n    assert same_and_same_prec(Add(Float('1.0', 4), Float('1.1', 3)).evalf(), Float('2.1', 3))\n    assert same_and_same_prec(Add(x, Float('1.1', 3)).evalf(subs={x: Float('1.0', 4)}), Float('2.1', 3))\n", "def test_add_extract_leading_order():\n    assert Add(4*x**2 + 2*x + 3*x + 4*x**2).extract_leading_order(x) == ((8*x**2, O(x**2)),)\n    assert Add(x + 1 + 1/x**5).extract_leading_order(x) == ((x**(-5), O(x**(-5))),)\n    assert Add(x + x**2).extract_leading_order(x) == ((x, O(x)),)\n", "def test_Add_removeO():\n    a = Symbol('a')\n    r = Add(a, S(1), O(a))\n    assert r.removeO() == a + 1\n", "def test_Add_as_coeff_add():\n    assert Add(2, 3*x, evaluate=False).as_coeff_add(*[x]) == (2, (3*x,))\n    assert Add(2*x, 3*x**2, evaluate=False).as_coeff_add(*[x]) == (0, (2*x, 3*x**2))\n    assert Add(x, sqrt(2)*x, evaluate=False).as_coeff_add(*[x]) == (0, (x, sqrt(2)*x))\n    assert Add(2, 3*x, 4*y, evaluate=False).as_coeff_add(*[x]) == (2, (3*x, 4*y))\n    assert Add(2, 3*x, 4*y, evaluate=False).as_coeff_add(*[x, y]) == (2, (3*x, 4*y))\n", "def test_add_removeO():\n    from sympy import O\n    assert Add(x, O(x)) == Add(x, O(x))\n    assert Add(x, O(x)).removeO() == x\n    assert Add(x, O(x)).getO() == O(x)\n    assert Add(x, y, O(x), O(y)).removeO() == x + y\n    assert Add(x, y, O(x), O(y)).getO() == O(x) + O(y)\n"], "sample_1034": ["def test_grover_iteration():\n    numqubits = 2\n    basis_states = superposition_basis(numqubits)\n    f = return_one_on_two\n    v = OracleGate(numqubits, f)\n    iterated = grover_iteration(basis_states, v)\n    result = qapply(iterated)\n    assert result == IntQubit(2)\n", "def test_WGate_apply():\n    wgate = WGate(2)\n    basis_states = superposition_basis(2)\n    result = qapply(wgate*basis_states)\n    assert result == basis_states\n", "def test_WGate_apply():\n    wgate = WGate(2)\n    state = superposition_basis(2)\n    result = qapply(wgate*state)\n    assert result == state\n", "def test_WGate_apply():\n    wgate = WGate(2)\n    state = superposition_basis(2)\n    result = qapply(wgate*state)\n    assert result == state\n", "def test_oracle_gate_v_repr():\n    v = OracleGate(2, return_one_on_two)\n    # The matrix representation of the oracle when applied to the standard basis\n    v_mat = represent(v, nqubits=2)\n    # The output of the oracle on the third basis state |2should have a -1 in front\n    assert v_mat[2, 2] == -1\n"], "sample_1035": ["def test_measure_all():\n    # Test measure_all on a simple case\n    q = IntQubit(1, nqubits=2)\n    result = q.measure_all()\n    assert len(result) == 1\n    assert result[0][0] == q\n    assert result[0][1] == 1\n    # Test measure_all on a superposition\n    q = qapply(H(0)*H(1)*IntQubit(0, nqubits=2))\n    result = q.measure_all()\n    assert len(result) == 4\n    for state, prob in result:\n        assert prob == 1/4\n", "def test_grover_iterations():\n    # Make sure the number of iterations is correct\n    nqubits = 2\n    num_iterations = 1\n    oracle = OracleGate(return_one_on_two, nqubits)\n    iterations = grover_iteration(oracle, nqubits, num_iterations)\n    assert len(iterations) == num_iterations + 1\n\n    nqubits = 3\n    num_iterations = 3\n    oracle = OracleGate(return_one_on_one, nqubits)\n    iterations = grover_iteration(oracle, nqubits, num_iterations)\n    assert len(iterations) == num_iterations + 1\n", "def test_apply_grover():\n    # Test the full grover algorithm\n    nqubits = 2\n    target = IntQubit(2, nqubits=nqubits)\n    oracle = OracleGate(nqubits, return_one_on_two)\n    initial_state = superposition_basis(nqubits)\n    result = qapply(apply_grover(initial_state, oracle, nqubits))\n    assert result == -IntQubit(2, nqubits=nqubits)/2 +  IntQubit(0, nqubits=nqubits)/2 + IntQubit(1, nqubits=nqubits)/2 + IntQubit(3, nqubits=nqubits)/2\n", "def test_measure_all():\n    q = IntQubit(2, nqubits=2)\n    result = q.measure_all()\n    assert len(result) == 1\n    assert result[0][0] == q\n    assert result[0][1] == 1\n\n    # create superposition\n    superposition = apply_grover(q, iterations=1)\n    result = superposition.measure_all()\n    assert len(result) == 4\n    assert sum([prob for state, prob in result]) == 1\n", "def test_measure_all():\n    # Ensure that measure_all returns the correct states with the correct probabilities\n    q = IntQubit(0, nqubits=2).H(0).H(1)\n    q = qapply(q)\n    result = q.measure_all()\n    assert len(result) == 4\n    for state, prob in result:\n        assert prob == 1/4\n"], "sample_1036": ["def test_Mul_doit():\n    assert Mul(1, 2, evaluate=False).doit() == 2\n    assert Mul(1, 2, evaluate=False).doit(deep=False) == 2\n    assert Mul(1, 2, evaluate=False).doit(deep=True) == 2\n\n    assert Mul(x, 2, evaluate=False).doit() == 2*x\n    assert Mul(x, 2, evaluate=False).doit(deep=False) == 2*x\n    assert Mul(x, 2, evaluate=False).doit(deep=True) == 2*x\n\n    class NonCommutative(Basic):\n        pass\n\n    nc = NonCommutative()\n    assert Mul(nc, nc, evaluate=False).doit() == nc*nc\n    assert Mul(nc, nc, evaluate=False).doit(deep=False) == nc*nc\n    assert Mul(nc, nc, evaluate=False).doit(deep=True) == nc*nc\n", "def test_Mul_doit():\n    x = symbols('x')\n    A11, A12, A21, A22 = symbols('A11 A12 A21 A22')\n    B11, B12, B21, B22 = symbols('B11 B12 B21 B22')\n    A = Matrix(2, 2, [A11, A12, A21, A22])\n    B = Matrix(2, 2, [B11, B12, B21, B22])\n    assert Mul(A, B).doit() == A * B\n    assert Mul(x, A).doit() == x * A\n    assert Mul(x, A, B).doit() == x * A * B\n    assert Mul(x, A, B, evaluate=False).doit() == x * A * B\n", "def test_mul_matmul():\n    assert (2*A)*B == 2*(A*B)\n    assert 2*(A*B) == (2*A)*B\n    assert (2*A)*B*2 == 4*A*B\n    assert (2*A).T*B == 2*(A.T*B)\n    assert (2*A).T*B*2 == 4*A.T*B\n", "def test_Mul_doit():\n    assert Mul(0, 1).doit() == 0\n    assert Mul(0, 1, evaluate=False).doit() == 0\n    assert Mul(0.0, 1, evaluate=False).doit() == 0.0\n    assert Mul(1, 2, evaluate=False).doit() == 2\n    assert Mul(0, 1, evaluate=False).doit(deep=False) == Mul(0, 1, evaluate=False)\n    assert Mul(1, 2, evaluate=False).doit(deep=False) == Mul(1, 2, evaluate=False)\n", "def test_Mul_doit():\n    x = Symbol('x')\n    assert Mul(x, x, evaluate=False).doit() == x**2\n    assert Mul(x, 2, evaluate=False).doit() == 2*x\n    assert Mul(x, x, 2, evaluate=False).doit() == 2*x**2\n"], "sample_1037": ["def test_doit():\n    a, b = symbols('a b')\n    X = MatrixSymbol('X', 2, 2)\n    Y = MatrixSymbol('Y', 2, 2)\n    assert MatMul(a, X).doit() == a*X\n    assert MatMul(X, a).doit() == a*X\n    assert MatMul(a, X, b).doit() == a*b*X\n    assert MatMul(a, X, Y).doit() == a*X*Y\n    assert (MatMul(2, X, 3, Y).doit() == 6*X*Y).doit()\n    assert MatMul(a, Identity(2)).doit() == a*Identity(2)\n    assert MatMul(a, b, Identity(2)).doit() == a*b*Identity(2)\n    assert MatMul(Identity(2), a).doit() == a*Identity(2)\n    assert MatMul(Identity(2), a, Identity(2)).doit() == a*Identity(2)\n", "def test_only_squares():\n    raises(RuntimeError, lambda: only_squares(A, B))\n    assert only_squares(C, D) == [MatMul(C, D).doit()]\n", "def test_matmul_doit():\n    A = Matrix([[1, 2], [3, 4]])\n    B = Matrix([[5, 6], [7, 8]])\n    X = MatrixSymbol('X', 2, 2)\n    assert MatMul(A, B).doit() == A * B\n    assert MatMul(A, X).doit() == MatMul(A, X)\n    assert MatMul(X, A).doit() == MatMul(X, A)\n    assert MatMul(A, A.inv()).doit() == Identity(2)\n", "def test_MatMul_refine():\n    # This tests that refine function works with MatMul\n    from sympy import Q, refine, assuming\n    X = MatrixSymbol('X', 2, 2)\n    expr = X * X.T\n    assert expr.refine(Q.orthogonal(X)) == Identity(2)\n    with assuming(Q.orthogonal(X)):\n        assert refine(expr) == Identity(2)\n    Y = MatrixSymbol('Y', 2, 3)\n    assert refine(expr + Y) == expr + Y\n", "def test_MatMul_doit():\n    A1 = Matrix([[1, 2], [3, 4]])\n    A2 = Matrix([[5, 6], [7, 8]])\n    A3 = Matrix([[9, 10], [11, 12]])\n    assert MatMul(A, B).doit() == MatMul(A, B)\n    assert MatMul(A1, A2).doit() == A1*A2\n    assert MatMul(A1, A2, A3).doit() == A1*A2*A3\n    assert MatMul(A1, A2, A3, A1).doit() == A1*A2*A3*A1\n    assert MatMul(A, transpose(A)).doit() == MatMul(A, transpose(A))\n    assert MatMul(A1, transpose(A2)).doit() == A1*A2.T\n    assert (MatMul(A, B)*MatMul(B, C)).doit() == A*B*C\n    assert MatMul(Identity(n), A).doit() == A\n    assert MatMul(A, Identity(m)).doit() == A\n"], "sample_1038": ["def test_matrix_derivative():\n    from sympy import MatrixSymbol\n    from sympy.matrices.expressions.matexpr import _matrix_derivative\n    X = MatrixSymbol(\"X\", 2, 2)\n    A = MatrixSymbol(\"A\", 2, 2)\n    B = MatrixSymbol(\"B\", 2, 2)\n    expr = A * X * B\n    assert _matrix_derivative(expr, X) == A.T*B\n", "def test_MatrixExpr_from_index_summation():\n    from sympy import Sum, symbols, MatrixSymbol\n    from sympy.abc import i, j, k, l, N\n    A = MatrixSymbol(\"A\", N, N)\n    B = MatrixSymbol(\"B\", N, N)\n\n    # Contracted indices\n    expr = Sum(A[i, j]*B[j, k], (j, 0, N-1))\n    result = MatrixExpr.from_index_summation(expr)\n    assert result == A*B\n\n    # Transposed contraction\n    expr = Sum(A[j, i]*B[j, k], (j, 0, N-1))\n    result = MatrixExpr.from_index_summation(expr)\n    assert result == A.T*B\n\n    # Trace\n    expr = Sum(A[i, i], (i, 0, N-1))\n    result = MatrixExpr.from_index_summation(expr)\n    assert result == A.trace()\n\n    # Two sums with Kronecker Delta\n    expr = Sum(A[i, j]*B[k, j]*A[l, k], (j, 0, N-1), (k, 0, N-1))\n    result = MatrixExpr.from_index_summation(expr)\n    assert result == A*B.T*A.T\n", "def test_MatrixExpr_as_explicit():\n    A = MatrixSymbol('A', 3, 3)\n    assert A.as_explicit() == ImmutableMatrix([[A[0, 0], A[0, 1], A[0, 2]],\n                                               [A[1, 0], A[1, 1], A[1, 2]],\n                                               [A[2, 0], A[2, 1], A[2, 2]]])\n", "def test_MatrixElement():\n    i, j = symbols('i j')\n    M = MatrixSymbol('M', 3, 4)\n    assert MatrixElement(M, 0, 0) == M[0, 0]\n    assert MatrixElement(M, i, j).subs({i: 1, j: 2}) == M[1, 2]\n    assert MatrixElement(M, 1, 2).subs(M, zeros(3, 4)) == 0\n", "def test_matrix_element():\n    i, j = symbols('i j')\n    x = MatrixSymbol('x', 2, 2)\n    assert MatrixElement(x, 0, 0) == x[0, 0]\n    assert MatrixElement(x, i, j) == x[i, j]\n\n    assert MatrixElement(x, 0, 0).subs(x, [[1, 2], [3, 4]]) == 1\n    assert MatrixElement(x, 1, 0).subs(x, [[1, 2], [3, 4]]) == 3\n    assert MatrixElement(x, i, j).subs({x: [[1, 2], [3, 4]], i: 1, j: 0}) == 3\n\n    assert MatrixElement(x, i, j).doit() == MatrixElement(x, i, j)\n    assert MatrixElement(x, 1, 0).subs(x, [[1, 2], [3, 4]]).doit() == 3\n\n    assert diff(MatrixElement(x, i, j), x) == KroneckerDelta(i, 0)*KroneckerDelta(j, 0)\n    assert diff(MatrixElement(x, 1, 1), x) == 0\n"], "sample_1039": ["def test_mathml_printing_of_rational():\n    expr = Rational(1, 2)\n    assert mp.doprint(expr) == '<apply><divide/><cn>1</cn><cn>2</cn></apply>'\n    assert mpp.doprint(expr) == '<mfrac><mn>1</mn><mn>2</mn></mfrac>'\n", "def test_MathMLPrinter_settings():\n    mp = MathMLContentPrinter({'root_notation': False})\n    expr = x**Rational(1, 2)\n    expected = '<apply><power/><ci>x</ci><cn>1/2</cn></apply>'\n    assert mp.doprint(expr) == expected\n\n    mp = MathMLContentPrinter({'root_notation': True})\n    expected = '<apply><root/><degree><cn>2</cn></degree><ci>x</ci></apply>'\n    assert mp.doprint(expr) == expected\n", "def test_printing_mathml_functions():\n    # Testing printing of math functions\n    assert mp.doprint(sin(x)) == '<apply><sin/><ci>x</ci></apply>'\n    assert mpp.doprint(sin(x)) == '<mrow><mi>sin</mi><mfenced><mi>x</mi></mfenced></mrow>'\n\n    assert mp.doprint(cos(x)) == '<apply><cos/><ci>x</ci></apply>'\n    assert mpp.doprint(cos(x)) == '<mrow><mi>cos</mi><mfenced><mi>x</mi></mfenced></mrow>'\n\n    assert mp.doprint(tan(x)) == '<apply><tan/><ci>x</ci></apply>'\n    assert mpp.doprint(tan(x)) == '<mrow><mi>tan</mi><mfenced><mi>x</mi></mfenced></mrow>'\n\n    assert mp.doprint(asin(x)) == '<apply><arcsin/><ci>x</ci></apply>'\n    assert mpp.doprint(asin(x)) == '<mrow><mi>arcsin</mi><mfenced><mi>x</mi></mfenced></mrow>'\n\n    assert mp.doprint(acos(x)) == '<apply><arccos/><ci>x</ci></apply>'\n    assert mpp.doprint(acos(x)) == '<mrow><mi>arccos</mi><mfenced><mi>x</mi></mfenced></mrow>'\n\n    assert mp.doprint(atan(x)) == '<apply><arctan/><ci>x</ci></apply>'\n    assert mpp.doprint(atan(x)) == '<mrow><mi>arctan</mi><mfenced><mi>x</mi></mfenced></mrow>'\n\n    assert mp.doprint(sinh(x)) == '<apply><sinh/><ci>x</ci></apply>'\n    assert mpp.doprint(sinh(x)) == '<mrow><mi>sinh</mi><mfenced><mi>x</mi></mfenced></mrow>'\n\n    assert mp.doprint(cosh(x)) == '<apply><cosh/><ci>x</ci></apply>'\n    assert mpp.doprint(cosh(x)) == '<mrow><mi>cosh</mi><mfenced><mi>x</mi></mfenced></mrow>'\n\n    assert", "def test_mathml_functions():\n    # For each function, test both content and presentation MathML\n    m = Symbol('m')\n\n    # Test trig functions\n    assert mp.doprint(sin(x)) == '<apply><sin/><ci>x</ci></apply>'\n    assert mpp.doprint(sin(x)) == '<mrow><mi>sin</mi><mo data-mjx-texclass=\"NONE\">\u2061</mo><mfenced><mi>x</mi></mfenced></mrow>'\n\n    assert mp.doprint(cos(x)) == '<apply><cos/><ci>x</ci></apply>'\n    assert mpp.doprint(cos(x)) == '<mrow><mi>cos</mi><mo data-mjx-texclass=\"NONE\">\u2061</mo><mfenced><mi>x</mi></mfenced></mrow>'\n\n    assert mp.doprint(tan(x)) == '<apply><tan/><ci>x</ci></apply>'\n    assert mpp.doprint(tan(x)) == '<mrow><mi>tan</mi><mo data-mjx-texclass=\"NONE\">\u2061</mo><mfenced><mi>x</mi></mfenced></mrow>'\n\n    assert mp.doprint(asin(x)) == '<apply><arcsin/><ci>x</ci></apply>'\n    assert mpp.doprint(asin(x)) == '<mrow><mi>arcsin</mi><mo data-mjx-texclass=\"NONE\">\u2061</mo><mfenced><mi>x</mi></mfenced></mrow>'\n\n    assert mp.doprint(acos(x)) == '<apply><arccos/><ci>x</ci></apply>'\n    assert mpp.doprint(acos(x)) == '<mrow><mi>arccos</mi><mo data-mjx-texclass=\"NONE\">\u2061</mo><mfenced><mi>x</mi></mfenced></mrow>'\n\n    assert mp.doprint(atan(x)) == '<apply><arctan/><ci>x</ci></apply>'\n    assert mpp.doprint(atan(x)) == '<mrow><mi>arctan</mi><mo data-mjx-texclass=\"NONE\">\u2061</mo><mfenced><mi>x</mi></mfenced></mrow>'\n\n    # Test hyperbolic trig functions\n    assert mp", "def test_mathml_content_printers():\n    # Numbers\n    assert mp._print(Integer(1)) == mp.dom.createElement('cn')\n    assert mp._print(Integer(1)).getElementsByTagName('cn')[0].firstChild.data == '1'\n\n    assert mp._print(Rational(1, 2)) == mp.dom.createElement('apply')\n    assert mp._print(Rational(1, 2)).getElementsByTagName('divide')[0].tagName == 'divide'\n    assert mp._print(Rational(1, 2)).getElementsByTagName('cn')[0].firstChild.data == '1'\n    assert mp._print(Rational(1, 2)).getElementsByTagName('cn')[1].firstChild.data == '2'\n\n    assert mp._print(Float(2.5)) == mp.dom.createElement('cn')\n    assert mp._print(Float(2.5)).getElementsByTagName('cn')[0].firstChild.data == '2.5'\n\n    # Infinity\n    assert mp._print(oo) == mp.dom.createElement('infinity')\n    assert mp._print(-oo) == mp.dom.createElement('apply')\n    assert mp._print(-oo).getElementsByTagName('minus')[0].tagName == 'minus'\n\n    # Symbol\n    assert mp._print(x) == mp.dom.createElement('ci')\n    assert mp._print(x).getElementsByTagName('ci')[0].firstChild.data == 'x'\n\n    # MatrixSymbol\n    A = MatrixSymbol('A', 2, 2)\n    assert mp._print(A) == mp.dom.createElement('ci')\n    assert mp._print(A).getElementsByTagName('ci')[0].firstChild.data == 'A'\n\n    # RandomSymbol\n    R = RandomSymbol('R')\n    assert mp._print(R) == mp.dom.createElement('ci')\n    assert mp._print(R).getElementsByTagName('ci')[0].firstChild.data == 'R'\n\n    # Greek letters\n    alpha = Symbol('alpha')\n    assert mp._print(alpha) == mp.dom.createElement('ci')\n    assert mp._print(alpha).getElementsByTagName('ci')[0].firstChild.data == '\u03b1'\n\n    # Sums and Integrals\n    assert mp._print(Sum(x, (x, 0, 10))) == mp.dom.createElement('apply')\n    assert mp._print(Sum(x, (x, 0, 10))).getElementsByTagName('sum')[0].tagName == 'sum'\n\n    assert mp._print(Integral(x, (x, 0, 10))) == mp"], "sample_1040": ["def test_relational():\n    assert mp.doprint(Eq(x, y)) == '<apply><eq/><ci>x</ci><ci>y</ci></apply>'\n    assert mp.doprint(Ne(x, y)) == '<apply><neq/><ci>x</ci><ci>y</ci></apply>'\n    assert mp.doprint(Ge(x, y)) == '<apply><geq/><ci>x</ci><ci>y</ci></apply>'\n    assert mp.doprint(Lt(x, y)) == '<apply><lt/><ci>x</ci><ci>y</ci></apply>'\n    assert mpp.doprint(Eq(x, y)) == '<mrow><mi>x</mi><mo>=</mo><mi>y</mi></mrow>'\n    assert mpp.doprint(Ne(x, y)) == '<mrow><mi>x</mi><mo>&#x2260;</mo><mi>y</mi></mrow>'\n    assert mpp.doprint(Ge(x, y)) == '<mrow><mi>x</mi><mo>&#x2265;</mo><mi>y</mi></mrow>'\n    assert mpp.doprint(Lt(x, y)) == '<mrow><mi>x</mi><mo>&lt;</mo><mi>y</mi></mrow>'\n", "def test_mathml_functions():\n    assert mp.doprint(sin(x)) == '<apply><sin/><ci>x</ci></apply>'\n    assert mpp.doprint(sin(x)) == '<mrow><mi>sin</mi><mfenced><mi>x</mi></mfenced></mrow>'\n    assert mp.doprint(cos(x)) == '<apply><cos/><ci>x</ci></apply>'\n    assert mpp.doprint(cos(x)) == '<mrow><mi>cos</mi><mfenced><mi>x</mi></mfenced></mrow>'\n    assert mp.doprint(tan(x)) == '<apply><tan/><ci>x</ci></apply>'\n    assert mpp.doprint(tan(x)) == '<mrow><mi>tan</mi><mfenced><mi>x</mi></mfenced></mrow>'\n    assert mp.doprint(asin(x)) == '<apply><arcsin/><ci>x</ci></apply>'\n    assert mpp.doprint(asin(x)) == '<mrow><mi>arcsin</mi><mfenced><mi>x</mi></mfenced></mrow>'\n    assert mp.doprint(acos(x)) == '<apply><arccos/><ci>x</ci></apply>'\n    assert mpp.doprint(acos(x)) == '<mrow><mi>arccos</mi><mfenced><mi>x</mi></mfenced></mrow>'\n    assert mp.doprint(atan(x)) == '<apply><arctan/><ci>x</ci></apply>'\n    assert mpp.doprint(atan(x)) == '<mrow><mi>arctan</mi><mfenced><mi>x</mi></mfenced></mrow>'\n    assert mp.doprint(sinh(x)) == '<apply><sinh/><ci>x</ci></apply>'\n    assert mpp.doprint(sinh(x)) == '<mrow><mi>sinh</mi><mfenced><mi>x</mi></mfenced></mrow>'\n    assert mp.doprint(cosh(x)) == '<apply><cosh/><ci>x</ci></apply>'\n    assert mpp.doprint(cosh(x)) == '<mrow><mi>cosh</mi><mfenced><mi>x</mi></mfenced></mrow>'\n    assert mp.doprint(tanh(x)) == '<apply><tanh/><ci>x</", "def test_apply_patch():\n    mp.apply_patch()\n    mp.restore_patch()\n    raises(AttributeError, lambda: mp._Element_writexml_old)\n    raises(AttributeError, lambda: mp._Text_writexml_old)\n    assert mp.dom.createElement('apply') is not None\n    assert mp.dom.createTextNode('test') is not None\n", "def testPrintingSymbols():\n    x = Symbol('x')\n    a = mathml(x, printer='presentation')\n    b = '<mi>x</mi>'\n    assert a == b\n\n    a = mathml(x, printer='content')\n    b = '<ci>x</ci>'\n    assert a == b\n\n    x = Symbol('x1')\n    a = mathml(x, printer='presentation')\n    b = '<msub><mi>x</mi><mrow><mn>1</mn></mrow></msub>'\n    assert a == b\n\n    a = mathml(x, printer='content')\n    b = '<ci>x1</ci>'\n    assert a == b\n\n    x = Symbol('x**2')\n    a = mathml(x, printer='presentation')\n    b = '<msup><mi>x</mi><mrow><mn>2</mn></mrow></msup>'\n    assert a == b\n\n    a = mathml(x, printer='content')\n    b = '<ci>x**2</ci>'\n    assert a == b\n\n    x = Symbol('x1_2')\n    a = mathml(x, printer='presentation')\n    b = '<msubsup><mi>x</mi><mrow><mn>1</mn></mrow><mrow><mn>2</mn></mrow></msubsup>'\n    assert a == b\n\n    a = mathml(x, printer='content')\n    b = '<ci>x1_2</ci>'\n    assert a == b\n\n    x = Symbol('x__2')\n    a = mathml(x, printer='presentation')\n    b = '<msup><mi>x</mi><mrow><mn>2</mn></mrow></msup>'\n    assert a == b\n\n    a = mathml(x, printer='content')\n    b = '<ci>x__2</ci>'\n    assert a == b\n", "def test_MathMLContentPrinter_print_RandomSymbol():\n    mp.apply_patch()\n    expr = RandomSymbol('RandomSymbol')\n    assert mp._print(expr) == mp.dom.createElement('ci')\n    mp.restore_patch()\n"], "sample_1041": ["def test_matrix_symbol_derivative():\n    from sympy import Derivative\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    assert Derivative(A*B, A).doit() == B.T\n    assert Derivative(A*B, B).doit() == A\n    assert Derivative(A + B, A).doit() == Identity(2)\n    assert Derivative(A + B, B).doit() == Identity(2)\n    assert Derivative(A**2, A).doit() == 2*A.T\n", "def test_as_real_imag():\n    A = MatrixSymbol('A', 2, 2)\n    A_r, A_i = A.as_real_imag()\n    assert A_r + A_i * 1j == A\n    assert (A_r - A_i * 1j) == A.conjugate()\n", "def test_from_index_summation_matrix_derivative():\n    from sympy import symbols\n    i, j, k, n = symbols('i j k n', integer=True)\n    A = MatrixSymbol('A', n, n)\n    B = MatrixSymbol('B', n, n)\n    C = MatrixSymbol('C', n, n)\n    X = MatrixSymbol('X', n, n)\n\n    expr = Sum(A[i, j]*B[j, k]*C[k, i], (i, 0, n-1), (j, 0, n-1), (k, 0, n-1))\n    res = MatrixExpr.from_index_summation(expr)\n    assert res.diff(X).doit() == 0\n", "def test_MatrixElement_diff():\n    x = symbols('x')\n    A = MatrixSymbol('A', 2, 2)\n    A_diff = MatrixSymbol('A', 2, 2)\n    A_diff_diff = MatrixSymbol('A', 2, 2)\n\n    # case 1: derivative of A[i, j] wrt A\n    assert diff(A[0, 0], A) == Matrix([[1, 0], [0, 0]])\n    assert diff(A[1, 1], A) == Matrix([[0, 0], [0, 1]])\n\n    # case 2: derivative of A[i, j] wrt A, with A being a function of x\n    assert diff(A[0, 0], x) == A_diff[0, 0]\n    assert diff(A[1, 1], x) == A_diff[1, 1]\n\n    # case 3: second derivative of A[i, j] wrt x\n    assert diff(diff(A[0, 0], x), x) == A_diff_diff[0, 0]\n    assert diff(diff(A[1, 1], x), x) == A_diff_diff[1, 1]\n\n    # case 4: derivative of A[i, j] wrt x, with A being a function of x\n    assert diff(A[0, 0], x).diff(x) == A_diff_diff[0, 0]\n    assert diff(A[1, 1], x).diff(x) == A_diff_diff[1, 1]\n", "def test_matrix_derivative():\n    from sympy import Derivative\n    from sympy.abc import x\n    from sympy.matrices.expressions.matexpr import _LeftRightArgs\n\n    X = MatrixSymbol('X', 2, 2)\n    expr = X**2\n    x11, x12, x21, x22 = X.as_explicit().as_mutable().as_list()\n\n    expr_diff = diff(expr, X)\n    assert expr_diff == 2*X\n\n    # check with a more complex matrix expression\n    expr2 = C * X**2 * D\n    expr_diff2 = diff(expr2, X)\n    assert expr_diff2 == C.T * D.T + D * C * 2*X\n"], "sample_1042": ["def test_indexed_default_assumptions():\n    A = IndexedBase('A')\n    assert A.is_real is None\n    assert A.is_complex is None\n    assert A.is_commutative is True\n    A_real = IndexedBase('A', real=True)\n    assert A_real.is_real is True\n    assert A_real.is_complex is True\n    assert A_real.is_commutative is True\n    A_comm = IndexedBase('A', commutative=False)\n    assert A_comm.is_real is None\n    assert A_comm.is_complex is None\n    assert A_comm.is_commutative is False\n", "def test_Idx_properties():\n    i, j, k = symbols('i j k', integer=True)\n    idx1 = Idx(i)\n    idx2 = Idx(j, 5)\n    idx3 = Idx(k, (2, 6))\n    \n    assert idx1.label == i\n    assert idx2.lower == 0\n    assert idx2.upper == 4\n    assert idx3.lower == 2\n    assert idx3.upper == 6\n    \n    assert idx1.free_symbols == {i}\n    assert idx2.free_symbols == {j}\n    assert idx3.free_symbols == {k}\n", "def test_indexed_shape_precedence():\n    n, m = symbols('n m', integer=True)\n    a = IndexedBase('a', shape=(n, m))\n    i, j = symbols('i j', integer=True)\n    b = IndexedBase('b')\n    assert a[i, j].shape == (n, m)\n    assert b[i, j].shape == (None, None)\n    assert a[i, j].ranges == [(None, None), (None, None)]\n    assert b[i, j].ranges == [(None, None), (None, None)]\n", "def test_Idx_properties():\n    i, j, k, n, m = symbols('i j k n m', integer=True)\n    idx1 = Idx('test_idx', (n, m))\n    idx2 = Idx('test_idx', n)\n    idx3 = Idx('test_idx')\n    assert idx1.lower == n\n    assert idx1.upper == m\n    assert idx2.lower == 0\n    assert idx2.upper == n - 1\n    assert idx3.lower is None\n    assert idx3.upper is None\n", "def test_IndexedBase_assumptions():\n    A = IndexedBase('A', real=True)\n    assert A.is_real\n    assert not A.is_complex\n    assert A.is_real is True\n    assert A.is_complex is False\n\n    B = IndexedBase('B', complex=True)\n    assert B.is_real\n    assert B.is_complex\n    assert B.is_real is True\n    assert B.is_complex is True\n\n    C = IndexedBase('C', integer=True)\n    assert C.is_integer\n    assert C.is_real\n    assert not C.is_complex\n    assert C.is_integer is True\n    assert C.is_real is True\n    assert C.is_complex is False\n\n    D = IndexedBase('D', real=True, integer=True)\n    assert D.is_integer\n    assert D.is_real\n    assert not D.is_complex\n    assert D.is_integer is True\n    assert D.is_real is True\n    assert D.is_complex is False\n"], "sample_1043": ["def test_print_list():\n    assert mcode([1, 2, 3, 4, 5]) == \"{1, 2, 3, 4, 5}\"\n    assert mcode((1, 2, 3, 4, 5)) == \"{1, 2, 3, 4, 5}\"\n    assert mcode(Tuple(1, 2, 3, 4, 5)) == \"{1, 2, 3, 4, 5}\"\n", "def test_MCodePrinter_print_list():\n    assert mcode([1, 2, 3]) == \"{1, 2, 3}\"\n    assert mcode(Tuple(1, 2, 3)) == \"{1, 2, 3}\"\n    assert mcode(Tuple(x, y, z)) == \"{\" + mcode(x) + \", \" + mcode(y) + \", \" + mcode(z) + \"}\"\n", "def test_MCodePrinter_print_Mul():\n    # Test printing of unevaluated Mul objects\n    expr = x * y\n    assert mcode(expr) == 'x*y'\n    # Test printing of evaluated Mul objects\n    expr = x * x\n    assert mcode(expr) == 'x^2'\n    # Test printing of Mul objects with more than two arguments\n    expr = x * y * z\n    assert mcode(expr) == 'x*y*z'\n    # Test printing of Mul objects with coefficients\n    expr = 2 * x * y\n    assert mcode(expr) == '2*x*y'\n", "def test_print_Derivative():\n    assert mcode(Derivative(f(x), x)) == \"Hold[D[f[x], x]]\"\n    assert mcode(Derivative(f(x, y), x, 2, y)) == \"Hold[D[f[x, y], {x, 2}, y]]\"\n    assert mcode(Derivative(f(x), x, 3)) == \"Hold[D[f[x], {x, 3}]]\"\n", "def test_mcode_functions():\n    assert mcode(sin(x)) == 'Sin[x]'\n    assert mcode(cos(x)) == 'Cos[x]'\n    assert mcode(conjugate(x)) == 'Conjugate[x]'\n    assert mcode(Max(x, y)) == 'Max[x, y]'\n    assert mcode(Min(x, y)) == 'Min[x, y]'\n    assert mcode(exp(x)) == 'Exp[x]'\n"], "sample_1044": ["def test_Pow_is_integer():\n    x = Symbol('x', integer=True)\n    y = Symbol('y', integer=True)\n    z = Symbol('z')\n\n    assert Pow(x, y, evaluate=False).is_integer\n    assert Pow(2, y, evaluate=False).is_integer\n    assert Pow(x, 2, evaluate=False).is_integer\n    assert Pow(2, 3, evaluate=False).is_integer\n\n    assert not Pow(2, z, evaluate=False).is_integer\n    assert not Pow(x, z, evaluate=False).is_integer\n    assert not Pow(x, 2.5, evaluate=False).is_integer\n", "def test_Pow_as_content_primitive():\n    from sympy.abc import x\n    assert Pow(2*x, 2).as_content_primitive() == (4, Pow(x, 2))\n    assert Pow(2*x, 2.0).as_content_primitive() == (4, Pow(x, 2.0))\n    assert Pow(4*x, S.Half).as_content_primitive() == (2, Pow(x, S.Half))\n    assert Pow(4*x, S.Half).as_content_primitive(radical=True) == (2, Pow(x, S.Half))\n    assert Pow(2*x, -2).as_content_primitive() == (1, Pow(x, -2))\n    assert Pow(2*x, -2.0).as_content_primitive() == (1, Pow(x, -2.0))\n    assert Pow(4*x, S.Rational(-1, 2)).as_content_primitive() == (1, Pow(x, S.Rational(-1, 2)))\n    assert Pow(4*x, S.Rational(-1, 2)).as_content_primitive(radical=True) == (1, Pow(x, S.Rational(-1, 2)))\n", "def test_is_const():\n    x = Symbol('x')\n    y = Symbol('y')\n    b = Symbol('b', constant=True)\n    e = Symbol('e', constant=True)\n    assert Pow(x, e).is_constant() is None\n    assert Pow(b, e).is_constant()\n    assert Pow(b, x).is_constant() is False\n    assert Pow(x, y).is_constant() is None\n    assert Pow(x, 0).is_constant()\n    assert Pow(x, S(0)).is_constant()\n    assert Pow(S(1), x).is_constant()\n    assert Pow(S(0), x).is_constant() is False\n    assert Pow(S(2), x).is_constant() is False\n    assert Pow(x, S(1)).is_constant() is False\n    assert Pow(x, S(-1)).is_constant() is False\n    assert Pow(x, S(2)).is_constant() is False\n    assert Pow(S(1), S(2)).is_constant()\n    assert Pow(S(2), S(3)).is_constant()\n    assert Pow(S(0), S(0)).is_constant() is False  # 0**0 is not defined\n", "def test_pow_is_integer():\n    x = Symbol('x')\n    assert Pow(2, 3, evaluate=False).is_integer\n    assert Pow(2, -3, evaluate=False).is_integer\n    assert Pow(x, 0, evaluate=False).is_integer\n    assert Pow(x, 3, evaluate=False).is_integer is None\n    assert Pow(x, -3, evaluate=False).is_integer is None\n    assert Pow(Rational(7, 5), 3, evaluate=False).is_integer is False\n    assert Pow(Rational(7, 5), -3, evaluate=False).is_integer is False\n", "def test_Pow__eval_is_integer():\n    x = Symbol('x', integer=True)\n    y = Symbol('y')\n    assert Pow(x, 2, evaluate=False)._eval_is_integer() is True\n    assert Pow(y, 2, evaluate=False)._eval_is_integer() is None\n    assert Pow(x, 0.5, evaluate=False)._eval_is_integer() is False\n    assert Pow(y, 0.5, evaluate=False)._eval_is_integer() is None\n    assert Pow(2, 3, evaluate=False)._eval_is_integer() is True\n    assert Pow(2, 0.5, evaluate=False)._eval_is_integer() is False\n"], "sample_1045": ["def test_Float_issue_3785():\n    # create a Float with a big number that's exactly representable in binary\n    # the doctest in issue 3785 used 2**106, which is too big to create a Float\n    # from a Python int in Python 2, so we use a smaller one here\n    x = Float(2**50)\n    assert same_and_same_prec(x, Float(str(x)))\n    assert same_and_same_prec(x, Float(srepr(x)))\n    assert same_and_same_prec(x, Float(repr(x)))\n    assert same_and_same_prec(x, Float(Float(x)))\n", "def test_mpf_norm():\n    assert same_and_same_prec(mpf_norm((1, 5, 30, 3), 50), (1, 5, 30, 3))\n    assert same_and_same_prec(mpf_norm((1, 0, 15, 2), 50), (1, 0, 15, 2))\n    assert same_and_same_prec(mpf_norm((-1, 4, 325, 3), 50), (-1, 4, 325, 3))\n    assert same_and_same_prec(mpf_norm((-1, 0, 25, 3), 50), (-1, 0, 25, 3))\n", "def test_PythonRational():\n    a = PythonRational(1, 2)\n    assert sympify(a) == Rational(1, 2)\n    assert Float(a) == Float(1, 2)\n", "def test_comp():\n    assert comp(1, 1) is True\n    assert comp(1, 1, 0) is True\n    assert comp(1, 1, 1) is False\n    assert comp(1, 2) is False\n    assert comp(1, 2, 1) is True\n    assert comp(1, 2, 0) is False\n    assert comp(2, 1) is False\n    assert comp(2, 1, 1) is True\n    assert comp(2, 1, 0) is False\n    assert comp(1.0, 1.0) is True\n    assert comp(1.0, 1.0, 0) is True\n    assert comp(1.0, 1.0, 1) is False\n    assert comp(1.0, 2.0) is False\n    assert comp(1.0, 2.0, 1) is True\n    assert comp(1.0, 2.0, 0) is False\n    assert comp(2.0, 1.0) is False\n    assert comp(2.0, 1.0, 1) is True\n    assert comp(2.0, 1.0, 0) is False\n    assert comp(1.0, '1.0') is True\n    assert comp(1.0, '1.0', 0) is True\n    assert comp(1.0, '1.0', 1) is False\n    assert comp(1.0, '2.0') is False\n    assert comp(1.0, '2.0', 1) is True\n    assert comp(1.0, '2.0', 0) is False\n    assert comp(2.0, '1.0') is False\n    assert comp(2.0, '1.0', 1) is True\n    assert comp(2.0, '1.0', 0) is False\n    assert comp(1.0000000000000002, 1.0000000000000003, 1) is True\n", "def test_Float_gcd():\n    assert Float(2.5).gcd(Integer(3)) == 1\n    assert Float(2.5).gcd(Float(3.7)) == 1\n    raises(TypeError, lambda: Float(2.5).gcd(t))\n    raises(TypeError, lambda: Float(2.5).gcd(t + 1))\n"], "sample_1047": ["def test_algebraic_commutative():\n    x = Symbol('x')\n    assert x.is_algebraic == None\n    assert x.is_commutative == True\n    x = Symbol('x', algebraic=True)\n    assert x.is_algebraic == True\n    assert x.is_commutative == True\n    x = Symbol('x', commutative=False)\n    assert x.is_algebraic == None\n    assert x.is_commutative == False\n", "def test_integer():\n    x = Symbol('x', integer=True)\n    assert x.is_integer is True\n    assert x.is_integer\n    assert x.is_real\n    assert x.is_complex\n    assert x.is_finite\n    assert x.is_algebraic\n    assert x.is_commutative\n", "def testStdFactKB_copy():\n    # create a StdFactKB object\n    kb = StdFactKB({'real': True, 'integer': False})\n    \n    # copy the StdFactKB object\n    kb_copy = kb.copy()\n    \n    # check that the copy is equal to the original\n    assert kb == kb_copy\n    \n    # check that the copy is a separate object\n    assert kb is not kb_copy\n    \n    # modify the copy\n    kb_copy.deduce_all_facts({'integer': True})\n    \n    # check that the copy is no longer equal to the original\n    assert kb != kb_copy\n", "def test_std_fact_kb_copy():\n    fact_kb = StdFactKB({'integer': True, 'real': True})\n    copied_kb = fact_kb.copy()\n    assert isinstance(copied_kb, StdFactKB)\n    assert copied_kb.generator == {'integer': True, 'real': True}\n    assert copied_kb._generator is not fact_kb._generator\n", "compilation error"], "sample_1046": ["def test_TensExpr_doit():\n    Lorentz = TensorIndexType('Lorentz', dummy_fmt='L')\n    a, b, c, d = tensor_indices('a b c d', Lorentz)\n    e = Lorentz.metric\n    p, q = tensorhead('p q', [Lorentz], [[1]])\n    t1 = p(a)*(q(-a) + 2*q(-b))\n    t2 = 3*t1\n    assert _is_equal(t1.doit(), p(a)*q(-a) + 2*p(a)*q(-b))\n    assert _is_equal(t2.doit(), 3*p(a)*q(-a) + 6*p(a)*q(-b))\n", "def test_TensorElement():\n    L = TensorIndexType(\"L\")\n    i, j, k = symbols(\"i j k\")\n    A = tensorhead(\"A\", [L, L], [[1], [1]])\n    te = TensorElement(A(i, j), {i: 2})\n    assert te.get_free_indices() == [j,]\n    assert te.get_indices() == [j,]\n", "def test_TensMul_indices():\n    Lorentz = TensorIndexType('Lorentz', dummy_fmt='L')\n    a, b, c, d = tensor_indices('a,b,c,d', Lorentz)\n    g = Lorentz.metric\n    p, q = tensorhead('p,q', [Lorentz], [[1]])\n    t = p(a)*q(b)\n    assert t.get_indices() == [a, b]\n    assert t.get_free_indices() == [a, b]\n    t2 = p(a)*g(a, b)\n    assert t2.get_indices() == [L_0, -L_0]\n    assert t2.get_free_indices() == []\n", "def test_TensMul_get_free_indices():\n    Lorentz = TensorIndexType(\"Lorentz\", dummy_fmt=\"L\")\n    i0, i1, i2 = tensor_indices('i0:3', Lorentz)\n    A = tensorhead(\"A\", [Lorentz]*2, [[1], [1]])\n    assert A(i0, i1).get_free_indices() == [i0, i1]\n    assert A(i0, -i0).get_free_indices() == []\n    assert (A(i0, i1)*A(i1, i2)).get_free_indices() == [i0, i2]\n", "def test_components_data_full_destroy():\n    # tests that all associated data are destroyed after destroying the TensorHead\n    Lorentz = TensorIndexType('Lorentz', dummy_fmt='L')\n    i0, i1 = tensor_indices('i0:2', Lorentz)\n    A = tensorhead('A', [Lorentz, Lorentz], [[1], [1]])\n    Lorentz.data = [1, -1, -1, -1]\n    A.data = [[1, 2], [3, 4]]\n    assert A(i0, i1).data == [[1, 2], [3, 4]]\n    del A\n    raises(KeyError, lambda: Lorentz.data)\n    raises(KeyError, lambda: _TensorDataLazyEvaluator._substitutions_dict[(A, True, True)])\n"], "sample_1050": ["def test_print_CodegenArrayTensorProduct():\n    from sympy.codegen.array_utils import CodegenArrayTensorProduct\n    \n    m = MatrixSymbol(\"m\", 3, 3)\n    cgatp = CodegenArrayTensorProduct(m, m)\n    assert NumPyPrinter().doprint(cgatp) == \"numpy.einsum(m, [0, 1], m, [2, 3])\"\n", "def test_MpmathPrinter_print_Rational():\n    mp = MpmathPrinter()\n    expr = Rational(3, 4)\n    assert mp._print(expr) == 'mpmath.mpf(3)/mpmath.mpf(4)'\n", "def test_PythonCodePrinter_print_Piecewise():\n    expr = Piecewise((x, x < 1), (x**2, True))\n    result = PythonCodePrinter().doprint(expr)\n    assert result == \"(x if x < 1 else x**2)\"\n", "def test_pycode_reserved_words():\n    # Make sure reserved words in Python are escaped.\n    A = MatrixSymbol('if', 2, 1)\n    B = MatrixSymbol('and', 1, 1)\n    C = MatrixSymbol('or', 2, 2)\n    assign = Assignment(A, B * C)\n    py_str = pycode(assign)\n    assert py_str == \"if_ = and_ * or_\"\n", "def test_re():\n    assert NumPyPrinter().doprint(x.as_real_imag()[0]) == 'numpy.real(x)'\n    assert NumPyPrinter().doprint(x.as_real_imag()[0].as_numer_denom()[0]) == 'numpy.real(x)'\n"], "sample_1048": ["def test_parabola_equation():\n    x, y = symbols('x y', real=True)\n    p1 = Parabola(Point(0, 0), Line(Point(5, 8), Point(7, 8)))\n    assert p1.equation() == -x**2 - 16*y + 64\n    assert p1.equation('f') == -symbols('f')**2 - 16*y + 64\n    assert p1.equation(y='z') == -x**2 - 16*symbols('z') + 64\n", "def test_parabola_equation():\n    x, y = symbols('x y', real=True)\n    focus = Point(0, 0)\n    directrix = Line(Point(5, 8), Point(7, 8))\n    parabola = Parabola(focus, directrix)\n    assert parabola.equation() == -x**2 - 16*y + 64\n    assert parabola.equation('f') == -symbols('f')**2 - 16*y + 64\n    assert parabola.equation(y='z') == -x**2 - 16*symbols('z') + 64\n", "def test_parabola_equation():\n    x, y = symbols('x y', real=True)\n    p1 = Parabola(Point(0, 0), Line(Point(5, 8), Point(7, 8)))\n    assert p1.equation() == -x**2 - 16*y + 64\n    assert p1.equation(x='f') == -symbols('f')**2 - 16*y + 64\n    assert p1.equation(y='z') == -x**2 - 16*symbols('z') + 64\n", "def test_parabola():\n    # Test the equation method\n    p1 = Parabola(Point(0, 0), Line(Point(5, 8), Point(7, 8)))\n    assert p1.equation() == -symbols('x')**2 - 16*symbols('y') + 64\n    assert p1.equation('f') == -symbols('f')**2 - 16*symbols('y') + 64\n    assert p1.equation(y='z') == -symbols('x')**2 - 16*symbols('z') + 64\n", "def test_parabola_equation():\n    x, y = symbols('x y', real=True)\n    p1 = Parabola(Point(0, 0), Line(Point(5, 8), Point(7, 8)))\n    assert p1.equation() == -x**2 - 16*y + 64\n    assert p1.equation(y='z') == -x**2 - 16*z + 64\n    assert p1.equation(x='f') == -f**2 - 16*y + 64\n    assert p1.equation(x='f', y='z') == -f**2 - 16*z + 64\n"], "sample_1049": ["def test_parallel_plane():\n    a = Plane(Point3D(1, 4, 6), normal_vector=(2, 4, 6))\n    pt = Point3D(2, 3, 5)\n    assert a.parallel_plane(pt).equation() == a.equation()\n", "def test_parameter_value():\n    p1 = Point3D(1, 0, 0)\n    p2 = Point3D(1, 1, 0)\n    p3 = Point3D(0, 0, 0)\n    p = Plane(p1, normal_vector=(0, 0, 1))\n    t = symbols('t')\n    u, v = symbols('u v', real=True)\n\n    # test parameter value for point on plane\n    point_on_plane = p.arbitrary_point(t).subs(t, pi / 4)\n    assert p.parameter_value(point_on_plane, t) == {t: pi / 4}\n\n    # test parameter value for point not on plane\n    point_not_on_plane = p.p1 + (point_on_plane - p.p1) * 2\n    assert p.parameter_value(point_not_on_plane, t) == {t: pi / 4}\n\n    # test 2-value parameter\n    assert p.parameter_value(point_on_plane, u, v) == {u: sqrt(2)/2, v: 0}\n    assert p.parameter_value(point_not_on_plane, u, v) == {u: sqrt(2), v: 0}\n", "def test_plane_intersection():\n    p1 = Plane((1, 2, 3), normal_vector=(1, 1, 1))\n    p2 = Plane((2, 2, 2), normal_vector=(2, 2, 2))\n    assert p1.intersection(p2) == [p1]\n    p3 = Plane((1, 1, 1), normal_vector=(2, 2, 3))\n    i = p1.intersection(p3)\n    assert isinstance(i[0], Line3D)\n    assert i[0] in p1\n    assert i[0] in p3\n", "def test_is_coplanar():\n    a, b = Point3D(0, 0, 0), Point3D(1, 1, 1)\n    assert are_coplanar(a, b, a + Point3D(1, 0, 0), a + Point3D(0, 1, 0))\n    assert not are_coplanar(a, b, a + Point3D(1, 0, 0), Point3D(1, 0, 1))\n\n    # test Plane.is_coplanar\n    p1 = Plane(a, (1, 0, 0), (0, 1, 0))\n    assert p1.is_coplanar(a)\n    assert p1.is_coplanar(b) is False\n    p2 = Plane(a, (1, 0, 0), (0, 1, 0))\n    assert p1.is_coplanar(p2)\n    p3 = Plane(b, (1, 0, 0), (0, 1, 0))\n    assert p1.is_coplanar(p3) is False\n\n    # test with LinearEntity3D\n    l1 = Line3D(a, b)\n    assert p1.is_coplanar(l1) is False\n    l2 = Line3D(a, a + Point3D(1, 0, 0))\n    assert p1.is_coplanar(l2)\n", "def test_plane_projection_line():\n    a = Plane(Point3D(1, 1, 1), normal_vector=(1, 1, 1))\n    b = Line(Point3D(1, 1), Point3D(2, 2))\n    assert a.projection_line(b) == Line3D(Point3D(4/3, 4/3, 1/3), Point3D(5/3, 5/3, -1/3))\n    c = Line3D(Point3D(1, 1, 1), Point3D(2, 2, 2))\n    assert a.projection_line(c) == Point3D(1, 1, 1)\n"], "sample_1051": ["def test_purestr():\n    assert purestr(x) == \"Symbol('x')\"\n    assert purestr(x + 1) == \"Add(Symbol('x'), Integer(1))\"\n    assert purestr(x**2) == \"Pow(Symbol('x'), Integer(2))\"\n    assert purestr(Integer(2)) == \"Integer(2)\"\n    assert purestr(Float(2.0)) == \"Float('2.0', prec=53)\"\n    assert purestr(Integer(1)/2) == \"Rational(1, 2)\"\n", "def test_dotnode():\n    assert dotnode(x) == '\"Symbol(\\'x\\')_()\" [\"color\"=\"black\", \"label\"=\"x\", \"shape\"=\"ellipse\"];'\n    assert dotnode(x + 2) == '\"Add(Integer(2), Symbol(\\'x\\'))_()\" [\"color\"=\"black\", \"label\"=\"Add\", \"shape\"=\"ellipse\"];'\n    assert dotnode(2) == '\"Integer(2)_()\" [\"color\"=\"black\", \"label\"=\"2\", \"shape\"=\"ellipse\"];'\n\n    assert dotnode(x, labelfunc=srepr) == '\"Symbol(\\'x\\')_()\" [\"color\"=\"black\", \"label\"=\"Symbol(\\'x\\')\", \"shape\"=\"ellipse\"];'\n    assert dotnode(x + 2, labelfunc=srepr) == '\"Add(Integer(2), Symbol(\\'x\\'))_()\" [\"color\"=\"black\", \"label\"=\"Add\", \"shape\"=\"ellipse\"];'\n    assert dotnode(2, labelfunc=srepr) == '\"Integer(2)_()\" [\"color\"=\"black\", \"label\"=\"Integer(2)\", \"shape\"=\"ellipse\"];'\n", "def test_dotnode():\n    assert dotnode(x) == '\"Symbol(\\'x\\')_()\" [\"color\"=\"black\", \"label\"=\"x\", \"shape\"=\"ellipse\"];'\n    assert dotnode(x + 1) == '\"Add(Integer(1), Symbol(\\'x\\'))_()\" [\"color\"=\"black\", \"label\"=\"Add\", \"shape\"=\"ellipse\"];'\n    assert dotnode(x, repeat=False) == '\"Symbol(\\'x\\')_()\" [\"color\"=\"black\", \"label\"=\"x\", \"shape\"=\"ellipse\"];'\n    assert dotnode(x, labelfunc=srepr) == '\"Symbol(\\'x\\')_()\" [\"color\"=\"black\", \"label\"=\"Symbol(\\'x\\')\", \"shape\"=\"ellipse\"];'\n    assert dotnode(Integer(2)) == '\"Integer(2)_()\" [\"color\"=\"black\", \"label\"=\"2\", \"shape\"=\"ellipse\"];'\n", "def test_dotprint_maxdepth():\n    a, b = symbols('a b')\n    result = dotprint(a + b, maxdepth=0)\n    assert \"Add\" in result\n    assert \"Symbol('a')\" not in result\n    assert \"Symbol('b')\" not in result\n", "def test_purestr():\n    assert purestr(Symbol('x')) == \"Symbol('x')\"\n    assert purestr(Integer(2)) == 'Integer(2)'\n    assert purestr(Float(2.5)) == 'Float(2.5)'\n    assert purestr(x + 1) == 'Add(Integer(1), Symbol(\\'x\\'))'\n    assert purestr(x * 2) == 'Mul(Integer(2), Symbol(\\'x\\'))'\n"], "sample_1052": ["def test_fcodegen_matrixoutput():\n    x, y = symbols('x, y')\n    A = Matrix([[x, y], [y, x]])\n    expr = Equality(A, A)\n    raises(CodeGenError, lambda: make_routine('test', [expr], language='c89'))\n    raises(CodeGenError, lambda: make_routine('test', [expr], language='f95'))\n    raises(CodeGenError, lambda: make_routine('test', [A], language='c89'))\n    raises(CodeGenError, lambda: make_routine('test', [A], language='f95'))\n", "def test_CodeGenArgumentListError():\n    f, x, y, z = symbols('f x y z')\n    raises(CodeGenArgumentListError, lambda: make_routine('test', [Eq(f, 2*x), Eq(x, y+z)], argument_sequence=(x, y)))\n", "def test_RustCodeGen_get_prototype():\n    x = symbols('x')\n    y = symbols('y')\n    expr = x + y\n    r = make_routine('test', expr)\n    c = RustCodeGen()\n    prototype = c.get_prototype(r)\n    assert prototype == \"fn test(x: f64, y: f64) -> f64\"\n", "def test_code_gen_c99():\n    x = symbols('x')\n    c99_code_gen = C99CodeGen(project=\"test_project\")\n    expr = x**3\n    routine = c99_code_gen.routine('test', expr)\n    c_code = get_string(c99_code_gen.dump_c, [routine])\n    assert \"pow(x, 3)\" not in c_code\n    assert \"x*x*x\" in c_code\n", "def test_c_code_printer_with_function():\n    x = symbols('x')\n    routines = [make_routine('test', [erf(x)])]\n    f_code = get_string(CCodeGen().dump_c, routines)\n    assert 'double test(double x)' in f_code\n    assert 'erf(x)' in f_code\n"], "sample_1054": ["def test_ComplexRegion_contains():\n    a = Interval(1, 4)\n    b = Interval(1, 3)\n    c = Interval(0, 2*S.Pi)\n    C1 = ComplexRegion(a*b)\n    C2 = ComplexRegion(a*c, polar=True)\n    assert 2 + 2*I in C1\n    assert 2 + 4*I not in C1\n    assert 1.5 + 1.5*I in C1\n    assert 1 + sqrt(3)*I in C2\n    assert 1 + sqrt(5)*I not in C2\n    assert 1.5 + 3*I not in C2\n    assert 1.5 + I in C2\n", "def test_image_set_lambda_function():\n    # Testing imageset with a lambda function\n    x = Symbol('x')\n    lamda_function = Lambda(x, x**2)\n    set = Interval(1, 5)\n    image = imageset(lamda_function, set)\n    assert image == ImageSet(Lambda(x, x**2), Interval(1, 5))\n    assert 4 in image\n    assert 16 in image\n    assert 25 in image\n    assert 6 not in image\n    assert image.lamda(x) == x**2\n    assert image.base_set == Interval(1, 5)\n", "def test_ComplexRegion():\n    # Test cases for rectangular form\n    a = Interval(2, 3)\n    b = Interval(4, 5)\n    c = Interval(1, 7)\n    c1 = ComplexRegion(a*b)\n    c2 = ComplexRegion(Union(a*b, b*c))\n\n    assert c1.args == (a*b, False)\n    assert c2.args == (Union(a*b, b*c), False)\n\n    assert c1.sets == a*b\n    assert c2.sets == Union(a*b, b*c)\n\n    assert c1.psets == (a*b, )\n    assert c2.psets == (a*b, b*c)\n\n    assert c1.a_interval == a\n    assert c2.a_interval == Union(a, b)\n\n    assert c1.b_interval == b\n    assert c2.b_interval == c\n\n    assert c1.polar == False\n    assert c2.polar == False\n\n    assert 2.5 + 4.5*I in c1\n    assert 2.5 + 6.5*I not in c1\n\n    # Test cases for polar form\n    r = Interval(0, 1)\n    theta = Interval(0, 2*pi)\n    c3 = ComplexRegion(r*theta, polar=True)\n\n    assert c3.args == (r*theta, True)\n    assert c3.sets == r*theta\n\n    assert c3.psets == (r*theta, )\n    assert c3.a_interval == r\n    assert c3.b_interval == theta\n\n    assert c3.polar == True\n\n    assert 0.5 + 0.5*I in c3\n    assert 1 + 2*I not in c3\n", "def test_ComplexRegion_contains():\n    a, b = Interval(2, 5), Interval(4, 8)\n    c = Interval(0, 2*S.Pi)\n    c1 = ComplexRegion(a*b)\n    assert 3 + 5*I in c1\n    assert 6 + 5*I not in c1\n    assert 3 + 9*I not in c1\n    c2 = ComplexRegion(a*c, polar=True)\n    assert 3*(cos(S.Pi/2) + I*sin(S.Pi/2)) in c2\n    assert 6*(cos(S.Pi/2) + I*sin(S.Pi/2)) not in c2\n", "def test_ComplexRegion_contains():\n    a, b = Interval(2, 5), Interval(4, 8)\n    theta = Interval(0, 2*pi)\n    c1 = ComplexRegion(a*b)\n    c2 = ComplexRegion(a*theta, polar=True)\n    assert c1.contains(3 + 5*I) == True\n    assert c1.contains(3 + 5) == False\n    assert c1.contains(15 + 5*I) == False\n    assert c2.contains(3) == True\n    assert c2.contains(3 + I) == True\n    assert c2.contains(sqrt(3) + I) == True\n    assert c2.contains(6 + 5*I) == False\n    assert c2.contains(-2 + 3*I) == False\n    assert c2.contains(3 + 7*I) == False\n"], "sample_1053": ["def test_Float_mpf_norm():\n    from mpmath import mp\n    assert same_and_same_prec(Float(mpf_norm((0, 123, -2, 4), 53)), Float(30.75))\n    assert same_and_same_prec(Float(mpf_norm((1, 123, -2, 4), 53)), Float(-30.75))\n    assert same_and_same_prec(Float(mpf_norm((0, 123, 0, 4), 53)), Float(123.0))\n    assert same_and_same_prec(Float(mpf_norm((1, 123, 0, 4), 53)), Float(-123.0))\n    assert same_and_same_prec(Float(mpf_norm((0, 123, 2, 4), 53)), Float(492.0))\n    assert same_and_same_prec(Float(mpf_norm((1, 123, 2, 4), 53)), Float(-492.0))\n    assert same_and_same_prec(Float(mpf_norm((0, 0, 2, 4), 53)), Float(0))\n    assert same_and_same_prec(Float(mpf_norm((1, 0, 2, 4), 53)), Float(-0))\n    assert same_and_same_prec(Float(mpf_norm((0, 0, -2, 4), 53)), Float(0))\n    assert same_and_same_prec(Float(mpf_norm((1, 0, -2, 4), 53)), Float(-0))\n    assert same_and_same_prec(Float(mpf_norm((0, 0, 0, 4), 53)), Float(0))\n    assert same_and_same_prec(Float(mpf_norm((1, 0, 0, 4), 53)), Float(-0))\n", "def test_comp():\n    assert comp(1, 1) is True\n    assert comp(1, .1) is True\n    assert comp(.1, 1) is True\n    assert comp(.1, .1) is True\n    assert comp(.1, .100000001) is True\n    assert comp(.1, .1000000001) is True\n    assert comp(.1, .2) is False\n    assert comp(1e10, 1 + 1e-9) is True\n    assert comp(1e10, 1 + 1e-8) is False\n    assert comp('1', 1) is True\n    assert comp(1, '1') is True\n    assert comp(1, '2') is False\n    raises(ValueError, lambda: comp(1, 'a'))\n    raises(ValueError, lambda: comp(1, '1.1'))\n    assert comp(1.0, 1) is True  # coercion to Float\n    assert comp(1, 1.0) is True  # coercion to Float\n    # coercion to Float does not affect literal comparison\n    assert comp('1', 1.0) is True\n    assert comp('1', 1.1) is False\n", "def test_integer_nthroot():\n    assert integer_nthroot(27, 3) == (3, True)\n    assert integer_nthroot(28, 3) == (3, False)\n    assert integer_nthroot(16, 2) == (4, True)\n    assert integer_nthroot(26, 2) == (5, False)\n    assert integer_nthroot(12345678901234567890123456789**25, 25) == (12345678901234567890123456789, True)\n    assert integer_nthroot(123456789012345678901234567891**25, 25) == (123456789012345678901234567891, True)\n    assert integer_nthroot(1, 1) == (1, True)\n    assert integer_nthroot(1, 2) == (1, True)\n    assert integer_nthroot(0, 1) == (0, True)\n    assert integer_nthroot(0, 3) == (0, True)\n    assert integer_nthroot(4, 2) == (2, True)\n    assert integer_nthroot(16, 2) == (4, True)\n    assert integer_nthroot(27, 3) == (3, True)\n    assert integer_nthroot(256, 4) == (4, True)\n", "def test_integer_nthroot():\n    assert integer_nthroot(4, 2) == (2, True)\n    assert integer_nthroot(16, 2) == (4, True)\n    assert integer_nthroot(27, 3) == (3, True)\n    assert integer_nthroot(10000000000, 10) == (10, True)\n    assert integer_nthroot(5, 2) == (2, False)\n    assert integer_nthroot(-4, 2) == (-2, True)\n    assert integer_nthroot(-16, 2) == (-4, True)\n    assert integer_nthroot(-27, 3) == (-3, True)\n    assert integer_nthroot(-10000000000, 10) == (-10, True)\n    assert integer_nthroot(-5, 2) == (-2, False)\n", "def test_Rational_new():\n    assert Rational(1, 2) == Rational(Integer(1), Integer(2))\n    assert Rational(Integer(3), 4) == Rational(3, 4)\n    raises(TypeError, lambda: Rational(3, 4, 5))\n    raises(TypeError, lambda: Rational(3, '4'))\n"], "sample_1055": ["def test_bifid():\n    # Test bifid encryption and decryption with custom symbols\n    symbols = 'abcdefghi'\n    key = 'abc'\n    msg = 'abcdefghi'\n    enc_msg = encipher_bifid(msg, key, symbols)\n    dec_msg = decipher_bifid(enc_msg, key, symbols)\n    assert msg == dec_msg\n    \n    # Test bifid encryption and decryption with bifid5 square\n    key = 'gold bug'\n    msg = 'abcdefghiklmnopqrstuvwxyz'\n    enc_msg = encipher_bifid5(msg, key)\n    dec_msg = decipher_bifid5(enc_msg, key)\n    assert msg.replace('j', '') == dec_msg\n\n    # Test bifid encryption and decryption with bifid6 square\n    key = 'gold bug'\n    msg = 'abcdefghiklmnopqrstuvwxyz0123456789'\n    enc_msg = encipher_bifid6(msg, key)\n    dec_msg = decipher_bifid6(enc_msg, key)\n    assert msg == dec_msg\n", "def test_check_and_join():\n    assert check_and_join(['Hello', 'world!'], 'abc') == 'Helloworld!'\n    assert check_and_join('Hello, world!', 'abc', True) == 'Helloworld'\n    raises(ValueError, lambda: check_and_join('Hello, world!', 'abc', False))\n", "def test_gm_public_key():\n    # test for distinct primes p and q\n    raises(ValueError, lambda: gm_private_key(5, 5))\n    raises(ValueError, lambda: gm_private_key(4, 7))\n    raises(ValueError, lambda: gm_private_key(7, 4))\n\n    # test for invalid gm_public_key a\n    p, q = gm_private_key(7, 11)\n    a, N = gm_public_key(p, q)\n    assert gm_public_key(p, q, a=-1) == False\n", "def test_encipher_bifid():\n    p, k = \"playfair example\", \"bifidkey\"\n    c = encipher_bifid(p, k)\n    assert p != c\n    assert isinstance(c, str)\n    assert c.isalpha()\n    assert decipher_bifid(c, k) == AZ(p)\n    p, k = \"hide the gold in the tree stump\", \"playfair\"\n    c = encipher_bifid(p, k)\n    assert p != c\n    assert isinstance(c, str)\n    assert c.isalpha()\n    assert decipher_bifid(c, k) == AZ(p)\n    p, k = \"thequickbrownfoxjumpsoveralazydog\", \"\"\n    c = encipher_bifid(p, k)\n    assert p != c\n    assert isinstance(c, str)\n    assert c.isalpha()\n    assert decipher_bifid(c, k) == AZ(p)\n    p, k = \"9o pins on my lap\", bifid6\n    c = encipher_bifid(p, k)\n    assert p != c\n    assert isinstance(c, str)\n    assert c.isalpha()\n    assert decipher_bifid(c, k) == AZ(p)\n    p, k = \"playfair example\", \"bifidkey\"\n    c = encipher_bifid(p, k, symbols=bifid6)\n    assert p != c\n    assert isinstance(c, str)\n    assert c.isalpha()\n    assert decipher_bifid(c, k, symbols=bifid6) == AZ(p)\n", "def test_decipher_hill():\n    # Use a well-known key to test deciphering\n    key = Matrix([[3, 3], [2, 5]])\n    p = \"ATTACK\"\n    c = encipher_hill(p, key)\n    assert decipher_hill(c, key) == \"ATTACK\"\n\n    # Use a random key to test deciphering\n    key = Matrix([[7, 8], [11, 11]])\n    p = \"GONAVYBEATARMY\"\n    c = encipher_hill(p, key)\n    assert decipher_hill(c, key) == \"GONAVYBEATARMY\"\n"], "sample_1057": ["def test_render_as_module_fully_qualified():\n    settings = {'fully_qualified_modules': True}\n    content = Print('hello world')\n    module_str = render_as_module(content, standard='python3')\n    assert 'import sympy' in module_str\n", "def test_render_as_module_with_fully_qualified_modules():\n    content = [Print(1)]\n    module_str = render_as_module(content, standard='python3')\n    assert 'import builtins' in module_str\n    assert 'builtins.print(1)' in module_str\n", "def test_render_as_module_with_fully_qualified_modules():\n    printer = Print('x')\n    rendered_module = render_as_module(printer, standard='python3')\n    assert 'import sympy' in rendered_module\n    assert 'print(x)' in rendered_module\n", "def test_render_as_module_with_fully_qualified_modules():\n    content = Print(\"Hello, World!\")\n    module_str = render_as_module(content, standard='python3')\n    printer = PythonCodePrinter({'standard':'python3', 'fully_qualified_modules': True})\n    printer.doprint(content)\n    module_imports_str = '\\n'.join('import %s' % k for k in printer.module_imports)\n    assert module_str.startswith(module_imports_str + '\\n\\n')\n", "def test_render_as_module_with_fully_qualified_modules():\n    content = Print('Hello, World!')\n    result = render_as_module(content, 'python3')\n    assert 'import' in result\n    printer = PythonCodePrinter({'standard': 'python3', 'fully_qualified_modules': True})\n    pystr = printer.doprint(content)\n    module_imports_str = '\\n'.join('import %s' % k for k in printer.module_imports)\n    expected_result = module_imports_str + '\\n\\n' + pystr\n    assert result == expected_result\n"], "sample_1056": ["def test_LambdaPrinter_print_Boolean():\n    assert LambdaPrinter().doprint(True) == \"True\"\n    assert LambdaPrinter().doprint(False) == \"False\"\n    assert LambdaPrinter().doprint(x & y) == \"((x) and (y))\"\n    assert LambdaPrinter().doprint(x | y) == \"((x) or (y))\"\n    assert LambdaPrinter().doprint(~x) == \"(not (x))\"\n", "compilation error", "def test_LambdaPrinter_boolean_logic():\n    assert LambdaPrinter().doprint(x & y) == \"((x) and (y))\"\n    assert LambdaPrinter().doprint(x | y) == \"((x) or (y))\"\n    assert LambdaPrinter().doprint(~x) == \"(not (x))\"\n    assert LambdaPrinter().doprint(True) == \"True\"\n    assert LambdaPrinter().doprint(False) == \"False\"\n", "def test_LambdaPrinter_print_Boolean():\n    assert LambdaPrinter().doprint(True) == \"True\"\n    assert LambdaPrinter().doprint(False) == \"False\"\n    assert LambdaPrinter().doprint(x & y) == \"(x and y)\"\n    assert LambdaPrinter().doprint(x | y) == \"(x or y)\"\n    assert LambdaPrinter().doprint(~x) == \"(not (x))\"\n", "def test_Boolean_printing():\n    assert lambdarepr(True) == \"True\"\n    assert lambdarepr(False) == \"False\"\n    assert lambdarepr(x & y) == \"((x) and (y))\"\n    assert lambdarepr(x | y) == \"((x) or (y))\"\n    assert lambdarepr(~x) == \"(not (x))\"\n"], "sample_1058": ["def test_print_blockmatrix():\n    v = MatrixSymbol('v', 2, 1)\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    C = MatrixSymbol('C', 2, 2)\n    M = MatrixSymbol('M', 2, 1)\n    N = MatrixSymbol('N', 2, 1)\n    O = MatrixSymbol('O', 2, 1)\n    P = MatrixSymbol('P', 2, 1)\n    Q = MatrixSymbol('Q', 2, 1)\n    block1 = MatrixSymbol('block1', 1, 2)\n    block2 = MatrixSymbol('block2', 1, 2)\n    block3 = MatrixSymbol('block3', 1, 2)\n\n    expr = MatrixSymbol('block', 4, 4)\n    block_expr = block1.row_insert(1, block2).row_insert(2, block3)\n\n    test_mats = (\n        A,\n        B,\n        C,\n        block1,\n        block2,\n        block3,\n        v,\n        M,\n        N,\n        O,\n        P,\n        Q\n    )\n\n    test_block = (\n        MatrixSymbol('block', 2, 2),\n        MatrixSymbol('block', 2, 4),\n        MatrixSymbol('block', 4, 2),\n        MatrixSymbol('block', 4, 4),\n        block_expr,\n        expr\n    )\n\n    assign_expr_block = [Assignment(expr, test_block)]\n\n    assign_test_block = [\n        Assignment(expr, block1),\n        Assignment(expr, block1.col_insert(1, block2)),\n        Assignment(expr, block1.col_insert(1, block2).row_insert(1, block3)),\n    ]\n    assign_test_mat = [\n        Assignment(expr, B),\n        Assignment(expr, B.row_insert(0, block1)),\n        Assignment(expr, B.row_insert(0, block1).row_insert(2, block2)),\n        Assignment(expr, B.row_insert(0, block1).row_insert(2, block2).col_insert(2, block3)),\n        Assignment(expr, B.row_insert(0, block1).row_insert(2, block2).col_insert(2, block3).col_insert(1, v)),\n        Assignment(expr, B.row_insert(0,", "def test_SymPyPrinter():\n    n = symbols('n', integer=True)\n    printer = SymPyPrinter({'standard':'python3'})\n    assert printer.doprint(n) == 'n'\n    assert printer.doprint(Mod(x, y)) == 'sympy.Mod(x, y)'\n    assert printer.doprint(acos(x)) == 'sympy.acos(x)'\n    assert printer.doprint(sign(x)) == 'sympy.sign(x)'\n    assert printer.doprint(pi) == 'sympy.pi'\n    assert printer.doprint(S.Infinity) == 'sympy.oo'\n    assert printer.doprint(S.NegativeInfinity) == '-sympy.oo'\n    assert printer.doprint(S.ComplexInfinity) == 'sympy.zoo'\n    assert printer.doprint(sqrt(2)) == 'sympy.sqrt(2)'\n", "def test_module_imports():\n    printer = PythonCodePrinter({'standard':'python3'})\n\n    # Not imported by default\n    assert not printer.module_imports\n\n    # math is imported when needed\n    printer._print(sin(x))\n    assert 'math' in printer.module_imports\n    assert 'sin' in printer.module_imports['math']\n\n    # multiple math functions import math\n    printer._print(cos(x))\n    assert 'math' in printer.module_imports\n    assert 'sin' in printer.module_imports['math']\n    assert 'cos' in printer.module_imports['math']\n\n    # NumPy is imported when needed\n    printer = NumPyPrinter({'standard':'python3'})\n    printer._print(x + y)\n    assert 'numpy' in printer.module_imports\n    assert 'add' not in printer.module_imports['numpy']\n\n    # multiple modules can be imported\n    printer._print(sqrt(x))\n    assert 'math' in printer.module_imports\n    assert 'sqrt' in printer.module_imports['math']\n    assert 'numpy' in printer.module_imports\n", "def test_pycode_Piecewise():\n    expr = Piecewise((x, x < 1), (x**2, True))\n    assert pycode(expr) == \"((x) if (x < 1) else (x**2))\"\n    assert NumPyPrinter().doprint(expr) == \"\"\"numpy.select([x < 1],[x], default=(x**2))\"\"\"\n", "def test_PythonCodePrinter_print_Piecewise():\n    assignments = [\n        Assignment(x, Piecewise((x**2, x < 0), (x, x >= 0))),\n        Assignment(y, Piecewise((y**2, y < 0), (y, y >= 0), evaluate=False)),\n    ]\n    result = PythonCodePrinter().doprint(Assignment((), tuple(assignments)))\n    assert result == textwrap.dedent(\n        \"\"\"\n        x = x**2 if x < 0 else x\n        y = y**2 if y < 0 else y if y >= 0 else None\n    \"\"\"\n    ).strip()\n"], "sample_1060": ["def test_re():\n    assert PythonCodePrinter()._print_re(x) == 'x.as_real_imag()[0]'\n    assert NumPyPrinter()._print_re(x) == 'numpy.real(x)'\n", "def test_print_CodegenArrayPermuteDims():\n    n = symbols('n', integer=True, positive=True)\n    A = MatrixSymbol('A', n, n)\n    B = MatrixSymbol('B', n, n)\n    C = MatrixSymbol('C', n, n)\n    expr = A.permute(B, (0, 1))\n    assert NumPyPrinter().doprint(expr) == 'numpy.transpose(A, B)'\n", "def test_PythonCodePrinter_print_SparseMatrix():\n    M = SparseMatrix(10, 10, {})\n    assert PythonCodePrinter().doprint(M) == \"SparseMatrix(10, 10, {})\"\n", "def test_codegen_array_functions():\n    from sympy.codegen.array_utils import (CodegenArrayTensorProduct,\n                                           CodegenArrayContraction,\n                                           CodegenArrayDiagonal,\n                                           CodegenArrayPermuteDims,\n                                           CodegenArrayElementwiseAdd)\n    t1 = CodegenArrayTensorProduct(p[0], p[1])\n    t2 = CodegenArrayTensorProduct(p[2], p[3])\n    expr = CodegenArrayElementwiseAdd(t1, t2)\n    assert NumPyPrinter().doprint(expr) == \"numpy.add(numpy.einsum('i,j', p[0], p[1]), numpy.einsum('i,j', p[2], p[3]))\"\n", "def test_print_CodegenArrayContraction():\n    from sympy.codegen.array_utils import CodegenArrayContraction, CodegenArrayTensorProduct\n    expr = CodegenArrayContraction(CodegenArrayTensorProduct(p[x], p[y]))\n    assert NumPyPrinter().doprint(expr) == 'numpy.einsum(p_0, [0, 1])'\n"], "sample_1062": ["def test_TRpower():\n    assert TRpower(sin(x)**6) == -15*cos(2*x)/32 + 3*cos(4*x)/16 - cos(6*x)/32 + S(5)/16\n    assert TRpower(sin(x)**3*cos(2*x)**4) == \\\n        (3*sin(x)/4 - sin(3*x)/4)*(cos(4*x)/2 + cos(8*x)/8 + S(3)/8)\n    assert TRpower(sin(x)**2*sin(y)**3) == sin(x)**2*sin(y)**3\n", "def test_TR111():\n    assert TR111(1 - 1/tan(x)**2) == 1 - cot(x)**2\n    assert TR111(1 - 1/sin(x)**2) == 1 - csc(x)**2\n    assert TR111(1 - 1/cos(x)**2) == 1 - sec(x)**2\n    assert TR111(tan(x)**-2) == cot(x)**2\n    assert TR111(sin(x)**-2) == csc(x)**2\n    assert TR111(cos(x)**-2) == sec(x)**2\n", "def test_fu():\n    assert fu(sin(50)**2 + cos(50)**2 + sin(pi/6)) == S(3)/2\n    assert fu(sqrt(6)*cos(x) + sqrt(2)*sin(x)) == 2*sqrt(2)*sin(x + pi/3)\n    assert L(fu(cos(4*pi/9))) == 1\n    assert fu(cos(4*pi/9)) == sin(pi/18)\n    assert fu(cos(pi/9)*cos(2*pi/9)*cos(3*pi/9)*cos(4*pi/9)) == S(1)/16\n    assert fu(tan(7*pi/18)+tan(5*pi/18)-sqrt(3)*tan(5*pi/18)*tan(7*pi/18)) == -sqrt(3)\n    assert fu(cos(x)/sin(x)) == tan(x)\n    assert fu(cos(x)/sin(x), measure=lambda x: -x.count_ops()) == cos(x)/sin(x)\n    assert fu(-sqrt(6)*cos(x)*sin(y) - sqrt(2)*sin(x)*sin(y), measure=lambda x: -x.count_ops()) == (-sqrt(6)*cos(x) - sqrt(2)*sin(x))*sin(y)\n    assert fu(-sqrt(6)*cos(x)*sin(y) - sqrt(2)*sin(x)*sin(y)) == -2*sqrt(2)*sin(x + pi/3)*sin(y)\n    assert fu(cos(x)*cos(2*x)*cos(3*x)*cos(4*x)) == sin(8*x)/(16*sin(x))\n    assert fu(cos(x)*cos(2*x)*cos(3*x)*cos(4*x)*cos(5*x)) == sin(16*x)/(256*sin(x))\n", "def test_trig_split():\n    a, b = symbols('a b', positive=True)\n    assert trig_split(cos(a), cos(b)) == (1, 1, 1, a, b, True)\n    assert trig_split(cos(a), sin(b)) == None\n    assert trig_split(2*cos(a), -2*cos(b)) == (2, 1, -1, a, b, True)\n    assert trig_split(cos(a)*sin(x), cos(b)*sin(x)) == (sin(x), 1, 1, a, b, True)\n    assert trig_split(cos(a), -sqrt(3)*sin(a)) == (2, 1, -1, a, pi/3, False)\n    assert trig_split(sqrt(2)*cos(a), -sqrt(6)*sin(a)) == (2*sqrt(2), 1, -1, a, pi/3, False)\n    assert trig_split(cos(a)/sqrt(6), sin(a)/sqrt(2)) == (sqrt(6)/3, 1, 1, a, pi/6, False)\n", "def test_TR14():\n    assert TR14(1 - 1/cos(x)**2) == -tan(x)**2\n    assert TR14(1 - 1/sin(x)**2) == -cot(x)**2\n    assert TR14(cos(x)**2 + sin(x)**2) == 1\n    assert TR14(cos(x)**2 - sin(x)**2) == cos(2*x)\n    assert TR14(cos(x)*cos(y) + sin(x)*sin(y)) == cos(x-y)\n    assert TR14(1 - 1/cos(x)**2 - 1/sin(x)**2) == -tan(x)**2 - cot(x)**2\n"], "sample_1061": ["def test_Pow_evaluate():\n    x = Symbol('x')\n    y = Symbol('y')\n\n    assert Pow(x, 0, evaluate=True) == 1\n    assert Pow(0, y, evaluate=True) == 0**y\n    assert Pow(1, y, evaluate=True) == 1\n    assert Pow(x, 1, evaluate=True) == x\n    assert Pow(x, -1, evaluate=True) == 1/x\n    assert Pow(0, 0, evaluate=True) == 1\n    assert Pow(1, 0, evaluate=True) == 1\n", "def test_power_integer_log():\n    # Test integer_log(y, x)\n    assert integer_log(8, 2) == (3, True)\n    assert integer_log(16, 2) == (4, True)\n    assert integer_log(10, 2) == (3, False)\n    assert integer_log(25, 5) == (2, True)\n    assert integer_log(33, 2) == (5, False)\n", "def test_integer_nthroot():\n    assert integer_nthroot(16, 2) == (4, True)\n    assert integer_nthroot(26, 2) == (5, False)\n    assert integer_nthroot(27, 3) == (3, True)\n    assert integer_nthroot(28, 3) == (3, False)\n    raises(ValueError, lambda: integer_nthroot(-1, 2))\n    raises(ValueError, lambda: integer_nthroot(0, -1))\n    raises(ValueError, lambda: integer_nthroot(1, 0))\n", "def test_integer_nthroot():\n    assert integer_nthroot(16, 2) == (4, True)\n    assert integer_nthroot(26, 2) == (5, False)\n    assert integer_nthroot(0, 2) == (0, True)\n    assert integer_nthroot(-1, 3) == (-1, True)\n    assert integer_nthroot(2, 3) == (1, False)\n    assert integer_nthroot(27, 3) == (3, True)\n    assert integer_nthroot(28, 3) == (3, False)\n    assert integer_nthroot(16, 4) == (2, True)\n    assert integer_nthroot(17, 4) == (2, False)\n    assert integer_nthroot(81, 4) == (3, True)\n    assert integer_nthroot(82, 4) == (3, False)\n    assert integer_nthroot(0, 5) == (0, True)\n    assert integer_nthroot(1, 5) == (1, True)\n    assert integer_nthroot(2, 5) == (1, False)\n    assert integer_nthroot(243, 5) == (3, True)\n    assert integer_nthroot(244, 5) == (3, False)\n", "def test_integer_nthroot():\n    assert integer_nthroot(16, 2) == (4, True)\n    assert integer_nthroot(26, 2) == (5, False)\n    assert integer_nthroot(27, 3) == (3, True)\n    assert integer_nthroot(28, 3) == (3, False)\n    assert integer_nthroot(0, 3) == (0, True)\n    assert integer_nthroot(1, 3) == (1, True)\n    assert integer_nthroot(2, 3) == (1, False)\n    assert integer_nthroot(-1, 3) == (-1, True)\n    assert integer_nthroot(-2, 3) == (-1, False)\n    assert integer_nthroot(-27, 3) == (-3, True)\n    assert integer_nthroot(-28, 3) == (-3, False)\n"], "sample_1059": ["def test_jacobi_normalized():\n    n = Symbol('n')\n    a = Symbol('a')\n    b = Symbol('b')\n\n    assert jacobi_normalized(0, a, b, x) == 1 / sqrt(2**(a + b + 1) * gamma(a + 1) * gamma(b + 1) / (gamma(a + b + 1)))\n    assert jacobi_normalized(1, a, b, x) == (a/2 - b/2 + x*(a/2 + b/2 + 1)) / sqrt(2**(a + b + 1) * gamma(a + 2) * gamma(b + 1) / (2 * gamma(a + b + 2)))\n    assert jacobi_normalized(n, a, a, x) == jacobi_normalized(n, a, a, x)\n\n    assert jacobi_normalized(n, a, b, -x) == (-1)**n * jacobi_normalized(n, b, a, x)\n\n    assert jacobi_normalized(n, a, b, 0) == (2**(-n) * gamma(a + n + 1) / (gamma(a + 1) * factorial(n)) * hyper([-b - n, -n], [a + 1], -1)) / sqrt(2**(a + b + 1) * gamma(a + n + 1) * gamma(b + n + 1) / ((a + b + 2*n + 1) * factorial(n) * gamma(a + b + n + 1)))\n    assert jacobi_normalized(n, a, b, 1) == RisingFactorial(a + 1, n) / (factorial(n) * sqrt(2**(a + b + 1) * gamma(a + n + 1) * gamma(b + n + 1) / ((a + b + 2*n + 1) * factorial(n) * gamma(a + b + n + 1))))\n\n    assert conjugate(jacobi_normalized(n, a, b, x)) == jacobi_normalized(n, conjugate(a), conjugate(b), conjugate(x))\n\n    assert diff(jacobi_normalized(n, a, b, x), x) == (a/2 + b/2 + n/2 + 1/2) * jacobi_normalized(n - 1, a + 1, b + 1,", "def test_assoc_laguerre():\n    n = Symbol('n')\n    m = Symbol('m', positive=True)\n    a = Symbol('a')\n\n    assert assoc_laguerre(0, a, x) == 1\n    assert assoc_laguerre(1, a, x) == -x + a + 1\n    assert assoc_laguerre(2, a, x) == x**2/2 + x*(-a - 2) + a**2/2 + 3*a/2 + 1\n    assert assoc_laguerre(3, a, x) == (x**3*(-S(1)/6) + x**2*(a/2 + S(3)/2)\n        + x*(-a**2/2 - S(5)*a/2 - 3) + a**3/6 + a**2 + S(11)*a/6 + 1)\n\n    assert assoc_laguerre(n, 0, x) == laguerre(n, x)\n    assert assoc_laguerre(n, a, 0) == binomial(a + n, a)\n    assert assoc_laguerre(n, a, x).diff(x) == -assoc_laguerre(n - 1, a + 1, x)\n    assert assoc_laguerre(n, a, x).diff(a) == Sum(assoc_laguerre(m, a, x)/(-a + n), (m, 0, n - 1))\n", "def test_jacobi():\n    n = Symbol(\"n\")\n    a = Symbol(\"a\")\n    b = Symbol(\"b\")\n\n    assert jacobi(0, a, b, x) == 1\n    assert jacobi(1, a, b, x) == a/2 - b/2 + x*(a/2 + b/2 + 1)\n    assert jacobi(n, a, a, x) == RisingFactorial(a + 1, n)*gegenbauer(n, a + S.Half, x)/RisingFactorial(2*a + 1, n)\n    assert jacobi(n, 0, 0, x) == legendre(n, x)\n    assert jacobi(n, S.Half, S.Half, x) == RisingFactorial(3*S.Half, n)*chebyshevu(n, x)/factorial(n + 1)\n    assert jacobi(n, -S.Half, -S.Half, x) == RisingFactorial(S.Half, n)*chebyshevt(n, x)/factorial(n)\n    assert jacobi(n, a, b, -x) == (-1)**n*jacobi(n, b, a, x)\n    assert jacobi(n, a, b, 0) == 2**(-n)*gamma(a + n + 1)*hyper((-b - n, -n), (a + 1,), -1)/(factorial(n)*gamma(a + 1))\n    assert jacobi(n, a, b, 1) == RisingFactorial(a + 1, n)/factorial(n)\n    assert conjugate(jacobi(n, a, b, x)) == jacobi(n, conjugate(a), conjugate(b), conjugate(x))\n    assert diff(jacobi(n, a, b, x), x) == (a/2 + b/2 + n/2 + S.Half)*jacobi(n - 1, a + 1, b + 1, x)\n", "def test_jacobi_normalized():\n    n = Symbol('n')\n    a = Symbol('a')\n    b = Symbol('b')\n    x = Symbol('x')\n    assert jacobi_normalized(n, a, b, x) == jacobi(n, a, b, x)/sqrt(\n        2**(a + b + 1)*gamma(a + n + 1)*gamma(b + n + 1)/((a + b + 2*n + 1)*factorial(n)*gamma(a + b + n + 1)))\n", "def test_orthopoly_diff():\n    n = Symbol('n')\n    a = Symbol('a')\n    b = Symbol('b')\n    x = Symbol('x')\n\n    assert diff(jacobi(n, a, b, x), x) == (S.Half * (a + b + n + 1) * jacobi(n - 1, a + 1, b + 1, x))\n    assert diff(jacobi(n, a, b, x), a) == Sum((jacobi(n, a, b, x) + ((a + b + 2*k + 1) * RisingFactorial(b + k + 1, n - k) /\n        ((n - k) * RisingFactorial(a + b + k + 1, n - k))) * jacobi(k, a, b, x)) / (a + b + n + k + 1),\n        (k, 0, n - 1))\n\n    assert diff(gegenbauer(n, a, x), x) == 2*a*gegenbauer(n - 1, a + 1, x)\n    assert diff(chebyshevt(n, x), x) == n * chebyshevu(n - 1, x)\n    assert diff(chebyshevu(n, x), x) == ((n + 1) * chebyshevt(n + 1, x) - x * chebyshevu(n, x)) / (x**2 - 1)\n    assert diff(assoc_legendre(n, 1, x), x) == ((n + 1)*assoc_legendre(n, 1, x) - (2 + n)*assoc_legendre(n - 1, 1, x))/(x**2 - 1)\n    assert diff(hermite(n, x), x) == 2*n*hermite(n - 1, x)\n    assert diff(laguerre(n, x), x) == -assoc_laguerre(n - 1, 1, x)\n    assert diff(assoc_laguerre(n, a, x), x) == -assoc_laguerre(n - 1, a + 1, x)\n    assert diff(assoc_laguerre(n, a, x), a) == Sum(assoc_laguerre(k, a, x)/"], "sample_1063": ["def test_lambdify_with_tensorflow():\n    if not tensorflow:\n        skip(\"TensorFlow not installed\")\n    f = lambdify(x, x**2, 'tensorflow')\n    assert f(2) == 4\n    f = lambdify((x, y), x + y, 'tensorflow')\n    assert f(1, 2) == 3\n    f = lambdify(x, sin(x), 'tensorflow')\n    assert f(0) == 0\n", "def test_lambdastr():\n    f = lambdastr(x, x**2)\n    assert f == 'lambda x: (x**2)'\n    f = lambdastr((x, y, z), [z, y, x])\n    assert f == 'lambda x,y,z: ([z, y, x])'\n    f = lambdastr((x, (y, z)), x + y)\n    assert f.startswith('lambda _0,_1: (lambda x,y,z: (x + y))(_0,_1[0],_1[1])')\n", "def test_lambdify_piecewise():\n    f = Piecewise((x**2, x < 2), (x**3, x >= 2))\n    f_lambda = lambdify(x, f)\n    assert f_lambda(1) == 1\n    assert f_lambda(3) == 27\n    assert f_lambda(numpy.array([1, 2, 3])) == numpy.array([1, 8, 27])\n", "def test_lambdify_tuple_args():\n    # Tuple arguments should be handled and the lambdified function\n    # should be called with the same type of arguments as were used to create\n    # the function\n    f = lambdify((x, (y, z)), x + y)\n    assert f(1, (2, 4)) == 3\n    assert f(1, [2, 4]) != 3  # This should not work\n", "def test_lambdify_with_implemented_function_nested():\n    # Test lambdify with an implemented function that has a nested implemented function\n    f = implemented_function(Function('f'), lambda x: x+1)\n    g = implemented_function(Function('g'), lambda x: f(x)*2)\n    func = lambdify(x, g(x))\n    assert func(2) == 6\n"], "sample_1064": ["def test_tensorflow_code_piecewise():\n    from sympy import Piecewise\n    x = Symbol('x')\n    p = Piecewise((x, x > 1), (x**2, x < 1))\n    assert tensorflow_code(p) == (\n        'tensorflow.where(x > 1, x, tensorflow.where(x < 1, x**2, 0))')\n", "def test_tensorflow_code_Derivative():\n    f = Function('f')\n    x = Symbol('x')\n\n    # Higher order derivative\n    expr = Derivative(f(x), x, x)\n    result = tensorflow_code(expr)\n    assert \"tensorflow.gradients\" in result\n\n    # Higher order derivative with multiple variables\n    expr = Derivative(f(x, y), x, y)\n    result = tensorflow_code(expr)\n    assert \"tensorflow.gradients\" in result\n\n    # First order derivative of a multi-variable function\n    expr = Derivative(f(x, y), x)\n    result = tensorflow_code(expr)\n    assert \"tensorflow.gradients\" in result\n\n    # First order derivative of a single-variable function\n    expr = Derivative(f(x), x)\n    result = tensorflow_code(expr)\n    assert \"tensorflow.gradients\" in result\n", "def test_derivative():\n    f = Function('f')\n    expr = Derivative(f(x), x)\n    assert tensorflow_code(expr) == 'tensorflow.gradients(f(x), x)[0]'\n", "def test_tensorflow_code_Pow():\n    x = symbols('x')\n    expr = x**3.5\n    assert tensorflow_code(expr) == \"tensorflow.math.pow(x, 3.5)\"\n    expr = x**(1/2)\n    assert tensorflow_code(expr) == \"tensorflow.math.sqrt(x)\"\n", "def test_tensorflow_code_pow():\n    x = symbols('x')\n    expr = x**3\n    f = lambdify(x, expr, 'tensorflow')\n    rv = random.randint(0, 10)\n    graph = tf.Graph()\n    r = None\n    with graph.as_default():\n        tf_rv = eval(tensorflow_code(rv))\n        session = tf.compat.v1.Session(graph=graph)\n        r = session.run(f(tf_rv))\n    e = expr.subs(x, rv).evalf().doit()\n    assert abs(r-e) < 10**-6\n"], "sample_1065": ["def test_binomial_mod():\n    n, k = symbols('n k', integer=True)\n    assert binomial(n, k).subs({n: 5, k: 2})._eval_Mod(3) == 1\n    assert binomial(5, -2) % 3 == 0\n    assert binomial(10, 6) % 2 == 0\n    assert binomial(10, 6) % 3 == 1\n    assert binomial(10, 6) % 7 == 3\n    assert binomial(10, 6) % 13 == 6\n    assert binomial(10, 6) % 101 == 16\n", "def test_factorial_eval():\n    assert factorial(0) == 1\n    assert factorial(1) == 1\n    assert factorial(5) == 120\n    assert factorial(-1) == zoo\n    assert factorial(-3) == -zoo\n    assert factorial(3.0) == 6\n    assert factorial(-3.5) == zoo\n    assert factorial(nan) == nan\n    assert factorial(oo) == oo\n    assert factorial(-oo) == oo\n", "def test_issue_10388_tractable():\n    n = symbols('n')\n    assert binomial(n, 3).rewrite('tractable') == (n**3/6 - n**2/2 + n/3)*theta(n-2)*theta(n-1)*theta(n)\n    assert binomial(n, 0).rewrite('tractable') == 1\n    assert binomial(0, n).rewrite('tractable') == theta(-n+1)\n    assert binomial(-3, n).rewrite('tractable') == (-1)**n * theta(n+2) * theta(n+1) * theta(n) / factorial(n)\n    assert binomial(-4, n).rewrite('tractable') == (-1)**n * theta(n+3) * theta(n+2) * theta(n+1) * theta(n) / factorial(n)\n", "def test_factorial_mod():\n    assert factorial(100, evaluate=False).subs(n, Mod(n, 5)) == Mod(0, 5)\n    assert factorial(10, evaluate=False).subs(n, Mod(n, 7)) == Mod(0, 7)\n    assert factorial(10, evaluate=False).subs(n, Mod(n, 11)) == Mod(1, 11)\n    assert factorial(100, evaluate=False).subs(n, Mod(n, 101)) == Mod(1, 101)\n    assert factorial(100, evaluate=False).subs(n, Mod(n, 103)) == Mod(16, 103)\n    assert factorial(100, evaluate=False).subs(n, Mod(n, 10513)) == Mod(1, 10513)\n", "def test_factorial_as_poly():\n    x = symbols('x')\n    f = Poly(x**2 + 3*x + 2, x)\n    assert rf(f, 2) == Poly(x**4 + 6*x**3 + 13*x**2 + 12*x + 4, x)\n    assert ff(f, 2) == Poly(x**4 + 2*x**3 - 3*x**2 - 8*x + 4, x)\n"], "sample_1067": ["def test_mul_flatten():\n    x, y = symbols('x y')\n    a, b, c = symbols('a b c', commutative=False)\n    t = S(2)\n\n    assert Mul(x, x, 2) == 2*x**2\n    assert Mul(t, x, x) == x*2*x\n    assert Mul(t, x, y) == 2*x*y\n    assert Mul(x, y, t) == x*2*y\n    assert Mul(x, -2, y) == -2*x*y\n\n    assert Mul(a, b, c) == a*b*c\n", "def test_Mul_coeff():\n    x, y = symbols('x y')\n    assert (2*x).as_coeff_Mul()[0] == 2\n    assert (2*x).as_coeff_Mul()[1] == x\n    assert (x*2).as_coeff_Mul()[0] == 2\n    assert (x*2).as_coeff_Mul()[1] == x\n    assert (2*x*y).as_coeff_Mul()[0] == 2\n    assert (2*x*y).as_coeff_Mul()[1] == x*y\n    assert (x*2*y).as_coeff_Mul()[0] == 2\n    assert (x*2*y).as_coeff_Mul()[1] == x*y\n", "def test_Mul_coeff():\n    x, y = symbols('x y')\n    assert (3*x).coeff(x) == 3\n    assert (S(3)/2*x).coeff(x) == Rational(3, 2)\n    assert (3*x*y).coeff(x) == 3*y\n    assert (3*x*y).coeff(x, y) == 3\n    assert (S(5)/3*x).coeff(x) == Rational(5, 3)\n    assert (S(3)/2*x).coeff(x) == Rational(3, 2)\n    assert (S(5)/3*x).as_coeff_Mul()[0] == Rational(5, 3)\n    assert (3*x*y).as_coeff_Mul()[0] == 3\n    assert Mul(3, x, y).as_coeff_Mul()[0] == 3\n    assert (x*y).as_coeff_Mul()[0] == 1\n    assert (3*x).as_coeff_Mul()[1] == x\n    assert (3*x*y).as_coeff_Mul()[1] == x*y\n    assert Mul(3, x, y).as_coeff_Mul()[1] == Mul(x, y, evaluate=False)\n", "def test_Mul_coeff():\n    x = Symbol('x')\n    assert (3*x).as_coeff_Mul()[0] == 3\n    assert (x*3).as_coeff_Mul()[0] == 3\n    assert (S.Pi*x).as_coeff_Mul()[0] == S.Pi\n    assert (-S.Pi*x*y).as_coeff_Mul()[0] == -S.Pi\n    assert (x*y).as_coeff_Mul()[0] == 1\n    assert (2*x*y).as_coeff_Mul()[0] == 2\n    assert (-2*x*y).as_coeff_Mul()[0] == -2\n    assert (-2.0*x*y).as_coeff_Mul()[0] == -2.0\n    assert (-2*x*y).as_coeff_Mul()[0] == -2\n    assert (x*(2*y)).as_coeff_Mul()[0] == 2\n    assert ((2*x)*y).as_coeff_Mul()[0] == 2\n    assert (x*2*y).as_coeff_Mul()[0] == 2\n    assert (x*(y*2)).as_coeff_Mul()[0] == 2\n    assert (x*(2*y)).as_coeff_Mul()[0] == 2\n    assert ((2*x)*y).as_coeff_Mul()[0] == 2\n    assert (2*x*y).as_coeff_Mul()[0] == 2\n    assert (S(2)*x*y).as_coeff_Mul()[0] == S(2)\n    assert (x*(y*2)).as_coeff_Mul()[0] == 2\n    assert (x*y*2).as_coeff_Mul()[0] == 2\n", "def test_mul_new():\n    # Issue 19714\n    x, y = symbols('x y')\n    assert Mul(x, x) == x**2\n    assert Mul(y, x, x) == y * x**2\n"], "sample_1066": ["def test_MathMLPresentationPrinter_Lambda():\n    assert mpp.doprint(Lambda((x, y), x+y)) == \\\n        '<mrow><mfenced><mrow><mi>x</mi><mo>,</mo><mi>y</mi></mrow></mfenced><mo>\u21a6</mo><mrow><mi>x</mi><mo>+</mo><mi>y</mi></mrow></mrow>'\n", "def test_MathMLContentPrinter():\n    assert mp.doprint(diff(x**2, x)) == '<apply><diff/><bvar><ci>x</ci></bvar><apply><power/><ci>x</ci><cn>2</cn></apply></apply>'\n    assert mp.doprint(diff(x**2, x)/x) == '<apply><divide/><apply><diff/><bvar><ci>x</ci></bvar><apply><power/><ci>x</ci><cn>2</cn></apply></apply><ci>x</ci></apply>'\n    assert mp.doprint(Integral(x**2, x)) == '<apply><int/><bvar><ci>x</ci></bvar><apply><power/><ci>x</ci><cn>2</cn></apply></apply>'\n    assert mp.doprint(Sum(x**2, (x, 0, 10))) == '<apply><sum/><bvar><ci>x</ci></bvar><lowlimit><cn>0</cn></lowlimit><uplimit><cn>10</cn></uplimit><apply><power/><ci>x</ci><cn>2</cn></apply></apply>'\n    assert mp.doprint(Eq(x, 1)) == '<apply><eq/><ci>x</ci><cn>1</cn></apply>'\n    assert mp.doprint(Ne(x, 1)) == '<apply><neq/><ci>x</ci><cn>1</cn></apply>'\n    assert mp.doprint(Ge(x, 1)) == '<apply><geq/><ci>x</ci><cn>1</cn></apply>'\n    assert mp.doprint(Lt(x, 1)) == '<apply><leq/><ci>x</ci><cn>1</cn></apply>'\n", "def test_mathml_printer():\n    # Test that MathMLContentPrinter and MathMLPresentationPrinter are subclasses\n    # of MathMLPrinter\n    assert issubclass(MathMLContentPrinter, MathMLPrinter)\n    assert issubclass(MathMLPresentationPrinter, MathMLPrinter)\n", "def test_BasisDependent():\n    from sympy.vector import BasisDependent\n    from sympy.vector import CoordSys3D\n    C = CoordSys3D('C')\n    assert mpp.doprint(BasisDependent(C.i, 0)) == \\\n        '<msub><mover><mi mathvariant=\"bold\">i</mi><mo>^</mo></mover><mi mathvariant=\"bold\">C</mi></msub>'\n", "def test_MathMLPresentationPrinter():\n    m = Matrix(2, 2, [x, y, z, x])\n    assert mpp.doprint(m) == (\n        '<mtable><mtr><mtd><mi>x</mi></mtd><mtd><mi>y</mi></mtd></mtr>'\n        '<mtr><mtd><mi>z</mi></mtd><mtd><mi>x</mi></mtd></mtr></mtable>'\n    )\n\n    assert mpp.doprint(AccumBounds(1, 2)) == (\n        '<mrow><mover><mi></mi><mo>&#x2026;</mo></mover><mo>,</mo><mn>2</mn></mrow>'\n    )\n\n    assert mpp.doprint(Interval(1, 2)) == (\n        '<mfenced close=\"}\" open=\"{\"><mn>1</mn><mn>2</mn></mfenced>'\n    )\n\n    assert mpp.doprint(Limit(x, x, 0, '+')) == (\n        '<mrow><munder><mi>lim</mi><mrow><mi>x</mi><mo>&#x2192;</mo><mn>0</mn>'\n        '<mo>+</mo></mrow></munder><mi>x</mi></mrow>'\n    )\n\n    assert mpp.doprint(SingularityFunction(x, 1, 2)) == (\n        '<msup><mfenced close=\"&#x27e9;\" open=\"&#x27e8;\"><mrow><mi>x</mi><mo>-'\n        '</mo><mn>1</mn></mrow></mfenced><mrow data-mjx-texclass=\"ORD\"><mn>2</mn>'\n        '</mrow></msup>'\n    )\n\n    assert mpp.doprint(Range(5)) == (\n        '<mfenced close=\"}\" open=\"{\"><mn>0</mn><mn>1</mn><mo>,</mo><mo>&#x2026;</mo>'\n        '<mn>4</mn></mfenced>'\n    )\n\n    assert mpp.doprint(TribonacciConstant) == (\n        '<mi>&#x3A6;</mi>'\n    )\n\n    assert mpp.doprint(hbar) == (\n        '<mi>&#x210F;</mi>'\n    )\n\n    assert mpp.dop"], "sample_1068": ["def test_octave_code_Piecewise():\n    pw = Piecewise((x + 1, x > 0), (x, True))\n    assert octave_code(pw) == '((x > 0).*(x + 1) + (~(x > 0)).*(x))'\n    assert octave_code(pw, assign_to='y') == 'y = ((x > 0).*(x + 1) + (~(x > 0)).*(x));'\n\n    pw = Piecewise((x + 1, x > 0), (x - 1, x < 0), (x, True))\n    assert octave_code(pw) == '((x > 0).*(x + 1) + (~(x > 0) & (x < 0)).*(x - 1) + (~(x > 0) & ~(x < 0)).*(x))'\n    assert octave_code(pw, assign_to='y') == 'y = ((x > 0).*(x + 1) + (~(x > 0) & (x < 0)).*(x - 1) + (~(x > 0) & ~(x < 0)).*(x));'\n\n    pw = Piecewise((x + 1, x > 0), (x - 1, x < 0))\n    raises(ValueError, lambda: octave_code(pw))\n", "def test_indices():\n    n, m = symbols('n m', integer=True)\n    A = MatrixSymbol('A', n, m)\n    i = symbols('i', cls=Symbol, integer=True)\n    j = symbols('j', cls=Symbol, integer=True)\n\n    assert octave_code(A[i, j]) == \"A(i, j)\"\n    assert octave_code(A[1, 2]) == \"A(2, 3)\"\n\n    i, j = symbols('i j', integer=True, cls=Symbol)\n    assert octave_code(A[i, j]) == \"A(i, j)\"\n    assert octave_code(A[1, 2]) == \"A(2, 3)\"\n", "def test_octave_code_Assignment():\n    A = MatrixSymbol('A', 3, 3)\n    B = MatrixSymbol('B', 3, 3)\n    v = MatrixSymbol('v', 3, 1)\n    expr = Eq(A*v, B*v)\n    assert octave_code(expr) == \"\"\"v1 = v;", "def test_octave_code\u043c\u0435\u0446\u044cunsupported_functions():\n    class CustomFunction(Function):\n        pass\n\n    f = CustomFunction(\"f\")\n    raises(NotImplementedError, lambda: octave_code(f(x)))\n    assert octave_code(f(x), allow_unknown_functions=True) == \"f(x)\"\n", "def test_octave_code_Relational():\n    assert mcode(Eq(x, y), assign_to='y') == 'y = (x == y);'\n    assert mcode(Ne(x, y), assign_to='y') == 'y = (x ~= y);'\n    assert mcode(Gt(x, y), assign_to='y') == 'y = (x > y);'\n    assert mcode(Ge(x, y), assign_to='y') == 'y = (x >= y);'\n    assert mcode(Lt(x, y), assign_to='y') == 'y = (x < y);'\n    assert mcode(Le(x, y), assign_to='y') == 'y = (x <= y);'\n    assert mcode(Eq(x, y), assign_to=None) == 'x == y'\n"], "sample_1070": ["def test_log_expand_complex():\n    assert log(1 + I).expand() == log(sqrt(2)) + I*pi/4\n    assert log(1 - I).expand() == log(sqrt(2)) - I*pi/4\n    assert log(I).expand() == I*pi/2\n    assert log(-I).expand() == -I*pi/2\n    assert log(-1 + I).expand() == log(sqrt(2)) + I*3*pi/4\n    assert log(-1 - I).expand() == log(sqrt(2)) - I*3*pi/4\n", "def test_exp_assumptions():\n    x = Symbol('x', positive=True)\n    y = Symbol('y', real=True)\n    assert exp(x).is_positive\n    assert exp(x).is_real\n    assert exp(y).is_real\n    assert exp(y).is_positive is None\n    assert exp(I*pi/2).is_positive is None\n    assert exp(I*pi/2).is_nonnegative is None\n    assert exp(I*pi/2).is_real is None\n    assert exp(I*pi).is_positive is None\n    assert exp(I*pi).is_nonpositive\n    assert exp(I*pi).is_real\n    assert exp(I*3*pi/2).is_positive is None\n    assert exp(I*3*pi/2).is_nonnegative is None\n    assert exp(I*3*pi/2).is_real is None\n    assert exp(pi) > 20\n    assert exp(-3*pi) < 1/S(1000)\n", "def test_log_expand_complex():\n    x = Symbol('x', real=True)\n    y = Symbol('y', real=True)\n    assert log(x + I*y).expand() == log(sqrt(x**2 + y**2)) + I*sign(y)*acos(x/sqrt(x**2 + y**2))\n    assert log(x - I*y).expand() == log(sqrt(x**2 + y**2)) - I*sign(y)*acos(x/sqrt(x**2 + y**2))\n    assert log(x + I*y, 2).expand() == log(sqrt(x**2 + y**2), 2) + I*sign(y)*acos(x/sqrt(x**2 + y**2))/log(2)\n    assert log(x - I*y, 2).expand() == log(sqrt(x**2 + y**2), 2) - I*sign(y)*acos(x/sqrt(x**2 + y**2))/log(2)\n", "def test_exp_refine():\n    from sympy.assumptions import Q, ask\n    x = Symbol('x', real=True)\n    assert refine(exp(2*pi*I*x), Q.integer(x)) == 1\n    assert refine(exp(2*pi*I*x), ~Q.integer(x)) == exp(2*pi*I*x)\n    assert refine(exp(2*pi*I*x), Q.integer(2*x)) == exp(pi*I*(2*x % 2))\n", "def test_exp_polar():\n    assert exp_polar(2*pi*I) != 1\n    assert exp_polar(2*pi*I).simplify() == 1\n    assert exp_polar(I*pi/2).evalf() == I\n    assert exp_polar(-I*pi/2).evalf() == -I\n    assert exp_polar(I*(pi/2 + pi)).evalf() == -I\n    assert exp_polar(2*I).evalf() == -0.416146837 - 0.909297427*I\n"], "sample_1069": ["def test_glsl_code_Pow():\n    assert glsl_code(x**3.7) == 'pow(x, 3.7)'\n    assert glsl_code(x**-1) == '1.0/x'\n    assert glsl_code(x**0.5) == 'sqrt(x)'\n", "def test_glsl_code_Piecewise():\n    pw = Piecewise((x + 1, x > 0), (x, True))\n    assert glsl_code(pw) == '(x > 0) ? (x + 1) : x'\n    assert glsl_code(pw, assign_to='y') == \\\n        '''if (x > 0) {\n   y = x + 1;", "def test_glsl_code_MatrixElement_printing():\n    from sympy.matrices import MatrixSymbol\n    from sympy import symbols\n    A = MatrixSymbol(\"A\", 2, 3)\n    i, j = symbols('i j', integer=True)\n    assert glsl_code(A[i,j]) == 'A[%s][%s]' % (j,i)\n    assert glsl_code(A[i,j], mat_nested = False) == 'A[%s]' % (i + 3*j)\n    assert glsl_code(A[1,1], mat_nested = True, mat_transpose = True) == 'A[1][1]'\n    assert glsl_code(A[1,1], mat_nested = False, mat_transpose = True) == 'A[4]'\n", "def test_glsl_code_piecewise():\n    pw = Piecewise((x + 1, x > 0), (x, True))\n    assert glsl_code(pw) == 'x > 0 ? (x + 1) : x'\n    assign_to = symbols('y')\n    assert glsl_code(pw, assign_to) == 'if (x > 0) {\\n   y = x + 1;\\n}\\nelse {\\n   y = x;\\n}'\n", "def test_glsl_code_Assignment():\n    A, B = MatrixSymbol('A', 2, 2), MatrixSymbol('B', 2, 2)\n    f = Function('f')\n    v = MatrixSymbol('v', 2, 1)\n    n = symbols('n', integer=True)\n    assignment = A*v\n    assignment2 = B*A\n    assignment3 = f(n)\n    assert glsl_code(assignment, assign_to=v) == 'v = A*v;'\n    assert glsl_code(assignment2, assign_to=A) == 'A = B*A;'\n    assert glsl_code(assignment3, assign_to=n) == 'n = f(n);'\n"], "sample_1071": ["def test_quantity_simplify():\n    from sympy.physics.units.prefixes import kilo\n    from sympy.physics.units import foot, inch\n\n    assert quantity_simplify(kilo*foot*inch) == 250*foot**2/3\n    assert quantity_simplify(foot - 6*inch) == foot/2\n    assert quantity_simplify(foot + 6*inch) == 7*foot/2\n    assert quantity_simplify(foot - 12*inch) == -foot/2\n    assert quantity_simplify(foot + 12*inch) == 3*foot/2\n", "def test_convert_to_quantity_with_prefix():\n    # Test conversion to quantity with prefix\n    assert convert_to(mile, kilometer) == 25146*kilometer/15625\n    assert convert_to(mile, kilometer).n() == 1.609344*kilometer\n    assert convert_to(3*newton, kilogram*meter/second**2) == 3*kilogram*meter/second**2\n    assert convert_to(atomic_mass_constant, gram) == 1.660539060e-24*gram\n", "def test_quantity_simplify():\n    from sympy.physics.units.prefixes import kilo\n    from sympy.physics.units import foot, inch\n\n    # Test with prefix\n    expr = kilo * foot * inch\n    simplified_expr = quantity_simplify(expr)\n    assert simplified_expr == 250*foot**2/3\n\n    # Test with subtraction\n    expr = foot - 6*inch\n    simplified_expr = quantity_simplify(expr)\n    assert simplified_expr == foot/2\n\n    # Test with addition\n    expr = foot + 6*inch\n    simplified_expr = quantity_simplify(expr)\n    assert simplified_expr == 7*foot/6\n", "def test_convert_to_quantity_with_prefix():\n    # Test conversion to quantity with prefix\n    assert convert_to(1000*gram, kilogram) == 1*kilogram\n    assert convert_to(1000000*gram, kilogram) == 1000*kilogram\n\n    # Test conversion from quantity with prefix\n    assert convert_to(kilogram, gram) == 1000*gram\n    assert convert_to(2*kilogram, gram) == 2000*gram\n\n    # Test conversion between quantities with prefixes\n    assert convert_to(kilogram, kilogram) == 1*kilogram\n    assert convert_to(2*kilogram, kilogram) == 2*kilogram\n    assert convert_to(mile, kilometer) == 25146*kilometer/15625\n", "def test_convert_to_quantity_simplify():\n    q1 = 10 * meter\n    q2 = 2 * kilogram\n    assert convert_to(q1 / q2, [centimeter, gram]) == 500 * centimeter / gram\n\n    q1 = 10 * meter / second\n    q2 = 2 * kilogram\n    assert convert_to(q1 / q2, [centimeter, gram, second]) == 500 * centimeter / (gram * second)\n"], "sample_1073": ["def test_sqrt_match():\n    assert _sqrt_match(1 + r2 + r2*r3 + 2*r5) == [1 + r2 + r6, 2, 1 + r5]\n    assert _sqrt_match(1 + r2 + r2*r3 - 2*r5) == [1 + r2 + r6, -2, 1 + r5]\n    assert _sqrt_match(1 + r6) == [1, 1, r6]\n", "def test_sqrt_depth():\n    assert sqrtdenest.sqrt_depth(1 + sqrt(2)*(1 + sqrt(3))) == 1\n    assert sqrtdenest.sqrt_depth(1 + sqrt(2)*sqrt(1 + sqrt(3))) == 2\n    assert sqrtdenest.sqrt_depth(1 + sqrt(2)*sqrt(1 + sqrt(3)*sqrt(4))) == 3\n", "def test_sqrt_match():\n    assert _sqrt_match(1 + r2 + r2*r3 + 2*r5) == [1 + r2 + r6, 2, 1 + r5]\n    assert _sqrt_match(1 + r2 + r3 + 2*r5) == []\n    assert _sqrt_match(1 + r2 + r2*r3 + 2*r5 + 3*r7) == [1 + r2 + r6, 2, 1 + r5]\n", "def test_sqrtdenest_max_iter():\n    # Ensure max_iter limits the recursion depth\n    assert sqrtdenest(sqrt(2 + sqrt(2 + sqrt(2))), max_iter=1) == \\\n        sqrt(2 + sqrt(2 + sqrt(2)))\n    assert sqrtdenest(sqrt(2 + sqrt(2 + sqrt(2))), max_iter=2) == \\\n        sqrt(2 + sqrt(2 + sqrt(2)))\n", "def test_sqrt_match():\n    assert _sqrt_match(1 + 2*sqrt(3)) == [1, 2, 3]\n    assert _sqrt_match(1 + sqrt(2) + sqrt(3)) == [1 + sqrt(2), 1, 3]\n    assert _sqrt_match(sqrt(2) + sqrt(3)) == [0, sqrt(2), 3]\n    assert _sqrt_match(2 + 2*sqrt(3)) == [2, 2, 3]\n    assert _sqrt_match(1 + 2*sqrt(3) + 3*sqrt(4)) == [4, 2, 3]\n    assert _sqrt_match(sqrt(2)*sqrt(3) + sqrt(3)*sqrt(2)) == [0, 2*sqrt(2), 3]\n    assert _sqrt_match(1 + sqrt(2)*sqrt(3)) == [1, sqrt(2), 3]\n"], "sample_1072": ["def test_floor_inequalities():\n    x = Symbol('x', real=True)\n    assert (floor(x) > 3).simplify() == (x >= 4)\n    assert (floor(x) >= 3).simplify() == (x >= 3)\n    assert (floor(x) < 3).simplify() == (x < 3)\n    assert (floor(x) <= 3).simplify() == (x < 4)\n", "def test_floor_ceiling_frac_relational():\n    assert floor(x) <= x\n    assert floor(x) <= floor(x)\n    assert floor(x) < floor(x) + 1\n    assert floor(x) >= floor(x)\n    assert floor(x) > floor(x) - 1\n\n    assert ceiling(x) >= x\n    assert ceiling(x) >= ceiling(x)\n    assert ceiling(x) > ceiling(x) - 1\n    assert ceiling(x) <= ceiling(x)\n    assert ceiling(x) < ceiling(x) + 1\n\n    assert frac(x) >= 0\n    assert frac(x) < 1\n    assert frac(x) > -1\n    assert frac(x) <= 1\n    assert frac(x) >= frac(x)\n", "def test_floor_inequalities():\n    x = Symbol('x', real=True)\n    a = Symbol('a', integer=True)\n\n    assert floor(x) < a == Lt(x, a)\n    assert floor(x) <= a == Le(x, a)\n    assert floor(x) > a == Gt(x, a - 1)\n    assert floor(x) >= a == Ge(x, a - 1)\n\n    assert a < floor(x) == Lt(a, x)\n    assert a <= floor(x) == Le(a, x)\n    assert a > floor(x) == Gt(a - 1, x)\n    assert a >= floor(x) == Ge(a - 1, x)\n\n    assert floor(x) < floor(y) == Lt(x, floor(y))\n    assert floor(x) <= floor(y) == Le(x, floor(y))\n    assert floor(x) > floor(y) == Gt(x, floor(y) - 1)\n    assert floor(x) >= floor(y) == Ge(x, floor(y) - 1)\n", "def test_floor_inequalities():\n    x = symbols('x', real=True)\n    assert floor(x) <= x\n    assert floor(x) >= x - 1\n    assert floor(x) > x - 2\n    assert floor(x) < x + 1\n    assert floor(x) <= floor(x)\n    assert floor(x) >= floor(x - 1)\n    assert floor(x) < floor(x + 1)\n    assert floor(x) > floor(x - 2)\n    assert floor(x) <= floor(x + 1) - 1\n    assert floor(x) >= floor(x - 1) + 1\n", "def test_floor_ceiling_relational():\n    assert floor(y) >  3\n    assert floor(y) >= 3\n    assert floor(y) <  4\n    assert floor(y) <= 4\n\n    assert ceiling(y) >  3\n    assert ceiling(y) >= 4\n    assert ceiling(y) <  5\n    assert ceiling(y) <= 5\n\n    assert floor(y) >= ceiling(y) - 1\n    assert floor(y) <= ceiling(y)\n\n    assert floor(-y) >= -ceiling(y)\n    assert floor(-y) <= -floor(y)\n"], "sample_1075": ["def test_beta_eval_expand_func():\n    x = Symbol('x')\n    y = Symbol('y')\n    assert expand_func(beta(x, y)) == gamma(x)*gamma(y) / gamma(x + y)\n", "def test_beta_eval_expand_func():\n    x = Symbol('x')\n    y = Symbol('y')\n    assert beta(x, y)._eval_expand_func() == gamma(x)*gamma(y) / gamma(x + y)\n", "def test_beta_diff():\n    x = Symbol('x')\n    y = Symbol('y')\n    assert diff(beta(x, y), x) == beta(x, y)*(digamma(x) - digamma(x + y))\n    assert diff(beta(x, y), y) == beta(x, y)*(digamma(y) - digamma(x + y))\n    raises(ArgumentIndexError, lambda: beta(x, y).fdiff(3))\n", "def test_beta_eval_expand_func():\n    x = Symbol('x')\n    y = Symbol('y')\n    assert beta(x, y)._eval_expand_func() == gamma(x)*gamma(y) / gamma(x + y)\n    assert expand_func(beta(x, y)) == gamma(x)*gamma(y) / gamma(x + y)\n", "def test_beta_eval_expand_func():\n    x = Symbol('x')\n    y = Symbol('y')\n    assert beta(x, y)._eval_expand_func() == gamma(x)*gamma(y) / gamma(x + y)\n"], "sample_1074": ["def test_polycyclic_group():\n    G = DihedralGroup(8)\n    PcGroup = G.polycyclic_group()\n    assert is_isomorphic(PcGroup, G)\n", "def test_make_perm():\n    G = PermutationGroup([Permutation(0, 1, 2, 3), Permutation(0, 1)])\n    g = G.make_perm((0, 0))\n    assert g == Permutation()\n    g = G.make_perm((1, 0))\n    assert g == Permutation(0, 1, 2, 3)\n    g = G.make_perm((1, 1))\n    assert g == Permutation(0, 1)\n", "def test_coset_transversal():\n    # RFC: use a different method to compute transversal in this test\n    S = SymmetricGroup(8)\n    prop_even = lambda x: x.is_even\n    H = S.subgroup_search(prop_even)\n    assert len(S.coset_transversal(H)) == S.order()//H.order()\n", "def test_coset_transversal():\n    S = SymmetricGroup(4)\n    H = S.stabilizer(0)\n    C = S.coset_transversal(H)\n    assert len(C) == S.order()//H.order()\n    assert all(h*c not in C for h in H.generators for c in C)\n    assert all(h*c1 not in C or h*c1 == c2 for h in H.generators for c1, c2 in zip(C, C[1:]))\n", "def test_is_elementary():\n    a = PermutationGroup([Permutation(2)])\n    assert a.is_elementary(2) is True\n    b = PermutationGroup([Permutation(3)])\n    assert b.is_elementary(2) is False\n"], "sample_1076": ["def test_PythonCodePrinter_print_Piecewise():\n    prntr = PythonCodePrinter()\n    pw = Piecewise((x, x < -1), (0, x <= 1), (y, x > 1))\n    assert prntr._print_Piecewise(pw) == \"((x) if (x < -1) else ((0) if (x <= 1) else (y)))\"\n", "def test_tensor_indexed():\n    m = MatrixSymbol(\"M\", 3, 3)\n    assert pycode(m[1, 2], assign_to=None, standard='python3') == 'M[1, 2]'\n    p = p[x, y]\n    assert pycode(p, assign_to=None, standard='python3') == 'p[x, y]'\n", "def test_print_tensor():\n    M = MatrixSymbol(\"M\", 3, 3)\n    N = MatrixSymbol(\"N\", 3, 3)\n    Assign = Assignment(M, N)\n    assert PythonCodePrinter().doprint(Assign) == \"M = N\"\n", "def test_PythonCodePrinter_print_SparseMatrix():\n    n = 5\n    expr = SparseMatrix(n, n, {(1, 1): 2, (3, 3): 4})\n    result = PythonCodePrinter().doprint(expr)\n    assert result == \"SparseMatrix(5, 5, {(1, 1): 2, (3, 3): 4})\"\n", "def test_print_Rational():\n    assert PythonCodePrinter().doprint(Rational(3, 2)) == '3/2'\n    assert PythonCodePrinter({'standard': 'python2'}).doprint(Rational(3, 2)) == '3./2.'\n    assert NumPyPrinter().doprint(Rational(3, 2)) == '3/2'\n    assert SciPyPrinter().doprint(Rational(3, 2)) == '3/2'\n    assert SymPyPrinter().doprint(Rational(3, 2)) == 'sympy.Rational(3, 2)'\n    assert MpmathPrinter().doprint(Rational(3, 2)) == 'mpmath.mpf(3)/mpmath.mpf(2)'\n"], "sample_1077": ["def test_ComplexRegion_polar():\n    # Test polar in ComplexRegion\n    r1, theta1, r2, theta2 = symbols('r1 theta1 r2 theta2', real=True)\n    C1 = ComplexRegion(ImageSet(Lambda((r1, theta1), r1*exp(theta1*I)), Range(1, 5)*Interval(0, 2*pi)), polar=True)\n    C2 = ComplexRegion(ImageSet(Lambda((r2, theta2), r2*exp(theta2*I)), Range(1, 5)*Interval(0, pi)), polar=True)\n    assert C1.a_interval == Range(1, 5)\n    assert C1.b_interval == Interval(0, 2*pi)\n    assert C1.measure == 8*pi\n    assert C1.is_subset(C1)\n    assert C1.is_subset(C1)\n    assert C2.is_subset(C1)\n    assert C1.intersect(C2) == C2\n    assert C2.intersect(C1) == C2\n    assert C2.measure == 4*pi\n    assert C1.product(C2) == ProductSet(C1, C2)\n    p = S.Half + S.Half*I\n    assert p in C1\n    assert p not in C2\n", "def test_ComplexRegion_measure():\n    a, b = Interval(2, 5), Interval(4, 8)\n    c = Interval(0, 2*S.Pi)\n    c1 = ComplexRegion(a*b)\n    c2 = ComplexRegion(a*c, polar=True)\n    assert c1.measure == 12\n    assert c2.measure == 6*pi\n", "def test_Range_with_negative_step():\n    r = Range(10, 0, -2)\n    assert tuple(r) == (10, 8, 6, 4, 2)\n    assert r.reversed == Range(2, 12, 2)\n    assert 12 not in r\n    assert 5 not in r\n    assert 10 in r\n    assert r.inf == 2\n    assert r.sup == 10\n    assert len(r) == 5\n    assert r.size == 5\n    assert r.is_FiniteSet\n    assert not r.is_empty\n", "def test_ComplexRegion_contains():\n    a, b = Interval(2, 5), Interval(4, 8)\n    c = Interval(0, 2*S.Pi)\n    c1 = ComplexRegion(a*b)\n    assert c1.contains(3 + 5*I) is True\n    assert c1.contains(3 + 9*I) is False\n    assert c1.contains(6 + 5*I) is False\n    assert c1.contains(3 + 5*I) is True\n    assert c1.contains(3 + 5*I) is True\n\n    c2 = ComplexRegion(a*c, polar=True)\n    assert c2.contains(3 + 5*I) is True\n    assert c2.contains(3 + 9*I) is False\n    assert c2.contains(6 + 5*I) is False\n    assert c2.contains(3 + 5*I) is True\n    assert c2.contains(3 + 5*I) is True\n\n    a_tuple = (3, 5)\n    assert c1.contains(a_tuple) is True\n    assert c2.contains(a_tuple) is False\n    raises(ValueError, lambda: c1.contains((3, 5, 6)))\n", "def test_ComplexRegion_contains():\n    a, b = Interval(2, 5), Interval(4, 8)\n    c = Interval(0, 2*S.Pi)\n    c1 = ComplexRegion(a*b)\n    c2 = ComplexRegion(a*c, polar=True)\n    assert c1.contains(3 + 5*I) == True\n    assert c1.contains(6 + 5*I) == False\n    assert c2.contains(sqrt(13)*exp(3*I*pi/4)) == True\n    assert c2.contains(sqrt(13)*exp(5*I*pi/4)) == False\n    assert c1.contains((3, 5)) == True\n    assert c1.contains((6, 5)) == False\n    assert c2.contains((sqrt(13), 3*pi/4)) == True\n    assert c2.contains((sqrt(13), 5*pi/4)) == False\n"], "sample_1078": ["def test_indexedbase_assumptions():\n    i = Symbol('i', integer=True)\n    A_real = IndexedBase('A', real=True)\n    assert A_real.is_real\n    assert A_real[i].is_real\n\n    A_commutative = IndexedBase('A', commutative=False)\n    assert not A_commutative.is_commutative\n    assert not A_commutative[i].is_commutative\n\n    A_inherit = IndexedBase(i)\n    assert A_inherit.is_integer\n    assert A_inherit[i].is_integer\n\n    A_explicit = IndexedBase('A', integer=True)\n    assert A_inherit == A_explicit\n", "def test_Idx_properties():\n    i = Idx('i', 3)\n    assert i.label == Symbol('i', integer=True)\n    assert i.lower == 0\n    assert i.upper == 2\n\n    i = Idx('i', (1, 3))\n    assert i.label == Symbol('i', integer=True)\n    assert i.lower == 1\n    assert i.upper == 3\n\n    i = Idx('i')\n    assert i.label == Symbol('i', integer=True)\n    assert i.lower is None\n    assert i.upper is None\n", "def test_IndexedBase_assumptions():\n    a = IndexedBase('a', real=True)\n    assert a.is_real\n    assert not a.is_integer\n    assert not a.is_complex\n    assert a.is_commutative\n\n    a = IndexedBase('a', integer=True)\n    assert a.is_integer\n    assert a.is_real\n\n    a = IndexedBase('a', complex=True)\n    assert a.is_complex\n    assert not a.is_integer\n    assert not a.is_real\n\n    a = IndexedBase('a', commutative=False)\n    assert not a.is_commutative\n", "def test_Idx_shape_setting():\n    n, m, o, p = symbols('n m o p', integer=True)\n    i = Idx('i', (1, n))\n    j = Idx('j', m)\n    k = Idx('k', o)\n    l = Idx('l', p)\n\n    A = IndexedBase('A', shape=(n, m))\n    assert A.shape == (n, m)\n    assert A[i, j].shape == (n, m)\n\n    B = IndexedBase('B')\n    assert B[i, j, k, l].shape == (n, m, o, p)\n    assert B[i, j, k].shape == (n, m, o)\n\n    C = IndexedBase('C', shape=(o, p))\n    assert C[i, j].shape == (o, p)\n", "def test_indexed_shape_inference():\n    i, j = symbols('i j', integer=True)\n    n, m = symbols('n m', integer=True)\n    A = IndexedBase('A')\n    B = IndexedBase('B', shape=(n, m))\n\n    assert A[i, j].shape == (oo, oo)\n    assert B[i, j].shape == (n, m)\n    assert A[i, 2, j].shape == (oo, oo, oo)\n    assert B[i, 2, j].shape == (n, m)\n\n    raises(IndexException, lambda: IndexedBase('A', shape=(n, m))[i])\n    raises(IndexException, lambda: IndexedBase('A', shape=(n, m))[i, j, k])\n"], "sample_1079": ["def test_point_project():\n    p1 = Point(1, 2, 3)\n    p2 = Point(4, 5, 6)\n    p = p1.project(p2)\n    assert p.is_collinear(p1, p2)\n    assert p.is_collinear(p2, p1)\n    assert Point.project(p1, p2) == Point.project(p2, p1)\n    assert p1.project(p2) == p2.project(p1)\n    assert Point.project(p1, p1) == p1\n    assert p1.project(p1) == p1\n", "def test_point_transform():\n    p = Point(1, 2)\n    assert p.transform(Matrix([[1, 0, 1], [0, 1, 1], [0, 0, 1]])) == Point(2, 3)\n\n    p = Point2D(1, 2)\n    assert p.transform(Matrix([[1, 0, 1], [0, 1, 1], [0, 0, 1]])) == Point2D(2, 3)\n\n    p = Point3D(1, 2, 3)\n    assert p.transform(Matrix([[1, 0, 0, 1], [0, 1, 0, 1], [0, 0, 1, 1], [0, 0, 0, 1]])) == Point3D(2, 3, 4)\n", "def test_point_transform():\n    p = Point(1, 1)\n    assert p.transform(Matrix([[1, 0, 1], [0, 1, 1], [0, 0, 1]])) == Point(2, 2)\n    assert p.transform(Matrix([[1, 0, 0], [0, 1, 0], [0, 0, 1]])) == p\n\n    p3 = Point3D(1, 1, 1)\n    assert p3.transform(Matrix([[1, 0, 0, 1], [0, 1, 0, 1], [0, 0, 1, 1], [0, 0, 0, 1]])) == Point3D(2, 2, 2)\n    assert p3.transform(Matrix([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]])) == p3\n", "def test_Point_transform():\n    p = Point(1, 1)\n    assert p.transform(Matrix([[1, 0, 1], [0, 1, 1], [0, 0, 1]])) == Point(2, 2)\n    assert p.transform(Matrix([[1, 0, 0], [0, 1, 0], [0, 0, 1]])) == p\n    p3d = Point3D(1, 1, 1)\n    assert p3d.transform(Matrix([[1, 0, 0, 1], [0, 1, 0, 1], [0, 0, 1, 1], [0, 0, 0, 1]])) == Point3D(2, 2, 2)\n    assert p3d.transform(Matrix([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]])) == p3d\n", "def test_rotate():\n    p = Point2D(1, 1)\n    assert p.rotate(pi/2) == Point2D(-1, 1)\n    assert p.rotate(pi/2, (2, 0)) == Point2D(2, -1)\n    assert p.rotate(pi/2, Point2D(2, 0)) == Point2D(2, -1)\n"], "sample_1080": ["def test_refine_Pow():\n    assert refine((-1)**x, Q.real(x)) == (-1)**x\n    assert refine((-1)**x, Q.even(x)) == 1\n    assert refine((-1)**x, Q.odd(x)) == -1\n    assert refine((-1)**(x+y), Q.even(x)) == (-1)**y\n    assert refine((-1)**(x+y+z), Q.odd(x) & Q.odd(z)) == (-1)**y\n    assert refine((-1)**(x+y+2), Q.odd(x)) == (-1)**(y+1)\n    assert refine((-1)**(x+3), True) == (-1)**(x+1)\n", "def test_refine_pow():\n    assert refine((-1)**x, Q.real(x)) == (-1)**x\n    assert refine((-1)**x, Q.even(x)) == 1\n    assert refine((-1)**x, Q.odd(x)) == -1\n    assert refine((-1)**(x+y), Q.even(x)) == (-1)**y\n    assert refine((-1)**(x+y+z), Q.odd(x) & Q.odd(z)) == (-1)**y\n    assert refine((-1)**(x+y+2), Q.odd(x)) == (-1)**(y + 1)\n    assert refine((-1)**(x+3), True) == (-1)**(x + 1)\n", "def test_refine_re():\n    assert refine(re(x), Q.real(x)) == x\n    assert refine(re(x), Q.imaginary(x)) == 0\n    assert refine(re(x + y), Q.real(x) & Q.real(y)) == x + y\n    assert refine(re(x + y), Q.real(x) & Q.imaginary(y)) == x\n", "def test_refine_Pow():\n    assert refine((-1)**x, Q.real(x)) == (-1)**x\n    assert refine((-1)**x, Q.even(x)) == 1\n    assert refine((-1)**x, Q.odd(x)) == -1\n    assert refine((-1)**(x+y), Q.even(x)) == (-1)**y\n    assert refine((-1)**(x+y+z), Q.odd(x) & Q.odd(z)) == (-1)**y\n    assert refine((-1)**(x+y+2), Q.odd(x)) == (-1)**(y + 1)\n    assert refine((-1)**(x+3), True) == (-1)**(x + 1)\n    assert refine(sqrt(x**2), Q.real(x)) == Abs(x)\n    assert refine(sqrt(x**2), Q.positive(x)) == x\n    assert refine((2*x)**(2/3), Q.real(x)) == 2**(2/3)*Abs(x)**(2/3)\n", "def test_refine_Pow():\n    assert refine((-1)**x, Q.real(x)) == (-1)**x\n    assert refine((-1)**x, Q.even(x)) == 1\n    assert refine((-1)**x, Q.odd(x)) == -1\n    assert refine((-1)**(x+y), Q.even(x)) == (-1)**y\n    assert refine((-1)**(x+y+z), Q.odd(x) & Q.odd(z)) == (-1)**y\n    assert refine((-1)**(x+y+2), Q.odd(x)) == (-1)**(y + 1)\n    assert refine((-1)**(x+3), True) == (-1)**(x + 1)\n"], "sample_1081": ["def test_factorint():\n    # test some small factors\n    assert factorint(2**7*3**3, limit=10) == {2: 7, 3: 3}\n    # test with prime factor larger than limit\n    assert factorint(2**3*101, limit=10) == {2: 3, 101: 1}\n    # test with no small prime factors\n    assert factorint(10000000001, limit=10000) == {101: 1, 99009901: 1}\n    # test slow factor\n    assert factorint(8051) == {8051: 1}  # this will be slow if it doesn't use rho\n    # test factors that need pollard_rho\n    assert factorint(2417840*137438953347, limit=100000) == {2: 8, 3: 2, 5: 1, 7: 2, 13: 1, 17: 1, 2417840*137438953347//2**8/3**2/5/7**2/13/17: 1}\n    for x in range(1, 100):\n        assert multiproduct(factorint(x)) == x\n    # Test multiple factors of two\n    assert factorint(2**10) == {2: 10}\n    # test with -1 factor\n    assert factorint(-4) == {2: 2, -1: 1}\n    # test with unordered factors\n    assert sorted(factorint(6, visual=True).args) == sorted(Mul(2, 3, evaluate=False).args)\n    # test with powers of -1\n    assert factorint(2**2*3*(-2**2*3)**2) == {2: 6, 3: 3, -1: 2}\n    # test negatives and duplicates with factors\n    assert factorint(-12, visual=True) == -2**2*3\n    assert factorint(-12) == {2: 2, 3: 1, -1: 1}\n    # test for perfect squares\n    for x in [4, 9, 25, 36]:\n        assert factorint(sqrt(x)**2) == {sqrt(x): 2}\n    # tests for perfect_powers\n", "def test_smoothness():\n    assert smoothness(2**7*3**2) == (3, 128)\n    assert smoothness(2**4*13) == (13, 16)\n    assert smoothness(2) == (2, 2)\n", "def test_core():\n    assert core(24, 2) == 6\n    assert core(9424, 3) == 1178\n    assert core(379238, 10) == 379238\n    assert core(15**11, 10) == 15\n    raises(ValueError, lambda: core(0, 2))\n    raises(ValueError, lambda: core(-2, 2))\n    raises(ValueError, lambda: core(24, 1))\n    raises(ValueError, lambda: core(24, 0))\n    raises(ValueError, lambda: core(24, -2))\n", "def test_factorint():\n    assert factorint(1) == {}\n    assert factorint(2) == {2: 1}\n    assert factorint(3) == {3: 1}\n    assert factorint(4) == {2: 2}\n    assert factorint(5) == {5: 1}\n    assert factorint(6) == {2: 1, 3: 1}\n    assert factorint(7) == {7: 1}\n    assert factorint(8) == {2: 3}\n    assert factorint(9) == {3: 2}\n    assert factorint(10) == {2: 1, 5: 1}\n    assert factorint(11) == {11: 1}\n    assert factorint(12) == {2: 2, 3: 1}\n    assert factorint(1.0) == {}\n    raises(ValueError, lambda: factorint(0))\n    raises(ValueError, lambda: factorint(-1))\n    raises(ValueError, lambda: factorint(-2))\n    raises(ValueError, lambda: factorint(-3))\n\n    assert factorint(100) == {2: 2, 5: 2}\n    assert factorint(12345678901234567890) == {2: 1, 3: 2, 5: 1, 3607: 1, 3803: 1}\n\n    assert factorint(10**20) == {2: 20, 5: 20}\n    assert factorint(10**30) == {2: 30, 5: 30}\n    assert factorint(10**40) == {2: 40, 5: 40}\n", "def test_smoothness_p():\n    assert smoothness_p(10431, m=1) == (1, [(3, (2, 2, 4)), (19, (1, 5, 5)), (61, (1, 31, 31))])\n    assert smoothness_p(10431) == (-1, [(3, (2, 2, 2)), (19, (1, 3, 9)), (61, (1, 5, 5))])\n    assert smoothness_p(10431, power=1) == (-1, [(3, (2, 2, 2)), (61, (1, 5, 5)), (19, (1, 3, 9))])\n    assert smoothness_p({4: 2, 12: 3}) == 'p**i=3**2 has p-1 B=2, B-pow=2\\np**i=17**1 has p-1 B=2, B-pow=16'\n    assert smoothness_p({4: 2, 12: 3}, visual=False) == {3: 2, 17: 1}\n    assert smoothness_p(\"p**i=3**2 has p-1 B=2, B-pow=2\\np**i=17**1 has p-1 B=2, B-pow=16\") == 'p**i=3**2 has p-1 B=2, B-pow=2\\np**i=17**1 has p-1 B=2, B-pow=16'\n    assert smoothness_p(\"p**i=3**2 has p-1 B=2, B-pow=2\\np**i=17**1 has p-1 B=2, B-pow=16\", visual=False) == {3: 2, 17: 1}\n"], "sample_1082": ["def test_hyperbolic_functions():\n    x = Symbol('x')\n    y = Symbol('y')\n\n    # Test sinh\n    assert sinh(0) == 0\n    assert sinh(x).diff(x) == cosh(x)\n    assert sinh(x).series(x, 0, 10) == x + x**3/6 + x**5/120 + x**7/5040 + x**9/362880\n\n    # Test cosh\n    assert cosh(0) == 1\n    assert cosh(x).diff(x) == sinh(x)\n    assert cosh(x).series(x, 0, 10) == 1 + x**2/2 + x**4/24 + x**6/720 + x**8/40320\n\n    # Test tanh\n    assert tanh(0) == 0\n    assert tanh(x).diff(x) == 1 - tanh(x)**2\n    assert tanh(x).series(x, 0, 10) == x - x**3/3 + 2*x**5/15 - 17*x**7/315 + 62*x**9/2835\n\n    # Test coth\n    assert coth(1).diff() == -1/sinh(1)**2\n    assert coth(x).diff(x) == -1/sinh(x)**2\n    assert coth(x).series(x, 0, 4) == 1/x + x/3 - x**3/45 + 2*x**5/945\n\n    # Test csch\n    assert csch(1).diff() == -coth(1)*csch(1)\n    assert csch(x).diff(x) == -coth(x)*csch(x)\n    assert csch(x).series(x, 0, 5) == 1/x - x/6 + 7*x**3/360 - 31*x**5/15120\n\n    # Test sech\n    assert sech(1).diff() == -tanh(1)*sech(1)\n    assert sech(x).diff(x) == -tanh(x)*sech(x)\n    assert sech(x).series(x, 0, 5) == 1 - x**2/2 + 5*x**4/24\n\n    # Test asinh\n    assert asinh(", "def test_asinh():\n    x = Symbol('x')\n    assert asinh(x).diff(x) == 1/sqrt(x**2 + 1)\n    assert asinh(1) == log(1 + sqrt(2))\n    assert asinh(-1) == -log(1 + sqrt(2))\n    assert unchanged(asinh, 0)\n    assert asinh(-x) == -asinh(x)\n    assert asinh(I) == I*asin(1)\n", "def test_issue_18762():\n    x = Symbol('x')\n    assert acsch(x).diff(x) == -1/(x**2*sqrt(1/x**2 + 1))\n    assert asech(x).diff(x) == -1/(x*sqrt(1 - x**2))\n    assert acsch(1).diff(x) == 0\n    assert asech(1).diff(x) == 0\n", "def test_issue_21115():\n    x = Symbol('x')\n    assert csch(x)._eval_rewrite_as_cosh(x) == I / cosh(x + I * pi / 2)\n    assert sech(x)._eval_rewrite_as_sinh(x) == I / sinh(x + I * pi / 2)\n    assert coth(x)._eval_rewrite_as_sinh(x) == -I * sinh(I * pi / 2 - x) / sinh(x)\n    assert coth(x)._eval_rewrite_as_cosh(x) == -I * cosh(x) / cosh(I * pi / 2 - x)\n    assert tanh(x)._eval_rewrite_as_sinh(x) == I * sinh(x) / sinh(I * pi / 2 - x)\n    assert tanh(x)._eval_rewrite_as_cosh(x) == I * cosh(I * pi / 2 - x) / cosh(x)\n", "def test_csch():\n    x = Symbol('x')\n    y = Symbol('y')\n    assert csch(nan) == nan\n    assert csch(oo) == 0\n    assert csch(-oo) == 0\n    assert csch(0) == zoo\n    assert csch(x).diff(x) == -coth(x)*csch(x)\n    assert csch(x).as_real_imag() == (1 / (sinh(x)).as_real_imag()[0], 1 / (sinh(x)).as_real_imag()[1])\n    assert csch(I*y).as_real_imag() == (0, -1/sin(y))\n    assert csch(x).expand(frac=True) == 1/sinh(x)\n    assert csch(x).is_extended_real == sinh(x).is_extended_real\n    assert csch(x).is_finite == (1/sinh(x)).is_finite\n    assert csch(x).is_zero == (1/sinh(x)).is_zero\n"], "sample_1084": ["def test_intersection_Naturals_Rationals():\n    assert S.Naturals.intersect(S.Rationals) == S.Naturals\n", "def test_intersection_sets():\n    x, y = symbols('x y')\n    # test union subset\n    assert intersection_sets(FiniteSet(1, 2, 3), FiniteSet(2, 3, 4)) == FiniteSet(2, 3)\n    assert intersection_sets(FiniteSet(1, 2, 3), Interval(2, 4)) == FiniteSet(2, 3)\n    assert intersection_sets(FiniteSet(1, 2), Interval(3, 4)) == FiniteSet()\n    assert intersection_sets(Interval(1, 3), Interval(2, 4)) == Interval(2, 3)\n    assert intersection_sets(Interval(1, 3), Interval(4, 6)) == S.EmptySet\n    # test with symbolic end points\n    a, b = symbols('a b', real=True)\n    assert intersection_sets(Interval(1, 3), Interval(a, b)) == Interval(max(1, a), min(3, b))\n    # test with range\n    assert intersection_sets(Range(1, 5), Range(3, 7)) == Range(3, 5)\n    assert intersection_sets(Range(1, 5), Interval(3, 4)) == Range(3, 4)\n    assert intersection_sets(Range(1, 3), Interval(4, 6)) == S.EmptySet\n", "def test_intersection_Naturals_Rationals():\n    assert intersection_sets(S.Naturals, S.Rationals) == S.Naturals\n    assert intersection_sets(S.Naturals, S.Rationals).is_subset(S.Naturals) is True\n    assert intersection_sets(S.Naturals, S.Rationals).is_subset(S.Rationals) is True\n", "def test_intersection_sets_product_set():\n    s1 = ProductSet(Interval(1, 5), Interval(1, 5))\n    s2 = ProductSet(Interval(2, 6), Interval(2, 4))\n    assert intersection_sets(s1, s2) == ProductSet(Interval(2, 5), Interval(2, 4))\n    s1 = ProductSet(Interval(1, 5), Interval(1, 5), Interval(1, 5))\n    s2 = ProductSet(Interval(2, 6), Interval(2, 4), Interval(1, 5))\n    assert intersection_sets(s1, s2) == ProductSet(Interval(2, 5), Interval(2, 4), Interval(1, 5))\n    s1 = ProductSet(FiniteSet(1, 2, 3), FiniteSet(1, 2, 3), FiniteSet(1, 2, 3))\n    s2 = ProductSet(FiniteSet(2, 3), FiniteSet(1, 3), FiniteSet(1, 2, 4))\n    assert intersection_sets(s1, s2) == ProductSet(FiniteSet(2, 3), FiniteSet(1, 3), FiniteSet(1, 2))\n", "def test_intersection_sets_ComplexRegion():\n    # rectangular form\n    C1 = ComplexRegion(Interval(1, 3)*Interval(-1, 1))\n    C2 = ComplexRegion(Interval(2, 4)*Interval(-2, 0))\n    assert intersection_sets(C1, C2) == ComplexRegion(Interval(2, 3)*Interval(-1, 0))\n\n    # polar form\n    C1 = ComplexRegion(Interval(1, 2)*Interval(0, pi), polar=True)\n    C2 = ComplexRegion(Interval(1.5, 3)*Interval(pi/2, pi), polar=True)\n    assert intersection_sets(C1, C2) == ComplexRegion(Interval(1.5, 2)*Interval(pi/2, pi), polar=True)\n"], "sample_1083": ["def test_inverse_diff():\n    x = symbols('x', real=True)\n    assert asinh(x).diff(x) == 1/sqrt(1 + x**2)\n    assert acosh(x).diff(x) == 1/sqrt(x**2 - 1)\n    assert atanh(x).diff(x) == 1/(1 - x**2)\n    assert acoth(x).diff(x) == 1/(1 - x**2)\n    assert asech(x).diff(x) == -1/(x*sqrt(1 - x**2))\n    assert acsch(x).diff(x) == -1/(x**2*sqrt(1 + x**(-2)))\n", "def test_fdiff():\n    x = symbols('x')\n    assert sinh(x).fdiff() == cosh(x)\n    assert cosh(x).fdiff() == sinh(x)\n    assert tanh(x).fdiff() == 1 - tanh(x)**2\n    assert coth(x).fdiff() == -1/sinh(x)**2\n    assert sech(x).fdiff() == -sech(x)*tanh(x)\n    assert csch(x).fdiff() == -csch(x)*coth(x)\n    assert asinh(x).fdiff() == 1/sqrt(x**2 + 1)\n    assert acosh(x).fdiff() == 1/sqrt(x**2 - 1)\n    assert atanh(x).fdiff() == 1/(1 - x**2)\n    assert acoth(x).fdiff() == 1/(1 - x**2)\n    assert asech(x).fdiff() == -1/(x*sqrt(1 - x**2))\n    assert acsch(x).fdiff() == -1/(x**2*sqrt(1 + x**(-2)))\n", "def test_acsch():\n    x = Symbol('x')\n    y = Symbol('y')\n    pos = Symbol('pos', positive=True)\n    z = Symbol('z', zero=True)\n\n    assert acsch(x) == log(1/x + sqrt(1/x**2 + 1))\n    assert acsch(-x) == -log(1/x + sqrt(1/x**2 + 1))\n    assert acsch(I) == -I*pi/2\n    assert acsch(-2*I) == I*pi/6\n    assert acsch(I*(sqrt(6) - sqrt(2))) == -5*I*pi/12\n\n    assert acsch(x).diff(x) == -1/(x**2*sqrt(1 + x**(-2)))\n    assert acsch(pos).diff(pos) == -1/(pos**2*sqrt(1 + pos**(-2)))\n    assert acsch(z).diff(z) == -oo\n\n    assert acsch(x).as_real_imag()[0] == log(sqrt((x**2 + sqrt(x**4 + 1))/(x**2)))\n    assert acsch(x).as_real_imag()[1] == -I*pi/2\n\n    assert acsch(1) == log(1 + sqrt(2))\n    assert acsch(S(1)/2) == -2*acsch(2)\n", "def test_issue_17380():\n    x = symbols('x', real=True)\n    assert sinh(x).is_real\n    assert cosh(x).is_real\n    assert tanh(x).is_real\n    assert coth(x).is_real\n    assert sech(x).is_real\n    assert csch(x).is_real\n    assert asinh(x).is_real\n    assert acosh(x + 1).is_real\n    assert atanh(x/2).is_real\n    assert acoth(x + 1).is_real\n    assert asech(x/2).is_real\n    assert acsch(x).is_real\n", "def test_issue_10841():\n    x = Symbol('x')\n    assert sinh(x).rewrite(cosh) == -I*cosh(x + I*pi/2)\n    assert cosh(x).rewrite(sinh) == -I*sinh(x + I*pi/2)\n    assert tanh(x).rewrite(coth) == 1/coth(x)\n    assert coth(x).rewrite(tanh) == 1/tanh(x)\n    assert asinh(x).rewrite(acosh) == -I*acosh(x + I)\n    assert acosh(x).rewrite(asinh) == -I*asinh(x + I)\n    assert atanh(x).rewrite(acoth) == -I*acoth(x + I)\n    assert acoth(x).rewrite(atanh) == -I*atanh(x + I)\n    assert asech(x).rewrite(acsch) == -I*acsch(x + I)\n    assert acsch(x).rewrite(asech) == -I*asech(x + I)\n"], "sample_1086": ["def test_print_Quaternion():\n    q = Quaternion(1, 2, 3, 4)\n    assert sstr(q) == \"1 + 2*i + 3*j + 4*k\"\n    assert sstr(q, abbrev=True) == \"1 + 2*i + 3*j + 4*k\"\n", "def test_print_Predicate():\n    p = StrPrinter()\n    assert p._print_Symbol(S.is_integer) == 'Q.is_integer'\n    assert p._print_Predicate(S.is_integer) == 'Q.is_integer'\n", "def test_print_Dimension():\n    assert sstr(Dimension(\"length\")) == 'length'\n", "def test_print_Tensor():\n    from sympy.tensor.tensor import TensorIndexType, tensor_indices, TensorHead, Tensor\n    L = TensorIndexType(\"L\")\n    i, j, k = tensor_indices(\"i j k\", L)\n    A = TensorHead(\"A\", [L]*2, [[1]*2])\n    B = TensorHead(\"B\", [L]*2, [[1]*2])\n    t = Tensor(A, i, j)\n    assert str(t) == \"A(i, j)\"\n    assert str(t(i, -i)) == \"A(i, -i)\"\n    assert str(t(i, -i)*t(-i, i)) == \"A(i, -i)*A(-i, i)\"\n    assert str(t(i, -i)*t(-i, k)) == \"A(i, -i)*A(-i, k)\"\n    assert str(t(i, -i)*B(-i, i)) == \"A(i, -i)*B(-i, i)\"\n    assert str(t(i, k)*t(-i, -k)) == \"A(i, k)*A(-i, -k)\"\n", "def test_Tr():\n    A = MatrixSymbol('A', 3, 3)\n    assert sstr(Tr(A)) == 'Tr(A)'\n    assert sstr(Tr(A**2)) == 'Tr(A**2)'\n"], "sample_1085": ["def test_integer_nthroot():\n    assert integer_nthroot(16, 2) == (4, True)\n    assert integer_nthroot(27, 3) == (3, True)\n    assert integer_nthroot(16, 3) == (2, False)\n    assert integer_nthroot(-27, 3) == (-3, True)\n    assert integer_nthroot(0, 2) == (0, True)\n    assert integer_nthroot(-1, 3) == (-1, True)\n    assert integer_nthroot(2, 2) == (1, False)\n    assert integer_nthroot(-4, 2) == (2, False)\n", "def test_mpf_norm():\n    assert mpf_norm((1, 2, 3, 4), 10) == (1, 2, 3, 4)\n    assert mpf_norm((0, 0, 0, 0), 10) == (0, 0, 0, 0)\n    assert mpf_norm((1, 999999999, -14, 999999999), 10) == (1, 999999999, -14, 999999999)\n", "def test_algebraic_number():\n    from sympy.polys.numberfields import AlgebraicNumber, minimal_polynomial\n    from sympy.abc import x\n\n    minpoly = x**4 + 4*x**2 + 1\n    root = sqrt(sqrt(2) - 1)\n    an = AlgebraicNumber((minpoly, root))\n\n    assert an.is_real\n    assert an.is_algebraic\n    assert not an.is_Rational\n    assert an.is_integer is None\n    assert an.is AlgebraidNumber\n\n    assert same_and_same_prec(Float(an), Float(root.evalf()))\n\n    assert an.native_coeffs() == mpmath.mp.prec_to_dps(50)\n    assert an.coeffs() == mpmath.mp.prec_to_dps(50)\n\n    assert an.is_finite\n\n    assert an.floor() == 0\n    assert an.ceiling() == 1\n\n    assert an**2 == AlgebraicNumber((x**2 - 2*x + 1, sqrt(2) - 1))\n    assert an**3 == AlgebraicNumber((x**4 - 4*x**2 + 4, sqrt(2) - 1))\n\n    assert an != root\n    assert an == an\n    assert an != an.conjugate()\n\n    assert an.as_expr() == root\n\n    assert Integer(an) == 0\n    assert float(an) == float(root)\n\n    assert an.as_numer_denom() == (an, 1)\n\n    assert an > 0\n    assert an < 1\n\n    assert an.is_positive\n    assert not an.is_negative\n    assert not an.is_nonnegative\n    assert not an.is_nonpositive\n\n    assert an.as_poly(x) == an.as_poly()\n", "def test_Float_IS_NaN():\n    assert Float('nan').is_NaN\n    assert not Float('1.0').is_NaN\n    assert Float('nan')._mpf_ == _mpf_nan\n", "def test_Rational_as_content_primitive():\n    r = Rational(-3, 2)\n    c, p = r.as_content_primitive()\n    assert c == Rational(3, 2)\n    assert p == Rational(-1)\n"], "sample_1087": ["def test_swinnerton_dyer_poly():\n    raises(ValueError, lambda: swinnerton_dyer_poly(0, x))\n    raises(ValueError, lambda: swinnerton_dyer_poly(-1, x))\n    assert swinnerton_dyer_poly(1, x) == x**2 - 2\n    assert swinnerton_dyer_poly(2, x) == x**4 - 10*x**2 + 1\n    assert swinnerton_dyer_poly(3, x) == x**8 - 40*x**6 + 352*x**4 - 960*x**2 + 576\n    assert swinnerton_dyer_poly(4, x).is_irreducible\n    assert swinnerton_dyer_poly(5, x).is_irreducible\n    assert swinnerton_dyer_poly(6, x).is_irreducible\n", "def test_swinnerton_dyer_poly():\n    raises(ValueError, lambda: swinnerton_dyer_poly(0, x))\n    raises(ValueError, lambda: swinnerton_dyer_poly(-1, x))\n\n    assert swinnerton_dyer_poly(1, x) == Poly(x**2 - 2, x, domain=ZZ)\n    assert swinnerton_dyer_poly(2, x) == Poly(x**4 - 10*x**2 + 1, x, domain=ZZ)\n    assert swinnerton_dyer_poly(3, x) == Poly(x**8 - 40*x**6 + 352*x**4 - 960*x**2 + 576, x, domain=ZZ)\n\n    assert swinnerton_dyer_poly(4, x).domain == ZZ\n\n    assert swinnerton_dyer_poly(5, x, polys=False).is_Poly == False\n", "def test_swinnerton_dyer_poly():\n    raises(ValueError, lambda: swinnerton_dyer_poly(0))\n    raises(ValueError, lambda: swinnerton_dyer_poly(-1))\n\n    assert swinnerton_dyer_poly(1) == x**2 - 2\n    assert swinnerton_dyer_poly(2) == x**4 - 10*x**2 + 1\n    assert swinnerton_dyer_poly(3) == x**8 - 40*x**6 + 352*x**4 - 960*x**2 + 576\n\n    p = 2\n    a = [sqrt(2)]\n    for i in range(2, 5):\n        p = prime(p + 1)\n        a.append(sqrt(p))\n\n    assert swinnerton_dyer_poly(4, polys=True) == Poly(minimal_polynomial(Add(*a), x), x)\n", "def test_cyclotomic_poly():\n    raises(ValueError, lambda: cyclotomic_poly(0))\n    raises(ValueError, lambda: cyclotomic_poly(-1))\n\n    assert cyclotomic_poly(1) == Poly(x - 1, x, domain=ZZ)\n    assert cyclotomic_poly(2) == Poly(x + 1, x, domain=ZZ)\n    assert cyclotomic_poly(3) == Poly(x**2 + x + 1, x, domain=ZZ)\n    assert cyclotomic_poly(4) == Poly(x**2 + 1, x, domain=ZZ)\n    assert cyclotomic_poly(5) == Poly(x**4 + x**3 + x**2 + x + 1, x, domain=ZZ)\n    assert cyclotomic_poly(6) == Poly(x**2 - x + 1, x, domain=ZZ)\n    assert cyclotomic_poly(7) == Poly(x**6 + x**5 + x**4 + x**3 + x**2 + x + 1, x, domain=ZZ)\n    assert cyclotomic_poly(8) == Poly(x**4 + 1, x, domain=ZZ)\n    assert cyclotomic_poly(9) == Poly(x**6 + x**3 + 1, x, domain=ZZ)\n    assert cyclotomic_poly(10) == Poly(x**4 - x**3 + x**2 - x + 1, x, domain=ZZ)\n", "def test_swinnerton_dyer_poly():\n    raises(ValueError, lambda: swinnerton_dyer_poly(0))\n    raises(ValueError, lambda: swinnerton_dyer_poly(-1))\n    assert swinnerton_dyer_poly(1) == x**2 - 2\n    assert swinnerton_dyer_poly(2) == x**4 - 10*x**2 + 1\n    assert swinnerton_dyer_poly(3) == x**8 - 40*x**6 + 352*x**4 - 960*x**2 + 576\n    assert swinnerton_dyer_poly(4, polys=True).is_Poly\n"], "sample_1088": ["def test_symmetrize1():\n    raises(MultivariatePolynomialError, lambda: symmetrize(x + 2*y))\n    raises(MultivariatePolynomialError, lambda: symmetrize(x + 2*y, formal=True))\n    assert symmetrize(x + 2*y, x, y) == (2*x + 2*y - 2*x, 0)\n    assert symmetrize(x + 2*y, formal=True, x, y) == (2*s1 - 2*x, 0, [(s1, x + y)])\n", "def test_viete():\n    raises(ValueError, lambda: viete(7))\n    assert viete(x**2 + 2*x + 2) == [(x0 + x1, -2), (x0*x1, 2)]\n    assert viete(a*x**2 + b*x + c, [r1, r2], x) == [(r1 + r2, -b/a), (r1*r2, c/a)]\n    raises(ValueError, lambda: viete(a*x**2 + b*x + c, [r1], x))\n    raises(MultivariatePolynomialError, lambda: viete(a*x**2 + b*y + c, [r1, r2]))\n    assert viete(x**2 + y**2, [r1, r2], x) == [(r1 + r2, 0), (r1*r2, y**2)]\n", "def test_symmetrize():\n    raises(MultivariatePolynomialError, lambda: symmetrize(x**2 + y**2, x, y, formal=True, symbols=[x, y, z]))\n    assert symmetrize(x**2 + 2*y**2) == (-2*x*y + (x + y)**2, y**2)\n    assert symmetrize(x**2 + 2*y**2, formal=True) == (S(1)**2*(x + y)**2 - 2*x*y, y**2, [(S(1), x + y), (x*y, x*y)])\n", "def test_rational_interpolate():\n    data = [(1, -210), (2, -35), (3, 105), (4, 231), (5, 350), (6, 465)]\n    result = rational_interpolate(data, 2)\n    assert str(result) == '(105*x**2 - 525)/(x + 1)'\n", "def test_viete():\n    raises(MultivariatePolynomialError, lambda: viete(x**2 + y**2))\n    raises(ValueError, lambda: viete(x**2 + 2*x + 1, [a]))\n    raises(ValueError, lambda: viete(x**2 + 2*x + 1, [a, b, c]))\n"], "sample_1089": ["def test_decompose_power():\n    # when exponent is a non-integer Rational, leave as a single factor\n    assert decompose_power(x**(S(3)/2)) == (x**(S(1)/2), 3)\n    # when exponent is an Integer, it is returned as 1 power of base, i.e. x = x**1\n    assert decompose_power(x**3) == (x, 3)\n    # when exponent is a number, it is returned as is\n    assert decompose_power(x**2.0) == (x, 2.0)\n    assert decompose_power(x**S.One) == (x, 1)\n    # when exponent is a sympy fraction with a denominator that can be\n    # simplified, take out the gcf that is removed\n    assert decompose_power(x**(2/S(6))) == (x**(S(1)/3), 2)\n    assert decompose_power(x**(2/S(3))) == (x**(S(1)/3), 2)\n    assert decompose_power(x**(S(6)/2)) == (x**(S(1)/2), 6)\n    assert decompose_power(sqrt(x**(3*S(2)))) == (x**(3), S(1)/2)\n    assert decompose_power(sqrt(x**(2*S(2)))) == (x**(2), S(1)/2)\n    assert decompose_power(x**(2*y/3)) == (x**(y/3), 2)\n", "def test_factor_terms_fraction():\n    assert factor_terms(x/2 + 1, fraction=True) == (x + 2)/2\n    assert factor_terms(x/2 + 1/x, fraction=True) == (x**2 + 2)/(2*x)\n    assert factor_terms(x/2/y + 1/x/y, fraction=True) == (x**2 + 2)/(2*x*y)\n", "def test_factor_terms_fraction():\n    # issue 15475\n    expr = (x**2 + 2*x + 1)/(x + 1)\n    assert factor_terms(expr, fraction=True) == (x + 1)\n\n    expr = (x**2 + 2*x + 1)/(x + 1) + (x**2 + 3*x + 2)/(x + 2)\n    assert factor_terms(expr, fraction=True) == (x + 1)/(x + 2) + (x + 2)\n\n    expr = (x**2 + 2*x + 1)/(x + 1) + (x**2 + 3*x + 2)/(x + 2) + 3\n    assert factor_terms(expr, fraction=True) == (3*(x + 1)*(x + 2) + (x + 1)**2*(x + 2) + (x + 1)*(x + 2)**2)/((x + 1)*(x + 2))\n", "def test_monotonic_sign():\n    assert _monotonic_sign(x) is None\n    assert _monotonic_sign(-x) is None\n    assert _monotonic_sign(x + 1).is_positive\n    assert _monotonic_sign(-x - 1).is_negative\n    assert _monotonic_sign(x + x**2) is None\n    assert _monotonic_sign(-x - x**2).is_negative is None\n    assert _monotonic_sign(x + I) is None\n    assert _monotonic_sign(-x - I) is None\n\n    xpos = Dummy(integer=True, positive=True)\n    assert _monotonic_sign(xpos + 1).is_positive\n    assert _monotonic_sign(-xpos - 1).is_negative\n\n    xneg = Dummy(integer=True, negative=True)\n    assert _monotonic_sign(xneg + 1).is_positive\n    assert _monotonic_sign(-xneg - 1).is_negative\n\n    xnonneg = Dummy(integer=True, nonnegative=True)\n    assert _monotonic_sign(xnonneg + 1).is_positive\n    assert _monotonic_sign(-xnonneg - 1) is None\n\n    xnonpos = Dummy(integer=True, nonpositive=True)\n    assert _monotonic_sign(xnonpos + 1) is None\n    assert _monotonic_sign(-xnonpos - 1).is_negative\n\n    assert _monotonic_sign(2*x + 3) == 3\n    assert _monotonic_sign(x/2 + 2) == 2\n", "def test_gcd_terms_clear_coefficients():\n    assert gcd_terms(x/2 + 1, clear=False) == x/2 + 1\n    assert gcd_terms(x/2 + y/2, clear=False) == (x + y)/2\n    assert gcd_terms(x/2 + 1/x, clear=False) == x/2 + 1/x\n    assert gcd_terms(x/2 + 1, clear=True) == (x + 2)/2\n"], "sample_1091": ["def test_relational_binary_symbols():\n    assert Eq(x, True).binary_symbols == {x}\n    assert Eq(True, x).binary_symbols == {x}\n    assert Eq(x, y).binary_symbols == set()\n    assert Eq(x, False).binary_symbols == {x}\n    assert Eq(False, x).binary_symbols == {x}\n    assert Eq(x, y).binary_symbols == set()\n    assert Ne(x, True).binary_symbols == {x}\n    assert Ne(True, x).binary_symbols == {x}\n    assert Ne(x, y).binary_symbols == set()\n    assert Ge(x, 1).binary_symbols == set()\n    assert Lt(x, 1).binary_symbols == set()\n", "def test_reversed():\n    assert Eq(x, 1).reversed == Eq(1, x)\n    assert Lt(x, 1).reversed == Gt(1, x)\n    assert Le(x, 1).reversed == Ge(1, x)\n    assert Gt(x, 1).reversed == Lt(1, x)\n    assert Ge(x, 1).reversed == Le(1, x)\n    assert Ne(x, 1).reversed == Ne(1, x)\n", "def test_Relational_as_set():\n    from sympy.solvers.inequalities import solve_univariate_inequality\n    from sympy.sets.conditionset import ConditionSet\n    x = Symbol('x', real=True)\n\n    rel = Eq(x, 1)\n    assert rel.as_set() == FiniteSet(1)\n\n    rel = Lt(x, 1)\n    assert rel.as_set() == Interval(-oo, 1, right_open=True)\n\n    rel = Le(x, 1)\n    assert rel.as_set() == Interval(-oo, 1)\n\n    rel = Gt(x, 1)\n    assert rel.as_set() == Interval(1, oo, left_open=True)\n\n    rel = Ge(x, 1)\n    assert rel.as_set() == Interval(1, oo)\n\n    rel = Ne(x, 1)\n    assert rel.as_set() == Interval(-oo, 1, right_open=True) | Interval(1, oo, left_open=True)\n", "def test_all_inequalities():\n    # tests coverage of Relational classes\n    relations = [Eq, Lt, Le, Gt, Ge, Ne]\n    args = [(1, 2), (2, 1), (1, 1), (1, S.NaN), (S.NaN, 1), (S.NaN, S.NaN)]\n    for a, b in args:\n        for r in relations:\n            assert isinstance(r(a, b), (bool, Relational))\n", "def test_relational_simplify():\n    # issue 20714\n    e1 = Eq(x/y, 1)\n    e2 = Eq(x, y)\n    assert e1.simplify() == e2\n    assert e2.simplify() == e2\n\n    # issue 20759\n    e1 = Eq(x - y, -z)\n    e2 = Eq(x - y + z, 0)\n    e3 = Eq(x + z, y)\n    assert e1.simplify() == e2\n    assert e2.simplify() == e3\n    assert e3.simplify() == e3\n"], "sample_1090": ["def test_Rational_new():\n    r1 = S.Rational(1, 2)\n    r2 = r1.__new__(S.Rational, 3, 4)\n    assert r1 == S.Rational(1, 2)\n    assert r2 == S.Rational(3, 4)\n", "def test_issue_10554():\n    assert (S(2)/oo)**2 == S.Zero\n    assert (2/S.Infinity)**2 == S.Zero\n    assert (2/S.NegativeInfinity)**2 == S.Zero\n    assert (S(-2)/oo)**2 == S.Zero\n    assert (-2/S.Infinity)**2 == S.Zero\n    assert (-2/S.NegativeInfinity)**2 == S.Zero\n    assert S.NegativeInfinity/_sympify(3.0) == S.NegativeInfinity\n    assert S.Infinity/_sympify(3.0) == S.Infinity\n    assert S.Infinity/_sympify(-3.0) == S.NegativeInfinity\n    assert 1/_sympify(-3.0) == -S.One/_sympify(3.0)\n    assert S.ImaginaryUnit**S.ImaginaryUnit == exp(-S.Pi/2)\n    assert S.ImaginaryUnit**(S.ImaginaryUnit/3) == exp(-S.Pi/6)\n    assert S.ImaginaryUnit**(S.ImaginaryUnit/7) == exp(-S.Pi/14)\n    assert S.ImaginaryUnit**(3*S.ImaginaryUnit) == exp(-3*S.Pi/2)\n    assert S.ImaginaryUnit**(3*S.ImaginaryUnit/2) == exp(-3*S.Pi/4)\n    assert sqrt(2)*oo == oo\n    assert sqrt(2)*-oo == -oo\n    assert sqrt(2)/oo == S.Zero\n    assert sqrt(2)/-oo == S.Zero\n    assert sqrt(2)*S.Infinity == S.Infinity\n    assert sqrt(2)*S.NegativeInfinity == S.NegativeInfinity\n    assert sqrt(2)/S.Infinity == S.Zero\n    assert sqrt(2)/S.NegativeInfinity == S.Zero\n    assert oo.is_integer is None\n    assert -oo.is_integer is None\n    assert oo.is_nonnegative is None\n    assert -oo.is_nonnegative is None\n    assert oo.is_nonpositive is None\n    assert -oo.is_nonpositive is None\n    assert oo.is_real is True\n    assert -oo.is_real is True\n    assert oo.is_zero is False\n    assert -oo.is_zero is False\n    assert (S(0)/S(0)).simplify() == S.NaN\n   ", "def test_issue_17548():\n    assert (1 + oo)**2 == oo\n    assert (1 + oo)**3 == oo\n    assert (1 + oo)**(-1) == 0\n    assert (1 + oo)**(-2) == 0\n    assert (1 + oo)**(S.Half) == oo\n    assert (1 + oo)**(S(3)/2) == oo\n    assert (1 + oo)**(-S.Half) == 0\n    assert (1 + oo)**(S(-3)/2) == 0\n", "def test_issue_15946():\n    assert (S.Exp1 ** 2).evalf() == S.Exp1.evalf() ** 2\n    assert (S.Pi ** 2).evalf() == S.Pi.evalf() ** 2\n    assert (S.GoldenRatio ** 2).evalf() == S.GoldenRatio.evalf() ** 2\n    assert (S.TribonacciConstant ** 2).evalf() == S.TribonacciConstant.evalf() ** 2\n    assert (S.EulerGamma ** 2).evalf() == S.EulerGamma.evalf() ** 2\n    assert (S.Catalan ** 2).evalf() == S.Catalan.evalf() ** 2\n", "def test_Pow_number_args():\n    # x**(1/2) is automatically evaluated to sqrt(x) but\n    # this behavior is not wanted with evaluate=False. It should\n    # remain unevaluated, though it should still be canonicalized\n    assert Pow(x, S.Half, evaluate=False).is_Pow and not isinstance(Pow(x, S.Half, evaluate=False), (Float, Integer, Rational))\n    # if the base is a number then it will always be evaluated\n    assert Pow(S.Half, 2) == S.One/4\n    assert Pow(S.Half, 2, evaluate=False) == S.One/4\n    assert Pow(S.Zero, 0) == 1\n    assert Pow(S.Zero, 0, evaluate=False) == 1\n"], "sample_1092": ["def test_cse_funcargtracker_getargsinvalueorder():\n    tracker = cse_main.FuncArgTracker([x + y, x + y + z])\n    args = [tracker.get_or_add_value_number(arg) for arg in [x, y]]\n    assert tracker.get_args_in_value_order(args) == [x, y]\n", "compilation error", "def test_cse_empty():\n    assert cse([], symbols('x')) == ([], [])\n", "def test_cse_matrix_symbols():\n    # Test CSE with MatrixSymbols\n    A = MatrixSymbol('A', 3, 3)\n    B = MatrixSymbol('B', 3, 3)\n    exprs = [A*A*B, A*B*A]\n    replacements, reduced_exprs = cse(exprs)\n    assert len(replacements) == 1\n    assert isinstance(replacements[0][0], Symbol)\n    assert replacements[0][1] == A*B\n    assert reduced_exprs == [A*replacements[0][0], replacements[0][0]*A]\n", "def test_cse_prepreprocess():\n    # Test that cse_main.cse_main.preprocess_for_cse does not destroy Tuple\n    e = Tuple(Tuple(x), x)\n    assert preprocess_for_cse(e, basic_optimizations) == e\n"], "sample_1093": ["def test_pythoncode_reserved_words():\n    x, y = symbols('x y')\n    f = Piecewise((x + y, x > y), (x - y, True))\n    assert pycode(f) == \"\"\"((x + y) if (x > y) else (x - y))\"\"\"\n", "compilation error", "def test_pycode_IndexedBase():\n    assert pycode(p[0]) == 'p[0]'\n    assert pycode(p[0], fully_qualified_modules=False) == 'p[0]'\n    assert pycode(p[0], fully_qualified_modules=True) == 'p[0]'\n", "def test_pycode_Piecewise():\n    expr = Piecewise((x, x < 1), (x**2, x >= 1))\n    assert pycode(expr) == '((x) if (x < 1) else (x**2))'\n    assert NumPyPrinter().doprint(expr) == 'numpy.select([x < 1, True], [x, x**2], default=numpy.nan)'\n", "def test_pythoncodeprinter_print_relational():\n    prntr = PythonCodePrinter()\n    assert prntr._print_Relational(Eq(x, y)) == \"(x == y)\"\n    assert prntr._print_Relational(Le(x, y)) == \"(x <= y)\"\n    assert prntr._print_Relational(Gt(x, y)) == \"(x > y)\"\n"], "sample_1094": ["def test_doit():\n    # this is odd... so far, Basic has no doit methods so this is a\n    # default test that will have to be changed if/when Basic gets its own\n    # doit.\n    f = Basic()\n    assert f.doit() == f\n    assert f.doit(deep=False) == f\n    assert f.doit(deep=True) == f\n", "def test_Basic_compare():\n    assert b1.compare(b1) == 0\n    assert b1.compare(b2) < 0\n    assert b2.compare(b1) > 0\n    assert b2.compare(b21) < 0\n    assert b21.compare(b2) > 0\n    assert b21.compare(b3) < 0\n    assert b3.compare(b21) > 0\n\n    # Check that it can handle arbitrary Basic args\n    class MyBasic1(Basic):\n        pass\n\n    class MyBasic2(Basic):\n        pass\n\n    mb1a = MyBasic1()\n    mb1b = MyBasic1()\n    mb2a = MyBasic2()\n    mb2b = MyBasic2()\n    assert mb1a.compare(mb1b) == 0\n    assert mb1a.compare(mb2a) != 0\n    assert mb1a.compare(mb2a) < 0\n    assert mb2a.compare(mb1a) > 0\n    assert mb2a.compare(mb2b) == 0\n", "def test_compare():\n    assert Basic.compare(b1, b1) == 0\n    assert Basic.compare(b1, b2) == -1\n    assert Basic.compare(b2, b1) == 1\n    assert Basic.compare(b21, b21) == 0\n    assert Basic.compare(b3, b21) == 1\n    assert Basic.compare(b21, b3) == -1\n    assert Basic.compare(b1, b21) == -1\n    assert Basic.compare(b21, b1) == 1\n    assert Basic.compare(b1, b3) == -1\n    assert Basic.compare(b3, b1) == 1\n    assert Basic.compare(b2, b3) == -1\n    assert Basic.compare(b3, b2) == 1\n    assert Basic.compare(b21, b2) == 1\n    assert Basic.compare(b2, b21) == -1\n", "def test_as_Basic():\n    assert as_Basic(1) == S.One\n    raises(TypeError, lambda: as_Basic('1'))\n", "def test_compare():\n    assert Basic.compare(b1, b2) == -1\n    assert Basic.compare(b2, b2) == 0\n    assert Basic.compare(b21, b2) == 1\n    assert Basic.compare(b21, b3) == -1\n"], "sample_1095": ["def test_AppliedPermutation():\n    x = Symbol('x')\n    p = Permutation(0, 1, 2)\n    f = AppliedPermutation(p, x)\n    assert f.subs(x, 1) == 2\n    assert f.subs(x, 2) == 0\n    assert f.subs(x, 0) == 1\n    assert f.subs(x, 3) == 3\n", "def test_AppliedPermutation():\n    p = Permutation(0, 1, 2)\n    x = Symbol('x')\n\n    # test creation\n    f = AppliedPermutation(p, x)\n    assert f.perm == p\n    assert f.x == x\n\n    # test evaluation\n    assert f.subs(x, 0) == 0\n    assert f.subs(x, 1) == 2\n    assert f.subs(x, 2) == 1\n\n    # test printing\n    assert sstr(f) == 'AppliedPermutation((0 1 2), x)'\n    assert srepr(f) == \"AppliedPermutation(Permutation((0, 1, 2), size=3), Symbol('x'))\"\n    assert pretty(f, use_unicode=False) == 'AppliedPermutation((0 1 2), x)'\n    assert latex(f) == 'AppliedPermutation\\\\left((0 1 2), x\\\\right)'\n", "def test_AppliedPermutation():\n    x = Symbol('x', integer=True)\n    p = Permutation(0, 1, 2)\n    a = AppliedPermutation(p, x)\n    assert str(a) == 'AppliedPermutation((0 1 2), x)'\n    assert a.subs(x, 1) == p.apply(1)\n", "def test_AppliedPermutation_apply():\n    x = Symbol('x')\n    p = Permutation(0, 1, 2)\n    raises(NotImplementedError, lambda: p.apply(x + 1))\n    assert p.apply(x).subs(x, 1) == 2\n    assert p.apply(p.apply(x)).subs(x, 1) == 0\n    assert p.apply(x).subs(x, 1) != p.apply(x).subs(x, 2)\n", "def test_Permutation_from_inversion_vector():\n    # Test that from_inversion_vector correctly constructs a Permutation\n    # from an inversion vector\n    p1 = Permutation.from_inversion_vector([3, 2, 1, 0, 0])\n    assert p1 == Permutation([3, 2, 1, 0, 4, 5])\n    raises(ValueError, lambda: Permutation.from_inversion_vector([2, 1]))\n    assert Permutation.from_inversion_vector([]) == Permutation([])\n    assert Permutation.from_inversion_vector([0]) == Permutation([0, 1])\n"], "sample_1096": ["def test_indexed_shape():\n    i, j = symbols('i j', integer=True)\n    a = IndexedBase('a', shape=(3, 3))\n    assert a.shape == (3, 3)\n    assert a[i, j].shape == (3, 3)\n    b = IndexedBase('b')\n    assert b.shape is None\n    assert b[i, j].shape == Tuple(j.upper + 1, i.upper + 1)\n", "def test_Idx_subs():\n    i = Idx('i', 5)\n    j = Idx('j', 3)\n    assert i.subs(i, j) == j\n    assert j.subs(j, i) == i\n    assert i.subs(i.label, j.label) == Idx(j.label, i.upper + 1)\n    assert i.subs(i.upper, j.upper) == Idx(i.label, j.upper + 1)\n", "def test_indexedbase():\n    A = IndexedBase('A')\n    i, j = symbols('i j', integer=True)\n    assert A[i, j] == Indexed(A, i, j)\n    assert Indexed(A, i, j).base == A\n    assert Indexed(\"A\", i, j).base == IndexedBase(\"A\")\n    assert A[i, j] == Indexed(\"A\", i, j)\n", "def test_indexed_shape():\n    m, n, p, q = symbols('m n p q', integer=True)\n    a = IndexedBase('a', shape=(m, n))\n    b = IndexedBase('b', shape=(p, q))\n    i = Idx('i', m)\n    j = Idx('j', n)\n\n    assert a.shape == (m, n)\n    assert b.shape == (p, q)\n    assert a[i, j].shape == (m, n)\n    assert a[i, 0].shape == (m, n)\n    assert a[1, j].shape == (m, n)\n\n    raises(IndexException, lambda: IndexedBase('a', shape=(m, n, p)))\n    raises(IndexException, lambda: IndexedBase('a', shape=(m,)))\n\n    assert Indexed('a', i, j).shape == (m, n)\n    raises(IndexException, lambda: Indexed('a', i, j, 1))\n\n    x, y, z = symbols('x y z', integer=True)\n    a = IndexedBase('a')\n    assert Indexed(a, x, y, z).shape == (x, y, z)\n", "def test_IndexedBase_ranges():\n    n, m = symbols('n m', integer=True)\n    a = IndexedBase('a', shape=(n, m))\n    i, j = symbols('i j', cls=Idx)\n    assert a[i, j].ranges == [(0, n - 1), (0, m - 1)]\n    assert Indexed(a, i, 2).ranges == [(0, n - 1), None]\n    assert Indexed(a, 1, j).ranges == [None, (0, m - 1)]\n"], "sample_1097": ["def test_blockcut():\n    M = ImmutableMatrix(4, 4, range(16))\n    B = blockcut(M, (1, 3), (1, 3))\n    assert B.blocks[0, 0] == Matrix([[0]])\n    assert B.blocks[0, 1] == Matrix([[1, 2, 3]])\n    assert B.blocks[1, 0] == Matrix([[4], [8], [12]])\n    assert B.blocks[1, 1] == Matrix([[5, 6, 7], [9, 10, 11], [13, 14, 15]])\n", "def test_bc_inverse():\n    X = MatrixSymbol('X', n, n)\n    Y = MatrixSymbol('Y', m ,m)\n    Z = MatrixSymbol('Z', n, m)\n    B = BlockMatrix([[X, Z], [ZeroMatrix(m, n), Y]])\n    assert bc_inverse(Inverse(B)) == Inverse(B)\n\n    XInv = Inverse(X)\n    YInv = Inverse(Y)\n    BInv = BlockMatrix([[XInv, -XInv*Z*YInv], [ZeroMatrix(m, n), YInv]])\n    assert bc_inverse(Inverse(B)) == BInv\n", "def test_deblock():\n    A = MatrixSymbol('A', n, n)\n    B = MatrixSymbol('B', n, n)\n    C = MatrixSymbol('C', n, n)\n    D = MatrixSymbol('D', n, n)\n\n    X = BlockMatrix([[A, B], [C, D]])\n    Y = BlockMatrix([[X]])\n\n    assert deblock(Y) == X\n\n    Z = BlockMatrix([[X, X], [X, X]])\n    assert deblock(Z) == BlockMatrix([[A, B, A, B], [C, D, C, D], [A, B, A, B], [C, D, C, D]])\n", "def test_bc_inverse():\n    expr = Inverse(BlockDiagMatrix(A, B))\n    result = bc_inverse(expr)\n    assert result == BlockDiagMatrix(Inverse(A), Inverse(B))\n", "def test_blockcut():\n    M = ImmutableMatrix(4, 4, range(16))\n    assert M == blockcut(M, (4,), (4,)).blocks[0, 0]\n    B = blockcut(M, (1, 3), (1, 3))\n    assert B.blocks.shape == (2, 2)\n    assert B.blocks[0, 0].shape == (1, 1)\n    assert B.blocks[0, 1].shape == (1, 3)\n    assert B.blocks[1, 0].shape == (3, 1)\n    assert B.blocks[1, 1].shape == (3, 3)\n    assert B.blocks[0, 0] == M[0:1, 0:1]\n    assert B.blocks[0, 1] == M[0:1, 1:4]\n    assert B.blocks[1, 0] == M[1:4, 0:1]\n    assert B.blocks[1, 1] == M[1:4, 1:4]\n"], "sample_1098": ["def test_hyper_derivative():\n    from sympy import expand_func\n    a, b, c = symbols('a b c', positive=True)\n    assert hyper([a, b], [c], x).diff(x) == expand_func(a*b*hyper([a + 1, b + 1], [c + 1], x)/c)\n    assert hyper([a, b], [c], x).diff(x, 2) == expand_func(a*b*(a + 1)*(b + 1)*hyper([a + 2, b + 2], [c + 2], x)/(c*(c + 1)))\n    assert hyper([], [], x).diff(x) == hyper([1], [], x)\n    assert hyper([], [], x).diff(x, 2) == hyper([2], [], x)\n    assert hyper([], [], x).diff(x, 3) == hyper([3], [], x)\n    assert hyper([], [], x).diff(x, 5) == hyper([5], [], x)\n", "def test_hyper_derivative():\n    a, b, c, z = symbols('a b c z')\n    assert hyper([a], [], z).diff(z) == a * hyper([a+1], [1], z) / z\n    assert hyper([], [b], z).diff(z) == -b * hyper([1], [b+1], z) / z\n    assert hyper([a, b], [], z).diff(z) == a*b * hyper([a+1, b+1], [1, 2], z) / z\n    assert hyper([], [b, c], z).diff(z) == b*c * hyper([1, 2], [b+1, c+1], z) / z\n", "def test_hyper_diff():\n    p = hyper((1, 2), (3, 4), x)\n    d = p.diff(x)\n    assert d == hyper((2, 3), (4, 5), x)\n    d2 = d.diff(x)\n    assert d2 == hyper((3, 4), (5, 6), x)\n    d2_numeric = d.diff(x).evalf(subs={x: 0.5})\n    td(d, x, d2_numeric, x0=0.5)\n", "def test_hyper():\n    assert hyper((1, 2), (3, 4), x).is_commutative is True\n    assert hyper(Tuple(1, 2), Tuple(3, 4), x).is_commutative is True\n    assert hyper((1, 2, 3, 4, 5), (6, 7, 8, 9), x).doit() == hyper((1, 2, 3, 4, 5), (6, 7, 8, 9), x)\n    assert hyper((2, 2), (1, 1, 3), -1).doit() == hyper((2, 2), (1, 1, 3), -1)\n    assert hyper((1,), (), 0) == 1\n    assert hyper([], (), 1) == exp(1)\n    assert hyper([1], [1], 1) == 1/(1 - 1)\n    assert hyper([1, 2], [1, 2], 1) == 1/(1 - 1)**2\n    assert hyper([1], [], 0.5) == 1/(1 - 0.5)\n    assert hyper((1, 1, 1, 1), (), 2) == 1/(1 - 2)**4\n    assert hyper((1,), (1,), 3) == 1/(1 - 3)\n    assert hyper((1,), (2, 2), 1) == 1 - 1/(2) + 1/((2) * (3))\n    assert hyper([1, 1], [2, 2], 1).doit() == 1 + 1/(2) + 1/((2)*(3)) + 1/((2**2)*(3**2))\n    assert hyper((1, 2, 3), (4, 5, 6), x) == hyper((3, 2, 1), (6, 5, 4), x)\n    assert hyper((1, 2), (3,), x) == hyper((2, 1), (3,), x)\n    assert hyper((2, 1, 3), (4, 5), x) == hyper((1, 3, 2), (4,", "def test_meijerg_convergence():\n    raises(ValueError, lambda: meijerg([1, 2], [], [], [], -3/2))\n    raises(ValueError, lambda: meijerg([], [], [2, 2], [], 3/2))\n    raises(ValueError, lambda: meijerg([1, 1], [], [2, 2, 2, 2], [], 1))\n    raises(ValueError, lambda: meijerg([1, 1, 1, 1, 1], [], [], [], 1))\n"], "sample_1099": ["def test_PartialDerivative_multiple_variables():\n    # test with multiple variables\n    expr = PartialDerivative(A(i), B(j), C(k))\n    assert expr.variables == (B(j), C(k))\n    assert expr.get_indices() == [i, -j, -k]\n    assert expr.get_free_indices() == [i, j, k]\n\n    # test with multiple variables and contraction\n    expr = PartialDerivative(A(i), B(i), C(k))\n    assert expr.variables == (B(L_0), C(k))\n    assert expr.get_indices() == [L_0, -L_0, -k]\n    assert expr.get_free_indices() == [k]\n", "def test_PartialDerivative_expand_product():\n    # Test expansion of PartialDerivative of a product\n    expr = PartialDerivative(A(i)*B(j), A(k))\n    assert expr._expand_partial_derivative() == PartialDerivative(A(i), A(k))*B(j) + A(i)*PartialDerivative(B(j), A(k))\n\n    expr = PartialDerivative(A(i)*B(j), A(i))\n    assert expr._expand_partial_derivative() == PartialDerivative(A(i), A(i))*B(j) + A(i)*PartialDerivative(B(j), A(i))\n\n    # Test with multiple derivatives\n    expr = PartialDerivative(A(i)*B(j), A(k), A(m))\n    assert expr._expand_partial_derivative() == PartialDerivative(PartialDerivative(A(i), A(k)), A(m))*B(j) + \\\n        PartialDerivative(A(i), A(k))*PartialDerivative(B(j), A(m)) + \\\n        PartialDerivative(A(i), A(m))*PartialDerivative(B(j), A(k)) + \\\n        A(i)*PartialDerivative(PartialDerivative(B(j), A(k)), A(m))\n", "def test_PartialDerivative_doit():\n    expr = PartialDerivative(A(i), A(j))\n    assert expr.doit() == PartialDerivative(A(i), A(j))\n\n    expr = PartialDerivative(A(i), A(i))\n    assert expr.doit() == PartialDerivative(A(L_0), A(L_0))\n\n    expr = PartialDerivative(A(i) + B(i), A(j))\n    assert expr.doit() == PartialDerivative(A(i) + B(i), A(j))\n\n    expr = PartialDerivative(A(i)*B(j), A(k))\n    assert expr.doit() == PartialDerivative(A(i), A(k))*B(j) + A(i)*PartialDerivative(B(j), A(k))\n", "def test_partial_derivative_doit():\n    expr = PartialDerivative(A(i), A(j))\n    assert expr.doit() == expr\n\n    expr = PartialDerivative(A(i) + A(j), A(k))\n    assert expr.doit() == PartialDerivative(A(i), A(k)) + PartialDerivative(A(j), A(k))\n\n    expr = PartialDerivative(A(i)*A(j), A(k))\n    assert expr.doit() == A(i)*PartialDerivative(A(j), A(k)) + A(j)*PartialDerivative(A(i), A(k))\n", "def test_partial_derivative_properties():\n    expr = A(i)\n    var = B(j)\n    pd = PartialDerivative(expr, var)\n\n    assert pd.coeff == 1\n    assert pd.nocoeff == pd\n    assert pd.get_indices() == [i, -j]\n    assert pd.get_free_indices() == [i, j]\n"], "sample_1100": ["def test_Pow_as_content_primitive():\n    r = Symbol('r', real=True)\n    i = Symbol('i', real=False)\n    assert Pow(4, 2).as_content_primitive() == (16, 1)\n    assert Pow(4, r).as_content_primitive() == (1, Pow(4, r))\n    assert Pow(4, i).as_content_primitive() == (1, Pow(4, i))\n    assert Pow(2**i, r).as_content_primitive() == (1, Pow(2**i, r))\n    assert Pow(4**i, r).as_content_primitive() == (1, Pow(2**i, 2*r))\n    assert Pow(2*3, i).as_content_primitive() == (1, Pow(2*3, i))\n    assert Pow(2*3, r).as_content_primitive() == (1, Pow(6, r))\n    assert Pow(2*3, 2).as_content_primitive() == (36, 1)\n    assert Pow(2**(2*r), 3).as_content_primitive() == (1, Pow(2**(2*r), 3))\n    assert Pow(2**(2*r), 3*r).as_content_primitive() == (1, Pow(2**(2*r), 3*r))\n    assert Pow(2, 3).as_content_primitive() == (8, 1)\n    assert Pow(2, 3*r).as_content_primitive() == (1, Pow(2, 3*r))\n    assert Pow(2, r).as_content_primitive() == (1, Pow(2, r))\n    assert Pow(2, 2*r).as_content_primitive() == (1, Pow(2, 2*r))\n    assert Pow(2, 2).as_content_primitive() == (4, 1)\n    assert Pow(2, 1).as_content_primitive() == (2, 1)\n    assert Pow(2, 0).as_content_primitive() == (1, 1)\n    assert Pow(x, 0).as_content_primitive() == (1, 1)\n    assert Pow(x, 1).as_content_primitive() == (1, x)\n    assert Pow(x, 2).as_content_primitive() == (1, x**2)\n    assert Pow(x, 3).as_content_primitive() == (1, x**", "def test_Pow_is_integer():\n    x = Symbol('x', integer=True)\n    y = Symbol('y', integer=True, nonnegative=True)\n    assert Pow(x, 0, evaluate=False).is_integer\n    assert Pow(x, 1, evaluate=False).is_integer\n    assert Pow(x, -1, evaluate=False).is_integer is None  # depends on x != 0\n    assert Pow(x, 2, evaluate=False).is_integer is None  # depends on x >= 0\n    assert Pow(y, 2, evaluate=False).is_integer\n\n    assert Pow(x, Rational(1, 2)).is_integer is None\n    assert Pow(x, Rational(3, 2)).is_integer is None\n    assert Pow(x, -Rational(1, 2)).is_integer is None\n    assert Pow(x, -Rational(3, 2)).is_integer is None\n", "def test_Pow_is_integer():\n    x = Symbol('x')\n    assert Pow(2, 3, evaluate=False).is_integer\n    assert Pow(x, 3).is_integer is None\n    assert Pow(2, x).is_integer is None\n    assert Pow(x, x).is_integer is None\n    assert Pow(2, Rational(3, 2)).is_integer is False\n    assert Pow(x, Rational(3, 2)).is_integer is False\n    assert Pow(2, x/2).is_integer is None\n    assert Pow(x, x/2).is_integer is None\n    assert Pow(2, 1 + x).is_integer is None\n    assert Pow(x, 1 + x).is_integer is None\n    assert Pow(2, 1 + x/2).is_integer is None\n    assert Pow(x, 1 + x/2).is_integer is None\n    assert Pow(2, 3 + x/2).is_integer is None\n    assert Pow(x, 3 + x/2).is_integer is None\n    assert Pow(2, -3).is_integer is True\n    assert Pow(x, -3).is_integer is None\n    assert Pow(2, -x).is_integer is None\n    assert Pow(x, -x).is_integer is None\n    assert Pow(2, -Rational(3, 2)).is_integer is False\n    assert Pow(x, -Rational(3, 2)).is_integer is False\n    assert Pow(2, -x/2).is_integer is None\n    assert Pow(x, -x/2).is_integer is None\n    assert Pow(2, -1 - x).is_integer is None\n    assert Pow(x, -1 - x).is_integer is None\n    assert Pow(2, -1 - x/2).is_integer is None\n    assert Pow(x, -1 - x/2).is_integer is None\n    assert Pow(2, -3 - x/2).is_integer is None\n    assert Pow(x, -3 - x/2).is_integer is None\n", "def test_Pow_as_base_exp():\n    # no processing unless base is low-level\n    assert Pow(x, 2, evaluate=False).as_base_exp() == (x, 2)\n    # if base is low-level and 1/d then flip base and exp\n    assert Pow(S.Half, 2, evaluate=False).as_base_exp() == (2, -2)\n", "def test_Pow_is_integer():\n    x = Symbol('x', integer=True)\n    y = Symbol('y', integer=False)\n    z = Symbol('z', integer=None)\n    assert Pow(2, 3, evaluate=False).is_integer\n    assert Pow(2, y, evaluate=False).is_integer is False\n    assert Pow(x, 3, evaluate=False).is_integer\n    assert Pow(y, 3, evaluate=False).is_integer is False\n    assert Pow(x, y, evaluate=False).is_integer is None\n    assert Pow(x, z, evaluate=False).is_integer is None\n    assert Pow(2, z, evaluate=False).is_integer is None\n    assert Pow(x, -3, evaluate=False).is_integer is None\n    assert Pow(y, -3, evaluate=False).is_integer is False\n    assert Pow(x, -y, evaluate=False).is_integer is None\n    assert Pow(x, -z, evaluate=False).is_integer is None\n    assert Pow(2, -z, evaluate=False).is_integer is None\n    assert Pow(2, 3.5, evaluate=False).is_integer is False\n    assert Pow(2.5, 3, evaluate=False).is_integer is False\n"], "sample_1101": ["def test_SchurNumber_lower_bound():\n    k = _randint(5, 20)  # choose a random k > 4\n    assert SchurNumber(k).lower_bound() == (3**k - 1)/2\n", "def test_schur_number_lower_bound():\n    # Test lower bound for known Schur numbers\n    assert SchurNumber(1).lower_bound() == 0\n    assert SchurNumber(2).lower_bound() == 1\n    assert SchurNumber(3).lower_bound() == 4\n    assert SchurNumber(4).lower_bound() == 13\n\n    # Test lower bound for larger values of k\n    assert SchurNumber(6).lower_bound() == 364\n    assert SchurNumber(7).lower_bound() == 1093\n\n    # Test lower bound for non-integer values of k\n    k = symbols('k')\n    raises(ValueError, lambda: SchurNumber(k))\n    raises(ValueError, lambda: SchurNumber(Rational(3, 2)))\n\n    # Test lower bound for negative values of k\n    raises(ValueError, lambda: SchurNumber(-2))\n", "def test_schur_number():\n    # Test known Schur numbers\n    assert SchurNumber(1) == 1\n    assert SchurNumber(2) == 4\n    assert SchurNumber(3) == 13\n    assert SchurNumber(4) == 44\n    # Test unknown Schur number returns unevaluated\n    assert SchurNumber(5) == SchurNumber(5)\n    # Test lower bound method\n    assert SchurNumber(5).lower_bound() == (3**5 - 1)/2\n    # Test invalid input\n    raises(ValueError, lambda: SchurNumber(S.Infinity))\n    raises(ValueError, lambda: SchurNumber(-1))\n    raises(ValueError, lambda: SchurNumber(Rational(1, 2)))\n", "def test_schur_partition():\n    # Test partition for n <= 3\n    assert schur_partition(1) == [[1]]\n    assert schur_partition(2) == [[1, 2]]\n    assert schur_partition(3) == [[1, 2, 3]]\n\n    # Test partition for n > 3\n    assert schur_partition(5) == [[3, 2], [5], [1, 4]]\n    assert schur_partition(8) == [[3, 2], [6, 5, 8], [1, 4, 7]]\n\n    # Test for invalid input\n    raises(ValueError, lambda: schur_partition(0))\n    raises(ValueError, lambda: schur_partition(-5))\n    raises(ValueError, lambda: schur_partition(S.Infinity))\n    raises(ValueError, lambda: schur_partition(symbols('x')))\n", "def test_schur_number_eval():\n    # Test eval for known Schur numbers\n    assert SchurNumber(1).eval(1) == 1\n    assert SchurNumber(2).eval(2) == 4\n    assert SchurNumber(3).eval(3) == 13\n    assert SchurNumber(4).eval(4) == 44\n    \n    # Test eval for k > 4\n    k = _randint(5, 100)\n    assert SchurNumber(k).eval(k) == SchurNumber(k)\n    \n    # Test eval for non-integer k\n    raises(ValueError, lambda: SchurNumber(Rational(1, 2)).eval(Rational(1, 2)))\n    \n    # Test eval for negative k\n    raises(ValueError, lambda: SchurNumber(-1).eval(-1))\n    \n    # Test eval for k = 0\n    assert SchurNumber(0).eval(0) == 0\n    \n    # Test eval for k = S.Infinity\n    assert SchurNumber(S.Infinity).eval(S.Infinity) == S.Infinity\n"], "sample_1104": ["def test_printer_str_MatMul():\n    A, B = MatrixSymbol('A', 2, 2), MatrixSymbol('B', 2, 2)\n    assert sstr(A*B) == 'A*B'\n    assert sstr(-2*A*B) == '-2*A*B'\n    assert sstr(A*(B - A)) == 'A*(B - A)'\n", "def test_print_Predicate():\n    assert sstr(Q.is_true) == 'Q.is_true'\n", "def test_StrPrinter_print_TensorIndex():\n    from sympy.tensor.tensor import TensorIndex\n    i = TensorIndex('i', 3)\n    assert StrPrinter().doprint(i) == 'i'\n", "def test_sstr_repr_Tr():\n    A = Matrix([[1, 2], [3, 4]])\n    assert sstr(Tr(A)) == 'Tr(Matrix([[1, 2], [3, 4]]))'\n    assert sstrrepr(Tr(A)) == 'Tr(Matrix([[1, 2], [3, 4]]))'\n    assert sstr(Tr(x)) == 'Tr(x)'\n    assert sstrrepr(Tr(x)) == 'Tr(x)'\n", "def test_print_Quaternion():\n    q = Quaternion(1, 2, 3, 4)\n    assert sstr(q) == \"1 + 2*i + 3*j + 4*k\"\n"], "sample_1103": ["def test_Pow_as_content_primitive():\n    assert (4*x + 4)**2 == 16*(x + 1)**2\n    assert Pow(4*x + 4, 2).as_content_primitive() == (16, (x + 1)**2)\n    assert (4**((1 + y)/2)).as_content_primitive() == (2, 4**(y/2))\n    assert (3**((5 + y)/2)).as_content_primitive() == (9, 3**(y/2))\n    assert (3**((1 + y)/2)).as_content_primitive() == (1, 3**((1 + y)/2))\n    assert Pow(2*x + 2, y).as_content_primitive() == (1, (2*(x + 1))**y)\n    assert expand_power_base(Pow(2*x + 2, y).as_content_primitive()[1]) == 2**y*(x + 1)**y\n    assert (3**(2 + 2*x)).as_content_primitive() == (9, 3**(2*x))\n    assert Pow(4*x + 4, 2) == Pow(*Pow(4*x + 4, 2).as_content_primitive(), evaluate=False)\n", "def test_pow_is_real():\n    assert Pow(2, 3, evaluate=False).is_real\n    assert Pow(2, -3, evaluate=False).is_real\n    assert Pow(-2, 2).is_real\n    assert Pow(-2, -2).is_real\n    assert Pow(-2, 3).is_real is False\n    assert Pow(-2, -3).is_real is False\n    assert Pow(x, 2).is_real\n    assert Pow(x, -2).is_real\n    assert Pow(x, 3).is_real is None\n    assert Pow(x, -3).is_real is None\n", "def test_Pow_as_base_exp():\n    x = Symbol('x', extended_real=True)\n    y = Symbol('y', real=True)\n    z = Symbol('z')\n\n    assert Pow(x, 2).as_base_exp() == (x, 2)\n    assert Pow(x, S(1)/2).as_base_exp() == (x, S(1)/2)\n    assert Pow(x, y).as_base_exp() == (x, y)\n    assert Pow(x, -y).as_base_exp() == (1/x, y)\n    assert Pow(1/x, y).as_base_exp() == (1/x, y)\n    assert Pow(x, z).as_base_exp() == (x, z)\n    assert Pow(x, 0).as_base_exp() == (x, 0)\n    assert Pow(0, x).as_base_exp() == (0, x)\n    assert Pow(1, x).as_base_exp() == (1, x)\n    assert Pow(oo, x).as_base_exp() == (oo, x)\n    assert Pow(zoo, x).as_base_exp() == (zoo, x)\n    assert Pow(-1, x).as_base_exp() == (-1, x)\n    assert Pow(I, x).as_base_exp() == (I, x)\n    assert Pow(-I, x).as_base_exp() == (-I, x)\n    assert Pow(S.NegativeOne, x).as_base_exp() == (-1, x)\n    assert Pow(S.ImaginaryUnit, x).as_base_exp() == (I, x)\n    assert Pow(S.NegativeInfinity, x).as_base_exp() == (-oo, x)\n    assert Pow(S.Infinity, x).as_base_exp() == (oo, x)\n    assert Pow(S.ComplexInfinity, x).as_base_exp() == (zoo, x)\n    assert Pow(S.NaN, x).as_base_exp() == (nan, x)\n    assert Pow(nan, x).as_base_exp() == (nan, x)\n", "def test_Pow_as_content_primitive():\n    assert Pow(2*x + 2, 2).as_content_primitive() == (4, (x + 1)**2)\n    assert Pow(4, (1 + y)/2).as_content_primitive() == (2, 4**(y/2))\n    assert Pow(3, (1 + y)/2).as_content_primitive() == (1, 3**((y + 1)/2))\n    assert Pow(3, (5 + y)/2).as_content_primitive() == (9, 3**((y + 1)/2))\n    assert Pow(3**(2 + 2*x), 1).as_content_primitive() == (9, 3**(2*x))\n", "def test_Pow_is_integer():\n    x = Symbol('x', integer=True)\n    y = Symbol('y', integer=True, nonnegative=True)\n    assert Pow(x, 1, evaluate=False).is_integer\n    assert Pow(x, -1, evaluate=False).is_integer == None\n    assert Pow(x, 0, evaluate=False).is_integer\n    assert Pow(x, y, evaluate=False).is_integer\n    assert Pow(x, x, evaluate=False).is_integer == None\n    assert Pow(x, 2, evaluate=False).is_integer\n    assert Pow(x, -2, evaluate=False).is_integer == None\n    assert Pow(2, x, evaluate=False).is_integer\n    assert Pow(2, y, evaluate=False).is_integer\n    assert Pow(x, 2.0, evaluate=False).is_integer == False\n    assert Pow(Rational(3,4),2).is_integer == False\n    assert Pow(Rational(4,3),-2).is_integer == False\n"], "sample_1105": ["def test_canonicalize():\n    assert canonicalize(MatMul(A, Identity(n), B)) == MatMul(A, B)\n    assert canonicalize(MatMul(A, ZeroMatrix(n, m), B)) == ZeroMatrix(n, l)\n    assert canonicalize(MatMul(A, MatrixSymbol('X', n, n)**-1, A)) == Identity(n)\n    assert canonicalize(MatMul(A, MatrixSymbol('X', n, n), MatrixSymbol('X', n, n)**-1)) == A\n    assert canonicalize(MatMul(A, A.transpose(), A**-1)) == A.transpose()\n", "def test_doit():\n    X = MatrixSymbol('X', 2, 2)\n    Y = MatrixSymbol('Y', 2, 2)\n    assert MatMul(X, Y).doit() == X * Y\n    assert MatMul(X, X.inverse()).doit() == Identity(2)\n    assert MatMul(X, X.T).doit() == X * X.T\n    assert MatMul(X, Y, X).doit() == X * Y * X\n", "def test_matmul_doit():\n    X = MatrixSymbol('X', 2, 2)\n    Y = MatrixSymbol('Y', 2, 2)\n    assert MatMul(X, Y).doit() == MatMul(X, Y)\n    assert MatMul(X, X.inverse()).doit() == Identity(2)\n    assert MatMul(X, Y, Y.inverse()).doit() == MatMul(X, Identity(2))\n    assert MatMul(X, Identity(2), Y).doit() == MatMul(X, Y)\n    assert MatMul(Identity(2), X, Y).doit() == MatMul(X, Y)\n    assert MatMul(X, Y, Identity(2)).doit() == MatMul(X, Y)\n    assert MatMul(X, Y, ZeroMatrix(2, 2)).doit() == ZeroMatrix(2, 2)\n    assert MatMul(ZeroMatrix(2, 2), X, Y).doit() == ZeroMatrix(2, 2)\n", "def test_doit_matrix_lines_derivative():\n    B = MatrixSymbol('B', 3, 1)\n    J = MatrixSymbol('J', 1, 1)\n    K = MatrixSymbol('K', 1, 2)\n    X = MatrixSymbol('X', 3, 1)\n    M = MatMul(B, J, K, X)\n    x = Symbol('x')\n    lines = M._eval_derivative_matrix_lines(x)\n    assert len(lines) == 4\n", "def test_refine_MatMul():\n    assert refine(MatMul(A, A.T), Q.orthogonal(A)) == Identity(n)\n    assert refine(MatMul(A, A.conjugate()), Q.unitary(A)) == Identity(n)\n    assert refine(MatMul(A, B), Q.orthogonal(A)) == MatMul(A, B)\n"], "sample_1102": ["def test_poly_domain_conversion():\n    p = Poly(x**2 + 1, x, domain=ZZ)\n    assert p.as_expr() == x**2 + 1\n    \n    p_new_domain = p.set_domain(QQ)\n    assert p_new_domain.domain == QQ\n    \n    p_converted = p_new_domain.as_expr()\n    assert p_converted == x**2 + 1\n    assert _strict_eq(p_converted, p.as_expr())\n", "def test_Poly__convert_to_irred():\n    f = Poly(x**2 + 1, x, domain=ZZ)\n    g = Poly(x**2 - 1, x, domain=ZZ)\n\n    assert f._convert_to_irred(ZZ) == (ZZ, f)\n    assert g._convert_to_irred(ZZ) == (ZZ, g)\n\n    assert f._convert_to_irred(QQ) == (QQ, f)\n    assert g._convert_to_irred(QQ) == (QQ, g)\n", "def test_Poly_cancel():\n    f = Poly(x**2 + 1, x)\n    g = Poly(x**2 - 1, x)\n\n    p, q = f, g\n    h, p, q = p.cancel(q)\n    assert h == 1\n    assert p == f\n    assert q == g\n\n    p, q = f, g\n    h, p, q = q.cancel(p)\n    assert h == 1\n    assert p == f\n    assert q == g\n\n    f = Poly(x**2 + 2*x + 1, x)\n    g = Poly(x + 1, x)\n\n    p, q = f, g\n    h, p, q = p.cancel(q)\n    assert h == 1\n    assert p == f\n    assert q == g\n\n    p, q = f, g\n    h, p, q = q.cancel(p)\n    assert h == 1\n    assert p == f\n    assert q == g\n\n    f = Poly(x**2*y + x*y**2, x, y)\n    g = Poly(x*y + x + y, x, y)\n\n    p, q = f, g\n    h, p, q = p.cancel(q)\n    assert h == 1\n    assert p == f\n    assert q == g\n\n    p, q = f, g\n    h, p, q = q.cancel(p)\n    assert h == 1\n    assert p == f\n    assert q == g\n", "def test_Poly_gens_symbols():\n    assert Poly(x**2 + 1).gens == (x,)\n    assert Poly(x**2 + 1).free_symbols == {x}\n\n    assert PurePoly(x**2 + 1).gens == (x,)\n    assert PurePoly(x**2 + 1).free_symbols == set()\n", "def test_Poly_LC():\n    assert Poly(x + 2).LC() == 1\n    assert Poly(3*x + 2).LC() == 3\n    assert Poly(x*y + 2*y + x + 1).LC() == 1\n    assert Poly(2*x*y + 3*y + 4*x + 5).LC() == 2\n"], "sample_1106": ["def test_matadd_doit():\n    A = MatrixSymbol('A', 2, 2)\n    B = eye(2)\n    C = Matrix([[1, 2], [3, 4]])\n    X = MatAdd(A, B, C)\n    assert X.doit() == MatAdd(A, Matrix([[2, 2], [3, 5]]))\n    assert X.doit(deep=False) == MatAdd(A, B, C)\n", "def test_matadd_doit():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    C = MatrixSymbol('C', 2, 2)\n    assert MatAdd(A, B, C).doit() == MatAdd(A, B, C)\n    assert MatAdd(A, B, ZeroMatrix(2, 2)).doit() == MatAdd(A, B)\n    assert MatAdd(A, ZeroMatrix(2, 2), ZeroMatrix(2, 2)).doit() == A\n    assert MatAdd(A, A, A).doit() == 3*A\n", "def test_merge_explicit():\n    A = MatrixSymbol('A', 2, 2)\n    B = eye(2)\n    C = Matrix([[1, 2], [3, 4]])\n    X = MatAdd(A, B, C)\n    assert merge_explicit(X) == MatAdd(A, Matrix([[2, 2], [3, 5]]))\n    X = MatAdd(A, C)\n    assert merge_explicit(X) == MatAdd(A, C)\n", "def test_MatAdd_doit():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    C = Matrix([[1, 2], [3, 4]])\n    X = MatAdd(A, B, C)\n\n    assert X.doit() == MatAdd(A, B, C)\n    assert X.doit(deep=True) == MatAdd(A, B, ImmutableMatrix([[1, 2], [3, 4]]))\n    assert X.doit(deep=False) == MatAdd(A, B, C)\n", "def test_MatAdd_doit():\n    A = MatrixSymbol('A', 2, 2)\n    B = eye(2)\n    C = Matrix([[1, 2], [3, 4]])\n    X = MatAdd(A, B, C)\n    assert X.doit() == MatAdd(A, Matrix([[2, 2], [3, 5]]))\n    assert X.doit(deep=False) == MatAdd(A, B, C)\n"], "sample_1108": ["def test_roundrobin():\n    from sympy.utilities.iterables import roundrobin\n    assert list(roundrobin('ABC', 'D', 'EF')) == ['A', 'D', 'E', 'B', 'F', 'C']\n    assert list(roundrobin([1, 2], [3, 4, 5])) == [1, 3, 2, 4, 5]\n    assert list(roundrobin([1, 2, 3], [4, 5], [6, 7, 8, 9])) == [1, 4, 6, 2, 5, 7, 3, 8, 9]\n", "def test_roundrobin():\n    from sympy.utilities.iterables import roundrobin\n    assert list(roundrobin('ABC', 'D', 'EF')) == ['A', 'D', 'E', 'B', 'F', 'C']\n    assert list(roundrobin([1, 2, 3], [4, 5], [6, 7, 8, 9])) == [1, 4, 6, 2, 5, 7, 3, 8, 9]\n", "def test_size_intersection():\n    assert len(list(multiset_combinations([1, 2, 3, 4], 2))) == 10\n    assert len(list(multiset_combinations([1]*3, 2))) == 4\n", "def test_interactive_traversal():\n    # This test doesn't need to be interactive\n    v = interactive_traversal(w + (x + y)*z)\n    assert str(v) == 'w + z*(x + y)'\n", "def test_generate_oriented_forest():\n    result = list(generate_oriented_forest(3))\n    assert len(result) == 9\n    assert result == [[0, 1, 2], [0, 1, 1], [0, 1, 0],\n                     [0, 0, 2], [0, 0, 1], [0, 0, 0],\n                     [1, 0, 0], [0, 2, 0], [0, 1, 0]]\n"], "sample_1107": ["def test_ordered_partitions():\n    # make sure the results are valid and that they\n    # are returned in order\n    for i in range(1, 7):\n        p = [tuple(j) for j in ordered_partitions(i)]\n        assert len(p) == RGS_enum(i)\n        for j in range(len(p) - 1):\n            assert default_sort_key(p[j]) < default_sort_key(p[j + 1])\n\n    # make sure the results are the same when the integer\n    # is passed as a singleton list and that the results\n    # are returned in order\n    for i in range(1, 7):\n        p = [tuple(j) for j in ordered_partitions([i])]\n        assert len(p) == RGS_enum(i)\n        for j in range(len(p) - 1):\n            assert default_sort_key(p[j]) < default_sort_key(p[j + 1])\n\n    # pass a list\n    assert list(ordered_partitions([1,1,1])) == [[1, 1, 1]]\n\n    # test second argument\n    assert list(ordered_partitions([2,2,2], 1)) == [[2,2,2]]\n    assert list(ordered_partitions([2,2,2], 2)) == [[2,2], [2]]\n    assert list(ordered_partitions([2,2,2], 3)) == [[2,2], [2]]\n    assert list(ordered_partitions([2,2,2], 4)) == []\n    assert list(ordered_partitions([2,2,2], 5)) == []\n    raises(ValueError, lambda: list(ordered_partitions([2,2,2], 0)))\n    assert list(ordered_partitions([2,3,5], 3)) == [[2,3], [5]]\n\n    # test sort=False\n    assert list(ordered_partitions(6, 2, sort=False)) == [\n        [4, 2], [3, 3], [2, 4]]\n", "def test_rotations():\n    assert list(rotations([1, 2, 3])) == [[1, 2, 3], [2, 3, 1], [3, 1, 2]]\n    assert list(rotations([1, 2, 3], -1)) == [[1, 2, 3], [3, 1, 2], [2, 3, 1]]\n    assert list(rotations([1])) == [[1]]\n    assert list(rotations([])) == []\n", "def test_roundrobin():\n    from sympy.utilities.iterables import roundrobin\n    assert list(roundrobin('ABC', 'D', 'EF')) == list('A D E B F C')\n    assert list(roundrobin([0, 1], [2, 3, 4])) == [0, 2, 1, 3, 4]\n    assert list(roundrobin([], [0, 1, 2])) == [0, 1, 2]\n    assert list(roundrobin([0, 1, 2], [])) == [0, 1, 2]\n    assert list(roundrobin([], [], [0, 1, 2])) == [0, 1, 2]\n    assert list(roundrobin([], [], [])) == []\n    assert list(roundrobin([0], [1], [2])) == [0, 1, 2]\n", "def test_permute_signs():\n    assert list(permute_signs((0, 1, 2))) == [\n        (0, 1, 2), (0, -1, 2), (0, 1, -2), (0, -1, -2)\n    ]\n    assert len(list(permute_signs((1, 2, 3)))) == 2**3\n    assert len(list(permute_signs((1, 2, 3, 0)))) == 2**3\n", "def test_roundrobin():\n    from sympy.utilities.iterables import roundrobin\n    assert list(roundrobin('ABC', 'D', 'EF')) == ['A', 'D', 'E', 'B', 'F', 'C']\n    assert list(roundrobin([1, 2], [3, 4, 5, 6])) == [1, 3, 2, 4, 5, 6]\n"], "sample_1109": ["def test_floor_inequalities():\n    assert floor(x) <= x\n    assert floor(x) <= floor(x)\n    assert floor(x) <= ceiling(x)\n    assert floor(x) < x + 1\n    assert floor(x) < floor(x) + 1\n    assert floor(x) < ceiling(x) + 1\n    assert x - 1 < floor(x)\n    assert floor(x) - 1 < floor(x)\n    assert ceiling(x) - 1 < floor(x)\n", "def test_floor_inequalities():\n    x = Symbol('x', real=True)\n    assert floor(x) >= x - 1\n    assert floor(x) <= x\n    assert floor(-x) <= -floor(x)\n    assert floor(x + floor(x)) == floor(2*x)\n", "def test_floor_ceiling__eval_Eq():\n    assert Eq(floor(x), floor(x)) == S.true\n    assert Eq(ceiling(x), ceiling(x)) == S.true\n    assert Eq(floor(x), ceiling(x)) != S.true\n    assert Eq(frac(x), frac(x)) == S.true\n    assert Eq(frac(x), floor(x)) != S.true\n    assert Eq(frac(x), ceiling(x)) != S.true\n    assert Eq(frac(x), frac(x).rewrite(floor)) == S.true\n    assert Eq(frac(x), frac(x).rewrite(ceiling)) == S.true\n    assert Eq(ceiling(x), ceiling(-(-x))) == S.true\n    assert Eq(floor(x), -ceiling(-x)) == S.true\n    assert Eq(frac(x), x - floor(x)) == S.true\n    assert Eq(frac(x), x + ceiling(-x)) == S.true\n", "def test_floor_inequality():\n    assert floor(x) <= x\n    assert floor(2*E) <= 2*E\n    assert floor(-Float(0.567)) <= -Float(0.567)\n    assert floor(y) <= y\n    assert floor(5*pi) <= 5*pi\n    assert floor(n) == n\n    assert floor(I*y) <= I*y\n    assert floor(-I*y) >= -I*y\n    assert floor(I) <= I\n    assert floor(-I) >= -I\n    assert floor(0) == 0\n    assert floor(x).is_real\n    assert floor(x).is_finite\n    assert floor(x).is_integer\n", "def test_floor_ceiling_frac():\n    # Test floor and ceiling with real numbers\n    assert floor(3.7) == 3\n    assert floor(-3.7) == -4\n    assert ceiling(3.7) == 4\n    assert ceiling(-3.7) == -3\n    assert frac(3.7) == 0.7\n    assert frac(-3.7) == 0.3\n\n    # Test floor and ceiling with complex numbers\n    assert floor(3.7 + 2.3*I) == 3 + 2*I\n    assert floor(-3.7 - 2.3*I) == -4 - 3*I\n    assert ceiling(3.7 + 2.3*I) == 4 + 3*I\n    assert ceiling(-3.7 - 2.3*I) == -3 - 2*I\n    assert frac(3.7 + 2.3*I) == 0.7 + 0.3*I\n    assert frac(-3.7 - 2.3*I) == 0.3 + 0.7*I\n\n    # Test floor and ceiling with symbolic expressions\n    assert floor(x + 3.7) == floor(x) + 3\n    assert floor(x - 3.7) == floor(x) - 4\n    assert ceiling(x + 3.7) == ceiling(x) + 4\n    assert ceiling(x - 3.7) == ceiling(x) - 3\n    assert frac(x + 3.7) == frac(x) + 0.7\n    assert frac(x - 3.7) == frac(x) + 0.3\n\n    # Test comparisons\n    assert floor(x) <= x\n    assert ceiling(x) >= x\n    assert frac(x) >= 0\n    assert frac(x) < 1\n"], "sample_1110": ["def test_pythonCodePrinter_print_custom_functions():\n    p = PythonCodePrinter({'user_functions': {'sin': 'my_sin'}}, settings={'fully_qualified_modules': False})\n    assert p._print(sign(x)) == 'math.copysign(1, x)'\n    assert p._print(sqrt(x)) == 'math.sqrt(x)'\n    assert p._print(acos(x)) == 'math.acos(x)'\n    assert p._print(KroneckerDelta(x, y)) == '(1 if x == y else 0)'\n    assert p._print(sin(x)) == 'my_sin(x)'\n", "def test_pycode_MatMul():\n    A = MatrixSymbol('A', 2, 3)\n    B = MatrixSymbol('B', 3, 4)\n    C = MatrixSymbol('C', 4, 5)\n    expr = A * B * C\n    assert pycode(expr) == '(A).dot((B).dot(C))'\n", "def test_SymPyPrinter_print_Piecewise():\n    expr = Piecewise((x, x < 0), (y, True))\n    assert SymPyPrinter().doprint(expr) == 'sympy.Piecewise((x, x < 0), (y, True))'\n", "def test_print\u7a0eustom_CodegenArray():\n    from sympy.codegen.array_utils import CodegenArrayTensorProduct, CodegenArrayContraction, CodegenArrayDiagonal\n    from sympy.codegen.array_utils import CodegenArrayPermuteDims, CodegenArrayElementwiseAdd\n\n    arr = CodegenArrayTensorProduct(MatrixSymbol(\"A\", 3, 3), MatrixSymbol(\"B\", 3, 3))\n    assert NumPyPrinter().doprint(arr) == \"numpy.einsum('ij,jk->ik', A, B)\"\n\n    arr = CodegenArrayContraction(arr, (0, 1))\n    assert NumPyPrinter().doprint(arr) == \"numpy.einsum('ii->', A, B)\"\n\n    arr = CodegenArrayDiagonal(MatrixSymbol(\"A\", 3, 3), (0, 1))\n    assert NumPyPrinter().doprint(arr) == \"numpy.diagonal(A, 0, axis1=0, axis2=1)\"\n\n    arr = CodegenArrayPermuteDims(MatrixSymbol(\"A\", 3, 3), (1, 0))\n    assert NumPyPrinter().doprint(arr) == \"numpy.transpose(A, (1, 0))\"\n\n    arr = CodegenArrayElementwiseAdd(MatrixSymbol(\"A\", 3, 3), MatrixSymbol(\"B\", 3, 3))\n    assert NumPyPrinter().doprint(arr) == \"numpy.add(A, B)\"\n", "def test_PythonCodePrinter_print_SparseMatrix():\n    M = SparseMatrix(2, 2, {(0, 0): x, (1, 1): y})\n    assert PythonCodePrinter().doprint(M) == str(M.todense())\n"], "sample_1111": ["def test_textplot_str_linear():\n    x = Symbol('x')\n    expr = 2*x + 1\n    result = list(textplot_str(expr, 0, 10))\n    assert len(result) == 22\n    assert all(len(line) > 0 for line in result)\n", "def test_textplot_str_log():\n    x = Symbol('x')\n    expr = log(x)\n    a = 0.1\n    b = 10\n    result = list(textplot_str(expr, a, b))\n    assert len(result) == 21\n    assert result[0].startswith(\" 2 |\")\n    assert result[-1].startswith(\"     0.1\")\n", "def test_textplot_str_rescale():\n    x = Symbol('x')\n    expr = sin(x)\n    a = 0\n    b = 10\n    W = 20\n    H = 10\n    output = list(textplot_str(expr, a, b, W, H))\n    assert len(output) == H + 1\n", "def test_textplot_str_log():\n    x = Symbol('x')\n    expr = log(x)\n    result = list(textplot_str(expr, 0.1, 10))\n    assert len(result) == 22\n    assert result[0].startswith(\" 2.3\")\n    assert result[-1].startswith(\"    \")\n    assert \"nan\" not in \"\\n\".join(result)\n\n", "def test_textplot_str_log():\n    x = Symbol('x')\n    expr = log(x)\n    result = list(textplot_str(expr, 1, 10))\n    assert len(result) == 22\n    assert 'log' not in str(result)  # check that the function has been evaluated\n    assert '  2 |' in result[15]  # check that the y values are in the plot\n"], "sample_1112": ["def test_digits():\n    # Test base 10 digits with positive and negative numbers\n    assert digits(123) == [10, 1, 2, 3]\n    assert digits(-123) == [-10, 1, 2, 3]\n\n    # Test non-base 10 digits with positive and negative numbers\n    assert digits(123, b=2) == [2, 1, 1, 1, 1, 0, 1, 1]\n    assert digits(-123, b=2) == [-2, 1, 1, 1, 1, 0, 1, 1]\n\n    # Test digits with a specified number of digits\n    assert digits(123, digits=5) == [10, 0, 0, 1, 2, 3]\n    assert digits(123, b=2, digits=10) == [2, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n\n    # Test that an error is raised when the base is less than 2\n    raises(ValueError, lambda: digits(123, b=1))\n\n    # Test that an error is raised when the number of digits is too small\n    raises(ValueError, lambda: digits(123, digits=2))\n", "def test_digits_with_specified_digits():\n    assert digits(35, digits=4) == [10, 0, 0, 3, 5]\n    assert digits(123, digits=5) == [10, 0, 0, 1, 2, 3]\n    raises(ValueError, lambda: digits(123, digits=2))\n", "def test_count_digits():\n    assert count_digits(1111339) == {1: 4, 3: 2, 9: 1}\n    assert count_digits(0xFA) == {0: 1, 2: 1, 5: 1}\n    assert count_digits(0xFA, 16) == {10: 1, 15: 1}\n    from sympy import factorial\n    c77 = count_digits(factorial(77))\n    assert [i for i in range(10) if c77[i] == 7] == [1, 3, 7, 9]\n", "def test_digits_base2():\n    # Test digits function with base 2\n    assert digits(27, b=2) == [2, 1, 1, 0, 1, 1]\n    assert digits(10, b=2) == [2, 1, 0, 1, 0]\n    assert digits(1, b=2) == [2, 1]\n    assert digits(0, b=2) == [2, 0]\n", "def test_is_palindromic():\n    assert is_palindromic(121) is True\n    assert is_palindromic(123) is False\n    assert is_palindromic(-121) is True\n    assert is_palindromic(-123) is False\n    assert is_palindromic(0o121, 8) is True\n    assert is_palindromic(0o123, 8) is False\n    raises(ValueError, lambda: is_palindromic(123, 1))\n"], "sample_1113": ["def test_block_inverse():\n    block1 = BlockMatrix([[X1, X2], [X3, X4]])\n    block2 = BlockMatrix([[X1, ZeroMatrix(m, m)], [ZeroMatrix(m, m), X4]])\n    assert Inverse(block1).doit() == Inverse(block1)\n    assert Inverse(block2).doit() == BlockMatrix([[Inverse(X1), ZeroMatrix(m, m)], [ZeroMatrix(m, m), Inverse(X4)]])\n", "def test_blockdiag_structurally_equal():\n    M = BlockDiagMatrix(X1, X2)\n    N = BlockDiagMatrix(X3, X4)\n    assert M.structurally_equal(N) == True\n    assert M.structurally_equal(X1) == False\n", "def test_BlockMatrix_trace():\n    # Test trace of BlockMatrix\n    B = BlockMatrix([[X1, X2], [X3, X4]])\n    assert B.trace() == X1.trace() + X4.trace()\n\n    # Test trace of BlockMatrix with non-square blocks\n    B = BlockMatrix([[X, Z], [ZeroMatrix(m, l), Y]])\n    raises(NotImplementedError, lambda: B.trace())\n\n    # Test trace of BlockDiagMatrix\n    B = BlockDiagMatrix(X1, X2)\n    assert B.trace() == X1.trace() + X2.trace()\n", "def test_block_collapse():\n    # Test case for collapsing a BlockMatrix with a single block\n    B = BlockMatrix([[X]])\n    assert block_collapse(B) == X\n\n    # Test case for collapsing a BlockMatrix with multiple blocks\n    B = BlockMatrix([[X, Y], [ZeroMatrix(m, l), Z]])\n    assert block_collapse(B) == B\n\n    # Test case for collapsing a BlockMatrix with nested blocks\n    B = BlockMatrix([[BlockMatrix([[X, Y]]), Z]])\n    assert block_collapse(B) == BlockMatrix([[X, Y], [ZeroMatrix(m, l), Z]])\n\n    # Test case for collapsing a BlockMatrix with a MatrixExpr\n    B = BlockMatrix([[X + Y, Z]])\n    assert block_collapse(B) == BlockMatrix([[X + Y, Z]])\n", "def test_block_collapse():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    expr = BlockMatrix([[A, B], [ZeroMatrix(2, 2), Identity(2)]])\n    assert block_collapse(expr) == expr\n    expr = BlockMatrix([[A, B], [ZeroMatrix(2, 2), Identity(2)]]) * BlockMatrix([[Identity(2), ZeroMatrix(2, 2)], [ZeroMatrix(2, 2), A]])\n    assert block_collapse(expr) == BlockMatrix([[A, B*A + B], [ZeroMatrix(2, 2), A]])\n"], "sample_1114": ["def test_ComplexRegion_contains():\n    a = Interval(2, 3)\n    b = Interval(4, 5)\n    c1 = ComplexRegion(a*b)\n    assert 2.5 + 4.5*I in c1\n    assert 2.5 + 6.5*I not in c1\n\n    r = Interval(0, 1)\n    theta = Interval(0, 2*pi)\n    c2 = ComplexRegion(r*theta, polar=True)\n    assert 0.5 + 0.5*I in c2\n    assert 1 + 2*I not in c2\n", "def test_ComplexRegion_contains():\n    a, b = Interval(1, 3), Interval(2, 4)\n    theta = Interval(0, pi)\n    region1 = ComplexRegion(a*b)\n    region2 = ComplexRegion(a*theta, polar=True)\n\n    assert 2 + 3*I in region1\n    assert 1 + I in region2\n    assert 1 + 2*I not in region2\n    assert 0 + 0*I not in region2\n\n    # Issue 16420\n    c = Interval(0, 1)\n    d = Interval(0, 2*pi)\n    region3 = ComplexRegion(c*d, polar=True)\n    assert 1 in region3\n    assert I in region3\n    assert -1 in region3\n    assert -I in region3\n", "def test_image_set_empty():\n    assert ImageSet(Lambda(x, x**2), S.EmptySet) == S.EmptySet\n    assert ImageSet(Lambda(x, x**2), Interval(1, 2)).intersect(\n        ImageSet(Lambda(x, x**2), Interval(3, 4))) == S.EmptySet\n", "def test_range_with_negative_step():\n    r = Range(10, 0, -2)\n    assert list(r) == [10, 8, 6, 4, 2]\n    assert 10 in r\n    assert 8 in r\n    assert 6 in r\n    assert 4 in r\n    assert 2 in r\n    assert 1 not in r\n    assert 3 not in r\n    assert 5 not in r\n    assert 7 not in r\n    assert 9 not in r\n    assert r.start == 10\n    assert r.stop == 0\n    assert r.step == -2\n    assert len(r) == 5\n", "def test_ComplexRegion_contains():\n    a, b = Interval(1, 3), Interval(2, 4)\n    c = ComplexRegion(a*b)\n    assert 2 + 3*I in c\n    assert 5 + 5*I not in c\n"], "sample_1116": ["def test_Inverse_doit():\n    assert C.inverse().doit() == C**-1\n    assert (C**-1).doit() == C.inverse()\n    assert Inverse(C).doit() == C.inverse()\n    assert Inverse(C).doit(inv_expand=False) == Inverse(C)\n", "def test_inverse_properties():\n    assert Inverse(C).shape == (n, n)\n    raises(NonSquareMatrixError, lambda: Inverse(A))\n    assert Inverse(C).arg == C\n    assert Inverse(C).exp == S.NegativeOne\n    assert Inverse(C).is_Inverse\n    assert Inverse(C).doit() == C.inverse()\n    assert Inverse(C).doit(inv_expand=False) == Inverse(C)\n", "def test_refine_Inverse():\n    assert refine(Inverse(C), Q.orthogonal(C)) == C.T\n    assert refine(Inverse(C), Q.unitary(C)) == C.conjugate()\n    raises(ValueError, lambda: refine(Inverse(C), Q.singular(C)))\n    assert refine(Inverse(C), Q.invertible(C)) == C**-1\n", "def test_Inverse_properties():\n    assert Inverse(C).is_Inverse\n    assert Inverse(C).exp == S.NegativeOne\n    assert Inverse(C).arg == C\n    assert Inverse(C).shape == C.shape\n", "def test_Inverse_refine():\n    assert refine(C.I, Q.orthogonal(C)) == C.T\n    assert refine(C.I, Q.unitary(C)) == C.conjugate()\n    raises(ValueError, lambda: refine(C.I, Q.singular(C)))\n"], "sample_1115": ["def test_TensorElement():\n    Lorentz = TensorIndexType(\"L\")\n    i, j, k = symbols(\"i j k\")\n    A = TensorHead(\"A\", [Lorentz, Lorentz], TensorSymmetry.fully_symmetric(2))\n    te = TensorElement(A(i, j), {i: 2})\n    assert te.get_free_indices() == [j]\n    assert te.get_indices() == [j]\n    assert _is_equal(te.coeff, 1)\n\n    te2 = TensorElement(A(i, j), {i: 2, j: 1})\n    assert te2.get_free_indices() == []\n    assert te2.get_indices() == []\n    assert _is_equal(te2.coeff, 1)\n\n    te3 = TensorElement(A(i, j), {})\n    assert te3.get_free_indices() == [i, j]\n    assert te3.get_indices() == [i, j]\n    assert _is_equal(te3.coeff, 1)\n", "def test_TensExpr_data():\n    Lorentz = TensorIndexType(\"Lorentz\", dummy_name=\"L\")\n    i, j = tensor_indices(\"i j\", Lorentz)\n    A, B = tensor_heads(\"A B\", [Lorentz]*2)\n    t = A(i, j) + B(i, j)\n    t2 = A(i, j) + B(j, i)\n    replacement_dict = {A(i, j): [[1, 2], [3, 4]], Lorentz: diag(1, -1)}\n    assert _is_equal(t.replace_with_arrays(replacement_dict, [i, j]), [[2, 6], [4, 8]])\n    with raises(ValueError):\n        t2.replace_with_arrays(replacement_dict, [i, j])\n", "def test_TensorElement():\n    Lorentz = TensorIndexType(\"L\")\n    i, j = symbols(\"i j\")\n    A = TensorHead(\"A\", [Lorentz, Lorentz], TensorSymmetry.fully_symmetric(2))\n    te = TensorElement(A(i, j), {i: 2})\n    assert te.get_free_indices() == [j]\n    assert _is_equal(te, A(2, j))\n", "def test_TensorElement():\n    Lorentz = TensorIndexType(\"L\")\n    i, j, k = symbols(\"i j k\")\n    A = TensorHead(\"A\", [Lorentz, Lorentz], TensorSymmetry.fully_symmetric(2))\n    te = TensorElement(A(i, j), {i: 2})\n    assert te.get_free_indices() == [j]\n    assert te._replace_indices({j: k}).get_free_indices() == [k]\n    raises(ValueError, lambda: TensorElement(A(i, j), {Symbol(\"x\"): 2}))\n", "def test_TensorHead__components_data_full_destroy():\n    # create a tensor head for testing:\n    Lorentz = TensorIndexType(\"Lorentz\")\n    TH = TensorHead(\"TH\", [Lorentz, Lorentz])\n    i1, i2 = tensor_indices(\"i1:3\", Lorentz)\n    TH.data = [[1, 2], [3, 4]]\n\n    # updating Kronecker delta is the job of TensorIndexType:\n    assert Lorentz.get_kronecker_delta()(i1, i2).data == [[1, 0], [0, 1]]\n\n    # destroy components data of TH:\n    TH._components_data_full_destroy()\n\n    # assert that all components data have been deleted:\n    assert TH.data is None\n    assert Lorentz.get_kronecker_delta()(i1, i2).data == [[1, 0], [0, 1]]\n"], "sample_1117": ["def test_AskUpperTriangularHandler():\n    assert ask(Q.upper_triangular(X), Q.upper_triangular(X)) == True\n    assert ask(Q.upper_triangular(X + X.T), Q.upper_triangular(X)) == None\n    assert ask(Q.upper_triangular(X), Q.lower_triangular(X)) == None\n    assert ask(Q.upper_triangular(X), Q.orthogonal(X)) == None\n    assert ask(Q.upper_triangular(X.T), Q.upper_triangular(X)) == None\n    assert ask(Q.upper_triangular(X), Q.invertible(X)) == None\n    assert ask(Q.upper_triangular(X), Q.integer_elements(X)) == None\n    assert ask(Q.upper_triangular(X), Q.real_elements(X)) == None\n    assert ask(Q.upper_triangular(X + Identity(2)), Q.upper_triangular(X)) == True\n    assert ask(Q.upper_triangular(X + ZeroMatrix(2, 2)), Q.upper_triangular(X)) == True\n    assert ask(Q.upper_triangular(X + OneMatrix(2, 2)), Q.upper_triangular(X)) == True\n    assert ask(Q.upper_triangular(X**2), Q.upper_triangular(X)) == True\n", "def test_AskSquareHandler():\n    assert ask(Q.square(X), Q.square(X)) == True\n    assert ask(Q.square(Y), Q.square(Y)) == False\n    assert ask(Q.square(X), Q.invertible(X)) == None\n    assert ask(Q.square(ZeroMatrix(3, 3)), Q.invertible(X)) == True\n    assert ask(Q.square(Identity(2)), Q.invertible(X)) == True\n", "def test_AskHandlers():\n    assert ask(Q.upper_triangular(X), Q.upper_triangular(X)) == True\n    assert ask(Q.lower_triangular(X), Q.lower_triangular(X)) == True\n    assert ask(Q.upper_triangular(X), Q.lower_triangular(X)) == None\n    assert ask(Q.lower_triangular(X), Q.upper_triangular(X)) == None\n    assert ask(Q.upper_triangular(X + X.T), Q.upper_triangular(X)) == True\n    assert ask(Q.lower_triangular(X + X.T), Q.lower_triangular(X)) == True\n    assert ask(Q.upper_triangular(X * X.T), Q.upper_triangular(X)) == True\n    assert ask(Q.lower_triangular(X * X.T), Q.lower_triangular(X)) == True\n    assert ask(Q.upper_triangular(Identity(2)), Q.upper_triangular(X)) == True\n    assert ask(Q.lower_triangular(Identity(2)), Q.lower_triangular(X)) == True\n    assert ask(Q.upper_triangular(ZeroMatrix(2, 2)), Q.upper_triangular(X)) == True\n    assert ask(Q.lower_triangular(ZeroMatrix(2, 2)), Q.lower_triangular(X)) == True\n    assert ask(Q.upper_triangular(OneMatrix(1, 1)), Q.upper_triangular(A1x1)) == True\n    assert ask(Q.lower_triangular(OneMatrix(1, 1)), Q.lower_triangular(A1x1)) == True\n    assert ask(Q.upper_triangular(DiagonalMatrix([1, 2])), Q.upper_triangular(X)) == True\n    assert ask(Q.lower_triangular(DiagonalMatrix([1, 2])), Q.lower_triangular(X)) == True\n", "def test_AskFullRankHandler():\n    assert ask(Q.fullrank(X), Q.invertible(X)) is True\n    assert ask(Q.fullrank(X), ~Q.invertible(X)) is None\n    assert ask(Q.fullrank(X*Y), Q.invertible(X)) is None\n    assert ask(Q.fullrank(X*X), Q.invertible(X)) is True\n    assert ask(Q.fullrank(X + Y), Q.invertible(X)) is None\n    assert ask(Q.fullrank(Identity(2)), Q.invertible(X)) is True\n", "def test_AskPositiveDefiniteHandler():\n    assert ask(Q.positive_definite(X), Q.positive_definite(X)) == True\n    assert ask(Q.positive_definite(X + X.T), Q.positive_definite(X)) == True\n    assert ask(Q.positive_definite(X + Y + Y.T), Q.positive_definite(X)) == None\n    assert ask(Q.positive_definite(X), Q.invertible(X) & Q.positive(X)) == True\n"], "sample_1118": ["def test_MatPow_doit():\n    # Test cases for doit method in MatPow\n    assert MatPow(C, 0).doit() == Identity(n)\n    assert MatPow(C, 1).doit() == C\n    assert MatPow(C, -1).doit() == Inverse(C)\n    assert MatPow(ZeroMatrix(n, n), 2).doit() == ZeroMatrix(n, n)\n    assert MatPow(Identity(n), 3).doit() == Identity(n)\n    raises(NonInvertibleMatrixError, lambda: MatPow(ZeroMatrix(n, n), -1).doit())\n    raises(NonSquareMatrixError, lambda: MatPow(A, 2).doit())\n", "def test_MatPow_doit():\n    # Test doit for MatPow with PermutationMatrix\n    from sympy.matrices.expressions import PermutationMatrix\n    P = PermutationMatrix([0, 2, 1])\n    assert MatPow(P, 3).doit() == P**3\n\n    # Test doit for MatPow with Identity\n    I = Identity(3)\n    assert MatPow(I, 3).doit() == I\n\n    # Test doit for MatPow with ZeroMatrix\n    Z = ZeroMatrix(3, 3)\n    raises(NonInvertibleMatrixError, lambda: MatPow(Z, -1).doit())\n    assert MatPow(Z, 0).doit() == I\n\n    # Test doit for MatPow with negative exponent\n    assert MatPow(C, -1).doit() == Inverse(C)\n\n    # Test doit for MatPow with non-integer exponent\n    M = MatrixSymbol('M', 3, 3)\n    assert MatPow(M, S(1)/2).doit() == MatPow(M, S(1)/2)\n", "def test_matpow_doit():\n    # Test cases for doit method of MatPow\n    assert MatPow(C, 0).doit() == Identity(n)\n    assert MatPow(C, 1).doit() == C\n    assert MatPow(Identity(n), 3).doit() == Identity(n)\n    assert MatPow(ZeroMatrix(n, n), 3).doit() == ZeroMatrix(n, n)\n    raises(NonInvertibleMatrixError, lambda: MatPow(ZeroMatrix(n, n), -1).doit())\n", "def test_MatPow_doit():\n    # Test that doit() method of MatPow can handle different types of bases\n    assert MatPow(C, 0).doit() == Identity(n)\n    assert MatPow(C, 1).doit() == C\n    assert MatPow(Identity(n), 3).doit() == Identity(n)\n    assert MatPow(ZeroMatrix(n, n), 2).doit() == ZeroMatrix(n, n)\n    assert MatPow(ZeroMatrix(n, m), -1).doit() is None  # Can't invert non-square matrix\n    assert MatPow(C, -1).doit() == Inverse(C)\n    assert MatPow(MatPow(C, 2), 3).doit() == C**6\n", "def test_MatPow_doit():\n    assert MatPow(A, 0).doit() == MatPow(A, 0)\n    assert MatPow(C, 0).doit() == Identity(n)\n    assert MatPow(C, 1).doit() == C\n    assert MatPow(C, S.One).doit() == C\n    assert MatPow(C, S.NegativeOne).doit() == Inverse(C)\n    raises(NonInvertibleMatrixError, lambda: MatPow(ZeroMatrix(n, n), -1).doit())\n    raises(NonSquareMatrixError, lambda: MatPow(A, -1).doit())\n    assert MatPow(MatPow(C, 2), 3).doit() == MatPow(C, 6)\n"], "sample_1119": ["def test_matrix_analytic_func():\n    from sympy import symbols, Matrix, log, S\n    x = symbols('x', real=True)\n    A = Matrix([[S(5)/4, S(3)/4], [S(3)/4, S(5)/4]])\n    # log(x) is undefined at x=0\n    raises(ValueError, lambda: A.analytic_func(log(x), x))\n", "def test_condition_number():\n    A = MatrixSymbol('A', 3, 3)\n    assert A.condition_number() == Max(*A.singular_values()) / Min(*A.singular_values())\n    assert eye(3).condition_number() == 1\n", "def test_analytic_func():\n    x = symbols('x', real=True)\n    m = Matrix([[S(5)/4, S(3)/4], [S(3)/4, S(5)/4]])\n    f = symbols('f', cls=symbols)\n    raises(ValueError, lambda: m.analytic_func(f, x))\n\n    m = Matrix([[1, 2], [3, 4]])\n    f = symbols('f', cls=symbols)\n    raises(ValueError, lambda: m.analytic_func(f, x))\n\n    m = Matrix([[x, 1], [1, 2]])\n    f = x**2\n    raises(ValueError, lambda: m.analytic_func(f, x))\n", "def test_pinverse_noninvertible():\n    M = MatrixSymbol('M', 2, 2)\n    raises(NonInvertibleMatrixError, lambda: M.pinv(method='ED'))\n", "def test_inverse_LDL():\n    raises(NonSquareMatrixError, lambda: Inverse(A))\n    assert Inverse(C).doit() == Inverse(C)\n    assert Inverse(Identity(n)).doit() == Identity(n)\n    assert Inverse(Inverse(C)).doit() == C\n    raises(NonInvertibleMatrixError, lambda: Inverse(ZeroMatrix(n, n)).doit())\n    raises(NonInvertibleMatrixError, lambda: Inverse(OneMatrix(n, n) - Identity(n)).doit())\n"], "sample_1120": ["def test_matrix_from_index_summation():\n    from sympy import Sum, Symbol, MatrixSymbol, Identity\n    from sympy.abc import i, j, k, N\n\n    A = MatrixSymbol(\"A\", N, N)\n    B = MatrixSymbol(\"B\", N, N)\n    S = Sum(A[i, j]*B[j, k], (j, 0, N-1))\n\n    assert S.doit() == MatrixExpr.from_index_summation(S) == A*B\n    assert MatrixExpr.from_index_summation(Sum(A[i, j]*B[k, j], (j, 0, N-1))) == A.T*B\n\n    S = Sum(A[i, i], (i, 0, N-1))\n    assert MatrixExpr.from_index_summation(S) == Sum(A[i, i], (i, 0, N-1))\n\n    S = Sum(A[i, j]*B[k, j]*A[l, k], (j, 0, N-1), (k, 0, N-1))\n    assert MatrixExpr.from_index_summation(S) == A*B.T*A.T\n\n    # Issue 15188\n    i = Symbol(\"i\", integer=True)\n    s = Sum(Identity(i + 2)[i, 0], (i, 1, 3))\n    assert MatrixExpr.from_index_summation(s) == Matrix([[1, 0], [0, 1]])[:, 0]\n", "def test_matrix_symbol():\n    n, m = symbols('n m', integer=True)\n    A = MatrixSymbol('A', n, m)\n    assert A.shape == (n, m)\n    assert A.rows == n\n    assert A.cols == m\n    assert A.is_square == (n == m)\n\n    assert MatrixSymbol('A', n, m) == MatrixSymbol('A', n, m)\n    assert (MatrixSymbol('A', n, m) != MatrixSymbol('A', n, m)) == False\n    assert MatrixSymbol('A', n, m) != MatrixSymbol('B', n, m)\n    raises(TypeError, lambda: MatrixSymbol('A', n, m) < MatrixSymbol('B', n, m))\n\n    assert MatrixSymbol('A', n, m).args == (Symbol('A'), n, m)\n", "def test_matrix_element_construction():\n    A = MatrixSymbol('A', 2, 2)\n    assert MatrixElement(A, 0, 0) == A[0, 0]\n    assert MatrixElement(A, 1, 1) == A[1, 1]\n    assert MatrixElement(A, Symbol('i'), Symbol('j')) == A[Symbol('i'), Symbol('j')]\n\n    raises(TypeError, lambda: MatrixElement(A, 'a', 0))\n    raises(TypeError, lambda: MatrixElement(A, 0, 'b'))\n    raises(TypeError, lambda: MatrixElement(A, 3, 0))\n    raises(TypeError, lambda: MatrixElement(A, 0, 3))\n\n    a = Matrix([[1, 2], [3, 4]])\n    assert MatrixElement(a, 0, 0) == 1\n    assert MatrixElement(a, 1, 1) == 4\n    assert MatrixElement(a, Symbol('i'), Symbol('j')) == a[Symbol('i'), Symbol('j')]\n", "def test_matrix_element():\n    i, j = symbols('i j', integer=True)\n    assert MatrixElement(A, i, j).subs(A, ImmutableMatrix([[1, 2], [3, 4]])) == \\\n        Piecewise((1, Eq(i, 0)), (3, Eq(i, 1)))*(Piecewise((1, Eq(j, 0)), (2, Eq(j, 1))))\n    assert MatrixElement(B, i, j).subs(B, ImmutableMatrix([[1, 2], [3, 4]])) == \\\n        Piecewise((1, Eq(i, 0)), (3, Eq(i, 1)))*(Piecewise((1, Eq(j, 0)), (2, Eq(j, 1))))\n    assert MatrixElement(E, i, j).subs(E, ImmutableMatrix([[1, 2], [3, 4]])) == \\\n        Piecewise((1, Eq(i, 0)), (3, Eq(i, 1)))*(Piecewise((1, Eq(j, 0)), (2, Eq(j, 1))))\n    A_expr = MatrixElement(A, i, j)\n    assert A_expr.subs(A, Identity(n)) == KroneckerDelta(i, j, (0, n-1))\n    assert A_expr.subs(A, ZeroMatrix(n, n)) == 0\n    assert MatrixElement(A, i, j).subs({i: 1, j: 2}) == A[1, 2]\n", "def test_matrix_element():\n    i, j = symbols('i j')\n    assert MatrixElement(A, i, j).parent == A\n    assert MatrixElement(A, i, j).i == i\n    assert MatrixElement(A, i, j).j == j\n    assert MatrixElement(A, i, j).indices == (i, j)\n    assert MatrixElement(A, 0, 1).subs({i: 0, j: 1}) == A[0, 1]\n\n    X = MatrixSymbol('X', 2, 2)\n    assert MatrixElement(X, 0, 0).subs(X, Matrix([[1, 2], [3, 4]])) == 1\n    assert MatrixElement(X, 1, 0).subs(X, Matrix([[1, 2], [3, 4]])) == 3\n"], "sample_1121": ["def test_Mul_eval():\n    assert Mul(0, x).doit() == 0\n    assert Mul(1, x).doit() == x\n    assert Mul(-1, x).doit() == -x\n    assert Mul(2, x).doit() == 2*x\n    assert Mul(0.5, x).doit() == 0.5*x\n    assert Mul(Rational(1, 2), x).doit() == Rational(1, 2)*x\n    assert Mul(3*x, 2*x).doit() == 6*x**2\n    assert Mul(x/2, 2*x).doit() == x**2\n    assert Mul(pi, x).doit() == pi*x\n", "def test_Mul():\n    assert Mul(x, x) == x**2\n    assert Mul(x, 1) == x\n    assert Mul(x, S(1)/2) == x/2\n    assert Mul(x, 0) == 0\n    assert Mul(x, S(0)) == 0\n    assert Mul(2, 3) == 6\n    assert Mul(0, 1.0*x) == 0\n    assert Mul(2, S(3)/2) == 3\n    assert Mul(2, S(3)/2, evaluate=False) == Mul(2, S(3)/2, evaluate=False)\n    assert Mul(S(2), 3, evaluate=False) == Mul(S(2), 3, evaluate=False)\n    assert Mul(x, x*y) == x**2*y\n    assert Mul(x*y, x) == x**2*y\n    assert Mul(y, x*y) == x*y**2\n    assert Mul(x, 1, x) == x**2\n    assert Mul(5, 3*x, x/4) == 15*x**2/4\n    assert Mul(2*x*(3*y/4), 7) == 21*x*y/2\n    assert Mul(2*x*(3*y/4), x*(7*y/2)) == 21*x**2*y**2/4\n    assert Mul(2*x*(3*y/4), 4*x/3) == 8*x**2*y/4\n    assert Mul(2*x*(3*y/4), x/3) == 2*x**2*y/4\n    assert Mul(4, 4*x/3) == 16*x/3\n    assert Mul(-2, 2*x/3) == -4*x/3\n    assert Mul(x, y*(1/x)) == y\n    assert Mul(1/(2*x), y*x*x) == y*x/2\n    assert Mul(1/(2*x), Mul(y, x, x)) == y*x/2\n    assert Mul(1/(2*x), (y*x*x)) == y*x/2\n    assert Mul(x, y, 1/x, 2*x*y) == 2*y**2*x\n    assert Mul(x, y, 1, 1, x, y,", "def test_Mul_as_coeff_Mul():\n    assert (3*x*y).as_coeff_Mul() == (3, x*y)\n    assert (3*x*y).as_coeff_Mul()[1] == x*y\n    assert (x*y).as_coeff_Mul()[0] == 1\n    assert (-3*x*y).as_coeff_Mul() == (-3, x*y)\n    assert (-x*y).as_coeff_Mul() == (-1, x*y)\n    assert (2.0*x*y).as_coeff_Mul() == (2.0, x*y)\n    assert (Rational(2,3)*x*y).as_coeff_Mul() == (Rational(2,3), x*y)\n", "def test_Mul_expanded():\n    x = Symbol('x')\n    y = Symbol('y')\n    assert Mul(x + 2, y + 3).expand() == x*y + 3*x + 2*y + 6\n", "def test_Mul():\n    assert Mul(1, 2, evaluate=False).is_Mul\n    assert not Mul(x, 1, evaluate=False).is_number\n    assert Mul(x, 2, evaluate=False).is_Mul\n    assert (2*x).is_Mul\n    assert (x*y).is_Mul\n    assert (x/x).is_Mul\n    assert (x**2*sin(x)).is_Mul\n    assert (2*x*y).is_Mul\n    assert (2*x*y).is_commutative\n    assert (x*y).is_commutative\n    assert (x*2*x).is_Mul\n    assert not (x*2*x).is_commutative\n    assert (x*2*x).is_Mul\n    assert not (x*y*z).is_commutative\n    assert not (x*y*z).is_commutative\n    assert not (x*y).is_commutative\n    assert not (x*y*z).is_commutative\n    assert not (x*z*y).is_commutative\n"], "sample_1122": ["def test_abs():\n    assert Abs(0) == 0\n    assert Abs(1) == 1\n    assert Abs(-1) == 1\n    assert Abs(I) == 1\n    assert Abs(-I) == 1\n    assert Abs(I * 2) == 2\n    assert Abs(-3) == 3\n    assert Abs(Rational(1, 2)) == Rational(1, 2)\n    assert Abs(Rational(-1, 2)) == Rational(1, 2)\n    assert Abs(1.0) == 1.0\n    assert Abs(-1.0) == 1.0\n    assert Abs(1.0/2.0) == 1.0/2.0\n    assert Abs(-1.0/2.0) == 1.0/2.0\n    assert unchanged(Abs, S.Pi)\n    assert unchanged(Abs, S.E)\n    assert unchanged(Abs, Symbol('x', real=True))\n    assert unchanged(Abs, Symbol('x', complex=True))\n    assert unchanged(Abs, 3+4*I)\n    x = Symbol('x', real=True)\n    assert unchanged(Abs, exp(x))\n    assert unchanged(Abs, 1 + x)\n    assert unchanged(Abs, cos(x))\n    assert unchanged(Abs, x**2 + 2*x + 1)\n    x = Symbol('x', complex=True)\n    assert unchanged(Abs, exp(x))\n    assert unchanged(Abs, 1 + x)\n    assert unchanged(Abs, cos(x))\n    assert unchanged(Abs, x**2 + 2*x + 1)\n    x = Symbol('x', real=True)\n    assert unchanged(Abs, log(x))\n    assert unchanged(Abs, atan(x))\n    assert unchanged(Abs, sqrt(x))\n", "def test_principal_branch():\n    x = Symbol('x')\n    assert principal_branch(x, 2*pi) == principal_branch(x, 2*pi)\n    assert principal_branch(exp_polar(2*pi*I)*3, 2*pi) == 3*exp_polar(0)\n    assert principal_branch(exp_polar(2*pi*I)*3*x, 2*pi) == 3*principal_branch(x, 2*pi)\n    assert principal_branch(exp_polar(I*pi)*x, 2*pi) == -principal_branch(x, 2*pi)\n", "def test_polar_lift():\n    x = Symbol('x')\n    p = Symbol('p', polar=True)\n    assert polar_lift(0) == 0\n    assert polar_lift(2) == 2*exp_polar(0)\n    assert polar_lift(I) == exp_polar(I*pi/2)\n    assert polar_lift(-I) == exp_polar(-I*pi/2)\n    assert polar_lift(3 + 4*I) == polar_lift(3 + 4*I)\n    assert polar_lift(x) == polar_lift(x)\n    assert polar_lift(2*x) == 2*polar_lift(x)\n    assert polar_lift(p) == p\n    assert polar_lift(2*p) == 2*p\n", "def test_polar_lift():\n    x = Symbol('x')\n    p = Symbol('p', polar=True)\n    assert polar_lift(4) == 4*exp_polar(0)\n    assert polar_lift(-4) == 4*exp_polar(I*pi)\n    assert polar_lift(-I) == exp_polar(-I*pi/2)\n    assert polar_lift(I + 2) == polar_lift(2 + I)\n    assert polar_lift(4*x) == 4*polar_lift(x)\n    assert polar_lift(4*p) == 4*p\n", "def test_principal_branch():\n    from sympy import principal_branch\n    assert principal_branch(2*pi*I*exp_polar(2*pi*I), 2*pi) == 1\n    assert principal_branch(2*pi*I*exp_polar(4*pi*I), 2*pi) == 1\n    assert principal_branch(2*pi*I*exp_polar(2*pi*I), pi) == -1\n    assert principal_branch(2*pi*I*exp_polar(4*pi*I), pi) == 1\n    assert principal_branch(2*pi*I*exp_polar(2*pi*I + pi/2), 2*pi) == -I\n    assert principal_branch(2*pi*I*exp_polar(2*pi*I + pi/2), pi) == I\n"], "sample_1123": ["def test_ConditionSet_base_set():\n    # Test that base_set is a property\n    cs = ConditionSet(x, x>0)\n    assert cs.base_set == S.UniversalSet\n\n    # Test that base_set can be changed with subs\n    cs2 = cs.subs(x, y)\n    assert cs2.base_set == S.UniversalSet\n\n    # Test that base_set is respected in _contains\n    cs3 = ConditionSet(x, x>0, base_set=S.Integers)\n    assert cs3._contains(2) == True\n    assert cs3._contains(2.5) == False\n\n    # Test that base_set is respected in as_relational\n    cs4 = ConditionSet(x, x>0, base_set=S.Integers)\n    assert cs4.as_relational(2) == And(2 > 0, Contains(2, S.Integers))\n    assert cs4.as_relational(2.5) == And(2.5 > 0, Contains(2.5, S.Integers))\n", "def test_ConditionSet_dummy_symbol():\n    c = ConditionSet(x, x < 1, {x, z})\n    assert c.subs(x, y) == ConditionSet(x, x < 1, FiniteSet(y, z))\n    assert c.subs(x, y).subs(x, y) == ConditionSet(y, y < 1, FiniteSet(y, z))\n    c = ConditionSet(y, y < 1, {y, z})\n    assert c.subs(y, 1) == ConditionSet(y, y < 1, FiniteSet(z))\n    assert c.subs(y, 1).subs(y, 1) == ConditionSet(y, y < 1, FiniteSet(z))\n    c = ConditionSet(x, x < 1, S.Integers)\n    raises(ValueError, lambda: c.subs(x, x + 1))\n", "def test_ConditionSet_subs():\n    c = ConditionSet(x, x < 1, {x, z})\n    assert c.subs(x, y) == ConditionSet(x, x < 1, FiniteSet(y, z))\n    assert c.subs(x, y).subs(x, y) == ConditionSet(y, y < 1, FiniteSet(y, z))\n    assert ConditionSet(y, y < 1, {y, z}).subs(y, 1) == ConditionSet(y, y < 1, FiniteSet(z))\n", "def test_conditionset_eval_subs():\n    # Test substitution with different types of symbols\n    cs = ConditionSet(x, x < 1, S.Integers)\n    assert cs.subs(x, y) == ConditionSet(y, y < 1, S.Integers)\n    assert cs.subs(x, Symbol('x', positive=True)) == cs\n\n    # Test substitution with non-symbol\n    assert cs.subs(x, 1) == ConditionSet(x, x < 1, S.EmptySet)\n\n    # Test substitution with same assumptions\n    cs = ConditionSet(Symbol('x', real=True), x < 1, S.Reals)\n    assert cs.subs(x, Symbol('y', real=True)) == ConditionSet(Symbol('y', real=True), Symbol('y', real=True) < 1, S.Reals)\n\n    # Test substitution with single assumption having same commutativity\n    cs = ConditionSet(Symbol('x', commutative=True), x < 1, S.Integers)\n    assert cs.subs(x, Symbol('y', commutative=True)) == ConditionSet(Symbol('y', commutative=True), Symbol('y', commutative=True) < 1, S.Integers)\n", "def test_ConditionSet_dummy_symbol():\n    # Test if ConditionSet handles dummy symbol correctly\n    cs = ConditionSet(x, x < 1, S.Integers)\n    assert cs.sym == x\n    assert cs.condition == x < 1\n    assert cs.base_set == S.Integers\n    cs = ConditionSet(x, x < 1, {x, z})\n    assert cs.sym == x\n    assert cs.condition == x < 1\n    assert cs.base_set == FiniteSet(x, z)\n    cs = ConditionSet(x, x < 1, ConditionSet(x, x > 0, S.Integers))\n    assert cs.sym != x\n    assert cs.condition == (cs.sym < 1) & (cs.sym > 0)\n    assert cs.base_set == S.Integers\n"], "sample_1124": ["def test_FracField_to_domain():\n    F = FracField(\"x,y\", ZZ, lex)\n    assert isinstance(F.to_domain(), FractionField)\n", "def test_FracElement_diff():\n    F, x, y = field(\"x,y\", ZZ)\n    f = (x**2 + 2*y)/(y + 1)\n    assert f.diff(x) == 2*x/(y + 1)\n    assert f.diff(y) == (2 - x**2 - 2*y)/(y + 1)**2\n", "def test_FracElement___add__():\n    F, x, y = field('x,y', ZZ)\n    f = F(x + 1, y)\n    g = F(x, y + 1)\n\n    assert f + g == F(2*x + 1, y**2 + y)\n    assert f + 2 == F(x + 3, y)\n    assert 2 + f == F(x + 3, y)\n    assert f + F.ring(1) == F(x + 2, y)\n    assert F.ring(1) + f == F(x + 2, y)\n    raises(NotImplementedError, lambda: f + 1.0)\n", "def test_FracElement___truediv__():\n    F, x, y = field(\"x,y\", ZZ)\n    f = x/y\n    g = F(2, 3)\n    h = f / g\n\n    assert h == F(3*x, 2*y)\n    assert f / 2 == F(x, 2*y)\n    assert 2 / f == F(2*y, x)\n    raises(ZeroDivisionError, lambda: f / F.zero)\n    raises(ZeroDivisionError, lambda: f / 0)\n", "def test_FracElement___truediv__():\n    F, x, y = field(\"x,y\", ZZ)\n    f = F(1, x)\n    g = F(1, y)\n\n    assert f / g == F(y, x)\n    assert f / x == F(1, x**2)\n    assert f / 2 == F(1, 2*x)\n\n    raises(ZeroDivisionError, lambda: f / 0)\n    raises(ZeroDivisionError, lambda: f / F(0))\n"], "sample_1125": ["def test_operator_inv():\n    a = Operator('A')\n    assert a.inv() == a**(-1)\n    assert a.inv().inv() == a\n", "def test_operator_inv():\n    A = Operator('A')\n    assert A.inv() == A**(-1)\n    assert A.inv().inv() == A\n", "def test_operator_inv():\n    A = Operator('A')\n    assert A.inv() == A**(-1)\n    assert A.inv().inv() == A\n", "def test_operator():\n    x = symbols('x')\n    A = Operator('A', x)\n    assert isinstance(A, Operator)\n    assert A.is_commutative == False\n    assert str(A) == 'A'\n    assert A.hilbert_space == 'H'\n    assert A.inv() == A**(-1)\n    assert A.inv() != A\n    assert A*x == Mul(A, x)\n    assert Dagger(A) != A\n    assert A._eval_commutator(A) is None\n    assert A._eval_anticommutator(A) is None\n    assert A._apply_operator(x) is None\n", "def test_HermitianOperator():\n    op = Operator('H')\n    herm_op = op.as_expr()\n    conj_op = conjugate(herm_op)\n    assert herm_op == conj_op\n"], "sample_1126": ["def test_Dagger_mul():\n    A = Operator('A')\n    B = Operator('B')\n    C = Dagger(A*B)\n    assert C == Dagger(B)*Dagger(A)\n\n    D = Dagger(A)*Dagger(B)\n    assert D == Dagger(B)*Dagger(A)\n\n    I1 = IdentityOperator()\n    assert Dagger(A)*I1 == Dagger(A)\n", "def test_dagger_operation():\n    # Test Dagger operation on complex numbers and matrices\n    assert Dagger(I) == -I\n    assert Dagger(Matrix([[1, I], [2, I]])) == Matrix([[1, 2], [-I, -I]])\n\n    # Test Dagger operation on Operators\n    A = Operator('A')\n    assert Dagger(A) == Dagger(A)\n\n    # Test Dagger operation on IdentityOperator\n    I = IdentityOperator()\n    assert Dagger(A) * I == Dagger(A)\n\n    # Test Dagger operation on integers and symbols\n    assert Dagger(Integer(1)) == Integer(1)\n    x = symbols('x', real=True)\n    assert Dagger(x) == x\n", "def test_dagger_operation_on_matrix():\n    m = Matrix([[1, I], [2, I]])\n    assert Dagger(m) == m.H\n    assert Dagger(m) == m.adjoint()\n    assert Dagger(m) == conjugate(m).transpose()\n", "def test_Dagger_new():\n    # Test Dagger with arg that has 'adjoint' method\n    A = Operator('A')\n    assert Dagger(A) == A.adjoint()\n\n    # Test Dagger with arg that has 'conjugate' and 'transpose' methods\n    m = Matrix([[1, I], [2, I]])\n    assert Dagger(m) == m.conjugate().transpose()\n\n    # Test Dagger with arg that does not have 'adjoint', 'conjugate', or 'transpose' methods\n    x = symbols('x')\n    assert isinstance(Dagger(x), Expr)\n    assert Dagger(x).args == (x,)\n", "def test_dagger_on_matrix():\n    m = Matrix([[1, I], [2, I]])\n    assert Dagger(m) == m.adjoint()\n    assert Dagger(m) == m.conjugate().transpose()\n"], "sample_1128": ["def test_vel_calculation():\n    N = ReferenceFrame('N')\n    p1 = Point('p1')\n    p2 = Point('p2')\n    p1.set_vel(N, 10 * N.x)\n    p2.set_pos(p1, 5 * N.y)\n    raises(ValueError, lambda: p2.vel(ReferenceFrame('M')))\n    assert p2.vel(N) == 10 * N.x\n", "def test_set_vel():\n    N = ReferenceFrame('N')\n    p1 = Point('p1')\n    p1.set_vel(N, 10 * N.x)\n    assert p1.vel(N) == 10 * N.x\n    p1.set_vel(N, 20 * N.x)\n    assert p1.vel(N) == 20 * N.x\n", "def test_point_vel():\n    q, qd = dynamicsymbols('q qd')\n    N = ReferenceFrame('N')\n    B = N.orientnew('B', 'Axis', [q, N.z])\n    O = Point('O')\n    P = O.locatenew('P', 10 * B.x)\n    O.set_vel(N, 5 * N.x)\n    P.set_vel(B, qd * B.x)\n    assert P.vel(N) == 5*N.x + qd*B.x + 10*qd*B.y\n", "def test_set_vel():\n    N = ReferenceFrame('N')\n    p = Point('p')\n    v1 = 10 * N.x\n    p.set_vel(N, v1)\n    assert p.vel(N) == v1\n", "def test_set_vel():\n    N = ReferenceFrame('N')\n    p1 = Point('p1')\n    p1.set_vel(N, 10 * N.x)\n    assert p1.vel(N) == 10 * N.x\n    q = dynamicsymbols('q')\n    p2 = Point('p2')\n    p2.set_pos(p1, q * N.x)\n    assert p2.vel(N) == (q.diff() + 10) * N.x\n"], "sample_1127": ["def test_coset():\n    a = Permutation(1, 2)\n    b = Permutation(0, 1)\n    G = SymmetricGroup(3)\n    H = PermutationGroup([a])\n    assert Coset(a, H, G, dir=\"-\").as_list() == [Permutation(0, 2, 1), Permutation(0, 1, 2)]\n    assert Coset(b, H, G, dir=\"+\").as_list() == [Permutation(0, 1, 2), Permutation(0, 2, 1)]\n    assert Coset(b, H, dir=\"-\").as_list() == [Permutation(0, 2), Permutation(1, 2)]\n    assert Coset(b, H, dir=\"+\").as_list() == [Permutation(0, 1), Permutation(2, 1)]\n    assert Coset(b, H, SymmetricPermutationGroup(3), dir=\"+\").as_list() == [Permutation(0, 1, 2), Permutation(0, 2, 1)]\n", "def test_polycyclic_presentation():\n    G = DihedralGroup(8)\n    assert G.polycyclic_group() is not None\n    G = SymmetricGroup(4)\n    assert G.polycyclic_group() is not None\n    G = AlternatingGroup(4)\n    assert G.polycyclic_group() is not None\n", "def test_coset():\n    a = Permutation(1, 2)\n    b = Permutation(0, 1)\n    G = PermutationGroup([a, b])\n    cst = Coset(a, G, dir=\"-\")\n    assert cst.is_left_coset\n    assert cst.as_list() == [Permutation(0, 1, 2), Permutation(0, 2, 1), Permutation(0, 1)]\n    cst = Coset(a, G, dir=\"+\")\n    assert cst.is_right_coset\n    assert cst.as_list() == [Permutation(0, 1, 2), Permutation(0, 2, 1), Permutation(0, 1)]\n    c = Permutation(3)(1, 2)\n    assert Coset(c, G).as_list() == [Permutation(3)(0, 1, 2), Permutation(3)(0, 2, 1), Permutation(3)(0, 1)]\n", "def test_coset_transversal():\n    G = SymmetricGroup(4)\n    H = G.stabilizer(0)\n    T = G.coset_transversal(H)\n    assert len(T) == G.order() // H.order()\n    for g in G.generate():\n        assert any((g*t)**-1 in H for t in T)\n", "def test_coset():\n    a = Permutation(1, 2)\n    b = Permutation(0, 1)\n    G = PermutationGroup([a, b])\n    cst = Coset(a, G, dir=\"+\")\n    assert cst.is_right_coset\n    assert not cst.is_left_coset\n    assert cst.as_list() == [Permutation(1, 2), Permutation(0, 2, 1),\n                             Permutation(0, 1, 2), Permutation(2),\n                             Permutation(0, 2), Permutation(0, 1)]\n\n    cst = Coset(a, G, dir=\"-\")\n    assert not cst.is_right_coset\n    assert cst.is_left_coset\n    assert cst.as_list() == [Permutation(1, 2), Permutation(2),\n                             Permutation(1, 0, 2), Permutation(1, 0),\n                             Permutation(0, 2, 1), Permutation(0, 2)]\n"], "sample_1129": ["def test_PythonCodePrinter_print_ImaginaryUnit():\n    assert PythonCodePrinter().doprint(1j) == '1j'\n    assert PythonCodePrinter().doprint(2j) == '2*1j'\n", "def test_pycode_piecewise_default():\n    # Test that the default arg of NumPyPrinter._print_Piecewise is included when necessary\n    expr = Piecewise((1, x > 0), (0, True))\n    result = NumPyPrinter().doprint(expr)\n    assert 'default=' in result\n", "def test_pycode_sqrt():\n    assert pycode(sqrt(x)) == 'math.sqrt(x)'\n    assert NumPyPrinter().doprint(sqrt(x)) == 'numpy.sqrt(x)'\n    assert MpmathPrinter().doprint(sqrt(x)) == 'mpmath.sqrt(x)'\n    assert SciPyPrinter().doprint(sqrt(x)) == 'numpy.sqrt(x)'\n    assert SymPyPrinter().doprint(sqrt(x)) == 'sympy.sqrt(x)'\n    assert pycode(sqrt(x), standard='python3') == 'math.sqrt(x)'\n", "def test_symPyPrinter_Assignment():\n    n = symbols('n', integer=True)\n    a = MatrixSymbol(\"a\", n, n)\n    b = MatrixSymbol(\"b\", n, n)\n    # Matrix definition\n    f = MatrixSolve(a, b)\n    assignment = Assignment(\"x\", f)\n    assert SymPyPrinter().doprint(assignment) == \"x = sympy.solve(a, b)\"\n", "def test_pycode_Piecewise():\n    assert pycode(Piecewise((x, x < 1), (x**2, x >= 1))) == \\\n        \"(x if x < 1 else x**2)\"\n    assert pycode(Piecewise((x, x < 1), (x**2, True))) == \\\n        \"(x if x < 1 else x**2)\"\n    assert pycode(Piecewise((x, x < 1), (x**2, x >= 1), (x**3, x > 2))) == \\\n        \"(x if x < 1 else (x**2 if x >= 1 else x**3))\"\n    assert pycode(Piecewise((x, x < 1), (x**2, x >= 1), (x**3, True))) == \\\n        \"(x if x < 1 else (x**2 if x >= 1 else x**3))\"\n    assert pycode(Piecewise((x, True), (x**2, False))) == \"x\"\n    assert pycode(Piecewise((x, True), (x**2, False), (x**3, False))) == \"x\"\n"], "sample_1130": ["def test_locatenew():\n    N = ReferenceFrame('N')\n    p1 = Point('p1')\n    p2 = p1.locatenew('p2', 10 * N.x)\n    assert p2.pos_from(p1) == 10 * N.x\n    assert p1.pos_from(p2) == -10 * N.x\n", "def test_point_a1pt_theory():\n    q, q2 = dynamicsymbols('q q2')\n    qd, q2d = dynamicsymbols('q q2', 1)\n    N = ReferenceFrame('N')\n    B = ReferenceFrame('B')\n    B.set_ang_vel(N, 5 * B.y)\n    O = Point('O')\n    P = O.locatenew('P', q * B.x)\n    P.set_vel(B, qd * B.x + q2d * B.y)\n    O.set_vel(N, 0)\n    assert P.a1pt_theory(O, N, B) == (-25*q + qd.diff())*B.x + q2d.diff()*B.y - 10*qd*B.z\n", "def test_velocity_with_no_relative_position():\n    N = ReferenceFrame('N')\n    P1 = Point('P1')\n    P2 = Point('P2')\n    P1.set_vel(N, 10 * N.x)\n    raises(ValueError, lambda: P2.vel(N))\n", "def test_point_vel_with_intermediate_frame():\n    u = dynamicsymbols('u')\n    N = ReferenceFrame('N')\n    A = ReferenceFrame('A')\n    A.set_ang_vel(N, u * A.z)\n    P = Point('P')\n    P.set_vel(A, u * A.x)\n    O = Point('O')\n    O.set_vel(N, u * N.x)\n    P.set_pos(O, u * A.x)\n    assert P.v1pt_theory(O, N, A) == u * A.x + u * N.x - u**2 * A.y\n", "def test_locatenew():\n    N = ReferenceFrame('N')\n    P1 = Point('P1')\n    P2 = P1.locatenew('P2', 10 * N.x)\n    assert P1.pos_from(P2) == -10*N.x\n    assert P2.pos_from(P1) == 10*N.x\n"], "sample_1131": ["def test_pythoncodeprinter_print_Infinity():\n    assert PythonCodePrinter().doprint(oo) == 'float(\\'inf\\')'\n    assert PythonCodePrinter().doprint(-oo) == 'float(\\'-inf\\')'\n", "def test_python_code_printing_mod():\n    assert pycode(Mod(x, 2)) == \"%s %% %s\" % (pycode(x), pycode(2))\n    assert NumPyPrinter().doprint(Mod(x, 2)) == \"numpy.mod(%s, %s)\" % (pycode(x), pycode(2))\n    assert MpmathPrinter().doprint(Mod(x, 2)) == \"(lambda x: x)(x % 2)\"\n    assert SciPyPrinter().doprint(Mod(x, 2)) == \"numpy.mod(%s, %s)\" % (pycode(x), pycode(2))\n    assert SymPyPrinter().doprint(Mod(x, 2)) == 'sympy.Mod(x, 2)'\n", "def test_pycode_Rational():\n    assert pycode(Rational(3, 2)) == '3/2'\n    assert pycode(Rational(3, 2), standard='python2') == '3./2.'\n", "def test_pycode_Piecewise():\n    expr = Piecewise((x, x < 1), (x**2, True))\n    assert pycode(expr) == \"(x if x < 1 else x**2)\"\n", "compilation error"], "sample_1132": ["def test_multiset_partitions():\n    # uses method of generating functions from\n    # Concrete Mathematics, p122\n    assert [list(p) for p in multiset_partitions([1, 2, 2])] == [\n        [[1, 2, 2]], [[1, 2], [2]], [[1], [2, 2]], [[1], [2], [2]]\n    ]\n    assert [list(p) for p in multiset_partitions([1, 1, 1, 1])] == [\n        [[1, 1, 1, 1]], [[1, 1, 1], [1]], [[1, 1], [1, 1]],\n        [[1, 1], [1], [1]], [[1], [1], [1], [1]]]\n    assert [list(p) for p in multiset_partitions([1, 2, 2, 2])] == [\n        [[1, 2, 2, 2]], [[1, 2, 2], [2]], [[1, 2], [2, 2]], [[1, 2], [2], [2]],\n        [[1], [2, 2, 2]], [[1], [2, 2], [2]], [[1], [2], [2], [2]]]\n    assert [list(p) for p in multiset_partitions([1, 2, 3, 4])] == [\n        [[1, 2, 3, 4]], [[1, 2, 3], [4]], [[1, 2, 4], [3]], [[1, 2], [3, 4]],\n        [[1, 2], [3], [4]], [[1, 3, 4], [2]], [[1, 3], [2, 4]], [[1, 3], [2], [4]],\n        [[1, 4], [2, 3]], [[1, 4], [2], [3]], [[1], [2, 3, 4]], [[1], [2, 3], [4]],\n        [[1], [2, 4], [3]], [[1], [2], [3, 4]], [[1], [2], [3], [4]]]\n\n    assert [list(p) for p in multiset", "def test_generate_oriented_forest():\n    result = list(generate_oriented_forest(4))\n    assert len(result) == 9\n    assert result[0] == [0, 1, 2, 3]\n    assert result[-1] == [0, 0, 0, 0]\n\n    # Test if all elements are lists\n    assert all(isinstance(i, list) for i in result)\n\n    # Test if all elements have the correct length\n    assert all(len(i) == 4 for i in result)\n\n    # Test if all elements have values within the correct range\n    assert all(0 <= x <= 3 for i in result for x in i)\n", "def test_binary_partitions():\n    assert list(binary_partitions(4)) == [[4], [2, 2], [2, 1, 1], [1, 1, 1, 1]]\n    assert list(binary_partitions(3)) == [[2, 1], [1, 1, 1]]\n    assert list(binary_partitions(1)) == [[1]]\n    assert list(binary_partitions(0)) == [[]]\n", "def test_roundrobin():\n    from sympy.utilities.iterables import roundrobin\n    assert list(roundrobin('ABC', 'D', 'EF')) == ['A', 'D', 'E', 'B', 'F', 'C']\n    assert list(roundrobin([1, 2, 3], ['a', 'b'], (4, 5))) == [1, 'a', 4, 2, 'b', 5, 3]\n    assert list(roundrobin('ABC', [], 'DEF')) == ['A', 'D', 'B', 'E', 'C', 'F']\n    assert list(roundrobin([], [1, 2, 3])) == [1, 2, 3]\n    assert list(roundrobin([1, 2], [3, 4, 5])) == [1, 3, 2, 4, 5]\n", "def test_rotations():\n    assert list(rotations([1,2,3])) == [[1, 2, 3], [2, 3, 1], [3, 1, 2]]\n    assert list(rotations([1,2,3], -1)) == [[1, 2, 3], [3, 1, 2], [2, 3, 1]]\n    assert list(rotations([1])) == [[1]]\n    assert list(rotations([])) == []\n"], "sample_1133": ["def test_refraction_angle_with_ray():\n    n1, n2 = symbols('n1 n2')\n    r1 = Ray3D(Point3D(-1, -1, 1), Point3D(0, 0, 0))\n    P = Plane(Point3D(0, 0, 0), normal_vector=[0, 0, 1])\n    assert refraction_angle(r1, n1, n2, plane=P) == \\\n        Ray3D(Point3D(0, 0, 0), Point3D(n1/n2, n1/n2, -sqrt(3)*sqrt(-2*n1**2/(3*n2**2) + 1)))\n", "def test_refraction_angle_with_angle_of_incidence():\n    # Test with angle of incidence\n    angle_of_incidence = 0.3\n    n1, n2 = 1, 2\n    assert refraction_angle(angle_of_incidence, n1, n2).evalf(4) == 0.1492\n\n    # Test with total internal reflection\n    n1, n2 = 2, 1\n    angle_of_incidence = 0.9\n    with raises(ValueError):\n        refraction_angle(angle_of_incidence, n1, n2)\n", "def test_refraction_angle_with_plane():\n    n1, n2 = symbols('n1 n2')\n    P = Plane(Point3D(0, 0, 0), normal_vector=[0, 0, 1])\n    r1 = Ray3D(Point3D(-1, -1, 1), Point3D(0, 0, 0))\n    refracted_ray = refraction_angle(r1, n1, n2, plane=P)\n    assert isinstance(refracted_ray, Ray3D)\n    assert refracted_ray.direction_ratio == Matrix([n1/n2, n1/n2, -sqrt(3)*sqrt(-2*n1**2/(3*n2**2) + 1)])\n", "def test_refraction_angle_with_plane():\n    n1, n2 = symbols('n1 n2')\n    P = Plane(Point3D(0, 0, 0), normal_vector=[0, 0, 1])\n    r1 = Ray3D(Point3D(-1, -1, 1), Point3D(0, 0, 0))\n    assert refraction_angle(r1, n1, n2, plane=P) == Ray3D(Point3D(0, 0, 0), Point3D(n1/n2, n1/n2, -sqrt(3)*sqrt(-2*n1**2/(3*n2**2) + 1)))\n", "def test_deviation():\n    n1, n2 = symbols('n1 n2')\n    r1 = Ray3D(Point3D(-1, -1, 1), Point3D(0, 0, 0))\n    n = Matrix([0, 0, 1])\n    P = Plane(Point3D(0, 0, 0), normal_vector=[0, 0, 1])\n    assert deviation(r1, 1, 1, n) == 0\n    assert deviation(r1, n1, n2, plane=P) == -sqrt(-2*n1**2/(3*n2**2) + 1)/sqrt(3/3) + sqrt(3)/3\n    assert round(deviation(0.1, 1.2, 1.5), 5) == -0.02005\n"], "sample_1135": ["def test_mul_as_coefficients_dict():\n    x = Symbol('x')\n    assert Mul(2, x).as_coefficients_dict() == {x: 2}\n    assert Mul(x, 2).as_coefficients_dict() == {x: 2}\n    assert Mul(x, 2, evaluate=False).as_coefficients_dict() == {x: 2}\n    assert Mul(x, 2, 3).as_coefficients_dict() == {x: 6}\n    assert Mul(2, 3, x).as_coefficients_dict() == {x: 6}\n", "def test_Mul_new():\n    # make sure cache is not going to be used to create object\n    a = Symbol('a')\n    b = Symbol('b')\n    Mul._cache = {}\n    # the following should not raise an exception\n    Basic.__new__(Mul, b, a)\n", "def test_Mul():\n    assert Mul(x, 1) == x\n    assert Mul(x, -1) == -x\n    assert Mul(x, 0) == 0\n    assert Mul(x, x) == x**2\n    assert Mul(-x, x) == -x**2\n    assert Mul(x, y) == x*y\n    assert Mul(x, -y) == -x*y\n    assert Mul(x, x*y) == x**2*y\n    assert Mul(x, x**2*y) == x**3*y\n    assert Mul(x**2*y, x) == x**3*y\n    assert Mul(x**2*y, x**2) == x**4*y\n    assert Mul(x*y, x**2*y) == x**3*y**2\n    assert Mul(x*y, x**2) == x**3*y\n", "def test_Mul_eval_is_composite():\n    assert (2*x).is_composite is None\n    assert (2*3).is_composite is True\n    assert (2*x*y).is_composite is True\n    assert (2*3*4).is_composite is True\n    assert (x*y).is_composite is True\n    assert (2*x*y).is_composite is True\n    assert (2*3*x*y).is_composite is True\n", "def test_Mul_is_integer():\n    a = Symbol('a', integer=True)\n    b = Symbol('b', integer=True)\n    assert (2*a).is_integer\n    assert (a*b).is_integer\n    assert (a/2).is_integer is False\n    assert (2.0*a).is_integer is None  # Float on right makes it unknown\n    assert (a*b/2).is_integer is None\n    assert (2*a/2).is_integer is True\n    assert (a/2*2).is_integer is None  # cancel does not occur here\n    assert (a**2*sin(a)).is_integer is None\n    assert (a*b).is_integer\n    assert (-a).is_integer\n    assert (a*(-2)).is_integer\n    assert (a*(2*b)).is_integer\n\n    x = Symbol('x')\n    assert (2*x).is_integer is None\n    assert (2*x/3).is_integer is None\n"], "sample_1134": ["compilation error", "def test_latex_NDimArray():\n    from sympy.tensor.array import (ImmutableDenseNDimArray,\n                                    ImmutableSparseNDimArray,\n                                    MutableSparseNDimArray,\n                                    MutableDenseNDimArray,\n                                    tensorproduct)\n    M = ImmutableDenseNDimArray([[1, 2], [3, 4]])\n    assert latex(M) == r'\\begin{pmatrix}1 & 2\\\\3 & 4\\end{pmatrix}'\n\n    M = ImmutableSparseNDimArray([[1, 0], [0, 0]])\n    assert latex(M) == r'\\begin{pmatrix}1 & 0\\\\0 & 0\\end{pmatrix}'\n\n    M = MutableSparseNDimArray([[1, 0], [0, 0]])\n    assert latex(M) == r'\\begin{pmatrix}1 & 0\\\\0 & 0\\end{pmatrix}'\n\n    M = MutableDenseNDimArray([[1, 2], [3, 4]])\n    assert latex(M) == r'\\begin{pmatrix}1 & 2\\\\3 & 4\\end{pmatrix}'\n\n    M = ImmutableDenseNDimArray([1, 2])\n    assert latex(M) == r'\\begin{pmatrix}1\\\\2\\end{pmatrix}'\n\n    M = ImmutableSparseNDimArray([1, 0])\n    assert latex(M) == r'\\begin{pmatrix}1\\\\0\\end{pmatrix}'\n\n    M = tensorproduct([1, 2], [3, 4])\n    assert latex(M) == r'\\begin{pmatrix}3 & 4\\\\6 & 8\\end{pmatrix}'\n", "def test_LatexPrinter_settings():\n    # issue 6877\n    p = LatexPrinter({'ln_notation': True})\n    assert p._print(log(x)) == r\"\\ln{\\left(x \\right)}\"\n    p = LatexPrinter({'ln_notation': False})\n    assert p._print(log(x)) == r\"\\log{\\left(x \\right)}\"\n\n    p = LatexPrinter({'inv_trig_style': \"full\"})\n    assert p._print(asin(x)) == r\"\\arcsin{\\left(x \\right)}\"\n    p = LatexPrinter({'inv_trig_style': \"abbreviated\"})\n    assert p._print(asin(x)) == r\"\\operatorname{asin}{\\left(x \\right)}\"\n    p = LatexPrinter({'inv_trig_style': \"power\"})\n    assert p._print(asin(x)) == r\"\\sin^{-1}{\\left(x \\right)}\"\n\n    p = LatexPrinter({'mat_delim': \"(\"})\n    m = Matrix([[1, 2], [3, 4]])\n    assert p._print(m) == r\"\\left(\\begin{matrix}1 & 2\\\\3 & 4\\end{matrix}\\right)\"\n\n    p = LatexPrinter({'mul_symbol': \"times\"})\n    assert p._print(x * y) == r\"x \\times y\"\n\n    p = LatexPrinter({'imaginary_unit': \"i\"})\n    assert p._print(I) == r\"i\"\n    p = LatexPrinter({'imaginary_unit': \"j\"})\n    assert p._print(I) == r\"j\"\n    p = LatexPrinter({'imaginary_unit': \"ri\"})\n    assert p._print(I) == r\"\\mathrm{i}\"\n    p = LatexPrinter({'imaginary_unit': \"rj\"})\n    assert p._print(I) == r\"\\mathrm{j}\"\n    p = LatexPrinter({'imaginary_unit': \"ti\"})\n    assert p._print(I) == r\"\\text{i}\"\n    p = LatexPrinter({'imaginary_unit': \"tj\"})\n    assert p._print(I) == r\"\\text{j}\"\n    p = LatexPrinter({'imaginary_unit': \"j\"})\n    assert p._print(1 + I) == r\"1 + j\"\n\n    p = LatexPrinter({'decimal_separator': \"period\"})\n    assert p._print(Rational(3, 2)) == r\"\\frac{3}{2}\"\n", "def test_mellin_transform():\n    # transform\n    assert latex(MellinTransform(x**a, x, s)) == \\\n        r'\\int_{0}^{\\infty} x^{a} x^{s} \\, dx'\n    # function name\n    assert latex(MellinTransform(x**a, x, s, noconds=True)) == \\\n        r'\\mathcal{M}_{x}\\left(x^{a}\\right)\\left(s\\right)'\n    # function name with name\n    assert latex(MellinTransform(x**a, x, s, noconds=True), inv_trig_style='full', mat_str='matrix', mul_symbol='dot') == \\\n        r'\\mathcal{M}_{x}\\left(x^{a}\\right)\\left(s\\right)'\n", "def test_latex_ordered_set():\n    assert latex({1, 2, 3, 4}) == r'\\left\\{1, 2, 3, 4\\right\\}'\n    assert latex({1, 2, 3, 4, 'a'}) == r'\\left\\{1, 2, 3, 4, a\\right\\}'\n"], "sample_1136": ["def test_EX():\n    K = EX\n\n    assert K(1).is_positive\n    assert K(-1).is_negative\n    assert K(0).is_zero\n\n    assert K(1) + K(1) == K(2)\n    assert K(1) - K(1) == K(0)\n    assert K(1) * K(1) == K(1)\n    assert K(1) / K(1) == K(1)\n    assert K(1) ** K(1) == K(1)\n\n    assert K(1).gcd(K(1)) == K(1)\n    assert K(1).lcm(K(1)) == K(1)\n\n    assert K(1).numer() == K(1)\n    assert K(1).denom() == K(1)\n\n    assert K.is_Field\n    assert K.is_CharacteristicZero\n\n    assert K.get_field() == K\n    assert K.get_ring() == K\n\n    assert K.to_sympy(K(1)) == 1\n    assert K.from_sympy(1) == K(1)\n\n    assert K.from_ZZ_python(1, ZZ) == K(1)\n    assert K.from_QQ_python(1, QQ) == K(1)\n    assert K.from_ZZ_gmpy(1, ZZ) == K(1)\n    assert K.from_QQ_gmpy(1, QQ) == K(1)\n    assert K.from_GaussianIntegerRing(1, ZZ_I) == K(1)\n    assert K.from_GaussianRationalField(1, QQ_I) == K(1)\n    assert K.from_RealField(1, RR) == K(1)\n    assert K.from_PolynomialRing(1, ZZ[x]) == K(1)\n    assert K.from_FractionField(1, QQ[x]) == K(1)\n    assert K.from_ExpressionDomain(1, K) == K(1)\n", "def test_ExpressionDomain__from_sympy():\n    K = EX\n    a = K.from_sympy(x + 1)\n    assert a.ex == x + 1\n    b = K.from_sympy(a)\n    assert a == b\n", "def test_ExpressionDomain():\n    domain = EX\n\n    # Test conversion methods\n    assert domain.to_sympy(domain(1)) == 1\n    assert domain.from_sympy(1) == domain(1)\n    assert domain.from_ZZ_python(1, ZZ) == domain(1)\n    assert domain.from_QQ_python(Rational(1, 2), QQ) == domain(Rational(1, 2))\n    assert domain.from_ZZ_gmpy(1, ZZ) == domain(1)\n    assert domain.from_QQ_gmpy(Rational(1, 2), QQ) == domain(Rational(1, 2))\n    assert domain.from_GaussianIntegerRing(1, ZZ_I) == domain(1)\n    assert domain.from_GaussianRationalField(Rational(1, 2), QQ_I) == domain(Rational(1, 2))\n    assert domain.from_RealField(1.0, RR) == domain(1.0)\n    assert domain.from_PolynomialRing(poly(x), ZZ[x]) == domain(x)\n    assert domain.from_FractionField(x/y, ZZ.frac_field(x, y)) == domain(x/y)\n    assert domain.from_ExpressionDomain(domain(1), EX) == domain(1)\n\n    # Test algebraic operations\n    assert domain(1) + domain(2) == domain(3)\n    assert domain(1) - domain(2) == domain(-1)\n    assert domain(1) * domain(2) == domain(2)\n    assert domain(1) / domain(2) == domain(Rational(1, 2))\n    assert domain(1)**2 == domain(1)\n    assert abs(domain(-1)) == domain(1)\n\n    # Test comparison methods\n    assert domain(1) == domain(1)\n    assert domain(1) != domain(2)\n    assert domain(1) < domain(2)\n    assert domain(1) <= domain(2)\n    assert domain(2) > domain(1)\n    assert domain(2) >= domain(1)\n\n    # Test gcd and lcm methods\n    assert domain.gcd(domain(2), domain(4)) == domain(2)\n    assert domain.lcm(domain(2), domain(4)) == domain(4)\n", "def test_ExpressionDomain__from_sympy():\n    K = EX\n    assert K.from_sympy(1) == K(1)\n    assert K.from_sympy(2) == K(2)\n    assert K.from_sympy(x) == K(x)\n    assert K.from_sympy(x**2 + 2*x + 1) == K(x**2 + 2*x + 1)\n    assert K.from_sympy(S.Half) == K(S.Half)\n", "def test_ExpressionDomain():\n    domain = EX\n    a = domain(2)\n    b = domain(3)\n\n    assert domain.is_positive(a)\n    assert domain.is_positive(b)\n    assert domain.is_nonnegative(a)\n    assert domain.is_nonnegative(b)\n\n    assert not domain.is_negative(a)\n    assert not domain.is_negative(b)\n    assert not domain.is_nonpositive(a)\n    assert not domain.is_nonpositive(b)\n\n    assert domain.numer(a) == a\n    assert domain.numer(b) == b\n    assert domain.denom(a) == domain(1)\n    assert domain.denom(b) == domain(1)\n\n    assert domain.gcd(a, b) == domain(1)\n    assert domain.lcm(a, b) == a * b\n"], "sample_1137": ["def test_convert_to_Planck_units():\n    from sympy.physics.units import hbar, gravitational_constant, speed_of_light\n    from sympy.physics.units import atomic_mass_constant\n    from sympy.physics.units import convert_to\n\n    # Test conversion to Planck units\n    assert convert_to(atomic_mass_constant, [gravitational_constant, speed_of_light, hbar]).n() == \\\n        7.62963085040767e-20*gravitational_constant**(-0.5)*hbar**0.5*speed_of_light**0.5\n", "def test_convert_to_multiple_units():\n    # Test with multiple target units\n    expr = speed_of_light\n    target_units = [meter, second]\n    result = convert_to(expr, target_units)\n    assert result == 299792458*meter/second\n\n    expr = 3*newton\n    target_units = [centimeter, gram, second]\n    result = convert_to(expr, target_units)\n    assert result == 300000*centimeter*gram/second**2\n\n    # Test with Planck units\n    expr = atomic_mass_constant\n    target_units = [gravitational_constant, speed_of_light, hbar]\n    result = convert_to(expr, target_units).n()\n    assert result == 7.62963085040767e-20*gravitational_constant**(-0.5)*hbar**0.5*speed_of_light**0.5\n", "def test_quantity_simplify_with_prefixes():\n    from sympy.physics.units import kilometer, kilogram\n    from sympy.physics.units.prefixes import kilo\n    assert quantity_simplify(kilo*kilometer*kilogram) == 1000*kilogram*kilometer\n    assert quantity_simplify(kilo*kilometer) == 1000*meter\n    assert quantity_simplify(kilo*kilogram) == 1000*gram\n", "def test_convert_to():\n    # Test conversion to multiple units\n    assert convert_to(speed_of_light, [meter, second]) == 299792458*meter/second\n    assert convert_to(3*newton, [centimeter, gram, second]) == 300000*centimeter*gram/second**2\n\n    # Test conversion to Planck units\n    assert convert_to(atomic_mass_constant, [gravitational_constant, speed_of_light, hbar]).n() == 7.62963085040767e-20*gravitational_constant**(-0.5)*hbar**0.5*speed_of_light**0.5\n\n    # Test conversion of Add instances\n    expr = 2*newton + 3*newton\n    assert convert_to(expr, kilogram*meter/second**2) == 5*kilogram*meter/second**2\n\n    # Test conversion of Mul instances with multiple units\n    expr = 2*newton * 3*meter\n    assert convert_to(expr, kilogram*meter**2/second**2) == 6*kilogram*meter**2/second**2\n\n    # Test conversion to a unit system\n    expr = 2*newton\n    assert convert_to(expr, \"SI\") == 2*kilogram*meter/second**2\n", "def test_convert_to():\n    assert convert_to(kilometer, meter) == 1000 * meter\n    assert convert_to(meter, kilometer) == kilometer / 1000\n    assert convert_to(speed_of_light, meter / second) == 299792458 * meter / second\n    assert convert_to(3 * joule, kilogram * meter**2 / second**2) == 3 * kilogram * meter**2 / second**2\n\n    # test conversion to multiple units\n    assert convert_to(speed_of_light, [meter, second]) == 299792458 * meter / second\n    assert convert_to(3 * joule, [kilogram, meter, second]) == 3 * kilogram * meter**2 / second**2\n\n    # test conversion to Planck units\n    assert convert_to(kilogram, [gravitational_constant, speed_of_light, kilogram]).n() == kilogram\n"], "sample_1138": ["def test_process_common_addends():\n    # no common addends\n    eq = sin(x) + cos(x)\n    assert process_common_addends(eq, TR8) == eq\n    # common coefficients\n    eq = sin(x) + sin(y) + 2*sin(x) + 2*sin(y)\n    assert process_common_addends(eq, TR8) == 3*sin(x) + 3*sin(y)\n    # common symbols\n    eq = sin(x) + sin(x)*cos(y) + sin(x)*sin(y)\n    assert process_common_addends(eq, TR8) == sin(x) + sin(x)*(cos(y) + sin(y))\n    # common coefficients and symbols\n    eq = sin(x) + sin(x)*cos(y) + 2*sin(x) + 2*sin(x)*cos(y)\n    assert process_common_addends(eq, TR8) == 3*sin(x) + 3*sin(x)*cos(y)\n", "def test_process_common_addends():\n    x, y = symbols('x y')\n    assert process_common_addends(\n        sin(x) + sin(x + y), TR8) == sin(x) + sin(x + y)\n    assert process_common_addends(\n        sin(x) + sin(y), TR8) == sin(x) + sin(y)\n    assert process_common_addends(\n        sin(x) + sin(x), TR8) == 2*sin(x)\n    assert process_common_addends(\n        sin(x) + cos(x), TR8) == sin(x) + cos(x)\n", "def test_TR8():\n    assert TR8(cos(x) * cos(y)) == cos(x - y)/2 + cos(x + y)/2\n    assert TR8(cos(x) * sin(y)) == sin(x + y)/2 + sin(x - y)/2\n    assert TR8(sin(x) * sin(y)) == -cos(x + y)/2 + cos(x - y)/2\n\n    assert TR8(cos(x) * cos(x)) == cos(2*x)/2 + S.Half\n    assert TR8(cos(x) * sin(x)) == sin(2*x)/2\n    assert TR8(sin(x) * sin(x)) == -cos(2*x)/2 + S.Half\n\n    assert TR8(cos(x) * cos(x) * cos(y)) == \\\n        4*cos(x - y)*cos(x + y)*cos(x)/2 + cos(x)*cos(y)/2\n", "def test_TR11():\n    assert TR11(sin(x)) == sin(x)\n    assert TR11(sin(2*x)) == 2*sin(x)*cos(x)\n    assert TR11(sin(2*x), x) == 2*sin(x)*cos(x)\n    assert TR11(sin(3*x)) == sin(3*x)\n    assert TR11(sin(4*x), x) == -4*sin(x)*cos(x)**3 + 4*sin(x)**3*cos(x)\n    assert TR11(cos(x)) == cos(x)\n    assert TR11(cos(2*x)) == -sin(x)**2 + cos(x)**2\n    assert TR11(cos(2*x), x) == -sin(x)**2 + cos(x)**2\n    assert TR11(cos(3*x)) == cos(3*x)\n    assert TR11(cos(4*x), x) == cos(x)**4 - 6*sin(x)**2*cos(x)**2 + sin(x)**4\n", "def test_TR2i():\n    assert TR2i((sin(x)/cos(x))) == tan(x)\n    assert TR2i(sin(x)**2/(cos(x) + 1)**2, half=True) == tan(x/2)**2\n    assert TR2i(sin(x)**6/(cos(x) + 1)**6, half=True) == tan(x/2)**6\n    assert TR2i(sin(x)**8/(cos(x) + 1)**8, half=True) == tan(x/2)**8\n    assert TR2i(sin(x)**6/(cos(x) + 1)**6, half=False) == sin(x)**6/(cos(x) + 1)**6\n"], "sample_1139": ["def test_range_len():\n    assert len(Range(5)) == 5\n    assert len(Range(1, 4)) == 3\n    assert len(Range(1, 6, 2)) == 3\n    raises(ValueError, lambda: len(Range(oo)))\n", "def test_image_set_iter():\n    x = Symbol('x')\n    f = Lambda(x, x**2)\n    assert list(ImageSet(f, Range(3))) == [0, 1, 4]\n    assert list(ImageSet(f, Interval(1, 5))) == [1, 4, 9, 16, 25]\n    assert list(ImageSet(f, FiniteSet(1, 2, 3, 4, 5))) == [1, 4, 9, 16, 25]\n    sets = [FiniteSet(1, 2), FiniteSet(3, 4)]\n    assert list(ImageSet(f, Union(*sets))) == [1, 4, 9, 16]\n    assert list(ImageSet(f, Intersection(*sets))) == []\n    assert list(ImageSet(f, ProductSet(*sets))) == [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n", "def test_ComplexRegion_contains():\n    a, b = Interval(1, 3), Interval(2, 4)\n    theta = Interval(0, pi)\n    theta_norm = normalize_theta_set(theta)\n    c1 = ComplexRegion(a*b)\n    c2 = ComplexRegion(a*theta, polar=True)\n    assert 2 + 3*I in c1\n    assert 1 + 1*I not in c1\n    assert 2*(cos(pi/2) + I*sin(pi/2)) in c2\n    assert 2*(cos(3*pi/2) + I*sin(3*pi/2)) not in c2\n    assert 2*(cos(5*pi/2) + I*sin(5*pi/2)) in c2\n    assert c1.contains(2 + 3*I)\n    assert not c1.contains(1 + 1*I)\n    assert c2.contains(2*(cos(pi/2) + I*sin(pi/2)))\n    assert not c2.contains(2*(cos(3*pi/2) + I*sin(3*pi/2)))\n    assert c2.contains(2*(cos(5*pi/2) + I*sin(5*pi/2)))\n    assert 2 + 3*I in ComplexRegion(a*b, polar=False)\n    assert 2 + pi*I in ComplexRegion(a*theta_norm, polar=True)\n", "def test_ComplexRegion_intersect():\n    # Rectangular form\n    a = Interval(2, 5)\n    b = Interval(3, 4)\n    c = Interval(1, 3)\n    d = Interval(2, 7)\n    C1 = ComplexRegion(a*b)\n    C2 = ComplexRegion(c*d)\n    assert C1.intersect(C2) == ComplexRegion((a&c)*(b&d))\n\n    # Polar form\n    r1 = Interval(0, 5)\n    r2 = Interval(0, 4)\n    theta1 = Interval(0, pi)\n    theta2 = Interval(0, 3*pi/4)\n    C1 = ComplexRegion(r1*theta1, polar=True)\n    C2 = ComplexRegion(r2*theta2, polar=True)\n    assert C1.intersect(C2) == ComplexRegion((r1&r2)*(theta1&theta2), polar=True)\n", "def test_ComplexRegion_is_subset():\n    a = Interval(2, 4)\n    b = Interval(3, 5)\n    c = Interval(0, 2*pi)\n    C1 = ComplexRegion(a*b)\n    C2 = ComplexRegion(a*b)\n    C3 = ComplexRegion(a*c, polar=True)\n    C4 = ComplexRegion(b*c, polar=True)\n    assert C1.is_subset(C1)\n    assert C1.is_subset(C2)\n    assert not C1.is_subset(C3)\n    assert not C1.is_subset(C4)\n    assert not C3.is_subset(C1)\n    assert not C3.is_subset(C2)\n    assert C3.is_subset(C3)\n    assert not C3.is_subset(C4)\n    assert not C4.is_subset(C1)\n    assert not C4.is_subset(C2)\n    assert not C4.is_subset(C3)\n    assert C4.is_subset(C4)\n"], "sample_1141": ["def test_ExprBuilder():\n    from sympy.core.expr import ExprBuilder\n\n        if a > b:\n            return True\n        return False\n\n    builder = ExprBuilder(lambda a, b: a + b, validator=validator)\n\n    with raises(AssertionError):\n        builder.build()\n\n    builder.append_argument(2)\n    with raises(AssertionError):\n        builder.build()\n\n    builder.append_argument(1)\n    assert builder.build() == 3\n\n    builder = ExprBuilder(lambda a, b: a + b, validator=None)\n    builder.append_argument(2)\n    builder.append_argument(1)\n    assert builder.build() == 3\n\n        if a > b:\n            return True\n        return False\n\n    builder = ExprBuilder(lambda a, b: a + b, validator=None)\n    builder.append_argument(2)\n    builder.append_argument(1)\n    builder.validator = validator\n    with raises(AssertionError):\n        builder.build(check=True)\n\n    assert builder.build(check=False) == 3\n", "def test_pow():\n    x = Symbol('x')\n    y = Symbol('y')\n\n    assert str(Pow(x, 0)) == '1'\n    assert str(Pow(x, 0, evaluate=False)) == 'x**0'\n    assert str(Pow(x, 1)) == 'x'\n    assert str(Pow(x, 1, evaluate=False)) == 'x'\n    assert str(Pow(x, -1)) == '1/x'\n    assert str(Pow(x, -1, evaluate=False)) == 'x**-1'\n    assert str(Pow(x, -2)) == '1/x**2'\n    assert str(Pow(x, -2, evaluate=False)) == 'x**-2'\n\n    assert str(Pow(x, 2, evaluate=False)) == 'x**2'\n    assert Pow(x, 2)**3 == x**6\n    assert Pow(Pow(x, 3), 2) == x**6\n    assert Pow(x, -2)**-3 == x**6\n    assert Pow(x, 2)**-3 == 1/(x**6)\n    assert (1/x**-3) == x**3\n\n    # Test with composite numbers\n    assert Pow(x, 2) * Pow(y, 4) != Pow(x*y, 6)\n    assert Pow(x, 2) ** (3*y) == Pow(x, 6*y)\n    assert Pow(x, 2) ** Pow(y, 3) == Pow(x, 2*y**3)\n    assert Pow(x, 3) ** Pow(y, 2) == Pow(x, 3*y**2)\n\n    # Test with I\n    assert (I**2) == -1\n    assert (I**4) == 1\n    assert (I**-1) == -I\n    assert (I**-2) == -1\n    assert (I**-4) == 1\n\n    assert str(Pow(I, 0, evaluate=False)) == 'I**0'\n    assert str(Pow(I, 1, evaluate=False)) == 'I'\n    assert str(Pow(I, 2, evaluate=False)) == 'I**2'\n    assert str(Pow(I, 3, evaluate=False)) == 'I**3'\n    assert str(Pow(I, 4, evaluate=False)) == 'I**4", "def test_UnevaluatedExpr():\n    from sympy.core.expr import UnevaluatedExpr\n    x = Symbol('x')\n    expr = UnevaluatedExpr(x + x)\n    assert expr.doit() == 2*x\n    assert expr.doit(deep=False) == x + x\n    assert expr.args[0] == x + x\n", "def test_is_meromorphic():\n    x = Symbol('x')\n    y = Symbol('y')\n    f1 = 1/x + 1/(x - 1)\n    f2 = 1/sin(x) + 1/(x - 1)\n    assert f1.is_meromorphic(x, 0) is True\n    assert f1.is_meromorphic(x, 1) is True\n    assert f2.is_meromorphic(x, 0) is True\n    assert f2.is_meromorphic(x, pi) is False\n    assert (1/y).is_meromorphic(x, 0) is True\n", "def test_ExprBuilder():\n    from sympy.core.expr import ExprBuilder\n\n    x = Symbol(\"x\")\n    y = Symbol(\"y\")\n\n    # Create an ExprBuilder for x + y\n    builder = ExprBuilder(Add, [x, y])\n\n    # Append another argument to the builder\n    builder.append_argument(x)\n\n    # Build the expression\n    expr = builder.build()\n\n    # Check that the expression is correct\n    assert expr == x + y + x\n"], "sample_1140": ["def testprettyPolyElement():\n    R, x, y = QQ.poly_ring('x y')\n    assert upretty(R(3)*x**2) == '3x**2'\n    assert pretty(R(3)*x**2) == '3*x**2'\n    assert upretty(R(1)/3*x**2) == '1/3x**2'\n    assert pretty(R(1)/3*x**2) == 'x**2/3'\n    assert upretty(R(1,3)*x**2+R(1,2)*y) == '1/3x**2 + 1/2y'\n    assert pretty(R(1,3)*x**2+R(1,2)*y) == 'x**2/3 + y/2'\n    assert upretty(R(1, 3)*x**2+R(1, 2)*y, order='lex') == '1/3x**2 + 1/2y'\n    assert upretty(R(1, 3)*x**2+R(1, 2)*y, order='revlex') == '1/2y + 1/3x**2'\n    assert upretty(R(1, 3)*x**2+R(1, 2)*y, order='ilex') == '1/2y + 1/3x**2'\n    assert upretty(R(1, 3)*x**2+R(1, 2)*y, order='grevlex') == '1/2y + 1/3x**2'\n", "def test_pretty_printing_abstract_algebra():\n    from sympy.abc import x\n    from sympy.polys.domains import ZZ, QQ, RR, CC\n    from sympy.polys.fields import FracElement\n    from sympy.polys.rings import PolyElement\n\n    assert pretty(PolyElement(x**2 + 2*x + 1, ZZ[x])) == \"x**2 + 2*x + 1\"\n    assert pretty(PolyElement(x**2 + 2*x + 1, QQ[x])) == \"x**2 + 2*x + 1\"\n    assert pretty(PolyElement(x**2 + 2*x + 1, RR[x])) == \"x**2 + 2*x + 1\"\n    assert pretty(PolyElement(x**2 + 2*x + 1, CC[x])) == \"x**2 + 2*x + 1\"\n\n    assert pretty(FracElement(x**2 + 2*x + 1, ZZ[x])) == \"x**2 + 2*x + 1\"\n    assert pretty(FracElement(x**2 + 2*x + 1, QQ[x])) == \"x**2 + 2*x + 1\"\n    assert pretty(FracElement(x**2 + 2*x + 1, RR[x])) == \"x**2 + 2*x + 1\"\n    assert pretty(FracElement(x**2 + 2*x + 1, CC[x])) == \"x**2 + 2*x + 1\"\n", "def test_pretty_mod():\n    assert pretty(Mod(x, y)) == \"(\" + pretty(x) + \" mod \" + pretty(y) + \")\"\n    assert upretty(Mod(x, y)) == \"(\" + upretty(x) + \" mod \" + upretty(y) + \")\"\n", "def test_pretty_mod():\n    assert pretty(Mod(x, y)) == pretty(x) + ' mod ' + pretty(y)\n    assert upretty(Mod(x, y)) == upretty(x) + ' mod ' + upretty(y)\n", "def test_pretty_number_function():\n    assert pretty(euler(2)) == 'E_2'\n    assert pretty(catalan(3)) == 'C_3'\n    assert pretty(bernoulli(5)) == 'B_5'\n    assert pretty(bell(6)) == 'B_6'\n    assert pretty(lucas(7)) == 'L_7'\n    assert pretty(fibonacci(8)) == 'F_8'\n    assert pretty(tribonacci(9)) == 'T_9'\n    assert pretty(stieltjes(10)) == 'gamma_10'\n"], "sample_1142": ["def test_matrix_element():\n    i, j = symbols('i j')\n    x = MatrixSymbol('x', 2, 3)\n    expr = MatrixElement(x, i, j)\n    assert expr.parent == x\n    assert expr.i == i\n    assert expr.j == j\n    assert expr.indices == (i, j)\n    expr = MatrixElement(x, 0, 0)\n    assert expr.doit() == x[0, 0]\n", "def test_matrix_element():\n    i, j = symbols('i j')\n    A = MatrixSymbol('A', 2, 2)\n    assert MatrixElement(A, 0, 0) == A[0, 0]\n    assert MatrixElement(A, i, j).subs({i: 0, j: 1}) == A[0, 1]\n    assert MatrixElement(A, i, j).subs({i: 1, j: 0}) == A[1, 0]\n    assert MatrixElement(A, i, j).subs({i: 1, j: 1}) == A[1, 1]\n    raises(TypeError, lambda: MatrixElement(x, i, j))\n", "def test_matrix_element():\n    i, j = symbols('i j', integer=True)\n    A = MatrixSymbol('A', 2, 3)\n    assert MatrixElement(A, i, j).subs({i: 0, j: 0}) == A[0, 0]\n    assert MatrixElement(A, i, j).subs({i: 1, j: 1}) == A[1, 1]\n    raises(TypeError, lambda: MatrixElement(1, 0, 0))\n", "def test_matrix_element_is_symbol():\n    i, j = symbols('i j')\n    A = MatrixSymbol('A', 2, 2)\n    me = MatrixElement(A, i, j)\n    assert me.is_symbol\n    assert me.is_commutative\n", "def test_matrix_element():\n    i, j = symbols('i j')\n    assert MatrixElement(A, i, j).parent == A\n    assert MatrixElement(A, 0, 0).parent == A\n    assert MatrixElement(A, i, j).i == i\n    assert MatrixElement(A, i, j).j == j\n    assert MatrixElement(A, i, j).indices == (i, j)\n    raises(TypeError, lambda: MatrixElement(1, i, j))\n"], "sample_1143": ["def test_comp():\n    assert comp(Float('1.0'), Float('1.0', 2)) is True\n    assert comp(Float('1.0'), Float('1.1', 2)) is True\n    assert comp(Float('1.0'), Float('1.1', 5)) is False\n    assert comp(Float('1.0'), 1) is True\n    assert comp(Float('1.0'), 1.1) is False\n    assert comp(Float('1.0'), '1.0') is True\n    assert comp(Float('1.0'), '1.1') is False\n    assert comp(Float('1.0'), '1') is True\n    assert comp(Float('1.0'), '1.1', '') is False\n    assert comp(Float('123456789012345', 15), '123456789012345', '') is True\n    assert comp(Float('1234567890123456', 16), '1234567890123456', '') is True\n", "def test_AlgebraicNumber():\n    a = AlgebraicNumber((x**2 + 1, sqrt(2)), [1, 0])\n    assert a.rep.all_coeffs() == [1, 0]\n    assert a.root == sqrt(2)\n    assert a.minpoly == x**2 + 1\n    assert a.is_commutative\n    assert a.is_finite\n    assert a.is_number\n    assert a.is_AlgebraicNumber\n\n    assert same_and_same_prec(a._eval_evalf(10), Float(sqrt(2).evalf(10)))\n    assert same_and_same_prec(a._eval_evalf(20), Float(sqrt(2).evalf(20)))\n\n    assert a.as_expr() == sqrt(2)\n\n    assert a.coeffs() == [1, 0]\n    assert a.native_coeffs() == [1, 0]\n\n    b = AlgebraicNumber((x**2 + 1, sqrt(2)), [1, 0])\n    assert a == b\n    assert a != 0\n    assert a != S(1)\n\n    assert a.is_aliased is False\n\n    assert a.as_poly() == Poly(sqrt(2))\n    assert a.as_poly(x) == Poly(sqrt(2), x)\n\n    assert a.to_algebraic_integer() == AlgebraicNumber((x**2 + 1, sqrt(2)), [1, 0])\n", "def test_half():\n    h = S.Half\n    assert h.is_Rational\n    assert h.is_Integer is False\n    assert h.is_positive\n    assert 3*h == Rational(3, 2)\n    assert h**2 == Rational(1, 4)\n    assert h > 0\n    assert h < 1\n    assert h.is_integer is False\n", "def test_comp():\n    assert comp(1.1, 1) is False\n    assert comp(1.1, 1.1) is True\n    assert comp(1.0, 1) is True\n    assert comp(.99, 1) is False\n    assert comp(10000000000 + 0.1, 10000000000) is False\n    assert comp(10000000000 + 0.1, 10000000000, tol=0.1) is True\n    assert comp(1e-4, 0) is False\n    assert comp(1e-30, 0) is True\n    assert comp(1e-30, 0, tol='') is False\n\n    assert comp(t, t) is True\n    assert comp(1e-1000, 0, tol='') is False\n    assert comp(1e-1000, 1e-1000) is True\n    assert comp(1e-1000, 1e-1000, tol='') is True\n    assert comp(1.0, 1.0, tol='') is True\n    assert comp(1.1, 1.1, tol='') is False\n", "def test_comp():\n    assert comp(Float(.1, 1), 0.1) is False\n    assert comp(Float(.1, 1), 1) is False\n    assert comp(Float(.1, 1), 0.14) is False\n    assert comp(Float(.1, 1), 0.16) is False\n    assert comp(Float(.1, 1), 0.10001) is False\n    assert comp(Float(.1, 1), 0.105) is False\n    assert comp(Float(.1, 1), 0.11) is False\n    assert comp(Float(.1, 1), 0.09999) is False\n    assert comp(Float(.1, 1), 0.094) is False\n    assert comp(Float(.1, 1), 0.09) is False\n    assert comp(Float(.1, 1), 0.09, '') is False\n    assert comp(Float(.1, 1), 0.11, '') is False\n    assert comp(Float(.1, 1), 0.1, '') is True\n    assert comp(1e-1, .1, '') is True\n    assert comp(0.1, Float(.1, 1)) is False\n    assert comp(Float(.1, 3), Float(.1, 1)) is False\n    assert comp(Float(.1, 1), Float(.1, 4)) is False\n    assert comp(Float(.1, 3), Float(.1, 3)) is True\n    assert comp(0.1, .1) is True\n    assert comp(Float(.1, 3), .1) is True\n    assert comp(Float(.1, 3), .1, '') is False\n    assert comp(Float(.1, 3), .101, '') is False\n    assert comp(Float(.1, 3), .099, '') is False\n    assert comp(Float(.1, 3), .100, '') is True\n    assert comp(Float(.1, 3), .10002, '') is False\n    assert comp(Float(.1, 3), .09998, '') is False\n    assert comp(Float(.1, 3), .09999) is True\n    assert comp(.09999, Float(.1, 3)) is False\n    assert comp(sqrt(2"], "sample_1145": ["def test_refine_re():\n    # real\n    assert refine(re(x), Q.real(x)) == x\n    assert refine(re(x), Q.imaginary(x)) == 0\n    assert refine(re(x), True) == re(x)\n\n    # issue #17246\n    assert refine(re(x*y), Q.real(x) & Q.real(y)) == x*y\n", "def test_refine_matrixelement():\n    X = MatrixSymbol('X', 3, 3)\n    assert refine(X[0, 1], Q.symmetric(X)) == X[0, 1]\n    assert refine(X[1, 0], Q.symmetric(X)) == X[0, 1]\n    assert refine(X[1, 2], Q.symmetric(X)) == X[1, 2]\n    assert refine(X[2, 1], Q.symmetric(X)) == X[1, 2]\n", "def test_refine_atan2():\n    assert refine(atan2(y, x), Q.real(y) & Q.positive(x)) == atan(y/x)\n    assert refine(atan2(y, x), Q.negative(y) & Q.negative(x)) == atan(y/x) - pi\n    assert refine(atan2(y, x), Q.positive(y) & Q.negative(x)) == atan(y/x) + pi\n    assert refine(atan2(y, x), Q.zero(y) & Q.negative(x)) == pi\n    assert refine(atan2(y, x), Q.positive(y) & Q.zero(x)) == pi/2\n    assert refine(atan2(y, x), Q.negative(y) & Q.zero(x)) == -pi/2\n    assert refine(atan2(y, x), Q.zero(y) & Q.zero(x)) == nan\n", "def test_refine_abs():\n    assert refine(Abs(x), Q.real(x)) == Abs(x)\n    assert refine(Abs(x), Q.positive(x)) == x\n    assert refine(Abs(x), Q.negative(x)) == -x\n    assert refine(Abs(x*y), Q.positive(x) & Q.negative(y)) == -x*y\n", "def test_refine_Abs():\n    assert refine(Abs(x), Q.real(x)) == Abs(x)\n    assert refine(Abs(x), Q.positive(x)) == x\n    assert refine(Abs(x), Q.negative(x)) == -x\n    assert refine(Abs(x*y), Q.positive(x)) == Abs(y)*x\n"], "sample_1144": ["def test_split_super_sub():\n    assert split_super_sub('') == ('', [], [])\n    assert split_super_sub('a') == ('a', [], [])\n    assert split_super_sub('a1') == ('a', [], ['1'])\n    assert split_super_sub('a_x') == ('a', [], ['x'])\n    assert split_super_sub('a_x^1') == ('a', ['1'], ['x'])\n    assert split_super_sub('a__1') == ('a', ['1'], [])\n    assert split_super_sub('var_sub1__sup_sub2') == ('var', ['sup'], ['sub1', 'sub2'])\n    assert split_super_sub('a_123') == ('a', [], ['123'])\n    assert split_super_sub('a123') == ('a', [], ['123'])\n", "def test_split_super_sub():\n    assert split_super_sub('') == ('', [], [])\n    assert split_super_sub('a') == ('a', [], [])\n    assert split_super_sub('a1') == ('a', [], ['1'])\n    assert split_super_sub('a_1') == ('a', [], ['1'])\n    assert split_super_sub('a__1') == ('a', ['1'], [])\n    assert split_super_sub('a^1') == ('a', ['1'], [])\n    assert split_super_sub('a_1^2') == ('a', ['2'], ['1'])\n    assert split_super_sub('a__1^2') == ('a', ['1', '2'], [])\n", "def test_split_super_sub():\n    # normal cases\n    assert split_super_sub('a_x^1') == ('a', ['1'], ['x'])\n    assert split_super_sub('var_sub1__sup_sub2') == ('var', ['sup'], ['sub1', 'sub2'])\n    # edge cases\n    assert split_super_sub('') == ('', [], [])\n    assert split_super_sub('var') == ('var', [], [])\n    assert split_super_sub('var_') == ('var', [], [''])\n    assert split_super_sub('var^') == ('var', [''], [])\n    assert split_super_sub('var__') == ('var', [''], [])\n    # names ending with digits\n    assert split_super_sub('var1') == ('var', [], ['1'])\n    assert split_super_sub('var12') == ('var', [], ['12'])\n", "def test_requires_partial():\n    x, y = symbols('x y')\n    f = Function('f')\n    assert requires_partial(Derivative(f(x, y), x)) == True\n    assert requires_partial(Derivative(f(x), x)) == False\n    assert requires_partial(Derivative(f(x), (x, 2))) == False\n    assert requires_partial(Derivative(f(x, y), (x, 2), y)) == True\n", "def test_split_super_sub():\n    assert split_super_sub('') == ('', [], [])\n    assert split_super_sub('a') == ('a', [], [])\n    assert split_super_sub('a1') == ('a', [], ['1'])\n    assert split_super_sub('a_x') == ('a', [], ['x'])\n    assert split_super_sub('a^1') == ('a', ['1'], [])\n    assert split_super_sub('a_x^1') == ('a', ['1'], ['x'])\n    assert split_super_sub('var_sub1__sup_sub2') == ('var', ['sup'], ['sub1', 'sub2'])\n    assert split_super_sub('a__1') == ('a', ['1'], [])\n    assert split_super_sub('a_1__2') == ('a', ['2'], ['1'])\n"], "sample_1148": ["def test_matrix_element():\n    i, j = symbols('i j', integer=True)\n    assert MatrixElement(A, i, j).parent == A\n    assert MatrixElement(A, i, j).i == i\n    assert MatrixElement(A, i, j).j == j\n    raises(SympifyError, lambda: MatrixElement(1, i, j))\n    raises(SympifyError, lambda: MatrixElement(A, i, 2*j))\n    raises(SympifyError, lambda: MatrixElement(A, 2*i, j))\n", "def test_applyfunc():\n    A = MatrixSymbol('A', 2, 2)\n    f = lambda x: x**2\n    A.applyfunc(f)\n    assert A.applyfunc(f).doit().applyfunc(sqrt) == A\n", "def test_matrix_derivative():\n    from sympy.tensor.array.array_derivatives import ArrayDerivative\n    x = MatrixSymbol('x', 2, 1)\n    A = MatrixSymbol('A', 2, 2)\n    expr = A * x\n    result = diff(expr, x)\n    assert result.equals(A)\n", "def test_matrix_expr_as_coeff_Mul():\n    assert A.as_coeff_Mul()[0] == 1\n    assert A.as_coeff_Mul()[1] == A\n", "def test_matrix_element():\n    i, j = symbols('i j')\n    A = MatrixSymbol('A', 2, 2)\n    assert MatrixElement(A, 0, 0).doit() == A[0, 0]\n    assert MatrixElement(A, i, j).doit() == A[i, j]\n\n    assert MatrixElement(A, i, j).diff(A[0, 0]) == KroneckerDelta(0, i)*KroneckerDelta(0, j)\n    assert MatrixElement(A, i, j).diff(A[1, 1]) == KroneckerDelta(1, i)*KroneckerDelta(1, j)\n"], "sample_1149": ["def test_singleton_registry():\n    assert S.__call__ == sympify\n    assert S(1) == 1\n    assert S(1) is S(1)\n    assert isinstance(S(1), Basic)\n    assert isinstance(S(1)/2, Rational)\n    assert (S(1)/2) == Rational(1, 2)\n", "def test_singleton_registry():\n    # Test that S is a singleton registry\n    assert S is S\n    assert isinstance(S, SingletonRegistry)\n\n    # Test that accessing an attribute on S that has not been registered raises an AttributeError\n    with pytest.raises(AttributeError):\n        S.NonExistentAttribute\n\n    # Test that registering a class with S registers it correctly\n    class MySingleton(Basic, metaclass=Singleton):\n        pass\n    assert S.MySingleton is MySingleton()\n\n    # Test that accessing an attribute on S that has been registered returns the correct instance\n    assert S.MySingleton is MySingleton()\n\n    # Test that S can be used as a shortcut for sympify\n    assert S(1) == 1\n    assert isinstance(S(1), Rational)\n    assert S(\"x**2\") == Basic.sympify(\"x**2\")\n", "def test_SingletonRegistry_register():\n    class NewSingleton(Basic, metaclass=Singleton):\n        pass\n    assert S.NewSingleton is NewSingleton()\n    # make sure a duplicate class overwrites the old one\n    class NewSingleton(Basic, metaclass=Singleton):\n        pass\n    assert S.NewSingleton is NewSingleton()\n", "def test_singleton_registry():\n    # Test that the SingletonRegistry can correctly register and retrieve classes\n    class TestSingleton(Basic, metaclass=Singleton):\n        pass\n\n    # Check that the class is registered and can be retrieved\n    assert S.TestSingleton is TestSingleton()\n\n    # Check that the registered class is a singleton\n    assert S.TestSingleton is S.TestSingleton\n\n    # Check that the class can be correctly instantiated\n    test_singleton = TestSingleton()\n    assert test_singleton is S.TestSingleton\n", "def test_singleton_registry():\n    # Test registering and accessing a singleton class\n    class TestSingleton(Basic, metaclass=Singleton):\n        pass\n\n    assert S.TestSingleton is TestSingleton()\n    assert isinstance(S.TestSingleton, TestSingleton)\n    assert S.TestSingleton is S.TestSingleton\n\n    # Test __call__ method\n    assert S(1) == Basic(1)\n    assert S(Rational(1, 2)) == Rational(1, 2)\n\n    # Test accessing a non-registered class\n    try:\n        S.NonExistentClass\n        assert False, \"Expected AttributeError\"\n    except AttributeError:\n        pass\n"], "sample_1147": ["def test_LatexPrinter_settings():\n    lp = LatexPrinter({'decimal_separator': 'comma', 'order': 'none'})\n    assert lp._settings['decimal_separator'] == 'comma'\n    assert lp._settings['order'] == 'none'\n", "def test_latex_latticeops():\n    assert latex(Complement(UniversalSet, FiniteSet(x))) == r\"\\mathbb{U} \\setminus \\left\\{x\\right\\}\"\n    assert latex(SymmetricDifference(FiniteSet(1), FiniteSet(2))) == r\"\\left\\{1\\right\\} \\triangle \\left\\{2\\right\\}\"\n", "def test_LatexPrinter_settings():\n    x, tau = symbols('x tau')\n\n    # full_prec\n    l = LatexPrinter({'full_prec': True})\n    assert l.doprint(Float('3.14159', 3)) == r\"3.14159\"\n    l = LatexPrinter({'full_prec': False})\n    assert l.doprint(Float('3.14159', 3)) == r\"3.14\"\n\n    # fold_frac_powers\n    l = LatexPrinter({'fold_frac_powers': True})\n    assert l.doprint(tau**Rational(1, 2)) == r\"\\tau^{1/2}\"\n    l = LatexPrinter({'fold_frac_powers': False})\n    assert l.doprint(tau**Rational(1, 2)) == r\"\\sqrt{\\tau}\"\n\n    # fold_func_brackets\n    l = LatexPrinter({'fold_func_brackets': True})\n    assert l.doprint(sin(x**2)) == r\"\\sin {x^{2}}\"\n    l = LatexPrinter({'fold_func_brackets': False})\n    assert l.doprint(sin(x**2)) == r\"\\sin{\\left(x^{2} \\right)}\"\n\n    # fold_short_frac\n    l = LatexPrinter({'fold_short_frac': True})\n    assert l.doprint(1/y) == r\"\\frac{1}{y}\"\n    assert l.doprint(1/(y + z)) == r\"\\frac{1}{y + z}\"\n    l = LatexPrinter({'fold_short_frac': False})\n    assert l.doprint(1/y) == r\"\\frac{1}{y}\"\n    assert l.doprint(1/(y + z)) == r\"\\frac{1}{y + z}\"\n\n    # inv_trig_style\n    l = LatexPrinter({'inv_trig_style': 'full'})\n    assert l.doprint(asin(x)) == r\"\\arcsin{\\left(x \\right)}\"\n    l = LatexPrinter({'inv_trig_style': 'abbreviated'})\n    assert l.doprint(asin(x)) == r\"\\operatorname{asin}{\\left(x \\right)}\"\n    l = LatexPrinter({'inv_trig_style': 'power'})\n    assert l.doprint(asin(x)) == r\"\\sin^{-1}{\\left(x \\right)}\"\n\n    # itex\n    l = Lat", "def test_print_frac():\n    assert latex(frac(x, 1)) == r\"\\operatorname{frac}{\\left(x\\right)}\"\n    assert latex(frac(1, pi)) == r\"\\frac{1}{\\pi}\"\n", "def test_latex_Subs():\n    f = Function('f')\n    x = Symbol('x')\n    y = Symbol('y')\n    l = Limit(x, x, 0)\n    test = Subs(f(x, y), x, 0)\n    assert latex(test) == r'f{\\left(0,y \\right)}'\n\n    test = Subs(f(x, y), (x, y), (0, 0))\n    assert latex(test) == r'f{\\left(0,0 \\right)}'\n\n    test = Subs(l, x, 0)\n    assert latex(test) == r'\\lim_{0 \\to 0} 0'\n\n    test = Subs(l.doit(), x, 0)\n    assert latex(test) == '0'\n"], "sample_1146": ["def test_print_latex_RandomDomain():\n    D = S.Reals\n    assert latex(RandomDomain(D, 'D')) == r'\\text{Domain: }\\mathbb{R}'\n", "def test_latex_bessel():\n    assert latex(besselj(n, x)) == r\"J_{n}\\left(x\\right)\"\n    assert latex(besseli(n, x)) == r\"I_{n}\\left(x\\right)\"\n    assert latex(besselk(n, x)) == r\"K_{n}\\left(x\\right)\"\n    assert latex(bessely(n, x)) == r\"Y_{n}\\left(x\\right)\"\n    assert latex(hankel1(n, x)) == r\"H^{(1)}_{n}\\left(x\\right)\"\n    assert latex(hankel2(n, x)) == r\"H^{(2)}_{n}\\left(x\\right)\"\n", "def test_latex_matrix():\n    M = Matrix([[1, 2], [3, 4]])\n    assert latex(M) == r'\\left[\\begin{matrix}1 & 2\\\\3 & 4\\end{matrix}\\right]'\n    assert latex(M, mode='inline') == r'\\left[\\begin{smallmatrix}1 & 2\\\\3 & 4\\end{smallmatrix}\\right]'\n    assert latex(M, mat_str='bmatrix') == r'\\begin{bmatrix}1 & 2\\\\3 & 4\\end{bmatrix}'\n    assert latex(M, mat_delim='(') == r'\\left(\\begin{matrix}1 & 2\\\\3 & 4\\end{matrix}\\right)'\n", "def test_latex_empty():\n    assert latex(S.EmptySet) == r\"\\emptyset\"\n", "def test_LatexPrinter_settings():\n    # issue 11717\n    x = symbols('x')\n    l = LatexPrinter({'order': 'lex'})\n    assert l.doprint(x**2 + 2*x) == '2 x + x^{2}'\n\n    l = LatexPrinter({'order': 'old'})\n    assert l.doprint(x**2 + 2*x) == '2 x + x^{2}'\n\n    l = LatexPrinter({'order': 'none'})\n    assert l.doprint(x**2 + 2*x) == 'x^{2} + 2 x'\n"], "sample_1150": ["def test_ComplexRegion_contains():\n    a, b = Interval(1, 3), Interval(4, 6)\n    c1 = ComplexRegion(a*b)\n    assert 2 + 5*I in c1\n    assert 1 + 4*I in c1\n    assert 3 + 6*I not in c1\n    assert 0 + 0*I not in c1\n    assert (2 + 5*I).as_real_imag() in c1.as_set()\n\n    c2 = ComplexRegion(Union(a*b, b*a))\n    assert 2 + 5*I in c2\n    assert 5 + 2*I in c2\n    assert 1 + 4*I in c2\n    assert 4 + 1*I in c2\n    assert 3 + 6*I not in c2\n    assert 0 + 0*I not in c2\n\n    r, theta = Interval(1, 2), Interval(0, pi/2)\n    c3 = ComplexRegion(r*theta, polar=True)\n    assert 1 + I in c3\n    assert I not in c3\n    assert 1 not in c3\n    assert 2 + 0*I in c3\n    assert 3*I not in c3\n\n    c4 = ComplexRegion(Interval(0, 1)*Interval(0, 2*pi), polar=True)\n    assert 0 + 0*I in c4\n    assert 1 + 0*I in c4\n    assert 0 + I in c4\n    assert -1 + I not in c4\n", "def test_ComplexRegion_polar():\n    rset = Interval(1, 2)\n    thetaset = Interval(0, pi)\n    cset = ComplexRegion(rset * thetaset, polar=True)\n    assert 1.5 * exp(I * pi / 2) in cset\n    assert 2.5 * exp(I * pi / 2) not in cset\n    assert cset.measure == pi\n", "def test_ComplexRegion_contains():\n    a = Interval(1, 4)\n    b = Interval(1, 6)\n    c = Interval(3, 4)\n    d = Interval(5, 7)\n    r1 = ComplexRegion(Interval(0, 1)*Interval(0, 2*pi), polar=True)\n    assert 1 in r1\n    assert 1 + I in r1\n    assert -1 in r1\n    assert -1 - I in r1\n    assert 2*I in r1\n    assert -2*I in r1\n\n    r2 = ComplexRegion(a*b)\n    assert 2 + 2*I in r2\n    assert 2 + 6*I not in r2\n    assert 1 + 1*I in r2\n    assert 1 + 6*I not in r2\n\n    r3 = ComplexRegion(Union(a*b, c*d))\n    assert 2 + 2*I in r3\n    assert 2 + 6*I in r3\n    assert 1 + 1*I in r3\n    assert 1 + 6*I not in r3\n    assert 3 + 5*I in r3\n    assert 3 + 7*I not in r3\n", "def test_ComplexRegion_from_real():\n    unit = Interval(0, 1)\n    complex_unit = ComplexRegion.from_real(unit)\n    assert complex_unit.is_subset(ComplexRegion(unit * Interval(-oo, oo)))\n    assert complex_unit.is_ComplexRegion\n    assert complex_unit.polar is False\n\n    reals = Interval(-oo, oo)\n    complex_reals = ComplexRegion.from_real(reals)\n    assert complex_reals.is_subset(ComplexRegion(reals * Interval(-oo, oo)))\n    assert complex_reals.is_ComplexRegion\n    assert complex_reals.polar is False\n", "def test_ComplexRegion_contains():\n    a, b = Interval(1, 5), Interval(1, 5)\n    theta = Interval(0, pi)\n    c1 = ComplexRegion(a*b)\n    c2 = ComplexRegion(a*theta, polar=True)\n    assert 3 + 3*I in c1\n    assert 1 + 1*I in c1\n    assert 6 + 3*I not in c1\n    assert 1 + 1*I in c2\n    assert 2 + 2*I in c2\n    assert 1 + 6*I not in c2\n    assert I in ComplexRegion(Interval(0, 2)*Interval(0, 2*pi, left_open=True, right_open=True), polar=True)\n    assert -1 not in ComplexRegion(Interval(0, 2)*Interval(0, pi), polar=True)\n    assert 1 not in ComplexRegion(Interval(0, 2)*Interval(0, pi, left_open=True, right_open=True), polar=True)\n    assert 1 in ComplexRegion(Interval(1, 2)*Interval(0, 2*pi, left_open=True, right_open=True), polar=True)\n    r, theta = Interval(0, 1), Interval(0, 2*pi)\n    c = ComplexRegion(r * theta, polar=True)\n    assert -1 not in c\n    assert 1 not in c\n    assert I in c\n    assert -I in c\n    assert 0 in c\n    assert -sqrt(2)/2 + sqrt(2)*I/2 in c\n    assert sqrt(2)/2 + sqrt(2)*I/2 in c\n    assert 1 + I not in c\n"], "sample_1152": ["def test_powsimp_polar():\n    x = Symbol('x', polar=True)\n    assert powsimp(x**2) == x**2\n    assert powsimp(x**(2*I)) == (x**2)**I\n", "def test_powdenest_polar():\n    p = symbols('p', positive=True)\n    n = symbols('n', negative=True)\n    x = symbols('x')\n    assert powdenest((p**x)**n) == p**(x*n)\n    assert powdenest((n**x)**n) == (n**x)**n\n    assert powdenest((p**x)**(2*n)) == (p**x)**(2*n)\n", "def test_powsimp_exp():\n    x = Symbol('x', real=True)\n    assert powsimp(exp(x), deep=True, combine='exp') == exp(x)\n    assert powsimp(exp(2*x), deep=True, combine='exp') == exp(2*x)\n    assert powsimp(exp(x)*exp(2*x), deep=True, combine='exp') == exp(3*x)\n", "def test_powdenest_forcing():\n    x = Symbol('x', real=True)\n\n    # if forced, do not require base to be positive or the exponent to be an integer\n    assert powdenest((x**(2*a/3))**(3*x), force=True) == x**(2*a*x)\n    assert powdenest((x**(2*a/3))**(3*y), force=True) == x**(2*a*y)\n    assert powdenest((x**(2*a/3))**(3*y), force=True) != x**(2*a*y/3)\n\n    # if the base is negative, this denies the possibility of the exponent being an even integer\n    n = Symbol('n', negative=True)\n    assert powdenest((n**(2*a/3))**(3*y), force=True) == (n**(2*a/3))**(3*y)\n", "def test_powsimp_on_polar():\n    x = Symbol('x', polar=True)\n    assert powsimp(x**2) == x**2\n    assert powsimp(x**2*x**3) == x**5\n    assert powsimp(x**(2/3)*x**(3/2)) == x**(13/6)\n"], "sample_1151": ["def test_Mod_eval():\n    # Test Mod.eval\n    assert Mod(5, 3) == 2\n    assert Mod(-5, 3) == 1\n    assert Mod(5, -3) == -1\n    assert Mod(-5, -3) == -2\n    assert Mod(0, 3) == 0\n    assert Mod(0, -3) == 0\n    assert Mod(5, 1) == 0\n    assert Mod(-5, 1) == 0\n    assert Mod(5, -1) == 0\n    assert Mod(-5, -1) == 0\n\n    x = Symbol('x')\n    assert Mod(x, 1) == 0\n    assert Mod(x, -1) == 0\n    assert Mod(x, x) == 0\n    assert Mod(-x, x) == 0\n    assert Mod(x, -x) == 0\n    assert Mod(-x, -x) == 0\n", "def test_Mod():\n    assert Mod(2, 3).is_integer\n    assert Mod(-2, 3).is_integer\n    assert Mod(2, -3).is_integer\n    assert Mod(-2, -3).is_integer\n    assert Mod(2.5, 3).is_integer is None\n    assert Mod(x, 3).is_integer is None\n\n    assert Mod(x, 1) == 0\n    assert Mod(5, 3) == 2\n    assert Mod(-5, 3) == 1\n    assert Mod(2, -3) == -1\n    assert Mod(-2, -3) == -2\n    assert Mod(x, -3) == Mod(x, 3)\n\n    assert Mod(x, y).subs({x: 5, y: 3}) == 2\n    assert Mod(x, 3).subs(x, 5) == 2\n    assert Mod(x, y).subs({x: -5, y: 3}) == 1\n    assert Mod(x, 3).subs(x, -5) == 1\n\n    assert Mod(x + y, x) == Mod(y, x)\n    assert Mod(x + 2*y, y) == Mod(x, y)\n    assert Mod(x + 2*y, 3*y) == Mod(x, 3*y)\n    assert Mod(x + 2*y, y + x) == Mod(2*y, y + x)\n\n    assert Mod(2*x + 3*y, x + y) == Mod(x + 2*y, x + y)\n    assert Mod(x + 2*y, x + y) == Mod(y, x + y)\n    assert Mod(x + 2*y, x + 3*y) == Mod(2*x + y, x + 3*y)\n    assert Mod(x + 2*y, x + 3*y) == Mod(x - y, x + 3*y)\n\n    assert Mod(x*y, x) == 0\n    assert Mod(x*y, y) == 0\n    assert Mod(x*y, x*y) == 0\n\n    assert Mod(5*x, 3*x) == 2*x\n    assert Mod(5*x, 3*y) == Mod(5*x, 3*y)\n    assert Mod(5*x, 3) == Mod(", "def test_Mod():\n    # test simple cases\n    assert Mod(0, 1) == 0\n    assert Mod(1, 1) == 0\n    assert Mod(2, 1) == 0\n    assert Mod(2, 3) == 2\n    assert Mod(-2, 3) == 1\n    assert Mod(-3, -1) == 0\n    assert Mod(1, -2) == -1\n    assert Mod(1, -1) == 0\n    assert Mod(-1, 1) == 0\n    assert Mod(-1, -1) == 0\n    assert Mod(5, 2) == 1\n    assert Mod(5, -2) == -1\n    assert Mod(-5, 2) == 1\n    assert Mod(-5, -2) == -1\n    assert Mod(1.5, 1) == 0.5\n    assert Mod(1.5, -1) == -0.5\n    assert Mod(-1.5, 1) == 0.5\n    assert Mod(-1.5, -1) == -0.5\n    # test some larger numbers\n    assert Mod(100, 7) == 2\n    assert Mod(100, -7) == -2\n    assert Mod(-100, 7) == 5\n    assert Mod(-100, -7) == -5\n    # test properties of Mod\n    x, y, z = symbols('x y z')\n    assert Mod(x, 1) == 0\n    assert Mod(x, -1) == 0\n    assert Mod(x, x) == 0\n    assert Mod(x, -x) == 0\n    assert Mod(x, y) == x - y * floor(x / y)\n    assert Mod(x, -y) == x + y * floor(-x / y)\n    assert Mod(x + y, z) == Mod(x, z) + Mod(y, z)\n    assert Mod(x - y, z) == Mod(x, z) - Mod(y, z)\n    assert Mod(x * y, z) == Mod(x, z) * y\n    assert Mod(x / y, z) == Mod(x, z) / y\n    assert unchanged(Mod, x + y,", "def test_Mod():\n    assert Mod(0, 5) == 0\n    assert Mod(3, 5) == 3\n    assert Mod(-3, 5) == 2\n    assert Mod(5, 3) == 2\n    assert Mod(-5, 3) == 1\n    assert Mod(3.0, 5) == 3.0\n    assert Mod(-3.0, 5) == 2.0\n    assert Mod(3, 5.0) == 3.0\n    assert Mod(-3, 5.0) == 2.0\n    assert Mod(x, 5).subs(x, 3) == 3\n    assert Mod(x, y).subs({x: 3, y: 5}) == 3\n    assert Mod(x + 2, 5).subs(x, 3) == 0\n    assert Mod(x + 2, y).subs({x: 3, y: 5}) == 0\n    assert Mod(x, y).subs({x: 3, y: 5}) != Mod(3, 5)\n    assert Mod(x, y) != Mod(y, x)\n    assert Mod(x, y) != x\n    assert Mod(x, y) != y\n    assert Mod(3, 5) != Mod(5, 3)\n    assert Mod(3, 5) != 3\n    assert Mod(3, 5) != 5\n", "def test_Mod_eval():\n    # test simple evaluation\n    assert Mod(3, 2) == 1\n    assert Mod(4, 2) == 0\n    assert Mod(-1, 2) == 1\n    assert Mod(-3, 2) == 1\n\n    # test evaluation with symbolic arguments\n    assert Mod(x, 2).subs(x, 3) == 1\n    assert Mod(x, 2).subs(x, -1) == 1\n\n    # test evaluation with gcd extraction\n    assert Mod(6, 4) == 2\n    assert Mod(12, 8) == 4\n\n    # test nested Mod\n    assert Mod(Mod(x, 2), 2) == Mod(x, 2)\n    assert Mod(Mod(x, 3), 2) == Mod(x, 2)\n\n    # test for non-integer arguments\n    assert Mod(3.5, 2) == 1.5\n    assert Mod(-3.5, 2) == 0.5\n\n    # test for complex arguments\n    assert Mod(3 + 2*I, 2) == 1 + 2*I\n    assert Mod(-3 - 2*I, 2) == 1 - 2*I\n\n    # test for zero divisor\n    raises(ZeroDivisionError, lambda: Mod(3, 0))\n\n    # test for nan\n    assert Mod(nan, 2) == nan\n"], "sample_1153": ["def test_principal_branch():\n    x = Symbol('x')\n    assert principal_branch(exp_polar(2*pi*I)*3, 2*pi) == 3*exp_polar(0)\n    assert principal_branch(exp_polar(2*pi*I)*3*x, 2*pi) == 3*principal_branch(x, 2*pi)\n    assert principal_branch(exp_polar(pi*I)*x, 2*pi) == exp_polar(pi*I)*principal_branch(x, 2*pi)\n    assert principal_branch(exp_polar(pi*I/2)*x, 2*pi) == exp_polar(pi*I/2)*principal_branch(x, 2*pi)\n    assert principal_branch(exp_polar(3*pi*I/2)*x, 2*pi) == exp_polar(3*pi*I/2)*principal_branch(x, 2*pi)\n", "def test_re_evalf():\n    x, y = symbols('x y')\n    assert N_equals(re(exp_polar(1 + I*pi/2)), 0)\n    assert N_equals(re(exp_polar(1 + I*pi/3)), exp(1)/2)\n    assert N_equals(re(exp_polar(1 + I*pi/4)), exp(1)/sqrt(2))\n    assert re(x + y) == re(x + y)\n    assert N_equals(re(exp_polar(1 + I*pi/2)).evalf(), 0)\n    assert N_equals(re(exp_polar(1 + I*pi/3)).evalf(), exp(1)/2)\n    assert N_equals(re(exp_polar(1 + I*pi/4)).evalf(), exp(1)/sqrt(2))\n    assert re(x + y).evalf() == re(x + y)\n", "def test_polar_lift():\n    p = Symbol('p', polar=True)\n    x = Symbol('x')\n    assert polar_lift(4) == 4*exp_polar(0)\n    assert polar_lift(-4) == 4*exp_polar(I*pi)\n    assert polar_lift(-I) == exp_polar(-I*pi/2)\n    assert polar_lift(I + 2) == polar_lift(2 + I)\n    assert polar_lift(4*x) == 4*polar_lift(x)\n    assert polar_lift(4*p) == 4*p\n", "def test_polar_lift():\n    x = Symbol('x')\n    p = Symbol('p', polar=True)\n    assert polar_lift(4) == 4*exp_polar(0)\n    assert polar_lift(-4) == 4*exp_polar(I*pi)\n    assert polar_lift(-I) == exp_polar(-I*pi/2)\n    assert polar_lift(I + 2) == polar_lift(2 + I)\n    assert polar_lift(4*x) == 4*polar_lift(x)\n    assert polar_lift(4*p) == 4*p\n", "def test_transpose():\n    A = MatrixSymbol('A', 2, 3)\n    B = MatrixSymbol('B', 3, 2)\n    assert transpose(A) == A.T\n    assert transpose(A*B) == B.T*A.T\n    assert transpose(A + B.T) == A.T + B\n    assert transpose(2*A) == 2*A.T\n    assert transpose(A.T) == A\n    C = Matrix([[1, 2], [3, 4]])\n    assert transpose(C) == C.T\n    assert transpose(ImmutableMatrix(C)) == C.T\n    assert transpose(SparseMatrix(C)) == C.T\n    assert transpose(ImmutableSparseMatrix(C)) == C.T\n    assert transpose(FunctionMatrix(2, 2, Lambda((x, y), x + y))) == \\\n        FunctionMatrix(2, 2, Lambda((x, y), x + y))\n"], "sample_1154": ["def test__linsolve():\n    # No solution\n    assert _linsolve((Eq(x, 1), Eq(x, 2)), (x,)) is None\n\n    # Unique solution\n    assert _linsolve((Eq(x + y, 1), Eq(x - y, 2)), (x, y)) == {x: S(3)/2, y: S(-1)/2}\n\n    # Underdetermined system\n    assert _linsolve((Eq(x + y, 0),), (x, y)) == {x: -y, y: y}\n\n    # Complex coefficients\n    assert _linsolve((Eq(x + I*y, 1 + I),), (x, y)) == {x: 1, y: 1}\n\n    # Nonlinear term (raises PolyNonlinearError)\n    raises(PolyNonlinearError, lambda: _linsolve((Eq(x**2 + y, 1),), (x, y)))\n", "def test_linsolve_nonlinear_error():\n    # Test that a PolyNonlinearError is raised for nonlinear equations\n    eqs = [Eq(x**2 + y, 1), Eq(x - y, 2)]\n    with raises(PolyNonlinearError):\n        _linsolve(eqs, [x, y])\n", "def test__linsolve_non_unique_solution():\n    # In the case of underdetermined systems the solution will be expressed in\n    # terms of the unknown symbols that are unconstrained\n    sol = _linsolve([Eq(x + y, 0)], [x, y])\n    assert sol == {x: -y, y: y}\n", "def test__linear_eq_to_dict():\n    eq1 = Eq(x + y, 1)\n    eq2 = Eq(x - y, 2)\n    eqsdict, eqs_rhs = _linear_eq_to_dict([eq1, eq2], [x, y])\n    assert eqsdict == [{x: 1, y: 1}, {x: 1, y: -1}]\n    assert eqs_rhs == [1, 2]\n\n    # Test with Eq not using Equality\n    eq1 = x + y - 1\n    eq2 = x - y - 2\n    eqsdict, eqs_rhs = _linear_eq_to_dict([eq1, eq2], [x, y])\n    assert eqsdict == [{x: 1, y: 1}, {x: 1, y: -1}]\n    assert eqs_rhs == [1, 2]\n", "def test_linsolve_non_linear_error():\n    # Nonlinear equation\n    eq = Eq(x**2, 4)\n    raises(PolyNonlinearError, lambda: _linsolve([eq], [x]))\n\n    # Nonlinear equation after expansion\n    eq = Eq((x + 1)**2, 4)\n    raises(PolyNonlinearError, lambda: _linsolve([eq], [x]))\n\n    # Linear equation with nonlinear expansion\n    eq1 = Eq(x**2 - x, 0)  # x**2 - x = x*(x-1)\n    eq2 = Eq(x, 0)\n    assert _linsolve([eq1, eq2], [x]) == {x: 0}\n"], "sample_1155": ["def test_extension():\n    assert construct_domain([1, sqrt(2)], extension=True)[0].is_AlgebraicField\n    assert construct_domain([1, sqrt(2), sqrt(3)], extension=True)[0].is_AlgebraicField\n    assert construct_domain([1, E], extension=True) == (EX, [EX(1), EX(E)])\n    assert construct_domain([1, pi], extension=True) == (EX, [EX(1), EX(pi)])\n    assert construct_domain([1, Catalan], extension=True) == (EX, [EX(1), EX(Catalan)])\n", "def test_construct_domain_extension():\n    # Test extension=True and extension=False\n    K, elements = construct_domain([sqrt(2)], extension=True)\n    assert isinstance(K, type(QQ.algebraic_field((x**2 - 2, x))))\n    assert elements[0]**2 == K(2)\n\n    K, elements = construct_domain([sqrt(2)], extension=False)\n    assert K == EX\n    assert elements[0]**2 == EX(2)\n", "def test_construct_domain_extension():\n    K, elements = construct_domain([1, sqrt(2)], extension=True)\n    assert isinstance(K, type(QQ.algebraic_field(sqrt(2))))\n    assert len(elements) == 2\n\n    K, elements = construct_domain([1, sqrt(2), sqrt(3)], extension=True)\n    assert isinstance(K, type(QQ.algebraic_field((sqrt(2), sqrt(3)))))\n    assert len(elements) == 3\n\n    K, elements = construct_domain([1, sqrt(2), sqrt(3), pi], extension=True)\n    assert K == EX\n    assert len(elements) == 4\n", "def test_construct_domain_extension():\n    # test construct_domain with extension option\n    domain, coeffs = construct_domain([sqrt(2)], extension=True)\n    assert domain != EX\n    assert coeffs != [EX(sqrt(2))]\n    # test construct_domain with extension option, multiple coeffs\n    domain, coeffs = construct_domain([sqrt(2), sqrt(3)], extension=True)\n    assert domain != EX\n    assert coeffs != [EX(sqrt(2)), EX(sqrt(3))]\n    # test construct_domain with extension option, algebraic and rational coeffs\n    domain, coeffs = construct_domain([sqrt(2), Rational(1, 2)], extension=True)\n    assert domain != EX\n    assert coeffs != [EX(sqrt(2)), EX(Rational(1, 2))]\n", "def test_construct_domain_with_algebraic_numbers():\n    # Test with algebraic numbers\n    domain, coeffs = construct_domain([sqrt(2)], extension=True)\n    assert domain != EX\n    assert domain.is_AlgebraicField\n    assert coeffs[0].is_Unit\n\n    # Test with multiple algebraic numbers\n    domain, coeffs = construct_domain([sqrt(2), sqrt(3)], extension=True)\n    assert domain != EX\n    assert domain.is_AlgebraicField\n    assert coeffs[0].is_Unit\n    assert coeffs[1].is_Unit\n\n    # Test with algebraic numbers and other coefficients\n    domain, coeffs = construct_domain([sqrt(2), 2, 3/4], extension=True)\n    assert domain != EX\n    assert domain.is_AlgebraicField\n    assert coeffs[0].is_Unit\n    assert coeffs[1] == domain.dtype(2)\n    assert coeffs[2] == domain.dtype(3)/domain.dtype(4)\n"], "sample_1157": ["def test_factorial_notation():\n    # Test standard factorial notation\n    assert parse_expr('3!', evaluate=False) == factorial(3)\n    assert parse_expr(' 3! ', evaluate=False) == factorial(3)\n    assert parse_expr('(2!)', evaluate=False) == factorial(2)\n    # Test double factorial notation\n    assert parse_expr('3!!', evaluate=False) == factorial2(3)\n    assert parse_expr(' 3!! ', evaluate=False) == factorial2(3)\n    assert parse_expr('(2!!)', evaluate=False) == factorial2(2)\n    # Test invalid notations\n    raises(TokenError, lambda: parse_expr('2!!!', evaluate=False))\n    raises(TokenError, lambda: parse_expr('2!', transformations=[]))\n", "def test_evalf_parameter():\n    # Test that the `evaluate` parameter doesn't cause problems with `evalf`\n    assert parse_expr('1.2', evaluate=False).evalf() == Float('1.2', 53).evalf()\n", "def test_evaluate_false():\n    x = Symbol('x')\n    assert parse_expr('1 + x', evaluate=False).args == (1, x)\n    assert parse_expr('x + 1', evaluate=False).args == (x, 1)\n    assert parse_expr('2**3', evaluate=False) != parse_expr('2**3')\n    assert str(parse_expr('1/2', evaluate=False)) == '1/2'\n    assert parse_expr('2*x', transformations=standard_transformations + (implicit_multiplication_application,), evaluate=False) == Mul(2, x, evaluate=False)\n", "def test_factorial_notation():\n    x = Symbol('x')\n    assert parse_expr('x!', transformations=standard_transformations) == factorial(x)\n    assert parse_expr('x!!', transformations=standard_transformations) == factorial2(x)\n    raises(TokenError, lambda: parse_expr('x!!!', transformations=standard_transformations))\n    assert parse_expr('2*x!', transformations=standard_transformations) == 2 * factorial(x)\n", "def test_factorial_notation():\n    x = Symbol('x')\n    assert parse_expr('x!', transformations=standard_transformations) == factorial(x)\n    assert parse_expr('x!!', transformations=standard_transformations) == factorial2(x)\n    assert parse_expr('x!!!', transformations=standard_transformations) == factorial(factorial2(x))\n    raises(TokenError, lambda: parse_expr('x!!!!', transformations=standard_transformations))\n"], "sample_1156": ["def test_csch():\n    x = Symbol('x')\n    y = Symbol('y')\n    z = Symbol('z')\n    k = Symbol('k', integer=True)\n    n = Symbol('n', integer=True)\n\n    assert csch(z).diff(z) == -coth(z)*csch(z)\n    assert csch(-z).diff(z) == coth(z)*csch(z)\n    assert csch(z).diff(y) == 0\n\n    assert csch(z).as_real_imag() == (1 / sinh(z)).as_real_imag()\n\n    assert csch(0) == zoo\n    assert csch(-0) == -oo\n    assert unchanged(csch, pi)\n    assert unchanged(csch, -pi)\n    assert unchanged(csch, 2*pi*I)\n    assert unchanged(csch, -2*pi*I)\n    assert unchanged(csch, pi/2)\n    assert unchanged(csch, -pi/2)\n    assert unchanged(csch, pi/4)\n    assert unchanged(csch, -pi/4)\n    assert unchanged(csch, 2*I)\n    assert unchanged(csch, -2*I)\n\n    assert csch(k*pi*I) == (-1)**k / (sinh(k*pi*I))\n    assert unchanged(csch, k*pi/4)\n\n    assert csch(n).is_extended_real\n    assert csch(n).is_finite\n    assert csch(pi*I).is_complex\n    assert csch(2*pi*I).is_complex\n    assert csch(3*pi*I).is_complex\n    assert csch(4*pi*I).is_complex\n\n    assert re(csch(x)) == re(csch(re(x)))\n    assert im(csch(x)) == im(csch(re(x)))\n\n    assert unchanged(csch, 2+3*I)\n    assert unchanged(csch, 2-3*I)\n    assert unchanged(csch, -2+3*I)\n    assert unchanged(csch, -2-3*I)\n\n    assert unchanged(csch, E)\n    assert unchanged(csch, -E)\n\n    assert expand_mul(csch(x+y)) == expand_mul(csch(x)*csch(y)/(csch(x)*coth(y) + csch(y)*coth(x)))\n\n    assert expand_trig(expand_mul(csch(x+y))) == expand_trig(csch(x)*csch(y)/(csch(x)*coth(y) + csch(y)*coth", "def test_hyperbolic_leading_terms():\n    w = Symbol('w')\n    x = Symbol('x')\n    y = Symbol('y')\n    z = Symbol('z')\n\n    assert sinh(x).as_leading_term(x) == x\n    assert cosh(x).as_leading_term(x) == 1\n    assert tanh(x).as_leading_term(x) == x\n    assert coth(x).as_leading_term(x) == 1/x\n    assert sech(x).as_leading_term(x) == 1\n    assert csch(x).as_leading_term(x) == 1/x\n    assert asinh(x).as_leading_term(x) == x\n    assert acosh(x).as_leading_term(x) == I*pi/2\n    assert atanh(x).as_leading_term(x) == x\n    assert acoth(x).as_leading_term(x) == I*pi/2\n    assert asech(x).as_leading_term(x) == I*pi/2\n    assert acsch(x).as_leading_term(x) == 1/x\n", "def test_csch():\n    x = symbols('x')\n    z = Symbol('z', real=False)\n    k = Symbol('k', integer=True)\n    assert csch(0) == zoo\n    assert csch(oo) == 0\n    assert csch(-oo) == 0\n    assert csch(nan) == nan\n    assert csch(I*pi/2) == -I\n    assert csch(-I*pi/2) == I\n    assert csch(I*(pi/2 + pi*k)) == (-1)**k * I\n    assert unchanged(csch, z)\n    raises(ArgumentIndexError, lambda: csch(x).fdiff(2))\n    assert re(csch(x)) in (csch(x), -csch(x))\n    assert im(csch(x)) in (0, pi, -pi)\n    assert re(csch(z)) == -coth(z)*csch(z) if re(z) > 0 else coth(z)*csch(z)\n    assert csch(x).as_real_imag() == (csch(x), 0)\n    assert unchanged(expand_mul, csch(x))\n    assert expand_trig(csch(x)) == csch(x)\n    assert csch(x).as_leading_term(x) == 1/x\n    assert csch(z).as_leading_term(z) == 1/z\n    assert csch(x).is_extended_real\n    assert unchanged(csch, I + pi)\n    assert unchanged(csch, I + 2*pi)\n    assert unchanged(csch, 2*I + pi)\n    assert unchanged(csch, 2*I + 2*pi)\n    assert O(csch(x), x).contains(csch(O(x, x)))\n", "def test_hyperbolic_trig_rewrite_as_exp():\n    x = symbols('x')\n    assert sinh(x)._eval_rewrite_as_exp(x) == (exp(x) - exp(-x)) / 2\n    assert cosh(x)._eval_rewrite_as_exp(x) == (exp(x) + exp(-x)) / 2\n    assert tanh(x)._eval_rewrite_as_exp(x) == (exp(x) - exp(-x)) / (exp(x) + exp(-x))\n    assert coth(x)._eval_rewrite_as_exp(x) == (exp(x) + exp(-x)) / (exp(x) - exp(-x))\n    assert sech(x)._eval_rewrite_as_exp(x) == 2 / (exp(x) + exp(-x))\n    assert csch(x)._eval_rewrite_as_exp(x) == 2 / (exp(x) - exp(-x))\n", "def test_issue_21161():\n    x = Symbol('x')\n    assert asinh(x).diff(x) == 1/sqrt(x**2 + 1)\n    assert acsch(x).diff(x) == -1/(x**2*sqrt(1 + x**(-2)))\n    assert asech(x).diff(x) == -1/(x*sqrt(1 - x**2))\n    assert atanh(x).diff(x) == 1/(1 - x**2)\n    assert acoth(x).diff(x) == 1/(1 - x**2)\n"], "sample_1158": ["def test_sympify_strict():\n    raises(SympifyError, lambda: sympify(None, strict=True))\n    raises(SympifyError, lambda: sympify({}, strict=True))\n    raises(SympifyError, lambda: sympify([], strict=True))\n    raises(SympifyError, lambda: sympify(set(), strict=True))\n    raises(SympifyError, lambda: sympify(tuple(), strict=True))\n", "def test_sympify_strict():\n    # Test that sympify with strict=True only converts types for which an\n    # explicit conversion has been defined.\n    assert sympify(\"1\", strict=True) == Integer(1)\n    assert sympify(1, strict=True) == Integer(1)\n    assert sympify(1.0, strict=True) == Float(1.0)\n    raises(SympifyError, lambda: sympify(None, strict=True))\n    raises(SympifyError, lambda: sympify(object(), strict=True))\n    raises(SympifyError, lambda: sympify(\"None\", strict=True))\n", "def test_sympify_strict_not_numpy():\n    # when strict=True, only types for which an explicit conversion has been\n    # defined should be converted. This includes numpy data types but does not\n    # include numpy arrays.\n    strict_sympify_error = raises(SympifyError, lambda: sympify(numpy.array([1, 2]), strict=True))\n    assert \"SympifyError: SympifyError: [1 2]\" in str(strict_sympify_error)\n\n    # for numpy data types\n    numpy_integer = numpy.int64(3)\n    assert sympify(numpy_integer, strict=True) == 3\n", "def test_sympify_strict_error():\n    # Test strict=True with an undefined object that sympy can't convert\n    class TestClass:\n        pass\n\n    raises(SympifyError, lambda: sympify(TestClass(), strict=True))\n\n    # Test strict=True with an object that has _sympy_ method but can't convert\n    class TestClass2:\n            raise NotImplementedError\n\n    raises(SympifyError, lambda: sympify(TestClass2(), strict=True))\n", "def test_kernS():\n    assert kernS('2*(x + y)') == 2*(x + y)\n    assert kernS('-(x + 1)') == -(x + 1)\n    assert kernS('2*x') == 2*x\n    assert kernS('x**2') == x**2\n    assert kernS('x**(2*y)') == x**(2*y)\n"], "sample_1161": ["def test_print_Quaternion():\n    q = Quaternion(1, 2, 3, 4)\n    assert sstr(q) == \"1 + 2*i + 3*j + 4*k\"\n    assert sstr(-q) == \"-1 - 2*i - 3*j - 4*k\"\n    assert sstr(q + q) == \"2 + 4*i + 6*j + 8*k\"\n", "def test_sstrrebbe():\n    # Check if sstrrepr behaves as expected\n    expr = \"Hello, World!\"\n    assert sstrrepr(expr) == \"'Hello, World!'\"\n    assert sstr(expr) == expr\n\n    # Check for Str object\n    from sympy.printing import Str\n    str_obj = Str(expr)\n    assert sstr(str_obj) == expr\n    assert sstrrepr(str_obj) == \"Str('Hello, World!')\"\n", "def test_str_printer_for_ordinal():\n    from sympy.sets.fancysets import Naturals0\n    assert sstr(Naturals0()) == \"Naturals0\"\n    assert sstr(Ordinal(0)) == \"0\"\n    assert sstr(Ordinal(1)) == \"1\"\n    assert sstr(Ordinal(5)) == \"5\"\n    assert sstr(Ordinal(S.NegativeInfinity)) == \"-oo\"\n    assert sstr(Ordinal(S.Infinity)) == \"oo\"\n", "def test_print_Dummy():\n    assert sstr(Dummy('d')) == '_d'\n", "def test_str_printer_relational():\n    assert sstr(Eq(x, y)) == 'Eq(x, y)'\n    assert sstr(Ne(x, y)) == 'Ne(x, y)'\n    assert sstr(Xor(x, y)) == 'Xor(x, y)'\n    assert sstr(Equivalent(x, y)) == 'Equivalent(x, y)'\n    assert sstr(x > y) == 'x > y'\n    assert sstr(x < y) == 'x < y'\n    assert sstr(x >= y) == 'x >= y'\n    assert sstr(x <= y) == 'x <= y'\n"], "sample_1159": ["def test_failing_assumptions():\n    x = Symbol('x', real=True, positive=True)\n    y = Symbol('y')\n    assert failing_assumptions(6*x + y, real=True, positive=True) == {'positive': None, 'real': None}\n    assert failing_assumptions(x**2 - 1, positive=True) == {'positive': None}\n    assert failing_assumptions(x**2, positive=True) == {}\n    assert failing_assumptions(y, real=True) == {}\n", "def test_check_assumptions():\n    x = Symbol('x', real=True, positive=True)\n    y = Symbol('y')\n    i = Symbol('i', integer=True)\n    n = Symbol('n', negative=True)\n    p = Symbol('p', positive=True, integer=True, prime=True)\n    assert check_assumptions(x**2, real=True, positive=True)\n    assert not check_assumptions(x**2, real=True, negative=True)\n    assert check_assumptions(x**2 + 2*x, real=True, positive=True)\n    assert not check_assumptions(x**2 + 2*y, real=True, positive=True)\n    assert check_assumptions(i, integer=True)\n    assert not check_assumptions(i, real=True, integer=False)\n    assert check_assumptions(i, real=True)\n    assert not check_assumptions(i, integer=True, real=False)\n    assert check_assumptions(i, positive=True) is None\n    assert check_assumptions(n, positive=True) is False\n    assert check_assumptions(p, prime=True)\n    assert check_assumptions(p, composite=True) is False\n    assert check_assumptions(n, integer=True) is None\n    assert check_assumptions(i, prime=True) is None\n    assert check_assumptions(i, composite=True) is None\n    assert check_assumptions(i, nonnegative=True) is None\n    assert check_assumptions(n, nonnegative=True) is False\n    assert check_assumptions(p, nonnegative=True)\n", "def test_failing_assumptions():\n    x = Symbol('x', real=True, positive=True)\n    y = Symbol('y')\n    assert failing_assumptions(6*x + y, real=True, positive=True) == {'positive': None, 'real': None}\n    assert failing_assumptions(x**2 - 1, positive=True) == {'positive': None}\n    assert failing_assumptions(x**2, positive=True) == {}\n    assert failing_assumptions(y, real=True, positive=True) == {'real': None, 'positive': None}\n    assert failing_assumptions(I, real=True) == {'real': False}\n    assert failing_assumptions(pi, integer=True) == {'integer': False}\n", "def test_assumptions():\n    x = Symbol('x', real=True, positive=True)\n    y = Symbol('y', real=True, negative=True)\n    z = Symbol('z')\n    i = Symbol('i', integer=True)\n\n    assert assumptions(x) == {\n        'algebraic': True, 'commutative': True, 'complex': True,\n        'composite': False, 'even': False, 'extended_negative': False,\n        'extended_nonnegative': True, 'extended_nonpositive': False,\n        'extended_nonzero': True, 'extended_positive': True,\n        'extended_real': True, 'finite': True, 'hermitian': True,\n        'imaginary': False, 'infinite': False, 'integer': False,\n        'irrational': False, 'negative': False, 'noninteger': True,\n        'nonnegative': True, 'nonpositive': False, 'nonzero': True,\n        'odd': False, 'positive': True, 'prime': False, 'rational': False,\n        'real': True, 'transcendental': False, 'zero': False}\n\n    assert assumptions(y) == {\n        'algebraic': True, 'commutative': True, 'complex': True,\n        'composite': False, 'even': False, 'extended_negative': True,\n        'extended_nonnegative': False, 'extended_nonpositive': True,\n        'extended_nonzero': True, 'extended_positive': False,\n        'extended_real': True, 'finite': True, 'hermitian': True,\n        'imaginary': False, 'infinite': False, 'integer': False,\n        'irrational': False, 'negative': True, 'noninteger': True,\n        'nonnegative': False, 'nonpositive': True, 'nonzero': True,\n        'odd': False, 'positive': False, 'prime': False, 'rational': False,\n        'real': True, 'transcendental': False, 'zero': False}\n\n    assert assumptions(z) == {\n        'algebraic': None, 'commutative': True, 'complex': True,\n        'composite': None, 'even': None, 'extended_negative': None,\n        'extended_nonnegative': None, 'extended_nonpositive': None,\n        'extended_nonzero': None, 'extended_positive': None,\n        'extended_real': None, 'finite", "def test_assumptions():\n    x = Symbol('x', real=True)\n    z = Symbol('z')\n\n    # assumptions holds the known facts about a symbol\n    assert assumptions(x) == {'real': True, 'commutative': True,\n        'complex': True, 'finite': True, 'hermitian': True,\n        'extended_real': True}\n\n    # we don't know anything about 'z' by default\n    assert assumptions(z) == {'commutative': True}\n\n    # but we can get information about a generic Symbol\n    assert isinstance(assumptions(Symbol('x')), dict)\n\n    # we can pass a list of queries to assumptions\n    assert assumptions(x, _check=['real', 'integer', 'odd']) == {\n        'real': True, 'integer': None, 'odd': None}\n\n    # check_assumptions is similar, but it will also attempt\n    # to deduce values based on its input\n    assert check_assumptions(2*x, integer=True, real=True) == True\n    assert check_assumptions(2*x, integer=False, real=True) == None\n    assert check_assumptions(2*x, integer=False, real=False) == False\n    assert check_assumptions(x + I, real=False, complex=True) == True\n    assert check_assumptions(x + I, real=True, complex=True) == False\n    assert check_assumptions(x + I, real=True, complex=False) == False\n"], "sample_1160": ["def test_intersection_Naturals_Rationals():\n    assert intersection_sets(S.Naturals, S.Rationals) == S.Naturals\n    assert intersection_sets(S.Rationals, S.Naturals) == S.Naturals\n", "def test_intersection_sets():\n    # test intersection of sets\n    assert intersection_sets(Interval(1, 5), Interval(3, 7)) == Interval(3, 5)\n    assert intersection_sets(FiniteSet(1, 2, 3), FiniteSet(2, 3, 4)) == FiniteSet(2, 3)\n    assert intersection_sets(ImageSet(Lambda(x, x**2), Interval(0, 2)), Interval(1, 4)) == Interval(1, 4)\n    assert intersection_sets(ImageSet(Lambda(x, x**2), Interval(-2, 0)), Interval(1, 4)) == Interval(1, 4)\n    assert intersection_sets(ImageSet(Lambda(x, x**2), Interval(-2, 2)), Interval(1, 4)) == Interval(1, 4)\n", "def test_intersection():\n    x = Symbol('x')\n    assert Interval(1, 3).intersect(Interval(2, 4)) == Interval(2, 3)\n    assert Interval(1, 3).intersect(Interval(4, 5)) == S.EmptySet\n    assert Interval(1, 3, left_open=True).intersect(Interval(1, 3)) == Interval(1, 3, left_open=True)\n    assert Interval(1, 3).intersect(Interval(1, 3, left_open=True)) == Interval(1, 3, left_open=True)\n    assert Interval(1, 3, right_open=True).intersect(Interval(1, 3)) == Interval(1, 3, right_open=True)\n    assert Interval(1, 3).intersect(Interval(1, 3, right_open=True)) == Interval(1, 3, right_open=True)\n    assert Interval(1, 3, left_open=True, right_open=True).intersect(Interval(1, 3)) == Interval(1, 3, left_open=True, right_open=True)\n    assert Interval(1, 3).intersect(Interval(1, 3, left_open=True, right_open=True)) == Interval(1, 3, left_open=True, right_open=True)\n    assert Interval(1, 3).intersect(S.Reals) == Interval(1, 3)\n    assert Interval(1, 3).intersect(S.Integers) == {2}\n", "def test_intersection_sets_Range_Range():\n    assert intersection_sets(Range(5, 15, 5), Range(7, 12, 3)) == Range(10, 10, 5)\n    assert intersection_sets(Range(0, 10, 2), Range(0, 10, 2)) == Range(0, 10, 2)\n    assert intersection_sets(Range(0, 10, 2), Range(1, 10, 2)) == S.EmptySet\n    assert intersection_sets(Range(0, 10, 3), Range(1, 10, 2)) == S.EmptySet\n    assert intersection_sets(Range(0, 10, 2), Range(0, 10, 3)) == Range(0, 10, 6)\n    assert intersection_sets(Range(0, 10, 3), Range(0, 10, 4)) == Range(0, 10, 12)\n", "def test_intersection_sets_Range_Range():\n    assert intersection_sets(Range(1, 5, 2), Range(2, 6, 2)) == Range(2, 6, 2)\n    assert intersection_sets(Range(1, 5), Range(5, 10)) == S.EmptySet\n    assert intersection_sets(Range(1, 10, 3), Range(5, 10, 2)) == Range(7, 10, 6)\n    assert intersection_sets(Range(-oo, 5), Range(3, oo)) == Range(3, 5)\n    assert intersection_sets(Range(-oo, 3, 2), Range(2, oo)) == Range(2, 3, 2)\n"], "sample_1162": ["def test_derivative_subs():\n    x = Symbol('x')\n    expr = Derivative(x**2, x)\n    assert expr.subs(x, 2) == Derivative(4, 2)\n", "def test_coeff_isneg():\n    assert _coeff_isneg(-3*pi) == True\n    assert _coeff_isneg(S(3)) == False\n    assert _coeff_isneg(-oo) == True\n    assert _coeff_isneg(Symbol('n', negative=True)) == False  # coeff is 1\n    A = MatrixSymbol(\"A\", 3, 3)\n    assert _coeff_isneg(-sqrt(2)*A) == True\n    assert _coeff_isneg(sqrt(2)*A) == False\n", "def test_apply_must_not_define_new_variables():\n    f = Function('f')\n    x = Symbol('x')\n    assert f(x).diff(x).subs(f(x), x*y).atoms(Symbol) == {x, y}\n", "def test_Function__diff_wrt():\n    x = Symbol('x')\n    f = Function('f')\n    assert f(x)._diff_wrt\n    assert not f._diff_wrt\n", "def test_derivative_subs():\n    x, y = Symbol('x'), Symbol('y')\n    f = Function('f')\n    fx = f(x)\n    dfx = fx.diff(x)\n    assert dfx.subs(f(x), y) == dfx\n    assert dfx.subs(x, y) == f(y).diff(y)\n"], "sample_1163": ["def test_polarify():\n    x = Symbol('x')\n    y = Symbol('y')\n\n    expr = (-x)**y\n    polar_expr, reps = polarify(expr)\n    assert polar_expr == ((Symbol('_x')*exp_polar(I*pi))**Symbol('_y'))\n    assert reps == {Symbol('_x'): x, Symbol('_y'): y}\n\n    expr = x*(1+y)\n    assert polarify(expr, lift=True) == polar_lift(x)*polar_lift(y + 1)\n", "def test_re_im():\n    x, y = symbols('x y', real=True)\n    a, b = symbols('a b', complex=True)\n    assert re(2*E) == 2*E\n    assert re(2*I + 17) == 17\n    assert re(2*I) == 0\n    assert re(im(x) + x*I + 2) == 2\n    assert re(5 + I + 2) == 7\n    assert im(2*E) == 0\n    assert im(2*I + 17) == 2\n    assert im(x*I) == re(x)\n    assert im(re(x) + y) == im(y)\n    assert im(2 + 3*I) == 3\n    assert im(a + b) == im(b) + im(a)\n    assert im(a*b) == re(a)*im(b) + im(a)*re(b)\n    assert re(a + b) == re(b) + re(a)\n    assert re(a*b) == re(a)*re(b) - im(a)*im(b)\n", "def test_principal_branch_periodic_argument():\n    z = Symbol('z')\n    assert principal_branch(exp_polar(2*pi*I)*3, 2*pi) == 3*exp_polar(0)\n    assert periodic_argument(exp_polar(10*I*pi), 2*pi) == 0\n    assert periodic_argument(exp_polar(5*I*pi), 4*pi) == pi\n    assert periodic_argument(exp_polar(5*I*pi), pi) == 0\n    assert periodic_argument(z, 2*pi) == periodic_argument(z, 2*pi)\n", "def test_polar_lift():\n    # Test polar_lift with various inputs\n    assert polar_lift(4) == 4*exp_polar(0)\n    assert polar_lift(-4) == 4*exp_polar(I*pi)\n    assert polar_lift(-I) == exp_polar(-I*pi/2)\n    assert polar_lift(I + 2) == polar_lift(2 + I)\n    x = symbols('x')\n    assert polar_lift(4*x) == 4*polar_lift(x)\n\n    # Test polar_lift with period\n    assert polar_lift(exp_polar(10*I*pi), 2*pi) == exp_polar(0)\n    assert polar_lift(exp_polar(5*I*pi), 4*pi) == exp_polar(I*pi)\n    assert polar_lift(exp_polar(5*I*pi), pi) == exp_polar(0)\n\n    # Test polar_lift with principal_branch\n    assert polar_lift(principal_branch(exp_polar(2*pi*I), 2*pi)) == exp_polar(0)\n    assert polar_lift(principal_branch(exp_polar(5*I*pi), 4*pi)) == exp_polar(I*pi)\n", "def test_abs():\n    x, y = symbols('x y', real=True)\n    assert Abs(0) == 0\n    assert Abs(x) == Abs(x)\n    assert Abs(Abs(x)) == Abs(x)\n    assert Abs(-x) == Abs(x)\n    assert Abs(-5) == 5\n    assert N_equals(Abs(-1 - 2*I), sqrt(5))\n    assert Abs(-oo) == oo\n    assert Abs(oo) == oo\n    assert Abs(zoo) == zoo\n    assert Abs(nan) == nan\n    assert Abs(2*x) == 2*Abs(x)\n    assert Abs(x*y) == Abs(x)*Abs(y)\n    assert unchanged(Abs, x + y)\n    assert unchanged(Abs, x - y)\n    assert unchanged(Abs, x + 2*y)\n    assert unchanged(Abs, y - x)\n    assert unchanged(Abs, 2*y - x)\n    assert unchanged(Abs, I*x)\n    assert unchanged(Abs, I*y)\n    assert Abs(-2) == 2\n    assert Abs(complex(-2, 0)) == 2\n    assert Abs(3 - 4*I) == 5\n    assert unchanged(Abs, 1 + I)\n    assert unchanged(Abs, I)\n    assert unchanged(Abs, I + 1)\n    assert unchanged(Abs, I + 2)\n    assert unchanged(Abs, I - 1)\n    assert unchanged(Abs, I - 2)\n    assert unchanged(Abs, I + I)\n    assert unchanged(Abs, I + I + 1)\n    assert unchanged(Abs, I + I - 1)\n    assert unchanged(Abs, I - I + 1)\n    assert unchanged(Abs, I - I - 1)\n    assert unchanged(Abs, I + I + 2)\n    assert unchanged(Abs, I + I - 2)\n    assert unchanged(Abs, I - I + 2)\n    assert unchanged(Abs, I - I - 2)\n    assert unchanged(Abs, I + I + 3)\n    assert unchanged(Abs, I + I - 3)\n    assert unchanged(Abs, I - I + 3)\n    assert unchanged(Abs, I - I - 3)\n"], "sample_1165": ["def test_rotate_point():\n    q1 = Quaternion(cos(phi/2), 0, 0, sin(phi/2))\n    q2 = Quaternion.from_axis_angle((0, 0, 1), phi)\n    assert Quaternion.rotate_point((1, 1, 1), q1) == Quaternion.rotate_point((1, 1, 1), q2)\n    assert Quaternion.rotate_point((1, 1, 1), (0, 0, 1), phi) == Quaternion.rotate_point((1, 1, 1), q2)\n", "def test_from_axis_angle():\n    q = Quaternion.from_axis_angle((1, 0, 0), pi/2)\n    assert q == Quaternion(cos(pi/4), sin(pi/4), 0, 0)\n\n    q = Quaternion.from_axis_angle((0, 1, 0), pi/4)\n    assert q == Quaternion(cos(pi/8), 0, sin(pi/8), 0)\n\n    q = Quaternion.from_axis_angle((0, 0, 1), pi/3)\n    assert q == Quaternion(cos(pi/6), 0, 0, sin(pi/6))\n", "def test_pow_cos_sin():\n    q = Quaternion(1, 2, 3, 4)\n    assert q.pow_cos_sin(4) == 900*cos(4*acos(sqrt(30)/30)) + \\\n        1800*sqrt(29)*sin(4*acos(sqrt(30)/30))/29*i + \\\n        2700*sqrt(29)*sin(4*acos(sqrt(30)/30))/29*j + \\\n        3600*sqrt(29)*sin(4*acos(sqrt(30)/30))/29*k\n", "def test_to_axis_angle():\n    q = Quaternion(cos(phi/2), 0, 0, sin(phi/2))\n    (axis, angle) = q.to_axis_angle()\n    assert axis == (0, 0, 1)\n    assert angle == phi\n", "def test_quaternion_rotate_point():\n    q = Quaternion(cos(phi/2), 0, 0, sin(phi/2))\n    axis, angle = q.to_axis_angle()\n    assert trigsimp(Quaternion.rotate_point((1, 1, 1), q)) == trigsimp(Quaternion.rotate_point((1, 1, 1), (axis, angle)))\n    assert Quaternion.rotate_point((1, 0, 0), Quaternion(1, 0, 0, 0)) == (1, 0, 0)\n"], "sample_1164": ["def test_cg():\n    a, b, c, alpha, beta, gamma = symbols('a b c alpha beta gamma')\n    assert pretty(CG(a, alpha, b, beta, c, gamma)) == \\\n        'C       \\n' \\\n        'a alpha \\n' \\\n        'b beta  \\n' \\\n        'c gamma '\n    assert upretty(CG(a, alpha, b, beta, c, gamma)) == \\\n        'C       \\n' \\\n        'a \u03b1    \\n' \\\n        'b \u03b2    \\n' \\\n        'c \u03b3    '\n    assert latex(CG(a, alpha, b, beta, c, gamma)) == \\\n        r'C^{c,\\gamma}_{a,\\alpha,b,\\beta}'\n\n    assert CG(S(1)/2, S(1)/2, S(1)/2, S(1)/2, 1, 1).doit() == 1\n    assert CG(S(1)/2, S(1)/2, S(1)/2, -S(1)/2, 1, 0).doit() == sqrt(2)/2\n    assert CG(S(1)/2, S(1)/2, S(1)/2, -S(1)/2, 0, 0).doit() == sqrt(2)/2\n    assert CG(S(1)/2, -S(1)/2, S(1)/2, S(1)/2, 1, 0).doit() == sqrt(2)/2\n    assert CG(S(1)/2, -S(1)/2, S(1)/2, -S(1)/2, 1, -1).doit() == 1\n    assert CG(S(1)/2, S(1)/2, S(1)/2, -S(1)/2, 0, 0).doit() == sqrt(2)/2\n\n    assert sT(CG(S(1)/2, S(1)/2, S(1)/2, -S(1)/2, 1, 0),\n              'CG(S(1)/2, S(1)/2, S(1)/2, -S(1)/2, 1, 0)')\n", "def test_cg_simp_add():\n    a, alpha, b, beta, c, gamma = symbols('a alpha b beta c gamma')\n    cg1 = CG(a, alpha, b, beta, c, gamma)\n    cg2 = CG(a, alpha, b, beta, c, gamma)\n    assert cg_simp(cg1 + cg2) == cg1 + cg2\n    assert cg_simp(cg1 + cg1) == 2 * cg1\n", "def test_cg_printing():\n    # Test ASCII and Unicode pretty-printing\n    j1, j2, j3, m1, m2, m3 = symbols('j1 j2 j3 m1 m2 m3')\n    ascii_str = pretty(CG(j1, m1, j2, m2, j3, m3))\n    ucode_str = upretty(CG(j1, m1, j2, m2, j3, m3))\n    latex_str = latex(CG(j1, m1, j2, m2, j3, m3))\n    assert ascii_str == \"C{j1,m1;j2,m2;j3,m3}\"\n    assert ucode_str == \"C{j\u2081,m\u2081;j\u2082,m\u2082;j\u2083,m\u2083}\"\n    assert latex_str == r'C^{j_{1},m_{1}}_{j_{2},m_{2},j_{3},m_{3}}'\n\n    # Test LaTeX and srepr printing\n    wigner_str = srepr(Wigner3j(j1, m1, j2, m2, j3, m3))\n    wigner_latex = latex(Wigner3j(j1, m1, j2, m2, j3, m3))\n    assert wigner_str == \"Wigner3j(j1, m1, j2, m2, j3, m3)\"\n    assert wigner_latex == r'\\left(\\begin{array}{ccc} j_{1} & j_{2} & j_{3} \\\\ m_{1} & m_{2} & m_{3} \\end{array}\\right)'\n", "def test_Wigner6j():\n    # Test Wigner6j\n    j1, j2, j12, j3, j, j23 = symbols('j1 j2 j12 j3 j j23')\n    wigner6j = Wigner6j(j1, j2, j12, j3, j, j23)\n    assert wigner6j.j1 == j1\n    assert wigner6j.j2 == j2\n    assert wigner6j.j12 == j12\n    assert wigner6j.j3 == j3\n    assert wigner6j.j == j\n    assert wigner6j.j23 == j23\n    assert wigner6j.is_symbolic\n    assert wigner6j._pretty(None) is not None\n    assert latex(wigner6j) is not None\n", "def test_Wigner3j_printing():\n    # Test Wigner3j pretty printing\n    w3j = Wigner3j(6,0,4,0,2,0)\n    assert pretty(w3j) == (\n        \"  6 0 4 \\n\"\n        \"  0 0 2 \\n\"\n        \"  2 0 0 \\n\"\n        \"{'@cd': '\\\\left(\\\\begin{array}{ccc} 6 & 4 & 6 \\\\\\\\ 0 & 0 & 2 \\\\end{array}\\\\right)', '@el': '\\\\left(\\\\begin{array}{ccc} 6 & 4 & 2 \\\\\\\\ 0 & 0 & 0 \\\\end{array}\\\\right)', '@sl': 1}\"\n        )\n    assert upretty(w3j) == (\n        \"  6 0 4 \\n\"\n        \"  0 0 2 \\n\"\n        \"  2 0 0 \\n\"\n        \"{'@cd': '\\\\left(\\\\begin{array}{ccc} 6 & 4 & 6 \\\\\\\\ 0 & 0 & 2 \\\\end{array}\\\\right)', '@el': '\\\\left(\\\\begin{array}{ccc} 6 & 4 & 2 \\\\\\\\ 0 & 0 & 0 \\\\end{array}\\\\right)', '@sl': 1}\"\n        )\n    assert latex(w3j) == r'\\left(\\begin{array}{ccc} 6 & 4 & 2 \\\\ 0 & 0 & 0 \\end{array}\\right)'\n"], "sample_1166": ["def test_Monomial_pow():\n    x, y, z = symbols('x y z')\n    m = Monomial((2, 3, 4), (x, y, z))\n    assert m**0 == Monomial((0, 0, 0), (x, y, z))\n    assert m**1 == m\n    assert m**2 == Monomial((4, 6, 8), (x, y, z))\n    raises(ValueError, lambda: m**-1)\n", "def test_Monomial_as_expr():\n    p, q, r = symbols('p q r')\n    M = Monomial((2, 1, 3), (p, q, r))\n    assert M.as_expr() == p**2*q*r**3\n    assert M.as_expr(p, q, r) == p**2*q*r**3\n", "def test_Monomial_pow():\n    x, y, z = symbols('x y z')\n    m = Monomial((2, 3, 4), (x, y, z))\n    assert m**0 == Monomial((0, 0, 0), (x, y, z))\n    assert m**1 == m\n    assert m**2 == Monomial((4, 6, 8), (x, y, z))\n    raises(ValueError, lambda: m**-1)\n", "def test_Monomial_pow():\n    x, y = symbols('x y')\n    m = Monomial((2, 3), (x, y))\n    assert m**0 == Monomial((0, 0), (x, y))\n    assert m**1 == m\n    assert m**2 == Monomial((4, 6), (x, y))\n    raises(ValueError, lambda: m**-1)\n    raises(ValueError, lambda: m**S(1)/2)\n", "def test_monomial_pow():\n    a = (1, 2, 3)\n    assert monomial_pow(a, 0) == (0, 0, 0)\n    assert monomial_pow(a, 1) == a\n    assert monomial_pow(a, 2) == (2, 4, 6)\n    assert monomial_pow(a, 3) == (3, 6, 9)\n\n    a = Monomial((1, 2, 3))\n    assert a**0 == Monomial((0, 0, 0))\n    assert a**1 == a\n    assert a**2 == Monomial((2, 4, 6))\n    assert a**3 == Monomial((3, 6, 9))\n\n    raises(ValueError, lambda: monomial_pow(a, -1))\n    raises(ValueError, lambda: a**(-1))\n"], "sample_1167": ["def test_LatexPrinter_settings():\n    # Issue #11772\n    x = Symbol(\"x\")\n    with raises(ValueError):\n        latex(x, mode='invalidmode')\n\n    # Issue #12886\n    expr = Derivative(x**3, x, x)\n    assert latex(expr) == r\"x^{3}^{\\prime\\prime}\"\n    assert latex(expr, symbol_names={x: 'x_i'}) == r\"x_i^{3}^{\\prime\\prime}\"\n    assert latex(expr, long_frac_ratio=None) == r\"x^{3}^{\\prime\\prime}\"\n    assert latex(expr, parenthesize_super=False) == r\"x^{3}^{\\prime\\prime}\"\n    assert latex(expr, fold_func_brackets=True) == r\"x^{3}^{\\prime\\prime}\"\n    assert latex(expr, mode='plain') == r\"x^{3}^{\\prime\\prime}\"\n    assert latex(expr, mode='inline') == r\"$x^{3}^{\\prime\\prime}$\"\n    assert latex(expr, mode='equation') == r\"\\begin{equation}x^{3}^{\\prime\\prime}\\end{equation}\"\n    assert latex(expr, mode='equation*') == r\"\\begin{equation*}x^{3}^{\\prime\\prime}\\end{equation*}\"\n", "def test_mimoparallel():\n    s = symbols('s')\n    G11, G12, G21, G22 = symbols('G11 G12 G21 G22', cls=TransferFunction)\n    P = MIMOParallel(G11, G12, G21, G22)\n    assert latex(P) == r'\\begin{bmatrix}G_{11} & G_{12} \\\\ G_{21} & G_{22}\\end{bmatrix}'\n    assert latex(MIMOParallel(G11, G21)) == r'\\begin{bmatrix}G_{11} \\\\ G_{21}\\end{bmatrix}'\n    assert latex(MIMOParallel(1, 2/s, 3/s**2, 4/s**3)) == \\\n        r'\\begin{bmatrix}1 & \\frac{2}{s} \\\\ \\frac{3}{s^{2}} & \\frac{4}{s^{3}}\\end{bmatrix}'\n", "def test_LatexPrinter_settings():\n    from io import StringIO\n    import sys\n\n    # save stdout\n    stdout = sys.stdout\n\n    printer = LatexPrinter()\n    assert printer._settings['mat_delim'] == \"[\"\n\n    printer = LatexPrinter({\"mat_delim\": \"(\"})\n    assert printer._settings['mat_delim'] == \"(\"\n\n    # create own settings\n    settings = {\"mat_delim\": \"(\"}\n    printer = LatexPrinter(settings)\n    assert printer._settings['mat_delim'] == \"(\"\n\n    assert latex(x**2, mode='equation') == \"\\\\begin{equation}x^{2}\\\\end{equation}\"\n    assert latex(x**2, mode='equation*') == \"\\\\begin{equation*}x^{2}\\\\end{equation*}\"\n\n    printer = LatexPrinter({\"mode\": \"inline\"})\n    assert printer._settings['mode'] == \"inline\"\n\n    raises(ValueError, lambda: LatexPrinter({\"mode\": \"not a mode\"}))\n\n    assert latex(x**2, itex=True) == r\"$$x^{2}$$\"\n\n    assert latex(2*x, mul_symbol='ldot') == r'2\\,.\\,x'\n\n    assert latex(2*x, mul_symbol='times') == r'2 \\times x'\n\n    printer = LatexPrinter(settings={\"mul_symbol\": \"times\"})\n    assert printer._settings['mul_symbol'] == \"times\"\n    assert latex(2*x, mul_symbol=None) == r'2 x'\n    assert printer._settings['mul_symbol_latex'] == r\" \"\n    assert latex(2*3, mul_symbol=None) == r'2 \\cdot 3'\n    assert latex(2*3, mul_symbol=\"ldot\") == r'2 \\,.\\, 3'\n    assert latex(2*3, mul_symbol='times') == r'2 \\times 3'\n    assert printer._settings['mul_symbol_latex_numbers'] == r\" \\cdot \"\n\n    assert latex(2*x, long_frac_ratio=None) == r'\\frac{2 x}{1}'\n\n    assert latex(2*x, long_frac_ratio=1) == r'2 x'\n\n    assert latex(2*x, long_frac_ratio=2) == r'2 x'\n\n    assert latex(2*x, long_frac_ratio=3) == r'\\frac{2 x}{1}'\n\n    assert", "def test_LatexPrinter_settings():\n    # Test that global settings are passed to the LatexPrinter\n    lp = LatexPrinter({'mul_symbol_latex': r'\\times'})\n    assert lp._settings['mul_symbol_latex'] == r'\\times'\n    assert latex(x*y, mul_symbol_latex=r'\\times') == r'x \\times y'\n", "compilation error"], "sample_1168": ["def test_roundrobin():\n    assert list(roundrobin('ABC', 'D', 'EF')) == ['A', 'D', 'E', 'B', 'F', 'C']\n    assert list(roundrobin([1, 2, 3], [4, 5], [6])) == [1, 4, 6, 2, 5, 3]\n    assert list(roundrobin('abc', [1, 2, 3])) == ['a', 1, 'b', 2, 'c', 3]\n", "def test_roundrobin():\n    from sympy.utilities.iterables import roundrobin\n    assert list(roundrobin('ABC', 'D', 'EF')) == ['A', 'D', 'E', 'B', 'F', 'C']\n    assert list(roundrobin([], 'D', 'EF')) == ['D', 'E', 'F']\n    assert list(roundrobin('ABC', [], 'EF')) == ['A', 'E', 'B', 'F', 'C']\n    assert list(roundrobin('ABC', 'D', [])) == ['A', 'D', 'B', 'C']\n", "def test_permute_signs():\n    assert list(permute_signs((0, 1, 2))) == [(0, 1, 2), (0, -1, 2), (0, 1, -2), (0, -1, -2)]\n    assert list(permute_signs((1, 2))) == [(1, 2), (-1, 2), (1, -2), (-1, -2)]\n    assert list(permute_signs((1, 0, 0, 1))) == [(1, 0, 0, 1), (-1, 0, 0, 1), (1, 0, 0, -1), (-1, 0, 0, -1)]\n", "def test_roundrobin():\n    from sympy.utilities.iterables import roundrobin\n\n    a = [1, 2, 3]\n    b = ['a', 'b', 'c', 'd']\n    c = ['x', 'y']\n\n    result = list(roundrobin(a, b, c))\n    assert result == [1, 'a', 'x', 2, 'b', 'y', 3, 'c', 'd']\n", "def test_permute_signs():\n    assert list(permute_signs((1, 2))) == [(1, 2), (-1, 2), (1, -2), (-1, -2)]\n    assert list(permute_signs((1, 2, 0))) == [(1, 2, 0), (-1, 2, 0), (1, -2, 0), (-1, -2, 0)]\n    assert list(permute_signs((1, 1, 1))) == [(1, 1, 1), (-1, 1, 1), (1, -1, 1), (1, 1, -1), (-1, -1, 1), (-1, 1, -1), (1, -1, -1), (-1, -1, -1)]\n"], "sample_1169": ["def test_Commutator_doit():\n    x = Symbol('x', real=True)\n    y = Symbol('y', real=True)\n    p = Symbol('p', real=True)\n    q = Symbol('q', real=True)\n    a = Symbol('a', above_fermi=True)\n    i = Symbol('i', below_fermi=True)\n    assert Commutator(2*x, 3*y).doit() == 0\n    assert Commutator(B(x), B(y)).doit() == 0\n    assert Commutator(Bd(x), Bd(y)).doit() == 0\n    assert Commutator(B(x), Bd(y)).doit() == KroneckerDelta(x, y)\n    assert Commutator(Bd(x), B(y)).doit() == -KroneckerDelta(x, y)\n    assert Commutator(F(p)*Fd(q), F(i)).doit(wicks=True) == \\\n        -KroneckerDelta(i, p)*CreateFermion(q) + KroneckerDelta(i, q)*CreateFermion(p)\n", "def test_commutator():\n    p, q, r, s = symbols('p,q,r,s')\n    a, b, c, d = symbols('a,b,c,d', above_fermi=True)\n    i, j, k, l = symbols('i,j,k,l', below_fermi=True)\n    u, v, w, x = symbols('u,v,w,x', above_fermi=True, cls=Dummy)\n\n    # [a, b] == a*b - b*a\n    assert Commutator(a, b).doit(wicks=False) == a*b - b*a\n\n    # [CreateFermion(p)*CreateFermion(q), AnnihilateFermion(i)]\n    assert Commutator(Fd(p)*Fd(q), F(i)).doit(wicks=True) == \\\n        -KroneckerDelta(i, p)*CreateFermion(q) + \\\n        KroneckerDelta(i, q)*CreateFermion(p)\n\n    # [CreateFermion(p)*AnnihilateFermion(q), AnnihilateFermion(i)]\n    assert Commutator(Fd(p)*F(q), F(i)).doit(wicks=True) == \\\n        KroneckerDelta(i, q)*AnnihilateFermion(p) - \\\n        KroneckerDelta(i, p)*AnnihilateFermion(q)\n\n    # [CreateFermion(p), AnnihilateFermion(i)*AnnihilateFermion(j)]\n    assert Commutator(Fd(p), F(i)*F(j)).doit(wicks=True) == \\\n        -AnnihilateFermion(i)*KroneckerDelta(j, p) + \\\n        AnnihilateFermion(j)*KroneckerDelta(i, p)\n", "def test_FermionicOperator():\n    i = Symbol('i', below_fermi=True)\n    a = Symbol('a', above_fermi=True)\n    p = Symbol('p')\n\n    assert Fd(i).is_only_q_annihilator\n    assert Fd(a).is_only_q_creator\n    assert F(i).is_only_q_creator\n    assert F(a).is_only_q_annihilator\n\n    assert not Fd(p).is_only_q_annihilator\n    assert not Fd(p).is_only_q_creator\n    assert not F(p).is_only_q_annihilator\n    assert not F(p).is_only_q_creator\n", "def test_wicks():\n    p, q, r, s = symbols('p,q,r,s', cls=Dummy)\n    term = Fd(p)*Fd(q)*F(r)*F(s)\n    term2 = wicks(term)\n    assert term2.args[0] == -KroneckerDelta(_i, r)*KroneckerDelta(_i, s)*KroneckerDelta(p, q)\n", "def test_substitute_dummies_with_new_indices():\n    p,q = symbols('p q', cls=Dummy)\n    f = Function('f')\n    expr = f(p)*f(q) + f(q)*f(p)\n    new_dummies = { 'general':'xy' }\n    result = substitute_dummies(expr, new_indices=True, pretty_indices=new_dummies)\n    assert result == 2*f(Symbol('x',above_fermi=False,below_fermi=False,commutative=False, dummy=True))*f(Symbol('y',above_fermi=False,below_fermi=False,commutative=False,dummy=True))\n"], "sample_1170": ["def test_print_MatrixSlice():\n    M = MatrixSymbol('M', 4, 4)\n    assert sstr(M[:, 1]) == \"M[:, 1]\"\n    assert sstr(M[0, :]) == \"M[0, :]\"\n    assert sstr(M[0, 1:]) == \"M[0, 1:]\"\n    assert sstr(M[0:2, 1:3]) == \"M[0:2, 1:3]\"\n    assert sstr(M[0:2, :]) == \"M[0:2, :]\"\n    assert sstr(M[:, 1:3]) == \"M[:, 1:3]\"\n", "def test_sstr_UnaryRelational():\n    assert sstr(Rel(x < y, False)) == \"(x < y).negate\"\n", "def test_StrPrinter_print_DMP():\n    p = Poly(z*x**2 + y*x + x**2 + x + 1, x)\n    assert sstr(p) == \"Poly(x**2*(z + 1) + x*(y + 1) + 1, x, domain='ZZ[y,z]')\"\n", "def test_StrPrinter_print_TensorIndex():\n    a, b, c, d = symbols('a b c d', tensor_index=True)\n    assert sstr(a) == 'a'\n    assert sstr(b) == 'b'\n    assert sstr(c) == 'c'\n    assert sstr(d) == 'd'\n", "def test_StrPrinter_print_AppliedBinaryRelation():\n    x, y = symbols('x y')\n    u = Symbol('u', real=True)\n    v = Symbol('v', real=True)\n    assert sstr(Q.is_true(u)) == \"Q.is_true(u)\"\n    assert sstr(Q.is_const(u)) == \"Q.is_const(u)\"\n    assert sstr(Q.is_rational(u)) == \"Q.is_rational(u)\"\n    assert sstr(Q.is_real(u)) == \"Q.is_real(u)\"\n    assert sstr(Q.is_positive(u)) == \"Q.is_positive(u)\"\n    assert sstr(Q.is_integer(u)) == \"Q.is_integer(u)\"\n    assert sstr(Q.is_algebraic(u)) == \"Q.is_algebraic(u)\"\n    assert sstr(Q.is_negative(u)) == \"Q.is_negative(u)\"\n    assert sstr(Q.is_nonpositive(u)) == \"Q.is_nonpositive(u)\"\n    assert sstr(Q.is_nonzero(u)) == \"Q.is_nonzero(u)\"\n    assert sstr(Q.is_zero(u)) == \"Q.is_zero(u)\"\n    assert sstr(Q.is_commutative(u)) == \"Q.is_commutative(u)\"\n    assert sstr(Q.is_prime(u)) == \"Q.is_prime(u)\"\n    assert sstr(Q.is_composite(u)) == \"Q.is_composite(u)\"\n    assert sstr(Q.is_number(u)) == \"Q.is_number(u)\"\n    assert sstr(Q.is_even(u)) == \"Q.is_even(u)\"\n    assert sstr(Q.is_odd(u)) == \"Q.is_odd(u)\"\n    assert sstr(Q.is_imaginary(u)) == \"Q.is_imaginary(u)\"\n    assert sstr(Q.is_nonnegative(u)) == \"Q.is_nonnegative(u)\"\n    assert sstr(Q.is_gaussian_integer(u)) == \"Q.is_gaussian_integer(u)\"\n    assert sstr(Q.is_negative_one(u)) == \"Q.is_negative_one(u)\"\n    assert sstr(Q.is_positive_one(u)) == \"Q.is_positive_one(u)\"\n    assert sstr(Q.is_noninteger(u)) == \"Q.is_noninteger(u)\"\n    assert sstr(Q.is_transcendental(u)) == \"Q.is_transcendental(u)\"\n    assert sstr(Q.is_irrational(u)) == \"Q.is_irrational(u)\"\n    assert sstr(Q.is_symmetric(u, v)) == \"Q.is_symmetric(u, v)\"\n    assert sstr(Q.is"], "sample_1171": ["compilation error", "def test_ComplexRegion_polar():\n    rset = Interval(0, 1)\n    thetaset = Interval(0, pi)\n    region = ComplexRegion(rset * thetaset, polar=True)\n    assert 1 + I in region\n    assert 1 - I not in region\n    assert region._contains(1 + I) == True\n    assert region._contains(1 - I) == False\n    assert region._contains(2 + I) == False\n", "def test_ComplexRegion_contains():\n    a, b = Interval(1, 3), Interval(4, 6)\n    c1 = ComplexRegion(a*b)\n    assert 2 + 5*I in c1\n    assert 2 + 3*I not in c1\n    assert 1 + 4*I in c1\n    assert 4 + 4*I not in c1\n\n    c2 = ComplexRegion(Union(a*b, b*a))\n    assert 2 + 5*I in c2\n    assert 2 + 3*I not in c2\n    assert 1 + 4*I in c2\n    assert 4 + 4*I in c2\n\n    c3 = ComplexRegion(Interval(0, 1)*Interval(0, 2*pi), polar=True)\n    assert 1 in c3\n    assert -1 in c3\n    assert I in c3\n    assert -I in c3\n    assert (1 + I)/sqrt(2) in c3\n    assert 2 not in c3\n    assert 1 + 2*I not in c3\n", "def test_ComplexRegion():\n    # Test the ComplexRegion with real part as Interval and\n    # imaginary part as FiniteSet\n    a = Interval(2, 5)\n    b = FiniteSet(2, 4)\n    c = ComplexRegion(a * b)\n    d = Interval(2, 5)\n    e = FiniteSet(2, 4)\n    f = ComplexRegion(Union(d*b, e*b))\n    assert isinstance(c, ComplexRegion)\n    assert isinstance(f, ComplexRegion)\n\n    # ComplexRegion with negative real part and imaginary part as ointerval\n    g = Interval(-5, 0)\n    h = Interval(-2, 2)\n    i = ComplexRegion(g*h)\n    assert isinstance(i, ComplexRegion)\n\n    # Test the ComplexRegion with real part as FiniteSet and\n    # imaginary part as Interval\n    j = FiniteSet(1, 3, 5)\n    k = Interval(-2, 2)\n    l = ComplexRegion(j * k)\n    assert isinstance(l, ComplexRegion)\n\n    # ComplexRegion with negative real part as FiniteSet and\n    # imaginary part as Interval\n    m = FiniteSet(-3, -2, -1)\n    n = Interval(0, 5)\n    o = ComplexRegion(m*n)\n    assert isinstance(o, ComplexRegion)\n\n    # Test the ComplexRegion with real part as Interval and\n    # imaginary part as Interval\n    p = Interval(2, 5)\n    q = Interval(0, 5)\n    r = ComplexRegion(p * q)\n    assert isinstance(r, ComplexRegion)\n\n    # Test the ComplexRegion with real part as Interval and\n    # imaginary part as Interval with negative values\n    s = Interval(-5, 0)\n    t = Interval(-5, 0)\n    u = ComplexRegion(s*t)\n    assert isinstance(u, ComplexRegion)\n", "def test_rangesubber():\n    assert Range(10, 15, 2).subs({10: 2}) == Range(2, 7, 2)\n    raises(ValueError, lambda: Range(10, 15, 2).subs({2: 3}))\n    assert Range(10, 15, 2).subs({15: 3}) == Range(10, -7, 2)\n"], "sample_1173": ["def test_evaluateFalse_transformer():\n    expr = \"2*x + 3*y - 4*z\"\n    node = ast.parse(expr)\n    transformed_node = EvaluateFalseTransformer().visit(node)\n    transformed_expr = ast.unparse(transformed_node)\n    assert \"Add\" in transformed_expr and \"Mul\" in transformed_expr\n", "def test_factorial_notation():\n    assert parse_expr('2!', transformations=standard_transformations) == factorial(2)\n    assert parse_expr('2*3!', transformations=standard_transformations) == 2 * factorial(3)\n    assert parse_expr('2*3*4!', transformations=standard_transformations) == 2 * 3 * factorial(4)\n    assert parse_expr('2*3!!', transformations=standard_transformations) == 2 * factorial2(3)\n", "def test_apply_functions():\n    # Test that function applications are parsed correctly\n    assert parse_expr('f(x)', transformations=standard_transformations) == Function('f')(Symbol('x'))\n    assert parse_expr('f(x, y)', transformations=standard_transformations) == Function('f')(Symbol('x'), Symbol('y'))\n    assert parse_expr('f(x)(y)', transformations=standard_transformations) == Function('f')(Symbol('x'))(Symbol('y'))\n", "def test_factorial_notation():\n    assert parse_expr('2!', transformations=standard_transformations) == 2\n    assert parse_expr('factorial(2)', transformations=standard_transformations) == 2\n    assert parse_expr('2*factorial(3)', transformations=standard_transformations) == 12\n    assert parse_expr('2*3!', transformations=standard_transformations) == 12\n    raises(TokenError, lambda: parse_expr('2!!', transformations=standard_transformations))\n    assert parse_expr('factorial2(2)', transformations=standard_transformations) == 2\n    assert parse_expr('2*factorial2(3)', transformations=standard_transformations) == 12\n    assert parse_expr('2*3!!', transformations=standard_transformations) == 12\n", "def test_repeated_decimals():\n    assert parse_expr(\"0.2[1]\", transformations=standard_transformations) == Rational(19, 90)\n    assert parse_expr(\"0.[1]\", transformations=standard_transformations) == Rational(1, 9)\n    assert parse_expr(\"0.2[145]\", transformations=standard_transformations) == Rational(214, 999)\n"], "sample_1172": ["def test_solve_biquadratic():\n    f = Poly(x**2 + y**2 - 1, x, y, domain='ZZ')\n    g = Poly(x + y - 1, x, y, domain='ZZ')\n    assert solve_biquadratic(f, g, parallel_poly_from_expr([f, g], x, y)[1]) == \\\n        [(0, -1), (0, 1), (1, 0)]\n\n    assert solve_biquadratic(Poly(x**2 + y**2 - 2, x, y, domain='ZZ'),\n                             Poly(x - y, x, y, domain='ZZ'),\n                             parallel_poly_from_expr([x**2 + y**2 - 2, x - y], x, y)[1]) == \\\n        [(-1, -1), (-1, 1), (1, -1), (1, 1)]\n\n    raises(SolveFailed, lambda: solve_biquadratic(Poly(x + y, x, y, domain='ZZ'),\n                                                  Poly(x, x, y, domain='ZZ'),\n                                                  parallel_poly_from_expr([x + y, x], x, y)[1]))\n", "def test_solve_generic():\n    x, y = symbols('x y')\n    assert solve_generic([Poly(x - y + 5, x, y), Poly(x + y - 3, x, y)], Options((x, y), {'domain': 'ZZ'})) == [(-1, 4)]\n    assert solve_generic([Poly(x - 2*y + 5, x, y), Poly(2*x - y - 3, x, y)], Options((x, y), {'domain': 'ZZ'})) == [(11/3, 13/3)]\n    assert solve_generic([Poly(x**2 + y, x, y), Poly(x + y*4, x, y)], Options((x, y), {'domain': 'ZZ'})) == [(0, 0), (1/4, -1/16)]\n", "def test_solve_biquadratic():\n    x, y = symbols('x y')\n    f = Poly(x**2 + y**2 - 1, x, y, domain=QQ)\n    g = Poly(x + y - 2, x, y, domain=QQ)\n\n    with raises(SolveFailed):\n        solve_biquadratic(f, g, dict=True)\n\n    assert solve_biquadratic(f, g) == [(1, 0), (0, 1)]\n", "def test_solve_biquadratic():\n    f = Poly(x**2 + y**2 - 1)\n    g = Poly(x**2 - y**2)\n    assert solve_biquadratic(f, g, dict=True) == [{x: -sqrt(2)/2, y: -sqrt(2)/2}, {x: sqrt(2)/2, y: -sqrt(2)/2}, {x: -sqrt(2)/2, y: sqrt(2)/2}, {x: sqrt(2)/2, y: sqrt(2)/2}]\n\n    f = Poly(x**2 + 2*y**2 - 3)\n    g = Poly(x + y - 1)\n    assert solve_biquadratic(f, g, dict=True) == [{x: 3/7, y: 4/7}, {x: 1, y: 0}, {x: 0, y: -sqrt(3)/sqrt(2)}, {x: 0, y: sqrt(3)/sqrt(2)}]\n", "def test_solve_biquadratic():\n    # two bivariate quadratic polynomials\n    f = Poly(x**2 + 2*y**2 - 3, x, y)\n    g = Poly(x**2 - y**2 + 2, x, y)\n    sol = solve_biquadratic(f, g, None)\n    assert len(sol) == 4\n    for s in sol:\n        assert f.subs({x: s[0], y: s[1]}) == 0\n        assert g.subs({x: s[0], y: s[1]}) == 0\n\n    # with domain\n    domain = QQ\n    sol = solve_biquadratic(f, g, domain)\n    assert len(sol) == 4\n    for s in sol:\n        assert f.subs({x: s[0], y: s[1]}) == 0\n        assert g.subs({x: s[0], y: s[1]}) == 0\n        assert s[0].is_Rational or s[0].is_Add\n        assert s[1].is_Rational or s[1].is_Add\n"], "sample_1174": ["def test_re():\n    x, y = symbols('x y', real=True)\n    z = Symbol('z')\n    assert re(2*E) == 2*E\n    assert re(2*I + 17) == 17\n    assert re(2*I) == 0\n    assert re(im(x) + x*I + 2) == 2\n    assert re(5 + I + 2) == 7\n    assert re(sqrt(2) + sqrt(8)*I) == sqrt(2)\n    assert re(3 + 4*I) == 3\n    assert re(-3 + 4*I) == -3\n    assert re(Eq(3, 4*I)) == Eq(3, 0)\n    assert re((1 + 2*I)/(3 + 4*I)) == (11/25, False)\n", "def test_abs__unbranched():\n    x = Symbol('x', real=True)\n    assert unchanged(Abs, exp_polar(x))\n    assert Abs(exp_polar(2*pi*I)) == 1\n    assert Abs(exp_polar(2*pi*I + 1)) == exp(1)\n    assert unchanged(Abs, exp_polar(2*pi*I + x))\n    assert unchanged(Abs, exp_polar(I))\n    assert unchanged(Abs, exp_polar(1 + I))\n    assert unchanged(Abs, exp_polar(1 + I*x))\n    assert unchanged(Abs, exp_polar(1 + I*2*pi))\n    assert unchanged(Abs, exp_polar(1 + I*2*pi + x))\n", "def test_principal_branch():\n    x = Symbol('x')\n    assert principal_branch(exp_polar(2*pi*I)*3, 2*pi) == 3*exp_polar(0)\n    assert principal_branch(exp_polar(2*pi*I)*3*x, 2*pi) == 3*principal_branch(x, 2*pi)\n    assert principal_branch(x, oo) == x\n", "def test_principal_branch():\n    z = symbols('z')\n    assert principal_branch(exp_polar(2*pi*I)*3, 2*pi) == 3*exp_polar(0)\n    assert principal_branch(exp_polar(2*pi*I)*3*z, 2*pi) == 3*principal_branch(z, 2*pi)\n    assert unchanged(principal_branch, exp_polar(I*pi)*3, 2*pi)\n    assert unchanged(principal_branch, exp_polar(I*pi)*3*z, 2*pi)\n    assert principal_branch(exp_polar(I*pi/2)*3, 2*pi) == 3*exp_polar(I*pi/2)\n    assert principal_branch(exp_polar(I*pi/2)*3*z, 2*pi) == 3*principal_branch(z*exp_polar(I*pi/2), 2*pi)\n", "def test_polarify():\n    x, y = symbols('x y')\n    expr = -x**y\n    new_expr, reps = polarify(expr)\n    assert new_expr == (exp_polar(I*pi)*x)**y\n    assert reps == {x: x, y: y}\n    assert polarify(x, lift=True) == polar_lift(x)\n    assert polarify(x + y, lift=True) == polar_lift(x + y)\n    assert polarify(sin((1 + I)*x)) == (sin(x*polar_lift(1 + I)), {x: x})\n"], "sample_1175": ["def test_MatAdd():\n    A = Matrix([[1, 2], [3, 4]])\n    B = Matrix([[5, 6], [7, 8]])\n    C = Matrix([[0, 0], [0, 0]])\n    assert pretty(A + B) == pretty(B + A)\n    assert pretty(A + B) == \"Matrix([[6, 8], [10, 12]])\"\n    assert pretty(A + C) == pretty(A)\n", "def test_pretty_pow():\n    assert pretty(Pow(2, 3, evaluate=False)) == \"2**3\"\n    assert pretty(Pow(x, 3, evaluate=False)) == \"x**3\"\n    assert pretty(Pow(2, -3, evaluate=False)) == \"2**-3\"\n    assert pretty(Pow(x, -3, evaluate=False)) == \"x**-3\"\n    assert pretty(Pow(2, Rational(1, 3), evaluate=False)) == \"2**(1/3)\"\n    assert pretty(Pow(x, Rational(1, 3), evaluate=False)) == \"x**(1/3)\"\n    assert pretty(Pow(2, Rational(-1, 3), evaluate=False)) == \"2**(-1/3)\"\n    assert pretty(Pow(x, Rational(-1, 3), evaluate=False)) == \"x**(-1/3)\"\n", "def test_pretty_print_Feedback():\n    tf1 = TransferFunction(x**2, y**2, z)\n    tf2 = TransferFunction(z, x, y)\n    pprint(Feedback(tf1, tf2))\n    s1 = Series(tf1)\n    s2 = Series(tf2)\n    pprint(Feedback(s1, s2))\n    tf3 = TransferFunctionMatrix([[tf1, tf2], [tf2, tf1]])\n    tf4 = TransferFunctionMatrix([[tf2, tf1], [tf1, tf2]])\n    pprint(Feedback(tf3, tf4))\n    m1 = MIMOSeries(tf1, tf2)\n    m2 = MIMOSeries(tf2, tf1)\n    pprint(Feedback(m1, m2))\n", "def test_pretty_print_Asset():\n    f = symbols('f', cls=Function)\n    pretty_eq = pretty(Eq(f(0, 0), f(-1, -1)))\n    assert pretty_eq == 'Eq(f(0, 0), f(-1, -1))'\n    pretty_eq = pretty(Not(f(0, 0) > f(-1, -1)))\n    assert pretty_eq == 'Not(f(0, 0) > f(-1, -1))'\n", "def test_pretty_MatMul():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    C = MatrixSymbol('C', 2, 2)\n    assert pretty(A * B) == 'A B'\n    assert pretty(A * B * C) == 'A B C'\n    assert pretty(B * A) == 'B A'\n    assert pretty(B * A * C) == 'B A C'\n    assert pretty(A * (B * C)) == 'A (B C)'\n    assert pretty((A * B) * C) == '(A B) C'\n"], "sample_1176": ["def test_Float_evalf():\n    assert same_and_same_prec(Float(3.2).evalf(3), Float(3.2, 3))\n    assert same_and_same_prec(Float(3.2).evalf(50), Float(3.2, 50))\n    assert same_and_same_prec(Float(\"0.1\", 3).evalf(50), Float(\"0.1\", 50))\n", "def test_igcd2():\n    assert igcd2(48, 18) == 6\n    assert igcd2(54, 24) == 6\n    assert igcd2(17, 17) == 17\n    assert igcd2(0, 0) == 0\n    assert igcd2(1000000, 2000000) == 1000000\n", "def test_integer_nthroot():\n    assert integer_nthroot(0, 3) == (0, True)\n    assert integer_nthroot(27, 3) == (3, True)\n    assert integer_nthroot(28, 3) == (3, False)\n    assert integer_nthroot(27, 4) == (2, False)\n    assert integer_nthroot(-27, 3) == (-3, True)\n    assert integer_nthroot(10**20, 2) == (10**10, True)\n    for i in range(1, 1000):\n        root, exact = integer_nthroot(i*i*i, 3)\n        assert root == i\n        assert exact\n        root2, exact2 = integer_nthroot(i*i*i+1, 3)\n        assert root2 == i\n        assert not exact2\n", "def test_Float_rounding_up():\n    assert Float(0.99, 1).evalf(1) == 1.0\n    assert Float(1.01, 1).evalf(1) == 1.0\n    assert Float(1.1, 1).evalf(1) == 1.0\n    assert Float(1.5, 1).evalf(1) == 2.0\n    assert Float(1.9, 1).evalf(1) == 2.0\n    assert Float(2.1, 1).evalf(1) == 2.0\n\n    assert Float(-0.99, 1).evalf(1) == -1.0\n    assert Float(-1.01, 1).evalf(1) == -1.0\n    assert Float(-1.1, 1).evalf(1) == -1.0\n    assert Float(-1.5, 1).evalf(1) == -2.0\n    assert Float(-1.9, 1).evalf(1) == -2.0\n    assert Float(-2.1, 1).evalf(1) == -2.0\n", "def test_mpf_norm():\n    assert mpf_norm((0, 1, 1, 1), 100) == (0, 1, 1, 1)\n    assert mpf_norm((1, 1, 0, 1), 100) == (1, 1, 0, 1)\n    assert mpf_norm((1, 1, -1, 1), 100) == (1, 1, -1, 1)\n    assert mpf_norm((1, 1, 1, 1), 100) == (1, 1, 1, 1)\n    assert mpf_norm((1, 100, 100, 1), 100) == (1, 100, 100, 1)\n    assert mpf_norm((0, 0, 1, 0), 100) == (0, 0, 0, 0)  # hack for mpf_normalize\n"], "sample_1177": ["def test_polar_lift():\n    x = Symbol('x')\n    p = Symbol('p', polar=True)\n    assert polar_lift(4) == 4*exp_polar(0)\n    assert polar_lift(-4) == 4*exp_polar(I*pi)\n    assert polar_lift(-I) == exp_polar(-I*pi/2)\n    assert polar_lift(I + 2) == polar_lift(I + 2)\n    assert polar_lift(4*x) == 4*polar_lift(x)\n    assert polar_lift(4*p) == 4*p\n", "def test_polar_lift():\n    p = Symbol('p', polar=True)\n    x = Symbol('x')\n    assert polar_lift(4) == 4*exp_polar(0)\n    assert polar_lift(-4) == 4*exp_polar(I*pi)\n    assert polar_lift(-I) == exp_polar(-I*pi/2)\n    assert polar_lift(I + 2) != I + 2\n    assert polar_lift(4*x) == 4*polar_lift(x)\n    assert polar_lift(4*p) == 4*p\n", "def test_principal_branch():\n    z = symbols('z')\n    # Test argument reduction\n    assert principal_branch(exp_polar(2*pi*I)*3, 2*pi) == 3*exp_polar(0)\n    assert principal_branch(exp_polar(5*I*pi)*7, 4*pi) == 7*exp_polar(I*pi)\n    assert principal_branch(exp_polar(15*I*pi)*11, 6*pi) == 11*exp_polar(3*I*pi)\n    # Test with polar_lift\n    assert principal_branch(polar_lift(exp_polar(2*pi*I)*3), 2*pi) == 3*exp_polar(0)\n    assert principal_branch(polar_lift(exp_polar(5*I*pi)*7), 4*pi) == 7*exp_polar(I*pi)\n    # Test with period=oo\n    assert principal_branch(exp_polar(2*pi*I)*3, oo) == 3*exp_polar(2*pi*I)\n    assert principal_branch(exp_polar(5*I*pi)*7, oo) == 7*exp_polar(5*I*pi)\n    # Test with non-trivial argument\n    assert principal_branch(exp_polar((2*pi+1)*I)*3, 2*pi) == 3*exp_polar(I)\n    assert principal_branch(exp_polar((5*pi+1)*I)*7, 4*pi) == 7*exp_polar((pi+1)*I)\n    assert principal_branch(exp_polar((15*pi+1)*I)*11, 6*pi) == 11*exp_polar((3*pi+1)*I)\n", "def test_polar_lift():\n    p = Symbol('p', polar=True)\n    x = Symbol('x')\n    assert polar_lift(4) == 4*exp_polar(0)\n    assert polar_lift(-4) == 4*exp_polar(I*pi)\n    assert polar_lift(-I) == exp_polar(-I*pi/2)\n    assert polar_lift(I + 2) == polar_lift(2 + I)\n    assert polar_lift(4*x) == 4*polar_lift(x)\n    assert polar_lift(4*p) == 4*p\n    assert polar_lift(0) == 0\n    assert polar_lift(1) == exp_polar(0)\n    assert polar_lift(exp_polar(I*pi)) == exp_polar(I*pi)\n", "def test_sign():\n    assert sign(2.0) == 1\n    assert sign(-2) == -1\n    assert sign(S.Zero) == 0\n    assert sign(S.Infinity) == 1\n    assert sign(S.NegativeInfinity) == -1\n    assert sign(S.NaN) is S.NaN\n    assert sign(S.Pi) == 1\n    assert sign(S.NegativeOne) == -1\n    assert sign(S.ImaginaryUnit) == S.ImaginaryUnit\n    assert sign(-S.ImaginaryUnit) == -S.ImaginaryUnit\n    raises(TypeError, lambda: sign(S.One, S.Two))\n    assert sign(2 + 3*S.ImaginaryUnit) == sign(2 + 3*S.ImaginaryUnit)\n    assert sign(conjugate(2 + 3*S.ImaginaryUnit)) == sign(2 - 3*S.ImaginaryUnit)\n\n    a = Symbol('a', real=True, positive=True)\n    b = Symbol('b', real=True, negative=True)\n    c = Symbol('c', real=True, nonpositive=True)\n    d = Symbol('d', real=True, nonnegative=True)\n\n    s1 = sign(a)\n    s2 = sign(b)\n    s3 = sign(a*b)\n    s4 = sign(c)\n    s5 = sign(d)\n    assert s1.is_Integer\n    assert s2.is_Integer\n    assert s3.is_Integer\n    assert s4.is_nonpositive\n    assert s5.is_nonnegative\n\n    assert sign(2*a) == sign(a)\n    assert sign(-2*a) == -sign(a)\n    assert sign(2*b) == sign(b)\n    assert sign(-2*b) == -sign(b)\n    assert sign(2*a*b) == sign(a*b)\n    assert sign(-2*a*b) == -sign(a*b)\n\n    x = Symbol('x')\n    raises(ArgumentIndexError, lambda: Derivative(sign(x), x, 2).doit())\n    raises(ArgumentIndexError, lambda: Derivative(sign(x), y, 2).doit())\n"], "sample_1178": ["def test_Variable__sympystr():\n    x = Symbol('x')\n    v = Variable(x)\n    assert str(v) == 'Variable(x, type=untyped)'\n    assert str(Variable(x, type=integer)) == 'Variable(x, type=integer)'\n    assert str(Variable(x, attrs=[value_const])) == 'Variable(x, attrs=(value_const,), type=untyped)'\n    assert str(Variable(x, value=42)) == 'Variable(x, value=42, type=untyped)'\n", "def test_Variable_add_attrs():\n    v = Variable(x, attrs=[value_const])\n    assert value_const in v.attrs\n    v2 = v.func(**v.kwargs())\n    v2.attrs += (pointer_const,)\n    assert pointer_const not in v.attrs\n    assert pointer_const in v2.attrs\n    assert v != v2\n", "def test_Token_repr():\n    class MyToken(Token):\n        __slots__ = ('foo', 'bar')\n        defaults = {'bar': Integer(0)}\n\n    mt = MyToken(foo='baz')\n    assert repr(mt) == \"MyToken(String('baz'), bar=Integer(0))\"\n    mt2 = MyToken(foo='baz', bar=1)\n    assert repr(mt2) == \"MyToken(String('baz'), bar=Integer(1))\"\n", "def test_Variable_attributes():\n    v1 = Variable(x, attrs=[value_const])\n    assert value_const in v1.attrs\n    assert v1.attrs == (value_const,)\n    v2 = Variable(x, attrs=[pointer_const])\n    assert pointer_const in v2.attrs\n    assert v2.attrs == (pointer_const,)\n    assert v1 != v2\n", "def test_function_call():\n    fcall = FunctionCall('foo', 'bar baz'.split())\n    assert str(fcall.name) == 'foo'\n    assert fcall.function_args == Tuple('bar', 'baz')\n"], "sample_1179": ["def test_sstr_Control():\n    tf = TransferFunction(1, [1, 2, 3], z)\n    assert sstr(tf) == 'TransferFunction(1/(1 + 2*z + 3*z**2), z)'\n    tf = TransferFunction([1, 2], [1, 2, 3], z)\n    assert sstr(tf) == 'TransferFunction((1 + 2*z)/(1 + 2*z + 3*z**2), z)'\n    tf = TransferFunctionMatrix([[1, 2], [3, 4]], z)\n    assert sstr(tf) == 'TransferFunctionMatrix(Matrix(\\n[[1, 2], [3, 4]]\\n), z)'\n    s = Series(tf)\n    assert sstr(s) == 'Series(TransferFunction(1/(1 + 2*z + 3*z**2), z))'\n    p = Parallel(tf)\n    assert sstr(p) == 'Parallel(TransferFunction(1/(1 + 2*z + 3*z**2), z))'\n    f = Feedback(tf)\n    assert sstr(f) == 'Feedback(TransferFunction(1/(1 + 2*z + 3*z**2), z))'\n", "def test_print_LatticeOp():\n    pr = StrPrinter()\n    assert pr._print(Expr.__and__(x, y)) == 'And(x, y)'\n    assert pr._print(Expr.__or__(x, y)) == 'Or(x, y)'\n    assert pr._print(Expr.__xor__(x, y)) == 'Xor(x, y)'\n", "def test_StrPrinter_print_Predicate():\n    assert sstr(Q.positive(x)) == 'Q.positive(x)'\n    assert sstr(Q.negative(y)) == 'Q.negative(y)'\n", "def test_print_MatMul():\n    A, B = MatrixSymbol('A', 3, 3), MatrixSymbol('B', 3, 3)\n    assert sstr(A*B) == 'A*B'\n    assert sstr(2*A) == '2*A'\n    assert sstr(2*A*B) == '2*A*B'\n    assert sstr(-A*B) == '-A*B'\n", "def test_tensor():\n    assert sstr(ArraySymbol(\"A\", (2, 3))) == \"A\"\n    assert sstr(ArrayElement(ArraySymbol(\"A\", (2, 3)), (1, 2))) == \"A[1, 2]\"\n    assert sstr(ArraySymbol(\"A\", (2, 3)).applyfunc(lambda x: x**2)) == \"A.applyfunc(lambda x: x**2)\"\n    assert sstr(ArraySymbol(\"A\", (2, 3)) + ArraySymbol(\"B\", (2, 3))) == \"A + B\"\n    assert sstr(ArraySymbol(\"A\", (2, 3)) - ArraySymbol(\"B\", (2, 3))) == \"A - B\"\n"], "sample_1181": ["def test_Pow_negative_integer():\n    M = MatrixSymbol('M', 2, 2)\n    with warns_deprecated_sympy():\n        A = lambdify(M, M**(-1), 'numpy')\n    assert str(A(eye(2))) == str(numpy.linalg.inv(numpy.array(eye(2))))\n", "def test_print_Adjoint():\n    n = MatrixSymbol('n', 2, 2)\n    assert NumPyPrinter().doprint(n.adjoint()) == 'numpy.conjugate(numpy.transpose(n))'\n", "def test_CuPyPrinter():\n    n = 3\n    A, B = MatrixSymbol('A', n, n), MatrixSymbol('B', n, n)\n    x, y = MatrixSymbol('x', n, 1), MatrixSymbol('y', n, 1)\n\n    with warnings.catch_warnings():\n        warnings.filterwarnings('ignore', category=DeprecationWarning)\n        printer = CuPyPrinter()\n\n    s = Sum(x[i, 0], (i, 0, n - 1)).doit()\n    assert printer._print(s) == 'cupy.sum(x)'\n\n    assert printer._print(x + y) == 'cupy.add(x, y)'\n    assert printer._print(A * x) == 'cupy.dot(A, x)'\n    assert printer._print(x.T) == 'cupy.transpose(x)'\n    assert printer._print(A.T) == 'cupy.transpose(A)'\n    assert printer._print(A.I) == 'cupy.linalg.inv(A)'\n", "def test_array_expressions():\n    n = 3\n    A = MatrixSymbol('A', n, n)\n    B = MatrixSymbol('B', n, n)\n    C = MatrixSymbol('C', n, n)\n    I = Identity(n)\n\n    array_expressions = [\n        ArrayTensorProduct(A, B),\n        ArrayAdd(A, B),\n        PermuteDims(A, (1, 0)),\n        ArrayDiagonal(A, (0, 1))\n    ]\n\n    for expr in array_expressions:\n        array_expr = convert_matrix_to_array(expr)\n        assert NumPyPrinter().doprint(array_expr)  # Just test that it doesn't throw\n\n    # Test on some non-matrix inputs\n    assert NumPyPrinter().doprint(convert_matrix_to_array(x + 1)) == '(x + 1)'\n", "def test_matrix_solve():\n    n = 2\n    A = MatrixSymbol('A', n, n)\n    b = MatrixSymbol('b', n, 1)\n    with warns_deprecated_sympy(\n            \"MatrixSolve is deprecated and will be removed in a future SymPy version. \"\n            \"Use MatMul with matrix_inverse or MatPow(inv, -1) instead.\"):\n        expr = MatrixSolve(A, b)\n    assert NumPyPrinter().doprint(expr) == 'numpy.linalg.solve(A, b)'\n    assert SciPyPrinter().doprint(expr) == 'numpy.linalg.solve(A, b)'\n"], "sample_1180": ["def test_point_transform():\n    p = Point3D(1, 1, 1)\n    identity_matrix = Matrix([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]])\n    assert p.transform(identity_matrix) == p\n\n    # Test scale\n    scale_matrix = Matrix([[2, 0, 0, 0], [0, 3, 0, 0], [0, 0, 4, 0], [0, 0, 0, 1]])\n    assert p.transform(scale_matrix) == Point3D(2, 3, 4)\n\n    # Test translate\n    translate_matrix = Matrix([[1, 0, 0, 1], [0, 1, 0, 2], [0, 0, 1, 3], [0, 0, 0, 1]])\n    assert p.transform(translate_matrix) == Point3D(2, 3, 4)\n", "def test_point_transform():\n    p = Point(1, 1)\n    assert p.transform(Matrix([[1, 0, 1], [0, 1, 1], [0, 0, 1]])) == Point(2, 2)\n    p3d = Point3D(1, 1, 1)\n    assert p3d.transform(Matrix([[1, 0, 0, 1], [0, 1, 0, 1], [0, 0, 1, 1], [0, 0, 0, 1]])) == Point3D(2, 2, 2)\n", "def test_Point3D_transform():\n    p = Point3D(1, 1, 1)\n    identity_matrix = Matrix([[1, 0, 0, 0],\n                              [0, 1, 0, 0],\n                              [0, 0, 1, 0],\n                              [0, 0, 0, 1]])\n    assert p.transform(identity_matrix) == p\n\n    # Rotate 180 degrees around the z-axis\n    rotation_matrix = Matrix([[cos(pi), -sin(pi), 0, 0],\n                              [sin(pi), cos(pi), 0, 0],\n                              [0, 0, 1, 0],\n                              [0, 0, 0, 1]])\n    assert p.transform(rotation_matrix) == Point3D(-1, -1, 1)\n", "def test_3D_point_transform():\n    p = Point3D(1, 2, 3)\n    identity_matrix = Matrix([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]])\n    translated_matrix = Matrix([[1, 0, 0, 1], [0, 1, 0, 2], [0, 0, 1, 3], [0, 0, 0, 1]])\n    assert p.transform(identity_matrix) == p\n    assert p.transform(translated_matrix) == Point3D(2, 4, 6)\n", "def test_point_transform():\n    p = Point2D(1, 2)\n    # Identity transformation\n    assert p.transform(Matrix([[1, 0, 0], [0, 1, 0], [0, 0, 1]])) == p\n    # Translation by (3, 4)\n    assert p.transform(Matrix([[1, 0, 3], [0, 1, 4], [0, 0, 1]])) == Point2D(4, 6)\n    # Scaling by a factor of 3\n    assert p.transform(Matrix([[3, 0, 0], [0, 3, 0], [0, 0, 1]])) == Point2D(3, 6)\n    # Rotation by 90 degrees about the origin\n    assert p.transform(Matrix([[0, -1, 0], [1, 0, 0], [0, 0, 1]])) == Point2D(-2, 1)\n\n    p = Point3D(1, 2, 3)\n    # Identity transformation\n    assert p.transform(Matrix([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]])) == p\n    # Translation by (4, 5, 6)\n    assert p.transform(Matrix([[1, 0, 0, 4], [0, 1, 0, 5], [0, 0, 1, 6], [0, 0, 0, 1]])) == Point3D(5, 7, 9)\n    # Scaling by a factor of 3\n    assert p.transform(Matrix([[3, 0, 0, 0], [0, 3, 0, 0], [0, 0, 3, 0], [0, 0, 0, 1]])) == Point3D(3, 6, 9)\n"], "sample_1182": ["def test_printing_of_Rational():\n    # Test printing of Rational\n    assert pycode(Rational(3, 4)) == \"3/4\"\n    assert MpmathPrinter().doprint(Rational(3, 4)) == \"mpmath.mpf(3)/mpmath.mpf(4)\"\n    assert SymPyPrinter().doprint(Rational(3, 4)) == \"sympy.Rational(3, 4)\"\n", "def test_print_Pow():\n    expr = Pow(x, 2)\n    assert pycode(expr) == \"x**2\"\n\n    expr = Pow(x, S.Half)\n    assert pycode(expr) == \"math.sqrt(x)\"\n\n    expr = Pow(x, -S.Half)\n    assert pycode(expr) == \"1/math.sqrt(x)\"\n\n    expr = Pow(x, Rational(1, 3))\n    assert pycode(expr) == \"x**(1/3)\"\n", "def test_pycode_Piecewise():\n    expr = Piecewise((x, x < 1), (x**2, True))\n    assert pycode(expr) == (\"(x if x < 1 else x**2)\")\n    expr = Piecewise((x, x < 1), (x**2, x < 2), (x**3, True))\n    assert pycode(expr) == (\"(x if x < 1 else x**2 if x < 2 else x**3)\")\n", "def test_pycode_KroneckerDelta():\n    assert pycode(KroneckerDelta(x, y)) == '(1 if x == y else 0)'\n    assert pycode(KroneckerDelta(x, y), fully_qualified_modules=False) == '(1 if x == y else 0)'\n", "compilation error"], "sample_1184": ["def test_gaussian_conj():\n    s, z_r, f = symbols('s z_r f')\n    assert gaussian_conj(s, z_r, f)[0] == 1 / (-1/(s + z_r**2/(-f + s)) + 1/f)\n    assert gaussian_conj(s, z_r, f)[1] == z_r / (1 - s**2/f**2 + z_r**2/f**2)\n    assert gaussian_conj(s, z_r, f)[2] == 1 / sqrt(1 - s**2/f**2 + z_r**2/f**2)\n\n    # Numerical values for parameters\n    num_s, num_z_r, num_f = 1, 2, 3\n    result = gaussian_conj(num_s, num_z_r, num_f)\n\n    assert result[0] != oo\n    assert result[1] != oo\n    assert result[2] != oo\n\n    # Exception handling for invalid inputs\n    try:\n        gaussian_conj('invalid', z_r, f)\n        assert False, \"Expected ValueError\"\n    except ValueError:\n        pass\n\n    try:\n        gaussian_conj(s, 'invalid', f)\n        assert False, \"Expected ValueError\"\n    except ValueError:\n        pass\n\n    try:\n        gaussian_conj(s, z_r, 'invalid')\n        assert False, \"Expected ValueError\"\n    except ValueError:\n        pass\n", "def test_geometric_conj_bf():\n    a, b, f = symbols('a b f')\n    assert streq(geometric_conj_bf(b, f), b*f/(b - f))\n    assert geometric_conj_bf(oo, f) == oo\n    assert geometric_conj_bf(b, oo) == b\n    assert geometric_conj_bf(0, f) == 0\n", "def test_curved_mirror():\n    R = symbols('R')\n    mat = CurvedMirror(R)\n    assert streq(mat, Matrix([[1, 0], [-2/R, 1]]))\n    assert streq(mat*A, Matrix([[A, 0], [-2*A/R, 1]]))\n    assert streq(B*mat, Matrix([[B, 0], [-2*B/R, 1]]))\n    assert streq(mat*C, Matrix([[C, 0], [-2*C/R, 1]]))\n    assert streq(D*mat, Matrix([[D, 0], [-2*D/R, 1]]))\n    assert streq(mat*mat, Matrix([[1, 0], [-4/R, 1]]))\n    assert streq(GeometricRay(0, 0)*mat, GeometricRay(0, 0))\n    assert streq(mat*GeometricRay(0, 0), GeometricRay(0, 0))\n\n    p = BeamParameter(590e-9, 1, w=1e-3)\n    assert N(p.w_0) == Float(0.001, 3)\n    assert N(p.w) == Float(0.001, 3)\n    assert N(p.z_r) == Float(2.494, 3)\n    assert N((mat*p).w_0) == Float(0.001, 3)\n    assert N((mat*p).w) != Float(0.001, 3)\n    assert N((mat*p).z_r) == Float(2.494, 3)\n", "def test_BeamParameter_wavelen():\n    wavelen, z, w, n = symbols('wavelen z w n')\n    p = BeamParameter(wavelen, z, w=w)\n    assert p.wavelen == wavelen\n", "def test_gaussian_conj():\n    s_in, z_r_in, f = symbols('s_in z_r_in f')\n    s_out, z_r_out, m = gaussian_conj(s_in, z_r_in, f)\n    assert streq(s_out, 1 / (-1/(s_in + z_r_in**2/(-f + s_in)) + 1/f))\n    assert streq(z_r_out, z_r_in/(1 - s_in**2/f**2 + z_r_in**2/f**2))\n    assert streq(m, 1/sqrt(1 - s_in**2/f**2 + z_r_in**2/f**2))\n"], "sample_1185": ["def test_decompogen_with_Pow():\n    assert decompogen(x**2, x) == [x**2]\n    assert decompogen(exp(x), x) == [exp(x)]\n    assert decompogen(exp(x**2), x) == [exp(x), x**2]\n    assert decompogen(x**(2*x), x) == [x**x, 2*x]\n", "def test_decompogen_min_max():\n    # Test for decomposition of Min/Max functions\n    assert decompogen(Max(x**2, x), x) == [Max(x**2, x)]\n    assert decompogen(Min(x**2, x), x) == [Min(x**2, x)]\n    assert decompogen(Max(x, x**2 + 2*x + 1), x) == [Max(x, x**2 + 2*x + 1)]\n    assert decompogen(Max(sin(x), cos(x)), x) == [Max(sin(x), cos(x))]\n    assert decompogen(Min(sin(x), cos(x)), x) == [Min(sin(x), cos(x))]\n", "def test_decompogen_no_decomposition():\n    assert decompogen(x**2 + 2*x + 1, x) == [x**2 + 2*x + 1]\n    assert decompogen(x**3 - 2*x**2 - 5*x + 1, x) == [x**3 - 2*x**2 - 5*x + 1]\n    assert decompogen(sin(x) + cos(x), x) == [sin(x) + cos(x)]\n", "def test_decompogen_Pow():\n    assert decompogen(x**2, x) == [x**2]\n    assert decompogen(x**2.0, x) == [x**2.0]\n    assert decompogen(exp(x), x) == [exp(x)]\n    assert decompogen(exp(x**2), x) == [exp(x), x**2]\n    assert decompogen(x**(2*x), x) == [x**x, 2*x]\n", "def test_decompogen_relational():\n    assert decompogen(x > 2, x) == [x > 2]  # cannot decompose relational expressions\n    assert decompogen(x < 2, x) == [x < 2]  # cannot decompose relational expressions\n    assert decompogen(Eq(x, 2), x) == [Eq(x, 2)]  # cannot decompose relational expressions\n"], "sample_1183": ["def test_field_element_operations():\n    F, x, y, z = field(\"x,y,z\", ZZ)\n    f = F(x**2 + y)/(z + 1)\n    g = F(2*x + 3*y + 4*z + 5)\n\n    assert f + g == F(x**2 + 2*x*y + 3*y**2 + 4*x*z + 4*y*z + 5*z**2 + 5*z + x**2 + y)/(z + 1)\n    assert f - g == F(x**2 - 2*x*y - 3*y**2 - 4*x*z - 4*y*z - 5*z**2 - 5*z + x**2 + y)/(z + 1)\n    assert f * g == F((2*x + 3*y + 4*z + 5)*(x**2 + y))/(z + 1)\n    assert f / g == F((x**2 + y)/(z + 1)) / F(2*x + 3*y + 4*z + 5)\n    assert f**2 == F((x**2 + y)**2)/(z + 1)**2\n    assert f.diff(x) == F(2*x)/(z + 1)\n", "def test_FracField():\n    F, x, y = field(\"x,y\", ZZ)\n    f = F(x**2/y)\n    assert f.numer == x**2\n    assert f.denom == y\n    assert f == F(x**2, y)\n    assert f != F(x**2, x)\n    assert f != F(x**3, x)\n    assert f != x**2\n    assert f != 1\n    assert f == F(f)\n    assert f + F(x) == F(x**3 + x*y, y)\n    assert f - F(x) == F(x**3 - x*y, y)\n    assert f * F(x) == F(x**3, y)\n    assert f / F(x) == F(x, y)\n    assert f + x == F(x**3 + x*y, y)\n    assert f - x == F(x**3 - x*y, y)\n    assert f * x == F(x**3, y)\n    assert f / x == F(x, y)\n    assert -f == F(-x**2, y)\n    assert f**3 == F(x**6, y**3)\n    assert f.diff(x) == F(2*x, y)\n    assert f.diff(y) == F(-x**2, y**2)\n    assert f.as_expr() == x**2/y\n    assert f.as_expr(x, y) == x**2/y\n    assert f.as_expr(x, 1) == x**2\n    assert f.as_expr(1, y) == 1/y\n    assert f.as_expr(1, 1) == 1\n", "def test_frac_field():\n    F, x, y = field(\"x,y\", ZZ)\n    f = F(x, y)\n    assert f.numer == x\n    assert f.denom == y\n\n    f1 = F(x, 1)\n    assert f1.numer == x\n    assert f1.denom == 1\n\n    f2 = F(1, x)\n    assert f2.numer == 1\n    assert f2.denom == x\n\n    assert f1 + f2 == F(x**2 + 1, x)\n    assert f1 - f2 == F(x**2 - 1, x)\n    assert f1 * f2 == F(x, x)\n    assert f1 / f2 == F(x**2, 1)\n\n    raises(ZeroDivisionError, lambda: F(0, 0))\n    raises(ZeroDivisionError, lambda: F(x, 0))\n", "def test_FracField():\n    K, x, y, z = field(\"x,y,z\", ZZ)\n    assert K.order == LexOrder()\n    assert K.domain == ZZ\n    assert K.symbols == (x, y, z)\n    assert K.gens == (x, y, z)\n    assert K.zero == 0\n    assert K.one == 1\n    assert K.new(x, y) == K.new(x, y)\n    assert K.new(x, y).raw_new(y, x) == K.new(y, x)\n    assert K.from_expr(x/y) == K.new(x, y)\n    assert K.to_domain() == K\n    assert K.to_ring() != K\n\n    assert K.get_ring().to_field() == K\n    assert K.is_Field\n    assert K.is_CommutativeRing\n\n    assert K.get_field() == K\n    assert K.is_Field\n\n    assert K.domain_new(1) == 1\n    assert K.domain_new(x) == x\n\n    assert K.ground_new(1) == 1\n    assert K.ground_new(x) == x\n\n    assert K.field_new(K.one) == K.one\n    assert K.field_new(K.new(x, y)) == K.new(x, y)\n    assert K.field_new(x) == x\n    assert K.field_new(x/y) == K.new(x, y)\n", "def test_FracField():\n    F, x, y = field(\"x,y\", ZZ)\n    f = F(1, x)\n    assert f.numer == 1\n    assert f.denom == x\n\n    assert F(2*x, 3*y) == F(2*x, 3*y)\n    assert F(2*x, 3*y) != F(2*x, 3*x)\n\n    assert hash(F(2*x, 3*y)) == hash(F(2*x, 3*y))\n\n    assert F(0, x) == F.zero\n    assert bool(F(1, x)) == True\n    assert bool(F(0, x)) == False\n\n    assert F(1, x) < F(2, x)\n    assert F(1, x) <= F(2, x)\n    assert F(2, x) > F(1, x)\n    assert F(2, x) >= F(1, x)\n\n    assert -F(1, x) == F(-1, x)\n    assert F(1, x) + F(2, x) == F(3, x)\n    assert F(1, x) - F(2, x) == F(-1, x)\n    assert F(1, x) * F(2, x) == F(2, x**2)\n    assert F(1, x) / F(2, x) == F(1, 2)\n\n    assert F(1, x)**2 == F(1, x**2)\n    assert F(1, x)**-2 == F(x**2, 1)\n\n    assert F(1, x).diff(x) == F(0, x**2)\n"], "sample_1186": ["def test_array_methods():\n    for array_type in array_types:\n        # Test rank method\n        array = array_type([1, 2, 3, 4, 5, 6], (2, 3))\n        assert array.rank() == 2\n\n        # Test diff method\n        array = array_type([x, x**2, x**3])\n        assert array.diff(x) == array_type([1, 2*x, 3*x**2])\n\n        # Test applyfunc method\n        array = array_type([1, 2, 3])\n        assert array.applyfunc(lambda x: x**2) == array_type([1, 4, 9])\n\n        # Test _eval_derivative method\n        array = array_type([x, x**2])\n        assert array._eval_derivative(x) == array_type([1, 2*x])\n\n        # Test _eval_derivative_n_times method\n        array = array_type([x, x**2])\n        assert array._eval_derivative_n_times(x, 2) == array_type([0, 2])\n", "def test_NDimArray_shape():\n    a = MutableDenseNDimArray([1, 2, 3, 4], (2, 2))\n    assert a.shape == (2, 2)\n\n    b = MutableDenseNDimArray([1, 2, 3, 4, 5, 6], (2, 3))\n    assert b.shape == (2, 3)\n\n    c = MutableDenseNDimArray([1, 2], (2,))\n    assert c.shape == (2,)\n\n    d = MutableDenseNDimArray(1, ())\n    assert d.shape == ()\n\n    raises(TypeError, lambda: MutableDenseNDimArray([1, 2], \"not a tuple\"))\n    raises(TypeError, lambda: MutableDenseNDimArray([1, 2], (2.5,)))\n", "def test_array_kind():\n    from sympy.tensor.array import ArrayKind, NumberKind, UndefinedKind\n\n    assert ArrayKind(NumberKind) == ArrayKind(NumberKind)\n    assert ArrayKind(NumberKind) != ArrayKind(UndefinedKind)\n    assert isinstance(ArrayKind(NumberKind), ArrayKind)\n    assert ArrayKind(NumberKind) is not ArrayKind(UndefinedKind)\n\n    # Test union\n    kind1 = ArrayKind(NumberKind)\n    kind2 = ArrayKind(UndefinedKind)\n    assert ArrayKind._union([kind1, kind1]) == kind1\n    assert ArrayKind._union([kind1, kind2]) == ArrayKind(UndefinedKind)\n", "def test_array_rank():\n    for array_type in mutable_array_types:\n        # rank 0\n        assert array_type([1]).rank() == 0\n        \n        # rank 1\n        assert array_type([1, 2]).rank() == 1\n        \n        # rank 2\n        assert array_type([1, 2, 3, 4], (2, 2)).rank() == 2\n        \n        # rank 3\n        assert array_type([1, 2, 3, 4, 5, 6, 7, 8], (2, 2, 2)).rank() == 3\n", "def test_array_kind():\n    from sympy.tensor.array import ArrayKind, NumberKind\n    array = MutableDenseNDimArray([1, 2, 3])\n    assert isinstance(array.kind, ArrayKind)\n    assert array.kind is ArrayKind(NumberKind)\n"], "sample_1188": ["def test_print_IdentityMorphism():\n    from sympy.categories import Object, IdentityMorphism\n    A = Object('A')\n    assert upretty(IdentityMorphism(A)) == 'id:A\u2192A'\n    assert pretty(IdentityMorphism(A)) == 'id:A->A'\n", "def test_pretty_printing_basis_dependent():\n    from sympy.vector import Vector, CoordSys3D\n    from sympy.abc import x, y, z\n    N = CoordSys3D('N')\n\n    v = Vector.zero\n    upretty_v = \"\"\"\\", "def test_power_pretty():\n    # Test pretty printing of power\n    assert upretty(a**2) == 'a\u00b2'\n    assert pretty(a**2) == 'a**2'\n    assert upretty(a**-1) == '1/a'\n    assert pretty(a**-1) == '1/a'\n    assert upretty(a**-2) == '1/a\u00b2'\n    assert pretty(a**-2) == '1/a**2'\n    assert upretty(1/a) == '1/a'\n    assert pretty(1/a) == '1/a'\n    assert upretty(1/(a*b)) == '1/(a\u22c5b)'\n    assert pretty(1/(a*b)) == '1/(a*b)'\n", "def test_pretty_print_Pow():\n    x = N.x\n    assert upretty(x**0.5) == \"\u221ax_N\"\n    assert pretty(x**0.5) == \"sqrt(x_N)\"\n    assert upretty(x**-0.5) == \"1/\u221ax_N\"\n    assert pretty(x**-0.5) == \"1/sqrt(x_N)\"\n    assert upretty(x**-0.25) == \"1/\u2074\u221ax_N\"\n    assert pretty(x**-0.25) == \"1/x_N**(1/4)\"\n    assert upretty(x**(1/6)) == \"\u2076\u221ax_N\"\n    assert pretty(x**(1/6)) == \"x_N**(1/6)\"\n    assert upretty(x**(3/4)) == \"\u2074\u221ax_N\u00b3\"\n    assert pretty(x**(3/4)) == \"x_N**(3/4)\"\n", "def test_pretty_MatMul():\n    from sympy.matrices.expressions import MatrixSymbol, MatMul\n    A = MatrixSymbol('A', 3, 4)\n    B = MatrixSymbol('B', 4, 5)\n    pform = pretty(MatMul(A, B))\n    assert pform == pretty(A*B)\n"], "sample_1189": ["def test_lambdify_missing_args():\n    f = lambdify((x, y), x + y)\n    raises(TypeError, lambda: f(1))\n", "def test_lambdify_piecewise_with_non_sympy_types():\n    f = lambdify(x, Piecewise((x, x < 2), (x**2, True)))\n    assert f(1) == 1\n    assert f(3) == 9\n    f = lambdify(x, Piecewise((x, x < 2), (x**2, True)), 'math')\n    assert f(1) == 1\n    assert f(3) == 9\n    f = lambdify(x, Piecewise(((x, y), x < 2), ((x**2, y**2), True)))\n    assert f(1, 4) == (1, 4)\n    assert f(3, 4) == (9, 16)\n    f = lambdify(x, Piecewise(((x, y), x < 2), ((x**2, y**2), True)), 'math')\n    assert f(1, 4) == (1, 4)\n    assert f(3, 4) == (9, 16)\n", "def test_lambdify_matrix_with_Symbol_name():\n    # Test the case where Matrix has a Symbol with a name that is the same\n    # as a numpy function.\n    from sympy import Matrix, Symbol\n    import numpy as np\n\n    m = Matrix([[Symbol('sum')]])\n    l = lambdify((Symbol('x')), m.sum(), 'numpy')\n\n    assert np.isclose(l(1), 0)\n", "def test_lambdify_piecewise_numpy():\n    f = Piecewise((x**2, x < 2), (x**3, x >= 2))\n    f_np = lambdify(x, f, 'numpy')\n    assert f_np(1) == 1\n    assert f_np(2) == 8\n    assert np.all(f_np(np.array([1, 2, 3, 4])) == np.array([1, 8, 27, 64]))\n    assert f_np(np.array([1, 2, 3, 4], dtype=np.float_)).dtype == np.float_\n", "def test_lambdify_with_user_functions():\n    # Test that user functions are applied correctly in lambdify.\n    f = Function('f')\n    g = Function('g')\n    func_expr = f(x) + g(x)\n    x_val = 2\n\n        return x + 1\n\n        return x * 2\n\n    func = lambdify(x, func_expr, [{'f': f_impl, 'g': g_impl}])\n\n    assert func(x_val) == f_impl(x_val) + g_impl(x_val)\n"], "sample_1187": ["def test_gradient_terms():\n    # 2D case\n    terms = [[1, 0, 0, 0], [y, 0, 1, 0], [y**2, 0, 2, 0], [x, 1, 0, 0],\n             [x*y, 1, 1, 0], [x**2, 2, 0, 0]]\n    assert gradient_terms(binomial_power=2) == terms\n\n    # 3D case\n    terms = [[[[1, 0, 0, 0, 0, 0, 0, 0]]],\n             [[[y, 0, 1, 0, 1, 0, 1, 0], [z, 0, 0, 1, 1, 0, 1, 0]],\n              [[x, 1, 0, 0, 1, 1, 0, 0]]],\n             [[[y**2, 0, 2, 0, 2, 0, 2, 0], [y*z, 0, 1, 1, 2, 0, 1, 0],\n               [z**2, 0, 0, 2, 2, 0, 2, 0]],\n              [[x*y, 1, 1, 0, 2, 1, 0, 0], [x*z, 1, 0, 1, 2, 1, 1, 0]],\n              [[x**2, 2, 0, 0, 2, 2, 0, 0]]]]\n    assert gradient_terms(binomial_power=2, no_of_gens=3) == terms\n", "def test_hyperplane_parameters():\n    triangle = Polygon(Point(0, 3), Point(5, 3), Point(1, 1))\n    assert hyperplane_parameters(triangle) == [\n        ((S(1)/2, -S(1)), S(3)/2), ((-2, -1), -3), ((0, 1), 3)]\n    cube = [[(0, 0, 0), (0, 0, 5), (0, 5, 0), (0, 5, 5), (5, 0, 0),\n             (5, 0, 5), (5, 5, 0), (5, 5, 5)],\n            [2, 6, 7, 3], [3, 7, 5, 1], [7, 6, 4, 5], [1, 5, 4, 0],\n            [3, 1, 0, 2], [0, 4, 6, 2]]\n    assert hyperplane_parameters(cube[1:], cube[0]) == [\n        ([0, -1, 0], -5), ([0, 0, -1], -5), ([-1, 0, 0], -5),\n        ([0, 1, 0], 0), ([1, 0, 0], 0), ([0, 0, 1], 0)]\n", "def test_integration_reduction_dynamic():\n    facets = [[(0, 5, 0), (5, 5, 0), (5, 5, 5), (0, 5, 5)],\n              [(0, 0, 0), (5, 0, 0), (5, 5, 0), (0, 5, 0)],\n              [(0, 0, 0), (0, 5, 0), (0, 5, 5), (0, 0, 5)],\n              [(5, 0, 0), (5, 5, 0), (5, 5, 5), (5, 0, 5)],\n              [(0, 0, 0), (5, 0, 0), (5, 0, 5), (0, 0, 5)],\n              [(0, 5, 0), (0, 5, 5), (5, 5, 5), (5, 5, 0)]]\n    hp_param = (0, -5)\n    index = 0\n    expr = 1\n    degree = 0\n    x0 = facets[index][0]\n    dims = (x, y, z)\n    monomial_values = [[[[1, 0, 0, 0, 0, 0, 0, 0]]]]\n    monom_index = 0\n\n    assert integration_reduction_dynamic(facets, index, hp_param[0],\n                                         hp_param[1], expr, degree, dims, 0,\n                                         0, 0, x0, monomial_values,\n                                         monom_index) == -25\n", "def test_main_integrate():\n    triangle = Polygon(Point(0, 3), Point(5, 3), Point(1, 1))\n    facets = triangle.sides\n    hp_params = hyperplane_parameters(triangle)\n    assert main_integrate(x**2 + y**2, facets, hp_params) == Rational(325, 6)\n    assert main_integrate(x**2*y, facets, hp_params) == Rational(55, 3)\n    assert main_integrate(x*y**2, facets, hp_params) == Rational(605, 12)\n    assert main_integrate(x**3*y**2, facets, hp_params) == Rational(27475, 72)\n", "def test_distance_to_side():\n    point = (0, 0, 0)\n    line_seg = [(0, 0, 1), (0, 1, 0)]\n    A = (1, 0, 0)\n    assert distance_to_side(point, line_seg, A) == -sqrt(2)/2\n    assert distance_to_side((1, 1, 1), [(1, 1, 2), (1, 2, 1)], (0, 0, 1)) == -sqrt(2)/2\n"], "sample_1190": ["def test_UnitSystem_get_dimensional_expr():\n    # Test get_dimensional_expr with simple unit\n    assert SI.get_dimensional_expr(meter) == length.name\n\n    # Test get_dimensional_expr with compound unit\n    joule_dim = SI.get_dimensional_expr(joule)\n    assert joule_dim == energy.name\n\n    # Test get_dimensional_expr with function\n    sin_arg = Symbol(\"arg\")\n    assert SI.get_dimensional_expr(sin(sin_arg)) == 1\n\n    # Test get_dimensional_expr with Mul\n    assert SI.get_dimensional_expr(meter * kilogram) == length.name * mass.name\n\n    # Test get_dimensional_expr with Add\n    assert SI.get_dimensional_expr(meter + kilogram) == length.name\n", "def test_UnitSystem_extend():\n    base_units = (meter, kilogram, second)\n    units = (joule, newton)\n    name = \"SI_extend\"\n    description = \"Extension of SI system\"\n    dimension_system = SI.get_dimension_system()\n    derived_units = {pressure: pascal}\n\n    extended_system = SI.extend(base_units, units, name, description, dimension_system, derived_units)\n\n    assert extended_system.name == name\n    assert extended_system.descr == description\n    assert extended_system._base_units == base_units + SI._base_units\n    assert extended_system._units == units + SI._units\n    assert extended_system.get_dimension_system() == dimension_system\n    assert extended_system.derived_units == {**SI.derived_units, **derived_units}\n", "def test_extend_unit_system():\n    base_units = (meter, kilogram, second)\n    units = (joule,)\n    name = \"SI_extended\"\n    description = \"An extension of the SI system\"\n    dimension_system = SI.get_dimension_system()\n\n    extended_si = SI.extend(base_units, units, name, description, dimension_system)\n\n    assert extended_si.name == name\n    assert extended_si.descr == description\n    assert extended_si._base_units == base_units + SI._base_units\n    assert extended_si._units == units + SI._units\n    assert extended_si._dimension_system == dimension_system\n", "def test_UnitSystem_get_dimensional_expr():\n    # Check that get_dimensional_expr works correctly for different expressions\n    m = Quantity(\"meter\", length, \"m\")\n    s = Quantity(\"second\", time, \"s\")\n    v = Quantity(\"volt\", energy / charge, \"V\")\n    q = Quantity(\"coulomb\", charge, \"C\")\n\n    expr1 = m / s\n    assert SI.get_dimensional_expr(expr1) == length / time\n\n    expr2 = v * q\n    assert SI.get_dimensional_expr(expr2) == energy\n\n    expr3 = diff(m**2, s)\n    assert SI.get_dimensional_expr(expr3) == length**2 / time\n\n    expr4 = sin(m / s)\n    assert SI.get_dimensional_expr(expr4) == S.One\n\n    expr5 = m + s\n    raises(ValueError, lambda: SI.get_dimensional_expr(expr5))\n\n    # Check that it works correctly for quantities with unit prefixes\n    km = Quantity(\"kilometer\", length, \"km\")\n    assert SI.get_dimensional_expr(km) == length\n", "def test_unit_system_extend():\n    base_units = (meter, second, kilogram)\n    units = (centimeter, gram)\n    derived_units = {length: meter}\n    us = UnitSystem(base_units, units, \"test\", \"Test unit system\", derived_units=derived_units)\n\n    extended_base_units = (joule,)\n    extended_units = (kilo*gram,)\n    extended_derived_units = {energy: joule}\n\n    extended_us = us.extend(extended_base_units, extended_units, derived_units=extended_derived_units)\n\n    assert extended_us.name == \"\"\n    assert extended_us.descr == \"\"\n\n    assert set(extended_us._base_units) == set(base_units + extended_base_units)\n    assert set(extended_us._units) == set(units + base_units + extended_units + extended_base_units)\n\n    assert extended_us.derived_units == {**derived_units, **extended_derived_units}\n"], "sample_1192": ["def test_disambiguate():\n    x = Symbol('x')\n    y = Symbol('y')\n    d1 = Dummy('x')\n    d2 = Dummy('x')\n    ix = Symbol('x', integer=True)\n    vx = Symbol('x')\n    assert disambiguate(x, d1, d2) == (x, Symbol('x_1'), Symbol('x_2'))\n    assert disambiguate(vx + ix) == (vx + Symbol('x_1'),)\n    assert disambiguate(x/y, d1/y) == (x/y, Symbol('x_1')/y)\n    assert disambiguate(x, Symbol('x_1')) == (x, Symbol('x_1_1'))\n    assert disambiguate(Symbol('x1')) == (Symbol('x1'),)\n", "def test_disambiguate():\n    x = Symbol('x')\n    x1 = Symbol('x', integer=True)\n    y = Symbol('y')\n    d = Dummy('x')\n    d1 = Dummy('x')\n    assert disambiguate(x, x1, y, d, d1) == (x, x_1, y, x_2, x_3)\n    assert disambiguate(x + x1) == (x + x_1,)\n    assert disambiguate(x/y, d/y) == (x/y, x_1/y)\n    assert disambiguate(x, d, d1, d, x, x, x1, y, d1) == (\n        x, x_1, x_2, x_3, x, x, x_4, y, x_5)\n", "def test_disambiguate():\n    x = Symbol('x')\n    x1 = Symbol('x', integer=True)\n    y = Symbol('y')\n    d = Dummy('x')\n    d1 = Dummy('x')\n    assert disambiguate(x, d, d1) == (Symbol('x_2'), Symbol('x'), Symbol('x_1'))\n    assert disambiguate(x + x1) == (x + Symbol('x_1'),)\n    assert disambiguate(x/y, d/y) == (Symbol('x_1')/y, Symbol('x')/y)\n    assert disambiguate(d, x, y, d1) == (Symbol('x'), Symbol('x_2'), y, Symbol('x_1'))\n    f = Function('f')\n    assert disambiguate(f(x), f(x1)) == (f(x), f(Symbol('x_1')))\n", "def test_symbols_iterator():\n    \"\"\"Test symbols() when called with an iterator.\"\"\"\n    names = [f'x{i}' for i in range(5)]\n    syms = symbols(names)\n    assert isinstance(syms, tuple)\n    assert all(isinstance(s, Symbol) for s in syms)\n    assert len(syms) == len(names)\n\n    names = iter(names)\n    syms = symbols(names)\n    assert isinstance(syms, tuple)\n    assert all(isinstance(s, Symbol) for s in syms)\n    assert len(syms) == 5\n", "def test_disambiguate():\n    x = Symbol('x')\n    y = Symbol('y')\n    x1 = Symbol('x', integer=True)\n    d = Dummy('x')\n    d1 = Dummy('x')\n    assert disambiguate(x, d, d1) == (x, Symbol('x_1'), Symbol('x_2'))\n    assert disambiguate(x, x1, d, d1) == (Symbol('x_2'), Symbol('x_1'), x, Symbol('x_3'))\n    assert disambiguate(x/y, d/y) == (Symbol('x_1')/y, x/y)\n    assert disambiguate(x + x1) == (x + Symbol('x_1'),)\n"], "sample_1191": ["def test_hermite_normal_form_errors():\n    with raises(DMDomainError):\n        hermite_normal_form(DomainMatrix([[1, 2], [3, 4]], (2, 2), QQ))\n    with raises(DMDomainError):\n        _hermite_normal_form(DomainMatrix([[1, 2], [3, 4]], (2, 2), QQ))\n    with raises(DMDomainError):\n        _hermite_normal_form_modulo_D(DomainMatrix([[1, 2], [3, 4]], (2, 2), QQ), 5)\n    with raises(DMShapeError):\n        _hermite_normal_form_modulo_D(DomainMatrix([[1, 2], [3, 4], [5, 6]], (3, 2), ZZ), 5)\n", "def test_hermite_normal_form_errors():\n    # Test DMDomainError for non-ZZ domain\n    A = DomainMatrix([[1, 2], [3, 4]], (2, 2), QQ)\n    raises(DMDomainError, lambda: hermite_normal_form(A))\n\n    # Test DMDomainError for invalid D\n    A = DomainMatrix([[1, 2], [3, 4]], (2, 2), ZZ)\n    raises(DMDomainError, lambda: hermite_normal_form(A, D=-1))\n    raises(DMDomainError, lambda: hermite_normal_form(A, D=1.5))\n\n    # Test DMShapeError for mod D algorithm with more rows than columns\n    A = DomainMatrix([[1, 2], [3, 4], [5, 6]], (3, 2), ZZ)\n    raises(DMShapeError, lambda: hermite_normal_form(A, D=10))\n", "def test_hermite_normal_form_not_domain_ZZ():\n    A = DomainMatrix([[QQ(1, 2), QQ(1, 3)], [QQ(2, 3), QQ(1, 4)]], (2, 2), QQ)\n    raises(DMDomainError, lambda: hermite_normal_form(A))\n", "def test_hermite_normal_form_modulo_D():\n    # Test case for hermite_normal_form with a given multiple of the determinant\n    A = DomainMatrix([[12, 6, 4], [3, 9, 6], [2, 16, 14]], (3, 3), ZZ)\n    D = 100  # A multiple of the determinant of A's HNF\n    hnf = hermite_normal_form(A, D=D)\n    assert hnf == DomainMatrix([[10, 0, 2], [0, 15, 3], [0, 0, 2]], (3, 3), ZZ)\n\n    # Test case for hermite_normal_form with a given multiple of the determinant\n    # and check_rank=True\n    hnf = hermite_normal_form(A, D=D, check_rank=True)\n    assert hnf == DomainMatrix([[10, 0, 2], [0, 15, 3], [0, 0, 2]], (3, 3), ZZ)\n\n    # Test case for hermite_normal_form with a given multiple of the determinant\n    # and check_rank=False\n    hnf = hermite_normal_form(A, D=D, check_rank=False)\n    assert hnf == DomainMatrix([[10, 0, 2], [0, 15, 3], [0, 0, 2]], (3, 3), ZZ)\n\n    # Test case for _hermite_normal_form_modulo_D\n    hnf = _hermite_normal_form_modulo_D(A, D)\n    assert hnf == DomainMatrix([[10, 0, 2], [0, 15, 3], [0, 0, 2]], (3, 3), ZZ)\n", "def test_hermite_normal_form_modulo_D():\n    m = DomainMatrix([[ZZ(12), ZZ(6), ZZ(4)],\n                      [ZZ(3), ZZ(9), ZZ(6)],\n                      [ZZ(2), ZZ(16), ZZ(14)]], (3, 3), ZZ)\n    raises(DMShapeError, lambda: _hermite_normal_form_modulo_D(m, 5)[:2, :])\n    det = m.convert_to(QQ).det()\n    raises(DMDomainError, lambda: _hermite_normal_form_modulo_D(m, -abs(det)))\n    assert _hermite_normal_form_modulo_D(m, abs(det)) == hermite_normal_form(m)\n"], "sample_1193": ["def test_convex_hull():\n    # test with a set of points in a square\n    p1 = Point2D(0, 0)\n    p2 = Point2D(0, 1)\n    p3 = Point2D(1, 1)\n    p4 = Point2D(1, 0)\n    assert convex_hull(p1, p2, p3, p4) == Polygon(p1, p2, p3, p4)\n\n    # test with a set of points on a line\n    p1 = Point2D(0, 0)\n    p2 = Point2D(1, 0)\n    p3 = Point2D(2, 0)\n    p4 = Point2D(3, 0)\n    assert convex_hull(p1, p2, p3, p4) == Segment(p1, p4)\n\n    # test with a set of points with negative coordinates\n    p1 = Point2D(-1, -1)\n    p2 = Point2D(-1, 1)\n    p3 = Point2D(1, 1)\n    p4 = Point2D(1, -1)\n    assert convex_hull(p1, p2, p3, p4) == Polygon(p1, p2, p3, p4)\n\n    # test with a set of points with a single point\n    p1 = Point2D(0, 0)\n    assert convex_hull(p1) == p1\n", "def test_util_are_coplanar():\n    p1 = Point3D(1, 2, 3)\n    p2 = Point3D(4, 5, 6)\n    p3 = Point3D(7, 8, 9)\n    p4 = Point3D(10, 11, 12)\n    assert are_coplanar(p1, p2, p3) is False\n    assert are_coplanar(p1, p2, p3, p4) is False\n\n    l1 = Line3D(p1, p2)\n    l2 = Line3D(p3, p4)\n    assert are_coplanar(l1, l2) is False\n\n    p5 = Point3D(0, 0, 0)\n    p6 = Point3D(1, 1, 1)\n    p7 = Point3D(2, 2, 2)\n    assert are_coplanar(p5, p6, p7) is True\n", "def test_are_coplanar():\n    # 3D points\n    a = Point3D(1, 2, 3)\n    b = Point3D(4, 5, 6)\n    c = Point3D(7, 8, 9)\n    assert are_coplanar(a, b, c) is True\n\n    # 3D points not coplanar\n    a = Point3D(1, 2, 3)\n    b = Point3D(4, 5, 6)\n    c = Point3D(7, 8, 10)\n    assert are_coplanar(a, b, c) is False\n\n    # 3D lines\n    a = Line3D(Point3D(1, 2, 3), Point3D(4, 5, 6))\n    b = Line3D(Point3D(7, 8, 9), Point3D(10, 11, 12))\n    assert are_coplanar(a, b) is True\n\n    # 3D lines not coplanar\n    a = Line3D(Point3D(1, 2, 3), Point3D(4, 5, 6))\n    b = Line3D(Point3D(7, 8, 9), Point3D(10, 11, 13))\n    assert are_coplanar(a, b) is False\n\n    # Mix of points and lines\n    a = Point3D(1, 2, 3)\n    b = Line3D(Point3D(4, 5, 6), Point3D(7, 8, 9))\n    assert are_coplanar(a, b) is True\n\n    # Mix of points and lines not coplanar\n    a = Point3D(1, 2, 3)\n    b = Line3D(Point3D(4, 5, 6), Point3D(7, 8, 10))\n    assert are_coplanar(a, b) is False\n\n    # 2D entities\n    a = Point(1, 2)\n    b = Line(Point(3, 4), Point(5, 6))\n    raises(TypeError, lambda: are_coplanar(a, b))\n", "def test_idiff():\n    x, y, a = Symbol('x'), Symbol('y'), Symbol('a')\n    eq = x**2 + y**2 - 4\n    assert idiff(eq, y, x) == -x/y\n    assert idiff(eq, y, x, 2).simplify() == (-x**2 - y**2)/y**3\n    assert idiff(x + a + y, y, x) == -1\n    assert idiff(x + a + y, [y, a], x) == -Derivative(a, x) - 1\n", "def test_find():\n    x = Symbol('x')\n    assert find(x, x) == x\n    assert find('x', x) == x\n    raises(ValueError, lambda: find('y', x))\n"], "sample_1194": ["def test_julia_code_MatMul():\n    M = MatrixSymbol(\"M\", 3, 3)\n    P = MatrixSymbol(\"P\", 3, 3)\n    Q = MatrixSymbol(\"Q\", 3, 3)\n\n    assert julia_code(M*P) == '(M) * (P)'\n    assert julia_code(2*M*P) == '(2) * (M) * (P)'\n    assert julia_code(M*P*Q) == '(M) * (P) * (Q)'\n", "def test_julia_code_Rational():\n    assert julia_code(Rational(3, 7)) == \"3 // 7\"\n    assert julia_code(Rational(18, 7)) == \"18 // 7\"\n    assert julia_code(Rational(3, -7)) == \"-3 // 7\"\n    assert julia_code(Rational(-3, -7)) == \"3 // 7\"\n", "def test_julia_code_Assignment():\n    A = MatrixSymbol('A', 3, 4)\n    B = MatrixSymbol('B', 3, 4)\n    assign_to = A\n    expr = B\n    assert julia_code(expr, assign_to) == 'A = B'\n", "def test_julia_code_Pow():\n    assert julia_code(x**3) == 'x .^ 3'\n    assert julia_code(S.Half**x) == '(1//2) .^ x'\n    assert julia_code(2**x) == '2 .^ x'\n    assert julia_code(x**(2/3)) == 'x .^ (2 // 3)'\n    assert julia_code(sqrt(x)) == 'sqrt(x)'\n    assert julia_code(1/sqrt(x)) == '1 ./ sqrt(x)'\n    assert julia_code(x**-3) == '1 ./ x .^ 3'\n", "def test_julia_code_relational():\n    assert julia_code(Eq(x, y)) == \"x == y\"\n    assert julia_code(Ne(x, y)) == \"x != y\"\n    assert julia_code(Le(x, y)) == \"x <= y\"\n    assert julia_code(Lt(x, y)) == \"x < y\"\n    assert julia_code(Gt(x, y)) == \"x > y\"\n    assert julia_code(Ge(x, y)) == \"x >= y\"\n"], "sample_1196": ["def test_contains_eval_set():\n    i = Symbol('i', integer=True)\n    assert Contains(i, S.Naturals) == Contains(i, S.Naturals)\n    assert Contains(i, S.Integers) == Contains(i, S.Integers)\n    assert Contains(i, S.Reals) == Contains(i, S.Reals)\n\n    assert Contains(1, S.Naturals) == True\n    assert Contains(-1, S.Naturals) == False\n    assert Contains(1, S.Integers) == True\n    assert Contains(-1, S.Integers) == True\n\n    raises(TypeError, lambda: Contains(1, 2))\n", "def test_contains_eval():\n    x = Symbol('x', integer=True)\n    i = Symbol('i')\n    s1 = FiniteSet(1, 2, 3)\n    s2 = Interval(1, 5)\n    raises(TypeError, lambda: Contains(x, x))\n    assert Contains(i, s1).eval(i, s1) == Contains(i, s1)\n    assert Contains(3, s1).eval(3, s1) == S.true\n    assert Contains(5, s1).eval(5, s1) == S.false\n    assert Contains(3, s2).eval(3, s2) == S.true\n    assert Contains(6, s2).eval(6, s2) == S.false\n", "def test_contains_eval():\n    x = Symbol('x', integer=True)\n    s = FiniteSet(1, 2, 3)\n    assert Contains(1, s).eval(1, s) is S.true\n    assert Contains(4, s).eval(4, s) is S.false\n    assert Contains(x, s).eval(x, s) is Contains(x, s)\n\n    raises(TypeError, lambda: Contains(1, 2).eval(1, 2))\n", "def test_contains_eval_set():\n    i = Symbol('i', integer=True)\n    x = Symbol('x')\n    s1 = FiniteSet(1, 2, 3)\n    s2 = Interval(1, 5)\n\n    assert Contains(i, s1) == Contains(i, s1)\n    assert Contains(x, s2) == Contains(x, s2)\n    assert Contains(i, s1) != Contains(x, s2)\n\n    raises(TypeError, lambda: Contains(i, i))\n", "def test_contains_binary_symbols():\n    x = Symbol('x')\n    y = Symbol('y')\n    z = Symbol('z')\n    s = FiniteSet(x, y, z)\n    c = Contains(x, s)\n    assert c.binary_symbols == set()\n    c = Contains(x, s, evaluate=False)\n    assert c.binary_symbols == set()\n\n    i = Interval(0, 1)\n    c = Contains(x, i)\n    assert c.binary_symbols == set()\n    c = Contains(x, i, evaluate=False)\n    assert c.binary_symbols == set()\n\n    eq = Eq(x, y)\n    s = FiniteSet(eq)\n    c = Contains(eq, s)\n    assert c.binary_symbols == {x, y}\n    c = Contains(eq, s, evaluate=False)\n    assert c.binary_symbols == {x, y}\n"], "sample_1195": ["def test_kahane_simplify_trace():\n    i0, i1, i2, i3 = tensor_indices('i0:4', LorentzIndex)\n    p = TensorHead('p', [LorentzIndex])\n    ps = p(i0)*G(-i0)\n    qs = p(i0)*G(-i0)\n    t = ps*qs\n    assert _is_tensor_eq(gamma_trace(t), 4*p(i0)*p(-i0))\n    assert _is_tensor_eq(kahane_simplify(ps), Matrix([[4, 0, 0, 0], [0, 4, 0, 0], [0, 0, 4, 0], [0, 0, 0, 4]]))\n    assert _is_tensor_eq(kahane_simplify(ps*G(i1)), 2*G(i1))\n    assert _is_tensor_eq(kahane_simplify(ps*qs), -4*p(i0)*p(-i0))\n    assert _is_tensor_eq(kahane_simplify(ps*qs*G(i2)), 8*p(i0)*p(-i0)*G(i2))\n    assert _is_tensor_eq(kahane_simplify(ps*G(i1)*qs), -8*p(i0)*p(-i0)*G(i1))\n", "def test_simplify_gpgp():\n    p, q = tensor_heads('p, q', [LorentzIndex])\n    i0, i1, i2, i3, i4, i5 = tensor_indices('i0:6', LorentzIndex)\n    ps = p(i0)*G(-i0)\n    qs = q(i0)*G(-i0)\n    t = ps*qs*qs\n    assert _is_tensor_eq(simplify_gpgp(t), G(-i0)*p(i0)*q(i1)*q(-i1))\n", "def test_kahane_simplify():\n    i0, i1, i2, i3 = tensor_indices('i0:4', LorentzIndex)\n    assert _is_tensor_eq(kahane_simplify(G(i0)*G(i1)),\n                         G(i0)*G(i1))\n    assert _is_tensor_eq(kahane_simplify(G(i0)*G(-i0)),\n                         eye(4))\n    assert _is_tensor_eq(kahane_simplify(G(i0)*G(i1)*G(-i0)),\n                         -2*G(i1))\n    assert _is_tensor_eq(kahane_simplify(G(i0)*G(i1)*G(i2)*G(-i0)),\n                         2*G(i2)*G(i1))\n    assert _is_tensor_eq(kahane_simplify(G(i0)*G(i1)*G(i2)*G(i3)*G(-i0)),\n                         -2*G(i3)*G(i2)*G(i1))\n", "def test_kahane_simplify_trace():\n    LorentzIndex.auto_up = True\n    i0, i1, i2, i3 = tensor_indices('i0:4', LorentzIndex)\n    p = TensorHead('p', [LorentzIndex])\n    ps = p(i0)*G(-i0)\n    qs = G(i1)*p(-i1)\n    # simplify G(i0)*G(i1)*p(-i0)*p(-i1) from left and right:\n    t = G(i0)*ps*qs*G(-i0)\n    Kahane = kahane_simplify(t)\n    left = _simplify_single_line(ps*qs*G(-i0)*G(i0))\n    right = _simplify_single_line(G(i0)*G(-i0)*ps*qs)\n    assert _is_tensor_eq(Kahane, left)\n    assert _is_tensor_eq(Kahane, right)\n    LorentzIndex.auto_up = False\n", "def test_kahane_simplify_trace():\n    i0, i1, i2 = tensor_indices('i0:3', LorentzIndex)\n    t = G(i0)*G(i1)*G(-i0)*G(-i1)\n    kahane = kahane_simplify(t)\n    assert _is_tensor_eq(kahane, 4*eye(4))\n"], "sample_1197": ["def test_unit_system_extend():\n    base_units = (meter, kilogram, second)\n    units = (centimeter, gram, hour, minute)\n    name = \"SI_extended\"\n    description = \"SI system extended with more units\"\n    dimension_system = SI.get_dimension_system()\n    derived_units = {length: meter}\n\n    extended_system = SI.extend(\n        base_units, units, name, description, dimension_system, derived_units\n    )\n\n    assert extended_system._base_units == SI._base_units + base_units\n    assert extended_system._units == SI._units + units\n    assert extended_system.name == name\n    assert extended_system.descr == description\n    assert extended_system._dimension_system == dimension_system\n    assert extended_system._derived_units == {**SI._derived_units, **derived_units}\n", "def test_units_non_prefixed():\n    # Test that only non-prefixed units are returned\n    assert len(SI.get_units_non_prefixed()) > 0\n    assert all(not u.is_prefixed for u in SI.get_units_non_prefixed())\n    assert all(not u.is_physical_constant for u in SI.get_units_non_prefixed())\n\n    # Test that prefixed units are not included\n    assert kilometer not in SI.get_units_non_prefixed()\n", "def test_UnitSystem_extend():\n    base_units = (meter, kilogram, second)\n    units = (joule,)\n    name = \"SI\"\n    description = \"SI unit system\"\n    dimension_system = SI.get_dimension_system()\n    derived_units = {Dimension(\"length\"): meter}\n\n    unit_system = UnitSystem(base_units, units, name, description, dimension_system, derived_units)\n\n    extended_base_units = (ampere,)\n    extended_units = (volt,)\n    extended_name = \"SI_extended\"\n    extended_description = \"SI unit system extended\"\n    extended_dimension_system = unit_system.get_dimension_system()\n    extended_derived_units = {Dimension(\"time\"): second}\n\n    extended_unit_system = unit_system.extend(\n        extended_base_units,\n        extended_units,\n        extended_name,\n        extended_description,\n        extended_dimension_system,\n        extended_derived_units,\n    )\n\n    assert extended_unit_system._base_units == base_units + extended_base_units\n    assert extended_unit_system._units == units + extended_units\n    assert extended_unit_system.name == extended_name\n    assert extended_unit_system.descr == extended_description\n    assert extended_unit_system._dimension_system == extended_dimension_system\n    assert extended_unit_system._derived_units == {**derived_units, **extended_derived_units}\n", "def test_unit_system_get_dimensional_expr():\n    us = SI\n    q = Quantity('test_quantity', length, 1)\n    assert us.get_dimensional_expr(q) == length.name\n    assert us.get_dimensional_expr(q**2) == length.name**2\n    assert us.get_dimensional_expr(q * kilogram) == length.name * mass.name\n    assert us.get_dimensional_expr(diff(q, symbols('t'))) == length.name / time.name\n    assert us.get_dimensional_expr(sin(q)) == Function('sin')(length.name)\n    assert us.get_dimensional_expr(exp(q)) == Function('exp')(length.name)\n    assert us.get_dimensional_expr(q + kilogram) == length.name\n", "def test_get_dimensional_expr():\n    # Create a unit system with base units\n    u = UnitSystem((meter, second), name=\"test\")\n\n    # Test with different types of expressions\n    assert u.get_dimensional_expr(meter) == length.name\n    assert u.get_dimensional_expr(meter**2) == length.name**2\n    assert u.get_dimensional_expr(meter/second) == length.name/time.name\n    assert u.get_dimensional_expr(sin(meter/second)) == sin(length.name/time.name)\n    assert u.get_dimensional_expr(diff(meter**2, second)) == length.name**2/time.name\n    assert u.get_dimensional_expr(integrate(meter**2, second)) == length.name**2*time.name\n"], "sample_1199": ["def test_tensor_product_simp_Mul_multiple():\n    # Test that tensor_product_simp_Mul works with multiple tensor products\n    e = TensorProduct(A,B)*TensorProduct(C,D)*TensorProduct(x,A)\n    assert tensor_product_simp_Mul(e) == (A*C*x) * (B*D*A)\n", "def test_tensor_product_simp_Pow():\n    # Test Pow._eval_tensorproduct\n    assert tensor_product_simp_Pow(A**2) == A**2\n    assert tensor_product_simp_Pow((TP(A, B))**2) == TP(A**2, B**2)\n", "def test_tensor_product_simp_Mul():\n    e = TensorProduct(A, B) * TensorProduct(C, D)\n    assert tensor_product_simp(e) == TensorProduct(A*C, B*D)\n    assert tensor_product_simp(e**2) == TensorProduct((A*C)**2, (B*D)**2)\n    assert tensor_product_simp(e*x) == x*TensorProduct(A*C, B*D)\n\n    e = TensorProduct(A, B) * TensorProduct(C, D) * TensorProduct(A, B)\n    assert tensor_product_simp(e) == TensorProduct(A*C*A, B*D*B)\n\n    e = TensorProduct(A, B) * TensorProduct(C, D) * TensorProduct(A, B) * x\n    assert tensor_product_simp(e) == x*TensorProduct(A*C*A, B*D*B)\n", "def test_tensor_product_simp_Expansion():\n    e = TensorProduct(A + B, C)\n    assert tensor_product_simp(e.expand(tensorproduct=True)) == tensor_product_simp(e)\n", "def test_tensor_product_simp_Pow():\n    assert tensor_product_simp_Pow(A**2) == A**2\n    assert tensor_product_simp_Pow((TP(A, B))**2) == TP(A**2, B**2)\n    assert tensor_product_simp_Pow((TP(A, B))**x) == TP(A**x, B**x)\n"], "sample_1198": ["def test_mathematica_parser_issues():\n    parser = MathematicaParser()\n    assert parse_mathematica(\"Log[a, b]\") == log(b, a)\n    assert parse_mathematica(\"Log2[x]\") == log(x, 2)\n    assert parse_mathematica(\"Log10[x]\") == log(x, 10)\n    assert parse_mathematica(\"Mod[x, y]\") == Mod(x, y)\n    assert parse_mathematica(\"Max[x, y]\") == Max(x, y)\n    assert parse_mathematica(\"Min[x, y]\") == Min(x, y)\n    assert parse_mathematica(\"Pochhammer[x, y]\") == rf(x, y)\n    assert parse_mathematica(\"ExpIntegralEi[x]\") == Ei(x)\n    assert parse_mathematica(\"SinIntegral[x]\") == Si(x)\n    assert parse_mathematica(\"CosIntegral[x]\") == Ci(x)\n    assert parse_mathematica(\"AiryAi[x]\") == airyai(x)\n    assert parse_mathematica(\"AiryAiPrime[x]\") == airyaiprime(x)\n    assert parse_mathematica(\"AiryBi[x]\") == airybi(x)\n    assert parse_mathematica(\"LogIntegral[x]\") == li(x)\n    assert parse_mathematica(\"PrimePi[x]\") == primepi(x)\n    assert parse_mathematica(\"Prime[x]\") == prime(x)\n    assert parse_mathematica(\"PrimeQ[x]\") == isprime(x)\n", "def test_trig():\n    # Test trigonometric expressions\n    assert parse_mathematica(\"Sin[x]\") == sin(x)\n    assert parse_mathematica(\"Cos[x]\") == cos(x)\n    assert parse_mathematica(\"Tan[x]\") == sin(x)/cos(x)\n    assert parse_mathematica(\"ArcSin[x]\") == symbols('x').as_real_imag()[1]\n    assert parse_mathematica(\"ArcCos[x]\") == symbols('x').as_real_imag()[1]\n    assert parse_mathematica(\"ArcTan[x]\") == symbols('x').as_real_imag()[1]\n", "def test_mathematica_parser():\n    m = MathematicaParser()\n    assert m._from_mathematica_to_tokens(\"1 + 2\") == [\"1\", \"+\", \"2\"]\n    assert m._from_mathematica_to_tokens(\"Sin[x] + 2\") == [\"Sin\", \"[\", \"x\", \"]\", \"+\", \"2\"]\n    assert m._from_mathematica_to_tokens(\"(2 + 3)*4\") == [\"(\", \"2\", \"+\", \"3\", \")\", \"*\", \"4\"]\n", "def test_parse_mathematica_Prefix_postfix_notation():\n    assert parse_mathematica(\"x++\") == Function(\"Increment\")(x)\n    assert parse_mathematica(\"x--\") == Function(\"Decrement\")(x)\n    assert parse_mathematica(\"!x\") == Function(\"Not\")(x)\n    assert parse_mathematica(\"++x\") == Function(\"PreIncrement\")(x)\n    assert parse_mathematica(\"--x\") == Function(\"PreDecrement\")(x)\n    assert parse_mathematica(\"-x\") == -x\n    assert parse_mathematica(\"+x\") == x\n", "def test_parse_mathematica_unique():\n    assert parse_mathematica('Sin[x]') == sin(x)\n    assert parse_mathematica('2Sin[x]') == 2 * sin(x)\n    assert parse_mathematica('x*Sin[x]') == x * sin(x)\n    assert parse_mathematica('x Sin[x]') == x * sin(x)\n"], "sample_1200": ["def test_get_dimensional_expr():\n    x = symbols('x')\n    v = symbols('v')\n    t = symbols('t')\n    f = Function('f')\n\n    # Test simple expressions\n    assert SI.get_dimensional_expr(m) == length.name\n    assert SI.get_dimensional_expr(kg) == mass.name\n    assert SI.get_dimensional_expr(s) == time.name\n\n    # Test composite expressions\n    assert SI.get_dimensional_expr(m/s) == length.name / time.name\n    assert SI.get_dimensional_expr(m/s**2) == length.name / time.name**2\n    assert SI.get_dimensional_expr(kg * m / s**2) == mass.name * length.name / time.name**2\n\n    # Test functions\n    assert SI.get_dimensional_expr(f(m)) == f(length.name)\n    assert SI.get_dimensional_expr(f(m/s)) == f(length.name / time.name)\n\n    # Test derivatives\n    assert SI.get_dimensional_expr(diff(x, t)) == length.name / time.name\n    assert SI.get_dimensional_expr(diff(v, t)) == length.name / time.name**2\n\n    # Test complex expressions\n    assert SI.get_dimensional_expr(s * (m / s - m / s**2)) == time.name * length.name / time.name\n    assert SI.get_dimensional_expr((m/s) * (kg * m / s**2) * s**2) == length.name * mass.name * length.name\n\n    # Test dimensionless quantities\n    assert SI.get_dimensional_expr(1) == S.One\n    assert SI.get_dimensional_expr(S(1)) == S.One\n", "def test_unit_system_extend():\n    base_units = (meter, kilogram, second)\n    units = (centimeter, gram, kilometer)\n    derived_units = {length: meter}\n    new_dim_sys = Dimension(\n        base_units + units, name=\"SI_extended\", derived_units=derived_units\n    )\n    new_unit_sys = SI.extend(\n        base_units, units, name=\"SI_extended\", dimension_system=new_dim_sys\n    )\n    assert new_unit_sys.name == \"SI_extended\"\n    assert new_unit_sys._base_units == base_units + units\n    assert new_unit_sys.get_dimension_system().name == \"SI_extended\"\n    assert new_unit_sys.derived_units == {**SI.derived_units, **derived_units}\n", "def test_UnitSystem_get_dimensional_expr():\n    us = SI\n    assert us.get_dimensional_expr(kilometer) == length.name\n    assert us.get_dimensional_expr(meter**2) == length.name**2\n    assert us.get_dimensional_expr(kilometer + meter) == length.name\n    assert us.get_dimensional_expr(diff(kilometer, meter)) == 1 / length.name\n    assert us.get_dimensional_expr(sin(meter)) == 1\n    expr = kilometer * (meter**2)\n    assert us.get_dimensional_expr(expr) == length.name**3\n", "def test_unit_system_extend():\n    base_units = (meter, kilogram, second)\n    units = (joule, newton)\n    name = \"SI_extend\"\n    description = \"Extension of SI\"\n    dimension_system = SI.get_dimension_system()\n\n    si_extend = SI.extend(base_units, units, name, description, dimension_system)\n    assert si_extend.name == name\n    assert si_extend.descr == description\n    assert set(si_extend._base_units) == set(SI._base_units + base_units)\n    assert set(si_extend._units) == set(SI._units + units)\n    assert si_extend._dimension_system == dimension_system\n", "def test_get_units_non_prefixed():\n    # Create a UnitSystem\n    base_units = (meter, kilogram, second)\n    units = (joule, newton)\n    unit_system = UnitSystem(base_units, units, name=\"SI\")\n\n    # Test get_units_non_prefixed method\n    non_prefixed_units = unit_system.get_units_non_prefixed()\n\n    assert meter in non_prefixed_units\n    assert kilogram in non_prefixed_units\n    assert second in non_prefixed_units\n    assert joule in non_prefixed_units\n    assert newton in non_prefixed_units\n    assert kilometer not in non_prefixed_units\n    assert kibibyte not in non_prefixed_units\n\n    # Check that physical constants are excluded\n    assert speed_of_light not in non_prefixed_units\n\n    # Create another UnitSystem with prefixed units\n    base_units = (kilometer, kilogram, second)\n    units = (millimeter, joule)\n    unit_system = UnitSystem(base_units, units, name=\"SI\")\n\n    non_prefixed_units = unit_system.get_units_non_prefixed()\n\n    assert kilogram in non_prefixed_units\n    assert second in non_prefixed_units\n    assert joule in non_prefixed_units\n    assert kilometer not in non_prefixed_units\n    assert millimeter not in non_prefixed_units\n"], "sample_1203": ["def test_kernel():\n    G = PermutationGroup([Permutation([0, 2, 1])])\n    H = PermutationGroup([Permutation([0, 1])])\n    phi = homomorphism(G, H, [G.generator], [H.identity])\n    assert phi.kernel().is_subgroup(G)\n    assert phi.kernel().order() == 3\n\n    F, x, y = free_group(\"x, y\")\n    G = FpGroup(F, [x**2, y**2, x*y*x**-1*y**-1])\n    H = PermutationGroup([Permutation([0, 1])])\n    phi = homomorphism(G, H, [G.generators[0]], [H.identity])\n    assert phi.kernel().is_subgroup(G)\n", "def test_homomorphism_restrict_to():\n    S3 = PermutationGroup(Permutation(0, 1, 2), Permutation(0, 1))\n    A3 = AlternatingGroup(3)\n    H = homomorphism(S3, S3, S3.generators, S3.generators)\n    H_restricted = H.restrict_to(A3)\n    assert H_restricted.domain == A3\n    assert H_restricted.codomain == S3\n", "def test_block_homomorphism():\n    from sympy.combinatorics\timport block_homomorphism\n    from sympy.combinatorics.perm_groups import PermutationGroup\n    from sympy.combinatorics.permutations import Permutation\n    from sympy.combinatorics.named_groups import SymmetricGroup\n\n    S4 = SymmetricGroup(4)\n    blocks = [0, 0, 1, 1]\n    G = block_homomorphism(S4, blocks)\n    assert G.is_homomorphism\n    assert G.domain == S4\n    assert G.codomain == SymmetricGroup(2)\n    assert G.kernel().is_subgroup(S4)\n", "def test_kernel():\n    # define a homomorphism and check that the kernel is computed correctly\n    F, a, b = free_group(\"a, b\")\n    G = FpGroup(F, [a**2, b**2, (a*b)**4])\n    P = PermutationGroup([Permutation(0, 1, 2, 3)])\n    T = homomorphism(G, P, [a, b], [P.generators[0], P.generators[0]])\n    K = T.kernel()\n    assert K.order() == 4\n", "def test_kernel_and_image():\n    # test kernel for a trivial homomorphism\n    G = AlternatingGroup(4)\n    H = AlternatingGroup(5)\n    phi = homomorphism(G, H, G.generators, [H.identity]*len(G.generators))\n    assert phi.kernel().is_subgroup(G)\n    assert phi.image().is_subgroup(H)\n    assert phi.kernel().order() == G.order()\n    assert phi.image().order() == 1\n\n    # test kernel for a non-trivial homomorphism\n    F, a, b = free_group(\"a, b\")\n    G = FpGroup(F, [a**3, b**3, (a*b)**2])\n    H = AlternatingGroup(4)\n    phi = group_isomorphism(G, H, isomorphism=True)[1]\n    assert phi.kernel().is_subgroup(G)\n    assert phi.image().is_subgroup(H)\n    assert phi.kernel().order() == 1\n    assert phi.image().order() == H.order()\n\n    # test kernel for an injective homomorphism\n    G = CyclicGroup(5)\n    H = AlternatingGroup(5)\n    phi = homomorphism(G, H, G.generators, H.generators[:1])\n    assert phi.kernel().is_subgroup(G)\n    assert phi.image().is_subgroup(H)\n    assert phi.kernel().order() == 1\n    assert phi.image().order() == 5\n\n    # test kernel for a non-injective homomorphism\n    G = DihedralGroup(8)\n    H = AlternatingGroup(4)\n    phi = homomorphism(G, H, G.generators, [H.identity]*len(G.generators))\n    assert phi.kernel().is_subgroup(G)\n    assert phi.image().is_subgroup(H)\n    assert phi.kernel().order() == G.order()\n    assert phi.image().order() == 1\n"], "sample_1201": ["def test_cgs_gauss_units():\n    # Test conversions of units specific to cgs_gauss\n    assert convert_to(statcoulomb, cgs_gauss, coulomb) == cgs_gauss.get_quantity_scale_factor(coulomb) / cgs_gauss.get_quantity_scale_factor(statcoulomb)\n    assert convert_to(statvolt, cgs_gauss, volt) == cgs_gauss.get_quantity_scale_factor(volt) / cgs_gauss.get_quantity_scale_factor(statvolt)\n    assert convert_to(maxwell, cgs_gauss, weber) == cgs_gauss.get_quantity_scale_factor(weber) / cgs_gauss.get_quantity_scale_factor(maxwell)\n    assert convert_to(gauss, cgs_gauss, tesla) == cgs_gauss.get_quantity_scale_factor(tesla) / cgs_gauss.get_quantity_scale_factor(gauss)\n\n    # Test conversions of base units in cgs_gauss\n    assert convert_to(gram, cgs_gauss, gram) == 1\n    assert convert_to(centimeter, cgs_gauss, centimeter) == 1\n    assert convert_to(second, cgs_gauss, second) == 1\n\n    # Test conversions of derived units in cgs_gauss\n    assert convert_to(erg, cgs_gauss, joule) == cgs_gauss.get_quantity_scale_factor(joule) / cgs_gauss.get_quantity_scale_factor(erg)\n    assert convert_to(dyne, cgs_gauss, newton) == cgs_gauss.get_quantity_scale_factor(newton) / cgs_gauss.get_quantity_scale_factor(dyne)\n", "def test_cgs_gauss_definition():\n    # Test cgs_gauss definition\n    assert set(cgs_gauss._base_units) == {centimeter, gram, second}\n    assert cgs_gauss._name == \"cgs_gauss\"\n    assert cgs_gauss._dimension_system == dimsys_cgs\n\n    # Test dimensional dependencies for derived dimensions\n    assert dimsys_cgs.get_dimensional_dependencies(\"impedance\") == dict(time=1, length=-1)\n    assert dimsys_cgs.get_dimensional_dependencies(\"conductance\") == dict(time=-1, length=1)\n    assert dimsys_cgs.get_dimensional_dependencies(\"capacitance\") == dict(length=1)\n    assert dimsys_cgs.get_dimensional_dependencies(\"inductance\") == dict(time=2, length=-1)\n    assert dimsys_cgs.get_dimensional_dependencies(\"charge\") == dict(mass=S.Half, length=S(3)/2, time=-1)\n    assert dimsys_cgs.get_dimensional_dependencies(\"current\") == dict(mass=S.Half, length=S(3)/2, time=-2)\n    assert dimsys_cgs.get_dimensional_dependencies(\"voltage\") == dict(length=-S.Half, mass=S.Half, time=-1)\n    assert dimsys_cgs.get_dimensional_dependencies(\"magnetic_density\") == dict(length=-S.Half, mass=S.Half, time=-1)\n    assert dimsys_cgs.get_dimensional_dependencies(\"magnetic_flux\") == dict(length=S(3)/2, mass=S.Half, time=-1)\n", "def test_cgs_gauss_units_conversion():\n    # Test conversion of units between cgs_gauss and SI systems\n    assert convert_to(statcoulomb, SI).simplify() == coulomb / (10 * speed_of_light)\n    assert convert_to(coulomb, cgs_gauss).simplify() == 10 * speed_of_light * statcoulomb\n    assert convert_to(erg, SI).simplify() == joule / (10**7)\n    assert convert_to(dyne, SI).simplify() == newton / (10**5)\n    assert convert_to(statvolt, SI).simplify() == volt * speed_of_light / (10**6)\n    assert convert_to(volt, cgs_gauss).simplify() == statvolt * 10**6 / speed_of_light\n    assert convert_to(farad, cgs_gauss).simplify() == (10**5) / (speed_of_light**2 * centimeter)\n    assert convert_to(henry, cgs_gauss).simplify() == (10**5) / (speed_of_light**2 * centimeter * second**-2)\n    assert convert_to(ohm, cgs_gauss).simplify() == (10**5) / (speed_of_light**2 * second * centimeter**-1)\n", "def test_cgs_gauss_units():\n    # Test conversion of CGS-gaussian units\n    assert convert_to(statcoulomb, SI) == coulomb / (10 * speed_of_light)\n    assert convert_to(statvolt, SI) == 10**6 * volt / speed_of_light\n    assert convert_to(dyne, SI) == newton / 10**5\n    assert convert_to(erg, SI) == joule / 10**7\n    assert convert_to(centimeter, SI) == meter / 100\n    assert convert_to(gram, SI) == 0.001 * S.kg\n    assert convert_to(second, SI) == S.second\n", "def test_cgs_gauss_units():\n    # Test base units\n    assert cgs_gauss.get_unit_dimension(centimeter) == length\n    assert cgs_gauss.get_unit_dimension(gram) == mass\n    assert cgs_gauss.get_unit_dimension(second) == S.One\n\n    # Test derived units\n    assert cgs_gauss.get_unit_dimension(statcoulomb) == charge\n    assert cgs_gauss.get_unit_dimension(statampere) == current\n    assert cgs_gauss.get_unit_dimension(statvolt) == voltage\n    assert cgs_gauss.get_unit_dimension(gauss) == magnetic_density\n    assert cgs_gauss.get_unit_dimension(maxwell) == magnetic_flux\n    assert cgs_gauss.get_unit_dimension(oersted) == magnetic_density\n\n    # Test unit conversion to and from SI units\n    assert cgs_gauss.get_unit_scale_factor(coulomb) == 10*speed_of_light*statcoulomb\n    assert cgs_gauss.get_unit_scale_factor(ampere) == 10*speed_of_light*statcoulomb/second\n    assert cgs_gauss.get_unit_scale_factor(volt) == 10**6/speed_of_light*statvolt\n    assert cgs_gauss.get_unit_scale_factor(weber) == 10**8*maxwell\n    assert cgs_gauss.get_unit_scale_factor(tesla) == 10**4*gauss\n\n    # Test Coulomb's constant\n    assert cgs_gauss.get_quantity_dimension(coulomb_constant) == S.One\n    assert cgs_gauss.get_quantity_scale_factor(coulomb_constant) == S.One\n"], "sample_1202": ["def test_float_int():\n    assert Integer(4) * Float(1.0) == Float(4.0)\n    assert 4 * Float(1.0) == Float(4.0)\n    assert int(Float(3.3) / Integer(3)) == 1\n    assert int(Float(3.3) / 3) == 1\n    assert Float(3.3) / Integer(3) == Float(1.1)\n    assert Float(3.3) / 3 == Float(1.1)\n    assert Integer(4) / Float(1.0) == Float(4.0)\n    assert 4 / Float(1.0) == Float(4.0)\n    assert Float(1) / 3 != 0\n", "def test_Float_eq():\n    a = Float(0.1, 3)\n    b = Float('0.1', 3)\n\n    assert not a == b\n    assert a != b\n    assert a == a\n    assert a != a + 1\n    assert a == Float(a, 3)\n    assert not a == Float(a, 4)\n    assert not a == Rational(a)\n", "def test_comp():\n    assert comp(1, 1 + 1e-3) is False\n    assert comp(1, 1 + 1e-5) is True\n    assert comp(1, 1 + 1e-5, '') is False\n    assert comp(1, '1') is True\n    assert comp(1, '1.0') is True\n    assert comp(1, '1.0', '') is False\n    assert comp(1, 1.00000000001, 1e-3) is True\n    assert comp(1, 1.00000000001, 1e-13) is False\n    assert comp(1, 1.00000000001, 1e-14) is False\n", "def test_Float_mpf_norm():\n    # test that mpf_norm returns an mpf tuple that is like the input but\n    # with the sign handled so that zero is returned instead of\n    # (0, m, e, b) -- see issue 6639\n    from sympy import Float\n    f = Float(2.1)\n    assert f._mpf_ == mpf_norm(f._mpf_, f._prec)\n\n    # create a value that is zero but has a non-zero mantissa\n    # this would normally never happen with a valid float\n    # but might occur when coming from another base\n    f = Float((1, 2, 0, 3))\n    assert f._mpf_ == (0, 0, 0, 0)\n    assert f == 0\n", "def test_Float_mpf_norm():\n    assert mpf_norm((1, 2, 3, 4), 50) == (1, 2, 3, 4)\n    assert mpf_norm((0, 2, 3, 4), 50) == fzero\n    assert mpf_norm((0, 1, -452, 53), 50) == (0, 1, -452, 53)\n    assert mpf_norm((1, 2, -3, 4), 50) != (1, 2, -3, 4)\n"], "sample_1205": ["def test_PolyRing_to_domain():\n    R = ring('x,y', QQ)\n    assert R.to_domain() == R.ring\n", "def test_PolyElement_set_ring():\n    _, x, y = ring(\"x,y\", ZZ)\n    _, z = ring(\"z\", ZZ)\n\n    f = x + y\n    g = z + 1\n    raises(ValueError, lambda: f.set_ring(g.ring))\n", "def test_PolyRing_to_ground():\n    R = ring(\"x,y,z\", ZZ)\n\n    assert R.drop_to_ground(\"x\") == R.clone(symbols=\"y,z\", domain=R[\"x\"])\n    assert R.to_ground() == ZZ\n", "def test_PolyRing_getitem():\n    R, x, y, z = ring(\"x,y,z\", ZZ)\n    assert R['x'] == x\n    assert R['y'] == y\n    assert R['z'] == z\n    raises(GeneratorsError, lambda: R['t'])\n", "def test_PolyRing_new():\n    R = ring(\"x,y,z\", ZZ)\n    assert R.gens == (R.x, R.y, R.z)\n    assert R.domain == ZZ\n\n    raises(GeneratorsError, lambda: ring([], ZZ))\n    raises(GeneratorsError, lambda: ring([], ZZ, lex))\n\n    R = ring(\"x,y,z\", QQ)\n    assert R.gens == (R.x, R.y, R.z)\n    assert R.domain == QQ\n\n    R = ring(\"x,y,z\", ZZ, grlex)\n    assert R.gens == (R.x, R.y, R.z)\n    assert R.domain == ZZ\n    assert R.order == grlex\n"], "sample_1204": ["def test_coset_factor():\n    a = Permutation(0, 1, 3, 7, 6, 4)(2, 5)\n    b = Permutation(0, 1, 3, 2)(4, 5, 7, 6)\n    G = PermutationGroup([a, b])\n    c = Permutation(7)(1, 2, 4)(3, 6, 5)\n    assert G.coset_factor(c) == [G.coset_factor(c, True)[i] for i in range(len(G.base))]\n", "def test_coset():\n    # test construction and membership\n    g = Permutation(0, 1)\n    H = PermutationGroup([Permutation(1, 2)])\n    C = Coset(g, H)\n    assert C.is_right_coset\n    assert Permutation(0, 2) in C\n    assert Permutation(0, 1) not in C\n\n    # test that the group is computed correctly when not given\n    C = Coset(g, H)\n    assert C.args[2].is_SymmetricPermutationGroup\n\n    # left and right cosets should be different\n    C_left = Coset(g, H, dir='-')\n    assert C_left.is_left_coset\n    assert C != C_left\n\n    # test as_list\n    assert C.as_list() == [g*h for h in H.elements]\n", "def test_coset_product():\n    g1 = Permutation(0, 1, 2, 3, 4)\n    g2 = Permutation(4)(1, 2, 3)\n    G = PermutationGroup([g1, g2])\n    g = Permutation(4, 5)\n    H = PermutationGroup([g])\n    assert len((Coset(g, G, dir='-')*H).as_list()) == 60\n    assert len((G * Coset(g, H, dir='-')).as_list()) == 60\n", "def test_coset_transversal():\n    G = SymmetricGroup(3)\n    H = AlternatingGroup(3)\n    right_coset = G.coset_transversal(H)\n    assert len(right_coset) == 2\n    coset = [h*g for g in right_coset for h in H]\n    assert set(coset) == set(G.elements)\n", "def test_coset():\n    a = Permutation(1, 2)\n    b = Permutation(0, 1)\n    G = PermutationGroup([a, b])\n    cst = Coset(a, G, dir=\"-\")\n    assert cst.is_left_coset\n    assert not cst.is_right_coset\n\n    cst = Coset(a, G, dir=\"+\")\n    assert not cst.is_left_coset\n    assert cst.is_right_coset\n\n    assert len(cst.as_list()) == G.order()\n"], "sample_1206": ["def test_comp():\n    assert comp(0.5, 0.5) is True\n    assert comp(0.5, 1.0) is False\n    assert comp(0.5, 1.0, '') is False\n    assert comp(0.5, 0.6, 0.1) is True\n    assert comp(0.5, 0.4, 0.1) is True\n    assert comp(0.5, 0.6, 0.05) is False\n    assert comp(0.5, 0.4, 0.05) is False\n    assert comp(0.5, 0.6, 0.001) is False\n    assert comp(0.5, 0.4, 0.001) is False\n    assert comp('0.5', '0.5') is True\n    assert comp('0.5', '1.0') is False\n    assert comp('0.5', '1.0', '') is False\n", "def test_Float_hash():\n    assert hash(Float(1.0)) == hash(Float(1.0, 1))\n    assert hash(Float(1.0, 1)) != hash(Float(1.0, 2))\n    assert hash(Float(1.0)) != hash(Integer(1))\n    assert hash(Float(1.0)) != hash(Rational(1, 1))\n", "def test_Float_hash_and_eq():\n    f1 = Float(1.2, 3)\n    f2 = Float(1.2, 3)\n    f3 = Float(1.2, 4)\n    assert hash(f1) == hash(f2)\n    assert hash(f1) != hash(f3)\n    assert f1 == f2\n    assert f1 != f3\n    assert f1 != 1.2  # Float is not equal to a Python float\n", "def test_Float_eq():\n    assert Float(.1) == Float(.1)\n    assert Float(.1, 2) != Float(.1, 3)\n    assert Float(.1, 2) != Float(.10, 3)\n    assert same_and_same_prec(Float(.1, 4), Float(.1, 4))\n    assert same_and_same_prec(Float(.1, 4), Float(.1, 5)) is False\n    assert same_and_same_prec(Float(.1, 4), Float(.10, 4)) is False\n    assert same_and_same_prec(Float(.1, 4), Float(.100, 4)) is False\n    assert not Float(.1, 2) != Float(.1, 2)\n    assert Float(.1, 2) != Float(.11)\n    assert Float(.1, 3) == Float(.1, 3)\n    assert Float(.1, 3) == Float(.10, 3)\n    assert Float(.1, 3) == Float(.100, 3)\n    assert Float(.1, 3) == Float(.1000, 3)\n    assert Float(.1, 3) != Float(.1001, 3)\n    assert not Float(.1, 3) != Float(.100, 3)\n    assert Float(.1, 3) != Float(.100, 4)\n    assert not Float(.1, 3) != Float(.10, 3)\n    assert Float(.1, 3) != Float(.11, 3)\n    assert not Float(.1, 3) != Float(.1, 3)\n    assert Float(.1, 3) != Float(.11, 4)\n    assert Float(.1, 3) != Float(.101, 4)\n    assert Float(.1, 3) != Float(.1001, 4)\n    assert not Float(.1, 3) != Float(.10000, 5)\n    assert Float(.1, 3) != Float(.10001, 5)\n    assert not Float(.1, 3) != Float(.10, 3)\n    assert not Float(.1, 3) != Float(.100, 3)\n    assert not Float(.1, 3) != Float(.1000, 3)\n    assert not Float(.1, 3) != Float(.10000, 5)\n", "def test_Pow_number_infinity():\n    assert Pow(S.NegativeOne, oo, evaluate=False).is_number\n    assert Pow(S.NegativeOne, -oo, evaluate=False).is_number\n    assert Pow(S.NegativeOne, S(oo), evaluate=False).is_number\n    assert Pow(S.NegativeOne, S(-oo), evaluate=False).is_number\n    assert Pow(S(-2), oo, evaluate=False).is_number\n    assert Pow(S(-2), -oo, evaluate=False).is_number\n    assert Pow(S(-2), S(oo), evaluate=False).is_number\n    assert Pow(S(-2), S(-oo), evaluate=False).is_number\n    assert Pow(S(2), oo, evaluate=False).is_number\n    assert Pow(S(2), -oo, evaluate=False).is_number\n    assert Pow(S(2), S(oo), evaluate=False).is_number\n    assert Pow(S(2), S(-oo), evaluate=False).is_number\n    assert Pow(S(2), S.NegativeOne, evaluate=False).is_number\n    assert Pow(S(2), S(-2), evaluate=False).is_number\n    assert Pow(S(2), S(2), evaluate=False).is_number\n"], "sample_1207": ["def test_factorial_notation():\n    raises(TokenError, lambda: parse_expr('x!!', transformations=standard_transformations))\n    raises(TokenError, lambda: parse_expr('x!!!', transformations=standard_transformations))\n    assert parse_expr('x!', transformations=standard_transformations) == factorial(Symbol('x'))\n    assert parse_expr('x!!', transformations=(factorial_notation,)) == factorial2(Symbol('x'))\n    assert parse_expr('x!!!', transformations=(factorial_notation,)) == factorial2(factorial(Symbol('x')))\n", "def test_factorial_notation():\n    x = Symbol('x')\n    assert parse_expr('x!', transformations=standard_transformations) == factorial(x)\n    assert parse_expr('x!!', transformations=standard_transformations) == factorial2(x)\n    assert parse_expr('x!!!', transformations=standard_transformations) == factorial(factorial2(x))\n", "def test_evaluateFalse():\n    assert parse_expr('2**3', evaluate=False) == Pow(Integer(2), Integer(3), evaluate=False)\n    assert parse_expr('1+2', evaluate=False) == Integer(1) + Integer(2)\n    assert parse_expr('1/2', evaluate=False) == Mul(Integer(1), Pow(Integer(2), -1, evaluate=False), evaluate=False)\n    assert parse_expr('3/4', evaluate=False) == Mul(Integer(3), Pow(Integer(4), -1, evaluate=False), evaluate=False)\n    assert parse_expr('3/4*5', evaluate=False) == Mul(Integer(3), Mul(Pow(Integer(4), -1, evaluate=False), Integer(5), evaluate=False), evaluate=False)\n    assert parse_expr('2=3', evaluate=False) == Eq(Integer(2), Integer(3), evaluate=False)\n    assert parse_expr('2<3', evaluate=False) == Lt(Integer(2), Integer(3), evaluate=False)\n    assert parse_expr('2<=3', evaluate=False) == Le(Integer(2), Integer(3), evaluate=False)\n    assert parse_expr('2>3', evaluate=False) == Gt(Integer(2), Integer(3), evaluate=False)\n    assert parse_expr('2>=3', evaluate=False) == Ge(Integer(2), Integer(3), evaluate=False)\n    assert parse_expr('2!=3', evaluate=False) == Ne(Integer(2), Integer(3), evaluate=False)\n", "def test_factorial_notation():\n    x = Symbol('x')\n    assert parse_expr('3!', transformations=standard_transformations) == factorial(3)\n    assert parse_expr('3!!', transformations=standard_transformations) == factorial2(3)\n    assert parse_expr('3*x!', transformations=standard_transformations) == 3 * factorial(x)\n    raises(TokenError, lambda: parse_expr('3!!!', transformations=standard_transformations))\n", "def test_factorial_notation():\n    # Test factorial notation with a numeric literal\n    assert parse_expr('5!', transformations=standard_transformations) == factorial(5)\n\n    # Test factorial notation with a symbol\n    x = Symbol('x')\n    assert parse_expr('x!', transformations=standard_transformations) == factorial(x)\n\n    # Test factorial notation with a more complex expression\n    assert parse_expr('(2*x+1)!', transformations=standard_transformations) == factorial(2*x + 1)\n\n    # Test double factorial notation\n    assert parse_expr('5!!', transformations=standard_transformations) == factorial2(5)\n"], "sample_1209": ["def test_prefix_unit():\n    pref = {\"m\": PREFIXES[\"m\"], \"c\": PREFIXES[\"c\"], \"d\": PREFIXES[\"d\"]}\n    units = prefix_unit(meter, pref)\n    assert len(units) == 3\n\n    # Test dimensional equivalence and scale factors\n    for unit in units:\n        assert unit.dimension == meter.dimension\n        assert unit.scale_factor != meter.scale_factor\n", "def test_prefix_unit():\n    # Test that new units are created correctly\n    new_units = prefix_unit(length, PREFIXES)\n    assert len(new_units) == len(PREFIXES)\n\n    # Test that new units have the correct scale factor\n    for unit in new_units:\n        prefix = unit.abbrev[:-1]  # Get the prefix from the unit's abbreviation\n        assert unit.scale_factor == PREFIXES[prefix].scale_factor\n\n    # Test that new units are instances of Quantity\n    for unit in new_units:\n        assert isinstance(unit, Quantity)\n\n    # Test that binary prefixes work correctly\n    new_units = prefix_unit(meter, {\"Ki\": kibi})\n    assert len(new_units) == 1\n    assert new_units[0].scale_factor == kibi.scale_factor\n", "def test_prefix_unit():\n    # Test prefix_unit function\n    m = meter\n    pref = {\"m\": PREFIXES[\"m\"], \"c\": PREFIXES[\"c\"], \"d\": PREFIXES[\"d\"]}\n    prefixed_units = prefix_unit(m, pref)\n    assert len(prefixed_units) == 3\n    assert isinstance(prefixed_units[0], Quantity)\n    assert prefixed_units[0].name == 'millimeter'\n    assert prefixed_units[1].name == 'centimeter'\n    assert prefixed_units[2].name == 'decimeter'\n\n    # Test prefix_unit function with binary prefixes\n    bin_pref = {\"Ki\": kibi}\n    prefixed_units_bin = prefix_unit(m, bin_pref)\n    assert len(prefixed_units_bin) == 1\n    assert isinstance(prefixed_units_bin[0], Quantity)\n    assert prefixed_units_bin[0].name == 'kibimeter'\n", "def test_prefix():\n    # Test that a prefix can be created and that its properties are correctly set\n    p = Prefix('test', 't', 2)\n    assert str(p) == 't'\n    assert p.name == 'test'\n    assert p.abbrev == 't'\n    assert p.scale_factor == 100\n    assert p.base == 10\n    assert p._exponent == 2\n\n    # Test that a prefix can be created with a custom base\n    p = Prefix('test', 't', 2, base=2)\n    assert p.base == 2\n    assert p.scale_factor == 4\n\n    # Test that a prefix can be multiplied and divided\n    p1 = Prefix('test1', 't1', 2)\n    p2 = Prefix('test2', 't2', 3)\n    assert (p1 * p2).scale_factor == p1.scale_factor * p2.scale_factor\n    assert (p1 / p2).scale_factor == p1.scale_factor / p2.scale_factor\n\n    # Test that a prefix can be multiplied and divided by a number\n    assert (p1 * 2).scale_factor == p1.scale_factor * 2\n    assert (p1 / 2).scale_factor == p1.scale_factor / 2\n\n    # Test that a prefix can be used to create a new unit\n    assert str(kilo * meter) == '1000*meter'\n", "def test_prefix_operations():\n    assert kilo * 2 == 2000\n    assert kilo * kilo == 1000000\n    assert kilo / kilo == 1\n    assert 1 / kilo == Rational(1, 1000)\n    assert 1 / (kilo * kilo) == Rational(1, 1000000)\n    assert kilo * (1 / kilo) == 1\n    assert Prefix('kilo', 'k', 3) * Prefix('kilo', 'k', 3) == 1000000\n    assert (Prefix('kilo', 'k', 3) * Prefix('kilo', 'k', 3)).is_commutative\n"], "sample_1208": ["def test_matrix_student_t():\n    v = symbols('v', positive=True)\n    M = MatrixStudentT('M', v, [[1, 2]], [[1, 0], [0, 1]], [1])\n    assert M.pspace.distribution.set == MatrixSet(1, 2, S.Reals)\n    X = MatrixSymbol('X', 1, 2)\n    assert density(M)(X).doit() == gamma(v/2 + 1)*Determinant((Matrix([[-1, -2]]) + X)*(Matrix([[-1], [-2]]) + X.T) + Matrix([[1]]))**(-v/2 - 1)/(pi**1.0*gamma(v/2)*Determinant(Matrix([[1]]))**1.0*Determinant(Matrix([[1, 0], [0, 1]]))**0.5)\n    raises(ValueError, lambda: MatrixStudentT('M', -2, [[1, 2]], [[1, 0], [0, 1]], [1]))\n", "def test_matrix_student_t_sample():\n    try:\n        import scipy\n    except ImportError:\n        skip(\"Scipy is not installed. Skip sampling tests\")\n    M = MatrixStudentT('M', 5, [[1, 2]], [[1, 0], [0, 1]], [1])\n    sample(M, size=(3, 3), library='scipy')\n    sample(M, library='scipy')\n", "def test_matrix_distributions():\n    M = MatrixSymbol('M', 2, 2)\n    n, p, a, b = symbols('n p a b', positive=True)\n    # Matrix Gamma Distribution\n    X = MatrixGamma('X', a, b, [[2, 1], [1, 2]])\n    assert density(X)(M).doit() == exp(-Trace(Matrix([[-2/3, 1/3], [1/3, -2/3]])*M)/b)*Determinant(M)**(a - 3/2)/(3**a*sqrt(pi)*b**(2*a)*gamma(a)*gamma(a - 1/2))\n    # Wishart Distribution\n    W = Wishart('W', n, [[2, 1], [1, 2]])\n    assert density(W)(M).doit() == exp(Trace(Matrix([[-1/3, 1/6], [1/6, -1/3]])*M))*Determinant(M)**(n/2 - 3/2)/(2**n*3**(n/2)*sqrt(pi)*gamma(n/2)*gamma(n/2 - 1/2))\n    # Matrix Normal Distribution\n    N = MatrixNormal('N', [[1, 2]], [1], [[1, 0], [0, 1]])\n    assert density(N)(MatrixSymbol('X', 1, 2)).doit() == exp(-Trace((Matrix([[-1], [-2]]) + MatrixSymbol('X', 1, 2).T)*(Matrix([[-1, -2]]) + MatrixSymbol('X', 1, 2)))/2)/(2*pi)\n    # Matrix Student-T Distribution\n    T = MatrixStudentT('T', a, [[1, 2]], [[1, 0], [0, 1]], [1])\n    assert density(T)(MatrixSymbol('X', 1, 2)).doit() == gamma(a/2 + 1)*Determinant((Matrix([[-1, -2]]) + MatrixSymbol('X', 1, 2))*(Matrix([[-1], [-2]]) + MatrixSymbol('X', 1, 2).T) + Matrix([[1]]))**(-a/2 - 1)/(pi*gamma(a/2)*Determinant(Matrix([[1]]))*Determinant(Matrix([[1, ", "def test_matrix_gamma_distribution():\n    M = MatrixGamma('M', 1, 2, [[1, 0], [0, 1]])\n    assert M.pspace.distribution.set == MatrixSet(2, 2, S.Reals)\n    assert M.pspace.distribution.dimension == (2, 2)\n    X = MatrixSymbol('X', 2, 2)\n    assert density(M)(X).doit() == exp(-Trace(X)/2) * Determinant(X)**S.Half / (\n            4 * pi * sqrt(pi) * sqrt(gamma(1/2)))\n", "def test_matrix_distributions():\n    M = MatrixGamma('M', 1, 1, [[1, 0], [0, 1]])\n    N = MatrixNormal('N', [[1, 2]], [1], [[1, 0], [0, 1]])\n    W = Wishart('W', 2, [[1, 0], [0, 1]])\n    ST = MatrixStudentT('ST', 2, [[1, 2]], [[1, 0], [0, 1]], [1])\n\n    X = MatrixSymbol('X', 2, 2)\n    Y = MatrixSymbol('Y', 1, 2)\n    Z = MatrixSymbol('Z', 2, 2)\n    T = MatrixSymbol('T', 1, 2)\n\n    # Test Matrix Gamma Distribution\n    assert density(M)(X).doit() == exp(-Trace(X)/1)/(pi*gamma(1))\n\n    # Test Wishart Distribution\n    assert density(W)(Z).doit() == exp(-Trace(Z)/2)/(4*pi*sqrt(pi)*gamma(S(3)/2))\n\n    # Test Matrix Normal Distribution\n    assert density(N)(Y).doit() == exp(-Trace((Matrix([[-1, -2]]) + Y)*(Matrix([[-1], [-2]]) + Y.T))/2)/(2*pi)\n\n    # Test Matrix StudentT Distribution\n    assert density(ST)(T) == gamma(1)*Determinant((Matrix([[-1, -2]]) + T)*(Matrix([[-1], [-2]]) + T.T) + Matrix([[1]]))**(-1)/(pi*gamma(1)*Determinant(Matrix([[1]]))*Determinant(Matrix([[1, 0], [0, 1]]))**S(1)/2)\n"]}, "raw_scored_generations": {"sample_2": [false, false, false, true, false], "sample_3": [false, false, false, true, false], "sample_5": [false, false, false, false, false], "sample_4": [false, false, false, false, false], "sample_0": [true, false, false, false, false], "sample_1": [false, true, true, false, false], "sample_6": [false, false, true, false, false], "sample_7": [false, false, false, false, false], "sample_8": [false, false, false, false, false], "sample_9": [true, true, true, true, true], "sample_10": [false, false, false, false, false], "sample_11": [true, true, false, false, false], "sample_12": [false, false, false, true, false], "sample_13": [false, false, false, true, true], "sample_14": [true, true, false, true, true], "sample_15": [true, true, true, true, true], "sample_16": [false, false, true, false, false], "sample_17": [false, false, false, false, false], "sample_18": [false, false, false, false, false], "sample_19": [false, false, false, false, false], "sample_20": [false, false, false, true, false], "sample_21": [false, false, true, false, false], "sample_22": [false, false, false, false, false], "sample_23": [true, true, false, false, true], "sample_24": [false, false, false, false, false], "sample_25": [false, false, false, false, false], "sample_26": [false, false, false, false, false], "sample_27": [false, false, false, false, false], "sample_28": [false, true, false, false, false], "sample_29": [false, false, false, false, false], "sample_30": [false, true, false, false, false], "sample_31": [false, false, false, false, false], "sample_32": [false, false, false, false, false], "sample_33": [true, false, true, true, true], "sample_34": [false, false, false, false, true], "sample_35": [true, false, false, true, false], "sample_36": [true, true, true, true, true], "sample_37": [false, false, false, false, false], "sample_38": [false, false, false, false, false], "sample_39": [false, false, false, false, false], "sample_40": [true, true, false, false, false], "sample_41": [false, false, false, false, false], "sample_42": [false, true, true, false, false], "sample_43": [false, false, true, true, true], "sample_44": [true, false, false, false, false], "sample_45": [false, true, true, true, false], "sample_46": [false, false, false, false, false], "sample_47": [false, false, true, true, true], "sample_48": [true, true, true, true, true], "sample_49": [true, true, true, true, true], "sample_50": [true, true, true, true, true], "sample_51": [false, false, false, false, false], "sample_52": [true, true, true, true, true], "sample_54": [false, false, false, false, false], "sample_53": [false, true, true, true, false], "sample_55": [false, false, false, false, false], "sample_58": [false, false, false, false, false], "sample_56": [false, false, false, false, false], "sample_57": [true, false, true, false, true], "sample_59": [false, false, false, false, false], "sample_60": [true, true, true, true, true], "sample_61": [false, false, false, false, false], "sample_62": [true, true, true, true, true], "sample_63": [true, true, true, true, true], "sample_64": [false, false, true, true, false], "sample_65": [false, false, false, false, false], "sample_67": [false, false, false, false, false], "sample_66": [false, false, false, false, false], "sample_68": [true, true, true, false, false], "sample_69": [true, true, false, false, true], "sample_70": [true, true, true, true, true], "sample_71": [false, false, false, false, false], "sample_72": [false, false, false, false, false], "sample_73": [true, true, true, true, false], "sample_75": [true, true, true, true, true], "sample_74": [true, true, true, true, true], "sample_76": [true, true, true, true, true], "sample_77": [false, false, false, false, false], "sample_78": [false, false, false, false, false], "sample_79": [false, false, true, true, false], "sample_80": [false, false, false, false, false], "sample_82": [true, true, true, false, true], "sample_81": [false, false, false, false, false], "sample_83": [false, false, false, false, false], "sample_85": [true, true, true, true, true], "sample_84": [true, true, true, true, true], "sample_86": [false, false, false, false, false], "sample_88": [false, true, true, true, true], "sample_87": [true, true, false, false, false], "sample_89": [false, false, false, true, true], "sample_90": [false, false, false, false, false], "sample_91": [true, true, true, true, true], "sample_92": [false, false, false, false, false], "sample_93": [true, true, true, true, true], "sample_94": [false, false, false, false, false], "sample_95": [false, false, false, false, false], "sample_98": [false, false, false, false, false], "sample_96": [false, true, false, false, true], "sample_99": [false, true, false, true, true], "sample_97": [false, true, true, true, false], "sample_100": [true, false, true, true, true], "sample_102": [true, true, true, true, true], "sample_101": [true, true, true, true, true], "sample_103": [true, true, true, true, true], "sample_104": [false, true, true, true, true], "sample_107": [false, true, true, true, false], "sample_106": [true, true, true, false, false], "sample_105": [true, false, true, true, true], "sample_108": [false, false, false, false, false], "sample_109": [true, true, true, true, true], "sample_111": [false, true, true, false, false], "sample_110": [false, false, false, false, false], "sample_112": [true, true, true, true, true], "sample_113": [true, true, true, false, true], "sample_114": [true, true, true, true, true], "sample_115": [true, true, false, true, true], "sample_116": [false, false, false, false, false], "sample_117": [true, false, false, false, false], "sample_118": [true, false, false, true, true], "sample_119": [false, false, false, false, false], "sample_120": [false, false, false, false, false], "sample_121": [true, false, false, false, true], "sample_122": [false, true, false, true, true], "sample_123": [true, true, false, true, true], "sample_124": [false, false, false, false, false], "sample_125": [false, false, false, false, false], "sample_126": [true, true, true, true, true], "sample_127": [true, true, true, true, true], "sample_128": [true, true, true, false, true], "sample_129": [false, false, true, false, false], "sample_130": [false, false, false, false, false], "sample_131": [true, false, true, false, false], "sample_132": [true, true, true, false, true], "sample_133": [false, true, false, false, false], "sample_135": [true, true, true, true, true], "sample_134": [false, false, false, false, false], "sample_136": [false, false, false, false, false], "sample_139": [true, true, true, true, true], "sample_137": [true, true, true, true, true], "sample_138": [false, false, true, true, true], "sample_140": [false, false, false, false, false], "sample_141": [false, true, false, false, true], "sample_142": [false, false, false, false, false], "sample_143": [false, false, false, false, false], "sample_144": [false, false, false, false, false], "sample_145": [false, false, false, false, false], "sample_146": [true, true, true, false, true], "sample_147": [true, true, true, true, true], "sample_148": [true, true, true, true, true], "sample_151": [true, true, true, true, true], "sample_149": [false, false, false, false, false], "sample_152": [true, true, true, true, true], "sample_150": [false, false, false, false, false], "sample_153": [false, false, false, false, false], "sample_154": [false, false, false, false, false], "sample_155": [false, false, false, false, false], "sample_156": [false, false, false, false, false], "sample_157": [true, false, false, false, false], "sample_158": [false, false, false, false, false], "sample_159": [false, false, false, false, false], "sample_160": [false, false, false, false, false], "sample_161": [false, false, false, false, false], "sample_162": [true, false, false, false, false], "sample_163": [false, false, false, false, true], "sample_164": [false, false, false, false, false], "sample_165": [false, false, false, false, false], "sample_166": [false, false, false, false, false], "sample_167": [false, false, false, false, false], "sample_168": [true, true, true, true, true], "sample_169": [false, false, false, false, false], "sample_171": [false, false, false, false, false], "sample_170": [true, true, false, false, true], "sample_172": [false, false, false, true, true], "sample_173": [true, true, true, true, true], "sample_174": [true, true, true, true, true], "sample_175": [true, true, true, true, true], "sample_176": [true, true, true, true, true], "sample_177": [true, true, true, true, true], "sample_178": [true, true, true, true, true], "sample_180": [false, false, false, false, false], "sample_179": [true, false, false, false, false], "sample_182": [true, true, true, true, true], "sample_181": [true, true, true, true, true], "sample_183": [true, true, true, true, true], "sample_184": [false, false, false, false, false], "sample_185": [false, false, false, false, false], "sample_186": [false, false, false, false, false], "sample_187": [false, false, false, false, false], "sample_188": [true, true, true, true, true], "sample_189": [false, false, true, false, false], "sample_190": [true, true, true, true, true], "sample_191": [false, true, true], "sample_192": [true, true, true, true, true], "sample_193": [true, false, false, false, false], "sample_194": [false, false, false, false, false], "sample_195": [false, false, true, true, false], "sample_196": [true, true, true, true, true], "sample_198": [true, true, true, true, true], "sample_197": [true, true, true, true, true], "sample_199": [true, true, true, true, true], "sample_200": [true, true, true, true, true], "sample_201": [false, false, false, false, false], "sample_202": [false, false, false, false, false], "sample_203": [false, false, false, false, false], "sample_204": [false, true, false, false, false], "sample_205": [false, false, false, false, false], "sample_206": [false, false, false, false, false], "sample_207": [false, false, false, false, false], "sample_208": [true, true, true, true, true], "sample_209": [true, false, false, false, false], "sample_210": [true, true, true, true, true], "sample_211": [true, true, true, true, false], "sample_213": [false, false, false, false, true], "sample_212": [false, true, false, false, false], "sample_214": [false, false, false, false, false], "sample_215": [false, true, true, false, false], "sample_216": [true, true, true, true, true], "sample_217": [true, true, true, true, true], "sample_218": [true, true, false, false, true], "sample_219": [true, true, true, true, true], "sample_220": [false, false, false, false, false], "sample_221": [true, true, true, true, true], "sample_222": [false, false, false, false, false], "sample_223": [true, true, true, true, true], "sample_224": [true, true, true, true, true], "sample_225": [true, true, true, true, false], "sample_226": [true, false, true, true, false], "sample_227": [true, true, true, true, true], "sample_228": [true, true, true, true, true], "sample_229": [true, true, true, true, true], "sample_230": [false, false, false, false, false], "sample_231": [false, true, true, true, false], "sample_232": [false, false, false, false, false], "sample_233": [false, false, false, false, false], "sample_234": [true, true, true, true, true], "sample_235": [false, true, true, true, true], "sample_236": [true, true, true, true, true], "sample_237": [false, false, false, false, false], "sample_238": [false, false, false, false, false], "sample_239": [true, true, true, true, true], "sample_240": [false, false, false, false, false], "sample_241": [true, true, true, true, true], "sample_242": [false, false, false, false, false], "sample_243": [false, false, false, false, false], "sample_244": [true, true, true, true, true], "sample_245": [true, false, false, true, true], "sample_246": [false, false, false, true, true], "sample_247": [true, true, true, true, true], "sample_248": [true, true, true, true, true], "sample_249": [false, false, true, false, true], "sample_250": [true, true, true, true, true], "sample_251": [true, true, true, true, true], "sample_252": [false, false, false, false, false], "sample_253": [true, true, false, false, false], "sample_254": [true, true, true, true, true], "sample_256": [false, true, false, true, true], "sample_255": [true, true, true, true, true], "sample_257": [false, false, false, false, false], "sample_258": [false, false, false, false, false], "sample_259": [true, true, true, true, true], "sample_260": [true, true, true, true, true], "sample_261": [false, false, false, false, false], "sample_262": [false, false, false, false, false], "sample_263": [false, false, true, false, false], "sample_264": [false, false, false, false, false], "sample_265": [true, true, true, true, true], "sample_266": [false, false, false, false, false], "sample_267": [false, true, true, true, true], "sample_268": [true, false, false, false, true], "sample_269": [true, false, false, false, false], "sample_270": [false, false, false, false, false], "sample_271": [true, false, true, false, false], "sample_272": [false, false, false, false, false], "sample_273": [true, true, false, false, false], "sample_274": [false, false, false, false, false], "sample_275": [true, true, true, true, true], "sample_276": [true, true, true, true, true], "sample_277": [false, false, false, false, false], "sample_278": [false, true, true, true, true], "sample_279": [false, false, false, false, false], "sample_280": [true, true, true, true, true], "sample_281": [true, true, true, true, true], "sample_282": [true, true, true, true, true], "sample_283": [true, true, true, true, true], "sample_284": [true, true, true, true, true], "sample_285": [true, true, true, true, true], "sample_286": [true, false, false, false, false], "sample_287": [false, false, false, false, false], "sample_288": [false, false, false, false, false], "sample_289": [false, false, false, false, false], "sample_290": [true, true, true, true, true], "sample_291": [true, false, true, true, true], "sample_292": [true, true, true, true, true], "sample_293": [false, false, true, false, false], "sample_294": [true, true, true, true, true], "sample_295": [true, true, true, true, true], "sample_296": [false, false, false, false, false], "sample_297": [true, true, true, true, false], "sample_298": [false, false, false, false, false], "sample_299": [false, false, false, false, false], "sample_300": [false, false, false, false, false], "sample_301": [true, false, true, false, false], "sample_302": [true, true, true, true, true], "sample_303": [true, true, true, true, true], "sample_304": [false, false, false, false, false], "sample_305": [true, true, true, true, true], "sample_306": [false, false, false, false, false], "sample_307": [true, true, true, true, true], "sample_308": [true, true, true, true, true], "sample_309": [true, true, true, true, true], "sample_310": [true, true, true, true, true], "sample_312": [true, true, true, true, true], "sample_311": [true, false, false, false, true], "sample_313": [false, false, false, false, false], "sample_314": [true, true, false, false, true], "sample_315": [false, false, false, false, false], "sample_316": [false, false, false, false, false], "sample_317": [true, true, true, true, true], "sample_318": [false, false, false, false, false], "sample_319": [true, true, true, true, true], "sample_320": [false, false, false, false, false], "sample_321": [true, true, true, true, true], "sample_322": [false, false, false, false, false], "sample_323": [false, false, false, false, false], "sample_324": [false, true, true, true, true], "sample_325": [false, false, false, false, false], "sample_326": [false, false, true, true, false], "sample_327": [false, false, false, false, false], "sample_328": [true, true, true, true, true], "sample_329": [false, false, false, false, false], "sample_330": [false, false, false, false, false], "sample_331": [false, false, false, false, false], "sample_332": [true, true, true, true, true], "sample_333": [false, false, false, false, false], "sample_334": [false, false, false, false, false], "sample_335": [false, false, false, false, false], "sample_336": [true, false, false, false, false], "sample_337": [false, false, false, false, false], "sample_338": [true, true, true, true, true], "sample_339": [false, false, false, true, false], "sample_340": [false, false, false, false, false], "sample_341": [true, true, true, true, true], "sample_342": [true, true, true, true, true], "sample_343": [false, false, false, false, false], "sample_344": [true, true, true, true, true], "sample_345": [false, true, false, true, false], "sample_346": [false, false, false, false, false], "sample_347": [false, false, false, false, false], "sample_348": [false, false, false, false, false], "sample_349": [true, true, true, true, true], "sample_350": [true, true, true, true, true], "sample_351": [true, true, true, true, false], "sample_352": [true, true, false, false, true], "sample_353": [false, false, false, false, false], "sample_354": [false, false, false, false, false], "sample_355": [false, false, false, false, true], "sample_356": [true, true, true, true, true], "sample_357": [true, true, true, true, true], "sample_358": [false, false, false, false, false], "sample_359": [true, false, false, false, false], "sample_360": [false, false, false, false, false], "sample_361": [false, false, false, false, true], "sample_362": [true, true, true, true, true], "sample_363": [false, false, true, false, false], "sample_364": [false, false, false, false, false], "sample_365": [false, false, false, false, false], "sample_366": [false, false, false, false, false], "sample_367": [false, false, false, false, false], "sample_368": [false, false, false, false, false], "sample_369": [true, true, true, true, true], "sample_370": [true, true, true, true, true], "sample_371": [true, true, true, true, true], "sample_372": [false, false, false, false, true], "sample_373": [true, true, true, true, true], "sample_374": [true, true, true, true, true], "sample_375": [true, true, true, true, true], "sample_376": [false, false, false, false, false], "sample_377": [false, true, true, true, false], "sample_378": [true, true, true, true, true], "sample_379": [false, true, true, false, true], "sample_380": [true, true, true, true, true], "sample_381": [true, true, true, true, true], "sample_382": [false, false, false, true, true], "sample_383": [false, false, false, true, false], "sample_384": [true, true, true, true, true], "sample_385": [true, true, true, true, true], "sample_386": [true, false, false, true, true], "sample_387": [false, true, true, true, true], "sample_388": [true, true, true, true, true], "sample_389": [true, true, false, true, false], "sample_390": [true, true, true, true, true], "sample_391": [true, true, true, true, true], "sample_392": [false, false, false, false, false], "sample_393": [false, false, false, false, false], "sample_394": [false, false, false, false, false], "sample_395": [false, false, false, false, true], "sample_396": [false, true, true, true, true], "sample_397": [true, true, true, true, true], "sample_398": [true, false, false, false, false], "sample_399": [true, true, true, true, true], "sample_400": [true, true, true, true, true], "sample_401": [true, true, true, true, true], "sample_402": [false, false, false, false, false], "sample_403": [false, false, false, false, false], "sample_404": [true, true, true, true, true], "sample_405": [false, false, false, false, false], "sample_406": [false, true, false, false, true], "sample_407": [true, true, true, true, true], "sample_408": [true, true, true, true, true], "sample_409": [false, false, false, false, false], "sample_410": [false, false, false, false, false], "sample_411": [false, false, false, false, false], "sample_412": [false, false, false, false, false], "sample_413": [false, false, false, false, false], "sample_414": [false, false, false, true, false], "sample_415": [false, false, false, false, false], "sample_416": [true, true, false, true, false], "sample_417": [true, false, false, false, false], "sample_418": [false, false, false, false, false], "sample_419": [true, true, true, true, true], "sample_420": [false, false, false, false, false], "sample_421": [true, true, true, true, true], "sample_422": [false, true, true, true, true], "sample_423": [true, true, true, true, true], "sample_424": [false, true, true, false, false], "sample_425": [false, false, false, false, false], "sample_426": [true, true, true, true, true], "sample_427": [true, true, true, true, true], "sample_428": [false, false, false, false, false], "sample_429": [true, false, false, false, true], "sample_430": [false, true, true, true, true], "sample_431": [false, false, false, true, false], "sample_432": [true, true, true, true, false], "sample_433": [false, true, true, true, true], "sample_434": [false, true, false, false, false], "sample_435": [false, true, false, true, true], "sample_436": [false, false, false, false, false], "sample_437": [false, false, false, false, false], "sample_438": [false, false, false, false, false], "sample_439": [false, false, false, false, false], "sample_440": [true, true, true, true, true], "sample_441": [false, false, false, true, false], "sample_442": [false, false, false, false, false], "sample_443": [false, false, false, false, false], "sample_444": [true, true, false, false, true], "sample_445": [true, true, true, true, true], "sample_446": [false, false, false, false, true], "sample_447": [true, true, true, true, true], "sample_448": [false, false, false, false, false], "sample_449": [true, true, false, true, true], "sample_450": [true, true, true, true, true], "sample_451": [true, true, true, true, true], "sample_453": [true, true, true, true, true], "sample_452": [false, false, false, false, false], "sample_454": [false, false, false, false, false], "sample_455": [false, false, false, false, false], "sample_456": [true, true, true, true, false], "sample_457": [false, false, false, false, false], "sample_458": [false, true, false, true, false], "sample_459": [true, true, true, true, true], "sample_460": [true, false, false, false, false], "sample_461": [false, false, false, false, false], "sample_462": [false, true, true, false, false], "sample_463": [true, true, true, true, true], "sample_464": [false, false, false, false, false], "sample_465": [true, true, true, true, true], "sample_466": [false, false, false, false, false], "sample_467": [true, true, true, true, true], "sample_469": [true, true, true, true, true], "sample_468": [false, false, false, false, false], "sample_470": [false, false, false, false, false], "sample_471": [false, false, false, false, false], "sample_472": [true, true, true, true, true], "sample_473": [false, false, false, false, false], "sample_474": [false, false, false, false, false], "sample_475": [false, true, false, false, true], "sample_476": [true, true, false, true, false], "sample_477": [false, false, false, false, false], "sample_478": [true, true, true, false, false], "sample_479": [true, true, true, true, true], "sample_480": [false, false, false, false, false], "sample_481": [false, false, false, false, false], "sample_482": [false, false, false, false, false], "sample_483": [false, true, true, true, true], "sample_484": [false, false, false, false, false], "sample_485": [false, false, false, false, false], "sample_486": [false, false, false, false, false], "sample_487": [false, false, true, false, false], "sample_488": [false, false, false, false, false], "sample_489": [true, true, true, true, true], "sample_490": [false, false, false, false, false], "sample_491": [false, false, false, false, true], "sample_492": [false, false, false, false, false], "sample_493": [true, true, true, true, true], "sample_494": [false, false, false, false, false], "sample_495": [true, true, true, true, true], "sample_496": [true, false, false, false, false], "sample_497": [false, false, false, false, false], "sample_498": [false, false, false, false, false], "sample_499": [false, false, true, true, false], "sample_500": [false, false, false, false, false], "sample_501": [true, false, true, true, true], "sample_502": [true, true, false, false, false], "sample_503": [false, false, false, false, false], "sample_504": [false, false, false, false, false], "sample_505": [false, false, false, true, false], "sample_506": [false, false, false, true, false], "sample_507": [false, false, false, false, false], "sample_508": [false, true, false, true, false], "sample_509": [true, false, false, false, false], "sample_510": [false, true, true, false, true], "sample_511": [false, true, false, false, false], "sample_512": [false, true, false, true, true], "sample_513": [false, false, true, true, false], "sample_514": [false, false, false, false, false], "sample_515": [false, false, false, false, false], "sample_516": [false, false, false, false, false], "sample_517": [false, false, false, false, false], "sample_518": [false, false, false, false, true], "sample_519": [false, false, false, false, false], "sample_520": [false, false, false, false, false], "sample_521": [false, false, false, false, false], "sample_522": [false, false, false, false, false], "sample_523": [false, true, false, true, false], "sample_524": [false, false, false, false, false], "sample_525": [false, false, false, false, false], "sample_526": [false, true, false, true, false], "sample_527": [false, false, false, false, true], "sample_528": [false, false, true, true, true], "sample_529": [false, false, true, false, false], "sample_530": [false, false, false, false, false], "sample_531": [false, false, false, false, false], "sample_532": [false, false, false, false, false], "sample_533": [false, false, false, false, false], "sample_534": [false, false, false, false, false], "sample_535": [false, false, false, false, false], "sample_536": [false, true, false, false, false], "sample_537": [false, false, false, false, false], "sample_538": [true, false, true, true, false], "sample_539": [false, false, false, false, false], "sample_540": [false, false, false, false, false], "sample_541": [true, false, false, false, false], "sample_542": [false, false, false, false, false], "sample_543": [false, false, false, false, false], "sample_544": [false, false, false, false, false], "sample_545": [false, false, false, false, false], "sample_546": [false, false, false, true, false], "sample_547": [false, false, false, false, false], "sample_548": [false, false, false, false, false], "sample_549": [false, false, false, false, false], "sample_550": [false, false, false, false, false], "sample_551": [false, false, false, false, false], "sample_552": [false, false, false, false, false], "sample_553": [false, true, false, false, false], "sample_554": [false, false, false, false, false], "sample_555": [false, false, false, true, false], "sample_556": [false, false, false, false, false], "sample_557": [false, false, false, false, false], "sample_558": [false, false, false, false, false], "sample_559": [false, true, false, false, true], "sample_560": [false, false, false, true, true], "sample_561": [false, false, false, false, false], "sample_562": [false, false, false, false, true], "sample_563": [false, false, false, false, false], "sample_564": [false, false, false, false, false], "sample_565": [false, false, false, true, false], "sample_566": [false, false, false, false, false], "sample_567": [false, false, false, false, false], "sample_568": [false, false, false, false, false], "sample_569": [false, false, false, false, false], "sample_570": [false, false, false, false, false], "sample_571": [false, false, true, false, false], "sample_572": [false, false, false, false, false], "sample_573": [false, false, false, false, false], "sample_574": [false, false, false, false, false], "sample_575": [false, false, false, false, false], "sample_576": [false, false, true, false, false], "sample_577": [false, false, false, false, false], "sample_578": [false, false, false, false, false], "sample_579": [false, false, false, false, false], "sample_580": [true, true, true, false, true], "sample_581": [false, false, false, true, true], "sample_582": [false, false, true, false, true], "sample_583": [false, false, false, false, false], "sample_584": [false, false, false, false, false], "sample_585": [false, true, true, false, false], "sample_586": [true, true, true, false, false], "sample_587": [false, false, false, false, false], "sample_588": [false, false, false, false, false], "sample_589": [false, false, false, true, false], "sample_590": [false, false, false, false, false], "sample_591": [false, false, false, false, false], "sample_592": [false, false, false, false, false], "sample_593": [false, false, true, false, false], "sample_594": [false, false, false, false, false], "sample_595": [true, false, false, false, false], "sample_596": [false, true, false, true, false], "sample_597": [false, false, false, false, false], "sample_598": [false, false, false, false, false], "sample_599": [false, false, false, false, false], "sample_600": [false, false, true, false, false], "sample_601": [false, false, false, false, false], "sample_602": [true, true, false, true, false], "sample_603": [true, false, false, false, false], "sample_604": [false, false, false, false, false], "sample_605": [false, false, false, false, false], "sample_606": [false, false, false, false, false], "sample_607": [true, true, true, true, true], "sample_608": [false, false, false, false, false], "sample_609": [false, false, false, true, false], "sample_610": [false, false, false, false, false], "sample_611": [false, false, false, false, false], "sample_612": [false, false, false, false, false], "sample_613": [false, false, false, true, false], "sample_614": [false, false, false, false, false], "sample_615": [false, true, true, false, true], "sample_616": [false, true, false, false, false], "sample_617": [false, false, false, false, true], "sample_618": [true, true, false, true, false], "sample_619": [false, false, false, false, false], "sample_620": [true, true, false, false, true], "sample_621": [true, false, true, false, false], "sample_622": [false, false, false, false, true], "sample_623": [true, true, false, false, true], "sample_624": [false, false, false, false, false], "sample_625": [false, false, true, false, true], "sample_626": [false, false, false, true, false], "sample_627": [false, false, false, true, true], "sample_628": [false, true, false, false, true], "sample_629": [false, false, false, false, false], "sample_630": [false, false, false, false, false], "sample_631": [false, false, false, false, false], "sample_632": [false, false, true, true, true], "sample_633": [false, false, false, false, false], "sample_634": [true, false, false, false, false], "sample_635": [false, false, false, false, false], "sample_636": [false, false, false, false, false], "sample_637": [false, false, false, false, false], "sample_638": [false, false, false, true, false], "sample_639": [false, false, false, false, false], "sample_640": [false, false, false, false, false], "sample_641": [false, false, false, false, false], "sample_642": [false, false, false, false, false], "sample_643": [false, false, false, false, false], "sample_644": [false, false, false, false, false], "sample_645": [false, false, false, false, true], "sample_646": [false, false, false, false, false], "sample_647": [false, false, false, true, false], "sample_648": [false, false, false, false, false], "sample_649": [false, false, false, false, false], "sample_650": [false, false, false, false, false], "sample_651": [false, false, false, false, false], "sample_652": [false, false, false, false, false], "sample_653": [false, false, false, false, false], "sample_654": [false, false, false, false, false], "sample_655": [false, false, false, false, false], "sample_656": [false, false, false, false, false], "sample_657": [false, false, false, false, false], "sample_658": [false, false, false, false, false], "sample_659": [false, false, false, false, false], "sample_660": [false, false, false, false, false], "sample_661": [false, false, false, false, false], "sample_662": [false, false, false, false, false], "sample_663": [false, false, false, false, false], "sample_664": [true, true, true, true, true], "sample_665": [false, false, false, false, false], "sample_666": [false, false, false, false, false], "sample_667": [false, false, false, false, false], "sample_668": [false, true, true, true, true], "sample_669": [false, false, false, false, false], "sample_670": [true, true, false, true, false], "sample_671": [false, false, false, false, false], "sample_672": [false, false, false, false, false], "sample_673": [false, false, false, false, false], "sample_674": [false, false, false, false, false], "sample_675": [false, false, false, false, false], "sample_676": [false, false, false, false, false], "sample_677": [true, false, false, true, false], "sample_678": [false, false, false, false, false], "sample_679": [false, false, false, false, false], "sample_680": [false, false, false, false, false], "sample_681": [false, false, false, false, false], "sample_682": [false, false, false, false, false], "sample_683": [false, false, false, false, false], "sample_684": [false, false, false, false, false], "sample_685": [false, false, false, false, false], "sample_686": [false, false, false, false, false], "sample_687": [true, false, false, false, true], "sample_688": [false, false, false, false, false], "sample_689": [false, false, false, false, false], "sample_690": [false, false, false, false, false], "sample_691": [false, false, false, false, false], "sample_692": [false, false, true, false, false], "sample_693": [false, false, false, false, false], "sample_694": [false, false, false, false, false], "sample_695": [false, true, false, false, false], "sample_696": [false, false, false, false, false], "sample_697": [true, false, false, false, true], "sample_698": [false, false, false, false, false], "sample_699": [false, false, false, false, false], "sample_700": [false, false, false, false, false], "sample_701": [false, false, false, false, false], "sample_702": [false, false, false, true, false], "sample_703": [true, false, true, true, false], "sample_704": [false, false, false, false, false], "sample_705": [false, true, false, true, false], "sample_706": [false, true, false, true, false], "sample_707": [false, false, false, false, false], "sample_708": [false, false, false, false, false], "sample_709": [false, false, true, false, true], "sample_710": [false, false, false, false, false], "sample_711": [true, false, false, false, false], "sample_712": [true, false, false, false, false], "sample_713": [true, false, true, false, false], "sample_714": [false, false, false, true, true], "sample_715": [false, false, true, false, false], "sample_716": [true, true, true, true, false], "sample_717": [true, true, true, true, true], "sample_718": [false, false, false, false, false], "sample_719": [false, false, true, false, true], "sample_720": [false, true, false, true, false], "sample_721": [false, false, false, false, true], "sample_722": [false, false, false, false, false], "sample_723": [false, false, true, false, true], "sample_724": [false, false, false, false, false], "sample_725": [false, false, false, false, true], "sample_726": [false, true, false, true, false], "sample_727": [true, true, true, false, true], "sample_728": [false, true, false, false, false], "sample_729": [false, true, true, false, false], "sample_730": [false, true, false, false, false], "sample_731": [false, false, false, false, false], "sample_732": [false, false, false, false, true], "sample_733": [false, false, false, false, true], "sample_734": [false, true, false, false, true], "sample_735": [true, false, true, false, false], "sample_736": [false, true, false, false, false], "sample_737": [true, true, true, true, false], "sample_738": [true, false, true, true, false], "sample_739": [false, false, false, false, false], "sample_740": [false, false, false, false, false], "sample_741": [false, true, false, true, false], "sample_742": [false, false, false, false, false], "sample_743": [false, false, false, true, false], "sample_744": [false, false, true, false, true], "sample_745": [false, false, false, false, false], "sample_746": [false, true, false, true, false], "sample_747": [false, true, false, true, false], "sample_748": [false, false, false, false, false], "sample_749": [false, false, false, false, false], "sample_750": [false, false, false, true, false], "sample_751": [false, false, false, false, false], "sample_752": [false, false, false, true, false], "sample_753": [false, false, false, false, false], "sample_754": [false, false, false, false, false], "sample_755": [false, true, false, false, true], "sample_756": [false, false, false, true, true], "sample_757": [true, true, true, false, false], "sample_758": [false, false, false, false, false], "sample_759": [false, false, false, false, true], "sample_760": [false, true, false, false, false], "sample_761": [false, false, false, false, true], "sample_762": [true, false, false, true, true], "sample_763": [false, false, false, false, false], "sample_764": [false, false, false, false, false], "sample_765": [false, false, true, false, false], "sample_766": [true, false, true, true, true], "sample_767": [false, false, true, false, true], "sample_768": [false, false, true, true, true], "sample_769": [false, false, false, true, false], "sample_770": [true, false, false, false, false], "sample_771": [false, false, false, true, false], "sample_772": [false, false, false, false, false], "sample_773": [false, true, false, false, false], "sample_774": [false, false, true, false, false], "sample_775": [false, false, false, false, false], "sample_776": [false, false, true, false, false], "sample_777": [true, false, false, false, false], "sample_778": [false, false, false, true, false], "sample_779": [false, false, true, false, false], "sample_780": [true, false, true, false, true], "sample_781": [true, true, true, true, false], "sample_782": [false, false, false, false, true], "sample_783": [false, true, false, true, false], "sample_784": [false, false, false, false, true], "sample_785": [true, false, false, true, false], "sample_786": [false, false, false, false, false], "sample_787": [false, false, true, true, false], "sample_788": [false, false, false, false, false], "sample_789": [true, true, true, true, true], "sample_790": [true, true, true, false, false], "sample_791": [false, false, true, true, true], "sample_792": [true, true, false, true, false], "sample_793": [false, false, false, true, false], "sample_794": [false, false, false, false, false], "sample_795": [false, false, false, false, false], "sample_796": [true, true, false, true, false], "sample_797": [true, true, true, false, true], "sample_798": [false, false, false, false, false], "sample_799": [false, true, false, false, true], "sample_800": [false, false, false, false, false], "sample_801": [false, false, false, false, false], "sample_802": [false, false, false, false, false], "sample_803": [false, false, false, false, false], "sample_804": [false, false, true, false, false], "sample_805": [false, false, false, false, true], "sample_806": [false, false, false, false, false], "sample_807": [false, false, true, false, false], "sample_808": [true, false, false, false, true], "sample_809": [false, false, false, false, false], "sample_810": [false, false, false, true, true], "sample_811": [true, false, true, true, false], "sample_812": [false, false, false, false, true], "sample_813": [false, false, false, false, false], "sample_814": [false, false, false, false, false], "sample_815": [false, false, false, true, true], "sample_816": [true, true, false, true, true], "sample_817": [false, false, false, true, false], "sample_818": [false, false, false, false, false], "sample_819": [true, false, true, true, false], "sample_820": [false, true, true, true, false], "sample_821": [true, true, true, false, true], "sample_822": [false, true, false, false, true], "sample_823": [false, false, true, false, false], "sample_824": [false, true, false, true, true], "sample_825": [false, false, false, false, false], "sample_826": [true, false, false, false, false], "sample_827": [true, true, false, false, false], "sample_828": [false, false, true, false, false], "sample_829": [true, true, true, false, true], "sample_830": [false, true, true, true, false], "sample_831": [false, true, false, false, false], "sample_832": [false, false, false, false, false], "sample_833": [false, true, false, true, false], "sample_834": [false, false, false, true, false], "sample_835": [true, false, false, true, true], "sample_836": [true, false, false, false, false], "sample_837": [true, true, false, true, true], "sample_838": [false, false, false, true, false], "sample_839": [true, false, true, false, true], "sample_840": [true, true, false, false, false], "sample_841": [false, false, false, false, false], "sample_842": [false, false, false, false, false], "sample_843": [false, false, false, false, false], "sample_844": [false, false, false, false, false], "sample_845": [false, true, false, false, true], "sample_846": [false, false, false, true, false], "sample_847": [true, false, false, true, false], "sample_848": [true, false, false, false, false], "sample_849": [true, true, true, true, false], "sample_850": [true, false, true, false, true], "sample_851": [false, true, true, false, false], "sample_852": [true, true, false, false, true], "sample_853": [false, false, false, false, false], "sample_854": [true, true, true, true, false], "sample_855": [false, true, false, true, false], "sample_856": [true, false, true, true, true], "sample_857": [true, false, true, false, false], "sample_858": [false, true, true, true, true], "sample_859": [true, true, false, false, false], "sample_860": [true, false, true, true, true], "sample_861": [false, false, false, true, false], "sample_862": [true, false, false, true, true], "sample_863": [false, false, false, true, false], "sample_864": [true, true, false, true, true], "sample_865": [true, false, false, false, false], "sample_866": [true, true, true, true, true], "sample_867": [false, true, true, false, false], "sample_868": [false, false, false, true, false], "sample_869": [false, false, false, false, false], "sample_870": [false, false, false, false, true], "sample_871": [true, true, true, true, true], "sample_872": [false, false, false, false, false], "sample_873": [true, true, false, true, false], "sample_874": [false, false, false, false, false], "sample_875": [true, false, false, false, false], "sample_876": [false, true, false, true, false], "sample_877": [true, true, false, true, false], "sample_878": [false, false, false, false, false], "sample_879": [false, false, true, false, true], "sample_880": [true, false, true, false, false], "sample_881": [false, false, false, false, false], "sample_882": [true, false, false, true, false], "sample_883": [false, false, false, false, false], "sample_884": [false, false, false, false, false], "sample_885": [false, false, false, false, false], "sample_886": [true, true, false, true, true], "sample_887": [false, false, false, false, false], "sample_888": [true, true, true, true, false], "sample_889": [false, false, false, false, false], "sample_890": [true, true, true, false, true], "sample_891": [false, false, false, false, false], "sample_892": [true, true, false, false, true], "sample_893": [false, false, false, false, false], "sample_894": [false, false, false, false, false], "sample_895": [false, false, false, false, false], "sample_896": [false, false, false, false, false], "sample_897": [false, false, false, false, false], "sample_898": [false, false, false, false, false], "sample_899": [false, false, false, false, false], "sample_900": [false, false, false, false, false], "sample_901": [false, false, false, false, false], "sample_902": [false, false, true, false, false], "sample_903": [false, false, false, true, true], "sample_904": [false, false, false, false, false], "sample_905": [true, false, true, false, false], "sample_906": [false, false, true, false, false], "sample_907": [false, false, false, false, false], "sample_908": [false, false, false, false, false], "sample_909": [false, false, false, false, false], "sample_910": [false, false, false, true, false], "sample_911": [false, false, false, false, false], "sample_912": [false, false, false, false, false], "sample_913": [false, false, false, false, false], "sample_914": [false, false, false, false, false], "sample_915": [false, false, false, false, false], "sample_916": [false, false, false, false, false], "sample_917": [false, false, false, false, false], "sample_918": [false, false, false, false, false], "sample_919": [true, false, false, false, false], "sample_920": [false, false, false, false, false], "sample_921": [false, false, false, false, false], "sample_922": [false, false, false, false, false], "sample_923": [false, false, false, false, false], "sample_924": [false, false, false, false, false], "sample_925": [false, true, false, true, false], "sample_926": [false, false, false, true, false], "sample_927": [false, false, false, false, false], "sample_928": [false, false, false, false, false], "sample_929": [false, false, false, false, false], "sample_930": [false, false, true, false, false], "sample_931": [false, false, false, false, false], "sample_932": [false, false, false, false, false], "sample_933": [false, false, false, false, false], "sample_934": [false, false, false, false, false], "sample_935": [false, false, false, false, false], "sample_936": [false, true, true, false, true], "sample_937": [false, false, false, false, false], "sample_938": [false, false, false, false, false], "sample_939": [false, false, false, false, false], "sample_940": [false, false, false, false, true], "sample_941": [false, false, false, true, false], "sample_942": [false, false, false, false, false], "sample_943": [false, false, false, false, true], "sample_944": [true, false, true, false, false], "sample_945": [false, false, false, false, false], "sample_946": [false, false, false, false, false], "sample_947": [false, false, false, false, false], "sample_948": [false, false, false, false, false], "sample_949": [false, false, false, false, true], "sample_950": [false, false, false, false, false], "sample_951": [false, true, false, false, false], "sample_952": [false, true, false, false, false], "sample_953": [false, false, false, false, false], "sample_954": [false, false, false, false, false], "sample_955": [false, false, false, false, false], "sample_956": [false, false, false, false, false], "sample_957": [false, true, false, false, false], "sample_958": [false, false, false, false, false], "sample_959": [false, false, false, false, false], "sample_960": [false, false, false, false, false], "sample_961": [false, false, false, false, false], "sample_962": [false, false, false, false, false], "sample_963": [false, true, false, true, false], "sample_964": [false, false, false, false, false], "sample_965": [false, false, false, false, true], "sample_966": [false, false, false, false, false], "sample_967": [true, true, false, true, true], "sample_968": [false, false, false, false, false], "sample_969": [false, false, true, false, false], "sample_970": [false, false, false, false, false], "sample_971": [false, true, false, false, true], "sample_972": [false, false, false, false, false], "sample_973": [false, false, false, false, false], "sample_974": [false, true, false, false, false], "sample_975": [true, true, true, true, true], "sample_976": [false, false, false, false, false], "sample_977": [false, true, false, false, false], "sample_978": [false, false, false, false, false], "sample_979": [false, true, false, false, true], "sample_980": [true, false, false, false, false], "sample_981": [false, false, false, false, true], "sample_982": [false, false, true, true, false], "sample_983": [false, true, true, true, false], "sample_984": [true, true, true, false, true], "sample_985": [false, false, false, false, false], "sample_986": [false, false, false, false, false], "sample_987": [false, false, false, true, false], "sample_988": [true, false, false, false, false], "sample_989": [false, false, false, false, true], "sample_990": [false, false, false, false, true], "sample_991": [false, false, true, false, true], "sample_992": [false, false, false, false, false], "sample_993": [false, true, true, true, true], "sample_994": [false, true, false, false, false], "sample_995": [true, false, false, false, false], "sample_996": [false, false, false, false, false], "sample_997": [false, false, false, true, true], "sample_998": [false, false, false, false, false], "sample_999": [false, false, false, false, true], "sample_1000": [false, false, true, false, false], "sample_1001": [true, false, false, false, false], "sample_1002": [false, false, true, false, false], "sample_1003": [true, false, false, false, false], "sample_1004": [false, false, true, false, true], "sample_1005": [false, false, false, false, false], "sample_1006": [false, true, false, false, false], "sample_1007": [false, true, false, false, false], "sample_1008": [true, false, false, false, false], "sample_1009": [false, true, false, true, false], "sample_1010": [false, true, false, false, false], "sample_1011": [true, false, true, false, false], "sample_1012": [true, false, false, false, true], "sample_1013": [true, true, true, false, true], "sample_1014": [true, false, false, true, true], "sample_1015": [true, false, false, true, false], "sample_1016": [true, false, false, true, false], "sample_1017": [true, false, false, true, true], "sample_1018": [false, false, false, false, false], "sample_1019": [false, false, false, false, true], "sample_1020": [true, false, false, false, true], "sample_1021": [true, false, true, false, true], "sample_1022": [true, true, true, false, false], "sample_1023": [true, false, false, false, true], "sample_1024": [false, false, false, false, true], "sample_1025": [true, false, false, false, false], "sample_1026": [false, false, false, true, false], "sample_1027": [true, false, false, true, false], "sample_1028": [false, false, false, false, false], "sample_1029": [false, false, false, false, false], "sample_1030": [false, true, false, false, true], "sample_1031": [true, true, true, false, false], "sample_1032": [false, false, false, false, false], "sample_1033": [false, false, false, false, false], "sample_1034": [true, true, true, true, true], "sample_1035": [false, false, false, false, false], "sample_1036": [false, true, false, true, true], "sample_1037": [false, false, false, false, false], "sample_1038": [false, false, true, true, false], "sample_1039": [true, false, false, false, false], "sample_1040": [false, false, false, true, false], "sample_1041": [false, false, false, false, false], "sample_1042": [false, false, false, true, false], "sample_1043": [true, true, true, true, true], "sample_1044": [false, false, false, false, false], "sample_1045": [false, false, false, false, false], "sample_1046": [false, false, false, false, false], "sample_1047": [false, true, true, false, false], "sample_1048": [false, false, false, false, false], "sample_1049": [false, false, true, false, true], "sample_1050": [true, true, false, false, true], "sample_1051": [false, false, false, false, false], "sample_1052": [false, true, false, false, true], "sample_1053": [false, false, true, false, false], "sample_1054": [false, false, false, true, false], "sample_1055": [false, false, false, false, true], "sample_1056": [true, false, true, false, true], "sample_1057": [false, false, false, false, false], "sample_1058": [false, false, false, false, false], "sample_1059": [false, false, true, true, false], "sample_1060": [false, false, false, false, false], "sample_1061": [true, true, true, false, false], "sample_1062": [false, true, false, false, false], "sample_1063": [true, true, false, false, false], "sample_1064": [false, true, false, true, false], "sample_1065": [false, false, false, false, false], "sample_1066": [false, false, false, false, false], "sample_1067": [true, true, false, false, true], "sample_1068": [false, false, false, false, false], "sample_1069": [false, false, false, false, false], "sample_1070": [false, false, false, false, false], "sample_1071": [false, false, false, true, false], "sample_1072": [true, false, false, false, false], "sample_1073": [false, false, false, true, false], "sample_1074": [false, false, true, true, false], "sample_1075": [true, true, true, true, true], "sample_1076": [false, false, false, false, false], "sample_1077": [false, true, false, false, false], "sample_1078": [false, true, false, true, false], "sample_1079": [false, false, false, false, false], "sample_1080": [true, true, true, false, true], "sample_1081": [false, true, true, false, false], "sample_1082": [false, false, true, true, false], "sample_1083": [true, true, false, false, false], "sample_1084": [true, false, false, false, false], "sample_1085": [false, false, false, false, true], "sample_1086": [true, false, false, false, true], "sample_1087": [false, false, false, false, false], "sample_1088": [false, false, false, true, false], "sample_1089": [false, false, false, false, false], "sample_1090": [false, false, true, false, false], "sample_1091": [true, true, true, false, false], "sample_1092": [true, false, false, false, false], "sample_1093": [true, false, true, false, true], "sample_1094": [true, false, false, true, false], "sample_1095": [false, false, true, false, false], "sample_1096": [false, true, true, false, false], "sample_1097": [false, false, true, false, false], "sample_1098": [false, false, false, false, false], "sample_1099": [false, true, false, false, false], "sample_1100": [false, false, false, true, false], "sample_1101": [false, false, false, true, false], "sample_1102": [false, false, false, true, true], "sample_1103": [false, false, false, false, false], "sample_1104": [false, false, true, false, true], "sample_1105": [false, true, false, false, false], "sample_1106": [false, false, false, false, false], "sample_1107": [false, true, false, false, true], "sample_1108": [true, true, false, false, false], "sample_1109": [false, false, true, false, false], "sample_1110": [false, false, false, false, false], "sample_1111": [true, false, true, false, false], "sample_1112": [false, true, true, true, true], "sample_1113": [false, false, false, false, false], "sample_1114": [true, false, false, true, true], "sample_1115": [false, false, false, false, false], "sample_1116": [true, true, false, true, false], "sample_1117": [false, false, false, false, false], "sample_1118": [false, false, true, false, false], "sample_1119": [false, false, false, false, false], "sample_1120": [false, true, false, false, true], "sample_1121": [true, false, true, true, false], "sample_1122": [false, false, false, false, false], "sample_1123": [true, false, true, false, false], "sample_1124": [false, false, false, false, false], "sample_1125": [false, false, false, false, false], "sample_1126": [false, false, true, true, true], "sample_1127": [false, true, false, false, false], "sample_1128": [true, true, false, true, true], "sample_1129": [false, true, true, false, false], "sample_1130": [true, true, true, false, true], "sample_1131": [true, false, true, false, false], "sample_1132": [false, true, false, true, true], "sample_1133": [true, false, false, true, false], "sample_1134": [false, false, true, false, false], "sample_1135": [false, false, true, false, false], "sample_1136": [false, true, false, true, true], "sample_1137": [false, false, false, false, true], "sample_1138": [false, true, false, false, true], "sample_1139": [true, false, false, true, true], "sample_1140": [true, false, false, false, false], "sample_1141": [false, false, true, false, true], "sample_1142": [true, true, true, true, true], "sample_1143": [false, false, true, false, false], "sample_1144": [true, true, true, true, true], "sample_1145": [true, true, true, true, true], "sample_1146": [false, false, false, true, false], "sample_1147": [true, false, false, false, false], "sample_1148": [false, false, false, true, false], "sample_1149": [false, false, true, true, false], "sample_1150": [false, true, false, true, false], "sample_1151": [false, false, false, false, false], "sample_1152": [true, false, true, true, true], "sample_1153": [false, false, false, false, false], "sample_1154": [false, true, true, false, false], "sample_1155": [false, false, true, true, false], "sample_1156": [false, false, false, false, true], "sample_1157": [false, true, true, true, false], "sample_1158": [false, false, false, false, false], "sample_1159": [false, true, true, false, false], "sample_1160": [false, false, false, false, false], "sample_1161": [false, false, false, true, false], "sample_1162": [false, false, false, false, false], "sample_1163": [false, false, false, false, false], "sample_1164": [false, false, false, false, false], "sample_1165": [false, true, false, false, true], "sample_1166": [true, true, true, false, false], "sample_1167": [false, false, false, false, false], "sample_1168": [false, true, false, true, false], "sample_1169": [false, false, true, false, false], "sample_1170": [false, false, false, true, false], "sample_1171": [false, false, false, true, false], "sample_1172": [false, false, false, false, false], "sample_1173": [false, true, false, false, false], "sample_1174": [false, false, false, false, false], "sample_1175": [false, false, false, false, false], "sample_1176": [false, true, false, true, false], "sample_1177": [false, false, false, false, false], "sample_1178": [false, true, false, true, true], "sample_1179": [false, false, true, true, false], "sample_1180": [true, false, false, true, false], "sample_1181": [false, true, false, false, false], "sample_1182": [false, true, false, true, false], "sample_1183": [false, false, false, false, false], "sample_1184": [false, false, false, true, true], "sample_1185": [false, false, true, false, false], "sample_1186": [false, false, false, false, false], "sample_1187": [false, false, false, false, false], "sample_1188": [false, false, false, false, true], "sample_1189": [true, false, false, false, false], "sample_1190": [false, false, false, false, false], "sample_1191": [true, true, true, false, false], "sample_1192": [false, false, false, false, false], "sample_1193": [true, false, false, true, false], "sample_1194": [false, true, true, false, true], "sample_1195": [false, false, false, false, false], "sample_1196": [true, false, false, true, false], "sample_1197": [false, true, false, false, false], "sample_1198": [false, false, true, false, true], "sample_1199": [false, false, false, false, false], "sample_1200": [false, false, false, false, false], "sample_1201": [false, false, false, false, false], "sample_1202": [false, false, false, false, false], "sample_1203": [false, true, false, false, false], "sample_1204": [false, false, false, false, true], "sample_1205": [false, false, false, false, false], "sample_1206": [false, false, true, false, true], "sample_1207": [false, false, false, true, true], "sample_1208": [false, true, false, false, false], "sample_1209": [true, false, false, false, false]}}