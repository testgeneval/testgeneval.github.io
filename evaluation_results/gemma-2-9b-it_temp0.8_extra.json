{"raw_generations": {"sample_427": ["    def test_no_management_form_errors(self):\n        data = {\n            \"choices-TOTAL_FORMS\": \"2\",\n            \"choices-INITIAL_FORMS\": \"0\",\n            \"choices-MIN_NUM_FORMS\": \"0\",\n            \"choices-0-choice\": \"Zero\",\n            \"choices-0-votes\": \"0\",\n            \"choices-1-choice\": \"One\",\n            \"choices-1-votes\": \"1\",\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n        self.assertEqual(formset.errors, {})\n", "    def test_custom_template_name_renders_correctly(self):\n        class CustomForm(Form):\n            pass\n\n        custom_formset = formset_factory(\n            CustomForm,\n            template_name=\"custom/formset/template.html\",\n            renderer=DjangoTemplates(),\n        )\n        formset = custom_formset()\n        self.assertTrue(\n            \"custom/formset/template.html\" in str(formset),\n            \"The formset should be using the custom template name\",\n        )\n\n", "    def test_extra_forms_are_valid_when_full_clean_is_called(self):\n        data = {\n            \"choices-TOTAL_FORMS\": \"3\",\n            \"choices-INITIAL_FORMS\": \"0\",\n            \"choices-MIN_NUM_FORMS\": \"0\",\n            \"choices-0-choice\": \"Zero\",\n            \"choices-0-votes\": \"0\",\n            \"choices-1-choice\": \"One\",\n            \"choices-1-votes\": \"1\",\n            \"choices-2-choice\": \"Two\",\n            \"choices-2-votes\": \"2\",\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n        formset.full_clean()  # validate all forms, including extra forms\n        self.assertTrue(formset.is_valid())\n\n", "    def test_extra_forms_with_initial_data(self):\n        data = {\n            \"choices-TOTAL_FORMS\": \"3\",\n            \"choices-INITIAL_FORMS\": \"0\",\n            \"choices-MIN_NUM_FORMS\": \"0\",\n            \"choices-0-choice\": \"Zero\",\n            \"choices-0-votes\": \"0\",\n            \"choices-1-choice\": \"One\",\n            \"choices-1-votes\": \"1\",\n            \"choices-2-choice\": \"Two\",\n            \"choices-2-votes\": \"2\",\n        }\n        ChoiceFormSet = formset_factory(Choice, extra=1)\n        formset = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n        self.assertEqual(len(formset.forms), 3)\n        self.assertTrue(formset.is_valid())\n        self.assertEqual(formset.cleaned_data, [{'choice': 'Zero', 'votes': 0}, {'choice': 'One', 'votes': 1}, {'choice': 'Two', 'votes': 2}])\n\n", "    def test_can_render_formset_with_custom_template(self):\n        class CustomRenderer(TemplatesSetting):\n            formset_template_name = \"a/custom/formset/template.html\"\n\n        ChoiceFormSet = formset_factory(Choice, renderer=CustomRenderer)\n        formset = ChoiceFormSet()\n        with self.subTest(expected_repr=expected_repr):\n            self.assertEqual(\n                str(formset),\n                open(\"a/custom/formset/template.html\").read(),\n            )\n"], "sample_900": ["def test_validation_fraction():\n    X = [[3, 2], [1, 6], [5, 6], [-2, -4]]\n    y = [1, 1, 1, 0]\n\n    # Test that validation_fraction works for different split ratios\n    for fraction in [0.1, 0.2, 0.3]:\n        mlp = MLPClassifier(validation_fraction=fraction, solver='lbfgs',\n                            random_state=1)\n        mlp.fit(X, y)\n        assert mlp.validation_scores_.size == int(fraction * len(y))\n        assert mlp.n_iter_ < mlp.max_iter\n\n\n\n", "def test_max_fun():\n    # Test that `max_fun` is working as expected.\n    X = [[3, 2], [1, 6], [5, 6], [-2, -4]]\n    y = [1, 1, 1, 0]\n    mlp = MLPClassifier(solver='lbfgs', max_fun=10, random_state=1)\n    with ignore_warnings(category=ConvergenceWarning):\n        mlp.fit(X, y)\n    assert mlp.n_iter_ <= mlp.max_fun\n    assert mlp.n_iter_ == 1\n\n\n", "def test_validation_fraction():\n    # Test that validation_fraction works with different fractions and dataset sizes\n    for fraction in [0.1, 0.2, 0.5]:\n        for n_samples in [50, 100, 500]:\n            X = np.random.rand(n_samples, 10)\n            y = np.random.randint(0, 2, size=n_samples)\n\n            mlp = MLPClassifier(solver='lbfgs', hidden_layer_sizes=50,\n                                 validation_fraction=fraction, max_iter=100)\n            mlp.fit(X, y)\n\n            assert 0 <= mlp.validation_scores_.shape[0] <= n_samples\n            assert mlp.validation_scores_.shape[0] == int(n_samples * fraction)\n\n", "def test_validation_split():\n    # Test that validation_split works as expected\n    n_samples = 100\n    X = np.random.rand(n_samples, 10)\n    y = np.random.randint(0, 2, size=n_samples)\n\n    for split_ratio in [0.1, 0.2, 0.3, 0.5]:\n        mlp = MLPClassifier(validation_split=split_ratio, random_state=1)\n        with ignore_warnings(category=ConvergenceWarning):\n            mlp.fit(X, y)\n\n\n        # Ensure the split was correct\n        assert mlp.n_iter_ > 0\n        assert mlp.validation_scores_ is not None\n        assert mlp.best_validation_score_ is not None\n", "def test_validation_set_ratio():\n    # Test that validation set ratio parameter is working as expected.\n    X = X_digits_multi[:100]\n    y = y_digits_multi[:100]\n    for ratio in [0.1, 0.2, 0.3, 0.4, 0.5]:\n        mlp = MLPClassifier(validation_fraction=ratio, random_state=1)\n        mlp.fit(X, y)\n        assert mlp.n_iter_ <= 1000\n        assert mlp.validation_scores_.shape[0] == mlp.n_iter_\n\n\n"], "sample_708": ["def test_getstatement_with_docstring_in_body() -> None:\n    source = \"\"\"\\\n        \"\"\"This is a docstring.\"\"\"\n        x = 1\n    \"\"\"\n\n    source = getstatement(1, source)\n    assert str(source) == \"        x = 1\"\n", "    def test_getstartingblock_multiline_with_comments() -> None:\n        class A:\n                frame = sys._getframe(1)\n                self.source = Frame(frame).statement\n\n        # fmt: off\n        x = A('x',\n          'y\\n',\n          'z',\n          '# comment'\n          )\n        # fmt: on\n        values = [i for i in x.source.lines if i.strip()]\n        assert len(values) == 4\n", "def test_issue92() -> None:\n    source = \"\"\"\\", "def test_indentation_errors() -> None:\n    with pytest.raises(SyntaxError) as excinfo:\n        Source(\"    if True:\\n    pass\\n    else:\\n     x = 1\")\n    assert \"expected an indented block\" in str(excinfo.value)\n", "def test_issue64() -> None:\n    source = \"\"\"\\"], "sample_1039": ["def test_print_sets():\n    S = Set(Symbol('x'))\n    assert mpp.doprint(S) == '<mrow><mi>&#x203A;</mi><mi>x</mi></mrow>'\n    assert mp.doprint(S) == '<set><ci>x</ci></set>'\n    S = Set(Symbol('x'), Symbol('y'))\n    assert mpp.doprint(S) == '<mrow><mi>&#x203A;</mi><mi>x</mi><mo>&amp;</mo><mi>y</mi></mrow>'\n    assert mp.doprint(S) == '<set><ci>x</ci><ci>y</ci></set>'\n", "def test_print_matrix_element():\n    A = MatrixSymbol('A', 2, 3)\n    assert mpp.doprint(A[0, 1]) == '<msub><mi>A</mi><mn>0</mn></msub><mn>1</mn>'\n    assert mp.doprint(A[0, 1]) == '<ci>A</ci><msub><mn>0</mn><mn>1</mn></msub>'\n    assert mathml(A[0, 1], printer='presentation', mat_symbol_style=\"bold\" )== '<msub><mi mathvariant=\"bold\">A</mi><mn>0</mn></msub><mn>1</mn>'\n    assert mathml(A[0, 1], mat_symbol_style=\"bold\" )== '<ci>A</ci><msub><mn>0</mn><mn>1</mn></msub>' \n\n", "def test_print_matrix_content():\n    A = Matrix([[1, 2], [3, 4]])\n    assert mp.doprint(A) == '<apply><matrix/><row><entry><cn>1</cn></entry><entry><cn>2</cn></entry></row><row><entry><cn>3</cn></entry><entry><cn>4</cn></entry></row></matrix></apply>'\n\n    B = MatrixSymbol('B', 2, 2)\n    assert mp.doprint(B) == '<apply><matrix/><row><entry><ci>B</ci></entry><entry><ci>B</ci></entry></row><row><entry><ci>B</ci></entry><entry><ci>B</ci></entry></row></matrix></apply>'\n\n\n\n", "def test_print_diff():\n    raises(TypeError, lambda: mathml(diff(x, x), printer='presentation',method=\"garbage\")) \n    assert mpp.doprint(diff(x, x, evaluate=False)) == '<mfrac> \\\n                    <diff/><ci>x</ci></mfrac>'\n", "def test_print_matrix_with_content():\n    A = Matrix([[1, 2], [3, 4]])\n    assert mp.doprint(A) == \"\"\""], "sample_754": ["def test_spca_sparse_components(norm_comp):\n    rng = np.random.RandomState(0)\n    X = rng.randn(12, 10)\n    spca = SparsePCA(n_components=8, random_state=rng,\n                     normalize_components=norm_comp)\n    spca.fit(X)\n    \n    # Ensure components have sparse structures\n    for i in range(spca.components_.shape[0]):\n        assert np.sum(np.abs(spca.components_[i, :]) > 1e-6) < spca.components_.shape[1] \n", "def test_verbose_callback(sparse_pca):\n    rng = np.random.RandomState(0)\n    Y, _, _ = generate_toy_data(3, 10, (8, 8), random_state=rng)\n    n_iter = 50\n    verbose_callback = lambda i: print(f\"Iteration : {i}\")\n    spca = sparse_pca(n_components=3, n_iter=n_iter, verbose=0,\n                      callback=verbose_callback)\n    spca.fit(Y)\n    assert_equal(spca.n_iter_, n_iter)\n\n\n\n", "def test_sparse_pca_with_missing_data(norm_comp):\n    # Test that SparsePCA handles missing data appropriately.\n    rng = np.random.RandomState(0)\n    n_samples, n_features = 10, 10\n    X = np.zeros((n_samples, n_features))\n    X[:, 2] = rng.randn(n_samples)\n    X[3, :] = np.nan\n    spca = SparsePCA(n_components=3, random_state=rng,\n                     normalize_components=norm_comp)\n    spca.fit(X)\n    # The model should still be able to fit and produce components\n    assert_true(not np.all(spca.components_ == 0))\n", "def test_sparse_pca_with_large_sparse_data(norm_comp):\n    rng = np.random.RandomState(0)\n    n_samples = 10000\n    n_features = 10000\n    n_components = 50\n    X = rng.randn(n_samples, n_features)\n    X *= 0.01  # make X sparse\n    spca = SparsePCA(n_components=n_components,\n                     random_state=rng, normalize_components=norm_comp)\n    spca.fit(X)\n    assert_true(np.all(spca.components_ != 0))\n", "def test_sparse_components(norm_comp):\n    rng = np.random.RandomState(0)\n    X = rng.randn(100, 50)\n    spca = SparsePCA(n_components=10, alpha=0.1, random_state=rng,\n                     normalize_components=norm_comp)\n    spca.fit(X)\n    # Test that components are sparse\n    for comp in spca.components_:\n        assert_true(np.sum(np.abs(comp)) > 0)\n        assert_true(np.sum(np.abs(comp)) < np.linalg.norm(comp))\n\n\n\n"], "sample_168": ["    def test_include_stale_apps_false_with_dependent_objects(self):\n        \"\"\"\n        When include_stale_apps is False, stale content types from removed apps\n        are not deleted, even if they have dependent objects.\n        \"\"\"\n        post = Post.objects.create(title='post', content_type=self.content_type)\n        ModelWithNullFKToSite.objects.create(post=post)\n        with captured_stdout() as stdout:\n            call_command('remove_stale_contenttypes',\n                         include_stale_apps=False, verbosity=2)\n        self.assertEqual(Post.objects.count(), 1)\n        self.assertEqual(ContentType.objects.count(), self.before_count + 1)\n\n\n", "    def test_no_stale_content_types(self):\n        \"\"\"\n        No stale content types are deleted when there are none.\n        \"\"\"\n        with captured_stdout() as stdout:\n            call_command('remove_stale_contenttypes', verbosity=2)\n        self.assertNotIn('Deleting stale content type', stdout.getvalue())\n        self.assertEqual(ContentType.objects.count(), self.before_count)\n", "    def test_contenttypes_removed_for_apps_not_in_installed_apps_with_interactive(self):\n        ContentType.objects.create(app_label='empty_models', model='Fake 1')\n        ContentType.objects.create(app_label='no_models', model='Fake 2')\n        with mock.patch('builtins.input', return_value='no'):\n            with captured_stdout() as stdout:\n                call_command('remove_stale_contenttypes', include_stale_apps=True, verbosity=2)\n        self.assertNotIn(\"Deleting stale content type 'empty_models | Fake 1'\", stdout.getvalue())\n        self.assertNotIn(\"Deleting stale content type 'no_models | Fake 2'\", stdout.getvalue())\n        self.assertEqual(ContentType.objects.count(), self.before_count + 2)\n", "    def test_stale_content_types_from_removed_apps(self):\n        # Create some stale content types\n        ContentType.objects.create(app_label='removed_app', model='Fake1')\n        ContentType.objects.create(app_label='removed_app', model='Fake2')\n        \n        # Remove 'removed_app' from INSTALLED_APPS\n        with self.assertNumQueries(1):\n            apps.unregister_app('removed_app')\n\n        # Run the command to delete stale content types\n        with mock.patch('builtins.input', return_value='yes'):\n            with captured_stdout() as stdout:\n                call_command('remove_stale_contenttypes', verbosity=2)\n        self.assertIn(\n            \"Deleting stale content type 'removed_app | Fake1'\",\n            stdout.getvalue(),\n        )\n        self.assertIn(\n            \"Deleting stale content type 'removed_app | Fake2'\",\n            stdout.getvalue(),\n        )\n        self.assertEqual(ContentType.objects.count(), self.before_count)\n\n", "    def test_stale_content_types_with_empty_app_label(self):\n        \"\"\"\n        Tests that content types with empty app_labels are handled correctly.\n        \"\"\"\n        ContentType.objects.create(app_label='', model='Fake')\n        with captured_stdout() as stdout:\n            call_command('remove_stale_contenttypes', verbosity=2)\n        self.assertIn('Deleting stale content type \"\" | Fake', stdout.getvalue())\n"], "sample_92": ["    def test_has_perm(self):\n        self.assertFalse(self.user.has_perm('auth.change_user'))\n        self.assertFalse(self.user.has_perm('auth.add_user'))\n", "    def test_anonymous_user_authentication(self):\n        self.assertTrue(authenticate(username='test', password='test') is None)\n\n\n\n        \n", "    def test_get_user_anonymous(self):\n        self.client.logout()\n        request = HttpRequest()\n        request.session = self.client.session\n        user = get_user(request)\n        self.assertIsNone(user)\n", "    def test_inactive_user_no_login(self):\n        self.assertFalse(self.user.is_active)\n        with self.assertRaises(AuthenticationFailed):\n            authenticate(**self.user_credentials)\n", "    def test_no_backend_settings(self):\n        with self.assertRaises(ImproperlyConfigured):\n            authenticate(**self.user_credentials)\n"], "sample_580": ["def test_variable_type_mixed_types():\n    s = pd.Series(['a', 1, True, 2.5])\n    assert variable_type(s) == \"categorical\" \n    s = pd.Series([1, 2, 3, 'a'])\n    assert variable_type(s) == \"categorical\" \n    s = pd.Series([1, 2, 3, np.nan, 'a'])\n    assert variable_type(s) == \"categorical\"\n\n\n\n", "def test_variable_type_with_mixed_object_data():\n    s = pd.Series([1, \"a\", 2, \"b\", True, 3])\n    assert variable_type(s) == \"categorical\"\n", "def test_variable_type_mixed_dtype():\n    s = pd.Series([1, 2, 3, \"a\", \"b\"])\n    assert variable_type(s) == \"categorical\"\n\n    s = pd.Series([1, 2, 3, np.nan, \"a\"])\n    assert variable_type(s) == \"categorical\"\n", "def test_variable_type_mixed_dtype():\n    s = pd.Series([1, \"a\", True])\n    assert variable_type(s) == \"categorical\"\n\n    s = pd.Series([1, np.nan, True])\n    assert variable_type(s) == \"categorical\"\n\n    s = pd.Series([1, \"a\", pd.NA])\n    assert variable_type(s) == \"categorical\"\n", "def test_variable_type_mixed_object_dtypes():\n    s = pd.Series([1, '2', True])\n    assert variable_type(s) == \"categorical\"\n\n    s = pd.Series([1, np.nan, True])\n    assert variable_type(s) == \"categorical\"\n\n    s = pd.Series([1.0, '2', np.nan])\n    assert variable_type(s) == \"categorical\"\n\n    s = pd.Series([1, '2', True, np.nan])\n    assert variable_type(s) == \"categorical\"\n"], "sample_660": ["compilation error", "def test_record_property_with_unicode(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.fixture\n            record_property(\"bar\", \"\\u0041\\u00D1\\u00ED\\u00D1\")\n            record_property(\"foo\", \"<1\");\n    \"\"\"\n    )\n    result, dom = runandparse(testdir, \"-rw\")\n    node = dom.find_first_by_tag(\"testsuite\")\n    tnode = node.find_first_by_tag(\"testcase\")\n    psnode = tnode.find_first_by_tag(\"properties\")\n    pnodes = psnode.find_by_tag(\"property\")\n    pnodes[0].assert_attr(name=\"bar\", value=\"\\u0041\\u00D1\\u00ED\\u00D1\")\n    pnodes[1].assert_attr(name=\"foo\", value=\"<1\")\n", "def test_logging_failing_tests_disabled_logs_output(testdir):\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        junit_log_passing_tests=False\n        junit_logging=system-out\n    \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n        import sys\n\n            sys.stdout.write('This is stdout')\n            sys.stderr.write('This is stderr')\n            raise ValueError('boom')\n    \"\"\"\n    )\n    result, dom = runandparse(testdir)\n    assert result.ret != 0\n    node = dom.find_first_by_tag(\"testcase\")\n    assert len(node.find_by_tag(\"system-err\")) > 0\n    assert len(node.find_by_tag(\"system-out\")) > 0\n\n", "compilation error", "compilation error"], "sample_452": ["    def test_references_field_by_related_name(self):\n        operation = FieldOperation(\n            \"Model\",\n            \"field\",\n            models.ForeignKey(\"Other\", models.CASCADE, related_name=\"related_name\"),\n        )\n        self.assertIs(\n            operation.references_field(\"Other\", \"related_name\", \"migrations\"), True\n        )\n        self.assertIs(operation.references_field(\"Other\", \"whatever\", \"migrations\"), False)\n\n\n\n", "    def test_references_field_by_related_name(self):\n        operation = FieldOperation(\n            \"Model\",\n            \"field\",\n            models.ForeignKey(\"Other\", models.CASCADE, related_name=\"related_name\"),\n        )\n        self.assertIs(operation.references_field(\"Other\", \"related_name\", \"migrations\"), True)\n        self.assertIs(operation.references_field(\"Other\", \"whatever\", \"migrations\"), False)\n\n", "    def test_references_field_by_through_relation_field(self):\n        operation = FieldOperation(\n            \"Model\",\n            \"field\",\n            models.ManyToManyField(\n                \"Other\", through=\"Through\", through_relation_field=\"through_rel\"\n            ),\n        )\n        self.assertIs(\n            operation.references_field(\"Other\", \"whatever\", \"migrations\"), True\n        )\n        self.assertIs(\n            operation.references_field(\"Through\", \"whatever\", \"migrations\"), False\n        )\n        self.assertIs(\n            operation.references_field(\"Through\", \"through_rel\", \"migrations\"), True\n        )\n\n\n", "    def test_reference_field_with_null_to_field(self):\n        operation = FieldOperation(\n            \"Model\",\n            \"field\",\n            models.ForeignKey(\n                \"Other\", models.CASCADE, to_field=\"field\", null=True\n            ),\n        )\n        self.assertIs(\n            operation.references_field(\"Other\", \"field\", \"migrations\"), True\n        )\n", "    def test_references_field_by_null_through_fields(self):\n        operation = FieldOperation(\n            \"Model\",\n            \"field\",\n            models.ManyToManyField(\n                \"Other\", through=\"Through\", through_fields=(\"first\", \"second\")\n            ),\n        )\n        self.assertIs(\n            operation.references_field(\"Other\", \"whatever\", \"migrations\"), True\n        )\n        self.assertIs(\n            operation.references_field(\"Through\", \"whatever\", \"migrations\"), True\n        )\n        self.assertIs(\n            operation.references_field(\"Through\", \"first\", \"migrations\"), True\n        )\n        self.assertIs(\n            operation.references_field(\"Through\", \"second\", \"migrations\"), True\n        )\n"], "sample_646": ["    def test_dont_crash_on_empty_suite(pytester: Pytester) -> None:\n        pytester.makepyfile(\n            \"\"\"\n            import unittest\n\n            class TestIt(unittest.TestCase):\n                pass\n\n        suite = unittest.TestLoader().loadTestsFromTestCase(TestIt)\n        if not suite.countTestCases():\n            suite.addTest(unittest.TestLoader().loadTestsFromTestCase(MyTestCase))\n            assert suite.countTestCases() == 1\n        assert suite.countTestCases() == 1\n            \"\"\"\n        )\n        reprec = pytester.inline_run()\n        reprec.assertoutcome(passed=1, skipped=0, failed=0)\n", "def test_unittest_mark_for_xfail_works_with_pytest(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        class TestMyClass(unittest.TestCase):\n            @unittest.expectedFailure\n                assert 1 == 2\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines(\n        [\"*::TestMyClass::test_some_test* XFAIL*\"]\n    )\n    assert result.ret == ExitCode.NO_TESTS_COLLECTED\n\n\n\n", "compilation error", "def test_raising_unittest_skip_test_during_execution(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        class TestIt(unittest.TestCase):\n            @unittest.skip('This test should be skipped')\n                pass\n                pass\n\n        class TestIt2(unittest.TestCase):\n                pass\n                pass\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines([\"*test_it*test_it skipped*\"])\n    result.stdout.fnmatch_lines([\"*1 skipped in *\"])\n", "compilation error"], "sample_343": ["    def test_reverse_related_manager_filters(self):\n        with self.settings(INSTALLED_APPS={'contenttypes_tests': 'contenttypes_tests'}):\n            router = routers.DatabaseRouter()\n            question = Question.objects.create(text='Who?')\n            answer1 = Answer.objects.create(question=question)\n            answer2 = Answer.objects.create(question=question)\n\n            # Test filter and prefetch cache\n            with self.assertNumQueries(0):\n                self.assertEqual(question.answer_set.count(), 2)\n\n            with self.assertNumQueries(1):\n                answer_set = question.answer_set\n                self.assertEqual(len(list(answer_set)), 2)\n                self.assertEqual(answer_set.count(), 2)\n\n", "    def test_related_object_manager_creation(self):\n        question = Question.objects.create(text='test')\n        answer = Answer.objects.create(question=question)\n        manager = question.answer_set\n\n        self.assertIsInstance(manager, GenericRelatedObjectManager)\n        self.assertEqual(manager.model, Answer)\n\n        self.assertEqual(manager.get_queryset().count(), 1)\n        self.assertEqual(manager.get_queryset().first().question.id, question.id)\n\n\n", "    def test_related_manager_adds_filters(self):\n        question = Question.objects.create(text='test')\n        answer1 = Answer.objects.create(question=question)\n\n        related_manager = question.answer_set.all()\n\n        self.assertEqual(related_manager.count(), 1)\n        self.assertEqual(related_manager.get().question_id, question.id)\n\n\n", "    def test_check_generic_foreign_key_existence(self):\n        class ChildAnswer(Answer):\n            pass\n\n        class Model(models.Model):\n            post = GenericRelation(ChildAnswer)\n\n        with self.assertRaises(checks.Error) as cm:\n            Model._meta.get_field('post').check()\n        self.assertEqual(\n            str(cm.exception),\n            \"The GenericRelation defines a relation with the model 'contenttypes_tests.ChildAnswer', but that model does not have a GenericForeignKey.\"\n        )\n\n", "    def test_related_object_manager_with_m2m_field(self):\n        post = Post.objects.create(title='Post with answers')\n        answer1 = Answer.objects.create(question=post)\n        answer2 = Answer.objects.create(question=post)\n        with self.assertNumQueries(1):\n            related_answers = post.answer_set.all()\n            self.assertCountEqual(related_answers, [answer1, answer2])\n        with self.assertNumQueries(0):\n            related_answers = post.answer_set.all()\n            self.assertCountEqual(related_answers, [answer1, answer2])\n"], "sample_142": ["    def test_list_filter_on_through_field_with_model_admin(self):\n\n        class BookAdminWithListFilter(admin.ModelAdmin):\n            list_filter = ['authorsbooks__featured']\n            inlines = [BookAuthorInline]\n\n\n        class BookAuthorInline(admin.TabularInline):\n            model = BookAuthor\n\n\n        errors = BookAdminWithListFilter(Book, AdminSite()).check()\n        self.assertEqual(errors, [])\n", "    def test_list_display_works_on_through_field_even_when_apps_not_ready(self):\n        \"\"\"\n        Ensure list_display can access reverse fields even when the app registry\n        is not ready; refs #24146.\n        \"\"\"\n        class BookAdminWithListDisplay(admin.ModelAdmin):\n            list_display = ['title', 'authorsbooks__featured']\n\n        # Temporarily pretending apps are not ready yet. This issue can happen\n        # if the value of 'list_display' refers to a 'through__field'.\n        Book._meta.apps.ready = False\n        try:\n            errors = BookAdminWithListDisplay(Book, AdminSite()).check()\n            self.assertEqual(errors, [])\n        finally:\n            Book._meta.apps.ready = True\n", "    def test_list_filter_works_on_through_field_when_apps_are_ready(self):\n        \"\"\"\n        Regression test for #24146 - Ensure list_filter can access reverse\n        fields even when apps are ready and the through app is not.\n        \"\"\"\n        class BookAdminWithListFilter(admin.ModelAdmin):\n            list_filter = ['authorsbooks__featured']\n\n        # Temporarily pretending the app containing 'authorsbooks' is not ready\n        from django.apps import apps\n        apps.get_app_config('authors').ready = False\n        try:\n            errors = BookAdminWithListFilter(Book, AdminSite()).check()\n            self.assertEqual(errors, [])\n        finally:\n            apps.get_app_config('authors').ready = True\n", "    def test_list_filter_on_generic_m2m_field(self):\n        class InfluenceInline(admin.TabularInline):\n            model = Influence\n            ct_field = 'content_type'\n            fk_field = 'object_id'\n\n        class SongAdmin(admin.ModelAdmin):\n            inlines = [InfluenceInline]\n            list_filter = ['influences__content_type__app_label',\n                           'influences__object_id']\n\n        errors = SongAdmin(Song, AdminSite()).check()\n        self.assertEqual(errors, [])\n\n", "    def test_list_filter_works_even_when_through_model_not_ready(self):\n        \"\"\"\n        Ensure list_filter can access reverse fields even when the\n        through model's app is not ready; refs #24146.\n        \"\"\"\n        class BookAdminWithListFilter(admin.ModelAdmin):\n            list_filter = ['authorsbooks']\n\n        # Temporarily pretending the app containing the through\n        # model is not ready yet.\n        AuthorsBooks._meta.apps.ready = False\n        try:\n            errors = BookAdminWithListFilter(Book, AdminSite()).check()\n            self.assertEqual(errors, [])\n        finally:\n            AuthorsBooks._meta.apps.ready = True\n"], "sample_30": ["def test_get_infos_by_name_missing():\n    vot = parse(\n        io.BytesIO(\n            b\"\"\"\n        <VOTABLE xmlns=\"http://www.ivoa.net/xml/VOTable/v1.3\"\n          xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" version=\"1.4\">\n          <RESOURCE type=\"results\">\n            <INFO name=\"creator-name\" value=\"Cannon, A.\"/>\n            <INFO name=\"wrong-name\"/>\n          </RESOURCE>\n        </VOTABLE>\"\"\"\n        )\n    )\n    infos = vot.get_infos_by_name(\"wrong-name\")\n    assert not infos\n", "compilation error", "def test_get_infos_by_id():\n    vot = parse(\n        io.BytesIO(\n            b\"\"\"\n        <VOTABLE xmlns=\"http://www.ivoa.net/xml/VOTable/v1.3\"\n          xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" version=\"1.4\">\n          <RESOURCE type=\"results\">\n            <INFO id=\"info1\" name=\"creator-name\" value=\"Cannon, A.\"/>\n            <INFO id=\"info2\" name=\"creator-name\" value=\"Fleming, W.\"/>\n          </RESOURCE>\n        </VOTABLE>\"\"\"\n        )\n    )\n    info1 = vot.get_info_by_id(\"info1\")\n    assert info1.name == \"creator-name\"\n    assert info1.value == \"Cannon, A.\"\n    info2 = vot.get_info_by_id(\"info2\")\n    assert info2.name == \"creator-name\"\n    assert info2.value == \"Fleming, W.\"\n", "compilation error", "def test_error_handling_invalid_version():\n    with pytest.raises(VOTableParseError):\n        parse(io.BytesIO(b\"\"\"\n        <VOTABLE \n          xmlns=\"http://www.ivoa.net/xml/VOTable/v1.3\"\n          xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n          version=\"invalid\">\n        </VOTABLE>\n        \"\"\"))\n"], "sample_44": ["    def test_reshape_transpose_T(self, method):\n        for mag in self.mags:\n            res = getattr(mag, method)(2, 3)\n            assert res.shape == (2, 3)\n            assert np.all(res.value ==\n                          getattr(mag._function_view, method)(2, 3).value)\n            assert res.unit == mag.unit\n\n\n", "    def test_reshape_and_transpose(self, method):\n        for mag in self.mags:\n            res = getattr(mag, method)()\n            assert (res.value == getattr(mag._function_view, method)().value).all()\n            assert res.unit == mag.unit\n\n", "    def test_log_unit_creation(self):\n        # Test creating log units directly, with and without a physical unit.\n        for unit_class in (u.LogUnit, u.MagUnit):\n            u1 = unit_class('dB', 1.*u.Jy)\n            assert u1.physical_unit == u.Jy\n            assert u1.function_unit == 'dB'\n            u2 = unit_class('dB')\n            assert u2.physical_unit is None\n            assert u2.function_unit == 'dB'\n            with pytest.raises(u.UnitsError):\n                unit_class('invalid_name')\n            with pytest.raises(u.UnitsError):\n                unit_class('dB', 1.*u.m) \n\n\n", "    def test_logarithmic_operations(self):\n        lq1 = u.Magnitude(np.arange(1., 5.) * u.Jy)\n        lq2 = u.Magnitude(np.arange(1., 5.) * u.Jy)\n        with pytest.raises(u.UnitsError):\n            lq1 / lq2  \n        lq3 = lq1 * (lq2 ** 2)\n        assert lq3.unit == u.Jy**3\n        lq4 = u.Magnitude(np.log(lq1.value))\n        assert lq4.unit == u.mag\n", "    def test_log10(self):\n        lq = u.Magnitude(np.arange(1., 10.)*u.Jy)\n        assert np.allclose(lq.log10().value, np.log10(lq.value))\n        assert lq.log10().unit == u.dimensionless_unscaled\n        lq2 = u.Magnitude(np.arange(1, 10.)*u.m)\n        with pytest.raises(u.UnitsError):\n            lq2.log10()\n"], "sample_1103": ["def test_issue_18507_2():\n    assert Mul(zoo, zoo, evaluate=False) is nan\n\n", "def test_issue_18706():\n    assert (1/0).is_infinity is True\n    assert (1/(-0)).is_negative_infinity is True\n", "def test_issue_18507_2():\n    assert Mul(zoo, 1, evaluate=False) is nan\n\n\n", "def test_issue_18766():\n    from sympy.physics.quantum import tensor_product\n    a, b = symbols('a b', real=True)\n    c = tensor_product(a, b)\n    assert c.is_commutative is False\n    assert c.is_linearly_independent is True\n\n", "def test_issue_18507_continued():\n    assert Mul(zoo, 1, evaluate=False) is nan\n    assert Mul(1, zoo, evaluate=False) is nan\n    assert Mul(zoo, zoo, evaluate=False) is nan\n    assert Mul(zoo, zoo, 0, evaluate=False) is nan\n"], "sample_601": ["def test_cftime_normalize_accessor(cftime_rounding_dataarray, cftime_date_type, use_dask):\n    import dask.array as da\n\n    freq = \"D\"\n    expected = xr.DataArray(\n        [\n            [cftime_date_type(1, 1, 1, 0), cftime_date_type(1, 1, 1, 0)],\n            [cftime_date_type(1, 1, 1, 0), cftime_date_type(1, 1, 2, 0)],\n        ],\n        name=\"normalize\",\n    )\n\n    if use_dask:\n        chunks = {\"dim_0\": 1}\n        # Currently a compute is done to inspect a single value of the array\n        # if it is of object dtype to check if it is a cftime.datetime (if not\n        # we raise an error when using the dt accessor).\n        with raise_if_dask_computes(max_computes=1):\n            result = cftime_rounding_dataarray.chunk(chunks).dt.normalize(freq)\n        expected = expected.chunk(chunks)\n        assert isinstance(result.data, da.Array)\n        assert result.chunks == expected.chunks\n    else:\n        result = cftime_rounding_dataarray.dt.normalize(freq)\n\n    assert_identical(result, expected)\n\n\n\n\n", "def test_cftime_strftime_accessor(cftime_rounding_dataarray, cftime_date_type, use_dask):\n    import dask.array as da\n\n    date_format = \"%Y%m%d%H\"\n    expected = xr.DataArray(\n        [\n            [\n                cftime_date_type(1, 1, 1, 1).strftime(date_format),\n                cftime_date_type(1, 1, 1, 15).strftime(date_format),\n            ],\n            [\n                cftime_date_type(1, 1, 1, 23).strftime(date_format),\n                cftime_date_type(1, 1, 2, 1).strftime(date_format),\n            ],\n        ],\n        name=\"strftime\",\n    )\n\n    if use_dask:\n        chunks = {\"dim_0\": 1}\n        with raise_if_dask_computes(max_computes=1):\n            result = cftime_rounding_dataarray.chunk(chunks).dt.strftime(date_format)\n        expected = expected.chunk(chunks)\n        assert isinstance(result.data, da.Array)\n        assert result.chunks == expected.chunks\n    else:\n        result = cftime_rounding_dataarray.dt.strftime(date_format)\n\n    assert_identical(result, expected)\n", "def test_cftime_strftime_accessor(cftime_date_type, use_dask):\n    import dask.array as da\n\n    date_format = \"%Y%m%d\"\n    expected = cftime_date_type(2000, 1, 1).strftime(date_format)\n    if use_dask:\n        chunks = {\"dim_0\": 1}\n        with raise_if_dask_computes(max_computes=1):\n            result = xr.DataArray([cftime_date_type(2000, 1, 1)]).chunk(chunks).dt.strftime(date_format)\n        assert isinstance(result.data, da.Array)\n        assert result.chunks == chunks\n    else:\n        result = xr.DataArray([cftime_date_type(2000, 1, 1)]).dt.strftime(date_format)\n\n    assert_equal(result, expected) \n", "def test_cftime_normalize_accessor(cftime_rounding_dataarray, cftime_date_type, use_dask):\n    import dask.array as da\n\n    expected = xr.DataArray(\n        [\n            [cftime_date_type(1, 1, 1, 0), cftime_date_type(1, 1, 1, 0)],\n            [cftime_date_type(1, 1, 1, 0), cftime_date_type(1, 1, 1, 0)],\n        ],\n        name=\"normalize\",\n    )\n\n    if use_dask:\n        chunks = {\"dim_0\": 1}\n        # Currently a compute is done to inspect a single value of the array\n        # if it is of object dtype to check if it is a cftime.datetime (if not\n        # we raise an error when using the dt accessor).\n        with raise_if_dask_computes(max_computes=1):\n            result = cftime_rounding_dataarray.chunk(chunks).dt.normalize()\n        expected = expected.chunk(chunks)\n        assert isinstance(result.data, da.Array)\n        assert result.chunks == expected.chunks\n    else:\n        result = cftime_rounding_dataarray.dt.normalize()\n\n    assert_identical(result, expected) \n", "    def test_cftime_timedelta_accessor(cftime_rounding_dataarray, cftime_date_type, use_dask):\n        import dask.array as da\n\n        expected_days = xr.DataArray(\n            [\n                [0.0, 14.0],\n                [22.0, 24.0],\n            ],\n            name=\"days\",\n        )\n        expected_seconds = xr.DataArray(\n            [\n                [0.0, 50400.0],\n                [79200.0, 86400.0],\n            ],\n            name=\"seconds\",\n        )\n        \n        if use_dask:\n            chunks = {\"dim_0\": 1}\n            # Currently a compute is done to inspect a single value of the array\n            # if it is of object dtype to check if it is a cftime.datetime (if not\n            # we raise an error when using the dt accessor).\n            with raise_if_dask_computes(max_computes=1):\n                result_days = cftime_rounding_dataarray.chunk(chunks).dt.days\n                result_seconds = cftime_rounding_dataarray.chunk(chunks).dt.seconds\n        else:\n            result_days = cftime_rounding_dataarray.dt.days\n            result_seconds = cftime_rounding_dataarray.dt.seconds\n        assert isinstance(result_days.data, da.Array)\n        assert result_days.chunks == expected_days.chunks\n        assert_identical(result_days, expected_days)\n        assert isinstance(result_seconds.data, da.Array)\n        assert result_seconds.chunks == expected_seconds.chunks\n        assert_identical(result_seconds, expected_seconds) \n\n"], "sample_818": ["def test_spectral_clustering_with_n_neighbors():\n    # Test that spectral_clustering works with different n_neighbors values.\n    X, y = make_blobs(n_samples=20, random_state=0,\n                      centers=[[1, 1], [-1, -1]], cluster_std=0.01)\n    sp = SpectralClustering(n_clusters=2, affinity='nearest_neighbors',\n                            random_state=0, n_neighbors=10)\n    labels = sp.fit(X).labels_\n    assert adjusted_rand_score(y, labels) == 1\n\n    sp = SpectralClustering(n_clusters=2, affinity='nearest_neighbors',\n                            random_state=0, n_neighbors=5)\n    labels = sp.fit(X).labels_\n    assert adjusted_rand_score(y, labels) == 1\n\n", "def test_spectral_clustering_with_precomputed_affinity():\n    X, y_true = make_blobs(n_samples=50, centers=3, cluster_std=0.6, random_state=0)\n    affinity_matrix = pairwise_distances(X, metric='precomputed')\n    aff_matrix_sparse = sparse.csr_matrix(affinity_matrix)\n\n    sc = SpectralClustering(n_clusters=3,\n                            affinity='precomputed',\n                            random_state=0,\n                            eigen_solver='arpack')\n\n    # Test with a dense affinity matrix\n    sc.fit(affinity_matrix)\n    labels_dense = sc.labels_\n\n    # Test with a sparse affinity matrix\n    sc.fit(aff_matrix_sparse)\n    labels_sparse = sc.labels_\n    assert_array_equal(labels_dense, labels_sparse)\n", "def test_spectral_clustering_affinity_matrix():\n    X, y = make_samples(n_samples=100, random_state=0)\n\n    # Create an affinity matrix using pairwise_kernels\n    affinity_matrix = pairwise_kernels(X, metric='rbf', gamma=0.1)\n\n    # Fit SpectralClustering twice with different inputs: the affinity\n    # matrix directly, and the raw data\n    sp1 = SpectralClustering(n_clusters=2, affinity='precomputed',\n                            random_state=0).fit(affinity_matrix)\n    sp2 = SpectralClustering(n_clusters=2, affinity='rbf', gamma=0.1,\n                            random_state=0).fit(X)\n\n    assert adjusted_rand_score(sp1.labels_, sp2.labels_) == 1\n", "def test_spectral_clustering_with_kernel_params(affinity):\n    # Test that kernel parameters are passed correctly to the kernel\n    # function when affinity is set to 'rbf' or 'nearest_neighbors'\n\n    # Generate random data\n    X, y = make_blobs(n_samples=50, random_state=0,\n                      centers=[[1, 1], [-1, -1]], cluster_std=0.5)\n    \n    # Define kernel parameters\n    if affinity == 'rbf':\n        gamma = 0.5\n        degree = 3\n        coef0 = 1\n    else:\n        gamma = None\n        degree = None\n        coef0 = None\n\n    params = {'gamma': gamma, 'degree': degree, 'coef0': coef0}\n    \n    # Fit SpectralClustering with different kernel parameters\n    sp = SpectralClustering(n_clusters=2, affinity=affinity,\n                            kernel_params=params, random_state=0)\n    labels = sp.fit(X).labels_\n\n    # Check if the kernel function was called correctly\n    if affinity == 'rbf':\n        assert sp.affinity_matrix_.shape == (50, 50)\n        assert np.allclose(sp.affinity_matrix_ , rbf_kernel(X, **params))\n    else:\n        assert sp.affinity_matrix_.shape == (50, 50)\n        \n\n\n\n", "def test_affinity_from_data():\n    \"\"\"Test affinity matrix construction from data.\"\"\"\n    X = np.array([[1, 1], [2, 1], [1, 0], [4, 7], [3, 5], [3, 6]])\n    X_sparse = sparse.csr_matrix(X)\n\n    # Use default 'rbf' affinity with 'fit'\n    sc = SpectralClustering(n_clusters=2, random_state=0)\n    sc.fit(X)\n    aff = sc.affinity_matrix_\n    assert aff.shape == (X.shape[0], X.shape[0])\n\n    # Same as above but with sparse input\n    sc_sparse = SpectralClustering(n_clusters=2, random_state=0)\n    sc_sparse.fit(X_sparse)\n    aff_sparse = sc_sparse.affinity_matrix_\n    assert aff_sparse.shape == (X_sparse.shape[0], X_sparse.shape[0])\n    assert_array_equal(aff, aff_sparse.toarray())\n\n    # Ensure that affinity matrix is computed correctly for all\n    # kernel types, even when using callable functions\n    for kernel in ['rbf', 'poly', 'sigmoid', 'precomputed']:\n        sc = SpectralClustering(n_clusters=2, affinity=kernel,\n                random_state=0)\n        sc.fit(X)\n        if kernel == 'precomputed':\n            assert isinstance(sc.affinity_matrix_, np.ndarray)\n        else:\n            assert isinstance(sc.affinity_matrix_, sparse.csr_matrix)\n"], "sample_105": ["    def test_template_params_new_method(self):\n        \"\"\"A template view can be customized to return extra context using a new method.\"\"\"\n        response = self.client.get('/template/custom_new/bar1/bar2/')\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.context['foo1'], 'bar1')\n        self.assertEqual(response.context['foo2'], 'bar2')\n        self.assertEqual(response.context['key'], 'value')\n        self.assertIsInstance(response.context['view'], View)\n\n\n\n", "    def test_redirect_view_named_url_pattern_with_args(self):\n        \"\"\"\n        Test named url pattern with args.\n        \"\"\"\n        response = RedirectView.as_view(pattern_name='song_detail')(self.rf.get('/foo/1/'), song_pk=1)\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response['Location'], '/detail/song/1/')\n\n\n\n", "    def test_redirect_using_kwargs(self):\n        \"\"\"\n        RedirectView can use kwargs to construct the redirect URL\n        \"\"\"\n        response = RedirectView.as_view(\n            url='/bar/{pk}/',\n        )(self.rf.get('/foo/123/'))\n\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(\n            response.url, '/bar/123/'\n        )\n\n\n\n", "    def test_redirect_with_kwargs(self):\n        \"\"\"\n        Redirects can include URL kwargs when specified in the view\n        \"\"\"\n        response = RedirectView.as_view(url='/bar/%(object_id)d/', permanent=False)(self.rf.get('/foo/42/'), object_id=42)\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response.url, '/bar/42/')\n", "    def test_template_params_with_custom_context(self):\n        \"\"\"\n        A template view can handle overriding context with custom_context_to_render\n        \"\"\"\n        response = self.client.get('/template/custom_context/')\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.context['message'], 'Custom context message')\n        self.assertIsInstance(response.context['view'], View)\n"], "sample_1140": ["def test_issue_18393():\n    from sympy import MatrixSymbol, eye\n\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    assert pretty(eye(2)) == 'I'\n    assert upretty(eye(2)) == '\ud835\udd40'\n\n\n", "def test_issue_18479():\n    from sympy.functions.elementary.miscellaneous import coth\n    assert pretty(coth(1/x)) == 'coth(1/x)'\n    assert upretty(coth(1/x)) == 'coth(1/x)'\n", "def test_issue_18378():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    assert pretty(A.T*B) == 'A\u1d40 \u22c5 B'\n    assert upretty(A.T*B) == 'A\u1d40 \u22c5 B'\n\n", "def test_pretty_tensor_contraction():\n    from sympy.tensor.tensor import tensor_indices, Tensor, Symbol\n    i, j, k = tensor_indices(\"i j k\")\n    A = Tensor(\"A\", [i, j])\n    B = Tensor(\"B\", [j, k])\n    C = Tensor(\"C\", [k, i])\n\n    expr = A * B * C\n    ascii_str = \\", "def test_issue_18523():\n    from sympy.tensor import tensor_indices, tensor_heads\n    L = tensor_indices(\"L\")\n    I = tensor_indices(\"I\")\n\n    a = tensor_heads(\"a\", [L])\n    b = tensor_heads(\"b\", [L])\n\n    expr = a(L) * b(L)\n    assert pretty(expr) == 'a_{L}\u22c5b_{L}'\n    assert upretty(expr) == 'a{L}\u22c5b{L}'\n\n\n\n"], "sample_661": ["compilation error", "compilation error", "compilation error", "compilation error", "def test_legacy_mode_default_properties(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture\n            record_xml_attribute(\"fixed\", \"1\")\n\n            record_xml_attribute(\"changed\", \"2\")\n    \"\"\"\n    )\n    result, dom = runandparse(testdir, \"-rw\")\n    assert \"INTERNALERROR\" not in result.stdout.str()\n    node = dom.find_first_by_tag(\"testsuite\")\n    properties = node.find_first_by_tag(\"properties\")\n    assert properties\n    children = properties.find_by_tag(\"property\")\n    assert len(children) == 2\n    assert children[0].get(\"name\") == \"fixed\" \n    assert children[0].get(\"value\") == \"1\"\n    assert children[1].get(\"name\") == \"changed\" \n    assert children[1].get(\"value\") == \"2\"\n\n"], "sample_194": ["    def test_opclasses_with_condition(self):\n        with self.assertRaisesMessage(\n            ValueError,\n            'UniqueConstraint with conditions cannot have opclasses.',\n        ):\n            models.UniqueConstraint(\n                fields=['name'],\n                name='name_with_condition_and_opclasses',\n                condition=models.Q(color__isnull=True),\n                opclasses=['text_pattern_ops'],\n            )\n\n", "    def test_opclasses_with_condition(self):\n        with self.assertRaisesMessage(\n            ValueError, 'UniqueConstraint with conditions cannot have opclasses.'\n        ):\n            models.UniqueConstraint(\n                fields=['name'],\n                name='name_condition_opclasses',\n                condition=models.Q(color__isnull=True),\n                opclasses=['text_pattern_ops'],\n            )\n", "    def test_check_sql_with_deferred_unique_constraint(self):\n        with atomic(), connection.cursor() as cursor:\n            cursor.execute(\n                \"CREATE TABLE deferred_test (id INT PRIMARY KEY, name VARCHAR(255))\"\n            )\n            connection.close()\n            with atomic():\n                with connection.cursor() as cursor:\n                    cursor.execute(\n                        \"CREATE UNIQUE INDEX name_deferred_uniq ON deferred_test (name)\"\n                        \" deferrable initdeferred\"\n                    )\n                    connection.close()\n                deferrable_obj = DeferredTest.objects.create(name='p1')\n                model_instance = DeferredTest.objects.create(name='p2')\n                deferrable_obj.name = 'p2'\n                deferrable_obj.save()\n\n                constraint_name = connection.ops.quote_name('name_deferred_uniq')\n                cursor.execute('SET CONSTRAINTS %s IMMEDIATE' % constraint_name)\n                with self.assertRaises(IntegrityError):\n                    model_instance.delete()\n\n", "    def test_invalid_opclasses_argument_type(self):\n        msg = 'UniqueConstraint.opclasses must be a list or tuple.'\n        with self.assertRaisesMessage(ValueError, msg):\n            models.UniqueConstraint(\n                name='uniq_opclasses',\n                fields=['field'],\n                opclasses=123,\n            )\n", "    def test_database_constraint_with_opclasses(self):\n        with self.assertRaises(IntegrityError):\n            UniqueConstraintProduct.objects.create(\n                name='p1', color='red', extra_info='value_1'\n            )\n\n"], "sample_820": ["compilation error", "def test_weighted_voting_with_duplicates():\n    \"\"\"Test weighted voting with duplicated class labels.\"\"\"\n\n    # Generate data with duplicated class labels\n    X = np.random.rand(100, 2)\n    y = np.array([0, 0, 1, 1, 2, 2, 1, 1, 0, 0])  # Duplicates\n\n    clf1 = LogisticRegression(random_state=123)\n    clf2 = RandomForestClassifier(random_state=123)\n\n    eclf = VotingClassifier(\n        estimators=[('lr', clf1), ('rf', clf2)],\n        voting='hard',\n        weights=[0.7, 0.3]\n    )\n    eclf.fit(X, y)\n\n    # Check if the prediction is consistent\n    y_pred = eclf.predict(X)\n    assert len(np.unique(y_pred)) == len(np.unique(y))\n\n\n", "def test_nested_grid_search():\n    \"\"\"Check nested grid search with VotingClassifier.\"\"\"\n    from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n\n    clf1 = LogisticRegression(random_state=1)\n    clf2 = RandomForestClassifier(random_state=1)\n    clf3 = GaussianNB()\n    e_clf = VotingClassifier(estimators=[\n        ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n        voting='soft')\n    param_grid = {'lr__C': [1.0, 100.],\n                  'rf__n_estimators': [50, 100],\n                  'voting': ['soft', 'hard']}\n    grid = GridSearchCV(estimator=e_clf, param_grid=param_grid, cv=5)\n    grid.fit(X, y)\n", "def test_get_params():\n    clf1 = LogisticRegression(random_state=123, C=1.0)\n    clf2 = RandomForestClassifier(random_state=123, max_depth=None)\n    eclf = VotingClassifier([('lr', clf1), ('rf', clf2)], voting='soft',\n                             weights=[1, 2])\n    params = eclf.get_params()\n    assert 'lr' in params\n    assert 'rf' in params\n    assert 'voting' in params\n    assert 'weights' in params\n\n    assert params['lr'] == clf1.get_params()\n    assert params['rf'] == clf2.get_params()\n    assert params['voting'] == 'soft'\n    assert params['weights'] == [1, 2]\n\n    new_params = {'lr__C': 10.0, 'rf__max_depth': 5}\n    eclf.set_params(**new_params)\n    new_params_ = eclf.get_params()\n    assert new_params_['lr__C'] == 10.0\n    assert new_params_['rf__max_depth'] == 5\n", "compilation error"], "sample_1043": ["def test_user_functions():\n        return x**2 + y*3\n    known_functions['my_function'] = [(lambda x, y: True), 'myFunction']\n    assert mcode(my_function(x, y)) == \"myFunction[x, y]\"\n", "def test_TrigSubstitutions():\n    from sympy import symbols, sin, cos, tan, cot, asin, acos, atan, acot, \\\n        asinh, acosh, atanh, aacth, acsch,  \\\n        Substitutions\n\n    x = symbols('x')\n    test_subs = {\n        'sin': 'Sin',\n        'cos': 'Cos',\n        'tan': 'Tan',\n        'cot': 'Cot',\n        'asin': 'ArcSin',\n        'acos': 'ArcCos',\n        'atan': 'ArcTan',\n        'acot': 'ArcCot',\n        'asinh': 'ArcSinh',\n        'acosh': 'ArcCosh',\n        'atanh': 'ArcTanh',\n        'aacth': 'ArcActh',\n        'acsch': 'ArcCsch'\n    }\n\n    for k, v in test_subs.items():\n        assert mcode(k(x)) == f\"{v}[x]\"\n\n\n", "def test_user_functions():\n    class MyFunc(Function):\n            return x**3 + 2*x\n    mcode_printer = MCodePrinter(user_functions={'myfunc': [lambda x: True, MyFunc]})\n    assert mcode_printer.doprint(MyFunc(x)) == \"myfunc[x]\"\n", "def test_user_functions():\n    from sympy import Symbol, sin, cos, exp\n\n        return x**0.5\n\n    from sympy import mathematica_code as mcode\n\n    mcode_settings = {'user_functions': {'my_sqrt': my_sqrt}}\n    assert mcode(my_sqrt(x), **mcode_settings) == \"my_sqrt[x]\"\n\n\n", "def test_complex():\n    z = symbols('z', complex=True)\n    assert mcode(z.real) == \"Re[z]\"\n    assert mcode(z.imag) == \"Im[z]\"\n    assert mcode(z.abs) == \"Abs[z]\"\n    assert mcode(z.conjugate) == \"Conjugate[z]\"\n    assert mcode(z**2) == \"z^2\"\n"], "sample_1112": ["compilation error", "def test_digits_large_numbers():\n    assert digits(12345678901234567890, 10) == [10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0]\n    assert digits(12345678901234567890, 10, 20) == [10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0]\n", "compilation error", "compilation error", "compilation error"], "sample_422": ["    def test_prefetch_with_filter(self):\n        authors = Author.objects.filter(name__startswith=\"A\")\n        with self.assertNumQueries(3):\n            books = list(\n                Book.objects.prefetch_related(\n                    Prefetch(\n                        \"first_time_authors\",\n                        authors,\n                        queryset=authors.filter(age__gt=50),\n                    )\n                )\n            )\n        for book in books:\n            with self.subTest(book=book):\n                self.assertEqual(\n                    [a.name for a in book.first_time_authors.all()],\n                    [\"Anne\"],\n                )\n", "    def test_empty_querysets(self):\n        with self.assertNumQueries(1):\n            # Fetch books with empty related sets should only make one query\n            Book.objects.prefetch_related(Prefetch(\"authors\", Author.objects.none()))\n\n\n\n", "    def test_related_manager_cache_with_prefetch_related(self):\n        books = Book.objects.all()\n        \n        with self.assertNumQueries(3): \n            authors = Author.objects.prefetch_related(\n                Prefetch(\"books\", books)\n            )\n        \n        with self.assertNumQueries(0):\n            author = authors.first()\n            self.assertEqual(len(author.books.all()), len(books))\n            \n            # Verify that fetching the related objects from the cache doesn't trigger another query\n            self.assertEqual(len(author.books.all()), len(books))\n\n\n\n", "    def test_prefetch_with_limit_and_slice(self):\n        authors = Author.objects.all()\n        with self.assertNumQueries(3):\n            books = list(\n                Book.objects.prefetch_related(\n                    Prefetch(\n                        \"authors\",\n                        authors,\n                        limit=2,\n                        slice_by=1,\n                    ),\n                )\n            )\n        for book in books:\n            with self.subTest(book=book):\n                self.assertEqual(len(book.authors.all()), 2)\n                self.assertEqual(book.authors.all()[0], authors[1])\n\n\n\n", "    def test_ordering_with_prefetch(self):\n        authors = Author.objects.order_by(\"name\")\n        with self.assertNumQueries(3):\n            books = list(\n                Book.objects.prefetch_related(\n                    Prefetch(\n                        \"first_time_authors\",\n                        authors,\n                    ),\n                )\n            )\n\n        # Ensure the books are ordered by title as expected\n        self.assertEqual(\n            [book.title for book in books],\n            [\"Book one\", \"Book two\", \"Book three\", \"Book four\"],\n        )\n\n\n"], "sample_296": ["    def test_encode_empty_with_messages(self):\n        storage = self.get_storage()\n        encoded_data = storage._encode([])\n        self.assertEqual(encoded_data, b'')\n\n        encoded_data = storage._encode([Message(constants.INFO, 'test')])\n        self.assertNotEqual(encoded_data, b'')\n\n\n\n", "    def test_not_finished_sentinel(self):\n        storage = self.get_storage()\n\n        # Simulate a situation where not all messages can be stored due to\n        # cookie size limitation.\n        messages = [Message(c, f'Message {i}') for i, c in enumerate(range(10))]\n        set_cookie_data(storage, messages[:5], encode_empty=True)\n\n        retrieved_messages = list(storage)\n        self.assertEqual(len(retrieved_messages), 5)\n        self.assertIs(retrieved_messages[-1].message, 'Message 4')\n\n        # Check that the next attempt to fetch messages will retrieve the rest\n        messages = stored_cookie_messages_count(storage, self.get_response())\n        self.assertEqual(messages, 5)\n\n        set_cookie_data(storage, messages[5:])\n        retrieved_messages = list(storage)\n        self.assertEqual(len(retrieved_messages), 10)\n", "    def test_not_finished_flag(self):\n        max_cookie_size = CookieStorage.max_cookie_size\n        storage = self.get_storage()\n        response = self.get_response()\n\n        # Simulate a cookie with a size close to `max_cookie_size`,\n        # leaving room for a few more messages.\n        messages = [\n            Message(constants.INFO, get_random_string(max_cookie_size // 10))\n            for _ in range(CookieStorage.max_cookie_size // 10)\n        ]\n        storage.update(response)\n        messages.extend([\n            Message(constants.INFO, get_random_string(max_cookie_size // 10))\n            for _ in range(CookieStorage.max_cookie_size // 10)\n        ])\n        unstored_messages = storage.update(response, remove_oldest=False)\n\n        # Assert that `not_finished` flag was added.\n        self.assertEqual(stored_cookie_messages_count(storage, response), CookieStorage.max_cookie_size // 10)\n        self.assertEqual(len(unstored_messages), CookieStorage.max_cookie_size // 10)\n\n        # Assert that the next call to `get` will retrieve the remaining messages.\n        set_cookie_data(storage, messages + [storage.not_finished])\n        self.assertEqual(list(storage), messages)\n\n", "    def test_empty_cookie(self):\n        storage = self.storage_class(self.get_request())\n        # Set an empty cookie.\n        set_cookie_data(storage, [])\n        # The message contains what's expected.\n        self.assertEqual(list(storage), [])\n\n", "    def test_not_finished(self):\n        storage = self.get_storage()\n        response = self.get_response()\n        messages = ['message 1', 'message 2']\n        storage.add(constants.INFO, *messages)\n\n        # Send update with data\n        storage.update(response)\n\n        # Retrieve and ensure messages are not finished\n        _, all_retrieved = storage._get()\n        self.assertTrue(all_retrieved)\n\n        response = self.get_response()\n        storage.add(constants.INFO, 'message 3')\n\n        # Now the data is supposed to be incomplete\n\n        # Send update with more data\n        storage.update(response)\n\n        # Retrieve and ensure messages are not finished\n        messages, all_retrieved = storage._get()\n        self.assertEqual(messages, ['message 1', 'message 2'])\n        self.assertFalse(all_retrieved)\n        self.assertEqual(stored_cookie_messages_count(storage, response), 2)\n\n        # Retrieve all messages\n        storage.update(response)\n\n        # Retrieve and ensure all messages are now available\n        messages, all_retrieved = storage._get()\n        self.assertEqual(messages, ['message 1', 'message 2', 'message 3'])\n        self.assertTrue(all_retrieved)\n\n\n"], "sample_788": ["def test_fit_transform_constant_feature(strategy, expected_bin_edges):\n    X = np.array([[1, 1], [1, 1], [1, 1], [1, 1]]).reshape(-1, 2)\n    kbd = KBinsDiscretizer(n_bins=4, strategy=strategy, encode='ordinal')\n    kbd.fit(X)\n    assert_array_almost_equal(kbd.bin_edges_[0], expected_bin_edges)\n    assert_array_almost_equal(kbd.bin_edges_[1], expected_bin_edges)\n\n\n\n", "def test_fit_transform_n_bins_array_with_missing_values(strategy, expected):\n    X = np.array([[1, np.nan, 2],\n                  [1, 1, 2],\n                  [1, np.nan, 2]])\n    est = KBinsDiscretizer(n_bins=[2, 2, 2], strategy=strategy,\n                           encode='ordinal').fit(X)\n    assert_array_equal(expected, est.transform(X))\n\n\n\n", "def test_strategy_invalid_quantile_array():\n    est = KBinsDiscretizer(n_bins=3, strategy='quantile', encode='ordinal')\n    assert_raise_message(ValueError, \"Invalid quantile value detected.\",\n                         est.fit, np.array([100, 200, 300]))  \n", "def test_missing_values():\n    X = np.array([[1, 2, np.nan], [4, np.nan, 6], [np.nan, 8, 9]])\n    kbd = KBinsDiscretizer(n_bins=3, strategy='uniform', encode='ordinal')\n\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", category=UserWarning)\n        kbd.fit(X)\n\n    Xt = kbd.transform(X)\n    assert_array_equal(Xt, np.array([\n        [1, 1, 1],\n        [2, 2, 2],\n        [1, 2, 2]]))\n", "def test_inverse_transform_sparse():\n    kbd = KBinsDiscretizer(n_bins=3, strategy='uniform', encode='onehot')\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    Xt = kbd.fit_transform(X)\n    # make sure sparse matrix\n    assert sp.issparse(Xt)\n    Xinv = kbd.inverse_transform(Xt)\n    assert_array_equal(Xinv, np.array([[1. , 2.], [3. , 4.], [5. , 6.]]))\n"], "sample_704": ["def test_node_location_with_relative_path(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import os\n        import pytest\n\n            pass\n    \"\"\"\n    )\n    items = pytester.getitems()\n    item = items[0]\n    assert item.location[0] != item.reportinfo()[0]\n    assert item.location[0].startswith(os.path.dirname(item.reportinfo()[0]))\n", "def test_getitem_on_empty_iter(pytester: Pytester) -> None:\n    class MockCollector:\n            return iter(())\n\n    with pytest.raises(IndexError):\n        items = MockCollector()\n        items[0]\n", "def test_fspath_with_parent_is_properly_set() -> None:\n    # This test checks if the fspath is properly set when using Node.from_parent(parent=...)\n    # and if the deprecated fspath argument is ignored.\n\n    class MyCollector(nodes.Collector):\n            super().__init__(parent=parent, **kw)\n\n    with pytest.warns(DeprecationWarning, match=\"fspath\"):\n        collector = MyCollector(fspath=\"my_fspath\")\n\n    assert collector.path == Path(\"my_fspath\")\n\n", "def test_repr_failure_py_with_fulltrace(pytester: Pytester) -> None:\n        raise ValueError(\"boom!\")\n\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n            raise ValueError(\"boom!\")\n    \"\"\"\n    )\n    result = pytester.runpytest(\"--tb=full\")\n    result.stdout.fnmatch_lines(\n        [\n            str(pytester.getpyfile()),\n            \"*1 passed*\",\n            \"*1 failed in *\",\n            \"*Traceback (most recent call last):*\",\n        ]\n    )\n\n\n\n", "def test_relative_paths_with_trailing_slash(pytester: Pytester) -> None:\n    p = pytester.makepyfile(\n        \"\"\"\n            pass\n    \"\"\"\n    )\n    pytester.runpytest('-v')\n \n   \n    # Check that relative paths with trailing slashes are handled correctly\n    # and that they don't cause errors:\n    \n"], "sample_74": ["    def test_empty_dbinfo(self):\n        self.assertEqual(\n            self._run_it({}), (\n                ['psql'],\n                {},\n            )\n        )\n", "    def test_empty_database(self):\n        self.assertEqual(\n            self._run_it({\n                'user': 'someuser',\n                'password': 'somepassword',\n                'host': 'somehost',\n                'port': '444',\n            }), (\n                ['psql', '-U', 'someuser', '-h', 'somehost', '-p', '444'],\n                {'PGPASSWORD': 'somepassword'},\n            )\n        )\n", "    def test_empty_database(self):\n        self.assertEqual(self._run_it({}), (['psql'], {}))\n", "    def test_empty_connection_params(self):\n        self.assertEqual(\n            self._run_it({}), (\n                ['psql'],\n                {},\n            )\n        )\n", "    def test_empty_params(self):\n        self.assertEqual(\n            self._run_it({}), (['psql'], {})\n        )\n"], "sample_714": ["compilation error", "def test_brier_score_loss_multiclass():\n    y_true = np.array([0, 1, 2, 2, 0, 1, 2])\n    y_pred = np.array([[0.2, 0.3, 0.5], [0.6, 0.2, 0.2],\n                       [0.1, 0.3, 0.6], [0.3, 0.4, 0.3],\n                       [0.8, 0.1, 0.1], [0.2, 0.5, 0.3],\n                       [0.3, 0.2, 0.5]])\n    true_score = linalg.norm(y_true[:, None] - y_pred) ** 2 / len(y_true)\n\n    assert_almost_equal(brier_score_loss(y_true, y_pred), true_score)\n\n\n\n\n", "compilation error", "compilation error", "compilation error"], "sample_507": ["    def test_unicode_data_with_plot(self):\n        ax = plt.figure().subplots()\n        # Plot unicode characters to check if they are handled correctly\n        ax.plot(['\u4e2d\u6587', 'Fran\u00e7ais', 'Espa\u00f1ol'], [1, 2, 3])\n        axis_test(ax.xaxis, ['\u4e2d\u6587', 'Fran\u00e7ais', 'Espa\u00f1ol'])\n\n\n\n", "    def test_plot_bytes_with_units(self, plotter):\n        ax = plt.figure().subplots()\n        bdata = [b'a', b'b', b'c']\n        counts = np.array([4, 6, 5])\n        plotter(ax, bdata, counts)\n        axis_test(ax.xaxis, bdata)\n", "    def test_plot_nan(self, plotter):\n        ax = plt.figure().subplots()\n        data = ['a', 'b', 'nan', 'd']\n        counts = [1, 2, 0, 1]\n        plotter(ax, data, counts)\n        axis_test(ax.xaxis, data)\n", "    def test_mixed_type_update_exception(self, plotter, xdata):\n        ax = plt.figure().subplots()\n        with pytest.raises(TypeError):\n            plotter(ax, [0, 3], [1, 3])\n            plotter(ax, xdata, [1, 2])\n", "    def test_hist_multiple_categorical_data():\n        fig, ax = plt.subplots()\n        ax.hist(['a', 'b', 'a', 'c', 'ff'], bins=2)\n        np.testing.assert_allclose(ax.get_xticks(), [0, 1])\n"], "sample_431": ["    def test_refresh_with_related_objects(self):\n        article = Article.objects.create(\n            headline=\"Parrot programs in Python\",\n            pub_date=datetime(2005, 7, 28),\n        )\n        related_object = RelatedObject.objects.create(article=article)\n        article.refresh_from_db(fields=['relatedobject'])\n        article.refresh_from_db()\n\n\n\n", "    def test_refresh_invalid_pk(self):\n        a = Article.objects.create(headline=\"New headline\", pub_date=datetime.now())\n        invalid_pk = -1\n        with self.assertRaisesMessage(\n            ValueError, \"Object has no primary key value.\"\n        ):\n            Article.objects.get(pk=invalid_pk).refresh_from_db()\n\n\n", "    def test_refresh_fk_on_delete_cascade(self):\n        article = Article.objects.create(headline=\"Parrot programs in Python\", pub_date=datetime(2005, 7, 28))\n        s1 = SelfRef.objects.create(article=article)\n        with self.assertNumQueries(1):\n            article.delete()\n        with self.assertNumQueries(0):\n            s1.refresh_from_db()\n        self.assertIsNone(s1.article)\n", "    def test_refresh_from_db_on_thread(self):\n        from threading import Thread\n\n        class MyManager(BaseManager):\n                return Article.objects.filter(id__gt=10)\n\n        class MyModel(models.Model):\n            objects = MyManager()\n\n        a = MyModel.objects.create(headline=\"New headline\", pub_date=datetime.now())\n        a.save()\n\n            a.headline = \"Updated headline\"\n            a.save()\n            a.refresh_from_db()\n\n        t = Thread(target=update_and_refresh)\n        t.start()\n        t.join()\n        with self.assertNumQueries(1):\n            self.assertEqual(a.headline, \"Updated headline\")\n", "    def test_refresh_with_related_fields(self):\n        article = Article.objects.create(\n            headline=\"Parrot programs in Python\",\n            pub_date=datetime(2005, 7, 28),\n        )\n        s = SelfRef.objects.create(article=article)\n\n        with self.assertNumQueries(1):\n            s.refresh_from_db(fields=[\"article__headline\"])\n        self.assertEqual(s.article.headline, \"Parrot programs in Python\")\n\n\n\n"], "sample_66": ["    def test_missing_header(self):\n        environ = {\n            'CONTENT_TYPE': 'text/html',\n            'CONTENT_LENGTH': '100',\n            'HTTP_HOST': 'example.com',\n        }\n        headers = HttpHeaders(environ)\n        self.assertFalse('MissingHeader' in headers)\n", "    def test_wsgi_request_headers_setdefault(self):\n        request = WSGIRequest(self.ENVIRON)\n        self.assertEqual(request.headers.setdefault('New-Header', 'default'), 'default')\n        self.assertEqual(request.headers['New-Header'], 'default')\n", "    def test_iter_header_names(self):\n        environ = {\n            'CONTENT_TYPE': 'text/html',\n            'CONTENT_LENGTH': '100',\n            'HTTP_HOST': 'example.com',\n            'HTTP_OTHER_HEADER': 'value',\n        }\n        headers = HttpHeaders(environ)\n        for name in headers:\n            self.assertEqual(HttpHeaders.parse_header_name(name), name)\n\n\n\n", "    def test_missing_header(self):\n        environ = {\n            'CONTENT_TYPE': 'text/html',\n            'CONTENT_LENGTH': '100',\n            'HTTP_HOST': 'example.com',\n        }\n        headers = HttpHeaders(environ)\n        self.assertNotIn('MISSING_HEADER', headers)\n        self.assertEqual(headers.get('MISSING_HEADER'), None)\n\n", "    def test_case_insensitive_header(self):\n        environ = {\n            'Content-Type': 'text/html',\n            'CONTENT_LENGTH': '100',\n            'HTTP_HOST': 'example.com',\n        }\n        headers = HttpHeaders(environ)\n        self.assertEqual(headers['Content-Type'], 'text/html')\n        self.assertEqual(headers['content-type'], 'text/html')\n        self.assertEqual(headers['Content-Length'], '100')\n\n\n\n"], "sample_1204": ["compilation error", "compilation error", "compilation error", "def test_p_groups():\n    G = SymmetricGroup(4).sylow_subgroup(2)\n    assert G.is_p_group(2) == True\n    assert G.is_p_group(3) == False\n    H = DihedralGroup(6).sylow_subgroup(3)\n    assert H.is_p_group(3) == True\n    assert H.is_p_group(2) == False\n    assert G.order() % 2 == 0\n    assert G.order() // G.p_group(2) == 1\n    S = SymmetricGroup(10)\n    assert S.sylow_subgroup(2).order() == 16\n    assert S.sylow_subgroup(5).order() == 25\n    A = AlternatingGroup(10)\n    assert A.sylow_subgroup(2).order() == 8\n    assert A.sylow_subgroup(5).order() == 25\n", "    def test_is_simple():\n        G = SymmetricGroup(3)\n        assert G.is_simple == False\n\n        G = SymmetricGroup(4)\n        assert G.is_simple == False\n\n        G = AlternatingGroup(5)\n        assert G.is_simple == True\n\n        G = AlternatingGroup(6)\n        assert G.is_simple == True\n\n        G = DihedralGroup(4)\n        assert G.is_simple == False\n\n        G = DihedralGroup(6)\n        assert G.is_simple == False\n\n        G = AbelianGroup(3, 4)\n        assert G.is_simple == False"], "sample_961": ["def test_warn_circular_reference(app, status, warning):\n    app.build()\n    assert 'index.rst:7: WARNING: circular reference detected: a -> b -> a' in warning.getvalue()\n", "def test_python_python_use_unqualified_type_names_enabled(app, status, warning):\n    app.build()\n    content = (app.outdir / 'index.html').read_text()\n    assert ('<span class=\"n\"><a class=\"reference internal\" href=\"#foo.Name\" title=\"foo.Name\">'\n            '<span class=\"pre\">Name</span></a></span>' not in content)\n    assert ('<span class=\"n\"><span class=\"pre\">foo.Age</span></span>' not in content)\n    assert ('<p><strong>name</strong> (<a class=\"reference internal\" href=\"#foo.Name\" '\n            'title=\"foo.Name\"><em>foo.Name</em></a>) \u2013 blah blah</p>' not in content)\n    assert ('<p><strong>age</strong> (<em>foo.Age</em>) \u2013 blah blah</p>' not in content)  \n", "compilation error", "def test_unqualified_type_names_with_inheritance(app, status, warning):\n    app.build()\n    content = (app.outdir / 'index.html').read_text()\n    assert ('<p><strong>name</strong> (<a class=\"reference internal\" href=\"#Base.Name\" '\n            'title=\"Base.Name\"><em>Name</em></a>) \u2013 blah blah</p>' in content)\n    assert ('<p><strong>age</strong> (<em>Base.Age</em>) \u2013 blah blah</p>' in content)\n", "def test_python_python_use_unqualified_type_names_with_alias(app, status, warning):\n    app.build()\n    content = (app.outdir / 'index.html').read_text()\n    assert ('<span class=\"n\"><a class=\"reference internal\" href=\"#foo.NamedAlias\" '\n            'title=\"foo.NamedAlias\"><span class=\"pre\">NamedAlias</span></a></span>' in content)\n    assert ('<p><strong>name</strong> (<a class=\"reference internal\" href=\"#foo.NamedAlias\" '\n            'title=\"foo.NamedAlias\"><em>NamedAlias</em></a>) \u2013 blah blah</p>' in content)\n\n\n\n"], "sample_689": ["compilation error", "def test_pytest_warning_captured_is_deprecated(testdir: Testdir) -> None:\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n            pytest.warning.capture_warnings(\n                warnings.WARN,\n                \"Deprecated message\",\n            )  # type: ignore\n\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestDeprecationWarning: The pytest_warning_captured is deprecated and will be removed in a future release.*\",\n            \"*Please use pytest_warning_recorded instead.*\"\n        ]\n    )\n", "def test_pytest_warning_captured_is_deprecated(testdir: Testdir) -> None:\n    \"\"\"\n    Test deprecated pytest_warning_captured plugin hook.\n    \"\"\"\n    testdir.makeconftest(\n        \"\"\"\n        import pytest\n\n            pytest_warnings: list,\n            config: pytest.config.Config,\n            *,\n            exception: Exception = None,\n        ):\n            assert False, \"Warning captured should not be called.\"\n    \"\"\"\n    )\n    with pytest.warns(\n        deprecated.WARNING_CAPTURED_HOOK,\n        match=\"pytest_warning_captured is deprecated\",\n    ):\n        result = testdir.runpytest()\n        result.stdout.fnmatch_lines(\n            [\"Warning captured should not be called.\"]\n        )\n\n", "def test_pytest_warning_captured_is_deprecated(testdir: Testdir) -> None:\n    with pytest.warns(pytest.PytestDeprecationWarning, match=\"pytest_warning_captured\"):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n                pass\n            \"\"\"\n        )\n        result = testdir.runpytest()\n        result.stdout.fnmatch_lines([\"*pytest_warning_captured*deprecated*\"])\n", "def test_pytest_warning_captured_is_deprecated(testdir: Testdir) -> None:\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.warning\n            raise ValueError(\"this is a warning\")\n\n            import warnings\n\n            with warnings.catch_warnings():\n                warnings.simplefilter(\"ignore\")\n                pytest.warns(ValueError, match=\"this is a warning\")\n        \"\"\"\n    )\n    result = testdir.runpytest(\"--strict-markers\")\n    result.stdout.fnmatch_lines([\"*pytest_warning_captured*deprecated*\"])\n"], "sample_1115": ["def test_tensor_indices_manipulation():\n    L = TensorIndexType(\"L\")\n    i, j, k = tensor_indices(\"i j k\", L)\n    A = TensorHead(\"A\", [L, L])\n    assert A(i, j).get_indices() == [i, j]\n    assert A(i, j).get_free_indices() == [i, j]\n    assert A(i).get_indices() == [i]\n    assert A(i).get_free_indices() == [i]\n    assert A(i, j).get_indices() == [i, j]\n    assert A(i, j).contracted_indices() == []\n    assert A(i, -j).get_indices() == [i, -j]\n    assert A(i, -j).get_free_indices() == [i, -j]\n    assert A(i, -j).contracted_indices() == []\n    assert  A(-i, j).get_indices() == [-i, j]\n    assert  A(-i, j).get_free_indices() == [-i, j]\n    assert  A(-i, j).contracted_indices() == []\n\n    assert A(i, j).tensor_index_type() == L\n    assert A(i, j).tensor_index_type() == L\n    assert A(i).tensor_index_type() == L\n\n    assert A(i, j).indices_are_free()\n    assert A(i).indices_are_free()\n\n    assert A(i, j).indices_are_contracted() == False\n    assert A(i, -j).indices_are_contracted() == False\n\n\n\n", "compilation error", "def test_tens_add_with_wrong_indices():\n    L = TensorIndexType(\"L\")\n    A, B = tensor_heads(\"A B\", [L])\n    expr = A(i) + B(j)\n    raises(ValueError, lambda: expr.doit())\n", "def test_metric_tensors():\n    L = TensorIndexType(\"L\")\n    i, j = tensor_indices(\"i j\", L)\n    g = TensorHead(\"g\", [L, L], TensorSymmetry.fully_symmetric(2))\n    g._data = [[1, 0], [0, -1]]\n\n    assert g(i, j) == g(j, i)\n    assert g(i, i) == 1\n    assert g(j, j) == -1\n    assert g(i, j) != g(j, -i)\n\n    h = TensorHead(\"h\", [L, L], TensorSymmetry.fully_antisymmetric(-2))\n    h._data = [[0, 1], [-1, 0]]\n\n    assert h(i, j) == -h(j, i)\n    assert h(i, i) == 0\n    assert h(i, j) != h(j, i)\n\n    # Test metric in contraction\n    expr = g(i, j)*A(j)\n    assert expr.replace(g(i, j), 1).data == A(j)\n    assert expr.replace(g(i, j), -1).data == -A(j)\n\n    # Test antisymmetric tensor in contraction\n    expr = h(i, j)*B(j)\n    assert expr.replace(h(i, j), 1).data == B(j)\n    assert expr.replace(h(i, j), -1).data == -B(j)\n\n\n\n", "def test_tensor_index_substitution():\n    L = TensorIndexType(\"L\")\n    i, j, k = tensor_indices(\"i j k\", L)\n    A = TensorHead(\"A\", [L])\n    B = TensorHead(\"B\", [L, L])\n\n    expr = A(i) + A(j)\n    repl = {A(i): B(j, i)}\n    assert expr.substitute(repl) == B(j, i) + B(j, i)\n    assert expr.substitute(repl, [i]) == B(j, i) + B(j, i)\n    assert expr.substitute(repl, [j]) == B(j, i) + B(j, i)\n\n    expr = A(i)*B(j, k)\n    repl = {A(i): B(i, j)}\n    assert expr.substitute(repl) == B(i, j)*B(j, k)\n    assert expr.substitute(repl, [i]) == B(i, j)*B(j, k)\n\n    expr = B(i, j)*A(k)\n    repl = {A(k): B(k, j)}\n    assert expr.substitute(repl) == B(i, j)*B(k, j)\n    assert expr.substitute(repl, [k]) == B(i, j)*B(k, j)\n\n    expr = A(i)*A(j)\n    repl = {A(i): B(j, i)}\n    raises(TypeError, lambda: expr.substitute(repl))\n"], "sample_359": ["    def test_references_field_by_related_name(self):\n        operation = FieldOperation(\n            'Model', 'field', models.OneToOneField('Other', models.CASCADE, related_name='my_related_name')\n        )\n        self.assertIs(operation.references_field('Other', 'my_related_name', 'migrations'), True)\n        self.assertIs(operation.references_field('Other', 'something_else', 'migrations'), False)\n\n\n\n", "    def test_references_field_by_source(self):\n        operation = FieldOperation('Model', 'field', models.OneToOneField('Other', models.CASCADE, source_fields=['source_field']))\n        self.assertIs(operation.references_field('Other', 'whatever', 'migrations'), True)\n        self.assertIs(operation.references_field('Other', 'source_field', 'migrations'), True)\n        self.assertIs(operation.references_field('Model', 'source_field', 'migrations'), False)\n        self.assertIs(operation.references_field('Missing', 'whatever', 'migrations'), False)\n", "def test_references_field_by_proxy(self):\n    from django.db import models\n\n    class ProxyModel(models.Model):\n        content = models.TextField()\n\n        class Meta:\n            proxy = True\n\n    operation = FieldOperation('Model', 'field', models.ForeignKey('ProxyModel', models.CASCADE))\n    self.assertTrue(operation.references_field('ProxyModel', 'content', 'migrations'))\n    self.assertIs(operation.references_field('ProxyModel', 'foo', 'migrations'), False)\n\n\n\n", "    def test_references_field_by_related_name(self):\n        operation = FieldOperation('Model', 'field', models.ForeignKey('Other', models.CASCADE, related_name='related_to_model'))\n        self.assertIs(operation.references_field('Other', 'related_to_model', 'migrations'), True)\n        self.assertIs(operation.references_field('Other', 'whatever', 'migrations'), False)\n\n", "    def test_foreign_key_relational_lookup(self):\n        # Ensure that referencing a model using a foreign key with\n        # a related manager is handled correctly\n        operation = FieldOperation(\n            'Model', 'field', models.ForeignKey('RelatedManagerModel', models.CASCADE,\n            related_name=\"my_related_manager\")\n        )\n        self.assertIs(operation.references_field('RelatedManagerModel', 'whatever', 'migrations'), True)\n\n"], "sample_988": ["def test_issue_11988():\n    x = symbols('x')\n    y = symbols('y')\n    f = symbols('f')\n    assert Eq(f(x), f(y)).subs({x: 2, y: 3}) == Eq(f(2), f(3))\n\n\n\n", "def test_issue_13400():\n    x = symbols('x', real=True)\n    y = symbols('y', real=True)\n    z = symbols('z', real=True)\n\n    assert (x > y) & (y > z) == (x > z)\n    assert (x < y) & (y < z) == (x < z)\n    assert (x >= y) & (y >= z) == (x >= z)\n    assert (x <= y) & (y <= z) == (x <= z)\n", "def test_issue_14508():\n    x, y = symbols('x y')\n    assert Eq(x, y).has(*[x, y]) is True\n    assert Eq(x, y).has(z) is False\n\n\n", "def test_issue_11488():\n    x = symbols('x', positive=True)\n    y = symbols('y', positive=True)\n    assert (x/y).is_nonzero is S.true\n    assert (y/x).is_nonzero is S.true\n    assert (x/oo).is_zero is S.true\n    assert (oo/x).is_zero is S.false\n\n", "def test_issue_11026():\n    from sympy.simplify import simplify\n    x = Symbol('x')\n    y = Symbol('y')\n    assert simplify(Eq(x / (x + 1), x / (x + 1))) == Eq(x / (x + 1), x / (x + 1))\n    assert simplify(Eq(x + y, x + y)) == Eq(x + y, x + y)\n    assert simplify(Eq(x / (x + 1), None)) == Eq(x / (x + 1), None)\n"], "sample_1046": ["compilation error", "compilation error", "compilation error", "def test_tensor_substitution():\n    L = TensorIndexType(\"L\")\n    i, j, k = tensor_indices(\"i j k\", L)\n    A = tensorhead(\"A\", [L], [[1]])\n    B = tensorhead(\"B\", [L], [[1]])\n    C = tensorhead(\"C\", [L, L], [[1], [1]])\n\n    expr = A(i)*A(-i) + A(i)*B(j)\n\n    repl = {A(i): 2}\n    assert expr.subs(repl) == 4 + 2*B(j)\n\n    repl = {A(i): 2, B(j): 3}\n    assert expr.subs(repl) == 4 + 6\n\n    repl = {C(i, j): [[1, 2], [3, 4]]}\n    assert expr.subs(repl) == expr  # no substitutions made\n\n    repl = {A(i): C(i, i)}\n    assert expr.subs(repl) == C(i, i)*C(-i, -i) + C(i, i)*B(j)\n", "compilation error"], "sample_61": ["    def test_username_validators_help_texts(self):\n        help_texts = validators.get_username_validators_help_texts()\n        self.assertEqual(len(help_texts), 2)\n        self.assertIn('ASCII characters only.', help_texts[0])\n        self.assertIn('letters, numbers, and @/./+/-/_ characters.', help_texts[1])\n", "    def test_unicode_validator_flags(self):\n        v = validators.UnicodeUsernameValidator(flags=re.IGNORECASE)\n        self.assertTrue(v('JOE'))\n        self.assertTrue(v('JoE'))\n\n        v = validators.UnicodeUsernameValidator(flags=0)\n        self.assertFalse(v('JOE'))\n\n\n\n", "    def test_username_validator_error_messages(self):\n        v_unicode = UnicodeUsernameValidator()\n        v_ascii = ASCIIUsernameValidator()\n        invalid_usernames = [\n            \"o'connell\", \"\u0639\u0628\u062f \u0627\u0644\", \"zerowidth\\u200Bspace\", \"nonbreaking\\u00A0space\",\n            \"en\\u2013dash\", 'trailingnewline\\u000A'\n        ]\n        for username in invalid_usernames:\n            with self.subTest(username=username):\n                with self.assertRaises(ValidationError) as cm:\n                    v_unicode(username)\n                self.assertEqual(cm.exception.messages, [v_unicode.message])\n        for username in invalid_usernames:\n            with self.subTest(username=username):\n                with self.assertRaises(ValidationError) as cm:\n                    v_ascii(username)\n                self.assertEqual(cm.exception.messages, [v_ascii.message])\n", "    def test_unicode_validator_flags(self):\n        v = validators.UnicodeUsernameValidator(flags=re.IGNORECASE)\n        self.assertIsNone(v('glenn'))\n        self.assertIsNone(v('GLENN'))\n\n\n", "    def test_unicode_validator_with_custom_error_message(self):\n        v = UnicodeUsernameValidator(message='This username contains invalid characters.')\n        with self.assertRaises(ValidationError) as cm:\n            v(\"o'connell\")\n        self.assertEqual(cm.exception.messages, ['This username contains invalid characters.'])\n"], "sample_392": ["    def test_key_transform_annotation_expression_with_nested_access(self):\n        related_obj = RelatedJSONModel.objects.create(\n            value={\"d\": {\"e\": {\"f\": \"g\"}}},\n            json_model=self.objs[4],\n        )\n        RelatedJSONModel.objects.create(\n            value={\"d\": {\"e\": {\"f\": \"h\"}}},\n            json_model=self.objs[4],\n        )\n        self.assertSequenceEqual(\n            RelatedJSONModel.objects.annotate(\n                key=KeyTransform(\"d\", \"value\"),\n                related_key=F(\"json_model__value__d\"),\n                chain=F(\"key__e__f\"),\n                expr=Cast(\"key\", models.JSONField()),\n            ).filter(chain=\"g\"),\n            [related_obj],\n        )\n", "    def test_key_transform_annotation_expression_with_rawsql(self):\n        related_obj = RelatedJSONModel.objects.create(\n            value={\"d\": [\"f\", \"e\"]},\n            json_model=self.objs[4],\n        )\n        RelatedJSONModel.objects.create(\n            value={\"d\": [\"e\", \"f\"]},\n            json_model=self.objs[4],\n        )\n        self.assertSequenceEqual(\n            RelatedJSONModel.objects.annotate(\n                key=RawSQL(self.raw_sql, ['{\"d\": [\"f\", \"e\"]}]),\n                related_key=F(\"json_model__value__d\"),\n                chain=F(\"key__1\"),\n                expr=Cast(\"key\", models.JSONField()),\n            ).filter(chain=F(\"related_key__0\")),\n            [related_obj],\n        )\n", "    def test_key_transform_with_f_expression(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(\n                value__d__1__f=F(\"value__d__0__0\")\n            ),\n            [self.objs[4]],\n        )\n", "    def test_deep_contains(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(value__d__1__f__contains=\"g\"),\n            [self.objs[4]],\n        )\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(value__d__1__f__icontains=\"G\"),\n            [self.objs[4]],\n        )\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(value__d__1__f__startswith=\"g\"),\n            [self.objs[4]],\n        )\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(value__d__1__f__istartswith=\"G\"),\n            [self.objs[4]],\n        )\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(value__d__1__f__endswith=\"g\"),\n            [self.objs[4]],\n        )\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(value__d__1__f__iendswith=\"G\"),\n            [self.objs[4]],\n        )\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(value__d__1__f__regex=r\"^g.*$\"),\n            [self.objs[4]],\n        )\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(value__d__1__f__iregex=r\"^g.*$\"),\n            [self.objs[4]],\n        )", "    def test_key_exists(self):\n        self.assertIs(\n            NullableJSONModel.objects.filter(value__a__exists=True).exists(), True\n        )\n        self.assertIs(\n            NullableJSONModel.objects.filter(value__a__exists=False).exists(), False\n        )\n        self.assertIs(\n            NullableJSONModel.objects.filter(value__d__exists=True).exists(), True\n        )\n        self.assertIs(\n            NullableJSONModel.objects.filter(value__d__exists=False).exists(), False\n        )\n        self.assertIs(\n            NullableJSONModel.objects.filter(value__zz__exists=True).exists(), False\n        )\n"], "sample_868": ["def test_empty_input(metric_name):\n    metric = SUPERVISED_METRICS[metric_name]\n    with pytest.raises(ValueError, match='Input arrays should not be empty'):\n        metric([], [])\n    with pytest.raises(ValueError, match='Input arrays should not be empty'):\n        metric([1, 2], [])\n\n\n\n", "def test_adjusted_mutual_info_score_bounds(metric_name, y_true, y_pred):\n    score = adjusted_mutual_info_score(y_true, y_pred)\n    assert -1.0 <= score <= 1.0\n", "def test_empty_labels(metric_name):\n    metric = SUPERVISED_METRICS[metric_name]\n    # Test empty labels\n    with pytest.raises(ValueError):\n        metric([], [])\n\n    with pytest.raises(ValueError):\n        metric([], [1, 2])\n\n    with pytest.raises(ValueError):\n        metric([1, 2], [])\n\n\n\n", "def test_empty_input(metric_name):\n    metric = SUPERVISED_METRICS.get(metric_name) or UNSUPERVISED_METRICS[metric_name] \n    with pytest.raises(ValueError, match='Input arrays cannot be empty'):\n      metric([], [])\n", "def test_empty_array(metric_name):\n    metric = SUPERVISED_METRICS[metric_name]\n    with pytest.raises(ValueError, match=r\"Data is empty\"):\n        metric([], [])\n"], "sample_830": ["    def test_show_versions_missing_dependency(capsys):\n    \n        # Temporarily remove numpy from sys.modules\n        import numpy as np\n        del sys.modules['numpy'] \n\n        try:\n            show_versions()\n            out, err = capsys.readouterr()\n            assert 'numpy' not in out \n        finally:\n            # Restore numpy\n            sys.modules['numpy'] = np\n", "def test_show_versions_with_missing_dependencies(capsys):\n    import unittest\n    from unittest.mock import patch\n\n    class MockModule:\n        __version__ = None\n\n    with patch('sklearn.utils._show_versions.importlib.import_module', \n               side_effect=ImportError):\n        with patch('sklearn.utils._show_versions.sys.modules', {'pandas': MockModule}):\n            show_versions()\n            out, err = capsys.readouterr()\n            assert 'pandas' in out\n            assert 'None' in out\n", "def test_show_versions_missing_dep():\n    import unittest\n    from unittest.mock import patch\n    from sklearn.utils._show_versions import _get_deps_info\n    from sklearn.utils._show_versions import show_versions\n\n    class MockModule:\n        __version__ = None\n\n    with patch('sklearn.utils._show_versions.importlib.import_module', side_effect=ImportError) as mock_import:\n        mock_import.return_value = MockModule()\n        show_versions()\n        \n    assert 'sklearn' in _get_deps_info()  \n    assert _get_deps_info()['sklearn'] is None \n\n\n\n", "def test_show_versions_missing_dependency(capsys):\n    import unittest\n    from unittest.mock import patch\n\n    class MockModule:\n        __version__ = None\n\n        return module.__version__\n\n    with patch('sklearn.utils._show_versions.importlib.import_module', side_effect=ImportError):\n        with patch('sklearn.utils._show_versions.get_version', side_effect=get_version):\n            show_versions()\n            out, err = capsys.readouterr()\n            assert 'matplotlib' in out\n            assert 'None' in out\n", "def test_show_versions_missing_deps():\n    import unittest\n    from unittest.mock import patch\n    from sklearn.utils._show_versions import _get_deps_info\n\n    class MockModule:\n        __version__ = None\n\n    with patch('sklearn.utils._show_versions.importlib.import_module', \n               side_effect=ImportError):\n        with patch('sklearn.utils._show_versions._get_deps_info', \n                   return_value={\n                       'pip': None,\n                       'setuptools': None,\n                       'sklearn': None,\n                       'numpy': None,\n                       'scipy': None,\n                       'Cython': None,\n                       'pandas': None,\n                       'matplotlib': None\n                   }):\n            show_versions()\n"], "sample_946": ["def test_py_example_html_output(app, status, warning):\n    app.build()\n    # Check if the example code is rendered as an HTML code block.\n    content = (app.outdir / 'examples.html').read_text()\n    assert '<pre><code class=\"language-python\">print(\"Hello, world!\")</code></pre>' in content\n", "def test_warn_missing_reference_link(app, status, warning):\n    app.build()\n    assert 'index.rst:6: WARNING: undefined label: no-label' in warning.getvalue()\n    assert ('index.rst:6: WARNING: Failed to create a cross reference. A title or caption not found: existing-label'\n            in warning.getvalue())\n", "compilation error", "def test_python_python_use_unqualified_type_names_default(app, status, warning):\n    app.build()\n    content = (app.outdir / 'index.html').read_text()\n    assert ('<span class=\"n\"><a class=\"reference internal\" href=\"#foo.Name\" title=\"foo.Name\">'\n            '<span class=\"pre\">Name</span></a></span>' in content)\n    assert '<span class=\"n\"><span class=\"pre\">foo.Age</span></span>' in content\n", "def test_warn_missing_reference_to_module_in_docstring(app, status, warning):\n    app.build()\n    assert 'index.rst:6: WARNING: undefined label: no-module' in warning.getvalue()\n"], "sample_207": ["    def test_contains_with_mixed_types(self):\n        value = {'a': 'b', 'c': 14, 'd': [1, 2.0, True]}\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(value__contains=value),\n            self.objs[4:],\n        )\n\n", "    def test_contains_with_key_transform_order(self):\n        obj = NullableJSONModel.objects.create(value = {'a': [1, 2], 'b': {'x': 'y'}})\n        self.assertIs(NullableJSONModel.objects.filter(value__a__contains=KeyTransform('1', 'value')).exists(), True)\n        self.assertIs(NullableJSONModel.objects.filter(value__b__contains=KeyTextTransform('x', 'value')).exists(), True)\n", "    def test_complex_key_transform(self):\n        obj = NullableJSONModel.objects.create(value={'a': {'b': {'c': 1}}})\n        qs = NullableJSONModel.objects.filter(value__a__b__c__gt=0)\n        self.assertSequenceEqual(qs, [obj])\n\n\n\n", "    def test_contains_contained_by_with_key_transform_invalid(self):\n        with self.assertRaises(FieldError):\n            NullableJSONModel.objects.filter(\n                value__contains=KeyTransform('invalid_key', 'value')\n            )\n", "    def test_key_transform_with_invalid_input(self):\n        with self.assertRaises(TypeError):\n            NullableJSONModel.objects.filter(value__x__y__z=KeyTransform(1, 2))\n\n\n"], "sample_649": ["def test_log_file_level_subdirectories_are_successfully_created(\n    pytester: Pytester,", "def test_log_level_hierarchy_propagation(testdir):\n    parent_logger = logging.getLogger(\"parent\")\n    child_logger = parent_logger.getChild(\"child\")\n    \n    testdir.makepyfile(\n        \"\"\"\n        import logging\n        import os\n        \n        parent_logger = logging.getLogger(\"parent\")\n        child_logger = parent_logger.getChild(\"child\")\n        \n            parent_logger.setLevel(logging.WARNING)\n            child_logger.setLevel(logging.DEBUG)\n            \n            with caplog.at_level(logging.DEBUG):\n                parent_logger.info(\"Parent logger info message\")\n                child_logger.debug(\"Child logger debug message\")\n                assert len(caplog.record_tuples) == 1\n                assert caplog.record_tuples[0][0] == \"parent\"\n                assert caplog.record_tuples[0][2] == \"Parent logger info message\"\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    assert result.ret == ExitCode.OK\n    assert not result.stderr.lines\n\n\n\n", "def test_log_disabling_works_with_log_file(testdir):\n    testdir.makepyfile(\n        \"\"\"\n    import logging\n    disabled_log = logging.getLogger('disabled')\n    test_log = logging.getLogger('test')\n\n        test_log.info(\"Visible text!\")\n        disabled_log.warning(\"This string will be suppressed.\")\n    \"\"\"\n    )\n    log_file = os.path.join(testdir.tmpdir, \"test.log\")\n    result = testdir.runpytest(\n        f\"--log-file={log_file}\",\n        \"--log-disable=disabled\",\n    )\n    assert result.ret == ExitCode.OK\n    with open(log_file, \"r\") as f:\n        content = f.read()\n    assert \"Visible text!\" in content\n    assert \"This string will be suppressed.\" not in content\n    assert not result.stderr.lines\n", "    def test_log_disable_empty_string(testdir):\n        testdir.makepyfile(\n            \"\"\"", "    def test_log_disabling_multiple_loggers(testdir):\n        testdir.makepyfile(\n            \"\"\""], "sample_1052": ["compilation error", "compilation error", "def test_fcode_array_output():\n    from sympy import symbols\n    x = symbols('x:5')\n    y = symbols('y:5')\n    z = symbols('z:5')\n    expr = x + y + z\n    name_expr = (\"test\", expr)\n    result = codegen(name_expr, \"f95\", \"test\", header=False, empty=False,\n                     array_output='REAL*8, intent(out), dimension(1:5)')\n    source = result[0][1]\n    expected = (\n        \"subroutine test(x, y, z, out_%(hash)s)\\n\"\n        \"implicit none\\n\"\n        \"REAL*8, intent(in) :: x(1:5)\\n\"\n        \"REAL*8, intent(in) :: y(1:5)\\n\"\n        \"REAL*8, intent(in) :: z(1:5)\\n\"\n        \"REAL*8, intent(out), dimension(1:5) :: out_%(hash)s\\n\"\n        \"out_%(hash)s(1) = x(1) + y(1) + z(1)\\n\"\n        \"out_%(hash)s(2) = x(2) + y(2) + z(2)\\n\"\n        \"out_%(hash)s(3) = x(3) + y(3) + z(3)\\n\"\n        \"out_%(hash)s(4) = x(4) + y(4) + z(4)\\n\"\n        \"out_%(hash)s(5) = x(5) + y(5) + z(5)\\n\"\n        \"end subroutine\\n\"\n    ) % {'hash': 'array_out'}\n    assert source == expected\n\n\n", "compilation error", "compilation error"], "sample_266": ["    def test_no_init_file(self):\n        \"\"\"Migration directories without an __init__.py file are ignored.\"\"\"\n        loader = MigrationLoader(connection)\n        loader.load_disk()\n        migrations = [name for app, name in loader.disk_migrations if app == 'app1']\n        self.assertEqual(migrations, [])\n", "    def test_multiple_imports_in_migration(self):\n        \"\"\"\n        Tests that migrations can import from multiple modules without error.\n        \"\"\"\n        loader = MigrationLoader(connection)\n        loader.load_disk()\n        self.assertCountEqual(\n            [name for app, name in loader.disk_migrations if app == 'migrations'],\n            ['0001_initial'],\n        )\n\n\n", "    def test_detect_conflicts(self):\n        loader = MigrationLoader(connection)\n        conflicts = loader.detect_conflicts()\n        self.assertEqual(\n            conflicts,\n            {\n                'migrations': ['0001_initial', '0002_second', '0003_third']\n            },\n        )\n\n\n\n", "    def test_loading_multiple_apps(self):\n        loader = MigrationLoader(connection)\n        self.assertEqual(len(loader.disk_migrations), 7)\n        self.assertIn(('migrations', '0001_initial'), loader.disk_migrations)\n        self.assertIn(('app1', '0001_initial'), loader.disk_migrations)\n        self.assertIn(('app2', '0001_initial'), loader.disk_migrations)\n\n\n\n", "    def test_loading_run_before_duplicate(self):\n        \"\"\"\n        Tests how MigrationLoader handles duplicate 'run_before' dependencies\n        from different migrations in the same app.\n        \"\"\"\n        loader = MigrationLoader(connection)\n        recorder = MigrationRecorder(connection)\n        self.addCleanup(recorder.flush)\n\n        # Load and check order with run_before duplicates\n        loader.build_graph()\n        plan = set(loader.graph.forwards_plan(('migrations', '0003_third')))\n        self.assertEqual(plan, {('migrations', '0001_initial'), ('migrations', '0002_second'), ('migrations', '0003_third')})\n\n\n\n\n        # Apply some migrations and check order again.\n        recorder.record_applied('migrations', '0001_initial')\n        loader.build_graph()\n        plan = set(loader.graph.forwards_plan(('migrations', '0003_third')))\n        self.assertEqual(plan, {('migrations', '0002_second'), ('migrations', '0003_third')})\n\n        recorder.record_applied('migrations', '0002_second')\n        loader.build_graph()\n        plan = set(loader.graph.forwards_plan(('migrations', '0003_third')))\n        self.assertEqual(plan, {('migrations', '0003_third')})\n"], "sample_810": ["def test_pipeline_with_preprocessor_and_transformer():\n    \n    from sklearn.preprocessing import StandardScaler\n    \n    scaler = StandardScaler()\n    transf = Transf()\n    clf = Mult()\n    pipeline = Pipeline([('scaler', scaler), ('transf', transf), ('clf', clf)])\n    X = np.array([[1, 2]])\n    y = np.array([3])\n    pipeline.fit(X, y)\n    \n    assert_array_equal(pipeline.named_steps['scaler'].mean_, np.mean(X, axis=0))\n    assert_array_equal(pipeline.named_steps['scaler'].scale_, np.std(X, axis=0))\n    assert_array_equal(pipeline.named_steps['transf'].means_,\n                       pipeline.named_steps['transf'].means_)\n\n", "def test_pipeline_with_sparse_data():\n    iris = load_iris()\n    X = sparse.csr_matrix(iris.data)\n    y = iris.target\n\n    clf = Pipeline([('transf', Transf()), ('svc', SVC())])\n    clf.fit(X, y)\n    assert_array_equal(clf.predict(X), clf.predict_proba(X))\n    assert_array_equal(clf.predict(X), clf.predict_log_proba(X))\n\n    clf = Pipeline([('pca', PCA(n_components=2)), ('svc', SVC())])\n    clf.fit(X, y)\n    assert_array_equal(clf.predict(X), clf.predict_proba(X))\n    assert_array_equal(clf.predict(X), clf.predict_log_proba(X))\n\n    clf = Pipeline([('transf', StandardScaler()), ('svc', SVC())])\n    clf.fit(X, y)\n    assert_array_equal(clf.predict(X), clf.predict_proba(X))\n    assert_array_equal(clf.predict(X), clf.predict_log_proba(X))\n\n\n\n", "def test_pipeline_with_sparse_array():\n    X = sparse.csr_matrix([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n    y = np.array([0, 1, 2])\n    clf = make_pipeline(StandardScaler(), LogisticRegression())\n    clf.fit(X, y)\n    assert_array_like(clf.predict(X), y)\n    assert_array_like(clf.transform(X), clf.named_steps['scaler'].transform(X))\n\n\n", "def test_pipeline_steps_attribute():\n    pipeline = make_pipeline(Transf(), Mult(5))\n    assert_equal(pipeline.steps, [('transf', Transf()), ('mult', Mult(5))])\n", "def test_pipeline_fit_transform_with_non_array_data():\n    pipeline = make_pipeline(DummyTransf(), Mult())\n    with pytest.raises(ValueError, match=\"Data should be a NumPy array\"):\n        pipeline.fit_transform(list([1]))\n"], "sample_598": ["def test_unindexed_dims_repr():\n    ds = xr.Dataset(\n        data_vars={\"var\": ((\"x\", \"y\"), np.arange(12).reshape(3, 4))},\n        coords={\"x\": [1, 2, 3], \"y\": [4, 5, 6]},\n    )\n    ds = ds.drop_coords(\"time\")\n    actual = formatting.unindexed_dims_repr(ds)\n    expected = \"Unindexed dimensions:\\n    time\"\n    assert actual == expected\n\n\n\n", "    def test_dataset_repr_length(dataset_size, display_max_rows):\n        n_vars = dataset_size // 10\n        n_attrs = dataset_size\n        long_name = \"long_name\"\n        a = np.core.defchararray.add(long_name, np.arange(0, n_vars).astype(str))\n        b = np.core.defchararray.add(\"attr_\", np.arange(0, n_attrs).astype(str))\n        attrs = {k: 2 for k in b}\n        coords = dict(time=np.array([0, 1]))\n        data_vars = dict()\n        for v in a:\n            data_vars[v] = xr.DataArray(\n                name=v,\n                data=np.random.randn(2, 2),\n                dims=[\"time\"],\n                coords=coords,\n            )\n        ds = xr.Dataset(data_vars)\n        ds.attrs = attrs\n\n        with xr.set_options(display_max_rows=display_max_rows):\n            repr_result = repr(ds).splitlines()\n            assert len(repr_result) <= display_max_rows\n", "    def test_format_item_datetime_like(self, value):\n        expected = f\"{value}\"\n        actual = formatting.format_item(value)\n        assert actual == expected\n\n", "    def test_diff_dataset_repr_attributes(show_attributes):\n        ds_a = xr.Dataset(\n            data_vars={\"var1\": ((\"x\", \"y\"), np.array([[1, 2, 3], [4, 5, 6]])), \"var2\": ((\"x\",), np.array([3, 4]))},\n            coords={\"x\": np.array([\"a\", \"b\"]), \"y\": np.array([1, 2, 3])},\n            attrs={\"units\": \"m\", \"description\": \"desc\", \"extra_attr\": 123},\n        )\n        ds_b = xr.Dataset(\n            data_vars={\"var1\": ((\"x\",), np.array([1, 2]))},\n            coords={\"x\": np.array([\"a\", \"c\"])},\n            attrs={\"units\": \"kg\", \"extra_attr\": 456},\n        )\n\n        expected = dedent(\n            \"\"\"\\\n        Left and right Dataset objects are not identical\n        Differing dimensions:\n            (x: 2, y: 3) != (x: 2)\n        Differing data variables:\n        L   var1     (x, y) int64 1 2 3 4 5 6\n        R   var1     (x) int64 1 2\n        Data variables only on the left object:\n            var2     (x) int64 3 4\n        Differing attributes:\n        L   units: m\n        R   units: kg\n        Attributes only on the left object:\n            description: desc\n            extra_attr: 123\n        Attributes only on the right object:\n            extra_attr: 456\"\"\"\n        )\n\n        if show_attributes:\n            actual = formatting.diff_dataset_repr(ds_a, ds_b, \"identical\")\n            assert actual == expected\n        else:\n            actual = formatting.diff_dataset_repr(ds_a, ds_b, \"identical\", show_attributes=False)\n            expected = dedent(\n                \"\"\"\\\n        Left and right Dataset objects", "    def test_diff_dataset_repr_with_same_attrs(self):\n        ds_a = xr.Dataset(\n            data_vars={\"var1\": ((\"x\", \"y\"), np.arange(6).reshape(2, 3))},\n            coords={\"x\": [\"a\", \"b\"], \"y\": [1, 2]},\n            attrs={\"units\": \"m\", \"description\": \"desc\"},\n        )\n        ds_b = xr.Dataset(\n            data_vars={\"var1\": ((\"x\", \"y\"), np.arange(6).reshape(2, 3))},\n            coords={\"x\": [\"a\", \"b\"], \"y\": [1, 2]},\n            attrs={\"units\": \"m\", \"description\": \"desc\"},\n        )\n\n        # Attributes are the same, but the data is different\n        expected = dedent(\n            \"\"\"\\\n        Left and right Dataset objects are not identical\n        Differing data variables:\n        L   var1     (x, y) int64 0 1 2 3 4 5\n        R   var1     (x, y) int64 0 1 2 3 4 5\n        \"\"\"\n        )\n\n        actual = formatting.diff_dataset_repr(ds_a, ds_b, \"identical\")\n        assert actual == expected\n\n"], "sample_147": ["    def test_queryset_combination_with_annotate(self):\n        qs1 = Number.objects.filter(num=1).annotate(count=Value(1))\n        qs2 = Number.objects.filter(num=2).annotate(count=Value(2))\n        self.assertEqual(list(qs1.union(qs2).values_list('num', 'count')), [(1, 1), (2, 2)])\n\n\n\n", "    def test_union_with_multiple_relations(self):\n        Parent.objects.create(pk=1, name='Parent 1')\n        Child1 = Parent.objects.get(pk=1)\n        Child1.children.create(name='Child 1')\n        Child2 = Parent.objects.get(pk=1)\n        Child2.children.create(name='Child 2')\n        qs1 = Child1.children.all()\n        qs2 = Child2.children.all()\n        self.assertEqual(qs1.union(qs2).count(), 2)\n", "    def test_union_subquery_ordering(self):\n        qs1 = Number.objects.filter(num__lte=2).values('num')\n        qs2 = Number.objects.filter(num__gte=3).order_by('num').values('num')\n        self.assertNumbersEqual(qs1.union(qs2), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n", "    def test_related_manager_on_union(self):\n        class Child(models.Model):\n            name = models.CharField(max_length=10)\n            parent = models.ForeignKey(Number, on_delete=models.CASCADE)\n\n        Child.objects.create(name='child1', parent=Number.objects.get(num=1))\n        Child.objects.create(name='child2', parent=Number.objects.get(num=2))\n\n        qs1 = Number.objects.filter(num__lt=2).union(Number.objects.filter(num__gt=3))\n        qs2 = Number.objects.filter(num__gte=1).union(Number.objects.filter(num__lt=0))\n\n        self.assertEqual(qs1.count(), 3)\n        self.assertEqual(qs2.count(), 0)\n\n        for num in qs1.values('num'):\n            with self.subTest(num=num['num']):\n                child_qs = Child.objects.filter(parent__num=num['num'])\n                self.assertEqual(child_qs.count(), 1)\n\n        self.assertEqual(qs2.count(), 0)\n\n", "    def test_union_with_subquery(self):\n        qs1 = Number.objects.filter(num__lte=5)\n        qs2 = Number.objects.filter(num__gte=5).values('num')\n        self.assertNumbersEqual(qs1.union(qs2), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n        "], "sample_157": ["    def test_multiple_objects(self):\n        # deserialize_db_from_string() handles multiple objects.\n        data = \"\"\"\n        [\n            {\n                \"model\": \"backends.object\",\n                \"pk\": 1,\n                \"fields\": {\"obj_ref\": None, \"related_objects\": []}\n            },\n            {\n                \"model\": \"backends.object\",\n                \"pk\": 2,\n                \"fields\": {\"obj_ref\": 1, \"related_objects\": []}\n            },\n            {\n                \"model\": \"backends.objectreference\",\n                \"pk\": 1,\n                \"fields\": {\"obj\": 1}\n            }\n        ]\n        \"\"\"\n        connection.creation.deserialize_db_from_string(data)\n        obj1 = Object.objects.get(pk=1)\n        obj2 = Object.objects.get(pk=2)\n        obj_ref = ObjectReference.objects.get()\n        self.assertIsNone(obj1.obj_ref)\n        self.assertEqual(obj2.obj_ref, obj1)\n        self.assertEqual(obj_ref.obj, obj1)\n\n\n\n", "    def test_large_data(self):\n        # deserialize_db_from_string() handles large data. \n        # This test should generate a large amount of data, serialize it, and then deserialize it back\n        # to the database.\n        # ... (Implementation required - should involve generating a large number of objects and serializing them) \n", "    def test_invalid_data(self):\n        # deserialize_db_from_string() should handle invalid data gracefully.\n        invalid_data = \"\"\"\n        [\n            {\n                \"model\": \"backends.object\",\n                \"pk\": 1,\n                \"fields\": {\"obj_ref\": \"invalid\", \"related_objects\": []}\n            }\n        ]\n        \"\"\"\n        try:\n            connection.creation.deserialize_db_from_string(invalid_data)\n        except Exception as e:\n            self.assertTrue('Error decoding JSON data' in str(e))\n", "    def test_deserialization_with_foreign_keys(self):\n        # deserialize_db_from_string() handles deserialization with foreign keys.\n        data = \"\"\"\n        [\n            {\n                \"model\": \"backends.objectreference\",\n                \"pk\": 1,\n                \"fields\": {\"obj\": 1}\n            },\n            {\n                \"model\": \"backends.object\",\n                \"pk\": 1,\n                \"fields\": {\"obj_ref\": 1, \"related_objects\": []}\n            }\n        ]\n        \"\"\"\n        connection.creation.deserialize_db_from_string(data)\n        obj_ref = ObjectReference.objects.get()\n        obj = Object.objects.get()\n\n        self.assertEqual(obj.obj_ref, obj_ref)\n        self.assertEqual(obj_ref.obj, obj)\n", "    def test_object_with_foreign_key(self):\n        # deserialize_db_from_string() handles foreign keys.\n        data = \"\"\"\n        [\n            {\n                \"model\": \"backends.object\",\n                \"pk\": 1,\n                \"fields\": {\"obj_ref\": 2, \"related_objects\": []}\n            },\n            {\n                \"model\": \"backends.objectreference\",\n                \"pk\": 2,\n                \"fields\": {\"obj\": None}\n            }\n        ]\n        \"\"\"\n        connection.creation.deserialize_db_from_string(data)\n        obj = Object.objects.get()\n        obj_ref = ObjectReference.objects.get()\n        self.assertEqual(obj.obj_ref, obj_ref)\n"], "sample_466": ["    def test_serialize_custom_migrations(self):\n        class MyCustomMigration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\n                    \"MyCustomModel\",\n                    fields=[\n                        models.CharField(\n                            max_length=255,\n                            null=True,\n                            blank=True,\n                            default=None,\n                            db_index=True,\n                        )\n                    ],\n                    options={\"indexes\": [(models.Index(fields=[\"mycharfield\"],),)]},\n                )\n            ]\n\n        writer = MigrationWriter(MyCustomMigration(\"0001_initial\", \"my_app\"))\n        output = writer.as_string()\n        self.assertEqual(output, \"operations: [CreateModel(MyCustomModel, fields=[CharField(max_length=255, null=True, blank=True, default=None, db_index=True)], options={'indexes': [(Index(fields=[mycharfield],))], name='MyCustomModel')])\\n\")\n\n", "    def test_serialize_related_manager(self):\n        class Person(models.Model):\n            name = models.CharField(max_length=100)\n\n        class Post(models.Model):\n            title = models.CharField(max_length=200)\n            author = models.ForeignKey(Person, on_delete=models.CASCADE)\n\n        related_manager = Post.objects.filter(author__name=\"John Doe\")\n        self.assertSerializedEqual(related_manager)\n\n", "    def test_serialize_nested_default(self):\n        class Subclass(models.Model):\n            pass\n\n        class NestedModel(models.Model):\n            nested = models.ForeignKey(Subclass, on_delete=models.CASCADE)\n            \n        class MyModel(models.Model):\n            nested = models.ForeignKey(NestedModel, on_delete=models.CASCADE, default=NestedModel.objects.create(nested=Subclass.objects.create()))\n        \n        string = MigrationWriter.serialize(MyModel.nested.field.default)[0]\n        self.assertIn(\"NestedModel.objects.create(nested=NestedModel.objects.create())\", string)\n\n", "    def test_serialize_custom_operations(self):\n        class MyCustomOperation(migrations.MigrationOperation):\n                super().__init__()\n                self.name = name\n\n        migration = type(\n            \"Migration\", (migrations.Migration,), {\"operations\": [MyCustomOperation(\"test\")]}\n        )\n        writer = MigrationWriter(migration)\n        output = writer.as_string()\n        self.assertIn('MyCustomOperation(\"test\")', output)\n\n", "    def test_serialize_generic_relation(self):\n        class MyModel(models.Model):\n            pk = models.AutoField(primary_key=True)\n            name = models.CharField(max_length=255)\n\n        class AnotherModel(models.Model):\n            pk = models.AutoField(primary_key=True)\n            name = models.CharField(max_length=255)\n            my_relation = models.ForeignKey(MyModel, on_delete=models.CASCADE)\n\n        migration = type(\n            \"Migration\", (migrations.Migration,), {\"operations\": [migrations.CreateModel(AnotherModel)]}\n        )\n        writer = MigrationWriter(migration)\n        output = writer.as_string()\n        self.assertIn(\"models.ForeignKey(MyModel, on_delete=models.CASCADE)\", output)"], "sample_386": ["    def test_mark_safe_on_iterable(self):\n        \"\"\"\n        Calling mark_safe on an iterable should result in a safe iterable.\n        \"\"\"\n        l = [\"a\", \"b&c\"]\n        safe_list = [mark_safe(x) for x in l]\n        self.assertRenderEqual(\"{{ l }}\" , \"a&b&c\", l=safe_list)\n", "    def test_mark_safe_with_unicode_characters(self):\n        non_ascii_str = \"This string contains some unicode characters like \u00e9\u00e0\u00e7\u00fc\u00f6\"\n        safe_str = mark_safe(non_ascii_str)\n        self.assertRenderEqual(\"{{ safe_str }}\", non_ascii_str, safe_str=safe_str)\n", "    def test_mark_safe_with_non_string_input(self):\n        with self.assertRaises(TypeError):\n            mark_safe(123)\n", "    def test_mark_safe_with_custom_escape(self):\n        class CustomEscape(escape_strategy(customescape)):\n            pass\n\n        s = CustomEscape(\"a&b\")\n        self.assertRenderEqual(\"{{ s }}\", \"<<a&b>>\", s=s)\n        self.assertRenderEqual(\"{{ s|force_escape }}\", \"&lt;a&amp;b&gt;\", s=s)\n", "    def test_mark_safe_with_None(self):\n        \"\"\"\n        Calling mark_safe with None should return a SafeString with an empty string.\n        \"\"\"\n        self.assertEqual(mark_safe(None), SafeString(\"\"))\n        self.assertRenderEqual(\"{{ s }}\", \"\", s=mark_safe(None))\n"], "sample_1002": ["def test_issue_10421():\n    assert Float(str(S(123)/4)).as_real() == 30.75\n    assert Float(str(S(123)/4)).as_mpf() == mpf(30.75)\n", "def test_issue_10786():\n    assert Float('1e100')._prec == 53\n    assert Float('1e100', precision=100)._prec == 100\n    assert Float('1e100', dps=100)._prec == 100\n\n\n", "compilation error", "compilation error", "def test_issue_10650():\n    assert float(S(10000000000000000000)/10000000000000000000) == 1.0\n"], "sample_148": ["    def test_quote_with_single_quotes(self):\n        self.assertEqual(quote('something\\'s or \\'other'), 'something_27s_0Aor_0A_27other')\n", "    def test_quote_with_special_characters(self):\n        self.assertEqual(quote('something with & < > \" \\' _ +'), 'something_0Awith_0A_0A_0A_0A_0A_0A')\n", "    def test_construct_change_message_with_related_fields(self):\n        # Arrange\n        obj = Site(domain=\"example.com\", name=\"Example Site\")\n        obj.save()\n\n        form = forms.ModelForm(\n            instance=obj,\n            fields=('domain', 'name', 'location'),\n            )\n        form.initial = {\n            'domain': 'newdomain.com',\n            'name': 'New Example Site',\n            'location': Location.objects.create(name='New Location'),\n        }\n        form.save(commit=False)\n        add = True  \n\n        # Act\n        change_message = construct_change_message(form, [], add)\n\n        # Assert\n        self.assertEqual(\n            change_message,\n            [{'added': {'name': 'New Example Site', 'object': 'New Location'}}],\n        )\n", "    def test_quote_special_chars(self):\n        self.assertEqual(quote(\"'\\\"\\\\ \"), \"'_0A_0A_0A\")\n", "    def test_quote_special_chars(self):\n        self.assertEqual(quote('something\"\\'other'), 'something_0A_0Aother')\n"], "sample_372": ["    def test_trailing_forward_slash(self):\n        test_urls = [\n            ('lookahead-positive', {'city': 'a-city'}, '/lookahead+/a-city/'),\n            ('lookahead-positive-slash', {'city': 'a-city'}, '/lookahead+/a-city'),\n            ('lookbehind-positive', {'city': 'a-city'}, '/lookbehind+/a-city/'),\n            ('lookbehind-positive-slash', {'city': 'a-city'}, '/lookbehind+/a-city'),\n        ]\n        for name, kwargs, expected in test_urls:\n            with self.subTest(name=name, kwargs=kwargs):\n                self.assertEqual(reverse(name, kwargs=kwargs), expected)\n\n", "    def test_lookahead_and_lookbehind_together(self):\n        test_urls = [\n            ('/lookbehind/lookahead-city/1/'),\n            ('/lookbehind/lookahead-city/2/'),\n            ('/lookahead/lookbehind-city/1/'),\n            ('/lookahead/lookbehind-city/2/'),\n        ]\n        for test_url in test_urls:\n            with self.subTest(url=test_url):\n                self.assertEqual(resolve(test_url).kwargs, {'city': 'lookbehind-city', 'num': '1'})\n", "    def test_lookahead_capture_group_name(self):\n        test_urls = [\n            ('/my-lookahead-city', {'city': 'a-city'}),\n            ('/my-lookahead-city', {'city': 'another-city'}),\n        ]\n        for test_url, kwargs in test_urls:\n            with self.subTest(url=test_url, kwargs=kwargs):\n                match = resolve(test_url)\n                self.assertEqual(match.url_name, 'my_lookahead_city')\n                self.assertEqual(match.args, ())\n                self.assertEqual(match.kwargs, kwargs)\n\n\n\n", "    def test_lookarounds_with_capture_groups(self):\n        test_urls = [\n            ('/lookahead-pos-capture', {'city': 'a-city', 'id': 42}, '/lookahead/([0-9]+)/a-city/'),\n            ('/lookahead-neg-capture', {'city': 'a-city', 'id': 42}, '/lookahead/([0-9]+)/a-city/'),\n            ('/lookbehind-pos-capture', {'city': 'a-city', 'id': 42}, '/lookbehind/(?P<id>[0-9]+)/a-city/'),\n            ('/lookbehind-neg-capture', {'city': 'a-city', 'id': 42}, '/lookbehind/(?P<id>[0-9]+)/a-city/'),\n        ]\n        for name, kwargs, expected in test_urls:\n            with self.subTest(name=name, kwargs=kwargs):\n                self.assertEqual(reverse(name, kwargs=kwargs), expected)\n", "    def test_nested_lookahead_and_lookbehind(self):\n        test_urls = [\n            ('/nested-lookahead-positive/', {'city': 'a-city', 'state': 'CA'}),\n            ('/nested-lookahead-negative/', {'city': 'a-city', 'state': 'NY'}),\n            ('/nested-lookbehind-positive/', {'city': 'a-city', 'state': 'CA'}),\n            ('/nested-lookbehind-negative/', {'city': 'a-city', 'state': 'NY'}),\n        ]\n        for test_url, kwargs in test_urls:\n            with self.subTest(url=test_url, kwargs=kwargs):\n                self.assertEqual(resolve(test_url).kwargs, kwargs)\n\n        \n"], "sample_363": ["    def test_ManyToMany_using_to_field(self):\n        from selenium.webdriver.common.by import By\n        self.admin_login(username='super', password='secret', login_url='/')\n        self.selenium.get(self.live_server_url + reverse('admin:admin_widgets_profile_add'))\n\n        main_window = self.selenium.current_window_handle\n        # Click the Add User button to add new\n        self.selenium.find_element(By.ID, 'add_id_user').click()\n        self.wait_for_and_switch_to_popup()\n        password_field = self.selenium.find_element(By.ID, 'id_password')\n        password_field.send_keys('password')\n\n        username_field = self.selenium.find_element(By.ID, 'id_username')\n        username_value = 'newuser'\n        username_field.send_keys(username_value)\n\n        save_button_css_selector = '.submit-row > input[type=submit]'\n        self.selenium.find_element(By.CSS_SELECTOR, save_button_css_selector).click()\n        self.selenium.switch_to.window(main_window)\n        # The field now contains the new user\n        self.selenium.find_element(By.CSS_SELECTOR, '#id_user option[value=newuser]')\n\n        # Click the Add Book button to add new\n        self.selenium.find_element(By.ID, 'add_id_book').click()\n        self.wait_for_and_switch_to_popup()\n        title_field = self.selenium.find_element(By.ID, 'id_title')\n        title_field.send_keys('new book')\n\n        save_button_css_selector = '.submit-row > input[type=submit]'\n        self.selenium.find_element(By.CSS_SELECTOR, save_button_css_selector).click()\n        self.selenium.switch_to.window(main_window)\n        # The field now contains the new book\n\n        # Click the Change Book button to change it\n        self.selenium.find_element(By.ID,", "    def test_ManyToMany_using_to_field(self):\n        from selenium.webdriver.common.by import By\n        self.admin_login(username='super', password='secret', login_url='/')\n        self.selenium.get(self.live_server_url + reverse('admin:admin_widgets_profile_add'))\n\n        main_window = self.selenium.current_window_handle\n        # Click the Add Band button to add new bands\n        self.selenium.find_element(By.ID, 'add_id_bands').click()\n        self.wait_for_and_switch_to_popup()\n        for band in Band.objects.all():\n            link = self.selenium.find_element(By.LINK_TEXT, band.name)\n            link.click()\n            self.selenium.switch_to.window(main_window)\n        # The field now contains the selected bands' ids\n\n        # Click the Change bands button to change them\n        self.selenium.find_element(By.ID, 'change_id_bands').click()\n\n        self.wait_for_and_switch_to_popup()\n\n        # Clear the existing selections\n        self.selenium.find_element(By.CSS_SELECTOR, 'input[type=\"checkbox\"]')\n\n        # Check the Bogey Blues checkbox\n        link = self.selenium.find_element(By.LINK_TEXT, 'Bogey Blues')\n        link.click()\n        self.selenium.switch_to.window(main_window)\n\n        # Check the Green Potatoes checkbox\n        link = self.selenium.find_element(By.LINK_TEXT, 'Green Potatoes')\n        link.click()\n        self.selenium.switch_to.window(main_window)\n\n        \n        # Go ahead and submit the form to make sure it works\n        self.selenium.find_element(By.CSS_SELECTOR, save_button_css_selector).click()\n        self.wait_for_text('li.success', 'The profile \u201cchangednewuser\u201d was added successfully.')\n        profiles = Profile.objects.all()\n        self.assertEqual(len(profiles), 1)\n        self.assertEqual(profiles[0].user.username, username_value) \n", "    def test_ManyToMany_using_to_field(self):\n        from selenium.webdriver.common.by import By\n        self.admin_login(username='super', password='secret', login_url='/')\n        self.selenium.get(self.live_server_url + reverse('admin:admin_widgets_event_add'))\n\n        main_window = self.selenium.current_window_handle\n        # Click the Add Band button to add new\n        self.selenium.find_element(By.ID, 'add_id_band').click()\n        self.wait_for_and_switch_to_popup()\n        self.selenium.find_element(By.LINK_TEXT, 'Bogey Blues').click()\n\n        self.selenium.switch_to.window(main_window)\n        # The field now contains the new band\n        self.selenium.find_element(By.CSS_SELECTOR, '#id_band option[value=42]')\n\n        \n        # Click the Add Band button to add another\n        self.selenium.find_element(By.ID, 'add_id_band').click()\n        self.wait_for_and_switch_to_popup()\n        self.selenium.find_element(By.LINK_TEXT, 'Green Potatoes').click()\n        self.selenium.switch_to.window(main_window)\n        # The field now contains both bands\n        self.selenium.find_element(By.CSS_SELECTOR, '#id_band option[value=42,98]')\n\n        \n\n", "    def test_related_field_widget_with_prepopulated_value(self):\n        from django.contrib.auth.models import User\n        from selenium.webdriver.common.by import By\n        self.admin_login(username='super', password='secret', login_url='/')\n\n        user = User.objects.create_user('prepopulateduser', 'prepopulateduser@example.com', 'password')\n        self.selenium.get(self.live_server_url + reverse('admin:admin_widgets_relatedfield_change', args=(user.pk,)))\n\n        # Assert that the related field has the existing user's ID\n        self.wait_for_value('#id_related_user', str(user.pk)) \n\n\n", "    def test_ManyToMany_using_to_field(self):\n        from selenium.webdriver.common.by import By\n        self.admin_login(username='super', password='secret', login_url='/')\n        self.selenium.get(self.live_server_url + reverse('admin:admin_widgets_event_add'))\n\n        main_window = self.selenium.current_window_handle\n        # Click the Add User button to add new\n        self.selenium.find_element(By.ID, 'add_id_genre').click()\n        self.wait_for_and_switch_to_popup()\n        genre_name_field = self.selenium.find_element(By.ID, 'id_genre_name')\n        genre_name_field.send_keys('Jazz')\n        save_button_css_selector = '.submit-row > input[type=submit]'\n        self.selenium.find_element(By.CSS_SELECTOR, save_button_css_selector).click()\n\n        self.selenium.switch_to.window(main_window)\n        self.selenium.find_element(By.CSS_SELECTOR, '#id_genre option[value=\"Jazz\"]')\n"], "sample_383": ["    def test_ticket_24605_complex_query(self):\n        \"\"\"\n        Test for more complex queries involving nested subqueries using\n        distinct() and related fields.\n        \"\"\"\n        for i in range(2):\n            for j in range(2):\n                School.objects.create(name=f\"School {i}\")\n\n        school1 = School.objects.get(name=\"School 0\")\n        school2 = School.objects.get(name=\"School 1\")\n        s1_s2 = Student.objects.create(school=school1)\n        s2_s2 = Student.objects.create(school=school2)\n        s1_s1 = Student.objects.create(school=school1)\n        s2_s1 = Student.objects.create(school=school2)\n        qs = Student.objects.filter(\n            Q(school__name=\"School 0\") & Q(pk__in=Student.objects.filter(school=school1).distinct(\"pk\").values_list(\"pk\", flat=True))\n        )\n        self.assertSequenceEqual(qs, [s1_s1, s1_s2])\n", "    def test_ticket_23622_with_negation(self):\n        a1 = Ticket23605A.objects.create()\n        a2 = Ticket23605A.objects.create()\n        c1 = Ticket23605C.objects.create(field_c0=0.0)\n        Ticket23605B.objects.create(\n            modela_fk=a1,\n            field_b0=123,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        Ticket23605B.objects.create(\n            modela_fk=a1,\n            field_b0=23,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        Ticket23605B.objects.create(\n            modela_fk=a1,\n            field_b0=234,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        Ticket23605B.objects.create(\n            modela_fk=a1,\n            field_b0=12,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        Ticket23605B.objects.create(\n            modela_fk=a2,\n            field_b0=567,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        Ticket23605B.objects.create(\n            modela_fk=a2,\n            field_b0=76,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        Ticket23605B.objects.create(\n            modela_fk=a2,\n            field_b0=7,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        Ticket23605B.objects.create(\n            modela_fk", "    def test_ticket_parent_related_name_case_sensitivity(self):\n        Parent.objects.create(name=\"parent1\")\n        Child.objects.create(parent__name=\"parent1\", name=\"child1\")\n        Child.objects.create(parent__name=\"Parent1\", name=\"child2\")\n        child= Child.objects.get(name=\"child1\")\n        self.assertEqual(child.parent.name, \"parent1\")\n\n\n", "    def test_ticket_24913(self):\n        # Test performance regression in case of many to many with joins.\n        # The original issue was causing slow query builds.\n        Group.objects.create(name=\"Group 1\")\n        Group.objects.create(name=\"Group 2\")\n        User.objects.create(username=\"testuser1\")\n        User.objects.create(username=\"testuser2\")\n        m1 = Membership.objects.create(\n            group_id=1, user_id=1, role=\"member\"\n        )\n        m2 = Membership.objects.create(\n            group_id=1, user_id=2, role=\"admin\"\n        )\n        m3 = Membership.objects.create(\n            group_id=2, user_id=1, role=\"member\"\n        )\n        m4 = Membership.objects.create(\n            group_id=2, user_id=2, role=\"admin\"\n        )\n\n        # Test should not cause a performance regression\n        memberships = Membership.objects.filter(\n            group__name__startswith=\"Group\"\n        ).values_list(\"user__username\", flat=True)\n\n        self.assertSequenceEqual(\n            memberships, [\"testuser1\", \"testuser2\"]\n        )\n", "    def test_ticket_24901(self):\n        \"\"\"\n        Test that a query with empty values in subquery results in expected behavior\n        for values related fields.\n        \"\"\"\n        # Create some data\n        obj1 = ObjectA.objects.create(name=\"obj1\")\n        obj2 = ObjectA.objects.create(name=\"obj2\")\n        obj3 = ObjectA.objects.create(name=\"obj3\")\n        relation1 = ObjectRelation.objects.create(objecta=obj1)\n        relation2 = ObjectRelation.objects.create(objecta=obj2)\n\n        # Test case: Filter on empty values in subquery\n        qs = ObjectA.objects.filter(\n            objectrelation__isnull=True\n        ) # Should not return any objects as we have populated the objects above\n\n        self.assertQuerysetEqual(qs, [])\n\n"], "sample_328": ["    def test_related_objects_in_bulk_update(self):\n        # Create some related objects\n        tags = Tag.objects.bulk_create([Tag(name=str(i)) for i in range(10)])\n        notes = Note.objects.bulk_create([Note(note=str(i), misc=str(i)) for i in range(10)])\n\n        # Assign tags to notes\n        for note, tag in zip(notes, tags):\n            note.tag = tag\n            note.save()\n\n        # Update some notes and their related tags\n        for note, tag in zip(notes[:5], tags[:5]):\n            note.note = 'Updated Note-' + str(note.id)\n            tag.name = 'Updated Tag-' + str(tag.id)\n\n        # Update the notes and their tags in bulk\n        Note.objects.bulk_update(notes[:5], ['note', 'tag'])\n\n        # Assert the updates\n        for i in range(5):\n            note = Note.objects.get(id=i + 1)\n            self.assertEqual(note.note, 'Updated Note-' + str(i + 1))\n            self.assertEqual(note.tag.name, 'Updated Tag-' + str(i + 1))\n\n\n\n", "    def test_json_field_update_inner_dict(self):\n        JSONFieldNullable.objects.bulk_create([\n            JSONFieldNullable(json_field={'a': {'b': i}}) for i in range(10)\n        ])\n        objs = JSONFieldNullable.objects.all()\n        for obj in objs:\n            obj.json_field['a']['b'] = obj.json_field['a']['b'] + 1\n        JSONFieldNullable.objects.bulk_update(objs, ['json_field'])\n        self.assertCountEqual(\n            JSONFieldNullable.objects.filter(json_field__contains={'a': {'b': 1}}), objs\n        )\n", "    def test_null_values_update(self):\n        tag = Tag.objects.create(name='Initial Tag')\n        notes = [Note.objects.create(note='test-%s' % i, tag=tag) for i in range(10)]\n\n        # Set the tag to null for some notes\n        for note in notes[:5]:\n            note.tag = None\n        Note.objects.bulk_update(notes, ['tag'])\n\n        # Assert that the tags are updated as expected\n        self.assertCountEqual(Note.objects.filter(tag__isnull=True), notes[:5])\n        self.assertCountEqual(Note.objects.filter(tag__isnull=False), notes[5:])\n\n\n\n", "    def test_json_field_mutation(self):\n        JSONFieldNullable.objects.bulk_create([\n            JSONFieldNullable(json_field={'a': i}) for i in range(10)\n        ])\n        objs = JSONFieldNullable.objects.all()\n        for obj in objs:\n            obj.json_field['a'] = obj.json_field['a'] + 1\n        JSONFieldNullable.objects.bulk_update(objs, ['json_field'])\n        self.assertCountEqual(JSONFieldNullable.objects.filter(json_field__contains={'a': 2}), objs)\n", "    def test_date_field(self):\n        articles = [\n            Article.objects.create(name=str(i), published_date=datetime.date.today())\n            for i in range(10)\n        ]\n        past_date = datetime.date(2022, 1, 1)\n        for article in articles:\n            article.published_date = past_date\n        Article.objects.bulk_update(articles, ['published_date'])\n        self.assertCountEqual(Article.objects.filter(published_date=past_date), articles)\n\n"], "sample_408": ["    def test_migration_name_includes_operation_types(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\"Person\", fields=[]),\n                migrations.AddField(\"Person\", \"age\", models.IntegerField()),\n            ]\n\n        migration = Migration(\"some_migration\", \"test_app\")\n        self.assertEqual(\n            migration.suggest_name(), \"person_add_age\"\n        )\n\n\n\n", "    def test_operation_with_no_suggested_name_and_extra_data(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\"Person\", fields=[]),\n                migrations.RunSQL(\"SELECT 1 FROM person;\"),\n                migrations.RemoveField(\n                    model_name=\"Person\",\n                    field=\"id\",\n                ),\n            ]\n\n        migration = Migration(\"some_migration\", \"test_app\")\n        suggest_name = migration.suggest_name()\n        self.assertIs(suggest_name.startswith(\"auto_\"), True)\n", "    def test_suggest_name_handles_unique_together_operations(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.AlterField(\"Person\", \"email\", unique=True),\n            ]\n\n        migration = Migration(\"0001_initial\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"unique_email\")\n", "    def test_operation_with_non_standard_name(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\"Person\", fields=[]),\n                migrations.RenameField(\n                    model_name=\"Person\", old_name=\"id\", new_name=\"user_id\"\n                ),\n            ]\n\n        migration = Migration(\"some_migration\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"person_rename_id_to_user_id\")\n\n\n", "    def test_remove_fields_same_name_from_different_models(self):\n        before = [\n            ModelState(\n                \"testapp\", \"Person\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"name\", models.CharField(max_length=200)),\n                ],\n            ),\n            ModelState(\n                \"testapp\", \"Dog\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"name\", models.CharField(max_length=200)),\n                ],\n            ),\n        ]\n        after = [\n            ModelState(\n                \"testapp\", \"Person\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                ],\n            ),\n            ModelState(\n                \"testapp\", \"Dog\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                ],\n            ),\n        ]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"RemoveField\", \"RemoveField\"])\n        self.assertOperationAttributes(\n            changes, \"testapp\", 0, 0, name=\"name\", model_name=\"Person\"\n        )\n        self.assertOperationAttributes(\n            changes, \"testapp\", 0, 1, name=\"name\", model_name=\"Dog\"\n        )\n\n"], "sample_1027": ["def test_issue_15752():\n    assert lead_term(Poly(x**3 + 2*x**2 + x + 1, x)) == x**3\n    assert lead_term(Poly(2*x**2 + x + 1, x)) == 2*x**2\n    assert lead_term(Poly(x + 1, x)) == x\n    assert lead_term(Poly(1, x)) == 1\n\n\n", "def test_issue_14459():\n    assert Poly(x**2 + 2*x + 1, x, domain='ZZ').as_expr() == x**2 + 2*x + 1\n    assert Poly(x**2 + 2*x + 1, x, domain='RR').as_expr() == x**2 + 2*x + 1\n    assert Poly(x**2 + 2*x + 1, x, domain='QQ').as_expr() == x**2 + 2*x + 1\n", "compilation error", "def test_issue_16075():\n    p = Poly(x**2 + 2*x, x, y, domain='QQ')\n    assert p.subs(x, y) == Poly(y**2 + 2*y, y, domain='QQ')\n", "def test_issue_15577():\n    assert poly(sqrt(x)*Exp(x), x) == Poly(sqrt(x)*Exp(x), x)\n\n\n"], "sample_377": ["    def test_sensitive_variables_with_request_method(self):\n        with self.settings(DEBUG=True):\n            self.verify_unsafe_response(sensitive_method_view)\n            self.verify_unsafe_email(sensitive_method_view)\n\n        with self.settings(DEBUG=False):\n            self.verify_safe_response(sensitive_method_view)\n            self.verify_safe_email(sensitive_method_view)\n", "    def test_sensitive_post_parameters_class_method(self):\n        class MyClass:\n            @method_decorator(sensitive_post_parameters())\n                return HttpResponse()\n\n        self.assertIsInstance(\n            MyClass.a_view.__wrapped__,\n            sensitive_post_parameters.__wrapped__\n        )\n", "    def test_sensitive_post_parameters_classmethod(self):\n        class MyClass:\n            @method_decorator(sensitive_post_parameters())\n            @classmethod\n                return HttpResponse()\n\n        response = MyClass.a_view(HttpRequest())\n        self.assertEqual(response.status_code, 200)\n\n\n\n", "    def test_sensitive_post_parameters_classmethod_decorator(self):\n        class MyClass:\n            @method_decorator(sensitive_post_parameters())\n                return HttpResponse()\n\n        self.assertIsInstance(MyClass().a_view(HttpRequest()), HttpResponse)\n\n\n\n", "    def test_method_decorator_with_sensitive_post_parameters(self):\n        class MyClass:\n            @method_decorator(sensitive_post_parameters())\n                return HttpResponse()\n\n        instance = MyClass()\n        response = instance.a_view(HttpRequest())\n        self.assertContains(response, b\"HTTP_COOKIE\",  status_code=500)\n"], "sample_954": ["def test_manpage_macros(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'python.1').read_text()\n\n    assert r'\\fBMyMacro\\fP' in content\n    assert r'\\fBMacro with argument\\en\\fP' in content\n", "def test_productionlist(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'python.1').read_text()\n    assert '.sp\\n.nf\\n' in content\n    assert 'ABC::= DEF\\n' in content\n    assert 'GHI::= JKL\\n' in content\n    assert '.fi\\n' in content\n", "def test_nested_inline(app, status, warning):\n    app.build()\n    content = (app.outdir / 'manpage.1').read_text()\n    assert r'\\fBfoo=\\\\fIvar\\fI\\fP' in content\n", "def test_docinfo(app, status, warning):\n    app.build()\n    content = (app.outdir / 'python.1').read_text()\n    assert 'TITLE: sphinx.writers.manpage' in content\n    assert 'SUBTITLE: ~~~~' in content\n    assert 'AUTHOR: ~~~~' in content\n    assert 'MANUAL_SECTION: ~~~~' in content\n    assert 'DATE: ~~~~' in content\n    assert 'COPYRIGHT: ~~~~' in content\n    assert 'VERSION: ~~~~' in content\n    assert 'MANUAL_GROUP: ~~~~' in content\n", "def test_productionlist(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'python.1').read_text()\n    assert ' ::= ' in content\n\n\n"], "sample_239": ["    def test_all_valid_with_arbitrary_errors(self):\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '',\n            'choices-1-choice': 'One',\n            'choices-1-votes': '1',\n            'choices-1-extra': 'foobar',  # Field not present in the forms\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset1 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        formset2 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertIs(all_valid((formset1, formset2)), False)\n        expected_errors = [{'votes': ['This field is required.']}, {'votes': ['This field is required.'], 'extra': ['Invalid choice.']}]\n        self.assertEqual(formset1._errors, expected_errors)\n        self.assertEqual(formset2._errors, expected_errors)\n\n    \n", "    def test_empty_formset_errors(self):\n        empty_formset = ArticleFormSet(data={'form-TOTAL_FORMS': '0'})\n        self.assertFalse(empty_formset.is_valid())\n\n\n", "    def test_all_valid_with_non_form_errors(self):\n        data = {\n            'choices-TOTAL_FORMS': '1',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-MAX_NUM_FORMS': '1',  \n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '0',\n        }\n        ChoiceFormSet = formset_factory(Choice, max_num=1)\n        formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        formset.non_form_errors.append(\"This is a non-form error.\")\n\n        self.assertIs(all_valid((formset, formset)), False)\n", "    def test_all_valid_with_non_form_errors(self):\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '0',\n            'choices-1-choice': 'One',\n            'choices-1-votes': '-'\n        }\n        class CustomErrorFormSet(BaseFormSet):\n                raise ValidationError(\"This is a non-form error\")\n        ChoiceFormSet = formset_factory(Choice, formset=CustomErrorFormSet)\n        formset1 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        formset2 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertIs(all_valid((formset1, formset2)), False)\n        expected_errors = [{}, {}]\n        self.assertEqual(formset1._errors, expected_errors)\n        self.assertEqual(formset2._errors, expected_errors)\n\n\n\n", "    def test_all_valid_with_formset_errors(self):\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-MAX_NUM_FORMS': '1',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '0',\n            'choices-1-choice': 'One',\n            'choices-1-votes': '1',\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset1 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        formset2 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertIs(all_valid((formset1, formset2)), False)\n        expected_errors = [{'votes': ['This field is required.']}, {'votes': ['This field is required.']}]\n        self.assertEqual(formset1._errors, expected_errors)\n        self.assertEqual(formset2._errors, expected_errors)\n"], "sample_411": ["    def test_normalize_path_patterns_skips_double_slash_normalization(self):\n        self.assertEqual(normalize_path_patterns([\"**/foo//bar\"]), [\"**/foo/bar\"])\n", "    def test_normalize_path_patterns_handles_duplicate_elements(self):\n        self.assertEqual(\n            normalize_path_patterns([\"foo/bar\", \"foo/bar\", \"bar/*\"]),\n            [\"foo/bar\", \"bar/*\"],\n        )\n", "    def test_normalize_path_patterns_handles_trailing_slashes(self):\n        expected = [\"/path/to/file/\", \"/path/to/file/*\"]\n        self.assertEqual(normalize_path_patterns([\"/path/to/file/\", \"/path/to/file/*\"]), expected)\n    ", "    def test_normalize_path_patterns_with_trailing_slash(self):\n        expected = [os.path.normcase(p) for p in [\"foo/bar/\", \"bar/*\"]]\n        self.assertEqual(\n            normalize_path_patterns([\"foo/bar/*\", \"bar/*\"]),\n            expected,\n        )\n\n", "    def test_normalize_path_patterns_handles_relative_paths(self):\n        expected = [os.path.normcase(p) for p in [\"/foo/bar\", \"bar/*/\"]]\n        self.assertEqual(normalize_path_patterns([\"/foo/bar/*\", \"bar/*/\"]), expected)\n"], "sample_96": ["    def test_actions_with_custom_permissions(self):\n        permissions = ('custom_permission',)\n\n            pass\n        \n        action.allowed_permissions = permissions\n        \n        class BandAdmin(ModelAdmin):\n            actions = [action]\n\n        self.assertIsValid(BandAdmin, Band)\n\n\n\n", "    def test_actions_with_custom_permissions_are_valid(self):\n            pass\n\n        custom_permission_action.allowed_permissions = ('custom',)\n\n        class CustomPermissionAdmin(ModelAdmin):\n            has_custom_permission = lambda self, request, obj: request.user.has_perm('custom')\n\n            actions = (custom_permission_action,)\n\n        site = AdminSite()\n        site.register(Band, CustomPermissionAdmin)\n        self.assertIsValid(CustomPermissionAdmin, Band, admin_site=site)\n\n\n\n", "    def test_actions_callable_not_func(self):\n        class BadAction:\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = (BadAction,)\n\n        self.assertIsInvalid(\n            BandAdmin, Band,\n            \"The value of 'actions[0]' must be a callable.\",\n            id='admin.E131',\n        )\n\n\n\n", "    def test_actions_not_callable(self):\n        class BandAdmin(ModelAdmin):\n            actions = (1,)\n\n        self.assertIsInvalid(\n            BandAdmin, Band,\n            \"__name__ attributes of actions defined in \"\n            \"<class 'modeladmin.test_checks.ActionsCheckTests.\"\n            \"test_actions_not_callable.<locals>.BandAdmin'> must be callable.\",\n            id='admin.E131',\n        )\n\n", "    def test_actions_callable(self):\n        class BandAdmin(ModelAdmin):\n            actions = ('non_callable',)\n\n        self.assertIsInvalid(\n            BandAdmin, Band,\n            \"The value of 'actions[0]' must be a callable.\",\n            id='admin.E125',\n        )\n"], "sample_432": ["    def test_filter_dropdown_renders_correctly(self):\n        from selenium.webdriver.common.by import By\n\n        self.admin_login(username=\"super\", password=\"secret\")\n        self.selenium.get(self.live_server_url + reverse(\"admin:auth_user_changelist\"))\n\n        # Find the filter dropdown.\n        filter_dropdown = self.selenium.find_element(\n            By.CSS_SELECTOR, \".filter-dropdown\"\n        )\n\n        # Assert that the dropdown option for \"First Name\"\n        # appears with the correct label.\n        first_name_option = filter_dropdown.find_element(\n            By.XPATH, \"//option[text()='First Name']\")\n        self.assertEqual(first_name_option.text, \"First Name\")\n\n        # Assert that the dropdown option for \"Last Name\"\n        # appears with the correct label.\n        last_name_option = filter_dropdown.find_element(\n            By.XPATH, \"//option[text()='Last Name']\")\n        self.assertEqual(last_name_option.text, \"Last Name\")\n\n\n\n", "    def test_filter_with_empty_string_value(self):\n        \"\"\"\n        Regression test for #20452: ensure filtering works with empty string values.\n        \"\"\"\n        Band.objects.create(name=\"Band A\")\n        Band.objects.create(name=\"\")\n        self.admin_login(username=\"super\", password=\"secret\")\n        self.selenium.get(\n            self.live_server_url + reverse(\"admin:admin_changelist_band_changelist\")\n        )\n        # Find the filter input for 'name'\n        name_filter = self.selenium.find_element(By.ID, \"id_name\")\n        # Set the filter to an empty string\n        name_filter.clear()\n        name_filter.send_keys(\"\")\n        # Trigger the filter\n        name_filter.submit()\n        # Assert that the band with empty name is present\n        rows = self.selenium.find_elements(By.CSS_SELECTOR, \"tbody tr\")\n        self.assertIn(\n            self.selenium.find_element(By.XPATH, \"//td[contains(text(), '')]\").text,\n            rows[1].text,\n        )\n\n\n\n", "    def test_no_actions_when_no_selection(self):\n        Parent.objects.create(name=\"parent\")\n\n        self.admin_login(username=\"super\", password=\"secret\")\n        self.selenium.get(\n            self.live_server_url + reverse(\"admin:admin_changelist_parent_changelist\")\n        )\n\n        # Ensure there are no action buttons when no rows are selected\n        actions = self.selenium.find_elements(By.CSS_SELECTOR, \"div.actions\")\n        self.assertEqual(len(actions), 1)\n        self.assertIs(actions[0].is_displayed(), True)\n        self.assertEqual(actions[0].find_elements(By.TAG_NAME, \"button\"), [])\n\n", "    def test_multiple_sort_selectors(self):\n        from selenium.webdriver.common.by import By\n\n        Parent.objects.bulk_create(\n            [\n                Parent(name=\"parent %d\" % i)\n                for i in range(10)\n            ]\n        )\n        self.admin_login(username=\"super\", password=\"secret\")\n        self.selenium.get(\n            self.live_server_url + reverse(\"admin:admin_changelist_parent_changelist\")\n        )\n\n        # Assert there's a sort selector\n        self.assertIsNotNone(\n            self.selenium.find_element(By.ID, \"ordering_name\")\n        )\n\n        # Sort by name\n        self.selenium.find_element(By.ID, \"ordering_name\").click()\n        rows = self.selenium.find_elements(\n            By.CSS_SELECTOR, \"tbody tr\"\n        )\n        names = [row.find_element(By.CSS_SELECTOR, \"td:first-child\").text for row in rows]\n        self.assertEqual(\n            sorted(names),\n            [\n                \"parent 0\",\n                \"parent 1\",\n                \"parent 2\",\n                \"parent 3\",\n                \"parent 4\",\n                \"parent 5\",\n                \"parent 6\",\n                \"parent 7\",\n                \"parent 8\",\n                \"parent 9\",\n            ],\n        )\n\n        # Assert we clicked on that selector again\n        self.selenium.find_element(By.ID, \"ordering_name\").click()\n        names = [row.find_element(By.CSS_SELECTOR, \"td:first-child\").text for row in rows]\n        self.assertEqual(\n            sorted(names),\n            [\n                \"parent 9\",\n                \"parent 8\",\n                \"parent 7\",\n                \"parent 6\",\n                \"parent 5\",\n                \"parent 4\",\n                \"parent 3\",\n                \"parent 2\",\n                \"parent 1\",\n                \"parent 0\",\n            ],\n        )\n\n\n", "    def test_changelist_pagination_with_empty_data(self):\n        from django.contrib.admin.views.main import ChangeList\n        from django.core.exceptions import ObjectDoesNotExist\n\n        with self.assertNumQueries(0) as context:\n            m = BandAdmin(Band, custom_site)\n            request = self._mocked_authenticated_request(\"/band/\", superuser)\n            cl = m.get_changelist_instance(request)\n            cl.list_per_page = 10\n            cl.page_num = 1\n\n        # There are no objects to show\n        self.assertEqual(cl.object_list.query.count(), 0)\n        self.assertEqual(cl.paginator.page_range, [1])\n        self.assertEqual(len(cl.object_list), 0)\n\n"], "sample_781": ["def test_invalid_n_jobs_value():\n    for estimator in FOREST_ESTIMATORS:\n        est = estimator(random_state=0)\n        with pytest.raises(ValueError):\n            est.fit(X, y, n_jobs='abc')\n        with pytest.raises(ValueError):\n            est.fit(X, y, n_jobs=-10)\n", "def test_forest_oob_score_with_extra_trees():\n    X, y = datasets.make_classification(random_state=0)\n    extra_tree_clf = ExtraTreesClassifier(random_state=0, oob_score=True)\n    extra_tree_clf.fit(X, y)\n    assert hasattr(extra_tree_clf, 'oob_score_')\n    assert not np.isnan(extra_tree_clf.oob_score_)\n", "def test_forest_fit_nonconforming_samples():\n    X = np.random.rand(10, 3)\n    y = np.random.rand(10)\n    \n    # Test fit with nonconforming sample in the shape\n    with assert_raises(ValueError):\n        RandomForestClassifier(random_state=0).fit(\n            np.array([X[0]]), y)\n\n    # Test fit with nonconforming sample in the shape\n    with assert_raises(ValueError):\n        RandomForestClassifier(random_state=0).fit(\n            X[:10], np.array([y[0]]))\n\n\n\n", "compilation error", "def test_forest_sparse_categorical_target():\n    X, y = make_classification(n_samples=100, n_features=10,\n                               n_informative=5,\n                               n_redundant=0,\n                               random_state=random_state)\n    y = y.astype('category')\n\n    clf = RandomForestClassifier(n_estimators=100, random_state=0)\n    clf.fit(X, y)\n\n    # Check that prediction works\n    predicted = clf.predict(X)\n    assert_array_equal(predicted.astype(int), y.cat.codes)\n\n    # Check that feature importances are valid\n    assert_equal(clf.feature_importances_.shape, (10,))\n    assert_greater(clf.feature_importances_.sum(), 0)\n\n\n"], "sample_1192": ["compilation error", "def test_symbols_with_cls():\n    from sympy.core.symbol import Function\n\n    x = Symbol('x')\n    y = Symbol('y')\n\n    class MyFunction(Function):\n        @property\n            return True\n\n    f = MyFunction('f')\n\n    f_sym = symbols('f', cls=MyFunction)\n\n    assert isinstance(f_sym, MyFunction)\n    assert f_sym.is_myfunction\n    assert f_sym(x) == f(x)\n    assert f_sym(x, y) == f(x, y)\n\n\n\n", "def test_disambiguate_complex_exprs():\n    x, y, z = symbols('x y z')\n    a, b, c, d = symbols('a b c d')\n    p = symbols('p')\n    q = symbols('q')\n\n    expr1 = (x + y) * (x - y)\n    expr2 = (x + y) * p * (x - y)\n    expr3 = (x + y) * q * (x - y) * (a + b*c)\n\n    res1 = disambiguate(*expr1)\n    res2 = disambiguate(*expr2)\n    res3 = disambiguate(*expr3)\n\n    assert res1 == (x + y, x - y)\n    assert res2 == (x + y, p, x - y)\n    assert res3 == (x + y, q, x - y, a + b*c)\n\n    expr4 = (x + y)*(x - y)*(p + q)\n    expr5 = (x + y)*(x - y)*(p + q)*(a + b*c)\n\n    res4 = disambiguate(*expr4)\n    res5 = disambiguate(*expr5)\n\n    assert res4 == (x + y, x - y, p + q)\n    assert res5 == (x + y, x - y, p + q, a + b*c)\n\n    expr6 = ((x + y)*(x - y))*(p + q)\n    expr7 = ((x + y)*(x - y))*(p + q)*(a + b*c)\n\n    res6 = disambiguate(*expr6)\n    res7 = disambiguate(*expr7)\n\n    assert res6 == ((x + y, x - y), p + q)\n    assert res7 == ((x + y, x - y), p + q, a + b*c)\n\n\n", "def test_issue_4431():\n    x, y = symbols('x,y')\n    z = Symbol('z')\n    expr = (x - y)**2 + 2*z*x\n    assert disambiguate(expr) == (x - y)**2 + 2*z*x  \n\n", "def test_symbols_with_complex_names():\n    from sympy import symbols\n\n    # Test symbols with names containing special characters\n    symbols('x_+-')  # Test underscore, plus, and minus\n    symbols('x.y')  # Test dot\n    symbols('x$y')  # Test dollar\n    symbols('x#y')  # Test hash symbol\n    symbols('x%y')  # Test percent symbol\n    symbols('x@y')  # Test at symbol\n\n    # Test symbols with names containing whitespace\n    symbols('x y')  # Test multiple symbols with whitespace\n    symbols('x y z')  # Test multiple symbols with whitespace\n    symbols('x_y z')  # Test symbols with underscore and whitespace\n    symbols('x y_z')  # Test symbols with whitespace and underscore\n\n    # Test symbols with names containing brackets\n    symbols('x(y)')\n    symbols('x[y]')\n    symbols('x[y] z')\n\n    # Test symbols with names containing parentheses\n    symbols('(x)')\n    symbols('x(y)')\n\n\n\n"], "sample_984": ["def test_issue_11921():\n    from sympy.physics.mechanics import dynamicsymbols\n    q1, q2, m = dynamicsymbols(\"q1 q2 m\")\n    assert str(m * (q1**2 + q2**2)) == \"m*(q1**2 + q2**2)\"\n", "def test_MatrixElement_printing_with_complex():\n    M = MatrixSymbol(\"M\", 2, 2, complex=True)\n    assert str(M[0, 1]) == \"M[0, 1]\"\n    assert str(M[1, 0].conjugate()) == \"conjugate(M[1, 0])\"\n    assert str(M[0, 0] + M[1, 1].conjugate()) == \"M[0, 0] + conjugate(M[1, 1])\"\n\n", "def test_KroneckerDelta():\n    assert str(KroneckerDelta(x, y)) == \"KroneckerDelta(x, y)\"\n    assert str(KroneckerDelta(x, x)) == \"KroneckerDelta(x, x)\"\n    assert str(KroneckerDelta(x, y + 1)) == \"KroneckerDelta(x, y + 1)\"\n", "def test_transpose():\n    from sympy.matrices import Matrix, eye\n    A = Matrix([[1, 2], [3, 4]])\n    assert str(A.transpose()) == \"Matrix([[1, 3], [2, 4]])\"\n    assert str(eye(3).transpose()) == \"Matrix([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\"\n", "def test_Mul():\n    assert str(2 * x*y*z) == \"2*x*y*z\"\n    assert str(2 * x * y*z) == \"2*x*y*z\"\n    assert str(x * y * z * 2) == \"2*x*y*z\"\n    assert str(2 * x + 3 * y) == \"2*x + 3*y\"\n    assert str(x * (y + z)) == \"x*(y + z)\"\n    assert str(x * (y + z) * 2) == \"2*x*(y + z)\"\n    assert str(2 * (x * y)) == \"2*x*y\"\n    assert str(x * (2 * y)) == \"2*x*y\"\n    assert str(2 * x * y * z) == \"2*x*y*z\"\n"], "sample_678": ["def test_on_rm_rf_error(tmp_path):\n    path = Path(tmp_path) / \"temp_dir\"\n    path.mkdir()\n\n        raise PermissionError(\"Operation requires elevated privileges\")\n\n    with unittest.mock.patch(\"os.rmdir\", side_effect=mock_rmdir):\n        on_rm_rf_error(\n            \"rmdir\", str(path), (OSError, PermissionError), start_path=path\n        )\n\n\n", "def test_register_cleanup_lock_removal(tmp_path):\n    lock_path = tmp_path / \"lock\"\n    lock_path.touch()\n\n        lock_path.unlink()\n\n    register = unittest.mock.MagicMock()\n    register.side_effect = lambda f: f\n    register_cleanup_lock_removal(lock_path, register)\n\n    assert register.call_count == 1\n    assert register.call_args[0][0] is cleanup_function\n\n\n\n", "def test_on_rm_rf_error_returns_true_for_other_functions(monkeypatch):\n        return None\n\n    monkeypatch.setattr(os, \"open\", mock_open)\n        return None\n\n    monkeypatch.setattr(os.stat, \"st_mode\", lambda *args: 0o644)\n    monkeypatch.setattr(os, \"chmod\", mock_chmod)\n\n    success = on_rm_rf_error(\n        \"mock_function\",\n        \"/path/to/file\",\n        (FileNotFoundError, ),\n        start_path=Path(\"/path/to/start\")\n    )\n    assert success\n\n\n\n\n", "def test_cleanup_numbered_dir_with_cleanup_lock_removal(tmp_path):\n    for i in range(3):\n        prefix = f\"test_{i}\"\n        numbered_dir = make_numbered_dir_with_cleanup(\n            tmp_path, prefix, keep=1, lock_timeout=1\n        )\n        cleanup_numbered_dir(tmp_path, prefix, keep=1, consider_lock_dead_if_created_before=0)\n        assert not numbered_dir.exists()\n\n\n", "def test_cleanup_candidates():\n    root = py.path.local.Path(\".\")\n    prefix = \"test_\"\n    keep = 2\n\n    # Test with no numbered directories\n    assert list(cleanup_candidates(root, prefix, keep)) == []\n\n    # Test with numbered directories\n    for i in range(5):\n        root.joinpath(f\"{prefix}{i}).mkdir()\n\n    candidates = cleanup_candidates(root, prefix, keep)\n    assert len(list(candidates)) == 3\n\n\n"], "sample_409": ["    def test_repr_with_plural(self):\n        block_translate_node = BlockTranslateNode(\n            extra_context={},\n            singular=[\n                Token(TokenType.TEXT, \"content\"),\n                Token(TokenType.VAR, \"variable\"),\n            ],\n            plural=[\n                Token(TokenType.TEXT, \"plural_content\"),\n                Token(TokenType.VAR, \"plural_variable\"),\n            ],\n        )\n        self.assertEqual(\n            repr(block_translate_node),\n            \"<BlockTranslateNode: extra_context={} \"\n            'singular=[<Text token: \"content...\">, <Var token: \"variable...\">] '\n            'plural=[<Text token: \"plural_content...\">, <Var token: \"plural_variable...\">]>'\n        )\n", "    def test_render_with_plural_context(self):\n        with self.subTest(\"singular\"):\n            block_translate_node = BlockTranslateNode(\n                extra_context={\"count\": 1},\n                singular=[Token(TokenType.TEXT, \"singular content\")],\n                plural=[Token(TokenType.TEXT, \"plural content\")],\n            )\n            rendered = block_translate_node.render(Context())\n            self.assertEqual(rendered, \"singular content\")\n\n        with self.subTest(\"plural\"):\n            block_translate_node = BlockTranslateNode(\n                extra_context={\"count\": 2},\n                singular=[Token(TokenType.TEXT, \"singular content\")],\n                plural=[Token(TokenType.TEXT, \"plural content\")],\n            )\n            rendered = block_translate_node.render(Context())\n            self.assertEqual(rendered, \"plural content\")\n\n", "    def test_render_context_with_default(self):\n        block_translate_node = BlockTranslateNode(\n            extra_context={\"default_context_key\": \"default_value\"},\n            singular=[\n                Token(TokenType.TEXT, \"content\"),\n                Token(TokenType.VAR, \"variable\"),\n            ],\n        )\n        with self.create_context() as context:\n            context.update({\"variable\": \"test_variable\"})\n            self.assertEqual(\n                block_translate_node.render(context),\n                \"contenttest_variable\",\n            )\n\n\n", "    def test_percent_formatting_in_blocktranslate_with_empty_string(self):\n        \"\"\"\n        Zero-length strings are escaped correctly within blocktranslate, singular,\n        or plural.\n        \"\"\"\n        t_sing = self.get_template(\n            \"{% load i18n %}{% blocktranslate %}There are %(num_comments)s comments\"\n            \"{% endblocktranslate %}\"\n        )\n        t_plur = self.get_template(\n            \"{% load i18n %}{% blocktranslate count num as number %}\"\n            \"%(percent)s% represents {{ num }} object{% plural %}\"\n            \"%(percent)s% represents {{ num }} objects{% endblocktranslate %}\"\n        )\n        with translation.override(\"de\"):\n            self.assertEqual(\n                t_sing.render(Context({\"num_comments\": \"\"})),\n                \"There are %(num_comments)s comments\",\n            )\n            self.assertEqual(\n                t_plur.render(Context({\"percent\": \"\", \"num\": 1})),\n                \"%(percent)s% represents 1 object\",\n            )\n            self.assertEqual(\n                t_plur.render(Context({\"percent\": \"\", \"num\": 4})),\n                \"%(percent)s% represents 4 objects\",\n            )\n\n", "    def test_repr_with_plural(self):\n        block_translate_node = BlockTranslateNode(\n            extra_context={},\n            singular=[\n                Token(TokenType.TEXT, \"content\"),\n                Token(TokenType.VAR, \"variable\"),\n            ],\n            plural=[\n                Token(TokenType.TEXT, \"plural_content\"),\n                Token(TokenType.VAR, \"plural_variable\"),\n            ],\n        )\n        self.assertEqual(\n            repr(block_translate_node),\n            \"<BlockTranslateNode: extra_context={} \"\n            'singular=[<Text token: \"content...\">, <Var token: \"variable...\">] '\n            \"plural=[<Text token: 'plural_content...'>, <Var token: 'plural_variable...'>]>\",\n        )\n"], "sample_797": ["def test_power_transformer_sparse_input(method):\n    X_sparse = sparse.csr_matrix(X_2d)\n    pt = PowerTransformer(method=method)\n\n    # Test that fit works on sparse input\n    pt.fit(X_sparse)\n\n    # Test that transform works on sparse input\n    X_trans = pt.transform(X_sparse)\n    assert isinstance(X_trans, sparse.csr_matrix)\n\n    # Test that inverse_transform works on sparse input\n    X_inv_trans = pt.inverse_transform(X_trans)\n    assert isinstance(X_inv_trans, sparse.csr_matrix)\n\n", "compilation error", "def test_power_transformer_sparse_matrix():\n    X_sparse = sparse.csr_matrix(X_1col)\n\n    for method in ['box-cox', 'yeo-johnson']:\n        pt = PowerTransformer(method=method)\n        X_trans = pt.fit_transform(X_sparse)\n        assert isinstance(X_trans, sparse.csr_matrix)\n        assert X_sparse.shape == X_trans.shape\n\n        X_inv_trans = pt.inverse_transform(X_trans)\n        assert isinstance(X_inv_trans, sparse.csr_matrix)\n        assert X_sparse.shape == X_inv_trans.shape\n\n\n\n", "def test_power_transformer_sparse_input(method):\n    X = sparse.csr_matrix(X_2d)\n    pt = PowerTransformer(method=method)\n    X_trans = pt.fit_transform(X)\n    assert isinstance(X_trans, sparse.csr_matrix)\n    X_inv_trans = pt.inverse_transform(X_trans)\n    assert isinstance(X_inv_trans, sparse.csr_matrix)\n\n\n\n", "def test_power_transformer_sparse():\n    # Test on sparse matrices\n\n    X_sparse = sparse.csr_matrix(X_2d)\n\n    for method in ['box-cox', 'yeo-johnson']:\n        pt = PowerTransformer(method=method)\n        X_trans = pt.fit_transform(X_sparse)\n        assert isinstance(X_trans, sparse.csr_matrix), X_trans\n        assert_array_almost_equal(pt.inverse_transform(X_trans).toarray(),\n                                  X_2d)\n\n    pt = PowerTransformer(method='box-cox', standardize=False)\n    pt.fit(X_sparse)\n    assert pt.lambdas_.shape == (X_sparse.shape[1],)\n\n\n"], "sample_1161": ["def test_printing_matrix_slice():\n\n    M = MatrixSymbol('M', 3, 3)\n\n    assert str(M[0, :]) == 'M[0, :]'\n    assert str(M[:, 0]) == 'M[:, 0]'\n    assert str(M[1:2, 1:2]) == 'M[1:2, 1:2]'\n    assert str(M[::2, ::2]) == 'M[::2, ::2]'\n    assert str(M[1:3:2, 1:3:2]) == 'M[1:3:2, 1:3:2]'\n    assert str(M[1, 2:3]) == 'M[1, 2:3]'\n    assert str(M[1:2, 2:]) == 'M[1:2, 2:]'\n    assert str(M[1:2, :2]) == 'M[1:2, :2]'\n    assert str(M[None, 1:2]) == 'M[:, 1:2]'\n    assert str(M[1:2, None]) == 'M[1:2, :]'\n\n\n\n", "def test_printing_functions():\n    from sympy import cos, sin, tan, exp, log, sqrt\n    assert sstr(cos(x)) == 'cos(x)'\n    assert sstr(sin(x)) == 'sin(x)'\n    assert sstr(tan(x)) == 'tan(x)'\n    assert sstr(exp(x)) == 'exp(x)'\n    assert sstr(log(x)) == 'log(x)'\n    assert sstr(sqrt(x)) == 'sqrt(x)'\n", "def test_issue_22279():\n    from sympy.physics.quantum import QuantumState, QTensor\n    psi = QuantumState(create_tensor_basis(['a', 'b'], [2, 3]))\n    assert str(psi) == \"QuantumState(tensor_basis(['a', 'b'], [2, 3]))\"\n    A = QTensor(3, 2, 5)\n    assert str(A) == \"QTensor(3, 2, 5)\"\n\n", "def test_issue_16551():\n    from sympy.matrices import Matrix, Eye\n    A = Matrix([[1, 2], [3, 4]])\n    I = Eye(2)\n    assert str(A + I) == 'Matrix([[2, 2], [3, 5]])' \n    assert str(A*I) == 'Matrix([[1, 2], [3, 4]])'\n", "compilation error"], "sample_139": ["    def test_get_changelist_instance(self):\n        \"\"\"\n        Test that get_changelist_instance is returning the correct instance of ChangeList\n        regardless of the request.\n        \"\"\"\n        m = GroupAdmin(Group, custom_site)\n        request_without_permission = self.factory.get('/group/')\n        request_with_permission = self.factory.get('/group/', user=self.superuser)\n\n        # Test with and without permission for similar outputs.\n        changelist_instance_without_permission = m.get_changelist_instance(request_without_permission)\n        changelist_instance_with_permission = m.get_changelist_instance(request_with_permission)\n        self.assertEqual(changelist_instance_without_permission, changelist_instance_with_permission)\n\n", "    def test_pagination_with_empty_results(self):\n        \"\"\"\n        Pagination should display the correct page range and information\n        when the changelist is empty (#15653).\n        \"\"\"\n        m = GroupAdmin(Group, custom_site)\n        request = self.factory.get('/group/')\n        request.user = self.superuser\n        cl = m.get_changelist_instance(request)\n        cl.list_per_page = 10\n        cl.page_num = 1\n\n        with self.assertNumQueries(1):  # Only one query for the empty result set\n            cl.get_results(request)\n\n        # Empty changelist should show a message\n        response = m.changelist_view(request)\n        self.assertIn('No matching query found.', response.content.decode())\n        self.assertIn('<div class=\"dataTables_info\">', response.content.decode())\n        self.assertIn('showing 0 to 0 of 0 entries', response.content.decode())\n\n\n\n", "    def test_change_list_view_for_model_with_related_fields(self):\n        \"\"\"\n        ChangeList view displays properly for models with related fields\n        (#17803, #19973).\n        \"\"\"\n        superuser = self._create_superuser('superuser')\n        m = RelatedFieldAdmin(RelatedField, custom_site)\n        request = self._mocked_authenticated_request('/relatedfield/', superuser)\n        response = m.changelist_view(request)\n        self.assertIn('<tr class=\"row1\">', response.rendered_content)  # Check for a row with related field\n        self.assertIn('Related Field', response.rendered_content)\n\n", "    def test_get_changelist_instance_with_extra_context(self):\n        \"\"\"\n        When `extra_context` is provided to `get_changelist_instance`,\n        it should be included in the resulting ChangeList instance's context.\n        \"\"\"\n        m = SwallowAdmin(Swallow, custom_site)\n        request = self._mocked_authenticated_request('/swallow/', self.superuser)\n        extra_context = {'foo': 'bar'}\n        cl = m.get_changelist_instance(request, extra_context=extra_context)\n        self.assertEqual(cl.context.get('foo'), 'bar')\n\n\n\n", "    def test_ordering_with_fields_not_in_model(self):\n        class UnorderedModel(models.Model):\n            name = models.CharField(max_length=100)\n\n        class UnorderedModelAdmin(admin.ModelAdmin):\n            ordering = ['non_existent_field']\n\n        custom_site.register(UnorderedModel, UnorderedModelAdmin)\n        try:\n            site = admin.AdminSite(name='admin')\n            model_admin = UnorderedModelAdmin(UnorderedModel, site)\n            request = self.factory.get('/')\n            request.user = self.superuser\n            changelist = model_admin.get_changelist_instance(request)\n            self.assertEqual(changelist._get_ordering(), ['unindexed_field'])\n        finally:\n            custom_site.unregister(UnorderedModel)\n"], "sample_1176": ["def test_issue_11870():\n    from sympy.core import Symbol, oo\n    x = Symbol('x')\n    assert (oo / x).is_infinite == True\n    assert (-oo / x).is_infinite == True\n\n\n\n", "compilation error", "def test_issue_10982():\n    assert S(1.23456789).floor() == 1\n    assert S(1.23456789).ceil() == 2\n\n\n\n", "def test_mod_issue_8025():\n    assert 10 % S.Half == 5\n    assert -10 % S.Half == -5\n", "def test_issue_11018():\n    assert S(1).is_integer is True\n    assert S(1.0).is_integer is True\n    assert S(1.1).is_integer is False\n    assert S(1.000000000001).is_integer is False\n\n\n\n"], "sample_764": ["def test_column_transformer_callable_specifier_invalid_return():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    ct = ColumnTransformer([('trans', Trans(), lambda X: [1])],\n                           remainder='drop')\n\n    assert_raise_message(ValueError, 'callable should return a list',\n                         ct.fit_transform, X_array)\n\n", "def test_column_transformer_callable_specifier_errors():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n\n    # invalid return type\n        return 1\n\n    assert_raise_message(TypeError, \"Callable must return a list or tuple of column names\",\n                         ColumnTransformer([('trans', Trans(), func)],\n                                          remainder='drop').fit_transform, X_array)\n\n    # return type is not list or tuple\n        return {'col1': 0}\n\n    assert_raise_message(TypeError, \"Callable must return a list or tuple of column names\",\n                         ColumnTransformer([('trans', Trans(), func)],\n                                          remainder='drop').fit_transform, X_array)\n\n\n    # raise an error inside the callable\n        raise ValueError('Some error')\n\n    assert_raise_message(ValueError, 'Some error',\n                         ColumnTransformer([('trans', Trans(), func)],\n                                          remainder='drop').fit_transform, X_array)\n\n    # non-existent columns\n        return ['does_not_exist']\n\n    assert_raise_message(ValueError, \"Column 'does_not_exist' not found\",\n                         ColumnTransformer([('trans', Trans(), func)],\n                                          remainder='drop').fit_transform, X_array)\n\n", "def test_column_transformer_callable_specifier_remainder():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    X_res_first = np.array([[0, 1, 2]]).T\n\n        return [0]\n\n    ct = ColumnTransformer([('trans', Trans(), [0])], remainder=func)\n    assert_array_equal(ct.fit_transform(X_array), X_res_first)\n    assert_array_equal(ct.fit(X_array).transform(X_array), X_res_first)\n    assert callable(ct.transformers[1][1])\n    assert ct.transformers_[1][2] == [1, 2]\n\n    pd = pytest.importorskip('pandas')\n    X_df = pd.DataFrame(X_array, columns=['first', 'second'])\n\n        return ['first']\n\n    ct = ColumnTransformer([('trans', Trans(), [0])], remainder=func)\n    assert_array_equal(ct.fit_transform(X_df), X_res_first)\n    assert_array_equal(ct.fit(X_df).transform(X_df), X_res_first)\n    assert callable(ct.transformers[1][1])\n    assert ct.transformers_[1][2] == ['second']\n", "    def func(X):\n        return {'col1': X[:, 0], 'col2': X[:, 1]}", "def test_column_transformer_sparse_output():\n\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    ct = ColumnTransformer([('trans1', StandardScaler(), [0])],\n                           sparse_output=True)\n\n    # Ensure sparse output setting is respected\n    assert sparse.issparse(ct.fit_transform(X_array))\n\n\n"], "sample_226": ["    def test_non_migrated_app(self, mocked_sync_apps, mocked_migrate, *mocked_objects):\n        test_connection = get_connection_copy()\n        test_connection.settings_dict['TEST']['MIGRATE'] = True\n        creation = test_connection.creation_class(test_connection)\n        if connection.vendor == 'oracle':\n            # Don't close connection on Oracle.\n            creation.connection.close = mock.Mock()\n        old_database_name = test_connection.settings_dict['NAME']\n        try:\n            with mock.patch.object(creation, '_create_test_db'):\n                creation.create_test_db(verbosity=0, autoclobber=True, serialize=False)\n            # Migrations run, but only for migrated apps.\n            mocked_migrate.assert_called()\n            args, kwargs = mocked_migrate.call_args\n            self.assertEqual(args, ([('backends.app_unmigrated', '0001_initial')],))\n            self.assertEqual(len(kwargs['plan']), 1)\n\n            # App is synced.\n            mocked_sync_apps.assert_called()\n            mocked_args, _ = mocked_sync_apps.call_args\n            self.assertEqual(mocked_args[1], {'app_unmigrated'})\n        finally:\n            with mock.patch.object(creation, '_destroy_test_db'):\n                creation.destroy_test_db(old_database_name, verbosity=0)\n", "    def test_object_reference_with_non_existent_object(self):\n        # serialize_db_to_string() and deserialize_db_from_string() handle\n        # cases where object references point to non-existent objects.\n        try:\n            ObjectReference.objects.create(obj=Object.objects.create(id=1))\n            # Serialize objects.\n            with mock.patch('django.db.migrations.loader.MigrationLoader') as loader:\n                # serialize_db_to_string() serializes only migrated apps, so mark\n                # the backends app as migrated.\n                loader_instance = loader.return_value\n                loader_instance.migrated_apps = {'backends'}\n                data = connection.creation.serialize_db_to_string()\n            Object.objects.get(id=1).delete()\n            # Deserialize objects.\n            with self.assertRaises(ObjectDoesNotExist):\n                connection.creation.deserialize_db_from_string(data)\n        except ObjectDoesNotExist:\n            pass\n", "    def test_preserve_existing_test_database(self, *mocked_objects):\n        test_connection = get_connection_copy()\n        # Set existing data in the test database\n        Object.objects.create(name='Existing')\n\n        creation = test_connection.creation_class(test_connection)\n        if connection.vendor == 'oracle':\n            # Don't close connection on Oracle.\n            creation.connection.close = mock.Mock()\n        old_database_name = test_connection.settings_dict['NAME']\n        try:\n            with mock.patch.object(creation, '_create_test_db'):\n                creation.create_test_db(verbosity=0, autoclobber=False, keepdb=True)\n            # Test data should be preserved\n            self.assertEqual(Object.objects.count(), 1)\n        finally:\n            with mock.patch.object(creation, '_destroy_test_db'):\n                creation.destroy_test_db(old_database_name, verbosity=0)\n\n\n", "    def test_empty_database(self):\n        # serialize_db_to_string() and deserialize_db_from_string() handle\n        # empty databases.\n        Object.objects.all().delete()\n        ObjectReference.objects.all().delete()\n        ObjectSelfReference.objects.all().delete()\n        CircularA.objects.all().delete()\n        CircularB.objects.all().delete()\n\n        data = connection.creation.serialize_db_to_string()\n\n        ObjectSelfReference.objects.all().delete()\n        CircularA.objects.all().delete()\n        CircularB.objects.all().delete()\n        Object.objects.all().delete()\n        ObjectReference.objects.all().delete()\n        connection.creation.deserialize_db_from_string(data)\n        self.assertEqual(Object.objects.count(), 0)\n        self.assertEqual(ObjectReference.objects.count(), 0)\n        self.assertEqual(ObjectSelfReference.objects.count(), 0)\n        self.assertEqual(CircularA.objects.count(), 0)\n        self.assertEqual(CircularB.objects.count(), 0) \n", "    def test_circular_reference_with_many_to_many(self):\n        # serialize_db_to_string() and deserialize_db_from_string() handles\n        # circular references for models with many-to-many relationships.\n        obj_a = Object.objects.create()\n        obj_b = Object.objects.create()\n        obj_a.related_objects.add(obj_b)\n        obj_b.related_objects.add(obj_a)\n        obj_a.save()\n        obj_b.save()\n\n        # Serialize objects.\n        with mock.patch('django.db.migrations.loader.MigrationLoader') as loader:\n            # serialize_db_to_string() serializes only migrated apps, so mark\n            # the backends app as migrated.\n            loader_instance = loader.return_value\n            loader_instance.migrated_apps = {'backends'}\n            data = connection.creation.serialize_db_to_string()\n        Object.objects.all().delete()\n\n        # Deserialize objects.\n        connection.creation.deserialize_db_from_string(data)\n        obj_a = Object.objects.get()\n        obj_b = Object.objects.get()\n        self.assertIn(obj_b, obj_a.related_objects.all())\n        self.assertIn(obj_a, obj_b.related_objects.all())\n"], "sample_787": ["def test_balanced_accuracy_score_multiclass_with_different_class_counts():\n    y_true = np.array(['a', 'a', 'a', 'b', 'b', 'c', 'c', 'c', 'c'])\n    y_pred = np.array(['a', 'a', 'a', 'b', 'b', 'a', 'c', 'c', 'c'])\n    balanced = balanced_accuracy_score(y_true, y_pred)\n    assert_almost_equal(balanced, 0.75)\n\n    adjusted_balanced = balanced_accuracy_score(y_true, y_pred, adjusted=True)\n    chance = balanced_accuracy_score(y_true, np.full_like(y_true, y_true[0]))\n    assert_almost_equal(adjusted_balanced, (balanced - chance) / (1 - chance))\n", "    def test_balanced_accuracy_score_tie():\n        y_true = ['a', 'a', 'b', 'b']\n        y_pred = ['a', 'a', 'a', 'b']\n        balanced = balanced_accuracy_score(y_true, y_pred)\n        assert balanced == 1.0\n", "def test_brier_score_loss_multiclass():\n    y_true = np.array([0, 1, 2, 1, 2, 2])\n    y_pred = np.array([[0.1, 0.4, 0.5],\n                       [0.8, 0.1, 0.1],\n                       [0.2, 0.3, 0.5],\n                       [0.8, 0.1, 0.1],\n                       [0.2, 0.3, 0.5],\n                       [0.2, 0.3, 0.5]])\n    true_score = linalg.norm(y_true[:, None] - y_pred) ** 2 / len(y_true)\n\n    assert_almost_equal(brier_score_loss(y_true, y_pred, average=None),\n                        true_score)\n\n    assert_almost_equal(brier_score_loss(y_true, y_pred, average='macro'),\n                        true_score.mean())\n\n    assert_almost_equal(brier_score_loss(y_true, y_pred, average='micro'),\n                        true_score.sum() / len(y_true))\n\n    assert_raises(ValueError, brier_score_loss,\n                  y_true, y_pred[1:])\n    assert_raises(ValueError, brier_score_loss,\n                  y_true, y_pred + 1.)\n    assert_raises(ValueError, brier_score_loss,\n                  y_true, y_pred - 1.)\n", "def test_balanced_accuracy_score_with_different_class_distribution(y_true, y_pred):\n    balanced = balanced_accuracy_score(y_true, y_pred)\n    assert balanced != pytest.approx(accuracy_score(y_true, np.round(y_pred)))\n\n\n\n", "def test_zero_sum_loss_with_empty_arrays():\n    assert_equal(zero_sum_loss(np.array([]), np.array([])), 0.0)\n    assert_equal(zero_sum_loss(np.array([1]), np.array([])), 0.0)\n\n\n\n"], "sample_597": ["    def test_merge_dicts_multi_level(self):\n        actual = xr.merge([\n            {\"foo\": {\"bar\": 0}, \"baz\": \"one\"},\n            {\"foo\": {\"bar\": 1}, \"baz\": \"two\"},\n            {\"foo\": {\"baz\": \"three\"}},\n        ])\n        expected = xr.Dataset(\n            {\"foo\": {\"bar\": [0, 1, np.nan], \"baz\": [\"one\", \"two\", \"three\"]},\n             \"baz\": [\"one\", \"two\", \"three\"]}\n        )\n        assert actual.identical(expected)\n\n        actual = xr.merge([\n            {\"foo\": {\"bar\": {\"x\": 0, \"y\": 1}}},\n            {\"foo\": {\"bar\": {\"x\": 2, \"y\": 3}}},\n        ])\n        expected = xr.Dataset(\n            {\"foo\": {\"bar\": {\"x\": [0, 2], \"y\": [1, 3]}}}\n        )\n        assert actual.identical(expected)\n", "    def test_merge_dataset_update_with_missing_coords(self):\n        data = create_test_data()\n        ds1 = data[[\"var1\"]]\n        ds2 = data[[\"var3\"]].rename({\"var3\": \"var1\"})\n\n        # Ensure 'var1' exists before updating\n        ds1 = ds1.merge(ds2)\n\n        # Update with a dataset that has different coordinates\n        ds3 = xr.Dataset({\"x\": (\"x\", [0, 1, 2])})\n        with pytest.raises(ValueError):\n            ds1.update(ds3)\n\n        # Update with a dataset that has the same coordinates\n        ds4 = xr.Dataset({\"var1\": (\"x\", [0, 1, 2])})\n        ds1.update(ds4)\n        assert ds1.identical(data)\n\n\n\n", "    def test_merge_overwite_vars(self):\n        data = create_test_data()\n        ds1 = data[[\"var1\", \"var2\"]]\n        ds2 = data[[\"var2\", \"var3\"]]\n\n        # Test with a single var to override\n        expected = data[[\"var1\", \"var2\", \"var3\"]]\n        actual = ds1.merge(ds2, overwrite_vars=[\"var2\"])\n        assert expected.identical(actual)\n\n        # Test with multiple vars to override\n        actual = ds1.merge(ds2, overwrite_vars=[\"var1\", \"var2\"])\n        assert expected.identical(actual)\n\n        # Test with an invalid var name\n        with pytest.raises(ValueError):\n            ds1.merge(ds2, overwrite_vars=[\"invalid_var\"])\n\n        # Test with no vars to override\n        actual = ds1.merge(ds2)\n        expected = data[[\"var1\", \"var2\", \"var3\"]]\n        assert expected.identical(actual)\n\n        # Test with a var name that doesn't exist\n        with pytest.raises(ValueError):\n            ds1.merge(ds2, overwrite_vars=[\"non_existing_var\"])\n\n\n", "    def test_merge_coordinate_mismatch(self):\n        ds1 = xr.Dataset({\"a\": (\"x\", [1, 2]), \"x\": [0, 1]})\n        ds2 = xr.Dataset({\"b\": (\"y\", [3, 4]), \"y\": [1, 2]})\n        with raises_regex(ValueError, \"indexes .* not equal\"):\n            ds1.merge(ds2)\n        with raises_regex(ValueError, \"indexes .* not equal\"):\n            ds2.merge(ds1)\n\n\n", "    def test_merge_nested_dataarrays(self):\n        ds1 = xr.Dataset({\"a\": xr.DataArray([1, 2])})\n        ds2 = xr.Dataset({\"b\": xr.DataArray([3, 4])})\n        ds3 = xr.Dataset({\"c\": xr.DataArray([5, 6])})\n\n        merged_ds12 = ds1.merge(ds2)\n        merged_ds123 = merged_ds12.merge(ds3)\n\n        expected_ds12 = xr.Dataset({\"a\": xr.DataArray([1, 2]), \"b\": xr.DataArray([3, 4])})\n        expected_ds123 = xr.Dataset(\n            {\"a\": xr.DataArray([1, 2]), \"b\": xr.DataArray([3, 4]), \"c\": xr.DataArray([5, 6])}\n        )\n        assert expected_ds12.identical(merged_ds12)\n        assert expected_ds123.identical(merged_ds123)\n"], "sample_642": ["def test_find_default_config_files_in_git_env() -> None:\n    \"\"\"Test that we find the config file in a git repository.\"\"\"\n    with tempdir() as chroot:\n        chroot_path = Path(chroot)\n        testutils.create_files(\n            [\n                \".git/config\",\n                \".gitignore\",\n                \"pylintrc\",\n                \"a/pylintrc\",\n                \"a/b/pylintrc\",\n                \"a/b/c/d/__init__.py\",\n            ]\n        )\n        with fake_home():\n            config_files = list(config.find_default_config_files())\n            assert len(config_files) == 2\n            assert chroot_path / \"pylintrc\" in config_files\n            assert chroot_path / \"a\" / \"pylintrc\" in config_files\n", "def test_pre_process_options_callback_action() -> None:\n    with mock.patch('pylint.config.arguments.Run') as mock_run:\n        # Create a mock callback action\n        class MockCallbackAction(object):\n                self.kwargs = kwargs\n\n                assert self.kwargs == kwargs\n\n        # Set up the mock arguments\n        args = [\"--init-hook\", \"{'value': 'some code'}\"]\n        run = mock.Mock(spec=Run)\n        mock_run.return_value = run\n\n        # Call the _preprocess_options function\n        result = _preprocess_options(run, args)\n\n        # Check that the callback action was called with the correct arguments\n        assert len(result) == 0\n        assert mock_run.called with args  \n", "def test_find_default_config_files_with_relative_path() -> None:\n    \"\"\"Test that relative paths in PYLINTRC are resolved correctly.\"\"\"\n    with tempdir() as chroot:\n        chroot_path = Path(chroot)\n        testutils.create_files(\n            [\"a/b/c/d/__init__.py\", \"a/pylintrc\"]\n        )\n        os.chdir(chroot_path / \"a/b/c\")\n        with pytest.warns(DeprecationWarning):\n            for config_file in config.find_default_config_files():\n                assert config_file.relative_to(chroot_path).parts == [\"..\", \"pylintrc\"]\n", "def test_preprocessable_options() -> None:\n    \"\"\"Test that preprocessable options are correctly handled.\"\"\"\n    with tempdir() as chroot:\n        with fake_home():\n            chroot_path = Path(chroot)\n            testutils.create_files([\"a/b/c/d/__init__.py\"])\n            os.chdir(chroot_path / \"a/b/c\")\n            args = [\"--init-hook\", \"print('hello world!')\", \"--verbose\"]\n            processed_args = config._preprocess_options(None, args)\n            assert processed_args == [\"--init-hook\", \"print('hello world!')\", \"--verbose\"]\n\n            processed_args = config._preprocess_options(None, [\"--init-hook=hello\", \"--verbose\"])\n            assert processed_args == [\"--init-hook\", \"hello\", \"--verbose\"]\n", "def test_preprocess_options_no_match(\n    capsys: CaptureFixture,"], "sample_261": ["    def test_invalid_iso_8601(self):\n        self.assertIsNone(parse_duration('P4'))\n        self.assertIsNone(parse_duration('P4Y1M'))\n        self.assertIsNone(parse_duration('P4Y1M1D1T'))\n        self.assertIsNone(parse_duration('P4Y1M1D1T5H'))\n        self.assertIsNone(parse_duration('P4Y1M1D1T5H:00:00.001'))\n        self.assertIsNone(parse_duration('P4Y1M1D1T5H:00:00,001'))\n\n\n\n", "    def test_mixed_duration_formats(self):\n        test_values = (\n            ('4 days 10:15:30', timedelta(days=4, hours=10, minutes=15, seconds=30)),\n            ('10:15:30,400', timedelta(hours=10, minutes=15, seconds=30, microseconds=400000)),\n            ('10:15:30.400 P4Y', timedelta(days=1461, hours=10, minutes=15, seconds=30, microseconds=400000)),\n            ('-4 days 10:15:30', timedelta(days=-4, hours=10, minutes=15, seconds=30)),\n            ('-10:15:30', timedelta(hours=-10, minutes=-15, seconds=-30)),\n            ('-10:15:30.400', timedelta(hours=-10, minutes=-15, seconds=-30, microseconds=-400000)),  \n            ('30.1 seconds', timedelta(seconds=30, milliseconds=100)),\n            ('15:30.0001', timedelta(minutes=15, seconds=30, microseconds=100)),\n            ('-15:30.400', timedelta(minutes=-15, seconds=-30, microseconds=-400)),\n            ('P4D 15:30.400', timedelta(days=4, minutes=15, seconds=30, microseconds=400)),\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_duration(source), expected)\n", "    def test_invalid_iso_8601(self):\n        invalid_inputs = (\n            'P4Y5M',  \n            'P4D5H',\n            'P4D-15M',\n            'PT100H',\n            'PT100M',\n            'PT1000S',\n            'PT1000.1S',\n            'Pfoobar',\n            'PTfoo',\n        )\n        for source in invalid_inputs:\n            with self.subTest(source=source):\n                self.assertIsNone(parse_duration(source))\n", "    def test_invalid_iso_8601(self):\n        test_values = (\n            'P4Y2',\n            'P4Y2M',\n            'P4Y2W',\n            'P4Y2D2',\n            'P4Y2D1H',\n            'P4Y2D1H1M',\n            'P4Y2D1H1M1S.1',\n            'P4Y2D1H1M1S,1',\n            'P4Y2D1H1M1S.123',\n            'P4Y2D1H1M1S,123',\n            'PT1H2M3S4',\n            'PT1H2M3S4.0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000',\n            'PT1H2M3S4,000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000',\n        )\n        for source in test_values:\n            with self.subTest(source=source):\n                self.assertIsNone(parse_duration(source))\n", "    def test_invalid_duration_formats(self):\n        invalid_inputs = (\n            'days',\n            '1 2 3',\n            '1 day 1 hour 1 minute 1 second day',\n            '1 day 1, hour 1 minute 1 second',\n            '1 day 1:hour 1 minute 1 second',\n            '1 day 1 hour 1:minute 1 second',\n            '1 day 1 hour 1 minute 1:second',\n            '1 day 1:hour 1 minute 1 second',\n            '1 day 1 hour 1:minute:second',\n            '1 day 1:hour 1:minute 1:second',\n            '1 day 1 hour 1 minute 1.1',\n            '1 day 1 hour 1 minute 1.1.1',\n            '1 day 1 hour 1 minute 1.1111111',\n            '1 day 1 hour 1 minute 1.11111111',\n            '1 day 1 hour 1 minute 1,111',\n        )\n        for source in invalid_inputs:\n            with self.subTest(source=source):\n                self.assertIsNone(parse_duration(source))\n"], "sample_1182": ["def test_MatrixSolve():\n    from sympy.matrices.expressions.matrix_solve import MatrixSolve\n    from sympy.matrices import Matrix\n    prntr = SciPyPrinter()\n    A = Matrix([[1, 2], [3, 4]])\n    b = Matrix([5, 6])\n    sol = MatrixSolve(A, b)\n    assert prntr.doprint(sol) == 'scipy.linalg.solve(A, b)'\n\n", "compilation error", "def test_hypergeometric_functions():\n    from sympy.functions.special.hypergeometric import hyper\n\n    expr = hyper([1, 2], [3, 4], x)\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr) == 'scipy.special.hyper(1, 2, 3, 4, x)'\n\n    prntr = NumPyPrinter()\n    assert \"Not supported\" in prntr.doprint(expr)\n\n    prntr = PythonCodePrinter()\n    assert \"Not supported\" in prntr.doprint(expr)\n", "def test_MatrixSolve():\n    from sympy.integrals.integrals import Integral\n    from sympy.matrices.expressions.solve import MatrixSolve\n    A = MatrixSymbol(\"A\", 2, 2)\n    b = MatrixSymbol(\"b\", 2, 1)\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(MatrixSolve(A, b)) == \\\n        'numpy.linalg.solve(A, b)' \n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(MatrixSolve(A, b)) == 'numpy.linalg.solve(A, b)' \n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(MatrixSolve(A, b)) == 'numpy.linalg.solve(A, b)' \n", "def test_MatrixSolve():\n    from sympy.matrices import MatrixSymbol\n    from sympy.integrals.integrals import Integral\n    \n    M = MatrixSymbol(\"M\", 2, 2)\n    b = MatrixSymbol(\"b\", 2, 1)\n    \n    prntr = SciPyPrinter()\n    assert prntr.doprint(MatrixSolve(M, b)) == 'scipy.linalg.solve(M, b)'\n"], "sample_1056": ["def test_lambda_with_arguments():\n    f = lambdarepr((x**2 + y) / z)\n    assert f == \"((x**2 + y) / z)\" \n", "def test_complex():\n    z = 1 + 2j\n    assert lambdarepr(z) == \"complex(1, 2)\"\n", "def test_mpmath():\n    prntr = LambdaPrinter(settings={'standard': 'mpmath'})\n    assert prntr._print_Pow(sqrt(x), rational=False) == 'sqrt(x)'\n    assert prntr._print_Pow(sqrt(x), rational=True) == 'x**(1./2.)'\n", "compilation error", "compilation error"], "sample_696": ["def test_argument_type_str(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.parametrize(\"arg\", [\"string_type\"])\n            pass\n\n        @pytest.mark.parametrize(\"arg\", [1])\n            pass\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*The `type` argument to addoption() is the string 'str'.*\",\n            \"*The `type` argument to addoption() is the string 'int'.*\",\n        ]\n    )\n", "def test_argument_percent_default(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.parametrize(\"x\", [\"test\"], indirect=True)\n            assert x == \"test\"\n            assert request.config.parse_args('\"%default\"') == \"test\"\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines(\n        [\"*pytest now uses argparse. \\\"\\\\%default\\\" should be changed to \\\"\\\\%(default)s\\\"*\"]\n    )\n", "def test_argument_percent_default(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import argparse\n        parser = argparse.ArgumentParser()\n        parser.add_argument('-p', '--param', type=str, default='%default')\n        args = parser.parse_args()\n        print(args.param)\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines(\n        [\"*pytest.PytestDeprecationWarning: pytest now uses argparse.*\"])\n", "def test_argument_percent_default(pytester: Pytester) -> None:\n    pytester.makepyfile(\"\"\"\n            assert arg == \"%(default)s\"\n    \"\"\")\n    result = pytester.runpytest(\"-v\", \"--parse-args=['arg=%default']\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*pytest._parse_args: PytestDeprecationWarning:*\n        ]\n    )\n    \n", "def test_argument_percent_default(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import argparse\n            print(f'arg1: {arg1}, arg2: {arg2}')\n        \"\"\"\n    )\n    result = pytester.runpytest(\n        \"--arg1\", \"value1\", \"--arg2\", \"%default\"\n    )\n    result.stdout.fnmatch_lines(\n        [\n            \"arg1: value1, arg2: default\",\n        ]\n    )\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestDeprecationWarning: pytest now uses argparse. \\\"\\\\%default\\\" should be changed to \\\"\\\\%(default)s\\\"*\",\n        ]\n    )\n"], "sample_295": ["    def test_output_field_with_none(self):\n        expr = ExpressionWrapper(Value(1), output_field=None)\n        self.assertEqual(expr.output_field, None)", "    def test_field_output_field_preservation(self):\n        value = Value('foo')\n        value.output_field = CharField()\n        expr = ExpressionWrapper(value, output_field=CharField(max_length=10))\n        self.assertEqual(expr.output_field, CharField(max_length=10))\n\n\n\n", "    def test_get_group_by_cols_with_subquery(self):\n        from django.db.models.functions import Subquery\n        subquery = Subquery(\n            Experiment.objects.values('pk').filter(name='e0')\n        ).alias('subquery_pk')\n        expr = ExpressionWrapper(subquery, output_field=IntegerField())\n        group_by_cols = expr.get_group_by_cols(alias=None)\n        self.assertEqual(group_by_cols, [expr.expression])\n        self.assertEqual(group_by_cols[0].output_field, expr.output_field)\n", "    def test_resolve_output_field_with_nested_expressions(self):\n        tests = [\n            ((IntegerField, IntegerField, IntegerField),\n             (Combinable.ADD, IntegerField),\n             [IntegerField]),\n            ((IntegerField, IntegerField, IntegerField),\n             (Combinable.SUB, IntegerField),\n             [IntegerField]),\n            ((IntegerField, IntegerField, IntegerField),\n             (Combinable.MUL, IntegerField),\n             [IntegerField]),\n            ((IntegerField, IntegerField, IntegerField),\n             (Combinable.DIV, IntegerField),\n             [IntegerField]),\n            ((CharField, CharField, CharField),\n             (Combinable.ADD, CharField),\n             [CharField]),\n        ]\n        for lhs, connector, expected_output_field in tests:\n            with self.subTest(lhs=lhs, connector=connector, expected_output_field=expected_output_field):\n                expr1 = ExpressionWrapper(F('field1'), output_field=lhs[0])\n                expr2 = ExpressionWrapper(F('field2'), output_field=lhs[1])\n                expr3 = CombinedExpression(expr1, connector, expr2)\n                expr = ExpressionWrapper(expr3, output_field=expected_output_field)\n                self.assertIsInstance(expr.output_field, expected_output_field)\n\n\n\n", "    def test_output_field_resolution(self):\n        value1 = Value('foo', output_field=CharField())\n        value2 = Value('bar', output_field=TextField())\n        expr = CombinedExpression(value1, Combinable.ADD, value2)\n        with self.assertRaises(FieldDoesNotExist):\n            expr.output_field  # This should raise an error\n\n\n"], "sample_399": ["    def test_aggregation_default_with_filter_on_related_field(self):\n        expr = Max(\n            \"contact__age\",\n            filter=Q(contact__age__gt=25),\n            default=None,\n        )\n        queryset = Book.objects.annotate(highest_contact_age=expr).order_by(\"isbn\")\n        self.assertSequenceEqual(\n            queryset.values(\"isbn\", \"highest_contact_age\"),\n            [\n                {\"isbn\": \"013235613\", \"highest_contact_age\": 45},\n                {\"isbn\": \"013790395\", \"highest_contact_age\": 39},\n                {\"isbn\": \"067232959\", \"highest_contact_age\": None},\n                {\"isbn\": \"155860191\", \"highest_contact_age\": 45},\n                {\"isbn\": \"159059725\", \"highest_contact_age\": 39},\n                {\"isbn\": \"159059996\", \"highest_contact_age\": 45},\n            ],\n        )\n\n", "    def test_aggregation_default_with_distinct(self):\n        # Test default value with DISTINCT\n        result = Publisher.objects.filter(\n            name__in=[\"Apress\", \"Morgan Kaufmann\"],\n        ).annotate(\n            avg_rating=Avg(\"book__rating\"),\n        ).values(\"name\", \"avg_rating\").annotate(\n            count=Count(\"book\", distinct=True)\n        )\n        self.assertSequenceEqual(\n            result,\n            [\n                {\"name\": \"Apress\", \"avg_rating\": 4.3, \"count\": 1},\n                {\"name\": \"Morgan Kaufmann\", \"avg_rating\": 8.0, \"count\": 1},\n            ],\n        )\n\n", "    def test_aggregation_with_group_by_and_having(self):\n        authors = Author.objects.annotate(\n            book_count=Count(\"book\"),\n            avg_age=Avg(\"age\"),\n        ).filter(avg_age__gt=30).values(\"name\", \"book_count\", \"avg_age\").order_by(\"name\")\n        self.assertSequenceEqual(\n            list(authors),\n            [\n                {\"name\": \"Peter Norvig\", \"book_count\": 2, \"avg_age\": 35.0},\n            ],\n        )\n\n", "    def test_aggregation_with_subquery_and_outerref(self):\n        subquery = (\n            Book.objects.filter(publisher__name=\"Apress\")\n            .values(\"pk\", \"rating\")\n            .annotate(\n                average_rating=Avg(\"rating\"),\n            )\n        )\n        publisher_avg_rating = Publisher.objects.annotate(\n            avg_apress_book_rating=Subquery(subquery),\n        ).values(\"name\", \"avg_apress_book_rating\")\n        self.assertSequenceEqual(\n            publisher_avg_rating,\n            [\n                {\"name\": \"Apress\", \"avg_apress_book_rating\": 4.5},\n                {\"name\": \"Jonno's House of Books\", \"avg_apress_book_rating\": None},\n                {\"name\": \"Morgan Kaufmann\", \"avg_apress_book_rating\": None},\n                {\"name\": \"Prentice Hall\", \"avg_apress_book_rating\": None},\n                {\"name\": \"Sams\", \"avg_apress_book_rating\": None},\n            ],\n        )\n\n", "    def test_aggregation_exists_with_subquery_using_django_alias(self):\n        subquery_qs = (\n            Author.objects.filter(name=\"Adrian Holovaty\")\n            .values(\"id\")\n            .annotate(alias=F(\"id\"))\n        )\n        result = Book.objects.annotate(\n            exists_with_alias=Exists(subquery_qs.annotate(alias_id=F(\"id\"))),\n        )\n        self.assertEqual(result.count(), 1)\n\n\n\n"], "sample_1164": ["compilation error", "compilation error", "compilation error", "def test_bra_ket():\n    x = symbols('x')\n    t = symbols('t')\n    psi = Function('psi')\n    ket = Ket(Symbol('x'))\n    bra = Bra(Symbol('x'))\n    tket = TimeDepKet(Symbol('psi'),Symbol('t'))\n    tbra = TimeDepBra(Symbol('psi'),Symbol('t'))\n    assert str(ket) == '|x>'\n    assert pretty(ket) == '|x>'\n    assert upretty(ket) == '\u2758x\u27e9'\n    assert latex(ket) == r'{\\left|x\\right\\rangle }'\n    sT(ket, \"Ket(Symbol('x'))\")\n    assert str(bra) == '<x|'\n    assert pretty(bra) == '<x|'\n    assert upretty(bra) == '\u27e8x\u2758'\n    assert latex(bra) == r'{\\left\\langle x\\right|}'\n    sT(bra, \"Bra(Symbol('x'))\")\n    assert str(tket) == '|\u03c8;t>'\n    assert pretty(tket) == '|\u03c8;t>'\n    assert upretty(tket) == '\u2758\u03c8;t\u27e9'\n    assert latex(tket) == r'{\\left|\\psi;t\\right\\rangle }'\n    sT(tket, \"TimeDepKet(Symbol('\u03c8'),Symbol('t'))\")\n    assert str(tbra) == '<\u03c8;t|'\n    assert pretty(tbra) == '<\u03c8;t|'\n    assert upretty(tbra) == '\u27e8\u03c8;t\u2758'\n    assert latex(tbra) == r'{\\left\\langle \\psi;t\\right|}'\n    sT(tbra, \"TimeDepBra(Symbol('\u03c8'),Symbol('t'))\")\n    assert str(bra * ket) == '<x|x>'\n    ascii_str = \\", "compilation error"], "sample_571": ["    def test_regplot_multiple_variables(self):\n\n        df2 = pd.DataFrame(dict(\n            x=self.df.x,\n            y=self.df.y,\n            z=self.df.y + self.rs.randn(60)\n        ))\n        f, ax = plt.subplots()\n        lm.regplot(x='x', y='y', data=df2, ax=ax)\n        lm.regplot(x='x', y='z', data=df2, ax=ax, color='r')\n        assert len(ax.lines) == 2\n", "    def test_lmplot_robust(self):\n\n        data = pd.DataFrame(dict(\n            x=np.random.randn(100),\n            y=np.random.randn(100) + 5,\n            z=np.random.randn(100),  # Some outlier data point\n        ))\n\n        g = lm.lmplot(x=\"x\", y=\"y\", data=data, robust=True)\n        ax = g.axes[0, 0]\n\n        # Check if the regression line is less affected by outliers\n        # by comparing slopes/intercepts of robust vs non-robust fitting\n        robust_slope, robust_intercept = np.polyfit(data[\"x\"], data[\"y\"], 1, w=data[\"z\"])\n        nonrobust_slope, nonrobust_intercept = np.polyfit(data[\"x\"], data[\"y\"], 1)\n\n        assert abs(robust_slope - nonrobust_slope) > 1e-3\n        assert abs(robust_intercept - nonrobust_intercept) > 1e-3\n\n\n", "    def test_regplot_ylim(self):\n\n        f, ax = plt.subplots()\n        x, y1, y2 = np.random.randn(3, 50)\n        lm.regplot(x=x, y=y1, truncate=False)\n        lm.regplot(x=x, y=y2, truncate=False)\n        line1, line2 = ax.lines\n        assert np.array_equal(line1.get_ydata(), line2.get_ydata())\n", "    def test_regplot_legend(self):\n\n        f, ax = plt.subplots()\n        lm.regplot(x=\"x\", y=\"y\", data=self.df, hue=\"h\")\n        legend = ax.legend()\n        assert len(legend.get_texts()) == 3\n        assert len(legend.get_lines()) == 3\n\n\n\n", "    def test_regplot_ylim(self):\n\n        f, ax = plt.subplots()\n        x, y1, y2 = np.random.randn(3, 50)\n        lm.regplot(x=x, y=y1, truncate=False)\n        lm.regplot(x=x, y=y2, truncate=False)\n        line1, line2 = ax.lines\n        assert np.array_equal(line1.get_ydata(), line2.get_ydata())\n"], "sample_1191": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_424": ["    def test_references_field_by_through_model(self):\n        operation = FieldOperation(\n            \"Model\",\n            \"field\",\n            models.ManyToManyField(\n                \"Other\", through=\"Through\", through_fields=(\"first\", \"second\")\n            ),\n        )\n        self.assertIs(\n            operation.references_model(\"Through\", \"migrations\"), True\n        )\n\n\n\n        ", "    def test_references_field_by_related_name(self):\n        operation = FieldOperation(\n            \"Model\",\n            \"field\",\n            models.ForeignKey(\"Other\", models.CASCADE, related_name=\"related_name\"),\n        )\n        self.assertIs(operation.references_field(\"Other\", \"related_name\", \"migrations\"), True)\n        self.assertIs(operation.references_field(\"Other\", \"whatever\", \"migrations\"), False)\n\n\n\n", "    def test_references_field_by_reverse_related_name(self):\n        operation = FieldOperation(\n            \"Model\",\n            \"field\",\n            models.ForeignKey(\n                \"Other\", models.CASCADE, related_name=\"related_field\"\n            ),\n        )\n        self.assertIs(\n            operation.references_field(\"Other\", \"related_field\", \"migrations\"),\n            True,\n        )\n\n\n", "    def test_references_field_by_related_name(self):\n        operation = FieldOperation(\n            \"Model\", \"field\", models.ForeignKey(\"Other\", models.CASCADE, related_name=\"related_to_models\")\n        )\n        self.assertIs(operation.references_field(\"Other\", \"related_to_models\", \"migrations\"), True)\n        self.assertIs(operation.references_field(\"Other\", \"whatever\", \"migrations\"), False)\n\n\n\n", "    def test_references_field_by_model_and_field(self):\n        operation = FieldOperation(\n            \"Model\", \"field\", models.ForeignKey(\"Other\", models.CASCADE)\n        )\n        self.assertIs(\n            operation.references_field_by_model_and_field(\"Other\", \"field\"), True\n        )\n        self.assertIs(operation.references_field_by_model_and_field(\"Missing\", \"field\"), False)\n        self.assertIs(operation.references_field_by_model_and_field(\"Other\", \"whatever\"), False)\n"], "sample_14": ["def test_angle_with_invalid_units():\n    with pytest.raises(ValueError, match=\"Invalid unit.*\"):\n        Angle(1, 'invalid_unit')\n", "def test_angle_array_wrap_at(input):\n    \"\"\"\n    Tests that wrap_at works correctly with numpy arrays.\n    \"\"\"\n    a = Angle(input)\n    a.wrap_at(180 * u.deg)\n    assert np.all(a.value >= -180)\n", "def test_angle_radians_to_standard_units():\n    a = Angle(np.pi / 2, u.rad)\n    assert a.to(u.deg).value == 90.0\n    assert a.to(u.hourangle).value == 6.0\n    assert a.to(u.arcminute).value == 90.0 * 60.0\n\n\n\n", "def test_angle_invalid_format_string():\n    with pytest.raises(ValueError, match=r\"Invalid angle format\"):\n        Angle('10d20m')\n    with pytest.raises(ValueError, match=r\"Invalid angle format\"):\n        Angle('10.23d')\n    with pytest.raises(ValueError, match=r\"Invalid angle format\"):\n        Angle('10d20m30abc')\n\n\n\n", "def test_angle_with_invalid_unit():\n    with pytest.raises(ValueError):\n        Angle(1, u.meter)\n"], "sample_89": ["    def test_should_stop_based_on_mtime_diff(self):\n        with mock.patch.object(os, 'stat') as mocked_stat:\n            # Mock initial stat information\n            mocked_stat.return_value = mock.MagicMock(st_mtime=1599014742)\n            self.reloader.last_mtime = 1599014742\n            self.reloader.should_stop = False\n            self.assertFalse(self.reloader.should_stop_based_on_mtime_diff())\n\n            # Mock updated stat information with a different mtime\n            mocked_stat.return_value = mock.MagicMock(st_mtime=1599014743)\n            self.reloader.last_mtime = 1599014742\n            self.assertTrue(self.reloader.should_stop_based_on_mtime_diff())\n\n            # Test cases where mtime is the same\n            mocked_stat.return_value = mock.MagicMock(st_mtime=1599014742)\n            self.reloader.last_mtime = 1599014742\n            self.assertFalse(self.reloader.should_stop_based_on_mtime_diff())\n\n            # Test cases where mtime is in the future (using a timestamp from the future)\n            mocked_stat.return_value = mock.MagicMock(st_mtime=1599014742)\n            self.reloader.last_mtime = 1599014741\n            self.assertTrue(self.reloader.should_stop_based_on_mtime_diff())\n\n", "    def test_watch_pattern_with_recursive_glob(self):\n        inner_py_file = self.ensure_file(self.tempdir / 'test' / 'test.py')\n        self.reloader.watch_dir(self.tempdir, '**/*.py')\n        watched_files = list(self.reloader.watched_files())\n        self.assertIn(self.existing_file, watched_files)\n        self.assertIn(inner_py_file, watched_files)\n", "    def test_should_stop_return_true_when_no_changes_and_no_errors(self):\n        with mock.patch.object(os, 'stat') as mocked_stat, mock.patch.object(autoreload, '_error_files') as mocked_error_files:\n            mocked_stat.return_value = (\n                0, os.stat(str(self.existing_file)), os.fstat(int(1))).st_mtime\n            mocked_error_files.return_value = ()\n            self.assertTrue(self.reloader.should_stop())\n\n\n\n", "    def test_snapshot_files_with_glob(self):\n        self.reloader.watch_dir(self.tempdir, '*.py')\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file, self.nonexistent_file]):\n            snapshot = list(self.reloader.snapshot_files())\n            self.assertEqual(len(snapshot), 1)\n            self.assertEqual(snapshot[0][0], self.existing_file)\n", "    def test_has_changed_files_returns_true_when_file_modified_since_last_check(self):\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file]):\n            self.reloader.snapshot_files()\n            self.increment_mtime(self.existing_file)\n            self.assertTrue(self.reloader.has_changed_files())\n\n\n\n"], "sample_479": ["    def test_alter_index_rename_index(self):\n        self.assertOptimizesTo(\n            [\n                migrations.AlterField(\n                    \"Pony\",\n                    \"name\",\n                    models.CharField(max_length=255),\n                ),\n                migrations.AlterIndex(\n                    \"Pony\",\n                    new_fields=(\"height\", \"name\"),\n                    old_name=\"idx_pony_weight_pink\",\n                    old_fields=(\"weight\", \"pink\"),\n                ),\n                migrations.RenameIndex(\n                    \"Pony\",\n                    new_name=\"new_name\",\n                    old_name=\"idx_pony_height_name\",\n                ),\n            ],\n            [\n                migrations.AlterField(\n                    \"Pony\",\n                    \"name\",\n                    models.CharField(max_length=255),\n                ),\n                migrations.RenameIndex(\n                    \"Pony\",\n                    new_name=\"new_name\",\n                    old_fields=(\"height\", \"name\"),\n                ),\n            ],\n        )\n\n", "    def test_rename_field_with_unique_index(self):\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel(\n                    \"Foo\",\n                    fields=[\n                        (\"name\", models.CharField(max_length=255)),\n                        (\"age\", models.IntegerField()),\n                    ],\n                ),\n                migrations.AddUniqueConstraint(\"Foo\", [\"name\", \"age\"]),\n                migrations.RenameField(\"Foo\", \"name\", \"title\"),\n            ],\n            [\n                migrations.CreateModel(\n                    \"Foo\",\n                    fields=[\n                        (\"title\", models.CharField(max_length=255)),\n                        (\"age\", models.IntegerField()),\n                    ],\n                    options={\"unique_together\": [(\"title\", \"age\")]},\n                ),\n            ],\n        )\n\n\n\n", "    def test_alter_unique_together(self):\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel(\n                    \"Foo\", [(\"name\", models.CharField(max_length=255))]\n                ),\n                migrations.AddUniqueConstraint(\"Foo\", [\"name\", \"id\"]),\n                migrations.AlterUniqueTogether(\"Foo\", [(\"name\", \"id\")]),\n            ],\n            [\n                migrations.CreateModel(\n                    \"Foo\", [(\"name\", models.CharField(max_length=255))]\n                ),\n                migrations.UniqueConstraint(\n                    \"Foo\", [\"name\", \"id\"], name=\"unique_foo_name_id\"\n                ),\n            ],\n        )\n\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel(\n                    \"Foo\", [(\"name\", models.CharField(max_length=255))]\n                ),\n                migrations.AddUniqueConstraint(\"Foo\", [\"name\", \"id\"]),\n                migrations.AlterUniqueTogether(\n                    \"Foo\", [\"name\", \"id\"]\n                ),\n                migrations.AlterUniqueTogether(\"Foo\", [(\"name\", \"age\")]),\n            ],\n            [\n                migrations.CreateModel(\n                    \"Foo\", [(\"name\", models.CharField(max_length=255))]\n                ),\n                migrations.UniqueConstraint(\n                    \"Foo\", [\"name\", \"id\"], name=\"unique_foo_name_id\"\n                ),\n                migrations.UniqueConstraint(\n                    \"Foo\", [\"name\", \"age\"], name=\"unique_foo_name_age\"\n                ),\n            ],\n        )\n", "    def test_add_remove_index_with_alter(self):\n        self.assertOptimizesTo(\n            [\n                migrations.AddIndex(\n                    \"Pony\",\n                    models.Index(\n                        fields=[\"weight\", \"pink\"],\n                        name=\"idx_pony_weight_pink\",\n                    ),\n                ),\n                migrations.AlterIndex(\n                    \"Pony\",\n                    name=\"idx_pony_weight_pink\",\n                    fields=[\"weight\", \"color\"],\n                ),\n                migrations.RemoveIndex(\"Pony\", \"idx_pony_weight_pink\"),\n            ],\n            [\n                migrations.AddIndex(\n                    \"Pony\",\n                    models.Index(\n                        fields=[\"weight\", \"color\"],\n                        name=\"idx_pony_weight_pink\",\n                    ),\n                ),\n            ],\n        )\n", "    def test_rename_unique(self):\n        self.assertOptimizesTo(\n            [\n                migrations.RenameUniqueConstraint(\n                    \"Pony\", from_name=\"unique_name\", to_name=\"new_name\", columns=(\"weight\",)\n                ),\n                migrations.RenameUniqueConstraint(\n                    \"Pony\", from_name=\"new_name\", to_name=\"another_new_name\", columns=(\"weight\",)\n                ),\n            ],\n            [\n                migrations.RenameUniqueConstraint(\n                    \"Pony\", from_name=\"unique_name\", to_name=\"another_new_name\", columns=(\"weight\",)\n                ),\n            ],\n        )\n\n\n\n"], "sample_995": ["def test_issue_10835():\n    assert Float(S(1e100))._evalf(50) == float(S(1e100).evalf(50))\n    assert Float(S(1e-100))._evalf(50) == float(S(1e-100).evalf(50))\n\n\n\n", "def test_is_integer_and_is_rational():\n    assert Integer(1).is_integer is True\n    assert Integer(1).is_rational is True\n    assert Float(1).is_integer is False\n    assert Float(1).is_rational is True\n    assert Rational(1).is_integer is True\n    assert Rational(1).is_rational is True\n    assert oo.is_integer is False\n    assert oo.is_rational is False\n    assert -oo.is_integer is False\n    assert -oo.is_rational is False\n    assert zoo.is_integer is False\n    assert zoo.is_rational is False\n    assert nan.is_integer is False\n    assert nan.is_rational is False\n", "def test_issue_10689():\n    assert Float('0.12345678901234567890123456789012', 30) == Float('0.1234567890123456789012345678901234567890', 30)\n", "def test_issue_10699():\n    assert Float(Symbol('x')**2).n(5) == Float(x**2).n(5)\n\n\n", "compilation error"], "sample_593": ["def test_summarize_multiindex_level_detail(multiindex):\n    idx = multiindex.x.variable.to_index_variable()\n    coords = multiindex.coords\n    \n    level_details = fh._summarize_coord_multiindex(\"foo\", idx)\n    for lname in idx.level_names:\n        var = idx.get_level_variable(lname)\n        summary = fh.summarize_variable(lname, var, is_index=False)\n        assert summary in level_details\n\n\n\n", "def test_summarize_attrs_empty():\n    assert fh.summarize_attrs({}) == \"\"\n", "def test_summarize_attrs_empty_dict(dataset):\n    attrs = {}\n    formatted = fh.summarize_attrs(attrs)\n    assert formatted == \"\"\n", "def test_summarize_variable_is_index():\n    var = xr.DataArray(np.random.randn(2, 3), dims=(\"x\", \"y\"), name=\"test_var\")\n    var_as_index = var.set_index(\"x\")\n    formatted = fh.summarize_variable(\"test_var\", var_as_index)\n    assert formatted.count(\"class='xr-has-index'\") == 1\n    assert \"x:\" in formatted\n    assert \"MultiIndex\" not in formatted\n\n\n\n", "def test_repr_of_dataset_with_unnamed_coords(dataset):\n    ds = dataset.rename({\"time\": \"temp\"})\n    formatted = fh.dataset_repr(ds)\n    assert \"temp\" in formatted\n"], "sample_999": ["def test_MatrixSymbol_printing():\n    A = MatrixSymbol(\"A\", 2, 2)\n    assert latex(A) == r\"\\mathbf{A}\"\n\n\n\n", "def test_issue_14866():\n    from sympy.matrices import Matrix, Eye\n    A = Matrix([[1, 2], [3, 4]])\n    B = Eye(2)\n    assert latex(A*B) == r\"\\begin{bmatrix}1 & 2 \\\\ 3 & 4\\end{bmatrix} \\begin{bmatrix}1 & 0 \\\\ 0 & 1\\end{bmatrix} = \\begin{bmatrix}1 & 2 \\\\ 3 & 4\\end{bmatrix}\"\n", "def test_Differential():\n    from sympy.diffgeom.rn import R2\n    from sympy.diffgeom import Differential\n    d = Differential(R2.x)\n    assert latex(d) == r\"\\mathrm{d}x\"\n    ddx = Differential(R2.x, 2)\n    assert latex(ddx) == r\"\\mathrm{d}^{2}x\"\n    dy = Differential(R2.y)\n    assert latex(dy) == r\"\\mathrm{d}y\"\n    ddxy = Differential(R2.x, 2, R2.y)\n    assert latex(ddxy) == r\"\\mathrm{d}^{2}x \\wedge \\mathrm{d}y\"\n", "def test_TensorProduct_commutative():\n    from sympy.tensor.functions import TensorProduct\n    A = MatrixSymbol(\"A\", 3, 3)\n    B = MatrixSymbol(\"B\", 3, 3)\n    assert latex(TensorProduct(A, B)) == latex(TensorProduct(B, A)) == r\"A \\otimes B\"\n", "def test_issue_13998():\n    from sympy.tensor import tensor\n    A = tensor('A', (2, 3))\n    B = tensor('B', (3, 1))\n    assert latex(A * B) == r\"A_{i}^{a} B^{a}_{j}\"\n\n\n\n"], "sample_459": ["    def test_integerfield_cleans_float_to_int(self):\n        f = models.IntegerField()\n        self.assertEqual(f.clean(1.0, None), 1)\n        self.assertEqual(f.clean(1.5, None), 1)\n        self.assertEqual(f.clean(-1.0, None), -1)\n\n", "    def test_integerfield_cleans_int_with_leading_and_trailing_whitespace(self):\n        f = models.IntegerField()\n        self.assertEqual(f.clean(\"   2   \", None), 2)\n", "    def test_integerfield_cleans_int_with_leading_and_trailing_whitespace(self):\n        f = models.IntegerField()\n        self.assertEqual(f.clean(\" 2 \", None), 2)\n", "    def test_integerfield_cleans_with_custom_validator(self):\n        class CustomValidator:\n                self.min_value = min_value\n                self.max_value = max_value\n\n                if value < self.min_value:\n                    raise ValidationError(\n                        f\"Value must be greater than or equal to {self.min_value}.\"\n                    )\n                if value > self.max_value:\n                    raise ValidationError(\n                        f\"Value must be less than or equal to {self.max_value}.\"\n                    )\n                return value\n\n        f = models.IntegerField(validators=[CustomValidator(1, 5)])\n        self.assertEqual(f.clean(3, None), 3)\n        with self.assertRaises(ValidationError):\n            f.clean(0, None)\n        with self.assertRaises(ValidationError):\n            f.clean(6, None)\n\n\n\n", "    def test_enum_choices_cleans_valid_int(self):\n        f = models.IntegerField(choices=self.Choices.choices)\n        self.assertEqual(f.clean(1, None), 1)\n\n\n"], "sample_827": ["def test_assign_rows_csr_with_copy():\n    X = np.array([[0, 3, 0],\n                  [2, 4, 0],\n                  [0, 0, 0],\n                  [9, 8, 7],\n                  [4, 0, 5]], dtype=np.float64)\n    X_csr = sp.csr_matrix(X)\n    out = np.ones_like(X)\n    out_rows = np.array([0, 2, 3], dtype=np.intp)\n\n    assign_rows_csr(X_csr, out_rows, out_rows, out, copy=True)\n    assert_array_equal(out, X_csr.toarray())\n    assert_array_almost_equal(X_csr.toarray(), X.astype(np.float64))\n\n", "def test_row_sum_sparse():\n    X = np.array([[0, 3, 0],\n                  [2, -1, 0],\n                  [0, 0, 0],\n                  [9, 8, 7],\n                  [4, 0, 5]], dtype=np.float64)\n    X_csr = sp.csr_matrix(X)\n    X_csc = sp.csc_matrix(X)\n    assert_array_almost_equal(row_sum(X_csr), X.sum(axis=1))\n    assert_array_almost_equal(row_sum(X_csc), X.sum(axis=1))\n    assert_raises(TypeError, row_sum, X_csr.tolil())\n    assert_raises(ValueError, row_sum, X_csr, axis=1)\n", "def test_row_median_axis1():\n    # Test csc_row_median for axis=1\n    # Test that it gives the same output when X is dense.\n    rng = np.random.RandomState(0)\n    X = rng.rand(50, 100)\n    dense_median = np.median(X, axis=1)\n    csc = sp.csc_matrix(X)\n    sparse_median = csc_median_axis_1(csc)\n    assert_array_almost_equal(sparse_median, dense_median)\n\n    # Test that it gives the same output when X is sparse\n    X = rng.rand(51, 100)\n    X[X < 0.7] = 0.0\n    ind = rng.randint(0, 100, 10)\n    X[ind] = -X[ind]\n    csc = sp.csc_matrix(X)\n    dense_median = np.median(X, axis=1)\n    sparse_median = csc_median_axis_1(csc)\n    assert_array_almost_equal(sparse_median, dense_median)\n\n\n\n   # Test for toy data.\n    X = [[0, -2], [-1, -1], [1, 0], [2, 1]]\n    csc = sp.csc_matrix(X)\n    assert_array_equal(csc_median_axis_1(csc), np.array([0.5, -0.5, 0., 1.]))\n    X = [[0, -2], [-1, -5], [1, -3]]\n    csc = sp.csc_matrix(X)\n    assert_array_equal(csc_median_axis_1(csc), np.array([0., -3.]))\n\n\n    # Test that it raises an Error for non-csc matrices.\n    assert_raises(TypeError, csc_median_axis_1, sp.csr_matrix(X))\n", "def test_inplace_column_scale_sparse_with_sample_weight():\n    rng = np.random.RandomState(0)\n    X = sp.rand(100, 200, 0.05)\n    Xr = X.tocsr()\n    Xc = X.tocsc()\n    XA = X.toarray()\n    scale = rng.rand(200)\n    XA *= scale.reshape(-1, 1)\n    sample_weight = rng.rand(100)\n\n    inplace_column_scale(Xc, scale, sample_weight)\n    inplace_column_scale(Xr, scale, sample_weight)\n    assert_array_almost_equal(Xr.toarray(), Xc.toarray())\n    assert_array_almost_equal(XA, Xc.toarray())\n    assert_array_almost_equal(XA, Xr.toarray())\n    assert_raises(TypeError, inplace_column_scale, X.tolil(), scale)\n\n    X = X.astype(np.float32)\n    scale = scale.astype(np.float32)\n    sample_weight = sample_weight.astype(np.float32)\n    Xr = X.tocsr()\n    Xc = X.tocsc()\n    XA = X.toarray()\n    XA *= scale.reshape(-1, 1)\n    inplace_column_scale(Xc, scale, sample_weight)\n    inplace_column_scale(Xr, scale, sample_weight)\n    assert_array_almost_equal(Xr.toarray(), Xc.toarray())\n    assert_array_almost_equal(XA, Xc.toarray())\n    assert_array_almost_equal(XA, Xr.toarray())\n    assert_raises(TypeError, inplace_column_scale, X.tolil(), scale)\n", "def test_csc_row_median_axis_1():\n    # Test csc_row_median_axis_1.\n    # Test that it gives the same output when X is dense.\n    rng = np.random.RandomState(0)\n    X = rng.rand(10, 50)\n    dense_median = np.median(X, axis=1)\n    csc = sp.csc_matrix(X)\n    sparse_median = csc_median_axis_1(csc)\n    assert_array_almost_equal(sparse_median, dense_median)\n\n    # Test that it gives the same output when X is sparse\n    X = rng.rand(51, 100)\n    X[X < 0.7] = 0.0\n    ind = rng.randint(0, 50, 10)\n    X[ind] = -X[ind]\n    csc = sp.csc_matrix(X)\n    dense_median = np.median(X, axis=1)\n    sparse_median = csc_median_axis_1(csc)\n    assert_array_almost_equal(sparse_median, dense_median)\n\n    # Test for toy data.\n    X = [[[0, -2], [-1, -1]], [[1, 0], [2, 1]]]\n    csc = sp.csc_matrix(X)\n    assert_array_equal(csc_median_axis_1(csc), np.array([0.5, -0.5]))\n    X = [[[0, -2], [-1, -5]], [[1, -3]]]\n    csc = sp.csc_matrix(X)\n    assert_array_equal(csc_median_axis_1(csc), np.array([0., -3]))\n\n    # Test that it raises an Error for non-csc matrices.\n    assert_raises(TypeError, csc_median_axis_1, sp.csr_matrix(X))\n\n\n"], "sample_976": ["    def test_cache():\n        x, y, z = symbols('x y z')\n        f = lambda x: x**2\n        g = lambda x: x + 1\n        assert (f(x) == x**2)\n        assert (g(x) == x + 1)\n\n        with CacheClearer() as c:\n            assert (f(x) == x**2)\n            assert (g(x) == x + 1)\n            assert (f(x) == x**2)\n            assert (g(x) == x + 1)\n\n        a = sympify('sqrt(x)')\n        with CacheClearer() as c:\n            assert (a == sympify('sqrt(x)'))\n            assert (a == sympify('sqrt(x)'))\n\n", "def test_symbols_whitespace():\n    x = Symbol('x')\n    y = Symbol('y')\n    z = Symbol('z')\n\n    assert symbols(' x  , y  , z ') == (x, y, z)\n    assert symbols(' x,y, z ') == (x, y, z)\n    assert symbols(' x\\ty\\tz ') == (x, y, z)\n    assert symbols(' \\t x, y, z \\t ') == (x, y, z)\n\n\n\n", "def test_complex():\n    x = Symbol('x')\n    y = Symbol('y')\n    z = Symbol('z')\n\n    assert isinstance(x + y + z, Add)\n    assert isinstance(x - y - z, Subtract)\n    assert isinstance(x * y * z, Mul)\n    assert isinstance(x / y / z, Pow)\n    assert isinstance(x ** y ** z, Pow)\n\n\n", "compilation error", "def test_replace():\n    x, y = symbols('x,y')\n    f = Function('f')\n\n    expr = f(x + y)\n\n    assert expr.replace(x, y) == f(2*y)\n    assert expr.replace(y, x) == f(x + 2*x)\n    assert expr.replace(x, 2*x) == f(2*x + y)\n    assert expr.replace(y, 2*y) == f(x + 2*y)\n\n    assert f(x + y).replace(f, Function('g')) == g(x + y)\n\n\n"], "sample_1118": ["def test_matpow_power_simplification():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatPow(A, 2)\n    assert B.doit() == MatMul(A, A)\n  \n    C = MatPow(MatPow(A, 3), 2)\n    assert C.doit() == MatPow(A, 6)\n    \n    D = MatPow(A, -1)\n    assert D.doit() == Inverse(A)\n\n\n\n", "def test_matpow_properties():\n    A = MatrixSymbol('A', 2, 2)\n    raises(TypeError, lambda: MatPow(5, A))\n    raises(TypeError, lambda: MatPow(A, 'b'))\n    assert MatPow(A, 0).doit() == Identity(A.shape[0])\n    assert MatPow(A, 1).doit() == A\n    assert MatPow(A, 2).doit() == A*A\n    assert MatPow(A, -1).doit() == Inverse(A)\n\n    B = MatrixSymbol('B', 2, 2)\n    assert MatPow(A, 2).base == A\n    assert MatPow(A, 2).exp == 2\n    assert MatPow(A, 2).shape == (2, 2)\n", "    def test_matpow_simplify():\n        A = MatrixSymbol('A', 2, 2)\n        assert (MatPow(A, 0)).doit() == Identity(2)\n        assert (MatPow(A, -1)).doit() == Inverse(A)\n        assert (MatPow(A, 2)).doit() == A*A\n        assert (MatPow(A, 3)).doit() == A*A*A\n        assert (MatPow(A*B, 2)).doit() == (A*B)*(A*B)\n        assert (MatPow(MatPow(A, 2), 3)).doit() == MatPow(A, 6)\n", "def test_matpow_zero_exponent():\n    A = MatrixSymbol('A', 3, 3)\n    assert MatPow(A, 0).doit() == Identity(3)\n    assert (A**0).doit() == Identity(3)\n\n\n", "def test_matpow_negative_exponent():\n    A = MatrixSymbol('A', 2, 2)\n    assert MatPow(A, -1).doit() == Inverse(A)\n    assert MatPow(A, -2).doit() == Inverse(A) * Inverse(A)\n    assert MatPow(A, -3).doit() == Inverse(A) * Inverse(A) * Inverse(A)\n\n"], "sample_867": ["def test_grid_search_cv_refit_with_multiple_scoring_and_metric_set():\n    # Test refit with multiple scoring metric and `scoring` parameter.\n    # Should use the best scoring metric.\n    X, y = make_classification(n_samples=50, random_state=0)\n    from sklearn.datasets import make_classification\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.metrics import accuracy_score\n\n    param_grid = {\n        'penalty': ['l1', 'l2'],\n        'C': [0.01, 0.1, 1, 10],\n    }\n\n    # Define custom scoring function using accuracy_score\n        return accuracy_score(y_true, y_pred)\n\n    # Define multi-metric evaluation\n    grid_search = GridSearchCV(\n        LogisticRegression(random_state=0),\n        param_grid,\n        scoring=['accuracy', 'f1', 'roc_auc'],\n        n_jobs=-1,\n        cv=5\n    )\n    grid_search.fit(X, y)\n\n    # Make sure the best estimator based on 'accuracy' metric\n    best_estimator = grid_search.best_estimator_\n    best_score = grid_search.best_score_\n\n    # Check predicted scores\n    y_pred = best_estimator.predict(X)\n    assert isinstance(best_score, float)\n    assert isinstance(best_estimator, LogisticRegression)\n    assert best_score == grid_search.best_score_\n    assert accuracy_score(y, y_pred) == best_score\n", "def test_refit_with_error_scoring():\n\n    # Test refit with scorers that raise errors\n    X, y = make_classification(n_samples=100, random_state=0)\n    class ClassifierWithErrorScoring(BaseEstimator):\n            raise ValueError('Classifier raised error during fit')\n\n            return np.zeros(X.shape[0])\n\n    gs = GridSearchCV(ClassifierWithErrorScoring(),\n                      {'param': [1, 2]}, scoring='accuracy')\n    with pytest.raises(ValueError,\n                       match='Classifier raised error during fit'):\n        gs.fit(X, y)\n    assert not hasattr(gs, 'best_estimator_')\n\n\n\n", "def test_grid_search_custom_scoring():\n    X, y = make_classification(n_samples=100, random_state=0)\n    class CustomScorer:\n            self.name = name\n\n            return (y_true == y_pred).mean()\n\n    scorer = CustomScorer()\n    grid_search = GridSearchCV(LogisticRegression(), {'C': [0.1, 1, 10]}, cv=3, scoring=scorer)\n    grid_search.fit(X, y)\n    assert grid_search.best_score_ == max(\n        grid_search.cv_results_['mean_test_score']\n    )\n", "def test_search_cv_refit_and_predict():\n    X, y = make_classification(n_samples=100, random_state=0)\n    clf = SVC(random_state=0)\n    gs = GridSearchCV(clf, {'C': [0.1, 1, 10]}, cv=5)\n    gs.fit(X, y)\n    X_new = np.random.rand(20, X.shape[1])\n    y_pred = gs.predict(X_new)\n    assert y_pred.shape[0] == X_new.shape[0]\n    y_pred_best = gs.best_estimator_.predict(X_new)\n    assert (y_pred == y_pred_best).all()\n\n\n\n", "    def test_search_cv_with_transformer_estimator():\n        from sklearn.pipeline import Pipeline\n        from sklearn.preprocessing import StandardScaler\n        from sklearn.linear_model import LogisticRegression\n\n        # Using regressor to make sure each score differs\n        X, y = make_classification(n_samples=100, n_informative=4,\n                                   random_state=0)\n        transformer = StandardScaler()\n        estimator = LogisticRegression()\n        pipeline = Pipeline([\n            ('scaler', transformer),\n            ('clf', estimator)\n        ])\n        param_grid = {\n            'clf__C': [0.1, 1, 10],\n            'scaler__with_mean': [True, False]\n        }\n        gs = RandomizedSearchCV(pipeline, param_grid, cv=3, n_jobs=-1,\n                               return_train_score=True)\n        gs.fit(X, y)\n\n        # Check if the pipeline was correctly fitted\n        assert isinstance(gs.best_estimator_, Pipeline)\n        assert isinstance(gs.best_estimator_.steps[0][1], StandardScaler)\n        assert isinstance(gs.best_estimator_.steps[1][1], LogisticRegression)\n\n        # Check the results\n        assert gs.best_params_ == {\n            'clf__C': gs.best_params_['clf__C'],\n            'scaler__with_mean': gs.best_params_['scaler__with_mean']\n        }\n\n\n"], "sample_933": ["def test_gettext_use_message_catalog(app):\n    app.builder.build_all()\n\n    assert (app.outdir / 'extapi.pot').isfile()\n\n    result = (app.outdir / 'extapi.pot').read_text()\n    assert 'msgid \"This is a test message\"' in result\n", "def test_gettext_additional_targets(app):\n    app.builder.build_all()\n\n    assert (app.outdir / 'extapi.pot').isfile()\n    assert (app.outdir / 'subdir.pot').isfile()\n    assert (app.outdir / 'markup.pot').isfile()\n\n    additional_targets = ['index_entries']\n    app.conf.setdefault('gettext_additional_targets', []).extend(additional_targets)\n    app.builder.build_all()\n\n    assert (app.outdir / 'index_entries.pot').isfile()\n\n\n\n", "def test_gettext_domain(app):\n    app.builder.build_all()\n\n    assert (app.outdir / 'extapi.pot').isfile()\n    assert (app.outdir / 'subdir.pot').isfile()\n\n\n\n    # assert that the domain is taken into account in the message catalog\n    with open(app.outdir / 'extapi.pot', 'r', encoding='utf-8') as pot:\n        pot_content = pot.read()\n\n    assert 'msgid \"This is a message from subdir\"' in pot_content\n    assert 'msgid \"This is a message from extapi\"' in pot_content\n\n\n\n", "def test_gettext_domain_overrides(app):\n    app.builder.build(['index'])\n    \n    # Check if the 'somedomain' catalog exists\n    assert (app.outdir / 'somedomain.pot').isfile()\n\n    # Check if messages are extracted with the correct domain\n    with open(app.outdir / 'somedomain.pot', 'r', encoding='utf-8') as f:\n        content = f.read()\n        assert \"msgid \\\"This is a message in somedomain\\\"\" in content\n\n\n", "def test_gettext_no_location(app):\n    app.builder.build_all()\n\n    pot = (app.outdir / 'extapi.pot').read_text()\n    assert 'Project-Id-Version: extapi' in pot\n    assert 'Report-Msgid-Bugs-To:' not in pot\n    assert 'Last-Translator:' not in pot\n\n\n\n"], "sample_28": ["    def test_write_to_compressed_image_to_same_file(self):\n        with fits.open(self.data(\"zerowidth.fits\"), mode=\"update\") as hdul:\n            hdu = hdul[0]\n            hdu.data = np.random.rand(100, 100)\n            hdu.writeto(self.data(\"zerowidth.fits\"), overwrite=True, checksum=True)\n            with fits.open(self.data(\"zerowidth.fits\")) as hdul:\n                assert \"COMPRESSED\" in hdul[0].header\n\n            hdul[0].compressed = False\n            hdul[0].writeto(self.data(\"zerowidth.fits\"), overwrite=True, checksum=True)\n            with fits.open(self.data(\"zerowidth.fits\")) as hdul:\n                assert \"COMPRESSED\" not in hdul[0].header\n\n", "    def test_header_comments(self):\n        h = fits.Header()\n        h.comments[\"FOO\"] = \"This is a comment\"\n        assert h.comments[\"FOO\"] == \"This is a comment\"\n        assert h[\"FOO\"].comment == \"This is a comment\"\n        h[\"BAR\"] = 10\n        assert h.comments[\"BAR\"] is None\n        h.comments[\"BAR\"] = \"Another comment\"\n        assert h.comments[\"BAR\"] == \"Another comment\"\n\n\n", "    def test_keyword_duplication_handling(self):\n        header = fits.Header()\n        header.set(\"FOO\", 1)\n        header.set(\"FOO\", 2)  # Attempt to set the same keyword twice\n        assert header[\"FOO\"] == 2  # Ensure the last value wins\n        assert len(header) == 1  # Ensure only one \"FOO\" card remains\n", "    def test_duplicate_header_keys(self):\n        h = fits.Header()\n        h.append(('FOO', 1))\n        h.append(('FOO', 2))\n        with pytest.raises(ValueError):\n            h.append(('FOO', 3))\n\n\n\n", "    def test_header_set_with_duplicate_keywords(self):\n        \"\"\"\n        Tests setting a header value with duplicate keywords. This shouldn't\n        raise an error, but the value should overwrite previous entries.\n        \"\"\"\n        h = fits.Header()\n        h[\"FOO\"] = \"BAR\"\n        h[\"FOO\"] = \"BAZ\"\n        assert h[\"FOO\"] == \"BAZ\"\n"], "sample_934": ["compilation error", "def test_build_domain_cpp_with_add_function_parentheses_is_True_and_noindexentry(app, status, warning):\n    app.builder.build_all()\n\n        pattern = '<li><p>%s<a .*?><code .*?><span .*?>%s</span></code></a></p></li>' % spec\n        res = re.search(pattern, text)\n        if not res:\n            print(\"Pattern\\n\\t%s\\nnot found in %s\" % (pattern, file))\n            assert False\n    rolePatterns = [\n        ('', 'Sphinx'),\n        ('', 'Sphinx::version'),\n        ('', 'version'),\n        ('', 'List'),\n        ('', 'MyEnum')\n    ]\n    parenPatterns = [\n        ('ref function without parens ', r'paren_1\\(\\)'),\n        ('ref function with parens ', r'paren_2\\(\\)'),\n        ('ref function without parens, explicit title ', 'paren_3_title'),\n        ('ref function with parens, explicit title ', 'paren_4_title'),\n        ('ref op call without parens ', r'paren_5::operator\\(\\)\\(\\)'),\n        ('ref op call with parens ', r'paren_6::operator\\(\\)\\(\\)'),\n        ('ref op call without parens, explicit title ', 'paren_7_title'),\n        ('ref op call with parens, explicit title ', 'paren_8_title')\n    ]\n\n    f = 'roles.html'\n    t = (app.outdir / f).read_text()\n    for s in rolePatterns:\n        check(s, t, f)\n    for s in parenPatterns:\n        check(s, t, f)\n\n    f = 'any-role.html'\n    t = (app.outdir / f).", "compilation error", "def test_noindexentry_with_parent(app):\n    text = (\".. cpp:function:: void f()\\n\"\n            \"   :noindexentry:\\n\"\n            \".. cpp:function:: void g()\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index, desc, addnodes.index, desc))\n    assert_node(doctree[0], addnodes.index, entries=[('single', 'f (C++ function)', '_CPPv41fv', '', None)])\n    assert_node(doctree[2], addnodes.index, entries=[('single', 'g (C++ function)', '_CPPv41fv', '', None)])\n", "compilation error"], "sample_1016": ["def test_logsumexp():\n    assert mcode(logsumexp(x, y)) == \"log1p(exp(x) + exp(y))\"\n", "def test_sympy_functions_with_args():\n    assert mcode(loggamma(x) + 1) == \"gammaln(x) + 1\"\n    assert mcode((polylog(2, x) + 1) * (sqrt(pi) + 1)) == (\n        \"polylog(2, x) + 1 .* (sqrt(pi) + 1)\"\n    )\n    assert mcode(risingfactorials(x, 3)) == \"pochhammer(x, 3)\"\n\n\n", "def test_special_functions_issue_15865():\n    from sympy import hyp2f1, MeijerG\n    assert octave_code(hyp2f1(x, y, z)) == 'hyp2f1(x, y, z)'\n    assert octave_code(MeijerG(\n        [(1, 1, 1), (2, 1, 1)], [(1, 1), (1, 2)], z\n    )) == 'MeijerG([1 1 1], [1 1; 1 2], z)'\n", "def test_polylog():\n    assert octave_code(polylog(x, y)) == 'polylog(x, y)'\n", "def test_polygamma():\n    assert octave_code(polygamma(1, x)) == 'digamma(x)'\n    assert octave_code(polygamma(2, x)) == 'polygamma(1, x)'\n    assert octave_code(polygamma(x, x)) == '% Not supported in Octave:\\n% polygamma\\npolygamma(x, x)'\n"], "sample_891": ["def test_label_ranking_average_precision_score_with_ties():\n    # Check that the metric handles ties correctly\n\n    y_true = np.array([1, 0, 2, 1])\n    y_score = np.array([[0.5, 0.3, 0.2], [0.8, 0.6, 0.4], [0.2, 0.3, 0.5], [0.7, 0.9, 0.6]])\n    ap = label_ranking_average_precision_score(y_true, y_score)\n    assert ap == pytest.approx(0.25)\n", "def test_average_precision_score_multiclass_with_labels():\n    y_true = np.array([0, 1, 2, 3])\n    y_score = np.array(\n        [\n            [0.4, 0.3, 0.2, 0.1],\n            [0.1, 0.3, 0.4, 0.2],\n            [0.4, 0.1, 0.2, 0.3],\n            [0.3, 0.2, 0.4, 0.1],\n        ]\n    )\n    labels = [0, 1, 2, 3]\n    assert average_precision_score(y_true, y_score, labels=labels) == pytest.approx(0.4)\n\n\n\n", "def test_label_ranking_avg_precision_score_with_n_labels():\n    y_true = np.array([[0, 1, 0], [1, 0, 1]])\n    y_score = np.array([[0.5, 0.9, 0.6], [0, 0.3, 1]])\n\n    # Test with n_labels provided\n    with pytest.raises(ValueError):\n        label_ranking_average_precision_score(y_true, y_score, n_labels=2)\n    # Test with n_labels equal to the number of unique labels\n    n_labels = 3\n    result = label_ranking_average_precision_score(\n        y_true, y_score, n_labels=n_labels\n    )\n    assert result == pytest.approx(1)\n\n    # Test with n_labels larger than the number of unique labels\n    with pytest.raises(ValueError):\n        label_ranking_average_precision_score(\n            y_true, y_score, n_labels=5\n        )\n\n\n\n", "def test_precision_at_k_and_recall_at_k_with_empty_array():\n    y_true = np.array([], dtype=int)\n    y_score = np.array([], dtype=float)\n\n    assert precision_at_k(y_true, y_score, k=2) == 0\n    assert recall_at_k(y_true, y_score, k=2) == 0\n", "def test_roc_curve_with_sparse_data():\n    \"\"\"Test roc_curve with sparse data.\"\"\"\n    rng = np.random.RandomState(0)\n    n_samples = 100\n    n_features = 20\n    y_true = rng.randint(0, 2, size=n_samples)\n    X = rng.randn(n_samples, n_features)\n    X_sparse = csr_matrix(X)\n    clf = LogisticRegression(random_state=0)\n    clf.fit(X_sparse, y_true)\n    _, _, thresholds = roc_curve(y_true, clf.predict_proba(X_sparse)[:, 1])\n    assert all(np.isfinite(thresholds))\n"], "sample_153": ["    def test_mysql_strict_mode_warning_suppressed(self, mocked_warn):\n        connection.vendor = 'mysql'  # Set up connection to be MySQL\n        with mock.patch(\n            'django.db.backends.utils.CursorWrapper.fetchone', create=True,\n            return_value=('STRICT_TRANS_TABLES',)\n        ):\n            result = check_database_backends(databases=self.databases, ignore_warnings=['mysql.W002'])\n        self.assertEqual(len(result), 0)\n        mocked_warn.assert_not_called()\n", "    def test_postgresql_check_constraints(self):\n        with mock.patch(\n            'django.db.backends.postgresql.base.DatabaseOperations.has_table',\n            return_value=True\n        ):\n            with mock.patch(\n                'django.db.backends.postgresql.features.PostgreSQLFeatures.supports_table_check_constraints',\n                return_value=False\n            ):\n                result = check_database_backends(databases=self.databases)\n                self.assertEqual(len(result), 2)\n                self.assertEqual([r.id for r in result], ['models.W027', 'models.W027'])\n", "    def test_check_index_together(self):\n        # Test case for _check_index_together of Model base.\n        from django.db.models import Index\n\n        model = type('Model', (models.Model,), {})\n        model._meta = type('Meta', (object,), {'indexes': [\n            Index(fields=['field1', 'field2'], name='name'),\n            Index(fields=['field2', 'field3'], name='name'),\n        ]})\n\n        with mock.patch('django.db.models.ModelBase._check_local_fields', return_value=[]):\n            errors = model._meta._check_index_together()\n            self.assertEqual(errors, [])\n\n        with mock.patch('django.db.models.ModelBase._check_local_fields', side_effect=ValueError):\n            errors = model._meta._check_index_together()\n            self.assertEqual(len(errors), 1)\n", "    def test_custom_database_check_function(self):\n            return [checks.Warning('Custom check warning', id='custom.W001')]\n        with mock.patch('django.core.checks.registry.get_checks', return_value=[]), \\\n             mock.patch('django.core.checks.database.check_database_backends', autospec=True) as mock_check_backends:\n            \n            mock_check_backends.side_effect = fake_custom_check\n\n            result = check_database_backends(databases=self.databases)\n            self.assertEqual([r.id for r in result], ['custom.W001', 'custom.W001'])\n\n\n", "    def test_postgresql_unique_check_constraints(self):\n        with mock.patch('django.db.backends.utils.CursorWrapper.fetchone', create=True,\n                         return_value=('uuid',)):\n            with mock.patch('django.db.backends.utils.CursorWrapper.execute', create=True):\n                self.assertEqual(check_database_backends(databases=self.databases), [])\n\n\n\n"], "sample_98": ["    def test_keep_alive_on_streaming_response(self):\n        conn = HTTPConnection(LiveServerViews.server_thread.host, LiveServerViews.server_thread.port, timeout=1)\n        try:\n            conn.request('GET', '/streaming_example_view/', headers={\"Connection\": \"keep-alive\"})\n            response = conn.getresponse()\n            self.assertFalse(response.will_close)\n            self.assertEqual(response.read(), b'Iamastream')  \n            self.assertEqual(response.status, 200)\n            self.assertIsNone(response.getheader('Connection'))\n\n            conn.request('GET', '/streaming_example_view/', headers={\"Connection\": \"keep-alive\"})\n            response = conn.getresponse()\n            self.assertFalse(response.will_close) \n            self.assertEqual(response.read(), b'Iamastream')\n            self.assertEqual(response.status, 200)\n            self.assertIsNone(response.getheader('Connection'))\n        finally:\n            conn.close() \n\n", "    def test_threaded_server_handles_concurrent_requests(self):\n        urls = [\n            '/example_view/',\n            '/example_view/',\n            '/streaming_example_view/',\n            '/method_view/',\n        ]\n        threads = [\n            LiveServerThread(self.live_server_url + url,\n                             LiveServerBase.server_thread) for url in urls\n        ]\n        for thread in threads:\n            thread.start()\n\n        for thread in threads:\n            thread.join()\n\n        # Assert that all threads completed successfully\n        for thread in threads:\n            self.assertTrue(thread.is_alive() is False)\n", "    def test_content_type_header(self):\n        with self.urlopen('/content_type_view/') as f:\n            self.assertEqual(f.headers['Content-Type'], 'text/html; charset=utf-8')\n", "    def test_broken_pipe_error_handling(self):\n        \"\"\"\n        Test that a BrokenPipeError is handled gracefully.\n        \"\"\"\n        url = '/broken_pipe_view/'\n        with self.urlopen(url) as f:\n            self.assertEqual(f.read().strip(), b'The connection was closed prematurely.')\n", "    def test_content_length_header(self):\n        conn = HTTPConnection(LiveServerViews.server_thread.host, LiveServerViews.server_thread.port)\n        try:\n            conn.request('GET', '/content_length_view/', headers={'Connection': 'keep-alive'})\n            response = conn.getresponse()\n            self.assertEqual(response.getheader('Content-Length'), '16')\n            self.assertEqual(response.read(), b'example view')\n        finally:\n            conn.close()\n"], "sample_201": ["    def test_not_finished_sentinel(self):\n        storage = self.get_storage()\n        response = self.get_response()\n\n        # Add more messages than will fit in the cookie.\n        for i in range(CookieStorage.max_cookie_size // 10 + 1):\n            storage.add(constants.INFO, f'Message {i}')\n        unstored_messages = storage.update(response)\n\n        # Assert that not_finished was added to the cookie.\n        cookie_data = response.cookies['messages'].value\n        self.assertEqual(json.loads(cookie_data, cls=MessageDecoder)[-1], CookieStorage.not_finished)\n\n        # Assert that the unstored messages are what we expect.\n        self.assertEqual(len(unstored_messages), 1)\n        self.assertEqual(unstored_messages[0].message, 'Message 5')\n", "    def test_not_finished_sentinel(self):\n        storage = self.get_storage()\n        response = self.get_response()\n\n        # Store a set of messages and mark that not all are stored.\n        some_messages = ['message 1', 'message 2']\n        storage.update(response, messages=some_messages + ['message 3'])\n\n        # Retrieve and check the messages\n        messages1 = list(storage)\n        self.assertEqual(messages1, some_messages)\n\n        # Attempt to get remaining messages.\n        _, all_retrieved = storage._get(*args, **kwargs)\n        self.assertFalse(all_retrieved)\n\n        # Store the remaining message and check it's present\n        storage.update(response, messages=['message 3'])\n        messages2 = list(storage)\n        self.assertEqual(messages2, some_messages + ['message 3'])\n\n\n\n", "    def test_not_finished_sentinel(self):\n        storage = self.get_storage()\n        response = self.get_response()\n\n        storage.add(constants.INFO, 'message 1')\n        storage.add(constants.INFO, 'message 2')\n        storage.add(constants.INFO, 'message 3')\n        storage.update(response)\n\n        # retrieve messages from the cookie\n        messages = list(storage)\n        self.assertEqual(len(messages), 3)\n\n        # add a message while not finished, simulating the need to\n        # split the message storage across multiple cookies\n        storage.add(constants.INFO, 'message 4')\n        storage.update(response)\n\n        # retrieve messages again, this time we expect not_finished\n        # to be present and the last message to be missing\n        messages = list(storage)\n        self.assertEqual(len(messages), 3)\n", "    def test_not_finished_flag(self):\n        storage = self.get_storage()\n        response = self.get_response()\n\n        # Store more messages than the allowed cookie size.\n        for i in range(CookieStorage.max_cookie_size // (len(\"message\")) + 1):\n            storage.add(constants.INFO, f\"message {i}\")\n\n        # Retrieve the messages.\n        storage.update(response)\n\n        # Check the not_finished flag.\n        self.assertEqual(stored_cookie_messages_count(storage, response), CookieStorage.max_cookie_size // (len(\"message\")) )\n        self.assertTrue(storage._loaded_data.endswith('[CookieStorage.not_finished]'))\n\n\n\n", "    def test_not_finished_sentinel(self):\n        storage = self.get_storage()\n        response = self.get_response()\n\n        # Send a message that will be split across two cookies due to max_cookie_size\n        for i in range(CookieStorage.max_cookie_size):\n            storage.add(constants.INFO, str(i))\n\n        # The cookie should have been set, but not all messages will be in it\n        # because of the sentinel value\n        unstored_messages = storage.update(response)\n        cookie_storing = stored_cookie_messages_count(storage, response)\n        self.assertEqual(cookie_storing, CookieStorage.max_cookie_size)\n        self.assertEqual(len(unstored_messages), 0) \n\n"], "sample_798": ["    def test_ridge_classifier_class_weight():\n        # Test class_weight parameter for RidgeClassifier\n\n        X, y = make_classification(n_samples=100, n_features=10,\n                                  n_informative=5, random_state=0)\n        class_weights = {0: 3, 1: 1}\n\n        ridge_classifier = RidgeClassifier(class_weight=class_weights)\n        ridge_classifier.fit(X, y)\n        assert_array_almost_equal(ridge_classifier.coef_,\n                                  ridge_classifier.coef_,\n                                  decimal=5)  \n", "def test_ridge_with_categorical_features():\n    rng = np.random.RandomState(0)\n    n_samples, n_features = 100, 5\n    n_categories = 2\n    X = np.random.rand(n_samples, n_features)\n    y = rng.randn(n_samples)\n\n    # Create categorical features\n    category_features = np.random.randint(0, n_categories, size=(n_samples, 1))\n    X = np.hstack((X, category_features))\n\n    # Test with different solvers\n    for solver in ['sag', 'saga', 'lsqr', 'sparse_cg']:\n        ridge = Ridge(alpha=1.0, solver=solver)\n        ridge.fit(X, y)\n\n", "def test_sparse_matrix_handling():\n    import scipy.sparse as sp\n\n    # Test if sparse matrices are handled correctly\n\n    X_sparse = sp.csr_matrix(X_diabetes)\n    y = y_diabetes\n\n    # Test Ridge\n    ridge = Ridge()\n    ridge.fit(X_sparse, y)\n\n    # Test RidgeCV\n    ridge_cv = RidgeCV()\n    ridge_cv.fit(X_sparse, y)\n\n", "compilation error", "def test_sparse_matrix_with_sample_weight():\n    # Test Ridge with sparse matrix and sample weights\n\n    from sklearn.sparse import csr_matrix\n\n    n_samples = 100\n    n_features = 5\n    random_state = 42\n    rng = check_random_state(random_state)\n    X = rng.rand(n_samples, n_features)\n    y = rng.randn(n_samples)\n    sample_weights = rng.rand(n_samples) ** 2 + 1\n\n    X_sparse = csr_matrix(X)\n\n    ridge = Ridge(alpha=1, fit_intercept=True)\n    ridge.fit(X, y, sample_weight=sample_weights)\n    ridge_sparse = Ridge(alpha=1, fit_intercept=True)\n    ridge_sparse.fit(X_sparse, y, sample_weight=sample_weights)\n\n    assert_array_almost_equal(ridge.coef_, ridge_sparse.coef_)\n\n\n"], "sample_1061": ["def test_mod_int_negative():\n    assert mod_inverse( -2, 5) == 3\n    assert mod_inverse(-12, 5) == 1\n    assert mod_inverse(-2, -5) == -3\n    assert mod_inverse(-12, -5) == 4\n    assert mod_inverse(-21, 10) == 8\n\n\n\n", "def test_mod_with_large_numbers():\n    assert mod_inverse(2147483647, 2147483648) == 1\n    assert mod_inverse(1234567890, 987654321) == 548765432\n    assert mod_inverse(2**31 - 1, 2**32) == 2**31 - 1\n", "def test_mod_arithmetic():\n    assert (S(5) % 2) == 1\n    assert (S(6) % 2) == 0\n    assert (S(7) % 2) == 1\n    assert (S(10) % 3) == 1\n    assert (S(11) % 3) == 2\n    assert (S(12) % 3) == 0\n    assert (S(13) % 3) == 1\n    assert (S(10) % 0) == S.ZeroDivisionError\n    assert (S(0) % 2) == 0\n    assert (S(0) % 3) == 0\n    assert (S(-5) % 2) == 1\n    assert (S(-6) % 2) == 0\n    assert (S(-7) % 2) == 1\n    assert (S(-10) % 3) == 1\n    assert (S(-11) % 3) == 2\n    assert (S(-12) % 3) == 0\n    assert (S(-13) % 3) == 1\n    assert (S(10) % -3) == 1\n    assert (S(11) % -3) == 2\n    assert (S(12) % -3) == 0\n    assert (S(13) % -3) == 1\n\n\n\n", "def test_issue_10719():\n    assert Float(exp(1)).n(10) == Float(exp(1)).n(50)\n\n\n", "def test_mod_inverse_large_numbers():\n    assert mod_inverse(1234567890, 987654321) == 876543210\n"], "sample_1160": ["def test_issue_18255():\n    interval = Interval(1, 2)\n    raises(TypeError, lambda: interval.intersect(FiniteSet(oo)))\n", "def test_issue_19037():\n    r = S.Naturals\n    assert (r.inf, r.sup) == (0, oo)\n    assert r.is_bounded == False\n    assert r.is_finite == False\n    r = S.Naturals0\n    assert (r.inf, r.sup) == (0, oo)\n    assert r.is_bounded == False\n    assert r.is_finite == False\n    r = S.Integers\n    assert (r.inf, r.sup) == (-oo, oo)\n    assert r.is_bounded == False\n    assert r.is_finite == False\n    r = S.Rationals\n    assert (r.inf, r.sup) == (-oo, oo)\n    assert r.is_bounded == False\n    assert r.is_finite == False\n    r = S.Reals\n    assert (r.inf, r.sup) == (-oo, oo)\n    assert r.is_bounded == False\n    assert r.is_finite == False\n    r = S.Complexes\n    assert (r.inf, r.sup) == (-oo, oo)\n    assert r.is_bounded == False\n    assert r.is_finite == False\n", "def test_issue_19228():\n    from sympy import symbols, pi\n    x, y = symbols('x y')\n    assert imageset(Lambda((x, y), x + I*y), S.Reals) == \\\n       ImageSet(Lambda((x, y), x + I*y), S.Reals)\n    assert imageset(Lambda((x, y), x + I*y), S.Integers) == \\\n       ImageSet(Lambda((x, y), x + I*y), S.Integers)\n    assert imageset(Lambda((x, y), x + I*y), S.Naturals) == \\\n       ImageSet(Lambda((x, y), x + I*y), S.Naturals)\n    assert imageset(Lambda((x, y), x + I*y), S.Naturals0) == \\\n       ImageSet(Lambda((x, y), x + I*y), S.Naturals0)\n    assert imageset(Lambda((x, y), x + I*y), FiniteSet(1, 2)) == \\\n       Set(1 + I*1, 2 + I*1, 1 + I*2, 2 + I*2)\n\n\n", "def test_issue_18134_complex():\n    assert S.Complexes.boundary == S.Complexes\n    assert S.Complexes.closure == S.Complexes\n    assert S.Complexes.is_open == False\n    assert S.Complexes.is_closed == True  \n", "def test_imageSet_infinite_range():\n    assert ImageSet(Lambda(x, x), S.Reals).is_subset(S.Reals)\n    assert ImageSet(Lambda(x, 2*x + 1), S.Reals).is_subset(S.Reals)\n    assert ImageSet(Lambda(x, x**2), S.Reals).is_subset(S.Reals)\n    assert ImageSet(Lambda(x, sin(x)), S.Reals).is_subset(S.Reals)\n    assert ImageSet(Lambda(x, cos(x)), S.Reals).is_subset(S.Reals)\n    \n    a = Symbol(\"a\", real=True)\n    b = Symbol(\"b\", real=True)\n    assert ImageSet(Lambda(x, x**2 + a*x + b), S.Reals\n        ).is_subset(S.Reals)\n\n"], "sample_901": ["def test_k_means_feature_scaling():\n    from sklearn.preprocessing import StandardScaler\n\n    # Test that KMeans works correctly with scaled features\n    X = StandardScaler().fit_transform(X)\n    km = KMeans(n_clusters=n_clusters, random_state=42)\n    km.fit(X)\n    assert_almost_equal(km.inertia_, np.mean(np.abs(X - km.cluster_centers_)**2),\n                        decimal=4)\n", "def test_partial_fit_with_different_sample_weights():\n    # Check that partial_fit works correctly with different sample_weights\n    # across different calls to partial_fit. GH #20072\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    sample_weights = np.array([0.5, 0.7, 0.8])\n    km = MiniBatchKMeans(n_clusters=2, random_state=0)\n    km.partial_fit(X[0:1], sample_weight=sample_weights[0])\n    km.partial_fit(X[1:2], sample_weight=sample_weights[1])\n    km.partial_fit(X[2:], sample_weight=sample_weights[2])\n    labels = km.labels_\n    assert np.unique(labels).size == 2\n", "def test_k_means_fit_transform_sparse_matrix():\n    # Test fit_transform with sparse matrix input.\n    X_csr = sp.csr_matrix(X)\n    km = KMeans(n_clusters=3)\n    km.fit_transform(X_csr)\n    assert km.labels_.shape[0] == X_csr.shape[0]\n\n\n", "def test_minibatch_kmeans_with_sparse_and_dense_input():\n    # GH 19001: MiniBatchKMeans should work with sparse and dense input\n    # interchangeably\n    mb_k_means = MiniBatchKMeans(n_clusters=3,\n                                 batch_size=20,\n                                 n_init=1,\n                                 random_state=42)\n\n    # Fit on sparse data\n    mb_k_means.fit(X_csr)\n    sparse_labels = mb_k_means.labels_\n\n    # Predict on dense data transformed from the sparse data\n    dense_labels = mb_k_means.predict(X)\n\n    # Check that labels match\n    assert_array_equal(sparse_labels, dense_labels)\n", "def test_minibatch_kmeans_sparse_fit():\n    # GH #15068\n    n_samples = 100\n    n_features = 5\n    X = sp.csc_matrix(np.random.rand(n_samples, n_features))\n    km = MiniBatchKMeans(n_clusters=5, batch_size=10, n_init=1,\n                         random_state=0).fit(X)\n    assert km.cluster_centers_.shape[0] == 5\n    assert km.cluster_centers_.shape[1] == n_features\n\n\n\n"], "sample_117": ["    def test_password_too_short(self):\n        user = User.objects.get(username='testclient')\n        data = {\n            'password1': '123',\n            'password2': '123',\n        }\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        self.assertIn(\n            'Your password must be at least 8 characters long.',\n            form['password1'].errors\n        )\n\n\n\n", "    def test_bug_19349_bound_password_field(self):\n        user = User.objects.get(username='testclient')\n        form = AdminPasswordChangeForm(data={}, instance=user)\n        # When rendering the bound password field,\n        # ReadOnlyPasswordHashWidget needs the initial\n        # value to render correctly\n        self.assertEqual(form.initial['password'], form['password'].value())\n", "    def test_invalid_password(self):\n        user = User.objects.get(username='testclient')\n        data = {\n            'password1': 'short',\n            'password2': 'short',\n        }\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['password1'], [\n            'The password is too short. It must contain at least 8 characters.',\n        ])\n\n        self.assertEqual(form.errors['password2'], [\n            'The password is too short. It must contain at least 8 characters.',\n        ])\n", "    def test_password_complexity_error(self):\n        user = User.objects.get(username='testclient')\n        data = {'password1': 'test', 'password2': 'test'}\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        self.assertIn(\n            'Password must meet the following criteria: Minimum 8 characters.',\n            form.errors['password1']\n        )  \n\n\n\n", "    def test_bug_19133_password_change_admin_form(self):\n        user = User.objects.get(username='testclient')\n        form = AdminPasswordChangeForm(instance=user)\n        post_data = form.initial\n        post_data['password1'] = 'new password'\n        \n        form = AdminPasswordChangeForm(instance=user, data=post_data)\n        self.assertTrue(form.is_valid())\n        # original hashed password contains $\n        self.assertIn('$', form.cleaned_data['password'])\n"], "sample_1136": ["def test_issue_19879():\n    assert poly(x**2 + 1, wrt=x) == Poly(x**2 + 1, x)\n", "compilation error", "def test_issue_19907():\n    x = Symbol('x')\n    y = Symbol('y')\n    p = Poly(x**3 + y**3, x, y)\n    assert p.as_expr() == x**3 + y**3\n    assert p.as_poly().as_expr() == x**3 + y**3\n", "def test_poly_ordering():\n    p1 = Poly(x*y + x**2, x, y, order='lex')\n    p2 = Poly(x*y + x**2, x, y, order='grlex')\n    p3 = Poly(x*y + x**2, x, y, order='grevlex')\n    assert p1 != p2 and p2 != p3 and p1 != p3\n    assert p1.order == 'lex'\n    assert p2.order == 'grlex'\n    assert p3.order == 'grevlex'\n    assert p1.exprs == [x*y, x**2]\n    assert p2.exprs == [x**2, x*y]\n    assert p3.exprs == [x*y, x**2]\n\n\n", "def test_issue_19971():\n    expr = Poly(x**2*y + x*y**2 + x*z + y*z, x, y, z)\n    assert expr.subs(y, 0) == Poly(x**2 + x*z, x, z)\n\n\n\n"], "sample_763": ["compilation error", "def test_check_sample_weights_for_sparse_arrays():\n    # Test that sample_weights are handled correctly for sparse arrays.\n    X = [[1, 2], [3, 4]]\n    sample_weights = sp.csr_matrix([0.5, 1.0])\n    assert_equal(check_sample_weights(X, sample_weights), [0.5, 1.0])\n    sample_weights = sp.array([[0.5], [1.0]], dtype=np.float64)\n    assert_equal(check_sample_weights(X, sample_weights), [0.5, 1.0])\n    \n    # Test error handling:\n    assert_raises(ValueError, check_sample_weights, X, sp.csr_matrix((1, 2)))\n\n\n\n", "compilation error", "compilation error", "def test_check_array_sparse_from_dense():\n    X_dense = np.array([[1, 2], [3, 4]])\n    X_sparse = sp.csr_matrix(X_dense)\n    assert_raises(TypeError, check_array, X_sparse, dtype=np.float32,\n                  accept_sparse=['format', 'coo'])\n    assert_raises(TypeError, check_array, X_sparse, dtype=np.float32,\n                  accept_sparse=False)\n    assert check_array(X_sparse, accept_sparse='csr').shape == X_dense.shape\n    assert check_array(X_sparse, accept_sparse=True).shape == X_dense.shape\n\n\n\n"], "sample_270": ["    def test_unique_constraint_include_duplicate_fields(self):\n        class Model(models.Model):\n            name = models.CharField(max_length=10)\n            age = models.IntegerField()\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(\n                        fields=['name', 'age', 'name'],\n                        include=['id'],\n                        name='name',\n                    ),\n                ]\n\n        self.assertEqual(Model.check(databases=self.databases), [\n            Error(\n                \"'constraints' defines duplicate field 'name'.\",\n                obj=Model,\n                id='models.E014',\n            ),\n        ])\n", "    def test_unique_constraint_include_pointing_to_joined_fields(self):\n        class Model(models.Model):\n            name = models.CharField(max_length=10)\n            field1 = models.PositiveSmallIntegerField()\n            field2 = models.PositiveSmallIntegerField()\n            parent = models.ForeignKey('self', models.CASCADE)\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(\n                        fields=['id'],\n                        include=['parent__field1', 'parent__name'],\n                        name='name',\n                    ),\n                ]\n\n        joined_fields = ['parent__field1', 'parent__name']\n        errors = Model.check(databases=self.databases)\n        expected_errors = [\n            Error(\n                \"'constraints' refers to the joined field '%s'.\" % field_name,\n                obj=Model,\n                id='models.E041',\n            ) for field_name in joined_fields\n        ]\n        self.assertCountEqual(errors, expected_errors)\n", "    def test_unique_constraint_include_circular_dependency(self):\n        class Parent(models.Model):\n            parent = models.ForeignKey('self', models.CASCADE, related_name='children')\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(fields=['id'], include=['parent'], name='parent_id_unique'),\n                ]\n\n        self.assertEqual(Parent.check(databases=self.databases), [\n            Error(\n                \"'constraints' refers to a field that causes a circular \"\n                \"dependency: 'parent'.\",\n                obj=Parent,\n                id='models.E040',\n            ),\n        ])\n", "    def test_unique_constraint_exclude_pointing_to_missing_field(self):\n        class Model(models.Model):\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(\n                        fields=['id'],\n                        name='name',\n                        exclude=['missing_field'],\n                    ),\n                ]\n\n        self.assertEqual(Model.check(databases=self.databases), [\n            Error(\n                \"'constraints' refers to the nonexistent field \"\n                \"'missing_field'.\",\n                obj=Model,\n                id='models.E012',\n            ),\n        ])\n\n", "    def test_unique_constraint_include_combined_with_condition(self):\n        class Model(models.Model):\n            age = models.IntegerField()\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(\n                        fields=['age'],\n                        include=['id'],\n                        name='unique_age_include_id',\n                        condition=models.Q(age__gte=100),\n                    ),\n                ]\n\n        errors = Model.check(databases=self.databases)\n        expected = [] if connection.features.supports_partial_indexes else [\n            Warning(\n                '%s does not support unique constraints with non-key columns and conditions.'\n                % connection.display_name,\n                hint=(\n                    \"A constraint won't be created. Silence this warning if \"\n                    \"you don't care about it.\"\n                ),\n                obj=Model,\n                id='models.W040',\n            ),\n        ]\n        self.assertEqual(errors, expected)\n\n"], "sample_824": ["def test_check_sparse_arrays_different_shapes():\n    # Ensure that an error is raised if the dimensions are different.\n    rng = np.random.RandomState(0)\n    XA = rng.random_sample((5, 4)).astype(np.float32)\n    XA_sparse = csr_matrix(XA)\n    XB = rng.random_sample((4, 5)).astype(np.float32)\n    XB_sparse = csr_matrix(XB)\n    assert_raises(ValueError, check_pairwise_arrays, XA_sparse, XB_sparse)\n\n    \n", "def test_pairwise_distances_with_precomputed( ):\n    # Check that pairwise_distances works correctly when\n    # the distances are already computed.\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((10, 10))\n    D = pairwise_distances(X, metric='euclidean')\n    \n    # Check that using precomputed distances works as expected\n    D_precomputed = pairwise_distances(D, metric='precomputed')\n    assert_array_equal(D, D_precomputed)\n", "def test_euclidean_distances_with_precomputed():\n    # Check euclidean_distances with precomputed distances\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((10, 10))\n    D = cdist(X, X)\n    result = euclidean_distances(X, X, precomputed=D)\n    assert_array_equal(result, D)\n\n    # Test that precomputed distances for different shapes raise an error\n    with pytest.raises(ValueError):\n        euclidean_distances(X, X, precomputed=np.random.random((10, 5)))\n\n    # test with sparse X\n    X_sparse = csr_matrix(X)\n    D_sparse = cdist(X_sparse, X_sparse)\n    result = euclidean_distances(X_sparse, X_sparse, precomputed=D_sparse)\n    assert_array_equal(result.toarray(), D_sparse)\n\n\n\n", "def test_pairwise_distances_with_precomputed():\n    # Check pairwise_distances when distances are precomputed.\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((5, 4))\n    D = cdist(X, X)\n    # Test that the pre-computed distances are used correctly.\n    distances = pairwise_distances(X, metric='precomputed',\n                                  distances=D)\n    assert_array_almost_equal(distances, D)\n\n\n", "def test_cosine_similarity_with_missing_values():\n    # Check if cosine similarity handles missing values correctly\n    X = np.array([[1, 2, np.nan], [4, np.nan, 6], [7, 8, 9]])\n    Y = np.array([[10, 11, 12], [13, 14, np.nan], [15, 16, 17]])\n    \n    # Calculate cosine similarity\n    cosine_sim = cosine_similarity(X, Y)\n    \n    # Assert that NaNs are correctly propagated\n    assert np.isnan(cosine_sim).any()  \n\n    #  Test with sparse matrices\n    X_sparse = csr_matrix(X)\n    Y_sparse = csr_matrix(Y)\n    cosine_sim_sparse = cosine_similarity(X_sparse, Y_sparse)\n    assert np.isnan(cosine_sim_sparse).any() \n"], "sample_1142": ["def test_matrix_transpose():\n    A = MatrixSymbol('A', 2, 3)\n    assert A.T.shape == (3, 2)\n    assert isinstance(A.T, Transpose)\n    assert (A.T).as_explicit() == A.T\n    assert (A + B).T == A.T + B.T\n    assert (A * B).T == B.T * A.T\n    assert ((A * B) * C).T == C.T * (B.T * A.T)\n    assert (A.T.T) == A\n    assert (A.T[0, 1]) == A[1, 0]\n\n\n\n\n", "compilation error", "def test_matrix_with_function():\n    f = Function('f')(x)\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    assert isinstance(A + f, MatAdd)\n    assert isinstance(A * f, MatMul)\n    assert isinstance(f * A, MatMul)\n    assert isinstance(f ** 2 * A, MatMul)\n    assert isinstance(A + f * B, MatAdd)\n    assert isinstance(A * (f + B), MatAdd)\n", "def test_matrix_transpose():\n    A = MatrixSymbol('A', 2, 3)\n    B = MatrixSymbol('B', 3, 2)\n    C = Transpose(A)\n    assert C.shape == (3, 2)\n    assert C[i, j] == A[j, i]\n    assert Transpose(Transpose(A)) == A\n\n    assert Transpose(A*B).as_explicit() == Transpose(B).as_explicit() * Transpose(A).as_explicit()\n    assert Transpose(A*B) != B*A\n    assert Transpose(A + B) == Transpose(A) + Transpose(B)\n    assert Transpose(A - B) == Transpose(A) - Transpose(B)\n    assert Transpose(A**2) == Transpose(A)**2\n    assert Transpose(A**-1) == Transpose(A)**(-1)\n\n\n\n", "def test_issue_2865():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    assert (A + B).transpose() == A.transpose() + B.transpose()\n    assert (A * B).transpose() == B.transpose() * A.transpose()\n    assert (A * 2).transpose() == 2 * (A.transpose())\n    assert (2 * A).transpose() == 2 * (A.transpose())\n"], "sample_1037": ["def test_transpose_simplified():\n    x, y, z, w = symbols(\"x y z w\")\n    M = Matrix([[x, y], [z, w]])\n    M_t = M.T\n    assert M_t == Matrix([[x, z], [y, w]])\n    assert str(M) != str(M_t)\n    assert M_t.T == M\n    assert (M_t*M).doit() == (M*M_t).doit() \n    assert simplify(M_t) == M_t\n    assert simplify(M) == M\n\n\n", "def test_commutativity_of_matrix_multiplication():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    assert (A*B).doit() == (B*A).doit()\n\n\n", "def test_transpose():\n    A = MatrixSymbol(\"A\", 3, 2)\n    assert (A.T).shape == (2, 3)\n    assert (A.T).T == A\n\n\n\n", "def test_inverse_properties():\n    A = MatrixSymbol('A', n, n)\n    assert (Inverse(A)*A).doit() == Identity(n)\n    assert (A*Inverse(A)).doit() == Identity(n)\n    assert Inverse(A)*A == Identity(n)\n    assert A*Inverse(A) == Identity(n)\n    raises(ValueError, lambda: Inverse(ZeroMatrix(n, n)))\n    raises(ValueError, lambda: Inverse(A).subs(A, ZeroMatrix(n, n)))\n\n", "def test_simplify_matrix_expressions():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    C = MatrixSymbol('C', 2, 2)\n    D = MatrixSymbol('D', 2, 2)\n    assert simplify(A * B * C + B * A * C) == simplify(2 * A * B * C)\n    assert simplify(A * (B + C) * D) == simplify(A * B * D + A * C * D)\n    assert simplify((A + B) * (C + D)) == simplify(A * C + A * D + B * C + B * D)\n"], "sample_846": ["    def test_column_transformer_with_list_like_sparse_remainder():\n        X_array = np.array([[1, 2], [3, 4], [5, 6]]).astype('float').T\n        sparse_remainder = [\n            SparseMatrixTrans(),\n            'drop'\n        ]\n        ct = ColumnTransformer([('trans1', StandardScaler(), [0])],\n                               remainder=sparse_remainder)\n        ct.fit(X_array)\n        assert len(ct.transformers_) == 2\n        assert isinstance(ct.transformers_[-1][1], list)\n\n\n\n", "def test_column_transformer_sparse_remainder_transformation():\n    X_array = np.array([[0, 1, 2],\n                        [2, 4, 6],\n                        [8, 6, 4]]).astype('float').T\n    sparse_remainder_transform = lambda x: sparse.csr_matrix(x)\n\n    ct = ColumnTransformer([('trans1', StandardScaler(), [0])],\n                           remainder=sparse_remainder_transform)\n\n    X_trans = ct.fit_transform(X_array)\n    assert sparse.issparse(X_trans)\n    assert X_trans.shape == X_array.shape\n    assert len(ct.transformers_) == 2\n    assert ct.transformers_[-1][0] == 'remainder'\n\n\n\n", "    def test_column_transformer_mask_with_remainder():\n        X = np.transpose([[1, 2, 3], [4, 5, 6], [5, 6, 7], [8, 9, 10]])\n        X = sparse.csr_matrix(X)\n        column_transformer = ColumnTransformer(\n            [('identity', FunctionTransformer(), [False, True, False, True]),\n             ('remainder', StandardScaler(), [True])],\n        )\n        X_trans = column_transformer.fit_transform(X)\n        assert X_trans.shape == (3, 2)\n", "def test_column_transformer_sparse_remainder_type():\n    X_array = np.array([[0, 1, 2],\n                        [2, 4, 6],\n                        [8, 6, 4]]).astype('float').T\n    ct = ColumnTransformer([('trans1', StandardScaler(), [0])],\n                           remainder=SparseMatrixTrans())\n    X_trans = ct.fit_transform(X_array)\n\n    # Assert that the remainder output is of the expected type\n    assert isinstance(X_trans, sparse.csr_matrix)\n", "def test_column_transformer_errors_with_invalid_remainder():\n    with pytest.raises(ValueError):\n        ct = ColumnTransformer([('trans1', Trans(), [0])], remainder='invalid')\n    with pytest.raises(ValueError):\n        ct = ColumnTransformer([('trans1', Trans(), [0])],\n                               remainder=lambda x: x)\n\n"], "sample_69": ["    def test_should_stop_returns_false_on_first_iteration(self):\n        self.assertFalse(self.reloader.should_stop()) \n", "    def test_should_stop_returns_false_if_no_events(self):\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[]):\n            with mock.patch.object(self.reloader, 'snapshot_files') as mocked_snapshot:\n                mocked_snapshot.return_value = {}\n                self.assertFalse(self.reloader.should_stop())\n", "    def test_should_stop_returns_true_when_no_changes(self):\n        with mock.patch.object(self.reloader, 'snapshot_files', return_value={self.existing_file: (1, 1)}):\n            self.assertTrue(self.reloader.should_stop())\n", "    def test_tick_with_no_changes(self):\n        with mock.patch.object(os, 'stat') as mocked_stat:\n            mocked_stat.return_value = (\n                os.stat_result(\n                    st_mtime=1678886400, st_ino=1234567890\n                )\n            )\n            self.reloader.snapshot_files()\n            with self.tick_twice():\n                pass  \n            self.assertEqual(self.reloader.should_reload(), False) \n\n", "    def test_restart_subprocess(self):\n        with mock.patch('subprocess.run') as mocked_run:\n            self.reloader._restart_subprocess()\n            mocked_run.assert_called_once_with(['/usr/bin/python', '-Wall', '-m', 'django', 'runserver'],\n                                                check=True)\n"], "sample_806": ["def test_loss_functions_with_validation_fraction(loss):\n    X, y = make_classification(n_samples=1000, random_state=0)\n    gbc = GradientBoostingClassifier(loss=loss, n_estimators=100,\n                n_iter_no_change=10, validation_fraction=0.1,\n                learning_rate=0.1, max_depth=3, random_state=42)\n\n    gbc.fit(X, y)\n\n\n\n", "def test_gradient_boosting_scoring(scoring):\n    X, y = make_regression(random_state=42)\n    gb = GradientBoostingRegressor(n_estimators=10, random_state=42)\n    gb.set_params(scoring=scoring)\n    gb.fit(X, y)\n    assert scoring in gb.get_params().keys()\n    assert isinstance(gb.best_score_, (float, int))\n\n", "def test_missing_values_handling_in_fitting():\n    # Test handling of missing values in the training data through setting \n    # the `max_features` parameter.\n    X_with_missing = np.array([[1, 2, None],\n                              [4, 5, 6],\n                              [7, None, 9]])\n    y = np.array([0, 1, 0])\n\n    gb = GradientBoostingClassifier(max_features=0.8, random_state=42)\n\n    gb.fit(X_with_missing, y) \n\n    # Check that the model is able to fit the data with missing values and \n    # that the number of features used is consistent with the max_features\n    # parameter.\n\n\n\n", "def test_missing_values(estimator):\n    X = np.array([[1, 2, np.nan], [4, 5, 6], [7, 8, np.nan]])\n    y = np.array([0, 1, 1])\n    estimator = estimator()\n    # check if it raises ValueError if missing values are not handled\n    with pytest.raises(ValueError, match='missing values.*'\n                       'not handled'):\n        estimator.fit(X, y)\n    # check if it raises ValueError if missing values are not handled\n    with pytest.raises(ValueError, match='missing values.*'\n                       'not handled'):\n        estimator.fit(X, y, sample_weight=np.ones(X.shape[0]))\n\n\n\n", "def test_gradient_boosting_oob_score_with_class_weight(GBEstimator):\n    X, y = make_classification(n_samples=100, random_state=0,\n                                n_classes=2, weights=[0.8, 0.2])\n    gb = GBEstimator(n_estimators=100, oob_score=True,\n                     class_weight='balanced', random_state=0)\n    gb.fit(X, y)\n    assert_almost_equal(gb.oob_score_, gb.score(X, y))\n\n\n"], "sample_1066": ["compilation error", "compilation error", "def test_print_Hypergeometric2F1():\n    assert mathml(Hypergeometric2F1(a, b, c, x), printer='presentation') == \\\n        '<mrow><mmultiscripts><mi>F</mi><mprescripts></mprescripts><mn>2</mn><none></none></mmultiscripts><mfenced><mrow><mi>a</mi><mo>,</mo><mi>b</mi></mrow><mi>c</mi><mi>x</mi></mfenced></mrow>'\n    assert mathml(Hypergeometric2F1(a, b, c, x, y), printer='presentation') == \\\n        '<mrow><mmultiscripts><mi>F</mi><mprescripts></mprescripts><mn>2</mn><none></none></mmultiscripts><mfenced><mrow><mi>a</mi><mo>,</mo><mi>b</mi></mrow><mi>c</mi><mi>x</mi><mi>y</mi></mfenced></mrow>'\n", "def test_print_sympy_functions():\n    from sympy import sin, cos, tan, exp, log, sqrt, abs, arg\n    assert mathml(sin(x), printer='presentation') == '<mi>sin</mi><mfenced><mi>x</mi></mfenced>'\n    assert mathml(cos(x), printer='presentation') == '<mi>cos</mi><mfenced><mi>x</mi></mfenced>'\n    assert mathml(tan(x), printer='presentation') == '<mi>tan</mi><mfenced><mi>x</mi></mfenced>'\n    assert mathml(exp(x), printer='presentation') == '<mi>exp</mi><mfenced><mi>x</mi></mfenced>'\n    assert mathml(log(x), printer='presentation') == '<mi>log</mi><mfenced><mi>x</mi></mfenced>'\n    assert mathml(sqrt(x), printer='presentation') == '<mi>sqrt</mi><mfenced><mi>x</mi></mfenced>'\n    assert mathml(abs(x), printer='presentation') == '<mi>abs</mi><mfenced><mi>x</mi></mfenced>'\n    assert mathml(arg(x), printer='presentation') == '<mi>arg</mi><mfenced><mi>x</mi></mfenced>'\n", "def test_print_KroneckerProduct():\n    from sympy.matrices import kroneckerproduct\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    assert mathml(kroneckerproduct(A, B), printer='presentation') == \\\n        '<mrow><mi>A</mi><mo>&#x22C5;</mo><mi>B</mi></mrow>'\n\n\n"], "sample_495": ["    def test_paginator_with_empty_query_set(self):\n        paginator = Paginator(Article.objects.none(), 5)\n        self.assertEqual(paginator.count, 0)\n        self.assertEqual(paginator.num_pages, 0)\n        self.assertEqual(paginator.page_range, ())\n        with self.assertRaises(EmptyPage):\n            paginator.page(1)\n\n", "    def test_paginator_is_not_cached(self):\n        \"\"\"\n        Each time Paginator is called with the same arguments\n        a new instance should be returned.\n        \"\"\"\n        paginator1 = Paginator(Article.objects.order_by('id'), 5)\n        paginator2 = Paginator(Article.objects.order_by('id'), 5)\n        self.assertNotEqual(paginator1, paginator2)\n\n\n\n", "    def test_paginator_with_empty_object_list_and_no_orphans(self):\n        paginator = Paginator([], 5, orphans=0)\n        self.assertEqual(0, paginator.count)\n        self.assertEqual(0, paginator.num_pages)\n        self.assertEqual(paginator.page_range, [])\n        with self.assertRaises(EmptyPage):\n            paginator.page(1)\n", "    def test_paginating_queryset_with_order_by_on_non_model_field(self):\n        class MyModel(models.Model):\n            name = models.CharField(max_length=100)\n            created_at = models.DateTimeField(auto_now_add=True)\n\n        MyModel.objects.create(name='test1', created_at=datetime(2000, 1, 1))\n        MyModel.objects.create(name='test2', created_at=datetime(2001, 1, 1))\n        MyModel.objects.create(name='test3', created_at=datetime(2002, 1, 1))\n\n        paginator = Paginator(MyModel.objects.order_by('created_at'), 2)\n        p = paginator.page(1)\n        self.assertQuerysetEqual(p.object_list, ['test1', 'test2'])\n", "    def test_paginator_with_complex_object_list(self):\n        class ComplexObject:\n                self.value = value\n\n        objects = [\n            ComplexObject(i) for i in range(1, 11)\n        ]\n        paginator = Paginator(objects, 5)\n        self.assertEqual(paginator.count, 10)\n        self.assertEqual(paginator.num_pages, 2)\n        page_one = paginator.page(1)\n        self.assertEqual(len(page_one), 5)\n        self.assertEqual([obj.value for obj in page_one], [1, 2, 3, 4, 5])\n        page_two = paginator.page(2)\n        self.assertEqual(len(page_two), 5)\n        self.assertEqual([obj.value for obj in page_two], [6, 7, 8, 9, 10])\n\n\n"], "sample_1200": ["def test_issue_24737():\n    from sympy.physics.units import Quantity, meter, second, kilogram\n\n    u = Quantity('u', dimension=meter**2/second**2)\n    v = Quantity('v', dimension=meter/second)\n    w = Quantity('w', dimension=kilogram)\n\n    assert u/v == Quantity('u/v', dimension=meter)\n    assert u*v == Quantity('u*v', dimension=meter**2/second)\n    assert u/w == Quantity('u/w', dimension=meter**2/(second**2*kilogram))\n", "def test_issue_24895():\n    from sympy.physics.units import Quantity, inch\n\n    q = Quantity(\"q\")\n    SI.set_quantity_dimension(q, length)\n    SI.set_quantity_scale_factor(q, 1, inch)\n    assert q.get_base_units() == inch\n    assert str(q) == \"q [inch]\" \n\n\n\n", "def test_issue_25614():\n    from sympy.physics.units import speed_of_light\n\n    q = Quantity('q')\n    v = Quantity('v')\n    SI.set_quantity_dimension(q, speed_of_light.dimension)\n    SI.set_quantity_scale_factor(q, speed_of_light.scale_factor)\n\n    expr = q * 1/speed_of_light\n    assert SI._collect_factor_and_dimension(expr)[0] == 1 * speed_of_light\n", "def test_issue_19948():\n    from sympy.physics.units import Quantity\n    m = Quantity(\"m\")\n    SI.set_quantity_dimension(m, length)\n    SI.set_quantity_scale_factor(m, 1)\n    n = Quantity(\"n\")\n    SI.set_quantity_dimension(n, length)\n    SI.set_quantity_scale_factor(n, 1)\n    expr = (m/n)**2\n    assert SI._collect_factor_and_dimension(expr) == (1, length**(-2))\n", "def test_issue_24677():\n    from sympy.physics.units import mass, length, time, speed_of_light\n    from sympy.physics.units.dimensions import Dimension\n\n    symbol = Symbol(\"v\")\n    m = Quantity(\"m\")\n    l = Quantity(\"l\")\n    t = Quantity(\"t\")\n    SI.set_quantity_dimension(m, mass)\n    SI.set_quantity_dimension(l, length)\n    SI.set_quantity_dimension(t, time)\n    m.set_global_relative_scale_factor(1, kilogram)\n    l.set_global_relative_scale_factor(1, meter)\n    t.set_global_relative_scale_factor(1, second)\n\n    expr = (m*l**2*speed_of_light)/(t**2)\n\n    SI._collect_factor_and_dimension(expr)\n    expected_dimension = (mass * length**2 * speed_of_light) / (time**2)\n    assert SI.get_dimensional_expr(expr).simplify() == expected_dimension\n\n\n\n"], "sample_100": ["    def test_run_loop_with_file_change(self):\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file]):\n            with mock.patch.object(self.reloader, 'snapshot_files', return_value={self.existing_file: 1}) as mocked_snapshot_files:\n                with mock.patch.object(self.reloader, 'notify_file_changed') as mock_notify_file_changed:\n                    ticker = self.reloader.tick()\n                    next(ticker)\n                    self.increment_mtime(self.existing_file)\n                    next(ticker)\n                    self.assertEqual(mock_notify_file_changed.call_count, 1)\n                    self.assertEqual(mocked_snapshot_files.call_count, 1)\n\n\n\n", "    def test_notify_file_changed_calls_os_stat(self, mock_watched_files):\n        with mock.patch('os.stat') as mock_stat:\n            self.reloader.notify_file_changed(self.existing_file)\n            mock_stat.assert_called_once_with(str(self.existing_file))\n\n\n", "    def test_notify_file_changed(self):\n        with mock.patch.object(self.reloader, '_notify_file_changed') as mocked_notify:\n            self.reloader.notify_file_changed(self.existing_file)\n            mocked_notify.assert_called_once_with(self.existing_file)\n", "    def test_run_with_existing_and_new_files(self):\n        non_existing_file = self.ensure_file(self.tempdir / 'non_existing.py')\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file, non_existing_file]):\n            self.reloader.run_loop()\n\n\n", "    def test_tick_handles_multiple_changes(self, mock_notify_file_changed):\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file, self.nonexistent_file]):\n            ticker = self.reloader.tick()\n            self.increment_mtime(self.existing_file)\n            self.increment_mtime(self.existing_file)\n            next(ticker)\n            self.assertEqual(mock_notify_file_changed.call_count, 1)\n\n"], "sample_470": ["    def test_keep_lazy(self):\n        @keep_lazy(int)\n            return x + 1\n\n        self.assertEqual(some_func(1), 2)\n\n        @keep_lazy(int, str)\n            return x + y\n        self.assertEqual(some_other_func(1, 2), 3)\n        self.assertEqual(some_other_func(1, \"2\"), \"12\")\n\n", "    def test_simple_lazy_object(self):\n        class Klazz:\n                self.x = x\n\n        t = SimpleLazyObject(lambda: Klazz(10))\n        self.assertEqual(t._wrapped.x, 10)\n        self.assertEqual(t._setupfunc(), Klazz(10))\n\n", "    def test_keep_lazy(self):\n            return lazy(func)\n\n        @keep_lazy\n            return arg1 + arg2\n\n        @keep_lazy(str)\n            return arg\n\n        # Test with all arguments lazy\n        self.assertEqual(\n            func_with_lazy(lazy_factory(lambda: 1), lazy_factory(lambda: 2)),\n            3,\n        )\n\n        # Test with some arguments lazy\n        self.assertEqual(\n            func_with_lazy(1, lazy_factory(lambda: 2)), 3\n        )\n\n        # Test with only a lazy string argument\n        self.assertEqual(lazy_str_func(\"hello\") + \" world\", \"hello world\")\n\n        # Test with a single lazy function as an argument\n        self.assertEqual(func_with_lazy(lazy_factory(lambda: 1)), 1)\n\n\n", "    def test_lazy_simple_object(self):\n        class Klazz:\n                self.value = value\n\n            return Klazz(value)\n\n        lazy_obj = SimpleLazyObject(create_klazz)\n        self.assertIsInstance(lazy_obj._wrapped, Klazz)\n        self.assertEqual(lazy_obj._wrapped.value, None)\n\n        lazy_obj2 = SimpleLazyObject(create_klazz)(10)\n        self.assertIsInstance(lazy_obj2, Klazz)\n        self.assertEqual(lazy_obj2.value, 10)\n", "    def test_lazy_object_to_int(self):\n        class Klazz:\n                return 123\n\n        t = lazy(lambda: Klazz(), Klazz)()\n        self.assertEqual(int(t), 123)\n\n\n\n"], "sample_947": ["def test_build_duplicate_member_ref(app, warning):\n    text = \"\"\"", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_994": ["compilation error", "def test_issue_10803():\n    assert Float(oo).is_positive == True\n    assert Float(-oo).is_positive == False\n    assert Float(zoo).is_positive == None\n", "compilation error", "def test_issue_10639():\n    assert Float('1.234E123').evalf() == 1.234e123\n    assert Float('1.234e-123').evalf() == 1.234e-123\n", "def test_issue_10603():\n    assert Float(E**-oo).is_zero is True\n\n\n\n"], "sample_419": ["    def test_custom_template_overrides_no_management_form_warning(self):\n        with isolate_lru_cache(get_default_renderer), self.settings(\n            FORM_RENDERER=\"django.forms.renderers.DjangoTemplates\"\n        ):\n            class MyFormSet(BaseFormSet):\n                template_name = 'some/custom/template.html'\n\n            ChoiceFormSet = formset_factory(Choice, formset=MyFormSet)\n            formset = ChoiceFormSet()\n            str(formset.management_form)\n", "    def test_extra_forms_with_can_delete(self):\n        ChoiceFormset = formset_factory(Choice, extra=1, can_delete=True)\n        formset = ChoiceFormset(data={\n            \"form-TOTAL_FORMS\": \"2\",\n            \"form-INITIAL_FORMS\": \"1\",\n            \"form-MIN_NUM_FORMS\": \"0\",\n            \"form-0-choice\": \"Zero\",\n            \"form-0-votes\": \"0\",\n            \"form-1-choice\": \"One\",\n            \"form-1-votes\": \"1\",\n            \"form-2-DELETE\": \"on\",\n        }, prefix=\"form\")\n        self.assertEqual(formset.cleaned_data, [{\"choice\": \"Zero\", \"votes\": 0}, {\"choice\": \"One\", \"votes\": 1}, {}])\n        self.assertEqual(formset.total_form_count(), 2)\n        self.assertEqual(formset.initial_form_count(), 1)\n\n\n", "    def test_no_auto_id(self):\n        \"\"\"\n        Test that formset fields are rendered without auto_ids\n        when auto_id is not set in the formset.\n        \"\"\"\n        data = {\"choices-TOTAL_FORMS\": \"1\", \"choices-INITIAL_FORMS\": \"0\"}\n        ChoiceFormSet = formset_factory(Choice, auto_id=False)  \n        formset = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n        rendered_html = str(formset)\n        self.assertFalse(\"id\" in rendered_html) \n", "    def test_custom_renderer(self):\n        from django.forms.renderers import Jinja2\n\n        renderer = Jinja2()\n        data = {\n            \"choices-TOTAL_FORMS\": \"2\",\n            \"choices-INITIAL_FORMS\": \"0\",\n            \"choices-MIN_NUM_FORMS\": \"0\",\n            \"choices-0-choice\": \"Zero\",\n            \"choices-0-votes\": \"\",\n            \"choices-1-choice\": \"One\",\n            \"choices-1-votes\": \"\",\n        }\n        ChoiceFormSet = formset_factory(Choice, renderer=renderer)\n        formset = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n        self.assertEqual(formset.renderer, renderer)\n        self.assertEqual(formset.forms[0].renderer, renderer)\n        self.assertEqual(formset.management_form.renderer, renderer)\n        self.assertEqual(formset.non_form_errors().renderer, renderer)\n        self.assertEqual(formset.empty_form.renderer, renderer)\n\n", "    def test_empty_formset_is_bound_after_full_clean(self):\n        formset = ArticleFormSet(data={\"form-TOTAL_FORMS\": \"0\"})\n        self.assertFalse(formset.is_bound)\n        formset.full_clean()\n        self.assertTrue(formset.is_bound)\n"], "sample_528": ["def test_reload_library():\n    original_library = style.library.copy()\n    with TemporaryDirectory() as tmpdir:\n        style_path = Path(tmpdir, 'my_style.mplstyle')\n        style_path.write_text('font.size: 18', encoding='utf-8')\n        USER_LIBRARY_PATHS.append(tmpdir)\n        style.reload_library()\n        assert style.library != original_library\n        assert 'my_style' in style.library\n        assert style.library['my_style']['font.size'] == 18\n", "def test_reload_library():\n    original_library = style.library.copy()\n    with temp_style('_test_', DUMMY_SETTINGS):\n        style.reload_library()\n        assert '_test_' in style.library\n    style.reload_library()\n    assert style.library == original_library\n\n\n\n", "def test_reload_library():\n    original_library = style.library.copy()\n    new_style = {'test_param': 'test_value'}\n    with temp_style('test_style', new_style):\n        reload_library()\n        assert 'test_param' in style.library and style.library['test_param'] == new_style\n    assert style.library == original_library\n", "def test_reload_library():\n    with temp_style('test', DUMMY_SETTINGS):\n        existing_styles = style.library.keys()\n    style.reload_library()\n    assert 'test' in style.library.keys()\n    assert set(existing_styles) != set(style.library.keys())\n\n\n\n\n", "def test_load_base_library():\n    with mpl.style.context('default'):\n        original_library = style.library.copy() \n        style.load_base_library()\n        assert style.library != original_library\n"], "sample_880": ["compilation error", "compilation error", "def test_class_distribution_multilabel():\n    # Test for multilabel classification\n\n    y = np.array(\n        [[0, 1], [1, 0], [0, 1, 1], [1, 1, 1], [0, 1, 0]]\n    )\n    (classes, n_classes, class_prior) = class_distribution(y)\n\n    classes_expected = [[0, 1], [0, [], 1], [0, 1, 1], [0, 1, 1], [0, 1]]\n    n_classes_expected = [2, 2, 3, 3, 2]\n\n    for k in range(y.shape[1]):\n        assert_array_equal(classes[k], classes_expected[k])\n        assert_array_equal(n_classes[k], n_classes_expected[k])\n\n\n", "compilation error", "def test_is_multilabel_multioutput():\n    for group, group_examples in EXAMPLES.items():\n        if group in [\"multilabel-indicator\", \"multiclass-multioutput\"]:\n            for example in group_examples:\n                assert is_multilabel(example) == (group == \"multilabel-indicator\")\n                assert is_multioutput(example) == (group == \"multiclass-multioutput\")\n\n"], "sample_668": ["def test_funcargnames_deprecation_warning():\n    with pytest.warns(deprecated.FUNCARGNAMES) as w:\n        pytest.run(\"/path/to/some/fixture_file.py\",  # Replace with an actual path\n                   \"--verbose\")\n    assert len(w) == 1\n", "def test_deprecated_function_argname_warning(testdir, function_argname):\n    testdir.makepyfile(\n        \"\"\"\n            pytest.funcargnames()\n    \"\"\"\n    )\n    with pytest.warns(deprecated.FUNCARGNAMES):\n        result = testdir.runpytest()\n    result.stdout.fnmatch_lines([f\"*The '{function_argname}' attribute was an alias for 'fixturenames', since pytest 2.3 - use the newer attribute instead.*\"])\n", "def test_external_plugins_disabled(testdir, plugin):\n    testdir.syspathinsert()\n    testdir.makepyfile(**{plugin: \"\"})\n\n    with pytest.warns(pytest.PytestConfigWarning):\n        testdir.parseconfig(\"-p\", plugin, \"--disable-pytest-plugins\")\n", "def test_deprecated_funcargnames_warning(testdir, arg):\n    testdir.makepyfile(\n        \"\"\"\n            pass\n    \"\"\"\n    )\n\n    with pytest.warns(deprecated.FUNCARGNAMES) as w:\n        result = testdir.runpytest(f\"-{arg}=test_example\")\n    assert len(w) == 1\n    assert str(w[0].message) == \"The `funcargnames` attribute was an alias for `fixturenames`, since pytest 2.3 - use the newer attribute instead.\"\n", "compilation error"], "sample_839": ["def test_vectorizer_custom_stop_words_list():\n    cv = CountVectorizer(stop_words=['test', 'another_word'])\n    X = cv.fit_transform(['This is a test document', 'another_word here'])\n    assert_array_equal(X.toarray(),\n                       [[0, 0, 1, 1, 0],\n                        [0, 1, 0, 0, 1]])\n\n\n\n\n", "def test_vectorizer_encoding(vectorizer_class):\n    data = ['Ceci est un test', 'Test de l\\'encodage', 'Hola mundo']\n    vectorizer = vectorizer_class(encoding='latin-1')\n    X = vectorizer.fit_transform(data)\n\n    assert X.shape[0] == len(data)\n    assert isinstance(X, sparse.csr_matrix)  \n\n\n\n", "def test_vectorizer_with_stop_words_and_custom_tokenizer():\n    data = ['This is a test document.', 'This is another document.']\n    vec = Estimator(stop_words='english',\n                     tokenizer=lambda doc: [w for w in doc.split() if\n                                           w.lower() not in ['the', 'is']])\n    X = vec.fit_transform(data)\n    assert_array_equal(vec.get_feature_names_out(),\n                       ['test', 'document', 'another'])\n\n\n\n", "def test_custom_analyzer_with_stop_words(Estimator):\n    if issubclass(Estimator, HashingVectorizer):\n        pytest.xfail('HashingVectorizer is not supported on PyPy')\n    data = ['this is a test']\n    stop_words = ['is']\n\n        tokens = [token for token in doc.split() if token not in stop_words]\n        return tokens\n\n    vec = Estimator(analyzer=custom_analyzer, stop_words=stop_words)\n    X = vec.fit_transform(data)\n\n    assert len(X.toarray()[0]) == 2\n    assert X.toarray()[0][0] == 1\n    assert X.toarray()[0][1] == 1\n\n\n\n", "def test_fit_transform_empty_dataset(Estimator):\n    if issubclass(Estimator, HashingVectorizer):\n        pytest.xfail('HashingVectorizer is not supported on PyPy')\n\n    # check that fit_transform handles empty dataset correctly\n    vec = Estimator()\n    result = vec.fit_transform([])\n    assert result.shape == (0, 0)\n\n\n\n"], "sample_844": ["def test_xi_parameter():\n    # test that 'xi' parameter controls density-function\n    centers = [[1, 1], [-1, -1], [1, -1]]\n    X, labels_true = make_blobs(n_samples=750, centers=centers,\n                cluster_std=0.4, random_state=0)\n    \n    # Run OPTICS with different xi values\n    clust_xi_0 = OPTICS(min_samples=10, cluster_method='xi', xi=0.0).fit(X)\n    clust_xi_0p5 = OPTICS(min_samples=10, cluster_method='xi', xi=0.5).fit(X)\n    clust_xi_1 = OPTICS(min_samples=10, cluster_method='xi', xi=1.0).fit(X)\n\n    # Compare cluster sizes for different xi values\n    labels_0 = clust_xi_0.labels_\n    labels_0p5 = clust_xi_0p5.labels_\n    labels_1 = clust_xi_1.labels_\n\n    assert set(np.unique(labels_0)).issubset(set(np.unique(labels_0p5)))\n    assert set(np.unique(labels_0p5)).issubset(set(np.unique(labels_1)))\n    # xi = 0 should produce the most clusters\n    # xi = 1 should produce the least\n    assert np.sum(labels_0 != -1) > np.sum(labels_1 != -1)\n    assert np.sum(labels_1 != -1) > np.sum(labels_0p5 != -1)\n", "def test_xi_parameter():\n    # Test how xi parameter affects clustering with varying densities\n    rng = np.random.RandomState(0)\n    n_points_per_cluster = 20\n    C1 = [-5, -2] + 0.6 * rng.randn(n_points_per_cluster, 2)\n    C2 = [4, -1] + 0.6 * rng.randn(n_points_per_cluster, 2)\n    X = np.vstack((C1, C2))\n\n    # Use xi=0.1, which is close to the default value.\n    clust1 = OPTICS(min_samples=5, xi=0.1).fit(X)\n    # Use xi=0.4, which will have a stronger impact on density-based\n    # separation\n    clust2 = OPTICS(min_samples=5, xi=0.4).fit(X)\n\n    # Check that xi=0.4 clusters more strictly, and potentially\n    # separates the clusters more completely\n    assert_array_equal(np.sort(np.unique(clust1.labels_)), [0, 1])\n    assert_array_equal(np.sort(np.unique(clust2.labels_)), [0, 1])\n\n    # Further validation might involve comparing reachability distances and\n    # examining cluster sizes.\n\n\n\n", "def test_xi_parameter():\n    # Test xi=0 to 1 and its effect on results\n    X = gen_clustered_data(3, 20, 0.5)\n    for xi in range(10):\n        xi /= 10\n        clust1 = OPTICS(xi=xi, min_samples=10).fit(X)\n        clust2 = OPTICS(xi=xi + 0.1, min_samples=10).fit(X)\n        assert_array_not_equal(clust1.labels_, clust2.labels_)\n", "def test_xi_parameter():\n    # Test that the xi parameter works correctly\n    centers = [[1, 1], [-1, -1], [1, -1]]\n    X, labels_true = make_blobs(n_samples=750, centers=centers,\n                                cluster_std=0.4, random_state=0)\n    clust1 = OPTICS(min_samples=10, xi=0.2).fit(X)\n    clust2 = OPTICS(min_samples=10, xi=0).fit(X)\n    clust3 = OPTICS(min_samples=10, xi=0.8).fit(X)\n\n    assert_array_equal(clust1.labels_, clust2.labels_)\n    assert_array_equal(clust1.labels_, clust3.labels_)\n    \n    # Test a case where xi might be more important\n    centers = [[1, 1], [-1, 1], [1, -1]]\n    X, labels_true = make_blobs(n_samples=750, centers=centers,\n                                cluster_std=0.4, random_state=0)\n\n    clust1 = OPTICS(min_samples=10, xi=0.3).fit(X)\n    clust2 = OPTICS(min_samples=10, xi=0.1).fit(X)\n\n    assert_array_equal(clust1.labels_, clust2.labels_)\n\n\n\n", "def test_xi():\n    # test xi parameter with different values\n    centers = [[1, 1], [-1, -1], [1, -1]]\n    X, labels_true = make_blobs(n_samples=750, centers=centers,\n                cluster_std=0.4, random_state=0)\n    \n    # test xi value close to 0\n    clust_xi_0 = OPTICS(min_samples=10, cluster_method='dbscan',\n                        xi=0.01).fit(X)\n    clust_xi_0_labels = clust_xi_0.labels_\n    assert_array_equal(clust_xi_0_labels, labels_true) \n\n    # test xi value close to 1\n    clust_xi_1 = OPTICS(min_samples=10, cluster_method='dbscan',\n                        xi=0.99).fit(X)\n    clust_xi_1_labels = clust_xi_1.labels_\n    assert_array_equal(clust_xi_1_labels, labels_true) \n\n    # test xi value > 1\n    with pytest.raises(ValueError, match=\"xi must be in the range [0, 1]\"):\n        OPTICS(min_samples=10, cluster_method='dbscan', xi=1.1).fit(X)   \n\n    # test xi value < 0\n    with pytest.raises(ValueError, match=\"xi must be in the range [0, 1]\"):\n        OPTICS(min_samples=10, cluster_method='dbscan', xi=-0.1).fit(X)   \n"], "sample_607": ["def test_list_engines_empty( ):\n    with pytest.raises(ValueError, match=r\"xarray is unable to open\"):\n        plugins.list_engines()    \n", "def test_get_backend_raises_for_invalid_engine():\n    with pytest.raises(ValueError, match=r\"unrecognized engine\"):\n        plugins.get_backend(\"invalid_engine\")\n", "def test_get_backend():\n    engines = plugins.list_engines()\n    backend1 = plugins.get_backend(\"engine_1\")\n    assert isinstance(backend1, DummyBackendEntrypoint1)\n    backend2 = plugins.get_backend(\"engine_2\")\n    assert isinstance(backend2, DummyBackendEntrypoint2)\n\n    with pytest.raises(ValueError, match=r\"unrecognized engine\"):\n        plugins.get_backend(\"unknown_engine\")\n\n    with pytest.raises(TypeError, match=r\"engine must be\"):\n        plugins.get_backend(DummyBackendEntrypoint1)\n\n\n\n", "def test_list_engines_empty(mock_iter_entry_points):\n    engines = plugins.list_engines()\n    assert not engines\n\n", "def test_guess_engine_engine_ordering(\n    mock_load_entry_point,"], "sample_115": ["    def test_sensitive_variables_decorator_multiple_calls(self):\n        @sensitive_variables\n            return password\n\n        with self.assertRaises(ValueError):\n            test_func(\"secret\")\n        \n        @sensitive_variables\n            return password, username\n\n        with self.assertRaises(ValueError):\n            test_func(\"secret\", \"admin\")\n\n\n", "    def test_sensitive_variables_decorator_with_function_args(self):\n        @sensitive_variables\n            return HttpResponse('Hello, %s!' % password)\n\n        with self.settings(DEBUG=True):\n            response = self.client.get('/test/', {'password': 'my_password'})\n            self.assertContains(response, 'my_password', status_code=200)\n", "    def test_sensitive_variables_decorator_in_function_with_local_scope(self):\n        @sensitive_variables\n            local_secret = secret\n            return local_secret\n\n        with self.settings(DEBUG=True):\n            response = self.client.get('/raises500/', HTTP_SECRET_HEADER='super_secret')\n            self.assertNotContains(response, 'super_secret', status_code=500)\n", "    def test_sensitive_variables_decorator_with_args(self):\n        @sensitive_variables(ignore_args=['sausage'])\n            return password + sausage + bacon\n\n        with self.settings(DEBUG=True):\n            response = self.client.post('/test/', data={'password': 'secret', 'sausage': 'sausage', 'bacon': 'bacon'})\n            self.assertIn(b'secret' + b'sausage' + b'bacon', response.content)\n        \n        with self.settings(DEBUG=False):\n            response = self.client.post('/test/', data={'password': 'secret', 'sausage': 'sausage', 'bacon': 'bacon'})\n            self.assertIn(b'XXXXXXXXXXXXXXXXXXXX' + b'sausage' + b'bacon', response.content) \n", "    def test_sensitive_variables_decorator_with_callable_settings(self):\n            return \"This should not be displayed\"\n        @sensitive_variables\n            return setting_value\n\n        with self.settings(DEBUG=True, MY_SENSITIVE_SETTING=callable_setting):\n            self.assertEqual(test_func(MY_SENSITIVE_SETTING()), callable_setting().__name__)\n        with self.settings(DEBUG=False, MY_SENSITIVE_SETTING=callable_setting):\n            self.assertEqual(test_func(MY_SENSITIVE_SETTING()), callable_setting().__name__)\n\n\n\n"], "sample_294": ["    def test_csrf_failure_view_called_with_invalid_token(self):\n        response = self.client.post('/test/', {'csrfmiddlewaretoken': 'invalid'})\n        self.assertContains(response, 'CSRF verification failed')\n", "    def test_csrf_error_handler_accepts_valid_token(self):\n        self.client.login(username='user', password='password')\n        response = self.client.get('/csrf_error/')\n        self.assertEqual(response.status_code, 200)\n", "    def test_csrf_token_in_bad_request_response(self):\n        response = self.client.post('/invalid/', data={'csrfmiddlewaretoken': 'some_invalid_token'})\n        self.assertEqual(response.status_code, 400)\n        self.assertIn('CSRF verification failed.', response.content.decode('ascii')) \n", "    def test_csrf_token_error_handler_empty_request(self):\n        \"\"\"\n        The CSRF token in the error handler is also sent when the request is empty\n        \"\"\"\n        response = self.client.get('/does not exist/', HTTP_HOST='www.example.com')\n        self.assertEqual(response.status_code, 599)\n        token = response.content\n\n        # Make an empty request\n        empty_response = self.client.post('/does not exist/', HTTP_HOST='www.example.com', data='')\n        self.assertEqual(empty_response.status_code, 599)\n        empty_token = empty_response.content\n\n        self.assertTrue(equivalent_tokens(token.decode('ascii'), empty_token.decode('ascii')))\n\n\n\n", "    def test_csrf_token_on_timeout_stays_constant(self):\n        response = self.client.get('/timeout/')\n        # The error handler returns status code 599.\n        self.assertEqual(response.status_code, 599)\n        token1 = response.content\n        response = self.client.get('/timeout/')\n        self.assertEqual(response.status_code, 599)\n        token2 = response.content\n        self.assertTrue(equivalent_tokens(token1.decode('ascii'), token2.decode('ascii')))\n"], "sample_628": ["    def test_skip_words_with_underscores_in_docstring(self):\n        stmt = astroid.extract_node(\n            'class TestClass(object):\\n   \"\"\"This is a test with an underscore_word in it\"\"\"\\n   pass'\n        )\n        self.checker.visit_classdef(stmt)\n        assert self.linter.release_messages() == []\n", "    def test_docstring_lines_that_look_like_comments_7(self):\n        stmt = astroid.extract_node(\n            '''def f():", "    def test_docstring_lines_that_look_like_comments_7(self):\n        stmt = astroid.extract_node(\n            '''def f():", "    def test_docstring_lines_that_look_like_comments_7(self):\n        stmt = astroid.extract_node(\n            '''def f():", "    def test_docstring_lines_that_look_like_comments_7(self):\n        stmt = astroid.extract_node(\n            '''def f():"], "sample_831": ["def test_export_text_multiple_outputs():\n    clf = DecisionTreeClassifier(max_depth=2, random_state=0,\n                n_classes=3)\n    clf.fit(X, y2)\n\n    expected_report = dedent(\"\"\"\n    |--- feature_1 <= 0.00\n    |   |--- value: [0.00, 0.00, 3.00] class: 0\n    |--- feature_1 >  0.00\n    |   |--- value: [0.00, 3.00, 0.00] class: 1\n    \"\"\").lstrip()\n    assert export_text(clf) == expected_report\n\n\n\n", "compilation error", "compilation error", "def test_export_text_regression():\n    reg = DecisionTreeRegressor(max_depth=2, random_state=0)\n    reg.fit(X, y)\n    expected_report = dedent(\"\"\"\n    |--- feature_0 <= 0.0\n    |   |--- value: [0.0, 0.0]\n    |--- feature_0 >  0.0\n    |   |--- value: [1.0, 1.0]\n    \"\"\").lstrip()\n    assert export_text(reg) == expected_report\n\n", "def test_export_text_with_missing_data():\n    X = [['a', 1], ['b', 2], ['a', None], ['b', None]]\n    y = [0, 1, 0, 1]\n\n    clf = DecisionTreeClassifier()\n    clf.fit(X, y)\n\n    report = export_text(clf)\n    assert \"missing\" in report\n\n\n"], "sample_1207": ["def test_issue_17692():\n    x = Symbol('x')\n    assert parse_expr('2**x*x**2', evaluate=False) == 2**x * x**2\n", "def test_issue_12634():\n    x = Symbol('x')\n    y = Symbol('y')\n    assert parse_expr('(x + y)**2') == (x + y)**2\n", "def test_issue_23456():\n    x = Symbol('x')\n    y = Symbol('y')\n    assert parse_expr('x + y + 1', evaluate=False) == x + y + 1\n    assert parse_expr('1 + x + y', evaluate=False) == 1 + x + y\n\n", "compilation error", "def test_issue_19047():\n    x = Symbol('x')\n    y = Symbol('y')\n    assert parse_expr('sin(1/x)').diff(x) == -cos(1/x)/x**2\n    assert parse_expr('cos(1/x)').diff(x) == -sin(1/x)/x**2\n    assert parse_expr('x*sin(y)').diff(y) == x*cos(y)\n    assert parse_expr('x**y').diff(x, y) ==  y*x**(y-1)\n    assert parse_expr('x**y').diff(y, x) == x**y * ln(x) \n"], "sample_650": ["def test_log_file_level_override(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n        logger.setLevel(logging.WARNING)\n\n            logger.info('This message should be suppressed')\n            assert True\n        \"\"\"\n    )\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        log_file = pytest.log\n        log_file_level = INFO\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines(\n        [\"INFO     test_log_file_level_override:test_first:7 This message should be suppressed\"]\n    )\n", "def test_timestamp_format_custom(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            logger.warning('text')\n            assert False\n        \"\"\"\n    )\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        log_format=%(asctime)s; %(levelname)s; %(message)s\n        log_date_format=%d/%m/%Y %H:%M:%S\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    assert result.ret == 1\n    result.stdout.re_match_lines([r\"^[0-9]{2}/[0-9]{2}/[0-9]{4} [0-9:]{8}; WARNING; text\"])\n\n", "def test_log_format(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            logger.debug('debug message')\n            logger.info('info message')\n            logger.warning('warning message')\n            logger.error('error message')\n            logger.critical('critical message')\n            assert False\n        \"\"\"\n    )\n    pytester.makeini(\"\"\"\n    [pytest]\n    log_format=%(asctime)s %(levelname)-8s %(message)s\n    \"\"\")\n    result = pytester.runpytest()\n    assert result.ret == 1\n    result.stdout.fnmatch_lines([\n        *[_ for _ in result.stdout.lines if \"DEBUG\" in _],\n        *[_ for _ in result.stdout.lines if \"INFO\" in _],\n        *[_ for _ in result.stdout.lines if \"WARNING\" in _],\n        *[_ for _ in result.stdout.lines if \"ERROR\" in _],\n        *[_ for _ in result.stdout.lines if \"CRITICAL\" in _],\n    ])\n", "def test_log_level_priority(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            logger.debug('DEBUG log message')\n            logger.info('INFO log message')\n            logger.warning('WARNING log message')\n            logger.error('ERROR log message')\n            logger.critical('CRITICAL log message')\n\n            logger.info('INFO log message')\n        \"\"\"\n    )\n\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        log_level = WARNING\n        \"\"\"\n    )\n\n    result = pytester.runpytest()\n\n    result.stdout.fnmatch_lines(\n        [\n            \"*WARNING  test_log_level_priority:test_log_level_priority.py:7 WARNING log message*\",\n            \"*WARNING  test_log_level_priority:test_log_level_priority.py:11 WARNING log message*\",\n        ]\n    )\n    result.stdout.no_fnmatch_lines(\n        [\n            \"*DEBUG  test_log_level_priority:test_log_level_priority.py:4 DEBUG log message*\",\n            \"*INFO  test_log_level_priority:test_log_level_priority.py:5 INFO log message*\",\n            \"*ERROR  test_log_level_priority:test_log_level_priority.py:6 ERROR log message*\",\n            \"*CRITICAL  test_log_level_priority:test_log_level_priority.py:7 CRITICAL log message*\",\n            \"*INFO  test_log_level_priority:test_log_level_priority.py:12 INFO log message*\",\n        ]\n    )\n\n\n\n", "def test_log_date_format_with_timezone_offset(pytester: Pytester) -> None:\n    \"\"\"Ensure that timezone offset in log_date_format is correctly applied.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            logger.warning('text')\n            assert False\n        \"\"\"\n    )\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        log_format=%(asctime)s; %(levelname)s; %(message)s\n        log_date_format=%Y-%m-%d %H:%M:%S.%f %Z%z\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    assert result.ret == 1\n    result.stdout.re_match_lines(\n        [r\"^[0-9-]{10} [0-9:]{8}.[0-9]{6} [A-Z][+-]\\d{2}\\d{2}; WARNING; text\"]\n    )\n"], "sample_99": ["    def test_trunc_tz_aware_datetime_field_with_no_tzinfo(self):\n        start_datetime = datetime(2015, 6, 15, 14, 30, 50, 321)\n        end_datetime = datetime(2016, 6, 15, 14, 10, 50, 123)\n        start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n        end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n        self.create_model(start_datetime, end_datetime)\n\n        with self.assertRaises(AttributeError):\n            DTModel.objects.annotate(truncated=Trunc('start_datetime', 'day')).get()\n", "    def test_trunc_func_with_timezone_aware_datetime(self):\n        start_datetime = timezone.make_aware(datetime(2015, 6, 15, 14, 30, 50, 321), tz=pytz.utc)\n        end_datetime = timezone.make_aware(datetime(2016, 6, 15, 14, 10, 50, 123), tz=pytz.utc)\n        self.create_model(start_datetime, end_datetime)\n        self.create_model(end_datetime, start_datetime)\n\n        melb = pytz.timezone('Australia/Melbourne')\n        self.assertQuerysetEqual(\n            DTModel.objects.annotate(\n                truncated=Trunc('start_datetime', 'day', output_field=DateField(), tzinfo=melb)\n            ).order_by('start_datetime'),\n            [\n                (start_datetime, truncate_to(start_datetime.astimezone(melb).date(), 'day', melb)),\n                (end_datetime, truncate_to(end_datetime.astimezone(melb).date(), 'day', melb))\n            ],\n            lambda m: (m.start_datetime, m.truncated)\n        )\n", "    def test_trunc_func_with_timezone_and_dst_transition(self):\n        start_datetime = timezone.make_aware(datetime(2016, 3, 14, 1, 30, 50, tzinfo=pytz.timezone('US/Eastern')), is_dst=False)\n        end_datetime = timezone.make_aware(datetime(2016, 3, 14, 2, 30, 50, tzinfo=pytz.timezone('US/Eastern')), is_dst=True)\n        self.create_model(start_datetime, end_datetime)\n        self.create_model(end_datetime, start_datetime)\n\n        self.assertQuerysetEqual(\n            DTModel.objects.annotate(\n                truncated=TruncHour('start_datetime', tzinfo=pytz.timezone('US/Eastern'))\n            ).order_by('start_datetime'),\n            [\n                (start_datetime, truncate_to(start_datetime, 'hour', pytz.timezone('US/Eastern'))),\n                (end_datetime, truncate_to(end_datetime, 'hour', pytz.timezone('US/Eastern'))),\n            ],\n            lambda m: (m.start_datetime, m.truncated)\n        )\n\n\n\n", "    def test_trunc_func_with_timezone_aware_datetime_fields(self):\n        start_datetime = datetime(2015, 6, 15, 14, 30, 50, 321)\n        end_datetime = datetime(2016, 6, 15, 14, 10, 50, 123)\n        start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n        end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n        self.create_model(start_datetime, end_datetime)\n\n        melb = pytz.timezone('Australia/Melbourne')\n        pacific = pytz.timezone('US/Pacific')\n\n        class CustomModel(models.Model):\n            start_datetime = models.DateTimeField(default=start_datetime, tzinfo=timezone.utc)\n            end_datetime = models.DateTimeField(default=end_datetime, tzinfo=timezone.utc)\n\n        CustomModel.objects.create()\n\n        with timezone.override(melb):\n            model = CustomModel.objects.annotate(\n                melb_year=TruncYear('start_datetime', tzinfo=melb),\n                pacific_year=TruncYear('start_datetime', tzinfo=pacific),\n            ).get()\n            self.assertEqual(model.start_datetime, start_datetime.astimezone(melb))\n            self.assertEqual(model.melb_year.year, 2016)\n            self.assertEqual(model.pacific_year.year, 2015)\n\n\n", "    def test_trunc_timezone_aware_datetime(self):\n        start_datetime = datetime(2015, 6, 15, 14, 30, 50, 321)\n        end_datetime = datetime(2016, 6, 15, 14, 10, 50, 123)\n        start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n        end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n        self.create_model(start_datetime, end_datetime)\n        self.assertQuerysetEqual(\n            DTModel.objects.annotate(\n                truncated=Trunc('start_datetime', 'day', output_field=DateTimeField())\n            ).order_by('start_datetime'),\n            [\n                (start_datetime, truncate_to(start_datetime, 'day')),\n                (end_datetime, truncate_to(end_datetime, 'day')),\n            ],\n            lambda m: (m.start_datetime, m.truncated)\n        )\n\n\n"], "sample_277": ["    def test_resolve_expression_with_filter(self):\n        from django.db.models import ForeignKey, Model\n\n        class MyModel(Model):\n            my_field = ForeignKey('MyOtherModel')\n\n        class MyOtherModel(Model):\n            pass\n\n        q = Q(my_field__name='something')\n        query = MyModel.objects.filter(pk=1)\n        result = q.resolve_expression(query, allow_joins=True)\n        self.assertIn('my_field__name__contains', result)\n\n\n\n", "    def test_resolve_expression(self):\n        from django.db.models.query import QuerySet\n        from django.db.models import F\n\n        class Model(models.Model):\n            name = models.CharField(max_length=100)\n            price = models.DecimalField(max_digits=10, decimal_places=2)\n\n        obj = Model.objects.create(name='Example', price=100)\n        q = Q(name='Example')\n        q_resolved = q.resolve_expression(reuse={})\n        self.assertEqual(q_resolved, (\n            ('name', 'Example'),\n            {},\n        ))\n        \n        q2 = Q(price__gt=F('price'))\n        q2_resolved = q2.resolve_expression(reuse={})\n        self.assertEqual(q2_resolved, (\n            ( 'price__gt', F('price')),\n            {},\n        ))\n\n        q = Q(name='Example') & Q(price__gt=F('price'))\n        q_resolved = q.resolve_expression(reuse={})\n        self.assertEqual(q_resolved, (\n            (\n                ('name', 'Example'),\n                ('price__gt', F('price')),\n            ),\n            {},\n        ))\n\n\n", "    def test_resolve_expression(self):\n        q = Q(name='test')\n        clause, joins = q.resolve_expression()\n        self.assertIsNone(joins)\n        self.assertEqual(clause, ('django.db.models.Q', 'name__exact', 'test'))\n", "    def test_resolve_expression(self):\n        q = Q(name='value')\n        clause, _ = q.resolve_expression()\n        self.assertEqual(clause, 'name = \"value\"')\n", "    def test_resolve_expression(self):\n        q = Q(pk=1)\n        q = q.resolve_expression()\n        self.assertEqual(q.pk, 1)\n\n\n"], "sample_980": ["compilation error", "def test_identity():\n    assert Permutation(0).is_Identity\n    assert Permutation(list(range(5))).is_Identity == False\n    assert Permutation([0, 1, 2, 3, 4]) == Permutation([0, 1, 2, 3, 4]).identity()\n    assert Permutation([1, 2, 3, 4, 0]).identity() == Permutation([0, 1, 2, 3, 4])\n    assert Permutation([0, 1, 2, 3]).identity() == Permutation([0, 1, 2, 3])\n    h = Permutation([0, 1, 2, 3])\n    assert h.identity() == h is not True\n    assert Permutation(list(range(5))).rank() != 0\n", "def test_next_lex():\n    p = Permutation([1, 5, 2, 0, 3, 6, 4])\n    assert p.next_lex() == Permutation([1, 5, 3, 0, 2, 6, 4])\n    q = Permutation([0, 1, 2, 3])\n    assert q.next_lex() == Permutation([0, 1, 3, 2])\n    assert q.next_lex() == None\n    p = Permutation([0, 2, 1, 3])\n    assert p.next_lex() == Permutation([1, 2, 0, 3])\n    assert p.next_lex() is None\n    assert Permutation([1, 0]).next_lex() == None\n    p = Permutation(0)\n    assert p.next_lex() is None\n    assert Permutation([0, 1, 2, 3]).next_trotterjohnson() == \\\n        Permutation([0, 1, 3, 2])\n    assert Permutation([0, 1, 2, 3]).next_trotterjohnson().next_trotterjohnson() == \\\n        Permutation([0, 3, 1, 2])\n    p = Permutation([1, 5, 2, 0, 3, 6, 4])\n    q = Permutation([[1, 2, 3, 5, 6], [0, 4]])\n    assert p.next_trotterjohnson() == Permutation([1, 5, 2, 0, 3, 6, 4])\n    assert q.next_trotterjohnson() == Permutation([[1, 2, 3, 5, 6], [0, 4]])\n\n", "def test_parity():\n    assert Permutation([0, 1, 2, 3]).parity() == 0\n    assert Permutation([0, 2, 1, 3]).parity() == 1\n    assert Permutation([0, 1, 3, 2]).parity() == 1\n    assert Permutation([0]).parity() == 0\n    assert Permutation([0, 1]).parity() == 0\n    assert Permutation([0, 1, 2, 3, 4]).parity() == 0\n    assert Permutation([0, 2, 1, 3, 4]).parity() == 1\n    assert Permutation([1, 0, 2, 3, 4]).parity() == 1\n\n    p = Permutation([1, 2, 3, 0])\n    assert p.is_even\n    p = Permutation([1, 3, 2, 0])\n    assert not p.is_even\n    assert ~p.is_even\n\n    assert Permutation(3).parity() == 0\n    assert Permutation([0, 1, 2]).parity() == 0\n    assert Permutation([0, 2, 1]).parity() == 1\n    assert Permutation([1, 2, 0]).parity() == 1\n    assert Permutation([1, 0, 2, 3]).parity() == 1\n\n    assert _af_parity([0, 2, 1]) == 1\n\n    a = Permutation([0, 1, 2, 3])\n    b = Permutation([1, 2, 3, 0])\n    assert a.parity() == 0\n    assert b.parity() == 1\n    assert (a*b).parity() == 1\n    assert (~a).parity() == 1\n\n", "def test_inversions():\n    a = Permutation([0, 2, 1])\n    b = Permutation([0, 1, 2])\n    c = Permutation([1, 2, 0])\n    assert (a.inversions()) == 1\n    assert (b.inversions()) == 0\n    assert (c.inversions()) == 2\n    assert Permutation([3, 2, 1, 0]).inversions() == 6\n    assert Permutation([0, 1, 2, 3]).inversions() == 0\n    assert Permutation([0, 1, 3, 2]).inversions() == 2\n\n\n\n"], "sample_600": ["def test_roundtrip_with_dtype_encoding(dtype):\n    original = xr.Variable(\n        (\"x\",), [0.0, np.nan, 1.0], encoding={\"dtype\": dtype}\n    )\n    coder = variables.CFMaskCoder()\n    roundtripped = coder.decode(coder.encode(original))\n    assert roundtripped.dtype == dtype\n", "def test_unsigned_float_dtype_mismatch(encoding):\n    original = xr.Variable((\"x\",), np.arange(10, dtype=np.float32), encoding=encoding)\n    coder = variables.UnsignedIntegerCoder()\n    with pytest.raises(ValueError, match=r\"UnsignedIntegerCoder: dtype mismatch\"):\n        coder.encode(original)\n", "def test_CFScaleOffsetCoder_decode_with_dask():\n    original = xr.Variable((\"x\",), [0, -1, 1], encoding=dict(scale_factor=10, add_offset=5))\n    original_dask = original.chunk()\n    coder = variables.CFScaleOffsetCoder()\n    encoded = coder.encode(original_dask)\n    decoded = coder.decode(encoded)\n    assert isinstance(decoded.data, da.Array)\n    assert_allclose(decoded, original)\n", "def test_unsigned_integer_dtype_roundtrip(dtype):\n    original = xr.Variable(\n        (\"x\",), np.arange(10, dtype=dtype), encoding=dict(_Unsigned=\"true\")\n    )\n    coder = variables.UnsignedIntegerCoder()\n    roundtripped = coder.decode(coder.encode(original))\n    assert_identical(original, roundtripped)\n", "def test_unsigned_integer_coder_handle_missing_value(dtype):\n    original = xr.Variable((\"x\",), np.arange(10, dtype=dtype), {\"_FillValue\": 2**8})\n    coder = variables.UnsignedIntegerCoder()\n    encoded = coder.encode(original)\n    roundtripped = coder.decode(encoded)\n    assert_identical(original, roundtripped)\n"], "sample_1206": ["compilation error", "def test_issue_10694():\n    assert Float(1.0/3, 10).rewrite() == Float('0.3333333333', 10)\n    assert Float(1.0/3, 1).rewrite() == Float('0.3333333333', 1)\n", "def test_issue_10975():\n    assert Float('1.0e100').as_mpf() in (mpmath.mpf(1e100), mpmath.mpf(-1e100))\n    assert Float('-1.0e100').as_mpf() in (mpmath.mpf(-1e100), mpmath.mpf(1e100))\n", "def test_issue_10610():\n    assert Float('0.001', 10).as_mpf_val(10) == mpf('0.001')\n    assert Float('0.001', 10).as_mpf_val(5) == mpf('0.00100')\n\n\n\n", "def test_compatibility_with_mpmath():\n    from mpmath import mp\n    mp.dps = 53\n    assert Float(mp.pi) == pi\n    assert Float(mp.e) == E\n    assert Float(mp.sqrt(2)) == sqrt(2)\n    assert Float(mp.gamma) == EulerGamma\n"], "sample_230": ["    def test_invalid_json_handling(self):\n        field = JSONField(required=False)\n        self.assertIsNone(field.clean(None))\n        self.assertIsNone(field.clean(''))\n        with self.assertRaisesMessage(ValidationError, 'Enter a valid JSON.'):\n            field.clean('invalid json')\n        with self.assertRaisesMessage(ValidationError, 'Enter a valid JSON.'):\n            field.clean('{\"a\": 1, \"b\":\"text\"')\n\n\n\n", "    def test_custom_encoder_decoder_nested_objects(self):\n        class CustomDecoder(json.JSONDecoder):\n                return super().__init__(object_hook=self.as_uuid, *args, **kwargs)\n\n                if 'uuid' in dct:\n                    dct['uuid'] = uuid.UUID(dct['uuid'])\n                if 'nested' in dct and isinstance(dct['nested'], dict):\n                    self.as_uuid(dct['nested'])\n                return dct\n\n        value = {\n            'uuid': uuid.UUID('{c141e152-6550-4172-a784-05448d98204b}'),\n            'nested': {'uuid': uuid.UUID('{f042e152-6550-4172-a784-05448d98204c}')}\n        }\n        encoded_value = json.dumps(\n            value, cls=DjangoJSONEncoder\n        )\n        field = JSONField(encoder=DjangoJSONEncoder, decoder=CustomDecoder)\n        self.assertEqual(field.prepare_value(value), encoded_value)\n        self.assertEqual(field.clean(encoded_value), value)\n", "    def test_unicode_json(self):\n        field = JSONField(allow_unicode=True)\n        unicode_str = '\u4f60\u597d\uff0c\u4e16\u754c'\n        value = field.clean(json.dumps({'a': unicode_str}))\n        self.assertEqual(value['a'], unicode_str)\n\n\n", "    def test_custom_encoder_decoder_from_string(self):\n        class CustomDecoder(json.JSONDecoder):\n                return super().__init__(object_hook=self.as_uuid, *args, **kwargs)\n\n                if 'uuid' in dct:\n                    dct['uuid'] = uuid.UUID(dct['uuid'])\n                return dct\n\n        value = '{\"uuid\": \"c141e152-6550-4172-a784-05448d98204b\"}'\n        field = JSONField(encoder=DjangoJSONEncoder, decoder=CustomDecoder)\n        self.assertEqual(field.clean(value), {'uuid': uuid.UUID('{c141e152-6550-4172-a784-05448d98204b}')})\n\n", "    def test_invalid_with_custom_error_messages(self):\n        field = JSONField(error_messages={'invalid': 'Invalid JSON format!'})\n        with self.assertRaisesMessage(ValidationError, 'Invalid JSON format!'):\n            field.clean('{some badly formed: json}')\n"], "sample_583": ["compilation error", "compilation error", "compilation error", "def test_posify_mask_subindexer_with_slice( ):\n    indices = (slice(1, 4),)\n    expected = (slice(1, 4),)\n    actual = indexing._posify_mask_subindexer(indices)\n    assert actual == expected \n", "compilation error"], "sample_416": ["    def test_empty_dbname(self):\n        self.assertEqual(\n            self.settings_to_cmd_args_env({\"HOST\": \"somehost\", \"PORT\": \"444\"}),\n            ([\"psql\", \"-h\", \"somehost\", \"-p\", \"444\", \"postgres\"], None),\n        )\n", "    def test_empty_settings(self):\n        self.assertEqual(\n            self.settings_to_cmd_args_env({}),\n            ([\"psql\"], None),\n        )\n", "    def test_empty_service_and_dbname(self):\n        args, env = self.settings_to_cmd_args_env({})\n        self.assertEqual(args, [\"psql\"])\n        self.assertIsNone(env) \n", "    def test_empty_environment(self):\n        self.assertEqual(\n            self.settings_to_cmd_args_env({\"NAME\": \"dbname\"}),\n            ([\"psql\", \"dbname\"], None),\n        )\n", "    def test_invalid_ssl_paths(self):\n        self.assertEqual(\n            self.settings_to_cmd_args_env(\n                {\n                    \"NAME\": \"dbname\",\n                    \"USER\": \"someuser\",\n                    \"HOST\": \"somehost\",\n                    \"PORT\": \"444\",\n                    \"OPTIONS\": {\n                        \"sslmode\": \"verify-ca\",\n                        \"sslrootcert\": \"nonexistent_file.crt\",\n                        \"sslcert\": \"client.crt\",\n                        \"sslkey\": \"client.key\",\n                    },\n                }\n            ),\n            ([\"psql\", \"-U\", \"someuser\", \"-h\", \"somehost\", \"-p\", \"444\", \"dbname\"],\n             {\"PGSSLCERT\": \"client.crt\", \"PGSSLKEY\": \"client.key\", \"PGSSLMODE\": \"verify-ca\"}),\n        )\n"], "sample_35": ["def test_isinstancemethod():\n    class C:\n            pass\n\n    c = C()\n\n    assert isinstancemethod(C, C.m)\n    assert isinstancemethod(C, C.m.__get__(c)) \n", "def test_resolve_name():\n    assert introspection.resolve_name('astropy.utils.introspection.resolve_name') == introspection.resolve_name\n    assert introspection.resolve_name('astropy', 'utils', 'introspection', 'resolve_name') == introspection.resolve_name\n    with pytest.raises(ImportError):\n        introspection.resolve_name('bogus.module.doesnotexist')\n", "def test_resolve_name():\n    assert introspection.resolve_name('astropy.utils.introspection.resolve_name') == introspection.resolve_name\n    assert introspection.resolve_name('astropy', 'utils', 'introspection', 'resolve_name') == introspection.resolve_name\n    with pytest.raises(ImportError):\n        introspection.resolve_name('nonexistent_module.nonexistant_object')\n\n\n", "def test_resolve_name():\n    assert introspection.resolve_name('astropy.utils.introspection.resolve_name') is introspection.resolve_name\n    assert introspection.resolve_name('astropy.utils.introspection', 'resolve_name') is introspection.resolve_name\n    with pytest.raises(ImportError):\n        introspection.resolve_name('this_doesnt_exist') \n", "def test_resolve_name():\n    assert introspection.resolve_name('astropy.utils.introspection') is introspection  \n    assert introspection.resolve_name('astropy.utils.introspection.resolve_name') is introspection.resolve_name\n\n\n\n"], "sample_1087": ["def test_f_polys():\n    for poly in f_polys():\n        assert isinstance(poly, Poly)\n        assert poly.is_ground is True\n", "def test_f_polys():\n    for f in f_polys():\n        assert isinstance(f, Poly)\n", "def test_f_polys():\n    F1, F2, F3, F4, F5, F6, F7 = f_polys()\n\n    assert F1.rep == Poly(\n        _f_0(), x, y, z\n    )\n\n    assert F2.rep == Poly(\n        _f_1(), x, y, z\n    )\n\n    assert F3.rep == Poly(\n        _f_2(), x, y, z\n    )\n\n    assert F4.rep == Poly(\n        _f_3(), x, y, z\n    )\n\n    assert F5.rep == Poly(\n        _f_4(), x, y, z\n    )\n\n    assert F6.rep == Poly(\n        _f_5(), x, y, z\n    )\n\n    assert F7.rep == Poly(\n        _f_6(), x, y, z, t\n    )\n\n    \n\n", "def test_f_polys():\n    for func in f_polys():\n        assert isinstance(func, Poly)\n", "def test_f_polys():\n    for f in f_polys():\n        assert isinstance(f, Poly)\n"], "sample_121": ["    def test_check_constraints_error_if_not_required(self):\n        class Model(models.Model):\n            age = models.IntegerField()\n\n            class Meta:\n                constraints = [models.CheckConstraint(check=models.Q(age__gte=18), name='is_adult')]\n\n        if connection.features.supports_table_check_constraints:\n            self.assertEqual(Model.check(), [])\n        else:\n            warnings = [\n                Warning(\n                    '%s does not support check constraints.' % connection.display_name,\n                    hint=(\n                        \"A constraint won't be created. Silence this warning if you \"\n                        \"don't care about it.\"\n                    ),\n                    obj=Model,\n                    id='models.W027',\n                )\n            ]\n            self.assertEqual(Model.check(), warnings) \n", "    def test_check_constraints_with_invalid_expression(self):\n        class Model(models.Model):\n            age = models.IntegerField()\n\n            class Meta:\n                constraints = [models.CheckConstraint(check='invalid_expression', name='is_adult')]\n\n        self.assertEqual(Model.check(), [\n            Error(\n                \"Check constraint 'is_adult' references an invalid expression: 'invalid_expression'. \"\n                \"Consult the database documentation for the supported expression syntax.\",\n                obj=Model,\n                id='models.E027',\n            )\n        ])\n", "    def test_check_constraints_with_error_message(self):\n        class Model(models.Model):\n            age = models.IntegerField()\n\n            class Meta:\n                constraints = [\n                    models.CheckConstraint(\n                        check=models.Q(age__gte=18),\n                        name='is_adult',\n                        message='Age must be at least 18',\n                    )\n                ]\n\n        errors = Model.check()\n        self.assertEqual(len(errors), 0)\n", "    def test_check_constraints_with_custom_error_message_works(self):\n        class Model(models.Model):\n            age = models.IntegerField()\n\n            class Meta:\n                constraints = [\n                    models.CheckConstraint(\n                        check=models.Q(age__gte=18),\n                        name='is_adult',\n                        error_message='You must be at least 18 years old.'\n                    )\n                ]\n\n        errors = Model.check()\n        self.assertEqual(len(errors), 0)  \n", "    def test_check_constraints_with_error_message(self):\n        class Model(models.Model):\n            age = models.IntegerField()\n\n            class Meta:\n                constraints = [\n                    models.CheckConstraint(\n                        check=models.Q(age__gte=18), name='is_adult',\n                        error_message='You must be at least 18 years old.'\n                    ),\n                ]\n\n        errors = Model.check()\n        self.assertEqual(\n            errors,\n            [\n                Error(\n                    'Error message for check constraint \"is_adult\" is too long. '\n                    'Please keep it under 255 characters.',\n                    obj=Model,\n                    id='models.E030'\n                )\n            ]\n        )\n"], "sample_921": ["        def __init__(self, value):\n            self._value = value\n", "        def __init__(self, value):\n            self._value = value\n", "def test_is_builtin_class_method_with_invalid_input():\n    class MyInt(int):\n            pass\n\n    with pytest.raises(TypeError):\n        inspect.is_builtin_class_method(MyInt, 'my_method', 1)\n\n    with pytest.raises(TypeError):\n        inspect.is_builtin_class_method(1, 'to_bytes')\n\n\n\n", "compilation error", "        def __init__(self, mro_attr):\n            self.__mro__ = mro_attr\n"], "sample_307": ["    def test_weekday_names(self):\n        for i, day_name in enumerate(WEEKDAYS_AB):\n            with self.subTest(day=day_name):\n                my_birthday = datetime(2009, 5, 16, 0, 0)\n                my_birthday = my_birthday.replace(weekday=i)\n                self.assertEqual(dateformat.format(my_birthday, 'D'), day_name)\n", "    def test_invalid_format_specifiers(self):\n        my_birthday = datetime(1979, 7, 8, 22, 00)\n        for specifier in ['x', 'X', 'B', 'C', 'D', 'E', 'F', 'H', 'l', 'L', 'z']:\n            with self.assertRaisesMessage(ValueError, \"Invalid format specifier '%s'\" % specifier):\n                dateformat.format(my_birthday, specifier)\n", "    def test_i_format_trailing_zeros(self):\n        tests = [\n            (0, '00'),\n            (3, '03'),\n            (12, '12'),\n        ]\n        for minute, expected in tests:\n            with self.subTest(minute=minute):\n                self.assertEqual(dateformat.format(datetime(2000, 1, 1, 0, minute), 'i'), expected) \n", "    def test_epoc_for_datetime(self):\n        dt = datetime.fromtimestamp(0)  \n        self.assertEqual(dateformat.format(dt, 'U'), '0')\n\n", "    def test_invalid_datetime_format_specifiers(self):\n        my_date = date(1984, 8, 7)\n\n        for specifier in ['A', 'a', 'f', 'g', 'G', 'h', 'H', 'i', 'P', 'r', 's', 'u', 'T', 'Z']:\n            msg = \"The format for date objects may not contain time-related format specifiers (found '%s').\" % specifier\n            with self.assertRaisesMessage(TypeError, msg):\n                dateformat.format(my_date, specifier)\n"], "sample_559": ["def test_imagegrid_cbar_mode_each():\n    arr = np.arange(16).reshape((4, 4))\n\n    fig = plt.figure(figsize=(18, 9))\n\n    grid = ImageGrid(fig, 111, nrows_ncols=(2, 2),\n                     cbar_location='right',\n                     cbar_size='20%',\n                     cbar_mode='each')\n    ax1, ax2, ax3, ax4 = grid\n\n    ax1.imshow(arr, cmap='nipy_spectral')\n    ax2.imshow(arr.T, cmap='hot')\n    ax3.imshow(np.hypot(arr, arr.T), cmap='jet')\n    ax4.imshow(np.arctan2(arr, arr.T), cmap='hsv')\n", "def test_subplots_adjust_after_grid():\n    fig = plt.figure()\n    grid = Grid(fig, 111, (2, 2))\n    grid.subplots_adjust(wspace=0.2, hspace=0.3)\n    ax1 = grid[0]\n    ax2 = grid[1]\n    assert ax1.get_position()._x0 == pytest.approx(0.05)\n    assert ax1.get_position()._y0 == pytest.approx(0.25)\n    assert ax2.get_position()._x0 == pytest.approx(0.25)\n    assert ax2.get_position()._y0 == pytest.approx(0.25)\n\n\n\n", "def test_grid_with_axes_class_overriding_axis():\n    class MyAxes(mpl.axes.Axes):\n            super().__init__(*args, **kwargs)\n            self.override = True\n\n    Grid(plt.figure(), 111, (2, 2), axes_class=MyAxes)\n\n", "def test_inset_on_host_axes():\n    fig = plt.figure(figsize=(6, 4))\n    host = fig.add_subplot(111)\n    inset = zoomed_inset_axes(host, zoom=2, loc='upper right')\n    inset.plot([1, 2, 3], [4, 5, 6])\n    host.set_axis_off()\n\n", "def test_grid_with_axes_class_not_overriding_axis():\n    fig = plt.figure()\n    grid = Grid(fig, 111, (2, 2), axes_class=mpl.axes.Axes,\n                label_mode='all')\n    for i in range(4):\n        ax = grid[i]\n        ax.plot([1, 2, 3], [1, 2, 3])\n"], "sample_893": ["def test_plot_tree_no_feature_names():\n    clf = DecisionTreeClassifier(max_depth=3, random_state=0)\n    clf.fit(X, y)\n    \n    with pytest.raises(ValueError):\n        plot_tree(clf)\n", "compilation error", "def test_export_text_multioutput_regression():\n    rng = RandomState(42)\n    X = rng.random((100, 2))\n    y = rng.randint(0, 10, size=(100, 2))\n    clf = DecisionTreeRegressor(max_depth=2, random_state=0)\n    clf.fit(X, y)\n    expected_report = dedent(\"\"\"\n    |--- feature_0 <= 0.5\n    |   |--- value: [[3.0, 5.0]]\n    |--- feature_0 >  0.5\n    |   |--- value: [[7.0, 9.0]]\n    \"\"\").lstrip()\n\n    assert export_text(clf, decimals=1) == expected_report\n", "def test_plot_tree_no_feature_names():\n    clf = DecisionTreeClassifier(max_depth=2, random_state=0)\n    clf.fit(X, y)\n    with pytest.raises(ValueError, match=\"None given for feature_names\"):\n        plot_tree(clf)\n", "def test_plot_tree_with_decimals():\n    clf = DecisionTreeRegressor(max_depth=2, random_state=0)\n    clf.fit(X, y)\n\n    # Test export code\n    nodes = plot_tree(clf, decimals=2)\n    for node in nodes:\n        text = node.get_text()\n        if \"value\" in text:\n            value = float(text.split(\"value: \")[1].split(\" \")[0])\n            assert value.is_integer()\n"], "sample_794": ["def test_ridge_with_sparse_multilabel():\n    # Test ridge with sparse data and multilabel targets\n    X, y = make_multilabel_classification(n_samples=10, n_features=100,\n                                         random_state=42)\n\n    # Convert to sparse format\n    X_sparse = sp.csr_matrix(X)\n    y_sparse = sp.csr_matrix(y)\n\n    for solver in ['sag', 'saga']:\n        ridge = Ridge(alpha=0.5, solver=solver, max_iter=500)\n        ridge.fit(X_sparse, y_sparse)\n        # Check if predictions are made correctly\n        assert_array_almost_equal(ridge.predict(X_sparse),\n                                  ridge.predict(X_sparse),\n                                  decimal=4)\n\n\n\n", "def test_ridge_cv_solver_autoswitch():\n    rng = np.random.RandomState(42)\n    X = rng.randn(100, 5)\n    y = rng.randn(100)\n    alphas = [0.1, 1, 10]\n\n    # Test with different solvers specified explicitly\n\n    for solver in ['sparse_cg', 'lsqr']:\n        ridge = RidgeCV(alphas=alphas, solver=solver, cv=5)\n        ridge.fit(X, y)\n        assert ridge.best_params_['solver'] == solver\n\n    # Test with solver set to 'auto'\n\n    ridge_auto = RidgeCV(alphas=alphas, solver='auto', cv=5)\n    ridge_auto.fit(X, y)\n    assert ridge_auto.best_params_['solver'] in ['sparse_cg', 'lsqr']\n\n    #  Test that solver switch does not cause unexpected results\n\n    assert_allclose(ridge_auto.best_score_, ridge.best_score_, rtol=1e-3)\n\n\n\n", "def test_ridge_classifier_multi_class_validation():\n    X, y = make_classification(n_samples=100, n_features=10, n_informative=5,\n                               n_classes=3, random_state=42)\n    clf = RidgeClassifier()\n    scores = cross_val_score(clf, X, y, cv=5, scoring='accuracy')\n    assert np.all(np.isfinite(scores))\n\n\n", "    def test_ridge_regression_handle_zero_variance_features(solver):\n        rng = np.random.RandomState(0)\n        n_samples, n_features = 6, 5\n        X = rng.randn(n_samples, n_features)\n\n        # Create a feature with zero variance\n        X[:, 0] = 0\n        y = np.dot(X, rng.randn(n_features))\n\n        alpha = 1.0\n\n        ridge = Ridge(alpha=alpha, solver=solver, max_iter=500, tol=1e-10)\n        ridge.fit(X, y)\n        assert np.isclose(ridge.coef_[0], 0).all()\n\n", "    def test_ridge_classification_multiclass():\n        X, y = make_classification(n_samples=600, n_features=20, n_classes=3,\n                                   random_state=0)\n        ridge = RidgeClassifier()\n        ridge.fit(X, y)\n        assert_equal(ridge.coef_.shape[0], X.shape[1])\n\n\n\n\n"], "sample_681": ["def test_log_file_cli_level_subdirectories_are_successfully_created(testdir):\n    testdir.makepyfile(\"\"\"def test_logger(): pass\"\"\")\n    expected_path = os.path.join(os.path.dirname(testdir.tmpdir.join(\"test_logger.py\").strpath), \"foo\", \"bar\", \"logf.log\")\n    testdir.runpytest(\"--log-file=foo/bar/logf.log\", \"--log-file-level=WARNING\")\n    assert os.path.exists(expected_path)\n", "        def test_log_relative_path(request):\n            plugin = request.config.pluginmanager.getplugin('logging-plugin')\n            assert plugin.log_file_handler.baseFilename == os.path.join(\n                request.config.invocation_dir, \"logf.log\"\n            )", "compilation error", "    def test_log_file_cli_relative_path(testdir):\n        testdir.makepyfile(\"\"\" def test_logger(): pass \"\"\")\n\n        # Using a relative path to a directory inside the testdir\n        rel_path = \"subfolder/logs/logf.log\"\n\n        result = testdir.runpytest(\"--log-file={}\".format(rel_path))\n        assert os.path.isfile(os.path.join(testdir.tmpdir.strpath, rel_path))\n        assert result.ret == ExitCode.OK\n", "def test_log_file_cli_empty_subdirectories_are_successfully_created(testdir):\n    path = testdir.makepyfile(\"\"\" def test_logger(): pass \"\"\")\n    expected = os.path.join(os.path.dirname(str(path)), \"foo\", \"bar\", \"baz\")\n    result = testdir.runpytest(\"--log-file=foo/bar/baz/logf.log\")\n    assert \"logf.log\" in os.listdir(expected)\n    assert result.ret == ExitCode.OK\n"], "sample_832": ["def test_ARDRegression_large_dataset():\n    # Test ARDRegression on a larger dataset\n    # This is inspired by the test case in BayesianRidge.py\n\n    n_samples = 1000\n    n_features = 20\n    random_state = check_random_state(42)\n    X = random_state.randn(n_samples, n_features)\n    y = np.dot(X, random_state.randn(n_features)) + random_state.randn(n_samples)\n\n    clf = ARDRegression(compute_score=True, n_iter=500)\n    clf.fit(X, y)\n    \n    # Check that the model did not diverge\n    assert clf.n_iter_ > 0\n\n    # Check that the results are reasonable\n    r2 = clf.score(X, y)\n    assert r2 > 0.5\n\n", "def test_ard_with_multivariate_output():\n    # Test ARDRegression with multivariate output\n    from sklearn.linear_model.bayes import BayesianRidge\n\n    X = np.random.rand(100, 5)\n    y = np.random.rand(100, 3)  # 3 output variables\n\n    # Bayesian Ridge (for comparison)\n    br = BayesianRidge()\n    br.fit(X, y)\n\n    # ARDRegression\n    ard = ARDRegression()\n    ard.fit(X, y)\n\n    # Check that prediction works correctly for both regressors\n    for clf in [br, ard]:\n        y_pred = clf.predict(X)\n        assert y_pred.shape == (100, 3)\n\n", "def test_ard_convergence_on_large_dataset():\n    # Test ARD convergence with a larger dataset\n    # and check the resulting parameters are meaningful\n    # (Issue #12725)\n    n_samples = 1000\n    n_features = 30\n    random_state = np.random.RandomState(42)\n    X = random_state.randn(n_samples, n_features)\n    y = 2 * X[:, 0] + 3 * X[:, 1] - X[:, 2] + random_state.randn(n_samples)\n\n    # Run ARDRegression\n    ard = ARDRegression(n_iter=500)\n    ard.fit(X, y)\n    \n\n    # Check if weights make sense\n    # For example, check if the variance of the coefficients \n    # is consistent with the expected noise level\n    mean_coef = np.mean(ard.coef_)\n    std_coef = np.std(ard.coef_)\n    assert std_coef > 0.01, 'Expected coefficients with some variation'\n\n\n\n", "def test_ard_with_sample_weights():\n    # Test ARDRegression with sample weights\n    n_samples = 10\n    n_features = 5\n    random_state = check_random_state(42)\n    X = random_state.random_sample((n_samples, n_features))\n    y = np.random.rand(n_samples)\n    sample_weights = random_state.rand(n_samples)\n\n    # Initialize weights\n    w_true = random_state.rand(n_features)\n    true_y = np.dot(X, w_true)\n\n    # Fit ARDRegression with sample weights\n    clf = ARDRegression(compute_score=True)\n    clf.fit(X, y, sample_weight=sample_weights)\n\n    # Check that the predicted values are close to the true values\n    y_pred = clf.predict(X, sample_weight=sample_weights)\n    assert_array_almost_equal(y_pred, true_y, decimal=4)\n\n    # Check that the fitted coefficients reflect the sample weights\n    w_fitted = clf.coef_\n    w_expected = w_true * sample_weights / np.sum(sample_weights)\n    assert_array_almost_equal(w_fitted, w_expected, decimal=4)\n\n\n\n", "    def test_ard_with_n_iter_zero():\n        \"\"\"Test ARDRegression with n_iter=0\"\"\"\n        X = np.array([[1], [2], [3]])\n        y = np.array([1, 2, 3])\n        clf = ARDRegression(n_iter=0)\n        with assert_raises(ValueError):\n            clf.fit(X, y)\n"], "sample_845": ["def test_analyzer_as_string(Estimator):\n    vec = Estimator(analyzer='word')\n    assert vec.get_params()['analyzer'] == 'word'\n\n    with pytest.raises(ValueError):\n        vec = Estimator(analyzer='invalid_analyzer')\n\n\n\n\n", "def test_callable_analyzer_raises_exception_with_non_string_input(Estimator):\n    if issubclass(Estimator, HashingVectorizer):\n        pytest.xfail('HashingVectorizer is not supported on PyPy')\n    data = [{'text': 'this is text, not file or filename'}]\n    analyzer = lambda x: open(x, 'r')\n\n    with pytest.raises(AttributeError, match=r\"'str' object has no attribute 'read'\"):\n        Estimator(analyzer=analyzer, input='file').fit_transform(data)\n    with pytest.raises(TypeError):\n        Estimator(analyzer=analyzer, input='filename').fit_transform(data)\n\n", "def test_min_df_threshold(min_df):\n    v = CountVectorizer(min_df=min_df)\n    X = v.fit_transform(JUNK_FOOD_DOCS)\n    support = v.get_feature_names_out()\n    assert len(support) == (\n        len(set(support)) if min_df == 0.0 else\n        len(list(filter(lambda x: x in ALL_FOOD_DOCS, support))))\n", "def test_vectorizer_use_idf_with_fixed_vocabulary():\n    vocab = [\"apple\", \"banana\", \"cherry\"]\n    data = [\"apple banana\", \"cherry\", \"apple\"]\n    vec = Estimator(vocabulary=vocab, use_idf=True)\n    # Check that using 'use_idf=True' with a fixed vocabulary results in all\n    # IDF values being 1. This is because IDF is calculated based on term\n    # frequency across the corpus, and with a fixed vocabulary, all terms\n    # are present\n    X = vec.fit_transform(data)\n    for i in range(len(vec.idf_)):\n        assert vec.idf_[i] == 1.0\n\n\n\n", "def test_vectorizer_default_stop_words_removal(Estimator):\n    data = ['This is a test document.',\n            'This document is about testing.']\n    vec = Estimator(stop_words='english')\n    X = vec.fit_transform(data)\n    expected_words = ['test', 'document', 'about', 'testing']\n    assert_array_equal([\n        word for word in vec.vocabulary_ if word in expected_words\n    ], expected_words)\n\n\n\n"], "sample_543": ["def test_polygon_selector_empty_selection(ax, draw_bounding_box):\n    tool = widgets.PolygonSelector(ax, onselect=noop,\n                                    draw_bounding_box=draw_bounding_box)\n\n    # No verts have been set\n    assert tool.verts == []\n\n    # Trigger an empty selection\n    tool.onselect(tool.verts)\n    assert tool.verts == []\n\n\n\n", "def test_multicursor_with_mouse_events():\n    fig, (ax1, ax2) = plt.subplots(2, sharex=True)\n    multi = widgets.MultiCursor(None, (ax1, ax2), useblit=False)\n\n    # Simulate mouse events:\n    #   1. Button press followed by a move and release.\n\n    canvas = ax1.figure.canvas\n    t = ax1.transData\n\n    # Button press\n    event = MouseEvent(\n        \"button_press_event\", canvas, *t.transform((0.1, 0.3)), 1\n    )._process()\n    multi.onpress(event)\n\n    # Move event\n    event = MouseEvent(\n        \"motion_notify_event\", canvas, *t.transform((0.5, 0.7))\n    )._process()\n    multi.onmove(event)\n\n    # Button release\n    event = MouseEvent(\n        \"button_release_event\", canvas, *t.transform((0.5, 0.7)), 1\n    )._process()\n    multi.onrelease(event)\n\n    # Ensure the cursor lines are in the expected positions. \n    for vline in multi.vlines:\n        assert np.isclose(vline.get_xdata(), [0.1, 0.5])\n    forhline in multi.hlines:\n        assert np.isclose(hline.get_ydata(), [0.3, 0.7])\n", "def test_multicursor_cursor_properties():\n    fig, axs = plt.subplots(3, 1, sharex=True)\n    multi = widgets.MultiCursor(\n        None, (axs[0], axs[1], axs[2]),\n        useblit=False, line_width=3, color='red'\n    )\n\n    assert multi.hlines[0].get_linestyle() == '-'\n    assert multi.hlines[0].get_linewidth() == 3\n    assert multi.hlines[0].get_color() == 'red'\n\n    assert multi.vlines[0].get_linestyle() == '-'\n    assert multi.vlines[0].get_linewidth() == 3\n    assert multi.vlines[0].get_color() == 'red'\n\n    multi.set_properties(line_width=5, color='blue')\n    assert multi.hlines[0].get_linewidth() == 5\n    assert multi.hlines[0].get_color() == 'blue'\n\n    assert multi.vlines[0].get_linewidth() == 5\n    assert multi.vlines[0].get_color() == 'blue'\n", "def test_polygon_selector_draw_bbox(ax, draw_bounding_box):\n    verts = [(50, 50), (150, 50), (50, 150)]\n    event_sequence = [\n        *polygon_place_vertex(*verts[0]),\n        *polygon_place_vertex(*verts[1]),\n        *polygon_place_vertex(*verts[2]),\n        *polygon_place_vertex(*verts[0]),\n    ]\n\n    tool = widgets.PolygonSelector(ax, onselect=noop, draw_bounding_box=draw_bounding_box)\n    for (etype, event_args) in event_sequence:\n        do_event(tool, etype, **event_args)\n\n    artist = tool.get_artist()\n    for bbox in artist.collections:\n        if hasattr(bbox, 'visibility'):\n            assert bbox.visibility is not None and bbox.visibility\n        else:\n            assert isinstance(bbox, matplotlib.patches.Rectangle)\n\n\n\n", "def test_MultiCursor_canvas_events(horizOn, vertOn):\n    (ax1, ax3) = plt.figure().subplots(2, sharex=True)\n    ax2 = plt.figure().subplots()\n    multi = widgets.MultiCursor(\n        None, (ax1, ax2), useblit=False, horizOn=horizOn, vertOn=vertOn\n    )\n\n    fig = ax1.figure\n    canvas = fig.canvas\n\n    # mock motion_notify event\n    event = mock_event(ax1, xdata=.5, ydata=.25)\n    multi.onmove(event)\n\n    # force a draw\n    fig.canvas.draw()\n\n\n    # simulate mouse motion event outside of any axes\n    x, y = 100, 100\n    canvas.mpl_callbacks_pick = []\n    canvas.on_motion(event=MouseEvent(\n        \"motion_notify_event\", canvas, x, y, 1))\n    assert len(multi.vlines) == 2\n    assert len(multi.hlines) == 2\n\n    # check that lines are not moved\n    for l in multi.vlines:\n        assert l.get_xdata() == (.5, .5)\n    for l in multi.hlines:\n        assert l.get_ydata() == (.25, .25)\n"], "sample_1208": ["def test_matrix_distribution_printing():\n    M = MatrixGamma('M', 2, 3, [[1, 0], [0, 1]])\n    assert str(M) == \"MatrixGamma('M', 2, 3, [[1, 0], [0, 1]])\"\n    W = Wishart('W', 5, [[1, 0], [0, 1]])\n    assert str(W) == \"Wishart('W', 5, [[1, 0], [0, 1]])\"\n    N = MatrixNormal('N', [[1, 2], [3, 4]], [[5, 6], [6, 7]], [[8, 9], [9, 10]])\n    assert str(N) == \"MatrixNormal('N', [[1, 2], [3, 4]], [[5, 6], [6, 7]], [[8, 9], [9, 10]])\"\n    T = MatrixStudentT('T', 3, [[1, 2], [3, 4]], [[5, 6], [6, 7]], [[8, 9], [9, 10]])\n    assert str(T) == \"MatrixStudentT('T', 3, [[1, 2], [3, 4]], [[5, 6], [6, 7]], [[8, 9], [9, 10]])\"\n", "def test_matrix_distributions_args():\n    for distrib, args in [\n        (MatrixGammaDistribution, (1, 2, [[2, 1], [1, 2]])),\n        (WishartDistribution, (5, [[1, 0], [0, 1]])),\n        (MatrixNormalDistribution, (2, [[5, 6]], [[2, 1], [1, 2]], [4])),\n        (MatrixStudentTDistribution, (3, [[5, 6]], [[2, 1], [1, 2]], [4]))\n    ]:\n        raises(TypeError, lambda: distrib(*range(10)))\n        raises(ValueError, lambda: distrib(1, 2, [[1, 0], [0, 1]], *[4, 5]))\n\n\n\n\n", "def test_MatrixGamma_multivariate_pdf():\n    M = MatrixGamma('M', 2, 3, [[1, 0], [0, 1]])\n    X = MatrixSymbol('X', 2, 2)\n    term1 = exp(Trace(Matrix([[-1/6, 0], [0, -1/6]])*X))\n    # assert density(M)(X).doit() == term1/(6*pi*sqrt(Determinant(X)))\n    #  This assertion broke in the last PR \n\n    n = symbols('n', positive=True)\n    d = symbols('d', positive=True, integer=True)\n    Y = MatrixSymbol('Y', d, d)\n    SM = MatrixSymbol('SM', d, d)\n    M = MatrixGamma('M', n, SM)\n    k = Dummy('k')\n    exprd = pi**(-d*(d - 1)/4)*3**(-n*d)*exp(Trace(-(1/3)*SM**(-1)*Y)\n        )*Determinant(SM)**(-n)*Determinant(Y)**(n - d/2 - S(1)/2)/Product(\n        gamma(-k/2 + n + S(1)/2), (k, 1, d))\n    # assert density(M)(Y).dummy_eq(exprd) \n\n    raises(ValueError, lambda: density(M)(1))\n    raises(ValueError, lambda: MatrixGamma('M', -1, [[1, 0], [0, 1]]))\n    raises(ValueError, lambda: MatrixGamma('M', -1, [[1, 0], [2, 1]]))\n    raises(ValueError, lambda: MatrixGamma('M', -1, [[1, 0], [0]]))\n\n\n", "compilation error", "def test_MatrixPSpace_inverse():\n    M = MatrixGammaDistribution(1, 2, [[2, 1], [1, 2]])\n    MP = MatrixPSpace('M', M, 2, 2)\n    assert (MP.inverse).distribution == M  \n    \n    \n    \n"], "sample_688": ["def test_collect_ignore_pattern(testdir):\n    testdir.makepyfile(\n        \"def test_one(): pass\",\n        \"def test_two(): pass\",\n    )\n    testdir.mkdir(\"subdir\").join(\"conftest.py\").write(\n        textwrap.dedent(\"\"\"\n        collect_ignore = [\"*two.py\"]\n    \"\"\")\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*1 passed in*\"])\n", "    def test_collect_import_hooks(testdir):\n        \"\"\"Test that import hooks created by @pytest.hookimpl behave correctly.\"\"\"\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.hookimpl(hookwrapper=True)\n                yield\n                return MyError(f\"{name} collected!\")\n\n            class MyError(Exception):\n                pass\n\n            @pytest.hookimpl(hookwrapper=True)\n                yield\n                assert isinstance(name, str)\n                return MyError(f\"Hook called for {name}! \")\n            \"\"\"\n        )\n        result = testdir.runpytest()\n        result.stdout.fnmatch_lines(\n            [\n                \"*ERROR collecting test_collect_import_hooks.py*\",\n                \"*MyError: Hook called for test_collect_import_hooks.py! \",\n            ]\n        )\n", "    def test_collect_non_existing_fixture(testdir: Testdir) -> None:\n        testdir.makepyfile(\n            \"\"\"\n        @pytest.fixture\n            assert a == 4\n            assert a == 5\n        \"\"\"\n        )\n        result = testdir.runpytest()\n        result.stdout.fnmatch_lines(\n            [\n                \"*tests.py::test_a* *ERROR* *AssertionError*\",\n                \"*tests.py::test_a* *AssertionError: True is not True*\",\n            ]\n        )\n", "    def test_collect_function_with_fixture(testdir: Testdir) -> None:\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n                assert a == 4\n            \"\"\"\n        )\n        result = testdir.runpytest()\n        result.stdout.fnmatch_lines(\n            [\n                \"test_function_with_fixture PASSED\",\n                \"* 1 passed in *\",\n            ]\n        )\n", "    def test_collect_import_error_in_fixture_setup(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n            @pytest.fixture\n                raise ValueError(\"oops\")\n                assert a == 4\n            \"\"\"\n        )\n        result = testdir.runpytest()\n        result.stdout.fnmatch_lines(\n            [\n                \"*ERROR collecting test_*,*ValueError: oops*\",\n                \"*1 failed*,*1 skipped in*\",\n            ]\n        )\n"], "sample_164": ["    def test_server_formatter_handles_datetime_format(self):\n        formatter = ServerFormatter(default_time_format='%Y-%m-%d %H:%M:%S')\n        log_msg = 'log message'\n        record = logging.makeLogRecord({'msg': log_msg, 'server_time': '2023-10-27 14:32:18'})\n        self.assertEqual(formatter.format(record), '2023-10-27 14:32:18 log message')\n", "    def test_include_html_in_admin_email(self):\n        # Ensure the AdminEmailHandler includes HTML in the email when\n        # include_html is set to True.\n        self.settings.DEBUG = True\n        handler = AdminEmailHandler(include_html=True)\n        record = self.logger.makeRecord('name', logging.ERROR, 'function', 'lno', 'message', None, None)\n        record.request = self.request_factory.get('/')\n        handler.emit(record)\n        self.assertEqual(len(mail.outbox), 1)\n        msg = mail.outbox[0]\n        self.assertIn('<html>', msg.body)\n        self.assertIn('</html>', msg.body)\n\n\n\n\n", "    def test_server_formatter_extra_fields(self):\n        formatter = ServerFormatter()\n        log_msg = 'log message'\n        record = logging.makeLogRecord({'msg': log_msg, 'extra': {'request': 'GET /status', 'status_code': 200}})\n        formatted_record = formatter.format(record)\n        self.assertIn('GET /status', formatted_record)\n        self.assertIn('HTTP_SUCCESS', formatted_record)\n\n\n\n", "    def test_server_formatter_escapes_newlines(self):\n        server_time = '2016-09-25 10:20:30'\n        log_msg = 'This is a log message\\nwith multiple lines\\nand some \\t tabs'\n        logger = logging.getLogger('django.server')\n\n        @contextmanager\n            old_stream = logger.handlers[0].stream\n            new_stream = StringIO()\n            logger.handlers[0].stream = new_stream\n            yield new_stream\n            logger.handlers[0].stream = old_stream\n\n        with patch_django_server_logger() as logger_output:\n            logger.info(log_msg, extra={'server_time': server_time})\n            self.assertEqual(\n                f'[{server_time}] {log_msg.replace(\"\\n\", \" \\\\n\")}\\n',\n                logger_output.getvalue()\n            )\n", "    def test_server_formatter_handles_empty_status_code(self):\n        formatter = ServerFormatter()\n        log_msg = 'log message'\n        record = logging.makeLogRecord({'msg': log_msg, 'status_code': None})\n        self.assertEqual(formatter.format(record), log_msg)\n"], "sample_573": ["    def test_order(self, df):\n\n        groupby = GroupBy([\"group\"])\n        for order in [1, 2, 3, 4]:\n            res = PolyFit(order=order)(df[[\"x\", \"y\"]], groupby, \"x\", {})\n            assert res.shape[0] == 100  # Check grid size is still respected\n", "    def test_order_too_large(self, df):\n        groupby = GroupBy([\"group\"])\n        res = PolyFit(order=df[\"x\"].nunique() + 1)(df[[\"x\", \"y\"]], groupby, \"x\", {})\n        assert res.empty\n", "    def test_order_param(self, df):\n        groupby = GroupBy([\"group\"])\n        res_order1 = PolyFit(order=1)(df[[\"x\", \"y\"]], groupby, \"x\", {})\n        res_order2 = PolyFit(order=3)(df[[\"x\", \"y\"]], groupby, \"x\", {})\n\n        # Check that the degree of the polynomial is reflected in the output\n\n        for _, part in res_order1.groupby(\"group\"):\n            assert len(np.polyfit(part[\"x\"], part[\"y\"], 1)) == 1\n\n        for _, part in res_order2.groupby(\"group\"):\n            assert len(np.polyfit(part[\"x\"], part[\"y\"], 3)) == 3\n", "    def test_order(self, df):\n\n        groupby = GroupBy([\"group\"])\n        res_linear = PolyFit(order=1)(df[[\"x\", \"y\"]], groupby, \"x\", {})\n        res_quadratic = PolyFit(order=2)(df[[\"x\", \"y\"]], groupby, \"x\", {})\n\n        for _, part in res_linear.groupby(\"group\"):\n            assert part[\"y\"].diff().diff().dropna().abs().gt(0).all()\n        for _, part in res_quadratic.groupby(\"group\"):\n            assert part[\"y\"].diff().diff().dropna().abs().gt(0).all()\n\n        assert_array_almost_equal(res_linear[\"y\"].values, res_quadratic[\"y\"].values) \n", "    def test_order(self, df):\n\n        groupby = GroupBy([\"group\"])\n\n        # Test with different orders\n        orders = [0, 1, 2, 3, 4]\n        for order in orders:\n            res = PolyFit(order=order)(df[[\"x\", \"y\"]], groupby, \"x\", {})\n\n            # Check if the number of fitted coefficients is as expected\n            if order == 0:\n                assert len(res.columns) == 2  # Only x and y are returned\n            else:\n                assert len(res.columns) == 3  # Add the group column\n"], "sample_221": ["    def test_in_lookup_with_prefetch_related(self):\n        group = Group.objects.create(name='Test Group')\n        Event.objects.create(title='Event 1', group=group)\n        Event.objects.create(title='Event 2', group=group)\n\n        qs = Event.objects.prefetch_related('group').filter(group__in=Group.objects.values('id'))\n        with self.assertNumQueries(1):\n            dumped = pickle.dumps(qs.query)\n\n        reloaded = pickle.loads(dumped)\n        with self.assertNumQueries(0):\n            reloaded_events = Event.objects.none()\n            reloaded_events.query = reloaded\n\n        self.assertSequenceEqual(reloaded_events, [\n            Event.objects.get(title='Event 1'),\n            Event.objects.get(title='Event 2'),\n        ])\n", "    def test_in_lookup_with_deferred_fields(self):\n        class DeferredGroup(models.Model):\n            name = models.CharField(max_length=100)\n            some_field = models.IntegerField(blank=True, null=True)\n\n        DeferredGroup.objects.create(name='DeferredGroup')\n        deferred_group = DeferredGroup.objects.get(name='DeferredGroup')\n        Event.objects.create(\n            title='Event 1', group=deferred_group\n        )\n        with self.assertNumQueries(1):  # Query for the deferred group and event\n            events = Event.objects.defer('group__some_field').filter(\n                group__in=DeferredGroup.objects.all()\n            )\n        dumped = pickle.dumps(events.query)\n        with self.assertNumQueries(0):\n            reloaded = pickle.loads(dumped)\n            reloaded_events = Event.objects.none()\n            reloaded_events.query = reloaded\n        self.assertSequenceEqual(reloaded_events, [self.e1])\n\n", "    def test_in_lookup_with_fk_and_m2m(self):\n        g1 = Group.objects.create(name='group 1')\n        g2 = Group.objects.create(name='group 2')\n        m2m_model = M2MModel.objects.create()\n        m2m_model.groups.add(g1)\n        m2m_model.groups.add(g2)\n\n        events = Event.objects.filter(group__in=M2MModel.objects.values('groups__id'))\n\n        with self.assertNumQueries(0):\n            dumped = pickle.dumps(events.query)\n\n        with self.assertNumQueries(0):\n            reloaded = pickle.loads(dumped)\n            reloaded_events = Event.objects.none()\n            reloaded_events.query = reloaded\n\n        self.assertSequenceEqual(reloaded_events, [self.e1]) \n", "    def test_in_lookup_complex_queryset(self):\n        events = Event.objects.filter(group__in=Event.objects.values('group_id').distinct())\n\n        with self.assertNumQueries(0):\n            dumped = pickle.dumps(events.query)\n\n        with self.assertNumQueries(0):\n            reloaded = pickle.loads(dumped)\n            reloaded_events = Event.objects.none()\n            reloaded_events.query = reloaded\n\n        self.assertSequenceEqual(reloaded_events, [self.e1])\n\n\n\n", "    def test_in_lookup_with_deferred_fields(self):\n        \"\"\"\n        Test that in lookups with deferred fields don't result in extra queries.\n        \"\"\"\n        from django.db import models\n        class DeferredModel(models.Model):\n            name = models.CharField(max_length=100)\n            deferred_field = models.CharField(max_length=100, db_index=True, blank=True)\n\n        DeferredModel.objects.create(name='Test', deferred_field='test-value')\n        deferred_objects = DeferredModel.objects.defer('deferred_field').all()\n\n        with self.assertNumQueries(1):\n            deferred_ids = deferred_objects.values_list('id', flat=True)\n            lookup_result = DeferredModel.objects.filter(id__in=deferred_ids)\n\n        self.assertSequenceEqual(lookup_result.values_list('name', flat=True), ['Test'])\n\n\n\n"], "sample_352": ["compilation error", "    def test_ticket_24605_distinct_on_non_pk(self):\n        \"\"\"\n        Subquery distinct ON clause should work with custom fields.\n        \"\"\"\n        m1 = Model1.objects.create(field_a='a1')\n        m2 = Model1.objects.create(field_a='a2')\n        m3 = Model1.objects.create(field_a='a3')\n\n        # Test a subquery that uses distinct on a field other than pk\n        qs = Model2.objects.filter(\n            model1__in=Model1.objects.filter(field_a__in=['a1', 'a2']).distinct('field_a'),\n        )\n        self.assertSequenceEqual(qs, [m1, m2])\n\n", "    def test_ticket_24605_complex_join(self):\n        \"\"\"\n        More complex join structure to ensure correct quoting.\n        \"\"\"\n        School.objects.create(name='school1')\n        School.objects.create(name='school2')\n        Student.objects.create(school_id=1, name='john')\n        Student.objects.create(school_id=1, name='jane')\n        Student.objects.create(school_id=2, name='peter')\n\n        class StudentGroup(models.Model):\n            name = models.CharField(max_length=50)\n            students = models.ManyToManyField(Student, through=\"StudentGroupMembership\")\n\n        class StudentGroupMembership(models.Model):\n            group = models.ForeignKey(StudentGroup, on_delete=models.CASCADE)\n            student = models.ForeignKey(Student, on_delete=models.CASCADE)\n\n        grp1 = StudentGroup.objects.create(name=\"group1\")\n        grp2 = StudentGroup.objects.create(name=\"group2\")\n        grp1.students.add(Student.objects.get(name=\"john\"))\n        grp1.students.add(Student.objects.get(name=\"jane\"))\n        grp2.students.add(Student.objects.get(name=\"peter\"))\n\n        query = (\n            StudentGroup.objects\n            .filter(\n                StudentGroupMembership.objects.filter(\n                    student__school__name='school1',\n                    student__name__contains='j'\n                ).exists()\n            )\n        )\n        self.assertSequenceEqual(query.values_list('name', flat=True), ['group1'])\n\n\n\n\n", "    def test_ticket_24605_complex(self):\n        \"\"\"\n        Make sure __pk__in and __in work the same for related fields when\n        using a distinct on subquery with more complex queries.\n        \"\"\"\n        a1 = Ticket23605A.objects.create()\n        a2 = Ticket23605A.objects.create()\n        c1 = Ticket23605C.objects.create(field_c0=0.0)\n        Ticket23605B.objects.create(\n            modela_fk=a1, field_b0=123,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        Ticket23605B.objects.create(\n            modela_fk=a1, field_b0=23,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        Ticket23605B.objects.create(\n            modela_fk=a1, field_b0=234,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        Ticket23605B.objects.create(\n            modela_fk=a1, field_b0=12,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        Ticket23605B.objects.create(\n            modela_fk=a2, field_b0=567,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        Ticket23605B.objects.create(\n            modela_fk=a2, field_b0=76,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        qx = Q(\n            ticket23605b__pk__in=Ticket23605B.objects.filter(\n                Q(field_b0__gte=100) & Q(field_b1=True)\n            ).distinct('modela_fk').order_", "    def test_ticket_24605_ordering(self):\n        \"\"\"\n        Ensure subquery ordering is respected with complex queries.\n        \"\"\"\n        i1 = Individual.objects.create(alive=True)\n        RelatedIndividual.objects.create(related=i1)\n        i2 = Individual.objects.create(alive=False)\n        RelatedIndividual.objects.create(related=i2)\n        \n        i3 = Individual.objects.create(alive=True)\n        i4 = Individual.objects.create(alive=False)\n\n        qs = Individual.objects.filter(Q(alive=True)).exclude(\n            Q(related_individual__isnull=True)\n        ).order_by('-pk') \n        self.assertSequenceEqual(list(qs), [i1, i3]) \n        \n"], "sample_757": ["compilation error", "compilation error", "compilation error", "def test_one_hot_encoder_sparse_output():\n    X = np.array([['a', 'b'], ['c', 'd']]).T\n    enc = OneHotEncoder(sparse=True)\n    X_tr = enc.fit_transform(X)\n    assert isinstance(X_tr, sparse.csr_matrix)\n", "def test_one_hot_encoder_sparse_output():\n    X = np.array([['a', 2, 55], ['def', 1, 55]])\n    enc = OneHotEncoder(sparse=True)\n    X_tr = enc.fit_transform(X)\n    assert isinstance(X_tr, sparse.csr_matrix)\n\n"], "sample_183": ["    def test_when_output_field_for_values(self):\n        with self.assertRaises(ValueError):\n            Case(\n                When(True, then=Value(True, output_field=CharField())),\n            )\n        with self.assertRaises(ValueError):\n            Case(\n                When(True, then=Value(True)),\n            )\n\n", "    def test_invalid_output_field_type(self):\n        msg = 'The output_field argument in When() must be a Field or None.'\n        with self.assertRaisesMessage(TypeError, msg):\n            Case(When(Q(pk=1), then=Value('1', output_field=str)),)\n", "    def test_unsupported_output_fields(self):\n        with self.assertRaises(AttributeError):\n            Case(When(Q(pk=1), then=Value('string', output_field=object())), output_field=IntegerField())\n        with self.assertRaises(AttributeError):\n            Case(When(Q(pk=1), then=Value(1, output_field=Value)), output_field=DateTimeField())\n", "    def test_non_callable_then_argument(self):\n        msg = 'Then argument must be a callable.'\n        with self.assertRaisesMessage(TypeError, msg):\n            Case(When(Q(pk=1), then=object())\n", "    def test_invalid_output_field(self):\n        with self.assertRaisesMessage(TypeError, \"output_field must be a Model field or a Field object\"):\n            Case(When(Q(pk=1)), then=1, output_field='invalid')\n"], "sample_262": ["    def test_lazy_unpickle(self):\n        t = lazy(lambda: 'hello', str)\n        pickled_t = pickle.dumps(t)\n        unpickled_t = pickle.loads(pickled_t)\n        self.assertEqual(unpickled_t(), 'hello')\n\n        t = lazy(lambda: object(), object)\n        pickled_t = pickle.dumps(t)\n        unpickled_t = pickle.loads(pickled_t)\n        self.assertIsInstance(unpickled_t(), object)\n", "    def test_keep_lazy(self):\n            return x * 2\n\n        @keep_lazy\n            return lazy(double, int)(x)\n\n        self.assertEqual(delayed_double(2), 4)\n\n        @keep_lazy_text\n            return lazy(str, str)(f'{x}!')\n\n        self.assertEqual(delayed_str('Hello'), 'Hello!')\n\n\n", "    def test_keep_lazy(self):\n        @keep_lazy(int)\n            return a + b\n\n        self.assertEqual(addition_func(1, 2), 3)\n        self.assertEqual(addition_func(lazy(lambda: 1), 2), 3)\n        self.assertEqual(addition_func(1, lazy(lambda: 2)), 3)\n        self.assertEqual(addition_func(lazy(lambda: 1), lazy(lambda: 2)), 3)\n\n\n", "    def test_keep_lazy(self):\n            return x + y\n\n        @keep_lazy(int, int)\n            return inner_func(x, y)\n\n        self.assertEqual(outer_func(1, 2), 3)\n        self.assertEqual(outer_func(3, 4), 7)\n\n        @keep_lazy()\n            return 5\n\n        self.assertEqual(outer_func(lazy_func(), 2), 7)\n        self.assertEqual(outer_func(1, lazy_func()), 6)\n", "    def test_lazy_with_different_order_arguments(self):\n        class Klazz:\n                self.a = a\n                self.b = b\n\n        t = lazy(lambda x, y: Klazz(y, x), Klazz)\n        self.assertEqual(t(2, 3).a, 3)\n        self.assertEqual(t(2, 3).b, 2)\n\n\n\n"], "sample_617": ["def test_mean_reduce_multiindex() -> None:\n    data = np.array(\n        [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]]\n    )\n    index = pd.MultiIndex.from_tuples(\n        [(\"A\", i) for i in range(3)], names=[\"group\", \"idx\"]\n    )\n    da = xr.DataArray(data, dims=[\"idx\", \"value\"], index=index)\n    expected = xr.array([\n        [6.0], \n        [15.0], \n    ], dims=[\"group\"], index=index.get_level_values(\"group\"))\n\n    assert_identical(da.reduce(np.mean, \"idx\"), expected)\n\n\n", "def test_polyval_invalid_inputs(use_dask: bool) -> None:\n    x = xr.DataArray([1, 2, 3], dims=\"x\")\n    coeffs = xr.DataArray([2, 3, 4], dims=\"degree\", coords={\"degree\": [0, 1, 2]})\n    \n    with pytest.raises(ValueError, match=r\"Dimensions of x and coeffs must be same\"):\n        xr.polyval(x, coeffs.transpose(\"degree\", \"x\"))\n    \n    if use_dask:\n        if not has_dask:\n            pytest.skip(\"requires dask\")\n        coeffs = coeffs.chunk({\"degree\": 2})\n        x = x.chunk({\"x\": 2})\n\n    with pytest.raises(ValueError, match=r\"Non-numeric dtype\"):\n        xr.polyval(x, xr.DataArray([1, 2, 3], dims=\"x\", dtype=\"category\"))\n\n    with pytest.raises(ValueError, match=r\"coeff must be a 1D array\"):\n        xr.polyval(x, xr.DataArray([1, 2, 3], dims=(\"x\", \"y\")))\n\n\n", "    def test_cross_zero_vector_result(a, b, ae, be, dim: str, axis: int) -> None:\n        expected = np.cross(ae, be, axis=axis)\n        actual = xr.cross(a, b, dim=dim)\n        xr.testing.assert_duckarray_allclose(expected, actual)\n", "def test_cross_ndim(a, b, axis, expected_shape):\n    actual = xr.cross(a, b, axis=axis)\n    assert actual.shape == expected_shape\n", "compilation error"], "sample_489": ["    def test_update_conflicts_null_values(self):\n        NullableFields.objects.bulk_create(\n            [\n                NullableFields(datetime_field=Now()),\n                NullableFields(integer_field=10),\n                NullableFields(text_field=\"hello\"),\n            ]\n        )\n        self.assertEqual(NullableFields.objects.count(), 3)\n\n        new_data = [\n            NullableFields(pk=1, datetime_field=None),\n            NullableFields(pk=2, integer_field=None),\n            NullableFields(pk=3, text_field=None),\n        ]\n        results = NullableFields.objects.bulk_create(\n            new_data,\n            update_conflicts=True,\n            update_fields=[\n                \"datetime_field\",\n                \"integer_field\",\n                \"text_field\",\n            ],\n        )\n        self.assertEqual(len(results), len(new_data))\n        if connection.features.can_return_rows_from_bulk_insert:\n            for instance in results:\n                self.assertIsNotNone(instance.pk)\n        self.assertEqual(NullableFields.objects.count(), 3)\n\n        self.assertCountEqual(\n            NullableFields.objects.values(\n                \"id\", \"datetime_field\", \"integer_field\", \"text_field\"\n            ),\n            [\n                {\"id\": 1, \"datetime_field\": None, \"integer_field\": None, \"text_field\": None},\n                {\"id\": 2, \"datetime_field\": None, \"integer_field\": None, \"text_field\": None},\n                {\"id\": 3, \"datetime_field\": None, \"integer_field\": None, \"text_field\": None},\n            ],\n        )\n\n\n\n", "    def test_update_conflicts_unique_fields_update_fields_with_pk(self):\n        PkWithDbColumns.objects.bulk_create(\n            [\n                PkWithDbColumns(id=1, rank=1, name=\"a\"),\n                PkWithDbColumns(id=2, rank=2, name=\"b\"),\n            ]\n        )\n        self.assertEqual(PkWithDbColumns.objects.count(), 2)\n\n        conflicting_objects = [\n            PkWithDbColumns(id=1, rank=1, name=\"c\"),\n            PkWithDbColumns(id=2, rank=2, name=\"d\"),\n        ]\n        results = PkWithDbColumns.objects.bulk_create(\n            conflicting_objects,\n            update_conflicts=True,\n            unique_fields=[\"id\"],\n            update_fields=[\"rank\", \"name\"],\n        )\n        self.assertEqual(len(results), len(conflicting_objects))\n        if connection.features.can_return_rows_from_bulk_insert:\n            for instance in results:\n                self.assertIsNotNone(instance.pk)\n        self.assertEqual(PkWithDbColumns.objects.count(), 2)\n        self.assertCountEqual(\n            PkWithDbColumns.objects.values(\"id\", \"rank\", \"name\"),\n            [\n                {\"id\": 1, \"rank\": 1, \"name\": \"c\"},\n                {\"id\": 2, \"rank\": 2, \"name\": \"d\"},\n            ],\n        )\n\n\n\n", "    def test_update_conflicts_no_unique_fields_multiple_update_fields(self):\n        TwoFields.objects.bulk_create(\n            [\n                TwoFields(f1=1, f2=1, name=\"a\"),\n                TwoFields(f1=2, f2=2, name=\"b\"),\n            ]\n        )\n        self.assertEqual(TwoFields.objects.count(), 2)\n\n        conflicting_objects = [\n            TwoFields(f1=1, f2=1, name=\"c\", f3=1),\n            TwoFields(f1=2, f2=2, name=\"d\", f3=2),\n        ]\n        results = TwoFields.objects.bulk_create(\n            conflicting_objects,\n            update_conflicts=True,\n            update_fields=[\"name\", \"f3\"],\n        )\n        self.assertEqual(len(results), len(conflicting_objects))\n        if connection.features.can_return_rows_from_bulk_insert:\n            for instance in results:\n                self.assertIsNotNone(instance.pk)\n        self.assertEqual(TwoFields.objects.count(), 2)\n        self.assertCountEqual(\n            TwoFields.objects.values(\"f1\", \"f2\", \"name\", \"f3\"),\n            [\n                {\"f1\": 1, \"f2\": 1, \"name\": \"c\", \"f3\": 1},\n                {\"f1\": 2, \"f2\": 2, \"name\": \"d\", \"f3\": 2},\n            ],\n        )\n", "    def test_update_conflicts_no_unique_fields_no_update_fields(self):\n        with self.assertRaises(ValueError):\n            UpsertConflict.objects.bulk_create(\n                [\n                    UpsertConflict(number=1, rank=1, name=\"John\"),\n                    UpsertConflict(number=2, rank=2, name=\"Mary\"),\n                    UpsertConflict(number=3, rank=3, name=\"Hannah\"),\n                ],\n                update_conflicts=True,\n            )\n", "    def test_update_conflicts_no_unique_fields(self):\n        self._test_update_conflicts(unique_fields=[])\n"], "sample_1145": ["def test_refine_power_with_zero():\n    x = Symbol('x', real=True)\n    y = Symbol('y', real=True)\n    assert refine(x**0, Q.real(x)) == 1\n    assert refine(x**0, Q.positive(x)) == 1\n    assert refine(x**0, Q.negative(x)) == 1\n    assert refine(0**x, Q.real(x)) == 0\n    assert refine(0**x, Q.positive(x)) == 0\n    assert refine(0**x, Q.negative(x)) == 0\n    assert refine((x*y)**0, Q.real(x) & Q.real(y)) == 1\n    assert refine((x*y)**0, Q.positive(x) & Q.positive(y)) == 1\n    assert refine((x*y)**0, Q.negative(x) & Q.negative(y)) == 1\n", "def test_refine_Mul():\n    from sympy import Q, Mul\n    x = Symbol('x', real=True)\n    y = Symbol('y', real=True)\n    z = Symbol('z', real=True)\n    assert refine(Mul(x, y), Q.positive(x) & Q.positive(y)) == x * y\n    assert refine(Mul(x, y), Q.positive(x) & Q.negative(y)) == -x * y\n    assert refine(Mul(x, y), Q.negative(x) & Q.positive(y)) == -x * y\n    assert refine(Mul(x, y), Q.negative(x) & Q.negative(y)) == x * y\n    assert refine(Mul(x, y, z), Q.positive(x) & Q.positive(y) & Q.positive(z)) == x * y * z\n    assert refine(Mul(x, y, z), Q.positive(x) & Q.positive(y) & Q.negative(z)) == -x * y * z\n    assert refine(Mul(x, y, z), Q.positive(x) & Q.negative(y) & Q.positive(z)) == -x * y * z\n\n\n", "def test_refine_non_numeric_args():\n    x = Symbol('x', real = True)\n    y = Symbol('y', positive = True)\n    assert refine(sqrt(x**2 + y**2), Q.real(x) & Q.real(y)) == sqrt(x**2 + y**2)\n    assert refine(sqrt(x*y), Q.positive(x) & Q.positive(y)) == sqrt(x*y)\n    assert refine(sign(x*y), Q.positive(x) & Q.positive(y)) == 1\n    assert refine(sign(x*y), Q.positive(x) & Q.negative(y)) == -1\n", "def test_refine_with_Assumptions():\n    from sympy import Q, Symbol\n    x = Symbol('x')\n    assert refine(x**2, Q.even(x)) == x**2\n    assert refine(x**2, Q.odd(x)) == x**2\n    assert refine(x**2, Q.integer(x)) == x**2\n    assert refine(sqrt(x**2), Q.even(x)) == x\n\n    assert refine(sqrt(x**2), Q.odd(x)) == -x\n", "def test_refine_recursive():\n    from sympy.core.symbol import Symbol\n    x = Symbol('x', real=True)\n    y = Symbol('y', real=True)\n\n    # Test recursive calls within refine\n    expr = x**2 + 2*x*y + y**2\n    refined_expr = refine(expr, Q.real(x) & Q.real(y))\n    assert refined_expr == (x + y)**2\n\n    # Test handling of assumptions in nested expressions\n    expr2 = sqrt(x**2 + y**2)\n    refined_expr2 = refine(expr2, Q.positive(x) & Q.positive(y))\n    assert refined_expr2 == x + y \n"], "sample_1180": ["def test_issue_17037():\n    p = Point(1, 2, 3)\n    assert p.translate(Matrix([[1, 0, 0], [0, 1, 0], [0, 0, 1]])) == Point(1, 2, 3) \n", "compilation error", "compilation error", "compilation error", "def test_projection():\n    p = Point2D(1, 1)\n    v = Point2D(0, 1)\n    proj = Point2D(0, 1)\n    assert p.projection(v) == proj\n"], "sample_75": ["    def test_prefetch_with_multiple_levels(self):\n        with self.assertNumQueries(5):\n            authors = AuthorWithAge.objects.prefetch_related(\n                Prefetch(\n                    'author',\n                    queryset=Author.objects.select_related(\n                        'favorite_authors',\n                        'favorite_authors__first_book'\n                    ),\n                )\n            )\n            self.assertQuerysetEqual(authors, ['<AuthorWithAge: Rousseau>', '<AuthorWithAge: Voltaire>'])\n        with self.assertNumQueries(0):\n            for author in authors:\n                self.assertEqual(\n                    author.author.favorite_authors.count(), 1\n                )\n", "    def test_prefetching_with_empty_m2m_cache(self):\n        with self.assertNumQueries(2):\n            authors = AuthorWithAge.objects.prefetch_related(\n                Prefetch('favorite_authors__first_book')\n            )\n            self.assertCountEqual(authors, [self.author1, self.author2])\n\n        with self.assertNumQueries(0):\n            self.assertEqual(self.author1.favorite_authors.first().first_book, None)\n\n\n\n", "    def test_prefetch_related_with_multiple_nested_prefetches(self):\n        with self.assertNumQueries(5):\n            authors = AuthorWithAge.objects.prefetch_related(\n                Prefetch(\n                    'first_book',\n                    queryset=Book.objects.prefetch_related(\n                        Prefetch('authors', queryset=Author.objects.select_related('addresses'))\n                    ),\n                )\n            )\n        with self.assertNumQueries(0):\n            author = authors[0]\n            self.assertEqual(len(author.first_book.authors.all()), 1)\n            self.assertEqual(len(author.first_book.authors.all()[0].addresses.all()), 1)\n", "    def test_prefetch_related_with_intersection_lookup(self):\n        B1 = Book.objects.create(title=\"Poems\")\n        B2 = Book.objects.create(title=\"Jane Eyre\")\n        A1 = Author.objects.create(name=\"Charlotte\", first_book=B1)\n        A2 = Author.objects.create(name=\"Anne\", first_book=B1)\n        A3 = Author.objects.create(name=\"Emily\", first_book=B2)\n        B1.authors.add(A1)\n        B1.authors.add(A2)\n        B2.authors.add(A3)\n\n        with self.assertNumQueries(2):\n            qs = Book.objects.prefetch_related(\n                Prefetch('authors', queryset=Author.objects.filter(name__startswith='A'))\n            )\n\n        with self.assertNumQueries(0):\n            for book in qs:\n                self.assertCountEqual(book.authors.all(), [A1, A2])\n\n            self.assertEqual(qs.count(), 2)\n\n\n\n", "    def test_prefetch_related_with_related_manager(self):\n        \"\"\"\n        Tests prefetch_related() with a RelatedManager that has its own filter.\n        \"\"\"\n        with self.assertNumQueries(2):\n            authors = AuthorWithAge.objects.prefetch_related(\n                Prefetch(\n                    'first_book__title',\n                    queryset=Book.objects.filter(title__contains='Volume'),\n                ),\n            )\n        with self.assertNumQueries(0):\n            self.assertSequenceEqual([a.first_book.title for a in authors], ['Les confessions Volume I'])\n\n\n"], "sample_579": ["    def test_clustermap_with_cmap(self):\n        kws = self.default_kws.copy()\n        kws['cmap'] = 'viridis'\n        g = mat.clustermap(self.df_norm, **kws)\n        # Ensure the colormap is applied to the heatmap\n        cmap = g.ax_heatmap.get_cmap()\n        assert cmap.name == 'viridis' \n", "    def test_mask_with_color_mapping(self):\n\n        cmaps = [\"viridis\", \"coolwarm\", \"RdBu_r\"]\n\n        for cmap in cmaps:\n            kws = self.default_kws.copy()\n            kws[\"mask\"] = self.df_norm > 0\n            kws[\"cmap\"] = cmap\n            g = mat.clustermap(self.df_norm, **kws)\n            for i in range(self.df_norm.shape[0]):\n                for j in range(self.df_norm.shape[1]):\n                    if self.df_norm.iloc[i, j] > 0:\n                        assert g.ax_heatmap.patches[i, j].get_facecolor() != (1, 1, 1)\n                    else:\n                        assert g.ax_heatmap.patches[i, j].get_facecolor() == (1, 1, 1) \n", "    def test_clustermap_with_matplotlib_backend_pdf(self):\n\n        # This tests that everything renders correctly when using the pdf backend.\n        mpl.use(\"pdf\")\n        fig = mat.clustermap(self.df_norm, **self.default_kws)\n        fig.savefig(\"test.pdf\")\n        mpl.pyplot.close(\"all\")\n\n\n\n", "    def test_clustermap_with_masked_data(self):\n        mask = np.random.choice([True, False], size=self.df_norm.shape)\n        masked_df = pd.DataFrame(np.where(mask, self.df_norm.values, np.nan))\n        \n        kws = self.default_kws.copy()\n        kws[\"mask\"] = mask\n\n        g = mat.clustermap(masked_df, **kws)\n        assert np.isnan(g.data2d).all() == False\n        assert np.isnan(g.data2d[~mask]).all() == True\n\n\n\n", "    def test_linkage_argument(self):\n        x = np.random.rand(10, 10)\n\n        with pytest.raises(ValueError):\n            mat.clustermap(x, linkage='foo')\n\n        for linkage in ['ward', 'single', 'complete', 'average']:\n            mat.clustermap(x, linkage=linkage)\n"], "sample_1012": ["def test_print_matrix_with_complex_numbers():\n    from sympy.matrices import Matrix\n\n    A = Matrix([[1 + 2j, 3 + 4j], [5 + 6j, 7 + 8j]])\n    p = NumPyPrinter()\n    assert p.doprint(A) \\\n        == \"numpy.array([[1+2j, 3+4j], [5+6j, 7+8j]], dtype=complex)\" \n", "def test_issue_17499():\n    p = PythonCodePrinter()\n    assert p.doprint(expm1(x)) == 'math.expm1(x)'\n", "compilation error", "def test_print_MatrixBase():\n    from sympy import Matrix, eye\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(Matrix([[1, 2], [3, 4]])) == 'numpy.array([[1, 2], [3, 4]])'\n    assert prntr.doprint(eye(2)) == 'numpy.eye(2)'\n", "def test_logical_functions():\n    p = NumPyPrinter()\n    assert p.doprint(And(x > 0, y < 1)) == 'numpy.logical_and(x > 0, y < 1)'\n    assert p.doprint(Or(x > 0, y < 1)) == 'numpy.logical_or(x > 0, y < 1)'\n    assert p.doprint(Not(x > 0)) == 'numpy.logical_not(x > 0)'\n"], "sample_756": ["def test_dbscan_optics_parity_with_varying_eps():\n    # Test that OPTICS clustering labels are <= 5% difference of DBSCAN for a range of eps values\n    centers = [[1, 1], [-1, -1], [1, -1]]\n    X, labels_true = make_blobs(n_samples=750, centers=centers,\n                                cluster_std=0.4, random_state=0)\n\n    # calculate optics with dbscan extract at 0.3 epsilon\n    for eps in [0.1, 0.2, 0.3, 0.4, 0.5]:\n        op = OPTICS(min_samples=9, max_eps=1).fit(X)\n        core_optics, labels_optics = op.extract_dbscan(eps)\n\n        # calculate dbscan labels\n        db = DBSCAN(eps=eps, min_samples=9).fit(X)\n\n        contingency = contingency_matrix(db.labels_, labels_optics)\n        agree = min(np.sum(np.max(contingency, axis=0)),\n                    np.sum(np.max(contingency, axis=1)))\n        disagree = X.shape[0] - agree\n\n        # verify core_labels match\n        assert_array_equal(core_optics, db.core_sample_indices_)\n\n        # verify label mismatch is <= 5% labels\n        percent_mismatch = np.round((disagree - 1) / (X.shape[0] - len(db.core_sample_indices_)), 2)\n\n        assert percent_mismatch <= 0.05\n", "def test_random_state():\n    rng = np.random.RandomState(42)\n    X = generate_clustered_data(n_samples=100, n_clusters=3,\n                                 cluster_std=0.4, rng=rng)\n    clust1 = OPTICS(random_state=rng).fit(X)\n    clust2 = OPTICS(random_state=rng).fit(X)\n    assert_array_equal(clust1.labels_, clust2.labels_)\n    assert_array_equal(clust1.ordering_, clust2.ordering_)\n", "def test_connectivity_of_clusters():\n    # Test connectivity of clusters when varying eps & min_samples\n\n    centers = [[1, 1], [-1, -1], [1, -1]]\n    X, labels_true = make_blobs(n_samples=750, centers=centers,\n                                cluster_std=0.4, random_state=0)\n\n    # Test with a small eps and high min_samples (should mostly find single pts)\n    clust1 = OPTICS(eps=0.1, min_samples=10).fit(X)\n    assert_array_equal(clust1.labels_, np.array([-1, -1, -1, -1, -1, -1,\n                                                -1, -1, -1, -1, -1, -1,\n                                                -1, -1, -1, -1, -1, -1, -1,\n                                                -1, -1, -1, -1, -1, -1, -1,\n                                                -1, -1, -1, -1, -1, -1, -1,\n                                                -1, -1, -1, -1, -1, -1, -1,\n                                                -1, -1, -1, -1, -1, -1, -1,\n                                                -1, -1, -1, -1, -1, -1, -1,\n                                                -1, -1, -1, -1, -1, -1, -1,\n                                                -1, -1, -1, -1, -1, -1, -1,\n                                                -1, -1, -1, -1, -1, -1, -1,\n                                                -1, -1, -1, -1, -1, -1, -1,\n                                                -1, -1, -1, -1, -1, -1, -1,\n                                                -1, -1, -1, -1, -1, -1, -1,\n                                                -1, -1, -1, -1, -1, -1, -1,\n                                                -1, -1, -1", "    def test_cluster_limit(cluster_limit):\n        # test that cluster_limit correctly limits the number of clusters produced.\n        X = generate_clustered_data(n_clusters=10)  # generate data with 10 clusters.\n        clust = OPTICS(min_samples=10, cluster_limit=cluster_limit).fit(X)\n        if cluster_limit is None:\n            assert len(set(clust.labels_)) >= 10\n        else:\n            assert len(set(clust.labels_)) <= cluster_limit\n\n\n\n", "compilation error"], "sample_154": ["    def test_empty_databases(self):\n        issues = check_database_backends(databases=[])\n        self.assertEqual(issues, [])\n", "    def test_empty_databases_list(self):\n        issues = check_database_backends(databases=[])\n        self.assertEqual(issues, [])\n", "    def test_empty_databases(self):\n        issues = check_database_backends(databases=[])\n        self.assertEqual(issues, [])\n", "    def test_no_databases(self, mock_connections):\n        mock_connections.return_value = []\n        self.assertEqual(check_database_backends(), [])\n", "    def test_empty_databases(self, mocked_connection):\n        mocked_connection.return_value = None  \n        issues = check_database_backends(databases=['fake', 'another'])\n        self.assertEqual(issues, [])\n"], "sample_274": ["    def test_modelchoicefield_empty_label(self):\n        # When empty_label is True, the empty choice should be considered valid.\n        f = ModelChoiceField(\n            queryset=ChoiceModel.objects.all(),\n            empty_label=True,\n            error_messages={\n                'invalid_choice': '\"%(value)s\" is not one of the available choices.',\n            },\n        )\n        self.assertEqual(f.clean(''), None)  \n        self.assertFormErrors(\n            ['\"invalid\" is not one of the available choices.'],\n            f.clean,\n            'invalid',\n        )  \n\n\n", "    def test_custom_queryset(self):\n        ChoiceModel.objects.create(pk=1, name='a')\n        ChoiceModel.objects.create(pk=2, name='b')\n        ChoiceModel.objects.create(pk=3, name='c')\n        \n        class CustomChoiceModel(ChoiceModel):\n            pass\n        CustomChoiceModel.objects.create(pk=4, name='d')\n\n        f = ModelChoiceField(queryset=ChoiceModel.objects.filter(name__startswith='b'), error_messages={'invalid_choice': '\"%(value)s\" is not one of the available choices.'})\n        self.assertFormErrors(['\"4\" is not one of the available choices.'], f.clean, 4)\n        self.assertFormErrors(['\"1\" is not one of the available choices.'], f.clean, 1)\n        self.assertFormErrors(['\"c\" is not one of the available choices.'], f.clean, 3)\n\n\n\n", "    def test_modelchoicefield_empty_choices(self):\n        # ModelChoiceField\n        e = {\n            'required': 'REQUIRED',\n            'invalid_choice': 'INVALID CHOICE',\n        }\n        f = ModelChoiceField(queryset=ChoiceModel.objects.none(), error_messages=e)\n        self.assertFormErrors(['REQUIRED'], f.clean, '')\n        self.assertFormErrors(['INVALID CHOICE'], f.clean, '1')\n\n\n\n", "    def test_modelchoicefield_pk_value(self):\n        ChoiceModel.objects.create(pk=1, name='a')\n        ChoiceModel.objects.create(pk=2, name='b')\n        ChoiceModel.objects.create(pk=3, name='c')\n\n        f = ModelChoiceField(\n            queryset=ChoiceModel.objects.all(),\n            error_messages={\n                'invalid_pk_value': '\"%(pk)s\" is not a valid value.',\n            },\n        )\n        self.assertFormErrors(\n            ['\"invalid\" is not a valid value.'],\n            f.clean,\n            'invalid'\n        )\n\n\n\n", "    def test_modelchoicefield_pk_value_placeholder(self):\n        f = ModelChoiceField(\n            queryset=ChoiceModel.objects.all(),\n            error_messages={\n                'invalid_pk_value': '\"%(pk)s\" is not a valid value.',\n            },\n        )\n        self.assertFormErrors(\n            ['\"123\" is not a valid value.'],\n            f.clean,\n            '123',\n        )\n"], "sample_429": ["    def test_file_extension_validator_custom_error_codes(self):\n        v = FileExtensionValidator(\n            [\"txt\"], code=\"invalid_extension\"\n        )\n        with self.assertRaisesMessage(\n            ValidationError, \"invalid_extension: ['The file extension is invalid.']\"\n        ):\n            v(ContentFile(\"contents\", name=\"file.jpg\"))\n        v = FileExtensionValidator(\n            [\"txt\"], message=\"custom error message for invalid extension\"\n        )\n        with self.assertRaisesMessage(\n            ValidationError, \"custom error message for invalid extension: ['The file extension is invalid.']\"\n        ):\n            v(ContentFile(\"contents\", name=\"file.jpg\"))\n\n", "    def test_custom_code_message_ordering(self):\n        validator = RegexValidator(\n            r\"^a$\", custom_message=\"Invalid format: %(code)s: %(message)s\", code=\"invalid\"\n        )\n        with self.assertRaisesMessage(\n            ValidationError,\n            \"Invalid format: invalid: Your input does not start with 'a'\",\n        ):\n            validator(\"b\")\n\n\n", "    def test_validate_slug_with_invalid_characters(self):\n        validator = validate_slug\n        for char in [\"!\", \"@\", \"#\", \"$\", \"%\", \"^\", \"&\", \"*\", \"(\", \")\", \"-\", \"_\", \"+\", \"=\", \"[\", \"]\", \"{\", \"}\", \"|\", \"\\\\\", \": \", \";\", \"'\", '\"', \"<\", \">\", \"?\"]:\n            with self.subTest(char=char):\n                self.assertRaises(ValidationError, validator, f\"slug{char}slug\")\n\n\n", "    def test_invalid_decimal_values(self):\n        for value in [\n            \"invalid\",\n            \"1.2.3\",\n            \"1e\",\n            \"1e+\",\n            \"1e-\",\n            \"1E+\",\n            \"1E-\",\n            \"1.23.\",\n            \"1.23e1000\",\n            \"1.23E-1000\",\n            \"123\",\n            \"1.23456789012345678901234567890\",\n        ]:\n            with self.subTest(value=value):\n                for validator in (\n                    DecimalValidator(max_digits=10, decimal_places=2),\n                    DecimalValidator(max_digits=20, decimal_places=2),\n                ):\n                    with self.assertRaises(ValidationError):\n                        validator(value)\n\n", "    def test_url_validator_allow_schemes(self):\n        validator = URLValidator(schemes=[\"http\", \"https\", \"ftp\"])\n        with self.assertRaises(ValidationError):\n            validator(\"invalid:scheme://example.com\")\n        with self.assertRaises(ValidationError):\n            validator(\"http://example.com/path?q=test#frag\")\n        for url in VALID_URLS_WITH_SCHEMES:\n            if url.startswith((\"ftp\", \"http\", \"https\")):\n                validator(url)\n\n\n\n"], "sample_132": ["    def test_sensitive_variables_decorator_with_multiple_arguments(self):\n        @sensitive_variables\n            return password, secret_key, some_other_data\n\n        result = test_func('password123', 'secret_key456', 'other_data')\n        self.assertEqual(result, (test_func.cleansed_substitute, test_func.cleansed_substitute, 'other_data'))\n\n\n", "    def test_sensitive_variables_decorator_with_args(self):\n        @sensitive_variables(sensitive_attrs=['password', 'secret_key'])\n            return f'Hello {name}, your password is {password} and secret key is {secret_key}'\n\n        with self.settings(DEBUG=True):\n            response = self.client.post('/test_view/', data={'password': 'secret', 'secret_key': 'topsecret'})\n            self.assertContains(response, 'Hello John Doe', status_code=500)\n            self.assertNotIn('secret', response.content)\n            self.assertNotIn('topsecret', response.content)\n\n        with self.settings(DEBUG=False):\n            response = self.client.post('/test_view/', data={'password': 'secret', 'secret_key': 'topsecret'})\n            self.assertContains(response, 'Hello John Doe', status_code=500)\n            self.assertNotIn('secret', response.content)\n            self.assertNotIn('topsecret', response.content)\n\n", "    def test_sensitive_variables_decorator_with_kwargs(self):\n        @sensitive_variables\n            return f\"User: {user}, Password: {password}, Extra: {extra_info}\"\n\n        response = self.client.get('/test_view/', {'user': 'johndoe', 'password': 'secret', 'extra_info': 'test'})\n        self.assertContains(response, 'User: johndoe', status_code=200)\n        self.assertNotContains(response, 'Password: secret', status_code=200)\n        self.assertContains(response, 'Extra: test', status_code=200)\n\n", "    def test_sensitive_variables_decorator_works_with_function_arguments(self):\n        @sensitive_variables\n            return 'Password: ' + password + ', Username: ' + username\n\n        response = test_func('should not be displayed', 'should be displayed')\n        self.assertNotContains(response, 'should not be displayed')\n\n\n\n", "    def test_sensitive_variables_decorator_with_arguments(self):\n        @sensitive_variables(exclude=['cooked_eggs'])\n            pass\n        self.assertEqual(test_func.__dict__['__wrapped__'].__name__, 'test_func')\n        self.assertEqual(test_func.__dict__.get('sensitive_variables_exclude'), ['cooked_eggs'])\n"], "sample_726": ["def test_label_binarizer_empty_output():\n    mlb = MultiLabelBinarizer()\n    y = []\n    assert_array_equal(mlb.fit_transform(y), np.array([]))\n\n    mlb = MultiLabelBinarizer(classes=['a', 'b'])\n    assert_array_equal(mlb.fit_transform(y), np.array([]))\n", "compilation error", "def test_label_binarizer_with_custom_classes():\n    mlb = MultiLabelBinarizer(classes=['a', 'b', 'c', 'd'])\n    y = [('a', 'c'), ('b',), ('a', 'b', 'd')]\n    expected = np.array([[1, 0, 1, 0],\n                         [1, 1, 0, 0],\n                         [1, 0, 1, 1]])\n    assert_array_equal(mlb.fit_transform(y), expected)\n\n\n    y_inverse = mlb.inverse_transform(expected)\n    assert_array_equal(y_inverse, y)\n", "compilation error", "def test_label_binarize_sparse_output_errors():\n    y = np.array([0, 1, 0])\n    classes = [0, 1]\n    with assert_raises(ValueError):\n        label_binarize(y, classes, sparse_output=True, neg_label=-1,\n                       pos_label=2)\n\n    y_sparse = coo_matrix(y)\n    with assert_raises(ValueError):\n        label_binarize(y_sparse, classes, sparse_output=True, neg_label=-1,\n                       pos_label=2)\n"], "sample_214": ["    def test_key_transform_with_null(self):\n        obj = NullableJSONModel.objects.create(value={'a': None})\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(value__a=KeyTransform(None, KeyTransform('a', 'value'))),\n            [obj],\n        )\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(value__a=KeyTransform(None, 'value__a')),\n            [obj],\n        )\n\n", "    def test_key_contains_with_parent_transform(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(\n                value__d__0__contains=KeyTransform('f', 'value'),\n            ),\n            [self.objs[4]],\n        )\n\n\n\n", "    def test_contains_complex_with_key_transform(self):\n        value = KeyTransform('baz', 'value')\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(value__contains=value).values('value__baz'),\n            [{'a': 'b', 'c': 'd'}]\n        )\n", "    def test_contains_contained_by_with_key_transform_complex(self):\n        obj = NullableJSONModel.objects.create(\n            value={'baz': {'a': 'b', 'c': {'x': 'y', 'z': 'w'}}},\n        )\n        tests = [\n            ('value__baz__contains', {'a': 'b', 'c': {'y': 'y'}}),\n            ('value__baz__contains', {'a': 'b'}),\n            ('value__baz__contained_by', {'a': 'b', 'c': {'y': 'y'}}),\n            ('value__baz__contained_by', {'a': 'b', 'c': {'x': 'y', 'z': 'w'}}),\n        ]\n        for lookup, value in tests:\n            with self.subTest(lookup=lookup, value=value):\n                self.assertIs(NullableJSONModel.objects.filter(\n                    **{lookup: value},\n                ).exists(), True)\n\n        test_cases = [\n            ('value__baz__contains', {'c': {'x': 'y', 'z': 'w'}}),\n            ('value__baz__contained_by', {'c': {'x': 'y'}}),\n            ('value__baz__contained_by', {'c': {'y': 'y'}}),\n            ('value__baz__contained_by', {'c': {'x': 'y', 'z': 'w'}}),\n        ]\n        for lookup, value in test_cases:\n            with self.subTest(lookup=lookup, value=value):\n                self.assertFalse(NullableJSONModel.objects.filter(\n                    **{lookup: value},\n                ).exists())\n\n", "    def test_contains_with_key_transform_deep(self):\n        self.assertIs(NullableJSONModel.objects.filter(\n            value__d__1__contains=KeyTransform('f', 'value')).exists(), True)"], "sample_298": ["    def test_token_with_different_algorithm(self):\n        user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n\n        # Test with the default algorithm\n        p0 = PasswordResetTokenGenerator()\n        tk0 = p0.make_token(user)\n        self.assertIs(p0.check_token(user, tk0), True)\n\n        # Test with a different algorithm\n        p1 = PasswordResetTokenGenerator()\n        p1.algorithm = 'sha512'\n        tk1 = p1.make_token(user)\n        self.assertNotEqual(tk0, tk1)  # Tokens should be different\n        self.assertNotEqual(p0.check_token(user, tk1), True)  # Token should not be valid with the default algorithm\n        self.assertIs(p1.check_token(user, tk1), True)  # Token should be valid with the new algorithm\n\n\n\n", "    def test_token_not_generated_with_secret_when_secret_is_unset(self):\n        class SecretlessPasswordResetTokenGenerator(PasswordResetTokenGenerator):\n            secret = None\n\n        user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n        generator = SecretlessPasswordResetTokenGenerator()\n        with self.assertRaises(ImproperlyConfigured):\n            generator.make_token(user) \n", "    def test_token_generation_with_microseconds(self):\n        \"\"\"\n        Tests that the token generation handles microseconds correctly,\n        ensuring consistent token generation even across databases with\n        varying microsecond support.\n        \"\"\"\n        user = User.objects.create_user('testuser', 'test@example.com', 'testpw')\n        p0 = PasswordResetTokenGenerator()\n\n        # Force microseconds into the timestamp\n        now_with_microseconds = datetime.now(tz=datetime.timezone.utc).replace(microsecond=123456)\n        p1 = MockedPasswordResetTokenGenerator(now_with_microseconds)\n        tk1 = p1.make_token(user)\n\n        # Ensure microseconds are truncated during generation\n        p2 = PasswordResetTokenGenerator()\n        tk2 = p2.make_token(user)\n\n        self.assertEqual(tk1, tk2)\n", "    def test_token_generation_with_microseconds(self):\n        user = User.objects.create_user('testuser', 'test@example.com', 'testpassword')\n        p0 = PasswordResetTokenGenerator()\n        # Force the timestamp to have microseconds\n        now = datetime.now().replace(microsecond=123456)\n        with override_settings(PASSWORD_RESET_TIMEOUT=0):\n            p0._now = lambda: now\n            tk1 = p0.make_token(user)\n        \n        # Check that the token is valid\n        self.assertIs(p0.check_token(user, tk1), True)\n", "    def test_token_with_different_algorithm(self):\n        user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n        p0 = PasswordResetTokenGenerator()\n        p0.algorithm = 'sha1'\n        tk0 = p0.make_token(user)\n        self.assertIs(p0.check_token(user, tk0), True)\n\n        p1 = PasswordResetTokenGenerator()\n        p1.algorithm = 'sha256'\n        tk1 = p1.make_token(user)\n        self.assertIs(p1.check_token(user, tk1), True)\n\n        self.assertIs(p0.check_token(user, tk1), False)\n        self.assertIs(p1.check_token(user, tk0), False)\n"], "sample_1044": ["def test_issue_10436():\n    x = Symbol('x')\n    y = Symbol('y')\n    assert(x + y).is_commutative is True\n    assert(x*y).is_commutative is None\n    assert(x*x).is_commutative is True\n    assert(x**y).is_commutative is None\n    assert(y**x).is_commutative is None\n\n\n\n", "def test_issue_10378():\n    x = Symbol('x', nonzero=True)\n    assert (x**0).is_positive\n    assert (x**0).is_nonzero is True\n    assert (x**0).is_negative is False\n    assert (x**0).is_integer is True  \n", "def test_issue_10793():\n    x = Symbol('x', real=True)\n    y = Symbol('y', real=True)\n    z = Symbol('z', real=True)\n    assert (x**y).is_integer is None\n    assert (x**y).is_rational is None\n    assert (x**z).is_integer is None\n    assert (x**z).is_rational is None\n\n    assert (int(x**y)).is_integer\n    assert (int(x**z)).is_integer\n\n\n", "def test_issue_10977():\n    x = Symbol('x', real=True)\n    y = Symbol('y', real=True)\n    assert (x**y).is_finite is None\n    assert (x**y).is_real is None\n    assert (x**I).is_real is None\n    assert (x**2*I).is_real is None\n    assert (x**2 + x*I).is_real is None\n    assert (x**2 + x*I).is_imaginary is None\n\n\n\n", "def test_issue_10415():\n    x = Symbol('x', real=True)\n    assert (exp(I*x)).is_real is None\n"], "sample_193": ["    def test_circular_m2m_through(self):\n        A = self.create_model(\"A\", foreign_keys=[models.ManyToManyField('B', through='T')])\n        B = self.create_model(\"B\", foreign_keys=[models.ManyToManyField('A', through='T')])\n        T = self.create_model(\"T\", foreign_keys=[\n            models.ForeignKey('A', models.CASCADE),\n            models.ForeignKey('B', models.CASCADE),\n        ])\n        self.assertRelated(A, [B, T])\n        self.assertRelated(B, [A, T])\n        self.assertRelated(T, [A, B])\n\n", "    def test_proxy_to_pk(self):\n        A = self.create_model(\"A\", foreign_keys=[models.ForeignKey('B', models.CASCADE)])\n        B = self.create_model(\"B\")\n        C = self.create_model(\"C\", bases=(A,), proxy=True)\n        self.assertRelated(A, [B])\n        self.assertRelated(B, [A, C])\n        self.assertRelated(C, [A, B])\n", "    def test_proxy_through_m2m(self):\n        A = self.create_model(\"A\")\n        B = self.create_model(\"B\", proxy=True)\n        T = self.create_model(\"T\", foreign_keys=[\n            models.ForeignKey('A', models.CASCADE),\n            models.ForeignKey('B', models.CASCADE),\n        ])\n        self.assertRelated(A, [B, T])\n        self.assertRelated(B, [T])\n\n\n", "    def test_m2m_and_fk_with_same_model(self):\n        A = self.create_model(\"A\", foreign_keys=[models.ForeignKey('A', models.CASCADE)])\n        B = self.create_model(\"B\", foreign_keys=[models.ManyToManyField('A')])\n        self.assertRelated(A, [A, B])\n        self.assertRelated(B, [A])\n\n\n\n", "    def test_concrete_proxy_with_fk_to_abstract(self):\n        A = self.create_model(\"A\", abstract=True)\n        B = self.create_model(\"B\", bases=(A,), proxy=True)\n        C = self.create_model(\"C\")\n        self.assertRelated(A, [B, C])\n        self.assertRelated(B, [C])\n        self.assertRelated(C, [])\n"], "sample_1022": ["def test_repeated_decimals():\n    transformations = standard_transformations + (repeated_decimals,)\n    cases = {\n        '1.2[3]': '1.2 + 0.2 / 9',  \n        '0.123[45]': '0.123 + 0.001 / 99',   \n        '1.234[567]': '1.234 + 0.001 / 999',   \n        '0.1234[56789]': '0.1234 + 0.0001 / 99999', \n        '0.01(2)': '0.01 + 0.002 / 10'\n    }\n    for case, expected in cases.items():\n        assert(parse_expr(case, transformations=transformations) ==\n               parse_expr(expected))\n\n    \n", "def test_repeated_decimals():\n    cases = {\n        '0.2[1]': '0.21111111111111111...',\n        '0.12[34]': '0.12343434343434343...',\n        '0.123[4567]': '0.1234567456745674567...',\n        '0.[123]': '0.123123123123123123...',\n        '0.1[23]': '0.123232323232323232...',\n        '0.123[45]': '0.123454545454545454...',\n        '0.[123456789]': '0.123456789123456789...',\n        '0.01[2]': '0.012222222222222222...',\n        '0.1234[567]': '0.1234567567567567567...',\n        '0.00[1]': '0.0011111111111111111...',\n    }\n    transformations = standard_transformations\n    for case, expected in cases.items():\n        result = parse_expr(case, transformations=transformations)\n        assert(result.evalf(10) == float(expected))\n", "def test_repeated_decimals():\n    transformations = standard_transformations + (repeated_decimals,)\n    cases = {\n        '1.2[3]': '1.2 + 0.3/9',\n        '1.23[45]': '1.23 + 0.0045/99',\n        '0.1[23]': '0.1 + 0.023/9',\n        '0.123[456]': '0.123 + 0.000456/999',\n        '0.1[234]': '0.1 + 0.0234/999',\n        '1.234[567]': '1.234 + 0.000567/999',\n    }\n\n    for case in cases:\n        implicit = parse_expr(case, transformations=transformations)\n        normal = parse_expr(cases[case], transformations=standard_transformations)\n        assert(implicit == normal)\n\n\n\n", "def test_repeated_decimals():\n    transformations = standard_transformations + (repeated_decimals,)\n    cases = {\n        '0.1(1)': '0.1111111111111111',\n        '0.21(1)': '0.2121212121212121',\n        '0.123(456)': '0.123456456456456456',\n        '0.01(1)': '0.0111111111111111',\n        '0.023(4567)': '0.0234567456745674567',\n        '0.123456789(1)': '0.123456789123456789',\n        '0.1234567890(123456789)': '0.1234567890123456789',\n    }\n    for case, expected in cases.items():\n        assert(parse_expr(case, transformations=transformations) == parse_expr(expected)) \n\n\n", "def test_repeated_decimals():\n    cases = {\n        '0.2[1]': '0.21111111111111111',\n        '1.23[45]': '1.2345454545454545',\n        '0.123[456]': '0.123456456456456456',\n        '0.2[12]': '0.21212121212121212',\n        '0.12[3]': '0.12333333333333333',\n        '0.123[4567]': '0.1234567456745674567',\n        '0.123[456789]': '0.123456789456789456789',\n    }\n\n    transformations = standard_transformations + (\n        repeated_decimals,)\n    for case in cases:\n        implicit = parse_expr(case, transformations=transformations)\n        normal = parse_expr(cases[case], transformations=transformations)\n        assert(implicit == normal)\n"], "sample_1032": ["def test_issue_14000_evaluation():\n    assert sqrt(4).evalf() == 2\n    assert cbrt(3.5).evalf() == 1.5163\n    assert root(16, 4).evalf() == 2\n    assert real_root(-8, 3).evalf() == -2\n\n\n\n", "def test_issue_14906():\n    from sympy.abc import x, y\n    assert Min(x, y).subs(x, y) == y\n    assert Max(x, y).subs(x, y) == y\n", "def test_issue_14000_continued():\n\n    from sympy.abc import x\n    assert root(x, 2, evaluate=False).has(Pow) == True\n    assert real_root(x, 3, evaluate=False).has(Pow) == True\n\n\n", "compilation error", "def test_issue_12638():\n    from sympy.abc import a, b, c, d\n    assert Min(a, b, c, Max(a, b)) == Min(a, b, c)\n    assert Min(a, b, Max(a, b, c)) == Min(a, b)\n    assert Min(a, b, Max(a, c)) == Min(a, b)\n"], "sample_837": ["def test_show_versions_without_dependency():\n    import unittest\n    from unittest.mock import patch\n\n    class MockModule:\n        __version__ = \"1.2.3\"\n\n    with patch(\"sklearn.utils._show_versions.importlib.import_module\",\n               side_effect=ImportError):\n        show_versions()\n        \n\n", "def test_show_versions_missing_dep():\n    import unittest\n    from unittest.mock import patch\n    from sklearn.utils._show_versions import _get_deps_info\n\n    with patch('sklearn.utils._show_versions.importlib.import_module',\n               side_effect=ImportError):\n        show_versions()\n        \n        deps_info = _get_deps_info()\n\n        assert 'numpy' in deps_info\n        assert deps_info['numpy'] is None \n", "def test_show_versions_with_missing_dep():\n    import unittest\n    from unittest.mock import patch\n    from sklearn.utils._show_versions import show_versions\n    from sklearn.utils._show_versions import _get_deps_info\n\n    class MockModule:\n        __version__ = None\n\n        if module_name == 'missing_dep':\n            return MockModule()\n        return None\n\n    with patch('sklearn.utils._show_versions.importlib.import_module',\n               side_effect=mock_import_module):\n        show_versions()\n        deps_info = _get_deps_info()\n\n    assert 'missing_dep' in deps_info\n    assert deps_info['missing_dep'] is None\n", "def test_show_versions_with_missing_dep():\n    import unittest.mock\n\n    with unittest.mock.patch('sklearn.utils._show_versions.importlib.import_module',\n                             side_effect=ImportError):\n        show_versions()\n    # Check that missing dependencies are reported as None\n    #  in the output. \n\n\n\n", "def test_show_versions_missing_deps():\n    import unittest\n    from unittest.mock import patch\n    from sklearn.utils._show_versions import _get_deps_info, show_versions\n\n\n    class MockModule:\n        __version__ = None\n\n    @patch('sklearn.utils._show_versions._get_deps_info', side_effect=lambda: {\n        'pip': '20.3.4',\n        'setuptools': '66.0.0',\n        'sklearn': '1.2.3',\n        'numpy': None,\n        'scipy': None,\n        'Cython': None,\n        'pandas': None,\n        'matplotlib': None,\n        'joblib': None\n    })\n        show_versions()\n        # Assert that missing dependencies are displayed as None\n        assert 'numpy: None' in  str(mock_deps_info.return_value)\n        assert 'scipy: None' in  str(mock_deps_info.return_value)\n        assert 'Cython: None' in  str(mock_deps_info.return_value)\n        assert 'pandas: None' in  str(mock_deps_info.return_value)\n        assert 'matplotlib: None' in  str(mock_deps_info.return_value)\n        assert 'joblib: None' in  str(mock_deps_info.return_value)\n\n\n\n"], "sample_662": ["    def test_extra_data_in_report(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n                assert False\n        \"\"\"\n        )\n        reprec = testdir.inline_run()\n        reports = reprec.getreports(\"pytest_runtest_logreport\")\n        assert len(reports) == 3\n        test_a_call = reports[1]\n        test_a_call.extra = {\"key1\": \"value1\", \"key2\": \"value2\"}\n        data = test_a_call._to_json()\n        newrep = TestReport._from_json(data)\n        assert newrep.extra == {\"key1\": \"value1\", \"key2\": \"value2\"}\n", "    def test_longrepr_serialization_with_exception_chain(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n            class MyExc(Exception):\n                pass\n                raise MyExc('test exception')\n            \"\"\"\n        )\n        reprec = testdir.inline_run()\n        reports = reprec.getreports(\"pytest_runtest_logreport\")\n        assert len(reports) == 3\n        rep = reports[1]\n        data = rep._to_json()\n        assert isinstance(data[\"longrepr\"], dict)\n        # Check that the ExceptionChainRepr is serialized correctly\n        assert isinstance(data[\"longrepr\"][\"chain\"], list)\n        newrep = TestReport._from_json(data)\n        assert isinstance(newrep.longrepr, ExceptionChainRepr)\n\n\n\n", "    def test_user_properties(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n                user_property = \"my property\"\n                assert False\n        \"\"\"\n        )\n        reprec = testdir.inline_run()\n        reports = reprec.getreports(\"pytest_runtest_logreport\")\n\n        assert len(reports) == 3\n        for rep in reports:\n            assert rep.user_properties == [\"my property\"]\n", "    def test_collectreport_with_kwargs(self, testdir, pytestconfig):\n        testdir.makepyfile(\n            \"\"\"\n            import os\n           \n                assert False\n            class MyPlugin:\n                    session.config.pluginmanager.register(\n                        MyPlugin2(), name=\"myplugin2\"\n                    )\n            \n            class MyPlugin2:\n                    config.pluginmanager.register(\n                        MyPlugin3(), name=\"myplugin3\"\n                    )\n            \n            class MyPlugin3:\n                    metadata[\"test_plugin3\"] = True\n            \"\"\"\n        )\n        reprec = testdir.inline_run()\n        reports = reprec.getreports(\"pytest_collectreport\")\n        assert len(reports) == 2\n        for rep in reports:\n            data = pytestconfig.hook.pytest_report_to_serializable(\n                config=pytestconfig, report=rep\n            )\n            new_rep = pytestconfig.hook.pytest_report_from_serializable(\n                config=pytestconfig, data=data\n            )\n            assert new_rep.nodeid == rep.nodeid\n            assert new_rep.when == \"collect\"\n            assert new_rep.outcome == rep.outcome\n            assert \"test_plugin3\" in new_rep.metadata\n\n\n\n", "    def test_user_properties(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n                x=1\n                test_user_property = \"my_user_property\"\n\n\n            \"\"\"\n        )\n        reprec = testdir.inline_run()\n        reports = reprec.getreports(\"pytest_runtest_logreport\")\n        assert len(reports) == 3\n        test_a_call = reports[1]\n        assert test_a_call.user_properties == [('test_user_property', 'my_user_property')]\n\n"], "sample_1134": ["def test_issue_17369():\n    a, b, c = symbols('a b c')\n    x = Function('x')\n    assert latex(Integrate(x(a) * b, (a, 0, 1)) , use_unicode=True) == r\"\\int_{0}^{1} x(a) b \\, da\"\n", "def test_issue_17428():\n    from sympy import symbols, sin, cos, pi\n    x, y = symbols('x y')\n    \n    assert latex(sin(x+y) + cos(x+y)) == r' \\sin{\\left(x + y \\right)} + \\cos{\\left(x + y \\right)}'\n    assert latex(sin(x + y) + cos(x + y + pi)) == r' \\sin{\\left(x + y \\right)} + \\cos{\\left(x + y + \\pi \\right)}'\n", "compilation error", "def test_issue_18177():\n    from sympy import Function, Basic, symbols\n    x = symbols('x')\n    f = Function('f')(x)\n    assert latex(f, use_unicode=False) == r'f\\left(x\\right)'\n", "def test_issue_17392():\n    from sympy.tensor import tensor_indices\n\n    x, y, z = symbols('x y z')\n    i, j, k = tensor_indices('i j k')\n    a = MatrixSymbol('a', 3, 3)\n    b = MatrixSymbol('b', 3, 3)\n    c = MatrixSymbol('c', 3, 3)\n\n    expr = a[i, j] * b[j, k] * c[k, i]\n    expected = r\"\\sum_{j = 1}^{3} \\left(a_{i}^{j} b^{j}_{k} c_{k}^{i}\\right)\"\n    assert latex(expr) == expected\n"], "sample_951": ["        def my_classmethod(cls):\n            pass\n", "def test_getmembers_with_slots():\n    class MyClassWithSlots:\n        __slots__ = ['attr1', 'attr2']\n\n    instance = MyClassWithSlots()\n    instance.attr1 = 1\n    instance.attr2 = 2\n\n    members = inspect.getmembers(instance)\n    assert len(members) == 2\n    assert 'attr1' in [member[0] for member in members]\n    assert 'attr2' in [member[0] for member in members]\n\n    names = [member[0] for member in inspect.getmembers(MyClassWithSlots)]\n    assert len(names) == 2\n    assert 'attr1' in names\n    assert 'attr2' in names \n\n", "compilation error", "        def static_method():\n            pass\n", "compilation error"], "sample_405": ["    def test_references_field_by_through_reversed_fields(self):\n        operation = FieldOperation(\n            \"Model\",\n            \"field\",\n            models.ManyToManyField(\n                \"Other\", through=\"Through\", related_name=\"reverse_through\"\n            ),\n        )\n        self.assertIs(\n            operation.references_field(\"Other\", \"reverse_through\", \"migrations\"), True\n        )\n        self.assertIs(\n            operation.references_field(\"Through\", \"whatever\", \"migrations\"), False\n        )\n", "    def test_references_field_by_related_name(self):\n        operation = FieldOperation(\n            \"Model\",\n            \"field\",\n            models.ForeignKey(\"Other\", models.CASCADE, related_name=\"related_to_model\"),\n        )\n        self.assertIs(\n            operation.references_field(\"Other\", \"related_to_model\", \"migrations\"),\n            True,\n        )\n        self.assertIs(\n            operation.references_field(\"Other\", \"whatever\", \"migrations\"), False\n        )\n", "    def test_references_field_by_reverse_field(self):\n        operation = FieldOperation(\n            \"Person\",\n            \"best_friend\",\n            models.ForeignKey(\"Person\", models.CASCADE, related_name=\"worsest_enemy\"),\n        )\n        self.assertIs(operation.references_field(\"Person\", \"worsest_enemy\", \"migrations\"), True)\n        self.assertIs(\n            operation.references_field(\"Person\", \"whatever\", \"migrations\"), False\n        )\n\n\n\n", "    def test_references_nested_field(self):\n        operation = FieldOperation(\n            \"Model\", \"field\", models.ForeignKey(\"Other\", models.CASCADE, to_field=\"nested.inner_field\")\n        )\n        self.assertIs(\n            operation.references_field(\"Other\", \"nested.inner_field\", \"migrations\"), True\n        )\n        self.assertIs(\n            operation.references_field(\"Other\", \"nested.non_existent\", \"migrations\"), False\n        )\n\n\n\n", "    def test_references_field_by_remote_field(self):\n        operation = FieldOperation(\n            \"Model\",\n            \"field\",\n            models.ForeignKey(\n                \"Other\",\n                models.CASCADE,\n                remote_field=models.fields.FieldOperations.ForeignRelatedFields(\n                    to_field=\"remote_field\"\n                ),\n            ),\n        )\n        self.assertIs(operation.references_field(\"Other\", \"remote_field\", \"migrations\"), True)\n        self.assertIs(\n            operation.references_field(\"Other\", \"whatever\", \"migrations\"), False\n        )\n\n"], "sample_878": ["def test_column_transformer_check_unexpected_columns(\n    transformers, remainder, unexpected_col", "def test_verbose_feature_names_out_false_no_remainder(\n    transformers, remainder, expected_names", "def test_column_transformer_set_output_after_fitting_with_remainder(remainder):\n    pd = pytest.importorskip(\"pandas\")\n    df = pd.DataFrame(\n        {\n            \"pet\": pd.Series([\"dog\", \"cat\", \"snake\"], dtype=\"category\"),\n            \"age\": [1.4, 2.1, 4.4],\n            \"height\": [20, 40, 10],\n            \"weight\": [10, 20, 15],\n        }\n    )\n    ct = ColumnTransformer(\n        [\n            (\n                \"color_encode\",\n                OneHotEncoder(sparse_output=False, dtype=\"int16\"),\n                [\"pet\"],\n            ),\n            (\"age\", StandardScaler(), [\"age\"]),\n        ],\n        remainder=remainder,\n        verbose_feature_names_out=False,\n    )\n\n    # fit without calling set_output\n    X_trans = ct.fit_transform(df)\n    assert isinstance(X_trans, np.ndarray)\n    assert X_trans.dtype == \"float64\"\n\n    ct.set_output(transform=\"pandas\")\n    X_trans_df = ct.transform(df)\n    expected_dtypes = {\n        \"pet_cat\": \"int16\",\n        \"pet_dog\": \"int16\",\n        \"pet_snake\": \"int16\",\n        \"height\": \"int64\",\n        \"age\": \"float64\",\n        \"weight\": \"float64\",\n    }\n    for col, dtype in X_trans_df.dtypes.items():\n        assert dtype == expected_dtypes[col]\n\n", "compilation error", "def test_column_transformer_with_custom_transformer(remainder):\n    class CustomTransformer(BaseEstimator, TransformerMixin):\n            self.multiplier = multiplier\n\n            return self\n\n            return X * self.multiplier\n\n    ct = ColumnTransformer(\n        [\n            (\"custom\", CustomTransformer(multiplier=3), [\"a\"]),\n            (\"scaler\", StandardScaler(), [\"b\"]),\n        ],\n        remainder=remainder,\n    )\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    X_trans = ct.fit_transform(X)\n    assert (X_trans[:, 0] == [3, 9, 15]).all()\n    assert (X_trans[:, 1] == [np.mean(X[:, 1]), np.mean(X[:, 1]), np.mean(X[:, 1])]).all()\n\n\n"], "sample_906": ["def test_domain_cpp_parse_mixed_types(app, warning):\n    text = (\".. cpp:struct:: A\\n\"\n            \".. cpp:function:: void f()\\n\"\n            \".. cpp:enum:: E\\n\"\n            \".. cpp:concept:: C\\n\"\n            \".. cpp:class:: B\\n\")\n    restructuredtext.parse(app, text)\n    ws = warning.getvalue().split(\"\\n\")\n    assert len(ws) == 0\n", "compilation error", "    def check(output):\n      assert output.count('A') == 4\n      assert output.count('void A()') == 1\n      assert output.count('A (C++ struct)') == 1", "def test_domain_cpp_parse_param_types(app):\n    text = (\".. cpp:function:: void f(int a, T b, const char *c)\\n\"\n            \" Where T is a template parameter\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index, desc, addnodes.index, desc))\n    assert_node(doctree[0], addnodes.index, entries=[('single', 'f (C++ function)', '_CPPv1fIii', '', None)])\n    assert_node(doctree[0], addnodes.index, entries=[('single', 'f (C++ function)', '_CPPv1fIii', '', None)])\n", "def test_domain_cpp_parse_multi_lines_template(app):\n    text = (\".. cpp:template<typename T>\\n\"\n            \"   class A {\\n\"\n            \"   \\n\"\n            \"   };\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index, desc, addnodes.index, desc))\n    assert_node(doctree[0], addnodes.index, entries=[('single', 'A (C++ template class)', '_CPPv4I0E8A', '', [\n        ('template', 'template<typename T>')\n    ])])\n\n\n\n"], "sample_520": ["def test_set_box_aspect_3d():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    ax.set_box_aspect([1, 2, 3])\n", "def test_text_3d_position():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    txt = ax.text(0.5, 0.5, 1, 'Hello World!', fontsize=16)\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    ax.set_zlim(0, 1)\n    fig.canvas.draw()\n\n\n\n", "def test_grid():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    ax.plot_surface(np.arange(-5, 5, 0.25), np.arange(-5, 5, 0.25),\n                    np.zeros_like(np.arange(-5, 5, 0.25)), alpha=0.2)\n    ax.grid(True)\n    \n    # force at least 1 draw!\n    fig.canvas.draw()\n\n\n", "def test_surface_lighting_with_different_lighting(fig_test, fig_ref):\n\n    fig = fig_test\n    ax = fig.add_subplot(projection='3d')\n    X, Y = np.meshgrid(np.arange(-2, 2, 0.25), np.arange(-2, 2, 0.25))\n    Z = np.sin(np.sqrt(X**2 + Y**2))\n    surf = ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap='viridis')\n    ax.view_init(elev=30, azim=45)\n    fig.canvas.draw()\n    # test with different lighting direction\n    ax.set_ Lighting(dir='x')\n    fig.canvas.draw()\n", "def test_view_init_clipping():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    ax.set_xlim3d([-10, 10])\n    ax.set_ylim3d([-10, 10])\n    ax.set_zlim3d([-10, 10])\n    ax.plot(range(-9, 10), range(-9, 10), range(-9, 10))\n    fig.canvas.draw()\n    ax.view_init(elev=30, azim=30)\n    fig.canvas.draw()\n    assert 'clipping' in ax.get_children()[0].__dict__\n\n\n\n"], "sample_1135": ["def test_issue_18590():\n    x, y = symbols('x y', real=True)\n    assert (x*y).is_extended_real is True\n    assert (x*y).is_real is True\n\n    x = symbols('x', real=True)\n    y = symbols('y', finite=False)\n    assert (x*y).is_extended_real is True\n    assert (x*y).is_real is False\n", "compilation error", "def test_issue_18507_2():\n    assert Mul(zoo, S.One) is nan\n    assert Mul(S.One, zoo) is nan\n\n\n\n", "def test_Mul_handling_of_infinity_in_powers():\n    a, b = symbols('a b', nonzero=True)\n    assert (a**oo).is_infinity is True\n    assert (b**oo).is_infinity is True\n    assert (a**(-oo)).is_negative_infinity is True\n    assert (b**(-oo)).is_negative_infinity is True\n    assert (a**oo) * (b**oo) == oo\n    assert (a**(-oo)) * (b**(-oo)) == -oo\n    assert (a**oo) / (b**oo) == (a/b)**oo\n    assert (a**(-oo)) / (b**(-oo)) == (a/b)**(-oo)\n    assert (a**oo).is_finite is False\n    assert (b**oo).is_finite is False\n    assert (a**(-oo)).is_finite is False\n    assert (b**(-oo)).is_finite is False\n\n", "def test_issue_16712():\n    from sympy import log, re\n    x = Symbol('x')\n    assert re(log(x, base=10)) != -oo\n    assert re(log(x, base=\"a\")) != -oo\n"], "sample_768": ["compilation error", "def test_train_test_split_categorical_data():\n    X = np.array([\"A\", \"B\", \"C\", \"A\", \"B\"])\n    y = np.array([0, 1, 0, 1, 0])\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    assert set(X_train) == set(X)\n    assert set(y_train) == set(y)\n    assert set(X_test) == set(X)\n    assert set(y_test) == set(y)\n", "def test_build_repr_with_none_params():\n    class MockSplitter:\n            self.a = a\n            self.b = b\n            self.c = c\n\n            return _build_repr(self)\n    assert_equal(repr(MockSplitter(5)), \"MockSplitter(a=5, b=None, c=None)\")\n\n", "compilation error", "def test_check_cv_iterable_wrapper_invalid_input():\n    # Check that check_cv wrapper handles invalid inputs\n    kf = KFold(n_splits=5)\n    \n    with pytest.raises(TypeError):\n        check_cv(\"Invalid Input\")\n    with pytest.raises(TypeError):\n        check_cv(kf, \"Invalid Input\")\n    with pytest.raises(TypeError):\n        check_cv(123)\n"], "sample_930": ["def test_create_index_skip_empty_entries(app):\n    text = (\".. index:: \\n\"\n            \".. index::  \\n\"\n            \".. index:: docutils\\n\")\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder)\n    assert len(index) == 1\n    assert index[0] == ('D', [('docutils', [[('', '#index-0')], [], None])])\n", "def test_create_index_with_subword(app):\n    text = (\".. index:: single: docutils;  python\\n\"\n            \".. index:: single: Python;  docutils\\n\")\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder)\n\n    assert len(index) == 2\n    assert index[0] == ('D', [('docutils', [[('main', '#index-0'),\n                                               ('', '#index-1')], [], None])])\n    assert index[1] == ('P', [('Python', [[('main', '#index-1'),\n                '', '#index-0')], [], None])])\n", "def test_create_index_with_sub_index(app):\n    text = (\".. index:: pair: docutils; reStructuredText\\n\"\n            \".. index:: pair: Python; interpreter\\n\"\n            \".. index:: pair: Sphinx; documentation tool\\n\"\n            \"  :subindex:\\n\"\n            \"    .. index:: see: Sphinx; Python\\n\")\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder)\n    assert len(index) == 5\n    assert index[0] == ('D', [('documentation tool', [[], [('Sphinx', [('', '#index-2')])], None])])\n    assert index[1] == ('I', [('interpreter', [[], [('Python', [('', '#index-1')])], None])])\n    assert index[2] == ('P', [('Python', [[], [('interpreter', [('', '#index-1')])], None])])\n    assert index[3] == ('R', [('reStructuredText', [[], [('docutils', [('', '#index-0')])], None])])\n    assert index[4] == ('S', [('Sphinx', [[], [('documentation tool', [('', '#index-2')]), ('Python', [('', '#index-3')])], None])])\n", "def test_create_index_with_group_entries(app):\n    text = (\".. index:: pair: docutils; reStructuredText\\n\"\n            \".. index:: pair: Python; interpreter\\n\"\n            \".. index:: pair: Sphinx; documentation tool\\n\"\n            \".. index:: pair: Sphinx; :+1:\\n\"\n            \".. index:: pair: Sphinx; \u0415\u043b\u044c\\n\"\n            \".. index:: pair: Sphinx; \u0451\u043b\u043a\u0430\\n\")\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder, group_entries=True)\n    assert len(index) == 5\n    assert index[0] == ('S', [('Sphinx', [[('', '#index-3'),\n                ('documentation tool', [('', '#index-2')]),\n                ('\u0451\u043b\u043a\u0430', [('', '#index-5')]),\n                ('\u0415\u043b\u044c', [('', '#index-4')])], None])])\n    assert index[1] == ('D', [('docutils', [[], [('reStructuredText', [('', '#index-0')])], None])])\n    assert index[2] == ('I', [('interpreter', [[], [('Python', [('', '#index-1')])], None])])\n    assert index[3] == ('P', [('Python', [[], [('interpreter', [('', '#index-1')])], None])])\n    assert index[4] == ('R', [('reStructuredText', [[], [('docutils', [('', '#index-0')])], None])])\n", "def test_create_index_with_sub_index(app):\n    text = (\".. index:: pair: foo; bar\\n\"\n            \"   :sub: index\\n\"\n            \".. index:: pair: bar; baz\\n\"\n            \"   :sub: index\\n\")\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder)\n    assert len(index) == 2\n    assert index[0] == ('B', [('bar', [[], [('baz', [('', '#index-1')])], None])])\n    assert index[1] == ('F', [('foo', [[], [('bar', [('', '#index-0')])], None])])\n"], "sample_822": ["def test_check_sparse_matrices_with_identical_rows():\n    # Ensures proper handling of sparse matrices with identical rows\n    rng = np.random.RandomState(0)\n    n_samples = 5\n    n_features = 4\n    X = rng.random_sample((n_samples, n_features))\n    X_sparse = csr_matrix(X)\n    # Create a sparse matrix with identical rows, ensuring it's not empty\n    identical_rows = np.repeat(X[0], n_samples, axis=0)\n    identical_rows_sparse = csr_matrix(identical_rows)\n    XA_checked, XB_checked = check_pairwise_arrays(X_sparse, identical_rows_sparse)\n    assert issparse(XA_checked)\n    assert issparse(XB_checked)\n\n    # Expected result: all distances should be 0\n    expected_distances = np.zeros((n_samples, n_samples))\n    assert_array_equal(XA_checked.todense(), expected_distances)\n    assert_array_equal(XB_checked.todense(), expected_distances)\n\n", "def test_pairwise_distances_with_custom_metric():\n        return np.sum(X * Y, axis=1)\n\n    X = np.array([[1, 2], [3, 4]])\n    Y = np.array([[5, 6], [7, 8]])\n\n    dist = pairwise_distances(X, Y, metric=custom_metric)\n    # Check that the custom metric produces the expected result\n    assert_array_almost_equal(dist, [[13, 30], [43, 70]])\n\n", "        def test_pairwise_distances_bad_inputs():", "def test_check_sparse_input():\n    # Ensures that checks return valid sparse matrices\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((5, 4))\n    XA_sparse = csr_matrix(X)\n    XB = rng.random_sample((5, 4))\n    XB_sparse = csr_matrix(XB)\n    XA_checked, XB_checked = check_pairwise_arrays(XA_sparse, XB_sparse)\n    assert issparse(XA_checked)\n    assert_equal(abs(XA_sparse - XA_checked).sum(), 0)\n    assert issparse(XB_checked)\n    assert_equal(abs(XB_sparse - XB_checked).sum(), 0)\n", "    def test_check_pairwise_arrays_mixed_types():\n        # Ensures that checks handle mixed type inputs correctly.\n        XA = np.array([1, 2, 3], dtype=np.float64)\n        XB = np.array([4, 5, 6], dtype=np.int32)\n        XA_checked, XB_checked = check_pairwise_arrays(XA, XB)\n        assert_equal(XA_checked.dtype, XA.dtype)\n        assert_equal(XB_checked.dtype, XB.dtype)\n"], "sample_667": ["    def test_custom_basetemp_with_empty_dir(testdir):\n        testdir.mkdir(\"custom_basetemp\")\n\n        testdir.makepyfile(\n            \"\"\"\n            import os\n                assert os.path.exists(f\"{tmpdir}tmp_path\")\n                assert os.path.exists(f\"{tmpdir}other_dir\")", "    def test_tmpdir_equals_tmp_path(tmpdir, tmp_path):\n        assert Path(tmpdir) == tmp_path\n", "    def test_basetemp_non_existing_directory(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import os\n            import pytest\n\n                assert not os.path.exists(tmp_path.joinpath(\"xyz\"))\n            \"\"\"\n        )\n        result = testdir.runpytest(\"--basetemp=/doesnotexist\")\n        assert result.ret != 0\n", "    def test_basetemp_nonexistent(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import os\n                assert tmpdir.is_dir()\n                with open(tmpdir / \"hello.txt\", \"w\") as f:\n                    f.write(\"hello\")\n\n                with open(tmpdir / \"hello.txt\", \"r\") as f:\n                    assert f.read() == \"hello\"\n\n\n            \"\"\"\n        )\n        testdir.runpytest(\"--basetemp=/nonexistent/path\")\n        # testdir.runpytest(\"--basetemp=/nonexistent/path\")\n\n", "def test_tmpdir_with_non_existent_basetemp(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n            pass\n    \"\"\"\n    )\n\n    result = testdir.runpytest(\"--basetemp=idonotexist\")\n    result.stdout.fnmatch_lines(\"*new tempdir created*\")\n"], "sample_364": ["    def test_converter_with_multiple_conversions(self):\n        @DynamicConverter.register_to_python\n            return (value.lower(), \"RGB\")\n\n        @DynamicConverter.register_to_url\n            return value[0].upper()\n\n        with self.subTest(url=\"/color/RED\"):\n            match = self.client.get('/color/RED/')\n            self.assertEqual(match.status_code, 200)\n            self.assertEqual(match.kwargs, {'color': ('red', 'RGB')})\n        with self.subTest(reverse_url):\n            url = reverse('color_converter', kwargs={'color': ('blue', 'RGB')})\n            self.assertEqual(url, '/color/BLUE')\n\n\n\n\n", "    def test_invalid_converter_in_include(self):\n        with self.assertRaises(ImproperlyConfigured):\n            include((lambda: None, 'app_name', 'converter_name'), converter=DynamicConverter.invalid_converter)\n", "    def test_converter_with_invalid_converter_raises_error(self):\n        msg = (\n            \"URL route 'test/<int:invalid_converter>/' uses invalid \"\n            \"converter 'invalid_converter'.\"\n        )\n        with self.assertRaisesMessage(ImproperlyConfigured, msg):\n            path('test/<int:invalid_converter>/', empty_view)\n", "    def test_reverse_with_kwargs_and_converter(self):\n        @DynamicConverter.register_to_url\n            return f'converted-{value}'\n        with self.subTest(value='test'):\n            url = reverse('dynamic', kwargs={'value': 'test'})\n            self.assertEqual(url, '/dynamic/converted-test/')\n", "    def test_invalid_converter_registration(self):\n        with self.assertRaises(TypeError):\n            DynamicConverter.register_to_url(lambda x: x)\n        with self.assertRaises(TypeError):\n            DynamicConverter.register_to_python(lambda x: x)\n"], "sample_732": ["compilation error", "def test_download_if_missing():\n    try:\n        # Test that the data is downloaded if it doesn't exist\n        data = fetch_kddcup99(download_if_missing=True)\n    except IOError:\n        raise SkipTest(\"kddcup99 dataset can not be loaded.\")\n\n    # Check that the data was downloaded\n    assert_equal(data.data.shape, (494021, 41))\n    assert_equal(data.target.shape, (494021,))\n\n    # Delete the downloaded data\n    import os\n    from sklearn.datasets.tests.test_common import _cleanup\n\n    _cleanup()\n\n\n", "compilation error", "compilation error", "def test_percent10_with_return_X_y():\n    try:\n        data, target = fetch_kddcup99(download_if_missing=False,\n                                    return_X_y=True)\n    except IOError:\n        raise SkipTest(\"kddcup99 dataset can not be loaded.\")\n\n    assert_equal(data.shape, (494021, 41))\n    assert_equal(target.shape, (494021,)) \n"], "sample_1162": ["def test_nfloat_dict():\n    from sympy.core.symbol import Symbol\n    x = Symbol('x')\n    expr = {x: x**2, 'y': 2}\n    nfloat_expr = nfloat(expr, dkeys=True)\n    assert isinstance(nfloat_expr, dict)\n    assert nfloat_expr == {x: x**2, 'y': 2}\n    nfloat_expr = nfloat(expr, dkeys=False)\n    assert isinstance(nfloat_expr, dict)\n    assert nfloat_expr == {x: x**2.0, 'y': 2.0}\n\n\n", "def test_nfloat_containers():\n    from sympy import Tuple, Dict\n    assert nfloat((1, 2, 3)) == Tuple(1.0, 2.0, 3.0)\n    assert nfloat((1.*2, 3.0)) == Tuple(2.0, 3.0)\n    assert nfloat({1: 2, 3: 4}) == {1.0: 4.0, 3.0: 4.0}\n    assert nfloat({1: 2 * 3, 3: 4.0}) == {1.0: 6.0, 3.0: 4.0}\n\n\n", "def test_nfloat_symbols():\n    from sympy import Symbol\n    x = Symbol('x')\n    y = Symbol('y')\n    assert nfloat(x) == x\n    assert nfloat(y) == y\n\n\n\n", "def test_MatrixBase_kind():\n    from sympy.matrices import MatrixBase\n    class MyMatrixBase(MatrixBase):\n        pass\n    m = MyMatrixBase.zeros(2,2)\n    assert m.kind is MatrixKind(NumberKind)\n", "def test_nfloat_matrix():\n    from sympy.matrices import Matrix, eye\n    m = Matrix([[1, 2], [3, 4]])\n    m_nfloat = nfloat(m)\n    assert isinstance(m_nfloat, Matrix)\n    assert all(isinstance(e, float) for row in m_nfloat for e in row)\n"], "sample_39": ["compilation error", "compilation error", "def test_wcs_to_string():\n    w = wcs.WCS(naxis=2)\n    w.wcs.ctype = [\"RA---TAN\", \"DEC--TAN\"]\n    w.wcs.cdelt = [0.1, 0.1]\n    w.wcs.crval = [-90, 90]\n    w.wcs.crpix = [1, 1]\n    string = w.to_string()\n    assert \"CTYPE1 = 'RA---TAN'\" in string\n    assert \"CTYPE2 = 'DEC--TAN'\" in string\n    assert \"CDELT1 = 0.1\" in string\n    assert \"CDELT2 = 0.1\" in string\n    assert \"CRVAL1 = -90.0\" in string\n    assert \"CRVAL2 = 90.0\" in string\n    assert \"CRPIX1 = 1.0\" in string\n    assert \"CRPIX2 = 1.0\" in string\n\n\n\n", "compilation error", "def test_wcs_to_header_to_wcs():\n    \"\"\"\n    Test that WCS object roundtrip with to_header() works correctly\n    \"\"\"\n    hdu = fits.open(get_pkg_data_filename('data/sip.fits'))\n    w = wcs.WCS(hdu[0].header)\n    h = w.to_header()\n    w2 = wcs.WCS(h)\n    assert w.wcs.compare(w2.wcs)\n\n\n            "], "sample_795": ["def test_check_get_params_invariance_in_sparse_transformers():\n    class SparseTransformerInvariance(SparseTransformer):\n            super().__init__()\n            self.X_shape_ = X_shape_\n            return {'X_shape_': self.X_shape_}\n\n    check_get_params_invariance(SparseTransformerInvariance())\n", "def test_check_get_params_invariance():\n    class InvariantEstimator(BaseEstimator):\n            self.a = a\n\n            return self\n\n            return {\"a\": self.a}\n\n    set_checking_parameters(InvariantEstimator())\n    assert_raises(AssertionError, check_get_params_invariance, InvariantEstimator)\n\n\n", "def test_check_get_params_invariance():\n    # check that get_params(deep=False) is a subset of get_params(deep=True)\n    for estimator_name in [RandomForestClassifier, AdaBoostClassifier,\n                           LinearRegression, SGDClassifier, GaussianMixture]:\n        estimator = clone(getattr(estimator_name, estimator_name)())\n        check_get_params_invariance('estimator_name', estimator)\n", "    def test_check_estimator_multioutput(self):\n        from sklearn.multiclass import OneVsRestClassifier\n        from sklearn.linear_model import LogisticRegression\n\n        # Test multi-output estimators\n        o_vs_r = OneVsRestClassifier(LogisticRegression())\n        check_estimator(o_vs_r)\n\n\n", "def test_check_get_params_invariance():\n    class InconsistentGetParams(BaseEstimator):\n            self.param = 1\n            return {'param': 2}\n    assert_raises_regex(AssertionError,\n                        \"Estimator InconsistentGetParams doesn't have invariant\"\n                        \" get_params\",\n                        check_get_params_invariance,\n                        InconsistentGetParams())\n\n\n"], "sample_1070": ["def test_log_finite():\n    assert log(oo).is_finite is False\n    assert log(-oo).is_finite is False\n    assert log(0).is_finite is False\n\n    assert log(Float(\"1e-300\")).is_finite\n    assert log(Float(\"1e300\")).is_finite\n\n\n", "def test_log_rewrite():\n    x = symbols('x')\n    assert log(x).rewrite(exp) == log(x)\n    assert log(exp(x)).rewrite(x) == x\n    assert log(x**2).rewrite(log) == 2*log(x)\n    assert log(x*y).rewrite(add) == log(x) + log(y)\n    assert log(x/y).rewrite(subtract) == log(x) - log(y)\n    assert log(sqrt(x)).rewrite(pow) == log(x**(1/2))\n    assert log(sqrt(x)).rewrite(log) == (1/2)*log(x)\n    assert log(1/x).rewrite(subtract) == -log(x)\n\n\n\n", "def test_issue_9968():\n    x, y = symbols('x y', real=True, nonzero=True)\n    assert log(x/y) == log(x) - log(y)\n    assert log(x/y, evaluate=False) == log(x) - log(y)\n\n\n\n\n", "def test_log_complex():\n    z = symbols('z', complex=True)\n    assert log(1 + I).expand(complex=True) == 0.5*log(2) + 0.5*pi*I\n    assert log(-1).expand(complex=True) == pi*I\n\n\n", "def test_issue_10451():\n    x = Symbol('x', complex=True)\n    assert log(x).is_real is None\n    assert log(x).is_imaginary is None\n"], "sample_346": ["    def test_cache_control_decorator(self):\n        @cache_control(max_age=3600, no_cache=True, must_revalidate=True,\n                       no_store=True)\n            return HttpResponse()\n        r = a_view(HttpRequest())\n        self.assertEqual(\n            set(r.headers['Cache-Control'].split(', ')),\n            {'max-age=3600', 'no-cache', 'must-revalidate', 'no-store'},\n        )\n\n", "    def test_cache_page_decorator_with_kwargs(self):\n        @cache_page(60 * 5, key_prefix=\"test_\")\n            return HttpResponse()\n        r = a_view(HttpRequest())\n        self.assertEqual(r.headers['Cache-Control'], 'max-age=300, s-maxage=300')\n        self.assertEqual(r.cache_control, 'max-age=300, s-maxage=300')\n        self.assertIn('test_', r.response_cache_key)\n", "    def test_cache_control_defaults(self):\n        @cache_control\n            return HttpResponse()\n        r = a_view(HttpRequest())\n        self.assertEqual(\n            set(r.headers['Cache-Control'].split(', ')),\n            {'private'},\n        )\n\n\n    ", "    def test_cache_control_decorator(self):\n        @cache_control(max_age=60, must_revalidate=True, private=True)\n            return HttpResponse()\n        r = a_view(HttpRequest())\n        self.assertEqual(\n            set(r.headers['Cache-Control'].split(', ')),\n            {'max-age=60', 'must-revalidate', 'private'}\n        )\n\n", "    def test_cache_control_decorator(self):\n        @cache_control(no_cache=True, max_age=3600)\n            return HttpResponse()\n\n        r = a_view(HttpRequest())\n        self.assertEqual(\n            set(r.headers['Cache-Control'].split(', ')),\n            {'no-cache', 'max-age=3600'},\n        )\n"], "sample_972": ["def test_stringify_type_hints_TypedDict():\n    from typing import TypedDict  # type: ignore\n    class MyTypedDict(TypedDict):\n        name: str\n        age: int\n\n    assert stringify(MyTypedDict) == \"MyTypedDict\"\n    assert stringify(MyTypedDict, \"smart\") == \"~typing.TypedDict['MyTypedDict']\"\n", "def test_stringify_type_hints_typing_ForwardRef():\n    from typing import ForwardRef  # type: ignore\n    assert stringify(ForwardRef(\"myint\")) == \"myint\"\n    assert stringify(ForwardRef(\"myint\"), \"smart\") == \"myint\"\n", "def test_stringify_callable_params(input_type, expected_output):\n    assert stringify(input_type) == expected_output\n    assert stringify(input_type, \"fully-qualified\") == f\"typing.Callable[{expected_output}]\"\n    assert stringify(input_type, \"smart\") == f\"~typing.Callable[{expected_output}]\"\n", "def test_stringify_type_hints_Annotated_with_generic():\n    from typing import Annotated, Generic  # type: ignore\n\n    class MyGeneric(Generic[T]):\n        pass\n\n    assert stringify(Annotated[MyGeneric[int], \"foo\"]) == \"MyGeneric[int]\"\n    assert stringify(Annotated[MyGeneric[int], \"foo\", \"bar\"]) == \"MyGeneric[int]\"\n    assert stringify(Annotated[MyGeneric[int], \"foo\"], \"smart\") == \"~MyGeneric[int]\"\n    assert stringify(Annotated[MyGeneric[int], \"foo\", \"bar\"], \"smart\") == \"~MyGeneric[int]\"\n", "compilation error"], "sample_941": ["def test_stringify_type_hints_generics():\n    from typing import Generic, TypeVar\n\n    T = TypeVar('T')\n\n    class MyClass(Generic[T]):\n            self.value = value\n\n    assert stringify(MyClass) == \"tests.test_util_typing.MyClass[T]\"\n    assert stringify(MyClass[str]) == \"tests.test_util_typing.MyClass[str]\"\n", "def test_stringify_type_hints_Union_with_ellipsis():\n    assert stringify(Union[str, ...]) == \"Union[str, ...]\"\n    assert stringify(Union[List[int], ...]) == \"Union[List[int], ...]\"\n", "    def test_stringify_Annotated(annotation):\n        assert stringify(annotation) == \"int\"\n", "compilation error", "def test_stringify_type_hints_GenericMeta():\n    from typing import GenericMeta, TypeVar\n\n    T = TypeVar('T')\n    class MyGenericMeta(GenericMeta):\n            attrs['__origin__'] = str\n            return super().__new__(mcs, name, bases, attrs)\n\n    MyGeneric = type('MyGeneric', (MyGenericMeta,), {})\n    assert stringify(MyGeneric[T]) == \"MyGeneric[T]\"\n"], "sample_680": ["    def test_skipif_with_importorskip(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n            from my_package import some_module\n\n            @pytest.mark.skipif(\"some_module is None\", reason=\"Package not installed\")\n                assert some_module.some_function()\n            \"\"\"\n        )\n        testdir.makepyfile(\n            \"my_package/__init__.py\", content=\"\"\"\n            pass\n            \"\"\"\n        )\n        result = testdir.runpytest()\n        result.stdout.fnmatch_lines(\n            [\n                \"*SKIP*Package not installed\",\n            ]\n        )\n", "compilation error", "compilation error", "def test_skip_with_custom_marker(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.skip_complex\n            pass\n\n        @pytest.mark.skip_if(condition=True)\n            pass\n\n            pass\n        \"\"\"\n    )\n    result = testdir.runpytest(\"-rs\")\n    result.stdout.fnmatch_lines([\"*1 skipped*\"])\n\n", "def test_xfail_when_raising_exception(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail\n            raise ValueError\n    \"\"\"\n    )\n    result = testdir.runpytest(\"-rv\")\n    result.stdout.fnmatch_lines([\"*1 xfailed*\"])\n    # The exception should be printed\n\n\n"], "sample_329": ["    def test_serialize_foreignkey(self):\n        class Author(models.Model):\n            name = models.CharField(max_length=100)\n\n        class Book(models.Model):\n            title = models.CharField(max_length=100)\n            author = models.ForeignKey(Author, on_delete=models.CASCADE)\n\n        migration = type(\"Migration\", (migrations.Migration,), {\n            \"operations\": [\n                migrations.CreateModel(name=\"Book\", fields=[\n                    ('title', models.CharField(max_length=100)),\n                    ('author', models.ForeignKey(on_delete=models.CASCADE, to=Author, related_name='books')),\n                ], options={'verbose_name': 'Book', 'verbose_name_plural': 'Books'},),\n            ],\n            \"dependencies\": [],\n        })\n        writer = MigrationWriter(migration)\n        output = writer.as_string()\n        self.assertIn(\"models.ForeignKey(on_delete=models.CASCADE, to=Author, related_name='books')\", output)\n", "    def test_serialize_nested_model_dependencies(self):\n        class NestedModel(models.Model):\n            pass\n\n        class ParentModel(models.Model):\n            nested = models.ForeignKey(NestedModel, on_delete=models.CASCADE)\n\n        migration = type(\"Migration\", (migrations.Migration,), {\n            \"operations\": [\n                migrations.CreateModel(\"ParentModel\", (), {}, (ParentModel,)),\n                migrations.CreateModel(\"NestedModel\", (), {}, (NestedModel,)),\n            ],\n            \"dependencies\": []\n        })\n        writer = MigrationWriter(migration)\n        output = writer.as_string()\n        self.assertIn(\"operations: [\\n    migrations.CreateModel('ParentModel', [], {}, (ParentModel,)),\\n    migrations.CreateModel('NestedModel', [], {}, (NestedModel,)),\", output)\n        self.assertIn(\"dependencies: []\", output)\n", "    def test_serialize_custom_field(self):\n        class MyCustomField(models.Field):\n                super().__init__(*args, **kwargs)\n\n                return \"MyCustomFieldType\"\n\n                return value\n\n                return value\n\n        class MyModel(models.Model):\n            my_field = MyCustomField()\n\n        with self.subTest(\"serialize\"):\n            writer = MigrationWriter(None)\n            serialized = writer.serialize(MyCustomField())\n            self.assertEqual(serialized, \"migrations.test_writer.MyCustomField\")\n\n        with self.subTest(\"deserialize\"):\n            deserialized_field = writer.deserialize(\n                \"migrations.test_writer.MyCustomField\"\n            )\n            self.assertIsInstance(deserialized_field, MyCustomField)\n\n\n\n", "    def test_preserve_custom_field_types(self):\n        class MyCustomField(models.Field):\n                super().__init__(*args, **kwargs)\n\n        class MyModel(models.Model):\n            field = MyCustomField()\n\n        migration = type(\"Migration\", (migrations.Migration,), {\n            \"operations\": [\n                migrations.CreateModel(\"MyModel\", ((\"field\", MyCustomField()),), (), (models.Model,)),\n            ]\n        })\n        writer = MigrationWriter(migration)\n        output = writer.as_string()\n        self.assertIn(\n            \"class MyCustomField(models.Field):\", output\n        )\n", "    def test_serialize_lazy_objects(self):\n        class LazyObject:\n                self.value = value\n\n                return f'LazyObject({self.value})'\n\n        serialized_object = self.serialize_round_trip(LazyObject(42))\n        self.assertEqual(serialized_object.value, 42)\n"], "sample_442": ["    def test_cookie_signer(self):\n        signer = signing.get_cookie_signer()\n        signed = signer.sign(\"abc\")\n        self.assertEqual(signer.unsign(signed), \"abc\")\n", "    def test_compress(self):\n        signer = signing.Signer(key=\"predictable-secret\")\n        long_string = \"{\" * 1000\n        compressed = signing.dumps(long_string, compress=True)\n        self.assertNotEqual(len(compressed), len(signing.dumps(long_string)))\n        self.assertEqual(signing.loads(compressed), long_string)\n\n", "    def test_compress_not_useful_for_short_strings(self):\n        signer = signing.Signer(key=\"predictable-secret\")\n        short_string = \"hello\"\n        with self.subTest(short_string):\n            compressed_signed = signing.dumps(short_string, compress=True)\n            self.assertGreater(len(compressed_signed), len(signing.dumps(short_string)))\n            self.assertEqual(\n                signer.unsign_object(compressed_signed),\n                signer.unsign_object(signing.dumps(short_string)),\n            )\n\n            \n        long_string = \"a\" * 2000\n        with self.subTest(long_string):\n            compressed_signed = signing.dumps(long_string, compress=True)\n            self.assertLess(len(compressed_signed), len(signing.dumps(long_string)))\n            self.assertEqual(\n                signer.unsign_object(compressed_signed),\n                signer.unsign_object(signing.dumps(long_string)),\n            )\n\n\n\n", "    def test_compression(self):\n        signer = signing.Signer(key=\"predictable-secret\")\n        data = b'x' * 1024\n        compressed = zlib.compress(data)\n        uncompressed = zlib.decompress(compressed)\n        \n        compressed_signed = signer.sign_object(data, compress=True)\n        self.assertTrue(compressed_signed.startswith(\".\"))\n        \n        decompressed_signed = signer.unsign_object(compressed_signed)\n        self.assertEqual(data, uncompressed)\n        self.assertEqual(data, zlib.decompress(decompressed_signed))\n", "    def test_compress(self):\n        compressed_data = zlib.compress(b\"test compressed data\")\n        signer = signing.Signer(key=\"predictable-secret\")\n        signed_compressed = signer.sign_object(\n            \"test compressed data\", compress=True\n        )\n        self.assertEqual(\n            signer.unsign_object(signed_compressed), \"test compressed data\"\n        )\n        signed_uncompressed = signer.sign_object(\"test uncompressed data\")\n        self.assertNotEqual(signed_compressed, signed_uncompressed)\n        self.assertEqual(\n            signer.unsign_object(signed_uncompressed), \"test uncompressed data\"\n        )\n"], "sample_291": ["    def test_object_not_found(self):\n        test_view = views.CustomSingleObjectMixinView()\n        with self.assertRaises(Http404):\n            test_view.get(self.rf.get('/object/1'))\n", "    def test_get_context_data_with_object(self):\n        test_view = views.CustomSingleObjectView()\n        object = {'name': 'Alice'}\n        test_view.object = object\n        context = test_view.get_context_data()\n        self.assertEqual(context['object'], object)\n\n\n\n", "    def test_template_with_object(self):\n        response = self.client.get('/template/object/')\n        self.assertEqual(response.status_code, 200)\n        self.assertContains(response, 'This is an object')\n", "    def test_reverse_redirect_view(self):\n        response = self.client.get('/redirect-test/')\n        self.assertRedirects(response, '/bar/', status_code=302, target_status_code=200)\n", "    def test_single_object_template_response_mixin(self):\n        view = views.SingleObjectTemplateResponseMixinView()\n        request = self.rf.get('/')\n        response = view.get(request)\n        self.assertEqual(response.status_code, 200)\n        self.assertIn('object', response.context)\n        self.assertEqual(response.context['object'].name, 'testing')\n"], "sample_998": ["def test_braket_printing():\n    from sympy.physics.quantum import Bra, Ket\n\n    a = symbols(\"a\")\n    assert latex(Bra(a) * Ket(a)) == r\"\\left\\langle a \\right| \\left| a \\right\\rangle\"\n    assert latex(Bra(a) * Ket(b)) == r\"\\left\\langle a \\right| \\left| b \\right\\rangle\"\n", "def test_Issue_15026():\n    from sympy.tensor.array import TensorArray\n    A = TensorArray('A', shape=(2,2))\n    assert latex(A) == r\"A\"\n    A[0, 0] = x\n    A[0, 1] = y\n    A[1, 0] = z\n    A[1, 1] = w\n    assert latex(A) == r\"\\begin{bmatrix}x & y \\\\ z & w\\end{bmatrix}\"\n", "def test_issue_9737():\n    from sympy.combinatorics import Permutation\n    p = Permutation([1, 2, 3, 4])\n    assert latex(p) == r\"\\left(1\\ 2\\ 3\\ 4\\right)\"\n\n\n", "def test_OuterProduct_printing():\n    from sympy.diffgeom.rn import R2\n    from sympy.diffgeom import OuterProduct\n    v = R2.x\n    w = R2.y\n    op = OuterProduct(v, w)\n    assert latex(op) == r\"\\begin{pmatrix} x & 0 \\\\ 0 & 0 \\end{pmatrix}\"\n", "def test_cross_product_printing():\n    from sympy.vector import Vector\n    A = Vector('A')\n    B = Vector('B')\n    assert latex(A^B) == r'\\mathbf{A} \\times \\mathbf{B}'\n"], "sample_136": ["    def test_non_header_keys_are_ignored(self):\n        environ = {\n            'PATH_INFO': '/somepath/',\n            'REQUEST_METHOD': 'get',\n            'wsgi.input': BytesIO(b''),\n            'SERVER_NAME': 'internal.com',\n            'SERVER_PORT': 80,\n            'CONTENT_TYPE': 'text/html',\n            'CONTENT_LENGTH': '100',\n            'HTTP_HOST': 'example.com',\n        }\n        headers = HttpHeaders(environ)\n        self.assertNotIn('PATH_INFO', headers)\n        self.assertNotIn('REQUEST_METHOD', headers)\n        self.assertNotIn('wsgi.input', headers)\n        self.assertNotIn('SERVER_NAME', headers)\n        self.assertNotIn('SERVER_PORT', headers)\n", "    def test_parse_header_name_invalid(self):\n        tests = (\n            ('InvalidHeader', None),\n            ('_InvalidHeader', None),\n            ('HTTP_InvalidHeader', None),\n        )\n        for header in tests:\n            with self.subTest(header=header):\n                self.assertEqual(HttpHeaders.parse_header_name(header), None)\n", "    def test_wsgi_request_headers_with_encoding(self):\n        environ = {\n            'CONTENT_TYPE': 'text/html; charset=utf-8',\n            'CONTENT_LENGTH': '100',\n            'HTTP_HOST': 'example.com',\n            'wsgi.input': BytesIO(b''),\n            'HTTP_ACCEPT_LANGUAGE': 'en-us,en;q=0.5',\n        }\n        request = WSGIRequest(environ)\n        self.assertEqual(request.headers, {\n            'Content-Type': 'text/html; charset=utf-8',\n            'Content-Length': '100',\n            'Host': 'example.com',\n            'Accept-Language': 'en-us,en;q=0.5',\n        })\n\n", "    def test_header_case_is_preserved(self):\n        environ = {\n            'CONTENT_TYPE': 'text/html',\n            'CONTENT-LENGTH': '100',\n            'HTTP_Host': 'example.com',\n        }\n        headers = HttpHeaders(environ)\n        self.assertEqual(headers['Content-Type'], 'text/html')\n        self.assertEqual(headers['Content-Length'], '100')\n        self.assertEqual(headers['Host'], 'example.com')\n\n", "    def test_get_host(self):\n        environ = {\n            'HTTP_HOST': 'example.com',\n            'SERVER_NAME': 'www.example.com',\n            'SERVER_PORT': 80,\n        }\n        headers = HttpHeaders(environ)\n        self.assertEqual(headers.get_host(), 'example.com')\n\n        environ['HTTP_HOST'] = 'example.com:8080'\n        headers = HttpHeaders(environ)\n        self.assertEqual(headers.get_host(), 'example.com:8080')\n\n        environ = {\n            'SERVER_NAME': 'www.example.com',\n            'SERVER_PORT': 443,\n            'HTTP_HOST': None,\n        }\n        headers = HttpHeaders(environ)\n        self.assertEqual(headers.get_host(), 'www.example.com:443')\n\n"], "sample_1089": ["compilation error", "compilation error", "def test_issue_18977():\n    from sympy.tensor import TensorIndex, dummy_index\n    a, b, c = TensorIndex('a b c')\n    i = dummy_index('i')\n    expr = a[i, j] + b[i, j]\n    assert expr.subs({i: a}) == a[a, j] + b[a, j]\n\n\n", "compilation error", "compilation error"], "sample_683": ["compilation error", "compilation error", "def test_logging_warning_during_test_teardown(capsys):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n            logging.warning(\"during test\")\n            # no assertions\n            logging.warning(\"during teardown\")\n        \"\"\"\n    )\n    result = testdir.runpytest_subprocess()\n    assert result.ret == 0\n    out, err = capsys.readouterr()\n    assert \"during test\" in out\n    assert \"during teardown\" in out\n", "def test_dontreadfrominput_in_loop(testdir):\n    p = testdir.makepyfile(\n        \"\"\"\n        import time\n            for i in range(10):\n                print(i)\n                time.sleep(0.1)\n    \"\"\"\n    )\n    with captured_output() as capsys:\n        result = testdir.runpytest()\n    assert capsys.stdout.str() == \"\\n\".join([str(i) for i in range(10)])\n\n\n", "def test_capture_in_nested_thread():\n    import threading\n    import sys\n    from _pytest import capture\n\n    class CaptureThread(threading.Thread):\n            super().__init__()\n            self.capture = capture\n            self.msg = msg\n\n            print(self.msg, file=sys.stderr)\n            self.capture.stop_capturing()\n            print(\"Stop captured\", file=sys.stderr)\n\n    testdir.makepyfile(\n        \"\"\"\n        import sys\n        \n            cap = capture.StdCapture()\n            t = CaptureThread(cap, \"hello from thread\")\n            t.start()\n            t.join()\n            out, err = cap.readouterr()\n            assert \"hello from thread\" in err\n            assert \"Stop captured\" in err\n    \"\"\"\n    )\n    result = testdir.runpytest_subprocess()\n    assert result.ret == 0\n\n"], "sample_524": ["def test_colorbar_fractional_position():\n    fig, ax = plt.subplots()\n    pc = ax.pcolormesh(np.random.randn(10, 10))\n    cb = fig.colorbar(pc, ax=ax, shrink=0.5, pad=0.05, fraction=0.05)\n\n    ax.set_xlabel(\"X\")\n    ax.set_ylabel(\"Y\")\n\n    # Check that the colorbar is positioned correctly\n    bbox = cb.ax.get_position()\n    assert bbox.x0 >= 0\n    assert bbox.x1 <= 1\n    assert bbox.y0 >= 0\n    assert bbox.y1 <= 1\n\n    # Check that shrink and pad are applied correctly\n    expected_width = 0.05 * fig.get_figwidth()\n    expected_height = 0.05 * fig.get_figheight()\n    assert abs(cb.ax.get_window_extent().width - expected_width) < 1e-3\n    assert abs(cb.ax.get_window_extent().height - expected_height) < 1e-3\n", "def test_colorbar_shared_axes():\n    fig, ax1 = plt.subplots()\n    ax2 = ax1.twinx()\n    pc1 = ax1.pcolormesh(np.random.randn(10, 10))\n    pc2 = ax2.pcolormesh(np.random.randn(10, 10), cmap='coolwarm')\n\n    cb = fig.colorbar(pc1, ax=[ax1, ax2], orientation='vertical')\n    assert len(cb.ax.get_children()) == 3  # Title, ticks, and scalebar\n    assert isinstance(cb.ax, Axes)\n\n    cb = fig.colorbar(pc2, ax=[ax1, ax2], orientation='vertical')\n    assert len(cb.ax.get_children()) == 3  # Title, ticks, and scalebar\n    assert isinstance(cb.ax, Axes)\n", "compilation error", "compilation error", "def test_colorbar_with_nan():\n    fig, ax = plt.subplots()\n    data = np.arange(100).reshape(10, 10)\n    data[0, :5] = np.nan\n    im = ax.imshow(data)\n    cb = fig.colorbar(im, ax=ax)\n    assert np.isnan(cb.get_ticks()).any()\n    assert np.isnan(cb.get_ticklabels()).any()\n\n\n\n"], "sample_1120": ["compilation error", "def test_matrix_power_commutativity():\n    A = MatrixSymbol('A', 3, 3)\n    B = MatrixSymbol('B', 3, 3)\n\n    assert (A*B)**2 == A**2*B**2\n    assert (A*B)**3 == A**3*B**3\n    assert (A*B)**-1 == A**-1*B**-1\n\n\n", "def test_matrix_transpose():\n    A = MatrixSymbol('A', 2, 3)\n    B = A.T\n    assert B.shape == (3, 2)\n    assert B[i, j] == A[j, i]\n    assert (A.T).T == A\n    assert (A + B).T == A.T + B.T\n    assert (A * B).T == B.T * A.T\n", "def test_matrix_constructor_with_expression_symbols():\n    x, y = symbols('x y')\n    A = Matrix([[x, y], [2*x, 3*y]])\n    assert isinstance(A[0, 0], MatrixElement)\n    assert A[0, 0].doit() == x\n    assert A[1, 1].doit() == 3*y\n    assert A.subs(x, 1).subs(y, 2) == Matrix([[1, 2], [2, 6]])\n", "compilation error"], "sample_1185": ["def test_decompogen_nested_functions():\n    assert decompogen(sin(cos(sqrt(x))), x) == [sin(x), cos(x), sqrt(x)]\n    assert decompogen(sqrt(x**2 + 1), x) == [sqrt(x), x**2 + 1]\n    assert decompogen(exp(x**2 + cos(x)), x) == [exp(x), x**2 + cos(x)]\n    assert decompogen(Max(sin(x), cos(sqrt(y))), x) == [Max(sin(x), cos(sqrt(y))), x]\n", "def test_decompogen_nested_functions():\n    assert decompogen(sin(cos(sqrt(x))), x) == [sin(x), cos(x), sqrt(x)]\n    assert decompogen(exp(sin(cos(x**2))), x) == [exp(x), sin(x), cos(x), x**2]\n    assert decompogen(sqrt(sin(x)**2 + 1), x) == [sqrt(x), sin(x)]\n", "compilation error", "def test_compogen_with_max():\n    assert compogen([Max(x, x**2), x], x) == Max(x, x**2)\n    assert compogen([Max(x, sin(x)), x], x) == Max(x, sin(x))\n    assert compogen([x**2 + x + 1, Max(sin(x), cos(x))], x) == (sin(x)**2 + sin(x) + 1) if (sin(x)**2 + sin(x) + 1) <= (cos(x)**2 + cos(x) + 1) else (cos(x)**2 + cos(x) + 1)\n", "def test_decompogen_nested_functions():\n    assert decompogen(sin(cos(x**2)), x) == [sin(x), cos(x), x**2]\n    assert decompogen(sqrt(cos(sin(x))) , x) == [sqrt(x), cos(x), sin(x)]\n    assert decompogen(exp(sin(sqrt(x))), x) == [exp(x), sin(x), sqrt(x)]\n"], "sample_406": ["    def test_refresh_from_db_with_force_update(self):\n        a = Article.objects.create(headline=\"Old Headline\", pub_date=datetime.now())\n        with self.assertNumQueries(1):\n            a.refresh_from_db(force_update=True)\n            self.assertEqual(a.headline, \"Old Headline\")\n\n        # Now simulate a change that would normally not affect the\n        # cached state of the object but should if force_update=True\n        a.headline = \"Old Headline\"  # No change, but force_update should query the db\n        a.save()\n        with self.assertNumQueries(1):\n            a.refresh_from_db(force_update=True)\n            self.assertEqual(a.headline, \"Old Headline\")\n\n        a.headline = \"New Headline\"\n        a.save()\n        with self.assertNumQueries(1):\n            a.refresh_from_db(force_update=True)\n            self.assertEqual(a.headline, \"New Headline\")\n", "    def test_refresh_with_select_related(self):\n        article = Article.objects.create(\n            headline=\"Parrot programs in Python\",\n            pub_date=datetime(2005, 7, 28),\n        )\n        self.assertFalse(hasattr(article, \"featured\"))\n        featured = FeaturedArticle.objects.create(article_id=article.pk)\n        with self.assertNumQueries(1):\n            article.refresh_from_db(select_related=\"featured\")\n        self.assertTrue(hasattr(article, \"featured\"))\n        self.assertIs(article.featured, featured)\n\n\n", "    def test_refresh_from_db_with_nonexistent_pk(self):\n        with self.assertRaisesMessage(\n            DoesNotExist, \"Article matching query does not exist.\"\n        ):\n            a = Article.objects.get(pk=9999)\n            a.refresh_from_db()\n        with self.assertRaisesMessage(\n            DoesNotExist, \"Article matching query does not exist.\"\n        ):\n            a = Article.objects.get(pk=9999)\n            a.refresh_from_db(fields=[\"headline\"])\n", "    def test_refresh_related_manager(self):\n        a = Article.objects.create(pub_date=datetime.now())\n        s1 = SelfRef.objects.create(article=a)\n        s2 = SelfRef.objects.create(article=a)\n        s1.refresh_from_db()\n\n        # Check related manager is refreshed\n        self.assertEqual(s1.article, a)\n        a.delete()\n        s1.refresh_from_db()\n        self.assertIsNone(s1.article)\n\n        # Refreshing related manager should not trigger extra queries\n        with self.assertNumQueries(1):  \n            s1.article_set.all() \n", "    def test_refresh_from_db_with_related_objects(self):\n        parent = Article.objects.create(headline=\"Parent article\")\n        child = ChildArticle.objects.create(parent=parent, headline=\"Child article\")\n        with self.assertNumQueries(1):\n            parent.refresh_from_db(fields=[\"headline\", \"childarticle_set\"])\n        self.assertEqual(parent.headline, \"Parent article\")\n        self.assertEqual(len(parent.childarticle_set.all()), 1)\n        self.assertEqual(parent.childarticle_set.all()[0].headline, \"Child article\")\n\n"], "sample_460": ["    def test_non_admin_url_404_if_authenticated_without_final_catch_all_view(self):\n        superuser = User.objects.create_superuser(\n            username=\"super\",\n            password=\"secret\",\n            email=\"super@example.com\",\n        )\n        self.client.force_login(superuser)\n        unknown_url = \"/unknown/\"\n        response = self.client.get(unknown_url)\n        self.assertEqual(response.status_code, 404)\n", "    def test_unknown_url_with_trailing_slash_without_final_catch_all_view(self):\n        unknown_url = \"/test_admin/admin10/unknown/\"\n        response = self.client.get(unknown_url)\n        self.assertEqual(response.status_code, 404)\n", "    def test_non_admin_url_404_if_authenticated(self):\n        superuser = User.objects.create_superuser(\n            username=\"super\",\n            password=\"secret\",\n            email=\"super@example.com\",\n        )\n        self.client.force_login(superuser)\n        unknown_url = \"/unknown/\"\n        response = self.client.get(unknown_url)\n        self.assertEqual(response.status_code, 404)\n\n", "    def test_non_admin_url_404_if_authenticated(self):\n        superuser = User.objects.create_superuser(\n            username=\"super\",\n            password=\"secret\",\n            email=\"super@example.com\",\n        )\n        self.client.force_login(superuser)\n        unknown_url = \"/unknown/\"\n        response = self.client.get(unknown_url)\n        self.assertEqual(response.status_code, 404)\n", "    def test_non_admin_url_404_if_authenticated(self):\n        superuser = User.objects.create_superuser(\n            username=\"super\",\n            password=\"secret\",\n            email=\"super@example.com\",\n        )\n        self.client.force_login(superuser)\n        unknown_url = \"/unknown/\"\n        response = self.client.get(unknown_url)\n        self.assertEqual(response.status_code, 404)\n"], "sample_871": ["def test_silhouette_samples_sparse_precomputed():\n    \"\"\"Check that silhouette_samples works for sparse matrices.\"\"\"\n    # Using sparse matrix for both X and pairwise_distances\n    from scipy import sparse\n\n    X = sparse.csr_matrix(\n        [[0.2, 0.1, 0.1, 0.2, 0.1, 1.6, 0.2, 0.1], [0.1, 0.2, 0.3, 0.1, 0.2, 0.4, 0.0, 0.2]]\n    )\n    y = [0, 0, 0, 0, 1, 1, 1, 1]\n    pdist_sparse = pairwise_distances(X, metric=\"precomputed\", dense_output=False)\n    output_with_sparse_input = silhouette_samples(\n        pdist_sparse, y, metric=\"precomputed\"\n    )\n    # Check that the output is as expected for sparse input\n    # (we compare to dense input here for verification)\n    pdist_dense = pairwise_distances(X.toarray())\n    output_with_dense_input = silhouette_samples(pdist_dense, y, metric=\"precomputed\")\n    assert_allclose(output_with_sparse_input, output_with_dense_input)\n\n", "def test_silhouette_score_sparse_matrix():\n    # Asserts silhouette_score works with sparse matrices\n\n    X_dense = np.array([[0.2, 0.1, 0.1, 0.2, 0.1, 1.6, 0.2, 0.1]], dtype=np.float32).T\n    X_csr = csr_matrix(X_dense)\n    y = [0, 0, 0, 0, 1, 1, 1, 1]\n\n    assert silhouette_score(X_dense, y) == silhouette_score(X_csr, y)\n\n\n", "def test_silhouette_with_missing_values():\n    # Test handling of missing values\n    X = np.array([[1, 2, np.nan], [4, 5, 6], [7, 8, 9]])\n    labels = [0, 0, 1]\n    with pytest.raises(ValueError, match=\"The array to be clustered cannot contain missing values. \"\n                                          \"Use sklearn.impute.SimpleImputer to fill missing values \"\n                                          \"before clustering.\"):\n        silhouette_score(X, labels)\n", "def test_silhouette_score_array_like_labels():\n    \"\"\"Check that silhouette_score accepts array-like labels.\"\"\"\n    dataset = datasets.load_iris()\n    X = dataset.data\n    labels_list = dataset.target.tolist()\n    labels_tuple = tuple(dataset.target)\n    scores_list = silhouette_score(X, labels_list)\n    scores_tuple = silhouette_score(X, labels_tuple)\n    assert scores_list == scores_tuple\n", "def test_silhouette_distance_matrix_type():\n    X = np.array([[0.2, 0.1, 0.1, 0.2, 0.1, 1.6, 0.2, 0.1]], dtype=np.float32).T\n    y = [0, 0, 0, 0, 1, 1, 1, 1]\n    # Test with invalid types\n    invalid_types = [int, bool, str, complex]\n    for dtype in invalid_types:\n        with pytest.raises(TypeError, match=\"Input distance matrix should be numeric\"):\n            silhouette_samples(np.array(X, dtype=dtype), y)\n\n\n"], "sample_1013": ["def test_issue_15437():\n    # Test for issue 15437, lambdify with symbols in a list\n\n    x = symbols('x')\n    y = symbols('y')\n    z = symbols('z')\n    expr = (x + y)**2 - z\n    f = lambdify([x, y, z], expr)\n    assert f(1, 2, 3) == 9\n\n\n", "def test_issue_15301():\n    # Ensure lambdify can handle recursive functions with symbol arguments\n\n    from sympy import Symbol, lambdify\n\n    x = Symbol('x')\n\n        if x == 0:\n            return 1\n        else:\n            return x * factorial(x-1)\n\n    f = lambdify(x, factorial(x))\n    assert f(5) == 120\n", "def test_lambdify_issue_15670():\n    x, y = symbols('x y')\n    f = lambdify((x, y), (x + y) * (x - y), 'sympy')\n    assert f(2, 3) == -1 \n", "def test_issue_16393():\n    # Test for issue 16393 - lambdify with  functions defined inside a function\n            return x + y\n        return inner_func\n\n    f = lambdify(x, my_func(x)(2), 'numpy')\n    assert f(1) == 3\n", "def test_issue_15823():\n    x = symbols('x')\n    f = lambdify(x, (x**2 + 1)/x, \"numpy\")\n    assert f(numpy.array([1.0, 2.0])) == numpy.array([2.0, 1.5]) \n"], "sample_513": ["def test_legend_label_sorting():\n    fig, ax = plt.subplots()\n    ax.plot(range(5), label='c')\n    ax.plot(range(5), label='a')\n    ax.plot(range(5), label='b')\n    leg = ax.legend(loc='best', title=\"My legend\",\n                    label_sort='alpha')\n    texts = [t.get_text() for t in leg.get_texts()]\n    assert texts == ['a', 'b', 'c']\n\n\n\n\n", "def test_legend_handle_label_override():\n    fig, ax = plt.subplots()\n    ax.plot([0, 1], [0, 1], label='testline')\n\n    handles, labels = ax.get_legend_handles_labels()\n    handles[0].set_label('new label')\n\n    legend = ax.legend()\n    for i, text in enumerate(legend.get_texts()):\n        assert text.get_text() == labels[i]\n\n", "compilation error", "def test_legend_with_handles_and_labels():\n    fig, ax = plt.subplots()\n    lines = [mlines.Line2D([0], [], color='r', label='line1'),\n             mlines.Line2D([0], [], color='g', label='line2')]\n    ax.plot([0, 1], [0, 1], label='data')\n    ax.legend(handles=lines, labels=['A', 'B'])\n\n\n", "compilation error"], "sample_68": ["    def test_cleanse_setting_handles_nested_dictionaries(self):\n        initial = {'user': {'name': 'john', 'password': 'doe'}, 'other_setting': {'sensitive': 'data'}}\n        expected = {'user': {'name': 'john', 'password': CLEANSED_SUBSTITUTE}, 'other_setting': {'sensitive': CLEANSED_SUBSTITUTE}}\n        self.assertEqual(cleanse_setting('SETTING_NAME', initial), expected)\n", "    def test_cleanse_setting_complex_dictionary(self):\n        initial = {\n            'user_profile': {\n                'name': 'John Doe',\n                'password': 'secret123'\n            },\n            'api_key': 'my_api_key'\n        }\n        expected = {\n            'user_profile': {\n                'name': 'John Doe',\n                'password': CLEANSED_SUBSTITUTE\n            },\n            'api_key': 'my_api_key'\n        }\n        self.assertEqual(cleanse_setting('SETTING_NAME', initial), expected)\n", "    def test_cleanse_setting_recurses_in_nested_dictionaries(self):\n        initial = {'level1': {'level2': {'password': 'secret'}}}\n        expected = {'level1': {'level2': {'password': CLEANSED_SUBSTITUTE}}}\n        self.assertEqual(cleanse_setting('SETTING_NAME', initial), expected)\n", "    def test_cleanse_setting_with_nested_dictionaries(self):\n        initial = {'foo': {'bar': 'value', 'password': 'secret'}, 'baz': 'other'}\n        expected = {'foo': {'bar': 'value', 'password': CLEANSED_SUBSTITUTE}, 'baz': 'other'}\n        self.assertEqual(cleanse_setting('PASSWORD', initial), expected)\n\n", "    def test_cleanse_setting_ignore_nested_non_string_keys(self):\n        initial = {'login': 'cooper', 42: 'secret'}\n        expected = {'login': 'cooper', 42: CLEANSED_SUBSTITUTE}\n        self.assertEqual(cleanse_setting('SETTING_NAME', initial), expected)\n"], "sample_116": ["    def test_with_custom_backend(self):\n        custom_backend = 'django.core.cache.backends.locmem.LocMemCache'\n        settings_overrides = {\n            'CACHES': {\n                'custom_cache': {\n                    'BACKEND': custom_backend,\n                },\n            }\n        }\n        with override_settings(**settings_overrides):\n            cache = caches['custom_cache']\n            self.assertEqual(cache.__class__.__name__, 'LocMemCache')  \n", "    def test_cache_key_with_session(self):\n        request = self.factory.get(self.path)\n        request.session = {\"test_session_key\": \"test_session_value\"}\n        template = engines['django'].from_string(\"This is a test\")\n        response = TemplateResponse(HttpRequest(), template)\n        learn_cache_key(request, response)\n        self.assertEqual(\n            get_cache_key(request),\n            'views.decorators.cache.cache_page.settingsprefix.GET.'\n            'c3e6694b5b2312037034a8761419247e.d41d8cd98f00b204e9800998ecf8427e'\n        )\n", "    def test_different_aliases(self):\n        \"\"\"\n        Requesting different aliases should yield different instances.\n        \"\"\"\n        cache1 = caches['default']\n        cache2 = caches['other']\n\n        self.assertIsNot(cache1, cache2)\n", "    def test_cache_key_with_headers(self):\n        request = self.factory.get(self.path, headers={'Cookie': 'testcookie'})\n        template = engines['django'].from_string(\"This is a test\")\n        response = TemplateResponse(HttpRequest(), template)\n        # Expect None if no headers have been set yet.\n        self.assertIsNone(get_cache_key(request))\n        # Set headers to an empty list.\n        learn_cache_key(request, response)\n        # The headers are taken into account.\n        self.assertEqual(\n            get_cache_key(request),\n            'views.decorators.cache.cache_page.settingsprefix.GET.'\n            'a8a88704a08f382a3448b913913d8393.d41d8cd98f00b204e9800998ecf8427e'\n        )\n\n", "    def test_different_caches(self):\n        cache1 = caches['default']\n        cache2 = caches['other']\n\n        self.assertIsNot(cache1, cache2)\n"], "sample_1060": ["compilation error", "def test_NumPyPrinter_logical_operators():\n    n = NumPyPrinter()\n    assert n._print_And(And(x > 0, y < 10)) == 'numpy.logical_and(x > 0, y < 10)'\n    assert n._print_Or(Or(x > 0, y < 10)) == 'numpy.logical_or(x > 0, y < 10)'\n    assert n._print_Not(Not(x > 0)) == 'numpy.logical_not(x > 0)'\n", "def test_NumPyPrinter_MatrixBase():\n    n = NumPyPrinter()\n\n    A = MatrixSymbol('A', 2, 2)\n    assert n.doprint(A) == \"numpy.array([[None, None], [None, None]])\"\n    \n\n", "def test_NumPyPrinter_MatrixPow():\n    p = NumPyPrinter()\n    A = MatrixSymbol('A', 2, 2)\n    result = p.doprint(A**3)\n    assert result == 'numpy.linalg.matrix_power(A, 3)'\n", "def test_NumPyPrinter_print_MatrixBase():\n    n = NumPyPrinter()\n    A = MatrixSymbol(\"A\", 2, 2)\n    B = MatrixSymbol(\"B\", 2, 2)\n    assert n._print_MatrixBase(A) == \"numpy.array([[None, None], [None, None]])\"\n    assert n._print_MatrixBase(B) == \"numpy.array([[None, None], [None, None]])\"\n    assert n._print_MatrixBase(A.transpose()) == \"numpy.transpose(numpy.array([[None, None], [None, None]]))\"\n    assert n._print_MatrixBase(A + B) == \"numpy.add(numpy.array([[None, None], [None, None]]), numpy.array([[None, None], [None, None]]))\"\n\n\n"], "sample_380": ["    def test_exists_with_subquery_and_aggregate(self):\n        exists_qs = Subquery(\n            Author.objects.filter(pk__in=OuterRef('book__authors')).values('pk'),\n        )\n        qs = Book.objects.annotate(\n            count=Count('id'),\n            has_author=Exists(exists_qs),\n        )\n        self.assertEqual(qs.count(), 6)\n", "    def test_aggregation_default_with_compound_expression(self):\n        expr = Avg('book__rating', default=Value(1.0) * Value(8.0))\n        result = Book.objects.aggregate(value=expr)\n        self.assertAlmostEqual(result['value'], 3.3, places=1)\n", "    def test_exists_with_subquery_and_outerref(self):\n        author_books = Book.objects.filter(\n            author__age__gt=30,\n        ).values_list('author_id', flat=True)\n        exists_books = Book.objects.annotate(\n            exists=Exists(\n                Subquery(\n                    Author.objects.filter(pk__in=author_books)\n                )\n            )\n        ).all()\n        self.assertEqual(len(exists_books), 6)\n\n\n", "    def test_exists_with_subquery_alias(self):\n        subquery = Author.objects.filter(pk=OuterRef('pk')).values_list('name', flat=True)\n        qs = Book.objects.annotate(\n            exists=Exists(subquery),\n        ).filter(exists=True)\n        self.assertSequenceEqual(\n            qs.values_list('title', flat=True),\n            [\n                'Practical Django Projects',\n                'The Definitive Guide to Django: Web Development Done Right',\n                'Django for Beginners',\n                'Django Web Development',\n                'Test Driven Development with Django',\n                'Building Web Applications with Django',\n            ],\n        )\n\n\n\n", "    def test_aggregate_with_subquery_using_outer_reference_in_subquery(self):\n        with self.assertNumQueries(2):\n            subquery = Subquery(\n                Publisher.objects.filter(\n                    pk=OuterRef('publisher'),\n                ).values('name')\n            )\n            result = Book.objects.annotate(\n                publisher_name=subquery\n            ).all()\n        self.assertSequenceEqual(result, self.books)\n"], "sample_21": ["def test_err_specs_are_preserved_when_writing_and_reading(tmp_path):\n    test_file = tmp_path / \"test.qdp\"\n    t1 = Table()\n    col1 = Column(name=\"a\", data=[1, 2, 3, 4])\n    col2 = MaskedColumn(\n        data=[4.0, np.nan, 3.0, 1.0], name=\"b\", mask=[False, False, False, True]\n    )\n    t1.add_column(col1)\n    t1.add_column(col2)\n    t1.write(test_file, format=\"ascii.qdp\", err_specs={'terr': [1], 'serr': [3]})\n    t2 = Table.read(test_file, names=[\"a\", \"b\"], err_specs={'terr': [1], 'serr': [3]})\n    assert np.allclose(t2[\"a\"], t1[\"a\"])\n    assert np.allclose(t2[\"b\"], t1[\"b\"])\n    \n", "def test_read_write_with_err_specs(tmp_path):\n    test_file = tmp_path / \"test.qdp\"\n    t1 = Table()\n    t1.add_column(Column(name=\"a\", data=[1, 2, 3, 4]))\n    t1.add_column(\n        MaskedColumn(\n            data=[4.0, np.nan, 3.0, 1.0],\n            name=\"b\",\n            mask=[False, False, False, True],\n        )\n    )\n    t1.write(test_file, format=\"ascii.qdp\", err_specs={\"terr\": [1], \"serr\": [3]})\n    t2 = Table.read(test_file, format=\"ascii.qdp\", err_specs={\"terr\": [1], \"serr\": [3]})\n    assert np.allclose(t2[\"a\"], t1[\"a\"])\n    assert np.allclose(t2[\"b\"], t1[\"b\"])\n\n\n\n", "def test_read_multiple_tables_with_different_num_columns(tmp_path):\n    example_qdp = \"\"\"\n    ! Swift/XRT hardness ratio of trigger: XXXX, name: BUBU X-2\n    ! Columns are as labelled\n    READ TERR 1\n    READ SERR 2\n    ! WT -- hard data\n    !MJD            Err (pos)       Err(neg)        Rate       Error\n    53000.123456 2.37847222222222e-05    -2.37847222222222e-05   NO       0.212439\n    55045.099887 1.14467592592593e-05    -1.14467592592593e-05   0.000000        0.000000\n    NO NO NO NO NO\n    ! WT -- soft data\n    !MJD            Err (pos)       Err(neg)        Rate\n    53000.123456 2.37847222222222e-05    -2.37847222222222e-05   0.726155\n    55045.099887 1.14467592592593e-05    -1.14467592592593e-05   2.410935\n    NO NO NO NO\n    ! WT -- hardness ratio\n    !MJD            Err (pos)       Err(neg)        Rate\n    53000.123456 2.37847222222222e-05    -2.37847222222222e-05   -0.292553\n    55045.09", "def test_read_write_error_columns(tmp_path):\n    test_file = tmp_path / \"test.qdp\"\n    t1 = Table()\n    t1.add_column(Column(name=\"a\", data=[1, 2, 3]))\n    t1.add_column(\n        MaskedColumn(\n            data=[4.0, np.nan, 3.0], name=\"b\", mask=[False, False, True]\n        )\n    )\n    t1.add_column(Column(name=\"c\", data=[5, 6, 7]))\n    t1.write(test_file, format=\"ascii.qdp\", err_specs={'terr': [1]})\n    t2 = Table.read(test_file, names=[\"a\", \"b\", \"c\"], table_id=0, format=\"ascii.qdp\")\n    assert np.allclose(t2[\"a\"], t1[\"a\"])\n    assert np.allclose(t2[\"b\"], t1[\"b\"])\n    assert np.allclose(t2[\"c\"], t1[\"c\"])\n", "    def test_read_write_masked_columns(tmp_path):\n        test_file = tmp_path / \"test.qdp\"\n        t1 = Table()\n        t1[\"col1\"] = [1, 2, np.nan, 4]\n        t1.add_column(MaskedColumn(data=[np.nan, 2.0, 3.0, 4.0], name=\"col2\", mask=[True, False, False, False]))\n        t1.write(test_file, format=\"ascii.qdp\")\n        t2 = Table.read(test_file, format=\"ascii.qdp\")\n\n        assert np.allclose(t1[\"col1\"], t2[\"col1\"])\n        assert np.allclose(t1[\"col2\"], t2[\"col2\"])\n        assert np.all(t1[\"col2\"].mask == t2[\"col2\"].mask)\n"], "sample_376": ["    def test_not_finished_sentinel(self):\n        storage = self.get_storage()\n        response = self.get_response()\n        storage.add(constants.INFO, 'test')\n        storage.add(constants.INFO, 'another')\n        unstored_messages = storage._store(messages=[\n            Message(constants.INFO, 'test'),\n            Message(constants.INFO, 'another')\n        ], response=response, remove_oldest=False)\n        self.assertEqual(stored_cookie_messages_count(storage, response), 1)\n        self.assertEqual(unstored_messages, [Message(constants.INFO, 'another')])\n\n\n\n\n", "    def test_not_finished_sentinel(self):\n        storage = self.get_storage()\n        response = self.get_response()\n        # Add messages to storage\n        storage.add(constants.INFO, 'message1')\n        storage.add(constants.INFO, 'message2')\n        storage.add(constants.INFO, 'message3')\n\n        # Update cookie with messages\n        storage.update(response)\n\n        # Simulate partially fetching messages\n        messages1 = list(storage)\n        self.assertEqual(len(messages1), 2)\n        self.assertEqual(messages1, ['message1', 'message2'])\n\n        # Update cookie again\n        storage.update(response)\n\n        # Retrieve remaining messages\n        messages2 = list(storage)\n        self.assertEqual(len(messages2), 1)\n        self.assertEqual(messages2, ['message3'])\n", "    def test_not_finished_sentinel(self):\n        storage = self.get_storage()\n        response = self.get_response()\n\n        messages = ['test', 'me']\n        set_cookie_data(storage, messages, encode_empty=True)\n        self.assertEqual(stored_cookie_messages_count(storage, response), 2)\n\n        # Add more messages and update the cookie, but remove the oldest message to simulate exceeding the max_cookie_size\n        storage.add(constants.INFO, 'something else')\n        storage.add(constants.INFO, 'another message')\n        unstored_messages = storage.update(response)\n\n        # There should be one message still in storage that couldn't be inserted due to size limitations\n        self.assertEqual(len(unstored_messages), 1)\n        self.assertEqual(stored_cookie_messages_count(storage, response), 3)\n\n\n\n        # Check if the sentinel value is present\n\n        cookie = response.cookies.get(storage.cookie_name)\n        self.assertEqual(cookie.value[-1], CookieStorage.not_finished)\n\n\n", "    def test_not_finished(self):\n        storage = self.get_storage()\n        response = self.get_response()\n        messages = ['message 1', 'message 2']\n        storage._store(messages, response, remove_oldest=False)\n\n        # The cookie should contain the sentinel value indicating\n        # not all messages were stored.\n        self.assertEqual(self.stored_messages_count(storage, response), 2)\n        self.assertTrue(storage._loaded_data[-1] == CookieStorage.not_finished)\n\n        new_messages = ['message 3']\n        storage._store(new_messages, response, remove_oldest=False)\n\n        # The cookie should now contain all messages\n        self.assertEqual(self.stored_messages_count(storage, response), 3)\n        self.assertFalse(storage._loaded_data[-1] == CookieStorage.not_finished)", "    def test_not_finished(self):\n        storage = self.get_storage()\n\n        # Simulate a partial message store followed by completion.\n        # The first partial store should leave off the last message,\n        # and the second store should append it, and we should receive all\n        # messages in total.\n        messages = ['msg1', 'msg2', 'msg3']\n        set_cookie_data(storage, messages[:2])\n        self.assertEqual(list(storage), messages[:2])\n\n        set_cookie_data(storage, messages[2:], encode_empty=True)\n        self.assertEqual(list(storage), messages)\n\n\n"], "sample_199": ["    def test_annotation_with_nested_m2m(self):\n        Book.objects.create(\n            title='The Hitchhiker\\'s Guide to the Galaxy',\n            authors=[self.a1, self.a2],\n            pubdate=datetime.date(1979, 10, 12),\n        )\n        Book.objects.create(\n            title='Pride and Prejudice',\n            authors=[self.a3],\n            pubdate=datetime.date(1813, 1, 28),\n        )\n        qs = Book.objects.annotate(\n            author_name=F('authors__name'),\n            author_age=F('authors__age'),\n        ).values_list('title', 'author_name', 'author_age')\n        self.assertCountEqual(qs, [\n            ('The Hitchhiker\\'s Guide to the Galaxy', 'Douglas Adams', 34),\n            ('The Hitchhiker\\'s Guide to the Galaxy', 'John Lloyd', 35),\n            ('Pride and Prejudice', 'Jane Austen', 29),\n        ])\n", "    def test_annotate_with_null_values(self):\n        book = Book.objects.create(\n            title='The Hitchhiker\\'s Guide to the Galaxy',\n            authors=Author.objects.get(name='Douglas Adams'),\n            pubdate=datetime.date(1979, 10, 12),\n            pages=224,\n        )\n        book_with_null_pubdate = Book.objects.create(\n            title='The Restaurant at the End of the Universe',\n            authors=Author.objects.get(name='Douglas Adams'),\n            pubdate=None,\n            pages=256,\n        )\n        qs = Book.objects.annotate(\n            pubyear=F('pubdate__year'),\n            pubyear_null=Value(None, output_field=IntegerField()),\n        ).filter(pubdate__isnull=True).values(\n            'title', 'pubyear', 'pubyear_null'\n        ).order_by('title')\n        self.assertCountEqual(\n            qs, [\n                {'title': 'The Restaurant at the End of the Universe', 'pubyear': None, 'pubyear_null': None},\n            ]\n        )\n", "    def test_annotation_with_complex_ordering(self):\n        # Test ordering with multiple annotations and F expressions.\n        books = Book.objects.annotate(\n            rating_rank=Func(\n                Rank(),\n                output_field=IntegerField(),\n                order_by=F('rating').desc()\n            ),\n            price_rank=Func(\n                Rank(),\n                output_field=IntegerField(),\n                order_by=F('price').asc()\n            ),\n        ).order_by('rating', 'price')\n\n        self.assertCountEqual(\n            books.values_list('pk', 'rating', 'price', 'rating_rank', 'price_rank'),\n            [\n                (self.b6.pk, 5.0, 75.00, 1, 1),\n                (self.b5.pk, 4.5, 24.95, 2, 2),\n                (self.b1.pk, 4.0, 19.95, 3, 3),\n                (self.b2.pk, 3.5, 14.95, 4, 4),\n                (self.b3.pk, 3.0, 12.99, 5, 5),\n                (self.b4.pk, 2.5, 10.99, 6, 6),\n            ]\n        )\n", "    def test_annotation_with_raw_subquery(self):\n        subquery = (\n            Book.objects.select_related('publisher')\n            .values('isbn', 'title')\n            .annotate(\n                publisher_name=F('publisher__name'),\n            )\n        )\n        qs = Book.objects.annotate(\n            subquery_result=Subquery(subquery, output_field=CharField(max_length=100)),\n        )\n        self.assertQuerysetEqual(\n            qs,\n            [\n                ('155860191', 'Paradigms of Artificial Intelligence Programming: Case Studies in Common Lisp', 'Apress'),\n                ('159059725', 'Practical Django Projects', 'Morgan Kaufmann'),\n                ('159327039', 'Python Cookbook', 'O\\'Reilly'),\n                ('156592174', 'The 7 Habits of Highly Effective People', 'Simon & Schuster'),\n                ('193239447', 'Core Python Programming', 'Pragmatic Bookshelf'),\n            ],\n            lambda b: (b.isbn, b.title, b.subquery_result),\n        )\n\n\n", "    def test_multi_values_annotation_with_aggregate(self):\n        qs = Book.objects.annotate(\n            avg_rating=Avg('rating'),\n            min_rating=Min('rating'),\n            count_isbn=Count('isbn'),\n        ).values('publisher__name', 'avg_rating', 'min_rating', 'count_isbn').order_by('publisher__name')\n        self.assertCountEqual(\n            qs, [\n                {'avg_rating': 4.0, 'min_rating': 4.0, 'count_isbn': 1, 'publisher__name': 'Apress'},\n                {'avg_rating': 4.333333333333333, 'min_rating': 4.0, 'count_isbn': 2, 'publisher__name': 'Jonno\\'s House of Books'},\n                {'avg_rating': 4.6, 'min_rating': 4.6, 'count_isbn': 1, 'publisher__name': 'Morgan Kaufmann'},\n                {'avg_rating': 4.5, 'min_rating': 4.0, 'count_isbn': 1, 'publisher__name': 'Prentice Hall'},\n                {'avg_rating': 4.0, 'min_rating': 4.0, 'count_isbn': 1, 'publisher__name': 'Sams'},\n            ]\n        )\n\n"], "sample_973": ["        def __init__(self, func):\n            self.func = func\n", "        def __init__(self, value):\n            self._value = value\n", "    def test_getfullargspec_with_default_args():\n            pass\n\n        full_argspec = inspect.getfullargspec(func)\n        assert full_argspec.args == ['a', 'b']\n        assert full_argspec.varargs == 'args'\n        assert full_argspec.varkw == 'kwargs'\n        assert full_argspec.defaults == [1]\n\n", "compilation error", "compilation error"], "sample_534": ["compilation error", "def test_contour_negative_cmap():\n    x = np.arange(-10, 10, 0.25)\n    y = np.arange(-10, 10, 0.25)\n    X, Y = np.meshgrid(x, y)\n    Z = np.sin(np.sqrt(X**2 + Y**2))\n\n    fig, ax = plt.subplots()\n\n    cs = ax.contourf(X, Y, Z, cmap=\"inferno_r\")\n    cs = ax.contour(X, Y, Z, cmap=\"inferno_r\")\n\n    assert cs.cmap.is_grayscale() is False\n\n    plt.close(fig)\n", "def test_contour_fill():\n    x = np.arange(5)\n    y = x\n    X, Y = np.meshgrid(x, y)\n    Z = np.sin(X**2 + Y**2)\n    fig, ax = plt.subplots()\n    cs = ax.contourf(X, Y, Z, 4)\n    ax.clabel(cs)\n    assert isinstance(cs, ContourSet)\n    assert cs.filled\n    \n    # test that fill works for single non-filled contour\n    cs = ax.contour(X, Y, Z, 4, linewidths=2, colors='gray')\n    ax.clabel(cs)\n    assert not isinstance(cs, ContourSet)\n    assert not cs.filled\n", "compilation error", "def test_contour_label_limits():\n    x, y = np.mgrid[-2:2:20j, -1.5:1.5:20j]\n    z = np.sin(np.sqrt(x**2 + y**2))\n    fig, ax = plt.subplots()\n    cs = ax.contour(x, y, z, levels=10)\n    ax.clabel(cs, inline=True)\n\n    # Check label limits are set correctly\n    assert cs.label_limits[0] == min(x.flatten())\n    assert cs.label_limits[1] == max(x.flatten())\n    assert cs.label_limits[2] == min(y.flatten())\n    assert cs.label_limits[3] == max(y.flatten())\n\n"], "sample_166": ["    def test_get_random_string_length(self):\n        self.assertEqual(len(get_random_string(length=20)), 20)\n", "    def test_get_random_string_length(self):\n        self.assertEqual(len(get_random_string(length=10)), 10)\n        self.assertEqual(len(get_random_string(length=20)), 20)\n", "    def test_get_random_string_length(self):\n        self.assertEqual(len(get_random_string(length=36)), 36)\n        self.assertEqual(len(get_random_string(length=100)), 100)\n\n\n", "    def test_get_random_string_length(self):\n        self.assertEqual(len(get_random_string(length=10)), 10)\n        self.assertEqual(len(get_random_string(length=20)), 20)\n", "    def test_get_random_string_length(self):\n        self.assertEqual(length_expected, get_random_string(length=length_expected))\n\n\n"], "sample_310": ["    def test_simplify_regex_with_groups(self):\n        tests = (\n            (r'^app/(?P<model>\\w+)/(?P<id>\\d+)$', '/app/<model>/<id>'),\n            (r'^user/(?P<user_id>\\d+)/(?P<profile_id>\\d+)$', '/user/<user_id>/<profile_id>'),\n            (r'^api/(?P<version>\\d+)/(?P<resource>\\w+)(/(?P<id>\\d+))?$', '/api/<version>/<resource>'),\n        )\n        for pattern, output in tests:\n            with self.subTest(pattern=pattern):\n                self.assertEqual(simplify_regex(pattern), output)\n\n\n", "    def test_simplify_regex_unicode(self):\n        tests = (\n            (r'^\\x87\\w+', '/\\x87\\w+'),\n            (r'\\x87\\w+\\$', r'/\\x87\\w+'),\n            (r'\\A\\x87\\w+\\Z', r'/\\x87\\w+'),\n            (r'^\\b\\x87\\w+\\B', r'/\\x87\\w+'),\n        )\n        for pattern, output in tests:\n            with self.subTest(pattern=pattern):\n                self.assertEqual(simplify_regex(pattern), output)\n\n\n\n", "    def test_simplify_regex_with_parentheses_and_groups(self):\n        tests = (\n            (r'^(?P<category>\\w+)\\/(?P<id>[0-9]+)$', '/<category>/<id>'),\n            (r'^admin/(?P<app_label>\\w+)/(?P<model_name>\\w+)/$', '/admin/<app_label>/<model_name>'),\n        )\n        for pattern, output in tests:\n            with self.subTest(pattern=pattern):\n                self.assertEqual(simplify_regex(pattern), output)\n", "    def test_simplify_regex_with_group_named_in_parentheses(self):\n        tests = (\n            (r'^(?P<a>\\w+)/b/\\((?P<c>\\w+)\\)$', '/<a>/b/<c>'),\n        )\n        for pattern, output in tests:\n            with self.subTest(pattern=pattern):\n                self.assertEqual(simplify_regex(pattern), output)\n\n\n\n", "    def test_simplify_regex_with_lookarounds(self):\n        tests = (\n            (r'(?<!a)b(?!c)', '/b'),\n            (r'(?<=a)b(?!c)', '/b'),\n            (r'(?<=a)(?=b)', '/b'),\n            (r'(?<!a)(?=b)', '/b'),\n        )\n        for pattern, output in tests:\n            with self.subTest(pattern=pattern):\n                self.assertEqual(simplify_regex(pattern), output)\n"], "sample_558": ["def test_grid_with_different_axes_classes():\n    fig = plt.figure()\n    \n    grid = Grid(fig, 111, (1, 2), axes_class=mpl_toolkits.axes_grid1.mpl_axes.Axes)\n\n    ax1 = grid[0]\n    ax2 = grid[1]\n\n    ax1.set_xlabel('X axis')\n    ax2.set_ylabel('Y axis')\n", "def test_grid_axes_locator():\n    fig = plt.figure()\n    grid = Grid(fig, 111, (2, 2), direction='row')\n    for i, ax in enumerate(grid):\n        ax.set_axis_off()\n    # Check if the axes are in the expected locations.\n    ax = grid[0]\n    assert ax.get_axes_locator().args[0] == pytest.approx(0)\n    ax = grid[-1]\n    assert ax.get_axes_locator().args[0] == pytest.approx(0.5)\n", "def test_grid_axes_locator(direction, ngrids):\n    fig = plt.figure()\n    grid = Grid(fig, 111, ngrids, direction=direction)\n    for ax in grid:\n        assert isinstance(ax.get_axes_locator(), mpl.transforms.Locator)\n", "def test_grid_label_visibility(direction):\n    fig = plt.figure()\n    grid = Grid(fig, 111, (2, 2), direction=direction)\n    # Turn off all labels\n    for ax in np.ravel(grid.axes_all):\n        ax.set_xlabel('')\n        ax.set_ylabel('')\n    # Check that labels are hidden\n    for ax in np.ravel(grid.axes_all):\n        assert ax.get_xlabel() == ''\n        assert ax.get_ylabel() == ''\n    # Ensure that the labels are visible if they are not turned off\n    grid.set_labels_visible(True)\n    for ax in np.ravel(grid.axes_all):\n        assert ax.get_xlabel() != ''\n        assert ax.get_ylabel() != ''\n", "def test_inset_bbox():\n    fig, ax = plt.subplots()\n    ax.plot([0, 1, 2, 3], [0, 1, 2, 3])\n    inset_ax = zoomed_inset_axes(ax, zoom=1.5, loc='upper left', bbox_to_anchor=(0.1, 0.3, 0.6, 0.6))\n    inset_ax.plot([0, 1, 2, 3], [0, 1, 2, 3])\n"], "sample_67": ["    def test_exclude_in_model_to_dict(self):\n        \"\"\"Can exclude fields from model_to_dict.\"\"\"\n        item = ColourfulItem.objects.create(name='test', colours=Colour.objects.create(name='red'))\n        data = model_to_dict(item, exclude=['name'])\n        self.assertEqual(data, {'colours': 1})\n", "    def test_nested_objects(self):\n        \"\"\"\n        Nested objects should be converted to dictionaries.\n        \"\"\"\n        parent = Category.objects.create(name='Parent')\n        child = Category.objects.create(name='Child', parent=parent)\n        data = model_to_dict(child)\n        self.assertIn('parent', data)\n        self.assertIsInstance(data['parent'], dict)\n        self.assertEqual(data['parent']['id'], parent.id)\n\n", "    def test_null_values(self):\n        \"\"\"\n        Make sure null values are represented correctly in model_to_dict().\n        \"\"\"\n        item = ColourfulItem.objects.create()\n        data = model_to_dict(item)\n        for field_name in ColourfulItem._meta.get_fields():\n            if field_name.name == 'colours':\n                continue\n            if field_name.null is True:\n                self.assertIsNone(data[field_name.name])\n", "    def test_exclude_not_empty_with_custom_fields(self):\n        \"\"\"Regression for #19733: Excluding fields should work with custom data.\"\"\"\n        class MyForm(forms.ModelForm):\n            new_field = forms.CharField()\n\n            class Meta:\n                model = Person\n                fields = '__all__'\n                exclude = ['name']\n\n        form = MyForm({'new_field': 'test'})\n        self.assertTrue(form.is_valid())\n        self.assertEqual(form.cleaned_data, {'new_field': 'test'})\n\n", "    def test_order_by_in_modelform(self):\n        class OrderForm(forms.ModelForm):\n            class Meta:\n                model = Order\n                fields = ['name', 'created_at']\n                ordering = 'created_at'\n\n        form = OrderForm(data={'name': 'test', 'created_at': datetime.datetime.now()})\n        self.assertTrue(form.is_valid())\n        self.assertEqual(dict(form.cleaned_data), {'name': 'test', 'created_at': datetime.datetime.now()})\n\n\n\n"], "sample_860": ["def test_check_array_dtype(dtype, expected_dtype):\n    X = np.ones((2, 2), dtype=dtype)\n    X_checked = check_array(X)\n    assert X_checked.dtype == expected_dtype\n", "def test_check_sparse_matrix_dtype():\n    # Check that check_sparse_matrix_dtype handles various dtypes correctly\n    X_csr = sp.csr_matrix([[1, 2], [3, 4]], dtype=np.int32)\n    X_csc = sp.csc_matrix([[1, 2], [3, 4]], dtype=np.float64)\n    X_coo = sp.coo_matrix([[1, 2], [3, 4]], dtype=np.complex128)\n\n    with pytest.warns(None):\n        check_sparse_matrix_dtype(X_csr, dtype=np.int32)\n        check_sparse_matrix_dtype(X_csc, dtype=np.float64)\n        check_sparse_matrix_dtype(X_coo, dtype=np.complex128)\n\n    # Check that it raises an error if the dtype is incompatible\n    with pytest.raises(ValueError,\n                       match=\"$X_sparse.dtype is incompatible with dtype\"):\n        check_sparse_matrix_dtype(X_csr, dtype=np.str)\n\n\n", "def test_allclose_dense_sparse_identity(toarray):\n    # Check if identity matrices are correctly compared\n    x = toarray(np.identity(5))\n    y = toarray(np.identity(5))\n    assert _allclose_dense_sparse(x, y)\n", "def test_check_array_dtype_validation(dtype):\n    X = np.ones((3, 3), dtype=dtype)\n    X_checked = check_array(X, dtype=dtype, force_all_finite=False)\n    assert X_checked.dtype == dtype\n\n    X_checked = check_array(X, dtype=dtype, force_all_finite=True)\n    assert X_checked.dtype == dtype\n\n\n\n", "def test_check_consistent_feature_names():\n    X = pd.DataFrame({'a': [1, 2], 'b': [3, 4]})\n    y = [5, 6]\n\n    # Check success case\n    with pytest.warns(None) as record:\n        check_consistent_feature_names(X, y)\n    assert len(record) == 0\n\n    # Check for inconsistent feature names\n    X = pd.DataFrame({'a': [1, 2], 'b': [3, 4]})\n    y = [5, 6]\n    with pytest.warns_message(\n            \"Feature names in X and y should be consistent, got ['a', 'b'] \"\n            \"and ['c', 'd'] respectively.\",\n            match=r\"Feature names in X and y should be consistent\",\n            ):\n        check_consistent_feature_names(X, y)\n"], "sample_882": ["compilation error", "def test_mlp_early_stopping_with_validation_fraction(MLPEstimator):\n    \"\"\"Check that early stopping works with validation_fraction.\"\"\"\n    mlp = MLPEstimator(\n        max_iter=10,\n        random_state=0,\n        early_stopping=True,\n        validation_fraction=0.2,\n    )\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", UserWarning)\n        mlp.fit(X_iris, y_iris)\n\n    assert len(mlp.validation_scores_) == 10\n", "def test_early_stopping_validation_fraction_shuffled(\n    validation_fraction, shuffled", "def test_mlp_partial_fit_after_partial_fit(MLPEstimator):\n    mlp = MLPEstimator(early_stopping=True, random_state=0)\n    X_train = X_digits[:100]\n    y_train = y_digits[:100]\n    mlp.partial_fit(X_train, y_train)\n    mlp.partial_fit(X_train, y_train)\n    assert mlp.n_iter_ > 0\n", "def test_mlp_partial_fit_validation_fraction(MLPEstimator):\n    \"\"\"Check partial_fit works correctly with early_stopping and validation_fraction.\"\"\"\n    mlp = MLPEstimator(\n        early_stopping=True,\n        validation_fraction=0.2,\n        random_state=0,\n    )\n\n    # Fit on the whole dataset\n    X, y = X_iris, y_iris\n    mlp.fit(X, y)\n\n    # Partial fit with a subset of the data\n    new_data = X[:50]\n    new_target = y[:50]\n    mlp.partial_fit(new_data, new_target)\n\n    # Check if validation scores were updated\n    assert len(mlp.validation_scores_) > 0\n\n\n"], "sample_803": ["compilation error", "compilation error", "def test_label_ranking_loss_with_sparse_matrices():\n    y_true = csr_matrix(\n        [[0, 1, 0], [1, 1, 0], [0, 1, 1]])\n    y_score = csr_matrix(\n        [[0.1, 10., -3], [3, 1, 3], [0, 2, 0]])\n    assert_almost_equal(label_ranking_loss(y_true, y_score),\n                        (0 + 2 / 2 + 1 / 2) / 3)\n\n\n", "def test_ranking_loss_with_sparse_matrix():\n    y_true = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n    y_score = csr_matrix(np.array([[0.5, 0.2, 0.3], [0.8, 0.1, 0.1], [0.1, 0.9, 0.0]]))\n    assert_almost_equal(label_ranking_loss(y_true, y_score),\n                        label_ranking_loss(y_true, y_score.toarray()))\n", "compilation error"], "sample_630": ["                def my_method(self, a: str) -> str:\n                    return a", "def test_get_annotation_arg(init_method, label):\n    \"\"\"Arg\"\"\"\n    assign = rf\"\"\"\n        class A:\n            {init_method}\n    \"\"\"\n    node = astroid.extract_node(assign)\n    for arg in node.args:\n        got = get_annotation(arg).name\n        assert isinstance(arg, astroid.arg), f\"got {got} instead of {label} for value {node}\"\n\n\n\n", "def test_get_values_method_signature(code, expected_label):\n    node = astroid.extract_node(code)\n    instance_attrs = node.instance_attrs\n    for _, methods in instance_attrs.items():\n        for method in methods:\n            if isinstance(method, astroid.FunctionDef):\n                got = DotWriter.get_values()(method)[\"label\"]\n                assert got == expected_label\n\n", "def test_get_annotation_type_comment(type_comment, expected):\n    \"\"\"type comment\"\"\"\n    node = astroid.extract_node(f\"a: {type_comment}\")\n    got = get_annotation(node).name\n    assert got == expected, f\"got {got} instead of {expected} for value {node}\"\n", "def test_get_method_parameters(code, expected):\n    node = astroid.extract_node(code)\n    got = {}\n    for method in node.nodes:\n        if isinstance(method, astroid.FunctionDef):\n            for arg in method.args.args:\n                if arg.annotation:\n                    got.setdefault(method.name, {})[arg.arg] = get_annotation(arg.annotation).name\n    assert got == expected, f\"got {got} instead of {expected}\"\n"], "sample_493": ["    def test_subquery_in_aggregate_with_exists(self):\n        with CaptureQueriesContext(connection) as ctx:\n            aggregate = Book.objects.annotate(\n                has_author=Exists(Author.objects.filter(age__gt=1))\n            ).filter(has_author).aggregate(count=Count(\"id\"))\n        self.assertEqual(aggregate[\"count\"], 5)\n        sql = ctx.captured_queries[0][\"sql\"].lower()\n        self.assertEqual(sql.count(\"select\"), 3, \"Subquery wrapping required\")\n\n\n", "    def test_aggregate_with_unrelated_alias(self):\n        with self.assertRaises(ValueError) as e:\n            Author.objects.annotate(\n                book_count=Count(\"book\"),\n            ).aggregate(\n                total_books=Count(\"book_count\", distinct=True),\n            )\n        self.assertIn(\n            \"Cannot aggregate over a column that is not in the current query.\",\n            str(e.exception),\n        )\n\n\n\n", "    def test_window_function_with_multiple_aggregates(self):\n        with self.assertNumQueries(1) as ctx:\n            aggregate = Book.objects.annotate(\n                avg_pages_per_publisher=Window(Avg(\"pages\"), partition_by=F(\"publisher\")),\n                total_pages=Sum(\"pages\"),\n            ).values(\n                \"publisher__name\",\n                \"avg_pages_per_publisher\",\n                \"total_pages\",\n            ).order_by(\"publisher__name\")\n        sql = ctx.captured_queries[0][\"sql\"].lower()\n        self.assertEqual(sql.count(\"select\"), 1)\n        self.assertIn(\n            \"window avg_pages_per_publisher\", sql,\n        )\n        self.assertCountEqual(\n            aggregate,\n            [\n                {\"publisher__name\": \"Apress\", \"avg_pages_per_publisher\": 267.0, \"total_pages\": 534},\n                {\"publisher__name\": \"Morgan Kaufmann\", \"avg_pages_per_publisher\": 1750.0, \"total_pages\": 3500},\n                {\"publisher__name\": \"Prentice Hall\", \"avg_pages_per_publisher\": 825.0, \"total_pages\": 1650},\n                {\"publisher__name\": \"Sams\", \"avg_pages_per_publisher\": 1000.0, \"total_pages\": 2000},\n            ],\n        )\n", "    def test_window_aggregate_pruning_with_non_referenced_annotation(self):\n        with CaptureQueriesContext(connection) as ctx:\n            Book.objects.annotate(\n                avg_publisher_pages=Window(Avg(\"pages\"), partition_by=F(\"publisher\"))\n            ).count()\n        sql = ctx.captured_queries[0][\"sql\"].lower()\n        self.assertEqual(sql.count(\"select\"), 1, \"No subquery wrapping required\")\n\n\n\n", "    def test_multiple_window_functions_aggregate(self):\n        with self.assertNumQueries(1) as ctx:\n            aggregate = Book.objects.annotate(\n                avg_pages=Avg(\"pages\"),\n                rank_by_pages=Rank()\n            ).values(\"name\", \"avg_pages\", \"rank_by_pages\").annotate(\n                sum_avg_pages=Sum(\"avg_pages\")\n            ).aggregate(\n                num_books=Count(\"id\"),\n            )\n        sql = ctx.captured_queries[0][\"sql\"].lower()\n        self.assertEqual(sql.count(\"select\"), 2, \"Window functions require subquery wrapping\")\n        self.assertEqual(aggregate, {\"num_books\": 2, \"sum_avg_pages\": 1100.0})\n\n\n\n"], "sample_563": ["def test_anchored_text_bbox_alignment():\n    fig, ax = plt.subplots()\n\n    text = AnchoredText(\"Test\", loc=\"center\", pad=0.2)\n    ax.add_artist(text)\n\n    # alignment='center' and padding\n    original_bbox = text.get_window_extent()\n    text.bbox_align = 'center'\n    text.draw(ax.figure.canvas.renderer)\n    new_bbox = text.get_window_extent()\n\n    assert new_bbox.width == original_bbox.width\n    assert new_bbox.height == original_bbox.height\n\n\n", "def test_anchoredtext_horizontal_alignment_multi():\n    fig, ax = plt.subplots()\n\n    text0 = AnchoredText(\"test\\ntest long text\", loc=\"center left\",\n                         pad=0.2, prop={\"ha\": \"left\"})\n    text1 = AnchoredText(\"test\\ntest long text\", loc=\"center\",\n                         pad=0.2, prop={\"ha\": \"center\"})\n    text2 = AnchoredText(\"test\\ntest long text\", loc=\"center right\",\n                         pad=0.2, prop={\"ha\": \"right\"})\n\n    ax.add_artist(text0)\n    ax.add_artist(text1)\n    ax.add_artist(text2)\n\n\n\n", "def test_offsetbox_padding():\n    fig, ax = plt.subplots()\n    da = DrawingArea(100, 50, clip=True)\n    bg = mpatches.Rectangle((0, 0), 100, 50,\n                            facecolor='#CCCCCC',\n                            edgecolor='None',\n                            linewidth=0)\n    da.add_artist(bg)\n    ab = AnnotationBbox(\n        da, (0.5, 0.5),\n        xycoords='axes fraction',\n        boxcoords='axes fraction',\n        pad=10,\n        box_alignment=(0.5, 0.5),\n    )\n    ax.add_artist(ab)\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n\n\n", "compilation error", "def test_offsetbox_arrowprops():\n    fig, ax = plt.subplots()\n\n    for direction in [\"left\", \"right\", \"top\", \"bottom\"]:\n        da = DrawingArea(100, 100)\n        ab = AnnotationBbox(\n            da, (0.5, 0.5), xybox=(0.2, 0.2),\n            xycoords='data',\n            boxcoords=\"axes fraction\",\n            arrowprops=dict(arrowstyle=\"->\",\n                           connectionstyle=(f\"{direction}\"),\n                           linestyle='--',\n                           color='red')\n        )\n        ab.set_bbox_to_anchor((0.5, 0.5))\n        ax.add_artist(ab)\n\n    fig.tight_layout()\n"], "sample_855": ["def test_dummy_estimator_predict_proba_multi_output( ):\n    X = [[0]] * 5\n    y = np.array([[2, 1],\n                  [2, 2],\n                  [1, 1],\n                  [1, 4],\n                  [1, 1]])\n\n    clf = DummyClassifier(strategy=\"prior\", random_state=0)\n    clf.fit(X, y)\n\n    y_pred = clf.predict_proba(X)\n    assert y_pred.shape == (5, 2)\n\n\n\n", "def test_dummy_classifier_with_sparse_target_for_most_frequent_and_prior():\n    X = [[0]] * 5  # ignored\n    y = sp.csc_matrix(np.array([[0, 1],\n                [1, 0],\n                [1, 1],\n                [1, 0],\n                [0, 1]]))\n\n    n_samples = len(X)\n    y_expected = np.hstack([np.ones((n_samples, 1)), np.zeros((n_samples, 1))])\n    for strategy in (\"most_frequent\", \"prior\"):\n        clf = DummyClassifier(strategy=strategy)\n        clf.fit(X, y)\n\n        y_pred = clf.predict(X)\n        assert sp.issparse(y_pred)\n        assert_array_equal(y_pred.toarray(), y_expected)\n", "def test_dummy_regressor_with_multioutput_and_sample_weight(strategy):\n    random_state = np.random.RandomState(seed=1)\n\n    X = np.array([[0]] * 5)  # shape (5, 1) \n    y = random_state.rand(5, 2) \n    sample_weight = random_state.rand(5)\n\n    est = DummyRegressor(strategy=strategy).fit(X, y, sample_weight)\n    y_pred = est.predict(X)\n\n    assert y_pred.shape == (5, 2)\n\n    if strategy == \"quantile\":\n        assert_array_almost_equal(\n            y_pred,\n            np.array([\n                [np.percentile(y[:, 0], q=est.quantile, axis=0),\n                 np.percentile(y[:, 1], q=est.quantile, axis=0)],\n                [np.percentile(y[:, 0], q=est.quantile, axis=0),\n                 np.percentile(y[:, 1], q=est.quantile, axis=0)],\n                [np.percentile(y[:, 0], q=est.quantile, axis=0),\n                 np.percentile(y[:, 1], q=est.quantile, axis=0)],\n                [np.percentile(y[:, 0], q=est.quantile, axis=0),\n                 np.percentile(y[:, 1], q=est.quantile, axis=0)],\n                [np.percentile(y[:, 0], q=est.quantile, axis=0),\n                 np.percentile(y[:, 1], q=est.quantile, axis=0)],\n            ]),\n            decimal=3\n        )\n    else:\n        assert_array_almost_equal(\n            y_pred,\n            np.array([\n                [np.mean(y[:, 0], weights=sample_weight),\n                 np.mean(y[:, 1], weights=sample_weight)],\n                [np.mean(y[:, 0], weights=sample", "def test_dummy_classifier_predict_proba_with_multioutput():\n    X = [[0], [0], [1]]\n    y = np.array([[2, 1], [1, 2], [1, 1]])\n    clf = DummyClassifier(strategy=\"most_frequent\")\n    clf.fit(X, y)\n    y_pred_proba = clf.predict_proba(X)\n\n    assert_array_almost_equal(\n        y_pred_proba,\n        np.array([[0. , 1. ],\n                  [1. , 0. ],\n                  [0.5, 0.5]]))\n\n\n\n", "def test_regressor_predict_proba():\n    X = [[0]] * 5\n    y = [1, 2, 1, 2, 1]\n    reg = DummyRegressor(strategy=strategy)\n    reg.fit(X, y)\n    y_proba = reg.predict_proba(X)\n    assert y_proba.shape == (5, 1)\n    if strategy == \"mean\":\n        assert_almost_equal(y_proba[:, 0], np.mean(y, axis=0))\n    elif strategy == \"median\":\n        assert_almost_equal(y_proba[:, 0], np.median(y, axis=0))\n    elif strategy == \"quantile\":\n        assert_almost_equal(y_proba[:, 0], np.percentile(y, 50, axis=0))\n\n\n\n"], "sample_616": ["def test_apply_ufunc_nan_handling(use_dask) -> None:\n    if use_dask:\n        if not has_dask:\n            pytest.skip(\"requires dask\")\n\n        return x + y\n\n    da_a = xr.DataArray([1, np.nan, 3], dims=\"x\")\n    da_b = xr.DataArray([4, 5, 6], dims=\"x\")\n    with raise_if_dask_computes():\n        actual = xr.apply_ufunc(nan_adder, da_a, da_b, vectorize=True)\n        assert (actual.data == [5, np.nan, 9]).all()\n", "compilation error", " def test_dot_with_scalar_data(use_dask) -> None:\n     if use_dask:\n         if not has_dask:\n             pytest.skip(\"requires dask\")\n     da_a = xr.DataArray(np.arange(6).reshape((2, 3)), dims=[\"a\", \"b\"])\n     da_b = xr.DataArray(4, dims=(\"scalar\",))\n     actual = xr.dot(da_a, da_b)\n     expected = xr.DataArray(np.array([12, 30]), dims=\"a\")\n     assert_identical(expected, actual)\n\n     da_a = xr.DataArray(x=np.arange(6).reshape((2, 3)))\n     da_b = xr.DataArray(y=4, dims=(\"scalar\",))\n     actual = xr.dot(da_a, da_b)\n     expected = xr.DataArray(np.array([12, 30]), dims=\"a\")\n     assert_identical(expected, actual)\n", "def test_cross_broadcast() -> None:\n    a = xr.DataArray([1, 2, 3], dims=\"x\")\n    b = xr.DataArray([[1, 1], [2, 2], [3, 3]], dims=(\"y\", \"x\"))\n    actual = xr.cross(a, b, dim=\"x\")\n    expected = xr.DataArray([[0, 0, 0], [0, 0, 0], [0, 0, 0]], dims=(\"y\",))\n    assert_identical(expected, actual)\n\n", "def test_cross_broadcast(use_dask: bool) -> None:\n    if use_dask:\n        if not has_dask:\n            pytest.skip(\"requires dask\")\n\n    a = xr.DataArray(np.arange(3 * 4).reshape(3, 4), dims=[\"x\", \"y\"])\n    b = xr.DataArray(np.arange(4), dims=\"z\")\n    actual = xr.cross(a, b, dim=\"y\")\n    expected = xr.zeros_like(a)\n    xr.testing.assert_allclose(actual, expected)\n\n"], "sample_562": ["def test_axline_transform(fig_test, fig_ref):\n    fig_test, ax_test = plt.subplots()\n    ax_test.set_xlim([0, 1])\n    ax_test.set_ylim([0, 1])\n\n    line = ax_test.axline((0.2, 0.2), (0.8, 0.8))\n    fig_ref, ax_ref = plt.subplots()\n    ax_ref.set_xlim([0, 1])\n    ax_ref.set_ylim([0, 1])\n\n    line_ref = ax_ref.axline((0.2, 0.2), (0.8, 0.8))\n\n    ax_test.draw_artist(line)\n    ax_ref.draw_artist(line_ref)\n", "def test_axline_transform(fig_test, fig_ref):\n    fig_test.subplots().plot([0, 1, 2], [0, 1, 2], marker=\"o\",\n                            transform=fig_test.transData)\n    ax_test = fig_test.add_subplot(111)\n    ax_test.axline((0.1, 0.1), (0.8, 0.8), transform=ax_test.transAxes)\n    fig_ref.add_subplot(111).plot([0, 1, 2], [0, 1, 2], marker=\"o\",\n                                  transform=fig_ref.transData)\n    ax_ref = fig_ref.add_subplot(111)\n    ax_ref.axline((0.1, 0.1), (0.8, 0.8), transform=ax_ref.transAxes) \n", "def test_axline_limits():\n    fig, ax = plt.subplots()\n    line1 = ax.axline((.1, .1), slope=0.6)\n    line2 = ax.axline((.1, .1), (.8, .4))\n    # Test axline behavior with varying  limits.\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    assert line1.get_transform().transform((line1.get_xy1(),)).x >= 0\n    assert line1.get_transform().transform((line1.get_xy1(),)).y >= 0\n    assert line1.get_transform().transform((line1.get_xy2(),)).x <= 1\n    assert line1.get_transform().transform((line1.get_xy2(),)).y <= 1\n\n    ax.set_xlim(0, 0.5)\n    ax.set_ylim(0.5, 1)\n    assert line1.get_transform().transform((line1.get_xy1(),)).x >= 0\n    assert line1.get_transform().transform((line1.get_xy1(),)).y >= 0.5\n    assert line1.get_transform().transform((line1.get_xy2(),)).x <= 0.5\n    assert line1.get_transform().transform((line1.get_xy2(),)).y <= 1\n    \n    ax.set_xlim(-0.5, 1.5)\n    ax.set_ylim(-0.5, 1.5)\n    assert line1.get_transform().transform((line1.get_xy1(),)).x >= -0.5\n    assert line1.get_transform().transform((line1.get_xy1(),)).y >= -0.5\n    assert line1.get_transform().transform((line1.get_xy2(),)).x <= 1.5\n    assert line1.get_transform().transform((line1.get_xy2(),)).y <= 1.5 \n\n", "def test_axline_set_color():\n    fig, ax = plt.subplots()\n    line = ax.axline((0, 0), (1, 1), color='red')\n    line.set_color('blue')\n    assert line.get_color() == 'blue'\n", "    def test_axline_slope_variants(fig_test, fig_ref):\n        fig_test, ax_test = plt.subplots()\n        fig_ref, ax_ref = plt.subplots()\n        x = np.linspace(0, 10)\n        for i in range(10):\n            ax_test.axline((x[i], 1), slope=i / 3, color=f'C{i}')\n            ax_ref.axline((x[i], 1), slope=i / 3, color=f'C{i}')\n\n"], "sample_613": ["    def test_resample_groupby_multiple_coords(self):\n        times = pd.date_range(\"2000-01-01\", freq=\"6H\", periods=10)\n        ys = np.arange(3)\n        data = np.tile(np.arange(5), (6, 3, 1))\n        array = DataArray(data, {\"time\": times, \"y\": ys}, (\"time\", \"y\",))\n\n        expected_data = np.array(\n            [[1, 2, 3], [1, 2, 3], [1, 2, 3]]\n        )\n        actual = array.groupby(\"y\").resample(time=\"1D\").mean().data\n        assert_identical(expected_data, actual)\n", "    def test_resample_with_coords_mismatch(self):\n        times = pd.date_range(\"2000-01-01\", freq=\"6H\", periods=10)\n        array = DataArray(np.ones(10), [(\"time\", times)])\n        other_times = pd.date_range(\"2000-01-01\", freq=\"12H\", periods=5)\n        other_array = DataArray(np.ones(5), [(\"time\", other_times)])\n\n        with pytest.raises(ValueError, match=r\"Resampling dimensions\"):\n            array.resample(time=other_times.index).mean()\n", "    def test_resample_groupby_and_mean(self):\n        times = pd.date_range(\"2000-01-01\", freq=\"6H\", periods=10)\n        ds = Dataset(\n            {\n                \"foo\": ([\"time\", \"x\", \"y\"], np.random.randn(10, 5, 3)),\n                \"bar\": (\"time\", np.random.randn(10), {\"meta\": \"data\"}),\n                \"time\": times,\n            }\n        )\n        actual = ds.groupby(\"time.month\").resample(time=\"1D\").mean()\n        expected = ds.groupby(\"time.month\").resample(time=\"1D\").apply(lambda x: x.mean(\"time\"))\n        assert_identical(actual, expected) \n", "    def test_resample_with_multiple_coords(self):\n        times = pd.date_range(\"2000-01-01\", freq=\"6H\", periods=10)\n        lat = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n        lon = np.array([10, 11, 12, 13, 14, 15, 16, 17, 18, 19])\n        data = np.random.randn(10, 10, 10)\n        da = DataArray(data, {\"time\": times, \"latitude\": lat, \"longitude\": lon}, (\"time\", \"latitude\", \"longitude\"))\n        actual = da.resample(time=\"1D\").mean()\n        expected_times = pd.date_range(\"2000-01-01\", freq=\"1D\", periods=3)\n        expected = DataArray(data.mean(axis=(1, 2)), {\"time\": expected_times, \"latitude\": lat, \"longitude\": lon}, (\"time\", \"latitude\", \"longitude\"))\n        assert_identical(expected, actual)\n", "    def test_resample_groupby_apply_func_kwargs(self):\n            return arg1.mean(\"time\") * kwargs[\"scale\"]\n\n        times = pd.date_range(\"2000-01-01\", freq=\"D\", periods=3)\n        ds = xr.Dataset({\"foo\": (\"time\", [1.0, 1.0, 1.0]), \"time\": times})\n        expected = xr.Dataset({\"foo\": (\"time\", [3.0, 3.0, 3.0]), \"time\": times})\n        actual = ds.resample(time=\"D\").groupby(\"time\").apply(func, scale=1.0)\n        assert_identical(expected, actual)\n\n"], "sample_886": ["def test_set_output_with_autowrap_off():\n    \"\"\"Check that auto-wrapping is disabled when `auto_wrap_output_keys` is None.\n\n    GH-25293\n    \"\"\"\n\n    class EstimatorWithAutoWrapOff(_SetOutputMixin, auto_wrap_output_keys=None):\n            return X\n\n    estimator = EstimatorWithAutoWrapOff()\n\n    # Ensure `transform` is not wrapped\n    assert not hasattr(estimator, \"set_output\")\n    assert estimator.transform(np.array([[1, 2, 3]])) is not None\n\n\n", "def test_set_output_pandas_with_sparse_input():\n    \"\"\"Check pandas output with sparse input.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n\n    X_sparse = csr_matrix([[1, 0, 3], [0, 0, 1]])\n    est = EstimatorWithSetOutputIndex().set_output(transform=\"pandas\")\n    est.fit(X_sparse)  # fit should not raise an error\n\n    X_trans = est.transform(X_sparse)\n    assert isinstance(X_trans, pd.DataFrame)\n    assert X_trans.sparse.base is X_sparse  # Check that the sparse structure is retained\n\n\n\n", "def test_auto_wrap_output_keys_default():\n    \"\"\"Auto-wrap should be enabled when `_sklearn_auto_wrap_output_keys` is not set.\"\"\"\n    est = EstimatorWithSetOutput()\n    has_wrapped = hasattr(est, \"transform\")\n    assert has_wrapped\n    assert hasattr(est, \"_sklearn_auto_wrap_output_keys\")\n    assert \"transform\" in est._sklearn_auto_wrap_output_keys\n\n\n\n", "def test_set_output_with_sparse_input():\n    \"\"\"Check that set_output raises an error when `transform` receives a sparse input.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n    from scipy.sparse import csr_matrix\n\n    X_sparse = csr_matrix([[1, 0, 3], [0, 0, 1]])\n    est = EstimatorWithSetOutput().set_output(transform=\"pandas\")\n    with pytest.raises(ValueError, match=\"Pandas output does not support sparse data\"):\n        est.transform(X_sparse)\n", "def test__wrap_method_output():\n    \"\"\"Check the behavior of _wrap_method_output.\"\"\"\n    class EstimatorWithWrappedTransform(_SetOutputMixin):\n            return X\n\n    est = EstimatorWithWrappedTransform()\n    wrapped_transform = est._wrap_method_output(\"transform\", \"transform\")\n    X = np.asarray([[1, 2, 3]])\n    result = wrapped_transform(est, X)\n    assert result is X\n\n\n"], "sample_192": ["    def test_all_valid_with_different_formset_prefixes(self):\n        data = {\n            'choices1-TOTAL_FORMS': '2',\n            'choices1-INITIAL_FORMS': '0',\n            'choices1-MIN_NUM_FORMS': '0',\n            'choices1-0-choice': 'Zero',\n            'choices1-0-votes': '0',\n            'choices1-1-choice': 'One',\n            'choices1-1-votes': '1',\n            'choices2-TOTAL_FORMS': '1',\n            'choices2-INITIAL_FORMS': '0',\n            'choices2-MIN_NUM_FORMS': '0',\n            'choices2-0-choice': 'Two',\n            'choices2-0-votes': '2',\n        }\n        ChoiceFormset1 = formset_factory(Choice, prefix='choices1')\n        ChoiceFormset2 = formset_factory(Choice, prefix='choices2')\n        formset1 = ChoiceFormset1(data)\n        formset2 = ChoiceFormset2(data)\n        self.assertIs(all_valid((formset1, formset2)), True)\n\n\n", "    def test_all_valid_with_extra_forms(self):\n        data = {\n            'choices-TOTAL_FORMS': '3',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '0',\n            'choices-1-choice': 'One',\n            'choices-1-votes': '1',\n            'choices-2-choice': 'Two',\n            'choices-2-votes': '2',\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset1 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        formset2 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertIs(all_valid((formset1, formset2)), True)\n        expected_errors = [{}, {}]\n        self.assertEqual(formset1._errors, expected_errors)\n        self.assertEqual(formset2._errors, expected_errors)\n", "    def test_all_valid_with_non_form_errors(self):\n        data = {\n            'choices-TOTAL_FORMS': '2',  # the number of forms rendered\n            'choices-INITIAL_FORMS': '0',  # the number of forms with initial data\n            'choices-MAX_NUM_FORMS': '2',  # max number of forms - should be ignored\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '0',\n            'choices-1-choice': 'One',\n            'choices-1-votes': '1',\n        }\n        ChoiceFormSet = formset_factory(Choice, extra=1, max_num=1, validate_max=True)\n        formset1 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        formset2 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertIs(all_valid((formset1, formset2)), False)\n        expected_errors = [{'choice': ['You may only specify a choice once.']}, {'choice': ['You may only specify a choice once.']}]\n        self.assertEqual(formset1._errors, expected_errors)\n        self.assertEqual(formset2._errors, expected_errors)\n\n", "    def test_all_valid_with_different_formsets(self):\n        data1 = {\n            'choices-TOTAL_FORMS': '1',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '0',\n        }\n        data2 = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-0-choice': 'One',\n            'choices-0-votes': '1',\n            'choices-1-choice': 'Two',\n            'choices-1-votes': '2',\n        }\n        formset1 = formset_factory(Choice)(data1, auto_id=False, prefix='choices')\n        formset2 = formset_factory(Choice)(data2, auto_id=False, prefix='choices')\n        self.assertIs(all_valid((formset1, formset2)), True)\n\n", "    def test_empty_formset_is_valid(self):\n        \"\"\"An empty formset is always valid.\"\"\"\n        ChoiceFormSet = formset_factory(Choice, extra=0)\n        formset = ChoiceFormSet(data={'form-TOTAL_FORMS': '0'})\n        self.assertTrue(formset.is_valid())\n"], "sample_996": ["compilation error", "def test_Product_as_Sum():\n    assert Product(1 + 1/k, (k, 1, oo)).as_sum() == Sum(1/k + 1, (k, 1, oo))\n    assert Product(1 - 1/k, (k, 1, oo)).as_sum() == Sum( -1/k + 1, (k, 1, oo))\n    assert Product(2*k, (k, 1, oo)).as_sum() == Sum(2*k, (k, 1, oo))\n    assert Product(k**2, (k, 1, oo)).as_sum() == Sum(k**2, (k, 1, oo))\n", "compilation error", "def test_issue_15224():\n    f = Function('f')\n    n = Symbol('n', integer=True)\n    p = Product(f(n), (n, 1, oo))\n    assert p.rewrite(Sum).doit() == Sum(log(f(n)), (n, 1, oo))\n", "def test_special_product_with_limits():\n    from sympy.abc import k, n\n    a = Symbol('a')\n    assert product(a**(k**2/(k**2 + 1)), (k, 1, n)).doit() == \\\n        a**(sum(k**2/(k**2 + 1) for k in range(1, n + 1)))\n"], "sample_800": ["def test_check_estimators_unfitted_raises_correct_error(self):\n    msg = \"estimator is not fitted\"\n    with ignore_warnings(category=(FutureWarning, DeprecationWarning)):\n        assert_raises_regex(CorrectNotFittedError, msg,\n                            check_estimators_unfitted,\n                            \"estimator\",\n                            CorrectNotFittedErrorClassifier())\n\n", "def test_check_estimator_categorical_features():\n    \n    class MyEstimator(BaseEstimator):\n            # Simulate a classifier that handles categorical features\n            self.feature_names_ = X.columns \n            return self\n\n            return np.zeros(X.shape[0])\n\n    check_estimator(MyEstimator()) \n", "def test_check_estimator_sparse_input_handling():\n    # check for sparse matrix input handling on a proper estimator\n    est = RandomForestClassifier()\n    check_estimator(est, sparse_input=True)\n", "def test_check_estimator_non_fitted_methods():\n    msg = \"estimator is not fitted\"\n    estimator = NoCheckinPredict()\n\n    # Check that we raise an exception for methods that should only\n    # be called after fitting\n    with ignore_warnings(category=(FutureWarning, DeprecationWarning)):\n      assert_raises_regex(AssertionError, msg, estimator.predict, np.array([[1, 2]]))\n\n\n", "def test_check_estimator_missing_required_params():\n    class MyEstimator(BaseEstimator):\n        _required_parameters = [\"param1\", \"param2\"]\n\n            self.param1 = param1\n            self.param2 = param2\n\n    assert_raises_regex(ValueError, r\"Estimator MyEstimator requires\",\n                        check_estimator, MyEstimator, param1=1)\n    assert_raises_regex(ValueError, r\"Estimator MyEstimator requires\",\n                        check_estimator, MyEstimator, param2=2)\n    assert_raises_regex(ValueError,\n                        r\"Estimator MyEstimator requires\",\n                        check_estimator, MyEstimator, param1=1, param2=\"string\")\n\n\n\n"], "sample_745": ["def test_sparse_matrix_input():\n    X_sparse = sparse.csr_matrix([[1, 2], [3, 4]])\n    transformer = FunctionTransformer(func=np.sum, accept_sparse=True)\n    transformed = transformer.transform(X_sparse)\n    assert isinstance(transformed, sparse.csr_matrix)\n\n    transformer = FunctionTransformer(func=lambda x: x * 2,\n                                     accept_sparse=True)\n    transformed = transformer.transform(X_sparse)\n    assert isinstance(transformed, sparse.csr_matrix)\n", "def test_sparse_matrix():\n    X = sparse.csr_matrix([[1, 2], [3, 4]])\n\n    transformer = FunctionTransformer(\n        func=lambda X: X.toarray(),\n        accept_sparse=True,\n        validate=True,\n    )\n\n    transformed_dense = transformer.transform(X)\n\n    assert_allclose_dense_sparse(transformed_dense, X.toarray())\n", "def test_sparse_input():\n    X = sparse.csr_matrix((np.ones(10), (np.arange(10), np.arange(10))),\n                          shape=(5, 2))\n    transformer = FunctionTransformer(np.log1p, accept_sparse=True)\n    assert_allclose_dense_sparse(transformer.transform(X),\n                                np.log1p(X.toarray()))\n    transformer = FunctionTransformer(np.log1p, accept_sparse=False)\n    with pytest.raises(ValueError):\n        transformer.transform(X)\n", "def test_function_transformer_sparse_matrix():\n    X = sparse.csr_matrix([[1, 2], [3, 4]])\n    transformer = FunctionTransformer(func=lambda X: X * 2, accept_sparse=True)\n    transformed_X = transformer.transform(X)\n    assert isinstance(transformed_X, sparse.csr_matrix)\n    assert_array_equal(transformed_X.toarray(), X.toarray() * 2)\n", "def test_sparse_matrix():\n    X_sparse = sparse.csr_matrix((np.ones(10),\n                                  (np.arange(10), np.arange(10))),\n                                 shape=(5, 2))\n\n    trans = FunctionTransformer(\n        func=lambda X: X * 2, accept_sparse=True, validate=False)\n    Xt = trans.transform(X_sparse)\n\n    assert_allclose_dense_sparse(Xt * 2, trans.transform(Xt))\n"], "sample_1184": ["def test_conjugate_gauss_beams():\n    from sympy.physics.optics import conjugate_gauss_beams\n    wavelen, w_i, w_o, f = symbols('wavelen w_i w_o f')\n\n    # Test with different waist combinations\n    assert conjugate_gauss_beams(wavelen, w_i, w_o, f=f)[2] == f\n    assert conjugate_gauss_beams(wavelen, w_i, w_o, f=f)[0] \\\n        == f*(1 - sqrt(w_i**2/w_o**2 - pi**2*w_i**4/(f**2*l**2)))\n    assert factor(conjugate_gauss_beams(wavelen, w_i, w_o, f=f)[1]) == \\\n        f*w_o**2*(w_i**2/w_o**2 - sqrt(w_i**2/w_o**2 -\n              pi**2*w_i**4/(f**2*l**2)))/w_i**2\n\n    # Test with different wavelength values\n    l = 1 \n    w_i = 1e-3\n    w_o = 2e-3\n    f = 0.5\n    assert conjugate_gauss_beams(1e-6, w_i, w_o, f=f)[2] == f\n    assert conjugate_gauss_beams(1e-4, w_i, w_o, f=f)[2] == f\n\n    # Test with different focal lengths\n    l = 1\n    w_i = 1e-3\n    w_o = 2e-3\n    f = 0.5\n    assert conjugate_gauss_beams(wavelen, w_i, w_o, f=f)[2] == f\n    assert conjugate_gauss_beams(wavelen, w_i, w_o, f=1)[2] != f \n", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_923": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_684": ["    def test_repr_with_nested_exceptions(self) -> None:\n        try:\n            raise ValueError(\"Outer exception\")\n        except ValueError as e:\n            raise TypeError(\"Nested exception\", cause=e) from None\n        with pytest.raises(TypeError) as excinfo:\n            pass\n        repr_chain = excinfo.value.getrepr()\n        assert isinstance(repr_chain, ExceptionChainRepr)\n        assert len(repr_chain.chain) == 2\n        assert \"Outer exception\" in repr_chain.chain[0][2]\n        assert \"Nested exception\" in repr_chain.chain[1][2]\n", "    def test_reprcrash_reprtraceback_reprlocals(self):\n        try:\n            raise ValueError(\"This is a test error\")\n        except ValueError:\n            exci = ExceptionInfo.from_current()\n            repr_exci = exci.getrepr()\n            assert isinstance(repr_exci, ExceptionChainRepr)\n            assert repr_exci.chain\n            assert repr_exci.reprtraceback\n            assert repr_exci.reprcrash is not None\n\n", "    def test_reprcrash_with_zero_lineno(self) -> None:\n        try:\n            raise ValueError()\n        except ValueError:\n            excinfo1 = ExceptionInfo.from_current()\n            # Set the lineno to 0 to mimic the behavior of some exceptions\n            # where lineno is not set properly.\n            excinfo1.traceback[0].lineno = 0\n            repr1 = excinfo1.getrepr()\n            assert repr1 is not None\n            assert isinstance(repr1, ExceptionChainRepr)\n            assert repr1.chain[0][2] is None\n", "    def test_repr_traceback_short_no_locals(self) -> None:\n        exinfo = ExceptionInfo.from_current()\n        rep = ReprTraceback(exinfo.traceback, style='short')\n        assert rep.toterminal.call_count == 1\n        assert len(rep.reprentries) == 1\n\n\n", "    def test_repr_traceback_entry_with_locals_and_file_loc(self) -> None:\n        try:\n            if False:\n                pass\n            else:\n                assert False\n        except AssertionError:\n            exci = ExceptionInfo.from_current()\n        entry = exci.traceback[0]\n        repr_entry = ReprTracebackEntry(\n            entry,\n            None,\n            None,\n        )\n\n        repr_entry._locals = {\n            \"test_var\": \"test_value\",\n        }\n        repr_entry.reprfileloc = ReprFileLocation(\n            path=\"test_test_file.py\", lineno=12, message=\"AssertionError\"\n        )\n\n\n"], "sample_553": ["def test_invalid_frame_format(tmpdir, writer, frame_format):\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3])\n\n        ax.lines[0].set_ydata(i * np.ones(3))\n        return ax.lines[0],\n\n    anim = animation.FuncAnimation(fig, animate, frames=10)\n\n    with pytest.raises(ValueError):\n        anim.save(\n            str(tmpdir.join(\"test.{}\".format(frame_format))),\n            writer=writer,\n            frame_format=frame_format,\n        )\n\n\n\n", "def test_animation_from_generator(tmpdir):\n    fig, ax = plt.subplots()\n\n        yield {'x': np.linspace(0, 10, 100), 'y': np.sin(np.linspace(0, 10, 100))}\n        yield {'x': np.linspace(0, 10, 100), 'y': np.cos(np.linspace(0, 10, 100))}\n\n    anim = animation.FuncAnimation(\n        fig,\n        lambda frame: ax.plot([], [], data=frame['x'], data=frame['y'])  # update frame\n        ,\n        frames=frame_generator,\n        cache_frame_data=False\n    )\n    with tmpdir.as_cwd():\n        anim.save(\"test.gif\") \n", "def test_save_animation_with_invalid_output_format(tmpdir, writer):\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3])\n    anim = animation.FuncAnimation(fig, lambda i: ax.set_ydata([i, i**2, i**3]),\n                                  frames=10, blit=False, repeat=False)\n\n    with pytest.raises(ValueError, match=r\"Unsupported format .*\"):\n        anim.save(str(tmpdir / \"test.xyz\"), writer=writer) \n", "def test_movie_writer_invalid_format(tmpdir, writer):\n    with tmpdir.as_cwd():\n        if writer == 'ffmpeg':\n            invalid_format = 'bogus'\n        else:\n            invalid_format = 'bogus.blah'\n        anim = animation.FuncAnimation(plt.figure(), lambda i: None, frames=1)\n        with pytest.raises(ValueError, match=f\"Unsupported format: {invalid_format}\"):\n            anim.save(f'test.{invalid_format}', writer=writer)\n\n\n\n", "def test_animation_blitting_with_updating_artists(tmpdir):\n\n    fig, ax = plt.subplots()\n\n    # Define a function to update an Artist object\n        artist.set_data(np.sin(i * 0.5), np.cos(i * 0.5))\n\n        line, = ax.plot([], [])\n        return line\n\n        update_artist(line, i)\n        return line,\n\n    # Create the animation with blitting enabled\n    anim = animation.FuncAnimation(fig, animate, init_func=init,\n                                  frames=100, blit=True, repeat=False)\n\n    with tmpdir.as_cwd():\n        anim.save('test_blitting.gif', writer='pillow')\n"], "sample_450": ["    def test_custom_admin_log_entries(self):\n        from django.contrib.admin.models import LogEntry\n\n        LogEntry.objects.log_action(\n            self.user.pk,\n            ContentType.objects.get_for_model(Car).pk,\n            c1.pk,\n            \"Car created\",\n            ADDITION,\n            change_message=\"Car created message\",\n        )\n\n        response = self.client.get(reverse(\"custom_admin:index\"))\n        self.assertContains(response, \"Car created\")\n        self.assertContains(response, \"Car created message\")\n\n\n\n\n", "    def test_get_admin_log_template_tag(self):\n        \"\"\"\n        Test the get_admin_log template tag functionality.\n        \"\"\"\n        with self.subTest(\"empty log\"):\n            context = {\"log_entries\": LogEntry.objects.none()}\n            result = get_admin_log(context, \"\", \"\", \"\")\n            self.assertEqual(result, \"\")\n\n        with self.subTest(\"filter by user\"):\n            context = {\"log_entries\": LogEntry.objects.all(), \"user\": self.user}\n            result = get_admin_log(context, \"10\", \"admin_log\", str(self.user.pk))\n            self.assertEqual(result, \"\")\n            self.assertEqual(context[\"admin_log\"], LogEntry.objects.filter(user__pk=self.user.pk)[:10])\n\n        with self.subTest(\"no user filter\"):\n            context = {\"log_entries\": LogEntry.objects.all()}\n            result = get_admin_log(context, \"10\", \"admin_log\")\n            self.assertEqual(result, \"\")\n            self.assertEqual(context[\"admin_log\"], LogEntry.objects.all()[:10])\n\n\n", "    def test_get_admin_log_template_tag(self):\n        context = {\"log_entries\": LogEntry.objects.all()}\n        # Add a few log entries for different models\n        LogEntry.objects.log_action(\n            self.user.pk,\n            ContentType.objects.get_for_model(Article).pk,\n            self.a1.pk,\n            \"Article changed\",\n            CHANGE,\n            change_message=\"Article changed message\",\n        )\n\n        LogEntry.objects.log_action(\n            self.user.pk,\n            ContentType.objects.get_for_model(Car).pk,\n            c1.pk,\n            \"Car created\",\n            ADDITION,\n            change_message=\"Car created message\",\n        )\n\n        # Render the template tag within a test context\n        result = register.get_admin_log(\n            tokens=[\"2\", \"test_log\", \"5\", \"user\"]\n        ).render(context)\n\n        # Verify the result contains the expected log entries\n        self.assertEqual(result, \"\")\n        self.assertIn(\"<ul>\", self.html_content)\n        self.assertIn(\"Article changed\", self.html_content)\n        self.assertIn(\"Car created\", self.html_content)\n\n\n\n", "    def test_get_admin_log_template_tag(self):\n        context = {\"log_entries\": LogEntry.objects.all(), \"user\": self.user}\n        with self.subTest(\"no user\"):\n            rendered_output = template.Template(\n                \"{{ get_admin_log limit=10 as log_entries }}\"\n            ).render(context)\n            self.assertEqual(rendered_output, \"\")\n\n        with self.subTest(\"with user\"):\n            rendered_output = template.Template(\n                \"{{ get_admin_log 10 as log_entries for_user 23 }}\"\n            ).render(context)\n            self.assertEqual(rendered_output, \"\")\n\n\n", "    def test_logentry_ordering(self):\n        LogEntry.objects.log_action(\n            self.user.pk,\n            ContentType.objects.get_for_model(Article).pk,\n            self.a1.pk,\n            \"Article changed\",\n            CHANGE,\n            change_message=\"Article changed message\",\n            timestamp=datetime(2023, 3, 1, 10, 0),\n        )\n        LogEntry.objects.log_action(\n            self.user.pk,\n            ContentType.objects.get_for_model(Article).pk,\n            self.a1.pk,\n            \"Article changed again\",\n            CHANGE,\n            change_message=\"Article changed message again\",\n            timestamp=datetime(2023, 3, 1, 12, 0),\n        )\n        latest_log_entry = LogEntry.objects.latest(\"action_time\")\n        older_log_entry = LogEntry.objects.order_by(\"action_time\").first()\n        self.assertEqual(latest_log_entry.get_action_flag_display(), \"Changed\")\n        self.assertEqual(older_log_entry.get_action_flag_display(), \"Changed\")\n        self.assertLess(older_log_entry.action_time, latest_log_entry.action_time)\n"], "sample_1094": ["compilation error", "compilation error", "def test_has_no_children():\n    assert Basic().has_no_children()\n    assert b1.has_no_children()\n\n    assert b21.has_no_children() is False\n\n    class Custom(Basic):\n        pass\n\n    assert Custom().has_no_children()\n    assert Custom(Basic()).has_no_children() is False\n", "def test_rewrite_with_undeclared_functions():\n    from sympy.core.function import UndefinedFunction as UndefFunc\n    x, y = symbols('x y')\n    f = UndefFunc('f')\n    g = UndefFunc('g')\n\n    # Test rewriting a term containing an UndefFunc\n    expr = f(x) + g(y)\n    result = expr.rewrite(f, lambda x: x**2)\n    assert result == x**2 + g(y)\n\n    # Test rewriting with a rule that is a function of UndefFunc\n    result = expr.rewrite(g, lambda x: x**3)\n    assert result ==  x**2 + y**3\n\n", "compilation error"], "sample_41": ["def test_composite_unit_equality():\n    \"\"\"See #3849\"\"\"\n    unit1 = u.m * u.s  \n    unit2 = u.Unit('m*s')\n    unit3 = u.m * u.s\n    assert unit1 == unit2\n    assert unit1 == unit3\n    assert unit1 != u.kg\n\n", "def test_unit_conversion_with_complex_units():\n    comp_unit = u.m * u.s * u.deg\n\n    result = comp_unit.to(u.cm * u.s * u.rad)\n\n    assert isinstance(result, u.CompositeUnit)\n    assert result.bases == [u.cm, u.s, u.rad]\n    assert_allclose(result.scale, 1.0)\n", "def test_composite_unit_repr():\n    unit = u.Unit(\"10*m/s\")\n    assert repr(unit) == \"10 * m / s\"\n", "def test_validate_power_with_zeros():\n    with pytest.raises(ValueError):\n        utils.validate_power(0) \n\n    assert utils.validate_power(0.0) == Fraction(0)\n\n    assert utils.validate_power(1.0) == Fraction(1)\n    assert utils.validate_power(Fraction(1, 1)) == Fraction(1)\n\n", "def test_compose_with_fractional_powers_and_empty_units():\n    \"\"\"See #8260: Ensure fractional powers work when composing with empty units.\"\"\"\n    x = u.m ** Fraction(1, 2)\n    result = x.compose(units=[u.m * u.s, u.Unit('')])  \n    assert len(result) == 2\n    assert isinstance(result[0].powers[0], Fraction)\n    assert isinstance(result[1].powers[0], Fraction)\n"], "sample_486": ["    def test_inlineformset_factory_nulls_default_pks_unique_parent_field(self):\n        \"\"\"\n        #24958 - Variant of test_inlineformset_factory_nulls_default_pks for\n        the case of a parent object with a unique field and a child object with\n        an AutoField primary key.\n        \"\"\"\n        class ChildWithUniqueParentField(models.Model):\n            name = models.CharField(max_length=100)\n            parent_unique_field = models.CharField(\n                max_length=100, unique=True\n            )\n            parent = models.ForeignKey(\n                AutoPKParent, on_delete=models.CASCADE\n            )\n\n        FormSet = inlineformset_factory(\n            AutoPKParent, ChildWithUniqueParentField, fields=\"__all__\"\n        )\n        formset = FormSet()\n        self.assertIsNone(formset.forms[0].fields[\"parent\"].initial)\n", "    def test_inlineformset_factory_nulls_default_pks_auto_parent_auto_child(self):\n        \"\"\"\n        #24958 - Variant of test_inlineformset_factory_nulls_default_pks for\n        the case of both parent and child objects with AutoField primary keys.\n        \"\"\"\n        FormSet = inlineformset_factory(AutoPKParent, AutoPKChildOfUUIDPKParent, fields=\"__all__\")\n        formset = FormSet()\n        self.assertIsNone(formset.forms[0].fields[\"parent\"].initial)\n", "    def test_inlineformset_factory_with_fk_and_unique_field(self):\n        \"\"\"\n        Tests that an inline formset with a foreign key that\n        references a unique field in the parent model works correctly.\n        \"\"\"\n\n        FormSet = inlineformset_factory(\n            ParentWithUUIDAlternateKey, ChildRelatedViaAK, fields=\"__all__\"\n        )\n        formset = FormSet()\n\n        first_form = formset.forms[0]\n        # The parent's 'uuid' field should be the default initial value\n        # for the child's 'parent' field\n        self.assertEqual(first_form.fields['parent'].initial, formset.instance.uuid)\n\n        # Verify the FK isn't automatically populated when the form is\n        # submitted.\n        formset_data = {\n            \"childrelatedviaak_set-TOTAL_FORMS\": 1,\n            \"childrelatedviaak_set-INITIAL_FORMS\": 0,\n            \"childrelatedviaak_set-MAX_NUM_FORMS\": \"\",\n            \"childrelatedviaak_set-0-name\": \"Test\",\n        }\n        formset = FormSet(formset_data)\n        formset.save()\n        self.assertEqual(formset.instance.uuid, formset.saved_forms[0].instance.parent_id)\n\n\n\n", "    def test_inlineformset_factory_pk_uniqueness(self):\n        \"\"\"\n        #25961 - If the foreign key to the parent model is unique, the\n        `max_num` parameter should be enforced.\n        \n        This tests the case where the foreign key relationship between the\n        parent and child models is set to be unique.\n        \"\"\"\n        FormSet = inlineformset_factory(\n            AutoPKParent, ChildRelatedViaAK, fields=\"__all__\",\n            max_num=1\n        )\n        formset = FormSet()\n        formset.data = {\n            \"childrelatedviaak_set-TOTAL_FORMS\": 2,\n            \"childrelatedviaak_set-INITIAL_FORMS\": 0,\n            \"childrelatedviaak_set-MAX_NUM_FORMS\": \"\",\n            \"childrelatedviaak_set-0-name\": \"Foo\",\n            \"childrelatedviaak_set-1-name\": \"Bar\",\n        }\n        self.assertFalse(formset.is_valid())\n        self.assertEqual(\n            formset.errors[\"childrelatedviaak_set\"].as_text(),\n            \"You may only add one item.\"\n        )\n", "    def test_inlineformset_factory_nulls_default_pks_child_auto_parent_edit_only(self):\n        \"\"\"\n        #24958 - Variant of test_inlineformset_factory_nulls_default_pks for\n        the case of a parent object with an AutoField primary key and a child\n        object with an AutoField primary key, when edit_only is True, we should\n        not be filling in the parent field.\n        \"\"\"\n        FormSet = inlineformset_factory(\n            AutoPKParent, AutoPKChildOfUUIDPKParent, fields=\"__all__\", edit_only=True\n        )\n        formset = FormSet()\n        self.assertIsNone(formset.forms[0].fields[\"parent\"].initial)\n\n"], "sample_585": ["def test_da_groupby_apply_datetime_with_nan():\n    times = pd.date_range('2020-01-01', periods=5, freq='D',\n                          dtype='datetime64[ns]')\n    data = xr.DataArray([1, np.nan, 3, 4, 5], \n                       coords={'time': times}, dims=['time'])\n\n        return ds.fillna(0).sum(dim='time')\n\n    result = data.groupby('time.month').apply(func)\n\n    expected = xr.DataArray([0.0, 12], coords={'time.month': [1, 2]},\n                           dims=['time.month'])\n    assert_identical(result, expected)\n\n\n\n", "def test_da_groupby_invalid_quantile():\n    array = xr.DataArray([1, 2, 3], [('x', [1, 2, 3])])\n    with pytest.raises(ValueError):\n        array.groupby('x').quantile(1.1)\n    with pytest.raises(ValueError):\n        array.groupby('x').quantile(-1)\n\n", "def test_da_groupby_fillna():\n    array = xr.DataArray([1, np.nan, 3], [('x', [1, 2, 3])])\n\n    # Test fillna with scalar value\n    expected = xr.DataArray([1, 0, 3], [('x', [1, 2, 3])])\n    actual = array.groupby('x').fillna(0)\n    assert_identical(expected, actual)\n\n    # Test fillna with another array\n    fill_array = xr.DataArray([5, 6], [('x', [1, 2])])\n    expected = xr.DataArray([1, 6, 3], [('x', [1, 2, 3])])\n    actual = array.groupby('x').fillna(fill_array)\n    assert_identical(expected, actual)\n\n    # Test fillna with a method\n        return -1\n    expected = xr.DataArray([-1, -1, 3], [('x', [1, 2, 3])])\n    actual = array.groupby('x').fillna(fill_method)\n    assert_identical(expected, actual)\n", "def test_da_groupby_apply_with_kwargs():\n    array = xr.DataArray([1, 1, 1, 2, 2, 2], [('x', [1, 1, 1, 2, 2, 2])])\n\n        return arg1 * kwarg1\n\n    expected = xr.DataArray([0, 0, 0, 4, 4, 4], [('x', [1, 1, 1, 2, 2, 2])])\n    actual = array.groupby('x').apply(func, kwarg1=0)\n    assert_identical(expected, actual)\n\n    expected = xr.DataArray([2, 2, 2, 8, 8, 8], [('x', [1, 1, 1, 2, 2, 2])])\n    actual = array.groupby('x').apply(func, kwarg1=2)\n    assert_identical(expected, actual)\n\n    expected = xr.DataArray([1, 1, 1, 2, 2, 2], [('x', [1, 1, 1, 2, 2, 2])])\n    actual = array.groupby('x').apply(func, kwarg1=1, another_kwarg=True)\n    assert_identical(expected, actual)\n\n\n", "def test_da_groupby_fill_na():\n    # test .fillna() with groupby\n    array = xr.DataArray([1, np.nan, 3], [('x', [1, 1, 2])])\n    expected = xr.DataArray([1, 2, 3], [('x', [1, 1, 2])])\n    actual = array.groupby('x').fillna(method='ffill')\n    assert_identical(expected, actual)\n\n\n\n"], "sample_1151": ["compilation error", "def test_issue_21034_continue():\n    e = -I*log((re(asin(5)) + I*im(asin(5)))/sqrt(re(asin(5))**2 + im(asin(5))**2))/pi\n    assert e.eval() == e.round(2) \n", "def test_issue_21034():\n    e = -I*log((re(asin(5)) + I*im(asin(5)))/sqrt(re(asin(5))**2 + im(asin(5))**2))/pi\n    assert e.round(2)\n", "compilation error", "def test_issue_21373():\n    from sympy import Symbol, exp, log, cos\n    x = Symbol('x')\n    assert (exp(log(x)) == x).subs(x, 2)\n\n\n\n\n"], "sample_747": ["def test_power_transformer_nan_handling():\n    X = np.array([[1, 2, np.nan], [4, 5, 6]])\n    for standardize in [True, False]:\n        pt = PowerTransformer(method='box-cox', standardize=standardize)\n        with pytest.raises(ValueError):\n            pt.fit_transform(X)\n\n        pt = PowerTransformer(method='yeo-johnson', standardize=standardize)\n        with pytest.raises(ValueError):\n            pt.fit_transform(X)\n\n\n\n", "def test_power_transformer_yule_box_cox_with_zeros():\n    X = np.array([[0, 2, 50, 75, 100],\n                  [2, 4, 6, 8, 10],\n                  [2.6, 4.1, 2.3, 9.5, 0.1]])\n    for standardize in [True, False]:\n        pt = PowerTransformer(method='yule-box-cox', standardize=standardize)\n        X_trans = pt.fit_transform(X)\n        X_trans_func = power_transform(X, method='yule-box-cox', standardize=standardize)\n        assert_array_almost_equal(X_trans, X_trans_func)\n        assert_array_almost_equal(pt.transform(X), X_trans)\n", "def test_power_transformer_missing_data():\n    pt = PowerTransformer(method='box-cox')\n    X = np.abs(X_2d)\n    X[0, 1] = np.nan\n    X[1, 0] = np.nan\n\n    # Check if fit handles NaN values\n    with np.errstate(divide='ignore'):\n        pt.fit(X)\n    assert_array_almost_equal(pt.lambdas_, np.nan)\n\n    # Check if transform handles NaN values\n    with np.errstate(divide='ignore'):\n        X_trans = pt.transform(X)\n    assert_raises(ValueError, pt.inverse_transform, X_trans)\n\n\n", "def test_power_transformer_nan_handling():\n    pt = PowerTransformer(method='box-cox')\n    X = np.abs(X_2d)\n    X_with_nan = np.copy(X)\n    np.nan_to_num(X_with_nan, nan=0)  \n    X_with_nan[0, 0] = np.nan\n    \n    with np.errstate(divide='ignore', invalid='ignore'):\n        msg = \"Input contains NaN, but the method doesn't handle NaN\"\n        assert_raises_regex(ValueError, msg, pt.fit, X_with_nan)\n        assert_raises_regex(ValueError, msg, pt.transform, X_with_nan)\n\n    X_with_nan = np.copy(X)\n    with np.errstate(divide='ignore', invalid='ignore'):\n        X_with_nan[0, 0] = np.nan\n        pt.fit(X_with_nan)\n    assert_raises_regex(ValueError,  msg, pt.transform, X_with_nan)\n", "def test_power_transformer_nan_values():\n    pt = PowerTransformer(method='box-cox')\n\n    for method in ['box-cox', 'yeo-johnson']:\n        pt = PowerTransformer(method=method)\n        X = np.abs(X_2d).astype(float)\n        X[0, 0] = np.nan\n        assert_raises(ValueError, pt.fit, X)\n\n        X[0, 1] = np.nan\n        assert_raises(ValueError, pt.fit, X)\n        \n\n        X = np.abs(X_2d).astype(float)\n        X[0, 0] = np.nan\n        assert_raises(ValueError, pt.transform, X)\n\n        X[0, 1] = np.nan\n        assert_raises(ValueError, pt.transform, X)\n\n        # Test with NaN in the inverse transform\n        X_trans = pt.fit_transform(X_2d)\n        X_trans[0, 0] = np.nan\n        assert_raises(ValueError, pt.inverse_transform, X_trans)\n"], "sample_739": ["def test_label_binarize_sparse_output_multiclass():\n    y = np.array([[0, 1, 2], [1, 2, 0], [0, 2, 1]])\n    classes = [0, 1, 2]\n    sparse_output = True\n    expected = csr_matrix([[0, 1, 0], [1, 2, 0], [0, 2, 1]])\n    binarized = label_binarize(y, classes, sparse_output=sparse_output)\n    assert_array_equal(toarray(binarized), toarray(expected))\n\n    inverse_binarized = _inverse_binarize_multiclass(binarized, classes=classes)\n    assert_array_equal(toarray(inverse_binarized), toarray(y))\n\n\n\n", "def test_label_binarize_sparse_output_multiclass():\n    y = [0, 1, 2]\n    classes = [0, 1, 2]\n    expected = csr_matrix(np.array([[1, 0, 0],\n                                    [0, 1, 0],\n                                    [0, 0, 1]]))\n\n    binarized = label_binarize(y, classes, sparse_output=True)\n    assert_array_equal(binarized.toarray(), expected.toarray())\n\n    binarized = label_binarize(y, classes, sparse_output=True,\n                              output_type=\"csr\")\n    assert_array_equal(binarized.toarray(), expected.toarray())\n\n    binarized = label_binarize(y, classes, sparse_output=True,\n                              output_type=\"csc\")\n    assert_array_equal(binarized.toarray(), expected.toarray())\n\n    binarized = label_binarize(y, classes, sparse_output=True,\n                              output_type=\"coo\")\n    assert_array_equal(binarized.toarray(), expected.toarray())\n\n", "compilation error", "def test_label_binarizer_empty_classes():\n    mlb = MultiLabelBinarizer()\n    with assert_raises(ValueError):\n        mlb.fit([])\n    with assert_raises(ValueError):\n        mlb.transform([])\n", "def test_label_binarizer_errors():\n    # Check that invalid arguments yield ValueError\n    lb = LabelBinarizer()\n    assert_raises(ValueError, lb.transform, [])\n    assert_raises(ValueError, lb.inverse_transform, [])\n\n    # Fail on unseen labels\n    le = LabelEncoder()\n    le.fit([1, 2, 3, -1, 1])\n    msg = \"contains previously unseen labels\"\n    assert_raise_message(ValueError, msg, le.inverse_transform, [-2])\n    assert_raise_message(ValueError, msg, le.inverse_transform, [-2, -3, -4])\n\n    # Fail on inverse_transform(\"\")\n    msg = \"bad input shape ()\"\n    assert_raise_message(ValueError, msg, le.inverse_transform, \"\")\n"], "sample_966": ["def test_warn_missing_reference_with_explicit_refid(app, status, warning):\n    app.build()\n    assert 'index.rst:6: WARNING: undefined label: no-label' in warning.getvalue()\n    assert ('index.rst:7: WARNING: Failed to create a cross reference. A title or caption not found: existing-label'\n            in warning.getvalue())\n", "def test_ref_to_undefined_object(app):\n    text = (\".. py:class:: MyClass\\n\"\n            \"   :members:\\n\"\n            \"   :undoc-members:\\n\"\n            \"\\n\"\n            \"   .. py:method:: my_method()\\n\"\n            \"\\n\"\n            \".. py:method:: non_existing_method()\")\n    restructuredtext.parse(app, text)\n    domain = app.env.get_domain('py')\n    assert 'non_existing_method' not in domain.objects\n\n\n\n", "def test_default_role(app, status, warning):\n    app.build()\n    content = (app.outdir / 'index.html').read_text()\n    assert '<span class=\"xref\">some_function</span>' in content  \n", "def test_warn_missing_reference_target(app, status, warning):\n    app.build()\n    assert 'index.rst:7: WARNING: undefined target: no-label' in warning.getvalue()\n", "def test_warn_missing_reference_with_explicit_label(app, status, warning):\n    app.build()\n    assert 'index.rst:6: WARNING: undefined label: no-label' in warning.getvalue()\n"], "sample_65": ["    def test_i18n_with_catalog_from_app(self):\n        with self.settings(LANGUAGE_CODE='en'), override('en'):\n            self.selenium.get(self.live_server_url + '/jsi18n_app6/')\n            elem = self.selenium.find_element_by_id('app6_string')\n            self.assertEqual(elem.text, 'translated string from app6')\n", "    def test_i18n_with_json_catalog(self):\n        with self.settings(LANGUAGE_CODE='en-us'), override('fr'):\n            response = self.client.get('/jsi18n_json_catalog/')\n            self.assertContains(response, 'App6 translated string')\n", "    def test_plural_forms(self):\n        self.selenium.get(self.live_server_url + '/jsi18n_plural_forms/')\n\n        elem = self.selenium.find_element_by_id('plural_one')\n        self.assertEqual(elem.text, '1 elemento')\n\n        elem = self.selenium.find_element_by_id('plural_two')\n        self.assertEqual(elem.text, '2 elementos')\n\n        elem = self.selenium.find_element_by_id('plural_few')\n        self.assertEqual(elem.text, 'unos pocos elementos')\n\n        elem = self.selenium.find_element_by_id('plural_many')\n        self.assertEqual(elem.text, 'much\u00edsimos elementos')\n\n        elem = self.selenium.find_element_by_id('plural_other')\n        self.assertEqual(elem.text, 'otros elementos')\n\n", "    def test_i18n_with_context_and_pluralforms(self):\n        self.selenium.get(self.live_server_url + '/jsi18n_context_plural/')\n\n        elem = self.selenium.find_element_by_id('ngettext_context')\n        self.assertEqual(elem.text, '1 message \u00e0 traduire')\n\n        elem = self.selenium.find_element_by_id('ngettext_context_plural')\n        self.assertEqual(elem.text, '455 messages \u00e0 traduire')\n", "    def test_context_variables(self):\n        with self.settings(LANGUAGE_CODE='en-us'), override('fr'):\n            response = self.client.get('/jsi18n_context_variables/')\n            self.assertContains(response, '10')\n            self.assertContains(response, 'fr')\n"], "sample_499": ["def test_hide():\n    fig, ax = plt.subplots()\n    ax.plot([0, 1], [0, 1], label=\"line\")\n    leg = ax.legend()\n    leg.set_visible(False)\n    assert not leg.get_visible()\n\n\n", "def test_label_format():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3], [4, 5, 6], label=\"test\")\n    \n    # Test default format\n    legend = ax.legend()\n    assert legend.get_texts()[0].get_text() == 'test'\n    \n    # Test custom format\n    legend = ax.legend(title='My Legend', labelformat=\"Short label: {0}\")\n    assert legend.get_texts()[0].get_text() == 'test'\n\n", "def test_legend_fontsize_from_rc():\n    # test that legend.fontsize from rc file is properly applied\n    mpl.rcParams['legend.fontsize'] = 16\n    fig, ax = plt.subplots()\n    ax.plot([0, 1], [0, 1], label='line')\n    leg = ax.legend()\n    for text in leg.get_texts():\n        assert text.get_fontsize() == 16\n", "compilation error", "def test_legend_bbox_to_anchor_with_subplots():\n    fig, axes = plt.subplots(2, 2)\n\n    axes[0, 0].plot([1, 2, 3], label='Line 1')\n    axes[0, 1].plot([1, 2, 3], label='Line 2')\n    axes[1, 0].plot([1, 2, 3], label='Line 3')\n\n    fig.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n\n    # Test that the legend doesn't overlap any subplots\n    for ax in axes.flat:\n        assert not ax.get_bbox().instersects(fig.legend().get_bbox())\n\n\n\n"], "sample_150": ["    def test_postgresql_extensions(self):\n        good_extensions = ['pgcrypto', 'btree_gist']\n        for extension in good_extensions:\n            with mock.patch(\n                'django.db.backends.utils.CursorWrapper.execute', create=True,\n                return_value=(('OK',),)\n            ), mock.patch(\n                'django.db.backends.utils.CursorWrapper.fetchone', create=True,\n                return_value=(True,)\n            ):\n                self.assertEqual(check_database_backends(databases=self.databases), [])\n\n        bad_extensions = ['bogus_extension']\n        for extension in bad_extensions:\n            with mock.patch(\n                'django.db.backends.utils.CursorWrapper.execute', create=True,\n                return_value=(('FALSE',),)\n            ):\n                result = check_database_backends(databases=self.databases)\n                self.assertEqual(len(result), 2)\n                self.assertEqual([r.id for r in result], ['postgresql.W001', 'postgresql.W001'])\n", "    def test_postgresql_search_path(self):\n        with mock.patch(\n            'django.db.backends.utils.CursorWrapper.fetchone', side_effect=[\n                ('foo::public',),\n                ('bar::public',),\n                # Another schema\n                ('baz::other_schema',),\n            ]\n        ):\n            result = check_database_backends(databases=self.databases)\n            self.assertEqual(len(result), 1)\n            self.assertEqual(result[0].id, 'postgresql.W001')\n", "    def test_only_one_database(self, mock_fetchone):\n        mock_fetchone.return_value = ('STRICT_TRANS_TABLES',)\n        result = check_database_backends(databases={'default'})\n        self.assertEqual(len(result), 0)\n\n\n\n\n", "    def test_postgres_default_transaction_isolation(self):\n        with mock.patch(\n            'django.db.backends.utils.CursorWrapper.fetchone', create=True,\n            return_value=('SERIALIZABLE',)  # Default for PostgreSQL\n        ):\n            self.assertEqual(check_database_backends(databases=self.databases), [])\n\n", "    def test_postgresql_timezone_checks(self):\n        with mock.patch('django.db.backends.utils.CursorWrapper.fetchone', create=True):\n            result = check_database_backends(databases=self.databases)\n            self.assertEqual(len(result), 2)\n            self.assertEqual(result[0].id, 'postgresql.W001')  # timezone is not set\n            self.assertEqual(result[1].id, 'postgresql.W001')  # timezone is not set\n\n\n\n"], "sample_962": ["def test_stringify_nested_types(target, expected):\n    assert stringify(target) == expected\n", "def test_stringify_type_hints_TypedDict():\n    from typing import TypedDict  # type: ignore\n\n    class MyTypedDict(TypedDict):\n        name: str\n        age: int\n\n    assert stringify(MyTypedDict) == \"tests.test_util_typing.MyTypedDict\"\n\n", "def test_restify_various_types(expected):\n    with mock(['unknown']):\n        import unknown\n        assert restify(unknown.secret.Class) == expected\n", "    def test_stringify_type_hints_forward_ref():\n        from typing import ForwardRef  # type: ignore\n        assert stringify(ForwardRef(\"myint\")) == \"myint\"\n", "def test_stringify_type_hints_typing_cast():\n    from typing import cast  # type: ignore\n    assert stringify(cast(str, 1)) == \"str\"\n    assert stringify(cast(int, \"2\")) == \"int\"\n\n\n"], "sample_857": ["def test_sparse_min_samples_split_behaviour():\n    X_sparse = csc_matrix([[1, 2], [3, 4]])\n    y = [0, 1]\n\n    for tree_cls in [DecisionTreeClassifier, DecisionTreeRegressor]:\n        for min_samples_split in [1, 2]:\n            est = tree_cls(min_samples_split=min_samples_split, random_state=0)\n            est.fit(X_sparse, y)\n\n            assert est.tree_.min_samples_split == min_samples_split\n            assert all(\n                n_samples >= min_samples_split\n                for n_samples in est.tree_.n_node_samples\n            )\n\n\n\n", "def test_fit_sparse_data_with_sample_weights():\n    X_sparse = csc_matrix([[1, 2], [3, 4], [5, 6]])\n    y = np.array([0, 1, 0])\n    sample_weight = np.array([0.5, 0.7, 0.8])\n    \n    clf = DecisionTreeClassifier(random_state=0)\n    clf.fit(X_sparse, y, sample_weight=sample_weight)\n    \n    assert_tree_equal(clf.tree_, clf.tree_)\n\n\n", "def test_prune_tree_raises_valueerror_if_fit_not_called():\n    clf = DecisionTreeClassifier()\n    msg = \"Estimator has not been fit\"\n\n    with pytest.raises(ValueError, match=msg):\n        clf.cost_complexity_pruning_path(np.zeros((3, 2)), np.zeros(3))\n\n    with pytest.raises(ValueError, match=msg):\n        clf._prune_tree()\n\n\n", "def test_multi_class_pruning():\n    X = np.arange(100).reshape((100, 1))\n    y = np.random.choice(4, size=100)\n\n    est = DecisionTreeClassifier(max_leaf_nodes=20, random_state=0)\n    info = est.cost_complexity_pruning_path(X, y)\n    pruning_path = info.ccp_alphas\n\n    estimators = []\n    for ccp_alpha in pruning_path:\n        est = DecisionTreeClassifier(\n            max_leaf_nodes=20, ccp_alpha=ccp_alpha, random_state=0).fit(X, y)\n        estimators.append(est)\n    # A pruned tree must be a subtree of the previous tree (which had a\n    # smaller ccp_alpha)\n    for prev_est, next_est in zip(estimators, estimators[1:]):\n        assert_is_subtree(prev_est.tree_, next_est.tree_)\n\n\n\n", "def test_prune_tree_with_missing_values():\n    X = np.random.RandomState(0).random_sample((100, 10))\n    y = np.random.RandomState(0).randn(100)\n    X[:, 5] = np.nan\n\n    for tree_cls in [DecisionTreeClassifier, DecisionTreeRegressor]:\n        est = tree_cls(max_leaf_nodes=20, random_state=0)\n\n        # fit with missing values\n        est.fit(X, y)\n        assert_pruning_creates_subtree(tree_cls, X, y, est.cost_complexity_pruning_path(X, y))\n\n\n\n\n"], "sample_1127": ["compilation error", "compilation error", "def test_derived_subgroup():\n    S = SymmetricGroup(4)\n    assert S.derived_subgroup() == AlternatingGroup(4)\n    A = AlternatingGroup(4)\n    assert A.derived_subgroup() == PermutationGroup([Permutation(size = 4)])\n    G = PermutationGroup(Permutation(0, 1, 2), Permutation(0, 2, 3))\n    assert G.derived_subgroup().is_trivial \n    G = PermutationGroup(Permutation(0, 1), Permutation(0, 2, 3))\n    assert G.derived_subgroup() == PermutationGroup([Permutation(size = 2)])\n", "compilation error", "compilation error"], "sample_374": ["    def test_nested_prefetch_with_inverse_fk_is_not_overwritten(self):\n        \"\"\"\n        When prefetching a child object with a inverse foreign key relationship\n        and that child object is used in the queryset of a nested prefetch,\n        the prefetched relationship is used rather than populating the\n        reverse relationship from the parent.\n        \"\"\"\n        house = House.objects.create(name='Big house', address='123 Main St')\n        room1 = Room.objects.create(name='Kitchen', house=house)\n        room2 = Room.objects.create(name='Lounge', house=house)\n\n        class NestedPrefetchTestManager(models.Manager):\n                return super().get_queryset().prefetch_related('house')\n\n        House.objects.add_to_class('rooms', NestedPrefetchTestManager())\n\n        with self.assertNumQueries(3):\n            rooms = House.objects.prefetch_related(\n                Prefetch('rooms', queryset=Room.objects.prefetch_related(\n                    Prefetch('house', queryset=House.objects.only('name')),\n                )),\n            ).first().rooms.all()\n        with self.assertNumQueries(0):\n            for room in rooms:\n                self.assertIs(Room.house.is_cached(room), True)\n                self.assertEqual(room.house.name, 'Big house')\n", "    def test_nested_prefetch_with_related_manager_on_child(self):\n        \"\"\"\n        The prefetched relationship is used rather than populating the reverse\n        relationship from the parent, when prefetching a set of child objects\n        related to a set of parent objects and the child queryset itself\n        specifies a prefetch back to the parent.\n        \"\"\"\n        class BookWithAuthorManager(models.Manager):\n                return self.filter(author_id__isnull=False).prefetch_related(\n                    'author'\n                )\n\n        class Book(models.Model):\n            title = models.CharField(max_length=255)\n            author = models.ForeignKey(Author, on_delete=models.SET_NULL, null=True)\n            objects = BookWithAuthorManager()\n\n        class Author(models.Model):\n            name = models.CharField(max_length=255)\n\n        book1 = Book.objects.create(title='New Book')\n        book2 = Book.objects.create(title='Another Book')\n        author1 = Author.objects.create(name='John Doe')\n        book1.author = author1\n        book1.save()\n\n        books = Book.objects.filter(title__in=['New Book', 'Another Book']).values('title')\n        with self.assertNumQueries(3):\n            for book in list(books):\n                book['author']['name']\n\n        self.assertEqual(len(books), 2)\n\n\n        # Should only fetch from cache, no new queries\n\n        with self.assertNumQueries(0):\n            for book in books:\n                book['author']['name']\n", "    def test_nested_prefetch_with_distinct(self):\n        \"\"\"\n        Nested prefetch with distinct on child objects.\n        \"\"\"\n        House.objects.create(name='Big house', address='123 Main St')\n        House.objects.create(name='Small house', address='456 Oak St')\n        Room.objects.create(name='Kitchen', house__name='Big house')\n        Room.objects.create(name='Living Room', house__name='Small house')\n        queryset = House.objects.prefetch_related(\n            Prefetch(\n                'rooms',\n                queryset=Room.objects.distinct('name').prefetch_related(\n                    Prefetch('house')\n                ),\n            )\n        )\n\n        with self.assertNumQueries(3):\n            houses = queryset.all()\n\n        with self.assertNumQueries(0):\n            for house in houses:\n                for room in house.rooms.all():\n                    assert isinstance(room, Room)\n                self.assertEqual(len(set(room.name for room in house.rooms.all())), len(house.rooms.all()))\n", "    def test_nested_prefetch_with_distinct(self):\n        \"\"\"\n        Nested prefetch works correctly with distinct().\n        \"\"\"\n        distinct_author_list = [\n            AuthorWithAge.objects.create(name='John', first_book=Book.objects.create(title='Book1')),\n            AuthorWithAge.objects.create(name='Jane', first_book=Book.objects.create(title='Book2')),\n            AuthorWithAge.objects.create(name='John', first_book=Book.objects.create(title='Book3')),\n        ]\n        list(AuthorWithAge.objects.prefetch_related(\n            Prefetch(\n                'first_book',\n                queryset=Book.objects.filter(title='Book1').distinct()\n            ),\n        ))\n        self.assertEqual(distinct_author_list[0].first_book.title, 'Book1')\n        self.assertEqual(distinct_author_list[1].first_book.title, 'Book2')\n        self.assertEqual(distinct_author_list[2].first_book.title, 'Book1')\n\n\n\n", "    def test_nested_prefetch_with_filtering(self):\n        house1 = House.objects.create(name='Big house', address='123 Main St')\n        house2 = House.objects.create(name='Small house', address='456 Elm St')\n        Room.objects.create(name='Kitchen', house=house1)\n        Room.objects.create(name='Bedroom', house=house1)\n        Room.objects.create(name='Living room', house=house2)\n\n        queryset = House.objects.filter(name__contains='house').prefetch_related(\n            Prefetch('rooms', queryset=Room.objects.filter(name='Bedroom').prefetch_related(\n                Prefetch('house'),\n            )),\n        )\n        with self.assertNumQueries(3):\n            house = queryset.first()\n\n        with self.assertNumQueries(0):\n            house.rooms.first().name\n            house.rooms.first().house.name\n"], "sample_267": ["    def test_transactions_rollback(self):\n        with transaction.atomic():\n            Object.objects.create(name='test')\n            raise ValueError\n        objects = Object.objects.all()\n        self.assertEqual(len(objects), 0)\n\n\n", "    def test_transaction_blocking_in_threaded_environment(self):\n        with connection.cursor() as cursor:\n            cursor.execute(\"BEGIN TRANSACTION\")\n        thread = threading.Thread(target=lambda: self.create_object())\n        thread.start()\n        thread.join()\n        with connection.cursor() as cursor:\n            cursor.execute(\"ROLLBACK\")\n        self.assertEqual(Object.objects.count(), 0)\n\n", "    def test_transaction_manager_in_threads(self):\n            with transaction.atomic():\n                pass\n\n        thread1 = threading.Thread(target=acquire_lock)\n        thread2 = threading.Thread(target=acquire_lock)\n\n        thread1.start()\n        thread2.start()\n\n        thread1.join()\n        thread2.join()\n", "    def test_atomic_transaction_with_external_connection(self):\n        \"\"\"\n        Atomic transactions should work even with an external SQLite connection\n        open.\n        \"\"\"\n        connection_settings = {\n            'ENGINE': 'django.db.backends.sqlite3',\n            'NAME': ':memory:',\n        }\n\n        # Create a second connection to the same database in a separate thread.\n            return ConnectionHandler(connection_settings)['default']\n\n        external_connection = open_external_connection()\n\n        # Start a transaction in the main thread.\n        with connection.cursor() as cursor:\n            cursor.execute(\"BEGIN\")\n            with transaction.atomic():\n                # Simulate some work.\n                Object.objects.create(name='Test Object')\n                external_connection.cursor().execute(\"SELECT 1\")\n                # Commit the transaction.\n                cursor.execute(\"COMMIT\")\n\n        # Check that the changes were committed and are visible to the external connection.\n        with external_connection.cursor() as cursor:\n            cursor.execute(\"SELECT name FROM backends_object\")\n            result = cursor.fetchone()\n            self.assertEqual(result, ('Test Object',))\n", "    def test_connection_persistence(self):\n        with connection.cursor() as cursor:\n            cursor.execute(\"CREATE TABLE IF NOT EXISTS test_connection_persistence (id INTEGER PRIMARY KEY)\")\n\n        with connection.cursor() as cursor:\n            cursor.execute(\"INSERT INTO test_connection_persistence (id) VALUES (1)\")\n\n        connection.close()\n\n        with connection.cursor() as cursor:\n            cursor.execute(\"SELECT id FROM test_connection_persistence\")\n            result = cursor.fetchone()\n\n        self.assertEqual(result, (1,))\n"], "sample_384": ["    def test_cached_attribute_updates(self):\n        for note in Note.objects.all():\n            note.cached_data = \"test\"\n        with self.assertNumQueries(1):\n            Note.objects.bulk_update(Note.objects.all(), [\"cached_data\"], batch_size=10)\n        self.assertCountEqual(Note.objects.values_list(\"cached_data\", flat=True), [\"test\"])\n", "    def test_json_field_update_with_null(self):\n        obj = JSONFieldNullable.objects.create(json_field={\"a\": 1})\n        obj.json_field = None\n        JSONFieldNullable.objects.bulk_update([obj], fields=[\"json_field\"])\n        self.assertIsNone(obj.json_field)\n", "    def test_null_values(self):\n        notes = [Note.objects.create(note=\"test_note\", misc=\"test_misc\") for _ in range(10)]\n        obj1, obj2 = notes[0], notes[1]\n        obj1.note = None\n        obj2.misc = None\n        Note.objects.bulk_update([obj1, obj2], fields=[\"note\", \"misc\"])\n        self.assertIsNone(Note.objects.get(pk=obj1.pk).note)\n        self.assertIsNone(Note.objects.get(pk=obj2.pk).misc)\n", "    def test_null_values(self):\n        notes = [Note.objects.create(note=str(i), misc=str(i)) for i in range(10)]\n        for note in notes:\n            note.note = None\n        with self.assertNumQueries(1):\n            Note.objects.bulk_update(notes, [\"note\"])\n        self.assertCountEqual(\n            Note.objects.filter(note__isnull=True), notes\n        )\n\n        for note in notes:\n            note.misc = None\n        with self.assertNumQueries(1):\n            Note.objects.bulk_update(notes, [\"misc\"])\n        self.assertCountEqual(\n            Note.objects.filter(misc__isnull=True), notes\n        )\n\n\n", "    def test_json_field_update(self):\n        json_obj = JSONFieldNullable.objects.create(json_field={\"a\": 1})\n        json_obj.json_field = {\"b\": 1}\n        JSONFieldNullable.objects.bulk_update([json_obj], fields=[\"json_field\"])\n        self.assertEqual(json_obj.json_field, {\"b\": 1})\n\n"], "sample_387": ["compilation error", "    def test_ManyToMany_using_to_fields(self):\n        from selenium.webdriver.common.by import By\n        from selenium.webdriver.support.ui import Select\n\n        self.admin_login(username=\"super\", password=\"secret\", login_url=\"/\")\n        self.selenium.get(\n            self.live_server_url + reverse(\"admin:admin_widgets_profile_add\")\n        )\n        main_window = self.selenium.current_window_handle\n\n        # No value has been selected yet\n        self.assertEqual(\n            self.selenium.find_element(By.ID, \"id_related_users\").get_attribute(\n                \"value\"\n            ),\n            \"\",\n        )\n\n        # Add some related users\n        self.selenium.find_element(By.ID, \"add_id_related_user\").click()\n\n        self.wait_for_and_switch_to_popup()\n\n        username_field = self.selenium.find_element(By.ID, \"id_username\")\n        username_value = \"relateduser1\"\n        username_field.send_keys(username_value)\n        save_button_css_selector = \".submit-row > input[type=submit]\"\n        self.selenium.find_element(By.CSS_SELECTOR, save_button_css_selector).click()\n\n        self.selenium.switch_to.window(main_window)\n\n        # Select the newly created user from the related users dropdown\n        select = Select(self.selenium.find_element(By.ID, \"id_related_users\"))\n        select.select_by_visible_text(username_value)\n\n        # Add another related user\n        self.selenium.find_element(By.ID, \"add_id_related_user\").click()\n        self.wait_for_and_switch_to_popup()\n\n        username_field = self.selenium.find_element(By.ID, \"id_username\")\n        username_value = \"relateduser2\"\n        username_field.send_keys(username_value)\n        save_button_css_selector = \".submit-row > input[type=submit]\"\n        self.selenium.find_element(By.CSS_SELECTOR,", "    def test_ManyToMany_using_to_field(self):\n        from selenium.webdriver.common.by import By\n        from selenium.webdriver.support.ui import Select\n\n        self.admin_login(username=\"super\", password=\"secret\", login_url=\"/\")\n        self.selenium.get(\n            self.live_server_url + reverse(\"admin:admin_widgets_event_add\")\n        )\n\n        main_window = self.selenium.current_window_handle\n\n        # Click the Add Supporting Band button to add new\n        self.selenium.find_element(By.ID, \"add_id_supporting_bands\").click()\n        self.wait_for_and_switch_to_popup()\n\n        self.selenium.find_element(By.LINK_TEXT, \"Bogey Blues\").click()\n\n        self.selenium.switch_to.window(main_window)\n\n        # The field now contains the selected band\n        select = Select(self.selenium.find_element(By.ID, \"id_supporting_bands\"))\n        self.assertEqual(select.first_selected_option.text, \"Bogey Blues\")\n\n        # Click the Add Supporting Band button again\n        self.selenium.find_element(By.ID, \"add_id_supporting_bands\").click()\n        self.wait_for_and_switch_to_popup()\n\n        self.selenium.find_element(By.LINK_TEXT, \"Green Potatoes\").click()\n\n        self.selenium.switch_to.window(main_window)\n\n        # The field now contains both selected bands\n        select = Select(self.selenium.find_element(By.ID, \"id_supporting_bands\"))\n        self.assertIn(\"Bogey Blues\", select.all_selected_options)\n        self.assertIn(\"Green Potatoes\", select.all_selected_options)\n\n        # Go ahead and submit the form\n        self.selenium.find_element(By.CSS_SELECTOR, \"input[type='submit']\").click()\n        self.wait_for_text(\n            \"li.success\", \"The event was added successfully.\"\n        )\n\n\n\n", "    def test_related_field_widget_popup(self):\n        from selenium.webdriver.common.by import By\n\n        self.admin_login(username=\"super\", password=\"secret\", login_url=\"/\")\n        self.selenium.get(\n            self.live_server_url + reverse(\"admin:admin_widgets_profile_add\")\n        )\n\n        main_window = self.selenium.current_window_handle\n\n        # Click the Add Related User button to open the popup\n        self.selenium.find_element(By.ID, \"add_related_user\").click()\n\n        self.wait_for_and_switch_to_popup()\n\n        # The popup should display the related user form\n        form_title = self.selenium.find_element(By.CSS_SELECTOR, \"h2\").text\n        self.assertEqual(form_title, \"Add Related User\")\n\n        # Close the popup\n        self.selenium.find_element(By.CSS_SELECTOR, '[data-dismiss=\"modal\"]').click()\n        self.selenium.switch_to.window(main_window)\n\n\n\n", "    def test_lookup_field_with_many_to_many(self):\n        \"\"\"\n        Test that lookup field with many-to-many relationships works\n        correctly.\n        \"\"\"\n        from selenium.webdriver.common.by import By\n        from selenium.webdriver.support.ui import Select\n \n        self.admin_login(username=\"super\", password=\"secret\", login_url=\"/\")\n        self.selenium.get(self.live_server_url + reverse(\"admin:admin_widgets_event_add\"))\n\n        main_window = self.selenium.current_window_handle\n        \n        # Open the popup window and click on a band\n        self.selenium.find_element(By.ID, \"lookup_id_supporting_bands\").click()\n        self.wait_for_and_switch_to_popup()\n        link = self.selenium.find_element(By.LINK_TEXT, \"Bogey Blues\")\n        self.assertIn(\"/band/42/\", link.get_attribute(\"href\"))\n        link.click()\n        self.selenium.switch_to.window(main_window)\n\n        # Check that the selected band is visible\n        self.wait_for_value(\"#id_supporting_bands\", \"42\")\n\n        # Reopen the popup window and click on another band\n        self.selenium.find_element(By.ID, \"lookup_id_supporting_bands\").click()\n        self.wait_for_and_switch_to_popup()\n        link = self.selenium.find_element(By.LINK_TEXT, \"Green Potatoes\")\n        self.assertIn(\"/band/98/\", link.get_attribute(\"href\"))\n        link.click()\n        self.selenium.switch_to.window(main_window)\n\n        # Check that both bands are shown\n        self.wait_for_value(\"#id_supporting_bands\", \"42,98\")\n"], "sample_537": ["def test_psd_zero_frequency():\n    u = np.array([0, 1, 2, 3, 1, 2, 1])\n    dt = 1.0\n    P, f = mlab.psd(u, NFFT=len(u), Fs=1/dt, window=mlab.window_none,\n                    detrend=mlab.detrend_none, noverlap=0, pad_to=None,\n                    scale_by_freq=None,\n                    sides='onesided')\n    assert np.isclose(P[0], 0.0)\n\n", "def test_psd_frequencies_edges():\n    u = np.array([0, 1, 2, 3, 1, 2, 1])\n    dt = 1.0\n    NFFT = u.size\n    Fs = 1/dt\n    f = mlab.psd_frequencies(NFFT, Fs)\n    assert f[0] == 0\n    assert f[-1] == Fs / 2\n", "def test_psd_default_window():\n    u = np.random.randn(1024)\n    P, f = mlab.psd(u, NFFT=1024, Fs=1000, window=None)\n    assert_equal(mlab.window_hanning(1024), mlab.psd._default_window)\n    P2, f2 = mlab.psd(u, NFFT=1024, Fs=1000)\n    assert_allclose(P, P2, atol=1e-06)\n", "    def test_psd_detrending_linear(self):\n        u = np.array([0, 1, 2, 3, 2, 1, 0])\n        dt = 1.0\n        Fs = 1/dt\n        NFFT = 8\n        window = mlab.window_hanning(NFFT)\n        trend = np.polyfit(np.arange(NFFT), u, 1)\n        u_detrended = u - np.polyval(trend, np.arange(NFFT))\n        Su = np.abs(np.fft.fft(u) * dt)**2 / (dt * u.size)\n        Su_detrended = np.abs(np.fft.fft(u_detrended) * dt)**2 / (dt * u.size)\n        P, f = mlab.psd(u, NFFT=NFFT, Fs=Fs, window=window,\n                        detrend=mlab.detrend_linear, noverlap=0,\n                        pad_to=None, scale_by_freq=None, sides='onesided')\n        assert_allclose(P, Su_detrended, atol=1e-06)\n\n\n\n", "def test_psd_window_array_float_input():\n    win = mlab.window_hanning(np.ones(1024, dtype=float))\n    spec, fs = mlab.psd(x=np.sin(np.linspace(0, 2*np.pi, 1024)),\n                         NFFT=512,\n                         Fs=1,\n                         noverlap=256,\n                         pad_to=None,\n                         sides='onesided',\n                         window=win)\n    assert_allclose(spec.dtype, np.float64)\n    assert_allclose(fs.dtype, np.float64)\n"], "sample_922": ["    def test_pyfunction_signature_full_with_defaults(app):\n        text = \".. py:function:: hello(a: int, b: str = 'default', c: bool = True)\\n\"\n        doctree = restructuredtext.parse(app, text)\n        assert_node(doctree, (addnodes.index,\n                              [desc, ([desc_signature, ([desc_name, \"hello\"],\n                                                    [desc_parameterlist,\n                                                     [desc_parameter, desc_sig_name, \"a\"],\n                                                     [desc_parameter, desc_sig_operator, \"=\"],\n                                                     [desc_parameter, desc_sig_name, \"b\"],\n                                                     [desc_parameter, desc_sig_punctuation, \":\"],\n                                                     [desc_parameter, desc_sig_name, \"str\"],\n                                                     [desc_parameter, desc_sig_punctuation, \",\"],\n                                                     [desc_parameter, desc_sig_operator, \"=\"],\n                                                     [desc_parameter, desc_sig_name, \"c\"],\n                                                     [desc_parameter, desc_sig_punctuation, \":\"],\n                                                     [desc_parameter, desc_sig_name, \"bool\"],\n                                                     [desc_parameter, desc_sig_punctuation, \",\"]],\n                                                    [desc_returns, pending_xref, \"None\"])],\n                                                 desc_content)]))\n", "def test_pyfunction_signature_with_keyword_only_args(app):\n    text = \".. py:function:: my_function\\n\"\n    text += \"   :param a: This is argument a\\n\"\n    text += \"   :param b: This is argument b\\n\"\n    text += \"   :param c: This is argument c\\n\"\n    text += \"   (keyword only)\\n\"\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"my_function\"],\n                                                    desc_parameterlist,\n                                                    [desc_annotation, (\": \",\n                                [desc_sig_name, \"a\"],\n                [desc_sig_punctuation, \": \"],\n                [pending_xref, \"This is argument a\"])]\n                ),\n                [desc_annotation, (\": \",\n                [desc_sig_name, \"b\"],\n                [desc_sig_punctuation, \": \"],\n                [pending_xref, \"This is argument b\"])]\n                ,\n                [desc_annotation, (\": \",\n                [desc_sig_name, \"c\"],\n                [desc_sig_punctuation, \": \"],\n                [pending_xref, \"This is argument c\"])]\n                \n                ),\n                [desc_content, ()])]))\n\n\n\n\n", "def test_module_index_duplicates(app):\n    text = \".. py:module:: docutils\\n\"\n    text += \".. py:module:: docutils\\n\"\n    restructuredtext.parse(app, text)\n    index = PythonModuleIndex(app.env.get_domain('py'))\n    assert index.generate() == (\n        [('d', [IndexEntry('docutils', 0, 'index', 'module-docutils', '', '', '')])],\n        False\n    )\n", "    def test_pyfunction_signature_with_varargs(app):\n        text = \".. py:function:: some_func\\n\"\n        text += \"   :args: arg1, arg2, *args\\n\"\n        domain = app.env.get_domain('py')\n        doctree = restructuredtext.parse(app, text)\n        assert_node(doctree, (addnodes.index,\n                              [desc, ([desc_signature, ([desc_name, \"some_func\"],\n                                        [desc_parameterlist,\n                                         [desc_parameter, \"arg1\"],\n                                         [desc_parameter, \"arg2\"],\n                                         [desc_parameter, desc_sig_punctuation, \"*\",\n                                          desc_parameter, \"args\"]])],\n                                        desc_content)]))\n", "    def test_pyexception_signature_with_base(app):\n        text = \".. py:exception:: MyException\\n\"\n        text += \"    :base: Exception\\n\"\n        domain = app.env.get_domain('py')\n        doctree = restructuredtext.parse(app, text)\n        assert_node(doctree, (addnodes.index,\n                              [desc, ([desc_signature, ([desc_annotation, \"exception \"],\n                                                    [desc_name, \"MyException\"],\n                                                    [desc_parameterlist, ()],\n                                                    [desc_annotation, \": BaseException\"])],\n                                                [desc_content, ()])]))\n        assert 'MyException' in domain.objects\n        assert domain.objects['MyException'] == ('index', 'MyException', 'exception')\n\n\n\n"], "sample_591": ["    def test_merge_identical_dataarrays(self):\n        ds1 = xr.Dataset({\"a\": xr.DataArray([1, 2], dims=\"x\")})\n        ds2 = xr.Dataset({\"a\": xr.DataArray([1, 2], dims=\"x\")})\n        merged = ds1.merge(ds2)\n        assert len(merged.variables) == 1\n        assert merged.variables[\"a\"].data.shape == (2,)\n", "    def test_merge_with_missing_dims(self):\n        ds1 = xr.Dataset({\"a\": (\"y\", [1, 2, 3])})\n        ds2 = xr.Dataset({\"b\": (\"x\", [4, 5])})\n        merged = ds1.merge(ds2)\n        assert merged[\"a\"].dims == (\"y\",)\n        assert merged[\"b\"].dims == (\"x\",)\n        assert \"y\" in merged.coords\n        assert \"x\" in merged.coords\n\n\n\n", "    def test_merge_dataset_with_different_dims(self):\n        ds1 = xr.Dataset({\"x\": ((\"x\", \"y\"), [[1, 2], [3, 4]])})\n        ds2 = xr.Dataset({\"y\": ((\"x\", \"y\"), [[5, 6], [7, 8]])})\n        with pytest.raises(ValueError):\n            ds1.merge(ds2)\n        with pytest.raises(ValueError):\n            ds2.merge(ds1)\n\n", "    def test_merge_dataarray_with_coords(self):\n        ds = xr.Dataset()\n        da = xr.DataArray(data=1, name=\"b\", coords={\"x\": [1, 2]})\n        merged = ds.merge(da)\n        assert merged.coords[\"x\"].size == 2\n        assert merged.data_vars == {\"b\": da}\n", "    def test_merge_with_existing_coords(self):\n        ds1 = xr.Dataset({\"a\": (\"x\", [1, 2]), \"x\": [0, 1]})\n        ds2 = xr.Dataset({\"b\": (\"x\", [3, 4]), \"x\": [0, 1], \"y\": [5, 6]})\n        expected = xr.Dataset({\"a\": (\"x\", [1, 2]), \"b\": (\"x\", [3, 4]), \"x\": [0, 1], \"y\": [5, 6]})\n\n        assert expected.identical(ds1.merge(ds2))\n        assert expected.identical(ds2.merge(ds1))\n\n\n"], "sample_173": ["    def test_sequence_reset_sql(self):\n        with transaction.atomic():\n            Author.objects.create(name='George Orwell')\n            Book.objects.create(author=Author.objects.get(name='George Orwell'))\n\n        sql_list = connection.ops.sql_flush(\n            no_style(),\n            [Author._meta.db_table, Book._meta.db_table],\n            reset_sequences=True,\n            allow_cascade=True,\n        )\n        connection.ops.execute_sql_flush(connection.alias, sql_list)\n\n        with transaction.atomic():\n            self.assertEqual(Author.objects.count(), 0)\n            self.assertEqual(Book.objects.count(), 0)\n            if connection.features.supports_sequence_reset:\n                author = Author.objects.create(name='F. Scott Fitzgerald')\n                self.assertEqual(author.pk, 1)\n                book = Book.objects.create(author=author)\n                self.assertEqual(book.pk, 1)\n\n\n\n", "    def test_explain_query_prefix_raise_not_supported_error(self):\n        msg = 'This backend does not support explaining query execution.'\n        with self.assertRaisesMessage(NotSupportedError, msg):\n            connection.ops.explain_query_prefix()\n", "    def test_convert_durationfield_value(self):\n        ops = BaseDatabaseOperations(connection=connection)\n        value = 12345678\n        with self.assertRaisesMessage(NotImplementedError, 'This backend does not support duration field casting'):\n            ops.convert_durationfield_value(value, None, None)\n", "    def test_sequence_reset_sql(self):\n        with self.assertRaises(NotImplementedError):\n            self.ops.sequence_reset_sql(None)\n\n\n\n", "    def test_sql_flush_no_reset_sequences(self):\n        with transaction.atomic():\n            author = Author.objects.create(name='George Orwell')\n            Book.objects.create(author=author)\n            author = Author.objects.create(name='Harper Lee')\n            Book.objects.create(author=author)\n            Book.objects.create(author=author)\n            self.assertIs(Author.objects.exists(), True)\n            self.assertIs(Book.objects.exists(), True)\n\n        sql_list = connection.ops.sql_flush(\n            no_style(),\n            [Author._meta.db_table, Book._meta.db_table],\n            reset_sequences=False,\n            allow_cascade=True,\n        )\n        connection.ops.execute_sql_flush(connection.alias, sql_list)\n\n        with transaction.atomic():\n            self.assertIs(Author.objects.exists(), False)\n            self.assertIs(Book.objects.exists(), False)\n"], "sample_750": ["def test_omp_path_with_tol():\n    path = orthogonal_mp(X, y, n_nonzero_coefs=5, tol=0.1, return_path=True)\n    last = orthogonal_mp(X, y, n_nonzero_coefs=5, tol=0.1, return_path=False)\n    assert_equal(path.shape, (n_features, n_targets, 5))\n    assert_array_almost_equal(path[:, :, -1], last)\n    path = orthogonal_mp_gram(G, Xy, n_nonzero_coefs=5, tol=0.1, return_path=True)\n    last = orthogonal_mp_gram(G, Xy, n_nonzero_coefs=5, tol=0.1, return_path=False)\n    assert_equal(path.shape, (n_features, n_targets, 5))\n    assert_array_almost_equal(path[:, :, -1], last)\n", "def test_omp_with_precomputed_gram():\n    n_samples, n_features, n_nonzero_coefs = 20, 30, 5\n    y, X, G = make_sparse_coded_signal(n_targets=3, n_features=n_features, \n                                     n_samples=n_samples, n_nonzero_coefs=n_nonzero_coefs, \n                                     random_state=0)\n    \n    gamma_direct = orthogonal_mp_gram(G, Xy, n_nonzero_coefs=n_nonzero_coefs)\n    gamma_compute_gram = orthogonal_mp(X, y, n_nonzero_coefs=n_nonzero_coefs, precompute=True)\n    assert_array_almost_equal(gamma_direct, gamma_compute_gram)\n\n", "def test_omp_gram_with_different_sample_weights():\n    # Test that orthogonal_mp_gram handles different sample weights in X\n    # by creating a sparse matrix with different weights for each sample.\n    from scipy.sparse import coo_matrix\n    rng = check_random_state(0)\n    n_samples, n_features, n_targets = 20, 30, 3\n    X = rng.randn(n_samples, n_features)\n    y = rng.randn(n_samples, n_targets)\n    # Create a sparse matrix with random sample weights\n    weights = rng.randint(1, 5, size=n_samples)\n    X_sparse = coo_matrix((weights, (np.arange(n_samples),\n                                     np.arange(n_features))))\n    G, Xy = np.dot(X_sparse.transpose(), X_sparse), np.dot(X_sparse.transpose(), y)\n    # Compare the results with and without sample weights\n    omp_no_weights = orthogonal_mp_gram(G, Xy, n_nonzero_coefs=5)\n    omp_with_weights = orthogonal_mp_gram(G, Xy, n_nonzero_coefs=5,\n                                           copy_Gram=True)\n    assert_array_almost_equal(omp_no_weights, omp_with_weights)\n\n\n\n", "def test_omp_with_sparse_data():\n    n_samples, n_features, n_nonzero_coefs, n_targets = 100, 1000, 10, 3\n    X, y, gamma = make_sparse_coded_signal(n_targets, n_features,\n                                          n_samples, n_nonzero_coefs,\n                                          random_state=0)\n    X_sparse = X.copy()\n    X_sparse.data = X.data\n    X_sparse.eliminate_zeros()\n\n    omp_dense = orthogonal_mp(X, y, n_nonzero_coefs=n_nonzero_coefs)\n    omp_sparse = orthogonal_mp(X_sparse, y,\n                                n_nonzero_coefs=n_nonzero_coefs)\n    assert_array_almost_equal(omp_dense, omp_sparse, decimal=3)\n\n\n    omp_gram_dense = orthogonal_mp_gram(np.dot(X.T, X),\n                                        np.dot(X.T, y),\n                                        n_nonzero_coefs=n_nonzero_coefs)\n    omp_gram_sparse = orthogonal_mp_gram(np.dot(X_sparse.T, X_sparse),\n                                        np.dot(X_sparse.T, y),\n                                        n_nonzero_coefs=n_nonzero_coefs)\n    assert_array_almost_equal(omp_gram_dense, omp_gram_sparse, decimal=3)\n", "def test_omp_with_different_precompute_values():\n    gamma_precompute = orthogonal_mp_gram(G, Xy, 5, precompute=True)\n    gamma_no_precompute = orthogonal_mp_gram(G, Xy, 5, precompute=False)\n    assert_array_almost_equal(gamma_precompute, gamma_no_precompute)\n"], "sample_1108": ["def test_int_to_bin_len():\n    assert int_to_bin_len(0) == 0\n    assert int_to_bin_len(1) == 1\n    assert int_to_bin_len(2) == 2\n    assert int_to_bin_len(3) == 2\n    assert int_to_bin_len(15) == 4\n    assert int_to_bin_len(32767) == 15\n", "compilation error", "def test_all_permutations():\n    assert list(all_permutations([1, 2, 3])) == [[1, 2, 3], [1, 3, 2], [2, 1, 3], [2, 3, 1], [3, 1, 2], [3, 2, 1]]\n    assert list(all_permutations(range(5))) == list(permutations(range(5)))\n    assert list(all_permutations(list('abc'))) == list(permutations(list('abc')))\n    assert list(all_permutations([1, 2, 2])) == [[1, 2, 2], [1, 2, 2], [2, 1, 2], [2, 2, 1], [2, 2, 1]]\n    assert list(all_permutations([])) == [[]]\n", "def test_list_permutations():\n    assert list_permutations([]) == []\n    assert list_permutations([1]) == [[1]]\n    assert list_permutations([1, 2]) == [[1, 2], [2, 1]]\n    assert list_permutations(list(range(3))) == [[0, 1, 2], [0, 2, 1], [1, 0, 2], [1, 2, 0], [2, 0, 1], [2, 1, 0]]\n    assert list_permutations([1, 1, 2]) == [[1, 1, 2], [1, 2, 1], [2, 1, 1]]\n    assert list_permutations([1, 2, 2]) == [[1, 2, 2], [1, 2, 2], [2, 1, 2], [2, 2, 1]]\n    assert list_permutations([1, 2, 3, 4, 5, 6]) == [[1, 2, 3, 4, 5, 6], [1, 2, 3, 4, 6, 5], [1, 2, 3, 5, 4, 6], [1, 2, 3, 5, 6, 4], [1, 2, 3, 6, 4, 5], [1, 2, 3, 6, 5, 4], [1, 2, 4, 3, 5, 6], ...]\n", "def test_stirling_numbers():\n    assert stirling_numbers(1, 0) == 1\n    assert stirling_numbers(1, 1) == 0\n    assert stirling_numbers(2, 0) == 1\n    assert stirling_numbers(2, 1) == 1\n    assert stirling_numbers(2, 2) == 1\n    assert stirling_numbers(3, 0) == 1\n    assert stirling_numbers(3, 1) == 3\n    assert stirling_numbers(3, 2) == 3\n    assert stirling_numbers(3, 3) == 1\n\n\n"], "sample_618": ["def test_cross_array_like_input(use_dask: bool) -> None:\n    a = np.array([1, 2, 3])\n    b = np.array([4, 5, 6])\n    expected = np.cross(a, b)\n    if use_dask:\n        if not has_dask:\n            pytest.skip(\"test for dask.\")\n        a = xr.DataArray(a)  \n        b = xr.DataArray(b) \n    actual = xr.cross(a, b)\n    xr.testing.assert_allclose(expected, actual)\n\n\n\n", "def test_promote_like_with_scalar_data_array() -> None:\n    # GH 3918: handle scalar DataArrays\n    a = xr.DataArray(1, dims=\"x\")\n    b = xr.DataArray([1, 2, 3], dims=\"y\")\n    xr.promote_like(a, b)  # should not raise error, promote a to match b\n", "def test_outer_product() -> None:\n    a = xr.DataArray([1, 2], dims=\"x\")\n    b = xr.DataArray([3, 4], dims=\"y\")\n    expected = xr.DataArray(\n        [[3, 4], [6, 8]], dims=(\"x\", \"y\")\n    )\n    actual = xr.outer_product(a, b)\n    xr.testing.assert_identical(expected, actual)\n\n\n\n", "    def test_cross_attrs(a, b, ae, be, dim: str, keep_attrs: bool, use_dask: bool) -> None:\n        if use_dask:\n            if not has_dask:\n                pytest.skip(\"requires dask\")\n        a = a.chunk()\n        b = b.chunk()\n\n        actual = xr.cross(a, b, dim=dim, keep_attrs=keep_attrs)\n        expected = np.cross(ae, be)\n        if keep_attrs:\n            assert actual.attrs == {\"a\": \"attr A\", \"b\": \"attr B\"}\n        \n        xr.testing.assert_duckarray_allclose(expected, actual)\n", "def test_broadcast_like_array_chunk_1D_missing_dims(use_dask: bool) -> None:\n    if use_dask:\n        if not has_dask:\n            pytest.skip(\"requires dask\")\n\n    a = xr.DataArray(np.arange(4), dims=\"x\")\n    b = xr.DataArray(np.arange(3), dims=\"y\")\n    c = xr.DataArray(np.arange(2), dims=\"z\")\n\n    if use_dask:\n        a = a.chunk({\"x\": 2})\n        b = b.chunk({\"y\": 2})\n        c = c.chunk({\"z\": 2})\n\n    with pytest.raises(ValueError, match=\"Input arrays must have matching shape\"):\n        xr.broadcast_like(a, b)\n    with pytest.raises(ValueError, match=\"Input arrays must have matching shape\"):\n        xr.broadcast_like(a, c)\n\n    a = xr.broadcast_like(a, c[...])\n    assert a.dims == (\"x\", \"z\")\n\n"], "sample_308": ["    def test_alternative_month_names(self):\n        my_birthday = datetime(1979, 7, 8, 22, 00)\n\n        self.assertEqual(dateformat.format(my_birthday, 'E'), 'Jul')  \n", "    def test_relative_time_format(self):\n        now = datetime.now()\n        future = now + timedelta(days=1)\n        past = now - timedelta(hours=2)\n        relative_future = datetime(2024, 1, 1)\n\n        self.assertEqual(dateformat.format(future, 'a'), 'tomorrow')  \n        self.assertEqual(dateformat.format(past, 'a'), 'yesterday') \n        self.assertEqual(dateformat.format(relative_future, 'a'), 'in 4 years') \n", "    def test_e_format_timezone_aware_datetime(self):\n        tz = get_fixed_timezone(-510)\n        dt = make_aware(datetime(2009, 5, 16, 5, 30, 30), tz)\n        self.assertEqual(dateformat.format(dt, 'e'), '-0530')\n", "    def test_weekday_formatting(self):\n        tests = [\n            (datetime(2009, 5, 16), 'Saturday'),\n            (datetime(2009, 5, 17), 'Sunday'),\n            (datetime(2009, 5, 18), 'Monday'),\n        ]\n        for dt, expected in tests:\n            with self.subTest(date=dt):\n                self.assertEqual(dateformat.format(dt, 'l'), expected)\n", "    def test_t_format_with_future_months(self):\n        self.assertEqual(dateformat.format(datetime(2023, 3, 1), 't'), '28')\n        self.assertEqual(dateformat.format(datetime(2023, 10, 1), 't'), '31') \n"], "sample_135": ["    def test_iso_8601_timezone_handling(self):\n        dt = datetime(2023, 10, 26, 10, 30, 0, tzinfo=get_default_timezone())\n\n        self.assertEqual(dateformat.format(dt, 'c'), '2023-10-26T10:30:00+00:00') # Using the default timezone\n        \n        # Test with a specific timezone\n        tz = get_fixed_timezone(-5 * 3600) \n        dt_aware_with_tz = make_aware(dt, tz)\n        self.assertEqual(dateformat.format(dt_aware_with_tz, 'c'), '2023-10-26T10:30:00-05:00') \n", "    def test_escape_literal_chars(self):\n        dt = datetime(2008, 5, 19, 11, 45, 23, 123456)\n        self.assertEqual(\n            dateformat.format(dt, r'\\t'), '\\t'\n        )\n        self.assertEqual(\n            dateformat.format(dt, r'\\\\'), '\\\\'\n        )\n\n\n\n", "    def test_day_of_year_non_leap(self):\n        self.assertEqual(dateformat.format(datetime(2001, 1, 1), 'z'), '001')\n\n\n", "    def test_escaped_characters(self):\n        self.assertEqual(dateformat.format(datetime(2009, 5, 16), '\\\\a'), '\\\\a')\n\n\n", "    def test_invalid_format_specifiers(self):\n        my_birthday = datetime(1979, 7, 8, 22, 00)\n\n        for specifier in ['X', 'x', 'c', ':', ';', '<', '>']:\n            msg = \"Invalid format specifier: '%s'\" % specifier\n            with self.assertRaisesMessage(ValueError, msg):\n                dateformat.format(my_birthday, specifier)\n\n\n\n        self.assertEqual(dateformat.format(my_birthday, 'I'), '1')\n\n\n\n        \n"], "sample_1051": ["def test_dotprint_with_custom_styles():\n    custom_styles = [(Basic, {'shape': 'diamond'}),\n                     (Expr, {'color': 'red'})]\n    text = dotprint(x + 2, styles=custom_styles)\n    assert 'shape=\"diamond\"' in text\n    assert 'color=\"red\"' in text\n\n\n\n", "def test_custom_style():\n    styles = [(Basic, {'color': 'red'}), (Expr, {'shape': 'square'})]\n    text = dotprint(x+2, styles=styles)\n    assert 'color=\"red\"' in text\n    assert 'shape=\"square\"' in text\n\n", "def test_custom_styles():\n    styles = [(Basic, {'color': 'green', 'shape': 'star'}),\n              (Expr,  {'color': 'red', 'shape': 'circle'})]\n    text = dotprint(x + 2, styles=styles)\n    assert 'color=\"green\"' in text\n    assert 'shape=\"star\"' in text\n    assert 'color=\"red\"' in text\n    assert 'shape=\"circle\"' in text\n", "def test_custom_styles():\n    styles = [(Basic, {'color': 'red', 'shape': 'box'}),\n              (Expr, {'color': 'blue', 'shape': 'circle'})]\n    text = dotprint(x + 2, styles=styles)\n    assert '\"Add(Integer(2), Symbol(\\'x\\'))_()\" [\"color\"=\"blue\", \"label\"=\"Add\", \"shape\"=\"circle\"];' in text\n    assert '\"Integer(2)_(0,)\" [\"color\"=\"red\", \"label\"=\"2\", \"shape\"=\"box\"];' in text\n    assert '\"Symbol(\\'x\\')_(1,)\" [\"color\"=\"blue\", \"label\"=\"x\", \"shape\"=\"circle\"];' in text \n", "def test_custom_styles():\n    styles = [(Basic, {'color': 'green', 'shape': 'diamond'}),\n              (Expr, {'color': 'red', 'shape': 'circle'})]\n    text = dotprint(x+2, styles=styles)\n    assert 'color=\"green\"' in text\n    assert 'shape=\"diamond\"' in text\n"], "sample_125": ["    def test_path_and_domain(self):\n        response = HttpResponse()\n        response.delete_cookie('c', path='/admin', domain='example.com')\n        cookie = response.cookies['c']\n        self.assertEqual(cookie['path'], '/admin')\n        self.assertEqual(cookie['domain'], 'example.com')\n", "    def test_delete_cookie_with_path(self):\n        response = HttpResponse()\n        response.set_cookie('c', path='/admin')\n        response.delete_cookie('c', path='/admin')\n        cookie = response.cookies.get('c')\n        self.assertIsNone(cookie)\n", "    def test_path(self):\n        response = HttpResponse()\n        response.set_cookie('c', path='/admin')\n        self.assertEqual(response.cookies['c']['path'], '/admin')\n", "    def test_delete_cookie_path(self):\n        response = HttpResponse()\n        response.set_cookie('c', path='/path1')\n        response.delete_cookie('c', path='/path2')\n        cookie = response.cookies.get('c')\n        self.assertEqual(cookie['path'], '/path2')\n", "    def test_delete_cookie_with_path(self):\n        response = HttpResponse()\n        response.set_cookie('c', path='/admin')\n        response.delete_cookie('c', path='/admin')\n        cookie = response.cookies['c']\n        self.assertEqual(cookie['expires'], 'Thu, 01 Jan 1970 00:00:00 GMT')\n        self.assertEqual(cookie['max-age'], 0)\n        self.assertEqual(cookie['path'], '/admin')\n\n\n\n"], "sample_1121": ["def test_issue_18507_continued():\n    assert Mul(zoo, 1, 0) is nan\n    assert Mul(1, zoo, 0) is nan\n    assert Mul(zoo, 1/2, 0) is nan\n    assert Mul(1/2, zoo, 0) is nan\n", "def test_issue_6611():\n    assert Mul.flatten([3**Rational(1, 3), Pow(-Rational(1, 9), Rational(2, 3))]) == \\\n        ([Rational(1, 3), (-1)**Rational(2, 3)], [], None)\n", "def test_issue_19760():\n    from sympy import sign\n    x = symbols('x')\n    assert sign(Mul(x, x, evaluate=False)) == sign(x**2)\n", "compilation error", "def test_issue_18507_cont():\n    assert Mul(zoo, x, evaluate=False) is None\n    assert Mul(zoo, x, evaluate=True) is nan \n"], "sample_1019": ["def test_gcd_terms_issue_7551():\n    f = (x**2 + 1)**3 + (x**2 + 1)**2*x**2\n    assert gcd_terms(f) == (x**2 + 1)**2 * (x**2 + 1 + x**2)\n\n", "def test_issue_8962():\n    from sympy import S\n    x, y = symbols('x y')\n    a = symbols('a', real=True)\n    eq = (x + y)**(2*a) * (x - y)**(-a)\n    assert factor_terms(eq).expand() == eq\n", "def test_issue_8611():\n    x, y, z = symbols('x, y, z')\n    expr = (x + y)*(x + z)*(y + z)\n    assert factor_nc(expr) == mul(x + y, x + z, y + z)\n    assert factor_nc(expr).as_expr() == expr\n\n", "compilation error", "def test_gcd_terms_with_special_functions():\n    from sympy import sin, cos, exp, sqrt, log\n    x, y = symbols('x y')\n    assert _gcd_terms(sin(x) + cos(x)) == (1, sin(x) + cos(x), 1)\n    assert _gcd_terms(exp(x) + exp(-x)) == (1, exp(x) + exp(-x), 1)\n    assert _gcd_terms(sqrt(x) + sqrt(y)) == (1, sqrt(x) + sqrt(y), 1)\n    assert _gcd_terms(log(x) + log(y)) == (1, log(x) + log(y), 1)\n    assert _gcd_terms(sqrt(x**2 + y)) == (1, sqrt(x**2 + y), 1)\n    assert _gcd_terms(sin(2*x) + cos(2*y)) == (1, sin(2*x) + cos(2*y), 1)\n    assert _gcd_terms(sin(exp(x)) + cos(exp(y))) == (1, sin(exp(x)) + cos(exp(y)), 1)\n"], "sample_1114": ["def test_issue_18088():\n    x = Symbol('x')\n    assert ImageSet(Lambda(x, x**2), Interval(-1, 1)) == Interval(0, 1)  \n    assert ImageSet(Lambda(x, x**2), Interval(-1, 0.5)) == Interval(0, 0.25) \n    assert ImageSet(Lambda(x, x**2), Interval(0, 1)) == Interval(0, 1) \n    assert ImageSet(Lambda(x, x**2), Interval(0, 0.5)) == Interval(0, 0.25) \n    r = Symbol('r')\n    assert ImageSet(Lambda(x, r*x**2), Interval(-1, 1)) == Interval(0, r) if r > 0 else Interval(0, 0) \n    assert ImageSet(Lambda(x, r*x**2), Interval(-1, 0.5)) == Interval(0, 0.25 * r) if r > 0 else Interval(0, 0) \n", "def test_issue_18395():\n    from sympy.sets import Union, FiniteSet\n\n    assert Union(FiniteSet(1, 2, 3), FiniteSet(4, 5)) == FiniteSet(1, 2, 3, 4, 5)\n", "def test_issue_17860():\n    r = Range(-oo, -1)\n    assert r[1:] == Range(-oo, -1)\n    assert r[:-1] == Range(-oo, -1)\n    r = Range(1, oo)\n    assert r[1:] == Range(2, oo)\n    assert r[:-1] == Range(1, oo)\n", "def test_issue_19002():\n    c1 = ComplexRegion(Interval(0, pi)*Interval(0, pi/2))\n    c2 = ComplexRegion(Interval(pi/2, pi)*Interval(0, pi))\n    assert c1.union(c2).measure == pi**2/2\n\n\n", "def test_issue_18227():\n    a = Interval(0, 1)\n    b = Interval(-1, 2)\n    c = ComplexRegion(a * b)\n    assert c.measure == 3\n    assert c.measure == simplify(c.measure)\n"], "sample_663": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_505": ["def test_epoch2num_with_tzinfo():\n    dt = datetime.datetime(1970, 1, 1, tzinfo=dateutil.tz.tzutc())\n    mdates._reset_epoch_test_example()\n    mdates.set_epoch('1970-01-01T00:00:00')\n    assert mdates.epoch2num(dt) == 1.0\n    assert mdates.num2epoch(1.0).tzinfo is None\n\n    mdates._reset_epoch_test_example()\n", "def test_monthlocator():\n    loc = mdates.MonthLocator()\n    dates = [datetime.datetime(2023, month, 1) for month in range(1, 13)]\n    locator = loc\n    locator.create_dummy_axis()\n    locator.axis.set_view_interval(mdates.date2num(dates[0]),\n                                   mdates.date2num(dates[-1]))\n    assert list(locator()) == [d.toordinal() for d in dates]\n", "compilation error", "compilation error", "def test_date2num_dst_transition():\n    # Test handling of DST transitions with date2num()\n\n    import pytz\n    naive_date_start = datetime.datetime(2017, 3, 11, 2, 0)\n    naive_date_end = datetime.datetime(2017, 3, 13, 2, 0)\n    tz = pytz.timezone('US/Eastern')\n    aware_start = tz.localize(naive_date_start)\n    aware_end = tz.localize(naive_date_end)\n\n    # Convert to ordinal\n    ordinal_start = mdates.date2num(aware_start)\n    ordinal_end = mdates.date2num(aware_end)\n\n    # Test that the dates are correctly represented as ordinals\n    assert isinstance(ordinal_start, float)\n    assert isinstance(ordinal_end, float)\n    assert ordinal_start + 1 == ordinal_end\n"], "sample_1110": ["def test_sympy_printing():\n    from sympy import exp\n\n    prntr = SymPyPrinter()\n    assert prntr.doprint(exp(x)) == 'sympy.exp(x)'\n\n\n", "def test_sympy_special_functions():\n    from sympy import gamma, gammainc, gammaincc,  digamma, lambertw, \\\n        erf, erfc, besselj, bessely, besseli, besselk, risingfactorial, \\\n        eval_chebyt, eval_chebyu, eval_legendre, eval_hermite, \\\n        eval_laguerre\n\n    prntr = SymPyPrinter()\n    assert prntr.doprint(gamma(x)) == 'sympy.gamma(x)'\n    assert prntr.doprint(gammainc(x, y)) == 'sympy.gammainc(x, y)'\n    assert prntr.doprint(gammaincc(x, y)) == 'sympy.gammaincc(x, y)'\n    assert prntr.doprint(digamma(x)) == 'sympy.psi(x)'\n    assert prntr.doprint(lambertw(x)) == 'sympy.lambertw(x)'\n    assert prntr.doprint(erf(x)) == 'sympy.erf(x)'\n    assert prntr.doprint(erfc(x)) == 'sympy.erfc(x)'\n\n    assert prntr.doprint(besselj(x, y)) == 'sympy.besselj(x, y)'\n    assert prntr.doprint(bessely(x, y)) == 'sympy.bessely(x, y)'\n    assert prntr.doprint(besseli(x, y)) == 'sympy.besseli(x, y)'\n    assert prntr.doprint(besselk(x, y)) == 'sympy.besselk(x, y)'\n    assert prntr.doprint(risingfactorial(x, y)) == 'sympy.poch(x, y)'\n    assert prntr.doprint(eval_chebyt(x, y)) == 'sympy.eval_chebyt(x, y)'\n    assert prntr.doprint(eval_chebyu(x, y)) == 'sympy.eval_chebyu(x, y)'\n    assert prntr.doprint(eval_legendre(x, y)) == 'sympy", "compilation error", "def test_sympy_printing():\n\n    from sympy import Matrix, Symbol, zeros, ones, eye, diff, Function, simplify\n\n    x, y = Symbol('x y')\n\n    expr1 = Matrix([[x, y], [y, x]])\n    expr2 = zeros(3, 4)\n    expr3 = ones(2, 2)\n    expr4 = eye(3)\n    expr5 = diff(x**2 + y**2, x)\n    expr6 = Function('f')(x)\n\n    prntr = SymPyPrinter()\n    assert prntr.doprint(expr1)  == 'Matrix([[x, y], [y, x]])'\n    assert prntr.doprint(expr2)  == 'zeros(3, 4)'\n    assert prntr.doprint(expr3) == 'ones(2, 2)'\n    assert prntr.doprint(expr4) == 'eye(3)'\n    assert prntr.doprint(expr5) == '2*x'\n    assert prntr.doprint(expr6) == 'f(x)'\n\n", "def test_lambertw():\n    from sympy import lambdify, lambertw\n\n    expr = lambertw(x, 0)\n    func = lambdify(x, expr, \"scipy\")\n\n    assert func(0) == 0.0\n"], "sample_172": ["    def test_related_field_widget_bulk_edit(self):\n        self.admin_login(username='super', password='secret', login_url='/')\n        self.selenium.get(self.live_server_url + reverse('admin:admin_widgets_profile_changelist'))\n\n        # Select multiple profiles\n        self.selenium.find_elements_by_css_selector('input[type=\"checkbox\"]')[0].click()\n        self.selenium.find_elements_by_css_selector('input[type=\"checkbox\"]')[1].click()\n\n        # Open the bulk edit form\n        self.selenium.find_element_by_id('changelist-bulk-edit').click()\n\n        # Set the 'user' field value\n        value_select = self.selenium.find_element_by_id('bulk_user')\n        value_select.find_element_by_css_selector('option[value=\"newuser\"]').click()\n\n        # Save the changes\n        self.selenium.find_element_by_css_selector('button.submit').click()\n\n        # Wait for a success message\n        self.wait_for_text('li.success', 'The 2 selected profiles were changed successfully.')\n\n        # Load the profiles again to verify the change\n        self.selenium.get(self.live_server_url + reverse('admin:admin_widgets_profile_changelist'))\n        profiles = Profile.objects.all()\n\n        # Verify that all selected profiles now have the new user\n        for profile in profiles:\n            self.assertEqual(profile.user.username, 'newuser')\n\n\n\n", "    def test_ManyToManyField(self):\n        self.admin_login(username='super', password='secret', login_url='/')\n        self.selenium.get(self.live_server_url + reverse('admin:admin_widgets_project_add'))\n\n        main_window = self.selenium.current_window_handle\n        # Click the Add Band button to add new band\n        self.selenium.find_element_by_id('add_id_bands').click()\n        self.wait_for_and_switch_to_popup()\n        band_name_field = self.selenium.find_element_by_id('id_name')\n        band_name_field.send_keys('New Rock Band')  \n\n        save_button_css_selector = '.submit-row > input[type=submit]'\n        self.selenium.find_element_by_css_selector(save_button_css_selector).click()\n        self.selenium.switch_to.window(main_window)\n        self.selenium.find_element_by_css_selector('#id_bands option[value=\"New Rock Band\"]')\n\n\n", "    def test_ToManyField(self):\n        self.admin_login(username='super', password='secret', login_url='/')\n        self.selenium.get(self.live_server_url + reverse('admin:admin_widgets_profile_add'))\n        main_window = self.selenium.current_window_handle\n\n        # Click the Add Tag button to add new tags\n        self.selenium.find_element_by_id('add_id_tags').click()\n        self.wait_for_and_switch_to_popup()\n        tag1_field = self.selenium.find_element_by_id('id_tags_to_name')\n        tag1_field.send_keys('tag1')\n        tag1_field.find_element_by_tag_name('button').click()\n        self.selenium.switch_to.window(main_window)\n        # The field now contains the new tag\n        self.selenium.find_element_by_css_selector('#id_tags option[value=\"tag1\"]')\n\n        # Add another tag\n        self.selenium.find_element_by_id('add_id_tags').click()\n        self.wait_for_and_switch_to_popup()\n        tag2_field = self.selenium.find_element_by_id('id_tags_to_name')\n        tag2_field.send_keys('tag2')\n        tag2_field.find_element_by_tag_name('button').click()\n        self.selenium.switch_to.window(main_window)\n        self.selenium.find_element_by_css_selector('#id_tags option[value=\"tag2\"]')\n\n        # Go ahead and submit the form to make sure it works\n        save_button_css_selector = 'input[type=submit]'\n        self.selenium.find_element_by_css_selector(save_button_css_selector).click()\n        self.wait_for_text('li.success', 'The profile was added successfully.')\n        profiles = Profile.objects.all()\n        self.assertEqual(len(profiles), 1)\n        self.assertEqual(set(profiles[0].tags.all().values_list('name", "    def test_ManyToMany_using_to_field(self):\n        self.admin_login(username='super', password='secret', login_url='/')\n        self.selenium.get(self.live_server_url + reverse('admin:admin_widgets_profile_add'))\n\n        main_window = self.selenium.current_window_handle\n        # Click Add Band button to add new band selection to the popup\n        self.selenium.find_element_by_id('add_id_band').click()\n        self.wait_for_and_switch_to_popup()\n\n        band_search_field = self.selenium.find_element_by_id('search_band')\n        band_search_field.send_keys('Bogey Blues')\n        band_results = self.selenium.find_elements_by_css_selector('#search_results > li')\n        band_results[0].click() \n\n        # Switch back to the main window and check the to_field\n        self.selenium.switch_to.window(main_window)\n        self.wait_for_value('#id_band', '42')\n\n        # Click the Add Band button again\n        self.selenium.find_element_by_id('add_id_band').click()\n        self.wait_for_and_switch_to_popup()\n\n        band_search_field = self.selenium.find_element_by_id('search_band')\n        band_search_field.send_keys('Green Potatoes')\n        band_results = self.selenium.find_elements_by_css_selector('#search_results > li')\n        band_results[0].click()\n\n        # Switch back to the main window and check the to_field\n        self.selenium.switch_to.window(main_window)\n        self.wait_for_value('#id_band', '42,98')\n", "    def test_change_many_to_many_relationship(self):\n        self.admin_login(username='super', password='secret', login_url='/')\n        self.selenium.get(self.live_server_url + reverse('admin:admin_widgets_profile_change', args=(self.profile.id,)))\n\n        # Add tags\n        self.selenium.find_element_by_id('add_id_tags').click()\n        self.wait_for_and_switch_to_popup()\n        for tag in ['python', 'django', 'selenium']:\n            self.selenium.find_element_by_link_text(tag).click()\n        self.selenium.switch_to.window(main_window)\n\n        # Confirm tags are selected\n        self.assertTrue(all(tag in self.selenium.find_element_by_id('id_tags').text for tag in ['python', 'django', 'selenium']))\n\n        # Remove a tag\n        self.selenium.find_element_by_id('remove_id_tags').click()\n\n        # Confirm tag is removed\n        self.assertTrue('python' not in self.selenium.find_element_by_id('id_tags').text)\n\n        # Select the tag again\n        self.selenium.find_element_by_id('add_id_tags').click()\n        self.wait_for_and_switch_to_popup()\n        self.selenium.find_element_by_link_text('python').click()\n        self.selenium.switch_to.window(main_window)\n\n        # Confirm tag is selected again\n        self.assertTrue('python' in self.selenium.find_element_by_id('id_tags').text)\n\n\n\n"], "sample_1048": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_848": ["def test_multi_output_predict_proba_with_different_number_of_classes():\n    # Test predict_proba with data having different number of classes\n    # for each output\n    X, y = make_classification(n_samples=100,\n                               n_features=10,\n                               n_classes=[2, 3, 4],\n                               random_state=0)\n\n    clf = MultiOutputClassifier(LogisticRegression())\n    clf.fit(X, y)\n\n    y_proba = clf.predict_proba(X)\n    assert len(y_proba) == 3\n    for i in range(3):\n        assert y_proba[i].shape == (100, 2) if len(classes[i]) == 2 else \\\n            (100, 3) if len(classes[i]) == 3 else (100, 4)\n\n\n", "def test_multi_output_classification_partial_fit_weights():\n    # weighted classifier\n    Xw = [[1, 2, 3], [4, 5, 6], [1.5, 2.5, 3.5]]\n    yw = [[3, 2], [2, 3], [3, 2]]\n    w = np.asarray([2., 1., 1.])\n    sgd_linear_clf = SGDClassifier(random_state=1, max_iter=20)\n\n    multi_target_linear = MultiOutputClassifier(sgd_linear_clf)\n    multi_target_linear.partial_fit(Xw[:2], yw[:2], w[:2])\n    multi_target_linear.partial_fit(Xw[2:], yw[2:], w[2:])\n\n    # unweighted, but with repeated samples\n    X = [[1, 2, 3], [1, 2, 3], [4, 5, 6], [1.5, 2.5, 3.5]]\n    y = [[3, 2], [3, 2], [2, 3], [3, 2]]\n    sgd_linear_clf = SGDClassifier(random_state=1, max_iter=20)\n    clf = MultiOutputClassifier(sgd_linear_clf)\n    clf.fit(X, y)\n    X_test = [[1.5, 2.5, 3.5]]\n    assert_array_almost_equal(clf.predict(X_test), multi_target_linear.predict(X_test))\n\n\n\n\n\n", "def test_multi_output_predict_proba_exceptions():\n    # Test for exceptions raised when predict_proba is called on estimators \n    # that don't support it.\n    sgd_linear_clf = SGDClassifier(random_state=1, max_iter=5)\n    multi_target_linear = MultiOutputClassifier(sgd_linear_clf)\n    multi_target_linear.fit(X, y)\n\n    assert_raises( AttributeError, multi_target_linear.predict_proba, X)\n\n    forest = RandomForestClassifier(n_estimators=10, random_state=1)\n    multi_target_forest = MultiOutputClassifier(forest)\n    multi_target_forest.fit(X, y)\n\n    assert not hasattr(multi_target_forest, 'predict_proba')  \n", "compilation error", "def test_multi_output_classes_with_sgd(estimator):\n    estimator.fit(X, y)\n    assert isinstance(estimator.classes_, list)\n    assert len(estimator.classes_) == n_outputs\n    for estimator_classes, expected_classes in zip(classes,\n                estimator.classes_):\n        assert_array_equal(estimator_classes, expected_classes)\n"], "sample_755": ["compilation error", "compilation error", "def test_check_number_of_labels_error():\n    \n    check_number_of_labels(2, 10)\n\n    with pytest.raises(ValueError) as e:\n        check_number_of_labels(100, 10)\n\n    with pytest.raises(ValueError) as e:\n        check_number_of_labels(10, 100)\n", "compilation error", "compilation error"], "sample_72": ["    def test_serialize_custom_field(self):\n        class MyCustomField(models.Field):\n                super().__init__(*args, **kwargs)\n\n                return 'MyCustomField', [], {}\n\n                return forms.CharField(**kwargs)\n\n                pass\n\n        field = MyCustomField()\n        string = MigrationWriter.serialize(field)[0]\n        self.assertEqual(string, \"MyCustomField()\")\n\n", "    def test_migrate_model_inheritance(self):\n        class ParentModel(models.Model):\n            pass\n\n        class ChildModel(ParentModel):\n            field = models.IntegerField()\n\n        migration = type(\"Migration\", (migrations.Migration,), {\n            \"operations\": [\n                migrations.CreateModel(\"ChildModel\", (ParentModel, ), {}, (ChildModel,)),\n            ]\n        })\n        writer = MigrationWriter(migration)\n        output = writer.as_string()\n        self.assertIn('ParentModel', output)\n", "    def test_serialize_custom_field(self):\n        class MyCustomField(models.Field):\n                super().__init__(*args, **kwargs)\n                return 'MyCustomDbType'\n                return django.forms.Field(widget=models.CharField)\n\n        class MyModel(models.Model):\n            my_field = MyCustomField()\n\n        migration = type(\"Migration\", (migrations.Migration,), {\n            \"operations\": [\n                migrations.CreateModel(\"MyModel\", (\n                    ('id', models.AutoField(primary_key=True)),\n                    ('my_field', MyModel.my_field),\n                ), {}, (models.Model,)),\n            ]\n        })\n        writer = MigrationWriter(migration)\n        output = writer.as_string()\n        self.assertIn('MyCustomDbType', output)\n        self.assertIn('MyCustomField', output)\n", "    def test_serialize_path_with_spaces(self):\n        \"\"\"\n        Tests that paths with spaces in them are serialized correctly.\n        #22723\n        \"\"\"\n        migration = type(\"Migration\", (migrations.Migration,), {\n            \"operations\": [],\n            \"path\": \"this/is/a/path with spaces\"\n        })\n        writer = MigrationWriter(migration)\n        output = writer.as_string()\n        self.assertIn('path = \"this/is/a/path with spaces\"', output)\n\n\n\n", "    def test_skip_fields(self):\n        class MyModel(models.Model):\n            field1 = models.CharField(max_length=255)\n            field2 = models.IntegerField()\n            field3 = models.DateTimeField()\n\n        migration = type(\"Migration\", (migrations.Migration,), {\n            \"operations\": [\n                migrations.AlterField(\n                    model_name='MyModel',\n                    old_field='field1',\n                    new_field=models.CharField(max_length=500),\n                )\n            ],\n            \"dependencies\": []\n        })\n        writer = MigrationWriter(migration)\n        output = writer.as_string()\n        self.assertIn(\"migrations.AlterField\", output)\n        self.assertIn('old_field=\\'field1\\'', output)\n        self.assertIn('new_field=models.CharField(max_length=500)', output)\n"], "sample_483": ["    def test_list_filter_with_fk_nonexistent_field(self):\n        class BookAdmin(admin.ModelAdmin):\n            list_filter = [\"nonexistent_fk\"]\n\n        errors = BookAdmin(Book, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"The value of 'list_filter' refers to 'nonexistent_fk', which is not a field \"\n                \"of 'admin_checks.Book'.\",\n                obj=BookAdmin,\n                id=\"admin.E011\",\n            )\n        ]\n        self.assertEqual(errors, expected)\n\n\n\n", "    def test_list_filter_works_on_through_field_case_insensitive(self):\n        \"\"\"\n        Ensure list_filter can access reverse fields (case-insensitively)\n        even when the app registry is not ready; refs #24146.\n        \"\"\"\n\n        class BookAdminWithListFilter(admin.ModelAdmin):\n            list_filter = [\"Authorsbooks__featured\"]\n\n        # Temporarily pretending apps are not ready yet. This issue can happen\n        # if the value of 'list_filter' refers to a 'through__field'.\n        Book._meta.apps.ready = False\n        try:\n            errors = BookAdminWithListFilter(Book, AdminSite()).check()\n            self.assertEqual(errors, [])\n        finally:\n            Book._meta.apps.ready = True\n\n", "    def test_list_filter_works_with_foreign_key_to_another_model(self):\n        class GenreAdmin(admin.ModelAdmin):\n            list_filter = [\"songs__title\"]\n\n        errors = GenreAdmin(Genre, AdminSite()).check()\n        self.assertEqual(errors, [])\n", "    def test_list_filter_on_manytomany_relationship_with_related_manager(self):\n        class AlbumAdmin(admin.ModelAdmin):\n            list_filter = [\"songs__artist\"]\n\n        errors = AlbumAdmin(Album, AdminSite()).check()\n        self.assertEqual(errors, [])\n\n\n", "    def test_list_filter_works_on_through_field_even_when_apps_not_ready(self):\n        \"\"\"\n        Ensure list_filter can access reverse fields even when the app registry\n        is not ready; refs #24146.\n        \"\"\"\n\n        class BookAdminWithListFilter(admin.ModelAdmin):\n            list_filter = [\"authorsbooks__featured\"]\n\n        # Temporarily pretending apps are not ready yet. This issue can happen\n        # if the value of 'list_filter' refers to a 'through__field'.\n        Book._meta.apps.ready = False\n        try:\n            errors = BookAdminWithListFilter(Book, AdminSite()).check()\n            self.assertEqual(errors, [])\n        finally:\n            Book._meta.apps.ready = True\n"], "sample_958": ["def test_domain_cpp_parse_missing_target(app, warning):\n    text = \".. cpp:function:: missing_target()\"\n    doctree = restructuredtext.parse(app, text)\n    ws = warning.getvalue().split(\"\\n\")\n    assert len(ws) == 1\n    assert \"WARNING: cpp: undefined target: missing_target\" in ws[0]\n", "def test_domain_cpp_parse_implicit_return(app):\n    text = (\".. cpp:function:: void f()\\n\"\n            \"   :return: int\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index, desc, addnodes.index, desc))\n    assert_node(doctree[0], addnodes.index, entries=[('single', 'f (C++ function)', '_CPPv41fv', '', None)])\n    assert_node(doctree[0], addnodes.index, entries=[('single', 'f (C++ function)', '_CPPv41fv', '', {'return': 'int'})])\n\n", "    def check_warning(text, file, message):\n        parser = DefinitionParser(text, location=None)\n        doctree = parser.parse()\n        assert len(parser.warnings) == 1\n        assert parser.warnings[0].message == message", "def test_domain_cpp_parse_xref_target_attr(app, warning):\n    text = (\".. cpp:function:: void f()\\n\"\n            \"   :target: target_f\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index, desc, addnodes.index, desc))\n    assert_node(doctree[0], addnodes.index, entries=[('single', 'f (C++ function)', '_CPPv41fv', '', None)])\n    assert_node(doctree[1], addnodes.desc, refuri='target_f')\n", "def test_domain_cpp_parse_function_param_order(app, warning):\n    text = (\".. cpp:function:: void f(int a, int b)\\n\"\n            \".. cpp:function:: void g(int b, int a)\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index, desc, addnodes.index, desc))\n    assert_node(doctree[0], addnodes.index, entries=[('single', 'f (C++ function)', '_CPPv41fv', '', None)])\n    assert_node(doctree[2], addnodes.index, entries=[('single', 'g (C++ function)', '_CPPv41gv', '', None)])\n"], "sample_540": ["def test_animation_repeated_save():\n    fig, ax = plt.subplots()\n    line, = ax.plot([], [])\n\n        line.set_data([], [])\n        return line,\n\n        line.set_data(np.linspace(0, 10, 100), np.sin(np.linspace(0, 10, 100) + i))\n        return line,\n\n    anim = animation.FuncAnimation(fig, animate, init_func=init, frames=5, blit=True, repeat=True)\n    writer = NullMovieWriter()\n    \n\n    # Save the animation twice\n    anim.save(\"test.gif\", writer=writer, fps=30)\n    anim.save(\"test.gif\", writer=writer, fps=30) \n\n\n\n", "def test_movie_writer_invalid_args(tmpdir, writer):\n    anim = animation.FuncAnimation(plt.figure(), lambda _: None)\n    with pytest.raises(ValueError):\n        anim.save(tmpdir.join('test.gif'), writer=writer,\n                  invalid_arg='this_should_fail')\n\n", "def test_animation_empty_frame_data(anim):\n    fig, ax = plt.subplots()\n    line, = ax.plot([], [])\n        return line,\n    anim = animation.FuncAnimation(fig, animate, init_func=lambda: line,\n                                       frames=range(5), blit=True)\n    with pytest.raises(RuntimeError) as e:\n        anim._draw_frame(None)\n    assert str(e.value) == \"The animate function should return a sequence of Artists\"\n", "def test_movie_writer_save_frame_format(tmpdir, writer, anim):\n    if writer == 'pillow':\n        from PIL import Image\n\n            if not isinstance(frame, Image.Image):\n                raise AssertionError(\n                    f\"Expected Image object, got {type(frame)}\"\n                )\n            assert frame.mode == expected_mode\n\n    with tmpdir.as_cwd():\n        with plt.rc_context({'animation.writer': writer}):\n            anim = animation.FuncAnimation(\n                plt.figure(), lambda x: None, frames=1\n            )\n            anim.save('test.gif', fps=10)\n            if writer == 'pillow':\n                with Image.open('test.gif') as im:\n                    compare_frame_format(im, 'RGBA')\n\n", "def test_save_animation_with_extra_args(tmpdir, writer, anim):\n    with tmpdir.as_cwd():\n        anim.save('test.' + writer, writer=writer,\n                  extra_args=['--extra_arg'])\n    # Verify that extra args were passed correctly\n    # (this is implementation-dependent, so we just check for\n    # the existence of the extra arg in the output filename)\n    # ----------------------------------------\n    # Note: This assumes that the chosen writer will\n    # include the extra_arg in its output filename.\n    # This might not be true for all writers, and\n    # further testing may be needed.\n    # ----------------------------------------\n\n    # ... Add assertion to verify that the extra_arg is in the output filename ...\n"], "sample_501": ["def test_legend_label_positioning_with_bbox_to_anchor():\n    fig, ax = plt.subplots()\n    ax.plot([0, 1], [0, 1], label=\"line\")\n    leg = ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n    assert leg.get_bbox_to_anchor() == (1.05, 1)\n\n\n\n", "def test_legend_label_sorting():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3], label='3')\n    ax.plot([4, 5, 6], label='1')\n    ax.plot([7, 8, 9], label='2')\n    \n    # Test default sorting\n    leg = ax.legend()\n    labels = [t.get_text() for t in leg.get_texts()]\n    assert labels == ['1', '2', '3']\n\n    # Test custom sorting\n    ax.legend(title='Custom Legend', labels=['b', 'c', 'a'])\n    leg = ax.legend()\n    labels = [t.get_text() for t in leg.get_texts()]\n    assert labels == ['a', 'b', 'c']\n", "    def test_legend_shadow_transparent():\n        fig, ax = plt.subplots()\n        ax.plot([0, 1], [0, 1], label=\"line\")\n        leg = ax.legend(shadow=True, facecolor='w', framealpha=0.5)\n        assert leg.get_frame().get_alpha() == 0.5\n\n\n", "def test_legend_with_custom_handler():\n    from matplotlib.patches import Circle\n    class CustomHandler(LegendHandler):\n                           xdescent, ydescent):\n            circle = Circle((0, 0), 0.5, color=orig_handle.get_color(),\n                           label=orig_handle.get_label())\n            return [circle]\n\n    fig, ax = plt.subplots()\n    ax.plot([0, 1], [0, 1], label=\"line\", color='red')\n    handles = [ax.plot([0, 1], [0, 1], label=\"line\")[0]]\n    leg = ax.legend(handles, title='My legend', handler_map=\n                    {'line2d': CustomHandler})\n    assert all([isinstance(h, Circle) for h in leg.legendHandles])\n", "def test_legend_sticky_placement():\n    fig, ax = plt.subplots()\n    ax.plot([0, 1], [0, 1], label='line')\n    \n    # Test that sticky placement works as expected\n    leg = ax.legend(loc='upper left', bbox_to_anchor=(0, 1.2))\n    assert leg.get_bbox().is_valid()\n    \n    # Test with other loc options\n    leg = ax.legend(loc='lower right', bbox_to_anchor=(1.2, 0),\n                    sticky=\"out\")\n    assert leg.get_bbox().is_valid() \n"], "sample_652": ["compilation error", "compilation error", "def test_fixture_indirect_call_error(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture\n            return 1\n\n            request.fixture('fix')\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*Error*test_indirect_call*Cannot directly call fixture functions*\",\n        ]\n    )\n    assert result.ret != 0\n", "compilation error", "compilation error"], "sample_76": ["    def test_language_settings_consistent_with_unicode_language(self):\n        with self.settings(LANGUAGE_CODE='zh-Hans', LANGUAGES=[('en', 'English'), ('zh-Hans', '\u4e2d\u6587')]):\n            self.assertEqual(check_language_settings_consistent(None), [])\n", "    def test_valid_language_code_in_languages(self):\n        for tag in self.valid_tags:\n            with self.subTest(tag), self.settings(LANGUAGE_CODE=tag, LANGUAGES=[(tag, tag)]):\n                self.assertEqual(check_language_settings_consistent(None), [])\n", "    def test_language_settings_consistent_with_en_us(self):\n        with self.settings(LANGUAGE_CODE='en-us', LANGUAGES=[('en-US', 'English')]):\n            self.assertEqual(check_language_settings_consistent(None), [])\n", "    def test_language_code_missing_in_languages(self):\n        msg = (\n            'You have provided a value for the LANGUAGE_CODE setting that is '\n            'not in the LANGUAGES setting.'\n        )\n        with self.settings(LANGUAGE_CODE='invalid', LANGUAGES=[]):\n            self.assertEqual(check_language_settings_consistent(None), [\n                Error(msg, id='translation.E004'),\n            ])\n", "    def test_language_settings_consistent_with_default(self):\n        with self.settings(LANGUAGE_CODE='en', LANGUAGES=[('en', 'English'),]):\n            self.assertEqual(check_language_settings_consistent(None), [])\n"], "sample_701": ["def test_deprecated_argument_type_str_warning(old_arg, new_arg, pytester: Pytester) -> None:\n    pytester.makeconftest(f\"\"\"\n            parser.addoption(\n                \"--{old_arg}\",\n                type='int',\n                help='{old_arg} option',\n            )\n    \"\"\")\n    with pytest.warns(UnformattedWarning, match=re.escape(f\"Argument `{old_arg}`\")):\n        pytester.runpytest()\n", "def test_argument_type_str_choice_deprecation(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n        from _pytest.config import Config\n\n            parser.addoption(\n                \"--my_choice\",\n                choices=[\"a\", \"b\", \"c\"],\n                type=\"str\",  # deprecated type argument\n            )\n    \"\"\"\n    )\n    with pytest.warns(PytestDeprecationWarning, match=r\"choice.*string\"):\n        pytester.runpytest([\"--my_choice=d\"])\n", "def test_argument_percent_default(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import argparse\n\n        parser = argparse.ArgumentParser()\n        parser.add_argument('-f', '--foo', type=str, default='%default')\n        args = parser.parse_args()\n", "def test_argument_percent_default_is_deprecated(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n        import argparse\n\n            parser.add_argument(\n                \"--arg\",\n                type=str,\n                default=\"%default\",  # Deprecated\n            )\n        \"\"\"\n    )\n    result = pytester.runpytest(\"--arg\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestDeprecationWarning: pytest now uses argparse. \\\"%\\default\\\" should be changed to \\\"%(default)s\\\"*\",\n        ]\n    )\n\n\n\n", "def test_argument_percent_default_deprecated(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n        import argparse\n\n        parser = argparse.ArgumentParser()\n\n            parser.add_argument(\"--foo\", default=\"%default\", type=str)\n            return parser.parse_args(args)\n    \n        argparse_mod = argparse\n    \n            config.pluginmanager.register(parse_args)\n\n        \"\"\"\n    )\n\n    result = pytester.runpytest(\"--foo\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestDeprecationWarning: pytest now uses argparse.*\",\n            \"*'%\"\n            \"default' should be changed to '%(default)s'\",\n        ]\n    )\n\n\n\n"], "sample_529": ["def test_legend_title_fontproperties_fontsize_with_scalar():\n    fig, ax = plt.subplots()\n    ax.plot([0, 1], [0, 1], label='test')\n    leg = ax.legend(title='Aardvark', title_fontsize=14)\n    title = leg.get_title()\n    assert title.get_fontsize() == 14\n\n", "def test_legend_title_fontproperties():\n    # Test that title fontproperties are applied correctly.\n    fig, ax = plt.subplots()\n    ax.plot(range(10), label='Aardvark')\n    legend = ax.legend(title=\"A Title\", title_fontproperties={'size': 18, 'family': 'serif'})\n    assert legend.get_title().get_fontsize() == 18\n    assert legend.get_title().get_fontname() == 'serif'\n", "def test_bbox_to_anchor_in_subplots():\n    fig, axes = plt.subplots(2, 2)\n    axes[0, 0].plot([1, 2, 3])\n    axes[0, 1].plot([1, 2, 3])\n\n    # Test with a location that should be within the bounds of the subfigure\n    legend = axes[0, 0].legend(loc='upper left', bbox_to_anchor=(0.5, 1.1))\n\n    # Test with a location that should be outside the bounds of the subfigure\n    with pytest.raises(RuntimeError):\n        legend = axes[0, 0].legend(loc='upper left', bbox_to_anchor=(1.2, 1.1))\n    # Test with bbox_to_anchor set to None (default)\n    legend = axes[0, 0].legend(loc='upper left')\n", "def test_legend_handles_with_artist_with_no_data():\n    # Test legend handles are created even if the artist doesn't have data (#12490)\n    fig, ax = plt.subplots()\n    line = mlines.Line2D([], [], color='red', label='line')\n    handles = [line]\n    leg = ax.legend(handles=handles)\n    assert len(leg.legendHandles) == 1 \n", "def test_legend_loc_outside():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2], [3, 4], label='line')\n    with pytest.raises(ValueError):\n        ax.legend(loc='outside')\n"], "sample_812": ["def test_nested_list_params():\n    param_grid = {'param1': [1, 2, [3, 4]],\n                  'param2': [5, [6, 7]]}\n    gs = GridSearchCV(SVC(), param_grid)\n    expected = \"\"\"", "def test_custom_repr_with_estimator():\n    class MyEstimator(BaseEstimator):\n            self.param1 = param1\n            self.param2 = param2\n\n            return f\"MyEstimator(param1={self.param1}, param2={self.param2})\"\n\n    estimator = MyEstimator(param1=\"value1\", param2=\"value2\")\n\n    expected = \"MyEstimator(param1=value1, param2=value2)\"\n    assert estimator.__repr__() == expected\n\n    # Check that the customized repr is used by the internal\n    # pretty printer\n    pp = _EstimatorPrettyPrinter(compact=True, indent=1, indent_at_name=True)\n    assert pp.pformat(estimator) == expected\n", "def test_empty_params():\n    lr = LogisticRegression(C=1.0, random_state=42)\n    expected = \"\"\"LogisticRegression(C=1.0, class_weight=None, dual=False,\n                           fit_intercept=True, intercept_scaling=1,\n                           l1_ratio=None, max_iter=100,\n                           multi_class='warn', n_jobs=None, penalty='l2',\n                           random_state=42, solver='warn', tol=0.0001,\n                           verbose=0, warm_start=False)\"\"\"\n\n    assert lr.__repr__() == expected\n", "def test_n_components_handling():\n    pca = PCA(n_components=2)\n    expected = \"\"\"", "def test_unicode_repr():\n    lr = LogisticRegression(class_weight={'foo': 1, '\u65e5\u672c\u8a9e': 2})\n    expected = \"\"\""], "sample_38": ["compilation error", "def test_wcs_with_null_crval():\n    \"\"\"\n    Test handling of WCS objects with NaN values for CRVAL. Fixes #8144.\n    \"\"\"\n    hdr = get_pkg_data_contents(\"data/null_crval.fits\")\n    w = wcs.WCS(hdr)\n    assert np.isnan(w.wcs.crval[0])\n    assert np.isnan(w.wcs.crval[1])\n\n    ra, dec = w.wcs_pix2world([100, 100], 0)\n    assert np.isnan(ra)\n    assert np.isnan(dec)\n\n    ra2, dec2 = w.wcs_world2pix([100, 100], 0)\n    assert np.isnan(ra2)\n    assert np.isnan(dec2)\n\n", "def test_world2pix_error():\n    w = wcs.WCS(naxis=2)\n    with pytest.raises(ValueError):\n        w.wcs_world2pix([1, 2, 3], 0)\n", "compilation error", "def test_wcs_type_checking():\n    \"\"\"\n    Test type checking in WCS object creation and methods.\n    \"\"\"\n    with pytest.raises(TypeError) as exc:\n        wcs.WCS(123)\n    assert exc.value.args[0] == \"'fits_header' must be a string or a list of strings\"\n\n\n    with pytest.raises(TypeError) as exc:\n        wcs.WCS(naxis=[1, 2, 3, 'a'])\n    assert exc.value.args[0] == \"'naxis' must be a list of integers\"\n\n\n\n    with pytest.raises(TypeError) as exc:\n        wcs.WCS(naxis=1, header=['a', 'b'])\n    assert exc.value.args[0] == \"'header' must be a fits.Header object or a string\"\n\n\n    with pytest.raises(TypeError) as exc:\n        wcs.WCS(header=123)\n    assert exc.value.args[0] == \"'header' must be a fits.Header object or a string\"\n\n\n    with pytest.raises(TypeError) as exc:\n        wcs.WCS(naxis=1, header='a',  fix=123)\n    assert exc.value.args[0] == \"'fix' must be a boolean\"\n\n\n    h = fits.Header()\n    w = wcs.WCS(h)\n    with pytest.raises(TypeError) as exc:\n        w.all_pix2world(\"a\", 1, 0)\n    assert exc.value.args[0] == \"'x' must be a scalar or a NumPy array-like object\"\n\n\n\n\n    with pytest.raises(TypeError) as exc:\n        w.all_pix2world([1, 2], \"a\", 0)\n    assert exc.value.args[0] == \"'y' must be a scalar or a NumPy array-like object\"\n\n\n\n    with pytest.raises(TypeError) as exc:\n        w.all_pix2world([1, 2], 1, \"a\")\n    assert exc.value.args[0] == \"'order' must be an integer\"\n\n\n\n\n    with pytest.raises(TypeError) as exc:\n        w.wcs_pix2"], "sample_735": ["compilation error", "compilation error", "def test_gaussian_mixture_missing_data():\n    rng = np.random.RandomState(0)\n    n_samples = 100\n    n_features = 5\n    n_components = 3\n\n    # Generate some data with missing values\n    X = rng.randn(n_samples, n_features)\n    X[::2, 1] = np.nan  # Introduce missing values in every other sample\n\n    for covar_type in COVARIANCE_TYPE:\n        gmm = GaussianMixture(n_components=n_components,\n                              covariance_type=covar_type, random_state=rng)\n        assert_raise_message(ValueError, \"The input data contains missing \"\n                             \"values.\", gmm.fit, X)\n\n\n\n        # Handle missing data\n        gmm = GaussianMixture(n_components=n_components,\n                              covariance_type=covar_type, random_state=rng,\n                              handle_missing='value')\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\", ConvergenceWarning)\n            gmm.fit(X)\n        \n\n\n\n", "compilation error", "def test_n_init_affecting_result():\n    rng = np.random.RandomState(0)\n    rand_data = RandomData(rng, scale=1)\n    n_components = rand_data.n_components\n    n_init_list = [1, 5, 10, 20]\n    X = rand_data.X['full']\n\n    for n_init in n_init_list:\n        gmm = GaussianMixture(n_components=n_components, n_init=n_init,\n                              max_iter=1, random_state=rng)\n        gmm.fit(X)\n        gmm_lowest_log_score = gmm.score(X)\n\n        for _ in range(n_init):\n            gmm = GaussianMixture(n_components=n_components, n_init=n_init,\n                                 max_iter=1, random_state=rng)\n            gmm.fit(X)\n            log_score = gmm.score(X)\n            assert_greater_equal(log_score, gmm_lowest_log_score)\n\n"], "sample_887": ["def test_calibration_with_random_state(method):\n    X, y = make_classification(n_samples=100, random_state=42)\n    clf = LogisticRegression(random_state=42)\n    ccvf = CalibratedClassifierCV(clf, method=method, cv=5, random_state=42)\n    ccvf.fit(X, y)\n    \n    # Test that setting random_state on cal_clf consistently reproduces results\n    ccvf2 = CalibratedClassifierCV(clf, method=method, cv=5, random_state=42)\n    ccvf2.fit(X, y)\n    assert_allclose(ccvf.calibrated_classifiers_[0].estimator.coef_, ccvf2.calibrated_classifiers_[0].estimator.coef_)\n", "def test_calibration_with_invalid_method(method):\n    with pytest.raises(ValueError, match=\"Invalid method\"):\n        CalibratedClassifierCV(estimator=LogisticRegression(), method=method + \"_invalid\")\n\n\n\n", "def test_calibration_with_prefit_estimator(data):\n    X, y = data\n    clf = LogisticRegression().fit(X, y)\n    calibrated_clf = CalibratedClassifierCV(clf)\n    calibrated_clf.fit(X, y)  # should not raise an error\n", "def test_calibrated_classifier_cv_early_stopping(method):\n    \"\"\"Check that `early_stopping` is working correctly.\"\"\"\n    X, y = make_classification(\n        n_samples=100, n_features=20, n_classes=2, random_state=42\n    )\n    estimator = LogisticRegression(\n        random_state=42, class_weight=\"balanced\", solver=\"lbfgs\"\n    )\n    calibrated_clf = CalibratedClassifierCV(estimator, method=method, cv=5, early_stopping=True)\n\n    # Set up early stopping parameters\n    calibrated_clf.fit(X, y)\n    assert calibrated_clf.base_estimator_ is not None\n    assert calibrated_clf.n_iter_ is not None and calibrated_clf.n_iter_ > 0\n    assert calibrated_clf.n_iter_ < calibrated_clf.max_iter_\n\n\n\n", "def test_calibration_with_already_calibrated_estimator(method):\n    # Check that passing already calibrated estimator to\n    # CalibratedClassifierCV works as expected.\n    X, y = make_classification(n_samples=10, n_features=5, n_classes=2, random_state=7)\n    clf = LogisticRegression()\n    clf_probs = clf.predict_proba(X)\n    clf_calib = CalibratedClassifierCV(\n        estimator=clf, method=method, cv=\"prefit\"\n    )\n    clf_calib.fit(X, y, clf_probs)\n\n    assert_allclose(clf_calib.predict_proba(X), clf_probs)\n\n"], "sample_325": ["    def test_render_with_non_string_field_values(self):\n        class MyForm(Form):\n            my_field = CharField()\n\n        form = MyForm(data={'my_field': 123})\n        self.assertHTMLEqual(form.as_p(), '<p><label for=\"id_my_field\">My field:</label> <input id=\"id_my_field\" name=\"my_field\" type=\"text\" value=\"123\"></p>')", "    def test_renderer_as_string(self):\n        form = Form(renderer='django.template.backends.django.DjangoTemplates')\n        self.assertEqual(form.renderer, get_default_renderer())\n", "    def test_field_errors_with_custom_renderer(self):\n        class CustomRenderer(DjangoTemplates):\n                result = super().render_field(field, **kwargs)\n                if field.errors:\n                    result += f'<span class=\"error\">{field.errors}</span>'\n                return result\n\n        renderer = CustomRenderer()\n        class MyForm(Form):\n            name = CharField(max_length=10, required=True)\n\n        form = MyForm({'name': ''})\n        form.renderer = renderer\n        self.assertHTMLEqual(form.as_p(),\n            '<p><label for=\"id_name\">Name:</label> <input id=\"id_name\" maxlength=\"10\" name=\"name\" type=\"text\"> <span class=\"error\">This field is required.</span></p>')\n", "    def test_custom_renderer_arguments(self):\n        class MyRenderer(DjangoTemplates):\n                return f'<div class=\"custom-field\">{field.label_tag} {field}</div>'\n\n        form = Form(renderer=MyRenderer())\n        html = form.as_p()\n        self.assertIn('<div class=\"custom-field\">', html)\n", "    def test_cleaning_files_with_empty_data(self):\n        class FormWithFile(Form):\n            file = FileField()\n\n        form = FormWithFile({'file': ''})\n        self.assertTrue(form.is_valid())\n        self.assertTrue(form.cleaned_data['file'] is None)\n"], "sample_48": ["    def test_aggregate_with_related_fields(self):\n        avg_price = Book.objects.filter(authors__name__contains=\"Norvig\").aggregate(avg_price=Avg('price'))\n        self.assertEqual(avg_price['avg_price'], Decimal(\"29.8\") ) \n\n\n", "    def test_subqueries_in_aggregate_with_distinct(self):\n        qs = Book.objects.filter(rating__in=Book.objects.filter(rating=4).values_list('rating', flat=True).distinct()).annotate(\n            avg_rating=Avg('rating')\n        )\n        self.assertEqual(qs.first().avg_rating, 4)\n\n        qs = Book.objects.filter(rating__in=Book.objects.filter(isbn__startswith='1590').values_list('rating', flat=True).distinct()).annotate(\n            avg_rating=Avg('rating')\n        )\n        self.assertEqual(qs.first().avg_rating, 4)\n\n\n", "    def test_aggregate_with_empty_queryset(self):\n        empty_qs = Book.objects.filter(pk__in=[]).annotate(avg_rating=Avg('rating'))\n        empty_result = empty_qs.aggregate(average_rating=Avg('avg_rating'))\n        self.assertDictEqual(empty_result, {'average_rating': None})", "    def test_aggregate_over_expression(self):\n        with self.assertRaisesMessage(TypeError, 'Expression is not valid for aggregate'):\n            Book.objects.aggregate(Sum(F(\"rating\") + F(\"price\")))\n", "    def test_nested_aggregations(self):\n        avg_rating_by_author = Book.objects.values('authors__name').annotate(\n            avg_rating=Avg('rating')\n        ).annotate(\n            max_avg_rating=Max('avg_rating')\n        ).order_by('avg_rating')\n        self.assertEqual(\n            list(avg_rating_by_author), [\n                {'authors__name': 'Peter Norvig', 'avg_rating': 3.0, 'max_avg_rating': 5.0},\n                {'authors__name': 'Adrian Holovaty', 'avg_rating': 4.0, 'max_avg_rating': 5.0},\n                {'authors__name': 'Brad Dayley', 'avg_rating': 3.5, 'max_avg_rating': 5.0},\n                {'authors__name': 'Jacob Kaplan-Moss', 'avg_rating': 4.0, 'max_avg_rating': 5.0},\n                {'authors__name': 'James Bennett', 'avg_rating': 3.5, 'max_avg_rating': 5.0},\n                {'authors__name': 'Jeffrey Forcier', 'avg_rating': 3.0, 'max_avg_rating': 5.0},\n                {'authors__name': 'Paul Bissex', 'avg_rating': 3.0, 'max_avg_rating': 5.0},\n                {'authors__name': 'Stuart Russell', 'avg_rating': 5.0, 'max_avg_rating': 5.0},\n                {'authors__name': 'Wesley J. Chun', 'avg_rating': 3.0, 'max_avg_rating': 5.0},\n            ]\n        )\n\n"], "sample_456": ["    def test_all_valid_with_form_errors_and_management_form_errors(self):\n        data = {\n            \"choices-TOTAL_FORMS\": \"2\",\n            \"choices-INITIAL_FORMS\": \"0\",\n            \"choices-MIN_NUM_FORMS\": \"0\",\n            \"choices-0-choice\": \"Zero\",\n            \"choices-0-votes\": \"\",\n            \"choices-1-choice\": \"One\",\n            \"choices-1-votes\": \"1\",\n            \"form-TOTAL_FORMS\": \"abc\",  # Incorrect value for TOTAL_FORMS\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset1 = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n        formset2 = ChoiceFormSet(data, auto_id=False, prefix=\"choices2\")\n        self.assertIs(all_valid((formset1, formset2)), False)\n        expected_errors = [\n            {\"votes\": [\"This field is required.\"], \"form-TOTAL_FORMS\": [\"This field must be a whole number.\"] },\n            {\"votes\": [\"This field is required.\"],},\n        ]\n        self.assertEqual(formset1._errors, expected_errors)\n        self.assertEqual(formset2._errors, expected_errors)\n\n", "    def test_initial_form_count_on_invalid_formset(self):\n        data = {\n            \"form-TOTAL_FORMS\": \"2\",\n            \"form-INITIAL_FORMS\": \"0\",\n            \"form-0-choice\": \"Zero\",\n            \"form-0-votes\": \"\",\n            \"form-1-choice\": \"One\",\n            \"form-1-votes\": \"\",\n        }\n        formset = ArticleFormSet(data, prefix=\"form\")\n        self.assertEqual(formset.initial_form_count(), 0)\n\n\n\n", "    def test_all_valid_with_empty_formsets(self):\n        \"\"\"all_valid() works with empty formsets.\"\"\"\n        empty_formset = formset_factory(Choice, extra=0)()\n        valid_formset = self.make_choiceformset([(\"Zero\", 1)])\n        self.assertIs(all_valid((empty_formset, valid_formset)), True)\n        self.assertEqual(empty_formset._errors, {})\n        self.assertEqual(valid_formset._errors, {})\n\n", "    def test_all_valid_with_optional_fields(self):\n        data = {\n            \"choices-TOTAL_FORMS\": \"2\",\n            \"choices-INITIAL_FORMS\": \"0\",\n            \"choices-MIN_NUM_FORMS\": \"0\",\n            \"choices-0-choice\": \"Zero\",\n            \"choices-0-votes\": \"\",\n            \"choices-1-choice\": \"One\",\n            \"choices-1-votes\": \"1\",\n        }\n        ChoiceFormSet = formset_factory(ChoiceWithOptionalVotes)\n        formset1 = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n        formset2 = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n        self.assertIs(all_valid((formset1, formset2)), True)\n        expected_errors = [{}, {}]\n        self.assertEqual(formset1._errors, expected_errors)\n        self.assertEqual(formset2._errors, expected_errors)\n\n", "    def test_empty_form_set_errors(self):\n        empty_formset = formset_factory(Choice, extra=0)()\n        self.assertEqual(empty_formset.errors, [])\n        self.assertIs(empty_formset.is_valid(), True)\n"], "sample_918": ["def test_pyclass_and_modules_in_index():\n    text = \".. py:class:: ClassInModule\\n\"\n    text += \". py:module:: MyModule\\n\"\n    text += \"   .. py:class:: AnotherClass\\n\"\n    restructuredtext.parse(app, text)\n    index = PythonModuleIndex(app.env.get_domain('py'))\n    assert index.generate() == (\n        [('c', [IndexEntry('ClassInModule', 0, 'index', 'module-MyModule.ClassInModule', '', '', ''),\n                IndexEntry('AnotherClass', 1, 'index', 'module-MyModule.AnotherClass', '', '', '')]),\n         ('m', [IndexEntry('MyModule', 0, 'index', 'module-MyModule', '', '', '')])],\n        True)\n", "def test_pydata_multiple_signatures(app):\n    text = (\".. py:data:: var\\n\"\n            \"   :type: int\\n\"\n            \"   :value: 1\\n\"\n            \".. py:data:: var\\n\"\n            \"   :type: str\\n\"\n            \"   :value: 'hello'\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, desc_name, \"var\"], [desc_content, ()])],\n                          addnodes.index,\n                          [desc, ([desc_signature, desc_name, \"var\"], [desc_content, ()])]))\n\n    assert 'var' in domain.objects\n    assert domain.objects['var'] == ('index', 'var', 'data')\n    assert 'var' in domain.objects\n    assert domain.objects['var'] == ('index', 'var', 'data')\n", "    def test_pyfunction_signature_options(app):\n        text = (\".. py:function:: func\\n\"\n                \"   :param x: the first argument\\n\"\n                \"   :type: int\\n\"\n                \"   :return: the sum of x and y\\n\"\n                \"   :raises ValueError: if x is negative\\n\")\n        domain = app.env.get_domain('py')\n        doctree = restructuredtext.parse(app, text)\n        assert_node(doctree, (addnodes.index,\n                              [desc, ([desc_signature, ([desc_name, \"func\"],\n                                                    [desc_parameterlist,\n                                                     ([desc_parameter, \"x\",\n                                                      [desc_annotation, (\": \",\n                                                                      [pending_xref, \"int\"])],\n                                                      )],\n                                                    [desc_returns, pending_xref, \"int\"],\n                                                    [desc_raises, pending_xref, \"ValueError\"])],\n                                                 desc_content)]))\n\n        assert 'func' in domain.objects\n        assert domain.objects['func'] == ('index', 'func', 'function')\n\n\n", "    def test_py_function_signature_with_args(app):\n        text = (\".. py:function:: my_function(a: int, b: str, *c: float)\\n\"\n                \"   :returns: bool\\n\")\n        doctree = restructuredtext.parse(app, text)\n        assert_node(doctree, (addnodes.index,\n                              [desc, ([desc_signature, ([desc_name, \"my_function\"],\n                                                        [desc_parameterlist,\n                                                         [desc_parameter, desc_sig_name, \"a\"],\n                                                         [desc_sig_punctuation, \":\"],\n                                                         [desc_sig_name, pending_xref, \"int\"],\n                                                         [desc_parameter, desc_sig_name, \"b\"],\n                                                         [desc_sig_punctuation, \":\"],\n                                                         [desc_sig_name, pending_xref, \"str\"],\n                                                         [desc_parameter, desc_sig_operator, \"*\"],\n                                                         [desc_parameter, desc_sig_name, \"c\"],\n                                                         [desc_sig_punctuation, \":\"],\n                                                         [desc_sig_name, pending_xref, \"float\"]],\n                                                        [desc_returns, pending_xref, \"bool\"])],\n                                                 desc_content)]))\n", "def test_pyfunction_signature_full_py38(app):\n    # case: separator at head\n    text = \".. py:function:: hello(*, a)\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree[1][0][1],\n                [desc_parameterlist, ([desc_parameter, nodes.inline, \"*\"],\n                                      [desc_parameter, desc_sig_name, \"a\"])])\n\n    # case: separator in the middle\n    text = \".. py:function:: hello(a, /, b, *, c)\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree[1][0][1],\n                [desc_parameterlist, ([desc_parameter, desc_sig_name, \"a\"],\n                                      [desc_parameter, desc_sig_operator, \"/\"],\n                                      [desc_parameter, desc_sig_name, \"b\"],\n                                      [desc_parameter, desc_sig_operator, \"*\"],\n                                      [desc_parameter, desc_sig_name, \"c\"])])\n\n    # case: separator in the middle (2)\n    text = \".. py:function:: hello(a, /, *, b)\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree[1][0][1],\n                [desc_parameterlist, ([desc_parameter, desc_sig_name, \"a\"],\n                                      [desc_parameter, desc_sig_operator, \"/\"],\n                                      [desc_parameter, desc_sig_operator, \"*\"],\n                                      [desc_parameter, desc_sig_name, \"b\"])])\n\n    # case: separator at tail\n    text = \".. py:function:: hello(a, /)\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree[1][0][1],\n                [desc_parameterlist, ([desc_parameter, desc_sig_name, \"a\"],\n                                      [desc_parameter, desc"], "sample_16": ["    def test_structured_array_units(self):\n        \"\"\"\n        Test that structured arrays preserve units\n        \"\"\"\n        arr = np.array(\n            [\n                ((1.0, 2.0), \"hello\"),\n                ((3.0, 4.0), \"world\"),\n            ],\n            dtype=[(\"f1\", float), (\"f2\", float), (\"str\", str)],\n        )\n        arr3 = arr << u.Unit(\"(float, float, str)\")\n        assert_array_equal(arr3[\"f1\"], arr[\"f1\"])\n        assert_array_equal(arr3[\"f2\"], arr[\"f2\"])\n        assert_array_equal(arr3[\"str\"], arr[\"str\"])\n        assert arr3.unit == u.Unit(\"(float, float, str)\")\n\n\n\n", "    def test_all_functions_covered(self):\n        # Ensure all functions are covered.\n        for func_name in all_wrapped_functions:\n            if func_name in untested_functions:\n                raise ValueError(f\"Function {func_name} is not tested.\")\n", "    def test_array_dtype_conversion(self):\n        # This test case covers the conversion of array dtypes\n        #   during unit quantity operations. \n        #   We'll use a structured array with a mix of numeric and string fields.\n        arr = np.array(\n            (\n                (1.0, \"a\", 2.5),\n                (3.14, \"b\", 4.0),\n                (5.0, \"c\", 6.28),\n            ),\n            dtype=[(\"f1\", float), (\"f2\", str), (\"f3\", float)],\n        ) * u.one\n        # Check conversion for addition\n        with pytest.raises(u.UnitsError, match=\"Incompatible Units\"):\n            arr + 2 * u.m\n        # Check conversion for multiplication\n        result = arr * 5 * u.one\n        assert_array_equal(result, np.array(\n            (\n                (5.0, \"a\", 12.5),\n                (15.7, \"b\", 20.0),\n                (25.0, \"c\", 31.4),\n            ),\n            dtype=[(\"f1\", float), (\"f2\", str), (\"f3\", float)],\n        )) * u.one\n\n        # Test basic field access\n        assert_array_equal(arr[\"f1\"], np.array([1.0, 3.14, 5.0]))\n        assert_array_equal(arr[\"f2\"], np.array([\"a\", \"b\", \"c\"]))\n        assert_array_equal(arr[\"f3\"], np.array([2.5, 4.0, 6.28]))\n\n\n\n", "    def test_unsupported_functions(self):\n        func_names = list(UNSUPPORTED_FUNCTIONS)\n        for func_name in func_names:\n            with pytest.raises(TypeError, match=f\"Unsupported function: {func_name}\"):\n                getattr(np, func_name)(np.array([1, 2, 3]))\n", "def test_array_split(self):\n    x = np.arange(10).reshape((2, 5))\n    y = np.array_split(x, 3)\n    assert len(y) == 3\n    assert_array_equal(np.concatenate(y), x)\n"], "sample_612": ["    def test_resample_by_interp_with_bounds_error(self):\n        times = pd.date_range(\"2000-01-01\", freq=\"6H\", periods=5)\n        array = DataArray(np.zeros((5, 3)), {\"time\": times}, (\"time\"))\n        with pytest.raises(ValueError, match=\"bounds_error is not implemented in resample\"):\n            array.resample(time=\"1H\").interpolate(kind=\"linear\", bounds_error=True)\n\n\n\n", "    def test_resample_groupby(self):\n        times = pd.date_range(\"2000-01-01\", freq=\"6H\", periods=10)\n        ds = Dataset(\n            {\n                \"foo\": ([\"time\", \"x\", \"y\"], np.random.randn(10, 5, 3)),\n                \"bar\": (\"time\", np.random.randn(10), {\"meta\": \"data\"}),\n                \"time\": times,\n            }\n        )\n        \n        resampled_ds = ds.resample(time=\"1D\").mean(keep_attrs=True).groupby(\"x\")\n        assert isinstance(resampled_ds, xr.core.groupby.GroupBy)\n        assert len(resampled_ds.groups) == 5\n\n        \n", "    def test_resample_groupby_mean_with_non_numeric_coords(self):\n        times = pd.date_range(\"2000-01-01\", freq=\"6H\", periods=10)\n        ds = Dataset(\n            {\n                \"foo\": ([\"time\", \"x\", \"category\"], np.random.randn(10, 5, 3)),\n                \"bar\": (\"time\", np.random.randn(10), {\"meta\": \"data\"}),\n                \"time\": times,\n                \"category\": [\"A\", \"B\", \"A\", \"B\", \"A\", \"B\", \"A\", \"B\", \"A\", \"B\"],\n            }\n        )\n        resampled_ds = ds.resample(time=\"1D\").mean(keep_attrs=True)\n        assert \"category\" in resampled_ds.coords\n\n        resampled_ds_grouped = ds.groupby(\"category\").resample(time=\"1D\").mean(keep_attrs=True)\n        assert \"category\" in resampled_ds_grouped.coords\n\n\n\n", "    def test_resample_groupby_mean_with_custom_time_dim(self):\n        times = pd.date_range(\"2000-01-01\", freq=\"6H\", periods=10)\n        ds = Dataset(\n            {\n                \"foo\": ([\"time\", \"x\", \"y\"], np.random.randn(10, 5, 3)),\n                \"group\": (\"x\", [0, 0, 1, 1, 1, 2, 2, 2, 2, 2]),\n                \"time\": times,\n            }\n        )\n\n        resampled_ds = ds.resample(time=\"1D\", groupby=\"group\").mean()\n        assert \"time\" in resampled_ds\n\n\n", "    def test_resample_chunk_size_handling(self):\n        xs = np.arange(6)\n        ys = np.arange(3)\n        times = pd.date_range(\"2000-01-01\", freq=\"6H\", periods=10)\n        data = np.tile(np.arange(5), (6, 3, 1))\n        array = DataArray(data, {\"time\": times, \"x\": xs, \"y\": ys}, (\"x\", \"y\", \"time\"))\n\n        # Test when chunk size is larger than dataset\n        with pytest.raises(ValueError, match=r\"Chunk size must be less than or equal to\"):\n            array.resample(time=\"12H\", chunk_size=12).mean()\n\n        # Test when chunk size is smaller than the dataset\n        actual = array.chunk(chunks={\"time\": 3}).resample(time=\"12H\").mean(dim=\"time\")\n        expected = array.resample(time=\"12H\").mean()\n        assert_identical(expected, actual)\n\n        # Test when chunk size is larger than the dataset\n        actual = array.chunk(chunks={\"time\": 1}).resample(time=\"12H\").mean(dim=\"time\")\n        expected = array.resample(time=\"12H\").mean()\n        assert_identical(expected, actual)\n\n        # Test when chunk size matches the dataset\n        actual = array.chunk(chunks={\"time\": 5}).resample(time=\"12H\").mean(dim=\"time\")\n        expected = array.resample(time=\"12H\").mean()\n        assert_identical(expected, actual)\n\n\n\n"], "sample_651": ["    def test_re_emit_match_multiple_repetition(self) -> None:\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"error\")  # if anything is re-emitted\n            with pytest.warns(UserWarning, match=\"user\"):\n                warnings.warn(\"user warning 1\", UserWarning)\n                warnings.warn(\"user warning 2\", UserWarning)\n\n\n\n", "    def test_warn_stacklevel_with_match(self) -> None:\n        with pytest.warns(DeprecationWarning, match=\"deprecated\"):\n            warnings.warn_explicit(\n                \"deprecated\", category=DeprecationWarning, filename=\"hello\", lineno=3\n            )\n", "    def test_re_emit_multiple_regex(self) -> None:\n        with pytest.warns(UserWarning, match=r\"user\"):\n            warnings.warn(\"user warning 1\", UserWarning)\n            warnings.warn(\"user warning 2\", UserWarning)\n            warnings.warn(\"not a user warning\", DeprecationWarning)\n", "    def test_re_emit_match_multiple_unmatched(self) -> None:\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"error\")  # if anything is re-emitted\n            with pytest.warns(UserWarning, match=\"user warning\"):\n                warnings.warn(\"first user warning\", UserWarning)\n                warnings.warn(\"second user warning\", RuntimeWarning)\n\n", "    def test_deprecated_call_with_call_arg(self) -> None:\n        with pytest.deprecated_call():\n            pytest.deprecated_call(self.dep, 0)\n\n\n\n"], "sample_531": ["compilation error", "compilation error", "compilation error", "def test_subfigure_gridspec_with_share():\n    fig, ax_list = plt.subplots(2, 2, figsize=(10, 6), constrained_layout=True)\n    shared_axes = fig.add_subfigure(ax_list[0:2, 0], sharex=True, sharey=True)\n    sub_fig_2 = fig.add_subplot(2, 2, 3)\n    sub_fig_3 = fig.add_subplot(2, 2, 4)\n\n    # Check if sharex and sharey are correctly applied\n    assert shared_axes.axes[0].get_xlabel() == ax_list[0, 0].get_xlabel()\n    assert shared_axes.axes[1].get_ylabel() == ax_list[1, 0].get_ylabel()\n", "compilation error"], "sample_752": ["def test_contamination_auto_performance():\n    \"\"\"Test Isolation Forest with auto contamination performs well.\"\"\"\n\n    # Generate train/test data\n    rng = check_random_state(2)\n    n_samples = 100\n    X = 0.3 * rng.randn(n_samples, 2)\n    # Define outlier samples\n    outliers = 2 * rng.randn(20, 2)\n\n    X_train = np.r_[X[:80], outliers[:10]]\n    X_test = np.r_[X[80:], outliers[10:]]\n    y_test = np.array([0] * 80 + [1] * 20)\n\n    # Fit the model\n    clf = IsolationForest(random_state=rng, contamination='auto')\n    clf.fit(X_train)\n\n    # predict scores (the lower, the more normal)\n    y_pred = - clf.decision_function(X_test)\n\n    # check that there is at most 6 errors (false positive or false negative)\n    assert_greater(roc_auc_score(y_test, y_pred), 0.98)\n\n", "def test_iforest_oob_score():\n    X = iris.data\n    y = iris.target\n\n    # Check that oob_score is None\n    clf = IsolationForest(random_state=42, oob_score=True)\n    with pytest.raises(AttributeError):\n        clf.oob_score_\n\n\n\n\n    clf = IsolationForest(random_state=42).fit(X, y)\n    assert clf.oob_score_ is None\n", "def test_iforest_contamination_auto():\n    # Test the behavior of contamination='auto'\n    rng = check_random_state(0)\n    X = rng.randn(100, 2)\n    y = np.zeros(100, dtype=int)\n    y[20:] = 1\n    clf = IsolationForest(contamination='auto')\n    clf.fit(X, y)\n    # Check if the fitted contamination is between 0. and 0.5\n    assert 0. <= clf._contamination <= 0.5, (\n        f\"Fitted contamination ({clf._contamination}) is outside valid range\"\n    )\n\n\n\n\n", "def test_iforest_outlier_detection():\n    # Test outlier detection with sample data\n\n\n", "def test_contamination_auto():\n    rng = check_random_state(0)\n    X = rng.randn(100, 2)\n    y = rng.randint(0, 2, size=100)\n    contamination = 0.2\n\n    clf_auto = IsolationForest(contamination='auto').fit(X)\n    clf_specified = IsolationForest(contamination=contamination).fit(X)\n    \n    assert_array_almost_equal(clf_auto.decision_function(X), \n                             clf_specified.decision_function(X))\n\n"], "sample_407": ["    def test_save_related_object_with_non_string_pk(self):\n        # Regression for #24394 -- Saving a related object with a non-string primary key.\n        parent = ParentIntegerPrimaryKey.objects.create(name=\"parent_name\")\n        child = ChildIntegerPrimaryKey.objects.create(parent=parent)\n        child.parent = parent\n        child.save()\n        child_from_db = ChildIntegerPrimaryKey.objects.get(pk=child.pk)\n        self.assertEqual(child_from_db.parent, parent)\n\n\n    ", "    def test_add_remove_set_by_instance(self):\n        usa = Country.objects.create(name=\"United States\")\n        chicago = City.objects.create(name=\"Chicago\")\n        texas = City.objects.create(name=\"Texas\")\n        usa.cities.add(chicago)\n        usa.cities.add(texas)\n        self.assertSequenceEqual(usa.cities.all(), [chicago, texas])\n        usa.cities.remove(chicago)\n        self.assertSequenceEqual(usa.cities.all(), [texas])\n        usa.cities.set([chicago])\n        self.assertSequenceEqual(usa.cities.all(), [chicago])\n", "    def test_cached_related_object_after_update(self):\n        parent = Parent.objects.create(name=\"Original\")\n        child = Child.objects.create(parent=parent)\n        self.assertEqual(child.parent, parent)\n\n        parent.name = \"Updated\"\n        parent.save()\n        self.assertEqual(child.parent, parent)\n\n", "    def test_add_remove_set_by_id(self):\n        usa = Country.objects.create(name=\"United States\")\n        chicago = City.objects.create(name=\"Chicago\")\n        usa.cities.add(chicago.id)\n        self.assertEqual(usa.cities.count(), 1)\n        usa.cities.remove(chicago.id)\n        self.assertEqual(usa.cities.count(), 0)\n        usa.cities.set([chicago.id])\n        self.assertEqual(usa.cities.count(), 1)\n\n", "    def test_related_object_manager_name(self):\n        # Regression for #21590 - make sure that related object managers\n        # using the default manager name are resolved correctly.\n\n        class Parent(models.Model):\n            name = models.CharField(max_length=100)\n            children = models.Manager()\n            related_children = models.Manager(\"related_children\")\n            objects = models.Manager() # default\n\n        class Child(models.Model):\n            parent = models.ForeignKey(Parent, on_delete=models.CASCADE)\n            objects = models.Manager() # default\n\n            @classmethod\n                return cls.objects.filter(parent__name=\"some_parent\")\n\n        p = Parent.objects.create(name=\"some_parent\")\n        Child.objects.create(parent=p)\n\n        related_children = Parent.objects.get(name=\"some_parent\").related_children.all()\n        self.assertEqual(related_children.count(), 1)\n"], "sample_186": ["    def test_list_filter_works_on_through_field_with_custom_model(self):\n        \"\"\"\n        Ensure list_filter works on through fields even when it's\n        a custom model.\n        \"\"\"\n        class CustomTagThrough(models.Model):\n            song = models.ForeignKey(Song, on_delete=models.CASCADE)\n            tag = models.ForeignKey(Tag, on_delete=models.CASCADE)\n\n        class SongAdminWithListFilter(admin.ModelAdmin):\n            list_filter = ['tags__tag']\n\n        # Temporarily pretending apps are not ready yet\n        CustomTagThrough._meta.apps.ready = False\n        try:\n            errors = SongAdminWithListFilter(Song, AdminSite()).check()\n            self.assertEqual(errors, [])\n        finally:\n            CustomTagThrough._meta.apps.ready = True\n", "    def test_duplicate_list_filter_fields(self):\n        class MyModelAdmin(admin.ModelAdmin):\n            list_filter = ['title', 'title']\n\n        errors = MyModelAdmin(Song, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"The value of 'list_filter' contains duplicate field(s).\",\n                obj=MyModelAdmin,\n                id='admin.E005',\n            )\n        ]\n        self.assertEqual(errors, expected)\n\n", "    def test_list_filter_handles_through_field_with_model_attribute(self):\n        class BookAdminWithListFilter(admin.ModelAdmin):\n            list_filter = ['authorsbooks__book__title']\n\n        errors = BookAdminWithListFilter(Book, AdminSite()).check()\n        self.assertEqual(errors, [])\n", "    def test_list_display_works_on_through_field_even_when_apps_not_ready(self):\n        \"\"\"\n        Ensure list_display can access reverse fields even when the app registry\n        is not ready; refs #24146.\n        \"\"\"\n        class BookAdminWithListDisplay(admin.ModelAdmin):\n            list_display = ['title', 'authorsbooks__featured']\n\n        # Temporarily pretending apps are not ready yet. This issue can happen\n        # if the value of 'list_display' refers to a 'through__field'.\n        Book._meta.apps.ready = False\n        try:\n            errors = BookAdminWithListDisplay(Book, AdminSite()).check()\n            self.assertEqual(errors, [])\n        finally:\n            Book._meta.apps.ready = True\n", "    def test_list_filter_requires_field_presence(self):\n        class BookAdminWithMissingListFilter(admin.ModelAdmin):\n            list_filter = ['missing_authorsbooks__featured']\n\n        errors = BookAdminWithMissingListFilter(Book, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"The value of 'list_filter[0]' refers to a field that does not \"\n                \"exist on 'admin_checks.Book'.\",\n                obj=BookAdminWithMissingListFilter,\n                id='admin.E009',\n            ),\n        ]\n        self.assertEqual(errors, expected)\n"], "sample_851": ["def test_tweedie_deviance_multioutput():\n    y_true = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n    y_pred = np.array([[[2, 1], [4, 3]], [[6, 7], [8, 9]]])\n    deviance = mean_tweedie_deviance(y_true, y_pred, power=2)\n    assert deviance.shape == (y_true.shape[1],)\n\n", "compilation error", "compilation error", "compilation error", "    def test_mean_tweedie_deviance_multioutput():\n        y_true = [[1, 2], [2.5, -1], [4.5, 3], [5, 7]]\n        y_pred = [[1, 1], [2, -1], [5, 4], [5, 6.5]]\n        power = 2\n\n        mse = mean_squared_error(y_true, y_pred, multioutput='raw_values')\n        tweedie_deviance = mean_tweedie_deviance(y_true, y_pred,\n                                               multioutput='raw_values',\n                                               power=power)\n        assert_array_almost_equal(tweedie_deviance,\n                                 mse,\n                                 decimal=2)\n\n\n"], "sample_271": ["    def test_snapshot_files_with_different_paths(self):\n        file1 = self.ensure_file(self.tempdir / 'file1.py')\n        file2 = self.ensure_file(self.tempdir / 'file2.py')\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[file1, file2]):\n            snapshot = dict(self.reloader.snapshot_files())\n            self.assertIn(file1, snapshot)\n            self.assertIn(file2, snapshot)\n", "    def test_snapshot_files_for_empty_watched_files(self):\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[]):\n            self.assertEqual(dict(self.reloader.snapshot_files()), {})\n\n\n\n", "    def test_snapshot_files_with_non_existent_duplicates(self):\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file, self.nonexistent_file]):\n            snapshot = list(self.reloader.snapshot_files())\n            self.assertEqual(len(snapshot), 1)\n            self.assertEqual(snapshot[0][0], self.existing_file)\n", "    def test_should_stop_returns_true_when_stat_is_non_existent(self):\n        with mock.patch.object(os, 'stat', side_effect=FileNotFoundError) as mocked_stat:\n            self.assertTrue(self.reloader._should_stop(self.nonexistent_file))\n            mocked_stat.assert_called_once_with(str(self.nonexistent_file))\n", "    def test_snapshot_files_with_different_paths_to_same_file(self):\n        file_path = self.tempdir / 'test.py'\n        file_path2 = self.tempdir / 'test.py.1'\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[file_path, file_path2]):\n            self.ensure_file(file_path)\n            snapshot = dict(self.reloader.snapshot_files())\n            self.assertIn(file_path, snapshot)\n            self.assertIn(file_path2, snapshot)\n"], "sample_894": ["def test_n_jobs_with_single_tree(n_jobs):\n    X = np.ones((10, 10))\n    y = np.zeros(10)\n\n    clf = RandomForestClassifier(\n        n_estimators=1, n_jobs=n_jobs, random_state=0\n    )\n    clf.fit(X, y)\n\n    assert len(clf.estimators_) == 1\n", "compilation error", "def test_n_jobs_1_with_multiprocessing_pool():\n    from concurrent.futures import ThreadPoolExecutor\n    original_pool = multiprocessing.Pool\n    multiprocessing.Pool = ThreadPoolExecutor\n  \n    # This test ensures that n_jobs=1 still works correctly\n    # when multiprocessing is enabled but limited to a single thread.\n    with pytest.warns(UserWarning, match=r\"Using 1 processes because n_jobs is set to 1\"):\n      clf = RandomForestClassifier(n_jobs=1)\n      clf.fit(X, y)\n\n    multiprocessing.Pool = original_pool\n\n\n\n", "compilation error", "def test_oob_score_reset_after_fit(name):\n    X, y = hastie_X, hastie_y\n    clf = FOREST_CLASSIFIERS[name](\n        n_estimators=10, oob_score=True, random_state=0\n    )\n\n    # initial fit\n    clf.fit(X, y)\n    assert hasattr(clf, \"oob_score_\")\n    assert clf.oob_score_ is not None\n\n    # reset oob_score_ by refitting with same data\n    clf.fit(X, y)\n    assert hasattr(clf, \"oob_score_\")\n    assert clf.oob_score_ is not None \n"], "sample_1167": ["def test_printing_latex_tensor_expressions():\n    from sympy.tensor import Tensor, TensorIndexType, tensor_indices\n    L = TensorIndexType(\"L\")\n    i, j, k, l = tensor_indices(\"i j k l\", L)\n    A = Tensor(\"A\", (L,))\n    B = Tensor(\"B\", (L, L))\n    C = Tensor(\"C\", (L, L, L))\n\n    assert latex(A(i)) == r\"A^{i}\"\n    assert latex(A(i) + B(i, j)) == r\"A^{i} + B^{i}_{j}\"\n\n    assert latex(B(i, j)) == r\"B^{i}_{j}\"\n    assert latex(B(i, j) * C(k, l, m)) == r\"B^{i}_{j} C^{k}_{l m}\"\n    assert latex(C(i, j, k)) == r\"C^{i}_{j k}\"\n\n\n", "compilation error", "def test_issue_19028():\n    from sympy import symbols, Function, Matrix\n    x, y = symbols('x y')\n    A = Matrix([[x, y], [y, x]])\n    f = Function('f')\n    assert latex(f(A)) == r'f\\left(\\mathbf{A}\\right)'\n", "compilation error", "def test_MatrixSymbol_repeated():\n    A = MatrixSymbol('A', 3, 3)\n    B = MatrixSymbol('B', 3, 3)\n    assert latex(A.repeated_indices(A)) == r'A_{\\text{A}}'\n    assert latex(A.repeated_indices(A, B)) == r'A_{\\text{A}}^{\\text{B}}'\n    assert latex(A.repeated_indices(B)) == r'A_{\\text{B}}'\n\n\n\n\n\n"], "sample_574": ["    def test_label_format(self, t, fmt):\n\n        s = Temporal().label(fmt=fmt)\n        a = PseudoAxis(s._setup(t, Coordinate())._matplotlib_scale)\n        a.set_view_interval(0, 365)\n        label, = a.major.formatter.format_ticks([0])\n        assert label.endswith(fmt % t[0])\n", "    def test_label_format(self, t):\n\n        s = Temporal().label(format=\"%b %Y\")._setup(t, Coordinate())\n        \n        a = PseudoAxis(s._matplotlib_scale)\n        a.set_view_interval(0, 365)\n        label, = a.major.formatter.format_ticks([100])\n        assert label == \"Sep 1972\"\n", "    def test_label_concise_with_format(self, t, x):\n\n        ax = mpl.figure.Figure().subplots()\n        Temporal().label(concise=\"%Y-%m\")._setup(t, Coordinate(), ax.xaxis)\n        formatter = ax.xaxis.get_major_formatter() \n        assert isinstance(formatter, mpl.dates.ConciseDateFormatter)\n        label, = formatter.format_ticks([100])\n        assert label == \"1970-01\"\n", "    def test_label_fmt(self, t, x):\n\n        label_format = \"%B %Y\"\n        s = Temporal().label(fmt=label_format)\n        a = PseudoAxis(s._setup(t, Coordinate())._matplotlib_scale)\n        a.set_view_interval(10, 1000)\n        label, = a.major.formatter.format_ticks([100])\n        assert label == \"December 1972\"\n", "    def test_label_with_format(self, t):\n\n        format = \"%Y-%m\"\n        s = Temporal().label(format)._setup(t, Coordinate())\n        a = PseudoAxis(s._matplotlib_scale)\n        a.set_view_interval(0, 365)\n        label, = a.major.formatter.format_ticks([100])\n        assert label == \"1970-01\"\n"], "sample_306": ["    def test_timezone_offsets(self):\n        test_values = (\n            ('2012-04-23T10:20:30+02:30', datetime(2012, 4, 23, 10, 20, 30, 0, get_fixed_timezone(150))),\n            ('2012-04-23T10:20:30-02:30', datetime(2012, 4, 23, 10, 20, 30, 0, get_fixed_timezone(-150))),\n            ('2012-04-23T10:20:30+02', datetime(2012, 4, 23, 10, 20, 30, 0, get_fixed_timezone(120))),\n            ('2012-04-23T10:20:30-02', datetime(2012, 4, 23, 10, 20, 30, 0, get_fixed_timezone(-120))),\n            ('2012-04-23T10:20:30Z', datetime(2012, 4, 23, 10, 20, 30, 0, get_fixed_timezone(0))),\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_datetime(source), expected)\n", "    def test_invalid_iso_8601(self):\n        with self.assertRaises(ValueError):\n            parse_duration('P4Y3M')\n        with self.assertRaises(ValueError):\n            parse_duration('P4Y1S')\n        with self.assertRaises(ValueError):\n            parse_duration('P4Y1D1H')\n        with self.assertRaises(ValueError):\n            parse_duration('P4YZ')\n        with self.assertRaises(ValueError):\n            parse_duration('PT1H1M')\n", "    def test_invalid_iso_8601(self):\n        invalid_inputs = (\n            'P4Y2M',\n            'P4Y-1M',\n            'P4YX',\n            'PT123H',\n            'PT123M',\n            'P123S',\n            'PT123H45M67S',\n            'P123H45M67S,abcdefg',\n            'P123H45M67.abcdefgS',\n            'P123H45M67.abcS',\n            'PT123H45M67.abcdefg',\n            'P123.abcS',\n            'P123.abcdefg',\n        )\n        for input_string in invalid_inputs:\n            with self.subTest(input=input_string):\n                self.assertIsNone(parse_duration(input_string))\n\n", "    def test_invalid_iso_8601(self):\n        self.assertIsNone(parse_duration('P4YZ'))\n        self.assertIsNone(parse_duration('P4Y-1M'))\n        self.assertIsNone(parse_duration('P4Y1M'))\n        self.assertIsNone(parse_duration('P4Y1M1D'))\n        self.assertIsNone(parse_duration('PT5H1M'))\n        self.assertIsNone(parse_duration('PT5HS'))\n        self.assertIsNone(parse_duration('PT5H1M1S'))\n        self.assertIsNone(parse_duration('PT5H1M1S.1'))\n        self.assertIsNone(parse_duration('PT5H1M1S.123'))\n", "    def test_postgres_interval_with_mixed_types(self):\n        with self.assertRaises(ValueError):\n            parse_duration('3 days 04:05:06.1')\n"], "sample_657": ["def test_pytest_param_id_no_id_on_tuple():\n    with pytest.raises(TypeError) as excinfo:\n        pytest.param(1, 2)\n    msg, = excinfo.value.args\n    assert msg == \"id is required for parametrized fixtures\"\n\n\n\n", "def test_pytest_param_id_requires_string():\n    with pytest.raises(TypeError) as excinfo:\n        pytest.param(id=True)\n    msg, = excinfo.value.args\n    if six.PY2:\n        assert msg == \"Expected id to be a string, got <type 'bool'>: True\"\n    else:\n        assert msg == \"Expected id to be a string, got <class 'bool'>: True\"\n\n", "def test_pytest_param_deprecated_id_arg():\n    with pytest.warns(PytestDeprecationWarning) as warninfo:\n        pytest.param(1, id=2)\n    assert warninfo[0].filename == __file__\n    msg, = warninfo[0].message.args\n    assert msg == (\n        \"The 'id' argument is deprecated in pytest.param().\\n\"\n        \"Use 'id=' in the parametrization fixture instead.\"\n    )\n", "def test_pytest_param_extra_kwargs_are_preserved(\n    mark_params, expected_kwargs", "def test_pytest_param_id_with_callable():\n    with pytest.raises(TypeError) as excinfo:\n        pytest.param(id=lambda: None)\n    msg, = excinfo.value.args\n    assert msg == \"Expected id to be a scalar or NoneType, got <function <lambda> at 0x...>\"\n\n\n"], "sample_461": ["    def test_urlfield_with_fragment(self):\n        f = URLField()\n        url = \"http://example.com/path/to/page#fragment\"\n        self.assertEqual(f.clean(url), \"http://example.com/path/to/page#fragment\")\n\n\n\n", "    def test_urlfield_with_non_standard_port(self):\n        f = URLField()\n        self.assertEqual(f.clean(\"http://example.com:8080/\"), \"http://example.com:8080/\")\n        self.assertEqual(f.clean(\"https://example.com:443/\"), \"https://example.com:443/\")\n\n\n\n", "    def test_urlfield_clean_invalid_international(self):\n        f = URLField(require_tld=False)\n        tests = [\n            \"http://www.xn--example-qya.com\",\n            \"http://xn--example.com\",\n            \"http://example.xn--test\",\n            \"http://example.com./\",  # extra dot after domain\n            \"http://example.com?param=value\",\n        ]\n        for url in tests:\n            with self.subTest(url=url):\n                self.assertEqual(f.clean(url), url)\n\n\n", "    def test_urlfield_clean_with_username_and_password(self):\n        f = URLField()\n        tests = [\n            (\"http://user:password@example.com\", \"http://user:password@example.com\"),\n            (\"http://user:password@example.com/path\", \"http://user:password@example.com/path\"),\n        ]\n        for url, expected in tests:\n            with self.subTest(url=url):\n                self.assertEqual(f.clean(url), expected)\n", "    def test_urlfield_with_default_scheme(self):\n        f = URLField(default_scheme=\"https\")\n        self.assertEqual(f.clean(\"example.com\"), \"https://example.com\")\n\n\n\n"], "sample_181": ["    def test_filtered_aggregate_with_related_and_excluded_relations(self):\n        agg = Count('authors', filter=~Q(authors__in=Author.objects.filter(name='test')))\n        self.assertEqual(Book.objects.aggregate(cnt=agg)['cnt'], 2)\n", "    def test_filtered_aggregate_with_ordering(self):\n        authors = Author.objects.annotate(\n            average_age=Avg('age', filter=Q(name__startswith='test')),\n        ).order_by('average_age').values('name', 'average_age')\n        self.assertSequenceEqual([{'name': 'test', 'average_age': 100},\n                                  {'name': 'test2', 'average_age': 60} ], list(authors))\n", "    def test_filtered_aggregate_with_outer_ref(self):\n        # Test case for using OuterRef in filter conditions with aggregates\n        # that reference the OuterRef.\n        authors = Author.objects.annotate(\n            total_pages_published=Sum('book__pages', filter=Q(book__published=True))\n        ).using('default')\n        aggregate = authors.annotate(\n            avg_pages_authored=Avg('total_pages_published', filter=Q(\n                pk=OuterRef('pk')\n            ))\n        ).aggregate(\n            avg_pages_published=Avg('avg_pages_authored')\n        )\n\n        self.assertEqual(aggregate['avg_pages_published'], 447.5)\n", "    def test_filtered_aggregate_exists_subquery(self):\n        q1 = Author.objects.filter(\n            name__startswith='test'\n        ).exists()\n\n        q2 = Book.objects.filter(pubdate__year=2008).exists()\n\n        aggregate = Author.objects.annotate(\n            has_test_books=Q(book__filter=q2)\n        ).aggregate(\n            cnt=Count('pk', filter=q1 & ~Q(has_test_books)),\n        )\n        self.assertEqual(aggregate, {'cnt': 1})\n", "    def test_related_aggregate_subquery_filter(self):\n        qs = Author.objects.annotate(\n            avg_book_pages=Avg('book__pages', filter=~Q(book__rating__gt=4)),\n        ).filter(avg_book_pages__gt=400)\n        self.assertEqual(qs.get().pk, self.a3.pk)\n"], "sample_1010": ["def test_Issue_17080():\n    from sympy.matrices import MatrixSymbol\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    C = MatrixSymbol('C', 2, 2)\n    D = MatrixSymbol('D', 2, 2)\n    assert latex(A*B + C**2) == r\"A B + C^{2}\"\n\n\n", "def test_OuterProduct_printing():\n    from sympy.diffgeom.rn import R2\n    from sympy.diffgeom import OuterProduct\n    u = R2.x\n    v = R2.y\n    op = OuterProduct(u, v)\n    assert latex(op) == r'x y'\n", "def test_DotProduct_printing():\n    from sympy.diffgeom.rn import R2\n    from sympy.diffgeom import DotProduct\n    v = R2.vectores[0]\n    w = R2.vectores[1]\n    dp = DotProduct(v, w)\n    assert latex(dp) == r\"\\left\\langle v, w \\right\\rangle\"\n", "def test_LieBracket_printing():\n    from sympy.diffgeom.rn import R2\n    from sympy.diffgeom import LieBracket\n    X = R2.dx\n    Y = R2.dy\n    assert latex(LieBracket(X, Y)) == r\"\\left[ \\mathrm{d}x, \\mathrm{d}y \\right]\" \n", "def test_OuterProduct_printing():\n    from sympy.diffgeom.rn import R2\n    from sympy.diffgeom import OuterProduct\n    x, y = R2.coords\n    op = OuterProduct(x, y)\n    assert latex(op) == r'x \\otimes y'\n\n\n\n"], "sample_472": ["    def test_paginating_large_queryset_with_ordering(self):\n        \"\"\"\n        Tests performance and behavior with a very large queryset.\n        \"\"\"\n        # Create a huge number of articles.\n        self.articles = [\n            Article.objects.create(headline=f\"Article {x}\", pub_date=datetime(2005, 7, 29))\n            for x in range(1000)\n        ]\n        paginator = Paginator(Article.objects.order_by(\"id\"), 20)\n\n        # Check the page count.\n        self.assertEqual(paginator.num_pages, 51)  # 1000 objects / 20 per page = 50 pages\n\n        # Check that we can iterate through all pages.\n        for page in paginator.page_range:\n            page_object = paginator.page(page)\n            self.assertEqual(len(page_object.object_list), 20)\n        \n        # Check that we can access specific pages.\n        for page_number in range(1, 51):\n            page_object = paginator.page(page_number)\n            self.assertEqual(page_object.number, page_number)\n            self.assertEqual(len(page_object.object_list), 20)\n\n\n\n", "    def test_paginating_with_custom_paginator_class(self):\n        class CustomPaginator(Paginator):\n                return super().get_elided_page_range(number, *args, **kwargs)\n\n        paginator = CustomPaginator(Article.objects.order_by(\"id\"), 5)\n        p = paginator.page(1)\n        self.assertEqual(str(p), \"<Page 1 of 2>\")\n\n\n\n", "    def test_paginating_with_object_count(self):\n        paginator = Paginator(\n            Article.objects.order_by(\"id\"), 5, orphans=1, allow_empty_first_page=False\n        )\n        self.assertEqual(paginator.count, len(self.articles))\n        self.assertEqual(paginator.num_pages, 2)\n\n\n\n", "    def test_paginating_with_reverse_order(self):\n        paginator = Paginator(Article.objects.order_by(\"-id\"), 5)\n        p = paginator.page(1)\n        self.assertEqual(\"<Page 1 of 2>\", str(p))\n        self.assertSequenceEqual(p.object_list, self.articles[-5:])\n        self.assertTrue(p.has_next())\n        self.assertFalse(p.has_previous())\n        self.assertTrue(p.has_other_pages())\n        self.assertEqual(2, p.next_page_number())\n        with self.assertRaises(InvalidPage):\n            p.previous_page_number()\n        self.assertEqual(6, p.start_index())\n        self.assertEqual(10, p.end_index())\n\n", "    def test_paginating_with_custom_ordering(self):\n        paginator = Paginator(Article.objects.order_by('headline'), 5)\n        p = paginator.page(1)\n        self.assertSequenceEqual(\n            p.object_list,\n            sorted(self.articles, key=lambda x: x.headline)[:5],\n        )\n"], "sample_697": ["    def test_tmp_path_factory_no_uid_collision(tmp_path_factory: TempPathFactory):\n        \"\"\"Test that the user id from os.getuid() is not used if it's invalid.\"\"\"\n        monkeypatch.setattr(\"os.getuid\", lambda: -1)\n\n        tmp_factory = tmp_path_factory(None, lambda *args: None, _ispytest=True)\n        basetemp = tmp_factory.getbasetemp()\n\n        assert \"unknown\" in str(basetemp)\n\n\n\n", "def test_tmp_path_factory_handles_malformed_basetemp(tmp_path_factory: TempPathFactory, monkeypatch: MonkeyPatch) -> None:\n    monkeypatch.setattr(\"getpass.getuser\", lambda: \"os/<:*?;>agnostic\")\n    # _basetemp / _given_basetemp are cached / set in parallel runs, patch them\n    monkeypatch.setattr(tmp_path_factory, \"_basetemp\", None)\n    monkeypatch.setattr(tmp_path_factory, \"_given_basetemp\", None)\n    with pytest.raises(ValueError) as exc:\n        tmp_path_factory.from_config(FakeConfig(\"/tmp/foo/invalid\\\\path\"), _ispytest=True)\n    assert \"Invalid path\" in str(exc.value)\n", "def test_tmp_path_factory_handles_environment_variables(\n    tmp_path_factory: TempPathFactory, monkeypatch: MonkeyPatch", "def test_tmp_path_factory_handles_malformed_usernames(\n    tmp_path_factory: TempPathFactory, monkeypatch: MonkeyPatch", "    def test_tmp_path_factory_prefers_basetemp(\n        tmp_path_factory: TempPathFactory, monkeypatch: MonkeyPatch"], "sample_1017": ["compilation error", "def test_issue_12862():\n    x, y = symbols('x,y')\n    assert (x & y).as_set() == Interval(-oo, oo) & Interval(-oo, oo)\n    assert (x | y).as_set() == Interval(-oo, oo) | Interval(-oo, oo)\n    assert (~x).as_set() == Interval(-oo, oo)\n", "compilation error", "def test_ITE_with_symbolic_symbols():\n    x, y, z = symbols('x,y,z')\n    assert ITE(x, y, z).diff(x) == Piecewise((z, Eq(x, False)), (y, True))\n    assert ITE(y, x, z).diff(y) == Piecewise((z, Eq(y, False)), (x, True))\n    assert ITE(z, x, y).diff(z) == Piecewise((x, Eq(z, False)), (y, True))\n", "compilation error"], "sample_884": ["def test_property_deprecation():\n    with pytest.warns(FutureWarning, match=\"n_features_ is deprecated\"):\n        instance = MockClass2()\n        val = instance.n_features_\n    assert val == 10\n", "def test_property_deprecation():\n    with pytest.warns(FutureWarning, match=\"n_features_ is deprecated\"):\n        mock = MockClass2()\n        assert mock.n_features_ == 10\n", "compilation error", "def test_property_deprecation():\n    with pytest.warns(FutureWarning, match=\"n_features_ is deprecated\"):\n        instance = MockClass2()\n        assert instance.n_features_ == 10\n", "def test_property_deprecation():\n    with pytest.warns(FutureWarning, match=\"n_features_ is deprecated\"):\n        instance = MockClass2()\n        n_features = instance.n_features_\n    assert n_features == 10\n"], "sample_492": ["    def test_serialize_complex_objects(self):\n        class ComplexObject:\n                self.a = a\n                self.b = b\n\n                return f\"ComplexObject({self.a}, {self.b})\"\n\n        self.assertSerializedEqual(ComplexObject(1, \"hello\"))\n\n\n\n", "    def test_serialize_lazy_instance(self):\n        class LazyInstance:\n                self.value = value\n\n                return str(self.value)\n\n            @property\n                return 'LazyInstance', (self.value,)\n\n        writer = MigrationWriter(\n            migrations.Migration(), \n            include_header=False\n        )\n        string, imports = writer.serialize(LazyInstance(10))\n        self.assertEqual(string, \"'LazyInstance'(10)\")\n        self.assertEqual(imports, set())\n", "    def test_serialize_custom_options(self):\n        class CustomOption(models.Options):\n                return \"MyCustomOptions\"\n\n        class CustomModel(models.Model):\n            pass\n\n        CustomModel.custom_options = CustomOption()\n        \n        migration = type(\n            \"Migration\",\n            (migrations.Migration,),\n            {\n                \"operations\": [\n                    migrations.CreateModel(\n                        \"MyModel\", (), dict(options=CustomModel.custom_options), (models.Model,)\n                    )\n                ],\n                \"dependencies\": [],\n            },\n        )\n\n        writer = MigrationWriter(migration)\n        output = writer.as_string()\n\n        self.assertIn(\n            f\"MyCustomOptions\", output, \"Custom options should be included in migration output\"\n        )\n", "    def test_serialize_custom_base_serializers(self):\n        class CustomBaseSerializer(BaseSerializer):\n                return \"custom(%r)\" % obj, {}\n\n        class MySerializer(CustomBaseSerializer):\n            pass\n\n        MigrationWriter.register_serializer(\n            complex, MySerializer\n        )\n        self.assertSerializedEqual(complex(1, 2), \"custom( (1+2j))\", {'from migrations.test_writer import CustomBaseSerializer, MySerializer})\n        MigrationWriter.unregister_serializer(complex)\n\n\n\n", "    def test_serialize_migration_dependencies(self):\n        migration = type(\n            \"Migration\",\n            (migrations.Migration,),\n            {\n                \"operations\": [\n                    migrations.CreateModel(\"MyModel\", (), {}, (models.Model,)),\n                ],\n                \"dependencies\": [\n                    (\"testapp\", \"0005_fifth\"),\n                    (\"testapp02\", \"0005_third\"),\n                    (\"testapp02\", \"0004_sixth\"),\n                ],\n            },\n        )\n        writer = MigrationWriter(migration)\n        output = writer.as_string()\n        self.assertIn(\n            \"dependencies = [\\n\"\n            \"    ('testapp', '0005_fifth'),\\n\"\n            \"    ('testapp02', '0004_sixth'),\\n\"\n            \"    ('testapp02', '0005_third'),\\n\"\n            \"]\",\n            output,\n        )\n\n"], "sample_674": ["def test_prunetraceback_works(testdir):\n    p = testdir.makepyfile(\n        \"\"\"\n        import traceback\n            x = 1 / 0\n        \"\"\"\n    )\n    excinfo = p.run_test([\"--tb=short\"])[0].excinfo\n    item = p.itemlist[0]\n    item._prunetraceback(excinfo)\n    # Check that the traceback is shorter and doesn't contain the path\n    # from the test function's source line\n    assert len(excinfo.traceback) > 0\n    assert \"test_func.py\" not in ''.join(excinfo.traceback)\n", "def test_prunetraceback_removes_cutdir(testdir):\n    node = nodes.Collector(\n        fspath=testdir.tmpdir.join(\"example.py\"),\n        parent=None,\n        config=testdir.parseconfig(\n            \"-s\", \"--fulltrace\"\n        ),\n        session=testdir.getsession(),\n    )\n    excinfo = pytest._code.ExceptionInfo(\n        Exception(),\n        traceback=[\n            nodes.Traceback(\n                [\n                    node._prunetraceback(\n                        pytest._code.ExceptionInfo(Exception(), traceback=[\n                            nodes.Traceback([py.builtin._FrameObject(\n\n                                f\"example.py\", 1, \"test_example\")])\n                            ])\n                    )\n                ]\n            )\n        ],\n    )\n    node._repr_failure_py(excinfo, style=\"long\")\n\n", "def test_report_sections(testdir):\n    p = testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n            item = pytest.current_item\n            item.add_report_section(\"setup\", \"custom_section\", \"setup content\")\n            item.add_report_section(\"call\", \"more_setup\", \"more setup content\")\n            assert False\n    \"\"\"\n    )\n    result = testdir.runpytest(\n        \"-v\",\n        \"--collect-only\",\n    )\n    result.stdout.fnmatch_lines([\"test_one\"])\n\n\n\n", "def test_node_repr(testdir):\n    class MyNode(nodes.Node):\n        pass\n    n = MyNode(name=\"foo\", fspath=py.path.local(\"test\"))\n    assert repr(n) == \"<MyNode foo>\"\n", "def test_ischildnode_with_trailing_slash(testdir):\n    p = testdir.makepyfile(\n        \"\"\"\n            pass\n        \n            pass\n        \"\"\"\n    )\n    items = testdir.getitems(p)\n    assert nodes.ischildnode(\"tests\", \"tests/test_1.py\")\n    assert nodes.ischildnode(\"tests\", \"tests/test_1.py::test_1\")\n    assert nodes.ischildnode(\"tests\", \"tests/test_2.py::test_2\")\n\n\n\n"], "sample_1072": ["def test_mixed_arithmetic():\n    x, y = symbols('x,y')\n    assert floor(x + Rational(1, 2)) == floor(x) + 0\n    assert ceiling(x + Rational(1, 2)) == ceiling(x)\n    assert floor(x*Rational(1, 2)) == floor(x/2)\n    assert ceiling(x*Rational(1, 2)) == ceiling(x/2)\n    assert floor(x/Rational(1, 2)) == floor(2*x)\n    assert ceiling(x/Rational(1, 2)) == ceiling(2*x)\n\n    assert floor(x*pi) == floor(pi*x)\n    assert ceiling(x*pi) == ceiling(pi*x)\n\n    assert floor(x/pi) == floor(x/pi)\n    assert ceiling(x/pi) == ceiling(x/pi)\n\n    assert floor(x + y + pi) == floor(x) + floor(y) + pi\n    assert ceiling(x + y + pi) == ceiling(x) + ceiling(y) + pi\n\n    assert floor(x * y + pi) == floor(x*y) + pi\n    assert ceiling(x * y + pi) == ceiling(x*y) + pi\n", "def test_floor_with_complex_arguments():\n    from sympy.abc import x, y\n\n    assert floor(1 + 2*I) == 1 + floor(2*I)\n    assert floor(1 + 2*I + y*I) == 1 + floor(2*I + y*I)\n    assert floor(1 + 2*I*x) == 1 + floor(2*I*x)\n    assert floor(1 + 2*I + x*y*I) == 1 + floor(2*I + x*y*I)\n    assert floor(1 + 2*I * (x + y)) == 1 + floor(2*I * (x + y))\n    assert floor(1 + 2*I * complex(x, y)) == 1 + floor(2*I * complex(x, y))\n \n    assert floor(x + 2*I) == floor(x) + floor(2*I)\n    assert floor(x + 2*I + y*I) == floor(x) + floor(2*I + y*I)\n    assert floor(x + 2*I*x) == floor(x) + floor(2*I*x)\n    assert floor(x + 2*I + x*y*I) == floor(x) + floor(2*I + x*y*I)\n    assert floor(x + 2*I * (x + y)) == floor(x) + floor(2*I * (x + y))\n    assert floor(x + 2*I * complex(x, y)) == floor(x) + floor(2*I * complex(x, y))\n\n\n\n", "def test_issue_16154():\n    assert floor(x + I*y).as_real() == floor(x)\n    assert ceiling(x + I*y).as_real() == ceiling(x)\n    assert floor(x - I*y).as_real() == floor(x)\n    assert ceiling(x - I*y).as_real() == ceiling(x)\n\n", "def test_issue_16988():\n    assert floor(1.23456 + 0.76543*I) == 1 + 0.76543*I\n    assert ceiling(1.23456 + 0.76543*I) == 2 + 0.76543*I\n", "def test_issue_14139():\n    x = symbols('x')\n    assert floor(x).rewrite(floor) == x - floor(x)\n    assert ceiling(x).rewrite(ceiling) == x + ceiling(-x)\n    assert floor(floor(x)).rewrite(floor) == x - floor(x)\n    assert ceiling(ceiling(x)).rewrite(ceiling) == x + ceiling(-x)\n"], "sample_902": ["def test_pipeline_clone():\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    pca = PCA(n_components=2)\n    clf = LogisticRegression()\n\n    pipeline = Pipeline([('pca', pca), ('clf', clf)])\n    cloned_pipeline = clone(pipeline)\n\n    assert_true(pipeline.steps == cloned_pipeline.steps)\n    assert_true(pipeline.get_params() == cloned_pipeline.get_params())\n    assert_not_equal(pipeline, cloned_pipeline)\n    assert_true(isinstance(clone(pipeline), Pipeline))\n\n    pipeline.fit(X, y)\n    cloned_pipeline.fit(X, y)\n    assert_array_equal(pipeline.predict(X), cloned_pipeline.predict(X))\n\n    # Check that changing the parameters of one pipeline doesn't affect the\n    # other\n    pipeline.set_params(pca__n_components=1)\n    cloned_pipeline.set_params(clf__penalty='l2')\n\n    assert_array_equal(pipeline.predict(X),\n                       cloned_pipeline.predict(X))\n\n    # Check for nested pipelines\n    nested_pipeline = Pipeline([\n        ('outer', Pipeline([\n            ('inner', DummyTransf()),\n            ('inner_clf', DummyRegressor())\n        ])),\n        'final_clf', LogisticRegression()\n    ])\n    cloned_nested_pipeline = clone(nested_pipeline)\n\n    assert_true(nested_pipeline.steps == cloned_nested_pipeline.steps)\n    assert_true(nested_pipeline.get_params() == cloned_nested_pipeline.get_params())\n    assert_not_equal(nested_pipeline, cloned_nested_pipeline)\n    assert_true(isinstance(clone(nested_pipeline), Pipeline))\n\n    nested_pipeline.fit(X, y)\n    cloned_nested_pipeline.fit(X, y)\n    assert_array_equal(nested_pipeline.predict(X),\n                       cloned_nested_pipeline.predict(X))\n\n\n\n", "def test_pipeline_with_cache_dir():\n    cachedir = 'tmp/pipeline_test_cache'\n    os.makedirs(cachedir, exist_ok=True)\n    \n    transf = DummyTransf()\n    clf = SVC()\n    pipe = Pipeline([('transf', transf), ('clf', clf)], memory=cachedir)\n    pipe.fit(X, y)\n    \n    # Check the cache directory exists and has files\n    assert_true(os.path.exists(cachedir))\n    assert_true(len(os.listdir(cachedir)) > 0)\n\n    pipe2 = Pipeline([('transf', transf), ('clf', clf)], memory=cachedir)\n    pipe2.fit(X, y)\n    # Check the cache directory has not changed\n    assert_true(len(os.listdir(cachedir)) > 0) \n\n    os.rmdir(cachedir)\n", "def test_pipeline_invalid_attributes():\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n\n    pipe = Pipeline([('transf', Transf()), ('clf', Mult())])\n\n    # Test for invalid attributes\n    assert_raises_regex(AttributeError, 'Pipeline object has no attribute',\n                        getattr, pipe, 'nonexistent_attribute')\n\n    # Test for invalid attribute calls\n    assert_raises_regex(AttributeError, 'Pipeline object has no attribute',\n                        setattr, pipe, 'nonexistent_attribute', 1)\n\n\n\n", "def test_pipeline_sparse_input():\n    # Test handling of sparse matrices in Pipelines\n    from sklearn.datasets import load_svmlight_file\n    from sklearn.linear_model import LogisticRegression\n\n    X, y = load_svmlight_file(\"http://www.csie.ntu.edu.tw/~cjlin/libsvm/datasets/rcv1_train.libsvm\")\n    X = sparse.csr_matrix(X)\n\n    pipe = Pipeline([('clf', LogisticRegression())])\n    pipe.fit(X, y)\n    assert_array_almost_equal(pipe.predict(X), pipe.predict(X))\n", "def test_pipeline_with_custom_memory_location():\n    cachedir = \"tmp/my_cache\"\n    try:\n        os.makedirs(cachedir)\n        memory = Memory(cachedir=cachedir, verbose=0)\n        pipe = Pipeline([('transf', DummyTransf()), ('clf', SVC())], memory=memory)\n        pipe.fit(JUNK_FOOD_DOCS, np.repeat(0, len(JUNK_FOOD_DOCS)))\n        assert_true(os.path.exists(os.path.join(cachedir, 'pipeline_cache.pkl')))\n    finally:\n        shutil.rmtree(cachedir)\n\n"], "sample_288": ["    def test_key_transform_with_nested_contains(self):\n        tests = [\n            ('value__d__contains', KeyTransform('0', F('value__1__0')), 'e'),\n            ('value__d__contains', F('value__1__0'), 'e'),\n            ('value__d__contains', KeyTransform('1__f', KeyTransform('d', 'value')), 'g'),\n            ('value__d__contains', KeyTransform('1__f', 'g'), 'g'),\n            ('value__baz__contains', KeyTransform('a', 'value')),\n            ('value__baz__contains', F('value__a')),\n            ('value__baz__contains', KeyTransform('c', KeyTransform('a', 'value'))),\n        ]\n        for lookup, key_transform, value in tests:\n            with self.subTest(lookup=lookup, key_transform=key_transform, value=value):\n                self.assertIs(NullableJSONModel.objects.filter(\n                    **{lookup: key_transform},\n                ).exists(), True)\n\n\n\n", "    def test_ordering_complex_key_transform(self):\n        NullableJSONModel.objects.create(value={'a': 1, 'b': 2, 'c': 3})\n        NullableJSONModel.objects.create(value={'b': 2, 'a': 1, 'c': 3})\n        NullableJSONModel.objects.create(value={'a': 3, 'b': 2, 'c': 1})\n        qs = NullableJSONModel.objects.annotate(\n            sorted_d=ExpressionWrapper(\n                KeyTransform('d', 'value'), output_field=models.JSONField()\n            )\n        ).order_by('sorted_d')\n        self.assertSequenceEqual(\n            qs,\n            [\n                NullableJSONModel.objects.get(pk=1),\n                NullableJSONModel.objects.get(pk=2),\n                NullableJSONModel.objects.get(pk=3),\n            ],\n        )\n", "    def test_key_transform_with_null_value(self):\n        obj = NullableJSONModel.objects.create(value={'a': None})\n        self.assertEqual(NullableJSONModel.objects.filter(value__a=KeyTransform(None, 'value')).get(), obj)\n\n\n\n", "    def test_key_transform_with_subquery(self):\n        qs = NullableJSONModel.objects.annotate(\n            subquery_value=Subquery(\n                NullableJSONModel.objects.filter(pk=OuterRef('pk')).values('value')\n            ),\n            target_key=KeyTransform('d', 'subquery_value'),\n        )\n        self.assertSequenceEqual(\n            qs.filter(target_key__contains={'a': 1}), [self.objs[4]],\n        )\n", "    def test_key_transform_annotation_expression_nested(self):\n        related_obj = RelatedJSONModel.objects.create(\n            value={'d': [{'e': 'f', 'g': 'h'}]},\n            json_model=self.objs[4],\n        )\n        RelatedJSONModel.objects.create(\n            value={'d': [{'h': 'g', 'e': 'f'}]},\n            json_model=self.objs[4],\n        )\n        self.assertSequenceEqual(\n            RelatedJSONModel.objects.annotate(\n                key=F('value__d__0'),\n                related_key=F('json_model__value__d__0'),\n                chain=F('key__g'),\n                expr=Cast('key', models.JSONField()),\n            ).filter(chain=F('related_key__g')),\n            [related_obj],\n        )\n"], "sample_279": ["    def test_database_constraint_with_opclasses(self):\n        with self.assertRaises(IntegrityError):\n            UniqueConstraintProduct.objects.create(\n                name='p1', color='red', category='X'\n            )\n\n", "    def test_opclasses_name_validation(self):\n        msg = 'UniqueConstraint.opclasses must be a list or tuple.'\n        with self.assertRaisesMessage(ValueError, msg):\n            models.UniqueConstraint(\n                name='uniq_opclasses',\n                fields=['field'],\n                opclasses=123,\n            )\n\n\n", "    def test_unique_constraint_with_deferred_and_opclasses(self):\n        with self.assertRaisesMessage(\n            ValueError,\n            'UniqueConstraint with opclasses cannot be deferred.'\n        ):\n            models.UniqueConstraint(\n                fields=['name'],\n                name='deferred_opclass',\n                deferrable=models.Deferrable.DEFERRED,\n                opclasses=['text_pattern_ops'],\n            )\n\n", "    def test_opclasses_type(self):\n        msg = 'UniqueConstraint.opclasses must be a list or tuple.'\n        with self.assertRaisesMessage(ValueError, msg):\n            models.UniqueConstraint(\n                name='uniq_opclasses',\n                fields=['field'],\n                opclasses={1, 2},\n            )\n\n\n\n\n", "    def test_opclasses_with_condition(self):\n        msg = 'UniqueConstraint with conditions cannot have opclasses.'\n        with self.assertRaisesMessage(ValueError, msg):\n            models.UniqueConstraint(\n                fields=['name'],\n                name='name_with_condition_and_opclasses',\n                condition=models.Q(color__isnull=True),\n                opclasses=['text_pattern_ops'],\n            )\n\n\n"], "sample_1202": ["compilation error", "def test_issue_11297():\n    assert Float(1, 100).as_mpf_val(100) == Float(1, 100)._mpf_\n", "compilation error", "def test_issue_11707():\n    a = Float('1.1')\n    b = Rational(11, 10)\n    assert a != b\n    assert b != a\n\n\n\n", "def test_sympy_numbers_in_comparisons():\n    from sympy.external import import_module\n    import numpy as np\n    if not import_module('numpy'):\n        skip('numpy not installed. Abort numpy tests.')\n\n    for i in [S(1), S.Half, S(np.sqrt(2)), S(pi), S(np.e), S(2), -S(1), -S.Half, -S(np.sqrt(2)), -S(pi), -S(np.e), -S(2)]:\n        assert i != i\n        assert i == i\n        assert i <= i\n        assert i >= i\n        assert i < i\n        assert i > i\n\n\n"], "sample_1031": ["def test_quantity_from_string_with_unit_and_value():\n    quantity = Quantity.from_string(\"2.5 kg\")\n    assert quantity.name == \"kg\"\n    assert quantity.value == 2.5\n    assert quantity.dimension == mass\n\n    quantity = Quantity.from_string(\"1.2 m\")\n    assert quantity.name == \"m\"\n    assert quantity.value == 1.2\n    assert quantity.dimension == length\n\n    quantity = Quantity.from_string(\"3.14 s\")\n    assert quantity.name == \"s\"\n    assert quantity.value == 3.14\n    assert quantity.dimension == time  \n", "    def test_derived_units_conversion():\n        sys = UnitSystem((m, kg, s), (c,))\n        newton = sys.expand_unit(\"N\")\n        assert newton.dimension == mass * length/time**2\n        joule = sys.expand_unit(\"J\")\n        assert joule.dimension == mass * length**2/time**2\n\n        force = sys.expand_unit(\"force\")\n        assert force.dimension == mass * length / time**2\n\n        volume = sys.expand_unit(\"volume\")\n        assert volume.dimension == length**3\n\n        area = sys.expand_unit(\"area\")\n        assert area.dimension == length**2\n\n        angular_frequency = sys.expand_unit(\"rad/s\")\n        assert angular_frequency.dimension == 1/time \n\n        # Test dimensional analysis with scaled units\n        meter_per_second = sys.expand_unit(\"m/s\")\n        assert meter_per_second.dimension == length/time\n        kilogram_per_meter_cube = sys.expand_unit(\"kg/m^3\")\n        assert kilogram_per_meter_cube.dimension == mass / length**3\n\n", "def test_quantity_creation():\n    assert Quantity(\"meter\")._base_unit == m\n    assert Quantity(\"meter\")._dimension == length\n    assert Quantity(\"sec\")._base_unit == s\n    assert Quantity(\"sec\")._dimension == time\n\n    assert Quantity(\"kg\")._base_unit == kg\n    assert Quantity(\"kg\")._dimension == mass\n    assert Quantity(\"meter/second\")._base_unit == (m, s)\n    assert Quantity(\"meter/second\")._dimension == velocity\n\n    assert Quantity(\"kbyte\")._base_unit == byte\n    assert Quantity(\"kbyte\")._dimension == Quantity.INFORMATION\n    assert Quantity(\"kbyte\")._scale_factor == Rational(1024) * byte\n\n    assert Quantity(\"m/s\")._dimension == velocity\n    assert Quantity(\"m/s\")._units == (m, s)\n\n    assert Quantity(\"kg*m^2/s^2\")._dimension == energy\n", "def test_conversion():\n    # this test needs to be expanded to cover more conversion cases.\n    ms = UnitSystem((meter, second), (speed_of_light,))\n\n\n    # test conversion between units within the same dimension.\n\n    assert ms.convert(2*meter, dm).is_numeric\n    assert ms.convert(2*meter, inch).is_numeric\n\n    #\n    assert ms.convert(2*meter, meter) == 2*meter\n\n    # test conversion to / from derived units.\n    assert ms.convert(2*meter/second, velocity).is_numeric\n    assert ms.convert(2*meter/second, speed_of_light).is_numeric\n\n    # test conversion to / from units with SI prefixes.\n\n    assert ms.convert(2*km, meter).is_numeric\n    assert ms.convert(2*ms, meter).is_numeric\n\n    # test conversion between different unit systems (after adding support)\n    # currently this is a no-op as the unit system doesn't have\n    # the ability to convert between different unit systems.\n\n\n\n", "def test_dimension_equivalence():\n    ms = UnitSystem((m, s))\n    c = Quantity(\"c\")\n    c.set_dimension(velocity)\n    c.set_scale_factor(299792458)\n    m_with_c = ms.extend((c,), ())\n\n    assert m_with_c.dims_equivalent(ms.dims)\n    assert m_with_c.dims_equivalent((m, s, c))\n    assert not m_with_c.dims_equivalent((m, s, kg))\n\n\n\n"], "sample_1063": ["def test_incomplete_beta_scipy():\n    if not scipy:\n        skip(\"scipy not installed\")\n\n    f = incompletebeta(x, a, b)\n    F = lambdify((x, a, b), f, modules='scipy')\n\n    assert abs(incompletebeta(0.5, 1.3, 2.3) - F(0.5, 1.3, 2.3)) <= 1e-10\n", "compilation error", "def test_issue_14266():\n    if not mpmath:\n        skip(\"mpmath not installed\")\n    x = symbols('x')\n    f = lambdify(x, E**x, modules='mpmath')\n    assert abs(f(1.234) - mpmath.exp(mpmath.mpf('1.234'))) < 1e-10\n", "    def test_special_functions_issue_17220():\n        if not scipy:\n            skip(\"scipy not installed\")\n\n        from sympy.functions import gamma\n        x = symbols('x')\n        f = lambdify(x, gamma(x), modules='scipy')\n        assert abs(gamma(2.5) - f(2.5)) < 1e-10\n\n\n", "def test_g_test():\n    if not scipy:\n        skip(\"scipy not installed\")\n    f = lambdify(x, scipy.stats.chi2.ppf(x), modules='scipy')\n    assert abs(scipy.stats.chi2.ppf(0.9) - f(0.9)) < 1e-10\n\n\n"], "sample_126": ["    def test_mti_inheritance_model_removal_no_dependencies(self):\n        \"\"\" mti inheritance model removal even if it has no dependencies \"\"\"\n        Animal = ModelState('app', 'Animal', [\n            (\"id\", models.AutoField(primary_key=True)),\n        ])\n        Dog = ModelState('app', 'Dog', [], bases=('app.Animal',))\n        changes = self.get_changes([Animal, Dog], [Animal])\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['DeleteModel'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='Dog')\n\n\n\n", "    def test_mti_inheritance_model_removal_remaining_parent(self):\n        Animal = ModelState('app', 'Animal', [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=255)),\n        ])\n        Dog = ModelState('app', 'Dog', [], bases=('app.Animal',))\n        changes = self.get_changes([Animal, Dog], [Animal])\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['DeleteModel'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='Dog')\n        self.assertNotIn('name', [name for name, field in changes['app'][0].operations[0].fields])\n", "    def test_mti_inheritance_model_removal_with_fk_to_other_mti(self):\n        Animal = ModelState('app', 'Animal', [\n            (\"id\", models.AutoField(primary_key=True)),\n        ])\n        Dog = ModelState('app', 'Dog', [], bases=('app.Animal',))\n        Cat = ModelState('app', 'Cat', [], bases=('app.Animal',))\n        CatOwner = ModelState('app', 'CatOwner', [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"cat\", models.ForeignKey('app.Cat', models.CASCADE)),\n        ])\n        changes = self.get_changes([Animal, Dog, Cat, CatOwner], [Animal, CatOwner])\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['DeleteModel', 'RemoveField'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='Dog', db_name='Dog')\n        self.assertOperationAttributes(\n            changes, 'app', 0, 1, name='CatOwner', model_name='CatOwner', field_name='cat'\n        )\n\n", "    def test_add_existing_relation(self):\n        class MyModel(models.Model):\n            pass\n        \n        changes = self.get_changes([MyModel], [MyModel, MyModel])\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"AddField\"])\n\n\n\n\n", "    def test_mti_removal_without_base_model(self):\n        \"\"\"\n        #23731 - Removing an MTI model that isn't the base model\n        should keep the base model.\n        \"\"\"\n        Dog = ModelState('app', 'Dog', [], bases=('app.Animal',))\n        Animal = ModelState('app', 'Animal', [\n            (\"id\", models.AutoField(primary_key=True)),\n        ])\n        changes = self.get_changes([Dog, Animal], [])\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['DeleteModel'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='Dog')\n\n"], "sample_861": ["    def custom_scorer(y_true, y_pred):\n        return np.mean((y_true[:, 0] - y_pred[:, 0])**2)\n", "def test_grid_search_with_parameter_grid_with_nested_dict():\n    X, y = make_classification(n_samples=100, random_state=0)\n\n    param_grid = {\n        'tree': {'max_depth': [2, 5, 10], 'min_samples_split': [2, 5]},\n        'estimator': ['LogisticRegression', 'DecisionTreeClassifier']\n    }\n    grid_search = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5)\n    grid_search.fit(X, y)\n\n    for i in range(len(param_grid['tree'])):\n        for j in range(len(param_grid['estimator'])):\n            params = {key: value[i] for key, value in\n                      param_grid.items() for i in range(len(value))}\n            assert any(np.array_equal(gs.cv_results_['params'][k],\n                                      [params]) for k in range(len(gs.cv_results_['params'])))\n\n\n\n", "def test_grid_search_with_custom_splitter():\n    # Test with user-defined splitter.\n\n    class CustomSplitter(object):\n            self.n_splits = n_splits\n\n            return [(range(0, len(X)), range(len(X), len(X) * 2))] * self.n_splits\n\n    X, y = make_classification(n_samples=100, random_state=0)\n    grid_search = GridSearchCV(\n        SVC(random_state=0), param_grid={'C': [10]}, cv=CustomSplitter(n_splits=2)\n    )\n    grid_search.fit(X, y)\n    assert len(grid_search.cv_results_) == 2\n\n\n\n", "def test_random_search_empty_param_grid():\n    # Test if `RandomizedSearchCV` raises an error with an empty param_grid\n    with pytest.raises(ValueError,\n                       match='Param_grid must be a non-empty dictionary'):\n        RandomizedSearchCV(LogisticRegression(), {}, cv=5)\n", "def test_grid_search_with_precomputed_train_scores():\n    X, y = make_classification(n_samples=100, random_state=0)\n    param_grid = {'C': [0.1, 1, 10]}\n    train_scores = np.random.rand(100, 3)\n    clf = GridSearchCV(SVC(), param_grid, cv=3,\n                       return_train_score=True,\n                       precompute_train_scores=train_scores)\n    clf.fit(X, y)\n    assert_array_equal(clf.cv_results_['train_score'], train_scores)\n\n\n\n"], "sample_433": ["    def test_rename_field_with_initial_and_existing_model(self):\n        changes = self.get_changes(\n            [\n                ModelState(\n                    \"testapp\",\n                    \"Author\",\n                    [\n                        (\"id\", models.AutoField(primary_key=True)),\n                        (\"name\", models.CharField(max_length=200)),\n                        (\"published_date\", models.DateTimeField()),\n                    ],\n                )\n            ],\n            [\n                ModelState(\n                    \"testapp\",\n                    \"Author\",\n                    [\n                        (\"id\", models.AutoField(primary_key=True)),\n                        (\"name\", models.CharField(max_length=200)),\n                        (\"publication_date\", models.DateTimeField()),\n                    ],\n                    initial=True\n                )\n            ],\n            MigrationQuestioner({\"ask_rename\": True}),\n        )\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(\n            changes,\n            \"testapp\",\n            0,\n            [\"RenameField\", \"AlterModel\"],\n        )\n        self.assertOperationAttributes(\n            changes,\n            \"testapp\",\n            0,\n            1,\n            model_name=\"Author\",\n            field_name=\"published_date\",\n            new_field_name=\"publication_date\",\n        )\n\n", "    def test_suggest_name_with_conflicting_names(self):\n        class Migration1(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\n                    \"Person\", fields=[models.CharField(max_length=100)]\n                )\n            ]\n\n        class Migration2(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\n                    \"Person\", fields=[models.CharField(max_length=100)]\n                )\n            ]\n\n        migration1 = Migration1(\"0001_initial\", \"test_app\")\n        migration2 = Migration2(\"0002_another_person\", \"test_app\")\n\n        self.assertEqual(migration1.suggest_name(), \"person\")\n        self.assertEqual(migration2.suggest_name(), \"another_person\")\n", "    def test_suggest_name_with_order_with_respect_to(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\n                    \"Author\",\n                    fields=[\n                        (\"id\", models.AutoField(primary_key=True)),\n                        (\"name\", models.CharField(max_length=200)),\n                        (\"book\", models.ForeignKey(\"otherapp.Book\", models.CASCADE)),\n                    ],\n                    options={\"order_with_respect_to\": \"book\"},\n                )\n            ]\n\n        migration = Migration(\"some_migration\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"author_order_with_respect_to_book\") \n", "    def test_operation_order_affects_suggest_name(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\"Person\", fields=[]),\n                migrations.DeleteModel(\"Animal\"),\n            ]\n\n        migration = Migration(\"some_migration\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"person_delete_animal\")\n\n        migration = migrations.Migration(\"some_migration\", \"test_app\")\n        migration.operations = [\n            migrations.DeleteModel(\"Animal\"),\n            migrations.CreateModel(\"Person\", fields=[]),\n        ]\n        self.assertEqual(migration.suggest_name(), \"delete_animal_person\") \n\n\n", "    def test_suggest_name_for_rename_field(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.RemoveField(\n                    model_name=\"Author\",\n                    fields=[\"name\"],\n                ),\n            ]\n\n        migration = Migration(\"0001_initial\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"delete_author_name\")\n"], "sample_503": ["def test_marker_edgecolor_fill():\n    fig, ax = plt.subplots()\n    colors = ['red', 'green', 'blue']\n    edgecolors = ['black', 'white', 'cyan']\n    for i, (color, edgecolor) in enumerate(zip(colors, edgecolors)):\n        ax.plot([0, 1], [i, i], marker='o',\n                markerfacecolor=color, markeredgecolor=edgecolor,\n                markersize=10)\n    ax.legend(\n        ['red', 'green', 'blue'], title='Marker Colors')\n", "def test_path_with_nan():\n    x = np.array([1, 2, np.nan, 4])\n    y = np.array([5, 6, 7, np.nan])\n    line = mlines.Line2D(x, y)\n    with pytest.raises(ValueError):\n        line.get_path()\n", "def test_set_markersize_by_index():\n    fig, ax = plt.subplots()\n    x = np.array([1, 2, 3])\n    y = np.array([4, 5, 6])\n    line, = ax.plot(x, y)\n    line.set_markersize([10, 20, 30])\n    assert line.get_markersize(0) == 10\n    assert line.get_markersize(1) == 20\n    assert line.get_markersize(2) == 30\n\n\n\n", "def test_marker_scaling():\n    fig, ax = plt.subplots()\n    sizes = [2, 5, 10]\n    for i, s in enumerate(sizes):\n        ax.plot(np.arange(4), [i] * 4, marker='o', markersize=s)\n\n\n", "def test_line_dashed_with_marker(fig_test, fig_ref):\n    fig_test.add_subplot().plot([0, 1, 2], [0, 1, 2], linestyle=\"dashed\", marker=\"o\")\n    fig_ref.add_subplot().plot([0, 1, 2], [0, 1, 2], linestyle=\"dashed\", marker=\"o\")\n"], "sample_342": ["    def test_limit_choices_to_with_pagination(self):\n        q1 = Question.objects.create(question='question 1', pk=1)\n        q2 = Question.objects.create(question='question 2', pk=2)\n        q3 = Question.objects.create(question='question 3', pk=3)\n        q4 = Question.objects.create(question='question 4', pk=4)\n        q5 = Question.objects.create(question='question 5', pk=5)\n        q6 = Question.objects.create(question='question 6', pk=6)\n\n        # Limit choices to questions starting with 'question 1' or 'question 2'\n        q1.limit_choices_to_questions = ['question 1', 'question 2']\n        q1.save()\n        self.selenium.get(self.live_server_url + reverse('autocomplete_admin:admin_views_answer_add'))\n        elem = self.selenium.find_element_by_css_selector('.select2-selection')\n        elem.click()  # Open the autocomplete dropdown.\n        results = self.selenium.find_element_by_css_selector('.select2-results')\n        self.assertTrue(results.is_displayed())\n        \n        # Check that only the relevant results are displayed\n        options = self.selenium.find_elements_by_css_selector('.select2-results__option')\n        self.assertEqual(len(options), 2)  \n        self.assertIn('question 1', [option.text for option in options])\n        self.assertIn('question 2', [option.text for option in options])\n\n        # Load next page of results (if available) and check if the limit still applies\n        self.selenium.find_element_by_css_selector('.select2-results__footer button.select2-results__pagination--next').click()\n        with self.select2_ajax_wait():\n            results = self.selenium.find_element_by_css_selector('.select2-results')\n            self.assertTrue(results.is_displayed())\n            options = self.selenium.find_elements_by_css_selector('.select2-results__option')", "    def test_complex_filter(self):\n        q1 = Question.objects.create(question='question 1', name='q1')\n        q2 = Question.objects.create(question='question 2', name='q2')\n        q3 = Question.objects.create(question='question 3', name='q3')\n        with model_admin(Question, QuestionAdmin):\n            qs = Question.objects.filter(name__startswith='q')\n            self.assertEqual(len(qs), 3)\n\n            class ComplexFilteringQuestionAdmin(QuestionAdmin):\n                complex_filter = lambda qs, value: qs.filter(\n                    name__startswith=value\n                )\n\n            with model_admin(Question, ComplexFilteringQuestionAdmin):\n                request = self.factory.get(self.url, {'term': 'q', **self.opts})\n                request.user = self.superuser\n                response = AutocompleteJsonView.as_view(**self.as_view_args)(request)\n                self.assertEqual(response.status_code, 200)\n                data = json.loads(response.content.decode('utf-8'))\n                self.assertEqual(data, {\n                    'results': [{'id': str(q.pk), 'text': q.question} for q in qs],\n                    'pagination': {'more': False},\n                })\n\n", "    def test_autocomplete_permissions(self):\n        class PermissionDeniedModelAdmin(Admin.ModelAdmin):\n            pass\n        with model_admin(Question, PermissionDeniedModelAdmin):\n            self.selenium.get(self.live_server_url + reverse('autocomplete_admin:admin_views_question_add'))\n            self.selenium.find_element_by_css_selector('.select2-selection').click()\n            results = self.selenium.find_element_by_css_selector('.select2-results')\n            self.assertFalse(results.is_displayed())\n\n\n\n", "    def test_remote_field_with_pk(self):\n        class PKChildAdmin(admin.ModelAdmin):\n            search_fields = ['name']\n\n        with model_admin(PKChild, PKChildAdmin):\n            # Add some parent records\n            Parent.objects.bulk_create(\n                Parent(name=str(i)) for i in range(10)\n            )\n            # Create some child records with different parent relations\n            child_pks = [Parent.objects.get(name=str(i)).pk for i in range(10)]\n            PKChild.objects.bulk_create(\n                PKChild(parent=parent, name=str(j))\n                for j in range(10) for parent in child_pks\n            )\n            self.selenium.get(self.live_server_url + reverse('autocomplete_admin:admin_views_toy_add'))\n            elem = self.selenium.find_element_by_css_selector('.select2-selection')\n            elem.click()\n            results = self.selenium.find_element_by_css_selector('.select2-results')\n            self.assertTrue(results.is_displayed())\n            search = self.selenium.find_element_by_css_selector('.select2-search__field')\n            search.send_keys('3')  \n            with self.select2_ajax_wait():\n                results = self.selenium.find_elements_by_css_selector('.select2-results__option')\n                self.assertEqual(len(results), 1)\n\n\n\n", "    def test_autocomplete_results_respect_field_name(self):\n        class CustomAdmin(admin.ModelAdmin):\n            search_fields = ['question']\n            autocomplete_fields = ['related_question']\n\n        with model_admin(Question, CustomAdmin):\n            site.autocomplete_view(self.factory.get(self.url, {'term': '', **self.opts}))\n\n\n"], "sample_248": ["    def test_no_startup_option(self):\n        with captured_stdout() as stdout:\n            call_command('shell', no_startup=True)\n        # Check we didn't execute any initialization code\n        self.assertEqual(stdout.getvalue().strip(), \"\")\n", "    def test_no_startup_option(self, select):\n        with captured_stdout() as stdout:\n            call_command('shell', no_startup=True, interface='python')\n        self.assertFalse(stdout.getvalue().strip())  \n\n", "    def test_no_startup_option(self):\n        with captured_stdout() as stdout:\n            call_command('shell', no_startup=True)\n        self.assertEqual(stdout.getvalue().strip(), '')\n", "    def test_no_startup_option(self, select):\n        with captured_stdout() as stdout:\n            call_command('shell', no_startup=True)\n        self.assertNotIn('exec(compile(pythonrc_code, pythonrc, \\'exec\\')', stdout.getvalue())\n\n", "    def test_no_startup_option(self):\n        with captured_stdout() as stdout:\n            call_command(\n                'shell',\n                no_startup=True,\n                command=self.script_globals,\n            )\n        self.assertEqual(stdout.getvalue().strip(), 'True') \n"], "sample_280": ["    def test_aggregation_default_with_subquery(self):\n        long_books_avg_price = Book.objects.filter(pages__gt=600).values('publisher').annotate(\n            avg_price=Avg('price')\n        ).order_by('publisher')\n        default_avg_price = Avg('price', filter=~Q(pages__gt=600), default=0.0)\n        result = Book.objects.annotate(\n            weighted_avg_price=default_avg_price + (\n                greatest(F('publisher__avg_price'), 0) * 0.4\n            )\n        ).values().order_by('id')\n        self.assertSequenceEqual(\n            list(result),\n            [{'weighted_avg_price': 14.11083333333333},\n             {'weighted_avg_price': 14.11083333333333},\n             {'weighted_avg_price': 14.11083333333333},\n             {'weighted_avg_price': 14.11083333333333},\n             {'weighted_avg_price': 14.11083333333333},\n             {'weighted_avg_price': 14.11083333333333},\n            ],\n        )\n", "    def test_aggregation_default_with_subquery(self):\n        subquery_qs = Publisher.objects.filter(book__contact__name='Adrian Holovaty').values('name')\n        result = Publisher.objects.annotate(\n            publisher_count=Subquery(subquery_qs),\n        ).aggregate(\n            value=Sum(\n                'num_awards',\n                filter=Q(publisher_count__gt=0),\n                default=Avg('num_awards'),\n            )\n        )\n        self.assertAlmostEqual(result['value'], 10.5, places=1)\n", "    def test_aggregation_invalid_default(self):\n        msg = 'Default value must be a constant expression.'\n        with self.assertRaisesMessage(TypeError, msg):\n            Book.objects.aggregate(value=Avg('rating', default=F('rating'))\n            )\n", "    def test_aggregation_default_with_subquery(self):\n        with self.assertNumQueries(1):\n            result = Publisher.objects.annotate(\n                avg_rating=Avg('book__rating'),\n            ).filter(avg_rating__lt=4).aggregate(\n                value=Sum('num_awards', default=Subquery(\n                    Publisher.objects.filter(avg_rating__gt=4).values('num_awards').annotate(\n                        count=Count('*'),\n                    ).values('count'),\n                )),\n            )\n        self.assertEqual(result['value'], 1)\n\n\n\n", "    def test_aggregation_default_using_expression_with_subquery(self):\n        with self.assertNumQueries(1):\n            result = Publisher.objects.aggregate(\n                value=Sum('book__rating', default=Subquery(\n                    Book.objects.filter(rating__gt=4).values('rating').order_by('rating').first().values_list('rating', flat=True)[0]\n                )),\n            )\n        self.assertIsInstance(result['value'], int)\n\n\n"], "sample_403": ["    def test_references_field_by_auto_created(self):\n        operation = FieldOperation(\n            \"Model\",\n            \"field\",\n            models.CharField(\n                max_length=100, auto_created=True, blank=True, null=True\n            ),\n        )\n        self.assertIs(\n            operation.references_field(\"Model\", \"field\", \"migrations\"), True\n        )\n\n\n\n\n", "    def test_references_field_by_related_name(self):\n        operation = FieldOperation(\n            \"Model\",\n            \"field\",\n            models.ForeignKey(\"Other\", models.CASCADE, related_name=\"related\"),\n        )\n        self.assertIs(operation.references_field(\"Other\", \"related\", \"migrations\"), True)\n        self.assertIs(operation.references_field(\"Other\", \"whatever\", \"migrations\"), False)", "    def test_references_field_by_through_field_defaults(self):\n        operation = FieldOperation(\n            \"Model\",\n            \"field\",\n            models.ManyToManyField(\n                \"Other\", through=\"Through\", through_fields=(\"through_field\", )\n            ),\n        )\n        self.assertIs(operation.references_field(\"Other\", \"whatever\", \"migrations\"), True)\n        self.assertIs(operation.references_field(\"Through\", \"through_field\", \"migrations\"), True)\n\n", "    def test_references_field_by_through_model_field(self):\n        operation = FieldOperation(\n            \"Model\",\n            \"field\",\n            models.ManyToManyField(\n                \"Other\", through=\"Through\", through_model_field=\"rel\"\n            ),\n        )\n        self.assertIs(\n            operation.references_field(\"Other\", \"whatever\", \"migrations\"), True\n        )\n        self.assertIs(\n            operation.references_field(\"Through\", \"rel\", \"migrations\"), True\n        )\n        self.assertIs(\n            operation.references_field(\"Through\", \"whatever\", \"migrations\"), False\n        )\n", "    def test_references_field_by_primary_key_relation(self):\n        operation = FieldOperation(\n            \"Model\",\n            \"field\",\n            models.OneToOneField(\n                \"Other\", models.CASCADE, primary_key=True, on_delete=models.CASCADE\n            ),\n        )\n        self.assertIs(\n            operation.references_field(\"Other\", \"id\", \"migrations\"), True\n        )\n        self.assertIs(\n            operation.references_field(\"Other\", \"whatever\", \"migrations\"), False\n        )\n        self.assertIs(\n            operation.references_field(\"Missing\", \"whatever\", \"migrations\"), False\n        )\n\n\n\n"], "sample_821": ["def test_affinity_propagation_preference_range():\n    # Test affinity_propagation() with various preference ranges\n    X = np.array([[0, 0], [1, 1], [-2, -2]])\n    S = -euclidean_distances(X, squared=True)\n\n    # Test with preference less than all similarities\n    for pref in [-10, -1, -0.5]:\n        cluster_center_indices, labels = affinity_propagation(\n            S, preference=pref)\n        assert_array_equal([0], cluster_center_indices)\n        assert_array_equal([0, 0, 0], labels)\n\n    # Test with preference greater than all similarities\n    for pref in [1, 10, 100]:\n        cluster_center_indices, labels = affinity_propagation(\n            S, preference=pref)\n        assert_array_equal([0, 1, 2], cluster_center_indices)\n        assert_array_equal([0, 1, 2], labels)\n\n    # Test with random preference\n", "    def test_affinity_propagation_sparse_input():\n        \"\"\"Test AffinityPropagation with sparse input\"\"\"\n        from scipy.sparse import csr_matrix\n        X_sparse = csr_matrix(np.random.randn(100, 10))\n        ap = AffinityPropagation(affinity=\"euclidean\")\n        ap.fit(X_sparse)\n        assert_no_warnings(ap.predict, X_sparse)\n", "def test_affinity_propagation_preference_dtype():\n    \"\"\"\n    Test that the preference array can take different data types.\n    \"\"\"\n\n    X = np.array([[1, 2], [1, 4], [1, 0], [4, 2], [4, 4], [4, 0]])\n    for dtype in [np.float32, np.float64, np.int32, np.int64]:\n        af = AffinityPropagation(preference=np.array([1, 2, 3, 4], dtype=dtype))\n        af.fit(X)\n        assert af.preference.dtype == dtype\n\n\n\n", "compilation error", "def test_affinity_propagation_preference():\n    # Test affinity_propagation() with various preference values\n    X = np.array([[1, 2], [1, 4], [1, 0], [4, 2], [4, 4], [4, 0]])\n    S = -euclidean_distances(X, squared=True)\n\n    # Test with no preference\n    cluster_centers_indices, labels = affinity_propagation(\n        S, preference=None)\n    assert_array_equal(np.unique(labels), np.array([0, 1, 2]))\n\n    # Test with preference larger than similarities\n    preference = np.max(S) * 2\n    cluster_centers_indices, labels = affinity_propagation(\n        S, preference=preference)\n    assert_array_equal(np.unique(labels), np.array([0]))\n\n    # Test with preference smaller than similarities\n    preference = np.min(S) / 2\n    cluster_centers_indices, labels = affinity_propagation(\n        S, preference=preference)\n    assert_array_equal(np.unique(labels), np.array([0, 1, 2]))\n\n    # Test with preference equal to similarities\n    preference = np.median(S)\n    cluster_centers_indices, labels = affinity_propagation(\n        S, preference=preference)\n    assert np.all(np.diff(labels) != 0)\n\n\n"], "sample_290": ["    def test_rename_field(self):\n        changes = self.get_changes(\n            [self.author_empty], [self.author_with_name_field]\n        )\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"RenameField\"])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name='first_name', field='given_name') \n\n\n", "    def test_suggest_name_with_existing_migrations(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel('Person', fields=[]),\n            ]\n        migration1 = Migration('0001_initial', 'test_app')\n        migration2 = Migration('0002_add_field', 'test_app')\n        self.assertEqual(migration1.suggest_name(), 'person')\n        self.assertEqual(migration2.suggest_name(), 'add_field')\n", "    def test_auto_with_operations(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel('Person', fields=[]),\n            ]\n\n        migration = migrations.Migration('0001_initial', 'test_app')\n        suggest_name = migration.suggest_name()\n        self.assertEqual(suggest_name, 'person')\n", "    def test_multi_parts_model_name(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\n                    'test_app.multi_parts.subpackage.MyModel', fields=[]\n                )\n            ]\n\n        migration = Migration('0001_initial', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'multi_parts_subpackage_mymodel')\n", "    def test_suggest_name_with_field_added_to_existing_model(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.AddField('Person', 'age', models.IntegerField(default=0)),\n            ]\n\n        migration = Migration('some_migration', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'add_age_to_person')\n"], "sample_638": ["def test_output_directory(mock_writer, capsys):\n    \"\"\"Test the output directory functionality.\"\"\"\n    main.Run([\"-o\", \"dot\", TEST_DATA_DIR, \"-d\", \"output_dir\"])\n    mock_writer.DiagramWriter().write.assert_called_once()\n    assert \"output_dir\" in mock_writer.DiagramWriter().write.call_args[1]\n", "def test_project_name_is_set(mock_writer, mock_subprocess):\n    \"\"\"Test that the project name is correctly set.\"\"\"\n    with mock.patch(\"sys.argv\", [\"pyreverse\", \"-p\", \"my_project\", TEST_DATA_DIR]):\n        main.Run([])\n    assert main.Run.project == \"my_project\"\n", "def test_project_name_is_used(mock_writer, mock_subprocess):\n    \"\"\"Test that the project name set with the --p flag is used in the output.\"\"\"\n    with mock.patch(\"pylint.pyreverse.main.sys\") as mock_sys:\n        mock_sys.argv = [\"pyreverse\", \"-p\", \"my_project\", \"-o\", \"dot\", TEST_DATA_DIR]\n        main.Run(mock_sys.argv)\n    mock_writer.DiagramWriter().write.assert_called_once()\n    mock_project = mock.MagicMock()\n    mock_writer.DiagramWriter().write.call_args_list[0][0][0].project_name.assert_equal(\"my_project\")\n", "def test_handle_empty_command_line_args(capsys):\n    \"\"\"Test handling of empty command line arguments.\"\"\"\n    with pytest.raises(SystemExit) as wrapped_sysexit:\n        main.Run([])\n    assert wrapped_sysexit.value.code == 1\n    stdout = capsys.readouterr().out\n    assert \"usage: pylint.pyreverse [options] <packages>\" in stdout\n", "def test_output_directory_is_used(capsys, mock_subprocess):\n    \"\"\"Test that the output directory is used.\"\"\"\n    main.Run([\"-o\", \"png\", \"-d\", \"output_dir\", TEST_DATA_DIR])\n    mock_subprocess.run.assert_called_with(\n        ... , check=True, cwd=\"output_dir\"\n    )\n    \n\n\n\n"], "sample_225": ["    def test_unregister_model(self):\n        class ArticleAdmin(admin.ModelAdmin):\n            pass\n        site.register(Article, ArticleAdmin)\n        site.unregister(Article)\n        self.assertNotIn(Article, site._registry)\n", "    def test_check(self):\n        class BadModel(object):\n            _meta = object()\n            _meta.abstract = True\n\n        class GoodModel(object):\n            _meta = object()\n\n        self.site.register(BadModel)\n        self.site.register(GoodModel)\n\n        errors = self.site.check([])\n        self.assertEqual(len(errors), 1)\n        self.assertIn(\n            \"The model BadModel is abstract, so it cannot be registered with admin.\",\n            str(errors[0]),\n        )\n", "    def test_register_unregister(self):\n        class TestModel(models.Model):\n            pass\n\n        with self.assertRaises(AlreadyRegistered):\n            admin.site.register(TestModel)\n            admin.site.register(TestModel)\n\n        self.assertIn(TestModel, admin.site._registry)\n        admin.site.unregister(TestModel)\n        with self.assertRaises(NotRegistered):\n            admin.site._registry[TestModel]\n\n\n\n        \n", "    def test_register_unregister(self):\n        self.assertIn(User, site._registry)\n        self.assertIn(Article, site._registry)\n        site.unregister(User)\n        self.assertNotIn(User, site._registry)\n        self.assertIn(Article, site._registry)\n\n        with self.assertRaises(NotRegistered):\n            site.unregister(User)\n", "    def test_unregister_action(self):\n        action_name = 'delete_selected'\n        self.assertEqual(self.site._actions[action_name], delete_selected)\n        self.site.unregister(Article)\n        with self.assertRaises(KeyError):\n            self.site._actions[action_name]\n"], "sample_743": ["compilation error", "compilation error", "    def my_metric(x, y):\n        return np.sum(x * y)\n  ", "compilation error", "def test_predict_sparse_kneighbors():\n    rng = np.random.RandomState(0)\n    X = rng.rand(5, 5)\n    y = rng.randint(0, 2, 5)\n\n    # Fit on dense and sparse data\n    nn_dense = neighbors.KNeighborsClassifier(n_neighbors=1).fit(X, y)\n    nn_sparse = neighbors.KNeighborsClassifier(n_neighbors=1).fit(csr_matrix(X), y)\n\n    # Predict on sparse data\n    assert_array_equal(nn_dense.predict(csr_matrix(X)), nn_sparse.predict(csr_matrix(X)))\n\n    # Predict on dense data\n    assert_array_equal(nn_dense.predict(X), nn_sparse.predict(X))\n"], "sample_546": ["compilation error", "def test_toolmanager_toggle_tool():\n    with pytest.warns(UserWarning, match=_EXPECTED_WARNING_TOOLMANAGER):\n        plt.rcParams['toolbar'] = 'toolmanager'\n    fig = plt.gcf()\n    assert fig.canvas.manager.toolmanager.get_tool('pan') is not None\n    fig.canvas.manager.toolmanager.toggle_tool('pan')\n    assert fig.canvas.manager.toolmanager.get_tool('pan') is None\n    fig.canvas.manager.toolmanager.toggle_tool('pan')\n    assert fig.canvas.manager.toolmanager.get_tool('pan') is not None\n\n", "compilation error", "def test_toolmanager_clear_selected_tools():\n    with pytest.warns(UserWarning, match=_EXPECTED_WARNING_TOOLMANAGER):\n        plt.rcParams['toolbar'] = 'toolmanager'\n    fig = plt.gcf()\n    manager = fig.canvas.manager.toolmanager\n    manager.add_tool('rubberband')\n    manager.add_tool('pan')\n    manager.add_tool('zoom')\n    assert len(manager.selected_tools) == 3\n    selected_tools = list(manager.selected_tools)\n    manager.clear_selected_tools()\n    assert len(manager.selected_tools) == 0\n    assert selected_tools == manager.selected_tools\n\n    \n", "def test_toolmanager_set_mode():\n    with pytest.warns(UserWarning, match=_EXPECTED_WARNING_TOOLMANAGER):\n        plt.rcParams['toolbar'] = 'toolmanager'\n    fig = plt.gcf()\n    initial_mode = fig.canvas.manager.toolmanager.get_mode()\n    assert initial_mode is None\n\n    # Set zoom mode\n    fig.canvas.manager.toolmanager.get_tool('zoom').activate()\n    assert fig.canvas.manager.toolmanager.get_mode() == 'ZOOM'\n\n    # Set pan mode\n    fig.canvas.manager.toolmanager.get_tool('pan').activate()\n    assert fig.canvas.manager.toolmanager.get_mode() == 'PAN'\n\n    # Set None mode (should clear previous modes)\n    fig.canvas.manager.toolmanager.set_mode(None)\n    assert fig.canvas.manager.toolmanager.get_mode() is None\n\n\n"], "sample_428": ["    def test_grouping_with_custom_intervals(self):\n        self.assertEqual(\n            nformat(123456789, \".\", grouping=(3, 2, 0), thousand_sep=\",\"),\n            \"123,456,789\",\n        )\n        self.assertEqual(\n            nformat(123456789, \".\", grouping=(3, 3, 0), thousand_sep=\",\"),\n            \"123,456,789\",\n        )\n", "    def test_non_numeric_input(self):\n        self.assertEqual(nformat(\"abc\", \".\"), \"abc\")\n        self.assertEqual(nformat(True, \".\"), \"True\")\n        self.assertEqual(nformat(False, \".\"), \"False\")\n        self.assertEqual(nformat(list(range(10)), \".\"), \"list(range(10))\")\n        self.assertEqual(nformat({}, \".\"), \"{}\")\n", "    def test_grouping_with_list(self):\n        self.assertEqual(nformat(1234567890123, \".\", grouping=(3, 3, 0), thousand_sep=\",\"), \"12,345,678,901,23\")\n", "    def test_large_scientific_notation(self):\n        number = Decimal(\"1.23456789012345678901234567890123456789012345678901234567890\")\n        self.assertEqual(nformat(number, \".\", decimal_pos=20), \"1.23456789012345678901234567890123456789012345678901234567890e+20\")  \n", "    def test_grouping_with_sequence(self):\n        self.assertEqual(nformat(123456789, \".\", grouping=(3, 2, 0), thousand_sep=\",\"), \"123,456,789\")\n        self.assertEqual(nformat(123456789, \".\", grouping=(3, 2, 0), thousand_sep=\" \"), \"123 456,789\")\n        self.assertEqual(nformat(123456789, \".\", grouping=(2, 3, 0), thousand_sep=\",\"), \"123,456,789\")\n        self.assertEqual(nformat(123456789, \".\", grouping=(2, 3, 0), thousand_sep=\" \"), \"123 456,789\")\n\n\n"], "sample_587": ["    def test_merge_compat_with_attrs(self):\n        ds1 = xr.Dataset({\"x\": ([], 0, {\"foo\": \"bar\"})})\n        ds2 = xr.Dataset({\"x\": ([], 1, {\"baz\": \"qux\"})})\n        for compat in [\"identical\", \"equals\", \"broadcast_equals\"]:\n            with pytest.raises(xr.MergeError) as exc:\n                ds1.merge(ds2, compat=compat)\n            assert \"Attributes 'foo' and 'baz' are different\" in str(exc.value)\n\n        assert ds1.identical(ds1.merge(ds2, compat=\"override\"))\n", "    def test_merge_update(self):\n        data = create_test_data()\n        ds1 = data[[\"var1\"]]\n        ds2 = data[[\"var3\"]]\n        with raises_regex(ValueError, \"should be coordinates or not\"):\n            ds1.update(ds2.rename({\"var3\": \"var1\"}))\n\n        updated = ds1.update(ds2)\n        expected = data[[\"var1\", \"var3\"]]\n        assert expected.identical(updated)\n\n\n\n        ds2 = ds2.rename({\"var3\": \"foo\"})\n        updated = ds1.update(ds2)\n        assert expected.identical(updated)\n", "    def test_merge_dataset_with_dataarray_coords(self):\n        data = create_test_data()\n        ds = data.copy()\n        ds = ds.rename({\"var1\": \"new_var1\"})\n        ds = ds.merge(\n            xr.DataArray(\n                [1, 2, 3],\n                dims=\"x\",\n                coords={\"x\": ds.coords[\"x\"]},\n            )\n        )\n        assert ds.identical(data)\n", "    def test_merge_no_conflicts_broadcast_equals(self):\n        ds1 = xr.Dataset({\"x\": 0, \"y\": (\"x\", [0])})\n        ds2 = xr.Dataset({\"x\": (\"y\", [0, 0])})\n        actual = ds1.merge(ds2, compat=\"no_conflicts\")\n        assert ds2.identical(actual)\n\n        ds2 = xr.Dataset({\"x\": np.nan})\n        actual = ds1.merge(ds2, compat=\"no_conflicts\")\n        assert ds2.identical(actual)\n", "    def test_merge_explicit_coords(self):\n        data = create_test_data()\n        ds1 = data.copy(deep=True)\n        ds2 = data.copy(deep=True)\n        ds1 = ds1.rename({\"var1\": \"var1_prime\"})\n        ds2 = ds2.rename({\"var2\": \"var2_prime\"})\n        ds1 = ds1.reset_coords(drop=True)\n        ds2 = ds2.reset_coords(drop=True)\n\n        ds1 = ds1.merge(ds2)\n        assert ds1.identical(data)\n        ds1 = ds1.merge(ds2, explicit_coords=[(\"var1\",)])\n        assert ds1.identical(data)\n        ds1 = ds1.merge(ds2, explicit_coords=[(\"var2\",)])\n        assert ds1.identical(data)\n        with raises_regex(ValueError, \"cannot determine if 'var3' should be\"):\n            ds1 = ds1.merge(ds2, explicit_coords=[(\"var3\",)])\n\n"], "sample_1137": ["def test_issue_19124():\n    from sympy.physics.units import joule\n    assert joule.is_unit is True\n    assert joule.is_commutative is True\n    assert joule.is_dimensional_quantity is True\n    assert joule.is_symbol is False\n    assert joule.dim is not None\n    assert joule.name == \"joule\"\n    assert joule.symbol == 'J'\n    assert joule.get_global_relative_scale_factor() == 1\n    assert joule.get_global_relative_scale_factor(joule) == 1\n    assert joule.convert_to(joule) == joule\n    assert joule.unit_system == SI\n\n\n\n", "def test_issue_11092():\n    from sympy.physics.units import  meter, second\n    from sympy.physics.units import Quantity\n    \n    q = Quantity(\"q\", dimension=meter*second**2)\n    q.set_global_relative_scale_factor(1, meter*second**2)\n    assert q.dimension == meter*second**2\n    assert q.scale_factor == 1 * meter*second**2\n    \n    s = Quantity(\"s\", dimension=second)\n    s.set_global_relative_scale_factor(1, second)\n\n    assert q * s == Quantity(\"q*s\", dimension=meter*second**3)\n    assert q / s == Quantity(\"q/s\", dimension=meter*second)\n", "def test_issue_1227():\n    a = Quantity('a')\n    b = Quantity('b')\n    a.set_global_relative_scale_factor(1, meter)\n    b.set_global_relative_scale_factor(1, second)\n    assert (a / b).n() == 1.0*meter/second\n\n", "def test_issue_16336():\n    # Test that Quantity objects can be directly compared \n    # for equality with symbolic expressions\n    speed_of_light = Quantity(\"speed_of_light\", abbrev=\"c\", scale_factor=299792458, dimension=meter/second)\n    \n    assert speed_of_light == Quantity(\"speed_of_light\", abbrev=\"c\", scale_factor=299792458, dimension=meter/second)\n    assert speed_of_light != Quantity(\"speed_of_light\", abbrev=\"c\", scale_factor=300000000, dimension=meter/second)\n\n    assert speed_of_light == 299792458*meter/second\n    assert speed_of_light != 300000000*meter/second\n\n    assert speed_of_light == Quantity(\"c\", dimension=meter/second)\n\n    with warns_deprecated_sympy():\n        assert speed_of_light == Quantity.get_quantity_from_string(\"speed_of_light\", abbrev=\"c\")\n", "def test_issue_14547_continued():\n    from sympy.physics.units import foot, inch\n    from sympy import Eq\n\n    assert Eq(log(foot), log(inch) + log(12)).is_true\n    assert Eq(log(inch), log(foot) - log(12)).is_true\n    assert log(foot).is_algebraic is True\n    assert log(foot).is_rational is False\n\n\n\n"], "sample_780": ["def test_lda_perplexity_subsampling(sub_sampling):\n    # Test LDA perplexity with sub-sampling\n    n_components, X = _build_sparse_mtx()\n    lda = LatentDirichletAllocation(n_components=n_components, max_iter=1,\n                                    learning_method='batch', random_state=0)\n    lda.fit(X)\n    perp = lda.perplexity(X, sub_sampling=sub_sampling)\n    assert perp > 0.0\n", "def test_lda_empty_input():\n    \"\"\"Test LDA with empty input (all-zero features).\"\"\"\n    n_components, X = _build_sparse_mtx()\n    lda = LatentDirichletAllocation(n_components=n_components, max_iter=750,\n                                    random_state=0)\n    lda.fit(X)\n    assert_almost_equal(lda.components_.sum(axis=1), np.ones(n_components))\n    assert_equal(lda.transform(X).shape, (X.shape[0], n_components))\n\n\n\n", "    def test_lda_n_jobs_default():\n        # Test default n_jobs behavior\n        n_components, X = _build_sparse_mtx()\n        lda = LatentDirichletAllocation(n_components=n_components,\n                                        max_iter=1, learning_method='batch',\n                                        random_state=0)\n        lda.fit(X)\n\n        # Verify n_jobs defaults to 1\n        assert_equal(lda.n_jobs, 1)\n", "def test_lda_missing_samples():\n    n_components, X = _build_sparse_mtx()\n    lda = LatentDirichletAllocation(n_components=n_components, \n                                     total_samples=X.shape[0], random_state=0)\n    lda.fit(X)\n", "def test_lda_non_zero_offset():\n    # Test LDA with a non-zero learning offset\n    rng = np.random.RandomState(0)\n    n_components, X = _build_sparse_mtx()\n    offset = rng.randint(1, 10)\n    lda = LatentDirichletAllocation(n_components=n_components,\n                                    learning_offset=offset,\n                                    total_samples=100, random_state=rng)\n    lda.fit(X)\n\n    # Check if topics have been learned properly\n    correct_idx_grps = [(0, 1, 2), (3, 4, 5), (6, 7, 8)]\n    for c in lda.components_:\n        top_idx = set(c.argsort()[-3:][::-1])\n        assert tuple(sorted(top_idx)) in correct_idx_grps\n"], "sample_609": ["def test_polyval_broadcast(use_dask) -> None:\n    if use_dask and not has_dask:\n        pytest.skip(\"requires dask\")\n\n    xcoord = np.arange(10)\n    da = xr.DataArray(\n        np.stack((1.0 + xcoord, 1.0 + xcoord ** 2)),\n        dims=(\"d\", \"x\"),\n        coords={\"x\": xcoord, \"d\": [0, 1]},\n    )\n    coeffs = xr.DataArray(\n        [[2, 1], [3, 2]],\n        dims=(\"d\", \"degree\"),\n        coords={\"d\": [0, 1], \"degree\": [1, 0]},\n    )\n    if use_dask:\n        coeffs = coeffs.chunk({\"d\": 2})\n\n    da_pv = xr.polyval(da.x, coeffs)\n\n    xr.testing.assert_allclose(da, da_pv.T)\n\n", "compilation error", "compilation error", "def test_polyval_array_input(use_dask, use_datetime) -> None:\n    if use_dask and not has_dask:\n        pytest.skip(\"requires dask\")\n\n    if use_datetime:\n        xcoord = xr.DataArray(\n            pd.date_range(\"2000-01-01\", freq=\"D\", periods=10), dims=(\"x\",), name=\"x\"\n        )\n        x = xr.core.missing.get_clean_interp_index(xcoord, \"x\")\n    else:\n        xcoord = x = np.arange(10)\n\n    da = xr.DataArray(\n        np.stack((1.0 + x + 2.0 * x ** 2, 1.0 + 2.0 * x + 3.0 * x ** 2)),\n        dims=(\"d\", \"x\"),\n        coords={\"x\": xcoord, \"d\": [0, 1]},\n    )\n    coeffs_array = np.array([[[2, 1, 1], [3, 2, 1]]])\n\n    if use_dask:\n        coeffs_array = xr.DataArray(coeffs_array, dims=(\"d\", \"degree\", \"x\"), chunks={\"d\": 2, \"degree\": 2})\n    else:\n        coeffs_array = xr.DataArray(coeffs_array, dims=(\"d\", \"degree\", \"x\"))\n\n    da_pv = xr.polyval(da.x, coeffs_array)\n\n    xr.testing.assert_allclose(da, da_pv.T)\n", "compilation error"], "sample_475": ["    def test_actions_not_callable(self):\n        class BandAdmin(ModelAdmin):\n            actions = \"hello\"\n\n        self.assertIsInvalid(\n            BandAdmin,\n            Band,\n            \"'actions' should be a list of callable objects.\",\n            id=\"admin.E128\",\n        ) \n\n\n", "    def test_actions_valid_usage(self):\n        @admin.action(description=\"Mark as published\")\n            queryset.update(is_published=True)\n\n        class ValidationTestInline(TabularInline):\n            model = ValidationTestInlineModel\n            formset = BaseModelFormSet\n\n        class BandAdmin(ModelAdmin):\n            inlines = [ValidationTestInline]\n            actions = (mark_published,)\n\n        self.assertIsValid(BandAdmin, Band)\n", "    def test_actions_invalid_type(self):\n        class BandAdmin(ModelAdmin):\n            actions = \"hello\"\n\n        self.assertIsInvalid(\n            BandAdmin,\n            Band,\n            \"The value of 'actions' must be a list or tuple.\",\n            id=\"admin.E125\",\n        )\n\n\n\n\n", "    def test_actions_missing_permission(self):\n        @admin.action\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = [my_action]\n            permissions = (\"can_view_band\",)\n\n        self.assertIsInvalid(\n            BandAdmin,\n            Band,\n            \"The value of 'actions[0]' ('my_action') requires a permission \"\n            \"'can_view_band' which doesn't exist.\",\n            id=\"admin.E128\",\n        )\n\n", "    def test_actions_custom_actions(self):\n        @admin.action(\n            description=\"Action description for the custom action.\",\n            permission_required=None,\n        )\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = (custom_action,)\n\n        self.assertIsValid(BandAdmin, Band)\n"], "sample_1107": ["compilation error", "compilation error", "def test_is_prime():\n    assert is_prime(2) is True\n    assert is_prime(3) is True\n    assert is_prime(4) is False\n    assert is_prime(5) is True\n    assert is_prime(6) is False\n    assert is_prime(7) is True\n    assert is_prime(8) is False\n    assert is_prime(13) is True\n    assert is_prime(17) is True\n    assert is_prime(20) is False\n    assert is_prime(23) is True\n    assert is_prime(100) is False\n    assert is_prime(101) is True\n    assert is_prime(2**31 - 1) is True\n    assert is_prime(2**31) is False\n", "    def test_multiset_partitions_taocp():\n        assert list(multiset_partitions_taocp([3, 1])) == [[[3, 3], []], [[3], [3, 1]], [[3], [1, 3]]]\n        assert list(multiset_partitions_taocp([3, 1], [2, 3])) == [[[3, 3], []], [[3, 2], [1]], [[3], [3, 1]]]\n        assert list(multiset_partitions_taocp([3, 1], [1, 3, 2], 2)) == [[[3, 3], []], [[3, 2], [1]], [[3], [3, 1]]]\n        assert list(multiset_partitions_taocp([1], [2, 1])) == [[[1]], [[1]]]\n\n", "compilation error"], "sample_564": ["    def test_3d_plot_with_kwargs_color_alpha():\n        x = np.array([1, 2, 3])\n        y = np.array([4, 5, 6])\n        z = np.array([7, 8, 9])\n        \n        fig = plt.figure()\n        ax = fig.add_subplot(111, projection='3d')\n        ax.scatter(x, y, z, c='red', alpha=0.5, s=100)\n\n        # Assert that the plot has been drawn with the specified color and alpha\n        for c in ax.collections[0].get_facecolors():\n            assert np.isclose(c[0], 1.0)\n            assert np.isclose(c[1], 0.0)\n            assert np.isclose(c[2], 0.0)\n            assert np.isclose(c[3], 0.5) \n", "def test_scatter_with_non_finite_values_no_errors():\n    \"\"\"\n    Test scatter with non-finite coordinates, ensuring no errors.\n    GH#10060\n    \"\"\"\n    x = np.array([1, 2, np.inf, -np.inf, np.nan])\n    y = np.array([1, 2, np.inf, -np.inf, np.nan])\n    z = np.array([1, 2, np.inf, -np.inf, np.nan])\n    colors = [0.5, 0.7, 0.9, 0.2, 0.4]\n\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    ax.scatter(x, y, z, c=colors)\n    fig.canvas.draw()  # Force at least one drawing cycle\n\n\n\n", "def test_box():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    box = art3d.Poly3DCollection(\n        [\n            [[0, 0, 0], [1, 0, 0], [1, 1, 0], [0, 1, 0], [0, 0, 0]],\n            [[0, 0, 1], [1, 0, 1], [1, 1, 1], [0, 1, 1], [0, 0, 1]],\n            [[0, 0, 0], [0, 0, 1], [0, 1, 1], [0, 1, 0], [0, 0, 0]],\n            [[1, 0, 0], [1, 0, 1], [1, 1, 1], [1, 1, 0], [1, 0, 0]],\n            [[0, 0, 0], [1, 0, 0], [1, 0, 1], [0, 0, 1], [0, 0, 0]],\n            [[0, 1, 0], [1, 1, 0], [1, 1, 1], [0, 1, 1], [0, 1, 0]],\n        ],\n        facecolors=\"black\",\n        edgecolors=\"white\",\n    )\n    ax.add_collection3d(box)\n    assert isinstance(box, art3d.Poly3DCollection)\n\n\n\n\n", "def test_text_3d_bbox():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    txt = Text(0.5, 0.5, r'Foo bar')\n    art3d.text_2d_to_3d(txt, z=1)\n    ax.add_artist(txt)\n\n    bbox = txt.get_bbox()\n    assert bbox.width > 0\n    assert bbox.height > 0\n    assert bbox.zmin == 1\n    assert bbox.zmax == 1\n", "def test_text_3d_alignment():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    text = ax.text(0.5, 0.5, 0.5, 'Test', zorder=10,\n                   ha='center', va='center')\n    assert text.get_position()[2] == 0.5\n\n\n\n"], "sample_567": ["def test_baseline_alignment():\n    fig, ax = plt.subplots()\n    ax.text(0.5, 0.5, 'A', baseline='top', ha='center')\n    ax.text(0.5, 0.5, 'B', baseline='bottom', ha='center')\n    ax.text(0.5, 0.5, 'C', baseline='middle', ha='center')\n", "compilation error", "def test_text_clip_bbox_padding():\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, \"foo\", clip_on=True, bbox=dict(pad=2))\n    bbox = text.get_window_extent()\n\n    # Check that the bbox is larger than the text itself due to padding.\n    assert bbox.width > text.get_window_extent(renderer=fig.canvas.get_renderer()).width\n    assert bbox.height > text.get_window_extent(renderer=fig.canvas.get_renderer()).height\n\n", "def test_bbox_clip_inside():\n    fig, ax = plt.subplots()\n    ax.set_xlim(0, 10)\n    ax.set_ylim(0, 10)\n    text = ax.text(5, 5, 'foo', clip_on=True)\n    text.set_bbox(dict(\n        facecolor='red',\n        facealpha=0.5,\n        )\n    )\n    fig.canvas.draw()\n", "def test_annotate_and_offsetfrom_copy_input_different_methods(fig_test, fig_ref):\n    ax = fig_test.add_subplot()\n    l, = ax.plot([0, 2], [0, 2])\n    off_xy = np.array([.5, .5])\n    ax.annotate(\"foo\", textcoords=OffsetFrom(l, off_xy), xytext= (10, 0), xy=(0, 0))  # xy is unused.\n\n    ax = fig_ref.add_subplot()\n    l, = ax.plot([0, 2], [0, 2])\n    off_xy = np.array([.5, .5])\n    ax.annotate(\"foo\", xy=off_xy, xycoords=l, xytext=(10, 0), textcoords=\"offset points\",\n                textcoords='offset points')\n\n\n\n"], "sample_647": ["    def test_warn_explicit_for_handles_empty_registry(\n        ):\n            pass\n        with pytest.raises(\n            PytestWarning, match=r\"some warning\"\n        ) as excinfo:\n            warning_types.warn_explicit_for(\n                test_func, warning_types.PytestWarning(\"some warning\")\n            )\n        assert excinfo.value.filename == inspect.getfile(test_func)\n        assert excinfo.value.lineno == test_func.__code__.co_firstlineno\n", "def test_unformatted_warning_format(warning_class: UnformattedWarning) -> None:\n    assert isinstance(warning_class.format(), type(warning_class.category))\n", "    def test_unformatted_warning_format(template_message: str) -> None:\n        warning_class = PytestWarning\n\n        warning = UnformattedWarning(warning_class, template_message)\n        formatted_warning = warning.format(arg1=\"hello\", arg2=\"world\")\n        assert isinstance(formatted_warning, warning_class)\n        assert str(formatted_warning) == (\n            f\"This is a test hello world!\"  # Example format based on the first template\n        )\n", "    def test_unformatted_warning(\n        template: str, kwargs: dict, expected_category: Type[Warning]", "    def test_warning_class_format(warning_class: Any) -> None:\n        instance = warning_class(\"format works!\")\n        assert isinstance(instance, warning_class)\n        assert str(instance) == \"format works!\"\n"], "sample_594": ["def test_inline_variable_array_repr_numpy_array():\n    value = np.array([1, 2, 3, 4, 5])\n    variable = xr.Variable(\"x\", value)\n\n    max_width = 10\n    actual = formatting.inline_variable_array_repr(variable, max_width=10)\n    expected = f\"array([1 2 3 4 5])\"\n    assert actual == expected\n", "    def test_short_data_repr_large_array(self):\n        da = xr.DataArray(np.random.randn(100, 500), dims=(\"x\", \"y\"))\n        expected_length = 10\n        actual_length = len(formatting.short_data_repr(da).splitlines())\n        assert actual_length <= expected_length\n", "def test_format_items_with_datetimelike_array():\n    from datetime import datetime\n\n    dt_array = np.array([datetime(2023, 1, 1), datetime(2023, 1, 2), datetime(2023, 1, 3)], dtype=\"datetime64[D]\")\n\n    expected = \"2023-01-01 2023-01-02 2023-01-03\"\n    actual = \" \".join(formatting.format_items(dt_array))\n    assert expected == actual\n", "compilation error", "    def test_diff_dataset_repr_empty(self):\n        ds_a = xr.Dataset(coords={\"x\": [1, 2, 3]})\n        ds_b = xr.Dataset()\n        expected = dedent(\n            \"\"\"\\\n        Left and right Dataset objects are not identical\n        Differing dimensions:\n            (x: 3) != ()\n        Coordinates only on the left object:\n          * x        (x) int64 1 2 3\"\"\"\n        ).strip()\n        actual = formatting.diff_dataset_repr(ds_a, ds_b, \"identical\")\n        assert actual == expected\n"], "sample_1026": ["def test_lambdify_non_numeric_results():\n    # Test for cases where lambdify returns non-numeric results,\n    # like symbols or strings.\n\n    f = lambdify(x, x.expr, 'sympy')\n    assert f(2) == sympify(2)\n\n    g = lambdify(x, x + 1, 'sympy')\n    assert isinstance(g(2), sympy.Expr)\n    assert g(2) == sympify(3)\n\n    h = lambdify(x, \"hello, \" + str(x))\n    assert h(2) == \"hello, 2\"\n\n\n\n", "def test_lambdify_complex_nested_expressions():\n    x, y = symbols('x y')\n    z = x + 1j*y\n    expr = (z**2 + z.conjugate()) / (z*z.conjugate())\n    func = lambdify((x, y), expr)\n    assert func(1, 2) == (5 + 4j)\n\n\n\n", "def test_lambdify_issue_16119():\n    x, y = symbols('x y')\n    f = lambdify((x, y), x**2 + y**2, modules=\"numpy\")\n    assert f(1, 2) == 5\n    # Test with a list of arguments\n    assert f([1, 2]) == 5\n\n\n", "def test_issue_16310():\n    from sympy import sin, cos, tan, symbols, pi\n    x, y = symbols('x y')\n    f = lambdify((x, y), sin(x) + cos(y))\n    assert f(pi/4, pi/3) == sin(pi/4) + cos(pi/3)\n    g = lambdify((x, y), tan(x) * cos(y))\n    assert g(pi/4, pi/3) == tan(pi/4) * cos(pi/3)\n", "compilation error"], "sample_467": ["    def test_empty_labels(self):\n        widget = SelectDateWidget(years=(\"2010\",), empty_label=\"\")\n        self.check_html(\n            widget,\n            \"mydate\",\n            \"\",\n            html=(\n                \"\"\"\n            <select name=\"mydate_month\" id=\"id_mydate_month\">\n                <option value=\"\"> </option>\n                <option value=\"1\">January</option>\n                <option value=\"2\">February</option>\n                <option value=\"3\">March</option>\n                <option value=\"4\">April</option>\n                <option value=\"5\">May</option>\n                <option value=\"6\">June</option>\n                <option value=\"7\">July</option>\n                <option value=\"8\">August</option>\n                <option value=\"9\">September</option>\n                <option value=\"10\">October</option>\n                <option value=\"11\">November</option>\n                <option value=\"12\">December</option>\n            </select>\n\n            <select name=\"mydate_day\" id=\"id_mydate_day\">\n                <option value=\"\"> </option>\n                <option value=\"1\">1</option>\n                <option value=\"2\">2</option>\n                <option value=\"3\">3</option>\n                <option value=\"4\">4</option>\n                <option value=\"5\">5</option>\n                <option value=\"6\">6</option>\n                <option value=\"7\">7</option>\n                <option value=\"8\">8</option>\n                <option value=\"9\">9</option>\n                <option value=\"10\">10</option>\n                <option value=\"11\">11</option>\n                <option value=\"12\">12</option>\n                <option value=\"13\">13</option>\n                <option value=\"14\">14</option>\n                <option value=\"15\">15</option>\n                <option value=\"16\">16</option>\n                <option value=\"17\">17</option>\n                <option value=\"18\">18</option>\n                <option value=\"1", "    def test_widget_attrs(self):\n        widget = SelectDateWidget(years=(2007, 2008, 2009), attrs={'class': 'my-date-picker'})\n        self.assertEqual(widget.attrs, {'class': 'my-date-picker'})\n        self.assertHTMLEqual(\n            widget.render(\"mydate\", \"\"),\n            \"\"\"\n            <select name=\"mydate_month\" id=\"id_mydate_month\" class=\"my-date-picker\">\n                <option selected value=\"\">---</option>\n                <option value=\"1\">January</option>\n                <option value=\"2\">February</option>\n                <option value=\"3\">March</option>\n                <option value=\"4\">April</option>\n                <option value=\"5\">May</option>\n                <option value=\"6\">June</option>\n                <option value=\"7\">July</option>\n                <option value=\"8\">August</option>\n                <option value=\"9\">September</option>\n                <option value=\"10\">October</option>\n                <option value=\"11\">November</option>\n                <option value=\"12\">December</option>\n            </select>\n            <select name=\"mydate_day\" id=\"id_mydate_day\" class=\"my-date-picker\">\n                <option selected value=\"\">---</option>\n                <option value=\"1\">1</option>\n                <option value=\"2\">2</option>\n                <option value=\"3\">3</option>\n                <option value=\"4\">4</option>\n                <option value=\"5\">5</option>\n                <option value=\"6\">6</option>\n                <option value=\"7\">7</option>\n                <option value=\"8\">8</option>\n                <option value=\"9\">9</option>\n                <option value=\"10\">10</option>\n                <option value=\"11\">11</option>\n                <option value=\"12\">12</option>\n                <option value=\"13\">13</option>\n                <option value=\"14\">14</option>\n                <option value=\"15\">15", "    def test_widget_attrs(self):\n        widget = SelectDateWidget(years=(\"2007\",), attrs={\"class\": \"my-date-widget\"})\n        self.assertEqual(widget.render(\"mydate\", \"\"), '<select name=\"mydate_month\" id=\"id_mydate_month\" class=\"my-date-widget\"><option selected value=\"\">---</option><option value=\"1\">January</option><option value=\"2\">February</option><option value=\"3\">March</option><option value=\"4\">April</option><option value=\"5\">May</option><option value=\"6\">June</option><option value=\"7\">July</option><option value=\"8\">August</option><option value=\"9\">September</option><option value=\"10\">October</option><option value=\"11\">November</option><option value=\"12\">December</option></select><select name=\"mydate_day\" id=\"id_mydate_day\" class=\"my-date-widget\"><option selected value=\"\">---</option><option value=\"1\">1</option><option value=\"2\">2</option><option value=\"3\">3</option><option value=\"4\">4</option><option value=\"5\">5</option><option value=\"6\">6</option><option value=\"7\">7</option><option value=\"8\">8</option><option value=\"9\">9</option><option value=\"10\">10</option><option value=\"11\">11</option><option value=\"12\">12</option><option value=\"13\">13</option><option value=\"14\">14</option><option value=\"15\">15</option><option value=\"16\">16</option><option value=\"17\">17</option><option value=\"18\">18</option><option value=\"19\">19</option><option value=\"20\">20</option><option value=\"21\">21</option><option value=\"22\">22</option><option value=\"23\">23</option><option value=\"24\">24</option><option value=\"25\">25</option><option value=\"26\">26</option><option value=\"27\">27</option", "    def test_clean(self):\n        widget = SelectDateWidget(years=(\"2010\",))\n\n        # Test invalid input\n        self.assertRaises(ValidationError, widget.clean, \"\")\n        self.assertRaises(ValidationError, widget.clean, \"invalid\")\n        self.assertRaises(ValidationError, widget.clean, \"2010-13-31\")\n        self.assertRaises(ValidationError, widget.clean, \"2010-02-30\")\n        self.assertRaises(ValidationError, widget.clean, \"abc\")\n\n        # Test valid input\n        self.assertEqual(widget.clean(\"2010-01-01\"), \"2010-01-01\")\n        self.assertEqual(widget.clean(\"2010-02-28\"), \"2010-02-28\")\n        self.assertEqual(widget.clean(\"2010-12-31\"), \"2010-12-31\")\n\n\n\n", "    def test_widget_attrs(self):\n        widget = SelectDateWidget(years=(\"2013\",), attrs={\"class\": \"my-date-picker\"})\n        self.assertInHTML('<select name=\"mydate_month\" id=\"id_mydate_month\" class=\"my-date-picker\">', widget.render(\"mydate\", \"\"))\n        self.assertInHTML('<select name=\"mydate_day\" id=\"id_mydate_day\" class=\"my-date-picker\">', widget.render(\"mydate\", \"\"))\n        self.assertInHTML('<select name=\"mydate_year\" id=\"id_mydate_year\" class=\"my-date-picker\">', widget.render(\"mydate\", \"\"))\n"], "sample_146": ["    def test_invalid_variant_language_settings(self):\n        msg = (\n            'You have provided a value for the LANGUAGE_CODE setting that is '\n            'not in the LANGUAGES setting.'\n        )\n        for tag in ['fr-XX', 'es-999', 'de-invalid']:\n            with self.subTest(tag), self.settings(LANGUAGE_CODE=tag):\n                self.assertEqual(check_language_settings_consistent(None), [\n                    Error(msg, id='translation.E004'),\n                ])\n", "    def test_language_code_already_in_langs(self):\n        msg = (\n            'You have provided a value for the LANGUAGE_CODE setting that is '\n            'not in the LANGUAGES setting.'\n        )\n        with self.assertRaises(LookupError):\n            check_language_settings_consistent(None)\n\n", "    def test_invalid_language_variant(self):\n        msg = (\n            'You have provided a value for the LANGUAGE_CODE setting that is '\n            'not in the LANGUAGES setting.'\n        )\n        for tag in ['zh-Hans-HK', 'es-MX-Latin']:  # Example invalid variants\n            with self.subTest(tag), self.settings(LANGUAGE_CODE=tag):\n                self.assertEqual(check_language_settings_consistent(None), [\n                    Error(msg, id='translation.E004'),\n                ])\n\n", "    def test_invalid_variant_inconsistent_language_settings(self):\n        msg = (\n            'You have provided a value for the LANGUAGE_CODE setting that is '\n            'not in the LANGUAGES setting.'\n        )\n        for tag in ['fr-CA', 'fr-357', 'fr-BE', 'es-419']:\n            with self.subTest(tag), self.settings(LANGUAGE_CODE=tag):\n                self.assertEqual(check_language_settings_consistent(None), [\n                    Error(msg, id='translation.E004'),\n                ])\n", "    def test_invalid_language_variant_consistent_language_settings(self):\n        for tag in ['de-invalid', 'es-invalid', 'fr-invalid', 'ca-invalid']:\n            with self.subTest(tag), self.settings(LANGUAGE_CODE=tag):\n                self.assertEqual(check_language_settings_consistent(None), [\n                    Error(\n                        'You have provided a value for the LANGUAGE_CODE setting that is not in the LANGUAGES setting.',\n                        id='translation.E004',\n                    )\n                ])\n"], "sample_1201": ["def test_cgs_gauss_convert_derived():\n    assert convert_to(impedance, ohm, cgs_gauss) == ohm\n    assert convert_to(conductance, 1/ohm, cgs_gauss) == 1/ohm\n    assert convert_to(capacitance, farad, cgs_gauss) == farad\n    assert convert_to(inductance, henry, cgs_gauss) == henry\n    assert convert_to(charge, statcoulomb, cgs_gauss) == statcoulomb\n    assert convert_to(current, statampere, cgs_gauss) == statampere\n    assert convert_to(voltage, statvolt, cgs_gauss) == statvolt\n    assert convert_to(magnetic_density, gauss, cgs_gauss) == gauss\n    assert convert_to(magnetic_flux, maxwell, cgs_gauss) == maxwell\n", "compilation error", "compilation error", "def test_cgs_gauss_dimensional_analysis():\n    assert cgs_gauss.dimension_of(charge) == dict(mass=S.Half, length=S(3)/2, time=-1)\n    assert cgs_gauss.dimension_of(current) == dict(mass=One/2, length=3*One/2, time=-2)\n    assert cgs_gauss.dimension_of(voltage) == dict(length=-One/2, mass=One/2, time=-1)\n    assert cgs_gauss.dimension_of(magnetic_density) == dict(length=-One/2, mass=One/2, time=-1)\n    assert cgs_gauss.dimension_of(magnetic_flux) == dict(length=3*One/2, mass=One/2, time=-1)\n", "def test_cgs_gauss_unit_conversions():\n    assert convert_to(1*millimeter, centimeter, cgs_gauss) == 0.1*centimeter\n    assert convert_to(1*microgram, gram, cgs_gauss) == 10**-6*gram\n    assert convert_to(1*microsecond, second, cgs_gauss) == 10**-6*second\n    \n    assert convert_to(1*inch, centimeter, cgs_gauss) == 2.54*centimeter\n    assert convert_to(1*ounce, gram, cgs_gauss) == 28.3495*gram\n    assert convert_to(1*minute, second, cgs_gauss) == 60*second\n"], "sample_299": ["    def test_cache_location_empty_string(self):\n        with self.settings(CACHES={\n            'default': {\n                'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache',\n                'LOCATION': '',\n            },\n        }):\n            self.assertEqual(check_file_based_cache_is_absolute(None), [\n                Warning(\n                    \"Your 'default' cache LOCATION path is relative. Use an \"\n                    \"absolute path instead.\",\n                    id='caches.W003',\n                ),\n            ])\n\n\n\n", "    def test_cache_location_not_filebased(self):\n        with self.settings(CACHES={\n            'default': {\n                'BACKEND': 'django.core.cache.backends.locmem.LocMemCache',\n                'LOCATION': pathlib.Path.cwd() / 'cache',\n            },\n        }):\n            self.assertEqual(check_cache_location_not_exposed(None), [])\n", "    def test_cache_path_contains_media_static_setting_multiple_dirs(self):\n        root = pathlib.Path.cwd()\n        for setting in ('MEDIA_ROOT', 'STATIC_ROOT', 'STATICFILES_DIRS'):\n            settings = self.get_settings(setting, root / 'cache',\n                                         (root / 'other', root / 'another'))\n            with self.subTest(setting=setting), self.settings(**settings):\n                msg = self.warning_message % ('contains', setting)\n                self.assertEqual(check_cache_location_not_exposed(None), [\n                    Warning(msg, id='caches.W002'),\n                ])\n", "    def test_cache_location_is_not_existing_path(self):\n        with self.settings(CACHES={\n            'default': {\n                'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache',\n                'LOCATION': '/non/existing/path',\n            },\n        }):\n            self.assertEqual(check_cache_location_not_exposed(None), [\n                Warning(\n                    \"Your 'default' cache configuration might expose your cache or lead \"\n                    \"to corruption of your data because its LOCATION /non/existing/path \"\n                    \"is not an existing path.\",\n                    id='caches.W002',\n                ),\n            ])\n", "    def test_cache_outside_media_static_setting(self):\n        root = pathlib.Path.cwd()\n        other_path = root / 'other'\n        for setting in ('MEDIA_ROOT', 'STATIC_ROOT', 'STATICFILES_DIRS'):\n            settings = self.get_settings(setting, other_path, root)\n            with self.subTest(setting=setting), self.settings(**settings):\n                self.assertEqual(check_cache_location_not_exposed(None), [])\n"], "sample_1080": ["def test_refine_mul():\n    assert refine(Mul(x, y), Q.real(x) & Q.real(y)) == x * y\n    assert refine(Mul(x, y), Q.imaginary(x) & Q.imaginary(y)) == -I*x*y\n    assert refine(Mul(x, y), Q.real(x) & Q.imaginary(y)) == x * I * y\n\n\n", "def test_refine_nan():\n    assert refine(nan, Q.real(x)) is nan\n    assert refine(nan, Q.imaginary(x)) is nan\n    assert refine(nan + 1, Q.real(x)) is nan\n    assert refine(nan * 2, Q.real(x)) is nan\n    assert refine(sqrt(nan), Q.real(x)) is nan\n", "compilation error", "def test_refine_rational():\n    x = Symbol('x', real=True)\n    assert refine((x**Rational(2,3)), Q.integer(x)) == x**(2/3)\n    assert refine((x**Rational(2,3)), Q.positive(x)) == x**(2/3)\n    assert refine((x**Rational(2,3)), Q.negative(x)) == -abs(x)**(2/3)\n    assert refine((x**Rational(2,3)), Q.nonzero(x)) == x**(2/3)\n    assert refine((x**Rational(3,2)), Q.integer(x)) == x**(3/2)\n    assert refine((x**Rational(3,2)), Q.positive(x)) == x**(3/2)\n    assert refine((x**Rational(3,2)), Q.negative(x)) == -abs(x)**(3/2)\n    assert refine((x**Rational(3,2)), Q.nonzero(x)) == x**(3/2)\n\n\n", "def test_refine_rational():\n    x = Symbol('x')\n    assert refine(x**(Rational(1,2)), Q.positive(x)) == x**(Rational(1,2))\n    assert refine(x**(Rational(1,2)), Q.negative(x)) == -abs(x)**(Rational(1,2))\n"], "sample_252": ["    def test_complex_key_transform_subquery(self):\n        objs = [\n            NullableJSONModel.objects.create(value={'a': {'b': {'c': 1, 'd': 2}}}),\n            NullableJSONModel.objects.create(value={'a': {'b': {'c': 3, 'd': 4}}}),\n        ]\n        with self.subTest(value=1):\n            self.assertSequenceEqual(\n                NullableJSONModel.objects.filter(\n                    value__a__b__c=1\n                ).values_list('id', flat=True),\n                [objs[0].id],\n            )\n        with self.subTest(value=3):\n            self.assertSequenceEqual(\n                NullableJSONModel.objects.filter(\n                    value__a__b__c=3\n                ).values_list('id', flat=True),\n                [objs[1].id],\n            )\n        with self.subTest(invalid_key):\n            self.assertQuerysetEqual(\n                NullableJSONModel.objects.filter(\n                    value__a__b__invalid_key=1\n                ),\n                [],\n            )\n\n\n\n", "    def test_key_transform_with_operators(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(\n                value__d__1__f__gt=KeyTransform('g', 'value__d__0'),\n            ),\n            [self.objs[4]],\n        )\n\n\n\n", "    def test_lookup_with_null_value(self):\n        obj = NullableJSONModel.objects.create(value={'a': None})\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(value__a__isnull=True),\n            self.objs[:3] + self.objs[5:] + [obj],\n        )\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(value__a__isnull=False),\n            [self.objs[3], self.objs[4]],\n        )\n\n\n\n", "    def test_key_transform_with_empty_values(self):\n        for lookup in ['value__a', 'value__c', 'value__d__0']:\n            with self.subTest(lookup=lookup):\n                obj = NullableJSONModel.objects.create(value={'a': '', 'c': None, 'd': ['']})\n                self.assertIs(\n                    NullableJSONModel.objects.filter(**{lookup: KeyTransform('value', 'a')}).exists(),\n                    True,\n                )\n                self.assertIs(\n                    NullableJSONModel.objects.filter(**{lookup: ''}).exists(),\n                    True,\n                )\n\n", "    def test_contains_raw_sql(self):\n        expr = RawSQL(self.raw_sql, ['{\"x\": {\"a\": \"b\", \"c\": 1}}'])\n        self.assertIs(NullableJSONModel.objects.filter(value__contains=expr).exists(), True)\n\n\n\n"], "sample_983": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_931": ["def test_pyliteral_signature(app):\n    text = (\".. py:literal:: 123\\n\"\n            \".. py:literal:: True\\n\"\n            \".. py:literal:: None\\n\"\n            \".. py:literal:: 123.45\\n\"\n            \".. py:literal:: \\\"hello\\\"\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"literal \"],\n                                                    [desc_name, \"123\"],\n                                                    ,)], desc_content)]))\n    assert_node(doctree[1], addnodes.index,\n                entries=[('single', '123 (literal)', '123', '', None)])\n    assert '123' in domain.objects\n    assert domain.objects['123'] == ('index', '123', 'literal')\n", "compilation error", "compilation error", "def test_pyalias(app):\n    text = (\".. py:alias:: alias1\\n\"\n            \"   :func: function1\\n\"\n            \".. py:alias:: alias2\\n\"\n            \"   :class: Class1\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_addname, \"alias1\"],\n                                [desc_signature, ([desc_name, \"alias1\"],\n                                [desc_annotation, \" = \"],\n                [desc_addname, \"function1\"])],\n                [desc_content, ()]),\n                [desc, ([desc_addname, \"alias2\"],\n                [desc_signature, ([desc_name, \"alias2\"],\n                [desc_annotation, \" = \"],\n                [desc_addname, \"Class1\"])],\n                [desc_content, ()])]))\n    assert 'alias1' in domain.objects\n    assert domain.objects['alias1'] == ('index', 'alias1', 'alias')\n    assert 'alias2' in domain.objects\n    assert domain.objects['alias2'] == ('index', 'alias2', 'alias')\n\n\n\n", "def test_pyfunction_signature_full_py38_new_style(app):\n    text = \".. py:function:: hello(*, a)\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree[1][0][1],\n                [desc_parameterlist, ([desc_parameter, nodes.inline, \"*\"],)])\n\n\n\n"], "sample_719": ["def test_tfidfvectorizer_with_custom_stop_words():\n    # Test with custom stop words\n    custom_stop_words = [\"the\", \"a\", \"an\", \"in\"]\n    tfidf = TfidfVectorizer(stop_words=custom_stop_words)\n    documents = [\"This is a test document.\", \"Another document with some words.\"]\n    X_tfidf = tfidf.fit_transform(documents)\n    assert_array_equal(X_tfidf.shape, (2, 4))  # 2 documents, 4 remaining words\n\n\n", "def test_countvectorizer_custom_stop_words():\n    stop_words = [\"the\", \"a\", \"an\", \"and\", \"is\"]\n    vect = CountVectorizer(stop_words=stop_words)\n    X = vect.fit_transform(JUNK_FOOD_DOCS)\n\n    for term in stop_words:\n        assert_equal(vect.vocabulary_.get(term), None)\n\n    for doc in JUNK_FOOD_DOCS:\n        assert_true(term not in doc for term in stop_words)\n\n\n", "def test_vectorizer_with_custom_stop_words():\n    custom_stop_words = [\"hello\", \"world\"]\n    vect = CountVectorizer(stop_words=custom_stop_words)\n    X = vect.fit_transform(JUNK_FOOD_DOCS)\n    assert_array_equal(X.getnnz(axis=0), 0)  # All stop words should be removed\n\n\n", "def test_vectorizer_empty_strings(input_strings):\n    for vect in (CountVectorizer(), TfidfVectorizer(), HashingVectorizer()):\n        X = vect.fit_transform(input_strings)\n        assert_array_equal(X.shape[1], 0 if None in input_strings else len(set(vect.get_feature_names())))\n\n\n\n", "    def test_ngram_range_shape(ngram_range, expected_n_features):\n        vect = CountVectorizer()\n        docs = [\"this is a test\", \"this is another test\"]\n        vect.set_params(ngram_range=ngram_range)\n        X = vect.fit_transform(docs)\n        assert_equal(X.shape[1], expected_n_features)\n\n\n"], "sample_790": ["def test_kernel_pca_with_sparse_precomputed_kernel():\n    rng = np.random.RandomState(0)\n    X_fit = sp.csr_matrix(rng.random_sample((5, 4)))\n    X_pred = sp.csr_matrix(rng.random_sample((2, 4)))\n\n    kpca = KernelPCA(4, kernel='precomputed',\n                     fit_inverse_transform=False)\n\n    # Construct a sparse precomputed kernel matrix\n    kernel_matrix = X_fit @ X_fit.T\n    kpca.fit(kernel_matrix)\n    X_transformed = kpca.transform(kernel_matrix)\n\n    assert_equal(X_transformed.shape, (5, 4))\n", "def test_kernel_pca_with_sparse_data():\n    rng = np.random.RandomState(0)\n    X_dense = rng.random_sample((100, 10))\n    X_sparse = sp.csr_matrix(rng.random_sample((100, 10)))\n\n    for kernel in (\"linear\", \"rbf\", \"poly\"):\n        for eigen_solver in (\"dense\", \"arpack\"):\n            kpca_dense = KernelPCA(n_components=2, kernel=kernel,\n                                    eigen_solver=eigen_solver).fit(X_dense)\n            X_dense_transformed = kpca_dense.transform(X_dense)\n\n            kpca_sparse = KernelPCA(n_components=2, kernel=kernel,\n                                    eigen_solver=eigen_solver).fit(X_sparse)\n            X_sparse_transformed = kpca_sparse.transform(X_sparse)\n\n            assert_array_almost_equal(\n                X_dense_transformed, X_sparse_transformed)\n", " def test_kernel_pca_copy_X():\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    kpca = KernelPCA(n_components=2, copy_X=True)\n    kpca.fit(X)\n\n    assert_not_equal(X, kpca.X_fit_)  \n\n\n\n", "def test_kernel_pca_eigen_solver_performance():\n    rng = np.random.RandomState(0)\n    X = rng.randn(1000, 10)\n    n_components = 50\n\n    # Compare performance of eigen_solver 'dense' and 'arpack'\n    for solver in (\"dense\", \"arpack\"):\n        kpca = KernelPCA(n_components=n_components, eigen_solver=solver)\n        t0 = time.time()\n        kpca.fit_transform(X)\n        t1 = time.time()\n\n        print(f\"Kernel PCA with {solver} solver took {t1-t0:.2f} seconds\")\n        assert_less(t1 - t0, 10)  # Allow for some variance in execution time\n\n\n", "def test_kernel_pca_precomputed_sparse():\n    rng = np.random.RandomState(0)\n    X_fit = sp.csr_matrix(rng.random_sample((5, 4)))\n    X_pred = sp.csr_matrix(rng.random_sample((2, 4)))\n\n    for eigen_solver in (\"dense\", \"arpack\"):\n        X_kpca = KernelPCA(4, eigen_solver=eigen_solver).\\\n            fit(X_fit).transform(X_pred)\n        X_kpca2 = KernelPCA(\n            4, eigen_solver=eigen_solver, kernel='precomputed').fit(\n                np.dot(X_fit, X_fit.T)).transform(np.dot(X_pred, X_fit.T))\n\n        assert_array_almost_equal(np.abs(X_kpca),\n                                  np.abs(X_kpca2))\n\n\n\n"], "sample_210": ["    def test_template_params_with_object_list(self):\n        class CustomObjectMixinView(TemplateView):\n            template_name = 'generic_views/about.html'\n\n                context = super().get_context_data(**kwargs)\n                context['objects'] = objects\n                return context\n\n        objects = [\n            {'name': 'John'},\n            {'name': 'Paul'},\n        ]\n        response = CustomObjectMixinView.as_view()(self.rf.get('/'), objects=objects)\n        self.assertEqual(response.context_data['objects'], objects)\n", "    def test_template_params_with_get_method(self):\n        \"\"\"\n        A template view can be customized to return extra context from\n        a get_method.\n        \"\"\"\n        response = self.client.get('/template/get_method/bar1/bar2/')\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.context['foo1'], 'bar1')\n        self.assertEqual(response.context['foo2'], 'bar2')\n        self.assertEqual(response.context['key'], 'value')\n        self.assertIsInstance(response.context['view'], View)\n", "    def test_template_params_kwargs(self):\n        response = self.client.get('/template/kwargs/')\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.context['key'], 'value')\n        self.assertIsInstance(response.context['view'], View)\n\n", "    def test_template_params_with_invalid_kwargs(self):\n        with self.assertRaises(TypeError):\n            TemplateView.as_view()(self.rf.get('/template/custom/invalid/'))\n", "    def test_template_params_error_raise(self):\n        \"\"\"Test that a TypeError is raised when a template param is passed as a keyword argument to a view without a corresponding context.\"\"\"\n        view = TemplateView()\n        get_context_data_called = False\n\n            nonlocal get_context_data_called\n            get_context_data_called = True\n            raise TypeError(\"Required keyword argument 'template_name' is missing.\")\n\n        with self.subTest(\"template_name\"):\n            with self.assertRaises(TypeError):\n                with self.setattr(view, 'get_context_data', mock_get_context_data):\n                    view.get_context_data(template_name='foo.html')\n\n        self.assertTrue(get_context_data_called)\n"], "sample_549": ["    def test_auto_format_str_unicode():\n        assert cbook._auto_format_str('{}', '\u4f60\u597d').encode('utf-8') == b'\u4f60\u597d'  \n", "compilation error", "def test_format_float_array(x):\n    result = cbook._format_float_array(x, precision=2)\n    assert isinstance(result, list)\n    for val in result:\n        assert isinstance(val, str)\n        assert val.endswith('0' if val[-1] == '0' else ' ')\n        assert len(val) <= 6\n\n\n\n", "def test_format_time_series():\n    from datetime import datetime, timedelta\n    ts = [datetime.now() - timedelta(days=i) for i in range(5)]\n    assert cbook._format_time_series(ts, fmt='%Y-%m-%d %H:%M:%S') == \\\n        [f'{t.strftime(\"%Y-%m-%d %H:%M:%S\")}' for t in ts]\n", "compilation error"], "sample_1139": ["compilation error", "compilation error", "def test_issue_19013():\n    a = Symbol('a', real=True)\n    b = Symbol('b', real=True)\n    assert ImageSet(Lambda(x, x**a), S.Reals).is_subset(\n        ImageSet(Lambda(x, x**a), S.Reals)\n    )\n    assert ImageSet(Lambda(x, x**a), S.Naturals).is_subset(\n        ImageSet(Lambda(x, x**a), S.Reals)\n    )\n    assert ImageSet(Lambda(x, x**a), S.Integers).is_subset(\n        ImageSet(Lambda(x, x**a), S.Reals)\n    )\n    assert ImageSet(Lambda(x, x**a), S.Rationals).is_subset(\n        ImageSet(Lambda(x, x**a), S.Reals)\n    )\n", "def test_issue_18134_complexes():\n    assert S.Complexes.boundary == S.Complexes\n    assert S.Complexes.closure == S.Complexes\n    assert S.Complexes.is_open == False\n    assert S.Complexes.is_closed == True\n", "def test_issue_18134_boundary():\n    assert S.Integers.boundary == S.Integers\n    assert S.Naturals.boundary == S.Naturals\n    assert S.Naturals0.boundary == S.Naturals0\n    assert S.Rationals.boundary == S.Reals\n    assert S.Reals.boundary == S.Reals\n    assert S.Complexes.boundary == S.Complexes\n"], "sample_547": ["compilation error", "def test_anchored_bbox_limits():\n    fig, ax = plt.subplots()\n    ax.set_xlim(-1, 1)\n    ax.set_ylim(-1, 1)\n\n    da = DrawingArea(20, 20)\n    ab = AnnotationBbox(da, (0.5, 0.5), xycoords='axes fraction',\n                        boxcoords=\"data\", pad=0.1)\n    ax.add_artist(ab)\n\n    fig.tight_layout()\n    fig.canvas.draw()\n\n\n\n", "def test_offsetbox_clipping_tight_layout():\n    fig, ax = plt.subplots()\n    da = DrawingArea(100, 100)\n    bg = mpatches.Rectangle((0, 0), 100, 100,\n                            facecolor='#CCCCCC',\n                            edgecolor='None',\n                            linewidth=0)\n    line = mlines.Line2D([-size*.5, size*1.5], [size/2, size/2],\n                         color='black',\n                         linewidth=10)\n    anchored_box = AnchoredOffsetbox(\n        loc='center',\n        child=da,\n        pad=0.,\n        frameon=False,\n        bbox_to_anchor=(.5, .5),\n        bbox_transform=ax.transAxes,\n        borderpad=0.)\n\n    da.add_artist(bg)\n    da.add_artist(line)\n    ax.add_artist(anchored_box)\n    ax.set_xlim((-1, 1))\n    ax.set_ylim((-1, 1))\n    fig.tight_layout()\n", "def test_get_packed_offsets_zero_widths(mode):\n    # Check handling of zero-width children in different packing modes\n    result = _get_packed_offsets([0, 1, 0], total=5, sep=1, mode=mode)\n    assert result[0] == 5  # total should be used\n    assert_allclose(result[1], [0, 5, 5])  # one child occupies all space\n", "def test_offsetbox_clipping_with_transform():\n    # - create a plot\n    # - put an AnchoredOffsetbox with a child DrawingArea\n    #   at the center of the axes\n    # - give the DrawingArea a gray background\n    # - put a black line across the bounds of the DrawingArea\n    # - see that the black line is clipped to the edges of\n    #   the DrawingArea.\n    fig, ax = plt.subplots()\n    size = 100\n    da = DrawingArea(size, size, clip=True)\n    bg = mpatches.Rectangle((0, 0), size, size,\n                            facecolor='#CCCCCC',\n                            edgecolor='None',\n                            linewidth=0)\n    line = mlines.Line2D([-size*.5, size*1.5], [size/2, size/2],\n                         color='black',\n                         linewidth=10)\n    anchored_box = AnchoredOffsetbox(\n        loc='center',\n        child=da,\n        pad=0.,\n        frameon=False,\n        bbox_to_anchor=(.5, .5),\n        bbox_transform=ax.transAxes,\n        borderpad=0.)\n\n    transform = ax.transAxes.transform((0.5, 0.5))\n    da.add_artist(bg)\n    da.add_artist(line)\n    anchored_box.set_transform(transform)\n    ax.add_artist(anchored_box)\n    ax.set_xlim((0, 1))\n    ax.set_ylim((0, 1))\n"], "sample_394": ["    def test_unknown_url_404_if_authenticated_without_final_catch_all_view(self):\n        superuser = User.objects.create_superuser(\n            username=\"super\",\n            password=\"secret\",\n            email=\"super@example.com\",\n        )\n        self.client.force_login(superuser)\n        unknown_url = \"/test_admin/admin10/unknown/\"\n        response = self.client.get(unknown_url)\n        self.assertEqual(response.status_code, 404)\n", "    def test_missing_slash_append_slash_true_force_script_name_without_final_catch_all_view(self):\n        superuser = User.objects.create_user(\n            username=\"staff\",\n            password=\"secret\",\n            email=\"staff@example.com\",\n            is_staff=True,\n        )\n        self.client.force_login(superuser)\n        known_url = reverse(\"admin10:admin_views_article_changelist\")\n        response = self.client.get(known_url[:-1])\n        self.assertRedirects(\n            response, \"/prefix\" + known_url, status_code=301, fetch_redirect_response=False\n        )\n", "    def test_non_admin_url_404_if_authenticated_without_final_catch_all_view(self):\n        superuser = User.objects.create_superuser(\n            username=\"super\",\n            password=\"secret\",\n            email=\"super@example.com\",\n        )\n        self.client.force_login(superuser)\n        unknown_url = \"/unknown/\"\n        response = self.client.get(unknown_url)\n        self.assertEqual(response.status_code, 404)\n", "    def test_non_admin_url_404_if_authenticated(self):\n        unknown_url = \"/unknown/\"\n        superuser = User.objects.create_superuser(\n            username=\"super\", password=\"secret\", email=\"super@example.com\"\n        )\n        self.client.force_login(superuser)\n        response = self.client.get(unknown_url)\n        self.assertEqual(response.status_code, 404)\n", "    def test_final_catch_all_view_disabled(self):\n        test_site = AdminSite()\n        test_site.final_catch_all_view = False\n        with override_settings(ADMIN_SITE=test_site):\n            unknown_url = \"/test_admin/admin/unknown/\"\n            response = self.client.get(unknown_url)\n            self.assertEqual(response.status_code, 404)\n"], "sample_1132": ["compilation error", "def test_is_partially_ordered():\n    assert is_partially_ordered([(1, 2), (2, 3), (1, 3)]) is True\n    assert is_partially_ordered([(1, 2), (2, 1)]) is False\n    assert is_partially_ordered([(1, 2), (2, 3), (1, 1)]) is False\n    assert is_partially_ordered([(1, 2), (3, 2)]) is True\n    assert is_partially_ordered([(1, 2), (2, 3), (3, 1)]) is False\n\n\n\n", "compilation error", "compilation error", "compilation error"], "sample_1000": ["compilation error", "def test_indexed_to_string():\n    from sympy import IndexedBase, Idx, Symbol\n    x = Symbol('x')\n    i = Idx('i', 0, 10)\n    a = IndexedBase('a')\n    assert mcode(a[i]) == \"a(i)\"\n    assert mcode(a[i].subs(a, x)) == \"x(i)\"\n    assert mcode(a[1:5]) == \"a([1, 2, 3, 4])\"\n    assert mcode(a[0:5]) == \"a([0, 1, 2, 3, 4])\"\n    assert mcode(a[1:10:2]) == \"a([1, 3, 5, 7, 9])\"\n    assert mcode(a[::2]) == \"a([0 2 4 6 8 10])\" \n    assert mcode(a[::-1]) == \"a([10, 9, 8, 7, 6, 5, 4, 3, 2, 1])\"\n    assert mcode(a[i:i+5]) == \"a(i:i + 4)\"\n    assert mcode(a[(i+1):i+5]) == \"a(i + 1:i + 4)\"\n    assert mcode(a[i] + a[i+1]) == \"a(i) + a(i + 1)\"\n\n\n\n", "def test_indexing_matrices():\n    A = Matrix([[1, 2, 3], [4, 5, 6]])\n    assert mcode(A[0]) == \"[1 2 3]\"\n    assert mcode(A[1, 1]) == \"A(2, 2)\"\n    assert mcode(A[0:2, 0:2]) == \"[1 2; 4 5]\"\n    assert mcode(A[:1, :2]) == \"[1 2]\"\n    assert mcode(A[1:, 1:]) == \"[5 6]\"\n\n", "def test_Indexed():\n    i = IndexedBase('i', shape=(5,))\n    j = IndexedBase('j', shape=(5,))\n    x = Symbol('x')\n    y = Symbol('y')\n\n    assert mcode(i[x]) == 'i(x)'\n    assert mcode(i[x] + j[y]) == 'i(x) + j(y)'\n    assert mcode(i[x]*x) == 'i(x)*x'\n    assert mcode(i[x]**2) == 'i(x).^2'\n    assert mcode(i[x] * j[y] * x) == 'i(x).*j(y).*x'\n    assert mcode(i[x] * j[y] * y) == 'i(x).*j(y).*y'\n    assert mcode(i[x] * j[y] * x * y) == 'i(x).*j(y).*x.*y'\n\n    with octave_code(\n        i[x] + j[y], assign_to='a', contract=False\n    ):\n        assert mcode(i[x] + j[y]) == 'i(x) + j(y)'\n    with octave_code(\n        i[x] + j[y], assign_to='a', contract=True\n    ):\n        assert mcode(i[x] + j[y]) == 'sum(i(x) + j(y))'\n", "def test_custom_functions():\n    custom_functions = {\n        \"my_fcn\": \"x*2 + 1\",\n        \"my_mat_fcn\": \"x.*2 + 1\"\n    }\n    f = Function('f', lambda x: x**2)\n    assert mcode(f(x), user_functions=custom_functions) == \"f(x)\"\n    assert mcode(f(x) + 1, user_functions=custom_functions) == \"f(x) + 1\"\n    assert mcode(x*f(x) + my_fcn(x), user_functions=custom_functions) == \"x.*f(x) + my_fcn(x)\"\n    assert mcode(my_mat_fcn(Matrix(1, 2, [1, 2])), user_functions=custom_functions) == \"my_mat_fcn([1 2])\"\n"], "sample_451": ["    def test_replace_metacharacters(self):\n        pattern = r\"^(?:(?P<a>\\w+)/b/)?(.*)$\"\n        modified_pattern = replace_metacharacters(pattern)\n        self.assertEqual(modified_pattern, r\"^(?:(?P<a>\\w+)/b/(\\\\?)([?*+^$]|\\\\[bBAZ])?)?(.*)\")\n\n        pattern = r\"^(?:(?P<a>\\w+)/b/)?(.*)$\"\n        modified_pattern = replace_metacharacters(pattern)\n        self.assertEqual(modified_pattern, r\"^(?:(?P<a>\\w+)/b/)?(.*)$\")\n", "    def test_parse_rst_escaped_metacharacters(self):\n        source = r\"(?P<a>\\w+)/b/(?:\\w+)\"\n        expected_output = r'<p><a class=\"reference external\" href=\"/admindocs/views/<\\w+>/\">&lt;a&gt;\\w+&lt;/a&gt;/b/(&#x0025;.*?)</a></p>'\n        rendered = parse_rst(source, \"view\")\n        self.assertHTMLEqual(rendered, expected_output)\n\n", "    def test_parse_rst_with_multiple_lines(self):\n        docstring = \"\"\"\n        This is a docstring with multiple lines\n\n        And more lines\n        with inline code like this: `some_code`\n        \"\"\"\n        output = parse_rst(docstring, \"model\", \"model:admindocs\")\n        expected_output = (\n            '<p>This is a docstring with multiple lines\\n'\n            'And more lines\\nwith inline code like this: `some_code`</p>\\n'\n        )\n        self.assertHTMLEqual(output, expected_output)\n", "    def test_replace_metacharacters(self):\n        pattern = r\"(?P<a>\\w+)/(?:\\w+)\"\n        expected = r\"<a>/<var>\"\n        self.assertEqual(replace_metacharacters(pattern), expected)\n\n        pattern = r\"(?P<a>\\w+)/(?P<b>\\w+)\"\n        expected = r\"<a>/<b>\"\n        self.assertEqual(replace_metacharacters(pattern), expected)\n\n        pattern = r\"(?P<a>\\\\w+)/(?:\\w+)\"\n        expected = r\"(?P<a>\\\\w+)/(?:\\w+)\"\n        self.assertEqual(replace_metacharacters(pattern), expected)\n\n        pattern = r\"(?:\\\\?\\w+)/(?:\\w+)\"\n        expected = r\"(?:\\\\?\\w+)/(?:\\w+)\"\n        self.assertEqual(replace_metacharacters(pattern), expected)\n", "    def test_replace_metacharacters(self):\n        pattern = r\"^(\\?P<a>[\\w]+)/b/([\\w\\*\\+\\?])\"\n        expected_pattern = r\"^<a>/b/\\2\"\n        self.assertEqual(replace_metacharacters(pattern), expected_pattern)\n\n        pattern = r\"^(\\?P<a>[\\w]+)/b/(?:\\w*)\" \n        expected_pattern = r\"^<a>/b/.*\"\n        self.assertEqual(replace_metacharacters(pattern), expected_pattern)\n"], "sample_496": ["    def test_suggestions_with_typo(self):\n        args = ['ruunserver', '--settings=test_project.settings']\n        out, err = self.run_django_admin(args)\n        self.assertNoOutput(out)\n        self.assertOutput(err, \"Unknown command: 'ruunserver'. Did you mean runserver?\")\n\n", "    def test_suggestions_with_arguments(self):\n        args = ['rnserver', '127.0.0.1:8081', '--settings=test_project.settings']\n        out, err = self.run_django_admin(args)\n        self.assertNoOutput(out)\n        self.assertOutput(err, \"Unknown command: 'rnserver'. Did you mean runserver?\")\n", "    def test_suggest_with_extra_options(self):\n        args = ['rnserver', '--settings=test_project.settings', '--port=8000']\n        out, err = self.run_django_admin(args)\n        self.assertNoOutput(out)\n        self.assertNotInOutput(err, 'Did you mean runserver?')\n", "    def test_suggestions_for_subcommands(self):\n        args = ['cr', '--settings=test_project.settings', 'myapp']\n        out, err = self.run_django_admin(args)\n        self.assertNoOutput(out)\n        self.assertOutput(err, \"Unknown command: 'cr'. Did you mean 'createapp'?\")\n", "    def test_suggestions_with_options(self):\n        args = ['cr_server', '--settings=test_project.settings', '--help']\n        out, err = self.run_django_admin(args)\n        self.assertNoOutput(out)\n        self.assertOutput(err, \"Unknown command: 'cr_server'. Did you mean createproject or runserver?\") \n"], "sample_188": ["    def test_output_field_with_group_by(self):\n        expr = ExpressionWrapper(Lower(Value('f')), output_field=CharField())\n        self.assertEqual(expr.get_group_by_cols(alias='lower_f'), ['lower_f'])\n\n\n\n", "    def test_aggregate_with_output_field(self):\n        expr = ExpressionWrapper(Lower(F('name')), output_field=CharField())\n        with self.assertRaises(ValueError):\n            expr.aggregate(Count('id'))\n", "    def test_output_field_with_default(self):\n        expr = ExpressionWrapper(F('name'), output_field=CharField(default='default'))\n        self.assertEqual(expr.output_field, CharField())\n", "    def test_non_constant_group_by(self):\n        expr = ExpressionWrapper(Lower(F('name')), output_field=CharField())\n        self.assertEqual(expr.get_group_by_cols(alias=None), [expr.expression])\n", "    def test_output_field_overwriting(self):\n        expr = ExpressionWrapper(F(\"name\"), output_field=CharField())\n        self.assertEqual(expr.output_field, CharField())\n        expr = ExpressionWrapper(expr, output_field=IntegerField())\n        self.assertEqual(expr.output_field, IntegerField())\n"], "sample_1157": ["compilation error", "compilation error", "def test_issue_20837():\n    x = Symbol('x')\n    y = Symbol('y')\n    assert parse_expr('x + y * y') == x + y*y\n    assert parse_expr('x + y * y', evaluate=False) == x + y*y\n", "def test_issue_19501_continued():\n    x = Symbol('x')\n    eq = parse_expr('E^(x*(1 + x))', local_dict={'x': x}, transformations=(\n        standard_transformations +\n        (implicit_multiplication_application,)\n    ))\n    assert eq.free_symbols == {x}\n\n\n\n", "def test_issue_19501_continued():\n    x = Symbol('x')\n    eq = parse_expr('E**(x*(1+x))', local_dict={'x': x}, transformations=(\n        standard_transformations +\n        (implicit_multiplication_application,)))\n    assert eq.free_symbols == {x}\n\n"], "sample_145": ["    def test_actions_callable(self):\n        class BandAdmin(ModelAdmin):\n            actions = ('non_callable',)\n\n        self.assertIsInvalid(\n            BandAdmin, Band,\n            \"The value of 'actions' must be a callable or a tuple/list of callable objects.\",\n            id='admin.E128',\n        )\n\n\n\n", "    def test_actions_with_custom_permissions(self):\n            pass\n\n        custom_permission_action.allowed_permissions = ('custom_permission',)\n\n        class BandAdmin(ModelAdmin):\n            actions = (custom_action,)\n\n        self.assertIsInvalid(\n            BandAdmin, Band,\n            'BandAdmin must define a has_custom_permission() method for the '\n            'custom_action action.',\n            id='admin.E129',\n        )\n\n        class BandAdminWithPermission(ModelAdmin):\n                return request.user.is_superuser\n\n            actions = (custom_action,)\n\n        self.assertIsValid(BandAdminWithPermission, Band)\n", "    def test_actions_require_methods(self):\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = (action,)\n\n        self.assertIsInvalid(\n            BandAdmin, Band,\n            'The action method defined in BandAdmin must have a \"has_perm\" '\n            'method.',\n            id='admin.E131',\n        )\n", "    def test_actions_not_callable(self):\n        class BandAdmin(ModelAdmin):\n            actions = ('non_callable',)\n\n        self.assertIsInvalid(\n            BandAdmin, Band,\n            \"The value of 'actions[0]' must be a callable.\",\n            id='admin.E128',\n        )\n", "    def test_action_invalid_callable(self):\n        class BandAdmin(ModelAdmin):\n            actions = ['hello']\n\n        self.assertIsInvalid(\n            BandAdmin, Band,\n            'The value of \"actions\" must be a list of callable objects.',\n            id='admin.E128',\n        ) \n"], "sample_205": ["    def test_update_error_dict(self):\n        error_dict = {}\n        exception = ValidationError(error_dict)\n        self.assertEqual(error_dict, {})\n        error_dict2 = {'field1': ['E1', 'E2']}\n        exception = ValidationError(error_dict)\n        exception.update_error_dict(error_dict2)\n        self.assertEqual(error_dict2, {'field1': ['E1', 'E2']})\n\n        error_dict3 = {'field2': ['E3', 'E4']}\n        exception = ValidationError(error_dict2)\n        exception.update_error_dict(error_dict3)\n        self.assertEqual(error_dict, {'field1': ['E1', 'E2'], 'field2': ['E3', 'E4']})\n\n        error_dict4 = {'field1': ValidationError({'field_nested': ['Nested']})}\n        exception = ValidationError(error_dict)\n        exception.update_error_dict(error_dict4)\n        self.assertEqual(error_dict, {'field1': [ValidationError({'field_nested': ['Nested']}) ]})\n\n", "    def test_update_error_dict(self):\n        error_dict = {}\n        exception = ValidationError(error_dict)\n        self.assertEqual(error_dict, {})\n\n        error_dict = {'field1': ['E1', 'E2']}\n        exception = ValidationError(error_dict)\n        self.assertEqual(error_dict, {'field1': ['E1', 'E2']})\n        exception.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'field1': ['E1', 'E2']})\n\n        error_dict = {'field2': ['E3', 'E4']}\n        exception = ValidationError(error_dict)\n        self.assertEqual(error_dict, {'field2': ['E3', 'E4']})\n        exception.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'field1': ['E1', 'E2'], 'field2': ['E3', 'E4']})\n\n        error_dict = {'field1': ['E1', 'E2']}\n        exception = ValidationError({'field2': ['E3', 'E4']})\n        self.assertEqual(error_dict, {'field1': ['E1', 'E2']})\n        exception.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'field1': ['E1', 'E2'], 'field2': ['E3', 'E4']})\n\n        error_dict = {}\n        exception = ValidationError(error_dict)\n        self.assertEqual(error_dict, {})\n        exception.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {})\n\n        exception = ValidationError(NON_FIELD_ERRORS)\n        self.assertEqual(error_dict, {})\n        exception.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {})\n", "    def test_update_error_dict(self):\n        error_dict = {}\n        exception = ValidationError(error_dict)\n        self.assertEqual(error_dict, {})\n\n        error_dict = {'field1': ['E1', 'E2']}\n        exception = ValidationError(error_dict)\n        self.assertEqual(error_dict, {'field1': ['E1', 'E2']})\n        exception.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'field1': ['E1', 'E2']})\n\n        error_dict = {'field2': ['E3', 'E4']}\n        exception = ValidationError({'field1': ['E1', 'E2']})\n        exception.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'field1': ['E1', 'E2'], 'field2': ['E3', 'E4']})\n\n        error_dict = {}\n        exception = ValidationError({'field1': ['E1', 'E2']})\n        exception.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'field1': ['E1', 'E2']})\n\n        error_dict = {'field1': ['E1', 'E2'], 'field2': ['E3', 'E4']}\n        exception = ValidationError({'field1': ['E1']})\n        exception.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'field1': ['E1', 'E2'], 'field2': ['E3', 'E4']})\n\n        error_dict = {'field1': []}\n        exception = ValidationError({'field1': ['E1', 'E2']})\n        exception.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'field1': ['E1', 'E2']})\n", "    def test_update_error_dict(self):\n        error_dict = {}\n        error1 = ValidationError('message1')\n        error2 = ValidationError('message2')\n        error1.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'': [error1]})\n        error2.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'': [error1, error2]})\n        error_dict = {'field1': []}\n        error1 = ValidationError({'field1': 'error1'})\n        error1.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'field1': [error1]})\n        error2 = ValidationError({'field1': 'error2'})\n        error2.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'field1': [error1, error2]})\n        error_dict = {'field1': [ValidationError('error_a')]}\n        error3 = ValidationError({'field1': 'error_b'})\n        error3.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'field1': [ValidationError('error_a'), error3]})\n\n\n\n", "    def test_message_dict_updates(self):\n        error_dict = {'field1': 'error1', 'field2': 'error2'}\n        error1 = ValidationError(error_dict)\n        error2_dict = {}\n        error2 = ValidationError(error2_dict)\n\n        error1.update_error_dict(error2_dict)\n        self.assertEqual(error1.error_dict, {'field1': 'error1', 'field2': 'error2'})\n\n        error2_dict = {'field1': 'error3', 'field3': 'error4'}\n        error2 = ValidationError(error2_dict)\n        error1.update_error_dict(error2_dict)\n        self.assertEqual(error1.error_dict, {'field1': 'error3', 'field2': 'error2', 'field3': 'error4'})\n\n        error2_dict = {'field2': ValidationError('error5')}\n        error2 = ValidationError(error2_dict)\n        error1.update_error_dict(error2_dict)\n        self.assertEqual(error1.error_dict, {'field1': 'error3', 'field2': ['error2', 'error5'], 'field3': 'error4'})\n\n\n"], "sample_494": ["    def test_serialize_forward_references(self):\n        class ModelA(models.Model):\n            pass\n\n        class ModelB(models.Model):\n            related_a = models.ForeignKey(ModelA, on_delete=models.CASCADE)\n\n        with self.assertRaises(ValueError):\n            MigrationWriter.serialize(ModelB)\n", "    def test_serialize_custom_field_class(self):\n        class MyCustomField(models.Field):\n                super().__init__(**kwargs)\n                return \"CharField\"\n                return forms.CharField(**kwargs)\n                return f\"MyCustomField({field_name}='{value}')\"\n\n        with self.subTest(\"standard usage\"):\n            field = MyCustomField(max_length=255)\n            output = MigrationWriter.serialize(field)[0]\n            self.assertEqual(output, \"MyCustomField(name='name')\")\n", "    def test_serialize_custom_field_with_deconstruction(self):\n        class CustomField(models.Field):\n\n                return self.__class__.__name__, (), {}\n\n        class MyModel(models.Model):\n            my_field = CustomField()\n\n        string, imports = MigrationWriter.serialize(MyModel)\n        self.assertEqual(string, \"class MyModel(models.Model):\\n    my_field = migrations.test_writer.CustomField()\")\n        self.assertEqual(imports, {\"from django.db import models\"})\n", "    def test_handle_special_fields(self):\n\n        # Test serialization of fields with special handling in MigrationWriter.\n        from django.db import models\n        class MyModel(models.Model):\n            pk = models.AutoField(primary_key=True)\n\n        class MyBinaryField(models.BinaryField):\n            pass\n        \n        class MyDateTimeField(models.DateTimeField):\n                return 'timestamp with time zone' \n        \n        class MyJSONField(models.JSONField):\n                return 'json'\n        \n        class MyIPAddressField(models.IPAddressField):\n            pass\n        \n        class MyURLField(models.URLField):\n            pass\n\n        test_data = {\n            (\"MyModel\", \"pk\"): models.AutoField(primary_key=True),\n            (\"MyModel\", \"my_binary_field\"): MyBinaryField(),\n            (\"MyModel\", \"my_date_time_field\"): MyDateTimeField(),\n            (\"MyModel\", \"my_json_field\"): MyJSONField(),\n            (\"MyModel\", \"my_ip_address_field\"): MyIPAddressField(),\n            (\"MyModel\", \"my_url_field\"): MyURLField(),\n        }\n\n        for model_name, field_name, field in test_data.items():\n            with self.subTest(model=model_name, field=field_name):\n                string, imports = MigrationWriter.serialize(getattr(MyModel, field_name))\n                self.assertIn(field_name, string)\n                self.assertIn(\"from django.db import models\", imports)\n\n\n\n", "    def test_serialize_custom_field(self):\n        class MyCustomField(models.Field):\n            description = \"My custom field\"\n\n                return \"MyCustomFieldType\"\n\n                return \"MyCustomFieldType\"\n\n                self, form_class=None, form_field_class=forms.CharField, **kwargs\n            ):\n                return super().formfield(\n                    form_class=form_class,\n                    form_field_class=form_field_class,\n                    **kwargs\n                )\n\n        model = type(\n            \"MyModel\",\n            (models.Model,),\n            {\"field\": MyCustomField() },\n        )\n        string, imports = MigrationWriter.serialize(model)\n        self.assertIn(\n            f\"MyCustomFieldType\", string\n        )\n        self.assertIn(f\"from migrations.test_writer import MyCustomField\", imports)\n"], "sample_509": ["compilation error", "compilation error", "def test_date_formatting_offset():\n    formatter = mdates.DateFormatter('%Y-%m-%d %H:%M')\n    for date_num, expected_format in [\n        (1678886400, '2023-03-15 00:00'),\n        (1678886400 + 3600, '2023-03-15 01:00'),\n        (1678886400 + 3600 * 24, '2023-03-16 00:00'),\n    ]:\n        assert formatter.format_data(date_num) == expected_format\n\n", "def test_get_epoch():\n    with pytest.raises(RuntimeError):\n        mdates.get_epoch()\n    mdates._reset_epoch_test_example()\n    assert mdates.get_epoch() == '1970-01-01T00:00:00'\n\n\n\n", "def test_date_ticker_factory_zero():\n    with pytest.raises(ValueError, match=\"span must be positive\"):\n        mdates.date_ticker_factory(0)\n\n\n\n"], "sample_766": ["def test_sparse_coder_partial_fit():\n    n_components = 12\n    rng = np.random.RandomState(0)\n    V = rng.randn(n_components, n_features)  # random init\n    V /= np.sum(V ** 2, axis=1)[:, np.newaxis]\n    sc = SparseCoder(dictionary=V, transform_algorithm='lasso_lars',\n                     transform_alpha=0.001)\n\n    sc.partial_fit(X)  # Fit on the first chunk of data\n    sc.partial_fit(X)  # Fit on the remaining data\n\n    code = sc.transform(X)\n    assert not np.all(code == 0)\n    assert_less(np.sqrt(np.sum((np.dot(code, V) - X) ** 2)), 0.1)\n\n", "def test_dict_learning_online_alpha_impact():\n    n_components = 5\n    rng = np.random.RandomState(0)\n    X = rng.randn(100, 8)\n    dico = MiniBatchDictionaryLearning(n_components, n_iter=20, random_state=rng)\n    code1 = dico.fit_transform(X, alpha=0.01)\n    code2 = dico.fit_transform(X, alpha=1.0)\n    assert_array_almost_equal(code1, code2, decimal=2)\n", "def test_sparse_coder_fit_transform_and_transform():\n    n_components = 12\n    n_jobs = 2\n    rng = np.random.RandomState(0)\n    V = rng.randn(n_components, n_features)  # random init\n    V /= np.sum(V ** 2, axis=1)[:, np.newaxis]\n    sc = SparseCoder(dictionary=V, transform_algorithm='lasso_lars',\n                       transform_alpha=0.001, random_state=0, n_jobs=n_jobs)\n    sc.fit_transform(X)\n    code = sc.transform(X)\n    assert not np.all(code == 0)\n    assert_less(np.sqrt(np.sum((np.dot(code, V) - X) ** 2)), 0.1)\n\n\n\n\n", "def test_sparse_coder_partial_fit():\n    n_components = 12\n    rng = np.random.RandomState(0)\n    V = rng.randn(n_components, n_features)  # random init\n    V /= np.sum(V ** 2, axis=1)[:, np.newaxis]\n    sc = SparseCoder(dictionary=V, transform_algorithm='omp',\n                     transform_alpha=0.001)\n    sc.partial_fit(X[:100])\n    \n    code = sc.transform(X[:100])\n    assert (code.shape == (100, n_components))\n\n\n\n\n", "def test_dict_learning_online_alpha():\n    n_components = 5\n    dico = MiniBatchDictionaryLearning(n_components, n_iter=20,\n                                       random_state=0)\n    with pytest.raises(ValueError) as err:\n        dico.fit(X, alpha=-1)\n    assert str(err.value) == \"Alpha parameter should be positive.\"\n\n    with pytest.raises(ValueError) as err:\n        dict_learning_online(X, n_components, alpha=-1, random_state=0)\n    assert str(err.value) == \"Alpha parameter should be positive.\"\n"], "sample_220": ["    def test_delete_cookie_domain(self):\n        response = HttpResponse()\n        response.set_cookie('c', domain='example.com')\n        response.delete_cookie('c', domain='example.com')\n        self.assertEqual(response.cookies['c']['domain'], '')\n", "    def test_delete_cookie_existing_path(self):\n        response = HttpResponse()\n        response.set_cookie('c', path='/admin')\n        response.delete_cookie('c')\n        cookie = response.cookies['c']\n        self.assertEqual(cookie['path'], '/')\n\n\n\n", "    def test_delete_cookie_path(self):\n        response = HttpResponse()\n        response.set_cookie('c', path='/special')\n        response.delete_cookie('c', path='/else')\n        self.assertEqual(response.cookies['c']['path'], '/')\n", "    def test_delete_cookie_path(self):\n        response = HttpResponse()\n        response.set_cookie('c', path='/admin')\n        response.delete_cookie('c', path='/')\n        self.assertEqual(response.cookies['c']['path'], '/')\n", "    def test_delete_cookie_path(self):\n        response = HttpResponse()\n        response.set_cookie('c', path='/some/path')\n        response.delete_cookie('c', path='/another/path')\n        self.assertEqual(response.cookies['c']['path'], '/')\n"], "sample_826": ["def test_one_hot_encoder_handle_unknown_ignore():\n    enc = OneHotEncoder(handle_unknown='ignore')\n    X = [['abc', 2, 55], ['def', 1, 55], ['abc', 4, 55]]\n    exp = np.array([[1., 0., 1., 1.], [0., 1., 0., 1.], [1., 0., 1., 1.]],\n                    dtype='float64')\n    assert_array_equal(enc.fit_transform(X).toarray(), exp)\n\n    X2 = [['abc', 2, 55], ['def', 1, 55], ['ghi', 1, 55]]\n\n    exp2 = np.array([[1., 0., 1., 1.], [0., 1., 0., 1.], [0., 0., 0., 0.]],\n                    dtype='float64')\n    assert_array_equal(enc.transform(X2).toarray(), exp2)\n", "def test_one_hot_encoder_fit_transform_multi_category(X, expected):\n    enc = OneHotEncoder(categories='auto')\n    res = enc.fit_transform(X)\n    assert_array_equal(res.toarray(), expected)\n", "def test_one_hot_encoder_dtype_mismatch():\n    X = np.array([\n        ['a', 2], ['b', 3], ['a', 2], ['c', 4]\n    ], dtype=object)\n\n    for dtype in [np.int32, np.float32]:\n        with pytest.raises(ValueError,\n                          match=\"Input data and categories dtype mismatch\"):\n            enc = OneHotEncoder(categories=[['a', 'b', 'c'], [1, 2, 3, 4]],\n                               dtype=dtype)\n            enc.fit(X)\n\n\n\n", "def test_one_hot_encoder_invalid_dtype():\n    X = np.array([[1, 2], [3, 4]], dtype='object')\n    enc = OneHotEncoder(dtype='int8')\n    with pytest.raises(ValueError, match=\"Input dtype\"):\n        enc.fit_transform(X)\n    enc = OneHotEncoder(dtype='str')\n    with pytest.raises(ValueError, match=\"Input dtype\"):\n        enc.fit_transform(X)\n\n\n\n", "def test_one_hot_encoder_drop_from_fit(X, exp_drop):\n    enc = OneHotEncoder(drop=None)\n    enc.fit(X)\n    assert_array_equal(enc.drop_idx_, exp_drop)\n"], "sample_413": ["    def test_template_tags_with_same_name_different_order(self):\n        self.assertEqual(\n            check_for_template_tags_with_the_same_name(None),\n            [\n                Error(\n                    E003.msg.format(\n                        \"'tags'\",\n                        \"'check_framework.template_test_apps.same_tags_app_1.\"\n                        \"templatetags.same_tags', \"\n                        \"'check_framework.template_test_apps.same_tags_app_2.\"\n                        \"templatetags.same_tags'\",\n                    ),\n                    id=E003.id,\n                )\n            ],\n        )\n", "    def test_template_tags_with_same_library_name_in_different_settings(self):\n        with self.settings(\n            TEMPLATES=[\n                self.get_settings(\n                    \"same_tags\", \"same_tags_app_1.templatetags.same_tags\"\n                ),\n            ]\n        ):\n            with self.settings(\n                TEMPLATES=[\n                    self.get_settings(\n                        \"same_tags\", \"same_tags_app_2.templatetags.same_tags\"\n                    ),\n                ]\n            ):\n                self.assertEqual(\n                    check_for_template_tags_with_the_same_name(None),\n                    [\n                        Error(\n                            E003.msg.format(\n                                \"'same_tags'\",\n                                \"'check_framework.template_test_apps.same_tags_app_1.\"\n                                \"templatetags.same_tags', \"\n                                \"'check_framework.template_test_apps.same_tags_app_2.\"\n                                \"templatetags.same_tags'\",\n                            ),\n                            id=E003.id,\n                        )\n                    ],\n                )\n\n\n\n", "    def test_template_tags_with_same_library_name_and_multiple_modules(self):\n        self.assertEqual(\n            check_for_template_tags_with_the_same_name(None),\n            [self.error_same_tags],\n        )\n", "    def test_template_tags_with_different_module_name(self):\n        with self.settings(\n            TEMPLATES=[\n                self.get_settings(\n                    \"same_tags\", \"check_framework.template_test_apps.same_tags_app_1.templatetags.same_tags\"\n                ),\n                self.get_settings(\n                    \"same_tags\",\n                    \"check_framework.template_test_apps.different_tags_app.templatetags.same_tags\",\n                ),\n            ]\n        ):\n            self.assertEqual(check_for_template_tags_with_the_same_name(None), [])\n", "    def test_template_tags_with_same_library_name_and_module_path_with_different_folders(self):\n        with self.settings(\n            TEMPLATES=[\n                self.get_settings(\n                    \"same_tags\", \"same_tags_app_1.templatetags.same_tags\"\n                ),\n                self.get_settings(\n                    \"same_tags\", \"same_tags_app_1.subdir.templatetags.same_tags\"\n                ),\n            ]\n        ):\n            self.assertEqual(\n                check_for_template_tags_with_the_same_name(None),\n                [\n                    Error(\n                        E003.msg.format(\n                            \"'same_tags'\",\n                            \"'check_framework.template_test_apps.same_tags_app_1.\"\n                            \"templatetags.same_tags', \"\n                            \"'check_framework.template_test_apps.same_tags_app_1.\"\n                            \"subdir.templatetags.same_tags'\",\n                        ),\n                        id=E003.id,\n                    )\n                ],\n            )\n"], "sample_137": ["    def test_parse_rst_with_named_groups(self):\n        markup = '<p>This is a <a href=\"/admindocs/models/myapp.MyModel/\">named group</a> named <a href=\"/admindocs/models/myapp.MyModel/\">named group</a>.</p>\\n'\n        self.assertEqual(parse_rst('This is a (?P<named_group>named group) named (?P<named_group>named group).', 'model'), markup)\n\n", "    def test_replace_named_groups(self):\n        pattern = r'^(?P<a>\\w+)/b/(\\w+)$'\n        replacement = replace_named_groups(pattern)\n        self.assertEqual(replacement, r'^<a>/b/<var>$')\n", "    def test_replace_named_groups(self):\n        pattern = r\"\\(\\?P<a>\\w+\\)\\/b/(\\w+)$\"\n        expected = r\"\\<a\\>/b/(\\w+)$\"\n        self.assertEqual(replace_named_groups(pattern), expected)\n        \n        pattern = r\"\\(\\?P<a>\\w+\\)\\/b/(?P<c>\\w+)$\"\n        expected = r\"\\<a\\>/b/<c>$\"\n        self.assertEqual(replace_named_groups(pattern), expected)\n        \n        pattern = r\"\\(\\?P<a>\\w+\\)\\/b/(\\w+)$\"\n        expected = r\"\\<a\\>/b/(\\w+)$\"\n        self.assertEqual(replace_named_groups(pattern), expected)\n\n        pattern = r\"\\(\\?P<a>\\w+\\)\\/b/(?P<c>\\w+)$\"\n        expected = r\"\\<a\\>/b/<c>\"\n        self.assertEqual(replace_named_groups(pattern), expected)\n        \n        pattern = r\"^(\\w+)\\/b/([^ ]+)$\"\n        expected = r\"^<var>/b/<var>$\"\n        self.assertEqual(replace_named_groups(pattern), expected)\n", "    def test_replace_named_groups(self):\n        pattern = r'^(?P<a>\\w+)/b/(?P<c>\\w+)$'\n        replaced_pattern = replace_named_groups(pattern)\n        self.assertEqual(replaced_pattern, r'^<a>/b/<c>$')\n", "    def test_replace_named_groups(self):\n        self.assertEqual(replace_named_groups('(?P<a>\\w+)/b/(\\w+)$'), '<a>/b/(\\w+)$')\n        self.assertEqual(replace_named_groups('(?P<a>\\w+)/b/(?P<c>\\w+)/$'), '<a>/b/<c>$')\n        self.assertEqual(replace_named_groups('(?P<a>\\w+)/b/(\\w+)'), '<a>/b/(\\w+)')\n        self.assertEqual(replace_named_groups('(?P<a>\\w+)/b/(?P<c>\\w+)'), '<a>/b/<c>')\n"], "sample_1168": ["compilation error", "compilation error", "def test_make_sequence():\n    assert make_sequence(1, 3) == [1, 2, 3]\n    assert make_sequence(10, 1, 3) == [10, 11, 12]\n    assert make_sequence('a', 2, 3) == ['a', 'ab', 'abc']\n    assert make_sequence('a', -1, 3) == ['a', 'aa', 'aaa']\n    assert make_sequence(1, 2, 0) == [1]\n    assert make_sequence(1, 0, 3) == [1, 2, 3]\n    assert make_sequence(1.0, 2.0, 3.0) == [1.0, 2.0, 3.0]\n", "    def test_permutations_with_repetition():\n        assert list(permutations_with_repetition(range(3), 2)) == [\n            (0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n        assert list(permutations_with_repetition('abc', 2)) == [\n            ('a', 'a'), ('a', 'b'), ('a', 'c'), ('b', 'a'), ('b', 'b'),\n            ('b', 'c'), ('c', 'a'), ('c', 'b'), ('c', 'c')]\n        assert list(permutations_with_repetition([1, 2, 3], 3)) == [\n            (1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 2, 1), (1, 2, 2),\n            (1, 2, 3), (1, 3, 1), (1, 3, 2), (1, 3, 3), (2, 1, 1),\n            (2, 1, 2), (2, 1, 3), (2, 2, 1), (2, 2, 2), (2, 2, 3),\n            (2, 3, 1), (2, 3, 2), (2, 3, 3), (3, 1, 1), (3, 1, 2),\n            (3, 1, 3), (3, 2, 1), (3, 2, 2), (3, 2, 3), (3, 3, 1),\n            (3, 3, 2), (3, 3, 3)]\n", "compilation error"], "sample_1083": ["def test_issue_15113():\n    x = Symbol('x')\n    assert (cosh(x) + sinh(x)).expand(trig=True) == exp(x)\n    assert (cosh(x) - sinh(x)).expand(trig=True) == exp(-x)\n", "    def test_complex_args():\n        x, y = symbols('x,y', complex=True)\n        assert sinh(x + y).expand(complex=True) == sinh(x)*cosh(y) + cosh(x)*sinh(y)\n        assert cosh(x + y).expand(complex=True) == cosh(x)*cosh(y) + sinh(x)*sinh(y)\n        assert tanh(x + y).expand(complex=True) == (sinh(x + y))/cosh(x + y)\n        assert csch(x + y).expand(complex=True) == 1/sinh(x + y)\n        assert sech(x + y).expand(complex=True) == 1/cosh(x + y)\n        assert coth(x + y).expand(complex=True) == cosh(x + y)/sinh(x + y)\n\n\n\n", "def test_inverse_functions_complex():\n    a, b = symbols('a,b', real=True)\n    z = a + b*I\n    for func in [asinh, acosh, atanh, acoth, asech, acsch]:\n        assert func(z).conjugate() == func(a - b*I)\n", "def test_issue_13327():\n    x = Symbol('x')\n    assert tanh(x).rewrite(exp).expand(func=True).is_valid\n    assert sech(x).rewrite(exp).expand(func=True).is_valid\n    assert csch(x).rewrite(exp).expand(func=True).is_valid\n    assert coth(x).rewrite(exp).expand(func=True).is_valid\n", "compilation error"], "sample_890": ["def test_n_jobs_parameter(direction):\n    # Test the n_jobs parameter\n    X, y = make_regression(n_samples=100, n_features=50, random_state=0)\n\n    sfs_1 = SequentialFeatureSelector(\n        LinearRegression(), n_features_to_select=\"auto\", direction=direction, cv=2, n_jobs=1\n    )\n    sfs_2 = SequentialFeatureSelector(\n        LinearRegression(), n_features_to_select=\"auto\", direction=direction, cv=2, n_jobs=None\n    )\n    sfs_3 = SequentialFeatureSelector(\n        LinearRegression(), n_features_to_select=\"auto\", direction=direction, cv=2, n_jobs=-1\n    )\n\n    sfs_1.fit(X, y)\n    sfs_2.fit(X, y)\n    sfs_3.fit(X, y)\n\n    # Assert that the number of features selected are the same\n    assert sfs_1.n_features_to_select_ == sfs_2.n_features_to_select_\n    assert sfs_1.n_features_to_select_ == sfs_3.n_features_to_select_\n", "def test_scoring_parameter(direction):\n    X, y = make_classification(random_state=0)\n\n    # Test custom scoring function\n        return np.mean(np.abs(y_true - y_pred))\n\n    sfs = SequentialFeatureSelector(\n        KNeighborsClassifier(),\n        n_features_to_select=2,\n        direction=direction,\n        scoring=custom_scoring,\n    )\n    sfs.fit(X, y)\n\n    # Check that the custom scoring is used\n    assert sfs._scorer == custom_scoring\n", "def test_empty_X():\n    # Test handling empty input X\n    for direction in [\"forward\", \"backward\"]:\n        sfs = SequentialFeatureSelector(\n            LinearRegression(), n_features_to_select=\"auto\", direction=direction, cv=2\n        )\n        with pytest.raises(ValueError, match=\"Input X is empty\"):\n            sfs.fit(X=np.array([]), y=np.array([]))\n", "def test_large_datasets(n_samples):\n    \"\"\"\n    Test SequentialFeatureSelector with larger datasets.\n    This test aims to ensure that the logic works correctly\n    when handling a larger number of samples.\n    \"\"\"\n    X, y = make_regression(n_samples, n_features=20, random_state=0)\n    sfs = SequentialFeatureSelector(LinearRegression(), n_features_to_select=\"auto\", cv=5)\n    sfs.fit(X, y)\n    sfs.transform(X)\n\n", "def test_n_features_to_select_tol_with_uninformative_features(direction):\n    np.random.seed(0)\n    n_samples, n_features = 100, 20\n    X, y = make_regression(n_samples, n_features, random_state=0)\n    # Introduce some uninformative features\n    X = np.hstack((X, np.random.randn(n_samples, n_features // 2)))\n    sfs = SequentialFeatureSelector(\n        LinearRegression(),\n        n_features_to_select=\"auto\",\n        tol=0.01,\n        direction=direction,\n        cv=LeaveOneGroupOut(),\n    )\n    sfs.fit(X, y)\n\n    if direction == \"forward\":\n        # If `n_features_to_select` is reached too quickly, it might\n        # select uninformative features.\n        assert sfs.n_features_to_select_ > 0 \n    \n    # Check that the selected features are not all uninformative\n    selected_coefficients = sfs.estimator_.coef_\n    assert np.any(np.abs(selected_coefficients) > 1e-5)\n"], "sample_569": ["    def test_regplot_units(self):\n        x = np.arange(10)\n        y = 2 * x\n\n        p = lm._RegressionPlotter(\"x\", \"y\", data=pd.DataFrame(dict(x=x, y=y)), units=\"units\")\n        f, ax = plt.subplots()\n        p.plot(ax=ax)\n        plt.show()\n\n\n\n        assert \"units\" in ax.get_xlabel()\n", "    def test_regplot_groupby(self):\n\n        df = pd.DataFrame(data={\"x\": [1, 1, 2, 2, 3, 3],\n                                \"y\": [2, 3, 4, 5, 6, 7],\n                                \"g\": [\"a\", \"a\", \"b\", \"b\", \"a\", \"b\"]})\n\n        f, ax = plt.subplots()\n        lm.regplot(x=\"x\", y=\"y\", data=df, hue=\"g\")\n        assert len(ax.lines) == 2\n        assert len(ax.collections) == 4\n\n", "    def test_regplot_ylim(self):\n\n        f, ax = plt.subplots()\n        x, y1, y2 = np.random.randn(3, 50)\n        lm.regplot(x=x, y=y1, truncate=False)\n        lm.regplot(x=x, y=y2, truncate=False)\n        line1, line2 = ax.lines\n        assert np.array_equal(line1.get_ydata(), line2.get_ydata()) \n", "    def test_regplot_non_numeric_x(self):\n\n        df = pd.DataFrame({'x': ['a', 'a', 'b', 'b', 'c', 'c'],\n                           'y': [1, 2, 3, 4, 5, 6]})\n        with pytest.raises(TypeError):\n            lm.regplot(x=\"x\", y=\"y\", data=df)\n", "    def test_regplot_err_bands(self):\n\n        f, ax = plt.subplots()\n        lm.regplot(x=\"x\", y=\"y\", data=self.df, ci=95)\n        assert len(ax.lines) == 3\n        \n\n"], "sample_938": ["def test_sections_toc(app, status, warning):\n    app.build()\n    content = (app.outdir / 'python.1').read_text()\n    assert r'.\\\"TOC\\\"' in content \n", "def test_toc(app, status, warning):\n    app.build()\n    content = (app.outdir / 'python.1').read_text()\n    assert '\\n.TH \\fBSee Also\\fP\\n' in content\n    assert '\\n.RS\\n' in content \n\n\n\n", "def test_images(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'python.1').read_text()\n    assert r'\\includegraphics[width=0.5\\textwidth]{images/test.png}' in content\n", "def test_links(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'python.1').read_text()\n    assert r'sphinx' in content\n    assert r'https://www.sphinx-doc.org/' in content\n", "def test_images(app, status, warning):\n    app.build()\n    content = (app.outdir / 'python.1').read_text()\n    assert r'\\fIexample.png\\fP' in content\n"], "sample_1122": ["def test_issue_16126():\n    from sympy import Symbol, exp, log, I\n    x = Symbol('x')\n    y = Symbol('y')\n    a = Symbol('a', positive=True)\n    assert log(exp(x*I)) == x*I\n    assert log(exp(x*I) * a) == x*I + log(a)\n    assert log(exp(x*I) / a) == x*I - log(a)\n    assert log(exp(x*I + y*I)) == x*I + y*I\n\n\n\n", "compilation error", "def test_issue_16104():\n    from sympy import Abs, sqrt, I\n    x = Symbol('x')\n    assert Abs(sqrt(x)).conjugate() == sqrt(Abs(x))\n    assert Abs(sqrt(I*x)).conjugate() == sqrt(Abs(I*x))\n", "def test_issue_15893_complex():\n    f = Function('f', complex=True)\n    x = Symbol('x', complex=True)\n    eq = Derivative(Abs(f(x)), f(x))\n    assert eq.doit() == sign(f(x)) / (f(x)/Abs(f(x)))\n", "def test_issue_15893_complex():\n    f = Function('f')\n    x = Symbol('x', real=True)\n    eq = Derivative(Abs(f(x)), f(x))\n    assert eq.doit() == sign(f(x))\n"], "sample_710": ["    def test_setup_teardown_order(self):\n        class MyTestCase(unittest.TestCase):\n            setup_called = False\n            teardown_called = False\n\n                self.setup_called = True\n\n                self.teardown_called = True\n\n                assert self.setup_called\n                assert not self.teardown_called\n        reprec = self.run_test_file(\"test_teardown_order.py\")\n", "    def test_custom_testcase_name(pytester: Pytester) -> None:\n        pytester.makepyfile(\n            \"\"\"\n            import unittest\n\n            class MyTestCase(unittest.TestCase):\n                    unittest.TestCase.__init__(self, test_method)\n                    self.test_method = test_method\n\n            class TestIt(MyTestCase):\n                    self.assertEqual(self.test_method, \"test_custom_name\")\n\n            suite = unittest.TestSuite()\n            suite.addTest(TestIt(\"test_custom_name\"))\n            unittest.TextTestRunner().run(suite)\n\n\n        \"\"\"\n        )\n\n\n", "def test_ignore_cleanup_errors(pytester: Pytester) -> None:\n    testpath = pytester.makepyfile(\n        \"\"\"\n        import unittest\n        class MyTestCase(unittest.TestCase):\n            values = []\n                    self.values.append(1)\n                self.addCleanup(cleanup)\n                pass\n                assert 0\n            assert MyTestCase.values == [1, 1]\n    \"\"\"\n    )\n    reprec = pytester.inline_run(testpath, \"-i\")  \n    passed, skipped, failed = reprec.countoutcomes()\n    assert failed == 1  \n    assert passed == 1\n", "    def test_traceback_pruning_with_setup_exception(self):\n        pytester.makepyfile(\n            \"\"\"\n            import unittest\n\n            class MyTestCase(unittest.TestCase):\n                    raise Exception(\"setup exception\")\n\n                    pass\n            \"\"\"\n        )\n        result = pytester.runpytest()\n        assert \"setup exception\" in result.stdout.str()\n", "    def test_traceback_pruning_with_custom_init(pytester: Pytester) -> None:\n        pytester.makepyfile(\n            \"\"\"\n            import unittest\n\n            class MyTestCase(unittest.TestCase):\n                    unittest.TestCase.__init__(self, test_method)\n                    self.extra_arg = extra_arg\n\n            class TestIt(MyTestCase):\n                @classmethod\n                    assert False\n\n                    pass\n            \"\"\"\n        )\n        reprec = pytester.inline_run()\n        passed, skipped, failed = reprec.countoutcomes()\n        assert passed == 1\n        assert failed == 1\n        assert reprec.ret == 1\n"], "sample_720": ["def test_power_transformer_robust_with_outliers():\n    X = np.abs(X_2d)\n\n    for standardize in [True, False]:\n        pt = PowerTransformer(method='yeo-johnson', standardize=standardize)\n        # Add some outliers to simulate real-world data\n        X_with_outliers = np.concatenate(\n            (X, np.random.rand(X.shape[0], X.shape[1]) * 10), axis=0)\n        pt.fit(X_with_outliers)\n\n        X_trans = pt.transform(X_with_outliers)\n\n        # Check if outliers are handled correctly\n        # (the transformation should not be heavily influenced by them)\n        # This is a rudimentary check, more robust analysis might be needed\n        # for a thorough evaluation\n\n        for i in range(X.shape[1]):\n            X_exp, lmbda = stats.boxcox(X_with_outliers[:, i].flatten())\n            assert_allclose(X_trans[:, i], X_exp)\n\n        # Test inverse transformation with outliers\n        X_inv = pt.inverse_transform(X_trans)\n        assert_array_almost_equal(X_inv, X_with_outliers)\n\n        assert len(pt.lambdas_) == X.shape[1]\n        assert isinstance(pt.lambdas_, np.ndarray)\n\n", "compilation error", "compilation error", "def test_power_transformer_y_fit_transform():\n    X = np.abs(X_2d)\n\n    for method in ['box-cox', 'yeo-johnson']:\n        pt = PowerTransformer(method=method)\n        y = np.random.rand(X_2d.shape[0])\n        pt.fit(X)\n        X_trans = pt.transform(X)\n\n        y_trans = pt.transform(y)\n        y_inv = pt.inverse_transform(y_trans)\n\n        assert_almost_equal(y_inv, y)\n\n\n", "def test_power_transformer_y_scale():\n    X = np.abs(X_2d)\n\n    for standardize in [True, False]:\n        for method in ['box-cox', 'yeo-johnson']:\n            pt = PowerTransformer(method=method, standardize=standardize)\n            X_trans = pt.fit_transform(X)\n            assert_raises(\n                ValueError,\n                pt.transform,\n                X + 10,\n                y_scale=1\n            )\n\n\n\n\n    \n"], "sample_565": ["def test_grid_axes_overlap():\n    fig = plt.figure()\n    grid = Grid(fig, 111, (2, 2), direction=\"row\")\n    # Test that axes don't overlap.\n    for ax1, ax2 in zip(grid, np.concatenate([np.roll(grid, 1, axis=0),\n                                            np.roll(grid, -1, axis=0)]):\n        assert not np.any( ax1.get_position() == ax2.get_position())\n\n\n\n", "def test_grid_with_axes_class_custom():\n    class MyAxes(mpl.axes.Axes):\n            # Do something custom to draw the artist\n            pass\n    fig = plt.figure()\n    grid = Grid(fig, 111, (2, 2), axes_class=MyAxes)\n    for ax in grid:\n        ax.plot([1, 2, 3], [4, 5, 6])\n\n\n\n", "def test_grid_subfig_with_axes_class():\n    fig = plt.figure(figsize=(6, 4))\n    grid = Grid(fig, 111, (2, 2), axes_class=mpl.axes.Axes)\n    for i, ax in enumerate(grid):\n        ax.plot([i, i + 1], [i, i + 1])\n\n\n", "def test_grid_with_axes_class_not_overriding_axis():\n    fig = plt.figure()\n    grid = Grid(fig, 111, (2, 2), axes_class=mpl.axes.Axes,\n                share_all=True)\n    for ax in grid.axes_all:\n        ax.plot([0, 1])\n", "def test_anchored_artist_padding():\n    fig, ax = plt.subplots(figsize=(3, 3))\n    ada = AnchoredDrawingArea(40, 20, 0, 0, loc='upper right', pad=0.5,\n                              frameon=False)\n    p = Circle((10, 10), 5, facecolor='r')\n    ada.drawing_area.add_artist(p)\n    ax.add_artist(ada)\n\n\n"], "sample_292": ["    def test_csrf_token_on_404_removed_in_debug(self):\n        with self.settings(DEBUG=True):\n            response = self.client.get('/does not exist/')\n            # The error handler returns status code 599.\n            self.assertEqual(response.status_code, 599)\n            token1 = response.content\n            response = self.client.get('/does not exist/')\n            self.assertEqual(response.status_code, 599)\n            token2 = response.content\n            self.assertFalse(equivalent_tokens(token1.decode('ascii'), token2.decode('ascii')))\n", "    def test_csrf_token_on_500_stays_constant(self):\n        response = self.client.get('/internal_server_error/')\n        self.assertEqual(response.status_code, 500)\n        token1 = response.content\n        response = self.client.get('/internal_server_error/')\n        self.assertEqual(response.status_code, 500)\n        token2 = response.content\n        self.assertTrue(equivalent_tokens(token1.decode('ascii'), token2.decode('ascii')))\n\n\n\n", "    def test_csrf_token_error_handling_view(self):\n        response = self.client.get('/csrf_token_error/')\n        self.assertEqual(response.status_code, 200)\n        self.assertContains(response, 'CSRF token error handling view')\n\n\n", "    def test_csrf_token_on_500_stays_constant(self):\n        response = self.client.get('/error/')\n        # The error handler returns a 500 status code on purpose\n        self.assertEqual(response.status_code, 500)\n        token1 = response.content\n        response = self.client.get('/error/')\n        self.assertEqual(response.status_code, 500)\n        token2 = response.content\n        self.assertTrue(equivalent_tokens(token1.decode('ascii'), token2.decode('ascii')))\n\n", "    def test_csrf_token_on_404_without_csrf_cookie(self):\n        self.client.cookies.clear()\n        response = self.client.get('/does not exist/')\n        self.assertEqual(response.status_code, 599)\n        token = response.content\n        response = self.client.get('/does not exist/')\n        self.assertEqual(response.status_code, 599)\n        new_token = response.content\n        self.assertNotEqual(token, new_token)\n\n\n"], "sample_614": ["    def test_diff_coords_repr_with_different_attrs(self) -> None:\n        coords_a = {\"x\": np.array([\"a\", \"b\"], dtype=\"U1\"), \"y\": np.array([1, 2, 3], dtype=\"int64\")}\n        coords_a[\"x\"].attrs[\"source\"] = \"file1\"\n        coords_b = {\"x\": np.array([\"a\", \"c\"], dtype=\"U1\"), \"y\": np.array([1, 2], dtype=\"int64\")}\n        coords_b[\"x\"].attrs[\"source\"] = \"file2\"\n        expected = dedent(\n            \"\"\"\\\n        Left and right coordinate objects are not equal\n        Differing dimensions:\n            (x: 2) != (x: 2)\n        Differing coordinates:\n        L * x        (x) %cU1 'a' 'b' source: file1\n        R * x        (x) %cU1 'a' 'c' source: file2\"\"\"\n            % (byteorder, byteorder)\n        )\n\n        actual = formatting.diff_coords_repr(coords_a, coords_b, \"equals\")\n        assert actual == expected\n", "    def test_summary_repr_large_dataset(capsys):\n        # Create a large dataset with many variables and attributes\n        data = np.random.randn(100, 100, 100)\n        ds = xr.Dataset(\n            {\n                \"var_{}\".format(i): xr.DataArray(data[:, :, i], dims=(\"x\", \"y\", \"z\"))\n                for i in range(100)\n            },\n            coords={\"x\": np.arange(100), \"y\": np.arange(100), \"z\": np.arange(100)},\n            attrs={\"large_attr\": \"This is a large attribute value\"},\n        )\n        # Call the summary_repr function\n        summary = formatting.summary_repr(ds, 5)\n\n        # Capture the output\n        captured = capsys.readouterr()\n\n        # Assert that the summary includes the most important information\n        # while being concise\n        assert \"Dimensions\" in captured.out\n        assert \"Data variables\" in captured.out\n        assert \"Attributes\" in captured.out\n        assert \"large_attr\" in captured.out\n\n        # Assert that the summary does not display the full contents of each variable or attribute\n        assert \"...\" in captured.out\n", "    def test_data_var_repr(self) -> None:\n        var = xr.DataArray(np.arange(10), dims=\"x\", coords={\"x\": [1, 2, 3, 4, 5]})\n        actual = formatting.data_var_repr(var)\n        expected = dedent(\n            \"\"\"\\\n        <xarray.DataArray (x: 5)>\n        array([ 0,  1,  2,  3,  4])\n        Dimensions:\n            * x: 5\"\"\"\n        )\n        assert actual == expected\n\n\n", "def test_array_repr_with_large_data_array(tmp_path) -> None:\n    with open(tmp_path / \"test.nc\", \"w\") as f:\n        pass\n\n    big_array = np.random.randn(1000, 1000)\n    ds = xr.Dataset(\n        data_vars={\"var\": ((\"x\", \"y\"), big_array)},\n        coords={\n            \"x\": np.arange(1000),\n            \"y\": np.arange(1000),\n        },\n    )\n    ds.to_netcdf(tmp_path / \"test.nc\")\n\n    with xr.open_dataset(tmp_path / \"test.nc\") as ds:\n        actual = formatting.array_repr(ds[\"var\"])\n        assert \"array([[...\" in actual\n        assert \"...\" in actual\n", "compilation error"], "sample_905": ["        def __init__(self, func):\n            self.func = func\n", "def test_getargvalues_unbound():\n        pass\n\n    args = [1, 2]\n    kwargs = {'c': 3}\n    values = inspect.getargvalues(func, *args, **kwargs)\n    assert values.args == ['a', 'b']\n    assert values.defaults == ()\n    assert values.varargs == 'args'\n    assert values.keywords == ['c']\n    assert values.values == [1, 2, 3]\n", "        def __init__(self):\n            pass\n", "        def __init__(self, *args, **kwargs):\n            super().__init__(*args, **kwargs)\n", "def test_getargspec_with_defaults():\n        pass\n\n    argspec = inspect.getargspec(func)\n    assert argspec.args == ['a', 'b', *args, **kwargs]\n    assert argspec.defaults == (1,)\n    assert argspec.varargs == 'args'\n    assert argspec.keywords == 'kwargs'\n\n\n\n"], "sample_200": ["    def test_smtp_server_timeout_connection_error(self):\n        \"\"\"\n        Connection errors are correctly handled when a timeout is set.\n        \"\"\"\n        self.server.flush_sink()  # Clear any previous messages.\n\n        backend = smtp.EmailBackend(timeout=1)\n        backend.open()\n        try:\n            with self.assertRaises(TimeoutError):\n                backend.send_messages([EmailMessage('Subject', 'Content',\n                'from@example.com', ['to@example.com'])])\n        finally:\n            backend.close() \n", "    def test_connection_errors_not_raised_if_closed(self):\n        \"\"\"\n        If a connection error happens, it won't raise an exception if the\n        backend is already closed.\n        \"\"\"\n        with self.assertRaises(ConnectionError):\n            self.backend.open()\n\n        self.backend.close()\n        with self.assertRaises(ConnectionError):\n            self.backend.send_messages([], connection=self.backend)\n\n\n", "    def test_connection_error_with_timeout(self):\n        \"\"\"\n        Timeout exception is raised with fail_silently=False.\n        \"\"\"\n        class FailingConnection(FakeSMTPChannel):\n                raise ConnectionError('Fake Connection Error')\n\n        self.server.channel_class = FailingConnection\n        self.backend.open()\n        with self.assertRaises(TimeoutError):\n            self.backend.send_messages([EmailMessage('Subject', 'Content', 'from@example.com', ['to@example.com'])])\n", "    def test_send_messages_with_exception_handling(self):\n        class ExceptionThrowingSMTPBackend(smtp.EmailBackend):\n                raise Exception(\"Simulated connection error\")\n\n        backend = ExceptionThrowingSMTPBackend()\n        email = EmailMessage('Subject', 'Content', 'from@example.com', ['to@example.com'])\n        sent = backend.send_messages([email])\n        self.assertEqual(sent, 0)\n        backend.fail_silently = True\n        sent = backend.send_messages([email])\n        self.assertEqual(sent, 0)\n", "    def test_send_messages_with_data_callback(self):\n        class MockDataCallback:\n                self.processed_messages.append(msg)\n        mock_callback = MockDataCallback()\n        mock_callback.processed_messages = []\n        backend = smtp.EmailBackend()\n        backend.connection = True\n        email = EmailMessage('Subject', 'Content', 'from@example.com', ['to@example.com'])\n        backend.send_messages([email], data_callback=mock_callback)\n        self.assertEqual(len(mock_callback.processed_messages), 1)\n\n\n"], "sample_275": ["    def test_delete_with_filter_and_related_objects(self):\n        \"\"\"Tests deleting objects with a filter on related objects and the related objects themselves.\n\n        Tests cases outlined in #26743 where deleting an object based on \n        related object data would lead to unexpected results if the\n        related object is also deleted during the same query.\n        \"\"\"\n        child = Child.objects.create(name='Alice')\n        toy1 = Toy.objects.create(name='Toy 1')\n        toy2 = Toy.objects.create(name='Toy 2')\n        PlayedWith.objects.create(child=child, toy=toy1)\n\n        self.assertEqual(Child.objects.count(), 1)\n\n        # Delete a child, along with all related PlayedWith instances\n        # (and Toys, cascading through PlayedWith).\n        Child.objects.filter(name='Alice').delete()\n\n        self.assertEqual(Child.objects.count(), 0)\n        self.assertEqual(PlayedWith.objects.count(), 0)\n        self.assertEqual(Toy.objects.count(), 0)\n\n", "    def test_delete_with_subquery_filter(self):\n        \"\"\"\n        Deleting a model instance using a subquery as a filter should be\n        supported and not result in a TypeError.\n\n        Refers to #21135.\n        \"\"\"\n        with self.assertNumQueries(1):\n            child1 = Child.objects.create(name='alice')\n\n            played_with = PlayedWith.objects.create(child=child1, toy=Toy.objects.create(name='ball'), date=datetime.date.today())\n\n            # Delete the child, filtering using a subquery to get the\n            # child's played_with instances.\n            Child.objects.filter(\n                playedwith__id=played_with.id\n            ).delete()\n        self.assertFalse(Child.objects.filter(name='alice').exists())\n\n\n\n", "    def test_delete_with_values_list(self):\n        with self.assertRaisesMessage(TypeError, \"Cannot call delete() after .values_list().\"):\n            Book.objects.values_list('id', flat=True).delete()\n", "    def test_delete_with_subquery_ordering(self):\n        h = House.objects.create(address='Foo')\n        OrderedPerson.objects.create(name='Jack', lives_in=h)\n        OrderedPerson.objects.create(name='Bob', lives_in=h)\n        \n        # ordering in subquery\n        with self.assertNumQueries(1):\n            OrderedPerson.objects.filter(\n                lives_in__address='Foo'\n            ).order_by('name').delete()\n        self.assertEqual(OrderedPerson.objects.count(), 0)\n", "    def test_delete_with_subquery_values(self):\n        \"\"\"\n        Regression for #24570: Deleting across subqueries with complex\n        SELECT statements (including `.values()` and `.annotate()`) should\n        work correctly.\n        \"\"\"\n        # Create a bunch of books\n        for i in range(10):\n            Book.objects.create(title=f\"Book {i}\", pagecount=i * 10)\n\n        with self.assertNumQueries(1):\n            # Subquery for books with pagecount > 5, then filter by title\n            # and delete them.\n            Book.objects.raw(\n                \"DELETE FROM delete_regress_book WHERE id IN (SELECT id FROM delete_regress_book WHERE pagecount > 5)\"\n            ).delete()\n\n        # All books with pagecount > 5 should be deleted\n        self.assertEqual(Book.objects.filter(pagecount__gt=5).count(), 0)\n"], "sample_223": ["    def test_ticket_24848(self):\n        \"\"\"\n        Check that querying with .in does not break joins when a related\n        object is related to an FExpression.\n        \"\"\"\n        p1 = ParentClass.objects.create(id=1)\n        p2 = ParentClass.objects.create(id=2)\n        c1 = ChildClass.objects.create(parent=p1)\n        c2 = ChildClass.objects.create(parent=p2)\n        c3 = ChildClass.objects.create(parent=p1)\n        c4 = ChildClass.objects.create(parent=p2)\n        self.assertEqual(\n            ChildClass.objects.filter(id__in=[c1.id, c2.id]).count(), 2\n        )\n        qs = ChildClass.objects.filter(\n            id__in=F('parent__id').cast('IntegerField')\n        )\n        self.assertEqual(qs.count(), 4)\n\n\n", "    def test_ticket_24888(self):\n        \"\"\"\n        Test for ticket 24888 - nested related lookups with None values\n        should not break queries.\n        \"\"\"\n        parent = ParentObject.objects.create(name='parent')\n        child1 = ChildObject.objects.create(parent=parent, name='child1')\n        child2 = ChildObject.objects.create(parent=None, name='child2')\n        nested_child = NestedObject.objects.create(child=child1)\n\n        q1 = ParentObject.objects.filter(childobject__name='child1')\n        self.assertSequenceEqual(q1, [parent])  \n        q2 = ParentObject.objects.filter(\n            childobject__nestedobject__isnull=False\n        )\n        self.assertSequenceEqual(q2, [parent])  \n        q3 = ParentObject.objects.filter(childobject__nestedobject__isnull=True)\n        self.assertSequenceEqual(q3, []) \n", "    def test_ticket_24605_continued(self):\n        \"\"\"Test ticket 24605 continued - ensure nested subqueries with quoted table names.\"\"\"\n        # This test case builds upon previous test, ensuring that the subquery's alias is properly quoted\n        # even when nested within another subquery.\n\n        School.objects.create(name='SchoolA')\n        School.objects.create(name='SchoolB')\n        Student.objects.create(school_id=1, name='Student1')\n        Student.objects.create(school_id=1, name='Student2')\n        Student.objects.create(school_id=2, name='Student3')\n\n        query_with_nested_subqueries = (\n            Q(pk__in=Student.objects.filter(\n                Q(school__name='SchoolA') & Q(pk__in=Student.objects.filter(name='Student1'))\n            ))\n            &\n            ~Q(name__startswith='Student')\n        )\n        self.assertQuerysetEqual(\n            Student.objects.filter(query_with_nested_subqueries),\n            []\n        )\n\n\n\n", "    def test_ticket_25010(self):\n        \"\"\"\n        Ensure queries with nested F expressions in Q objects are handled correctly\n        \"\"\"\n        c1 = Category.objects.create(name='c1')\n        c2 = Category.objects.create(name='c2')\n        i1 = Item.objects.create(category=c1)\n        i2 = Item.objects.create(category=c2)\n\n        q = Q(category__name=F('name')) & Q(pk__gt=F('category__pk'))\n        self.assertQuerysetEqual(Item.objects.filter(q), [i1])\n", "    def test_ticket_24605_multiple_conditions(self):\n        School.objects.create()\n        qs = School.objects.filter(\n            Q(pk__in=()) | Q(name__startswith='S'),\n        )\n        self.assertQuerysetEqual(qs, [])\n\n\n\n"], "sample_731": ["def test_data_attributes():\n    data = fetch()\n    assert(data.feature_names == ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms',\n                                   'Population', 'AveOccup', 'Latitude', 'Longitude'])\n    assert(data.DESCR == MODULE_DOCS) \n", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_550": ["def test_toolmanager_auto_add():\n    with pytest.warns(UserWarning, match=_EXPECTED_WARNING_TOOLMANAGER):\n        plt.rcParams['toolbar'] = 'toolmanager'\n    fig = plt.gcf()\n    initial_len = len(fig.canvas.manager.toolmanager.tools)\n    assert 'pan' in fig.canvas.manager.toolmanager.tools\n\n    fig.canvas.manager.toolmanager.clear_tools()\n    assert len(fig.canvas.manager.toolmanager.tools) == 0\n\n    with pytest.warns(UserWarning, match=\"ToolManager automatically added\"):\n        fig.canvas.manager.toolmanager.add_tools('pan')\n    assert len(fig.canvas.manager.toolmanager.tools) == initial_len - 1\n    assert 'pan' in fig.canvas.manager.toolmanager.tools\n", "def test_toolmanager_get_tool_kwargs():\n    with pytest.warns(UserWarning, match=_EXPECTED_WARNING_TOOLMANAGER):\n        plt.rcParams['toolbar'] = 'toolmanager'\n    fig = plt.gcf()\n    rubberband = fig.canvas.manager.toolmanager.get_tool('rubberband')\n    assert rubberband.get_kwargs() == {}\n    rubberband_kwargs = {'mode': 'xyxy'}\n    fig.canvas.manager.toolmanager.set_tool_kwargs('rubberband', rubberband_kwargs)\n    assert rubberband.get_kwargs() == rubberband_kwargs\n    with pytest.warns(UserWarning,\n                      match=\"ToolManager does not control tool 'foo'\"):\n        assert fig.canvas.manager.toolmanager.set_tool_kwargs('foo', {}) is None\n\n", "def test_toolmanager_disable_tool():\n    with pytest.warns(UserWarning, match=_EXPECTED_WARNING_TOOLMANAGER):\n        plt.rcParams['toolbar'] = 'toolmanager'\n    fig = plt.gcf()\n    rubberband = fig.canvas.manager.toolmanager.get_tool('rubberband')\n    assert rubberband.enabled\n    fig.canvas.manager.toolmanager.disable_tool('rubberband')\n    assert not rubberband.enabled\n    with pytest.raises(ValueError, match=\"Tool not found: 'foo'\"):\n        fig.canvas.manager.toolmanager.disable_tool('foo')\n", "def test_toolmanager_switch_tool():\n    with pytest.warns(UserWarning, match=_EXPECTED_WARNING_TOOLMANAGER):\n        plt.rcParams['toolbar'] = 'toolmanager'\n    fig = plt.gcf()\n    rb = fig.canvas.manager.toolmanager.get_tool('rubberband')\n    assert rb.active\n    fig.canvas.manager.toolmanager.switch_tool('pan')\n    assert not rb.active and fig.canvas.manager.toolmanager.get_tool('pan').active\n\n\n\n", "def test_toolmanager_set_active_tool():\n    with pytest.warns(UserWarning, match=_EXPECTED_WARNING_TOOLMANAGER):\n        plt.rcParams['toolbar'] = 'toolmanager'\n    fig = plt.gcf()\n    initial_tool = fig.canvas.manager.toolmanager.get_active_tool()\n    assert initial_tool is not None  # Should be the default tool\n\n    fig.canvas.manager.toolmanager.set_active_tool('pan')\n    active_tool = fig.canvas.manager.toolmanager.get_active_tool()\n    assert active_tool.name == 'pan'\n    fig.canvas.manager.toolmanager.set_active_tool(\n        'zoom')  # Switch to zoom tool\n    active_tool = fig.canvas.manager.toolmanager.get_active_tool()\n    assert active_tool.name == 'zoom'\n    fig.canvas.manager.toolmanager.set_active_tool('foo')  # Try invalid tool\n    with pytest.warns(UserWarning,\n                      match=\"ToolManager doesn't know tool 'foo'\"):\n        assert fig.canvas.manager.toolmanager.get_active_tool() is None\n\n    fig.canvas.manager.toolmanager.set_active_tool(initial_tool)\n"], "sample_957": ["def test_stringify_type_hints_callable_complex(annotation):\n    assert stringify(annotation) == f\"Callable[[{stringify(annotation.__args__[0])}], {stringify(annotation.__args__[-1])}]\"\n", "def test_stringify_type_hints_ForwardRef():\n    from typing import ForwardRef  # type: ignore\n    assert stringify(ForwardRef(\"myint\")) == \"myint\"  \n\n\n\n", "def test_stringify_nested_types(annotation, expected):\n    assert stringify(annotation) == expected\n\n\n\n", "def test_restify_type_hints_Callable_variadic(annotation: Callable):\n    assert restify(annotation) == (\n        \":class:`~typing.Callable`\\\\ \"\n        f\"[[{', '.join(repr(a) for a in annotation.__args__)}], \"\n        f\"{restify(annotation.__return__)}]\"\n    )\n\n", "def test_stringify_type_hints_GenericMeta():\n    from typing import GenericMeta\n\n    class MyGenericMeta(GenericMeta):\n            return super().__new__(cls, name, bases, attrs)\n\n    class MyClass(metaclass=MyGenericMeta):\n            self.a = a\n            self.b = b\n\n    assert stringify(MyClass) == \"tests.test_util_typing.MyClass\"\n\n\n\n"], "sample_1172": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_700": ["def test_xfail_with_fixture_scope_teardown(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture(scope=\"function\")\n            print(\"Setting up fixture\")\n            yield\n            print(\"Tearing down fixture\")\n\n        @pytest.mark.xfail\n            assert 0\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"Setting up fixture\",\n            \"Tearing down fixture\",\n            \"*XFAIL*\",\n        ]\n    )\n\n", "compilation error", "    def test_relpath_no_rootdir(pytester: Pytester) -> None:\n        pytester.makepyfile(\n            **{\n                \"tests/test_1.py\": \"\"\"\n            import pytest\n            @pytest.mark.skip()\n                pass\n                \"\"\",\n            }\n        )\n        result = pytester.runpytest(\"-rs\", \"tests/test_1.py\")\n        result.stdout.fnmatch_lines(\n            [\"SKIPPED [[]1[]] tests/test_1.py:2: unconditional skip\"]\n        ) \n", "    def test_relative_import_path(pytester: Pytester) -> None:\n        pytester.makepyfile(\n            \"test_1.py\": \"\"\"\n        import pytest\n        from .sub import test_sub\n        @pytest.mark.skip()\n            pass\n            \n            pass\n        \"\"\",\n            \"tests/sub.py\": \"\"\"\n            pass\n        \"\"\",\n        )\n        result = pytester    \n        result.stdout.fnmatch_lines(\n            [\"SKIPPED [[]1[]] tests/test_1.py:2: unconditional skip\"]\n        )\n", "compilation error"], "sample_249": ["    def test_clone_test_db(self, mocked_sync_apps, mocked_migrate, *mocked_objects):\n        test_connection = get_connection_copy()\n        test_connection.settings_dict['TEST']['MIGRATE'] = True\n        creation = test_connection.creation_class(test_connection)\n        if connection.vendor == 'oracle':\n            # Don't close connection on Oracle.\n            creation.connection.close = mock.Mock()\n        old_database_name = test_connection.settings_dict['NAME']\n        try:\n            with mock.patch.object(creation, '_create_test_db'):\n                creation.create_test_db(verbosity=0, autoclobber=True, serialize=False)\n            # Clone the test db\n            creation.clone_test_db(suffix=\"clone\")\n\n            # Assert that the clone's settings dict is modified correctly\n            clone_settings = creation.get_test_db_clone_settings(\"clone\")\n            self.assertDictEqual(clone_settings, {\n                'HOST': test_connection.settings_dict['HOST'],\n                'NAME': f'{test_connection.settings_dict[\"NAME\"]}_clone',\n                'PORT': test_connection.settings_dict['PORT'],\n                'ENGINE': test_connection.settings_dict['ENGINE'],\n                'USER': test_connection.settings_dict['USER'],\n                'PASSWORD': test_connection.settings_dict['PASSWORD'],\n                'ATOMIC_REQUESTS': test_connection.settings_dict['ATOMIC_REQUESTS'],\n                'TEST': test_connection.settings_dict['TEST']\n            })\n\n            # Migrations don't run\n            mocked_migrate.assert_not_called()\n\n\n\n            \n        finally:\n            with mock.patch.object(creation, '_destroy_test_db'):\n                creation.destroy_test_db(old_database_name, verbosity=0)\n", "    def test_serialize_db_to_string_with_objects_not_migrated(self):\n        # serialize_db_to_string() doesn't serialize objects from non-migrated apps.\n        no_migration_app_name = 'no_migrations'\n        with mock.patch('django.apps.AppConfig.apps', autospec=True) as mocked_apps:\n            mocked_apps.get_app_config.return_value = mock.Mock(name=no_migration_app_name)\n            # Create an object in the non-migrated app.\n            ObjectReference.objects.create(name='test')\n            data = connection.creation.serialize_db_to_string()\n        self.assertNotIn(f'\"model\": \"no_migrations.objectreference\"', data)\n", "    def test_deserialize_db_from_string_with_missing_model(self):\n        # serialize_db_to_string() and deserialize_db_from_string() handles\n        # the case when a model is missing from the database schema.\n\n        data = \"\"\"\n        [\n            {\n                \"model\": \"nonexistent_app.nonexistent_model\",\n                \"pk\": 1,\n                \"fields\": {\"field1\": \"value1\", \"field2\": \"value2\"}\n            }\n        ]\"\"\"\n        connection.creation.deserialize_db_from_string(data)\n        # Check that no exception is raised and the database remains intact.\n        with self.assertRaises(DoesNotExist):\n            nonexistent_model = Object.objects.get()\n\n\n\n", "    def test_deserialize_db_from_string_with_no_objects(self):\n        # deserialize_db_from_string() handles cases with no objects in the database.\n        data = '[]'\n        connection.creation.deserialize_db_from_string(data)\n        self.assertEqual(Object.objects.count(), 0)\n", "    def test_object_reference_delete_with_circular_reference(self):\n        obj_a = ObjectReference.objects.create(obj=Object.objects.create())\n        Object.objects.create()  \n        obj_b = ObjectReference.objects.create(obj=obj_a)\n\n        # Serialize objects.\n        with mock.patch('django.db.migrations.loader.MigrationLoader') as loader:\n            # serialize_db_to_string() serializes only migrated apps, so mark\n            # the backends app as migrated.\n            loader_instance = loader.return_value\n            loader_instance.migrated_apps = {'backends'}\n            data = connection.creation.serialize_db_to_string()\n\n        # Delete object_a which is referenced by object_b\n        ObjectReference.objects.filter(obj__pk=obj_a.pk).delete()\n\n        # Deserialize objects.\n        connection.creation.deserialize_db_from_string(data)\n        \n\n"], "sample_101": ["    def test_script_name_rewrite(self):\n        \"\"\"\n        get_script_name() handles rewritten URLs correctly.\n        \"\"\"\n        environ = self.request_factory._base_environ(\n            PATH_INFO='/something',\n            SCRIPT_URL=b'/app/something',\n            REQUEST_METHOD='GET'\n        )\n        script_name = get_script_name(environ)\n        self.assertEqual(script_name, '/app')\n", "    def test_limited_stream(self):\n        content_length = 10\n        stream = LimitedStream(BytesIO(b'Hello World!'), content_length)\n        self.assertEqual(stream.read(5), b'Hello')\n        self.assertEqual(stream.read(5), b' World!')\n        self.assertEqual(stream.read(), b'')\n\n\n", "    def test_content_length_works_as_expected(self):\n        application = get_wsgi_application()\n        environ = self.request_factory._base_environ(\n            PATH_INFO=\"/\",\n            CONTENT_TYPE=\"text/plain\",\n            REQUEST_METHOD=\"POST\",\n            CONTENT_LENGTH=\"5\",\n        )\n        response_data = []\n\n            response_data.append((status, headers))\n\n        response = application(environ, start_response)\n        self.assertEqual(len(response_data), 1)\n        self.assertEqual(response_data[0][0], \"200 OK\")\n        self.assertIn(('Content-Length', '5'), response_data[0][1])\n        self.assertEqual(response.read(), b\"hello\")\n", "    def test_content_reading(self):\n        \"\"\"\n        Content is read correctly even if accessed multiple times\n        (regression for #23456).\n        \"\"\"\n        application = get_wsgi_application()\n        environ = self.request_factory._base_environ(\n            PATH_INFO=\"/\",\n            CONTENT_TYPE=\"text/plain\",\n            REQUEST_METHOD=\"POST\",\n            CONTENT_LENGTH=\"10\",\n        )\n        environ['wsgi.input'] = BytesIO(b'Hello World')\n        response_data = {}\n\n            response_data['status'] = status\n            response_data['headers'] = headers\n\n        response = application(environ, start_response)\n\n        # Read the content twice and ensure both reads are consistent.\n        self.assertEqual(response.read(), b'Hello World')\n        self.assertEqual(response.read(), b'')\n\n", "    def test_post_data(self):\n        \"\"\"\n        POST data is correctly loaded from input stream.\n        \"\"\"\n        request = self.request_factory.post('/my-view/', {'key1': 'value1', 'key2': 'value2'})\n        response = get_wsgi_application()(request._environ, lambda status, headers: [])\n        self.assertEqual(request.POST, {'key1': 'value1', 'key2': 'value2'}) \n\n\n"], "sample_862": ["def test_callable_analyzer_no_input(Vectorizer):\n    if issubclass(Vectorizer, HashingVectorizer):\n        pytest.xfail('HashingVectorizer is not supported on PyPy')\n\n    analyzer = lambda x: x\n\n    with pytest.raises(ValueError):\n        Vectorizer(analyzer=analyzer, input='some_input').fit_transform([])\n\n", "def test_vectorizer_non_string_input_error(Vectorizer):\n    if issubclass(Vectorizer, HashingVectorizer):\n        pytest.xfail('HashingVectorizer is not supported on PyPy')\n\n    data = [1, 2, 3]\n    with pytest.raises(TypeError, match=\"Input must be a string\"):\n        Vectorizer().fit_transform(data)\n", "def test_vectorizer_partial_fit(Estimator):\n    if issubclass(Estimator, HashingVectorizer):\n        pytest.xfail('HashingVectorizer is not supported on PyPy')\n\n    X = [['this is text', 'a test'], ['this is another text']]\n    vectorizer = Estimator()\n\n    # partial fit with first document\n    vectorizer.partial_fit(X[:1])\n\n    # fit the remaining document\n    vectorizer.partial_fit(X[1:])\n\n    # check if the vocabulary is consistent\n    assert set(vectorizer.get_feature_names_out()) == set(\n        vectorizer.transform(X).toarray().sum(axis=0) > 0)\n\n", "def test_vectorizer_invalid_analyzer_type():\n    with pytest.raises(TypeError):\n        CountVectorizer(analyzer=123).fit([])\n    with pytest.raises(TypeError):\n        HashingVectorizer(analyzer=123).fit([])\n    with pytest.raises(TypeError):\n        TfidfVectorizer(analyzer=123).fit([])\n    with pytest.raises(TypeError):\n        CountVectorizer(analyzer=[1, 2, 3]).fit([])\n    with pytest.raises(TypeError):\n        HashingVectorizer(analyzer=[1, 2, 3]).fit([])\n    with pytest.raises(TypeError):\n        TfidfVectorizer(analyzer=[1, 2, 3]).fit([])\n\n\n\n", "def test_vectorizer_validation_error_types(Vectorizer):\n    data = ['this is text']\n    if issubclass(Vectorizer, HashingVectorizer):\n        pytest.xfail('HashingVectorizer is not supported on PyPy')\n    for param_name in ['stop_words', 'tokenizer', 'preprocessor',\n                       'ngram_range', 'token_pattern', 'analyzer',\n                       'max_features', 'max_df', 'min_df',\n                       'vocabulary']:\n        for invalid_type in [int, float, None, list]:\n            with pytest.raises(ValueError,\n                               match=f\"Parameter '{param_name}' \"\n                                     f\"must be of type 'str', 'None', \"\n                                     f\"or callable, not {type(invalid_type)}\"):\n                Vectorizer().set_params(**{param_name: invalid_type})\n            with pytest.raises(ValueError, match=f\"Parameter '{param_name}' \"\n                                     f\"must be of type 'str', 'None', \"\n                                     f\"or callable, not {type(invalid_type)}\"):\n                Vectorizer().fit_transform([data], **{param_name: invalid_type})\n\n"], "sample_85": ["    def test_fast_delete_signals(self):\n            self.assertEqual(kwargs['instance'].pk, 1)\n            pass\n        models.signals.pre_delete.connect(pre_delete, sender=User)\n        for signal_name in ('pre_delete', 'post_delete'):\n            with self.subTest(signal=signal_name):\n                u = User.objects.create()\n                signal = getattr(models.signals, signal_name)\n                signal.connect(post_delete, sender=User)\n                self.assertNumQueries(2, u.delete)\n                signal.disconnect(post_delete, sender=User)\n\n\n\n", "    def test_fast_delete_signals(self):\n        deleted_objects = []\n\n            deleted_objects.append(instance.pk)\n\n        models.signals.post_delete.connect(delete_handler, sender=User)\n        \n        u = User.objects.create()\n        u.delete()\n        self.assertEqual([u.pk], deleted_objects)\n\n        models.signals.post_delete.disconnect(delete_handler, sender=User)\n\n\n", "    def test_fast_delete_on_empty_m2m(self):\n        f = M2MFrom.objects.create()\n        t = M2MTo.objects.create()\n        f.m2m.add(t)\n        f.m2m.remove(t)\n        # 1 query for deleting f.\n        self.assertNumQueries(1, f.delete)\n        self.assertFalse(M2MFrom.objects.exists())\n        self.assertTrue(M2MTo.objects.filter(pk=t.pk).exists())\n\n\n\n", "    def test_fast_delete_with_related_manager(self):\n        class RelatedManager(models.Manager):\n                return self.filter(m2m__related_field='related')\n\n        class RelatedModel(models.Model):\n            m2m = models.ManyToManyField(M, related_name='related_field')\n            related_field = models.CharField(max_length=100)\n\n        m1 = M.objects.create()\n        r1 = RelatedModel.objects.create(related_field='related')\n        r1.m2m.add(m1)\n        # The related manager should filter correctly.\n        self.assertEqual(RelatedModel.objects.get_related().count(), 1)\n\n        # Fast delete should propagate to related Model\n        r1.delete()\n        self.assertFalse(RelatedModel.objects.filter(pk=r1.pk).exists())\n        self.assertFalse(M.objects.filter(pk=m1.pk).exists())\n\n\n\n", "    def test_fast_delete_after_m2m_create(self):\n        t = M2MTo.objects.create()\n        f = M2MFrom.objects.create()\n        f.m2m.add(t)\n        # 1 to delete f, 1 to fast-delete m2m for f\n        f.delete()\n        self.assertEqual(M2MFrom.objects.count(), 0)\n        self.assertEqual(M2MTo.objects.count(), 0)\n\n\n\n"], "sample_340": ["    def test_multiple_apps(self):\n        \"\"\"\n        Checks that migrations from multiple apps can be loaded correctly.\n        \"\"\"\n        loader = MigrationLoader(connection)\n        loader.build_graph()\n        self.assertEqual(\n            set(x[0] for x in loader.graph.nodes),\n            {'migrations.app1', 'migrations.app2', 'migrations'},\n        )\n", "    def test_handling_multiple_versions(self):\n        \"\"\"\n        Tests that MigrationLoader can handle multiple versions of the same migration\n        name from different apps.\n        \"\"\"\n        loader = MigrationLoader(connection)\n        loader.build_graph()\n\n        # Check that the migrations are loaded correctly, with\n        # distinct names for migrations with the same name from different apps.\n        self.assertEqual(\n            sorted([m[1] for m in loader.graph.nodes if m[0] == \"migrations\"]),\n            sorted(\n                [\n                    \"0001_initial\",\n                    \"0002_second\",\n                    \"0003_third\",\n                    \"app2_0001_initial\",\n                    \"app2_0002_second\",\n                ]\n            ),\n        )\n\n\n", "    def test_unapplied_migrations_recorded(self):\n        loader = MigrationLoader(connection)\n        recorder = MigrationRecorder(connection)\n        self.assertEqual(loader.unapplied_migrations, {'migrations': '0001_initial'})\n        self.record_applied(recorder, 'migrations', '0001_initial')\n        self.assertEqual(loader.unapplied_migrations, set())\n\n        self.record_applied(recorder, 'migrations', '0002_second')\n        self.assertEqual(loader.unapplied_migrations, {'migrations': '0003_third'})\n", "    def test_missing_module(self):\n        \"\"\"\n        If a MIGRATION_MODULES override points to a missing module, the error\n        raised during the importation attempt should be propagated unless\n        `ignore_no_migrations=True`.\n        \"\"\"\n        with self.assertRaisesMessage(ImportError, \"No module named 'migrations.test_migrations_missing'\"):\n            MigrationLoader(connection)\n\n        with self.assertRaisesMessage(ImportError, \"No module named 'migrations.test_migrations_missing'\"):\n            MigrationLoader(connection, ignore_no_migrations=False)\n", "    def test_replace_migrations(self):\n        loader = MigrationLoader(connection)\n        self.assertEqual(loader.graph.forwards_plan(('migrations', '0002_second')), [\n            ('migrations', '0001_initial'),\n            ('migrations', '0003_third'),\n            ('migrations', '0002_second'),\n        ])\n\n"], "sample_84": ["    def test_relative_paths(self):\n        tests = (\n            ('/path/to/resource', '/path/to/resource'),\n            ('/path/to/resource/', '/path/to/resource/'),\n        )\n        for url, expected in tests:\n            with self.subTest(url=url):\n                self.assertEqual(escape_leading_slashes(url), expected)\n", "    def test_trailing_slashes(self):\n        tests = (\n            ('https://example.com/', 'https%3A%2F%2Fexample.com%2F'),\n            ('http://example.com//', 'http%3A%2F%2Fexample.com%2F%2F'),\n        )\n        for url, expected in tests:\n            with self.subTest(url=url):\n                self.assertEqual(escape_leading_slashes(url), expected)\n", "    def test_already_escaped(self):\n        self.assertEqual(escape_leading_slashes('/%2Fexample.com'), '/%2Fexample.com')\n", "    def test_no_change(self):\n        self.assertEqual(escape_leading_slashes('http://example.com'), 'http://example.com')\n        self.assertEqual(escape_leading_slashes('https://example.com/path'), 'https://example.com/path')\n\n", "    def test_no_change(self):\n        tests = (\n            ('http://example.com', 'http://example.com'),\n            ('https://example.com', 'https://example.com'),\n            ('/example', '/example'),\n            ('example.com', 'example.com'),\n        )\n        for url, expected in tests:\n            with self.subTest(url=url):\n                self.assertEqual(escape_leading_slashes(url), expected)\n\n"], "sample_694": ["def test_argument_type_str_choice_warning(pytester: Pytester) -> None:\n    with pytest.warns(\n        pytest.PytestRemovedIn8Warning,\n        match=\"For choices this is optional and can be omitted\",\n    ) as record:\n        pytester.makepyfile(\n            \"\"\"\n            import pytest\n\n                pass\n\n            @pytest.mark.parametrize(\"arg\", [None, 1, \"one\"])\n                pass\n\n                pass\n\n            \"\"\"\n        )\n        pytester.runpytest()\n    assert len(record) == 1\n\n", "def test_argument_type_str_choice_warning(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n            pass\n\n            pass\n\n            pass\n        \"\"\"\n    )\n    result = pytester.runpytest(\n        \"-Wdefault::pytest.PytestDeprecationWarning\"\n        \"-Wdefault::pytest.PytestRemovedIn8Warning\",\n    )\n    # Check for warnings on the test functions using implicit choice\n    result.stdout.fnmatch_lines(\n        [\n            \"*pytest.PytestRemovedIn8Warning: The `type` argument to addoption() is deprecated\",\n            \"* 'a'.*\"\n        ]\n    )\n    result.stdout.fnmatch_lines(\n        [\n            \"*pytest.PytestRemovedIn8Warning: The `type` argument to addoption() is deprecated\",\n            \"* `choice` argument to addoption() is optional and can be omitted,\"\n            \" but when supplied should be a type (for example `str` or `int`)\",\n            \"*'str'\"\n        ]\n    )\n    result.stdout.fnmatch_lines(\n        [\n            \"*pytest.PytestRemovedIn8Warning: The `type` argument to addoption() is deprecated\",\n            \"* `choice` argument to addoption() is optional and can be omitted,\"\n            \" but when supplied should be a type (for example `str` or `int`)\",\n            \"*'str'\"\n        ]\n    )\n\n    # Ensure no warnings for explicit type usage\n    result.assert_outcomes(passed=3)\n\n", "def test_argparse_type_str_choice_warning(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n            assert test_type == 'default'\n        \"\"\"\n    )\n    result = pytester.runpytest(\n        \"-Wdefault::pytest.PytestDeprecationWarning\",\n    )\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestRemovedIn8Warning:  `type` argument to addoption() is the string 'str', but when supplied should be a type (for example `str` or `int`).\",\n        ]\n    )\n    result.assert_outcomes(passed=1)\n\n\n\n\n", "def test_type_argument_to_addoption_is_deprecated(pytester: Pytester) -> None:\n    with pytest.warns(\n        pytest.PytestRemovedIn8Warning,\n        match=re.escape(\n            \"The `type` argument to addoption() is the string {typ!r}, \"\n            \"but when supplied should be a type (for example `str` or `int`). \"\n            \"(options: {names})\"\n        ),\n    ) as record:\n        from _pytest.config import addoption  # noqa: F401\n\n        addoption(\n            \"--deprecated-type\",\n            action=\"store\",\n            type=\"str\",\n            help=\"a deprecated type\",\n        )\n    assert len(record) == 1\n", "def test_argument_type_string_is_deprecated(pytester: Pytester) -> None:\n    with pytest.warns(\n        PytestRemovedIn8Warning,\n        match=\"`type` argument to addoption() is the string {typ!r}, \"\n        \"but when supplied should be a type (for example `str` or `int`)\",\n    ) as r:\n        pytester.makepyfile(\n            \"\"\"\n            import pytest\n\n                pass\n\n                pass\n\n            @pytest.mark.parametrize(\"t\", [\"hello\", 1, 2, 3])\n                type_arg = \"int\"\n                pytest.addoption(\n                    \"--t\",\n                    action=\"store\",\n                    type=type_arg,\n                    dest=\"value\",\n                )\n            \"\"\"\n        )\n        result = pytester.runpytest()\n    assert len(r) == 1\n"], "sample_345": ["    def test_snapshot_files_with_non_existing_files(self):\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file, self.nonexistent_file]):\n            snapshot = dict(self.reloader.snapshot_files())\n            self.assertEqual(len(snapshot), 1)\n            self.assertIn(self.existing_file, snapshot)\n            self.assertNotIn(self.nonexistent_file, snapshot)\n\n\n\n", "    def test_snapshot_files_with_different_paths_to_same_file(self):\n        file1 = self.ensure_file(self.tempdir / 'test.py')\n        file2 = self.ensure_file(self.tempdir / 'test_copy.py')\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[file1, file2]):\n            snapshot = list(self.reloader.snapshot_files())\n            self.assertEqual(len(snapshot), 2)\n            self.assertIn(file1, snapshot)\n            self.assertIn(file2, snapshot)\n", "    def test_snapshot_files_ignore_non_files(self):\n        non_file = self.tempdir / 'non_file'\n        non_file.touch()\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[non_file]):\n            snapshot = dict(self.reloader.snapshot_files())\n            self.assertNotIn(non_file, snapshot)\n\n", "    def test_snapshot_files_with_directories(self):\n        dir_path = self.tempdir / 'dir'\n        dir_path.mkdir(exist_ok=True)\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[dir_path]):\n            snapshot = list(self.reloader.snapshot_files())\n            self.assertCountEqual(snapshot, [])\n\n\n\n", "    def test_tick_handles_already_updated_files(self):\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file]):\n            snapshot = dict(self.reloader.snapshot_files())\n            self.increment_mtime(self.existing_file)\n            self.increment_mtime(self.existing_file)  # Duplicate mtime update\n            ticker = self.reloader.tick()\n            next(ticker)\n            next(ticker)  # Consume the initial notification\n            self.assertEqual(ticker.send(snapshot[self.existing_file]), None) \n\n\n"], "sample_928": ["def test_default_role():\n    with default_role('test.rst', 'myrole'):\n        pass  # assert role is registered\n", "def test_default_role():\n    with default_role('dummy.rst', 'myrole'):\n        assert docutils.role_find('myrole', english) is not None\n", "def test_default_role():\n    with default_role('my_doc', 'my_role'):\n        pass\n\n    with default_role('my_doc', 'nonexistent_role'):\n        pass\n", "def test_default_role():\n    with default_role(\"dummy.rst\", \"link\") as dummy:\n        pass\n\n    with default_role(\"dummy.rst\", \"nonexistent_role\"):\n        pass\n", "def test_default_role():\n    with default_role('testdoc.rst', 'myrole'):\n        role_fn, _ = roles.role('myrole', english, 0, Reporter('', 4, 4))\n        assert role_fn is not None\n"], "sample_1130": ["def test_point_vel_multiple_paths_no_warning():\n    t = dynamicsymbols._t\n    q1, q2, u1 = dynamicsymbols('q1 q2 u1')\n    B = ReferenceFrame('B')\n    O = Point('O')\n    P = Point('P')\n    P1 = Point('P1')\n    O.set_vel(B, u1 * B.x)\n    P.set_pos(O, q1 * B.y)\n    P1.set_pos(P, q2 * B.z)\n    with warnings.catch_warnings(): #There are no possible paths in this point tree, thus no warning is raised\n        warnings.simplefilter(\"error\")\n        assert P1.vel(B) == q2.diff(t) * B.z + u1 * B.y\n", "def test_auto_point_vel_no_pos_defined():\n    t = dynamicsymbols._t\n    q1, q2 = dynamicsymbols('q1 q2')\n    N = ReferenceFrame('N')\n    P = Point('P')\n    raises(ValueError, lambda: P.vel(N)) \n    P1 = Point('P1')\n    raises(ValueError, lambda: P1.vel(N))\n\n", "def test_auto_vel_multiple_paths_same_vel():\n    t = dynamicsymbols._t\n    q1, q2, u1, u2 = dynamicsymbols('q1 q2 u1 u2')\n    B = ReferenceFrame('B')\n    P = Point('P')\n    P.set_vel(B, u1 * B.x)\n    P1 = Point('P1')\n    P1.set_pos(P, q2 * B.y)\n    P1.set_vel(B, u2 * B.z)\n    P2 = Point('P2')\n    P2.set_pos(P1, q1 * B.z)\n    P3 = Point('P3')\n    P3.set_pos(P2, 10 * q1 * B.y)\n    O = Point('O')\n    O.set_vel(B, u2 * B.y)\n    O1 = Point('O1')\n    O1.set_pos(O, q2 * B.z)\n    P4 = Point('P4')\n    P4.set_pos(O1, q1 * B.x + q2 * B.z)\n    assert P4.vel(B) == u2 * B.y + 2 * q2.diff(t) * B.z + q1.diff(t) * B.x\n\n\n\n", "def test_point_vel_multiple_frames():\n    N = ReferenceFrame('N')\n    B = ReferenceFrame('B')\n    O = Point('O')\n    P = Point('P')\n    P.set_pos(O, N.x)\n    O.set_vel(N, N.y)\n    P.set_vel(B, 2 * B.y)\n    assert P.vel(N) == 2 * B.y - O.vel(N)\n\n\n\n", "def test_point_pos_with_dependent_coordinates():\n    q1, q2 = dynamicsymbols('q1 q2')\n    N = ReferenceFrame('N')\n    B = N.orientnew('B', 'Axis', [q1, N.z])\n    O = Point('O')\n    P = O.locatenew('P', q2 * B.x)\n    assert P.pos_from(O) == q2 * B.x\n    assert P.pos_from(O).express(N) == q2 * B.x.express(N)\n"], "sample_189": ["    def test_cache_key_with_user(self):\n        request = self.factory.get(self.path)\n        request.user = User(id=1)\n        template = engines['django'].from_string(\"This is a test\")\n        response = TemplateResponse(request, template)\n\n        # Expect None if no headers have been set yet.\n        self.assertIsNone(get_cache_key(request))\n        # Set headers to an empty list.\n        learn_cache_key(request, response)\n        # The user is taken into account.\n        self.assertEqual(\n            get_cache_key(request),\n            'views.decorators.cache.cache_page.settingsprefix.GET.user1.'\n            'd41d8cd98f00b204e9800998ecf8427e'\n        )\n", "    def test_get_cache_key_with_content_type(self):\n        request = self.factory.get(self.path)\n        request.META['CONTENT_TYPE'] = 'application/json'\n        template = engines['django'].from_string(\"This is a test\")\n        response = TemplateResponse(HttpRequest(), template)\n        # Expect None if no headers have been set yet.\n        self.assertIsNone(get_cache_key(request))\n        # Set headers to an empty list.\n        learn_cache_key(request, response)\n        # The Content-Type is taken into account.\n        self.assertEqual(\n            get_cache_key(request), \n            'views.decorators.cache.cache_page.settingsprefix.GET.'\n            'application/json.d41d8cd98f00b204e9800998ecf8427e'\n        )\n", "    def test_cache_header_vary(self):\n        request = self.factory.get(self.path)\n        template = engines['django'].from_string(\"This is a test\")\n        response = TemplateResponse(request, template)\n        \n        # Default Vary header is None\n        response['Vary'] = None\n        key = get_cache_key(request)\n        self.assertEqual(key, 'views.decorators.cache.cache_page.settingsprefix.GET.d41d8cd98f00b204e9800998ecf8427e')\n        \n        # Vary header is set\n        response['Vary'] = 'Accept-Encoding'\n        key = get_cache_key(request)\n        self.assertEqual(key, 'views.decorators.cache.cache_page.settingsprefix.GET.Accept-Encoding.d41d8cd98f00b204e9800998ecf8427e')\n", "    def test_cache_control_no_store(self):\n        \"\"\"\n        When a response has Cache-Control: no-store, the cache should not be used.\n        \"\"\"\n        view = cache_page(3)(cache_control(no_store=True)(hello_world_view))\n        request = self.factory.get('/view/')\n\n        response = view(request, '1')\n        self.assertEqual(response.content, b'Hello World 1')\n        time.sleep(1)  # Wait a little for cache to expire\n        response = view(request, '2')\n        self.assertEqual(response.content, b'Hello World 2')\n", "    def test_cached_response_with_custom_headers(self):\n        \"\"\"\n        Ensure that custom headers from the response are applied to the cached object.\n        \"\"\"\n        custom_headers = {\"X-Custom-Header\": \"test\"}\n        view = mock.Mock(return_value=HttpResponse(\"This is a cached response\", headers=custom_headers))\n        cache_middleware = CacheMiddleware(view)\n        request = self.factory.get(self.path)\n\n        response = cache_middleware.process_request(request)\n        self.assertIsNotNone(response)\n\n        # Assert custom headers are present in the cached Response object\n        self.assertEqual(response.headers[\"X-Custom-Header\"], \"test\")\n\n"], "sample_63": ["    def test_invalid_template_loader(self):\n        with self.assertRaises(ImproperlyConfigured):\n            Engine(loaders=['invalid_loader'])\n", "    def test_template_libraries(self):\n        library_name = 'my_library'\n        library_path = 'path.to.my.library'\n        engine = Engine(libraries={library_name: library_path})\n\n        # Check if the library was loaded successfully\n        self.assertIn(library_name, engine.template_libraries)\n        self.assertIsNotNone(engine.template_libraries[library_name])\n\n        # Try using a library function in a template\n        template = engine.get_template('test_library.html')\n        self.assertEqual(template.render(Context()), f'Library: {library_name}')\n", "    def test_invalid_template_loader(self):\n        with self.assertRaises(ImproperlyConfigured):\n            Engine(loaders=['invalid_loader'])\n", "    def test_template_path_resolution(self):\n        engine = Engine(dirs=[TEMPLATE_DIR])\n        # Test template resolution from a relative path\n        template = engine.get_template('nested/subfolder/index.html')\n        self.assertEqual(template.origin.template_name, 'nested/subfolder/index.html')\n", "    def test_find_template_with_skip(self):\n        engine = Engine(dirs=[TEMPLATE_DIR])\n        # Create a template 'base.html' that includes 'include.html'\n        with open(os.path.join(TEMPLATE_DIR, 'base.html'), 'w') as f:\n            f.write('{% include \"include.html\" %}')\n        # Create a template 'include.html'\n        with open(os.path.join(TEMPLATE_DIR, 'include.html'), 'w') as f:\n            f.write('This is an included template.')\n\n        # Attempt to find 'base.html', skipping 'include.html'\n        with self.assertRaises(TemplateDoesNotExist):\n            engine.find_template('base.html', skip=['include.html'])\n"], "sample_723": ["def test_imputation_with_nans_in_statistics():\n    # Test imputation with NaN values in the statistics_\n    X = np.array([[1, 2, np.nan],\n                 [3, np.nan, 5],\n                 [np.nan, 7, 8]])\n\n    # Test with strategy=\"mean\"\n    imputer = SimpleImputer(missing_values=0, strategy=\"mean\")\n    imputer.fit(X)\n\n    # Check that the statistics_ contains NaNs\n    assert np.isnan(imputer.statistics_[1]).any()\n\n    # Check that the imputation is done correctly\n    X_imputed = imputer.transform(X.copy())\n    assert_array_almost_equal(X_imputed,\n                             np.array([[1, 2, 6.66666667],\n                                       [3, 4.5, 5],\n                                       [4.5, 7, 8]])\n                             )\n\n    # Test with strategy=\"median\"\n    imputer = SimpleImputer(missing_values=0, strategy=\"median\")\n    imputer.fit(X)\n\n    # Check that the statistics_ contains NaNs\n    assert np.isnan(imputer.statistics_[0]).any()\n\n    # Check that the imputation is done correctly\n    X_imputed = imputer.transform(X.copy())\n    assert_array_almost_equal(X_imputed,\n                             np.array([[1, 2, 5],\n                                       [3, 4.5, 5],\n                                       [2.5, 7, 8]])\n                             )\n", "def test_imputation_sparse_handling():\n    # Test handling of sparse matrices with different missing_values\n    # strategies.\n    for strategy in [\"mean\", \"median\", \"most_frequent\"]:\n\n        # Test case with a sparse matrix where missing values are not zeros\n        X = sparse.csr_matrix([\n            [1, 2, np.nan],\n            [4, np.nan, 6],\n            [7, 8, 9]\n        ], dtype=float)\n        missing_values = 10\n\n        imputer = SimpleImputer(missing_values=missing_values,\n                                strategy=strategy,\n                                copy=False)\n\n        imputer.fit(X)\n\n        X_ imputed = imputer.transform(X.copy())\n        assert_array_almost_equal(X_imputed.data,\n                                  imputer.statistics_,\n                                  err_msg=f\"Strategy: {strategy}\")\n\n\n\n        # Test case with a sparse matrix with zeros as missing values\n        X = sparse.csr_matrix([\n            [1, 2, 0],\n            [4, 0, 6],\n            [7, 8, 9]\n        ], dtype=float)\n        missing_values = 0\n\n        imputer = SimpleImputer(missing_values=missing_values,\n                                strategy=strategy,\n                                copy=False)\n\n        imputer.fit(X)\n        X_imputed = imputer.transform(X.copy())\n\n        assert_array_almost_equal(X_imputed.data,\n                                  imputer.statistics_,\n                                  err_msg=f\"Strategy: {strategy}\")\n\n", "def test_imputation_sparse_matrix_axis_1():\n    # Test imputation using the mean strategy on sparse matrix with axis=1\n    X = sparse.csc_matrix(np.random.rand(10, 10))\n    X[:, 5:7] = 0\n    statistics = np.nan\n\n    imputer = SimpleImputer(missing_values=0, strategy='mean', axis=1)\n    imputer.fit(X)\n    assert statistics == np.nan\n\n    X_transformed = imputer.transform(X)\n    assert_array_almost_equal(X_transformed[:, 5:7].toarray(),\n                              statistics)\n\n\n\n", "def test_imputation_errors():\n    X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n\n    # Check for errors when trying to impute with invalid strategies\n    for invalid_strategy in [\"invalid_strategy\", 1, 2.5]:\n        imputer = SimpleImputer(missing_values=0, strategy=invalid_strategy)\n        assert_raises(ValueError, imputer.fit, X)\n\n    # Check for errors when missing_values is not a number\n    imputer = SimpleImputer(missing_values=[\"invalid\"], strategy=\"mean\")\n    assert_raises(ValueError, imputer.fit, X)\n\n    # Check for errors when axis is not 0 or 1\n    imputer = SimpleImputer(missing_values=0, strategy=\"mean\", axis=2)\n    assert_raises(ValueError, imputer.fit, X)\n", "def test_imputation_errors():\n    # Test for various errors.\n    X = np.array([[1, 2, 3],\n                  [4, 5, np.nan],\n                  [7, 8, np.nan]])\n\n    # Test missing_values as non-numeric\n    imputer = SimpleImputer(missing_values=\"abc\", strategy=\"mean\")\n    assert_raises(TypeError, imputer.fit, X)\n\n    # Test invalid strategy\n    imputer = SimpleImputer(missing_values=0, strategy=\"invalid\")\n    assert_raises(ValueError, imputer.fit, X)\n\n    # Test invalid axis\n    imputer = SimpleImputer(missing_values=0, strategy=\"mean\", axis=3)\n    assert_raises(ValueError, imputer.fit, X)\n\n\n"], "sample_815": ["def test_multilabel_jaccard_similarity_score_binary_labels():\n    # Test with binary labels\n    y1 = np.array([[0, 1], [0, 1]])\n    y2 = np.array([[0, 1], [1, 0]])\n    assert_equal(multilabel_jaccard_similarity_score(y1, y2), 0.5)\n", "compilation error", "def test_brier_score_loss_multiclass():\n    y_true = np.array([0, 1, 2, 0])\n    y_pred = np.array([0.8, 0.6, 0.4, 0.2])\n    error_message = (\"Only binary classification is supported. \"\n                     \"Labels in y_true: {}\".format(np.array([0, 1, 2])))\n    assert_raise_message(ValueError, error_message, brier_score_loss,\n                         y_true, y_pred)\n", "def test_adjusted_balanced_accuracy_score():\n    y_true = np.array([0, 0, 1, 1, 0, 1])\n    y_pred = np.array([0.1, 0.8, 0.9, 0.3, 1., 0.95])\n\n    # Calculate ABAC\n    abac = balanced_accuracy_score(y_true, y_pred, adjusted=True)\n\n    # Calculate expected ABAC - note this is a rough estimate due to rounding\n    # in the underlying functions\n    macro_prec = precision_score(y_true, y_pred, average='macro')\n    macro_rec = recall_score(y_true, y_pred, average='macro')\n    expected_abac = (macro_prec + macro_rec) / 2\n    assert_almost_equal(abac, expected_abac, decimal=4)\n\n\n\n    assert_almost_equal(balanced_accuracy_score(y_true, y_pred, adjusted=True),\n                        balanced_accuracy_score(y_true, y_pred),\n                        decimal=4)\n", "compilation error"], "sample_956": ["    def test_get_safe_url_port(self):\n        \"\"\"_get_safe_url() with a url with basic auth having port\"\"\"\n        url = 'https://user:12345@domain.com:8080/project/objects.inv'\n        expected = 'https://user@domain.com:8080/project/objects.inv'\n        actual = _get_safe_url(url)\n        assert expected == actual\n", "def test_inventory_with_redirect(tempdir, app, status, warning):\n    inv_file = tempdir / 'inventory'\n    inv_file.write_bytes(\n        b'# Sphinx inventory version 2\\n'\n        b'http://docs.example.com/new_location/objects.inv'\n    )\n    set_config(app, {\n        'https://docs.python.org/': inv_file,\n    })\n\n    # load the inventory and check if it's done correctly\n    normalize_intersphinx_mapping(app, app.config)\n    load_mappings(app)\n\n    rn = reference_check(app, 'py', 'mod', 'module1', 'foo')\n    assert isinstance(rn, nodes.reference)\n    assert rn['refuri'] == 'http://docs.example.com/new_location/foo.html#module-module1'\n    assert rn['reftitle'] == '(in foo)'\n    assert rn[0].astext() == 'module1'\n", "    def test_getsafeurl_invalid(self, url):\n        \"\"\"_get_safe_url() handles invalid URLs gracefully.\"\"\"\n        with pytest.raises(ValueError):\n            _get_safe_url(url)\n\n        \n", "    def test_getsafe_url_various_formats(self, url):\n        \"\"\"_get_safe_url() handles various URL formats\"\"\"\n        expected = url.split('?')[0]\n        actual = _get_safe_url(url)\n        assert expected == actual\n", "def test_missing_reference_version_mismatch(tempdir, app, status, warning):\n    inv_file = tempdir / 'inventory'\n    inv_file.write_bytes(inventory_v2)\n    inv_file_v3 = tempdir / 'inventory_v3'\n    inv_file_v3.write_bytes(inventory_v3)\n    set_config(app, {\n        'https://docs.python.org/': inv_file,\n    })\n\n    # load the inventory and check if it's done correctly\n    normalize_intersphinx_mapping(app, app.config)\n    load_mappings(app)\n\n    # check if it resolves correctly\n    node, contnode = fake_node('py', 'func', 'module1.func', 'module1.func',\n                               )\n    rn = missing_reference(app, app.env, node, contnode)\n    assert isinstance(rn, nodes.reference)\n    assert rn['refuri'] == 'https://docs.python.org/foo.html#module-module-v2'\n    assert rn['reftitle'] == '(in foo v2.0)'\n\n    # check if a warning is issued for other versioned inventory\n    rn = missing_reference(app, app.env, node, contnode, refdoc='v3/index.html')\n    assert rn is None\n    assert warning.getvalue() == \"Inventory version mismatch detected for ref: module1.func\\n\"\n\n\n\n"], "sample_5": ["def test_models_bounding_box_magunits(model):\n    if not HAS_SCIPY and model['class'] in SCIPY_MODELS:\n        pytest.skip()\n\n    m = model['class'](**model['parameters'])\n\n    # In the following we need to explicitly test that the value is False\n    # since Quantities no longer evaluate as as True\n    if model['bounding_box'] is False:\n        # Check that NotImplementedError is raised, so that if bounding_box is\n        # implemented we remember to set bounding_box=True in the list of models\n        # above\n        with pytest.raises(NotImplementedError):\n            m.bounding_box\n\n\n\n", "compilation error", "def test_models_evaluate_magunits_different_systems_from_params(model):\n    if not HAS_SCIPY and model['class'] in SCIPY_MODELS:\n        pytest.skip()\n\n    m = model['class'](**model['parameters'])\n    for args in model['evaluation']:\n        if isinstance(args[0], u.Quantity):\n            args[0] = args[0].to(u.Jy)\n        assert_quantity_allclose(m(*args[:-1]), args[-1]) \n", "def test_models_bounding_box_magunits(model):\n    if not HAS_SCIPY and model['class'] in SCIPY_MODELS:\n        pytest.skip()\n\n    m = model['class'](**model['parameters'])\n    if model['bounding_box'] is not None:\n        for args in model['evaluation']:\n            bbox = m.bounding_box\n            assert_quantity_allclose(bbox, model['bounding_box'])\n\n\n\n", "compilation error"], "sample_975": ["def test_issue_11768():\n    from sympy.solvers.solvers import nsolve\n    from sympy import Symbol, sin\n    x = Symbol('x')\n    \n    # Test case from issue 11768\n    solution = nsolve(sin(x) - x, x, 0.5)\n    assert solution > 0 and solution < 1\n", "def test_nsolve_complex():\n    x = Symbol('x')\n    sol = nsolve(sin(x) - x, x, 1, complexo=True)\n    assert abs(sol.imag) > 0 \n", "def test_issue_11768():\n    # Issue 11768: handle cases where simplification leads to missing roots\n    import mpmath\n    mpmath.mp.dps = 128\n    x = Symbol('x')\n    eq = Eq(x**6 - 1, 0)\n    sol = nsolve(eq, x, 0)\n    assert abs(1.0 - sol[0]) < 1e-128\n", "def test_issue_11768():\n    # Issue 11768: Test where using the numerator in nsolve fails\n    x = Symbol('x')\n    eq = (x**2 + 1) / (x**2 + x + 1) - 1\n    sol = nsolve(eq, x, 0.1)\n    assert abs(sol - 0.41421356237309507) < 1e-10\n\n    # Test where using the numerator in nsolve succeeds\n    eq = (x**2 + 1) / (x**2 + x + 1) - 100\n    sol = nsolve(eq, x, 0.1) \n    assert abs(sol - 0.0001289472719) < 1e-10 \n", "def test_issue_11768():\n    from sympy.solvers import nsolve\n    x = symbols('x')\n    eq = (x**2/(1 - x))**2 - 10\n    sol = nsolve(eq, x, 0.5)\n    assert sol < 1.\n\n\n\n"], "sample_368": ["    def test_backwards_partial_rollbacks_single_dependency(self):\n        \"\"\"\n        Partial rollbacks work when a migration depends on a partially\n        applied branch.\n\n        a: 1 <--- 2\n        b:    \\- 1 <--- 3\n        c:     \\- 1\n        \"\"\"\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        a2_impl = FakeMigration('a2')\n        a2 = ('a', '2')\n        b1_impl = FakeMigration('b1')\n        b1 = ('b', '1')\n        b3_impl = FakeMigration('b3')\n        b3 = ('b', '3')\n        c1_impl = FakeMigration('c1')\n        c1 = ('c', '1')\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(a2, a2_impl)\n        graph.add_node(b1, b1_impl)\n        graph.add_node(b3, b3_impl)\n        graph.add_node(c1, c1_impl)\n        graph.add_dependency(None, a2, a1)\n        graph.add_dependency(None, b1, a1)\n        graph.add_dependency(None, b3, b1)\n        graph.add_dependency(None, c1, a1)\n\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {\n            a1: a1_impl,\n            b1: b1_impl,\n            c1: c1_impl,\n        })\n\n        plan = executor.migration_plan({\n            a1: a1_impl,\n            b1: b1_impl,\n            c1: c1_impl,\n        }, target_state={a1: a1_impl})\n\n        self.assertEqual(plan, [(b3_impl, True)])\n\n\n", "    def test_apply_multiple_migrations_in_order(self):\n        # Test applies migrations in order and records them correctly.\n        p1_impl = FakeMigration('p1')\n        p2_impl = FakeMigration('p2')\n        p3_impl = FakeMigration('p3')\n        p1 = ('p', '1')\n        p2 = ('p', '2')\n        p3 = ('p', '3')\n        graph = MigrationGraph()\n        graph.add_node(p1, p1_impl)\n        graph.add_node(p2, p2_impl)\n        graph.add_node(p3, p3_impl)\n        graph.add_dependency(None, p2, p1)\n        graph.add_dependency(None, p3, p2)\n\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {})\n\n        executor.apply_migration(\n            ProjectState(), p1_impl,\n        )\n        executor.apply_migration(\n            ProjectState(), p2_impl,\n        )\n        executor.apply_migration(\n            ProjectState(), p3_impl,\n        )\n\n        self.assertIn(p1, executor.loader.applied_migrations)\n        self.assertIn(p2, executor.loader.applied_migrations)\n        self.assertIn(p3, executor.loader.applied_migrations)\n\n\n\n", "    def test_minimize_rollbacks_circularity(self):\n        \"\"\"\n        Minimize rollbacks when circular dependency is detected.\n\n        a: 1 <--- 2\n        b:    \\- 1\n        c:     \\- 1\n\n        If a1 is applied already and a2 is not,\n        and we're asked to migrate to a1, don't apply or unapply b1 or c1,\n        regardless of their current state.\n        \"\"\"\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        a2_impl = FakeMigration('a2')\n        a2 = ('a', '2')\n        b1_impl = FakeMigration('b1')\n        b1 = ('b', '1')\n        c1_impl = FakeMigration('c1')\n        c1 = ('c', '1')\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(a2, a2_impl)\n        graph.add_node(b1, b1_impl)\n        graph.add_node(c1, c1_impl)\n        graph.add_dependency(None, a2, a1)\n        graph.add_dependency(None, b1, a1)\n        graph.add_dependency(None, c1, a1)\n        graph.add_dependency(None, a1, b1)  # Circular dependency\n\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {\n            a1: a1_impl,\n            b1: b1_impl,\n        })\n\n        plan = executor.migration_plan({a1})\n\n        self.assertEqual(plan, [])\n", "    def test_migration_plan_with_missing_app(self, mock_get_app_config):\n        \"\"\"\n        Migration plan should not include migrations for missing apps.\n        \"\"\"\n        mock_get_app_config.return_value = None\n\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        b1_impl = FakeMigration('b1')\n        b1 = ('b', '1')\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(b1, b1_impl)\n        graph.add_dependency(None, b1, a1)\n\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {})\n\n        plan = executor.migration_plan({a1})\n\n        self.assertEqual(plan, [(a1_impl, True)])\n\n", "    def test_minimize_rollbacks_already_applied(self):\n        \"\"\"\n        Minimize rollbacks when migrations are already applied.\n\n        a: 1 <---- 2\n        b:    \\- 1\n        c:     \\- 1\n        When both a1 and b1 are already applied, only a2 is applied\n        \"\"\"\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        a2_impl = FakeMigration('a2')\n        a2 = ('a', '2')\n        b1_impl = FakeMigration('b1')\n        b1 = ('b', '1')\n        c1_impl = FakeMigration('c1')\n        c1 = ('c', '1')\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(a2, a2_impl)\n        graph.add_node(b1, b1_impl)\n        graph.add_node(c1, c1_impl)\n        graph.add_dependency(None, a2, a1)\n        graph.add_dependency(None, b1, a1)\n        graph.add_dependency(None, c1, a1)\n\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {\n            a1: a1_impl,\n            b1: b1_impl,\n        })\n\n        plan = executor.migration_plan({a1, a2})\n\n        self.assertEqual(plan, [(a2_impl, False)])\n"], "sample_1057": ["def test_module_imports():\n    ast = Print('x y'.split(), \"coordinate: %12.5g %12.5g\")\n    printer = PythonCodePrinter({'standard': 'python3'})\n    printer.module_imports = {'sympy': ['Print']}\n    assert render_as_module(ast, standard='python3') == \\\n        'import sympy\\n\\nprint(\"coordinate: %12.5g %12.5g\" % (x, y))' \n", "def test_multiple_imports():\n    ast = Print('x y'.split(), \"coordinate: %12.5g %12.5g\")\n    printer = PythonCodePrinter({'standard': 'python3'})\n    printer.module_imports = {'numpy': ['sin', 'cos'], 'math': ['pi']}\n    pystr = printer.doprint(ast)\n    expected_imports = \"\"\"", "compilation error", "compilation error", "compilation error"], "sample_929": ["def test_pyexception_signature_with_args(app):\n    text = (\".. py:exception:: MyError\\n\"\n            \"   :arg: message: str\\n\"\n            \"   :arg: code: int\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"exception \"],\n                                                    [desc_name, \"MyError\"],\n                                                    [desc_parameterlist, ([desc_parameter, \"message\"],\n                                                    [desc_parameter, \"code\"])],\n                                                    [desc_content, ()])],\n                                  [desc_content, ()])]))\n", "def test_py_exception_signature_with_bases(app):\n    text = (\".. py:exception:: MyException\\n\"\n            \"   :base: Exception\\n\"\n            \"   :base: RuntimeError\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"MyException\"],\n                                                    [desc_annotation, (\": \",\n                                                                        [pending_xref, \"Exception\"],\n                                                        [desc_sig_punctuation, \",\"],\n                                                        [pending_xref, \"RuntimeError\"])])],\n                                  desc_content)]))\n    assert_node(doctree[1][0][1][0][1][1], pending_xref, **{\"py:class\": \"Exception\"})\n    assert_node(doctree[1][0][1][0][1][3], pending_xref, **{\"py:class\": \"RuntimeError\"})\n    assert 'MyException' in domain.objects\n    assert domain.objects['MyException'] == ('index', 'MyException', 'exception')\n", "compilation error", "def test_pyfunction_signature_varargs_kwargs(app):\n    text = (\".. py:function:: my_func\\n\"\n            \"   :varargs:\\n\"\n            \"   :kwargs:\\n\"\n            \"\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"my_func\"],\n                                [desc_parameterlist, ([desc_parameter, \"*args\"],\n                                [desc_parameter, \"**kwargs\"])])],\n                                  desc_content)]))\n    assert_node(doctree[1][0][1][0], desc_parameter, [desc_sig_name, \"*args\"])\n    assert_node(doctree[1][0][1][1], desc_parameter, [desc_sig_name, \"**kwargs\"])\n\n\n\n", "def test_pyexception_signature_with_args(app):\n    text = \".. py:exception:: MyException\\n\"\n    text += \"  :args:\\n\"\n    text += \"    arg1\\n\"\n    text += \"    arg2\\n\"\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"exception \"],\n                                                    [desc_name, \"MyException\"],\n                                                    [desc_parameterlist, ([desc_parameter, \"arg1\"],\n                                                    [desc_parameter, \"arg2\"])],\n                                                    [])]),\n                                  [desc_content, ()])]))\n\n\n\n"], "sample_1062": ["compilation error", "compilation error", "compilation error", "def test_TR18():\n    assert TR18(cos(2*x)) == cos(2*x)\n    assert TR18(sin(2*x)) == sin(2*x)\n    assert TR18(cos(x)**2) == cos(2*x)/2 + 1/2\n    assert TR18(sin(x)**2) == -cos(2*x)/2 + 1/2\n    assert TR18(tan(x)**2) == sec(2*x) - 1\n    assert TR18(cot(x)**2) == csc(2*x) - 1\n\n    eq = cos(2*x)**2*sin(2*x)\n    assert TR18(eq) == eq\n    eq = sin(2*x)**2*cos(2*x)\n    assert TR18(eq) == eq\n    eq = cos(2*x)*sin(2*x)**2\n    assert TR18(eq) == eq\n\n    eq = (cos(x)**2 - sin(x)**2)**2\n    assert TR18(eq) == cos(4*x)\n\n    eq = (cos(x)**2 + sin(x)**2)**2\n    assert TR18(eq) == 1\n\n    eq = (sin(x)**2 - cos(x)**2)**2\n    assert TR18(eq) == cos(4*x)\n    \n    eq = cos(2*x)*(sin(2*x)**2 + cos(2*x)**2)\n    assert TR18(eq) == eq\n\n", "def test_TR18():\n    assert TR18(cos(x)*sin(y)) == (cos(x)*sin(y), None, None)\n    assert TR18(cos(x) * sin(y) + cos(y) * sin(x)) == \\\n        (cos(x)*sin(y) + cos(y)*sin(x), None, None)\n    assert TR18(cos(x) * cos(y)) == (cos(x)*cos(y), None, None)\n    assert TR18(sin(x) * sin(y)) == (sin(x)*sin(y), None, None)\n    assert TR18(cos(x)**2 - sin(x)**2) == (cos(x)**2 - sin(x)**2, None, None)\n    assert TR18(cos(x)**2 + sin(x)**2) == (cos(x)**2 + sin(x)**2, None, None)\n    assert TR18(cos(x) / sin(x)) == (cos(x) / sin(x), None, None)\n    assert TR18(tan(x)) == (tan(x), None, None)\n    assert TR18(cot(x)) == (cot(x), None, None)\n    assert TR18(sec(x)) == (sec(x), None, None)\n    assert TR18(csc(x)) == (csc(x), None, None)\n    assert TR18(sin(x)**2 + cos(x)**2) == (sin(x)**2 + cos(x)**2, None, None)\n    assert TR18(sin(x) * cos(x)) == (sin(x)*cos(x), None, None)\n\n\n\n"], "sample_548": ["def test_colorbar_set_ticks():\n    fig, ax = plt.subplots()\n    pc = ax.pcolormesh(np.random.randn(10, 10))\n    cb = fig.colorbar(pc)\n\n    # check that we can set ticks explicitly\n    cb.set_ticks(np.linspace(0, 1, 5))\n    assert np.allclose(cb.ax.get_yticks(), np.linspace(0, 1, 5))\n\n    # check that setting ticks modifies the colorbar's minor ticks too\n\n    cb.set_ticks(np.linspace(0, 1, 4))\n    ticks = cb.ax.get_yticks()\n    assert len(ticks) == 4\n\n    minor_ticks = cb.ax.yaxis.get_ticklocs()\n    assert len(minor_ticks) == 8\n    assert np.allclose(minor_ticks, np.linspace(0, 1, 12))\n\n\n", "def test_colorbar_orientation_from_axes():\n    fig, axs = plt.subplots(1, 2, figsize=(6, 2))\n    im = axs[0].imshow([[0, 1], [2, 3]])\n    cb = fig.colorbar(im, ax=axs[0], orientation=None)\n    assert cb.ax.yaxis.is_horizontal()\n    im = axs[1].imshow([[0, 1], [2, 3]], orientation='landscape')\n    cb = fig.colorbar(im, ax=axs[1], orientation=None)\n    assert not cb.ax.yaxis.is_horizontal() \n\n\n", "compilation error", "def test_colorbar_padding():\n    fig, ax = plt.subplots()\n    im = ax.imshow([[0, 1], [2, 3]])\n    cb = fig.colorbar(im, ax=ax, pad=0.05)\n    #make sure padding is applied\n    assert cb.ax.get_position().y1 - cb.ax.yaxis.tick_bottom > 0.05\n    assert cb.ax.get_position().x0 + cb.ax.xaxis.tick_left < 1.05 \n\n\n\n", "compilation error"], "sample_471": ["    def test_integerfield_input_formats(self):\n        f = IntegerField(input_formats=['%d', '%s'])\n        self.assertEqual(1, f.clean(\"1\"))\n        self.assertEqual(1, f.clean(\"   1   \"))\n        with self.assertRaisesMessage(ValidationError, \"'Enter a whole number.'\"):\n            f.clean(\"1.5\")\n        with self.assertRaisesMessage(ValidationError, \"'Enter a whole number.'\"):\n            f.clean(\"a\")\n\n", "    def test_integerfield_null(self):\n        f = IntegerField()\n        self.assertIsNone(f.clean(None))\n        self.assertEqual(\"None\", repr(f.clean(None)))\n        with self.assertRaisesMessage(\n            ValidationError, \"'This field is required.'\"\n        ):\n            f.clean(\"\")\n", "    def test_integerfield_invalid_input(self):\n        f = IntegerField()\n        with self.assertRaisesMessage(ValidationError, \"'Enter a whole number.'\"):\n            f.clean(\"abc\")\n        with self.assertRaisesMessage(ValidationError, \"'Enter a whole number.'\"):\n            f.clean(\"1.5\")\n        with self.assertRaisesMessage(ValidationError, \"'Enter a whole number.'\"):\n            f.clean(\"   \")\n        with self.assertRaisesMessage(ValidationError, \"'Enter a whole number.'\"):\n            f.clean(\"1\\n\")\n\n\n\n", "    def test_integerfield_empty(self):\n        f = IntegerField()\n        self.assertEqual(None, f.clean(\"\"))\n        self.assertEqual(None, f.clean(None))\n\n\n\n", "    def test_integerfield_empty_values(self):\n        f = IntegerField()\n        with self.assertRaisesMessage(ValidationError, \"'This field is required.'\"):\n            f.clean(\"\")\n        with self.assertRaisesMessage(ValidationError, \"'This field is required.'\"):\n            f.clean(None)\n        self.assertIsNone(f.clean(None))\n\n"], "sample_477": ["    def test_random_dict(self):\n        output = self.engine.render_to_string( \"random_dict\", {\"dict\": {\"a\": 1, \"b\": 2}})\n        self.assertIn(\"a\", output)\n        self.assertIn(\"b\", output)\n", "    def test_random_non_string(self):\n        output = self.engine.render_to_string(\n            \"random_non_string\", {\"a\": 123}\n        )\n        self.assertEqual(output, \"123\")\n", "    def test_random_dict(self):\n        output = self.engine.render_to_string(\n            \"random_dict\", {\"dict\": [\"a&b\", \"a&b\"]}\n        )\n        self.assertEqual(output, \"a&b\" or \"a&b\") \n\n", "    def test_random_list(self):\n        output = self.engine.render_to_string(\"random_list\")\n        self.assertIn(str(1), output)\n        self.assertIn(str(2), output)\n        self.assertIn(str(3), output)\n", "    def test_random_list2(self):\n        output = self.engine.render_to_string(\"random_list2\", {\"list\": [\"a\", \"b\", \"c\"]})\n        self.assertIn(\"a\", output)\n        self.assertIn(\"b\", output)\n        self.assertIn(\"c\", output)  \n"], "sample_510": ["def test_subplot_add_axes():\n    fig = plt.figure()\n    ax1 = fig.add_subplot(2, 2, 1)\n    ax2 = fig.add_subplot(2, 2, 2)\n    ax3 = fig.add_subplot(2, 2, 3)\n    ax4 = fig.add_subplot(2, 2, 4)\n\n    assert len(fig.axes) == 4\n    assert ax1 is fig.axes[0]\n    assert ax2 is fig.axes[1]\n    assert ax3 is fig.axes[2]\n    assert ax4 is fig.axes[3]\n\n\n\n", "compilation error", "def test_sca_on_nonexistent_axes():\n    with pytest.raises(ValueError,\n                       match=\"Could not find axes to set current\"):\n        plt.sca('nonexistent_axes')\n", "def test_subplot_tight_layout():\n    fig, axes = plt.subplots(2, 2)\n    fig.tight_layout()\n    \n    # Add some data to the axes to force a layout\n    axes[0, 0].plot([1, 2, 3])\n    axes[1, 1].scatter([1, 2, 3], [4, 5, 6])\n\n    # Check that the layout is correct\n    bbox_artist = fig.get_children()[0]\n    for ax in axes.flatten():\n        assert ax.get_position() != (0, 0, 1, 1)  # Avoids layout issues\n    fig.canvas.draw_idle() \n\n", "def test_title_and_label_positioning():\n    fig, ax = plt.subplots()\n    ax.set_title(\"Figure Title\")\n    ax.set_xlabel(\"X-axis Label\")\n    ax.set_ylabel(\"Y-axis Label\")\n\n    # Check initial positions\n    title_bbox = ax.get_title_bbox()\n    x_label_bbox = ax.get_xlabel_bbox()\n    y_label_bbox = ax.get_ylabel_bbox()\n\n    # Adjust tick locations and check bounding box changes\n    ax.set_xticks([1, 2, 3])\n    ax.set_yticks([1, 2, 3])\n    title_bbox_new = ax.get_title_bbox()\n    x_label_bbox_new = ax.get_xlabel_bbox()\n    y_label_bbox_new = ax.get_ylabel_bbox()\n\n    # Assert that bbox changed due to tick adjustments\n    assert title_bbox_new != title_bbox\n    assert x_label_bbox_new != x_label_bbox\n    assert y_label_bbox_new != y_label_bbox\n    plt.close()\n\n\n"], "sample_474": ["    def test_reverse(self):\n        authors = Author.objects.annotate(reversed_name=Reverse(\"name\"))\n        self.assertEqual(reversed_name.first(), \"htmiS nhoJ\")\n        self.assertEqual(reversed_name.first(), \"nradnoR\")", "    def test_concat_null_handling(self):\n        authors = Author.objects.annotate(\n            full_name=Concat(\"name\", Value(\" \"), F(\"alias\"))\n        )\n        self.assertCountEqual(authors.all(), [self.john, self.elena, self.rhonda])\n", "    def test_ascii_in_non_ascii_context(self):\n        authors = Author.objects.annotate(name_code_point=Ord(\"name\"))\n        self.assertCountEqual(\n            authors.filter(name_code_point__chr__gt=Chr(ord(\"A\"))),\n            [self.john, self.elena, self.rhonda],\n        )\n", "    def test_non_ascii_reverse(self):\n        authors = Author.objects.annotate(\n            reversed_name=Reverse(\"name\")\n        )\n        self.assertEqual(\n            authors.filter(reversed_name=\"thmiS nhoJ\").first().name,\n            \"John Smith\"\n        )\n        self.assertEqual(\n            authors.filter(reversed_name=\"nadrohR\").first().name,\n            \"Rhonda\"\n        )\n        self.assertEqual(\n            authors.filter(reversed_name=\"nadroJ nE\u00e1lE\").first().name,\n            \"\u00c9lena Jordan\"\n        )\n", "    def test_mysql_specific(self):\n        with self.settings(DATABASE_ENGINE='mysql'):\n            authors = Author.objects.annotate(name_length=Length(\"name\"))\n            self.assertCountEqual(\n                authors.filter(name_length=8), [self.john, self.elena, self.rhonda]\n            )\n"], "sample_238": ["    def test_aggregation_with_subquery_ordering(self):\n        latest_book_pubdate_qs = Book.objects.filter(\n            publisher=OuterRef('pk')\n        ).order_by('-pubdate').values('pubdate')[:1]\n        publisher_qs = Publisher.objects.annotate(\n            latest_book_pubdate=Subquery(latest_book_pubdate_qs),\n            count=Count('book'),\n        ).order_by('-latest_book_pubdate')\n        with self.assertNumQueries(1) as ctx:\n            list(publisher_qs)\n        self.assertEqual(ctx[0]['sql'].count('SELECT'), 2)\n        # The GROUP BY should not be by alias either.\n        self.assertEqual(ctx[0]['sql'].lower().count('latest_book_pubdate'), 1)\n\n", "    def test_aggregation_distinct_count(self):\n        with self.assertRaises(TypeError):\n            Book.objects.annotate(\n                distinct_count=DistinctCount('rating'),\n            ).aggregate(sum_distinct_count=Sum('distinct_count'))\n        self.assertQuerysetEqual(\n            Book.objects.annotate(\n                distinct_count=DistinctCount('rating')\n            ).values('price', 'distinct_count'), [\n                {'price': Decimal('17.99'), 'distinct_count': 1},\n                {'price': Decimal('19.99'), 'distinct_count': 1},\n                {'price': Decimal('21.99'), 'distinct_count': 1},\n                {'price': Decimal('24.99'), 'distinct_count': 1},\n                {'price': Decimal('29.69'), 'distinct_count': 1},\n                {'price': Decimal('31.99'), 'distinct_count': 1},\n                {'price': Decimal('34.99'), 'distinct_count': 1},\n                {'price': Decimal('39.99'), 'distinct_count': 1},\n                {'price': Decimal('41.99'), 'distinct_count': 1},\n                {'price': Decimal('44.99'), 'distinct_count': 1},\n            ]\n        )\n\n\n", "    def test_aggregation_with_invalid_expression(self):\n        with self.assertRaisesMessage(\n            FieldError, 'Invalid expression: \"invalid_expression\"'\n        ):\n            Book.objects.aggregate(\n                invalid_expression=Sum('price') + 'invalid_expression'\n            )\n", "    def test_aggregation_multi_subquery(self):\n        books_rating_qs = Book.objects.filter(\n            publisher=OuterRef('pk'),\n            price=Decimal('29.69'),\n        ).values('rating')\n        publisher_qs = Publisher.objects.filter(\n            book__contact__age__gt=20,\n            name=self.p1.name,\n        ).annotate(\n            rating=Subquery(books_rating_qs),\n            contacts_count=Count('book__contact'),\n            related_books=Subquery(\n                Book.objects.filter(\n                    publisher=OuterRef('pk'),\n                    rating__gt=4,\n                ).values('name').annotate(\n                    count=Count('*')\n                ),\n            ),\n        ).values('rating', 'contacts_count').annotate(total_count=Count('rating'))\n        self.assertEqual(list(publisher_qs), [\n            {'rating': 4.0, 'contacts_count': 2, 'total_count': 2}\n        ])\n\n\n\n\n", "    def test_multi_aggregation_over_subquery(self):\n        latest_book_pubdate_qs = Book.objects.filter(\n            publisher=OuterRef('pk')\n        ).order_by('-pubdate').values('pubdate')[:1]\n        publisher_qs = Publisher.objects.annotate(\n            latest_book_pubdate=Subquery(latest_book_pubdate_qs),\n            num_books=Count('book'),\n            avg_rating=Avg('book__rating')\n        ).filter(latest_book_pubdate__isnull=False).order_by('num_books')\n        with self.assertNumQueries(1) as ctx:\n            list(publisher_qs)\n        self.assertEqual(ctx[0]['sql'].count('SELECT'), 2)\n        self.assertEqual(\n            list(publisher_qs), [\n                {'latest_book_pubdate': datetime.date(2019, 12, 6), 'num_books': 1, 'avg_rating': Decimal('2.5')},\n                {'latest_book_pubdate': datetime.date(2019, 12, 5), 'num_books': 2, 'avg_rating': Decimal('4.0')},\n                {'latest_book_pubdate': datetime.date(2019, 12, 4), 'num_books': 3, 'avg_rating': Decimal('4.0')},\n                {'latest_book_pubdate': datetime.date(2019, 12, 3), 'num_books': 1, 'avg_rating': Decimal('4.0')},\n                {'latest_book_pubdate': datetime.date(2019, 12, 2), 'num_books': 1, 'avg_rating': Decimal('4.0')},\n            ]\n        )\n"], "sample_111": ["    def test_object_tools_displayed_no_add_permission(self):\n        \"\"\"\n        When ModelAdmin.has_add_permission() returns False, the object-tools\n        block is still shown.\n        \"\"\"\n        superuser = self._create_superuser('superuser')\n        m = EventAdmin(Event, custom_site)\n        request = self._mocked_authenticated_request('/event/', superuser)\n        self.assertFalse(m.has_add_permission(request))\n        response = m.changelist_view(request)\n        self.assertIn('<ul class=\"object-tools\">', response.rendered_content)\n        # The \"Add\" button inside the object-tools shouldn't appear.\n        self.assertNotIn('Add ', response.rendered_content)\n", "    def test_save_all_with_related_objects(self):\n        \"\"\"\n        Ensure that saving all objects in a changelist works correctly\n        even when they have related objects (#15403).\n        \"\"\"\n        superuser = self._create_superuser('superuser')\n        m = SwallowAdmin(Swallow, custom_site)\n        request = self._mocked_authenticated_request('/swallow/', superuser)\n        cl = m.get_changelist_instance(request)\n        swallow1 = Swallow.objects.create(origin='Swallow A', load=4, speed=1)\n        swallow2 = Swallow.objects.create(origin='Swallow B', load=2, speed=2)\n        cl.queryset = Swallow.objects.all()\n        # This creates a fake edit context for these swallows\n        cl.result_list = [swallow1, swallow2]\n        cl.get_results(request)\n        for obj in cl.result_list:\n            obj.load = 5\n            obj.speed = 5\n        cl.save_related_objects(request)\n        self.assertEqual(swallow1.load, 5)\n        self.assertEqual(swallow1.speed, 5)\n        self.assertEqual(swallow2.load, 5)\n        self.assertEqual(swallow2.speed, 5)\n\n\n\n", "    def test_paginated_list_display_links(self):\n        \"\"\"\n        Pagination should affect the displayed list_display links (#11062).\n        \"\"\"\n        superuser = self._create_superuser('superuser')\n        m = DynamicListDisplayLinksChildAdmin(Child, custom_site)\n        request = self._mocked_authenticated_request('/child/', superuser)\n        request.GET['p'] = '1'  # Simulate a second page\n\n        cl = m.get_changelist_instance(request)\n        cl.get_results(request)\n        self.assertEqual(len(cl.result_list), 20)  # Assuming list_per_page=20\n\n        for obj in cl.result_list:\n            self.assertIsInstance(obj, Child)\n    \n", "    def test_multiple_page_changelist(self):\n        superuser = self._create_superuser('superuser')\n        m = SwallowAdmin(Swallow, custom_site)\n        request = self._mocked_authenticated_request('/swallow/', superuser)\n        for i in range(100):\n            Swallow.objects.create(origin='Swallow %s' % i, load=i, speed=i)\n        cl = m.get_changelist_instance(request)\n        cl.get_results(request)\n        self.assertEqual(len(cl.result_list), 10)\n        self.assertEqual(cl.page_num, 0)\n        cl.page_num = 1\n        cl.get_results(request)\n        self.assertEqual(len(cl.result_list), 10)\n        self.assertEqual(cl.page_num, 1)", "    def test_add_row_selection_with_multiple_rows(self):\n        \"\"\"\n        The status line for selected rows gets updated correctly when multiple rows are selected (#22038).\n        \"\"\"\n        self.admin_login(username='super', password='secret')\n        self.selenium.get(self.live_server_url + reverse('admin:auth_user_changelist'))\n\n        form_id = '#changelist-form'\n\n        # Test amount of rows in the Changelist\n        rows = self.selenium.find_elements_by_css_selector(\n            '%s #result_list tbody tr' % form_id)\n        self.assertEqual(len(rows), 1)\n\n        # Select a row and check again\n        first_row_selector = self.selenium.find_element_by_css_selector(\n            '%s #result_list tbody tr:first-child .action-select' % form_id)\n        first_row_selector.click()\n\n        second_row_selector = self.selenium.find_element_by_css_selector(\n            '%s #result_list tbody tr:nth-child(2) .action-select' % form_id)\n        second_row_selector.click()\n        selection_indicator = self.selenium.find_element_by_css_selector(\n            '%s .action-counter' % form_id)\n        self.assertEqual(selection_indicator.text, \"2 of 2 selected\")\n\n"], "sample_924": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_622": ["    def test_decode_cf_with_fill_value_and_encoding(self) -> None:\n        ds = Dataset(\n            {\n                \"x\": ([\"time\"], np.array([1, 2, 3]), {\"units\": \"m\", \"_FillValue\": 999}),\n            }\n        )\n        ds = ds.to_netcdf()\n        ds = open_dataset(ds, engine=\"netcdf4\")\n        decoded_ds = conventions.decode_cf(ds)\n        assert decoded_ds.x.values[0] == 1\n        assert decoded_ds.x.attrs[\"_FillValue\"] == 999\n\n", "def test_decode_cf_with_incompatible_cftime_units() -> None:\n    ds = Dataset({\"time\": (\"time\", [0, 1], {\"units\": \"days since 1900-01-01\", \"calendar\": \"noleap\"})})\n    with pytest.raises(ValueError, match=r\"unable to decode time\"):\n        decode_cf(ds)\n", "def test_decode_cf_variable_with_missing_value_and_fill_value() -> None:\n    var = Variable([\"t\"], [np.nan, 1, 2], {\"missing_value\": 0, \"_FillValue\": 1})\n    expected = Variable([\"t\"], [np.nan, 1, 2], {})\n    actual = conventions.decode_cf_variable(\"t\", var)\n    assert_identical(expected, actual)\n", "    def test_decode_cf_variable_with_invalid_units(self) -> None:\n        variable = Variable([\"time\"], [0, 1, 2], {\"units\": \"invalid\"})\n        with pytest.raises(ValueError, match=\"Unable to decode time\"):\n            conventions.decode_cf_variable(\"time\", variable)\n", "    def test_decode_cf_variable_with_missing_values(self) -> None:\n        variable = Variable(\n            [\"time\"],\n            np.array([1, 2, np.nan], dtype=\"float32\"),\n            {\"missing_value\": np.nan},\n        )\n        decoded = conventions.decode_cf_variable(\"time\", variable)\n        assert_identical(decoded, variable)\n"], "sample_27": ["    def test_fitsdiff_missing_header_keywords(self):\n        \"\"\"\n        Test that FITSDiff handles missing keywords correctly.\n        \"\"\"\n        path1 = self.temp(\"test1.fits\")\n        path2 = self.temp(\"test2.fits\")\n        ha = Header()\n        hb = ha.copy()\n        hb[\"FOO\"] = \"BAR\"\n\n        hdul1 = HDUList([PrimaryHDU(header=ha)])\n        hdul2 = HDUList([PrimaryHDU(header=hb)])\n\n        hdul1.writeto(path1)\n        hdul2.writeto(path2)\n\n        diff = FITSDiff(path1, path2)\n        report = diff.report()\n\n        assert \"Header keywords differ\" in report\n        assert \"a:  \\n  b: FOO = 'BAR'\" in report\n", "    def test_fitsdiff_with_different_column_names(self):\n        path1 = self.temp(\"test1.fits\")\n        path2 = self.temp(\"test2.fits\")\n        a = np.arange(100).reshape(10, 10)\n        hdulist = HDUList(\n            [\n                BinTableHDU(\n                    data=a,\n                    columns=[\n                        Column(\"A\", format=\"I\", array=a),\n                        Column(\"B\", format=\"I\", array=a),\n                    ],\n                )\n            ]\n        )\n        hdulist.writeto(path1)\n        hdulist.append(\n            BinTableHDU(\n                data=a,\n                columns=[\n                    Column(\"COL1\", format=\"I\", array=a),\n                    Column(\"COL2\", format=\"I\", array=a),\n                ],\n            )\n        )\n        hdulist.writeto(path2)\n        diff = FITSDiff(path1, path2)\n        assert diff.identical == False\n        assert \"Different column names found\" in diff.report()\n", "    def test_fitsdiff_diff_with_zero_rtol(tmp_path):\n        \"\"\"regression test for https://github.com/astropy/astropy/issues/13330\"\"\"\n        path1 = tmp_path / \"test1.fits\"\n        path2 = tmp_path / \"test2.fits\"\n        a = np.zeros((10, 2), dtype=\"float32\")\n        a[:, 0] = np.arange(10, dtype=\"float32\") + 10\n        a[:, 1] = np.arange(10, dtype=\"float32\") + 20\n        b = a.copy()\n        changes = [(3, 13.1, 23.1), (8, 20.5, 30.5)]\n        for i, v, w in changes:\n            b[i, 0] = v\n            b[i, 1] = w\n\n        ca = Column(\"A\", format=\"20E\", array=[a])\n        cb = Column(\"A\", format=\"20E\", array=[b])\n        hdu_a = BinTableHDU.from_columns([ca])\n        hdu_a.writeto(path1, overwrite=True)\n        hdu_b = BinTableHDU.from_columns([cb])\n        hdu_b.writeto(path2, overwrite=True)\n        with fits.open(path1) as fits1:\n            with fits.open(path2) as fits2:\n                diff = FITSDiff(fits1, fits2, atol=0, rtol=0)\n                str1 = diff.report(fileobj=None, indent=0)\n                assert \"Data contains differences:\" in str1\n", "    def test_fitsdiff_with_binary_data(tmp_path):\n        path1 = tmp_path / \"test1.fits\"\n        path2 = tmp_path / \"test2.fits\"\n\n        a = np.random.rand(100).astype(\"uint8\")\n        hdu_a = PrimaryHDU(data=a)\n        hdu_a.writeto(path1)\n\n        b = a.copy()\n        b[50:60] = 255\n\n        hdu_b = PrimaryHDU(data=b)\n        hdu_b.writeto(path2)\n        with fits.open(path1) as fits1:\n            with fits.open(path2) as fits2:\n                diff = FITSDiff(fits1, fits2)\n                assert not diff.identical\n\n                report = diff.report()\n                assert \"Data contains differences:\" in report\n                assert \"primary header\" in report\n                assert \"primary data\" in report\n", "    def test_fitsdiff_with_comments(tmp_path):\n        path1 = tmp_path / \"test1.fits\"\n        path2 = tmp_path / \"test2.fits\"\n\n        hdu1 = PrimaryHDU()\n        hdu1.header.set(\"COMMENT\", \"This is a comment\")\n        hdu2 = PrimaryHDU()\n        hdu2.header.set(\"COMMENT\", \"This is a different comment\")\n\n        hdulist1 = HDUList([hdu1])\n        hdulist2 = HDUList([hdu2])\n\n        hdulist1.writeto(path1)\n        hdulist2.writeto(path2)\n\n        diff = FITSDiff(path1, path2)\n        assert \"Header COMMENT differs:\" in diff.report()\n"], "sample_686": ["def test_external_plugins_registration_warnings(testdir, plugin, option):\n    testdir.syspathinsert()\n    testdir.makepyfile(**{plugin: \"\"})\n\n    with pytest.warns(pytest.PytestConfigWarning):\n        testdir.parseconfig(option, plugin)\n", "def test_external_plugins_integrated_with_config(testdir, plugin):\n    testdir.syspathinsert()\n    testdir.makepyfile(**{plugin: \"\"})\n\n    config = testdir.parseconfig(plugin)\n    assert config.pluginmanager.get_plugin(plugin)\n\n\n", "def test_warn_about_invalid_junit_family(testdir, value):\n    testdir.makepyfile(\n        \"\"\"\n            pass\n    \"\"\"\n    )\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        junit_family = {value}\n        \"\"\"\n            .format(value=value)\n    )\n    result = testdir.runpytest(\"--junit-xml=foo.xml\")\n    if value != \"xunit2\":\n        result.stdout.fnmatch_lines(\n            [\n                \"*PytestDeprecationWarning: The 'junit_family' default value will change to 'xunit2' in pytest 6.0.*\",\n                \"*See https://docs.pytest.org/en/stable/deprecations.html#junit-family-default-value-change-to-xunit2*\",\n            ]\n        )\n    else:\n        result.stdout.no_fnmatch_line(\"*PytestDeprecationWarning:\")\n\n\n\n", "def test_external_plugins_integrated_warning(testdir, plugin):\n    testdir.syspathinsert()\n    testdir.makepyfile(**{plugin: \"\"})\n\n    with pytest.warns(pytest.PytestConfigWarning) as w:\n        testdir.parseconfig(\"-p\", plugin)\n    assert len(w) == 1\n    assert w[0].filename == __file__\n\n\n\n", "def test_warn_about_junit_family_if_junitxml_is_used(testdir, junit_family):\n    \"\"\"Show a warning if junit_family is not defined and --junitxml is used,\n    and the specified value is not the default.\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n            pass\n    \"\"\"\n    )\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        junit_family = {junit_family}\n    \"\"\".format(\n                junit_family=junit_family\n            )\n        )\n\n    result = testdir.runpytest(\"--junit-xml=foo.xml\")\n    warning_msg = (\n        \"*PytestDeprecationWarning: The 'junit_family' default value will change*\"\n    )\n    result.stdout.no_fnmatch_line(warning_msg)\n\n\n"], "sample_315": ["    def test_incorrect_language_code(self):\n        response = self.client.get('/fr/account/register/', HTTP_ACCEPT_LANGUAGE='fr')\n        self.assertEqual(response.status_code, 404)\n", "    def test_redirect_respect_script_prefix_in_url(self):\n        prefix = '/script_prefix'\n        with override_script_prefix(prefix):\n            response = self.client.get('/nl/account/register/', HTTP_ACCEPT_LANGUAGE='en', SCRIPT_NAME=prefix)\n            self.assertRedirects(response, '%s/en/account/register/' % prefix, 301)\n\n\n", "    def test_custom_language_code(self):\n        with translation.override('en'):\n            response = self.client.get('/not-prefixed/', HTTP_ACCEPT_LANGUAGE='fr')\n            self.assertEqual(response.status_code, 404)\n        with translation.override('nl'):\n            response = self.client.get('/not-prefixed/', HTTP_ACCEPT_LANGUAGE='fr')\n            self.assertEqual(response.status_code, 404)\n        with translation.override('pt-br'):\n            response = self.client.get('/not-prefixed/', HTTP_ACCEPT_LANGUAGE='fr')\n            self.assertEqual(response.status_code, 404)\n\n\n\n", "    def test_language_from_function(self):\n        response = self.client.get('/nl/')\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.headers['content-language'], 'nl')\n        self.assertEqual(response.context['LANGUAGE_CODE'], 'nl')\n", "    def test_language_selection_with_explicit_url_language(self):\n        response = self.client.get('/nl/not-prefixed/', HTTP_ACCEPT_LANGUAGE='en')\n        self.assertEqual(response.status_code, 200) \n        self.assertEqual(response.headers['content-language'], 'nl')\n        self.assertEqual(response.context['LANGUAGE_CODE'], 'nl')\n        \n"], "sample_899": ["    def __init__(self, metric_function):\n        self.metric_function = metric_function\n", "compilation error", "def test_check_estimator_sparse_matrices_with_int64_indices():\n    # Check that estimator does not raise if sparse matrix has int64 indices\n    class ToleratesInt64IndicesClassifier(BaseEstimator):\n            if sp.issparse(X):\n                if X.getformat() == \"coo\":\n                    if X.row.dtype == \"int64\":\n                        pass\n                    if X.col.dtype == \"int64\":\n                        pass\n                elif X.getformat() in [\"csc\", \"csr\"]:\n                    if X.indices.dtype == \"int64\":\n                        pass\n                    if X.indptr.dtype == \"int64\":\n                        pass\n            return self\n\n    check_estimator(ToleratesInt64IndicesClassifier())\n", "def test_check_estimator_sparse_input( ) :\n    class SparseInputClassifier(BaseEstimator):\n            X = check_array(X)\n            self.X_shape_ = X.shape\n            return self\n\n            X = check_array(X)\n            if X.shape[1] != self.X_shape_[1]:\n                raise ValueError('Bad number of features')\n            return np.ones(X.shape[0])\n\n    msg = (\"Estimator SparseInputClassifier should not modify X during \"\n           \"fit.\")\n    assert_raises_regex(AssertionError, msg, check_estimator, SparseInputClassifier()) \n\n\n\n", "def test_check_estimator_large_sparse_matrix_indices():\n    # check that LargeSparseNotSupportedClassifier throws an error\n    # for sparse matrices with 64-bit indices\n    msg = \"Estimator doesn't support 64-bit indices\"\n    from sklearn.sparse import csr_matrix\n    X = csr_matrix(\n        [[1, 2], [3, 4]],\n        shape=(2, 2),\n        dtype=np.int64\n    )\n    est = LargeSparseNotSupportedClassifier()\n    with assert_raises_regex(ValueError, msg):\n        est.fit(X, np.zeros(2))\n"], "sample_155": ["    def test_file_content_length_for_large_files(self):\n        with tempfile.NamedTemporaryFile(mode='wb', delete=False) as tmp:\n            tmp.write(b'a' * 1024 * 1024)  # 1MB\n            filepath = tmp.name\n        response = FileResponse(open(filepath, 'rb'))\n        self.assertEqual(response['Content-Length'], str(1024 * 1024))\n        os.remove(filepath)\n", "    def test_file_not_found(self):\n        with self.assertRaises(FileNotFoundError):\n            FileResponse(open('/nonexistent_file', 'rb'))\n\n\n", "    def test_file_with_close_method(self):\n        with tempfile.TemporaryFile('w+b') as tmp:\n            tmp.write(b'some data')\n            tmp.seek(0)\n            response = FileResponse(tmp)\n            self.assertEqual(list(response), [b'some data'])\n            tmp.close() \n        \n\n", "    def test_file_with_no_content_length(self):\n        response = FileResponse(io.BytesIO(b'binary content'), as_attachment=True)\n        self.assertEqual(response['Content-Length'], '14')\n\n\n        temp_file = tempfile.NamedTemporaryFile(delete=False)\n        temp_file.write(b'binary content')\n        temp_file.flush()\n        response = FileResponse(temp_file.name, as_attachment=True)\n        self.assertFalse(response.has_header('Content-Length'))\n        os.remove(temp_file.name)\n", "    def test_file_response_with_no_content_length(self):\n        with tempfile.NamedTemporaryFile() as temp_file:\n            response = FileResponse(temp_file)\n            self.assertNotIn('Content-Length', response.headers)\n\n            # Ensure the response doesn't try to read the entire file\n            # if Content-Length is missing.\n            response.close()\n"], "sample_161": ["    def test_nonunique_fields_in_through_model(self):\n        class Fan(models.Model):\n            pass\n\n        class Event(models.Model):\n            invitees = models.ManyToManyField(\n                Fan,\n                through='Invitation',\n                through_fields=('invitee', 'event'),\n            )\n\n        class Invitation(models.Model):\n            event = models.ForeignKey(Event, models.CASCADE)\n            invitee = models.ForeignKey(Fan, models.CASCADE)\n            inviter = models.ForeignKey(Fan, models.CASCADE, related_name='+')\n\n        field = Event._meta.get_field('invitees')\n        self.assertEqual(field.check(from_model=Event), [\n            Error(\n                \"Cannot have fields in a ManyToMany field's through model \"\n                \"that don't have a unique constraint when related as 'ForeignObject'.\"\n                ,\n                hint=(\n                    \"Either ensure that the fields 'invitee' and 'event' in \"\n                    \"'invalid_models_tests.Invitation' have a unique \"\n                    \"constraint, or provide a unique constraint to the \"\n                    \"through model 'invalid_models_tests.Invitation'.\"\n                ),\n                obj=field,\n                id='fields.E319',\n            ),\n        ])\n", "    def test_intersection_and_superset_foreign_object(self):\n        class Parent(models.Model):\n            a = models.PositiveIntegerField()\n            b = models.PositiveIntegerField()\n            c = models.PositiveIntegerField()\n            d = models.PositiveIntegerField()\n\n            class Meta:\n                unique_together = (('a', 'b', 'c'),)\n\n        class Child(models.Model):\n            a = models.PositiveIntegerField()\n            b = models.PositiveIntegerField()\n            c = models.PositiveIntegerField()\n            d = models.PositiveIntegerField()\n            value = models.CharField(max_length=255)\n            parent = models.ForeignObject(\n                Parent,\n                on_delete=models.SET_NULL,\n                from_fields=('a', 'b', 'c'),\n                to_fields=('a', 'b', 'c'),\n                related_name='children',\n            )\n\n        field = Child._meta.get_field('parent')\n        self.assertEqual(field.check(from_model=Child), [\n            Error(\n                \"The fields 'a', 'b', 'c' on model 'Parent' are part of a unique_together constraint. \"\n                \"The parent ForeignObject relation must use a subset of these fields.\",\n                hint=(\n                    'Specify a subset of the fields '\n                    \"('a', 'b', 'c') that uniquely identify a \"\n                    \"Parent object.\"\n                ),\n                obj=field,\n                id='fields.E311',\n            ),\n        ])\n", "    def test_missing_required_fields(self):\n        class Fan(models.Model):\n            pass\n\n        with self.assertRaises(ValueError):\n            class Event(models.Model):\n                invitees = models.ManyToManyField(\n                    Fan, through='Invitation', required=True\n                )\n\n        class Invitation(models.Model):\n            event = models.ForeignKey(Event, models.CASCADE)\n            invitee = models.ForeignKey(Fan, models.CASCADE)\n            inviter = models.ForeignKey(Fan, models.CASCADE, related_name='+')\n\n\n", "    def test_circular_m2m_through(self):\n        class A(models.Model):\n            b = models.ForeignKey('B', on_delete=models.CASCADE)\n            c = models.ManyToManyField('C', through='D')\n\n        class B(models.Model):\n            a = models.ForeignKey('A', on_delete=models.CASCADE)\n\n        class C(models.Model):\n            pass\n\n        class D(models.Model):\n            a = models.ForeignKey('A', on_delete=models.CASCADE)\n            c = models.ForeignKey('C', on_delete=models.CASCADE)\n\n        with self.assertRaisesMessage(ValueError, \"'A' and 'B' recursively reference each other\"):\n            models.ManyToManyField(B, through='D', through_fields=('b', 'a'))\n\n\n\n", "    def test_m2m_through_fields_with_related_name(self):\n        class Fan(models.Model):\n            pass\n\n        class Event(models.Model):\n            invitees = models.ManyToManyField(\n                Fan,\n                through='Invitation',\n                through_fields=('event', 'invitee'),\n                related_name='invited_events',\n            )\n\n        class Invitation(models.Model):\n            event = models.ForeignKey(Event, models.CASCADE)\n            invitee = models.ForeignKey(Fan, models.CASCADE)\n            inviter = models.ForeignKey(Fan, models.CASCADE, related_name='+')\n\n        field = Event._meta.get_field('invitees')\n        self.assertEqual(field.check(from_model=Event), [])\n\n"], "sample_1146": ["def test_latex_unicode_char():\n    assert latex(u\"\\u03C0\") == r\"\\pi\" \n    assert latex(u\"\\u03A9\") == r\"\\omega\"\n", "def test_latex_printing_MatrixElement_with_matrices():\n    from sympy import MatrixSymbol\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    assert latex(MatrixElement(A, 0, 1) + MatrixElement(B, 0, 1)) == \\\n        r\"{A}_{0, 1} + {B}_{0, 1}\"\n\n\n", "def test_latex_printing_of_matrices_with_indices():\n    from sympy import MatrixSymbol, Matrix, symbols\n\n    x, y, z = symbols('x y z')\n    A = MatrixSymbol('A', 3, 3)\n    B = MatrixSymbol('B', 2, 2)\n    C = Matrix([[A[0, 0], A[0, 1]], [A[1, 0], A[1, 1]]])\n\n    assert latex(A) == r'A'\n    assert latex(B) == r'B'\n    assert latex(C) == r'\\begin{bmatrix} A_{0, 0} & A_{0, 1} \\\\ A_{1, 0} & A_{1, 1} \\end{bmatrix}'\n    assert latex(A[0, 1], subs=[(A[0, 1], 2*x + y)]) == r'2 x + y'\n    assert latex(B[1, 0]) == r'B_{1, 0}'\n\n\n\n", "def test_latex_with_lambdify():\n    from sympy import lambdify, symbols, sin, cos, latex\n    x = symbols('x')\n    f = lambdify(x, sin(x), 'numpy')\n    assert latex(f(1.0)) == r'sin{\\left(1.0 \\right)}'\n    _ = latex(f(x)) # should not raise an error\n", "def test_latex_printing_of_MatrixSymbols_with_multi_index():\n    from sympy import MatrixSymbol\n    A = MatrixSymbol(\"A\", 3, 4, dtype=float)\n    assert latex(A[1:3, 2:4]) == r\"A_{1, 2}^{3, 4}\"\n    assert latex(A[:, 0:2]) == r\"A_{1}^{2,}\"\n\n"], "sample_1020": ["def test_user_functions():\n    from sympy import Symbol\n    user_func = Function('my_user_func')\n    user_func.funcname = 'my_user_func'\n    settings = {'user_functions': [{\n        'my_user_func': lambda x, y: x**2 + y**2\n    }]}\n    assert mcode(user_func(x, y), settings=settings) == 'my_user_func[x, y]'\n", "def test_user_functions():\n    user_func = lambda x: x**2 + 1\n    settings = {'user_functions': {'myfunc': [lambda x: True, user_func]}}\n    assert mcode(myfunc(x), **settings) == \"myfunc[x]\"\n", "def test_String():\n    assert mcode(\"Hello, World!\") == \"Hold[StringQ['Hello, World!']] \" \n", "def test_order():\n    assert mcode(x + y, order='lex') == 'x + y'\n    assert mcode(x + 2*y**2, order='lex') == 'x + 2*y^2'\n    assert mcode(x + 2*y**2, order='code') == '2*y^2 + x' \n", "def test_complex():\n    assert mcode(complex(2,3)) == \"2 + 3 I\" \n    assert mcode(2*complex(3, 4)) == \"6 + 8 I\"\n    assert mcode(complex(1,2) + complex(3,4)) == \"4 + 6 I\"\n    assert mcode(complex(1,2) - complex(3,4)) == \"-2 - 2 I\"\n"], "sample_742": ["def test_coef_names():\n    # Test that coef_ names are correctly set when fit_intercept=False\n\n    X, y = iris.data, iris.target\n    lr = LogisticRegression(fit_intercept=False, random_state=0)\n    lr.fit(X, y)\n    assert_equal(len(lr.coef_[0]), X.shape[1])\n    assert_equal(lr.coef_.shape, (len(np.unique(y)), X.shape[1]))\n\n\n\n", "def test_coef_scaling_with_sample_weights():\n    # Test that the coefficients are correctly scaled with sample weights.\n    rng = np.random.RandomState(0)\n    X = np.random.randn(100, 2)\n    y = 2 * (X[:, 0] > 0) - 1\n    sample_weights = np.array([1, 2, 3, 4, 5] * 20)\n\n    # Fit the model with sample weights\n    clf = LogisticRegression(fit_intercept=False)\n    clf.fit(X, y, sample_weight=sample_weights)\n    coefs_with_weights = clf.coef_\n\n    # Fit the model without sample weights\n    clf_no_weights = LogisticRegression(fit_intercept=False)\n    clf_no_weights.fit(X, y)\n    coefs_no_weights = clf_no_weights.coef_\n\n    # Check if the coefficients scaled with sample weights are proportional \n    # to the coefficients without weights, scaled by the respective\n    # sample weights.\n    assert_array_almost_equal(coefs_with_weights,\n                            coefs_no_weights * sample_weights / np.sum(sample_weights))\n", "def test_l2_penalty_coef_zeros():\n\n    # Logistic regression should not give infinite values when C is infinity.\n    X, y = iris.data, iris.target\n    lr_inf = LogisticRegression(C=np.inf, random_state=0)\n    lr_inf.fit(X, y)\n    assert_array_almost_equal(lr_inf.coef_[0], np.zeros(X.shape[1]))\n    lr_inf = LogisticRegression(C=np.inf, penalty='l1', random_state=0)\n\n    lr_inf.fit(X, y)\n    assert_array_almost_equal(lr_inf.coef_[0], np.zeros(X.shape[1]))\n", "compilation error", "def test_predict_proba_multi_class():\n    X, y = make_classification(n_samples=100, n_features=20, random_state=0,\n                               n_classes=3, n_informative=10)\n\n    lr = LogisticRegression(multi_class='multinomial', random_state=0)\n    lr.fit(X, y)\n    preds = lr.predict_proba(X)\n    assert_equal(preds.shape[1], 3)  # 3 classes, 1 for each class\n\n"], "sample_624": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_410": ["    def test_repr(self):\n        p = Permission.objects.get(codename=\"view_customemailfield\")\n        self.assertEqual(repr(p), \"<Permission: auth_tests.CustomEmailField.view_customemailfield>\")\n", "    def test_create_user_with_custom_email_field_is_not_default(self):\n        user = User.objects.create_user(\n            \"user\", \"user@example.com\", email_address=\"user@example.com\"\n        )\n        self.assertEqual(user.email, \"user@example.com\")\n        self.assertEqual(user.email_address, \"user@example.com\")\n\n\n\n", "    def test_permission_str_with_content_type_label(self):\n        from django.apps import apps\n        content_type = apps.get_model('auth_tests', 'CustomEmailField')._meta.model_name\n        p = Permission.objects.get(codename=\"view_custom_email_field\")\n        self.assertEqual(str(p), f\"auth_tests | {content_type} | Can view {content_type}\")\n\n\n", "    def test_get_group_permissions(self):\n        permission = Permission.objects.create(\n            codename=\"can_view\", name=\"Can view\"\n        )\n        group = Group.objects.create(name=\"test_group\")\n        group.permissions.add(permission)\n        user = User.objects.create(username=\"testuser\")\n        user.groups.add(group)\n        self.assertEqual(user.get_group_permissions(), {permission})\n\n\n", "    def test_str_unicode_handling(self):\n        p = Permission.objects.get(codename=\"view_customemailfield\")\n        self.assertEqual(\n            str(p),\n            \"auth_tests | custom email field | Can view custom email field\",\n        )\n"], "sample_544": ["def test_imshow_with_non_default_norm(fig_test, fig_ref, norm):\n    data = np.random.rand(10, 10)\n    ax = fig_test.add_subplot()\n    if norm is None:\n        im = ax.imshow(data)\n    else:\n        im = ax.imshow(data, norm=norm)\n    ax.set_title(f\"Norm: {norm}\")\n\n    # Ensure that the image is rendered correctly regardless of the norm.\n\n\n", "def test_image_array_masked():\n    # Test handling of masked arrays in imshow\n    data = np.ma.masked_array(np.random.rand(10, 10),\n                              mask=np.random.rand(10, 10) < 0.5)\n    fig, ax = plt.subplots()\n    im = ax.imshow(data, cmap='viridis')\n    ax.set_xlabel(\"X\")\n    ax.set_ylabel(\"Y\")\n\n    fig.canvas.draw()\n\n\n\n", "def test_imshow_masked_array():\n    masked_array = np.ma.masked_array(np.arange(6).reshape((2, 3)),\n                                      mask=[False, True, True])\n    fig, ax = plt.subplots()\n    ax.imshow(masked_array)\n    fig.canvas.draw()\n", "def test_imshow_no_data():\n    fig, ax = plt.subplots()\n    with pytest.raises(ValueError):\n        ax.imshow(None)\n    with pytest.raises(ValueError):\n        ax.imshow(\n            np.array([], dtype=np.float64)\n        )\n    with pytest.raises(TypeError):\n        ax.imshow(\"hello\")\n\n\n\n", "def test_axesimage_get_extent():\n    ax = plt.gca()\n    im = AxesImage(ax)\n    with pytest.raises(RuntimeError, match=\"You must first set the image array\"):\n        im.get_extent()\n    z = np.arange(12, dtype=float).reshape((4, 3))\n    im.set_data(z)\n    assert im.get_extent() == (0, 3, 0, 4)\n    im.set_extent((0, 10, 0, 10))\n    assert im.get_extent() == (0, 10, 0, 10)\n\n"], "sample_1024": ["def test_issue_10443():\n    assert Float(1.0 / 0, 15).is_infinite is True\n    assert Float(-1.0 / 0, 15).is_infinite is True\n", "compilation error", "def test_issue_10507():\n    assert Float('0.1').as_integer_ratio() == Rational('1', 10)\n    assert Float('0.123456789').as_integer_ratio() == Rational('123456789', 1000000000) \n    assert Float('0.1234567890123456789').as_integer_ratio() == Rational('1234567890123456789', 1000000000000000000) \n", "compilation error", "compilation error"], "sample_802": ["def test_pipeline_fit_transform_memory():\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    cachedir = mkdtemp()\n    if LooseVersion(joblib_version) < LooseVersion('0.12'):\n        # Deal with change of API in joblib\n        memory = Memory(cachedir=cachedir, verbose=10)\n    else:\n        memory = Memory(location=cachedir, verbose=10)\n\n    pipe = Pipeline([('transf', DummyTransf()), ('svc', SVC())], memory=memory)\n    pipe.fit_transform(X, y)\n    pipe.fit_transform(X, y)\n    \n    shutil.rmtree(cachedir)\n\n\n\n", "def test_pipeline_with_custom_transformer():\n    class CustomTransformer:\n            self.a = a\n            self.b = b\n\n            return self\n\n            return X * self.a + self.b\n\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([1, 2])\n\n    pipeline = Pipeline([('transf', CustomTransformer(a=2, b=3)),\n                        ('clf', DummyRegressor())])\n    result = pipeline.fit(X, y)\n    assert_array_equal(result.transform(X), X * 2 + 3)\n    assert_array_equal(result.predict(X), result.named_steps['clf'].predict(X))\n\n\n\n", "def test_pipeline_with_validation_set():\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    pipe = Pipeline([('transf', DummyTransf()), ('clf', DummyClassifier())])\n    pipe.fit(X, y)\n    # Test with custom validation set\n    X_val = iris.data[50:]\n    y_val = iris.target[50:]\n    scores = pipe.score(X_val, y_val)\n    assert_is_instance(scores, float)\n    # Test with validation set specified in fit\n    scores = pipe.fit(X, y, X_val=X_val, y_val=y_val).score(X_val, y_val)\n    assert_is_instance(scores, float)\n\n\n\n", "def test_pipeline_memory_transform():\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    cachedir = mkdtemp()\n    try:\n        if LooseVersion(joblib_version) < LooseVersion('0.12'):\n            # Deal with change of API in joblib\n            memory = Memory(cachedir=cachedir, verbose=10)\n        else:\n            memory = Memory(location=cachedir, verbose=10)\n        # Test with Transformer + SVC and transform\n        clf = SVC(gamma='scale', probability=True, random_state=0)\n        transf = DummyTransf()\n        pipe = Pipeline([('transf', transf), ('svc', clf)])\n        cached_pipe = Pipeline([('transf', transf), ('svc', clf)],\n                               memory=memory)\n\n        # Memoize the pipeline at the first fit\n        cached_pipe.fit(X, y)\n        pipe.fit(X, y)\n        # Check that cached_pipe and pipe yield identical results\n        assert_array_equal(pipe.transform(X), cached_pipe.transform(X))\n        assert_array_equal(pipe.predict(X), cached_pipe.predict(X))\n        assert_array_equal(pipe.predict_proba(X),\n                           cached_pipe.predict_proba(X))\n        assert_array_equal(pipe.predict_log_proba(X),\n                           cached_pipe.predict_log_proba(X))\n        assert_array_equal(pipe.score(X, y), cached_pipe.score(X, y))\n\n    finally:\n        shutil.rmtree(cachedir)\n\n\n\n", "def test_pipeline_with_invalid_memory():\n    from joblib import Memory\n    # Test with cached pipeline to make sure invalid memory\n    # object raises error\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    cachedir = mkdtemp()\n    try:\n        memory = Memory(location=cachedir, verbose=10)\n        invalid_memory = \"invalid_memory\"\n        # Test with invalid memory type\n        with assert_raises(ValueError):\n            Pipeline([('transf', DummyTransf()), ('svc', SVC())],\n                     memory=invalid_memory)\n        # Test with invalid memory object\n        with assert_raises(ValueError):\n            pipeline = Pipeline([('transf', DummyTransf()), ('svc', SVC())],\n                               memory=memory)\n    finally:\n        shutil.rmtree(cachedir)\n"], "sample_268": ["    def test_snapshot_files_with_non_existing_files(self):\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.nonexistent_file, self.existing_file]):\n            snapshot = dict(self.reloader.snapshot_files())\n            self.assertEqual(snapshot[self.existing_file], self.nonexistent_file)\n\n\n\n", "    def test_should_stop_with_non_existing_files(self):\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.nonexistent_file]):\n            self.assertTrue(self.reloader.should_stop())\n\n\n\n", "    def test_notify_file_changed_runs_when_mtime_changed(self, mock_notify_file_changed):\n        self.reloader.watched_files = dict(self.reloader.snapshot_files())\n        self.reloader.watched_files[self.existing_file] = (time.time(), os.stat(self.existing_file.as_posix()).st_size)\n        self.increment_mtime(self.existing_file)\n        self.reloader.run_one_iteration()\n        self.assertEqual(mock_notify_file_changed.call_count, 1)\n        self.assertCountEqual(mock_notify_file_changed.call_args_list, [mock.call(self.existing_file)])\n", "    def test_refresh_watched_files_when_files_changed(self):\n        self.reloader.watched_files = mock.MagicMock(return_value=[self.existing_file])\n        with mock.patch.object(self.reloader, '_refresh_watched_files'):\n            self.reloader.refresh_watched_files()\n            self.reloader._refresh_watched_files.assert_called_once()\n\n\n\n", "    def test_snapshot_files_handles_symlinks(self):\n        link = self.tempdir / 'link'\n        link.symlink_to(self.existing_file)\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[link]):\n            snapshot = dict(self.reloader.snapshot_files())\n            self.assertIn(self.existing_file, snapshot)\n\n"], "sample_511": ["def test_subplot_projection_after_adding_subplot():\n    fig = plt.figure()\n    # Create a rectilinear subplot\n    ax1 = plt.subplot(1, 2, 1)\n\n    # Attempt to add a polar subplot after\n    with pytest.raises(ValueError,\n                       match=\"cannot be changed\"):\n        plt.subplot(1, 2, 2, projection='polar')\n", "def test_grid_alpha():\n    fig, ax = plt.subplots()\n    ax.grid(True, alpha=0.5)\n    assert ax.get_grid() is not None\n    ax.grid(False)\n    assert ax.get_grid() is None\n\n\n\n", "def test_subplot_projection_reuse():\n    # create an Axes\n    ax1 = plt.subplot(111)\n    # check that it is current\n    assert ax1 is plt.gca()\n    # make sure we get it back if we ask again\n    assert ax1 is plt.subplot(111)\n    # remove it\n    ax1.remove()\n    # create a polar plot\n    ax2 = plt.subplot(111, projection='polar')\n    assert ax2 is plt.gca()\n    # this should have deleted the first axes\n    assert ax1 not in plt.gcf().axes\n    # assert we get it back if no extra parameters passed\n    assert ax2 is plt.subplot(111)\n    ax2.remove()\n    # now check explicitly setting the projection to rectilinear\n    # makes a new axes\n    ax3 = plt.subplot(111, projection='rectilinear')\n    assert ax3 is plt.gca()\n    assert ax3 is not ax2\n    assert ax2 not in plt.gcf().axes\n\n", "def test_subplot_projection_reuse_3d():\n    fig = plt.figure()\n    ax1 = plt.subplot(111, projection='3d')\n    ax2 = plt.subplot(111, projection='3d')\n    assert ax1 is ax2\n    ax1.remove()\n    ax3 = plt.subplot(111, projection='3d')\n    assert ax3 is ax1\n\n\n\n", "def test_stackplot_kwargs():\n    plt.stackplot([1, 2, 3], [1, 2, 3], labels=['a', 'b', 'c'])\n    assert plt.gca().get_legend_handles_labels()[0][1] == 'a'\n    plt.stackplot([1, 2, 3], [1, 2, 3], labels=['a', 'b', 'c'], \n                  colors=['red', 'green', 'blue'])\n    assert plt.gca().get_legend_handles_labels()[0][1] == 'a' and \\\n           plt.gca().get_legend_handles_labels()[0][0].get_color() == 'red'\n    plt.stackplot([1, 2, 3], [1, 2, 3], labels=['a', 'b', 'c'], \n                  alpha=0.5)\n    assert plt.gca().get_legend_handles_labels()[0][0].get_alpha() == 0.5 \n\n\n\n"], "sample_1058": ["def test_NumPyPrinter_array_operations():\n    from sympy.tensor import Array\n\n    p = NumPyPrinter()\n\n    a = Array('a', (2, 3))\n    b = Array('b', (3, 2))\n\n    assert p.doprint(a + b) == 'numpy.add(a, b)'\n    assert p.doprint(a * b) == 'numpy.multiply(a, b)'\n    assert p.doprint(a - b) == 'numpy.subtract(a, b)'\n    assert p.doprint(a / b) == 'numpy.divide(a, b)'\n", "def test_NumPyPrinter_MatrixBase():\n    n = NumPyPrinter()\n    A = MatrixSymbol('A', 2, 2)\n    assert n._print_MatrixBase(A) == 'numpy.array([[0, 0], [0, 0]])'\n", "def test_NumPyPrinter_MatrixPower():\n    n = NumPyPrinter()\n    A = MatrixSymbol(\"A\", 2, 2)\n    assert n.doprint(A**2) == \"numpy.dot(A, A)\" \n\n\n", "def test_NumPyPrinter_CodegenArrayTensorProduct():\n    from sympy.codegen.array_utils import CodegenArrayTensorProduct\n\n    n = NumPyPrinter()\n    a = MatrixSymbol(\"a\", 2, 2)\n    b = MatrixSymbol(\"b\", 2, 2)\n    c = CodegenArrayTensorProduct([a, b], contraction_indices=[])\n    assert n.doprint(c) == \"numpy.einsum('', '', a, b)\"\n", "def test_NumPyPrinter_special_functions():\n    p = NumPyPrinter()\n    assert p.doprint(p.erf(x)) == 'numpy.erf(x)'\n    assert p.doprint(p.erfc(x)) == 'numpy.erfc(x)'\n\n\n\n"], "sample_259": ["    def test_prefetch_related_through_reverse_fk(self):\n        reader1 = Reader.objects.get(id=self.reader1.id)\n        with self.assertNumQueries(1):\n            prefetch_related_objects([reader1],\n                                    Prefetch('books_read__first_time_authors'))\n\n        with self.assertNumQueries(0):\n            self.assertEqual(\n                reader1.books_read.all().first().first_time_authors.all(),\n                [self.author1]\n            )\n\n", "    def test_prefetch_related_nested(self):\n        reader1 = Reader.objects.get(id=self.reader1.id)\n        with self.assertNumQueries(2):\n            prefetch_related_objects(\n                [reader1],\n                Prefetch(\n                    'books_read',\n                    queryset=Book.objects.all(),\n                    to_attr='the_books',\n                    prefetch_related=[\n                        Prefetch('authors', to_attr='the_authors')\n                    ]\n                )\n            )\n\n        with self.assertNumQueries(0):\n            for book in reader1.the_books:\n                self.assertCountEqual(book.the_authors, book.authors.all())\n\n\n", "    def test_prefetch_related_with_nested_prefetch(self):\n        readers = Reader.objects.all()\n        with self.assertNumQueries(2):\n            prefetch_related_objects(\n                readers, Prefetch('books_read', queryset=Book.objects.all(),\n                                 to_attr='read_books',\n                                 prefetch_related=[\n                                     Prefetch('authors', to_attr='book_authors')]))\n\n        with self.assertNumQueries(0):\n            for reader in readers:\n                self.assertEqual(len(reader.read_books), 2)  \n                for book in reader.read_books:\n                    self.assertCountEqual(book.book_authors, book.authors.all())\n\n\n\n", "    def test_prefetch_related_with_chained_lookups(self):\n        authors = list(Author.objects.all())\n        readers = list(Reader.objects.all())\n\n        with self.assertNumQueries(2):\n            prefetch_related_objects(authors, Prefetch('books__read_by', to_attr='readers'))\n\n        with self.assertNumQueries(0):\n            self.assertEqual(\n                [\n                    [r for r in a.books.all()[0].read_by.all()]\n                    for a in authors\n                ],\n                [[self.reader1], [self.reader1], [], [self.reader1, self.reader2]]\n            )\n\n\n", "    def test_prefetch_related_object_with_nested_prefetch(self):\n        book1 = Book.objects.get(id=self.book1.id)\n        with self.assertNumQueries(2):\n            prefetch_related_objects(\n                [book1],\n                Prefetch(\n                    'authors',\n                    queryset=Author.objects.all(),\n                    to_attr='the_authors',\n                    Prefetch(\n                        'books',\n                        to_attr='the_books',\n                    )\n                )\n            )\n\n        with self.assertNumQueries(0):\n            for author in book1.the_authors:\n                self.assertCountEqual(author.the_books, [self.book1])\n\n\n\n"], "sample_1117": ["def test_matrix_element_sets_mat_add():\n    X = MatrixSymbol('X', 4, 4)\n    Y = MatrixSymbol('Y', 4, 4)\n    assert ask(Q.integer_elements(X + Y), Q.integer_elements(X) &\n                Q.integer_elements(Y))\n    assert ask(Q.real_elements(X + Y), Q.real_elements(X) &\n                Q.real_elements(Y))\n    assert ask(Q.complex_elements(X + Y), Q.complex_elements(X) &\n                Q.complex_elements(Y))\n\n    assert ask(Q.integer_elements(X + Y), Q.integer_elements(X) &\n                Q.real_elements(Y)) is None\n    assert ask(Q.real_elements(X + Y), Q.real_elements(X) &\n                Q.complex_elements(Y)) is None\n", "def test_matrix_element_sets_transposed():\n    X = MatrixSymbol('X', 4, 4)\n    assert ask(Q.real_elements(X.T), Q.real_elements(X))\n    assert ask(Q.integer_elements(X.T), Q.integer_elements(X))\n    assert ask(Q.complex_elements(X.T), Q.complex_elements(X))\n    assert ask(Q.integer_elements(Identity(3).T), Q.integer_elements(Identity(3)))\n    assert ask(Q.integer_elements(ZeroMatrix(3, 3).T), Q.integer_elements(ZeroMatrix(3, 3)))\n    assert ask(Q.integer_elements(OneMatrix(3, 3).T), Q.integer_elements(OneMatrix(3, 3)))\n\n", "def test_factorizations():\n    X = MatrixSymbol('X', 3, 3)\n    assert ask(Q.invertible(X), Q.invertible(LofLU(X)))\n    assert ask(Q.invertible(X), Q.invertible(X.LU))\n    assert ask(Q.positive_definite(X), Q.positive_definite(LofLU(X)))\n    assert ask(Q.positive_definite(X), Q.positive_definite(X.Cholesky()))\n    assert ask(Q.orthogonal(X), Q.orthogonal(X.QR()))\n    assert ask(Q.unitary(X), Q.unitary(X.QR(method='full')))\n\n\n\n", "def test_matrix_element_sets_mat_mul():\n    X = MatrixSymbol('X', 3, 3)\n    Y = MatrixSymbol('Y', 3, 3)\n    assert ask(Q.integer_elements(X*Y), Q.integer_elements(X) & Q.integer_elements(Y))\n    assert ask(Q.real_elements(X*Y), Q.real_elements(X) & Q.real_elements(Y))\n    assert ask(Q.complex_elements(X*Y), Q.complex_elements(X) & Q.complex_elements(Y))  \n", "def test_matrix_element_sets_matadd():\n    X = MatrixSymbol('X', 4, 4)\n    Y = MatrixSymbol('Y', 4, 4)\n    assert ask(Q.integer_elements(X + Y), Q.integer_elements(X) &\n               Q.integer_elements(Y))\n    assert ask(Q.real_elements(X + Y), Q.real_elements(X) &\n               Q.real_elements(Y))\n    assert ask(Q.complex_elements(X + Y), Q.complex_elements(X) &\n               Q.complex_elements(Y))\n"], "sample_1091": ["def test_issue_19087():\n    x, y = symbols('x y')\n    assert Eq(x*y, y*x).simplify() == S.true\n    assert Eq(x*y, x*y).simplify() == S.true\n    assert Eq(x*y, 2*x).simplify() == False\n    assert Eq(x*y, 2*x).reversed.simplify() == False\n    assert Ne(x*y, 2*x).simplify() == True\n    assert Ne(x*y, 2*x).reversed.simplify() == True\n", "def test_issue_17302():\n    x = symbols('x')\n    y = symbols('y')\n    assert simplify(Eq(-x + 1, y)).as_expr() == Ne(-x, y - 1)\n    assert simplify(Eq(x - 1, y)).as_expr() == Ne(x, y + 1)\n\n\n", "def test_issue_18992():\n    x, y = symbols('x y')\n    eq = Eq(log(x * y), log(x) + log(y))\n    assert eq.can_be_simplified is True\n    assert simplify(eq) == S.true\n    assert simplify(Eq(log(x * y), log(x) + log(y)), evaluate=False) == S.true \n\n\n\n", "def test_issue_17484():\n    x, y = symbols('x y', real=True)\n    assert simplify(Eq(x, sqrt(y**2)).simplify()) == Eq(x, abs(y))\n", "compilation error"], "sample_910": ["def test_pending_logging_flush(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    with logging.pending_logging() as memhandler:\n        logger.warning('message1')\n        logger.warning('message2')\n\n    assert 'WARNING: message1\\nWARNING: message2' in strip_escseq(warning.getvalue())\n", "    def test_info_location_with_nodes(app, status, warning):\n        logging.setup(app, status, warning)\n        logger = logging.getLogger(__name__)\n\n        node = nodes.Node()\n        node.source, node.line = ('index.txt', 10)\n        logger.info('message', location=node)\n        assert 'index.txt:10: message' in status.getvalue()\n\n\n", "def test_log_collector(app, status, warning):\n    with logging.LogCollector() as collector:\n        logger = logging.getLogger(__name__)\n        logger.info('message1')\n        logger.warning('message2')\n        logger.error('message3')\n\n    logs = collector.logs\n\n    assert 'message1' in logs[0].msg\n    assert 'message2' in logs[1].msg\n    assert 'message3' in logs[2].msg\n", "def test_pending_logging_flushed(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    with logging.pending_logging() as memhandler:\n        logger.warning('message1')\n        logger.warning('message2')\n\n    assert 'WARNING: message1\\nWARNING: message2' in strip_escseq(warning.getvalue())\n", "def test_info_location_with_node(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    node = nodes.Node()\n    node.source = 'index.txt'\n    node.line = 10\n    logger.info('message1', location=node)\n    assert 'index.txt:10: message1' in status.getvalue()\n"], "sample_320": ["    def test_references_field_by_to_fields_null(self):\n        operation = FieldOperation(\n            \"Model\",\n            \"field\",\n            models.ForeignKey(\"Other\", models.CASCADE, to_field=\"field\", null=True),\n        )\n        self.assertIs(operation.references_field(\"Other\", \"field\", \"migrations\"), True)\n\n\n\n\n", "    def test_references_field_by_related_name(self):\n        operation = FieldOperation(\n            \"Model\", \"field\", models.ForeignKey(\"Other\", models.CASCADE, related_name=\"related_to_me\")\n        )\n        self.assertIs(operation.references_field(\"Other\", \"related_to_me\", \"migrations\"), True)\n        self.assertIs(operation.references_field(\"Other\", \"whatever\", \"migrations\"), False)\n\n\n\n", "    def test_references_field_by_related_name(self):\n        operation = FieldOperation(\n            \"Model\", \"field\", models.OneToOneField(\"Other\", models.CASCADE, related_name=\"related_model\")\n        )\n        self.assertIs(operation.references_field(\"Other\", \"related_model\", \"migrations\"), True)\n        self.assertIs(\n            operation.references_field(\"Other\", \"whatever\", \"migrations\"), False\n        )\n\n\n\n", "    def test_references_field_by_related_name(self):\n        operation = FieldOperation(\n            \"Model\", \"field\", models.ForeignKey(\"Other\", models.CASCADE, related_name=\"my_related_name\")\n        )\n        self.assertIs(\n            operation.references_field(\"Other\", \"my_related_name\", \"migrations\"), True\n        )\n        self.assertIs(\n            operation.references_field(\"Other\", \"whatever\", \"migrations\"), False\n        )\n", "    def test_references_field_by_related_name(self):\n        operation = FieldOperation(\n            \"Model\",\n            \"field\",\n            models.ForeignKey(\n                \"Other\", models.CASCADE, related_name=\"related_to_model\"\n            ),\n        )\n        self.assertIs(operation.references_field(\"Other\", \"related_to_model\", \"migrations\"), True)\n        self.assertIs(operation.references_field(\"Other\", \"whatever\", \"migrations\"), False)\n"], "sample_202": ["    def test_not_finished(self):\n        storage = self.storage_class(self.get_request())\n        messages = [Message(constants.INFO, 'message 1'),\n                    Message(constants.INFO, 'message 2')]\n        set_cookie_data(storage, messages)\n\n        # The message contains what's expected.\n        self.assertEqual(list(storage), messages)\n\n        # Simulate exceeding max_cookie_size and storing the not_finished sentinel.\n        max_cookie_size = CookieStorage.max_cookie_size\n        storage = self.storage_class(self.get_request())\n        for i in range(max_cookie_size // 5 + 1):\n            storage.add(constants.INFO, str(i) * 10)\n        # Make sure 'not_finished' is appended to the cookie\n        response = self.get_response()\n        storage.update(response)\n        decoded_messages = storage._decode(response.cookies[CookieStorage.cookie_name].value)\n        self.assertEqual(decoded_messages[-1], CookieStorage.not_finished)\n        self.assertEqual(len(decoded_messages), CookieStorage.max_cookie_size // 5 + 1)\n\n        # Retrieve the rest of the messages.\n        storage = self.storage_class(self.get_request())\n        response = self.get_response()\n        storage.update(response)\n        remaining_messages = storage._decode(response.cookies[CookieStorage.cookie_name].value)\n\n        self.assertEqual(len(remaining_messages), 1)\n        self.assertEqual(remaining_messages[0], CookieStorage.not_finished)\n\n", "    def test_not_finished_sentinel(self):\n        storage = self.get_storage()\n        response = self.get_response()\n        messages = [Message(constants.INFO, 'test') for _ in range(10)]\n        storage.update(response, messages)\n        # Check the cookie store message count\n        self.assertEqual(stored_cookie_messages_count(storage, response), 0)\n        # Get the data from the cookie\n        set_cookie_data(storage, messages, encode_empty=True)\n        self.assertEqual(list(storage), messages)\n        # Send more messages\n        storage.update(response, messages + [Message(constants.INFO, 'test')])\n        self.assertEqual(stored_cookie_messages_count(storage, response), 5)\n\n\n\n", "    def test_not_finished_sentinel(self):\n        storage = self.get_storage()\n        response = self.get_response()\n\n        # Fill the cookie with a bunch of messages, exceeding the max cookie size.\n        msgs = [Message(constants.INFO, 'message %s' % i) for i in range(10)]\n        storage._store(msgs, response, remove_oldest=False)\n\n        # Check that the cookie was truncated correctly and contains the\n        # sentinel value.\n        self.assertEqual(\n            self.stored_messages_count(storage, response),\n            9\n        )\n        self.assertEqual(storage._decode(response.cookies['messages'].value)[-1],\n                         CookieStorage.not_finished)\n\n        # Consume messages until we've retrieved all of them\n        messages = list(storage)\n        self.assertEqual(len(messages), 10)\n\n        # The sentinel should be gone when all messages have been retrieved\n        set_cookie_data(storage, messages)\n        self.assertEqual(self.stored_messages_count(storage, response), 10)\n        self.assertNotIn(CookieStorage.not_finished, storage._decode(response.cookies['messages'].value))\n\n\n", "    def test_not_finished_sentinel(self):\n        storage = self.get_storage()\n        response = self.get_response()\n        messages = [Message(constants.INFO, 'message %s') for x in range(5)]\n        storage.update(response, messages=messages)\n\n        # Check that the cookie was updated with the complete messages\n        self.assertEqual(self.stored_messages_count(storage, response), 5)\n\n        # Simulate another update to the storage\n        new_messages = [Message(constants.INFO, 'message %s') for x in range(5, 10)]\n        storage.update(response, messages=new_messages)\n\n        # Check that the cookie was updated with the remaining messages\n        self.assertEqual(self.stored_messages_count(storage, response), 10)\n\n        # Check that not_finished is not present on the last response\n        self.assertEqual(storage.stored_data(response)[0].message, new_messages[-1].message)\n\n        response = self.get_response()\n        self.assertEqual(self.stored_messages_count(storage, response), 0)\n\n        # Check that after retrieving the messages, the cookie should now contain the full set of messages\n        storage.update(response, messages=new_messages[:3])\n        self.assertEqual(self.stored_messages_count(storage, response), 3)\n\n\n\n", "    def test_cookie_not_finished(self):\n        storage = self.get_storage()\n        response = self.get_response()\n        messages = ['test1', 'test2', 'test3']\n        storage.add_messages(messages)\n        storage.update(response)\n\n        # Check that the cookie contains all the messages and that the\n        # 'not_finished' sentinel value has been added when the cookie size limit\n        # has been reached.\n        cookie_data = response.cookies.get(storage.cookie_name).value\n        set_cookie_data(storage, messages, encode_empty=True)\n        self.assertEqual(stored_cookie_messages_count(storage, response), 3)\n\n\n"], "sample_453": ["    def test_cell_count_inline_admin_form(self):\n        request = self.request_factory.get(\n            reverse(\"admin:admin_views_article_change\", args=[Article.objects.first().pk])\n        )\n        request.user = self.superuser\n        admin = ArticleAdmin(Article, site)\n        response = admin.change_view(request, str(Article.objects.first().pk))\n        template_context = response.context_data\n\n        inline_admin_formset = template_context[\"inline_admin_formsets\"][0]\n        count = cell_count(inline_admin_formset)\n        self.assertEqual(count, 3)  # Assume 3 columns in the inline form by default\n\n", "    def test_date_hierarchy_with_custom_format(self):\n        modeladmin = ModelAdmin(Question, site)\n        modeladmin.date_hierarchy = \"posted\"\n        modeladmin.date_hierarchy_format = \"%Y-%m-%d\"\n        Question.objects.bulk_create(\n            Question(question=\"q\", posted=datetime.date(2017, 10, 1))\n            for _ in range(3)\n        )\n        request = self.factory.get(\"/\", {\"posted__year\": \"2017\"})\n        request.user = self.superuser\n        changelist = modeladmin.get_changelist_instance(request)\n        spec = date_hierarchy(changelist)\n        self.assertEqual(spec[\"choices\"][0][\"link\"], \"posted__2017\")\n", "    def test_date_hierarchy_with_multiple_date_fields(self):\n        modeladmin = ModelAdmin(Question, site)\n        modeladmin.date_hierarchy = [\"posted\", \"expires\"]\n\n        Question.objects.bulk_create(\n            [\n                Question(question=\"q1\", posted=datetime.date(2017, 10, 1), expires=datetime.date(2017, 10, 1)),\n                Question(question=\"q2\", posted=datetime.date(2017, 10, 1), expires=datetime.date(2017, 12, 15)),\n                Question(question=\"q3\", posted=datetime.date(2017, 12, 15), expires=datetime.date(2017, 12, 31)),\n                Question(question=\"q4\", posted=datetime.date(2017, 12, 31), expires=datetime.date(2018, 2, 1)),\n            ]\n        )\n        tests = [\n            ({}, [[\"year=2017\"], [\"year=2018\"]]),\n            ({\"year\": 2016}, []),\n            ({\"year\": 2017}, [\n                [\"month=10\", \"year=2017\"],\n                [\"month=12\", \"year=2017\"]\n            ]),\n            ({\"year\": 2017, \"month\": 9}, []),\n            ({\"year\": 2017, \"month\": 10}, [\n                [\"day=01\", \"month=10\", \"year=2017\"]\n            ]),\n            ({\"year\": 2017, \"month\": 12}, [\n                [\"day=15\", \"month=12\", \"year=2017\"],\n                [\"day=31\", \"month=12\", \"year=2017\"]\n            ]),\n        ]\n        for query, expected_choices in tests:\n            with self.subTest(query=query):\n                query = {\"posted__%s\" % q: val for q, val in query.", "    def test_cell_count_hidden_fields(self):\n        inline_admin_form = ArticleAdmin(Article, site).inline_admin_formset.__class__()\n        inline_admin_form.fields = [\n            \"title\",\n            {\"name\": \"hidden_field\", \"widget\": \"HiddenInput\", \"is_hidden\": True},\n        ]\n        self.assertEqual(cell_count(inline_admin_form), 2)\n", "    def test_override_dates_format(self):\n        \"\"\"\n        admin_list template tags uses the format specified in the admin class's date_format.\n        \"\"\"\n        modeladmin = ModelAdmin(Question, site)\n        modeladmin.date_format = \"j M Y\"\n        Question.objects.create(question=\"q\", posted=datetime.date(2018, 1, 1))\n        request = self.factory.get(\"/\")\n        request.user = self.superuser\n        changelist = modeladmin.get_changelist_instance(request)\n        response = changelist.render(request)\n        self.assertContains(response, \"01 Jan 2018\")\n\n\n\n"], "sample_903": ["def test_pairwise_distances_with_sparse_matrix():\n    # test pairwise_distances function with sparse matrix\n    random_state = check_random_state(0)\n    X = sparse.rand(10, 5, density=0.2)\n    X_dense = X.toarray()\n    distances_dense = pairwise_distances(X_dense)\n    distances_sparse = pairwise_distances(X)\n    assert_array_almost_equal(distances_dense, distances_sparse.A, decimal=5)\n\n\n\n", "def test_early_exaggeration():\n    # test that the early_exaggeration parameter works as expected\n    random_state = check_random_state(0)\n    X = random_state.randn(100, 2)\n    tsne_1 = TSNE(early_exaggeration=1.0, verbose=2, random_state=0,\n                  method='exact')\n    tsne_2 = TSNE(early_exaggeration=10.0, verbose=2, random_state=0,\n                  method='exact')\n    tsne_1.fit_transform(X)\n    tsne_2.fit_transform(X)\n    # Compare the early_exaggeration effect\n    assert np.allclose(tsne_1.X_ , tsne_2.X_ , atol=0.01)\n\n\n", "    def test_kl_divergence_convergence():\n        random_state = check_random_state(0)\n        X = random_state.randn(100, 2)\n        tsne = TSNE(n_components=2, perplexity=30, learning_rate=50,\n                    random_state=0, verbose=2, n_iter=1000)\n        tsne.fit_transform(X)\n        # We expect the KL divergence to go down over time\n        kl_divergences = tsne.kl_divergence_history\n        assert len(kl_divergences) > 1\n        assert kl_divergences[-1] < kl_divergences[0] \n", "    def test_distance_metric_not_available():\n        # 'metric' must be valid.\n        tsne = TSNE(metric=\"not available\")\n        assert_raises_regexp(ValueError, \"Unknown metric 'not available'\",\n                             tsne.fit_transform, np.array([[0.0], [1.0]]))\n\n\n\n", "def test_sparse_matrices():\n    # t-SNE should handle sparse matrices as input.\n    random_state = check_random_state(0)\n    n_samples = 100\n    n_features = 5\n    X = random_state.rand(n_samples, n_features)\n    X_sparse = sp.csr_matrix(X)\n\n    tsne = TSNE(n_components=2, perplexity=30, random_state=0)\n    X_embedded = tsne.fit_transform(X_sparse)\n    assert_array_almost_equal(X_embedded, tsne.fit_transform(X))\n\n"], "sample_322": ["    def test_minimize_rollbacks_with_conflicting_dependencies(self):\n        \"\"\"\n        Minimize rollbacks when target has conflicting dependencies.\n\n        a: 1 <---- 2\n        b:      \\- 1 <---- 3\n                \\       /\n                 \\- 4  \n\n        \"\"\"\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        a2_impl = FakeMigration('a2')\n        a2 = ('a', '2')\n        b1_impl = FakeMigration('b1')\n        b1 = ('b', '1')\n        b2_impl = FakeMigration('b2')\n        b2 = ('b', '2')\n        c1_impl = FakeMigration('c1')\n        c1 = ('c', '1')\n        c2_impl = FakeMigration('c2')\n        c2 = ('c', '2')\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(a2, a2_impl)\n        graph.add_node(b1, b1_impl)\n        graph.add_node(b2, b2_impl)\n        graph.add_node(c1, c1_impl)\n        graph.add_node(c2, c2_impl)\n        graph.add_dependency(None, a2, a1)\n        graph.add_dependency(None, b1, a1)\n        graph.add_dependency(None, b2, b1)\n        graph.add_dependency(None, c1, b1)\n        graph.add_dependency(None, c2, b2)\n\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {\n            a1: a1_impl,\n            b1: b1_impl,\n            c1: c1_impl,\n        })\n\n        plan = executor.migration_plan({a2})\n\n        should_be_rolled_back = [b2_impl, c2_impl]\n        exp = [(m, True) for m in should_be_rolled_back]\n        self.assertEqual(", "    def test_minimize_rollbacks_multiple_branches(self):\n        \"\"\"\n        Minimize rollbacks when target has multiple branches with dependencies.\n        a: 1 <---- 2 --> 3\n              \\       ^  \n               \\      |  \n                \\- 4 <--- 5\n        b:      \\- 1\n        c:      \\- 1\n        \"\"\"\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        a2_impl = FakeMigration('a2')\n        a2 = ('a', '2')\n        a3_impl = FakeMigration('a3')\n        a3 = ('a', '3')\n        a4_impl = FakeMigration('a4')\n        a4 = ('a', '4')\n        a5_impl = FakeMigration('a5')\n        a5 = ('a', '5')\n        b1_impl = FakeMigration('b1')\n        b1 = ('b', '1')\n        c1_impl = FakeMigration('c1')\n        c1 = ('c', '1')\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(a2, a2_impl)\n        graph.add_node(a3, a3_impl)\n        graph.add_node(a4, a4_impl)\n        graph.add_node(a5, a5_impl)\n        graph.add_node(b1, b1_impl)\n        graph.add_node(c1, c1_impl)\n        graph.add_dependency(None, a2, a1)\n        graph.add_dependency(None, a3, a2)\n        graph.add_dependency(None, a4, a1)\n        graph.add_dependency(None, a5, a3)\n        graph.add_dependency(None, b1, a1)\n        graph.add_dependency(None, c1, a1)\n\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {\n            a1: a1_impl,\n            b1: b1_impl,\n", "    def test_applied_and_unapplied_migrations_are_recorded_correctly(self):\n        \"\"\"\n        Migrations are record both applied and unapplied states correctly.\n        \"\"\"\n        applied_migs = {}\n        unapplied_migs = {}\n        class Recorder:\n                if applied:\n                    applied_migs[(app, name)] = True\n                else:\n                    unapplied_migs[(app, name)] = True\n\n        executor = MigrationExecutor(None)\n        executor.recorder = Recorder()\n        a1_impl = FakeMigration('a1')\n        executor.apply_migration(ProjectState(), a1_impl, FakeLoader(None, {}))\n        executor.migrate([('a', '0001_initial')])\n\n        # applied migrations\n        self.assertIn(('a', '0001_initial'), applied_migs)\n        # unapplied migrations\n        self.assertNotIn(('a', '0001_initial'), unapplied_migs)\n\n        # apply migration again, it should be no applied\n        with self.assertRaises(migrations.exceptions.MigrationPlanError):\n            executor.migrate([('a', '0001_initial')])\n\n        # unapplied migrations\n        self.assertIn(('a', '0001_initial'), unapplied_migs)\n        # applied migrations\n        self.assertNotIn(('a', '0001_initial'), applied_migs)\n\n\n\n\n        executor.migrate([('a', None)])\n        # applied migrations\n        self.assertNotIn(('a', '0001_initial'), applied_migs)\n        # unapplied migrations\n        self.assertNotIn(('a', '0001_initial'), unapplied_migs)\n\n", "    def test_minimize_rollbacks_no_dependencies(self):\n        r\"\"\"\n        Minimize rollbacks when target has no dependencies.\n\n        If the target migration has no dependencies, even if there are\n        other applied migrations, we should only rollback the target and any\n        migrations that would need to be reapplied to reach the target.\n        \"\"\"\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        a2_impl = FakeMigration('a2')\n        a2 = ('a', '2')\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(a2, a2_impl)\n\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {a1: a1_impl})\n\n        plan = executor.migration_plan({a2})\n        self.assertEqual(plan, [(a1_impl, True)])\n", "    def test_backwards_from_state_that_cant_be_reached(self):\n        \"\"\"\n        If the current state can't be reached, don't apply migrations.\n\n        a: 1 <-- 2\n        b:  \\--- 1\n        c:      \\- 1\n\n        If a1 is applied already and a2 is missing, and we're asked to migrate\n        to b1, don't apply a1 or a2.\n\n        \"\"\"\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        a2_impl = FakeMigration('a2')\n        a2 = ('a', '2')\n        b1_impl = FakeMigration('b1')\n        b1 = ('b', '1')\n        c1_impl = FakeMigration('c1')\n        c1 = ('c', '1')\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(a2, a2_impl)\n        graph.add_node(b1, b1_impl)\n        graph.add_node(c1, c1_impl)\n        graph.add_dependency(None, a2, a1)\n        graph.add_dependency(None, b1, a1)\n        graph.add_dependency(None, c1, a1)\n        graph.add_dependency(None, b1, a2)  # This dependency ensures a1 is applied before b1\n\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {\n            a1: a1_impl,\n        })\n\n        plan = executor.migration_plan({b1})\n   \n        self.assertEqual(plan, [])\n"], "sample_434": ["    def test_redirect_view_handles_missing_url(self):\n        class RedirectViewWithoutURL(RedirectView):\n            permanent = False\n\n        request_factory = RequestFactory()\n        response = RedirectViewWithoutURL().get(request_factory.get(\"/\"))\n        self.assertIsInstance(response, HttpResponseGone)\n        self.assertEqual(response.status_code, 410)\n", "    def test_redirect_view_urls(self):\n        class RedirectViewTest(RedirectView):\n            url = \"https://www.example.com/{arg}\"\n\n        redirect_view = RedirectViewTest()\n        request = RequestFactory().get(\"/test\")\n        response = redirect_view.get(request, arg=\"test\")\n        self.assertEqual(response, HttpResponseRedirect(\"https://www.example.com/test\"))\n\n\n", "    def test_view_with_extra_context(self):\n        class MyView(TemplateView):\n            template_name = \"my_template.html\"\n            extra_context = {\"message\": \"Hello from extra_context!\"}\n\n        request_factory = RequestFactory()\n        response = MyView.as_view()(request_factory.get(\"/\"))\n        self.assertIn(\"message\", response.context_data)\n        self.assertEqual(response.context_data[\"message\"], \"Hello from extra_context!\")\n", "    def test_view_dispatch_to_correct_method(self):\n        request_factory = RequestFactory()\n        view = SyncView()\n        response = view.dispatch(request_factory.get(\"/\"))\n        self.assertEqual(response.content, b\"Hello (sync) world!\")\n\n\n\n", "    def test_redirect_view_works_with_kwargs(self):\n        class RedirectViewWithKwargs(RedirectView):\n            pattern_name = \"redirect_view_test\"\n\n        # Make sure the redirect URL is constructed correctly with kwargs\n        url = reverse(\"redirect_view_test\", kwargs={\"id\": 123})\n        view = RedirectViewWithKwargs()\n        response = view.get(\n            RequestFactory().get(\"/\"), kwargs={\"id\": 123}\n        )\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response[\"Location\"], url) \n\n\n\n"], "sample_347": ["    def test_make_aware_ambiguous_zoneinfo(self):\n        # 2:30 happens twice, once before DST ends and once after\n        ambiguous = datetime.datetime(2015, 10, 25, 2, 30)\n\n        std = timezone.make_aware(ambiguous.replace(fold=1), timezone=PARIS_ZI)\n        dst = timezone.make_aware(ambiguous, timezone=PARIS_ZI)\n\n        self.assertEqual(\n            std.astimezone(UTC) - dst.astimezone(UTC),\n            datetime.timedelta(hours=1)\n        )\n        self.assertEqual(std.utcoffset(), datetime.timedelta(hours=1))\n        self.assertEqual(dst.utcoffset(), datetime.timedelta(hours=2))\n\n\n", "    def test_localtime_ambiguous(self):\n        naive = datetime.datetime(2011, 10, 25, 2, 30)\n        with self.assertRaises(pytz.AmbiguousTimeError):\n            timezone.localtime(naive)\n\n        # Same test with zoneinfo\n        if HAS_ZONEINFO:\n            with self.assertRaises(pytz.AmbiguousTimeError):\n                timezone.localtime(naive, timezone=PARIS_ZI)\n\n\n\n", "    def test_ambiguous_or_imaginary_time(self):\n        for tz in PARIS_IMPLS:\n            with self.subTest(repr(tz)):\n                for dt in [\n                    datetime.datetime(2015, 10, 25, 2, 30),\n                    datetime.datetime(2015, 3, 29, 2, 30),\n                ]:\n                    with self.subTest(dt=dt):\n                        self.assertTrue(\n                            timezone._datetime_ambiguous_or_imaginary(dt, tz),\n                            f'dt={dt} is ambiguous or imaginary in tz={tz}'\n                        )\n\n", "    def test_datetime_ambiguous_or_imaginary(self):\n        for tz in PARIS_IMPLS:\n            test_dt = datetime.datetime(2015, 10, 25, 2, 30)\n            with self.subTest(tz=tz):\n                self.assertTrue(timezone._datetime_ambiguous_or_imaginary(test_dt, tz))\n\n            test_dt = datetime.datetime(2015, 3, 29, 2, 30)\n            with self.subTest(tz=tz, dt=test_dt):\n                self.assertTrue(timezone._datetime_ambiguous_or_imaginary(test_dt, tz))\n\n", "    def test_make_aware_ambiguous_zoneinfo(self):\n        ambiguous = datetime.datetime(2015, 10, 25, 2, 30)\n        with self.assertRaises(zoneinfo.AmbiguousTimeError):\n            timezone.make_aware(ambiguous, PARIS_ZI)\n\n        std = timezone.make_aware(ambiguous.replace(fold=1), PARIS_ZI)\n        dst = timezone.make_aware(ambiguous, PARIS_ZI)\n        self.assertEqual(std - dst, datetime.timedelta(hours=1))\n        self.assertEqual(std.utcoffset(), datetime.timedelta(hours=1))\n        self.assertEqual(dst.utcoffset(), datetime.timedelta(hours=2))\n\n\n\n"], "sample_1054": ["def test_issue_12212():\n    a = Interval(0, 1)\n    b = Interval(1, 2)\n    c = FiniteSet(1, 2)\n    cp1 = ComplexRegion(a * c, polar=True)\n    cp2 = ComplexRegion(b * c, polar=True)\n\n    assert cp1.union(cp2) == ComplexRegion(Interval(0, 2) * c, polar=True)\n", "def test_issue_12157():\n    from sympy.abc import x, y\n    c1 = ComplexRegion(FiniteSet(x + y*I) for x in S.Integers for y in S.Integers)\n    c2 = ComplexRegion(Interval(-1, 1)*Interval(-1, 1))\n    assert c1.measure == oo\n    assert c2.measure == 4\n    assert c1.intersect(c2) == c2\n\n\n\n", "compilation error", "def test_issue_12055():\n    unit = Interval(0, 1)\n    cr1 = ComplexRegion(unit * unit)\n\n    assert cr1.measure == 1\n    assert cr1.measure == cr1.measure\n\n\n", "def test_issue_11937():\n    a = Interval(0, 1)\n    b = Interval(1, 2)\n    c = Interval(-1, 0)\n    d = Interval(0, 1)\n    cp1 = ComplexRegion(a * b)\n    cp2 = ComplexRegion(c * d)\n    assert Intersection(cp1, cp2) == S.EmptySet\n"], "sample_124": ["    def test_template_context_data(self):\n        class MyForm(Form):\n            name = CharField(max_length=30)\n\n        form = MyForm({'name': 'John Doe'})\n        context = form.render_context\n        self.assertDictEqual(context, {'form': MyForm, 'form.name': 'John Doe'})\n\n", "    def test_error_rendering(self):\n        class MyForm(Form):\n            name = CharField(max_length=30)\n            email = EmailField()\n\n        form = MyForm({'name': ' ', 'email': 'invalid'})\n        self.assertFalse(form.is_valid())\n        rendered_html = form.as_p()\n        self.assertIn('<ul class=\"errorlist\">', rendered_html)\n        self.assertIn('<li>This field is required.</li>', rendered_html)\n        self.assertIn('<li>Enter a valid email address.</li>', rendered_html)\n", "    def test_renderer_non_callable(self):\n        with self.assertRaises(TypeError):\n            Form(renderer='not a callable')\n", "    def test_renderer_inheritance(self):\n        class MyRenderer(DjangoTemplates):\n            pass\n\n        class MyForm(Form):\n            default_renderer = MyRenderer\n\n        form = MyForm()\n        self.assertTrue(isinstance(form.renderer, MyRenderer))\n", "    def test_renderer_callable(self):\n        class CustomRenderer(object):\n                return 'CUSTOM RENDERED FORM'\n\n        form = Form()\n        form.renderer = CustomRenderer()\n        self.assertEqual(form.as_p(), 'CUSTOM RENDERED FORM')\n"], "sample_992": ["compilation error", "def test_piecewise():\n    prntr = PythonCodePrinter()\n    expr = (x >= 0) * x + (x < 0) * -x\n    assert prntr.doprint(expr) == '((x >= 0) * x) + ((x < 0) * -x)'\n\n    expr = Piecewise((x, x < 0), (x**2, x >= 0))\n    assert prntr.doprint(expr) == '(x if x < 0 else x**2)'\n\n\n", "def test_custom_functions():\n    class MyFunction(Expr):\n            return 'myfunc'\n\n    printer = PythonCodePrinter()\n    assert printer.doprint(MyFunction()) == 'myfunc'\n    \n    class MyConstant(Expr):\n        _c= 'myc'\n\n    printer = PythonCodePrinter()\n    assert printer.doprint(MyConstant()) == 'myc' \n", "def test_matrix_printing():\n    from sympy.matrices import Matrix\n\n    p = NumPyPrinter()\n    mat = Matrix([[1, 2], [3, 4]])\n    assert p.doprint(mat) == 'numpy.array([[1., 2.], [3., 4.]])'\n    assert 'numpy' in p.module_imports\n\n    mat2 = Matrix([[5, 6, 7], [8, 9, 10]])\n    assert p.doprint(mat2) == 'numpy.array([[5., 6., 7.], [8., 9., 10.]])'\n    assert 'numpy' in p.module_imports\n\n    mat3 = SparseMatrix(2, 3, {(0, 1): 5, (1, 2): 6})\n    assert p.doprint(mat3) == 'scipy.sparse.coo_matrix([5, 6], ([0, 1], [1, 2]), shape=(2, 3))'\n    assert 'numpy' in p.module_imports\n\n\n\n", "def test_pieceswise_functions():\n    prntr = NumPyPrinter()\n    expr = (\n        Expr.Piecewise(\n            (sympy.sin(x) * sympy.cos(x), sympy.less(x, sympy.pi / 2)),\n            (sympy.cos(x) * sympy.sin(x), sympy.greater(x, sympy.pi / 2)),\n        )\n    )\n    assert prntr.doprint(expr) == \"\"\"numpy.select(\n      (x < numpy.pi / 2),\n      (numpy.sin(x) * numpy.cos(x)),\n      (numpy.cos(x) * numpy.sin(x)),\n      default=numpy.nan\n    )\"\"\"\n"], "sample_108": ["    def test_reverse_type_error_propagates(self):\n        @DynamicConverter.register_to_url\n            raise TypeError('This type error propagates.')\n        with self.assertRaisesMessage(TypeError, 'This type error propagates.'):\n            reverse('dynamic', kwargs={'value': object()})\n\n", "    def test_reverse_type_error_propagates(self):\n        @DynamicConverter.register_to_url\n            raise TypeError('This type error propagates.')\n        with self.assertRaisesMessage(TypeError, 'This type error propagates.'):\n            reverse('dynamic', kwargs={'value': object()})\n", "    def test_reverse_type_error_propagates(self):\n        @DynamicConverter.register_to_url\n            raise TypeError('This type error propagates.')\n        with self.assertRaisesMessage(TypeError, 'This type error propagates.'):\n            reverse('dynamic', kwargs={'value': object()})\n", "    def test_reverse_type_error_propagates(self):\n        @DynamicConverter.register_to_url\n            raise TypeError('This type error propagates.')\n        with self.assertRaisesMessage(TypeError, 'This type error propagates.'):\n            reverse('dynamic', kwargs={'value': object()})\n\n\n\n", "    def test_resolve_no_match_with_dynamic_converter(self):\n        @DynamicConverter.register_to_python\n            raise ValueError('This converter always raises a ValueError')\n        with self.assertRaises(Resolver404):\n            resolve('/dynamic/abc/other/')  \n"], "sample_335": ["    def test_decimalfield_empty_string(self):\n        f = DecimalField(required=True)\n        with self.assertRaisesMessage(ValidationError, \"'This field is required.'\"):\n            f.clean('')\n\n\n\n", "    def test_decimalfield_with_empty_values(self):\n        f = DecimalField(required=False)\n        self.assertIsNone(f.clean(None))\n        self.assertIsNone(f.clean(''))\n        self.assertIsNone(f.clean('  '))\n        f = DecimalField(required=True)\n        with self.assertRaisesMessage(ValidationError, \"'This field is required.'\"):\n            f.clean(None)\n        with self.assertRaisesMessage(ValidationError, \"'This field is required.'\"):\n            f.clean('')\n\n", "    def test_decimalfield_with_custom_widget(self):\n        class MyCustomWidget(Widget):\n                return f'<input id=\"{attrs.get(\"id\", name)}\" type=\"text\" name=\"{name}\" value=\"{value}\" step=\"0.01\">'\n\n        class MyDecimalField(DecimalField):\n                super().__init__(widget=MyCustomWidget(), **kwargs)\n\n        f = MyDecimalField(max_digits=4, decimal_places=2)\n        self.assertWidgetRendersTo(\n            f,\n            '<input id=\"id_f\" type=\"text\" name=\"f\" value=\"\" step=\"0.01\">',\n        )\n\n        f.value = decimal.Decimal('1.23')\n        self.assertWidgetRendersTo(\n            f,\n            '<input id=\"id_f\" type=\"text\" name=\"f\" value=\"1.23\" step=\"0.01\">',\n        )\n", "    def test_decimalfield_empty_string(self):\n        f = DecimalField()\n        self.assertIsNone(f.clean(''))\n        self.assertIsNone(f.clean(None))\n\n\n", "    def test_decimalfield_widget(self):\n        f = DecimalField()\n        self.assertIsInstance(f.widget, NumberInput)\n        f = DecimalField(widget=Textarea)\n        self.assertIsInstance(f.widget, Textarea)\n"], "sample_51": ["    def test_timezone_offsets(self):\n        test_cases = (\n            ('2012-04-23T09:15:00+0200', datetime(2012, 4, 23, 9, 15, 0, 0, get_fixed_timezone(120))),\n            ('2012-04-23T09:15:00-0500', datetime(2012, 4, 23, 9, 15, 0, 0, get_fixed_timezone(-300))),\n        )\n        for source, expected in test_cases:\n            with self.subTest(source=source):\n                self.assertEqual(parse_datetime(source), expected)\n", "    def test_invalid_duration_formats(self):\n        invalid_inputs = (\n            'invalid',\n            '1 day 1:2:3.4abc',\n            '1 day 1:2:3.',\n            '1 day 1:2:3.12345678901234567890',\n            'P4Y1M',\n            'PT5H1M',\n            'PT5.6S',\n        )\n        for source in invalid_inputs:\n            with self.subTest(source=source):\n                self.assertIsNone(parse_duration(source))\n", "    def test_invalid_duration_formats(self):\n        invalid_inputs = (\n            '1 day 10:15:30.1 day',  \n            '1 day 10:15:30.1h',\n            '1 day 10:15:30.1m',\n            '1 day 10:15:30.1s',\n            '1 day 10:15:30.1foo',\n            '10:15:30.1',\n            '1 day 10:15',\n            '1 day 10',\n            '10:15:30',\n            'invalid',\n        )\n        for source in invalid_inputs:\n            with self.subTest(source=source):\n                self.assertIsNone(parse_duration(source))\n", "    def test_ambiguous_duration_input(self):\n        self.assertEqual(parse_duration('3 days 10:15:30'), timedelta(days=3, hours=10, minutes=15, seconds=30))\n\n\n", "    def test_timezone_offsets(self):\n        valid_inputs = (\n            ('2012-04-23T09:15:00+02:30', datetime(2012, 4, 23, 9, 15, 0, 0, get_fixed_timezone(150))),\n            ('2012-04-23T10:20:30-0300', datetime(2012, 4, 23, 10, 20, 30, 0, get_fixed_timezone(-180))),\n        )\n        for source, expected in valid_inputs:\n            with self.subTest(source=source):\n                self.assertEqual(parse_datetime(source), expected)\n"], "sample_979": ["def test_matrix_transpose_simplify():\n    A = MatrixSymbol('A', 3, 2)\n    B = MatrixSymbol('B', 2, 4)\n    C = MatrixSymbol('C', 4, 4)\n    T_A = Transpose(A)\n    assert T_A.shape == (2, 3)\n    assert (A*B).shape == (3, 4)\n    assert (Transpose(A)*B).shape == (2, 4)\n    assert (A*Transpose(B)).shape == (3, 4)\n    assert (Transpose(A)*Transpose(B)).shape == (2, 4)\n    assert (C*Transpose(C)).shape == (4, 4)\n\n    assert simplify(Transpose(Transpose(A))) == A\n    assert simplify(Transpose(A*B)) == Transpose(B)*Transpose(A)\n    assert simplify(Transpose(A * Transpose(B))) == Transpose(B.T)*Transpose(A)\n\n\n\n\n", "def test_matrix_element_subs():\n    A = MatrixSymbol('A', 2, 2)\n    B = Matrix([\n        [1, 2],\n        [3, 4]\n    ])\n    assert A[0, 0].subs(A, B) == B[0, 0]\n    assert A[1, 1].subs(A, B) == B[1, 1]\n    assert (A[0, 1] + A[1, 0]).subs(A, B) == B[0, 1] + B[1, 0]\n\n\n", "def test_matrix_indexing():\n    A = MatrixSymbol('A', 3, 3)\n    B = MatrixSymbol('B', 2, 2)\n    \n    assert A[1, 2].shape == (1, 1)\n    assert A[0:2, 1:3].shape == (2, 2)\n    assert A[0, :].shape == (3, )\n    assert A[:, 1].shape == (3, 1)\n    assert A[:2, 2].shape == (2, 1)\n    \n    raises(IndexError, lambda: A[3, 1])\n    raises(IndexError, lambda: A[1, 3])\n    raises(IndexError, lambda: A[3:, 1])\n    raises(IndexError, lambda: A[1, 3:])\n\n    assert B[0, :] == B[0, slice(None)]\n    assert A[1, 1:3] == A[1, slice(1, 3)]\n    assert A[1:2, :] == A[slice(1, 2), :]\n    assert A[1::2, 1::2] == A[slice(1, None, 2), slice(1, None, 2)]\n\n\n", "def test_matrix_subs():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    C = MatrixSymbol('C', 2, 2)\n    M = Matrix([[1, 2], [3, 4]])\n\n    assert (A + B).subs({A: M, B: M}).doit() == Matrix([[2, 4], [6, 8]])\n    assert (A*B).subs({A: M, B: M}).doit() == Matrix([[ 7, 10], [15, 22]])\n    assert (A.T).subs({A: M}).doit() == Matrix([[1, 3], [2, 4]])\n\n    assert (A + C).subs({A: M, C: B}).doit() == Matrix([[1, 2], [3, 4]]) + B\n    assert (A*C).subs({A: M, C: B}).doit() == M*B\n\n\n", "def test_transpose_symbolic():\n    A = MatrixSymbol('A', n, m)\n    assert A.T.T == A\n    assert (A*B).T == B.T*A.T\n    assert (A*B).T == transpose(A*B)\n    assert (A + B).T == A.T + B.T\n    assert (A - B).T == A.T - B.T \n\n\n\n\n"], "sample_353": ["    def test_create_permissions_in_multiple_databases(self):\n        new_io = StringIO()\n        with settings(DATABASE_ROUTERS=[\n            'auth_tests.Router'\n        ]):\n            with patch('django.db.connection', spec=True) as connection:\n                connection.settings_dict = {'name': 'other'}\n                create_permissions(self.app_config, verbosity=0, database='other',\n                                  stdout=new_io)\n                self.assertEqual(new_io.getvalue().strip(), 'Superuser created successfully.')\n                connection.settings_dict = {'name': 'default'}\n                create_permissions(self.app_config, verbosity=0, database='default',\n                                  stdout=new_io)\n                self.assertEqual(new_io.getvalue().strip(), 'Superuser created successfully.')\n", "    def test_permission_with_proxy_content_type_created(self):\n        opts = UserProxy._meta\n        codename = get_permission_codename('add', opts)\n        self.assertTrue(\n            Permission.objects.filter(\n                content_type__model=opts.model_name,\n                content_type__app_label=opts.app_label,\n                codename=codename,\n            ).exists()\n        )\n", "    def test_create_permissions_handles_custom_permissions_app(self):\n        # Ensure permissions are created for a custom app.\n        class CustomPermission(Permission):\n            pass\n        try:\n            CustomPermission._meta.verbose_name = \"Custom Permission\"\n            apps.register_model('auth_tests', CustomPermission)\n\n            # ...\n            app_config = apps.get_app_config('auth_tests')\n            create_permissions(app_config, verbosity=0)\n        finally:\n            apps.unregister_model('auth_tests', CustomPermission)\n", "    def test_create_permissions_with_custom_permissions_objects(self):\n        \"\"\"\n        create_permissions() should correctly handle custom Permission objects.\n        \"\"\"\n        custom_permissions = [\n            Permission(\n                name='my_custom_permission',\n                content_type=ContentType.objects.get_for_model(User),\n                codename='my_custom_codename',\n                default=False,\n            ),\n            Permission(\n                name='another_custom_permission',\n                content_type=ContentType.objects.get_for_model(Group),\n                codename='another_custom_codename',\n                default=False,\n            ),\n        ]\n        with mock.patch('auth.management.commands.createsuperuser.Permission.objects.all',\n                        return_value=custom_permissions):\n            create_permissions(self.app_config, verbosity=0)\n\n        self.assertEqual(\n            Permission.objects.count(),\n            2  \n        )\n\n        first_permission = Permission.objects.get(codename='my_custom_codename')\n        self.assertEqual(first_permission.name, 'my_custom_permission')\n\n        second_permission = Permission.objects.get(codename='another_custom_codename')\n        self.assertEqual(second_permission.name, 'another_custom_permission')\n", "    def test_create_permissions_with_custom_permissions(self):\n        # Adding custom permissions should not overwrite existing ones.\n        original_permissions = Permission._meta.permissions[:]\n        \n        custom_permissions = [\n            ('my_custom_permission', 'Some custom permission'),\n            ('another_custom_permission', 'Another custom permission'),\n        ]\n        Permission._meta.permissions = original_permissions + custom_permissions\n        \n        create_permissions(self.app_config, verbosity=0)\n\n        # Check if both default and custom permissions are present.\n        self.assertEqual(Permission.objects.all().count(), len(original_permissions) + len(custom_permissions) )\n"], "sample_532": ["compilation error", "def test_contour_with_bad_data():\n    with pytest.raises(ValueError, match=\"Input data must be a 2D array\"):\n        plt.contour([1, 2, 3])\n    with pytest.raises(ValueError, match=\"Input data must be a 2D array\"):\n        plt.contour([[1, 2], [3, 4]])\n\n\n", "def test_contour_cbar_orientation():\n    fig, ax = plt.subplots()\n    x = np.arange(10)\n    y = np.arange(10)\n    X, Y = np.meshgrid(x, y)\n    Z = np.sin(X * Y)\n    cs = ax.contour(X, Y, Z)\n    fig.colorbar(cs, orientation='vertical')\n    fig.colorbar(cs, orientation='horizontal')\n    fig.colorbar(cs, orientation='horizontal', shrink=0.5)\n    fig.colorbar(cs, orientation='vertical', shrink=0.5)\n\n\n\n", "compilation error", "def test_contour_with_masked_data():\n    x = np.arange(10)\n    y = np.arange(10)[:, None]\n    X, Y = np.meshgrid(x, y)\n    Z = np.exp(-(X**2 + Y**2))\n\n    mask = np.zeros_like(Z, dtype=bool)\n    mask[5, 5] = True\n    masked_Z = np.ma.masked_array(Z, mask=mask)\n\n    fig, ax = plt.subplots()\n    cs = ax.contour(X, Y, masked_Z)\n    plt.colorbar(cs)\n    assert len(cs.collections) > 0\n\n\n\n"], "sample_2": ["def test_wcs_from_header(tmpdir):\n    # Test if a CCDData object can be created with a header and WCS information\n    # is extracted correctly.\n    header = fits.Header()\n    header.update(\n        {'CTYPE1': 'RA---TAN', 'CTYPE2': 'DEC--TAN', 'CRVAL1': 10,\n         'CRVAL2': 20, 'CDELT1': 0.1, 'CDELT2': 0.1, })\n    ccd = CCDData(np.ones((10, 10)), header=header)\n    assert isinstance(ccd.wcs, WCS)\n    assert ccd.wcs.wcs[0, 0] == 0.1\n\n\n\n", "def test_to_hdu_with_custom_extension_names():\n    ccd_data = create_ccd_data()\n    ccd_data.write(\n        'test.fits',\n        hdu_mask='MYMASK',\n        hdu_uncertainty='MYUNCERTAINTY'\n    )\n    ccd_from_file = CCDData.read('test.fits')\n    hdulist = ccd_from_file.to_hdu(hdu_mask='MYMASK', hdu_uncertainty='MYUNCERTAINTY')\n    assert hdulist[0].name == 'PRIMARY'\n    assert hdulist[1].name == 'MYMASK'\n    assert hdulist[2].name == 'MYUNCERTAINTY'\n", "def test_read_from_header_no_wcs_keywords():\n    # Check that if the header has no WCS keywords it\n    # is treated as a regular image.\n    headers = [\n        fits.Header(\n            fits.CardList([\n                fits.Card('SIMPLE', 'T', comment='Standard fits file'),\n                fits.Card('BITPIX', '16', comment='Number of bits per pixel'),\n                fits.Card('NAXIS', '2', comment='Number of axes'),\n                fits.Card('NAXIS1', '100', comment='Size of axis 1'),\n                fits.Card('NAXIS2', '100', comment='Size of axis 2'),\n                fits.Card('CRPIX1', '50.0', comment='Reference pixel in X'),\n                fits.Card('CRPIX2', '50.0', comment='Reference pixel in Y'),\n                fits.Card('CDELT1', '0.1', comment='X Scale'),\n                fits.Card('CDELT2', '0.1', comment='Y Scale')\n            ])\n        )\n    ]\n    for header in headers:\n        ccd = CCDData.read_from_header(header)\n        assert ccd.wcs is None\n\n", "def test_read_invalid_wcs_header():\n    # Test that if the header has invalid WCS keywords\n    # it raises an appropriate error.\n    data_file = get_pkg_data_filename('data/invalid_wcs_header.fits')\n\n    with pytest.raises(fits_wcs.WCSParseError):\n        CCDData.read(data_file)\n", "compilation error"], "sample_718": ["def test_check_estimators_data_not_an_array():\n    from sklearn.datasets import make_classification\n    X, y = make_classification(n_samples=100, random_state=0)\n    y = [NotAnArray(np.asarray(y))]\n\n    # test that check_estimators_data_not_an_array raises ValueError\n\n    for estimator in [LinearRegression, SGDClassifier]:\n        try:\n            check_estimators_data_not_an_array(estimator, X, y)\n        except ValueError:\n            continue\n        raise AssertionError(\"ValueError not raised. Estimator should raise ValueError when data is not an array.\")\n", "def test_check_estimators_data_not_an_array():\n    # Test that estimators raise errors when provided with data that is\n    # not an array or sparse matrix\n\n    # Test that estimators raise an error for non-array data\n    estimator = NoCheckinPredict()\n    X_not_array = {\"a\": 1, \"b\": 2}\n    y_not_array = [1, 2, 3]\n\n    msg = (\"Estimator expects numpy.array or sparse matrix \"\n           \"input, got {} instead.\").format(type(X_not_array))\n    assert_raises_regex(TypeError, msg, estimator.fit, X_not_array, y_not_array)\n    msg = (\"Estimator expects numpy.array or sparse matrix \"\n           \"input, got {} instead.\").format(type(y_not_array))\n    assert_raises_regex(TypeError, msg, estimator.fit, X_not_array, y_not_array)\n\n    # Test that estimators raise an error when given a list as data\n\n\n", "def test_check_estimator_multi_output_sparse():\n    # Check handling of multi-output and sparse matrices\n    class MultiSparseClassifier(BaseEstimator):\n            pass\n\n            X = check_array(X)\n            y = check_array(y)\n            return self\n\n            return np.ones((X.shape[0], 2))\n\n    estimator = MultiSparseClassifier()\n    X = sp.csr_matrix([[1, 2, 3], [4, 5, 6]])\n    y = np.array([[0, 1], [2, 3]])\n    check_estimator(estimator, X, y)\n\n\n", "def test_check_estimators_data_not_an_array():\n    # check that estimator does not accept data in a dictionary\n    msg = 'Input data should be a NumPy array or a pandas DataFrame.'\n    for estimator in [GaussianMixture, LinearRegression,\n                      RandomForestClassifier, NMF, SGDClassifier,\n                      MiniBatchKMeans]:\n        with ignore_warnings(category=FutureWarning):\n            estimator_ = estimator()\n        estimator_.fit({1: 1}, {2: 2})\n        assert_raises_regex(AssertionError, msg, estimator_.predict, {})\n\n\n\n", "        def fit(self, X, y):\n            return self\n"], "sample_570": ["    def test_bootstrap_errorbars(self, long_df):\n\n        agg = EstimateAggregator(\"mean\", \"ci\", n_boot=1000, seed=0)\n        out = agg(long_df, \"y\")\n\n        agg_ref = EstimateAggregator(\"mean\", (\"ci\", \"percentile\"), n_boot=1000, seed=0)\n        out_ref = agg_ref(long_df, \"y\")\n\n        assert out[\"ymin\"] == pytest.approx(out_ref[\"ymin\"], abs=1e-2)\n        assert out[\"ymax\"] == pytest.approx(out_ref[\"ymax\"], abs=1e-2)\n", "    def test_discrete_variables(self, rng):\n\n        x = rng.choice(range(10), size=100)\n        agg = EstimateAggregator(\"mean\")\n        out = agg(pd.DataFrame(dict(x=x)), \"x\")\n        assert out[\"x\"] == x.mean()\n", "    def test_errorbar_label(self, method, err_type, long_df):\n        agg = EstimateAggregator(\"mean\", method)\n        out = agg(long_df, \"y\")\n\n        expected_label = f\"{err_type}_interval\" if err_type else \"mean\"\n        assert out[f\"ymin\"] == long_df[\"y\"].min()\n        assert out[f\"ymax\"] == long_df[\"y\"].max()\n\n\n\n", "    def test_ci_bootstrapping(self, long_df):\n        agg = EstimateAggregator(\"mean\", \"ci\", n_boot=10000)\n        out = agg(long_df, \"y\")\n\n        # Check if 95% CI is approximately correct\n        assert 0.95 < (out[\"ymax\"] - out[\"ymin\"]) / (long_df[\"y\"].max() - long_df[\"y\"].min()) < 1.05  \n\n", "    def test_bivariate_errorbars(self, long_df):\n\n        agg = EstimateAggregator(\"mean\", \"se\")\n        out = agg(long_df, [\"x\", \"y\"])\n        assert out[\"x\"] == long_df[\"x\"].mean()\n        assert out[\"xmin\"] == (long_df[\"x\"].mean() - long_df[\"x\"].sem())\n        assert out[\"xmax\"] == (long_df[\"x\"].mean() + long_df[\"x\"].sem())\n        assert out[\"y\"] == long_df[\"y\"].mean()\n        assert out[\"ymin\"] == (long_df[\"y\"].mean() - long_df[\"y\"].sem())\n        assert out[\"ymax\"] == (long_df[\"y\"].mean() + long_df[\"y\"].sem())\n\n        agg = EstimateAggregator(\"mean\", (\"sd\", 2))\n        out = agg(long_df, [\"x\", \"y\"])\n        assert out[\"x\"] == long_df[\"x\"].mean()\n        assert out[\"xmin\"] == (long_df[\"x\"].mean() - 2 * long_df[\"x\"].std())\n        assert out[\"xmax\"] == (long_df[\"x\"].mean() + 2 * long_df[\"x\"].std())\n        assert out[\"y\"] == long_df[\"y\"].mean()\n        assert out[\"ymin\"] == (long_df[\"y\"].mean() - 2 * long_df[\"y\"].std())\n        assert out[\"ymax\"] == (long_df[\"y\"].mean() + 2 * long_df[\"y\"].std())\n\n"], "sample_11": ["def test_coupled_world_slicing_2d():\n    fits_wcs = WCS(header=COUPLED_WCS_HEADER)\n    sl = SlicedLowLevelWCS(fits_wcs, np.s_[:, :, None])\n    world = fits_wcs.pixel_to_world_values(0, 0, [0])\n    out_pix = sl.world_to_pixel_values(world[0], world[1], [0])\n\n    assert np.allclose(out_pix[0], 0)\n\n\n\n", "compilation error", "def test_coupled_world_slicing_multiple():\n    fits_wcs = WCS(header=COUPLED_WCS_HEADER)\n    sl = SlicedLowLevelWCS(fits_wcs, np.s_[[0,1], [0, 1], [0, 1]])\n    world = fits_wcs.pixel_to_world_values(0,0,0)\n    for pix in sl.world_to_pixel_values(world[0], world[1]):\n        assert np.allclose(pix, [0, 0])\n", "def test_coupled_world_slicing_different_axes():\n    fits_wcs = WCS(header=COUPLED_WCS_HEADER)\n    sl = SlicedLowLevelWCS(fits_wcs, np.s_[0:1, 0:1, :])\n    world = fits_wcs.pixel_to_world_values(0,0,0)\n    out_pix = sl.world_to_pixel_values(world[0], world[1], 0)\n\n    assert np.allclose(out_pix[0], 0)\n", "compilation error"], "sample_631": ["    def test_module_attribute_not_used(self):\n        node = astroid.parse(\n            \"\"\"\n        import module\n        \"\"\"\n        )\n\n        with self.assertAddsMessages(\n            Message(\"unused-import\", node=node.body[0], args=(\"module\",))\n        ):\n            self.walk(node)\n", "    def test_redefined_builtin_in_nested_function(self):\n        node = astroid.extract_node(\n            \"\"\"\n            from os import open\n                pass\n            inner()\n        outer()\n        \"\"\"\n        )\n        with self.assertNoMessages():\n            self.checker.visit_module(node.root())\n            self.checker.visit_functiondef(node.children[0])\n            self.checker.leave_functiondef(node.children[0])\n\n\n\n", "    def test_future_builtins_and_undefined_vars(self):\n        node = astroid.parse(\n            \"\"\"\n        from future.builtins import open\n\n            print(missing)\n        \"\"\"\n        )\n        with self.assertAddsMessages(\n            Message(\"undefined-variable\", node=node.body[1].args[0], args=(\"missing\",))\n        ):\n            self.walk(node)\n", "    def test_unused_self(self):\n        node = astroid.parse(\n            \"\"\"\n        class MyClass:\n                pass\n        \"\"\"\n        )\n        with self.assertNoMessages():\n            self.walk(node)\n", "    def test_redefined_builtin_with_scope(self):\n        node = astroid.parse(\n            \"\"\"\n            from os import open\n        func()\n        \"\"\"\n        )\n        with self.assertNoMessages():\n            self.walk(node)\n"], "sample_1011": ["def test_polygamma_printing():\n    n = Symbol('n')\n    assert octave_code(polygamma(n, x)) == 'polygamma(n, x)'\n", "def test_log_base_printing():\n    assert octave_code(log(x, 2)) == 'log2(x)'\n    assert octave_code(log(x, 10)) == 'log10(x)'\n    assert octave_code(log(x, e)) == 'log(x)'\n", "def test_poly_printing():\n    assert mcode(poly(x)) == \"poly(x)\"\n    assert mcode(poly(x, 3)) == \"poly(x, 3)\"\n    assert mcode(poly(x, 3, [1, 2, 3, 4])) == \"poly(x, 3, [1 2 3 4])\"\n    assert mcode(poly2str(x)) == \"x\"\n    assert mcode(poly2str(x, 3)) == \"x^3\"\n    assert mcode(poly2str(x, 3, [1, 2, 3, 4])) == \"4*x^3 + 3*x^2 + 2*x + 1\"\n", "def test_sympify():\n    from sympy import sympify\n    assert mcode(sympify('x^2')) == 'x.^2'\n    assert mcode(sympify('a + b*c')) == 'a + b.*c'\n    assert mcode(sympify('sin(x) * cos(y)')) == 'sin(x).*cos(y)'\n    assert mcode(sympify('2*x/y')) == '2*x./y'\n    assert mcode(sympify('sqrt(x)')) == 'sqrt(x)'\n    assert mcode(sympify('log(x)')) == 'log(x)'\n    assert mcode(sympify('4*(x + y)')) == '4*(x + y)'\n\n\n\n", "def test_log2():\n    assert mcode(log2(x)) == \"log2(x)\"\n    assert mcode(log2(2*x)) == \"log2(2.*x)\"\n    assert mcode(log2(x/2)) == \"log2(x/2)\"\n"], "sample_37": ["def test_multi_wcs():\n    \"\"\"\n    Test that multi-wcs works\n    \"\"\"\n    hdu = fits.PrimaryHDU(np.zeros((100, 100)), header=get_pkg_data_contents('data/multiwcs.hdr'))\n    hdu.header['SIMPLE'] = 'T'\n    hdu.header['BITPIX'] = 8\n    hdu.header['NAXIS'] = 2\n    wcs_list = wcs.WCSList(hdu)\n    assert len(wcs_list) == 2\n\n    w1 = wcs_list[0]\n    w2 = wcs_list[1]\n\n    assert w1.naxis == 2\n    assert w2.naxis == 2\n\n\n\n    ra, dec = w1.wcs_pix2world([50, 50], 0)\n    assert not np.isnan(ra)\n    assert not np.isnan(dec)\n\n    ra, dec = w2.wcs_pix2world([50, 50], 0)\n    assert not np.isnan(ra)\n    assert not np.isnan(dec)\n\n\n\n", "compilation error", "def test_invalid_projection():\n    \"\"\"\n    Test for invalid projections.\n    \"\"\"\n    w = wcs.WCS(naxis=2)\n    w.wcs.ctype = [\"RA---TAN\", \"DEC--TAN\"]\n    w.wcs.cdelt = [0.1, 0.1]\n    w.wcs.crval = [-90, 90]\n    w.wcs.crpix = [1, 1]\n    with pytest.raises(ValueError):\n        w.wcs_world2pix([1, 2], 0)\n\n\n", "compilation error", "    def test_invalid_sip_header():\n        # See https://github.com/astropy/astropy/issues/7485\n        hdr = get_pkg_data_contents(\"data/sip-invalid.hdr\")\n        with pytest.raises(wcs.InvalidSIPHeaderError):\n            wcs.WCS(hdr)\n"], "sample_838": ["def test_column_transformer_dict_feature_names():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    ct = ColumnTransformer([('trans1', Trans(), [0]), ('trans2', Trans(), [1])],\n                           remainder='passthrough')\n    ct.fit(X_array)\n    feature_names = ct.get_feature_names_out()\n    assert feature_names == ['trans1__0', 'trans1__1', 'trans2__1', 'trans2__2']\n\n    ct = ColumnTransformer([('trans1', DictVectorizer(), [0]), ('trans2', DictVectorizer(), [1])],\n                           remainder='passthrough')\n    ct.fit(X_array.tolist())\n    feature_names = ct.get_feature_names_out()\n    assert feature_names == ['trans1__0', 'trans1__1', 'trans2__1', 'trans2__2']\n\n", "def test_column_transformer_sparse_threshold_behaviour():\n    X_array = np.array([[0, 1, 2], [2, 4, 6], [8, 6, 4]]).astype('float').T\n\n    class SparseMatrixTrans:\n            return self\n\n            return sparse.csr_matrix(X)\n\n    ct = ColumnTransformer([('trans1', StandardScaler(), [0])],\n                           remainder=SparseMatrixTrans(),\n                           sparse_threshold=0.8)\n\n    X_trans = ct.fit_transform(X_array)\n    assert isinstance(X_trans, sparse.csr_matrix)\n    assert X_trans.nnz > 0\n\n    ct = ColumnTransformer([('trans1', StandardScaler(), [0])],\n                           remainder=SparseMatrixTrans(),\n                           sparse_threshold=1.0)\n    X_trans = ct.fit_transform(X_array)\n    assert isinstance(X_trans, sparse.csr_matrix)\n    assert X_trans.nnz == 0\n\n    ct = ColumnTransformer([('trans1', StandardScaler(), [0])],\n                           remainder='passthrough',\n                           sparse_threshold=0.8)\n    X_trans = ct.fit_transform(X_array)\n    assert not isinstance(X_trans, sparse.csr_matrix)\n    assert X_trans.shape == X_array.shape\n", "compilation error", "def test_column_transformer_sparse_transformer_remainder():\n    X_array = sp.csr_matrix([[1, 2, 3], [4, 5, 6]])\n    ct = ColumnTransformer([('trans1', Trans(), [0])],\n                           remainder=SparseMatrixTrans(),\n                           sparse_threshold=0.5)\n\n    X_trans = ct.fit_transform(X_array)\n    assert sparse.issparse(X_trans)\n    assert X_trans.shape == (2, 6)\n    # SparseMatrixTrans creates 3 features for each column\n    assert_array_equal(X_trans.toarray(), np.hstack((\n        X_array.toarray(), np.eye(2))))\n\n    assert len(ct.transformers_) == 2\n    assert ct.transformers_[-1][0] == 'remainder'\n    assert isinstance(ct.transformers_[-1][1], SparseMatrixTrans)\n    assert_array_equal(ct.transformers_[-1][2], [1, 2])\n", "def test_column_transformer_sparse_threshold_categorical_remainder():\n    X_array = [[0, 1, 2], [2, 4, 6]]\n\n    ct = ColumnTransformer(\n        [('trans1', {'feature_transform': StandardScaler()}, [0, 1]),\n         ('trans2', DictVectorizer(), [2])],\n        remainder='passthrough',\n        sparse_threshold=0.5)\n    \n    X_trans = ct.fit_transform(X_array)\n    assert sparse.issparse(X_trans)\n\n    #  Check that trans2 remains sparse\n    assert X_trans[:, 2:].nnz == X_array[:, 2:].size\n\n\n"], "sample_1183": ["    def f(x):\n        return exp(x)", "def test_fraction_field():\n    K = ZZ[x]\n    F = K.frac_field()\n    assert isinstance(F, FractionField)\n    assert F.domain == K\n\n    a = K(2)\n    b = K(x + 1)\n    f = F(a, b)\n\n    assert f.numerator == a\n    assert f.denominator == b\n    assert f.as_expr() == a/b\n    assert str(f) == '2/(x + 1)'\n\n    assert f * 2 == F(2*a, b)\n    assert f + 1 == F(a + b, b)\n    assert f // K(x) == F(a*K(x), b*K(x))\n    assert F(a, b) == F(a, b)\n    assert F(a, b) != F(a + 1, b)\n    raises(ZeroDivisionError, lambda: F(a, K(0)))\n    raises(TypeError, lambda: f + x)\n    raises(TypeError, lambda: f * x)\n    raises(TypeError, lambda: f / x)\n    raises(TypeError, lambda: f // x)\n\n\n\n", "def test_Domain_abs():\n    I = S.ImaginaryUnit\n    a, b = [CC.convert(x) for x in (2 + I, -5)]\n    assert CC.abs(a) == sqrt(5)\n    assert CC.abs(b) == 5\n", "def test_issue_22944():\n    K = ZZ[x]\n    Q = K.frac_field()\n    a = Q(2*x + 1)/(x**2 + 1)\n    b = Q(x)\n    assert Q.gcd(a, b) == Q(1)\n    assert Q.lcm(a, b) == Q(2*x + 1)\n", "def test_fractional_field():\n    for K in [ZZ, QQ, RR]:\n        assert K.frac_field(x).is_FractionField(K)\n        F = K.frac_field(x)\n        assert F(1//2) == F(1)/F(2)\n        assert F(1//3) == F(1)/F(3)\n        assert F(1) == K(1)\n        assert F(0) == K(0)\n        assert F(x) == F.gen(0)\n\n    for K in [ZZ_I, QQ_I]:\n        assert K.frac_field(x).is_FractionField(K)\n        F = K.frac_field(x)\n        assert F(1//2) == F(1)/F(2)\n        assert F(1//3) == F(1)/F(3)\n        assert F(1) == K(1)\n        assert F(0) == K(0)\n        assert F(x) == F.gen(0)\n\n\n\n"], "sample_177": ["    def test_inheritance_circular_m2m(self):\n        A = self.create_model(\"A\", foreign_keys=[models.ManyToManyField('B', through='T')])\n        B = self.create_model(\"B\", foreign_keys=[models.ManyToManyField('A', through='T')])\n        T = self.create_model(\"T\", foreign_keys=[\n            models.ForeignKey('A', models.CASCADE),\n            models.ForeignKey('B', models.CASCADE),\n        ])\n        self.assertRelated(A, [B, T])\n        self.assertRelated(B, [A, T])\n\n\n", "    def test_m2m_through_with_base(self):\n        A = self.create_model(\"A\", foreign_keys=[models.ManyToManyField('B', through='T')])\n        B = self.create_model(\"B\")\n        S = self.create_model(\"S\")\n        T = self.create_model(\"T\", foreign_keys=[\n            models.ForeignKey('A', models.CASCADE),\n            models.ForeignKey('B', models.CASCADE),\n        ], bases=(S,))\n        self.assertRelated(A, [B, S, T])\n        self.assertRelated(B, [A, S, T])\n        self.assertRelated(S, [A, B, T])\n        self.assertRelated(T, [A, B, S])\n", "    def test_multiple_mixed_bases_with_fk_to_abstract(self):\n        A = self.create_model(\"A\", abstract=True)\n        B = self.create_model(\"B\", bases=(A,))\n        C = self.create_model(\"C\", abstract=True)\n        D = self.create_model(\"D\", bases=(B, C), foreign_keys=[models.ForeignKey('E', models.CASCADE)])\n        E = self.create_model(\"E\")\n        self.assertRelated(A, [B, D])\n        self.assertRelated(B, [D])\n        self.assertRelated(C, [D])\n        self.assertRelated(D, [E])\n        self.assertRelated(E, [])\n", "    def test_inherited_abstract_bases(self):\n        A = self.create_model(\"A\", abstract=True)\n        AB = self.create_model(\"AB\", bases=(A,), abstract=True)\n        B = self.create_model(\"B\", bases=(AB,))\n        self.assertRelated(A, [AB, B])\n        self.assertRelated(AB, [B])\n        self.assertRelated(B, [])\n", "    def test_field_choices(self):\n        A = self.create_model(\"A\", foreign_keys=[models.ForeignKey('B', models.CASCADE)])\n        B = self.create_model(\"B\")\n        class ChoiceField(models.CharField):\n                super().__init__(**kwargs)\n                self.choices = choices\n        B._meta.fields.append(ChoiceField(choices=[('a', 'A'), ('b', 'B')]))\n        self.assertRelated(A, [B])\n        self.assertEqual(B._meta.get_field('foo').choices, [('a', 'A'), ('b', 'B')])\n"], "sample_881": ["def test_label_ranking_average_precision_score_with_weights():\n    \"\"\"Test label_ranking_average_precision_score with weights.\"\"\"\n    n_samples = 10\n    n_classes = 3\n    y_true = np.random.randint(n_classes, size=(n_samples, n_classes))\n    y_score = np.random.rand(n_samples, n_classes)\n    weights = np.random.rand(n_samples)\n\n    # Compute average precision with and without weights\n    without_weights = label_ranking_average_precision_score(\n        y_true, y_score\n    )\n    with_weights = label_ranking_average_precision_score(\n        y_true, y_score, sample_weight=weights\n    )\n    assert without_weights != with_weights\n\n\n\n", "def test_label_ranking_average_precision_score_empty_set():\n    y_true = np.array([], dtype=int)\n    y_score = np.array([])\n    assert label_ranking_average_precision_score(y_true, y_score) == 0.0\n", "def test_label_ranking_average_precision_score_handles_empty_predictions():\n    y_true = np.array([0, 1, 2])\n    y_score = np.array([])\n    with pytest.raises(ValueError, match=\"y_score is empty\"):\n        label_ranking_average_precision_score(y_true, y_score)\n\n    y_true = csr_matrix([\n        [1, 0, 0],\n        [0, 1, 0],\n        [0, 0, 1]\n    ])\n    y_score = np.array([])\n    with pytest.raises(ValueError, match=\"y_score is empty\"):\n        label_ranking_average_precision_score(y_true, y_score)\n\n\n", "    def test_top_k_accuracy_score_multiclass_with_labels_and_ties(\n        y_true, y_score, labels, k", "def test_label_ranking_average_precision_score_with_ties_handling():\n    # Check that ties among labels are handled correctly.\n    y_true = np.array([[1, 0, 1], [0, 1, 0], [1, 1, 0]])\n    y_score = np.array(\n        [\n            [0.7, 0.2, 0.3],\n            [0.3, 0.7, 0.1],\n            [0.6, 0.5, 0.4],\n        ]\n    )\n    expected_ap = (\n        2 / 3\n        * (  # For the first example: 2nd label is correct\n            (1 / 2)\n            + (1 / 3)\n        )\n        + 1 / 3\n        * (  # For the second example: 2nd label is correct\n            (1 / 2)\n        )\n    )\n    ap = label_ranking_average_precision_score(y_true, y_score)\n    assert ap == pytest.approx(expected_ap)\n"], "sample_107": ["    def test_cleanse_setting_in_list(self):\n        initial = ['item1', 'item2', 'password']\n        expected = ['item1', 'item2', CLEANSED_SUBSTITUTE]\n        self.assertEqual(cleanse_setting('SETTING_NAME', initial), expected)\n", "    def test_cleanse_setting_complex_nested_dict(self):\n        initial = {'user': {'name': 'jane', 'password': 'pass123'}}\n        expected = {'user': {'name': 'jane', 'password': CLEANSED_SUBSTITUTE}}\n        self.assertEqual(cleanse_setting('USER_DETAILS', initial), expected)\n\n\n", "    def test_cleanse_setting_with_nested_dicts(self):\n        initial = {\n            'some_key': {\n                'secret_password': 'top_secrets',\n                'other_key': 'not_sensitive',\n            },\n            'another_key': 'some_value'\n        }\n        expected = {\n            'some_key': {\n                'secret_password': CLEANSED_SUBSTITUTE,\n                'other_key': 'not_sensitive',\n            },\n            'another_key': 'some_value'\n        }\n        self.assertEqual(cleanse_setting('SETTING_NAME', initial), expected)\n", "    def test_cleanse_setting_recurses_in_nested_dictionaries(self):\n        initial = {'a': {'b': {'password': 'secret_data'}}, 'c': 'test'}\n        expected = {'a': {'b': {'password': CLEANSED_SUBSTITUTE}}, 'c': 'test'}\n        self.assertEqual(cleanse_setting('SETTING_NAME', initial), expected)\n", "    def test_cleanse_setting_handles_nested_dictionaries(self):\n        initial = {'app': {'database': {'username': 'admin', 'password': 'secret'}}}\n        expected = {'app': {'database': {'username': 'admin', 'password': CLEANSED_SUBSTITUTE}}}\n        self.assertEqual(cleanse_setting('APP_DATABASE_PASSWORD', initial), expected)\n"], "sample_897": ["def test_partial_dependence_features_error(\n    pyplot, clf_diabetes, diabetes, features, expected_err", "def test_partial_dependence_display_method_brute_uniform(\n    pyplot, clf_diabetes, diabetes, kind, method", "    def test_partial_dependence_display_with_categorical_features(\n        pyplot, clf_diabetes, diabetes", "def test_partial_dependence_display_with_sample_weight_different_lengths(\n    pyplot, clf_diabetes, diabetes, sample_weight", "    def test_plot_partial_dependence_with_categorical_interaction(\n        pyplot, categorical_features, array_type"], "sample_285": ["    def test_app_directories_finder(self):\n        with self.settings(STATICFILES_DIRS=[],\n                           INSTALLED_APPS=['app1', 'app2']):\n            finder = get_finder('django.contrib.staticfiles.finders.AppDirectoriesFinder')\n            app1_storage = FileSystemStorage(location=os.path.join(TEST_ROOT, 'app1/static'))\n            app2_storage = FileSystemStorage(location=os.path.join(TEST_ROOT, 'app2/static'))\n            self.assertEqual(list(finder.list(None)), [\n                ('/static/a.css', app1_storage),\n                ('/static/b.js', app1_storage),\n                ('/static/c.css', app2_storage),\n            ])\n", "    def test_find_in_specified_location(self):\n        finder = get_finder('django.contrib.staticfiles.finders.FileSystemFinder')\n        with self.assertNumQueries(1):\n            path = 'my_file.css'\n            result = finder.find(path)\n        expected_path = os.path.join(TEST_ROOT, 'project', 'static', 'my_file.css')\n        self.assertIsNotNone(result)\n        self.assertEqual(result, expected_path)\n\n\n\n", "    def test_find_from_staticfiles_dirs(self):\n        with self.settings(\n            STATICFILES_DIRS=[('prefix', Path(TEST_ROOT) / 'project' / 'static')],\n            STATIC_ROOT=Path(TEST_ROOT) / 'staticfiles',\n        ):\n            finder = get_finder(\n                'django.contrib.staticfiles.finders.FileSystemFinder'\n            )\n            self.assertEqual(finder.find('css/style.css'),\n                             Path(TEST_ROOT) / 'project' / 'static' / 'css' / 'style.css')\n\n            self.assertEqual(finder.find('images/logo.png', all=True),\n                             [Path(TEST_ROOT) / 'project' / 'static' / 'images' / 'logo.png'])\n\n", "    def test_find_in_app_directories(self):\n        with self.settings(STATICFILES_DIRS=[],\n                           INSTALLED_APPS=['app1', 'app2']):\n            with self.subTest('app1'):\n                with mock.patch('django.contrib.staticfiles.finders.FileSystemFinder.storages',\n                               new_dict={'app1': FileSystemStorage(location=TEST_ROOT)}):\n                    finder = get_finder('django.contrib.staticfiles.finders.AppDirectoriesFinder')\n                    self.assertEqual(finder.find('file.txt', all=False),\n                                     Path(TEST_ROOT, 'app1', 'static', 'file.txt'))\n\n            with self.subTest('app2'):\n                with mock.patch('django.contrib.staticfiles.finders.FileSystemFinder.storages',\n                               new_dict={'app2': FileSystemStorage(location=TEST_ROOT)}):\n                    finder = get_finder('django.contrib.staticfiles.finders.AppDirectoriesFinder')\n                    self.assertEqual(finder.find('file.txt', all=False),\n                                     Path(TEST_ROOT, 'app2', 'static', 'file.txt'))\n\n            with self.subTest('nonexistent_app'):\n                finder = get_finder('django.contrib.staticfiles.finders.AppDirectoriesFinder')\n                self.assertIsNone(finder.find('file.txt', all=False))\n\n\n\n", "    def test_find_location_handles_no_prefix(self):\n        finder = get_finder('django.contrib.staticfiles.finders.FileSystemFinder')\n        path = 'my_static_file.css'\n        result = finder.find_location(\n            Path(TEST_ROOT) / 'project' / 'documents', path, prefix=None\n        )\n        self.assertEqual(result, (Path(TEST_ROOT) / 'project' / 'documents' / path).resolve())\n\n        result = finder.find(path)\n        self.assertEqual(result, (Path(TEST_ROOT) / 'project' / 'documents' / path).resolve())\n"], "sample_859": ["def test_multi_task_lasso_cv_sparse_input():\n    n_samples, n_features = 10, 3\n    rng = np.random.RandomState(42)\n    X = rng.binomial(1, .5, size=(n_samples, n_features))\n    X = sparse.csr_matrix(X)\n    y = X[:, [0, 0]].copy()\n\n    est = MultiTaskLassoCV(n_alphas=5, fit_intercept=True).fit(X, y)\n    assert_array_almost_equal(est.coef_, [[1, 0, 0]] * 2, decimal=3) \n", "def test_multitask_enet_sparse_data():\n    X, y, _, _ = build_dataset(n_samples=100, n_features=100)\n    X_sparse = sparse.csr_matrix(X)\n    y_sparse = sparse.csr_matrix(y)\n    clf = MultiTaskElasticNet(alpha=0.5, tol=1e-8)\n    clf.fit(X_sparse, y_sparse)\n\n\n\n", "def test_multi_task_lasso_cv_dense_array():\n    n_samples, n_features = 100, 50\n    rng = np.random.RandomState(42)\n    X = rng.randn(n_samples, n_features)\n    y = rng.randn(n_samples, 2) \n\n    est = MultiTaskLassoCV(n_alphas=5, fit_intercept=True).fit(X, y)\n    assert_array_almost_equal(est.coef_[0], est.coef_[1], decimal=3)\n\n\n\n", "def test_enet_with_group_lasso_regularization():\n    X, y, _, _ = build_dataset(n_samples=10, n_features=5)\n    for penalty in ['elasticnet', 'l1']:\n        for group_counts in [(1, 0, 1, 0, 1), (2, 2, 1, 1, 2)]:\n            group_l1_ratio = 0.5 if penalty == 'elasticnet' else 0\n\n            clf = ElasticNet(\n                alpha=0.1, l1_ratio=group_l1_ratio,\n                group_l1_ratio=group_l1_ratio,\n                group_penalty='l1' if penalty == 'l1' else 'elasticnet', random_state=42)\n            clf.fit(X, y)\n            assert_is_increasing(clf.coef_.ravel(), \"Coefficients should be increasing\")\n\n\n\n", "def test_multi_task_lasso_cv_sparse_input():\n    n_samples, n_features = 10, 3\n    rng = np.random.RandomState(42)\n    X = rng.binomial(1, .5, size=(n_samples, n_features))\n    X = sparse.csr_matrix(X)\n    y = X[:, [0, 0]].copy()\n    est = MultiTaskLassoCV(n_alphas=5, fit_intercept=True).fit(X, y)\n    assert_array_almost_equal(est.coef_, [[1, 0, 0]] * 2, decimal=3)\n"], "sample_1171": ["def test_issue_18401():\n    assert ImageSet(Lambda(x, x*x), S.Complexes) == ImageSet(Lambda(x, x*x), S.Complexes)\n", "def test_issue_17860():\n    r = Range(-oo, oo)\n    assert r[1:3] == Range(1, 3)\n    assert r[1:-1] == Range(1, -1)\n    assert r[:3] == Range(-oo, 3)\n    assert r[1:] == Range(1, oo)\n    r = Range(oo,-oo,-1)\n    assert r[1:3] == Range(oo, -3, -1)\n    assert r[1:-1] == Range(oo, -2, -1)\n    assert r[:3] == Range(oo, -3, -1)\n    assert r[1:] == Range(oo, -oo, -1)\n\n", "def test_issue_19147():\n    assert ImageSet(Lambda(x, 2*x + 3*I), S.Reals) == ImageSet(Lambda(x, 2*x + 3*I), S.Reals)\n    assert ImageSet(Lambda(x, 3*x + 8 + 5*I), S.Reals) == ImageSet(Lambda(x, 3*x + 2 + 5*I), S.Reals)\n", "def test_issue_17860():\n    raises(ValueError, lambda: Range(-oo, oo, oo))\n    raises(ValueError, lambda: Range(oo, -oo, -oo))\n    raises(ValueError, lambda: Range(1, 2, -1))\n    raises(ValueError, lambda: Range(-2, 1, 2))\n", "def test_issue_18135():\n    c1 = ComplexRegion.from_real(Interval(0, 1))\n    c2 = ComplexRegion.from_real(Interval(1, 2))\n    c3 = Union(c1, c2)\n    assert c3.measure == 1\n    c1 = ComplexRegion.from_real(Interval(0, 1) * Interval(0, 2 * pi))\n    c2 = ComplexRegion.from_real(Interval(1, 2) * Interval(0, 2 * pi))\n    c3 = Union(c1, c2)\n    assert c3.measure == 2 * pi\n\n\n"], "sample_1106": ["def test_mat_mul_with_zero_matrix():\n    A = MatrixSymbol('A', 3, 2)\n    B = MatrixSymbol('B', 2, 3)\n    Z = ZeroMatrix(3, 3)\n    assert MatMul(A, Z) == ZeroMatrix(3, 3)\n    assert MatMul(Z, B) == ZeroMatrix(3, 3)\n    assert MatMul(A, B, Z) == ZeroMatrix(3, 3)\n", "def test_matmul_shape_error():\n    A = MatrixSymbol('A', 2, 3)\n    B = MatrixSymbol('B', 3, 4)\n    C = MatrixSymbol('C', 2, 4)\n    with pytest.raises(ShapeError):\n        MatMul(A, B, C) \n", "def test_shape_properties():\n    A = MatrixSymbol('A', 2, 3)\n    B = MatrixSymbol('B', 3, 4)\n    C = MatMul(A, B)\n    assert C.shape == (2, 4)\n    assert C.rows == 2\n    assert C.cols == 4\n    D = MatMul(B, A)\n    assert D.shape == (3, 3)\n", "def test_matmul_transpose_simplify():\n    A = MatrixSymbol('A', 2, 3)\n    B = MatrixSymbol('B', 3, 2)\n    assert MatMul(A, B).transpose() == MatMul(B.transpose(), A.transpose())\n    assert MatMul(A, B).transpose().doit(deep=True) == MatMul(B.transpose(), A.transpose()).doit(deep=True) \n", "def test_matmul_with_zero_matrices():\n    A = Matrix([[1, 2], [3, 4]])\n    B = ZeroMatrix(2, 2)\n    assert MatMul(A, B) == ZeroMatrix(2, 2)\n    assert MatMul(B, A) == ZeroMatrix(2, 2)\n"], "sample_730": ["def test_sparse_multioutput_enet_cv():\n    X, y, _, _ = build_dataset(n_samples=50, n_features=50, n_targets=3)\n    y = np.hstack((y[:, np.newaxis], y[:, np.newaxis], y[:, np.newaxis]))\n    sparse_X = sparse.csr_matrix(X)\n\n    clf = MultiTaskElasticNetCV(n_alphas=5, eps=2e-3, l1_ratio=[0.5, 0.7])\n    clf.fit(sparse_X, y)\n    assert_array_almost_equal(clf.coef_.shape, (3, 50))\n    assert_array_almost_equal(clf.intercept_.shape, (3,))\n\n\n\n", "def test_enet_warm_start_with_l1_ratio():\n    X, y, _, _ = build_dataset(n_samples=50, n_features=20)\n    enet = ElasticNet(alpha=0.1, l1_ratio=0.5, tol=1e-8, warm_start=True)\n    enet.fit(X, y)\n    enet.set_params(l1_ratio=0.2)\n    enet.fit(X, y)\n    assert_not_equal(enet.coef_, enet.coef_)  # Coefficient should have changed\n\n\n\n", "def test_enet_precompute_with_sparse_y():\n    X, y, _, _ = build_dataset()\n    X_sparse = sparse.csr_matrix(X)\n    y_sparse = sparse.csr_matrix(y)\n\n    enet = ElasticNet(precompute=True)\n    enet.fit(X_sparse, y_sparse)\n    assert_raises(ValueError, enet.predict, X)\n    assert_raises(ValueError, enet.predict, X_sparse)\n    assert_raises(ValueError, enet.predict, sparse.csc_matrix(X))\n\n\n", "compilation error", "def test_enet_multioutput_sparse_input():\n    X, y, _, _ = build_dataset(n_samples=50, n_features=10)\n    y = y[:, np.newaxis] \n    X_sparse = sparse.csr_matrix(X)\n\n    clf = ElasticNetCV(n_alphas=5, eps=2e-3, l1_ratio=[0.3, 0.7])\n    clf.fit(X, y)\n    clf_sparse = ElasticNetCV(n_alphas=5, eps=2e-3, l1_ratio=[0.3, 0.7])\n    clf_sparse.fit(X_sparse, y)\n    assert_array_almost_equal(clf.coef_, clf_sparse.coef_[0], decimal=6)\n    assert_array_almost_equal(clf.intercept_, clf_sparse.intercept_[0], decimal=6)\n"], "sample_140": ["    def test_sensitive_variables_decorator_with_no_args(self):\n        @sensitive_variables\n            pass\n        with self.assertRaises(ValueError):\n            test_func()\n", "    def test_sensitive_variables_decorator_on_method(self):\n        class View(object):\n            @sensitive_variables\n                pass\n        view = View()\n        with self.settings(DEBUG=True):\n            response = self.client.get('/view/', HTTP_X_REQUESTED_WITH='XMLHttpRequest')\n        self.assertNotContains(response, password)\n", "    def test_sensitive_variables_decorator_with_none_argument(self):\n        @sensitive_variables\n            return password\n\n        self.assertEqual(test_func('secret'), 'secret')\n        self.assertEqual(test_func(None), CLEANSED_SUBSTITUTE) \n", "    def test_sensitive_variables_decorator_on_method(self):\n        class MyClass:\n            @sensitive_variables\n                return f'Password: {password}'\n\n        instance = MyClass()\n        with self.settings(DEBUG=True):\n            with self.assertRaises(Exception):\n                instance.my_method('secret')\n        with self.settings(DEBUG=False):\n            result = instance.my_method('secret')\n            self.assertIn('Password:', result)\n            self.assertNotContains(result, 'secret')\n", "    def test_sensitive_variables_decorator_with_multiple_args(self):\n        @sensitive_variables\n            pass\n\n        # Should not include email or password in the debug report\n        with self.settings(DEBUG=True):\n            response = self.client.get('/some_url/', {'email': 'test@example.com', 'password': 'secret', 'name': 'John Doe'})\n            self.assertNotContains(response, 'test@example.com', status_code=500)\n            self.assertNotContains(response, 'secret', status_code=500)\n\n        with self.settings(DEBUG=False):\n            response = self.client.get('/some_url/', {'email': 'test@example.com', 'password': 'secret', 'name': 'John Doe'})\n            self.assertNotContains(response, 'test@example.com', status_code=500)\n            self.assertNotContains(response, 'secret', status_code=500)\n"], "sample_575": ["    def test_label_with_format(self, t, x):\n\n        fmt = \"%Y-%m\"\n        s = Temporal().label(fmt)._setup(t, Coordinate())\n        a = PseudoAxis(s._matplotlib_scale)\n        a.set_view_interval(0, 365)\n        label, = a.major.formatter.format_ticks([100])\n        assert label == \"1970-01\"\n", "    def test_interval_with_values(self, values, expected, t, x):\n        s = Temporal(values)._setup(t, IntervalProperty())\n        assert_array_equal(s(t), expected)\n", "    def test_label_format(self, t, x, fmt):\n\n        s = Temporal().label(fmt=fmt)._setup(t, Coordinate())\n        labels = s(t)\n        expected = [mpl.dates.DateFormatter(fmt)(date) for date in t]\n        assert_array_equal(labels, expected)\n", "    def test_time_range_in_label(self, t):\n\n        s = Temporal().label(time_range=True)\n        a = PseudoAxis(s._setup(t, Coordinate())._matplotlib_scale)\n        a.set_view_interval(0, 365)\n        label, = a.major.formatter.format_ticks([100])\n        assert \"Jan 1, 1972\" in label \n", "    def test_tick_locator(self, t):\n\n        locator = mpl.dates.MonthLocator()\n        s = Temporal().tick(locator)\n        a = PseudoAxis(s._setup(t, Coordinate())._matplotlib_scale)\n        a.set_view_interval(0, 365)\n        assert all([t.month in loc for loc in a.major.locator()]) \n"], "sample_333": ["    def test_custom_renderer(self):\n        class CustomRenderer(DjangoTemplates):\n                return f'<form>{super().render(form, **context)}</form>'\n\n        form = Form(renderer=CustomRenderer())\n        result = form.as_p()\n        self.assertTrue('<form>' in result)\n        self.assertTrue('</form>' in result)\n", "    def test_renderer_error_messages(self):\n        class CustomRenderer(DjangoTemplates):\n                error_messages = kwargs.get('error_messages', None)\n                if error_messages:\n                    kwargs['error_messages'] = error_messages\n                return super().render_field(field, **kwargs)\n\n        class FormWithCustomRenderer(Form):\n            name = CharField()\n\n        form = FormWithCustomRenderer({'name': 'John'}, renderer=CustomRenderer())\n        form.fields['name'].errors = {'invalid': 'Custom error'}\n\n        rendered_html = form.as_p()\n        self.assertIn('<li>Custom error</li>', rendered_html)\n\n", "    def test_clean_method_can_return_none(self):\n        class UserForm(Form):\n            username = CharField()\n\n                data = self.cleaned_data\n                if data['username'] == 'admin':\n                    return None\n\n                return data['username']\n\n        f = UserForm({'username': 'admin'})\n        self.assertFalse(f.is_valid())\n        self.assertIsNone(f.cleaned_data.get('username'))\n\n\n\n", "    def test_renderer_error_messages(self):\n        class CustomRenderer(DjangoTemplates):\n                return f'<div>{field.error_messages.get(\"required\", \"\")}</div>'\n        \n        class MyForm(Form):\n            name = CharField(max_length=50, required=True)\n\n        form = MyForm({}, renderer=CustomRenderer())\n        self.assertHTMLEqual(form.as_p(), '<div>This field is required.</div><p><label for=\"id_name\">Name:</label> <input id=\"id_name\" maxlength=\"50\" name=\"name\" type=\"text\"></p>')\n\n\n\n", "    def test_form_rendering_with_disabled_fields(self):\n        class MyForm(Form):\n            name = CharField(max_length=50, required=True, disabled=True)\n            email = EmailField(required=True)\n\n        form = MyForm({'email': 'test@example.com'})\n        self.assertHTMLEqual(\n            form.as_p(),\n            '<p><label for=\"id_name\">Name:</label> <input id=\"id_name\" maxlength=\"50\" name=\"name\" type=\"text\" value=\"\" disabled required></p>'\n            '<p><label for=\"id_email\">Email:</label> <input id=\"id_email\" name=\"email\" type=\"email\" value=\"test@example.com\" required></p>'\n        )\n"], "sample_671": ["    def test_relpath_rootdir_relative_path(testdir):\n        testdir.makepyfile(\n            **{\n                \"tests/subdir/test_2.py\": \"\"\"\n        import pytest\n        @pytest.mark.skip()\n            pass\n            \"\"\",\n            }\n        )\n        result = testdir.runpytest(\"-rs\", \"tests/subdir/test_2.py\", \"--rootdir=tests\")\n        result.stdout.fnmatch_lines(\n            [\"SKIPPED [[]1[]] tests/subdir/test_2.py:2: unconditional skip\"]\n        )\n", "def test_relpath_rootdir_empty_fixture(testdir):\n    testdir.makepyfile(\n        **{\n            \"tests/test_1.py\": \"\"\"\n        import pytest\n        @pytest.fixture\n            pass\n        @pytest.mark.skip()\n            pass\n            \"\"\",\n        }\n    )\n    result = testdir.runpytest(\"-rs\", \"tests/test_1.py\", \"--rootdir=tests\")\n    result.stdout.fnmatch_lines(\n        [\"SKIPPED [[]1[]] tests/test_1.py:3: unconditional skip\"]\n    )\n", "    def test_relpath_rootdir_with_mark_ordering(testdir):\n        testdir.makepyfile(\n            \"tests/setup.py\",\n            \"\"\"\n            import pytest\n            @pytest.fixture\n                return \"setup_order_ok\"\n\n                pass\n\n                assert setup_this\n            \"\"\"\n        )\n        result = testdir.runpytest(\n            \"-rs\", \"tests/test_1.py\", \"--rootdir=tests\"\n        )\n        result.stdout.fnmatch_lines(\n            [\n                \"*1 passed*\",\n            ]\n        )\n\n\n\n", "def test_importorskip_with_reason(testdir):\n    with pytest.raises(\n        pytest.skip.Exception,\n        match=\"^importorskip failure: doesnotexist\",\n    ):\n        pytest.importorskip(\"doesnotexist\", reason=\"this is a test\")\n\n\n\n", "def test_importorskip_with_relative_path(testdir):\n    testdir.makepyfile(\n        \"module_to_skip/module.py\",\n        \"\"\"\n            pass\n        \"\"\",\n        \"test_importorskip.py\": \"\"\"\n        import pytest\n        import module_to_skip\n\n        @pytest.mark.skipif(\"__name__ != '__main__'\")\n            module_to_skip.some_function()\n    \"\"\"\n    )\n    with pytest.raises(\n        pytest.skip.Exception,\n        match=r\"condition: __name__ != '__main__' passed\",\n    ):\n        pytest.importorskip(\"module_to_skip.module\")\n    result = testdir.runpytest(\"test_importorskip.py\")\n    result.stdout.fnmatch_lines(\n        [\"*SKIP*module_to_skip.module:condition: __name__ != '__main__' passed*\"]\n    )\n\n\n\n"], "sample_379": ["    def test_mark_safe_with_unicode_characters(self):\n        \"\"\"\n        Test that mark_safe handles Unicode characters correctly.\n        \"\"\"\n        # Example Unicode characters that should be escaped\n        char1 = '\\u00A1'\n        char2 = '\\u20AC'\n\n        s = mark_safe(f\"This string contains {char1} and {char2}.\")\n\n        self.assertRenderEqual('{{ s }}', f\"This string contains {char1} and {char2}.\", s=s)\n\n\n", "    def test_mark_safe_on_safe_string_no_change(self):\n        s = SafeString('a&b')\n        self.assertEqual(mark_safe(s), s)\n\n\n\n", "    def test_mark_safe_with_multiple_calls(self):\n        s = mark_safe('a&b')\n        s = mark_safe(s)  # Second call to mark_safe\n        self.assertRenderEqual('{{ s }}', 'a&b', s=s)\n\n        self.assertIsInstance(s, SafeString)\n", "    def test_mark_safe_with_custom_escaping(self):\n        class CustomEscaping(str):\n                return self.replace(\"<\", \"&lt;\").replace(\">\", \"&gt;\")\n\n        s = CustomEscaping(\"Hello <world>\")\n        safe_s = mark_safe(s)\n\n        self.assertRenderEqual('{{ safe_s }}', 'Hello &lt;world&gt;', safe_s=safe_s)\n\n", "    def test_mark_safe_with_non_string_argument(self):\n        with self.assertRaises(TypeError):\n            mark_safe(123)\n"], "sample_758": ["def test_check_array_sparse_and_dense_data_types():\n    # Check that check_array handles sparse and dense data with different dtypes correctly\n    sparse_data = sp.csr_matrix([[1, 2], [3, 4]], dtype=np.float32)\n    dense_data = np.array([[1, 2], [3, 4]], dtype=np.float64)\n    \n    check_array(sparse_data, dtype=np.float64)\n    check_array(dense_data, dtype=np.float32)\n\n    check_array([sparse_data, dense_data], dtype=np.float32)\n\n\n", "compilation error", "def test_check_array_categorical_dtype():\n    X = np.array([\"a\", \"b\", \"a\"])\n    X_int = check_array(X, dtype=np.int)\n    assert_equal(X_int.dtype, np.int32)\n    assert_equal(X_int, np.array([0, 1, 0]))\n\n    X = pd.Categorical([\"a\", \"b\", \"a\"])\n    X_int = check_array(X, dtype=np.int)\n    assert_equal(X_int.dtype, np.int32)\n    assert_equal(X_int, np.array([0, 1, 0]))\n\n    X = sp.csr_matrix(np.array([{\"a\": 1, \"b\": 0}, \n                                {\"a\": 0, \"b\": 1}, \n                                {\"a\": 1, \"b\": 0}]))\n    X_int = check_array(X, dtype=np.int)\n    assert_equal(X_int.dtype, np.int32)\n    assert_array_equal(X_int.toarray(), np.array([[1, 0], [0, 1], [1, 0]]))\n\n    assert_raises_regex(ValueError, \"Categorical dtype not supported\", \n                        check_array, X, dtype=object)\n", "    def test_retrieve_samples_from_non_standard_shape_with_extra_tuple_dimension():\n        class TestNonNumericShape:\n                self.shape = (\"not numeric\", 2, )\n\n                return len([1, 2, 3])\n\n        X = TestNonNumericShape()\n        assert _num_samples(X) == len(X)\n\n\n\n", "def test_check_X_y_dtype_float64():\n    X = np.ones((10, 5))\n    y = np.ones(10)\n    X = X.astype(np.float64)\n    y = y.astype(np.float64)\n    check_X_y(X, y)\n\n    X = np.zeros((10, 5), dtype=np.float32)\n    y = np.zeros(10, dtype=np.float32)\n    check_X_y(X, y)\n"], "sample_584": ["    def test_auto_combine_with_different_x_sizes(self):\n        objs = [Dataset({'x': [0, 1], 'y': [2, 3]}),\n                Dataset({'x': [2, 3], 'y': [4, 5]})]\n        with pytest.warns(FutureWarning, match=\"The dimensions do not\"):\n            auto_combine(objs)\n", "    def test_auto_combine_with_incompatible_data_types(self):\n        ds1 = Dataset({'x': (('time', 'station'), np.array([[1, 2], [3, 4]])),\n                       'time': ('time', ['A', 'B']),\n                       'station': ('station', ['C', 'D'])})\n        ds2 = Dataset({'x': (('time', 'station'), np.array([[5, 6], [7, 8]])),\n                       'time': ('time', ['B', 'C']),\n                       'station': ('station', ['D', 'E'])})\n        with pytest.raises(ValueError, match=\"Dimensions\"):\n            auto_combine([ds1, ds2])\n", "    def test_auto_combine_with_invalid_concat_dim(self):\n        objs = [Dataset({'x': [0]}), Dataset({'x': [1]})]\n        with pytest.raises(ValueError, match=\"concat_dim argument must\"):\n            auto_combine(objs, concat_dim=['x'])\n", "    def test_auto_combine_with_mismatched_array_dask(self):\n        ds1 = Dataset({'x': 1}, chunks={'x': 1})\n        ds2 = Dataset({'x': 2})\n        with pytest.warns(FutureWarning, match=r\"Encountered a mix of\"):\n            auto_combine([ds1, ds2])\n", "    def test_auto_combine_preserve_existing_dims(self):\n        ds1 = Dataset({'a': ('x', [1, 2]), 'b': ('x', [3, 4])},\n                      coords={'x': ('x', [0, 1])})\n        ds2 = Dataset({'a': ('x', [5, 6]), 'b': ('x', [7, 8])},\n                      coords={'x': ('x', [2, 3])})\n        expected = Dataset({'a': ('x', [1, 2, 5, 6]),\n                           'b': ('x', [3, 4, 7, 8])},\n                          coords={'x': ('x', [0, 1, 2, 3])})\n        actual = auto_combine([ds1, ds2])\n        assert_identical(expected, actual)\n"], "sample_1028": ["def test_issue_15877():\n    n = symbols('n', integer=True, positive=True)\n    assert (1/factorial(n)).as_numer_denom() == (1, factorial(n))\n", "def test_issue_14911():\n    x, y, z = symbols('x y z')\n    assert (x**2 + y**2)**(1/2) == sqrt(x**2 + y**2)\n    assert (x**2 + y**2 + z**2)**(1/2) == sqrt(x**2 + y**2 + z**2)\n", "def test_issue_16297():\n    p = symbols('p', positive=True)\n    q = symbols('q', positive=True)\n    assert (p**q).is_positive\n    assert (p**(Rational(-1, 2))).is_positive\n    assert (p**(Rational(1, 2))).is_positive\n    assert (p**(Rational(-1))).is_positive\n    assert (p**(Rational(1))).is_positive\n    assert (p**(Rational(-2))).is_positive\n\n\n", "def test_issue_16594():\n    from sympy import symbols, exp, oo\n    x = symbols('x')\n    assert (exp(oo) + exp(-oo)).is_real is False\n    assert (exp(oo) + exp(-oo)).is_imaginary is False\n    assert (exp(oo) + exp(-oo)).is_complex is True\n", "def test_issue_14392():\n    assert (sin(zoo)**2).as_real_imag() == (nan, nan)\n"], "sample_605": ["def test_groupby_map_reduce_keep_attrs(dim, keep_attrs):\n    ds = xr.Dataset(\n        {\n            \"foo\": ((\"x\", \"y\"), np.arange(12).reshape((3, 4))),\n            \"bar\": ((\"x\", \"y\", \"z\"), np.arange(24).reshape((3, 4, 2))),\n        },\n        attrs={\"test_attr\": \"test_value\"},\n    )\n        return group.foo.sum()\n\n    result = ds.groupby(dim).map(f, keep_attrs=keep_attrs)\n    if keep_attrs and dim == \"x\":\n        assert \"test_attr\" in result.attrs\n    else:\n        assert \"test_attr\" not in result.attrs\n\n    if keep_attrs and dim in [\"y\", \"z\"]:\n        assert len(result.dims) == len(ds.dims) - 1\n    else:\n        assert len(result.dims) == len(ds.dims) - 1\n\n    assert isinstance(result, xr.Dataset)\n\n\n\n", "def test_groupby_apply_mapping():\n    array = xr.DataArray([1, 2, 3, 4, 5], [(\"x\", [1, 1, 1, 2, 2])])\n    mapping = {\"1\": 10, \"2\": 20}\n\n        return group.map(lambda x: mapping.get(str(x), x))\n\n    actual = array.groupby(\"x\").apply(func)\n    expected = xr.DataArray([10, 10, 10, 20, 20], [(\"x\", [1, 1, 1, 2, 2])])\n    assert_identical(expected, actual)\n\n\n", "def test_groupby_empty_groups():\n    empty_array = xr.DataArray([], dims=\"dim\")\n    with pytest.raises(ValueError):\n        empty_array.groupby(\"dim\")\n\n\n\n", "def test_groupby_groupby_index():\n    # GH 2193: Test groupby with index as the groupby key\n    idx = pd.Index([\"A\", \"B\", \"C\", \"A\", \"B\", \"C\"])\n    data = np.arange(6)\n    dataset = xr.Dataset({\"foo\": (idx, data)})\n    result = dataset.groupby(\"foo\").sum()\n    expected = xr.Dataset({\"foo\": ([\"A\", \"B\", \"C\"], [2, 2, 2])})\n    assert_identical(result, expected)\n\n\n\n", "def test_groupby_custom_reduce_function():\n        return np.sqrt(np.sum(x))\n\n    array = xr.DataArray([1, 2, 3, 4, 5, 6], [(\"x\", [1, 1, 1, 2, 2, 2])])\n\n    result = array.groupby(\"x\").reduce(my_reduce, dim=\"x\")\n\n    expected = xr.DataArray([np.sqrt(6), np.sqrt(12)], [(\"x\", [1, 2])])\n    assert_identical(result, expected)\n\n\n"], "sample_1055": ["def test_encipher_decipher_atbash():\n    assert encipher_atbash(\"ABC\") == \"ZYX\"\n    assert decipher_atbash(\"ZYX\") == \"ABC\"\n    assert encipher_atbash(\"ab\") == \"zy\"\n    assert decipher_atbash(\"zy\") == \"ab\"\n    assert encipher_atbash(\"This is a test.\") == \"Ghvz  qeb n  qbt.\"\n    assert decipher_atbash(\"Ghvz  qeb n  qbt.\") == \"This is a test.\"\n", "def test_encipher_decipher_bifid():\n    for i in range(26):\n        key = chr(ord('a') + i)\n        message = ''.join(chr(ord('a') + j) for j in range(i, i+10))\n        encrypted = encipher_bifid(message, key)\n        decrypted = decipher_bifid(encrypted, key)\n        assert message == decrypted\n", "def test_bg_public_key():\n    assert 5293 == bg_public_key(67, 79)\n    assert 713 == bg_public_key(23, 31)\n    raises(ValueError, lambda: bg_private_key(13, 17))\n", "def test_gm_sequence_length():\n    p = 17\n    q = 19\n    n = (p-1)*(q-1)\n    for i in range(n):\n        pri = gm_private_key(p, q)\n        pub = gm_public_key(p, q)\n        assert len(gm_sequence(pub,pri,i))==n\n", "def test_encipher_decipher_pad():\n    assert encipher_pad(\"ABC\", 3) == \"ABA\"\n    assert decipher_pad(\"ABA\", 3) == \"ABC\"\n    assert encipher_pad(\"ABC\", 4) == \"CAB\"\n    assert decipher_pad(\"CAB\", 4) == \"ABC\"\n    assert encipher_pad(\"ABCDEF\", 3) == \"ACEFB\"\n    assert decipher_pad(\"ACEFB\", 3) == \"ABCDEF\"\n    assert encipher_pad(\"abcdef\", 3) == \"acefb\"\n    assert decipher_pad(\"acefb\", 3) == \"abcdef\"\n    assert encipher_pad(\" \", 3) == \" \"\n    assert decipher_pad(\" \", 3) == \" \"\n"], "sample_106": ["    def test_cache_key_with_template_namespace(self):\n        request = self.factory.get(self.path)\n        template = engines['django'].from_string(\"{% load my_template_tags %}{{ my_template_tag }}\")\n        response = TemplateResponse(HttpRequest(), template)\n        learn_cache_key(request, response)\n        self.assertIn('namespace.template_tag_result', get_cache_key(request))\n", "    def test_with_custom_cache_backend(self):\n        \"\"\"\n        Test caching with a custom cache backend.\n        \"\"\"\n        class CustomCache(CacheBase):\n                return \"Custom Cache Value\"\n\n        # Set up a custom cache backend\n        cache_backend = CustomCache()\n        CACHES = {\n            'default': {\n                'BACKEND': CustomCache,\n            },\n        }\n\n        # Retrieve the cache instance\n        default_cache = caches['default']\n        self.assertEqual(default_cache.get('test'), \"Custom Cache Value\")\n\n\n\n", "    def test_different_caches(self):\n        \"\"\"\n        Testing different cache instances being retrieved by alias.\n        \"\"\"\n        cache1 = caches['default']\n        cache2 = caches['other']\n\n        self.assertIsNot(cache1, cache2)\n\n\n\n", "    def test_get_cache_key_with_template_context(self):\n        request = self.factory.get(self.path)\n        template = engines['django'].from_string(\"This is a test\")\n        response = TemplateResponse(HttpRequest(), template, {'test': 1})\n        # Expect None if no headers have been set yet.\n        self.assertIsNone(get_cache_key(request))\n        # Set headers to an empty list.\n        learn_cache_key(request, response)\n        # The querystring is taken into account.\n        self.assertEqual(\n            get_cache_key(request),\n            'views.decorators.cache.cache_page.settingsprefix.GET.'\n            '0f1c2d56633c943073c4569d9a9502fe.d41d8cd98f00b204e9800998ecf8427e'\n        )\n\n\n\n\n", "    def test_different_caches(self):\n        \"\"\"\n        Requesting different aliases should yield distinct instances.\n        \"\"\"\n        cache1 = caches['default']\n        cache2 = caches['other']\n        self.assertNotIsInstance(cache1, type(cache2))\n"], "sample_344": ["    def test_base_class_fields_with_related_fields(self):\n        A = self.create_model(\"A\", foreign_keys=[models.ForeignKey('B', models.CASCADE)])\n        B = self.create_model(\"B\", foreign_keys=[models.ForeignKey('A', models.CASCADE)])\n        C = self.create_model(\"C\", bases=(A,))\n        self.assertRelated(A, [B, C])\n        self.assertRelated(B, [A, C]) \n        self.assertRelated(C, [A, B]) \n", "    def test_proxy_with_fields(self):\n        A = self.create_model(\"A\", foreign_keys=[models.ForeignKey('B', on_delete=models.CASCADE)])\n        B = self.create_model(\"B\")\n        C = self.create_model(\"C\", bases=(A,), proxy=True)\n        C.name = models.CharField(max_length=50)\n        self.assertRelated(A, [B])\n        self.assertRelated(B, [A, C])\n        self.assertRelated(C, [A, B])\n\n", "    def test_m2m_self_circular_proxy(self):\n        A = self.create_model(\"A\", foreign_keys=[models.ManyToManyField('A', through='T')])\n        T = self.create_model(\"T\", foreign_keys=[\n            models.ForeignKey('A', models.CASCADE),\n            models.ForeignKey('A', models.CASCADE),\n        ], proxy=True)\n        self.assertRelated(A, [T])\n        self.assertRelated(T, [])\n", "    def test_through_model_with_self_related_fields(self):\n        A = self.create_model(\n            \"A\", foreign_keys=[models.ManyToManyField(\"A\", through=\"T\")]\n        )\n        T = self.create_model(\n            \"T\",\n            foreign_keys=[\n                models.ForeignKey(\"A\", on_delete=models.CASCADE),\n                models.ForeignKey(\"A\", on_delete=models.CASCADE),\n                models.IntegerField(\"some_field\"),\n            ],\n        )\n        self.assertRelated(A, [T])\n        self.assertRelated(T, [A])\n\n\n\n", "    def test_abstract_proxy_base(self):\n        A = self.create_model(\"A\", abstract=True)\n        B = self.create_model(\"B\", bases=(A,), proxy=True)\n        C = self.create_model(\"C\", bases=(B,))\n        self.assertRelated(A, [B, C])\n        self.assertRelated(B, [])\n        self.assertRelated(C, [])\n"], "sample_656": ["def test_capture_errors_on_non_text_streams(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import sys\n            sys.stdout = capfd.write\n            sys.stdout.write(b\"hello\")\n            sys.stdout.flush()\n    \"\"\"\n    )\n    result = testdir.runpytest_subprocess()\n    result.stdout.fnmatch_lines(\n        \"*test_write_bytes*\"\n    )\n    assert result.ret == 0\n", "        def test_fails():\n            for i in [1,2,3]:\n                sys.stdout.write(i)\n                sys.stdout.write('\\n')", "def test_capture_with_encoding_errors(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import sys\n        sys.stdout.encoding = 'ascii'\n            sys.stdout.write(b\"foo\\xff\")\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\"E           UnicodeEncodeError: 'ascii' codec can't encode character '\\\\xff' in position 3: ordinal not in range(128)\"]\n    )\n", "def test_encodedfile_constructor(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import sys\n            e = capture.EncodedFile(sys.stdout, \"utf8\")\n            assert e.encoding == \"utf8\"\n            assert isinstance(e.file, io.TextIOWrapper)\n    \"\"\"\n    )\n    reprec = testdir.inline_run()\n    reprec.assertoutcome(passed=1)\n", "def test_capture_with_no_output(testdir):\n    testdir.makepyfile(\n        \"\"\"\n            pass\n    \"\"\"\n    )\n    result = testdir.runpytest_subprocess()\n    result.stdout.fnmatch_lines(\n        [\n            \"*test_no_output*\",\n        ]\n    )\n"], "sample_398": ["    def test_changelist_template_with_empty_queryset(self):\n        # Make sure the changelist template works even when there are no users\n        User.objects.all().delete()\n        response = self.client.get(reverse(\"custom_user_admin:auth_tests_uuiduser_changelist\"))\n        self.assertContains(response, \"No users found.\")\n", "    def test_admin_password_change_invalid_url(self):\n        response = self.client.get(\n            reverse(\"auth_test_admin:auth_user_password_change\", args=(\"foobar\",))\n        )\n        self.assertEqual(response.status_code, 404)\n", "    def test_user_change_username_unique(self):\n        u = UUIDUser.objects.create_superuser(\n            username=\"uuid\", email=\"foo@bar.com\", password=\"test\"\n        )\n        self.assertTrue(self.client.login(username=\"uuid\", password=\"test\"))\n\n        response = self.client.get(\n            reverse(\"custom_user_admin:auth_tests_uuiduser_change\", args=(u.pk,))\n        )\n        self.assertEqual(response.status_code, 200)\n\n        change_url = reverse(\"custom_user_admin:auth_tests_uuiduser_change\", args=(u.pk,))\n        data = self.get_user_data(u)\n        data[\"username\"] = \"duplicate\"  # Use a username already in use.\n        response = self.client.post(change_url, data)\n        self.assertEqual(response.status_code, 200)\n        self.assertContains(response, \"Username must be unique.\")\n", "    def test_password_change_with_custom_user_model(self):\n        u = UUIDUser.objects.create_superuser(\n            username=\"uuid\", email=\"foo@bar.com\", password=\"test\"\n        )\n        self.assertTrue(self.client.login(username=\"uuid\", password=\"test\"))\n\n        user_change_url = reverse(\n            \"custom_user_admin:auth_tests_uuiduser_change\", args=(u.pk,)\n        )\n        response = self.client.get(user_change_url)\n        self.assertEqual(response.status_code, 200)\n\n        password_change_url = reverse(\n            \"custom_user_admin:auth_user_password_change\", args=(u.pk,)\n        )\n        response = self.client.post(\n            password_change_url,\n            {\n                \"password1\": \"new_password\",\n                \"password2\": \"new_password\",\n            },\n        )\n        self.assertRedirects(response, user_change_url)\n        u.refresh_from_db()\n        self.assertNotEqual(u.password, \"test\")\n        self.assertEqual(u.password, make_password(\"new_password\"))\n", "    def test_user_change_password_existing_password_field(self):\n        u = UUIDUser.objects.create_superuser(\n            username=\"uuid\", email=\"foo@bar.com\", password=\"test\"\n        )\n        self.assertTrue(self.client.login(username=\"uuid\", password=\"test\"))\n\n        user_change_url = reverse(\n            \"custom_user_admin:auth_tests_uuiduser_change\", args=(u.pk,)\n        )\n        response = self.client.get(user_change_url)\n        self.assertEqual(response.status_code, 200)\n\n        # The existing_password field should be present on the password change form\n        self.assertContains(response, '<input type=\"password\" id=\"id_existing_password\" name=\"existing_password\"')\n"], "sample_968": ["def test_python_python_use_unqualified_type_names_confdir(app, status, warning):\n    app.config.domain_config['py'].setdefault('python_use_unqualified_type_names', True)\n    app.build()\n    content = (app.outdir / 'index.html').read_text()\n    assert ('<span class=\"n\"><a class=\"reference internal\" href=\"#foo.Name\" title=\"foo.Name\">'\n            '<span class=\"pre\">Name</span></a></span>' in content)\n    assert '<span class=\"n\"><span class=\"pre\">foo.Age</span></span>' in content\n    assert ('<p><strong>name</strong> (<a class=\"reference internal\" href=\"#foo.Name\" '\n            'title=\"foo.Name\"><em>Name</em></a>) \u2013 blah blah</p>' in content)\n    assert '<p><strong>age</strong> (<em>foo.Age</em>) \u2013 blah blah</p>' in content\n", "def test_no_docstring_class(app, status, warning):\n    app.build()\n    \n    with open(app.outdir / 'index.html') as f:\n        html = f.read()\n    \n    assert '<a href=\"https://docs.python.org/3/library/functions.html#round\" class=\"reference internal\" title=\"Round to a specified number of digits.\">' not in html\n", "def test_class_members(app, status, warning):\n    app.build()\n    content = (app.outdir / 'index.html').read_text()\n    assert '<dt><span class=\"py-class-name\">MyClass</span></dt>' in content\n    assert '<dd>...</dd>' in content\n    assert '<dt><span class=\"py-class-name\">AnotherClass</span></dt>' in content\n    assert '<dd>...</dd>' in content\n", "def test_warn_missing_reference_target(app, status, warning):\n    app.build()\n    assert 'index.rst:7: WARNING: Failed to create a cross reference. A label not found: nonexistent' in warning.getvalue()\n", "def test_python_python_use_unqualified_type_names_custom_prefix(app, status, warning):\n    app.build()\n    content = (app.outdir / 'index.html').read_text()\n    assert ('<span class=\"n\"><a class=\"reference internal\" href=\"#foo.Name\" '\n            'title=\"foo.Name\"><span class=\"pre\">Name</span></a></span>' in content)\n    assert ('<p><strong>name</strong> (<a class=\"reference internal\" href=\"#foo.Name\" '\n            'title=\"foo.Name\"><em>Name</em></a>) \u2013 blah blah</p>' in content)\n\n\n\n    assert ('<span class=\"n\"><span class=\"pre\">foo.Age</span></span>' in content)\n    assert ('<p><strong>age</strong> (<em>foo.Age</em>) \u2013 blah blah</p>' in content)\n\n"], "sample_302": ["    def test_runshell_handles_sigint(self):\n        sigint_handler = signal.getsignal(signal.SIGINT)\n        with mock.patch('subprocess.run') as run:\n            with mock.patch.object(\n                BaseDatabaseClient,\n                'settings_to_cmd_args_env',\n                return_value=([], None),\n            ):\n                self.client.runshell(None)\n        run.assert_called_once_with([], env=None, check=True)\n        # Assert that the original SIGINT handler was restored.\n        self.assertEqual(signal.getsignal(signal.SIGINT), sigint_handler)\n", "    def test_runshell_handling_sigint(self):\n        sigint_handler = signal.getsignal(signal.SIGINT)\n        with mock.patch('subprocess.run') as run:\n            with mock.patch.object(\n                BaseDatabaseClient,\n                'settings_to_cmd_args_env',\n                return_value=([], None),\n            ):\n                with self.assertRaises(KeyboardInterrupt):\n                    self.client.runshell(None)\n                run.assert_called_once_with([], env=None, check=True)\n        # Ensure the original SIGINT handler was restored\n        signal.signal(signal.SIGINT, sigint_handler)\n", "    def test_runshell_handle_sigint(self):\n        sigint_handler = signal.getsignal(signal.SIGINT)\n        with mock.patch('subprocess.run') as run:\n            with mock.patch.object(\n                BaseDatabaseClient,\n                'settings_to_cmd_args_env',\n                return_value=([], None),\n            ):\n                self.client.runshell([])\n        run.assert_called_once_with([], env=None, check=True)\n        # Verify that the original SIGINT handler was restored.\n        self.assertEqual(signal.getsignal(signal.SIGINT), sigint_handler)\n", "    def test_runshell_handles_sigint(self):\n        sigint_handler = signal.getsignal(signal.SIGINT)\n        with mock.patch('subprocess.run') as run:\n            with mock.patch.object(\n                BaseDatabaseClient,\n                'settings_to_cmd_args_env',\n                return_value=([], None),\n            ):\n                self.client.runshell(None)\n        run.assert_called_once_with([], env=None, check=True)\n        signal.signal(signal.SIGINT, sigint_handler)\n", "    def test_runshell_subprocess_args(self):\n        with mock.patch('subprocess.run') as run:\n            with mock.patch.object(\n                BaseDatabaseClient,\n                'settings_to_cmd_args_env',\n                return_value= ['psql', '-h', 'localhost', '-p', '5432', '-U', 'user', 'database', ['parameter1', 'parameter2']],\n            ):\n                self.client.runshell(['parameter1', 'parameter2'])\n            run.assert_called_once_with(['psql', '-h', 'localhost', '-p', '5432', '-U', 'user', 'database', 'parameter1', 'parameter2'], check=True)\n"], "sample_664": ["def test_funcargnames_alias(testdir):\n    testdir.makepyfile(\n        \"\"\"\n            assert hasattr(pytest.funcargnames, \"fixturenames\")\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    if arg == \"funcargnames\":\n        result.stdout.fnmatch_lines([*deprecated.FUNCARGNAMES.format()])\n    else:\n        result.stdout.no_fnmatch_lines([*deprecated.FUNCARGNAMES.format()])\n", "def test_deprecated_funcargnames(argname):\n    with pytest.warns(deprecated.FUNCARGNAMES) as warnings:\n        getattr(deprecated, argname)\n    assert len(warnings) == 1\n    assert isinstance(warnings[0].warning, deprecated.PytestDeprecationWarning)\n", "def test_funcargnames_alias(pytestconfig):\n    with pytest.warns(deprecated.FUNCARGNAMES) as warnings:\n        getattr(pytestconfig.pluginmanager, argnames) \n    assert len(warnings) == 1\n    assert warnings[0].category is deprecated.FUNCARGNAMES\n\n\n\n", "def test_fixture_positional_arguments(testdir, argnames):\n    testdir.makepyfile(\n        \"\"\"\n        @pytest.fixture\n            pass\n\n            pass\n    \"\"\"\n    )\n\n    if argnames is None:\n        with pytest.warns(deprecated.FIXTURE_POSITIONAL_ARGUMENTS):\n            result = testdir.runpytest()\n    else:\n        testdir.makeini(\n            \"\"\"\n            [pytest]\n            funcargnames={argnames}\n            \"\"\".format(argnames=argnames)\n        )\n        result = testdir.runpytest()\n\n    if argnames is not None:\n        result.stdout.no_fnmatch_line(\n            \"*Passing arguments to pytest.fixture() as positional arguments is deprecated - pass them *\")\n    else:\n        result.stdout.fnmatch_lines(\n            [\n                \"*Passing arguments to pytest.fixture() as positional arguments is deprecated - pass them *\"\n            ]\n        )\n", "def test_deprecated_funcargnames_attribute(testdir, argname):\n    testdir.makepyfile(\n        \"\"\"\n            pass\n    \"\"\"\n    )\n    with pytest.warns(deprecated.FUNCARGNAMES) as warning:\n        result = testdir.runpytest(\"--run-slow\")\n    assert warning[0].message == f\"The `{argname}` attribute was an alias for `fixturenames`, since pytest 2.3 - use the newer attribute instead.\" \n"], "sample_7": ["compilation error", "def test_masked_column_sort():\n    mc = table.MaskedColumn([1, 2, 3, np.ma.masked, 1], mask=[False, False, False, True, False])\n    mc_sorted = mc.sort()\n    assert np.all(mc_sorted.data == [1, 1, 2, 3, np.ma.masked])\n    assert np.all(mc_sorted.mask == [False, False, False, True, True])\n\n    # Sort in descending order\n    mc_sorted_desc = mc.sort(ascending=False)\n    assert np.all(mc_sorted_desc.data == [3, 2, 1, 1, np.ma.masked])\n    assert np.all(mc_sorted_desc.mask == [False, False, False, False, True])\n\n    mc_sorted_desc_inplace = mc.sort(ascending=False, inplace=True)\n    assert np.all(mc_sorted_desc_inplace.data == [3, 2, 1, 1, np.ma.masked])\n    assert np.all(mc_sorted_desc_inplace.mask == [False, False, False, False, True])\n\n\n\n", "compilation error", "def test_column_creation_from_scalar():\n    \"\"\"\n    Test creation of columns from scalars, ensuring the scalar is\n    properly promoted to a 1-element array.\n    \"\"\"\n\n    for dt in [np.int32, np.float64, str, bool, bytes]:\n        c = table.Column(1, dtype=dt)\n        assert c.dtype == dt\n        assert c.data.shape[0] == 1\n\n        try:\n            c = table.MaskedColumn(1, dtype=dt)\n        except TypeError:\n            continue  # masked columns don't currently allow this\n        assert c.dtype == dt\n        assert c.data.shape[0] == 1\n", "def test_masked_column_string_encoding():\n    \"\"\"\n    Test that MaskedColumn with string dtype handle encoding properly.\n    \"\"\"\n    # Test with ASCII and non-ASCII characters (including umlauts)\n    data = ['This', 'is \u00e4 test'.encode('utf-8')]\n    mc = table.MaskedColumn(data, mask=[False, True], dtype='S')\n    assert mc[0] == 'This'\n    assert mc[1] is np.ma.masked\n\n    mc2 = table.MaskedColumn(mc, dtype='S')\n    assert mc2[0] == 'This'\n    assert mc2[1] is np.ma.masked\n\n\n    mc3 = table.MaskedColumn(mc.astype('S1'), dtype='S1')\n    assert mc3[0] == b'This'\n    assert mc3[1] is np.ma.masked\n"], "sample_80": ["    def test_join_promotion(self):\n        query = Query(Item)\n        where = query.build_where(Q(author__name__icontains='Alice') | Q(author__num__gt=10))\n        query.set_extra_mask(None)\n        query.set_annotation_mask(None)\n        query.values_select = ()\n        query.defer_to_db = True\n        query.add_deferred_loading(\n            ['id', 'created', 'modified']\n        )\n        query.add_extra(\n            {'author_name': 'SELECT name FROM Author WHERE id = Item.author_id'},\n            {},\n            [],\n            {},\n            [],\n        )\n        query.add_extra_order_by(['author_name'])\n        \n        # This is where we need to set up a way to test the join promotion logic.\n        # We could do this by inspecting the SQL query generated by the query object\n        # and seeing if the join promotion rules have been applied as expected.\n        pass \n", "    def test_deferred_loading(self):\n        query = Query(Author)\n        query.add_deferred_loading(['name', 'email'])\n        self.assertEqual(query.deferred_to_data(), {Author: {'name', 'email'}})\n        \n        query = Query(Author)\n        query.add_immediate_loading(['name'])\n        self.assertEqual(query.deferred_to_data(), {Author: set()})\n", "    def test_related_object_query(self):\n        query = Query(Author)\n        where = query.build_where(Q(item__name__icontains='test'))\n        lookup = where.children[0]\n        self.assertIsInstance(lookup, Q)\n\n        inner_lookup = lookup.children[0]\n        self.assertIsInstance(inner_lookup, Exact)\n        self.assertEqual(inner_lookup.lhs.target, Item._meta.get_field('name'))\n        self.assertEqual(inner_lookup.rhs, 'test')\n        self.assertEqual(inner_lookup.lhs.related_name, 'author')\n\n\n\n", "    def test_complex_lookups(self):\n        query = Query(Author)\n        where = query.build_where(Q(name__icontains='foo') & Q(num__gt=F('age')))\n        lookup = where.children[0]\n        self.assertIsInstance(lookup, Exact)\n        self.assertIsInstance(lookup.lhs, F)\n        self.assertEqual(lookup.lhs.target, Author._meta.get_field('name'))\n        lookup = where.children[1]\n        self.assertIsInstance(lookup, GreaterThan)\n        self.assertEqual(lookup.rhs, None)\n        self.assertEqual(lookup.lhs.target, Author._meta.get_field('num'))\n\n", "    def test_related_object_filter(self):\n        query = Query(Author)\n        with self.assertRaises(FieldError):\n            query.build_where(Q(item__name='foo'))\n"], "sample_397": ["    def test_invalid_template_loaders(self):\n        with self.assertRaises(ImproperlyConfigured):\n            DjangoTemplates(\n                {\n                    \"DIRS\": [],\n                    \"APP_DIRS\": False,\n                    \"NAME\": \"django\",\n                    \"OPTIONS\": {\"loaders\": [(\"invalid_loader\",)]},\n                }\n            )\n", "    def test_template_loaders_custom(self):\n        engine = DjangoTemplates(\n            {\n                \"DIRS\": [],\n                \"APP_DIRS\": False,\n                \"NAME\": \"django\",\n                \"OPTIONS\": {\n                    \"loaders\": [\n                        (\"my_loader\", \"myapp.template_loaders.MyLoader\"),\n                        (\"django.template.loaders.filesystem.Loader\", []),\n                    ]\n                },\n            }\n        )\n\n        self.assertEqual(\n            engine.engine.template_loaders,\n            [\n                {\"__class__\": \"myapp.template_loaders.MyLoader\"},\n                {\"__class__\": \"django.template.loaders.filesystem.Loader\"},\n            ],\n        )\n\n\n\n", "    def test_template_loaders_invalid_setting(self):\n        with self.assertRaises(ImproperlyConfigured):\n            DjangoTemplates(\n                {\n                    \"DIRS\": [],\n                    \"APP_DIRS\": False,\n                    \"NAME\": \"django\",\n                    \"OPTIONS\": {\"loaders\": [\"nope\"]},\n                }\n            )\n", "    def test_template_loaders_override(self):\n        \"\"\"\n        The template loaders can be overridden.\n        \"\"\"\n        engine = DjangoTemplates(\n            {\n                \"DIRS\": [],\n                \"APP_DIRS\": False,\n                \"NAME\": \"django\",\n                \"OPTIONS\": {\n                    \"loaders\": [\n                        (\"django.template.loaders.filesystem.Loader\", [Path(__file__).parent / \"templates\" / \"template_backends\"]),\n                    ]\n                },\n            }\n        )\n        template = engine.get_template(\"hello.html\")\n        self.assertEqual(template.render({\"name\": \"Jane\"}), \"Hello Jane!\\n\")\n", "    def test_template_loaders_from_config(self):\n        engine = DjangoTemplates(\n            {\n                \"DIRS\": [],\n                \"APP_DIRS\": False,\n                \"NAME\": \"django\",\n                \"OPTIONS\": {\n                    \"loaders\": [\n                        (\"django.template.loaders.filesystem.Loader\", [\"/path/to/templates\"]),\n                        (\"django.template.loaders.app_directories.Loader\", []),\n                    ]\n                },\n            }\n        )\n        loaders = engine.engine.template_loaders\n        self.assertEqual(len(loaders), 2)\n        self.assertEqual(loaders[0].loader_class, \"django.template.loaders.filesystem.Loader\")\n        self.assertEqual(loaders[0].search_path, [\"/path/to/templates\"])\n        self.assertEqual(loaders[1].loader_class, \"django.template.loaders.app_directories.Loader\")\n        self.assertEqual(loaders[1].search_path, [])\n"], "sample_1148": ["def test_matrixelement_substitution():\n    A = MatrixSymbol('A', 3, 3)\n    B = MatrixSymbol('B', 3, 3)\n    C = MatrixSymbol('C', 3, 3)\n    \n    x = symbols('x')\n    \n    a = A[1, 2]\n    b = B[0, 1]\n    c = C[2, 0]\n    \n    assert a.subs({A: x*B}) == x*B[1, 2]\n    assert a.subs({A: B}) == B[1, 2]\n    assert a.subs({A: C}) == C[1, 2]\n    \n    assert b.subs({B: x*C}) == x*C[0, 1]\n    assert b.subs({B: A}) == A[0, 1]\n    assert b.subs({B: C}) == C[0, 1]\n    \n    assert c.subs({C: x*A}) == x*A[2, 0]\n    assert c.subs({C: B}) == B[2, 0]\n    assert c.subs({C: A}) == A[2, 0]\n\n    \n    expr = A[1, 2] * B[0, 1] * C[2, 0]\n    expr_subs = expr.subs({A: x*B, B: y*C})\n    assert expr_subs == x*y*B[1, 2] * C[0, 1] * C[2, 0]\n    \n", "def test_matrix_indices():\n    A = MatrixSymbol('A', 3, 3)\n    assert A[0, 0].indices == (0, 0)\n    assert A[1, 2].indices == (1, 2)\n    assert A[i, j].indices == (i, j)\n    assert A[i, :].indices == (i, slice(None))\n    assert A[:, j].indices == (slice(None), j)\n    assert A[i, j:k].indices == (i, slice(j, k))\n\n    raises(TypeError, lambda: A[i, j].indices == (str(i), int(j)))\n\n    B = MatrixSymbol('B', 4, 2)\n    assert B[0, 1].indices == (0, 1)\n    \n    raises(IndexError, lambda: A[3, 3])\n    raises(IndexError, lambda: A[-1, 0])\n", "def test_matrix_transpose():\n    A = MatrixSymbol('A', 2, 3)\n    assert A.T.shape == (3, 2)\n    assert (A.T).T == A\n    B = Matrix([[1, 2, 3], [4, 5, 6]])\n    assert B.T.shape == (3, 2)\n    assert (B.T).T == B\n    C = Matrix([[1, 2], [3, 4], [5, 6]])\n    assert C.T.shape == (3, 2)\n    assert (C.T).T == C\n    D = ZeroMatrix(2, 3)\n    assert D.T.shape == (3, 2)\n    assert (D.T).T == D\n", "def test_expand_add():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    C = MatrixSymbol('C', 2, 2)\n    expr1 = A + B + C\n    expr2 = B + A + C\n    expr3 = C + A + B\n    assert expr1 == expr2 == expr3\n    assert simplify(expr1) == expr1\n    assert expand(expr1) == expr1\n\n\n\n", "def test_matrix_transpose():\n    A = MatrixSymbol('A', 2, 3)\n    assert A.T.shape == (3, 2)\n    assert (A.T).T == A\n    assert (A + B).T == A.T + B.T\n    assert (A*B).T == B.T*A.T\n    assert (A**2).T == (A.T)**2\n    assert (A.inv()).T == (A.T).inv()\n    assert (A.T[i, j]) == A[j, i]\n\n    raises(ShapeError, lambda: (A**-1).T.T)\n    \n    A = Matrix([[1, 2, 3], [4, 5, 6]])\n    assert (A.T).doit() == A.T\n    \n    assert Transpose(A).doit() == A.T \n"], "sample_964": ["def test_python_types_simple(app, status, warning):\n    app.build()\n    content = (app.outdir / 'index.html').read_text()\n    assert ('<span class=\"p\">int</span>' in content)\n    assert ('<span class=\"p\">str</span>' in content)\n    assert ('<span class=\"p\">float</span>' in content)\n    assert ('<span class=\"p\">bool</span>' in content)\n    assert ('<span class=\"p\">None</span>' in content)\n\n", "def test_warn_missing_reference_domain(app, status, warning):\n    app.build()\n    assert 'index.rst:6: WARNING: undefined label: no-label' in warning.getvalue()\n    assert ('index.rst:6: WARNING: Failed to create a cross reference. A title or caption not found: existing-label'\n            in warning.getvalue())\n\n\n\n", "def test_warn_missing_reference_in_example(app, status, warning):\n    app.build()\n    assert 'index.rst:7: WARNING: undefined label: no-label' in warning.getvalue()\n", "def test_warn_missing_reference_in_html(app, status, warning):\n    app.build()\n    assert 'index.rst:6: WARNING: undefined label: no-label' in warning.getvalue()\n    assert ('index.rst:6: WARNING: Failed to create a cross reference. A title or caption not found: existing-label'\n            in warning.getvalue())\n\n\n\n", "def test_warn_missing_reference_html(app, status, warning):\n    app.build()\n    assert 'index.rst:6: WARNING: undefined label: no-label' in warning.getvalue()\n    assert ('index.rst:6: WARNING: Failed to create a cross reference. A title or caption not found: existing-label'\n            in warning.getvalue())\n"], "sample_1175": ["compilation error", "def test_issue_18541():\n    from sympy.abc import a, b\n    from sympy.calculus.limits import limit\n\n    expr = limit((a*x + b)**2, x, 0)\n    assert pretty(expr) == 'lim_(x \u2192 0) ((a\u22c5x + b)\u00b2)'\n\n\n", "compilation error", "compilation error", "def test_issue_18590():\n    from sympy.matrices import Matrix\n\n    A = Matrix([[1, 2], [3, 4]])\n    assert pretty(A.copy()) == A\n    assert upretty(A.copy()) == A\n\n    B = A.copy()\n    assert pretty(B) == pretty(A)\n    assert upretty(B) == upretty(A)\n"], "sample_1064": ["def test_tensorflow_Piecewise():\n    if not tf:\n        skip(\"TensorFlow not installed\")\n\n    expr = Piecewise((x > 0, x*2), (x <= 0, -x))\n    assert tensorflow_code(expr) == \\\n        \"tensorflow.where(tensorflow.math.greater(x, 0), 2*x, -x)\"\n\n", "compilation error", "def test_tensorflow_Piecewise():\n    if not tf:\n        skip(\"TensorFlow not installed\")\n\n    expr = Piecewise((x < 0, x**2, x >= 0, x))\n    assert tensorflow_code(expr) == \\\n        \"tensorflow.math.where(x < 0, tensorflow.math.pow(x, 2), x)\"\n\n    _compare_tensorflow_scalar((x,), expr)\n\n\n\n", "compilation error", "def test_tensorflow_piecewise():\n    if not tf:\n        skip(\"TensorFlow not installed\")\n\n    expr = Piecewise((x > 0, sin(x), x <= 0, cos(x)))\n    assert tensorflow_code(expr) == \\\n        \"tensorflow.where(tensorflow.math.greater(x, 0), tensorflow.math.sin(x), tensorflow.math.cos(x))\"\n    _compare_tensorflow_scalar((x,), expr) \n"], "sample_219": ["    def test_resolve_output_field_from_expression(self):\n        expr = ExpressionWrapper(F('cost').add(F('tax')), output_field=FloatField())\n        self.assertIsInstance(expr.output_field, FloatField)\n", "    def test_resolve_output_field_from_expression(self):\n        expr = ExpressionWrapper(\n            F('cost') + F('tax'), output_field=DateField()\n        )\n        with self.assertRaises(ValueError):\n            expr.output_field  \n", "    def test_output_field_not_specified(self):\n        expr = ExpressionWrapper(Value(3))\n        self.assertIsNone(expr.output_field)\n", "    def test_output_field_resolution(self):\n        with self.subTest(non_explicit_field=True):\n            expr = ExpressionWrapper(F('name'), output_field=CharField(max_length=50))\n            self.assertEqual(expr.output_field.deconstruct(), CharField(max_length=50).deconstruct())\n        with self.subTest(explicit_field=True):\n            expr = ExpressionWrapper(F('name'), output_field=TextField())\n            self.assertEqual(expr.output_field.deconstruct(), TextField().deconstruct())\n", "    def test_invalid_output_field(self):\n        with self.assertRaises(TypeError):\n            ExpressionWrapper(Value(1), output_field=datetime.date)\n"], "sample_506": ["def test_circular_spine_positions():\n    fig, ax = plt.subplots()\n    ax.spines['top'].set_position(('data', 0.5))\n    ax.spines['top'].set_patch_circle((0.5, 0.5), 0.3)\n    ax.set_xlim([0, 1])\n    ax.set_ylim([0.2, 0.8])\n", "def test_spines_circular():\n    fig, ax = plt.subplots()\n    ax.spines['top'].set_position(('data', 0.5))\n    ax.spines['top'].set_patch_circle((0.5, 0.5), 0.2)\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n", "def test_spine_set_bounds():\n    fig, ax = plt.subplots()\n    ax.set_xlim(0, 5)\n    ax.set_ylim(0, 5)\n    ax.spines.top.set_bounds(1, 4)\n    ax.spines.bottom.set_bounds(0.5, 4.5)\n    ax.spines.left.set_bounds(0, 4)\n    ax.spines.right.set_bounds(0.5, 4.5)\n", "def test_spines_circular():\n    fig, ax = plt.subplots()\n    ax.spines['top'].set_patch_circle((0.5, 0.5), 0.4)\n    ax.spines['bottom'].set_patch_circle((0.5, 0.5), 0.4)\n    ax.spines['left'].set_patch_circle((0.5, 0.5), 0.4)\n    ax.spines['right'].set_patch_circle((0.5, 0.5), 0.4)\n", "def test_spine_circular():\n    fig, ax = plt.subplots()\n    ax.spines['top'].set_patch_circle((0.5, 0.5), 0.2)\n    ax.spines['bottom'].set_patch_circle((0.5, 0.5), 0.1)\n    ax.spines['left'].set_patch_circle((0.5, 0.5), 0.3)\n    ax.spines['right'].set_patch_circle((0.5, 0.5), 0.4)\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n"], "sample_258": ["    def test_receiver_signal_list_disconnect(self):\n        @receiver([a_signal, b_signal, c_signal])\n            self.state.append(val)\n        self.state = []\n        a_signal.send(sender=self, val='a')\n        b_signal.connect(f, sender=self)\n        b_signal.send(sender=self, val='b2')\n        b_signal.disconnect(f)\n        c_signal.send(sender=self, val='c')\n        self.assertIn('a', self.state)\n        self.assertNotIn('b2', self.state)\n        self.assertIn('c', self.state) \n", "    def test_receiver_sender_argument(self):\n        @receiver(a_signal)\n            self.assertEqual(sender, self)\n        a_signal.send(sender=self, val=True)\n", "    def test_receiver_multiple_arguments(self):\n        @receiver(a_signal)\n            self.state = (val1, val2)\n        self.state = None\n        a_signal.send(sender=self, val1=1, val2=2)\n        self.assertEqual(self.state, (1, 2))\n", "    def test_receiver_dispatch_uid(self):\n        @receiver(a_signal, dispatch_uid=\"uid1\")\n            self.state.append(\"f_1\")\n        @receiver(a_signal, dispatch_uid=\"uid1\")\n            self.state.append(\"f_2\")\n        @receiver(a_signal, dispatch_uid=\"uid2\")\n            self.state.append(\"f_3\")\n        self.state = []\n        a_signal.send(sender=self, val=True)\n        self.assertEqual(self.state, [\"f_1\", \"f_2\"])\n        a_signal.disconnect(dispatch_uid=\"uid1\")\n        a_signal.send(sender=self, val=True)\n        self.assertEqual(self.state, [\"f_1\", \"f_2\", \"f_3\"])\n\n", "    def test_receiver_dispatch_uid(self):\n        @receiver(a_signal, dispatch_uid=\"uid_1\")\n            self.state.append(\"f_1\")\n        @receiver(a_signal, dispatch_uid=\"uid_1\")\n            self.state.append(\"f_2\")\n\n        self.state = []\n        a_signal.send(sender=self, val=\"test\")\n        self.assertEqual(self.state, [\"f_1\", \"f_2\"])\n\n        @receiver(a_signal, dispatch_uid=\"uid_2\")\n            self.state.append(\"f_3\")\n        a_signal.send(sender=self, val=\"test\")\n        self.assertEqual(self.state, [\"f_1\", \"f_2\", \"f_3\"])\n"], "sample_1133": ["def test_hyperfocal_distance_edge_cases():\n    f, N, c = symbols('f, N, c')\n    assert hyperfocal_distance(f=0, N=8, c=0.0033) == 0\n    assert hyperfocal_distance(f=0.5, N=0, c=0.0033) == oo\n    assert hyperfocal_distance(f=0.5, N=oo, c=0.0033) == 0\n    assert hyperfocal_distance(f=0.5, N=8, c=0) == oo\n", "compilation error", "def test_deviation_for_real_and_virtual_objects():\n    n1, n2 = symbols('n1, n2')\n    r1 = Ray3D(Point3D(-1, -1, 1), Point3D(0, 0, 0))\n    n = Matrix([0, 0, 1])\n    i = Matrix([-1, -1, -1])\n    normal_ray = Ray3D(Point3D(0, 0, 0), Point3D(0, 0, 1))\n    P = Plane(Point3D(0, 0, 0), normal_vector=[0, 0, 1])\n    \n    # Real Object\n    assert deviation(r1, 1.33, 1, plane=P).evalf(3) > 0\n    \n    # Virtual Object\n    r2 = Ray3D(Point3D(1, 1, 1), Point3D(0, 0, 0))\n    assert deviation(r2, 1, 1.33, plane=P).evalf(3) < 0\n", "def test_lens_makers_formula_infinite_focal_length():\n    n1, n2 = symbols('n1, n2')\n    m1 = Medium('m1', permittivity=e0, n=1)\n    m2 = Medium('m2', permittivity=e0, n=1.33)\n    assert ae(lens_makers_formula(m1, m2, oo, 10), 10, 2)\n    assert lens_makers_formula(n1, n2, oo, 10) == 10.0*n2/(n1 - n2)\n", "compilation error"], "sample_94": ["    def test_permission_creation_with_custom_app(self):\n        # Create a custom app to test permission creation\n        # For testing purposes, we create permissions for a custom app\n        custom_app_name = 'mytestapp'\n        apps.register_app(\n            AppConfig(\n                app_label=custom_app_name,\n                models=list(),\n            )\n        )\n\n        # Create a custom model in the custom app\n        class CustomUser(AbstractUser):\n            pass\n\n        create_permissions(apps.get_app_config(custom_app_name), verbosity=0)\n\n        # Check if permissions for the custom model are created\n        permission_content_type = ContentType.objects.get_by_natural_key(custom_app_name, 'customuser')\n        self.assertTrue(Permission.objects.filter(content_type=permission_content_type).exists())\n\n        # Clean up the custom app for future tests\n        apps.unregister_app(custom_app_name)\n\n\n\n", "    def test_custom_permissions(self):\n        from django.contrib.auth.models import Group\n\n        # Custom permissions are created as a child of the\n        # permissions for the same model.\n        # Check that the custom permission is created\n        # for the Group model.\n        group_content_type = ContentType.objects.get_for_model(Group)\n        new_permission_name = \"custom_group_permission\"\n        new_permission_codename = f\"can_{new_permission_name}\"\n        create_permissions(self.app_config, verbosity=0)\n\n        # We need to check if the new permission exists\n        # in the database.\n        new_permission = Permission.objects.get(\n            codename=new_permission_codename,\n            content_type=group_content_type\n        )\n\n        self.assertEqual(new_permission.name,\n                         f\"{new_permission_name} - Permission\")\n", "    def test_group_permissions_created(self):\n        permission_content_type = ContentType.objects.get_by_natural_key('auth', 'permission')\n\n        with self.assertNumQueries(2):\n            create_permissions(self.app_config, verbosity=0)\n\n        # Ensure both the default permissions and the group-specific permissions are created.\n        self.assertEqual(Permission.objects.filter(\n            content_type=permission_content_type,\n        ).count(), 6)\n\n\n", "    def test_create_permissions_handles_empty_apps(self):\n        \"\"\"\n        Handles cases where the app config list is empty.\n        \"\"\"\n        empty_apps_config = apps.AppConfig(\n            'auth', path='path/to/auth', module='auth'\n        )\n        with self.assertNumQueries(0):\n            create_permissions(empty_apps_config, verbosity=0)\n", "    def test_create_permissions_with_custom_content_type_app_label(self):\n        \"\"\"\n        Test that permissions are created with a custom content type app_label.\n        \"\"\"\n        from django.apps import AppConfig\n        from django.contrib.auth.models import Permission\n\n        class CustomAppConfig(AppConfig):\n            name = 'custom_auth'\n            verbose_name = 'Custom Auth'\n\n        CustomAppConfig.default_permissions = ['custom_permission']\n\n        with patch('django.apps.apps.get_app_config', side_effect=lambda name: CustomAppConfig if name == 'custom_auth' else None):\n            create_permissions(CustomAppConfig, verbosity=0)\n\n        permission_content_type = ContentType.objects.get_by_natural_key('custom_auth', 'permission')\n        self.assertEqual(Permission.objects.filter(content_type=permission_content_type).count(), 1)\n"], "sample_253": ["    def test_stat_returns_stat_for_file(self, mocked_stat):\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file]):\n            self.assertEqual(self.reloader._stat_file(self.existing_file), stat(os.stat(self.existing_file)))\n", "    def test_tick_handles_non_existing_files(self, mock_notify_file_changed):\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.nonexistent_file]):\n            ticker = self.reloader.tick()\n            next(ticker)\n            self.assertEqual(mock_notify_file_changed.call_count, 0)\n\n", "    def test_should_stop_based_on_mtime(self):\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file]):\n            self.reloader.snapshot_files()[self.existing_file] = 1\n            self.assertFalse(self.reloader._should_stop())\n        self.increment_mtime(self.existing_file)\n        self.assertTrue(self.reloader._should_stop())\n", "    def test_snapshot_files_no_duplicates_with_relative_paths(self):\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.tempdir / 'file1.py', self.tempdir / 'file2.py']):\n            snapshot = list(self.reloader.snapshot_files())\n            self.assertEqual(len(snapshot), 2) \n", "    def test_snapshot_files_non_existent_file(self):\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.nonexistent_file]):\n            snapshot = dict(self.reloader.snapshot_files())\n            self.assertNotIn(self.nonexistent_file, snapshot)\n"], "sample_59": ["    def test_ordering_with_related_model(self):\n        \"\"\"\n        Filtering with related models should handle ordering correctly.\n        \"\"\"\n        department = Department.objects.create(name='IT')\n        worker1 = Worker.objects.create(department=department, name='Alice')\n        worker2 = Worker.objects.create(department=department, name='Bob')\n        worker3 = Worker.objects.create(department=department, name='Charlie')\n\n        # Ordering should work with related model names\n        self.assertQuerysetEqual(\n            Worker.objects.filter(department__name='IT').order_by('name'),\n            ['Alice', 'Bob', 'Charlie'],\n            attrgetter('name')\n        )\n\n        # Ordering should work even if related model is not primary field\n        self.assertQuerysetEqual(\n            Worker.objects.filter(department__name='IT').order_by('-name'),\n            ['Charlie', 'Bob', 'Alice'],\n            attrgetter('name')\n        )\n", "    def test_auto_generated_field_names(self):\n        \"\"\"\n        Regression test for #13926: \n\n        - Auto-generated field names in related models should not clash with\n          regular model fields.\n        - This also tests that the 'db_column' attribute is set correctly.\n        \"\"\"\n        with self.subTest('check_related_model'):\n            model1 = Model1.objects.create(pkey=1000)\n            m2 = Model2.objects.create(model1=model1, name='Foo')\n            self.assertEqual(m2.model1_id, model1.pkey)\n            self.assertIn('model1', m2._meta.fields)\n        with self.subTest('check_related_name_clash'):\n            class ClashingModel(models.Model):\n                field = models.CharField(max_length=255)\n            ClashingModel.objects.create(field='clash')\n            with self.assertRaises(models.ValidationError):\n                class RelatedClash(models.Model):\n                    clash = models.ForeignKey(ClashingModel, on_delete=models.CASCADE)\n\n", "    def test_related_object_with_multiple_related_managers(self):\n        \"\"\"\n        Regression test for #23139: Ordering by a field in a related manager\n        that shares the same related field with a 'related_name' field.\n        \"\"\"\n        # Create some related objects.\n        dept1 = Department.objects.create(name=\"Sales\")\n        dept2 = Department.objects.create(name=\"Marketing\")\n        worker1 = Worker.objects.create(department=dept1, name=\"Alice\")\n        worker2 = Worker.objects.create(department=dept2, name=\"Bob\")\n\n        # Create some articles.\n        article1 = Article.objects.create(headline=\"Article 1\", pub_date=datetime.datetime.now(), author=worker1)\n        article2 = Article.objects.create(headline=\"Article 2\", pub_date=datetime.datetime.now(), author=worker2)\n\n        # Assert that we can access related objects by their related manager.\n        self.assertEqual(\n            article1.author.name,\n            \"Alice\",\n        )\n        self.assertEqual(\n            article2.author.name,\n            \"Bob\",\n        )\n\n\n\n        # Assert that ordering by a field in a related manager\n        # that shares the same related field with a 'related_name' field\n        # works as expected.\n        self.assertQuerysetEqual(\n            worker1.articles.order_by(\"headline\"),\n            [\n                article1,\n            ]\n        )\n        self.assertQuerysetEqual(\n            worker2.articles.order_by(\"headline\"),\n            [\n                article2,\n            ]\n        )\n", "    def test_ordering_with_related_field(self):\n        \"\"\"\n        Regression test for #16188: Ordering by related fields with\n        explicit OrderWithRespectTo.\n        \"\"\"\n        dept1 = Department.objects.create(name='A')\n        dept2 = Department.objects.create(name='B')\n        worker1 = Worker.objects.create(department=dept1, name='Alice')\n        worker2 = Worker.objects.create(department=dept1, name='Bob')\n        worker3 = Worker.objects.create(department=dept2, name='Charlie')\n\n        # Order by related field with explicit OrderWithRespectTo\n        ordered_workers = Worker.objects.order_by(\n            'department__name', '-ordering'\n        )\n\n        # Assert expected order\n        self.assertEqual(list(ordered_workers), [\n            worker3,  # Department B comes first\n            worker1,  # Alice comes before Bob because ordering is descending\n            worker2\n        ])\n\n        # Order by related field with using \"__\" lookup\n        ordered_workers = Worker.objects.order_by(\n            'department__name__iexact', '-ordering'\n        )\n\n        # Assert expected order\n        self.assertEqual(list(ordered_workers), [\n            worker3,\n            worker1,\n            worker2\n        ]) \n", "    def test_model_ordering_with_related_object(self):\n        \"\"\"\n        Test ordering related objects using both model name and field name\n        \"\"\"\n        dept1 = Department.objects.create(name='Tech')\n        dept2 = Department.objects.create(name='Sales')\n        \n        worker1 = Worker.objects.create(department=dept1, name='Alice')\n        worker2 = Worker.objects.create(department=dept1, name='Bob')\n        worker3 = Worker.objects.create(department=dept2, name='Charlie')\n\n        # Order by department name using model name\n        ordered_workers = Worker.objects.order_by('department__name')\n        self.assertEqual([w.name for w in ordered_workers], ['Alice', 'Bob', 'Charlie'])\n\n        # Order by department name using field name\n        ordered_workers = Worker.objects.order_by('department__name')\n        self.assertEqual([w.name for w in ordered_workers], ['Alice', 'Bob', 'Charlie'])\n\n        # Order by related object field with both model name\n        # and field name\n        ordered_workers = Worker.objects.order_by('department__name')\n        self.assertEqual([w.name for w in ordered_workers], ['Alice', 'Bob', 'Charlie'])\n"], "sample_621": ["    def test_isel(self, indexes) -> None:\n        assert indexes.isel({\"x\": 1, \"y\": 0, \"z\": [0, 2]})[\"x\"].equals(\n            pd.Index([2], name=\"x\")\n        )\n        assert indexes.isel({\"x\": [0, 2], \"y\": [1, 2]}).equals(\n            indexes\n        )\n\n        with pytest.raises(ValueError, match=r\"cannot perform isel on single index\"):\n            indexes.isel({\"one\": 0})\n", "    def test_concat(self, indexes) -> None:\n        combined_indexes = indexes.concat(\n            {\"x\": [1, 2, 3, 4], \"y\": [5, 6, 7, 8], \"z\": [(\"a\", 1), (\"a\", 2), (\"b\", 1), (\"b\", 2)]}\n        )\n        assert combined_indexes.get_unique() == [\n            (\n                PandasIndex(pd.Index([1, 2, 3, 4], name=\"x\"), \"x\"),\n                PandasIndex(pd.Index([5, 6, 7, 8], name=\"y\"), \"y\"),\n                PandasMultiIndex(pd.MultiIndex.from_product([[\"a\", \"b\"], [1, 2]], names=(\"one\", \"two\")), \"z\"),\n            )\n        ]\n\n        with pytest.raises(ValueError, match=\".*dimensions not equal.*\"):\n            indexes.concat({\"x\": [1, 2], \"y\": [5, 6, 7, 8]})\n        with pytest.raises(ValueError, match=\".*multi-index has different length.*\"):\n            indexes.concat({\"x\": [1, 2, 3, 4], \"y\": [5, 6, 7]})\n", "    def test_reindex_like(self, indexes) -> None:\n        new_indexes = {\n            \"x\": PandasIndex(pd.Index([1, 2, 3, 4], name=\"x\"), \"x\"),\n            \"y\": PandasIndex(pd.Index([4, 5, 6, 7], name=\"y\"), \"y\"),\n            \"z\": PandasMultiIndex(\n                pd.MultiIndex.from_arrays([[1, 2, 3, 4], [2, 3, 4, 5]], names=(\"one\", \"two\")),\n                \"z\",\n            ),\n        }\n        reindexed = indexes.reindex_like(new_indexes)\n        assert reindexed[\"x\"].equals(new_indexes[\"x\"])\n        assert reindexed[\"y\"].equals(new_indexes[\"y\"])\n        assert reindexed[\"z\"].equals(new_indexes[\"z\"]) \n", "    def test_isel(self, indexes) -> None:\n        # Test isel with single and multi-level indexers\n        assert indexes.isel({\"x\": 1}) == {\n            \"x\": indexes[\"x\"][1],\n            \"y\": indexes[\"y\"],\n            \"z\": indexes[\"z\"],\n            \"one\": indexes[\"one\"],\n            \"two\": indexes[\"two\"],\n        }\n        assert indexes.isel({\"y\": [0, 2]}) == {\n            \"x\": indexes[\"x\"],\n            \"y\": indexes[\"y\"][[0, 2]],\n            \"z\": indexes[\"z\"],\n            \"one\": indexes[\"one\"],\n            \"two\": indexes[\"two\"],\n        }\n        assert indexes.isel({\"z\": slice(1, 3)}) == {\n            \"x\": indexes[\"x\"],\n            \"y\": indexes[\"y\"],\n            \"z\": indexes[\"z\"][1:3],\n            \"one\": indexes[\"one\"],\n            \"two\": indexes[\"two\"],\n        }\n        assert indexes.isel({\"one\": [\"a\"], \"two\": [2]}) == {\n            \"x\": indexes[\"x\"],\n            \"y\": indexes[\"y\"],\n            \"z\": indexes[\"z\"],\n            \"one\": indexes[\"one\"][[\"a\"]],\n            \"two\": indexes[\"two\"][[2]],\n        }\n\n\n", "    def test_isel(self, indexes) -> None:\n        assert indexes.isel({\"x\": [0, 2]}) == {\"x\": indexes[\"x\"][[0, 2]]}\n        assert indexes.isel({\"y\": [1], \"z\": [1]}) == {\n            \"y\": indexes[\"y\"][[1]],\n            \"z\": indexes[\"z\"][[1]],\n        }\n\n        with pytest.raises(ValueError, match=r\"no index found.*\"):\n            indexes.isel({\"no_coord\": [0]})\n        with pytest.raises(IndexError, match=r\"index.*out of range.*\"):\n            indexes.isel({\"x\": [3]})\n\n\n\n"], "sample_449": ["    def test_close_connections_on_request_end(self):\n        \"\"\"\n        Tests that connections are closed after the request is handled.\n        \"\"\"\n        request = WSGIRequest(self.request_factory.get(\"/\").environ)\n        client_address = (\"192.168.2.0\", 8080)\n        with self.assertLogs(\"django.server\", \"INFO\") as cm:\n            server = ThreadedWSGIServer((\"localhost\", 0), WSGIRequestHandler)\n            server.connections_override = {\n                \"default\": \"django.db.backends.sqlite3\"\n            }\n            with server.make_serving_thread(1) as thread:\n                with server.accept_connections():\n                    server.handle_error(request, client_address)\n        self.assertEqual(len(cm.output), 1)\n        self.assertIn(\n            f\"- Closed connection from {client_address}\", cm.output[0]\n        )\n", "    def test_connections_override(self):\n            return [b\"Connection overridden\"]\n\n        server = WSGIServer((\"localhost\", 0), WSGIRequestHandler)\n        connections_override = {\"default\": \"override conn\"}\n        with self.assertLogs(\"django.server\", \"INFO\") as cm:\n            with server.test_server(\n                get_wsgi_application=lambda: test_app,\n                connections=connections_override,\n            ) as httpd:\n                response = httpd.test_client().get(\"/\")\n                self.assertEqual(response.content, b\"Connection overridden\")\n\n        self.assertEqual(len(cm.records), 0)\n\n", "    def test_close_connections_on_server_shutdown(self):\n        request = WSGIRequest(self.request_factory.get(\"/\").environ)\n        client_address = (\"127.0.0.1\", 8080)\n        with self.assertLogs(\"django.server\", \"INFO\") as cm:\n            server = ThreadedWSGIServer((\"localhost\", 0), WSGIRequestHandler)\n            server.connections_override = {\"default\": None}\n            server.handle_error(request, client_address)\n        self.assertEqual(\n            [\n                record.getMessage()\n                for record in cm.records\n                if record.name == \"django.server\"\n            ],\n            [\n                f\"- Broken pipe from {client_address}\",\n            ],\n        )\n\n        # Verify that the connection was closed before the server shut down\n        self.assertEqual(connections.db.connections, 0)\n", "    def test_close_connections_on_thread_shutdown(self):\n        connections_override = {\n            \"default\": connections.get_connection(\"default\")\n        }\n        request = self.request_factory.get(\"/\")\n        rfile = BytesIO()\n        rfile.write(b\"GET / HTTP/1.0\\r\\n\")\n        rfile.seek(0)\n        wfile = UnclosableBytesIO()\n\n            if mode == \"rb\":\n                return rfile\n            elif mode == \"wb\":\n                return wfile\n\n        server = ThreadedWSGIServer(\n            (\"localhost\", 0),\n            WSGIRequestHandler,\n            connections_override=connections_override,\n        )\n        request = Stub(makefile=makefile)\n\n        with self.assertLogs(\"django.server\", \"INFO\") as cm:\n            server.process_request_thread(\n                request, (\"127.0.0.1\", 0)\n            )\n\n        self.assertEqual(len(cm.records), 1)\n        self.assertEqual(cm.records[0].getMessage(), \"- Closing connection to 127.0.0.1\")\n        self.assertEqual(connections.get_connection(\"default\").closed, True)\n\n\n\n\n", "    def test_connections_override(self):\n        \"\"\"Test connections override in ThreadedWSGIServer.\"\"\"\n        with self.subTest(exception=BrokenPipeError):\n            from django.db import connection\n            connections_override = {}\n            connections_override[\"default\"] = connection\n\n                start_response(\"200 OK\", [])\n                return [b\"OK\"]\n\n            server = ThreadedWSGIServer((\"localhost\", 0), WSGIRequestHandler, connections_override=connections_override)\n            try:\n                with server.make_request_handler((\"localhost\", 0)) as handler:\n                    response = handler(b\"GET / HTTP/1.1\\r\\n\\r\\n\", {}, {})\n                self.assertEqual(response[0][0], b\"b'OK'\")\n            finally:\n                server.server_close()\n"], "sample_246": ["    def test_makemessages_no_settings_directory(self):\n        os.rmdir('django/locale')\n        out, err = self.run_django_admin(['makemessages', '-l', 'en', '-v', '0'])\n        self.assertNoOutput(err)\n\n        self.assertIn(\"Using default locale paths\", out)\n", "    def test_makemessages_with_no_locale_path(self):\n        with override_settings(LOCALE_PATHS=[]):\n            with self.assertRaises(management.CommandError) as cm:\n                management.call_command('makemessages', locale=[LOCALE], verbosity=0)\n            self.assertIn(\"Unable to find a locale path to store translations for file __init__.py\", str(cm.exception))\n", "    def test_makemessages_with_custom_domain(self):\n        management.call_command('makemessages', locale=['en'], domain='my_app', verbosity=0)\n        self.assertTrue(os.path.exists('locale/en/LC_MESSAGES/my_app.po'))\n\n", "    def test_makemessages_with_invalid_locale(self):\n\n        with self.assertRaisesRegex(\n                management.CommandError, 'Invalid locale: \"invalid_locale\"'\n        ):\n            self.run_django_admin(['makemessages', '-l', 'invalid_locale', '-v', '0'])\n", "    def test_makemessages_no_settings_with_custom_locale(self):\n        _, err = self.run_django_admin(['makemessages', '-l', 'invalid_locale', '-v', '0'])\n        self.assertIn(\"Unknown locale 'invalid_locale'\", err)\n"], "sample_151": ["    def test_mti_inheritance_model_removal_with_fk(self):\n        Animal = ModelState('app', 'Animal', [\n            (\"id\", models.AutoField(primary_key=True)),\n        ])\n        Dog = ModelState('app', 'Dog', [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"owner\", models.ForeignKey(\"app.Person\", models.CASCADE)),\n        ], bases=('app.Animal',))\n        Person = ModelState('app', 'Person', [\n            (\"id\", models.AutoField(primary_key=True)),\n        ])\n        changes = self.get_changes([Animal, Dog, Person], [Animal, Person])\n        self.assertNumberMigrations(changes, 'app', 2)\n        self.assertOperationTypes(changes, 'app', 0, ['DeleteModel'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='Dog')\n        self.assertOperationTypes(changes, 'app', 1, ['RemoveField'])\n        self.assertOperationAttributes(changes, 'app', 1, 0, name='Person', model_name='Dog', field_name='owner')\n", "    def test_mti_inheritance_model_removal_base_dependent(self):\n        Animal = ModelState('app', 'Animal', [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"breed\", models.CharField(max_length=255)),\n        ])\n        Dog = ModelState('app', 'Dog', [], bases=('app.Animal',))\n        changes = self.get_changes([Animal, Dog], [Animal])\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['DeleteModel', 'RemoveField'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='Dog')\n", "    def test_mti_inheritance_model_removal_with_fields(self):\n        Animal = ModelState('app', 'Animal', [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=255)),\n        ])\n        Dog = ModelState('app', 'Dog', [], bases=('app.Animal',))\n        Dog.fields.append(('breed', models.CharField(max_length=255)))\n        changes = self.get_changes([Animal, Dog], [Animal])\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['DeleteModel'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='Dog')\n\n", "    def test_mti_inheritance_model_removal_order(self):\n        Animal = ModelState('app', 'Animal', [\n            (\"id\", models.AutoField(primary_key=True)),\n        ])\n        Dog = ModelState('app', 'Dog', [], bases=('app.Animal',))\n        Cat = ModelState('app', 'Cat', [], bases=('app.Animal',))\n        changes = self.get_changes([Animal, Dog, Cat], [Animal])\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['DeleteModel', 'DeleteModel'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='Dog')\n        self.assertOperationAttributes(changes, 'app', 0, 1, name='Cat')\n", "    def test_order_with_respect_to_on_m2m_field(self):\n        \"\"\"\n        Setting order_with_respect_to on an M2M field does things in the right order.\n        \"\"\"\n        changes = self.get_changes(\n            [self.author_empty], [self.author_with_order_with_respect_to_m2m]\n        )\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"AddField\", \"AlterField\", \"AlterModelOptions\"])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"books\")\n        self.assertOperationAttributes(changes, 'testapp', 0, 1, name=\"books\", order_with_respect_to=\"author_name\")\n        self.assertOperationAttributes(changes, 'testapp', 0, 2, name=\"Author\", options={\"default_related_name\": \"related_name\"}) \n\n\n"], "sample_1199": ["def test_combined_tensor_printing():\n    combined_tensor_printing(True)\n    assert (TP(Qubit('1'), Qubit('2'))).pretty() == '|{1,2}>'\n    combined_tensor_printing(False)\n    assert (TP(Qubit('1'), Qubit('2'))).pretty() == 'Qubit(\"1\")xQubit(\"2\")'\n", "def test_tensor_product_with_matrices():\n    assert TensorProduct(mat1, mat2) == \\\n        Matrix([\n            [1*2*I, 2*3, 2*3*I, 2*4*I],\n            [1 + I, 3*2, (1 + I)*3, (1 + I)*4*I],\n            [1*4*I, 3*4*I, 4*2*I, 4*2*I],\n            [1*4*I, 3*4*I, 4*2*I, 4*2*I]\n        ])\n\n    assert TensorProduct(mat1, mat2).simplify() == \\\n        TensorProduct(mat1, mat2)\n\n\n\n", "def test_tensor_product_commutative():\n\n    A = symbols('A', commutative=True)\n    B = symbols('B', commutative=True)\n    C = symbols('C', commutative=True)\n\n    assert TP(A, B).is_commutative is True\n    assert TP(A, B) == TP(B, A)\n    assert TP(A*B, C) == TP(A, C)*TP(B, C)\n\n\n", "def test_tensor_product_combined_printing():\n    combined_printing(True)\n    assert str(TensorProduct(Qubit('0'), Qubit('1')) + TensorProduct(Qubit('1'), Qubit('0'))) == 'Qubit(0)Qubit(1) + Qubit(1)Qubit(0)'\n    combined_printing(False)  \n\n\n", "def test_tensor_product_outer_product():\n    A, B = symbols('A B', commutative=False)\n    assert TensorProduct(OuterProduct(A, B), Qubit('1')) == \\\n        TensorProduct(A*Qubit('1'), B*Qubit('1'))\n    assert TensorProduct(Qubit('1'), OuterProduct(A, B)) == \\\n        TensorProduct(A*Qubit('1'), B*Qubit('1'))\n"], "sample_204": ["    def test_missing_imports(self):\n        \"\"\"MigrationLoader raises an ImportError if a migration has missing imports.\"\"\"\n        with self.assertRaises(ImportError):\n            MigrationLoader(connection)\n\n\n\n", "    def test_wrong_migration_module(self):\n        with self.assertRaises(ImportError):\n            loader = MigrationLoader(connection)\n", "    def test_multiple_apps(self):\n        \"\"\"\n        The MigrationLoader should be able to handle multiple apps with migrations.\n        \n        This test ensures that MigrationLoader can correctly:\n        1. identify migrations from multiple apps in the INSTALLED_APPS\n        2. store the migrated_apps and unmigrated_apps correctly \n        \"\"\"\n        loader = MigrationLoader(connection)\n        loader.build_graph()\n        expected_migrated_apps = {'migrations', 'apps.app2'}\n        self.assertEqual(loader.migrated_apps, expected_migrated_apps)\n        expected_unmigrated_apps = set()\n        self.assertEqual(loader.unmigrated_apps, expected_unmigrated_apps)\n\n\n", "    def test_multiple_apps(self):\n        loader = MigrationLoader(connection)\n        self.assertIn(('migrations', '0001_initial'), loader.disk_migrations)\n        self.assertIn(('app1', '0001_initial'), loader.disk_migrations)\n        self.assertIn(('app2', '0001_initial'), loader.disk_migrations)\n", "    def test_order_dependent_migrations(self):\n        \"\"\"\n        Migrations can depend on each other and it should be enforced.\n        \"\"\"\n        with self.assertRaises(AmbiguityError):\n            loader = MigrationLoader(connection)\n            loader.graph.forwards_plan(('migrations', '0001_second'))\n"], "sample_541": ["compilation error", "def test_polygon_selector_events(ax):\n    verts = [(50, 50), (150, 50), (50, 150)]\n    event_sequence = [\n        *polygon_place_vertex(*verts[0]),\n        *polygon_place_vertex(*verts[1]),\n        *polygon_place_vertex(*verts[2]),\n        *polygon_place_vertex(*verts[0]),\n    ]\n\n    tool = widgets.PolygonSelector(ax, onselect=noop, draw_bounding_box=draw_bounding_box)\n    onselect_events = []\n    tool.onselect = lambda verts: onselect_events.append(verts)\n\n    for (etype, event_args) in event_sequence:\n        do_event(tool, etype, **event_args)\n    assert onselect_events == [verts]\n\n    # Remove an extra point after the polygon is closed\n    event_sequence = [*polygon_remove_vertex(*verts[1])]\n    for (etype, event_args) in event_sequence:\n        do_event(tool, etype, **event_args)\n\n    assert onselect_events == [verts]\n\n\n\n\n", "compilation error", "def test_MultiCursor_key_events(horizOn, vertOn):\n    (ax1, ax3) = plt.figure().subplots(2, sharex=True)\n    ax2 = plt.figure().subplots()\n\n    multi = widgets.MultiCursor(\n        None, (ax1, ax2), useblit=False, horizOn=horizOn, vertOn=vertOn\n    )\n\n    # mock keydown and keyup events\n    event_keydown = mock_event(ax1, key='c')\n    event_keyup = mock_event(ax1, key=' ')\n    multi.on_key_press(event_keydown)\n    multi.on_key_release(event_keyup)\n\n    # Check if cursor position updates\n    assert multi.vlines[0].get_xdata() == multi.x\n\n\n\n    \n", "def test_multicursor_on_close():\n    (ax1, ax3) = plt.figure().subplots(2, sharex=True)\n    ax2 = plt.figure().subplots()\n\n    multi = widgets.MultiCursor(\n        None, (ax1, ax2), useblit=False\n    )\n\n    # Trigger the on_close event\n    multi.on_close()\n\n    # Assert that the lines have been removed\n    assert len(multi.vlines) == 0\n    assert len(multi.hlines) == 0\n"], "sample_256": ["    def test_bug_18155_empty_password(self):\n        user = User.objects.get(username='testclient')\n        form = AdminPasswordChangeForm(user, {'password1': '', 'password2': ''})\n        self.assertTrue(form.is_valid())\n        self.assertEqual(form.cleaned_data['password1'], '')\n        self.assertEqual(form.cleaned_data['password2'], '')\n", "    def test_invalid_password_length(self):\n        user = User.objects.get(username='testclient')\n        data = {\n            'password1': 't',\n            'password2': 't',\n        }\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        self.assertIn('password1', form.errors)\n        self.assertIn('The password must be at least 8 characters long.', form.errors['password1'])\n\n\n\n", "    def test_password_too_short(self):\n        user = User.objects.get(username='testclient')\n        data = {'password1': 'tes', 'password2': 'tes'}\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        self.assertIn(\n            'This password is too short. It must contain at least 8 characters.',\n            form.errors['password1'],\n        )\n        self.assertIn(\n            'This password is too short. It must contain at least 8 characters.',\n            form.errors['password2'],\n        )\n", "    def test_bug_19349_password_field_not_rendered(self):\n        user = User.objects.get(username='testclient')\n        form = AdminPasswordChangeForm(user)\n        html = form.as_p()\n        self.assertNotIn('name=\"password1\"', html)\n\n\n", "    def test_invalid_password_fields(self):\n        user = User.objects.get(username='testclient')\n        data = {\n            'password1': 'password',\n            'password2': 'passwordabc',\n        }\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['password1'], [\n            form.error_messages['password_too_short']\n        ])\n        self.assertEqual(form.errors['password2'], [\n            form.error_messages['password_too_short']\n        ])\n\n"], "sample_1098": ["def test_hyper_large_args():\n    from sympy.functions.special.hyper import hyper, meijerg\n    from sympy import oo, pi, symbols, S, exp\n\n    a = symbols('a')\n    x = symbols('x')\n\n    # Test that hyper works for large arguments\n    assert hyper((a*500, a*1000), (a*500 + 1, a*1000 + 1, a*1500), x).n() \\\n        is not None\n    assert hyper((a*1000), (a*1000 + 1, a*1000 + 2), x).n() \\\n        is not None\n\n    # Test that meijerg works for large arguments\n    assert meijerg((a*500, a*1000), (a*500 + 1, a*1000 + 1, a*1500), x) \\\n        .n() is not None\n    assert meijerg((a*1000), (a*1000 + 1, a*1000 + 2), x) \\\n        .n() is not None\n\n    # Test that hyper works for arguments involving pi\n    assert hyper((a*pi, a*pi*2), (a*pi + 1, a*pi*2 + 1), x).n() \\\n        is not None\n    assert hyper((a*pi), (a*pi + 1), x).n() \\\n        is not None\n\n    # Test that meijerg works for arguments involving pi\n    assert meijerg((a*pi, a*pi*2), (a*pi + 1, a*pi*2 + 1), x) \\\n        .n() is not None\n    assert meijerg((a*pi), (a*pi + 1), x) \\\n        .n() is not None\n\n    # Test that hyper works for arguments involving exp\n    assert hyper((a*exp(1), a*exp(2)), (a*exp(1) + 1, a*exp(2) + 1), x).n() \\\n        is not None\n    assert", "def test_meijerg_zeros():\n    from sympy.functions import besselj\n    from sympy import S, oo\n    a, b = symbols('a b')\n    z = symbols('z', complex=True)\n    for a0 in [0, 1, 2, -1, -2]:\n        for b0 in [0, 1, 2, -1, -2]:\n            meijerg_z = meijerg([], [(a0), (b0)], [0, 0], [], z)\n            for i in range(5):\n                val = meijerg_z.subs(z, 1 + 2*i*pi*I).n()\n                assert abs(val) > 1e-10\n            # Test that besselj(a0, z) has some zeros.\n            bessel = besselj(a0, z)\n            for i in range(5):\n                val = bessel.subs(z, i*pi).n()\n                if abs(val) > 1e-10:\n                    print(f'Besselj({a0}, {i*pi}) = {val}')\n\n\n", "def test_meijerg_series():\n    from sympy import O, symbols, sin, cos, exp, exp_polar\n    a, b, c, d, n, z = symbols('a b c d n z')\n    assert meijerg([[a], [b]], [[c], [d]], z).series(z, 0, 5) == \\\n        z**0*gamma(c)*gamma(d) / gamma(c + d) + \\\n        z**1*gamma(a + 1)*gamma(c + 1)*gamma(d)/ (gamma(a + c + d + 1)*gamma(b + 1)) + \\\n        z**2*gamma(a + 2)*gamma(c + 2)*gamma(d)/ (gamma(a + c + d + 2)*gamma(b + 2)) + \\\n        z**3*gamma(a + 3)*gamma(c + 3)*gamma(d)/ (gamma(a + c + d + 3)*gamma(b + 3)) + \\\n        z**4*gamma(a + 4)*gamma(c + 4)*gamma(d)/ (gamma(a + c + d + 4)*gamma(b + 4)) + O(z**5)\n    assert meijerg([1, 1], [1, 1], z).series(z, 0, 5) == \\\n        z**0*1 + z**1*1 + z**2*1 + z**3*1 + z**4*1 + O(z**5)\n    assert meijerg([], [1], [0], [], z).series(z, 0, 5) == 1 + z + z**2/2 + z**3/6 + z**4/24 + O(z**5)\n    assert meijerg([], [], [0], [1], z).series(z, 0, 5) == 1 + z + z**2/2 + z**3/6 + z**4/24 + O(z**5)\n    assert meijerg([1], [1], [0], [], z).series(z, 0, 5) == 1 + z + z", "def test_meijerg_with_special_values():\n    from sympy import besselj, bessely, gamma, pi, oo, S, log\n    a, b, c, d, z = symbols('a b c d z')\n\n    assert meijerg([1, 1], [1], [1], [0], z).rewrite('besselj') == besselj(0, z)\n    assert meijerg([1, 1], [1], [1], [0], z).rewrite('bessely') == bessely(0, z)\n    assert meijerg([], [1], [0], [], z).rewrite('exp') == exp(z) # exp(z)\n    assert meijerg([1], [], [], [1], z).rewrite('log') == -log(z)\n\n    #test special values for Gamma function\n    assert meijerg([], [], [0], [], z).rewrite('gamma') == gamma(1 - z)\n    assert meijerg([], [], [1], [], z).rewrite('gamma') == gamma(z)\n    assert meijerg([], [1], [0], [], z).rewrite('gamma') == gamma(z + 1) / z\n\n    # test for infinity limits\n    assert limit(meijerg([], [], [0], [], z), z, oo) == 0\n    assert limit(meijerg([1], [], [], [0], z), z, oo) == oo\n\n\n", "compilation error"], "sample_807": ["    def test_calibration_with_pipeline():\n        \"\"\"Test calibration with pipeline including a preprocessor.\"\"\"\n        n_samples = 100\n        X, y = make_classification(n_samples=2 * n_samples, n_features=6,\n                                   random_state=42)\n\n        # split train and test\n        X_train, y_train = X[:n_samples], y[:n_samples]\n        X_test = X[n_samples:]\n\n        # Create a pipeline with a simple scaling step and a classifier\n        pipeline = Pipeline(\n            steps=[\n                ('scaler', StandardScaler()),\n                ('clf', RandomForestClassifier())\n            ]\n        )\n\n        # Fit the pipeline to the training data\n        pipeline.fit(X_train, y_train)\n\n        # Create a calibrated version of the pipeline\n        calibrated_pipeline = CalibratedClassifierCV(pipeline, method='sigmoid', cv=2)\n        calibrated_pipeline.fit(X_train, y_train)\n\n        # Make predictions using the calibrated pipeline and compare them to\n        # the predictions of the original pipeline\n        y_pred_calibrated = calibrated_pipeline.predict_proba(X_test)\n        y_pred_original = pipeline.predict_proba(X_test)\n\n        # Check that the calibrated predictions have a better brier score\n        # than the original predictions\n        assert_greater(brier_score_loss(y_test, y_pred_original),\n                       brier_score_loss(y_test, y_pred_calibrated))\n\n\n\n", "def test_calibration_with_sparse_data():\n    \"\"\"Test calibration with sparse data\"\"\"\n    n_samples = 100\n    n_features = 1000\n    X, y = make_classification(n_samples=n_samples, n_features=n_features,\n                               random_state=42)\n    X_sparse = sparse.csr_matrix(X)\n\n    # Naive Bayes with calibration\n    for method in ['isotonic', 'sigmoid']:\n        clf = MultinomialNB()\n        pc_clf = CalibratedClassifierCV(clf, method=method, cv=2)\n        pc_clf.fit(X_sparse, y)\n        probs_sparse = pc_clf.predict_proba(X_sparse)\n\n        # Check that probabilities are close to the probabilities obtained\n        # with dense data\n        clf.fit(X, y)\n        probs_dense = clf.predict_proba(X)\n        assert_array_almost_equal(probs_dense, probs_sparse.toarray())\n\n        # Test that predict_proba is called with the sparse matrix\n        # Note: We need to convert back to dense array for comparison\n        with patch.object(pc_clf.calibrated_classifier,\n                         'predict_proba', autospec=True) as mock_proba:\n            pc_clf.predict_proba(X_sparse)\n        mock_proba.assert_called_once_with(X_sparse)\n\n", "def test_calibration_sparse_data():\n    \"\"\"Test calibration with sparse data\"\"\"\n    n_samples = 100\n    n_features = 1000\n    X, y = make_classification(n_samples=n_samples, n_features=n_features,\n                               random_state=42)\n\n    # convert to sparse format\n    X_sparse = scipy.sparse.csr_matrix(X)\n\n    clf = MultinomialNB()\n    clf.fit(X, y)\n\n    for method in ['isotonic', 'sigmoid']:\n        pc_clf = CalibratedClassifierCV(clf, method=method, cv=2)\n        pc_clf.fit(X_sparse, y)\n\n        # check that the predictions are consistent with the dense format\n        prob_pos_clf = clf.predict_proba(X)[:, 1]\n        prob_pos_pc_clf = pc_clf.predict_proba(X_sparse)[:, 1]\n        assert_array_almost_equal(prob_pos_clf, prob_pos_pc_clf)\n\n\n\n", "    def test_calibration_sparse_matrix():\n        \"\"\"Test calibration with sparse matrices.\"\"\"\n        n_samples = 100\n        n_features = 100\n        X, y = make_classification(n_samples=n_samples, n_features=n_features,\n                                   random_state=42)\n\n        # Convert to sparse matrix\n        X_sparse = sparse.csr_matrix(X)\n\n        clf = LinearSVC()\n        clf.fit(X, y)\n\n        # Test calibration with sparse input\n        calibrated_clf = CalibratedClassifierCV(clf, method='sigmoid', cv=2)\n        calibrated_clf.fit(X_sparse, y)\n        \n        # Predict probabilities on both dense and sparse inputs\n        prob_dense = calibrated_clf.predict_proba(X)\n        prob_sparse = calibrated_clf.predict_proba(X_sparse)\n        assert_array_almost_equal(prob_dense, prob_sparse)\n\n        # Ensure sparse matrix is not modified\n        assert_array_equal(X_sparse.toarray(), calibrated_clf.X_train_)  \n", "def test_calibration_multi_label():\n    \"\"\"Test calibration for multi-label classification\"\"\"\n    n_samples = 10\n    X, y = make_classification(n_samples=n_samples, n_classes=3,\n                               random_state=42, multi_label=True)\n    \n    # Use `RandomForestClassifier` because it supports multi-label\n    clf = RandomForestClassifier(n_estimators=10, random_state=42)\n\n    # Naive Bayes with calibration\n    for method in ['isotonic', 'sigmoid']:\n        cal_clf = CalibratedClassifierCV(clf, method=method, cv=2)\n        cal_clf.fit(X, y)\n        probs = cal_clf.predict_proba(X)\n        assert_array_almost_equal(probs.sum(axis=1), np.ones(probs.shape[0]),\n                                 atol=1e-2) \n"], "sample_349": ["    def test_autocomplete_select_with_no_choices(self):\n        with self.assertRaises(TypeError):\n            AutocompleteSelect(Album._meta.get_field('band').remote_field, admin.site)\n", "    def test_render_options_with_remote_related_field(self):\n        # Create a band, an album, and a release event.\n        band = Band.objects.create(name='The Beatles', style='rock')\n        album = Album.objects.create(name='Rubber Soul', band=band)\n        release_event = ReleaseEvent.objects.create(name='Test Target', album=album)\n        # Create a VideoStream with the release event\n        form = VideoStreamForm(initial={'release_event': release_event.pk})\n        output = form.as_table()\n        # Ensure the release event name is rendered as an option.\n        selected_option = '<option value=\"%s\" selected>Test Target</option>' % release_event.pk\n        self.assertIn(selected_option, output)\n", "    def test_media_no_lang_set(self):\n        with translation.override(None):\n            self.assertEqual(AutocompleteSelect(Album._meta.get_field('band').remote_field, admin.site).media._js,\n                             ('admin/js/vendor/jquery/jquery.min.js', 'admin/js/vendor/select2/select2.full.min.js',\n                              'admin/js/jquery.init.js', 'admin/js/autocomplete.js'))\n", "    def test_url_parameters(self):\n        rel = Album._meta.get_field('band')\n        w = AutocompleteSelect(rel, admin.site)\n        params = w.url_parameters()\n        self.assertEqual(\n            params, {'TO_FIELD': 'band', 'IS_POPUP': 1}\n        )\n", "    def test_optgroups(self):\n        beatles = Band.objects.create(name='The Beatles', style='rock')\n        who = Band.objects.create(name='The Who', style='rock')\n        albums = [\n            Album(name='Abbey Road', band=beatles),\n            Album(name='Let It Be', band=beatles),\n            Album(name='Who\\'s Next', band=who),\n        ]\n        Album.objects.bulk_create(albums)\n        form = AlbumForm(initial={'band': beatles.pk, 'featuring': [who.pk]})\n        output = form.as_table()\n        # Ensure the correct options are present.\n        self.assertIn('<optgroup label=\"The Beatles\">', output)\n        self.assertIn('<option value=\"%s\" selected>Abbey Road</option>' % albums[0].pk, output)\n        self.assertIn('<option value=\"%s\" selected>Let It Be</option>' % albums[1].pk, output)\n        self.assertIn('<optgroup label=\"The Who\">', output)\n        self.assertIn('<option value=\"%s\" selected>Who\\'s Next</option>' % albums[2].pk, output)\n\n        # Ensure the 'featuring' options are rendered correctly.\n        featuring_optgroup = '<optgroup label=\"Featuring\">'\n        self.assertIn(featuring_optgroup, output)\n        self.assertIn(f'<option value=\"{who.pk}\" selected>', output)\n\n\n\n"], "sample_898": ["compilation error", "def test_threshold_adjustment_in_multiclass():\n    random_state = check_random_state(0)\n    y_true = random_state.randint(0, 5, size=(100, ))\n    y_pred = random_state.uniform(size=(100, 5))\n    for name in [ 'accuracy', 'f1_score', 'balanced_accuracy']:\n        metric = ALL_METRICS[name]\n        for threshold in [0.1, 0.5, 0.9]:\n            score_unadjusted = metric(y_true, y_pred, average=None)\n            score_adjusted = metric(y_true, y_pred,\n                                    average=None,\n                                    threshold=threshold)\n            assert_not_equal(score_unadjusted, score_adjusted,\n                               \"Score should change with threshold \"\n                               \"adjustment\")\n\n\n\n", "compilation error", "compilation error", "compilation error"], "sample_114": ["    def test_mti_inheritance_model_reordering(self):\n        Animal = ModelState('app', 'Animal', [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=100)),\n        ])\n        Dog = ModelState('app', 'Dog', [], bases=('app.Animal',))\n        Cat = ModelState('app', 'Cat', [], bases=('app.Animal',))\n        changes = self.get_changes([Animal, Dog, Cat], [Animal, Cat, Dog])\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, [\"RemoveField\", \"AddField\"])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name=\"Dog\", model_name='app.Animal')\n", "    def test_mti_inheritance_model_change(self):\n        Animal = ModelState('app', 'Animal', [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=100)),\n        ])\n        Dog = ModelState('app', 'Dog', [], bases=('app.Animal',))\n        changes = self.get_changes([Animal, Dog], [Animal, Dog])\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, [\"CreateModel\", \"CreateModel\"])  # Create both Animal & Dog\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='Animal')\n        self.assertOperationAttributes(changes, 'app', 0, 1, name='Dog')\n\n", "    def test_delete_mti_with_fk_to_base(self):\n        \"\"\"\n        Deleting an MTI model with an FK to the base model should remove the FK.\n        \"\"\"\n        Animal = ModelState('app', 'Animal', [\n            (\"id\", models.AutoField(primary_key=True)),\n        ])\n        Dog = ModelState('app', 'Dog', [\n            (\"animal_id\", models.ForeignKey(Animal, models.CASCADE)),\n            (\"id\", models.AutoField(primary_key=True)),\n        ], bases=(\"app.Animal\",))\n        changes = self.get_changes([Animal, Dog], [Animal])\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, [\"DeleteModel\"])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='Dog')\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='Animal')\n", "    def test_mti_inheritance_abstract_removal(self):\n        Animal = ModelState('app', 'Animal', [\n            (\"id\", models.AutoField(primary_key=True)),\n        ], abstract=True)\n        Dog = ModelState('app', 'Dog', [], bases=('app.Animal',))\n        changes = self.get_changes([Animal, Dog], [])\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['DeleteModel'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='Animal')\n", "    def test_mti_inheritance_model_removal_with_fk(self):\n        \"\"\"\n        #23337 - Removing the MTI model when it has a FK should correctly\n        depened the FK.\n        \"\"\"\n        Owner = ModelState('app', 'Owner', [\n            (\"id\", models.AutoField(primary_key=True)),\n        ])\n        Dog = ModelState('app', 'Dog', [], bases=('app.Animal',),\n                        fields=[(\"owner\", models.ForeignKey(\"app.Owner\", models.CASCADE))])\n        Animal = ModelState('app', 'Animal', [\n            (\"id\", models.AutoField(primary_key=True)),\n        ])\n        changes = self.get_changes([Owner, Animal, Dog], [Owner, Animal])\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['DeleteModel', 'RemoveField'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='Dog')\n        self.assertOperationAttributes(changes, 'app', 0, 1, model_name='Owner', name='Dog')\n"], "sample_1131": ["compilation error", "compilation error", "def test_issue_20083():\n    from sympy import cos\n    p = PythonCodePrinter()\n    assert p.doprint(cos(x**2)) == 'math.cos(x**2)'\n\n\n\n", "def test_sympy_functions():\n    from sympy import cos, sin, tan, log, exp\n    prntr = SymPyPrinter()\n    assert prntr.doprint(cos(x)) == 'sympy.cos(x)'\n    assert prntr.doprint(sin(x)) == 'sympy.sin(x)'\n    assert prntr.doprint(tan(x)) == 'sympy.tan(x)'\n    assert prntr.doprint(log(x)) == 'sympy.log(x)'\n    assert prntr.doprint(exp(x)) == 'sympy.exp(x)'\n", "compilation error"], "sample_914": ["    def test_unparse_assignment(source, expected):\n        module = ast.parse(source)\n        assert ast.unparse(module.body[0]) == expected\n", "def test_unparse_assignment(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0]) == expected\n", "def test_unparse_assignment(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0]) == expected\n", "    def test_unparse_keyword_only_args(source, expected):\n        module = ast.parse(source)\n        assert ast.unparse(module.body[0].value) == expected\n", "def test_unparse_slice(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].value) == expected \n"], "sample_179": ["    def test_deferrable_unique_constraint_with_condition(self):\n        class Model(models.Model):\n            age = models.IntegerField()\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(\n                        fields=['age'],\n                        name='unique_age_gte_100_deferrable',\n                        condition=models.Q(age__gte=100),\n                        deferrable=models.Deferrable.DEFERRED,\n                    ),\n                ]\n\n        errors = Model.check(databases=self.databases)\n        expected = [] if connection.features.supports_deferrable_unique_constraints and connection.features.supports_partial_indexes else [\n            Warning(\n                '%s does not support deferrable unique constraints '\n                'with conditions.' % connection.display_name,\n                hint=(\n                    \"A constraint won't be created. Silence this warning if \"\n                    \"you don't care about it.\"\n                ),\n                obj=Model,\n                id='models.W039',\n            ),\n        ]\n        self.assertEqual(errors, expected)\n", "    def test_unique_constraint_on_proxy_model(self):\n        class Parent(models.Model):\n            value = models.CharField(max_length=100)\n\n        class Proxy(Parent):\n            class Meta:\n                proxy = True\n\n        with self.assertRaises(Exception) as cm:\n            Model.check(databases=self.databases, using='default')\n        self.assertIn(\"Unique constraints cannot be defined on proxy models.\", str(cm.exception))\n", "    def test_deferred_unique_constraints_error_if_deffered_and_not_supported(self):\n        class Model(models.Model):\n            age = models.IntegerField()\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(\n                        fields=['age'],\n                        name='unique_age_deferred',\n                        deferrable=models.Deferrable.DEFERRED,\n                    )\n                ]\n\n        if not connection.features.supports_deferrable_unique_constraints:\n            self.assertEqual(Model.check(databases=self.databases), [\n                Error(\n                    \"The 'deferrable' option is set on 'unique_age_deferred' but \"\n                    \"'%s' does not support deferrable unique constraints.\" %\n                    connection.display_name,\n                    obj=Model._meta.constraints[0],\n                    id='models.E039',\n                ),\n            ])\n", "    def test_deferrable_unique_constraint_deferrable_immediate(self):\n        class Model(models.Model):\n            age = models.IntegerField()\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(\n                        fields=['age'],\n                        name='unique_age_deferrable',\n                        deferrable=models.Deferrable.IMMEDIATE,\n                    ),\n                ]\n\n        errors = Model.check(databases=self.databases)\n        expected = [] if connection.features.supports_deferrable_unique_constraints else [\n            Warning(\n                '%s does not support deferrable unique constraints with '\n                'deferrable=models.Deferrable.IMMEDIATE.'\n                % connection.display_name,\n                hint=(\n                    \"A constraint won't be created. Silence this warning if \"\n                    \"you don't care about it.\"\n                ),\n                obj=Model,\n                id='models.W038',\n            ),\n        ]\n        self.assertEqual(errors, expected)\n", "    def test_deferrable_unique_constraint_with_condition(self):\n        class Model(models.Model):\n            age = models.IntegerField()\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(\n                        fields=['age'],\n                        name='unique_age_deferrable_gte_100',\n                        condition=models.Q(age__gte=100),\n                        deferrable=models.Deferrable.DEFERRED,\n                    ),\n                ]\n\n        errors = Model.check(databases=self.databases)\n        expected = [] if connection.features.supports_partial_deferrable_unique_constraints else [\n            Warning(\n                '%s does not support deferrable unique constraints with conditions.'\n                % connection.display_name,\n                hint=(\n                    \"A constraint won't be created. Silence this warning if \"\n                    \"you don't care about it.\"\n                ),\n                obj=Model,\n                id='models.W039',\n            ),\n        ]\n        self.assertEqual(errors, expected)\n"], "sample_643": ["def test_multi_reporter_json_output(tmp_path):\n    json_file = tmp_path / \"results.json\"\n    text_file = tmp_path / \"results.txt\"\n\n    class JsonReporter(BaseReporter):\n            super().__init__(output)\n            self.json_output = json_file\n        \n            pass  \n        \n            pass\n\n    class TextReporter(BaseReporter):\n            super().__init__(output)\n            self.text_output = text_file\n        \n            pass  \n\n            super().writeln(string)\n        \n            pass\n\n    multi_reporter = MultiReporter(\n        [JsonReporter(json_file.open(\"w\")), TextReporter(text_file.open(\"w\"))], lambda: None\n    )\n\n    with pytest.warns(UserWarning) as record:\n        multi_reporter.open()\n        multi_reporter.add_message(Message(\"C0301\", line=1, args=(1, 2)))\n        multi_reporter.generate_reports()\n\n    assert len(record) == 1\n    assert record[0].message.args[0] == \"The specified reporters (json, text) are incompatible.\"\n\n    # Check if files were created and contain something\n    with open(json_file, 'r', encoding=\"utf-8\") as f:\n        assert f.read() != \"\"\n    with open(text_file, 'r', encoding=\"utf-8\") as f:\n        assert f.read() != \"\"\n\n    \n\n\n\n", "def test_colorize_ansi(capfd):\n    \"\"\"Test the colorize_ansi function with various input.\"\"\"\n    self = TextReporter()\n    # Test no color\n    assert self.colorize_ansi(\"Hello world\", None) == \"Hello world\"\n    # Test color\n    assert self.colorize_ansi(\"Hello world\", self.color_mapping[\"C0301\"]) == (\n        \"\\033[31mHello world\\033[0m\"\n    )\n    # Test no color map\n    assert self.colorize_ansi(\"Hello world\", \"invalid\") == \"Hello world\"\n    # Test with style\n    assert self.colorize_ansi(\n        \"Hello world\",\n        MessageStyle(None, (\"bold\",)),\n    ) == \"\\033[1mHello world\\033[0m\"\n\n", "def test_multi_reporter_output_format(tmp_path, capsys):\n    json_file = tmp_path / \"somefile.json\"\n    text_file = tmp_path / \"somefile.txt\"\n\n    class JsonReporter(BaseReporter):\n            super().__init__()\n            self.out = json_file.open('w')\n\n            json.dump(msg.as_dict(), self.out)\n            self.out.write(\"\\n\")\n\n    class TextReporter(BaseReporter):\n            super().__init__()\n            self.out = text_file.open('w')\n\n            self.out.write(f\"{msg.symbol}:{msg.line}: {msg.msg}\\n\")\n\n    multi_reporter = MultiReporter([JsonReporter(), TextReporter()], lambda: None)\n\n    linter = PyLinter()\n    linter.reporter = multi_reporter\n    linter.open()\n    linter.add_message(\n        \"line-too-long\", line=1, args=(1, 2)\n    )\n    linter.generate_reports()\n\n    captured = capsys.readouterr()\n    assert captured.out == (\n        \"C0301:1: Line too long (1/2)\\n\"\n        )\n\n    with open(json_file, encoding='utf-8') as f:\n        json_output = json.load(f)\n    assert json_output[0]['msg_id'] == 'C0301'\n    assert json_output[0]['line'] == 1\n", "def test_template_option_with_invalid_msg_id(linter: PyLinter) -> None:\n    output = StringIO()\n    linter.reporter.out = output\n    linter.config.msg_template = \"{path}:{line}:{invalid_msg_id}\"\n    linter.open()\n    linter.set_current_module(\"my_mod\")\n    with pytest.warns(UserWarning) as records:\n        linter.add_message(\"C0301\", line=1, args=(1, 2))\n    assert len(records) == 1\n    assert \"Don't recognize the argument 'invalid_msg_id'\" in records[0].message.args[0]\n    assert output.getvalue() == \"\"\n", "def test_multi_reporter_with_different_output_types(tmp_path):\n    text_output = StringIO()\n    json_output = tmp_path / \"somefile.json\"\n    nop_output = StringIO()\n\n    linter = PyLinter()\n    linter.load_default_plugins()\n    linter.set_option(\"persistent\", False)\n    linter.set_option(\"reports\", True)\n    linter.set_option(\"output-format\", \"text,json,nop\")\n\n    linter.reporter = MultiReporter(\n        [\n            TextReporter(output=text_output),\n            ReporterWriteToJSON(json_output),\n            NopReporter(output=nop_output),\n        ],\n        lambda: None,\n    )\n\n    linter.open()\n    linter.check_single_file_item(FileItem(\"somemodule\", tmp_path / \"somemodule.py\", \"somemodule\"))\n    linter.add_message(\"line-too-long\", line=1, args=(1, 2))\n    linter.generate_reports()\n    linter.reporter.close_output_files()\n\n    assert text_output.getvalue() != \"\"\n    assert json_output.exists()\n\n"], "sample_243": ["    def test_join_promotion_complex(self):\n        query = Query(Author)\n        query.add_join(Item, 'author', 'rel_a', 'OR')\n        query.add_join(ObjectC, 'author', 'rel_b', 'OR')\n        query.add_where(Q(rel_a__num__gt=2) | Q(rel_b__name__icontains='foo'))\n        # For the OR case, we need at least one join to be \"inner\".\n        query.set_extra_mask({'rel_a__num': 1})\n\n        votes = {}\n        for alias, join_type in query.alias_map.items():\n            if alias in ['rel_a', 'rel_b']:\n                votes[alias] = 2\n        join_promoter = JoinPromoter(OR, len(votes), False)\n        join_promoter.add_votes(votes.keys())\n        join_promoter.update_join_types(query)\n", "    def test_join_promotion(self):\n        query = Query(Author)\n        query.add_filter(Q(item__name__icontains='foo') | Q(item__name__icontains='bar'))\n        query.build_where(None)\n\n        results = query.promote_joins(set(['a']))  \n        self.assertEqual(results, set())\n\n        results1 = query.demote_joins(set(['a']))\n        self.assertEqual(results1, set())\n\n\n\n\n", "    def test_related_lookups(self):\n        query = Query(Author)\n        where = query.build_where(Q(item__rank__in=[1, 2, 3]))\n        lookup = where.children[0]\n        self.assertIsInstance(lookup.rhs, Col)\n        self.assertEqual(lookup.lhs.target, Author._meta.get_field('item'))\n        self.assertIsInstance(lookup.lhs.alias, Name)\n        self.assertEqual(lookup.lhs.alias.name, 'item')\n", "    def test_related_lookups_in_where(self):\n        query = Query(Item)\n        where = query.build_where(\n            Q(creator__name__isnull=False) & Q(creator__num__gt=10)\n        )\n        self.assertEqual(where.connector, AND)\n        creator_isnull = where.children[0]\n        self.assertIsInstance(creator_isnull, RelatedIsNull)\n        self.assertTrue(creator_isnull.lhs.target == Author._meta.get_field('name'))\n        creator_num_gt = where.children[1]\n        self.assertIsInstance(creator_num_gt, GreaterThan)\n        self.assertTrue(creator_num_gt.lhs.target == Author._meta.get_field('num'))\n        self.assertEqual(creator_num_gt.rhs, 10)\n\n    \n\n\n", "    def test_join_promotion(self):\n        query = Query(Author)\n        query.add_related_many('items', 'item__name__icontains', 'val')\n        query.promote_joins(set())\n        self.assertEqual(len(query.alias_map), 1)\n        with self.subTest('join_promotion should work for a single table'):\n            query.add_related_many('items', 'item__name__icontains', 'val')\n            self.assertEqual(len(query.alias_map), 2)\n            query.promote_joins(set())\n            query.demote_joins(set())\n            self.assertEqual(len(query.alias_map), 1)\n"], "sample_120": ["    def test_serialize_model_with_many_to_many(self):\n        class Author(models.Model):\n            name = models.CharField(max_length=255)\n\n        class Book(models.Model):\n            title = models.CharField(max_length=255)\n            authors = models.ManyToManyField(Author)\n\n        migration = type(\"Migration\", (migrations.Migration,), {\n            \"operations\": [\n                migrations.CreateModel(\"Book\", (), {}, (Book,)),\n                migrations.CreateModel(\"Author\", (), {}, (Author,)),\n                migrations.AddField(\"Book\", \"authors\", migrations.ManyToManyField(\n                    to=\"Author\",\n                    blank=True,\n                    related_name=\"books\",\n                    through=\"BookAuthor\",\n                    symmetrical=True,\n                )),\n                migrations.CreateModel(\n                    name=\"BookAuthor\",\n                    fields=[\n                        (\"id\", models.AutoField(verbose_name=\"ID\", primary_key=True, serialize=False, auto_created=True)),\n                        (\"created\", models.DateTimeField(auto_now_add=True)),\n                        (\"updated\", models.DateTimeField(auto_now=True)),\n                    ],\n                    options={\n                        \"verbose_name\": \"BookAuthor\",\n                        \"verbose_name_plural\": \"BookAuthors\",\n                    },\n                ),\n            ],\n            \"dependencies\": [],\n        })\n        writer = MigrationWriter(migration)\n        output = writer.as_string()\n        self.assertIn(\"migrations.CreateModel('Book', (), {}, (Book,))\", output) \n        self.assertIn(\"migrations.CreateModel('Author', (), {}, (Author,))\", output)\n        self.assertIn(\"migrations.AddField('Book', 'authors', migrations.ManyToManyField(to='Author', blank=True, related_name='books', through='BookAuthor', symmetrical=True))\", output)\n\n\n\n", "    def test_serialize_nested_objects(self):\n        class NestedObject:\n                self.value = value\n\n        class ModelWithNested:\n                self.name = name\n                self.nested = nested\n\n        nested = NestedObject(\"hello\")\n        model = ModelWithNested(\"some name\", nested)\n        string, imports = MigrationWriter.serialize(model)\n        self.assertEqual(string, \"migrations.test_writer.ModelWithNested(name='some name', nested=migrations.test_writer.NestedObject(value='hello'))\")\n        self.assertEqual(imports, {'from migrations.test_writer import ModelWithNested, NestedObject'})\n\n", "    def test_serialize_field_choices(self):\n        class ChoiceEnum(enum.Enum):\n            OPTION1 = 1\n            OPTION2 = 2\n\n        choices = [\n            (ChoiceEnum.OPTION1.value, 'Option One'),\n            (ChoiceEnum.OPTION2.value, 'Option Two'),\n        ]\n\n        string, imports = MigrationWriter.serialize(models.CharField(\n            max_length=255,\n            choices=choices\n        ))\n        self.assertEqual(string, \"models.CharField(choices=[(1, 'Option One'), (2, 'Option Two')], max_length=255)\")\n        self.assertEqual(imports, {'from enum import Enum'})\n\n\n\n", "    def test_register_serializer_with_imports(self):\n        class SimpleSerializer(BaseSerializer):\n                return 'from_another_module.MyClass(%r)' % self.value, {'from another_module import MyClass'}\n        MigrationWriter.register_serializer(complex, SimpleSerializer)\n        string, imports = MigrationWriter.serialize(complex(1, 2))\n        self.assertEqual(string, 'from_another_module.MyClass(1+2j)')\n        self.assertEqual(imports, {'from another_module import MyClass'})\n        MigrationWriter.unregister_serializer(complex)\n", "    def test_serialize_custom_model_base_class(self):\n        class MyModelBase(models.Model):\n            pass\n\n        class MyModel(MyModelBase):\n            field = models.CharField(max_length=255)\n\n        migration = type(\"Migration\", (migrations.Migration,), {\n            \"operations\": [\n                migrations.CreateModel(\"MyModel\", bases=(MyModelBase,), fields=[(\"field\", models.CharField(max_length=255))],\n            ],\n        })\n        writer = MigrationWriter(migration)\n        output = writer.as_string()\n        self.assertIn(\"MyModelBase\", output)\n"], "sample_940": ["        def __init__(self, a, b, c):\n            self.a = a\n            self.b = b\n            self.c = c\n", "        def __init__(self, arg):\n            self.arg = arg\n", "        def __init__(self, value):\n            self.value = value\n", "def test_is_builtin_class_method_with_custom_class():\n    class MyClass:\n      @classmethod\n        return \"This is a custom class method.\"\n\n    assert inspect.is_builtin_class_method(MyClass, 'my_method') is False\n", "def test_is_builtin_class_method_with_args():\n    class MyInt(int):\n            pass\n\n    assert inspect.is_builtin_class_method(MyInt, 'to_bytes', arg1=None, arg2=b'')\n    assert inspect.is_builtin_class_method(MyInt, '__init__', arg1=1, arg2=b'')\n    assert not inspect.is_builtin_class_method(MyInt, 'my_method', arg1=None, arg2=b'')\n    assert not inspect.is_builtin_class_method(MyInt, 'does_not_exist', arg1=None, arg2=b'')\n"], "sample_536": ["def test_MultiCursor_reset(horizOn, vertOn):\n    (ax1, ax3) = plt.figure().subplots(2, sharex=True)\n    ax2 = plt.figure().subplots()\n    multi = widgets.MultiCursor(\n        None, (ax1, ax2), useblit=False, horizOn=horizOn, vertOn=vertOn\n    )\n\n    # Initially, the cursor should not be visible\n    for line in multi.vlines:\n        assert not line.get_visible()\n    for line in multi.hlines:\n        assert not line.get_visible()\n\n    # Simulate a move event to make the cursor visible\n    event = mock_event(ax1, xdata=.5, ydata=.25)\n    multi.onmove(event)\n    for line in multi.vlines:\n        assert line.get_visible()\n    for line in multi.hlines:\n        assert line.get_visible()\n\n    # Reset the cursor\n    multi.reset()\n\n    for line in multi.vlines:\n        assert not line.get_visible()\n    for line in multi.hlines:\n        assert not line.get_visible()\n", "def test_MultiCursor_selection(horizOn, vertOn):\n    (ax1, ax3) = plt.figure().subplots(2, sharex=True)\n    ax2 = plt.figure().subplots()\n\n    # useblit=false to avoid having to draw the figure to cache the renderer\n    multi = widgets.MultiCursor(\n        None, (ax1, ax2), useblit=False, horizOn=horizOn, vertOn=vertOn\n    )\n\n    # mock a button press event\n    event = mock_event(ax1, xdata=.5, ydata=.25)\n    multi.onpress(event)\n\n    # check that lines are selected\n    for l in multi.vlines:\n        assert l.get_color() != \"k\"\n    for l in multi.hlines:\n        assert l.get_color() != \"k\"\n\n\n", "def test_MultiCursor_invalid_axes(horizOn, vertOn):\n    (ax1, ax3) = plt.figure().subplots(2, sharex=True)\n    ax2 = plt.figure().subplots()\n    # Create a MultiCursor instance with invalid axes\n    with pytest.raises(TypeError):\n        multi = widgets.MultiCursor(None, (ax1, ax2, ax3),\n                                   horizOn=horizOn, vertOn=vertOn)\n", "def test_polygon_selector_reset(ax, draw_bounding_box):\n    verts = [(50, 50), (150, 50), (50, 150)]\n    event_sequence = [\n        *polygon_place_vertex(*verts[0]),\n        *polygon_place_vertex(*verts[1]),\n        *polygon_place_vertex(*verts[2]),\n        *polygon_place_vertex(*verts[0]),\n    ]\n    tool = widgets.PolygonSelector(ax, onselect=noop,\n                                   draw_bounding_box=draw_bounding_box)\n    for (etype, event_args) in event_sequence:\n        do_event(tool, etype, **event_args)\n\n    tool.reset()\n\n    assert tool.verts == []\n", "def test_multi_cursor_clear():\n    (ax1, ax3) = plt.figure().subplots(2, sharex=True)\n    ax2 = plt.figure().subplots()\n\n    multi = widgets.MultiCursor(\n        None, (ax1, ax2), useblit=False\n    )\n\n    #  mock a motion_notify_event\n    event = mock_event(ax1, xdata=.5, ydata=.25)\n    multi.onmove(event)\n\n    #  clear the MultiCursor\n    multi.clear()\n\n    # Check that all lines are gone\n    assert len(multi.vlines) == 0\n    assert len(multi.hlines) == 0\n"], "sample_523": ["def test_legend_handles_from_artist():\n    fig, ax = plt.subplots()\n    ax.plot([0, 1], [0, 1], label='Line')\n    ax.scatter([0.5], [0.5], label='Scatter')\n    handles = [ax.lines[0], ax.scatter[0]]\n    legend = ax.legend()\n\n    assert len(legend.legendHandles) == len(handles)\n    assert all(h.get_label() == h.get_label() for h in legend.legendHandles)\n\n\n\n", "def test_legend_handles_are_ordered():\n    fig, axes = plt.subplots(2, 2)\n    for ax in axes.flat:\n        ax.plot(range(10), label=f'line {len(axes.flat)}')\n\n    # Get a list of legends for all axes\n    legends = [ax.legend() for ax in axes.flat]\n\n    # Check that the legend handles are ordered correctly\n    for i, leg in enumerate(legends):\n        for j, h in enumerate(leg.legendHandles):\n            assert h.get_label() == f'line {i+1}'\n", "def test_legend_fontsize():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3], [4, 5, 6], label='foo')\n    ax.legend(fontsize=12)\n    \n    text = ax.legend().get_texts()[0]\n    assert text.get_fontsize() == 12\n\n\n\n", "def test_legend_handles_with_artist_type(fig_test):\n    # Test handling of artists other than Line2D in legend\n    ax = fig_test.add_subplot(111)\n    ax.plot([0, 1], [0, 1], label=\"line\")\n    ax.scatter([0, 1], [0, 1], label=\"scatter\")\n    ax.text(0.5, 0.5, \"text\", label=\"text\")\n    legend = ax.legend()\n    assert isinstance(legend.legendHandles[0], mlines.Line2D)\n    assert isinstance(legend.legendHandles[1], mcollections.MarkerCollection)\n    assert isinstance(legend.legendHandles[2], mpl.texts.Text)\n\n\n\n", "def test_legend_loc_bbox_inches():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3], [4, 5, 6])\n    ax.set_xlim(0, 4)\n    ax.set_ylim(0, 7)\n\n    leg = ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left',\n                    bbox_inches='tight')\n\n    assert leg.get_window_extent(fig.canvas.get_renderer()).xmax > 1\n    assert leg.get_window_extent(fig.canvas.get_renderer()).ymax < 1\n\n\n"], "sample_561": ["compilation error", "def test_marker_size():\n    marker = markers.MarkerStyle(\"*\")\n    for size in [0.1, 1, 10, 100]:\n        m = marker.set_size(size)\n        assert isinstance(m, markers.MarkerStyle)\n        assert m.get_size() == size\n\n", "def test_marker_scaled_invalid():\n    marker = markers.MarkerStyle(\"o\")\n    with pytest.raises(TypeError):\n        marker.scaled(\"2\")\n    with pytest.raises(ValueError):\n        marker.scaled(2, \"3\")\n", "def test_marker_transformed_invalid():\n    marker = markers.MarkerStyle(\"o\")\n    with pytest.raises(ValueError):\n        marker.transformed(transform=\"invalid\")\n    with pytest.raises(ValueError):\n        marker.transformed(transform=1)\n\n", "def test_marker_init_linewidth():\n    marker = markers.MarkerStyle(\"*\")\n    styled_marker = markers.MarkerStyle(\"*\", linewidth=2)\n    assert styled_marker.get_linewidth() == 2\n    assert marker.get_linewidth() != 2\n\n"], "sample_911": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_1165": ["compilation error", "def test_quaternion_complex_field_arithmetic():\n    q1 = Quaternion(1, 2, 3, 4, real_field = False)\n    q2 = Quaternion(5, 6, 7, 8, real_field = False)\n\n    assert q1 + q2 == Quaternion(6, 8, 10, 12, real_field = False)\n    assert q1 - q2 == Quaternion(-4, -4, -4, -4, real_field = False)\n    assert q1 * q2 == Quaternion( -11 - 28*I, -33 - 4*I, -5 - 16*I, -11 + 16*I, real_field = False)\n    assert q1 / q2 == Quaternion( 0.16216216216216216, -0.20425531914893617, -0.19148936170212767, 0.3096774193548387, real_field = False)\n\n\n\n", "compilation error", "def test_quaternion_order_of_operations():\n    q1 = Quaternion(1, 2, 3, 4)\n    q2 = Quaternion(5, 6, 7, 8)\n    \n    # Test multiplication order\n    assert (q1 * q2) != (q2 * q1)\n\n    # Test multiplication with scalar\n    assert (2 * q1) == q1 * 2\n    assert (q1 * 2) == 2 * q1\n\n    # Test addition and subtraction with scalars\n    assert (q1 + 1) == Quaternion(2, 2, 3, 4)\n    assert (q1 - 1) == Quaternion(0, 2, 3, 4)\n\n    \n", "compilation error"], "sample_478": ["    def test_actions_not_callable(self):\n        class BandAdmin(ModelAdmin):\n            actions = (\"not_callable\",)\n\n        self.assertIsInvalid(\n            BandAdmin,\n            Band,\n            \"The value of 'actions[0]' must be a callable.\",\n            id=\"admin.E131\",\n        )\n", "    def test_actions_missing_permissions(self):\n        @admin.action\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = (action,)\n\n        self.assertIsInvalid(\n            BandAdmin,\n            Band,\n            \"Action 'action' does not have defined permissions.\",\n            id=\"admin.E128\",\n        )\n", "    def test_actions_with_missing_permissions(self):\n        @admin.action(permissions=[\"custom1\", \"custom2\"])\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = (action,)\n\n        self.assertIsInvalid(\n            BandAdmin,\n            Band,\n            \"BandAdmin must define a has_custom1() and a has_custom2()\"\n            \" method for the action action.\",\n            id=\"admin.E129\",\n        )\n", "    def test_actions_with_invalid_permissions(self):\n        @admin.action(permissions=[\"does_not_exist\"])\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = (invalid_permission_action,)\n\n        self.assertIsInvalid(\n            BandAdmin,\n            Band,\n            \"Could not find permission 'does_not_exist'.\",\n            id=\"admin.E128\",\n        )\n", "    def test_actions_with_custom_permissions(self):\n        @admin.action(permissions=[\"custom_permission\"])\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = [custom_action]\n            has_custom_permission = lambda obj: obj.is_admin\n\n        self.assertIsValid(BandAdmin, Band)\n\n\n"], "sample_896": ["def test_nmf_sparse_data():\n    rng = np.random.RandomState(42)\n    n_samples = 6\n    n_features = 10\n    n_components = 3\n    sparse_data = sp.csr_matrix((rng.rand(n_samples, n_features),\n                                (np.arange(n_samples), np.arange(n_features))),\n                               shape=(n_samples, n_features))\n\n    nmf = NMF(\n        n_components=n_components,\n        init=\"random\",\n        random_state=0,\n        tol=1e-6,\n    )\n    nmf.fit(sparse_data)\n\n", "def test_nmf_n_components_auto_sparse_matrix():\n    # Check that n_components is correctly inferred from sparse matrices\n    # in custom initialization.\n    rng = np.random.RandomState(0)\n    X = sp.csr_matrix(rng.random_sample((6, 5)))\n    W_init = rng.random_sample((6, 2))\n    H_init = rng.random_sample((2, 5))\n    W, H, _ = non_negative_factorization(\n        X, H=H_init, n_components=\"auto\", update_H=False\n    )\n    assert_allclose(H.toarray(), H_init)\n    assert W.shape == (X.shape[0], H_init.shape[0])\n\n\n\n", "def test_nmf_sparse_data():\n    # Test the behavior of NMF with sparse input data\n    n_samples = 100\n    n_features = 500\n    n_components = 10\n    sparsity = 0.95\n\n    # Create a sparse matrix with specified sparsity\n    rng = np.random.RandomState(42)\n    X = sp.rand(n_samples, n_features, density=sparsity)\n\n    # Test with both NMF and MiniBatchNMF\n    estimators = [NMF, MiniBatchNMF]\n    for Estimator in estimators:\n        nmf = Estimator(n_components=n_components, solver=\"mu\", random_state=0)\n        W = nmf.fit_transform(X)\n        H = nmf.components_\n\n        assert W.shape == (n_samples, n_components)\n        assert H.shape == (n_components, n_features)\n\n        # Check that the output is also sparse\n        assert isinstance(W, sp.csr_matrix)\n        assert isinstance(H, sp.csr_matrix)\n\n\n\n", "def test_nmf_sparse_data():\n    # Test NMF with sparse input data\n    rng = np.random.mtrand.RandomState(42)\n    n_samples = 20\n    n_features = 10\n    n_components = 5\n    X_dense = rng.rand(n_samples, n_features)\n    X_sparse = sp.csr_matrix(X_dense)\n\n    nmf_dense = NMF(n_components=n_components).fit(X_dense)\n    nmf_sparse = NMF(n_components=n_components).fit(X_sparse)\n\n    assert_allclose(nmf_dense.components_, nmf_sparse.components_)\n", "def test_nmf_sparse_data():\n    # Test handling of sparse data with MiniBatchNMF\n    n_samples = 100\n    n_features = 500\n    n_components = 10\n    \u7a00\u758f\u6bd4\u4f8b = 0.95  # 95% of the values are set to zero\n    rng = np.random.RandomState(42)\n    X = sp.csr_matrix((rng.rand(n_samples, n_features),\n                       (rng.randint(n_samples, size=n_samples),\n                        rng.randint(n_features, size=n_samples))))\n\n    mbnmf = MiniBatchNMF(n_components=n_components, random_state=0,\n                        batch_size=10, tol=1e-2)\n    mbnmf.fit(X)\n    assert mbnmf.components_.shape == (n_components, n_features)\n\n    # Check that the fit_transform results are similar\n    # to dense matrix for comparison purposes\n    X_dense = X.toarray()\n    W_dense = mbnmf.fit_transform(X_dense)\n    W_sparse = mbnmf.components_\n    assert_allclose(W_sparse, W_dense, rtol=1e-3)\n\n"], "sample_776": ["def test_lasso_lars_sparse_data():\n    from sklearn.datasets import make_sparse_coded_regression\n    X, y = make_sparse_coded_regression(\n        n_samples=100, n_features=1000, sparse_ratio=0.95\n    )\n    lars = LassoLars().fit(X, y)\n    assert lars.coef_.nnz == 0\n    \n    lars = LassoLars(alpha=0.1).fit(X, y)\n    assert lars.coef_.nnz > 0\n\n\n\n", "def test_lars_multi_output():\n    # Test LassoLars with multiple outputs.\n    # We can create a dataset with multiple correlated outputs.\n\n    np.random.seed(0)\n    n_samples = 100\n    n_features = 5\n    n_targets = 3\n    X = np.random.rand(n_samples, n_features)\n    y = np.zeros((n_samples, n_targets))\n\n    for i in range(n_targets):\n        y[:, i] = np.sin(2 * np.pi * X[:, i % n_features]) + np.random.randn(n_samples) * 0.2\n\n    lars = linear_model.LassoLars(fit_intercept=False, normalize=False)\n    lars.fit(X, y)\n    assert lars.coef_.shape == (n_targets, n_features)\n\n    # Check that the results for each target are reasonable\n    for i in range(n_targets):\n        assert np.allclose(lars.coef_[i], np.unwrap(np.polyfit(X[:, i % n_features],\n                                                           y[:, i], 1)))\n\n\n\n", "def test_lasso_lars_sparse_data():\n    \"\"\"Test LassoLars on sparse input data\"\"\"\n    from scipy.sparse import csr_matrix\n    n_samples = 100\n    n_features = 1000\n    # Create a sparse dataset\n    X = csr_matrix((np.random.rand(n_samples, n_features),\n                    (np.random.randint(n_features, size=n_samples),\n                     np.random.randint(n_samples, size=n_samples))))\n    y = np.random.rand(n_samples)\n\n    # Call LassoLars with the sparse data\n    lars = linear_model.LassoLars(alpha=0.1)\n    lars.fit(X, y)\n    assert lars.coef_.shape == (n_features,)\n\n\n\n", "def test_lasso_lars_precompute_is_True():\n    # Test that precompute=True speeds up the fitting process\n    # compared to precompute=False, when fit_intercept is True.\n    rng = np.random.RandomState(42)\n    n_samples = 1000\n    n_features = 50\n    X = rng.randn(n_samples, n_features)\n    y = rng.randn(n_samples)\n\n    # Create lasso models with and without precompute\n    model_precompute = linear_model.LassoLars(fit_intercept=True,\n                                              precompute=True)\n    model_no_precompute = linear_model.LassoLars(fit_intercept=True,\n                                                precompute=False)\n\n    # Time the fitting process for both models\n    import time\n    t1 = time.time()\n    model_precompute.fit(X, y)\n    t2 = time.time()\n\n    t3 = time.time()\n    model_no_precompute.fit(X, y)\n    t4 = time.time()\n\n    # Assert that precompute speeds up fitting\n    assert t2 - t1 < t3 - t4\n\n\n", "def test_lasso_lars_sparse_data():\n    # Test LassoLars with sparse input data\n    from scipy.sparse import random_spmatrix\n    n_samples = 100\n    n_features = 1000\n    X = random_spmatrix(n_samples, n_features, density=0.1, random_state=42)\n    y = np.random.rand(n_samples)\n\n    lars = linear_model.LassoLars()\n    lars.fit(X, y)\n\n\n"], "sample_484": ["    def test_negative_length(self):\n        with self.assertRaisesMessage(ValueError, \"'length' must be greater than 0\"):\n            Author.objects.annotate(raises=Right(\"name\", -1))\n", "    def test_negative_length(self):\n        with self.assertRaisesMessage(ValueError, \"'length' must be greater than 0\"):\n            Author.objects.annotate(raises=Right(\"name\", -1)) \n", "    def test_negative_length(self):\n        with self.assertRaisesMessage(\n            ValueError, \"'length' must be greater than 0\"\n        ):\n            Author.objects.annotate(raises=Right(\"name\", -1))\n", "    def test_negative_length(self):\n        with self.assertRaisesMessage(ValueError, \"'length' must be greater than 0\"):\n            Author.objects.annotate(raises=Right(\"name\", -1))\n", "    def test_missing_alias(self):\n        authors = Author.objects.annotate(name_part=Right(\"name\"))\n        self.assertQuerySetEqual(\n            authors.order_by(\"name\"), [\"John Smith\", \"Rhonda\"], lambda a: a.name_part\n        )\n\n\n\n"], "sample_187": ["    def test_camel_case_to_spaces(self):\n        items = [\n            ('ThisIsCamelCase', 'this is camel case'),\n            ('thisIsCamelCase', 'this is camel case'),\n            ('mixedCase', 'mixed case'),\n            ('snake_case', 'snake case'),\n            ('camelCaseWithNumbers', 'camel case with numbers'),\n            ('PascalCase', 'pascal case'),\n        ]\n        for value, output in items:\n            self.assertEqual(text.camel_case_to_spaces(value), output)\n            self.assertEqual(text.camel_case_to_spaces(lazystr(value)), output) \n", "    def test_camel_case_to_spaces(self):\n        testdata = [\n            ('theQuickBrownFox', 'the Quick Brown Fox'),\n            ('thisIsATestString', 'this Is A Test String'),\n            ('camelCase', 'camel Case'),\n            ('snake_case', 'snake case'),\n            ('fooBar', 'foo Bar'),\n            ('aBcDeF', 'a B c D e F'),\n            ('a1b2c3', 'a1 b2 c3'),\n            ('', ''),\n            ('_', '_'),\n        ]\n        for test, expected in testdata:\n            self.assertEqual(text.camel_case_to_spaces(test), expected)\n\n\n", "    def test_camel_case_to_spaces(self):\n        self.assertEqual(text.camel_case_to_spaces('thisIsACamelCaseString'), 'this is a camel case string')\n        self.assertEqual(text.camel_case_to_spaces('ThisIsAnotherCamelCaseString'), 'This is another camel case string')\n        self.assertEqual(text.camel_case_to_spaces('snake_case_string'), 'snake case string')\n        self.assertEqual(text.camel_case_to_spaces('mixedCamelCaseWithUnderscores'), 'mixed camel case with underscores')\n\n\n", "    def test_camel_case_to_spaces(self):\n        self.assertEqual(text.camelCase_to_spaces('myVariableName'), 'my variable name')\n        self.assertEqual(text.camelCase_to_spaces('MYVARIABLE'), 'MY VARIABLE')\n        self.assertEqual(text.camelCase_to_spaces('myVariable123'), 'my variable123')\n        self.assertEqual(text.camelCase_to_spaces('my_variable_name'), 'my_variable_name')\n        self.assertEqual(text.camelCase_to_spaces(''), '')\n\n\n\n", "    def test_camel_case_to_spaces(self):\n        self.assertEqual(text.camel_case_to_spaces('HelloWorld'), 'Hello World')\n        self.assertEqual(text.camel_case_to_spaces('ThisIsATest'), 'This is a test')\n        self.assertEqual(text.camel_case_to_spaces('AllWords'), 'All Words')\n        self.assertEqual(text.camel_case_to_spaces('aBcDeF'), 'a B c D e F')\n        self.assertEqual(text.camel_case_to_spaces('MixedCaseWithNums123'), 'Mixed Case With Nums123')\n"], "sample_367": ["    def test_cache_control_with_private(self):\n        @cache_control(private=True)\n            return HttpResponse()\n\n        response = a_view(HttpRequest())\n        self.assertEqual(response.headers['Cache-Control'], 'private')\n", "    def test_cache_control_decorator_multiple_arguments(self):\n        class MyClass:\n            @method_decorator(cache_control(public=True, max_age=100))\n                return HttpResponse()\n\n        request = HttpRequest()\n        response = MyClass().a_view(HttpRequestProxy(request))\n        self.assertEqual(response.headers['Cache-Control'], 'public, max-age=100')\n", "    def test_cache_control_decorator_multiple_args(self):\n        class MyClass:\n            @method_decorator(cache_control(a='b', c='d'))\n                return HttpResponse()\n\n        request = HttpRequest()\n        response = MyClass().a_view(HttpRequestProxy(request))\n        self.assertEqual(response.headers['Cache-Control'], 'a=b, c=d')\n", "    def test_cache_control_override_existing_headers(self):\n        @cache_control(private=True, max_age=60)\n            return HttpResponse(headers={'Cache-Control': 'no-cache'})\n\n        response = a_view(HttpRequest())\n        self.assertEqual(response.headers['Cache-Control'], 'private, max-age=60')\n", "    def test_cache_control_decorator_multiple_args(self):\n        class MyClass:\n            @method_decorator(cache_control(private=True, max_age=3600))\n                return HttpResponse()\n\n        request = HttpRequest()\n        response = MyClass().a_view(HttpRequestProxy(request))\n        self.assertEqual(response.headers['Cache-Control'], 'private, max-age=3600')\n\n\n"], "sample_539": ["def test_MultiCursor_handles_multiple_axes_types(horizOn, vertOn):\n    # Use axes with different types (lines vs. image) to ensure consistent behavior\n    fig = plt.figure()\n    ax1 = fig.add_subplot(121)\n    ax2 = fig.add_subplot(122)\n    ax2.imshow(np.random.rand(10, 10))\n    multi = widgets.MultiCursor(\n        None, (ax1, ax2), useblit=False, horizOn=horizOn, vertOn=vertOn\n    )\n\n    # Test cursor behavior on both axes types\n    event = mock_event(ax1, xdata=.5, ydata=.25)\n    multi.onmove(event)\n    assert len(multi.vlines) == 2 \n    assert len(multi.hlines) == 2\n\n    for l in multi.hlines:\n        assert l.get_ydata() == (.25, .25)\n    for l in multi.vlines:\n        assert l.get_xdata() == (.5, .5)  \n\n    event = mock_event(ax2, xdata=.75, ydata=.75)\n    multi.onmove(event)\n    for l in multi.hlines:\n        assert l.get_ydata() == (.25, .25)\n    for l in multi.vlines:\n        assert l.get_xdata() == (.5, .5)  \n\n", "def test_polygon_selector_draw_bounding_box(draw_bounding_box):\n    verts = [(50, 50), (150, 50), (50, 150)]\n    event_sequence = [\n        *polygon_place_vertex(*verts[0]),\n        *polygon_place_vertex(*verts[1]),\n        *polygon_place_vertex(*verts[2]),\n        *polygon_place_vertex(*verts[0]),\n    ]\n\n    ax = plt.subplots().gca()\n    tool = widgets.PolygonSelector(ax, onselect=noop, draw_bounding_box=draw_bounding_box)\n    for (etype, event_args) in event_sequence:\n        do_event(tool, etype, **event_args)\n\n    if draw_bounding_box:\n        bbox = tool._bbox\n        assert bbox.get_visible() is True\n        plt.draw()\n    else:\n        assert hasattr(tool, '_bbox')\n        assert tool._bbox is None\n", "def test_polygon_selector_oncomplete(ax, draw_bounding_box):\n    verts = [(50, 50), (150, 50), (50, 150)]\n    event_sequence = [\n        *polygon_place_vertex(*verts[0]),\n        *polygon_place_vertex(*verts[1]),\n        *polygon_place_vertex(*verts[2]),\n        *polygon_place_vertex(*verts[0]),\n    ]\n\n    tool = widgets.PolygonSelector(\n        ax, onselect=noop, draw_bounding_box=draw_bounding_box)\n\n    # mock the onselect event\n    onselect_called = False\n    tool.onselect = lambda verts, : onselect_called = True\n\n    for (etype, event_args) in event_sequence:\n        do_event(tool, etype, **event_args)\n\n    assert onselect_called\n", "def test_selector_props_changed(ax):\n    verts = [(50, 50), (150, 50), (50, 150)]\n\n    event_sequence = [\n        *polygon_place_vertex(*verts[0]),\n        *polygon_place_vertex(*verts[1]),\n        *polygon_place_vertex(*verts[2]),\n        *polygon_place_vertex(*verts[0]),\n    ]\n\n    tool = widgets.PolygonSelector(ax, onselect=noop, color='blue', alpha=0.5)\n\n    for (etype, event_args) in event_sequence:\n        do_event(tool, etype, **event_args)\n\n    assert tool._selection_artist.get_color() == 'blue'\n    assert tool._selection_artist.get_alpha() == 0.5\n\n    tool.set_props(color='red', alpha=0.8)\n\n    assert tool._selection_artist.get_color() == 'red'\n    assert tool._selection_artist.get_alpha() == 0.8\n\n\n\n", "def test_polygon_selector_on_key_press(ax):\n    verts = [(50, 50), (150, 50), (50, 150)]\n    event_sequence = [\n        *polygon_place_vertex(*verts[0]),\n        *polygon_place_vertex(*verts[1]),\n        *polygon_place_vertex(*verts[2]),\n        *polygon_place_vertex(*verts[0]),\n    ]\n\n    tool = widgets.PolygonSelector(ax, onselect=noop, draw_bounding_box=draw_bounding_box)\n    for (etype, event_args) in event_sequence:\n        do_event(tool, etype, **event_args)\n\n    # Trigger key press event\n    key_press_event = {'key': 'a'}\n    tool.on_key_press(key_press_event)\n\n    # Assert that verts attribute is not changed\n    assert tool.verts == verts\n\n\n\n"], "sample_81": ["    def test_resolver_cache_route_pattern_regex_compile(self):\n        # Test RegexPattern.regex is compiled only once per language\n        resolver = get_resolver()\n        resolver._populate()\n        route_pattern = resolver.url_patterns[0].pattern\n        self.assertEqual(route_pattern.regex.pattern,\n                         route_pattern._regex_dict[setting.LANGUAGE_CODE].pattern)\n\n", "    def test_resolver_cache_namespace(self):\n        resolver1 = get_ns_resolver('^app1/(?P<id>\\w+)/$', get_resolver(), converters={'id': lambda v: int(v)})\n        resolver2 = get_ns_resolver('^app1/(?P<id>\\w+)/$', get_resolver(), converters={'id': lambda v: int(v)})\n        self.assertIs(resolver1, resolver2)\n", "    def test_check_warnings(self):\n        # Check for warnings in URL patterns using URLResolver's check method\n        resolver = get_resolver('urlpatterns.path_urls')\n        warnings = resolver.check()\n        self.assertEqual(len(warnings), 2)  # Expect two warnings for route names\n\n\n\n", "    def test_resolver_cache_ns_resolver(self):\n        # resolver for a default URLconf (passing no argument) and for the\n        # settings.ROOT_URLCONF is the same cached object.\n        resolver = get_resolver()\n        ns_resolver = get_ns_resolver('^my_namespace/', resolver, {})\n        self.assertIsNot(resolver, ns_resolver)\n", "    def test_resolver_cache_route(self):\n        # test resolver cache with a specific route pattern.\n        resolver1 = get_resolver()\n        resolver2 = get_resolver('urlpatterns.path_urls')\n        self.assertIs(resolver1, resolver2)\n\n\n\n"], "sample_281": ["    def test_related_queryset(self):\n        class RelatedQuestionAdmin(QuestionAdmin):\n            search_fields = ['related_questions__question']\n\n        with model_admin(Question, RelatedQuestionAdmin):\n            q1 = Question.objects.create(question='Question 1')\n            q2 = Question.objects.create(question='Question 2')\n            q1.related_questions.add(q2)\n            request = self.factory.get(self.url, {'term': 'question', **self.opts})\n            request.user = self.superuser\n            response = AutocompleteJsonView.as_view(**self.as_view_args)(request)\n            self.assertEqual(response.status_code, 200)\n            data = json.loads(response.content.decode('utf-8'))\n            self.assertEqual(len(data['results']), 2)\n", "    def test_exclude_related_objects(self):\n        \"\"\"\n        Users with 'can_view' permission on a related object should\n        not see it in autocomplete results.\n        \"\"\"\n        related_user = User.objects.create_user(\n            username='related_user', password='secret',\n            email='related_user@example.com', is_staff=True,\n        )\n        related_user.user_permissions.add(\n            Permission.objects.get(\n                content_type=ContentType.objects.get_for_model(Author),\n                codename='view_author',\n            ),\n        )\n        q = Question.objects.create(question='Who am I?')\n        a1 = Answer.objects.create(question=q)\n        a2 = Answer.objects.create(question=q)\n        a1.author = related_user\n        a1.save()\n        self.selenium.get(self.live_server_url + reverse('autocomplete_admin:admin_views_question_edit', args=[q.pk]))\n        elem = self.selenium.find_element_by_css_selector('.select2-selection')\n        elem.click()\n        results = self.selenium.find_element_by_css_selector('.select2-results')\n        self.assertTrue(results.is_displayed())\n        authorship_options = results.find_elements_by_css_selector('.select2-results__option')\n        self.assertEqual(len(authorship_options), 2)\n        self.selenium.find_element_by_css_selector('.select2-search__field').send_keys('related_user')\n        with self.select2_ajax_wait():\n            authorship_options = results.find_elements_by_css_selector('.select2-results__option')\n        self.assertEqual(len(authorship_options), 1)\n        self.assertEqual(authorship_options[0].text, related_user.username)\n\n\n\n", "    def test_no_results(self):\n        Question.objects.all().delete()\n        request = self.factory.get(self.url, {'term': 'nonexistent', **self.opts})\n        request.user = self.superuser\n        with model_admin(Question, QuestionAdmin):\n            response = AutocompleteJsonView.as_view(**self.as_view_args)(request)\n        self.assertEqual(response.status_code, 200)\n        data = json.loads(response.content.decode('utf-8'))\n        self.assertEqual(data, {'results': [], 'pagination': {'more': False}}) \n", "    def test_complex_filter(self):\n        class ComplexFilterAdmin(QuestionAdmin):\n            search_fields = ['question__contains']\n\n        with model_admin(Question, ComplexFilterAdmin):\n            q1 = Question.objects.create(question='This is a question.')\n            q2 = Question.objects.create(question='This is another question.')\n        request = self.factory.get(self.url, {'term': 'this', **self.opts})\n        request.user = self.superuser\n        response = AutocompleteJsonView.as_view(**self.as_view_args)(request)\n        self.assertEqual(response.status_code, 200)\n        data = json.loads(response.content.decode('utf-8'))\n        self.assertEqual(data, {\n            'results': [{'id': str(q1.pk), 'text': q1.question}, {'id': str(q2.pk), 'text': q2.question}],\n            'pagination': {'more': False},\n        })\n\n", "    def test_autocomplete_with_ordering_and_search(self):\n        class PKOrderingAuthorAdmin(AuthorAdmin):\n            ordering = ['name']\n\n        with model_admin(Author, PKOrderingAuthorAdmin):\n            authors = Author.objects.bulk_create([\n                Author(name='Bartholomew'),\n                Author(name='Charles'),\n                Author(name='David'),\n            ])\n            self.selenium.get(self.live_server_url + reverse('autocomplete_admin:admin_views_answer_add'))\n            elem = self.selenium.find_element_by_css_selector('.select2-selection')\n            elem.click()  # Open the autocomplete dropdown.\n            results = self.selenium.find_element_by_css_selector('.select2-results')\n            self.assertTrue(results.is_displayed())\n            search = self.selenium.find_element_by_css_selector('.select2-search__field')\n            search.send_keys('Bar')\n            with self.select2_ajax_wait():\n                results = results.find_elements_by_css_selector('.select2-results__option')\n                self.assertEqual(len(results), 1)\n                self.assertEqual(results[0].text, 'Bartholomew')\n            search.clear()\n            search.send_keys('Dav')\n            with self.select2_ajax_wait():\n                results = results.find_elements_by_css_selector('.select2-results__option')\n                self.assertEqual(len(results), 1)\n                self.assertEqual(results[0].text, 'David')\n\n\n"], "sample_234": ["    def test_related_lookups_on_combined_queryset(self):\n        Reservation.objects.create(\n            number_id=1,\n            name='Reservation 1',\n        )\n        Reservation.objects.create(\n            number_id=2,\n            name='Reservation 2',\n        )\n        qs1 = Number.objects.filter(num=1).select_related('reservations')\n        qs2 = Number.objects.filter(num=2).select_related('reservations')\n        combined_qs = qs1.union(qs2)\n        # Make sure the related lookups are properly populated\n        for obj in combined_qs:\n            self.assertEqual(len(obj.reservations.all()), 1)\n", "    def test_values_with_union(self):\n        ReservedName.objects.create(name='a', order=2)\n        qs1 = ReservedName.objects.all().values('name', 'order')\n        qs2 = ReservedName.objects.all().values('name').values('order')\n        self.assertEqual(list(qs1.union(qs2)), [OrderedDict([('name', 'a'), ('order', 2)]), OrderedDict([('name', 'a')])])\n\n\n", "    def test_union_after_exists(self):\n        ReservedName.objects.create(name='a', order=2)\n        qs1 = Number.objects.filter(num=1).annotate(\n            has_reserved_name=Exists(ReservedName.objects.filter(order=OuterRef('num'))),\n        )\n        qs2 = Number.objects.filter(num=2)\n        intersection = qs1.union(qs2)\n        self.assertEqual(list(intersection.values_list('num', flat=True)), [1, 2])\n\n", "    def test_distinct_on_combined_qs(self):\n        qs1 = Number.objects.filter(num=1)\n        qs2 = Number.objects.filter(num=1)\n        self.assertEqual(len(list(qs1.union(qs2).distinct())), 1)\n        qs1 = Number.objects.filter(num=1)\n        qs2 = Number.objects.filter(num=2)\n        self.assertEqual(len(list(qs1.union(qs2).distinct())), 2)\n        qs1 = Number.objects.filter(num=1).values('num')\n        qs2 = Number.objects.filter(num=1).values('num')\n        self.assertEqual(len(list(qs1.union(qs2).distinct())), 1)\n        qs1 = Number.objects.filter(num=1).values('num')\n        qs2 = Number.objects.filter(num=2).values('num')\n        self.assertEqual(len(list(qs1.union(qs2).distinct())), 2)\n\n", "    def test_clear_cache_on_combined_qs(self):\n        # Simulate caching\n        Number._meta.get_field('num').cache_value = lambda obj: obj.num\n\n        qs1 = Number.objects.filter(num__lt=5)\n        qs2 = Number.objects.filter(num__gte=5)\n        # Clear cache directly on the combined qs\n        (qs1.union(qs2))._state.clear_cache()\n        for num in range(10):\n            self.assertIsNone(Number._meta.get_field('num').cache_value(Number.objects.get(num=num)))\n\n        qs1 = Number.objects.filter(num__lt=5)\n        qs2 = Number.objects.filter(num__gte=5)\n        # Clear cache on the individual qs before union\n        qs1._state.clear_cache()\n        qs2._state.clear_cache()\n        (qs1.union(qs2))._state.clear_cache()\n        for num in range(10):\n            self.assertIsNone(Number._meta.get_field('num').cache_value(Number.objects.get(num=num)))\n\n\n\n"], "sample_190": ["    def test_isnull_null_value(self):\n        \"\"\"\n        Check that a null value works as expected with isnull=True\n        \"\"\"\n        season = Season.objects.create(year=2012, gt=None)\n        self.assertTrue(Season.objects.filter(gt=None).exists())\n        self.assertFalse(Season.objects.filter(gt__isnull=False).exists())\n", "    def test_exact_with_datetime(self):\n        now = datetime.now()\n        article = Article.objects.create(pub_date=now)\n\n        self.assertTrue(Article.objects.filter(pub_date__exact=now).exists())\n\n        # Test with time components set to 0\n        now_midnight = now.replace(hour=0, minute=0, second=0, microsecond=0)\n        self.assertTrue(Article.objects.filter(pub_date__exact=now_midnight).exists())\n\n        # Test with time components not set\n        now_midnight = now.replace(hour=0, minute=0, second=0, microsecond=0)\n        self.assertFalse(Article.objects.filter(pub_date__exact=now).exists())\n\n\n\n", "    def test_isnull_with_foreign_key_field_null(self):\n        author = Author.objects.create(name='Author 1')\n        article = Article.objects.create(author=None)\n        self.assertTrue(Article.objects.filter(author__isnull=True).exists()) \n", "    def test_order_by_related_fields_with_complex_lookups(self):\n        # Tests ordering by related fields with complex lookups, like nested lookups\n        # and using aliases. \n        article1 = Article.objects.create(headline=\"Article 1\", author=self.Author1)\n        article2 = Article.objects.create(headline=\"Article 2\", author=self.Author1)\n        article3 = Article.objects.create(headline=\"Article 3\", author=self.Author2)\n        article4 = Article.objects.create(headline=\"Article 4\", author=self.Author3)\n        \n        # Create some Article tags\n        Tag.objects.create(name=\"tag1\").articles.add(article1)\n        Tag.objects.create(name=\"tag2\").articles.add(article2)\n        Tag.objects.create(name=\"tag3\").articles.add(article3)\n\n        # Test ordering by related field with a lookup on the related object\n        articles = Article.objects.filter(\n            author__name__startswith='Author ').order_by('-author__alias__contains', 'headline')\n        self.assertEqual(\n            [a.headline for a in articles],\n            ['Article 4', 'Article 2', 'Article 1']\n        )\n\n        # Test ordering by related field with multiple lookups\n        articles = Article.objects.filter(\n            author__name__startswith='Author ').order_by(\n            'author__alias__contains',\n            '-author__name'\n        )\n        self.assertEqual(\n            [a.headline for a in articles],\n            ['Article 2', 'Article 1', 'Article 4']\n        )\n\n", "    def test_exact_query_rhs_with_nested_lookup(self):\n        a1 = Author.objects.create(name='John Doe', alias='johndoe')\n        a2 = Author.objects.create(name='Jane Doe', alias='janedoe')\n        a3 = Author.objects.create(name='John Doe', alias='johndoe_2')\n        article1 = Article.objects.create(author=a1, headline='Article 1')\n        article2 = Article.objects.create(author=a2, headline='Article 2')\n        article3 = Article.objects.create(author=a3, headline='Article 3')\n        # Check if the query works correctly with a nested exact lookup\n        result = Article.objects.filter(\n            author__alias__iexact='johndoe',\n            headline__icontains='Article',\n        )\n        self.assertCountEqual(result, [article1])\n        # Check if the query works correctly with multiple nested exact lookups\n        result = Article.objects.filter(\n            author__name__iexact='john doe',\n            headline__icontains='Article',\n        )\n        self.assertCountEqual(result, [article1, article3])\n\n\n\n"], "sample_852": ["def test_make_circles_sparse():\n    factor = 0.3\n\n    for (n_samples, n_outer, n_inner) in [(7, 3, 4), (8, 4, 4)]:\n        # Testing odd and even case, because in the past make_circles always\n        # created an even number of samples.\n        X, y = make_circles(n_samples, shuffle=False, noise=None,\n                            factor=factor, random_state=0, sparse=True)\n        assert X.shape == (n_samples, 2)\n        assert y.shape == (n_samples,)\n        assert isinstance(X, sp.csr_matrix) or isinstance(X, sp.csc_matrix)\n\n        center = [0.0, 0.0]\n        for x, label in zip(X.toarray(), y):\n            dist_sqr = ((x - center) ** 2).sum()\n            dist_exp = 1.0 if label == 0 else factor**2\n            assert_almost_equal(dist_sqr, dist_exp,\n                                err_msg=\"Point is not on expected circle\")\n\n        assert X[y == 0].shape == (n_outer, 2), (\n            \"Samples not correctly distributed across circles.\")\n        assert X[y == 1].shape == (n_inner, 2), (\n            \"Samples not correctly distributed across circles.\")\n\n\n\n", "def test_make_multilabel_classification_sparse_weights():\n    for allow_unlabeled, sparse in zip((True, False), (True, False)):\n        X, Y, p_c, p_w_c = make_multilabel_classification(\n            n_samples=25, n_features=20, n_classes=3, random_state=0,\n            allow_unlabeled=allow_unlabeled,\n            return_distributions=True,\n            return_indicator='sparse')\n\n        assert X.shape == (25, 20), \"X shape mismatch\"\n        assert Y.shape == (25, 3), \"Y shape mismatch\"\n\n        assert p_c.shape == (3,), \"p_c shape mismatch\"\n        assert_almost_equal(p_c.sum(), 1)\n        assert p_w_c.shape == (20, 3)\n        assert_almost_equal(p_w_c.sum(axis=0), [1] * 3)\n\n", "def test_make_friedman1_noise():\n    noise_levels = [0.1, 0.2, 0.5]\n    for noise in noise_levels:\n        X, y = make_friedman1(n_samples=5, n_features=10, noise=noise,\n                              random_state=0)\n        assert_array_almost_equal(y,\n                              10 * np.sin(np.pi * X[:, 0] * X[:, 1])\n                              + 20 * (X[:, 2] - 0.5) ** 2\n                              + 10 * X[:, 3] + 5 * X[:, 4],\n                              decimal=1 if noise > 0 else 5)\n\n\n\n", "def test_make_circles_noise():\n    for noise in [0.0, 0.1, 0.5]:\n        factor = 0.3\n        X, y = make_circles(n_samples=50, shuffle=False, noise=noise,\n                            factor=factor)\n        center = [0.0, 0.0]\n        for x, label in zip(X, y):\n            dist_sqr = ((x - center) ** 2).sum()\n            dist_exp = 1.0 if label == 0 else factor**2\n            assert_almost_equal(dist_sqr, dist_exp,\n                                err_msg=\"Point is not on expected circle\",\n                                decimal=2)  # Allow for noise\n\n\n\n", "def test_make_sparse_coded_signal_with_large_samples(n_samples):\n    Y, D, X = make_sparse_coded_signal(n_samples=n_samples, \n                n_components=8, n_features=10, n_nonzero_coefs=3,\n                random_state=0)\n    assert Y.shape == (10, n_samples), \"Y shape mismatch\"\n    assert D.shape == (10, 8), \"D shape mismatch\"\n    assert X.shape == (8, n_samples), \"X shape mismatch\"\n    for col in X.T:\n        assert len(np.flatnonzero(col)) == 3, 'Non-zero coefs mismatch'\n    assert_array_almost_equal(np.dot(D, X), Y)\n    assert_array_almost_equal(np.sqrt((D ** 2).sum(axis=0)),\n                              np.ones(D.shape[1]))\n\n\n\n"], "sample_122": ["    def test_get_cache_key_with_user(self):\n        request = self.factory.get(self.path)\n        request.user = User(pk=1)\n        template = engines['django'].from_string(\"This is a test\")\n        response = TemplateResponse(request, template)\n        # Expect None if no headers have been set yet.\n        self.assertIsNone(get_cache_key(request))\n        # Set headers to an empty list.\n        learn_cache_key(request, response)\n        # The user object is taken into account.\n        self.assertEqual(\n            get_cache_key(request),\n            'views.decorators.cache.cache_page.settingsprefix.GET.'\n            '1.d41d8cd98f00b204e9800998ecf8427e'\n        )\n", "    def test_get_cache_key_with_different_paths(self):\n        request1 = self.factory.get('/cache/test/')\n        template1 = engines['django'].from_string(\"This is a test\")\n        response1 = TemplateResponse(HttpRequest(), template1)\n        learn_cache_key(request1, response1)\n        \n        request2 = self.factory.get('/cache/other/')\n        template2 = engines['django'].from_string(\"This is another test\")\n        response2 = TemplateResponse(HttpRequest(), template2)\n        learn_cache_key(request2, response2)\n\n        self.assertNotEqual(get_cache_key(request1), get_cache_key(request2))\n", "def test_vary_header(self):\n    request = self.factory.get(self.path)\n    template = engines['django'].from_string(\"This is a test\")\n    response = TemplateResponse(request, template)\n    patch_vary_headers(response, ('Accept-Encoding',))\n    self.assertEqual(response['Vary'], 'Accept-Encoding')\n\n    response = TemplateResponse(request, template)\n    patch_vary_headers(response, ('accept-encoding',))\n    self.assertEqual(response['Vary'], 'Accept-Encoding')\n\n    response = TemplateResponse(request, template)\n    patch_vary_headers(response, ('X-My-Custom-Header',))\n    self.assertEqual(response['Vary'], 'X-My-Custom-Header')\n", "    def test_non_cacheable_response(self):\n        response = HttpResponse(status=304)\n        response['Cache-Control'] = 'no-cache'\n        request = self.factory.get(self.path)\n        caches['default'].add(self.path, response.content, 3600)\n        self.assertIsNone(caches['default'].get(self.path))\n", "    def test_cache_middleware_with_csrf(self):\n        \"\"\"\n        Tests the interaction between CacheMiddleware and the CSRF middleware.\n        \"\"\"\n        csrf_view = csrf_view\n        view = cache_page(3)(csrf_view)\n\n        request = self.factory.get('/view/')\n        response = view(request)\n\n        # First request should hit the view and cache the response\n        self.assertEqual(response.content, csrf(request)['csrf_token'].encode())\n        # Second request should hit the cached response\n        response = view(request)\n        self.assertEqual(response.content, csrf(request)['csrf_token'].encode())\n"], "sample_1053": ["def test_issue_10533():\n    assert (S(1)/S(0)) == oo\n    assert (S(0)/S(1)) == 0\n    assert (S(-1)/S(0)) == -oo\n    assert (S(0)/S(-1)) == 0\n", "def test_negative_zero():\n    assert -0 == -S.Zero\n    assert -0.0 == -S.Zero\n    assert -0.0 == -S.Half + S.Half\n    assert -0.0 != 0.0\n    assert -0 < 0\n\n\n\n", "def test_mod_behavior():\n    assert (S(5) % S(2)) == 1\n    assert (S(-5) % S(2)) == -1\n    assert (S(5) % S(-2)) == 1\n    assert (S(-5) % S(-2)) == -1\n    with raises(ZeroDivisionError):\n        S(5) % S(0)\n    assert (S(5) % S(0)) == 0\n", "def test_issue_12941():\n    assert S(4)/S(3).n(10) == 1.3333333333\n    assert S(4)/S(3).n(5) == 1.33333\n\n\n\n", "def test_mod_for_zero():\n    assert 5 % 0 == 0\n    assert 0 % 5 == 0\n    assert (-5) % 0 == 0\n    assert 0 % (-5) == 0\n"], "sample_716": ["def test_sparse_ridge_solver_auto():\n    rng = np.random.RandomState(0)\n\n    n_samples = 1000\n    n_features = 50\n\n    X = rng.randn(n_samples, n_features)\n    y = rng.randn(n_samples)\n\n    X_sparse = sp.csc_matrix(X)\n    sparse_ridge = Ridge(solver='auto', alpha=1)\n\n    sparse_ridge.fit(X_sparse, y)\n    assert_equal(sparse_ridge.solver, 'sparse_cg')\n\n    dense_ridge = Ridge(solver='auto', alpha=1)\n    dense_ridge.fit(X, y)\n    assert_equal(dense_ridge.solver, 'sag')\n", "def test_ridge_sparse_matrix_weights():\n    X = sp.csr_matrix(X_diabetes)\n    y = y_diabetes\n    ridge = Ridge(fit_intercept=False)\n    # Test with sample weights\n    sample_weight = np.ones(len(y)) * 2\n    ridge.fit(X, y, sample_weight=sample_weight)\n    assert_array_almost_equal(ridge.coef_, ridge.fit(X, y).coef_)\n\n", "    def test_ridge_cv_normalize():\n        X = np.array([[1, 2], [3, 4], [5, 6]])\n        y = np.array([1, 2, 3])\n        ridge_cv = RidgeCV(normalize=True)\n        ridge_cv.fit(X, y)\n        assert_true(ridge_cv.coef_.shape == (2,))\n", "def test_ridge_classifier_multiclass():\n    X, y = make_classification(n_samples=100, random_state=42, n_classes=3)\n    n_samples, n_features = X.shape\n    ridge_classifier = RidgeClassifier(random_state=42)\n    ridge_classifier.fit(X, y)\n    assert_array_equal(ridge_classifier.coef_.shape, (n_classes, n_features))\n    assert_array_equal(ridge_classifier.intercept_.shape, (n_classes,))\n    assert_array_equal(ridge_classifier.predict(X).shape, (n_samples,))\n\n\n\n", "def test_ridge_classifier_multiclass_support():\n    X, y = make_classification(n_samples=10, n_features=2, n_informative=2,\n                              n_classes=3, random_state=0)\n    ridge_classifier = RidgeClassifier()\n    ridge_classifier.fit(X, y)\n    assert_array_equal(ridge_classifier.coef_.shape[0], X.shape[1])\n    y_pred = ridge_classifier.predict(X)\n    assert_array_equal(y_pred.shape, (X.shape[0],))\n\n\n\n"], "sample_1173": ["def test_issue_18281():\n    expr = parse_expr(\"2**(-1)+1/2\", evaluate=False)\n    assert expr == parse_expr(\"2**(-1)+1/2\", transformations=(\n        standard_transformations + (implicit_multiplication,)))\n\n\n\n", "def test_issue_19501_nested_funcs():\n    x = Symbol('x')\n    y = Symbol('y')\n    z = Symbol('z')\n    f = Function('f')\n    g = Function('g')\n    expr = parse_expr('f(g(x, y), z)', local_dict={'x':x, 'y':y, 'z':z}, transformations=(\n        standard_transformations + \n        (implicit_multiplication_application,)))\n    assert expr.free_symbols == {x, y, z}\n", "def test_parse_complex_number():\n    assert parse_expr('3 + 4j') == 3 + 4*I\n", "def test_issue_10560_nested():\n    inputs = {\n        '4*-3*x': '4*(-3)*x',\n        '-4*3*x': '(-4)*3*x',\n        \"-2*x*-4*y\": '(-2)*x*(-4)*y',\n        \"x*-4*y*z\": \"x*(-4)*y*z\"\n    }\n    for text, result in inputs.items():\n        assert parse_expr(text, evaluate=False) == parse_expr(result, evaluate=False)\n", "def test_issue_19501_nested():\n    x = Symbol('x')\n    eq = parse_expr(\n        'E**(x**(1+x))', local_dict={'x': x},\n        transformations=(standard_transformations + (implicit_multiplication_application,)))\n    assert eq.free_symbols == {x}\n"], "sample_128": ["    def test_complex_partial_index_condition(self):\n        index = Index(\n            name='complex_partial_index',\n            fields=['headline'],\n            condition=(\n                Q(published=True) &\n                Q(pub_date__gt=timezone.datetime(2023, 1, 1))\n            ),\n        )\n        with connection.schema_editor() as editor:\n            self.assertIn(\n                'WHERE %s AND %s' % (\n                    editor.quote_name('published'),\n                    editor.quote_name('pub_date'),\n                ),\n                str(index.create_sql(Article, editor)),\n            )\n            editor.add_index(Article, index)\n            with connection.cursor() as cursor:\n                constraints = connection.introspection.get_constraints(\n                    cursor=cursor, table_name=Article._meta.db_table,\n                )\n                self.assertIn(index.name, constraints)\n                self.assertEqual(constraints[index.name]['columns'], ['headline'])\n            editor.remove_index(Article, index)\n", "    def test_multi_column_covering_index(self):\n        index = Index(\n            name='multi_column_covering_idx',\n            fields=['headline', 'pub_date'],\n            include=['published'],\n        )\n        with connection.schema_editor() as editor:\n            self.assertIn(\n                '(%s, %s) INCLUDE (%s)' % (\n                    editor.quote_name('headline'),\n                    editor.quote_name('pub_date'),\n                    editor.quote_name('published'),\n                ),\n                str(index.create_sql(Article, editor)),\n            )\n            editor.add_index(Article, index)\n            with connection.cursor() as cursor:\n                constraints = connection.introspection.get_constraints(\n                    cursor=cursor, table_name=Article._meta.db_table,\n                )\n                self.assertIn(index.name, constraints)\n                self.assertEqual(\n                    constraints[index.name]['columns'],\n                    ['headline', 'pub_date', 'published'],\n                )\n            editor.remove_index(Article, index)\n\n\n\n", "    def test_covering_index_with_condition(self):\n        index = Index(\n            name='covering_partial_headline_idx',\n            fields=['headline'],\n            include=['pub_date', 'published'],\n            condition=Q(pub_date__lt=datetime.datetime(2023, 1, 1)),\n        )\n        with connection.schema_editor() as editor:\n            self.assertIn(\n                '(%s) INCLUDE (%s, %s) WHERE %s' % (\n                    editor.quote_name('headline'),\n                    editor.quote_name('pub_date'),\n                    editor.quote_name('published'),\n                    editor.quote_name('pub_date'),\n                ),\n                str(index.create_sql(Article, editor)),\n            )\n            editor.add_index(Article, index)\n            with connection.cursor() as cursor:\n                constraints = connection.introspection.get_constraints(\n                    cursor=cursor, table_name=Article._meta.db_table,\n                )\n                self.assertIn(index.name, constraints)\n                self.assertEqual(\n                    constraints[index.name]['columns'],\n                    ['headline', 'pub_date', 'published'],\n                )\n            editor.remove_index(Article, index)\n            with connection.cursor() as cursor:\n                self.assertNotIn(index.name, connection.introspection.get_constraints(\n                    cursor=cursor, table_name=Article._meta.db_table,\n                )) \n", "    def test_index_name_with_prefix(self):\n        editor = connection.schema_editor()\n        index_name_prefix = 'my_app_'\n        index = Index(\n            name=f'{index_name_prefix}test_index',\n            fields=['headline'],\n        )\n        with connection.schema_editor() as editor:\n            editor.add_index(Article, index)\n        with connection.cursor() as cursor:\n            constraints = connection.introspection.get_constraints(\n                cursor=cursor, table_name=Article._meta.db_table,\n            )\n            self.assertIn(index.name, constraints)\n            self.assertEqual(constraints[index.name]['name'], f'{index_name_prefix}test_index')\n", "    def test_ops_class_with_covering_index(self):\n        index = Index(\n            name='test_ops_class_covering',\n            fields=['headline'],\n            opclasses=['text_pattern_ops'],\n            include=['body'],\n        )\n        with connection.schema_editor() as editor:\n            self.assertIn(\n                '(%s text_pattern_ops) INCLUDE (%s)' % (\n                    editor.quote_name('headline'),\n                    editor.quote_name('body'),\n                ),\n                str(index.create_sql(IndexedArticle2, editor)),\n            )\n            editor.add_index(IndexedArticle2, index)\n"], "sample_4": ["    def test_readwrite_html_table_metadata(self, cosmo_cls, cosmo, read, write, tmp_path, add_cu):\n        \"\"\"Test metadata is read/written.\"\"\"\n        fp = tmp_path / \"test_readwrite_html_table_metadata.html\"\n\n        # ------------\n        # To Table\n        write(fp, format=\"ascii.html\", **cosmo.meta)\n\n        # ------------\n        # From Table\n        got = cosmo_cls.read(fp, format=\"ascii.html\")\n        assert got.meta == cosmo.meta\n", "    def test_readwrite_html_table_units(self, cosmo, read, write, tmp_path, add_cu):\n        \"\"\"Tests handling of units in HTML table serialization.\"\"\"\n        fp = tmp_path / \"test_readwrite_html_table_units.html\"\n\n        write(fp, format=\"ascii.html\", latex_names=latex_names)\n        \n        # Read the table\n        tbl = QTable.read(fp)\n        \n        # Compare the table units to the original cosmology\n        for col in tbl.columns.values():\n            for row in col:\n                assert isinstance(row, (int, float))\n                param_name = tbl.colnames[tbl.columns.keys()[0]]\n                param = getattr(cosmo, param_name)\n                assert param.unit in (None, u.one) or isinstance(row, (int, float))  # Check if units are converted or handled\n", "    def test_readwrite_html_table_with_metadata(self, cosmo_cls, cosmo, read, write, tmp_path, add_cu):\n        \"\"\"Test if metadata is preserved.\"\"\"\n        fp = tmp_path / \"test_readwrite_html_table_with_metadata.html\"\n        metadata = {\"key\": 42, \"another_key\": \"value\"}\n\n        #  write with metadata\n        write(fp, format=\"ascii.html\", metadata=metadata)\n\n        # read back and check metadata\n        got = cosmo_cls.read(fp, format=\"ascii.html\")\n        assert got.meta == metadata\n\n        # test that metadata can be removed on write\n        write(fp, format=\"ascii.html\", metadata=None)\n        got = cosmo_cls.read(fp, format=\"ascii.html\")\n        assert got.meta == {}\n", "    def test_readwrite_html_table_metadata(self, cosmo, read, write, tmp_path, add_cu):\n        \"\"\"Test if metadata is read and written.\"\"\"\n        fp = tmp_path / \"test_readwrite_html_table_metadata.html\"\n\n        # write cosmology with metadata\n        cosmo_with_metadata = cosmo.clone(name=\"test_cosmo\", meta={\"test_key\": 123})\n\n        write(fp, format=\"ascii.html\")\n\n        read_cosmo = read(fp, format=\"ascii.html\")\n\n        assert read_cosmo.meta == cosmo_with_metadata.meta\n        assert read_cosmo.name == cosmo_with_metadata.name\n", "    def test_readwrite_html_table_with_metadata(self, cosmo, read, write, tmp_path, add_cu):\n        \"\"\"Test cosmology -> ascii.html -> cosmology with metadata.\"\"\"\n        fp = tmp_path / \"test_readwrite_html_table_with_metadata.html\"\n\n        # ------------\n        # To Table\n        write(fp, format=\"ascii.html\",\n              metadata={\"a\": 10, \"b\": \"hello\", \"c\": True})\n\n        # some checks on the saved file\n        tbl = QTable.read(fp)\n        assert tbl.meta == {\"a\": 10, \"b\": \"hello\", \"c\": True}\n        assert tbl[\"name\"] == cosmo.name\n\n        # ------------\n        # From Table\n\n        # read with metadata in meta section\n        got = read(fp, format=\"ascii.html\", move_to_meta=True)\n        assert got.meta == {\"a\": 10, \"b\": \"hello\", \"c\": True}\n        assert got == cosmo\n\n"], "sample_782": ["compilation error", "compilation error", "def test_column_transformer_feature_names_remainder():\n    X = np.random.randn(2, 3)\n    ohe = OneHotEncoder(handle_unknown='ignore', sparse=False)\n\n    ct = ColumnTransformer([\n        ('ohe1', ohe, [0]),\n        ('ohe2', ohe, [1]),\n    ], remainder='passthrough')\n    X_trans = ct.fit_transform(X)\n\n    assert_equal(ct.get_feature_names_out(), ['ohe1_0', 'ohe1_1',\n                                                'ohe2_0', 'ohe2_1', '2',\n                                                '1'])\n\n    # check that names are consistent when using drop\n    ct = ColumnTransformer([\n        ('ohe1', ohe, [0]),\n        ('ohe2', ohe, [1]),\n    ], remainder='drop')\n    X_trans = ct.fit_transform(X)\n\n    assert_equal(ct.get_feature_names_out(), ['ohe1_0', 'ohe1_1',\n                                                'ohe2_0', 'ohe2_1'])\n\n\n\n", "compilation error", "def test_column_transformer_fit_transform_errors():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    ct = ColumnTransformer([('trans', ValueError, [0])])\n    with pytest.raises(ValueError):\n        ct.fit_transform(X_array)\n    with pytest.raises(ValueError):\n        ct.fit(X_array).transform(X_array)\n\n\n\n\n"], "sample_455": ["    def test_expressions_with_include(self):\n        msg = (\n            \"UniqueConstraint.include cannot be used with expressions. \"\n            \"Use django.db.models.F() instead.\"\n        )\n        with self.assertRaisesMessage(ValueError, msg):\n            models.UniqueConstraint(\n                Lower(\"field\"), include=[\"other_field\"], name=\"name\"\n            )\n", "    def test_violation_error_message_and_code(self):\n        constraint = models.UniqueConstraint(\n            fields=[\"name\"],\n            name=\"name_unique\",\n            violation_error_message=\"Custom message for name\",\n            violation_error_code=\"name_unique_error\",\n        )\n        msg = \"Custom message for name\"\n        with self.assertRaisesMessage(ValidationError, msg) as cm:\n            constraint.validate(UniqueConstraintProduct, UniqueConstraintProduct(name=self.p1.name))\n        self.assertEqual(cm.exception.code, \"name_unique_error\")\n", "    def test_expression_with_f_expression(self):\n        msg = \"UniqueConstraint.expressions cannot include F expressions.\"\n        with self.assertRaisesMessage(ValueError, msg):\n            models.UniqueConstraint(\n                Lower(\"name\"), F(\"something_else\"), name=\"name_something_else_uniq\"\n            )\n\n\n", "    def test_unique_constraint_with_condition_and_deferrable(self):\n        message = (\n            \"UniqueConstraint with conditions cannot be deferred.\"\n        )\n        with self.assertRaisesMessage(\n            ValueError, message\n        ):\n            models.UniqueConstraint(\n                fields=[\"name\"],\n                name=\"name_without_color_unique\",\n                condition=models.Q(color__isnull=True),\n                deferrable=models.Deferrable.DEFERRED,\n            )\n", "    def test_unique_constraint_with_violation_error_message_and_code(self):\n        constraint = models.UniqueConstraint(\n            fields=[\"name\"],\n            name=\"unique_name\",\n            violation_error_message=\"Custom name error\",\n            violation_error_code=\"custom_code\",\n        )\n        msg = \"Custom name error\"\n        with self.assertRaisesMessage(ValidationError, msg):\n            constraint.validate(UniqueConstraintProduct, UniqueConstraintProduct(name=\"existing_name\"))\n        self.assertEqual(\n            str(constraint.violation_error_message), \"Custom name error\"\n        )\n        self.assertEqual(\n            str(constraint.violation_error_code), \"custom_code\"\n        )\n\n\n\n"], "sample_391": ["    def test_optimize_nested_create_alter(self):\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel(\"Outer\", [(\"o\", models.IntegerField())]),\n                migrations.CreateModel(\"Inner\", [(\"i\", models.IntegerField())]),\n                migrations.AddField(\n                    \"Outer\", \"inner\", models.ForeignKey(\"migrations.Inner\", models.CASCADE)\n                ),\n                migrations.AlterModelTable(\"Inner\", \"inner_table\"),\n            ],\n            [\n                migrations.CreateModel(\n                    \"Outer\",\n                    [\n                        (\"o\", models.IntegerField()),\n                        (\"inner\", models.ForeignKey(\"migrations.Inner\", models.CASCADE)),\n                    ],\n                ),\n            ],\n        )  \n", "    def test_optimize_remove_field_after_rename_field(self):\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel(\n                    \"Foo\", [(\"name\", models.CharField(max_length=255))]\n                ),\n                migrations.RenameField(\"Foo\", \"name\", \"new_name\"),\n                migrations.RemoveField(\"Foo\", \"new_name\"),\n            ],\n            [\n                migrations.CreateModel(\n                    \"Foo\", [(\"new_name\", models.CharField(max_length=255))],\n                ),\n            ],\n        )\n\n\n\n", "    def test_optimize_reordering_with_operations(self):\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel(\n                    \"Foo\", [(\"name\", models.CharField(max_length=255))]\n                ),\n                migrations.CreateModel(\n                    \"Bar\", [(\"url\", models.TextField())]\n                ),\n                migrations.AddField(\n                    \"Foo\", \"link\", models.ForeignKey(\"migrations.Bar\", models.CASCADE)\n                ),\n                migrations.AlterField(\n                    \"Foo\", \"name\", models.IntegerField()\n                ),\n            ],\n            [\n                migrations.CreateModel(\n                    \"Foo\",\n                    [\n                        (\"name\", models.IntegerField()),\n                        (\"link\", models.ForeignKey(\"migrations.Bar\", models.CASCADE)),\n                    ],\n                ),\n                migrations.CreateModel(\n                    \"Bar\",\n                    [(\"url\", models.TextField())],\n                ),\n            ],\n        )\n", "    def test_optimize_rename_field_after_add(self):\n        self.assertOptimizesTo(\n            [\n                migrations.AddField(\"Foo\", \"new_field\", models.IntegerField()),\n                migrations.RenameField(\"Foo\", \"existing_field\", \"renamed_field\"),\n            ],\n            [\n                migrations.CreateModel(\n                    \"Foo\",\n                    [\n                        (\"renamed_field\", models.IntegerField()),\n                        (\"new_field\", models.IntegerField()),\n                    ],\n                ),\n            ],\n        )\n\n\n", "    def test_optimize_unique_together_and_index_together(self):\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel(\n                    \"Foo\", [(\"a\", models.IntegerField())],\n                ),\n                migrations.CreateModel(\n                    \"Bar\", [(\"b\", models.IntegerField())],\n                ),\n                migrations.AlterUniqueTogether(\"Foo\", [(\"a\", \"b\")]),\n                migrations.AlterIndexTogether(\"Foo\", [(\"a\", \"b\")]),\n                migrations.AddField(\"Foo\", \"c\", models.IntegerField()),\n            ],\n            [\n                migrations.CreateModel(\n                    \"Foo\", [(\"c\", models.IntegerField()), (\"a\", models.IntegerField())],\n                ),\n            ],\n        )\n\n\n"], "sample_420": ["    def test_exclusion_from_queryset(self):\n        \"\"\"\n        Ensures that exclusion attributes are applied to the queryset of returned data.\n\n        \"\"\"\n        class ExcludingItem(models.Model):\n            name = models.CharField(max_length=255)\n            value = models.IntegerField()\n\n            class Meta:\n                model = ExcludingItem\n\n        class ExcludingForm(forms.ModelForm):\n            class Meta:\n                model = ExcludingItem\n                exclude = ['value']\n                fields = '__all__'  # This should be overridden by exclude\n\n        form = ExcludingForm()\n        self.assertEqual(list(form.fields.keys()), ['name'])\n        item = ExcludingItem.objects.create(name='Test', value=42)\n        data = model_to_dict(item)\n        self.assertNotIn('value', data)\n\n        self.assertEqual(data, {'name': 'Test'})\n", "    def test_exclude_related_fields(self):\n        \"\"\"'exclude' in Meta works as expected with related fields.\"\"\"\n        class NestedModel(models.Model):\n            name = models.CharField(max_length=100)\n\n        class ParentModel(models.Model):\n            name = models.CharField(max_length=100)\n            nested = models.ForeignKey(NestedModel, on_delete=models.CASCADE)\n\n        parent = ParentModel.objects.create(name=\"Parent\")\n        nested = NestedModel.objects.create(name=\"Nested\")\n        parent.nested = nested\n\n        form_class = modelform_factory(ParentModel, exclude=(\"nested\",))\n        form = form_class(instance=parent)\n        data = form.cleaned_data\n        self.assertNotIn(\"nested\", data)\n        self.assertIn(\"name\", data)\n\n\n", "    def test_related_object_with_custom_field(self):\n        \"\"\"\n        Regression: Ensure that related objects have their custom fields\n        preserved when serialized.\n\n        See: https://code.djangoproject.com/ticket/19178\n\n        \"\"\"\n        publisher = Publisher.objects.create(name=\"Big Publishing House\")\n        article = Article.objects.create(title=\"My Article\", publisher=publisher)\n\n        class ArticleForm(forms.ModelForm):\n            class Meta:\n                model = Article\n                fields = [\"title\", \"publisher\"]\n\n        form_data = ArticleForm(instance=article).data\n        deserialized_article = Article.objects.get(pk=article.pk)\n\n        self.assertEqual(deserialized_article.publisher_name, \"Big Publishing House\")\n", "    def test_nested_objects(self):\n        parent = Category.objects.create(name=\"Parent Category\")\n        child = SubCategory.objects.create(name=\"Child Category\", parent=parent)\n        data = model_to_dict(child, fields=[\"name\", \"parent\"])\n        self.assertEqual(data[\"parent\"], {\"id\": parent.pk, \"name\": \"Parent Category\"})\n", "    def test_related_many_to_one_fields(self):\n        parent = Inventory.objects.create(barcode=86, name=\"Parent Item\")\n        child = Inventory.objects.create(barcode=87, name=\"Child Item\", parent=parent)\n        data = model_to_dict(child)\n        self.assertEqual(data[\"parent\"], parent)\n"], "sample_337": ["    def test_csrf_token_not_present_on_500(self):\n        response = self.client.get('/500_error/')\n        self.assertEqual(response.status_code, 500)\n        self.assertNotIn(settings.CSRF_COOKIE_NAME, response.cookies)\n\n", "def test_csrf_token_on_error_403_stays_constant(self):\n    response = self.client.get('/csrf-test/', HTTP_REFERER='http://bad-referer.com')\n    self.assertEqual(response.status_code, 403)\n    token1 = response.content\n    response = self.client.get('/csrf-test/', HTTP_REFERER='http://bad-referer.com')\n    self.assertEqual(response.status_code, 403)\n    token2 = response.content\n    self.assertTrue(equivalent_tokens(token1.decode('ascii'), token2.decode('ascii')))\n", "    def test_csrf_token_on_500_stays_constant(self):\n        response = self.client.get('/error/')\n        self.assertEqual(response.status_code, 500)\n        token1 = response.content\n        response = self.client.get('/error/')\n        self.assertEqual(response.status_code, 500)\n        token2 = response.content\n        self.assertTrue(equivalent_tokens(token1.decode('ascii'), token2.decode('ascii')))\n", "    def test_csrf_token_on_500_stays_constant(self):\n        response = self.client.get('/internal_server_error/')\n        self.assertEqual(response.status_code, 500)\n        token1 = response.content\n        response = self.client.get('/internal_server_error/')\n        self.assertEqual(response.status_code, 500)\n        token2 = response.content\n        self.assertTrue(equivalent_tokens(token1.decode('ascii'), token2.decode('ascii')))\n\n", "    def test_csrf_token_on_404_matches_post_request(self):\n        response = self.client.get('/does not exist/')\n        token1 = response.content\n        response = self.client.post('/does not exist/', {'csrfmiddlewaretoken': token1.decode('ascii')})\n        self.assertEqual(response.status_code, 403)  \n\n\n\n"], "sample_916": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_1113": ["compilation error", "def test_index_slicing_symbols():\n    i, j = symbols('i j')\n\n    A = MatrixSymbol('A', 3, 3)\n    row_slice = A[i:j]\n    col_slice = A[:, i:j]\n    \n\n    assert isinstance(row_slice, MatrixExpr)\n    assert isinstance(col_slice, MatrixExpr)\n    assert row_slice.args == (A, (slice(i, j), slice(None)))\n    assert col_slice.args == (A, (slice(None), slice(i, j)))\n\n    # make sure indexing can be done by a symbol and a constant\n    assert A[1:3, 0].args == (A, (slice(1, 3), slice(0, 1)))\n\n\n\n", "def test_blockmatrix_transpose():\n    A = BlockMatrix([[1, 2], [3, 4]])\n    B = A.transpose()\n    assert B[0, 0] == A[0, 0]\n    assert B[0, 1] == A[1, 0]\n    assert B[1, 0] == A[0, 1]\n    assert B[1, 1] == A[1, 1]\n\n    C = BlockMatrix([[W, X], [Y, Z]])\n    assert C.transpose().as_explicit() == \\\n        BlockMatrix([[W.T, Y.T], [X.T, Z.T]])\n\n\n\n", "def test_block_matrix_transpose():\n    A = BlockMatrix([[1, 2], [3, 4]])\n    assert (A.T).blocks == [[1, 3], [2, 4]]\n    A = BlockMatrix([[1, 2, 3], [4, 5, 6]])\n    assert (A.T).blocks == [[1, 4], [2, 5], [3, 6]]\n    A = BlockMatrix([[BlockMatrix([[7, 8]], [9, 10]]), \n                     [BlockMatrix([[11, 12]], [13, 14])]])\n    assert (A.T).blocks == [[BlockMatrix([[7, 11]]), \n                             BlockMatrix([[8, 12]])],\n                            [BlockMatrix([[9, 13]]), \n                             BlockMatrix([[10, 14]])]]\n\n\n\n\n", "def test_block_inverse_zero_blocks():\n    A = BlockMatrix([[2, 0], [0, 0]])\n    try:\n        A.inv()\n    except TypeError:\n        pass\n    else:\n        raise AssertionError(\"Matrix with zero block should not be invertible\")\n\n\n\n"], "sample_303": ["    def test_runshell_merge_environment(self):\n        test_env = {\"TEST_VAR\": \"test_value\"}\n        with mock.patch('subprocess.run') as run:\n            with mock.patch.object(\n                BaseDatabaseClient,\n                'settings_to_cmd_args_env',\n                return_value=([], test_env),\n            ):\n                self.client.runshell(None)\n            run.assert_called_once_with([], env=test_env, check=True)\n", "    def test_runshell_uses_settings(self):\n        with mock.patch(\n            'django.db.backends.base.client.BaseDatabaseClient.settings_to_cmd_args_env',\n            return_value=([], {'MY_ENV_VAR': 'my_value'}),\n        ) as mock_settings_to_cmd_args_env:\n            self.client.runshell(None)\n            mock_settings_to_cmd_args_env.assert_called_once_with(\n                self.client.connection.settings_dict, None\n            )\n", "    def test_runshell_with_provided_env(self):\n        test_env = {'FOO': 'BAR'}\n        with mock.patch('subprocess.run') as run:\n            with mock.patch.object(\n                BaseDatabaseClient,\n                'settings_to_cmd_args_env',\n                return_value=([], test_env),\n            ):\n                self.client.runshell(None)\n            run.assert_called_once_with([], env=test_env, check=True)\n", "    def test_runshell_set_env(self):\n        test_env = {'TEST_VAR': 'test_value'}\n        with mock.patch('subprocess.run') as run:\n            with mock.patch.object(\n                BaseDatabaseClient,\n                'settings_to_cmd_args_env',\n                return_value=([], test_env),\n            ):\n                self.client.runshell(None)\n            run.assert_called_once_with([], env=test_env, check=True)\n", "    def test_runshell_custom_env(self):\n        test_env = {\"FOO\": \"BAR\"}\n        with mock.patch('subprocess.run') as run:\n            with mock.patch.object(\n                BaseDatabaseClient,\n                'settings_to_cmd_args_env',\n                return_value=([], test_env),\n            ):\n                self.client.runshell(None)\n            run.assert_called_once_with([], env=test_env, check=True)\n"], "sample_289": ["    def test_update(self):\n        dict1 = CaseInsensitiveMapping({'name': 'John'})\n        dict1.update({'NAME': 'Jane'})\n        self.assertEqual(dict1, {'name': 'Jane'})\n\n        dict2 = CaseInsensitiveMapping({'name': 'John'})\n        dict2.update({'Name': 'Jane', 'age': 30})\n        self.assertEqual(dict2, {'name': 'Jane', 'age': 30})\n\n\n\n", "    def test_setdefault_key_exists(self):\n        self.dict1 = CaseInsensitiveMapping({'Accept': 'application/json'})\n        self.assertEqual(self.dict1.setdefault('Accept', 'text/plain'), 'application/json')\n        self.assertEqual(self.dict1['Accept'], 'application/json')\n\n", "    def test_setdefault(self):\n        self.assertEqual(self.dict1.setdefault('new_key', 'default'), 'default')\n        self.assertEqual(self.dict1['new_key'], 'default')\n        self.assertEqual(self.dict1.setdefault('Accept', 'new_default'), 'application/json')\n        self.assertEqual(self.dict1['Accept'], 'application/json')\n\n\n\n", "    def test_setdefault(self):\n        self.assertEqual(self.dict1.setdefault('newkey', 'default'), 'default')\n        self.assertEqual(self.dict1['newkey'], 'default')\n        self.assertEqual(self.dict1.setdefault('Accept', 'different'), 'application/json')\n\n", "    def test_setdefault(self):\n        with self.subTest(\"Case sensitive key\"):\n            self.assertEqual(self.dict1.setdefault('New Key', 'default'), 'default')\n            self.assertEqual(self.dict1, {'Accept': 'application/json', 'content-type': 'text/html', 'New Key': 'default'})\n\n\n        with self.subTest(\"Existing key\"):\n            self.assertEqual(self.dict1.setdefault('Accept', 'new_value'), 'application/json')\n            self.assertEqual(self.dict1, {'Accept': 'application/json', 'content-type': 'text/html'})\n"], "sample_733": ["    def test_vectorizer_empty_vocabulary():\n        # test for empty vocabulary case\n        vect = CountVectorizer(vocabulary=[])\n\n        X = vect.fit_transform(JUNK_FOOD_DOCS)\n        assert_array_equal(X.shape, (len(JUNK_FOOD_DOCS), 0))\n        assert_array_equal(vect.get_feature_names(), [])\n        assert_equal(len(vect.vocabulary_), 0)\n\n\n", "def test_vectorizer_string_type_vocabulary():\n    message = (\"Vocabulary must be a set of strings, \"\n               \"not a list of (string, int) pairs.\")\n    for vec in [CountVectorizer(), TfidfVectorizer(), HashingVectorizer()]:\n        assert_raise_message(\n            TypeError, message, vec.set_params, vocabulary=[('a', 1)])\n\n    \n\n\n\n", "    def test_vectorizer_empty_vocabulary():\n        vect = CountVectorizer(vocabulary=set())\n        assert_raises(ValueError, vect.fit, [])\n        assert_raises(ValueError, vect.transform, [])\n", "def test_vectorizer_max_features():\n    test_data = ['apple', 'banana', 'orange', 'apple', 'banana']\n    vect = CountVectorizer(max_features=2)\n    X = vect.fit_transform(test_data)\n    assert_equal(X.shape[1], 2)\n    assert_array_equal(vect.get_feature_names(), ['apple', 'banana'])\n\n    vect = TfidfVectorizer(max_features=2)\n    X = vect.fit_transform(test_data)\n    assert_equal(X.shape[1], 2)\n    assert_array_equal(vect.get_feature_names(), ['apple', 'banana'])\n\n    vect = HashingVectorizer(max_features=2)\n    X = vect.fit_transform(test_data)\n    assert_equal(X.shape[1], 2)\n\n\n\n", "def test_vectorizer_max_df_and_min_df_filtering():\n    data = [\n        \"the the quick brown fox jumps over the lazy dog\",\n        \"the quick brown fox jumps over lazy dog\",\n        \"the dog jumps over the lazy fox\",\n        \"the fox jumps over the lazy dog\",\n    ]\n    for vec in [CountVectorizer(), TfidfVectorizer()]:\n        vec.fit(data)\n\n        # Test max_df\n        vec_max_df = vec.copy()\n        vec_max_df.set_params(max_df=0.5)\n        X_max_df = vec_max_df.fit_transform(data)\n        assert_array_equal(vec_max_df.get_feature_names(),\n                          ['the', 'quick', 'brown', 'fox', 'jumps',\n                           'over', 'lazy', 'dog'])\n\n        # Test min_df\n        vec_min_df = vec.copy()\n        vec_min_df.set_params(min_df=2)\n        X_min_df = vec_min_df.fit_transform(data)\n        assert_array_equal(vec_min_df.get_feature_names(),\n                          ['the', 'quick', 'brown', 'jumps', 'over',\n                           'lazy', 'dog'])\n"], "sample_1007": ["def test_subfactorial_rewrite():\n    x = Symbol('x')\n    assert subfactorial(x).rewrite(factorial) == factorial(x)/factorial(x + 1)\n    assert subfactorial(x).rewrite(uppergamma) == uppergamma(x + 1, -1)/S.Exp1\n\n\n", "def test_factorial_series_complex():\n    n = Symbol('n', integer=True)\n    k = Symbol('k', integer=True)\n\n    assert binomial(n, k).series(n, 0, 3) == \\\n        1 - n*EulerGamma + n**2*(EulerGamma**2/2 + pi**2/12) + O(n**3)\n", "compilation error", "def test_subfactorial_rewrite():\n    x = Symbol('x')\n    assert subfactorial(x).rewrite(factorial) == (\n        factorial(x)/gamma(x + 1))\n    assert subfactorial(x).rewrite(gamma) == (\n        -gamma(x + 1)/gamma(S(1))).simplify()\n", "def test_uppergamma_rewrite():\n    x = Symbol('x')\n    n = Symbol('n', integer=True)\n    a = Symbol('a')\n    assert uppergamma(x, n).rewrite(gamma) == gamma(x + n)/gamma(x)\n    assert uppergamma(x, n).rewrite(factorial) == gamma(x + n)/factorial(n)\n    assert uppergamma(x, 0).rewrite(gamma) == gamma(x)\n    assert uppergamma(x, 1).rewrite(gamma) == gamma(x + 1)/x\n    assert uppergamma(x, a).rewrite(uppergamma) == uppergamma(x, a) \n    assert uppergamma(x, S(-1)).rewrite(uppergamma) == uppergamma(x, S(-1))\n    assert uppergamma(1, a).rewrite(uppergamma) == uppergamma(1, a) \n"], "sample_783": ["def test_imputation_constant_with_sparse(array_constructor):\n    # Test imputation using the constant strategy on sparse arrays\n    X = array_constructor(np.array([[np.nan, 1.1, 0, np.nan],\n                                   [1.2, np.nan, 1.3, np.nan],\n                                   [0, 0, np.nan, np.nan],\n                                   [1.4, 1.5, 0, np.nan]]))\n\n    X_true = array_constructor(np.array([[ -1, 1.1, 0, -1],\n                                       [1.2, -1, 1.3, -1],\n                                       [0, 0, -1, -1],\n                                       [1.4, 1.5, 0, -1]]))\n\n    imputer = SimpleImputer(strategy=\"constant\", fill_value=-1)\n    X_trans = imputer.fit_transform(X)\n\n    assert_allclose_dense_sparse(X_trans, X_true)\n\n\n\n", "def test_imputation_with_dtype(dtype):\n    X = sparse_random_matrix(10, 10, density=0.5, random_state=42).astype(dtype)\n    imputer = SimpleImputer(missing_values=0, strategy=\"mean\",\n                            copy=True)\n    X_trans = imputer.fit_transform(X)\n    assert X_trans.dtype == dtype\n", "def test_missing_indicator_with_custom_missing_values(sparse):\n    X = np.array([[np.nan, 1, 2], [3, 4, np.nan]], dtype=float)\n    if sparse:\n        X = sparse.csr_matrix(X)\n    indicator = MissingIndicator(missing_values=[np.nan], features='all')\n    X_trans = indicator.fit_transform(X)\n    assert_array_almost_equal(X_trans, np.array([[True, False, False],\n                [False, False, True]]))\n\n\n\n", "def test_imputation_constant_dtype(dtype):\n    X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=dtype)\n    X[0, 0] = np.nan\n\n    imputer = SimpleImputer(missing_values=np.nan, strategy=\"constant\",\n                            fill_value=0, copy=True)\n    Xt = imputer.fit_transform(X)\n\n    assert_array_equal(Xt.dtype, dtype)\n\n    for i in range(Xt.shape[0]):\n        for j in range(Xt.shape[1]):\n            if np.isnan(X[i, j]):\n                assert Xt[i, j] == 0\n            else:\n                assert Xt[i, j] == X[i, j]\n", "def test_imputation_constant_object_sparse(X_data, missing_value):\n    # Test imputation using the constant strategy on sparse objects\n    sparse_X = sparse.csr_matrix(X_data).astype(object)\n    imputer = SimpleImputer(missing_values=missing_value, strategy='constant',\n                            fill_value='missing_value')\n    X_trans = imputer.fit_transform(sparse_X)\n    assert_array_equal(X_trans.toarray(), np.array([['missing_value',\n                                                   'missing_value',\n                                                   'missing_value',\n                                                   'missing_value']]))\n\n\n\n"], "sample_711": ["def test_getfslineno(pytester: Pytester) -> None:\n    \"\"\"Test getfslineno for cases with multiple imports and complex paths\n\n\n    \"\"\"\n    p = pytester.makepyfile(\n        \"\"\"\n        import sys\n        import os\n\n\n            pass\n        \n            pass\n    \"\"\"\n    )\n    for item in pytester.getitems():\n        path = item.location[0]\n        lineno = nodes.getfslineno(item)\n        assert lineno is not None\n", "def test_repr_failure_with_custom_traceback(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import traceback\n            raise ValueError(\"test fail\")\n\n        class CustomTraceback(Exception):\n                super().__init__(message)\n                self.traceback = traceback\n\n            raise CustomTraceback(\"custom traceback\", traceback.extract_stack())\n\n\n\n\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines([\"*Exception: test fail\", \"*custom traceback\", \"*test_fail_with_custom_traceback\"])\n", "def test_node_repr_failure_short_with_fulltrace(pytester: Pytester) -> None:\n    with pytester.raises_warning(\n        PytestWarning,\n        match=\"The 'tbstyle' option is deprecated in favor of using 'fulltrace'.\",\n    ):\n        pytester.runpytest(\"-tb=short\", \"-rs\", \"test_repr_failure.py\")\n", "def test_node_get_closest_marker(\n    pytester: Pytester,", "def test_node_repr_failure_with_fixture_import_failure(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture\n            import nonexistent_module\n            return 0\n\n            pass\n    \"\"\"\n    )\n\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            str(Path(__file__).parent / \"test_*.py\"),\n            \"*1 failed*\",\n            \"*fixture 'my_fixture' failed to load*\",\n        ]\n    )\n\n\n\n"], "sample_52": ["    def test_extra_filters(self):\n        extra_filter = forms.modelformset_factory(\n            Category,\n            form=forms.ModelForm,\n            fields=['name'],\n            extra=1,\n            can_delete=True,\n        )\n        formset = extra_filter(data={'name': ['Test Category']}, prefix='cat')\n        self.assertEqual(formset.errors, {})\n        self.assertTrue(formset.is_valid())\n", "    def test_empty_label_with_empty_choices(self):\n        f = forms.ModelChoiceField(queryset=Category.objects.none(), empty_label='select one')\n        self.assertEqual(f.clean(''), '')\n        self.assertEqual(f.choices, [('', 'select one')])\n        with self.assertRaises(ValidationError):\n            f.clean(42)\n\n", "    def test_choice_order_respected(self):\n        f = forms.ModelChoiceField(Category.objects.order_by('id'))\n        self.assertEqual(list(f.choices), [\n            (self.c1.pk, 'Entertainment'),\n            (self.c2.pk, 'A test'),\n            (self.c3.pk, 'Third'),\n        ])\n\n", "    def test_prepared_value_from_object(self):\n        field = forms.ModelChoiceField(Category.objects.all())\n        obj = self.c1\n        self.assertEqual(field.prepare_value(obj), self.c1.pk)\n", "    def test_empty_label_with_optional_empty_choice(self):\n        f = forms.ModelChoiceField(Category.objects.all(), empty_label='Choose an option')\n        self.assertEqual(f.choices[0], ('', 'Choose an option'))\n        with self.assertRaises(ValidationError):\n            f.clean('')\n\n\n"], "sample_863": ["compilation error", "compilation error", "def test_pipeline_steps_reordering():\n    transf1 = Transf()\n    transf2 = Transf()\n    clf = FitParamT()\n    pipe = Pipeline([('transf1', transf1), ('clf', clf)])\n    pipe2 = Pipeline([('clf', clf), ('transf1', transf1)])\n\n    assert_array_equal(pipe.fit_transform(np.array([[1]]), [1]),\n                       pipe2.fit_transform(np.array([[1]]), [1]))\n    assert pipe.predict(np.array([[1]])) == pipe2.predict(np.array([[1]]))\n\n\n\n", "compilation error", "def test_pipeline_reset():\n    X = iris.data\n    y = iris.target\n    clf = Pipeline([('transf', DummyTransf()), ('svc', SVC())])\n\n    # Test that pipelines can be reset \n    clf.fit(X, y)\n\n    # Check that the fit parameters are reset\n    assert clf.named_steps['transf'].means_ is not None\n    clf.reset_params()\n    assert clf.named_steps['transf'].means_ is None\n"], "sample_804": ["def test_one_hot_encoder_mapping_error():\n    ohe = OneHotEncoder(handle_unknown='ignore')\n    X = [['abc', 'xyz'], ['def', 'pqr']]\n    with pytest.raises(ValueError, match=\"Could not find a mapping\"):\n        ohe.fit(X)\n\n\n", "def test_one_hot_encoder_sparse_dense(X):\n    # test both sparse and dense output\n    for sparse in [True, False]:\n        ohe_sparse = OneHotEncoder(sparse=sparse)\n        ohe_dense = OneHotEncoder(sparse=not sparse)\n\n        ohe_sparse.fit(X)\n        ohe_dense.fit(X)\n\n        # ensure transformed arrays are equivalent (shape, values)\n        assert_array_equal(ohe_sparse.transform(X), ohe_dense.transform(X))\n\n        if sparse:\n            assert isinstance(ohe_sparse.transform(X), sparse.csr_matrix)\n\n\n", "def test_categorical_encoder_order(X):\n    enc = OneHotEncoder(handle_unknown='ignore', sparse=False, categories='auto')\n    X_tr = enc.fit_transform(X)\n    assert_array_equal(X_tr[:, 0],\n                        np.array([1., 0., 1.]))\n\n\n\n", "def test_one_hot_encoder_multiple_columns(X):\n    enc = OneHotEncoder(sparse=False)\n    exp = np.array([[1, 0, 0, 0, 0, 0, 0, 0],\n                    [0, 1, 0, 0, 0, 0, 0, 0],\n                    [0, 0, 1, 0, 0, 0, 0, 0],\n                    [0, 0, 0, 1, 0, 0, 0, 0]])\n    assert_array_equal(enc.fit_transform(X), exp)\n\n\n\n", "def test_one_hot_encoder_sparse_output_with_dtype():\n    # Test specifying dtype for sparse output\n    X = np.array([['a'], ['b']])\n    ohe = OneHotEncoder(sparse=True, dtype='int16')\n    trans = ohe.fit_transform(X)\n    assert trans.dtype == 'int16'\n"], "sample_648": ["compilation error", "compilation error", "def test_mark_indirection(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.skip\n            pass\n        @pytest.mark.xfail\n            pass\n        class SkipClass(object):\n            @pytest.mark.skip\n                pass\n\n        class XFailClass(object):\n            @pytest.mark.xfail\n                pass\n        \"\"\"\n    )\n\n    reprec = pytester.inline_run(\"-m\", \"skip\")\n    passed, skipped, failed = reprec.countoutcomes()\n    assert passed == 0\n    assert skipped == 3\n    assert failed == 0\n\n\n", "def test_mark_order_with_multiple_bases(pytester: Pytester) -> None:\n\n\n\n    class Base1(object):\n        @pytest.mark.mark1\n            pass\n\n    class Base2(object):\n        @pytest.mark.mark2\n            pass\n\n    class TestClass(Base1, Base2):\n        @pytest.mark.mark3\n            pass\n\n    node = pytester.inline_runsource(\n        f\"\"\"\n        class TestClass(Base1, Base2):\n            pass\n    \"\"\")\n\n    extracted = [x.name for x in node.iter_markers()]\n    # check the order of markers\n    assert extracted == [\"mark3\", \"mark1\", \"mark2\"]\n", "def test_mark_mro_no_repeat() -> None:\n    xfail = pytest.mark.xfail\n\n    @xfail(\"a\")\n    class A:\n        pass\n\n    @xfail(\"a\")\n    class B:\n        pass\n\n    class C(A, B):\n        pass\n\n    assert len(get_unpacked_marks(C)) == 1\n\n"], "sample_29": ["    def test_write_latex_with_metadata(self, write, tmp_path):\n        \"\"\"\n        Test writing LaTeX output with metadata.\n\n        This test ensures that when cosmology metadata is included, it is\n        correctly serialized into the LaTeX output.\n        \"\"\"\n        fp = tmp_path / \"test_write_latex_metadata.tex\"\n        cosmo = self.cosmo\n        cosmo_with_metadata =  cosmo.copy('test_metadata')\n        write(fp, format=\"latex\", cosmology=cosmo_with_metadata, latex_names=True)\n        tbl = QTable.read(fp)\n   \n        # Assert that metadata is present in the table\n        assert len(tbl.meta) > 0\n\n\n", "    def test_to_latex_custom_unit(self, write, tmp_path):\n        \"\"\"Test writing cosmology with custom units.\"\"\"\n        fp = tmp_path / \"test_to_latex_custom_unit.tex\"\n\n        # Create a custom cosmology with a custom unit\n        class CustomCosmology(Cosmology):\n            _parameters = {'custom_param': {'unit': 'km/s'}}\n\n        cosmo = CustomCosmology()\n        cosmo.custom_param = 70\n\n        write(fp, format=\"latex\", cosmology=cosmo)\n\n        # ... assertions to check the output ...\n", "    def test_write_latex_empty_cosmology(self, write, tmp_path):\n        empty_cosmo = Cosmology()\n        fp = tmp_path / \"test_empty_cosmology.tex\"\n        write(fp, format=\"latex\", cosmology=empty_cosmo)\n        tbl = QTable.read(fp)\n        assert len(tbl.data) == 0\n\n\n\n", "    def test_write_latex_existing_file_overwrite(self, write, tmp_path):\n        fp = tmp_path / \"test_write_latex_existing_file_overwrite.tex\"\n        write(fp, format=\"latex\")\n        with open(fp, \"w\") as f:\n            f.write(\"This is some old content.\")\n\n        # Test that overwriting works as expected\n        write(fp, format=\"latex\", overwrite=True)\n", "    def test_latex_names_false(self, write, tmp_path):\n        fp = tmp_path / \"test_latex_names_false.tex\"\n        write(fp, format=\"latex\", latex_names=False)\n        tbl = QTable.read(fp)\n        # asserts each column name is not in _FORMAT_TABLE values\n        for column_name in tbl.colnames[2:]:\n            assert column_name not in _FORMAT_TABLE.values()\n"], "sample_1071": ["def test_quantity_simplify_nested():\n    from sympy.physics.units import kilo, foot, inch, gram, meter\n    from sympy.physics.units.util import quantity_simplify\n\n    assert quantity_simplify(quantity_simplify(kilo*meter*foot*inch)) == 250*kilo*foot**2/12\n    assert quantity_simplify(quantity_simplify((kilo*meter*foot*inch))) == 250*kilo*foot**2/12\n\n\n", "def test_conflict_prefixes():\n    from sympy.physics.units import meter, centimeter, kilometer, milli, micro\n\n    assert quantity_simplify(meter + centimeter) == meter + centimeter\n    assert quantity_simplify(meter + milli*meter) == meter * 1.001\n    assert quantity_simplify(meter + micro*meter) == meter * 1.000001\n    assert quantity_simplify(kilometer + centimeter) == kilometer + centimeter\n    assert quantity_simplify(kilometer + milli*kilometer) == kilometer * 1.001 \n", "def test_convert_to_with_functions():\n    from sympy.physics.units import speed_of_light\n\n    assert convert_to(sqrt(speed_of_light), meter/second) == sqrt(299792458)*meter/second\n    assert convert_to(sin(speed_of_light), meter) == sin(299792458)*meter\n    assert convert_to(pi*sqrt(speed_of_light), meter) == pi*sqrt(299792458)*meter\n\n\n\n", "compilation error", "compilation error"], "sample_1040": ["def test_print_indices():\n    A = MatrixSymbol('A', 1, 2)\n    assert mpp.doprint(A[S(0), S(1)]) == '<apply><index/><ci>A</ci><apply><row /><col/><ci>1</ci></apply></apply>'\n\n    assert mpp.doprint(A[S(1), S(0)]) == '<apply><index/><ci>A</ci><apply><row/><col/><ci>0</ci></apply></apply>'\n\n\n\n", "def test_presentation_matrix_printing():\n    A = Matrix([[1, 2], [3, 4]])\n    mml_A = mpp._print(A)\n    assert mml_A.nodeName == 'mtable'\n    assert len(mml_A.childNodes) == 3\n    assert mml_A.childNodes[0].nodeName == 'mtr'\n    assert mml_A.childNodes[0].childNodes[0].nodeName == 'mtd'\n    assert mml_A.childNodes[0].childNodes[0].childNodes[0].nodeValue == '1'\n    assert mml_A.childNodes[0].childNodes[1].nodeName == 'mtd'\n    assert mml_A.childNodes[0].childNodes[1].childNodes[0].nodeValue == '2'\n    assert mml_A.childNodes[1].nodeName == 'mtr'\n    assert mml_A.childNodes[1].childNodes[0].nodeName == 'mtd'\n    assert mml_A.childNodes[1].childNodes[0].childNodes[0].nodeValue == '3'\n    assert mml_A.childNodes[1].childNodes[1].nodeName == 'mtd'\n    assert mml_A.childNodes[1].childNodes[1].childNodes[0].nodeValue == '4'\n\n\n\n", "def test_print_conditional_expression():\n    expr = Piecewise((x > 0, x**2, True, 1))\n    assert mathml(expr, printer='presentation') == '<piecewise><when><lt><ci>x</ci><cn>0</cn></when><then><msup><mi>x</mi><mn>2</mn></msup></then><otherwise><cn>1</cn></otherwise></piecewise>'\n    assert mathml(expr, printer='content') == '<apply><piecewise><when><apply><lt><ci>x</ci><cn>0</cn></apply></when><then><apply><power/><ci>x</ci><cn>2</cn></apply></then><otherwise><cn>1</cn></otherwise></piecewise></apply>'\n", "def test_presentation_mathml_functions():\n    assert mathml(sin(x), printer='presentation') == '<mi>sin</mi><mfenced><mi>x</mi></mfenced>'\n    assert mathml(cos(x), printer='presentation') == '<mi>cos</mi><mfenced><mi>x</mi></mfenced>'\n    assert mathml(tan(x), printer='presentation') == '<mi>tan</mi><mfenced><mi>x</mi></mfenced>'\n    assert mathml(log(x, base=2), printer='presentation') == '<mi>log</mi><mfenced><mi>x</mi></mfenced><mstyle displaystyle=\"true\" scriptlevel=\"0\"><mo>&#x2081;</mo></mstyle><mstyle displaystyle=\"true\" scriptlevel=\"0\"><mn>2</mn></mstyle>'\n    assert mathml(sqrt(x), printer='presentation') == '<msqrt><mi>x</mi></msqrt>'\n    assert mathml(abs(x), printer='presentation') == '<mfenced close=\"|\" open=\"|\"><mi>x</mi></mfenced>'\n\n\n", "def test_print_matrix_elements():\n    A = Matrix([1, 2, 3, 4])\n    assert mpp._print(A) == '<mtable><mtr><td class=\"mord\">1</td>\\"], "sample_22": ["def test_matrix_product_stack():\n    m1 = np.array([[1, 2], [3, 4]])\n    m2 = np.array([[5, 6], [7, 8]])\n    m3 = np.array([[9, 10], [11, 12]])\n\n    stacked = np.stack((m1, m2, m3), axis=-2)\n    result = matrix_product(stacked)\n\n    expected = np.stack(\n        (\n            np.dot(m1, np.dot(m2, m3)),\n            np.dot(m1, np.dot(m2, m3)),\n        ),\n        axis=-2,\n    )\n    assert_allclose(result, expected)\n\n\n\n", "def test_matrix_transpose():\n    m1 = rotation_matrix(35 * u.deg, \"x\")\n    m2 = matrix_transpose(m1)\n    assert_allclose(m1 @ m2, np.eye(3), atol=1e-12)\n    # and (M, 3, 3)\n    n1 = np.tile(m1, (2, 1, 1))\n    n2 = matrix_transpose(n1)\n    assert_allclose(n1 @ n2, np.eye(2) * np.eye(3), atol=1e-12)\n\n\n\n", "def test_matrix_transpose():\n    m1 = rotation_matrix(35 * u.deg, \"x\")\n    m2 = matrix_transpose(m1)\n    assert_allclose(m2, m1.T)\n    \n    n1 = np.tile(m1, (2, 1, 1))\n    n2 = matrix_transpose(n1)\n    assert_allclose(n2, n1.transpose((0, 2, 1))) \n", "def test_matrix_product_stack():\n    m1 = np.eye(2)\n    m2 = np.eye(2)\n    m3 = np.eye(2)\n    m_stack = np.stack((m1, m2, m3), axis=-2)\n    result = matrix_product(m_stack)\n    expected = np.stack((m1 @ m2 @ m3, m1 @ m2 @ m3), axis=-2)\n    assert_allclose(result, expected)\n", "compilation error"], "sample_864": ["def test_seeding(bin_seeding, seeds):\n    ms1 = MeanShift(bandwidth=1.2, bin_seeding=bin_seeding, seeds=seeds)\n    ms2 = MeanShift(bandwidth=1.2, bin_seeding=bin_seeding, seeds=None)\n\n    labels1 = ms1.fit(X).labels_\n    labels2 = ms2.fit(X).labels_\n\n    assert_array_equal(labels1, labels2)\n", "def test_bin_seeds_with_sparse_matrix():\n    # Test get_bin_seeds with a sparse matrix\n\n    X = sparse.lil_matrix((1000, 1000))\n    msg = \"A dense array is expected for binning, got a sparse matrix.\"\n    with pytest.raises(TypeError, match=msg):\n        get_bin_seeds(X, 0.5, 1)\n", "def test_bin_seeding_large_data():\n    # Test bin seeding with a large dataset to ensure it scales\n    n_samples = 10000\n    X, _ = make_blobs(n_samples=n_samples, n_features=2, centers=centers,\n                      cluster_std=0.4, shuffle=True, random_state=11)\n    ms = MeanShift(seeds=None, bin_seeding=True, bandwidth=1.2)\n\n    ms.fit(X)\n", "    def test_bin_seeds_with_warnings():\n        # Test get_bin_seeds with warnings\n        X = np.array([[1, 1], [1.4, 1.4], [1.8, 1.2],\n                      [2., 1.], [2.1, 1.1], [0., 0.]])\n        \n        with warnings.catch_warnings(record=True) as w:\n            warnings.simplefilter('always')\n            get_bin_seeds(X, 0.1, 2)  \n        \n        assert len(w) == 1\n        assert \"using data points as seeds\" in str(w[-1].message) \n", "def test_meanshift_seeds_and_bin_seeding():\n    # Test when seeds are provided, and bin_seeding = False\n    X = np.array([[1, 1], [2, 1], [1, 0],\n                  [4, 7], [3, 5], [3, 6]])\n    seeds = np.array([[0, 0], [5, 5]])\n    ms = MeanShift(seeds=seeds, bin_seeding=False)\n    labels = ms.fit_predict(X)\n    assert_array_equal(labels, [0, 0, 0, 1, 1, 1])\n\n    # Test when bin_seeding = True and seeds are not provided\n    ms = MeanShift(bin_seeding=True)\n    labels = ms.fit_predict(X)\n    assert_array_equal(labels, [1, 1, 1, 0, 0, 0])\n\n"], "sample_12": ["def test_array_angle_to_string_raises_error():\n    # Test that the to_string method raises an error when called on an Angle array with a non-string unit.\n    with pytest.raises(TypeError, match='Units must be a string'):\n        Angle([1, 2] * u.deg).to_string(unit=u.hour)\n", "def test_angle_invalid_input(value, expected_error):\n    with pytest.raises(ValueError, match=expected_error):\n        Angle(value)\n", "def test_angle_from_strings(input):\n    a = Angle(input)\n    assert a.unit == u.deg\n    assert a.value == 10.0\n\n\n", "    def test_angle_with_empty_string():\n        with pytest.raises(ValueError):\n            Angle('')\n", "def test_angle_from_string_with_unit_mismatch():\n    with pytest.raises(ValueError) as exc:\n        Angle('10d', u.hour)\n    assert \"Units do not match for Angle\" in str(exc.value)\n\n\n\n"], "sample_761": ["def test_imputation_add_indicator_sparse_matrix_with_missing_0(arr_type):\n    X_sparse = arr_type([\n        [0, 1, 5],\n        [2, 0, 1],\n        [6, 3, 0],\n        [1, 2, 9]\n    ])\n    X_true = np.array([\n        [3., 1., 5., 0., 0., 0.],\n        [2., 2., 1., 0., 1., 0.],\n        [6., 3., 5., 0., 0., 1.],\n        [1., 2., 9., 0., 0., 0.],\n    ])\n\n    imputer = SimpleImputer(missing_values=0, add_indicator=True)\n    X_trans = imputer.fit_transform(X_sparse)\n\n    assert sparse.issparse(X_trans)\n    assert X_trans.shape == X_true.shape\n    assert_allclose(X_trans.toarray(), X_true)\n", "def test_imputation_add_indicator_different_strategies(strategy):\n    X = np.array([\n        [np.nan, 1, 5, np.nan, 1],\n        [2, np.nan, 1, np.nan, 2],\n        [6, 3, np.nan, np.nan, 3],\n        [1, 2, 9, np.nan, 4]\n    ])\n    if strategy == \"most_frequent\":\n        X_true = np.array([\n            [3., 1., 5., 1., 1., 0., 0., 1.],\n            [2., 2., 1., 2., 0., 1., 0., 1.],\n            [6., 3., 5., 3., 0., 0., 1., 1.],\n            [1., 2., 9., 4., 0., 0., 0., 1.]\n        ])\n    else:\n        X_true = np.array([\n            [2., 1., 5., 2., 1., 0., 0., 1.],\n            [2., 2., 1., 2., 0., 1., 0., 1.],\n            [6., 3., 5., 3., 0., 0., 1., 1.],\n            [1., 2., 9., 2., 0., 0., 0., 1.]\n        ])\n\n    imputer = SimpleImputer(missing_values=np.nan, strategy=strategy, add_indicator=True)\n    X_trans = imputer.fit_transform(X)\n\n    assert_allclose(X_trans, X_true)\n    assert_array_equal(imputer.indicator_.features_, np.array([0, 1, 2, 3]))\n", "def test_imputation_with_missing_indicator(strategy):\n    X = np.array([\n        [1, np.nan, 5, np.nan, 1],\n        [2, np.nan, 1, np.nan, 2],\n        [6, 3, np.nan, np.nan, 3]\n    ])\n\n    mi = MissingIndicator(missing_values=np.nan)\n    X_imputed_mi = mi.fit_transform(X)\n\n    imputer = SimpleImputer(missing_values=np.nan, strategy=strategy)\n    X_imputed = imputer.fit_transform(X)\n\n    assert X_imputed_mi.shape == X_imputed.shape\n    assert_allclose(X_imputed_mi, X_imputed)\n", "def test_imputation_add_indicator_mixed_dtypes():\n    X = np.array([\n        [np.nan, 1, 5., 'a'],\n        [2, np.nan, 1, 'b'],\n        [6, 3, np.nan, 'c'],\n        [1, 2, 9, 'd']\n    ])\n\n    imputer = SimpleImputer(missing_values=np.nan, add_indicator=True)\n    X_trans = imputer.fit_transform(X)\n\n    assert X_trans.dtype == object\n    assert X_trans.shape[1] == 8\n    assert X_trans[0, 4] == 0\n    assert X_trans[1, 5] == 0\n    assert X_trans[2, 6] == 0\n    assert X_trans[3, 7] == 0\n\n", "def test_imputation_add_indicator_sparse_mixed_dtype(arr_type):\n    X_sparse = arr_type([\n        [np.nan, 1, 5.5],\n        [2, np.nan, 1.0],\n        [6, 3, np.nan],\n        [1, 2, 9]\n    ])\n    X_true = np.array([\n        [3., 1., 5.5, 1., 0., 0.],\n        [2., 2., 1., 0., 1., 0.],\n        [6., 3., 5., 0., 0., 1.],\n        [1., 2., 9., 0., 0., 0.],\n    ], dtype=float)\n\n    imputer = SimpleImputer(missing_values=np.nan, add_indicator=True)\n    X_trans = imputer.fit_transform(X_sparse)\n\n    assert sparse.issparse(X_trans)\n    assert X_trans.shape == X_true.shape\n    assert_allclose(X_trans.toarray(), X_true)\n"], "sample_817": ["def test_non_numerical_data():\n    # Test that VarianceThreshold raises an error when given non-numerical data.\n    for X in [[1, 'a', 3], [2, 'b', 4], [3, 'c', 5]], \\\n            csr_matrix([ [1, 'a', 3], [2, 'b', 4], [3, 'c', 5]]):\n        with pytest.raises(TypeError):\n            VarianceThreshold().fit(X)\n", "def test_threshold_rounding():\n    # Test behavior when threshold is non-integer.\n    for X in [data, csr_matrix(data)]:\n        threshold = np.pi / 4\n        sel = VarianceThreshold(threshold=threshold).fit_transform(X)\n        assert_equal((len(data), 1), sel.shape)\n        assert_equal(threshold, sel.threshold)\n\n", "def test_sparse_matrix_threshold():\n    # Test VarianceThreshold with custom variance on a sparse matrix.\n    X = csc_matrix(data)\n    sel = VarianceThreshold(threshold=.4).fit_transform(X)\n    assert_equal((len(data), 1), sel.shape)\n\n", "def test_threshold_with_nan():\n    # Test behavior with NaN values in the input data\n\n    for X in [\n        np.array([[1, 2, np.nan], [3, 4, np.nan]]),\n        csr_matrix([[1, 2, np.nan], [3, 4, np.nan]]),\n    ]:\n        sel = VarianceThreshold()\n        with pytest.raises(ValueError, match=\"Variance threshold .* with NaNs\"):\n            sel.fit(X)\n\n", "def test_sparse_threshold():\n    # Test VarianceThreshold with sparse input and custom threshold.\n    sparse_data = csr_matrix(data)\n    sel = VarianceThreshold(threshold=1.5).fit(sparse_data)\n    assert_array_equal([1, 2, 3, 4], sel.get_support(indices=True))\n"], "sample_412": ["    def test_urlize_trim_url_limit(self):\n        self.assertEqual(urlize(\"This is a long URL: http://www.example.com/path/to/verylongpage.html\", trim_url_limit=20),\n            'This is a long URL: <a href=\"http://www.example.com/path/to/verylongpage.html\">http://www.example.com/path/to/verylongpage.html</a>')\n        self.assertEqual(urlize(\"This is a long URL: http://www.example.com/path/to/verylongpage.html\", trim_url_limit=50),\n            'This is a long URL: http://www.example.com/path/to/verylongpage.html')\n        self.assertEqual(urlize(\"This is a long URL: http://www.example.com/path/to/verylongpage.html\", trim_url_limit=10),\n            'This is a long URL: <a href=\"http://www.example.com/path/to/verylongpage.html\">http://www.example.com/path/to/verylongpage.html</a>')\n\n\n", "    def test_urlize_with_trim_url_limit(self):\n        tests = (\n            (\n                \"This is a long URL: http://example.com/this-is-a-very-long-url-that-will-be-truncated\",\n                'This is a long URL: <a href=\"http://example.com/this-is-a-very-long-url-that-will-be-truncated\">http://example.com/this-is-a-very-long-url-that-will-be-truncated</a>',\n            ),\n            (\n                \"This is a long URL: https://example.com/this-is-a-very-long-url-that-will-be-truncated\",\n                'This is a long URL: <a href=\"https://example.com/this-is-a-very-long-url-that-will-be-truncated\">https://example.com/this-is-a-very-long-url-that-will-be-truncated</a>',\n            ),\n            (\"This is a long URL: http://example.com/this-is-a-very-long-url-that-will-be-truncated\",\n             'This is a long URL: <a href=\"http://example.com/this-is-a-very-long-url-that-will-be-truncated\">http://example.com/this-is-a-very-long-url-that-will-be-truncated...</a>',\n             trim_url_limit=20),\n            (\n                \"This is a short URL: http://example.com/this-is-a-short-url\",\n                'This is a short URL: <a href=\"http://example.com/this-is-a-short-url\">http://example.com/this-is-a-short-url</a>',\n            ),\n\n        )\n        for value, expected, limit in tests:\n            with self.subTest(value=value, expected=expected, limit=limit):\n                self.assertEqual(urlize(value, trim_url_limit=limit), expected)\n\n", "    def test_urlize_trim_url_limit(self):\n        tests = (\n            (\"This is a very very very long url: <a href='https://www.google.com/url?q=someurl&t=123456789012345678901234567890123456789012345678901234567890'>Extremely long URL</a>\", \"This is a very very very long url: <a href='https://www.google.com/url?q=someurl&t=12345678901234567890...'>Extremely long URL</a>\"),\n            (\"This is a very very very long url: <a href='https://www.google.com/url?q=someurl&t=123456789012345678901234567890123456789012345678901234567890'>Extremely long URL</a>\", \"This is a very very very long url: <a href='https://www.google.com/url?q=someurl&t=12345678901234567890...'>Extremely long URL</a>', 20),\n        )\n        for value, output in tests:\n            with self.subTest(value=value):\n                self.assertEqual(urlize(value, trim_url_limit=20), output)\n", "    def test_urlize_trim_url_limit(self):\n        url_limit = 10\n        self.assertEqual(\n            urlize(\"This is a long URL: http://example.com/this-is-a-really-long-url-that-needs-to-be-trimmed\", trim_url_limit=url_limit),\n            '<p>This is a long URL: http://examp...</p>',\n        )\n\n\n", "compilation error"], "sample_390": ["    def test_was_modified_since_zero_length_file(self):\n        \"\"\"\n        was_modified_since should handle zero length files correctly.\n        \"\"\"\n        with open(__file__, \"wb\") as f:\n            f.write(b\"\")\n        file_path = __file__\n        mtime = path.getmtime(file_path)\n        header = http_date(mtime)\n        self.assertFalse(was_modified_since(header, mtime, 0))\n", "    def test_was_modified_since_invalid_length(self):\n        mtime = 1343416141\n        header = http_date(mtime)\n        response = self.client.get(\n            \"/%s/file.txt\" % self.prefix,\n            HTTP_IF_MODIFIED_SINCE=header,\n            # Incorrect length header\n            HTTP_IF_MODIFIED_SINCE=header + \"; length=99999\",\n        )\n        self.assertEqual(response.status_code, 200)\n\n", "    def test_was_modified_since_missing_length(self):\n        mtime = 1\n        header = http_date(mtime) + '; length=123'\n        self.assertFalse(was_modified_since(header, mtime))\n", "    def test_was_modified_since_with_encoding(self):\n        \"\"\"\n        was_modified_since should ignore the Content-Encoding header. (#28122)\n        \"\"\"\n        with open(\"media/file.txt.gz\", \"rb\") as f:\n            content = f.read()\n\n        response = FileResponse(f, content_type=\"application/gzip\")\n        response.headers[\"Content-Encoding\"] = \"gzip\"\n\n        mtime = 1343416141\n        header = http_date(mtime)\n        self.assertFalse(was_modified_since(header, mtime, len(content)))\n", "    def test_was_modified_since_future_timestamp(self):\n        mtime = 1343416141\n        header = http_date(mtime + 1)\n        self.assertTrue(was_modified_since(header, mtime))\n"], "sample_138": ["    def test_manifest_cache_invalidated_on_file_change(self):\n        # Create initial static files.\n        file_contents = (\n            ('foo.png', 'foo'),\n            ('bar.css', 'url(\"foo.png\")\\nurl(\"xyz.png\")'),\n            ('xyz.png', 'xyz'),\n        )\n        for filename, content in file_contents:\n            with open(self._get_filename_path(filename), 'w') as f:\n                f.write(content)\n\n        with self.modify_settings(STATICFILES_DIRS={'append': self._temp_dir}):\n            finders.get_finder.cache_clear()\n            err = StringIO()\n            # First collectstatic run.\n            call_command('collectstatic', interactive=False, verbosity=0, stderr=err)\n\n            # Check manifest is generated\n            manifest_path = settings.STATIC_ROOT / storage.staticfiles_storage.manifest_name\n            self.assertTrue(manifest_path.exists())\n\n            # Change the contents of the png files.\n            for filename in ('foo.png', 'xyz.png'):\n                with open(self._get_filename_path(filename), 'w+b') as f:\n                    f.write(b\"new content of file to change its hash\")\n\n            # Second collectstatic run should update the manifest\n            call_command('collectstatic', interactive=False, verbosity=0, stderr=err)\n\n            # Check manifest is updated\n            new_manifest = json.loads(storage.staticfiles_storage.read_manifest())\n            self.assertIn(\n                'foo.57a5cb9ba68d.png',\n                new_manifest['paths']\n            )\n            self.assertIn(\n                'xyz.57a5cb9ba68d.png',\n                new_manifest['paths']\n            )\n", "    def test_manifest_hash(self):\n        # Write files to static dir and trigger collectstatic\n        with self.modify_settings(STATICFILES_DIRS={'append': self._temp_dir}):\n            finders.get_finder.cache_clear()\n            call_command('collectstatic', interactive=False, verbosity=0)\n\n        # Check if the manifest file exists and has been generated\n        manifest_path = storage.staticfiles_storage.path(storage.staticfiles_storage.manifest_name)\n        self.assertTrue(manifest_path.exists())\n        with open(manifest_path, 'r') as manifest_file:\n            manifest = json.load(manifest_file)\n\n        # Check if manifest contains expected keys and values\n        self.assertIn('version', manifest)\n        self.assertIn('files', manifest)\n        self.assertEqual(manifest['version'], storage.staticfiles_storage.manifest_version)\n        self.assertTrue('files' in manifest)\n        # Verify file and its hash are present in the manifest\n        self.assertIn(f'test/foo.png', manifest['files'])\n        self.assertIn(f'test/bar.css', manifest['files'])\n        self.assertIn(f'test/xyz.png', manifest['files'])\n", "    def test_custom_manifest_storage_paths_are_populated(self):\n        relpath = self.hashed_file_path('cached/styles.css')\n        self.assertEqual(relpath, 'cached/styles.deploy12345.css')\n        with storage.staticfiles_storage.open(relpath) as relfile:\n            content = relfile.read()\n            self.assertIn(b\"other.deploy12345.css\", content)\n", "    def test_collectstatic_with_relative_url_and_directories(self):\n        # Create initial static files.\n        file_contents = (\n            ('test/nested/images/foo.png', 'foo'),\n            ('test/nested/styles.css', 'url(\"../images/foo.png\")'),\n        )\n        for filename, content in file_contents:\n            with open(self._get_filename_path(filename), 'w') as f:\n                f.write(content)\n\n        with self.modify_settings(STATICFILES_DIRS={'append': self._temp_dir}):\n            finders.get_finder.cache_clear()\n            err = StringIO()\n            # First collectstatic run.\n            call_command('collectstatic', interactive=False, verbosity=0, stderr=err)\n            relpath = self.hashed_file_path('test/nested/styles.css')\n            with storage.staticfiles_storage.open(relpath) as relfile:\n                content = relfile.read()\n                self.assertIn(b'../images/foo.acbd18db4cc2.png', content)\n", "    def test_manifest_version_update(self):\n        # create initial manifest\n        manifest_content = {\n            'version': '1',\n            'paths': {},\n        }\n        manifest_path = self.staticfiles_storage.path(self.staticfiles_storage.manifest_name)\n        with open(manifest_path, 'w') as manifest_file:\n            json.dump(manifest_content, manifest_file)\n        \n        # trigger collectstatic and update manifest\n        finders.get_finder.cache_clear()\n        err = StringIO()\n        call_command(\n            'collectstatic', interactive=False, verbosity=0, stderr=err\n        )\n        \n        # verify manifest version was updated\n        with open(manifest_path, 'r') as manifest_file:\n            updated_manifest = json.load(manifest_file)\n        self.assertNotEqual(updated_manifest['version'], '1')\n\n\n"], "sample_673": ["    def test_is_setup_py_empty(tmpdir):\n        setup_py = tmpdir.join(\"setup.py\")\n        setup_py.write('')\n        assert _is_setup_py(setup_py)\n", "    def test_is_setup_py_imports_none(tmpdir, mod):\n        setup_py = tmpdir.join(\"setup.py\")\n        setup_py.write(f\"import {mod}; setup(name='foo')\")\n        assert _is_setup_py(setup_py)\n", "    def test_is_setup_py_missing_encoding_declaration(tmpdir, encoding):\n        setup_py = tmpdir.join(\"setup.py\")\n        setup_py.write('from setuptools import setup; setup(name=\"foo\")')\n        assert _is_setup_py(setup_py)\n", "def test_is_setup_py_with_shebang(tmpdir):\n    setup_py = tmpdir.join(\"setup.py\")\n    setup_py.write(\"#!/usr/bin/env python\\nfrom setuptools import setup; setup(name='foo')\")\n    assert _is_setup_py(setup_py)\n", "    def test_is_setup_py_not_a_setup_py(tmpdir, mod):\n        not_setup_py = tmpdir.join(\"not_setup.py\")\n        not_setup_py.write('from {} import setup; setup(name=\"foo\")'.format(mod))\n        assert not _is_setup_py(not_setup_py)\n"], "sample_176": ["    def test_rename_field_after_remove_field(self):\n        \"\"\"\n        Renaming a field after removing a field from a base model should\n        work correctly.\n        \"\"\"\n        before = [\n            ModelState('app', 'readable', [\n                ('id', models.AutoField(primary_key=True)),\n                ('title', models.CharField(max_length=200)),\n            ]),\n        ]\n        after = [\n            ModelState('app', 'readable', [\n                ('id', models.AutoField(primary_key=True)),\n            ]),\n            ModelState('app', 'book', [\n                ('new_title', models.CharField(max_length=200)),\n            ], bases=('app.readable',)),\n        ]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['RemoveField', 'RenameField', 'CreateModel'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='title', model_name='readable')\n        self.assertOperationAttributes(changes, 'app', 0, 1, name='new_title')\n        self.assertOperationAttributes(changes, 'app', 0, 2, name='book')\n\n\n", "    def test_remove_model_with_field_present_in_inherited_model(self):\n        \"\"\"\n        Removing a model that has a field present in an inherited model\n        should correctly handle the dependencies.\n        \"\"\"\n        before = [\n            ModelState('app', 'readable', [\n                ('id', models.AutoField(primary_key=True)),\n                ('title', models.CharField(max_length=200)),\n            ]),\n            ModelState('app', 'book', [\n                ('id', models.AutoField(primary_key=True)),\n                ('author', models.ForeignKey('app.readable', models.CASCADE)),\n            ], bases=('app.readable',)),\n        ]\n        after = [\n            ModelState('app', 'readable', [\n                ('id', models.AutoField(primary_key=True)),\n            ]),\n        ]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['CreateModel', 'DeleteModel'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='book')\n        self.assertOperationAttributes(changes, 'app', 0, 1, name='readable')\n\n\n\n", "    def test_rename_model_field_and_fk(self):\n        \"\"\"\n        Rename a model field and its corresponding FK in dependent models.\n        \"\"\"\n        changes = self.get_changes(\n            [self.author_with_book, self.book],\n            [self.author_with_new_name_book, self.book]\n        )\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, ['RenameField', 'RenameField', 'RenameField', 'RenameField'])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, old_name=\"author_id\", new_name=\"author_fk\", field=\"testapp.book.author_id\", model_name='testapp.Author')\n\n", "    def test_m2m_with_null_from_related_fk(self):\n        \"\"\"\n        M2M fields pointing to models with a nullable foreign key should create\n        the migration for the M2M field first.\n        \"\"\"\n        class NullAuthor(models.Model):\n            id = models.AutoField(primary_key=True)\n\n        class AuthorWithNullForeign(models.Model):\n            id = models.AutoField(primary_key=True)\n            author = models.ForeignKey(NullAuthor, null=True)\n\n        class BookWithNullAuthor(models.Model):\n            id = models.AutoField(primary_key=True)\n            authors = models.ManyToManyField(AuthorWithNullForeign)\n\n        changes = self.get_changes(\n            [], [BookWithNullAuthor]\n        )\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, ['CreateModel', 'CreateModel', 'CreateModel'])\n        self.assertMigrationDependencies(changes, 'testapp', 0, [('testapp', '0001_initial')])\n        self.assertOperationAttributes(changes, 'testapp', 0, 2, name='authors', model_name='BookWithNullAuthor')\n", "    def test_add_model_with_field_changed_from_base_model(self):\n        \"\"\"\n        Adding a new inherited model with a field rename from the base model\n        handles the rename correctly.\n        \"\"\"\n        before = [\n            ModelState('app', 'readable', [\n                ('id', models.AutoField(primary_key=True)),\n                ('title', models.CharField(max_length=200)),\n            ]),\n        ]\n        after = [\n            ModelState('app', 'readable', [\n                ('id', models.AutoField(primary_key=True)),\n                ('name', models.CharField(max_length=200)),\n            ]),\n            ModelState('app', 'book', [\n                ('id', models.AutoField(primary_key=True)),\n                ('title', models.CharField(max_length=200)),\n            ], bases=('app.readable',)),\n        ]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['RemoveField', 'RenameField', 'CreateModel'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='title', model_name='readable')\n        self.assertOperationAttributes(changes, 'app', 0, 1, name='title', model_name='readable', to='name')\n        self.assertOperationAttributes(changes, 'app', 0, 2, name='book') \n"], "sample_952": ["def test_signature_from_function():\n        pass\n    sig = inspect.signature(func)\n    assert list(sig.parameters.keys()) == ['a', 'b', 'args', 'kwargs']\n", "def test_getargspec_signature():\n    sig = inspect.getargspec_signature(inspect.signature(os.path.join))\n    assert sig.parameters == {'path': inspect.Parameter('path', inspect.Parameter.POSITIONAL_OR_KEYWORD)}\n", "        def static_method(self):\n            pass\n", "    def test_getargspec_with_keyword_only_args():\n            pass\n\n        argspec = inspect.getargspec(func)\n        assert argspec.args == ['a']\n        assert argspec.keywords == ['b', 'c']\n        assert argspec.defaults == (None, 1)\n        assert argspec.varargs is None\n        assert argspec.varkw is None\n\n", "def test_signature_from_callable_partial():\n        pass\n\n    func_partial = functools.partial(func, 1)\n    sig = inspect.signature_from_callable(func_partial)\n    assert len(sig.parameters) == 2\n    assert sig.parameters['b'].name == 'b'\n    assert sig.parameters['c'].name == 'c'\n\n\n\n"], "sample_944": ["def test_stringify_complex_type_hints(annotation):\n    assert isinstance(stringify(annotation), str)\n", "    def test_stringify_complex_type_hints(annotation: Any):\n        assert stringify(annotation)  == stringify(annotation)\n", "    def test_stringify_type_hints_typing_Callable_with_args_and_return_type():\n\n        from typing import Callable\n\n        assert stringify(Callable[[str, int], str]) == \"Callable[[str, int], str]\"\n        assert stringify(Callable[..., str]) == \"Callable[[...], str]\"\n", "def test_stringify_type_hints_GenericMeta():\n    from typing import GenericMeta\n    \n    class MyClass(GenericMeta):\n        __origin__ = list\n        __args__ = [int]\n    \n    assert stringify(MyClass) == \"tests.test_util_typing.MyClass[int]\"\n", "    def test_stringify_type_hints_GenericMeta():\n        from typing import GenericMeta\n        class MyGenericMeta(GenericMeta):\n            pass\n        class MyClass(metaclass=MyGenericMeta):\n            pass\n\n        assert stringify(MyClass) == \"tests.test_util_typing.MyClass\"\n\n\n"], "sample_1155": ["compilation error", "def test_zero_divisors():\n    from sympy.polys.domains import ZZ\n    \n    domains, elements = construct_domain([0*x, x])\n    assert domains is ZZ\n    assert elements == [ZZ(0), ZZ(x)] \n", "def test_issue_14877():\n    from sympy.polys.polyoptions import build_options\n    result = construct_domain([4**(1/3)], extension=True)\n    assert result[0] == QQ.algebraic_field(4**(1/3))\n    result = construct_domain([4**(1/3)], extension=True, composite=False)\n    assert result[0] == EX\n", "compilation error", "def test_issue_15727():\n    from sympy.polys.domains.domain import Domain\n    from sympy.polys.polyoptions import build_options\n\n        _, result = construct_domain(expr, extension=True, field=True)\n        return result[0], result[1]\n\n    # Test cases for issue 15727\n    cases = [\n        (2*x**2 + 3*x + 1, \n         Domain, QQ, [QQ(2), QQ(3), QQ(1)]),\n        (x**2 + 1, \n         Domain, QQ, [QQ(1), QQ(1)]),\n        (sqrt(2)*x**2 + 3*sqrt(2)*x + 1, \n         Domain, QQ.algebraic_field(sqrt(2)), [QQ.algebraic_field(sqrt(2))(sqrt(2)), QQ.algebraic_field(sqrt(2))(3*sqrt(2)), QQ.algebraic_field(sqrt(2))(1)]),\n    ]\n\n    for expr, expected_type, _, expected_coeffs in cases:\n        domain, coeffs = get_domain_and_coeffs(expr)\n        assert isinstance(domain, expected_type)\n        assert coeffs == expected_coeffs\n\n    # Test case with custom options\n    options = build_options(extension=True, field=True)\n    domain, coeffs = construct_domain(x**2 + 1, **options)\n    assert isinstance(domain, QQ.algebraic_field)\n    assert coeffs == [QQ(1), QQ(1)]\n\n\n\n"], "sample_366": ["    def test_invalid_iso_8601(self):\n        with self.assertRaises(ValueError):\n            parse_duration('P4Y2M')\n\n\n\n", "    def test_invalid_time_formats(self):\n        invalid_inputs = (\n            '09:15:90', '09:15:00Z',  '09:15+04:00', '09:15:00-', '09:15.15', '09:15.150000',\n            '19:15:00', 'A:15:00', '09:A:00', '09:15:A0', '09:15:15A', '09:15.1A', '09:15.150A',\n        )\n        for input_string in invalid_inputs:\n            with self.subTest(input_string=input_string):\n                self.assertIsNone(parse_time(input_string))\n\n", "    def test_timezone_offset(self):\n        test_values = (\n            ('2012-04-23T10:20:30+02:30', datetime(2012, 4, 23, 10, 20, 30, 0, get_fixed_timezone(150))),\n            ('2012-04-23T10:20:30-02:30', datetime(2012, 4, 23, 10, 20, 30, 0, get_fixed_timezone(-150))),\n            ('2012-04-23T10:20:30+02', datetime(2012, 4, 23, 10, 20, 30, 0, get_fixed_timezone(120))),\n            ('2012-04-23T10:20:30-02', datetime(2012, 4, 23, 10, 20, 30, 0, get_fixed_timezone(-120))),\n            ('2012-04-23T10:20:30Z', datetime(2012, 4, 23, 10, 20, 30, 0, get_fixed_timezone(0))),\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_datetime(source), expected)\n\n", "    def test_invalid_iso_8601(self):\n        test_values = (\n            ('P3W2', None),\n            ('P4Y2D', None),\n            ('P4Y1M2D', None),\n            ('PT5H2M', None),\n            ('PT5.5S', None),\n            ('-PT5.5S', None),\n            ('P.5D', None),\n            ('P4Y-1M', None),\n            ('PT100H', None),\n            ('PT100M', None),\n            ('PT1000S', None),\n            ('P4Y1M2D1H', None),\n            ('P4Y1M2D1H1M', None),\n            ('P4Y1M2D1H1M1S', None),\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertIsNone(parse_duration(source))\n\n\n", "    def test_invalid_iso_8601(self):\n        test_values = (\n            ('P4Y1M', None),  # Invalid combination\n            ('P4Y1M2D', None),  # Too many units\n            ('P4Y1M2D1S', None),  # Too many units\n            ('P4Y1M2D1S1', None),  # Too many units\n            ('P4YZ', None),  # Invalid character in duration\n            ('P4Y1M2D1S1.1', None),  # Invalid fractional part separator\n        )\n        for source in test_values:\n            with self.subTest(source=source):\n                self.assertIsNone(parse_duration(source))\n"], "sample_360": ["    def test_cache_headers(self):\n        request = self.factory.get(self.path)\n        template = engines['django'].from_string(\"This is a test\")\n        response = TemplateResponse(request, template)\n        cache_middleware = CacheMiddleware(empty_response)\n        cache_middleware.process_response(request, response)\n\n        self.assertIn('Cache-Control', response)\n        self.assertIn('Vary', response)\n        self.assertIn('ETag', response)\n\n\n", "    def test_cache_timeout(self):\n        \"\"\"\n        Tests the cache timeout functionality.\n        \"\"\"\n        test_caches = CacheHandler({\n            'cache_1': {\n                'BACKEND': 'django.core.cache.backends.dummy.DummyCache',\n                'TIMEOUT': 10,\n            },\n        })\n        cache_1 = test_caches['cache_1']\n\n        # Set a key-value pair with an initial timeout of 10 seconds.\n        cache_1.set('test_key', 'test_value', timeout=10)\n\n        # Check if the value is present.\n        self.assertEqual(cache_1.get('test_key'), 'test_value')\n\n        # Wait for the timeout to expire.\n        time.sleep(11)\n\n        # Check if the value is no longer present.\n        self.assertIsNone(cache_1.get('test_key'))\n", "    def test_cache_key_with_special_characters(self):\n        request = self.factory.get(self.path, {'key': 'value with %&*()<>:;, .?'})\n        template = engines['django'].from_string(\"This is a test\")\n        response = TemplateResponse(HttpRequest(), template)\n        learn_cache_key(request, response)\n\n        self.assertIn('value with %&*()<>:;, .?', get_cache_key(request))\n", "    def test_cache_headers_with_content_length_and_vary(self):\n        response = HttpResponse(\"Hello World!\")\n        response.headers['Content-Length'] = len(response.content)\n        response.headers['Vary'] = 'Accept-Encoding'\n        response.status_code = 200\n\n        cache_headers(response)\n\n        self.assertIn('Content-Length', response.headers)\n        self.assertIn('Vary', response.headers)\n        self.assertEqual(response.headers['Content-Length'], str(len(response.content)))\n", "    def test_backend_creation(self):\n        \"\"\"\n        Ensures that caches are created with the correct backend objects,\n        and handles potential errors gracefully.\n        \"\"\"\n        test_caches = CacheHandler({\n            'cache_1': {\n                'BACKEND': 'django.core.cache.backends.locmem.LocMemCache',\n            },\n            'cache_2': {\n                'BACKEND': 'django.core.cache.backends.dummy.DummyCache',\n            },\n        })\n        # Assert that cache_1 uses the LocMemCache backend.\n        self.assertIsInstance(test_caches['cache_1'], LocMemCache)\n        # Assert that cache_2 uses the DummyCache backend.\n        self.assertIsInstance(test_caches['cache_2'], DummyCache)\n\n\n"], "sample_653": ["def test_log_level_override_from_file(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n            logger = logging.getLogger(__name__)\n            logger.info(\"logging message\")\n    \"\"\"\n    )\n\n    log_file = testdir.tmpdir.join(\"pytest.ini\").strpath\n\n    with open(log_file, \"w\") as f:\n        f.write(\n            \"\"\"\n            [pytest]\n            log_file = {}\n            log_file_level = DEBUG\n            \"\"\".format(\n                log_file\n            )\n        )\n\n    result = testdir.runpytest()\n\n    result.stdout.fnmatch_lines([\"test_log_level_override.py PASSED\"])\n\n\n", "compilation error", "def test_log_handler_level_overrides_global_level(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n        logging.basicConfig(level=logging.WARNING)  # Set global logging level to WARNING\n\n            logging.getLogger('test_log_handler').info(\"This message should be logged\")\n            logging.getLogger('test_log_handler').warning(\"This message should be logged\")\n    \"\"\"\n    )\n\n\n    result = testdir.runpytest(\n        \"--log-cli-level=DEBUG\",\n        \"--log-file=test_log_handler_level_overrides_global_level.log\"\n    )\n\n    result.stdout.fnmatch_lines(\n        [\n            \"*test_log_handler_level_overrides_global_level.py::test_log_level_override*\",\n            \"PASSED\"\n        ]\n    )\n    assert result.ret == 0 \n\n\n", "def test_log_in_event_hook(testdir):\n    log_file = testdir.tmpdir.join(\"pytest.log\").strpath\n\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_file={}\n        log_file_level = INFO\n        log_cli=true\n        \"\"\".format(\n            log_file\n        )\n    )\n    testdir.makeconftest(\n        \"\"\"\n        import logging\n        logger = logging.getLogger(__name__)\n\n            logger.info(f'internal event: {event.name}')\n    \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n                assert True\n        \"\"\"\n    )\n    testdir.runpytest()\n    with open(log_file) as rfh:\n        contents = rfh.read()\n        assert contents.count(\"internal event:\") >= 6 \n", "    def test_log_cli_level_override_config(testdir):\n        testdir.makeini(\n            \"\"\"\n            [pytest]\n            log_cli=true\n            log_cli_level=WARNING\n            \"\"\"\n        )\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n            import logging\n                plugin = request.config.pluginmanager.getplugin('logging-plugin')\n                assert plugin.log_cli_handler.level == logging.WARNING\n                logging.getLogger('catchlog').debug(\"This log message won't be shown\")\n                logging.getLogger('catchlog').warning(\"This log message will be shown\")\n            \"\"\"\n        )\n\n        result = testdir.runpytest()\n\n        result.stdout.fnmatch_lines(\n            [\n                \"*test_log_cli_level_override_config.py::test_log_cli_level_override  \",\n                \"*-- live log call --*\",\n                \"*WARNING*This log message will be shown*\",\n            ]\n        )\n        assert \"This log message won't be shown\" not in result.stdout.str()\n\n        # make sure that that we get a '0' exit code for the testsuite\n        assert result.ret == 0\n"], "sample_1101": ["def test_schur_partition_edge_cases():\n    assert schur_partition(1) == [[1]]\n    assert schur_partition(2) == [[1, 2]]\n    assert schur_partition(3) == [[1, 2, 3]]\n    assert schur_partition(4) == [[1, 4], [2, 3]]\n    assert schur_partition(5) == [[3, 2], [5], [1, 4]]\n    assert schur_partition(6) == [[3, 2], [6, 5, 8], [1, 4, 7]]\n    assert schur_partition(7) == [[3, 2], [6, 5], [7, 1, 4], [8]]\n    assert schur_partition(8) == [[3, 2], [6, 5, 8], [1, 4, 7]]\n", "def test_schur_partition_large_n():\n    \"\"\"\n    Test schur_partition with larger values of n (n > 10) to ensure\n    it works as expected and generates a valid partition.\n    \"\"\"\n    for n in range(11, 21):\n        result = schur_partition(n)\n        # Check if each sublist is sum-free\n        for subset in result:\n            _sum_free_test(subset)\n        # Check if all elements from 1 to n are present\n        assert set(sum(subset for subset in result)) == set(range(1, n + 1)) \n", "compilation error", "def test_schur_partition_large_n():\n    for n in [45, 50, 55, 60]:\n        partition = schur_partition(n)\n        assert len(partition) >= _schur_subsets_number(n)\n        for subset in partition:\n            _sum_free_test(subset)\n            assert all(1 <= num <= n for num in subset)\n\n\n\n", "compilation error"], "sample_1025": ["def test_issue_18140():\n    p = PythonCodePrinter()\n    expr = Piecewise((x, Eq(x, 2)), (y, True))\n    assert p.doprint(expr) == '((x) if (x == 2) else y)' \n", "def test_piecewise_with_default():\n    p = PythonCodePrinter()\n    expr = Piecewise((1, Eq(x, 0)), (2, x > 6), default=x**2)\n    assert p.doprint(expr) == '((1) if (x == 0) else (2) if (x > 6) else x**2)'\n", "def test_dot_product_with_shape_change():\n    from sympy.matrices import Matrix\n    x = Matrix([[1, 2], [3, 4]])\n    y = Matrix([[5, 6]])\n    p = NumPyPrinter()\n    assert p.doprint(x.dot(y)) == 'numpy.dot(x, y)'\n", "def test_issue_16563():\n    p = PythonCodePrinter()\n    expr = (x + 1) * y\n    assert p.doprint(expr) == '((x + 1) * y)'\n\n\n", "def test_print_MatrixBase():\n    from sympy.matrices import Matrix\n    p = PythonCodePrinter()\n    A = Matrix([[1, 2], [3, 4]])\n    assert p.doprint(A) == '[ [1, 2], [3, 4] ]'\n"], "sample_666": ["compilation error", "def test_capture_with_custom_encoding():\n    testdir.makepyfile(\n        \"\"\"\n        import sys\n            sys.stdout.write(\"hello\\u0648\".encode(\"utf-8\"))\n            print(\"world\")\n    \"\"\"\n    )\n    with testdir.tmp_path(\"file.txt\") as filepath:\n        result = testdir.runpytest(\n            str(p),\n            \"--capture=no\",\n            \"--capture-file=file.txt\",\n            \"--capture-encoding=utf-8\",\n        )\n        result.stdout.fnmatch_lines(\n            [\n                \"*test_custom_encoding*\",\"*hello*\",\"\",\"*world*\",\n            ]\n        )\n        assert result.ret == 0\n        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n            assert f.read() == \"hello\\u0648\"\n", "def test_capture_with_non_standard_streams(testdir):\n    from _pytest.capture import _py36_windowsconsoleio_workaround\n\n    class MyStream:\n            pass\n\n    class FakeProcess:\n        std_out = MyStream()\n        std_err = MyStream()\n\n    # Capture outputs from a fake process.\n    fake_process = FakeProcess()\n    capsys = capture.MultiCapture(\n        out=fake_process.std_out, err=fake_process.std_err\n    )\n    capsys.start_capturing()\n    print(\"Hello, world!\")  # Output to stdout\n    sys.stderr.write(\"This is an error!\\n\")  # Output to stderr\n    capsys.stop_capturing()\n\n    captured_out, captured_err = capsys.readouterr()\n    assert captured_out == \"\"\n    assert captured_err == \"This is an error!\\n\"\n\n    # Test that the _py36_windowsconsoleio_workaround function handles non-standard streams properly.\n    _py36_windowsconsoleio_workaround(fake_process.std_out)\n    _py36_windowsconsoleio_workaround(fake_process.std_err)\n", "def test_capture_errors(testdir):\n    testdir.makepyfile(\n        \"\"\"\n            raise Exception(\"test error\")\n        \"\"\"\n    )\n    result = testdir.runpytest('--capture=no')\n    result.stdout.fnmatch_lines(['*test_this*'])\n    result.stderr.fnmatch_lines(\n        [\n            \"*ERROR:* test error*\",\n        ]\n    )\n    # check that capture=no captures nothing\n    assert len(result.stdout.str()) == 0\n    assert len(result.stderr.str()) > 0\n\n\n\n", "    def test_logging_in_teardown(testdir):\n        p = testdir.makepyfile(\n            \"\"\"\n            import logging\n            \n            logging.basicConfig(level=logging.WARNING)\n\n                logging.warning(\"during test\")\n                assert False\n            \n                logging.warning(\"during teardown\")\n            \"\"\"\n        )\n        result = testdir.runpytest_subprocess(p)\n        assert result.ret == ExitCode.TESTS_FAILED\n        result.stdout.fnmatch_lines(\n            [\n                \"*test_*.py F*\",\n                \"====* FAILURES *====\",\n                \"____*____\",\n                \"*--- Captured log call*\",\n                \"WARNING * during test\",\n                \"*1 failed*\",\n                \"*--- Captured log call*\",\n                \"WARNING * during teardown\",\n                \"*1 failed*\",\n            ]\n        )\n"], "sample_967": ["def test_mathjax_escaped_characters(app, status, warning):\n    app.builder.build_all()\n\n    content = (app.outdir / 'index.html').read_text()\n    html = (r'<div class=\"math notranslate nohighlight\">\\s*'\n            r'\\\\\\[ \\\\begin\\{align\\}\\\\begin\\{aligned\\}'\n            r'a &= \\\\(b\\\\) \\\\\\\\\\\\'\n            r'c &= \\\\{b\\}\\\\end\\{aligned\\}\\\\end\\{align\\}'\n            r'\\\\\\]</div>')\n    assert re.search(html, content, re.S)\n", "def test_mathjax_inline_and_display_custom(app, status, warning):\n    app.builder.build_all()\n\n    content = (app.outdir / 'index.html').read_text()\n    html = r'<div class=\"math notranslate nohighlight\">\\s*\\\\\\(a\\&amp;b\\\\\\)\\s*</div>'\n    assert re.search(html, content, re.S)\n\n    html = r'<div class=\"math notranslate nohighlight\">\\s*\\\\\\\\[a^2+b^2=c^2\\\\\\\\]\\s*</div>'\n    assert re.search(html, content, re.S)\n", "def test_mathjax_inline_and_display(app, status, warning):\n    app.builder.build_all()\n\n    content = (app.outdir / 'index.html').read_text()\n    html_inline = (r'<p>Inline math: <span class=\"math notranslate nohighlight\">\\((a + b)\\)</span></p>')\n    html_display = (r'<p>Display math: <div class=\"math notranslate nohighlight\">\\\\[a^2 + b^2 = c^2\\\\]</div> </p>')\n\n    assert html_inline in content\n    assert html_display in content\n", "def test_custom_inline_chars(app, status, warning):\n    app.builder.build_all()\n\n    content = (app.outdir / 'index.html').read_text()\n    html = r'<div class=\"math notranslate nohighlight\">\\s*' \\\n           r'\\u2018This is inline math\\u2019\\s*</div>'\n    assert re.search(html, content, re.S)\n", "def test_mathjax_inline_and_display_custom(app, status, warning):\n    app.builder.build_all()\n\n    inline_content = (app.outdir / 'index.html').read_text()\n    assert r'<span class=\"math notranslate nohighlight\">(\\\\(a + b\\\\))</span>' in inline_content\n    display_content = (app.outdir / 'index.html').read_text()\n    assert r'<div class=\"math notranslate nohighlight\">\\n[a^2 + b^2 = c^2]</div>' in display_content \n"], "sample_1156": ["compilation error", "def test_cosh_neg_inf():\n    assert cosh(-oo) == oo\n    assert cosh(oo) == oo\n\n\n", "def test_special_values():\n    assert sinh(0) == 0\n    assert cosh(0) == 1\n    assert tanh(0) == 0\n    assert sech(0) == 1\n    assert csch(0) == zoo\n    assert coth(0) == zoo\n\n    assert sinh(oo) == oo\n    assert cosh(oo) == oo\n    assert tanh(oo) == 1\n    assert sech(oo) == 0\n    assert csch(oo) == 0\n    assert coth(oo) == 1\n\n    assert sinh(-oo) == -oo\n    assert cosh(-oo) == oo\n    assert tanh(-oo) == -1\n    assert sech(-oo) == 0\n    assert csch(-oo) == 0\n    assert coth(-oo) == -1\n\n    assert sinh(I*pi/2) == I\n    assert cosh(I*pi/2) == 1\n    assert tanh(I*pi/2) == I\n    assert sech(I*pi/2) == 1/1\n    assert csch(I*pi/2) == 1/I\n    assert coth(I*pi/2) == I\n\n    assert sinh(I*pi) == 0\n    assert cosh(I*pi) == 2\n    assert tanh(I*pi) == 0\n    assert sech(I*pi) == 1/2\n    assert csch(I*pi) == oo\n    assert coth(I*pi) == oo\n\n\n", "    def test_issue_8365():\n        x = Symbol('x')\n        assert simplify(acosh(cosh(x))) == x\n        assert simplify(asinh(sinh(x))) == x\n", "    def test_atanh_series_limits():\n        x = Symbol('x')\n        assert atanh(x).series(x, 0, 10).limit(x, 1).evalf() == 0.6931471805599453\n        assert atanh(x).series(x, 0, 10).limit(x, -1).evalf() == -0.6931471805599453\n\n\n\n"], "sample_1141": ["def test_transpose_properties():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    C = MatrixSymbol('C', 2, 2)\n    assert (A.T).T == A\n    assert (A + B).T == A.T + B.T\n    assert (A * B).T == B.T * A.T\n    assert ((A * B).T).T == A * B\n    assert (A.T * B).T == B.T * A\n    assert (A**2).T == A.T**2\n    assert (A**(-1).T == (A.T)**(-1)\n    assert (A.I).T == (A.T).I\n\n\n\n", "def test_matrix_substitution():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    C = MatrixSymbol('C', 2, 2)\n    D = MatrixSymbol('D', 2, 2)\n\n    expr = A*B + C*D\n    expected = A.subs({B: C}) * C + C.subs({D: A}) * A\n    assert expr.subs({B: C, D: A}) == expected\n    assert expr.subs({A: C, B: D}) == expected.subs({A: D, B: C})\n\n\n\n", "def test_expand_matrix_expressions():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    expr = (A + B)**2\n    expr_expanded = A**2 + 2*A*B + B**2\n    assert expr != expr_expanded\n    assert expand(expr) == expr_expanded\n    assert factor(expr_expanded) == expr\n\n\n\n", "def test_zero_matrix_properties():\n    Z = ZeroMatrix(3, 2)\n    assert Z.det() == 0\n    assert Z.rank() == 0\n    assert Z.is_square == False\n    assert Z.is_zero == True\n    assert Z.shape == (3, 2)\n    assert (Z + Z).equals(Z)\n    assert (Z * 2).equals(Z)\n    assert (Z * 0).equals(Z)\n    assert Z.transpose() == Z.T\n    assert Z.as_explicit() == [[0, 0], [0, 0], [0, 0]]\n    assert Z.as_mutable() == Matrix([[0, 0], [0, 0], [0, 0]])\n\n\n\n", "def test_matrix_multiplication_with_scalar_and_matrix():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    c = symbols('c')\n    assert (c * A) * B == c * (A * B)\n    assert A * (c * B) == (c * A) * B\n    assert (A + B) * c == c * A + c * B\n    assert c * (A + B) == c * A + c * B\n\n\n"], "sample_338": ["    def test_operation_with_name(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\n                    name='Person', fields=[]\n                ),\n                migrations.RenameModel('Person', 'Customer'),\n            ]\n\n        migration = Migration('some_migration', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'rename_person_to_customer')\n", "    def test_operation_with_no_suggested_name_after_initial(self):\n        class Migration(migrations.Migration):\n            initial = True\n            operations = [\n                migrations.CreateModel('Person', fields=[]),\n                migrations.RunSQL('SELECT 1 FROM person;'),\n            ]\n\n        migration = Migration('0001_initial', 'test_app')\n        suggest_name = migration.suggest_name()\n        self.assertEqual(suggest_name, 'initial')\n\n\n\n", "    def test_operation_with_no_suggested_name_order(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel('Person', fields=[]),\n                migrations.RunSQL('SELECT 1 FROM person;'),\n                migrations.RemoveField('Person', 'id'),\n            ]\n\n        migration = Migration('some_migration', 'test_app')\n        suggest_name = migration.suggest_name()\n        self.assertIs(suggest_name.startswith('auto_'), True)\n", "    def test_field_type_name_squash(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.AddField('Person', 'age', models.IntegerField(default=0)),\n                migrations.AlterField('Person', 'name', models.CharField(max_length=20)),\n            ]\n\n        migration = Migration('some_migration', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'person_age_add_name_alter')\n", "    def test_operation_with_extra_args(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\n                    name='Person',\n                    fields=[],\n                    options={'extra_kwargs': 'extra_stuff'}\n                ),\n            ]\n\n        migration = Migration('0001_initial', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'person')\n\n"], "sample_158": ["    def test_circular_foreign_object(self):\n        class A(models.Model):\n            b = models.ForeignObject(\n                'B', on_delete=models.SET_NULL, from_fields=('id',), to_fields=('id',), related_name='a'\n            )\n\n        class B(models.Model):\n            a = models.ForeignObject(\n                'A', on_delete=models.SET_NULL, from_fields=('id',), to_fields=('id',), related_name='b'\n            )\n\n        with self.assertRaises(CircularDependencyError):\n            A.check()\n", "    def test_invalid_field_types(self):\n        class Fan(models.Model):\n            pass\n\n        class Event(models.Model):\n            invitees = models.ManyToManyField(\n                Fan,\n                through='Invitation',\n                through_fields=('invalid_field_1', 'invalid_field_2'),\n            )\n\n        class Invitation(models.Model):\n            event = models.ForeignKey(Event, models.CASCADE)\n            invitee = models.IntegerField()  # Should be ForeignKey here\n            inviter = models.IntegerField()  # Should be ForeignKey here\n\n        field = Event._meta.get_field('invitees')\n        self.assertEqual(field.check(from_model=Event), [\n            Error(\n                \"The intermediary model 'invalid_models_tests.Invitation' has a field 'invalid_field_1' with an invalid type for a many-to-many relationship.\",\n                hint=\"Field 'invalid_field_1' should be a ForeignKey.\",\n                obj=field,\n                id='fields.E338',\n            ),\n            Error(\n                \"The intermediary model 'invalid_models_tests.Invitation' has a field 'invalid_field_2' with an invalid type for a many-to-many relationship.\",\n                hint=\"Field 'invalid_field_2' should be a ForeignKey.\",\n                obj=field,\n                id='fields.E338',\n            ),\n        ])\n\n\n\n", "    def test_overlapping_fields(self):\n        class Parent(models.Model):\n            a = models.PositiveIntegerField()\n            b = models.PositiveIntegerField()\n            c = models.PositiveIntegerField()\n\n            class Meta:\n                unique_together = (('a', 'b'),)\n\n        class Child(models.Model):\n            a = models.PositiveIntegerField()\n            b = models.PositiveIntegerField()\n            c = models.PositiveIntegerField()\n            parent = models.ForeignObject(\n                Parent,\n                on_delete=models.SET_NULL,\n                from_fields=('a', 'b'),\n                to_fields=('a', 'b'),\n                related_name='children',\n            )\n\n        field = Child._meta.get_field('parent')\n        self.assertEqual(field.check(from_model=Child), [\n            Error(\n                \"Fields 'a', 'b' on model 'Child' are used as foreign keys to \"\n                \"fields 'a', 'b' on model 'Parent', but these fields are \"\n                \"already unique together in 'Parent'.\",\n                hint=(\n                    'Either use unique_together fields that are not already '\n                    'used in a ForeignObject or adjust the \"\n                    'unique_together'\n                    \"fields in the parent model 'Parent'.\"\n                ),\n                obj=field,\n                id='fields.E314',\n            ),\n        ])\n", "    def test_non_overlapping_fields(self):\n        class Parent(models.Model):\n            a = models.PositiveIntegerField()\n            b = models.PositiveIntegerField()\n            c = models.PositiveIntegerField()\n\n            class Meta:\n                unique_together = (('a', 'b'),)\n\n        class Child(models.Model):\n            a = models.PositiveIntegerField()\n            d = models.PositiveIntegerField()\n            value = models.CharField(max_length=255)\n            parent = models.ForeignObject(\n                Parent,\n                on_delete=models.SET_NULL,\n                from_fields=('a',),\n                to_fields=('a',),\n                related_name='children',\n            )\n\n        field = Child._meta.get_field('parent')\n        self.assertEqual(field.check(from_model=Child), [])\n\n\n", "    def test_valid_m2m_through_fields(self):\n        class Fan(models.Model):\n            pass\n\n        class Event(models.Model):\n            invitees = models.ManyToManyField(\n                Fan,\n                through='Invitation',\n                through_fields=('event', 'invitee')\n            )\n\n        class Invitation(models.Model):\n            event = models.ForeignKey(Event, models.CASCADE)\n            invitee = models.ForeignKey(Fan, models.CASCADE)\n\n        field = Event._meta.get_field('invitees')\n        self.assertEqual(field.check(from_model=Event), [])\n\n"], "sample_502": ["def test_subplot_projection_interpolation():\n    fig = plt.figure()\n    ax1 = plt.subplot(111)\n    ax2 = plt.subplot(111, projection='polar')\n    assert ax1.name == 'rectilinear'\n    assert ax2.name == 'polar'\n\n    ax3 = plt.subplot(111, projection='polar', aspect='equal')\n\n    assert ax3.get_xaxis().get_scale() == 'linear'\n\n    plt.close(fig)\n\n\n", "def test_subplot_axis_order():\n    fig, axes = plt.subplots(2, 2)\n    \n    # Check that axis order is correct\n    assert axes[0, 0].get_position() == (0.1, 0.7, 0.3, 0.2)\n    assert axes[0, 1].get_position() == (0.4, 0.7, 0.3, 0.2)\n    assert axes[1, 0].get_position() == (0.1, 0.1, 0.3, 0.2)\n    assert axes[1, 1].get_position() == (0.4, 0.1, 0.3, 0.2)\n\n\n", "def test_subplot_no_figures():\n    fig = None\n    with pytest.raises(ValueError):\n        plt.subplot(111, fig=fig)\n\n\n", "def test_subplot_projection_overwrite():\n    fig, axes = plt.subplots(1, 2)\n\n    # Creating a new polar subplot in an existing figure\n    axes[0].set_projection('polar')\n    axes[1].set_projection('polar')\n\n    # Checking that the projection of the axes has been changed\n    assert axes[0].get_projection() == 'polar'\n    assert axes[1].get_projection() == 'polar'\n", "def test_set_facecolor_and_figure_facecolor():\n    fig, ax = plt.subplots()\n\n    # Test set_facecolor on axes\n    ax.set_facecolor('green')\n    assert ax.get_facecolor() == 'green'\n\n    # Test set_facecolor on figure\n    fig.set_facecolor('red')\n    assert fig.get_facecolor() == 'red'\n\n\n"], "sample_655": ["def test_write_unicode_to_fdcapture_file(tmpfile):\n    fd = tmpfile.fileno()\n    cap = capture.FDCapture(fd)\n    cap.start_capturing()\n    text = \"Hello, world!\"\n    os.write(fd, text.encode(\"utf-8\"))\n    s = cap.snap()\n    cap.done()\n    assert s == text\n\n\n\n", "def test_capture_with_encoding_errors(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import sys\n            sys.stdout.write(\"hello\\ufffdworld\".encode(\"utf-8\"))\n    \"\"\"\n    )\n    result = testdir.runpytest_subprocess(\"--capture=sys\")\n    result.stdout.fnmatch_lines(\n        \"*hello\\ufffdworld*\"\n    )    \n", "def test_capture_with_unicode_output(testdir):\n    testdir.makepyfile(\n        \"\"\"\n            print(u\"\u4f60\u597d\")\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    assert \"\u4f60\u597d\" in result.stdout.str()\n", "    def test_capture_with_encoding(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import sys\n\n                sys.stdout.write(b\"hello\".decode(\"utf-8\"))\n                captured = capsys.readouterr()\n                assert captured.out == \"hello\"\n            \"\"\"\n        )\n        result = testdir.runpytest()\n        assert result.ret == 0\n", "    def test_capture_with_non_string_encoding(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            encoding = \"cp1252\"\n            with open(\"example.txt\", \"w\", encoding=encoding) as f:\n                f.write(\"hola\")\n\n                with open(\"example.txt\", encoding=encoding) as f:\n                    content = f.read()\n                assert content == \"hola\"\n            \"\"\"\n        )\n        result = testdir.runpytest()\n        assert result.ret == 0\n\n\n\n"], "sample_993": ["def test_FreeGroupElm_is_cyclically_reduced():\n    assert (x**2*y**2*x**-1).is_cyclically_reduced() == False\n    assert (x**2*y*x**-1).is_cyclically_reduced() == True\n    assert (y**-1*x**2).is_cyclically_reduced() == True\n    assert (y**-1*x**2*y).is_cyclically_reduced() == False\n\n\n\n", "def test_FreeGroupElm_cyclic_properties():\n    w = x**5*y*x**2*y**-4*x\n    assert w.is_cyclically_reduced() == False\n    w_reduced = w.identity_cyclic_reduction()\n    assert w_reduced.is_cyclically_reduced() == True\n\n    w_red, r = w.cyclic_reduction(removed=True)\n    assert r != F.identity\n    assert w == r*w_red*r**-1 \n    \n    assert w_reduced.power_of(x*y) == True\n    assert w_reduced.power_of(x**2) == False \n\n", "def test_FreeGroupElm_cyclic_conjugate():\n    w = x**5*y*x**2*y**-4*x\n    assert w.cyclic_conjugates() == {w.cyclic_subword(i, i+len(w)) for i in range(len(w))}\n    assert w.is_cyclic_conjugate(w.cyclic_subword(0, len(w)))\n    assert not w.is_cyclic_conjugate(x**2*y**4*x**-3)\n\n\n\n\n", "def test_FreeGroupElm_cyclic_conjugate():\n    w = x**5 * y * x**2 * y**-4 * x\n    assert w in w.cyclic_conjugates()\n\n\n", "def test_FreeGroupElm_cyclic_conjugates():\n    w = x*y*x*y*x\n    conjugates = w.cyclic_conjugates()\n    assert len(conjugates) == 5\n    assert w in conjugates\n    assert x*y*x**2*y in conjugates\n    assert y*x**2*y*x in conjugates\n    assert x*y*x*y*x in conjugates\n    assert y*x*y*x**2 in conjugates\n\n    w = x**-1*y**5*x**-1\n    conjugates = w.cyclic_conjugates()\n    assert len(conjugates) == 5\n    assert w in conjugates\n    assert y*x**-2*y**5 in conjugates\n    assert y**-1*x*y**4*x**-1 in conjugates\n    assert x**-1*y**5*x in conjugates\n    assert y*x**-2*y**5 in conjugates\n\n\n\n"], "sample_512": ["def test_set_subplots_projection_after_axes_creation():\n    fig, axes = plt.subplots()\n    with pytest.raises(ValueError) as e:\n        axes.set_projection('polar')\n    assert str(e.value) == \"Cannot change projection after axes creation.\"\n    \n    fig, axes = plt.subplots(projection='polar')\n    with pytest.raises(ValueError) as e:\n        axes.set_projection('rectilinear')\n    assert str(e.value) == \"Cannot change projection after axes creation.\"\n\n", "def test_close_all_with_no_open_figures():\n    plt.close('all')\n    assert len(plt.get_fignums()) == 0\n\n", "def test_set_current_subplot_index():\n    fig, axes = plt.subplots(2, 2)\n    plt.set_current_subplot(1, 1)\n    assert plt.gca() is axes[0, 0]\n    plt.set_current_subplot(1, 2)\n    assert plt.gca() is axes[0, 1]\n    plt.set_current_subplot(2, 0)\n    assert plt.gca() is axes[1, 0]   \n    with pytest.raises(ValueError):\n        plt.set_current_subplot(3, 1) \n\n\n", "def test_subplots_ncols_nrows_kwarg_order():\n    fig, axes = plt.subplots(ncols=2, nrows=1)\n    assert axes.shape == (1, 2)\n    fig, axes = plt.subplots(nrows=1, ncols=2)\n    assert axes.shape == (1, 2)\n\n\n", "def test_subplots_with_kwarg_ordering():\n    fig = plt.figure()\n    # Test that keyword arguments to subplots() are processed as expected\n    # regardless of order.\n    ax1 = plt.subplots(nrows=2, ncols=1, figsize=(4, 6),\n                       subplot_kw={'projection': 'polar'})\n    ax2 = plt.subplots(nrows=2, ncols=1, figsize=(4, 6),\n                       subplot_kw={'projection': 'polar'})\n\n    assert ax1 is ax2\n\n\n\n\n"], "sample_425": ["    def test_serialize_custom_field(self):\n        class MyCustomField(models.Field):\n\n                return value * 2\n\n                return value * 3\n\n                return \"TextField\"\n\n                return \"TextField\"\n\n        field = models.CharField(max_length=255, default=10, db_column='my_column')\n        with self.subTest('custom field'):\n            self.assertSerializedResultEqual(\n                field,\n                (\"models.CharField(max_length=255, default=10, db_column='my_column')\", {\"from django.db import models\"})\n            )\n", "    def test_serialize_unknown_type(self):\n        with self.assertRaisesMessage(\n            ValueError, \"Cannot serialize: unknown type\"\n        ):\n            self.assertSerializedEqual(\n                \"unknown type\"\n            )\n\n", "    def test_serialize_custom_serializer_with_imports(self):\n        class CustomSerializer(BaseSerializer):\n                return \"custom_serialize(%r)\" % self.value, {\"import custom_module\"}\n\n        MigrationWriter.register_serializer(complex, CustomSerializer)\n        string, imports = MigrationWriter.serialize(complex(1, 2))\n        self.assertEqual(string, \"custom_serialize(1+2j)\")\n        self.assertEqual(imports, {\"import custom_module\"})\n        MigrationWriter.unregister_serializer(complex)\n\n\n\n", "    def test_serialize_custom_field(self):\n        class MyCustomField(models.CharField):\n                super().__init__(**kwargs)\n\n        field = MyCustomField(max_length=100)\n        string = MigrationWriter.serialize(field)[0]\n        self.assertEqual(\n            string, \"models.CharField(max_length=100)\"\n        )\n", "    def test_serialize_custom_field(self):\n        class MyCustomField(models.CharField):\n                super().__init__(*args, **kwargs)\n                self.custom_data = kwargs.pop(\"custom_data\", None)\n\n        class MyModel(models.Model):\n            my_field = MyCustomField(max_length=128, custom_data=\"test\")\n\n        string, imports = MigrationWriter.serialize(MyModel)\n        self.assertIn(\n            \"'MyCustomField'\", string\n        )  # Check if the custom field is referenced.\n        self.assertIn(\"migrations.test_writer.MyCustomField\", imports)  # Check if the custom field's import is added.\n\n\n\n"], "sample_1169": ["def test_issue_19661():\n    a = Symbol('0')\n    assert latex(Commutator(Bd(a)**2, B(a))\n                 ) == '- \\\\left[b_{0},{b^\\\\dagger_{0}}^{2}\\\\right]'\n", "def test_internal_external_VT2T2_AT_with_permutations():\n    ii, jj = symbols('i j', below_fermi=True)\n    aa, bb = symbols('a b', above_fermi=True)\n    k, l = symbols('k l', below_fermi=True, cls=Dummy)\n    c, d = symbols('c d', above_fermi=True, cls=Dummy)\n\n    exprs = [\n        atv(k, l, c, d)*att(aa, c, ii, k)*att(bb, d, jj, l),\n        atv(l, k, c, d)*att(aa, c, ii, l)*att(bb, d, jj, k),\n        atv(k, l, d, c)*att(aa, d, ii, k)*att(bb, c, jj, l),\n        atv(l, k, d, c)*att(aa, d, ii, l)*att(bb, c, jj, k),\n    ]\n    for permut in exprs[1:]:\n        assert substitute_dummies(exprs[0]) == substitute_dummies(permut)\n\n    # Now test the same with permutated indices in the AntiSym\n\n    exprs = [\n        atv(k, l, c, d)*att(aa, c, ii, k)*att(bb, d, jj, l),\n        atv(l, k, c, d)*att(aa, c, ii, l)*att(bb, d, jj, k),\n        atv(k, l, d, c)*att(aa, d, ii, k)*att(bb, c, jj, l),\n        atv(l, k, d, c)*att(aa, d, ii, l)*att(bb, c, jj, k),\n    ]\n    for permut in exprs[1:]:\n        assert substitute_dummies(exprs[0]) == substitute_dummies(permut)\n\n\n\n", "def test_anti_symmetric_tensor_with_duplicates():\n    v = symbols(\"v\")\n\n    a = symbols('a', above_fermi=True)\n    b = symbols('b', above_fermi=True)\n    i = symbols('i', below_fermi=True, cls=Dummy)\n    j = symbols('j', below_fermi=True, cls=Dummy)\n\n    # Anti-symmetric tensor with duplicate indices should raise an error\n    with pytest.raises(ValueError):\n        AntiSymmetricTensor(v, (i, i, j), (a, b))\n\n\n\n", "compilation error", "compilation error"], "sample_1181": ["def test_sympy_tocupy():\n    if not np:\n        skip(\"NumPy not installed\")\n\n    from sympy.tensor.array import Array\n    a = Array(((1, 2), (3, 4)))\n    b = Array((1, 2))\n    c = Array((2, 2))\n\n    assert np.array_equal(NumPyPrinter().doprint(a), NumPyPrinter().doprint(a))\n    assert np.array_equal(SciPyPrinter().doprint(a), NumPyPrinter().doprint(a))\n    assert np.array_equal(CuPyPrinter().doprint(a), np.array([[1, 2], [3, 4]]))\n    assert np.array_equal(CuPyPrinter().doprint(b), np.array((1, 2)))\n    assert np.array_equal(CuPyPrinter().doprint(c), np.array([[0, 0], [0, 0]]))\n\n\n", "def test_scipy_fresnel():\n    if not np:\n        skip(\"NumPy not installed\")\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(scipy.special.fresnel(1.5)) == \"scipy.special.fresnel(1.5)\"\n    assert prntr.doprint(scipy.special.fresnel(1.5)[0]) == \"scipy.special.fresnel(1.5)[0]\"\n    assert prntr.doprint(scipy.special.fresnel(1.5)[1]) == \"scipy.special.fresnel(1.5)[1]\"\n\n\n", "def test_numpy_array_transpose():\n    if not np:\n        skip(\"NumPy not installed\")\n\n    f = lambdify((a,), a.transpose(), 'numpy')\n    a = np.array([[1, 2, 3], [4, 5, 6]])\n    assert np.array_equal(f(a), np.transpose(a))\n", "def test_scipy_log_gamma():\n    if not np:\n        skip(\"NumPy not installed\")\n    from scipy.special import loggamma\n\n    f = lambdify((a,), loggamma(a), 'scipy')\n    assert abs(f(1.5) - 0.6931471805599453) < 1e-10\n\n", "def test_scipy_log_gamma_function():\n    if not np:\n        skip(\"SciPy not installed\")\n\n    f = lambdify((x,), scipy.special.gammaln(x), 'numpy')\n    x_ = np.array([0.1, 1, 1.5, 2])\n    assert np.allclose(f(x_), scipy.special.loggamma(x_))\n"], "sample_62": ["    def test_registration_with_custom_admin_site(self):\n        custom_admin_site = CustomSite()\n        custom_admin_site.register(Person, NameAdmin)\n        self.assertIsInstance(custom_admin_site._registry[Person], NameAdmin)\n", "    def test_unregister(self):\n        self.site.register(Person)\n        self.assertTrue(self.site._registry)\n        self.site.unregister(Person)\n        self.assertFalse(self.site._registry)\n", "    def test_custom_index_template(self):\n        class CustomSite(admin.AdminSite):\n            index_template = 'admin/custom_index.html'\n\n        custom_site = CustomSite()\n        custom_site.register(Person)\n        context = {'app_list': []}\n        response = TemplateResponse(\n            MockedRequest(), 'admin/custom_index.html', context\n        )\n        self.assertEqual(response.template.template_name, 'admin/custom_index.html')\n", "    def test_registration_with_custom_site_and_options(self):\n        my_custom_site = CustomSite()\n        register(Person, site=my_custom_site, search_fields=['name', 'email'], list_display=['name', 'email'])\n        self.assertIsInstance(my_custom_site._registry[Person], admin.ModelAdmin)\n        self.assertEqual(my_custom_site._registry[Person].search_fields, ['name', 'email'])\n        self.assertEqual(my_custom_site._registry[Person].list_display, ['name', 'email'])\n\n\n\n", "    def test_registration_with_custom_admin_site(self):\n        custom_site = CustomSite()\n        custom_site.register(Person, NameAdmin)\n        self.assertIsInstance(custom_site._registry[Person], NameAdmin)\n        self.assertNotEqual(custom_site._registry[Person], self.default_site._registry.get(Person))\n"], "sample_879": ["def test_one_hot_encoder_with_categorical_dtype():\n    \"\"\"Check OneHotEncoder works with categorical dtype from Pandas.\n\n    Non-regression test for #25186\n    \"\"\"\n    pd = pytest.importorskip(\"pandas\")\n    X = pd.DataFrame({\"A\": pd.Categorical([\"a\", \"b\", \"a\"])})\n\n    ohe = OneHotEncoder(sparse_output=False)\n    X_encoded = ohe.fit_transform(X)\n\n    assert X_encoded.dtype == object\n    assert all(col in X_encoded.columns for col in [\"A_a\", \"A_b\"])\n\n", "def test_ordinal_encoder_max_value_handling():\n    \"\"\"Check OrdinalEncoder handles max value when categories are not provided.\"\"\"\n    X = np.array([1, 2, 3, 4, 5], dtype=np.int32)\n\n    oe = OrdinalEncoder()\n\n    # Fit using default settings\n    oe.fit(X)\n    assert_allclose(oe.transform(X), np.array([[0], [1], [2], [3], [4]]).T)\n\n    # Test with a wider range of integer values\n    X_wide = np.array([100, 101, 102, 103, 104])\n    oe.fit(X_wide)\n    assert_allclose(oe.transform(X_wide), np.array([[0], [1], [2], [3], [4]]).T)\n\n    # Check that `categories_` attribute is populated correctly\n    assert_array_equal(oe.categories_[0], np.array([1, 2, 3, 4, 5]))\n\n\n\n", "def test_ordinal_encoder_with_zero_categories():\n    \"\"\"Check OrdinalEncoder behavior when categories array is empty.\"\"\"\n    X = np.array([0, 1, 2], dtype=np.int64)\n\n    with pytest.raises(ValueError, match=\"Categories array is empty\"):\n        OrdinalEncoder(categories=[]).fit(X)\n\n    oe = OrdinalEncoder(categories=[0, 1, 2]).fit(X)\n    X_trans = oe.transform(X)\n    assert_allclose(X_trans, [[0], [1], [2]])\n", "def test_onehotencoder_handle_unknown_invalid_handling():\n    \"\"\"Check that invalid handle_unknown values raise an error.\"\"\"\n\n    with pytest.raises(ValueError, match=\"Invalid value for 'handle_unknown'\"):\n        OneHotEncoder(handle_unknown=\"invalid\")\n\n", "def test_ordinal_encoder_empty_categories():\n    \"\"\"Check OrdinalEncoder handles empty categories gracefully.\"\"\"\n    with pytest.raises(ValueError, match=\"Categories must be provided\"):\n        OrdinalEncoder().fit(np.array([]))\n\n    with pytest.raises(ValueError, match=\"Empty categories\"):\n        OrdinalEncoder(categories=[[], [1, 2, 3]]).fit(np.array([]))\n\n\n"], "sample_1170": ["def test_printing_str_array_expressions():\n    A = ArraySymbol(\"A\", 2, 3)\n    B = ArraySymbol(\"B\", 3, 2)\n    assert str(A[2, :]) == 'A[2, :]'\n    assert str(A[:, 1]) == 'A[:, 1]'\n    assert str(A[0:2, 1:3]) == 'A[0:2, 1:3]'\n    assert str(A[0:2:2, 1:3:2]) == 'A[0:2:2, 1:3:2]'\n    assert str(A * B) == 'A*B'\n    assert str(A + B) == 'A + B'\n", "def test_printing_commutative_symbol_with_args():\n    from sympy import commutative_symbol, Symbol\n    x, y = Symbol(\"x\", commutative=True), Symbol(\"y\", commutative=True)\n    a = commutative_symbol(\"a\")\n    assert str(a(x,y)) == \"a(x, y)\"\n    assert str(a(x+y)) == \"a(x + y)\"\n\n\n", "def test_issue_23197():\n    from sympy.geometry import Point, Line\n    p = Point(1, 2)\n    l = Line(p, 2*p)\n    assert str(l) == 'Line(Point(1, 2), 2*Point(1, 2))'\n\n\n", "def test_issue_23250():\n    from sympy.logic.boolalg import And, Or\n    assert str(And(x > 1, x < 5)) == \"x > 1 and x < 5\"\n    assert str(Or(x > 1, x < 5)) == \"x > 1 or x < 5\"\n", "def test_MatrixRank():\n    from sympy.matrices import Matrix\n    assert str(MatrixRank(Matrix([[1, 2], [3, 4]]))) == \"MatrixRank(Matrix([[1, 2], [3, 4]]))\"\n    assert str(MatrixRank(Matrix([[1, 2, 3], [4, 5, 6]]))) == \"MatrixRank(Matrix([[1, 2, 3], [4, 5, 6]]))\"\n\n\n\n"], "sample_516": ["def test_transparent_background():\n    fig, ax = plt.subplots(figsize=(1, 1))\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    for i in range(5):\n        ax.add_patch(Rectangle((i * 0.2, 0), 0.1, 0.1, facecolor='C0', alpha=0.5))\n\n    fig.savefig(io.BytesIO(), format='pdf', transparent=True)\n", "def test_font_properties():\n    fig, ax = plt.subplots()\n    ax.set_title('Testing font properties',\n             fontproperties=FontProperties(family='serif',\n                                          size=14,\n                                          weight='bold',\n                                          style='italic'))\n    ax.plot([1, 2, 3], [4, 5, 6])\n", "def test_text_alignment():\n    fig, ax = plt.subplots()\n    ax.text(0.1, 0.9, 'Top Center', ha='center', va='top')\n    ax.text(0.1, 0.1, 'Bottom Center', ha='center', va='bottom')\n    ax.text(0.9, 0.5, 'Right Top', ha='right', va='top')\n    ax.text(0.1, 0.5, 'Left Bottom', ha='left', va='bottom')\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    ax.axis('off')\n", "def test_font_size_change():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3])\n\n    # Test changing font size within a figure\n    ax.set_xlabel(\"X\", fontsize=10)\n    ax.set_ylabel(\"Y\", fontsize=12)\n\n    ax.text(1.5, 2, \"A test\", fontsize=14)\n\n\n", "def test_transparency_gradient():\n    fig, ax = plt.subplots()\n    x = np.linspace(0, 1, 100)\n    y = np.sin(2 * np.pi * x)\n    line, = ax.plot(x, y, color='red', alpha=0.5)\n    ax.fill_between(x, 0, y, color='blue', alpha=0.2)\n\n"], "sample_15": ["        def test_gamma_scalar(self, function):\n            TestQuantityMathFuncs.test_exp_scalar(None, function)\n", "        def test_jv_out(self, function):\n            q1 = function(2.0 * u.m / (2.0 * u.m), 3.0 * u.m / (6.0 * u.m), out=u.Quantity(0))\n            assert q1.value == function(1.0, 0.5)\n            assert q1.unit == u.dimensionless_unscaled\n\n            q2 = function(\n                np.ones(3) * u.m / (1.0 * u.m),\n                np.array([2.0, 3.0, 6.0]) * u.m / (6.0 * u.m),\n                out=np.empty(3),\n            )\n            assert np.all(\n                q2 == function(np.ones(3), np.array([1.0 / 3.0, 1.0 / 2.0, 1.0]))\n            )\n", "        def test_jv_out(self, function):\n            q1 = function(2.0 * u.m / (2.0 * u.m), 3.0 * u.m / (6.0 * u.m), out=np.zeros(3))\n            assert q1.unit == u.dimensionless_unscaled\n            q2 = function(2.0 * u.m / (2.0 * u.m), 3.0 * u.m / (6.0 * u.m), dtype=complex)\n            assert q2.unit == u.dimensionless_unscaled\n            assert q2.dtype == complex\n\n\n\n", "        def test_bessel_like_ufuncs(self, function):\n            q = function(2.0 * u.m / (2.0 * u.m), 3.0 * u.m / (6.0 * u.m))\n            assert q.unit == u.dimensionless_unscaled\n            assert q.value == function(1.0, 0.5)\n", "        def test_general_scipy_ufuncs(self, function):\n            q1 = function(1.0 * u.m)\n            assert q1.unit == u.dimensionless_unscaled\n            assert q1.value == function(1.0)\n\n            q2 = function(1.0 * u.m * u.s)\n            assert q2.unit == function(1.0 * u.m * u.s).unit\n\n\n"], "sample_469": ["    def test_alias_with_nested_annotations(self):\n        qs = Book.objects.annotate(\n            rating_diff=F(\"rating\") - 3\n        ).alias(\n            rating_diff_alias=F(\"rating_diff\"),\n        ).annotate(rating_sum=F(\"rating_diff_alias\") + F(\"rating\"))\n        for book in qs:\n            with self.subTest(book=book):\n                self.assertEqual(book.rating_sum, book.rating + book.rating_diff)\n\n", "    def test_alias_with_functions(self):\n        qs = Book.objects.alias(\n            len_title_alias=Length(\"title\"),\n        ).annotate(title_length=F(\"len_title_alias\"))\n        self.assertIs(hasattr(qs.first(), \"len_title_alias\"), False)\n        for book in qs:\n            with self.subTest(book=book):\n                self.assertEqual(book.title_length, len(book.title))\n\n", "    def test_alias_with_nested_expressions(self):\n        qs = (\n            Book.objects.alias(\n                other_rating=F(\"rating\") + 1,\n                another_rating=F(\"other_rating\") * 2,\n            )\n            .annotate(\n                final_rating=Coalesce(\"another_rating\", 0),\n            )\n        )\n        self.assertIs(hasattr(qs.first(), \"other_rating\"), False)\n        self.assertIs(hasattr(qs.first(), \"another_rating\"), False)\n        for book in qs:\n            with self.subTest(book=book):\n                self.assertEqual(book.final_rating, 2 * (book.rating + 1)) \n\n\n", "    def test_alias_with_complex_expression(self):\n        qs = Book.objects.alias(\n            rating_adj=F(\"rating\") * 1.1\n        ).filter(rating_adj__gte=5.0)\n        self.assertIs(hasattr(qs.first(), \"rating_adj\"), False)\n        self.assertSequenceEqual(qs, [self.b1])\n\n", "    def test_alias_reserved_keywords(self):\n        msg = (\n            \"Column aliases cannot contain reserved SQL keywords like 'group'.\"\n        )\n        with self.assertRaisesMessage(ValueError, msg):\n            Book.objects.alias(group=Value(1))\n\n\n"], "sample_784": ["def test_calibration_pipeline():\n    \"\"\"Test calibration on a pipeline with feature scaling.\"\"\"\n    from sklearn.preprocessing import StandardScaler\n    \n    X, y_true = make_classification(n_samples=100, n_features=10, random_state=42)\n    X -= X.min()  # Remove negative values\n\n    # Define a pipeline with feature scaling and classification\n    pipeline = Pipeline([('scaler', StandardScaler()),\n                         ('clf', LinearSVC())])\n\n    # Fit the pipeline and then calibrate\n    pipeline.fit(X, y_true)\n    calibrated_pipeline = CalibratedClassifierCV(pipeline, method='isotonic', cv=2)\n    calibrated_pipeline.fit(X, y_true)\n\n    # Make predictions and compare with uncalibrated pipeline\n    probs_uncalibrated = pipeline.predict_proba(X)\n    probs_calibrated = calibrated_pipeline.predict_proba(X)\n    \n    # Check if calibrated probabilities have improved brier score\n    assert_greater(brier_score_loss(y_true, probs_uncalibrated),\n                   brier_score_loss(y_true, probs_calibrated)) \n", "def test_calibration_sparse_data():\n    \"\"\"Test calibration with sparse data\"\"\"\n    # Test calibration with sparse data\n    n_samples = 100\n    n_features = 1000\n    X, y = make_classification(n_samples=n_samples, n_features=n_features,\n                               sparse=True,\n                               random_state=42)\n    X_train, y_train = X[:n_samples // 2], y[:n_samples // 2]\n    X_test, y_test = X[n_samples // 2:], y[n_samples // 2:]\n\n    clf = RandomForestClassifier(n_estimators=10, random_state=42)\n    clf.fit(X_train, y_train)\n\n    for method in ['isotonic', 'sigmoid']:\n        cal_clf = CalibratedClassifierCV(clf, method=method, cv=3)\n        cal_clf.fit(X_train, y_train)\n        cal_probs = cal_clf.predict_proba(X_test)\n        # Check that brier score has improved after calibration\n        uncalibrated_loss = brier_score_loss(y_test, clf.predict_proba(X_test))\n        calibrated_loss = brier_score_loss(y_test, cal_probs)\n        assert_greater(uncalibrated_loss, calibrated_loss)\n\n\n\n", "def test_calibration_multi_estimator_fit():\n    \"\"\"Test calibration with different base estimators and fitting strategies.\n\n    This test ensures that CalibratedClassifierCV can handle a variety\n    of base estimators and fit strategies, including those that use\n    decision_function or predict_proba methods.\n    \"\"\"\n    data = np.random.rand(100, 5)\n    target = np.random.randint(2, size=100)\n\n    # Test with a Random Forest classifier\n    rf_clf = RandomForestClassifier(n_jobs=-1, random_state=42)\n    calibrated_rf = CalibratedClassifierCV(rf_clf, method=\"sigmoid\", cv=3)\n    calibrated_rf.fit(data, target)\n    # Test with a Logistic Regression classifier\n    lr_clf = LogisticRegression(random_state=42)\n    calibrated_lr = CalibratedClassifierCV(lr_clf, method=\"isotonic\", cv=\"prefit\")\n    calibrated_lr.fit(data, target)\n    # Test with a Gaussian Process classifier\n    gp_clf = GaussianProcessClassifier(random_state=42)\n    calibrated_gp = CalibratedClassifierCV(gp_clf, method=\"isotonic\")\n    calibrated_gp.fit(data, target)\n\n\n\n", "def test_calibration_multi_label():\n    \"\"\"Test calibration for multi-label classification\"\"\"\n    # Generate multi-label data\n    X, y = make_classification(n_samples=100, n_features=2, random_state=42,\n                              n_informative=2, n_classes=3,\n                              multi_label=True)\n\n    # Use the 'sigmoid' method as 'isotonic' might not perform well with\n    # multi-label data\n    clf = CalibratedClassifierCV(base_estimator=RandomForestClassifier(),\n                                  method=\"sigmoid\", cv=2)\n    clf.fit(X, y)\n    probs_with_sw = clf.predict_proba(X)\n    \n    # Check that sum of probabilities for each class is 1\n    \n    for i in range(probs_with_sw.shape[1]):  \n        assert_array_almost_equal(np.sum(probs_with_sw[:, i], axis=1),\n                                  np.ones(probs_with_sw.shape[0]))\n", "def test_calibration_multiclass_predict():\n    \"\"\"Test calibration for multiclass with predict method\"\"\"\n    # test multi-class setting with classifier that implements only predict\n    # method\n    clf = RandomForestClassifier(random_state=42)\n    X, y = make_blobs(n_samples=100, n_features=2, random_state=42,\n                      centers=5, cluster_std=3.0)\n    target_names = np.array(['a', 'b', 'c', 'd', 'e'])\n    y = target_names[y]\n\n    X_train, y_train = X[::2], y[::2]\n    X_test, y_test = X[1::2], y[1::2]\n\n    clf.fit(X_train, y_train)\n    for method in ['isotonic', 'sigmoid']:\n        cal_clf = CalibratedClassifierCV(clf, method=method, cv=2)\n        cal_clf.fit(X_train, y_train)\n        y_pred = cal_clf.predict(X_test)\n        assert_array_equal(y_pred, clf.predict(X_test))\n\n"], "sample_1158": ["compilation error", "def test_issue_21537():\n    #Test sympify with nested lists\n    expr1 = sympify([[1, 2, 3], [4, 5, 6]], evaluate=False)\n    expr2 = sympify([[1, 2, 3], [4, 5, 6]], evaluate=True)\n    expr3 = sympify(\"[[1, 2, 3], [4, 5, 6]]\")\n\n    assert isinstance(expr1, ImmutableDenseNDimArray)\n    assert expr1.shape == (2, 3)\n    assert isinstance(expr2, ImmutableDenseNDimArray)\n    assert expr2.shape == (2, 3)\n    assert isinstance(expr3, ImmutableDenseNDimArray)\n    assert expr3.shape == (2, 3)\n\n    assert expr1[0,0] == expr2[0,0] == expr3[0,0] == 1\n    assert expr1[0,1] == expr2[0,1] == expr3[0,1] == 2\n    assert expr1[0,2] == expr2[0,2] == expr3[0,2] == 3\n    assert expr1[1,0] == expr2[1,0] == expr3[1,0] == 4\n    assert expr1[1,1] == expr2[1,1] == expr3[1,1] == 5\n    assert expr1[1,2] == expr2[1,2] == expr3[1,2] == 6\n", "def test_sympify_issue_22591():\n    # test for sympy error when sympify input contains a string literal\n    # inside a list\n    string_list = [1, \"x\"]\n    raises(SympifyError, lambda: sympify(string_list))\n\n\n", "def test_issue_21536_continued():\n    #test to check sympify with mixed inputs\n    assert sympify([1, 'x+3*x+2', 2*x+4*x+2+4]) == [1, \n                                                      x + 3*x + 2,\n                                                      2*x + 4*x + 2 + 4]\n    assert sympify([1, 2*x+4*x+2+4, 'x+3*x+2']) == [1, \n                                                      2*x + 4*x + 2 + 4,\n                                                      x + 3*x + 2]\n", "def test_issue_21096():\n    raises(SympifyError, lambda: sympify(\"int(x)\", locals={\"x\": S(1.2)}))\n"], "sample_371": ["    def test_sensitive_method_with_kwargs(self):\n        \"\"\"\n        The sensitive_variables decorator works with object methods when given kwargs.\n        \"\"\"\n        with self.settings(DEBUG=True):\n            self.verify_unsafe_response(sensitive_kwargs_function_caller)\n            self.verify_unsafe_email(sensitive_kwargs_function_caller)\n\n        with self.settings(DEBUG=False):\n            self.verify_safe_response(sensitive_kwargs_function_caller)\n            self.verify_safe_email(sensitive_kwargs_function_caller)\n", "    def test_sensitive_post_parameters_method_decorator(self):\n        class MyClass:\n            @method_decorator(sensitive_post_parameters())\n                return HttpResponse()\n\n        instance = MyClass()\n        self.assertIsInstance(instance.a_view(HttpRequest()), HttpResponse)\n", "    def test_sensitive_post_parameters_class_method(self):\n        class MyClass:\n            @method_decorator(sensitive_post_parameters())\n                return HttpResponse()\n\n        instance = MyClass()\n        response = instance.a_view(self.rf.get('/'))\n        self.assertEqual(response.status_code, 200)\n", "    def test_sensitive_post_parameters_method_decorator(self):\n        class MyClass:\n            @method_decorator(sensitive_post_parameters)\n                return HttpResponse()\n\n        instance = MyClass()\n        response = instance.a_view(HttpRequest())\n        self.assertEqual(response.status_code, 200)\n\n", "    def test_sensitive_post_parameters_method_decorator(self):\n        class MyClass:\n            @method_decorator(sensitive_post_parameters())\n                return HttpResponse()\n\n        instance = MyClass()\n        response = instance.a_view(HttpRequest())\n        self.assertEqual(response.status_code, 200)\n\n\n\n"], "sample_576": ["    def test_default_object_is_hashable(self):\n        assert Hash({'x': Default()}) == \n        Hash({'x': Default()})\n", "    def test_default_object_is_hashable(self):\n\n        assert hash(Default()) == hash(Default())\n", "    def test_default_object_creation(self):\n\n        dp = Plot().pair().facet()\n        assert isinstance(dp.default_objects, Default)\n", "    def test_default_to_object(self):\n        p = Plot()\n        assert isinstance(p.color, Default)\n        assert isinstance(p.fill, Default)\n        assert isinstance(p.facet_row, Default)\n        assert isinstance(p.facet_col, Default)\n        assert isinstance(p.wrap, Default)\n\n\n\n", "    def test_resolve_scales(self, long_df):\n\n        p = Plot(long_df)\n        p.pair(x=[\"x\"], y=[\"y\"])\n        scales = p._resolve_scales(\n            groupby=(\"x\", \"y\"),\n            variables=(\"x\", \"y\"),\n            data=long_df,\n        )\n\n        assert \"x\" in scales\n        assert \"y\" in scales\n        assert isinstance(scales[\"x\"], NominalScale)\n        assert isinstance(scales[\"y\"], NumericalScale)\n\n        # Ensure scales are not shared between unrelated variables\n        p2 = Plot(long_df)\n        p2.pair(x=[\"y\"], y=[\"z\"])\n        scales2 = p2._resolve_scales(\n            groupby=(\"y\", \"z\"),\n            variables=(\"y\", \"z\"),\n            data=long_df,\n        )\n\n        assert \"y\" in scales2\n        assert \"z\" in scales2\n        assert scales2[\"y\"] is not scales[\"y\"]\n        assert scales2[\"z\"] is not scales[\"x\"]\n\n\n\n"], "sample_83": ["    def test_tag_compile_function(self):\n            return Node()\n        self.library.tag('name', compile_function)\n        self.assertEqual(self.library.tags['name'], compile_function)\n\n\n\n", "    def test_simple_tag_args(self):\n        @self.library.simple_tag(takes_context=True)\n            return arg1 + arg2\n        node = func.func(None, None, [])\n        self.assertEqual(node.takes_context, True)\n        self.assertEqual(len(node.args), 2)\n", "    def test_tag_compile_function(self):\n            return Node()\n        self.library.tag('name', compile_func)\n        self.assertEqual(self.library.tags['name'], compile_func)\n\n\n", "    def test_tag_compile_function(self):\n            return Node()\n        @self.library.tag('name', compile_function)\n            return Node()\n        self.assertEqual(self.library.tags['name'], compile_function)\n", "    def test_filter_flags(self):\n        @self.library.filter(expects_localtime=True, is_safe=True)\n            return ''\n        self.assertTrue(func.expects_localtime)\n        self.assertTrue(func.is_safe)\n        self.assertFalse(func.needs_autoescape)\n\n\n\n"], "sample_640": ["def test_is_overload_stub() -> None:\n    code = astroid.extract_node(\n        \"\"\"\n        from typing import List\n        \n        class MyClass:\n                pass\n\n            @overload\n                pass\n\n        \"\"\"\n    )\n    assert utils.is_overload_stub(code[1].parent) is True\n    assert utils.is_overload_stub(code[1]) is False\n", "def test_is_overload_stub() -> None:\n    code = astroid.extract_node(\n        \"\"\"\n        from typing import overload\n\n        @overload\n            pass\n\n            return \"hello\"\n        \"\"\"\n    )\n    assert utils.is_overload_stub(code[0]) is True\n    assert utils.is_overload_stub(code[1]) is False\n", "    def test_is_overload_stub(decorated_node, expected):\n        assert utils.is_overload_stub(decorated_node) == expected\n", "def test_is_overload_stub() -> None:\n    code = astroid.extract_node(\"\"\"\n    from typing import Literal\n        return x * 2\n    \"\"\")\n    assert utils.is_overload_stub(code[0]) is True\n\n\n    code = astroid.extract_node(\"\"\"\n        return x * 2\n    \"\"\")\n    assert utils.is_overload_stub(code[0]) is False\n\n", "def test_is_call_of_name() -> None:\n    node = astroid.extract_node(\"print('hello')\")\n    assert utils.is_call_of_name(node.func, \"print\")\n    assert not utils.is_call_of_name(node.func, \"len\")\n\n\n"], "sample_801": ["def test_repr_with_nested_lists():\n    from sklearn.pipeline import Pipeline\n\n    # Render a pipeline with nested lists for parameters\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.feature_selection import SelectKBest\n\n    param_grid = {\n        'input_features': [ [\n            {'estimator': StandardScaler(), '\n            param_key': '1'},\n            {'estimator': SelectKBest(k=2), 'param_key': '2'}\n        ]],\n        'classifier': [LogisticRegression(random_state=0)]\n    }\n    pipeline = Pipeline(param_grid)\n    expected = \"\"\"Pipeline(memory=None,\n         steps=[('input_features',\n                 Pipeline(memory=None,\n                          steps=[('1', StandardScaler()),\n                                 ('2', SelectKBest(k=2))])),\n                ('classifier', LogisticRegression(C=1.0,\n                random_state=0, class_weight=None, dual=False,\n                fit_intercept=True, intercept_scaling=1,\n                l1_ratio=None, max_iter=100, multi_class='warn',\n                n_jobs=None, penalty='l2', solver='warn', tol=0.0001,\n                verbose=0, warm_start=False))])\"\"\"\n\n    expected = expected[1:]  # remove first \\n\n    assert pipeline.__repr__() == expected\n\n\n\n\n", "def test_categorical_features():\n    from sklearn.datasets import make_classification\n    from sklearn.linear_model import LogisticRegression\n    X, y = make_classification(n_samples=100, n_features=5, n_informative=3,\n                               n_redundant=1, random_state=42)\n    X[:, 3] = np.array(['A', 'B', 'A', 'B'] * 25)\n    X[:, 4] = np.array([1, 2, 1, 2] * 25)\n\n    lr = LogisticRegression()\n    lr.fit(X, y)\n\n    # Assert that categorical features have been properly handled\n    # Note: the exact output might vary depending on the random\n    # initialization of the estimator\n    expected_output = r\"\"\"LogisticRegression(C=1.0, class_weight=None, dual=False,\n                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class='warn', n_jobs=None,\n                   penalty='l2', random_state=None, solver='warn', tol=0.0001,\n                   verbose=0, warm_start=False)\"\"\"\n    assert lr.__repr__() == expected_output\n", "def test_complex_pipeline_with_named_steps():\n    pipeline = Pipeline([\n        ('scaler', StandardScaler(with_mean=False)),\n        ('pca', PCA(n_components=2)),\n        ('clf', LogisticRegression())\n    ])\n    expected = \"\"\"", "def test_handle_nested_lists():\n    from sklearn.linear_model import LogisticRegression\n\n    # Create a nested list structure for parameters\n    param_grid = {\n        'logisticregression__C': [0.1, 1, 10],\n        'nested_list': [[1, 2], [3, 4], [5, 6]]\n    }\n\n    pipeline = make_pipeline(\n        SimpleImputer(),\n        LogisticRegression(),\n    )\n\n    gs = GridSearchCV(pipeline, param_grid=param_grid, cv=3)\n    expected = \"\"\"", "def test_estimator_type():\n    # Ensure that the estimator type is correctly handled\n\n    lr = LogisticRegression()\n    assert is_classifier(lr)\n    assert not is_regressor(lr)\n    assert not is_outlier_detector(lr)\n\n    svc = SVC()\n    assert is_classifier(svc)\n    assert not is_regressor(svc)\n    assert not is_outlier_detector(svc)\n\n    pca = PCA()\n    assert not is_classifier(pca)\n    assert not is_regressor(pca)\n    assert not is_outlier_detector(pca)\n\n    imputer = SimpleImputer()\n    assert not is_classifier(imputer)\n    assert not is_regressor(imputer)\n    assert not is_outlier_detector(imputer)\n\n    # Test for regressors\n    reg = LinearRegression()\n    assert not is_classifier(reg)\n    assert is_regressor(reg)\n    assert not is_outlier_detector(reg)\n\n    # Test for outlier detector\n    # We need a real outlier detector, or a mock one\n    # For this example, let's just check if a known outlier detector returns\n    # True\n    from sklearn.ensemble import IsolationForest\n    outlier_detector = IsolationForest()\n    assert not is_classifier(outlier_detector)\n    assert not is_regressor(outlier_detector)\n    assert is_outlier_detector(outlier_detector) \n\n\n\n"], "sample_20": ["def test_masked_table_serialize_data_mask_round_trip(tmp_path):\n    \"\"\"\n    Regression test for masked values being written and read properly with\n    'data_mask' serialize_method.\n\n    This test focuses on the case where mask values are stored as the\n    data itself, not in a separate column.\n    \"\"\"\n    filename = tmp_path / \"test.fits\"\n\n    t = simple_table(masked=True, serialize_method=\"data_mask\")  \n\n    t.write(filename)\n\n    t2 = Table.read(filename)\n\n    assert t2.masked is False\n    assert t2.colnames == t.colnames\n    for name in t2.colnames:\n        assert np.all(t2[name].mask == t[name].mask)\n       \n        # Data under the mask round-trips also (unmask data to show this).\n        t[name].mask = False\n        t2[name].mask = False\n        assert np.all(t2[name] == t[name])\n\n\n\n", "def test_fits_header_keyword_handling():\n    filename = get_pkg_data_filename(\"data/tb.fits\")\n    with fits.open(filename) as hdul:\n        header = hdul[1].header\n        assert header[\"OBJECT\"] == \"M42\"\n        assert header[\"TELESCOP\"] == \"Hubble\"\n        assert header[\"EXPOSURE\"] == 10.2\n\n    # Test for creation of keywords with potential issues:\n    t = Table([np.array([1, 2]), np.array([3, 4])], names=[\"a\", \"b\"])\n    t.write(\"test.fits\", format=\"fits\", overwrite=True)\n    with fits.open(\"test.fits\") as hdul:\n        header = hdul[1].header\n        assert header[\"TFIELDS\"] == 2\n        assert \"COMMENT\" in header\n        # assert len(header[\"COMMENT\"]) > 0  \n\n\n\n", "def test_masked_table_roundtrip_serialize_mask():\n    filename = tmp_path / \"test_masked.fits\"\n\n    t = simple_table(masked=True)  # int, float, and str cols with one masked element\n\n    t.write(filename, serialize_method=\"data_mask\")\n\n    t2 = Table.read(filename, astropy_native=True, hdu=1)\n    assert t2.masked is True\n    assert t2.colnames == t.colnames\n    for name in t2.colnames:\n        assert np.all(t2[name].mask == t[name].mask)\n        assert np.all(t2[name] == t[name])\n", "    def test_fits_header_keywords(tmp_path):\n        \"\"\"Test writing keywords to the FITS header.\"\"\"\n        filename = tmp_path / \"test.fits\"\n        t = Table([np.arange(5), np.array([True, False, True, False, True])],\n                  names=('a', 'b'))\n        t.meta['KEY1'] = 'VALUE1'\n        t.meta['KEY2'] = 123\n        t.write(filename, format='fits', overwrite=True)\n\n        with fits.open(filename) as hdul:\n            assert hdul[1].header['KEY1'] == 'VALUE1'\n            assert hdul[1].header['KEY2'] == '123'\n\n\n\n", "def test_is_fits_valid_format(tmp_path):\n    \"\"\"\n    Test various valid FITS file formats.\n    \"\"\"\n    with open(tmp_path / \"test.fits\", \"wb\") as f:\n        f.write(b\"\\x1a\\x00\\x00\\x01\\x00\\x00\\x00\\x00\")  # Valid FITS header\n    assert connect.is_fits(tmp_path / \"test.fits\", \"\", None)\n\n    with open(tmp_path / \"test.fits\", \"wb\") as f:\n        f.write(b\"\\x1a\\x00\\x00\\x00\\x00\")  # Truncated FITS header\n    assert connect.is_fits(tmp_path / \"test.fits\", \"\", None)\n\n    assert connect.is_fits(\n        path=get_pkg_data_filename(\"data/testdata.fits\"),\n        format=\"fits\",\n        verify=\"ignore\",\n    )\n\n\n\n"], "sample_1105": ["def test_matmul_with_symbols():\n    A = MatrixSymbol('A', 2, 3)\n    B = MatrixSymbol('B', 3, 2)\n    C = MatrixSymbol('C', 2, 2)\n    assert MatMul(A, B, C).shape == (2, 2)\n    assert MatMul(A, B, C).args[0] == A\n    assert MatMul(A, B, C).args[1] == B\n    assert MatMul(A, B, C).args[2] == C\n\n\n", "def test_inverse():\n    assert MatMul(A, Inverse(A)).doit() == Identity(n)\n    assert MatMul(A, Inverse(B)).doit() == MatMul(A, Inverse(B))\n\n\n\n", "def test_matmul_with_permutation_matrices():\n    P = PermutationMatrix([1, 2, 3])\n    Q = PermutationMatrix([2, 3, 1])\n    assert combine_permutations(MatMul(P, Q)).doit() == PermutationMatrix([3, 1, 2])\n    assert combine_permutations(MatMul(P, Q, P)).doit() == PermutationMatrix([1, 3, 2])\n\n\n", "def test_matmul_with_matrices_and_symbols():\n    A = MatrixSymbol('A', 2, 2)\n    B = Matrix(2, 2, [1, 2, 3, 4])\n    C = MatrixSymbol('C', 2, 2)\n    result = MatMul(A, B, C)\n    assert result.args == (A, B, C)\n    assert isinstance(result, MatMul)\n", "def test_matmul_noncommutative_scalars():\n    a, b = symbols('a b', commutative=False)\n    A = Matrix([[1, 2], [3, 4]])\n    with pytest.raises(NotImplementedError):\n        MatMul(a, A, b).doit() \n"], "sample_959": ["    def test_domain_cpp_parse_ref_template_param_qualified_name(app):\n        text = (\".. cpp:class:: X\\n\"\n                \".. cpp:template:: class Y<typename T>\\n\"\n                \"   :member:: int X::T::member;\\n\")\n        restructuredtext.parse(app, text)\n        tree = app.builder.get_doctree('index.rst')\n        assert tree.children[0].children[0].children[0].children[0].tag == 'member'\n        assert tree.children[0].children[0].children[0].children[0].attributes['id'] == 'X::T::member'\n", "    def test_domain_cpp_parse_implicit_this(app, status, warning):\n        text = (\n            \".. cpp:class:: MyClass\\n\"\n            \"    def my_method(self, arg):\\n\"\n            \"        print(self.arg)\\n\"\n            \"    \"\n            )\n        doctree = restructuredtext.parse(app, text)\n        assert_node(doctree, (addnodes.member, 'my_method', 'MyClass'))\n        assert_node(doctree[0], addnodes.member,\n                    name='my_method', parent=doctree[1],\n                    )\n\n", "def test_domain_cpp_build_unquoted_keywords(app, status, warning):\n    app.builder.build_all()\n    ws = filter_warnings(warning, \"unquoted-keywords\")\n    assert len(ws) == 1\n    assert \"WARNING: cpp:identifier reference target not found: return\" in ws[0]\n", "def test_domain_cpp_parse_complex_ref(app, warning):\n    text = (\".. cpp:struct:: A\\n\"\n            \"  cdef int x\\n\"\n            \".. cpp:member:: x :cpp:ref:A::x\")\n    restructuredtext.parse(app, text)\n    ws = warning.getvalue().split(\"\\n\")\n    assert len(ws) == 0\n", "    def check(spec, text, file):\n        pattern = '<li><p>%s<a .*?><code .*?><span .*?>%s</span></code></a></p></li>' % spec\n        res = re.search(pattern, text)\n        if not res:\n            print(\"Pattern\\n\\t%s\\nnot found in %s\" % (pattern, file))\n            assert False"], "sample_590": ["    def test_concat_compat_equals(self):\n        ds1 = Dataset(\n            {\"x\": (\"x\", [1, 2]), \"y\": (\"x\", [3, 4])}, coords={\"x\": [0, 1]}\n        )\n        ds2 = Dataset(\n            {\"x\": (\"x\", [1, 2, 3]), \"y\": (\"x\", [3, 4, 5])}, coords={\"x\": [0, 1, 2]}\n        )\n\n        with raises_regex(ValueError, \"arrays have different shapes\"):\n            concat([ds1, ds2], dim=\"x\", compat=\"identical\")\n\n        with raises_regex(ValueError, \"arrays have different shapes\"):\n            concat([ds1, ds2], dim=\"x\", compat=\"broadcast_equals\")\n\n\n\n", "    def test_concat_broadcast(self):\n        ds1 = Dataset({\"a\": ((\"x\", \"y\"), np.ones((2, 1))), \"b\": ((\"x\", \"y\"), np.zeros((2, 1)))}, {\"x\": [0, 1], \"y\": [0, 1]})\n        ds2 = Dataset({\"a\": ((\"x\", \"y\"), np.ones((1, 2))), \"b\": ((\"x\", \"y\"), np.ones((1, 2)))}, {\"x\": [1, 2], \"y\": [0, 1]})\n        expected = Dataset({\"a\": ((\"x\", \"y\"), np.array([[1, 1, 1, 1], [1, 1, 1, 1]])),\n                          \"b\": ((\"x\", \"y\"), np.array([[0, 0], [0, 0], [1, 1], [1, 1]])),\n                          }, {\"x\": [0, 1, 1, 2], \"y\": [0, 1, 0, 1]})\n        actual = concat([ds1, ds2], dim=\"x\")\n        assert_identical(expected, actual)\n", "    def test_concat_broadcastable_variables(self):\n        ds1 = Dataset({\"a\": ([1, 2], np.random.rand(2)), \"b\": ([1, 2], [3, 4])}, {\"dim1\": [1, 2]})\n        ds2 = Dataset({\"a\": ([1, 2], np.random.rand(2)), \"b\": ([1, 2], [5, 6])}, {\"dim1\": [1, 2]})\n\n        expected = Dataset(\n            {\"a\": ([1, 2, 1, 2], np.concatenate((ds1[\"a\"].values, ds2[\"a\"].values), axis=0)),\n             \"b\": ([1, 2, 1, 2], np.concatenate((ds1[\"b\"].values, ds2[\"b\"].values), axis=0))\n            },\n            {\"dim1\": [1, 2, 1, 2]},\n        )\n\n        actual = concat([ds1, ds2], dim=\"dim1\")\n        assert_identical(expected, actual)\n", "    def test_concat_multiindex_compat(self):\n        x = pd.MultiIndex.from_product([[1, 2, 3], [\"a\", \"b\"]])\n        ds1 = Dataset({\"x\": x, \"y\": np.random.randn(len(x))})\n        ds2 = Dataset({\"x\": x[::-1], \"y\": np.random.randn(len(x))})\n        with raises_regex(ValueError, \"coordinate in some datasets\"):\n            concat([ds1, ds2], dim=\"x\", compat=\"identical\")\n", "    def test_concat_preserve_attrs(self):\n        ds1 = Dataset({\"a\": ((\"x\", \"y\"), np.random.random((2, 3))), \"b\": 1}, attrs={\"foo\": \"bar\"})\n        ds2 = Dataset({\"a\": ((\"x\", \"y\"), np.random.random((2, 3))), \"b\": 2}, attrs={\"foo\": \"baz\"})\n        \n        concat_ds = concat([ds1, ds2], dim=\"x\", data_vars=[\"a\"])\n        assert concat_ds.attrs == {\"foo\": \"bar\"}\n\n        concat_ds = concat([ds1, ds2], dim=\"x\", data_vars=[\"a\"], compat=\"identical\")\n        assert concat_ds.attrs == {\"foo\": \"bar\"} \n"], "sample_777": ["def test_gradient_boosting_random_state_consistency(EstimatorClass):\n    X, y = datasets.make_regression(n_samples=100, random_state=0)\n    estimator1 = EstimatorClass(random_state=42)\n    estimator2 = EstimatorClass(random_state=42)\n    \n    estimator1.fit(X, y)\n    estimator2.fit(X, y)\n\n    assert_array_almost_equal(estimator1.predict(X), estimator2.predict(X))\n    assert_array_almost_equal(estimator1.feature_importances_, estimator2.feature_importances_)     \n", "    def test_gradient_boosting_missing_values():\n        # Test handling of missing values\n        X = [[1, 2, np.nan], [2, 3, 4], [np.nan, 5, 6]]\n        y = [0, 1, 1]\n        for estimator in [GradientBoostingClassifier, GradientBoostingRegressor]:\n            est = estimator(random_state=0).fit(X, y)\n            assert_array_equal(est.predict(X), [0, 1, 1])\n", "    def test_gradient_boosting_with_sparse_data(estimator):", "def test_gradient_boosting_validation_split():\n    X, y = make_classification(n_samples=1000, random_state=0)\n\n    gbc = GradientBoostingClassifier(n_estimators=100,\n                                    n_iter_no_change=10,\n                                    validation_split=0.1,\n                                    learning_rate=0.1, max_depth=3,\n                                    random_state=42)\n\n    gbc2 = clone(gbc).set_params(validation_split=0.3)\n\n    gbr = GradientBoostingRegressor(n_estimators=100, n_iter_no_change=10,\n                                    learning_rate=0.1, max_depth=3,\n                                    validation_split=0.1,\n                                    random_state=42)\n\n    gbr2 = clone(gbr).set_params(validation_split=0.3)\n\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1,\n                                                      random_state=42)\n\n    # Check if validation_split works as expected\n    gbc.fit(X_train, y_train)\n    gbc2.fit(X_train, y_train)\n    assert gbc.n_estimators_ != gbc2.n_estimators_\n\n\n\n    gbr.fit(X_train, y_train)\n    gbr2.fit(X_train, y_train)\n    assert gbr.n_estimators_ != gbr2.n_estimators_\n\n\n", "def test_gradient_boosting_with_missing_values():\n    # Test handling of missing values in input data.\n\n    X = np.array([[1, 2, np.nan],\n                  [4, 5, 6],\n                  [7, 8, np.nan]])\n    y = np.array([0, 1, 0])\n    for est in (GradientBoostingClassifier(), GradientBoostingRegressor()):\n        est.fit(X, y)\n"], "sample_317": ["    def test_empty_feed(self):\n        response = self.client.get('/syndication/empty/')\n        doc = minidom.parseString(response.content)\n        feed = doc.getElementsByTagName('rss')[0]\n        chan = feed.getElementsByTagName('channel')[0]\n        self.assertEqual(len(chan.getElementsByTagName('item')), 0)\n", "    def test_alternate_links(self):\n        response = self.client.get('/syndication/rss2/alternate-links/', **{'HTTP_ACCEPT': 'text/html'})\n        doc = minidom.parseString(response.content)\n        chan = doc.getElementsByTagName('channel')[0]\n        links = chan.getElementsByTagName('link')\n        self.assertEqual(len(links), 2)\n        self.assertEqual(links[0].getAttribute('href'), 'http://example.com/blog/')\n        self.assertEqual(links[1].getAttribute('rel'), 'alternate')\n        self.assertEqual(links[1].getAttribute('type'), 'text/html')\n        self.assertEqual(links[1].getAttribute('href'), 'http://example.com/blog/')\n", "    def test_custom_feed_generator_with_custom_attributes(self):\n        response = self.client.get('/syndication/custom_attributes/')\n        feed = minidom.parseString(response.content).firstChild\n        self.assertEqual(feed.getAttribute('django-version'), '2.2.23')\n\n\n\n", "    def test_feed_with_no_articles(self):\n        with self.settings(INSTALLED_APPS={'django.contrib.sites.apps': 'sites'}), self.subTest():\n            Site.objects.clear_cache()\n            Site.objects.create(domain='example.com', name='example.com')\n\n            # Delete all articles\n            Article.objects.all().delete()\n\n            response = self.client.get('/syndication/rss2/')\n            doc = minidom.parseString(response.content)\n            chan = doc.getElementsByTagName('channel')[0]\n            items = chan.getElementsByTagName('item')\n            self.assertEqual(len(items), 0)\n", "    def test_feed_custom_attributes(self):\n        response = self.client.get('/syndication/custom_attributes/')\n        doc = minidom.parseString(response.content)\n        feed = doc.getElementsByTagName('rss')[0]\n        self.assertEqual(feed.getAttribute('custom_attr_1'), 'value_1')\n        self.assertEqual(feed.getAttribute('custom_attr_2'), 'value_2')\n\n\n\n"], "sample_131": ["    def test_clone_test_db_with_existing_suffix(self, mocked_migrate, mocked_ensure_connection):\n        creation = connection.creation_class(connection)\n        saved_settings = copy.deepcopy(connection.settings_dict)\n        try:\n            # Simulate an existing clone\n            test_connection = self.get_connection_copy()\n            test_connection.settings_dict['NAME'] = 'existing_clone'\n            connections['existing_clone'] = test_connection\n\n            with mock.patch.object(creation, '_clone_test_db'):\n                creation.clone_test_db(suffix='_1', verbosity=0, autoclobber=True, keepdb=False)\n            # Assert that the clone method is called\n        finally:\n            connection.settings_dict = saved_settings\n            del connections['existing_clone']\n", "    def test_destroy_test_db_preserves_existing_db(self, mocked_migrate, mocked_ensure_connection):\n        creation = connection.creation_class(connection)\n        saved_settings = copy.deepcopy(connection.settings_dict)\n        try:\n            test_db_name = 'preserved_test'\n            connection.settings_dict['TEST'] = {'NAME': test_db_name}\n            creation.destroy_test_db(old_database_name=test_db_name, keepdb=True)\n            self.assertEqual(connection.settings_dict['NAME'], test_db_name)\n        finally:\n            connection.settings_dict = saved_settings\n", "    def test_destroy_test_db_with_keepdb(self, mocked_migrate, mocked_ensure_connection):\n        creation = connection.creation_class(connection)\n        saved_settings = copy.deepcopy(connection.settings_dict)\n        try:\n            connection.settings_dict['TEST']['NAME'] = 'hodor'\n            with mock.patch.object(creation, '_destroy_test_db'):\n                creation.destroy_test_db(old_database_name='default', verbosity=0, keepdb=True)\n            mocked_destroy_test_db = mock.call(test_database_name='hodor', verbosity=0)\n            self.assertFalse(mocked_destroy_test_db.called)  \n        finally:\n            connection.settings_dict = saved_settings\n", "    def test_clone_test_db(self, mocked_migrate, mocked_ensure_connection):\n        creation = connection.creation_class(connection)\n        saved_settings = copy.deepcopy(connection.settings_dict)\n        try:\n            connection.settings_dict['TEST']['MIGRATE'] = True\n            with mock.patch.object(creation, '_clone_test_db'):\n                creation.clone_test_db(suffix='1', verbosity=0, autoclobber=True, keepdb=False)\n            mocked_migrate.assert_called_once()\n        finally:\n            connection.settings_dict = saved_settings\n", "    def test_create_test_db_autoclobber(self, mocked_migrate, mocked_ensure_connection):\n        creation = connection.creation_class(connection)\n        saved_settings = copy.deepcopy(connection.settings_dict)\n        try:\n            connection.settings_dict['TEST']['NAME'] = 'existing_test_db'\n            with mock.patch.object(creation, '_create_test_db') as mocked_create_test_db:\n                creation.create_test_db(verbosity=0, autoclobber=True)\n            mocked_create_test_db.assert_called_once()\n            mocked_migrate.assert_called_once()\n        finally:\n            connection.settings_dict = saved_settings\n"], "sample_514": ["def test_colorbar_fractional_position():\n    fig, ax = plt.subplots()\n    pc = ax.pcolormesh(np.ones(100).reshape(10, 10))\n    cb = fig.colorbar(pc, ax=ax, fraction=0.1, pad=0.04)\n    assert isinstance(cb.ax.get_position(), BoundingBox)\n    # check that the position is within the figure limits\n    assert 0 <= cb.ax.get_position().x <= 1\n    assert 0 <= cb.ax.get_position().y <= 1\n    assert cb.ax.get_position().width > 0\n    assert cb.ax.get_position().height > 0\n", "compilation error", "def test_colorbar_fractional_location_and_shrink():\n    fig, ax = plt.subplots()\n    pc = ax.pcolormesh(np.arange(100).reshape(10, 10))\n    cb = fig.colorbar(pc, ax=ax, fraction=0.2, shrink=0.8,\n                      location='right')\n    assert cb.ax.get_window_extent().width / fig.get_figwidth() == 0.2\n    assert cb.ax.get_window_extent().height / fig.get_figheight() == 0.8\n\n\n", "def test_colorbar_set_label_updates_title():\n    fig, ax = plt.subplots()\n    pc = ax.pcolormesh(np.random.randn(10, 10))\n    cb = fig.colorbar(pc)\n    cb.set_label('A label')\n    fig.draw_without_rendering()\n    assert cb.ax.get_title() == 'A label'\n\n\n", "def test_colorbar_spacing():\n    fig, axs = plt.subplots(1, 2, figsize=(8, 3))\n    for i, orient in enumerate(['vertical', 'horizontal']):\n        pc = axs[i].pcolormesh(np.random.randn(10, 10))\n        cb = fig.colorbar(pc, cax=axs[i], orientation=orient,\n                          spacing='uniform', extend='both')\n        cb2 = fig.colorbar(pc, cax=axs[i], orientation=orient,\n                           spacing='proportional', extend='both')\n\n    fig.draw_without_rendering()\n    # make sure the spacing is as expected\n    for i, orient in enumerate(['vertical', 'horizontal']):\n        pos1 = cb.ax.get_position()\n        pos2 = cb2.ax.get_position()\n        if orient == 'vertical':\n            assert pos1.height < pos2.height\n        else:\n            assert pos1.width < pos2.width\n\n\n\n"], "sample_659": ["    def test_raises_with_different_input_types(\n        self, input_type, expected_type", "    def test_raises_with_custom_traceback(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import traceback\n            import pytest\n\n                return \"My custom traceback\"\n\n            @pytest.raises(ZeroDivisionError)\n            @pytest.mark.parametrize(\"enable_custom_traceback\", [True, False])\n                if enable_custom_traceback:\n                    traceback.print_exc = my_custom_traceback\n                1 / 0\n            \"\"\"\n        )\n        result = testdir.runpytest()\n        if result.retcode == 0:\n            result.stdout.fnmatch_lines([\"*2 passed*\"])\n        else:\n            result.stderr.fnmatch_lines([\"*Error: Custom traceback function not working as expected.*\"])\n\n\n", "    def test_raises_contextmanager_with_non_zero_exitstatus(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n                with pytest.raises(ValueError) as excinfo:\n                    1/0\n                assert excinfo.type == ValueError\n                assert excinfo.value.args[0] == 'division by zero'\n        \"\"\"\n        )\n        result = testdir.runpytest()\n        result.stdout.fnmatch_lines([\"*1 passed*\"])\n", "    def test_raises_custom_excinfo(self):\n        class MyCustomException(Exception):\n            pass\n        try:\n            with pytest.raises(MyCustomException, match=\"expected\"):\n                raise MyCustomException(\"expected\")\n        except pytest.raises.Exception as excinfo:\n            assert excinfo.type is MyCustomException\n            assert excinfo.value.args[0] == \"expected\"\n        else:\n            assert False, \"Expected pytest.raises.Exception\"\n\n", "    def test_raises_with_unicode_arg(self):\n        with pytest.raises(TypeError, match=r\"int\\(\\) argument must be a string or a number, not 'u'.*\"):\n            int(u\"10\")\n"], "sample_819": ["compilation error", "compilation error", "def test_handle_none_estimators():\n    # Test handling of None estimators during fit.\n    eclf = VotingClassifier()\n    try:\n        eclf.fit(X, y)\n    except ValueError as e:\n        assert 'at least one estimator' in str(e)\n", "def test_voting_classifier_with_precomputed_transform():\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([0, 1, 0])\n\n    class MockClassifier:\n            pass\n\n            return np.zeros(len(X))\n\n            return np.zeros((len(X), 2))\n\n            return np.eye(2)\n\n    clf1 = MockClassifier()\n    clf2 = MockClassifier()\n    eclf = VotingClassifier(estimators=[('clf1', clf1), ('clf2', clf2)],\n                            voting='hard')\n    eclf.fit(X, y)\n    assert_array_equal(eclf.transform(X), np.hstack((np.eye(2), np.eye(2))))\n\n\n", "compilation error"], "sample_736": ["def test_lgb_logloss():\n    # Test that the log loss is computed correctly using the lgb solver\n    rng = np.random.RandomState(42)\n    n_samples = 50\n    X, y = make_classification(n_samples=n_samples, n_features=20, random_state=0)\n    lr = LogisticRegression(solver='lgb', multi_class='ovr')\n    lr.fit(X, y)\n\n    y_pred_proba = lr.predict_proba(X)\n    log_loss_val = log_loss(y, y_pred_proba)\n    assert_almost_equal(lr.loss_, log_loss_val)\n\n\n\n", "def test_lbfgs_with_small_datasets():\n    # Test L-BFGS with small datasets where its performance is known to be\n    # suboptimal due to its memory footprint.\n    for n_samples in [10, 50, 100]:\n        X, y = make_classification(n_samples=n_samples, n_features=20,\n                                   random_state=0)\n        lr = LogisticRegression(solver='lbfgs', tol=1e-4, max_iter=100)\n        lr.fit(X, y)\n        assert_greater(lr.n_iter_, 0)\n\n\n\n", "def test_logistic_regression_sample_weight_and_class_weight():\n    # Test for sample_weight and class_weight combination\n    X, y = make_classification(n_samples=100, n_features=5, random_state=0)\n    # Create sample weights\n    sample_weights = np.ones(X.shape[0]) * 2\n    sample_weights[50:75] *= 0.5\n\n    # Create class weights\n    class_weight = {0: 0.5, 1: 1.5}\n\n    # Test with L1 penalty and liblinear solver\n    lr_liblinear = LogisticRegression(penalty=\"l1\", solver='liblinear',\n                max_iter=1000, random_state=42)\n    lr_liblinear.fit(X, y, sample_weight=sample_weights,\n                    class_weight=class_weight)\n    assert_array_almost_equal(lr_liblinear.coef_, lr_liblinear.coef_, 3)\n\n\n    # Test with L2 penalty and newton-cg solver\n    lr_newton_cg = LogisticRegression(penalty=\"l2\", solver='newton-cg',\n                max_iter=1000, random_state=42)\n    lr_newton_cg.fit(X, y, sample_weight=sample_weights,\n                    class_weight=class_weight)\n    assert_array_almost_equal(lr_newton_cg.coef_, lr_newton_cg.coef_, 3)\n\n    # Test with L1 penalty and saga solver\n    lr_saga = LogisticRegression(penalty=\"l1\", solver='saga',\n                max_iter=1000, random_state=42)\n    lr_saga.fit(X, y, sample_weight=sample_weights,\n                    class_weight=class_weight)\n    assert_array_almost_equal(lr_saga.coef_, lr_saga.coef_, 3)\n", "def test_logreg_class_weight_with_sparse_data():\n    # Test that class weights work correctly with sparse data\n\n    rng = np.random.RandomState(42)\n    n_samples = 50\n    n_features = 20\n    X, y = make_classification(n_samples=n_samples, n_features=n_features,\n                               random_state=0)\n    X_sparse = sparse.csr_matrix(X)\n    \n    class_weight = {0: 1.0, 1: 2.0}\n    \n\n    lr_sparse = LogisticRegression(class_weight=class_weight, solver='saga',\n                                  fit_intercept=False)\n    lr_sparse.fit(X_sparse, y)\n\n    lr_dense = LogisticRegression(class_weight=class_weight, solver='saga',\n                                 fit_intercept=False)\n    lr_dense.fit(X, y)\n\n    assert_array_almost_equal(lr_sparse.coef_, lr_dense.coef_)\n\n\n\n", "compilation error"], "sample_1005": ["def test_issue_14715():\n    from sympy.matrices import Matrix\n    a = Matrix([[1, 2], [3, 4]])\n    b = Matrix([[5, 6], [7, 8]])\n    assert latex(a*b) == r\"\\left(\\begin{array}{cc}1 & 2\\\\3 & 4\\end{array}\\right) \\left(\\begin{array}{cc}5 & 6\\\\7 & 8\\end{array}\\right)\" \n", "def test_OuterProduct_printing():\n    from sympy.diffgeom.linegeom import Line\n    from sympy.diffgeom import OuterProduct\n    a = Line(1, 2)\n    b = Line(3, 4)\n    op = OuterProduct(a, b)\n    assert latex(op) == r\"\\mathrm{d}x \\otimes \\mathrm{d}y\"\n", "def test_Derivative_printing():\n    from sympy.diffgeom.rn import R2\n    from sympy.diffgeom import Derivative\n    x, y = symbols('x y')\n    assert latex(Derivative(x, x)) == r\"\\frac{d x}{d x}\"\n    assert latex(Derivative(x, y)) == r\"\\frac{\\partial x}{\\partial y}\"\n    assert latex(Derivative(sin(x), x)) == r\"\\cos{\\left (x \\right )}\"\n    assert latex(Derivative(sin(x), y)) == r\"\\frac{\\partial}{\\partial y} \\sin{\\left (x \\right )}\"\n    assert latex(Derivative(sin(x*y), x)) == r\"\\cos{\\left (x y \\right )} y\"\n    assert latex(Derivative(sin(x*y), y)) == r\"x \\cos{\\left (x y \\right )}\"\n    d = Derivative(sin(x*y), (x, y))\n    assert latex(d) == r\"\\frac{\\partial^{2} \\sin{\\left (x y \\right )}}{\\partial x \\partial y}\"\n\n\n\n", "def test_Pow_issue_14742():\n    x = symbols('x')\n    assert latex(Pow(x, 1/2)) == r\"x^{\\frac{1}{2}}\"\n\n\n", "compilation error"], "sample_240": ["    def test_token_expiry(self):\n        user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n        now = datetime.now()\n        p0 = MockedPasswordResetTokenGenerator(now)\n        tk1 = p0.make_token(user)\n        \n        # Test token expiry\n        time_threshold = settings.PASSWORD_RESET_TIMEOUT + 1  # One second past the timeout\n        p1 = MockedPasswordResetTokenGenerator(now + timedelta(seconds=time_threshold))\n        self.assertIs(p1.check_token(user, tk1), False)\n", "    def test_token_generation_with_custom_email_field(self):\n        from django.contrib.auth.models import User\n        from .models import CustomEmailField\n\n        class CustomModel(User):\n            email = CustomEmailField(blank=True)\n\n        user = CustomModel.objects.create_user(\n            'custom_email_user', email='test5email@example.com', password='testpw'\n        )\n        p0 = PasswordResetTokenGenerator()\n        tk1 = p0.make_token(user)\n        self.assertIs(p0.check_token(user, tk1), True)\n", "    def test_token_deprecation_warning(self):\n\n        # Ensure a warning is raised when DEFAULT_HASHING_ALGORITHM is not set.\n        user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n        with self.settings(DEFAULT_HASHING_ALGORITHM=None):\n            with self.assertLogs(level='WARNING') as cm:\n                generator = PasswordResetTokenGenerator()\n                generator.make_token(user)\n            self.assertIn(\n                \"DEFAULT_HASHING_ALGORITHM is no longer configured, please set it explicitly\",\n                cm.output[0],\n            )\n\n\n\n", "    def test_token_generation_with_custom_email_field(self):\n        \"\"\"\n        Test token generation with a custom email field.\n        \"\"\"\n        CustomEmailField.objects.create(user=User.objects.create_user('custom_email_user', 'custom_email@example.com', 'testpw'))\n        p0 = PasswordResetTokenGenerator()\n        token = p0.make_token(User.objects.get(username='custom_email_user'))\n        self.assertIs(p0.check_token(User.objects.get(username='custom_email_user'), token), True)\n", "    def test_token_expiration(self):\n        user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n        p0 = PasswordResetTokenGenerator()\n        tk1 = p0.make_token(user)\n\n        # Check that the token is valid immediately after creation.\n        self.assertIs(p0.check_token(user, tk1), True)\n\n        # Advance the mocked time past the password reset timeout.\n        with self.settings(PASSWORD_RESET_TIMEOUT=60):\n            p1 = MockedPasswordResetTokenGenerator(datetime.now() + timedelta(seconds=settings.PASSWORD_RESET_TIMEOUT + 1))\n            self.assertIs(p1.check_token(user, tk1), False)\n"], "sample_853": ["def test_transform_target_regressor_multi_output_sample_weight():\n    X, y = make_regression(n_samples=100, n_features=5, n_informative=3,\n                          random_state=0,\n                          multi_output=True)\n    sample_weight = np.random.rand(100)\n\n    regr = TransformedTargetRegressor(\n        regressor=LinearRegression(), transformer=StandardScaler()\n    )\n    regr.fit(X, y, sample_weight=sample_weight)\n\n\n", "def test_transform_target_regressor_copy_regressor():\n    X, y = friedman\n    regr = LinearRegression()\n    ttr = TransformedTargetRegressor(regressor=regr,\n                                     transformer=DummyTransformer())\n    ttr.fit(X, y)\n    assert regr is not ttr.regressor_\n\n\n\n", "def test_transform_target_regressor_y_multivariate_categorical():\n    X, y = datasets.make_classification(n_samples=100, n_features=20,\n                n_informative=10, n_redundant=5, n_clusters_per_class=1,\n                random_state=42)\n    y = y.astype('category')\n    regr = TransformedTargetRegressor(regressor=DummyRegressor(),\n                    transformer=FunctionTransformer(\n                        func=lambda x: x.astype(np.float64)))\n    with pytest.raises(ValueError,\n                       match=\"y cannot be of type 'category'\"):\n        regr.fit(X, y)\n\n", "def test_transform_target_regressor_single_transformer_as_string():\n    from sklearn.pipeline import Pipeline\n\n    X, y = friedman\n    regr = TransformedTargetRegressor(\n        regressor=LinearRegression(),\n        transformer=\"StandardScaler\"\n    )\n    pipeline = Pipeline([('trans', regr)])\n    pipeline.fit(X, y)\n    assert isinstance(pipeline.named_steps['trans'].transformer_, StandardScaler)\n\n\n", "def test_transform_target_regressor_sparse_data():\n    X, y = make_regression(n_samples=100, random_state=0, n_features=10,\n                           sparse_X=True)\n    regr = TransformedTargetRegressor(regressor=LinearRegression(),\n                transformer=StandardScaler())\n    regr.fit(X, y)\n    assert regr.transformer_.fit_transform(y).nnz == y.nnz\n"], "sample_1174": ["compilation error", "compilation error", "compilation error", "compilation error", "def test_issue_15893_polar():\n    from sympy import polar_lift\n    f = Function('f', real=True)\n    x = Symbol('x', real=True)\n    z = polar_lift(f(x))\n    eq = Derivative(z.abs(), z)\n    assert eq.doit() == sign(f(x)) \n"], "sample_485": ["    def test_urlize_nofollow(self):\n        tests = (\n            (\n                \"Search for <a href='http://google.com'>google.com</a>.\",\n                'Search for <a href=\"http://google.com\" rel=\"nofollow\">google.com</a>.'\n            ),\n        )\n        for value, output in tests:\n            with self.subTest(value=value):\n                self.assertEqual(urlize(value, nofollow=True), output)\n\n", "    def test_urlize_with_trim_url_limit(self):\n        self.assertEqual(urlize(\"long long url https://www.example.com/this-is-a-very-very-long-url-that-should-be-trimmed\", trim_url_limit=10),\n            \"long long url <a href=\\\"https://www.example.com/this-is-a-very-very-long-url-that-should-be-trimmed\\\">...</a>\")\n", "    def test_urlize_mailto_relative_path(self):\n        self.assertEqual(urlize(\"mail.to:user@example.local\"), \"<a href=\\\"mailto:user@example.local\\\">mail.to:user@example.local</a>\")\n\n\n\n", "    def test_urlize_trim_url_limit(self):\n        tests = (\n            (\"http://www.longurl.com/this-is-a-very-very-long-url-that-needs-to-be-truncated\",\n             'http://www.longurl.com/this-is-a-very-very-long-url-that-needs-to-...'),\n            (\"http://www.example.com/this-is-a-much-shorter-url\",\n             \"http://www.example.com/this-is-a-much-shorter-url\"),\n            (\"http://example.com/path/to/long/url\",\n             \"http://example.com/path/to/long/url\"),\n        )\n        for original_url, truncated_url in tests:\n            with self.subTest(original_url=original_url, truncated_url=truncated_url):\n                self.assertEqual(urlize(original_url, trim_url_limit=30), truncated_url)\n", "    def test_urlize_non_alphanumeric_with_trailing_punctuation(self):\n        for value in (\n            \"!@#$%^&*()_+`~-=\\[\\]{};':\\\"\\\\|,.<>/?\",\n            \"!@#$%^&*()_+`~-=\\[\\]{};':\\\"\\\\|,.<>/?.\",\n            \"!@#$%^&*()_+`~-=\\[\\]{};':\\\"\\\\|,.<>/?!\",\n        ):\n            with self.subTest(value=value):\n                self.assertEqual(urlize(value, nofollow=True), value)\n"], "sample_948": ["compilation error", "compilation error", "def test_build_domain_cpp_warn_no_type_ref(app, status, warning):\n    app.builder.build_all()\n    ws = filter_warnings(warning, \"warn-no-type-ref\")\n    assert len(ws) == 1\n    assert \"WARNING: cpp:no type reference found: \" in ws[0]\n", "def test_build_domain_cpp_with_multiple_declarations_of_same_identifier(app, status, warning):\n    app.builder.build_all()\n    ws = warning.getvalue().split(\"\\n\")\n    assert len(ws) == 2\n    assert \"WARNING: cpp:identifier reference target not found: @a\" in ws[0]\n    assert \"WARNING: cpp:identifier reference target not found: @b\" in ws[1]\n", "compilation error"], "sample_645": ["def test_log_file_path(caplog: caplog, pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n            caplog.set_log_path(\"test.log\")\n            logging.getLogger().info(\"This should go to test.log\")\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    assert \"This should go to test.log\" in open(\"test.log\").read()\n", "def test_log_file_level_inheritance(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n            plugin = request.config.pluginmanager.getplugin('logging-plugin')\n            assert plugin.log_file_level == logging.DEBUG\n\n            logger = logging.getLogger('catchlog')\n            logger.debug('DEBUG message')\n            logger.info('INFO message')\n            logger.warning('WARNING message')\n            logger.error('ERROR message')\n            logger.critical('CRITICAL message')\n\n    \"\"\"\n    )\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        log_file_level=WARNING\n    \"\"\"\n    )\n\n    result = pytester.runpytest()\n    result.stdout.no_fnmatch_line(\"*DEBUG message*\")\n    result.stdout.fnmatch_lines([\"*INFO message*\", \"*WARNING message*\", \"*ERROR message*\", \"*CRITICAL message*\"])\n    assert result.ret == 0\n", "    def test_empty_log_capture(caplog):\n        with caplog.at_level(logging.INFO):\n            pass\n        assert not caplog.messages\n\n\n", "def test_log_format_with_custom_formatter(caplog, pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n\n            caplog.set_level(logging.INFO)\n            logging.info(\"info message with custom format\")\n    \"\"\"\n    )\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        logging_format = \"[%(asctime)s] [%(-8levelname)s] %(message)s\"\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines(\n        [\"* [INFO] info message with custom format*\",]\n    )\n\n\n\n", "def test_log_file_level(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n        import pytest\n\n            caplog.set_level(logging.INFO)\n            logger = logging.getLogger(__name__)\n            logger.info(\"INFO\")\n            logger.debug(\"DEBUG\")\n    \"\"\"\n    )\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        log_file=log.txt\n        log_file_level=DEBUG\n    \"\"\"\n    )\n\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines([\"*INFO\", \"*DEBUG*\"])\n    assert os.path.exists(\"log.txt\") and os.path.getsize(\"log.txt\") > 0\n"], "sample_610": ["def test_infer_freq_from_scalar_cftime(freq, calendar):\n    cftime_value = xr.cftime_range(\"2000-01-01\", periods=1, freq=freq, calendar=calendar)[0]\n    out = xr.infer_freq(cftime_value)\n    assert out == freq\n\n", "def test_infer_freq_invalid(freq, calendar):\n    indx = xr.cftime_range(\"2000-01-01\", periods=3, freq=freq, calendar=calendar)\n    with pytest.raises(ValueError, match=\"could not infer a frequency\"):\n        xr.infer_freq(indx) \n", "def test_infer_freq_from_datetimes():\n    date_times = [\n        datetime.datetime(2000, 1, 1, 0, 0, 0),\n        datetime.datetime(2000, 1, 2, 0, 0, 0),\n        datetime.datetime(2000, 1, 3, 0, 0, 0),\n    ]\n    indx = xr.DataArray(date_times)\n    assert xr.infer_freq(indx) == \"D\"\n\n\n", "def test_cftime_from_datetimelike():\n    date_type = xr.cftime.DatetimeNoLeap\n    from datetime import datetime\n\n    idx = xr.cftime_from_datetimelike(datetime(2000, 1, 1), calendar=\"noleap\")\n    assert isinstance(idx, date_type)\n    assert idx.date() == datetime(2000, 1, 1).date()\n", "def test_infer_freq_mixed_freq():\n    cf_indx = xr.cftime_range(\"2000-01-01\", periods=3, freq=\"D\")\n    mixed_indx = xr.concat([cf_indx, cf_indx.shift(periods=1, freq=\"M\")], dim=0)\n    with pytest.raises(ValueError, match=\"Cannot infer frequency from mixed\"):\n        xr.infer_freq(mixed_indx)\n"], "sample_796": ["def test_huber_with_intercept_large_scale():\n    # Test that HuberRegressor handles large scale values without issues\n\n    X, y = make_regression_with_outliers()\n    # Scale up X by a large factor\n    X_scaled = X * 1000.0\n\n    huber = HuberRegressor(\n        fit_intercept=True, alpha=0.01, max_iter=10000, epsilon=1.35)\n    huber.fit(X_scaled, y)\n    assert not np.isnan(huber.coef_).any()\n    assert not np.isnan(huber.intercept_)\n    assert huber.scale_ > 0.0\n", "def test_huber_with_intercept():\n    X, y = make_regression_with_outliers()\n\n    # Make sure that the intercept gets set to the correct value\n    huber = HuberRegressor(fit_intercept=True)\n    huber.fit(X, y)\n    assert huber.intercept_ is not None and not np.isnan(huber.intercept_)\n\n    # Test that the intercept is zero when fit_intercept is False\n    huber_no_intercept = HuberRegressor(fit_intercept=False)\n    huber_no_intercept.fit(X, y)\n    assert huber_no_intercept.intercept_ == 0.0\n\n\n", "    def test_huber_intercept():\n        # Test that intercept is estimated correctly\n        X, y = make_regression_with_outliers(n_samples=20, n_features=1)\n        huber = HuberRegressor(fit_intercept=True, alpha=0.0)\n        huber.fit(X, y)\n        # Make sure the intercept is reasonably close to 0\n        assert_almost_equal(huber.intercept_, 0.0, 3)\n\n\n\n", "def test_huber_with_missing_values():\n    # Test that the model handles missing values, if present in both X and y\n    X, y = make_regression_with_outliers(n_samples=50, n_features=20)\n    n_missing = 10\n    # Introduce missing values in X and y\n    rng = np.random.RandomState(0)\n    missing_idx = rng.choice(X.shape[0], n_missing, replace=False)\n    X[missing_idx] = np.nan\n    y[missing_idx] = np.nan\n    huber = HuberRegressor(fit_intercept=True)\n    huber.fit(X, y)\n    assert huber.n_iter_ > 0\n", "def test_huber_with_sparse_input():\n    # Test handling of sparse input data\n    n_samples = 100\n    n_features = 50\n    X = sparse.csr_matrix((np.random.rand(n_samples, n_features),\n                           (np.random.randint(n_features, size=n_samples),\n                            np.random.randint(n_features, size=n_samples))),\n                          shape=(n_samples, n_features))\n    y = np.random.rand(n_samples)\n    huber = HuberRegressor(fit_intercept=True, alpha=0.01, max_iter=100)\n    huber.fit(X, y)\n    \n    # Check that the coefficients are close to what we'd get with a dense matrix\n    dense_X = X.toarray()\n    dense_huber = HuberRegressor(fit_intercept=True, alpha=0.01, max_iter=100)\n    dense_huber.fit(dense_X, y)\n    assert_array_almost_equal(huber.coef_, dense_huber.coef_, 3)\n"], "sample_641": ["def test_empty_path(path: str) -> None:\n    save_results(LinterStats(), path)\n    loaded = load_results(path)\n    assert loaded is None\n\n\n", "def test_load_results_file_not_found(\n    path: str, recur: int, pylint_home: Path, expected: Path", "def test_save_and_load_nonexistent_file(\n    path: str, recur: int, pylint_home: Path", "def test_load_results_nonexistent_file(path: str) -> None:\n    assert load_results(path) is None\n", "def test_save_and_load_empty_path(path: str) -> None:\n    with pytest.raises(ValueError) as ex:\n        _get_pdata_path(Path(path), 1)\n    assert \"Path must be provided\" in str(ex.value)\n"], "sample_915": ["    def test_getdoc_classmethod(app):\n        from target.methods import Base\n\n        assert inspect.getdoc(Base.classmeth, getattr, False, Base, 'classmeth') is None\n        assert inspect.getdoc(Base.classmeth, getattr, True, Base, 'classmeth') == \"Class method docstring\"\n", "        def meth(self):\n            pass\n", "def test_getdoc_missing_docstring():\n    class Foo:\n            pass\n\n    assert inspect.getdoc(Foo.meth, getattr, False, Foo, \"meth\") is None\n    assert inspect.getdoc(Foo.meth, getattr, True, Foo, \"meth\") is None\n", "def test_getdoc_bound_method():\n    class Foo:\n            \"\"\"docstring.\"\"\"\n\n    obj = Foo()\n    bound_meth = obj.meth\n\n    assert inspect.getdoc(bound_meth) == \"docstring.\"\n\n\n", "        def meth(self):\n            pass\n"], "sample_551": ["def test_poly_collection_3d():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    polygons = [\n        [[0, 0, 0], [1, 0, 0], [1, 1, 0], [0, 1, 0]],\n        [[0, 0, 1], [1, 0, 1], [1, 1, 1], [0, 1, 1]]\n    ]\n    poly_coll = Poly3DCollection(polygons, facecolors='r', edgecolors='b')\n    ax.add_collection3d(poly_coll)\n    ax.set_xlim(0, 2)\n    ax.set_ylim(0, 2)\n    ax.set_zlim(0, 2)\n    plt.show()\n", "def test_Poly3DCollection_label_inheritance():\n    verts = np.asarray([[0, 0, 0], [0, 1, 1], [1, 0, 1]])\n    mesh = art3d.Poly3DCollection([verts], label=\"original label\")\n    fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n    mesh.set_edgecolor('k')\n    handle = ax.add_collection3d(mesh)\n    ax.set_title(\"Original Label\")\n    leg = ax.legend()\n    assert leg.get_title() == \"Original Label\"\n\n\n\n\n", "def test_poly_legend():\n    fig, ax = plt.subplots(subplot_kw=dict(projection='3d'))\n    verts = np.array([[(0, 0, 0), (1, 0, 0), (1, 1, 0), (0, 1, 0)],\n                      [(0, 0, 1), (1, 0, 1), (1, 1, 1), (0, 1, 1)]])\n    poly3d = Poly3DCollection(verts, label='poly3d', facecolors='b', edgecolors='k')\n    ax.add_collection3d(poly3d)\n    ax.legend()\n", "def test_line3d_collection_with_codes():\n    fig, ax = plt.subplots(subplot_kw=dict(projection='3d'))\n    lines = [[(0, 0, 0), (1, 1, 1)], [(0, 1, 0), (1, 0, 1)]]\n    codes = [Line2D.MOVETO, Line2D.CURVE3]\n    collection = art3d.Line3DCollection(lines, facecolors='r', edgecolors='k',\n                                      linewidths=3, linestyles='-',\n                                      codes=codes)\n    ax.add_collection(collection)\n\n    fig.show()\n\n\n", "def test_polycollection_3d_legend():\n    fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n    verts = np.array([[0, 0, 0], [1, 0, 0], [1, 1, 0], [0, 1, 0]])\n    codes = [\n        'M', 'L', 'L', 'C',\n        'M', 'L', 'L', 'C'\n    ]\n    colors = ['blue', 'red', 'green', 'orange']\n    poly_collection = Poly3DCollection(verts, facecolors=colors, edgecolors='k', linestyle='-', linewidth=1)\n    ax.add_collection3d(poly_collection)\n    ax.legend()\n"], "sample_705": ["compilation error", "def test_pytester_outcomes_skipped(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.skip\n            pass\n\n            pass\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1, skipped=1)\n", "compilation error", "compilation error", "compilation error"], "sample_260": ["    def test_optimize_with_fk_to_deleted_model(self):\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel('Foo', [('name', models.CharField(max_length=255))]),\n                migrations.CreateModel('Bar', [('other', models.ForeignKey('Foo', models.CASCADE))]),\n                migrations.DeleteModel('Foo'),\n            ],\n            [],\n        ) \n\n", "    def test_no_optimization_for_non_matching_operations(self):\n        self.assertDoesNotOptimize(\n            [\n                migrations.CreateModel(\"Foo\", [(\"name\", models.CharField(max_length=255))]),\n                migrations.CreateModel(\"Bar\", [(\"size\", models.IntegerField())]),\n                migrations.RenameModel(\"Foo\", \"Phou\"),\n                migrations.CreateModel(\"Baz\", [(\"name\", models.CharField(max_length=255))]),\n            ],\n            [\n                migrations.CreateModel(\"Phou\", [(\"name\", models.CharField(max_length=255))]),\n            ],\n        )\n\n\n\n", "    def test_create_model_add_field_with_constraints(self):\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel('Foo', [\n                    ('name', models.CharField(max_length=255)),\n                ]),\n                migrations.AddField('Foo', 'age', models.IntegerField(unique=True)),\n            ],\n            [\n                migrations.CreateModel('Foo', [\n                    ('name', models.CharField(max_length=255)),\n                    ('age', models.IntegerField(unique=True)),\n                ]),\n            ],\n        )\n\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel('Foo', [\n                    ('name', models.CharField(max_length=255)),\n                ]),\n                migrations.AddField('Foo', 'age', models.IntegerField(null=True)),\n            ],\n            [\n                migrations.CreateModel('Foo', [\n                    ('name', models.CharField(max_length=255)),\n                    ('age', models.IntegerField(null=True)),\n                ]),\n            ],\n        )\n\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel('Foo', [\n                    ('name', models.CharField(max_length=255)),\n                ]),\n                migrations.AddField('Foo', 'age', models.IntegerField(blank=True)),\n            ],\n            [\n                migrations.CreateModel('Foo', [\n                    ('name', models.CharField(max_length=255)),\n                    ('age', models.IntegerField(blank=True)),\n                ]),\n            ],\n        )\n", "    def test_create_model_add_field_with_fk(self):\n        \"\"\"\n        AddField with a FK should optimize into CreateModel\n        \"\"\"\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel('Foo', [('name', models.CharField(max_length=255))]),\n                migrations.CreateModel('Bar', [('foo', models.ForeignKey('migrations.Foo', models.CASCADE))]),\n            ],\n            [\n                migrations.CreateModel('Foo', [('name', models.CharField(max_length=255))]),\n                migrations.CreateModel('Bar', [('foo', models.ForeignKey('migrations.Foo', models.CASCADE))]),\n            ],\n        )\n", "    def test_order_with_respect_to_optimization(self):\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel('Foo', [('name', models.CharField(max_length=255))]),\n                migrations.CreateModel('Bar', [('size', models.IntegerField())]),\n                migrations.AlterOrderWithRespectTo('Foo', 'Bar'),\n                migrations.CreateModel('Baz', [('name', models.CharField(max_length=255))]),\n                migrations.AlterOrderWithRespectTo('Baz', 'Foo'),\n            ],\n            [\n                migrations.CreateModel('Foo', [('name', models.CharField(max_length=255))]),\n                migrations.CreateModel('Bar', [('size', models.IntegerField())]),\n                migrations.CreateModel('Baz', [('name', models.CharField(max_length=255))]),\n            ],\n        )\n\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel('Foo', [('name', models.CharField(max_length=255))]),\n                migrations.CreateModel('Bar', [('size', models.IntegerField())]),\n                migrations.AlterOrderWithRespectTo('Foo', 'Bar'),\n                migrations.AlterOrderWithRespectTo('Bar', 'Foo'),\n            ],\n            [\n                migrations.CreateModel('Foo', [('name', models.CharField(max_length=255))]),\n                migrations.CreateModel('Bar', [('size', models.IntegerField())]),\n            ],\n        )\n"], "sample_482": ["    def test_safe(self):\n        output = self.engine.render_to_string(\n            \"escapeseq_safe\",\n            {\"a\": [\"x&y\", \"<p>\"], \"b\": [mark_safe(\"x&y\"), mark_safe(\"<p>\")]},\n        )\n        self.assertEqual(output, \"x&amp;y, &lt;p&gt; -- x&amp;y, &lt;p&gt;\")\n\n", "    def test_chain_join_safe(self):\n        output = self.engine.render_to_string(\n            \"escapeseq_safe\", {\"a\": [mark_safe(\"x&y\"), mark_safe(\"<p>\")]},\n        )\n        self.assertEqual(output, \"x&amp;y<br/>&lt;p&gt;\")\n", "    def test_safe(self):\n        output = self.engine.render_to_string(\"escapeseq_safe\",\n                                             {\"a\": mark_safe(\"<p>\")})\n        self.assertEqual(output, \"<p>\")\n", "    def test_list(self):\n        output = self.engine.render_to_string(\"escapeseq_list\", {\"my_list\": [\"x&y\", \"<p>\"]})\n        self.assertEqual(output, \"x&amp;y, &lt;p&gt;\")\n\n", "    def test_safe_input(self):\n        output = self.engine.render_to_string(\"escapeseq_safe\", {\"a\": [\"x&y\", \"<p>\"]})\n        self.assertEqual(output, \"x&y, <p>\")\n\n"], "sample_395": ["    def test_get_template_directories_nested(self):\n        self.assertSetEqual(\n            autoreload.get_template_directories(),\n            {\n                ROOT / \"templates_extra\",\n                ROOT / \"templates_nested\",\n                ROOT / \"templates_nested\" / \"subdir\",\n            },\n        )\n", "    def test_get_template_directories_with_app_dirs(self):\n        self.assertSetEqual(\n            autoreload.get_template_directories(),\n            {\n                Path.cwd() / \"template_tests/templates\",\n            },\n        )\n", "    def test_get_template_directories_no_dirs_in_settings(self):\n        self.assertSetEqual(\n            autoreload.get_template_directories(),\n            set(),\n        )\n", "    def test_custom_template_loader(self):\n        mock_custom_loader = mock.MagicMock()\n        mock_custom_loader.get_dirs.return_value = [ROOT / \"custom_templates\"]\n        with mock.patch(\n            \"django.template.backends.django.DjangoTemplates.engine\",\n            autospec=True,\n        ) as mock_engine:\n            mock_engine.template_loaders = [\n                mock_custom_loader,\n                # Other loaders would be here\n            ]\n            self.assertSetEqual(\n                autoreload.get_template_directories(),\n                {ROOT / \"custom_templates\", ROOT / \"templates_extra\", ROOT / \"templates\"},\n            )\n\n", "    def test_get_template_directories_handles_non_existent_dirs(self):\n        with self.assertRaises(FileNotFoundError):\n            autoreload.get_template_directories()\n\n"], "sample_257": ["    def test_key_transform_with_nested_f_expression(self):\n        obj = NullableJSONModel.objects.create(\n            value={'d': [{'f': 'g'}]},\n        )\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.annotate(\n                key=KeyTransform('d', 'value'),\n                expr=F('key__0__f'),\n                nested_expr=F('key__0__f'),\n            ).filter(expr='g'),\n            [obj],\n        )\n\n\n", "    def test_key_transform_null_values(self):\n        obj = NullableJSONModel.objects.create(value={'a': 14, 'b': None})\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(value__b__isnull=True),\n            [obj],\n        )\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(value__a=14, value__b__isnull=True),\n            [obj],\n        )\n\n\n\n", "    def test_lookup_with_nested_key_transform_and_sql_injection(self):\n        with CaptureQueriesContext(connection) as queries:\n            self.assertIs(\n                NullableJSONModel.objects.filter(**{\n                    'value__a__contains': KeyTextTransform('bax', 'value')\n                }).exists(),\n                True,\n            )\n        self.assertIn(\n            'value->>a->>bax',\n            queries[0]['sql'],\n        )\n", "    def test_key_transform_with_null_value(self):\n        obj = NullableJSONModel.objects.create(value={'a': None})\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(\n                value__a__isnull=True,\n            ),\n            [obj],\n        )\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(\n                value__a=KeyTransform('a', KeyTransform('b', F('value__a'))),\n            ),\n            [],\n        )\n\n", "    def test_complex_key_transforms_in_filter(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(\n                value__d__1__f=KeyTransform('a', KeyTransform('b', 'value'))\n            ),\n            [self.objs[4]],\n        )\n\n\n"], "sample_1029": ["def test_Sum2():\n    sT(Sum2(x**2, (x, 1, 3, 1)), \"Sum2(Pow(Symbol('x'), Integer(2)), (Symbol('x'), Integer(1), Integer(3), Integer(1)))\")\n", "def test_Sum2():\n    sT(Sum2(x, (x, 1, 2, 1), 3, 4),\n       \"Sum2(Symbol('x'), (Add(Symbol('x'), Integer(1)), Integer(2), Integer(1), Integer(1)), Integer(3), Integer(4))\")\n\n\n", "def test_Sum2():\n    sT(Sum2(expr=x**2, i=Symbol('i'), a=Integer(0), b=Integer(5)),\n       \"Sum2(x**2, Symbol('i'), Integer(0), Integer(5))\")\n\n", "def test_Sum2():\n    sT(Sum2(x**2, (x, 1, 3, 2),),\n       \"Sum2(Pow(Symbol('x'), Integer(2)), (%s, %s, %s, %s))\" % (\n           \"Symbol('x')\", \"Integer(1)\", \"Integer(3)\", \"Integer(2)\"))\n", "def test_Sum2():\n    sT(Sum2(x**2 + 1, (x, 0, 1, 2)), \"Sum2(Add(Pow(Symbol('x'), Integer(2)), Integer(1)), (Symbol('x'), Integer(0), Integer(1), Integer(2)))\")\n\n\n\n"], "sample_919": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_889": ["def test_calibration_with_prefit_estimator(method):\n    X, y = make_classification(n_samples=100, random_state=42)\n    clf = LogisticRegression().fit(X, y)\n    calibrated_clf = CalibratedClassifierCV(clf, method=method)\n    calibrated_clf.fit(X, y)\n    assert isinstance(calibrated_clf.estimator_, LogisticRegression)\n", "def test_calibration_with_multi_class_classification(data):\n    \"\"\"Test calibration with multi-class classification using a one-vs-rest strategy.\"\"\"\n    X, y = data\n    clf = OneVsRestClassifier(LogisticRegression(multi_class=\"ovr\"))\n    calibrated_clf = CalibratedClassifierCV(clf, method='sigmoid')\n    calibrated_clf.fit(X, y)\n    y_pred = calibrated_clf.predict_proba(X)\n    assert y_pred.shape[1] == len(np.unique(y))\n\n\n\n", "def test_calibration_with_cv_and_sample_weight(method, data):\n    X, y = data\n    sample_weight = np.random.rand(len(y))\n    clf = LogisticRegression()\n    calibrated_clf = CalibratedClassifierCV(clf, method=method, cv=5)\n    calibrated_clf.fit(X, y, sample_weight=sample_weight)\n\n    # Check that the sample_weight is passed to the fitted estimator\n    assert calibrated_clf.calibrated_classifiers_[0].estimator.sample_weight_.shape == (len(y),)\n    assert np.allclose(calibrated_clf.calibrated_classifiers_[0].estimator.sample_weight_, sample_weight)\n", "    def test_calibration_display_wrong_y_shape(pyplot, iris_data_binary):\n        X, y = iris_data_binary\n        lr = LogisticRegression().fit(X, y)\n        \n        y_wrong_shape = y.reshape((len(y), 1))\n        with pytest.raises(ValueError, match=\"Shape of y_true does not match shape of y_prob\"):\n            CalibrationDisplay.from_estimator(lr, X, y_wrong_shape) \n", "def test_calibration_with_estimator_without_predict_proba(data):\n    class CustomEstimator:\n            pass\n\n        # Custom estimator without predict_proba method\n            pass\n\n    with pytest.raises(AttributeError):\n        CalibratedClassifierCV(estimator=CustomEstimator()).fit(*data)\n"], "sample_1190": ["def test_issue_23264():\n    from sympy.physics.units import Quantity, get_unit_from_dimension, Symbol\n\n    g = Quantity('g')\n    t = Quantity('t')\n    SI.set_quantity_dimension(g, length)\n    SI.set_quantity_dimension(t, time)\n\n    g_unit = get_unit_from_dimension(g)\n    t_unit = get_unit_from_dimension(t)\n\n    assert g_unit == 'meter'\n    assert t_unit == 'second'\n\n    # Issue: The `set_global_relative_scale_factor` method should not be used without a valid `base_unit` argument.\n\n    with raises(ValueError, match=\"Invalid base unit\"):\n        g.set_global_relative_scale_factor(1, Symbol('invalid_unit'))\n\n\n\n", "def test_issue_20976():\n    from sympy.physics.units import Quantity, meters\n    from sympy.physics.units.definitions import meter\n    u = Quantity(\"u\")\n    SI.set_quantity_dimension(u, meter)\n    u.set_global_relative_scale_factor(1, meter)\n\n    assert u == meter\n", "def test_quantity_with_units_in_arguments():\n    from sympy.physics.units import (\n        meter,\n        second,\n        kilogram,\n        joule,\n        speed_of_light,\n    )\n    x = Quantity('x')\n    y = Quantity('y')\n    # These functions have arguments with units.\n    assert convert_to(\n        sqrt(x*meter + y * kilogram / second), meter\n    ) != convert_to(sqrt(x*meter), meter)\n    assert convert_to(\n        speed_of_light * (x * meter / second) + y * joule / meter, meter/second\n    ) != convert_to(speed_of_light * y * joule / meter, meter/second)\n\n\n\n", "def test_issue_8469():\n    from sympy.physics.units import ampere, coulomb, ohm\n    assert ampere.convert_to(coulomb/second) == 1*coulomb/second\n    assert ohm.convert_to(volt/ampere) == 1*volt/ampere \n", "def test_issue_23469():\n    from sympy.physics.units import ampere\n    from sympy.physics.units.systems.si import dimsys_SI\n    assert ampere.convert_to(coulomb/second) == 1*coulomb/second\n    assert dimsys_SI.get_dimensional_dependencies(ampere) == {current: 1}\n"], "sample_908": ["def test_unparse_compare(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].value) == expected\n", "def test_unparse_function_defaults(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0]) == expected\n", "    def test_unparse_type_hints(source, expected):\n        module = ast.parse(source)\n        assert ast.unparse(module.body[0].value) == expected\n", "def test_unparse_function_def(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0]) == expected  \n", "def test_unparse_assign(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].value) == expected\n\n"], "sample_713": ["def test_ridge_class_weights_with_sample_weights():\n    rng = np.random.RandomState(0)\n    n_samples = 10\n    n_features = 5\n    n_targets = 3\n\n    X = rng.randn(n_samples, n_features)\n    y = rng.randint(0, 2, size=(n_samples, n_targets))\n    sample_weights = rng.randn(n_samples)\n    class_weights = {0: 1., 1: 10.}\n\n    ridge_classifier = RidgeClassifier(class_weight=class_weights, sample_weight=sample_weights)\n    ridge_classifier.fit(X, y)\n\n    # Check that the predicted probabilities are affected by both\n    # class_weight and sample_weights.\n    y_pred = ridge_classifier.predict_proba(X)\n    assert_array_almost_equal(y_pred.mean(axis=1), \n                             np.array([0.5] * n_samples)\n                             , decimal=3)\n", "def test_ridge_with_zero_alpha():\n    X, y = make_classification(n_samples=100, random_state=42)\n    ridge = Ridge(alpha=0.)\n    ridge.fit(X, y)\n    assert_array_almost_equal(ridge.coef_, np.linalg.inv(X.T @ X) @ X.T @ y)\n\n\n", "def test_ridge_sparse_array_fit():\n    X = sp.csc_matrix(np.random.random((10, 5)))\n    y = np.random.random((10,))\n    ridge = Ridge()\n    ridge.fit(X, y)\n    assert_array_almost_equal(ridge.coef_, ridge.coef_)\n", "def test_ridge_class_weight_on_multiclass():\n    \"\"\"Test class weights with multiclass classification.\"\"\"\n    X = np.array([[-1.0, -1.0], [-1.0, 0], [-.8, -1.0],\n                  [1.0, 1.0], [1.0, 0.0]])\n    y = np.array([1, 1, 1, -1, -1])\n\n    n_samples = X.shape[0]\n    alphas = [1e-1, 1e0, 1e1]\n    classes = np.unique(y)\n\n    for alpha in alphas:\n        ridge = RidgeClassifier(alpha=alpha, class_weight='balanced', random_state=42)\n        ridge.fit(X, y)\n        assert_almost_equal(ridge.coef_.shape[0], n_samples)\n        assert_array_almost_equal(ridge.coef_[y == 1], ridge.coef_[y == -1])\n\n\n\n", "def test_ridge_class_weights_multiclass():\n    rng = np.random.RandomState(0)\n    X = np.random.rand(100, 5)\n    y = np.random.randint(3, size=100)  # Multiclass\n\n    class_weights = {0: 0.1, 1: 1, 2: 10}\n\n    reg = RidgeClassifier(class_weight=class_weights)\n    reg.fit(X, y)\n    assert_array_almost_equal(reg.coef_[0], reg.coef_[1] * 10, decimal=5)\n    assert_array_almost_equal(reg.coef_[0], reg.coef_[2] * 100, decimal=5)\n\n\n\n"], "sample_208": ["    def test_suggest_name_with_multiple_operations(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel('Person', fields=[]),\n                migrations.RemoveField('Person', 'title'),\n                migrations.AddField('Person', 'age', models.IntegerField()),\n            ]\n\n        migration = Migration('0001_initial', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'person_add_age_remove_title')\n\n\n\n", "    def test_add_existing_field_to_model(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.AddField('Person', 'bio', models.TextField(null=True)),\n            ]\n\n        migration = Migration('0001_initial', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'add_bio_to_person')\n", "    def test_complex_scenario(self):\n        \"\"\"\n        Tests various combinations of model changes, including\n        adding, removing, renaming, and altering fields.\n        \"\"\"\n        model_state_a = ModelState('testapp', 'A', [\n            ('id', models.AutoField(primary_key=True)),\n            ('name', models.CharField(max_length=100)),\n        ])\n        changes = self.get_changes([model_state_a], [\n            ModelState('testapp', 'A', [\n                ('id', models.AutoField(primary_key=True)),\n                ('name', models.CharField(max_length=150)),\n                ('new_field', models.IntegerField()),\n            ]),\n            ModelState('testapp', 'B', [\n                ('id', models.AutoField(primary_key=True)),\n                ('a_id', models.ForeignKey('testapp.A', models.CASCADE)),\n            ]),\n        ])\n        self.assertNumberMigrations(changes, 'testapp', 2)\n        self.assertOperationTypes(changes, 'testapp', 0, ['AlterField', 'AddField'])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name='name', model_name='A', field_length=150)\n        self.assertOperationAttributes(changes, 'testapp', 0, 1, name='new_field', model_name='A', field_type='IntegerField')\n        self.assertOperationTypes(changes, 'testapp', 1, ['CreateModel'])\n        self.assertOperationAttributes(changes, 'testapp', 1, 0, name='B')\n\n\n", "    def test_rename_field_in_existing_model(self):\n        changes = self.get_changes(\n            [self.author_empty],\n            [self.author_with_different_name],\n        )\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"RenameField\"])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0,\n            old_name=\"name\", new_name=\"first_name\", model_name=\"Author\")\n", "    def test_auto_fallback_for_no_name(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.RenameField('Person', 'first_name', 'given_name'),\n                migrations.AddField('Person', 'last_name', models.CharField(max_length=200)),\n            ]\n\n        migration = Migration('0001_initial', 'test_app')\n        self.assertEqual(migration.suggest_name(), \"rename_person_first_name_to_given_name_add_last_name\")\n"], "sample_458": ["    def test_empty_string(self):\n        self.assertEqual(floatformat(\"\"), \"\")\n        self.assertEqual(floatformat(\" \"), \"\")\n        self.assertEqual(floatformat(\"\\t\"), \"\") \n        self.assertEqual(floatformat(\"\\n\"), \"\")        \n        self.assertEqual(floatformat(\"\\r\"), \"\")\n        self.assertEqual(floatformat(\"\\x00\"), \"\")\n\n\n\n", "    def test_currency(self):\n        with translation.override(\"en\", deactivate=True):\n            self.assertEqual(floatformat(1234.56, \"c\"), \"$1,234.56\")\n            self.assertEqual(floatformat(1234.56, \"2c\"), \"$1,234.56\")\n            with self.settings(\n                USE_THOUSAND_SEPARATOR=True,\n                NUMBER_GROUPING=3,\n                THOUSAND_SEPARATOR=\"!\",\n            ):\n                self.assertEqual(floatformat(1234.56, \"2c\"), \"$1!234.56\")\n            # Invalid currency code.\n            self.assertEqual(floatformat(1234.56, \"invalid\"), \"1234.56\")\n", "    def test_invalid_flags(self):\n        self.assertEqual(floatformat(1.23, \"x\"), \"1.23\")\n        self.assertEqual(floatformat(1.23, \"x2\"), \"1.23\")\n        self.assertEqual(floatformat(1.23, \"2x\"), \"1.23\")\n        self.assertEqual(floatformat(1.23, \"xy\"), \"1.23\")\n\n\n\n", "    def test_floatformat_with_thousands_sep(self):\n        with self.settings(\n            USE_THOUSAND_SEPARATOR=True, THOUSAND_SEPARATOR=\",\"\n        ):\n            self.assertEqual(floatformat(10000), \"10.000\")\n            self.assertEqual(floatformat(66666.666), \"66.666,7\")\n            self.assertEqual(floatformat(1234567.89), \"1.234.567,89\")\n\n\n\n\n", "    def test_zero_with_grouping(self):\n        with self.settings(\n            NUMBER_GROUPING=3, THOUSAND_SEPARATOR=\",\"\n        ):\n            self.assertEqual(floatformat(0, \"g\"), \"0\")\n            self.assertEqual(floatformat(0, \"2g\"), \"0.00\")\n            self.assertEqual(floatformat(0.0, \"2g\"), \"0.00\")\n"], "sample_1187": ["def test_hyperplane_parameters():\n    triangle = Polygon(Point2D(0, 0), Point2D(1, 0), Point2D(0, 1))\n    hp_params = hyperplane_parameters(triangle)\n    assert hp_params[0] == (1, -1)\n    assert hp_params[1] == (1, 0)\n    assert hp_params[2] == (0, 1)\n\n", "compilation error", "def test_integration_reduction_dynamic_2D():\n    triangle = Polygon(Point(0, 3), Point(5, 3), Point(1, 1))\n    facets = triangle.sides\n    a, b = hyperplane_parameters(triangle)[0]\n    x0 = facets[0].points[0]\n    monomial_values = [[0, 0, 0, 0], [1, 0, 0, 5],\n                       [y, 0, 1, 15], [x, 1, 0, None]]\n\n    assert integration_reduction_dynamic(facets, 0, a, b, x, 1, (x, y), 1,\n                0, 1, x0, monomial_values, 2) == Rational(25, 2)\n    assert integration_reduction_dynamic(facets, 0, a, b, 0, 1, (x, y), 1,\n                0, 1, x0, monomial_values, 2) == 0\n", "compilation error", "def test_polytope_integrate_complex_polygon():\n    polygon = Polygon(\n        [(6.933, 1.492), (6.933, 0.918), (6.111, 0.918), (6.111, 1.492),\n         (5.462, 1.492), (5.462, 0.918), (4.639, 0.918), (4.639, 1.492)],\n        [(0, 0), (0, 1), (1, 1), (1, 0)])  \n    assert polytope_integrate(polygon, x**2 + y**2) == Rational(643.5, 16)\n"], "sample_216": ["    def test_add_model_with_unique_constraint(self):\n        \"\"\"\n        Adding a model with a unique constraint is captured correctly.\n        \"\"\"\n        before = []\n        after = [\n            ModelState('app', 'user', [\n                ('id', models.AutoField(primary_key=True)),\n                ('username', models.CharField(max_length=100, unique=True)),\n            ],\n        )\n        ]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, [\"CreateModel\", ])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='user')\n        self.assertEqual(len(changes['app'][0].operations[0].constraints), 1)\n        self.assertEqual(changes['app'][0].operations[0].constraints[0]['type'], 'unique')\n\n", "    def test_remove_field_on_mti_parent_with_removal_child(self):\n        \"\"\"\n        Removing a field on an MTI parent model before removing a child\n        model should work correctly.\n        \"\"\"\n        Animal = ModelState('app', 'Animal', [\n            ('id', models.AutoField(primary_key=True)),\n            ('name', models.CharField(max_length=200)),\n        ])\n        Dog = ModelState('app', 'Dog', [], bases=('app.Animal',))\n        changes = self.get_changes(\n            [Animal, Dog],\n            [Animal, ModelState('app', 'Dog', [], bases=('app.Animal',),\n                           fields=[('tail_length', models.IntegerField())])]\n        )\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['RemoveField', 'AddField'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='name', model_name='Animal')\n\n\n\n", "    def test_delete_field_from_base_model_in_multi_inheritance(self):\n        \"\"\"\n        #23432 - Deleting a field from a base model in multi-inheritance\n        order should be correct.\n        \"\"\"\n        Base = ModelState('app', 'Base', [\n            ('id', models.AutoField(primary_key=True)),\n            ('name', models.CharField(max_length=200)),\n        ])\n        Mix = ModelState('app', 'Mix', [], bases=('app.Base',))\n        Spec = ModelState('app', 'Spec', [], bases=('app.Base',))\n        changes = self.get_changes([Base, Mix, Spec], [Base, Mix])\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['RemoveField', 'RemoveField'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='name', model_name='Base')\n        self.assertOperationAttributes(changes, 'app', 0, 1, name='name', model_name='Mix') \n", "    def test_add_unique_constraint(self):\n        changes = self.get_changes(\n            [self.author_empty],\n            [self.author_with_unique_name],\n        )\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"AddUniqueConstraint\"])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name='author', model_name='Author', fields=['name'])\n", "    def test_add_model_with_field_removed_from_base_model_after_add_field(self):\n        before = [\n            ModelState('app', 'readable', [\n                ('id', models.AutoField(primary_key=True)),\n                ('title', models.CharField(max_length=200)),\n            ]),\n        ]\n        after = [\n            ModelState('app', 'readable', [\n                ('id', models.AutoField(primary_key=True)),\n            ]),\n            ModelState('app', 'book', [\n                ('title', models.CharField(max_length=200)),\n            ], bases=('app.readable',)),\n            ModelState('app', 'read', [\n                ('id', models.AutoField(primary_key=True)),\n                ('readable_id', models.ForeignKey('app.readable', on_delete=models.CASCADE)),\n            ]),\n        ]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['RemoveField', 'CreateModel', 'CreateModel'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='title', model_name='readable')\n        self.assertOperationAttributes(changes, 'app', 0, 1, name='book')\n        self.assertOperationAttributes(changes, 'app', 0, 2, name='read')\n\n"], "sample_91": ["    def test_error_pages_custom_templates(self):\n        \"\"\"\n        Error pages should use custom templates when provided.\n        \"\"\"\n        @override_settings(TEMPLATES=[{\n            'BACKEND': 'django.template.backends.django.DjangoTemplates',\n            'OPTIONS': {\n                'loaders': [\n                    ('django.template.loaders.locmem.Loader', {\n                        '404.html': 'This is a custom 404 template.',\n                        '403.html': 'This is a custom 403 template.',\n                        '500.html': 'This is a custom 500 template.',\n                        '400.html': 'This is a custom 400 template.',\n                    }),\n                ],\n            },\n        }])\n            return HttpResponseBadRequest(\"Error\")\n\n        response = self.client.get('/test/')\n        self.assertContains(response, 'This is a custom 400 template.', status_code=400)\n\n        response = self.client.get('/custom_403/')\n        self.assertContains(response, 'This is a custom 403 template.', status_code=403)\n\n        response = self.client.get('/custom_500/')\n        self.assertContains(response, 'This is a custom 500 template.', status_code=500)\n\n        response = self.client.get('/custom_404/')\n        self.assertContains(response, 'This is a custom 404 template.', status_code=404)\n", "    def test_custom_template_errors(self):\n        \"\"\"\n        Error views should display user-friendly messages when custom templates\n        raise exceptions.\n        \"\"\"\n        request = self.request_factory.get('/')\n\n        @override_settings(TEMPLATES=[{\n            'BACKEND': 'django.template.backends.django.DjangoTemplates',\n            'OPTIONS': {\n                'loaders': [\n                    ('django.template.loaders.locmem.Loader', {\n                        '404.html': '{% for exception in exceptions %}{% if exception.args %}{% for arg in exception.args %}{{ arg }} {% endfor %}{% endif %}{% endfor %}',\n                    }),\n                ],\n            },\n        }])\n            return bad_request(request, exception)\n\n        response = bad_request_with_exceptions(request, ValueError('Boom'))\n        self.assertIn(b'Boom', response.content)\n\n        response = permission_denied(request, ValueError('Boop'))\n        self.assertIn(b'Boop', response.content)\n\n        response = page_not_found(request, Http404('Oops'))\n        self.assertIn(b'Oops', response.content)\n\n", "    def test_custom_template_context(self):\n        \"\"\"\n        Custom templates passed to error views should render correctly with the context.\n        \"\"\"\n        request = self.request_factory.get('/')\n        response = permission_denied(request, Exception(), template_name=\"403.html\")\n        self.assertContains(response, 'exception: Testing custom 403', status_code=403)\n", "    def test_csrf_token_in_permission_denied(self):\n        \"\"\"\n        The 403 page should have the csrf_token available in the context\n        \"\"\"\n        # See ticket #14565\n        response = self.client.get('/technical404/')\n        self.assertNotEqual(response.content, b'NOTPROVIDED')\n        self.assertNotEqual(response.content, b'')\n", "    def test_custom_template_context(self):\n        \"\"\"\n        Custom 403 templates should receive the exception object in the context.\n        \"\"\"\n        request = self.request_factory.get('/')\n        response = permission_denied(request,  Exception('Custom error message'), template_name='403.html')\n        self.assertContains(response, b'Custom error message', status_code=403) \n"], "sample_1068": ["def test_printing_issue_14042():\n    assert mcode(exp(x*y)) == \"exp(x.*y)\"\n", "def test_polygamma():\n    assert octave_code(polygamma(n, x)) == 'psi(x, n)'\n", "def test_polygamma():\n    assert octave_code(polygamma(n, x)) == 'psi(n + 1, x)'\n    assert octave_code(polygamma(0, x)) == 'psi(x)'\n", "def test_trigonometric_shortcuts():\n    assert octave_code(sin(2*x)) == 'sin(2*x)'\n    assert octave_code(cos(pi*x)) == 'cos(pi*x)'\n    assert octave_code(tan(x/2)) == 'tan(x/2)'\n    assert octave_code(arctan(x/y)) == 'atan(x/y)'\n    assert octave_code(sqrt(1-x**2)) == 'sqrt(1-x.^2)'\n    assert octave_code(sqrt(1-x**2)).subs(x, sin(y)) == 'sqrt(1-sin(y).^2)'\n", "def test_polylog():\n    assert octave_code(polylog(n, x)) == 'polylog(n, x)'\n"], "sample_480": ["    def test_key_text_transform_invalid_nested_index(self):\n        with self.assertRaisesMessage(ValueError, \"Invalid index\"):\n            KT(\"value__baz__1__foo\")\n", "    def test_key_text_transform_from_lookup_multiple_objects(self):\n        obj1 = NullableJSONModel.objects.create(value={\"bax\": {\"foo\": \"bar\"}, \"o\": \"quoted\"})\n        obj2 = NullableJSONModel.objects.create(value={\"bax\": {\"foo\": \"baz\"}, \"o\": \"unquoted\"})\n        qs = NullableJSONModel.objects.annotate(\n            b=KT(\"value__bax__foo\"),\n            o=KT(\"value__o\"),\n        ).filter(\n            b__contains=\"ar\", o__contains=\"uo\",\n        )\n        self.assertSequenceEqual(qs, [obj1])\n", "    def test_key_text_transform_from_lookup_with_null(self):\n        qs = NullableJSONModel.objects.annotate(\n            b=KT(\"value__bax__foo\"),\n        ).filter(b__isnull=True)\n        self.assertSequenceEqual(qs, [self.objs[6]])\n\n\n", "    def test_key_text_transform_from_lookup_with_raw_sql(self):\n        msg = \"Raw SQL is not supported in KeyTextTransform.\"\n        with self.assertRaisesMessage(TypeError, msg):\n            KT(\"value__bax\", raw_sql=f'{{\"foo\":\"{self.objs[7].value[\"bax\"][\"foo\"]}\"}}')\n\n", "    def test_key_text_transform_from_subquery(self):\n        subquery = (\n            NullableJSONModel.objects.filter(value__h=True)\n            .values(\"value__s\")\n            .annotate(\n                key=KT(\"value__s\"),\n            )\n        )\n        qs = NullableJSONModel.objects.filter(\n            value__s__in=subquery.values(\"key\")\n        )\n        self.assertSequenceEqual(qs, self.objs[4:6])\n\n\n"], "sample_926": ["def test_noindexentry_multiple_targets(app):\n    text = (\".. cpp:function:: void f()\\n\"\n            \"   :noindexentry:\\n\"\n            \".. cpp:function:: void g()\\n\"\n            \".. cpp:class:: C\\n\"\n            \"   :noindexentry:\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index, desc, addnodes.index, desc))\n    assert_node(doctree[0], addnodes.index, entries=[('single', 'f (C++ function)', '_CPPv41fv', '', None)])\n    assert_node(doctree[2], addnodes.index, entries=[])\n    assert_node(doctree[4], addnodes.index, entries=[])\n", "compilation error", "compilation error", "def test_paramlist_in_docstrings(app):\n    text = (\".. cpp:function:: void f(T t)\\n\"\n            \"   This function takes a parameter of type T.\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.function, addnodes.arguments))\n    assert_node(doctree[0], addnodes.function, args='t')\n", "compilation error"], "sample_203": ["    def test_value_placeholder_with_ipv46_address_validator(self):\n        cases = [\n            ('192.168.1.1', 'invalid'),\n            ('2001:db8::1', 'invalid'),\n            ('256.1.1.1', 'invalid'),\n            ('::1', 'invalid'),\n            ('fe80::1', 'invalid'),\n            ('192.168.1.1', 'invalid'),\n        ]\n        for value, code in cases:\n            with self.subTest(value=value):\n                class MyForm(forms.Form):\n                    field = forms.CharField(\n                        validators=[validators.ip_address_validators('both')],\n                        error_messages={code: '%(value)s'},\n                    )\n\n                form = MyForm({'field': value})\n                self.assertIs(form.is_valid(), False)\n                self.assertEqual(form.errors, {'field': [value]})\n\n\n", "    def test_value_placeholder_with_custom_validator(self):\n        class CustomValidator:\n                self.message = message\n\n                raise ValidationError(self.message, code='custom_error')\n\n        class MyForm(forms.Form):\n            field = forms.CharField(\n                validators=[CustomValidator('Custom validation error')],\n            )\n\n        form = MyForm({'field': 'test'})\n        self.assertIs(form.is_valid(), False)\n        self.assertEqual(form.errors, {'field': ['Custom validation error']})\n", "    def test_value_placeholder_with_prohibit_null_characters(self):\n        class MyForm(forms.Form):\n            field = forms.CharField(\n                validators=[validators.ProhibitNullCharactersValidator()],\n                error_messages={'null_characters_not_allowed': '%(value)s'},\n            )\n\n        form = MyForm({'field': 'a\\0b'})\n        self.assertIs(form.is_valid(), False)\n        self.assertEqual(form.errors, {'field': ['a\\x00b']})\n", "    def test_value_placeholder_with_custom_validator(self):\n        class MyCustomValidator(object):\n            message = 'Custom validation message: %(value)s'\n            code = 'custom_error'\n\n                if message is not None:\n                    self.message = message\n                if code is not None:\n                    self.code = code\n\n                raise ValidationError(self.message, code=self.code, params={'value': value})\n\n        class MyForm(forms.Form):\n            field = forms.CharField(\n                validators=[MyCustomValidator()],\n            )\n\n        form = MyForm({'field': 'test'})\n        self.assertIs(form.is_valid(), False)\n        self.assertEqual(form.errors, {'field': ['Custom validation message: test']})\n\n\n\n", "    def test_file_extension_validator_with_allowed_extensions(self):\n        allowed_extensions = ['jpg', 'png', 'jpeg']\n        class MyForm(forms.Form):\n            field = forms.FileField(\n                validators=[\n                    FileExtensionValidator(\n                        allowed_extensions=allowed_extensions,\n                        message='Invalid file extension. Allowed extensions are: %(allowed_extensions)s.',\n                    )\n                ]\n            )\n\n        files = [\n            SimpleUploadedFile('image1.jpg', b'abc'),\n            SimpleUploadedFile('image2.png', b'def'),\n            SimpleUploadedFile('image3.txt', b'ghi'),\n            SimpleUploadedFile('image4.JPEG', b'jkl'),\n            SimpleUploadedFile('image5.gif', b'mno'),\n            SimpleUploadedFile('image6.BMP', b'pqr'),\n            SimpleUploadedFile('image7.svg', b'stu'),\n        ]\n\n        for uploaded_file in files:\n            with self.subTest(filename=uploaded_file.name):\n                form = MyForm(files={'field': uploaded_file})\n\n                if uploaded_file.name.lower().split('.')[-1] in allowed_extensions:\n                    self.assertTrue(form.is_valid())\n                else:\n                    self.assertFalse(form.is_valid())\n                    self.assertEqual(\n                        form.errors['field'],\n                        ['Invalid file extension. Allowed extensions are: jpg, png, jpeg.'],\n                    )\n\n\n\n"], "sample_658": ["    def test_doctest_inline_module(self, testdir):\n        p = testdir.makepyfile(\n            \"\"\"\n                '''\n                >>> a = 1 + 1\n                >>> a\n                2\n                '''\n            \"\"\"\n        )\n        reprec = testdir.inline_run(p, \"--doctest-modules\")\n        reprec.assertoutcome(passed=1)\n", "    def test_doctest_with_fixture_on_module_level(self, testdir):\n        testdir.makeconftest(\n            \"\"\"\n            import pytest\n\n            @pytest.fixture(autouse=True)\n                return 99\n        \"\"\"\n        )\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n                '''\n                >>> my_fixture()\n                99\n                '''\n        \"\"\"\n        )\n        result = testdir.runpytest(\"--doctest-modules\")\n        result.stdout.fnmatch_lines([\"* 1 passed *\"])\n\n\n\n", "    def test_doctest_option_in_ini(self, testdir):\n        testdir.makeini(\n            \"\"\"\n            [pytest]\n            doctest_optionflags = ELLIPSIS\n            \"\"\"\n        )\n        p = testdir.makepyfile(\n            \"\"\"\n                '''\n                >>> a = [1, 2, 3]\n                >>> b = a\n                >>> b[0] = 5\n                >>> print(a)\n                [5, 2, 3]\n                '''\n        \"\"\"\n        )\n        reprec = testdir.inline_run(p, \"--doctest-modules\")\n        reprec.assertoutcome(passed=1)\n", "    def test_doctest_fixture_scope_class(self, testdir):\n        testdir.makeconftest(\n            \"\"\"\n            import pytest\n            class MyFixture:\n                    self.value = value\n\n            @pytest.fixture(scope=\"class\")\n                return MyFixture(value=10)\n        \"\"\"\n        )\n        p = testdir.makepyfile(\n            \"\"\"\n            class TestClass:\n                    '''\n                    >>> my_fixture.value\n                    10\n                    '''\n        \"\"\"\n        )\n        reprec = testdir.inline_run(p)\n        reprec.assertoutcome(passed=1)\n", "    def test_warn_twice_if_wrapped_object_is_already_mocked(self):\n        with _patch_unwrap_mock_aware():\n            bad_instance = Broken()\n            wrapped_instance = Mock(spec=bad_instance)\n            with pytest.warns(\n                pytest.PytestWarning, match=\"^Mock object is already wrapped\"\n            ) as warning:\n                inspect.unwrap(wrapped_instance)\n            assert len(list(warning)) == 1\n            with pytest.warns(\n                pytest.PytestWarning, match=\"^Mock object is already wrapped\"\n            ):\n                inspect.unwrap(wrapped_instance)\n\n"], "sample_672": ["def test_circular_references():\n    class A:\n            self.b = B()\n\n    class B:\n            self.a = A()\n\n    a = A()\n    s = saferepr(a)\n    assert \"circular\" in s\n", "def test_circular_dependencies():\n    class A:\n            return \"A\"\n\n    class B:\n            return \"B\"\n\n    a = A()\n    b = B()\n    a.b = b\n    b.a = a\n\n    assert saferepr(a) != \"<[...circular reference...]>\"\n", "            def __repr__(self):\n                raise TypeError\n", "def test_empty_repr():\n    class EmptyRepr:\n            return \"\"\n    assert saferepr(EmptyRepr()) == \"<[<BLANKLINE> raised in repr()] EmptyRepr object at 0x{:x}>\".format(id(EmptyRepr()))\n", "def test_empty_repr():\n    class EmptyRepr:\n            return \"\"\n\n    assert saferepr(EmptyRepr()) == \"<[repr() returned '' ] EmptyRepr object at 0x{:x}>\".format(id(EmptyRepr()))\n"], "sample_778": ["def test_nmf_custom_init():\n    # Test that custom initialization works as expected\n    rng = np.random.mtrand.RandomState(42)\n    n_samples = 10\n    n_features = 5\n    n_components = 3\n\n    X = np.abs(rng.randn(n_samples, n_features))\n\n    # Generate custom initializations for W and H\n    W_init = np.abs(rng.randn(n_samples, n_components) * 10)\n    H_init = np.abs(rng.randn(n_components, n_features) * 10)\n\n    # Fit NMF with custom initialization\n    model = NMF(n_components=n_components, init='custom',\n                random_state=42)\n    model.fit(X, W=W_init, H=H_init)\n\n    # Check that the fitted W and H are close to the initialized values\n    assert_array_almost_equal(model.components_, H_init, decimal=2)\n    assert_array_almost_equal(model.transform(X), W_init, decimal=2)\n\n\n\n", "def test_nmf_sparse_input_beta_loss_zero():\n    # Test that beta_loss=0 works correctly with sparse input\n    rng = np.random.mtrand.RandomState(42)\n    n_samples = 20\n    n_features = 10\n    n_components = 5\n    X = rng.randn(n_samples, n_features)\n    np.abs(X, X)  # Ensure matrix is non-negative\n    X_csr = sp.csr_matrix(X)\n\n    W0, H0 = nmf._initialize_nmf(X, n_components, init='random',\n                                 random_state=42)\n    W, H = W0.copy(), H0.copy()\n\n    # Test with beta_loss = 0\n    W, H, _ = non_negative_factorization(X_csr, W, H, n_components,\n                                        beta_loss=0, init='custom',\n                                        solver='mu', random_state=0,\n                                        tol=1e-4, max_iter=100)\n\n    # Calculate reference results using dense array\n    W_ref, H_ref, _ = non_negative_factorization(X, W, H, n_components,\n                                                beta_loss=0, init='custom',\n                                                solver='mu', random_state=0,\n                                                tol=1e-4, max_iter=100)\n\n    assert_array_almost_equal(W, W_ref, decimal=7)\n    assert_array_almost_equal(H, H_ref, decimal=7)\n\n\n\n", "def test_nmf_sparse_transformation_stability():\n    # Test stability of transformation on sparse data\n    # with varying sparsity levels\n    rng = np.random.mtrand.RandomState(42)\n    n_samples = 100\n    n_features = 50\n    n_components = 10\n    sparsity_levels = [0.1, 0.3, 0.5, 0.7, 0.9]\n\n    for sparsity in sparsity_levels:\n        # Create a sparse matrix with the given sparsity\n        X = rng.randn(n_samples, n_features)\n        X[rng.rand(n_samples, n_features) < sparsity] = 0\n\n        # Fit the NMF model on the sparse data\n        model = NMF(n_components=n_components, random_state=0)\n        model.fit(X)\n\n        # Transform the sparse data using the fitted model\n        W = model.transform(X)\n\n        # Check that the transformed W has the expected shape\n        assert_array_equal(W.shape, (n_samples, n_components))\n        assert_almost_equal(W.sum(axis=1).mean(), 1, decimal=2)  # Check for approximate unit sums in each row\n\n\n\n", "def test_nmf_sparse_init():\n    # Smoke test: Test that sparse initialization works\n\n    n_samples = 10\n    n_features = 5\n    n_components = 3\n    rng = np.random.mtrand.RandomState(666)\n\n    # Create a random sparse matrix for initialization\n    init_H = sp.csr_matrix((rng.rand(n_components, n_features),\n                           (rng.randint(0, n_samples, n_components),\n                            rng.randint(0, n_features, n_components))),\n                          shape=(n_components, n_features))\n    init_W = sp.csr_matrix((rng.rand(n_samples, n_components),\n                           (rng.randint(0, n_samples, n_samples),\n                            rng.randint(0, n_components, n_samples))),\n                          shape=(n_samples, n_components))\n\n    # Test with NMF\n    nmf = NMF(n_components=n_components, init='custom', random_state=0)\n    nmf.fit_transform(init_W, init_H)\n\n    # Test with _initialize_nmf for consistency\n    W, H = nmf._initialize_nmf(None, n_components, init='custom',\n                              random_state=0, H=init_H, W=init_W)\n    assert_array_almost_equal(W, nmf.components_, decimal=7)\n    assert_array_almost_equal(H, nmf.transform(None), decimal=7)\n\n\n\n", "def test_nmf_sparse_matrix_input():\n    # Test handling of sparse input matrices\n    # Test that the decomposition works correctly for sparse matrices.\n    n_samples = 1000\n    n_features = 500\n    n_components = 100\n    rng = np.random.mtrand.RandomState(42)\n    X = sp.csr_matrix(rng.randn(n_samples, n_features))\n    \n    nmf = NMF(n_components=n_components, solver='mu', random_state=0)\n    W = nmf.fit_transform(X)\n    H = nmf.components_\n    \n    # Reconstruct and check that the result is close to the original\n    X_reconstructed = W @ H\n    assert_array_almost_equal(X.toarray(), X_reconstructed.toarray(), decimal=4)\n\n"], "sample_813": ["def test_ard_prune_weights():\n    # Test weight pruning in ARDRegression\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([1, 2, 3])\n    n_features = X.shape[1]\n\n    # Fit ARDRegression and check that some weights are zero\n    clf = ARDRegression(threshold_lambda=1e+4)\n    clf.fit(X, y)\n    coef = clf.coef_\n    \n    # Assert that some weights are close to zero\n    for i in range(n_features):\n        assert_array_almost_equal(coef[i], 0, 4)\n\n", "def test_ward_bayesian_ridge():\n    \"\"\"Test BayesianRidge with ward linkage\n    for k-means cluster labels.\n    \n    The test case ensures that Bayesian Ridge\n    can correctly handle data where k-means\n    cluster labels are used as additional features.\n    \"\"\"\n    from sklearn.cluster import KMeans\n    from sklearn.datasets import make_blobs\n    \n    X, _ = make_blobs(n_samples=100, centers=3, random_state=42)\n    kmeans = KMeans(n_clusters=3, random_state=42)\n    kmeans.fit(X)\n    X_with_labels = np.hstack((X, kmeans.labels_.reshape(-1, 1)))\n \n    y = np.zeros(len(X))\n    \n    clf = BayesianRidge(compute_score=True)\n    clf.fit(X_with_labels, y)\n\n    \n    # Prediction is not a focus, just check for no errors.\n    clf.predict(X_with_labels)\n", "def test_ard_threshold_lambda():\n    # Test that the threshold lambda parameter works correctly\n    X = np.array([[1, 0],\n                  [0, 1]])\n    y = np.array([1, 1])\n    clf = ARDRegression(threshold_lambda=1e5)  \n    clf.fit(X, y)\n    assert_equal(clf.lambda_.shape, (X.shape[1],))\n    assert_almost_equal(clf.lambda_[0], 0) \n    assert_almost_equal(clf.lambda_[1], 0)\n\n\n\n", "def test_ard_large_dataset():\n    # Test ARDRegression with a large dataset\n    iris = datasets.load_iris()\n    X = iris.data\n    y = iris.target\n    clf = ARDRegression(n_iter=50)  # Increase iterations for larger datasets\n    clf.fit(X, y)\n    \n    # Check that the model correctly predicts classes\n    y_pred = clf.predict(X)\n    assert_array_almost_equal(y_pred, y)\n", "def test_ard_regression_sparse_data():\n    # Test ARDRegression with sparse data\n    from sklearn.datasets import make_sparse_dataset\n    from scipy.sparse import csr_matrix\n\n    X, y = make_sparse_dataset(n_samples=100, n_features=1000,\n                               random_state=42)\n    X = csr_matrix(X)\n\n    clf = ARDRegression()\n    clf.fit(X, y)\n    X_test = csr_matrix(X.shape)  # Same shape as training data\n    clf.predict(X_test)  \n\n\n\n"], "sample_873": ["def test_feature_names_out():\n    sel = StepSelector()\n    sel.fit(X, y)\n    assert_array_equal(sel.get_feature_names_out(input_features=feature_names), feature_names_t)\n    with pytest.raises(ValueError):\n        sel.get_feature_names_out(input_features=[\"a\", \"c\", \"e\", \"g\", \"i\"])\n\n\n\n\n", "def test_get_feature_names_out():\n    sel = StepSelector()\n    sel.fit(X, y)\n    names_out = sel.get_feature_names_out(input_features=feature_names)\n    assert_array_equal(feature_names_t, names_out)\n", "def test_get_feature_names_out():\n    sel = StepSelector()\n    sel.fit(X, y)\n    assert_array_equal(feature_names_t, sel.get_feature_names_out(feature_names))\n", "def test_get_feature_names_out():\n    sel = StepSelector(step=2)\n    sel.fit(X, y)\n    assert_array_equal(\n        sel.get_feature_names_out(input_features=feature_names), feature_names_t\n    )\n\n\n", "def test_get_feature_names_out():\n    sel = StepSelector()\n    sel.fit(X, y)\n    sel.get_feature_names_out()\n    assert_array_equal(feature_names_t, sel.get_feature_names_out(input_features=feature_names))\n\n    with pytest.raises(ValueError):\n        sel.get_feature_names_out(input_features=[1, 2, 3])\n"], "sample_744": ["def test_power_transformer_sparse_input():\n    X = sparse.csr_matrix(data=np.abs(X_2d),\n                          shape=X_2d.shape,\n                          dtype=np.float64)\n\n    pt = PowerTransformer(method='box-cox')\n    with pytest.raises(TypeError):\n        pt.fit(X)\n\n    pt = PowerTransformer(method='box-cox', sparse_input='passthrough')\n    X_trans = pt.fit_transform(X)\n    assert isinstance(X_trans, sparse.csr_matrix)\n\n    X_inv = pt.inverse_transform(X_trans)\n    assert isinstance(X_inv, sparse.csr_matrix)\n\n    # Test inverse transformation\n    assert_array_almost_equal(X_inv.toarray(), X.toarray())  # Compare dense representation\n\n\n\n", "compilation error", "def test_power_transformer_custom_lambda():\n    pt = PowerTransformer(method='box-cox', standardize=False)\n    X = np.abs(X_2d)[:, 0:1]\n\n    # Test using a custom lambda\n    custom_lambda = 0.5\n    pt.lambdas_ = np.array([custom_lambda])\n\n    X_trans = pt.transform(X)\n    expected_transform = power_transform(X.flatten(), custom_lambda)\n    assert_almost_equal(X_trans, expected_transform.reshape(-1, 1))\n    assert_almost_equal(pt.inverse_transform(X_trans), X)\n\n    \n\n", "def test_power_transformer_data_types():\n    pt = PowerTransformer(method='box-cox')\n    X = np.abs(X_1col).astype(np.float16)\n\n    pt.fit_transform(X)\n    assert_array_equal(pt.lambdas_.dtype, np.float64)\n    assert_array_equal(pt.inverse_transform(pt.transform(X)).dtype, np.float64)\n", "def test_power_transformer_all_zeros():\n    pt = PowerTransformer(method='box-cox', standardize=False)\n    X = np.zeros((10, 2))\n    with pytest.raises(ValueError) as e:\n        pt.fit_transform(X)\n    assert \"Input data must contain strictly positive values\" in str(e.value)\n\n\n"], "sample_118": ["    def test_exact_query_rhs_with_multiple_relations(self):\n        author1 = Author.objects.create(name='Author 1')\n        author2 = Author.objects.create(name='Author 2')\n        tag1 = Tag.objects.create(name='Tag 1')\n        tag2 = Tag.objects.create(name='Tag 2')\n\n        article1 = Article.objects.create(author=author1, tags=[tag1, tag2])\n        article2 = Article.objects.create(author=author2, tags=[tag1])\n\n        latest_article_id = Article.objects.order_by('-id').values_list('id', flat=True)[0]\n        latest_article = Article.objects.filter(pk=latest_article_id)\n\n        self.assertEqual(latest_article.get().author.name, 'Author 2')\n        self.assertEqual(latest_article.get().tags.all().count(), 1)\n\n\n\n", "    def test_exact_query_rhs_with_related_objects(self):\n        season = Season.objects.create(year=2012)\n        game1 = Game.objects.create(season=season, home=\"Astros\", away=\"Cubs\")\n        game2 = Game.objects.create(season=season, home=\"Astros\", away=\"Cardinals\")\n        games = Game.objects.filter(season__year=2012)\n        self.assertEqual(\n            Game.objects.filter(pk__in=games.values_list('id', flat=True)),\n            games.all()\n        )\n", "    def test_exact_query_rhs_with_filter_and_values(self):\n        newest_author = Author.objects.create(name='Author 2')\n        authors = Author.objects.filter(name='Author 2').values('id').annotate(\n            max_id=Max('id')\n        ).filter(id=F('max_id'))\n        self.assertEqual(authors.get(), {'id': newest_author.id})\n", "    def test_query_on_related_fields_with_complex_lookups(self):\n        tag1 = Tag.objects.create(name='python')\n        tag2 = Tag.objects.create(name='django')\n        author1 = Author.objects.create(name='John Doe')\n        author2 = Author.objects.create(name='Jane Doe')\n        article1 = Article.objects.create(author=author1, headline='Django Tutorial')\n        article2 = Article.objects.create(author=author2, headline='Python Libraries')\n        article1.tags.add(tag1)\n        article2.tags.add(tag2)\n\n        # Test a complex lookup on a related field\n        # Example: Articles where the author's name starts with 'J' and the article has the 'python' tag\n        articles = Article.objects.filter(\n            author__name__istartswith='J',\n            tags__name='python'\n        )\n\n        self.assertCountEqual(articles, [article1])\n", "def test_complex_nested_lookups():\n    # Test for complex nested lookups with multiple fields and operators\n    tag1 = Tag.objects.create(name='Technology')\n    tag2 = Tag.objects.create(name='Science')\n    Article.objects.create(headline='AI Revolution', tags=[tag1, tag2])\n    Article.objects.create(headline='Space Exploration', tags=[tag2])\n\n    # Use Django filters to mimic more complex query construction\n    filter_class = django_filters.rest_framework.DjangoFilterBackend.filter_queryset\n    filtered_qs = filter_class(\n        Article.objects.all(),\n        filterset_class=django_filters.rest_framework.FilterSet(\n            fields=['tags__name']\n        )\n    )\n\n    # Assert that only articles tagged with 'Technology' are returned\n    self.assertQuerysetEqual(filtered_qs, ['<Article: AI Revolution>'])\n\n\n"], "sample_437": ["    def test_multi_database_init_connection_state_called_once_for_each_database(self):\n        with patch.object(connections[\"default\"], \"commit\", return_value=None):\n            with patch.object(\n                connections[\"default\"],\n                \"check_database_version_supported\",\n            ) as mocked_check_default_db:\n                with patch.object(connections[\"other\"], \"commit\", return_value=None):\n                    with patch.object(\n                        connections[\"other\"],\n                        \"check_database_version_supported\",\n                    ) as mocked_check_other_db:\n                        connections[\"default\"].init_connection_state()\n                        connections[\"other\"].init_connection_state()\n\n                        self.assertEqual(len(mocked_check_default_db.mock_calls), 1)\n                        self.assertEqual(len(mocked_check_other_db.mock_calls), 1)\n\n\n", "    def test_multi_database_connection_state_retains_after_close(self):\n        for db in self.databases:\n            with self.subTest(database=db):\n                connections[db].init_connection_state()\n                connections[db].close()\n                connections[db].init_connection_state()\n\n\n\n", "    def test_multi_database_close_all_connections(self):\n        for db in self.databases:\n            with self.subTest(database=db):\n                with patch.object(connections[db], \"close\", return_value=None):\n                    connections[db].close()\n                    connections[db].close()  \n", "    def test_multi_database_get_connection(self):\n        for db in self.databases:\n            with self.subTest(database=db):\n                conn1 = connections[db].get_connection()\n                conn2 = connections[db].get_connection()\n                self.assertIs(conn1, conn2)\n\n", "    def test_connection_close_on_error(self, mock_is_usable):\n        with self.assertRaises(Exception):\n            with connection.cursor() as cursor:\n                cursor.execute(\"SELECT 1\" + connection.features.bare_select_suffix)\n"], "sample_970": ["        def __init__(self):\n            pass\n", "        def __init__(self):\n            self.__mro__ = [ObjectWithCustomMro, object]\n", "compilation error", "        def __init__(self):\n            pass\n", "def test_getfullargspec_nested_partial():\n    from functools import partial\n\n        pass\n\n    func2 = partial(func1, 1)\n    func3 = partial(func2, 2)\n\n    spec = inspect.getfullargspec(func3)\n    assert spec.args == ['b', 'c']\n    assert spec.varkwargs == None\n    assert spec.varkargs == None\n    assert spec.defaults == (None, None)\n\n\n"], "sample_64": ["    def test_cookie_encoding_and_unicode(self):\n        # Test for encoding and decoding of cookies with Unicode characters\n        cookie_str = 'key=\u00e9\u00e0\u00e7\u00fc\u00f6; another_key=\u4f60\u597d\u4e16\u754c'\n        parsed_cookies = parse_cookie(cookie_str)\n\n        self.assertEqual(parsed_cookies['key'], '\u00e9\u00e0\u00e7\u00fc\u00f6')\n        self.assertEqual(parsed_cookies['another_key'], '\u4f60\u597d\u4e16\u754c')\n\n        c = SimpleCookie()\n        c['key'] = '\u00e9\u00e0\u00e7\u00fc\u00f6'\n        c['another_key'] = '\u4f60\u597d\u4e16\u754c'\n        encoded_cookies = c.output()\n\n        self.assertIn('key=\u00e9\u00e0\u00e7\u00fc\u00f6', encoded_cookies)\n        self.assertIn('another_key=\u4f60\u597d\u4e16\u754c', encoded_cookies)\n", "    def test_cookie_order(self):\n        \"\"\"\n        Test that cookies are output in the order they were added.\n        \"\"\"\n        c = SimpleCookie()\n        c['a'] = '1'\n        c['b'] = '2'\n        c['c'] = '3'\n\n        output = c.output().strip('; \\r\\n')\n        self.assertEqual(output, 'a=1; b=2; c=3')\n", "    def test_cookie_security_headers(self):\n        c = SimpleCookie()\n        c['test'] = 'value'\n        c['test']['secure'] = True\n        c['test']['httpOnly'] = True\n        self.assertTrue(c['test']['secure'])\n        self.assertTrue(c['test']['httpOnly'])\n        self.assertIn('Secure', c.output())\n        self.assertIn('HttpOnly', c.output())\n", "    def test_cookies_with_unicode_values(self):\n        c = SimpleCookie()\n        c['test'] = '\u00e9\u00e0\u00e7\u00fc'\n        self.assertEqual(c['test'].value, '\u00e9\u00e0\u00e7\u00fc')\n        self.assertIn('test=\u00e9\u00e0\u00e7\u00fc', c.output())\n\n\n\n", "    def test_duplicate_cookies(self):\n        # Test cases for #20192: Duplicate cookies in response\n        c = SimpleCookie()\n        c['hello'] = 'world'\n        c['hello'] = 'universe'\n        self.assertEqual(c['hello'].value, 'universe')\n        self.assertEqual(c.output(header='Set-Cookie'), 'Set-Cookie: hello=universe')\n        \n        c = SimpleCookie()\n        c['good_cookie'] = 'yes'\n        c['bad_cookie'] = 'no'\n        c['good_cookie'] = 'yes'\n\n\n        res = HttpResponse()\n        res.set_cookie('good_cookie', 'yes')\n        res.set_cookie('bad_cookie', 'no')\n        res.set_cookie('good_cookie', 'yes')\n        self.assertEqual(len(res._cookies), 2)\n        self.assertEqual(res._cookies['good_cookie'].value, 'yes')\n        self.assertEqual(res._cookies['bad_cookie'].value, 'no')\n\n        # Test with more complex cookies\n\n        c['other_cookie'] = 'yes'\n        r = HttpResponse()\n        r.set_cookie(c)\n        self.assertEqual(len(r._cookies), 3)\n"], "sample_1097": ["def test_block_inverse_idempotency():\n    A = MatrixSymbol('A', n, n)\n    B = BlockMatrix([[A, A], [A, A]])\n    assert block_collapse(B.inverse() * B) == Identity(2*n)\n\n\n\n", "def test_block_collapse_identity():\n    A = MatrixSymbol('A', 3, 3)\n    B = Identity(3)\n\n    X = BlockMatrix([[A, A], [A, B]])\n    assert block_collapse(X).shape == (6, 6) and block_collapse(X).blockshape == (2, 2)\n\n    assert block_collapse(X).blocks[0, 0] == A\n    assert block_collapse(X).blocks[0, 1] == A\n    assert block_collapse(X).blocks[1, 0] == A\n    assert block_collapse(X).blocks[1, 1] == B\n\n\n\n", "def test_block_collapse_with_different_shapes():\n    A = MatrixSymbol('A', 2, 3)\n    B = MatrixSymbol('B', 3, 4)\n    C = MatrixSymbol('C', 4, 2)\n    D = MatrixSymbol('D', 2, 3)\n\n    X = BlockMatrix([[A, B], [C, D]])\n\n    assert block_collapse(X + BlockDiagMatrix(A, C)).shape == X.shape\n\n\n", "def test_block_collapse_non_square_matrices():\n    A = MatrixSymbol('A', 3, 2)\n    B = MatrixSymbol('B', 2, 3)\n    C = MatrixSymbol('C', 3, 3)\n    D = MatrixSymbol('D', 2, 2)\n\n    X = BlockMatrix([[A, B], [C, D]])\n\n    assert block_collapse(X.T).shape == (2, 5)\n    assert block_collapse(X*Transposed(X)).shape == (3, 3)\n\n    assert block_collapse(X.inverse()).shape == (5, 5)\n\n\n", "def test_block_collapse_sparse():\n    A = ImmutableSparseMatrix([[1, 2], [3, 4]])\n    assert block_collapse(BlockMatrix([[A]])) == A\n    B = BlockMatrix([[A, MatrixSymbol('B', 2, 1)]])\n    assert block_collapse(B).blocks[0, 1] == MatrixSymbol('B', 2, 1)\n\n\n\n"], "sample_836": ["compilation error", "compilation error", "def test_class_distribution_multiclass_multioutput():\n    y = np.array([[1, 0, 2, 2],\n                  [1, 4, 2, 4],\n                  [1, 0, 2, 2],\n                  [1, 4, 2, 4]])\n    (classes, n_classes, class_prior) = class_distribution(y)\n\n    classes_expected = [[1, 0, 2, 4],\n                        [1, 4, 2, 4],\n                        [1, 0, 2, 2],\n                        [1, 4, 2, 4]]\n    n_classes_expected = [4, 4, 3, 4]\n    class_prior_expected = [[1/4, 1/4, 1/4, 1/4],\n                            [1/4, 1/4, 1/4, 1/4],\n                            [1/3, 1/3, 1/3, 0],\n                            [1/4, 1/4, 1/4, 1/4]]\n\n    for k in range(y.shape[1]):\n        assert_array_almost_equal(classes[k], classes_expected[k])\n        assert_array_almost_equal(n_classes[k], n_classes_expected[k])\n        assert_array_almost_equal(class_prior[k], class_prior_expected[k])\n\n\n\n", "compilation error", "compilation error"], "sample_602": ["def test_to_netcdf_encoding():\n    ds = xr.Dataset(\n        {\n            \"a\": ((\"x\", \"y\"), np.zeros((5, 3))),\n            \"b\": ((\"x\",), np.array([1, 2, 3, 4, 5])),\n        },\n        coords={\"x\": np.arange(5), \"y\": np.arange(3)},\n    )\n\n    with xr.backends.NetCDF4DataStore(tempfile.mkstemp()[1], \"w\") as store:\n        ds.to_netcdf(store, encoding={\"a\": {\"zlib\": True, \"complevel\": 3}})\n\n        with open(store.filepath, \"rb\") as f:\n            data = f.read()\n\n        assert b\"encoding=\\\"{'a': {'zlib': True, 'complevel': 3}}\\\"\" in data\n\n\n\n", "def test_to_netcdf_compute_false():\n    ds = xr.Dataset(\n        dict(a=np.arange(5)), coords=dict(x=(\"x\", np.arange(5), dict(units=\"s\")))\n    )\n\n    with patch.object(backends.scipy.NetCDF4DataStore, \"open\") as mock_open:\n        mock_open.return_value = Mock(close=Mock())\n        to_netcdf(ds, target=BytesIO(), compute=False)\n        mock_open.assert_called_once()\n\n\n", "def test_to_netcdf_invalid_data_type():\n    dataset = xr.Dataset(\n        data_vars=dict(a=((\"x\",), np.arange(5).astype(object))),\n        coords=dict(x=np.arange(5)),\n    )\n\n    with pytest.raises(ValueError) as e:\n        to_netcdf(dataset, \"test.nc\", mode=\"w\")\n    assert \"dtype must be a subtype of number, datetime, bool, a fixed sized string, a fixed size unicode string or an object\" in str(e.value)\n", "def test_open_mfdataset_with_remote_urls():\n    # Test with remote URLs for files in a directory list\n    remote_path_list = [\"http://example.org/file1.nc\", \"http://example.org/file2.nc\"]\n    ds_list = [\n        xr.Dataset(\n            dict(a=2 * np.arange(5)),\n            coords=dict(x=(\"x\", np.arange(5), dict(units=\"s\"))),\n        )\n        for _ in range(2)\n    ]\n\n    # Mock the remote files\n        return ds_list[remote_path_list.index(path)]\n\n    # Mock out the open_dataset function\n    with patch(\n        \"xarray.open_dataset.open_dataset\", side_effect=_open_remote_file\n    ):\n        combined_ds = xr.open_mfdataset(remote_path_list)\n\n    # Assert that the combined dataset is as expected\n    assert_identical(ds_list[0].concatenate(ds_list[1]), combined_ds)\n\n\n\n", "def test_to_netcdf_invalid_netcdf_path():\n    with pytest.raises(\n        ValueError,\n        match=r\"unrecognized option 'invalid_netcdf' for engine \"\n        r\"scipy\",\n    ):\n        ds = xr.Dataset(\n            {\"a\": (\"x\", np.arange(5))},\n            coords={\"x\": np.arange(5)},\n        )\n        ds.to_netcdf(\n            \"invalid.nc\",\n            mode=\"w\",\n            engine=\"scipy\",\n            invalid_netcdf=True,\n        )\n"], "sample_702": ["compilation error", "def test_parse_summary_line_no_summary_line(\n    testdir: Testdir,", "compilation error", "def test_pytester_assert_outcomes_warnings_with_custom_message(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import warnings\n\n        class MyWarning(UserWarning):\n            pass\n\n            warnings.warn(\"Custom warning message\", MyWarning)\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1, warnings=1)\n", "compilation error"], "sample_357": ["    def test_suggest_name_with_custom_operations(self):\n        class CustomMigration(migrations.Migration):\n                super().__init__(name, app)\n                self.operations = operations or []\n\n                return self.name\n\n        migration = CustomMigration('0001_custom', 'test_app', [\n            migrations.CreateModel('Person', fields=[]),\n        ])\n        self.assertEqual(migration.suggest_name(), '0001_custom')\n", "    def test_operation_with_no_suggested_name_and_initial_true(self):\n        class Migration(migrations.Migration):\n            initial = True\n            operations = [\n                migrations.RunSQL('SELECT 1 FROM person;'),\n            ]\n\n        migration = Migration('0001_initial', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'initial')\n", "    def test_alter_field_name(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.RenameField(\n                    model_name='Person', old_name='name', new_name='full_name'\n                ),\n            ]\n\n        migration = Migration('0001_rename_field', 'testapp')\n        self.assertEqual(migration.suggest_name(), 'rename_person_name_to_full_name')\n\n\n", "    def test_auto_with_operations(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel('Person', fields=[]),\n            ]\n\n        migration = Migration('0001_initial', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'person')\n", "    def test_operation_with_no_name(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel('Person', fields=[]),\n                migrations.RemoveField(\n                    model_name='Person',\n                    related_name='person',\n                ),\n            ]\n\n        migration = Migration('some_migration', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'person_remove_person')\n"], "sample_1074": ["compilation error", "def test_order():\n    G = PermutationGroup([Permutation(0, 1, 2), Permutation(0, 2, 3)])\n    assert G.order() == 4\n    G = SymmetricGroup(3)\n    assert G.order() == 6\n    G = DihedralGroup(4)\n    assert G.order() == 8\n    G = AlternatingGroup(5)\n    assert G.order() == 60\n    G = CyclicGroup(10)\n    assert G.order() == 10\n    G = AbelianGroup(2, 3, 4)\n    assert G.order() == 24\n    G = PermutationGroup()\n    assert G.order() == 1\n\n\n\n", "compilation error", "compilation error", "compilation error"], "sample_1006": ["def test_factorial_series():\n    n = Symbol('n', integer=True)\n\n    assert factorial(n).series(n, 0, 3) == \\\n        1 - n*EulerGamma + n**2*(EulerGamma**2/2 + pi**2/12) + O(n**3)\n", "def test_subfactorial_rewrite():\n    x = Symbol('x')\n    assert subfactorial(x).rewrite(factorial) == factorial(x)/gamma(x + 1)\n\n\n", "compilation error", "def test_subfactorial_rewrite():\n    x = Symbol('x')\n    assert subfactorial(x).rewrite(gamma) == uppergamma(x + 1, -1)/S.Exp1\n    assert subfactorial(x).rewrite(factorial) == factorial(x)/gamma(x + 1)\n    assert subfactorial(x).rewrite(Product) == 'Product(i, (i, 1, x), (-1)**(i - 1))'\n", "def test_factorial_series_expansion():\n    n = Symbol('n', integer=True)\n\n    assert factorial(n).series(n, 0, 5) == \\\n        1 + n*EulerGamma - n**2*(EulerGamma**2/2 - pi**2/12) + O(n**3)\n"], "sample_1059": ["    def test_jacobi():\n        n = Symbol(\"n\")\n        a = Symbol(\"a\")\n        b = Symbol(\"b\")\n\n        assert jacobi(0, a, b, x) == 1\n        assert jacobi(1, a, b, x) == (2*a + 2*b + 1)*x\n        assert jacobi(2, a, b, x) == (2*a + 2*b + 1)*(2*a + 2*b)*x**2/2 - (a + b + 1)*(a + b)\n\n        assert jacobi(-1, a, b, x) == 0\n        assert jacobi(n, a, b, -x) == (-1)**n*jacobi(n, a, b, x)\n        assert jacobi(n, a, b, 0) == 1\n\n        assert jacobi(n, a, b, oo) == oo\n\n        assert conjugate(jacobi(n, a, b, x)) == jacobi(n, conjugate(a), conjugate(b), conjugate(x))\n\n        _k = Dummy('k')\n        assert jacobi(n, a, b, x).rewrite(\"polynomial\").dummy_eq(\n            Sum((-1)**_k*(a + b + 1)_k*(b + n + _k)!/(a + n + _k)!*\n            (x**(2*_k) * (a + b + 1 + 2*_k)) / (factorial(_k)\n            * (2*_k + 1) * (b + _k)! * (a + _k)!), (_k, 0, n)))\n\n        raises( ValueError, lambda: jacobi(-2.1, a, b, x))\n        raises(ValueError, lambda: jacobi(n,  Rational(5, 2), b, x))\n        raises(ArgumentIndexError, lambda: jacobi(n, a, b, x).fdiff(1))\n        raises(ArgumentIndexError, lambda: jacobi(n, a, b, x).fdiff(3))\n\n\n\n", "compilation error", "def test_gegenbauer():\n    n = Symbol(\"n\")\n    a = Symbol(\"a\")\n\n    # Test edge cases for Gegenbauer polynomial\n    assert gegenbauer(0, a, x) == 1\n    assert gegenbauer(1, a, x) == 2*a*x\n    assert gegenbauer(2, a, x) == ((3*a**2 - 1)*x**2 - a**2)/2\n    assert gegenbauer(3, a, x) == (\n        (5*a**3 - 3*a)*x**3 - (3*a**2 - 1)*(a*x)\n    )/2\n    assert gegenbauer(4, a, x) == (\n        (35*a**4 - 30*a**2 + 3)*x**4 - (30*a**3 - 10*a)*x**2 + a**4 \n    )/8\n\n    # Test negative n\n    assert gegenbauer(-1, a, x) == 1\n    assert gegenbauer(-2, a, x) == 1\n    assert gegenbauer(-3, a, x) == 1\n    \n    # Test a = 0\n    assert gegenbauer(n, 0, x) == legendre(n, x)\n    assert gegenbauer(n, 0, 0) == 1\n\n    # Test a = 1\n    assert gegenbauer(n, 1, x) == chebyshevu(n, x)\n    assert gegenbauer(n, 1, 1) == 1\n\n    # Test a = -1\n    assert gegenbauer(n, -1, x) == 0\n    assert gegenbauer(n, -1, x) == 0\n\n    X = gegenbauer(n, a, x)\n    assert isinstance(X, gegenbauer)\n\n    # Test with large n\n    assert gegenbauer(10, a, x).expand().is_valid\n    assert gegenbauer(100, a, x).expand().is_valid\n\n    assert conjugate(gegenbauer(n, a, x)) == gegenbauer(n, conjugate(a), conjugate(x))\n\n    _k = Dummy('k')\n    assert gegenbauer", "    def test_gegenbauer():\n        n = Symbol(\"n\")\n        a = Symbol(\"a\")\n\n        assert gegenbauer(0, a, x) == 1\n        assert gegenbauer(1, a, x) == 2*a*x\n        assert gegenbauer(2, a, x) == ((3*a**2 - 1)*x**2)/(2)\n        assert gegenbauer(3, a, x) == ((5*a**3 - 3*a)*x**3)/(2)\n        assert gegenbauer(4, a, x) == ((35*a**4 - 30*a**2 + 3)*x**4)/(8)\n        assert gegenbauer(5, a, x) == ((63*a**5 - 70*a**3 + 15*a)*x**5)/(8)\n        assert gegenbauer(6, a, x) == ((231*a**6 - 315*a**4 + 105*a**2 - 5)*x**6)/(16)\n\n        assert gegenbauer(10, -1) == 1\n        assert gegenbauer(11, -1) == -1\n        assert gegenbauer(10, 1) == 1\n        assert gegenbauer(11, 1) == 1\n        assert gegenbauer(10, 0) != 0\n        assert gegenbauer(11, 0) == 0\n\n        assert gegenbauer(-1, a, x) == gegenbauer(1, a, x)\n        k = Symbol('k')\n        assert gegenbauer(5 - k, a, x).subs(k, 2) == \\\n        ((5*a**3 - 3*a)*x**3)/(2)\n\n        assert roots(gegenbauer(4, a, x), x) == {\n            sqrt(Rational(3, 7) - Rational(2, 35)*sqrt(30)): 1,\n            -sqrt(Rational(3, 7) - Rational(2, 35)*sqrt(30)): 1,\n            sqrt(Rational(3, 7) + Rational(2, 35)*sqrt(30)):", "def test_gegenbauer():\n    n = Symbol(\"n\")\n    a = Symbol(\"a\")\n\n    # Test negative values of a\n    assert gegenbauer(-1, a, x) == 0\n    assert gegenbauer(0, -1, x) == 1\n    assert gegenbauer(1, -1, x) == x\n\n    # Test gegenbauer with high order\n    assert gegenbauer(10, 2, x).expand().count('*') == 40\n\n    # Test convergence at x = 1\n    assert gegenbauer(n, a, 1).doit() == 2**n*gamma(a + n/2)/gamma(a)*\\\n            1/(gamma(n/2 + 1)) \n\n    # Test the case where n is a float\n    assert gegenbauer(2.5, 0.5, x).expand().count('*') > 10\n\n    # Test the derivative\n    assert diff(gegenbauer(n, a, x), x).expand() == a*x*(\n        a + n - 1)*gegenbauer(n - 1, a, x) - (a + n)*gegenbauer(n - 1, a, x)\n\n    _k = Dummy('k')\n\n    assert gegenbauer(n, a, x).rewrite(\"polynomial\").dummy_eq(\n        Sum((-1)**_k*(2*x)**(-2*_k + n)*RisingFactorial(a, -_k + n) /\n        (factorial(_k)*factorial(-2*_k + n)), (_k, 0, floor(n/2))))\n    raises(ArgumentIndexError, lambda: gegenbauer(n, a, x).fdiff(4))\n\n"], "sample_435": ["    def test_password_too_short(self):\n        user = User.objects.get(username=\"testclient\")\n        data = {\"password1\": \"a\", \"password2\": \"a\"}\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        self.assertEqual(\n            form.errors[\"password1\"],\n            [form.error_messages[\"password_too_short\"]],\n        )\n        self.assertEqual(form.errors[\"password2\"], [form.error_messages[\"password_too_short\"]])\n        self.assertEqual(form.changed_data, [])\n\n", "    def test_exclude_password_field(self):\n        class CustomUserChangeForm(AdminPasswordChangeForm):\n            password = None\n\n            class Meta(AdminPasswordChangeForm.Meta):\n                exclude = [\"password\"]\n\n        user = User.objects.get(username=\"testclient\")\n        form = CustomUserChangeForm(user)\n        self.assertNotIn(\"password1\", form.fields)\n        self.assertNotIn(\"password2\", form.fields)\n\n\n\n", "    def test_password_complexity(self):\n        user = User.objects.get(username=\"testclient\")\n        data = {\"password1\": \"shortpassword\", \"password2\": \"shortpassword\"}\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        self.assertIn(\n            \"This password is too short. It must contain at least 12 characters.\",\n            form.errors[\"password1\"],\n        )\n        self.assertIn(\n            \"This password is too short. It must contain at least 12 characters.\",\n            form.errors[\"password2\"],\n        )\n\n", "    def test_bug_19349_bound_password_field(self):\n        user = User.objects.get(username=\"testclient\")\n        form = AdminPasswordChangeForm(data={}, instance=user)\n        # When rendering the bound password field,\n        # ReadOnlyPasswordHashWidget needs the initial\n        # value to render correctly\n        self.assertEqual(form.initial[\"password\"], form[\"password\"].value())\n\n\n", "    def test_render_with_no_password(self):\n        user = User.objects.get(username=\"testclient\")\n        form = AdminPasswordChangeForm(user)\n        rendered_html = form.render_with_context(request=None)[0]\n        self.assertTrue(rendered_html.startswith(\"<div\"))\n        self.assertIn('id=\"id_password1\"', rendered_html)\n        self.assertIn('class=\"form-control\"', rendered_html)\n        self.assertIn('placeholder=\"Enter a new password\"', rendered_html)\n        self.assertIn('id=\"id_password2\"', rendered_html)\n        self.assertIn('class=\"form-control\"', rendered_html)\n        self.assertIn('placeholder=\"Confirm new password\"', rendered_html)\n"], "sample_232": ["    def test_contains_with_raw_expression(self):\n        expr = RawSQL(self.raw_sql, ['{\"a\": \"b\", \"c\": 1}'])\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(value__contains=expr),\n            [self.objs[3]],\n        )\n\n", "    def test_contains_with_key_transform_invalid_type(self):\n        with self.assertRaises(TypeError):\n            NullableJSONModel.objects.filter(value__baz__contains=14)\n\n\n\n", "    def test_json_functions(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(\n                value__json_extract('a')=14\n            ),\n            self.objs[3:5],\n        )\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(\n                value__json_extract_path_text('a', 'b')='x'\n            ),\n            [self.objs[4]],\n        )\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(\n                value__json_length('baz')==11\n            ),\n            [self.objs[7]],\n        )\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(\n                value__json_object_keys('baz').contains('a')\n            ),\n            [self.objs[7]],\n        )\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(\n                value__json_array_length('d')==2\n            ),\n            [self.objs[4]],\n        )\n", "    def test_contains_with_nested_key_lookup_transform(self):\n        tests = [\n            ('value__baz__d__contains', 'e'),\n            ('value__baz__d__contains', [{'f': 'g'}]),\n            ('value__baz__1__contains', 'e'),\n            ('value__baz__1__contains', [{'f': 'g'}]),\n            ('value__baz__contains', KeyTransform('d', 'value')),\n            ('value__contains', KeyTransform('baz', KeyTransform('d', 'value'))),\n        ]\n        for lookup, value in tests:\n            with self.subTest(lookup=lookup, value=value):\n                self.assertIs(NullableJSONModel.objects.filter(\n                    **{lookup: value},\n                ).exists(), True)\n\n", "    def test_contains_with_nested_key_transform(self):\n        self.assertIs(NullableJSONModel.objects.filter(\n            value__d__1__contains=KeyTransform('f', 'value')\n        ).exists(), True)\n\n"], "sample_31": ["    def test_write_latex_with_units(self, write, tmp_path, format):\n        fp = tmp_path / \"test_write_latex_with_units.tex\"\n        cosmo = Cosmology(H0=70 * u.km / u.s / u.Mpc, Om0=0.3)\n        write(fp, format=format, cosmology=cosmo)\n        tbl = QTable.read(fp, format=format)\n        # assert that units are correctly represented in LaTeX\n        assert tbl[\"H0\"].unit.to_string() == \"$km\\,s^{-1}Mpc^{-1}$\"\n        assert tbl[\"Om0\"].unit.to_string() == \"\" \n", "    def test_write_latex_with_metadata(self, write, tmp_path, format):\n        \"\"\"Test writing cosmology with metadata.\"\"\"\n        fp = tmp_path / \"test_write_latex_with_metadata.tex\"\n        cosmo = self.cosmo\n        write(fp, format=format, cosmology_in_meta=True)\n        tbl = QTable.read(fp)\n        # assert metadata is correctly stored in the table\n        assert \"Cosmology\" in tbl.meta\n        assert tbl.meta[\"Cosmology\"] == str(type(cosmo))\n\n\n\n", "    def test_write_latex_no_params(self, write, tmp_path, format):\n        \"\"\"Test writing cosmology with no parameters\"\"\"\n        from astropy.cosmology import FlatLambdaCDM\n        cosmo = FlatLambdaCDM(H0=67.4, Om0=0.315, Ode0=0.685)\n        fp = tmp_path / \"test_write_latex_no_params.tex\"\n        write(fp, format=format, cosmology=cosmo)\n", "    def test_write_latex_empty_cosmology(self, write, tmp_path, format):\n        \"\"\"Test writing an empty cosmology to LaTeX.\"\"\"\n        fp = tmp_path / \"test_write_latex_empty_cosmology.tex\"\n        empty_cosmology = Cosmology()\n        write(fp, format=format, cosmology=empty_cosmology)\n        tbl = QTable.read(fp)\n        assert len(tbl) == len(_FORMAT_TABLE) + 2  # +2 for Cosmology and name\n", "    def test_write_latex_with_metadata(self, write, tmp_path, format):\n        fp = tmp_path / \"test_write_latex_with_metadata.tex\"\n        cosmo = self.cosmo\n        cosmo.metadata = {\"test_key\": \"test_value\"}\n        write(fp, format=format, latex_names=True)\n        tbl = QTable.read(fp)\n        # assert that metadata has been stored\n        assert tbl.meta['test_key'] == \"test_value\" \n"], "sample_1178": ["def test_IfElse():\n    x = Variable('x', real)\n    y = Variable('y', real)\n    n = Variable('n', integer)\n\n    if_body = [Assignment(x, 2*y)]\n    else_body = [Assignment(x, -y)]\n    ifelse = IfElse(Lt(x, 0), if_body, else_body)\n\n    assert ifelse.condition == Lt(x, 0)\n    assert ifelse.if_body == CodeBlock(*if_body)\n    assert ifelse.else_body == CodeBlock(*else_body)\n    assert ifelse.func(*ifelse.args) == ifelse\n\n    ifelse2 = IfElse(Gt(x, 0), if_body, else_body)\n    assert ifelse != ifelse2\n\n\n", "def test_FunctionCall__with_kwargs():\n    fc = FunctionCall('fma', {\n        'a': 2,\n        'b': 3,\n        'c': 4\n    })\n    assert len(fc.function_args) == 0\n    assert len(fc.keyword_args) == 3\n    assert fc.keyword_args[0] == ('a', 2)\n    assert fc.keyword_args[1] == ('b', 3)\n    assert fc.keyword_args[2] == ('c', 4)\n\n    assert fc == FunctionCall('fma', keyword_args={'a': 2, 'b': 3, 'c': 4})\n    assert fc != FunctionCall('fma', keyword_args={'a': 2, 'b': 3})\n\n\n\n", "def test_Call():\n    fc = Call(Symbol('foo'), [x, y])\n    assert fc.function == Symbol('foo')\n    assert fc.function_args == [x, y]\n    assert fc.func(*fc.args) == fc\n    \n    # Check for proper representation of `call` with special methods\n    fc = Call(Attribute(x, \"__len__\"), [])\n    assert fc.function == Attribute(x, \"__len__\")\n    assert fc.function_args == []\n\n\n", "def test_For():\n    n = symbols('n', integer=True)\n    x = symbols('x', real=True)\n    y = symbols('y', real=True)\n    \n    for_expr = For(n, Range(0, 3), (Assignment(x, n*n), Assignment(y, x + 1)))\n    assert for_expr.variable == n\n    assert for_expr.iter == Range(0, 3)\n    assert for_expr.body == CodeBlock(Assignment(x, n*n), Assignment(y, x + 1))\n    assert for_expr.func(*for_expr.args) == for_expr\n\n\n    for_expr2 = For(n, 0, 3, (Assignment(x, n*n), Assignment(y, x + 1)))\n    assert for_expr == for_expr2\n\n\n    raises(TypeError, lambda: For(n, x, (Assignment(x, n*n), Assignment(y, x + 1))))\n    raises(TypeError, lambda: For(n, 1, 3, (Assignment(x, n*y))))\n    \n    \n    # Check topological sorting\n    for_expr3 = For(n, 0, 3, (Assignment(x, n*n), Assignment(y, x + n)))\n    for_expr4 = For(n, 0, 3, (Assignment(y, x*n), Assignment(x, n*n)))\n\n\n\n", "compilation error"], "sample_180": ["    def test_check_unique_constraint_with_multiple_keys(self):\n        class Model(models.Model):\n            name = models.CharField(max_length=100)\n            age = models.IntegerField()\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(\n                        fields=['name', 'age'],\n                        name='unique_name_age',\n                    ),\n                ]\n\n        errors = Model.check(databases=self.databases)\n        expected = [] if connection.features.supports_unique_contraints_on_multiple_keys else [\n            Warning(\n                '%s does not support unique constraints on multiple keys.'\n                % connection.display_name,\n                hint=(\n                    \"A constraint won't be created. Silence this warning if \"\n                    \"you don't care about it.\"\n                ),\n                obj=Model,\n                id='models.W039',\n            ),\n        ]\n        self.assertEqual(errors, expected)\n\n\n\n", "    def test_unique_constraint_duplicate_name(self):\n        class Model(models.Model):\n            field1 = models.IntegerField()\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(fields=['field1'], name='unique_name'),\n                    models.UniqueConstraint(fields=['field1'], name='unique_name'),\n                ]\n\n        self.assertEqual(Model.check(databases=self.databases), [\n            Error(\n                \"Constraint 'unique_name' is defined more than once.\",\n                obj=Model,\n                id='models.E035',\n            ),\n        ])\n\n\n\n", "    def test_unique_constraint_pointing_to_self_related_name(self):\n        class Model(models.Model):\n            other = models.ForeignKey('self', models.CASCADE, related_name='related_models')\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(fields=['other__id'], name='unique_related')\n                ]\n\n        self.assertEqual(Model.check(databases=self.databases), [\n            Error(\n                \"'constraints' refers to a related field (other__id) \"\n                \"which is not allowed in a 'UniqueConstraint'.\",\n                obj=Model,\n                id='models.E017',\n            ),\n        ])\n\n", "    def test_unique_constraint_with_fk_and_related_name(self):\n        class Target(models.Model):\n            pass\n\n        class Model(models.Model):\n            fk = models.ForeignKey(Target, models.CASCADE, related_name='related')\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(fields=['fk_id', 'related__name'], name='name'),\n                ]\n\n        self.assertEqual(Model.check(databases=self.databases), [])\n", "    def test_unique_constraint_with_duplicate_field(self):\n        class Model(models.Model):\n            field1 = models.IntegerField()\n            field2 = models.CharField(max_length=100)\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(\n                        fields=['field1', 'field2', 'field1'],\n                        name='unique_constraint',\n                    ),\n                ]\n\n        self.assertEqual(Model.check(databases=self.databases), [\n            Error(\n                \"'constraints' defines a unique constraint with duplicate fields: ['field1', 'field2', 'field1']\",\n                obj=Model,\n                id='models.E039',\n            ),\n        ])\n\n\n"], "sample_1003": ["def test_allowed_flags():\n    options = {'domain': ZZ}\n\n    allowed_flags = ['frac']\n\n    allowed_flags_result = allowed_flags\n    with raises(FlagError):\n        allowed_flags(options, allowed_flags_result)\n\n", "def test_allowed_flags():\n    args = {'domain': ZZ, 'field': True}\n    allowed_flags(['frac'])\n    raises(FlagError, lambda: allowed_flags(['frac'], args))\n\n", "def test_allowed_flags():\n    with raises(FlagError, match=\"'frac' flag is not allowed in this context\"):\n        allowed_flags({'domain': ZZ}, ['frac'])\n", "def test_allowed_flags():\n    allowed = {'domain': ZZ}\n    \n    raises(FlagError, lambda: allowed_flags(allowed, ['frac']))\n\n    raises(FlagError, lambda: allowed_flags(allowed, ['gaussian']))\n", "def test_allowed_flags():\n    allowed_flags({'domain': ZZ}, [])\n\n    with raises(FlagError, match=\"'frac' flag is not allowed\"):\n        allowed_flags({'domain': ZZ, 'frac': True}, [])\n\n\n    allowed_flags({'domain': ZZ, 'frac': True}, ['frac'])\n"], "sample_1096": ["def test_subs_with_indexed_and_symbols():\n    A = IndexedBase(\"A\")\n    i, j, k = symbols(\"i,j,k\")\n    x, y, z = symbols(\"x,y,z\")\n    \n    assert Subs(A[i], A[i], x).subs(x, y) == A[i] \n    assert Subs(A[i] * x, A[i], x).subs(x, y) == A[i] * y\n    assert Subs(A[i] * x, x, y).subs(y, z) == A[i] * z\n    assert Subs(A[i] * x, x, x).subs(x, y) == A[i] * x\n    assert Subs(A[i] + A[j], A[i], x).subs(x, y) == A[i] + A[j]\n    assert Subs(A[i] + A[j], A[j], x).subs(x, y) == A[i] + A[j]\n    assert Subs(A[i] + A[j] * x, A[i], x).subs(x, y) == A[i] + A[j] * y\n    assert Subs(A[i] + A[j] * x, A[j], x).subs(x, y) == A[i] + A[j] * y\n\n\n\n", "def test_issue_19032():\n    x = IndexedBase('x')\n    i = symbols('i', integer=True)\n    expr = x[i] * S(2)\n    assert expr.series(x[i], n=3) == 2*x[i] + 0*x[i]**2 + 0*x[i]**3 + Order(x[i]**4, x[i])\n", "compilation error", "compilation error", "def test_Indexed_free_symbols():\n    i, j, k = symbols('i j k', integer=True)\n    A = Indexed('A', i, j)\n    assert A.free_symbols == {A.base.label, i, j}\n    assert IndexedBase('A').free_symbols == {A.base.label}\n    assert Indexed('A', i).free_symbols == {A.base.label, i}\n\n\n\n"], "sample_665": ["def test_package_discovery_with_mixed_files(testdir):\n    testdir.mkdir(\"test_pkg\").ensure(\"__init__.py\")\n    testdir.mkdir(\"test_pkg/sub\").ensure(\"__init__.py\")\n    testdir.mkdir(\"test_pkg/sub/\").ensure(\"test_sub.py\").write(\"def test_sub(): pass\")\n    testdir.mkdir(\"test_pkg/\").ensure(\"test_pkg.py\").write(\"def test_pkg(): pass\")\n    result = testdir.runpytest(\"test_pkg\")\n    result.stdout.fnmatch_lines([\"test_pkg/test_pkg.py::test_pkg PASSED*\", \"test_pkg/sub/test_sub.py::test_sub PASSED*\"])\n\n\n\n", "compilation error", "def test_collect_nested_packages(testdir):\n    testdir.mkdir(\"pkg1\").mkdir(\"pkg2\").mkdir(\"pkg3\")\n    testdir.mkdir(\"pkg1\").join(\"pkg2\").join(\"__init__.py\").write(\"\")\n    testdir.mkdir(\"pkg1\").join(\"pkg2\").join(\"pkg3\").join(\"__init__.py\").write(\"\")\n    testdir.mkdir(\"pkg1\").join(\"pkg2\").join(\"pkg3\").join(\"test_file.py\").write(\n        \"def test_nested(): pass\"\n    )\n    result = testdir.runpytest(str(testdir.tmpdir / \"pkg1\"))\n    result.stdout.fnmatch_lines(\n        [\n            \"pkg1/pkg2/pkg3/test_file.py::test_nested PASSED*\",\n            \"*1 passed in*\",\n        ]\n    )\n", "compilation error", "def test_collect_import_error_in_early_phase(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import asdfasdfasdf\n\n            pass\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\"*ERROR collecting test_collector_import_error.py*\", \"*No module named *asdfasdfasdf*\"]\n    )\n"], "sample_341": ["    def test_all_valid_mixed_formset_types(self):\n        ChoiceForm = formset_factory(Choice)\n        TextForm = formset_factory(forms.CharField)\n        formset1 = ChoiceForm(data={'choice': 'Zero'})\n        formset2 = TextForm(data={'charfield': 'One'})\n        self.assertIs(all_valid((formset1, formset2)), True)\n\n", "    def test_all_valid_with_formset_errors(self):\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '',\n            'choices-1-choice': 'One',\n            'choices-1-votes': '',\n        }\n        ChoiceFormSet = formset_factory(Choice, validate_on_bind=False)\n        formset1 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        formset2 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        formset1._errors = {'__all__': ['This is a formset error.']}\n        self.assertIs(all_valid((formset1, formset2)), False)\n        self.assertEqual(formset1._errors, {'__all__': ['This is a formset error.']})\n        self.assertEqual(formset2._errors, [{'votes': ['This field is required.']}, {'votes': ['This field is required.']}])\n\n\n\n", "    def test_all_valid_handles_formset_errors(self):\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '',\n            'choices-1-choice': 'One',\n            'choices-1-votes': '',\n        }\n        class FormSetWithErrors(BaseFormSet):\n                raise ValidationError('This is an error from clean()')\n        ChoiceFormSet = formset_factory(Choice, formset=FormSetWithErrors)\n        formset1 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        formset2 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n\n        self.assertIs(all_valid((formset1, formset2)), False)\n        self.assertEqual(formset1._errors, [{'': ['This is an error from clean()']}] * 2)\n        self.assertEqual(formset2._errors, [{'': ['This is an error from clean()']}] * 2)\n", "    def test_all_valid_with_formset_errors(self):\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '',\n            'choices-1-choice': 'One',\n            'choices-1-votes': '',\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset1 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        formset2 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        formset1.non_form_errors = ['This is a custom error']\n        self.assertIs(all_valid((formset1, formset2)), False)\n        expected_errors = [{'votes': ['This field is required.']}, {'votes': ['This field is required.']}]\n        self.assertEqual(formset1._errors, expected_errors)\n        self.assertEqual(formset2._errors, expected_errors)\n", "    def test_formset_errors_are_not_lost_after_clean_and_valid(self):\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '',\n            'choices-1-choice': 'One',\n            'choices-1-votes': '',\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        formset.clean()  # Trigger formset.clean()\n        self.assertFalse(formset.is_valid())\n        expected_errors = [{'votes': ['This field is required.']}, {'votes': ['This field is required.']}]\n        self.assertEqual(formset._errors, expected_errors)\n\n"], "sample_1123": ["compilation error", "compilation error", "def test_issue_17651():\n    with warns_deprecated_sympy():\n        cs = ConditionSet(x, Eq(x, 0), S.Reals)\n        assert cs == ConditionSet(x, Eq(x, 0), S.Reals)\n        assert cs.args[1] is Eq(x, 0)\n", "def test_ConditionSet_from_unsolved_eq():\n    c = ConditionSet(\n        (x, y),\n        Eq(x + y, 0),\n        S.Reals\n    )\n    assert c.free_symbols == {x, y}\n    assert c.sym == (x, y)\n    assert c.condition == Eq(x + y, 0)\n    assert c.base_set == S.Reals\n\n    c = ConditionSet((x, y),\n                    (Eq(x, 1) & Eq(y, 2)), \n                    S.Integers)\n    assert c.free_symbols == set()\n    assert c.sym == (x, y)\n    assert c.condition == Eq(x, 1) & Eq(y, 2)\n    assert c.base_set == S.Integers\n", "compilation error"], "sample_327": ["    def test_encoder_with_custom_object(self):\n        class CustomEncoder(DjangoJSONEncoder):\n                if isinstance(obj, uuid.UUID):\n                    return str(obj)\n                return super().default(obj)\n\n        value = {'a': uuid.uuid4()}\n        encoded_value = json.dumps(value, cls=CustomEncoder)\n        field = JSONField(encoder=CustomEncoder)\n        self.assertEqual(field.prepare_value(value), encoded_value)\n        self.assertEqual(field.clean(encoded_value), value)\n\n", "    def test_custom_encoder_decoder_object_hook(self):\n        class CustomDecoder(json.JSONDecoder):\n                return super().__init__(object_hook=self.as_uuid, *args, **kwargs)\n\n                if 'uuid' in dct:\n                    dct['uuid'] = uuid.UUID(dct['uuid'])\n                return dct\n\n        value = {'uuid': 'c141e152-6550-4172-a784-05448d98204b'}\n        encoded_value = json.dumps(value, cls=DjangoJSONEncoder)\n        field = JSONField(encoder=DjangoJSONEncoder, decoder=CustomDecoder)\n        self.assertEqual(field.prepare_value(value), encoded_value)\n        self.assertEqual(field.clean(encoded_value)['uuid'], uuid.UUID('c141e152-6550-4172-a784-05448d98204b'))\n", "    def test_unicode_support(self):\n        from django.utils.encoding import force_str\n        field = JSONField(allow_unicode=True)\n\n        # Ensure that unicode strings are properly serialized and deserialized\n        unicode_string = '\u4f60\u597d\uff0c\u4e16\u754c'\n        json_str = field.prepare_value(unicode_string)\n        self.assertEqual(force_str(json_str), f'\"{unicode_string}\"')\n        self.assertEqual(field.to_python(json_str), unicode_string)\n\n        # Test deserialization of unicode strings from invalid JSON\n        with self.assertRaisesMessage(ValidationError, 'Enter a valid JSON.'):\n            field.clean('{ \"key\": \"\u4f60\u597d\uff0c\u4e16\u754c\" }')\n\n", "    def test_custom_encoder_decoder_errors(self):\n        class CustomDecoder(json.JSONDecoder):\n                return super().__init__(object_hook=self.as_uuid, *args, **kwargs)\n\n                if 'uuid' in dct:\n                    try:\n                        dct['uuid'] = uuid.UUID(dct['uuid'])\n                    except ValueError:\n                        raise json.JSONDecodeError(\"Invalid UUID format\", None, dct['uuid'])\n                return dct\n\n        field = JSONField(encoder=DjangoJSONEncoder, decoder=CustomDecoder)\n\n        with self.assertRaisesMessage(ValidationError, \"Invalid UUID format\"):\n            field.clean('{\"uuid\": \"invaliduuid\"}')\n\n\n\n", "    def test_custom_encoder_decoder_with_errors(self):\n        class CustomDecoder(json.JSONDecoder):\n                return super().__init__(object_hook=self.as_uuid, *args, **kwargs)\n\n                if 'uuid' in dct:\n                    # Simulate an error decoding UUID\n                    raise ValueError(\"Invalid UUID format\")\n                return dct\n\n        value = {'uuid': uuid.UUID('{c141e152-6550-4172-a784-05448d98204b}')}\n        encoded_value = '{\"uuid\": \"c141e152-6550-4172-a784-05448d98204b\"}'\n        field = JSONField(encoder=DjangoJSONEncoder, decoder=CustomDecoder)\n        with self.assertRaisesMessage(ValidationError, 'Enter a valid JSON.'):\n            field.clean(encoded_value)\n\n\n"], "sample_843": ["def test_kernel_is_stationary(kernel):\n    # Test that the is_stationary method of a kernel is working correctly.\n    if isinstance(kernel, PairwiseKernel):\n        # For PairwiseKernel: stationarity depends on metric\n        if kernel.metric in [\"rbf\"]:\n            assert kernel.is_stationary()\n        else:\n            assert not kernel.is_stationary()\n    else:\n        assert kernel.is_stationary() == kernel.hyperparameters[0].is_stationary\n\n\n", "def test_kernel_from_dict(kernel):\n    # Test that the kernel can be reconstructed from its parameter dictionary.\n\n    params = kernel.get_params()\n    kernel_from_dict = KernelOperator.from_dict(params)\n    assert kernel_from_dict.__class__ == kernel.__class__\n    assert_almost_equal(kernel(X), kernel_from_dict(X))", "def test_kernel_repr_with_bounds(kernel):\n    repr_string = repr(kernel)\n    for hyperparameter in kernel.hyperparameters:\n        if hyperparameter.bounds is not None:\n            assert f\"{hyperparameter.name}={kernel.get_params()[hyperparameter.name]}\" in repr_string\n", "def test_kernel_is_stationary(kernel):\n    # Test is_stationary method for kernels with and without\n    # isotropic length scales.\n\n    assert (kernel.is_stationary() ==\n            False if isinstance(kernel, DotProduct) else (\n                kernel.is_stationary() ==\n                not isinstance(kernel, PairwiseKernel]))\n", "def test_kernel_with_fixed_params(kernel):\n    # Test that kernels with fixed parameters behave as expected\n    for hyperparameter in kernel.hyperparameters:\n        if hyperparameter.fixed:\n            # Set a fixed parameter to a value\n            kernel_fixed = kernel.clone()\n            # Setting them via set_params does not modify the fixed\n            # parameters\n            kernel_fixed.set_params(**{hyperparameter.name: 100})\n            assert kernel_fixed.hyperparameters[0].value == 100\n            assert kernel_fixed.theta[0] == np.log(kernel_fixed.hyperparameters[0].value)\n            assert kernel_fixed.theta[0] == kernel.theta[0]\n            assert kernel_fixed.get_params()[hyperparameter.name] == 100\n    \n"], "sample_441": ["    def test_password_too_short(self):\n        user = User.objects.get(username=\"testclient\")\n        data = {\"password1\": \"123\", \"password2\": \"123\"}\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        self.assertEqual(\n            form.errors[\"password1\"],\n            [\n                form.error_messages[\"password_too_short\"]\n            ],\n        )\n        self.assertEqual(\n            form.errors[\"password2\"],\n            [\n                form.error_messages[\"password_too_short\"]\n            ],\n        )\n        self.assertEqual(form.changed_data, [])\n\n\n\n", "    def test_password_complexity_validation(self):\n        user = User.objects.get(username=\"testclient\")\n        data = {\"password1\": \"password\", \"password2\": \"password\"}\n        form = AdminPasswordChangeForm(user, data)\n        self.assertTrue(form.is_valid())  # Password complexity might still pass  \n        \n        # Set a stricter password complexity requirement to test\n        from django.contrib.auth.password_validation import validate_password\n        validate_password.minimum_length = 10\n\n        with self.assertRaises(ValidationError):\n            form.save() \n        \n        validate_password.minimum_length = 8  # Reset to default\n\n\n\n", "    def test_password_reset_token_validity(self):\n        user = User.objects.create_user(\"testuser\", \"test@example.com\", \"testpassword\")\n        reset_token = PasswordResetToken.objects.create(user=user, key=\"abcdefg123456\")\n        form = AdminPasswordChangeForm(instance=user)\n        form.fields[\"reset_token\"] = forms.CharField()\n        self.assertFalse(form.is_valid())\n        form.cleaned_data[\"reset_token\"] = reset_token.key\n        self.assertTrue(form.is_valid())\n        \n        # Check that token is invalid after expiration\n        reset_token.created_at = timezone.now() - timedelta(days=7)\n        reset_token.save()\n        form.cleaned_data[\"reset_token\"] = reset_token.key\n        self.assertFalse(form.is_valid())\n", "    def test_invalid_password_errors(self):\n        user = User.objects.get(username=\"testclient\")\n        data = {\"password1\": \"abcde\", \"password2\": \"fghij\"}\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        validation_errors = [\n            v for v in form.errors[\"password1\"]\n            if v\n            and not v.startswith(\n                \"This field is required.\"\n            )\n        ]\n        self.assertEqual(\n            len(validation_errors),\n            len(password_validation.PASSWORD_VALIDATORS),\n        )\n        for i, error in enumerate(validation_errors):\n            self.assertIn(\n                error,\n                [\n                    v\n                    for v in form.error_messages.values()\n                    if isinstance(v, list)\n                ][i],\n            )", "    def test_bug_22532_password_reset_with_extra_form_fields(self):\n        class MyUserChangeForm(forms.ModelForm):\n\n            class Meta:\n                model = User\n                fields = [\"username\", \"password\"]\n\n        form = MyUserChangeForm(data={})\n        self.assertIn(\"password\", form.fields)\n\n\n"], "sample_1049": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_765": ["def test_brier_score_loss_multilabel():\n    y_true = np.array([[0, 1], [1, 0], [0, 1], [1, 1]])\n    y_pred = np.array([[0.1, 0.9], [0.8, 0.2], [0.3, 0.7], [0.6, 0.4]])\n    true_score = linalg.norm(y_true - y_pred, ord=2, axis=1).mean()\n    assert_almost_equal(brier_score_loss(y_true, y_pred), true_score)\n", "    def test_balanced_accuracy_score_with_missing_labels():\n        y_true = np.array([0, 1, 2, 2])\n        y_pred = np.array([0, 1, 2, 3])\n        labels = np.array([0, 1, 2, 3])\n        balanced = balanced_accuracy_score(y_true, y_pred, labels=labels)\n        assert_almost_equal(balanced, 1.0)\n", "def test_sparse_matrices_in_hinge_loss():\n    # tests if hinge loss handles sparse matrices as input\n    y_true = np.array([0, 1, 2, 1, 3])\n    pred_decision = sp.csr_matrix([\n        [+0.36, -0.17, 0.58, -0.99],\n        [-0.55, -0.38, -0.48, -0.58],\n        [-1.45, -0.58, -0.38, -0.17],\n        [-0.55, -0.38, -0.48, -0.58],\n        [-1.45, -0.58, -0.38, -0.17]\n    ])\n    dummy_losses = np.array([\n        1 - pred_decision[0][0] + pred_decision[0][1],\n        1 - pred_decision[1][1] + pred_decision[1][2],\n        1 - pred_decision[2][2] + pred_decision[2][3],\n        1 - pred_decision[3][1] + pred_decision[3][2],\n        1 - pred_decision[4][3] + pred_decision[4][2]\n    ])\n    np.clip(dummy_losses, 0, None, out=dummy_losses)\n    dummy_hinge_loss = np.mean(dummy_losses)\n    assert_almost_equal(hinge_loss(y_true, pred_decision),\n                        dummy_hinge_loss)\n", "def test_binary_classification_metrics_with_multilabel_data():\n    y_true = [[0, 1], [1, 0], [0, 1], [1, 1]]\n    y_pred = [[0.1, 0.9], [0.8, 0.2], [0.2, 0.8], [0.9, 0.1]]\n\n    # Check for appropriate warnings\n    with pytest.warns(UserWarning,\n                     match=\"Multilabel data is not supported\"):\n        precision_score(y_true, y_pred)\n    with pytest.warns(UserWarning,\n                     match=\"Multilabel data is not supported\"):\n        recall_score(y_true, y_pred)\n    with pytest.warns(UserWarning,\n                     match=\"Multilabel data is not supported\"):\n        f1_score(y_true, y_pred)\n\n", "compilation error"], "sample_685": ["def test_log_file_level_controls_file_logging(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n            logger = logging.getLogger('catchlog')\n            plugin = request.config.pluginmanager.getplugin('logging-plugin')\n            assert plugin.log_file_level == logging.DEBUG\n            logger.debug(\"DEBUG message to file\")\n            logger.info(\"INFO message to file\")\n            logger.warning(\"WARNING message to file\")\n            logger.error(\"ERROR message to file\")\n            logger.critical(\"CRITICAL message to file\")\n    \"\"\"\n    )\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_file_level=WARNING\n    \"\"\"\n    )\n\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*WARNING message to file*\"])\n    result.stdout.no_fnmatch_lines([\"*DEBUG message to file*\", \"*INFO message to file*\", \"*ERROR message to file*\", \"*CRITICAL message to file*\"])\n    assert result.ret == 0\n", "def test_caplog_handles_custom_formatters(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n        import pytest\n        from _pytest.logging import caplog_records_key\n\n        class CustomFormatter(logging.Formatter):\n                return f\"[{record.levelname}] {record.message}\"\n\n            logger = logging.getLogger(__name__)\n            logger.setLevel(logging.DEBUG)\n            handler = logging.StreamHandler()\n            handler.setFormatter(CustomFormatter())\n            logger.addHandler(handler)\n\n            logger.debug(\"Custom formatted DEBUG message\")\n            logger.info(\"Custom formatted INFO message\")\n            logger.warning(\"Custom formatted WARNING message\")\n\n            plugin = request.config.pluginmanager.getplugin('logging-plugin')\n            assert plugin.log_level == logging.WARNING\n\n            assert caplog.records[0].levelname == \"DEBUG\"\n            assert caplog.records[0].msg == \"[DEBUG] Custom formatted DEBUG message\"\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*[DEBUG] Custom formatted DEBUG message*\",\n            \"*[INFO] Custom formatted INFO message*\",\n            \"*[WARNING] Custom formatted WARNING message*\",\n        ]\n    )\n    assert result.ret == 0\n", "    def test_log_file_path_setting(testdir, caplog):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n            import logging\n                plugin = pytest.current_config.pluginmanager.getplugin('logging-plugin')\n                plugin.set_log_path('path/to/log.txt')\n                logging.info('log message')\n            \"\"\"\n        )\n        testdir.makeini(\n            \"\"\"\n            [pytest]\n            log_file = 'path/to/log.txt'\n            log_level = INFO\n            \"\"\"\n        )\n        result = testdir.runpytest()\n        assert result.ret == 0\n", "def test_log_format(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n            assert caplog.text == ''\n\n            caplog.set_level(logging.INFO)\n\n            logger = logging.getLogger('catchlog')\n\n            logger.info(\"Test log message\")\n        \"\"\"\n    )\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_format = %(asctime)s - %(name)s - %(levelname)s - %(message)s\n    \"\"\"\n    )\n\n    result = testdir.runpytest()\n    assert result.ret == 0\n", "        def test_custom_formatter(caplog):\n            caplog.set_level(logging.INFO)\n            logger.info(\"This log message has been formatted\")\n            assert \"This log message has been formatted\" in caplog.text"], "sample_1102": ["def test_issue_18250():\n    assert poly(x**2 + y**2, x, y).as_expr() == x**2 + y**2\n    assert poly(x**2 + y**2, y, x).as_expr() == x**2 + y**2\n", "def test_issue_18929():\n    p = Poly(x**3 + 2*x - 1, x)\n    assert p.is_squarefree() is True\n    p = Poly(x**2 + x, x)\n    assert p.is_squarefree() is False\n\n\n", "def test_issue_18311():\n    assert Poly(x**3 + 2*x + 1, x, domain='ZZ[y]').as_expr() == x**3 + 2*x + 1\n", "def test_issue_19481():\n    p = Poly(x**3 + y**3, x, y)\n    q = Poly(x**3 + y**3, y, x)\n    assert p != q\n", "def test_issue_18455():\n    assert groebner([x**2 + xy - y**2, x - y], x, y, order='lex') == [x - y, x**2 + xy - y**2]\n"], "sample_945": ["def test_warn_missing_reference_explicit_type(app, status, warning):\n    app.build()\n    assert 'index.rst:7: WARNING: undefined label: no-label' in warning.getvalue()\n", "def test_python_python_use_unqualified_type_names_with_import(app, status, warning):\n    app.build()\n    content = (app.outdir / 'index.html').read_text()\n    assert ('<span class=\"n\"><a class=\"reference internal\" href=\"#module-foo.Name\" title=\"module-foo.Name\">'\n            '<span class=\"pre\">Name</span></a></span>' in content)\n    assert ('<span class=\"n\"><a class=\"reference internal\" href=\"#module-foo.Age\" title=\"module-foo.Age\">'\n            '<span class=\"pre\">Age</span></a></span>' in content)   \n\n\n\n", "def test_warn_missing_reference_in_html(app, status, warning):\n    app.build()\n    assert 'index.html:6: WARNING: undefined label: no-label' in warning.getvalue()\n    assert ('index.html:6: WARNING: Failed to create a cross reference. A title or caption not found: existing-label'\n            in warning.getvalue())\n", "def test_type_display(app, status, warning):\n    app.build()\n    content = (app.outdir / 'index.html').read_text()\n    assert ('<span class=\"xref py:type\">List[str]</span>' in content)\n    assert ('<span class=\"xref py:type\">Union[int, str]</span>' in content)\n", "compilation error"], "sample_314": ["    def test_password_change_validation_errors(self):\n        user = User.objects.get(username='testclient')\n        data = {\n            'password1': 'password1',\n            'password2': 'password2',\n        }\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        self.assertTrue(\n            form.errors,\n            [\n                {\n                    'password1': [\n                        \"The password you entered doesn't meet the complexity requirements.\",\n                    ],\n                    'password2': [\n                        \"The password you entered doesn't meet the complexity requirements.\",\n                    ],\n                }\n            ],\n        )\n        # Mock the password_validation.validate_password to raise a ValidationException\n        with mock.patch('django.contrib.auth.password_validation.validate_password') as validate_password:\n            validate_password.side_effect = ValueError(\"Bad password\")\n            form = AdminPasswordChangeForm(user, data)\n            self.assertFalse(form.is_valid())\n            self.assertTrue(\n                form.errors,\n                [\n                    {\n                        'password1': ['Invalid password'],\n                        'password2': ['Invalid password'],\n                    }\n                ],\n            )\n\n\n\n", "    def test_password_validation_fails(self, validate_password):\n        user = User.objects.get(username='testclient')\n        data = {'password1': 'short', 'password2': 'short'}\n        form = AdminPasswordChangeForm(user, data)\n        validate_password.side_effect = ValueError('Password too short')\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['password1'], ['Password must be at least 8 characters long.'])\n        self.assertEqual(form.errors['password2'], ['Password must be at least 8 characters long.'])\n\n\n\n", "    def test_password_too_short(self):\n        user = User.objects.get(username='testclient')\n        data = {'password1': 'short', 'password2': 'short'}\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['password1'], [form.error_messages['password_too_short']])\n        self.assertEqual(form.errors['password2'], [form.error_messages['password_too_short']])\n", "    def test_invalid_password_strength(self):\n        user = User.objects.get(username='testclient')\n        data = {'password1': 'password', 'password2': 'password'}\n        form = AdminPasswordChangeForm(user, data)\n        with self.assertRaisesMessage(ValidationError, 'Password is too short. It must be at least 8 characters long.'):\n            form.clean() \n\n\n", "    def test_password_changed_error_message(self):\n        user = User.objects.get(username='testclient')\n        data = {\n            'password1': 'test123',\n            'password2': 'test123',\n        }\n        form = AdminPasswordChangeForm(user, data)\n        form.save(commit=False)\n        with mock.patch('django.contrib.auth.password_validation.validate_password') as validate_password:\n            validate_password.side_effect = ValidationError(\"Password is too short.\")\n            with self.assertRaises(ValidationError):\n                form.save()\n        self.assertEqual(form.errors['password1'], [\"Password is too short.\"])\n\n"], "sample_1129": ["def test_log_gamma():\n    from sympy import loggamma\n\n    expr = loggamma(x)\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr) == 'scipy.special.loggamma(x)'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr) == '  # Not supported in Python with NumPy:\\n  # loggamma\\nloggamma(x)'\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr) == '  # Not supported in Python:\\n  # loggamma\\nloggamma(x)'\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(expr) == 'mpmath.loggamma(x)'\n\n\n\n", "def test_MatrixBase():\n    from sympy import MatrixSymbol, Matrix\n\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    C = Matrix(2, 2, [[1, 2], [3, 4]])\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(A) == 'numpy.array([[0, 0], [0, 0]])'\n    assert prntr.doprint(B) == 'numpy.array([[0, 0], [0, 0]])'\n    assert prntr.doprint(C) == 'numpy.array([[1, 2], [3, 4]])'\n", "def test_sympy_special_functions():\n    from sympy import gamma, digamma, loggamma, risingfactorials\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(gamma(x)) == 'scipy.special.gamma(x)'\n    assert prntr.doprint(digamma(x)) == 'scipy.special.digamma(x)'\n    assert prntr.doprint(loggamma(x)) == 'scipy.special.loggamma(x)'\n    assert prntr.doprint(risingfactorials(x, y)) == 'scipy.special.poch(x, y)'\n    \n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(gamma(x)) == 'math.gamma(x)'\n    assert prntr.doprint(digamma(x)) == '  # Not supported in Python with NumPy:\\n  # digamma\\ndigamma(x)'\n    assert prntr.doprint(loggamma(x)) == 'math.log(math.gamma(x))'\n    assert prntr.doprint(risingfactorials(x, y)) == '  # Not supported in Python with NumPy:\\n  # risingfactorials\\nrisingfactorials(x, y)'\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(gamma(x)) == 'math.gamma(x)'\n    assert prntr.doprint(digamma(x)) == '  # Not supported in Python:\\n  # digamma\\ndigamma(x)'\n    assert prntr.doprint(loggamma(x)) == 'math.log(math.gamma(x))'\n    assert prntr.doprint(risingfactorials(x, y)) == '  # Not supported in Python:\\n  # risingfactorials\\nrisingfactorials(x, y)'\n", "def test_MatrixBase():\n    from sympy.matrices import MatrixSymbol, Matrix\n    from sympy.testing.pytest import raises\n\n    p = NumPyPrinter()\n\n    A = MatrixSymbol('A', 2, 2)\n    assert p.doprint(A) == \"numpy.array([[0, 0], [0, 0]])\"\n    assert p.doprint(Matrix(\"1 2; 3 4\")) == \"numpy.array([[1, 2], [3, 4]])\"\n\n\n    raises(NotImplementedError, lambda: p.doprint(sparse_matrix))  \n", "def test_complex_exp():\n    from sympy import exp, I\n    x = symbols('x')\n    expr = exp(I*x)\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr) == 'numpy.exp(1j * x)'\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr) == 'numpy.exp(1j * x)'\n\n"], "sample_381": ["    def test_operation_with_no_suggested_name_multiple(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel('Person', fields=[]),\n                migrations.RunSQL('SELECT 1 FROM person;'),\n                migrations.RunSQL('SELECT 2 FROM person;'),\n            ]\n\n        migration = Migration('some_migration', 'test_app')\n        suggest_name = migration.suggest_name()\n        self.assertIs(suggest_name.startswith('auto_'), True)\n", "    def test_suggest_name_for_model_rename(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.RenameModel('Person', 'User'),\n            ]\n\n        migration = Migration('0001_initial', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'rename_person_to_user')\n", "    def test_name_collision_with_squashed_migration(self):\n        history = [\n            migrations.Migration('0001_initial', 'app'),\n            migrations.Migration('0002_squashed_0003', 'app'),\n            migrations.Migration('0004_second', 'app'),\n        ]\n        autodetector = MigrationAutodetector.from_migrations_history(\n            history,\n            None,\n            None,\n        )\n        migration = autodetector._suggest_name(\n            operations=[migrations.CreateModel('Person', fields=[])],\n            app_label='app',\n        )\n        self.assertEqual(migration, 'person_squashed_0003')\n\n\n\n", "    def test_initial_with_custom_name(self):\n        class Migration(migrations.Migration):\n            initial = True\n            operations = [migrations.CreateModel('Person', fields=[])]\n            name = 'custom_initial'\n\n        migration = Migration('0001_custom_initial', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'custom_initial')\n", "    def test_suggest_name_with_no_operations(self):\n        class Migration(migrations.Migration):\n            operations = []\n\n        migration = Migration('0001_initial', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'initial')\n"], "sample_936": ["def test_stringify_type_hints_forward_refs():\n    from typing import ForwardRef\n\n    MyRef = ForwardRef('MyClass')\n\n    assert stringify(MyRef) == 'MyClass'\n", "    def test_stringify_typing_generic(annotation):\n        assert stringify(annotation) == annotation.__name__\n", "compilation error", "def test_stringify_nested_type_hints():\n    T = TypeVar('T')\n    NestedType = List[Dict[str, T]]\n\n    assert stringify(NestedType) == \"List[Dict[str, T]]\"\n", "def test_stringify_GenericMeta():\n    from typing import Generic\n\n    class MyGeneric(Generic[T]):\n        pass\n\n    assert stringify(MyGeneric) == \"test_util_typing.MyGeneric[T]\"\n"], "sample_1076": ["def test_sympy_printer_codegeneration_ast_nodes():\n    from sympy.codegen import CodegenArrayDiagonal\n    prntr = SymPyPrinter()\n    expr = CodegenArrayDiagonal(MatrixSymbol('A', 2, 2), [1, 0], axis1=0, axis2=1)\n    assert prntr.doprint(expr) == \"sympy.transpose(A, (0, 1))\" \n", "def test_sympy_printing_for_indexed_symbols():\n    x = IndexedBase('x')\n    expr = x[0,1] + x[1,2]\n    assert SymPyPrinter().doprint(expr) == 'x[0, 1] + x[1, 2]'\n", "def test_sympy_printing_issue_18645():\n    from sympy import MatrixSymbol\n\n    x = MatrixSymbol('x', 2, 2)\n    y = MatrixSymbol('y', 2, 2)\n\n    prntr = SymPyPrinter()\n    assert prntr.doprint(x + y) == 'x + y'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(x + y) == 'x + y'\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(x + y) == 'x + y'\n    \n    prntr = SciPyPrinter()\n    assert prntr.doprint(x + y) == 'x + y'\n", "def test_sympy_printing():\n    prntr = SymPyPrinter()\n    expr = (x**2 + y**2).sqrt()\n    assert prntr.doprint(expr) == 'sympy.sqrt(x**2 + y**2)'\n    expr = acos(x)\n    assert prntr.doprint(expr) == 'sympy.acos(x)'\n    expr = sign(x)\n    assert prntr.doprint(expr) == '(0.0 if x == 0 else sympy.sign(x))'\n", "def test_sympy_builtin_functions():\n    from sympy import sin, cos, tan, log, exp, atan, acos, arcsin, arccos\n\n    for func in [sin, cos, tan, log, exp, atan, acos, arcsin, arccos]:\n        expr = func(x)\n        prntr = SymPyPrinter()\n        assert prntr.doprint(expr) == f'sympy.{func.__name__}(x)'\n\n    prntr = PythonCodePrinter()\n    for func in [sin, cos, tan, log, exp, atan, acos, arcsin, arccos]:\n        expr = func(x)\n        assert prntr.doprint(expr) == f'{func.__name__}(x)'\n\n    prntr = NumPyPrinter()\n    for func in [sin, cos, tan, log, exp, atan, acos, arcsin, arccos]:\n        expr = func(x)\n        assert prntr.doprint(expr) == f'numpy.{func.__name__}(x)'\n\n\n    prntr = MpmathPrinter()\n    for func in [sin, cos, tan, log, exp, atan, acos, arcsin, arccos]:\n        expr = func(x)\n        assert prntr.doprint(expr) == f'mpmath.{func.__name__}(x)'\n    \n"], "sample_119": ["    def test_complex_conditions_with_f_expressions(self):\n        query = Query(Item)\n        where = query.build_where(\n            Q(\n                id__gt=F('creator__num')\n            ) | Q(\n                id__lt=F('creator__num')\n            )\n        )\n        children = where.children\n        self.assertEqual(len(children), 2)\n        self.assertIsInstance(children[0].children[0], GreaterThan)\n        self.assertIsInstance(children[1].children[0], LessThan)\n\n\n\n", "    def test_set_annotation(self):\n        query = Query(Author)\n\n        query.set_annotation('total_likes', Author.likes.count())\n        self.assertEqual(query.annotations, {'total_likes': Author.likes.count()})\n        self.assertTrue(query._annotation_select_cache is None)\n\n        query.set_annotation('total_likes', 0)\n        self.assertEqual(query.annotations, {'total_likes': 0})\n\n        query.set_annotation_mask(['total_likes'])\n        self.assertEqual(query.annotation_select, {'total_likes': 0})\n        self.assertTrue(query._annotation_select_cache is not None)\n\n\n\n", "    def test_deferred_loading(self):\n        query = Query(Item)\n        query.add_deferred_loading(['id', 'created'])\n        self.assertEqual(query.deferred_loading, (frozenset(['id', 'created']), True))\n        query.add_fields(['name'])\n        self.assertEqual(query.deferred_loading, (frozenset(), False))\n        query.add_deferred_loading(['slug'])\n        self.assertEqual(query.deferred_loading, (frozenset(['slug']), True))\n", "    def test_join_promotion(self):\n        query = Query(Item)\n        query.add_filter(Q(creator__num__gt=2) & Q(creator__name__icontains='John'))\n        query.add_select_related(['creator'])\n\n        # Create a mock JoinPromoter object\n        votes = {'Item_creator': 2}\n        promoter = JoinPromoter(connector=AND, num_children=2, negated=False)\n        promoter.add_votes(votes)\n\n        query._join_promoter = promoter\n        promoter.update_join_types(query)\n\n        # Assert that the creator join is promoted to INNER\n        self.assertEqual(query.alias_map['creator'].join_type, 'INNER')\n\n", "    def test_ordering_with_join_promoted_to_left_outer(self):\n        query = Query(Author)\n        query.add_join(\n            'books',\n            'author_id',\n            models.Book,\n            'id',\n            'LEFT OUTER',\n            join_type='LEFT OUTER',\n        )\n\n        query.add_ordering('name')\n        query.promote_joins(set(['books']))\n        where = query.build_where(Q(num__gt=2))\n        lookup = where.children[0]\n        self.assertIsInstance(lookup, GreaterThan)\n        self.assertEqual(lookup.rhs, 2)\n        self.assertEqual(lookup.lhs.target, Author._meta.get_field('num'))\n    \n"], "sample_627": ["def test_concat_mismatched_dims() -> None:\n    with pytest.raises(ValueError, match=r\"Concatenation along dimension 'x'\"\n                         r\" requires dimensions 'x' to have the same length in all datasets\"):\n        ds1 = Dataset(data={\"a\": [1, 2]}, coords={\"x\": [0, 1]})\n        ds2 = Dataset(data={\"a\": [3, 4, 5]}, coords={\"x\": [0, 1, 2]})\n        concat([ds1, ds2], dim=\"x\")\n", "    def test_concat_with_single_index_no_other_coords(self) -> None:\n        ds1 = Dataset(data=({\"foo\": [1, 2]},), coords={\"x\": [1, 2]})\n        ds2 = Dataset(data=({\"foo\": [3, 4]},), coords={\"x\": [3, 4]})\n\n        concat(ds1, ds2, dim=\"x\")\n", "def test_concat_overwrites_attrs_along_dim(self) -> None:\n    ds1 = Dataset(\n        {\"a\": ({\"x\": [0, 1], \"y\": [0, 1]}, np.array([[1, 2], [3, 4]])),\n         \"b\": ({\"x\": [0, 1], \"y\": [0, 1]}, np.array([[5, 6], [7, 8]])),\n         \"c\": ({\"x\": [0, 1], \"y\": [0, 1]}, np.array([[9, 10], [11, 12]])),\n    }\n    )\n    ds2 = Dataset(\n        {\"a\": ({\"x\": [2, 3], \"y\": [2, 3]}, np.array([[13, 14], [15, 16]])),\n         \"b\": ({\"x\": [2, 3], \"y\": [2, 3]}, np.array([[17, 18], [19, 20]])),\n         \"c\": ({\"x\": [2, 3], \"y\": [2, 3]}, np.array([[21, 22], [23, 24]])),\n    )\n\n    concat_ds = concat([ds1, ds2], dim=\"x\", combine_attrs=\"override\")\n    assert_identical(concat_ds.attrs, ds1.attrs)\n\n", "def test_concat_dim_mismatch_indexes() -> None:\n    ds1 = Dataset(coords={\"x\": (\"t\", [0, 1])})\n    ds2 = Dataset(coords={\"x\": (\"x\", [2, 3])})\n\n    with pytest.raises(\n        ValueError,\n        match=r\"Cannot concatenate along dimension 'x' because the indexes have different shapes.*\",\n    ):\n        concat([ds1, ds2], dim=\"x\")\n", "compilation error"], "sample_969": ["def test_stringify_type_hints_overload():\n    from typing import overload  # type: ignore\n\n    @overload\n        ...\n\n    @overload\n        ...\n\n    assert stringify(my_func, False) == \"my_func\"\n    assert stringify(my_func, True) == \"my_func\"\n", "def test_stringify_type_hints_Annotated_with_callable():\n    from typing import Annotated, Callable  # type: ignore\n    assert stringify(Annotated[str, Callable[[int], int]], False) == \"Annotated[str, Callable[[int], int]]\"\n    assert stringify(Annotated[str, Callable[[int], int]], True) == \"~Annotated[str, ~typing.Callable[[int], int]]\"\n", "def test_stringify_forward_refs():\n    from typing import ForwardRef  # type: ignore\n    assert stringify(ForwardRef(\"myint\"), False) == \"myint\"\n    assert stringify(ForwardRef(\"myint\"), True) == \"myint\"\n", "def test_stringify_type_hints_typing_TypedDict():\n    from typing import TypedDict  # type: ignore\n    class MyClass(TypedDict):\n        a: int\n        b: str\n\n    assert stringify(MyClass, False) == \"MyClass\"\n    assert stringify(MyClass, True) == \"~MyClass\"\n\n\n\n", "def test_stringify_type_hints_Annotated_with_args():\n    from typing import Annotated  # type: ignore\n    assert stringify(Annotated[str, \"foo\", \"bar\"], False) == \"Annotated[str, 'foo', 'bar']\"\n    assert stringify(Annotated[str, \"foo\", \"bar\"], True) == \"~typing.Annotated[str, 'foo', 'bar']\"\n\n    assert stringify(Annotated[int, 10], False) == \"Annotated[int, 10]\"\n    assert stringify(Annotated[int, 10], True) == \"~typing.Annotated[int, 10]\"\n\n    assert stringify(Annotated[str, \"foo\", \"bar\", 123], False) == \"Annotated[str, 'foo', 'bar', 123]\"\n    assert stringify(Annotated[str, \"foo\", \"bar\", 123], True) == \"~typing.Annotated[str, 'foo', 'bar', 123]\"\n"], "sample_978": ["def test_bspline_with_non_integer_knots():\n    d = 2\n    knots = [1.5, 2.0, 3.0, 4.5]\n    splines = bspline_basis_set(d, knots, x)\n    b0 = Piecewise((x**2/2 - 1.5*x + 0.75, Interval(1.5, 2.0).contains(x)),\n                   (Rational(9, 4) - 3*x + x**2/2, Interval(2.0, 3.0).contains(x)),\n                   (0, True))\n    assert splines[0] == b0\n", "def test_bspline_degree_3_discontinuities():\n    d = 3\n    knots = [0, 0, 0, 1, 2, 3, 4, 4, 4]\n    splines = bspline_basis_set(d, knots, x)\n\n    b0 = Piecewise(\n        (x**3/6, Interval(0, 1).contains(x)),\n        (Rational(5, 6) - x**3/2 + x**2, Interval(1, 2).contains(x)),\n        (Rational(11, 6) - 3*x**2 + 2*x**3/3, Interval(2, 3).contains(x)),\n        (Rational(11, 6) - 3*x**2 + 2*x**3/3, Interval(3, 4).contains(x)),\n        (0, True)\n    )\n    assert splines[0] == b0 \n", "def test_bsplines_with_rational_arguments():\n    d = 2\n    knots = [Rational(1, 2), Rational(1, 2), 2, 3, 4]\n    x = symbols('x')\n    splines = bspline_basis_set(d, knots, x)\n    # test for cases with rational arguments\n    for spline in splines:\n        assert isinstance(spline, Piecewise) \n        for cond in spline.args:\n            assert isinstance(cond[0], Interval) or isinstance(cond[0], And)\n            if isinstance(cond[0], Interval):\n                assert isinstance(cond[1], Rational) or isinstance(cond[1], int)\n            if isinstance(cond[0], And):\n                for expr in cond[0].args:\n                    assert isinstance(expr, Rational) or isinstance(expr, int)\n", "def test_knot_handling():\n    d = 2\n    knots = [0, 1, 2, 3, 4]\n    splines = bspline_basis_set(d, knots, x)\n    assert splines[0] == Piecewise((x**2/2, Interval(0, 1).contains(x)),\n                (Rational(-3, 2) + 3*x - x**2, Interval(1, 2).contains(x)),\n                (Rational(9, 2) - 3*x + x**2/2, Interval(2, 3).contains(x)),\n                (0, True))\n\n\n\n", "def test_repeated_knots_degree_3():\n    d = 3\n    knots = [0, 0, 1, 2, 2, 2, 3, 3, 4]\n    splines = bspline_basis_set(d, knots, x)\n    b0 = Piecewise((x**3/6, Interval(0, 1).contains(x)),\n                   (Rational(-1, 2) + x**3/2 - 2*x**2, Interval(1, 2).contains(x)),\n                   (Rational(-11, 2) + x**3/2 - 4*x**2 + 3*x, Interval(2, 3).contains(x)),\n                   (Rational(11, 6) - x**2/2 + 2*x, Interval(3, 4).contains(x)),\n                   (0, True)\n                   )\n    assert splines[0] == b0\n"], "sample_313": ["    def test_get_template_directories_with_disabled_app_directories(self):\n        self.assertSetEqual(\n            autoreload.get_template_directories(),\n            {\n                ROOT / 'templates_extra',\n            }\n        )\n", "    def test_get_template_directories_prioritized_dirs(self):\n        self.assertSetEqual(\n            autoreload.get_template_directories(),\n            {\n                ROOT / 'templates',\n                ROOT / 'templates_extra',\n            }\n        )\n", "    def test_custom_loaders_respected(self):\n        self.assertSetEqual(\n            autoreload.get_template_directories(),\n            {\n                Path.cwd() / 'template_tests/relative_str',\n                Path.cwd() / 'template_tests/relative_path',\n            }\n        )\n", "    def test_app_dirs_and_extra_dirs(self):\n        self.assertSetEqual(\n            autoreload.get_template_directories(),\n            {\n                ROOT / 'templates',\n                ROOT / 'templates_extra',\n            }\n        )\n", "    def test_get_template_directories_with_app_dirs(self):\n        self.assertSetEqual(\n            autoreload.get_template_directories(),\n            {\n                ROOT / 'absolute_str',\n                Path.cwd() / 'template_tests/relative_path',\n                Path.cwd() / 'template_tests/relative_str',\n            }\n        )\n"], "sample_8": ["    def test_masked_array_from_masked_quantity(self):\n        \"\"\"Check that we can initialize a MaskedArray properly from a MaskedQuantity.\"\"\"\n        np_ma = np.ma.MaskedArray(self.mq)\n        assert type(np_ma) is np.ma.MaskedArray\n        assert type(np_ma.data) is self._data_cls\n        assert type(np_ma.mask) is np.ndarray\n        assert_array_equal(np_ma.data, self.a)\n        assert_array_equal(np_ma.mask, self.mask_a)\n", "def test_masked_array_from_masked_quantity():\n    \"\"\"Check that we can initialize a MaskedArray properly.\"\"\"\n    np_ma = np.ma.MaskedArray(self.mq)\n    assert type(np_ma) is np.ma.MaskedArray\n    assert type(np_ma.data) is self._data_cls\n    assert type(np_ma.mask) is np.ndarray\n    assert_array_equal(np_ma.data, self.mq.value)\n    assert_array_equal(np_ma.mask, self.mq.mask)\n\n\n", "    def test_masked_array_arithmetic(self):\n        ma1 = self.ma.copy()\n        ma2 = self.mb.copy()\n        # Arithmetic operations\n        for op in ['+', '-', '*', '/']:\n            result = eval(f\"ma1 {op} ma2\")\n            np_result = np.ma.masked_array(self.a, self.mask_a)\n            np_result = eval(f\"np_result {op} np.ma.masked_array(self.b, self.mask_b)\")\n            assert_masked_equal(result, np_result)\n\n\n", "    def test_masked_array_ops(self):\n        # Check that basic arithmetic operations work correctly\n        ma_plus = self.ma + self.mb\n        np_ma_plus = np.ma.MaskedArray(self.ma) + np.ma.MaskedArray(self.mb)\n        assert_masked_equal(ma_plus, np_ma_plus)\n        ma_minus = self.ma - self.mb\n        np_ma_minus = np.ma.MaskedArray(self.ma) - np.ma.MaskedArray(self.mb)\n        assert_masked_equal(ma_minus, np_ma_minus)\n        ma_mul = self.ma * self.mb\n        np_ma_mul = np.ma.MaskedArray(self.ma) * np.ma.MaskedArray(self.mb)\n        assert_masked_equal(ma_mul, np_ma_mul)\n        ma_div = self.ma / self.mb\n        np_ma_div = np.ma.MaskedArray(self.ma) / np.ma.MaskedArray(self.mb)\n        assert_masked_equal(ma_div, np_ma_div)\n", "    def test_masked_array_methods_with_masked_array(self):\n        \"\"\"Check interactions between MaskedArrays and  \n        numpy.ma.MaskedArray.\n        \"\"\"\n        np_ma = np.ma.MaskedArray(self.ma)\n        # Apply some common methods\n        np_prod = np_ma.prod()\n        assert_array_equal(np_prod.data, self.ma.prod().unmasked)\n        assert_array_equal(np_prod.mask, self.ma.prod().mask)\n        np_mean = np_ma.mean()\n        assert_array_equal(np_mean.data, self.ma.mean().unmasked)\n        assert_array_equal(np_mean.mask, self.ma.mean().mask)\n\n\n\n"], "sample_695": ["def test_collector_gethockproxy(pytester: Pytester) -> None:\n    p = pytester.makepyfile(\n        \"\"\"\n            pass\n    \"\"\"\n    )\n    session = pytester.parse(p)\n    collector = session.items[0].parent\n    assert collector.gethookproxy(legacypath(p.dir / \"test_one.py\")) is not None\n", "def test_node_getparent_returns_correct_parent(pytester: Pytester) -> None:\n    p = pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        class MyCollector(nodes.Collector):\n            pass\n\n        @pytest.mark.parametrize(\"parent_type\", [MyCollector, nodes.Collector])\n            yield parent_type, parent \n\n        @pytest.mark.parametrize(\"parent_type\", [MyCollector, nodes.Collector])\n            yield parent_type, parent\n\n    \"\"\"\n    )\n    items = pytester.getitems()\n\n    for parent_type, parent in [\n        (MyCollector, items[0].parent),\n        (MyCollector, items[1].parent),\n        (nodes.Collector, items[0].parent),\n        (nodes.Collector, items[1].parent),\n    ]:\n        assert items[0].getparent(parent_type) is parent\n\n\n\n", "def test_node_add_report_section() -> None:\n    item = nodes.Item(name=\"test\")\n    item.add_report_section(\"setup\", \"stdout\", \"setup output\\n\")\n    item.add_report_section(\"call\", \"stderr\", \"error\\n\")\n    assert item._report_sections == [(\"setup\", \"stdout\", \"setup output\\n\"), (\"call\", \"stderr\", \"error\\n\")]\n", "def test_collector_gethookproxy_initial_paths(pytester: Pytester) -> None:\n    # Make sure it respects initialpaths\n    p = pytester.makepyfile(\n        \"\"\"\n            pass\n        \"\"\"\n    )\n    session = pytester.get_session()\n    session._initialpaths = {\"/my/initial/path\"}  # Set initial paths\n    item = pytester.getitems(p)[0]\n    assert item.gethookproxy(\"/my/path/to/test.py\") == item.session.gethookproxy(\"/my/path/to/test.py\")\n\n    item = pytester.getitems(p)[0]\n    assert item.gethookproxy(p) == item.session.gethookproxy(p)\n", "    def test_iterparentnodeids_handles_empty_nodeid(pytester: Pytester) -> None:\n        for nodeid in [\"\", ::]:\n            result = list(nodes.iterparentnodeids(nodeid))\n            assert result == []\n\n\n\n"], "sample_907": ["def test_domain_cpp_parse_noindexentry_multiple_roles(app):\n    text = (\".. cpp:function:: void f()\\n\"\n            \".. cpp:function:: void g()\\n\"\n            \"   :noindexentry:\\n\"\n            \".. cpp:member:: int m\\n\"\n            \"   :noindexentry:\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index, desc, addnodes.index, desc))\n    assert_node(doctree[0], addnodes.index, entries=[('single', 'f (C++ function)', '_CPPv41fv', '', None)])\n    assert_node(doctree[2], addnodes.index, entries=[('single', 'g (C++ function)', '_CPPv41gv', '', None)])\n    assert_node(doctree[4], addnodes.index, entries=[])\n    assert_node(doctree[6], addnodes.index, entries=[])\n", "def test_domain_cpp_parse_template_param_default_value(app):\n    text = (\".. cpp:template:: template<typename T=int> void f(T x)\\n\"\n            \"   :param T x: A parameter\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.field, 'param', addnodes.index, desc, addnodes.index, desc))\n    assert_node(doctree[0], addnodes.field, label='param',\n                content=[addnodes.paragraph, 'A parameter'])\n    assert_node(doctree[2], addnodes.index, entries=[('single', 'x (C++ template parameter)', '_CPPv41fT1x', '', None)])\n    assert 'T=int' in doctree[2].parent.parent.parent.astext()\n    \n", "def test_domain_cpp_parse_ref_to_non_existent_target(app, warning):\n    text = (\".. cpp:class:: MyNotExistentClass\\n\")\n    restructuredtext.parse(app, text)\n    ws = warning.getvalue().split(\"\\n\")\n    assert len(ws) == 1\n    assert \"WARNING: cpp: undefined target 'MyNotExistentClass'\" in ws[0]\n\n", "def test_domain_cpp_parse_reference_with_parentheses(app, warning):\n    text = (\".. cpp:function:: void f()\\n\"\n            \".. cpp:function:: (void) f()\\n\")\n    restructuredtext.parse(app, text)\n    ws = warning.getvalue().split(\"\\n\")\n    assert len(ws) == 1\n    assert \"index.rst:2: WARNING: Unexpected parentheses around function reference\" in ws[0]\n", "def test_domain_cpp_parse_template_param_with_parentheses(app, warning):\n    text = \".. cpp:function:: template<typename T(int)> void f(T t) \"\n    text += \"\\n.. cpp:function:: template<typename T(int)> void g()\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index, desc, addnodes.index, desc))\n    assert_node(doctree[0], addnodes.index, entries=[('single', 'f (C++ function)', '_CPPv4I0E8_functionv', '', None)])\n    assert_node(doctree[2], addnodes.index, entries=[('single', 'g (C++ function)', '_CPPv4I0E8_functionv', '', None)])\n\n\n"], "sample_682": ["def test_importorskip_failure(testdir):\n    with pytest.raises(pytest.skip.Exception, match=\"Module doesnotexist missing\"):\n        pytest.importorskip(\"doesnotexist\")\n", "            def test_with_param(param):\n                assert param == param\n                pytest.skip(f\"Skipping for param {param}\")", "def test_importorskip_existing():\n    pytest.importorskip(\"unittest\")\n\n\n", "def test_xfail_with_assertion_error(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail\n            assert 0\n        \"\"\"\n    )\n    result = testdir.runpytest(\"-r\")\n    result.stdout.fnmatch_lines(\n        [\n            \"XFAIL test_xfail_with_assertion_error.py::test_func*1 xfailed*\"\n        ]\n    )\n", "def test_importorskip_module_relative(testdir):\n    testdir.mkdir(\"subdir\")\n    testdir.write(\"subdir/test_mod.py\", \"\"\"\n            pass\n    \"\"\")\n    testdir.write(\"test_importorskip.py\", \"\"\"\n        import pytest\n        from subdir import test_mod\n\n        @pytest.mark.skip\n            test_mod.test_something()\n    \"\"\")\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*SKIP*test_skip_import*\"])\n"], "sample_1014": ["def test_tomatrix_with_symbolic_indices():\n    from sympy.abc import x, y\n    md = ImmutableDenseNDimArray([[x, y], [x + 1, y + 1]], (2, 2))\n    matrix = md.tomatrix()\n    assert isinstance(matrix, Matrix)\n    assert matrix.shape == md.shape\n    assert matrix[0, 0] == x\n    assert matrix[0, 1] == y\n    assert matrix[1, 0] == x + 1\n    assert matrix[1, 1] == y + 1\n\n    sd = ImmutableSparseNDimArray([[x, y], [x + 1, y + 1]], (2, 2))\n    sparse_matrix = sd.tomatrix()\n    assert isinstance(sparse_matrix, SparseMatrix)\n    assert sparse_matrix.shape == sd.shape\n    assert sparse_matrix[0, 0] == x\n    assert sparse_matrix[0, 1] == y\n    assert sparse_matrix[1, 0] == x + 1\n\n    x_subs = {x: 2}\n    matrix_subs = matrix.subs(x_subs)\n    assert matrix_subs.shape == md.shape\n    assert matrix_subs[0, 0] == 2\n    assert matrix_subs[0, 1] == y\n\n\n", "def test_reshape_on_mutable_array():\n    a = MutableDenseNDimArray([1, 2, 3, 4, 5, 6], shape=(2, 3))\n    assert a.shape == (2, 3)\n    \n    a.reshape(3, 2)\n    assert a.shape == (3, 2)\n    \n    a.reshape(2, 3)\n    assert a.shape == (2, 3)\n\n", "def test_reshape_with_symbolic_indices():\n    x = Symbol('x')\n    arr = ImmutableDenseNDimArray([x, x*2, x*3])\n    reshaped_arr = arr.reshape((3, 1))\n    assert reshaped_arr.shape == (3, 1)\n    assert reshaped_arr[0, 0] == x\n    assert reshaped_arr[1, 0] == x*2\n    assert reshaped_arr[2, 0] == x*3\n\n    reshaped_arr = arr.reshape((1, 3))\n    assert reshaped_arr.shape == (1, 3)\n    assert reshaped_arr[0, 0] == x\n    assert reshaped_arr[0, 1] == x*2\n    assert reshaped_arr[0, 2] == x*3\n\n    with raises(ValueError):\n        arr.reshape((2, 2))\n\n", "def test_reshape_with_symbolic_indices():\n    x, y, z = symbols(\"x y z\")\n    M = ImmutableDenseNDimArray([[x, y], [z, x + y]])\n    new_shape = (2, 2)\n    reshaped_M = M.reshape(new_shape)\n    assert reshaped_M.shape == new_shape\n    assert reshaped_M[0, 0] == x\n    assert reshaped_M[0, 1] == y\n    assert reshaped_M[1, 0] == z\n    assert reshaped_M[1, 1] == x + y\n\n\n", "compilation error"], "sample_504": ["def test_colorbar_gridspec_placement():\n    fig = plt.figure()\n    gs = fig.add_gridspec(2, 2)\n    ax = fig.add_subplot(gs[0, 0])\n    pc = ax.pcolormesh(np.arange(90).reshape(9, 10))\n    cbar_ax = fig.add_subplot(gs[0, 1])\n    cb = fig.colorbar(pc, cax=cbar_ax)\n    assert cbar_ax.is_valid()\n", "def test_colorbar_fraction():\n    fig, ax = plt.subplots()\n    pc = ax.pcolormesh(np.arange(100).reshape(10, 10))\n    cb = fig.colorbar(pc, ax=ax, fraction=0.2)\n    assert cb.ax.get_position().height == 0.2, \\\n        f\"colorbar height is not as expected: {cb.ax.get_position()}\"\n    np.testing.assert_array_equal(cb.ax.get_position().bounds[1:],\n        np.array([0.9, 0.1, 0.0237, 0.2]))\n    \n\n\n\n", "def test_colorbar_with_empty_data():\n    fig, ax = plt.subplots()\n    with pytest.raises(ValueError):\n        fig.colorbar(plt.cm.ScalarMappable(norm=plt.Normalize(0, 1)))\n\n\n", "compilation error", "def test_colorbar_ax_kwargs():\n    fig, ax = plt.subplots()\n    im = ax.imshow([[1, 2, 3], [4, 5, 6]])\n    cb = fig.colorbar(im, ax=ax, label='example')\n\n    # various keyword arguments should be passed to the axes\n    cb.ax.set_title('Title')\n    assert cb.ax.get_title() == 'Title'\n\n    cb.ax.set_xlabel('Xlabel')\n    assert cb.ax.get_xlabel() == 'Xlabel'\n\n    cb.ax.set_ylabel('Ylabel')\n    assert cb.ax.get_ylabel() == 'Ylabel'\n\n    cb.ax.set_facecolor('green')\n    assert cb.ax.get_facecolor() == 'green'\n\n    cb.ax.set_edgecolor('red')\n    assert cb.ax.get_edgecolor() == 'red'\n\n    cb.ax.tick_params(axis='both', direction='out')\n    assert cb.ax.yaxis.get_tickdir() == 'out'\n    assert cb.ax.xaxis.get_tickdir() == 'out'\n\n\n\n"], "sample_759": ["def test_one_hot_encoder_get_feature_names_out_of_order():\n    enc = OneHotEncoder(handle_unknown='ignore')\n    X = np.array([['a', 'b'], ['c', 'd']], dtype=object).T\n    enc.fit(X)\n    feature_names = enc.get_feature_names()\n    assert_array_equal(feature_names, ['x0_a', 'x0_b', 'x1_c', 'x1_d'])\n\n\n    \n", "def test_one_hot_encoder_sparse_output_consistency():\n    X = [['abc', 2, 55], ['def', 1, 55]]\n    enc = OneHotEncoder(sparse=True)\n    X_tr_sparse = enc.fit_transform(X)\n    X_tr_dense = enc.fit_transform(X).toarray()\n\n    assert_array_equal(X_tr_sparse.toarray(), X_tr_dense)\n\n\n\n", "def test_one_hot_encoder_with_n_values():\n    X = np.array([['abc', 2, 55], ['def', 1, 55]])\n    enc = OneHotEncoder(n_values='auto', handle_unknown='ignore')\n    X_tr = enc.fit_transform(X)\n    assert X_tr.shape[1] == 3  # One hot for each categorical column\n\n    enc = OneHotEncoder(n_values=[3, 2, 1])\n    with pytest.raises(ValueError, match=\"Number of categories should be\"):\n        enc.fit(X)\n", "compilation error", "def test_one_hot_encoder_sparse_output():\n    X = np.array([[1, 2], [3, 4]])\n    enc = OneHotEncoder(sparse=True)\n    X_tr = enc.fit_transform(X)\n    assert isinstance(X_tr, sparse.csr_matrix)\n    assert X_tr.shape == (2, 4)\n    assert_array_equal(X_tr.toarray(), np.array([[1., 0., 1., 0.],\n                                               [0., 1., 0., 1.]])\n\n\n"], "sample_581": ["compilation error", "def test_blueprint_inheritance(app, client) -> None:\n    parent = flask.Blueprint(\"parent\", __name__)\n    @parent.route(\"/\")\n        return \"Parent Index\"\n\n    child = flask.Blueprint(\"child\", __name__, parent_blueprint=parent)\n\n    @child.route(\"/\")\n        return \"Child Index\"\n\n    app.register_blueprint(child)\n\n    response = client.get(\"/child/\")\n    assert response.data == b\"Child Index\"\n\n\n", "def test_blueprint_url_pattern_override(app, client) -> None:\n    bp = flask.Blueprint(\"bp\", __name__)\n\n    @bp.route(\"/test/<path:path>\")\n        return f\"Test: {path}\"\n\n    app.register_blueprint(bp, url_prefix=\"/override\")\n\n    assert client.get(\"/override/test/hello\").data == b\"Test: hello\"\n    assert client.get(\"/override/test/world/\").data == b\"Test: world/\"\n\n\n\n", "compilation error", "    def test_view():\n        return \"Hello\"\n"], "sample_596": ["def test_concat_variable_coords(self):\n    x = np.arange(0, 5)\n    y = np.arange(0, 10)\n    time = np.arange(0, 4)\n    data = np.zeros((4, 10, 5), dtype=bool)\n\n    ds1 = Dataset(\n        {\"data\": ([\"time\", \"y\", \"x\"], data[0:2]), \"x_var\": ([\"x\"], x)},\n        coords={\"time\": time[0:2], \"y\": y, \"x\": x},\n    )\n    ds2 = Dataset(\n        {\"data\": ([\"time\", \"y\", \"x\"], data[2:4]), \"x_var\": ([\"x\"], x)},\n        coords={\"time\": time[2:4], \"y\": y, \"x\": x},\n    )\n\n    expected = Dataset(\n        {\"data\": ([\"time\", \"y\", \"x\"], data), \"x_var\": ([\"x\"], x)},\n        coords={\"time\": time, \"y\": y, \"x\": x},\n    )\n\n    actual = concat([ds1, ds2], dim=\"time\", data_vars=[\"data\", \"x_var\"])\n\n    assert_identical(actual, expected)\n\n", "    def test_concat_compat_exact(self):\n        da1 = DataArray([1, 2, 3], dims=\"x\", coords={\"x\": [1, 2, 3]})\n        da2 = DataArray([4, 5, 6], dims=\"x\", coords={\"x\": [4, 5, 6]})\n\n        with raises_regex(\n            ValueError,\n            r\"cannot concatenate with compat='exact'\"\n        ):\n            concat([da1, da2], dim=\"x\", compat=\"exact\")\n", "def test_concat_same_dims_different_sizes(self):\n    da1 = DataArray([1, 2, 3], dims=\"x\")\n    da2 = DataArray([4, 5], dims=\"x\")\n    with raises_regex(ValueError, \"cannot be concatenated\"):\n        concat([da1, da2], dim=\"x\")\n\n    da1 = DataArray([1, 2, 3, 4], dims=\"x\")\n    da2 = DataArray([5, 6], dims=\"x\")\n    with raises_regex(ValueError, \"cannot be concatenated\"):\n        concat([da1, da2], dim=\"x\")\n\n    da1 = DataArray([1, 2, 3], dims=(\"x\", \"y\"), coords={\"x\": [1, 2, 3], \"y\": [0, 1, 2]})\n    da2 = DataArray([4, 5, 6], dims=(\"x\", \"y\"), coords={\"x\": [4, 5, 6], \"y\": [0, 1, 2]})\n    expected = DataArray(\n        np.array([[1, 2, 3, 4, 5, 6]], dtype=int),\n        dims=(\"x\", \"y\"),\n        coords={\"x\": [1, 2, 3, 4, 5, 6], \"y\": [0, 1, 2]},\n    )\n    actual = concat([da1, da2], dim=\"x\")\n    assert_identical(actual, expected)\n\n\n", "    def test_concat_coordinate_data_vars(self):\n        ds1 = Dataset(\n            {\"data\": ((\"x\", \"y\"), np.random.rand(2, 3)), \"extra\": ((\"x\",), [1.0, 2.0])},\n            coords={\"x\": [0, 1], \"y\": [0, 1, 2]},\n        )\n        ds2 = Dataset(\n            {\"data\": ((\"x\", \"y\"), np.random.rand(2, 3)), \"extra\": ((\"x\",), [3.0, 4.0])},\n            coords={\"x\": [2, 3], \"y\": [0, 1, 2]},\n        )\n\n        actual = concat([ds1, ds2], dim=\"x\")\n        expected = Dataset(\n            {\"data\": ((\"x\", \"y\"), np.concatenate((ds1[\"data\"].values, ds2[\"data\"].values), axis=0)),\n             \"extra\": ((\"x\",), np.concatenate((ds1[\"extra\"].values, ds2[\"extra\"].values), axis=0))},\n            coords={\"x\": [0, 1, 2, 3], \"y\": [0, 1, 2]},\n        )\n        assert_identical(actual, expected)\n\n\n", "    def test_concat_dataset_with_empty_dims(self):\n        ds1 = Dataset({\"a\": ([\"x\"], [1])}, coords={\"x\": [0]})\n        ds2 = Dataset({\"b\": ([\"y\", \"x\"], [2, 3])}, coords={\"x\": [0], \"y\": [0]})\n\n        with raises_regex(ValueError, \"cannot be concatenated\"):\n            concat([ds1, ds2], dim=\"y\")\n\n        with raises_regex(ValueError, \"cannot be concatenated\"):\n            concat([ds1, ds2], dim=\"x\")\n"], "sample_454": ["    def test_with_violation_error_message(self):\n        constraint = models.UniqueConstraint(\n            fields=[\"foo\", \"bar\"],\n            name=\"unique_fields\",\n            violation_error_message=\"BAZ\",\n        )\n        path, args, kwargs = constraint.deconstruct()\n        reconstructed_constraint = models.UniqueConstraint(**kwargs)\n        self.assertEqual(reconstructed_constraint.violation_error_message, \"BAZ\")\n\n", "    def test_validation_order_matters_with_expressions(self):\n        msg = \"Expressions for unique constraints cannot include F() objects.\"\n        with self.assertRaisesMessage(ValueError, msg):\n            models.UniqueConstraint(\n                Lower(\"title\"), F(\"author\"), name=\"unique_lower_author\"\n            )\n\n\n", "    def test_unique_constraint_with_expression_and_condition(self):\n        with self.assertRaises(ValueError):\n            models.UniqueConstraint(\n                Lower(\"name\"),\n                condition=models.Q(color__isnull=True),\n                name=\"name_lower_uniq_condition\",\n            )\n", "    def test_null_with_deferrable(self):\n        with self.assertRaises(ValueError):\n            models.UniqueConstraint(\n                fields=[\"field\"],\n                name=\"null_deferrable_constraint\",\n                deferrable=models.Deferrable.DEFERRED,\n                null=True,\n            )\n", "    def test_deferrable_behaviour_on_delete(self):\n        class ModelWithDeferredConstraint(models.Model):\n            name = models.CharField(max_length=255)\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(\n                        fields=[\"name\"],\n                        deferrable=models.Deferrable.DEFERRED,\n                        name=\"deferred_name_uniq\",\n                    )\n                ]\n\n        obj = ModelWithDeferredConstraint.objects.create(name=\"test\")\n        with self.assertRaises(IntegrityError):\n            obj.delete()\n        # Behavior can be changed with SET CONSTRAINTS.\n        with atomic(), connection.cursor() as cursor:\n            constraint_name = connection.ops.quote_name(\"deferred_name_uniq\")\n            cursor.execute(\"SET CONSTRAINTS %s IMMEDIATE\" % constraint_name)\n            obj.delete()\n\n\n\n"], "sample_113": ["    def test_simplify_regex_with_optional_groups(self):\n        tests = (\n            (r'^a/(?P<b>.*)?$', '/a/<var>'),\n            (r'^a/(?P<b>\\w+)?$', '/a/<var>'),\n        )\n        for pattern, output in tests:\n            with self.subTest(pattern=pattern):\n                self.assertEqual(simplify_regex(pattern), output)\n\n\n\n", "    def test_simplify_regex_group_escaping(self):\n        tests = (\n            (r'^(?P<a>\\w+)\\((?P<b>\\w+)\\)$', '/<a>(<b>)'),\n            (r'^(?P<a>\\w+)\\((?:(?P<b>\\w+))\\)$', '/<a>(?:<b>)'),\n        )\n        for pattern, output in tests:\n            with self.subTest(pattern=pattern):\n                self.assertEqual(simplify_regex(pattern), output)\n\n", "    def test_simplify_regex_no_capture_groups(self):\n        tests = (\n            (r'^a', '/a'),\n            (r'^(?P<a>\\w+)$', '/<a>'),\n            (r'^a/b/c', '/a/b/c'),\n            (r'^a/b/c/?$', '/a/b/c'),\n        )\n        for pattern, output in tests:\n            with self.subTest(pattern=pattern):\n                self.assertEqual(simplify_regex(pattern), output)\n", "    def test_simplify_regex_with_named_groups(self):\n        tests = (\n            (r'^test/(?P<id>\\d+)/$', '/test/<id>/'),\n            (r'^test/(?P<id>\\d+)/(?P<name>.*)$', '/test/<id>/<name>'),\n            (r'^test/(?P<id>\\d+)/(?P<name>.*)/$', '/test/<id>/<name>'),\n        )\n        for pattern, output in tests:\n            with self.subTest(pattern=pattern):\n                self.assertEqual(simplify_regex(pattern), output)\n", "    def test_simplify_regex_extra_groups(self):\n        tests = (\n            (r'^a/(?P<b>123)/(?P<c>456)$', '/a/<b>/<c>'),\n            (r'^a/(?P<b>123)/(?P<c>456)(/|$)', '/a/<b>/<c>/'),\n        )\n        for pattern, output in tests:\n            with self.subTest(pattern=pattern):\n                self.assertEqual(simplify_regex(pattern), output)\n"], "sample_443": ["    def test_key_prefix(self):\n        test_caches = CacheHandler(\n            {\n                \"cache_1\": {\"BACKEND\": \"django.core.cache.backends.dummy.DummyCache\"},\n            }\n        )\n        cache_1 = test_caches[\"cache_1\"]\n\n        # Default key prefix is empty.\n        self.assertEqual(cache_1.key_prefix, \"\")\n\n        # Setting the key prefix should work.\n        test_caches[\"cache_1\"].key_prefix = \"myprefix\"\n        self.assertEqual(cache_1.key_prefix, \"myprefix\")\n\n\n", "    def test_backend_specific_settings(self):\n        test_caches = CacheHandler(\n            {\n                \"cache_1\": {\n                    \"BACKEND\": \"django.core.cache.backends.dummy.DummyCache\",\n                    \"OPTIONS\": {\"key_prefix\": \"prefix1\"},\n                },\n                \"cache_2\": {\n                    \"BACKEND\": \"django.core.cache.backends.dummy.DummyCache\",\n                    \"OPTIONS\": {\"key_prefix\": \"prefix2\"},\n                },\n            }\n        )\n        cache_1 = test_caches[\"cache_1\"]\n        cache_2 = test_caches[\"cache_2\"]\n        self.assertEqual(cache_1.options[\"key_prefix\"], \"prefix1\")\n        self.assertEqual(cache_2.options[\"key_prefix\"], \"prefix2\")\n\n\n\n", "    def test_cache_alias_with_prefix(self):\n        \"\"\"\n        The cache alias should be set to django.core.cache.backends.locmem.LocMemCache\n        (in this case its a dummy cache) when no cache backend is specified in the settings\n        \"\"\"\n        settings = override_settings(\n            CACHES={\n                \"default\": {\n                    \"BACKEND\": \"django.core.cache.backends.locmem.LocMemCache\",\n                    \"KEY_PREFIX\": \"myprefix\",\n                },\n                \"my_alias\": {\n                    \"BACKEND\": \"django.core.cache.backends.dummy.DummyCache\",\n                },\n            },\n        )\n        with settings:\n            caches = CacheHandler(settings.CACHES)\n            default_cache = caches[\"default\"]\n            my_alias_cache = caches[\"my_alias\"]\n\n            self.assertEqual(isinstance(default_cache, LocMemCache), True)\n            self.assertEqual(isinstance(my_alias_cache, DummyCache), True)\n\n\n", "    def test_invalid_cache_backend(self):\n        with self.assertRaises(InvalidCacheBackendError):\n            CacheHandler(\n                {\n                    \"invalid\": {\"BACKEND\": \"not_a_real_backend\"}\n                }\n            )\n\n", "    def test_cache_backend_class(self):\n        \"\"\"\n        Test that cache backend classes are correctly registered.\n        \"\"\"\n        test_caches = CacheHandler(\n            {\n                \"cache_1\": {\n                    \"BACKEND\": \"django.core.cache.backends.dummy.DummyCache\",\n                },\n            }\n        )\n\n        self.assertEqual(\n            test_caches[\"cache_1\"].backend_class, DummyCache\n        )\n\n"], "sample_620": ["    def test_concat_with_data_attrs(self) -> None:\n        ds1 = Dataset({\"a\": ([\"y\"], [1, 2]), \"b\": ([\"y\"], [3, 4])}, coords=[(\"y\", [0, 1])])\n        ds2 = Dataset({\"a\": ([\"y\"], [5, 6]), \"b\": ([\"y\"], [7, 8])}, coords=[(\"y\", [2, 3])])\n        combined = concat([ds1, ds2], dim=\"y\")\n        assert combined.attrs == ds1.attrs  # Should inherit attrs from first dataset\n", "def test_concat_along_existing_dimension_with_different_sizes() -> None:\n    ds1 = Dataset({\"a\": [1, 2, 3]}, {\"b\": [4, 5, 6]})\n    ds2 = Dataset({\"a\": [7, 8], \"b\": [9, 10]})\n    \n    with pytest.raises(ValueError, match=r\"Dimensions 'a' and 'b' must have the same size along the concatenation axis\"):\n        concat([ds1, ds2], dim=\"a\")\n", "def test_concat_non_identical_coord_names() -> None:\n    ds1 = Dataset(\n        {\"data\": ([\"x\", \"y\"], np.arange(4).reshape((2, 2))), \"z\": (\"x\", [1, 2])}\n    )\n    ds2 = Dataset(\n        {\"data\": ([\"x\", \"y\"], np.arange(4, 8).reshape((2, 2))), \"coord\": (\"x\", [1, 2])}\n    )\n\n    expected_coords = {\"x\": [1, 2], \"y\": [0, 1, 2, 3], \"data\": (\"x\", \"y\"), \"coord\": (\"x\", [1, 2])}\n    expected_data = np.arange(8).reshape((2, 4))\n\n    actual = concat([ds1, ds2], dim=\"x\")\n    assert_identical(actual, expected_data)\n    assert actual.coords.keys() == expected_coords.keys()\n", "def test_concat_with_missing_coord_in_datasets() -> None:\n    ds1 = Dataset({\"a\": ((\"x\", \"y\"), np.arange(4).reshape((2, 2))), \"b\": [1, 1]})\n    ds2 = Dataset({\"a\": ((\"x\", \"y\"), np.arange(4, 8).reshape((2, 2), order=\"F\")), \"c\": [2, 2]})\n    \n    expected = Dataset(\n        {\"a\": ((\"x\", \"y\"), np.arange(8).reshape((2, 4))), \"b\": [1, 1, 1, 1], \"c\": [2, 2, 2, 2]},\n        coords={\"x\": [0, 1, 2, 3], \"y\": [0, 1]},\n    )\n\n    actual = concat([ds1, ds2], dim=\"x\")\n    assert_identical(actual, expected) \n", "def test_concat_with_same_dim_with_different_index_types() -> None:\n    ds1 = Dataset(coords={\"x\": (\"x\", [1, 2])})\n    ds2 = Dataset(coords={\"x\": pd.Index([3, 4])})\n\n    with pytest.raises(\n        ValueError,\n        match=r\"Cannot concatenate along dimension 'x' indexes with different types.*\",\n    ):\n        concat([ds1, ds2], dim=\"x\")\n\n"], "sample_1194": ["def test_Indexed():\n    from sympy.indexed import IndexedBase, Idx\n    x, y = symbols('x y')\n    i = Idx('i', 1, 5)\n    A = IndexedBase('A', 5)\n    assert julia_code(A[i]) == \"A[i]\"\n    assert julia_code(A[i] + A[i + 1]) == \"A[i] + A[i + 1]\"\n    assert julia_code(A[i] * A[i + 1]) == \"A[i] * A[i + 1]\"\n    assert julia_code(sum(A[i] for i in range(1, 5))) == \"sum(A[i] for i in 1:4)\" # Julia uses 1-based indexing!\n\n\n\n", "def test_IndexedBase():\n    x = IndexedBase('x')\n    y = IndexedBase('y')\n    i = Idx('i')\n    j = Idx('j')\n\n    assert julia_code(x[i]) == \"x[i]\"\n    assert julia_code(x[i] + y[j]) == \"x[i] + y[j]\"\n    assert julia_code(x[i] * y[j]) == \"x[i] .* y[j]\"\n    assert julia_code(sum(x[i for i in range(5)])) == \"sum(x[1:5])\"\n    assert julia_code(x[i]**2 + y[j]**2) == \"x[i] .^ 2 + y[j] .^ 2\"\n    assert julia_code(x[i:j]) == \"x[i:j]\"\n    assert julia_code(x[i::j]) == \"x[i:end(x):j]\"\n\n\n\n", "compilation error", "def test_indexed_symbols():\n    x = IndexedBase('x', shape=(3,))\n    y = IndexedBase('y', shape=(3,))\n    z = IndexedBase('z', shape=(2,))\n    i = Idx('i', range(3))\n    j = Idx('j', range(3))\n    k = Idx('k', range(2))\n\n\n    assert julia_code(x[i] + y[j]) == \"x[i] .+ y[j]\"\n    assert julia_code(x[i] * y[j]) == \"x[i] .* y[j]\"\n    assert julia_code(x[i] ** 2) == \"x[i] .^ 2\"\n    assert julia_code(x[i] / y[j]) == \"x[i] ./ y[j]\"\n\n    \n\n    assert julia_code(x[i, j]) == \"x[i, j]\"\n    assert julia_code(x[i, j] + y[i, k]) == \"x[i, j] .+ y[i, k]\"\n    assert julia_code(x[i, j] * y[i, k]) == \"x[i, j] .* y[i, k]\"\n    assert julia_code(x[i, j] / y[i, k]) == \"x[i, j] ./ y[i, k]\"\n    assert julia_code((x[i] * y[j]) .sum(axis=1)) == \"sum(x[i] .* y[j], dims=1)\"\n\n", "def test_indexed_base():\n    from sympy.tensor import IndexedBase\n    i = IndexedBase('i')\n    x = Symbol('x')\n    expr = i[x]\n    assert julia_code(expr) == \"i[x]\"\n"], "sample_927": ["def test_template_args_with_specializations():\n    check('function',\n          \"template<typename T> void f(T v = T{} )\", {1: 'f__T1v'})\n    check('function', \"template<typename T> void f(T v = T{42})\", {1: 'f__T1v'})\n    check('function', \"template<> void f<int>(int v = 42)\", {1: 'f__i1v'})\n    check('function', \"template<typename T, typename U> void f(T v = T{}, U w = U{})\", {1: 'f__T1vU1w'})\n    check('function', \"template<typename T, typename U> void f(T v = T{42}, U w = U{42})\", {1: 'f__T1vU1w'})\n    check('function', \"template<> void f<int, float>(int v = 42, float w = 3.14)\", {1: 'f__i1vF1w'})\n    check('function', \"template<typename T, typename U> void f(T v, U w = U{})\", {1: 'f__T1vU1w'})\n    check('function', \"template<typename T, typename U> void f(T v = T{42}, U w)\", {1: 'f__T1vU1w'})\n    check('function', \"template<> void f<int, float>(int v = 42)\", {1: 'f__i1v'})\n\n", "compilation error", "compilation error", "def test_template_args_in_type_alias():\n    check('type', \"template<typename T> using Alias = T;\",\n          {2: \"I_1TE1Alias\"}, key='using')\n    check('type', \"template<typename T> using Alias = T;\",\n          {2: \"I_1TE1Alias\"}, output='template<typename T> using Alias = T;')\n    check('type', \"template<typename T, typename U> using Alias = T;\",\n          {2: \"I_2TE1Alias1U\"}, key='using')\n    check('type', \"template<typename T, typename U> using Alias = T;\",\n          {2: \"I_2TE1Alias1U\"}, output='template<typename T, typename U> using Alias = T;')\n    check('type', \"template<typename... Args> using Alias = Args;\",\n          {2: \"I_DpTE1Alias\"}, key='using')\n    check('type', \"template<typename... Args> using Alias = Args;\",\n          {2: \"I_DpTE1Alias\"}, output='template<typename... Args> using Alias = Args;')\n    check('type', \"template<typename... Args> using Alias = std::tuple<Args...>;\",\n          {2: \"I_DpTE1Alias\"}, key='using')\n    check('type', \"template<typename... Args> using Alias = std::tuple<Args...>;\",\n          {2: \"I_DpTE1Alias\"}, output='template<typename... Args> using Alias = std::tuple<Args...>;')\n    check('type', \"template<> using Alias = int;\",\n          {2: \"IE1Alias1i\"}, key='using')\n    check('type', \"template<> using Alias = int;\",\n          {2: \"IE1Alias1i\"}, output='template<> using Alias = int;')\n\n\n\n", "compilation error"], "sample_816": ["def test_sparse_matrix_output_is_consistent(vectorizer_class):\n    test_data = [\"This is a test\", \"This is another test.\"]\n    vectorizer = vectorizer_class(sparse=True)\n    X = vectorizer.fit_transform(test_data)\n    X_dense = X.toarray()\n    assert_allclose_dense_sparse(X, X_dense)\n", "def test_vectorizer_empty_vocabulary(vectorizer):\n    vec = vectorizer()\n    X = vec.fit_transform([])\n    assert X.shape == (0, 0)\n", "compilation error", "def test_vectorizer_with_unknown_stopwords(vectorizer_class):\n    vec = vectorizer_class(stop_words=['unknown_stopword'])\n    data = ['this is a document with unknown stopword']\n    X = vec.fit_transform(data)\n    assert_array_equal(X.shape, (1, 1))\n\n\n", "def test_custom_preprocessor_with_none_returns_none(Estimator):\n    vec = Estimator(preprocessor=lambda x: None if x is None else x)\n    X = vec.fit_transform([None, 'hello world'])\n    assert_array_equal(X.toarray(), [[0], [1, 1, 1]])\n\n\n"], "sample_141": ["    def test_json_deserializer_with_deferred_field(self):\n        test_string = \"\"\"[{\n            \"pk\": 1,\n            \"model\": \"serializers.article\",\n            \"fields\": {\n                \"author\": 1,\n                \"headline\": \"Unknown many to many\",\n                \"pub_date\": \"2014-09-15T10:35:00\",\n                \"categories\": [DEFER_FIELD]\n            }\n        }]\"\"\"\n        with self.assertRaisesMessage(\n            DeserializationError,\n            r\"(serializers\\.article:pk=1) field_value was '.*' for field 'categories'\"\n        ):\n            for obj in serializers.deserialize('json', test_string, ignore=False):\n                obj.save()\n", "    def test_date_time_encoding(self):\n        dt = datetime.datetime(2023, 10, 26, 10, 30, 0)\n        self.assertEqual(json.dumps({'date_time': dt}, cls=DjangoJSONEncoder), '{\"date_time\": \"2023-10-26T10:30:00\"}')\n", "    def test_datetime(self):\n        dt = datetime.datetime(2023, 10, 26, 10, 30, 0)\n        self.assertEqual(\n            json.dumps({'dt': dt}, cls=DjangoJSONEncoder),\n            '{\"dt\": \"2023-10-26T10:30:00\"}'\n        )\n", "    def test_deferred_fields(self):\n        test_string = \"\"\"[{\n            \"pk\": 1,\n            \"model\": \"serializers.article\",\n            \"fields\": {\n                \"author\": DEFER_FIELD,\n                \"headline\": \"Unknown many to many\",\n                \"pub_date\": \"2014-09-15T10:35:00\",\n                \"categories\": [1]\n            }\n        }]\"\"\"\n        for obj in serializers.deserialize('json', test_string, ignore=False):\n            obj.save()\n            obj.refresh_from_db()\n            self.assertIsNotNone(obj.author)\n\n\n", "    def test_date_serialization(self):\n        dt = datetime.date(2023, 10, 26)\n        self.assertEqual(json.dumps({'date': dt}, cls=DjangoJSONEncoder), '{\"date\": \"2023-10-26\"}')\n"], "sample_623": ["    def test_too_small_chunks(self, shape, pref_chunks, req_chunks):\n        \"\"\"Reject chunks that are too small for the preferred chunking.\"\"\"\n        initial = self.create_dataset(shape, pref_chunks)\n        with pytest.raises(ValueError):\n            xr.open_dataset(\n                initial,\n                engine=PassThroughBackendEntrypoint,\n                chunks=dict(zip(initial[self.var_name].dims, req_chunks)),\n            )\n\n\n\n", "    def test_invalid_chunks(self, shape, pref_chunks, req_chunks):\n        \"\"\"Raise an error when the requested chunks are invalid.\"\"\"\n        initial = self.create_dataset(shape, pref_chunks)\n        with pytest.raises(ValueError):\n            xr.open_dataset(\n                initial,\n                engine=PassThroughBackendEntrypoint,\n                chunks=dict(zip(initial[self.var_name].dims, req_chunks)),\n            )\n\n\n\n", "    def test_smaller_chunks(self, shape, pref_chunks, req_chunks):\n        \"\"\"Don't warn when the requested chunks are smaller than the preferred chunks.\"\"\"\n        initial = self.create_dataset(shape, pref_chunks)\n        with assert_no_warnings():\n            final = xr.open_dataset(\n                initial,\n                engine=PassThroughBackendEntrypoint,\n                chunks=dict(zip(initial[self.var_name].dims, req_chunks)),\n            )\n        self.check_dataset(initial, final, explicit_chunks(req_chunks, shape))\n\n", "    def test_chunks_with_remainder(self, shape, pref_chunks, req_chunks):\n        \"\"\"Test handling of chunk sizes when the requested chunks don't fully divide the preferred chunks.\"\"\"\n        initial = self.create_dataset(shape, pref_chunks)\n        \n        final = xr.open_dataset(\n            initial,\n            engine=PassThroughBackendEntrypoint,\n            chunks=dict(zip(initial[self.var_name].dims, req_chunks)),\n        )\n        self.check_dataset(initial, final, explicit_chunks(req_chunks, shape))\n", "    def test_join_chunks(self, shape, pref_chunks, req_chunks):\n        \"\"\"Don't warn when the requested chunks join or keep the preferred chunks.\"\"\"\n        initial = self.create_dataset(shape, pref_chunks)\n        with assert_no_warnings():\n            final = xr.open_dataset(\n                initial,\n                engine=PassThroughBackendEntrypoint,\n                chunks=dict(zip(initial[self.var_name].dims, req_chunks)),\n            )\n        self.check_dataset(initial, final, explicit_chunks(req_chunks, shape))\n"], "sample_462": ["    def test_choicefield_with_custom_error_message(self):\n        f = ChoiceField(\n            choices=[(\"J\", \"John\"), (\"P\", \"Paul\")],\n            error_messages={\"invalid_choice\": \"This is a custom message.\"},\n        )\n        msg = \"This is a custom message.\"\n        with self.assertRaisesMessage(ValidationError, msg):\n            f.clean(\"3\")\n", "    def test_choicefield_empty_values(self):\n        f = ChoiceField(choices=[(\"1\", \"One\"), (\"2\", \"Two\")], empty_values=[\"\", None])\n        self.assertEqual(\"\", f.clean(\"\"))\n        self.assertEqual(\"1\", f.clean(1))\n        self.assertEqual(\"1\", f.clean(\"1\"))\n        msg = \"'Select a valid choice. 3 is not one of the available choices.'\"\n        with self.assertRaisesMessage(ValidationError, msg):\n            f.clean(\"3\")\n", "    def test_choicefield_empty_choices(self):\n        f = ChoiceField(choices=[])\n        with self.assertRaisesMessage(ValidationError, \"'This field is required.'\"):\n            f.clean(\"\")\n        with self.assertRaisesMessage(ValidationError, \"'This field is required.'\"):\n            f.clean(None)\n        msg = \"'Select a valid choice. 1 is not one of the available choices.'\"\n        with self.assertRaisesMessage(ValidationError, msg):\n            f.clean(\"1\") \n\n\n\n", "    def test_choicefield_empty_values(self):\n        f = ChoiceField(choices=[(\"1\", \"One\"), (\"2\", \"Two\")], empty_values=(\"\")),\n        self.assertEqual(\"\", f.clean(\"\"))\n\n\n", "    def test_choicefield_widgets(self):\n        f = ChoiceField(choices=[(\"1\", \"One\"), (\"2\", \"Two\")], widget=forms.RadioSelect)\n        self.assertWidgetRendersTo(\n            f,\n            '<div class=\"radio\"><input type=\"radio\" name=\"id_f\" value=\"1\" id=\"id_f_0\" checked=\"\">'\n            '<label for=\"id_f_0\">One</label></div>'\n            '<div class=\"radio\"><input type=\"radio\" name=\"id_f\" value=\"2\" id=\"id_f_1\">'\n            '<label for=\"id_f_1\">Two</label></div>',\n        )\n\n        f = ChoiceField(choices=[(\"1\", \"One\"), (\"2\", \"Two\")], widget=forms.SelectMultiple)\n        self.assertWidgetRendersTo(\n            f,\n            '<select name=\"id_f\" multiple=\"\" size=\"1\"><option value=\"1\">One</option>'\n            '<option value=\"2\">Two</option></select>',\n        )\n"], "sample_245": ["    def test_makemessages_with_invalid_locale(self):\n        with self.assertRaises(CommandError):\n            self.run_django_admin(['makemessages', '-l', 'invalid_locale', '-v', '0'])\n", "    def test_makemessages_with_no_templates_directory(self):\n        \"\"\"\n        makemessages should not raise an error if there is no templates directory.\n        \"\"\"\n        # Remove the templates directory\n        try:\n            os.rmdir(os.path.join(self.test_dir, 'templates'))\n        except OSError:\n            pass  # Directory might not exist\n\n        out, err = self.run_django_admin(['makemessages', '-l', 'en', '-v', '0'])\n        self.assertNoOutput(err)\n        self.assertNoOutput(out)\n", "    def test_makemessages_in_production(self):\n        with override_settings(DEBUG=False, TEMPLATE_DEBUG=False):\n            out, err = self.run_django_admin(['makemessages', '-l', 'en', '-v', '0'])\n        self.assertNoOutput(err)\n        self.assertNoOutput(out)\n\n\n\n", "    def test_makemessages_no_settings_with_locale_path(self):\n        os.environ['DJANGO_SETTINGS_MODULE'] = 'django.conf.global_settings'\n        out, err = self.run_django_admin(['makemessages', '-l', 'en', '-v', '0', '--locale-paths', '/no/such/path'])\n        self.assertNoOutput(err)\n        self.assertIn(\"Ignoring setting DJANGO_SETTINGS_MODULE as it was provided as an environment variable.\", out)\n        self.assertIn(\"locale directory not found in '/no/such/path'\", out)\n\n\n", "    def test_makemessages_no_settings_file(self):\n        os.remove(os.path.join(self.test_dir, 'settings.py'))\n        try:\n            out, err = self.run_django_admin(['makemessages', '-l', 'en', '-v', '0'])\n            self.assertNoOutput(err)\n            self.assertIn(\"django.conf.settings\", out)\n\n        finally:\n            with open(os.path.join(self.test_dir, 'settings.py'), 'w') as fp:\n                fp.write('')\n"], "sample_792": ["compilation error", "def test_fit_predict_multiclass():\n    # Test that fit and predict work correctly for multiclass NB \n    # when using pairwise classification.\n    X = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1],\n                  [1, 1, 0], [0, 1, 1], [1, 0, 1]])\n    y = np.array([0, 1, 2, 0, 2, 0])\n    clf = MultinomialNB()\n    clf.fit(X, y)\n    y_pred = clf.predict(X)\n    assert_array_equal(y_pred, y)\n\n", "def test_discrete_nb_with_informative_priors():\n    # Test setting class_prior with MultinomialNB and BernoulliNB\n    # using informative priors\n\n    X = [[1, 0], [0, 1], [1, 1]]\n    y = [0, 1, 0]\n    prior = np.array([0.8, 0.2])\n\n    clf_multinomial = MultinomialNB(class_prior=prior)\n    clf_multinomial.fit(X, y)\n    assert_array_almost_equal(clf_multinomial.class_log_prior_,\n                              np.log(prior))\n\n    clf_bernoulli = BernoulliNB(class_prior=prior)\n    clf_bernoulli.fit(X, y)\n    assert_array_almost_equal(clf_bernoulli.class_log_prior_,\n                              np.log(prior))\n", "def test_discrete_nb_warnings():\n    # Test warnings for fit and classes\n    X = np.array([[0, 1], [1, 0], [1, 1]])\n    y = np.array([0, 1, 0])\n\n    # Check for a warning when using fit_prior=True and class_prior is None\n    for clf in [BernoulliNB(), MultinomialNB()]:\n        with warnings.catch_warnings(record=True) as w:\n            clf.fit(X, y, fit_prior=True)\n            assert len(w) == 1, \"Expected one warning\"\n            assert issubclass(w[-1].category, UserWarning)\n            assert \"class_prior is None\" in str(w[-1].message)\n\n    # Check for a warning when using fit_prior=True and class_prior is not None\n    clf = BernoulliNB()\n    with warnings.catch_warnings(record=True) as w:\n        clf.fit(X, y, fit_prior=True, class_prior=[0.1, 0.9])\n        assert len(w) == 1, \"Expected one warning\"\n        assert issubclass(w[-1].category, UserWarning)\n        assert \"fit_prior=True while class_prior is provided\" in str(w[-1].message)\n\n    # Check for a warning when providing classes for partial_fit without \n    # setting fit_prior=True\n    for clf in [BernoulliNB(), MultinomialNB()]:\n        with warnings.catch_warnings(record=True) as w:\n            clf.partial_fit(X[:1], y[:1], classes=[0, 1])\n            assert len(w) == 1, \"Expected one warning\"\n            assert issubclass(w[-1].category, UserWarning)\n            assert \"fit_prior=True\" in str(w[-1].message)\n\n    clf = BernoulliNB()\n    with warnings.catch_warnings(record=True) as w:\n        clf.partial_fit(X[:1], y[:1], classes=[0, 1], fit_prior=True)\n        assert len(w) == 0\n\n\n", "def test_partial_fit_with_missing_data():\n    # Test partial_fit with missing data (NaN values)\n    X = np.array([[1, 2, np.nan],\n                  [4, 5, 6],\n                  [7, np.nan, 9]])\n    y = np.array([0, 1, 0])\n\n    # Test BernoulliNB\n    clf = BernoulliNB()\n    clf.partial_fit(X[:, :2], y, classes=[0, 1])  # fit on non-missing features\n    assert_raises(ValueError, clf.partial_fit, X[:, 2:], y)\n\n    # Test MultinomialNB\n    clf = MultinomialNB()\n    clf.partial_fit(X[:, :2], y, classes=[0, 1])  # fit on non-missing features\n    assert_raises(ValueError, clf.partial_fit, X[:, 2:], y)\n\n"], "sample_840": ["def test_pls_with_missing_values():\n    d = load_linnerud()\n    X = d.data\n    Y = d.target\n\n    # Introduce some missing values\n    for i in range(X.shape[0]):\n        X[i, rng.choice(X.shape[1])] = np.nan\n\n    for clf in [pls_.PLSCanonical(), pls_.PLSRegression(),\n                pls_.PLSSVD()]:\n        clf.fit(X, Y)\n        # Check that the fit did not raise an error\n        assert not isinstance(clf.transform(X), ValueError)\n\n\n\n", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_162": ["    def test_makemessages_file_errors(self):\n        \"\"\"Test 'gettext' output for files containing errors.\"\"\"\n        with tempfile.NamedTemporaryFile(mode='w+t', delete=False) as temp_file:\n            temp_file.write(\"This is a bad file\\n\")\n            temp_file.flush()\n            temp_file_path = temp_file.name\n\n        with self.assertRaises(CommandError) as context:\n            management.call_command('makemessages',\n                                    locale=['en'],\n                                    input_files=[temp_file_path],\n                                    verbosity=0)\n\n        self.assertIn(\"Error during extraction: unable to find the right charset\", str(context.exception))\n        os.remove(temp_file_path)\n\n\n\n", "    def test_makemessages_in_project_directory(self):\n        with override_settings(BASE_DIR=os.path.join(self.test_dir, 'project_dir')):\n            out, err = self.run_django_admin(['makemessages', '-l', 'en', '-v', '0'])\n            self.assertNoOutput(err)\n            self.assertNoOutput(out)\n", "    def test_makemessages_with_fake_settings(self):\n        with settings(INSTALLED_APPS=(), LANGUAGES=(), LOCALE_PATHS=()):\n            out, err = self.run_django_admin(['makemessages', '-l', 'en', '-v', '0'])\n            self.assertEmpty(out)\n            self.assertEmpty(err)\n\n\n\n\n", "    def test_makemessages_with_nonexistent_locale(self):\n        with self.assertRaises(CommandError):\n            self.run_django_admin(['makemessages', '-l', 'nonexistent_locale', '-v', '0'])\n", "    def test_no_settings_explicit_locale_path(self):\n        os.environ['DJANGO_SETTINGS_MODULE'] = ''\n        with tempfile.NamedTemporaryFile(delete=False) as f:\n            f.write(b\"locale_paths = ['/tmp/fake_locale']\\n\")\n            f.flush()\n            out, err = self.run_django_admin(\n                ['makemessages', '-l', 'en', '-v', '0', '--locale-paths', f.name]\n            )\n            self.assertNoOutput(out) \n            self.assertNoOutput(err)\n        os.remove(f.name) \n\n"], "sample_191": ["    def test_notify_file_changed_with_none_file(self):\n        with self.assertRaises(ValueError):\n            self.reloader.notify_file_changed(None)\n\n\n\n", "    def test_should_stop_returns_true_if_no_changes_in_files(self):\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file]):\n            snapshot = self.reloader.snapshot_files()\n            self.assertTrue(self.reloader.should_stop(snapshot))\n\n            with self.tick_twice():\n                self.assertFalse(self.reloader.should_stop(snapshot))\n", "    def test_stat_checks_for_modifications(self, mock_getmtime):\n        mock_getmtime.return_value = 1\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file]):\n            self.reloader._stat_checks()  \n        mock_getmtime.assert_called_once_with(self.existing_file)\n\n", "    def test_snapshot_files_no_modification(self):\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file]):\n            snapshot1 = dict(self.reloader.snapshot_files())\n            self.assertIn(self.existing_file, snapshot1)\n            snapshot2 = dict(self.reloader.snapshot_files())\n            self.assertEqual(snapshot1, snapshot2)\n", "    def test_watch_glob_without_existing_parent_directory(self):\n        with mock.patch.object(self.reloader, '_subscribe') as mocked_subscribe:\n            self.reloader._watch_glob(self.tempdir / 'does_not_exist', ['*'])\n        self.assertTrue(mocked_subscribe.called)\n        self.assertCountEqual(mocked_subscribe.call_args[0], [\n            self.tempdir, 'glob-parent-does_not_exist:%s' % self.tempdir,\n            ['anyof', ['match', 'does_not_exist/*', 'wholename']]\n        ])\n\n\n"], "sample_1209": ["def test_latex():\n    m = PREFIXES['m']\n    assert m._latex(None) == r'\\text{m}'\n    assert kibi._latex(None) == r'\\text{kibi}'\n    assert isinstance(kibi._latex(None), str)\n", "def test_latex():\n    assert PREFIXES['m']._latex(None) == r'\\text{m}'\n    assert PREFIXES['m']._latex(None) == r'\\text{m}'\n    assert PREFIXES['k']._latex(None) == r'\\text{k}'\n    assert PREFIXES['k']._latex(None) == r'\\text{k}'\n    assert PREFIXES['m']._latex(None) == r'\\text{m}'\n    assert PREFIXES['k']._latex(None) == r'\\text{k}'\n    assert PREFIXES['mu']._latex(None) == r\"\\mu\"\n\n\n\n\n", "def test_latex_repr():\n    assert PREFIXES['m']._latex(None) == r'\\text{m}'\n    assert PREFIXES['m']._latex(None) == r'\\text{m}'\n    assert isinstance(PREFIXES['m']._latex(None), str)\n    assert PREFIXES['mu']._latex(None) == r\"\\mu\"\n\n\n\n", "def test_latex_repr():\n    assert repr(PREFIXES['m']) == 'Prefix(m, m, 0)'\n    assert PREFIXES['m']._latex(None) == r'\\text{m}'\n    assert PREFIXES['m'].__repr__() == \"Prefix('m', 'm', 0)\"\n    assert PREFIXES['m'].__str__() == 'm'\n\n    assert repr(PREFIXES['mu']) == 'Prefix(micro, mu, -6, latex_repr=r\"\\mu\")'\n    assert PREFIXES['mu']._latex(None) == r\"\\mu\"\n\n", "def test_latex():\n    assert PREFIXES['m']._latex(None) == r'\\text{m}'\n    assert PREFIXES['k']._latex(None) == r'\\text{k}'\n    assert PREFIXES['k']._latex(None) == r'\\text{k}'\n    assert PREFIXES['m']._latex(None) != r'\\text{K}'\n"], "sample_175": ["    def test_fast_delete_with_signals(self):\n            pass\n\n        origin = Origin.objects.create()\n        signal = models.signals.pre_delete\n        signal.connect(receiver, sender=Referrer)\n        # With signals connected, we should see a query to retrieve all fields\n        with self.assertNumQueries(2) as ctx:\n            origin.delete()\n        signal.disconnect(receiver, sender=Referrer)\n\n\n\n", "    def test_fast_delete_with_signals(self):\n        \"\"\"\n        The signals `pre_delete` and `post_delete` are called even if a model\n        is fast-deleted.\n        \"\"\"\n        calls_pre = 0\n        calls_post = 0\n\n            nonlocal calls_pre\n            calls_pre += 1\n\n            nonlocal calls_post\n            calls_post += 1\n\n        models.signals.pre_delete.connect(pre, sender=Avatar)\n        models.signals.post_delete.connect(post, sender=Avatar)\n\n        a = Avatar.objects.create()\n        a.delete()\n\n        self.assertGreaterEqual(calls_pre, 1)\n        self.assertGreaterEqual(calls_post, 1)\n\n        models.signals.pre_delete.disconnect(pre, sender=Avatar)\n        models.signals.post_delete.disconnect(post, sender=Avatar)\n\n", "    def test_fast_delete_self_referential_relationships(self):\n        self_ref_node = SelfReferentialNode.objects.create(parent=None)\n        child_node = SelfReferentialNode.objects.create(parent=self_ref_node)\n        self.assertNumQueries(2, child_node.delete)\n        self.assertFalse(SelfReferentialNode.objects.filter(pk=child_node.pk).exists())\n        self.assertFalse(SelfReferentialNode.objects.filter(pk=self_ref_node.pk).exists())\n", "    def test_fast_delete_complex_hierarchy(self):\n        p = Parent.objects.create()\n        c1 = Child.objects.create(parent=p)\n        c2 = Child.objects.create(parent=p)\n        grandchild = Grandchild.objects.create(parent=c1)\n        self.assertNumQueries(1, p.delete)\n\n\n\n", "    def test_fast_delete_no_update_can_self_select_with_m2m(self):\n        \"\"\"\n        #25932 - Fast deleting on backends that don't have the\n        `no_update_can_self_select` feature should work even if the specified\n        filter doesn't match any row.\n        \"\"\"\n        \n        user = User.objects.create()\n        m2m_model = M2MTo.objects.create()\n        user.m2m.add(m2m_model)\n\n        with self.assertNumQueries(1):\n            self.assertEqual(\n                User.objects.filter(\n                    m2m__desc='missing'\n                ).delete(),\n                (1, {}),\n            )\n        \n        self.assertFalse(User.objects.exists())\n        self.assertFalse(M2MTo.objects.exists())\n"], "sample_71": ["    def test_grouping_with_non_uniform_digits(self):\n        self.assertEqual(nformat(1234567890, '.', grouping=(3, 2, 0), thousand_sep=','), '123,456,7890')\n        self.assertEqual(nformat(1234567890, '.', grouping=(3, 2, 0, 0), thousand_sep=','), '123,456,7890')\n", "    def test_scientific_notation(self):\n        self.assertEqual(nformat(1e200, '.'), '1e+200')\n        self.assertEqual(nformat(1e200, '.', decimal_pos=2), '1.00e+200')\n        self.assertEqual(nformat(1e-200, '.'), '1e-200')\n        self.assertEqual(nformat(1e-200, '.', decimal_pos=2), '1.00e-200')\n", "    def test_negative_grouping(self):\n        self.assertEqual(nformat(-1234, '.', grouping=3, thousand_sep=','), '-1,234')\n        self.assertEqual(nformat(-1234.56, '.', grouping=3, thousand_sep=','), '-1,234.56')\n        self.assertEqual(nformat(-1234, '.', grouping=3, thousand_sep=',', force_grouping=True), '-1,234')\n        self.assertEqual(nformat(-1234.56, '.', grouping=3, thousand_sep=',', force_grouping=True), '-1,234.56')\n", "    def test_grouping_with_negative_numbers(self):\n        self.assertEqual(nformat(-1234, '.', grouping=3, thousand_sep=','), '-1,234')\n        self.assertEqual(nformat(-1234.56, '.', grouping=3, thousand_sep=','), '-1,234.56')\n        self.assertEqual(nformat(-1234, '.', grouping=3, thousand_sep=',', force_grouping=True), '-1,234')\n        self.assertEqual(nformat(-1234.56, '.', grouping=3, thousand_sep=',', force_grouping=True), '-1,234.56')\n", "    def test_empty_number(self):\n        self.assertEqual(nformat(0), '0')\n        self.assertEqual(nformat(0.0), '0.0')\n        self.assertEqual(nformat(Decimal(0)), '0')\n        self.assertEqual(nformat(\"\"), '0')\n"], "sample_88": ["    def test_send_to_recipients_with_escapes(self):\n        backend = smtp.EmailBackend()\n        msg = EmailMessage('Subject', 'Content', 'from@example.com', ['to@example.com\\x08','to@example.com\\n'])\n        with backend:\n            backend.send_messages([msg])\n        messages = self.server.get_sink()\n        self.assertEqual(len(messages), 2)\n        self.assertMessageHasHeaders(messages[0], {\n            ('MIME-Version', '1.0'),\n            ('Content-Type', 'text/plain; charset=\"utf-8\"'),\n            ('Content-Transfer-Encoding', '7bit'),\n            ('Subject', 'Subject'),\n            ('From', 'from@example.com'),\n            ('To', 'to@example.com\\\\x08'),\n        })\n        self.assertMessageHasHeaders(messages[1], {\n            ('MIME-Version', '1.0'),\n            ('Content-Type', 'text/plain; charset=\"utf-8\"'),\n            ('Content-Transfer-Encoding', '7bit'),\n            ('Subject', 'Subject'),\n            ('From', 'from@example.com'),\n            ('To', 'to@example.com\\\\n'),\n        })\n\n\n\n", "    def test_mail_server_errors(self):\n        \"\"\"\n        When sending a mail a SMTP server error is raised\n        \"\"\"\n        class FakeSMTPServerError(Exception):\n            pass\n\n        with self.assertRaises(smtp.SMTPException):\n            server = FakeSMTPServer(('127.0.0.1', 0), None)\n            server.start()\n            server.server_loop = lambda: raise FakeSMTPServerError()\n\n            backend = smtp.EmailBackend(\n                host=server.socket.getsockname()[0], port=server.socket.getsockname()[1],\n                username='', password=''\n            )\n            with backend:\n                email = EmailMessage('Subject', 'Content', 'from@example.com', ['to@example.com'])\n                backend.send_messages([email])\n\n\n\n", "    def test_connection_closed_after_send_messages(self):\n        \"\"\"\n        Connection is closed after send_messages() completes.\n        \"\"\"\n        with self.assertRaises(ConnectionError):\n            with self.backend:\n                self.backend.send_messages([])\n        self.assertIsNone(self.backend.connection)\n\n", "    def test_send_messages_with_connection_errors(self):\n        \"\"\"\n        send_messages() should handle connection errors gracefully (if\n        fail_silently is True).\n        \"\"\"\n        self.backend.fail_silently = True\n        self.backend.open = lambda: None\n        email = EmailMessage('Subject', 'Content', 'from@example.com', ['to@example.com'])\n        sent = self.backend.send_messages([email])\n        self.assertEqual(sent, 0)\n\n\n", "    def test_send_message_with_custom_port(self):\n        with override_settings(EMAIL_PORT=9999):\n            backend = smtp.EmailBackend(host='127.0.0.1', port=9999)\n            email = EmailMessage('Subject', 'Content', 'from@example.com', ['to@example.com'])\n            self.assertEqual(backend.send_messages([email]), 1)\n            self.assertEqual(self.server.socket.getsockname()[1], 9999)\n"], "sample_305": ["    def test_self_referential_pre_fetch(self):\n        t1 = SelfRefFK.objects.create(name='t1')\n        t2 = SelfRefFK.objects.create(name='t2', parent=t1)\n        t3 = SelfRefFK.objects.create(name='t3', parent=t1)\n        qs = SelfRefFK.objects.select_related('parent').annotate(num_children=Count('children')).order_by('name')\n        self.assertQuerysetEqual(\n            qs,\n            [('t1', 2), ('t2', 0), ('t3', 0)],\n            lambda x: (x.name, x.num_children)\n        )\n\n\n\n", "    def test_ticket_24748_distinct(self):\n        t1 = SelfRefFK.objects.create(name='t1')\n        t2 = SelfRefFK.objects.create(name='t2', parent=t1)\n        t3 = SelfRefFK.objects.create(name='t3', parent=t1)\n        self.assertQuerysetEqual(\n            SelfRefFK.objects.annotate(num_children=Count('children')).filter(children__name='t2').distinct('name').order_by('name'),\n            [('t1', 1)],\n            lambda x: (x.name, x.num_children)\n        )\n", "    def test_prefetch_related_with_annotation(self):\n        t1 = SelfRefFK.objects.create(name='t1')\n        t2 = SelfRefFK.objects.create(name='t2', parent=t1)\n        t3 = SelfRefFK.objects.create(name='t3', parent=t1)\n        t4 = SelfRefFK.objects.create(name='t4', parent=t2)\n\n        qs = SelfRefFK.objects.prefetch_related('children').annotate(num_children=Count('children')).filter(name='t1')\n        result = list(qs)\n        self.assertEqual(result[0].num_children, 2)\n        self.assertEqual(len(result[0].children.all()), 2)\n", "    def test_self_ref_fk_multiple_levels(self):\n        t1 = SelfRefFK.objects.create(name='t1')\n        t2 = SelfRefFK.objects.create(name='t2', parent=t1)\n        t3 = SelfRefFK.objects.create(name='t3', parent=t2)\n        t4 = SelfRefFK.objects.create(name='t4', parent=t3)\n        self.assertQuerysetEqual(\n            SelfRefFK.objects.annotate(num_descendants=Count('children')).order_by('name'),\n            [('t1', 3), ('t2', 1), ('t3', 1), ('t4', 0)],\n            lambda x: (x.name, x.num_descendants)\n        )\n\n", "    def test_self_referential_fk_with_select_related(self):\n        t1 = SelfRefFK.objects.create(name='t1')\n        t2 = SelfRefFK.objects.create(name='t2', parent=t1)\n        t3 = SelfRefFK.objects.create(name='t3', parent=t1)\n        self.assertQuerysetEqual(\n            SelfRefFK.objects.select_related('parent').annotate(num_children=Count('children')).order_by('name'),\n            [('t1', 2), ('t2', 0), ('t3', 0)],\n            lambda x: (x.name, x.num_children)\n        )\n\n\n"], "sample_1116": ["def test_inverse_derivative():\n    A = MatrixSymbol('A', 2, 2)\n    x = symbols('x')\n    dC_dx = A.diff(x)\n    assert isinstance(Inverse(A).diff(x), Inverse)\n    assert (Inverse(A).diff(x) == -Inverse(A)*dC_dx)\n", "def test_inverse_derivative():\n    x = symbols('x')\n    A = MatrixSymbol('A', 2, 2)\n    A_prime = A.diff(x)\n\n    d_inv_A = Inverse(A).diff(x)\n    assert isinstance(d_inv_A, Inverse)\n    assert d_inv_A.arg.shape == (2, 2)\n\n    # Compare the derivatives\n    expected_d_inv_A = -Inverse(A) * A_prime * Inverse(A)\n    assert d_inv_A == expected_d_inv_A\n", "def test_inverse_derivative_matrix():\n    A = MatrixSymbol('A', 2, 2)\n    x = symbols('x')\n    dA_dx = MatrixSymbol('dA_dx', 2, 2)\n    A_inv = Inverse(A)\n\n    A_inv_lines = A_inv._eval_derivative_matrix_lines(x)\n    \n    assert len(A_inv_lines) == 4\n    assert all(isinstance(line, tuple) and len(line) == 2 for line in A_inv_lines)\n    assert all(isinstance(pointer, Basic) for line in A_inv_lines for pointer in line)\n    \n    # Verify derivative result\n    expected_derivative = -A_inv.arg._eval_derivative_matrix_lines(x)\n    \n    for i in range(len(A_inv_lines)):\n        assert A_inv_lines[i] == expected_derivative[i]\n\n", "def test_inverse_derivative():\n    A = MatrixSymbol('A', 2, 2)\n    x = symbols('x')\n    dA_dx = MatrixSymbol('dA_dx', 2, 2)\n    A_d = A + x*dA_dx\n\n    assert (A_d.I).doit()._eval_derivative_matrix_lines(x)[0].first_pointer.shape == (2, 2)\n    assert (A_d.I).doit()._eval_derivative_matrix_lines(x)[0].second_pointer.shape == (2, 2)\n", "compilation error"], "sample_336": ["    def test_lookaround_capture(self):\n        test_urls = [\n            ('/lookaround-capture/a-city/42', 'a-city', 42),\n            ('/lookbehind-capture/a-city/42', 'a-city', 42),\n        ]\n        for url, city, number in test_urls:\n            with self.subTest(url=url):\n                match = resolve(url)\n                self.assertEqual(match.kwargs['city'], city)\n                self.assertEqual(match.kwargs['number'], str(number))\n", "    def test_lookahead_and_lookbehind_combinations(self):\n        test_urls = [\n            ('/lookahead-/a-city/lookbehind+/another-city/', {'city': 'a-city', 'another_city': 'another-city'}, ),\n            ('/lookbehind-/a-city/lookahead+/another-city/', {'city': 'a-city', 'another_city': 'another-city'}, ),\n            ('/lookahead-/a-city/lookbehind-/another-city/', {'city': 'a-city', 'another_city': 'another-city'}, ),\n            ('/lookbehind-/a-city/lookahead-/another-city/', {'city': 'a-city', 'another_city': 'another-city'}, ),\n        ]\n        for test_url, kwargs in test_urls:\n            with self.subTest(url=test_url, kwargs=kwargs):\n                self.assertEqual(resolve(test_url).kwargs, kwargs)\n                self.assertEqual(reverse('lookahead_and_lookbehind', kwargs=kwargs), test_url)\n\n\n\n", "    def test_lookahead_groups(self):\n        test_urls = [\n            ('lookahead-group', {'city': 'a-city', 'number': '1'}, '/lookahead/(?P<number>[0-9]+)/(?P<city>a-city)/'),\n            ('lookbehind-group', {'city': 'a-city', 'number': '1'}, '/lookbehind/(?P<number>[0-9]+)/(?P<city>a-city)/'),\n        ]\n        for name, kwargs, expected in test_urls:\n            with self.subTest(name=name, kwargs=kwargs):\n                self.assertEqual(reverse(name, kwargs=kwargs), expected)\n\n\n\n", "    def test_precedence_ahead_of_lookbehind(self):\n        test_urls = [\n            ('/lookahead-positive/a-city/lookbehind-positive/another-city/', {'city': 'a-city'}),\n            ('/lookahead-positive/a-city/lookbehind-negative/another-city/', {'city': 'a-city'}),\n        ]\n        for test_url, kwargs in test_urls:\n            with self.subTest(url=test_url):\n                self.assertEqual(resolve(test_url).kwargs, kwargs)\n\n", "    def test_lookbehind_lookahead_nested(self):\n        test_urls = [\n            ('/lookbehind_lookahead/a-city/b-city/', {'city1': 'a-city', 'city2': 'b-city'}),\n        ]\n        for test_url, kwargs in test_urls:\n            with self.subTest(url=test_url, kwargs=kwargs):\n                self.assertEqual(resolve(test_url).kwargs, kwargs)\n\n\n\n"], "sample_953": ["def test_quickstart_special_characters(tempdir):\n    answers = {\n        'Root path': tempdir,\n        'Project name': 'Project *with _special_ chars!',\n        'Author name': 'Georg Brandl',\n        'Project version': '0.1',\n    }\n    qs.term_input = mock_input(answers)\n    d = {}\n    qs.ask_user(d)\n    qs.generate(d)\n\n    conffile = tempdir / 'conf.py'\n    assert conffile.isfile()\n    ns = {}\n    exec(conffile.read_text(), ns)\n    assert ns['project'] == 'Project *with _special_ chars!'\n    assert ns['html_static_path'] == ['_static']\n\n\n", "def test_quickstart_custom_templates(tempdir):\n    answers = {\n        'Root path': tempdir,\n        'Project name': 'Custom Templates',\n        'Author name': 'Test User',\n        'Project version': '0.1',\n    }\n    qs.term_input = mock_input(answers)\n    d = {}\n    qs.ask_user(d)\n    qs.generate(d, templatedir=tempdir)\n\n    # Check that a custom template file was used\n    conffile = tempdir / 'conf.py'\n    assert conffile.isfile()\n    ns = {}\n    exec(conffile.read_text(), ns)\n    assert ns.get('html_theme') == 'alabaster'\n\n    master_doc = tempdir / 'source' / 'master_doc.rst'\n    assert master_doc.isfile()\n    assert master_doc.read_text().startswith('...')\n", "def test_quickstart_custom_template(tempdir):\n    answers = {\n        'Root path': tempdir,\n        'Project name': 'Custom Template Test',\n        'Author name': 'Georg Brandl',\n        'Project version': '0.1',\n    }\n    qs.term_input = mock_input(answers)\n    d = {}\n    qs.ask_user(d)\n    \n    # Ensure a custom template file is provided to the generate function\n    custom_template_dir = tempdir / 'templates'\n    custom_template_dir.mkdir()\n    (custom_template_dir / 'root_doc.rst_t').write_text(\n        \"Custom template content for root_doc.rst\"\n    )\n    d['templatedir'] = str(custom_template_dir)\n    qs.generate(d)\n    \n    # Check if the custom template content is used in output files\n    masterfile = tempdir / 'source' / 'root_doc.rst'\n    assert masterfile.isfile()\n    assert masterfile.read_text() == \"Custom template content for root_doc.rst\"\n\n\n", "def test_quickstart_with_custom_templates(tempdir):\n    answers = {\n        'Root path': tempdir,\n        'Project name': 'Custom Templates',\n        'Author name': 'Test User',\n        'Project version': '0.1',\n    }\n    qs.term_input = mock_input(answers)\n    d = {}\n    qs.ask_user(d)\n    # This test relies on the `quickstart.py` script\n    # having a `templatedir` argument,\n    # which is used to set a custom template directory.\n    qs.generate(d, templatedir=tempdir / 'custom_templates')\n\n    masterfile = tempdir / 'source' / 'master_doc.rst'\n    assert masterfile.isfile()\n    with open(masterfile, 'r') as f:\n        content = f.read()\n    assert 'Custom Template' in content\n\n", "    def test_quickstart_existing_confpy_with_options(monkeypatch, tempdir):\n        (tempdir / 'conf.py').write(\n            'project = \"Old Project\"')\n        answers = {\n            'Please enter a new root path (or just Enter to exit)': str(tempdir),\n            'Project name': 'New Project',\n            'Author name': 'New Author',\n            'Project version': '1.0',\n        }\n        qs.term_input = mock_input(answers)\n        d = {}\n        with pytest.raises(SystemExit):\n            qs.ask_user(d)\n\n\n\n"], "sample_774": ["def test_one_hot_encoder_categorical_features_index():\n    enc = OneHotEncoder(sparse=False, categorical_features=2)\n    X = np.array([[1, 2, 3], [4, 5, 6]])\n    enc.fit(X)\n    assert_array_equal(enc.get_feature_names_out(), ['x2_2', 'x2_3'])\n    enc = OneHotEncoder(sparse=True, categorical_features=[1, 2])\n    X = np.array([[1, 2, 3], [4, 5, 6]])\n    enc.fit(X)\n    assert_array_equal(enc.get_feature_names_out(), ['x1_1', 'x1_4', 'x2_2', 'x2_3'])\n\n    with pytest.raises(ValueError, match=\"categorical_features should be\"):\n        enc = OneHotEncoder(sparse=False, categorical_features=[1, 2, 3])\n        enc.fit(X)\n\n\n\n", "def test_one_hot_encoder_handle_unknown_unknown_categories(X, handle_unknown):\n    enc = OneHotEncoder(handle_unknown=handle_unknown)\n    X_tr = enc.fit_transform(X)\n    if handle_unknown == 'ignore':\n        assert_array_equal(X_tr.toarray(), np.zeros((2, 4)))\n    else:\n        with pytest.raises(ValueError, match=\"Found unknown categories\"):\n            enc.fit_transform(X)\n", "def test_one_hot_encoder_missing_value_handling():\n    enc = OneHotEncoder(handle_unknown='ignore')\n    X = [['abc', 1, 55], ['def', 1, 55], ['ghi', 1, np.nan]]\n    X_transformed = enc.fit_transform(X)\n    expected_output = np.array([[1., 0., 1., 0.],\n                                [0., 1., 1., 0.],\n                                [0., 0., 1., 0.]])\n    assert_array_equal(X_transformed.toarray(), expected_output)\n\n\n\n", "def test_one_hot_encoder_categorical_to_numerical():\n    X = np.array([['a', 1, 'cat'], ['b', 2, 'dog'], ['a', 3, 'cat']]).T\n    enc = OneHotEncoder(handle_unknown='ignore', categories=[['a', 'b'], [1, 2], ['cat', 'dog']])\n    \n    ncols = enc.transform(X).shape[1]\n    assert ncols == 6\n    \n    enc = OneHotEncoder(handle_unknown='error', categories=[['a', 'b'], [1, 2], ['cat', 'dog']])\n    with pytest.raises(ValueError,\n                       match=r\"Found unknown categories\"):\n        enc.fit(X)\n", "compilation error"], "sample_1111": ["def test_multi_symbol():\n    x = Symbol('x')\n    y = Symbol('y')\n    lines = [\n        '      1 |                .      .                 ..',\n        '        |               / \\      .                         ',\n        '        |              /   \\     ..                     ',\n        '        |             /     \\    ..                    ',\n        '        |            /       \\   ..                   ',\n        '        |           /         \\  .                    ',\n        '        |          /           \\                       ',\n        '        |         /             \\                      ',\n        '        |        /               \\                     ',\n        '      0.5 |------------------------------/------------',\n        '        |         \\               /                    ',\n        '        |          \\             /                     ',\n        '        |           \\           /                      ',\n        '        |            \\         /                       ',\n        '        |             \\       /                        ',\n        '        |              \\     /                         ',\n        '        |               \\   /                          ',\n        '        |                \\\\ /                           ',\n        '        |                 \\/                            ',\n        '     -0.5 |_______________________________________________',\n        '         -1                          0                          1'\n    ]\n    assert lines == list(textplot_str(x * y, -1, 1)) \n", "def test_multiple_variables():\n    x = Symbol('x')\n    y = Symbol('y')\n    lines = [\n        '      1 |                                                    . ',\n        '        |                                                   ..   ',\n        '        |                                                  ...    ',\n        '        |                                                 ....   ',\n        '        |                                                .....  ',\n        '        |                                               ...... ',\n        '        |                                             .....      ',\n        '        |                                            ......       ',\n        '        |                                           .........    ',\n        '        |                                          ..........    ',\n        '    0.5 |-------------------------------------------------------',\n        '        |                                          .........    ',\n        '        |                                           ......        ',\n        '        |                                            .....       ',\n        '        |                                             .....      ',\n        '        |                                                .....   ',\n        '        |                                                 ....    ',\n        '        |                                                  ...     ',\n        '        |                                                   ..     ',\n        '        |                                                    .     ',\n        '      0 |_______________________________________________________',\n        '         -1                         0                          1'\n    ]\n    assert list(textplot_str(x**2 + y**2, -1, 1, W=21, H=11)) == lines\n\n", "def test_quadratic():\n    x = Symbol('x')\n    lines = [\n        '      1 |                 .................                         ',\n        '        |                ......................................        ',\n        '        |               ........................................    ',\n        '        |             ........................................... ',\n        '        |            ............................................ ',\n        '        |           ............................................ ',\n        '        |          ............................................. ',\n        '        |         .............................................. ',\n        '        |        .............................................. ',\n        '        |       .............................................. ',\n        '     -2 |-------------------------------------------------------',\n        '        |                ........................................ ',\n        '        |               ........................................ ',\n        '        |              ......................................... ',\n        '        |             .......................................... ',\n        '        |            .......................................... ',\n        '        |           ........................................... ',\n        '        |          ............................................ ',\n        '        |         ............................................ ',\n        '        |        ............................................. ',\n        '   -4 |_______________________________________________________',\n        '         -3                         0                          1'\n    ]\n    assert list(textplot_str(x**2 - 1, -3, 1)) == lines\n", "def test_exponential():\n    x = Symbol('x')\n    lines = [\n        '      1 |                                   .                 ',\n        '        |                                    .                ',\n        '        |                                   .                ',\n        '        |                                  .                 ',\n        '        |                              .....                 ',\n        '        |                           .....                  ',\n        '        |                         .........                 ',\n        '        |                       ...........                ',\n        '        |                    ............               ',\n        '        |               ..............              ',\n        '    0.5 |--------------------------------...--------------',\n        '        |              ....                    .....        ',\n        '        |        ...                        .....        ',\n        '        |       ...                         ..........      ',\n        '        |      ...                          ..........      ',\n        '        |     ...                           ..........     ',\n        '        |    ...                            ..........    ',\n        '        |   .                             ..........       ',\n        '        |   ..                            ..........      ',\n        '        |   ...                           ..........     ',\n        '      0 |_______________________________________________________',\n        '         -1                         0                          1'\n    ]\n    assert list(textplot_str(2**x, -1, 1)) == lines\n", "def test_zero():\n    x = Symbol('x')\n    lines = [\n        '      0 |-------------------------------------------------------',\n        '        |_______________________________________________________',\n        '         -1                         0                          1'\n    ]\n    assert lines == list(textplot_str(0, -1, 1)) \n\n\n\n"], "sample_1009": ["compilation error", "def test_vector_dot_operations():\n    N = ReferenceFrame('N')\n    A = N.orientnew('A', 'Axis', [pi/2, N.y])\n    B = N.orientnew('B', 'Axis', [pi/3, N.z])\n\n    x, y, z = symbols('x y z')\n    v1 = x*A.x + y*A.y + z*A.z\n    v2 = x*N.x + y*N.y + z*N.z\n    v3 = x*B.x + y*B.y + z*B.z\n\n    assert dot(v1, v2) == x**2 + y**2 + z**2\n    assert dot(v1, v3) == x*cos(pi/3) + y*cos(pi/2) + z*cos(pi/2)\n    assert dot(v2, v3) == x*cos(pi/3) + y*cos(pi/2) + z*cos(pi/2)\n    assert dot(v1, v1) == x**2 + y**2 + z**2\n\n    assert dot(v1, v1 * 2) == 2*(x**2 + y**2 + z**2)\n\n    # Test dot product with a scalar\n    scalar = 3\n    assert dot(scalar * v1, v2) == 3 * (x**2 + y**2 + z**2)\n\n    # Test dot product with zero vector\n    v4 = 0*A.x + 0*A.y + 0*A.z\n    assert dot(v1, v4) == 0\n\n    # Test dot product with a vector in a different frame\n    v5 = 2 * N.x + 3 * N.y + 4 * N.z\n    assert dot(v1.express(N), v5) == 2*x + 3*y + 4*z\n\n\n", "def test_vector_cross():\n    N = ReferenceFrame('N')\n    A = N.orientnew('A', 'Axis', [pi/2, N.x])\n\n    x, y, z = symbols('x y z')\n\n    v1 = x*A.x + y*A.y + z*A.z\n    v2 = x*N.x + y*N.y + z*N.z\n\n    assert (v1 ^ v2) == (z*N.x - y*N.z) \n   \n    # test with a scalar multiple\n    assert (2*v1 ^ v2) == 2*(z*N.x - y*N.z)\n    assert (v1 ^ 2*v2) == 2*(z*N.x - y*N.z)\n\n    # test with 0 vectors\n    assert (v1 ^ Vector(0)) == Vector(0)\n    assert (Vector(0) ^ v2) == Vector(0)\n", "def test_vector_cross():\n    N = ReferenceFrame('N')\n    A = N.orientnew('A', 'Axis', [pi/4, N.y])\n    B = N.orientnew('B', 'Axis', [pi/4, N.z])\n\n    v1 = A.x + A.y\n    v2 = B.x + B.z\n    \n    cross_result = v1 ^ v2\n    assert isinstance(cross_result, Vector)\n    assert cross_result.separate() == {N: - N.y + N.z}\n\n    v3 = A.x + 2*A.y\n    v4 = B.x + B.z\n\n    cross_result = v3 ^ v4\n    assert isinstance(cross_result, Vector)\n    assert cross_result.separate() == {N: - 2*N.y + N.z}\n", "compilation error"], "sample_0": ["def test_conversion_to_unknown_type_raises_error(UncertClass):\n    uncert = np.arange(1, 11).reshape(2, 5) * u.adu\n    start_uncert = UncertClass(uncert)\n    with pytest.raises(UnknownUncertainty):\n        start_uncert.represent_as(UnknownUncertainty)\n\n", "def test_conversion_to_from_variance_not_supported(UncertClass):\n    uncert = np.arange(1, 11).reshape(2, 5) * u.adu\n    start_uncert = UncertClass(uncert)\n    with pytest.raises(TypeError):\n        var_uncert = start_uncert.represent_as(VarianceUncertainty)        \n    with pytest.raises(TypeError):\n        final_uncert = var_uncert.represent_as(UncertClass)\n", "def test_conversion_to_from_variance_with_unit_conversion(UncertClass):\n    uncert = np.arange(1, 11).reshape(2, 5) * u.adu\n    start_uncert = UncertClass(uncert)\n    var_uncert = start_uncert.represent_as(VarianceUncertainty)\n    final_uncert = var_uncert.represent_as(UncertClass)\n    assert_array_equal(start_uncert.array, final_uncert.array)\n    assert start_uncert.unit == final_uncert.unit\n\n\n", "def test_assigning_uncertainty_with_invalid_array_fails(UncertClass):\n    with pytest.raises(TypeError):\n        UncertClass(1)\n    with pytest.raises(TypeError):\n        UncertClass(True)\n    with pytest.raises(TypeError):\n        UncertClass([1, 2, 3, 'a'])\n\n\n\n", "def test_pickle_with_parent_nddata(UncertClass):\n    ndd = NDData(np.ones(5))\n    uncert = UncertClass(np.ones(5))\n    ndd.uncertainty = uncert\n    uncert_pickled = pickle.loads(pickle.dumps(uncert))\n    assert uncert_pickled.parent_nddata is not None\n    assert uncert_pickled.parent_nddata.data is not None\n\n\n\n"], "sample_182": ["    def test_union_with_different_field_names(self):\n        ReservedName.objects.create(name='a', order=2)\n        qs1 = ReservedName.objects.values('name', 'order', 'id')\n        qs2 = ReservedName.objects.values('id', 'order', 'name')\n        self.assertEqual(list(qs1.union(qs2).values_list('name', 'order', 'id')), [\n            ('a', 2, 1),\n        ])\n\n", "    def test_union_with_distinct_and_ordering(self):\n        qs1 = Number.objects.filter(num__lte=1)\n        qs2 = Number.objects.filter(num__gte=8)\n        self.assertNumbersEqual(qs1.union(qs2, all=True).order_by('num'),\n        [1, 0, 8, 9], ordered=False)\n", "    def test_union_with_ambiguous_fields(self):\n        ReservedName.objects.create(name='a', order=2)\n        qs1 = ReservedName.objects.all().values('name', 'order')\n        qs2 = Number.objects.all().values('num', 'num')\n        \n        self.assertRaises(ValueError, lambda: qs1.union(qs2))\n\n\n\n", "    def test_values_list_with_union(self):\n        ReservedName.objects.create(name='a', order=2)\n        qs1 = ReservedName.objects.all().values_list('name', 'order')\n        qs2 = Number.objects.filter(num=1).values_list('num')\n        self.assertCountEqual(qs1.union(qs2), [('a', 2), (1,)])\n\n", "    def test_select_related_with_union(self):\n        ReservedName.objects.create(name='a', order=2)\n        qs1 = Number.objects.filter(num=1).select_related('reserved_name')\n        qs2 = Number.objects.filter(num=2).select_related('reserved_name')\n        qs3 = Number.objects.filter(num=3).select_related('reserved_name')\n        self.assertQuerysetEqual(\n            qs1.union(qs2, qs3).select_related('reserved_name'),\n            qs1.union(qs2, qs3).values('num', 'reserved_name__order'),\n            lambda obj: (obj['num'], obj['reserved_name__order'])\n        )\n\n"], "sample_519": ["def test_unpickle_with_figure_extension():\n    fig = Figure(figsize=(10, 5), dpi=100, tight_layout=True, facecolor='lightgray')\n    fig.savefig(\"test_fig.png\")\n    with open(\"test_fig.png\", \"rb\") as f:\n        fig_bytes = f.read()\n\n    fig2 = Figure.from_binary(fig_bytes)\n    assert fig2.get_size_inches() == (10, 5)\n    assert fig2.dpi == 100\n    assert fig2.tight_layout == True\n    assert fig2.get_facecolor() == 'lightgray'\n\n    plt.close(\"all\")\n", "def test_figure_savefig_with_dpi(recwarn):\n    warnings.filterwarnings(\"ignore\", \"cannot show the figure\")\n    fig = plt.figure()\n    dpi = 150\n    fig.savefig('test.png', dpi=dpi)\n    assert os.path.exists('test.png')\n\n    # check the image dimensions\n    img = imageio.imread('test.png')\n    assert img.shape[0] * img.shape[1] == dpi * dpi  \n    os.remove('test.png')\n", "def test_subplot_on_gridspec_with_existing_subplot():\n    fig = plt.figure()\n    gs = fig.add_gridspec(3, 2)\n    ax1 = fig.add_subplot(gs[0, 0])\n    ax2 = fig.add_subplot(gs[1, 1])\n\n    with pytest.raises(ValueError, match='subplot already exists'):\n        fig.add_subplot(gs[0, 1])\n\n\n\n", "compilation error", "compilation error"], "sample_990": ["compilation error", "def test_tanh_expansion():\n    x, y = symbols('x,y')\n    assert tanh(x+y).expand(trig=True) == (tanh(x) + tanh(y))/(1 + tanh(x)*tanh(y))\n    assert tanh(2*x).expand(trig=True) == 2*tanh(x)/(1 - tanh(x)**2)\n\n\n\n", "compilation error", "def test_atanh_expansion():\n    x = Symbol('x')\n    assert atanh(x).expand(trig=True).simplify() == (1/2)*log((1 + x)/(1 - x))\n    assert atanh(2*x).expand(trig=True).simplify() == \\\n        (1/2)*log((1 + 2*x)/(1 - 2*x))\n    assert atanh(x**2).expand(trig=True).simplify() == \\\n        (1/2)*log((1 + x**2)/(1 - x**2))\n  ", "def test_tanh_expansion():\n    x, y = symbols('x,y')\n    assert tanh(x + y).expand(trig=True) == (tanh(x) + tanh(y))/(1 + tanh(x)*tanh(y))\n    assert tanh(2*x).expand(trig=True) == 2*tanh(x)/(1 - tanh(x)**2)\n    assert tanh(3*x).expand(trig=True).expand() == \\\n        (3*tanh(x) - tanh(x)**3)/(1 - tanh(x)**2) ** 3/2 \n"], "sample_438": ["    def test_through_model_ordering(self):\n        # Create a through model for a ManyToMany relationship\n        class ThroughModel(models.Model):\n            question = models.ForeignKey(Question, on_delete=models.CASCADE)\n            answer = models.ForeignKey(Answer, on_delete=models.CASCADE)\n            order = models.IntegerField(default=0)\n\n        question = Question.objects.create(text=\"test\")\n        answer1 = Answer.objects.create(question=question)\n        post = Post.objects.create(title=\"Answer\", parent=question)\n\n        # Set the order for the answers through the through model\n        through_objects = ThroughModel.objects.bulk_create([\n            ThroughModel(question=question, answer=answer1, order=1),\n            ThroughModel(question=question, answer=answer2, order=2),\n        ])\n\n        # Assert that the answers are ordered correctly\n        sorted_answers = [\n            through_obj.answer for through_obj in ThroughModel.objects.filter(question=question).order_by('order')\n        ]\n        self.assertEqual(sorted_answers, [answer1, answer2])\n\n\n\n", "    def test_get_content_type_with_arguments(self):\n        answer = Answer.objects.create(text=\"Yes\", question=Question.objects.create(text=\"Do you like pizza?\"))\n        self.assertEqual(answer.question.get_content_type(), ContentType.objects.get_for_model(Question))\n", "    def test_value_from_string(self):\n        question = Question.objects.create(text=\"test\")\n        answer1 = Answer.objects.create(question=question)\n        answer2 = Answer.objects.create(question=question)\n        result = Question.answer_set.field.value_from_string(json.dumps([answer1.pk, answer2.pk]))\n        self.assertEqual(result, {answer1.pk: None, answer2.pk: None})\n", "    def test_through_model(self):\n        Question.objects.create(text=\"test\")\n        Answer.objects.create(text=\"answer1\", question_id=1)\n        Answer.objects.create(text=\"answer2\", question_id=1)\n\n        through_model_objects = Post.objects.with_related_objects().filter(\n            content_type=Post._meta.model_name,\n            object_id=1,\n        ).values()\n        self.assertEqual(len(list(through_model_objects)), 2)\n\n\n\n", "    def test_related_model_ordering(self):\n        with self.subTest(\"order_with_respect_to\"):\n            question1 = Question.objects.create(text=\"Test question 1\", ordering=1)\n            question2 = Question.objects.create(text=\"Test question 2\", ordering=2)\n\n            question1_answer = Answer.objects.create(question=question1, text=\"Answer 1\")\n            answer2 = Answer.objects.create(question=question2, text=\"Answer 2\")\n\n            answers = Answer.objects.all().order_by(\n                \"question__ordering\"\n            )\n            self.assertEqual(answers[0].question, question1)\n            self.assertEqual(answers[1].question, question2)\n\n            # Using the model accessor for ordering\n            question = Post.objects.create(\n                title=\"Test post\", parent=question1\n            )\n            questions = Post.objects.filter(parent__ordering__gt=1)\n            self.assertEqual(questions[0].parent, question2)\n\n\n\n        with self.subTest(\"ordering_reverse\"):\n            question1 = Question.objects.create(text=\"Test question 1\", ordering=1)\n            question2 = Question.objects.create(text=\"Test question 2\", ordering=2)\n\n            question1_answer = Answer.objects.create(question=question1, text=\"Answer 1\")\n            answer2 = Answer.objects.create(question=question2, text=\"Answer 2\")\n\n            answers = Answer.objects.all().order_by(\n                \"-question__ordering\"\n            )\n            self.assertEqual(answers[0].question, question2)\n            self.assertEqual(answers[1].question, question1)\n\n\n\n"], "sample_1079": ["compilation error", "def test_project_onto_line():\n    p = Point3D(1, 2, 3)\n    line = Line3D((0, 0, 0), (1, 1, 1))\n    assert p.project_onto_line(line) == Point3D(1, 1, 1)\n    p = Point3D(2, 4, 6)\n    line = Line3D((0, 0, 0), (1, 1, 1))\n    assert p.project_onto_line(line) == Point3D(1, 1, 1)\n    p = Point3D(1, 2, 3)\n    line = Line3D((0, 0, 0), (1, 0, 0))\n    assert p.project_onto_line(line) == Point3D(1, 0, 0)\n    p = Point3D(1, 2, 3)\n    line = Line3D((1, 2, 3), (2, 3, 4))\n    assert p.project_onto_line(line) == Point3D(1, 2, 3)\n    with warns(UserWarning):\n        assert Point2D(1, 2).project_onto_line(Line((0, 0), (1, 1))) == Point(1, 1)\n", "compilation error", "compilation error", "compilation error"], "sample_991": ["def test_symbolic_limits():\n    n = Symbol('n', integer=True)\n    k = Symbol('k', integer=True)\n\n    p1 = Product(n, (k, 1, n))\n    assert p1.doit() == n**n\n    p2 = Product(k, (k, 1, n))\n    assert p2.doit() == factorial(n)\n    p3 = Product(n**k, (k, 0, n)).doit()\n    assert p3 == n**n*(n+1)//2\n\n    p4 = Product(n + k, (k, 0, n))\n    assert p4.doit() == (n*(n+1)//2)**2\n    p5 = Product(n**k, (n, 1, 5))\n    assert p5.doit() == n**(5*(5+1)//2)\n\n    p6 = Product(k**2, (k, 0, n))\n    assert p6.doit() == (n*(n+1)*(2*n+1))/6 \n\n\n\n", "def test_issue_15380():\n    k, n = symbols('k, n', integer = True)\n    assert Product(cos(k*pi/(n + 1)), (k, 0, n)).rewrite(Sum) == \\\n        Sum(Sum(cos(j*k*pi/(n + 1))*cos(j*k*pi/(n + 1)), (j, 0, n - 1)), (k, 0, n))\n\n", "def test_product_limits_with_oo():\n    n = Symbol('n', integer=True)\n    m = Symbol('m', integer=True)\n\n    assert product(1/n, (n, 1, oo)).doit() == oo  # Issue 8301\n    assert product(2, (n, 1, oo)).doit() == oo\n    assert product(1/n**2, (n, 1, oo)).doit() == 0\n    assert product(2**n, (n, m, oo)).doit() == oo\n    assert product(2**n, (n, 1, m)).doit() == 2**rf(1, m)\n    assert product(2**n, (n, 1, oo)).rewrite(Sum).doit().simplify() == oo\n", "compilation error", "def test_Product_eval_simplify():\n    from sympy import simplify\n    x, y, z = symbols('x, y, z')\n    p = Product((x + y)**2, (x, 1, 3))\n    assert simplify(p.evalf()) == simplify( (x + y)**6 )\n\n\n"], "sample_212": ["    def test_session_middleware_skips_save_on_500_response(self):\n        with self.settings(SESSION_SAVE_EVERY_REQUEST=True):\n            request = HttpRequest()\n            request.session.modified = True\n            response = HttpResponse(status=500)\n            middleware = SessionMiddleware()\n            response = middleware.process_response(request, response)\n            self.assertFalse(request.session.modified)\n            self.assertEqual(response.status_code, 500)\n", "    def test_session_middleware(self):\n        request = HttpRequest()\n        response = HttpResponse()\n        middleware = SessionMiddleware(None)\n\n        # Simulate session being modified.\n        request.session.modified = True\n\n        middleware.process_response(request, response)\n        self.assertIn(settings.SESSION_COOKIE_NAME, response.cookies)\n\n\n\n", "    def test_session_middleware_deprecation(self):\n        with self.subTest(middleware=SessionMiddleware):\n            with self.assertRaisesMessage(RemovedInDjango40Warning, self.msg):\n                SessionMiddleware()\n", "    def test_session_middleware_access_attributes(self):\n        session_middleware = SessionMiddleware()\n        request = HttpRequest()\n        request.session = None\n\n        # Invoke the process_request method to initialize session\n        session_middleware.process_request(request)\n        self.assertIsNotNone(request.session)\n        self.assertEqual(request.session.accessed, True)\n        self.assertEqual(request.session.modified, False)\n        self.assertFalse(request.session.is_empty())\n\n        # Modify session data\n        request.session['test_key'] = 'test_value'\n        self.assertEqual(request.session.modified, True)\n\n        # Invoke process_response to trigger session save\n        response = HttpResponse()\n        session_middleware.process_response(request, response)\n        self.assertEqual(request.session.modified, False)  \n        self.assertEqual(request.session.accessed, True) \n", "    def test_session_middleware(self):\n        request = HttpRequest()\n        request.COOKIES[settings.SESSION_COOKIE_NAME] = 'test_session_key'\n        response = HttpResponse()\n\n        with self.subTest('SessionMiddleware'):\n            middleware = SessionMiddleware()\n            middleware.process_request(request)\n            # Assert that the session object is created and populated.\n            self.assertIsNotNone(request.session)\n            self.assertEqual(request.session.session_key, 'test_session_key')\n\n            middleware.process_response(request, response)\n            # Assert that the session cookie is set in the response.\n            self.assertIn(settings.SESSION_COOKIE_NAME, response.cookies)\n"], "sample_86": ["    def test_lazy_keep_lazy(self):\n        @keep_lazy(str)\n            return f\"Hello, {text}!\"\n\n        result = my_func(\"world\")\n        self.assertEqual(result, \"Hello, world!\")\n\n        lazy_text = lazy(lambda: \"world\", str)\n        result = my_func(lazy_text)\n        self.assertEqual(result, \"Hello, world!\")\n\n        @keep_lazy_text\n            return f\"Greetings, {text}!\"\n\n        result = my_other_func(\"Alice\")\n        self.assertEqual(result, \"Greetings, Alice!\")\n\n        lazy_text = lazy(lambda: \"Alice\", str)\n        result = my_other_func(lazy_text)\n        self.assertEqual(result, \"Greetings, Alice!\")\n\n\n\n", "    def test_keep_lazy(self):\n            return a + b + c + d\n\n        @keep_lazy(int)\n            return wrapped_func(a, b, c, d)\n\n        # All arguments are integer\n\n        self.assertEqual(my_func(1, 2, 3, 4), 10)\n\n        # Some arguments are lazy\n\n        lazy_a = lazy(lambda: 1)\n        lazy_b = lazy(lambda: 2)\n        self.assertEqual(my_func(lazy_a(), 3, 4, lazy_b()), 10)\n\n        # Some arguments are integers, some are lazy\n\n        self.assertEqual(my_func(1, lazy(lambda: 2), 3, 4), 10)\n\n        self.assertEqual(my_func(1, 2, lazy(lambda: 3), 4), 10)\n\n        self.assertEqual(my_func(1, 2, 3, lazy(lambda: 4)), 10)\n\n\n\n", "    def test_keep_lazy(self):\n        @keep_lazy(list)\n            return list(items)\n\n        result = add_items(1, 2, 3)\n        self.assertEqual(result, [1, 2, 3])\n\n        lazy_item = lazy(lambda: 4, int)\n        result = add_items(1, lazy_item(), 3)\n        self.assertEqual(result, [1, 4, 3])\n\n\n\n\n", "    def test_keep_lazy_no_lazy_args(self):\n        @keep_lazy()\n            return a + b\n\n        self.assertEqual(func(1, 2), 3)\n", "    def test_keep_lazy_with_mixed_args(self):\n        @keep_lazy(str, int)\n            return f\"Text: {text}, Number: {number}\"\n\n        self.assertEqual(my_func(\"hello\", 42), \"Text: hello, Number: 42\")\n        self.assertEqual(my_func(lazystr(\"world\"), 42), \"Text: world, Number: 42\")\n        self.assertEqual(my_func(\"hello\", lazy(lambda: 42, int)), \"Text: hello, Number: 42\")\n\n        with self.assertRaises(TypeError) as ctx:\n            my_func(lazystr(\"hello\"), lazy(lambda: 42, int))\n        self.assertEqual(\n            str(ctx.exception),\n            \"keep_lazy decorator can only be used \"\n            \"with function arguments of known return type or lazy arguments.\"\n        )\n"], "sample_169": ["    def test_key_transform_multiple_levels(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(value__d__0__2__foo__iexact='bar'),\n            [self.objs[4]],\n        )\n", "    def test_key_transform_with_raw_expression(self):\n        expr = RawSQL(self.raw_sql, ['{\"x\": {\"a\": \"b\", \"c\": \"d\"}}'])\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(value__baz__x__contains=expr),\n            [self.objs[7]],\n        )\n\n\n\n", "    def test_order_by_key_transform_with_null_value(self):\n        obj1 = NullableJSONModel.objects.create(value={'d__0': None})\n        obj2 = NullableJSONModel.objects.create(value={'d__0': 1})\n        qs = NullableJSONModel.objects.order_by('value__d__0')\n        self.assertSequenceEqual(qs, [obj1, obj2])\n        qs = NullableJSONModel.objects.order_by(KeyTransform('d', 'value'))\n        self.assertSequenceEqual(qs, [obj1, obj2])\n", "    def test_ordering_with_nulls(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(value__isnull=False).order_by('value__a'),\n            [self.objs[3], self.objs[4]],\n        )\n\n", "    def test_key_transform_order_by(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.values(value__d__0).order_by('value__d__0'),\n            [\n                {'value__d__0': None}, {'value__d__0': 1},\n                {'value__d__0': 1}, {'value__d__0': 2},\n                {'value__d__0': 2}, {'value__d__0': 93},\n                {'value__d__0': 21.931902}, {'value__d__0': -100291029}\n            ],\n        )\n"], "sample_521": ["def test_grid_lines_display():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection=\"3d\")\n\n    # Test default grid lines\n    ax.grid(True)\n    ax.set_xlim([0, 10])\n    ax.set_ylim([0, 10])\n    ax.set_zlim([0, 10])\n\n    fig.canvas.draw()\n\n\n", "def test_equal_aspect_ratio_3d():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    ax.set_box_aspect((1, 1, 1))\n    ax.plot([0, 1, 2], [0, 1, 2], [0, 1, 2])\n    fig.canvas.draw()\n\n\n", "def test_grid_3d_labels():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    ax.plot([1, 2, 3], [4, 5, 6], [7, 8, 9])\n    ax.grid(True, which='major', linestyle='--', linewidth=0.5, color='gray')\n    fig.canvas.draw()\n\n    # Check that axis labels are placed correctly\n    labels = ax.get_xticklabels()\n    x_label_positions = [l.get_position() for l in labels]\n    assert np.all(np.array(x_label_positions) > 0.5) and np.all(\n        np.array(x_label_positions) < 0.7\n    )\n\n\n\n", "def test_set_box_aspect_with_projection_type():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    ax.set_box_aspect((1, 1, 1))  \n    ax.set_projection('ortho')  \n    assert ax.get_box_aspect() == (1, 1, 1)\n\n\n", "def test_grid():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    ax.grid(True)\n    ax.set_xlim(0, 5)\n    ax.set_ylim(0, 5)\n    ax.set_zlim(0, 5)\n\n\n\n"], "sample_476": ["compilation error", "    def test_upload_to_with_prefix(self):\n        \"\"\"\n        Tests using upload_to with a prefix added to filenames.\n        \"\"\"\n        p = self.PersonModel(name=\"Joe\")\n        p.mugshot.save(\"mug.jpg\", self.file1)\n        p.save()\n        self.assertEqual(p.mugshot.path, f\"mugshots/mug.jpg\")\n\n\n\n        with self.settings(\n            MIDDLEWARE=settings.MIDDLEWARE + [\"django.middleware.locale.LocaleMiddleware\"],\n        ):\n            with self.subTest(\"with locale\"):\n                from django.utils import translation\n                translation.activate(\"en\")\n\n                p.mugshot.save(\"mug.jpg\", self.file1)\n                p.save()\n                self.assertEqual(p.mugshot.path, f\"mugshots/mug.jpg\")\n\n\n", "    def test_assignment_to_none_with_other_image(self):\n        p = self.PersonModel()\n        p.mugshot = self.file1\n        self.check_dimensions(p, 4, 8, \"mugshot\")\n        self.check_dimensions(p, None, None, \"headshot\")\n\n        p.headshot = self.file2\n        self.check_dimensions(p, 4, 8, \"mugshot\")\n        self.check_dimensions(p, 8, 4, \"headshot\")\n\n        p.mugshot = None\n        self.check_dimensions(p, None, None, \"mugshot\")\n        self.check_dimensions(p, 8, 4, \"headshot\")\n", "    def test_field_assignment_dimensions_cleared(self):\n        p = self.PersonModel()  \n        p.mugshot = self.file1\n        self.check_dimensions(p, 4, 8, \"mugshot\")\n        p.mugshot = self.file2\n        self.check_dimensions(p, 8, 4, \"mugshot\")\n        p.mugshot = None\n        self.check_dimensions(p, None, None, \"mugshot\")\n        p.headshot = self.file1\n        self.check_dimensions(p, None, None, \"mugshot\")\n        self.check_dimensions(p, 4, 8, \"headshot\")\n        p.headshot = self.file2\n        self.check_dimensions(p, None, None, \"mugshot\")\n        self.check_dimensions(p, 8, 4, \"headshot\")\n        p.headshot = None\n        self.check_dimensions(p, None, None, \"mugshot\")\n        self.check_dimensions(p, None, None, \"headshot\") \n", "    def test_dimensions_from_file(self):\n        \"\"\"\n        Dimensions are updated correctly when assigned from a\n        non-ImageFieldFile instance.\n        \"\"\"\n        p = self.PersonModel(name=\"Joe\")\n\n        # Create an ImageFile from the file\n        test_file = self.File(open(os.path.join(os.path.dirname(__file__), \"8x4.png\"), \"rb\"), name=\"8x4.png\")\n\n        # Assign directly from the ImageFile\n        p.mugshot = test_file\n        self.check_dimensions(p, 8, 4, \"mugshot\")\n\n        p.headshot.save(\"head\", test_file)\n        self.check_dimensions(p, 8, 4, \"headshot\")\n"], "sample_829": ["def test_incremental_pca_sparse_fit_transform():\n    # Test fit_transform for sparse matrices\n    X = np.random.rand(1000, 50).astype(np.float32)\n    X_sparse = sparse.csc_matrix(X)\n    ipca = IncrementalPCA(n_components=10, batch_size=25)\n    X_transformed = ipca.fit_transform(X_sparse)\n\n    # Ensure that the transformation results match those obtained using dense arrays\n\n    ipca_dense = IncrementalPCA(n_components=10, batch_size=25)\n    X_dense = X.copy()\n    X_transformed_dense = ipca_dense.fit_transform(X_dense)\n    assert_almost_equal(X_transformed, X_transformed_dense, decimal=3)\n\n\n\n", "def test_incremental_pca_batch_sizes():\n    # Test that the IncrementalPCA behaves meaningfully with different batch sizes.\n    rng = np.random.RandomState(1999)\n    n_samples = 100\n    n_features = 3\n    X = rng.randn(n_samples, n_features)\n    batch_size_list = [1, 5, 10, 20, n_samples]\n    for batch_size in batch_size_list:\n        ipca = IncrementalPCA(n_components=2, batch_size=batch_size).fit(X)\n        # Check that the components are reasonably aligned regardless of batch size\n        prev_components = None\n        for i in range(0, n_samples, batch_size):\n            if prev_components is not None:\n                assert_almost_equal(np.abs(ipca.components_), np.abs(prev_components), \n                                    decimal=2)\n            prev_components = ipca.components_\n\n", "def test_incremental_pca_sparse_transform():\n    # Test that transform works correctly with sparse input\n    rng = np.random.RandomState(1999)\n    n_samples = 100\n    n_features = 50\n    X = rng.rand(n_samples, n_features)\n    X_sparse = sparse.csr_matrix(X)\n\n    ipca = IncrementalPCA(n_components=20).fit(X)\n    ipca_sparse = IncrementalPCA(n_components=20).fit(X_sparse)\n\n    X_dense_transformed = ipca.transform(X)\n    X_sparse_transformed = ipca_sparse.transform(X_sparse)\n\n    assert_allclose_dense_sparse(X_dense_transformed, X_sparse_transformed)\n", "def test_incremental_pca_batch_size_0():\n    # Test that batch size 0 raises ValueError\n    rng = np.random.RandomState(1999)\n    n_samples = 100\n    X = rng.randn(n_samples, 3)\n\n    with pytest.raises(ValueError, match=\"batch_size must be greater than 0\"):\n        IncrementalPCA(n_components=2, batch_size=0).fit(X)\n", "def test_incremental_pca_sparse_input():\n    # Test that IncrementalPCA can handle sparse input correctly\n    from sklearn.datasets import make_sp_random\n    n_samples, n_features = 100, 50\n    X, _ = make_sp_random(n_samples, n_features, density=0.1, random_state=42)\n    ipca = IncrementalPCA(n_components=10, batch_size=10).fit(X)\n    assert ipca.components_.shape[0] == 10\n\n\n\n"], "sample_751": ["compilation error", "def test_warm_start():\n    X, y = datasets.make_classification(n_samples=50, n_features=10,\n                                        random_state=42)\n    for alg in ['SAMME', 'SAMME.R']:\n        clf = AdaBoostClassifier(algorithm=alg, warm_start=False)\n        clf.fit(X, y)\n\n        # Check saving and loading with warm_start=True\n        clf_warm = AdaBoostClassifier(algorithm=alg, warm_start=True)\n        clf_warm.fit(X, y)\n        assert_equal(clf_warm.n_estimators_, clf.n_estimators_)\n        assert_array_equal(clf_warm.estimators_, clf.estimators_)\n        assert_equal(clf_warm.estimator_weights_, clf.estimator_weights_)\n        assert_equal(clf_warm.estimator_errors_, clf.estimator_errors_)\n        \n        # Check adding estimators\n        clf_warm.fit(X, y, sample_weight=None)\n        assert_equal(clf_warm.n_estimators_, clf.n_estimators_ + 1)\n\n\n\n", "def test_sample_weight_adaboost_classifier():\n    \"\"\"\n    AdaBoostClassifier should work without sample_weights in the base estimator\n\n    The random weighted sampling is done internally in the _boost method in\n    AdaBoostClassifier.\n    \"\"\"\n    class DummyEstimator(BaseEstimator):\n\n            pass\n\n            return np.zeros(X.shape[0])\n\n    boost = AdaBoostClassifier(DummyEstimator(), n_estimators=3)\n    boost.fit(X, y_class)\n    assert_equal(len(boost.estimator_weights_), len(boost.estimator_errors_))\n\n", "def test_sample_weight_adaboost_classifier():\n    \"\"\"\n    AdaBoostClassifier should work without sample_weights in the base estimator\n\n    The random weighted sampling is done internally in the _boost method in\n    AdaBoostClassifier.\n    \"\"\"\n    class DummyEstimator(BaseEstimator):\n\n            pass\n\n            return np.zeros(X.shape[0])\n\n    boost = AdaBoostClassifier(DummyEstimator(), n_estimators=3)\n    boost.fit(X, y_class)\n    assert_equal(len(boost.estimator_weights_), len(boost.estimator_errors_))\n", "def test_sparse_sample_weights():\n    # Check classification with sparse sample weights.\n    from sklearn.ensemble import AdaBoostClassifier\n    from sklearn.datasets import make_classification\n    from sklearn.svm import SVC\n    from scipy.sparse import csr_matrix\n\n    X, y = make_classification(n_samples=100, n_features=20, random_state=0)\n    sample_weight = csr_matrix(np.random.rand(100))\n\n    clf = AdaBoostClassifier(base_estimator=SVC(gamma='scale'),\n                             algorithm='SAMME', random_state=0)\n    clf.fit(X, y, sample_weight=sample_weight)\n\n    # Check that sample_weight attribute is set correctly\n    assert isinstance(clf.sample_weight_, csr_matrix)\n\n\n\n\n"], "sample_606": ["def test_diff_axis(use_dask) -> None:\n    if use_dask and not has_dask:\n        pytest.skip(\"requires dask\")\n    da = xr.DataArray([1, 2, 3, 4, 5], dims=\"time\")\n    actual = xr.diff(da, dim=\"time\")\n    expected = xr.DataArray([1, 1, 1, 1], dims=\"time\")\n    assert_identical(expected, actual)\n\n    actual = xr.diff(da, dim=\"time\", prepend=0)\n    expected = xr.DataArray([0, 1, 1, 1, 1], dims=\"time\")\n    assert_identical(expected, actual)\n\n    actual = xr.diff(da, dim=\"time\", append=0)\n    expected = xr.DataArray([1, 1, 1, 1, 0], dims=\"time\")\n    assert_identical(expected, actual)\n\n    with pytest.raises(ValueError, match=r\"Dimension 'time' not found\"):\n        xr.diff(da, dim=\"nonexistent\")\n\n    with pytest.raises(ValueError, match=r\"Invalid value for 'prepend' or 'append'\"):\n        xr.diff(da, dim=\"time\", prepend=\"a\")\n\n    da = xr.DataArray(\n        [[1, 2], [3, 4], [5, 6]], dims=(\"x\", \"y\"), coords={\"x\": [1, 2, 3], \"y\": [0, 1]}\n    )\n    actual = xr.diff(da, dim=\"x\")\n    expected = xr.DataArray(\n        [[1, 2], [2, 4], [2, 4]], dims=(\"x\", \"y\"), coords={\"x\": [1, 2, 3], \"y\": [0, 1]}\n    )\n    assert_identical(expected, actual)\n\n\n\n", "    def test_polyval_multi_dim(use_dask) -> None:\n        if use_dask and not has_dask:\n            pytest.skip(\"requires dask\")\n\n        x = np.arange(5)\n        y = np.arange(5)[:, None]\n\n        da_x = xr.DataArray(x, dims=\"x\")\n        da_y = xr.DataArray(y, dims=(\"y\", \"x\"))\n\n        da = xr.stack((da_x, da_y), dim=\"coords\")\n        coeffs = xr.DataArray(\n            [[2, 1, 1], [3, 2, 1]],\n            dims=(\"d\", \"degree\"),\n            coords={\"d\": [0, 1], \"degree\": [2, 1, 0]},\n        )\n        if use_dask:\n            coeffs = coeffs.chunk({\"d\": 2})\n\n        with pytest.raises(ValueError) as e:\n            xr.polyval(da, coeffs)\n        assert (\n            \"Input DataArray must have a single 'coords' dimension.\" in str(e.value)\n        )\n\n\n\n", "    def test_cross_multi_dim(a, b, ae, be, dim: str, axis: int, use_dask: bool) -> None:\n        expected = np.cross(ae, be, axis=axis)\n    \n        if use_dask and not has_dask:\n            pytest.skip(\"requires dask\")\n        if use_dask:\n            a = a.chunk()\n            b = b.chunk()\n    \n        actual = xr.cross(a, b, dim=dim)\n        xr.testing.assert_duckarray_allclose(expected, actual)\n\n\n\n", "def test_cross_multi_dims() -> None:\n    a = xr.DataArray(\n        np.arange(2 * 3 * 4).reshape((2, 3, 4)),\n        dims=[\"i\", \"j\", \"k\"],\n        coords={\n            \"i\": [\"x\", \"y\"],\n            \"j\": [\"u\", \"v\", \"w\"],\n            \"k\": [\"p\", \"q\", \"r\", \"s\"],\n        },\n    )\n    b = xr.DataArray(\n        np.arange(2 * 3 * 4 * 5).reshape((2, 3, 4, 5)),\n        dims=[\"i\", \"j\", \"k\", \"l\"],\n        coords={\n            \"i\": [\"x\", \"y\"],\n            \"j\": [\"u\", \"v\", \"w\"],\n            \"k\": [\"p\", \"q\", \"r\", \"s\"],\n            \"l\": [\"a\", \"b\", \"c\", \"d\", \"e\"],\n        },\n    )\n    actual = xr.cross(a, b, dim=[\"i\", \"j\"])\n    expected = np.cross(\n        a.isel(i=0, j=\"u\", k=[\"p\", \"q\"]).values,\n        b.isel(i=0, j=\"u\", k=[\"p\", \"q\"], l=[\"a\", \"b\"]).values,\n        axis=1,\n    )\n    xr.testing.assert_allclose(actual.values, expected)\n\n\n\n", "    def test_reduce(use_dask) -> None:\n        if use_dask and not has_dask:\n            pytest.skip(\"requires dask\")\n\n        da = xr.DataArray([1, 2, 3, 4], dims=\"x\")\n        actual = xr.reduce(\"sum\", da, dims=\"x\")\n        expected = 10\n        assert actual.values == expected\n\n        with pytest.raises(ValueError) as e:\n            xr.reduce(\"sum\", da, dims=(\"x\",))\n        assert str(e.value) == \"reduce_kwargs['axis'] must be a single dimension.\"\n\n        actual = xr.reduce(\"sum\", da, axis=0)\n        expected = 10\n        assert actual.values == expected\n\n        actual = xr.reduce(\"mean\", da, dims=\"x\")\n        expected = 2.5\n        assert xr.testing.assert_allclose(actual.values, expected)\n\n        actual = xr.reduce(\"mean\", da, axis=0)\n        expected = 2.5\n        assert xr.testing.assert_allclose(actual.values, expected)\n\n        with pytest.raises(ValueError) as e:\n            xr.reduce(\"max\", da, dims=\"x\")\n        assert str(e.value) == \"reduce_kwargs['func'] must be one of: 'sum', 'mean', 'min', 'max'\"\n\n        if use_dask:\n            da = da.chunk(2)\n            actual = xr.reduce(\"sum\", da, dims=\"x\")\n            expected = 10\n            assert actual.values == expected\n\n\n"], "sample_770": ["def test_silhouette_with_sparse_data():\n    dataset = datasets.load_iris()\n    X_dense = dataset.data\n    X_sparse = csr_matrix(X_dense)\n\n    labels = dataset.target\n\n    # Test with precomputed distances\n    D_dense = pairwise_distances(X_dense, metric='euclidean')\n    D_sparse = pairwise_distances(X_sparse, metric='euclidean')\n\n    score_dense = silhouette_score(D_dense, labels, metric='precomputed')\n    score_sparse = silhouette_score(D_sparse, labels, metric='precomputed')\n\n    pytest.approx(score_dense, score_sparse)\n\n    # Test without calculating distances explicitly\n    score_dense = silhouette_score(X_dense, labels, metric='euclidean')\n    score_sparse = silhouette_score(X_sparse, labels, metric='euclidean')\n\n    pytest.approx(score_dense, score_sparse)\n\n\n", "def test_silhouette_with_sparse_data():\n    X_dense = datasets.load_iris().data\n    X_csr = csr_matrix(X_dense)\n    X_dok = sp.dok_matrix(X_dense)\n    X_lil = sp.lil_matrix(X_dense)\n    y = datasets.load_iris().target\n\n    for X in [X_dense, X_csr, X_dok, X_lil]:\n        D = pairwise_distances(X, metric='euclidean')\n        score_dense = silhouette_score(D, y, metric='precomputed')\n        score_sparse = silhouette_score(X, y, metric='euclidean')\n        pytest.approx(score_dense, score_sparse)\n", "def test_silhouette_with_sparse_matrix():\n    # Tests the Silhouette Coefficient with a sparse matrix representation\n    dataset = datasets.load_iris()\n    X_dense = dataset.data\n    X_csr = csr_matrix(X_dense)\n\n    y = dataset.target\n\n    for X in [X_dense, X_csr]:\n        D = pairwise_distances(X, metric='euclidean')\n        score_precomputed = silhouette_score(D, y, metric='precomputed')\n        assert_greater(score_precomputed, 0)\n\n        score_euclidean = silhouette_score(X, y, metric='euclidean')\n        pytest.approx(score_precomputed, score_euclidean)\n\n", "def test_silhouette_sparse_matrices():\n    from sklearn.datasets import make_blobs\n    from sklearn.datasets import load_iris\n    from scipy.sparse import csr_matrix\n\n    # Test with sparse matrices\n\n    # 1. Generate synthetic data \n    X, y = make_blobs(n_samples=50, centers=3, random_state=0)\n    X_sparse = csr_matrix(X)\n\n    # 2. Calculate silhouette score\n    score_sparse = silhouette_score(X_sparse, y)\n    score_dense = silhouette_score(X, y)\n    pytest.approx(score_sparse, score_dense)\n\n    # 3. Test with real-world dataset\n    iris = load_iris()\n    X_iris_sparse = csr_matrix(iris.data)\n\n    score_sparse_iris = silhouette_score(X_iris_sparse, iris.target)\n    score_dense_iris = silhouette_score(iris.data, iris.target)\n    assert_equal(score_sparse_iris, score_dense_iris)\n\n    \n\n\n", "def test_silhouette_with_sparse_data():\n    X_sparse = sp.csc_matrix(np.random.rand(100, 10))\n    y = np.random.randint(0, 5, size=(100,))\n\n    for metric in ['euclidean', 'precomputed']:\n        score_silhouette = silhouette_score(X_sparse, y, metric=metric)\n        score_silhouette_dense = silhouette_score(\n            X_sparse.toarray(), y, metric=metric\n        )\n        assert_greater(score_silhouette, 0)\n        assert_greater(score_silhouette_dense, 0)\n        pytest.approx(score_silhouette, score_silhouette_dense)\n\n        score_samples = silhouette_samples(X_sparse, y, metric=metric)\n        score_samples_dense = silhouette_samples(\n            X_sparse.toarray(), y, metric=metric\n        )\n        assert_array_equal(score_samples, score_samples_dense)\n"], "sample_805": ["compilation error", "def test_mean_tweedie_deviance_with_nan():\n    y_true = np.array([1, 2, np.nan, 4])\n    y_pred = np.array([1, 2, np.nan, 4])\n\n    with pytest.raises(ValueError, match=\"Target values cannot contain NaN.\"):\n        mean_tweedie_deviance(y_true, y_pred, p=1)\n", "def test_multioutput_regression_with_weights():\n    y_true = [[1, 2], [2.5, -1], [4.5, 3], [5, 7]]\n    y_pred = [[1, 1], [2, -1], [5, 4], [5, 6.5]]\n    sample_weight = np.array([[1, 2], [0.5, 0.5], [1, 0.5], [2, 1]])\n\n    mse = mean_squared_error(y_true, y_pred, multioutput='raw_values',\n                             sample_weight=sample_weight)\n    mae = mean_absolute_error(y_true, y_pred, multioutput='raw_values',\n                              sample_weight=sample_weight)\n    r = r2_score(y_true, y_pred, multioutput='raw_values',\n                  sample_weight=sample_weight)\n    evs = explained_variance_score(y_true, y_pred,\n                                   multioutput='raw_values',\n                                   sample_weight=sample_weight)\n\n    assert_array_almost_equal(mse, [0.39, 0.5625], decimal=2)\n    assert_array_almost_equal(mae, [0.475, 0.625], decimal=2)\n    assert_array_almost_equal(r, [0.95, 0.93], decimal=2)\n    assert_array_almost_equal(evs, [0.95, 0.93], decimal=2)\n\n\n", "def test_regression_multioutput_variance_weights():\n    y_true = [[1, 2], [2.5, -1], [4.5, 3], [5, 7]]\n    y_pred = [[1, 1], [2, -1], [5, 4], [5, 6.5]]\n\n    mse = mean_squared_error(y_true, y_pred,\n                             multioutput='variance_weighted')\n    mae = mean_absolute_error(y_true, y_pred,\n                              multioutput='variance_weighted')\n    r = r2_score(y_true, y_pred, multioutput='variance_weighted')\n    evs = explained_variance_score(y_true, y_pred,\n                                   multioutput='variance_weighted')\n\n    assert_array_almost_equal(mse, [0.125, 0.5625], decimal=2)\n    assert_array_almost_equal(mae, [0.25, 0.625], decimal=2)\n    assert_array_almost_equal(r, [0.95, 0.93], decimal=2)\n    assert_array_almost_equal(evs, [0.95, 0.93], decimal=2)\n\n\n\n", "compilation error"], "sample_1093": ["compilation error", "def test_lambdify_print():\n    from sympy import lambdify\n    from sympy.core import symbols, sin\n    x = symbols('x')\n    f = lambdify(x, sin(x))\n    n = NumPyPrinter()\n    assert n.doprint(f(x)) == 'numpy.sin(x)' \n", "def test_sympy_printer_matrix_expressions():\n    from sympy import MatrixSymbol, Matrix, eye, zeros, ones, transpose\n\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    C = MatrixSymbol('C', 3, 3)\n    D = Matrix( [[1, 2], [3, 4]])\n    E = zeros(2, 2)\n    F = ones(2, 2)\n    \n    prntr = SymPyPrinter()\n    assert prntr.doprint(A.T) == 'A.T'\n    assert prntr.doprint(eye(2)) == 'sympy.Matrix([[1, 0], [0, 1]])'\n    assert prntr.doprint(zeros(2, 3)) == 'sympy.zeros(2, 3)'\n    assert prntr.doprint(ones(2, 3)) == 'sympy.ones(2, 3)'\n    assert prntr.doprint(D + E) == 'D + E'\n    assert prntr.doprint(D * F) == 'D * F'\n    assert prntr.doprint(D @ B) == 'D.dot(B)'\n    assert prntr.doprint(transpose(C)) == 'C.T' \n", "def test_sympy_printing():\n    prntr = SymPyPrinter()\n    expr = 2*x + 3*y**2\n    assert prntr.doprint(expr) == '2*x + 3*y**2'\n    expr = sympy.sqrt((x**2 + y**2))\n    assert prntr.doprint(expr) == 'sympy.sqrt(x**2 + y**2)'\n    expr = Piecewise((1, x < 0), (2, x >= 0))\n    assert prntr.doprint(expr) == '((1) if (x < 0) else (2) if (x >= 0) else None)'\n", "def test_SymPyPrinter_matrices():\n    from sympy import MatrixSymbol, Matrix, zeros, ones, eye, diag,\n    hstack, vstack, det, inv, transpose, kron\n    from sympy.printing.pycode import SymPyPrinter\n    prntr = SymPyPrinter()\n\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    C = Matrix(2, 2, [ [1, 2], [3, 4] ])\n    D = zeros(2, 3)\n    E = ones(3, 2)\n    F = eye(3)\n    G = diag([1, 2, 3])\n\n    assert prntr.doprint(A) == 'MatrixSymbol(\"A\", 2, 2)'\n    assert prntr.doprint(C) == 'Matrix([[1, 2], [3, 4]])'\n    assert prntr.doprint(D) == 'Matrix([[0, 0, 0], [0, 0, 0]])'\n    assert prntr.doprint(E) == 'Matrix([[1, 1, 1], [1, 1, 1], [1, 1, 1]])'\n    assert prntr.doprint(F) == 'Matrix([[1, 0, 0], [0, 1, 0], [0, 0, 1]])'\n    assert prntr.doprint(G) == 'Matrix([[1, 0, 0], [0, 2, 0], [0, 0, 3]])'\n\n    assert prntr.doprint(hstack([A, B])) == 'Matrix([[A[0, 0], A[0, 1], B[0, 0], B[0, 1]], [A[1, 0], A[1, 1], B[1, 0], B[1, 1]]])'\n    assert prntr.doprint(vstack([C, D])) == 'Matrix([[1, 2], [3, 4], [0, 0], [0, 0]])'\n\n    assert prntr.doprint(det(A)) =="], "sample_206": ["    def test_file_storage(self):\n        \"\"\"\n        FileField uses the defined storage.\n        \"\"\"\n        with TemporaryUploadedFile('something.txt', 'text/plain', 0, 'UTF-8') as tmp_file:\n            # Define custom storage\n            class CustomStorage(Storage):\n                    return name\n\n            Document.objects.create(myfile=tmp_file, myfile_storage=CustomStorage())\n\n            # Ensure the file is saved with the custom storage\n            document = Document.objects.get(pk=1)\n            self.assertIsInstance(document.myfile.storage, CustomStorage)\n\n\n", "    def test_upload_to_with_callable(self):\n            return f'generated_{filename}'\n\n        class MyDocument(models.Model):\n            myfile = models.FileField(upload_to=generate_filename)\n\n        with TemporaryUploadedFile('foo.txt', 'text/plain', 1, 'utf-8') as tmp_file:\n            document = MyDocument.objects.create(myfile=tmp_file)\n            self.assertEqual(document.myfile.name, 'generated_foo.txt')\n\n", "    def test_save_with_callable_upload_to(self):\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            with override_settings(MEDIA_ROOT=Path(tmp_dir)):\n                    return f'callable_{instance.id}_{filename}'\n                class MyDocument(Document):\n                    myfile = models.FileField(upload_to=upload_to_callable)\n\n                with TemporaryUploadedFile('foo.txt', 'text/plain', 1, 'utf-8') as tmp_file:\n                    document = MyDocument.objects.create(myfile=tmp_file)\n\n                expected_path = os.path.join(tmp_dir, 'unused', f'callable_{document.id}_foo.txt')\n                self.assertTrue(os.path.exists(expected_path))\n\n\n\n", "    def test_generated_filename(self):\n        field = Document._meta.get_field('myfile')\n        filename = 'original_filename.txt'\n        instance = Document()\n        new_filename = field.generate_filename(instance, filename)\n        self.assertNotEqual(filename, new_filename)\n", "    def test_save_form_data_with_file_object(self):\n        d = Document()\n        with open(__file__, 'rb') as fp:\n            file_obj = File(fp)\n            d.myfile = file_obj\n            field = d._meta.get_field('myfile')\n            field.save_form_data(d, file_obj)\n        self.assertEqual(d.myfile.name, os.path.basename(__file__))\n\n\n\n"], "sample_36": ["def test_biweight_midcovariance_large_arrays():\n    \"\"\"\n    Test biweight_midcovariance with larger arrays\n    to ensure stability and correctness.\n    \"\"\"\n    with NumpyRNGContext(12345):\n        n = 1000\n        d = np.random.normal(size=(n, n))\n        cov = biweight_midcovariance(d)\n        assert cov.shape == (n, n)\n        assert_allclose(cov, cov.T)\n\n        # Test modify_sample_size=True\n        cov_mod = biweight_midcovariance(d, modify_sample_size=True)\n        assert cov_mod.shape == (n, n)\n        assert_allclose(cov_mod, cov_mod.T)\n", "def test_biweight_midcovariance_modify_sample_size_nan():\n    with NumpyRNGContext(12345):\n        data = np.array([1, 2, np.nan, 4, 5])\n        with pytest.raises(ValueError) as e:\n            biweight_midcovariance(data, modify_sample_size=True)\n        assert 'Input contains NaN' in str(e.value)\n", "def test_biweight_midcovariance_modified_n():\n    \"\"\"\n    Test the `modify_sample_size` parameter of `biweight_midcovariance`.\n    \"\"\"\n    rng = np.random.RandomState(12345)\n    n_points = 50\n    x = rng.randn(n_points)\n    y = rng.randn(n_points)\n    \n    # Create a few outliers\n    x[10] = 100.\n    \n    # calculate the midcovariance with and without modifying n\n    cov_full = biweight_midcovariance(np.array([x,y]),\n                                     modify_sample_size=False)\n    cov_modified = biweight_midcovariance(np.array([x,y]),\n                                         modify_sample_size=True)\n    \n    # check that cov_modified is smaller than cov_full\n    assert cov_modified[0, 1] < cov_full[0, 1]\n", "def test_biweight_midcovariance_modify_sample_size_consistency():\n    \"\"\"\n    Test that biweight_midcovariance with modify_sample_size=True\n    produces consistent results for small sample sizes.\n    \"\"\"\n    rng = np.random.RandomState(12345)\n    for i in range(10):\n        n = 10 * (i + 1)\n        data = rng.normal(0, 1, n)\n        # Introduce a few outliers\n        outliers = np.random.uniform(low=-10, high=10, size=n // 10)\n        data[::10] = outliers\n        cov1 = biweight_midcovariance(data, modify_sample_size=True)\n        cov2 = biweight_midcovariance(data, modify_sample_size=False)\n\n        assert_allclose(cov1, cov2, atol=1e-5)\n\n\n", "compilation error"], "sample_989": ["def test_issue_10501():\n    x = Float(sqrt(2), precision=5)\n    assert x.as_mpf_val(precision=5) == (0, long(14142), -18, 18)\n    with raises(ValueError):\n        x.as_mpf_val() \n", "compilation error", "def test_issue_10508():\n    assert Float(1.0).as_mpf_val(50) == (1, 0, 0, 0, 50)\n    assert Float(2.0).as_mpf_val(50) == (2, 0, 0, 0, 50)\n    assert Float(3.14).as_mpf_val(50) == (3, 14, 0, 0, 50)\n    assert Float(1.234567890123456789).as_mpf_val(50) == (1, 234567890123456789, 0, 0, 50) \n\n\n", "def test_issue_10768():\n    x = S(1000000000000000000)\n    assert x.is_integer is True\n    assert type(x) == Integer\n    assert x > 10**12\n    assert x < 10**13\n    assert x.as_mpf() == (0, long(1000000000000000000), 0, 0)\n    assert x.as_tuple() == (0, long(1000000000000000000))\n", "compilation error"], "sample_278": ["    def test_expression_wrapper_with_aggregates(self):\n        with self.subTest(func=Avg):\n            expr = ExpressionWrapper(Avg('a'), output_field=FloatField())\n            self.assertEqual(expr.get_group_by_cols(alias=None), [])\n            self.assertEqual(expr.aggregation, 'avg')\n\n        with self.subTest(func=Count):\n            expr = ExpressionWrapper(Count('a'), output_field=IntegerField())\n            self.assertEqual(expr.get_group_by_cols(alias=None), [])\n            self.assertEqual(expr.aggregation, 'count')\n\n\n", "    def test_resolve_output_field_with_function(self):\n        expr = ExpressionWrapper(\n            Func(\n                'a', function='LOWER',\n            ),\n            output_field=CharField(),\n        )\n        self.assertEqual(expr.output_field.get_internal_type(), 'CharField')\n", "    def test_resolve_output_field_unwrapped(self):\n        expr = ExpressionWrapper(F('cost') + F('tax'), IntegerField())\n        self.assertIsInstance(expr.output_field, IntegerField)\n\n        expr = ExpressionWrapper(F('cost') + F('tax'), None)\n        with self.assertRaises(ValueError):\n            expr.output_field\n\n\n", "    def test_datetime_fields_with_func_expressions(self):\n        expr = ExpressionWrapper(F('start') + Value(datetime.timedelta(days=1)), output_field=DateTimeField())\n        self.assertIsInstance(expr.output_field, DateTimeField)\n", "    def test_output_field_not_provided(self):\n        with self.assertRaises(ValueError):\n            ExpressionWrapper(Value('foo'))\n"], "sample_932": ["def test_param_nesting(app, status, warning):\n    app.builder.build_all()\n\n    test = 'param_nesting.html'\n    output = (app.outdir / test).read_text()\n\n    pattern = r'Nested::f\\(\\s*T\\s*,\\s*const\\s*U\\s*\\)'\n    res = re.search(pattern, output)\n    assert res, f\"Pattern\\n\\t{pattern}\\nnot found in {test}\"\n", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_1050": ["def test_NumPyPrinter_MatrixBase_raises_NotImplementedError():\n    from sympy.matrices import MutableDenseMatrix\n    n = NumPyPrinter()\n    raises(NotImplementedError, lambda: n._print_MatrixBase(MutableDenseMatrix([[1,2],[3,4]])))\n", "def test_NumPyPrinter_complex_arrays():\n    p = NumPyPrinter()\n    a = MatrixSymbol('a', 2, 2)\n    assert p.doprint(a + 1j) == 'numpy.array([[a[0, 0] + 1j, a[0, 1] + 1j], [a[1, 0] + 1j, a[1, 1] + 1j]])'\n    assert p.doprint(a * 2j) == 'numpy.array([[2j * a[0, 0], 2j * a[0, 1]], [2j * a[1, 0], 2j * a[1, 1]]])'\n\n\n\n", "compilation error", "def test_NumPyPrinter_print_MatrixBase():\n    n = NumPyPrinter()\n    A = MatrixSymbol(\"A\", 2, 2)\n    assert n.doprint(A) == 'numpy.array([[None, None], [None, None]])' \n\n\n", "def test_NumPyPrinter_print_matrix_operations():\n    n = NumPyPrinter()\n    A = MatrixSymbol(\"A\", 2, 2)\n    B = MatrixSymbol(\"B\", 2, 2)\n    C = MatrixSymbol(\"C\", 2, 2)\n\n    assert n.doprint(A + B) == \"A + B\"\n    assert n.doprint(A - B) == \"A - B\"\n    assert n.doprint(A * B) == \"A @ B\"\n    assert n.doprint(A / B) == \"A / B\"\n    assert n.doprint(A.transpose()) == \"A.T\"\n    assert n.doprint(A.inv()) == \"numpy.linalg.inv(A)\"\n    assert n.doprint(A.shape) == \"A.shape\"\n    assert n.doprint(A.trace()) == \"numpy.trace(A)\"\n    assert n.doprint(A.determinant()) == \"numpy.linalg.det(A)\"\n"], "sample_436": ["    def test_suggestions_with_arguments(self):\n        args = [\"runserver\", \"9000\", \"--settings=test_project.settings\"]\n        out, err = self.run_django_admin(args)\n        self.assertNoOutput(out)\n        self.assertOutput(err, \"Unknown command: 'runserver'. Did you mean 'runserver'? \") \n", "    def test_invalid_settings_module(self):\n        args = [\"dumpdata\", \"--settings=noSuchFile\"]\n        out, err = self.run_manage(args)\n        self.assertOutput(err, \"Error loading settings module: 'noSuchFile'\")\n\n", "    def test_suggestions_invalid_option(self):\n        args = [\"runserver\", \"--invalid-option\", \"--settings=test_project.settings\"]\n        out, err = self.run_django_admin(args)\n        self.assertNoOutput(out)\n        self.assertOutput(err, \"Invalid option: '--invalid-option'.\")\n        self.assertIn(\n            \"Available options:\", err,\n        ) \n\n\n\n", "    def test_suggestions_with_subcommand(self):\n        args = [\"startapp\", \"abcf\", \"--settings=test_project.settings\"]\n        out, err = self.run_django_admin(args)\n        self.assertNoOutput(out)\n        self.assertOutput(err, \"Unknown command: 'abcf'. Did you mean 'startapp'?\")\n", "    def test_suggestions_with_subcommand(self):\n        args = [\"server\", \"--settings=test_project.settings\"]\n        out, err = self.run_django_admin(args)\n        self.assertNoOutput(out)\n        self.assertOutput(\n            err,\n            \"Unknown command: 'server'. Did you mean 'runserver'? \"\n            \"Type 'python manage.py help' for help on available commands.\",\n        )\n"], "sample_703": ["def test_nested_parens(expr: str, expected: bool) -> None:\n    matcher = {\"true\": True, \"false\": False}.__getitem__\n    assert evaluate(expr, matcher) is expected\n", "def test_ident_with_booleans(expr: str, expected: bool) -> None:\n    matcher = {\"ident\": True}\n    assert evaluate(expr, matcher) is expected\n", "def test_whitespace_surrounding_operators(expr: str, expected: bool) -> None:\n    matcher = {\"true\": True, \"false\": False}.__getitem__\n    assert evaluate(expr, matcher) is expected\n", "    def test_nested_expressions(expr: str, matcher: dict, expected: bool) -> None:\n        assert evaluate(expr, matcher.__getitem__) == expected\n", "def test_parentheses_precedence(expr: str, matcher: Callable, expected: bool) -> None:\n    assert evaluate(expr, matcher) is expected\n"], "sample_1189": ["    def test_lambdify_with_matrices():\n        if not numpy:\n            skip(\"numpy not installed\")\n\n        A = MatrixSymbol('A', 2, 2)\n        B = MatrixSymbol('B', 2, 2)\n        f = lambdify(A, A.T * A)\n        assert numpy.allclose(f(numpy.array([[1, 2], [3, 4]])),\n                             numpy.array([[1, 2, 3, 4]]))\n\n        g = lambdify((A, B), A + B)\n        assert numpy.allclose(g(numpy.array([[1, 2], [3, 4]]),\n                                numpy.array([[5, 6], [7, 8]])),\n                             numpy.array([[6, 8], [10, 12]]))\n", "compilation error", "def test_issue_16176():\n    if not cupy:\n        skip(\"CuPy not installed\")\n\n    f = lambdify((x,), x**2, modules='cupy')\n    assert f(cupy.array([1, 2, 3])) == cupy.array([1, 4, 9])\n\n", "def test_issue_22841():\n    if not scipy:\n        skip(\"scipy not installed\")\n\n    x = symbols('x')\n    f = lambdify(x, exp(x)**2, modules='scipy')\n    assert abs(f(1) - exp(exp(1)).evalf()) < 1e-10\n", "def test_issue_23402():\n    if not scipy:\n        skip(\"scipy not installed\")\n\n    x = Symbol('x')\n    y = Symbol('y')\n    f = lambdify((x, y), erfc(x, y), modules='scipy')\n    assert abs(erfc(1, 2) - f(1, 2)) < 1e-10\n\n\n"], "sample_323": ["    def test_minimize_rollbacks_circular_dependency(self):\n        \"\"\"\n        Minimize rollbacks in presence of circular dependencies.\n\n        a: 1 <--- 2\n        b:    \\- 1\n        c:  \\  /- 2 <--- 3\n           \\ /\n           4\n\n        If a1 and b1 are applied, and c1 has a dependency on both, and a2 has a\n        dependency on c1, ensure that, after migrating to a1, we don't rollback\n        b1, even though b1 is a dependency on c1, and c1's rollback would\n        require rolling back b1.\n        \"\"\"\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        a2_impl = FakeMigration('a2')\n        a2 = ('a', '2')\n        b1_impl = FakeMigration('b1')\n        b1 = ('b', '1')\n        c1_impl = FakeMigration('c1')\n        c1 = ('c', '1')\n        c2_impl = FakeMigration('c2')\n        c2 = ('c', '2')\n        c3_impl = FakeMigration('c3')\n        c3 = ('c', '3')\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(a2, a2_impl)\n        graph.add_node(b1, b1_impl)\n        graph.add_node(c1, c1_impl)\n        graph.add_node(c2, c2_impl)\n        graph.add_node(c3, c3_impl)\n        graph.add_dependency(None, a2, c1)\n        graph.add_dependency(None, b1, a1)\n        graph.add_dependency(None, c1, a1)\n        graph.add_dependency(None, c2, c1)\n        graph.add_dependency(None, c3, c2)\n\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {\n            a1: a1_impl,\n            b", "    def test_minimize_rollbacks_circular(self):\n        \"\"\"\n        Minimize rollbacks when target has circular dependencies.\n\n        a: 1 <--- 2\n        b:     \\<--- 3\n           \\- 1\n\n        \"\"\"\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        a2_impl = FakeMigration('a2')\n        a2 = ('a', '2')\n        b1_impl = FakeMigration('b1')\n        b1 = ('b', '1')\n        b2_impl = FakeMigration('b2')\n        b2 = ('b', '2')\n        c1_impl = FakeMigration('c1')\n        c1 = ('c', '1')\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(a2, a2_impl)\n        graph.add_node(b1, b1_impl)\n        graph.add_node(b2, b2_impl)\n        graph.add_node(c1, c1_impl)\n        graph.add_dependency(None, a2, a1)\n        graph.add_dependency(None, b2, a2)\n        graph.add_dependency(None, b1, a1)\n        graph.add_dependency(None, c1, b1)\n\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {\n            a1: a1_impl,\n            b1: b1_impl,\n            c1: c1_impl,\n        })\n\n        plan = executor.migration_plan({a1})\n\n        self.assertEqual(plan, [(a2_impl, True)])\n", "    def test_minimize_rollbacks_cycle(self):\n        \"\"\"\n        Minimize rollbacks when migrations form a cycle.\n\n        a: 1 <--- 2\n        b:    \\- 1\n\n        If a1 and b1 are applied, and we're asked to migrate to a1, don't\n        unapply b1.\n        \"\"\"\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        a2_impl = FakeMigration('a2')\n        a2 = ('a', '2')\n        b1_impl = FakeMigration('b1')\n        b1 = ('b', '1')\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(a2, a2_impl)\n        graph.add_node(b1, b1_impl)\n        graph.add_dependency(None, a2, a1)\n        graph.add_dependency(None, b1, a1)\n\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {\n            a1: a1_impl,\n            b1: b1_impl,\n            a2: a2_impl,\n        })\n\n        plan = executor.migration_plan({a1})\n\n        self.assertEqual(plan, [])\n\n\n", "    def test_minimize_rollbacks_circular_dependencies(self):\n        \"\"\"\n        Minimize rollbacks when there are circular dependencies.\n\n        a: 1 <---- 2\n           \\     ^\n            \\    /\n             \\- 3 --\n        b:      \\- 4\n        \"\"\"\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        a2_impl = FakeMigration('a2')\n        a2 = ('a', '2')\n        a3_impl = FakeMigration('a3')\n        a3 = ('a', '3')\n        b1_impl = FakeMigration('b1')\n        b1 = ('b', '1')\n        b2_impl = FakeMigration('b2')\n        b2 = ('b', '2')\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(a2, a2_impl)\n        graph.add_node(a3, a3_impl)\n        graph.add_node(b1, b1_impl)\n        graph.add_node(b2, b2_impl)\n        graph.add_dependency(None, a2, a1)\n        graph.add_dependency(None, a3, a2)\n        graph.add_dependency(None, b1, a3)\n        graph.add_dependency(None, b2, a1)\n\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {\n            a1: a1_impl,\n            b1: b1_impl,\n            a2: a2_impl,\n            b2: b2_impl,\n            a3: a3_impl,\n        })\n\n        plan = executor.migration_plan({a1})\n\n        # In this scenario, we should only unapply b2 since\n        # it depends on a1 and is in a circular dependency.\n        should_be_rolled_back = [b2_impl]\n        exp = [(m, True) for m in should_be_rolled_back]\n        self.assertEqual(plan, exp) \n", "    def test_minimize_rollbacks_circular(self):\n        \"\"\"Minimize rollbacks when there's a circular dependency.\"\"\"\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        a2_impl = FakeMigration('a2')\n        a2 = ('a', '2')\n        b1_impl = FakeMigration('b1')\n        b1 = ('b', '1')\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(a2, a2_impl)\n        graph.add_node(b1, b1_impl)\n        graph.add_dependency(None, a2, a1)\n        graph.add_dependency(None, a1, b1)\n        graph.add_dependency(None, b1, a2)\n\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {\n            a1: a1_impl,\n            b1: b1_impl,\n            a2: a2_impl,\n        })\n\n        plan = executor.migration_plan({a1})\n\n        # In a circular dependency, we should minimize rollbacks to the point\n        # where the dependency cycle is broken. \n        self.assertEqual(plan, [(b1_impl, True), (a2_impl, True)])\n\n"], "sample_421": ["    def test_when_order_matters(self):\n        # Test that the order of When clauses matters.\n        class Person(models.Model):\n            name = models.CharField(max_length=100)\n            age = models.IntegerField()\n            country = models.CharField(max_length=100)\n\n        Person.objects.create(name=\"Alice\", age=25, country=\"USA\")\n        Person.objects.create(name=\"Bob\", age=30, country=\"Canada\")\n        Person.objects.create(name=\"Charlie\", age=20, country=\"USA\")\n\n            if discount_type == \"high\":\n                return 10\n            elif discount_type == \"medium\":\n                return 5\n            else:\n                return 0\n\n        queryset = Person.objects.annotate(\n            discount=Case(\n                When(age__gte=30, then=Value(\"high\")),\n                When(age__gte=25, then=Value(\"medium\")),\n                default=Value(\"low\"),\n            )\n        )\n        self.assertQuerysetEqual(\n            queryset,\n            [\n                (\"Bob\", \"high\"),\n                (\"Alice\", \"medium\"),\n                (\"Charlie\", \"low\"),\n            ],\n            transform=attrgetter(\"name\", \"discount\"),\n        )\n\n\n\n", "    def test_invalid_when_condition_in_then(self):\n        with self.assertRaisesMessage(TypeError, \"Only Values, Field objects, or expressions are allowed in the 'then' argument\"):\n            Case(When(Q(pk__gt=0), then=object())\n", "    def test_order_by_when_expression(self):\n        self.assertQuerysetEqual(\n            Client.objects.annotate(\n                priority=Case(\n                    When(account_type=Client.PLATINUM, then=1),\n                    When(account_type=Client.GOLD, then=2),\n                    default=3,\n                )\n            ).order_by(\"priority\"),\n            [\n                (\"Jack Black\", 1),\n                (\"James Smith\", 2),\n                (\"Jane Doe\", 3),\n            ],\n            transform=attrgetter(\"name\", \"priority\"),\n        )\n", "    def test_when_with_f_expression(self):\n        with self.subTest(\"Valid usage\"):\n            self.assertQuerysetEqual(\n                Order.objects.annotate(\n                    price=Case(\n                        When(\n                            F(\"quantity\") * F(\"price_per_unit\") > 100,\n                            then=Value(\"High\"),\n                        )\n                    )\n                ).order_by(\"price\"),\n                [(\"High\",), (\"Low\",)],\n                transform=attrgetter(\"price\"),\n            )\n\n        with self.subTest(\"Error handling\"):\n            with self.assertRaises(ValueError):\n                Case(When(F(\"quantity\") * F(\"price_per_unit\") > 100, then=F(\"price_per_unit\")))\n", "    def test_complex_lookup_with_f_expressions(self):\n        with self.settings(INSTALLED_APPS={'django.contrib.postgres': 'apps.postgres_setup'}):\n            Client = models.Model.objects.get(pk=1)\n            # In postgres\n            self.assertQuerysetEqual(\n                Client.objects.annotate(\n                    discount=Case(\n                        When(\n                            F(\"registered_on\") < date(\"now\", \"-1 year\"),\n                            then=Value(10),\n                        ),\n                        When(\n                            F(\"registered_on\") < date(\"now\", \"-3 month\"),\n                            then=Value(5),\n                        ),\n                        default=Value(0),\n                    ),\n                ).order_by('id'),\n                [\n                    (\"Jane Doe\", 5),\n                    (\"James Smith\", 0),\n                    (\"Jack Black\", 10),\n                ],\n                transform=attrgetter(\"name\", \"discount\"),\n            )\n\n\n\n"], "sample_110": ["    def test_pickle_exists_queryset_with_subquery(self):\n        group = Group.objects.create(name='group')\n        Event.objects.create(title='event', group=group)\n        groups = Group.objects.annotate(has_event=models.Exists(\n            Event.objects.filter(\n                group_id=models.OuterRef('id'),\n            ).values('id')\n        ))\n        self.assert_pickles(groups)\n", "    def test_in_lookup_with_subquery(self):\n        events = Event.objects.filter(group__in=Event.objects.values('group_id').annotate(name=models.Value('foo')).query)\n        with self.assertNumQueries(0):\n            dumped = pickle.dumps(events.query)\n\n        with self.assertNumQueries(0):\n            reloaded = pickle.loads(dumped)\n            reloaded_events = Event.objects.none()\n            reloaded_events.query = reloaded\n\n        self.assertSequenceEqual(reloaded_events, [self.e1])\n", "    def test_in_lookup_with_related_field_evaluation(self):\n        \"\"\"\n        The `in` lookup on a related field should not evaluate the related queryset\n        when pickling and unpickling.\n        \"\"\"\n        qs1 = Group.objects.filter(name='Group 1')\n        qs2 = Event.objects.filter(group__in=qs1)\n\n        with self.assertNumQueries(1):  # Only one query for `qs1`\n            dumped = pickle.dumps(qs2.query)\n\n        with self.assertNumQueries(0):  # No queries when unpickling\n            reloaded = pickle.loads(dumped)\n            reloaded_events = Event.objects.none()\n            reloaded_events.query = reloaded\n\n        self.assertSequenceEqual(reloaded_events, [self.e1])\n\n\n\n", "    def test_in_lookup_queryset_with_order_by(self):\n        events = Event.objects.order_by('title').filter(group__in=Group.objects.all())\n\n        with self.assertNumQueries(0):\n            dumped = pickle.dumps(events.query)\n\n        with self.assertNumQueries(0):\n            reloaded = pickle.loads(dumped)\n            reloaded_events = Event.objects.none()\n            reloaded_events.query = reloaded\n\n        self.assertSequenceEqual(reloaded_events, [self.e1])\n", "    def test_pickle_query_with_subquery(self):\n        g1 = Group.objects.create(name='group1')\n        g2 = Group.objects.create(name='group2')\n        events = Event.objects.filter(group__in=Group.objects.filter(name__startswith='g').values('id'))\n        self.assert_pickles(events)\n"], "sample_445": ["    def test_different_timezones_naive_datetime(self):\n        naive_t = datetime.datetime(2023, 1, 1)\n        aware_t = timezone.make_aware(naive_t, timezone.get_default_timezone())\n        self.assertEqual(timesince(naive_t), \"0\\xa0minutes\")\n        self.assertEqual(timesince(aware_t), \"0\\xa0minutes\")\n        self.assertEqual(timeuntil(naive_t), \"0\\xa0minutes\")\n        self.assertEqual(timeuntil(aware_t), \"0\\xa0minutes\")\n", "    def test_future_date_and_time(self):\n        future_datetime = datetime.datetime(2024, 1, 1, 10, 30, 0)\n        future_datetime = timezone.make_aware(future_datetime, timezone.get_default_timezone())\n        self.assertEqual(timesince(self.t, future_datetime), \"3\\xa0years, 2\\xa0months\")\n        self.assertEqual(timeuntil(future_datetime, self.t), \"3\\xa0years, 2\\xa0months\")\n", "    def test_timeuntil_aware_datetime(self):\n        t = timezone.make_aware(self.t, timezone.get_default_timezone())\n        self.assertEqual(timeuntil(t, self.t), \"0\\xa0minutes\")\n\n\n", "    def test_timeuntil_with_datetime_strings(self):\n        time_strings = {\n            \"minute\": npgettext_lazy(\n                \"naturaltime-past\",\n                \"%(num)d minute\",\n                \"%(num)d minutes\",\n                \"num\",\n            ),\n        }\n        with translation.override(\"cs\"):\n            for now in [self.t, self.t + self.onemicrosecond, self.t + self.oneday]:\n                with self.subTest(now):\n                    self.assertEqual(timeuntil(now, self.t, time_strings=time_strings), \"0\\xa0minut\") \n", "    def test_non_integer_depth(self):\n        with self.assertRaises(ValueError):\n            timesince(self.t, self.t + self.oneminute, depth=1.5)\n"], "sample_842": ["def test_kernel_with_invalid_input_shape(kernel):\n    # Check that kernels raise ValueError for invalid input shapes.\n\n    # Test with a single data point\n    with pytest.raises(ValueError):\n        kernel.fit(X[:1])\n\n    # Test with incompatible shapes\n    with pytest.raises(ValueError):\n        kernel(X, Y[:X.shape[0]//2])\n\n\n\n\n", "def test_kernel_repr(kernel):\n    # Smoke-test for repr in kernels.\n    repr(kernel)\n", "def test_kernel_with_invalid_input(kernel):\n    # Check for TypeError when passing invalid input to kernel function\n\n    with pytest.raises(TypeError):\n        kernel(X, Y=np.array([1, 2, 3]))  # Y should be None or array-like\n", "def test_kernel_multiplication(kernel_1, kernel_2):\n    # Check multiplication of two kernels is commutative.\n    K1 = kernel_1 * kernel_2\n    K2 = kernel_2 * kernel_1\n    assert_almost_equal(K1(X), K2(X)) \n", "def test_kernel_bounds(kernel):\n    # Check that bounds are respected when setting parameters.\n\n    for hyperparameter in kernel.hyperparameters:\n        bounds = kernel.get_bounds(hyperparameter.name)\n        if bounds == \"fixed\":\n            continue\n        key = hyperparameter.name\n\n        # Test lower bound\n        kernel.set_params(**{key: bounds[0] - 1e-6})\n        # Kernel should raise an error if the new value is outside\n        # the bounds\n\n        with pytest.raises(ValueError, match=f\"Parameter '{key}' is out of bounds.\"):\n            kernel.set_params(**{key: bounds[0] - 1e-6})\n\n        # Test upper bound\n        kernel.set_params(**{key: bounds[1] + 1e-6})\n        # Kernel should raise an error if the new value is outside\n        # the bounds\n\n        with pytest.raises(ValueError, match=f\"Parameter '{key}' is out of bounds.\"):\n            kernel.set_params(**{key: bounds[1] + 1e-6})\n\n"], "sample_1205": ["def test_PolyElement_monic_factor_list():\n    R, x = ring(\"x\", ZZ)\n\n    f = 2*x**4 - 8*x**3 + 12*x**2 - 8*x + 2\n    g = x**2 - 2*x + 1\n    \n    assert f.monic_factor_list() == (1*2, [(g, 2)])\n    \n    f = 2*x**3 - 6*x**2 + 6*x - 2\n    g = 2*x - 2\n    \n    assert f.monic_factor_list() == (1, [(g, 2)]) \n", "compilation error", "compilation error", "def test_PolyElement_monic_transform():\n    R, x = ring(\"x\", ZZ)\n\n    f = 2*x**3 + 4*x**2 + 2*x\n\n    q = f.monic_transform()\n    assert q == x**3 + 2*x**2 + 1 and f.is_monic() == False \n", "compilation error"], "sample_224": ["    def test_aggregation_subquery_annotation_with_ordering(self):\n        books_qs = Book.objects.annotate(\n            related_author_name=Subquery(\n                Author.objects.filter(\n                    pk=OuterRef('contact'),\n                ).order_by('name')[0:1],\n            )\n        ).order_by(F('related_author_name'))\n        self.assertSequenceEqual(\n            list(books_qs), [\n                {'pk': self.b1.pk, 'related_author_name': 'Adrian Holovaty'},\n                {'pk': self.b2.pk, 'related_author_name': 'Adrian Holovaty'},\n                {'pk': self.b3.pk, 'related_author_name': 'Brad Dayley'},\n                # ... and so on\n            ]\n        )\n\n\n\n", "    def test_aggregation_subquery_annotation_with_filter(self):\n        \"\"\"\n        Subquery annotations can be used within the filter clause.\n        \"\"\"\n        subquery_qs = Author.objects.filter(\n            pk=OuterRef('pk'),\n            age__gt=25,\n        ).values('pk')\n        author_qs = Author.objects.annotate(\n            subquery_id=Subquery(subquery_qs),\n        ).filter(subquery_id__exists=True).annotate(count=Count('book'))\n        self.assertEqual(author_qs.count(), 3)\n        self.assertEqual(author_qs.first().count, 3)", "    def test_aggregation_subquery_annotation_with_ordering(self):\n        books_qs = Book.objects.annotate(\n            min_related_age=Min('authors__age'),\n        ).annotate(\n            sorted_authors=Subquery(\n                Author.objects.filter(\n                    pk=OuterRef('authors__pk')\n                ).order_by('-age').values('name', 'age')[:1]\n            ),\n        ).filter(\n            sorted_authors__age__gt=25\n        ).annotate(count=Count('authors')).order_by('sorted_authors__name')\n        self.assertEqual(list(books_qs), [\n            {'count': 2, 'sorted_authors': [{'name': 'Jeffrey Forcier', 'age': 31}, {'min_related_age': 31}, {'min_related_age': 31}]},\n            {'count': 2, 'sorted_authors': [{'name': 'Brad Dayley', 'age': 29}, {'min_related_age': 29}, {'min_related_age': 29}]},\n            {'count': 2, 'sorted_authors': [{'name': 'Jacob Kaplan-Moss', 'age': 32}, {'min_related_age': 32}, {'min_related_age': 32}]},\n            {'count': 1, 'sorted_authors': [{'name': 'Peter Norvig', 'age': 63}, {'min_related_age': 63}, {'min_related_age': 63}]},\n            {'count': 1, 'sorted_authors': [{'name': 'Wesley J. Chun', 'age': 37}, {'min_related_age': 37}, {'min_related_age': 37}]},\n        ])\n\n\n", "    def test_aggregation_with_outer_join(self):\n        class MyCount(Count):\n                return super().__new__(cls, *args, **kwargs)\n\n                if connection.vendor == 'sqlite':\n                    return 'COUNT(1)'\n                return super().as_sql(compiler, connection)\n\n        books_with_authors = Book.objects.left_outerjoin(\n            'authors',\n        ).annotate(\n            books_count=MyCount('id')\n        ).values('id', 'name', 'books_count')\n        self.assertEqual(\n            list(books_with_authors), [\n                {\n                    'id': self.b1.id,\n                    'name': 'Django 1.9 - The Definitive Guide',\n                    'books_count': 1,\n                },\n                {\n                    'id': self.b2.id,\n                    'name': 'The Django Book',\n                    'books_count': 1,\n                },\n                {\n                    'id': self.b3.id,\n                    'name': 'Django for Beginners',\n                    'books_count': 1,\n                },\n                {\n                    'id': self.b4.id,\n                    'name': 'Practical Django Projects',\n                    'books_count': 1,\n                },\n                {\n                    'id': self.b5.id,\n                    'name': 'The Definitive Guide to Django: Web Development Done Right',\n                    'books_count': 1,\n                },\n                {\n                    'id': self.b6.id,\n                    'name': 'The Django Cookbook',\n                    'books_count': 1,\n                },\n            ]\n        )\n\n\n\n", "    def test_complex_subquery_annotation_with_expression_field(self):\n        class MySum(Sum):\n            output_field = DecimalField()\n\n            substitutions = {'function': 'SUM', 'expressions': '2', 'distinct': ''}\n            substitutions.update(self.extra)\n            return self.template % substitutions, ()\n        setattr(MySum, 'as_' + connection.vendor, be_evil)\n\n        latest_book_pubdate_qs = Book.objects.filter(\n            publisher=OuterRef('pk')\n        ).order_by('-pubdate').values('pubdate')[:1]\n        publisher_qs = Publisher.objects.annotate(\n            latest_book_pubdate=Subquery(\n                latest_book_pubdate_qs,\n                MySum(F('pubdate') + F('id'))\n            ),\n            count=Count('book')\n        ).values('latest_book_pubdate', 'count')\n        self.assertCountEqual(list(publisher_qs), [\n            {'latest_book_pubdate': Decimal('2'), 'count': 3},\n        ])\n\n"], "sample_184": ["    def test_unique_constraint_include_with_condition(self):\n        class Model(models.Model):\n            age = models.IntegerField()\n            parent = models.ForeignKey('self', models.CASCADE)\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(\n                        fields=['age'],\n                        include=['parent__age'],\n                        name='unique_age_include_parent',\n                        condition=models.Q(age__gte=100),\n                    ),\n                ]\n\n        self.assertEqual(Model.check(databases=self.databases), [])\n", "    def test_unique_constraint_include_pointing_to_joined_fields(self):\n        class Model(models.Model):\n            name = models.CharField(max_length=10)\n            field1 = models.PositiveSmallIntegerField()\n            field2 = models.PositiveSmallIntegerField()\n            parent = models.ForeignKey('self', models.CASCADE)\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(\n                        fields=['name'],\n                        include=['parent__field1', 'parent__field2'],\n                        name='name',\n                    ),\n                ]\n\n        errors = Model.check(databases=self.databases)\n        expected_errors = [\n            Error(\n                \"'constraints' refers to the joined field '%s'.\" % field_name,\n                obj=Model,\n                id='models.E041',\n            ) for field_name in ['parent__field1', 'parent__field2']\n        ]\n        self.assertCountEqual(errors, expected_errors)\n\n\n\n", "    def test_unique_constraint_include_with_condition(self):\n        class Model(models.Model):\n            age = models.IntegerField()\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(\n                        fields=['age'],\n                        name='unique_age_include_condition',\n                        include=['id'],\n                        condition=models.Q(age__gte=100),\n                    ),\n                ]\n\n        errors = Model.check(databases=self.databases)\n        expected = [] if connection.features.supports_partial_indexes else [\n            Warning(\n                '%s does not support unique constraints with conditions and '\n                'non-key columns.'\n                % connection.display_name,\n                hint=(\n                    \"A constraint won't be created. Silence this warning if \"\n                    \"you don't care about it.\"\n                ),\n                obj=Model,\n                id='models.W040',\n            ),\n        ]\n        self.assertEqual(errors, expected) \n", "    def test_check_unique_constraint_include_invalid_field_combination(self):\n        class Model(models.Model):\n            name = models.CharField(max_length=100)\n            age = models.IntegerField()\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(\n                        fields=['name'],\n                        include=['age'],\n                        name='name_age_unique',\n                    ),\n                ]\n\n        self.assertEqual(Model.check(databases=self.databases), [\n            Error(\n                \"'constraints' refers to a field 'age' that is not included in \"\n                \"the specified unique fields 'name'.\",\n                obj=Model,\n                id='models.E040',\n            ),\n        ])\n", "    def test_unique_constraint_include_with_condition(self):\n        class Model(models.Model):\n            age = models.IntegerField()\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(\n                        fields=['age'],\n                        name='unique_age_include_condition',\n                        include=['id'],\n                        condition=models.Q(age__gte=100),\n                    ),\n                ]\n\n        errors = Model.check(databases=self.databases)\n        expected = [] if connection.features.supports_partial_indexes else [\n            Warning(\n                '%s does not support unique constraints with conditions.'\n                % connection.display_name,\n                hint=(\n                    \"A constraint won't be created. Silence this warning if \"\n                    \"you don't care about it.\"\n                ),\n                obj=Model,\n                id='models.W036',\n            ),\n        ]\n        self.assertEqual(errors, expected)\n"], "sample_538": ["compilation error", "compilation error", "def test_transform_with_non_numpy_array():\n    t = mtransforms.Affine2D()\n    with pytest.raises(TypeError):\n        t.transform((1, 1, 1))\n", "def test_transform_path_non_affine_path_equality():\n    # tests that a transform path with a non affine transform \n    # can correctly be compared for equality with its original Path \n\n    points = np.array([[0, 0], [1, 0], [1, 1], [0, 1]], dtype=np.float64)\n    path = Path(points, closed=True)\n\n    tn1 = NonAffineForTest(mtransforms.Affine2D().translate(1, 1))\n    tpath = tn1.transform_path_non_affine(path)\n\n    # check that using == operator returns False with different paths\n    assert not (path == tpath)  \n    assert not (tpath == path)\n", "def test_rotation_in_polar(fig_test, fig_ref):\n    np.random.seed(19680801)\n    samples = np.random.uniform(0, 2 * np.pi, 100)\n    radii = np.random.uniform(0.1, 1, 100)\n\n    for fig, log_state in zip([fig_test, fig_ref], [True, False]):\n        ax = fig.subplots(projection=\"polar\")\n        ax.hist(samples, radii, bins=20, log=log_state, density=True)\n        ax.set_theta_zero_location(\"N\")\n        ax.set_theta_direction(-1)\n        fig.canvas.draw()\n        ax.set_xticks(np.linspace(0, 2 * np.pi, 6))\n        ax.set_xticks(np.linspace(0, 2 * np.pi, 6), minor=True)\n        ax.set_yticks(np.linspace(0.1, 1, 5))\n    \n\n"], "sample_309": ["    def test_already_escaped(self):\n        tests = (\n            ('/%2Fexample.com', '/%2Fexample.com'),\n            ('/http://example.com', '/http://example.com'),\n        )\n        for url, expected in tests:\n            with self.subTest(url=url):\n                self.assertEqual(escape_leading_slashes(url), expected)\n", "    def test_no_change(self):\n        tests = (\n            ('/path/to/example.com', '/path/to/example.com'),\n            ('http://example.com', 'http://example.com'),\n            ('https://example.com', 'https://example.com'),\n            ('relative/path', 'relative/path'),\n        )\n        for url, expected in tests:\n            with self.subTest(url=url):\n                self.assertEqual(escape_leading_slashes(url), expected) \n", "    def test_empty_string(self):\n        self.assertEqual(escape_leading_slashes(''), '')\n\n\n\n", "    def test_url_with_fragment(self):\n        tests = (\n            ('//example.com#section', '/%2Fexample.com%23section'),\n            ('//example.com/#section', '/%2Fexample.com%23section'),\n        )\n        for url, expected in tests:\n            with self.subTest(url=url):\n                self.assertEqual(escape_leading_slashes(url), expected) \n", "    def test_absolute_urls(self):\n        tests = (\n            ('http://example.com', 'http://example.com'),\n            ('https://example.com', 'https://example.com'),\n        )\n        for url, expected in tests:\n            with self.subTest(url=url):\n                self.assertEqual(escape_leading_slashes(url), expected)\n\n"], "sample_213": ["    def test_file_like_object_close(self):\n        \"\"\"Test closing a File object before it's written to disk.\"\"\"\n        file_like_object = StringIO(\"content\")\n        f = File(file_like_object)\n        with self.storage.open('test_file', 'wb') as destination:\n            f.seek(0)\n            destination.write(f.read())\n        os.remove(self.storage.path('test_file'))\n        self.assertFalse(os.path.exists(self.storage.path('test_file')))\n\n\n\n\n", "    def test_streaming_file_upload(self):\n        \"\"\"\n        Test uploading a large file by reading chunks from a tempfile.\n        \"\"\"\n        with tempfile.NamedTemporaryFile() as temp_file:\n            temp_file.write(b'A' * 1024 * 1024)  # Write 1MB of data\n            temp_file.seek(0)\n            stored_filename = self.storage.save(\"large_file.txt\", temp_file)\n            with self.storage.open(stored_filename) as stored_file:\n                self.assertEqual(stored_file.read(), b'A' * 1024 * 1024)\n", "    def test_content_file_chunk_size(self):\n        \"\"\"\n        Test that ContentFile's chunk_size is respected.\n        \"\"\"\n        temp_file = tempfile.NamedTemporaryFile(delete=False)\n        with open(temp_file.name, 'wb') as f:\n            f.write(b\"ABCD\" * 100)\n\n        temp_file_content = ContentFile(open(temp_file.name, 'rb'))\n        chunks = list(temp_file_content.chunks(chunk_size=10))\n\n        self.assertEqual(len(chunks), 10)\n        self.assertEqual(len(b''.join(chunks)), 400)  # 100 chunks * 4 bytes\n        os.remove(temp_file.name)\n\n\n\n", "    def test_file_like_object_from_file_object(self):\n        \"\"\"\n        Test the File storage API with a file-like object coming from a\n        regular file object.\n        \"\"\"\n        with open(self.temp_dir + '/local_file.txt', 'w') as f:\n            f.write(\"This is a local file\")\n\n        with open(self.temp_dir + '/local_file.txt', 'r') as local_file:\n            file_like_object = local_file\n            f = File(file_like_object)\n            stored_filename = self.storage.save(\"local_file.txt\", f)\n\n        with open(self.temp_dir + '/local_file.txt', 'r') as local_file:\n            content = local_file.read()\n\n        with self.storage.open(stored_filename) as stored_file:\n            self.assertEqual(stored_file.read(), content)\n\n\n\n", "    def test_stringio(self):\n        # Test passing StringIO instance as content argument to save\n        output = StringIO()\n        output.write('content')\n        output.seek(0)\n\n        # Save it and read written file\n        self.storage.save('tests/stringio', output)\n        self.assertTrue(self.storage.exists('tests/stringio'))\n        with self.storage.open('tests/stringio') as f:\n            self.assertEqual(f.read(), b'content')\n"], "sample_955": ["    def test_unparse_assignments(source, expected):\n        module = ast.parse(source)\n        assert ast.unparse(module.body[0].value, source) == expected\n", "    def test_unparse_keywords():\n        source = \"def func(a, *, b=1, c): pass\"\n        module = ast.parse(source)\n        args = module.body[0].args\n        assert ast.unparse(args, source) == 'a, *, b=1, c'\n", "    def test_unparse_slice(source, expected):\n        module = ast.parse(source)\n        assert ast.unparse(module.body[0].value, source) == expected\n", "    def test_unparse_slice(source, expected):\n        module = ast.parse(source)\n        assert ast.unparse(module.body[0].value, source) == expected\n", "    def test_unparse_slice(source, expected):\n        module = ast.parse(source)\n        assert ast.unparse(module.body[0].value, source) == expected \n"], "sample_942": ["def test_py_param_type_ref(app):\n    text = (\".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   :param str name: blah blah\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                [desc_name, \"Class\"])],\n                                  [desc_content, (addnodes.index,\n                                [desc, addnodes.index, desc,\n                addnodes.field_list, nodes.field]))]))\n    assert_node(doctree[1][1][0][1][0],\n                ([nodes.field_name, \"Parameters\"],\n                 [nodes.field_body, nodes.paragraph]))\n    \n    assert_node(doctree[1][1][0][1][0][1][0],\n                ([addnodes.literal_strong, \"name\"],\n                 \" (\",\n                 [pending_xref, addnodes.literal_emphasis, \"str\"],           \n                 \")\",\n                 \" -- \",\n                 \"blah blah\"))\n\n\n\n", "compilation error", "def test_py_constant_signature(app):\n    text = (\n        \".. py:constant:: MY_CONSTANT\\n\"\n        \"   :type: int\\n\"\n        \"   :value: 42\\n\"\n    )\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"MY_CONSTANT\"],\n                                [desc_annotation, (\": \",\n                                [pending_xref, \"int\"])],\n                [desc_annotation, \" = 42\"])],\n                desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"constant\",\n                domain=\"py\", objtype=\"constant\", noindex=False)\n\n\n\n", "compilation error", "compilation error"], "sample_58": ["    def test_as_form(self):\n        class MyForm(Form):\n            field1 = CharField()\n            field2 = CharField()\n\n        form = MyForm()\n        self.assertTrue(isinstance(form.as_form(), Form))\n\n        form.renderer = DjangoTemplates()\n        self.assertTrue(isinstance(form.as_form(), Form))\n\n        form.renderer = CustomRenderer()\n        self.assertTrue(isinstance(form.as_form(), Form))\n\n\n\n", "    def test_render_with_custom_attrs(self):\n        class MyForm(Form):\n            field1 = CharField()\n\n        class MyRenderer(BaseRenderer):\n                return f'<div class=\"custom-field\">{super().render_field(field)}</div>'\n\n        form = MyForm()\n        form.renderer = MyRenderer()\n\n        self.assertEqual(form.as_p(),\n                         '<div class=\"custom-field\"><p><label for=\"id_field1\">Field 1:</label> <input id=\"id_field1\" name=\"field1\" type=\"text\"></p></div>')\n", "    def test_renderer_with_errors(self):\n        class MyForm(Form):\n            name = CharField(required=False)\n\n        form = MyForm({'name': ''})\n\n        class CustomRenderer(DjangoTemplates):\n                return f\"<p>Error: {form['name'].errors}</p>\"\n\n        form = MyForm(renderer=CustomRenderer(),)\n        self.assertHTMLEqual(form.as_p(), '<p>Error: This field is required.</p>')\n", "    def test_field_names_are_unique(self):\n        with self.assertRaises(ValueError):\n            class DuplicateForm(Form):\n                field1 = CharField()\n                field2 = CharField()\n                field1 = CharField()\n", "    def test_custom_renderer_with_template(self):\n        class CustomRenderer(DjangoTemplates):\n                return field.label + ' <div>' + value + '</div>'\n\n        class MyForm(Form):\n            name = CharField()\n\n        form = MyForm({'name': 'John'})\n        form.renderer = CustomRenderer()\n        self.assertHTMLEqual(form.as_p(), '<p>Name <div>John</div></p>')\n"], "sample_1015": ["def test_ccode_for_loops():\n    assert ccode(For(x, Range(0, 10, 2), [print(x**2)])) == (\n        'for (x = 0; x < 10; x += 2) {\\n'\n        '   printf(\"%d \", x * x);\\n'\n        '}'\n    )\n", "def test_ccode_complex():\n    x = symbols('x', complex=True)\n    assert ccode(x.real) == 'creal(x)'\n    assert ccode(x.imag) == 'cimag(x)'\n    assert ccode(abs(x)) == 'cabs(x)'\n    assert ccode(arg(x)) == 'carg(x)'\n    assert ccode(x + 1j) == 'x + 1.0j'\n    assert ccode(x * y) == 'creal(x)*creal(y) - cimag(x)*cimag(y) + 1j*(creal(x)*cimag(y) + cimag(x)*creal(y))'\n", "def test_ccode_complex():\n    x = symbols('x')\n    y = symbols('y')\n    z = symbols('z')\n    assert ccode(z**2) == 'z*z'\n    assert ccode(complex(x, y)) == '({}.{})'.format(x, y)\n    assert ccode(x + 1j*y) == '({}+{}i)'.format(x, y)\n    assert ccode(exp(complex(x, y))) == 'cexp({}+{}i)'.format(x, y)\n", "def test_ccode_invalid_type():\n    with pytest.raises(TypeError):\n        ccode(Symbol('x', dtype=complex), standard='C89')\n\n    with pytest.raises(TypeError):\n        ccode(Symbol('x', dtype=complex), standard='C99')\n\n    with pytest.raises(TypeError):\n        ccode(Symbol('x', dtype=MatrixSymbol('M', 2, 2)), standard='C89')\n\n    with pytest.raises(TypeError):\n        ccode(Symbol('x', dtype=MatrixSymbol('M', 2, 2)), standard='C99')\n\n\n\n", "def test_ccode_complex():\n    z = S('2 + 3j')\n    assert ccode(z.real) == 'creal(z)'\n    assert ccode(z.imag) == 'cimag(z)'\n    assert ccode(z.abs()) == 'cabs(z)'\n    assert ccode(z.conjugate()) == 'conj(z)'\n    assert ccode(z**2) == 'cpow(z, 2)'\n    assert ccode(z + 1) == 'z + 1'\n    assert ccode(z * 2) == '2*z'\n\n\n\n"], "sample_814": ["def test_gradient_boosting_interaction_terms():\n    # Check if interaction terms can be estimated\n    X, y = datasets.make_classification(n_features=5, random_state=0, n_informative=2)\n    y = np.where(y == 1, 1, 0)\n\n    gb = GradientBoostingClassifier(max_depth=1, random_state=0)\n\n    # Fit GBR model and check feature importances\n    gb.fit(X, y)\n    initial_importances = gb.feature_importances_\n\n    # Introduce interaction terms\n    X_interaction = np.array([\n        [1, 0, 0, 0, 0],\n        [0, 1, 0, 0, 0],\n        [1, 1, 0, 0, 0],\n        [0, 0, 1, 0, 0],\n        [1, 0, 1, 0, 0],\n        [0, 1, 1, 0, 0],\n        [1, 1, 1, 0, 0],\n        [0, 0, 0, 1, 0],\n        [1, 0, 0, 1, 0],\n        [0, 1, 0, 1, 0],\n    ])\n\n    gb = GradientBoostingClassifier(max_depth=1, random_state=0)\n    gb.fit(X_interaction, y)\n    final_importances = gb.feature_importances_\n\n    # Check if interaction terms are reflected in importances\n    assert_array_almost_equal(initial_importances, final_importances)   \n", "def test_gradient_boosting_categorical_features():\n    # Test handling of categorical features\n    X, y = datasets.make_classification(n_samples=100, n_features=10,\n                n_categories=2, random_state=0)\n    gbr = GradientBoostingClassifier(max_depth=2,\n                                   random_state=0).fit(X, y)\n    assert_equal(gbr.n_features_, 10)\n\n\n\n", "def test_gradient_boosting_oob_improvement_with_presort():\n    X, y = datasets.make_classification(n_samples=100, random_state=1)\n    n_estimators = 10\n    oob_score = True\n    for presort in [True, False]:\n        est = GradientBoostingClassifier(n_estimators=n_estimators,\n                                        oob_score=oob_score,\n                                        presort=presort,\n                                        random_state=1)\n        est.fit(X, y)\n        assert_array_equal(est.oob_improvement_.shape[0], n_estimators)\n        assert_array_equal(est.oob_score_, est.oob_improvement_.sum() / n_estimators)\n\n\n\n", "def test_gradient_boosting_with_max_depth_and_min_impurity_split():\n    # Test interactions between max_depth and min_impurity_split.\n    X, y = datasets.make_classification(n_samples=100, n_features=5, random_state=1)\n\n    for max_depth in [2, 3, 5]:\n        for min_impurity_split in [0.01, 0.1, 0.2]:\n            gb = GradientBoostingClassifier(max_depth=max_depth,\n                                    min_impurity_split=min_impurity_split)\n            gb.fit(X, y)\n            # Check that tree depths respect max_depth\n            for tree in gb.estimators_:\n                assert_equal(tree.max_depth, max_depth)\n\n\n\n", "def test_gradient_boosting_sparse_categorical_targets():\n    # Test handling of sparse categorical targets with GradientBoostingClassifier.\n    from sklearn.preprocessing import OneHotEncoder\n    X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n    y = np.array([0, 1, 0, 1])  \n    encoder = OneHotEncoder(sparse=False)\n    y_encoded = encoder.fit_transform(y.reshape(-1, 1))\n\n    gb = GradientBoostingClassifier(random_state=42)\n    gb.fit(X, y_encoded)\n    y_pred = gb.predict(X)\n    assert_array_equal(y_pred, np.argmax(y_encoded, axis=1))\n\n"], "sample_589": ["def test_interpolate_na_method_choice(da):\n    vals = np.array([1, 2, 3, 4, np.nan, np.nan, 6, 7, np.nan, 9, 10], dtype=np.float64)\n    expected = xr.DataArray(vals, dims=\"time\")\n    da = xr.DataArray(vals, dims=\"time\")\n\n    methods = [\"linear\", \"nearest\", \"zero\", \"slinear\", \"quadratic\", \"cubic\"]\n    for method in methods:\n        actual = da.interpolate_na(dim=\"time\", method=method)\n        assert_equal(actual, expected)\n\n    with raises_regex(ValueError, \"Not a valid interpolation method\"):\n        da.interpolate_na(dim=\"time\", method=\"foo\")\n\n", "def test_interpolate_na_methods_2d(coords):\n    da = xr.DataArray(\n        [\n            [1, 2, 3, 4, np.nan, 6, 7, np.nan, np.nan, np.nan, 11],\n            [1, 2, 3, np.nan, np.nan, 6, 7, np.nan, np.nan, np.nan, 11],\n            [1, 2, 3, np.nan, np.nan, 6, 7, np.nan, np.nan, np.nan, 11],\n            [1, 2, 3, 4, np.nan, 6, 7, np.nan, np.nan, np.nan, 11],\n        ],\n        dims=[\"x\", \"y\"],\n        coords=coords,\n    )\n\n    for method in method:\n        actual = da.interpolate_na(\"y\", method=method)\n        expected_y = da.copy(\n            data=[\n                [1, 2, 3, 4, 6, 6, 7, np.nan, np.nan, np.nan, 11],\n                [1, 2, 3, np.nan, np.nan, 6, 7, np.nan, np.nan, np.nan, 11],\n                [1, 2, 3, np.nan, np.nan, 6, 7, np.nan, np.nan, np.nan, 11],\n                [1, 2, 3, 4, 6, 6, 7, np.nan, np.nan, np.nan, 11],\n            ]\n        )\n        assert_equal(actual, expected_y)\n\n        actual = da.interpolate_na(\"x\", method=method)\n        expected_x = xr.DataArray(\n            [\n                [1, 2, 3, 4, np.nan", "def test_interpolate_na_non_numeric(da):\n    da = xr.DataArray(\n        [[1, 2, 3, 4, np.nan, 6, 7, np.nan, np.nan, np.nan, 11],\n         [np.nan, 2, 3, 4, np.nan, 6, 7, np.nan, np.nan, np.nan, 11],\n         [1, 2, 3, 4, np.nan, 6, 7, np.nan, np.nan, np.nan, 11],\n         [1, 2, 3, 4, np.nan, 6, 7, np.nan, np.nan, np.nan, 11]],\n        dims=[\"x\", \"y\"],\n    )\n    \n    da = da.astype(\"object\")\n    with raises_regex(TypeError, \"Only numeric data is supported\"):\n        da.interpolate_na(\"y\")\n", "def test_interpolate_na_methods(da):\n    expected = xr.DataArray(\n        [\n            np.nan,\n            1,\n            2,\n            np.nan,\n            np.nan,\n            5,\n            np.nan,\n            np.nan,\n            np.nan,\n            np.nan,\n            10,\n        ],\n        dims=[\"t\"],\n    )\n    for method in [\"linear\", \"nearest\", \"zero\", \"slinear\", \"quadratic\", \"cubic\"]:\n        actual = da.interpolate_na(\"t\", method=method)\n        assert_equal(actual, expected)\n\n\n", "def test_interpolate_method_kwargs():\n    da = xr.DataArray(np.array([4, 5, np.nan], dtype=np.float64), dims=\"x\")\n    expected = xr.DataArray(np.array([4, 5, 6], dtype=np.float64), dims=\"x\")\n\n    actual = da.interpolate_na(dim=\"x\", method=\"linear\", fill_value=6)\n    assert_equal(actual, expected)\n\n    actual = da.interpolate_na(dim=\"x\", method=\"linear\", fill_value=\"extrapolate\")\n    assert_equal(actual, expected)\n\n    with raises_regex(ValueError, \"Invalid method\"):\n        da.interpolate_na(dim=\"x\", method=\"foo\", fill_value=6)\n"], "sample_1138": ["def test_TR18():\n    assert TR18(sin(x)**2 + cos(x)**2) == 1\n    assert TR18(sqrt(3)*cos(x) + sin(x)) == 2*sin(x + pi/6)\n    assert TR18(sin(2*x)) == 2*sin(x)*cos(x)\n    assert TR18(cos(2*x)) == 1 - 2*sin(x)**2\n    assert TR18(1 - sin(2*x)**2/4 - sin(y)**2 - cos(x)**4) == -cos(x)**2 + cos(y)**2\n    assert TR18(1/cos(x)) == sec(x)\n    assert TR18(cos(x)/sin(x)) == cot(x)\n    assert TR18(sin(x)/cos(x)) == tan(x)\n    assert TR18(sqrt(6)*cos(x) + sqrt(2)*sin(x)) == 2*sin(x + pi/3)\n    assert TR18(sqrt(x)*sin(x)) == sqrt(x)*sin(x)\n\n", "def test_TR15_16_17():\n    assert TR17(1 - 1/tan(x)**2) == cot(x)**2 - 1\n", "def test_TR18():\n    assert TR18(sin(x) + cos(x)) == sqrt(2)*sin(x + pi/4)\n    assert TR18(-sin(x) + cos(x)) == sqrt(2)*sin(x - pi/4)\n    assert TR18(sin(x) - cos(x)) == sqrt(2)*sin(x - pi/4)\n    assert TR18(sin(x) + sin(y)) == 2*sin((x + y)/2)*cos((x - y)/2)\n    assert TR18(sin(x) - sin(y)) == 2*cos((x + y)/2)*sin((x - y)/2)\n    assert TR18(cos(x) + cos(y)) == 2*cos((x + y)/2)*cos((x - y)/2)\n    assert TR18(cos(x) - cos(y)) == -2*sin((x + y)/2)*sin((x - y)/2)\n    assert TR18(2*sin(x)) == 2*sin(x)\n    assert TR18(2*cos(x)) == 2*cos(x)\n    assert TR18(sin(x)*cos(y)) == (1/2)*sin((x + y)) + (1/2)*sin((x - y))\n    assert TR18(cos(x)*sin(y)) == (1/2)*sin((x + y)) - (1/2)*sin((x - y))\n\n\n\n\n", "def test_TR18():\n    assert TR18(sin(x)*cos(x)) == 1/2*sin(2*x)\n    assert TR18(sin(x)*cos(2*x)) == 1/2*sin(3*x) + 1/2*sin(x)\n    assert TR18(sin(2*x)*cos(x)) == 1/2*sin(3*x) - 1/2*sin(x)\n    assert TR18(cos(x)**2*sin(x)) ==1/3*cos(x)**3 + 1/3*cos(x)\n    assert TR18(sin(x)**2*cos(x)) == -1/3*sin(x)**3 + 1/3*sin(x)\n    assert TR18(sin(x)*cos(x)*sin(y)) == 1/2*sin(2*x)*sin(y)\n    assert TR18(sin(x)*cos(x)*cos(y)) == 1/2*sin(2*x)*cos(y)\n\n\n", "def test_TR18():\n    assert TR18(sqrt(1 - sin(x)**2)) == cos(x)\n    assert TR18(sqrt(1 - cos(x)**2)) == sin(x)\n    assert TR18(sqrt(1 - tan(x)**2)) == 1/cos(x)\n    assert TR18(sqrt(1 - cot(x)**2)) == sin(x)/cos(x)\n    assert TR18(sqrt(1 - tanh(x)**2)) == 1/cosh(x)\n    assert TR18(sqrt(1 - coth(x)**2)) == sinh(x)/cosh(x)\n    assert TR18(sqrt(1 - sech(x)**2)) == sinh(x)\n    assert TR18(sqrt(1 - csch(x)**2)) == cosh(x)\n\n\n\n"], "sample_876": ["compilation error", "def test_mlp_batch_size_default(MLPEstimator):\n    X = X_digits[:100]\n    y = y_digits[:100]\n\n    # Test that the batch_size defaults to the sample size when not specified.\n    mlp = MLPEstimator(random_state=0, max_iter=50)\n    mlp.fit(X, y)\n    assert mlp._optimizer.batch_size == len(X)\n", "def test_mlp_partial_fit_sparse_data():\n    \"\"\"Test partial_fit with sparse data.\n\n    Non-regression test for https://github.com/scikit-learn/scikit-learn/issues/21021\n    \"\"\"\n    from sklearn.neural_network import MLPClassifier\n    from sklearn.datasets import make_classification\n    from scipy.sparse import csr_matrix\n    X, y = make_classification(n_samples=100, n_features=10, random_state=0)\n    X_sparse = csr_matrix(X)\n\n    mlp = MLPClassifier(hidden_layer_sizes=(5,), max_iter=10, random_state=0)\n    mlp.fit(X, y)\n\n    # Partial fit with a different subset of data\n    new_X = X[:50]\n    new_y = y[:50]\n    new_X_sparse = csr_matrix(new_X)\n    mlp.partial_fit(new_X_sparse, new_y)\n\n    # Verify accuracy on original data\n    assert_allclose(mlp.predict(X), y)\n\n\n\n", "def test_mlp_fit_partial_fit_with_validation():\n    \"\"\"\n    Test that partial_fit works correctly with validation_fraction when\n    early_stopping is True.\n    \"\"\"\n    X, y = X_digits[:200], y_digits[:200]\n    mlp = MLPClassifier(\n        hidden_layer_sizes=(10,), random_state=42, early_stopping=True,\n        validation_fraction=0.2\n    )\n    mlp.fit(X[:100], y[:100])\n    mlp.partial_fit(X[100:150], y[100:150])\n    assert len(mlp.validation_scores_) > 0  # Check data was used for validation\n\n\n\n\n", "compilation error"], "sample_808": ["def test_iforest_sparse_behaviour():\n    \"\"\"Test Isolation Forest on sparse input, both dense and sparse estimators\"\"\"\n\n    X_train = csr_matrix([[1, 1], [1, 2], [2, 1]])\n    X_test = csr_matrix([[2., 2.]])\n\n    clf = IsolationForest(random_state=0, max_samples=20,\n                          behaviour='new').fit(X_train)\n    assert clf.predict(X_test).all() == 1   \n", "def test_iforest_sparse_data():\n    \"\"\"Test if IsolationForest works correctly with sparse data.\"\"\"\n    from scipy.sparse import csr_matrix\n\n    # Generate sparse data\n    X_train = csr_matrix([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n    X_test = csr_matrix([[1, 1, 0], [0, 0, 1]])\n\n    # Fit and predict using sparse data\n    clf = IsolationForest().fit(X_train)\n    y_pred = clf.predict(X_test)\n\n    assert y_pred.shape == (2,)\n", "def test_iforest_clone():\n    # Test that .clone() returns a deep copy of the estimator\n\n    # generate some random data\n    X = rng.randn(50, 2)\n    y = (rng.randn(50) > 0).astype(int)\n\n    # fit the model\n    clf = IsolationForest().fit(X)\n    cloned_clf = clf.clone()\n\n    # check that the cloned model has independent parameters\n    cloned_clf.set_params(contamination=0.3)\n    assert_array_almost_equal(clf.contamination_, 0.1)\n    assert_array_almost_equal(cloned_clf.contamination_, 0.3)\n\n    # check that the cloned model has independent trees\n    for i in range(len(clf.estimators_)):\n        assert clf.estimators_[i] is not cloned_clf.estimators_[i]\n\n\n", "def test_iforest_sparse_fit_predict():\n    # Test fitting and predicting on sparse arrays\n    from scipy.sparse import csr_matrix\n    X = csr_matrix([[1, 2], [3, 4], [5, 6]])\n    y = np.array([1, 1, 1])\n\n    clf = IsolationForest(random_state=0)\n    clf.fit(X, y)\n    y_pred = clf.predict(X)\n    assert_array_equal(y_pred, y)\n\n", "def test_iforest_with_sparse_data():\n    X_train = csc_matrix([[1, 2], [3, 4], [5, 6]])\n    X_test = csc_matrix([[7, 8], [9, 10]])\n\n    clf = IsolationForest(random_state=0).fit(X_train)\n    y_pred = clf.predict(X_test)\n\n    assert_array_equal(y_pred, np.array([1, 1]))\n"], "sample_595": ["    def test_startswith_endswith(dtype):\n        values = xr.DataArray([\"apple\", \"Banana\", \"cherry\", \"DATE\"]).astype(dtype)\n\n        result = values.str.startswith(\"a\")\n        expected = xr.DataArray([True, False, False, False]).astype(bool)\n        assert_equal(result, expected)\n\n        result = values.str.endswith(\"e\")\n        expected = xr.DataArray([False, False, True, False]).astype(bool)\n        assert_equal(result, expected)\n\n        result = values.str.startswith(\"ba\", 1)\n        expected = xr.DataArray([False, True, False, False]).astype(bool)\n        assert_equal(result, expected)\n\n        result = values.str.endswith(\"ry\", 2)\n        expected = xr.DataArray([False, False, True, False]).astype(bool)\n        assert_equal(result, expected)\n", "compilation error", "def test_contains_regex(dtype):\n    values = xr.DataArray([\"hello world\", \"world hello\", \"foobar\", \"apple\", \"banana\"]).astype(dtype)\n\n    result = values.str.contains(\"world\")\n    expected = xr.DataArray([True, True, False, False, False])\n    assert_equal(result, expected)\n\n    result = values.str.contains(r\"hello.*\")\n    expected = xr.DataArray([True, False, False, False, False])\n    assert_equal(result, expected)\n\n    result = values.str.contains(r\".*banana\")\n    expected = xr.DataArray([False, False, False, False, True])\n    assert_equal(result, expected)\n\n", "def test_contains_unicode(dtype):\n    values = xr.DataArray([\"a\\xff\\xf0\", \"b\", \"c\\u20ac\"])\n    result = values.str.contains(r\"\\xff\")\n\n    expected = xr.DataArray([True, False, False])\n    assert_equal(result, expected)\n\n    result = values.str.contains(r\"\\u20ac\")\n    expected = xr.DataArray([False, False, True])\n    assert_equal(result, expected)\n", "def test_contains(dtype):\n    values = xr.DataArray([\"hello\", \"world\", \"python\", \"coding\"]).astype(dtype)\n\n    result = values.str.contains(\"o\")\n    expected = xr.DataArray([True, True, True, True])\n    assert_equal(result, expected)\n\n    result = values.str.contains(\"xyz\")\n    expected = xr.DataArray([False, False, False, False])\n    assert_equal(result, expected)\n\n    result = values.str.contains(\"python\", case=False)\n    expected = xr.DataArray([False, False, True, False])\n    assert_equal(result, expected)\n\n    result = values.str.contains(\"w\", regex=True)\n    expected = xr.DataArray([False, True, False, False])\n    assert_equal(result, expected)\n"], "sample_334": ["    def test_renderer_is_called(self):\n        called = False\n        class CustomRenderer(DjangoTemplates):\n                nonlocal called\n                called = True\n                return super().render(form, **kwargs)\n\n        form = Form(renderer=CustomRenderer())\n        form.as_p()\n        self.assertTrue(called)\n", "    def test_get_default_renderer(self):\n        self.assertIsInstance(get_default_renderer(), DjangoTemplates)\n", "    def test_renderer_instance_attribute(self):\n        class CustomForm(Form):\n            renderer = CustomRenderer()\n\n        form = CustomForm()\n        self.assertEqual(form.renderer, CustomForm.renderer)\n", "    def test_repr_with_errors(self):\n        class MyForm(Form):\n            name = CharField(label='Name', max_length=100, required=True)\n            age = IntegerField(label='Age', required=True)\n        \n        data = {'name': 'John', 'age': 'invalid'}\n        form = MyForm(data)\n        self.assertFalse(form.is_valid())\n        self.assertEqual(\n            repr(form),\n            \"<MyForm bound=True, valid=False, fields=(name;age), errors={'name': [], 'age': ['Enter a valid integer.']}>\"\n        )", "    def test_non_field_errors_in_template(self):\n        class MyForm(Form):\n            username = CharField()\n\n                raise ValidationError('Non-field error')\n\n        data = {'username': 'john'}\n        f = MyForm(data)\n        self.assertFalse(f.is_valid())\n        self.assertEqual(f.errors, {'__all__': ['Non-field error']})\n        response = f.as_p()\n        self.assertIn('<p class=\"errorlist nonfield\">', response)\n        self.assertIn('Non-field error', response)\n"], "sample_247": ["    def test_alias_with_f_expression_in_subquery(self):\n        subquery_qs = Book.objects.alias(\n            author_count=Count(\"authors\")\n        ).values(\"author_count\")\n        qs = Publisher.objects.annotate(\n            avg_author_count=Subquery(subquery_qs, output_field=IntegerField())\n        ).order_by('avg_author_count')\n        self.assertCountEqual(qs, [\n            {'avg_author_count': 2},  # Apress\n            {'avg_author_count': 1},  # Google\n            {'avg_author_count': 2},  # Morgan Kaufmann\n            {'avg_author_count': 1},  # Prentice Hall\n            {'avg_author_count': 1},  # Sams\n        ])\n", "    def test_alias_with_subquery(self):\n        qs = Book.objects.alias(\n            max_rating=Subquery(Book.objects.filter(pubdate__year=F('pubdate__year')).order_by('-rating').values('rating')[:1])\n        ).values('pubdate', 'max_rating')\n        self.assertCountEqual(qs, [\n            {'pubdate': datetime.date(1991, 10, 15), 'max_rating': 5.0},\n            {'pubdate': datetime.date(1995, 1, 15), 'max_rating': 4.0},\n            {'pubdate': datetime.date(2007, 12, 6), 'max_rating': 4.5},\n            {'pubdate': datetime.date(2008, 6, 23), 'max_rating': 4.0},\n        ])\n", "    def test_alias_with_subquery(self):\n        qs = Book.objects.alias(\n            top_rated=Subquery(\n                Book.objects.filter(rating__gt=F('rating')).values('rating')[:1]\n            )\n        ).annotate(top_rating=F('top_rated'))\n        self.assertEqual(qs.count(), Book.objects.count())\n", "    def test_select_related_alias(self):\n        qs = Book.objects.select_related('publisher').alias(\n            publisher_name=F('publisher__name'),\n        )\n        self.assertIs(hasattr(qs.first(), 'publisher_name'), False)\n        self.assertQuerysetEqual(\n            qs.values_list('publisher_name'),\n            [('Apress',), ('Apress',), ('Apress',), ('Apress',), ('Apress',)],\n        )\n", "    def test_alias_with_null_value(self):\n        qs = Book.objects.alias(name_alias=F('name') if F('name') else Value(None))\n        self.assertQuerysetEqual(qs.values_list('name_alias', flat=True), [\n            'The Definitive Guide to Django: Web Development Done Right',\n            'Practical Django Projects',\n            'Artificial Intelligence: A Modern Approach',\n            'Paradigms of Artificial Intelligence Programming: Case Studies in Common Lisp',\n        ])\n\n\n\n"], "sample_997": ["def test_implicit_multiplication_with_parentheses():\n    transformations = standard_transformations + \\\n                      (implicit_multiplication,)\n    assert parse_expr(\"a(b+c)\") == a*(b+c)\n    assert parse_expr(\"(a+b)c\") == (a+b)*c\n    assert parse_expr(\"(a+b)(c+d)\") == (a+b)*(c+d)\n\n\n\n", "compilation error", "def test_implicit_multiplication_with_functions():\n    transformations = standard_transformations + (implicit_multiplication,)\n    x = Symbol('x')\n    f = Function('f')\n    g = Function('g')\n\n    assert parse_expr(\"f(x) sin(x)\", transformations=transformations) == f(x)*sin(x)\n    assert parse_expr(\"f(x)g(x)\", transformations=transformations) == f(x)*g(x)\n", "def test_complex_parentheses():\n    transformations = standard_transformations + \\\n                        (convert_equals_signs, )\n    x = Symbol('x')\n    y = Symbol('y')\n    assert parse_expr(\"((((x + y) = 2))) == True\", transformations=transformations) == Eq(Eq(x+y, 2), True)\n", "def test_implicit_multiplication_with_function_arguments():\n    transformations = standard_transformations + \\\n                      (implicit_multiplication,)\n    x = Symbol('x')\n    y = Symbol('y')\n    f = Function('f')\n\n    assert parse_expr('f(x) * y', transformations=transformations) == f(x) * y\n"], "sample_143": ["    def test_camel_case_to_spaces(self):\n        self.assertEqual(text.camel_Case_to_spaces('camelCase'), 'camel Case')\n        self.assertEqual(text.camel_case_to_spaces('myCamelCase'), 'my Camel Case')\n        self.assertEqual(text.camel_case_to_spaces('allCamelCase'), 'all Camel Case')\n        self.assertEqual(text.camel_case_to_spaces('mixedCase'), 'mixed Case')\n        self.assertEqual(text.camel_case_to_spaces('snake_case'), 'snake case')\n", "    def test_camel_case_to_spaces(self):\n        self.assertEqual(text.camel_case_to_spaces('camelCase'), 'camel Case')\n        self.assertEqual(text.camel_case_to_spaces('some_Variable_Name'), 'some Variable Name')\n        self.assertEqual(text.camel_case_to_spaces('snake_case'), 'snake case')\n        self.assertEqual(text.camel_case_to_spaces('CamelCaseWords'), 'Camel Case Words')\n        self.assertEqual(text.camel_case_to_spaces('one-two-three'), 'one two three')\n        self.assertEqual(text.camel_case_to_spaces('UpperCamelCase'), 'Upper Camel Case')\n", "    def test_camel_case_to_spaces(self):\n        self.assertEqual(text.camel_Case_to_spaces('camelCaseString'), 'camel Case String')\n        self.assertEqual(text.camel_case_to_spaces('thisIsATest'), 'this Is A Test')\n        self.assertEqual(text.camel_case_to_spaces('oneTwoThree'), 'one Two Three')\n        self.assertEqual(text.camel_case_to_spaces('aSingleWord'), 'a Single Word')\n        self.assertEqual(text.camel_case_to_spaces('HelloWorld'), 'Hello World')\n", "    def test_camel_case_to_spaces(self):\n        items = [\n            ('TheQuickBrownFox', 'The Quick Brown Fox'),\n            ('theQuickBrownFox', 'the Quick Brown Fox'),\n            ('quickBrownFox', 'quick Brown Fox'),\n            ('quickBrownFox123', 'quick Brown Fox123'),\n            ('', ''),\n        ]\n        for input_text, expected_output in items:\n            self.assertEqual(text.camel_case_to_spaces(input_text), expected_output)\n            self.assertEqual(text.camel_case_to_spaces(lazystr(input_text)), expected_output)\n\n\n\n", "    def test_normalize_newlines_with_non_ascii(self):\n        self.assertEqual(text.normalize_newlines('abc\\n\\ndef\\r\\nghi'), 'abc\\ndef\\nghi')\n        unicode_string = u'abc\\u2028def\\r\\nghi'.encode('utf-8')\n        self.assertEqual(text.normalize_newlines(unicode_string), b'abc\\ndef\\nghi')\n"], "sample_93": ["    def test_aggregate_with_multiple_subqueries(self):\n        num_books_per_publisher = Publisher.objects.values_list('name').annotate(\n            num_books=Count('book'),\n            avg_rating=Avg('book__rating'),\n            long_books=Count('book', filter=Q(book__pages__gt=400))\n        )\n        self.assertEqual(\n            dict(num_books_per_publisher),\n            {('Apress',): {'num_books': 1, 'avg_rating': 4.0, 'long_books': 0},\n             ('Prentice Hall',): {'num_books': 2, 'avg_rating': 4.5, 'long_books': 1},\n             ('Morgan Kaufmann',): {'num_books': 1, 'avg_rating': 4.0, 'long_books': 0},\n             ('Sams',): {'num_books': 1, 'avg_rating': 3.0, 'long_books': 0},\n             ('Expensive Publisher',): {'num_books': 1, 'avg_rating': 5.0, 'long_books': 1}}\n        )\n\n", "    def test_subquery_with_date_functions(self):\n        latest_book_date = Book.objects.filter(\n            publisher=OuterRef('pk')\n        ).order_by('-pubdate').values('pubdate')[:1]\n        publisher_qs = Publisher.objects.annotate(\n            latest_book_date=Subquery(latest_book_date),\n        ).annotate(\n            newest_book_year=ExtractYear('latest_book_date')\n        )\n        with self.assertNumQueries(1) as ctx:\n            list(publisher_qs)\n        self.assertEqual(ctx[0]['sql'].count('SELECT'), 2)\n\n\n", "    def test_annotate_multiple_expressions(self):\n        qs = Book.objects.annotate(\n            sum_price_and_pages=Sum(F('price') + F('pages')),\n            rating_avg=Avg('rating')\n        )\n        self.assertEqual(qs.filter(pk=self.b1).first().sum_price_and_pages, 1000.0)\n\n        qs = Book.objects.annotate(\n            rating_avg=Avg('rating'),\n            sum_price_and_pages=Sum(F('price') + F('pages'))\n        ).order_by('-rating_avg')\n        self.assertQuerysetEqual(\n            qs, [\n                {'rating_avg': 5.0, 'sum_price_and_pages': 340.75},\n                {'rating_avg': 5.0, 'sum_price_and_pages': 258.1},\n                {'rating_avg': 4.0, 'sum_price_and_pages': 236.1},\n                {'rating_avg': 3.0, 'sum_price_and_pages': 100.75},\n                {'rating_avg': 2.0, 'sum_price_and_pages': 100.9},\n            ], lambda v: (v['rating_avg'], v['sum_price_and_pages'])\n        )\n\n\n", "    def test_subquery_with_complex_select(self):\n        class MyCount(Count):\n                return super().as_sql(compiler, connection) + ' + 1'\n\n        long_books_count_qs = Book.objects.filter(\n            publisher=OuterRef('pk'),\n            pages__gt=400,\n        ).values('publisher').annotate(count=MyCount('pk'))\n\n        expensive_breakdown = Publisher.objects.values_list(\n            Subquery(long_books_count_qs, IntegerField()),\n            'name'\n        ).annotate(total=Count('*')).order_by('total')\n        self.assertEqual(\n            list(expensive_breakdown), [(5, 'Apress'), (3, 'Prentice Hall')]\n        )\n", "    def test_complex_expressions_in_annotate_order_by_values(self):\n        qs = Author.objects.annotate(\n            complex_age=Add(F('age'), F('friends__age')),\n        ).values('name', 'complex_age').order_by('complex_age')\n        self.assertQuerysetEqual(\n            qs, [\n                {'name': 'Brad Dayley', 'complex_age': None},\n                {'name': 'Jacob Kaplan-Moss', 'complex_age': 152},\n                {'name': 'James Bennett', 'complex_age': 63},\n                {'name': 'Jeffrey Forcier', 'complex_age': 128},\n                {'name': 'Paul Bissex', 'complex_age': 120},\n                {'name': 'Peter Norvig', 'complex_age': 103},\n                {'name': 'Stuart Russell', 'complex_age': 103},\n                {'name': 'Wesley J. Chun', 'complex_age': 176},\n                {'name': 'Adrian Holovaty', 'complex_age': 68},\n            ],\n            lambda x: (x['name'], x['complex_age'])\n        )\n\n        # Make sure the complex expression is handled correctly in\n        # the `order_by` clause.\n        qs = Author.objects.annotate(\n            complex_age=Add(F('age'), F('friends__age')),\n        ).order_by('complex_age')\n        self.assertQuerysetEqual(\n            qs, [\n                {'name': 'Brad Dayley', 'age': 32, 'friends__age': None},\n                {'name': 'Jacob Kaplan-Moss', 'age': 37, 'friends__age': 115},\n                {'name': 'James Bennett', 'age': 63, 'friends__age': 0},\n                {'name': 'Jeffrey Forcier', 'age': 65, 'friends__age': 63},\n                {'name': 'Paul Bissex', 'age': 58, 'friends__age': 62},\n                {'name': 'Peter Norvig', '"], "sample_515": ["def test_colorbar_aspect_ratio_fraction():\n    fig, axs = plt.subplots(2, 2, figsize=(8, 6))\n    for ax, aspect in zip(axs.flatten(), [0.5, 2, 5, 10]):\n        im = ax.imshow(np.random.randn(10, 10))\n        cb = fig.colorbar(im, ax=ax, aspect=aspect, fraction=0.5)\n\n\n", "def test_colorbar_ticklocation_orientation(orientation, ticklocation):\n    fig, ax = plt.subplots()\n    im = ax.imshow([[0, 1], [2, 3]])\n    cb = fig.colorbar(im, ax=ax, orientation=orientation)\n    cb.ax.yaxis.set_tick_params(labelbottom=orientation == 'horizontal')\n    cb.ax.yaxis.set_tick_params(labelleft=orientation == 'vertical')\n    cb.ax.set_ylabel('label')\n    ticks = cb.ax.get_yticks()\n    assert ticks.size > 0\n    assert cb.ax.yaxis.get_label() == 'label'\n    assert all(ticks[0] <= tick for tick in ticks)\n    assert all(tick <= ticks[-1] for tick in ticks)\n    if ticklocation == 'bottom':\n        assert cb.ax.yaxis.get_tick_params()[\"labelbottom\"] == True\n    elif ticklocation == 'top':\n        assert cb.ax.yaxis.get_tick_params()[\"labelbottom\"] == False\n    elif ticklocation == 'left':\n        assert cb.ax.yaxis.get_tick_params()[\"labelleft\"] == True\n    elif ticklocation == 'right':\n        assert cb.ax.yaxis.get_tick_params()[\"labelleft\"] == False\n\n\n\n", "def test_colorbar_fraction_aspect():\n\n    fig, axs = plt.subplots(2, 2, figsize=(6, 6))\n\n    for i, ax in enumerate(axs.flatten()):\n        im = ax.imshow(np.random.rand(10, 10), cmap='viridis')\n        cb = fig.colorbar(im, ax=ax, fraction=0.05 + i*0.05, aspect=10/3)\n        cb.ax.set_ylabel(\"Colorscale\")\n\n    plt.tight_layout()\n", "def test_colorbar_fractional_position(fig_ref, fig_test):\n    ax_ref = fig_ref.add_subplot()\n    im = ax_ref.imshow([[0, 1], [2, 3]])\n    ax_ref.figure.colorbar(im, fraction=0.15, pad=0.01) \n    ax_test = fig_test.add_subplot()\n    im = ax_test.imshow([[0, 1], [2, 3]])\n    ax_test.figure.colorbar(im, fraction=0.15, pad=0.01) \n", "def test_colorbar_aspect(fig_ref, fig_test):\n    ax = fig_test.add_subplot()\n    im = ax.imshow([[1, 2], [3, 4]], cmap='viridis')\n    fig_ref.colorbar(im, aspect=10)\n    fig_test.colorbar(im, aspect=10)\n"], "sample_227": ["    def test_emptylistfieldfilter_ordering(self):\n        modeladmin = BookAdminWithEmptyFieldListFilter(Book, site)\n        request = self.request_factory.get('/', {'title__isempty': '1'})\n        request.user = self.alfred\n        changelist = modeladmin.get_changelist_instance(request)\n\n        # Make sure default ordering is followed by default\n        self.assertEqual(changelist.get_queryset(request).first().title, '')\n\n        # Test ordering with `order_by` parameter\n        request = self.request_factory.get('/', {'title__isempty': '1', 'order_by': '-title'})\n        changelist = modeladmin.get_changelist_instance(request)\n        self.assertEqual(changelist.get_queryset(request).first().title, '')\n\n\n", "    def test_emptylistfieldfilter_non_string_values(self):\n        modeladmin = EmployeeAdminWithEmptyFieldListFilter(Employee, site)\n        request = self.request_factory.get('/', {'department__pk': self.design.pk})\n        request.user = self.alfred\n        msg = (\n            \"The list filter 'EmptyFieldListFilter' cannot be used with \"\n            \"non-string values for lookups.\"\n        )\n        with self.assertRaisesMessage(IncorrectLookupParameters, msg):\n            modeladmin.get_changelist_instance(request)\n", "    def test_emptylistfieldfilter_empty_string_with_isnull(self):\n        class DepartmentAdminWithEmptyFieldListFilterAndNull(ModelAdmin):\n            list_filter = (\n                ('description', EmptyFieldListFilter),\n            )\n\n        empty_description = Department.objects.create(code='EMPT', description='')\n        none_description = Department.objects.create(code='NONE', description=None)\n\n        department_admin = DepartmentAdminWithEmptyFieldListFilterAndNull(Department, site)\n        request = self.request_factory.get('/', {'description__isempty': '1'})\n        request.user = self.alfred\n        changelist = department_admin.get_changelist_instance(request)\n        queryset = changelist.get_queryset(request)\n        self.assertCountEqual(queryset, [empty_description, none_description])\n\n", "    def test_emptylistfieldfilter_with_model_field_containing_null_values(self):\n        class NullValueDepartment(models.Model):\n            code = models.CharField(max_length=10)\n\n        NullValueDepartment.objects.create(code='NULL')\n        modeladmin = DepartmentAdminWithNullValues(NullValueDepartment, site)\n        request = self.request_factory.get('/', {'code__isnull': '1'})\n        request.user = self.alfred\n        changelist = modeladmin.get_changelist_instance(request)\n        queryset = changelist.get_queryset(request)\n        self.assertCountEqual(list(queryset), [NullValueDepartment.objects.get(code='NULL')])\n\n", "    def test_emptylistfieldfilter_with_reverse_related_field(self):\n        class RelatedBookAdmin(ModelAdmin):\n            class Meta:\n                model = RelatedBook\n                list_filter = (\n                    ('book__title', EmptyFieldListFilter),\n                )\n        \n        related_book = RelatedBook.objects.create(book=self.django_book)\n        empty_related_book =  RelatedBook.objects.create(book=None)\n\n        modeladmin = RelatedBookAdmin(RelatedBook, site)\n        request = self.request_factory.get('/', {})\n        request.user = self.alfred\n        changelist = modeladmin.get_changelist_instance(request)\n        queryset = changelist.get_queryset(request)\n        self.assertEqual(list(queryset), [related_book, empty_related_book])\n\n        request = self.request_factory.get('/', {'book__title__isempty': '1'})\n        request.user = self.alfred\n        changelist = modeladmin.get_changelist_instance(request)\n\n        queryset = changelist.get_queryset(request)\n        self.assertEqual(list(queryset), [empty_related_book])\n\n        request = self.request_factory.get('/', {'book__title__isempty': '0'})\n        request.user = self.alfred\n        changelist = modeladmin.get_changelist_instance(request)\n\n        queryset = changelist.get_queryset(request)\n        self.assertEqual(list(queryset), [related_book])\n"], "sample_404": ["    def test_template_debug_info_preserved(self):\n        engine = self._engine(libraries={\"my_tag\": \"template_tests.templatetags.my_tag\"})\n        template = engine.from_string(\n            \"{% load my_tag %}\"\n            \"{% my_tag 'this' 'is' %}\"\n        )\n        with self.assertRaises(TemplateSyntaxError) as e:\n            template.render(Context())\n        if self.debug_engine:\n            debug = e.exception.template_debug\n            self.assertEqual(debug[\"start\"], 10)\n            self.assertEqual(debug[\"end\"], 23)\n", "    def test_interpolation_in_variable_node(self):\n        engine = self._engine()\n        template = engine.from_string(\"{{ some_var|default:'default' }}\")\n        context = Context({\"some_var\": 42})\n        self.assertEqual(template.render(context), \"42\")\n\n        context = Context({})\n        self.assertEqual(template.render(context), \"default\")\n\n\n\n", "    def test_string_nodes_in_context(self):\n        \"\"\"\n        #27229 - String nodes should be rendered correctly within a template string.\n        \"\"\"\n        template = self._engine().from_string(\"My value is {{ my_value }}!\")\n        context = Context({\"my_value\": \"Hello World\"})\n        rendered_template = template.render(context)\n        self.assertEqual(rendered_template, \"My value is Hello World!\")\n", "    def test_template_autoescape(self):\n        engine = self._engine()\n        template = engine.from_string(\"{{ '<b>Hello</b>' | autoescape }}\")\n        with self.assertRaises(TemplateSyntaxError) as e:\n            template.render(Context())\n        if self.debug_engine:\n            self.assertEqual(e.exception.template_debug[\"during\"], \"{{ '<b>Hello</b>' | autoescape }}\")\n\n", "    def test_render_raw_string(self):\n        \"\"\"\n        #22510 -- Templates should render raw strings correctly, regardless\n        of autoescape.\n        \"\"\"\n        engine = self._engine()\n        template = engine.from_string(\"{{ '<b>Hello, world!</b>' | raw }}\")\n        self.assertEqual(template.render(Context({})),\"<b>Hello, world!</b>\")\n        template = engine.from_string(\"{{ <b>Hello, world!</b> | raw }}\")\n        self.assertEqual(template.render(Context({})),\"<b>Hello, world!</b>\")\n"], "sample_1153": ["def test_issue_16069():\n    from sympy import Symbol, exp, I\n    x = Symbol('x')\n    assert exp(I*x).subs(x, -pi/2) == -I\n    assert exp(I*x).subs(x, pi/2) == I\n", "def test_issue_18755():\n    from sympy import Abs, sign\n    x = Symbol('x')\n    assert Abs(x + I*x).simplify() == x*sqrt(2)\n    assert sign(x + I*x).simplify() == sign(x)\n", "def test_issue_16516():\n    from sympy import (Abs, exp, sin, cos, re, im, sqrt,  EulerGamma,\n                       Symbol, pi)\n    x = Symbol('x', real=True)\n    y = Symbol('y', real=True)\n    z = Symbol('z', real=True)\n    a = Symbol('a', positive=True)\n\n    assert Abs(exp(x * I)).subs(x, 0) == 1\n    assert Abs(exp(x * I + y)).subs(x, 0) == exp(y)\n    assert Abs(sqrt(x) + I*sqrt(y)).subs(x, 4).subs(y, 9) == 5\n    assert Abs(sqrt(-x) + I*sqrt(y)).subs(x, -4).subs(y, 9) == 5\n\n\n    assert re(sin(x) + I*cos(x)).subs(x, pi/4) == sqrt(2)/2\n    assert im(sin(x) + I*cos(x)).subs(x, pi/4) == sqrt(2)/2\n\n    assert re(exp(sin(x) + I*cos(x))).subs(x, pi/4) == exp(sqrt(2)/2)\n    assert im(exp(sin(x) + I*cos(x))).subs(x, pi/4) == exp(sqrt(2)/2) * (-sqrt(2)/2)\n    assert Abs(exp(sin(x) + I*cos(x))).subs(x, pi/4) == exp(sqrt(2)/2)\n    \n    assert Abs(sqrt(x + I*y)).subs(x, 4).subs(y, 9) == sqrt(13)\n    assert Abs(sqrt(x - I*y)).subs(x, 4).subs(y, 9) == sqrt(13)\n\n    assert Abs(a**(x + I*y)).subs(a, 2).subs(x, 1).subs(y, 2) == \\\n           2**(1 + 2*I)\n\n", "def test_issue_18399():\n    from sympy.functions.elementary.complexes import polar_lift\n    z = Symbol('z', complex=True)\n    assert polar_lift(z).args[0] == z\n    assert polar_lift(z).args[1] == polar_lift(abs(z))\n", "compilation error"], "sample_195": ["    def test_bulk_batch_size(self):\n        self.assertEqual(connection.ops.bulk_batch_size(None, []), 500)\n        self.assertEqual(connection.ops.bulk_batch_size(['id'], []), 500)\n        self.assertEqual(connection.ops.bulk_batch_size(['id', 'name'], []), 250)\n\n\n\n\n", "    def test_bulk_insert_sql(self):\n        fields = ['id', 'name', 'author_id']\n        placeholder_rows = [\n            [1, 'Crime and Punishment', 1],\n            [2, 'The Catcher in the Rye', 2],\n            [3, 'Pride and Prejudice', 3],\n        ]\n        expected_sql = \"\"\"\n            SELECT 1, 'Crime and Punishment', 1\n            UNION ALL \n            SELECT 2, 'The Catcher in the Rye', 2\n            UNION ALL \n            SELECT 3, 'Pride and Prejudice', 3\n        \"\"\"\n        self.assertEqual(connection.ops.bulk_insert_sql(fields, placeholder_rows), expected_sql)\n", "    def test_bulk_batch_size(self):\n        with self.subTest('single field'):\n            self.assertEqual(connection.ops.bulk_batch_size(\n                ['id'], [dict(id=i) for i in range(500)]) , 500)\n        with self.subTest('multiple fields'):\n            self.assertEqual(connection.ops.bulk_batch_size(\n                ['id', 'title'], [dict(id=i, title=f'Book {i}') for i in range(1000)]),\n                333)\n        with self.subTest('no fields'):\n            self.assertEqual(connection.ops.bulk_batch_size([], []), 0)\n\n", "    def test_bulk_insert_sql(self):\n        msg = self.may_require_msg % 'bulk_insert_sql'\n        with self.assertRaisesMessage(NotImplementedError, msg):\n            connection.ops.bulk_insert_sql('a', [])\n", "    def test_view_create_sql(self):\n        msg = 'This backend does not support views.'\n        with self.assertRaisesMessage(NotSupportedError, msg):\n            connection.ops.view_create_sql(None, 'view_name', [])\n\n\n\n"], "sample_304": ["    def test_domain_allowlist_access(self):\n        validator = EmailValidator()\n        validator.domain_allowlist = ['mydomain']\n        self.assertEqual(validator.domain_allowlist, ['mydomain'])\n", "    def test_domain_allowlist_warning(self):\n        msg = \"The domain_allowlist argument is deprecated in favor of domain_whitelist.\"\n        with self.assertRaisesMessage(RemovedInDjango41Warning, msg):\n            EmailValidator(domain_allowlist=['mydomain'])\n\n", "    def test_domain_whitelist_with_allowlist(self):\n        validator = EmailValidator(allowlist=['localdomain'])\n        validator.domain_whitelist = ['mydomain']\n        self.assertEqual(validator.domain_allowlist, ['mydomain'])\n", "    def test_domain_blacklist(self):\n        validator = EmailValidator(domain_blacklist=['mydomain'])\n        self.assertEqual(validator.domain_blocklist, ['mydomain'])\n        self.assertIsNone(validator('email@mydomain.com'))\n        self.assertEqual(validator.domain_blocklist, validator.domain_blacklist)\n", "    def test_domain_allowlist_access(self):\n        validator = EmailValidator()\n        validator.domain_allowlist = ['mydomain']\n        self.assertEqual(validator.domain_allowlist, ['mydomain'])\n\n\n"], "sample_1198": ["compilation error", "compilation error", "compilation error", "def test_parser_mathematica_complex():\n    parser = MathematicaParser()\n    \n    full_forms = [\n        \"2 + 3 I\",\n        \"1 + 2 I * 3 - 4 I\",\n        \"I^2\",\n        \"Complex[1, 2]\",\n        \"ComplexExpand[sin[x + I y]]\",\n        \"(1 + I)^2\",\n        \"Re[1 + 2 I]\",\n        \"Im[1 + 2 I]\",\n        \"Abs[1 + 2 I]\",\n        \"Arg[1 + 2 I]\",\n        \"Conjugate[1 + 2 I]\"\n    ]\n    \n    for full_form in full_forms:\n        parsed = parser._from_fullform_to_sympy(full_form)\n        assert parsed.is_complex  \n        \n", "compilation error"], "sample_748": ["def test_grid_search_with_class_weight():\n    X, y = make_classification(n_samples=50, n_features=2,\n                              n_informative=2, n_classes=2,\n                              random_state=0,\n                              weights=[0.8, 0.2])  # Imbalanced data\n\n    param_grid = {'C': [0.1, 1, 10],\n                  'class_weight': ['balanced', None]}\n\n    clf = SVC(kernel='linear', probability=True)\n\n    grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='balanced_accuracy',\n                              error_score=0)\n\n    grid_search.fit(X, y)\n\n    assert_not_equal(grid_search.best_score_, 0)\n    assert_array_almost_equal(grid_search.cv_results_['mean_test_score'],\n                             grid_search.cv_results_['mean_train_score'])\n\n\n\n", "compilation error", "def test_cross_validate_on_multioutput_data():\n    # Test cross-validation with multi-output data\n    X, y = make_multilabel_classification(return_indicator=True,\n                                          random_state=0)\n\n    est = DecisionTreeClassifier(random_state=0)\n    scores = cross_validate(est, X, y, cv=5, scoring='accuracy',\n                           return_train_score=True)\n\n    assert_equal(len(scores['train_score']), 5)\n    assert_equal(len(scores['test_score']), 5)\n    assert_equal(len(scores['fit_time']), 5)\n    assert_equal(len(scores['score_time']), 5)\n    assert_equal(len(scores['recall']), 5)\n\n\n\n", "def test_estimator_fit_during_scoring():\n    class MockEstimator:\n            self.fitted = True\n            return self\n\n            return 0.5\n\n    clf = MockEstimator()\n    gs = GridSearchCV(\n        clf, {'foo_param': [1]}, scoring='accuracy', cv=2\n    )\n    gs.fit(X, y)\n    assert_true(clf.fitted)\n\n\n", "def test_grid_search_with_sample_weight_data():\n    # Test grid search with sample weight data\n    X, y = make_classification(n_samples=100, n_features=4,\n                               n_informative=2,\n                               random_state=42,\n                               weights=[0.1, 0.9])\n\n    est_parameters = {\"max_depth\": [1, 2, 3, 4]}\n    cv = KFold(random_state=0)\n\n    estimators = [DecisionTreeClassifier(random_state=0),\n                  DecisionTreeRegressor(random_state=0)]\n\n    # Test with grid search cv\n    for est in estimators:\n        grid_search = GridSearchCV(est, est_parameters, cv=cv,\n                                   sample_weight=X[:, -1])\n        grid_search.fit(X[:, :-1], y)\n        res_params = grid_search.cv_results_['params']\n        for cand_i in range(len(res_params)):\n            est.set_params(**res_params[cand_i])\n\n            for i, (train, test) in enumerate(cv.split(X[:, :-1], y)):\n                est.fit(X[train][:, :-1], y[train],\n                        sample_weight=X[train][-1])\n                correct_score = est.score(X[test][:, :-1], y[test],\n                                          sample_weight=X[test][-1])\n                assert_almost_equal(\n                    correct_score,\n                    grid_search.cv_results_['split%d_test_score' % i][cand_i])\n\n\n\n"], "sample_447": ["    def test_alias_ordering_with_f(self):\n        qs = Book.objects.alias(\n            rating_avg=F(\"rating\"),\n        ).order_by(\"rating_avg\")\n        self.assertIs(hasattr(qs.first(), \"rating_avg\"), False)\n        self.assertSequenceEqual(\n            qs, [self.b1, self.b2, self.b3, self.b4],\n            lambda a: a.rating\n        ) \n", "    def test_alias_ordering_with_subquery(self):\n        qs = Book.objects.alias(\n            top_rating=Subquery(\n                Book.objects.filter(pubdate__year=OuterRef(\"pubdate__year\"))\n                .order_by(\"-rating\")\n                .values(\"rating\")[:1],\n            ),\n        ).order_by(\"top_rating\")\n        self.assertCountEqual(\n            qs,\n            [\n                {\"pubdate__year\": 2008, \"top_rating\": 4.5},\n                {\"pubdate__year\": 2007, \"top_rating\": 4.5},\n                {\"pubdate__year\": 1995, \"top_rating\": 4.0},\n                {\"pubdate__year\": 1991, \"top_rating\": 5.0},\n            ],\n        )\n", "    def test_alias_contains_reserved_keyword(self):\n        msg = (\n            \"Column aliases cannot contain reserved keywords like 'select'.\"\n        )\n        with self.assertRaisesMessage(ValueError, msg):\n            Book.objects.alias(select=Value(1))\n\n", "    def test_alias_with_subquery(self):\n        qs = (\n            Book.objects.filter(\n                publisher=OuterRef(\"pk\")\n            )\n            .alias(\n                publisher_sum=Subquery(\n                    Publisher.objects.filter(\n                        book__pk=OuterRef(\"pk\")\n                    ).annotate(book_count=Count(\"book\")).values(\"book_count\")\n                )\n            )\n            .values(\"publisher_sum\")\n        )\n        self.assertCountEqual(qs, [{\"publisher_sum\": 1}, {\"publisher_sum\": 1}, {\"publisher_sum\": 1}, {\"publisher_sum\": 1}, {\"publisher_sum\": 1}])\n", "    def test_alias_forbidden_chars(self):\n        tests = [\n            'al\"ias',\n            \"a'lias\",\n            \"ali`as\",\n            \"alia s\",\n            \"alias\\t\",\n            \"ali\\nas\",\n            \"alias--\",\n            \"ali/*as\",\n            \"alias*/\",\n            \"alias;\",\n            # [] are used by MSSQL.\n            \"alias[\",\n            \"alias]\",\n        ]\n        msg = (\n            \"Column aliases cannot contain whitespace characters, quotation marks, \"\n            \"semicolons, or SQL comments.\"\n        )\n        for crafted_alias in tests:\n            with self.subTest(crafted_alias):\n                with self.assertRaisesMessage(ValueError, msg):\n                    Book.objects.alias(**{crafted_alias: Value(1)})\n"], "sample_721": ["def test_check_array_non_numeric_dtype():\n    X = np.array([[1, 2], [3, 4]], dtype=object)\n    X[0, 0] = 'a'\n\n    assert_raises(TypeError, check_array, X)\n    try:\n        check_array(X, dtype='object')\n    except TypeError as e:\n        assert_equal(str(e), \"Input contains data of type 'object' \"\n                      \"which is not a numeric type.\")\n\n\n\n", "    def test_check_memory_with_custom_cachedir():\n        memory = check_memory(cachedir=\"my_custom_cachedir\")\n        assert_equal(memory.cachedir, os.path.join('my_custom_cachedir',\n                'joblib'))\n", "    def test_cached_result():\n        from joblib import Memory\n\n        memory = Memory(\"cache_directory\", verbose=0)\n        @memory.cache\n            return x * 2\n\n        result = my_func(3)\n        assert_equal(result, 6)\n        result = my_func(3)  # Cache is used\n        assert_equal(result, 6)\n        \n\n        # Test cleaning the cache\n        memory.clear()\n        result = my_func(3)\n        assert_equal(result, 6)\n\n        # Test error handling\n        @memory.cache\n            raise ValueError(\"This function always fails\")\n        assert_raises(ValueError, my_func, 3)\n", "def test_check_array_dtype_nan_inf():\n    X = np.array([\n        [np.nan, 1, 2],\n        [3, 4, np.inf]\n    ])\n    assert_raises_regex(ValueError, \"Input contains NaN and/or inf\",\n                        check_array, X)\n\n    X_int = np.array([\n        [1, 2, 3],\n        [4, 5, 6]\n    ])\n\n    X_float = X_int.astype(float)\n    assert_all_finite(X_float)\n    assert_raises_regex(ValueError, \"Input contains NaN and/or inf\",\n                        check_array, X_float, allow_nan=False)\n\n    X_float[0, 0] = np.nan\n    assert_raises_regex(ValueError, \"Input contains NaN and/or inf\",\n                        check_array, X_float, allow_nan=False)\n\n    X_float[0, 0] = np.inf\n    assert_raises_regex(ValueError, \"Input contains NaN and/or inf\",\n                        check_array, X_float, allow_inf=False)\n\n\n\n", "    def test_check_array_one_hot_encoder():\n        \n        X = [[1, 2], [3, 4]]\n        X_oh = OneHotEncoder(sparse=False).fit_transform(X)\n\n        # Test with a dummy estimator\n        dummy_estimator = object()\n        dummy_estimator.fit = lambda *args, **kwargs: None\n        \n        assert_no_warnings(check_array, X_oh, estimator=dummy_estimator)\n\n\n"], "sample_965": ["def test_getfullargspec_with_defaults():\n        pass\n\n    argspec = inspect.getfullargspec(func)\n    assert argspec.args == ['a', 'b', ']args', 'kwargs']\n    assert argspec.varargs == 'args'\n    assert argspec.varkw == 'kwargs'\n    assert argspec.defaults == (0,)\n", "compilation error", "def test_is_builtin_class_method_with_super():\n    class MyInt(int):\n            return super().to_bytes(length, byteorder, signed)\n\n    assert inspect.is_builtin_class_method(MyInt, 'to_bytes')  \n", "def test_getargspec_missing_defaults():\n        pass\n\n    argspec = inspect.getargspec(func)\n    assert argspec.args == ['a', 'b', 'c']\n    assert argspec.defaults == ()\n    assert argspec.varargs is None\n    assert argspec.keywords is None\n\n\n", "def test_classify_class_method():\n    from typing import Callable, List\n    from inspect import classify_class_method\n\n        pass\n\n        pass\n\n        pass\n\n        pass\n\n    assert classify_class_method(classmethod1) == 'classmethod'\n    assert classify_class_method(classmethod2) == 'classmethod'\n    assert classify_class_method(staticmethod1) == 'staticmethod'\n    assert classify_class_method(instancemethod1) is None\n"], "sample_448": ["    def test_custom_error_message(self):\n        constraint = models.UniqueConstraint(\n            fields=[\"name\"],\n            name=\"name_uniq\",\n            violation_error_message=\"This name already exists!\",\n        )\n        msg = \"This name already exists!\"\n        with self.assertRaisesMessage(ValidationError, msg):\n            constraint.validate(UniqueConstraintProduct, UniqueConstraintProduct(name=\"p1\"))\n\n\n\n", "    def test_name_conflict(self):\n        with self.assertRaisesMessage(\n            ValueError, \"A unique constraint with the name 'uniq' already exists.\"\n        ):\n            models.UniqueConstraint(fields=[\"name\"], name=\"uniq\")\n        UniqueConstraintProduct._meta.add_field(\n            models.CharField(max_length=100, name=\"name\")\n        )\n        models.UniqueConstraint(fields=[\"name\"], name=\"uniq\")\n", "    def test_constraint_violation_with_expressions(self):\n        constraint = models.UniqueConstraint(\n            Lower(\"name\").desc(), Lower(\"age\").asc(), name=\"name_age_uniq_desc\"\n        )\n        msg = \"Constraint \u201cname_age_uniq_desc\u201d is violated.\"\n        with self.assertRaisesMessage(ValidationError, msg):\n            constraint.validate(\n                UniqueConstraintWithExpresion,\n                UniqueConstraintWithExpresion(name=\"john\", age=30),\n            )\n        with self.assertRaisesMessage(ValidationError, msg):\n            constraint.validate(\n                UniqueConstraintWithExpresion,\n                UniqueConstraintWithExpresion(name=\"john\", age=30),\n            )\n        constraint.validate(\n            UniqueConstraintWithExpresion,\n            UniqueConstraintWithExpresion(name=\"jane\", age=20),\n        )\n        constraint.validate(\n            UniqueConstraintWithExpresion,\n            UniqueConstraintWithExpresion(name=\"john\", age=20),\n        )\n\n\n\n", "    def test_opclasses_on_fields(self):\n        msg = \"UniqueConstraint.opclasses must be a list or tuple.\"\n        with self.assertRaisesMessage(ValueError, msg):\n            models.UniqueConstraint(\n                fields=[\"field\"], name=\"uniq_opclasses\", opclasses=\"jsonb_path_ops\"\n            )\n", "    def test_no_duplicate_constraints(self):\n        with self.assertRaisesMessage(\n            ValueError,\n            \"A unique constraint cannot be defined on a field that already has a unique constraint.\",\n        ):\n            models.UniqueConstraint(\n                fields=[\"name\"],\n                name=\"name_uniq_2\",\n                model=UniqueConstraintProduct,\n            )\n"], "sample_728": ["def test_make_multilabel_classification_with_zeros():\n    \"\"\"\n    Test whether make_multilabel_classification handles cases with multiple zeros in y.\n\n    \"\"\"\n    X, Y = make_multilabel_classification(\n        n_samples=5, n_features=3, n_classes=3,\n        allow_unlabeled=False, return_distributions=False)\n    for sample_y in Y:\n        assert_true(np.any(sample_y))  # Ensure at least one label is set\n\n\n\n", "def test_make_classification_empty_data():\n    for n_samples in [0, 1]:\n        for weights in [None, [0.1, 0.9]]:\n            X, y = make_classification(n_samples=n_samples,\n                                        n_features=10, n_informative=3,\n                                        n_redundant=0, n_clusters_per_class=1,\n                                        random_state=0, weights=weights)\n            assert_equal(X.shape, (n_samples, 10), 'X shape mismatch')\n            assert_equal(y.shape, (n_samples,), 'y shape mismatch')\n            if n_samples == 0:\n                assert_equal(len(y), 0)\n", "def test_make_checkerboard_custom_shape():\n    X, rows, cols = make_checkerboard(shape=(7, 8), n_clusters=(6, 3),\n                                       shuffle=True, random_state=0)\n    assert_equal(X.shape, (7, 8), \"X shape mismatch\")\n    assert_equal(rows.shape, (7, 8), \"rows shape mismatch\")\n    assert_equal(cols.shape, (7, 8,), \"columns shape mismatch\")\n\n", "def test_make_multilabel_classification_return_indicators():\n    for allow_unlabeled, min_length in zip((True, False), (0, 1)):\n        X, Y, p_c, p_w_c = make_multilabel_classification(n_samples=25,\n                n_features=20, n_classes=3, random_state=0,\n                allow_unlabeled=allow_unlabeled,\n                return_distributions=True)\n        assert_equal(X.shape, (25, 20), \"X shape mismatch\")\n        assert_equal(Y.shape, (25, 3), \"Y shape mismatch\")\n        assert_equal(p_c.shape, (3,))\n        assert_almost_equal(p_c.sum(), 1)\n        assert_equal(p_w_c.shape, (20, 3))\n        assert_almost_equal(p_w_c.sum(axis=0), [1] * 3)\n\n\n\n", "def test_make_biclusters_with_noise():\n    X, rows, cols = make_biclusters(\n        shape=(100, 100), n_clusters=4, shuffle=True, noise=0.1, random_state=0)\n    assert_equal(X.shape, (100, 100), \"X shape mismatch\")\n    assert_equal(rows.shape, (4, 100), \"rows shape mismatch\")\n    assert_equal(cols.shape, (4, 100,), \"columns shape mismatch\")\n    assert_all_finite(X)\n    assert_all_finite(rows)\n    assert_all_finite(cols)\n\n\n"], "sample_163": ["    def test_password_change_with_auth_token(self):\n        u = UUIDUser.objects.create_superuser(\n            username=\"uuid\", email=\"foo@bar.com\", password=\"test\"\n        )\n        self.assertTrue(self.client.login(username=\"uuid\", password=\"test\"))\n        response = self.client.get(reverse(\"custom_user_admin:auth_tests_uuiduser_change\", args=(u.pk,)))\n        self.assertEqual(response.status_code, 200)\n\n        password_change_url = reverse(\n            \"custom_user_admin:auth_user_password_change\", args=(u.pk,)\n        )\n        response = self.client.post(\n            password_change_url,\n            {\n                \"password1\": \"password1\",\n                \"password2\": \"password1\",\n            },\n        )\n        self.assertRedirects(response, reverse(\"custom_user_admin:auth_tests_uuiduser_change\", args=(u.pk,)))\n\n        \n        # ... (rest of the test as in the previous test)\n\n", "    def test_user_change_password_invalid_form_data(self):\n        u = UUIDUser.objects.get(username=\"uuid\")\n        response = self.client.post(\n            reverse(\"custom_user_admin:auth_tests_uuiduser_password_change\", args=(u.pk,)),\n            {\"password1\": \"password1password1\", \"password2\": \"password1\"},\n        )\n        self.assertContains(response, \"Invalid password.\")\n", "    def test_user_change_list_filters(self):\n        # Create a couple of users for filtering\n        user1 = UUIDUser.objects.create_user(username=\"user-1\", email=\"user1@example.com\")\n        user2 = UUIDUser.objects.create_user(username=\"user-2\", email=\"user2@example.com\")\n\n        # Filter by username\n        response = self.client.get(\n            reverse(\"custom_user_admin:auth_tests_uuiduser_changelist\")\n            + \"?username__startswith=user\"\n        )\n        self.assertContains(response, user1.username)\n        self.assertContains(response, user2.username)\n\n        # Filter by email\n        response = self.client.get(\n            reverse(\"custom_user_admin:auth_tests_uuiduser_changelist\")\n            + \"?email__contains=example.com\"\n        )\n        self.assertContains(response, user1.email)\n        self.assertContains(response, user2.email)\n\n        # Filter by both username and email\n        response = self.client.get(\n            reverse(\"custom_user_admin:auth_tests_uuiduser_changelist\")\n            + \"?username__startswith=user&email__contains=example.com\"\n        )\n        self.assertContains(response, user1.username)\n        self.assertContains(response, user1.email)\n        self.assertNotContains(response, user2.username)\n        self.assertNotContains(response, user2.email)\n\n\n\n", "    def test_custom_user_model_with_related_fields(self):\n        from auth_tests import CustomUser, Address  \n        CustomUser.objects.create_superuser(\n            username=\"uuid\", email=\"foo@bar.com\", password=\"test\"\n        )\n\n        # This test assumes that 'address' is a related field in CustomUser\n        address = Address.objects.create(\n            street=\"123 Main St\", city=\"Anytown\", country=\"USA\"\n        )\n        user = CustomUser.objects.get(username=\"uuid\")\n        user.address = address\n        user.save()\n\n        user_change_url = reverse(\n            \"custom_user_admin:auth_tests_uuiduser_change\", args=(user.pk,)\n        )\n        response = self.client.get(user_change_url)\n        self.assertEqual(response.status_code, 200)\n\n        # Make sure the related field is rendered in the change form\n        self.assertContains(response, 'name=\"address_id\">')\n        self.assertContains(response, 'value=\"%s\"' % address.id)\n\n\n\n", "    def test_user_change_password_with_extra_fields(self):\n        \"\"\"\n        Test that password change works when the user model has extra fields.\n\n        This test adds a custom field to the UUIDUser model and ensures that\n        the password change view still functions correctly.\n        \"\"\"\n        u = UUIDUser.objects.create_superuser(\n            username=\"uuid\", email=\"foo@bar.com\", password=\"test\"\n        )\n\n        # Add a custom field to the UUIDUser model.\n        class ExtraField(models.CharField):\n                super().__init__(max_length=255, *args, **kwargs)\n\n        class CustomUser(UUIDUser):\n            extra = ExtraField(blank=True, null=True)\n\n        CustomUser.objects.create(username=\"new_user\", email=\"new@example.com\", password=\"password\")\n        self.assertTrue(self.client.login(username=\"uuid\", password=\"test\"))\n        \n        # Change the default user model\n        settings.AUTH_USER_MODEL = \"auth_tests.CustomUser\"\n\n        user_change_url = reverse(\n            \"custom_user_admin:auth_tests_customuser_change\", args=(u.pk,)\n        )\n        response = self.client.get(user_change_url)\n        self.assertEqual(response.status_code, 200)\n        self.assertContains(response, '<input type=\"text\" name=\"extra\"')  \n\n        password_change_url = reverse(\n            \"custom_user_admin:auth_user_password_change\", args=(u.pk,)\n        )\n        response = self.client.get(password_change_url)\n        response = self.client.post(password_change_url, {\"password1\": \"password1\", \"password2\": \"password1\"})\n        self.assertRedirects(response, user_change_url)\n        row = LogEntry.objects.latest(\"id\")\n        self.assertEqual(row.user_id, 1) \n        self.assertEqual(row.object_id, str(u.pk))\n        self.assertEqual(row.get_change_message(), \"Changed"], "sample_491": ["    def test_custom_field_template_with_attributes(self):\n        class MyForm(Form):\n            first_name = CharField(template_name=\"forms_tests/custom_field_with_attributes.html\")\n\n        f = MyForm()\n        self.assertHTMLEqual(\n            f.render(),\n            '<div><label for=\"id_first_name\" class=\"required\">First name:</label>'\n            '<p>Custom Field with Attributes</p>'\n            '<input type=\"text\" name=\"first_name\" class=\"my-class\" required id=\"id_first_name\"></div>',\n        )\n", "    def test_field_errors_with_custom_template(self):\n        class CustomErrorList(ErrorList):\n            template_name = \"forms_tests/error.html\"\n\n        class CommentForm(Form):\n            name = CharField(max_length=50, required=False)\n            email = EmailField()\n            comment = CharField()\n\n        data = {\"email\": \"invalid\"}\n        f = CommentForm(data, auto_id=False, error_class=CustomErrorList)\n        self.assertHTMLEqual(\n            f.as_p(),\n            '<p>Name: <input type=\"text\" name=\"name\" maxlength=\"50\"></p>'\n            '<div class=\"errorlist\">'\n            '<div class=\"error\">Enter a valid email address.</div></div>'\n            \"<p>Email: \"\n            '<input type=\"email\" name=\"email\" value=\"invalid\" maxlength=\"320\" required>'\n            '</p><div class=\"errorlist\">'\n            '<div class=\"error\">This field is required.</div></div>'\n            '<p>Comment: <input type=\"text\" name=\"comment\" required></p>',\n        )\n", "    def test_template_autoescape(self):\n        class MyForm(Form):\n            description = CharField()\n\n        t = Template(\"{{ form.description.as_p }}\")\n        form = MyForm({'description': '<b>bold</b>'})\n        html = t.render(Context({\"form\": form}))\n        # Check that the '<b>' tag is escaped in the rendered HTML.\n        self.assertHTMLEqual(html, '<p><b>&lt;b&gt;</b></p>')\n\n", "    def test_form_template_context(self):\n        class ContactForm(Form):\n            name = CharField()\n            email = EmailField()\n            message = Textarea(attrs={\"rows\": 4, \"cols\": 50})\n\n        t = Template(\n            \"{{ form.as_p }}\"\n            \"{{ form.name.errors.as_ul }}\"\n            \"{{ form.email.errors.as_ul }}\"\n            \"{{ form.message.errors.as_ul }}\"\n        )\n\n        f = ContactForm(auto_id=False)\n        html = t.render(Context({\"form\": f}))\n        self.assertHTMLEqual(\n            html,\n            \"<p>Name: <input type='text' name='name' required></p>\"\n            \"<ul class='errorlist'></ul>\"\n            \"<p>Email: <input type='email' name='email' required></p>\"\n            \"<ul class='errorlist'></ul>\"\n            \"<p>Message: <textarea name='message' rows='4' cols='50'></textarea></p>\"\n            \"<ul class='errorlist'></ul>\",\n        )\n\n", "    def test_form_rendering_with_static_file_path(self):\n        class CustomForm(Form):\n            name = CharField()\n            template_name = 'forms_tests/custom_template.html'\n\n        with self.subTest(f_path='static/custom_template.html'):\n            with open('forms_tests/custom_template.html', 'w') as f:\n                f.write('<div class=\"custom-style\">Custom Template</div>')\n            renderer = CustomRenderer(static_url_path='/static/')\n            with self.override_settings(FORM_RENDERER=renderer):\n                f = CustomForm()\n                html = str(f)\n                self.assertHTMLEqual(html, '<div class=\"custom-style\">Custom Template</div>')\n        # Make sure the file is removed\n        os.remove('forms_tests/custom_template.html')\n"], "sample_1084": ["compilation error", "compilation error", "def test_complex_region_from_real_interval():\n    a = Interval(1, 2)\n    b = ComplexRegion.from_real(a)\n    assert b == ComplexRegion(a * Interval(0, 2*S.Pi)) \n    assert b.args[0] == a \n    assert b.args[1] == Interval(0, 2*S.Pi)\n    assert b.func == ComplexRegion.from_real\n\n    raises(ValueError, lambda: ComplexRegion.from_real(S.Complexes))\n    raises(ValueError, lambda: ComplexRegion.from_real(S.Integers))\n", "def test_imageset_with_function_domain():\n    from sympy.abc import x\n    f = lambda x: x**2 + 1\n    assert imageset(f, Interval(-2, 2)) == Interval(1, 5)\n    assert imageset(f, S.Integers) == S.Naturals0\n    assert imageset(f, S.Reals) == Interval(1, oo)\n    assert imageset(f, FiniteSet(-1, 0, 1)) == FiniteSet(0, 2, 2)\n    assert imageset(f, S.Complexes) == S.Complexes\n", "def test_issue_17070():\n    from sympy.abc import z\n    assert ImageSet(Lambda(z, z**2), S.Complexes).intersection(\n        ComplexRegion(Interval(-1, 1)*Interval(-1, 1))\n    ) == ImageSet(Lambda(z, z**2), S.Reals)\n\n\n\n"], "sample_725": ["compilation error", "def test_check_x_y_type():\n    X = [[1, 2], [3, 4]]\n    y = [5, 6]\n\n    assert_equal(check_X_y(X, y).dtype, X.dtype)\n    assert_equal(check_X_y(X, y).dtype, y.dtype)\n\n    X = np.array(X, dtype=np.float32)\n    y = np.array(y, dtype=np.int32)\n    assert_equal(check_X_y(X, y).dtype, X.dtype)\n    assert_equal(check_X_y(X, y).dtype, y.dtype)\n\n\n\n    X = pd.DataFrame(X)\n    y = pd.Series(y)\n    assert_equal(check_X_y(X, y).dtype, X.values.dtype)\n    assert_equal(check_X_y(X, y).dtype, y.values.dtype)\n\n    X = []\n    y = []\n    assert_raises(ValueError, check_X_y, X, y)\n\n    X = [1, 2]\n    y = [3, 4]\n    assert_raises(ValueError, check_X_y, X, y)\n", "def test_check_array_invalid_dtypes():\n    # Test that invalid dtypes raise ValueError\n    X = np.array([[1, 2], [3, 4]], dtype=object)\n    for invalid_dtype in [object, bytes]:\n        assert_raises_regex(ValueError,\n                            r\"Invalid dtype '(\\w+)' for array.\",\n                            check_array, X, dtype=invalid_dtype)\n\n    # Test that unsupported sparse dtype raise TypeError\n    X_csr = sp.csr_matrix([[1, 2], [3, 4]], dtype=object)\n    assert_raises_regex(TypeError,\n                        r\"Unsupported sparse dtype '(\\w+)' for array.\",\n                        check_array, X_csr)\n\n", "def test_check_array_dtype_deprecation():\n    from sklearn.base import BaseEstimator\n\n    class MyEstimator(BaseEstimator):\n            self.sample_weight = None\n\n            self.sample_weight = X\n            return self\n\n    X = np.array([[1, 2], [3, 4]])\n\n    # test that deprecated 'dtype' parameter is ignored\n    msg = r\"The 'dtype' parameter in check_array is deprecated since 0\\.24.\"\n    with assert_warns_message(DeprecationWarning, msg,\n                              check_array, X, dtype=np.float64):\n        check_array(X, dtype=np.float64)\n\n    # test that a TypeError is raised when a non-callable is passed\n    assert_raises_regex(TypeError,\n                        \"The 'dtype' parameter in check_array is deprecated since 0\\.24\",\n                        check_array, X, dtype=MyEstimator())\n\n", "def test_check_array_multidimensional_data():\n    # Test that multidimensional arrays are handled as expected\n    X = np.arange(24).reshape((2, 3, 4))\n    check_array(X)\n    check_array(X, ensure_2d=False)\n\n    X_list = [[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]]\n    check_array(X_list)\n    check_array(X_list, ensure_2d=False)\n\n    # Sparse matrices should be handled correctly\n    X_csr = sp.csr_matrix(X)\n    check_array(X_csr, accept_sparse=True)\n\n    # Different kinds of sparse matrices should be handled correctly\n    X_csc = sp.csc_matrix(X)\n    check_array(X_csc, accept_sparse=True)\n    X_coo = sp.coo_matrix(X)\n    check_array(X_coo, accept_sparse=True)\n    X_dok = sp.dok_matrix(X)\n    check_array(X_dok, accept_sparse=True)\n    X_lil = sp.lil_matrix(X)\n    check_array(X_lil, accept_sparse=True)\n\n    # Check that multidimensional arrays with non-standard shapes are handled\n    # appropriately\n    X_weird = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n    check_array(X_weird)\n    check_array(X_weird, ensure_2d=False)\n\n\n\n"], "sample_170": ["    def test_sensitive_variables_with_sensitive_args(self):\n        @sensitive_variables\n            return str(sauce) + str(eggs)\n\n        with self.settings(DEBUG=True):\n            self.assertEqual(test_func(sauce='secret_sauce', eggs='secret_eggs'),\n                             'XXXXXXXXXXXXXXXXXXXXsecret_eggs')\n        with self.settings(DEBUG=False):\n            self.assertEqual(test_func(sauce='secret_sauce', eggs='secret_eggs'),\n                             'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n\n", "    def test_sensitive_variables_decorator_with_nested_function(self):\n        @sensitive_variables\n                return password + secret\n            return inner_function('secret')\n\n        with self.assertRaises(TypeError) as context:\n            outer_function('super_secret')\n        self.assertIn('sensitive_variables() must be called to use it as a decorator', str(context.exception))\n\n", "    def test_sensitive_variables_in_decorated_function_with_string_return_value(self):\n        @sensitive_variables\n            return f'The password is: {password}'\n\n        with self.settings(DEBUG=True):\n            response = self.client.post('/some_url/', data={'password': 'secret'})\n            self.assertEqual(response.status_code, 200)\n            self.assertIn('The password is: XXXXXXXXXXXX', response.content.decode())\n", "    def test_sensitive_variables_with_nested_functions(self):\n        @sensitive_variables\n                return f'password: {password}'\n            return inner_func\n\n        with self.settings(DEBUG=True):\n            response = self.client.get('/test/', HTTP_PASSWORD='secret')\n            self.assertEqual(response.text, 'password: secret')\n\n", "    def test_sensitive_variables_decorator_with_multiple_arguments(self):\n        @sensitive_variables\n            return 'Username: {}, Password: {}, Secret Key: {}'.format(\n                username, password, secret_key\n            )\n\n        with self.settings(DEBUG=True):\n            response = self.client.get('/my_view/', username='john_doe', password='password123', secret_key='my_secret')\n            self.assertNotIn('Password:', response.content)\n            self.assertNotIn('Secret Key:', response.content)\n            self.assertIn('Username:', response.content)\n\n\n\n"], "sample_324": ["    def test_csrf_token_on_404_is_set(self):\n        response = self.client.get('/does not exist/')\n        self.assertEqual(response.status_code, 599)\n        self.assertIn('csrf_token', response.context)\n\n\n\n", "    def test_csrf_token_on_500_stays_constant(self):\n        response = self.client.get('/raise_error/')\n        self.assertEqual(response.status_code, 500)\n        token1 = response.content\n        response = self.client.get('/raise_error/')\n        self.assertEqual(response.status_code, 500)\n        token2 = response.content\n        self.assertTrue(equivalent_tokens(token1.decode('ascii'), token2.decode('ascii')))\n\n\n", "    def test_csrf_token_on_500_stays_constant(self):\n        response = self.client.get('/error/')\n        # The error handler returns status code 500.\n        self.assertEqual(response.status_code, 500)\n        token1 = response.content\n        response = self.client.get('/error/')\n        self.assertEqual(response.status_code, 500)\n        token2 = response.content\n        self.assertTrue(equivalent_tokens(token1.decode('ascii'), token2.decode('ascii')))\n", "    def test_csrf_token_on_500_error_stays_constant(self):\n        response = self.client.get('/error/')\n        # The error handler returns a 500 status code here.\n        self.assertEqual(response.status_code, 500)\n        token1 = response.content\n        response = self.client.get('/error/')\n        self.assertEqual(response.status_code, 500)\n        token2 = response.content\n        self.assertTrue(equivalent_tokens(token1.decode('ascii'), token2.decode('ascii')))\n\n\n", "    def test_csrf_token_on_500_stays_constant(self):\n        response = self.client.get('/internal_server_error/')\n        # The error handler returns status code 599.\n        self.assertEqual(response.status_code, 599)\n        token1 = response.content\n        response = self.client.get('/internal_server_error/')\n        self.assertEqual(response.status_code, 599)\n        token2 = response.content\n        self.assertTrue(equivalent_tokens(token1.decode('ascii'), token2.decode('ascii')))\n"], "sample_82": ["    def test_render_invalid_datetime(self):\n        with self.assertRaises(ValueError):\n            self.widget.render('mydate', 'banana')\n\n        with self.assertRaises(ValueError):\n            self.widget.render('mydate', '2023/13/1')\n\n\n", "    def test_validating_date_errors(self):\n        widget = SelectDateWidget()\n        form = Form({'mydate': 'invalid'})\n        self.assertFalse(form.is_valid())\n        self.assertIn('mydate', form.errors)\n        self.assertEqual(len(form.errors['mydate']), 1)\n        self.assertEqual(form.errors['mydate'][0], 'Invalid date format.')\n\n        form = Form({'mydate': '2023-13-01'})\n        self.assertFalse(form.is_valid())\n        self.assertIn('mydate', form.errors)\n        self.assertEqual(len(form.errors['mydate']), 1)\n        self.assertEqual(form.errors['mydate'][0], 'Invalid date format.')\n\n        form = Form({'mydate': '2023-02-30'})\n        self.assertTrue(form.is_valid())\n\n\n\n\n", "    def test_widget_context(self):\n        self.widget = SelectDateWidget(years=('2007', '2008'))\n\n        context = {'request': MockRequest(), 'my_date': datetime.date(2007, 10, 25)}\n        rendered_html = self.widget.render('my_date', context['my_date'], context=context)\n        self.assertInHTML('<input type=\"hidden\" name=\"my_date_year\" value=\"2007\">', rendered_html)\n        self.assertInHTML('<input type=\"hidden\" name=\"my_date_month\" value=\"10\">', rendered_html)\n        self.assertInHTML('<input type=\"hidden\" name=\"my_date_day\" value=\"25\">', rendered_html)\n\n", "    def test_format_value_with_invalid_year(self):\n        with self.assertRaises(ValueError):\n            self.widget.format_value('2000-1-1-1')\n\n\n\n", "    def test_invalid_date_format(self):\n        for invalid_date in ['2023-13-15', '2023-02-30', '2023-02-00', 'abc-12-13']:\n            with self.subTest(invalid_date=invalid_date):\n                self.assertEqual(self.widget.format_value(invalid_date), {'day': None, 'month': None, 'year': None})\n\n"], "sample_762": ["        def __init__(self, other):\n            self.other = other\n", "    def test_pickle_version_warning_is_not_raised_with_matching_version():\n        iris = datasets.load_iris()\n        tree = DecisionTreeClassifier().fit(iris.data, iris.target)\n        tree_pickle = pickle.dumps(tree)\n        assert b\"version\" in tree_pickle\n        tree_restored = assert_no_warnings(pickle.loads, tree_pickle)\n\n        # test that we can predict with the restored decision tree classifier\n        score_of_original = tree.score(iris.data, iris.target)\n        score_of_restored = tree_restored.score(iris.data, iris.target)\n        assert_equal(score_of_original, score_of_restored)\n", "def test_clone_sparse_matrix_creation():\n    # Test cloning works properly when the attribute is a sparse matrix created \n    # after instantiation. \n\n    class TestSparseCreation(BaseEstimator):\n            self.data = data\n\n            self.sparse_matrix = sp.csr_matrix(self.data)\n            return self\n        \n    data = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n    est = TestSparseCreation(data)\n    est.fit(data, data)\n    cloned_est = clone(est)\n\n    assert_array_equal(est.sparse_matrix.toarray(), cloned_est.sparse_matrix.toarray())\n", "def test_pickling_with_complex_params():\n    from sklearn.linear_model import LogisticRegression\n\n    # Test pickling with a complex parameter object\n    clf = LogisticRegression(random_state=np.random.RandomState(42),\n                             penalty=\"l2\",\n                             solver=\"newton-cg\",\n                             class_weight=\"balanced\",\n                             C=0.1)\n    # Ensure that the pickle output contains the complex parameter object\n    serialized = pickle.dumps(clf)\n    assert b\"{'random_state':\" in serialized\n    assert b\"penalty:'l2'\" in serialized\n    assert b\"solver:'newton-cg'\" in serialized\n    assert b\"class_weight:'balanced'\" in serialized\n    assert b\"C:0.1\" in serialized  \n\n    # Unpickle the estimator and verify that the parameters are the same\n    clf_restored = pickle.loads(serialized)\n    assert_equal(clf_restored.random_state.randint(0, 10),\n                 clf.random_state.randint(0, 10))\n    assert_equal(clf_restored.penalty, clf.penalty)\n    assert_equal(clf_restored.solver, clf.solver)\n    assert_equal(clf_restored.class_weight, clf.class_weight)\n    assert_equal(clf_restored.C, clf.C)\n", "    def test_pickle_version_warning_with_multiple_estimators():\n        iris = datasets.load_iris()\n\n        # Create a pipeline with mixed versions\n        tree_old = DecisionTreeClassifier().fit(iris.data, iris.target)\n        tree_new = DecisionTreeClassifier().fit(iris.data, iris.target)\n        pipeline = Pipeline([\n            (\"estimators\", [tree_old, tree_new])\n        ])\n        pickle_data = pickle.dumps(pipeline)\n        assert_warns_message(UserWarning,\n                             pickle_error_message.format(\n                                 estimator=\"Pipeline\",\n                                 old_version=\"pre-0.18\",\n                                 current_version=sklearn.__version__\n                             ),\n                             pickle.loads, pickle_data)\n"], "sample_414": ["    def test_ManyToMany_using_to_field(self):\n        from selenium.webdriver.common.by import By\n        from selenium.webdriver.support.ui import Select\n\n        self.admin_login(username=\"super\", password=\"secret\", login_url=\"/\")\n        self.selenium.get(\n            self.live_server_url + reverse(\"admin:admin_widgets_profile_add\")\n        )\n\n        main_window = self.selenium.current_window_handle\n        self.selenium.find_element(By.ID, \"add_id_user\").click()\n        self.wait_for_and_switch_to_popup()\n        password_field = self.selenium.find_element(By.ID, \"id_password\")\n        password_field.send_keys(\"password\")\n\n        username_field = self.selenium.find_element(By.ID, \"id_username\")\n        username_value = \"newuser\"\n        username_field.send_keys(username_value)\n\n        save_button_css_selector = \".submit-row > input[type=submit]\"\n        self.selenium.find_element(By.CSS_SELECTOR, save_button_css_selector).click()\n        self.selenium.switch_to.window(main_window)\n        self.selenium.find_element(\n            By.CSS_SELECTOR, \"#id_user option[value=newuser]\"\n        )\n\n        self.selenium.find_element(\n            By.ID, \"add_id_group\"\n        ).click()\n        self.wait_for_and_switch_to_popup()\n        # Make sure the \"groups\" popup has the correct default\n        self.assertIn(\"Groups\", self.selenium.title)\n\n        self.wait_for_and_switch_to_popup()\n        self.selenium.find_element(By.CSS_SELECTOR, \"#id_group option[value=1]\").click()\n\n        self.selenium.find_element(By.CSS_SELECTOR, \".submit-row > input[type=submit]\").click()\n\n        self.selenium.switch_to.window(main_window)\n        self.selenium.find_element(\n            By.CSS_SELECTOR, \"#id", "    def test_related_field_widget_autocomplete(self):\n        from selenium.webdriver.common.by import By\n        from django.test.utils import override_settings\n\n        with override_settings(\n            INSTALLED_APPS=[\n                \"django.contrib.auth\",\n                \"django.contrib.contenttypes\",\n                \"django.contrib.sessions\",\n                \"django.contrib.messages\",\n                \"django.contrib.staticfiles\",\n                \"widget_test\",\n            ]\n        ):\n            User.objects.create_user(username=\"user1\", email=\"user1@example.com\")\n            User.objects.create_user(username=\"user2\", email=\"user2@example.com\")\n            User.objects.create_user(username=\"user3\", email=\"user3@example.com\")\n\n            self.admin_login(username=\"super\", password=\"secret\", login_url=\"/\")\n            self.selenium.get(\n                self.live_server_url + reverse(\"admin:admin_widgets_profile_add\")\n            )\n\n        # Input text in the related field, triggering the autocomplete\n        self.selenium.find_element(By.ID, \"id_related_user\").send_keys(\"user\")\n\n        # Wait for the autocomplete to appear\n        self.wait_for_elements_visible(By.CSS_SELECTOR, \".autocomplete-choices\")\n\n        # Select a user from the autocomplete list\n        self.selenium.find_element(By.CSS_SELECTOR, \".autocomplete-choices :nth-child(1)\").click()\n\n        # Verify the selected user is displayed in the related field\n        self.wait_for_value(\n            \"#id_related_user\", \"user1\"\n        )\n\n        # Submit the form to save the changes\n        self.selenium.find_element(By.CSS_SELECTOR, \".submit-row > input[type=submit]\").click()\n\n\n\n", "    def test_ManyToManyField_using_to_field(self):\n        from selenium.webdriver.common.by import By\n        from selenium.webdriver.support.ui import Select\n\n        self.admin_login(username=\"super\", password=\"secret\", login_url=\"/\")\n        self.selenium.get(\n            self.live_server_url + reverse(\"admin:admin_widgets_band_add\")\n        )\n\n        main_window = self.selenium.current_window_handle\n        # Click the Add Location button to add new\n        self.selenium.find_element(By.ID, \"add_id_location\").click()\n        self.wait_for_and_switch_to_popup()\n        location_name_field = self.selenium.find_element(By.ID, \"id_name\")\n        location_name_field.send_keys(\"New Location\")\n\n        save_button_css_selector = \".submit-row > input[type=submit]\"\n        self.selenium.find_element(By.CSS_SELECTOR, save_button_css_selector).click()\n        self.selenium.switch_to.window(main_window)\n        # The field now contains the new location\n        self.selenium.find_element(By.CSS_SELECTOR, \"#id_location option[value=New Location]\")\n\n        self.selenium.find_element(By.ID, \"view_id_location\").click()\n        self.wait_for_value(\"#id_name\", \"New Location\")\n        self.selenium.back()\n\n        select = Select(self.selenium.find_element(By.ID, \"id_location\"))\n        select.select_by_value(\"New Location\")\n        # Click the Change Location button to change it\n        self.selenium.find_element(By.ID, \"change_id_location\").click()\n        self.wait_for_and_switch_to_popup()\n\n        location_name_field = self.selenium.find_element(By.ID, \"id_name\")\n        location_name_field.clear()\n        location_name_field.send_keys(\"Changed New Location\")\n\n        save_button_css_selector = \".submit-row > input[type=submit]\"\n        ", "    def test_ManyToMany_using_to_field(self):\n        from selenium.webdriver.common.by import By\n\n        self.admin_login(username=\"super\", password=\"secret\", login_url=\"/\")\n        self.selenium.get(\n            self.live_server_url + reverse(\"admin:admin_widgets_event_add\")\n        )\n\n        main_window = self.selenium.current_window_handle\n\n        # Click Add Band button to add new band\n        self.selenium.find_element(By.ID, \"add_id_band\").click()\n        self.wait_for_and_switch_to_popup()\n        band_name_field = self.selenium.find_element(By.ID, \"id_name\")\n        band_name_field.send_keys(\"New Band\")\n        save_button_css_selector = \".submit-row > input[type=submit]\"\n        self.selenium.find_element(By.CSS_SELECTOR, save_button_css_selector).click()\n        self.selenium.switch_to.window(main_window)\n\n        # The field now contains the new band\n        self.selenium.find_element(By.CSS_SELECTOR, \"#id_band option[value=New Band]\")\n\n        # Click the Change Band button to change it\n        self.selenium.find_element(By.ID, \"change_id_band\").click()\n        self.wait_for_and_switch_to_popup()\n\n        band_name_field = self.selenium.find_element(By.ID, \"id_name\")\n        band_name_field.clear()\n        band_name_field.send_keys(\"Changed Band\")\n\n        save_button_css_selector = \".submit-row > input[type=submit]\"\n        self.selenium.find_element(By.CSS_SELECTOR, save_button_css_selector).click()\n        self.selenium.switch_to.window(main_window)\n        self.selenium.find_element(\n            By.CSS_SELECTOR, \"#id_band option[value=Changed Band]\"\n        )\n\n\n\n", "    def test_many_to_many_with_search(self):\n        from selenium.webdriver.common.by import By\n        from selenium.webdriver.support.ui import Select\n\n        self.admin_login(username=\"super\", password=\"secret\", login_url=\"/\")\n        self.selenium.get(\n            self.live_server_url + reverse(\"admin:admin_widgets_event_add\")\n        )\n        main_window = self.selenium.current_window_handle\n        # Click the Add User button to add new\n        self.selenium.find_element(By.ID, \"lookup_id_supporting_bands\").click()\n        self.wait_for_and_switch_to_popup()\n        search_box = self.selenium.find_element(By.ID, \"lookup_id_supporting_bands_search\")\n        search_box.send_keys(\"Bogey\")\n\n        # Bogey Blues should be the first option\n        self.assertCountSeleniumElements(\n            'div.search-result a', 1,\n        )\n\n        self.selenium.find_element(\n            By.CSS_SELECTOR, 'div.search-result a[href*=\"42/\"]'\n        ).click()\n\n        self.selenium.switch_to.window(main_window)\n        # Check if the selected band is listed\n        self.assertCountSeleniumElements(\"#id_supporting_bands > option\", 1)\n        \n        # Now, add another band\n        self.selenium.find_element(By.ID, \"lookup_id_supporting_bands\").click()\n        # search for a new band that is not 'Bogey Blues'\n        search_box = self.selenium.find_element(By.ID, \"lookup_id_supporting_bands_search\")\n        search_box.send_keys(\"Green\")\n\n        # Green Potatoes should be the first option\n        self.assertCountSeleniumElements(\n            'div.search-result a', 1,\n        )\n\n        self.selenium.find_element(\n            By.CSS_SELECTOR, 'div.search-result a[href*=\"98/\"]'\n        ).click()\n\n        self.selenium.switch_to.window(main_window)\n"], "sample_24": ["    def test_masked_where(self):\n        with pytest.raises(TypeError):\n            np.masked_where(self.ma, np.zeros_like(self.ma))\n", "    def test_nanstd_nanvar(self):\n        self.check(np.nanstd, exact_fill_value=0)\n        out = np.nanvar(self.mb, axis=1)\n        assert_array_equal(out.unmasked, np.var(self.mb, axis=1, nan=True).unmasked)\n        assert_array_equal(out.mask, np.array([False, False]))\n", "    def test_masked_array_casting(self):\n        self.ma = Masked(np.array([1, 2, 3], dtype=np.uint8), mask=[True, False, True])\n        for dtype in [np.int32, np.float32, np.complex64]:\n            with pytest.raises(TypeError):\n                self.ma.astype(dtype)\n", "    def test_nanstd_datetime(self):\n        a = np.array([\"2020-12-31\", \"2021-01-01\", \"2021-01-02\"], dtype=\"M\")\n        mask_a = np.array([False, True, False])\n        ma = Masked(a, mask=mask_a)\n        self.check(np.nanstd, masked_result=False, exact_fill_value=np.nan)\n", "    def test_all_wrapped_functions(self, function):\n        assert function in all_wrapped_functions\n"], "sample_522": ["def test_colorbar_fractional_location():\n    fig, ax = plt.subplots()\n    pc = ax.pcolormesh(np.random.randn(10, 10))\n    cb = fig.colorbar(pc, ax=ax, fraction=0.1)\n    fig.draw_without_rendering()\n    assert 0.0 < cb.ax.get_position().x < 0.15\n    \n    cb = fig.colorbar(pc, ax=ax, fraction=0.2)\n    fig.draw_without_rendering()\n    assert 0.0 < cb.ax.get_position().x < 0.2\n\n\n\n", "def test_colorbar_fractional_ticks():\n    fig, ax = plt.subplots()\n    pc = ax.pcolormesh(np.arange(10).reshape(10, 1), np.arange(10).reshape(10, 1))\n    cb = fig.colorbar(pc, ax=ax, fraction=0.04)\n    assert cb.ax.get_position().width == 0.04 * fig.get_figwidth()\n\n\n\n", "def test_colorbar_tick_positions():\n    fig, ax = plt.subplots()\n    pc = ax.pcolormesh(np.random.randn(10, 10))\n    cb = fig.colorbar(pc, ax=ax, extend='both')\n    ticks = cb.set_ticks(np.linspace(0, 1, 5))\n    assert np.allclose(cb.ax.yaxis.get_ticklocs(), ticks)\n    \n    # Test that tick positions are updated when set_ticks is called again\n    ticks2 = np.linspace(0, 1, 7)\n    cb.set_ticks(ticks2)\n    assert np.allclose(cb.ax.yaxis.get_ticklocs(), ticks2)\n    \n    # Test that the ticks are correctly positioned even with a non-default\n    # aspect ratio\n    cb.ax.set_aspect(0.5)\n    ticks3 = np.linspace(0, 1, 9)\n    cb.set_ticks(ticks3)\n    assert np.allclose(cb.ax.yaxis.get_ticklocs(), ticks3)\n", "def test_colorbar_position():\n    fig, ax = plt.subplots(2, 2)\n    for i, orientation in enumerate(['vertical', 'horizontal']):\n        pc = ax[0, i].pcolormesh(np.arange(10).reshape(10, 10))\n        cb = fig.colorbar(pc, ax=ax[0, i], orientation=orientation,\n                          fraction=0.05)\n        if orientation == 'vertical':\n            assert cb.ax.get_position().height < 0.2\n            assert cb.ax.get_position().width > 0.1\n        else:\n            assert cb.ax.get_position().width < 0.2\n            assert cb.ax.get_position().height > 0.1\n", "def test_colorbar_align_label():\n    fig, ax = plt.subplots()\n    pc = ax.pcolormesh(np.random.randn(10, 10))\n    cb = fig.colorbar(pc, ax=ax, label='test',\n                      ticks=[0, 0.5, 1])\n    \n    cb.ax.yaxis.set_label_coords(1, 0.5)\n    fig.draw_without_rendering()\n    \n    cb = fig.colorbar(pc, ax=ax, label='test',\n                      ticks=[0, 0.5, 1], \n                      labelpad=-10)\n    fig.draw_without_rendering()\n    \n"], "sample_272": ["    def test_minimize_rollbacks_circular(self):\n        \"\"\"\n        Minimize rollbacks in circular dependency scenarios.\n\n        a: 1 <--- 2\n        b:  \\ <-- 1\n\n        If a1 and b1 are applied and we're asked to migrate to a2,\n        don't unapply b1.\n        \"\"\"\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        a2_impl = FakeMigration('a2')\n        a2 = ('a', '2')\n        b1_impl = FakeMigration('b1')\n        b1 = ('b', '1')\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(a2, a2_impl)\n        graph.add_node(b1, b1_impl)\n        graph.add_dependency(None, a2, a1)\n        graph.add_dependency(None, b1, a1)\n\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {\n            a1: a1_impl,\n            b1: b1_impl,\n            a2: a2_impl,\n        })\n\n        plan = executor.migration_plan({a1, b1})\n\n        self.assertEqual(plan, [(a2_impl, True)])\n\n", "    def test_minimize_rollbacks_multiple_apps(self):\n        \"\"\"\n        Minimize rollbacks across multiple apps with dependencies.\n        \"\"\"\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        a2_impl = FakeMigration('a2')\n        a2 = ('a', '2')\n        b1_impl = FakeMigration('b1')\n        b1 = ('b', '1')\n        b2_impl = FakeMigration('b2')\n        b2 = ('b', '2')\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(a2, a2_impl)\n        graph.add_node(b1, b1_impl)\n        graph.add_node(b2, b2_impl)\n        graph.add_dependency(None, b1, a1)\n        graph.add_dependency(None, b2, a2)\n\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {\n            a1: a1_impl,\n            b1: b1_impl,\n            a2: a2_impl,\n            b2: b2_impl,\n        })\n\n        plan = executor.migration_plan({a1, a2})\n\n        self.assertEqual(plan, [(a2_impl, True), (b2_impl, True)])\n", "    def test_minimize_rollbacks_cross_dependencies(self):\n        r\"\"\"\n        Minimize rollbacks when target has cross-dependencies.\n\n        a: 1 <---- 2 <-- 3\n                \\----\\--- b: 1\n                   \\---- 4\n                 \\------ 5\n\n\n        \"\"\"\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        a2_impl = FakeMigration('a2')\n        a2 = ('a', '2')\n        a3_impl = FakeMigration('a3')\n        a3 = ('a', '3')\n        a4_impl = FakeMigration('a4')\n        a4 = ('a', '4')\n        a5_impl = FakeMigration('a5')\n        a5 = ('a', '5')\n        b1_impl = FakeMigration('b1')\n        b1 = ('b', '1')\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(a2, a2_impl)\n        graph.add_node(a3, a3_impl)\n        graph.add_node(a4, a4_impl)\n        graph.add_node(a5, a5_impl)\n        graph.add_node(b1, b1_impl)\n        graph.add_dependency(None, a2, a1)\n        graph.add_dependency(None, a3, a2)\n        graph.add_dependency(None, a4, a3)\n        graph.add_dependency(None, a5, a4)\n        graph.add_dependency(None, b1, a2)\n        graph.add_dependency(None, b1, a5)\n\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {\n            a1: a1_impl,\n            b1: b1_impl,\n        })\n\n        plan = executor.migration_plan({a1})\n\n        self.assertEqual(plan, [(a2_impl, True), (a3_impl, True), (a4_impl, True), (a5_impl, True),", "    def test_minimize_rollbacks_cycle(self):\n        \"\"\"\n        Minimize rollbacks when there's a dependency cycle.\n        #23192\n        a: 1 <--\\\n          \\    2    \\\n           \\     \\   \\\n            \\    \\   \\\n             \\- 3   \\\n                 \\\n                  1 <-- 4\n\n        If a1 and b1 are applied, and a1 depends on b1 and b1 depends on a1,\n        we should not rollback a1 or b1.\n        \"\"\"\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        a2_impl = FakeMigration('a2')\n        a2 = ('a', '2')\n        a3_impl = FakeMigration('a3')\n        a3 = ('a', '3')\n        b1_impl = FakeMigration('b1')\n        b1 = ('b', '1')\n        b2_impl = FakeMigration('b2')\n        b2 = ('b', '2')\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(a2, a2_impl)\n        graph.add_node(a3, a3_impl)\n        graph.add_node(b1, b1_impl)\n        graph.add_node(b2, b2_impl)\n        graph.add_dependency(None, a2, a1)\n        graph.add_dependency(None, a3, a2)\n        graph.add_dependency(None, b2, a3)\n        graph.add_dependency(None, b1, b2)\n\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {\n            a1: a1_impl,\n            b1: b1_impl,\n        })\n\n        plan = executor.migration_plan({a1})\n\n        self.assertEqual(plan, [])\n", "    def test_minimize_rollbacks_unrelated_apps(self):\n        \"\"\"\n        Minimize rollbacks when target app doesn't depend on other apps\n        \"\"\"\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        a2_impl = FakeMigration('a2')\n        a2 = ('a', '2')\n        b1_impl = FakeMigration('b1')\n        b1 = ('b', '1')\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(a2, a2_impl)\n        graph.add_node(b1, b1_impl)\n\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {\n            a1: a1_impl,\n            b1: b1_impl,\n            a2: a2_impl,\n        })\n\n        plan = executor.migration_plan({a1})\n\n        self.assertEqual(plan, [(a2_impl, True)])\n"], "sample_850": ["def test_nystroem_sparse_data():\n    # test Nystroem with sparse data\n    from scipy.sparse import random_matrix\n    n_samples = 1000\n    n_features = 50\n    rnd = np.random.RandomState(42)\n    X = random_matrix(n_samples, n_features, density=0.01)\n    nystroem = Nystroem(n_components=100, random_state=42)\n    X_transformed = nystroem.fit_transform(X)\n    assert X_transformed.shape == (n_samples, 100)\n", "def test_nystroem_sparse_data():\n    # Test Nystroem with sparse data\n    rnd = np.random.RandomState(42)\n    n_samples = 100\n    n_features = 1000\n    X = rnd.rand(n_samples, n_features)\n    X_sparse = csr_matrix(X)\n\n    nystroem = Nystroem(n_components=10, random_state=42)\n    X_transformed = nystroem.fit_transform(X_sparse)\n    assert X_transformed.shape == (n_samples, 10)\n\n\n\n", "def test_nystroem_sparse_input():\n    # Test Nystroem with sparse input\n    from scipy.sparse import csr_matrix\n    rng = np.random.RandomState(42)\n    X = rng.uniform(size=(100, 20))\n    X_sparse = csr_matrix(X)\n\n    ny = Nystroem(n_components=10)\n    X_transformed = ny.fit_transform(X_sparse)\n    assert X_transformed.shape == (X.shape[0], 10)\n\n    # Test transform on sparse input\n    X_sparse_transformed = ny.transform(X_sparse)\n    assert X_sparse_transformed.shape == (X.shape[0], 10)\n\n\n\n", "def test_nystroem_sparse_data():\n    rnd = np.random.RandomState(42)\n    X = rnd.rand(100, 50)\n    X = csr_matrix(X)\n\n    nystroem = Nystroem(n_components=50)\n    X_transformed = nystroem.fit_transform(X)\n\n    assert X_transformed.shape == (X.shape[0], 50)\n\n\n\n", "def test_nystroem_sparse_data():\n    rnd = np.random.RandomState(0)\n    X = rnd.rand(1000, 10)\n    X_sparse = csr_matrix(X)\n    nystroem = Nystroem(n_components=100, random_state=42)\n    X_transformed = nystroem.fit_transform(X_sparse)\n    assert X_transformed.shape == (X_sparse.shape[0], 100)\n\n    # Also test transforming on a sparse matrix\n    X_transformed_sparse = nystroem.transform(X_sparse)\n    assert X_transformed_sparse.shape == (X_sparse.shape[0], 100)\n"], "sample_615": ["    def test_cross_with_different_dimensions() -> None:\n        a = np.arange(12).reshape((3, 4))\n        b = np.arange(12).reshape((4, 3))\n        da_a = xr.DataArray(a, dims=[\"i\", \"j\"])\n        da_b = xr.DataArray(b, dims=[\"j\", \"i\"])\n        with pytest.raises(ValueError, match=r\"a and b must have the same number of\"):\n            xr.cross(da_a, da_b, dim=\"i\")\n\n        with pytest.raises(ValueError, match=r\"a and b must have the same number of\"):\n            xr.cross(da_a, da_b, dim=\"j\")\n", "def test_interp_1d_missing_values() -> None:\n    if use_dask and not has_dask:\n        pytest.skip(\"requires dask\")\n\n    x = np.array([1, 2, 3, 4, 5])\n    y = np.array([1, 4, 9, 16, 25])\n\n    da = xr.DataArray(y, dims=\"x\", coords={\"x\": x})\n\n    # Test interpolation with missing values\n    da_interp = da.interp(0.5, \"x\", method=\"linear\")\n    expected = xr.DataArray(np.array([2.5, 4, 9, 16, 25]), dims=\"x\", coords={\"x\": x})\n    assert_identical(expected, da_interp)\n\n    da_interp = da.interp(1.5, \"x\", method=\"linear\")\n    expected = xr.DataArray(\n        np.array([1, 4, 9, 16, 25]), dims=\"x\", coords={\"x\": x}\n    )\n    assert_identical(expected, da_interp)\n\n    # Test interpolation with missing values using dask\n    if use_dask:\n        da = da.chunk()\n        da_interp = da.interp(0.5, \"x\", method=\"linear\")\n        expected = xr.DataArray(\n            np.array([2.5, 4, 9, 16, 25]), dims=\"x\", coords={\"x\": x}\n        )\n        assert_identical(expected, da_interp)\n\n\n\n\n", "    def test_interp(use_dask) -> None:\n        if use_dask and not has_dask:\n            pytest.skip(\"requires dask\")\n\n        a = xr.DataArray([1, 2, 3, 4, 5], dims=\"x\")\n        x = np.array([0, 1, 2, 3, 4])\n\n        interp_result = xr.interp(a, x, method=\"linear\")\n\n        expected = xr.DataArray(\n            [1, 1.5, 2, 2.5, 3], dims=\"x\"\n        )  \n        assert_identical(interp_result, expected)\n\n        # Test interpolation with dask\n        if use_dask:\n            a = a.chunk()\n\n            interp_result = xr.interp(a, x, method=\"linear\")\n            assert_identical(interp_result, expected)\n\n        with pytest.raises(ValueError):\n            xr.interp(a, x, method=\"none\")\n\n        with pytest.raises(ValueError):\n            xr.interp(a, x, method=\"nearest\")\n\n    ", "def test_interp_1d(use_dask) -> None:\n    if use_dask and not has_dask:\n        pytest.skip(\"requires dask\")\n\n    x = np.array([0, 1, 2, 3, 4])\n    y = np.array([1, 4, 2, 5, 3])\n    da_x = xr.DataArray(x, dims=\"x\")\n    da_y = xr.DataArray(y, dims=\"x\")\n\n    if use_dask:\n        da_x = da_x.chunk()\n        da_y = da_y.chunk()\n\n    interp_da = xr.interp(da_y, da_x, x=np.array([0.5, 1.5, 2.5, 3.5]))\n\n    expected = xr.DataArray([1.5, 3, 3.5, 4.5], dims=\"x\")\n    assert_identical(expected, interp_da)\n", "compilation error"], "sample_209": ["    def test_unique_constraints(self):\n        # Regression test for #1956: Unique constraints with auto-increment\n        # fields should work correctly.\n        with self.assertRaises(IntegrityError):\n            NonAutoPK.objects.create(name=\"unique\")\n            NonAutoPK.objects.create(name=\"unique\") \n", "    def test_ordering_with_related_fields(self):\n        # Test that ordering by related fields works correctly\n        department1 = Department.objects.create(name='abc')\n        department2 = Department.objects.create(name='def')\n\n        worker1 = Worker.objects.create(name='worker1', department=department1)\n        worker2 = Worker.objects.create(name='worker2', department=department1)\n        worker3 = Worker.objects.create(name='worker3', department=department2)\n\n        # Order by department name\n        workers = Worker.objects.order_by('department__name')\n        self.assertEqual([w.name for w in workers], ['worker1', 'worker2', 'worker3'])\n\n        # Order by department pk\n        workers = Worker.objects.order_by('department__pk')\n        self.assertEqual([w.name for w in workers], ['worker1', 'worker2', 'worker3'])\n", "    def test_ordering_with_respect_to(self):\n        model = Article.objects\n        # Create some articles, some with related authors, some without\n        for i in range(5):\n            a = model.create(headline=f'Article {i}', pub_date=datetime.datetime.now())\n            if i % 2 == 0:\n                a.author = Author.objects.create(name=f'Author {i}')\n\n        # Check ordering by a related field\n        ordered_articles = model.filter().order_by_author_name()\n        for i, article in enumerate(ordered_articles):\n            self.assertEqual(article.author.name, f'Author {2 * i}')\n\n        # Check ordering by a related field with reverse\n        reverse_ordered_articles = model.filter().order_by_author_name_reverse()\n        for i, article in enumerate(reverse_ordered_articles):\n            self.assertEqual(article.author.name, f'Author {2 * (4 - i)}')\n\n        \n", "    def test_related_fields_caching(self):\n        \"\"\"\n        Related fields are cached properly.\n        \"\"\"\n        department1 = Department.objects.create(id=1, name='department1')\n        department2 = Department.objects.create(id=2, name='department2')\n        worker1 = Worker.objects.create(name='worker1', department=department1)\n        worker2 = Worker.objects.create(name='worker2', department=department2)\n\n        # First lookup caches the related object\n        self.assertEqual(worker1.department, department1)\n        self.assertIs(worker1.department, department1)  \n\n        # Subsequent lookups on the same instance use the cached object\n        self.assertEqual(worker1.department_name, 'department1')\n        self.assertIs(worker1.department, department1) \n\n        # Changing the related object on a different instance doesn't affect the cache\n        worker2.department = department1\n        self.assertEqual(worker1.department, department1)\n\n\n\n", "    def test_ordering_with_respect_to_respect_field_ordering(self):\n        \"\"\"\n        Regression test for #715: ordering_with_respect_to should respect\n        the ordering of the related fields\n        \"\"\"\n        department1 = Department.objects.create(name='department1')\n        department2 = Department.objects.create(name='department2')\n\n        worker1 = Worker.objects.create(name='worker1', department=department1)\n        worker2 = Worker.objects.create(name='worker2', department=department1)\n        worker3 = Worker.objects.create(name='worker3', department=department2)\n\n        department1.workers.all().order_by('name')\n\n        workers = worker1.department.workers.all().order_by('name')\n        self.assertEqual(list(workers), [worker1, worker2])\n\n        workers = department2.workers.all().order_by('name')\n        self.assertEqual(list(workers), [worker3])\n\n\n\n"], "sample_722": ["def test_k_means_with_categorical_data():\n    from sklearn.datasets import make_blobs\n    X, y = make_blobs(n_samples=300, n_features=5, centers=4,\n                      cluster_std=0.60, random_state=0)\n\n    # Encode categorical features using one-hot encoding\n    X_categorical = np.array([\n        [1, 0, 0, 0, 0],\n        [0, 1, 0, 0, 0],\n        [0, 0, 1, 0, 0],\n        [0, 0, 0, 1, 0],\n        [0, 0, 0, 0, 1],\n    ])\n\n    # Create a categorical feature by duplicating the rows\n    X_categorical = np.vstack((X_categorical, X_categorical))\n\n    # Define a function to convert categorical data to numerical\n        return OrdinalEncoder().fit_transform(X)\n\n    # Fit KMeans with categorical data\n    km = KMeans(n_clusters=4, random_state=0)\n    km.fit(_encode_categorical(X_categorical))\n\n\n", "def test_fit_with_dtype_matching():\n    X = np.array([[1, 2], [3, 4]], dtype=np.float16)\n    km = KMeans(n_clusters=2, random_state=42, dtype=np.float16)\n    km.fit(X)\n    assert_equal(km.cluster_centers_.dtype, np.float16)\n    assert_equal(km.labels_.dtype, np.int32)\n\n\n\n", "def test_minibatch_kmeans_sparse_matrix_fit_predict():\n    mb_k_means = MiniBatchKMeans(n_clusters=3, batch_size=10,\n                random_state=42)\n\n    # fit should work on sparse matrix\n    mb_k_means.fit(X_csr)\n\n    # predict should work on sparse matrix as well\n    X_sparse_predict = X_csr\n    mb_k_means.predict(X_sparse_predict)\n\n\n\n", "def test_min_cluster_size():\n    X = np.array([[1, 2], [1, 3], [2, 2], [2, 3]])\n    for min_cluster_size in [0, 1, 2, 3, 4]:\n        km = KMeans(n_clusters=2, min_cluster_size=min_cluster_size)\n        km.fit(X)\n        assert_equal(len(km.labels_), len(X))\n        assert_equal(np.min(np.bincount(km.labels_)), min_cluster_size)\n\n    km = KMeans(n_clusters=2, min_cluster_size=10)\n    km.fit(X)\n    assert_equal(len(km.labels_), len(X))\n    assert_equal(np.min(np.bincount(km.labels_)), 10)\n\n", "def test_fit_partial_sparse():\n    mb_km = MiniBatchKMeans(n_clusters=n_clusters, batch_size=5,\n                random_state=42).fit(X_csr)\n    # now check that partial_fit works\n    mb_km.partial_fit(X_csr[0:10])\n    assert_array_almost_equal(mb_km.labels_[0:10],\n                              mb_km_original.labels_[0:10])\n\n    # check that partial_fit works even if the data has duplicates\n    mb_km_duplicate = MiniBatchKMeans(n_clusters=n_clusters, batch_size=5,\n                                  random_state=42).fit(X_csr)\n    mb_km_duplicate.partial_fit(X_csr[0:10])\n    mb_km_duplicate.partial_fit(X_csr[0:10])\n    assert_array_almost_equal(mb_km_duplicate.labels_[0:10],\n                              mb_km_original.labels_[0:10])\n\n\n\n"], "sample_388": ["    def test_with_perm(self):\n        \"\"\"\n        Tests the with_perm method.\n        \"\"\"\n        User.objects.create_user(username=\"user\", is_superuser=True)\n        User.objects.create_user(username=\"user2\")\n\n        with self.settings(AUTH_USER_MODEL=\"auth_tests.User\"):\n            with self.subTest(\"with_perm\"):\n                users = self.backend.with_perm(\"auth.view_user\")\n                self.assertIn(\"user\", {user.username for user in users})\n                self.assertNotIn(\"user2\", {user.username for user in users})\n            \n\n            with self.subTest(\"with_perm_includes_superusers\"):\n                users = self.backend.with_perm(\"auth.view_user\", include_superusers=True)\n                self.assertIn(\"user\", {user.username for user in users})\n                self.assertIn(\"user2\", {user.username for user in users})\n\n\n\n            with self.subTest(\"with_perm_is_active\"):\n                users = self.backend.with_perm(\"auth.view_user\", is_active=True)\n                self.assertIn(\"user\", {user.username for user in users})\n                self.assertNotIn(\"user2\", {user.username for user in users})\n\n\n\n\n", "    def test_header_with_non_string_value(self):\n        \"\"\"\n        Tests the response when REMOTE_USER header has a non-string value.\n        \"\"\"\n        response = self.client.get(\"/remote_user/\", **{self.header: 123})\n        self.assertTrue(response.context[\"user\"].is_anonymous)\n\n\n\n", "    def test_email_header_availability(self):\n        \"\"\"\n        Tests whether the email header is accessible and used by the CustomRemoteUserBackend.\n        \"\"\"\n        User.objects.create(username=\"knownuser\")\n        response = self.client.get(\n            \"/remote_user/\",\n            **{\n                self.header: \"knownuser\",\n                self.email_header: \"test@example.com\",\n            },\n        )\n        self.assertEqual(response.context[\"user\"].email, \"test@example.com\")\n", "    def test_user_login_sequence_with_header_removal(self):\n        \"\"\"\n        Tests logging in with the REMOTE_USER header and then authenticating\n        with a different mechanism (username/password) should not create a\n        new user but switch the user.\n        \"\"\"\n        User.objects.create(username=\"knownuser\")\n        User.objects.create(username=\"knownuser2\")\n\n        # Authenticate with REMOTE_USER\n        response = self.client.get(\"/remote_user/\", **{self.header: self.known_user})\n        self.assertEqual(response.context[\"user\"].username, \"knownuser\")\n\n        # Now login with username/password\n        self.client.login(username=\"knownuser2\", password=\"foo\")\n\n        # Verify the user switch\n        response = self.client.get(\"/remote_user/\")\n        self.assertEqual(response.context[\"user\"].username, \"knownuser2\")\n", "    def test_user_switch_forces_new_login(self):\n        \"\"\"\n        If the username in the header changes between requests\n        that the original user is logged out\n        \"\"\"\n        User.objects.create(username=\"knownuser\")\n        # Known user authenticates\n        response = self.client.get(\"/remote_user/\", **{self.header: self.known_user})\n        self.assertEqual(response.context[\"user\"].username, \"knownuser\")\n        # During the session, the REMOTE_USER changes to a different user.\n        response = self.client.get(\"/remote_user/\", **{self.header: \"newnewuser\"})\n        # The current user is not the prior remote_user.\n        # In backends that create a new user, username is \"newnewuser\"\n        # In backends that do not create new users, it is '' (anonymous user)\n        self.assertNotEqual(response.context[\"user\"].username, \"knownuser\")\n\n"], "sample_250": ["    def test_zero_minutes(self):\n        dt = datetime(2023, 10, 26, 10, 0)\n        self.assertEqual(dateformat.format(dt, 'f'), '10')\n        self.assertEqual(dateformat.format(dt, 'F'), '10:00')\n", "    def test_tzinfo_conversion(self):\n\n        dt = datetime(2009, 5, 16, 5, 30, 30, tzinfo=get_default_timezone())\n        localized_dt = make_aware(dt, tzinfo=get_fixed_timezone(-510))\n\n        self.assertEqual(\n            dateformat.format(dt, 'O'),\n            '-0100',\n        )\n        self.assertEqual(\n            dateformat.format(localized_dt, 'O'),\n            '-0530',\n        )\n\n\n", "    def test_iso_8601_with_timezone(self):\n        tz = get_fixed_timezone(-510)\n        dt = make_aware(datetime(2009, 5, 16, 5, 30, 30), tz)\n        self.assertEqual(format(dt, 'c'), '2009-05-16T05:30:30-08:30')\n", "    def test_tzaware_datetime_time_format_specifiers(self):\n        dt = datetime(2009, 5, 16, 5, 30, 30, tzinfo=get_fixed_timezone(-3))\n        with self.subTest('a'):\n            self.assertEqual(format(dt, 'a'), 'a.m.')\n        with self.subTest('A'):\n            self.assertEqual(format(dt, 'A'), 'AM')\n        with self.subTest('I'):\n            self.assertEqual(format(dt, 'I'), '1')\n        with self.subTest('O'):\n            self.assertEqual(format(dt, 'O'), '-0300')\n        with self.subTest('T'):\n            self.assertEqual(format(dt, 'T'), 'UTC-03:00')\n        with self.subTest('Z'):\n            self.assertEqual(format(dt, 'Z'), '-10800')\n\n\n", "    def test_custom_format_with_empty_string(self):\n        my_birthday = datetime(1979, 7, 8, 22, 00)\n        self.assertEqual(dateformat.format(my_birthday, ''), '')\n"], "sample_1188": ["def test_pretty_str_of_dyadics():\n    N = CoordSys3D('N')\n    v = N.i\n\n    dyads = [\n        v | N.j,\n        v | v,\n        N.i | N.j,\n        N.j | N.k,\n        N.k | N.i\n    ]\n    for dyadic in dyads:\n        assert pretty(dyadic) == f'({v}|{N.i.parent})'\n\n\n", "def test_pretty_printing_nested_expressions():\n    from sympy.vector import CoordSys3D, Vector\n\n    N = CoordSys3D('N')\n    v1 = N.i + N.j\n    v2 = N.k\n    v3 = v1 * v2\n    v4 = v1 * v2\n\n    assert pretty(v1) == 'i_N + j_N'\n    assert pretty(v2) == 'k_N'\n    assert pretty(v3) == '0'\n    assert pretty(v4) == '0'\n\n\n\n", "def test_pretty_print_unicode_vector_operations():\n    v = CoordSys3D('V')\n    u = v.i + v.j\n    w = v.k - 2*v.i\n    expr = u + v.j\n\n    assert upretty(u) == 'i_V + j_V'\n    assert upretty(w) == 'k_V - 2\u22c5i_V'\n    assert upretty(expr) == 'i_V + j_V + j_V'\n    assert upretty(u | v.j) == 'v.j * ( i_V | j_V )'\n    assert upretty(u | u) == 'u \u22c5 u'\n\n\n\n", "def test_issue_24309():\n    from sympy.vector import CoordSys3D, Vector\n    from sympy import symbols, sin, cos\n\n    N = CoordSys3D('N')\n    a, b = symbols('a b')\n    v = Vector(a * N.i + 2 * N.j + 3 * N.k)\n    assert upretty(v) ==  'a i_N + 2 j_N + 3 k_N'\n    assert pretty(v) ==  'a i_N + 2 j_N + 3 k_N'\n", "def test_pretty_print_unicode_symbols():\n    from sympy import symbols, pi, exp, sin, cos\n\n    x, y = symbols('x y')\n    assert upretty(pi) == r\"\\pi\"\n    assert upretty(exp(1)) == r\"e\"\n    assert upretty(sin(x)) == r\"\\sin(x)\"\n    assert upretty(cos(y)) == r\"\\cos(y)\"\n    assert upretty(x + y) == r\"x + y\"\n"], "sample_1195": ["compilation error", "compilation error", "compilation error", "compilation error", "def test_gamma_matrix_properties():\n    i, j, k = tensor_indices('i,j,k', LorentzIndex)\n\n    g = LorentzIndex.metric\n\n    # test the following properties:\n    # 1) G(i)*G(-i) is proportional to the metric tensor\n    # 2) G(i)*G(j) is anti-symmetric\n    # 3) The trace of G(i)*G(j)*G(k)*G(l) is a function of the metric tensor\n\n    assert _is_tensor_eq(G(i)*G(-i), 4*g(i, i))\n    assert _is_tensor_eq(G(i)*G(j), -G(j)*G(i))\n\n    t = G(i)*G(j)*G(k)*G(l)\n    t1 = gamma_trace(t)\n    assert _is_tensor_eq(t1, -4*g(i, k)*g(j, l) + 4*g(i, l)*g(j, k) + 4*g(i, j)*g(k, l))\n\n"], "sample_265": ["    def test_invalid_templatetag_option(self):\n        with self.assertRaisesMessage(InvalidTemplateLibrary, \"Invalid template library specified. 'template_backends.apps.doesnotexist' is not a valid module name\"):\n            DjangoTemplates({\n                'DIRS': [],\n                'APP_DIRS': False,\n                'NAME': 'django',\n                'OPTIONS': {\n                    'libraries': {\n                        'invalid': 'template_backends.apps.doesnotexist',\n                    },\n                },\n            })\n", "    def test_custom_context_processors(self):\n        custom_processor = lambda context: {'custom_var': 'value'}\n        engine = DjangoTemplates({\n            'DIRS': [],\n            'APP_DIRS': False,\n            'NAME': 'django',\n            'OPTIONS': {\n                'context_processors': [custom_processor],\n            },\n        })\n        template = engine.from_string('{{ custom_var }}')\n        request = self.request_factory.get('/')\n        content = template.render({}, request)\n        self.assertEqual(content, 'value')\n", "    def test_invalid_template_library_import(self):\n        with self.assertRaisesMessage(\n            InvalidTemplateLibrary,\n            \"Invalid template library specified. ImportError raised when trying to load \"\n            \"'template_backends.apps.invalid.templatetags.nonexistent': No module named 'template_backends.apps.invalid.templatetags.nonexistent'\"\n        ) as cm:\n            DjangoTemplates({\n                'DIRS': [],\n                'APP_DIRS': False,\n                'NAME': 'django',\n                'OPTIONS': {\n                    'libraries': {\n                        'invalid': 'template_backends.apps.invalid.templatetags.nonexistent',\n                    },\n                },\n            })\n\n\n", "    def test_invalid_library(self):\n        with self.assertRaisesMessage(InvalidTemplateLibrary, \"Invalid template library specified. ImportError raised when trying to load 'doesnotexist':\"):\n            DjangoTemplates({\n                'DIRS': [],\n                'APP_DIRS': False,\n                'NAME': 'django',\n                'OPTIONS': {\n                    'libraries': {\n                        'bogus': 'doesnotexist',\n                    },\n                },\n            }) \n", "    def test_template_name_in_error(self):\n        engine = DjangoTemplates({\n            'DIRS': [],\n            'APP_DIRS': False,\n            'NAME': 'django',\n            'OPTIONS': {},\n        })\n        with self.assertRaises(TemplateDoesNotExist) as cm:\n            engine.get_template('doesnotexist')\n        self.assertIn('doesnotexist', str(cm.exception))\n"], "sample_870": ["    def test_gpr_sample_y_shape_with_time_dependent_kernel():\n        rng = np.random.RandomState(1024)\n\n        n_samples = 10\n        n_features = 1\n        n_time_points = 5\n        X = np.tile(np.arange(n_samples)[:, None], (1, n_time_points)).reshape(\n            n_samples * n_time_points, n_features\n        )\n        y = rng.randn(n_samples * n_time_points)\n\n        kernel = TimeDecayKernel(\n            length_scale=1.0, decay_rate=0.1, constant_term=0.0\n        )\n        model = GaussianProcessRegressor(kernel=kernel)\n        model.fit(X, y)\n        samples = model.sample_y(X, n_samples=5)\n        assert samples.shape == (\n            n_samples * n_time_points,\n            5,\n        )\n\n\n", "def test_gpr_fit_sparse_data():\n    from sklearn.datasets import make_sparse_toy\n    from sklearn.preprocessing import StandardScaler\n\n    n_samples, n_features = 100, 10\n    X, y = make_sparse_toy(n_samples=n_samples, n_features=n_features)\n    X = StandardScaler().fit_transform(X)\n    gpr = GaussianProcessRegressor(kernel=RBF(length_scale=1.0)).fit(X, y)\n    # Check that the fit method works correctly with sparse data.\n    assert gpr.kernel_.theta_ is not None\n    assert gpr.kernel_.theta_.shape == (n_features,)\n\n\n\n", "compilation error", "def test_gpr_predict_with_sample_weights():\n    \"\"\"Check predict method works with sample weights.\"\"\"\n    rng = np.random.RandomState(42)\n    n_samples = 20\n    n_features = 3\n\n    X = rng.randn(n_samples, n_features)\n    y = rng.randn(n_samples)\n    sample_weights = rng.rand(n_samples)\n\n    kernel = RBF()\n    gpr = GaussianProcessRegressor(kernel=kernel)\n    gpr.fit(X, y, sample_weights=sample_weights)\n\n    y_pred, y_std = gpr.predict(\n        X, return_std=True, sample_weights=sample_weights\n    )\n    _, y_cov = gpr.predict(X, return_cov=True, sample_weights=sample_weights)\n\n    assert y_pred.shape == (n_samples,)\n    assert y_std.shape == (n_samples,)\n    assert y_cov.shape == (n_samples, n_samples)\n", "def test_noise_level_bounds(noise_level):\n    \"\"\"Check that the provided noise level bounds are respected.\"\"\"\n    gpr = GaussianProcessRegressor(kernel=RBF(), alpha=1e-3, noise_level=noise_level)\n    with pytest.raises(ValueError, match=\"noise_level must be >= 0\"):\n        gpr = GaussianProcessRegressor(kernel=RBF(), alpha=1e-3, noise_level=-1e-5)\n\n    with pytest.raises(ValueError, match=\"noise_level must be >= 0\"):\n        gpr = GaussianProcessRegressor(kernel=RBF(), alpha=1e-3, noise_level=1e-10)\n"], "sample_354": ["    def test_create_permissions_with_custom_model(self):\n        class CustomUser(AbstractBaseUser):\n            pass\n        CustomUser._meta.permissions = [\n            ('can_edit_profile', 'Can edit their own profile'),\n        ]\n        CustomUser._meta.verbose_name = 'Custom User'\n        CustomUser._meta.verbose_name_plural = 'Custom Users'\n        ContentTypes = ContentType.objects.create(\n            app_label='custom_auth', model='customuser'\n        )\n        create_permissions(apps.get_app_config('custom_auth'), verbosity=0)\n        permission_content_type = ContentType.objects.get_by_natural_key('custom_auth', 'customuser')\n        self.assertEqual(Permission.objects.filter(\n            content_type=permission_content_type,\n            codename='can_edit_profile'\n        ).count(), 1)\n\n\n\n", "    def test_createsuperuser_with_custom_permissions(self):\n        # Simulate adding custom permissions to the auth app.\n        Permission._meta.permissions = [\n            ('another_permission', 'Another permission'),\n        ]\n        # Clear any existing permissions in the cache\n        ContentType.objects.clear_cache()\n        create_permissions(self.app_config, verbosity=0)\n        new_io = StringIO()\n        call_command('createsuperuser', interactive=False, username='joe', email='joe@somewhere.org', stdout=new_io)\n        command_output = new_io.getvalue().strip()\n        self.assertEqual(command_output, 'Superuser created successfully.')\n        user = User.objects.get(username='joe')\n        self.assertEqual(user.user_permissions.count(), 3) # +1 for another_permission\n        # assert permissions have been granted to the created user\n\n\n\n", "    def test_permissions_created_with_non_string_codename(self):\n        with self.assertRaises(TypeError):\n            Permission._meta.permissions = [\n                ('my_custom_permission', 123),\n            ]\n            create_permissions(self.app_config, verbosity=0)\n", "    def test_create_permissions_with_custom_permissions(self):\n        new_perm = (\n            ('my_custom_permission', 'Custom permission for testing'),\n        )\n        Permission._meta.permissions = new_perm\n\n        create_permissions(self.app_config, verbosity=0)\n        self.assertEqual(Permission.objects.count(), len(new_perm) )\n        self.assertEqual(\n            Permission.objects.all()[0].codename, 'my_custom_permission'\n        )\n", "    def test_interactive_mode_with_existing_user(self):\n        user = User.objects.create_user('johndoe', 'johndoe@example.com', 'password123')\n        with self.assertNumQueries(1):\n            call_command(\n                'createsuperuser',\n                interactive=True,\n            )\n\n        self.assertEqual(User.objects.filter(is_superuser=True).count(), 2)\n\n"], "sample_545": ["def test_savefig_metadata_overwrites():\n    fig = plt.figure()\n    fig.savefig(io.BytesIO(), format='png', metadata={'key': 'value'})\n    with pytest.raises(\n            ValueError, match=\"metadata already set\"\n    ):\n        fig.savefig(io.BytesIO(), format='png', metadata={'key': 'new_value'})\n", "def test_subplots_adjust_nested(fig_test, fig_ref):\n    fig_ref.subplots_adjust(hspace=0.4, wspace=0.4)\n\n    fig_test.add_subplot(2, 2, 1)\n    fig_test.add_subplot(2, 2, 2)\n    fig_test.add_subplot(2, 2, 3)\n    fig_test.add_subplot(2, 2, 4)\n    fig_test.subplots_adjust(hspace=0.4, wspace=0.4)\n\n\n\n", "def test_savefig_empty_filename():\n    with pytest.raises(ValueError, match=\"filename cannot be empty\"):\n        Figure().savefig(format=\"png\")\n", "def test_savefig_dpi(dpi):\n    fig = Figure(dpi=100)\n    fig.savefig(io.BytesIO(), dpi=dpi)\n\n\n\n", "def test_subplots_align():\n    fig = plt.figure(constrained_layout=True)\n    gs = fig.add_gridspec(2, 2)\n\n    ax1 = fig.add_subplot(gs[0, 0])\n    ax2 = fig.add_subplot(gs[0, 1])\n    ax3 = fig.add_subplot(gs[1, 0])\n    ax4 = fig.add_subplot(gs[1, 1])\n\n    # Check that the axes are aligned properly\n    assert ax1.yaxis.get_label() == ax2.yaxis.get_label()\n    assert ax3.xaxis.get_label() == ax4.xaxis.get_label()\n"], "sample_1150": ["def test_issue_18098():\n    assert ImageSet(Lambda(x, x**2 + I*x), S.Integers).intersect(S.Reals) == ImageSet(Lambda(x, x**2), S.Integers)\n", "def test_issue_17920():\n    assert ImageSet(Lambda(x, x**2), S.Naturals0) == ImageSet(Lambda(x, x**2), S.Naturals)\n", "def test_issue_17877():\n    assert Range(1,3).intersection(Range(2,4)) == Range(2,3)\n    assert Range(1,3).intersection(Range(0,2)) == Range(1,2)\n    assert Range(1,3).intersection(Range(3,5)) == Range()\n    assert Range(1,3).intersection(Range(-1,1)) == Range()\n    assert Range(1,3).intersection(Range(2,1)) == Range()\n", "def test_issue_19794():\n    c1 = ComplexRegion(Interval(0, 1)*Interval(-1, 1), polar=True)\n    c2 = ComplexRegion(Interval(1, 2)*Interval(0, 1), polar=True)\n    assert c1.intersection(c2) == ComplexRegion(FiniteSet(), False)\n\n", "compilation error"], "sample_10": ["def test_iterrows_with_multiple_columns():\n    dat = [(1, 2, 3),\n           (4, 5, 6),\n           (7, 8, 9)]\n    t = table.Table(rows=dat, names=('a', 'b', 'c'))\n    c_s = []\n    a_s = []\n    for c, a in t.iterrows(['c', 'a']):\n        c_s.append(c)\n        a_s.append(a)\n    assert np.all(t['c'] == c_s)\n    assert np.all(t['a'] == a_s)\n\n\n\n", "def test_reproject_coordinate_frame():\n    from astropy.coordinates import SkyCoord, EarthLocation\n    from astropy import units as u\n    t = table.QTable([SkyCoord(0 * u.deg, 0 * u.deg, frame='icrs'),\n                       SkyCoord(1 * u.deg, 1 * u.deg, frame='icrs')],\n                     names=['coord'])\n\n    t['coord'].info.meta['frame'] = 'icrs'\n    t_projected = t.reproject_coordinate_frame('galactic')\n    assert np.allclose(t_projected['coord'].ra.to_value(u.deg),\n                       [0. * u.deg, 1. * u.deg])\n    assert np.allclose(t_projected['coord'].dec.to_value(u.deg),\n                       [0. * u.deg, 1. * u.deg])\n\n\n", "compilation error", "compilation error", "def test_table_from_numpy_array_with_attrs():\n    arr = np.array([[1, 2], [3, 4]], dtype={'names': ('a', 'b'),\n                                            'formats': ('i4', 'f8'),\n                                            'offsets': (0, 4)})\n    t = table.Table.from_numpy(arr)\n    assert t.info.dtype == arr.dtype\n    assert t.info.names == arr.dtype.names\n    assert t.info.formats == arr.dtype.formats\n\n\n"], "sample_1109": ["def test_issue_19746():\n    assert floor(x + oo) == oo\n    assert ceiling(x + oo) == oo\n    assert floor(-x - oo) == -oo\n    assert ceiling(-x - oo) == -oo\n", "def test_issue_15801():\n    x = symbols('x', real=True)\n    assert floor(x**2).args[0] == x\n    assert ceiling(x**2).args[0] == x \n", "def test_issue_18174():\n    assert floor(x**(1/3)).rewrite(floor) == floor(x)**(1/3)\n    assert ceiling(x**(1/3)).rewrite(ceiling) == ceiling(x)**(1/3)\n", "def test_issue_19649():\n    assert floor(sympify(True)) == 1\n    assert ceiling(sympify(True)) == 1\n    assert floor(sympify(False)) == 0\n    assert ceiling(sympify(False)) == 0\n", "def test_issue_18917():\n    assert floor(Rational(1, 2) * oo) == oo\n    assert ceiling(Rational(1, 2) * oo) == oo\n"], "sample_717": ["def test_lfw_pairs_subsets():\n    for subset in ['train', 'test', '10_folds']:\n        lfw_pairs = fetch_lfw_pairs(subset=subset, data_home=SCIKIT_LEARN_DATA,\n                                   download_if_missing=False)\n        assert_equal(len(lfw_pairs.target), lfw_pairs.data.shape[0])\n        assert_equal(lfw_pairs.target.shape[0], lfw_pairs.pairs.shape[0])\n", "def test_lfw_people_subset():\n    lfw_people = fetch_lfw_people(data_home=SCIKIT_LEARN_DATA, funneled=True,\n                                   subset='10_folds', download_if_missing=False)\n\n    # Check that subset = '10_folds' returns the correct shape for the data\n    assert_equal(lfw_people.data.shape, (5000, 2914))\n    assert_equal(lfw_people.images.shape, (5000, 62, 47))\n\n", "def test_load_lfw_pairs_subset():\n    for subset in ['train', 'test', '10_folds']:\n        lfw_pairs = fetch_lfw_pairs(subset=subset, data_home=SCIKIT_LEARN_DATA,\n                                    download_if_missing=False)\n        assert_equal(lfw_pairs.pairs.shape[0], 10) , \\\n            \"Test subset should have 10 pairs for 'train', 'test', '10_folds'\"\n", "def test_load_lfw_pairs_subsets():\n    for subset in ['train', 'test', '10_folds']:\n        lfw_pairs = fetch_lfw_pairs(subset=subset, data_home=SCIKIT_LEARN_DATA,\n                                     download_if_missing=False)\n\n        # Assert that the 'data' attribute has the expected shape\n        assert_equal(lfw_pairs.data.shape,\n                     (2200 if subset == 'train' else 2000 if subset == 'test' else 10000, 5828))\n\n\n", "def test_lfw_pairs_subset():\n    for subset in ['train', 'test', '10_folds']:\n        lfw_pairs = fetch_lfw_pairs(subset=subset,\n                                    data_home=SCIKIT_LEARN_DATA,\n                                    download_if_missing=False)\n\n        assert_equal(len(lfw_pairs.data), len(lfw_pairs.target))\n        assert_equal(len(lfw_pairs.pairs), len(lfw_pairs.target))\n"], "sample_3": ["def test_empty_data_with_empty_header():\n    \"\"\"Test reading an ECSV with an empty header.\"\"\"\n    txt = \"\"\"\n    # %ECSV 1.0\n    # ---\n    \n    a\n    1\n    2\n    \"\"\"\n    t = Table.read(txt, format='ascii.ecsv')\n    assert t.colnames == ['a']\n\n\n\n", "    def test_write_complex_with_subtype():\n        t = Table()\n        t['a'] = np.array([1+1j, 2+2j], dtype=complex)\n        t['a'].info.subtype = 'complex'\n        out = StringIO()\n        t.write(out, format='ascii.ecsv')\n        t2 = Table.read(out.getvalue(), format='ascii.ecsv')\n        assert t2['a'].dtype.type is np.complex128\n", "def test_ecsv_with_comments_in_header():\n    \"\"\"Test handling of comments in the header section.\"\"\"\n    txt = \"\"\"\n    # %ECSV 1.0\n    # This is a comment\n    # ---\n    # datatype:\n    # - {name: a, datatype: int64}\n    # schema: astropy-2.0\n    a\n    1\n    2\n    \"\"\"\n    t = Table.read(txt, format='ascii.ecsv')\n    assert t.colnames == ['a']\n    assert t['a'].dtype == np.int64\n\n\n", "def test_masked_values_with_schema():\n    \"\"\"Test saving and loading masked values with a schema.\"\"\"\n    t = Table()\n    t['col'] = np.ma.array([1, 2, None], mask=[False, False, True], dtype=np.int64)\n    t.meta['schema'] = 'astropy-2.0'\n    out = StringIO()\n    t.write(out, format='ascii.ecsv')\n    t2 = Table.read(out.getvalue(), format='ascii.ecsv')\n\n    assert t2['col'].mask[2] is True\n    assert t2.meta['schema'] == 'astropy-2.0'\n\n\n\n", "def test_ecsv_with_null_values():\n    \"\"\"Test writing and reading ECSV files with null (missing) values.\"\"\"\n    t = Table()\n    t['col1'] = np.array([1, 2, None, 4])\n    t['col2'] = np.array(['a', 'b', None, 'd'])\n    t['col3'] = np.array([True, False, None, True])\n    out = StringIO()\n    t.write(out, format='ascii.ecsv')\n    t2 = Table.read(out.getvalue(), format='ascii.ecsv')\n\n    assert t2['col1'].mask[2]\n    assert t2['col2'].mask[2]\n    assert t2['col3'].mask[2]\n\n    assert np.all(t2['col1'] == t['col1'])\n    assert np.all(t2['col2'] == t['col2'])\n    assert np.all(t2['col3'] == t['col3'])\n\n    # Make sure nulls are written out as blanks when no specified null value\n    # is given\n    out = StringIO()\n    t.write(out, format='ascii.ecsv', serialize_method='data_mask')\n    t2 = Table.read(out.getvalue(), format='ascii.ecsv')\n\n    assert t2['col1'].mask[2]\n    assert t2['col2'].mask[2]\n    assert t2['col3'].mask[2]\n\n    assert np.all(t2['col1'] == t['col1'])\n    assert np.all(t2['col2'] == t['col2'])\n    assert np.all(t2['col3'] == t['col3'])\n"], "sample_604": ["    def test_diff_dataset_repr_with_large_data_arrays(self):\n        large_array = np.random.random((1000, 1000))\n        ds_a = xr.Dataset(data_vars={\"var1\": ((\"x\", \"y\"), large_array)})\n        ds_b = xr.Dataset(data_vars={\"var1\": ((\"x\", \"y\"), large_array + 1e-10)})\n\n        expected = dedent(\n            \"\"\"\\\n        Left and right Dataset objects are not identical\n        Differing data variables:\n        L   var1     (x, y) float64 ...\n        R   var1     (x, y) float64 ...\n        \"\"\"\n        )\n\n        actual = formatting.diff_dataset_repr(ds_a, ds_b, \"identical\")\n        assert actual == expected\n\n\n\n\n", "    def test_dataset_repr_with_large_attrs(display_max_rows):\n        attrs = {f\"attr_{i}\": str(i) for i in range(50)}\n        ds = xr.Dataset(coords={\"foo\": [1, 2]}, attrs=attrs)\n        with xr.set_options(display_max_rows=display_max_rows):\n            actual = formatting.dataset_repr(ds)\n            assert \"Attributes: (49)\" in actual\n", "    def test_repr_with_long_attribute_names(self):\n        attrs = {\n            \"a very long attribute name with lots of characters that may be truncated\": \"value\"\n        }\n        ds = xr.Dataset(attrs=attrs)\n\n        with xr.set_options(display_max_rows=10):\n            actual = formatting.dataset_repr(ds)\n\n        assert \"a very long attribute name with lots of characters that may be truncated\" in actual\n", "    def test_diff_dataset_repr_with_large_arrays(self):\n        large_array_size = 1000\n        ds_a = xr.Dataset(\n            data_vars={\n                \"var1\": ((\"x\", \"y\"), np.random.randn(large_array_size, large_array_size)),\n            },\n            coords={\"x\": np.arange(large_array_size), \"y\": np.arange(large_array_size)},\n        )\n\n        ds_b = xr.Dataset(\n            data_vars={\"var1\": ((\"x\", \"y\"), np.random.randn(large_array_size // 2, large_array_size // 2))},\n            coords={\"x\": np.arange(large_array_size // 2), \"y\": np.arange(large_array_size // 2)},\n        )\n\n        expected = dedent(\n            \"\"\"\n        Left and right Dataset objects are not identical\n        Differing dimensions:\n            (x: {large_array_size}, y: {large_array_size}) != (x: {large_array_size // 2}, y: {large_array_size // 2})\n        Differing data variables:\n        L   var1     (x, y) float64 ...\n        R   var1     (x, y) float64 ...\n        \"\"\"\n        ).format(large_array_size=large_array_size)\n\n        actual = formatting.diff_dataset_repr(ds_a, ds_b, \"identical\")\n        assert actual == expected\n", "def test_summarize_attr_with_complex(self):\n    complex_attr = 1 + 2j\n    expected = \"    complex_attr: 1+2j\"\n    actual = formatting.summarize_attr(\"complex_attr\", complex_attr)\n    assert actual == expected\n"], "sample_1073": ["def test_issue_12420_rational():\n    I = S.ImaginaryUnit\n    assert _sqrt_ratcomb([1, 1, 1], [sqrt(4 + I), sqrt(1 + I), sqrt(2)]) == \\\n       sqrt(1 + I) + sqrt(1 + I) + sqrt(2) \n    assert _sqrt_ratcomb([1, 1, 1], [sqrt(4 + 2*I), sqrt(1 + I), sqrt(2)]) == \\\n       sqrt(1 + I) + sqrt(1 + I) + sqrt(2) \n", "compilation error", "def test_sqrtdenest_issue_8076():\n    assert sqrtdenest(sqrt(S(1)/2 + S(1)/2*sqrt(2))) == 1\n\n\n", "def test_sqrtdenest5():\n    z = sqrt(10 + 6*r3 - 2*sqrt(3 + 3*r3) - 2*sqrt(10 + 6*r3))\n    assert sqrtdenest(z) == \n    sqrt(3 + 3*r3) - sqrt(10 + 6*r3) - 1\n    z = sqrt((1 + sqrt(sqrt(2 + r2 + 5*r3))**2).expand())\n    assert sqrtdenest(z) == 1 + sqrt(sqrt(2 + r2 + 5*r3))\n    z = sqrt(sqrt(4*r2 + 4*r3 + 4) + sqrt(4*r2 + 4*r3 + 4))\n    assert sqrtdenest(z) == 2*sqrt(r2 + r3 + 1)\n    z = sqrt(sqrt(r2 + 2) + sqrt(r2 + 2))\n    assert sqrtdenest(z) == 2*sqrt(r2 + 2)**(S(1)/2)\n\n\n\n", "def test_issue_15570():\n    assert sqrtdenest(sqrt(sqrt(5) + 1) + sqrt(sqrt(5) - 1)) == sqrt(sqrt(5) + 1) + sqrt(sqrt(5) - 1)\n"], "sample_1177": ["def test_issue_16877():\n    from sympy.functions.elementary.complexes import (Abs, polar)\n    x = Symbol('x', real=True)\n    y = Symbol('y', real=True)\n    z = Symbol('z', complex=True)\n    assert Abs(polar(x + y, 0)).subs(z, x + y) == Abs(z)\n    assert Abs(polar(z)).subs(z, x + y) == Abs(x + y)\n", "def test_issue_19593():\n    from sympy.functions.complexes import Abs\n    x = Symbol('x', real=True)\n    f = Function('f')\n    assert Abs(f(x)*conjugate(f(x))) == Abs(f(x))**2\n    assert Abs(I*f(x)) == Abs(f(x))\n    assert Abs(Abs(f(x))) == Abs(f(x))\n\n\n\n", "compilation error", "compilation error", "compilation error"], "sample_300": ["    def test_extra_fields_annotation(self):\n        query = Query(Item)\n        query.annotations = {'extra_field': F('name') + 1}\n        query.add_select_related('creator')\n\n        where = query.build_where(Q(extra_field__gt=5))\n        lookup = where.children[0]\n        self.assertIsInstance(lookup, GreaterThan)\n\n        self.assertEqual(lookup.lhs.target, 'extra_field')\n\n", "    def test_extra_select_ordering(self):\n        query = Query(Item)\n        query.set_extra_mask({'modified': None})\n        query.add_order_by('modified')\n        with self.assertRaises(FieldError) as cm:\n            query.compile()\n        self.assertIn('Ordering expressions are not supported for extra select fields.', str(cm.exception))\n\n\n\n", "    def test_filter_complex_conditional(self):\n        query = Query(Ranking)\n        filter_expr = Func(\n            'author__num', filter_function='min', output_field=IntegerField()\n        )\n        query.build_where(filter_expr > 10)\n        lookup = query.where.children[0]\n        self.assertIsInstance(lookup, GreaterThan)\n        self.assertIsInstance(lookup.lhs, Func)\n        self.assertEqual(lookup.lhs.target, Ranking._meta.get_field('author__num'))\n\n", "    def test_filter_non_conditional_complex(self):\n        query = Query(Item)\n        filter_expr = Func('note__note__length', output_field=CharField(),\n                           function='length')\n        msg = 'Cannot filter against a non-conditional expression.'\n        with self.assertRaisesMessage(TypeError, msg):\n            query.build_where(filter_expr)\n", "    def test_filter_ambiguous_alias(self):\n        query = Query(Item)\n        query.alias_map['foo'] = BaseTable(Item._meta.db_table, 'foo')\n        msg = \"Ambiguous alias 'foo' used in field lookup.\"\n        with self.assertRaisesMessage(FieldError, msg):\n            query.build_where(Q(foo__created__gt=datetime(2022, 1, 1)))\n"], "sample_149": ["    def test_builtin_permission_name_length(self):\n        class Checked(models.Model):\n            class Meta:\n                verbose_name = 'very long verbose name' * 10\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Error(\n                \"The verbose_name of model 'auth_tests.Checked' must be at most 244 \"\n                \"characters for its builtin permission names to be at most 255 characters.\",\n                obj=Checked,\n                id='auth.E007',\n            ),\n        ])\n", "    def test_builtin_permission_name_max_length_with_short_verbose_name(self):\n        class Checked(models.Model):\n            class Meta:\n                verbose_name = 'C' * 5\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Error(\n                \"The verbose_name of model 'auth_tests.Checked' must be at most 5 \"\n                \"characters for its builtin permission names to be at most 255 \"\n                \"characters.\",\n                obj=Checked,\n                id='auth.E007',\n            ),\n        ])\n\n\n", "    def test_permission_clashing_with_other_app(self):\n        class OtherAppChecked(models.Model):\n            class Meta:\n                permissions = [\n                    ('change_checked', 'Can edit permission')\n                ]\n        with self.settings(INSTALLED_APPS=['auth_tests', 'other_app']):\n            errors = checks.run_checks(self.apps.get_app_configs())\n            self.assertEqual(errors, [\n                checks.Error(\n                    \"The permission codenamed 'change_checked' clashes with a builtin \"\n                    \"permission for model 'other_app.OtherAppChecked'.\",\n                    obj=OtherAppChecked,\n                    id='auth.E005',\n                ),\n            ])\n", "    def test_default_permissions_with_custom_permissions(self):\n        class Checked(models.Model):\n            class Meta:\n                default_permissions = [\n                    ('my_default_permission', 'Some default permission'),\n                ]\n                permissions = [\n                    ('my_custom_permission', 'Some custom permission'),\n                ]\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [])\n", "    def test_permission_name_conflict(self):\n        class Checked(models.Model):\n            class Meta:\n                permissions = [\n                    ('add_checked', 'Can add permission'),\n                    ('change_checked', 'Can edit permission'),\n                ]\n\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Error(\n                \"The permission codenamed 'add_checked' clashes with a builtin \"\n                \"permission for model 'auth_tests.Checked'.\",\n                obj=Checked,\n                id='auth.E005',\n            ),\n        ])\n"], "sample_402": ["    def test_etag_encoding(self):\n        \"\"\"\n        GZipMiddleware ensures that ETags are encoded correctly for\n        compressed responses.\n        \"\"\"\n\n            response = HttpResponse(self.compressible_string)\n            response.headers[\"ETag\"] = '\"eggs\"'\n            return response\n\n        request = self.rf.get(\"/\", HTTP_ACCEPT_ENCODING=\"gzip, deflate\")\n        gzip_response = GZipMiddleware(get_response)(request)\n        self.assertEqual(gzip_response.headers[\"ETag\"], 'W/\"eggs\"')\n        self.assertEqual(gzip_response.headers[\"Content-Encoding\"], \"gzip\")\n\n        with self.settings(DEBUG=True):\n           print(gzip_response.headers[\"ETag\"])\n           # TODO: Assert the correct ETag is set after decompression\n\n\n\n", "    def test_dont_compress_gzip_disabled(self):\n        \"\"\"\n        If the client explicitly disables gzip compression, don't compress the response.\n        \"\"\"\n        self.req.META[\"HTTP_ACCEPT_ENCODING\"] = \"identity\"\n        r = GZipMiddleware(self.get_response)(self.req)\n        self.assertEqual(r.content, self.compressible_string)\n        self.assertIsNone(r.get(\"Content-Encoding\"))\n\n\n\n", "    def test_etag_match_with_strong_etag(self):\n        \"\"\"\n        GZipMiddleware allows 304 Not Modified responses when a strong\n        ETag is provided.\n        \"\"\"\n\n            response = HttpResponse(self.compressible_string)\n            response.headers[\"ETag\"] = '\"eggs\"'\n            return response\n\n\n            return ConditionalGetMiddleware(get_response)(req)\n\n        request = self.rf.get(\"/\", HTTP_ACCEPT_ENCODING=\"gzip, deflate\")\n        response = GZipMiddleware(get_cond_response)(request)\n        gzip_etag = response.headers[\"ETag\"]\n        next_request = self.rf.get(\n            \"/\", HTTP_ACCEPT_ENCODING=\"gzip, deflate\", HTTP_IF_NONE_MATCH=gzip_etag\n        )\n        next_response = ConditionalGetMiddleware(get_response)(next_request)\n        self.assertEqual(next_response.status_code, 304)\n", "    def test_compress_unicode_response(self):\n        \"\"\"\n        Compression is performed on responses with Unicode content.\n        \"\"\"\n        self.resp.content = self.sequence_unicode\n        r = GZipMiddleware(self.get_response)(self.req)\n        self.assertEqual(\n            self.decompress(r.content), b\"\".join(x.encode() for x in self.sequence_unicode)\n        )\n        self.assertEqual(r.get(\"Content-Encoding\"), \"gzip\")\n\n", "    def test_etag_header_with_file_response(self):\n        with open(__file__, \"rb\") as file1:\n\n                file_resp = FileResponse(file1)\n                file_resp[\"Content-Type\"] = \"text/html; charset=UTF-8\"\n                return file_resp\n\n            request = self.rf.get(\"/\", HTTP_ACCEPT_ENCODING=\"gzip, deflate\")\n            gzip_response = GZipMiddleware(get_response)(request)\n            self.assertEqual(gzip_response.headers[\"ETag\"], 'W/\"{md5_hash}\"') # Should be a strong ETag\n"], "sample_1042": ["def test_symbolic_subs__with_Indexed_for_various_data_types():\n    A = IndexedBase(\"A\")\n    i, j = symbols(\"i,j\", integer=True)\n    x, y, z = symbols(\"x,y,z\")\n    a = S(2)\n    b = pi\n    c = oo\n\n    assert Subs(A[i], x, A[j]).subs(A[j], y).subs(x, z) == Subs(A[i], z, y)\n    assert Subs(A[i], a, b).subs(A[i], c) == Subs(a, b, c)\n    assert Subs(A[i], A[i], x).subs(A[i], y).subs(x, z) == Subs(A[i], y, z)\n    assert Subs(A[i] * A[j], A[i], x).subs(A[j], y).subs(x, z).subs(y, b) == Subs(A[i] * A[j], A[i], z).subs(A[j], b)\n\n", "def test_issue_14806():\n    i, j = symbols(\"i,j\", integer=True)\n    a = IndexedBase('a')\n    b = IndexedBase('b')\n    expr = a[i] * b[j]\n    assert expr.subs(a[i], b[i]).doit() == b[i] * b[j]\n", "def test_Indexed_free_symbols():\n    i, j, k = symbols('i j k', integer=True)\n    a = symbols('a')\n    A = Indexed('A', i, j)\n    assert A.free_symbols == {A.base.label, A.indices}\n    assert Indexed(a, i, j).free_symbols == {a}\n    assert Indexed(A, i, j).free_symbols == {A.base.label, i, j}\n    assert IndexedBase(a).free_symbols == {a}\n    assert IndexedBase(a, 2).free_symbols == {a}\n", "def test_Indexed_subs_with_indexed_base():\n    A = IndexedBase(\"A\")\n    a = symbols('a')\n    i, j = symbols(\"i,j\", integer=True)\n    n = symbols('n', integer=True, finite=True)\n    expr = i * A[i]\n    assert expr.subs(A, IndexedBase('B')).doit() == i * B[i]\n    assert expr.subs(A, IndexedBase('B', shape=(n,))).doit() == i * B[i]\n    assert expr.subs(A, IndexedBase('B', shape=(n, 2))).doit() == i * B[i]\n\n\n    expr = A[i] * A[j]\n    assert expr.subs(A, IndexedBase('B')).doit() == B[i] * B[j]\n    assert expr.subs(A, IndexedBase('B', shape=(n,))).doit() == B[i] * B[j]\n    assert expr.subs(A, IndexedBase('B', shape=(n, 2))).doit() == B[i] * B[j]\n\n    expr =  A[n - i]\n    assert expr.subs(A, IndexedBase('B')).doit() == B[n - i]\n    assert expr.subs(A, IndexedBase('B', shape=(n,))).doit() == B[n - i]\n    assert expr.subs(A, IndexedBase('B', shape=(n, 2))).doit() == B[n - i]\n\n\n\n", "def test_Indexed_subs_with_multiple_indices():\n    i, j, k = symbols('i j k', integer=True)\n    A = IndexedBase('A')\n    B = IndexedBase('B')\n    C = IndexedBase('C')\n    expr = A[i, j] * B[j, k] + C[i]\n    assert expr.subs({A[i, j]: B[i, j], C[i]: 2*i}).doit() == B[i, k] * B[i, j] + 2*i\n\n\n\n\n"], "sample_706": ["def test_unicode_idents() -> None:\n    for ident in (\"\u4f60\u597d\", \"\ud83d\ude0a\", \"\ud83e\udd80\"):\n        assert evaluate(ident, {ident: True}.__getitem__)\n", "compilation error", "def test_negation_with_ident(expr: str) -> None:\n    matcher = {\"ident\": True}.__getitem__\n    assert evaluate(expr, matcher)\n", "def test_nested_parens_and_logic(expr: str, expected: bool) -> None:\n    matcher = {\"true\": True, \"false\": False}.__getitem__\n    assert evaluate(expr, matcher) is expected\n", "def test_nested_parens(expr: str, expected: bool) -> None:\n    matcher = {\"true\": True, \"false\": False}.__getitem__\n    assert evaluate(expr, matcher) is expected\n"], "sample_1100": ["compilation error", "def test_issue_19000():\n    from sympy import exp, log, I\n    a, b, c = symbols('a b c', positive=True)\n    assert (exp(log(a)) - a).is_zero\n    assert (exp(log(a) + log(b)) - a*b).is_zero\n    assert (exp(log(a) + log(b) + log(c)) - a*b*c).is_zero\n    assert (exp(I*pi) - (-1)).is_zero\n", "def test_issue_19126():\n    a, b = symbols('a b')\n    assert (a**oo).is_positive\n    assert (b**(-oo)).is_positive\n    assert (a**oo * b **(-oo)).is_positive \n", "def test_issue_18507_continued():\n    assert Mul(zoo, zoo, evaluate=False) is nan\n    assert Mul(zoo, -zoo, evaluate=False) is nan\n    assert Mul(zoo, zoo, evaluate=False).is_numeric is None\n    assert Mul(zoo, -zoo, evaluate=False).is_numeric is None\n", "compilation error"], "sample_497": ["compilation error", "def test_fmt_with_axis_label():\n    fig, ax = plt.subplots()\n    ax.set_xlabel('My Axis Label')\n    formatter = mticker.EngFormatter()\n    ax.xaxis.set_major_formatter(formatter)\n\n    # Trigger formatting\n    ax.set_xticks([100, 1e3, 1e6])\n    fig.canvas.draw()\n\n    # Check that the label is correctly incorporated\n    for labl in ax.get_xticklabels():\n        assert all(part in labl.get_text() for part in ['100', '1 k', '1 M'])\n\n\n\n", "def test_MultipleFormatterModes():\n    fig, ax = plt.subplots()\n    ax.set_xlim(0, 10)\n\n    # Test using a combination of formatters\n\n    # Set major formatter to custom string, minor formatter to log\n    ax.xaxis.set_major_formatter(mticker.FormatStrFormatter('%02d'))\n    ax.xaxis.set_minor_formatter(mticker.LogFormatter())\n\n    ax.set_xticks(np.arange(0, 10, 2))\n\n    # Manually set minor ticks for comparison\n    ax.set_yticks(np.arange(1, 10, 1))\n\n    fig.autofmt_xdate()\n    fig.canvas.draw()\n\n    minor_ticks = ax.xaxis.get_minor_ticks()\n    assert len(minor_ticks) == 5\n    \n    # Assert tick labels are correctly formatted\n    assert all([f.get_text() == str(10**i) for i in range(len(minor_ticks))])\n", "    def test_update_ticks_filter_duplicate_dates():\n        import numpy as np\n        import matplotlib.dates as mdates\n\n        t = np.array([\"2023-10-26 00:00:00\", \"2023-10-26 01:00:00\",\n                      \"2023-10-26 02:00:00\", \"2023-10-26 03:00:00\",\n                      \"2023-10-26 04:00:00\", \"2023-10-26 05:00:00\",\n                      \"2023-10-26 06:00:00\", \"2023-10-26 07:00:00\",\n                      \"2023-10-26 08:00:00\", \"2023-10-26 09:00:00\"])\n        x = np.ones(len(t))\n\n        fig, ax = plt.subplots()\n        ax.plot(t, x)\n\n        ax.xaxis.set_major_locator(mdates.HourLocator(range(0, 24, 2)))\n        ax.xaxis.set_major_formatter(mdates.DateFormatter('\\n%H'))\n        ax.xaxis.set_minor_locator(mdates.MinuteLocator(range(0, 60, 10)))\n        ax.xaxis.set_minor_formatter(mdates.DateFormatter('%M'))\n\n        ax.xaxis.remove_overlapping_locs = True\n\n        # Update the ticks to ensure filtering is applied\n        ax.xaxis.update_ticks()\n\n        # Check that only unique dates are visible as minor ticks\n        minor_tick_dates = [t.get_text() for t in ax.xaxis.minorTicks]\n        unique_dates = set(minor_tick_dates)\n        assert len(unique_dates) == 14  # Each hour should have 1 minor tick\n\n\n\n", "    def test_num_ticks(self, n_ticks):\n        fig, ax = plt.subplots()\n        l = ax.plot([0, 1, 2, 3], [0, 1, 2, 3])\n        ax.set_xticks(np.linspace(0, 3, n_ticks + 1))\n        fig.canvas.draw()\n        # Check that the number of ticks actually matches\n        assert len(ax.xaxis.get_major_ticks()) == n_ticks + 1\n\n"], "sample_518": ["def test_linewidth_setting():\n    patch = Patch()\n    patch.set_linewidth(5)\n    assert patch.get_linewidth() == 5\n\n", "compilation error", "def test_default_linewidth():\n    patch = Patch()\n    assert patch.get_linewidth() == rcParams['lines.linewidth']\n\n", "def test_path_patch_fill_with_arraylike():\n    verts = np.array([[0, 0], [1, 0], [1, 1], [0, 1]])\n    codes = np.array([mpath.Path.MOVETO,\n                      mpath.Path.CURVE3,\n                      mpath.Path.CURVE3,\n                      mpath.Path.CURVE3])\n    path = mpath.Path(verts, codes)\n    with pytest.raises(TypeError):\n        mppatches.PathPatch(path, facecolor=[1, 2, 3])\n\n\n\n", "def test_default_linewidth():\n    patch = Patch()\n    assert patch.get_linewidth() == rcParams['patch.linewidth']\n\n\n\n"], "sample_734": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_237": ["    def test_custom_permissions_with_builtin_permission_names(self):\n        class Checked(models.Model):\n            class Meta:\n                permissions = [\n                    ('view_checked', 'Can view checked items'),\n                    ('add_checked', 'Can add checked items'),\n                ]\n\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Error(\n                \"The verbose_name of model 'auth_tests.Checked' must be at most 244 \"\n                \"characters for its builtin permission names to be at most 255 characters.\",\n                obj=Checked,\n                id='auth.E007',\n            ),\n        ])\n", "    def test_permission_codename_lowercase(self):\n        class Checked(models.Model):\n            class Meta:\n                permissions = [\n                    ('MY_CUSTOM_PERMISSION', 'Some permission'),\n                ]\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Error(\n                \"The permission codenamed 'MY_CUSTOM_PERMISSION' of model \"\n                \"'auth_tests.Checked' must be lowercase.\",\n                obj=Checked,\n                id='auth.E013',\n            ),\n        ])\n", "    def test_duplicate_custom_permissions(self):\n        class Checked(models.Model):\n            class Meta:\n                permissions = [\n                    ('my_permission', 'Permission 1'),\n                    ('my_permission', 'Permission 2'),\n                ]\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Error(\n                \"The permission codenamed 'my_permission' is duplicated for \"\n                \"model 'auth_tests.Checked'.\",\n                obj=Checked,\n                id='auth.E006',\n            ),\n        ])\n\n\n\n", "    def test_custom_permission_name_with_spaces(self):\n        custom_permission_name = 'some permission with spaces'\n\n        class Checked(models.Model):\n            class Meta:\n                permissions = [\n                    ('my_custom_permission', custom_permission_name),\n                ]\n\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [])\n", "    def test_custom_permission_name_length_with_builtin_clash(self):\n        class Checked(models.Model):\n            class Meta:\n                permissions = [\n                    ('my_clashing_permission', 'Some permission (clashing with builtin)'),\n                ]\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Error(\n                \"The permission named '%s' of model 'auth_tests.Checked' is longer \"\n                \"than 255 characters.\" % (\n                    'my_clashing_permission',\n                ),\n                obj=Checked,\n                id='auth.E008',\n            ),\n        ])\n"], "sample_858": ["def test_partial_fit():\n    clf1 = LogisticRegression(random_state=123)\n    clf2 = RandomForestClassifier(random_state=123)\n    clf3 = GaussianNB()\n    X = np.array([[-1.1, -1.5], [-1.2, -1.4], [-3.4, -2.2], [1.1, 1.2]])\n    y = np.array([1, 1, 2, 2])\n\n    eclf1 = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2),\n                ('gnb', clf3)],\n                voting='soft')\n\n    # Partial fit\n    eclf1.partial_fit(X[:2], y[:2], sample_weight=None)\n    assert_array_almost_equal(eclf1.predict_proba(X[:2]),\n                              eclf1.predict_proba(X[:2]))\n\n    eclf1.partial_fit(X[2:], y[2:], sample_weight=None)\n    assert_array_almost_equal(eclf1.predict_proba(X),\n                              eclf1.predict_proba(X))\n    assert_array_almost_equal(eclf1.predict(X),\n                              eclf1.predict(X))\n\n    # Partial fit with sample_weight\n    sample_weight = np.array([0.2, 0.3, 0.5, 0.7])\n    eclf1 = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2),\n                ('gnb', clf3)],\n                            voting='soft')\n\n    eclf1.partial_fit(X[:2], y[:2], sample_weight=sample_weight[:2])\n    eclf1.partial_fit(X[2:], y[2:], sample_weight=sample_weight[2:])\n    assert_array_almost_equal(eclf1.predict_proba(X),\n                              eclf1.predict_proba(X))\n    assert_array_almost_equal(eclf1.predict(X),\n                              eclf1.predict(X))\n\n", "def test_voting_classifier_with_preprocessor():\n    from sklearn.datasets import make_classification\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.pipeline import Pipeline\n\n    X, y = make_classification(random_state=123)\n\n    clf1 = LogisticRegression()\n    clf2 = RandomForestClassifier()\n\n    # Create pipeline with preprocessor\n    preprocessor = StandardScaler()\n    clf_pipeline = Pipeline(\n        steps=[\n            (\"scaler\", preprocessor),\n            (\"clf\", VotingClassifier(estimators=[(\"lr\", clf1),\n                                                    (\"rf\", clf2)],\n                                     voting='hard'))\n        ]\n    )\n\n    clf_pipeline.fit(X, y)\n    preds = clf_pipeline.predict(X)\n\n\n\n    # Check that the pipeline outputs correct predictions\n    assert_array_equal(preds, clf_pipeline.steps[-1].named_steps.clf.predict(\n        preprocessor.transform(X)))\n", "def test_multi_output_regression():\n    from sklearn.datasets import make_regression\n    from sklearn.linear_model import LinearRegression\n    from sklearn.ensemble import RandomForestRegressor\n\n    X, y = make_regression(n_samples=100, n_features=10, n_informative=5,\n                           random_state=123, multi_output=True)\n\n    lr = LinearRegression(n_jobs=-1)\n    rf = RandomForestRegressor(n_estimators=100, random_state=123)\n\n    ereg = VotingRegressor(estimators=[('lr', lr), ('rf', rf)],\n                           voting='soft',\n                           weights=[0.5, 0.5])\n    ereg.fit(X, y)\n    assert ereg.predict(X).shape == y.shape\n    assert ereg.score(X, y) is not None\n\n\n", "def test_clone():\n    clf1 = LogisticRegression(random_state=123)\n    clf2 = RandomForestClassifier(random_state=123)\n    clf3 = GaussianNB()\n    eclf = VotingClassifier(estimators=[\n                ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n                voting='soft', weights=[1, 2, 1])\n    eclf_clone = eclf.clone()\n    assert eclf.estimators != eclf_clone.estimators\n    assert eclf.get_params() != eclf_clone.get_params()\n\n\n\n", "def test_voting_classifier_with_custom_loss():\n    class CustomLossClassifier(BaseEstimator, ClassifierMixin):\n            self.custom_loss_func = custom_loss_func\n\n            pass  # Placeholder for actual fitting\n\n            return np.zeros(len(X))\n\n    custom_loss = lambda y_true, y_pred: np.sum(np.abs(y_true - y_pred))\n\n    eclf = VotingClassifier(\n        estimators=[\n            ('clf1', CustomLossClassifier(custom_loss)),\n            ('clf2', CustomLossClassifier(custom_loss)),\n        ],\n        voting='soft'\n    )\n    with pytest.raises(ValueError):\n        eclf.fit(X, y)\n\n"], "sample_79": ["    def test_empty_string(self):\n        self.assertEqual(pluralize(\"\"), '')\n", "    def test_unicode_inputs(self):\n        self.assertEqual(pluralize(0, 'votes'), 'votes')\n        self.assertEqual(pluralize(1, 'vote'), 'vote')\n        self.assertEqual(pluralize(2, 'votes'), 'votes')\n\n\n", "    def test_invalid_suffixes(self):\n        self.assertEqual(pluralize(1, 'nocomma'), 'nocomma')\n        self.assertEqual(pluralize(0, 'nocomma'), 'nocomma')\n        self.assertEqual(pluralize(2, 'nocomma'), 'nocomma')\n        self.assertEqual(pluralize(1, 'comma'), 'comma')\n        self.assertEqual(pluralize(0, 'comma'), 'comma')\n        self.assertEqual(pluralize(2, 'comma'), 'comma')\n", "    def test_no_plural_suffix(self):\n        self.check_values(('0', 'candies'), ('1', 'candy'), ('2', 'candies'))\n", "    def test_invalid_suffix(self):\n        with self.assertRaises(ValueError):\n            pluralize(1, 'invalid_suffix')\n"], "sample_273": ["    def test_auto_created_pk_across_apps(self, apps):\n        class Model1(models.Model):\n            class Meta:\n                app_label = 'basic'\n            \n        class Model2(models.Model):\n            class Meta:\n                app_label = 'check_framework'\n                default_auto_field = 'django.db.models.AutoField'\n\n        self.assertEqual(checks.run_checks(app_configs=apps.get_app_configs()), [\n            Warning(self.msg, hint=self.hint, obj=Model1, id='models.W042'),\n        ])\n", "    def test_constraint_names_with_interpolation(self):\n        constraint1 = models.CheckConstraint(check=models.Q(id__gt=0), name='%(app_label)s_%(class)s_foo')\n        constraint2 = models.CheckConstraint(check=models.Q(id__lt=100), name='%(app_label)s_%(class)s_bar')\n\n        class Model(models.Model):\n            class Meta:\n                constraints = [constraint1, constraint2]\n\n        self.assertEqual(checks.run_checks(app_configs=self.apps.get_app_configs()), [])\n\n\n\n", "    def test_no_collision_auto_field_app(self, apps):\n        class Model1(models.Model):\n            class Meta:\n                app_label = 'basic'\n\n        class Model2(models.Model):\n            class Meta:\n                app_label = 'check_framework'\n\n        self.assertEqual(checks.run_checks(app_configs=apps.get_app_configs()), [])\n\n", "    def test_ordering_field_must_be_related_field(self):\n        class ModelWithForeignKey(models.Model):\n            id = models.AutoField(primary_key=True)\n            name = models.CharField(max_length=100)\n            related_model = models.ForeignKey('check_framework.ModelWithForeignKey', on_delete=models.CASCADE)\n\n        class ModelWithoutForeignKey(models.Model):\n            id = models.AutoField(primary_key=True)\n            name = models.CharField(max_length=100)\n\n        class ModelWithInvalidOrdering(models.Model):\n            id = models.AutoField(primary_key=True)\n\n        with self.assertRaises(Error) as context:\n            ModelWithInvalidOrdering._meta.ordering = ['name']\n        self.assertIn('Ordering field \"name\" must refer to a related field.', str(context.exception))\n\n        with self.assertRaises(Error) as context:\n            ModelWithoutForeignKey._meta.ordering = ['related_model__name']\n        self.assertIn('Ordering field \"related_model__name\" does not exist on model check_framework.ModelWithoutForeignKey.', str(context.exception))\n\n        # Ensure ordering works for models with related fields\n        model_with_foreign_key = ModelWithForeignKey\n        with self.assertRaises(Error) as context:\n            model_with_foreign_key._meta.ordering = ['related_model__name']\n        self.assertIn('Ordering field \"related_model__name\" must refer to a related field.', str(context.exception))\n\n\n\n", "    def test_foreign_key_field_without_database_router(self):\n        class Model1(models.Model):\n            class Meta:\n                db_table = 'test_table1'\n\n        class Model2(models.Model):\n            class Meta:\n                db_table = 'test_table2'\n                routers = ['check_framework.test_model_checks.EmptyRouter']\n\n        class Model3(Model1):\n            related_model = models.ForeignKey(Model2, on_delete=models.CASCADE)\n\n        self.assertEqual(\n            checks.run_checks(app_configs=self.apps.get_app_configs()),\n            [\n                Warning(\n                    ' FOREIGN KEY constraint could not be fully validated due to the absence '\n                    'of a database router for the related model \"check_framework.Model2\". '\n                    'Verify that the router configuration for \"check_framework.Model2\" '\n                    'is correctly configured.',\n                    hint='Review your DATABASE_ROUTERS settings to ensure that '\n                          'the related model, check_framework.Model2, has a defined router.',\n                    obj='check_framework.Model3.related_model',\n                    id='models.W037',\n                )\n            ],\n        )\n\n"], "sample_423": ["    def test_complex_operations_ordering(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\"Person\", fields=[]),\n                migrations.DeleteModel(\"Animal\"),\n                migrations.RenameModel(\"Person\", \"Employee\"),\n            ]\n\n        migration = Migration(\"some_migration\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"employee_delete_animal\")\n", "    def test_index_together_order_renamed_field(self):\n        initial_author = ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n                (\"age\", models.IntegerField()),\n            ],\n            {\n                \"index_together\": {(\"name\",)},\n            },\n        )\n        author_new_constraints = ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"full_name\", models.CharField(max_length=200)),\n                (\"age\", models.IntegerField()),\n            ],\n            {\n                \"index_together\": {(\"full_name\",)},\n            },\n        )\n        changes = self.get_changes([initial_author], [author_new_constraints])\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(\n            changes,\n            \"testapp\",\n            0,\n            [\"RenameField\", \"AlterIndexTogether\"],\n        )\n        self.assertOperationAttributes(\n            changes,\n            \"testapp\",\n            0,\n            1,\n            name=\"author\",\n            index_together={(\"full_name\",)},\n        )\n", "    def test_operation_with_extra_kwargs(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\n                    \"Person\", fields=[], extra_kwargs={\"some_extra_field\": \"some_value\"}\n                )\n            ]\n\n        migration = Migration(\"some_migration\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"person\")\n", "    def test_operation_with_no_name(self):\n        class Migration(migrations.Migration):\n            operations = [migrations.CreateModel(\"Person\", fields=[\n                migrations.TextField(\"name\"),\n            ])]\n\n        migration = Migration(\"some_migration\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"person\")\n", "    def test_operation_with_long_field_name(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\n                    \"Person\", fields=[(\"really_really_long_field_name\", models.CharField(max_length=100))]\n                )\n            ]\n\n        migration = Migration(\"some_migration\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"person_really_really_long_field_name\")\n"], "sample_244": ["    def test_all_valid_with_non_form_errors(self):\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '',\n            'choices-1-choice': 'One',\n            'choices-1-votes': '1',\n        }\n        ChoiceFormSet = formset_factory(Choice, validate_max=True, max_num=1)\n        formset1 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        formset2 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertIs(all_valid((formset1, formset2)), False)\n        expected_errors = [\n            {'non_form': ['Max_num exceeded']},\n            {'non_form': ['Max_num exceeded']}\n        ]\n        self.assertEqual(formset1._errors, expected_errors)\n        self.assertEqual(formset2._errors, expected_errors)\n", "    def test_mixed_valid_invalid(self):\n        data = {\n            'choices-TOTAL_FORMS': '3',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '0',\n            'choices-1-choice': 'One',\n            'choices-1-votes': '',  # <-- This form is invalid\n            'choices-2-choice': 'Two',\n            'choices-2-votes': '2',\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertIs(all_valid((formset,)), False)\n        expected_errors = [{'votes': ['This field is required.']},\n                          {}, {}]\n        self.assertEqual(formset._errors, expected_errors)\n\n\n\n", "    def test_empty_formset_renders_correctly(self):\n        formset = formset_factory(Choice, extra=0)\n        self.assertHTMLEqual(str(formset), '')\n", "    def test_formset_errors_from_nested_forms(self):\n        class NestedFormSetForm(forms.Form):\n            test_form = formset_factory(NestedForm, prefix='test_form')\n\n\n        class NestedForm(forms.Form):\n            name = forms.CharField(max_length=10, required=True)\n\n        formset = NestedFormSetForm()\n        formset.save()\n        self.assertEqual(formset.errors, {'test_form': [{'name': ['This field is required.']}]})\n\n\n", "    def test_total_error_count_with_nested_formsets(self):\n        class NestedChoiceForm(formset_factory.BaseFormSet):\n                super().__init__(*args, **kwargs)\n                self.formset = formset_factory(Choice, can_delete=True)\n\n        class NestedForm(Form):\n            nested = NestedChoiceForm(\n                prefix='nested',\n                initial=[{'choice': 'Zero', 'votes': '1'}],\n            )\n\n        NestedFormSet = formset_factory(NestedForm)\n        formset = NestedFormSet(data={\n            'form-TOTAL_FORMS': '1',\n            'form-INITIAL_FORMS': '0',\n            'form-0-nested-TOTAL_FORMS': '1'\n        }, prefix='form')\n        self.assertEqual(formset.total_error_count(), 1)\n"], "sample_960": ["def test_warn_missing_reference_in_table(app, status, warning):\n    app.build()\n    assert 'index.rst:6: WARNING: undefined label: no-label' in warning.getvalue()\n    assert ('index.rst:7: WARNING: Failed to create a cross reference. A title or caption not found: existing-label'\n            in warning.getvalue())\n", "def test_literal_eval_example(app, status, warning):\n    app.build()\n    content = (app.outdir / 'index.html').read_text()\n    assert '<pre><span class=\"doctest literal-eval\">' in content\n    assert '</pre>' in content\n    assert '{\"a\": 1, \"b\": 2}' in content\n\n\n\n", "def test_warn_missing_reference_with_custom_label(app, status, warning):\n    app.build()\n    assert 'index.rst:7: WARNING: undefined label: no-label' in warning.getvalue()\n    assert ('index.rst:7: WARNING: Failed to create a cross reference. A title or caption not found: existing-label'\n            in warning.getvalue())\n\n\n", "compilation error", "def test_ref_missing_label_in_index(app, status, warning):\n    app.build()\n    content = (app.outdir / 'index.html').read_text()\n    assert 'no-label' not in content\n    assert 'existing-label' not in content\n\n\n\n"], "sample_809": ["def test_mutual_info_regression_sparse():\n    X = csr_matrix([[1, 0, 0],\n                    [0, 1, 0],\n                    [1, 1, 1]])\n    y = np.array([1, 0, 1])\n    mi = mutual_info_regression(X, y, random_state=0) \n    assert_array_equal(np.argsort(-mi), [0, 2, 1])\n", "def test_mutual_info_sparse_output():\n    rng = check_random_state(0)\n    X = rng.rand(100, 5).astype(float)\n    y = (X[:, 0] + X[:, 1] > 0.5).astype(int)\n\n    mi_dense = mutual_info_classif(X, y, discrete_features=False)\n    mi_sparse = mutual_info_classif(csr_matrix(X), y,\n                                    discrete_features=False)\n\n    assert_array_equal(mi_dense, mi_sparse)\n\n\n\n", "def test_mutual_info_sparse():\n    # Test mutual_info_regression and mutual_info_classif on sparse matrices\n\n    rng = check_random_state(0)\n    n_samples, n_features = 1000, 5\n    X_dense = rng.rand(n_samples, n_features)\n    X_sparse = csr_matrix(X_dense)\n\n    y = rng.rand(n_samples)\n\n    for func in (mutual_info_regression, mutual_info_classif):\n        mi_dense = func(X_dense, y, discrete_features='auto')\n        mi_sparse = func(X_sparse, y, discrete_features='auto')\n        assert_array_almost_equal(mi_dense, mi_sparse)  \n", "def test_mutual_info_sparse():\n    # Test that sparse matrix handling is correct.\n    X_sparse = csr_matrix([[1, 0, 0],\n                           [0, 1, 1],\n                           [1, 1, 0]])\n    y = np.array([0, 1, 0])\n\n    mi_sparse = mutual_info_regression(X_sparse, y, random_state=0)\n    mi_dense = mutual_info_regression(X_sparse.toarray(), y, random_state=0)\n    assert_array_almost_equal(mi_sparse, mi_dense)\n", "def test_mutual_info_classif_sparse():\n    # Test that mutual information estimation works correctly for sparse\n    # input matrices.\n    X = csr_matrix([[1, 0, 0],\n                    [0, 1, 0],\n                    [0, 0, 1],\n                    [1, 1, 1],\n                    [0, 1, 0]])\n    y = np.array([0, 1, 2, 0, 1])\n\n    mi = mutual_info_classif(X, y, discrete_features=True)\n    assert_equal(np.sum(mi), 10000)  \n"], "sample_1143": ["def test_floor_div_negative():\n    assert (-S(2)) // S.Half == -4\n    assert S(1) // (-S.Half) == -2\n    assert (-S(1)) // (-S.Half) == 2\n", "compilation error", "def test_gcd():\n    assert gcd(2, 3) == 1\n    assert gcd(6, 9) == 3\n    assert gcd(12, 18) == 6\n    assert gcd(12, 0) == 12\n    assert gcd(0, 12) == 12\n    assert gcd(0, 0) == 0\n    assert gcd(2, 2) == 2\n    assert gcd(2, -2) == 2\n    assert gcd(-2, 2) == 2\n    assert gcd(-2, -2) == 2\n\n\n\n", "def test_mod_with_infinities():\n    assert oo % S.Half is 0\n    assert -oo % S.Half is -0\n    assert oo % oo is oo\n    assert -oo % -oo is -oo\n    assert oo % (-oo) is oo\n    assert -oo % oo is -oo\n\n\n", "def test_issue_13096():\n    assert Float('0.0', 10) == Float('0.0')\n    assert Float('0.0', 10)._prec == 10\n    assert Float('nan', 10)._prec == 10\n"], "sample_282": ["    def test_form_is_bound_with_errors(self):\n        form = ComplexFieldForm({'field1_0': 'some text', 'field1_1': ['X'], 'field1_2_0': '2007-04-25', 'field1_2_1': '06:24:00'})\n        self.assertFalse(form.is_valid())\n        self.assertIn('Invalid choice.', form.errors)\n\n", "    def test_form_bound_data_with_widget_changes(self):\n        form = ComplexFieldForm()\n        form.data = {'field1_0': 'some text', 'field1_1': ['J', 'P'], 'field1_2_0': '2007-04-25', 'field1_2_1': '06:24:00'}\n        form.is_bound = True\n        self.assertEqual(form.cleaned_data['field1'], 'some text,JP,2007-04-25 06:24:00')\n", "    def test_boundfield_label_tag(self):\n        form = ComplexFieldForm({'field1': 'some text,JP,2007-04-25 06:24:00'})\n        bound_field = form.bound_fields['field1']\n        html = bound_field.label_tag()\n        self.assertInHTML('<label for=\"id_field1\">Field1:</label>', html)\n\n\n", "    def test_label_tag(self):\n        form = ComplexFieldForm()\n        self.assertHTMLEqual(\n            form['field1'].label_tag(),\n            '<label for=\"id_field1\">Field1:</label>',\n        )\n        self.assertHTMLEqual(\n            form['field1'].label_tag(contents='', label_suffix=None),\n            '<label for=\"id_field1\"></label>',\n        )\n        self.assertHTMLEqual(\n            form['field1'].label_tag(contents='My custom label'),\n            '<label for=\"id_field1\">My custom label</label>',\n        )\n        self.assertHTMLEqual(\n            form['field1'].label_tag(attrs={'class': 'my-custom-class'}),\n            '<label for=\"id_field1\" class=\"my-custom-class\">Field1:</label>',\n        )\n", "    def test_form_initial_value(self):\n        form = ComplexFieldForm(initial={'field1': 'some text,JP,2007-04-25 06:24:00'})\n        self.assertHTMLEqual(form.as_table(), \n            \"\"\"\n            <tr><th><label for=\"id_field1_0\">Field1:</label></th>\n            <td><input type=\"text\" name=\"field1_0\" value=\"some text\" id=\"id_field1_0\" required>\n            <select multiple name=\"field1_1\" id=\"id_field1_1\" required>\n            <option value=\"J\" selected>John</option>\n            <option value=\"P\" selected>Paul</option>\n            <option value=\"G\">George</option>\n            <option value=\"R\">Ringo</option>\n            </select>\n            <input type=\"text\" name=\"field1_2_0\" value=\"2007-04-25\" id=\"id_field1_2_0\" required>\n            <input type=\"text\" name=\"field1_2_1\" value=\"06:24:00\" id=\"id_field1_2_1\" required></td></tr>\n            \"\"\")\n"], "sample_156": ["    def test_render_with_field_class_override(self):\n        class CustomField(CharField):\n            pass\n\n        class CustomForm(Form):\n            name = CustomField()\n\n        form = CustomForm()\n        form.as_ul()\n        self.assertInHTML(\n            '<li><label for=\"id_name\">Name:</label> <input id=\"id_name\" name=\"name\" type=\"text\"></li>',\n            str(form),\n        )\n", "    def test_renderer_updates_on_form_creation(self):\n        custom = CustomRenderer()\n        form = Form()\n        self.assertEqual(form.renderer, get_default_renderer())\n\n        form = Form(renderer=custom)\n        self.assertEqual(form.renderer, custom)\n", "    def test_clean_with_errors_return_non_dict(self):\n        class FormWithInvalid(Form):\n            f1 = CharField()\n\n                data = self.cleaned_data\n                if not data.get('f1'):\n                    raise ValidationError('Field cannot be empty')\n                return 42  # Return a number instead of a dictionary\n\n        form = FormWithInvalid({'f1': ''})\n        self.assertFalse(form.is_valid())\n        with self.assertRaises(ValidationError):\n            form.clean()\n", "    def test_renderer_context(self):\n        class MyForm(Form):\n            name = CharField()\n\n        form = MyForm()\n        context = {'form': form}\n        renderer = DjangoTemplates()\n        rendered_html = renderer.render('my_form.html', context)\n        self.assertHTML('<form>'\n                        '<div><label for=\"id_name\">Name:</label>'\n                        ' <input id=\"id_name\" name=\"name\" type=\"text\"></div>'\n                        '</form>', rendered_html)\n\n\n\n", "    def test_renderer_is_callable(self):\n        class CustomRenderer:\n                return 'CUSTOM RENDERER OUTPUT'\n        form = Form(renderer=CustomRenderer())\n        self.assertEqual(form.render(), 'CUSTOM RENDERER OUTPUT')\n\n\n\n"], "sample_1075": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_626": ["compilation error", "compilation error", "    def test_restore_dtype_on_multiindexes_object(self) -> None:\n        foo = xr.Dataset(coords={\"bar\": (\"bar\", np.array([0, 1], dtype=object))})\n        foo = foo.stack(baz=(\"bar\",))\n        assert str(foo[\"bar\"].values.dtype) == object\n", "def test_create_variables_multi_level_index():\n    data = {\n        \"foo\": [1, 2, 3, 4, 5],\n        \"bar\": [10, 20, 30, 40, 50],\n        \"baz\": [100, 200, 300, 400, 500],\n    }\n    pd_idx = pd.MultiIndex.from_arrays(\n        [\n            np.array([\"a\", \"a\", \"b\", \"b\", \"c\"], dtype=\"U1\"),\n            np.array([1, 2, 1, 2, 3], dtype=\"i\"),\n        ],\n        names=[\"one\", \"two\"],\n    )\n    index = PandasMultiIndex(pd_idx, \"x\")\n    index_vars = {\n        \"x\": IndexVariable(\"x\", pd_idx),\n        \"foo\": IndexVariable(\"x\", data[\"foo\"], attrs={\"unit\": \"m\"}),\n        \"bar\": IndexVariable(\"x\", data[\"bar\"], encoding={\"fill_value\": 0}),\n        \"baz\": IndexVariable(\"x\", data[\"baz\"], encoding={\"dtype\": \"f4\"}),\n    }\n\n    actual = index.create_variables(index_vars)\n\n\n    # Assert that each variable was created correctly\n    assert_identical(actual[\"x\"], index_vars[\"x\"])\n    assert_identical(actual[\"foo\"], index_vars[\"foo\"])\n    assert_identical(actual[\"bar\"], index_vars[\"bar\"])\n    assert_identical(actual[\"baz\"], index_vars[\"baz\"])\n    assert actual[\"baz\"].dtype == index_vars[\"baz\"].dtype\n", "compilation error"], "sample_444": ["    def test_css_import_in_media_queries(self):\n        # Test that CSS imports within media queries are handled correctly.\n        with self.subTest(\n            \"Importing a different file for each media query\"\n        ):\n            self.create_media_query_test_files(\n                {\n                    \"screen\": \"screen.css\",\n                    \"print\": \"print.css\",\n                    \"all\": \"all.css\",\n                }\n            )\n\n            # The order in which files are listed in the manifest shouldn't change\n            # the ordering of the imports in the media queries.\n            with self.modify_settings(STATICFILES_DIRS={\"append\": self._temp_dir}):\n                finders.get_finder.cache_clear()\n                call_command(\n                    \"collectstatic\",\n                    interactive=False,\n                    verbosity=0,\n                )\n                relpath = self.hashed_file_path(\"test/media_queries.css\")\n                with storage.staticfiles_storage.open(relpath) as relfile:\n                    content = relfile.read()\n                    self.assertIn(b\"url('screen.c6b72ca8728e.css')\", content)\n                    self.assertIn(b\"url('print.ec9259214598.css')\", content)\n                    self.assertIn(b\"url('all.d9d17396803e.css')\", content)\n", "    def test_empty_manifest_after_remove_files(self):\n        # Create initial static files.\n        file_contents = (\n            (\"foo.png\", \"foo\"),\n            (\"bar.css\", 'url(\"foo.png\")\\nurl(\"xyz.png\")'),\n            (\"xyz.png\", \"xyz\"),\n        )\n        for filename, content in file_contents:\n            with open(self._get_filename_path(filename), \"w\") as f:\n                f.write(content)\n\n        with self.modify_settings(STATICFILES_DIRS={\"append\": self._temp_dir}):\n            finders.get_finder.cache_clear()\n            err = StringIO()\n            # First collectstatic run.\n            call_command(\"collectstatic\", interactive=False, verbosity=0, stderr=err)\n            #Remove some files\n            os.remove(self._get_filename_path(\"xyz.png\"))\n            #collectstatic again\n            call_command(\"collectstatic\", interactive=False, verbosity=0, stderr=err)\n            # The manifest should not contain \"xyz.png\" anymore\n            _, manifest_hash = storage.staticfiles_storage.load_manifest()\n            self.assertNotIn(\"xyz.png\", manifest_hash)\n\n\n\n", "    def test_empty_manifest(self):\n        with self.mock.patch(\"django.contrib.staticfiles.storage.ManifestStaticFilesStorage.load_manifest\") as load_manifest_mock:\n            load_manifest_mock.return_value = (\n                {},\n            )  \n            stat = storage.staticfiles_storage.get_file_path(\"nonexistent/file.txt\")\n            self.assertEqual(stat, None)\n", "    def test_non_static_files_ignored(self):\n        with self.modify_settings(STATICFILES_DIRS={\"append\": self._temp_dir}):\n            finders.get_finder.cache_clear()\n            call_command(\"collectstatic\", interactive=False, verbosity=0)\n\n        non_static_file_path = self._get_filename_path(\"non_static.txt\")\n        with self.assertRaises(FileNotFoundError):\n            self.hashed_file_path(non_static_file_path)\n\n", "    def test_manifest_hash_update_after_file_change(self):\n        with self.modify_settings(STATICFILES_DIRS={\"append\": self._temp_dir}):\n            finders.get_finder.cache_clear()\n            self._create_test_files()\n            initial_manifest_hash = storage.staticfiles_storage.manifest_hash\n            self.run_collectstatic()\n            self.assertNotEqual(storage.staticfiles_storage.manifest_hash, initial_manifest_hash)\n\n            file_path = self._get_filename_path(\"test/test.css\")\n            with open(file_path, \"w\") as f:\n                f.write(\"url('foo.png')\")\n            self.run_collectstatic()\n            self.assertNotEqual(storage.staticfiles_storage.manifest_hash, initial_manifest_hash)\n\n"], "sample_1081": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_254": ["    def test_inline_formset_error_list(self):\n        self.admin_login(username='super', password='secret')\n        self.selenium.get(self.live_server_url + reverse('admin:admin_inlines_holder5_add'))\n        # Inlines without errors, both inlines collapsed\n        self.selenium.find_element_by_xpath('//input[@value=\"Save\"]').click()\n        stacked_inline_formset_selector = 'div#inner5stacked_set-group'\n        tabular_inline_formset_selector = 'div#inner5tabular_set-group'\n        self.assertEqual(\n            len(self.selenium.find_elements_by_css_selector(stacked_inline_formset_selector + ' ul.errorlist')), 0\n        )\n        self.assertEqual(\n            len(self.selenium.find_elements_by_css_selector(tabular_inline_formset_selector + ' ul.errorlist')), 0\n        )\n        # Inlines with errors, both inlines expanded\n        show_links = self.selenium.find_elements_by_link_text('SHOW')\n        for show_index, _ in enumerate(show_links):\n            show_links[show_index].click()\n        hide_links = self.selenium.find_elements_by_link_text('HIDE')\n        for hide_index, _ in enumerate(hide_links):\n            hide_links[hide_index].click()\n        self.selenium.find_element_by_xpath('//input[@value=\"Save\"]').click()\n        self.assertEqual(\n            len(self.selenium.find_elements_by_css_selector(stacked_inline_formset_selector + ' ul.errorlist')), 1\n        )\n        self.assertEqual(\n            len(self.selenium.find_elements_by_css_selector(tabular_inline_formset_selector + ' ul.errorlist')), 1\n        )\n\n\n\n", "    def test_inline_formset_errors_on_initial_load(self):\n        self.admin_login(username='super', password='secret')\n        self.selenium.get(self.live_server_url + reverse('admin:admin_inlines_holder5_add'))\n        self.selenium.find_element_by_id('id_dummy').send_keys('1')\n        test_fields = ['#id_inner5stacked_set-0-dummy', '#id_inner5tabular_set-0-dummy']\n        for field_name in test_fields:\n            self.selenium.find_element_by_id(field_name).send_keys('1')\n\n        with self.wait_page_loaded():\n            self.selenium.find_element_by_xpath('//input[@value=\"Save\"]').click()\n\n        # Check that the error class is present on the fields due to initial load\n        for field_name in test_fields:\n            self.assertEqual(\n                self.selenium.find_element_by_id(field_name).get_attribute(\"class\"),\n                \"error\",\n            )\n\n\n\n", "    def test_inline_formset_error_with_stacked_inline_and_verbose_name(self):\n        self.admin_login(username='super', password='secret')\n        self.selenium.get(self.live_server_url + reverse('admin:admin_inlines_holder6_add'))\n        stacked_inline_formset_selector = 'div#inner6stacked_set-group fieldset.module.collapse'\n        # Fill in the first stacked inline form with a valid value.\n        self.selenium.find_element_by_name('id_inner6stacked_set-0-dummy').send_keys('1')\n        # Inlines with errors, both inlines expanded.\n        self.selenium.find_element_by_xpath('//input[@value=\"Save\"]').click()\n        self.assertEqual(\n            len(self.selenium.find_elements_by_css_selector(stacked_inline_formset_selector + '.collapsed')),\n            0,\n        )\n        self.assertEqual(\n            self.selenium.find_elements_by_css_selector('#inner6stacked_set-group h2').text,\n            \"Stacked Inline with Verbose\",\n        )\n\n\n\n", "    def test_inline_formset_error_message(self):\n        self.admin_login(username='super', password='secret')\n        self.selenium.get(self.live_server_url + reverse('admin:admin_inlines_holder5_add'))\n        self.wait_until_visible('#id_dummy')\n        self.selenium.find_element_by_id('id_dummy').send_keys(1)\n        fields = ['id_inner5stacked_set-0-dummy', 'id_inner5tabular_set-0-dummy']\n        show_links = self.selenium.find_elements_by_link_text('SHOW')\n        for show_index, field_name in enumerate(fields):\n            show_links[show_index].click()\n            self.wait_until_visible(field_name)\n            self.selenium.find_element_by_id(field_name[1:]).send_keys(1)\n        # Before save all inputs have default border\n        self.selenium.find_element_by_xpath('//input[@value=\"Save\"]').click()\n        # Test the error message by css selectors\n        stacked_error_message_selector = 'div#inner5stacked_set-group div.errorlist li'\n        tabular_error_message_selector = 'div#inner5tabular_set-group div.errorlist li'\n        stacked_error_message = self.selenium.find_element_by_css_selector(stacked_error_message_selector).text\n        tabular_error_message = self.selenium.find_element_by_css_selector(tabular_error_message_selector).text\n        self.assertEqual(stacked_error_message, \"This field is required.\")\n        self.assertEqual(tabular_error_message, \"This field is required.\")\n\n\n\n", "    def test_inline_formset_error_input_border_with_required(self):\n        self.admin_login(username='super', password='secret')\n        self.selenium.get(self.live_server_url + reverse('admin:admin_inlines_holder6_add'))\n        stacked_inline_formset_selector = 'div#inner6stacked_set-group fieldset.module.collapse'\n        tabular_inline_formset_selector = 'div#inner6tabular_set-group fieldset.module.collapse'\n\n        # Inlines with required fields and errors, both inlines expanded.\n        self.selenium.find_element_by_id('id_dummy').send_keys(1)\n\n        test_fields = ['#id_inner6stacked_set-0-dummy', '#id_inner6tabular_set-0-dummy']\n        for show_index, field_name in enumerate(test_fields):\n            show_links = self.selenium.find_elements_by_link_text('SHOW')\n            show_links[show_index].click()\n            self.wait_until_visible(field_name)\n            # Leave the required field empty on purpose.\n            # Test the red border around inputs by css selectors\n            stacked_selectors = ['.errors input', '.errors select', '.errors textarea']\n            for selector in stacked_selectors:\n                self.assertBorder(\n                    self.selenium.find_element_by_css_selector(selector),\n                    '1px solid #ba2121',\n                )\n            tabular_selectors = [\n                'td ul.errorlist + input', 'td ul.errorlist + select', 'td ul.errorlist + textarea'\n            ]\n            for selector in tabular_selectors:\n                self.assertBorder(\n                    self.selenium.find_element_by_css_selector(selector),\n                    '1px solid #ba2121',\n                )\n        self.selenium.find_element_by_xpath('//input[@value=\"Save\"]').click()\n\n        # The required field errors haven't been cleared\n        stacked_selectors = ['.errors input', '.errors select', '.errors textarea']\n        for selector in stacked_selectors:\n            self."], "sample_633": ["def test_ignore_signatures_with_docstrings_fail() -> None:\n    output = StringIO()\n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        similar.Run([SIMILAR5, SIMILAR6])\n    assert ex.value.code == 0\n    assert (\n        output.getvalue().strip()\n        == (\n            f'''", "def test_ignore_docstrings_with_empty_lines() -> None:\n    output = StringIO()\n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        similar.Run([\"--ignore-docstrings\", SIMILAR5, SIMILAR6])\n    assert ex.value.code == 0\n    assert (\n        output.getvalue().strip()\n        == \"\"\"", "def test_ignore_specific_lines() -> None:\n    output = StringIO()\n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        similar.Run([\"--ignore-lines=7-9\", SIMILAR1, SIMILAR2])\n    assert ex.value.code == 0\n    assert (\n        output.getvalue().strip()\n        == (\n            f\"\"\"", "def test_ignore_imports_with_same_name() -> None:\n    output = StringIO()\n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        similar.Run([\"--ignore-imports\", SIMILAR7, SIMILAR8])\n    assert ex.value.code == 0\n    assert (\n        output.getvalue().strip()\n        == \"\"\"", "def test_ignore_signatures_with_multiple_arguments() -> None:\n    output = StringIO()\n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        similar.Run([EMPTY_FUNCTION_1, EMPTY_FUNCTION_2])\n    assert ex.value.code == 0\n    assert (\n        output.getvalue().strip()\n        == (\n            f'''"], "sample_463": ["    def test_order_with_respect_to_and_index_together(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\n                    \"Author\",\n                    fields=[(\"id\", models.AutoField(primary_key=True)), (\"name\", models.CharField(max_length=200))],\n                    options={\"order_with_respect_to\": \"book\", \"index_together\": {(\"name\", \"_order\")},},\n                )\n            ]\n\n        migration = Migration(\"some_migration\", \"testapp\")\n        self.assertEqual(migration.suggest_name(), \"author\")\n", "    def test_suggest_name_handles_complex_constraint_names(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.AddConstraint(\n                    \"Person\",\n                    models.UniqueConstraint(\n                        fields=[\"name\", \"email\"],\n                        name=\"person_name_and_email_unique\",\n                    ),\n                )\n            ]\n\n        migration = Migration(\"some_migration\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"person_name_and_email_unique\")\n", "    def test_dont_suggest_name_for_no_migrations(self):\n        migration = migrations.Migration(None, \"test_app\")\n        self.assertIsNone(migration.suggest_name())\n", "    def test_suggest_name_with_custom_operations(self):\n        class CustomOperation(migrations.MigrationOperation):\n                super().__init__()\n                self.name = name\n                self.name_from_operation = name_from_operation\n\n        class Migration(migrations.Migration):\n            operations = [\n                CustomOperation(\"custom_operation_1\"),\n                CustomOperation(\"custom_operation_2\", name_from_operation=\"name2\"),\n            ]\n\n        migration = Migration(\"some_migration\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"custom_operation_1_custom_operation_2_name2\")\n", "    def test_order_with_respect_to_and_index_together(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\n                    \"Author\",\n                    fields=[\n                        (\"id\", models.AutoField(primary_key=True)),\n                        (\"name\", models.CharField(max_length=200)),\n                        (\"books\", models.ManyToManyField(\"Book\", options={\"order_with_respect_to\": \"title\"})),\n                    ],\n                    options={\n                        \"index_together\": {(\"name\", \"books__title\")},\n                    },\n                ),\n                migrations.CreateModel(\n                    \"Book\",\n                    fields=[\n                        (\"id\", models.AutoField(primary_key=True)),\n                        (\"title\", models.CharField(max_length=200)),\n                    ],\n                ),\n            ]\n\n        migration = Migration(\"some_migration\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"author_book\")\n\n"], "sample_350": ["    def test_ordering_and_limit_on_combined_qs(self):\n        qs1 = Number.objects.filter(num__lte=2).order_by('num')\n        qs2 = Number.objects.filter(num__gte=3).order_by('-num')\n        self.assertNumbersEqual(qs1.union(qs2)[:3], [0, 1, 2, 3])\n        self.assertNumbersEqual(\n            qs1.union(qs2).order_by('num')[:2], [0, 1]\n        )\n        self.assertNumbersEqual(\n            qs1.union(qs2).order_by('-num')[:2], [9, 8]\n        )\n\n", "    def test_union_with_order_by_and_limit(self):\n        qs1 = Number.objects.filter(num__lte=1).order_by('num')\n        qs2 = Number.objects.filter(num__gte=2, num__lte=3).order_by('num')\n        self.assertNumbersEqual(qs1.union(qs2)[:2], [0, 1])\n        self.assertNumbersEqual(qs1.union(qs2)[:1], [0])\n        self.assertNumbersEqual(qs1.union(qs2)[-1:], [3])\n\n\n\n", "    def test_union_subqueries(self):\n        qs1 = Number.objects.filter(num__lte=1)\n        qs2 = Number.objects.filter(num__gte=2).order_by('num')\n        self.assertNumbersEqual(\n            qs1.union(\n                Number.objects.filter(\n                    num__gte=1,\n                    num__lte=2,\n                ).order_by('num')\n            ),\n            [0, 1, 2],\n        )\n        self.assertNumbersEqual(\n            qs2.union(qs1).order_by('num'),\n            [0, 1, 2, 8, 9],\n        )\n", "    def test_union_with_distinct_values(self):\n        for num in range(10):\n            ReservedName.objects.create(name=f'name{num}', order=num)\n\n        qs1 = ReservedName.objects.filter(name__startswith='name')\n        qs2 = ReservedName.objects.filter(name__startswith='name2')\n\n        self.assertCountEqual(qs1.union(qs2, all=True).values('name').distinct(), [\n            'name0', 'name1', 'name2', 'name3', 'name4', 'name5', 'name6', 'name7', 'name8', 'name9'\n        ])\n        self.assertEqual(len(list(qs1.union(qs2, all=True))), 10) \n        self.assertEqual(len(list(qs1.union(qs2))), 10) \n\n        self.assertCountEqual(qs1.union(qs2).values('name').distinct(), [\n            'name0', 'name1', 'name2', 'name3', 'name4', 'name5', 'name6', 'name7', 'name8', 'name9'\n        ])\n        self.assertEqual(len(list(qs1.union(qs2))), 10) \n\n\n", "    def test_related_objects_on_combined_qs(self):\n        Celebrity.objects.create(name=\"John Doe\")\n        Number.objects.create(num=1, celebrity=Celebrity.objects.get(name=\"John Doe\"))\n\n        qs1 = Celebrity.objects.all()\n        qs2 = Number.objects.all()\n\n        qs1_combined = qs1.union(qs2)\n        with self.assertRaisesMessage(\n            AttributeError,\n            \"Cannot access related objects on combined querysets\",\n        ):\n            qs1_combined.first().celebrity\n\n        with self.assertRaisesMessage(\n            AttributeError,\n            \"Cannot access related objects on combined querysets\",\n        ):\n            qs1_combined.values_list('celebrity__name').get()\n\n\n\n"], "sample_874": ["def test_out_of_bounds_step():\n    sel = StepSelector(step=10)\n    with pytest.raises(ValueError):\n        sel.fit(X, y)\n", "def test_step_parameter():\n    sel = StepSelector(step=1)\n    Xt = sel.fit_transform(X)\n    assert_array_equal(Xt.shape, (X.shape[0], 5))\n    assert_array_equal(Xt, X[::2])\n\n    sel = StepSelector(step=2)\n    Xt = sel.fit_transform(X)\n    assert_array_equal(Xt.shape, (X.shape[0], 5))\n    assert_array_equal(Xt, X[::2])\n\n    with pytest.raises(ValueError):\n        StepSelector(step=-1).fit_transform(X)\n", "def test_step_value_validation():\n    with pytest.raises(ValueError):\n        StepSelector(step=-1).fit(X)\n    with pytest.raises(ValueError):\n        StepSelector(step=0).fit(X)\n    with pytest.raises(ValueError):\n        StepSelector(step=1.5).fit(X)\n\n\n", "def test_step_value_validation():\n    with pytest.raises(ValueError):\n        StepSelector(step=-1)  # Negative step values are invalid\n\n    with pytest.raises(ValueError):\n        StepSelector(step=0)  # Step 0 should not select any features\n\n    with pytest.raises(TypeError):\n        StepSelector(step=\"a\")  # Step should be a number\n", "def test_step_larger_than_data_size():\n    sel = StepSelector(step=10)\n    X = np.arange(5).reshape(1, 5)\n    with pytest.raises(ValueError):\n        sel.fit(X).transform(X)\n\n    with pytest.raises(ValueError):\n        sel.fit_transform(X)\n"], "sample_197": ["    def test_custom_time_strings(self):\n        time_strings = {\n            'year': ngettext_lazy('year ago', 'years ago'),\n            'month': ngettext_lazy('month ago', 'months ago'),\n        }\n        self.assertEqual(timesince(self.t, self.t + self.oneyear, time_strings=time_strings), '1\\xa0year ago')\n        self.assertEqual(timesince(self.t, self.t + self.onemonth, time_strings=time_strings), '1\\xa0month ago')\n", "    def test_custom_time_strings(self):\n        custom_strings = {\n            'year': ngettext_lazy('a year', 'some years'),\n            'month': ngettext_lazy('a month', 'some months'),\n        }\n        self.assertEqual(timesince(self.t + self.oneyear, self.t, time_strings=custom_strings), 'a year')\n        self.assertEqual(timesince(self.t + self.onemonth, self.t, time_strings=custom_strings), 'a month')\n        self.assertEqual(timeuntil(self.t - self.oneyear, self.t, time_strings=custom_strings), 'a year')\n        self.assertEqual(timeuntil(self.t - self.onemonth, self.t, time_strings=custom_strings), 'a month')\n", "    def test_reversed_timeuntil(self):\n        t = self.t + self.oneday + self.onehour + self.oneminute\n        self.assertEqual(timeuntil(t, self.t), '1\\xa0day, 1\\xa0hour, 1\\xa0minute')\n", "compilation error", "    def test_time_strings_override(self):\n        custom_time_strings = {\n            'year': ngettext_lazy('{} epoch', '{} epochs'),\n            'month': ngettext_lazy('{} lunar month', '{} lunar months'),\n        }\n        self.assertEqual(timesince(self.t, self.t + self.oneyear, time_strings=custom_time_strings), '1\\xa0epoch')\n        self.assertEqual(timesince(self.t, self.t + self.onemonth, time_strings=custom_time_strings), '1\\xa0lunar\\xa0month')\n"], "sample_109": ["    def test_render_options_empty_queryset(self):\n        Band.objects.all().delete()\n        form = AlbumForm()\n        output = form.as_table()\n        self.assertIn(self.empty_option, output)\n\n", "    def test_autocomplete_field_widget_class(self):\n        form = AlbumForm()\n        self.assertIsInstance(form['band'].field.widget, AutocompleteSelect)\n        self.assertIsInstance(form['featuring'].field.widget, AutocompleteSelect)\n", "    def test_render_options_with_empty_values(self):\n        beatles = Band.objects.create(name='The Beatles', style='rock')\n        who = Band.objects.create(name='The Who', style='rock')\n\n        # Check with an empty value and an empty option present\n        form = AlbumForm(initial={'band': None})\n        output = form.as_table()\n        self.assertIn(self.empty_option, output)\n\n        # Check with an empty value but without the empty option\n        form = AlbumForm(data={'band': ''})\n        output = form.as_table()\n        self.assertNotIn(self.empty_option, output)\n", "    def test_autocomplete_renders_placeholder(self):\n        form = AlbumForm()\n        output = form.as_table()\n        self.assertIn('<span class=\"select2-container select2-container--default\" style=\"width: 220px;\">', output)\n        self.assertIn('placeholder=\"Choose Band...\"', output)\n", "    def test_media_debug(self):\n        with override_settings(DEBUG=True):\n            rel = Album._meta.get_field('band').remote_field\n            w = AutocompleteSelect(rel, admin.site)\n            self.assertEqual(w.media._js, [\n                'admin/js/vendor/jquery/jquery.js',\n                'admin/js/vendor/select2/select2.full.js',\n                'admin/js/jquery.init.js',\n                'admin/js/autocomplete.js',\n            ])\n\n\n\n"], "sample_129": ["    def test_rounding_errors(self):\n        self.assertEqual(floatformat(0.1 + 0.2 - 0.3, 2), '0.00')\n        self.assertEqual(floatformat(1.1 + 2.2 - 3.3, 2), '0.00')\n\n", "    def test_floatformat_negative_integer_as_string(self):\n        self.assertEqual(floatformat(\"-123\"), '-123.0')\n        self.assertEqual(floatformat(\"-123.456\"), '-123.456')\n        self.assertEqual(floatformat(\"-123.456\", 2), '-123.46')\n", "    def test_locale_awareness(self):\n        with localcontext() as ctx:\n            ctx.rounding = 'ROUND_HALF_UP'\n            self.assertEqual(floatformat(1.2345, 2), '1.23')\n            self.assertEqual(floatformat(15.2042, -3), '15.204')\n            ctx.grouping = (',', '.')\n            self.assertEqual(floatformat(1234.56789, 2), '1.234,56')\n            self.assertEqual(floatformat(1234.56789, 0), '1.234')\n", "    def test_floatformat_with_error(self):\n        output = self.engine.render_to_string('floatformat_with_error', {\"a\": \"1.42\", \"b\": mark_safe(\"1.42\")})\n        self.assertEqual(output, \"1.42000 1.42000\")\n ", "    def test_negative_numbers(self):\n        tests = [\n            (-1.234, 2, '-1.23'),\n            (-123.456, 0, '-123'),\n            (-12345.678, -2, '-12345.68'),\n            (-1.2345, 3, '-1.235'),\n            (-0.12345, 2, '-0.12'),\n        ]\n        for num, decimal_places, expected in tests:\n            with self.subTest(num=num, decimal_places=decimal_places):\n                self.assertEqual(floatformat(num, decimal_places), expected)\n"], "sample_42": ["def test_equivalencies_subclassing():\n    class MyEquivalency(u.Equivalency):\n            super().__init__(\n                (u.m, u.s),\n                lambda x, y: x * u.k,\n                [u.m, u.s],\n                [u.k]\n            )\n\n    eq = MyEquivalency()\n    assert eq.get_scale_factor() == u.k\n    x = 1 * u.m / u.s\n    assert_quantity_allclose(x.to(u.k, equivalencies=eq), 1 * u.k)\n\n    y = 1 * u.k\n    assert_quantity_allclose(y.to(u.m / u.s, equivalencies=eq), 1 * u.m / u.s)\n\n\n\n", "def test_redshift():\n    z = 0.5 * u.one\n    Dl = 1000 * u.Mpc\n    assert_quantity_allclose(Dl.to(u.Mpc, redshift=z), 1500 * u.Mpc)\n    assert_quantity_allclose(Dl.to(u.cm, redshift=z), 1.5e+24 * u.cm)\n    assert_quantity_allclose(Dl.to(u.kpc, redshift=z), 1000 * u.kpc)\n\n\n\n\n", "def test_parse_equivalency_strings():\n    eq_str = \"1 m = 100 cm\"\n    eq = u.parse_equivalency_string(eq_str)\n    assert eq == ((u.m, 1), (u.cm, 100))\n\n    eq_str = \"1 pc = 3.086e+16 m\"\n    eq = u.parse_equivalency_string(eq_str)\n    assert eq == ((u.pc, 1), (u.m, 3.086e+16))\n\n    eq_str = \"1 Lsun = 3.828e+26 W\"\n    eq = u.parse_equivalency_string(eq_str)\n    assert eq == ((u.Lsun, 1), (u.W, 3.828e+26))\n\n    eq_str = \"1 erg = 1e-7 J\"\n    eq = u.parse_equivalency_string(eq_str)\n    assert eq == ((u.erg, 1), (u.J, 1e-7))\n\n    with pytest.raises(ValueError):\n        u.parse_equivalency_string(\"1 m = 1 cm 2\")\n\n    with pytest.raises(ValueError):\n        u.parse_equivalency_string(\"1 pc = 3.086e+16 km\")\n\n    with pytest.raises(ValueError):\n        u.parse_equivalency_string(\"1 pc = 3.086e+16 meter\")\n\n\n", "def test_distance_modules():\n    # Test distance modules, ensuring proper conversion\n    # between different distance scales\n\n    from astropy.units import Mpc, kpc, cm, parsecs\n    with u.set_enabled_equivalencies(u.cosmological()):\n        cosmo = cosmology.default_cosmology.get()\n\n        # Check conversion between Mpc and kpc\n        mpc = 1 * Mpc\n        kpc_equivalent = mpc.to(kpc)\n        assert_allclose(kpc_equivalent.value, 1000)\n\n        # Check conversion between Mpc and cm\n        mpc = 1 * Mpc\n        cm_equivalent = mpc.to(cm)\n        assert_allclose(cm_equivalent.value, 3.086e22)\n\n        # Check conversion between Mpc and parsecs\n        mpc = 1 * Mpc\n        pc_equivalent = mpc.to(parsecs)\n        assert_allclose(pc_equivalent.value, 3.261564e+6)\n\n        # Test inverse conversions\n        kpc_equivalent = 1000 * kpc\n        mpc_converted = kpc_equivalent.to(Mpc)\n        assert_allclose(mpc_converted.value, 1)\n\n        cm_equivalent = 3.086e22 * cm\n        mpc_converted = cm_equivalent.to(Mpc)\n        assert_allclose(mpc_converted.value, 1)\n\n        pc_equivalent = 3.261564e+6 * parsecs\n        mpc_converted = pc_equivalent.to(Mpc)\n        assert_allclose(mpc_converted.value, 1)\n\n        # Test conversions with cosmological distances for a specific redshift\n        z = 0.5\n        comoving_distance = cosmo.comoving_distance(z).to(Mpc)\n        assert_quantity_allclose(comoving_distance.to(kpc), cosmo.comoving_distance(z, unit=kpc))\n        assert_quantity_allclose(comoving_distance.to(cm), cosmo.comoving_distance(z, unit=cm))\n        assert_quantity_allclose(comoving_distance.to(parse", "compilation error"], "sample_592": ["compilation error", "compilation error", "    def test_array_repr_with_data_attrs(self):\n        da = xr.DataArray(\n            np.arange(100).reshape(10, 10),\n            dims=(\"x\", \"y\"),\n            coords={\"x\": np.arange(10), \"y\": np.arange(10)},\n            attrs={\"long_attr\": \"This is a long attribute value that should be shortened.\" * 10},\n        )\n        actual = formatting.array_repr(da)\n        expected = dedent(\n            \"\"\"\\\n        <xarray.DataArray (10, 10)>\n        array([[ 0  1  2 ... 8  9],\n               [10 11 12 ... 18 19],\n               [20 21 22 ... 28 29],\n               ...\n               [80 81 82 ... 88 89],\n               [90 91 92 ... 98 99]])\n        Dimensions without coordinates: None\n        Attributes:\n          long_attr: ...\"\"\"\n        )\n        assert actual == expected\n", "def test_unindexed_dims_repr(self):\n    da = xr.DataArray(np.arange(30).reshape(5, 6), dims=(\"x\", \"y\"))\n    da = da.rename({\"y\": \"time\"})\n    da = da.drop_dims(\"time\")\n    expected = \"Dimensions without coordinates: time\"\n    actual = unindexed_dims_repr(da.dims, da.coords)\n    assert actual == expected \n", "    def test_format_array_flat_truncated(self):\n        actual = formatting.format_array_flat(np.arange(100), 5)\n        expected = \"0 ... 99\"\n        assert actual == expected\n\n        actual = formatting.format_array_flat(np.arange(100), 10)\n        expected = \"...\"\n        assert actual == expected\n\n\n"], "sample_365": ["    def test_lazy_keep_lazy_decorator(self):\n        @keep_lazy(int, str)\n            return arg1 + arg2\n\n        # All arguments are lazy\n        lazy_arg1 = lazy(lambda: 1, int)\n        lazy_arg2 = lazy(lambda: '2', str)\n        self.assertEqual(func(lazy_arg1, lazy_arg2), 3)\n\n        # One argument is lazy, one is not\n        lazy_arg1 = lazy(lambda: 1, int)\n        arg2 = '2'\n        self.assertEqual(func(lazy_arg1, arg2), 3)\n\n        # Both arguments are not lazy\n        arg1 = 1\n        arg2 = '2'\n        self.assertEqual(func(arg1, arg2), 3)\n\n\n\n", "    def test_lazy_with_args(self):\n            return x * 2\n        lazy_times_two = lazy(times_two, int)\n        self.assertEqual(lazy_times_two(3), 6)\n        self.assertEqual(lazy_times_two(42), 84)\n", "    def test_classproperty_no_args(self):\n        class Foo:\n            @classproperty\n                return 123\n\n        self.assertEqual(Foo.foo, 123)\n", "    def test_keep_lazy_decorator(self):\n            return a + b\n\n        @keep_lazy\n            return my_func(a, b)\n\n        non_lazy_args = 1, 2\n        lazy_arg = lazy(lambda: 3, int)\n        self.assertEqual(wrapped_func(*non_lazy_args), 3)\n        self.assertEqual(wrapped_func(*non_lazy_args, lazy_arg), 6)\n        self.assertEqual(wrapped_func(lazy_arg, lazy_arg), 6)\n\n", "    def test_lazy_deepcopy(self):\n        class Klazz:\n                self.value = value\n\n                return Klazz(value=self.value)\n\n        lazy_klazz = lazy(lambda: Klazz(1), Klazz)\n        original_klazz = Klazz(1)\n\n        lazy_klazz_copy = copy.deepcopy(lazy_klazz())\n        original_klazz_copy = copy.deepcopy(original_klazz)\n\n        self.assertNotSame(lazy_klazz_copy, lazy_klazz())\n        self.assertNotSame(original_klazz_copy, original_klazz)\n        self.assertEqual(lazy_klazz_copy.value, 1)\n        self.assertEqual(original_klazz_copy.value, 1)\n"], "sample_1154": ["    def test_nullspace():\n        # Ensure that the nullspace routine handles cases with\n        # leading zeros and multiple leading zeros\n        eqs = [\n            x + y + z,\n            x + 2*y + 3*z,\n            2*x + 3*y + 4*z\n        ]\n        sol = _linsolve(eqs, [x, y, z])\n        assert sol == {x: -2*y - 3*z, y: y, z: z}\n", "def test__linsolve_non_homogeneous():\n    eqs = [\n        x + y + z + 1,\n        2*x - y + 2*z - 2,\n        x + 2*y - z + 1\n    ]\n    sol = {x:1, y:-1, z:0}\n    assert _linsolve(eqs, (x, y, z)) == sol\n\n", "def test__linsolve_large_sparse():\n    from sympy.abc import a, b, c, d, e, f, g, h, i, j\n    from sympy.polys.matrices.linsolve import _linsolve\n\n    n = 100\n    eqs = []\n    for i in range(n):\n        eq = Eq(sum(a*x for a, x in zip([1] * (n+1), [a, b, c, d, e, f, g, h, i, j])) , i*i)\n        eqs.append(eq)\n\n    sol = _linsolve(eqs, [a, b, c, d, e, f, g, h, i, j])\n    assert sol == {a:S.Zero, b:S.Zero, c:S.Zero, d:S.Zero, e:S.Zero, f:S.Zero, g:S.Zero, h:S.Zero, i:S.Zero, j:S.Zero} \n", "def test__linsolve_non_linear_term_in_coefficient():\n    with raises(PolyNonlinearError):\n        _linsolve([Eq(x + y, x*y)], [x, y])\n", "compilation error"], "sample_841": ["def test_ridge_use_sample_weights_with_sparse_matrix():\n    # Test fitting Ridge with sample_weights when X is sparse\n    rng = np.random.RandomState(42)\n    n_samples, n_features = 100, 5\n    X = rng.rand(n_samples, n_features)\n    y = rng.rand(n_samples)\n    sample_weights = rng.rand(n_samples)\n    X_sparse = sp.csr_matrix(X)\n\n    ridge = Ridge(alpha=1.)\n    ridge.fit(X_sparse, y, sample_weight=sample_weights)\n    assert isinstance(ridge.coef_, np.ndarray)\n", "def test_ridge_sparse_fit_intercept_on_small_dense():\n    rng = np.random.RandomState(42)\n    X = rng.rand(10, 5)\n    y = rng.rand(10)\n    dense_ridge = Ridge(fit_intercept=True).fit(X, y)\n    sparse_X = sp.csr_matrix(X)\n    sparse_ridge = Ridge(fit_intercept=True).fit(sparse_X, y)\n\n    assert_allclose(dense_ridge.intercept_, sparse_ridge.intercept_)\n\n    \n", "def test_ridge_sag_with_X_fortran_sparse():\n    # check that Fortran array are converted when using SAG solver with sparse matrix\n    n_samples, n_features = 100, 5\n    X, y = make_regression(n_samples=n_samples, n_features=n_features, random_state=42)\n    \n    # for the order of X and y to not be C-ordered arrays\n    X = np.asfortranarray(X)\n    X = sp.csr_matrix(X)\n\n    Ridge(solver='sag').fit(X, y)\n", "def test_ridge_validation_sets():\n    from sklearn.model_selection import train_test_split\n    X, y = make_regression(random_state=0)\n    X_train, X_val, y_train, y_val = train_test_split(\n        X, y, test_size=0.2, random_state=0\n    )\n    ridge = Ridge()\n    ridge.fit(X_train, y_train)\n    y_pred = ridge.predict(X_val)\n    assert len(y_pred) == len(y_val)\n\n\n\n", "def test_ridge_solver_warnings():\n    # Test for specific warnings raised by different solvers.\n    X, y = make_regression(n_samples=10, n_features=3, random_state=0)\n\n    for solver in ('sparse_cg', 'sag', 'saga', 'lsqr', 'cholesky'):\n        ridge = Ridge(solver=solver, alpha=1.0)\n        with pytest.warns(UserWarning,\n                          match=r\"The 'sparse_cg' solver doesn't support\"):\n            ridge.fit(X, y)\n\n    with pytest.raises(ValueError, match=r\"Unknown solver\\s*\"):\n        Ridge(solver='bogus').fit(X, y)\n\n    \n\n"], "sample_835": ["def test_feature_importances_sparse():\n    # Check feature importances with sparse input.\n    X, y = datasets.make_classification(n_samples=1000,\n                n_features=20,\n                n_informative=5,\n                n_redundant=0,\n                n_repeated=0,\n                shuffle=False,\n                random_state=1)\n    X_sparse = csc_matrix(X)\n\n    clf = AdaBoostClassifier()\n    clf.fit(X_sparse, y)\n    importances = clf.feature_importances_\n\n    assert importances.shape[0] == 20\n    assert (importances[:5] >= importances[5:]).all()\n", "def test_sparse_matrix_weighting():\n    # Check that sample_weights are properly handled with sparse matrices.\n\n    from sklearn.datasets import make_regression\n    from sklearn.metrics import mean_squared_error\n    from scipy.sparse import csr_matrix\n\n    X, y = make_regression(n_samples=100, n_features=20, random_state=42)\n    sample_weights = np.random.rand(100)\n\n    # Convert to sparse matrix\n    X_sparse = csr_matrix(X)\n\n    # Create an AdaBoostRegressor instance\n    boost = AdaBoostRegressor(random_state=42, n_estimators=10)\n\n    # Fit with dense weights\n    boost_dense = AdaBoostRegressor(random_state=42, n_estimators=10)\n    boost_dense.fit(X, y, sample_weight=sample_weights)\n\n    # Fit with sparse weights\n    boost_sparse = AdaBoostRegressor(random_state=42, n_estimators=10)\n    boost_sparse.fit(X_sparse, y, sample_weight=csr_matrix(sample_weights))\n\n    # Compare predictions\n    y_pred_dense = boost_dense.predict(X)\n    y_pred_sparse = boost_sparse.predict(X_sparse.toarray())\n\n    # Check that predictions are similar\n    assert_array_almost_equal(y_pred_dense, y_pred_sparse)\n\n    # Check that MSE is similar\n    mse_dense = mean_squared_error(y, y_pred_dense)\n    mse_sparse = mean_squared_error(y, y_pred_sparse)\n    assert_array_almost_equal(mse_dense, mse_sparse)\n\n\n\n", "def test_adaboost_consistent_predict_reg(algorithm):\n    # check that predict_proba and predict give consistent results\n    # regression test for:\n    # https://github.com/scikit-learn/scikit-learn/issues/14084\n    X_train, X_test, y_train, y_test = train_test_split(\n        *datasets.load_boston(return_X_y=True), random_state=42\n    )\n    model = AdaBoostRegressor(algorithm=algorithm, random_state=42)\n    model.fit(X_train, y_train)\n\n    assert_array_almost_equal(\n        model.predict(X_test),\n        model.predict(X_test)\n    )\n", "def test_sparse_predict_proba():\n    # Check predict_proba with sparse input.\n    from sklearn.ensemble import RandomForestClassifier\n\n    X, y = datasets.make_classification(n_samples=15, n_features=5,\n                random_state=42)\n\n    # Flatten y to a 1d array\n    y = np.ravel(y)\n\n    for sparse_format in [csc_matrix, csr_matrix, lil_matrix, coo_matrix, dok_matrix]:\n        X_sparse = sparse_format(X)\n        \n        # Train a classifier on the sparse input\n        clf = AdaBoostClassifier(base_estimator=RandomForestClassifier(),\n                            algorithm=\"SAMME\", random_state=1).fit(X_sparse, y)\n\n        # Predict probabilities for the sparse input\n        sparse_proba = clf.predict_proba(X_sparse)\n\n        # Compare to dense predict_proba\n        dense_proba = clf.predict_proba(X)\n        assert_array_almost_equal(sparse_proba, dense_proba) \n", "def test_adaboost_predict_proba_with_sample_weight(algorithm):\n    \"\"\"Test predict_proba with sample weights.\"\"\"\n    X, y = datasets.make_classification(n_samples=100,\n                n_features=20,\n                random_state=0)\n    w = np.random.rand(100)\n    clf = AdaBoostClassifier(algorithm=algorithm, random_state=0)\n    clf.fit(X, y, sample_weight=w)\n    from sklearn.metrics import classification_report\n\n    y_pred_proba = clf.predict_proba(X)\n    y_pred = clf.predict(X)\n    print(f\"y_pred_proba: {y_pred_proba}\")\n    print(f\"y_pred: {y_pred}\")\n    print(f\"w: {w}\")\n    print(classification_report(y, y_pred, sample_weight=w))\n\n\n\n\n"], "sample_370": ["    def test_prefetch_related_m2m_related_fields(self):\n        B = Book.objects.using('other')\n        A = Author.objects.using('other')\n        book1 = B.create(title=\"Poems\")\n        book2 = B.create(title=\"Sense and Sensibility\")\n\n        A.create(name=\"Charlotte Bronte\", first_book=book1)\n        A.create(name=\"Jane Austen\", first_book=book2)\n\n        # Forward\n        with self.assertNumQueries(2, using='other'):\n            books = B.objects.prefetch_related('authors').using('other')\n\n        with self.assertNumQueries(0):\n            self.assertEqual(len(books), 2)\n            self.assertEqual(books[0].authors.all().count(), 1)\n            self.assertEqual(books[1].authors.all().count(), 1)\n\n\n\n", "    def test_prefetch_related_with_chained_related_fields(self):\n        \"\"\"\n        Regression for https://www.djangoproject.com/queries/?test_id=67805\n\n        When prefetching a related field, and the related field has\n        its own related field, the prefetch should work correctly.\n        \"\"\"\n        author1 = AuthorWithAge.objects.create(name='Jane', first_book=None, age=30)\n        author2 = AuthorWithAge.objects.create(name='Peter', first_book=None, age=40)\n\n        book1 = BookWithYear.objects.create(title='Test Book 1', published_year=2021)\n        book2 = BookWithYear.objects.create(title='Test Book 2', published_year=2022)\n\n        book1.authors.add(author1)\n        book2.authors.add(author2)\n\n        BookReview.objects.create(book=book1, content='Good book')\n        BookReview.objects.create(book=book2, content='Better book')\n\n        with self.assertNumQueries(4):\n            authors = AuthorWithAge.objects.prefetch_related(\n                Prefetch('first_book', queryset=BookWithYear.objects.prefetch_related(\n                    Prefetch('bookreview_set')\n                ))\n            )\n        with self.assertNumQueries(0):\n            for author in authors:\n                reviews = author.first_book.bookreview_set.all()\n                self.assertTrue(reviews)\n\n", "    def test_nested_prefetch_with_multiple_related_objects(self):\n        \"\"\"\n        Nested prefetching should correctly handle multiple related objects,\n        even if the relationships are indirect.\n        \"\"\"\n        # Create some books, authors, and reviews\n        book1 = Book.objects.create(title='The Catcher in the Rye')\n        book2 = Book.objects.create(title='Pride and Prejudice')\n        author1 = AuthorWithAge.objects.create(name='J.D. Salinger',\n                                              first_book=book1,\n                                              age=37)\n        author2 = AuthorWithAge.objects.create(name='Jane Austen',\n                                              first_book=book2,\n                                              age=41)\n        review1 = BookReview.objects.create(book=book1,\n                                           content='A classic coming-of-age story.')\n        review2 = BookReview.objects.create(book=book2,\n                                           content='Love, marriage, and social class')\n\n        # Pre-fetch the books, authors, and reviews, including nested\n        # relationships\n        queryset = Book.objects.prefetch_related(\n            'authors',\n            Prefetch(\n                'reviews',\n                queryset=BookReview.objects.select_related('book',).prefetch_related(\n                    Prefetch('book__authors', to_attr='book_authors'),\n                ),\n            ),\n        )\n        with self.assertNumQueries(3):\n            books = queryset.all()\n\n        # Verify the results\n        for book in books:\n            self.assertEqual(book.reviews[0].book, book)  # Check the review's book\n            for review in book.reviews:\n                self.assertIn(book.authors[0], review.book_authors)\n\n", "    def test_nested_prefetch_with_multiple_levels_of_nesting(self):\n        \"\"\"\n        Similar to test_nested_prefetch_is_not_overwritten_by_related_object but\n        with an extra level of nesting.\n        \"\"\"\n        house = House.objects.create(name='Big house', address='123 Main St')\n        room = Room.objects.create(name='Kitchen', house=house)\n        appliance = Appliance.objects.create(name='Fridge', room=room)\n\n        queryset = House.objects.only('name').prefetch_related(\n            Prefetch(\n                'rooms',\n                queryset=Room.objects.prefetch_related(\n                    Prefetch(\n                        'appliances', queryset=Appliance.objects.only('name'),\n                    ),\n                ),\n            ),\n        )\n        with self.assertNumQueries(4):\n            house = queryset.first()\n\n        self.assertIs(Appliance.name.is_cached(appliance), True)\n        with self.assertNumQueries(0):\n            house.rooms.first().appliances.first().name\n", "    def test_nested_prefetch_with_transform_is_cached(self):\n        queryset = House.objects.only('name').prefetch_related(\n            Prefetch(\n                'rooms',\n                Room.objects.prefetch_related(\n                    Prefetch(\n                        'house', queryset=House.objects.only('address'),\n                        to_attr='house_address',\n                    ),\n                ).annotate(room_address=ExpressionWrapper(\n                    Concat(\"house_address\", \" \", \"name\"), output_field=CharField()\n                )),\n            ),\n        )\n        with self.assertNumQueries(3):\n            house = queryset.first()\n\n        with self.assertNumQueries(0):\n            for room in house.rooms.all():\n                self.assertIsNotNone(room.room_address)\n                self.assertIs(room.house_address, house.address)\n"], "sample_985": ["def test_as_dict():\n    x, y, z = symbols('x y z')\n    f1 = sin(x) + cos(y)/gamma(z)\n    d = f1.as_dict()\n    assert d == {'sin(x)': 1, 'cos(y)/gamma(z)': 1}\n    f2 = 2*x**2 + 3*y - z\n    d = f2.as_dict()\n    assert d == {'2*x**2': 2, '3*y': 3, '-z': -1}\n", "def test_pow():\n    from sympy import log, oo, pi, S\n\n    x = symbols('x')\n    y = symbols('y')\n    a = symbols('a')\n\n    assert (x**y).as_base_exp() == (x, y)\n    assert (x**y).func == Pow\n    assert (x**y).args == (x, y)\n\n    assert (x**0).evalf() == 1\n    assert (x**1).evalf() == x\n    assert (x**2).evalf() == x**2\n    assert (x**-1).evalf() == 1/x\n    assert (x**oo).evalf() == oo\n\n    p = Pow(x, y)\n    assert p.subs(y, 2) == x**2\n    assert p.subs(x, 2) == 2**y\n\n    assert (a**b).rewrite(exp, 2*log(a)) == (exp(2*log(a)*b))\n\n    assert (a**1/b).rewrite(exp) == exp(log(a)/b)\n    assert (a**-1/b).rewrite(exp) == exp(-log(a)/b)\n    assert (a**(1/(a*b))).rewrite(exp) == exp(log(a)/(a*b))\n\n    assert ((x**y)**z).rewrite(Pow) == Pow(x, y*z)\n    assert (x**(y*z)).rewrite(Pow) == x**(y*z)\n\n    assert (log(x)**y).rewrite(exp) == exp(y*log(log(x)))\n    assert (sin(x)**y).rewrite(exp) == exp(y*I*log(sin(x)))\n    assert (cos(x)**y).rewrite(exp) == exp(y*I*log(cos(x)))\n\n    assert (x**S('x')).rewrite(exp) == exp(x*log(x))\n\n    assert (x**S('2')).rewrite(Pow) == x**2\n    assert (x**S('3')).rewrite(Pow) == x**3\n\n    assert (x**y).evalf(2) == x**y.evalf", "compilation error", "def test_evalf_with_limits():\n    from sympy.integrals.integrals import Integral\n    from sympy.core import symbols, limit\n    x = symbols('x')\n\n    # check that evalf handles limits correctly\n    i = Integral(x, (x, 0, 1))\n    assert i.evalf(2) == 0.5\n    l = limit(i, x, 1, dir='+')\n    assert l.evalf(2) == 1.0\n", "    def test_is_comparable():\n        x, y, z = symbols('x y z')\n        a, b = symbols('a b')\n\n        expr1 = sin(x) + cos(x)\n        expr2 = sin(x) - cos(x)\n        expr3 = x**2\n\n        assert expr1.is_comparable is False\n        assert expr2.is_comparable is False\n        assert expr3.is_comparable is True\n\n        expr4 = Piecewise((x, x > 0), (y, x <= 0))\n        expr5 = Piecewise((y, x > 0), (x, x <= 0))\n\n        assert expr4.is_comparable is False\n        assert expr5.is_comparable is False\n\n        expr6 = 1 + sin(x)\n        expr7 = 1 - sin(x)\n        expr8 = x + y\n\n        assert expr6.is_comparable is False\n        assert expr7.is_comparable is False\n        assert expr8.is_comparable is True\n\n    \n\n"], "sample_670": ["def test_nested_parentheses() -> None:\n    assert evaluate(\"((true))\", lambda ident: True) == True\n    assert evaluate(\"(((true and false)))\", lambda ident: True) == False\n    assert evaluate(\"((true and true) or false)\", lambda ident: True) == True\n    assert evaluate(\"not ((true and (false or true)))\", lambda ident: True) == False\n", "def test_multiple_function_calls(expr: str, matcher: dict) -> None:\n    assert evaluate(expr,  matcher)\n", "def test_complex_expression_with_multiple_identifiers(expr: str, matcher: dict, expected: bool) -> None:\n    assert evaluate(expr, matcher.__getitem__) is expected\n", "def test_whitespace_around_parens(expr: str, expected: bool) -> None:\n    matcher = {\"true\": True, \"false\": False}.__getitem__\n    assert evaluate(expr, matcher) is expected\n", "    def test_nested_syntax_errors(expr: str, column: int, message: str) -> None:\n        with pytest.raises(ParseError) as excinfo:\n            evaluate(expr, lambda ident: True)\n        assert excinfo.value.column == column\n        assert excinfo.value.message == message\n"], "sample_1030": ["def test_intersection_polygon_line():\n    p = Polygon((0, 0), (1, 0), (1, 1))\n    l = Line((0, 0), (2, 2))\n    assert intersection(p, l) == [Point2D(1, 0), Point2D(1, 1)]\n    raises(TypeError, lambda: intersection(p, 3))\n    p = Polygon((0, 0), (1, 0), (1, 1), (0, 1))\n    l = Line((1, 0), (2, 2))\n    assert intersection(p, l) == [Point2D(1, 0)]\n    p = Polygon((0, 0), (1, 0), (1, 1), (0, 1))\n    l = Line((1, 0), (0, 1))\n    assert intersection(p, l) == [Point2D(1, 0), Point2D(0, 1)]\n", "def test_intersection():\n    from sympy.geometry import Line, Circle\n    c1 = Circle((0, 0), 1)\n    c2 = Circle((0, 2), 1)\n    assert intersection(c1, c2, pairwise=True) == [Point2D(0, 1)]\n    assert intersection(c1, c2) == [Point2D(0, 1)]\n    assert intersection(Line((0, 0), (1, 1)), c1) == [Point2D(0, 0), Point2D(1, 1)]\n    assert intersection(Line((0, 0), (1, 1)), c2) == [Point2D(0, 2), Point2D(2, 2)]\n\n\n", "compilation error", "def test_intersection_complex_cases():\n    c = Circle((0, 1), 1)\n    p = Point(0, 2)\n    assert intersection(c, p) == [Point2D(0, 2)]\n    assert intersection(c, c.center) == []\n    right = Ray((0, 0), (1, 0))\n    up = Ray((0, 0), (0, 1))\n    assert intersection(c, right, up) == [Point2D(0, 0)]\n\n    l1 = Line((0, 0), (1, 1))\n    l2 = Line((0, 1), (1, 0))\n    assert intersection(l1, l2).args == (Point2D(1/2, 1/2),)\n\n    p1 = Point(0, 0)\n    p2 = Point(1, 0)\n    p3 = Point(0, 1)\n    p4 = Point(1, 1)\n    poly1 = Polygon(p1, p2, p3, p4)\n    poly2 = Polygon(p1, p2, p3)\n    assert len(intersection(poly1, poly2)) == 3\n    assert intersection(poly1, poly2) == [p1, p2, p3]\n\n\n", "def test_intersection_complex():\n    c1 = Circle((0, 0), 2)\n    c2 = Circle((3, 0), 2)\n    raises(NotImplementedError, lambda: intersection(c1, c2, y=2))\n    assert intersection(c1, c2) == [\n        Point2D(1.5, 0), Point2D(1.5, sqrt(3)), Point2D(1.5, -sqrt(3))\n    ]\n"], "sample_276": ["    def test_field_name_without_description(self):\n        with self.assertRaises(AttributeError):\n            views.get_readable_field_data_type(fields.CharField())\n", "    def test_get_func_full_args(self):\n            pass\n        args = views.get_func_full_args(func_with_kwargs)\n        self.assertEqual(args, [('arg1',), ('arg2',), ('**kwargs',)])\n", "    def test_simplify_regex_groups(self):\n        tests = (\n            (r'^a/(?P<id>\\d+)$', '/a/<id>'),\n            (r'^a/(?P<id>\\d+)/(?P<name>.*)$', '/a/<id>/<name>'),\n            (r'^a/(?P<id>\\d+)/(?P<name>.*?)$', '/a/<id>/<name>'),\n        )\n        for pattern, output in tests:\n            with self.subTest(pattern=pattern):\n                self.assertEqual(simplify_regex(pattern), output)\n\n\n\n", "    def test_simplify_regex_with_named_groups(self):\n        tests = (\n            (r'^(?P<id>\\d+)/$', '/<id>'),\n            (r'^(?P<name>\\w+)/$', '/<name>'),\n            (r'^/(?P<month>\\w+)/(?P<day>\\d+)$', '/<month>/<day>'),\n        )\n        for pattern, output in tests:\n            with self.subTest(pattern=pattern):\n                self.assertEqual(simplify_regex(pattern), output)\n", "    def test_simplify_regex_with_groups(self):\n        tests = (\n            (r'^admin/docs/(?P<app_name>\\w+)/models/(?P<model_name>\\w+)/$', '/admin/docs/<app_name>/models/<model_name>/'),\n            (r'^(?P<category>\\w+)/(?P<slug>[\\w-]+)$', '/<category>/<slug>'),\n            (r'^(?P<user_id>\\d+)/edit$', '/<user_id>/edit'),\n        )\n        for pattern, output in tests:\n            with self.subTest(pattern=pattern):\n                self.assertEqual(simplify_regex(pattern), output)\n\n\n"], "sample_712": ["def test_one_hot_encoder_sparse_output():\n    X = np.array([[0, 1, 2], [3, 4, 5]])\n    enc = OneHotEncoder(sparse=True)\n    X_tr = enc.fit_transform(X)\n    assert isinstance(X_tr, sparse.csr_matrix)\n    X_tr2 = enc.fit_transform(X)\n    assert_array_equal(X_tr.toarray(), X_tr2.toarray())\n\n\n\n", "def test_one_hot_encoder_missing_data():\n    # test how missing data is handled in sparse and dense output\n    X = np.array([['a', None, 55], ['def', 1, 55], ['abc', None, None]])\n    enc = OneHotEncoder(handle_unknown='ignore')\n    Xtr = enc.fit_transform(X)\n    assert Xtr.shape == (3, 4)\n    assert_array_equal(Xtr.toarray(), np.array([[1., 0., 0., 0],\n                                                [0., 1., 0., 0],\n                                                [1., 0., 0., 0]]))\n\n    enc = OneHotEncoder(handle_unknown='error')\n    with pytest.raises(ValueError):\n        enc.fit_transform(X)\n\n\n\n", "def test_one_hot_encoder_handle_unknown_categorical():\n    X = np.array([[1, 2, 'apple'],\n                   [3, 4, 'banana'],\n                   [1, 2, 'grape']])\n    enc = OneHotEncoder(handle_unknown='ignore', categories='auto')\n    with pytest.raises(ValueError, match=\"Found unknown categories\"):\n        enc.fit(X)\n\n\n\n", "compilation error", "compilation error"], "sample_785": ["def test_repr_with_method_reference():\n    class MockSplitter:\n            self.a = a\n            self.b = b\n            self.c = c\n\n            return _build_repr(self)\n\n    mock_splitter = MockSplitter(5, 6)\n    mock_splitter_repr = repr(mock_splitter)\n    assert mock_splitter_repr\n    assert 'MockSplitter' in mock_splitter_repr\n    assert 'a=5' in mock_splitter_repr\n    assert 'b=6' in mock_splitter_repr\n", "def test_repeated_kfold_repr():\n    splitter = RepeatedKFold(n_splits=3, n_repeats=2)\n    assert_equal(repr(splitter), \"RepeatedKFold(n_splits=3, n_repeats=2)\")\n", "compilation error", "def test_get_n_splits():\n    assert_equal(ShuffleSplit(n_splits=3).get_n_splits(), 3)\n    assert_equal(ShuffleSplit(n_splits=None).get_n_splits(), None)\n    assert_equal(GroupShuffleSplit(n_splits=3).get_n_splits(), 3)\n    assert_equal(GroupShuffleSplit(n_splits=None).get_n_splits(), None)\n    assert_equal(StratifiedShuffleSplit(n_splits=3).get_n_splits(), 3)\n    assert_equal(StratifiedShuffleSplit(n_splits=None).get_n_splits(), None)\n    assert_equal(KFold(n_splits=3).get_n_splits(), 3)\n    assert_equal(KFold(n_splits=None).get_n_splits(), None)\n    assert_equal(GroupKFold(n_splits=3).get_n_splits(), 3)\n    assert_equal(GroupKFold(n_splits=None).get_n_splits(), None)\n    assert_equal(TimeSeriesSplit(n_splits=3).get_n_splits(), 3)\n    assert_equal(TimeSeriesSplit(n_splits=None).get_n_splits(), None)\n\n\n", "compilation error"], "sample_635": ["    def test_fail_docstring_min_length(self) -> None:\n        \"\"\"Example of a function that is less than 'docstring-min-length' config option\n\n        An error message is emitted.\n        \"\"\"\n        node = astroid.extract_node(\n            \"\"\"\n            '''function is too short'''\n            pass\n        \"\"\"\n        )\n        with self.assertAddsMessages(\n            MessageTest(msg_id=\"docstring-too-short\", node=node)\n        ):\n            self.checker.visit_functiondef(node)\n", "    def test_fail_docstring_min_length(self) -> None:\n        \"\"\"Example of a function that is less than 'docstring-min-length' config option\n\n        An error message is emitted.\n        \"\"\"\n        node = astroid.extract_node(\n            \"\"\"\n            '''function is too short'''\n            pass\n        \"\"\"\n        )\n        with self.assertAddsMessages(\n            MessageTest(msg_id=\"docstring-too-short\", node=node, args=(\"5\",)),\n        ):\n            self.checker.visit_functiondef(node)\n", "    def test_fail_docstring_min_length(self) -> None:\n        \"\"\"Example of a function that is less than 'docstring-min-length' config option\n\n        An error message is emitted.\n        \"\"\"\n        node = astroid.extract_node(\n            \"\"\"\n            '''function is too short'''\n            pass\n        \"\"\"\n        )\n        with self.assertAddsMessages(\n            MessageTest(msg_id=\"docstring-too-short\", node=node, args=(\"5\",)),\n        ):\n            self.checker.visit_functiondef(node)\n", "    def test_fail_docstring_min_length(self) -> None:\n        \"\"\"Example of a function that is less than 'docstring-min-length' config option\n\n        An error message is emitted.\n        \"\"\"\n        node = astroid.extract_node(\n            \"\"\"\n            '''function is too short'''\n            pass\n        \"\"\"\n        )\n        with self.assertAddsMessages(\n            MessageTest(msg_id=\"docstring-too-short\", node=node),\n        ):\n            self.checker.visit_functiondef(node)\n", "    def test_fail_docstring_min_length(self) -> None:\n        \"\"\"Example of a function that is less than 'docstring-min-length' config option\n\n        An error message is emitted.\n        \"\"\"\n        node = astroid.extract_node(\n            \"\"\"\n            '''short'''\n            pass\n        \"\"\"\n        )\n        with self.assertAddsMessages(\n            MessageTest(msg_id=\"docstring-too-short\", node=node)\n        ):\n            self.checker.visit_functiondef(node)\n"], "sample_1095": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_167": ["    def test_intcomma_with_large_number(self):\n        test_list = (\n            1234567890123456789,\n            12345678901234567890,\n            123456789012345678901234567890,\n        )\n        result_list = (\n            '1,234,567,890,123,456,789',\n            '12,345,678,901,234,567,890',\n            '12,345,678,901,234,567,890,123,456,789,0',\n        )\n        with translation.override('en'):\n            self.humanize_tester(test_list, result_list, 'intcomma')\n\n\n\n", "    def test_naturalday_with_naive_datetime(self):\n        test_list = [\n            datetime.datetime(2023, 3, 9, 10, 30),\n            datetime.datetime(2023, 3, 8, 10, 30),\n            datetime.datetime(2023, 3, 10, 10, 30),\n        ]\n        result_list = [\n            'today',\n            'yesterday',\n            'tomorrow',\n        ]\n\n        orig_humanize_datetime, humanize.datetime = humanize.datetime, MockDateTime\n        try:\n            with translation.override('en'):\n                self.humanize_tester(test_list, result_list, 'naturalday')\n        finally:\n            humanize.datetime = orig_humanize_datetime \n\n\n", "    def test_naturaltime_with_none_time(self):\n        dt = None\n        result = humanize.naturaltime(dt)\n        self.assertEqual(result, None)\n", "    def test_naturaltime_with_timezone(self):\n        from datetime import datetime, timezone\n\n        now_utc = datetime.now(timezone.utc)\n        now_pst = now_utc.astimezone(timezone(timedelta(hours=-8)))\n\n        test_list = [\n            now_utc,\n            now_pst,\n            now_utc - datetime.timedelta(seconds=1),\n            now_pst - datetime.timedelta(seconds=1),\n            now_utc + datetime.timedelta(seconds=1),\n            now_pst + datetime.timedelta(seconds=1),\n        ]\n        result_list = [\n            'now',\n            'now',\n            '1 second ago',\n            '1 second ago',\n            '1 second from now',\n            '1 second from now',\n        ]\n        orig_humanize_datetime, humanize.datetime = humanize.datetime, MockDateTime\n        try:\n            with translation.override('en'):\n                self.humanize_tester(test_list, result_list, 'naturaltime')\n        finally:\n            humanize.datetime = orig_humanize_datetime\n\n", "    def test_naturaltime_with_timezone(self):\n        now_utc = datetime.datetime.utcnow()\n        tz = datetime.timezone(datetime.timedelta(hours=3))\n        now_tz = now_utc.replace(tzinfo=tz)\n        \n        test_list = [\n            now_tz,\n            now_tz - datetime.timedelta(seconds=1),\n            now_tz + datetime.timedelta(seconds=1),\n        ]\n        result_list = [\n            'now',\n            'a second ago',\n            'a second from now',\n        ]\n\n        orig_humanize_datetime, humanize.datetime = humanize.datetime, MockDateTime\n        try:\n            with override_settings(USE_TZ=True):\n                self.humanize_tester(test_list, result_list, 'naturaltime')\n        finally:\n            humanize.datetime = orig_humanize_datetime\n"], "sample_981": ["compilation error", "compilation error", "compilation error", "    def test_inversion_vector():\n        p = Permutation([1, 5, 2, 0, 3, 6, 4])\n        assert p.inversion_vector() == [2, 4, 1, 3, 0]\n        p = Permutation([0, 1, 2, 3])\n        assert p.inversion_vector() == [0, 0, 0, 0]\n        p = Permutation([0, 3, 1, 2])\n        assert p.inversion_vector() == [1, 2, 0]\n        p = Permutation([3, 2, 1, 0])\n        assert p.inversion_vector() == [3, 2, 1]\n\n\n\n", "def test_cycle_equality():\n    assert Cycle(1, 2) == Cycle(1, 2)\n    assert Cycle(1, 2) !=  Cycle(2, 1)\n    assert Cycle(1, 2) != Cycle(1, 3)\n    assert Cycle(1, 2, 3) == Cycle(1, 2, 3)\n    assert Cycle(1, 2, 3) != Cycle(1, 2, 4)\n    assert Cycle(1, 2, 3) != Cycle(2, 1, 3)\n    assert Cycle(1, 2, 3) != Cycle(1, 3, 2)\n\n    assert Cycle(1, 2, 3)(2, 4) == Cycle(1, 4, 2, 3)\n    assert Cycle(1, 2, 3)(2, 4) != Cycle(1, 4, 3, 2)\n    assert Cycle(1, 2, 3).__eq__(Cycle(1, 2, 3))\n    assert not Cycle(1, 2, 3).__eq__(Cycle(1, 2))\n    assert Cycle(1, 2, 3).__hash__() == Cycle(1, 2, 3).__hash__()\n    assert Cycle(1, 2, 3).__hash__() != Cycle(1, 2, 4).__hash__()\n"], "sample_222": ["    def test_spooled_temp_file_size_limit(self):\n        with tempfile.SpooledTemporaryFile(max_size=5) as temp:\n            temp.write(b\"a\" * 5)\n            django_file = File(temp, name=\"something.txt\")\n            self.assertEqual(django_file.size, 5)\n            with self.assertRaises(IOError):\n                temp.write(b\"a\" * 5)  \n", "    def test_sized_spooled_temp_write_multiple_chunks(self):\n        with tempfile.SpooledTemporaryFile(max_size=4) as temp:\n            temp.write(b\"1234\")\n            temp.write(b\"5678\")\n            django_file = File(temp, name=\"something.txt\")\n            self.assertEqual(django_file.size, 8)\n", "    def test_content_file_bytes_encoding(self):\n        content = b\"This is a byte string\"\n        content_file = ContentFile(content, name='test.txt')\n        self.assertEqual(content_file.read().decode('utf-8'), 'This is a byte string')\n        with content_file.open() as f:\n            self.assertEqual(f.read().decode('utf-8'), 'This is a byte string')\n", "    def test_spooled_temp_file_path(self):\n        with tempfile.SpooledTemporaryFile() as temp:\n            self.assertIsNone(temp.name)\n", "    def test_spooled_temp_with_read_after_close(self):\n        with tempfile.SpooledTemporaryFile() as temp:\n            temp.write(b\"foo bar baz quux\\n\")\n            temp.close()\n            django_file = File(temp, name=\"something.txt\")\n            self.assertEqual(django_file.read(), b\"foo bar baz quux\\n\")\n\n\n"], "sample_1078": ["compilation error", "def test_Indexed_subs_with_Tuple():\n    i, j = symbols('i j', integer=True)\n    A = IndexedBase('A')\n    assert A[[i, i]].subs(i, j) == A[[j, j]]\n    assert A[[i, j]].subs(i, j) == A[[j, j]]\n\n\n\n\n", "def test_Indexed_subs_with_repeated_symbols():\n    i, j, k = symbols(\"i,j,k\")\n    A = IndexedBase(\"A\")\n    B = IndexedBase(\"B\")\n\n    assert (A[i] + B[i]).subs(A[i], B[i]) == 2*B[i]\n    assert (A[i] + B[i]).subs({A[i]: B[i], B[i]: A[i]}) == A[i] + B[i]\n    assert (A[i] + B[j]).subs({A[i]: B[i], B[j]: A[j]}) == B[i] + A[j]\n    assert (A[i] + B[i] + C[i]).subs({A[i]: B[i], C[i]: D[i]}) == B[i] + D[i]\n\n\n", "def test_issue_14962():\n    x, y = symbols(\"x,y\")\n    a = IndexedBase(\"a\", shape=(5, 5))\n    i, j = symbols(\"i,j\", integer=True)\n    assert a[i, j].subs(a[i, j], x).diff(x) == 1\n    assert a[i, j].subs(a[i, j], x).diff(y) == 0\n\n\n", "def test_Indexed_expand():\n    i, j, k = symbols('i j k', integer=True)\n    A = IndexedBase(\"A\")\n    a = symbols('a')\n    b = symbols('b')\n    expr1 = A[i] * A[j]\n    expr2 = A[i] * A[j] + A[i] * A[k]\n    expr3 = A[i]**2 + A[i] * A[j]**2\n    expr4 = (A[i] * A[j])**2\n\n    assert expr1.expand() == A[i] * A[j]\n    assert expr2.expand() == A[i] * A[j] + A[i] * A[k]\n    assert expr3.expand() == A[i]**2 + A[i] * A[j]**2\n    assert expr4.expand() == A[i]**2 * A[j]**2\n\n\n"], "sample_654": ["def test_fixture_params_and_function_scope(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture(params=['a', 'b'])\n            return request.param\n\n            assert param_fixture == 'a'\n\n            assert param_fixture == 'b'\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.assert_outcomes(passed=2)\n", "    def test_fixture_function_name(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.fixture(name='alias')\n                return 'fixture_value'\n\n                assert alias == 'fixture_value'\n        \"\"\"\n        )\n        result = testdir.runpytest()\n        result.stdout.fnmatch_lines([\"*fixture_value*\"])\n\n\n", "    def test_fixture_param_shadowing_indirect_ordering(testdir):\n        testdir.makepyfile(\n            \"\"\"\n        import pytest\n\n        @pytest.fixture(params=['a', 'b'])\n            return request.param\n\n        @pytest.fixture\n            return argroot\n\n        @pytest.mark.parametrize(\"arg\", [\"c\", \"d\"], indirect=True)\n            assert arg in (\"c\", \"d\")\n", "def test_fixture_param_shadowing_in_conftest(testdir):\n    testdir.makeconftest(\n        \"\"\"\n        @pytest.fixture(params=['a', 'b'])\n            return request.param\n\n        @pytest.fixture\n            return argroot\n\n        \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n            assert isinstance(arg, str)\n\n        @pytest.mark.parametrize(\"arg2\", [1], indirect=True)\n            assert arg2 == 2\n\n    \"\"\"\n    )\n    # Only one test should have run\n    result = testdir.runpytest(\"-v\")\n    result.assert_outcomes(passed=2)\n    result.stdout.fnmatch_lines([\"*::test_normal_fixture[[]a[]]*\"])\n    result.stdout.fnmatch_lines([\"*::test_normal_fixture[[]b[]]*\"])\n    result.stdout.fnmatch_lines([\"*::test_indirect[[]1[]]*\"])\n", "    def test_fixture_param_shadowing_indirect_scope(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.fixture(scope='module', params=['a', 'b'])\n                return request.param\n\n            @pytest.fixture\n                return argroot\n            @pytest.fixture\n                return 2 * 1\n\n            @pytest.mark.parametrize(\"arg\", [1], indirect=True)\n                assert arg == 2\n        \"\"\"\n        )\n        result = testdir.runpytest()\n        result.stdout.fnmatch_lines([\"*test_indirect[[]1[]]*\"])\n"], "sample_45": ["    def test_trunc_timezone_aware_only(self):\n        start_datetime = datetime(2015, 6, 15, 14, 30, 50, 321)\n        end_datetime = datetime(2016, 6, 15, 14, 10, 50, 123)\n        start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n        end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n        self.create_model(start_datetime, end_datetime)\n\n        melb = pytz.timezone('Australia/Melbourne')\n\n        with self.assertRaises(TypeError):\n            DTModel.objects.annotate(\n                melb_year=TruncYear('start_datetime', tzinfo=melb),\n                truncated_naive=TruncDay('start_datetime')\n            ).get()\n\n        with self.assertRaises(TypeError):\n            DTModel.objects.annotate(\n                melb_year=TruncYear('start_datetime', tzinfo=melb)\n            ).get()\n\n\n\n", "    def test_trunc_func_with_timezone_dst_transition(self):\n        # Test truncating datetime with timezone across DST transition\n        start_datetime = timezone.make_aware(datetime(2017, 3, 12, 2, 0, 0), pytz.timezone('America/Toronto'))\n        end_datetime = timezone.make_aware(datetime(2017, 3, 13, 2, 0, 0), pytz.timezone('America/Toronto'))\n        self.create_model(start_datetime, end_datetime)\n\n        melb = pytz.timezone('Australia/Melbourne')\n\n        self.assertQuerysetEqual(\n            DTModel.objects.annotate(\n                truncated=TruncHour('start_datetime', tzinfo=melb)\n            ).order_by('start_datetime'),\n            [\n                (start_datetime, truncate_to(start_datetime.astimezone(melb), 'hour', melb)),\n                (end_datetime, truncate_to(end_datetime.astimezone(melb), 'hour', melb))\n            ],\n            lambda m: (m.start_datetime, m.truncated)\n        )\n", "compilation error", "    def test_trunc_ambiguous_times_with_timezone(self):\n        sao = pytz.timezone('America/Sao_Paulo')\n        utc = pytz.timezone('UTC')\n        start_datetime = utc.localize(datetime(2016, 10, 16, 13))\n        end_datetime = utc.localize(datetime(2016, 2, 21, 1))\n        self.create_model(start_datetime, end_datetime)\n        with timezone.override(sao):\n            with self.assertRaisesMessage(pytz.AmbiguousTimeError, '2016-02-20 23:00:00'):\n                model = DTModel.objects.annotate(truncated=TruncHour('end_datetime')).get()\n            with self.assertRaisesMessage(pytz.AmbiguousTimeError, '2016-10-16 13:00:00'):\n                model = DTModel.objects.annotate(truncated=TruncHour('start_datetime')).get()\n            model = DTModel.objects.annotate(\n                truncated_start=TruncHour('start_datetime', is_dst=False),\n                truncated_end=TruncHour('end_datetime', is_dst=False),\n            ).get()\n            self.assertEqual(model.truncated_start.dst(), timedelta(0))\n            self.assertEqual(model.truncated_end.dst(), timedelta(0))\n            model = DTModel.objects.annotate(\n                truncated_start=TruncHour('start_datetime', is_dst=True),\n                truncated_end=TruncHour('end_datetime', is_dst=True),\n            ).get()\n            self.assertEqual(model.truncated_start.dst(), timedelta(0, 3600))\n            self.assertEqual(model.truncated_end.dst(), timedelta(0, 3600))\n", "    def test_trunc_func_with_timezone_aware_naive_datetime(self):\n        start_datetime = datetime(2015, 6, 15, 14, 30, 50, 321)\n        end_datetime = datetime(2016, 6, 15, 14, 10, 50, 123)\n        start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n        end_datetime = end_datetime.astimezone(timezone.utc)\n\n        self.create_model(start_datetime, end_datetime)\n        self.create_model(end_datetime, start_datetime)\n\n        melb = pytz.timezone('Australia/Melbourne')\n\n        melb_start_datetime = start_datetime.astimezone(melb)\n        with self.assertRaises(pytz.exceptions.NonExistentTimeError):\n            DTModel.objects.annotate(\n                truncated=Trunc(\n                    'start_datetime',\n                    'day',\n                    output_field=DateTimeField(),\n                    tzinfo=melb,\n                )\n            ).get()\n        with self.assertRaises(ValueError):\n            DTModel.objects.annotate(\n                truncated=Trunc('start_datetime', 'day', output_field=DateField(), tzinfo=melb)\n            ).get()\n"], "sample_393": ["    def test_po_changed_with_updated_strings(self):\n        \"\"\"\n        PO files are updated when existing strings are modified.\n        \"\"\"\n        Path(\"templates/test.html.py\").write_text(\n            \"\"\"", "    def test_po_updated_with_changes_to_existing_strings(self):\n        \"\"\"PO files are updated when existing strings are changed.\"\"\"\n        with open(self.PO_FILE, 'a') as fp:\n            fp.write(\n                f'msgid \"This is a translatable string.\" \\nmsgstr \"New translation for an existing string.\" \\n'\n            )\n\n        _, po_contents = self._run_makemessages()\n        self.assertNotEqual(po_contents, self.original_po_contents)\n        self.assertMsgId(\n            \"This is a translatable string.\", po_contents,\n            msgstr=\"New translation for an existing string.\"\n        )\n", "    def test_po_updated_with_existing_strings_with_new_translations(self):\n        \"\"\"PO files are updated when existing strings get new translations. \"\"\"\n        original_po_contents = self.original_po_contents\n        Path(\"templates/app_with_locale/test.html.tmp\").rename(\"templates/app_with_locale/test.html\")\n        _, po_contents = self._run_makemessages()\n        self.assertNotEqual(po_contents, original_po_contents)\n        self.assertMsgId(\n            \"This is a string that already exists in the .po file.\",\n            po_contents,\n        )\n        self.assertMsgId(\n            \"This is a string that already exists in the .po file.\",\n            po_contents,\n            msgid_index=1,\n        )\n\n\n\n", "    def test_po_changed_with_updated_strings(self):\n        \"\"\"\n        PO files are updated when existing strings change.\n        \"\"\"\n        with open(self.PO_FILE, \"a\", encoding=\"utf-8\") as fp:\n            fp.write(\n                \"msgid \\\"This is an old string that was previously translated\\n\"\n                \"msgstr \\\"Dieses ist ein alter String, der zuvor \u00fcbersetzt wurde\\\"\\n\"\n            )\n        _, po_contents = self._run_makemessages()\n\n        # Assert that the old string is still present and untranslated\n        self.assertIn(\n            'msgid \"This is an old string that was previously translated\"',\n            po_contents,\n        )\n        self.assertIn(\n            'msgstr \"Dieses ist ein alter String, der zuvor \u00fcbersetzt wurde\"',\n            po_contents,\n        )\n\n\n\n        \n", "    def test_po_changed_with_updated_strings(self):\n        \"\"\"PO files are updated when existing strings are modified.\"\"\"\n        with open(self.PO_FILE, \"w\") as fp:\n            fp.write(\n                f\"msgid ''\\nmsgstr 'This is a translatable string.'\\n\"\n            )  \n        _, po_contents = self._run_makemessages()\n        self.assertNotEqual(po_contents, self.original_po_contents)\n        self.assertMsgId(\n            \"This is a translatable string.\", po_contents, use_quotes=False\n        )\n"], "sample_1126": ["def test_complex_power():\n    x = symbols('x', complex=True)\n    assert Dagger(x**2) == conjugate(x)**2\n    assert Dagger(x**(-1)) == conjugate(x)**(-1)\n", "def test_dagger_bra_ket():\n    from sympy.physics.quantum.state import Bra, Ket\n\n    a = symbols('a')\n    b = symbols('b')\n\n    assert Dagger(Bra(a)) == Ket(a)\n    assert Dagger(Ket(b)) == Bra(b)\n", "def test_brackets():\n    a, b = symbols('a b')\n    assert Dagger(Ket(a)*Bra(b)) == Bra(b)*Ket(a)\n    assert Dagger(OuterProduct(Ket(a), Bra(b))) == OuterProduct(Bra(b), Ket(a))\n", "def test_bracket_notation():\n    from sympy.physics.quantum.state import Ket, Bra\n\n    ket = Ket('psi')\n    bra = Bra('phi')\n    assert Dagger(ket) == bra\n    assert Dagger(bra) == ket\n", "def test_complex_numbers():\n    z = complex(1, 2)\n    assert Dagger(z) == complex(1, -2)\n"], "sample_40": ["def test_flux_density_equivalencies():\n    # Test various flux density equivalencies with common unit systems\n    # including: Jy, erg/cm^2/s, ergs/s/cm^2, photons/cm^2/s, \n    # and mJy, MJy\n\n    for unit_system in ['Jy', 'erg/cm^2/s', 'ergs/s/cm^2', 'photons/cm^2/s', 'mJy', 'MJy']:\n        flux_density = 1 * u.Unit(unit_system)\n        # Convert to other common flux density units\n        for other_unit_system in ['Jy', 'erg/cm^2/s', 'ergs/s/cm^2', 'photons/cm^2/s', 'mJy', 'MJy']:\n            if unit_system != other_unit_system:\n                result = flux_density.to(other_unit_system)\n                assert result.unit.is_equivalent(u.Unit(other_unit_system))\n                assert abs(result.value - flux_density.value) < 1e-6\n    \n", "def test_equivalencies_in_quantities():\n    from astropy.units import m, s, Hz, sr, Quantity\n\n    # Test equivalencies defined in a Quantity's units\n    c = 299792458 * m/s\n    quantity_with_equivalency = Quantity(1, \"cycle\", unit=Quantity(0.5 * c))\n\n    assert quantity_with_equivalency.value == 0.5 * c\n\n    # Test equivalencies defined in a Quantity's unit\n    quantity_with_unit_equivalency = Quantity(1, \"cycle\", units=Quantity(1 * Hz, equivalencies=u.doppler_optical(500 * s)))\n    assert quantity_with_unit_equivalency.value == 1 * Hz\n\n\n\n", "compilation error", "compilation error", "compilation error"], "sample_872": ["def test_label_ranking_loss_categorical_integer_labels():\n    # Check that label_ranking_loss works with categorical integer labels\n    y_true = np.array([0, 1, 1, 2])\n    y_score = np.array([[0.1, 0.4, 0.5], [0.2, 0.3, 0.6], [0.3, 0.5, 0.2], [0.4, 0.1, 0.5]])\n    loss = label_ranking_loss(y_true, y_score)\n    assert loss != np.inf\n", "def test_label_ranking_average_precision_score_with_custom_labels(\n    y_true, y_score, labels", "def test_label_ranking_loss_ties_handling():\n    # Tie handling\n    assert_almost_equal(label_ranking_loss([[1, 0]], [[0.5, 0.5]]), 1)\n    assert_almost_equal(label_ranking_loss([[0, 1]], [[0.5, 0.5]]), 1)\n    assert_almost_equal(label_ranking_loss([[0, 0, 1]], [[0.25, 0.5, 0.5]]), 1 / 2)\n    assert_almost_equal(label_ranking_loss([[0, 1, 0]], [[0.25, 0.5, 0.5]]), 1 / 2)\n    assert_almost_equal(label_ranking_loss([[0, 1, 1]], [[0.25, 0.5, 0.5]]), 0)\n    assert_almost_equal(label_ranking_loss([[1, 0, 0]], [[0.25, 0.5, 0.5]]), 1)\n    assert_almost_equal(label_ranking_loss([[1, 0, 1]], [[0.25, 0.5, 0.5]]), 1)\n    assert_almost_equal(label_ranking_loss([[1, 1, 0]], [[0.25, 0.5, 0.5]]), 1)\n", "def test_top_k_accuracy_score_multilabel(y_true, y_score, k):\n    \"\"\"Test top_k_accuracy_score for multilabel classification.\"\"\"\n    for i in range(len(y_true)):\n        score = top_k_accuracy_score(y_true[i], y_score[i], k=k)\n        assert (\n            score == 1\n            if k >= len(np.unique(y_true[i]))\n            else score >= 0\n        )\n    \n", "def test_label_ranking_avg_precision_score_with_ties():\n    y_true = np.array([[0, 1, 0], [1, 1, 0], [0, 0, 1]])\n    y_score = np.array([[0.1, 0.2, 0.3], [0.4, 0.4, 0.2], [0.3, 0.1, 0.5]])\n    # Introduce ties in y_score\n    y_score[1, 0] = 0.4\n    score = label_ranking_average_precision_score(y_true, y_score)\n    assert score == pytest.approx(0.5)\n\n\n"], "sample_348": ["    def test_actions_not_function(self):\n        class BandAdmin(ModelAdmin):\n            actions = ('not_a_function',)\n\n        self.assertIsInvalid(\n            BandAdmin, Band,\n            \"The value of 'actions[0]' must be a function. Found 'not_a_function' which is not a callable.\",\n            id='admin.E131',\n        )\n", "    def test_actions_invalid_type(self):\n        class BandAdmin(ModelAdmin):\n            actions = 1\n\n        self.assertIsInvalid(\n            BandAdmin, Band,\n            \"The value of 'actions' must be a list or tuple.\",\n            id='admin.E125',\n        )\n", "    def test_actions_can_contain_actions_with_permissions(self):\n        @admin.action(permissions=['custom'])\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = (\n                custom_permission_action,\n                admin.actions.delete\n            )\n\n        self.assertIsValid(BandAdmin, Band)\n", "    def test_actions_with_custom_permissions(self):\n        @admin.action(permissions=['custom'])\n            pass\n\n        class BandAdmin(ModelAdmin):\n            has_custom_permission = property(lambda self: True)\n\n            actions = (custom_action,)\n\n        self.assertIsValid(BandAdmin, Band)\n\n\n\n", "    def test_invalid_action_type(self):\n        class FakeFunction:\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = (FakeFunction,)\n\n        self.assertIsInvalid(\n            BandAdmin, Band,\n            \"'modeladmin.FakeFunction' is not a valid action function.\",\n            id='admin.E131',\n        )\n"], "sample_33": ["compilation error", "def test_ordered_descriptor_container():\n    class TypedAttribute(metaclass=abc.ABCMeta):\n        _class_attribute_ = '_typed_attributes_'\n\n    class Point2D(metaclass=OrderedDescriptorContainer):\n        x = TypedAttribute((float, int))\n        y = TypedAttribute((float, int))\n\n    typed_attrs = Point2D._typed_attributes_\n    assert len(typed_attrs) == 2\n    assert 'x' in typed_attrs\n    assert 'y' in typed_attrs\n\n    class Point3D(Point2D):\n        z = TypedAttribute((float, int))\n        _inherit_descriptors_ = (TypedAttribute,)\n\n    typed_attrs = Point3D._typed_attributes_\n    assert len(typed_attrs) == 3\n\n\n\n", "compilation error", "def test_OrderedDescriptorContainerBase():\n    class TestClass(metaclass=misc.OrderedDescriptorContainer):\n        x = 1\n        y = 2\n\n    assert hasattr(TestClass, '_OrderedDescriptorContainer__descriptors')\n    assert isinstance(getattr(TestClass, '_OrderedDescriptorContainer__descriptors'), dict)\n    assert len(getattr(TestClass, '_OrderedDescriptorContainer__descriptors')) == 2\n\n\n\n\n", "def test_OrderedDescriptorContainer():\n    class TypedAttribute(OrderedDescriptor):\n        _class_attribute_ = 'typed_attributes'\n\n            self.type = type\n\n    class Point2D(metaclass=OrderedDescriptorContainer):\n        x = TypedAttribute((int, float))\n        y = TypedAttribute((int, float))\n\n    p1 = Point2D(1, 2.0)\n    assert isinstance(p1.typed_attributes, dict)\n    assert len(p1.typed_attributes) == 2\n    assert 'x' in p1.typed_attributes\n    assert isinstance(p1.typed_attributes['x'], TypedAttribute)\n    assert p1.typed_attributes['x'].type == (int, float)\n\n    class Point3D(Point2D):\n        z = TypedAttribute((int, float))\n\n    p2 = Point3D(1, 2.0, 3)\n    assert len(p2.typed_attributes) == 3\n    assert 'z' in p2.typed_attributes\n    assert isinstance(p2.typed_attributes['z'], TypedAttribute)\n    assert p2.typed_attributes['z'].type == (int, float)\n\n    with pytest.raises(ValueError):\n        Point2D(1, 'a')\n"], "sample_639": ["def test_base_checker_message_definition() -> None:\n    basic = OtherBasicChecker()\n    msg_definition = basic.get_message_definition(\"W0001\")\n    assert msg_definition.msgid == \"W0001\"\n    assert msg_definition.message == \"Basic checker has an example.\"\n    assert msg_definition.description == \"Used nowhere and serves no purpose.\"\n    assert msg_definition.symbol == \"basic-checker-example\"\n    assert msg_definition.scope == \"LINE\"\n", "def test_message_definition_creation() -> None:\n    basic = OtherBasicChecker()\n    msgid = \"W0001\"\n    msg_tuple = (\n        \"Basic checker has an example.\",\n        \"basic-checker-example\",\n        \"Used nowhere and serves no purpose.\",\n    )\n    message_definition = basic.create_message_definition_from_tuple(msgid, msg_tuple)\n    assert message_definition.msgid == msgid\n    assert message_definition.message == msg_tuple[0]\n    assert message_definition.symbol == msg_tuple[1]\n    assert message_definition.description == msg_tuple[2]\n\n\n\n", "    def test_base_checker_add_message() -> None:\n        checker = OtherBasicChecker()\n        checker.add_message(\"W0001\", line=10, node=None, args=[\"foo\", \"bar\"])\n        assert len(checker.linter.messages) == 1\n        assert checker.linter.messages[0].msgid == \"W0001\"\n        assert checker.linter.messages[0].line == 10\n\n\n\n\n\n\n", "def test_base_checker_add_message() -> None:\n    checker = OtherBasicChecker()\n    checker.add_message(\"W0001\", line=10, node=None, args=(\"test\",), confidence=Confidence.CRITICAL)\n    assert len(checker.linter.messages) == 1\n", "    def test_base_checker_add_message(\n        self,"], "sample_18": ["    def test_array_ops_info(self):\n        q_sum = np.sum(self.q, axis=0)  \n        assert_info_equal(q_sum, self.q)\n        q_mean = np.mean(self.q, axis=0)\n        assert_info_equal(q_mean, self.q)\n        q_std = np.std(self.q, axis=0)\n        assert_info_equal(q_std, self.q)        \n        q_dot = np.dot(self.q, self.q)\n        assert_no_info(q_dot)\n\n\n\n", "    def test_reshape(self):\n        q = self.q.reshape(1, 2, 2)\n        assert_info_equal(q, self.q)\n\n\n\n", "    def test_array_function(self):\n        q = np.sqrt(self.q[\"p\"])\n        assert_no_info(q)\n\n\n\n", "    def test_array_operations(self):\n        q_array = np.array(self.q)\n        assert_info_equal(q_array, self.q)\n        q_array_copy = copy.deepcopy(q_array)\n        assert_info_equal(q_array_copy, self.q)\n        q_array_copy.info.name = \"q_array_copy\"\n        assert_info_equal(q_array_copy, self.q, ignore={\"name\"})\n", "    def test_view(self):\n        q_view = self.q[0].view()\n        assert_info_equal(q_view, self.q)\n\n\n"], "sample_50": ["    def test_environment_variable_precedence(self):\n        \"\"\"PGPASSWORD env var takes precedence over connection parameters.\"\"\"\n        with mock.patch('subprocess.run') as mock_run:\n            DatabaseClient.runshell_db({\n                'database': 'dbname',\n                'user': 'someuser',\n                'password': 'somepassword',\n                'host': 'somehost',\n                'port': '444',\n            })\n        env = mock_run.call_args[1]['env']\n        self.assertEqual(env['PGPASSWORD'], 'somepassword')\n", "    def test_empty_dbinfo(self):\n        with self.assertRaises(ValueError) as context:\n            DatabaseClient.runshell_db({})\n        self.assertIn(\"Missing 'database'\", str(context.exception))\n", "    def test_empty_dbinfo(self):\n        self.assertEqual(\n            self._run_it({}),\n            (['psql'], None)\n        )\n\n", "    def test_empty_dbinfo(self):\n        self.assertEqual(\n            self._run_it({}), (\n                ['psql'],\n                None,\n            )\n        )\n", "    def test_empty_dbinfo(self):\n        with self.assertRaises(ValueError):\n            DatabaseClient.runshell_db({})\n"], "sample_634": ["    def test_expand_modules_with_missing_modules(\n        self, files_or_modules, expected_errors", "    def test_handle_non_existing_file_or_directory(self, files_or_modules):\n        ignore_list, ignore_list_re = [], []\n        modules, errors = expand_modules(\n            files_or_modules,\n            ignore_list,\n            ignore_list_re,\n            get_global_option(self, \"ignore-paths\"),\n        )\n        assert not modules\n        assert errors\n\n\n\n", "    def test_expand_modules_with_relative_paths(self, files_or_modules, expected):\n        ignore_list, ignore_list_re = [], []\n        modules, errors = expand_modules(\n            files_or_modules,\n            ignore_list,\n            ignore_list_re,\n            get_global_option(self, \"ignore-paths\"),\n        )\n        modules.sort(key=lambda d: d[\"name\"])\n        assert modules == expected\n        assert not errors\n", "    def test_expand_modules_with_subpackage(self, files_or_modules, expected):\n        \"\"\"Test expand_modules with a subpackage\"\"\"\n        ignore_list, ignore_list_re = [], []\n        modules, errors = expand_modules(\n            files_or_modules,\n            ignore_list,\n            ignore_list_re,\n            get_global_option(self, \"ignore-paths\"),\n        )\n        modules.sort(key=lambda d: d[\"name\"])\n        assert modules == expected\n        assert not errors\n", "    def test_expand_modules_with_non_existing_files(self, files_or_modules, expected):\n        \"\"\"Test expand_modules with non-existing files.\"\"\"\n        ignore_list, ignore_list_re = [], []\n        modules, errors = expand_modules(\n            files_or_modules,\n            ignore_list,\n            ignore_list_re,\n            get_global_option(self, \"ignore-paths\"),\n        )\n        modules.sort(key=lambda d: d[\"name\"])\n        assert modules == expected\n        assert not errors\n"], "sample_823": ["def test_pairwise_distances_with_precomputed(metric):\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((100, 10))\n    D = pairwise_distances(X, metric=metric)\n    D_precomputed = np.array(D)\n    dist = pairwise_distances(X, precomputed=D_precomputed, metric=metric)\n    assert_allclose(dist, D)\n", "def test_check_invalid_input_types():\n    # Ensures that an error is raised for unsuitable input types.\n    XA = np.array([1, 2, 3])\n    XB = np.array([4, 5, 6])\n    with pytest.raises(TypeError):\n        check_pairwise_arrays(XA, XB, 'abc')\n    with pytest.raises(TypeError):\n        check_pairwise_arrays(XA, XB, [1, 2, 3])\n    with pytest.raises(TypeError):\n        check_pairwise_arrays(XA, XB, None)\n\n    # Ensure that a single float is handled correctly.\n    XA = 1.0\n    XB = 2.0\n    XA_checked, XB_checked = check_pairwise_arrays(XA, XB)\n    assert_equal(XA_checked, XA)\n    assert_equal(XB_checked, XB)\n\n\n\n", "def test_pairwise_distances_invalid_metric_string():\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((10, 10))\n    with pytest.raises(ValueError, match=r\"Invalid metric: 'foobar'\"):\n        pairwise_distances(X, X, metric=\"foobar\")\n\n\n\n", "def test_invalid_input_types():\n    # Ensure an error is raised for invalid input types.\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((5, 4))\n\n    with pytest.raises(TypeError):\n        pairwise_distances(X, [1, 2, 3], metric=\"euclidean\")\n\n    with pytest.raises(TypeError):\n        pairwise_distances(str, X, metric=\"euclidean\")\n    with pytest.raises(TypeError):\n        pairwise_distances(X, Y=X, metric=\"euclidean\",\n                          n_jobs= \"hello\")\n\n", "def test_pairwise_distances_array_like_inputs():\n    # Ensure that pairwise_distances handles array-like inputs correctly\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((10, 5))\n    Y = list(zip(*X.transpose().tolist()))  # transpose and convert to list of list\n\n    with config_context(working_memory=1):\n        dist1 = pairwise_distances(X, Y, n_jobs=1)\n        dist2 = pairwise_distances(X, np.array(Y), n_jobs=1)\n        assert_allclose(dist1, dist2)\n"], "sample_586": ["    def test_concat_errors_dataArray(self):\n        data = create_test_data()\n        split_data = [data.isel(dim1=slice(3)), data.isel(dim1=slice(3, None))]\n\n        with raises_regex(ValueError, \"must supply at least one\"):\n            concat([], \"dim1\")\n\n        with raises_regex(ValueError, \"Cannot specify both .*='different'\"):\n            concat(\n                [data, data], dim=\"concat_dim\", data_vars=\"different\", compat=\"override\"\n            )\n\n        with raises_regex(ValueError, \"must supply at least one\"):\n            concat([], \"dim1\")\n\n        with raises_regex(ValueError, \"are not coordinates\"):\n            concat([Dataset({\"x\": 0}), Dataset({\"x\": [1]})], \"z\")\n\n        with raises_regex(ValueError, \"global attributes not\"):\n            data0, data1 = deepcopy(split_data)\n            data1.attrs[\"foo\"] = \"bar\"\n            concat([data0, data1], \"dim1\", compat=\"identical\")\n        assert_identical(data, concat([data0, data1], \"dim1\", compat=\"equals\"))\n\n        with raises_regex(ValueError, \"present in some datasets but not others\"):\n            data0, data1 = deepcopy(split_data)\n            data1[\"foo\"] = (\"bar\", np.random.randn(10))\n            concat([data0, data1], \"dim1\")\n\n        with raises_regex(ValueError, \"compat.* invalid\"):\n            concat(split_data, \"dim1\", compat=\"foobar\")\n\n\n\n", "    def test_concat_mixed_coords(self):\n        ds1 = Dataset(\n            {\"foo\": ((\"x\", \"y\"), np.random.random((2, 3))), \"bar\": (\"x\", [0, 1])},\n            {\"x\": [0, 1], \"y\": [0, 1, 2]},\n        )\n        ds2 = Dataset(\n            {\"foo\": ((\"x\", \"y\"), np.random.random((2, 3))), \"bar\": (\"x\", [2, 3])},\n            {\"x\": [0, 1], \"y\": [0, 1, 2]},\n        )\n        expected = Dataset(\n            {\"foo\": ((\"x\", \"y\"), np.concatenate((ds1[\"foo\"], ds2[\"foo\"]), axis=0)),\n             \"bar\": ((\"x\"), [0, 1, 2, 3])},\n            {\"x\": [0, 1], \"y\": [0, 1, 2]},\n        )\n        actual = concat([ds1, ds2], dim=\"x\")\n        assert_identical(actual, expected)\n", "    def test_concat_with_different_sizes(self):\n        data0 = Dataset({\"x\": ((\"i\",), [0, 1, 2]), \"y\": ((\"i\",), [3, 4, 5])})\n        data1 = Dataset({\"x\": ((\"i\",), [1, 2, 3]), \"y\": ((\"i\",), [6, 7, 8])})\n        expected = Dataset({\"x\": ((\"i\",), [0, 1, 2, 3]), \"y\": ((\"i\",), [3, 4, 5, 6])})\n        actual = concat((data0, data1), dim=\"i\")\n        assert_identical(expected, actual)\n", "    def test_concat_reordering(self):\n        data = Dataset({\"foo\": (\"x\", [1, 2, 3]), \"bar\": (\"x\", [4, 5, 6])})\n        order = ['bar', 'foo']\n        expected = Dataset({\"bar\": (\"x\", [4, 5, 6]), \"foo\": (\"x\", [1, 2, 3])})\n        actual = concat([data, data], dim=\"x\", data_vars=order)\n        assert_identical(expected, actual)\n\n\n\n", "    def test_concat_compat_errors(self):\n        ds1 = Dataset({\"foo\": (\"x\", [1, 2])}, {\"x\": [0, 1]})\n        ds2 = Dataset({\"foo\": (\"x\", [3, 4])}, {\"x\": [0, 1]})\n\n        with raises_regex(ValueError, \"'unknown' is not present\"):\n            concat([ds1, ds2], dim=\"x\", data_vars=\"minimal\", compat=\"identical\")\n\n        with raises_regex(ValueError, \"'y' is not present\"):\n            concat([ds1, ds2], dim=\"y\", data_vars=\"minimal\", compat=\"identical\")\n\n        with raises_regex(ValueError, \"'x' is not present\"):\n            concat([ds1, ds2], dim=\"x\", compat=\"equals\")\n\n\n\n"], "sample_1104": ["def test_issue_16911():\n    from sympy.tensor import tensor\n    a, b, c, d = symbols('a b c d')\n    T = tensor('T', (a, b, c, d))\n    assert str(T[a, b, d, c]) == 'T[a, b, d, c]'\n    assert str(T[a, b, -c]) == 'T[a, b, -c]'\n\n", "def test_issue_14567():\n    assert factorial(Sum(x, (x, 0, 1))) + y  # doesn't raise an error\n", "def test_issue_16869():\n    from sympy.expressions import Tuple, Product\n    x, y = symbols('x y')\n    t = Tuple(x, y)\n    p = Product(x**n for n in range(3))\n    assert str(p) == \"x**0*x**1*x**2\"  # should use ** instead of ^\n    assert str(t) == \"(x, y)\"  # should use parentheses\n\n\n", "def test_issue_18650():\n    from sympy import Integer, symbols\n    x, y = symbols('x,y')\n    assert str(Integer(x) + Integer(y)) == \"Integer(x) + Integer(y)\"\n    assert str(Integer(x) * Integer(y)) == \"Integer(x) * Integer(y)\"\n    assert str(Integer(x) / Integer(y)) == \"Integer(x) / Integer(y)\"\n", "def test_issue_15922():\n    a, b = symbols('a b')\n    expr = S(a) / S(b)\n\n\n    assert str(expr) == 'a/b' \n\n\n"], "sample_1086": ["def test_issue_16155():\n    from sympy.parsing.mathematica import parse_mathematica\n    expr = parse_mathematica(\"1/2+x/2\")\n    assert str(expr) == \"(1/2) + (x/2)\"\n", "def test_Binomial():\n    assert str(Binomial(n, k)) == \"Binomial(n, k)\"\n", "def test_Issue_16921():\n    from sympy.functions import tan\n    assert str(tan(sin(x))) == \"tan(sin(x))\"\n", "compilation error", "def test_issue_16463():\n    from sympy.parsing.mathematica import parse_mathematica\n    expr = parse_mathematica('a[i,j]*b[j,k]')\n    assert str(expr) == 'a[i, j]*b[j, k]' \n"], "sample_939": ["    def test_unparse_conditional_expression(source, expected):\n        module = ast.parse(source)\n        assert ast.unparse(module.body[0].value, source) == expected\n", "    def test_unparse_slices(source, expected):\n        module = ast.parse(source)\n        assert ast.unparse(module.body[0].value, source) == expected\n", "    def test_unparse_starred_args(source, expected):\n        module = ast.parse(source)\n        assert ast.unparse(module.body[0].value, source) == expected\n", "    def test_unparse_slice(source, expected):\n        module = ast.parse(source)\n        assert ast.unparse(module.body[0].value, source) == expected\n", "def test_unparse_py38_keyword_only(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].value, source) == expected  \n"], "sample_833": ["def test_LogisticRegression_coef_shape(multi_class):\n    # Test that coef_ shape is correct for different multi_class options\n\n    n_classes = 3\n    n_features = 20\n    X, y = make_classification(n_samples=200, n_classes=n_classes,\n                               n_informative=n_classes, n_features=n_features,\n                               random_state=0)\n\n    lr = LogisticRegression(penalty='l2', solver='saga', random_state=0,\n                            multi_class=multi_class)\n    lr.fit(X, y)\n\n    if multi_class == 'ovr':\n        assert lr.coef_.shape == (n_classes, n_features)\n    elif multi_class == 'multinomial':\n        assert lr.coef_.shape == (n_classes - 1, n_features)\n\n", "def test_LogisticRegression_sparse_coefs(penalty):\n    # Test that LogisticRegression works with sparse input data\n    # and returns sparse coef_ attributes when appropriate.\n\n    n_samples = 1000\n    n_features = 1000\n    X = sp.random.rand(n_samples, n_features)\n    X = X.tocsr()  # Convert to sparse format\n    y = np.random.randint(0, 2, size=n_samples)\n\n    lr = LogisticRegression(penalty=penalty, solver='saga',\n                            random_state=0)\n    lr.fit(X, y)\n\n    if penalty in ('l1', 'elasticnet'):\n        assert isinstance(lr.coef_, sp.csr_matrix)\n    else:\n        assert isinstance(lr.coef_, np.ndarray)\n\n\n", "def test_logistic_regression_predict_proba_ovr_multinomial(multi_class):\n    # Make sure predict_proba returns probabilities in the correct format\n    # for both ovr and multinomial\n    n_classes = 3\n    n_samples = 100\n    X, y = make_classification(n_samples=n_samples, n_classes=n_classes,\n                               n_informative=n_classes, n_features=20,\n                               random_state=0)\n    lr = LogisticRegression(random_state=0, multi_class=multi_class)\n    lr.fit(X, y)\n\n    # Check that probabilities sum to 1 for each class for both ovr and multinomial\n    for i in range(n_samples):\n        proba = lr.predict_proba(X[i].reshape(1, -1))\n        if multi_class == 'ovr':\n            assert np.allclose(np.sum(proba, axis=1)[0], 1.0, atol=1e-5)\n        else:\n            # For multinomial, each class has its own probability\n            assert np.allclose((np.sum(proba, axis=1)[0]), 1.0, atol=1e-5)\n", "def test_logistic_regression_tol(tol):\n    # test that tol parameter is passed correctly\n    # to fit method\n    X, y = make_classification(n_samples=100, random_state=0)\n\n    lr = LogisticRegression(solver='liblinear', tol=tol, random_state=0)\n    assert lr.fit(X, y).tol_ == tol\n\n\n", "def test_LogisticRegression_loss_parameter(loss):\n    msg = (\"Loss parameter is currently only supported for \"\n           \"SGDClassifier.\")\n    with pytest.raises(AttributeError) as e:\n        LogisticRegression(loss=loss).fit(X, Y1)\n    assert str(e.value) == msg\n\n"], "sample_95": ["    def test_cache_control_decorator(self):\n        @cache_control(private=True, max_age=3600)\n            return HttpResponse()\n        r = a_view(HttpRequest())\n        self.assertEqual(\n            set(r['Cache-Control'].split(', ')),\n            {'private', 'max-age=3600'},\n        )\n\n\n\n", "    def test_cache_control_with_override(self):\n        @never_cache\n        @cache_control(max_age=3600)\n            return HttpResponse()\n        r = a_view(HttpRequest())\n        self.assertEqual(\n            set(r['Cache-Control'].split(', ')),\n            {'max-age=0', 'no-cache', 'no-store', 'must-revalidate', 'private'},\n        )\n", "    def test_cache_control_decorator(self):\n        @cache_control(private=True, max_age=3600, no_cache=True)\n            return HttpResponse()\n        r = a_view(HttpRequest())\n        self.assertEqual(\n            set(r['Cache-Control'].split(', ')),\n            {'private', 'max-age=3600', 'no-cache'}\n        )\n", "    def test_cache_control_decorator(self):\n        @cache_control(\n            private=True,\n            no_cache=True,\n            no_store=True,\n            must_revalidate=True,\n            max_age=0,\n        )\n            return HttpResponse()\n        r = a_view(HttpRequest())\n        self.assertEqual(\n            set(r['Cache-Control'].split(', ')),\n            {'private', 'no-cache', 'no-store', 'must-revalidate', 'max-age=0'},\n        )\n\n\n\n", "    def test_cache_control_kwargs(self):\n        @never_cache(private=False, no_cache=False)\n            return HttpResponse()\n        r = a_view(HttpRequest())\n        self.assertEqual(\n            set(r['Cache-Control'].split(', ')),\n            {'max-age=0', 'no-store', 'must-revalidate'},\n        )\n"], "sample_159": ["    def test_non_string_permission_codename(self):\n        class Checked(models.Model):\n            class Meta:\n                permissions = [\n                    ('my_permission', 123),\n                ]\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Error(\n                \"Permission codename must be a string, not int, for model \"\n                \"'auth_tests.Checked'.\",\n                obj=Checked,\n                id='auth.E009',\n            ),\n        ])\n", "    def test_permission_codename_clashing_with_builtin(self):\n\n        class Checked(models.Model):\n            class Meta:\n                permissions = [\n                    ('change_group', 'Can edit groups'),\n                ]\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Error(\n                \"The permission codenamed 'change_group' clashes with a builtin \"\n                \"permission for model 'auth_tests.Checked'.\",\n                obj=Checked,\n                id='auth.E005',\n            ),\n        ]) \n", "    def test_required_fields_is_empty_string(self):\n        class CustomUserNonListRequiredFields(AbstractBaseUser):\n            username = models.CharField(max_length=30, unique=True)\n            date_of_birth = models.DateField()\n\n            USERNAME_FIELD = 'username'\n            REQUIRED_FIELDS = ''\n\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Error(\n                \"'REQUIRED_FIELDS' must be a list or tuple.\",\n                obj=CustomUserNonListRequiredFields,\n                id='auth.E001',\n            ),\n        ])\n", "    def test_unlisted_permission_in_default_permissions(self):\n        class Checked(models.Model):\n            class Meta:\n                permissions = [\n                    ('change_checked', 'Can edit permission'),\n                    ('my_custom_permission', 'Some permission with duplicate permission code'),\n                ]\n                default_permissions = ('change_checked',)\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Error(\n                \"The permission codenamed 'my_custom_permission' is duplicated for \"\n                \"model 'auth_tests.Checked'.\",\n                obj=Checked,\n                id='auth.E006',\n            ),\n        ])\n", "    def test_special_characters_in_permission_names(self):\n        class Checked(models.Model):\n            class Meta:\n                permissions = [\n                    ('change_special_chars', 'Can edit permission with special chars !@#$%^&*()'),\n                ]\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [])\n"], "sample_560": ["def test_loc_invalid_string():\n    fig, ax = plt.subplots()\n    with pytest.raises(ValueError, match=\"'invalid' is not a valid value for\"):\n        ax.legend(loc='invalid') \n", "def test_legend_labelspacing():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3], label='test')\n    leg = ax.legend()\n    assert leg.get_texts()[-1].get_bbox_to_anchor()[1] > 0.01\n    leg.set_labelspacing(0.0)\n    assert leg.get_texts()[-1].get_bbox_to_anchor()[1] == 0.0 \n", "def test_multiple_legend_handles_in_list():\n    fig, ax = plt.subplots()\n    l1 = plt.Line2D([], [], color='red', label='Line 1')\n    l2 = plt.Line2D([], [], color='blue', label='Line 2')\n    handles = [l1, l2]\n    ax.legend(handles=handles)\n", "def test_loc_positioned_legend():\n    fig, ax = plt.subplots()\n    ax.plot([0, 1], [0, 1], label='line')\n    leg = ax.legend(loc='upper right')\n    assert isinstance(leg.get_bbox_patch(), mpl.patches.Rectangle)\n\n", "def test_legend_loc_outside():\n    fig, ax = plt.subplots()\n    ax.plot([0, 1], [0, 1])\n    leg = ax.legend(loc='outside')\n    assert leg.get_bbox_to_anchor() != (0.5, 0.5)\n\n\n"], "sample_481": ["    def test_join09(self):\n        output = self.engine.render_to_string(\"join09\", {\"a\": [\"alpha\", \"beta & me\"], \"var\": \"&<\"})\n        self.assertEqual(output, \"alpha&lt;beta &amp; me\")\n", "    def test_join09(self):\n        output = self.engine.render_to_string(\"join09\", {\"a\": [\"alpha\", \"beta & me\"]})\n        self.assertEqual(output, \"alpha beta & me\")\n", "    def test_join_with_single_string_argument(self):\n        output = self.engine.render_to_string(\n            \"join_with_single_string_argument\", {\"a\": [\"alpha\", \"beta & me\"]}\n        )\n        self.assertEqual(output, \"alpha&amp; beta &amp; me\")\n\n", "    def test_join09(self):\n        output = self.engine.render_to_string(\"join09\", {\"a\": [\"alpha\", \"beta & me\"]})\n        self.assertEqual(output, \"alpha beta & me\")\n", "    def test_join09(self):\n        output = self.engine.render_to_string(\"join09\", {\"a\": [\"alpha\", \"beta & me\"]})\n        self.assertEqual(output, \"alphabeta & me\")\n\n\n\n\n"], "sample_949": ["def test_image(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'python.1').read_text()\n\n    assert r'\\fBi\\fP' in content\n", "def test_toc_with_urls(app, status, warning):\n    app.build()    \n    content = (app.outdir / 'python.1').read_text()\n    assert 'Table of Contents' in content \n    assert 'http://example.com/python/' in content\n", "def test_special_chars(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'python.1').read_text()\n    assert r'\\fBThis is a special character\\fP' in content\n\n", "def test_crossref(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'python.1').read_text()\n    assert r'See also \\fIexample.txt\\fP' in content\n\n\n\n\n", "def test_show_urls(app, status, warning):\n    app.build()\n    content = (app.outdir / 'python.1').read_text()\n    assert r'\\fBhttps://www.sphinx-doc.org\\en\\fP' in content \n"], "sample_982": ["def test_isprime():\n    assert isprime(2)\n    assert not isprime(1)\n    assert isprime(3)\n    assert not isprime(4)\n    assert isprime(5)\n    assert not isprime(6)\n    assert isprime(7)\n    assert not isprime(8)\n    assert isprime(11)\n    assert not isprime(12)\n    assert isprime(13)\n    assert not isprime(14)\n    assert isprime(17)\n    assert not isprime(18)\n    assert isprime(19)\n    assert not isprime(20)\n    assert isprime(23)\n    assert not isprime(24)\n    assert isprime(29)\n    assert not isprime(30)\n    assert isprime(31)\n    assert not isprime(32)\n    assert isprime(37)\n    assert not isprime(38)\n    assert isprime(41)\n    assert not isprime(42)\n    assert isprime(43)\n    assert not isprime(44)\n    assert isprime(47)\n    assert not isprime(48)\n    assert isprime(53)\n    assert not isprime(54)\n    assert isprime(59)\n    assert not isprime(60)\n    assert isprime(61)\n    assert not isprime(62)\n    assert isprime(67)\n    assert not isprime(68)\n    assert isprime(71)\n    assert not isprime(72)\n    assert isprime(73)\n    assert not isprime(74)\n    assert isprime(79)\n    assert not isprime(80)\n    assert isprime(83)\n    assert not isprime(84)\n    assert isprime(89)\n    assert not isprime(90)\n    assert isprime(97)\n    assert isprime(101)\n    assert not isprime(102)\n    assert isprime(103)\n    assert not isprime(104)\n    assert is", "def test_isprime():\n    assert isprime(2)\n    assert isprime(3)\n    assert isprime(5)\n    assert isprime(7)\n    assert isprime(11)\n    assert isprime(29)\n    assert not isprime(4)\n    assert not isprime(6)\n    assert not isprime(8)\n    assert not isprime(9)\n    assert not isprime(10)\n    assert not isprime(12)\n    assert not isprime(13)\n    assert not isprime(14)\n\n    assert isprime(nextprime(100000))\n    assert not isprime(nextprime(100000) + 1)\n\n    assert isprime(nextprime(2**64))\n    assert isprime(2**63 - 1)\n\n    assert isprime(-1) is False\n    assert isprime(0) is False\n    assert isprime(1) is False\n\n\n\n", "def test_is_prime():\n    assert is_prime(2)\n    assert is_prime(3)\n    assert is_prime(5)\n    assert is_prime(7)\n    assert is_prime(11)\n    assert is_prime(13)\n    assert is_prime(17)\n    assert not is_prime(1)\n    assert not is_prime(4)\n    assert not is_prime(6)\n    assert not is_prime(8)\n    assert not is_prime(9)\n    assert not is_prime(10)\n    assert is_prime(97)\n    assert is_prime(1009)\n    assert is_prime(1229)\n\n    for i in range(100, 1000):\n        assert is_prime(i) == (i > 1 and all(i % j for j in range(2, int(i**0.5) + 1)))\n\n    assert is_prime(2**32 - 1)\n    assert is_prime(2**61 - 1)\n\n\n\n", "def test_is_prime():\n    assert is_prime(2)\n    assert is_prime(3)\n    assert is_prime(5)\n    assert is_prime(7)\n    assert not is_prime(4)\n    assert not is_prime(6)\n    assert not is_prime(9)\n    assert not is_prime(10)\n    assert is_prime(11)\n    assert is_prime(13)\n    assert not is_prime(14)\n    assert is_prime(17)\n    assert not is_prime(18)\n    assert is_prime(19)\n    assert is_prime(23)\n    assert not is_prime(24)\n    assert is_prime(29)\n    assert not is_prime(30)\n    assert is_prime(31)\n    assert not is_prime(32)\n    assert is_prime(37)\n    assert not is_prime(38)\n    assert is_prime(41)\n    assert not is_prime(42)\n    assert is_prime(43)\n    assert not is_prime(44)\n    assert is_prime(47)\n    assert not is_prime(48)\n    assert is_prime(53)\n    assert not is_prime(54)\n    assert is_prime(59)\n    assert not is_prime(60)\n    assert is_prime(61)\n    assert not is_prime(62)\n    assert is_prime(67)\n    assert not is_prime(68)\n    assert is_prime(71)\n    assert not is_prime(72)\n    assert is_prime(73)\n    assert not is_prime(74)\n    assert is_prime(79)\n    assert not is_prime(80)\n    assert is_prime(83)\n    assert not is_prime(84)\n    assert is_prime(89)\n    assert not is_prime(90)\n    assert is_prime(97)\n    assert not is_prime(98)\n    assert is_prime", "compilation error"], "sample_1035": ["def test_measure_partial_oneshot():\n    from sympy.physics.quantum.qubit import measure_partial_oneshot\n\n    nqubits = 2\n    result = measure_partial_oneshot(Qubit(IntQubit(0, nqubits=nqubits))*sqrt(1/2) + Qubit(IntQubit(1, nqubits=nqubits))*sqrt(1/2), bits=(0,))\n    assert isinstance(result, Qubit)\n    assert result.as_int() in (0, 1)\n", "def test_measure_partial():\n    nqubits = 3\n    c = H(0)*H(1)*Qubit('000')\n    m = qapply(c)\n    result = measure_partial(m, (0,))\n    assert len(result) == 2\n    assert result[0][0] == sqrt(2)*Qubit('000') + sqrt(2)*Qubit('100')\n    assert abs(result[0][1]) == 1/2\n    assert result[1][0] == sqrt(2)*Qubit('010') + sqrt(2)*Qubit('110')\n    assert abs(result[1][1]) == 1/2\n\n\n\n", "def test_measure_all():\n    nbits = 3\n    c = superposition_basis(nbits)\n    result = measure_all(c)\n    assert len(result) == 8\n    for i in range(8):\n        assert abs(result[i][1] - 1/8) < 1e-6\n    \n\n    c = qubit_to_matrix(c)\n    c = c.normalized()\n    c = c*c.conjugate().transpose()\n\n    result = measure_all(c)\n    assert len(result) == 8\n    for i in range(8):\n        assert abs(result[i][1] - 1/8) < 1e-6\n", "def test_measure_all_oneshot():\n    m = IntQubit(2, nqubits=1)\n    result = measure_all_oneshot(m)\n    assert isinstance(result, IntQubit)\n    assert result.as_int() in (0, 1)\n\n", "def test_measure_partial_oneshot():\n    nqubits = 2\n    qubit = superposition_basis(nqubits)\n    result = measure_partial_oneshot(qubit, (0,))\n    assert isinstance(result, Qubit)\n    assert result.as_int() in range(2**nqubits) \n\n\n\n"], "sample_389": ["    def test_media_url_with_trailing_slash(self):\n        tests = (\"/media/\", \"/media\", \"/media/\")\n        for setting in (\"MEDIA_URL\",):\n            for path in tests:\n                new_settings = {setting: path}\n                with self.settings(**new_settings):\n                    with self.subTest(path=path):\n                        for script_name in [\"/\", \"/somesubpath/\"]:\n                            try:\n                                self.set_script_name(script_name)\n                                self.assertEqual(getattr(settings, setting), path)\n                            finally:\n                                clear_script_prefix()\n\n\n", "    def test_override_settings_with_envvar(self):\n        \"\"\"\n        Override settings using environment variable should work as expected.\n        \"\"\"\n        os.environ[ENVIRONMENT_VARIABLE] = \"fake_module.settings\"\n        class FakeSettingsModule:\n            MEDIA_URL = \"/media/\"\n        sys.modules[\"fake_module.settings\"] = FakeSettingsModule\n        try:\n            with self.settings():\n                self.assertEqual(settings.MEDIA_URL, \"/media/\")\n        finally:\n            del sys.modules[\"fake_module.settings\"]\n            del os.environ[ENVIRONMENT_VARIABLE]\n\n\n", "    def test_django_admin_url_pattern(self):\n        from django.conf import global_settings\n        tests = (\n            (\"django/admin/\", \"/admin/\", \"/admin/\"),\n            (\"django/admin/site/\", \"/admin/site/\", \"/admin/site/\"),\n        )\n        for script_name, expected_url, expected_admin_url in tests:\n            new_settings = {\"ADMIN_URL\": script_name}\n            with self.settings(**new_settings):\n                with self.subTest(script_name=script_name, **new_settings):\n                    try:\n                        self.set_script_name(script_name)\n                        self.assertEqual(global_settings.ADMIN_URL, expected_admin_url)\n                        self.assertEqual(settings.ADMIN_URL, expected_admin_url)\n                    finally:\n                        clear_script_prefix()\n\n\n\n", "    def test_empty_string_media_url(self):\n        with self.settings(MEDIA_URL=\"\"):\n            self.assertEqual(settings.MEDIA_URL, \"\")\n            self.assertEqual(getattr(settings, 'MEDIA_ROOT'), '')\n", "    def test_reverse_url_with_script_name_prefix(self):\n        import reverse_url\n\n        self.set_script_name(\"/somesubpath\")\n\n        with override_settings(MEDIA_URL=\"/media/\", STATIC_URL=\"/static/\"):\n            with self.subTest(\"media\"):\n                view = reverse_url.reverse(\"media-view\")\n                self.assertEqual(view, \"/somesubpath/media/')\n            with self.subTest(\"static\"):\n                view = reverse_url.reverse(\"static-view\")\n                self.assertEqual(view, \"/somesubpath/static/\")\n        clear_script_prefix() \n"], "sample_440": ["    def test_update_conflicts_with_target_unique_fields(self):\n        with self.subTest(\"Updating with target unique_fields\"):\n            target_field = \"name\"\n            unique_fields = [\"f1\"]\n            objects = [\n                TwoFields(f1=1, f2=1, name=\"a\"),\n                TwoFields(f1=2, f2=2, name=\"b\"),\n            ]\n            TwoFields.objects.bulk_create(objects)\n\n            conflicting_objects = [\n                TwoFields(f1=1, f2=1, name=\"c\"),\n                TwoFields(f1=2, f2=2, name=\"d\"),\n            ]\n            TwoFields.objects.bulk_create(\n                conflicting_objects,\n                update_conflicts=True,\n                update_fields=[\"name\"],\n                unique_fields=unique_fields,\n                target_field=target_field,\n            )\n            self.assertEqual(TwoFields.objects.count(), 2)\n            self.assertCountEqual(\n                TwoFields.objects.values(\"f1\", \"f2\", \"name\"),\n                [\n                    {\"f1\": 1, \"f2\": 1, \"name\": \"c\"},\n                    {\"f1\": 2, \"f2\": 2, \"name\": \"d\"},\n                ],\n            )\n\n        with self.subTest(\"Updating with target unique_fields, non-existent\"):\n            unique_fields = [\"non_existent_field\"]\n            objects = [\n                TwoFields(f1=1, f2=1, name=\"a\"),\n                TwoFields(f1=2, f2=2, name=\"b\"),\n            ]\n            TwoFields.objects.bulk_create(objects)\n\n            conflicting_objects = [\n                TwoFields(f1=1, f2=1, name=\"c\"),\n                TwoFields(f1=2, f2=2, name=\"d\"),\n            ]\n            with self.assertRaises(FieldDoesNotExist):\n                TwoFields.objects.bulk_create(", "    def test_update_conflicts_non_existing_update_field(self):\n        msg = \"UpsertConflict has no field named 'nonexistent'\"\n        with self.assertRaisesMessage(FieldDoesNotExist, msg):\n            UpsertConflict.objects.bulk_create(\n                [\n                    UpsertConflict(number=1, rank=1, name=\"John\"),\n                    UpsertConflict(number=2, rank=2, name=\"Mary\"),\n                    UpsertConflict(number=3, rank=3, name=\"Hannah\"),\n                ],\n                update_conflicts=True,\n                update_fields=[\"nonexistent\"],\n                unique_fields=[\"number\"],\n            )\n", "    def test_update_conflicts_unique_fields_reverse_one_to_one(self):\n        Country.objects.bulk_create(self.data)\n        self.assertEqual(Country.objects.count(), 4)\n\n        # Reverse one-to-one relationship: Country has a reverse relation to State\n        State.objects.bulk_create(\n            [\n                State(\n                    name=\"Illinois\",\n                    two_letter_code=\"IL\",\n                    country=country\n                )\n                for country in self.data\n            ]\n        )\n        self.assertEqual(State.objects.count(), 4)\n\n        conflicting_objects = [\n            Country(\n                name=\"Germany\",\n                iso_two_letter=\"DE\",\n                description=(\"Germany is a country in Central Europe.\"),\n            ),\n            Country(\n                name=\"Czech Republic\",\n                iso_two_letter=\"CZ\",\n                description=(\n                    \"The Czech Republic is a landlocked country in Central Europe.\"\n                ),\n            ),\n            Country(name=\"Australia\", iso_two_letter=\"AU\"),\n            Country(\n                name=\"Japan\",\n                iso_two_letter=\"JP\",\n                description=(\"Japan is an island country in East Asia.\"),\n            ),\n        ]\n        State.objects.bulk_create(\n            [\n                State(\n                    name=\"Illinois\",\n                    two_letter_code=\"IL\",\n                    country=conflicting_country\n                )\n                for conflicting_country in conflicting_objects\n            ],\n            update_conflicts=True,\n            update_fields=[\"description\"],\n            unique_fields=[\"country\"],  \n        )\n\n        self.assertEqual(Country.objects.count(), 8)\n        self.assertEqual(State.objects.count(), 8)\n        self.assertCountEqual(\n            Country.objects.values(\"iso_two_letter\", \"description\"),\n            [\n                {\"iso_two_letter\": \"US\", \"description\": \"\"},\n                {\"iso_two_letter\": \"NL\", \"description\": \"\"},\n                {\n                    \"iso_two_letter\": \"DE\",\n                    \"description\":", "    def test_update_conflicts_no_fields_to_update(self):\n        msg = \"Fields that will be updated when a row insertion fails on conflicts must be provided.\"\n        with self.assertRaisesMessage(ValueError, msg):\n            UpsertConflict.objects.bulk_create(\n                [\n                    UpsertConflict(number=1, rank=1, name=\"John\"),\n                    UpsertConflict(number=2, rank=2, name=\"Mary\"),\n                    UpsertConflict(number=3, rank=3, name=\"Hannah\"),\n                ],\n                update_conflicts=True,\n                unique_fields=[\"number\"],\n            )\n", "    def test_update_conflicts_with_target(self):\n        data = [\n            Country(name=\"Germany\", iso_two_letter=\"DE\", description=\"Germany\"),\n            Country(name=\"Netherlands\", iso_two_letter=\"NL\", description=\"Netherlands\"),\n        ]\n        Country.objects.bulk_create(data)\n        self.assertEqual(Country.objects.count(), 2)\n\n        conflicting_objects = [\n            Country(name=\"Germany\", iso_two_letter=\"DE\", description=\"Updated Germany\"),\n            Country(\n                name=\"Netherlands\", iso_two_letter=\"NL\", description=\"Updated Netherlands\"\n            ),\n        ]\n        Country.objects.bulk_create(\n            conflicting_objects,\n            update_conflicts=True,\n            update_fields=[\"description\"],\n            unique_fields=[\"iso_two_letter\"],\n            target_fields=[\"name\"],\n        )\n        self.assertEqual(Country.objects.count(), 2)\n        self.assertCountEqual(\n            Country.objects.values(\"iso_two_letter\", \"description\"),\n            [\n                {\"iso_two_letter\": \"DE\", \"description\": \"Updated Germany\"},\n                {\"iso_two_letter\": \"NL\", \"description\": \"Updated Netherlands\"},\n            ],\n        )\n\n"], "sample_527": ["def test_interactive_zoom_reset():\n    fig, ax = plt.subplots()\n    ax.plot(np.arange(10))\n    ax.set_xlim(1, 9)\n    ax.set_ylim(0, 8)\n    assert ax.get_navigate()\n\n    # Zoom in\n    start_event = MouseEvent(\"button_press_event\", fig.canvas, 0.1, 0.2,\n                             MouseButton.LEFT)\n    stop_event = MouseEvent(\"button_release_event\", fig.canvas, 0.9, 0.8,\n                             MouseButton.LEFT)\n    tb = NavigationToolbar2(fig.canvas)\n    tb.zoom()\n    tb.press_zoom(start_event)\n    tb.drag_zoom(stop_event)\n    tb.release_zoom(stop_event)\n    assert ax.get_xlim() != (1, 9)\n    assert ax.get_ylim() != (0, 8)\n\n    # Reset zoom\n    tb.reset_zoom()\n\n    assert ax.get_xlim() == (1, 9)\n    assert ax.get_ylim() == (0, 8)\n\n\n\n", "def test_interactive_pick_event_with_data():\n    fig, ax = plt.subplots()\n    x = np.random.rand(10)\n    y = np.random.rand(10)\n    scat = ax.scatter(x, y, s=100, picker=True)\n\n        assert event.artist is scat\n        assert isinstance(event.artist, mpl.collections.Collection)\n        assert isinstance(event.mouseevent, MouseEvent)\n\n    fig.canvas.mpl_connect('pick_event', on_pick)\n\n    # Trigger a pick event.\n    # Note that the event is not connected to a specific mouse button\n    # as pick events can be triggered by any mouse button\n    mouse_event = MouseEvent(\n        \"button_press_event\", fig.canvas, \n        xdata=x[0], ydata=y[0],\n    )\n    fig.canvas.callbacks.process('pick_event', mouse_event)\n\n\n", "def test_toolmanager_set_tool_visible():\n    with pytest.warns(UserWarning, match=_EXPECTED_WARNING_TOOLMANAGER):\n        plt.rcParams['toolbar'] = 'toolmanager'\n    fig = plt.gcf()\n    toolmanager = fig.canvas.manager.toolmanager\n    # Check if the 'zoom' tool is initially visible\n    assert toolmanager.get_tool('zoom').get_visible()\n    # Hide the 'zoom' tool\n    toolmanager.set_tool_visible('zoom', False)\n    assert not toolmanager.get_tool('zoom').get_visible()\n    # Hide a non-existent tool should not raise an error\n    toolmanager.set_tool_visible('nonexistent', False)\n    # Show the 'zoom' tool again\n    toolmanager.set_tool_visible('zoom', True)\n    assert toolmanager.get_tool('zoom').get_visible()\n\n\n", "def test_button_press_event():\n    fig, ax = plt.subplots()\n    canvas = fig.canvas\n    fig.canvas.figpatch.set_facecolor('green')\n    event = MouseEvent(\"button_press_event\", canvas, 0, 0,\n                       MouseButton.RIGHT)\n    event._process()\n    assert canvas.mpl_connect('button_press_event') is not None\n\n\n\n\n", "def test_toolmanager_visibility():\n    with pytest.warns(UserWarning, match=_EXPECTED_WARNING_TOOLMANAGER):\n        plt.rcParams['toolbar'] = 'toolmanager'\n    fig = plt.gcf()\n    toolmanager = fig.canvas.manager.toolmanager\n    \n    assert toolmanager.get_tool_visible('zoom')\n    toolmanager.set_tool_visible('zoom', False)\n    assert not toolmanager.get_tool_visible('zoom')\n    toolmanager.set_tool_visible('zoom', True)\n    assert toolmanager.get_tool_visible('zoom')\n\n    with pytest.warns(UserWarning, match=\"ToolManager does not control tool 'foo'\"):\n        toolmanager.set_tool_visible('foo', False)\n"], "sample_228": ["    def test_all_valid_empty_formsets(self):\n        \"\"\"all_valid() handles empty formsets correctly.\"\"\"\n        empty_formset = formset_factory(Choice)(initial=[])\n        self.assertIs(all_valid((empty_formset, empty_formset)), True)\n        self.assertEqual(empty_formset._errors, [{}, {}])\n\n", "    def test_all_valid_with_non_form_errors(self):\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MAX_NUM_FORMS': '1',  # should be ignored\n        }\n        ChoiceFormSet = formset_factory(Choice, extra=1, validate_max=True)\n        formset1 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        formset2 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertIs(all_valid((formset1, formset2)), False)\n        self.assertEqual(formset1._errors, [{'non_form_error': ['This is a non-form error.']}])\n        self.assertEqual(formset2._errors, [{'non_form_error': ['This is a non-form error.']}])\n", "    def test_all_valid_with_non_form_errors(self):\n        data = {\n            'choices-TOTAL_FORMS': '2',  # the number of forms rendered\n            'choices-INITIAL_FORMS': '0',  # the number of forms with initial data\n            'choices-MAX_NUM_FORMS': '2',  # max number of forms - should be ignored\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '0',\n            'choices-1-choice': 'One',\n            'choices-1-votes': '1',\n        }\n        ChoiceFormSet = formset_factory(Choice, extra=1, max_num=1, validate_max=True)\n        formset1 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        formset2 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n\n\n        self.assertIs(all_valid((formset1, formset2)), False)\n        expected_errors = [{}, {'choices-TOTAL_FORMS': ['You may only specify a maximum of 1 form.']}]\n        self.assertEqual(formset1._errors, expected_errors)\n        self.assertEqual(formset2._errors, expected_errors)\n\n", "    def test_can_delete_formset_forms(self):\n        ChoiceFormFormset = formset_factory(form=Choice, can_delete=True, extra=2)\n        formset = ChoiceFormFormset()\n        self.assertEqual(len(formset), 2)\n        self.assertIn('DELETE', formset.forms[0].fields)\n        self.assertIn('DELETE', formset.forms[1].fields)\n        formset.data = {'form-TOTAL_FORMS': '2', 'form-INITIAL_FORMS': '2', 'form-DELETE': 'on'}\n        self.assertEqual(len(formset.cleaned_data), 1)\n        self.assertTrue(formset.is_valid())\n\n\n", "    def test_formset_non_form_error(self):\n        class BaseCustomFormSet(BaseFormSet):\n                raise ValidationError(\"This is a non-form error\")\n\n        ChoiceFormSet = formset_factory(Choice, formset=BaseCustomFormSet)\n        data = {\n            'choices-TOTAL_FORMS': '1',\n            'choices-INITIAL_FORMS': '0',\n        }\n        formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertFalse(all_valid((formset, formset)))\n\n"], "sample_487": ["    def test_actions_with_permissions(self):\n        @admin.action(permissions=[\"can_delete_band\"])\n            queryset.delete()\n\n        class BandAdmin(ModelAdmin):\n            actions = (delete_selected_bands,)\n\n        self.assertIsInvalid(\n            BandAdmin,\n            Band,\n            \"BandAdmin must define a has_delete_selected_bands() method for the \"\n            \"delete_selected_bands action.\",\n            id=\"admin.E129\",\n        )\n", "    def test_actions_with_custom_permissions_require_matching_has_method(self):\n        @admin.action(permissions=[\"custom_permission\"])\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = [custom_action]\n\n        self.assertIsInvalid(\n            BandAdmin,\n            Band,\n            \"BandAdmin must define a has_custom_permission() method for the \"\n            \"custom_action action.\",\n            id=\"admin.E129\",\n        )\n\n", "    def test_actions_no_matching_permissions(self):\n        @admin.action(permissions=[\"fake_permission\"])\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = (custom_permission_action,)\n\n        self.assertIsInvalid(\n            BandAdmin,\n            Band,\n            \"BandAdmin action 'custom_permission_action' requires \"\n            \"permission 'fake_permission' which is not defined.\",\n            id=\"admin.E131\",\n        )\n", "    def test_actions_invalid_action(self):\n        class InvalidAction:\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = (InvalidAction,)\n\n        self.assertIsInvalid(\n            BandAdmin,\n            Band,\n            \"Actions must be callable objects.\",\n            id=\"admin.E131\",\n        )\n\n\n\n", "    def test_actions_with_invalid_permission(self):\n        @admin.action(permissions=[\"invalid_permission\"])\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = (some_action,)\n\n        self.assertIsInvalid(\n            BandAdmin,\n            Band,\n            \"The permission 'invalid_permission' is not defined.\",\n            id=\"admin.E131\",\n        )\n"], "sample_834": ["def test_precomputed_transformation():\n    X = iris_data\n    y = iris_target\n\n    # Generate a random precomputed transformation\n    n_components = X.shape[1]\n    transformation = np.random.randn(n_components, n_components)\n\n    nca = NeighborhoodComponentsAnalysis(\n        init='precomputed', n_components=n_components\n    )\n    nca.components_ = transformation\n\n    # Check if the output is the same as if fit was called\n    assert_array_equal(X, nca.transform(X))\n\n\n", "def test_nca_transform_shape(init):\n    nca = NeighborhoodComponentsAnalysis(init=init, n_components=2)\n    X = np.arange(12).reshape(4, 3)\n    y = [1, 1, 2, 2]\n    nca.fit(X, y)\n    transformed_X = nca.transform(X)\n    assert transformed_X.shape[1] == nca.n_components\n", "def test_n_components_too_large():\n    X, y = make_classification(n_samples=30, n_features=5, n_classes=4,\n                               n_redundant=0, n_informative=5, random_state=0)\n    n_components = X.shape[1] + 1\n\n    nca = NeighborhoodComponentsAnalysis(n_components=n_components)\n    assert_raise_message(ValueError,\n                         'The preferred dimensionality of the '\n                         'projected space `n_components` ({}) cannot '\n                         'be greater than the given data '\n                         'dimensionality ({})!'\n                         .format(n_components, X.shape[1]),\n                         nca.fit, X, y)\n\n\n\n\n", "    def test_nca_transform_after_fit():\n        nca = NeighborhoodComponentsAnalysis(n_components=2)\n        X = iris_data[:5]\n        y = iris_target[:5]\n        nca.fit(X, y)\n        assert_array_almost_equal(nca.transform(X), nca.transform(X),\n                                 decimal=5)\n\n\n\n\n", "    def test_precomputed_init(init_type):\n        # Test that precomputed transformations can be used with nca\n        # init with different precomputed type: pca lda identity random\n        X = iris_data\n        y = iris_target\n        rng = np.random.RandomState(42)\n\n        if init_type == 'pca':\n            init = np.random.randn(iris_data.shape[1], iris_data.shape[1])\n        elif init_type == 'lda':\n            init = np.random.randn(iris_data.shape[1], iris_data.shape[1])\n        elif init_type == 'identity':\n            init = np.identity(iris_data.shape[1])\n        else:\n            init = rng.randn(iris_data.shape[1], iris_data.shape[1])\n\n        nca = NeighborhoodComponentsAnalysis(init=init,\n                                              n_components=iris_data.shape[1])\n\n        nca.fit(X, y)\n\n\n        \n"], "sample_287": ["    def test_list_filter_on_through_field_with_no_inverse(self):\n        \"\"\"\n        Regression test for #24146: Ensure list_filter works on a through field\n        even if there's no inverse relationship defined.\n        \"\"\"\n        class BookAdminWithListFilter(admin.ModelAdmin):\n            list_filter = ['authorsbooks__featured']\n\n        class OneToManyThroughWithoutInverse(models.Model):\n            pass\n\n        class Book(models.Model):\n            authorsbooks = models.ManyToManyField(\n                OneToManyThroughWithoutInverse,\n                through='admin_checks.BookAuthors',\n            )\n\n        class BookAuthors(models.Model):\n            book = models.ForeignKey(Book, on_delete=models.CASCADE)\n            authorsthrough = models.ForeignKey(\n                OneToManyThroughWithoutInverse, on_delete=models.CASCADE\n            )\n            featured = models.BooleanField()\n\n        errors = BookAdminWithListFilter(Book, AdminSite()).check()\n        self.assertEqual(errors, [])\n\n\n\n", "    def test_related_name_in_list_filter(self):\n        class SongAdmin(admin.ModelAdmin):\n            list_filter = ['album', 'album__author__name']\n\n        errors = SongAdmin(Song, AdminSite()).check()\n        self.assertEqual(errors, [])\n\n", "    def test_list_filter_works_on_through_field_even_with_nonexistent_app(self):\n        \"\"\"\n        Ensure list_filter can access reverse fields even when the app containing\n        the 'through' field's model doesn't exist; refs #24146.\n        \"\"\"\n        class BookAdminWithListFilter(admin.ModelAdmin):\n            list_filter = ['authorsbooks__featured']\n        \n        # Temporarily removing the app containing the 'through' model\n        old_apps = list(admin.site._registry.keys())\n        admin.site._registry.pop('admin_checks.Author', None)\n        try:\n            errors = BookAdminWithListFilter(Book, AdminSite()).check()\n            self.assertEqual(errors, [])\n        finally:\n            admin.site._registry.update({app: _registry_value for app, _registry_value in old_apps}) \n\n\n\n", "    def test_list_filter_works_on_through_field_even_when_apps_not_ready(self):\n        \"\"\"\n        Ensure list_filter can access reverse fields even when the app registry\n        is not ready; refs #24146.\n        \"\"\"\n        class BookAdminWithListFilter(admin.ModelAdmin):\n            list_filter = ['authorsbooks__featured']\n\n        # Temporarily pretending apps are not ready yet. This issue can happen\n        # if the value of 'list_filter' refers to a 'through__field'.\n        Book._meta.apps.ready = False\n        try:\n            errors = BookAdminWithListFilter(Book, AdminSite()).check()\n            self.assertEqual(errors, [])\n        finally:\n            Book._meta.apps.ready = True\n", "    def test_list_display_works_on_through_field_even_when_apps_not_ready(self):\n        \"\"\"\n        Ensure list_display can access reverse fields even when the app registry\n        is not ready; refs #24146.\n        \"\"\"\n        class BookAdminWithListDisplay(admin.ModelAdmin):\n            list_display = ['authorsbooks__featured']\n\n        # Temporarily pretending apps are not ready yet. This issue can happen\n        # if the value of 'list_display' refers to a 'through__field'.\n        Book._meta.apps.ready = False\n        try:\n            errors = BookAdminWithListDisplay(Book, AdminSite()).check()\n            self.assertEqual(errors, [])\n        finally:\n            Book._meta.apps.ready = True\n"], "sample_895": ["def test_remainder_with_transformers_and_set_output():\n    \"\"\"Check that the output is set for the remainder when using transformers.\"\"\"\n\n    pd = pytest.importorskip(\"pandas\")\n    df = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [True, False, True], \"c\": [4, 5, 6]})\n\n    ct = ColumnTransformer(\n        [\n            (\"numerical\", StandardScaler(), [\"a\", \"c\"]),\n            (\"categorical\", \"passthrough\", [\"b\"]),\n        ],\n        remainder=StandardScaler(),\n        verbose_feature_names_out=False,\n    )\n\n    ct.set_output(transform=\"pandas\")\n\n    out = ct.fit_transform(df)\n\n    # Expect \"a\" and \"c\" to be scaled, \"b\" to be unchanged\n    # remainder will scale \"b\" as well\n    assert_array_equal(out[\"a\"], (df[\"a\"] - df[\"a\"].mean()) / df[\"a\"].std())\n    assert_array_equal(out[\"c\"], (df[\"c\"] - df[\"c\"].mean()) / df[\"c\"].std())\n    assert_array_equal(out[\"b\"], df[\"b\"])\n\n\n\n", "def test_remainder_set_output_after_fitting():\n    pd = pytest.importorskip(\"pandas\")\n    df = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [True, False, True]})\n\n    ct = make_column_transformer(\n        (StandardScaler(), make_column_selector(dtype_include=number)),\n        remainder=\"passthrough\",\n        verbose_feature_names_out=False,\n    )\n\n    ct.fit(df)\n    ct.set_output(transform=\"pandas\")\n\n    out = ct.transform(df)\n    assert isinstance(out, pd.DataFrame)\n    assert out.columns.tolist() == [\"a\", \"b\"]\n\n\n\n", "compilation error", "def test_categorical_feature_mapping():\n    pd = pytest.importorskip(\"pandas\")\n\n    X = pd.DataFrame({\"cat\": [\"A\", \"B\", \"C\", \"A\"], \"num\": [1, 2, 3, 4]})\n    ct = ColumnTransformer(\n        [(\"encoder\", OneHotEncoder(sparse_output=False), [\"cat\"]), (\"scaler\", StandardScaler(), [\"num\"])],\n    )\n    X_trans = ct.fit_transform(X)\n\n    assert isinstance(X_trans, np.ndarray)\n    assert X_trans.shape[1] == 3  # 2 for 'cat' and 1 for 'num'\n    assert (X_trans[:, 0] == 0).any()  # 'A' encoded as 0\n    assert (X_trans[:, 1] == 1).any()  # 'B' encoded as 1\n    assert (X_trans[:, 2] == 2).any()  # 'C' encoded as 2\n    assert (X_trans[:, 3] == 1).any()  # 'num' scaled\n\n\n\n", "def test_column_transformer_with_invalid_transformer():\n    from sklearn.base import BaseEstimator\n\n    class InvalidTransformer(BaseEstimator):\n            raise ValueError(\"This is an invalid transformer.\")\n\n            raise ValueError(\"This is an invalid transformer.\")\n\n    with pytest.raises(ValueError, match=\"This is an invalid transformer.\"):\n        ct = ColumnTransformer(\n            [\n                (\"invalid\", InvalidTransformer(), [\"a\"]),\n            ]\n        )\n        ct.fit(pd.DataFrame({\"a\": [1, 2]}))\n\n\n\n"], "sample_912": ["    def test_pyattribute_options(app):\n        text = (\".. py:class:: Class\\n\"\n                \"\\n\"\n                \"   .. py:attribute:: attr\\n\"\n                \"      :type: int\\n\"\n                \"      :default: 42\\n\"\n                \"      :readonly: True\\n\")\n        domain = app.env.get_domain('py')\n        doctree = restructuredtext.parse(app, text)\n        assert_node(doctree, (addnodes.index,\n                              [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"Class\"])],\n                                                  [desc_content, (addnodes.index,\n                                                                 desc)])]))\n        assert_node(doctree[1][1][0], addnodes.index,\n                    entries=[('single', 'attr (Class attribute)', 'class.attr', '', None)])\n        assert_node(doctree[1][1][1], ([desc_signature, ([desc_name, \"attr\"],\n                                                        [desc_annotation, \": int\"],\n                                                        [desc_annotation, \" = 42\"],\n                                                        [desc_annotation, \" :readonly\"])],\n                                       [desc_content, ()]))\n        assert 'Class.attr' in domain.objects\n        assert domain.objects['Class.attr'] == ('index', 'class.attr', 'attribute')\n\n\n\n", "def test_pyfunction_signature_with_args(app):\n    text = \".. py:function:: func1(a: int, *b: str, **c: float)\\n\"\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"function \"],\n                                                    [desc_name, \"func1\"],\n                                                    [desc_parameterlist, ([desc_parameter, desc_sig_name, \"a\"],\n                                                                     [desc_sig_punctuation, \":\"],\n                                                                     [desc_sig_name, \"int\"]),\n                                                                    ([desc_parameter, desc_sig_operator, \"*\"],\n                                                                     [desc_sig_name, \"b\"],\n                                                                     [desc_sig_punctuation, \":\"],\n                                                                     [desc_sig_name, \"str\"]),\n                                                                    ([desc_parameter, desc_sig_operator, \"**\"],\n                                                                     [desc_sig_name, \"c\"],\n                                                                     [desc_sig_punctuation, \":\"],\n                                                                     [desc_sig_name, \"float\"])),\n                                                    ]),\n                                  desc_content)]))\n    assert 'func1' in domain.objects\n    assert domain.objects['func1'] == ('index', 'func1', 'function')\n\n\n\n\n", "def test_module_index_duplicates(app):\n    text = \".. py:module:: docutils\\n\"\n    restructuredtext.parse(app, text)\n    index = PythonModuleIndex(app.env.get_domain('py'))\n    assert index.generate() == (\n        [('d', [IndexEntry('docutils', 0, 'index', 'module-docutils', '', '', '')])],\n        False\n    )\n", "def test_module_index_inheritance(app):\n    text = (\".. py:module:: Base\\n\"\n            \".. py:class:: Base\\n\"\n            \"   .. py:method:: Base.method\\n\"\n            \".. py:module:: Derived\\n\"\n            \"   .. py:class:: Derived\\n\"\n            \"      .. py:method:: Derived.method\\n\")\n    restructuredtext.parse(app, text)\n    index = PythonModuleIndex(app.env.get_domain('py'))\n    assert index.generate() == (\n        [('B', [IndexEntry('Base', 0, 'module', 'module-Base', '', '', ''),  # NOQA\n                IndexEntry('Base', 1, 'class', 'class-Base', '', '', ''),  # NOQA\n                IndexEntry('Base.method', 2, 'method', 'method-Base.method', '', '', '')]),\n         ('D', [IndexEntry('Derived', 0, 'module', 'module-Derived', '', '', ''),  # NOQA\n                IndexEntry('Derived', 1, 'class', 'class-Derived', '', '', ''),  # NOQA\n                IndexEntry('Derived.method', 2, 'method', 'method-Derived.method', '', '', '')])],  # NOQA\n        True\n    )\n", "def test_pymethod_special_cases(app):\n    text = (\".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   .. py:method:: meth\\n\"\n            \"      :raises: ValueError\\n\"\n            \"      :complexity: high\\n\"\n            \"   .. py:method:: meth2\\n\"\n            \"      :deprecated::\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"Class\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc)])]))\n    assert_node(doctree[1][1][0], addnodes.index,\n                entries=[('single', 'meth() (Class method)', 'class.meth', '', None)])\n    assert_node(doctree[1][1][1], ([desc_signature, ([desc_name, \"meth\"],\n                [desc_parameterlist, ()]),\n                [desc_annotation, \": raises ValueError\"],\n                [desc_annotation, \": complexity: high\"]],\n                [desc_content, ()]))\n    assert_node(doctree[1][1][2], addnodes.index,\n                entries=[('single', 'meth2() (Class method)', 'class.meth2', '', None)])\n    assert_node(doctree[1][1][3], ([desc_signature, ([desc_name, \"meth2\"],\n                [desc_parameterlist, ()]),\n                [desc_annotation, \": deprecated::\"]],\n                [desc_content, ()])) \n"], "sample_178": ["    def test_formset_is_valid_with_extra_form(self):\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '0',\n            'choices-1-choice': 'One',\n            'choices-1-votes': '1',\n        }\n        ChoiceFormSet = formset_factory(Choice, extra=1)\n        formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertTrue(formset.is_valid())\n        self.assertEqual(formset.initial_form_count(), 0)\n        self.assertEqual(formset.total_form_count(), 2)\n\n\n\n", "    def test_all_valid_with_non_form_errors(self):\n        data = {\n            'choices-TOTAL_FORMS': '2',  # the number of forms rendered\n            'choices-INITIAL_FORMS': '0',  # the number of forms with initial data\n            'choices-MAX_NUM_FORMS': '2',  # max number of forms - should be ignored\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '0',\n            'choices-1-choice': 'One',\n            'choices-1-votes': '1',\n        }\n        ChoiceFormSet = formset_factory(Choice, extra=1, max_num=1, validate_max=True)\n        formset1 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        formset2 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertIs(all_valid((formset1, formset2)), False)\n        expected_errors = [{}, {'votes': ['This field is required.']}]\n        self.assertEqual(formset1._errors, expected_errors)\n        self.assertEqual(formset2._errors, expected_errors)\n", "    def test_validate_max_form_count(self):\n        data = {\n            'choices-TOTAL_FORMS': '5',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-MAX_NUM_FORMS': '3',\n            'choices-0-choice': 'One',\n            'choices-0-votes': '1',\n            'choices-1-choice': 'Two',\n            'choices-1-votes': '2',\n            'choices-2-choice': 'Three',\n            'choices-2-votes': '3',\n            'choices-3-choice': 'Four',\n            'choices-3-votes': '4',\n            'choices-4-choice': 'Five',\n            'choices-4-votes': '5',\n        }\n        ChoiceFormSet = formset_factory(Choice, max_num=3, validate_max=True)\n        formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertFalse(formset.is_valid())\n        self.assertEqual(formset.errors['__all__'], ['You may only specify 3 forms.'])\n", "    def test_all_valid_with_non_form_errors(self):\n        data = {\n            'choices-TOTAL_FORMS': '2',  # the number of forms rendered\n            'choices-INITIAL_FORMS': '0',  # the number of forms with initial data\n            'choices-MIN_NUM_FORMS': '0',  # min number of forms\n            'choices-MAX_NUM_FORMS': '2',  # max number of forms - should be ignored\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '0',\n            'choices-1-choice': 'One',\n            'choices-1-votes': '1',\n        }\n        ChoiceFormSet = formset_factory(Choice, extra=1, max_num=1, validate_max=True)\n        formset1 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        formset2 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertIs(all_valid((formset1, formset2)), False)\n        expected_errors = [{'votes': ['You may only specify a choice once.']}, {'votes': ['You may only specify a choice once.']}]\n        self.assertEqual(formset1._errors, expected_errors)\n        self.assertEqual(formset2._errors, expected_errors)    \n", "    def test_total_error_count(self):\n        formset_data = [\n            ([('Calexico', '100')], 0),\n            ([('Calexico', '')], 1),\n            ([('', 'invalid')], 2),\n            ([('Calexico', '100'), ('Calexico', '')], 1),\n            ([('Calexico', ''), ('Calexico', '')], 2),\n        ]\n        for formset_data, expected_error_count in formset_data:\n            formset = self.make_choiceformset(formset_data)\n            self.assertEqual(formset.total_error_count(), expected_error_count)\n\n"], "sample_473": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_577": ["    def test_default_object_behavior(self):\n\n        p = Plot(x=\"x\", y=\"y\").add(MockMark())\n        p = p.set_defaults(x=\"a\", y=\"b\").plot()\n\n        assert p._x == \"a\"\n        assert p._y == \"b\"\n\n\n", "    def test_default_with_order(self):\n\n        p = Plot().facet(col=[\"a\", \"b\"], order=[\"b\", \"a\"]).plot()\n        assert p._subplots[1][\"ax\"].get_title() == \"b\"\n\n\n", "    def test_default_is_comparable(self):\n\n        a = Default()\n        b = Default()\n        assert a == b\n        assert a != \"some string\"\n        assert a != 123\n        assert a != [1, 2]\n\n\n\n", "    def test_default_object_usage(self):\n\n        plot = Plot().pair(x=[\"a\", \"b\"], y=[\"c\", \"d\"]).plot()\n        assert plot.x_scales[0].label == \"Default\"\n        assert plot.y_scales[1].label == \"Default\"\n", "    def test_wrap_no_args(self):\n\n        p = Plot(long_df)\n        p.pair(x=[\"a\"])\n        assert p._subplots[0][\"ax\"].get_xlabel() == \"a\"\n\n        p.wrap()\n        assert len(p._subplots) == 1\n        assert p._subplots[0][\"ax\"].get_xlabel() == \"a\"\n"], "sample_599": ["def test_UnsignedIntegerCoder_encode_decode_with_fill_value():\n    original = xr.Variable((\"x\",), np.array([1, 2, 3], dtype=np.uint16), {\"_FillValue\": 0})\n    coder = variables.UnsignedIntegerCoder()\n    roundtripped = coder.decode(coder.encode(original))\n    assert_identical(roundtripped, original)\n", "def test_UnsignedIntegerCoder_roundtrip(dtype):\n    original = xr.Variable((\"x\",), np.ones(5, dtype=dtype).astype(dtype), {\"_Unsigned\": \"true\"})\n    coder = variables.UnsignedIntegerCoder()\n    roundtripped = coder.decode(coder.encode(original))\n    assert_identical(original, roundtripped) \n", "def test_CFScaleOffsetCoder_missing_offset():\n    original = xr.Variable((\"x\",), np.arange(10.0), encoding=dict(scale_factor=5))\n    coder = variables.CFScaleOffsetCoder()\n    encoded = coder.encode(original)\n    roundtripped = coder.decode(encoded)\n    assert_allclose(roundtripped, original * 5)\n", "def test_CFScaleOffsetCoder_encode_decode_without_attrs():\n    original = xr.Variable((\"x\",), data, encoding=dict())\n    coder = variables.CFScaleOffsetCoder()\n    encoded = coder.encode(original)\n    roundtripped = coder.decode(encoded)\n    assert_identical(original, roundtripped)\n", "def test_CFScaleOffsetCoder_decode_large_offset():\n    # Test with a large offset to make sure it doesn't overflow\n    data = np.arange(10)\n    encoding = {\"add_offset\": 1e10}\n    original = xr.Variable((\"x\",), data, encoding=encoding)\n    coder = variables.CFScaleOffsetCoder()\n    encoded = coder.encode(original)\n    roundtripped = coder.decode(encoded)\n    assert_allclose(roundtripped.data, data + encoding[\"add_offset\"])\n"], "sample_321": ["    def test_csrf_token_on_500_stays_constant(self):\n        response = self.client.get('/raises_error/')\n        # The error handler returns status code 599.\n        self.assertEqual(response.status_code, 599)\n        token1 = response.content\n        response = self.client.get('/raises_error/')\n        self.assertEqual(response.status_code, 599)\n        token2 = response.content\n        self.assertTrue(equivalent_tokens(token1.decode('ascii'), token2.decode('ascii')))\n", "    def test_csrf_token_on_500_stays_constant(self):\n        response = self.client.get('/raises_exception/')\n        self.assertEqual(response.status_code, 500)\n        token1 = response.content\n        response = self.client.get('/raises_exception/')\n        self.assertEqual(response.status_code, 500)\n        token2 = response.content\n        self.assertTrue(equivalent_tokens(token1.decode('ascii'), token2.decode('ascii')))\n", "    def test_csrf_token_error_handler_not_empty_response(self):\n        response = self.client.get('/csrf_error/')\n        self.assertNotEqual(response.content, b'')\n", "    def test_csrf_token_on_404_persists_across_requests(self):\n        response = self.client.get('/does not exist/')\n        self.assertEqual(response.status_code, 599)\n        token1 = response.content\n\n        # Simulate a subsequent request to the same non-existent page\n        response = self.client.get('/does not exist/')\n        self.assertEqual(response.status_code, 599)\n        token2 = response.content\n\n        self.assertEqual(token1, token2)\n", "    def test_csrf_token_on_500_stays_constant(self):\n        from django.template import TemplateSyntaxError\n        # Trigger a 500 error\n        with self.assertRaises(TemplateSyntaxError):\n            self.client.get('/raises_template_error/')\n        response = self.client.get('/raises_template_error/')\n        self.assertEqual(response.status_code, 500)\n        token1 = response.content\n        response = self.client.get('/raises_template_error/')\n        self.assertEqual(response.status_code, 500)\n        token2 = response.content\n        self.assertTrue(equivalent_tokens(token1.decode('ascii'), token2.decode('ascii')))\n"], "sample_552": ["def test_savefig_bbox_inches():\n    fig = plt.figure()\n    fig.subplots_adjust(left=0.2, right=0.8, top=0.9, bottom=0.1)\n    ax = fig.add_subplot(111)\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n\n    with io.BytesIO() as buffer:\n        fig.savefig(buffer, bbox_inches='tight')\n        buffer.seek(0)\n        image = plt.imread(buffer)\n\n    assert image.shape[0] == int(fig.bbox.height)\n\n\n", "def test_subplot_mosaic_non_string_list():\n    with pytest.raises(TypeError, match=\".*must be a string.*\"):\n        fig = plt.figure()\n        fig.subplot_mosaic([[1, 2], [3, 4]])\n\n\n\n", "    def test_savefig_no_metadata(fmt):\n        with pytest.warns(UserWarning, match=\"metadata keyword argument is ignored\"):\n            Figure().savefig(io.BytesIO(), format=fmt, metadata={'key': 'value'})\n", "def test_gridspec_kw_not_mutated(gridspec_kw):\n    gs = {'left': .1}\n    gs_orig = dict(gs)\n    plt.subplots(1, 2, gridspec_kw=gs, **gridspec_kw)\n    assert gs == gs_orig\n", "def test_savefig_bbox_inches_tight():\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.set_xlim(-1, 1)\n    ax.set_ylim(-1, 1)\n    ax.plot([-0.5, 0.5], [0, 0])\n    bbox = fig.savefig(io.BytesIO(), bbox_inches='tight')\n    assert bbox is not None\n"], "sample_869": ["compilation error", "def test_accuracy_score_multiclass_with_unknown_labels():\n    y_true = np.array([0, 1, 2, 3, 4])\n    y_pred = np.array([0, 1, 2, 2, 1])\n\n    error_message = r'y_true contains classes not present in y_pred: \\[3, 4\\]'\n    with pytest.raises(ValueError, match=re.escape(error_message)):\n        accuracy_score(y_true, y_pred)\n", "def test_multilabel_jaccard_similarity_score_with_sparse_matrices():\n    y1 = sparse.csc_matrix([[0, 1, 1], [1, 0, 1]])\n    y2 = sparse.csc_matrix([[0, 0, 1], [1, 0, 1]])\n\n    # size(y1 \\inter y2) = [1, 2]\n    # size(y1 \\union y2) = [2, 2]\n\n    assert_almost_equal(jaccard_similarity_score(y1, y2), 0.75)\n    assert_almost_equal(jaccard_similarity_score(y1, y1), 1)\n    assert_almost_equal(jaccard_similarity_score(y2, y2), 1)\n    assert_almost_equal(jaccard_similarity_score(y2, np.logical_not(y2)), 0)\n    assert_almost_equal(jaccard_similarity_score(y1, np.logical_not(y1)), 0)\n    assert_almost_equal(jaccard_similarity_score(y1, np.zeros(y1.shape)), 0)\n    assert_almost_equal(jaccard_similarity_score(y2, np.zeros(y1.shape)), 0)\n", "def test_balanced_accuracy_score_single_class():\n    for target in [0, 1, 2]:\n        y_true = [target] * 10\n        y_pred = [target] * 10\n        assert balanced_accuracy_score(y_true, y_pred) == 1.0\n        \n        y_pred = [1 - target] * 10\n        assert balanced_accuracy_score(y_true, y_pred) == 0.0\n                \n        y_pred = [target if i % 2 == 0 else 1 - target for i in range(10)]\n        assert balanced_accuracy_score(y_true, y_pred) == 0.5\n", "def test_brier_score_loss_multiclass():\n    y_true = np.array([0, 1, 2, 0])\n    y_pred = np.array([0.8, 0.6, 0.4, 0.2])\n    error_message = (\"Only binary classification is supported. Labels \"\n                     \"in y_true: {}\".format(np.array([0, 1, 2])))\n    with pytest.raises(ValueError, match=re.escape(error_message)):\n        brier_score_loss(y_true, y_pred)\n\n\n\n"], "sample_566": ["def test_save_to_file_metadata():\n    fig = plt.figure()\n    metadata = {'author': 'John Doe', 'title': 'My Figure'}\n    fig.savefig('test_metadata.png', metadata=metadata)\n    loaded_fig = plt.imread('test_metadata.png')\n    assert loaded_fig.metadata == metadata\n    os.remove('test_metadata.png')\n\n\n\n", "def test_constrained_layout_with_shared_axes():\n    fig = plt.figure(constrained_layout=True)\n    gs = fig.add_gridspec(2, 2)\n    ax1 = fig.add_subplot(gs[0, 0])\n    ax2 = fig.add_subplot(gs[0, 1], sharey=ax1)\n    ax3 = fig.add_subplot(gs[1, 0], sharex=ax1)\n\n    assert ax1.get_yticks() != ax2.get_yticks()\n    assert ax1.get_xticks() != ax3.get_xticks()\n\n\n\n", "def test_empty_subplot_mosaic():\n    with pytest.raises(ValueError, match=\"must be at least 1 sub\"):\n        plt.subplot_mosaic(\"\")\n    with pytest.raises(ValueError, match=\"must be at least 1 sub\"):\n        plt.subplot_mosaic([])\n\n", "def test_gridspec_add_subplot_kwargs():\n    fig = plt.figure()\n    gs = fig.add_gridspec(2, 2)\n    for i in range(4):\n        ax = fig.add_subplot(gs[i // 2, i % 2], projection='polar')\n        assert ax.get_projection() == 'polar'\n", "def test_constrained_layout_setup():\n    fig = plt.figure(constrained_layout=True)\n    gs = fig.add_gridspec(1, 2)\n    ax1 = fig.add_subplot(gs[0, 0])\n    ax2 = fig.add_subplot(gs[0, 1])\n\n    with pytest.warns(PendingDeprecationWarning, match=\"will be deprecated\"):\n        assert fig.get_constrained_layout().get_subplot_geometry(ax1)\n        assert fig.get_constrained_layout().get_subplot_geometry(ax2)\n"], "sample_1186": ["compilation error", "def test_array_ndim():\n    for ArrayType in array_types:\n        a = ArrayType([1, 2, 3])\n        assert a.rank() == 1\n        b = ArrayType([[1, 2], [3, 4]])\n        assert b.rank() == 2\n        c = ArrayType([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n        assert c.rank() == 3\n        d = ArrayType({})\n        assert d.rank() == 0 \n", "def test_array_indexing_with_symbolic_indices():\n    for ArrayType in array_types:\n        A = ArrayType([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n        x = Symbol('x')\n        y = Symbol('y')\n        index = (x, y)\n        try:\n            A[index]\n        except ValueError:\n            continue\n        assert A[index] == 1  \n\n\n\n", "def test_array_shape():\n    for ArrayType in array_types:\n        a = ArrayType([1, 2, 3, 4, 5], shape=(5,))\n        assert a.shape == (5,)\n\n        b = ArrayType([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], shape=(3, 3, 1))\n        assert b.shape == (3, 3, 1)\n\n\n", "def test_array_index_bounds():\n    for ArrayType in array_types:\n        test_array = ArrayType([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n        assert test_array[0, 0] == 1\n        assert test_array[1, 1] == 5\n        assert test_array[2, 2] == 9\n        raises(IndexError, lambda: test_array[3, 0])\n        raises(IndexError, lambda: test_array[0, 3])\n"], "sample_517": ["def test_font_size_clipping(font_size):\n    fig = plt.figure()\n    ax = fig.add_subplot()\n    text = ax.text(0.5, 0.5, \"This is a test\", fontsize=font_size)\n    fig.canvas.draw()\n    bbox = text.get_window_extent(renderer=fig.canvas.get_renderer())\n    # Check that the text doesn't exceed the figure bounds\n    assert bbox.x0 > 0 and bbox.x1 < fig.canvas.get_width_height()[0]\n    assert bbox.y0 > 0 and bbox.y1 < fig.canvas.get_width_height()[1]  \n", "def test_font_size_units():\n    fig, ax = plt.subplots()\n        text = ax.text(0.5, 0.5, 'A', fontsize=size)\n        fig.canvas.draw()\n        bbox = text.get_window_extent(renderer=fig.canvas.get_renderer())\n        assert_almost_equal(bbox.width, expected_size)\n\n    check_size(10, 10)\n    check_size('10pt', 10)\n    check_size(10, 10)\n    check_size('10pt', 10)\n    check_size(10, 10)\n\n\n\n", "def test_bbox_padding():\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, 'Hello')\n    bbox = text.get_bbox()\n    assert bbox.width > text.get_window_extent(fig.canvas.renderer).width\n    assert bbox.height > text.get_window_extent(fig.canvas.renderer).height\n\n\n\n", "compilation error", "def test_text_background_color():\n    fig, ax = plt.subplots()\n    fig.set_facecolor('lightgrey')\n    text = ax.text(0.5, 0.5, 'test', color='white',\n                   backgroundcolor='lightblue')\n    fig.canvas.draw()\n    assert text.get_bbox_patch().get_facecolor() == 'lightblue'\n"], "sample_43": ["def test_events_fitness_validation():\n    rng = np.random.RandomState(42)\n    t = rng.rand(100)\n    x = np.random.randint(0, 2, size=100)\n\n    with pytest.raises(ValueError):\n        bayesian_blocks(t, x, fitness='events')\n\n\n\n", "def test_fitness_function_validation_errors():\n    rng = np.random.RandomState(42)\n    t = rng.randn(100)\n\n    # Events: x must be integer counts\n    with pytest.raises(ValueError):\n        bayesian_blocks(t, fitness='events', x=t) \n\n    # Regular Events: x must be binary\n    x = rng.rand(100)\n    with pytest.raises(ValueError):\n        bayesian_blocks(t, fitness='regular_events', x=x, dt=0.1)  \n  \n    # Measures: x must be provided\n    with pytest.raises(ValueError):\n        bayesian_blocks(t, fitness='measures')\n\n    # Measures: sigma must have matching shape with x\n    x = rng.randn(100)\n    sigma = rng.randn(99)\n    with pytest.raises(ValueError):\n        bayesian_blocks(t, x, sigma, fitness='measures')\n", "def test_large_scale_data():\n    rng = np.random.RandomState(42)\n    n_samples = 10000\n    t = rng.rand(n_samples)\n    x = np.exp(-0.5 * (t - 0.5) ** 2) + rng.randn(n_samples) * 0.05\n    sigma = 0.1\n\n    bins = bayesian_blocks(t, x, sigma, fitness='measures')\n\n    assert len(bins) > 10\n", "compilation error", "def test_event_data_repeated_bin_times():\n    rng = np.random.RandomState(42)\n    t = rng.rand(100)\n    t[80:] = t[:20]\n    t = np.concatenate((t, t[:20]))\n\n    # Repeated bin times should not throw an error\n    edges = bayesian_blocks(t, fitness='events')\n    assert_allclose(edges, [-2.6197451, -0.71094865, 0.36866702, 1.85227818, 0]) \n\n"], "sample_311": ["    def test_final_catch_all_view_callable(self):\n        class CallableCatchingView(view.APIView):\n                return HttpResponse('Callable Catch-all View')\n\n        # Replace the default view with our callable\n        admin_site = AdminSite()\n        admin_site.final_catch_all_view = CallableCatchingView.as_view()\n\n        response = self.client.get('/test_admin/admin10/unknown/')\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.content, b'Callable Catch-all View')\n", "    def test_unknown_url_with_final_catch_all_view_disabled_redirects_login_if_not_authenticated(self):\n        \"\"\"\n        Verifies that if final_catch_all_view is disabled,\n        unknown URLs still redirect to login for non-staff.\n        \"\"\"\n        with override_settings(\n                APPEND_SLASH=True,\n                ADMIN_SITE='admin_views.admin_site'\n        ):\n            admin_site = admin_site = admin.sites.AdminSite(name='admin_site')\n            admin_site.final_catch_all_view = None\n            unknown_url = '/test_admin/admin/unknown/'\n            response = self.client.get(unknown_url)\n            self.assertRedirects(response, '%s?next=%s' % (reverse('admin:login'), unknown_url))\n\n", "    def test_anonymous_access_403(self):\n        url = reverse('admin:admin_views_article_changelist')\n        response = self.client.get(url)\n        self.assertRedirects(response, reverse('admin:login'))\n\n\n", "    def test_known_url_404_if_not_authenticated_without_final_catch_all_view(self):\n        known_url = reverse('admin10:admin_views_article_changelist')\n        response = self.client.get(known_url)\n        self.assertEqual(response.status_code, 404)\n\n\n", "    def test_non_admin_url_404_if_authenticated(self):\n        unknown_url = '/unknown/'\n        superuser = User.objects.create_superuser(\n            username='super',\n            password='secret',\n            email='super@example.com',\n        )\n        self.client.force_login(superuser)\n        response = self.client.get(unknown_url)\n        self.assertEqual(response.status_code, 404)\n\n"], "sample_749": ["def test_column_transformer_sparse_matrix_trans_with_remainder():\n    X_array = sparse.csc_matrix(\n        [[0, 1, 2], [2, 4, 6], [8, 6, 4]])\n\n    ct = ColumnTransformer([('trans1', SparseMatrixTrans(), [0])],\n                           remainder=SparseMatrixTrans())\n\n    X_trans = ct.fit_transform(X_array)\n\n    assert sparse.issparse(X_trans)\n    assert X_trans.shape == (3, 6)\n    assert_array_equal(X_trans.toarray(), np.hstack(\n        (np.array([[0], [2], [8]]), \n         np.eye(3))))\n    assert len(ct.transformers_) == 2\n    assert ct.transformers_[-1][0] == 'remainder'\n    assert isinstance(ct.transformers_[-1][1], SparseMatrixTrans)\n    assert_array_equal(ct.transformers_[-1][2], [1, 2])\n\n\n\n", "def test_column_transformer_sparse_matrix_trans_with_fit_transform():\n    X = sparse.csr_matrix([\n        [1, 2, 0],\n        [0, 4, 3],\n        [5, 0, 6]\n    ])\n    ct = ColumnTransformer([\n        ('trans', SparseMatrixTrans(), [0])\n    ], remainder=StandardScaler())\n    X_trans = ct.fit_transform(X)\n    assert sparse.issparse(X_trans)\n\n    # check if the sparse matrix transformer is fit properly\n    assert X_trans.shape[1] == 3 + 1\n    assert_array_equal(X_trans.toarray()[:, 0], X[:, 0].toarray())\n    assert_array_equal(X_trans.toarray()[:, 1], X[:, 1].toarray())\n    assert_array_equal(X_trans.toarray()[:, 2], X[:, 2].toarray())\n\n    # check if the remainder transformer is fit properly\n    assert_array_equal(X_trans.toarray()[:, 3],\n                       StandardScaler().fit_transform(X.toarray()[:, 0]))\n\n\n\n", "def test_column_transformer_sparse_array_remainder_transformer():\n    X_sparse = sparse.csr_matrix([[0, 1, 2], [2, 4, 6], [8, 6, 4]])\n\n    ct = ColumnTransformer([('trans1', Trans(), [0])],\n                           remainder=SparseMatrixTrans())\n\n    X_trans = ct.fit_transform(X_sparse)\n    assert sparse.issparse(X_trans)\n\n    #  SparseMatrixTrans creates 3 features for each column, thus:\n    assert X_trans.shape == (3, 3 + 1)\n    assert_array_equal(X_trans.toarray(),\n                       np.hstack((X_sparse[:, 0].reshape(-1, 1),\n                                  np.eye(3))))\n    assert len(ct.transformers_) == 2\n    assert ct.transformers_[-1][0] == 'remainder'\n    assert isinstance(ct.transformers_[-1][1], SparseMatrixTrans)\n    assert_array_equal(ct.transformers_[-1][2], [1, 2])\n\n\n\n", "compilation error", "compilation error"], "sample_382": ["    def test_get_template_directories_no_django_templates(self):\n        self.assertSetEqual(\n            autoreload.get_template_directories(),\n            set()\n        )\n", "    def test_get_template_directories_with_single_loader(self):\n        self.assertSetEqual(\n            autoreload.get_template_directories(),\n            {ROOT / 'templates'},\n        )\n", "    def test_get_template_directories_inbuilt(self):\n        self.assertSetEqual(\n            autoreload.get_template_directories(),\n            {ROOT}\n        )\n", "    def test_get_template_directories_no_defined_dirs(self):\n        self.assertSetEqual(\n            autoreload.get_template_directories(),\n            set(),\n        )\n\n", "    def test_get_template_dirs_app_and_filesystem(self):\n        self.assertSetEqual(\n            autoreload.get_template_directories(),\n            {\n                Path(__file__).parent / 'templates_extra',\n                Path(__file__).parent / 'templates',\n            }\n        )\n"], "sample_1193": ["compilation error", "compilation error", "    def test_idiff_limits():\n        x = Symbol('x')\n        y = Symbol('y')\n        d = idiff(x**2 + y**2 - 1, y, x)\n        assert limit(d, (x, 1, 1)) == 0\n        assert limit(d, (x, 1, -1)) == 0\n        assert limit(d, (y, 1, 1)) == 0\n        assert limit(d, (y, 1, -1)) == 0\n", "compilation error", "compilation error"], "sample_1065": ["def test_subfactorial_rewrite():\n    x = Symbol('x')\n    assert subfactorial(x).rewrite(uppergamma) == uppergamma(x + 1, -1)/S.Exp1\n    assert subfactorial(x).rewrite(factorial) == (-1)**x*polygamma(0, x + 1)\n    assert subfactorial(x).rewrite(special.digamma) == (-1)**x*(-digamma(x + 1) + 1)\n", "def test_subfactorial_rewrite():\n    x = Symbol('x')\n    assert subfactorial(x).rewrite(factorial) == (factorial(x) - sum(\n        binomial(x, k) * factorial(k) for k in range(1, x))) / factorial(x)\n    assert subfactorial(x).rewrite(uppergamma) == uppergamma(x + 1, -1)/S.Exp1\n    assert subfactorial(x).rewrite(gamma) == gamma(x + 1)/gamma(x + 1)\n\n\n", "compilation error", "def test_subfactorial_mod():\n    p, q = 10**5 + 3, 10**9 + 33 # prime modulo\n    r, s = 10**7 + 5, 33333333 # composite modulo\n    assert Mod(subfactorial(1234), q) == Mod(subfactorial(1234), q)\n    assert Mod(subfactorial(-178911), p) == Mod(subfactorial(-178911), p)\n    assert Mod(subfactorial(9734), r) == Mod(subfactorial(9734), r)\n    assert Mod(subfactorial(-10733), q) == Mod(subfactorial(-10733), q) \n    assert Mod(subfactorial(753), r) == Mod(subfactorial(753), r)\n\n\n\n\n", "compilation error"], "sample_418": ["    def test_length_is12(self):\n        output = self.engine.render_to_string(\"length_is12\", {})\n        self.assertEqual(output, \"\")\n", "    def test_length_is12(self):\n        output = self.engine.render_to_string(\"length_is12\", {})\n        self.assertEqual(output, \"Two\")\n", "    def test_length_is12(self):\n        output = self.engine.render_to_string(\"length_is12\", {})\n        self.assertEqual(output, \"Five\")\n", "    def test_length_is12(self):\n        output = self.engine.render_to_string(\"length_is12\", {\"var\": \"12\"})\n        self.assertEqual(output, \"True\")\n\n\n\n", "    def test_length_is12(self):\n        output = self.engine.render_to_string(\"length_is12\", {\"var\": \"\"})\n        self.assertEqual(output, \"Length is 0\")\n"], "sample_312": ["    def test_add_squash_children(self):\n        node1 = Node([('a', 1)])\n        node2 = Node([('b', 2), ('c', 3)])\n        combined = node1.add(node2,  Node.default, squash=True)\n        self.assertEqual(combined, node2)\n        self.assertEqual(str(node1), \"(DEFAULT: ('a', 1), ('b', 2), ('c', 3))\")\n\n", "    def test_add_squash(self):\n        node1 = Node([('a', 1), ('b', 2)], conn_type='AND')\n        node2 = Node([('c', 3), ('d', 4)], conn_type='AND')\n        result = node1.add(node2, 'AND', squash=True)\n        self.assertEqual(str(result), \"(AND: ('a', 1), ('b', 2), ('c', 3), ('d', 4))\")\n        self.assertEqual(node1.children, [result])\n", "    def test_add_squash_single_connector(self):\n        node1 = Node([('a', 1)], 'AND')\n        node2 = Node([('b', 2)], 'AND')\n        node3 = node1.add(node2, 'AND', squash=True)\n        self.assertEqual(len(node3.children), 2)\n        self.assertEqual(str(node3), \"(AND: ('a', 1), ('b', 2))\") \n", "    def test_add_squash(self):\n        node1 = Node([('a', 1), ('b', 2)], 'OR')\n        node2 = Node([('c', 3)])\n        child = node1.add(node2, 'OR')\n        self.assertEqual(child, node2)\n        self.assertEqual(str(node1), \"(OR: ('a', 1), ('b', 2), ('c', 3))\")\n        \n        # Check behavior when squashing with different connectors\n        node3 = Node([('d', 4), ('e', 5)], 'AND')\n        node4 = Node([('f', 6)])\n        result = node3.add(node4, 'OR')\n        self.assertEqual(result, node4)\n        self.assertEqual(str(node3), \"(AND: ('d', 4), ('e', 5), ('f', 6))\")\n\n", "    def test_add_squash(self):\n        node1 = Node([('a', 1), ('b', 2)], connector='AND')\n        node2 = Node([('c', 3)], connector='AND')\n        result = node1.add(node2, connector='AND', squash=True)\n        self.assertEqual(len(node1.children), 1)\n        self.assertEqual(str(node1), \"(AND: (AND: ('a', 1), ('b', 2), ('c', 3)))\")\n        self.assertIs(result, node2) \n"], "sample_789": ["def test_adaboost_with_non_integer_n_estimators():\n    # Check that n_estimators accepts floats. \n    # This is useful for algorithms that benefit from iterative improvement.\n    for alg in ['SAMME', 'SAMME.R']:\n        for n_estimators in [1.5, 2.7]:\n            clf = AdaBoostClassifier(algorithm=alg, n_estimators=n_estimators)\n            clf.fit(X, y_class)\n            assert_equal(len(clf.estimators_), int(n_estimators))\n\n\n\n", "compilation error", "def test_sample_weights_classification():\n    # Check classification with sample weights.\n    X, y = datasets.make_classification(n_samples=200,\n                n_features=10,\n                n_informative=3,\n                n_redundant=0,\n                n_repeated=0,\n                shuffle=False,\n                random_state=42)\n    sample_weight = np.random.rand(len(X))\n    clf = AdaBoostClassifier()\n    clf.fit(X, y, sample_weight=sample_weight)\n    assert_array_almost_equal(clf.estimator_weights_, sample_weight / \\\n        sample_weight.sum())\n", "def test_sample_weight_classification():\n    # Check that sample_weight works as expected in classification.\n    for alg in ['SAMME', 'SAMME.R']:\n        X, y = datasets.make_classification(n_samples=100,\n                                            n_features=5,\n                                            random_state=0)\n\n        sample_weight = np.ones(len(y))\n        sample_weight[0:50] *= 2\n        clf = AdaBoostClassifier(algorithm=alg)\n        clf.fit(X, y, sample_weight=sample_weight)\n        assert_array_almost_equal(clf.estimators_[0].sample_weight_,\n                                 sample_weight)\n\n\n", "compilation error"], "sample_46": ["    def test_delete_cascade(self):\n        u1 = PrimaryKeyUUIDModel.objects.create()\n        r1 = RelatedToUUIDModel.objects.create(uuid_fk=u1)\n        u1.delete()\n        with self.assertRaises(RelatedObjectDoesNotExist):\n            RelatedToUUIDModel.objects.get(id=r1.id)\n", "    def test_deletion(self):\n        u1 = PrimaryKeyUUIDModel.objects.create()\n        r = RelatedToUUIDModel.objects.create(uuid_fk=u1)\n        u1.delete()\n        with self.assertRaises(models.ProtectedError):\n            RelatedToUUIDModel.objects.get(id=r.id) \n", "    def test_uuid_pk_temporary_tables(self):\n        with self.subTest('creation'):\n            # Ensure a UUID pk can be created within a temporary table\n            class TemporaryModel(models.Model):\n                id = models.UUIDField(primary_key=True)\n\n            with connection.schema_editor() as editor:\n                editor.create_model(TemporaryModel)\n            with connection.cursor() as cursor:\n                cursor.execute('SELECT 1')  # Ensure temporary table exists\n            TemporaryModel.objects.create()\n            with connection.cursor() as cursor:\n                cursor.execute('SELECT 1')  # Ensure temporary table exists\n            TemporaryModel.objects.all().delete()\n            with connection.schema_editor() as editor:\n                editor.delete_model(TemporaryModel)\n\n        with self.subTest('creation_and_lookup'):\n            class TemporaryModel(models.Model):\n                id = models.UUIDField(primary_key=True)\n\n            with connection.schema_editor() as editor:\n                editor.create_model(TemporaryModel)\n            with connection.cursor() as cursor:\n                cursor.execute('SELECT 1')  # Ensure temporary table exists\n            obj = TemporaryModel.objects.create()\n            self.assertEqual(obj.id, obj.pk)\n            TemporaryModel.objects.all().delete()\n            with connection.schema_editor() as editor:\n                editor.delete_model(TemporaryModel)\n", "    def test_pk_uuid_in_queryset(self):\n        with self.assertRaises(TypeError):\n            PrimaryKeyUUIDModel.objects.filter(pk='not a uuid') \n\n        pk_model = PrimaryKeyUUIDModel.objects.create()\n        self.assertEqual(PrimaryKeyUUIDModel.objects.filter(pk=pk_model.pk).first(), pk_model) \n", "    def test_uuid_pk_with_transaction(self):\n        with self.assertRaises(IntegrityError):\n            with self.assertNumQueries(2):\n                u1 = PrimaryKeyUUIDModel()\n                u1.save(save_point=True)\n                u2 = PrimaryKeyUUIDModel(id=u1.id)\n                u2.save()\n"], "sample_935": ["compilation error", "compilation error", "def test_autolink_noindexentry(app):\n    text = (\".. cpp:function:: void f()\\n\"\n            \"   :noindexentry:\\n\"\n            \".. cpp:autofunc:: void g()\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index, desc, addnodes.index, desc, addnodes.autolink))\n    assert_node(doctree[0], addnodes.index, entries=[('single', 'f (C++ function)', '_CPPv41fv', '', None)])\n    assert_node(doctree[2], addnodes.index, entries=[])\n", "compilation error", "compilation error"], "sample_760": ["def test_cluster_scorer_memmap_input(name):\n    scorer, estimator = SCORERS[name], ESTIMATORS[name]\n    score = scorer(estimator, X_mm, y_mm)\n    assert isinstance(score, numbers.Number), scorer_name\n", "def test_scorer_sample_weight_with_multiclass_classifiers():\n    X, y = make_classification(random_state=0, n_classes=5)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n    sample_weight = np.ones_like(y_test)\n    sample_weight[:10] = 0\n\n    estimator = _make_estimators(X_train, y_train, None)[\n        'accuracy']  # Use an estimator supporting sample_weights\n\n    for scorer_name in SCORERS:\n        if scorer_name == 'accuracy':\n            score1 = get_scorer(scorer_name)(estimator, X_test, y_test,\n                                             sample_weight=sample_weight)\n            score2 = get_scorer(scorer_name)(estimator, X_test[10:],\n                                             y_test[10:],\n                                             sample_weight=sample_weight[10:])\n            assert_almost_equal(score1, score2)\n        \n        elif scorer_name in MULTILABEL_ONLY_SCORERS:\n            continue\n        else:\n            with pytest.raises(TypeError):\n                get_scorer(scorer_name)(estimator, X_test, y_test,\n                                         sample_weight=sample_weight)\n\n\n\n", "def test_scoring_with_estimator_without_score(scorer_name):\n    scorer = make_scorer(SCORERS[scorer_name])\n    estimator = EstimatorWithFit()\n    with pytest.raises(TypeError) as e:\n        scorer(estimator, [[1], [2], [3]], [1, 0, 0])\n    assert \"estimator should have a 'score' method\" in str(e)\n", "compilation error", "    def test_scoring_is_not_metric_multi_class(name):\n        estimator = DecisionTreeClassifier()\n        assert_raises_regexp(ValueError, 'make_scorer', check_scoring,\n                             estimator, f'{name}_samples')\n"], "sample_196": ["    def test_generated_columns_sql(self):\n        msg = 'subclasses of BaseDatabaseOperations must provide a %s() method' % 'generated_columns_sql'\n        with self.assertRaisesMessage(NotImplementedError, msg):\n            self.ops.generated_columns_sql(None)\n", "    def test_table_exists(self):\n        with transaction.atomic():\n            with self.assertRaises(\n                connection.OperationalError,\n                msg='Table does not exist.',\n            ):\n                connection.ops.table_exists('non_existent_table')\n        connection.operations.create_table('non_existent_table')\n        self.assertTrue(connection.ops.table_exists('non_existent_table'))\n        connection.operations.drop_table('non_existent_table')\n", "    def test_sql_flush_with_tablespace(self):\n        if connection.features.supports_tablespaces:\n            tablespace = 'TEST_TABLESPACE'\n            with transaction.atomic():\n                author = Author.objects.create(name='George Orwell', tablespace=tablespace)\n                Book.objects.create(author=author, tablespace=tablespace)\n                author = Author.objects.create(name='Harper Lee', tablespace=tablespace)\n                Book.objects.create(author=author, tablespace=tablespace)\n                Book.objects.create(author=author, tablespace=tablespace)\n                self.assertIs(Author.objects.exists(), True)\n                self.assertIs(Book.objects.exists(), True)\n\n            sql_list = connection.ops.sql_flush(\n                no_style(),\n                [Author._meta.db_table, Book._meta.db_table],\n                reset_sequences=True,\n                allow_cascade=True,\n                tablespace=tablespace,\n            )\n            connection.ops.execute_sql_flush(sql_list)\n\n            with transaction.atomic():\n                self.assertIs(Author.objects.exists(), False)\n                self.assertIs(Book.objects.exists(), False)\n", "    def test_sequence_reset_sql(self):\n        with transaction.atomic():\n            self.assertSequenceEqual(self.ops.sequence_reset_sql(no_style(), [Author._meta.db_table]), [])\n        with transaction.atomic():\n            Author.objects.create(name='George Orwell')\n            self.assertSequenceEqual(self.ops.sequence_reset_sql(no_style(), [Author._meta.db_table]), [])\n            Author.objects.create(name='Harper Lee')\n\n\n\n", "    def test_window_frame_range_start_end(self):\n        msg = 'This backend does not support window expressions.'\n        with self.assertRaisesMessage(NotSupportedError, msg):\n            self.ops.window_frame_range_start_end()\n\n\n"], "sample_90": ["    def test_custom_to_representation(self):\n        class MyModel(models.Model):\n            name = models.CharField(max_length=50)\n                return 'Custom String: ' + self.name\n\n        form = modelform_factory(MyModel, fields=['name'])\n        form_instance = form({'name': 'Testing'})\n        self.assertEqual(form_instance.to_representation(form_instance), 'Custom String: Testing')\n\n\n\n", "    def test_nested_object_to_dict(self):\n        nested = NestedObject.objects.create(value='nested')\n        parent = ParentObject.objects.create(nested_object=nested)\n        data = model_to_dict(parent)\n        self.assertEqual(data[\"nested_object\"], nested)\n        self.assertEqual(data[\"nested_object\"][\"value\"], \"nested\")\n", "    def test_related_manager_returns_object_instances(self):\n        \"\"\"\n        Related managers, when included in model_to_dict, should return\n        object instances, not QuerySets.\n\n        This is important for consistency and avoids accidental re-evaluation.\n        \"\"\"\n        item = ColourfulItem.objects.create()\n        blue = Colour.objects.create(name='blue')\n        item.colours = [blue]\n\n        data = model_to_dict(item)\n        self.assertIsInstance(data['colours'], list)\n        self.assertIsInstance(data['colours'][0], Colour)\n\n\n\n\n", "    def test_no_value_for_null_m2m(self):\n        \"\"\"Many-to-many fields with no values should result in empty lists, not None.\"\"\"\n        item = ColourfulItem.objects.create()\n        data = model_to_dict(item)\n        self.assertEqual(data.get('colours'), [])\n", "    def test_exclude_with_related_fields(self):\n        user_with_posts = User.objects.create(username='testuser', name='Test User')\n        post = Post.objects.create(user=user_with_posts, title='Test Post')\n        form_class = modelform_factory(User, fields=['username'] + ['id'], exclude=['email'])\n\n        data = {'username': user_with_posts.username}\n        form = form_class(data=data)\n\n        self.assertTrue(form.is_valid())\n        self.assertEqual(form.cleaned_data, {'username': user_with_posts.username, 'id': user_with_posts.id})\n\n\n"], "sample_152": ["    def test_fast_delete_related_m2m_through(self):\n        a = A.objects.create()\n        b = B.objects.create()\n        ab = AB.objects.create(a=a, b=b)\n        self.assertNumQueries(2, a.delete)\n        self.assertFalse(A.objects.filter(pk=a.pk).exists())\n        self.assertFalse(AB.objects.filter(a=a).exists())\n        self.assertTrue(B.objects.filter(pk=b.pk).exists())\n\n\n", "    def test_fast_delete_with_bulk_create(self):\n        # Ensure fast deletes work correctly when combined with bulk creation\n        users = [User(avatar=Avatar.objects.create()) for _ in range(10)]\n        try:\n            User.objects.bulk_create(users)\n        finally:\n            self.assertNumQueries(1, User.objects.all().delete)\n\n\n\n\n", "    def test_fast_delete_with_m2m_from_multiple_fields(self):\n        t1 = M2MTo.objects.create()\n        t2 = M2MTo.objects.create()\n        f = M2MFrom.objects.create()\n        f.m2m_field_one.add(t1)\n        f.m2m_field_two.add(t2)\n        # 1 to delete f, 1 to fast-delete m2m for f\n        self.assertNumQueries(2, f.delete)\n\n\n\n", "    def test_fast_delete_with_signals(self):\n        \"\"\"\n        Fast delete should still respect any signals that are connected\n        to pre_delete or post_delete.\n        \"\"\"\n        counter = 0\n            nonlocal counter\n            counter += 1\n\n        models.signals.pre_delete.connect(callback, sender=Referrer)\n        models.signals.post_delete.connect(callback, sender=Referrer)\n\n        origin = Origin.objects.create()\n        referer = Referrer.objects.create(origin=origin, unique_field=42)\n        referer.delete()\n\n        self.assertEqual(counter, 2)  # Both pre_delete and post_delete fired.\n\n        models.signals.pre_delete.disconnect(callback, sender=Referrer)\n        models.signals.post_delete.disconnect(callback, sender=Referrer)\n", "    def test_fast_delete_circular_relationship(self):\n        a = A.objects.create()\n        b = B.objects.create(a=a)\n        a.b = b\n        a.save()\n\n        with self.assertRaises(IntegrityError):\n            a.delete()\n"], "sample_235": ["    def test_concurrent_on_commit_hooks(self):\n            with transaction.atomic():\n                transaction.on_commit(lambda: self.notify(f\"thread-{id}\"))\n\n        threads = [threading.Thread(target=run_in_thread, args=(i,)) for i in range(3)]\n        for t in threads:\n            t.start()\n        for t in threads:\n            t.join()\n\n        self.assertNotified(['thread-0', 'thread-1', 'thread-2']) \n\n", "    def test_hooks_run_after_reconnect_with_autocommit_enabled(self):\n        with transaction.atomic():\n            self.do(1)\n            connection.close()\n\n        connection.connect()\n        connection.set_autocommit(True)  \n\n        with transaction.atomic():\n            self.do(2)\n\n        self.assertDone([1, 2]) \n", "    def test_hooks_persist_across_connection_resets(self):\n        with transaction.atomic():\n            transaction.on_commit(lambda: self.notify(1))\n        self.assertDone([1])\n\n        connection.close()\n\n        # Reset connection and restore original autocommit state\n        connection.connect()\n        connection.set_autocommit(True)\n\n        # The hook should be executed despite the connection reset\n        with transaction.atomic():\n            self.do(2)\n\n        self.assertDone([1, 2]) \n\n\n\n", "    def test_hooks_preserved_across_multiple_commits(self):\n        with transaction.atomic():\n            self.do(1)\n            transaction.commit()\n\n        with transaction.atomic():\n            self.do(2)\n            transaction.commit()\n\n        self.assertDone([1, 2])\n", "    def test_hooks_are_not_executed_for_savepoints_from_different_transactions(self):\n        with transaction.atomic():\n            with transaction.atomic():\n                self.do(1)\n        with transaction.atomic():\n            self.do(2)\n\n        self.assertDone([1, 2])\n\n"], "sample_950": ["def test_ref_type_inference(app, status, warning):\n    app.build()\n    doc = app.env.get_doc('index')\n    assert doc.title == 'Name'\n    assert doc.docstring == 'This is the docstring for Name'\n    assert doc.ids == ['Name']\n    assert doc.interlinks == 'Name'\n", "def test_python_method_signature(app):\n    text = (\".. py:class:: Class\\n\"\n            \"   .. py:method:: meth(a, b, *args, **kwargs)\\n\"\n            \"      :param a: blah\\n\"\n            \"      :param b: blah\\n\"\n            \"      :type a: int\\n\"\n            \"      :type b: str\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                    [desc_name, \"Class\"])],\n                                  [desc_content, nodes.field_list, nodes.field, (nodes.field_name,\n                                [nodes.field_body, nodes.paragraph])])]))\n    assert_node(doctree[1][0][0][0][1],\n                ([nodes.field_name, \"Methods\"],\n                 [nodes.field_body, nodes.paragraph]))\n\n    assert_node(doctree[1][0][0][0][1][0][0],\n                ([addnodes.literal_strong, \"meth\"],\n                 \"(\",\n                 [addnodes.literal_emphasis, \"a\"],\n                 \", \",\n                 [addnodes.literal_emphasis, \"b\"],\n                 \", \",\n                 \"[addnodes.literal_emphasis, \"*args\"],\n                 \", \",\n                 \"[addnodes.literal_emphasis, \"**kwargs\"],\n                 \")\",\n                 \" -- \",\n                 [pending_xref, addnodes.literal_emphasis, \"int\"],\n                 \" -- \",\n                 \"blah\"))\n    assert_node(doctree[1][0][0][0][1][0][0][1], pending_xref,\n                refdomain=\"py\", reftype=\"class\", reftarget=\"int\",\n                **{\"py:module\": \"example\", \"py:class\": \"Class\"})\n\n\n", "def test_warn_missing_reference_in_html(app, status, warning):\n    app.build()\n    assert 'index.rst:6: WARNING: undefined label: no-label' in warning.getvalue()\n    assert ('index.rst:6: WARNING: Failed to create a cross reference. A title or caption not found: existing-label'\n            in warning.getvalue())\n", "def test_xref_missing_object(app, status, warning):\n    app.build()\n    content = (app.outdir / 'index.html').read_text()\n    assert '<p>Here is a crossreference to <tt class=\"xref\">no-such-object</tt>.</p>' in content\n", "def test_cross_reference_in_same_file(app, status, warning):\n    app.build()\n    content = (app.outdir / 'index.html').read_text()\n    assert '<a class=\"reference\" href=\"#class-definition\">class-definition</a>' in content \n"], "sample_26": ["def test_compress_non_contig(tmp_path):\n    \"\"\"\n    Regression test for https://github.com/astropy/astropy/issues/7256\n    This test ensures that we can compress non-contiguous arrays.\n\n    It is important to note that while we can compress arrays that are\n    not C-contiguous, some external libraries may have difficulty\n    reading them correctly.\n\n    \"\"\"\n    shape = (10, 10)\n    a = np.arange(100).reshape(shape).astype(np.int16)\n    a = np.ascontiguousarray(a)\n    a[::2, ::2] = -1\n    hdu = fits.CompImageHDU(a)\n    hdu.writeto(tmp_path / \"non_contig.fits\")\n    with fits.open(tmp_path / \"non_contig.fits\") as hdul:\n        assert_equal(hdul[1].data, a)\n\n\n", "def test_imagehdu_header_keywords():\n    \"\"\"Test that ImageHDU can correctly handle various keywords in the header.\"\"\"\n\n    hdu = fits.ImageHDU(np.zeros((10, 10)))\n    hdu.header.set(\"TEST1\", \"value1\")\n    hdu.header.set(\"TEST2\", \"value2\")\n\n    hdu.writeto(self.temp(\"test.fits\"))\n    with fits.open(self.temp(\"test.fits\")) as hdul:\n        assert hdul[0].header[\"TEST1\"] == \"value1\"\n        assert hdul[0].header[\"TEST2\"] == \"value2\"\n", "def test_compressed_header_with_no_extname():\n    \"\"\"\n    Test that CompImageHDU can be initialized with a Header that has no EXTNAME.\n\n    This was a point of breakage before, because the old code would\n    rely on EXTNAME to know the name of the image, but not all\n    applications set it.\n    \"\"\"\n\n    hdu = fits.CompImageHDU(data=np.arange(10), header=fits.Header())\n    assert hdu.name == \"IMAGE\"  # Default name should be used.\n\n    hdu.name = \"MYIMAGE\"\n    assert hdu.name == \"MYIMAGE\"\n    assert hdu.header[\"EXTNAME\"] == \"MYIMAGE\"\n\n    hdu.writeto(self.temp(\"test.fits\"))\n    with fits.open(self.temp(\"test.fits\")) as hdul:\n        assert hdul[1].name == \"MYIMAGE\"\n\n\n\n", "def test_comp_image_hdu_copy(tmp_path):\n    \"\"\"\n    Test that copying a CompImageHDU creates a new, independent object.\n\n    https://github.com/astropy/astropy/issues/12012\n\n    \"\"\"\n    data = np.random.rand(10, 10).astype(np.float32)\n    hdu = fits.CompImageHDU(data, compression_type=\"RICE_1\")\n    hdu.header[\"TEST_KEYWORD\"] = \"test_value\"\n\n    hdu_copy = hdu.copy()\n\n    assert hdu_copy.data is not hdu.data\n    assert hdu_copy.header is not hdu.header\n    assert hdu_copy.header[\"TEST_KEYWORD\"] == \"test_value\"\n\n    hdu.data *= 2\n    hdu.header[\"TEST_KEYWORD\"] = \"changed_value\"\n\n    assert hdu_copy.data != hdu.data\n    assert hdu_copy.header[\"TEST_KEYWORD\"] == \"test_value\"\n\n\n", "def test_write_with_invalid_tile_shape():\n    with pytest.raises(ValueError, match=\"Tile shape must be\"):\n        fits.CompImageHDU(np.zeros((10, 10)), tile_shape=(100, 10))\n"], "sample_771": ["def test_power_transformer_sparse_data():\n    X_sparse = sparse.csr_matrix([[1, 0, 0], [0, 1, 1], [0, 0, 1]])\n    for method in ['box-cox', 'yeo-johnson']:\n        pt = PowerTransformer(method=method)\n        X_trans = pt.fit_transform(X_sparse)\n        assert isinstance(X_trans, sparse.csr_matrix)\n        X_inv_trans = pt.inverse_transform(X_trans)\n        assert isinstance(X_inv_trans, sparse.csr_matrix)\n        assert_array_almost_equal(X_sparse.toarray(), X_inv_trans.toarray())\n\n\n\n", "def test_power_transformer_sparse_matrix():\n    X_sparse = sparse.csr_matrix(X_2d)\n    for method in ['box-cox', 'yeo-johnson']:\n        pt = PowerTransformer(method=method)\n        X_trans = pt.fit_transform(X_sparse)\n        assert isinstance(X_trans, sparse.csr_matrix)\n        assert_array_almost_equal(X_trans.toarray(), pt.transform(X_2d))\n\n        X_inv_trans = pt.inverse_transform(X_trans)\n        assert isinstance(X_inv_trans, sparse.csr_matrix)\n        assert_array_almost_equal(X_inv_trans.toarray(), X_2d)\n", "def test_power_transformer_non_numerical_data():\n    pt = PowerTransformer(method='box-cox')\n    X = np.array(['a', 1, 2, 'c'])\n\n    assert_raises(TypeError, pt.fit, X)\n    assert_raises(TypeError, pt.transform, X)\n    assert_raises(TypeError, pt.inverse_transform, X)\n\n    X = np.array([1, 2, np.nan, 4])\n    pt = PowerTransformer(method='yeo-johnson')\n\n    assert_raises(ValueError, pt.fit, X)\n    assert_raises(ValueError, pt.transform, X)\n    assert_raises(ValueError, pt.inverse_transform, X)\n", "def test_power_transformer_sparse_input():\n    X_sparse = sparse.csr_matrix(X_2d)\n    for method in ['box-cox', 'yeo-johnson']:\n        pt = PowerTransformer(method=method)\n        X_trans = pt.fit_transform(X_sparse)\n        assert isinstance(X_trans, sparse.csr_matrix)\n\n        X_inv = pt.inverse_transform(X_trans)\n        assert isinstance(X_inv, sparse.csr_matrix)\n", "compilation error"], "sample_568": ["def test_fill_solid_3d():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    ax.plot([1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], 'ro-')\n    ax.fill3d([1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], 'b', alpha=0.5)\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_zlabel('z')\n\n\n\n", "compilation error", "    def test_Poly3DCollection_set_facecolors_array_shape():\n        # GH#26420\n        poly = np.array([[0, 0, 1], [0, 1, 1], [0, 0, 0]], float)\n        c = art3d.Poly3DCollection([poly], color=['red'])\n        with pytest.raises(ValueError,\n                           match='facecolors array shape should be (\\\\d+, 3|4)' ):\n            c.set_facecolors(np.random.rand(2, 4))\n\n\n\n", "def test_artist_visibility():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    s = ax.scatter([1, 2, 3], [4, 5, 6], [7, 8, 9])\n    l = ax.plot([1, 2, 3], [4, 5, 6], [7, 8, 9])\n\n    # Make the scatter plot invisible\n    s.set_visible(False)\n\n    # Ensure scatter plot is not drawn\n    fig.canvas.draw()\n    assert not s.get_visible()\n\n    # Make the line plot visible\n    l.set_visible(True)\n\n    # Ensure line plot is drawn\n    fig.canvas.draw()\n    assert l.get_visible()\n\n    # Make both objects visible\n    s.set_visible(True)\n    l.set_visible(True)\n\n    # Ensure both objects are drawn\n    fig.canvas.draw()\n    assert s.get_visible()\n    assert l.get_visible()\n", "def test_patch_collection_3d_alpha():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    # Define patches with varying alpha values\n    patches = [\n        art3d.Patch3D(\n            [\n                (0, 0, 0), (1, 0, 0), (1, 1, 0), (0, 1, 0)\n            ],\n            facecolor=(1, 0, 0, 0.5),  # 50% alpha\n        ),\n        art3d.Patch3D(\n            [\n                (0, 0, 1), (1, 0, 1), (1, 1, 1), (0, 1, 1)\n            ],\n            facecolor=(0, 1, 0, 0.8),  # 80% alpha\n        ),\n    ]\n    collection = art3d.PathPatch3DCollection(patches)\n    ax.add_collection3d(collection)\n\n    # Assert that the alpha values are displayed correctly\n    assert collection.get_facecolors() == [\n        (1.0, 0.0, 0.0, 0.5), \n        (0.0, 1.0, 0.0, 0.8)\n    ]\n"], "sample_637": ["    def test_regex_notes(self) -> None:\n        code = \"\"\"a = 1\n                # TODO implement this later\n                # FIXME need to change this\n                # XXX what to do here?\n                \"\"\"\n        with self.assertAddsMessages(\n            MessageTest(\n                msg_id=\"fixme\", line=2, args=\"TODO implement this later\", col_offset=17\n            ),\n            MessageTest(\n                msg_id=\"fixme\", line=3, args=\"FIXME need to change this\", col_offset=17\n            ),\n            MessageTest(\n                msg_id=\"fixme\", line=4, args=\"XXX what to do here?\", col_offset=17\n            ),\n        ):\n            self.checker.process_tokens(_tokenize_str(code))\n", "    def test_issue_2006_encoding_comments(self) -> None:\n        code = \"\"\"\n        # -*- coding: utf-8 -*-\n        print(\"Hello world! \u3053\u3093\u306b\u3061\u306f\")\n        \"\"\"\n        with self.assertNoMessages():\n            self.checker.process_tokens(_tokenize_str(code))\n", "    def test_regex_notes(self) -> None:\n        code = \"\"\"a = 1\n                # TODO this is a test\n                # TODO another test\n                \"\"\"\n        with self.assertAddsMessages(\n            MessageTest(msg_id=\"fixme\", line=2, args=\"TODO this is a test\", col_offset=17),\n            MessageTest(msg_id=\"fixme\", line=3, args=\"TODO another test\", col_offset=17),\n        ):\n            self.checker.process_tokens(_tokenize_str(code))\n", "    def test_regex_notes(self) -> None:\n        code = \"\"\"a = 1\n                # TODO fix this\n                # FIXME another\n                \"\"\"\n        with self.assertAddsMessages(\n            MessageTest(msg_id=\"fixme\", line=2, args=\"FIXME another\", col_offset=17),\n            MessageTest(msg_id=\"fixme\", line=3, args=\"TODO fix this\", col_offset=17),\n        ):\n            self.checker.process_tokens(_tokenize_str(code))\n", "    def test_regex_codetag(self) -> None:\n        code = \"\"\"a = 1\n                # FIXME\n                # TODO\n                # TODOnote\n                \"\"\"\n        with self.assertAddsMessages(\n            MessageTest(msg_id=\"fixme\", line=2, args=\"FIXME\", col_offset=17),\n            MessageTest(msg_id=\"fixme\", line=3, args=\"TODO\", col_offset=17),\n            MessageTest(msg_id=\"fixme\", line=4, args=\"TODonote\", col_offset=17),\n        ):\n            self.checker.process_tokens(_tokenize_str(code))\n"], "sample_644": ["    def test_import_alias_within_module(capsys: CaptureFixture[str]) -> None:\n        \"\"\"Test import alias within a module.\"\"\"\n        Run(\n            [\n                f\"{os.path.join(REGR_DATA, 'import_alias_within_module')}\",\n                \"-d allow-reexport-from-package,unused-import\",\n            ],\n            exit=False,\n        )\n        output, errors = capsys.readouterr()\n        assert len(output.split(\"\\n\")) == 5\n        assert errors == \"\"\n\n\n\n", "    def test_import_fallback_block(capsys: CaptureFixture[str]) -> None:\n        Run(\n            [\n                f\"{os.path.join(REGR_DATA, 'fallback_blocks')}\",\n                \"-d all\",\n                \"-e wrong-import-order\",\n            ],\n            exit=False,\n        )\n        output, errors = capsys.readouterr()\n\n        # assert that wrong-import-order message is not emitted inside a fallback block\n        assert \"wrong-import-order\" not in output\n", "    def test_duplicate_import(capsys: CaptureFixture[str]) -> None:\n        Run(\n            [\n                f\"{os.path.join(REGR_DATA, 'duplicate_import')}\",\n                \"-d all\",\n                \"-e reimported\",\n            ],\n            exit=False,\n        )\n        output, errors = capsys.readouterr()\n        assert len(output.split(\"\\n\")) == 5\n        assert \"reimported\" in output\n", "    def test_invalid_wildcard_import(capsys: CaptureFixture[str]) -> None:\n        \"\"\"\n        Tests wildcard import in a module with no __all__\n        \"\"\"\n\n        Run(\n            [\n                f\"{os.path.join(REGR_DATA, 'invalid_wildcard')}\",\n                \"-d all\",\n                \"-e wildcard-import\",\n            ],\n            exit=False,\n        )\n        output, errors = capsys.readouterr()\n\n        assert \"wildcard-import\" in output\n        assert len(errors) == 0\n\n", "    def test_unused_import(self) -> None:\n        module = astroid.MANAGER.ast_from_module_name(\"unused_import\", REGR_DATA)\n        import_node = module.body[0]\n\n        msg = MessageTest(\n            msg_id=\"unused-import\",\n            node=import_node,\n            args=\"unused_import\",\n            line=1,\n            col_offset=0,\n            end_line=1,\n            end_col_offset=15,\n        )\n        with self.assertAddsMessages(msg):\n            self.checker.visit_import(import_node)\n"], "sample_741": ["compilation error", "compilation error", "def test_grid_search_cv_with_transformers_pipeline():\n    from sklearn.compose import ColumnTransformer\n    from sklearn.pipeline import Pipeline\n    from sklearn.feature_selection import SelectKBest\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.preprocessing import StandardScaler\n\n    X, y = make_classification(n_samples=100, n_features=10, random_state=0)\n\n    numeric_features = [i for i in range(5)]\n    categorical_features = [i for i in range(5, 10)]\n\n    preprocessor = ColumnTransformer(\n        transformers=[\n            ('num', StandardScaler(), numeric_features),\n            ('cat', OneHotEncoder(handle_unknown='ignore'),\n                 categorical_features),\n        ])\n\n    model = LogisticRegression(random_state=0)\n    pipeline = Pipeline([('preprocessor', preprocessor), ('model', model)])\n\n    param_grid = {'model__C': [0.1, 1, 10],\n                  'preprocessor__num__with_std': [False, True]}\n\n    gs = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')\n    gs.fit(X, y)\n\n    assert_equal(gs.best_params_, {'model__C': 1, 'preprocessor__num__with_std': True})\n\n    best_pipeline = gs.best_estimator_\n    assert isinstance(best_pipeline, Pipeline)\n    assert isinstance(best_pipeline.steps[0][1], StandardScaler)\n    assert isinstance(best_pipeline.steps[1][1], LogisticRegression)\n\n\n\n", "compilation error", "def test_grid_search_without_estimators():\n    # Test that an error is raised when estimator is not provided\n    param_grid = {'C': [0.1, 0.2, 0.3]}\n    with assert_raises(TypeError):\n        GridSearchCV(estimator=None, param_grid=param_grid)\n\n"], "sample_358": ["    def test_rename_column_references_with_opclasses(self):\n        compiler = Person.objects.all().query.get_compiler(connection.alias)\n        self.expressions = Expressions(\n            table=Person._meta.db_table,\n            expressions=ExpressionList(\n                IndexExpression(F('first_name'), opclass='text_primary_key'),\n                IndexExpression(F('last_name').desc(), opclass='text_parser'),\n            ).resolve_expression(compiler.query),\n            compiler=compiler,\n            quote_value=self.editor.quote_value,\n        )\n\n        table = Person._meta.db_table\n        self.expressions.rename_column_references(table, 'first_name', 'other')\n        self.assertIs(self.expressions.references_column(table, 'other'), True)\n        self.assertIs(self.expressions.references_column(table, 'first_name'), False)\n        self.assertIn(\n            '%s.%s USING text_primary_key' % (\n                self.editor.quote_name(table),\n                self.editor.quote_name('other'),\n            ),\n            str(self.expressions),\n        )\n\n\n", "    def test_rename_column_references_with_compound_indexes(self):\n        compiler = Person.objects.all().query.get_compiler(connection.alias)\n        self.expressions = Expressions(\n            table=Person._meta.db_table,\n            expressions=ExpressionList(\n                IndexExpression(F('first_name')),\n                IndexExpression(F('last_name').desc()),\n                IndexExpression(Upper('last_name')),\n            ).resolve_expression(compiler.query),\n            compiler=compiler,\n            quote_value=self.editor.quote_value,\n        )\n        table = Person._meta.db_table\n        self.expressions.rename_column_references(table, 'last_name', 'other_last_name')\n        expected_str = (\n            '%s.%s, %s.%s DESC, (UPPER(%s.%s))' % (\n                self.editor.quote_name(table),\n                self.editor.quote_name('first_name'),\n                self.editor.quote_name(table),\n                self.editor.quote_name('other_last_name'),\n                self.editor.quote_name(table),\n                self.editor.quote_name('other_last_name'),\n            )\n        )\n        self.assertEqual(str(self.expressions), expected_str)\n\n", "    def test_str_with_suffixes(self):\n        compiler = Person.objects.all().query.get_compiler(connection.alias)\n        self.expressions = Expressions(\n            table=Person._meta.db_table,\n            expressions=ExpressionList(\n                IndexExpression(F('first_name')),\n                IndexExpression(F('last_name').desc()),\n            ).resolve_expression(compiler.query),\n            compiler=compiler,\n            quote_value=self.editor.quote_value,\n            col_suffixes=[\n                self.editor.quote_name('_suffix'),\n                self.editor.quote_name('_suffix'),\n            ],\n        )\n        table_name = self.editor.quote_name(Person._meta.db_table)\n        expected_str = '%s.%s_suffix, %s.%s_suffix DESC' % (\n            table_name,\n            self.editor.quote_name('first_name'),\n            table_name,\n            self.editor.quote_name('last_name'),\n        )\n        self.assertEqual(str(self.expressions), expected_str) \n", "    def test_expressions_are_updated_after_rename(self):\n        table = Person._meta.db_table\n        self.expressions.rename_column_references(table, 'first_name', 'other')\n        self.assertIn(\n            '%s.other' % (self.editor.quote_name(table),),\n            str(self.expressions),\n        )\n        self.expressions.rename_column_references(table, 'other', 'first_name')\n        self.assertIn(\n            '%s.%s' % (self.editor.quote_name(table), self.editor.quote_name('first_name')),\n            str(self.expressions),\n        )\n\n", "    def test_table_references_are_preserved_across_rename_columns(self):\n        table = Person._meta.db_table\n        expressions = Expressions(\n            table=table,\n            expressions=ExpressionList(\n                IndexExpression(F('first_name')),\n            ).resolve_expression(self.compiler.query),\n            compiler=self.compiler,\n            quote_value=self.editor.quote_value,\n        )\n\n        expressions.rename_column_references(table, 'first_name', 'other')\n        expressions.rename_table_references(table, 'other')\n        self.assertIs(expressions.references_table('other'), True)\n        self.assertIs(expressions.references_column('other', 'other'), True)\n        expected_str = '%s.%s' % (\n            self.editor.quote_name('other'),\n            self.editor.quote_name('other'),\n        )\n        self.assertEqual(str(expressions), expected_str)\n"], "sample_144": ["    def test_update_with_parent_fk_change(self):\n        r = Restaurant.objects.create(name='Guido\\'s House of Pasta', address='944 W. Fullerton')\n        s = Supplier.objects.create(restaurant=r)\n        place = Place.objects.create(name='New Place', address='945 W. Fullerton')\n        with self.assertNumQueries(1):\n            r.place_ptr = place\n            self.assertEqual(r.pk, place.pk)\n        self.assertQuerysetEqual(\n            Supplier.objects.filter(restaurant__in=Place.objects.all()),\n            [s]\n        )\n", "    def test_related_m2m_with_foreign_key(self):\n        r = Restaurant.objects.create(name=\"Guido's House of Pasta\", address=\"944 W. Fullerton\")\n        s = Supplier.objects.create(restaurant=r)\n        supplier_article = ArticleWithAuthor.objects.create(\n            headline=\"Supplier Article\",\n            author=\"Bob\",\n            pub_date=datetime.datetime(2024, 1, 1, 10, 0),\n            restaurant=r\n        )\n        self.assertEqual(Supplier.objects.get(pk=s.pk).article_set.count(), 1)\n        self.assertEqual(supplier_article.restaurant.pk, r.pk)\n", "    def test_related_m2m_fields_with_parent_model_inheritance(self):\n        r = Restaurant.objects.create(name=\"Guido's House of Pasta\", address='944 W. Fullerton')\n        s1 = Supplier.objects.create(name=\"John\", restaurant=r)\n        s2 = Supplier.objects.create(name=\"Jane\", restaurant=r)\n        r.supplier_set.add(s1, s2)\n\n        with self.assertNumQueries(1):\n            self.assertQuerysetEqual(\n                Restaurant.objects.filter(pk=r.pk).prefetch_related('supplier_set').all(), [r]\n            )\n            self.assertQuerysetEqual(\n                r.supplier_set.all(),\n                [s1, s2],\n                attrgetter(\"name\"),\n            )\n\n", "    def test_inherited_fields_on_parent_objects(self):\n        \"\"\"\n        Regression test for #12074\n        Ensure inherited fields are available on parent objects when using \n        select_related()\n\n        \"\"\"\n        italian_restaurant = ItalianRestaurant.objects.create(\n            name=\"Guido's House of Pasta\",\n            address='944 W. Fullerton',\n            serves_hot_dogs=True,\n            serves_pizza=False,\n            serves_gnocchi=True,\n        )\n        with self.assertNumQueries(1):\n            restaurant = italian_restaurant.restaurant_ptr\n        self.assertEqual(restaurant.serves_gnocchi, italian_restaurant.serves_gnocchi)\n", "    def test_inheritance_and_related_objects_with_no_pk_in_child(self):\n        \"\"\"\n        Regression test for #23085\n        \"\"\"\n        class SomeModel(models.Model):\n            name = models.CharField(max_length=50)\n        \n        class ChildModel(models.Model):\n            some_model = models.OneToOneField(SomeModel, on_delete=models.CASCADE)\n\n        some_model = SomeModel.objects.create(name='Some Model')\n        child_model = ChildModel.objects.create(some_model=some_model)\n        related_child = ChildModel.objects.get(pk=child_model.pk).some_model\n\n        self.assertEqual(related_child.name, some_model.name)\n\n"], "sample_171": ["    def test_makemigrations_conflicts_with_noinput(self):\n        \"\"\"\n        makemigrations --noinput handles conflicts without prompting.\n        \"\"\"\n        with self.temporary_migration_module(module=\"migrations.test_migrations_conflict\") as migration_dir:\n            call_command(\"makemigrations\", \"migrations\", name=\"merge\", merge=True, interactive=False, verbosity=0)\n            merge_file = os.path.join(migration_dir, '0003_merge.py')\n            self.assertTrue(os.path.exists(merge_file))\n", "    def test_migration_label_no_app_in_installed_apps(self):\n        \"\"\"\n        makemigrations does not raise an error when specified label does not \n        exist in installed_apps if label is a name and a migration exists in \n        the project.\n        \"\"\"\n        with patch.dict(apps._registry, {}) as registry:\n            app_label = 'migrations'\n            migration_name = '0001_initial'\n            registry[app_label] = appRegistry.Appconfig(\n                app_label=app_label,\n                models= [],\n                models_cache=None,\n                migrations=None,\n                autodiscover=None,\n                ready=False\n            )\n            with self.temporary_migration_module(module=\"migrations.test_migrations\") as migration_dir:\n                with self.assertRaises(SystemExit):\n                    call_command('makemigrations', migration_name, stdout=io.StringIO())\n", "    def test_squashmigrations_nonexistent_app_label_with_name(self):\n        with self.assertRaisesMessage(CommandError, self.nonexistent_app_error):\n            call_command('squashmigrations', 'nonexistent_app', '0002', squashed_name='squashed_migration')\n", "    def test_squashmigrations_merge_conflicts(self):\n        \"\"\"\n        squashmigrations works with merged migrations even if they are not\n        in an explicit branch.\n        \"\"\"\n        out = io.StringIO()\n        with self.temporary_migration_module(module=\"migrations.test_migrations_conflict\") as migration_dir:\n            call_command(\"makemigrations\", \"migrations\", merge=True, verbosity=0)\n            call_command(\"squashmigrations\", \"migrations\", \"0002\", interactive=False, verbosity=0, stdout=out)\n\n            squashed_migration_file = os.path.join(migration_dir, \"0001_squashed_0002_second.py\")\n            self.assertTrue(os.path.exists(squashed_migration_file))\n\n\n", "    def test_app_label_from_settings(self):\n        \"\"\"\n        makemigrations uses the app label from settings.py if provided.\n        \"\"\"\n        with override_settings(INSTALLED_APPS=['migrations', 'auth']):\n            with self.temporary_migration_module(module='migrations.test_migrations') as migration_dir:\n                call_command('makemigrations', 'migrations', 'auth', verbosity=0)\n                self.assertTrue(os.path.exists(os.path.join(migration_dir, '0001_initial.py')))\n"], "sample_572": ["    def test_bootstrap_errorbars(self, long_df):\n\n        agg = EstimateAggregator(\"mean\", \"ci\", n_boot=100000, seed=0)\n        out = agg(long_df, \"y\")\n\n        agg_ref = EstimateAggregator(\"mean\", (\"sd\", 1.96))\n        out_ref = agg_ref(long_df, \"y\")\n\n        assert out[\"ymin\"] == pytest.approx(out_ref[\"ymin\"], abs=1e-2)\n        assert out[\"ymax\"] == pytest.approx(out_ref[\"ymax\"], abs=1e-2)\n", "    def test_bootstrap_errorbars(self, long_df):\n\n        agg = EstimateAggregator(\"mean\", \"boot\", n_boot=10000, seed=0)\n        out = agg(long_df, \"y\")\n\n        agg_ref = EstimateAggregator(\"mean\", (\"se\", 1.96))\n        out_ref = agg_ref(long_df, \"y\")\n\n        assert out[\"ymin\"] == pytest.approx(out_ref[\"ymin\"], abs=1e-2)\n        assert out[\"ymax\"] == pytest.approx(out_ref[\"ymax\"], abs=1e-2)\n\n", "    def test_errorbar_arg_validation(\n        self, method, level, expected_method, expected_level", "    def test_bootstrap_errorbars(self, long_df):\n\n        agg = EstimateAggregator(\"mean\", \"ci\", n_boot=1000, seed=0)\n        out = agg(long_df, \"y\")\n\n        agg_ref = EstimateAggregator(\"mean\", (\"sem\", 1.96))\n        out_ref = agg_ref(long_df, \"y\")\n\n        assert out[\"ymin\"] == pytest.approx(out_ref[\"ymin\"], abs=1e-2)\n        assert out[\"ymax\"] == pytest.approx(out_ref[\"ymax\"], abs=1e-2)\n\n\n\n", "    def test_bootstrap_errorbars(self, long_df):\n\n        agg = EstimateAggregator(\"mean\", \"ci\", n_boot=1000, seed=0)\n        out = agg(long_df, \"y\")\n\n        agg_ref = EstimateAggregator(\"mean\", (\"perc\", 2.5, 97.5), n_boot=1000, seed=0)\n        out_ref = agg_ref(long_df, \"y\")\n        assert out[\"ymin\"] == pytest.approx(out_ref[\"ymin\"], abs=1e-2)\n        assert out[\"ymax\"] == pytest.approx(out_ref[\"ymax\"], abs=1e-2)\n\n\n\n"], "sample_791": ["def test_one_hot_encoder_invalid_categories():\n    enc = OneHotEncoder(categories=[['apple', 'banana', 'orange', None]])\n    with pytest.raises(ValueError, match=\"Categories should not contain None\"):\n        enc.fit([[None]])\n\n    enc = OneHotEncoder(categories=[['apple', 'banana', 'cherry']])\n    with pytest.raises(ValueError, match=\"Found unknown categories\"):\n        enc.fit([['apple', 'banana', 'orange', 'cherry', 'grape']])\n\n\n\n    enc = OneHotEncoder(categories=[['apple', 'banana', 'cherry'],\n                                      [1, 2, 3]])\n    with pytest.raises(ValueError, match=\"Categories should have same length\"):\n        enc.fit([['apple', 1], ['banana', 2], ['cherry', 3]])\n", "def test_one_hot_encoder_missing_categories():\n    enc = OneHotEncoder(categories=[['a', 'b', 'c']], handle_unknown='ignore')\n    X = [['d', 2], ['e', 3]]\n    exp = np.array([[0., 0., 1., 1.],\n                    [0., 0., 0., 1.]])\n    assert_array_equal(enc.fit_transform(X).toarray(), exp)\n\n    enc = OneHotEncoder(categories=[['a', 'b', 'c']], handle_unknown='error')\n    with pytest.raises(ValueError, match=\"Found unknown categories\"):\n        enc.fit_transform(X)\n\n\n\n\n", "def test_categorical_encoder_fit_transform_with_unknown_categories():\n    X = [['abc', 2, 55], ['def', 1, 55], ['ghi', 3, 55]]\n    enc = OneHotEncoder(handle_unknown='ignore')\n    exp = np.array([[1., 0., 0., 1., 0., 1.],\n                    [0., 1., 0., 0., 1., 0.],\n                    [0., 0., 1., 0., 0., 1.]], dtype='float64')\n    assert_array_equal(enc.fit_transform(X), exp)\n\n\n", "def test_one_hot_encoder_numpy_array_input_with_empty_column():\n    enc = OneHotEncoder(sparse=False)\n    X = np.array([['a', '', 1], ['b', 'c', 2]], dtype=object)\n    sparse_transform = enc.fit_transform(X)\n    for col in sparse_transform.T:\n        assert not np.all(np.isnan(col))\n    dense_transform = enc.fit_transform(X, sparse=False)\n    for col in dense_transform.T:\n        assert not np.all(np.isnan(col)) \n    assert_array_equal(sparse_transform, dense_transform)\n\n\n\n\n", "compilation error"], "sample_799": ["def test_score_multioutput():\n    from sklearn.datasets import make_multilabel_classification\n    X, y = make_multilabel_classification(random_state=0)\n    clf = MockClassifier()\n    scores = clf.score(X, y)\n    assert isinstance(scores, list) or isinstance(scores, tuple)\n    assert len(scores) == X.shape[1]\n\n\n\n", "def test_score_with_dict_output():\n        return {'accuracy': 0.8, 'precision': 0.7}\n\n    fit_and_score_args = [None, None, None, scorer]\n    with pytest.raises(ValueError) as excinfo:\n        _score(*fit_and_score_args)\n    assert 'scoring must return a single number' in str(excinfo.value)\n\n", "compilation error", "def test_cross_val_score_error_score_nan():\n    # Check that passing error_score=np.nan correctly sets the CV score\n    # to NaN\n    iris = load_iris()\n    X, y = iris.data, iris.target\n    clf = MockClassifier()\n    \n    scores = cross_val_score(clf, X, y, scoring='accuracy', cv=3,\n                             error_score=np.nan)\n    assert_array_almost_equal(scores, np.full((3,), np.nan))\n\n\n", "def test_score_complex_scoring():\n    X, y = make_classification(n_samples=30, random_state=0)\n    clf = MockEstimator()\n    \n        return 2 * estimator.predict_proba(X)[:, 1].sum()\n    \n    score = _score(clf, X, y, custom_scorer)\n    assert isinstance(score, float)\n\n\n\n"], "sample_825": ["compilation error", "compilation error", "compilation error", "def test_pls_with_missing_values():\n    import pandas as pd\n    # Test handling of missing values\n    d = load_linnerud()\n    X = d.data\n    Y = d.target\n\n    # Introduce missing values in X\n    X_with_missing = X.copy()\n    X_with_missing[2, :] = np.nan\n    X_with_missing[5, :] = np.nan\n\n    for clf in [pls_.PLSCanonical(), pls_.PLSRegression(),\n                pls_.PLSSVD()]:\n        clf.fit(X_with_missing, Y)\n        print(clf.coef_) \n        # TODO: Add assertion to check if the model output is reasonable.\n        #  This might involve comparing the output to a model trained on the\n        #  complete data or checking for specific patterns in the output\n        #  that indicate proper handling of missing values. \n", "def test_sparse_data():\n    import scipy.sparse as sp\n\n    n_samples = 100\n    n_features = 1000\n    n_components = 5\n\n    # Create a sparse matrix\n    X = sp.csr_matrix(np.random.randn(n_samples, n_features))\n    Y = np.random.randn(n_samples, 1)\n\n    # Ensure it works with sparse data\n    for clf in [pls_.PLSCanonical(), pls_.PLSRegression(), pls_.PLSSVD()]:\n        clf.fit(X, Y)\n        X_scores = clf.transform(X)\n        assert_array_like(X_scores.toarray(), X_scores)\n\n"], "sample_32": ["    def test_Hubble_parameter(self, cosmo, z):\n        \"\"\"Test :meth:`astropy.cosmology.w0wzCDM.Hubble_parameter`.\n        \"\"\"\n        H = cosmo.Hubble_parameter(z)\n        # TODO: Add a suitable assertion. This might involve comparing\n        # against expected values or using a custom tolerance. \n        #  Perhaps compare against a known analytical or numerical solution\n        #  for a specific cosmology.\n        print(f\"Hubble parameter at z={z} is: {H}\") \n", "    def test_de_densityscale_with_m_nu(self, cosmo):\n        \"\"\"Test :meth:`astropy.cosmology.w0wzCDM.de_densityscale` with massive neutrinos.\"\"\"\n        cosmo = w0wzCDM(\n            H0=70,\n            Om0=0.3,\n            Ode0=0.5,\n            w0=-1,\n            wz=0.5,\n            m_nu=1 * u.eV,\n        )\n\n        z = np.array([0.1, 0.2, 0.5, 1.5, 2.5])\n        \n        expected = [\n            1.00717408, 1.02713908, 1.15283127, 2.40097931, 6.49481871\n        ]\n        assert u.allclose(\n            cosmo.de_density_scale(z), expected, rtol=1e-4\n        )\n", "    def test_de_densityscale_units(self, cosmo):\n        \"\"\"Test that de_density_scale returns the correct units.\"\"\"\n        z = 0.5\n        assert cosmo.de_density_scale(z).unit == u.one \n", "    def test_comoving_distance_z_tol(self, cosmo):\n        \"\"\"Test :meth:`astropy.cosmology.w0wzCDM.comoving_distance`  for z tolerance.\"\"\"\n        # This tests the behavior of comoving distance when z is given as a\n        # quantity with a tolerance\n        z_tol = np.array([0.5, 1.0, 2.0]) * u.Quantity(1, u.dimensionless_unscaled)\n        z_tol[0] += 0.1 * u.Mpc\n        z_tol[1] += 0.2 * u.Mpc\n        z_tol[2] += 0.5 * u.Mpc\n\n        # Using the tolerance, test against a reference value calculated\n        # without the tolerance\n        ref_distances = cosmo.comoving_distance(z_tol)\n\n        # Check that the tolerance has no significant effect\n        for i in range(len(z_tol)):\n            assert u.allclose(\n                cosmo.comoving_distance(z_tol[i]), ref_distances[i], atol=1e-4 * u.Mpc\n            )\n", "compilation error"], "sample_1018": ["def test_fcode_If():\n    x, y = symbols('x y')\n    assert fcode(If(x > y, x, y), source_format='free') == \"      if (x > y) then\\n\" \\\n                                                        \"         x\\n\" \\\n                                                        \"      else\\n\" \\\n                                                        \"         y\\n\" \\\n                                                        \"      end if\"\n\n\n", "def test_fcode_If():\n    x, y = symbols('x y')\n    assert fcode(If(x > y, [y = x], [x = y]), source_format='free') == (\n        '      if (x > y) then\\n'\n        '         y = x\\n'\n        '      else\\n'\n        '         x = y\\n'\n        '      end if'\n    )\n\n\n\n", "def test_fcode_If():\n    x, y = symbols('x y')\n    assert fcode(If(x > y, ['x + y'], ['x - y']), source_format='free') == (\n        '      if (x > y) then\\n'\n        '         x + y\\n'\n        '      else\\n'\n        '         x - y\\n'\n        '      end if'\n    )\n", "def test_fcode_Conditional_expressions():\n    x, y = symbols('x y')\n    expr = Piecewise((2*x, x > 0), (y, x <= 0))\n    assert fcode(expr, source_format='free') == \"      if (x > 0) then\\n\" \\\n                                             \"         2*x\\n\" \\\n                                             \"      else\\n\" \\\n                                             \"         y\\n\" \\\n                                             \"      end if\"\n", "compilation error"], "sample_401": ["    def test_all_valid_with_empty_formset(self):\n        ChoiceFormSet = formset_factory(Choice)\n        formset1 = ChoiceFormSet(data={}, auto_id=False, prefix=\"choices\")\n        formset2 = ChoiceFormSet(data={}, auto_id=False, prefix=\"choices\")\n        self.assertIs(all_valid((formset1, formset2)), True)\n\n\n\n", "    def test_all_valid_with_nonform_errors(self):\n        class ChoiceFormSetWithNonFormError(BaseFormSet):\n                return True\n\n                return [\"Some non-form error.\"]\n\n        data = {\n            \"choices-TOTAL_FORMS\": \"1\",\n            \"choices-INITIAL_FORMS\": \"0\",\n            \"choices-MIN_NUM_FORMS\": \"0\",\n            \"choices-0-choice\": \"Zero\",\n            \"choices-0-votes\": \"0\",\n        }\n        ChoiceFormSet = formset_factory(Choice, formset=ChoiceFormSetWithNonFormError)\n        formset = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n        self.assertIs(all_valid((formset,)), False)\n        self.assertEqual(formset._errors, [{\"non_form_errors\": [\"Some non-form error.\"] }])\n", "    def test_all_valid_handles_non_form_errors(self):\n        class FormSetWithNonFormError(BaseFormSet):\n                raise ValidationError(\"This is a non-form error\")\n\n        data = {\"choices-TOTAL_FORMS\": \"1\"}\n        formset = FormSetWithNonFormError(data, auto_id=False, prefix=\"choices\")\n        self.assertIs(all_valid((formset, formset)), False)\n        self.assertEqual(formset._errors, [{\"\": [\"This is a non-form error\"]}])\n\n\n\n", "    def test_all_valid_with_non_form_errors(self):\n        data = {\n            \"choices-TOTAL_FORMS\": \"2\",\n            \"choices-INITIAL_FORMS\": \"0\",\n            \"choices-MIN_NUM_FORMS\": \"0\",\n            \"choices-0-choice\": \"Zero\",\n            \"choices-0-votes\": \"\",\n            \"choices-1-choice\": \"One\",\n            \"choices-1-votes\": \"\",\n        }\n        class CustomFormset(BaseFormSet):\n                raise ValidationError(\"This is a non-form error\")\n        ChoiceFormSet = formset_factory(Choice, formset=CustomFormset)\n        formset1 = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n        formset2 = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n        self.assertIs(all_valid((formset1, formset2)), False)\n", "    def test_all_valid_with_non_form_errors(self):\n        data = {\n            \"form-TOTAL_FORMS\": \"2\",\n            \"form-INITIAL_FORMS\": \"0\",\n            \"form-MIN_NUM_FORMS\": \"0\",\n            \"form-0-choice\": \"Zero\",\n            \"form-0-votes\": \"0\",\n            \"form-1-choice\": \"One\",\n            \"form-1-votes\": \"1\",\n        }\n        class BaseFormWithNonFormError(BaseFormSet):\n            non_form_errors = [\"This is a non-form error.\"]\n\n        ChoiceFormSet = formset_factory(Choice, formset=BaseFormWithNonFormError)\n        formset1 = ChoiceFormSet(data, auto_id=False, prefix=\"form\")\n        formset2 = ChoiceFormSet(data, auto_id=False, prefix=\"form\")\n\n        self.assertIs(all_valid((formset1, formset2)), False)\n        expected_errors = [\n            {\"\": [\"This is a non-form error.\"]},\n            {\"\": [\"This is a non-form error.\"]},\n        ]\n        self.assertEqual(formset1._errors, expected_errors)\n        self.assertEqual(formset2._errors, expected_errors)\n"], "sample_373": ["    def test_simplify_regex_with_optional_groups(self):\n        tests = (\n            (r'^(?P<a>\\w+)?/b/(?P<c>\\w+)$', '/<a>/b/<c>'),\n            (r'^(?:\\w+)?/b/(?:\\w+)$', '/b/'),\n            (r'^(?P<a>\\w+)?/b/(?:\\w+)?$', '/<a>/b/'),\n        )\n        for pattern, output in tests:\n            with self.subTest(pattern=pattern):\n                self.assertEqual(simplify_regex(pattern), output)\n\n\n", "    def test_simplify_regex_with_brackets_and_groups(self):\n        tests = (\n            (r'^(?P<a>(?:\\(\\w+\\))?)\\w+/$', '/<a>'),\n            (r'^(\\w+)/(?P<b>[\\w\\-]+)$', '/<var>/<b>'),\n            (r'^/(?P<name>\\w+)\\/(?P<id>\\d+)$', '/<name>/<id>'),\n        )\n        for pattern, output in tests:\n            with self.subTest(pattern=pattern):\n                self.assertEqual(simplify_regex(pattern), output)\n", "    def test_extract_views_from_urlpatterns(self):\n        from django.urls import path, include\n        urlpatterns = [\n            path(\"admin/doc/\", include(\"django.contrib.admindocs.urls\")),\n            path(\"admin/\", admin.site.urls),\n            path(\"blog/\", include(\"myapp.urls\")),\n        ]\n        views = extract_views_from_urlpatterns(urlpatterns, base='', namespace='myapp')\n        expected_results = [\n            (path(\"blog/post/create/\", views.PostCreateView.as_view(), name=\"post_create\"), \"myapp:post_create\"),\n        ]\n        self.assertEqual(len(views), len(expected_results))\n        for i, view in enumerate(views):\n            self.assertEqual(view[0].callback, expected_results[i][0].callback)\n            self.assertEqual(view[2], expected_results[i][1])\n\n\n\n", "    def test_simplify_regex_non_capturing_groups(self):\n        tests = (\n            (r'^(?P<a>\\w+)(?:\\w+)\\b', '/<a>/'),\n            (r'^a(?:\\w+)?$', '/a'),\n        )\n        for pattern, output in tests:\n            with self.subTest(pattern=pattern):\n                self.assertEqual(simplify_regex(pattern), output)\n\n\n", "    def test_normalize_model_name(self):\n        self.assertEqual(views.normalize_model_name('myapp.Person'), 'myapp.Person')\n        self.assertEqual(views.normalize_model_name('myapp.Person_'), 'myapp.Person')  \n\n\n"], "sample_185": ["    def test_get_language_from_request_with_invalid_accept_language(self):\n        r = self.rf.get('/')\n        r.COOKIES = {}\n        r.META = {'HTTP_ACCEPT_LANGUAGE': 'foobar'}\n        lang = get_language_from_request(r)\n        self.assertEqual(lang, 'en')\n", "    def test_get_language_from_path_with_dot(self):\n        with self.settings(INSTALLED_APPS=['i18n.resolution']):\n            self.assertEqual(get_language_from_path('/de.en/'), 'de')\n\n\n\n\n", "    def test_normalize_language_code(self):\n        tests = [\n            ('en', 'en'),\n            ('EN', 'en'),\n            ('en-US', 'en-US'),\n            ('EN-US', 'en-US'),\n            ('en-us', 'en-us'),\n            ('en-US,en', 'en-US'),\n            ('en-us,en-US', 'en-US'),\n            ('en-US;q=0.8,en;q=0.6,fr;q=0.4', 'en-US'),\n        ]\n        for input_code, expected in tests:\n            with self.subTest(input_code=input_code):\n                self.assertEqual(normalize_language_code(input_code), expected)\n", "    def test_get_language_from_request_with_language_cookie(self):\n        r = self.rf.get('/')\n        r.COOKIES = {'language': 'de'}\n        r.META = {}\n        self.assertEqual('de', get_language_from_request(r))\n", "    def test_round_away_from_one_with_ndigits(self):\n        tests = [\n            (0, 0, 0),\n            (0., 0, 0),\n            (0.25, 0, 0),\n            (0.5, 0, 0),\n            (0.75, 0, 0),\n            (1, 1, 0),\n            (1., 1, 0),\n            (1.25, 2, 0),\n            (1.5, 2, 0),\n            (1.75, 2, 0),\n            (-0., 0, 0),\n            (-0.25, -1, 0),\n            (-0.5, -1, 0),\n            (-0.75, -1, 0),\n            (-1, -1, 0),\n            (-1., -1, 0),\n            (-1.25, -2, 0),\n            (-1.5, -2, 0),\n            (-1.75, -2, 0),\n            (0.25, 1, 1),\n            (0.75, 1, 1),\n            (0.25, 2, 0),\n            (0.75, 2, 1),\n            (1.25, 2, 2),\n            (1.25, 3, 2),\n        ]\n        for value, expected, ndigits in tests:\n            with self.subTest(value=value, ndigits=ndigits):\n                self.assertEqual(round_away_from_one(value, ndigits), expected)\n\n\n\n\n\n"], "sample_909": ["    def test_keywords_with_types_and_description(self):\n        docstring = \"\"\"\\", "    def test_duplicate_section_headers(self):\n        docstring = \"\"\"", "    def test_custom_sections(self):\n        docstring = \"\"\"\\\n\n        My Custom Section\n\n        This is some custom content.\n\n        Parameters\n        ----------\n        arg1 : int\n            Description for arg1\n\n        \"\"\"\n\n        config = Config(napoleon_custom_sections=['My Custom Section', ('Custom Section', 'cust')])\n        actual = str(GoogleDocstring(docstring, config))\n        expected = \"\"\"\\\n\n        My Custom Section\n\n        This is some custom content.\n\n        :Parameters: * **arg1** (*int*) -- Description for arg1", "    def test_numpydoc_docstring_with_special_characters(self):\n        docstring = \"\"\"", "    def test_raises_with_multiple_types(self):\n        docstring = \"\"\""], "sample_854": ["def test_one_class_svm_rbf_kernel():\n    # Test OneClassSVM with radial basis function kernel\n    X = np.array([[0], [0.44], [0.45], [0.46], [1]])\n    \n    clf = svm.OneClassSVM(kernel='rbf')\n    clf.fit(X)\n\n    # Check that some support vectors are selected\n    assert clf.support_.size > 0\n    assert clf.support_.shape[0] < X.shape[0]\n    \n    # Check prediction on new data points\n    new_data = np.array([[0.5], [1.1]])\n    assert clf.predict(new_data)\n\n\n", "compilation error", "    def test_n_support_with_kernel(kernel):\n        X, y = make_blobs(centers=3, random_state=42)\n        clf = svm.SVC(kernel=kernel, probability=False)\n        clf.fit(X, y)\n        assert clf.n_support_ == clf.support_vectors_.shape[0]\n        assert clf.n_support_.size == len(set(clf.support_))\n", "def test_svm_kernel_callable_with_precomputed():\n    # Test that callable kernels work with precomputed kernels\n\n    from sklearn.datasets import make_classification\n    X, y = make_classification(n_samples=10, n_features=5)\n    kernel = lambda x, y: np.dot(x, y)\n    X_precomputed = np.dot(X, X.T)\n\n    svm = svm.SVC(kernel=kernel, probability=True)\n    svm.fit(X, y)\n    svm_precomputed = svm.SVC(kernel=kernel, probability=True,\n                              kernel_precomputed=True)\n    svm_precomputed.fit(X_precomputed, y)\n\n    assert_array_almost_equal(svm.predict(X), svm_precomputed.predict(X_precomputed))\n    assert_array_almost_equal(svm.predict_proba(X),\n                             svm_precomputed.predict_proba(X_precomputed))\n\n\n\n", "def test_nu_parameter_in_prediction():\n\n    X = np.random.rand(10, 3)\n    y = np.random.randint(0, 2, size=10)\n\n    # Test NuSVC with different nu values\n\n    nu_values = [0.1, 0.5, 0.9, 1.0]\n\n    for nu in nu_values:\n        clf_nu = svm.NuSVC(nu=nu, random_state=42)\n        clf_nu.fit(X, y)\n\n        # check predictions\n        pred = clf_nu.predict(X)\n        assert set(pred) == set(y)\n\n        # Check prediction scores (decision function)\n        scores = clf_nu.decision_function(X)\n        assert np.all(scores > 0) if y.sum() == len(y) else np.all(scores < 0)\n\n"], "sample_9": ["def test_read_html_table_id_with_error():\n    \"\"\"\n    Test that an error is raised when the table_id is not found.\n    \"\"\"\n    with pytest.raises(core.InconsistentTableError):\n        Table.read(['<table><tr><td>Data</td></tr></table>'],\n                   format='ascii.html', htmldict={'table_id': 1})\n", "def test_html_write_unicode():\n    \"\"\"\n    Test that unicode characters are written correctly in HTML output\n    \"\"\"\n    t = Table([[u'\u0394']], names=['col1'])\n    expected = \"\"\"\\", "def test_write_column_type_inference():\n    # Test that column types are correctly inferred when using\n    # write_table.\n    col1 = ['1', '2', '3']\n    col2 = ['1.0', '2.5', '3.14']\n    col3 = ['True', 'False', 'True']\n    table = Table([col1, col2, col3], format='ascii.html')\n    expected_type = {'C1': 'int', 'C2': 'float', 'C3': 'bool'}\n    with StringIO() as buffer:\n        table.write(buffer, format='html', htmldict={'coltypes': 'infer'})\n    extracted_type = table.coltypes\n    assert expected_type == extracted_type\n", "def test_write_table_html_with_escapes():\n    \"\"\"\n    Test to make sure that the HTML writer properly escapes special\n    characters.\n    \"\"\"\n\n    table = Table([[1, '&'], [2, '<']], names=('a', 'b'))\n    buffer_output = StringIO()\n    ascii.write(table, buffer_output, format='html')\n\n    expected = \"\"\"", "def test_read_html_unicode_escaped():\n    \"\"\"\n    Test reading an HTML table with unicode values escaped with HTML entities\n    \"\"\"\n    table_in = ['<table>',\n                '<tr><td>&#956;</td></tr>',\n                '<tr><td>\u00b5</td></tr>',\n                '</table>']\n    dat = Table.read(table_in, format='ascii.html')\n    assert np.all(dat['col1'] == ['\u00b5', '\u00b5'])\n\n"], "sample_1128": ["def test_point_vel_no_defined_relative_velocities():\n    q1, q2 = dynamicsymbols('q1 q2')\n    N = ReferenceFrame('N')\n    B = ReferenceFrame('B')\n    O = Point('O')\n    P = Point('P')\n    P.set_pos(O, q1 * B.x)\n    raises(ValueError, lambda: P.vel(N))\n", "def test_point_vel_multiple_paths():\n    t = dynamicsymbols._t\n    q1, q2, q3, u1, u2, u3 = dynamicsymbols('q1 q2 q3 u1 u2 u3')\n    N = ReferenceFrame('N')\n    B = ReferenceFrame('B')\n    C = ReferenceFrame('C')\n    O = Point('O')\n    P = Point('P')\n    P1 = Point('P1')\n    P2 = Point('P2')\n\n    O.set_vel(N, u1 * N.x)\n    P.set_pos(O, q1 * N.x)\n    P.set_vel(N, u2 * N.y)\n    P1.set_pos(P, q2 * B.x)\n    P1.set_vel(B, u3 * B.y)\n    P2.set_pos(P1, q3 * C.z)\n    assert P2.vel(N) == (u1 + u2 * q1.diff(t)) * N.x + (u2 + u3 * q2.diff(t)) * N.y + q3.diff(t) * C.z\n\n", "def test_point_vel_circular_motion():\n    t = dynamicsymbols._t\n    q = dynamicsymbols('q')\n    N = ReferenceFrame('N')\n    B = ReferenceFrame('B')\n    O = Point('O')\n    P = O.locatenew('P', 10*B.x)\n    B.set_ang_vel(N, 2*N.y)\n    \n    P.set_vel(B, 2*N.x)\n    assert P.vel(N) == 2*N.x + (2*N.y) * 10*B.x \n\n    P.set_vel(B, q*N.x)\n    O.set_vel(N, 0)\n    P.set_vel(B, q*N.x)\n    assert P.vel(N) == q*N.x + 2*N.y * 10*B.x  \n\n\n\n", "def test_point_vel_multiple_velocities():\n    q1, q2, u1, u2 = dynamicsymbols('q1 q2 u1 u2')\n    N = ReferenceFrame('N')\n    B = ReferenceFrame('B')\n    O = Point('O')\n    O.set_vel(N, u1 * N.x)\n    P = Point('P')\n    P.set_pos(O, q1 * N.x)\n    P.set_vel(B, u2 * B.y)\n    assert P.vel(N) == u1 * N.x + u2 * B.y - q1 * u2 * N.z\n", "def test_auto_vel_multiple_velocities():\n    t = dynamicsymbols._t\n    q1, q2, u1, u2 = dynamicsymbols('q1 q2 u1 u2')\n    N = ReferenceFrame('N')\n    B = ReferenceFrame('B')\n    O = Point('O')\n    O.set_vel(N, u1 * N.x)\n    P = Point('P')\n    P.set_pos(O, q1 * B.x)\n    P.set_vel(B, u2 * B.y)\n    assert P.vel(N) == u1 * N.x + u2 * (B.y.express(N))\n    \n    P1 = Point('P1')\n    P1.set_pos(P, q2 * B.z)\n    P1.set_vel(B, q1 * B.x)\n    assert P1.vel(N) == u1 * N.x + u2 * (B.y.express(N)) + q1 * (B.x.express(N))\n\n"], "sample_263": ["    def test_many_to_many_with_null_fields(self):\n        management.call_command('loaddata', 'null_field_m2m.json', verbosity=0)\n        obj_a = Category.objects.get(title='News Stories')\n        self.assertEqual(obj_a.articles.count(), 2)\n\n        obj_b = Category.objects.get(title='Sports')\n        self.assertEqual(obj_b.articles.count(), 1)\n\n        self._dumpdata_assert(\n            ['fixtures'],\n            '[{\"model\": \"fixtures.category\", \"pk\": 1, \"fields\": {\"description\": \"Latest news stories\", \"title\": \"News Stories\", \"articles\": [2, 3]}}, {\"model\": \"fixtures.category\", \"pk\": 2, \"fields\": {\"description\": \"All about sports\", \"title\": \"Sports\", \"articles\": [1]}}, {\"model\": \"fixtures.article\", \"pk\": 1, \"fields\": {\"headline\": \"Poker has no place on ESPN\", \"pub_date\": \"2006-06-16T12:00:00\"}}, {\"model\": \"fixtures.article\", \"pk\": 2, \"fields\": {\"headline\": \"Time to reform copyright\", \"pub_date\": \"2006-06-16T13:00:00\"}}, {\"model\": \"fixtures.article\", \"pk\": 3, \"fields\": {\"headline\": \"Django pets kitten\", \"pub_date\": \"2006-06-16T14:00:00\"}}]',\n        )\n\n\n", "    def test_loading_with_exclude_app_and_model(self):\n        with patches.patch('django.core.management.commands.loaddata.sys.argv', ['django-admin', 'loaddata', 'fixture1', '--exclude', 'fixtures.Article', '--exclude', 'fixtures.Category']):\n            management.call_command('loaddata', verbosity=0)\n        self.assertFalse(Article.objects.exists())\n        self.assertFalse(Category.objects.exists())\n        self.assertEqual(Site.objects.get().domain, 'example.com')\n\n\n\n\n", "    def test_loading_with_json_api_compatibility(self):\n        management.call_command('loaddata', 'fixture4.json', verbosity=0)\n        self.assertSequenceEqual(\n            Article.objects.values_list('headline', flat=True),\n            ['Django pets kitten'],\n        )\n\n        # Dump the current contents of the database as a JSON fixture\n        self._dumpdata_assert(\n            ['fixtures'],\n            '[{\"pk\": 1, \"model\": \"fixtures.category\", \"fields\": {\"description\": \"Latest news stories\", \"title\": '\n            '\"News Stories\"}}, {\"pk\": 2, \"model\": \"fixtures.article\", \"fields\": {\"headline\": \"Poker has no place '\n            'on ESPN\", \"pub_date\": \"2006-06-16T12:00:00\"}}, {\"pk\": 3, \"model\": \"fixtures.article\", \"fields\": '\n            '{\"headline\": \"Time to reform copyright\", \"pub_date\": \"2006-06-16T13:00:00\"}}, {\"pk\": 4, \"model\": '\n            '\"fixtures.article\", \"fields\": {\"headline\": \"Django pets kitten\", \"pub_date\": \"2006-06-16T14:00:00\"}}]',\n            natural_primary_keys=True,\n            natural_foreign_keys=True,\n        )       \n\n\n\n", "    def test_empty_fixture(self):\n        management.call_command('loaddata', 'empty.json', verbosity=0)\n        self.assertEqual(Article.objects.count(), 0)\n        self.assertEqual(Category.objects.count(), 0)\n        self.assertEqual(Site.objects.count(), 0)\n\n\n\n", "    def test_exclude_app_with_app_name_mismatch(self):\n        with self.assertRaisesMessage(management.CommandError, \"No installed app with label 'fixtures_app' found.\"):\n            management.call_command('loaddata', 'fixture1', exclude=['fixtures_app'], verbosity=0)\n"], "sample_677": ["def test_nested_parentheses(expr: str, matcher: Callable[[str], bool], expected: bool) -> None:\n    assert evaluate(expr, matcher) is expected\n\n", "def test_multi_and_or(expr: str, expected: bool) -> None:\n    matcher = {\"a\": True, \"b\": True, \"c\": False, \"d\": False}.__getitem__\n    assert evaluate(expr, matcher) is expected\n", "def test_leading_underscore_idents(ident: str) -> None:\n    assert evaluate(ident, {ident: True}.__getitem__) \n", "def test_mixed_case_identifiers(expr: str, matcher: dict) -> None:\n    assert evaluate(expr, matcher.__getitem__)\n\n", "def test_parenthesis_grouping(expr: str, expected: bool) -> None:\n    matcher = {\"true\": True, \"false\": False}.__getitem__\n    assert evaluate(expr, matcher) is expected\n\n"], "sample_865": ["def test_sparse_data_with_multiple_sparse_features():\n    n_samples = 100\n    n_features = 5\n    X = data.csr_matrix((np.random.rand(n_samples, n_features),\n                     (np.random.rand(n_samples) * n_features,\n                      np.random.rand(n_samples) * n_features)),\n                     shape=(n_samples, n_features), dtype=np.float32)\n    y = np.random.rand(n_samples)\n    for tree_cls in [DecisionTreeClassifier, DecisionTreeRegressor]:\n        est = tree_cls(random_state=0)\n        est.fit(X, y)\n\n\n\n", "def test_sparse_categorical_regressor():\n    X_sparse = csr_matrix([[1, 2], [3, 4], [5, 6]])\n    y = np.array([0.1, 0.2, 0.3])\n    tree = DecisionTreeRegressor(random_state=0, max_leaf_nodes=2).fit(X_sparse, y)\n    assert_tree_equal(tree.tree_, tree.tree_)  \n", "def test_prune_tree_with_no_samples():\n    X = np.ones((100, 1))\n    y = np.zeros(100)\n    clf = DecisionTreeClassifier(random_state=0)\n    clf.fit(X, y)\n    prune_tree = clf.prune(ccp_alpha=1)\n\n    assert prune_tree.tree_.value.shape == (1, )\n    assert prune_tree.tree_.impurity.shape == (1, )\n    assert prune_tree.tree_.children_left.shape == (1, )\n    assert prune_tree.tree_.children_right.shape == (1, )\n\n", "def test_sparse_data_with_multiple_outputs():\n    X_sparse = csr_matrix([[1, 2], [3, 4], [5, 6]])\n    y1 = np.array([1, 2, 3])\n    y2 = np.array([4, 5, 6])\n    y = np.column_stack((y1, y2))\n\n    clf = DecisionTreeClassifier(random_state=0, max_depth=3).fit(X_sparse, y)\n    y_pred = clf.predict(X_sparse)\n\n\n\n", "def test_sparse_classification_with_categorical_features():\n    from sklearn.datasets import make_classification\n    from sklearn.preprocessing import OneHotEncoder\n    from scipy.sparse import csr_matrix\n\n    X, y = make_classification(n_samples=100, n_features=5,\n                               n_informative=3, random_state=0)\n    \n    # Create one categorical feature for testing\n    categorical_indices = [2]\n    encoder = OneHotEncoder(handle_unknown=\"ignore\")\n    X_categorical = encoder.fit_transform(X[:, categorical_indices]).toarray()\n\n    X_sparse = csr_matrix(X)\n    X_sparse_categorical = csr_matrix(X_categorical)\n\n    # Combine the categorical and numerical features\n    X_sparse_combined = np.hstack((X_sparse, X_sparse_categorical))\n    X_sparse_categorical_combined = csr_matrix(X_sparse_combined)\n    \n    clf = DecisionTreeClassifier(\n        criterion=\"gini\", splitter='best', random_state=42\n    )\n\n    # Train with sparse features\n    clf.fit(X_sparse_categorical_combined, y)\n\n    # Check if predictions are correct\n    y_pred = clf.predict(X_sparse_categorical_combined)\n    assert (y_pred == y).all()\n\n\n"], "sample_1033": ["def test_issue_16150():\n    from sympy.abc import x, y\n    assert (x + y).subs(x, 1, y, 2) == 3\n    assert ((x + y).subs(x, 1, y, 2)).is_real\n    assert (x + y).subs(x, 1, y, 2).is_finite\n\n\n\n", "def test_issue_6793():\n    assert (2*x + 3*y).evalf(20) == 2*x.evalf(20) + 3*y.evalf(20)\n    assert (2*x + 3*y).evalf(10) == 2*x.evalf(10) + 3*y.evalf(10)\n", "def test_issue_16314():\n    from sympy import Symbol, FiniteSet, Integer, pi\n    a = Symbol('a')\n    b = Symbol('b')\n    c = Symbol('c')\n    assert (FiniteSet(a, b, c)).union(FiniteSet(a, b)) == FiniteSet(a, b, c)\n    assert (FiniteSet(a, b, c)).intersection(FiniteSet(a, b)) == FiniteSet(a, b)\n    assert (FiniteSet(a, b, c)).difference(FiniteSet(a, b)) == FiniteSet(c)\n    assert (FiniteSet(a, b, c)).is_subset(FiniteSet(a, b, c, d))\n    assert not (FiniteSet(a, b, c)).is_subset(FiniteSet(d))\n\n    assert (FiniteSet(1, 2, 3)).is_subset(FiniteSet(1, 2, 3, 4))\n    assert not (FiniteSet(1, 2, 3)).is_subset(FiniteSet(1, 4))\n    assert (FiniteSet(1, 2, 3)).is_superset(FiniteSet(1, 2, 3))\n    assert (FiniteSet(1, 2, 3)).is_superset(FiniteSet(1))\n    assert not (FiniteSet(1, 2, 3)).is_superset(FiniteSet(4))\n\n\n    assert (FiniteSet(a, b, c)).issubset(FiniteSet(a, b, c, d)) is True\n    assert (FiniteSet(a, b)).issubset(FiniteSet(a, b, c, d)) is True\n\n    assert FiniteSet(1, 2, 3).is_disjoint(FiniteSet(4, 5)) is True\n    assert FiniteSet(1, 2, 3).is_disjoint(FiniteSet(1, 2, 3, 4)) is False  \n\n    assert (FiniteSet(pi, 1)).is_disjoint(FiniteSet(2, 3))  \n\n\n    \n", "compilation error", "    def test_divmod_with_symbols():\n        x, y = symbols('x y', real=True)\n        assert divmod(x, y) == (x // y, x % y)\n\n"], "sample_1119": ["def test_inverse_with_symbols():\n    a, b = symbols('a b')\n    A = Matrix([[a, b], [b, a]])\n    inverse_A = A.I\n    assert inverse_A.args == (A,)\n    assert inverse_A.shape == (2, 2)\n    assert inverse_A * A == Identity(2)\n    assert A * inverse_A == Identity(2)\n", "def test_jordan_form():\n    from sympy.matrices.densematrix import DenseMatrix\n    A = Matrix([[2, 1], [1, 2]])\n    P, J = A.jordan_form()\n    assert isinstance(P, DenseMatrix)\n    assert isinstance(J, DenseMatrix)\n    assert A == P * J * P.inv()\n\n    B = Matrix([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n    P, J = B.jordan_form()\n    assert P == Identity(3)\n    assert J == B  \n    \n    C = Matrix([[1, 2, 3], [0, 4, 5], [0, 0, 6]])\n    P, J = C.jordan_form()\n    assert isinstance(P, DenseMatrix)\n    assert isinstance(J, DenseMatrix)\n    assert C == P * J * P.inv()\n", "    def test_inv_mod():\n        a = Matrix([[1, 2], [3, 4]])\n        b = Matrix([[5, 6], [7, 8]])\n        assert a.inv_mod(5).doit() == b.doit()\n\n\n\n", "def test_inverse_simplify():\n    A = Matrix([[1, 2], [3, 4]])\n    B = Matrix([[5, 6], [7, 8]])\n    C = A * B\n    D = C.inverse()\n    assert D.simplify() == simplify(D)  \n\n    with raises(NonInvertibleMatrixError):\n      simplify(D*C)\n", "def test_jordan_form():\n    from sympy.matrices import jordan_form\n    A = Matrix([[1, 1], [0, 1]])\n    J, P = jordan_form(A)\n    assert J.is_jordan_block()\n    assert P.det() != 0\n    assert P*J*P.inv() == A\n    \n    B = Matrix([[2, 1], [0, 2]])\n    J, P = jordan_form(B)\n    assert J.is_jordan_block()\n    assert P.det() != 0\n    assert P*J*P.inv() == B\n    \n    C = Matrix([[1, 0], [0, 2]])\n    J, P = jordan_form(C)\n    assert J.is_jordan_block()\n    assert P.det() != 0\n    assert P*J*P.inv() == C\n    \n    D = Matrix([[1, 1, 1], [0, 2, 1], [0, 0, 3]])\n    J, P = jordan_form(D)\n    assert all(b.is_upper_triangular() for b in J.blocks)\n    assert P.det() != 0\n    assert P*J*P.inv() == D\n\n"], "sample_417": ["    def test_floatformat_nan(self):\n        self.assertEqual(floatformat(float(\"nan\")), \"nan\")\n", "    def test_format_negative_decimal(self):\n        with translation.override(\"en\"):\n            self.assertEqual(floatformat(-123.456, \"2\"), \"-123.46\")\n            self.assertEqual(floatformat(-123.456789, 2), \"-123.46\")\n", "    def test_currency_formatting(self):\n        with translation.override(\"en\"):\n            self.assertEqual(floatformat(12345.67, \"c\"), \"$12,345.67\")\n            self.assertEqual(floatformat(12345.67, \"c2\"), \"$12,345.67\")\n            self.assertEqual(floatformat(12345.67, \"c0\"), \"$12346\")\n            self.assertEqual(floatformat(12345.67, \"c.2\"), \"$12,345.67\")\n            self.assertEqual(floatformat(12345.67, \"c.0\"), \"$12,346\")\n\n        with translation.override(\"de\", deactivate=True):\n            self.assertEqual(floatformat(12345.67, \"c\"), \"12.345,67 \u20ac\")\n            self.assertEqual(floatformat(12345.67, \"c2\"), \"12.345,67 \u20ac\")\n            self.assertEqual(floatformat(12345.67, \"c0\"), \"12.346 \u20ac\")\n            self.assertEqual(floatformat(12345.67, \"c.2\"), \"12.345,67 \u20ac\")\n            self.assertEqual(floatformat(12345.67, \"c.0\"), \"12.346 \u20ac\")\n\n\n\n", "    def test_custom_decimal_context(self):\n        with localcontext() as ctx:\n            ctx.prec = 3\n            ctx.rounding = 'ROUND_HALF_UP'\n            self.assertEqual(floatformat(1.2345, 2), \"1.23\")\n            self.assertEqual(floatformat(123.4567, 3), \"123.457\")\n            self.assertEqual(floatformat(123.456789, 2), \"123.46\")\n", "    def test_floatformat_zero_decimal_places(self):\n        self.assertEqual(floatformat(1234.5678, 0), \"1235\")\n        self.assertEqual(floatformat(-1234.5678, 0), \"-1235\")\n        self.assertEqual(floatformat(0, 0), \"0\")\n\n\n\n    \n"], "sample_913": ["    def test_pyfunction_signature_full_py38(app):\n        # case: separator at the end\n        text = \".. py:function:: hello(*, a)\"\n        doctree = restructuredtext.parse(app, text)\n        assert_node(doctree[1][0][1],\n                    [desc_parameterlist, ([desc_parameter, desc_sig_operator, \"*\"],\n                                          [desc_parameter, desc_sig_name, \"a\",\n                                          desc_sig_operator, \"=\"],\n                                          [nodes.inline, \"None\"])])\n", "def test_pyfunction_signature_return_annotation(app):\n    text = \".. py:function:: compile(source [, filename [, symbol]]) -> ast.Module\\n\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"compile\"],\n                                                    desc_parameterlist,\n                                                    [desc_returns, pending_xref, \"ast.Module\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n", "def test_pyfunction_signature_keyword_only_arguments(app):\n    text = (\".. py:function:: compile\\n\"\n            \"   :keyword_only:\\n\"\n            \"   :param file: The source file to compile.\\n\"\n            \"   :param mode: The compilation mode.\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"compile\"],\n                                                    desc_parameterlist,\n                                                    [desc_returns, pending_xref, \"ast object\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n    assert_node(doctree[1][0][1],\n                ([desc_parameter, desc_keyword_argument, 'file'],\n                 [desc_parameter, desc_keyword_argument, 'mode']))\n", "def test_pyfunction_signature_no_args(app):\n    text = \".. py:function:: greet()\\n\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"greet\"],\n                                                    desc_parameterlist, ()),\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n\n\n\n", "    def test_class_method_function_data_exception_index(app, text, expected):\n        restructuredtext.parse(app, text)\n        index = PythonModuleIndex(app.env.get_domain('py'))\n        assert index.generate() == expected\n\n\n"], "sample_355": ["    def test_allow_all_backend_overrides_rowlevel(self):\n        self.user.is_active = False\n        self.user.save()\n        self.assertEqual(authenticate(**self.user_credentials), self.user)\n\n", "    def test_permission_denied_with_inactive_user(self):\n        self.assertFalse(self.user.is_active)\n        with self.assertRaises(PermissionDenied):\n            self.user.has_perm('auth.change_user')\n", "    def test_is_anonymous(self):\n        self.assertTrue(self.user.is_anonymous)\n", "    def test_anonymous_user_not_allowed(self):\n        with self.settings(AUTHENTICATION_BACKENDS=['django.contrib.auth.backends.ModelBackend']):\n            with self.assertRaises(PermissionDenied):\n                self.client.post('/accounts/logout/')\n\n", "    def test_has_perm(self):\n        with self.assertRaises(PermissionDenied):\n            self.user.has_perm('auth.view_group')\n"], "sample_525": ["def test_subplots_adjust_with_squeeze():\n    fig, axes = plt.subplots(1, 2)\n\n    # Test squeeze=False\n    fig.subplots_adjust(wspace=0.5, hspace=0.3, bottom=0.1, top=0.9,\n                       left=0.1, right=0.9, squeeze=False)\n    ax1 = fig.add_subplot(1, 2, 1)\n    ax2 = fig.add_subplot(1, 2, 2)\n    assert ax1.get_position().x0 > 0.1\n    assert ax1.get_position().y0 > 0.1\n    assert ax2.get_position().x1 < 0.9\n    assert ax2.get_position().y1 < 0.9\n\n    # Test squeeze=True\n    fig.subplots_adjust(wspace=0.5, hspace=0.3, bottom=0.1, top=0.9,\n                       left=0.1, right=0.9, squeeze=True)\n    ax3 = fig.add_subplot(1, 2, 1)\n    ax4 = fig.add_subplot(1, 2, 2)\n    assert ax3.get_position().x0 == 0.1\n    assert ax3.get_position().y0 == 0.1\n    assert ax4.get_position().x1 == 0.9\n    assert ax4.get_position().y1 == 0.9\n\n\n\n", "compilation error", "def test_unpickle_with_figsize():\n    fig = Figure(figsize=(10, 5))\n    fig2 = pickle.loads(pickle.dumps(fig))\n    assert fig2.get_size_inches() == (10, 5)\n", "compilation error", "def test_unpickle_with_subplotspec():\n    fig = Figure(constrained_layout=True)\n    gs = fig.add_gridspec(2, 2)\n    ax1 = fig.add_subplot(gs[0, 0])\n    ax2 = fig.add_subplot(gs[0, 1])\n    fig2 = pickle.loads(pickle.dumps(fig))\n\n    assert isinstance(fig2.axes[0], Axes)\n    assert isinstance(fig2.axes[1], Axes)\n\n\n\n"], "sample_1023": ["compilation error", "def test_prime_in_range():\n    sieve._reset()\n    assert all(p in sieve for p in range(2, 11))\n    sieve.extend(300)\n    assert all(p in sieve for p in range(2, 301))\n    assert 299 in sieve\n    assert 300 not in sieve\n\n\n\n", "def test_cycle_length_for_small_range():\n    func = lambda i: (i**2 + 1) % 5\n    for i in range(5):\n        for mu_val in range(12):\n            for lam_val in range(12):\n                result = cycle_length(func, i, lam=lam_val, mu=mu_val)\n                if lam_val is not None:\n                    assert lam_val == result[0]\n                if mu_val is not None:\n                    assert mu_val == result[1]\n\n\n\n\n", "compilation error", "compilation error"], "sample_582": ["    def empty():\n        pass\n", "def test_cli_command_args(app):\n    @app.cli.command()\n    @click.argument(\"name\")\n        click.echo(f\"Hello, {name}!\")\n\n    result = app.test_cli_runner().invoke(args=[\"hello\", \"World\"])\n    assert \"Hello, World!\" in result.output\n", "def test_cli_group_name(app):\n    \"\"\"Test custom cli group names work as expected.\"\"\"\n    custom = Blueprint(\"custom\", __name__, cli_group=\"my_customized\")\n    @custom.cli.command(\"custom\")\n        click.echo(\"custom_result\")\n\n    app.register_blueprint(custom)\n\n    app_runner = app.test_cli_runner()\n\n    result = app_runner.invoke(args=[\"my_customized\", \"custom\"])\n    assert \"custom_result\" in result.output\n", "def test_cli_order(app):\n    custom1 = Blueprint(\"custom1\", __name__, cli_group=\"custom\")\n    custom2 = Blueprint(\"custom2\", __name__, cli_group=\"custom\")\n    merged = Blueprint(\"merged\", __name__, cli_group=None)\n\n    @custom1.cli.command(\"custom1\")\n        click.echo(\"custom1_result\")\n\n    @custom2.cli.command(\"custom2\")\n        click.echo(\"custom2_result\")\n\n    @merged.cli.command(\"merged\")\n        click.echo(\"merged_result\")\n\n    app.register_blueprint(custom1)\n    app.register_blueprint(custom2)\n    app.register_blueprint(merged)\n\n    app_runner = app.test_cli_runner()\n\n    result = app_runner.invoke(args=[\"custom\", \"custom1\"])\n    assert \"custom1_result\" in result.output\n\n    result = app_runner.invoke(args=[\"custom\", \"custom2\"])\n    assert \"custom2_result\" in result.output\n\n    result = app_runner.invoke(args=[\"merged\"])\n    assert \"merged_result\" in result.output\n", "def test_cli_command_sorting(app):\n    \"\"\"Test command sorting when blueprints are registered.\"\"\"\n    custom = Blueprint(\"custom\", __name__, cli_group=\"customized\")\n    nested = Blueprint(\"nested\", __name__)\n\n    @custom.cli.command(\"custom\")\n        click.echo(\"custom_result\")\n\n    @nested.cli.command(\"nested\")\n        click.echo(\"nested_result\")\n\n    app.register_blueprint(nested)\n    app.register_blueprint(custom)\n\n    app_runner = app.test_cli_runner()\n    result = app_runner.invoke(args=[\"--help\"])\n    commands = result.output.splitlines()\n    # Check if commands are sorted alphabetically\n    assert commands[2].startswith(\"Customized\")\n    assert commands[4].startswith(\"Nested\")\n"], "sample_773": ["def test_logistic_regression_penalty_cv(penalty):\n    # Test penalty parameter when used with LogisticRegressionCV\n\n    X, y = make_classification(random_state=0)\n    Cs = np.logspace(-4, 4, 3)\n\n    lrcv = LogisticRegressionCV(penalty=penalty, Cs=Cs, solver='saga', cv=5,\n                                random_state=0)\n    lrcv.fit(X, y)\n\n    assert lrcv.penalty_ == penalty\n    assert_array_equal(lrcv.coef_.shape, (1, X.shape[1]))\n\n\n\n", "def test_logistic_regression_l1_ratio_elasticnet_multi_class():\n    # Test that l1_ratio works as expected when using elasticnet penalty\n    # and multi_class='multinomial'\n    X, y = make_classification(n_samples=200, n_classes=3, n_informative=3,\n                               n_features=10, random_state=0)\n    Cs = np.logspace(-3, 3, 5)\n    l1_ratios = np.linspace(0, 1, 5)\n\n    lrcv = LogisticRegressionCV(penalty='elasticnet', Cs=Cs, solver='saga',\n                cv=5, l1_ratios=l1_ratios, random_state=0,\n                multi_class='multinomial')\n    lrcv.fit(X, y)\n\n    for i, ratio in enumerate(l1_ratios):\n        for j, c in enumerate(Cs):\n            assert (lrcv.coef_[0][i, :].shape ==\n                    (X.shape[1],))  # Check coefs shapes are correct\n            assert (lrcv.coef_[1][i, :].shape ==\n                    (X.shape[1],))  # Check coefs shapes are correct\n            assert np.abs(lrcv.coef_[0][i, :]).sum() > 0\n            assert np.abs(lrcv.coef_[1][i, :]).sum() > 0\n            assert np.abs(lrcv.coef_[0][i, :]).sum() / np.abs(lrcv.coef_[1][i, :]).sum() == ratio\n\n\n\n", "def test_logistic_regression_coef_tol(penalty):\n    # Test that coefficients are properly updated when tol is specified, even\n    # if the optimizer doesn't reduce the objective significantly\n\n    X, y = make_classification(n_samples=1000, n_features=10, random_state=0)\n\n    for solver in ['liblinear', 'saga']:\n        # liblinear does not use the 'tol' parameter\n        if solver == 'liblinear' and penalty == 'elasticnet':\n            continue\n\n        lr = LogisticRegression(penalty=penalty, solver=solver, tol=1e-6,\n                                 random_state=0)\n        lr.fit(X, y)\n        coefs0 = lr.coef_\n        lr = LogisticRegression(penalty=penalty, solver=solver, tol=1e-10,\n                                 random_state=0)\n        lr.fit(X, y)\n        coefs1 = lr.coef_\n\n        # Check that coeefs changed, but not much\n        assert np.any(coefs0 != coefs1)\n        assert np.max(np.abs(coefs0 - coefs1)) < 1e-5\n", "def test_penalty_l1_l2_equivalence_with_lra(solver):\n    # Make sure that penalty='l1_l2' is equivalent to setting l1_ratio=0.5 \n    # with penalty='elasticnet'\n    X, y = make_classification(n_samples=1000, random_state=0)\n\n    lr_l1_l2 = LogisticRegression(penalty='l1_l2', solver=solver, C=1.,\n                random_state=0)\n    lr_elasticnet = LogisticRegression(penalty='elasticnet', solver=solver, C=1.,\n                                      l1_ratio=0.5, random_state=0)\n\n    lr_l1_l2.fit(X, y)\n    lr_elasticnet.fit(X, y)\n    assert_array_almost_equal(lr_l1_l2.coef_, lr_elasticnet.coef_, \n                             decimal=2)\n", "def test_LogisticRegression_n_jobs_with_multi_class_fit():\n    # n_jobs works as expected for multi-class LogisticRegression when\n    # fitting with fit_intercept=False.\n\n    n_jobs = -1\n    n_samples = 1000\n    n_classes = 3\n    n_features = 20\n    X, y = make_classification(n_samples=n_samples, n_classes=n_classes,\n            n_informative=n_features, random_state=0)\n    \n    for solver in ('lbfgs', 'sag', 'saga'):\n        lr_njobs_1 = LogisticRegression(penalty='l2', solver=solver,\n                                         random_state=0, n_jobs=1,\n                                         fit_intercept=False).fit(X, y)\n        lr_njobs_inf = LogisticRegression(penalty='l2', solver=solver,\n                                         random_state=0, n_jobs=-1,\n                                         fit_intercept=False).fit(X, y)\n        assert_array_almost_equal(lr_njobs_1.coef_, lr_njobs_inf.coef_)\n\n\n"], "sample_319": ["    def test_operation_with_custom_name(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\n                    \"Person\", fields=[], name=\"my_model_name\"\n                ),\n            ]\n\n        migration = Migration(\"some_migration\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"my_model_name\")\n\n\n", "    def test_alter_index_together_and_add_field(self):\n        changes = self.get_changes(\n            [AutodetectorTests.author_empty, self.book_index_together],\n            [AutodetectorTests.author_empty, self.book_index_together_3],\n        )\n        self.assertNumberMigrations(changes, \"otherapp\", 1)\n        self.assertOperationTypes(\n            changes,\n            \"otherapp\",\n            0,\n            [\"AlterIndexTogether\", \"AddField\"],\n        )\n        self.assertOperationAttributes(\n            changes,\n            \"otherapp\",\n            0,\n            0,\n            name=\"book\",\n            index_together={(\"title\", \"author\")},\n        )\n        self.assertOperationAttributes(\n            changes,\n            \"otherapp\",\n            0,\n            1,\n            name=\"book\",\n            field_name=\"newfield\",\n        )\n", "    def test_alter_field_order_without_index_together(self):\n        changes = self.get_changes(\n            [\n                AutodetectorTests.author_empty,\n                ModelState(\n                    \"otherapp\",\n                    \"Author\",\n                    [\n                        (\"id\", models.AutoField(primary_key=True)),\n                        (\"name\", models.CharField(max_length=200)),\n                        (\"age\", models.IntegerField()),\n                    ],\n                    options={\n                        \"indexes\": [\n                            models.Index(fields=[\"age\"])\n                        ]\n                    },\n                ),\n            ],\n            [\n                AutodetectorTests.author_empty,\n                ModelState(\n                    \"otherapp\",\n                    \"Author\",\n                    [\n                        (\"id\", models.AutoField(primary_key=True)),\n                        (\"age\", models.IntegerField()),\n                        (\"name\", models.CharField(max_length=200)),\n                    ],\n                    options={\n                        \"indexes\": [\n                            models.Index(fields=[\"age\"])\n                        ]\n                    },\n                ),\n            ],\n        )\n        self.assertNumberMigrations(changes, \"otherapp\", 1)\n        self.assertOperationTypes(\n            changes,\n            \"otherapp\",\n            0,\n            [\"AlterOrderWithRespectTo\", \"AlterIndexTogether\"],\n        )\n\n\n\n", "    def test_custom_operation_name(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\n                    name=\"Person\", fields=[], custom_name=\"MyNewPerson\"\n                ),\n            ]\n\n        migration = Migration(\"0001_initial\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"mynewperson\")\n", "    def test_operation_with_null_name(self):\n        class Migration(migrations.Migration):\n            operations = [migrations.RenameField(\"Person\", \"name\", \"new_name\", db_constraint=False)]\n\n        migration = Migration(\"some_migration\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"rename_person_name_to_new_name\")\n"], "sample_535": ["def test_table_bboxes():\n    fig = plt.figure()\n\n    ax1 = fig.add_subplot(1, 2, 1)\n    ax1.axis('off')\n    table = ax1.table(cellText=[['Data 1', 'Data 2']],\n                      loc='center',\n                      bbox=[0.1, 0.1, 0.4, 0.4])\n\n    ax2 = fig.add_subplot(1, 2, 2)\n    ax2.axis('off')\n    table2 = ax2.table(cellText=[['Data 1', 'Data 2']],\n                       loc='center',\n                       bbox=Bbox.from_extents(0.1, 0.1, 0.4, 0.4))\n", "def test_table_bbox_auto():\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.set_xlim(0, 10)\n    ax.set_ylim(0, 10)\n\n    table = ax.table(cellText=[[1, 2, 3], [4, 5, 6]],\n                     rowLabels=['A', 'B'],\n                     colLabels=['C', 'D', 'E'],\n                     loc='center', bbox=None)\n    fig.tight_layout()\n", "def test_bbox_loc():\n    data = [[2, 3],\n            [4, 5]]\n\n    col_labels = ('Foo', 'Bar')\n    row_labels = ('Ada', 'Bob')\n\n    cell_text = [[f\"{x}\" for x in row] for row in data]\n\n    fig = plt.figure()\n    ax1 = fig.add_subplot(1, 2, 1)\n    ax1.axis('off')\n    ax1.table(cellText=cell_text,\n              rowLabels=row_labels,\n              colLabels=col_labels,\n              loc='center',\n              bbox=[0.1, 0.2, 0.8, 0.6]\n              )\n\n    ax2 = fig.add_subplot(1, 2, 2)\n    ax2.axis('off')\n    ax2.table(cellText=cell_text,\n              rowLabels=row_labels,\n              colLabels=col_labels,\n              loc='bottom',\n              bbox=[0.1, 0.2, 0.8, 0.6]\n              )\n\n", "def test_clip_on():\n    fig, ax = plt.subplots()\n    table = Table(ax)\n\n    # add some cells out of the bounds\n    table.add_cell(0, 0, width=1.2, height=1.2)\n    table.add_cell(0, 1, width=1.2, height=1.2)\n\n    table.set_clip_on(True)\n    plt.show()\n\n\n\n", "def test_table_bbox_none():\n    data = [[2, 3],\n            [4, 5]]\n\n    col_labels = ('Foo', 'Bar')\n    row_labels = ('Ada', 'Bob')\n\n    cell_text = [[f\"{x}\" for x in row] for row in data]\n\n    fig_test = plt.figure()\n    ax_test = fig_test.subplots()\n    ax_test.table(cellText=cell_text,\n                  rowLabels=row_labels,\n                  colLabels=col_labels,\n                  loc='center',\n                  bbox=None\n                  )\n\n"], "sample_123": ["    def test_no_extra_slashes(self):\n        self.assertEqual(escape_leading_slashes('/path/to/resource'), '/path/to/resource')\n\n\n\n", "    def test_relative_urls(self):\n        tests = (\n            ('/foo', '/foo'),\n            ('/foo/', '/foo%2F'),\n            ('foo', 'foo'),\n        )\n        for url, expected in tests:\n            with self.subTest(url=url):\n                self.assertEqual(escape_leading_slashes(url), expected)\n", "    def test_query_string(self):\n        tests = (\n            ('//example.com?query=value', '/%2Fexample.com%3Fquery%3Dvalue'),\n            ('//example.com?query=value&another=param', '/%2Fexample.com%3Fquery%3Dvalue%26another%3Dparam'),\n        )\n        for url, expected in tests:\n            with self.subTest(url=url):\n                self.assertEqual(escape_leading_slashes(url), expected)\n", "    def test_url_has_allowed_host_and_scheme_with_query(self):\n        self.assertTrue(url_has_allowed_host_and_scheme(\n            'https://example.com/p?q=1', allowed_hosts={'example.com'}\n        ))\n        self.assertFalse(url_has_allowed_host_and_scheme(\n            'https://example.com/p?q=1', allowed_hosts={'example2.com'}\n        ))\n", "    def test_relative_urls(self):\n        tests = (\n            ('/example.com', '/example.com'),\n            ('/foo/bar', '/foo/bar'),\n            ('/foo', '/foo'),\n        )\n        for url, expected in tests:\n            with self.subTest(url=url):\n                self.assertEqual(escape_leading_slashes(url), expected)\n\n\n\n"], "sample_1038": ["def test_Kronecker_product():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 3, 3)\n    C = kronecker_product(A, B)\n    assert isinstance(C, MatrixExpr)\n    assert C.shape == (6, 6)\n    assert C[0, 0] == A[0, 0]*B[0, 0]\n    assert C[0, 1] == A[0, 0]*B[0, 1]\n    assert C[0, 2] == A[0, 0]*B[0, 2]\n    assert C[0, 3] == A[0, 1]*B[0, 0]\n    assert C[0, 4] == A[0, 1]*B[0, 1]\n    assert C[0, 5] == A[0, 1]*B[0, 2]\n    assert C[1, 0] == A[1, 0]*B[0, 0]\n    assert C[1, 1] == A[1, 0]*B[0, 1]\n    assert C[1, 2] == A[1, 0]*B[0, 2]\n    assert C[1, 3] == A[1, 1]*B[0, 0]\n    assert C[1, 4] == A[1, 1]*B[0, 1]\n    assert C[1, 5] == A[1, 1]*B[0, 2]\n\n\n\n", "def test_derivative_computation():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    w = MatrixSymbol('w', 2, 1)\n\n    # Test derivative with respect to a matrix element\n    d_dA = diff(A[0, 0] * w.T * B, A)\n    assert d_dA[0, 0] == w.T * B\n\n    d_dB = diff(A * B[1, 0], B)\n    assert d_dB.args == (A, 'B[1, 0]')\n    assert d_dB[0, 0] == A[0, 0]\n\n    # Test derivative of a matrix expression\n    M = A * B * w\n    dM_dw = diff(M, w)\n    assert len(dM_dw.args) == 1\n    assert dM_dw[0, 0] == A * B\n    \n    # Test chain rule\n    M = A * B\n    N = M * w\n    dN_dA = diff(N, A)\n    assert isinstance(dN_dA, MatMul) and dN_dA.args == (B*w, )\n\n    # Test derivative of a matrix power\n    M = A**2\n    dM_dA = diff(M, A)\n    assert isinstance(dM_dA, MatMul) and dM_dA.args == (2*A, )\n\n\n\n", "compilation error", "compilation error", "def test_transpose_symbolic():\n    A = MatrixSymbol('A', 3, 2)\n    B = MatrixSymbol('B', 2, 3)\n    C = MatrixSymbol('C', 2, 2)\n    assert Transpose(A).shape == (2, 3)\n    assert (A + B).transpose() == Transpose(A) + Transpose(B)\n    assert (A*B).transpose() == Transpose(B).transpose() * Transpose(A).transpose()\n    assert Transpose(Transpose(A)) == A\n    assert Transpose(C**2) == Transpose(C).transpose() * Transpose(C).transpose()\n    assert Transpose(A[1, :]) == A[:, 1].transpose()\n    assert Transpose(A[:, 0]) == A[0, :].transpose()\n\n\n"], "sample_533": ["def test_contour_extend():\n    fig, ax = plt.subplots()\n    x = np.arange(-10, 10, 0.1)\n    y = np.arange(-10, 10, 0.1)\n    X, Y = np.meshgrid(x, y)\n    Z = np.exp(-(X**2) - (Y**2))\n\n    # Test 'neither'\n    cs = ax.contourf(X, Y, Z, extend='neither')\n    assert cs.get_clim() == (Z.min(), Z.max())\n    ax.set_xlim(-11, 11)\n    ax.set_ylim(-11, 11)\n\n    # Test 'both'\n    cs = ax.contourf(X, Y, Z, extend='both')\n    assert cs.get_clim() == (Z.min(), Z.max())\n    ax.set_xlim(-11, 11)\n    ax.set_ylim(-11, 11)\n\n    # Test 'min'\n    cs = ax.contourf(X, Y, Z, extend='min')\n    assert cs.get_clim() == (Z.min(), Z.max())\n    ax.set_xlim(-11, 11)\n    ax.set_ylim(-11, 11)\n\n    # Test 'max'\n    cs = ax.contourf(X, Y, Z, extend='max')\n    assert cs.get_clim() == (Z.min(), Z.max())\n    ax.set_xlim(-11, 11)\n    ax.set_ylim(-11, 11)\n\n\n", "def test_contour_fill_missing_data():\n    # Check that contours handle missing values correctly\n\n    x, y = np.mgrid[0:1:0.1, 0:1:0.1]\n    z = np.sin(np.pi * x) * np.cos(np.pi * y)\n    z[::3, ::3] = np.nan  \n\n    fig, ax = plt.subplots()\n    cs = ax.contour(x, y, z)\n    ax.clabel(cs)\n    assert not np.isnan(cs.levels).any()\n\n\n\n", "def test_contour_handle_nan():\n    x, y = np.meshgrid(np.linspace(-1, 1, 10), np.linspace(-1, 1, 10))\n    z = np.ones((10, 10))\n    z[::2, ::2] = np.nan\n    with pytest.warns(UserWarning, match=r\"Contouring with NaN values\"):\n        plt.contour(x, y, z)\n\n\n\n    \n", "def test_contour_with_large_array():\n    # Test that contour works with larger arrays without error\n    x = np.arange(1000).reshape(100, 10)\n    y = np.arange(1000).reshape(10, 100)\n    z = np.sin(x * y)\n\n    plt.contour(x, y, z)\n    plt.show()\n\n\n\n", "def test_contour_filled_levels_with_nan():\n    x, y = np.mgrid[0:10, 0:10]\n    z = np.sin(x) * np.cos(y)\n    z[::2, ::2] = np.nan\n\n    with pytest.warns(UserWarning, match=r\"Contour levels are.*nan\"):\n        plt.contourf(x, y, z)\n\n\n"], "sample_87": ["    def test_get_file_mtime(self):\n        mtime = self.reloader.get_file_mtime(self.existing_file)\n        self.assertIsInstance(mtime, float)\n        self.assertGreater(mtime, 0)\n", "    def test_stat_file_changed_calls_notify_for_changed_files(self):\n        with mock.patch.object(self.reloader, 'notify_file_changed') as notify_mock:\n            self.reloader.stat_file_changed(self.existing_file)\n        self.assertEqual(notify_mock.call_count, 1)\n", "    def test_run_loop_handles_file_addition(self):\n        self.reloader.watch_file(self.existing_file)\n        with self.tick_twice():\n            new_file = self.ensure_file(self.tempdir / 'new_file.py')\n            self.reloader.watch_file(new_file)\n        watched_files = list(self.reloader.watched_files())\n        self.assertIn(new_file, watched_files)\n\n\n\n", "    def test_file_changed_signal_is_emitted(self):\n        with mock.patch.object(self.reloader, '_update_watched_files') as mocked_update:\n            file_changed = mock.Mock()\n            self.reloader.file_changed.connect(file_changed)\n            self.increment_mtime(self.existing_file)\n            mocked_update.assert_called()\n            file_changed.assert_called_once_with(self.existing_file)\n\n", "    def test_run_loop_with_reloader_events(self):\n            yield\n            self.reloader.signal_change(self.existing_file)\n            return\n\n        with mock.patch.object(self.reloader, 'tick', side_effect=mocked_tick) as tick:\n            with self.assertRaises(autoreload.RestartRequest) as exc:\n                self.reloader.run_loop()\n        self.assertEqual(tick.call_count, 1)\n\n\n\n"], "sample_283": ["    def test_empty_database_name(self):\n        self.assertEqual(\n            self.settings_to_cmd_args_env({}),\n            (['psql', '-U', '', '-h', '', '-p', '', 'postgres'], {}),\n        )\n", "    def test_connect_to_default_database(self):\n        self.assertEqual(\n            self.settings_to_cmd_args_env({}),\n            (['psql', 'postgres'], {}),\n        )\n", "    def test_default_database(self):\n        self.assertEqual(\n            self.settings_to_cmd_args_env({}),\n            (['psql', 'postgres'], {}),\n        )\n", "    def test_empty_username(self):\n        with self.assertRaises(ValueError):\n            self.settings_to_cmd_args_env({'NAME': 'dbname'})\n", "    def test_sslmode_defaults(self):\n        \"\"\"\n        SSL mode defaults to 'disable' if not explicitly set.\n        \"\"\"\n        args, env = self.settings_to_cmd_args_env({'NAME': 'dbname'})\n        self.assertEqual(env.get('PGSSLMODE'), 'disable')\n\n\n"], "sample_217": ["    def test_media_inheritance_from_property_with_extend(self):\n        class MyWidget1(TextInput):\n            class Media:\n                css = {\n                    'all': ('path/to/css1', '/path/to/css2')\n                }\n                js = ('/path/to/js1', 'http://media.other.com/path/to/js2', 'https://secure.other.com/path/to/js3')\n\n        class MyWidget4(TextInput):\n                return Media(css={'all': ('/some/path',)}, js=('/some/js',))\n            media = property(_media)\n\n        class MyWidget9(MyWidget4):\n            class Media:\n                extend = True\n                css = {\n                    'all': ('/other/path',)\n                }\n                js = ('/other/js',)\n\n        w9 = MyWidget9()\n        self.assertEqual(\n            str(w9.media),\n            \"\"\"<link href=\"/some/path\" type=\"text/css\" media=\"all\" rel=\"stylesheet\">", "    def test_custom_media_type(self):\n        class MyCustomWidget(TextInput):\n            class Media:\n                custom = ('/path/to/custom.css',)\n        widget = MyCustomWidget()\n        media = widget.media\n        self.assertEqual(media._custom, ('/path/to/custom.css',))\n        self.assertIn('custom', media._media_types)\n\n\n\n", "    def test_add_media_string_input(self):\n        widget1 = Media(css={'screen': ['a.css']}, js=['a.js'])\n        widget2 = Media({'screen': ['b.css'], 'all': ['b.js']})\n        merged = widget1 + widget2\n        self.assertEqual(merged._css, {'screen': ['a.css', 'b.css'], 'all': ['b.js']})\n        self.assertEqual(merged._js, ['a.js', 'b.js'])\n\n\n", "    def test_clear(self):\n        media = Media(css={'screen': ['a.css'], 'all': ['b.css']}, js=['a'])\n        media.clear()\n        self.assertEqual(media._css_lists, [])\n        self.assertEqual(media._js_lists, [])\n", "    def test_add_empty_with_media(self):\n        media = Media(css={'screen': ['a.css']}, js=['a'])\n        empty_media = Media(css={'screen': ['b.css']})\n        merged = media + empty_media\n        self.assertEqual(merged._css_lists, [{'screen': ['a.css']}, {'screen': ['b.css']}])\n        self.assertEqual(merged._js_lists, [['a']])\n"], "sample_1088": ["def test_symmetrize_non_linear():\n    assert symmetrize(x**3 + y**3) == (-3*x*y*(x + y), -y**3)\n    assert symmetrize(x**4 + y**4) == (\n        -4*x**2*y**2 + (x + y)**4, -y**4)\n\n    assert symmetrize(x**3 + a*x*y**2 + b*y**3, x, y) == (\n        -3*x*y*(x + y), -y**3 - a*x*y**2)\n", "def test_symmetrize_multivariate():\n    assert symmetrize(x**2 + y**2 + z**2, x, y, z) ==  \\\n       ( ((x + y + z)**2 - 2*(x*y + x*z + y*z)), 0 )\n    assert symmetrize(x**3 + x*y*z + y**3 + z**3, x, y, z) == \\\n       ( (x + y + z)**3 - 3*(x + y + z)*(x*y + x*z + y*z), 0)\n\n\n\n", "compilation error", "compilation error", "compilation error"], "sample_963": ["def test_stringify_basic_types(tp: type):\n    assert stringify(tp) == repr(tp).__name__.replace(\"<class '\", \"\").replace(\"'>\", \"\")\n\n", "def test_stringify_type_hints_TypedDict():\n    from typing import TypedDict  # type: ignore\n\n    MyTypedDict = TypedDict('MyTypedDict', {'a': int, 'b': str})\n    assert stringify(MyTypedDict) == \"tests.test_util_typing.MyTypedDict\"\n", "def test_stringify_complex_callables(annotation):\n    assert stringify(annotation) == (\n        str(annotation).replace(\n            'typing.', ''\n        ).replace(\n            'Optional[', 'Optional['\n        ).replace(' ', '')\n    )\n", "def test_stringify_type_hints_TypedDict():\n    from typing import TypedDict  # type: ignore\n\n    class MyTypedDict(TypedDict):\n        a: str\n        b: int\n    assert stringify(MyTypedDict) == \"tests.test_util_typing.MyTypedDict\"\n    assert stringify(TypedDict['MyDict', str]) == 'TypedDict[\"MyDict\", str]'\n\n\n\n", "def test_stringify_type_hints_origin():\n    from typing import Origin, Annotated  # type: ignore\n    \n    class MyOrigin(Origin):\n        __origin__ = 'MyOrigin'\n\n    assert stringify(MyOrigin) == \"MyOrigin\"\n    assert stringify(Annotated[str, \"foo\", \"bar\"]) == \"str\"  # type: ignore\n\n\n\n"], "sample_753": ["def test_multiclass_ovr_penalty_l1():\n    # Test that the l1 penalty is correctly applied when using the\n    # 'ovr' strategy for multiclass classification.\n\n    X, y = make_classification(n_samples=100, n_features=20, random_state=0,\n                               n_classes=3)\n\n    # Test with a small penalty to ensure the coefficients are non-zero\n    clf = LogisticRegression(penalty='l1', C=1.0, solver='liblinear',\n                             multi_class='ovr', max_iter=1000)\n    clf.fit(X, y)\n    assert_greater(np.count_nonzero(clf.coef_), 0)\n", "def test_l1_penalty_with_intercept():\n    # Test that the L1 penalty works correctly with fit_intercept=True\n    X, y = make_classification(n_samples=100, n_features=20, random_state=0)\n    lr = LogisticRegression(penalty=\"l1\", C=1.0, fit_intercept=True, solver='liblinear')\n    lr.fit(X, y)\n    assert_greater(np.count_nonzero(lr.coef_), 0)\n    assert_greater(np.count_nonzero(lr.intercept_), 0)\n", "def test_regularization_penalty_sparse():\n    # Test regularization penalty with sparse data\n\n    rng = np.random.RandomState(0)\n    n_samples = 50\n    n_features = 20\n    X, y = make_classification(n_samples=n_samples, n_features=n_features,\n                               random_state=0)\n    X_sparse = sparse.csr_matrix(X)\n\n    for penalty in ['l1', 'l2']:\n        for solver in ['liblinear', 'saga']:\n            if solver == 'liblinear' and penalty == 'l1':\n                continue\n            lr_sparse = LogisticRegression(penalty=penalty, solver=solver,\n                                            random_state=0)\n            lr_dense = LogisticRegression(penalty=penalty, solver=solver,\n                                            random_state=0)\n\n            lr_sparse.fit(X_sparse, y)\n            lr_dense.fit(X, y)\n            assert_array_almost_equal(lr_sparse.coef_, lr_dense.coef_, 4)\n", "def test_logreg_decision_function_scaling():\n    # Test that decision function values are not dependent on scaling\n    # factors of the input data.\n\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([1, 1, -1])\n\n    # Scale the data\n    X_scaled = (X - X.mean(axis=0)) / X.std(axis=0)\n\n    clf = LogisticRegression()\n\n    # Fit on both datasets and compare the decision function values\n    clf.fit(X, y)\n    df_original = clf.decision_function(X)\n    clf.fit(X_scaled, y)\n    df_scaled = clf.decision_function(X_scaled)\n\n    assert_array_almost_equal(df_original, df_scaled, decimal=3)\n\n", "    def test_class_weight_with_sparse_data():\n        # Test class_weight works with sparse data.\n        rng = np.random.RandomState(42)\n        n_samples = 50\n        n_features = 20\n        X, y = make_classification(n_samples=n_samples,\n                                   n_features=n_features,\n                                   random_state=0)\n        X = sparse.csr_matrix(X)\n\n        class_weight = {'0': 10, '1': 1}\n\n        lr = LogisticRegression(class_weight=class_weight, solver='saga')\n        lr.fit(X, y)\n        assert_array_almost_equal(lr.coef_, lr.coef_)\n"], "sample_77": ["    def test_urlize_trim_url_limit(self):\n        items = (\n            (\n                \"This is a very long URL: http://www.example.com/this-is-a-very-long-url-that-is-much-longer-than-the-trim-limit-of-25-characters\"\n                ,\n                \"This is a very long URL: http://www.example.com/this-is-a-very-long-url-that-is-much-longer-than-the-trim-limit-of-25-characters\"\n            ),\n            (\n                \"This is a very long URL: http://www.example.com/this-is-a-very-long-url-that-is-much-longer-than-the-trim-limit-of-25-characters\"\n                ,\n                \"This is a very long URL: http://www.example.com/this-is-a-very-long-url-that-is-much-longer-than-the-trim-limit-of-25-characters\"\n            ),\n        )\n        for value, output in items:\n            with self.subTest(value=value, output=output):\n                self.assertEqual(urlize(value, trim_url_limit=25), output)\n\n\n\n", "    def test_urlize_trim_url_limit(self):\n        urls = [\n            'http://example.com/longurl1234567890123456789012345678901234567890',\n            'http://example.com/anotherlongurl1234567890123456789012345678901234',\n        ]\n        for url in urls:\n            with self.subTest(url=url):\n                for limit in [25, 50, 100]:\n                    trimmed = urlize(url, trim_url_limit=limit)\n                    self.assertIn('...', trimmed)\n                    self.assertLessEqual(len(trimmed.split('...')[0]), limit)\n\n", "    def test_urlize_with_autoescape(self):\n        for value, output in [\n            ('Search for <a href=\"http://google.com/?q=!\">google.com/?q=!</a> and see.',\n             'Search for <a href=\"http://google.com/?q=!\">google.com/?q=!</a> and see.'),\n            ('Search for <a href=\"http://google.com/?q=!\">google.com/?q=!</a> and <script>alert(\"test\")</script> see.',\n             'Search for <a href=\"http://google.com/?q=!\">google.com/?q=!</a> and <script>alert(\"test\")</script> see.'),\n            (lazystr('Search for <a href=\"http://google.com/?q=!\">google.com/?q=!</a> and see.'),\n             'Search for <a href=\"http://google.com/?q=!\">google.com/?q=!</a> and see.'),\n        ]:\n            with self.subTest(value=value):\n                self.assertEqual(urlize(value, autoescape=True), output)\n", "    def test_urlize_trim_url_limit(self):\n        long_url = 'https://www.example.com/this-is-a-really-long-url-that-exceeds-the-trim_url_limit'\n        trimmed_url = urlize(long_url, trim_url_limit=20)\n        self.assertIn('...', trimmed_url)\n        self.assertEqual(len(trimmed_url.split('...')[0]), 20)\n\n", "    def test_urlize_with_trim_url_limit(self):\n        tests = (\n            (\n                'This is a long URL: http://www.example.com/this-is-a-very-long-url-that-is-over-the-limit-and-needs-to-be-trimmed',\n                'This is a long URL: <a href=\"http://www.example.com/this-is-a-very-long-url-that-is-over-the-limit-and-needs-to-be-trimmed\">http://www.example.com/this-is-a-very-long-url-that-is-over-the-limit-and-needs... </a>',\n            ),\n            (\n                'This is a long URL: http://www.example.com/this-is-a-very-long-url-that-is-over-the-limit-and-needs-to-be-trimmed?param=value',\n                'This is a long URL: <a href=\"http://www.example.com/this-is-a-very-long-url-that-is-over-the-limit-and-needs...\">http://www.example.com/this-is-a-very-long-url-that-is-over-the-limit-and-needs... </a>',\n            ),\n        )\n        for value, output in tests:\n            with self.subTest(value=value, output=output):\n                self.assertEqual(urlize(value, trim_url_limit=50), output)\n\n\n\n"], "sample_160": ["    def test_non_numeric_input(self):\n        with self.assertRaises(TypeError):\n            nformat('abc', '.')\n        with self.assertRaises(TypeError):\n            nformat(True, '.')\n        with self.assertRaises(TypeError):\n            nformat([1, 2], '.')\n", "    def test_empty_string(self):\n        self.assertEqual(nformat('', '.'), '')\n        self.assertEqual(nformat('', '.', decimal_pos=2), '')\n        self.assertEqual(nformat('', '.', grouping=2, thousand_sep=','), '')\n        self.assertEqual(nformat('', '.', grouping=2, thousand_sep=',', force_grouping=True), '')\n", "    def test_non_numeric_input(self):\n        self.assertEqual(nformat('abc', '.'), 'abc')\n        self.assertEqual(nformat(None, '.'), '')\n        self.assertEqual(nformat(True, '.'), '')\n        self.assertEqual(nformat(False, '.'), '')\n", "    def test_empty_string(self):\n        self.assertEqual(nformat(''), '')\n        self.assertEqual(nformat('', '.'), '')\n        self.assertEqual(nformat('', '.', decimal_pos=2), '')\n        self.assertEqual(nformat('', '.', grouping=2, thousand_sep=','), '')\n", "    def test_empty_string(self):\n        self.assertEqual(nformat(''), '')\n        self.assertEqual(nformat('', '.'), '')\n        self.assertEqual(nformat('', '.', decimal_pos=2), '0.00')\n        self.assertEqual(nformat('', '.', grouping=2, thousand_sep=','), '')\n        self.assertEqual(nformat('', '.', grouping=2, thousand_sep=',', force_grouping=True), '')\n\n"], "sample_6": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_103": ["    def test_field_lookup_in_subquery_annotation(self):\n        author_count_qs = Author.objects.filter(\n            pk=OuterRef('author_id')\n        ).values('name').annotate(count=Count('id'))\n        publisher_qs = Publisher.objects.annotate(\n            author_count=Subquery(author_count_qs, IntegerField()),\n        ).annotate(total=Count('book'))\n        self.assertEqual(dict(publisher_qs), {\n            1: {'author_count': 1, 'total': 1},\n            3: {'author_count': 1, 'total': 2},\n            7: {'author_count': 2, 'total': 1},\n            9: {'author_count': 1, 'total': 1},\n        })  \n", "    def test_aggregate_over_subquery(self):\n        subquery = (\n            Book.objects\n            .filter(publisher__id=OuterRef('id'))\n            .values('rating')\n            .annotate(avg_rating=Avg('rating'))\n            .values('avg_rating')\n        )\n        publishers = Publisher.objects.annotate(\n            avg_book_rating=Subquery(subquery)\n        ).all()\n        self.assertEqual(publishers.count(), 4)\n        for p in publishers:\n            self.assertIsInstance(p.avg_book_rating, float)\n\n\n\n", "    def test_complex_subquery_expression(self):\n        class Greatest(Func):\n            function = 'GREATEST'\n\n                return super().as_sql(compiler, connection, function='MAX')\n\n        long_books_qs = Book.objects.filter(pages__gt=400).values('publisher')\n        avg_rating_for_long_books = Max(\n            Subquery(Book.objects.filter(\n                publisher=OuterRef('publisher')).annotate(\n                avg_rating=Avg('rating'))\n                .values('avg_rating')), output_field=FloatField())\n        publisher_qs = Publisher.objects.annotate(\n            avg_rating_for_long_books=avg_rating_for_long_books,\n            count_long_books=Count(long_books_qs),\n        ).order_by('avg_rating_for_long_books')\n\n        self.assertQuerysetEqual(\n            publisher_qs,\n            [\n                {'avg_rating_for_long_books': 4.5, 'count_long_books': 2},\n                {'avg_rating_for_long_books': 4.25, 'count_long_books': 1},\n            ],\n            lambda x: (x['avg_rating_for_long_books'], x['count_long_books'])\n        )\n", "    def test_annotate_with_filter(self):\n        qs = Author.objects.filter(name=\"Adrian Holovaty\").annotate(\n            combined_ages=Sum(F('age') + F('friends__age'))\n        ).order_by('combined_ages')\n        self.assertQuerysetEqual(\n            qs, [\n                {'name': 'Adrian Holovaty', 'combined_ages': 63},\n                {'name': 'Adrian Holovaty', 'combined_ages': 68},\n            ],\n            lambda s: s['combined_ages']\n        )\n\n\n", "    def test_multi_level_annotations(self):\n        class GetRatingCount(Avg):\n                return 'COUNT(CASE WHEN \"rating\" > 4 THEN 1 ELSE NULL END)'\n\n        qs = Book.objects.values('id').annotate(\n            avg_rating=Avg('rating'),\n            rating_count=GetRatingCount('rating')\n        )\n        self.assertQuerysetEqual(\n            qs,\n            [\n                {'id': 1, 'avg_rating': 4.25, 'rating_count': 4},\n                {'id': 2, 'avg_rating': 5.0, 'rating_count': 1},\n                {'id': 3, 'avg_rating': 4.6666666666666665, 'rating_count': 3},\n                {'id': 4, 'avg_rating': 4.0, 'rating_count': 2},\n                {'id': 5, 'avg_rating': 3.8, 'rating_count': 2},\n            ],\n            lambda x: (x['id'], x['avg_rating'], x['rating_count'])\n        )\n\n\n"], "sample_709": ["def test_pytester_outcomes_with_skip(pytester: Pytester) -> None:\n    p1 = pytester.makepyfile(\n        \"\"\"\n            pytest.skip(\"This test should be skipped\")\n    \"\"\"\n    )\n    result = pytester.runpytest(str(p1))\n    result.assert_outcomes(skipped=1)\n    result.assert_outcomes(passed=0)\n", "compilation error", "def test_pytester_outcomes_skipped(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n            pass\n\n        @pytest.mark.skip\n            pass\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1, skipped=1)\n\n", "compilation error", "def test_pytester_outcomes_error_and_warning(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import warnings\n\n            raise ValueError(\"some error\")\n\n            warnings.warn(UserWarning(\"some custom warning\"))\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.assert_outcomes(errors=1, warnings=1)\n\n"], "sample_439": ["    def test_recursive_template_override(self):\n        class RecursiveForm(Form):\n            name = CharField()\n            template_name = \"forms_tests/recursive_form.html\"\n\n        with self.assertRaises(RecursionError):\n            str(RecursiveForm())\n", "    def test_custom_renderer_with_custom_template_name(self):\n        class MyCustomRenderer(DjangoTemplates):\n            default_template_name = \"forms_tests/custom_renderer_template.html\"\n\n        class Person(Form):\n            first_name = CharField()\n\n        with override_settings(FORM_RENDERER=MyCustomRenderer):\n            t = Template(\"{{ form }}\")\n            html = t.render(Context({\"form\": Person()}))\n            expected = \"\"\"\n            <div class=\"fieldWrapper\">\n            <label for=\"id_first_name\">First name:</label>\n            <input type=\"text\" name=\"first_name\" required id=\"id_first_name\">\n            </div>\n            \"\"\"\n            self.assertHTMLEqual(html, expected)\n", "    def test_hidden_fields_in_form_summary(self):\n        class MyForm(Form):\n            name = CharField(required=True)\n            hidden_field = HiddenInput()\n\n        form = MyForm(auto_id=False)\n        self.assertHTMLEqual(\n            form.as_p(),\n            '<p>Name: <input type=\"text\" name=\"name\" required></p>'\n            '<p><input type=\"hidden\" name=\"hidden_field\"></p>',\n        )\n", "    def test_auto_id_with_renderer(self):\n        class CustomRenderer(DjangoTemplates):\n            pass\n\n        class Person(Form):\n            name = CharField()\n\n        with self.settings(FORM_RENDERER=CustomRenderer):\n            f = Person(auto_id=True)\n            self.assertHTMLEqual(\n                str(f),\n                '<div class=\"fieldWrapper\"><label for=\"id_name\">Name:</label>'\n                '<input type=\"text\" name=\"name\" required id=\"id_name\"></div>',\n            )\n", "    def test_recursive_formset_rendering(self):\n        class NestedForm(forms.Form):\n            field = forms.CharField()\n\n        class ParentForm(forms.Form):\n            sub_forms = forms.modelformset_factory(NestedForm, fields=('field',))\n\n        parent_form = ParentForm()\n        with self.assertRaises(RecursionError):\n            str(parent_form)\n\n"], "sample_498": ["def test_legend_loc_keyword_argument_takes_precedence():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3], [4, 5, 6], label='Line1')\n    ax.plot([4, 5, 6], [7, 8, 9], label='Line2')\n    with pytest.warns(UserWarning):\n        leg = ax.legend(loc='best', title='My Legend')\n\n\n", "def test_legend_loc_outside():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2], [3, 4], label='line')\n    with pytest.raises(ValueError):\n        ax.legend(loc='outside')\n\n\n", "def test_legend_ncol_with_handle_size(ncol):\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3], label='line 1')\n    ax.plot([4, 5, 6], label='line 2')\n    ax.plot([7, 8, 9], label='line 3')\n\n    legend = ax.legend(ncol=ncol)\n\n    for i in range(0, len(legend.legend_handles)):\n        handle = legend.legend_handles[i]\n        # Make sure handle size is proportional to ncol\n        assert handle.get_width() >= 0.5 * (i + 1) * (1 / ncol)\n\n", "def test_legend_handle_color_to_line():\n    fig, ax = plt.subplots()\n    line, = ax.plot([0, 1], [0, 1], color='red', label='line')\n    leg = ax.legend()\n    handle = leg.get_legend_handles()[0]\n    assert handle.get_color() == 'red'\n", "def test_legend_title_is_visible():\n    fig, ax = plt.subplots()\n    ax.plot(range(10), label='Aardvark')\n    leg = ax.legend(title='Aardvark')\n    assert leg.get_title().get_visible()\n    leg.set_title(visible=False)\n    assert not leg.get_title().get_visible()\n\n"], "sample_430": ["    def test_multiple_create_models_with_ordering_and_index_together(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\n                    \"Person\",\n                    fields=[\n                        (\"id\", models.AutoField(primary_key=True)),\n                        (\"name\", models.CharField(max_length=200)),\n                    ],\n                    options={\"order_with_respect_to\": \"name\"},\n                ),\n                migrations.CreateModel(\n                    \"Pet\",\n                    fields=[\n                        (\"id\", models.AutoField(primary_key=True)),\n                        (\"name\", models.CharField(max_length=200)),\n                        (\"person\", models.ForeignKey(\"Person\", on_delete=models.CASCADE)),\n                    ],\n                    options={\"index_together\": {(\"person\", \"name\")},},\n                ),\n            ],\n\n        migration = Migration(\"0001_initial\", \"test_app\")\n\n        suggest_name = migration.suggest_name()\n        self.assertEqual(\n            suggest_name,\n            \"person_pet\"\n        )\n\n\n\n", "    def test_migration_name_collision(self):\n        class Migration1(migrations.Migration):\n            operations = [migrations.CreateModel(\"Person\", fields=[])]\n        class Migration2(migrations.Migration):\n            operations = [migrations.CreateModel(\"Person\", fields=[])]\n        migration1 = Migration1(\"0001_initial\", \"test_app\")\n        migration2 = Migration2(\"0001_initial\", \"test_app\")\n\n        self.assertEqual(migration1.suggest_name(), \"person\") \n        self.assertEqual(migration2.suggest_name(), \"person_2\")\n", "    def test_alter_index_together_with_fields_order(self):\n        initial_author = ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n                (\"age\", models.IntegerField()),\n            ],\n            {\n                \"index_together\": {(\"name\", \"age\")},\n            },\n        )\n        author_new_constraints = ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"age\", models.IntegerField()),\n                (\"name\", models.CharField(max_length=200)),\n            ],\n            {\n                \"index_together\": {(\"name\", \"age\")},\n            },\n        )\n        changes = self.get_changes([initial_author], [author_new_constraints])\n\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(\n            changes, \"testapp\", 0, [\"AlterIndexTogether\", \"RemoveField\", \"AddField\"],\n        )\n        self.assertOperationAttributes(\n            changes,\n            \"testapp\",\n            0,\n            0,\n            name=\"author\",\n            index_together={(\"name\", \"age\")},\n        )\n\n", "    def test_add_field_and_index_together_with_unique(self):\n        \"\"\"\n        Added fields with unique constraints will trigger index_together changes.\n        \"\"\"\n        initial_author = ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n            ],\n            {\n                \"index_together\": {(\"name\",)},\n            },\n        )\n        author_with_unique_field = ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200, unique=True)),\n            ],\n            {\n                \"index_together\": {(\"name\",)},\n            },\n        )\n        changes = self.get_changes([initial_author], [author_with_unique_field])\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(\n            changes,\n            \"testapp\",\n            0,\n            [\"AlterIndexTogether\", \"AlterField\"],\n        )\n        self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"author\", index_together={\"name\",})\n\n\n\n", "    def test_operation_with_custom_name(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\n                    name='MyModel', fields=[], custom_name='my_custom_model',\n                )\n            ]\n\n        migration = Migration(\"0001_initial\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"my_custom_model\")\n\n"], "sample_629": ["def test_expand_modules_with_files_and_directories(files_or_modules, expected):\n    ignore_list, ignore_list_re, ignore_list_paths_re = [], [], []\n    modules, errors = expand_modules(\n        files_or_modules, ignore_list, ignore_list_re, ignore_list_paths_re\n    )\n    modules.sort(key=lambda d: d[\"name\"])\n    assert modules == expected\n    assert not errors\n", "    def test_expand_modules_with_absolute_paths(files_or_modules, expected):\n        ignore_list, ignore_list_re, ignore_list_paths_re = [], [], []\n        modules, errors = expand_modules(\n            files_or_modules, ignore_list, ignore_list_re, ignore_list_paths_re\n        )\n        modules.sort(key=lambda d: d[\"name\"])\n        assert modules == expected\n        assert not errors\n", "    def test_expand_modules_non_existing_module(files_or_modules, expected):\n        ignore_list, ignore_list_re, ignore_list_paths_re = [], [], []\n        modules, errors = expand_modules(\n            files_or_modules, ignore_list, ignore_list_re, ignore_list_paths_re\n        )\n        assert modules == expected\n", "def test_expand_modules_duplicates(files_or_modules, expected):\n    ignore_list, ignore_list_re, ignore_list_paths_re = [], [], []\n    modules, errors = expand_modules(\n        files_or_modules, ignore_list, ignore_list_re, ignore_list_paths_re\n    )\n    modules.sort(key=lambda d: d[\"name\"])\n    assert modules == expected\n    assert not errors\n", "def test_expand_modules_import_error(files_or_modules, expected):\n    ignore_list, ignore_list_re, ignore_list_paths_re = [], [], []\n    modules, errors = expand_modules(\n        files_or_modules, ignore_list, ignore_list_re, ignore_list_paths_re\n    )\n    modules.sort(key=lambda d: d[\"name\"])\n    assert modules == []\n    assert errors == expected\n"], "sample_917": ["def test_build_domain_cpp_with_add_function_parentheses_is_True_and_template_args(app, status, warning):\n    app.builder.build_all()\n\n        pattern = '<li><p>%s<a .*?><code .*?><span .*?>%s</span></code></a></p></li>' % spec\n        res = re.search(pattern, text)\n        if not res:\n            print(\"Pattern\\n\\t%s\\nnot found in %s\" % (pattern, file))\n            assert False\n    rolePatterns = [\n        ('', 'Sphinx'),\n        ('', 'Sphinx::version'),\n        ('', 'version'),\n        ('', 'List'),\n        ('', 'MyEnum')\n    ]\n    parenPatterns = [\n        ('ref function without parens ', r'paren_1\\(\\)'),\n        ('ref function with parens ', r'paren_2\\(\\)'),\n        ('ref function without parens, explicit title ', 'paren_3_title'),\n        ('ref function with parens, explicit title ', 'paren_4_title'),\n        ('ref op call without parens ', r'paren_5::operator\\(\\)\\(\\)'),\n        ('ref op call with parens ', r'paren_6::operator\\(\\)\\(\\)'),\n        ('ref op call without parens, explicit title ', 'paren_7_title'),\n        ('ref op call with parens, explicit title ', 'paren_8_title')\n    ]\n\n    f = 'roles.html'\n    t = (app.outdir / f).read_text()\n    for s in rolePatterns:\n        check(s, t, f)\n    for s in parenPatterns:", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_361": ["    def test_urlize_with_trim_url_limit(self):\n        tests = (\n            ('Google Inc. <http://www.google.com',\n             'Google Inc. <a href=\"http://www.google.com\">...</a>'),\n            ('This site is awesome <http://www.example.com/page',\n             'This site is awesome <a href=\"http://www.example.com/page\">...</a>'),\n            ('Visit <a href=\"http://www.verylongurl.com/this/is/a/very/long/url\">verylongurl.com</a>',\n             'Visit <a href=\"http://www.verylongurl.com/this/is/a/very/long/url\">...</a>'),\n        )\n        for value, output in tests:\n            with self.subTest(value=value):\n                self.assertEqual(urlize(value, trim_url_limit=15), output)\n                self.assertEqual(urlize(value, trim_url_limit=20, nofollow=True),\n                                output.replace('<a', '<a rel=\"nofollow\"'))\n\n\n\n", "    def test_urlize_trim_url_limit(self):\n        self.assertEqual(urlize('This is a very long URL http://example.com/path/this-is-a-very-very-long-url-that-is-longer-than-the-limit-20', trim_url_limit=20),\n                         'This is a very long URL http://example.com/path/this-is-a-very...')\n        # Ensure trailing ellipsis is included\n        self.assertEqual(urlize('This is a very long URL http://example.com/path/this-is-a-very-very-long-url-that-is-longer-than-the-limit-20', trim_url_limit=19),\n                         'This is a very long URL http://example.com/path/this-is-a-very-...')\n", "    def test_urlize_with_trim_url_limit(self):\n        self.assertEqual(urlize('Search for longlinkthatwillgettrimmed.com?a=b&c=d', trim_url_limit=10),\n                         'Search for <a href=\"longlinkthatwillgettri...\">longlinkthatwillgettri...</a> ?a=b&c=d')\n", "    def test_urlize_with_trim_url_limit(self):\n        tests = (\n            (\"Google Search: https://www.google.com/search?q=django&oq=django\",\n             \"Google Search: <a href=\\\"https://www.google.com/search?q=django&oq=django\\\">Google Search</a>\"),\n            (\"Search for google.com/?q=django&oq=django&foo=bar in a really long query\",\n             \"Search for <a href=\\\"https://www.google.com/?q=django&oq=django&foo=bar\\\">https://www.google.com/?q=django&oq=django&foo=bar</a> in a really long query\"),\n            (\"Visit https://www.djangoproject.com for more information about Django\",\n             \"Visit <a href=\\\"https://www.djangoproject.com\\\">https://www.djangoproject.com</a> for more information about Django\"),\n        )\n        for value, output in tests:\n            with self.subTest(value=value, output=output):\n                self.assertEqual(urlize(value, trim_url_limit=30), output)\n\n\n\n", "    def test_urlize_trim_url_limit(self):\n        long_url = 'https://www.example.com/this-is-a-very-long-url-that-needs-to-be-truncated'\n        shortened_url = 'https://www.example.com/this-is-a-very'\n\n        with self.subTest(trim_url_limit=30):\n            self.assertEqual(urlize(long_url, trim_url_limit=30), \n                             f'This is <a href=\"{long_url}\" title=\"{long_url}\">a very long url</a> that needs to be truncated')\n            self.assertEqual(urlize(long_url, trim_url_limit=15), \n                             f'This is <a href=\"{long_url}\" title=\"{long_url}\">a very long url</a> that needs to be truncated')\n\n        with self.subTest(trim_url_limit=None):\n            self.assertEqual(urlize(long_url), \n                             f'This is <a href=\"{long_url}\">{long_url}</a>')\n\n\n"], "sample_1124": ["def test_FracElement_ground():\n    F, x, y, z = field(\"x,y,z\", QQ)\n    f = (3*x**2 + y) / (2*z)\n\n    assert f._extract_ground(1) == (1, 3*x**2 + y, 2*z)\n    assert f._extract_ground(2) == (2, 6*x**2 + 2*y, 4*z)\n    assert f._extract_ground(Rational(3, 2)) == (Rational(3,2), (9/2)*x**2 + Rational(3,2)*y, 3*z)\n\n\n\n", "def test_FracElement_simplify():\n    F, x, y, z = field(\"x,y,z\", QQ)\n\n    f = (x**2 + 3*x*y)/(x + y)\n    g = f.simplify()\n    assert g.numer == x*(x + 3*y)\n    assert g.denom == x + y\n\n    f = (x**3 + x*y)/(x*y + x)\n    g = f.simplify()\n    assert g.numer == x*(x**2 + y)\n    assert g.denom == x*(y + 1)\n\n    f = (x**2 + y)/(x + y)\n    g = f.simplify()\n    assert g == x - y/(x + y)\n\n\n    f = (x**2 + 3*y)/(z + 1)\n    g = f.simplify()\n    assert g == (x**2 + 3*y)/(z + 1)\n\n    f = (x**2 + 3*y)/(x + y)\n    g = f.simplify().subs(x, 0)\n    assert g == 3*y/y == 3\n", "compilation error", "def test_FracElement___str__():\n    F, x,y,z = field(\"x,y,z\", ZZ)\n\n    f = (x**2 + 3*y)/(z + 1)\n    assert str(f) == \"(x**2 + 3*y)/(z + 1)\"\n\n    g = 1/(x*y)\n    assert str(g) == \"1/(x*y)\"\n\n    h = (x**3 + y)/(z**2)\n    assert str(h) == \"(x**3 + y)/z**2\"\n    \n\n\n\n", "def test_FracElement_eq():\n    F, x,y,z = field(\"x,y,z\", QQ)\n\n    f = (x**2 + 3*y)/z\n    g = (x**2 + 3*y)/z\n\n    assert f == g\n    assert f.numer == g.numer\n    assert f.denom == g.denom\n\n    g = (x**2 + 3*y)/(z + 1)\n    assert f != g\n\n    g = (2*x**2 + 6*y)/(2*z)\n    assert f == g\n\n    raises(TypeError, lambda: f == x)\n    raises(TypeError, lambda: f == (x, y, z))\n\n\n"], "sample_1021": ["compilation error", "def test_quaternion_from_rotation_matrix_complex():\n    M = Matrix([[ cos(x), -sin(x), 0], [sin(x), cos(x), 0], [0, 0, 1]])\n    q = Quaternion.from_rotation_matrix(M, real_field=False)\n    assert q == Quaternion(sqrt(2)*sqrt(cos(x) + 1)/2, 0, 0, sqrt(-2*cos(x) + 2)/2)\n", "compilation error", "compilation error", "def test_quaternion_multiplication_complex():\n    q1 = Quaternion(1, 2, 3, 4)\n    q2 = Quaternion(5 + 6*I, 7 + 8*I, 9 + 10*I, 11 + 12*I, real_field = False)\n    assert q1 * q2 == Quaternion((1*5 + 2*7 + 3*9 + 4*11) + (1*8 + 2*10 + 3*12 + 4*12)*I,\n                                  (1*6 + 2*8 + 3*10 + 4*12)*I - (1*5 + 2*7 + 3*9 + 4*11),\n                                  (1*7 + 2*9 + 3*11 + 4*13)*I - (1*6 + 2*8 + 3*10 + 4*12),\n                                  (1*8 + 2*10 + 3*12 + 4*14)*I - (1*5 + 2*7 + 3*9 + 4*11))\n\n    assert q2 * q1 == Quaternion((5*1 + 6*2 + 9*3 + 11*4) + (8*1 + 10*2 + 12*3 + 12*4)*I,\n                                  (6*1 + 8*2 + 10*3 + 12*4)*I - (5*1 + 6*2 + 9*3 + 11*4),\n                                  (7*1 + 9*2 + 11*3 + 13*4)*I - (6*1 + 8*2 + 10*3 + 12*4),\n                                  (8*1 + 10*2 + 12*3 + 14*4)*I - (5*1 + 6*2 + 9*3 + 11*4))\n\n\n\n"], "sample_490": ["    def test_unique_constraint_with_violation_error_message_and_code(self):\n        constraint = models.UniqueConstraint(\n            models.F(\"baz__lower\"),\n            name=\"unique_lower_baz\",\n            violation_error_message=\"BAZ\",\n            violation_error_code=\"baz\",\n        )\n        table = \"unique_constraint_product\"\n        with self.assertRaisesMessage(IntegrityError, \"BAZ\") as cm:\n            UniqueConstraintProduct.objects.create(baz=\"BAZ\")\n        self.assertEqual(cm.exception.args[0], \"BAZ\")\n        self.assertEqual(cm.exception.code, \"baz\")\n\n\n\n", "    def test_violation_error_message_with_condition(self):\n        constraint = models.UniqueConstraint(\n            Lower(\"name\"),\n            condition=models.Q(color__isnull=True),\n            name=\"name_lower_without_color_uniq\",\n            violation_error_message=\"Custom message for null colors\",\n        )\n        msg = \"Custom message for null colors\"\n        with self.assertRaisesMessage(ValidationError, msg):\n            constraint.validate(\n                UniqueConstraintProduct, UniqueConstraintProduct(name=self.p1.name.upper(), color=None)\n            )\n", "    def test_expressions_with_condition(self):\n        constraint = models.UniqueConstraint(\n            Lower(\"name\"),\n            name=\"name_lower_without_color_uniq\",\n            condition=models.Q(color__isnull=True),\n        )\n        msg = \"Constraint \u201cname_lower_without_color_uniq\u201d is violated.\"\n        with self.assertRaisesMessage(ValidationError, msg):\n            constraint.validate(\n                UniqueConstraintProduct,\n                UniqueConstraintProduct(name=self.p1.name.upper(), color=self.p1.color),\n            )\n        # Values not matching condition are ignored.\n        constraint.validate(\n            UniqueConstraintProduct,\n            UniqueConstraintProduct(name=self.p1.name, color=self.p1.color),\n        )\n        # Existing instances have their existing row excluded.\n        constraint.validate(UniqueConstraintProduct, self.p1)\n        # Unique field is excluded.\n        constraint.validate(\n            UniqueConstraintProduct,\n            UniqueConstraintProduct(name=self.p1.name.upper(), color=self.p1.color),\n            exclude={\"name\"},\n        )\n        # Field from a condition is excluded.\n        constraint.validate(\n            UniqueConstraintProduct,\n            UniqueConstraintProduct(name=self.p1.name.upper(), color=self.p1.color),\n            exclude={\"color\"},\n        )\n", "    def test_nulls_distinct_behavior(self):\n        with self.assertRaises(IntegrityError):\n            UniqueConstraintNullsDistinct.objects.create(unique_name=\"same\", value=None)\n            UniqueConstraintNullsDistinct.objects.create(unique_name=\"same\", value=None)\n        obj = UniqueConstraintNullsDistinct.objects.create(unique_name=\"some\", value=\"some value\")\n        UniqueConstraintNullsDistinct.objects.create(unique_name=\"some\", value=None)\n        UniqueConstraintNullsDistinct.objects.create(unique_name=\"other\", value=\"other value\")\n", "    def test_nulls_distinct_behavior(self):\n        null_constraint = models.UniqueConstraint(\n            fields=[\"name\"],\n            name=\"name_nulls_unique\",\n            nulls_distinct=True,\n        )\n        with self.assertRaises(IntegrityError):\n            UniqueConstraintProduct.objects.create(name=None)\n        UniqueConstraintProduct.objects.create(name=\"unique_name\")\n        null_constraint_product = UniqueConstraintProduct.objects.create(name=None)\n\n        with self.assertRaises(IntegrityError):\n            UniqueConstraintProduct.objects.create(name=None)\n\n\n\n"], "sample_55": ["    def test_object_provided_for_change_view(self):\n        \"\"\"Test that the 'obj' argument is provided for change_view. \"\"\"\n        obj = ModelAdminExample.objects.create(name='Test Object')\n        response = self.client.get(reverse('admin:admin_views_ModelAdminExample_change', args=(obj.pk,)))\n        html = response.content.decode('utf-8')\n        self.assertIn(obj.name, html)\n\n\n", "    def test_get_formsets_with_inlines_add_view(self):\n        \"obj should be None in add_view\"\n        try:\n            get_formsets_with_inlines_admin = GetFormsetsArgumentCheckingAdmin(\n                GetFormsetsArgumentCheckingModel, self.admin_site\n            )\n            with self.assertRaises(AssertionError):\n                get_formsets_with_inlines_admin.get_formsets_with_inlines(None, None)\n        finally:\n            # Reset\n            GetFormsetsArgumentCheckingAdmin.view_on_site = True\n", "    def test_get_formsets_with_inlines_obj_is_none_for_add_view(self):\n        admin_class = GetFormsetsArgumentCheckingAdmin\n        with self.subTest(admin_class=admin_class):\n            request = HttpRequest()\n            request.method = 'POST'\n            # Simulate the admin's `get_formsets_with_inlines` being called during add_view\n            formsets = admin_class().get_formsets_with_inlines(request, None)\n            self.assertIsNone(formsets.instance)\n", "    def test_inline_formsets_with_get_formsets_with_inlines(self):\n        response = self.client.get(reverse('admin:admin_views_inlineforms_add'))\n        self.assertContains(response, 'InlineForm')\n        response = self.client.post(reverse('admin:admin_views_inlineforms_add'), {'InlineForm-TOTAL_FORMS': '1',\n                    'InlineForm-INITIAL_FORMS': '0', 'InlineForm-MAX_NUM_FORMS': '1',\n                    'InlineForm-0-field': 'Value'})\n        self.assertEqual(response.status_code, 302)\n", "    def test_empty_formset_error(self):\n        response = self.client.get(reverse('admin:admin_views_nullformset_empty_add'))\n        self.assertContains(response, 'Error: This field is required.', count=2)\n\n\n"], "sample_1090": ["compilation error", "def test_sympify_fractions():\n    assert sympify_fractions(fractions.Fraction(1, 2)) == Rational(1, 2)\n    assert sympify_fractions(fractions.Fraction(3, 4)) == Rational(3, 4)\n    assert sympify_fractions(fractions.Fraction(1, 1)) == Integer(1)  \n", "def test_symbols_in_expressions():\n    from sympy.core.symbol import Symbol\n    a = Symbol('a')\n    b = Symbol('b')\n    expr = a + b\n    assert isinstance(expr, Add)\n    assert expr.args == (a, b)\n\n    expr = a * b\n    assert isinstance(expr, Mul)\n    assert expr.args == (a, b)\n\n    expr = a**b\n    assert isinstance(expr, Pow)\n    assert expr.base == a and expr.exp == b\n\n\n\n", "compilation error", "compilation error"], "sample_699": ["def test_is_setup_py_empty(tmp_path: Path) -> None:\n    setup_py = tmp_path.joinpath(\"setup.py\")\n    setup_py.write_text('', \"utf-8\")\n    assert _is_setup_py(setup_py)\n", "    def test_is_setup_py_invalid_python(\n        tmp_path: Path", "def test_is_setup_py_with_docstring(tmp_path: Path) -> None:\n    setup_py = tmp_path.joinpath(\"setup.py\")\n    setup_py.write_text(\n        \"\"\"\n        from setuptools import setup\n\n        setup(\n            name=\"foo\",\n            version='0.1.0',\n            description=\"A foo project\",\n        )\n        \"\"\"\n    )\n    assert _is_setup_py(setup_py)\n", "    def test_is_setup_py_empty(tmp_path: Path) -> None:\n        setup_py = tmp_path.joinpath(\"setup.py\")\n        setup_py.write_text(\"\", \"utf-8\")\n        assert _is_setup_py(setup_py)\n", "compilation error"], "sample_264": ["    def test_not_finished_messages(self):\n        storage = self.get_storage()\n        response = self.get_response()\n\n        # Add more messages than the cookie size allows\n        messages = [get_random_string(32) for _ in range(100)]\n        storage.add(constants.INFO, message) for message in messages\n\n        # Expect that the storage will truncate the message list\n        unstored_messages = storage.update(response)\n        self.assertEqual(len(unstored_messages), len(messages) - 4)\n        self.assertTrue(storage._loaded_data[\n            -1\n        ] == CookieStorage.not_finished)\n\n        # Re-fetch the messages, which should now be incomplete\n        set_cookie_data(storage, unstored_messages, encode_empty=True)\n        self.assertEqual(len(list(storage)), 4)\n\n        # Re-add the remaining messages\n        storage.add(constants.INFO, message) for message in messages[4:]\n        # Now fetch back all messages\n        unstored_messages = storage.update(response)\n        self.assertEqual(len(unstored_messages), 0)\n        self.assertEqual(len(list(storage)), 100)\n", "    def test_cookie_not_finished(self):\n        storage = self.storage_class(self.get_request())\n        response = self.get_response()\n\n        # Add some messages to the storage.\n        storage.add(constants.INFO, 'test1')\n        storage.add(constants.INFO, 'test2')\n        storage.add(constants.INFO, 'test3')\n\n        # Update the response with the cookie, but only store the first two\n        # messages.\n        storage.update(response, remove_oldest=False)\n\n        # The cookie should contain the \"not_finished\" sentinel value.\n        self.assertEqual(stored_cookie_messages_count(storage, response), 2)\n        self.assertEqual(response.cookies[storage.cookie_name].value, storage._encode(\n            [storage.not_finished]\n        ))\n\n        # Fetch the messages from the storage.\n        messages = list(storage)\n        self.assertEqual(len(messages), 2)\n\n        # The messages should be the first two we added.\n        self.assertEqual(messages[0].message, 'test1')\n        self.assertEqual(messages[1].message, 'test2')\n\n        # Add the remaining messages to the cookie.\n        storage.update(response)\n\n        # Now all messages should be stored.\n        self.assertEqual(stored_cookie_messages_count(storage, response), 3)\n\n\n\n\n\n        # ... (Continue the test)\n", "    def test_encode_encode_empty(self):\n        storage = self.get_storage()\n        # Test that encoding with encode_empty=True results in an empty string\n        # when there are no messages to encode\n        encoded = storage._encode([])\n        decoded = storage._decode(encoded)\n        self.assertEqual(decoded, [])\n\n        # Test that encoding with encode_empty=True results in a non-empty\n        # string when there is at least one message\n        messages = ['test']\n        encoded = storage._encode(messages, encode_empty=True)\n        decoded = storage._decode(encoded)\n        self.assertEqual(decoded, messages)\n", "    def test_not_finished(self):\n        storage = self.get_storage()\n        response = self.get_response()\n\n        # Set some messages, then add the \"not finished\" sentinel.\n        messages = [Message(constants.INFO, 'message 1'),\n                    Message(constants.INFO, 'message 2')]\n        set_cookie_data(storage, messages + [CookieStorage.not_finished])\n\n        # Ensure that we don't add extra messages when the cookie is\n        # already marked as not finished.\n        storage.add(constants.INFO, 'message 3')\n        storage.update(response)\n        self.assertEqual(stored_cookie_messages_count(storage, response), 3)\n\n        # Retrieve messages from the cookie, and verify that the \"not\n        # finished\" sentinel is still present.\n        messages = list(storage)\n        self.assertEqual(messages, ['message 1', 'message 2'])\n        self.assertTrue(messages[-1] == CookieStorage.not_finished)\n\n\n\n", "    def test_not_finished_sentinel(self):\n        storage = self.storage_class(self.get_request())\n        messages = ['message 1', 'message 2', 'message 3']\n        # First send a message partially\n        set_cookie_data(storage, messages[:2], encode_empty=True)\n        self.assertEqual(list(storage), []) # should be empty\n\n        # send the remaining messages\n        set_cookie_data(storage, messages[2:])\n\n        self.assertEqual(list(storage), messages)\n\n"], "sample_215": ["    def test_sensitive_variables_decorator_works(self):\n        @sensitive_variables\n            return f'User: {password}'\n\n        with self.settings(DEBUG=True):\n            response = self.client.get('/test_view/', {'password': 'secret'})\n            self.assertContains(response, 'User: XXXXXXXXXXXXXXX', status_code=500)\n\n        with self.settings(DEBUG=False):\n            response = self.client.get('/test_view/', {'password': 'secret'})\n            self.assertNotContains(response, 'secret', status_code=500)\n\n\n\n\n", "    def test_sensitive_variables_decorator_works_with_function_arguments(self):\n        @sensitive_variables\n            return 'Password: {} Username: {}'.format(password, username)\n        with self.settings(DEBUG=True):\n            response = self.client.get('/test/', {'password': 'my_password', 'username': 'johndoe'})\n            self.assertContains(response, 'Password: XXXXXXXXXX Username: johndoe', status_code=500)\n        with self.settings(DEBUG=False):\n            response = self.client.get('/test/', {'password': 'my_password', 'username': 'johndoe'})\n            self.assertNotContains(response, 'Password: my_password Username: johndoe', status_code=500)\n\n", "    def test_sensitive_variables_decorator_filters_parameters(self):\n        @sensitive_variables\n            return {'password': password, 'user_id': user_id}\n\n        with self.settings(DEBUG=True):\n            response = self.client.post('/sensitive/', data={'password': 'secret', 'user_id': '123'})\n            self.assertContains(response, 'user_id: 123', status_code=200)\n            self.assertNotContains(response, 'password: secret', status_code=200)\n", "    def test_sensitive_args_function_decorator_with_string_args(self):\n        \"\"\"\n        Sensitive variables are not leaked in the sensitive_variables decorator's\n        frame, when those variables are passed as string arguments to the\n        decorated function.\n        \"\"\"\n        with self.settings(DEBUG=True):\n            self.verify_unsafe_response(sensitive_args_string_function_caller)\n            self.verify_unsafe_email(sensitive_args_string_function_caller)\n\n        with self.settings(DEBUG=False):\n            self.verify_safe_response(sensitive_args_string_function_caller, check_for_POST_params=False)\n            self.verify_safe_email(sensitive_args_string_function_caller, check_for_POST_params=False)\n\n", "    def test_sensitive_variables_decorator_with_args(self):\n        @sensitive_variables\n            return 'Password was: {}'.format(password)\n\n        with self.settings(DEBUG=True):\n            response = self.client.get('/test_view/', {'password': 'super_secret'})\n            self.assertEqual(response.status_code, 500)\n            self.assertIn('Password was: XXXXXXXXXXXX', response.content.decode('utf-8'))\n\n        with self.settings(DEBUG=False):\n            response = self.client.get('/test_view/', {'password': 'super_secret'})\n            self.assertEqual(response.status_code, 500)\n            self.assertNotIn('super_secret', response.content.decode('utf-8'))\n"], "sample_693": ["def test_do_cleanups_with_multiple_nested_cleanups(pytester: Pytester) -> None:\n    testpath = pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        class MyTestCase(unittest.TestCase):\n            values = []\n\n                    self.values.append(\"cleanup1\")\n                    self.values.append(\"cleanup2\")\n                self.addCleanup(cleanup1)\n                self.addCleanup(cleanup2)\n\n                pass\n\n            assert MyTestCase.values == [\"cleanup2\", \"cleanup1\"]\n    \"\"\"\n    )\n    reprec = pytester.inline_run(testpath)\n    passed, skipped, failed = reprec.countoutcomes()\n    assert failed == 0\n    assert passed == 1\n", "def test_pdb_teardown_skipped_with_errors(pytester: Pytester, monkeypatch: MonkeyPatch, mark: str) -> None:\n    \"\"\"Ensure tearDown() is never called for skipped tests, even if there are errors.\"\"\"\n    tracked: List[str] = []\n    monkeypatch.setattr(pytest, \"test_pdb_teardown_skipped_with_errors\", tracked, raising=False)\n\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n        import pytest\n\n        class MyTestCase(unittest.TestCase):\n            {mark}(\"skipped for reasons\")\n                raise Exception(\"Error in setUp\")\n\n                pytest.test_pdb_teardown_skipped_with_errors.append(\"tearDown:\" + self.id())\n\n                pass\n\n    \"\"\".format(\n            mark=mark\n        )\n    )\n    result = pytester.runpytest_inprocess(\"--pdb\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*1 skipped*\",\n            \"*Error in setUp*\",\n        ]\n    )\n    assert tracked == []\n\n", "    def test_do_cleanups_are_called_in_order(pytester: Pytester) -> None:\n        testpath = pytester.makepyfile(\n            \"\"\"\n            import unittest\n            class MyTestCase(unittest.TestCase):\n                values = []\n                    self.values.append(\"setup\")\n                    self.values.append(\"cleanup1\")\n                    self.values.append(\"cleanup2\")\n                    self.addCleanup(self.cleanup1)\n                    self.addCleanup(self.cleanup2)\n                    pass\n                assert MyTestCase.values == [\"setup\", \"cleanup1\", \"cleanup2\"]\n            \"\"\"\n        )\n        reprec = pytester.inline_run(testpath)\n        passed, skipped, failed = reprec.countoutcomes()\n        assert failed == 0\n        assert passed == 2\n", "    def test_do_cleanups_on_exception(pytester: Pytester) -> None:\n        testpath = pytester.makepyfile(\n            \"\"\"\n            import unittest\n            class MyTestCase(unittest.TestCase):\n                values = []\n                        self.values.append(1)\n                    self.addCleanup(cleanup)\n                    raise ValueError\n                assert MyTestCase.values == [1]\n            \"\"\"\n        )\n        reprec = pytester.inline_run(testpath)\n        passed, skipped, failed = reprec.countoutcomes()\n        assert failed == 1\n        assert passed == 1\n", "    def test_do_cleanups_on_multiple_teardown_failures(pytester: Pytester) -> None:\n        testpath = pytester.makepyfile(\n            \"\"\"\n            import unittest\n            class MyTestCase(unittest.TestCase):\n                values = []\n                        self.values.append(1)\n                    self.addCleanup(cleanup)\n                    assert False\n                    pass\n                    pass\n                assert MyTestCase.values == [1, 1]\n            \"\"\"\n        )\n        reprec = pytester.inline_run(testpath)\n        passed, skipped, failed = reprec.countoutcomes()\n        assert failed == 2\n        assert passed == 1\n\n"], "sample_174": ["    def test_integer_field_range(self):\n        integer_field_ranges = {\n            'PositiveIntegerField': (1, None),\n            'IntegerField': (-2**31, 2**31 - 1),\n            'SmallIntegerField': (-2**15, 2**15 - 1),\n        }\n        self.assertEqual(self.ops.integer_field_ranges, integer_field_ranges)\n", "    def test_select_sql_with_compound_select_unsupported(self):\n        msg = 'This backend does not support compound selects.'\n        with self.assertRaisesMessage(NotSupportedError, msg):\n            with connection.cursor() as cursor:\n                cursor.execute(\n                    \"SELECT 1 FROM (SELECT 1) AS subquery1 WHERE EXISTS (SELECT 1 FROM (SELECT 1) AS subquery2)\"\n                )\n\n\n\n", "    def test_sequence_reset_sql_raises_error(self):\n        msg = 'subclasses of BaseDatabaseOperations must provide a sql_flush() method'\n        with self.assertRaisesMessage(NotImplementedError, msg):\n            connection.ops.sequence_reset_sql(no_style(), [Author._meta.db_table]) \n", "    def test_explain_query_prefix_raise_exception(self):\n        msg = 'This backend does not support explaining query execution.'\n        with self.assertRaisesMessage(NotSupportedError, msg):\n            self.ops.explain_query_prefix()\n", "    def test_explain_query_prefix(self):\n        msg = 'This backend does not support explaining query execution.'\n        with self.assertRaisesMessage(NotSupportedError, msg):\n            self.ops.explain_query_prefix()\n\n"], "sample_920": ["    def test_docstrings_with_code_block(self):\n        docstring = \"\"\"\n\n        This function does something.\n\n        This function has a code block within it.\n\n        .. code-block:: python\n               return 42\n\n        Parameters\n        ----------\n        arg : int\n            Description of arg.\n\n        \"\"\"\n        expected = \"\"\"\n\n        This function does something.\n\n        This function has a code block within it.\n\n        .. code-block:: python\n               return 42\n\n        :param arg:\n        :type arg: int\n        ....\n\n        \"\"\"\n        config = Config()\n        actual = str(NumpyDocstring(docstring, config))\n        self.assertEqual(expected, actual)\n", "    def test_custom_sections_with_types(self):\n        docstring = \"\"\"\\", "    def test_custom_section_types(self):\n        docstring = \"\"\"\\", "    def test_special_characters_in_param_names(self):\n        docstring = \"\"\"\\", "    def test_custom_sections_with_types(self):\n        docstring = \"\"\"\\"], "sample_104": ["    def test_manifest_content_updates_on_change(self):\n        original_manifest = storage.staticfiles_storage.load_manifest()\n        os.makedirs(self._temp_dir, exist_ok=True)\n        with open(self._get_filename_path('test/a.css'), 'w') as f:\n            f.write('body {background-image: url(\"b.jpg\");}')\n        with open(self._get_filename_path('test/b.jpg'), 'wb') as f:\n            f.write(b'Some random image data')\n\n        finders.get_finder.cache_clear()\n        self.run_collectstatic()\n\n        updated_manifest = storage.staticfiles_storage.load_manifest()\n        self.assertNotEqual(original_manifest, updated_manifest)\n\n        # Ensure manifest reflects the newly added resource\n        self.assertIn('test/a.css', updated_manifest)\n        self.assertIn('test/b.jpg', updated_manifest)\n\n\n\n", "    def test_manifest_integrity(self):\n        original_manifest = storage.staticfiles_storage.load_manifest()\n\n        # Modify a file\n        with open(self._get_filename_path('bar.css'), 'w') as f:\n            f.write(\n                'url(\"foo.png\")\\nurl(\"xyz.png\")\\n'\n                '/* NEW LINE */'\n            )\n\n        # Collect static files again\n        call_command('collectstatic', **self.command_params)\n\n        new_manifest = storage.staticfiles_storage.load_manifest()\n        \n        # Assert that the manifest has been updated\n        self.assertNotEqual(original_manifest, new_manifest)\n\n        # Assert the modified file is now included\n        self.assertIn('bar.css', new_manifest)\n\n        # Assert the file that wasn't changed is still included\n        self.assertIn('foo.png', new_manifest)\n\n\n", "    def test_file_deletion_after_collectstatic(self):\n        # Create initial static files.\n        file_contents = (\n            ('foo.png', 'foo'),\n            ('bar.css', 'url(\"foo.png\")\\nurl(\"xyz.png\")'),\n            ('xyz.png', 'xyz'),\n        )\n        for filename, content in file_contents:\n            with open(self._get_filename_path(filename), 'w') as f:\n                f.write(content)\n\n        with self.modify_settings(STATICFILES_DIRS={'append': self._temp_dir}):\n            finders.get_finder.cache_clear()\n            err = StringIO()\n            # First collectstatic run.\n            call_command('collectstatic', interactive=False, verbosity=0, stderr=err)\n            relpath = self.hashed_file_path('test/bar.css')\n            with storage.staticfiles_storage.open(relpath) as relfile:\n                content = relfile.read()\n                self.assertIn(b'foo.acbd18db4cc2.png', content)\n                self.assertIn(b'xyz.d16fb36f0911.png', content)\n\n            # Delete one of the files\n            os.remove(self._get_filename_path('foo.png'))\n\n            # The second collectstatic run should reflect the deletion\n            err = StringIO()\n            call_command('collectstatic', interactive=False, verbosity=0, stderr=err)\n            relpath = self.hashed_file_path('test/bar.css')\n            with storage.staticfiles_storage.open(relpath) as relfile:\n                content = relfile.read()\n                self.assertNotIn(b'foo.acbd18db4cc2.png', content)\n                self.assertIn(b'xyz.d16fb36f0911.png', content)\n\n", "    def test_static_url_resolution(self):\n        # Test that the 'static' template tag resolves paths correctly when\n        # served from a custom URL scheme.\n\n        @patch.dict(settings.STATIC_URL, '/static_files/')\n            self.assertStaticRenders(\"test/file.txt\", \"/static_files/test/file.deploy12345.txt\")\n\n            # Test that the STATIC_URL setting is overridden within the view.\n            @override_settings(STATIC_URL='/my_custom_static/')\n                return render(request, 'test.html')\n\n            response = view(MockRequest(path='test_view'))\n            self.assertIn(\n                '/my_custom_static/test/file.deploy12345.txt',\n                response.content.decode(),\n            )\n\n        test_with_custom_url_scheme()\n\n", "    def test_file_name_with_spaces_and_special_characters(self):\n        file_name = 'cached/my file with spaces and %special characters.css'\n        self.assertEqual(self.hashed_file_path(file_name),\n                         'cached/my%20file%20with%20spaces%20and%25special%20characters.d16fb36f0911.css')\n\n\n\n"], "sample_23": ["def test_angle_array_arithmetic():\n    a = Angle([1, 2, 3], u.deg)\n    b = Angle([4, 5, 6], u.deg)\n\n    # Addition\n    c = a + b\n    assert_allclose(c.value, [5, 7, 9])\n\n    # Subtraction\n    d = a - b\n    assert_allclose(d.value, [-3, -3, -3])\n\n    # Multiplication\n    e = a * b\n    assert_allclose(e.value, [4, 10, 18])\n\n    # Division\n    f = a / b\n    assert_allclose(f.value, [0.25, 0.4, 0.5])\n", "def test_angle_with_invalid_string_representation():\n    with pytest.raises(ValueError, match=r\"Invalid angle string\"):\n        Angle(\"1h2m3s10ms\")  # Extra units not allowed \n    with pytest.raises(ValueError, match=r\"Invalid angle string\"):\n        Angle(\"abc\")  # Not a valid representation\n    with pytest.raises(ValueError, match=r\"Invalid angle string\"):\n        Angle(\"+1d\")  # Missing a sign\n\n\n\n", "def test_angle_invalid_to_string_format():\n    with pytest.raises(ValueError, match=\"Invalid format string\"):\n        Angle([1, 2] * u.deg).to_string(format=\"invalid\")\n", "def test_angle_round():\n    a1 = Angle(1.23456789 * u.deg)\n    assert_allclose(a1.round(2), 1.23 * u.deg)\n\n    a2 = Angle(1.2345 * u.deg)\n    assert_allclose(a2.round(3), 1.235 * u.deg)\n\n    a3 = Angle(1.2345 * u.deg)\n    assert_allclose(a3.round(0), 1 * u.deg)\n\n    a4 = Angle(12.345 * u.deg)\n    assert_allclose(a4.round(1), 12.3 * u.deg)\n", "def test_angle_array_to_string_precision():\n    a = Angle([1.2345, 6.7890, 12.3456], u.deg)\n    assert a.to_string(precision=2).strip() == \"1.23d14m29.79s 6.79d47m30.52s 12.35d20m28.43s\"\n"], "sample_892": ["def test_adaboost_predict_proba_proba_threshold(algorithm):\n    # check probability thresholding in .predict_proba result for AdaBoostClassifier\n    X_train, X_test, y_train, y_test = train_test_split(\n        *datasets.load_iris(return_X_y=True), random_state=42\n    )\n    model = AdaBoostClassifier(algorithm=algorithm, random_state=42)\n    model.fit(X_train, y_train)\n\n    proba = model.predict_proba(X_test)\n    # Check that predict_proba results are in the range [0, 1] for each class\n    assert np.all(proba >= 0)\n    assert np.all(proba <= 1)\n\n    # Check that the sum of probabilities for each sample is close to 1\n    assert np.allclose(np.sum(proba, axis=1), 1, atol=1e-6)\n\n\n", "def test_adaboost_large_n_estimators():\n    \"\"\"\n    Test AdaBoostClassifier with a large number of estimators to check for\n    performance issues and memory usage.\n    \"\"\"\n    # Increase n_estimators to a large number (e.g., 1000).\n    n_estimators = 1000\n    X, y = datasets.load_iris(return_X_y=True)\n    clf = AdaBoostClassifier(\n        n_estimators=n_estimators, random_state=42, algorithm=\"SAMME\"\n    )\n    clf.fit(X, y)\n    assert clf.n_estimators == n_estimators\n\n\n\n\n", "def test_adaboost_with_missing_data():\n    # Check behavior with missing values in data\n    X = np.array([[1, 2, np.nan], [4, 5, 6], [7, 8, np.nan]])\n    y = np.array([0, 1, 1])\n\n    # Test AdaBoostClassifier\n    clf = AdaBoostClassifier()\n    with pytest.raises(ValueError, match=\"Missing values are not supported\"):\n        clf.fit(X, y)\n\n    # Test AdaBoostRegressor\n    regr = AdaBoostRegressor()\n    with pytest.raises(ValueError, match=\"Missing values are not supported\"):\n        regr.fit(X, y)\n\n\n\n", "def test_adaboost_with_sparse_data():\n    # Check the behavior with sparse data\n\n    from sklearn.datasets import make_sparse_coded_samples\n    from scipy.sparse import csc_matrix\n\n    X, y = make_sparse_coded_samples(n_samples=100, n_features=100, random_state=42)\n\n    # Ensure that the sparse matrix is converted correctly\n    assert isinstance(X, csc_matrix)\n\n    # Test both regression and classification\n    for model_type in [AdaBoostClassifier, AdaBoostRegressor]:\n        model = model_type()\n        model.fit(X, y)\n\n\n\n", "def test_adaboost_with_penalty_param_in_gradient_boosting_like_estimators():\n    # Check that penalty parameter is handled correctly when using estimators\n    # with a penalty parameter (like GradientBoostingClassifier).\n    from sklearn.ensemble import GradientBoostingClassifier\n\n    X, y = datasets.load_iris(return_X_y=True)\n    clf = AdaBoostClassifier(\n        estimator=GradientBoostingClassifier(loss=\"deviance\", penalty=\"l2\"),\n        random_state=42,\n    )\n    clf.fit(X, y)\n\n    # Check if the penalty parameter is correctly passed through to the base\n    # estimator\n    assert clf.estimator.loss == \"deviance\"\n    assert clf.estimator.penalty == \"l2\"\n"], "sample_1152": ["def test_issue_20413():\n    x, y, z = symbols('x y z')\n    a, b = symbols('a b', positive=True)\n\n    assert powsimp((x**(a*y))**(z*x)) == x**(a*y*z*x)\n    assert powsimp((x**(a*y))**(z*x), combine='all') == x**(a*y*z*x)\n    assert powsimp((y**(a*x))**(z*y)) == y**(a*x*z*y)\n    assert powsimp((x**(a*y)*y**(b*x))**(z*x)) == x**(a*y*z*x)*y**(b*x*z*x)\n    assert powsimp(((x**(a*y))*y**(b*x))**(z*x), combine='all') == x**(a*y*z*x)*y**(b*x*z*x)\n    assert powsimp((x**(1/a)**(1/b)**(z)) == x**(1/(a*b*z)) \n", "def test_issue_20220():\n    x, y = symbols('x y', nonnegative=True)\n    assert powsimp(sqrt(x**2 + y**2)**2) == x**2 + y**2\n    assert powsimp(sqrt(x**2 + y**2)**2 - x**2 - y**2) == 0\n", "def test_issue_20039():\n    x, y = symbols('x y', positive=True)\n    a = symbols('a')\n    assert powsimp(exp(log(x)*a)*exp(log(y)*a)) == x**a*y**a\n    assert powsimp(exp(log(x)/a)*exp(log(y)/a)) == (x*y)**(1/a)\n    assert powsimp((exp(log(x)*a)*exp(log(y)*a))**(1/a)) == x*y\n    assert powsimp((exp(log(x)/a)*exp(log(y)/a))**(1/a)) == (x*y)**(1/a)\n    assert powsimp(exp(log(x)*a)*exp(log(y)*a), combine='exp') == (x*y)**a\n    assert powsimp((exp(log(x)*a)*exp(log(y)*a))**(1/a), combine='exp') == (x*y)**1\n    assert powsimp(2**(log(x)*a)*2**(log(y)*a)) == 2**(log(x*y)*a)\n    assert powsimp(2**(log(x)/a)*2**(log(y)/a)) == 2**(log(x*y)/a)\n\n\n", "    def test_issue_20380():\n        x, y = symbols('x y')\n        assert powsimp(sqrt(x**2 + y**2)**2) == x**2 + y**2\n", "def test_issue_21175():\n    x, y, z = symbols('x y z', integer=True)\n    a = symbols('a', integer=True)\n    assert powsimp((x**(2*y))**z) == x**(2*y*z)\n    assert powsimp((x**a)**z) == x**(a*z)\n    assert powsimp((x**z)**a) == x**(a*z)\n    assert powsimp((x*y)**z) == x**z * y**z\n    assert powsimp((x**y**z)**2) == x**(2*y*z)\n    assert powsimp( (x**(y**(z**a)))**2 ) == x**(2*y**(z**a)) \n    assert powsimp((x**y)**(z**a)) == x**(y*z**a)\n"], "sample_112": ["    def test_override_prepopulated_fields_js(self):\n        article = Article.objects.all()[0]\n        request = self.request_factory.get(reverse('admin:admin_views_article_change', args=[article.pk]))\n        request.user = self.superuser\n        admin = ArticleAdmin(Article, site)\n        response = admin.change_view(request, str(article.pk))\n        self.assertContains(response, 'override-prepopulated_fields_js')\n\n\n\n", "def test_cell_count(self):\n    request = self.request_factory.get(reverse('admin:admin_views_article_change', args=[self.article.pk]))\n    request.user = self.superuser\n    admin = ArticleAdmin(Article, site)\n    response = admin.change_form_view(request, self.article, form=None)\n    context = response.context_data\n    inline_admin_form = context['formset']\n    self.assertEqual(cell_count(inline_admin_form), 6)\n", "    def test_override_change_form_object_tools_empty(self):\n        \"\"\"\n        change_form_object_tools template tag should render empty\n        if there's nothing to display, no matter the context\n        \"\"\"\n        article = Article.objects.all()[0]\n        request = self.request_factory.get(reverse('admin:admin_views_article_change', args=[article.pk]))\n        request.user = self.superuser\n        admin = ArticleAdmin(Article, site)\n        response = admin.change_view(request, str(article.pk), extra_context={'show_publish': True})\n        response.render()\n        self.assertContains(response, 'override_change_form_object_tools')\n        self.assertNotContains(response, 'name=\"_publish\"')\n", "    def test_date_hierarchy_with_custom_field(self):\n        class QuestionInline(admin.StackedInline):\n            model = Question\n            extra = 1\n\n        class CustomFieldArticleAdmin(admin.ModelAdmin):\n            inlines = [QuestionInline]\n            date_hierarchy = 'question__posted'\n\n        site.register(Article, CustomFieldArticleAdmin)\n\n        article = Article.objects.create(title='My Article')\n        Question.objects.create(question='Test Question', posted=datetime.date(2023, 10, 26), article=article)\n        Question.objects.create(question='Another Question', posted=datetime.date(2023, 10, 27), article=article)\n\n        modeladmin = CustomFieldArticleAdmin(Article, site)\n        \n        request = self.factory.get(reverse('admin:admin_views_article_changelist'))\n        request.user = self.superuser\n        changelist = modeladmin.get_changelist_instance(request)\n        spec = date_hierarchy(changelist)\n        choices = [choice['link'] for choice in spec['choices']]\n        expected_choices = [['posted__year=2023'],]\n        self.assertEqual(choices, expected_choices)\n", "    def test_prepopulated_fields_js(self):\n        article = Article.objects.all()[0]\n        request = self.request_factory.get(reverse('admin:admin_views_article_change', args=[article.pk]))\n        request.user = self.superuser\n        admin = ArticleAdmin(Article, site)\n        response = admin.change_view(request, str(article.pk))\n        context = response.context_data\n        self.assertIsNotNone(context.get('prepopulated_fields_json'))\n        json_data = context['prepopulated_fields_json']\n        self.assertEqual(json.loads(json_data), [\n            {\n                \"id\": \"#id_title\",\n                \"name\": \"title\",\n                \"dependency_ids\": [],\n                \"dependency_list\": [],\n                \"maxLength\": 255,\n                \"allowUnicode\": True\n            },\n            {\n                \"id\": \"#id_content\",\n                \"name\": \"content\",\n                \"dependency_ids\": [],\n                \"dependency_list\": [],\n                \"maxLength\": 65535,\n                \"allowUnicode\": True\n            }\n        ])\n"], "sample_229": ["    def test_subquery_in_union(self):\n        reserved_names = ReservedName.objects.filter(order__gt=0)\n        qs1 = Number.objects.filter(num__lt=10).annotate(\n            has_reserved_name=Exists(reserved_names.filter(order=OuterRef('num')))\n        )\n        qs2 = Number.objects.filter(num__lt=10).annotate(\n            count=Value(0, IntegerField())\n        )\n\n        self.assertCountEqual(\n            qs1.union(qs2).values_list('num'), [(0, 0), (1, 0), (2, 0), (3, 0), (4, 0), (5, 0), (6, 0), (7, 0), (8, 0), (9, 0)]\n        )\n\n\n\n", "    def test_union_with_distinct_and_ordering(self):\n        qs1 = Number.objects.filter(num__lte=1)\n        qs2 = Number.objects.filter(num__gte=8)\n        self.assertEqual(len(list(qs1.union(qs2).distinct().order_by('-num'))), 3)\n", "    def test_combine_with_related_objects(self):\n        Parent.objects.create(name='parent1', age=30)\n        Parent.objects.create(name='parent2', age=40)\n        Child.objects.create(parent=Parent.objects.get(name='parent1'), name='child1', age=5)\n        Child.objects.create(parent=Parent.objects.get(name='parent2'), name='child2', age=10)\n        # Test UNION\n        qs1 = Child.objects.filter(age__lt=6).only('name', 'parent__name')\n        qs2 = Child.objects.filter(age__gte=6).only('name', 'parent__age')\n        qs_union = qs1.union(qs2)\n        expected_results = [\n            {'name': 'child1', 'parent__name': 'parent1'},\n            {'name': 'child2', 'parent__age': 40},\n        ]\n        self.assertQuerysetEqual(qs_union, expected_results, equal_to=lambda x: x)\n\n        # Test INTERSECTION\n        qs1 = Child.objects.filter(parent__name='parent1').only('name', 'parent__name')\n        qs2 = Child.objects.filter(age__lt=6).only('name', 'parent__name')\n        qs_intersection = qs1.intersection(qs2)\n        expected_results = [\n            {'name': 'child1', 'parent__name': 'parent1'},\n        ]\n        self.assertQuerysetEqual(qs_intersection, expected_results, equal_to=lambda x: x)\n\n        # Test DIFFERENCE\n        qs1 = Child.objects.filter(age__lt=6).only('name', 'parent__name')\n        qs2 = Child.objects.filter(age__gte=6).only('name', 'parent__age')\n        qs_diff = qs1.difference(qs2)\n        expected_results = [\n            {'name': 'child1', 'parent__name': 'parent1'},\n        ]\n        self.assertQuerysetEqual(qs_diff, expected_results, equal_to=lambda x: x)\n\n", "    def test_distinct_with_values(self):\n        ReservedName.objects.create(name='a', order=2)\n        qs1 = ReservedName.objects.all()\n        reserved_name = qs1.union(qs1).distinct().values('name', 'order', 'id').get()\n        self.assertEqual(reserved_name['name'], 'a')\n        self.assertEqual(reserved_name['order'], 2)\n        reserved_name = qs1.union(qs1).distinct().values_list('name', 'order', 'id').get()\n        self.assertEqual(reserved_name[:2], ('a', 2))\n\n\n", "    def test_clear_ordering(self):\n        qs1 = Number.objects.order_by('num')\n        qs2 = Number.objects.order_by('-num')\n        combined_qs = qs1.union(qs2)\n        self.assertEqual(list(combined_qs), list(range(10)) + list(range(9, -1, -1)))\n        self.assertEqual(combined_qs.order_by('num').query.params['ORDER_BY'], 'num')\n        self.assertEqual(list(combined_qs.clear_ordering()), list(range(10)) + list(range(9, -1, -1)))\n        self.assertEqual(combined_qs.clear_ordering().query.params['ORDER_BY'], None)\n"], "sample_70": ["    def test_fast_delete_with_auto_created_fields(self):\n\n        class AutoCreatedModel(models.Model):\n            name = models.CharField(max_length=100)\n            id_field = models.AutoField(primary_key=True, auto_created=True)\n\n        AutoCreatedModel.objects.create(name='test')\n        deleted, deleted_objects = AutoCreatedModel.objects.all().delete()\n        self.assertEqual(deleted, 1)\n        self.assertEqual(deleted_objects, {'delete.AutoCreatedModel': 1})\n\n\n\n", "    def test_fast_delete_no_pk(self):\n        u = User()\n        u.save()\n        a = Avatar.objects.create(desc='a')\n        u.avatar = a\n        u.save()\n        self.assertEqual(User.objects.count(), 1)\n        with self.assertNumQueries(1):\n            u.delete()\n        self.assertEqual(User.objects.count(), 0)\n        self.assertFalse(Avatar.objects.filter(pk=a.pk).exists())\n\n\n", "    def test_fast_delete_related_empty_m2m_set(self):\n        m = M.objects.create()\n        MR.objects.create(m=m, r=R.objects.create())\n        # 1 to delete m, 1 to fast-delete from MR related to m\n        self.assertNumQueries(2, m.delete)\n\n\n\n", "    def test_fast_delete_m2m_with_through_model(self):\n        m = M.objects.create()\n        t = ThroughModel.objects.create(m2m_field=m)\n        t2 = ThroughModel.objects.create(m2m_field=m)\n        m.through_m2m.add(t, t2)\n        # One query to delete m, one to delete the through model instances\n        self.assertNumQueries(2, m.delete)\n        self.assertFalse(M.objects.exists())\n        self.assertFalse(ThroughModel.objects.exists())\n", "    def test_fast_delete_with_related_models(self):\n        a = Avatar.objects.create(desc='a')\n        u = User.objects.create(avatar=a)\n        t = User.objects.create()\n\n        # 1 query to delete u, 1 to fast delete related avatar\n        self.assertNumQueries(2, u.delete)\n        self.assertFalse(User.objects.filter(pk=u.pk).exists())\n        self.assertFalse(Avatar.objects.filter(pk=a.pk).exists())\n        self.assertTrue(User.objects.filter(pk=t.pk).exists())\n"], "sample_508": ["def test_set_properties():\n    line = mlines.Line2D([], [])\n    line.set_label(\"Test Line\")\n    \n    # Check for common properties\n    line.set_color('red')\n    assert line.get_color() == 'red'\n    line.set_linewidth(5)\n    assert line.get_linewidth() == 5\n    line.set_linestyle('-')\n    assert line.get_linestyle() == '-'\n    line.set_marker('o')\n    assert line.get_marker() == 'o'\n    line.set_markersize(10)\n    assert line.get_markersize() == 10\n    line.set_alpha(0.5)\n    assert line.get_alpha() == 0.5\n\n\n    # Check for non-standard properties\n    line.set_foo('bar')\n    assert line.get_foo() == 'bar' \n", "compilation error", "def test_update_set_signature_and_docstring():\n    class MyArtist(martist.Artist):\n            pass\n\n    MyArtist._update_set_signature_and_docstring()\n    assert hasattr(MyArtist.set, 'new_prop')\n    assert 'new_prop' in MyArtist.set.__doc__\n\n\n\n", "def test_clip_path_with_transform():\n    fig, ax = plt.subplots()\n\n    rect = mpatches.Rectangle((0, 0), 3, 4, facecolor='red', edgecolor='k')\n    ax.add_patch(rect)\n\n    p = mpath.Path([\n        (0, 0),\n        (1, 0),\n        (1, 1),\n        (0, 1),\n        (0, 0),\n    ])\n    p = mpatches.PathPatch(\n        p, transform=ax.transData, facecolor='none', edgecolor='blue')\n    ax.add_patch(p)\n\n    ax.set_xlim(0, 4)\n    ax.set_ylim(0, 5)\n", "def test_format_cursor_data_clip():\n    \"\"\"Test if cursor data is correct when using clip=True.\"\"\"\n    X = np.empty((7, 1))\n    X[0] = -1.0\n    X[1] = 0.0\n    X[2] = 0.1\n    X[3] = 0.5\n    X[4] = 0.9\n    X[5] = 1.0\n    X[6] = 2.0\n\n    labels_list = [\n        \"[-1.0]\",\n        \"[0.0]\",\n        \"[0.1]\",\n        \"[0.5]\",\n        \"[0.9]\",\n        \"[1.0]\",\n        \"[1.0]\",\n    ]\n\n    fig, ax = plt.subplots()\n    fig.suptitle(\"clip=True\")\n    norm = mcolors.BoundaryNorm(\n        np.linspace(0, 1, 4, endpoint=True), 256, clip=True, extend='neither')\n    img = ax.imshow(X, cmap='RdBu_r', norm=norm)\n    for v, label in zip(X.flat, labels_list):\n        # label = \"[{:-#.{}g}]\".format(v, cbook._g_sig_digits(v, 0.33))\n        assert img.format_cursor_data(v) == label\n\n    plt.close()\n\n    fig, ax = plt.subplots()\n    fig.suptitle(\"clip=True, min\")\n    norm = mcolors.BoundaryNorm(\n        np.linspace(0, 1, 4, endpoint=True), 256, clip=True, extend='min')\n    img = ax.imshow(X, cmap='RdBu_r', norm=norm)\n    for v, label in zip(X.flat, labels_list):\n        # label = \"[{:-#.{}g}]\".format(v, cbook._g_sig_digits(v, 0.33))\n        assert img.format_cursor_data(v) == label\n\n    plt.close()\n\n    fig, ax = plt.subplots()\n    fig.suptitle(\"clip=True,"], "sample_746": ["def test_brier_score_loss_multiclass():\n    y_true = np.array([0, 1, 2,\n                       1, 0, 2,\n                       2, 0, 1])\n    y_pred = np.array([\n        [0.1, 0.4, 0.5],\n        [0.6, 0.2, 0.2],\n        [0.3, 0.4, 0.3],\n        [0.2, 0.7, 0.1],\n        [0.5, 0.2, 0.3],\n        [0.8, 0.1, 0.1],\n        [0.1, 0.3, 0.6]\n    ])\n    true_score = linalg.norm(y_true[:, None] - y_pred) ** 2 / len(y_true)\n    assert_almost_equal(brier_score_loss(y_true, y_pred), true_score)\n", "def test_brier_score_loss_multiclass():\n    y_true = np.array([0, 1, 2, 2])\n    y_pred = np.array([[0.2, 0.7, 0.1], [0.1, 0.4, 0.5],\n                       [0.6, 0.2, 0.2], [0.6, 0.1, 0.3]])\n    true_score = np.mean((y_true - y_pred)**2)\n\n    assert_almost_equal(brier_score_loss(y_true, y_pred), true_score)\n\n\n", "compilation error", "def test_brier_score_loss_with_sparse_array():\n    from scipy.sparse import csr_matrix\n\n    y_true = np.array([0, 1, 1, 0, 1, 1])\n    y_pred = np.array([0.1, 0.8, 0.9, 0.3, 1., 0.95])\n    y_pred_sparse = csr_matrix(y_pred.reshape(6, 1))\n\n    true_score = linalg.norm(y_true - y_pred) ** 2 / len(y_true)\n\n    assert_almost_equal(brier_score_loss(y_true, y_pred_sparse), true_score)\n\n", "def test_brier_score_loss_with_multiclass():\n    y_true = np.array([0, 1, 2, 0, 1, 2])\n    y_pred = np.array([\n        [0.1, 0.8, 0.1],\n        [0.2, 0.7, 0.1],\n        [0.3, 0.6, 0.1],\n        [0.1, 0.8, 0.1],\n        [0.2, 0.7, 0.1],\n        [0.3, 0.6, 0.1]\n    ])\n    dummy_score = np.sum(np.square(y_true - y_pred)) / len(y_true)\n    assert_almost_equal(brier_score_loss(y_true, y_pred), dummy_score)\n\n"], "sample_877": ["def test_isotonic_regression_with_repeated_duplicates():\n    # Check handling of repeated duplicate points (regression test\n    # for #17499)\n    X = np.array([1, 1, 2, 2, 3, 3, 3, 4, 4, 4, 4])\n    y = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n\n    reg = IsotonicRegression()\n    reg.fit(X, y)\n    assert_array_equal(reg.X_thresholds_, np.array([1, 2, 3, 4]))\n    assert_array_equal(reg.y_thresholds_, np.array([1, 3, 6, 10]))\n\n", "def test_isotonic_regression_with_empty_input():\n    # Test for empty input arrays\n    reg = IsotonicRegression()\n    with pytest.raises(ValueError):\n        reg.fit(np.array([]), np.array([]))\n    with pytest.raises(ValueError):\n        reg.predict(np.array([]))\n\n    with pytest.raises(ValueError):\n        reg.transform(np.array([]))\n\n\n", "def test_isotonic_regression_missing_data_handling():\n    # Test handling of missing data in both X and y\n    X = np.array([1, 2, 3, 4, np.nan, 6])\n    y = np.array([5, 6, np.nan, 8, 9, 10])\n    \n    # Check that fit works with missing data\n    reg = IsotonicRegression().fit(X, y)\n    \n    # Check predict handles missing data correctly\n    predictions = reg.predict([1, 2, 3, 4, np.nan, 6])\n    \n    assert np.isnan(predictions[2])\n\n\n\n", "def test_isotonic_regression_sparse_input():\n    from scipy.sparse import csr_matrix\n\n    # Test with sparse input data\n    n_samples = 10\n    n_features = 5\n    X = csr_matrix((np.ones(n_samples * n_features),\n                    (np.arange(n_samples) * n_features,\n                     np.tile(range(n_features), n_samples))),\n                   shape=(n_samples, n_features))\n    y = np.arange(n_samples)\n    reg = IsotonicRegression()\n    reg.fit(X, y)\n    y_pred = reg.predict(X)\n\n    assert np.allclose(y_pred, y)\n\n\n\n", "def test_isotonic_regression_with_tied_y_values():\n    # Test handling of tied y values when fitting the model\n    rng = np.random.RandomState(42)\n    X = np.array([1, 2, 3, 4, 5])\n    y = np.array([2, 2, 3, 3, 5])\n\n    # Fit the model\n    ir = IsotonicRegression()\n    ir.fit(X, y)\n\n    # Check the output\n    y_pred = ir.predict(X)\n    assert_array_equal(y_pred, [2, 2, 3, 3, 5])\n\n    # Check the thresholds\n    assert_array_equal(ir.X_thresholds_, [1, 3, 5])\n    assert_array_equal(ir.y_thresholds_, [2, 3, 5])\n\n\n\n"], "sample_698": ["def test_coloredlogformatter_with_custom_color_mapping() -> None:\n    logfmt = \"%(levelname)-8s %(message)s\"\n\n    record = logging.LogRecord(\n        name=\"dummy\",\n        level=logging.INFO,\n        pathname=\"dummypath\",\n        lineno=10,\n        msg=\"Test Message\",\n        args=(),\n        exc_info=None,\n    )\n\n    class CustomFormatter(ColoredLevelFormatter):\n        color_mapping = {\n            logging.DEBUG: \"\\x1b[34m\",  # Cyan\n            logging.INFO: \"\\x1b[32m\",  # Green\n            logging.WARNING: \"\\x1b[33m\",  # Yellow\n            logging.ERROR: \"\\x1b[31m\",  # Red\n            logging.CRITICAL: \"\\x1b[35m\",  # Magenta\n        }\n\n    tw = TerminalWriter()\n    tw.hasmarkup = True\n    formatter = CustomFormatter(tw, logfmt)\n    output = formatter.format(record)\n    assert output == (\n        \"\\x1b[32mINFO    Test Message\"\n    )\n\n\n\n", "def test_coloredlogformatter_with_no_color() -> None:\n    logfmt = \"%(levelname)-8s %(message)s\"\n\n    record = logging.LogRecord(\n        name=\"dummy\",\n        level=logging.INFO,\n        pathname=\"dummypath\",\n        lineno=10,\n        msg=\"Test Message\",\n        args=(),\n        exc_info=None,\n    )\n\n    tw = TerminalWriter()\n    tw.hasmarkup = False  # Disable color\n    formatter = ColoredLevelFormatter(tw, logfmt)\n    output = formatter.format(record)\n    assert output == (\" INFO     Test Message\")\n", "def test_coloredlogformatter_with_datefmt() -> None:\n    logfmt = \"%(asctime)s %(filename)-25s %(lineno)4d %(levelname)-8s %(message)s\"\n    record = logging.LogRecord(\n        name=\"dummy\",\n        level=logging.INFO,\n        pathname=\"dummypath\",\n        lineno=10,\n        msg=\"Test Message\",\n        args=(),\n        exc_info=None,\n    )\n    record.timestamp = 1678886400.0\n\n    tw = TerminalWriter()\n    tw.hasmarkup = True\n    formatter = ColoredLevelFormatter(tw, logfmt, datefmt=\"%Y-%m-%d %H:%M:%S\")\n    output = formatter.format(record)\n    assert output.startswith(\"2023-03-15 00:00:00 dummypath                   10 INFO     Test Message\")\n\n\n\n", "def test_coloredlogformatter_with_invalid_format() -> None:\n    logfmt = \"%(nonexistent_key)s\" \n\n    record = logging.LogRecord(\n        name=\"dummy\",\n        level=logging.INFO,\n        pathname=\"dummypath\",\n        lineno=10,\n        msg=\"Test Message\",\n        args=(),\n        exc_info=None,\n    )\n\n    tw = TerminalWriter()\n    tw.hasmarkup = True\n    formatter = ColoredLevelFormatter(tw, logfmt)\n    output = formatter.format(record)\n    assert output == (\"\") \n", "compilation error"], "sample_426": ["    def test_reversed(self):\n        t = self.t + self.oneweek\n        self.assertEqual(timeuntil(t, self.t), \"1\\xa0week\")\n        self.assertEqual(timesince(self.t, t), \"-1\\xa0week\")\n\n\n\n", "    def test_time_strings_customization(self):\n        custom_time_strings = {\n            \"year\": \"%(num)d yr\",\n            \"month\": \"%(num)d mo\",\n            \"week\": \"%(num)d wk\",\n            \"day\": \"%(num)d day\",\n            \"hour\": \"%(num)d hr\",\n            \"minute\": \"%(num)d min\",\n        }\n        self.assertEqual(\n            timesince(self.t, self.t + self.onemonth, time_strings=custom_time_strings),\n            \"1 mo\",\n        )\n        self.assertEqual(\n            timesince(self.t, self.t + 2 * self.oneyear, time_strings=custom_time_strings),\n            \"2 yr\",\n        )\n        self.assertEqual(\n            timesince(self.t, self.t + 3 * self.oneweek + 2 * self.oneday, time_strings=custom_time_strings),\n            \"3 wk, 2 day\",\n        )\n\n\n\n", "    def test_time_until_reversed_dates(self):\n        \"\"\"Check time until works correctly when the given date if in the future\"\"\" \n        t = self.t + self.oneday \n        self.assertEqual(timeuntil(t), \"1\\xa0day\")\n", "    def test_reversed_months_and_years(self):\n        \"\"\"\n        Test that months and years are correctly handled when reversed.\n        \"\"\"\n        t = self.t + self.onemonth + self.oneyear\n        self.assertEqual(timeuntil(t, self.t), \"1\\xa0year, 1\\xa0month\")\n", "    def test_empty_time_difference(self):\n        self.assertEqual(timesince(self.t, self.t), \"0\\xa0minutes\")\n        self.assertEqual(timeuntil(self.t, self.t), \"0\\xa0minutes\")\n"], "sample_611": ["compilation error", "compilation error", "    def test_cftime_range_with_None_inputs(start, periods, freq, calendar, use_cftime, expected):\n        result = cftime_range(\n            start=start, periods=periods, freq=freq, calendar=calendar, use_cftime=use_cftime\n        ).size\n        assert result == expected\n", "compilation error", "def test_date_range_like_scalar(\n    start, freq, cal, use_cftime, expected_type,"], "sample_1149": ["def test_sympify_non_instance():\n    class MyNonSingleton:\n        pass\n    assert S(MyNonSingleton()) is MyNonSingleton()\n    class MyNonSingletonWithSymbol(Basic):\n        pass\n    assert S(MyNonSingletonWithSymbol()) is MyNonSingletonWithSymbol()\n", "compilation error", "def test_singleton_attribute_access():\n    class MySingleton(Basic, metaclass=Singleton):\n        pass\n\n    # Test accessing the singleton object through S\n    assert S.MySingleton is MySingleton()\n\n    # Test accessing the singleton object through its class name\n    assert MySingleton() is S.MySingleton \n\n", "        def __init__(self, arg):\n            self.arg = arg\n", "compilation error"], "sample_251": ["    def test_alias_with_custom_function(self):\n        class AddOne(Func):\n            function = \"SUM\"\n\n        qs = Book.objects.alias(\n            sum_rating=AddOne('rating')\n        ).annotate(\n            sum_rating_alias=F('sum_rating')\n        )\n        self.assertIs(hasattr(qs.first(), 'sum_rating_alias'), False)\n        for book in qs:\n            with self.subTest(book=book):\n                self.assertEqual(book.sum_rating_alias, 5)\n", "    def test_select_related_with_alias(self):\n        qs = Book.objects.select_related('publisher').alias(\n            publisher_name=F('publisher__name'),\n            publisher_awards=F('publisher__num_awards'),\n        ).filter(\n            publisher_name='Apress',\n        )\n        self.assertIs(hasattr(qs.first(), 'publisher_name'), False)\n        self.assertIs(hasattr(qs.first(), 'publisher_awards'), False)\n        self.assertCountEqual(\n            qs, [self.b1, self.b4], lambda b: (b.isbn, b.publisher_name),\n        )\n", "    def test_alias_with_subquery(self):\n        qs = Book.objects.alias(\n            top_rating=Subquery(\n                Book.objects.filter(\n                    pubdate__year=OuterRef('pubyear')\n                ).order_by('-rating').values('rating')[:1]\n            ),\n        ).values_list('pubyear', 'top_rating')\n        self.assertCountEqual(qs, [\n            (1991, 5.0),\n            (1995, 4.0),\n            (2007, 4.5),\n            (2008, 4.0),\n        ])\n", "    def test_group_by_alias(self):\n        qs = Book.objects.alias(rating_alias=F('rating') - 1).values('publisher__name', 'rating_alias').annotate(\n            avg_rating=Avg('rating')\n        ).group_by('publisher__name', 'rating_alias')\n        msg = (\n            \"Cannot group by 'rating_alias'. Use annotate() to \"\n            \"promote it.\"\n        )\n        with self.assertRaisesMessage(FieldError, msg):\n            qs.filter(avg_rating__gt=5).first()\n\n", "    def test_alias_with_nested_m2m_fields(self):\n        qs = Author.objects.alias(other_friends=F('friends').distinct())\n        self.assertIs(hasattr(qs.first(), 'other_friends'), False)\n        self.assertCountEqual(\n            qs.values('name', 'other_friends__id').distinct(),\n            [\n                {'name': 'Adrian Holovaty', 'other_friends__id': [self.a2.id]},\n                {'name': 'Jacob Kaplan-Moss', 'other_friends__id': [self.a1.id]},\n            ],\n        )\n"], "sample_316": ["    def test_spooled_temp_file_content(self):\n        with tempfile.SpooledTemporaryFile(max_size=10) as temp:\n            temp.write(b'This is a test.\\n')\n            temp.seek(0)\n            content = temp.read()\n            self.assertEqual(content, b'This is a test.\\n')\n\n", "    def test_spooled_temp_size_limits(self):\n        with tempfile.SpooledTemporaryFile(max_size=5) as temp:\n            temp.write(b\"foo bar baz quux\" * 3)\n            django_file = File(temp, name=\"something.txt\")\n            self.assertEqual(django_file.size, 15)\n", "    def test_spooled_temp_close(self):\n        with tempfile.SpooledTemporaryFile() as temp:\n            temp.write(b\"foo bar baz quux\\n\")\n            django_file = File(temp, name=\"something.txt\")\n            self.assertEqual(django_file.size, 17)\n            django_file.close()\n        # temp should be closed and contents flushed\n        # to disk.  \n        # Check that accessing the file again raises an error\n        with self.assertRaises(IOError):\n            with open(\"something.txt\", 'r') as f:\n                f.read()\n", "compilation error", "    def test_file_get_last_modified(self):\n        with tempfile.NamedTemporaryFile() as temp:\n            temp.write(b'content')\n            temp.flush()\n            file = File(temp, name='tempfile')\n            self.assertIsNotNone(file.last_modified)\n\n\n\n"], "sample_464": ["    def test_content_type_buffer_from_filename(self):\n        response = FileResponse(io.BytesIO(b\"binary content\"), filename=\"test.jpg\")\n        self.assertEqual(response.headers[\"Content-Type\"], \"image/jpeg\")\n\n", "    def test_response_with_close_file_stream(self):\n                with tempfile.TemporaryFile(\"wb+t\") as tmp:\n                    tmp.write(\"binary content\")\n                    tmp.seek(0)\n                    response = FileResponse(tmp)\n                    self.assertEqual(list(response), [b\"binary content\"])\n                    tmp.close()\n", "    def test_response_content_disposition_with_directory_path(self):\n        with self.assertRaises(TypeError):\n            FileResponse(open(__file__, \"rb\"), filename=\"/foo/bar\")\n\n\n\n", "    def test_file_response_with_seekable_file(self):\n        with tempfile.NamedTemporaryFile(mode=\"w+b\", delete=False) as tmp:\n            tmp.write(b\"binary content\")\n            tmp.seek(0)\n            response = FileResponse(tmp.file)\n            self.assertEqual(response.headers[\"Content-Length\"], \"14\")\n            content = list(response)\n            self.assertEqual(content, [b\"binary content\"])\n            response.close()\n\n", "    def test_content_length_named_pipe(self):\n        with tempfile.NamedTemporaryFile() as temp_file:\n            pipe_file = temp_file.name\n            os.mkfifo(pipe_file)\n            with open(pipe_file, \"wb\") as pipe_for_write:\n                pipe_for_write.write(b\"binary content\")\n\n            with open(pipe_file, \"rb\") as pipe_for_read:\n                response = FileResponse(pipe_for_read)\n                self.assertTrue(response.has_header(\"Content-Length\"))\n                self.assertEqual(response.headers[\"Content-Length\"], \"14\")\n\n\n\n"], "sample_1": ["    def model_b(x, y):\n        return x * y\n", "    def model_b(x):\n        return x * x\n", "    def model_b(x, y):\n        return x * y\n", "def test_separability_with_mapping_and_arithmetic():\n    m = Mapping((1, 0, 2, 1), name='map1')\n    n = models.Addition(name='add')\n    cm = m & n | m\n    assert not is_separable(cm)\n    assert_allclose(separability_matrix(cm), np.array([\n        [True, True, True, False],\n        [True, True, True, False],\n        [True, True, True, False],\n        [True, True, True, False]\n    ]))\n", "    def model_d(x, y):\n        return x * y\n"], "sample_356": ["    def test_suggest_name_for_complex_operations(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel('Person', fields=[]),\n                migrations.RenameModel('Person', 'Human'),\n                migrations.AddField('Human', 'age', models.IntegerField()),\n                migrations.RemoveField('Human', 'name'),\n                migrations.AlterModelTable('Human', 'humans'),\n                migrations.DeleteModel('Animal'),\n            ]\n\n        migration = Migration('0001_initial', 'test_app')\n        self.assertEqual(\n            migration.suggest_name(),\n            'human_rename_add_age_remove_name_rename_table_delete_animal',\n        )\n\n\n\n", "    def test_operation_with_complex_name(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\n                    name='ComplexNameModelWithUnderscores', fields=[]\n                ),\n            ]\n\n        migration = Migration('some_migration', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'complex_name_model_with_underscores')\n\n", "    def test_auto_naming_with_app_label(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel('Person', fields=[]),\n            ]\n\n        migration = Migration('0001_initial', 'testapp')\n        self.assertEqual(migration.suggest_name(), 'person')\n\n\n\n", "    def test_auto_squash(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel('Person', fields=[]),\n                migrations.CreateModel('Animal', fields=[]),\n            ]\n            squashed_migrations = ['0002_initial', '0003_initial']\n\n        migration = Migration('0001_initial', 'test_app')\n        suggest_name = migration.suggest_name()\n        self.assertEqual(suggest_name, 'person_animal_squashed')\n", "    def test_add_field_to_existing_model(self):\n        before = [\n            ModelState('app', 'book', [\n                ('id', models.AutoField(primary_key=True)),\n                ('title', models.CharField(max_length=200)),\n            ]),\n        ]\n        after = [\n            ModelState('app', 'book', [\n                ('id', models.AutoField(primary_key=True)),\n                ('title', models.CharField(max_length=200)),\n                ('author', models.CharField(max_length=200)),\n            ]),\n        ]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['AddField'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='author')\n"], "sample_78": ["    def test_normalize_path_patterns_removes_duplicates(self):\n        self.assertEqual(normalize_path_patterns(['foo/bar', 'foo/bar', 'baz']), ['foo/bar', 'baz'])\n", "    def test_normalize_path_patterns_with_relative_paths(self):\n        expected = [os.path.normcase(p) for p in ['./foo/bar', 'bar/']]\n        self.assertEqual(normalize_path_patterns(['./foo/bar', 'bar/']), expected)\n\n", "    def test_normalize_path_patterns_handles_duplicate_patterns(self):\n        self.assertEqual(\n            normalize_path_patterns(['foo/bar', 'foo/bar', 'baz']),\n            ['foo/bar', 'baz'],\n        )\n\n\n\n", "    def test_normalize_path_patterns_handles_absolute_paths(self):\n        expected = ['/foo/bar', '/bar/*']\n        self.assertEqual(normalize_path_patterns([' /foo/bar ', '/bar/*/']), expected)\n", "    def test_normalize_path_patterns_handles_empty_patterns(self):\n        self.assertEqual(normalize_path_patterns([]), [])\n        self.assertEqual(normalize_path_patterns(['']), [])\n"], "sample_34": ["def test_complex_unit_equivalence():\n    with pytest.raises(TypeError):\n        (u.m / u.s).is_equivalent(u.kg)\n", "def test_unit_summary_table_generation():\n    from .. import astrophys\n    from ...utils import _iter_unit_summary\n\n    summary_table = _iter_unit_summary(astrophys.__dict__)\n    # Check that the summary table does not contain deprecated units\n    for unit_name, _, _, _, _ in summary_table:\n        assert unit_name not in ['old_unit_name']\n\n\n\n", "compilation error", "def test_custom_equivalencies():\n    from astropy import units as u\n\n    class MyEquivalency(u.equivalency):\n            return value * 2\n\n    custom_equivalency = MyEquivalency(name='my_equivalency')\n\n    x = u.m.to(u.km, equivalencies=[custom_equivalency])\n    assert x == 2 * u.m.to(u.km)\n", "def test_unit_in_string():\n    \"\"\"Regression test for https://github.com/astropy/astropy/issues/5078\"\"\"\n    assert u.m.in_string('1 m') is True\n    assert u.m.in_string('1m') is True\n    assert u.m.in_string('1 meter') is True\n    assert u.m.in_string('1 Meter') is True\n    assert u.m.in_string('10 m/s') is False\n    assert u.m.in_string('10 m/s') is False\n    assert u.m.in_string('10 meters/second') is False\n\n\n\n"], "sample_385": ["    def test_get_url_with_namespace(self):\n        from django.urls import reverse\n\n        class MockAdminSite(admin.site):\n                return [\n                    path(\"admin_widgets/\", include(\"admin_widgets.urls\")),\n                ]\n\n        with self.subTest(namespace=\"admin_widgets\"):\n            mock_admin_site = MockAdminSite()\n            rel = Album._meta.get_field(\"band\").remote_field\n            w = AutocompleteSelect(rel, mock_admin_site)\n            url = w.get_url()\n            self.assertEqual(url, \"/admin_widgets/autocomplete/\")\n\n\n\n", "    def test_optgroups(self):\n        beatles = Band.objects.create(name=\"The Beatles\", style=\"rock\")\n        who = Band.objects.create(name=\"The Who\", style=\"rock\")\n        Album.objects.create(name=\"Abbey Road\", band=beatles)\n        Album.objects.create(name=\"Who's Next\", band=who)\n\n        form = AlbumForm(initial={\"band\": beatles.uuid})\n        output = form.as_table()\n        self.assertIn(\n            '<option value=\"%s\" selected>The Beatles</option>' % beatles.uuid, output\n        )\n        self.assertIn(\n            '<option value=\"%s\">The Who</option>' % who.uuid, output\n        )\n", "    def test_autocomplete_url_with_using(self):\n        class TestAdmin(admin.ModelAdmin):\n                return reverse(\"admin_my_app_admin_widgets_album_change\", args=(obj.pk,))\n\n                if db_field.name == 'band':\n                    kwargs['widget'] = AutocompleteSelect(Album._meta.get_field('band'), admin.site, using='alternate_db')\n                return super().formfield_for_dbfield(db_field, **kwargs)\n\n        with override_settings(DATABASES={'default': {'NAME': 'test'}, 'ALTERNATE_DB': {'NAME': 'test_alt'}},):\n            admin.site.register(Album, TestAdmin)\n            form = AlbumForm()\n            url = form[\"band\"].field.widget.get_url()\n            self.assertEqual(url, \"/autocomplete/?using=alternate_db\")\n\n\n\n", "    def test_select2_language_fallback(self):\n        with translation.override(\"zh-hant-tw\"):\n            expected_files = (\n                \"admin/js/vendor/jquery/jquery.min.js\",\n                \"admin/js/vendor/select2/select2.full.min.js\",\n                (\"admin/js/vendor/select2/i18n/zh-TW.js\"),\n                \"admin/js/jquery.init.js\",\n                \"admin/js/autocomplete.js\",\n            )\n            self.assertEqual(\n                AutocompleteSelect(\n                    Album._meta.get_field(\"band\").remote_field, admin.site\n                ).media._js,\n                list(expected_files),\n            )\n\n\n\n", "    def test_media_when_no_language_found(self):\n        \"\"\"\n        When no matching language code is found, falls back to en as the default.\n        \"\"\"\n        with self.subTest(lang=\"zz\"):\n            expected_files = (\n                \"admin/js/vendor/jquery/jquery.min.js\",\n                \"admin/js/vendor/select2/select2.full.min.js\",\n                \"admin/js/jquery.init.js\",\n                \"admin/js/autocomplete.js\",\n                \"admin/js/vendor/select2/i18n/en.js\",\n            )\n            self.assertEqual(\n                AutocompleteSelect(Album._meta.get_field(\"band\").remote_field, admin.site).media._js,\n                list(expected_files),\n            )\n\n"], "sample_97": ["    def test_snapshot_files_with_different_modification_times(self):\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file, self.existing_file]) as mocked_watched_files:\n            mocked_watched_files.return_value = [self.existing_file, self.existing_file]\n            self.increment_mtime(self.existing_file)\n            snapshot = dict(self.reloader.snapshot_files())\n            self.assertIn(self.existing_file, snapshot)\n            self.assertEqual(len(snapshot), 1)\n\n", "    def test_should_stop_does_not_change_after_exception(self):\n        self.reloader.should_stop = True\n        with self.assertRaises(Exception):\n            self._reloader.stop()\n        self.assertTrue(self.reloader.should_stop)\n\n", "    def test_watchman_reloader_uses_existing_watchman_config(self):\n        with mock.patch('django.utils.autoreload.autoreload.WatchmanReloader.check_availability') as mocked_availability:\n            with mock.patch('pywatchman.client', return_value=mock.MagicMock(watch_config=mock.Mock(data={'config': mock.Mock(key='value')}))) as mocked_client:\n                mocked_availability.return_value = True\n                self.reloader = autoreload.WatchmanReloader()\n                self.reloader.watch_dir(self.tempdir, '*.py')\n                self.assertEqual(self.reloader.client.watch_config.data.config.key, 'value')\n\n\n\n", "    def test_snapshot_files_with_glob(self):\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file, self.tempdir / 'does_not_exist.py']):\n            snapshot = list(self.reloader.snapshot_files())\n            self.assertEqual(len(snapshot), 2)\n", "    def test_stat_reloader_with_no_changes(self):\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file]):\n            with mock.patch.object(stats, 'stat', return_value=mock.MagicMock(st_mtime=1, st_size=100)) as mocked_stat:\n                ticker = self.reloader.tick()\n                next(ticker)\n                self.assertEqual(mocked_stat.call_count, 1)\n                next(ticker)\n                self.assertEqual(mocked_stat.call_count, 2)\n\n\n\n"], "sample_446": ["    def test_float_with_decimal_rounding(self):\n        with localcontext() as ctx:\n            ctx.prec = 2\n            self.assertEqual(floatformat(1.23456, 2), \"1.23\")\n            self.assertEqual(floatformat(12.34567, 2), \"12.35\")\n\n\n", "    def test_locale_specific_floatformat(self):\n        with translation.override(\"de\", deactivate=True):\n            self.assertEqual(floatformat(12345.678, '2'), \"12345,68\")\n            self.assertEqual(floatformat(12345.678, '2u'), \"12345.68\")\n            self.assertEqual(floatformat(12345.678, '2g'), \"12.345,68\")\n        with translation.override(\"en\", deactivate=True):\n            self.assertEqual(floatformat(12345.678, '2'), \"12345.68\")\n            self.assertEqual(floatformat(12345.678, '2u'), \"12345.68\")\n            self.assertEqual(floatformat(12345.678, '2g'), \"12,345.68\")\n", "    def test_empty_string(self):\n        self.assertEqual(floatformat(\"\"), \"\")\n", "    def test_percent_format(self):\n        self.assertEqual(floatformat(0.12345, \"%\"), \"12.3% \")\n        self.assertEqual(floatformat(0.012345, \"%\"), \"1.2% \")\n        self.assertEqual(floatformat(1.2345, \"%\"), \"123.5% \")\n        self.assertEqual(floatformat(1.2345, \"%2.2f\"), \"123.45%\")\n        self.assertEqual(floatformat(1.23456789, \"%\"), \"123.5% \")\n        self.assertEqual(floatformat(1.23456789, \"%3.3f\"), \"123.457%\")\n        self.assertEqual(floatformat(-0.12345, \"%\"), \"-12.3% \")\n        self.assertEqual(floatformat(-0.012345, \"%\"), \"-1.2% \")\n        self.assertEqual(floatformat(-1.2345, \"%\"), \"-123.5% \")\n", "    def test_round_digits(self):\n        with localcontext() as ctx:\n            ctx.prec = 2\n            self.assertEqual(floatformat(1.2345, 2), \"1.23\")\n            self.assertEqual(floatformat(15.2042, -3), \"15.204\")\n            self.assertEqual(floatformat(1.2345, \"2\"), \"1.23\")\n            self.assertEqual(floatformat(15.2042, \"-3\"), \"15.204\")\n            self.assertEqual(floatformat(Decimal(\"1.2345\"), 2), \"1.23\")\n            self.assertEqual(floatformat(Decimal(\"15.2042\"), -3), \"15.204\")\n"], "sample_457": ["    def test_constraint_error_message_includes_field_names(self):\n        constraint = models.UniqueConstraint(\n            fields=[\"name\", \"color\"], name=\"name_color_uniq\"\n        )\n        msg = \"Constraint \u201cname_color_uniq\u201d is violated.\"\n        with self.assertRaisesMessage(ValidationError, msg):\n            constraint.validate(\n                UniqueConstraintProduct,\n                UniqueConstraintProduct(name=self.p1.name, color=self.p1.color),\n            )\n\n\n\n", "    def test_name_collision(self):\n        with self.assertRaises(ValueError):\n            models.UniqueConstraint(fields=[\"name\"], name=\"product_name_uniq\")\n            models.UniqueConstraint(fields=[\"name\"], name=\"product_name_uniq\")\n", "    def test_name_already_exists(self):\n        with self.assertRaisesMessage(ValueError, \"A unique constraint with name \u2018name_uniq\u2019 already exists.\"):\n            models.UniqueConstraint(fields=[\"name\"], name=\"name_uniq\", ) \n            models.UniqueConstraint(fields=[\"name\"], name=\"name_uniq\", )    \n", "    def test_name_already_exists(self):\n        with self.assertRaisesMessage(ValueError, \"A unique constraint with this name already exists.\"):\n            models.UniqueConstraint(fields=[\"name\"], name=\"name_uniq\")\n            models.UniqueConstraint(fields=[\"name\"], name=\"name_uniq\")\n\n\n\n\n", "    def test_unique_constraint_with_deferred_and_immediate_behavior(self):\n        with self.assertRaises(ValueError):\n            models.UniqueConstraint(\n                fields=[\"field\"],\n                name=\"unique_constraint\",\n                deferrable=models.Deferrable.DEFERRED,\n                immediate=True,  \n            )\n"], "sample_578": ["    def test_baseline(self, x, y):\n\n        p = Plot(x, y).add(Bars(baseline=2)).plot()\n        ax = p._figure.axes[0]\n        paths = ax.collections[0].get_paths()\n        for i, path in enumerate(paths):\n            verts = path.vertices\n            assert verts[0, 1] == pytest.approx(y[i] - 2)\n            assert verts[3, 1] == pytest.approx(y[i] - 2 + y[i])\n", "    def test_baseline(self, x, y):\n\n        p = Plot(x, y).add(Bars(baseline=3)).plot()\n        ax = p._figure.axes[0]\n        paths = ax.collections[0].get_paths()\n        for i, path in enumerate(paths):\n            verts = path.vertices\n            assert verts[0, 1] == pytest.approx(y[i] - 3)\n            assert verts[2, 1] == pytest.approx(y[i] - 3) \n", "    def test_baseline(self, x, y):\n\n        p = Plot(x, y).add(Bars(baseline=2)).plot()\n        ax = p._figure.axes[0]\n        paths = ax.collections[0].get_paths()\n        for i, path in enumerate(paths):\n            verts = path.vertices\n            assert verts[0, 1] == pytest.approx(y[i] + 2)\n            assert verts[3, 1] == pytest.approx(y[i] + 2)\n", "    def test_baseline(self, x, y):\n\n        p = Plot(x, y).add(Bars(baseline=2)).plot()\n        ax = p._figure.axes[0]\n        paths = ax.collections[0].get_paths()\n        for i, path in enumerate(paths):\n            verts = path.vertices\n            assert verts[0, 1] == pytest.approx(y[i] - 2)\n            assert verts[3, 1] == pytest.approx(y[i] - 2 + verts[1, 1] ) \n\n\n", "    def test_baseline(self, x, y):\n\n        p = Plot(x, y).add(Bars(baseline=3)).plot()\n        ax = p._figure.axes[0]\n        paths = ax.collections[0].get_paths()\n        for i, path in enumerate(paths):\n            verts = path.vertices\n            assert verts[3, 1] == y[i] + 3\n\n\n\n"], "sample_301": ["    def test_delete_snapshot_files_updates(self):\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file]):\n            snapshot = dict(self.reloader.snapshot_files())\n            self.assertIn(self.existing_file, snapshot)\n            self.reloader._delete_snapshot_files()\n            self.assertEqual(dict(self.reloader.snapshot_files()), {})\n\n\n\n", "    def test_notify_file_changed(self):\n        with mock.patch('django.utils.autoreload.common_log') as mocked_log:\n            self.reloader.notify_file_changed(self.existing_file)\n            self.assertIn('File changed:', mocked_log.call_args[0][0])\n", "    def test_snapshot_files_handles_os_errors(self):\n        with mock.patch.object(os, 'stat', side_effect=OSError()) as mock_stat:\n            with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file]):\n                snapshot = dict(self.reloader.snapshot_files())\n                self.assertEqual(snapshot, {})\n                mock_stat.assert_called_once_with(str(self.existing_file))\n", "    def test_should_stop_returns_false_when_no_errors(self):\n        with mock.patch('django.utils.autoreload._exception', None):\n            self.assertFalse(self.reloader.should_stop())\n", "    def test_run_loop_stops_on_error(self):\n            yield\n            raise Exception()\n\n        with mock.patch.object(self.reloader, 'tick', side_effect=mocked_tick) as tick:\n            with self.assertRaises(Exception):\n                self.reloader.run_loop()\n        self.assertEqual(tick.call_count, 1)\n\n\n\n"], "sample_318": ["    def test_lookahead_with_optional_capture_group(self):\n        test_urls = [\n            ('/lookahead-optional/?', {'city': 'a-city'}),\n            ('/lookahead-optional/?', {'city': 'another-city'}),\n            ('/lookahead-optional/?', {})\n        ]\n        for test_url, kwargs in test_urls:\n            with self.subTest(url=test_url, kwargs=kwargs):\n                self.assertEqual(resolve(test_url).kwargs, kwargs)\n", "    def test_lookahead_group_capture(self):\n        test_urls = [\n            ('/lookahead-group/a-city/123/', {'city': 'a-city', 'number': '123'}),\n        ]\n        for test_url, kwargs in test_urls:\n            with self.subTest(url=test_url):\n                match = resolve(test_url)\n                self.assertEqual(match.kwargs, kwargs)\n\n\n", "    def test_lookahead_capture_group(self):\n        test_urls = [\n            ('/lookahead/a-city/123/', {'city': 'a-city', 'id': '123'}),\n        ]\n        for test_url, kwargs in test_urls:\n            with self.subTest(url=test_url):\n                match = resolve(test_url)\n                self.assertEqual(match.kwargs, kwargs)\n                self.assertEqual(match.args, ('123',))\n", "    def test_lookahead_and_lookbehind_empty_string(self):\n        test_urls = [\n            ('/lookahead-empty', {}, '/lookahead/-/'),\n            ('/lookbehind-empty', {}, '/lookbehind/-/'),\n        ]\n        for name, kwargs, expected in test_urls:\n            with self.subTest(name=name, kwargs=kwargs):\n                self.assertEqual(reverse(name, kwargs=kwargs), expected)\n                with self.assertRaises(NoReverseMatch):\n                    reverse(name, kwargs={'city': 'a-city'})\n", "    def test_lookahead_capture_group(self):\n        test_urls = [\n            ('/lookahead-group/a-city/'),\n            ('/lookahead-group/b-city/'),\n        ]\n        for test_url in test_urls:\n            with self.subTest(url=test_url):\n                match = resolve(test_url)\n                self.assertEqual(match.kwargs, {'city': 'a-city'})\n                self.assertEqual(match.args, ('b-city',))\n\n\n\n"], "sample_1196": ["def test_contains_commutative():\n    x = Symbol('x')\n    y = Symbol('y')\n    assert Contains(x, FiniteSet(y)) == Contains(y, FiniteSet(x))\n    assert Contains(x, S.Integers) == Contains(S.Integers, x)\n", "def test_contains_empty_set():\n    x = Symbol('x')\n    assert Contains(x,  S.EmptySet) is S.false\n", "def test_contains_with_symbols():\n    x = Symbol('x')\n    y = Symbol('y')\n    z = Symbol('z')\n    assert Contains(x, FiniteSet(y, z)).args[1] == FiniteSet(y, z)\n    assert Contains(x, FiniteSet(y, z)).args[0] == x\n", "def test_contains_with_symbols():\n    x = Symbol('x')\n    y = Symbol('y')\n    z = Symbol('z')\n\n    assert Contains(x, FiniteSet(y, z)).args[0] == x\n    assert Contains(x, FiniteSet(y, z)).args[1] == FiniteSet(y, z) \n", "def test_contains_empty_set():\n    x = Symbol('x')\n    assert Contains(x, S.EmptySet) is S.false\n"], "sample_608": ["    def test_format_item_with_different_types(\n        self, value_type", "    def test_diff_dataset_repr_equals_attr(self, compat) -> None:\n        ds_a = xr.Dataset(\n            data_vars={\"var1\": ((\"x\", \"y\"), np.array([[1, 2, 3], [4, 5, 6]], dtype=\"int64\"))},\n            coords={\"x\": np.array([\"a\", \"b\"], dtype=\"U1\"), \"y\": np.array([1, 2, 3], dtype=\"int64\")},\n            attrs={\"units\": \"m\"},\n        )\n\n        ds_b = xr.Dataset(\n            data_vars={\"var1\": ((\"x\", \"y\"), np.array([[1, 2, 3], [4, 5, 6]], dtype=\"int64\"))},\n            coords={\"x\": np.array([\"a\", \"b\"], dtype=\"U1\"), \"y\": np.array([1, 2, 3], dtype=\"int64\")},\n            attrs={\"units\": \"m\"},\n        )\n\n        expected = dedent(\n            \"\"\"\\\n        Left and right Dataset objects are %s\n        Differing attributes:\n        L   units: m\n        R   units: m\n        Attributes only on the left object:\n            \"\"\"\n            % compat\n        )\n\n        actual = formatting.diff_dataset_repr(ds_a, ds_b, compat)\n        assert actual == expected\n\n\n\n", "    def test__get_numpy_repr_for_array_with_custom_repr(self) -> None:\n        class CustomArray:\n                self.value = value\n\n                return f\"CustomArray({self.value})\"\n\n                return NotImplemented\n\n        array = CustomArray(np.arange(5))\n        actual = formatting._get_numpy_repr_for_array(array)\n        assert actual == \"CustomArray(arange(5))\"\n\n\n", "    def test_format_attrs_repr_with_array_and_empty(self) -> None:\n        attrs_a = {\"attr\": np.array([0, 1])}\n        attrs_b = {}\n\n        expected = dedent(\n            \"\"\"\\\n            Differing attributes:\n            L   attr: [0 1]\n            R  \"\"\"\n        ).strip()\n        actual = formatting.diff_attrs_repr(attrs_a, attrs_b, \"equals\")\n        assert expected == actual\n", "    def test_format_timestamp_with_microseconds(self) -> None:\n        expected = \"2023-12-14 17:00:00.123456\"\n        result = formatting.format_timestamp(pd.Timestamp(\"2023-12-14 17:00:00.123456\"))\n        assert result == expected\n"], "sample_1147": ["def test_latex_printing_issue_18487():\n    from sympy import symbols, sin, cos, simplify, integrate\n    x, y = symbols('x y')\n    expr = simplify(integrate(sin(x) * cos(x), (x, 0, pi/2)))\n\n    assert latex(expr) == r' \\frac{1}{2}' \n", "compilation error", "def test_latex_printing_for_matrices_with_symbols_as_entries():\n    x, y, z = symbols('x y z')\n    A = MatrixSymbol(\"A\", 2, 2)\n    A[0,0] = x\n    A[0,1] = y\n    A[1,0] = z\n    A[1,1] = 1 \n    assert latex(A) == r'\\begin{bmatrix} x & y \\\\ z & 1 \\end{bmatrix}'\n\n", "compilation error", "def test_printing_latex_matrices_with_symbols_in_elements():\n    x, y, z = symbols('x y z')\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    assert latex(A*(x*B + y*eye(2))) == r'A \\left(x B + y \\mathbf{I}_{2}\\right)'\n\n\n\n\n"], "sample_1008": ["compilation error", "compilation error", "compilation error", "def test_issue_12083():\n    N = ReferenceFrame('N')\n    A = N.orientnew('A', 'Axis', [0, N.z])\n    B = A.orientnew('B', 'Axis', [pi/4, A.x])\n    C = B.orientnew('C', 'Axis', [pi/2, B.y])\n\n    assert N.dcm(A) == Matrix([[cos(0), 0, sin(0)], [0, 1, 0], [-sin(0), 0,\n    cos(0)]])\n    assert N.dcm(B) == Matrix([[cos(0), 0, sin(pi/4)], [0, 1, 0], [-sin(0),\n    0, cos(pi/4)]])\n    assert N.dcm(C) == Matrix([[cos(pi/4), 0, sin(pi/2)] , [ -sin(pi/4),\n    1, 0], [0, 0, cos(pi/2)]])\n\n\n\n\n\n\n\n", "compilation error"], "sample_218": ["    def test_datetime_trunc_with_invalid_timezone(self):\n        start_datetime = datetime(2015, 6, 15, 14, 30, 50, 321)\n        end_datetime = datetime(2016, 6, 15, 14, 10, 50, 123)\n        start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n        end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n        self.create_model(start_datetime, end_datetime)\n\n        with self.assertRaises(pytz.UnknownTimeZoneError):\n            DTModel.objects.annotate(trunc=Trunc('start_datetime', 'week', tzinfo='INVALID_TIMEZONE'))\n", "    def test_trunc_with_timezone_aware_and_naive(self):\n        naive_datetime = datetime(2015, 6, 15, 14, 30, 50, 321)\n        aware_datetime = timezone.make_aware(naive_datetime, is_dst=False)\n\n        self.create_model(aware_datetime, naive_datetime)\n        melb = pytz.timezone('Australia/Melbourne')\n\n        self.assertQuerysetEqual(\n            DTModel.objects.annotate(\n                truncated=Trunc('start_datetime', 'day', output_field=DateField(), tzinfo=melb)\n            ).order_by('start_datetime'),\n            [\n                (aware_datetime, truncate_to(aware_datetime.astimezone(melb).date(), 'day')),\n                (naive_datetime, truncate_to(naive_datetime.date(), 'day')),\n            ],\n            lambda m: (m.start_datetime, m.truncated)\n        )\n\n", "    def test_trunc_with_custom_timezone(self):\n        start_datetime = datetime(2015, 6, 15, 14, 30, 50, 321)\n        end_datetime = datetime(2016, 6, 15, 14, 10, 50, 123)\n        start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n        end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n        self.create_model(start_datetime, end_datetime)\n\n        custom_tz = pytz.timezone('America/Chicago')\n        self.assertQuerysetEqual(\n            DTModel.objects.annotate(\n                truncated=TruncWeek(\n                    'start_datetime', output_field=DateTimeField(), tzinfo=custom_tz\n                )\n            ).order_by('start_datetime'),\n            [\n                (start_datetime, truncate_to(start_datetime.astimezone(custom_tz), 'week', custom_tz)),\n                (end_datetime, truncate_to(end_datetime.astimezone(custom_tz), 'week', custom_tz))\n            ],\n            lambda m: (m.start_datetime, m.truncated)\n        )\n\n\n\n", "    def test_trunc_func_with_timezone_and_output_field(self):\n        start_datetime = datetime(2015, 6, 15, 14, 30, 50, 321)\n        end_datetime = datetime(2016, 6, 15, 14, 10, 50, 123)\n        start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n        end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n        self.create_model(start_datetime, end_datetime)\n\n        melb = pytz.timezone('Australia/Melbourne')\n\n        self.assertQuerysetEqual(\n            DTModel.objects.annotate(\n                truncated_date=Trunc('start_datetime', 'day', output_field=DateField(), tzinfo=melb)\n            ).order_by('start_datetime'),\n            [\n                (start_datetime, start_datetime.date()),\n                (end_datetime, end_datetime.date())\n            ],\n            lambda m: (m.start_datetime, m.truncated_date)\n        )\n        self.assertQuerysetEqual(\n            DTModel.objects.annotate(\n                truncated_time=Trunc('start_datetime', 'hour', output_field=TimeField(), tzinfo=melb)\n            ).order_by('start_datetime'),\n            [\n                (start_datetime, start_datetime.time()),\n                (end_datetime, end_datetime.time())\n            ],\n            lambda m: (m.start_datetime, m.truncated_time)\n        )\n\n\n", "    def test_trunc_func_with_timezone_custom_timezone(self):\n        start_datetime = datetime(2015, 6, 15, 14, 30, 50, 321)\n        end_datetime = datetime(2016, 6, 15, 14, 10, 50, 123)\n        start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n        end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n        self.create_model(start_datetime, end_datetime)\n        custom_timezone = pytz.timezone('Etc/GMT-10')\n\n            self.assertQuerysetEqual(\n                DTModel.objects.annotate(\n                    truncated=Trunc('start_datetime', kind, output_field=DateTimeField(), tzinfo=custom_timezone)\n                ).order_by('start_datetime'),\n                [\n                    (start_datetime, truncate_to(start_datetime.astimezone(custom_timezone), kind, custom_timezone)),\n                    (end_datetime, truncate_to(end_datetime.astimezone(custom_timezone), kind, custom_timezone)),\n                ],\n                lambda m: (m.start_datetime, m.truncated)\n            )\n\n        test_datetime_kind('year')\n        test_datetime_kind('quarter')\n        test_datetime_kind('month')\n        test_datetime_kind('week')\n        test_datetime_kind('day')\n        test_datetime_kind('hour')\n        test_datetime_kind('minute')\n        test_datetime_kind('second')\n\n\n\n"], "sample_977": ["def test_Complex():\n    assert mcode(S.Complex(2, 3)) == \"2 + 3 I\"\n    assert mcode(S.I) == \"I\"\n    assert mcode(S.Im(x + y*I)) == \"Im[x + y*I]\"\n    assert mcode(S.Re(x + y*I)) == \"Re[x + y*I]\"\n", "def test_user_functions():\n    mcode_printer = MCodePrinter({'user_functions': {'myfunc': [(lambda x: True, 'MyFunc')]})\n    assert mcode_printer.doprint(myfunc(x)) == 'MyFunc[x]'\n", "def test_Function_With_Arguments():\n    assert mcode(Function('foo')(x, y, z, 1, 2.5)) == \"foo[x, y, z, 1, 2.5]\"\n", "def test_user_functions():\n    mcode(x**2, user_functions={'Square': lambda x: x**2})\n", "def test_user_functions():\n        return x**2 + 1\n    print(mathematica_code(my_func(x), user_functions={'my_func': ['lambda x: True', 'my_func']}))\n"], "sample_351": ["    def test_queryset_limit_choices_to_is_respected(self):\n        limit_choices_to = lambda: {'pk__in': (self.c1.pk, self.c2.pk)}\n        f = forms.ModelChoiceField(Category.objects, limit_choices_to=limit_choices_to)\n        self.assertEqual(len(f.choices), 2)\n        self.assertEqual(list(f.choices), [\n            ('', '---------'),\n            (self.c1.pk, 'Entertainment'),\n            (self.c2.pk, 'A test'),\n        ])\n\n\n", "    def test_to_field_name_on_multi_choice(self):\n        f = forms.ModelMultipleChoiceField(\n            Category.objects.all(),\n            to_field_name='slug',\n            widget=forms.CheckboxSelectMultiple\n        )\n        self.assertEqual(\n            f.clean([self.c1.pk, self.c2.pk]).all().values_list('slug', flat=True),\n            ['entertainment', 'test']\n        )\n", "    def test_to_python_with_empty_value(self):\n        f = forms.ModelChoiceField(Category.objects.all())\n        self.assertEqual(f.to_python(None), None)\n        self.assertEqual(f.to_python(' '), None)\n        self.assertEqual(f.to_python(''), None)\n", "    def test_empty_label_with_no_queryset(self):\n        f = forms.ModelChoiceField(queryset=None, empty_label=\"---------\")\n        self.assertEqual(f.choices, [('---------', '')])\n        with self.assertRaises(ValueError):\n            f.clean('');\n", "    def test_queryset_limit_choices_to(self):\n        f = forms.ModelChoiceField(Category.objects.all(), limit_choices_to={'name__startswith': 'A'})\n        self.assertEqual(len(f.choices), 1)\n        self.assertEqual(list(f.choices), [\n            ('', '---------'),\n            (self.c2.pk, 'A test'),\n        ])\n\n\n"], "sample_526": ["def test_DateFormatter_tz_warning():\n    with pytest.warns(UserWarning, match=\"DateFormatter tzinfo\"):\n        formatter = mdates.DateFormatter('%Y-%b-%d %H:%M')\n        formatter(mdates.date2num(datetime.datetime(2011, 1, 1)))\n", "def test_DateFormatter_tz_mismatch():\n    time = mdates.date2num(datetime.datetime(2011, 1, 1, 0, 0,\n                tzinfo=mdates.UTC))\n    formatter = mdates.DateFormatter('%Y-%b-%d %H:%M')\n    formatter.set_tzinfo('Pacific/Kiritimati')\n    with pytest.raises(ValueError, match=\"Timezone mismatch\"):\n        formatter(time, tz=mdates.UTC)\n", "def test_DateFormatter_format_dt_with_tz_info(tz_str, expected):\n    formatter = mdates.DateFormatter('%Y-%b-%d %H:%M')\n    formatter.tz = tz_str\n    dt = datetime.datetime(2011, 1, 1, 0, 0, tzinfo=dateutil.tz.tzutc())\n    assert formatter.format_dt(dt) == expected.strftime('%Y-%b-%d %H:%M')\n", "def test_get_epoch_from_datetime():\n    with pytest.raises(RuntimeError):\n      mdates._reset_epoch_test_example()\n      dt = datetime.datetime(2023, 10, 26)\n      epoch = mdates.get_epoch(dt)\n    mdates._reset_epoch_test_example()\n    epoch = mdates.get_epoch()\n    assert epoch == datetime.datetime(1970, 1, 1, 0, 0, tzinfo=mdates.UTC)\n\n", "def test_DateFormatter_invalid_tz():\n    time = mdates.date2num(datetime.datetime(2011, 1, 1, 0, 0,\n                tzinfo=mdates.UTC))\n    formatter = mdates.DateFormatter('%Y-%b-%d %H:%M')\n    with pytest.raises(ValueError, match=\"Invalid timezone\"):\n        formatter.set_tzinfo('NotAValidTimezone')\n\n\n\n"], "sample_737": ["def test_vectorizer_empty_vocabulary():\n    for vec in [CountVectorizer(), TfidfVectorizer(), HashingVectorizer()]:\n        vect = vec()\n        with assert_raises(ValueError):\n            vect.fit(JUNK_FOOD_DOCS)\n", "def test_stop_words_removal(stop_words):\n    vect = CountVectorizer(stop_words=stop_words)\n    X = vect.fit_transform(JUNK_FOOD_DOCS)\n    assert_array_equal(X.shape, (len(JUNK_FOOD_DOCS), len(vect.vocabulary_)))\n", "def test_countvectorizer_preprocessor_pipe():\n    from sklearn.base import BaseEstimator\n\n    class MyPreprocessor(BaseEstimator):\n            self.char_to_remove = char_to_remove\n\n            return self\n\n            return [doc.replace(self.char_to_remove, ' ') for doc in X]\n\n    vect = CountVectorizer(preprocessor=MyPreprocessor(' '),\n                           stop_words='english')\n    docs = ['one two three',\n            'four  five six',\n            '   seven eight  nine  ']\n    X = vect.fit_transform(docs)\n    expected_result = [\n        [1, 1, 1],\n        [1, 1, 1],\n        [1, 1, 1]\n    ]\n    assert_array_equal(X.toarray(), expected_result)\n", "    def test_vectorizers_custom_analyzer(analyzer):\n        vect1 = CountVectorizer(analyzer=analyzer)\n        vect2 = TfidfVectorizer(analyzer=analyzer)\n        vect3 = HashingVectorizer(analyzer=analyzer)\n\n        data = [\"This is a test.\", \"This is another test.\"]\n        X1 = vect1.fit_transform(data)\n        X2 = vect2.fit_transform(data)\n        X3 = vect3.fit_transform(data)\n\n        assert_array_equal(X1.toarray(), X2.toarray())\n        assert_array_equal(X1.toarray(), X3.toarray())\n", "def test_vectorizer_empty_corpus():\n    for vectorizer_class in (CountVectorizer, TfidfVectorizer, HashingVectorizer):\n        vect = vectorizer_class()\n        with pytest.raises(ValueError) as exc_info:\n            vect.fit([])\n        assert \"empty corpus\" in str(exc_info.value)\n\n\n"], "sample_738": ["def test_vectorizer_stop_words_removal_english():\n    #  Test stop words removal with English stop words\n    cv = CountVectorizer(stop_words='english')\n    X = cv.fit_transform(JUNK_FOOD_DOCS)\n    assert_array_equal(\n        X.toarray()[0], [0, 0, 0, 1, 0, 0, 0, 0, 0])  # \"junk food\" should be present\n\n\n", "def test_vectorizer_analyzer(analyzer):\n    test_data = [['this', 'is', 'a', 'test'], ['this', 'is', 'another', 'test']]\n    for vec in [CountVectorizer, TfidfVectorizer, HashingVectorizer]:\n        vect = vec(analyzer=analyzer)\n        X = vect.fit_transform(test_data)\n        if analyzer == 'word':\n            if isinstance(vect, HashingVectorizer):\n                assert_array_equal(X.toarray(), [[1, 1, 1, 1], [1, 1, 1, 1]])  # all words are 1\n            else:\n                assert_array_almost_equal(X.toarray(),\n                                       [[1, 1, 1, 1], [1, 1, 1, 1]])\n        elif analyzer == 'char':\n            assert_array_almost_equal(X.toarray(),\n                                       [[1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1]])\n        elif analyzer == 'char_wb':\n            assert_array_almost_equal(\n                X.toarray(),\n                [[1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1]],\n            )\n\n", "def test_vectorizer_pipeline_empty_input():\n    pipeline = Pipeline([('vect', CountVectorizer()),\n                         ('svc', LinearSVC())])\n\n    # check that pipeline handle empty dataset correctly\n\n    cv_empty = pipeline.fit([])\n    assert_raises(ValueError, cv_empty.predict, [])\n\n\n", "def test_vectorizer_with_max_features():\n    vect = CountVectorizer(max_features=5)\n    X = vect.fit_transform(JUNK_FOOD_DOCS)\n    assert_equal(len(vect.vocabulary_), 5)\n    assert_equal(X.shape[1], 5)\n    \n    vect = HashingVectorizer(max_features=5)\n    X = vect.transform(JUNK_FOOD_DOCS)\n    assert_equal(X.shape[1], 5)\n\n    vect = TfidfVectorizer(max_features=5)\n    X = vect.fit_transform(JUNK_FOOD_DOCS)\n    assert_equal(X.shape[1], 5)\n\n", "def test_vectorizer_empty_vocabulary():\n    # When vocabulary is empty, the fit method should raise an error\n    vectorizers = [CountVectorizer(), TfidfVectorizer(), HashingVectorizer()]\n    for vect in vectorizers:\n        with assert_raises(ValueError) as context:\n            vect.fit(JUNK_FOOD_DOCS, vocabulary=[])\n        assert \"Vocabulary is empty\" in str(context.exception)\n\n\n\n"], "sample_1041": ["def test_transpose_simplify():\n    A = MatrixSymbol('A', n, m)\n    assert (A.T).T == A\n    assert Transpose(A).T == A\n    assert transpose(A).T == A\n    assert simplify(Transpose(MatAdd(A, A.T))) == A\n    assert simplify(transpose(MatAdd(A, A.T))) == A\n\n\n", "def test_matrix_transpose():\n    A = MatrixSymbol('A', n, m)\n    B = Transpose(A)\n    assert B.shape == (m, n)\n    assert B[i, j] == A[j, i]\n    assert (Transpose(B)).shape == (n, m)\n    assert (Transpose(A)).shape == (m, n)\n    assert transpose(A).shape == (m, n)\n    assert (Transpose(Transpose(A))).as_explicit() == A.as_explicit()\n    assert (A*B).shape == (n, n)\n    assert (B*A).shape == (m, m)\n\n    assert (Transpose(A*B) ).as_explicit() == (B.T * A.T ).as_explicit()\n    raises(TypeError, lambda: Transpose(1))\n\n\n\n", "def test_transpose_inplace():\n    A = MatrixSymbol('A', 3, 4)\n    B = A.transpose()\n    assert B != A\n    assert (A.transpose() * A) == (A * A.transpose())\n    A.transpose_inplace()\n    assert A == B\n\n\n\n", "    def test_transpose_identity():\n        assert transpose(Identity(n)) == Identity(n)\n", "def test_transpose_commutativity():\n    A = MatrixSymbol('A', n, m)\n    assert (Transpose(A)).T == A\n    assert Transpose(Transpose(A)) == A\n    assert Transpose(A + B) == Transpose(A) + Transpose(B)\n    assert Transpose(A * B) == Transpose(B).T * Transpose(A).T\n\n\n\n"], "sample_772": ["def check_missing_values(name):\n    ForestEstimator = FOREST_ESTIMATORS[name]\n\n    # Test with nan values\n    X = np.array([[1, 2, np.nan], [4, 5, 6], [7, 8, 9]])\n    y = [0, 1, 0]\n    est = ForestEstimator(random_state=0)\n\n    # Check for error\n    assert_raises(ValueError, est.fit, X, y) \n\n    # Test with a mix of integers and floats\n    X = np.array([[1, 2.0, 3], [4, 5.0, np.nan], [7, 8, 9]])\n    y = [0, 1, 0]\n    est = ForestEstimator(random_state=0)\n    assert_raises(ValueError, est.fit, X, y) \n\n\n", "def check_estimator_with_sparse_data(name):\n    X, y = datasets.make_classification(n_samples=100, n_features=20,\n                random_state=0)\n    X_sparse = sp.csr_matrix(X)\n    ForestEstimator = FOREST_ESTIMATORS[name]\n    est = ForestEstimator(random_state=0, n_estimators=5)\n\n    est.fit(X, y)\n    assert_array_equal(est.apply(X), est.apply(X_sparse))\n", "def test_set_params(name):\n    X, y = hastie_X, hastie_y\n    ForestEstimator = FOREST_ESTIMATORS[name]\n    clf = ForestEstimator(random_state=1, max_depth=1)\n    assert_equal(clf.max_features, clf.get_params()[\"max_features\"])\n    clf.set_params(max_features='sqrt')\n    assert_equal(clf.max_features, 'sqrt')\n    assert_equal(clf.get_params()[\"max_features\"], 'sqrt')\n    \n    # Check that setting a parameter will not change other parameters\n    clf.set_params(max_depth=2)\n    assert_equal(clf.max_features, 'sqrt')\n", "def test_fit_params(name):\n    X = np.random.rand(100, 5)\n    y = np.random.randint(2, size=100)\n    ForestEstimator = FOREST_ESTIMATORS[name]\n    est = ForestEstimator(n_estimators=10, max_depth=3)\n    est.fit(X, y)\n    for param in ['n_estimators', 'max_depth']:\n        assert_equal(est.get_params()[param], getattr(est, param))\n\n", "def test_sparse_array(name):\n    X = sparse_array(np.random.randn(10, 5))\n    y = np.random.randint(2, size=10)\n    ForestEstimator = FOREST_ESTIMATORS[name]\n    est = ForestEstimator(random_state=0)\n    est.fit(X, y)\n\n\n"], "sample_727": ["def test_imputation_sparse_in_dense_pipeline():\n    # Test imputation with a sparse matrix within a pipeline with a dense estimator\n\n    from sklearn.linear_model import LinearRegression\n\n    X = sparse_random_matrix(10, 5, density=0.2)\n    y = np.random.randn(10)\n\n    pipeline = Pipeline([\n        ('imputer', Imputer(missing_values=0, strategy='mean')),\n        ('model', LinearRegression())\n    ])\n\n    pipeline.fit(X, y)\n    X_new = sparse_random_matrix(10, 5, density=0.2)\n    y_pred = pipeline.predict(X_new)\n\n    assert_array_almost_equal(y_pred, pipeline.predict(X_new.toarray()))\n", "def test_imputation_sparse_missing_values_not_zero():\n    # Test imputation with sparse arrays and missing_values != 0\n    rng = np.random.RandomState(0)\n\n    dim = 10\n    dec = 10\n    shape = (dim * dim, dim + dec)\n\n    zeros = np.zeros(shape[0])\n    values = np.arange(1, shape[0] + 1)\n    values[4::2] = - values[4::2]\n\n    for strategy in ['mean', 'median', 'most_frequent']:\n        X = np.empty(shape)\n        X_true = np.empty(shape)\n\n        # Create a matrix X with columns\n        #    - with only zeros,\n        #    - with only missing values\n        #    - with zeros, missing values and values\n        # And a matrix X_true containing all true values\n        for j in range(shape[1]):\n            nb_zeros = (j - dec + 1 > 0) * (j - dec + 1) * (j - dec + 1)\n            nb_missing_values = max(shape[0] + dec * dec\n                                    - (j + dec) * (j + dec), 0)\n            nb_values = shape[0] - nb_zeros - nb_missing_values\n\n            z = zeros[:nb_zeros]\n            p = np.repeat(j, nb_missing_values)\n            v = values[rng.permutation(len(values))[:nb_values]]\n\n            X[:, j] = np.hstack((v, z, p))\n            X_true[:, j] = np.hstack((v, z, p))\n\n        # Mean doesn't support columns containing NaNs, median does\n        if strategy == \"median\":\n            cols_to_keep = ~np.isnan(X_true).any(axis=0)\n        else:\n            cols_to_keep = ~np.isnan(X_true).all(axis=0)\n\n        X_true = X_true[:, cols_to_keep]\n\n        # Create and fit the Imputer object\n        imputer = Imputer(missing_values=j, strategy=strategy)", "def test_imputation_sparse_with_different_features_types():\n    # Test imputation with sparse matrices containing different feature types.\n    # In particular, test imputing np.nan values with numerical data.\n    X = sparse.csc_matrix(\n        [[1.0, 2.0, np.nan],\n         [4.0, 5.0, 6.0],\n         [7.0, np.nan, 9.0]],\n        dtype=np.float)\n\n    X_true = sparse.csc_matrix(\n        [[1.0, 2.0, 3.0],\n         [4.0, 5.0, 6.0],\n         [7.0, 5.0, 9.0]],\n        dtype=np.float)\n\n    imputer = Imputer(missing_values=np.nan, strategy=\"mean\")\n    X_imputed = imputer.fit_transform(X)\n    assert_array_almost_equal(X_imputed.toarray(), X_true.toarray())\n\n\n\n", "def test_imputation_sparse_masking():\n    # Test imputation with sparse masking.\n    X = sparse_random_matrix(10, 10, density=0.3, random_state=0)\n    mask = np.zeros(X.shape[0], dtype=bool)\n    mask[2:6] = True\n    X_masked = X[mask]\n\n    for strategy in [\"mean\", \"median\", \"most_frequent\"]:\n        imputer = Imputer(missing_values=X.data[0], strategy=strategy)\n        imputer.fit(X)\n        X_imputed = imputer.transform(X)\n\n        assert_false(np.isnan(X_imputed).any())\n        assert_array_almost_equal(\n            X_imputed[mask], X_masked, err_msg=\"Fail to impute \"\n            \"correctly with strategy = %s\" % (strategy)\n        )\n\n\n", "def test_imputation_bad_input():\n    X = sparse_random_matrix(5, 5, density=0.75, random_state=0)\n\n    # Test with invalid strategies\n    for strategy in [\"invalid_strategy\", None, 1, [], \"\"]:\n        with assert_raises(ValueError):\n            Imputer(missing_values=0, strategy=strategy)\n    \n    # Test with invalid missing_values\n    with assert_raises(ValueError):\n        Imputer(missing_values=\"invalid_string\", strategy=\"mean\")\n    with assert_raises(ValueError):\n        Imputer(missing_values=[1, 2, 3], strategy=\"mean\")\n"], "sample_1069": ["def test_trigtransform():\n    for f in (sincos, cosin, sin, cos, tan, cot, sec, csc):\n        assert octave_code(f(x)) == f.__name__ + '(x)'\n    assert octave_code(sincos(x)) == 'sincos(x)'\n\n", "def test_polylog():\n    assert octave_code(polylog(n, x)) == 'polylog(n, x)'\n", "def test_polylog_printing():\n    assert octave_code(polylog(n, x)) == 'polylog(n, x)'\n", "def test_hypergeometric():\n    assert octave_code(hypergeometric(a, b, c, x)) == 'hypergeometric(a, b, c, x)'\n    assert octave_code(MeijerG(A, B, C, D, z)) == 'meijerG(A, B, C, D, z)'\n", "def test_nonconstant_symbols():\n    x, y, z = symbols('x y z')\n    assert mcode(x + y, assign_to='result') == 'result = x + y;'\n    assert mcode(2*x, assign_to='twiceX') == 'twiceX = 2*x;'\n    assert mcode(x**2 + y*z, assign_to='expression') == 'expression = x.^2 + y.*z;'\n    assert mcode(x/(y + z), assign_to='quotient') == 'quotient = x./(y + z);'\n"], "sample_286": ["    def test_refresh_with_related_manager(self):\n        a = Article.objects.create(pub_date=datetime.now())\n        s = SelfRef.objects.create(article=a)\n\n        with self.assertNumQueries(1):\n            a.refresh_from_db()\n        with self.assertNumQueries(1):\n            s.refresh_from_db()\n            self.assertIsInstance(s.article, Article)\n        s.article = None\n        s.save()\n        with self.assertNumQueries(1):\n            s.refresh_from_db(fields=['article'], using='default')\n        self.assertIsNone(s.article)\n\n", "    def test_refresh_only_specific_fields(self):\n        a = Article.objects.create(\n            headline='Parrot programs in Python',\n            pub_date=datetime(2005, 7, 28),\n        )\n        with self.assertNumQueries(1):\n            a.refresh_from_db(fields=['headline'])\n        self.assertEqual(a.headline, 'Parrot programs in Python')\n        self.assertNotEqual(a._state.fields, {})\n        with self.assertNumQueries(1):\n            a.refresh_from_db(fields=['headline', 'pub_date'])\n        self.assertEqual(a.headline, 'Parrot programs in Python')\n        self.assertEqual(a.pub_date, datetime(2005, 7, 28))\n        with self.assertNumQueries(1):\n            a.refresh_from_db(fields=[])\n\n\n\n", "    def test_refresh_from_db_with_select_related(self):\n        article = Article.objects.create(\n            headline='Parrot programs in Python',\n            pub_date=datetime(2005, 7, 28),\n        )\n        featured = FeaturedArticle.objects.create(article=article)\n        # Pre-fetch the related article to populate the cache.\n        featured.article = featured.article.objects.prefetch_related('selfref_set').first()\n        # Verify initial state.\n        self.assertEqual(featured.article.selfref_set.all().count(), 1)\n\n        # Modify the related object (the article)\n        featured.article.headline = \"New headline\"\n        featured.article.save()\n\n        # Test refresh_from_db with select_related.\n        featured.refresh_from_db(select_related=('article',))\n\n        # Verify the updated headline.\n        self.assertEqual(featured.article.headline, \"New headline\")\n        self.assertEqual(featured.article.selfref_set.all().count(), 1)\n", "    def test_refresh_many_to_one_field(self):\n        article = Article.objects.create(headline='Parrot programs in Python', pub_date=datetime(2005, 7, 28))\n        s = SelfRef.objects.create(article=article)\n        article.headline = 'Parrot programs in Python 2.0'\n        article.save()\n        s.refresh_from_db()\n        self.assertEqual(s.article.headline, 'Parrot programs in Python 2.0')\n", "    def test_refresh_with_transaction(self):\n        # Test refresh_from_db() within a transaction.\n        with transaction.atomic():\n            a = Article.objects.create(pub_date=datetime.now())\n            a.save()\n            with self.assertNumQueries(0):\n                a.refresh_from_db()\n            Article.objects.filter(pk=a.pk).update(headline='new headline')\n            with self.assertNumQueries(1):\n                a.refresh_from_db()\n                self.assertEqual(a.headline, 'new headline')\n"], "sample_236": ["    def test_fast_delete_related_queryset(self):\n        a = A.objects.create(b=B.objects.create())\n        a_qs = A.objects.filter(pk=a.pk)\n        with self.assertNumQueries(2):\n            a_qs.delete()\n        self.assertFalse(A.objects.filter(pk=a.pk).exists())\n        self.assertFalse(B.objects.filter(pk=a.b_id).exists())\n\n", "    def test_fast_delete_signals_without_update(self):\n        \"\"\"\n        #25665 - Fast-delete should not trigger update signals if\n        `no_update_can_self_select` is disabled.\n        \"\"\"\n        calls = []\n\n            calls.append(kwargs)\n\n        models.signals.post_delete.connect(\n            signal_receiver, sender=Avatar, weak=False\n        )\n        a = Avatar.objects.create()\n        a.delete()\n        self.assertEqual(len(calls), 0)\n        models.signals.post_delete.disconnect(signal_receiver, sender=Avatar)\n\n\n\n", "    def test_fast_delete_with_complex_relations(self):\n        r = R.objects.create()\n        g = GenericParent.objects.create()\n        s = SpecificChild.objects.create(generic_parent=g)\n        r.specific_child = s\n        r.save()\n        # 1 delete for the R instance, 1 for the SpecificChild.\n        self.assertNumQueries(2, r.delete)\n        self.assertFalse(R.objects.exists())\n        self.assertFalse(SpecificChild.objects.exists())\n        self.assertFalse(GenericParent.objects.exists())", "    def test_fast_delete_with_select_related(self):\n        parent = R.objects.create()\n        child = RChild.objects.create(r_ptr=parent)\n        with self.assertNumQueries(2):\n            child.delete(select_related='r_ptr')\n        self.assertFalse(RChild.objects.filter(id=child.id).exists())\n        self.assertTrue(R.objects.filter(id=parent.id).exists())\n", "    def test_fast_delete_with_related_objects(self):\n        a = Avatar.objects.create(desc='a')\n        u = User.objects.create(avatar=a)\n        s = S.objects.create(r=R.objects.create())\n        t = T.objects.create(s=s)\n        \n        # 1 query to delete the user, which will cascade delete the avatar\n        # 1 query to delete the related 's' and 't' instances.\n        self.assertNumQueries(3, u.delete)\n\n        self.assertFalse(User.objects.exists())\n        self.assertFalse(Avatar.objects.exists())\n        self.assertFalse(S.objects.exists())\n        self.assertFalse(T.objects.exists())\n"], "sample_1197": ["def test_issue_24466():\n    from sympy.physics.units import ampere, second, tesla, velocity\n\n    A = Quantity(\"A\")\n    S = Quantity(\"S\")\n    T = Quantity(\"T\")\n    V = Quantity(\"V\")\n\n    SI.set_quantity_dimension(A, ampere)\n    SI.set_quantity_dimension(S, second)\n    SI.set_quantity_dimension(T, tesla)\n    SI.set_quantity_dimension(V, velocity)\n\n    A.set_global_relative_scale_factor(1, ampere)\n    S.set_global_relative_scale_factor(1, second)\n    T.set_global_relative_scale_factor(1, tesla)\n    V.set_global_relative_scale_factor(1, meter/second)\n\n    expr = A * T * S\n    assert SI._collect_factor_and_dimension(expr)[1] == ampere*tesla*second\n\n    expr =  V * T\n    assert SI._collect_factor_and_dimension(expr)[1] == (meter/second)*tesla\n\n    expr =  A / T\n    assert SI._collect_factor_and_dimension(expr)[1] == ampere/tesla\n", "def test_issue_26447():\n    from sympy.physics.units import Quantity, dimension, kg, meter, second\n    q = Quantity(\"q\")\n    SI.set_quantity_dimension(q, dimension(kg*meter*second**(-1)))\n    assert q.set_global_relative_scale_factor(2*meter**-1*second**-1, meter/second)\n    assert q.scale_factor == 2*meter**-1*second**-1\n    assert convert_to(q, meter/second) == 2\n\n\n\n", "def test_quantity_comparison():\n    from sympy.physics.units import meter, kilogram, second\n\n    m1 = Quantity(\"m1\", meter)\n    m2 = Quantity(\"m2\", meter)\n    m1.set_global_relative_scale_factor(1, meter)\n    m2.set_global_relative_scale_factor(2, meter)\n\n    assert m1 == m1\n    assert m1 != m2\n    assert m1 < m2\n    assert m1 <= m2\n    assert m2 > m1\n    assert m2 >= m1\n\n    k1 = Quantity(\"k1\", kilogram)\n    k2 = Quantity(\"k2\", kilogram)\n    k1.set_global_relative_scale_factor(1, kilogram)\n    k2.set_global_relative_scale_factor(2, kilogram)\n\n    assert k1 == k1\n    assert k1 != k2\n    assert k1 < k2\n    assert k1 <= k2\n    assert k2 > k1\n    assert k2 >= k1\n\n    s1 = Quantity(\"s1\", second)\n    s2 = Quantity(\"s2\", second)\n    s1.set_global_relative_scale_factor(1, second)\n    s2.set_global_relative_scale_factor(2, second)\n\n    assert s1 == s1\n    assert s1 != s2\n    assert s1 < s2\n    assert s1 <= s2\n    assert s2 > s1\n    assert s2 >= s1\n\n\n\n", "    def test_issue_25117():\n        from sympy.physics.units import Quantity, meter, second, kilogram, joule\n        q1 = Quantity(\"q1\")\n        q2 = Quantity(\"q2\")\n\n        SI.set_quantity_dimension(q1, meter/second)\n        SI.set_quantity_dimension(q2, joule)\n\n        q1.set_global_relative_scale_factor(10,meter / second)\n        q2.set_global_relative_scale_factor(1, joule)\n        assert q1 / q2 == 10*meter / (second*joule)\n        assert (q1 / q2).convert_to(meter**2 / (second**2 * joule)) == 10*meter**2 / (second**2 * joule)\n", "def test_issue_24722():\n    from sympy.physics.units import ampere, coulomb, magnetic_flux\n\n    a = Quantity('a')\n    SI.set_quantity_dimension(a, ampere)\n    a.set_global_relative_scale_factor(1, ampere)\n\n    phi = Quantity('phi')\n    SI.set_quantity_dimension(phi, magnetic_flux)\n    phi.set_global_relative_scale_factor(1, weber)\n\n    expr = a * phi\n    assert SI._collect_factor_and_dimension(expr) == (ampere*weber, Dimension(1))\n\n\n"], "sample_554": ["def test_antialiased_backends():\n    # Test that antialiasing works consistently across backends\n\n    fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(8, 6))\n    for i, backend in enumerate(['Agg', 'pdf', 'svg', 'png']):\n        mpl.rcParams['backend'] = backend\n        axes[i // 2, i % 2].text(0.5, 0.5, \"Test\", antialiased=True,\n                                  fontsize=20)\n        axes[i // 2, i % 2].set_axis_off()\n\n    fig.tight_layout()\n", "def test_text_antialiased_mixed(fig_test, fig_ref):\n    # Test mixed configuration: global off, individual on\n    mpl.rcParams['text.antialiased'] = False\n    fig_test.text(0.5, 0.5, '6 inches x 2 inches', antialiased=True)\n\n\n", "def test_text_path_transform():\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, \"test\", transform=ax.transData)\n    fig.canvas.draw()\n    assert text._transform == ax.transData.inverted()\n", "def test_text_with_unicode():\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, \"Hello, \u4e16\u754c!\", fontname=\"Arial\", fontsize=16)\n    fig.canvas.draw()\n    # Check that the text is rendered correctly\n    assert text.get_text() == \"Hello, \u4e16\u754c!\"\n\n\n", "def test_text_stretch():\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, \"foo\", stretch=\"ultracondensed\",\n                   ha=\"center\", va=\"center\")\n    fig.canvas.draw()\n    bbox = text.get_window_extent()\n    assert bbox.width < 0.1\n\n"], "sample_690": ["def test_importorskip_error(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n            pass\n\n        pytest.importorskip(\"doesnotexist\")\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*could not import 'doesnotexist': No module named '.*'\",\n            \"*test_this.py*ERROR*\",\n        ]\n    )\n\n\n\n", "def test_importorskip_installed_module(pytester: Pytester) -> None:\n    import requests\n    pytest.importorskip('requests')\n\n\n", "compilation error", "compilation error", "def test_importorskip_without_error(pytester: Pytester) -> None:\n    pytest.importorskip(\"requests\")\n"], "sample_1179": ["def test_issue_25574():\n    from sympy.physics.quantum.quantum import QTensor, QApplyOp\n    a, b = symbols('a b')\n    q = QTensor('Q', 2)\n    op = QApplyOp(a*q, 1)\n    assert str(op) == f'a*Q[1]'\n", "def test_issue_19436():\n    from sympy.modules.tensor import tensor\n    a, b, c = symbols('a b c', real=True)\n    tensor_expr = tensor([a, b], [c])\n    assert str(tensor_expr) == 'tensor([a, b], [c])' \n", "def test_issue_23513():\n    from sympy.functions.special.gamma_functions import gamma\n    assert str(gamma(x + 1)) == 'gamma(x + 1)'\n", "def test_issue_23834():\n    from sympy.geometry import Point, Line\n    p = Point(1, 2)\n    l = Line(p, 2*p)\n    assert str(l.direction_vector) == 'Point(1, 2)'\n\n", "def test_issue_19948():\n    assert str(Sum(x**2, (x, -oo, oo), method='numerical')) == \"Sum(x**2, (x, -oo, oo), method='numerical')\"\n"], "sample_856": ["compilation error", "def test_leave_one_out_with_empty_groups():\n    cv = LeaveOneOut()\n    X, y = [[1], [2], [3]], [0, 1, 2]\n    with pytest.raises(ValueError, match='groups cannot be empty'):\n        next(cv.split(X, y, groups=[]))\n\n\n", "def test_repeated_stratified_kfold_with_groups():\n    rng = np.random.RandomState(42)\n    X = np.array([\n        [1, 2],\n        [3, 4],\n        [5, 6],\n        [7, 8],\n        [9, 10],\n        [11, 12],\n        [13, 14],\n        [15, 16],\n        [17, 18],\n        [19, 20]\n    ])\n    y = np.array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1])\n    groups = rng.randint(0, 5, size=X.shape[0])\n\n    rskf = RepeatedStratifiedKFold(n_splits=3, n_repeats=2, random_state=42)\n    for train_index, test_index in rskf.split(X, y, groups):\n        assert len(np.unique(groups[train_index])) >= 2\n        assert len(np.unique(groups[test_index])) >= 2\n\n        # Check that groups are not split across folds\n        for i in range(len(train_index)):\n            for j in range(i + 1, len(train_index)):\n                assert groups[train_index[i]] == groups[train_index[j]]\n        for i in range(len(test_index)):\n            for j in range(i + 1, len(test_index)):\n                assert groups[test_index[i]] == groups[test_index[j]]\n\n\n", "def test_time_series_cv_with_shuffle():\n    X = np.random.rand(10, 1)\n    tscv = TimeSeriesSplit(n_splits=3, shuffle=True)\n    splits = list(tscv.split(X))\n    for train, test in splits:\n        assert (all(train < test) or all(test < train))\n        for i in train:\n            assert i < len(X)\n        for i in test:\n            assert i < len(X)\n        num_samples = len(X)\n        assert len(train) + len(test) == num_samples    \n", "def test_time_series_cv_with_groups():\n\n    tscv = TimeSeriesSplit(n_splits=5)\n\n    X = np.zeros((100, 1))\n    groups = np.random.randint(0, 5, 100)\n\n    # Test that groups are preserved across splits\n    splits = tscv.split(X, groups=groups)\n    for train_index, test_index in splits:\n        assert len(set(groups[train_index])) == len(set(groups[test_index]))\n\n"], "sample_362": ["    def test_suggest_name_with_custom_operation(self):\n        class CustomOperation(migrations.MigrationOperation):\n                super().__init__()\n                self.output = name\n                return 'CustomOperation', (), {}, {'output': self.output}\n\n        class Migration(migrations.Migration):\n            operations = [\n                CustomOperation('my_custom_operation')\n            ]\n\n        migration = Migration('some_migration', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'my_custom_operation')\n\n\n\n\n", "    def test_single_field_operation(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.AddField('Person', 'birthdate', models.DateField(null=True)),\n            ]\n\n        migration = Migration('some_migration', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'add_birthdate')\n\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.RemoveField('Person', 'birthdate'),\n            ]\n\n        migration = Migration('some_migration', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'delete_birthdate')\n\n\n", "    def test_add_field_with_no_unique_constraint(self):\n        changes = self.get_changes(\n            [self.author_empty],\n            [self.author_with_biography],\n        )\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"AddField\"])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name='biography')\n        self.assertOperationAttributes(changes, 'testapp', 0, 0)\n", "    def test_operation_with_no_suggested_name_and_no_initial(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.RunSQL('SELECT 1 FROM person;'),\n            ]\n\n        migration = Migration('0001_initial', 'test_app')\n        suggest_name = migration.suggest_name()\n        self.assertIs(suggest_name.startswith('auto_'), True)\n", "    def test_operation_order_in_suggestion(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.RemoveField('test_app.Person', 'email'),\n                migrations.AddField('test_app.Person', 'phone', models.CharField(max_length=10)),\n            ]\n\n        migration = Migration('0001_initial', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'remove_email_add_phone')\n\n"], "sample_198": ["    def test_expression_wrapper_output_field(self):\n        expr = ExpressionWrapper(F('name') + Value(' '), output_field=CharField())\n        self.assertEqual(expr.output_field, CharField())\n        self.assertEqual(expr.expression, F('name') + Value(' '))\n", "    def test_output_field_is_preserved_in_clone(self):\n        expr = ExpressionWrapper(F('name'), output_field=CharField())\n        clone = expr.clone()\n        self.assertTrue(hasattr(clone, 'output_field'))\n        self.assertEqual(clone.output_field, expr.output_field)\n", "    def test_output_field_with_aggregation(self):\n        expr = ExpressionWrapper(\n            Func('max', function='MAX', input_params=[F('value')]),\n            output_field=IntegerField(),\n        )\n        group_by_cols = expr.get_group_by_cols(alias=None)\n        self.assertEqual(group_by_cols, [])\n", "    def test_complex_group_by(self):\n        expr1 = ExpressionWrapper(Lower(Value('f')), output_field=IntegerField())\n        expr2 = ExpressionWrapper(Length(Value('g')), output_field=IntegerField())\n        group_by_cols = expr1.get_group_by_cols(alias=None)\n        self.assertEqual(group_by_cols, [expr1.expression])\n        group_by_cols = [expr1.expression, expr2.expression]\n        self.assertEqual(group_by_cols, [expr1.expression, expr2.expression])\n\n\n", "    def test_output_field_in_clone(self):\n        expr = ExpressionWrapper(Value('foo'), output_field=CharField())\n        cloned_expr = expr.clone()\n        self.assertEqual(cloned_expr.output_field, CharField())\n        self.assertNotSame(cloned_expr.output_field, expr.output_field)\n"], "sample_130": ["    def test_nested_lookups(self):\n        query = Query(Item)\n        where = query.build_where(\n            Q(creator__name__icontains='Foo') & Q(modified__gt=F('created'))\n        )\n        nested_lookup = where.children[0].children[0]\n        self.assertIsInstance(nested_lookup, Exact)\n        self.assertIsInstance(nested_lookup.lhs.lhs, SimpleCol)\n        self.assertEqual(nested_lookup.lhs.lhs.target, Author._meta.get_field('name'))\n\n", "    def test_nested_foreign_key_join(self):\n        query = Query(Ranking)\n        # Simulate a complex join scenario with a nested foreign key relationship\n        where = query.build_where(Q(author__author_of__book__title='The Lord of the Rings'))\n        # Assert the query structure\n        nested_lookup = where.children[0].children[0].children[0]\n        self.assertIsInstance(nested_lookup, Exact)\n        self.assertEqual(nested_lookup.rhs, 'The Lord of the Rings')\n        self.assertEqual(nested_lookup.lhs.target, Book._meta.get_field('title'))\n        self.assertEqual(nested_lookup.lhs.lhs.target, Author._meta.get_field('book'))\n        self.assertEqual(nested_lookup.lhs.lhs.lhs.target, Ranking._meta.get_field('author'))\n\n", "    def test_related_to_in(self):\n        query = Query(Item)\n        where = query.build_where(Q(creator__name__in=['Alice', 'Bob']))\n        lookup = where.children[0]\n        self.assertCountEqual(lookup.rhs, ['Alice', 'Bob'])\n        self.assertIsInstance(lookup.lhs, SimpleCol)\n        self.assertEqual(lookup.lhs.target, Author._meta.get_field('name'))\n        self.assertEqual(lookup.connected_field.attname, 'creator')\n        self.assertEqual(lookup.lhs.target.related_model, Author)\n\n\n\n", "    def test_complex_lookup(self):\n        query = Query(Author)\n        where = query.build_where(Q(\n            name__icontains='foo'\n        ) & Q(num__in=[1, 2, 3]) & Q(\n            num__gt=2\n        ) | Q(name__exact=None))\n\n        self.assertEqual(where.connector, AND)\n        self.assertEqual(len(where.children), 2)\n\n        first_conjunction = where.children[0]\n        self.assertEqual(first_conjunction.connector, AND)\n        self.assertEqual(len(first_conjunction.children), 3)\n\n        self.assertIsInstance(first_conjunction.children[0], Exact)\n        self.assertEqual(first_conjunction.children[0].lhs.target, Author._meta.get_field('name'))\n        self.assertEqual(first_conjunction.children[0].rhs, 'foo')\n\n        self.assertIsInstance(first_conjunction.children[1], In)\n        self.assertEqual(first_conjunction.children[1].lhs.target, Author._meta.get_field('num'))\n        self.assertEqual(first_conjunction.children[1].rhs, [1, 2, 3])\n\n        self.assertIsInstance(first_conjunction.children[2], GreaterThan)\n        self.assertEqual(first_conjunction.children[2].lhs.target, Author._meta.get_field('num'))\n        self.assertEqual(first_conjunction.children[2].rhs, 2)\n\n        second_conjunction = where.children[1]\n        self.assertIsInstance(second_conjunction, IsNull)\n        self.assertEqual(second_conjunction.lhs.target, Author._meta.get_field('name'))\n", "    def test_multiple_related_lookups(self):\n        query = Query(Author)\n        where = query.build_where(\n            Q(books__title__icontains='django') | Q(articles__author__name__iexact='jane doe')\n        )\n\n        # Check the structure of the query\n        self.assertEqual(where.connector, OR)\n\n        # Check the first lookup\n        first_lookup = where.children[0]\n        self.assertIsInstance(first_lookup, Q)\n        self.assertEqual(first_lookup.children[0].func, 'icontains')\n        self.assertEqual(first_lookup.children[0].lhs.target, Author._meta.get_field('books'))\n        self.assertEqual(first_lookup.children[0].rhs, 'django')\n        \n        # Check the second lookup\n        second_lookup = where.children[1]\n        self.assertIsInstance(second_lookup, Q)\n        self.assertEqual(second_lookup.children[0].func, 'iexact')\n        self.assertEqual(second_lookup.children[0].lhs.target, Author._meta.get_field('articles'))\n        self.assertEqual(second_lookup.children[0].rhs, 'jane doe')\n\n\n\n\n"], "sample_242": ["    def test_year_lookup_bounds(self):\n        look_up = YearExact(lhs=Value(datetime(2010, 1, 1, 0, 0, 0), output_field=DateTimeField()), rhs=Value(2010))\n        connection = mock.MagicMock()\n        connection.features.has_native_uuid_field = False\n        look_up.connection = connection\n        start, finish = look_up.year_lookup_bounds(connection, 2010)\n        self.assertEqual(start, datetime(2010, 1, 1, 0, 0, 0))\n        self.assertEqual(finish, datetime(2010, 12, 31, 23, 59, 59)) \n", "    def test_year_lookup_bounds(self):\n        connection = mock.MagicMock()\n        look_up = YearExact(\n            lhs=Value(datetime(2010, 1, 1, 0, 0, 0), output_field=DateTimeField()),\n            rhs=Value(2010),\n        )\n        connection.ops.year_lookup_bounds_for_datetime_field.return_value = (\n            Value(datetime(2010, 1, 1, 0, 0, 0)), Value(datetime(2010, 12, 31, 23, 59, 59))\n        )\n        self.assertEqual(look_up.year_lookup_bounds(connection, 2010), (\n            datetime(2010, 1, 1, 0, 0, 0), datetime(2010, 12, 31, 23, 59, 59)\n        ))\n", "    def test_year_lookup_bounds(self):\n        year_lookup = YearExact(\n            lhs=Value(datetime(2023, 10, 26, 0, 0, 0), output_field=DateTimeField()),\n            rhs=Value(2023)\n        )\n        connection = mock.Mock()\n        connection.ops.year_lookup_bounds_for_datetime_field = mock.Mock(\n            return_value=(datetime(2023, 1, 1, 0, 0, 0), datetime(2023, 12, 31, 23, 59, 59))\n        )\n        bounds = year_lookup.year_lookup_bounds(connection, 2023)\n        self.assertEqual(bounds, (datetime(2023, 1, 1, 0, 0, 0), datetime(2023, 12, 31, 23, 59, 59)))\n", "    def test_year_lookup_bounds(self):\n        look_up = YearExact(\n            lhs=Value(datetime(2010, 1, 1, 0, 0, 0), output_field=DateTimeField()),\n            rhs=Value(2010, output_field=IntegerField())\n        )\n        self.assertEqual(look_up.year_lookup_bounds(None, 2010), (20100101, 20101231))\n", "    def test_year_exact(self):\n        lookup = YearExact(\n            lhs=Value(datetime(2023, 2, 1, 0, 0, 0), output_field=DateTimeField()),\n            rhs=Value(2023),\n        )\n        with mock.patch('django.db.models.sql.compiler.Compiler.compile') as mock_compile:\n            mock_compile.return_value = ('%s', [])\n            sql, params = lookup.as_sql(mock.Mock(), mock.Mock())\n        self.assertEqual(sql, 'year(%s) BETWEEN year(2023) AND year(2023)')\n        self.assertEqual(params, [])\n\n"], "sample_57": ["    def test_bug_19349_render_with_changed_value(self):\n        widget = ReadOnlyPasswordHashWidget()\n        value = 'pbkdf2_sha256$100000$a6Pucb1qSFcD$WmCkn9Hqidj48NVe5x0FEM6A9YiOqQcl/83m2Z5udm0='\n        html = widget.render('name', value, {'id': 'id_password'})\n        self.assertIn(_(\"No password set.\"), html)\n        new_value = 'pbkdf2_sha256$100000$a6Pucb1qSFcD$WmCkn9Hqidj48NVe5x0FEM6A9YiOqQcl/83m2Z5udm0='\n        html = widget.render('name', new_value, {'id': 'id_password'})\n        self.assertIn(_(\"No password set.\"), html)  \n", "    def test_password_validation_errors(self):\n        user = User.objects.get(username='testclient')\n        data = {\n            'password1': 'abc',\n            'password2': 'abc',\n        }\n        with self.settings(PASSWORD_MIN_LENGTH=8):\n            form = AdminPasswordChangeForm(user, data)\n            self.assertFalse(form.is_valid())\n            self.assertEqual(form.errors['password1'], [\n                \"Password must be at least 8 characters long.\",\n            ])\n            self.assertEqual(form.errors['password2'], [\n                \"Password must be at least 8 characters long.\",\n            ])\n\n\n\n", "    def test_password_reset_tokens_are_stored_encrypted(self):\n        user = User.objects.create_user('testclient', 'test@example.com', 'test123')\n\n        # Simulate a password reset\n        form = PasswordResetForm({'email': 'test@example.com'})\n        form.save()\n\n        # Check that the token is stored encrypted\n        token = user.password_reset_token\n        self.assertNotEqual(token, user.password_reset_token_cache)  \n\n\n", "    def test_password_too_short(self):\n        user = User.objects.get(username='testclient')\n        data = {'password1': '123', 'password2': '123'}\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['password1'], [\n            \"This password is too short. It must contain at least 8 characters.\"\n        ])\n\n", "    def test_password_validation(self):\n        user = User.objects.get(username='testclient')\n        data = {\n            'password1': 'test',\n            'password2': 'test',\n        }\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        for message in form.errors['password1']:\n            if message in form.error_messages['password_too_short']:\n                return\n        self.fail('Password should be too short')\n"], "sample_847": ["compilation error", "    def test_enet_positive_constraint_multioutput():\n        X, y, _, _ = build_dataset(n_samples=10, n_features=5, n_targets=2)\n        y1 = y[:, 0]\n        y2 = y[:, 1]\n        y = np.stack((y1, y2), axis=1)\n\n        clf = MultiTaskElasticNet(positive=True)\n        clf.fit(X, y)\n\n        assert (clf.coef_[:, :, 0] >= 0).all()\n        assert (clf.coef_[:, :, 1] >= 0).all()\n\n\n\n", "def test_enet_multitask_l1_ratio_with_validation():\n    X, y, X_test, y_test = build_dataset(n_samples=100, n_features=50,\n                                          n_targets=3)\n    \n    # Test multitask ElasticNetCV with validation data\n    # and changing l1_ratio across folds\n    enet_cv = MultiTaskElasticNetCV(n_alphas=20, l1_ratio=[0.1, 0.5, 0.9],\n                                   fit_intercept=False,\n                                   random_state=42, cv=5)\n    enet_cv.fit(X, y)\n    # assert that coefficients have been fitted for each target\n    assert enet_cv.coef_.shape == (3, 50)\n", "    def test_multitask_lasso_and_enet_with_positive_feature_scaling():\n        X, y, _, _ = build_dataset(n_samples=50, n_features=10)\n        Y = np.c_[y, y]  # dummy multi-output target\n        X_pos = X.copy()\n        X_pos[:, 0] *= -1\n        \n        # Test with MultiTaskLasso\n        clf_multitask_lasso_neg = MultiTaskLasso().fit(X_pos, Y)\n        clf_multitask_lasso_pos = MultiTaskLasso().fit(X, Y)\n\n        assert np.any(clf_multitask_lasso_neg.coef_ != clf_multitask_lasso_pos.coef_)\n\n        # Test with MultiTaskElasticNet\n        clf_multitask_enet_neg = MultiTaskElasticNet().fit(X_pos, Y)\n        clf_multitask_enet_pos = MultiTaskElasticNet().fit(X, Y)\n\n        assert np.any(clf_multitask_enet_neg.coef_ != clf_multitask_enet_pos.coef_)\n", "def test_enet_cv_positive_constraint_with_precompute():\n    X, y, X_test, y_test = build_dataset()\n    max_iter = 500\n\n    # Ensure the unconstrained fit has a negative coefficient\n    enetcv_unconstrained = ElasticNetCV(n_alphas=3, eps=1e-1,\n                                        max_iter=max_iter,\n                                        cv=2, n_jobs=1, precompute=True)\n    enetcv_unconstrained.fit(X, y)\n    assert min(enetcv_unconstrained.coef_) < 0\n\n    # On same data, constrained fit has non-negative coefficients\n    enetcv_constrained = ElasticNetCV(n_alphas=3, eps=1e-1, max_iter=max_iter,\n                                      cv=2, positive=True, n_jobs=1, precompute=True)\n    enetcv_constrained.fit(X, y)\n    assert min(enetcv_constrained.coef_) >= 0\n\n\n\n"], "sample_811": ["def test_euclidean_distances_with_missing_values():\n    # Test euclidean distances with missing values in input data\n    X = np.array([[1, 2, np.nan], [4, 5, 6], [7, 8, 9]])\n    Y = np.array([[10, 11, 12], [13, 14, np.nan], [15, 16, 17]])\n\n    # Test that nan propagate correctly across the distance matrix\n    distances = euclidean_distances(X, Y)\n    assert np.isnan(distances[0, 2])\n    assert np.isnan(distances[1, 1])\n\n\n\n", "    def test_check_array_shape_mismatch():\n        # Ensure an error is raised if the shape of X and Y is not compatible.\n        XA = np.array([[1, 2], [3, 4]])\n        XB = np.array([1, 2, 3, 4])\n        assert_raises(ValueError, check_pairwise_arrays, XA, XB)\n", "def test_cosine_similarity_sparse_output():\n    # Test that cosine similarity outputs sparse matrices for sparse inputs.\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((5, 4))\n    X_sparse = csr_matrix(X)\n    Y = rng.random_sample((3, 4))\n    Y_sparse = csr_matrix(Y)\n\n    K = cosine_similarity(X_sparse, Y_sparse, dense_output=False)\n    assert issparse(K)\n\n", "    def test_pairwise_distances_handling_integer_input():\n        # Test handling of integer input\n        X = np.arange(10).astype(np.int32)\n        Y = np.arange(10).astype(np.int32)\n        dist = pairwise_distances(X, Y)\n        assert dist.dtype == np.float64\n        assert np.all(dist >= 0)\n\n\n\n", "def test_pairwise_distances_chunked_bad_reduce():\n    X = np.arange(10).reshape(-1, 1)\n    S = np.array([[1, 2], [3, 4]])\n\n        return a + b * 2\n\n    with pytest.raises_any(ValueError, TypeError):\n        pairwise_distances_chunked(X, None, reduce_func=bad_reduce,\n                                          working_memory=64)\n"], "sample_396": ["    def test_ticket_24605_with_complex_related_fields(self):\n        # Test similar to ticket_24605 but with nested related fields.\n        School.objects.create()\n        class_ = Class.objects.create(school=School.objects.first())\n        student1 = Student.objects.create(class_id=class_.pk)\n        related_student = RelatedStudent.objects.create(related_student=student1)\n        student2 = Student.objects.create(class_id=class_.pk)\n        RelatedStudent.objects.create(related_student=student2)\n        related_student2 = RelatedStudent.objects.create(related_student=student1)\n        qs = RelatedStudent.objects.filter(related_student__in=Student.objects.filter(pk=student1.pk,).values_list('pk', flat=True))\n        self.assertQuerysetEqual(qs, [related_student, related_student2])\n\n\n", "    def test_ticket_24605_two_levels(self):\n        \"\"\"\n        Subquery table names should be quoted properly when dealing with\n        multiple nesting levels.\n        \"\"\"\n        i1 = Individual.objects.create(alive=True)\n        RelatedIndividual.objects.create(related=i1)\n        i2 = Individual.objects.create(alive=False)\n        RelatedIndividual.objects.create(related=i2)\n        related2 = RelatedIndividual.objects.create(related=i1)\n        related3 = RelatedIndividual.objects.create(related=i2)\n\n        self.assertSequenceEqual(\n            Individual.objects.filter(\n                Q(\n                    related_individual__isnull=False,\n                )\n                & Q(\n                    related_individual__related__pk__in=[i1.pk, i2.pk]\n                )\n            ),\n            [i1, i2],\n        )\n", "    def test_ticket_24607(self):\n        a1 = Ticket24605A.objects.create()\n        b1 = Ticket24605B.objects.create(modela_fk=a1)\n        b2 = Ticket24605B.objects.create(modela_fk=a1)\n        Ticket24605C.objects.create(field_c0=10000, field_c1=100)\n        self.assertSequenceEqual(\n            Ticket24605A.objects.filter(\n                ticket24605b__modelc__field_c1__lt=100\n            ),\n            [a1],\n        )\n\n\n", "    def test_ticket_24605_distinct_on_field(self):\n        \"\"\"\n        Make sure distinct on works correctly when filtering on a field that is\n        aliased in a subquery.\n        \"\"\"\n        i1 = Individual.objects.create(alive=True)\n        RelatedIndividual.objects.create(related=i1)\n        i2 = Individual.objects.create(alive=False)\n        RelatedIndividual.objects.create(related=i2)\n        i3 = Individual.objects.create(alive=True)\n        i4 = Individual.objects.create(alive=False)\n        q = Q(related_individual__isnull=True)\n        qs = Individual.objects.filter(q).distinct('pk')\n        self.assertSequenceEqual(list(qs), [i4]) \n", "    def test_ticket_24605_related_fields(self):\n        # Test with related fields and make sure table names are quoted.\n        School.objects.create(name=\"school1\")\n        School.objects.create(name=\"school2\")\n        Student.objects.create(name=\"student1\", school=School.objects.get(name=\"school1\"))\n        Student.objects.create(name=\"student2\", school=School.objects.get(name=\"school1\"))\n        Student.objects.create(name=\"student3\", school=School.objects.get(name=\"school2\"))\n\n        qs = Student.objects.filter(\n            Q(school__name=\"school1\"),\n            Q(pk__in=Student.objects.filter(school__name=\"school1\").order_by(\"pk\").values_list(\"pk\", flat=True)),\n        )\n        self.assertSequenceEqual(\n            qs, [Student.objects.get(name=\"student1\"), Student.objects.get(name=\"student2\")]\n        )\n"], "sample_675": ["def test_logging_emit_error_with_message(testdir: Testdir) -> None:\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n            logging.error('oops', exc_info=True, extra={'something': 'else'})\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.assert_outcomes(failed=1)\n\n", "def test_log_cli_level_defaults(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n            plugin = request.config.pluginmanager.getplugin('logging-plugin')\n            assert plugin.log_cli_handler.level == logging.WARNING\n            logging.getLogger('catchlog').debug(\"This log message won't be shown\")\n            logging.getLogger('catchlog').warning(\"This log message will be shown\")\n            print('PASSED')\n    \"\"\"\n    )\n\n    result = testdir.runpytest()\n\n    result.stdout.fnmatch_lines([\"test_log_cli_level_defaults.py PASSED\"]) \n\n", "        def test_log_in_fixture(request):\n            plugin = request.config.pluginmanager.getplugin('logging-plugin')\n            assert plugin.log_cli_handler.level == logging.INFO\n            logging.getLogger('catchlog').debug(\"This log message will not be shown\")\n            logging.getLogger('catchlog').warning(\"This log message will be shown\")\n            print('PASSED')", "def test_logging_emit_error_custom_handler(testdir: Testdir) -> None:\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n        class MyHandler(logging.Handler):\n                if record.levelno == logging.WARNING:\n                    raise ValueError(\"This is a warning but we make it an error\")\n                super().emit(record)\n\n            logger = logging.getLogger(__name__)\n            logger.setLevel(logging.WARNING)\n            handler = MyHandler()\n            logger.addHandler(handler)\n            logger.warning('oops', 'first', 2)\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.assert_outcomes(failed=1)\n    result.stdout.fnmatch_lines([\"*====* FAILURES *===*\"]) \n", "        def test_log_in_teardown(monkeypatch):\n            captured_messages = []\n\n                    captured_messages.append((level, msg, args, kwargs))\n\n                return handler\n\n            monkeypatch.setattr(logging, 'basicConfig', lambda *args, **kwargs: None)\n            monkeypatch.setattr(logging.getLogger, '__init__', lambda *args, **kwargs: None)\n\n            # setup\n            logging.basicConfig(\n                level=logging.DEBUG,\n                handlers=[logging.StreamHandler()],\n                format='%(asctime)s - %(levelname)s - %(message)s',\n            )\n\n            logger = logging.getLogger(__name__)\n            logger.setLevel(logging.DEBUG)\n\n\n            class TestLog:\n                    logger.info('Setup method called')\n\n                    logger.info('Test method called')\n\n                    logger.warning('Tear down method called')\n\n            logger.debug('Debug message')\n\n\n            test_log = TestLog()\n            test_log.setup_method()\n            test_log.test_something()\n            test_log.teardown_method()\n\n            assert captured_messages == [\n                ('DEBUG', 'Debug message', (), {}),\n                ('INFO', 'Setup method called', (), {}),\n                ('INFO', 'Test method called', (), {}),\n                ('WARNING', 'Tear down method called', (), {}),\n            ]"], "sample_866": ["compilation error", "def test_affinity_propagation_preference_handling():\n    # Test affinity_propagation() with various preference values.\n    X = np.array([[1, 2], [1, 4], [1, 0], [4, 2], [4, 4], [4, 0]])\n\n    # Case: preference is negative, forcing single cluster\n    preference = -10\n    cluster_centers_indices, labels = affinity_propagation(\n        S, preference=preference\n    )\n    assert len(cluster_centers_indices) == 1\n    assert np.all(labels == 0)\n\n    # Case: preference is zero, handling with median\n    preference = 0\n    cluster_centers_indices, labels = affinity_propagation(\n        S, preference=preference\n    )\n\n    assert len(cluster_centers_indices) > 1\n    assert np.max(labels) > 0\n\n    # Case: preference is positive, leading to multiple clusters\n    preference = 10\n    cluster_centers_indices, labels = affinity_propagation(\n        S, preference=preference\n    )\n\n    assert len(cluster_centers_indices) > 1\n    assert np.max(labels) >= 2\n\n\n\n", "def test_affinity_propagation_preference_nan():\n    # Test cases with NaN values in the preference array\n\n    X = np.array([[1, 1], [-1, -1], [1, -1]])\n\n    # Test with all NaN preferences\n    with pytest.raises(ValueError):\n        affinity_propagation(np.eye(3), preference=np.array([np.nan, np.nan, np.nan]))\n\n    # Test with some NaN preferences\n    with pytest.raises(ValueError):\n        affinity_propagation(np.eye(3), preference=np.array([1, np.nan, 1]))\n\n\n", "def test_affinity_propagation_preference_limits():\n    # Test the behavior of  preference when it is set to very large or very small values\n\n    X = np.array([[1, 1], [1, 2], [2, 1], [2, 2]])\n    S = -euclidean_distances(X, squared=True)\n    \n    # Very large preference\n    preference = np.max(S) * 100\n    cluster_center_indices, labels = affinity_propagation(\n        S, preference=preference,\n    )\n    assert len(cluster_center_indices) == 1\n\n    # Very small preference\n    preference = np.min(S) * 0.01\n    cluster_center_indices, labels = affinity_propagation(\n        S, preference=preference,\n    )\n    assert len(cluster_center_indices) == X.shape[0]\n\n", "def test_affinity_propagation_preference_effect():\n    # Test the effect of preference on the number of clusters\n    # created.\n\n    X = np.array([[1, 2], [1, 4], [1, 0],\n                  [4, 2], [4, 4], [4, 0]])\n\n    # Test with preference lower than the minimum similarity\n    preference = -5\n    af = AffinityPropagation(preference=preference)\n    labels = af.fit_predict(X)\n    assert np.unique(labels).size == X.shape[0]\n    # All samples become their own cluster centers\n\n    # Test with preference higher than the median similarity\n    preference = np.median(euclidean_distances(X, squared=True)) * 2\n    af = AffinityPropagation(preference=preference)\n    labels = af.fit_predict(X)\n    assert np.unique(labels).size == 1\n\n\n\n\n"], "sample_233": ["    def test_token_with_different_user(self):\n        user1 = User.objects.create_user('user1', 'user1@example.com', 'testpw')\n        user2 = User.objects.create_user('user2', 'user2@example.com', 'testpw')\n        p0 = PasswordResetTokenGenerator()\n        tk1 = p0.make_token(user1)\n        self.assertIs(p0.check_token(user2, tk1), False)\n", "    def test_token_generation_with_custom_timestamp(self):\n        user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n        p0 = MockedPasswordResetTokenGenerator(datetime(2024, 1, 1))\n        timestamp = 1700000000\n        tk1 = p0.make_token(user, timestamp)\n        self.assertIs(p0.check_token(user, tk1), True)\n", "    def test_token_generation_with_custom_fields(self):\n        \"\"\"\n        Custom user fields should be included in the hash\n        value used to generate the token.\n        \"\"\"\n        # Create a custom user model with a custom field\n        class CustomUser(User):\n            custom_field = CustomEmailField()\n\n        # Create a custom user instance with a value for the custom field\n        user = CustomUser.objects.create_user(\n            'customuser', 'test5@example.com', 'testpw', custom_field='testval'\n        )\n\n        p0 = PasswordResetTokenGenerator()\n        tk1 = p0.make_token(user)\n        self.assertIs(p0.check_token(user, tk1), True)\n\n        # Update the custom field value and check if the token is invalidated\n        user.custom_field = 'newval'\n        user.save()\n        self.assertIs(p0.check_token(user, tk1), False)\n", "    def test_token_with_custom_email_field(self):\n        \"\"\"\n        Ensures that tokens generated for users with a custom email field\n        work correctly.\n        \"\"\"\n        from .models import CustomEmailField\n\n        user = CustomEmailField.objects.create_user(\n            'custom_email_user',\n            'test5@example.com',\n            password='testpw',\n        )\n        p0 = PasswordResetTokenGenerator()\n        tk1 = p0.make_token(user)\n        self.assertIs(p0.check_token(user, tk1), True)\n", "    def test_token_generation_with_large_timestamp(self):\n        # Ensure tokens can be generated with timestamps that are far in the future\n        user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n        now = datetime.now() + timedelta(days=3650)\n        p0 = MockedPasswordResetTokenGenerator(now)\n        tk1 = p0.make_token(user)\n        self.assertIs(p0.check_token(user, tk1), True)\n"], "sample_1159": ["def test_issue_18906():\n    x = Symbol('x', extended_real=True)\n    assert x.is_real is True\n    assert x.is_finite is None\n\n\n\n", "def test_issue_18538():\n    x = Symbol('x')\n    y = Symbol('y')\n    assert (x*y).is_real is None  \n    assert (x*y).is_commutative is True\n    assert (x*y).is_associative is True\n    assert (x*y).is_distributive is True \n    assert (x*y).is_identity is None\n    assert (x*y).is_inverse is None\n", "def test_issue_19265():\n    x = Symbol('x', nonnegative=True, real=True)\n    y = Symbol('y', nonnegative=True)\n    assert (x + y).is_nonnegative is True\n    assert (x - y).is_nonnegative is None\n    assert (x*y).is_nonnegative is True\n    assert (x/y).is_nonnegative is None\n    assert (y/x).is_nonnegative is None\n", "compilation error", "def test_issue_17698():\n    x = Symbol('x', real=True, positive=True)\n    y = Symbol('y')\n    assert failing_assumptions(x + y, positive=True) == \\\n    {'real': True, 'positive': False}\n"], "sample_53": ["    def test_render_options_with_no_results(self):\n        with self.settings(USE_I18N=True):\n            # Remove all bands.\n            Band.objects.all().delete()\n            form = AlbumForm()\n            output = form.as_table()\n            # Check that an empty option is present\n            self.assertIn(self.empty_option, output)\n            # Check that there are no other options.\n            self.assertNotIn('<option>', output)\n", "    def test_render_options_with_isnull_false(self):\n        beatles = Band.objects.create(name='The Beatles', style='rock')\n        with self.settings(\n            INSTALLED_APPS={'django.contrib.admin': False},\n        ):\n            from django.contrib.admin.widgets import AutocompleteSelect\n            form = AlbumForm(initial={'band': beatles.pk})\n            output = form.as_table()\n            selected_option = '<option value=\"%s\" selected>The Beatles</option>' % beatles.pk\n            self.assertIn(selected_option, output)\n", "    def test_render_with_empty_search_term(self):\n        rel = Album._meta.get_field('band').remote_field\n        w = AutocompleteSelect(rel, admin.site)\n        output = w.render('name', value=None, attrs={'data-search-term': ''})\n        self.assertIn('data-select2-id=\"name\"', output)\n        self.assertIn('data-search-term=\"\"', output)\n\n\n\n", "    def test_autocomplete_widget_with_initial(self):\n        band = Band.objects.create(name='The Beatles', style='rock')\n        form = AlbumForm(initial={'band': band.pk})\n        output = form.as_table()\n        self.assertIn('<input type=\"hidden\" name=\"band\" value=\"%s\">' % band.pk, output)\n", "    def test_render_options_with_invalid_pk(self):\n        form = AlbumForm(initial={'band': 'invalid_pk'})\n        output = form.as_table()\n        self.assertNotIn('<option value=\"invalid_pk\" selected>', output)\n        self.assertIn(self.empty_option, output)\n\n"], "sample_687": ["def test_log_format_string(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n            logging.basicConfig(format='%(log_prefix)s %(message)s', level=logging.INFO)\n            logging.info('this is a test')\n            assert 'this is a test' in caplog.text\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(['*this is a test*'])\n    assert result.ret == 0\n    ", "    def test_handler_levels(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import logging\n            import pytest\n\n                logger = logging.getLogger('my_logger')\n                logger.info('info level')\n                caplog.set_level(logging.WARNING, logger=logger.name)\n                logger.warning('warning level')\n                logger.info('info level')\n\n                logger = logging.getLogger('parent_logger')\n                parent_logger = logging.getLogger('parent_logger')\n                parent_logger.setLevel(logging.ERROR)\n                child_logger = logging.getLogger('child_logger')\n                child_logger.setLevel(logging.DEBUG)\n                child_logger.info('info level')\n                parent_logger.info('info level')\n                child_logger.warning('warning level')\n                parent_logger.warning('warning level')\n                child_logger.error('error level')\n                parent_logger.error('error level')\n            \"\"\"\n        )\n        result = testdir.runpytest()\n        assert result.ret == 0\n\n\n", "def test_log_format(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n\n            caplog.set_level(logging.INFO)\n            logging.info(\"testing log format\")\n\n            expected_format = \"foo bar %s\"\n            logging.basicConfig(format=expected_format, level=logging.INFO)\n            logging.info(\"baz\")\n\n            assert caplog.text == f\"testing log format\\n{expected_format} baz\\n\"\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    print(result.stdout)\n    assert result.ret == 0\n\n\n\n", "    def test_log_file_handler_handles_multiple_tests(testdir):\n        testdir.makepyfile(\n            \"\"\"\n        import logging\n\n            logger.debug(\"log from test ONE\")\n            logger.info(\"log from test TWO\")\n        \"\"\"\n        )\n        testdir.makeini(\n            \"\"\"\n            [pytest]\n            log_file = test_logs.txt\n            log_file_level = DEBUG\n            log_format = %(asctime)s - %(levelname)s - %(message)s\n\n            [logging]\n            level = DEBUG\n            \"\"\"\n        )\n        result = testdir.runpytest()\n        result.stdout.fnmatch_lines([\"*test_logs.txt created*\"])\n        result.stdout.fnmatch_lines([\"*log from test ONE*\"])\n        result.stdout.fnmatch_lines([\"*log from test TWO*\"])\n        assert result.ret == 0\n\n", "def test_log_file_path_overrides_ini(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n            assert caplog.handler.baseFilename == 'test_log_file_path.log'\n    \"\"\"\n    )\n\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_file = /dev/null\n    \"\"\"\n    )\n    testdir.tmpdir.ensure('test_log_file_path.log')\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*test_log_file_path.log*\"])\n\n    result = testdir.runpytest(\"--log-file=test_log_file_path.log --log-level=DEBUG\")\n    result.stdout.fnmatch_lines([\"*test_log_file_path.log*\"])\n"], "sample_883": ["def test_bayesian_ridge_ard_sparse_data(Estimator):\n    \"\"\"Test Bayesian Ridge and ARD on sparse data.\"\"\"\n    from scipy.sparse import rand\n\n    n_samples = 100\n    n_features = 5000\n    X = rand(n_samples, n_features, density=0.1)\n    y = np.random.rand(n_samples)\n    model = Estimator()\n    model.fit(X, y)\n\n", "def test_ard_sparse_input():\n    # Test ARDRegression with sparse input data\n\n    from scipy.sparse import random_spmatrix\n\n    n_samples, n_features = 100, 1000\n    X = random_spmatrix(n_samples, n_features, density=0.01)\n    y = np.random.randn(n_samples)\n\n    model = ARDRegression()\n    model.fit(X, y)\n\n    # Check that predictions are made correctly on sparse input\n    assert_array_almost_equal(model.predict(X), model.predict(X.toarray()))\n", "def test_bayesian_ridge_ard_verbose_fit():\n    # Test that verbose output is correctly handled during fit\n    X, y = diabetes.data, diabetes.target\n    \n    messages = ['Iteration 1', 'Iteration 2', 'Converged']\n    \n    class MockPrint:\n            self.calls = []\n        \n            self.calls.append(args[0])\n    \n    \n    print_mock = MockPrint()\n    \n    # Using BayesianRidge for this test\n    model = BayesianRidge(verbose=True, max_iter=3)\n    \n    with patch('print', new=print_mock):\n        model.fit(X, y)\n        \n    assert print_mock.calls == messages\n\n\n\n", "def test_bayesian_ridge_ard_fit_intercept(Estimator):\n    X = np.array([[1, 1], [3, 4], [5, 7], [4, 1], [2, 6], [3, 10], [3, 2]])\n    y = np.array([1, 2, 3, 2, 0, 4, 5]).T\n    # Test fit_intercept is correctly handled\n    model = Estimator(fit_intercept=True)\n    model.fit(X, y)\n    assert model.intercept_ is not None\n\n    model = Estimator(fit_intercept=False)\n    model.fit(X, y)\n    assert model.intercept_ is None\n\n", "def test_bayesian_ridge_ard_sparse_data(Estimator):\n    # Test Bayesian Ridge and ARD on sparse data\n    n_samples = 200\n    n_features = 1000\n    # Create a sparse matrix with some non-zero values\n    X = sp.csr_matrix(np.random.rand(n_samples, n_features) > 0.8)\n    y = np.random.randn(n_samples)\n\n    model = Estimator()\n\n    model.fit(X, y)\n\n    # Note: prediction doesn't need to be accurate on sparse data\n\n    # regression = LinearRegression()\n    # regression.fit(X, y)\n    # assert_array_almost_equal(model.predict(X), regression.predict(X))\n\n\n\n"], "sample_326": ["    def test_urlize_trim_url_limit(self):\n        tests = (\n            ('http://www.longlonglongurl.com/this-is-a-very-long-url-that-will-be-trimmed',\n             '<a href=\"http://www.longlonglongurl.com/this-is-a-very-long-url-that-will-be-trimmed\">http://www.longlonglongurl.com/this-is-a-very-long-url-that-will-be-trimmed</a>'),\n            ('http://www.longlonglongurl.com/this-is-a-very-long-url-that-will-be-trimmed',\n             '<a href=\"http://www.longlonglongurl.com/this-is-a-very-long-url-that-will-be-trimmed...\">http://www.longlonglongurl.com/this-is-a-very-long-url-that-will-be-trimmed...</a>',\n             trim_url_limit=15),\n            ('http://www.longlonglongurl.com/this-is-a-very-long-url-that-will-be-trimmed',\n             '<a href=\"http://www.longlonglongurl.com/this-is-a-very-long-url-that-will-be-trimmed...\">http://www.longlonglongurl.com/this-is-a-very-long-url-that-will-be-trimmed...</a>',\n             trim_url_limit=20),\n        )\n        for value, output, trim_url_limit in tests:\n            with self.subTest(value=value, output=output, trim_url_limit=trim_url_limit):\n                self.assertEqual(urlize(value, trim_url_limit=trim_url_limit), output)\n", "    def test_urlize_trim_url(self):\n        long_url = 'https://www.example.com/this-is-an-extremely-long-url-that-is-longer-than-the-trim_url_limit-character-limit-and-should-be-trimmed' * 5\n        short_url = 'https://www.example.com/short'\n\n        with self.subTest(value=long_url, trim_url_limit=30):\n            self.assertEqual(urlize(long_url, trim_url_limit=30),\n                             '<a href=\"%s\">https://www.example.com/this-is-an-extremely-long-url-that-is-longer-than-the-trim_url_limit-character-limit-and-should-be-trimmed...</a>' % escape(short_url))\n\n        with self.subTest(value=short_url, trim_url_limit=None):\n            self.assertEqual(urlize(short_url, trim_url_limit=None),\n                             '<a href=\"%s\">https://www.example.com/short</a>' % escape(short_url))\n", "    def test_urlize_trim_url_limit(self):\n        long_url = 'https://www.example.com/this-is-a-very-long-url-that-needs-to-be-truncated-to-a-reasonable-length'\n        short_url = 'https://www.example.com/short'\n\n        with self.subTest(value=long_url, trim_url_limit=20):\n            self.assertEqual(urlize(long_url, trim_url_limit=20),\n                             'https://www.example.com/this-is-a-ve...')\n        with self.subTest(value=short_url, trim_url_limit=None):\n            self.assertEqual(urlize(short_url, trim_url_limit=None),\n                             short_url)\n\n\n\n", "    def test_urlize_options(self):\n        tests = (\n            (\n                'URL with long text: <a href=\"http://www.longurl.com/a-very-long-url-with-lots-of-text-and-characters?q=test\">long link</a>',\n                'URL with long text: <a href=\"http://www.longurl.com/a-very-long-url-with-lots-of-text-and-characters?q=test\">long link</a>',\n                trim_url_limit=100\n            ),\n            ('More text with <a href=\"http://www.longurl.com/a-very-long-url-with-lots-of-text-and-characters?q=test\">long link</a>',\n             'More text with <a href=\"http://www.longurl.com/a-very-long-url-with-lots-of-text-and-characters?q=test\">long link</a>',\n             trim_url_limit=100),\n            (\n                'http://www.longurl.com/a-very-long-url-with-lots-of-text-and-characters?q=test and more text.',\n                'http://www.longurl.com/a-very-long-url-with-lots-of-text-and-characters?q=test... and more text.',\n                trim_url_limit=100,\n            ),\n\n        )\n        for text, expected, trim_url_limit in tests:\n            with self.subTest(text=text, expected=expected, trim_url_limit=trim_url_limit):\n                self.assertEqual(urlize(text, trim_url_limit=trim_url_limit), expected)\n", "    def test_urlize_with_trim_url_limit(self):\n        tests = (\n            (\n                'This is a very long url: http://example.com/this-is-a-very-long-url-that-is-longer-than-the-limit-of-30-characters',\n                'This is a very long url: <a href=\"http://example.com/this-is-a-very-long-url-th...\" />'\n            ),\n            (\n                'This is a short url: http://example.com/this-is-a-short-url',\n                'This is a short url: <a href=\"http://example.com/this-is-a-short-url\">http://example.com/this-is-a-short-url</a>'\n            ),\n            (\n                lazystr('This is a very long url: http://example.com/this-is-a-very-long-url-that-is-longer-than-the-limit-of-30-characters'),\n                'This is a very long url: <a href=\"http://example.com/this-is-a-very-long-url-th...\" />'\n            ),\n        )\n        for value, expected in tests:\n            with self.subTest(value=value):\n                self.assertEqual(urlize(value, trim_url_limit=30), expected) \n"], "sample_556": ["def test_savefig_metadata_format_arg():\n    with pytest.raises(ValueError, match=\"Both 'format' \"\n                       \"and 'metadata' are incompatible\"):\n        Figure().savefig(io.BytesIO(), format='png', metadata={\"test\": \"value\"},\n                         metadata=None) \n", "def test_savefig_metadata_with_invalid_key():\n    with pytest.raises(ValueError, match=\"Invalid metadata key\"):\n        Figure().savefig(io.BytesIO(), format='png', metadata={'invalid_key': 'value'})\n", "def test_savefig_bbox_inches_tight(fig_test, fig_ref):\n    fig_test.savefig('test.png', bbox_inches='tight')\n    fig_ref.savefig('test.png', bbox_inches='tight')\n    # This needs to be image comparison driven,\n    # as the exact size will change due to the\n    # tight bbox affecting padding around the\n    # rendered image\n\n\n", "def test_add_subplot_gridspec_kw():\n    fig = plt.figure()\n    gs = fig.add_gridspec(2, 3, width_ratios=[1, 2, 3], height_ratios=[0.5, 1])\n    ax = fig.add_subplot(gs[0, 0])\n    assert ax.get_position().width == 1 / 6\n    assert ax.get_position().height == 0.5\n\n    ax = fig.add_subplot(gs[0, 1])\n    assert ax.get_position().width == 2 / 6\n    assert ax.get_position().height == 0.5\n\n\n\n", "def test_savefig_dpi_from_rc():\n    with mpl.rc_context({'figure.dpi': 240}):\n        fig = plt.figure()\n        fig.savefig(io.BytesIO(), dpi=None)\n        assert fig.savefig(io.BytesIO(), dpi=None).get_dpi() == 240\n\n\n"], "sample_1004": ["compilation error", "def test_condition_with_function():\n    f = Function('f')\n    c = ConditionSet(x, Eq(f(x), 1), Interval(0, 2*pi))\n    assert c.contains(0)\n    assert c.contains(pi)\n    assert not c.contains(3*pi)\n\n\n\n", "def test_issue_10177():\n    assert ConditionSet(x, x in FiniteSet(1, 2), FiniteSet(1, 2, 3)) == FiniteSet(1, 2)\n", "compilation error", "def test_ConditionSet_subs_non_symbol():\n    raises(ValueError, lambda: ConditionSet(x + 1, x < 1, S.Integers)\n    .subs('x', 1))\n"], "sample_636": ["    def test_duplicate_code_raw_strings_comments(self) -> None:\n        \"\"\"Tests if comments within similar code blocks are handled correctly.\"\"\"\n        path = join(DATA, \"raw_strings_comments\")\n        expected_output = \"Similar lines in 2 files\"\n        self._test_output(\n            [path, \"--disable=all\", \"--enable=duplicate-code\"],\n            expected_output=expected_output,\n        )\n", "    def test_duplicate_code_raw_strings_disable_scope_multi_levels(self) -> None:\n        \"\"\"Tests disabling duplicate-code at multiple inner scope levels.\"\"\"\n        path = join(DATA, \"raw_strings_disable_scope_multi_levels\")\n        expected_output = \"Similar lines in 2 files\"\n        self._test_output(\n            [path, \"--disable=all\", \"--enable=duplicate-code\"],\n            expected_output=expected_output,\n        )\n", "    def test_duplicate_code_raw_strings_ignore_comments(self) -> None:\n        \"\"\"Tests if comments are properly ignored in similar lines detection.\"\"\"\n        path = join(DATA, \"raw_strings_ignore_comments\")\n        self._runtest([path, \"--disable=all\", \"--enable=duplicate-code\"], code=0)\n", "    def test_duplicate_code_raw_strings_multiple_disable_lines(self) -> None:\n        \"\"\"Tests disabling duplicate-code at multiple lines in a single file.\"\"\"\n        path = join(DATA, \"raw_strings_multiple_disable_lines\")\n        expected_output = \"Similar lines in 2 files\"\n        self._test_output(\n            [path, \"--disable=all\", \"--enable=duplicate-code\"],\n            expected_output=expected_output,\n        ) \n", "    def test_duplicate_code_raw_strings_disable_scope_nested_func(self) -> None:\n        \"\"\"Tests disabling duplicate-code at an inner scope level with nested functions.\"\"\"\n        path = join(DATA, \"raw_strings_disable_scope_nested_functions\")\n        expected_output = \"Similar lines in 2 files\"\n        self._test_output(\n            [path, \"--disable=all\", \"--enable=duplicate-code\"],\n            expected_output=expected_output,\n        )\n"], "sample_73": ["    def test_file_addition_after_collectstatic(self):\n        # Create initial static files.\n        file_contents = (\n            ('foo.png', 'foo'),\n            ('bar.css', 'url(\"foo.png\")\\nurl(\"xyz.png\")'),\n        )\n        for filename, content in file_contents:\n            with open(self._get_filename_path(filename), 'w') as f:\n                f.write(content)\n\n        with self.modify_settings(STATICFILES_DIRS={'append': self._temp_dir}):\n            finders.get_finder.cache_clear()\n            # First collectstatic run.\n            call_command('collectstatic', interactive=False, verbosity=0)\n            relpath = self.hashed_file_path('test/bar.css')\n            with storage.staticfiles_storage.open(relpath) as relfile:\n                content = relfile.read()\n                self.assertIn(b'foo.acbd18db4cc2.png', content)\n\n            # Add a new file after the first collectstatic.\n            with open(self._get_filename_path('xyz.png'), 'w') as f:\n                f.write('xyz')\n\n            # The new file is included in the manifest after the second\n            # collectstatic run.\n            call_command('collectstatic', interactive=False, verbosity=0)\n            relpath = self.hashed_file_path('test/bar.css')\n            with storage.staticfiles_storage.open(relpath) as relfile:\n                content = relfile.read()\n                self.assertIn(b'foo.acbd18db4cc2.png', content)\n                self.assertIn(b'xyz.d16fb36f0911.png', content)\n", "    def test_file_missing_after_collectstatic(self):\n        # Create initial static files.\n        file_contents = (\n            ('foo.png', 'foo'),\n            ('bar.css', 'url(\"foo.png\")'),\n        )\n        for filename, content in file_contents:\n            with open(self._get_filename_path(filename), 'w') as f:\n                f.write(content)\n\n        with self.modify_settings(STATICFILES_DIRS={'append': self._temp_dir}):\n            finders.get_finder.cache_clear()\n            err = StringIO()\n            # First collectstatic run.\n            call_command('collectstatic', interactive=False, verbosity=0, stderr=err)\n            relpath = self.hashed_file_path('test/bar.css')\n            with storage.staticfiles_storage.open(relpath) as relfile:\n                content = relfile.read()\n                self.assertIn(b'foo.acbd18db4cc2.png', content)\n\n            # Delete the file referenced in the CSS file.\n            os.remove(self._get_filename_path('foo.png'))\n\n            # Run collectstatic again, expecting a 404.\n            err = StringIO()\n            with self.assertRaisesMessage(ValueError, \"The file 'foo.png' could not be found with\"):\n                call_command('collectstatic', interactive=False, verbosity=0, stderr=err)\n\n", "    def test_file_change_after_collectstatic_with_queryparams(self):\n        # Create initial static files.\n        file_contents = (\n            ('foo.png', 'foo'),\n            ('bar.css', 'url(\"foo.png\")\\nurl(\"xyz.png\")'),\n            ('xyz.png', 'xyz'),\n        )\n        for filename, content in file_contents:\n            with open(self._get_filename_path(filename), 'w') as f:\n                f.write(content)\n\n        with self.modify_settings(STATICFILES_DIRS={'append': self._temp_dir}):\n            finders.get_finder.cache_clear()\n            err = StringIO()\n            # First collectstatic run.\n            call_command('collectstatic', interactive=False, verbosity=0, stderr=err)\n            relpath = self.hashed_file_path('test/bar.css?v=1')\n            with storage.staticfiles_storage.open(relpath) as relfile:\n                content = relfile.read()\n                self.assertIn(b'foo.acbd18db4cc2.png', content)\n                self.assertIn(b'xyz.d16fb36f0911.png', content)\n\n            # Change the contents of the png files.\n            for filename in ('foo.png', 'xyz.png'):\n                with open(self._get_filename_path(filename), 'w+b') as f:\n                    f.write(b\"new content of file to change its hash\")\n\n            # The hashes of the png files in the CSS file are updated after\n            # a second collectstatic.\n            call_command('collectstatic', interactive=False, verbosity=0, stderr=err)\n            relpath = self.hashed_file_path('test/bar.css?v=1')\n            with storage.staticfiles_storage.open(relpath) as relfile:\n                content = relfile.read()\n                self.assertIn(b'foo.57a5cb9ba68d.png', content)\n                self.assertIn(b'xyz.57a5cb9ba68d.png', content)\n", "    def test_file_added_after_collectstatic(self):\n        # Create initial static files.\n        file_contents = (\n            ('bar.css', 'url(\"foo.png\")\\nurl(\"xyz.png\")'),\n        )\n        for filename, content in file_contents:\n            with open(self._get_filename_path(filename), 'w') as f:\n                f.write(content)\n\n        with self.modify_settings(STATICFILES_DIRS={'append': self._temp_dir}):\n            finders.get_finder.cache_clear()\n            err = StringIO()\n            # First collectstatic run.\n            call_command('collectstatic', interactive=False, verbosity=0, stderr=err)\n            relpath = self.hashed_file_path('test/bar.css')\n            with storage.staticfiles_storage.open(relpath) as relfile:\n                content = relfile.read()\n                self.assertIn(b'foo.acbd18db4cc2.png', content)\n                self.assertIn(b'xyz.d16fb36f0911.png', content)\n\n            # Add a new file after the first collectstatic.\n            with open(self._get_filename_path('foo.png'), 'w') as f:\n                f.write('foo')\n\n            # Collectstatic should pick up the new file and update the content.\n            call_command('collectstatic', interactive=False, verbosity=0, stderr=err)\n            relpath = self.hashed_file_path('test/bar.css')\n            with storage.staticfiles_storage.open(relpath) as relfile:\n                content = relfile.read()\n                self.assertIn(b'foo.acbd18db4cc2.png', content)\n                self.assertIn(b'xyz.d16fb36f0911.png', content)\n", "    def test_static_files_storage_with_override(self):\n        with self.override_settings(\n            STATICFILES_STORAGE='django.contrib.staticfiles.storage.FileSystemStorage'\n        ):\n            self.assertEqual(storage.staticfiles_storage.location, settings.STATIC_ROOT)\n            self.assertIsInstance(storage.staticfiles_storage, FileSystemStorage)\n\n\n"], "sample_56": ["    def test_inlines_with_multiple_fk_fields(self):\n        class SongInline1(admin.TabularInline):\n            model = Song\n            fields = ('title', 'album')\n            extra = 1\n\n        class SongInline2(admin.TabularInline):\n            model = Song\n            fields = ('title', 'artist')\n            extra = 1\n\n        class AlbumAdmin(admin.ModelAdmin):\n            inlines = [SongInline1, SongInline2]\n\n        errors = AlbumAdmin(Album, AdminSite()).check()\n        self.assertEqual(errors, [])\n", "    def test_list_filter_works_on_nested_through_fields(self):\n        \"\"\"\n        Ensure list_filter can access nested through fields even when the app registry\n        is not ready; refs #24146.\n        \"\"\"\n        class AuthorAdminWithListFilter(admin.ModelAdmin):\n            list_filter = ['books__albums__title']\n\n        # Temporarily pretending apps are not ready yet. This issue can happen\n        # if the value of 'list_filter' refers to a 'through__field'.\n        Book._meta.apps.ready = False\n        try:\n            errors = AuthorAdminWithListFilter(Author, AdminSite()).check()\n            self.assertEqual(errors, [])\n        finally:\n            Book._meta.apps.ready = True\n", "    def test_list_filter_works_on_through_field(self):\n        class BookAdminWithListFilter(admin.ModelAdmin):\n            list_filter = ['authorsbooks__featured']\n\n        errors = BookAdminWithListFilter(Book, AdminSite()).check()\n        self.assertEqual(errors, [])\n", "    def test_readonly_fields_list_order(self):\n        \"\"\"\n        Tests that the order of readonly_fields is preserved, even when it\n        contains attributes of the model and callable attributes.\n        \"\"\"\n        class SongAdmin(admin.ModelAdmin):\n            readonly_fields = (\n                \"title\",\n                lambda obj: \"test\",\n                \"not_a_readable_field\",\n            )\n\n        errors = SongAdmin(Song, AdminSite()).check()\n        self.assertEqual(errors, [])\n\n\n\n", "    def test_list_filter_works_on_through_field_even_when_apps_not_ready_another_field(self):\n        \"\"\"\n        Ensure list_filter can access reverse fields even when the app registry\n        is not ready; refs #24146.\n        \"\"\"\n        class BookAdminWithListFilterAnotherField(admin.ModelAdmin):\n            list_filter = ['authorsbooks__featured', 'genre']\n\n        # Temporarily pretending apps are not ready yet. This issue can happen\n        # if the value of 'list_filter' refers to a 'through__field'.\n        Book._meta.apps.ready = False\n        try:\n            errors = BookAdminWithListFilterAnotherField(Book, AdminSite()).check()\n            self.assertEqual(errors, [])\n        finally:\n            Book._meta.apps.ready = True\n"], "sample_415": ["    def test_invalid_field_types(self):\n        with self.assertRaisesMessage(\n            TypeError, \"UniqueConstraint fields must be of type str or model field.\"\n        ):\n            models.UniqueConstraint(name=\"test\", fields=[1])\n\n        with self.assertRaisesMessage(\n            TypeError, \"UniqueConstraint fields must be of type str or model field.\"\n        ):\n            models.UniqueConstraint(name=\"test\", fields=[True])\n\n", "    def test_condition_and_name_collision(self):\n        with self.assertRaisesMessage(ValueError, \"UniqueConstraint condition cannot have the same name with a field\"):\n            models.UniqueConstraint(\n                fields=[\"name\"],\n                name=\"name\",\n                condition=models.Q(name=models.F(\"name\")),\n            )\n", "    def test_unique_constraint_with_integer_expression(self):\n        msg = \"Unique constraint with integer expression is not allowed.\"\n        with self.assertRaisesMessage(ValueError, msg):\n            models.UniqueConstraint(\n                models.expressions.ExpressionWrapper(\n                    models.F(\"id\") + 1, output_field=models.IntegerField()\n                ),\n                name=\"id_plus_one_unique\",\n            ) \n", "    def test_invalid_char_in_name(self):\n        msg = r\"UniqueConstraint name cannot contain invalid characters\"\n        with self.assertRaisesMessage(ValueError, msg):\n            models.UniqueConstraint(fields=[\"field\"], name=\"!@#name\")\n", "    def test_abstract_name(self):\n        constraints = get_constraints(ChildModel._meta.db_table)\n        self.assertIn(\"constraints_childmodel_adult\", constraints)\n"], "sample_619": ["def test_decode_cf_datetime_with_invalid_units(calendar):\n    units = \"this-is-not-a-valid-units\"\n    with pytest.raises(ValueError, match=\"Invalid datetime units: this-is-not-a-valid-units\"):\n        decode_cf_datetime([0, 1], units, calendar)\n", "compilation error", "def test_decode_cf_datetime_nan(calendar) -> None:\n    # GH 2851\n    nan_datetime = np.array([np.nan, 1, 2])\n    units = \"days since 2000-01-01\"\n    result = decode_cf_datetime(nan_datetime, units, calendar)\n    expected = np.array([np.nan, 1, 2], dtype=np.datetime64)\n    np.testing.assert_equal(result, expected)\n\n\n\n", "def test_decode_cf_datetime_with_nan():\n    # GH 1469\n    data = np.array([1, np.nan, 3])\n    units = \"days since 2000-01-01\"\n    result = decode_cf_datetime(data, units, calendar=\"standard\")\n    expected = np.array([pd.Timestamp(\"2000-01-02\"), np.nan, pd.Timestamp(\"2000-01-04\")])\n    np.testing.assert_array_equal(result, expected)\n", "def test_decode_cf_datetime_negative_times():\n    units = \"days since 1970-01-01 00:00:00\"\n    num_dates = np.array([-1, -2, -3], dtype=np.int64)\n\n    with pytest.raises(ValueError) as e:\n        decode_cf_datetime(num_dates, units)\n    assert \"decode negative times\" in str(e.value)\n\n\n\n"], "sample_691": ["    def test_timeout_disabled(pytester: Pytester, timeout: float) -> None:\n        \"\"\"Test that timeout option is ignored when faulthandler is disabled.\"\"\"\n        pytester.makepyfile(\n            \"\"\"\n            import os, time\n                time.sleep(timeout)\n            \"\"\"\n        )\n        pytester.makeini(\n            \"\"\"\n            [pytest]\n            faulthandler_timeout = %s\n            \"\"\" % timeout\n        )\n        result = pytester.runpytest_subprocess(\"-p\", \"no:faulthandler\")\n        result.stdout.fnmatch_lines([\"*1 passed*\"])\n        assert result.ret == 0\n", "def test_timeout_with_pdb_entry(pytester: Pytester) -> None:\n    \"\"\"\n    Test that fault handler doesn't dump traceback when entering pdb due to timeout.\n\n\n    \"\"\"\n    pytester.makepyfile(\n        \"\"\"\n    import os, time\n    import faulthandler\n\n        time.sleep(0.1)\n        import pdb; pdb.set_trace()\n\n    \"\"\"\n    )\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        faulthandler_timeout = 0.05\n        \"\"\"\n    )\n    result = pytester.runpytest_subprocess()\n    result.stdout.fnmatch_lines([\"*1 passed*\"])\n    assert result.ret == 0\n\n\n\n", "    def test_disabled_still_works_with_other_plugins(pytester: Pytester) -> None:\n        \"\"\"Test that the fault handler still works when other plugins are enabled,\n        especially when they might modify sys.stderr (e.g., pytest-xdist).\"\"\"\n        pytester.makepyfile(\n            \"\"\"\n        import faulthandler\n            faulthandler._sigabrt()\n        \"\"\"\n        )\n\n        # Enable a plugin that might modify sys.stderr (pytest-xdist)\n        result = pytester.runpytest_subprocess(\n            \"-p\", \"xdist\"\n        )\n        result.stderr.fnmatch_lines([\"*Fatal Python error*\"])\n        assert result.ret != 0\n\n", "def test_timeout_long_running_test(pytester: Pytester) -> None:\n    \"\"\"Test that fault handler works with very long running tests.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n    import time\n    import faulthandler\n        faulthandler.dump_traceback_later(10, file=sys.stderr)\n        time.sleep(2)  # Run longer than the timeout\n    \"\"\"\n    )\n    result = pytester.runpytest_subprocess(\n        \"--timeout=1\"\n    )\n    assert result.ret != 0  \n    result.stderr.fnmatch_lines([\"*most recent call first*\"])\n", "def test_timeout_with_pdb(pytester: Pytester) -> None:\n    \"\"\"Test that timeout is honoured even when entering pdb.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n    import os, time\n    import faulthandler\n\n        time.sleep(1 if \"CI\" in os.environ else 0.1)\n        import pdb; pdb.set_trace()\n    \"\"\"\n    )\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        faulthandler_timeout = 0.05\n        \"\"\"\n    )\n    result = pytester.runpytest_subprocess()\n    result.stderr.fnmatch_lines([\"*%s*\" % \"most recent call first\"])\n    assert \"breakpoint\" in result.stderr.str()\n    result.stdout.fnmatch_lines([\"*1 passed*\"])\n    assert result.ret == 0\n"], "sample_740": ["def test_check_array_nbytes():\n    X = np.array([1, 2, 3], dtype=np.int32)\n    X_memmap = np.memmap(f\"test_memmap.dat\", dtype=X.dtype, mode='w+',\n                         shape=X.shape)\n    X_memmap[:] = X\n    X_memmap_ref = np.memmap(f\"test_memmap.dat\", dtype=X.dtype, mode='r')\n    assert_equal(check_array(X_memmap, nbytes=X.nbytes).nbytes, X.nbytes)\n    assert_equal(check_array(X_memmap_ref, nbytes=X.nbytes).nbytes, X.nbytes)\n\n    try:\n        check_array(X_memmap, nbytes=X.nbytes + 1)\n    except ValueError:\n        pass\n\n\n", "def test_check_array_sparse_dtype():\n    X_csr = sp.csr_matrix([[1, 2], [3, 4]], dtype=np.float16)\n    assert_equal(check_array(X_csr).dtype, np.float16)\n\n    X_csc = sp.csc_matrix([[1, 2], [3, 4]], dtype=np.int8)\n    assert_equal(check_array(X_csc).dtype, np.int8)\n\n    X_coo = sp.coo_matrix([[1, 2], [3, 4]], dtype=np.complex64)\n    assert_equal(check_array(X_coo).dtype, np.complex64)\n\n\n\n", "compilation error", "def test_check_array_memmap_dtype(dtype):\n    X = np.ones((4, 4), dtype=dtype)\n    with TempMemmap(X, mmap_mode='r') as X_memmap:\n        X_checked = check_array(X_memmap, dtype=dtype, copy=False)\n        assert X_checked.dtype == dtype\n", "compilation error"], "sample_588": ["    def test_auto_combine_with_mismatched_coords(self):\n        objs = [\n            Dataset({\"x\": (\"z\", [0])}, coords={\"z\": (\"z\", [1])}),\n            Dataset({\"x\": (\"z\", [1])}, coords={\"z\": (\"z\", [0])}),\n        ]\n        with pytest.warns(FutureWarning, match=\"supplied have global\"):\n            auto_combine(objs)\n", "    def test_auto_combine_with_missing_coords(self):\n        ds1 = Dataset({\"x\": [0, 1]}).copy()\n        ds2 = Dataset({\"x\": [2, 3]})\n        with pytest.warns(FutureWarning, match=\"auto_combine with varying\"):\n            auto_combine([ds1, ds2])\n", "    def test_auto_combine_non_consecutive_index(self):\n        objs = [\n            Dataset({\"foo\": (\"x\", [0, 2])}, coords={\"x\": (\"x\", [0, 1, 2])}),\n            Dataset({\"foo\": (\"x\", [1, 3])}, coords={\"x\": (\"x\", [1, 2, 3])}),\n        ]\n        with pytest.warns(FutureWarning, match=\"indexes are not\"):\n            auto_combine(objs)\n", "    def test_auto_combine_with_nested_dicts(self):\n        objs = [\n            Dataset({'data': {'a': ('x', [0]), 'b': ('y', [1])}, coords={'x': ('x', [0]), 'y': ('y', [0])}}),\n            Dataset({'data': {'a': ('x', [1]), 'b': ('y', [2])}, coords={'x': ('x', [1]), 'y': ('y', [1])}}),\n        ]\n        with pytest.warns(FutureWarning, match=\"`auto_combine`\"):\n            auto_combine(objs)\n", "    def test_auto_combine_with_empty_datasets(self):\n        with pytest.warns(FutureWarning, match=\"at least one\"):\n            auto_combine([])\n\n        with pytest.warns(FutureWarning, match=\"at least one\"):\n            auto_combine([Dataset()])\n"], "sample_241": ["    def test_output_field_from_expression(self):\n        value = Value('value')\n        expr = ExpressionWrapper(\n            value, output_field=IntegerField()\n        )\n        self.assertEqual(expr.output_field, IntegerField())", "    def test_non_empty_group_by_with_alias(self):\n        value = Value('f')\n        value.output_field = None\n        expr = ExpressionWrapper(Lower(value), output_field=IntegerField(), alias='test_alias')\n        group_by_cols = expr.get_group_by_cols(alias=None)\n        self.assertEqual(group_by_cols, [expr.expression])\n        self.assertEqual(group_by_cols[0].output_field, expr.output_field)\n        self.assertEqual(group_by_cols[0].alias, 'test_alias')\n", "    def test_output_field_not_specified(self):\n        expr = ExpressionWrapper(Value('f'), output_field=None)\n        with self.assertRaises(ValueError):\n            expr.get_group_by_cols(alias=None)\n", "    def test_transform_with_field(self):\n        expr = ExpressionWrapper(F('cost') + F('tax'), output_field=IntegerField())\n        with self.subTest(group_by=None):\n            group_by_cols = expr.get_group_by_cols(alias=None)\n            self.assertEqual(group_by_cols, [expr.expression])\n\n        with self.subTest(group_by='field'):\n            group_by_cols = expr.get_group_by_cols(alias='field')\n            self.assertEqual(group_by_cols, ['field'])\n\n\n\n", "    def test_non_empty_group_by_with_alias(self):\n        value = Value('f')\n        value.output_field = None\n        expr = ExpressionWrapper(Lower(value), output_field=IntegerField(), alias='f_lower')\n        group_by_cols = expr.get_group_by_cols(alias='f_lower')\n        self.assertEqual(group_by_cols, [expr.expression])\n        self.assertEqual(group_by_cols[0].output_field, expr.output_field)\n"], "sample_692": ["def test_tmpdir_basetemp_is_really_a_directory(pytester: Pytester) -> None:\n    p = pytester.mkdir(\"mytemp\")\n    with pytester.raises(ValueError):\n        pytester.makepyfile(\n            \"\"\"\n                assert tmpdir == str(mytemp)\n        \"\"\"\n        )\n\n\n\n", "compilation error", "compilation error", "compilation error", "def test_cleanup_numbered_dir_with_existing_lock(tmp_path):\n    d = tmp_path.joinpath(\"test\")\n    d.mkdir()\n    create_cleanup_lock(d)\n\n    cleanup_numbered_dir(\n        root=tmp_path,\n        prefix=\"test\",\n        keep=2,\n        consider_lock_dead_if_created_before=0,\n    )\n\n    assert d.exists() \n\n\n\n"], "sample_849": ["def test_time_series_cv_with_groups():\n    X = np.arange(10).reshape((10, 1))\n    groups = np.array([1, 1, 2, 2, 3, 3, 4, 4, 5, 5])\n    tscv = TimeSeriesSplit(n_splits=3)\n\n    splits = tscv.split(X, groups=groups)\n    for train_idx, test_idx in splits:\n        # Check if train and test sets maintain the group order\n        assert all(groups[train_idx] == groups[train_idx[0]]), \\\n            \"Train groups are not consistent within a fold\"\n        assert all(groups[test_idx] == groups[test_idx[0]]), \\\n            \"Test groups are not consistent within a fold\"\n", "def test_time_series_cv_with_groups():\n    X = np.zeros((10, 2))\n    y = np.zeros(10)\n    groups = np.repeat([1, 2], 5)\n    tscv = TimeSeriesSplit(n_splits=3)\n    \n    # Check that groups are maintained across folds\n    for train, test in tscv.split(X, y, groups=groups):\n        assert len(np.unique(groups[train])) == 1\n        assert len(np.unique(groups[test])) == 1\n\n\n", "def test_cross_val_predict_with_groups():\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12], [13, 14]])\n    y = np.array([0, 0, 0, 1, 1, 1, 1])\n    groups = np.array([0, 0, 0, 1, 1, 1, 1])\n\n    X_pred = cross_val_predict(\n        LogisticRegression(), X, y, cv=StratifiedKFold(n_splits=3, shuffle=False),\n        groups=groups)\n    assert X_pred.size == len(X)\n    assert X_pred.shape == X.shape[0]\n\n    X_pred_shuffled = cross_val_predict(\n        LogisticRegression(), X, y, cv=StratifiedKFold(n_splits=3),\n        groups=groups)\n    assert X_pred_shuffled.size == len(X)\n    assert X_pred_shuffled.shape == X.shape[0]\n\n\n", "def test_check_cv_string_input():\n    with pytest.raises(ValueError, match=\"cv should be an iterable\"):\n        check_cv(\"yolo\")\n\n    with pytest.raises(ValueError, match=\"cv should be an iterable\"):\n        check_cv([1, 2, 3])\n\n    with pytest.raises(ValueError, match=\"cv should be an iterable\"):\n        check_cv(ShuffleSplit(n_splits=3))\n\n", "def test_time_series_split_empty_dataset():\n    with pytest.raises(ValueError, match=r\"Cannot perform TimeSeriesSplit with n_samples=0\"):\n        TimeSeriesSplit(n_splits=3).split([])\n\n\n"], "sample_465": ["    def test_module_perms(self):\n        ma = ModelAdmin(Band, AdminSite())\n        request = MockRequest()\n        request.user = self.MockUser()\n        self.assertNotIn(\"modeladmin.add_band\", ma.get_actions(request))\n        self.assertNotIn(\"modeladmin.change_band\", ma.get_actions(request))\n        self.assertNotIn(\"modeladmin.delete_band\", ma.get_actions(request))\n        request.user = self.MockAddUser()\n        self.assertIn(\"add_band\", ma.get_actions(request))\n        self.assertNotIn(\"modeladmin.change_band\", ma.get_actions(request))\n        self.assertNotIn(\"modeladmin.delete_band\", ma.get_actions(request))\n        request.user = self.MockChangeUser()\n        self.assertIn(\"add_band\", ma.get_actions(request))\n        self.assertIn(\"change_band\", ma.get_actions(request))\n        self.assertNotIn(\"modeladmin.delete_band\", ma.get_actions(request))\n        request.user = self.MockDeleteUser()\n        self.assertIn(\"add_band\", ma.get_actions(request))\n        self.assertIn(\"change_band\", ma.get_actions(request))\n        self.assertIn(\"delete_band\", ma.get_actions(request))\n\n\n\n", "    def test_get_formset_for_add(self):\n        class AdminConcertForm(forms.ModelForm):\n            class Meta:\n                model = Concert\n                exclude = [\"transport\"]\n\n        class ConcertAdminWithForm(ModelAdmin):\n            form = AdminConcertForm\n\n        ma = ConcertAdminWithForm(Concert, self.site)\n        request = MockRequest()\n        self.assertEqual(\n            list(ma.get_formset_for_add(request)[0].fields),\n            [\"main_band\", \"opening_band\", \"day\"],\n        )\n\n", "    def test_get_permissions(self):\n        ma = ModelAdmin(Band, AdminSite())\n        self.assertEqual(ma.get_permissions(), [\n            (\"add_band\", \"Can add band\"),\n            (\"change_band\", \"Can change band\"),\n            (\"delete_band\", \"Can delete band\"),\n            (\"view_band\", \"Can view band\"),\n        ])\n\n\n", "    def test_has_add_permission_with_custom_function(self):\n        class BandAdmin(ModelAdmin):\n                return request.user.is_staff\n\n        ma = BandAdmin(Band, AdminSite())\n        request = MockRequest()\n        request.user = User.objects.create_superuser(\n            username=\"bob\", email=\"bob@test.com\", password=\"test\"\n        )\n        self.assertTrue(ma.has_add_permission(request))\n        request.user = User.objects.create_user(\n            username=\"user\", email=\"user@test.com\", password=\"test\"\n        )\n        self.assertFalse(ma.has_add_permission(request))\n\n\n\n", "    def test_get_urls(self):\n        ma = ModelAdmin(Band, self.site)\n        urls = ma.get_urls(self.site)\n        expected_urls = [\n            url(r\"^bands/\", self.admin_site.admin_view(ma.changelist_view), name=\"band_changelist\"),\n            url(r\"^bands/add/$\", self.admin_site.admin_view(ma.add_view), name=\"band_add\"),\n        ]\n        self.assertEqual(urls, expected_urls)\n"], "sample_369": ["    def test_remove_single_m2m_field(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.RemoveField('testapp.Book', 'authors'),\n            ]\n\n        migration = Migration('0001_initial', 'testapp')\n        self.assertEqual(migration.suggest_name(), 'remove_authors')\n", "    def test_operation_field_changes_suggest_name(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.AlterField('testapp.Person', 'first_name',\n                                    field=models.CharField(max_length=100)),\n            ]\n\n        migration = Migration('some_migration', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'person_first_name')\n", "    def test_custom_migration_name_priority(self):\n        class Migration(migrations.Migration):\n            name = 'my_custom_name'\n            operations = [\n                migrations.CreateModel('Person', fields=[])\n            ]\n\n        migration = Migration('0001_initial', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'my_custom_name')\n", "    def test_operation_with_long_name(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\n                    'SuperDuperLongModelName' * 53, fields=[]\n                ),\n            ]\n\n        migration = Migration('some_migration', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'superduperlongmodelname' * 53)\n", "    def test_suggest_name_with_default_args(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\n                    name='ModelWithDefaultArgs',\n                    fields=[\n                        migrations.CharField(max_length=100, default='default_value'),\n                    ],\n                ),\n            ]\n\n        migration = Migration('0001_initial', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'modelwithdefaultargs')\n"], "sample_1163": ["def test_issue_16183():\n    from sympy import Abs, symbols\n    x = symbols('x y z')\n    assert Abs(x + y + z).expand() == Abs(x + y + z)\n", "def test_issue_16308():\n    from sympy import exp, symbols, re, im\n\n    x, y = symbols('x y', complex=True)\n    assert re(exp(x + I*y)).subs(x, pi**2).subs(y, pi**2) == exp(pi**2) * cos(pi**2)\n    assert im(exp(x + I*y)).subs(x, pi**2).subs(y, pi**2) == exp(pi**2) * sin(pi**2)\n", "def test_issue_16725():\n    from sympy import sin, cos, tan, exp,  Symbol,  pi, sqrt, Abs\n    x = Symbol('x')\n    y = Symbol('y')\n    a = Symbol('a')\n    \n    assert Abs(sin(x) + cos(x) * I).rewrite(exp) == Abs(exp(I*x)  + exp(-I*x)).rewrite(exp)\n    assert Abs(sin(x)*exp(I*y) + cos(x)*exp(I*y)*I).rewrite(exp) == \\\n        abs(exp(I*y)*(sin(x) + cos(x)*I)).rewrite(exp)\n    assert Abs(x*sin(a) + y*cos(a) * I).rewrite(exp) == \\\n        sqrt((x*sin(a))**2 + (y*cos(a))**2).rewrite(exp) \n    assert Abs(exp(I*x) * tan(x)).rewrite(exp) ==  sqrt( (exp(2*I*x) - 1)**2/(exp(2*I*x) + 1)**2 ).rewrite(exp)\n    assert Abs(exp(I*x) * sqrt(exp(I*x) + exp(-I*x))).rewrite(exp) == \\\n        sqrt( (exp(2*I*x) + 1) / 2 ).rewrite(exp)\n\n", "def test_issue_15893_complex():\n    f = Function('f', complex=True)\n    x = Symbol('x', complex=True)\n    eq = Derivative(Abs(f(x)), f(x))\n    assert eq.doit() == sign(f(x)) \n\n", "def test_issue_19317():\n    from sympy import Symbol, Abs, re, im\n    z = Symbol('z', complex=True)\n    x = Symbol('x', real=True)\n    y = Symbol('y', real=True)\n    assert re(z**2).expand(trig=True) == re(z**2)\n    assert im(z**2).expand(trig=True) == im(z**2)\n    assert Abs(z*x).expand(trig=True) == Abs(z*x)\n    assert Abs(z*y).expand(trig=True) == Abs(z*y)\n\n\n\n"], "sample_375": ["    def test_proxy_inherited_pk(self):\n        A = self.create_model(\"A\", bases=(models.Model,))\n        B = self.create_model(\"B\", bases=(A,), proxy=True)\n        C = self.create_model(\"C\", bases=(B,))\n        self.assertRelated(A, [B, C])\n        self.assertRelated(B, [C])\n        self.assertRelated(C, [])\n", "    def test_proxy_with_concrete_inheriting_field(self):\n        A = self.create_model(\"A\", foreign_keys=[models.ForeignKey('P', models.CASCADE, related_name='+')])\n        B = self.create_model(\"B\", bases=(A,))\n        P = self.create_model(\"P\")\n        self.assertRelated(A, [P])\n        self.assertRelated(B, [P])\n        self.assertRelated(P, [A, B])\n", "    def test_proxy_through_m2m(self):\n        A = self.create_model(\"A\", foreign_keys=[models.ManyToManyField('B', through='T')])\n        B = self.create_model(\"B\")\n        T = self.create_model(\"T\", foreign_keys=[\n            models.ForeignKey('A', models.CASCADE),\n            models.ForeignKey('B', models.CASCADE),\n        ], proxy=True)\n        self.assertRelated(A, [B, T])\n        self.assertRelated(B, [A, T]) \n        self.assertRelated(T, [A, B]) \n\n", "    def test_inherited_abstract_proxy(self):\n        A = self.create_model(\"A\", abstract=True)\n        B = self.create_model(\"B\", bases=(A,), proxy=True)\n        C = self.create_model(\"C\", bases=(B,))\n        self.assertRelated(A, [B, C])\n        self.assertRelated(B, [C])\n        self.assertRelated(C, [])\n", "    def test_abstract_model_child_with_m2m(self):\n        Abstract = self.create_model(\"Abstract\", foreign_keys=[models.ManyToManyField('Related')])\n        Related = self.create_model(\"Related\")\n        Child = self.create_model(\"Child\", bases=(Abstract,), abstract=True)\n        self.assertRelated(Abstract, [Related])\n        self.assertRelated(Related, [Abstract, Child])\n        self.assertRelated(Child, [Related])\n\n"], "sample_1045": ["def test_issue_14462():\n    assert Float('0.123', 15) == Float('0.12345678901234567890')\n    assert Float('0.12345678901234567890', 15) == Float('0.12345678901234567890')\n    assert Float('0.12345678901234567890', 16) == Float('0.12345678901234567890')\n\n\n\n", "def test_issue_6263():\n    assert S(0).can_extract_integer() is True\n    assert S(1).can_extract_integer() is True\n    assert S(-1).can_extract_integer() is True\n    assert S(1.23).can_extract_integer() is False\n    assert S(-1.5).can_extract_integer() is False\n    assert S(Rational(1, 2)).can_extract_integer() is False\n\n", "def test_issue_10226():\n    n = Float(10**10, precision=10)\n    assert n.n(10) == 1e+10\n    assert n.n(20) == 1e+10\n    assert n.n(11) == 1e+10\n    assert n.n(100) == 1e+10\n    assert n.n(-1) == 1e+10\n", "def test_issue_6397():\n    assert Float('0.1', 10).as_mpf() == mpf('0.1')\n    assert Float('0.1', 10).as_expr() == S.One/10\n\n", "def test_issue_10784():\n    assert (S(2)**zoo).is_complex is True\n    assert (S(2)**(-oo)).is_complex is False\n"], "sample_1082": ["def test_complex_argument_handling():\n    z = complex(1, 2)\n    assert cosh(z).real == cosh(z.real)*cosh(z.imag) + I*sinh(z.real)*sinh(z.imag)\n    assert cosh(z).imag == sinh(z.real)*cosh(z.imag) + I*cosh(z.real)*sinh(z.imag)\n    assert sinh(z).real == cosh(z.real)*sinh(z.imag) + I*sinh(z.real)*cosh(z.imag)\n    assert sinh(z).imag == sinh(z.real)*cosh(z.imag) - I*cosh(z.real)*sinh(z.imag)\n    assert tanh(z).real == (sinh(z.real)*cosh(z.real) + I*sinh(z.imag)*cosh(z.imag))/(cosh(z.real)**2 + sinh(z.imag)**2)\n    assert tanh(z).imag == (-sinh(z.imag)*cosh(z.real) + I*sinh(z.real)*cosh(z.real))/(cosh(z.real)**2 + sinh(z.imag)**2)\n", "compilation error", "def test_asinh_rewrite():\n    x = Symbol('x')\n    assert asinh(x).rewrite(log) == log(x + sqrt(x**2 + 1))\n\n\n", "def test_hyperbolic_functions_with_integer_arguments():\n    for i in range(-5, 6):\n        for func in [sinh, cosh, tanh, coth, sech, csch]:\n            assert func(i).is_integer or func(i).is_rational  \n", "    def test_asinh_rewrite():\n        x = Symbol('x')\n        assert asinh(x).rewrite(log) == log(x + sqrt(x**2 + 1))\n        assert asinh(x).rewrite(atanh) == I*atanh(sqrt(x**2 + 1) / x)\n    "], "sample_1092": ["def test_issue_18818():\n    from sympy.abc import x, y\n    expr = x*y**(1/3) + y**3\n    assert cse(expr) == ([], [x*y**(1/3) + y**3])\n\n", "def test_issue_15082():\n    # Checks that cse handles constants correctly\n    const = 42\n    assert cse([const, const + 1]) == ([], [const, const + 1])\n\n\n\n", "def test_issue_16359():\n    expr = sin(x + y)*cos(x + y) + sin(x)*cos(y)\n    substs, reduced = cse(expr)\n    assert len(substs) > 0\n    assert reduced == [0.5*sin(2*(x + y))]\n\n", "def test_issue_18203():\n    eq = CRootOf(x**5 + 11*x - 2, 0) + CRootOf(x**5 + 11*x - 2, 1)\n    assert cse(eq) == ([], [eq])\n", "def test_issue_19237():\n    expr = sin(x)*cos(y)*sqrt(1 - x**2)*exp(z)\n    subst, red = cse([expr])\n    assert len(subst) == 3\n    assert red == [sin(x)*cos(y)*sqrt(1 - x**2)*exp(z)]\n"], "sample_13": ["def test_latitude_limits_rounding(value, expected_value):\n    result = Latitude(value, u.rad)\n    assert result.value == expected_value\n", "def test_angle_addition_mixed_units(input, expected_degrees):\n    a = Angle(input)\n    b = Angle(10 * u.deg)\n    result = a + b\n    assert result.value == expected_degrees.value\n    assert result.unit == expected_degrees.unit\n", "def test_angle_units_from_quantity():\n    a1 = Angle(1*u.deg)\n    assert a1.unit == u.deg\n\n    a2 = Angle(1*u.rad)\n    assert a2.unit == u.rad\n\n    a3 = Angle(1*u.hourangle)\n    assert a3.unit == u.hourangle\n", "    def test_latitude_invalid_values(value):\n        with pytest.raises(ValueError, match=r\"Invalid value\"):\n            Latitude(value, u.rad)\n\n\n\n", "def test_angle_tostring_with_nan():\n    \"\"\"\n    Test to_string() behavior with NaN values\n    \"\"\"\n    # Scalar Angle\n    nan_angle = Angle(np.nan, u.deg)\n    assert nan_angle.to_string() == 'nan'\n\n    # Array Angles\n    nan_array = np.array([1, np.nan, 3]) * u.deg\n    assert  str(Angle(nan_array)) == '[1d00m00s nan  3d00m00s]'\n"], "sample_400": ["    def test_suggest_name_with_unique_operations(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.RenameModel(\"Person\", \"Human\"),\n                migrations.CreateModel(\"Dog\", fields=[]),\n            ]\n\n        migration = Migration(\"some_migration\", \"test_app\")\n        self.assertEqual(\n            migration.suggest_name(), \"rename_person_to_human_dog\"\n        )\n\n\n", "    def test_single_operation_custom_name(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\n                    \"CustomNamedModel\", fields=[]\n                )\n            ]\n\n        migration = Migration(\"some_migration\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"custom_named_model\")\n", "    def test_suggest_name_with_custom_operations(self):\n        class CustomOperation(migrations.MigrationOperation):\n                return \"CustomOperation\"\n\n        class Migration(migrations.Migration):\n            operations = [CustomOperation()]\n\n        migration = Migration(\"0001_initial\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"custom_operation\")\n", "    def test_suggest_name_with_custom_operation(self):\n        class CustomOperation(migrations.MigrationOperation):\n                self.name = name\n\n        class Migration(migrations.Migration):\n            operations = [CustomOperation(\"SomeSpecificOperation\")]\n\n        migration = Migration(\"some_migration\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"some_specific_operation\")\n\n\n", "    def test_operation_with_custom_name(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\n                    \"Person\", fields=[], unique_together=([(\"name\", \"age\")]),\n                )\n            ]\n\n        migration = Migration(\"0001_initial\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"person\")\n\n"], "sample_555": ["compilation error", "def test_arc_invalid_arguments():\n    with pytest.raises(ValueError, match=\"theta1 must be less than theta2\"):\n        Arc((0, 0), 1, 1, theta1=90, theta2=30)\n\n    with pytest.raises(ValueError, match=\"radius must be non-negative\"):\n        Arc((0, 0), -1, 1)\n\n    with pytest.raises(ValueError, match=\"width must be non-negative\"):\n        Arc((0, 0), 1, -1)\n\n\n", "def test_arc_kwargs():\n    # Test that setting individual arc parameters works\n    fig, ax = plt.subplots()\n    for kind in ['circular', 'elliptical']:\n        a = Arc((0, 0), 1, 1,\n                theta1=0, theta2=90, angle=0,\n                **({'kind': kind} if kind == 'circular' else {'width': 0.5, 'height': 1}))\n        ax.add_patch(a)\n    ax.set_aspect('equal')\n\n    \n", "def test_arrow_clip():\n    fig, ax = plt.subplots()\n    ax.set_xlim(-1, 1)\n    ax.set_ylim(-1, 1)\n\n    arrow = ax.arrow(0, 0, 1, 0, head_width=0.2, head_length=0.3,\n                     fc='red', ec='black', clip_on=True)\n\n    ax.add_patch(FancyArrowPatch((0, 0), (1, 0),\n                                  length_includes_head=True,\n                                  fc='blue', ec='green',\n                                  arrowstyle=\"-|>\",\n                                  mutation_scale=20,\n                                  clip_on=True))\n    arrow.set_clip_on(False)\n\n    ax.set_aspect('equal')\n", "compilation error"], "sample_1036": ["def test_matmul_sympify_with_args():\n    assert isinstance(MatMul(eye(1), eye(1)).args[0], Basic)\n    assert isinstance(MatMul(2, eye(1), eye(1)).args[0], Basic)\n    assert isinstance(MatMul(eye(1), 2, eye(1)).args[1], Basic)\n\n\n", "def test_matmul_with_scalar_and_matrix():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    assert MatMul(2, A, B).args == (2, A, B)\n    assert MatMul(A, 2, B).args == (A, 2, B)\n    assert MatMul(A, B, 2).args == (A, B, 2)\n", "def test_matmul_scalar_Matrix_as_matrix():\n    # Issue 9053\n    X = Matrix([[1, 2], [3, 4]])\n    assert MatMul(2, X).as_matrix() ==  ImmutableMatrix([[2, 4], [6, 8]]) \n    assert isinstance(MatMul(2, X).as_matrix(), ImmutableMatrix) \n", "def test_sympify_matmul():\n    expr = \"(2*A + B) @ (C/3)\"\n    assert MatMul(*sympify(expr).args).doit() ==  (2/3)*MatMul(A, C) + (1/3)*MatMul(B, C)\n", "def test_matmul_with_matrices_and_numbers():\n    assert MatMul(2 * A, B, C) == 2 * MatMul(A, B, C)\n    assert MatMul(A, 3 * B, C) == 3 * MatMul(A, B, C)\n    assert MatMul(A, B, 4 * C) == 4 * MatMul(A, B, C)\n"], "sample_1034": ["def test_oracle_gate_with_multiple_targets():\n    nbits = 3\n    v = OracleGate(nbits, lambda qubits: qubits == IntQubit(2, nbits) or qubits == IntQubit(5, nbits))\n    assert qapply(v*IntQubit(0, nbits)) == IntQubit(0, nbits)\n    assert qapply(v*IntQubit(1, nbits)) == IntQubit(1, nbits)\n    assert qapply(v*IntQubit(2, nbits)) == -IntQubit(2, nbits)\n    assert qapply(v*IntQubit(3, nbits)) == IntQubit(3, nbits)\n    assert qapply(v*IntQubit(4, nbits)) == IntQubit(4, nbits)\n    assert qapply(v*IntQubit(5, nbits)) == -IntQubit(5, nbits)\n    assert qapply(v*IntQubit(6, nbits)) == IntQubit(6, nbits)\n    assert qapply(v*IntQubit(7, nbits)) == IntQubit(7, nbits)\n", "def test_apply_grover_multiple_solutions():\n    nqubits = 3\n    oracle = lambda qubits: (qubits == IntQubit(1, nqubits=nqubits) or \n                              qubits == IntQubit(5, nqubits=nqubits))\n    expected = (1/sqrt(8))*IntQubit(1, nqubits=nqubits) + (1/sqrt(8))*IntQubit(5, nqubits=nqubits)\n    assert apply_grover(oracle, nqubits) == qapply(expected)\n\n", "def test_grover_multiple_targets():\n    nqubits = 3\n    basis_states = superposition_basis(nqubits)\n    oracle_function = lambda qubits: (qubits == IntQubit(1, nqubits=nqubits) or qubits == IntQubit(5, nqubits=nqubits))\n    expected = (-13*basis_states)/64 + 264*IntQubit(1, nqubits=nqubits)/256 + 264*IntQubit(5, nqubits=nqubits)/256\n    assert apply_grover(oracle_function, 3) == qapply(expected)\n", "def test_general_oracle():\n    nqubits = 3\n    basis_states = superposition_basis(nqubits)\n    oracle_func = lambda qubits: (qubits == IntQubit(1, nqubits=nqubits)) or (qubits == IntQubit(5, nqubits=nqubits))\n    v = OracleGate(nqubits, oracle_func)\n    expected = (-13*basis_states)/64 + 264*IntQubit(1, nqubits=nqubits)/256 + 264*IntQubit(5, nqubits=nqubits)/256\n    assert apply_grover(oracle_func, nqubits) == qapply(expected)\n", "def test_grover_iterations():\n    numqubits = 3\n    basis_states = superposition_basis(numqubits)\n    oracle_function = lambda qubits: qubits == IntQubit(3, numqubits)\n    v = OracleGate(numqubits, oracle_function)\n    expected = (-(13*basis_states))/64 + 264*IntQubit(3, numqubits)/256\n    # Apply Grover algorithm for 5 iterations, and compare the result\n    # with the expected state\n    assert apply_grover(oracle_function, numqubits, iterations=5) == qapply(expected)\n"], "sample_133": ["    def test_i18n_with_custom_js_template(self):\n        with self.settings(LANGUAGE_CODE='en-us'), override('fr'):\n            response = self.client.get('/jsi18n_custom_template/')\n            self.assertContains(response, 'This is a test message from app6')\n", "    def test_i18n_context_data(self):\n        self.selenium.get(self.live_server_url + '/jsi18n_context_data/')\n\n        elem = self.selenium.find_element_by_id('formatted_string')\n        self.assertEqual(elem.text, 'App3 string with context data: 123')\n", "    def test_catalog_with_plural_rules(self):\n        with self.settings(LANGUAGE_CODE='fr'), override('fr'):\n            response = self.client.get('/jsi18n_app6/')\n            self.assertContains(response, '1 image')\n            self.assertContains(response, '2 images')\n            self.assertContains(response, '3 images')\n", "    def test_i18n_with_plural_forms(self):\n        self.selenium.get(self.live_server_url + '/jsi18n_plural_forms/')\n\n        elem = self.selenium.find_element_by_id(\"plural_one\")\n        self.assertEqual(elem.text, \"1 Element\")\n        elem = self.selenium.find_element_by_id(\"plural_few\")\n        self.assertEqual(elem.text, \"2 Elemente\")\n        elem = self.selenium.find_element_by_id(\"plural_many\")\n        self.assertEqual(elem.text, \"3 Elemente\")\n        elem = self.selenium.find_element_by_id(\"plural_other\")\n        self.assertEqual(elem.text, \"4 Elemente\")\n", "    def test_i18n_with_context(self):\n        self.selenium.get(self.live_server_url + '/jsi18n_context/')\n\n        elem = self.selenium.find_element_by_id('context_string')\n        self.assertEqual(elem.text, 'Bonjour, Jean Doe')\n\n\n\n"], "sample_284": ["    def test_file_change_after_manifest_update(self):\n        # Create initial static files.\n        file_contents = (\n            ('foo.png', 'foo'),\n            ('bar.css', 'url(\"foo.png\")\\nurl(\"xyz.png\")'),\n            ('xyz.png', 'xyz'),\n        )\n        for filename, content in file_contents:\n            with open(self._get_filename_path(filename), 'w') as f:\n                f.write(content)\n\n        with self.modify_settings(STATICFILES_DIRS={'append': self._temp_dir}):\n            finders.get_finder.cache_clear()\n            err = StringIO()\n            # First collectstatic run to create manifest.\n            call_command('collectstatic', interactive=False, verbosity=0, stderr=err)\n            relpath = self.hashed_file_path('test/bar.css')\n            with storage.staticfiles_storage.open(relpath) as relfile:\n                content = relfile.read()\n                self.assertIn(b'foo.acbd18db4cc2.png', content)\n                self.assertIn(b'xyz.d16fb36f0911.png', content)\n\n            # Manually update manifest without collectstatic (simulating a custom update)\n            manifest = storage.staticfiles_storage.load_manifest()\n            manifest['static/test/foo.png'] = 'foo.acbd18db4cc2.png'\n            storage.staticfiles_storage.save_manifest(manifest)\n\n            # Change the contents of the png files.\n            for filename in ('foo.png', 'xyz.png'):\n                with open(self._get_filename_path(filename), 'w+b') as f:\n                    f.write(b\"new content of file to change its hash\")\n\n            # Verify that the manifest contains the updated hash for foo.png\n            # even though collectstatic hasn't been called.\n            relpath = self.hashed_file_path('test/bar.css')\n            with storage.staticfiles_storage.open(relpath) as relfile:\n                content = relfile.read()\n                self.assertIn(b'", "    def test_manifest_creation_after_file_deletion(self):\n        relpath = self.hashed_file_path('test/bar.css')\n        with storage.staticfiles_storage.open(relpath) as relfile:\n            content = relfile.read()\n            self.assertIn(b'foo.acbd18db4cc2.png', content)\n            self.assertIn(b'xyz.d16fb36f0911.png', content)\n\n        os.remove(self._get_filename_path('test/foo.png'))\n\n        with self.modify_settings(STATICFILES_DIRS={'append': self._temp_dir}):\n            call_command('collectstatic', interactive=False, verbosity=0)\n        with storage.staticfiles_storage.open(relpath) as relfile:\n            content = relfile.read()\n            self.assertIn(b'xyz.d16fb36f0911.png', content)\n            self.assertNotIn(b'foo.acbd18db4cc2.png', content)\n\n\n", "    def test_empty_missing_files(self):\n        temp_path = self._get_filename_path('test_empty.css')\n        with open(temp_path, 'w') as f:\n            f.write('')\n\n        with self.modify_settings(STATICFILES_DIRS={'append': self._temp_dir}):\n            finders.get_finder.cache_clear()\n            err = StringIO()\n            call_command('collectstatic', interactive=False, verbosity=0, stderr=err)\n\n        relpath = self.hashed_file_path('test/test_empty.css')\n        with storage.staticfiles_storage.open(relpath) as relfile:\n            self.assertEqual(relfile.read(), b'')\n\n\n", "    def test_import_loop_without_conflict(self):\n        \"\"\"\n        Tests import loop behavior with non-conflicting file names.\n\n        See #19670.  This test case ensures that a circular import loop\n        does not cause unexpected behavior, ensuring that each file\n        is correctly hashed and referenced.\n        \"\"\"\n        # Create a directory for the test files\n        test_dir = tempfile.mkdtemp()\n        self.addCleanup(shutil.rmtree, test_dir)\n        os.makedirs(os.path.join(test_dir, 'test'))\n\n        file_paths = {\n            'loop1.css': \"\"\"\n                @import \"loop2.css\";\n            \"\"\",\n            'loop2.css': \"\"\"\n                @import \"loop1.css\";\n            \"\"\"\n        }\n\n        for filename, content in file_paths.items():\n            with open(os.path.join(test_dir, 'test', filename), 'w') as f:\n                f.write(content)\n\n        with self.modify_settings(STATICFILES_DIRS={'append': test_dir}):\n            finders.get_finder.cache_clear()\n            err = StringIO()\n            call_command('collectstatic', interactive=False, verbosity=0, stderr=err)\n\n            # Assert that both files were processed and hashed\n            for filename in file_paths:\n                relpath = self.hashed_file_path(f\"test/{filename}\")\n                with storage.staticfiles_storage.open(relpath) as relfile:\n                    content = relfile.read()\n                    self.assertIn(\n                        f'loop{1 if filename == \"loop1.css\" else 2}.{hashlib.sha256(content.encode()).hexdigest()}.css',\n                        content\n                    )\n\n\n\n", "    def test_no_storage(self):\n        with self.assertRaises(RuntimeError):\n            self.hashed_file_path('test/file.txt')\n\n"], "sample_707": ["def test_getfslineno(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n            pass\n    \"\"\"\n    )\n    item = pytester.getitems()[-1]\n    assert item.location[1] is not None\n    item.location = (None, None, \"\")\n    assert item.location[1] is None\n\n\n\n\n", "def test_repr_failure_with_filepaths(pytester: Pytester) -> None:\n    p = pytester.makepyfile(\n        \"\"\"\n        import os\n\n            os.chdir(private_dir)\n            assert os.getcwd() == private_dir\n\n            assert os.getcwd() == os.path.dirname(__file__)\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            str(p) + \":*: AssertionError\",\n            str(p) + \":*: AssertionError\",\n            \"*2 failed in *\",\n        ]\n    )\n    # check that the output filenames are correct\n    for line in result.stdout.str().splitlines():\n        if \"AssertionError\" in line:\n            assert any(\n                f\"({p.parent / 'test_wrong_path_1.py'})\" in line\n                or f\"({p.parent / 'test_wrong_path_2.py'})\" in line\n            )\n", "def test_fscollector_gethookproxy_isinitpath(pytester: Pytester) -> None:\n\n    with pytester.raises(DeprecationWarning, match=\"gethookproxy.*should\"):\n        p = pytester.makepyfile(\n            \"\"\"\n            import pytest\n\n            class MyCollector(pytest.collector.FSCollector):\n                    return super().gethookproxy(fspath)\n            @pytest.fixture\n                return MyCollector(path=pytester.path)\n\n                pass\n            \"\"\"\n        )\n        pytester.runpytest_subprocess(\n            \"-c\", p,\n            extra_args=[\"--collect-only\"]\n        )\n\n", "def test_node_repr_failure_long_traceback(pytester: Pytester) -> None:\n    \"\"\"\n    Test that the repr_failure method, when 'tbstyle' is 'long', respects the\n    fulltrace option from pytest.ini.\n    \"\"\"\n    pytester.writepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.parametrize(\"x\", [1, 2, 3])\n            assert x == 4\n    \"\"\"\n    )\n    result = pytester.runpytest(\n        \"-v\", \"--tb=long\", \"--fulltrace\"\n    )\n    result.assert_outcomes(passed=0)\n    result.stdout.fnmatch_lines(\n        [\n            \"*1 failed in *\",\n            f\"  *assert x == 4*\",\n            \"*long traceback*\",\n        ]\n    )\n\n\n", "def test_node_path_from_parent_handles_relative_paths() -> None:\n    with pytest.raises(ValueError):\n        nodes.Node.from_parent(None, fspath=legacy_path(\"..\"), path=None)  # type: ignore[arg-type]"], "sample_1077": ["def test_complex_region_from_real_with_intervals():\n    a, b = Interval(2, 5), Interval(4, 8)\n    c1 = ComplexRegion.from_real(a * b)\n    assert c1 == ComplexRegion(a*b)\n    c2 = ComplexRegion.from_real(Interval(2, 5) * Interval(4, 8))\n    assert c2 == ComplexRegion(a*b)\n    c3 = ComplexRegion.from_real(Interval(2, 5))\n    assert c3 == ComplexRegion(Interval(2, 5) * FiniteSet(0))\n", "compilation error", "def test_issue_13166():\n    a = Interval(0, 1)\n    b = Interval(-1, 0)\n    c = ComplexRegion(a * b)\n    assert c.intersect(S.Reals) == a\n    assert c.intersect(S.Im) == b\n    assert c.intersect(S.Complexes) == c\n    assert c.intersect(Interval(-2, 2)) == a\n    assert c.intersect(Interval(-1, 1)) == a\n    assert c.union(S.Reals) == S.Complexes\n", "def test_issue_13149():\n    c1 = ComplexRegion(Interval(-oo, oo)*Interval(0, pi))\n    c2 = ComplexRegion(Interval(-oo, 0)*Interval(pi, 2*pi))\n    assert c1.intersect(c2) == ImageSet(\n        Lambda(n, 2*n*pi + pi), S.Integers)\n\n\n", "def test_issue_13058():\n    a, b = symbols('a b', real=True)\n    c = ComplexRegion(Interval(a, b)*Interval(0, 2*pi), polar=True)\n    assert c.intersect(S.Reals) == Interval(a, b)\n"], "sample_468": ["    def test_push_context_manager_with_context_processor(self):\n        @contextmanager\n            with mock.patch.object(request, \"GET\", {\"foo\": \"bar\"}) as mock_get:\n                yield {\"foo\": \"baz\"}\n\n        request = self.request_factory.get(\"/\")\n        engine = Engine(\n            loaders=[\n                (\n                    \"django.template.loaders.locmem.Loader\",\n                    {\n                        \"foo_template\": \"<h1>{{ foo }}</h1>\",\n                    },\n                )\n            ]\n        )\n        template = engine.from_string(\"foo_template\")\n        ctx = RequestContext(request, {})\n        with ctx.bind_template(template) as bound_ctx:\n            with mocked_processor(request):\n                self.assertEqual(template.render(bound_ctx), \"<h1>baz</h1>\")\n\n\n\n", "    def test_context_processors_are_called_with_request(self):\n        request = self.request_factory.get(\"/\")\n        mock_processor = mock.Mock(return_value={\"foo\": \"bar\"})\n        with override_settings(\n            TEMPLATES=[\n                {\n                    \"BACKEND\": \"django.template.backends.django.DjangoTemplates\",\n                    \"OPTIONS\": {\n                        \"context_processors\": [mock_processor],\n                    },\n                }\n            ]\n        ):\n            ctx = RequestContext(request, {})\n            with ctx.bind_template(Template(\"\")):\n                pass\n            mock_processor.assert_called_once_with(request)\n", "    def test_context_pop_exception(self):\n        with self.assertRaises(ContextPopException):\n            # Try to pop from an empty context.\n            Context().pop() \n\n\n", "    def test_pop_exception(self):\n        ctx = Context()\n        with self.assertRaises(ContextPopException):\n            ctx.pop()\n", "    def test_context_processors_can_add_or_update_data(self):\n            return {\"added_key\": \"added_value\"}\n\n            return {\"updated_key\": \"updated_value\"}\n\n        request = self.request_factory.get(\"/\")\n        request.data = {\"key_to_update\": \"update_this\"}\n\n        @override_settings(\n            TEMPLATES=[\n                {\n                    \"BACKEND\": \"django.template.backends.django.DjangoTemplates\",\n                    \"OPTIONS\": {\n                        \"context_processors\": [\n                            adding_processor,\n                            updating_processor,\n                        ],\n                    },\n                }\n            ]\n        )\n        self.assertEqual(\n            RequestContext(request).dicts[0],\n            {\n                \"added_key\": \"added_value\",\n                \"updated_key\": \"updated_value\",\n                \"key_to_update\": \"update_this\",\n            },\n        )\n"], "sample_542": ["def test_bbox_clip_rotated():\n    fig, ax = plt.subplots()\n    ax.set_ylim(-1, 1)\n    ax.set_xlim(-1, 1)\n    t = ax.text(0, 0, 'foo', rotation=45, clip_on=True, transform=ax.transAxes)\n    fig.canvas.draw()\n\n\n\n", "def test_vertical_alignment():\n    fig, ax = plt.subplots(1, 2)\n    ax[0].text(0.5, 0.5, 'top', ha='center', va='top')\n    ax[1].text(0.5, 0.5, 'bottom', ha='center', va='bottom')\n    fig.canvas.draw()\n", "def test_text_position_with_bbox():\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, \"Test\", bbox=dict(facecolor='red', alpha=0.5))\n    width, height = text.get_window_extent().width, text.get_window_extent().height\n    assert width > 0\n    assert height > 0\n", "def test_text_clip_to_bbox(fig_test, fig_ref):\n    fig_test.patch.set_facecolor(\"lightblue\")\n    ax = fig_test.add_subplot()\n    ax.set_xlim(0, 2)\n    ax.set_ylim(0, 2)\n    ax.text(1.5, 1.5, '123456789',\n            ha='center', va='center',\n            bbox=dict(boxstyle=\"round, pad=0.1\", facecolor='red'))\n    fig_test.tight_layout()\n    fig_ref.patch.set_facecolor(\"lightblue\")\n    ax = fig_ref.add_subplot()\n    ax.set_xlim(0, 2)\n    ax.set_ylim(0, 2)\n    ax.text(1.5, 1.5, '123456789',\n            ha='center', va='center',\n            bbox=dict(boxstyle=\"round, pad=0.1\", facecolor='red'))\n    fig_ref.tight_layout()\n\n\n", "def test_mathtext_in_text_without_usetex():\n    with pytest.raises(ValueError, match='Cannot use mathtext with '\n                       'text.parse_math=False'):\n        fig, ax = plt.subplots()\n        ax.text(0.5, 0.5, r\"$foo$\\n$bar$\", parse_math=False, usetex=True)\n    fig, ax = plt.subplots()\n    ax.text(0.5, 0.5, r\"$foo$\\n$bar$\", parse_math=True, usetex=True)\n    fig.canvas.draw()\n"], "sample_904": ["def test_table_of_contents(app):\n    text = (\".. contents::\\n\"\n            \"\\n\"\n            \"   Foo\\n\"\n            \"\\n\"\n            \"   Bar\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.contents,\n                          [\n                              addnodes.topic(\n                                  ids=['foo'],\n                                  entries=[\n                                      addnodes.section(\n                                          ids=['foo'], text='Foo'\n                                      )\n                                  ]\n                              ),\n                              addnodes.topic(\n                                  ids=['bar'],\n                                  entries=[\n                                      addnodes.section(\n                                          ids=['bar'], text='Bar'\n                                      )\n                                  ]\n                              )\n                          ])\n\n\n\n", "    def test_inline_target_with_context(app):\n        text = \"blah _`inline target` blah\\n\\nAnother line\"\n        restructuredtext.parse(app, text)\n\n        domain = app.env.get_domain(\"std\")\n        assert 'inline target' in domain.labels\n        assert domain.labels['inline target'] == ('index', 'inline-target', 'inline target')\n", "def test_anonymous_labeled_rubric(app):\n    text = (\".. rubric:: blah *blah* blah\\n\"\n            \".. _label2:\\n\")\n    restructuredtext.parse(app, text)\n\n    domain = app.env.get_domain(\"std\")\n    assert 'label2' in domain.labels\n    assert domain.labels['label2'] == ('index', 'label2', '')\n", "def test_duplicate_target_warning(app, status, warning):\n    text = (\".. _my_target:\\n\"\n            \".. _my_target:\\n\"\n            \"My content\\n\")\n    restructuredtext.parse(app, text)\n    assert (\"duplicate target 'my_target' found\" in warning.getvalue())\n", "def test_reference_with_parentheses(app):\n    text = (\".. option:: -l(show listings)\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"-l\"],\n                                                    [desc_addname, \"(show listings)\"]\n                )]),\n                [desc_content, ()])]))\n"], "sample_255": ["    def test_close_connection(self):\n        request = WSGIRequest(self.request_factory.get('/').environ)\n        client_address = ('192.168.2.0', 8080)\n        server = WSGIServer(('localhost', 0), WSGIRequestHandler)\n        handler = server._handle_request(request, client_address)\n        self.assertTrue(handler.close_connection)\n        server.server_close()\n\n", "    def test_reuse_address(self):\n        with self.subTest(reuse_address=True):\n            server1 = WSGIServer(('localhost', 0), WSGIRequestHandler, allow_reuse_address=True)\n            server1.serve_forever()\n        with self.subTest(reuse_address=False):\n            server2 = WSGIServer(('localhost', 0), WSGIRequestHandler, allow_reuse_address=False)\n            server2.serve_forever()\n", "    def test_close_connection(self):\n        request = WSGIRequest(self.request_factory.get('/').environ)\n        client_address = ('192.168.2.0', 8080)\n        server = WSGIServer(('localhost', 0), WSGIRequestHandler)\n\n        with self.assertLogs('django.server', 'INFO'):\n            server.handle_error(request, client_address)\n        server.server_close()\n", "    def test_connection_close(self):\n        request = WSGIRequest(self.request_factory.get('/').environ)\n        client_address = ('192.168.2.0', 8080)\n        server = WSGIServer(('localhost', 0), WSGIRequestHandler)\n        handler = server.get_request_handler_class()(request, client_address, server)\n        # Set the connection to close\n        handler.close_connection = True\n\n        with self.subTest(close_connection=True):\n            handler.handle_one_request()\n        with self.subTest(close_connection=False):\n            # Reset the connection flag and simulate a new request\n            handler.close_connection = False\n            handler.handle_one_request()\n\n\n\n", "    def test_handle_error_other_errors(self):\n        \"\"\"WSGIServer logs other errors besides broken pipes.\"\"\"\n        request = WSGIRequest(self.request_factory.get('/').environ)\n        client_address = ('192.168.2.0', 8080)\n        msg = f'- Error handling request from {client_address}'\n        with self.assertLogs('django.server', 'ERROR') as cm:\n            server = WSGIServer(('localhost', 0), WSGIRequestHandler)\n            try:\n                raise ValueError(\"An error occurred in the request handling\")\n            except Exception:\n                server.handle_error(request, client_address)\n            finally:\n                server.server_close()\n        self.assertEqual(cm.records[0].getMessage(), msg)\n"], "sample_1203": ["def test_block_homomorphism():\n    from sympy.combinatorics import Permutation\n    from sympy.combinatorics.perm_groups import PermutationGroup\n\n    G = PermutationGroup([Permutation(0,1,2), Permutation(1,2,3)])\n\n    blocks = [0, 1, 2]\n    H = block_homomorphism(G, blocks)\n\n    assert H.codomain == PermutationGroup([0, 1, 2])\n    assert H.image().order() == 3\n\n\n\n", "compilation error", "def test_block_homomorphism():\n    from sympy.combinatorics import Permutation\n    from sympy.combinatorics.perm_groups import PermutationGroup\n    G = PermutationGroup([Permutation(0, 1, 2), Permutation(1, 2, 0)])\n    blocks = [0, 1, 2]\n    H = block_homomorphism(G, blocks)\n    assert H.domain == G\n    assert H.codomain == PermutationGroup([Permutation(0, 1), Permutation(1, 2)])\n    assert H(Permutation(0, 1, 2)) == Permutation(0, 1)\n    assert H(Permutation(1, 2, 0)) == Permutation(1, 2)\n\n    blocks = [[0, 1], [2]]\n    H = block_homomorphism(G, blocks)\n    assert H.domain == G\n    assert H.codomain == PermutationGroup([Permutation(0, 1), Permutation(2)])\n    assert H(Permutation(0, 1, 2)) == Permutation(0, 1)\n    assert H(Permutation(1, 2, 0)) == Permutation(2)\n\n", "def test_block_homomorphism():\n    from sympy.combinatorics import Permutation\n    from sympy.combinatorics.perm_groups import PermutationGroup\n    from sympy.combinatorics.homomorphisms import block_homomorphism\n    G = PermutationGroup([Permutation(0,1,2),Permutation(1,2,0)])\n    blocks = [0, 1, 2]\n    H = block_homomorphism(G, blocks)\n    assert H.domain == G\n    assert H.codomain.order == 3\n    assert H(G.generators[0]) == Permutation([0, 1, 2])\n    assert H(G.generators[1]) == Permutation([1, 2, 0])\n", "compilation error"], "sample_331": ["    def test_invalid_iso_8601(self):\n        invalid_values = (\n            'P4Y1M2D',\n            'P4Y1M2D3H',\n            'P4Y1M2D3H1M',\n            'P4Y1M2D3H1M2S.',\n            'P4Y1M2D3H1M2S.1',\n            'P4Y1M2D3H1M2S123',\n            'P4Y1M2D3H1M2S.123456789',\n            'P4Y1M2D3H1M2S-123',\n            'P4Y1M2D3H1M2S+123',\n            'P4Y1M2D3H1M2S-123,456',\n            'P4Y1M2D3H1M2S+123,456',\n        )\n        for source in invalid_values:\n            with self.subTest(source=source):\n                self.assertIsNone(parse_duration(source))\n", "    def test_non_standard_duration_input(self):\n        self.assertIsNone(parse_duration('abc'))\n        self.assertIsNone(parse_duration('4 days, 10 hours'))\n        self.assertIsNone(parse_duration('1 day, 1 hour, 1 minute'))\n        self.assertIsNone(parse_duration('1 day 10:15:30,000'))\n\n\n\n", "    def test_invalid_iso_8601(self):\n        invalid_inputs = (\n            'Pabc',\n            'P4YZ',\n            'PT10H:15M:30S',\n            'P4D,15:30:00',\n            'P4D +15:30:00',\n        )\n        for input in invalid_inputs:\n            with self.subTest(input=input):\n                self.assertIsNone(parse_duration(input))\n", "    def test_timezone(self):\n        valid_inputs = (\n            ('2012-04-23T09:15:00+02:00', datetime(2012, 4, 23, 9, 15, 0, 0, get_fixed_timezone(120))),\n            ('2012-04-23T10:20:30-02:30', datetime(2012, 4, 23, 10, 20, 30, 0, get_fixed_timezone(-150))),\n            ('2012-04-23T10:20:30Z', datetime(2012, 4, 23, 10, 20, 30, 0, get_fixed_timezone(0))),\n        )\n        for source, expected in valid_inputs:\n            with self.subTest(source=source):\n                self.assertEqual(parse_datetime(source), expected)\n\n", "    def test_invalid_iso_8601(self):\n        invalid_inputs = (\n            'P4Y2M',  \n            'P4Y2M3D',\n            'P4Y2M3D1H1M',  \n            'P4Y2M3D1H1M1S1',\n            'PT5H2M',\n            'PT5H2M1S',  \n            'P4Y2M3DPT5H2M',\n            'P4Y2M3DPT5H2M1S111',\n            'P4Y2M1DPT',\n            'P4Y2M1DPT5H2M1S',\n            'P4Y2M1DPT5H2M1S.',\n            'P4Y2M1D',\n            'PT5H2M1S',\n            'P4Y2M3DA',\n            'PT5',\n            'PT5H2',\n        )\n        for source in invalid_inputs:\n            with self.subTest(source=source):\n                self.assertIsNone(parse_duration(source))\n"], "sample_269": ["    def test_i18n_with_catalog_missing_default_language(self):\n        with self.settings(LANGUAGE_CODE='fr'), override('en'):\n            response = self.client.get('/jsi18n_app6/')\n            self.assertContains(response, 'Fallback message')\n", "    def test_i18n_domain_specific_translations(self):\n        with self.settings(LANGUAGE_CODE='en-us'), override('de'):\n            response = self.client.get('/jsi18n_domain_specific/')\n            self.assertContains(response, 'This is a translation from domain1')\n            self.assertContains(response, 'This is a translation from domain2')\n", "    def test_i18n_with_empty_translation(self):\n        with self.settings(LANGUAGE_CODE='fr'):\n            self.selenium.get(self.live_server_url + '/jsi18n_empty_translation/')\n        elem = self.selenium.find_element_by_id('empty_translation')\n        self.assertEqual(elem.text, 'missing_translation')\n\n", "    def test_i18n_with_default_domain(self):\n        self.selenium.get(self.live_server_url + '/jsi18n_default_domain/')\n        elem = self.selenium.find_element_by_id('default_domain_string')\n        self.assertEqual(elem.text, 'Texto con dominio vac\u00edo del archivo')\n", "    def test_i18n_with_gettext_lazy(self):\n        self.selenium.get(self.live_server_url + '/jsi18n_lazy_gettext/')\n        elem = self.selenium.find_element_by_id(\"lazy_gettext\")\n        self.assertEqual(elem.text, \"Entfernen\")\n\n\n"], "sample_779": ["def test_check_get_params_invariance():\n    class InvariantEstimator(BaseEstimator):\n            self.a = a\n            self.b = b\n\n            return self\n\n            return {'a': self.a, 'b': self.b}\n\n    check_get_params_invariance('estimator_name', InvariantEstimator())\n\n\n", "def test_check_classifiers_regression_target():\n    # check for exceptions raised when fitting classifiers on regression\n    # targets.\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.datasets import load_boston\n\n    boston = load_boston()\n    X, y = boston.data, boston.target\n\n    clf = LogisticRegression()\n    assert_raises_regex(ValueError,\n                        \"Classifier does not support real-valued target\"\n                        \" during fit.\",\n                        check_classifiers_regression_target, clf, X, y)\n\n", "        def fit(self, X, y):\n            X, y = check_X_y(X, y, accept_sparse=(\"csr\", \"csc\"),\n                             accept_large_sparse=True)\n            if X.getformat() == 'coo':\n                if X.row.dtype == 'int64' or X.col.dtype == 'int64':\n                    raise ValueError(\n                        \"Estimator doesn't support 64-bit indices\")\n            return self\n\n", "def test_cross_decomposition_methods():\n    estimators = [\n        _safe_tags([\"cross_val_score\", \"cross_validate\"], \"no_validation\"),\n        _safe_tags([\"cross_decomposition\", \"cross_validate\"], \"no_validation\"),\n        _safe_tags([\"cross_decomposition\", \"cross_validate\"], \"no_validation\"),\n    ]\n    for name in estimators:\n        _safe_tags(name, \"no_validation\")\n        with ignore_warnings(category=(FutureWarning, DeprecationWarning)):\n            estimator = clone(LinearRegression)\n            estimator.fit(None, None)\n            # Test that methods like cv_score do work with estimator\n\n\n", "def test_check_predict_proba_consistent(   ):\n    centers = [(2, 2), (4, 4)]\n    X, y = make_blobs(n_samples=100, random_state=0, n_features=4,\n                      centers=centers, cluster_std=1.0, shuffle=True)\n    X_test = np.random.randn(20, 2) + 4\n    estimator = clone(GaussianMixture)\n    estimator.fit(X)\n    a = estimator.predict_proba(X_test)\n    b = estimator.predict_proba(X_test)\n    assert_allclose_dense_sparse(a, b)  \n\n\n\n"], "sample_378": ["    def test_bulk_update_with_prefetch_related(self):\n        notes = [\n            Note.objects.create(note=str(i), misc=str(i))\n            for i in range(10)\n        ]\n        for note in notes:\n            note.tag = self.tags[i % 10]\n        Note.objects.bulk_create(notes)\n\n        with self.assertNumQueries(1):\n            notes = Note.objects.prefetch_related('tag').all()\n            Note.objects.bulk_update(notes, ['note'])\n        self.assertEqual(notes[0].note, 'test-0') \n        self.assertEqual(notes[0].tag.name, '0')\n\n\n\n", "    def test_related_object_update(self):\n        related_objects = [RelatedObject.objects.create(name='test-related-%s' % i) for i in range(10)]\n        for related_object in related_objects:\n            related_object.single = SingleObject(name='test-single-%s' % related_object.id)\n            related_object.single.save()\n        related_objects_updated = [\n            RelatedObject(name='updated-%s' % i, single=related_objects[i].single)\n            for i in range(10)\n        ]\n        RelatedObject.objects.bulk_update(related_objects_updated, ['name'])\n        self.assertCountEqual(\n            RelatedObject.objects.values_list('name', flat=True),\n            ['updated-%s' % i for i in range(10)]\n        )\n        self.assertCountEqual(\n            RelatedObject.objects.filter(single__isnull=False).values_list('single__name', flat=True),\n            ['test-single-%s' % i for i in range(10)]\n        )\n", "    def test_multiple_pk_fields(self):\n        with self.assertRaisesMessage(ValueError, self.pk_fields_error):\n            CustomPk.objects.bulk_update([], fields=['name', 'extra'])\n\n", "    def test_related_object_update_no_pk(self):\n        parent = RelatedObject.objects.create()\n        child = SingleObject()\n        parent.single = child\n        parent.save()\n        parent.single = None\n        RelatedObject.objects.bulk_update([parent], fields=['single'])\n        self.assertEqual(parent.single, None)\n", "    def test_json_field_update(self):\n        article = Article.objects.create(name='test', json_field={'a': 1})\n        update_data = {'b': 2}\n        Article.objects.bulk_update([article], ['json_field'], update_data=update_data)\n        article.refresh_from_db()\n        self.assertEqual(article.json_field, {'a': 1, 'b': 2})\n\n\n\n"], "sample_134": ["    def test_serialize_custom_field(self):\n        class CustomField(models.Field):\n                super().__init__(**kwargs)\n\n                return 'CUSTOM_FIELD'\n\n                return value\n\n                return value\n\n                return 'CharField'\n\n        model = models.Model(\n            custom_field=CustomField(max_length=50)\n        )\n        string, imports = MigrationWriter.serialize(model)\n        self.assertEqual(string, \"custom_migration_operations.operations.CustomField(max_length=50)\")\n        self.assertEqual(imports, {'import custom_migration_operations.operations'})\n\n\n\n", "    def test_serialize_related_fields(self):\n        class Author(models.Model):\n            name = models.CharField(max_length=255)\n\n        class Book(models.Model):\n            title = models.CharField(max_length=255)\n            author = models.ForeignKey(Author, on_delete=models.CASCADE)\n\n        migration = type(\"Migration\", (migrations.Migration,), {\n            \"operations\": [\n                migrations.CreateModel(\"Author\", (\n                    (\"name\", models.CharField(max_length=255)),\n                )),\n                migrations.CreateModel(\"Book\", (\n                    (\"title\", models.CharField(max_length=255)),\n                    (\"author\", models.ForeignKey(\n                        \"Author\", on_delete=models.CASCADE\n                    )),\n                )),\n            ],\n            \"dependencies\": []\n        })\n        writer = MigrationWriter(migration)\n        output = writer.as_string()\n        string, _ = MigrationWriter.serialize(Book.author)\n        self.assertEqual(string, \"migrations.test_writer.Book.Author\")\n\n        self.assertIn('migrations.test_writer.Author', output)\n        self.assertIn('migrations.test_writer.Book.author', output)\n\n\n\n", "    def test_serialize_custom_operation_with_kwargs(self):\n        class CustomOperation(migrations.MigrationOperation):\n                super().__init__()\n                self.kwarg1 = kwarg1\n                self.kwarg2 = kwarg2\n\n        migration = type(\"Migration\", (migrations.Migration,), {\n            \"operations\": [\n                CustomOperation(kwarg1='value1', kwarg2='value2')\n            ]\n        })\n        writer = MigrationWriter(migration)\n        output = writer.as_string()\n        self.assertIn('CustomOperation(kwarg1=\\'value1\\', kwarg2=\\'value2\\')', output)\n", "    def test_serialize_custom_operations(self):\n        class CustomOperation(migrations.MigrationOperation):\n                super().__init__()\n                self.value = value\n\n                return (\n                    'migrations.test_writer.CustomOperation',\n                    [],\n                    {'value': self.value},\n                    None,\n                )\n\n        migration = type(\"Migration\", (migrations.Migration,), {\n            \"operations\": [CustomOperation('some data')]\n        })\n        writer = MigrationWriter(migration)\n        output = writer.as_string()\n        self.assertIn('migrations.test_writer.CustomOperation', output)\n        self.assertIn('value', output)\n", "    def test_model_inheritance(self):\n        class ParentModel(models.Model):\n            pass\n\n        class ChildModel(ParentModel):\n            field = models.CharField(max_length=100)\n\n        migration = type(\"Migration\", (migrations.Migration,), {\n            \"operations\": [\n                migrations.CreateModel(name=\"ChildModel\", fields=[(\"field\", models.CharField(max_length=100))],\n                bases=(ParentModel,), options={}),\n            ],\n            \"dependencies\": [],\n        })\n        writer = MigrationWriter(migration)\n        output = writer.as_string()\n        self.assertIn(\"bases=(ParentModel,)\", output)\n"], "sample_211": ["    def test_no_object_in_context_data(self):\n        test_view = views.CustomSingleObjectView()\n        with self.assertRaises(ValueError):\n            test_view.get_context_data()\n", "    def test_single_object_template_response(self):\n        \"\"\"\n        Test a view that renders a template with a single object in context.\n        \"\"\"\n        response = self.client.get('/template/single_object/')\n        self.assertEqual(response.status_code, 200)\n\n\n", "    def test_error_handling_in_redirect_view(self):\n        view = RedirectView.as_view(url='/invalid/')\n        response = view(self.rf.get('/'))\n        self.assertEqual(response.status_code, 500)\n\n", "    def test_no_object_error(self):\n        view = views.CustomSingleObjectView()\n        msg = (\n            \"object attribute hasn't been set. You should either set it \"\n            \"in the view or provide it through the object attribute in \"\n            \"'get_context_data()'\"\n        )\n        with self.assertRaisesMessage(AttributeError, msg):\n            view.get_context_data()\n", "    def test_direct_instantiation_of_template_mixins(self):\n        \"\"\"\n        It should be possible to use SingleObjectTemplateResponseMixin\n        directly without going through .as_view() (#21564).\n        \"\"\"\n        view = views.SingleObjectTemplateResponseMixinView()\n        response = view.dispatch(self.rf.get('/'))\n        self.assertEqual(response.status_code, 200)\n"], "sample_1001": ["def test_issue_15329():\n    from sympy import symbols, sin\n    x, y = symbols('x y')\n    assert latex(sin(x + y)) == r'\\sin{\\left (x + y \\right )}'\n", "def test_latex_complex():\n    z = complex(2, 3)\n    assert latex(z) == r\"2 + 3 i\"\n    assert latex(z**2) == r\"\\left(2 + 3 i\\right)^{2}\"\n    assert latex(sqrt(z)) == r\"\\sqrt{2 + 3 i}\"\n", "def test_issue_15033():\n    a, b = symbols('a b')\n    expr = 1/a/b\n    assert latex(expr) == r\"\\frac{1}{a b}\"\n\n\n", "def test_issue_15137():\n    from sympy.polys import polynomial_ring\n    from sympy.geometry.point import Point\n    R = polynomial_ring(symbols('x y'))\n    P = Point(R.var(0), R.var(1))\n    assert latex(P) == r'P'\n    assert latex(P.x) == r'x'\n    assert latex(P.y) == r'y'\n", "def test_latex_sympy_printing():\n    from sympy import sqrt, exp, sin, cos, tan, log\n\n    assert latex(sqrt(2)) == r'\\sqrt{2}'\n    assert latex(exp(x)) == r'e^{x}'\n    assert latex(sin(x)) == r'\\sin{\\left (x \\right )}'\n    assert latex(cos(x)) == r'\\cos{\\left (x \\right )}'\n    assert latex(tan(x)) == r'\\tan{\\left (x \\right )}'\n    assert latex(log(x)) == r'\\log{\\left (x \\right )}'\n"], "sample_500": ["def test_colorbar_fraction_shrink_pad_args():\n    fig, ax = plt.subplots()\n    im = ax.imshow(np.random.rand(10, 10), cmap='viridis')\n    cb = fig.colorbar(im, ax=ax, fraction=0.05, shrink=0.8, pad=0.05)\n\n    # check if the default values are used for fraction, shrink, and pad\n    assert cb.ax.get_position().height == 0.05\n    assert cb.ax.get_position().width == 0.9\n    assert cb.ax.get_position().x == 0.1\n    assert cb.ax.get_position().y == 0.9\n\n\n", "def test_colorbar_ticks_with_explicit_bounds():\n    fig, ax = plt.subplots()\n    im = ax.imshow([[1, 2], [3, 4]])\n    norm = Normalize(vmin=0, vmax=5)\n    cb = fig.colorbar(im, norm=norm, ticks=[1, 2.5, 4])\n    np.testing.assert_equal(cb.ax.get_yticks(), [1, 2.5, 4])\n\n    cb = fig.colorbar(im, norm=norm, ticks=[1, 3, 5])\n    np.testing.assert_equal(cb.ax.get_yticks(), [1, 3, 5])\n\n    # test that it is possible to set the tick locations even when\n    # they are outside the bounds of the data.\n    cb = fig.colorbar(im, norm=norm, ticks=[0, 6])\n    np.testing.assert_equal(cb.ax.get_yticks(), [0, 6])\n", "compilation error", "def test_colorbar_set_ticks_from_data_ranges():\n    fig, ax = plt.subplots()\n    x = np.arange(-3.0, 4.001)\n    y = np.arange(-4.0, 3.001)\n    X, Y = np.meshgrid(x, y)\n    Z = X * Y\n    Z = Z[:-1, :-1]\n    pcm = ax.pcolormesh(X, Y, Z)\n\n    # Test setting ticks from data ranges\n    ticks = plt.cm.ScalarMappable(norm=pcm.norm).to_rgba(\n        pcm.norm.vmin, pcm.norm.vmax)\n    cb = fig.colorbar(pcm, ax=ax, ticks=ticks)\n\n\n\n", "def test_colorbar_fraction_padding_shrink():\n    fig, ax = plt.subplots()\n    pc = ax.imshow(np.arange(100).reshape(10, 10))\n    cb = fig.colorbar(pc, ax=ax, fraction=0.1, pad=0.05, shrink=0.8)\n    assert cb.ax.get_position().height == 0.1\n    assert cb.ax.get_position().y == 0.905\n    assert cb.ax.get_position().width == 0.9 * 0.1\n    assert cb.ax.get_position().x == 0.05\n\n\n\n"], "sample_828": ["def test_pairwise_distances_data_derived_params_n_jobs_none():\n    with config_context(working_memory=1):\n       rng = np.random.RandomState(0)\n       X = rng.random_sample((1000, 10))\n       Y = rng.random_sample((1000, 10))\n       metric = \"seuclidean\"\n       params = {'V': np.var(np.vstack([X, Y]), axis=0, ddof=1)}\n\n       dist_chunked = pairwise_distances_chunked(X, Y, metric=metric,\n                                               n_jobs=None, **params)\n       dist_default = pairwise_distances(X, Y, metric=metric, **params)\n       assert_allclose(dist_chunked, dist_default) \n\n\n", "def test_euclidean_distances_non_numeric_input():\n    # check that euclidean_distances raises an error for non-numeric input\n\n    #  Data should be a NumPy array of numbers\n    X = [[1, 2], [3, 4]]\n    Y = [['a', 'b'], ['c', 'd']]\n    assert_raises(ValueError, euclidean_distances, X, Y)\n    assert_raises(ValueError, euclidean_distances, X, [Y])\n\n", "def test_check_sparse_matrices_different_shapes():\n    # Ensures that check_pairwise_arrays returns a ValueError if sparse matrices\n    # have different shapes.\n    XA = csr_matrix([[1, 2], [3, 4], [5, 6]])\n    XB = csr_matrix([[7, 8], [9, 10], [11, 12], [13, 14]])\n    with pytest.raises(ValueError):\n        check_pairwise_arrays(XA, XB)\n\n", "def test_check_sparse_arrays_different_shapes():\n    # Ensures an error is raised if the sparse matrices have different shapes.\n    rng = np.random.RandomState(0)\n    XA = rng.random_sample((5, 4)).astype(np.float32)\n    XA_sparse = csr_matrix(XA)\n    XB = rng.random_sample((4, 5)).astype(np.float32)\n    XB_sparse = csr_matrix(XB)\n    assert_raises(ValueError, check_pairwise_arrays, XA_sparse, XB_sparse)\n", "def test_check_sparse_matrices_different_shape():\n    # Ensure an error is raised if sparse matrices have different shapes.\n    rng = np.random.RandomState(0)\n    XA = csr_matrix(rng.random_sample((5, 4)))\n    XB = csr_matrix(rng.random_sample((4, 5)))\n    assert_raises(ValueError, check_pairwise_arrays, XA, XB)\n\n\n\n"], "sample_1067": ["def test_issue_6387():\n    x = Symbol('x')\n    y = Symbol('y')\n    a = Wild('a')\n    b = Wild('b')\n    c = Wild('c')\n\n    expr = x*y + a*x + b*y + c\n\n    assert expr.match(a*x + b*y + c) == {a: 1, b: 1, c: 0}\n\n\n\n", "compilation error", "compilation error", "def test_issue_5834():\n    x = Symbol('x')\n    y = Symbol('y')\n\n    f = x**2 + y**2\n    g = Wild('g')\n\n    assert f.match(g) == {g: x**2 + y**2}\n", "def test_issue_5244():\n    from sympy import log, symbols\n    x = symbols('x')\n    y = Wild('y')\n    a = Wild('a')\n\n    assert log(x*y).match(a * log(x) + log(y)) == {a: 1}\n    assert log(x**2).match(2*a*log(x)) == {a: 1}\n    assert log(x*y).match(a * log(x) + b*log(y)) == {a: 1, b: 1}\n\n\n\n"], "sample_875": ["def test_average_precision_score_empty_classes():\n    \"\"\"Check that average_precision_score handles empty classes correctly.\"\"\"\n    y_true = np.array([0, 1, 0, 1, 0, 1])\n    y_score = np.array([[0.1, 0.9], [0.2, 0.8], [0.3, 0.7], [0.4, 0.6], [0.5, 0.5], [0.6, 0.4]])\n    with pytest.raises(ValueError):\n        average_precision_score(y_true, y_score, average='samples', labels=[1, 2])\n\n\n", "def test_log_loss_multiclass_with_labels(\n    y_true, y_pred, labels, eps, normalize", "    def test_fbeta_score_labels(y_true, y_pred, labels):\n        fbeta = partial(fbeta_score, beta=2)\n        with ignore_warnings():\n            result_with_labels = fbeta(y_true, y_pred, labels=labels)\n        result_without_labels = fbeta(y_true, y_pred)\n        assert result_with_labels == result_without_labels\n", "compilation error", "def test_classification_metric_pos_label_invalid(pos_label):\n    y_true = [0, 1, 0, 0, 1]\n    y_pred = [0.0, 0.1, 0.2, 0.9, 0.8]\n    with pytest.raises(ValueError, match=\"pos_label .* must be in y_true\"):\n        f1_score(y_true, y_pred, pos_label=pos_label)\n\n"], "sample_943": ["compilation error", "def test_subpackage_in_toc_with_empty_init(tempdir):\n    \"\"\"Make sure that empty subpackages with non-empty subpackages in them\n       are not skipped (issue #4520)\n    \"\"\"\n    outdir = path(tempdir)\n    (outdir / 'parent').makedirs()\n    (outdir / 'parent' / '__init__.py').write_text('')\n    (outdir / 'parent' / 'child').makedirs()\n    (outdir / 'parent' / 'child' / 'example.py').write_text('')\n    apidoc_main(['-o', tempdir, tempdir / 'parent'])\n    assert (outdir / 'parent.rst').exists()\n\n    content = (outdir / 'parent.rst').read_text()\n    assert 'parent.child' in content\n    assert 'parent.child.example' in content\n\n\n\n", "    def test_custom_template(tempdir):\n        outdir = path(tempdir)\n        (outdir / 'testpkg').makedirs()\n        (outdir / 'testpkg' / '__init__.py').write_text('')\n        (outdir / 'testpkg' / 'example.py').write_text('')\n        with open(tempdir / 'custom_template.rst', 'w') as f:\n            f.write(\"This is a custom template for sphinx!\")\n        apidoc_main(['-o', tempdir, tempdir / 'testpkg',\n                     '--template', tempdir / 'custom_template.rst'])\n        assert (outdir / 'testpkg.rst').exists()\n\n        content = (outdir / 'testpkg.rst').read_text()\n        assert content == \"This is a custom template for sphinx!\"\n", "    def test_package_file_with_empty_namespace(tempdir):\n        outdir = path(tempdir)\n        (outdir / 'testpkg').makedirs()\n        (outdir / 'testpkg' / '__init__.py').write_text('')\n        apidoc_main(['--implicit-namespace', '-o', tempdir, tempdir / 'testpkg'])\n        assert (outdir / 'testpkg.rst').exists()\n\n        content = (outdir / 'testpkg.rst').read_text()\n        assert content == (\"testpkg namespace\\n\"\n                           \"=================\\n\"\n                           \"\\n\"\n                           \"Module contents\\n\"\n                           \"---------------\\n\"\n                           \"\\n\"\n                           \".. automodule:: testpkg\\n\"\n                           \"   :members:\\n\"\n                           \"   :undoc-members:\\n\"\n                           \"   :show-inheritance:\\n\")\n", "def test_custom_template(tempdir):\n    tempdir_path = path(tempdir)\n    template_path = tempdir_path / 'mytemplate.rst'\n    template_path.write_text('My Custom Template\\n\\n.. automodule:: :project:\\n   :members:\\n   :undoc-members:\\n   :show-inheritance:\\n')\n    apidoc_main(['-t', template_path.as_posix(), '-o', tempdir, tempdir_path / 'mypkg'])\n    assert (tempdir_path / 'mypkg.rst').exists()\n    content = (tempdir_path / 'mypkg.rst').read_text()\n    assert content == 'My Custom Template\\n\\n.. automodule:: mypkg:\\n   :members:\\n   :undoc-members:\\n   :show-inheritance:\\n'\n"], "sample_293": ["    def test_overlapping_lookahead_lookbehind(self):\n        with self.assertRaises(ImproperlyConfigured):\n            from django.urls import path\n            path('', lambda r: None, name='overlap'),\n            path('lookbehind/a/(?P<city>.*)', lambda r: None, name='overlap'),\n", "    def test_lookbehind_with_quantifier(self):\n        with self.subTest():\n            test_url = '/lookbehind/a{2,3}-city/'\n            self.assertEqual(resolve(test_url).kwargs, {'city': 'a2-city'})\n\n        with self.subTest():\n            test_url = '/lookbehind/a{2,3}-city/'\n            self.assertEqual(resolve(test_url).kwargs, {'city': 'a2-city'})\n\n        with self.subTest():\n            test_url = '/lookbehind/a{2,3}-city/'\n            self.assertEqual(resolve(test_url).kwargs, {'city': 'a3-city'})\n\n", "    def test_lookahead_with_capture(self):\n        test_urls = [\n            ('/lookahead_capture/(?P<city>\\w+)/', {'city': 'a-city'}, '/lookahead_capture/a-city/'),\n        ]\n        for pattern, kwargs, expected in test_urls:\n            with self.subTest(pattern=pattern):\n                self.assertEqual(reverse(pattern, kwargs=kwargs), expected)\n", "    def test_lookahead_and_lookbehind_in_same_regex(self):\n        test_urls = [\n            ('/lookahead-and-lookbehind/a-city', {'city': 'a-city'}),\n        ]\n        for test_url, kwargs in test_urls:\n            with self.subTest(url=test_url):\n                self.assertEqual(resolve(test_url).kwargs, kwargs)\n\n\n\n", "    def test_nested_lookahead(self):\n        test_urls = [\n            ('/nested-lookahead-positive', {'city': 'a-city', 'town': 'b-town'}, '/nested-lookahead/a-city/b-town/'),\n            ('/nested-lookahead-negative', {'city': 'a-city', 'town': 'not-a-town'}, '/nested-lookahead/a-city/not-a-town/'),\n        ]\n        for name, kwargs, expected in test_urls:\n            with self.subTest(name=name, kwargs=kwargs):\n                self.assertEqual(reverse(name, kwargs=kwargs), expected)\n"], "sample_49": ["    def test_merge_empty_media(self):\n        self.assertEqual(Media.merge([], []), [])\n        self.assertEqual(Media.merge([], [1, 2]), [1, 2])\n        self.assertEqual(Media.merge([1, 2], []), [1, 2])\n\n\n\n", "    def test_merge_css_media_queries(self):\n        w1 = Media(css={'screen and (min-width: 768px)': ['screen_large.css']})\n        w2 = Media(css={'screen and (max-width: 768px)': ['screen_small.css']})\n        w3 = Media(css={'all': ['default.css']})\n        merged = w1 + w2 + w3\n        self.assertEqual(merged._css, {'screen and (min-width: 768px)': ['screen_large.css'], \n                                        'screen and (max-width: 768px)': ['screen_small.css'], \n                                        'all': ['default.css']})\n", "    def test_media_inheritance_priority(self):\n        # If a widget extends another but has its own media definition, the child's\n        # media takes precedence\n        class MyWidget1(TextInput):\n            class Media:\n                css = {\n                    'all': ('path/to/css1', '/path/to/css2')\n                }\n                js = ('/path/to/js1', 'http://media.other.com/path/to/js2', 'https://secure.other.com/path/to/js3')\n\n        class MyWidget11(MyWidget1):\n            class Media:\n                css = {\n                    'all': ('/path/to/css3',)\n                }\n                js = ('/path/to/js4',)\n\n        w11 = MyWidget11()\n        self.assertEqual(\n            str(w11.media),\n            \"\"\"<link href=\"/path/to/css3\" type=\"text/css\" media=\"all\" rel=\"stylesheet\">", "    def test_media_inheritance_with_override(self):\n        # Inheritance with override should respect the inheritance pattern and only\n        # include the overridden items\n        class MyWidget1(TextInput):\n            class Media:\n                css = {\n                    'all': ('path/to/css1', '/path/to/css2')\n                }\n                js = ('/path/to/js1', 'http://media.other.com/path/to/js2', 'https://secure.other.com/path/to/js3')\n\n        class MyWidget11(MyWidget1):\n            class Media:\n                extend = True\n                css = {\n                    'screen': ('/path/to/css3',)\n                }\n\n        w11 = MyWidget11()\n        self.assertEqual(\n            str(w11.media),\n            \"\"\"<link href=\"http://media.example.com/static/path/to/css1\" type=\"text/css\" media=\"all\" rel=\"stylesheet\">", "    def test_merge_empty_media(self):\n        self.assertEqual(Media.merge([], [] ), [])\n        self.assertEqual(Media.merge([1], []), [1])\n        self.assertEqual(Media.merge([], [1]), [1])\n\n"], "sample_679": ["def test_parametrize_extra_keyword_markers(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.extra_keyword\n        @pytest.mark.parametrize(\"arg\", [1, 2])\n            assert arg == 1\n    \"\"\"\n    )\n    result = testdir.runpytest(\"-k\", \"extra_keyword and arg=1\")\n    result.assert_outcomes(passed=1)\n\n\n", "    def test_param_with_keyword_and_id(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.mark.parametrize(\n                \"arg,id_str\",\n                [\n                    (\"value1\", \"id1\"),\n                    (\"value2\", \"id2\"),\n                ],\n                ids=[\"arg1\", \"arg2\"],\n            )\n                pass\n            \"\"\"\n        )\n        result = testdir.runpytest()\n        result.stdout.fnmatch_lines([\"*test_values(arg=value1, id_str=id1) id1*\"])\n        result.stdout.fnmatch_lines([\"*test_values(arg=value2, id_str=id2) id2*\"])\n\n\n\n", "    def test_multiple_markers_with_expressions(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.mark.skip\n            @pytest.mark.internal_err\n                pass\n        \"\"\"\n        )\n        result = testdir.runpytest(\"-m\", \"skip and internal_err\")\n        result.stdout.fnmatch_lines([\"*::test_foo PASSED*\"])\n        result = testdir.runpytest(\"-m\", \"not skip and internal_err\")\n        result.stdout.fnmatch_lines([\"*::test_foo SKIPPED*\"])\n", "def test_mark_expr_eval_failure_handling_with_existing_marker(testdir):\n    foo = testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.internal_err\n            pass\n        \"\"\"\n    )\n    result = testdir.runpytest(foo, \"-m\", \"internal_err and bogus/internal_err\")\n    result.stderr.fnmatch_lines(\n        [\n            \"ERROR: Wrong expression passed to '-m': internal_err and bogus/internal_err: *internal_err and bogus/internal_err*\"\n        ]\n    )\n    assert result.ret == ExitCode.USAGE_ERROR\n", "    def test_simple_marker_expression(testdir):\n        testdir.makepyfile(\n            \"\"\"\n                assert request.node.has_marker(\"foo\")\n            \"\"\"\n        )\n        reprec = testdir.inline_run(\"-m\", \"foo\")\n        passed, skipped, failed = reprec.countoutcomes()\n        assert passed == 1\n        assert skipped + failed == 0\n"], "sample_669": ["compilation error", "def test_multicapture_writeorg():\n    with saved_fd(1):\n        with capture.MultiCapture(\n            out=capture.FDCapture(1),\n            err=capture.FDCapture(2),\n            in_=capture.FDCapture(0),\n        ) as multi:\n            multi.out.write_org(\"hello\")\n            with pytest.raises(AttributeError):\n                multi.err.write_org(\"world\")\n            multi.in_.write_org(b\"test\")\n", "def test_capture_unicode_output_with_bytes_input(testdir):\n    p = testdir.makepyfile(\n        \"\"\"\n        import sys\n\n            sys.stdin.write(b\"hello\")\n            print(\"world\")\n    \"\"\"\n    )\n    result = testdir.runpytest_subprocess()\n    result.stdout.fnmatch_lines(\n        \"\"\"\n        *test_unicode_output_bytes_input*\n        *world*\n    \"\"\"\n    )\n\n", "def test_capture_with_file_objects():\n    from _pytest.capture import StdCaptureFD\n    with open(\"test.txt\", \"w\") as f:\n        f.write(\"initial content\")\n    cap = StdCaptureFD(out=True, err=False)\n    cap.start_capturing()\n    with open(\"test.txt\", \"a\") as f:\n        f.write(\"appended content\")\n    out, _ = cap.readouterr()\n    assert out == \"\"\n    with open(\"test.txt\", \"r\") as f:\n        assert f.read() == \"initial contentappended content\"\n\n\n\n", "def test_captured_output_can_be_used_in_assertions(capsys):\n    sys.stdout.write(\"hello\\n\")\n    sys.stderr.write(\"world\\n\")\n    out, err = capsys.readouterr()\n    assert out == \"hello\\n\"\n    assert err == \"world\\n\"\n\n\n"], "sample_603": ["def test_summarize_coords_with_empty_coords(dataset):\n    ds = dataset.where(dataset[\"tmin\"] > 10)\n    ds = ds.drop([\"tmin\"])\n    formatted = fh.summarize_coords(ds.coords)\n    assert formatted == \"\" \n", "def test_array_section_collapsed_default(dataarray):\n    formatted = fh.array_section(dataarray)\n    assert \"checked\" in formatted\n    assert \"hidden\" in formatted\n\n", "def test_summarize_attrs_empty(dataset):\n    dataset_no_attrs = dataset.copy()\n    dataset_no_attrs.attrs = {}\n    formatted = fh.summarize_attrs(dataset_no_attrs.attrs)\n    assert formatted == \"\"\n", "def test_summarize_attrs_empty():\n    attrs = {}\n    formatted = fh.summarize_attrs(attrs)\n    assert formatted == \"\"\n\n\n", "def test_repr_with_invalid_attrs_keys(dataset):\n    dataset = dataset.assign(attributes={\"<attr>\": 1})\n    formatted = fh.dataset_repr(dataset)\n    assert \"class='xr-has-index'\" not in formatted\n    assert \"&lt;attr&gt; :\" in formatted \n    assert \"1\" in formatted\n\n\n\n"], "sample_767": ["    def func(X):\n        return np.array([('a', 1), ('b', 2)])\n", "def test_column_transformer_sparse_threshold_edge_cases():\n    X_array = np.array([[0, 1, 2],\n                        [2, 4, 6],\n                        [8, 6, 4]])\n\n    # sparse threshold equal to 1 should completely skip sparse transformer\n    ct = ColumnTransformer([('trans1', SparseMatrixTrans(), [0])],\n                           sparse_threshold=1.0)\n    X_trans = ct.fit_transform(X_array)\n\n    assert not sparse.issparse(X_trans)\n\n    # sparse threshold equal to 0 should always apply sparse transformer\n    ct = ColumnTransformer([('trans1', SparseMatrixTrans(), [0])],\n                           sparse_threshold=0.0)\n    X_trans = ct.fit_transform(X_array)\n\n    assert sparse.issparse(X_trans)\n    assert X_trans.shape == (3, 3)\n\n    # sparse threshold equal to 0.5 should apply sparse transformer when\n    # appropriate\n    ct = ColumnTransformer([('trans1', SparseMatrixTrans(), [0])],\n                           sparse_threshold=0.5)\n    X_trans = ct.fit_transform(X_array)\n    assert sparse.issparse(X_trans)\n\n", "def test_column_transformer_transformer_weights():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n\n    ct = ColumnTransformer([('trans1', StandardScaler(), [0]),\n                            ('trans2', StandardScaler(), [1])],\n                           transformer_weights={'trans1': 0.5,\n                                                'trans2': 0.5})\n\n    transformed = ct.fit_transform(X_array)\n    \n    # check weights are applied correctly\n    assert not np.allclose(transformed, np.array([\n        [0.5 * (0 - 1.5), 0.5 * (1 - 2.5)],\n        [0.5 * (2 - 1.5), 0.5 * (4 - 2.5)]\n    ]))\n\n\n", "    def func(X):\n        return X.iloc[:, 0].values.reshape(-1, 1)\n", "def test_column_transformer_sparse_threshold():\n    X_array = np.array([[0, 1, 2],\n                        [2, 4, 6],\n                        [8, 6, 4]]).T\n    ct = ColumnTransformer([('trans1', Trans(), [0])],\n                           sparse_threshold=0.5)\n\n    X_trans = ct.fit_transform(X_array)\n    assert not sparse.issparse(X_trans)\n\n    ct = ColumnTransformer([('trans1', Trans(), [0])],\n                           sparse_threshold=0.0)\n    X_trans = ct.fit_transform(X_array)\n    assert sparse.issparse(X_trans)\n\n    ct = ColumnTransformer([('trans1', Trans(), [0])],\n                           sparse_threshold=1.0)\n    X_trans = ct.fit_transform(X_array)\n    assert not sparse.issparse(X_trans)\n"], "sample_297": ["    def test_ticket_24973(self):\n        \"\"\"\n        Test for ticket #24973: Null check in values() should not affect join.\n        \"\"\"\n        p1 = Parent.objects.create(pk=1, name=\"p1\")\n        p2 = Parent.objects.create(pk=2, name=\"p2\")\n        c1 = Child.objects.create(parent=p1, name=\"c1\")\n        c2 = Child.objects.create(parent=p2, name=\"c2\")\n\n        # Check that filtering by a null field in values() doesn't\n        # unexpectedly introduce a left outer join.\n        qs = Child.objects.filter(parent__isnull=True).values('parent__name')\n        self.assertEqual(str(qs.query).count('LEFT OUTER JOIN'), 0)\n        self.assertSequenceEqual(list(qs), [])\n\n\n\n        # Test when a null is expected.\n        qs2 = Child.objects.filter(parent=p1).values('parent__name')\n        self.assertEqual(str(qs2.query).count('LEFT OUTER JOIN'), 0)\n        self.assertSequenceEqual(list(qs2), [{'parent__name': 'p1'}])\n\n\n\n        c3 = Child.objects.create(parent=None, name=\"c3\")\n        qs3 = Child.objects.filter(parent__isnull=True).values('parent__name')\n        self.assertSequenceEqual(list(qs3), [{'parent__name': None}])\n", "    def test_ticket_24622_not_in(self):\n        \"\"\"\n        Make sure __pk__not_in and __not_in work the same for related fields\n        when using a distinct on subquery.\n        \"\"\"\n        a1 = Ticket23605A.objects.create()\n        a2 = Ticket23605A.objects.create()\n        c1 = Ticket23605C.objects.create(field_c0=0.0)\n        Ticket23605B.objects.create(\n            modela_fk=a1, field_b0=123,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        Ticket23605B.objects.create(\n            modela_fk=a1, field_b0=23,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        Ticket23605B.objects.create(\n            modela_fk=a1, field_b0=234,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        Ticket23605B.objects.create(\n            modela_fk=a1, field_b0=12,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        Ticket23605B.objects.create(\n            modela_fk=a2, field_b0=567,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        Ticket23605B.objects.create(\n            modela_fk=a2, field_b0=76,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        Ticket23605B.objects.create(\n            modela_fk=a2, field_b0=7,\n            field_b1=True,\n            modelc_fk=c1,\n        )\n        ", "    def test_ticket_24859(self):\n        p1 = Parent.objects.create()\n        c1 = Child.objects.create(parent=p1)\n        c2 = Child.objects.create(parent=p1)\n        p2 = Parent.objects.create()\n        c3 = Child.objects.create(parent=p2)\n\n        qs = Child.objects.filter(parent__id=p1.id).values('parent')\n        print(qs.query)\n        self.assertSequenceEqual(list(qs), [\n            {'parent': p1},\n            {'parent': p1},\n        ])\n\n\n        \n", "    def test_ticket_24605_subquery_with_distinct(self):\n        \n        school1 = School.objects.create(name=\"School 1\")\n        school2 = School.objects.create(name=\"School 2\")\n        \n        student1 = Student.objects.create(name=\"Student 1\", school=school1)\n        student2 = Student.objects.create(name=\"Student 2\", school=school1)\n        student3 = Student.objects.create(name=\"Student 3\", school=school2)\n        student4 = Student.objects.create(name=\"Student 4\", school=school2)\n\n        # Test case with distinct subquery in a Q object\n        qs = Student.objects.filter(\n            Q(pk__in=Student.objects.filter(school__name='School 1').distinct('school').values_list('school__pk', flat=True))\n        )\n        self.assertSequenceEqual(qs, [student1, student2])\n\n\n        \n", "    def test_ticket_24605_pk_in_subquery(self):\n        \"\"\"\n        Subquery table names should be quoted.\n        \"\"\"\n        i1 = Individual.objects.create(alive=True)\n        RelatedIndividual.objects.create(related=i1)\n        i2 = Individual.objects.create(alive=False)\n        RelatedIndividual.objects.create(related=i2)\n        i3 = Individual.objects.create(alive=True)\n        i4 = Individual.objects.create(alive=False)\n\n        self.assertSequenceEqual(Individual.objects.filter(\n            Q(alive=False),\n            Q(related_individual__pk__in=RelatedIndividual.objects.filter(related__pk=i4))\n        ), [i4])\n"], "sample_937": ["    def test_unparse_typed_comments():\n        source = \"\"\"\n            return a + int(b)\n        \"\"\"\n        module = ast.parse(source)\n        expected = \"\"\"", "def test_unparse_slice(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].value) == expected\n", "def test_unparse_slicing(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].value) == expected\n", "def test_unparse_subscript_slice(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].value) == expected \n", "    def test_unparse_arg_defaults():\n        source = \"def my_func(a: int, b: str = 'default', *args, **kwargs):\\n pass\"\n        expected = \"def my_func(a: int, b='default', *args, **kwargs): pass\"\n        module = ast.parse(source)\n        assert ast.unparse(module.body[0].args) == expected\n"], "sample_488": ["    def test_namespace_package_loading(self):\n        \"\"\"\n        Tests loading a namespace package with only one path using a custom app config.\n        \"\"\"\n        with extend_sys_path(self.base_location):\n            with self.settings(INSTALLED_APPS=[\"nsapp.apps.NSAppConfig\"]):\n                app_config = apps.get_app_config(\"nsapp\")  \n                self.assertEqual(app_config.path, self.app_path) \n", "    def test_namespace_package_with_missing_init(self):\n        \"\"\"\n        A namespace package with a missing __init__.py in one location\n        should be considered valid as long as an __init__.py exists\n        in another location.\n        \"\"\"\n        # Temporarily add two directories to sys.path, one with an\n        # __init__.py in the \"nsapp\" package and one without.\n        with extend_sys_path(\n            os.path.join(self.base_location, \"missing_init\"),\n            os.path.join(self.base_location, \"nsapp\"),\n        ):\n            with self.settings(INSTALLED_APPS=[\"nsapp\"]):\n                app_config = apps.get_app_config(\"nsapp\")\n                self.assertEqual(app_config.path, self.app_path)\n\n\n\n", "    def test_multiple_paths_explicit_path_override(self):\n        \"\"\"\n        Explicitly setting path in app_config overrides multiple locations\n        \"\"\"\n        # Temporarily add two directories to sys.path that both contain\n        # components of the \"nsapp\" package.\n        with extend_sys_path(self.base_location, self.other_location):\n            with self.settings(\n                INSTALLED_APPS=[\"nsapp.apps.NSAppConfig\"],\n            ):\n                app_config = apps.get_app_config(\n                    \"nsapp\",\n                )\n                self.assertEqual(app_config.path, \"custom_path\")\n", "    def test_app_config_label_in_path(self):\n        \"\"\"\n        Test that the app label can be included in the path attribute\n        of the AppConfig.\n        \"\"\"\n        with extend_sys_path(self.base_location):\n            with self.settings(INSTALLED_APPS=[\"nsapp.apps.NSAppConfig\"]):\n                app_config = apps.get_app_config(\"nsapp\")\n                self.assertEqual(app_config.path, os.path.join(self.app_path, \"nsapp\"))\n\n\n", "    def test_invalid_app_config_class_name(self):\n        \"\"\"\n        Tests that an invalid app config class name raises an error.\n        \"\"\"\n        with self.assertRaisesMessage(\n            ImproperlyConfigured,\n            \"App config class 'apps.apps.invalid' should be named 'MyAppConfig'\",\n        ):\n            with self.settings(INSTALLED_APPS=[\"apps.apps.invalid\"]):\n                pass\n"], "sample_769": ["def test_brier_score_loss_multiclass():\n    y_true = np.array([0, 1, 2, 2, 0, 1])\n    y_pred = np.array([[0.1, 0.3, 0.6], [0.8, 0.1, 0.1], [0.2, 0.6, 0.2],\n                       [0.2, 0.6, 0.2], [0.3, 0.3, 0.4], [0.7, 0.2, 0.1]])\n\n    true_score = linalg.norm(y_true.reshape(-1, 1) - y_pred, axis=1) ** 2 / len(y_true)\n    assert_almost_equal(brier_score_loss(y_true, y_pred), np.mean(true_score))\n\n\n", "def test_balanced_accuracy_score_with_weights():\n    weights = {'a': 2, 'b': 1}\n    y_true = ['a', 'b', 'a', 'b']\n    y_pred = ['a', 'a', 'a', 'b']\n    balanced = balanced_accuracy_score(y_true, y_pred, sample_weight=weights)\n    assert_almost_equal(balanced, 0.625)  \n", "    def test_balanced_accuracy_score_with_weights():\n        y_true = np.array([0, 0, 1, 0, 1, 1])\n        y_pred = np.array([0.1, 0.8, 0.9, 0.3, 1., 0.95])\n        sample_weight = np.array([0.2, 0.5, 0.8, 0.3, 0.1, 0.6])\n        weights = ['class', 'sample']\n        for w in weights:\n            for balanced in [balanced_accuracy_score,\n                             balanced_accuracy_score]:\n                assert_almost_equal(balanced(y_true, y_pred, sample_weight=sample_weight,\n                                            adjust=True,\n                                            average=w),\n                                    balanced(y_true, y_pred, sample_weight=sample_weight,\n                                            adjust=True)\n                                    )\n\n\n", "def test_balanced_accuracy_score_with_none_labels():\n    y_true = np.array([0, 1, 2, 1, 2])\n    y_pred = np.array([0, 1, 2, 1, 2])\n    balanced = balanced_accuracy_score(y_true, y_pred, labels=None)\n    assert_equal(balanced, 1.0)  \n", "def test_weighted_average_without_labels():\n    y_true = np.array([0, 1, 2])\n    y_pred = np.array([0.1, 0.8, 0.9])\n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter(\"always\")\n        _ = precision_score(y_true, y_pred, average='weighted')\n    assert len(w) == 1\n    assert str(w[-1].message) == (\n        'Warning:  The average parameter must be specified when\\\n            there are no labels.'\n    )\n\n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter(\"always\")\n        _ = recall_score(y_true, y_pred, average='weighted')\n    assert len(w) == 1\n    assert str(w[-1].message) == (\n        'Warning:  The average parameter must be specified when\\\n            there are no labels.'\n    )\n\n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter(\"always\")\n        _ = f1_score(y_true, y_pred, average='weighted')\n    assert len(w) == 1\n    assert str(w[-1].message) == (\n        'Warning:  The average parameter must be specified when\\\n            there are no labels.'\n    )\n\n\n"], "sample_17": ["    def test_ignored_functions(self, func_name):\n        assert func_name not in function_helpers\n\n\n\n", "    def test_all_functions_handle_quantity_input(self):\n        for func_name in all_wrapped_functions:\n            func = getattr(np, func_name)\n            try:\n                func(u.Quantity([1, 2, 3], u.m))\n            except (TypeError, ValueError, NotImplementedError) as e:\n                if \"cannot be converted\" in str(e) or \"unsupported operation\" in str(e):\n                    print(f\"Skipping {func_name} due to exception: {e}\")\n                    continue\n                raise\n\n           \n\n", "    def test_dispatch_functions(self):\n        for function_name in DISPATCHED_FUNCTIONS:\n            function = DISPATCHED_FUNCTIONS[function_name]\n            if isinstance(function, tuple):\n                # Handle cases where DISPATCHED_FUNCTIONS stores tuples (function, dispatch_name)\n                function, dispatch_name = function\n                args = [np.array([1, 2], dtype=float)] * len(function.args)\n                kwargs = {}\n                result = function(*args, **kwargs)\n                assert isinstance(result, Quantity)\n\n\n\n", "    def test_untested_functions(self, func_name):\n        with pytest.raises(NotImplementedError, match=str(func_name)):\n            getattr(np, func_name)()\n", "    def test_function_helpers_wrapper(self, func):\n        assert callable(func)\n        with pytest.raises(\n            TypeError,\n            match=rf\"the Quantity implementation cannot handle {func.__name__} for quantities\",\n        ):\n            func(u.Quantity([1], units=u.m))\n\n"], "sample_974": ["def test_ccode_non_integer_indices():\n    from sympy.tensor import IndexedBase, Idx\n    from sympy import symbols\n    n, m = symbols('n m', integer=False)\n    A = IndexedBase('A')\n    i = Idx('i', n)\n    j = Idx('j', m)\n\n    expr = A[i, j]\n    code = ccode(expr)\n    assert code == \"A[(int)i][(int)j]\"\n\n\n\n", "def test_ccode_for_nested():\n    f = For(x, Range(0, 10), [\n        For(y, Range(0, 5), [aug_assign(z, '+', x * y)])\n    ])\n    sol = ccode(f)\n    assert sol == (\"for (x = 0; x < 10; x++) {\\n\"\n                   \"   for (y = 0; y < 5; y++) {\\n\"\n                   \"      z += x*y;\\n\"\n                   \"   }\\n\"\n                   \"}\")\n", "def test_ccode_special_functions():\n    from sympy import gamma, factorial, erfc, log, exp\n    assert ccode(gamma(x)) == \"tgamma(x)\"\n    assert ccode(factorial(x)) == \"tgamma(x + 1)\"\n    assert ccode(erfc(x)) == \"erfc(x)\"\n    assert ccode(log(x)) == \"log(x)\"\n    assert ccode(exp(x)) == \"exp(x)\"\n\n\n", "def test_ccode_lambda_expressions():\n    from sympy import Symbol, Lambda\n    x = Symbol('x')\n    f = Lambda(x, x**2 + 2*x + 1)\n    g = Lambda((x, y), x*y)\n    assert ccode(f(x)) == 'pow(x, 2) + 2*x + 1'\n    assert ccode(g(x, y)) == 'x*y'\n", "def test_ccode_complex_numbers():\n    x = symbols('x', complex=True)\n    assert ccode(x.real) == \"creal(x)\"\n    assert ccode(x.imag) == \"cimag(x)\"\n    assert ccode(x.abs()) == \"cabs(x)\"\n    assert ccode(x**2) == \"pow(x, 2)\"\n"], "sample_885": ["def test_interval_integer_not_real():\n    \"\"\"Check for the type \"integer_not_real\" in the Interval constraint.\"\"\"\n    constraint = Interval(\"integer_not_real\", 0, 1, closed=\"both\")\n    assert constraint.is_satisfied_by(1)\n    assert not constraint.is_satisfied_by(1.0)\n\n\n", "def test_interval_with_numpy_arrays():\n    with pytest.raises(InvalidParameterError, match=\"The 'left' parameter\"):\n        Interval(Real, np.array([1, 2]), 0, closed=\"both\")\n\n", "def test_interval_real_int_with_closed_right():\n    \"\"\"Check that an Interval constraint with closed='right' accepts integers\n    as input if the right bound is also an integer.\"\"\"\n    constraint = Interval(Real, 1, 2, closed=\"right\")\n    assert constraint.is_satisfied_by(1)  \n    assert constraint.is_satisfied_by(2)\n    assert not constraint.is_satisfied_by(2.1)\n\n\n", "def test_interval_closed_behavior():\n    \"\"\"Check that the closed attribute of Interval constraints works as expected.\"\"\"\n    interval = Interval(Real, 0, 1, closed=\"left\")\n    assert interval.is_satisfied_by(0)  \n    assert interval.is_satisfied_by(0.99)\n    assert not interval.is_satisfied_by(1)\n\n    interval = Interval(Real, 0, 1, closed=\"right\")\n    assert interval.is_satisfied_by(0.99)\n    assert not interval.is_satisfied_by(1)\n    assert interval.is_satisfied_by(0)\n\n    interval = Interval(Real, 0, 1, closed=\"both\")\n    assert interval.is_satisfied_by(0)\n    assert interval.is_satisfied_by(0.99)\n    assert interval.is_satisfied_by(1)\n\n    interval = Interval(Real, 0, 1, closed=\"neither\")\n    assert not interval.is_satisfied_by(0)\n    assert interval.is_satisfied_by(0.99)\n    assert not interval.is_satisfied_by(1)\n\n\n\n", "compilation error"], "sample_676": ["compilation error", "        def _get_verbose_word(*args):\n            return mocked_verbose_word\n", "    def test_line_with_reprcrash_longrepr(monkeypatch):\n        import _pytest.terminal\n        from wcwidth import wcswidth\n\n        mocked_verbose_word = \"FAILED\"\n\n        mocked_pos = \"some::nodeid\"\n\n            return mocked_pos\n\n        monkeypatch.setattr(_pytest.terminal, \"_get_pos\", mock_get_pos)\n\n        class config(object):\n            pass\n\n        class rep(object):\n                return mocked_verbose_word\n\n            class longrepr:\n                class reprcrash:\n                        return u\"longrepr - content\"\n\n            __tracebackhide__ = True\n            if msg:\n                rep.longrepr.reprcrash.message = msg\n            actual = _get_line_with_reprcrash_message(config, rep(), width)\n\n            assert actual == expected\n            if actual != \"%s %s\" % (mocked_verbose_word, mocked_pos):\n                assert len(actual) <= width\n                assert wcswidth(actual) <= width\n\n        check(None, 80, \"FAILED %s - longrepr - content\" % mocked_pos)\n        check(\"msg\", 80, \"FAILED %s - msg\\nlongrepr - content\" % mocked_pos)\n\n", "compilation error", "compilation error"], "sample_987": ["def test_issue_10412():\n    x, y = S.symbols('x y')\n    f = x**2 + y**2\n    df = diff(f, x)\n    assert df.evalf() == 2*x\n    assert df.subs(x, 1).evalf() == 2\n    assert tensor(df, indices=[x]).evalf() == 2\n    assert tensor(df, indices=[y]).evalf() == 0\n    \n", "def test_issue_10916():\n    from sympy import symbols\n    x = symbols('x')\n    e = (x**2 + 1)**(1/2)\n    assert e.evalf(subs={x: 2}).n() == 2.23606797749979\n", "def test_issue_10700():\n    from sympy.polys import poly_from_expr, div\n    x = Symbol('x')\n    p = poly_from_expr(3*x**2 - 5*x - 2)\n    q = poly_from_expr(x**2 + 1)\n    p_evalf = p.evalf()\n    q_evalf = q.evalf()\n    poly_div = div(p_evalf, q_evalf)\n    assert poly_div == p.as_poly(x).as_expr() // q.as_poly(x).as_expr()\n", "def test_issue_11095():\n    from sympy import symbols, sqrt, Piecewise\n    x, y = symbols('x y')\n    p = Piecewise((x**2, x > 0), (0, x <= 0))\n    assert p.n() == p\n    assert p.evalf(subs={x: 1}) == 1\n    assert p.evalf(subs={x: -1}) == 0\n    assert (p + y).n() == p + y\n    assert (p * y).n() == p * y\n\n\n", "compilation error"], "sample_102": ["    def test_intersection_with_subquery_order_by(self):\n        qs1 = Number.objects.order_by('num')[:2]\n        qs2 = Number.objects.order_by('-num')[:2]\n        self.assertNumbersEqual(qs1.intersection(qs2).order_by('num'), [9, 8])\n\n\n\n", "    def test_union_with_distinct_and_ordering(self):\n        qs1 = Number.objects.filter(num__lte=1).union(Number.objects.filter(num__gte=8))\n        qs2 = Number.objects.filter(num__lte=1).union(Number.objects.filter(num__gte=8))\n        self.assertNumbersEqual(qs1.distinct().order_by('num'), [0, 1, 8, 9])\n        self.assertNumbersEqual(qs1.distinct().order_by('-num'), [9, 8, 1, 0])\n        self.assertNumbersEqual(qs2.distinct().order_by('num'), [0, 1, 8, 9])\n        self.assertNumbersEqual(qs2.distinct().order_by('-num'), [9, 8, 1, 0])\n        \n        # Ordering should work with distinct union.\n        self.assertNumbersEqual(qs1.distinct().order_by('num').values('num'), [0, 1, 8, 9])\n", "    def test_related_manager_intersection(self):\n        from django.db.models import query\n        RelatedModel = models.ForeignKey(Number, on_delete=models.CASCADE, related_name='related_models')\n\n        Number.objects.create(num=1, other_num=9)\n        related_obj1 = models.RelatedModel.objects.create(number=Number.objects.get(pk=1))\n        related_obj2 = models.RelatedModel.objects.create(number=Number.objects.get(pk=1))\n        qs1 = Number.objects.filter(num=1)\n        qs2 = Number.objects.filter(pk__in=[1])\n        self.assertEqual(qs1.intersection(qs2).count(), 1)\n        self.assertEqual(\n            qs1.intersection(qs2).values('num').count(),\n            1\n        )\n        qs3 = Number.objects.none()\n        self.assertEqual(qs1.intersection(qs3).count(), 0)\n        self.assertEqual(qs1.intersection(qs3).count(), 0)\n\n        self.assertEqual(Number.objects.filter(pk=1).intersection(qs2).count(), 1)\n        # test intersection with related objects\n        related_qs1 = related_obj1.number_set.all()\n        related_qs2 = related_obj2.number_set.all()\n        self.assertEqual(related_qs1.intersection(related_qs2).count(), 1)\n        related_qs3 = models.RelatedModel.objects.none()\n        self.assertEqual(related_qs1.intersection(related_qs3).count(), 0)\n        self.assertEqual(related_qs1.intersection(related_qs3).count(), 0)\n        \n        # test intersection with related objects using .get()\n        related_obj4 = models.RelatedModel.objects.get(pk=related_obj1.pk)\n        self.assertEqual(related_obj1.number_set.filter(pk=1).intersection(related_obj4.number_set).count(), 1)\n\n\n\n", "    def test_union_with_subquery(self):\n        qs1 = Number.objects.filter(num__lt=5)\n        qs2 = Number.objects.filter(pk__in=Number.objects.filter(num__gt=4).values_list('pk'))\n        self.assertNumbersEqual(qs1.union(qs2), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n\n\n", "    def test_ordering_with_distinct(self):\n        qs1 = Number.objects.filter(num__lte=1)\n        qs2 = Number.objects.filter(num__gte=2, num__lte=3)\n        self.assertNumbersEqual(qs1.union(qs2).distinct().order_by('-num'), [3, 2, 1, 0])\n"], "sample_971": ["    def test_message_log(app, status, warning):\n        logging.setup(app, status, warning)\n        logger = logging.getLogger(__name__)\n\n        assert not app.messagelog\n        with logging.suppress_logging():\n            logger.info('message1')\n        assert app.messagelog\n        assert 'message1' in app.messagelog[0]\n        \n        app.messagelog = []\n        logger.warning('message2')\n        assert app.messagelog\n        assert 'message2' in app.messagelog[0]\n\n\n", "def test_pending_logging_flush(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    with logging.pending_logging() as memhandler:\n        logger.warning('message1')\n        logger.warning('message2')\n        assert 'WARNING: message1' not in warning.getvalue()\n        assert 'WARNING: message2' not in warning.getvalue()\n\n    memhandler.flushTo(logger)\n    assert 'WARNING: message1\\nWARNING: message2' in strip_escseq(warning.getvalue()) \n", "def test_log_collector(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    with logging.LogCollector() as collector:\n        logger.warning('message1')\n        logger.info('message2')\n        logger.error('message3')\n\n    logs = collector.logs\n    assert len(logs) == 3\n    assert 'WARNING: message1' in logs[0].msg\n    assert 'message2' in logs[1].msg\n    assert 'ERROR: message3' in logs[2].msg\n", "def test_info_log_record_translator(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    info_record = logging.LogRecord(\"root\", \n                                     logging.INFO,\n                                     None,\n                                     None,\n                                     \"info message\",\n                                     (None, None),\n                                     None,\n                                     None,\n                                     {'location': ('index.txt', 10)})\n    translated = InfoLogRecordTranslator(app).filter(info_record)\n    assert translated.location == 'index.txt:10'\n\n    warning_record = logging.LogRecord(\"root\",\n                                        logging.WARNING,\n                                        None,\n                                        None,\n                                        \"warning message\",\n                                        (None, None),\n                                        None,\n                                        None,\n                                        {'location': ('index.txt', 10)})\n    translated = WarningLogRecordTranslator(app).filter(warning_record)\n    assert translated.location == 'index.txt:10'\n", "def test_pending_logging_flush(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    with logging.pending_logging() as memhandler:\n        logger.warning('message1')\n        logger.warning('message2')\n\n    assert 'WARNING: message1\\nWARNING: message2' in strip_escseq(warning.getvalue())\n\n"], "sample_729": ["def test_enet_solver_arguments():\n    X, y, _, _ = build_dataset()\n    for solver in ['saga', 'liblinear', 'lbfgs']:\n        clf = ElasticNet(solver=solver)\n        clf.fit(X, y)\n\n    for solver in ['adam', 'newton-cg']:\n        assert_raises(ValueError, ElasticNet, solver=solver)\n\n\n", "compilation error", "    def test_enet_sparse_multi_output():\n        # Test sparse multi output enet\n        X, y, _, _ = build_dataset(n_samples=20, n_features=100)\n        X = sparse.csr_matrix(X)\n        y = np.hstack((y[:, np.newaxis], y[:, np.newaxis]))\n\n        enet_sparse = MultiTaskElasticNet(selection='cyclic', tol=1e-8,", "def test_enet_multitask_sparse_data():\n    n_samples = 100\n    n_features = 50\n    n_targets = 3\n\n    X = sparse.csr_matrix(np.random.rand(n_samples, n_features))\n    y = np.random.rand(n_samples, n_targets)\n\n    clf = MultiTaskElasticNet(alpha=0.1, l1_ratio=0.5, tol=1e-4, max_iter=100)\n    clf.fit(X, y)\n    assert_almost_equal(clf.coef_.mean(), 0)\n    assert_almost_equal(clf.intercept_.mean(), 0)\n\n\n\n", "compilation error"], "sample_775": ["compilation error", "    def test_changed_only_with_nested_structure():\n        # Make sure changed_only works correctly with nested structures\n        from sklearn.base import BaseEstimator, TransformerMixin\n        from sklearn.preprocessing import StandardScaler\n\n        class MyNestedEstimator(BaseEstimator, TransformerMixin):\n                self.scaler = scaler\n                self.param = param\n            \n                if self.scaler:\n                    self.scaler.fit(X)\n                return self\n\n                return X * self.param\n\n        # Test with a nested estimator instance\n        scaler = StandardScaler()\n        estimator = MyNestedEstimator(scaler=scaler, param=2)\n        expected = \"\"\"", "compilation error", "def test_custom_estimator():\n    class CustomEstimator:\n            self.a = a\n            self.b = b\n\n            pass\n\n            return X * self.a + self.b\n\n    custom_estimator = CustomEstimator(a=2, b=5)\n    expected = \"\"\"", "        def __init__(self, a, b, c):\n            self.a = a\n            self.b = b\n            self.c = c\n"], "sample_54": ["    def test_file_from_named_pipe_with_content_length(self):\n        with tempfile.TemporaryDirectory() as temp_dir:\n            pipe_file = os.path.join(temp_dir, 'named_pipe')\n            os.mkfifo(pipe_file)\n            pipe_for_read = os.open(pipe_file, os.O_RDONLY | os.O_NONBLOCK)\n            with open(pipe_file, 'wb') as pipe_for_write:\n                pipe_for_write.write(b'binary content')\n\n            response = FileResponse(os.fdopen(pipe_for_read, mode='rb'),\n                                    content_length=14)\n\n            self.assertEqual(response['Content-Length'], '14')\n            self.assertEqual(list(response), [b'binary content'])\n            response.close()\n\n\n\n", "    def test_handle_missing_content_length(self):\n        with tempfile.TemporaryFile() as tmp:\n            response = FileResponse(tmp, as_attachment=True)\n            self.assertIn('Content-Length', response.headers)\n            self.assertEqual(response['Content-Length'], '0')\n", "    def test_file_not_found(self):\n        with self.assertRaises(FileNotFoundError):\n            response = FileResponse(open('nonexistent_file', 'rb'))\n", "    def test_file_response_with_custom_content_type(self):\n        with tempfile.NamedTemporaryFile(suffix=\".txt\") as tmp:\n            tmp.write(b\"Hello, world!\")\n            tmp.flush()\n            response = FileResponse(tmp, content_type=\"text/custom\")\n        self.assertEqual(response['Content-Type'], 'text/custom')\n\n\n\n", "    def test_empty_file_response(self):\n        response = FileResponse(io.BytesIO())\n        self.assertEqual(response['Content-Length'], '0')\n        self.assertEqual(response['Content-Type'], 'application/octet-stream')\n        self.assertFalse(response.has_header('Content-Disposition'))\n        self.assertEqual(list(response), [])\n        response.close()\n\n\n\n"], "sample_1085": ["def test_issue_10514():\n    assert Float('1.2345678901234567890', 12).as_decimal() == Decimal('1.234567890123456789') \n", "def test_mod_inverse_negative_moduli():\n    assert mod_inverse(-3, 7) == 5\n    assert mod_inverse(-3, -7) == 2\n    assert mod_inverse(3, -7) == -5\n    assert mod_inverse(-3, -7) == 2\n\n\n\n", "def test_mod_with_zero_denominator():\n    assert mod_inverse(2, 0) == S(None)\n    assert mod_inverse(2, Integer(0)) == S(None)\n\n\n\n", "compilation error", "def test_issue_11544():\n    assert float(S(2)/S(3)) == 0.6666666666666666\n"], "sample_715": ["def test_cross_val_predict_with_sparse_data():\n    iris = load_iris()\n    X, y = iris.data, iris.target\n\n    # Test with sparse data\n    X_sparse = csr_matrix(X)\n    y_sparse = csr_matrix(y)\n\n    # Test with LogisticRegression\n    clf = LogisticRegression()\n    predictions = cross_val_predict(clf, X_sparse, y_sparse, cv=5)\n    assert_array_almost_equal(predictions, clf.predict(X))\n\n    # Test with RandomForestClassifier\n    clf = RandomForestClassifier(random_state=0)\n    predictions = cross_val_predict(clf, X_sparse, y_sparse, cv=5)\n    assert_array_almost_equal(predictions, clf.predict(X))\n\n\n", "def test_cross_val_score_multioutput():\n    X, y = make_multilabel_classification(n_classes=2, n_labels=1,\n                                          allow_unlabeled=False,\n                                          return_indicator=True,\n                                          random_state=0)\n    clf = OneVsRestClassifier(SVC(kernel='linear'))\n    scores = cross_val_score(clf, X, y, cv=5)\n    assert_array_equal(scores.shape, (5,))\n", "def test_cross_val_predict_sparse_fit_params():\n    # Check that cross_val_predict handles sparse data and fit_params\n    X, y = make_classification(n_samples=100, n_features=20, random_state=42)\n    X_sparse = csr_matrix(X)\n\n    class SparseFitClassifier(BaseEstimator):\n            self.coef_ = coef\n\n            self.coef_ = X.toarray()\n            return self\n\n    clf = SparseFitClassifier(coef=np.zeros(20))\n    fit_params = {'random_state': 42}\n    predictions = cross_val_predict(clf, X_sparse, y, cv=5,\n                                    method='predict',\n                                    fit_params=fit_params)\n    assert_array_almost_equal(predictions,\n                             clf.predict(X_sparse))\n\n\n\n", "def test_cross_val_predict_sparse_fit_params():\n    X, y = make_classification(n_samples=100, n_features=10,\n                               n_informative=5, n_redundant=5, random_state=42)\n    X_sparse = csr_matrix(X)\n    y_sparse = csr_matrix(y)\n\n    class SparseFitClassifier(ClassifierMixin):\n        params = {'sparse_sample_weight': None}\n\n            self.params['sparse_sample_weight'] = fit_params.get(\n                'sparse_sample_weight', None)\n            return self\n\n            return np.zeros(len(X))\n\n    clf = SparseFitClassifier()\n    cross_val_predict(clf, X_sparse, y_sparse, cv=5,\n                      fit_params={'sparse_sample_weight': coo_matrix(\n                          np.eye(X.shape[0]))})\n    assert_array_equal(clf.params['sparse_sample_weight'].shape, X_sparse.shape)\n\n    \n", "def test_permutation_test_score_with_cv():\n    iris = load_iris()\n    X, y = iris.data, iris.target\n\n    # Test with multiple cross-validation splits\n    clf = LogisticRegression()\n    n_splits = 5\n    for cv in [KFold(n_splits=n_splits), StratifiedKFold(n_splits=n_splits)]:\n        result = permutation_test_score(clf, X, y, cv=cv, scoring='accuracy')\n        assert_isinstance(result, tuple)\n        assert_equal(len(result), n_splits)\n        for score in result:\n            assert_assert(np.isclose(score, 0.333333))\n\n\n\n"], "sample_724": ["def test_imputation_sparse_different_dtypes():\n    # Test imputation with sparse matrices and different data types.\n    X = sparse.csr_matrix(np.array([[1, 2], [3, 4], [5, 6]]),\n                          dtype='float16')\n    X[0, 1] = np.nan\n    X[1, 0] = np.nan\n\n    imputer = Imputer(missing_values=np.nan, strategy=\"mean\", axis=0)\n    X_imputed = imputer.fit_transform(X)\n\n    assert_array_almost_equal(X_imputed.data,\n                              np.array([1.5, 3.5]),\n                              err_msg=\"Imputation failed for sparse \"\n                              \"matrix with float16 dtype\")\n", "def test_imputation_invalid_missing_values():\n    # Test imputation with invalid missing_values\n    for invalid_missing_values in [1, 0.5, np.array(0), np.array(1)]:\n        with assert_raises(ValueError):\n            Imputer(missing_values=invalid_missing_values)\n\n\n", "def test_imputation_pipeline_with_null_values():\n    # Test imputation within a pipeline when the input data has null values\n    # (both dense and sparse)\n\n    from sklearn.datasets import make_regression\n\n    # Create some sample data\n    X, y = make_regression(n_samples=100, n_features=50, random_state=42)\n\n    # Create a pipeline with imputation and a decision tree regressor\n    pipeline = Pipeline([\n        ('imputer', Imputer(missing_values=np.nan, strategy='mean')),\n        ('tree', tree.DecisionTreeRegressor(random_state=0))\n    ])\n\n    # Fit the pipeline to the data\n    pipeline.fit(X, y)\n\n    # Test on sparse data\n    X_sparse = sparse.csr_matrix(X)\n    y_pred = pipeline.predict(X_sparse)\n\n    # Test on dense data\n    y_pred_dense = pipeline.predict(X)\n\n    # Assert that predictions are similar for both sparse and dense data\n    assert_array_almost_equal(y_pred, y_pred_dense)\n", "def test_imputation_invalid_missing_values():\n    # Test for invalid missing values\n    X = np.array([[1, 2, np.nan],\n                  [4, 5, 6]])\n\n    with assert_raises(ValueError):\n        Imputer(missing_values=\"abc\", strategy='mean')\n\n    with assert_raises(ValueError):\n        Imputer(missing_values=[1, 2], strategy='mean')\n", "def test_imputation_axis_0_with_missing_values():\n    # Test imputation along axis 0 with missing values.\n    X = np.array([\n        [np.nan, 0, 0],\n        [1, 2, 3],\n        [4, 5, np.nan],\n    ])\n\n    X_imputed = np.array([\n        [1, 0, 0],\n        [1, 2, 3],\n        [4, 5, 3],\n    ])\n\n    statistics_median = [2, 0, 3]\n\n    _check_statistics(X, X_imputed, \"median\", statistics_median, 'NaN')\n\n\n\n"], "sample_530": ["def test_offsetbox_pad(pad):\n    fig, ax = plt.subplots()\n    da = DrawingArea(10, 10)\n    anchored_box = AnchoredOffsetbox(loc='upper right', child=da, pad=pad)\n    ax.add_artist(anchored_box)\n    fig.canvas.draw()\n\n\n", "def test_anchoredtext_alignment(align):\n    fig, ax = plt.subplots()\n    text = AnchoredText(\"Test Text\", loc=\"upper left\", pad=0.1, prop={\"ha\": align})\n    ax.add_artist(text)\n\n    fig.canvas.draw()\n    renderer = fig.canvas.get_renderer()\n    bbox = text.get_tightbbox(renderer)\n    text_width, text_height = bbox.width, bbox.height\n    \n    if align == 'center':\n        x_offset = text_width / 2\n        y_offset = text_height / 2\n    elif align == 'left':\n        x_offset = 0\n        y_offset = text_height / 2\n    elif align == 'right':\n        x_offset = text_width\n        y_offset = text_height / 2\n    elif align == 'top':\n        x_offset = text_width / 2\n        y_offset = 0\n    elif align == 'bottom':\n        x_offset = text_width / 2\n        y_offset = text_height\n    elif align == 'baseline':\n        # baseline alignment is tricky, we'll just check if the text is visible\n        # and centered horizontally\n        x_offset = text_width / 2\n        y_offset = text_height / 2\n    \n    # roughly check if the text is centered within the bounding box\n    assert bbox.bounds[0] <= x_offset <= bbox.bounds[2]\n    assert bbox.bounds[1] <= y_offset <= bbox.bounds[3] \n", "def test_offsetbox_zorder_override():\n    fig, ax = plt.subplots()\n    d1 = AnchoredOffsetbox(loc='lower right', child=DrawingArea(50, 50))\n    d2 = AnchoredOffsetbox(loc='lower right', child=DrawingArea(50, 50))\n    d1.set_zorder(1)\n    d2.set_zorder(2)\n    ax.add_artist(d1)\n    ax.add_artist(d2)\n    fig.canvas.draw()\n    renderer = fig.canvas.get_renderer()\n    bb1 = d1.get_window_extent(renderer)\n    bb2 = d2.get_window_extent(renderer)\n    assert bb1.bounds < bb2.bounds\n\n\n\n", "def test_offsetbox_bbox_from_boxcoords():\n    fig, ax = plt.subplots()\n    ab = AnchordedOffsetbox(loc=\"center\", child=DrawingArea(10, 10))\n\n    # Test 'axes fraction'\n    ab.set_boxcoords('axes fraction')\n    ab.set_bbox_to_anchor((0.5, 0.5))\n    assert_allclose(ab.get_window_extent(ax.figure.canvas.get_renderer()).extents,\n                   [0.4, 0.4, 0.6, 0.6])\n\n    # Test 'axes pixels'\n    ab.set_boxcoords('axes pixels')\n    ab.set_bbox_to_anchor((50, 50))\n    assert_allclose(ab.get_window_extent(ax.figure.canvas.get_renderer()).extents,\n                   [48.5, 48.5, 51.5, 51.5])\n\n    # Test 'axes points'\n    ab.set_boxcoords('axes points')\n    ab.set_bbox_to_anchor((0.5, 0.5))\n    assert_allclose(ab.get_window_extent(ax.figure.canvas.get_renderer()).extents,\n                   [0.4, 0.4, 0.6, 0.6])\n\n    # Test 'data'\n    ab.set_boxcoords('data')\n    ab.set_bbox_to_anchor((0.5, 0.5))\n    # Assuming the default data limits are [0, 1, 0, 1]\n    assert_allclose(ab.get_window_extent(ax.figure.canvas.get_renderer()).extents,\n                   [0.4, 0.4, 0.6, 0.6])\n\n\n", "def test_offsetbox_update_position():\n    fig, ax = plt.subplots()\n\n    da = DrawingArea(10, 10)\n    ab = AnchoredOffsetbox(loc=\"center\", child=da, pad=0.1)\n    ax.add_artist(ab)\n\n    #  Translate the offsetbox to a new position\n    ab.set_offset((.2, .2))\n    fig.canvas.draw()\n    renderer = fig.canvas.get_renderer()\n    bbox = ab.get_window_extent(renderer)\n    assert_allclose(bbox.extents, [\n        20, 20, 30, 30], atol=1)\n\n"], "sample_786": ["def test_ignored_features():\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n    est = KBinsDiscretizer(n_bins=[2, 1], ignore_features=[1])\n    Xt = est.fit_transform(X)\n\n    assert Xt.shape == (4, 1)\n\n", "def test_ignored_features():\n    X = np.array([[1, 2, 3],\n                  [4, 5, 6],\n                  [7, 8, 9]])\n    kbd = KBinsDiscretizer(n_bins=3, ignore_features=[1])\n    Xt = kbd.fit_transform(X)\n    assert Xt.shape[1] == 2\n    assert_array_equal(Xt[:, 0].ravel(), [0, 0, 0])\n    assert_array_equal(Xt[:, 1].ravel(), [0, 1, 2])\n", "def test_ignored_features():\n    X = np.array([[1, 2, 3],\n                  [4, 5, 6],\n                  [7, 8, 9]])\n    kbd = KBinsDiscretizer(n_bins=3, ignore_features=[1], strategy='uniform', encode='ordinal')\n    kbd.fit(X)\n    Xt = kbd.transform(X)\n    assert Xt.shape == (3, 2)\n    for i in range(3):\n        assert Xt[i, 0] == 0  # First feature is binned\n        assert Xt[i, 1] == -1  # Second feature is ignored", "def test_n_bins_array_shape(n_bins):\n    est = KBinsDiscretizer(n_bins=n_bins, encode='ordinal')\n    assert_raises(ValueError, est.fit_transform, X)\n\n\n", "compilation error"], "sample_127": ["    def test_ignore_conflicts_with_existing_pk(self):\n        existing_country = Country.objects.create(name=\"Existing Country\")\n        data = [\n            Country(name=\"Existing Country\", iso_two_letter=\"XXX\"),  \n            # Same name, different iso_two_letter\n            Country(name=\"New Country\", iso_two_letter=\"YYY\"),\n        ]\n        with self.assertRaises(IntegrityError):\n            Country.objects.bulk_create(data, ignore_conflicts=True)\n", "    def test_ignore_conflicts_with_prefetch_related(self):\n        class RelatedModel(models.Model):\n            name = models.CharField(max_length=255)\n            country = models.ForeignKey(Country, on_delete=models.CASCADE)\n\n        related_data = [\n            RelatedModel(name='Related 1', country=self.data[0]),\n            RelatedModel(name='Related 2', country=self.data[0]),\n            RelatedModel(name='Related 3', country=self.data[1]),\n        ]\n        \n        Country.objects.bulk_create(self.data)\n        with self.assertRaises(IntegrityError):\n            RelatedModel.objects.bulk_create(related_data)\n\n        # Try again with ignore_conflicts=True\n        RelatedModel.objects.bulk_create(related_data, ignore_conflicts=True)\n        self.assertEqual(RelatedModel.objects.count(), 3) \n        self.assertEqual(RelatedModel.objects.filter(country=self.data[0]).count(), 2)\n        self.assertEqual(RelatedModel.objects.filter(country=self.data[1]).count(), 1) \n\n\n", "    def test_bulk_create_with_related_objects(self):\n        for i in range(3):\n            pizza = Pizzeria(name=f\"Pizza Place {i}\", country=Country.objects.get(pk=1))\n            Restaurant.objects.bulk_create([pizza])\n        pizzas = Pizzeria.objects.all()\n        self.assertEqual(len(pizzas), 3)\n        self.assertEqual(pizzas[0].country.id, 1)\n        self.assertEqual(pizzas[1].country.id, 1)\n        self.assertEqual(pizzas[2].country.id, 1)\n\n\n", "    def test_ignore_conflicts_multiple_exceptions(self):\n        data = [\n            TwoFields(f1=1, f2=1),\n            TwoFields(f1=2, f2=2),\n            TwoFields(f1=3, f2=3),\n        ]\n        TwoFields.objects.bulk_create(data)\n        self.assertEqual(TwoFields.objects.count(), 3)\n\n        conflicting_objects = [\n            TwoFields(f1=2, f2=2),\n            TwoFields(f1=3, f2=3),\n            TwoFields(f1=4, f2=4),\n        ]\n        with self.assertRaises(IntegrityError):\n            TwoFields.objects.bulk_create(conflicting_objects, ignore_conflicts=True)\n", "    def test_bulk_create_with_related_objects(self):\n        pizza_data = [\n            Pizzeria(name=\"Pizza Palace\", country=Country(name=\"Italy\"), rating=4.5),\n            Pizzeria(name=\"Big Slice\", country=Country(name=\"America\"), rating=4.2),\n        ]\n        with self.assertNumQueries(1):  \n            Pizzeria.objects.bulk_create(pizza_data)\n        \n        existing_pizzerias = Pizzeria.objects.all()\n        self.assertEqual(len(existing_pizzerias), 2)\n        self.assertEqual(existing_pizzerias[0].country.name, \"Italy\")\n        self.assertEqual(existing_pizzerias[1].country.name, \"America\")\n"], "sample_330": ["    def test_non_existent_pk_in_m2m(self):\n        obj = Object.objects.create()\n        with self.assertRaises(Object.DoesNotExist):\n            obj.related_objects.get(id=12345)\n", "    def test_many_to_many_remove(self):\n        obj = Object.objects.create()\n        obj.related_objects.create()\n        obj.related_objects.get().delete()\n        self.assertEqual(Object.objects.count(), 1)\n        self.assertEqual(obj.related_objects.count(), 0)\n", "    def test_delete_related(self):\n        obj = Object.objects.create()\n        related = obj.related_objects.create()\n        self.assertEqual(Object.objects.count(), 2)\n        obj.delete()\n        with self.assertRaises(Object.DoesNotExist):\n            Object.objects.get(pk=obj.id)\n        with self.assertRaises(Object.DoesNotExist):\n            related = related.related_objects.get()\n", "    def test_delete_related_object(self):\n        obj = Object.objects.create()\n        ref = ObjectReference.objects.create(obj=obj)\n        ref_id = ref.id\n        obj.delete()\n        with self.assertRaises(Object.DoesNotExist):\n            ObjectReference.objects.get(id=ref_id)\n\n\n\n", "    def test_delete_with_related_objects(self):\n        obj = Object.objects.create()\n        obj.related_objects.create()\n        self.assertEqual(Object.objects.count(), 2)\n\n        obj.delete()\n        self.assertFalse(Object.objects.filter(id=obj.id).exists())\n        self.assertEqual(ObjectReference.objects.count(), 0)\n"], "sample_1047": ["def test_issue_16313_2():\n    x = Symbol('x', real=False)\n    k = Symbol('k', real=True)\n    l = Symbol('l', real=True, zero=False)\n    assert (x.conjugate()).is_real is False\n    assert (k*x.conjugate()).is_real is None\n    assert (l*x.conjugate()).is_real is False\n\n\n\n", "def test_issue_16332_bis():\n    p = Symbol('p', positive=True)\n    q = Symbol('q', positive=True)\n    r = Symbol('r', zero=True)\n    x = Symbol('x', zero=True)\n    y = Symbol('y', zero=True)\n    assert (p * q * r).is_infinite is None\n    assert (p * q * x).is_infinite is None\n    assert (p * q * y).is_infinite is None\n    assert (p * p * r).is_infinite is None\n    assert (p * p * x).is_infinite is None\n    assert (p * p * y).is_infinite is None\n\n\n", "def test_issue_16551():\n    x = Symbol('x', zero=True)\n    y = Symbol('y', finite=True)\n    assert (x + y).is_finite is True\n    assert (x - y).is_finite is True\n    assert (x * y).is_finite is True\n    assert (y / x).is_finite is False\n", "def test_issue_16313_continue():\n    x = Symbol('x', real=False)\n    n = Symbol('n', integer=True)\n    a = Symbol('a', nonzero=True)\n    b = Symbol('b', nonzero=True)\n    assert (x**n).is_real is None\n    assert (a*x**n).is_real is None\n    assert (b/x**n).is_real is None\n    assert (n*x**(n*a)).is_real is None\n    assert (x**(a*b)).is_real is None\n    assert (x**(n*b)).is_real is None\n\n\n\n", "def test_issue_16314():\n    x = Symbol('x', irrational=True)\n    y = Symbol('y', real=True)\n    assert (x + y).is_irrational is True\n    assert (x + x).is_irrational \n    assert (x*y).is_irrational is None\n    assert (x**2).is_irrational is None\n    assert (y**x).is_irrational is None  \n    assert (x/y).is_irrational is None\n    assert (sqrt(x)).is_irrational\n    assert (x**y).is_irrational is None \n"], "sample_60": ["    def test_get_extra_default_for_modelform_fields(self):\n        class MediaForm(ModelForm):\n            class Meta:\n                model = Media\n                fields = '__all__'\n\n        class MediaInline(GenericTabularInline):\n            form = MediaForm\n            model = Media\n\n        class EpisodeAdmin(admin.ModelAdmin):\n            inlines = [\n                MediaInline\n            ]\n\n        ma = EpisodeAdmin(Episode, self.site)\n        formset = ma.get_formset(None)\n        self.assertEqual(formset.extra, 3)\n", "    def test_get_formset_with_related_fields(self):\n        class RelatedInline(InlineModelAdmin):\n            model = PhoneNumber\n            extra = 0\n            formset = None\n\n                formset = super().get_formset(request, obj, **kwargs)\n                formset.can_delete = False\n                return formset\n\n        class Contact(models.Model):\n            name = models.CharField(max_length=255)\n            Category = models.ForeignKey(Category, on_delete=models.CASCADE)\n\n        class ContactAdmin(admin.ModelAdmin):\n            inlines = [\n                RelatedInline,\n            ]\n\n        ma = ContactAdmin(Contact, self.site)\n        formset = ma.get_formset(request, Contact(), exclude=['name'])\n        self.assertEqual(len(formset.forms), 0)\n\n\n", "    def test_get_formset_kwargs_with_custom_formfield_callback(self):\n        media_inline = MediaInline(Media, AdminSite())\n\n            new_field = field.clone()\n            new_field.widget.attrs['class'] = 'my-custom-class'\n            return new_field\n\n        formset = media_inline.get_formset(request, formfield_callback=custom_formfield_callback)\n        widget = formset.forms[0].fields['url'].widget\n        self.assertEqual(widget.attrs.get('class'), 'my-custom-class')\n        self.assertIsNotNone(formset.can_order)\n\n\n\n", "    def test_get_form_for_add_view(self):\n        \"\"\"\n        This tests that the generated form for the ADD view in GenericInlineAdmin \n        correctly excludes fields specified with `exclude` in the Inline model.\n        \"\"\"\n        class MediaForm(ModelForm):\n            class Meta:\n                model = Media\n                exclude = ['url']\n\n        class MediaInline(GenericTabularInline):\n            form = MediaForm\n            model = Media\n\n        class EpisodeAdmin(admin.ModelAdmin):\n            inlines = [MediaInline]\n\n        ma = EpisodeAdmin(Episode, self.site)\n        formset = ma.get_formset(request, form_kwargs={'instance': None})\n        form = formset.forms[0]\n        self.assertEqual(\n            list(form.fields), ['description', 'id', 'DELETE']\n        )\n\n\n\n", "    def test_get_formset_kwargs_override_defaults(self):\n        media_inline = MediaInline(Media, AdminSite())\n\n        # Create a formset with default arguments\n        formset = media_inline.get_formset(request)\n        self.assertEqual(formset.max_num, DEFAULT_MAX_NUM)\n\n        # Create a formset with custom keyword arguments\n        media_inline = MediaInline(Media, AdminSite(), max_num=50, can_order=False)\n        formset = media_inline.get_formset(request)\n        self.assertEqual(formset.max_num, 50)\n        self.assertIs(formset.can_order, False)\n\n"], "sample_25": ["    def test_append_keyword_with_spaces(self):\n        header = fits.Header()\n        header.append(\"HELLO WORLD = 1\", \"This is a test\")\n        assert header[\"HELLO WORLD\"] == 1.0\n        assert header.comments[\"HELLO WORLD\"] == \"This is a test\"\n\n\n\n", "    def test_header_strip_comments(self):\n        h = fits.Header()\n        h[\"COMMENT\"] = \"This is a comment\"\n        h[\"KEYWORD\"] = 10\n        h[\"ANOTHER COMMENT\"] = \"This is another comment\"\n        h.strip()\n        assert h[\"KEYWORD\"] == 10\n        assert \"COMMENT\" not in h\n        assert \"ANOTHER COMMENT\" not in h\n", "    def test_rvkc_header_output(self):\n        \"\"\"\n        Tests that RVKCs are output correctly in the string representation\n        of a Header object.\n        \"\"\"\n        h = fits.Header()\n        h[\"DP1\"] = \"NAXIS: 2\"\n        h[\"DP1.AXIS.1\"] = 1\n        h[\"DP1.AXIS.2\"] = 2\n        h[\"DP1.NAXIS\"] = 2\n        h[\"DP1.AUX.1.COEFF.0\"] = 0\n        assert str(h).startswith(\"DP1      = 'NAXIS: 2'\")\n        assert \"DP1.NAXIS\" in str(h)\n        assert \"DP1.AXIS.1\" in str(h)\n        assert \"DP1.AXIS.2\" in str(h)\n        assert \"DP1.AUX.1.COEFF.0\" in str(h)\n", "    def test_header_comments(self):\n        h = fits.Header()\n        h[\"COMMENT\"] = (\"This is a comment\",)\n        assert h[\"COMMENT\"] == \"This is a comment\"\n        assert h.comments[\"COMMENT\"] == \"This is a comment\"\n\n        h[\"COMMENT\"] = (\"This is another comment\",)\n        assert h[\"COMMENT\"] == \"This is another comment\"\n        assert h.comments[\"COMMENT\"] == \"This is another comment\"\n\n        h[\"COMMENT1\"] = \"Another comment\"\n        assert h[\"COMMENT1\"] == \"Another comment\"\n        assert h.comments[\"COMMENT1\"] == \"Another comment\"\n\n        h[\"COMMENT\"] = (\"This is a comment\\nwith a newline\",)\n        assert h[\"COMMENT\"] == \"This is a comment\\nwith a newline\"\n        assert h.comments[\"COMMENT\"] == \"This is a comment\\nwith a newline\"\n\n        h[\"COMMENT2\"] = (\"This comment\\nhas\\nmultiple\\nlines\",)\n        assert h[\"COMMENT2\"] == \"This comment\\nhas\\nmultiple\\nlines\"\n        assert h.comments[\"COMMENT2\"] == \"This comment\\nhas\\nmultiple\\nlines\"\n", "    def test_header_invalid_comments(self):\n        header = fits.Header()\n        header[\"FOO\"] = \"This is a comment on FOO\\nwith multiple lines\"\n        assert header.comments[\"FOO\"] == \"This is a comment on FOO\\nwith multiple lines\"\n        header[\"BAR\"] = \"This is a comment on BAR with \\t tabs\"\n        assert header.comments[\"BAR\"] == \"This is a comment on BAR with \\t tabs\"\n        header[\"BAZ\"] = \"This is a comment on BAZ with \\\\n newlines\"\n        assert header.comments[\"BAZ\"] == \"This is a comment on BAZ with \\\\n newlines\"\n"], "sample_19": ["def test_zero_size_input():\n    with fits.open(get_pkg_data_filename(\"data/sip.fits\")) as f:\n        with pytest.warns(wcs.FITSFixedWarning):\n            w = wcs.WCS(f[0].header)\n\n    inp = np.zeros((0, 2))\n    assert_array_equal(inp, w.all_pix2world(inp, 0))\n    assert_array_equal(inp, w.all_world2pix(inp, 0))\n\n    inp = [], [1]\n    result = w.all_pix2world([], [1], 0)\n    assert_array_equal(inp[0], result[0])\n    assert_array_equal(inp[1], result[1])\n\n    result = w.all_world2pix([], [1], 0)\n    assert_array_equal(inp[0], result[0])\n    assert_array_equal(inp[1], result[1])\n", "def test_wcs_with_string_values():\n    \"\"\"\n    Test handling of WCS headers with string values (especially for CTYPE and CUNIT).\n    \"\"\"\n    header = fits.Header()\n    header.set(\"CTYPE1\", \"RA---TAN\")\n    header.set(\"CTYPE2\", \"DEC--TAN\")\n    header.set(\"CUNIT1\", \"hr\")\n    header.set(\"CUNIT2\", \"deg\")\n\n    w = wcs.WCS(header)\n    assert w.wcs.ctype == [\"RA---TAN\", \"DEC--TAN\"]\n    assert w.wcs.cunit == [\"hr\", \"deg\"]\n\n    # Ensure we can use it for transformations\n    assert_allclose(w.world2pix(SkyCoord(2, 1, unit=(\"deg\", \"hr\")), 0), [0, 0])\n\n    # Try setting a CTYPE with a missing key\n    header = fits.Header()\n    header.set(\"CTYPE1\", \"MISSING-KEY\")\n    with pytest.raises(ValueError, match=\"Missing key\"):\n        wcs.WCS(header)\n\n\n\n  ", "def test_wcs_with_missing_header_keys():\n    \"\"\"\n    Test handling of missing header keys\n    \"\"\"\n    w = wcs.WCS(naxis=2)\n    w.wcs.ctype = [\"RA---TAN\", \"DEC--TAN\"]\n    w.wcs.set()\n\n    missing_info = [\"CRVAL1\", \"CRVAL2\", \"PC1_1\", \"PC1_2\", \"PC2_1\", \"PC2_2\", \"CDELT1\", \"CDELT2\"]\n\n    for key in missing_info:\n        with pytest.raises(astropy.wcs.exceptions.WCSWarning):\n            w.to_header(relax=False, header=fits.Header())\n            \n        # Check that the to_header method returns the full header\n        w.to_header(relax=True, header=fits.Header())\n\n\n\n", "def test_wcs_with_bad_header():\n    with pytest.raises(fits.FITSError, match=\"Invalid value encountered in.*\"):\n        wcs.WCS(header=fits.Header())\n    \n    with pytest.raises(RuntimeError, match=\"CTYPE keyword not found\"):\n        wcs.WCS(header=fits.Header({\"SIMPLE\": True}))\n\n\n\n", "def test_wcs_time_interpolation():\n    w = wcs.WCS(naxis=3)\n    w.wcs.ctype = [\"RA---TAN\", \"DEC--TAN\", \"TIME\"]\n    w.wcs.cdelt = [0.1, 0.1, 0.01]\n    w.wcs.crval = [0.0, 0.0, 2459000.0]\n    w.wcs.set()\n\n    # Test interpolation of time coordinate\n    time_coords = np.array([2459000, 2459000.5, 2459001])\n    pix_coords_in = w.wcs_world2pix(\n        time_coords,\n        0,\n        1\n    )\n    pix_coords_out = w.wcs_world2pix(\n        time_coords,\n        0,\n        1,\n        interpolation=\"linear\"\n    )\n\n    assert_allclose(pix_coords_in, pix_coords_out)\n    # Test interpolation with a different method\n    pix_coords_out = w.wcs_world2pix(\n        time_coords,\n        0,\n        1,\n        interpolation=\"nearest\"\n    )\n    assert_allclose(pix_coords_in, pix_coords_out)\n\n\n"], "sample_231": ["    def test_sensitive_args_function_call(self):\n        \"\"\"\n        Sensitive variables don't leak when the decorated function is called\n        with sensitive arguments.\n        \"\"\"\n        with self.settings(DEBUG=True):\n            self.verify_safe_response(sensitive_args_function_caller, check_for_POST_params=False)\n        with self.settings(DEBUG=False):\n            self.verify_safe_response(sensitive_args_function_caller, check_for_POST_params=False)\n\n\n", "    def test_sensitive_variables_decorator_with_kwargs(self):\n\n        @sensitive_variables()\n            return password, kwargs\n\n        with self.settings(DEBUG=True):\n            response = self.client.post('/test_view/', data={'password': 'secret', **{'sensitive_var': 'value'}})\n            self.assertEqual(response.status_code, 200)\n            self.assertIn('XXXXXXXXXXXXXXXXXXXX', response.content)  \n            self.assertNotIn('secret', response.content)\n            self.assertNotIn('value', response.content)  \n            self.assertNotIn('sensitive_var', response.content)  \n", "    def test_sensitive_variables_nested_functions(self):\n        @sensitive_variables\n                return f'{password} {username}'\n            return inner_func('user')\n        self.assertEqual(outer_func('secret'), 'secret user')\n", "    def test_sensitive_variables_decorator_with_arguments(self):\n        @sensitive_variables(exclude=['cooked_eggs'])\n            return password, cooked_eggs, scrambled\n\n        response = self.client.post('/sensitive_function/', data={'password': 'super_secret', 'cooked_eggs': 'real_eggs', 'scrambled': 'yeah'})\n        self.assertContains(response, 'super_secret', status_code=200)\n        self.assertContains(response, 'real_eggs', status_code=200)\n        self.assertContains(response, 'yeah', status_code=200)\n\n\n\n        \n\n\n", "    def test_sensitive_variables_decorator_on_generator(self):\n        @sensitive_variables\n            yield 'sensitive data'\n\n        with self.settings(DEBUG=True):\n            gen = sensitive_generator()\n            self.assertEqual(next(gen), 'XXXXXXXXXXXXXXXXXXXX')\n"], "sample_625": ["def test_cross_dim_axis(a, b, ae, be, use_dask: bool) -> None:\n    if use_dask:\n        if not has_dask:\n            pytest.skip(\"requires dask\")\n        a = a.chunk()\n        b = b.chunk()\n\n    with pytest.raises(ValueError):\n        xr.cross(a, b, dim=\"invalid_dim\")\n\n    with pytest.raises(ValueError):\n        xr.cross(a, b, axis=-3)\n\n\n", "def test_cross_nd(use_dask: bool, a: xr.DataArray, b: xr.DataArray, expected: xr.DataArray) -> None:\n    if use_dask:\n        if not has_dask:\n            pytest.skip(\"test for dask.\")\n        a = a.chunk()\n        b = b.chunk()\n\n    actual = xr.cross(a, b, dim=1)\n    xr.testing.assert_duckarray_allclose(expected, actual)\n", "def test_cross_dims_mismatch(a, b, ae, be, dim: str, axis: int, use_dask: bool) -> None:\n    with pytest.raises(ValueError):\n        xr.cross(a, b, dim=dim)\n", "    def test_cross_axis(a, b, ae, be, dim: str, axis: int, use_dask: bool) -> None:\n        expected = np.cross(ae, be, axis=axis)\n\n        if use_dask:\n            if not has_dask:\n                pytest.skip(\"test for dask.\")\n            a = a.chunk()\n            b = b.chunk()\n\n        actual = xr.cross(a, b, dim=dim, axis=axis)\n        xr.testing.assert_duckarray_allclose(expected, actual)\n\n", "def test_cross_with_dataset(use_dask: bool) -> None:\n    if use_dask:\n        if not has_dask:\n            pytest.skip(\"test for dask.\")\n\n    ds_a = xr.Dataset({\"a\": ([\"x\"], [1, 2])})\n    ds_b = xr.Dataset({\"b\": ([\"x\"], [4, 5])})\n\n    with raise_if_dask_computes():\n        actual = xr.cross(ds_a, ds_b, dim=\"x\")\n\n    expected_a = xr.DataArray([0, 0], dims=[\"x\"], name=\"a\")\n    expected_b = xr.DataArray([0, 0], dims=[\"x\"], name=\"b\")\n    expected = xr.Dataset({\"a\": expected_a, \"b\": expected_b})\n    xr.testing.assert_identical(expected, actual)\n\n\n\n"], "sample_1144": ["compilation error", "    def test_requires_partial_undefined_symbols():\n        x, y, z = symbols('x y z')\n        # function of undefined symbols\n        f = x*y*z\n        assert requires_partial(Derivative(f, y)) is True\n", "def test_requires_partial_multiple_integer_free_symbols():\n    x, y, z, n = symbols('x y z n', integer=True)\n    f = x**n * y * z\n    assert requires_partial(Derivative(f, x)) is False\n    assert requires_partial(Derivative(f, y)) is True\n    assert requires_partial(Derivative(f, z)) is True\n", "compilation error", "def test_requires_partial_nested_derivatives():\n    x, y, z = symbols('x y z')\n    f = x*y*z\n    g = Derivative(f, x)\n    assert requires_partial(g) is True\n    assert requires_partial(Derivative(g, y)) is True\n    assert requires_partial(Derivative(g, z)) is True\n    h = Derivative(f, (x, y))\n    assert requires_partial(h) is True\n\n\n\n"], "sample_1099": ["def test_eval_partial_derivative_mixed_tensor_tensor_expr3():\n\n    expr1 = PartialDerivative(H(i, j), H(k, m))._perform_derivative()\n    expr2 = PartialDerivative(H(i, j)*tau , H(k, m))._perform_derivative()\n    expr3 = PartialDerivative(H(i, j)*A(k) , H(k, m))._perform_derivative()\n\n    assert (expr1 - L.delta(i, -k) * L.delta(j, -m)) == 0\n\n    assert (expr2 - L.delta(i, -k) * L.delta(j, -m) * tau) == 0\n\n    assert (expr3 - L.delta(i, -k) * L.delta(j, -m) * A(k)) == 0 \n\n", "def test_eval_partial_derivative_mixed_tensor_expr3():\n\n    tau, alpha = symbols(\"tau alpha\")\n    i, j, k, m, n = tensor_indices(\"i j k m n\", L)\n    A, B, C = tensor_heads(\"A B C\", [L])\n\n    base_expr3 = A(i)*B(j)*C(-i)*A(-j) + A(i)*B(i)*C(j)*H(-j, k)\n\n    tensor_derivative1 = PartialDerivative(base_expr3, A(k))._perform_derivative()\n    tensor_derivative2 = PartialDerivative(base_expr3, H(k, m))._perform_derivative()\n\n    assert (tensor_derivative1 -\n            (B(j)*C(-k)*A(-j) + B(j)*C(-k)*A(-j) + \n             B(k)*C(j)*A(j) + B(k)*C(j)*A(j))).expand() == 0\n\n    assert (tensor_derivative2 -\n            (A(i)*B(i)*C(j)*L.delta(j, -k))).expand() == 0\n", "def test_partial_derivative_contract_delta():\n    tau = symbols(\"tau\")\n\n    expr = PartialDerivative(A(i)*B(i), tau)\n\n    assert expr.contract_delta(L.delta).expand() == 0 \n\n    expr = PartialDerivative(A(i)*B(i), A(j))\n    assert expr.contract_delta(L.delta).expand() == \n           B(j)*L.delta(i, -j)\n\n    expr = PartialDerivative(A(i)*B(i), H(j, k))\n    assert expr.contract_delta(L.delta).expand() == 0\n\n    expr = PartialDerivative(A(i)*B(-i), H(j, k))\n    assert expr.contract_delta(L.delta).expand() == 0 \n    \n    expr = PartialDerivative(H(i, j)*A(i), H(k, m))\n\n    assert expr.contract_delta(L.delta).expand() - \\\n           L.delta(i, -k) * L.delta(j, -m) == 0\n", "compilation error", "def test_eval_partial_derivative_mixed_tensor_expr3():\n\n    tau, alpha = symbols(\"tau alpha\")\n\n    base_expr3 = A(i)*H(j, k) + H(i, j)*A(k)\n\n    tensor_derivative = PartialDerivative(base_expr3, H(m, n))._perform_derivative()\n    assert tensor_derivative - (L.delta(i, -m) * L.delta(j, -n) * A(k) +\n        L.delta(i, -m) * L.delta(k, -n) * A(j) +\n        L.delta(j, -m) * L.delta(i, -n) * A(k) +\n        L.delta(j, -m) * L.delta(k, -n) * A(i)) == 0 \n"], "sample_888": ["def test_iforest_contamination_auto():\n    \"\"\"Test that contamination = 'auto' works as expected.\"\"\"\n    rng = check_random_state(0)\n    X = rng.randn(1000, 20)\n\n    # Generate some outliers\n    X_outliers = X + 5 * rng.randn(100, 20)\n    X_all = np.vstack((X, X_outliers))\n\n    # Fit with contamination='auto'\n    iforest = IsolationForest(contamination='auto', random_state=rng)\n    iforest.fit(X_all)\n\n    # Check that the contamination estimate is reasonable\n    contamination = iforest.contamination_\n    assert 0.05 <= contamination <= 0.15  # Reasonable range\n    \n\n\n", "def test_iforest_sparse_invalid_data():\n    \"\"\"Test that Isolation Forest raises an error when fit with invalid sparse data types.\n    \"\"\"\n    from scipy import sparse\n\n    # Test with invalid sparse format\n    with pytest.raises(ValueError):\n        IsolationForest().fit(sparse.lil_matrix((2, 2)))\n\n", "def test_iforest_sparse_input_with_validation():\n    \"\"\" Test that sparse input works with data validation. \"\"\"\n\n    # Create a sparse matrix with some invalid data\n    sparse_matrix = csc_matrix(\n        [[1, 2, 3], [4, np.nan, 6], [7, 8, 9]]\n    )\n\n    # Try to fit a model on it\n    with pytest.raises(ValueError, match=\"Input data must be numeric\"):\n        IsolationForest().fit(sparse_matrix)\n\n    # Ensure that a good sparse matrix works\n    good_sparse_matrix = csc_matrix(\n        [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n    )\n    IsolationForest().fit(good_sparse_matrix)\n\n\n", "def test_iforest_contamination_auto_with_multiple_samples():\n    \"\"\"Test that contamination='auto' works correctly with multiple samples.\"\"\"\n    rng = check_random_state(0)\n    \n    for n_samples in [100, 500, 1000]:\n        X = rng.randn(n_samples, 2)\n        y = rng.rand(n_samples) < 0.2  # 20% outliers\n        iforest = IsolationForest(contamination=\"auto\")\n        iforest.fit(X)\n        assert 0.18 <= iforest.contamination <= 0.22\n        \n\n\n\n", "def test_iforest_sparse_fit_predict():\n    \"\"\"\n    Test that IsolationForest can fit on and predict from sparse data.\n\n    Issue #18132 introduced a bug where fit could fail when the input data\n    was sparse. This test ensures that fitting and predicting from sparse\n    data works correctly.\n    \"\"\"\n\n    rng = check_random_state(0)\n    n_samples = 100\n    n_features = 50\n    X_dense = rng.randn(n_samples, n_features)\n    X_sparse = csr_matrix(X_dense)\n\n    # Test fitting and predicting\n\n    iforest = IsolationForest(random_state=0)\n    iforest.fit(X_sparse)\n    predictions_sparse = iforest.predict(X_sparse)\n    predictions_dense = iforest.predict(X_dense)\n\n    assert_array_equal(predictions_sparse, predictions_dense)\n\n\n\n"], "sample_632": ["def test_ignore_signatures_pass_empty_function_with_docstring():\n    output = StringIO()\n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        similar.Run([\"--ignore-signatures\", EMPTY_FUNCTION_1, EMPTY_FUNCTION_2])\n    assert ex.value.code == 0\n    assert (\n        output.getvalue().strip()\n        == \"\"\"", "def test_ignore_signatures_fail_empty_functions():\n    output = StringIO()\n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        similar.Run([EMPTY_FUNCTION_1, EMPTY_FUNCTION_2])\n    assert ex.value.code == 0\n    assert (\n        output.getvalue().strip()\n        == (\n            '''", "    def test_empty_lines():\n        output = StringIO()\n        with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n            similar.Run([EMPTY_FUNCTION_1, EMPTY_FUNCTION_2])\n        assert ex.value.code == 0\n        assert (\n            output.getvalue().strip()\n            == \"\"\"", "def test_ignore_signatures_empty_functions_with_comments_pass():\n    output = StringIO()\n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        similar.Run([\"--ignore-signatures\", EMPTY_FUNCTION_1, EMPTY_FUNCTION_2])\n    assert ex.value.code == 0\n    assert (\n        output.getvalue().strip()\n        == \"\"\"", "def test_ignore_imports_multiline():\n    output = StringIO()\n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        similar.Run([\"--ignore-imports\", MULTILINE, MULTILINE])\n    assert ex.value.code == 0\n    assert (\n        output.getvalue().strip()\n        == \"\"\""], "sample_332": ["    def test_management_form_hidden_fields(self):\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-MAX_NUM_FORMS': '0',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '0',\n            'choices-1-choice': 'One',\n            'choices-1-votes': '1',\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        expected_html = (\n            '<input type=\"hidden\" name=\"choices-TOTAL_FORMS\" value=\"2\">' \n            '<input type=\"hidden\" name=\"choices-INITIAL_FORMS\" value=\"0\">'\n            '<input type=\"hidden\" name=\"choices-MIN_NUM_FORMS\" value=\"0\">'\n            '<input type=\"hidden\" name=\"choices-MAX_NUM_FORMS\" value=\"0\">'\n        )\n        self.assertEqual(formset.management_form.as_html(), expected_html)\n\n", "    def test_all_valid_with_different_prefixes(self):\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '0',\n            'choice2-TOTAL_FORMS': '2',\n            'choice2-INITIAL_FORMS': '0',\n            'choice2-MIN_NUM_FORMS': '0',\n            'choice2-0-choice': 'One',\n            'choice2-0-votes': '1',\n            'choice2-1-choice': 'Two',\n            'choice2-1-votes': '2',\n        }\n        ChoiceFormSet1 = formset_factory(Choice, prefix='choices')\n        ChoiceFormSet2 = formset_factory(Choice, prefix='choice2')\n        formset1 = ChoiceFormSet1(data, auto_id=False)\n        formset2 = ChoiceFormSet2(data, auto_id=False)\n        self.assertIs(all_valid((formset1, formset2)), True)\n        expected_errors = [{}, {}]\n        self.assertEqual(formset1._errors, expected_errors)\n        self.assertEqual(formset2._errors, expected_errors)\n\n\n", "    def test_all_valid_with_non_form_errors(self):\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '2',  # <-- conflicts with initial form count\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '0',\n            'choices-1-choice': 'One',\n            'choices-1-votes': '1',\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset1 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        formset2 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertIs(all_valid((formset1, formset2)), False)\n        expected_errors = [\n            {'__all__': [\n                'ManagementForm data is missing or has been tampered with. '\n                'Missing fields: form-TOTAL_FORMS, form-INITIAL_FORMS. '\n                'You may need to file a bug report if the issue persists.'\n            ]},\n            {'__all__': [\n                'ManagementForm data is missing or has been tampered with. '\n                'Missing fields: form-TOTAL_FORMS, form-INITIAL_FORMS. '\n                'You may need to file a bug report if the issue persists.'\n            ]},\n        ]\n        self.assertEqual(formset1._errors, expected_errors)\n        self.assertEqual(formset2._errors, expected_errors)\n", "    def test_formset_validation_with_none_initial_data(self):\n        data = {\n            'choices-TOTAL_FORMS': '1',\n            'form-INITIAL_FORMS': '0',\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertEqual(formset.initial_form_count(), 0)\n        self.assertEqual(formset.total_form_count(), 1)\n        self.assertFalse(formset.is_valid())\n        self.assertIsNotNone(formset.errors)\n        self.assertIsInstance(formset.errors, dict)\n\n\n\n        \n\n", "    def test_all_valid_empty_formset(self):\n        \"\"\"all_valid() returns True for an empty formset.\"\"\"\n        ChoiceFormSet = formset_factory(Choice, extra=0)\n        formset = ChoiceFormSet()\n        self.assertIs(all_valid((formset, formset)), True)\n"], "sample_1166": ["compilation error", "compilation error", "    def test_monomial_div_raises():\n        raises(ExactQuotientFailed, lambda: monomial_div((1, 2, 3), (0, 5, 6)))\n", "def test_Monomial_special_cases():\n    m = Monomial((0, 0, 0))\n    assert m.as_expr() == 1\n    assert m*Monomial((1, 2, 3)) == m\n    assert m/Monomial((1, 2, 3)) == m\n\n    m1 = Monomial((1, 0, 0))\n    assert m1.as_expr() == x\n    assert m1*Monomial((0, 0, 0)) == m1\n    assert m1/Monomial((0, 0, 0)) == m1\n\n    n1 = Monomial((0, 1, 0))\n    assert n1.as_expr() == y\n    assert n1*Monomial((0, 0, 0)) == n1\n    assert n1/Monomial((0, 0, 0)) == n1\n\n    m2 = Monomial((0, 0, 1))\n    assert m2.as_expr() == z\n    assert m2*Monomial((0, 0, 0)) == m2\n    assert m2/Monomial((0, 0, 0)) == m2\n", "compilation error"], "sample_165": ["    def test_empty_label_choicefield(self):\n        f = ModelChoiceField(\n            queryset=ChoiceModel.objects.all(),\n            empty_label=None,\n        )\n        self.assertIsNone(f.empty_label)\n", "    def test_choicefield_errors_with_custom_label(self):\n        class CustomChoiceField(ChoiceField):\n                return f'{obj}'\n\n        choice_field = CustomChoiceField(choices=[('a', 'One'), ('b', 'Two')], error_messages={'invalid_choice': 'invalid choice'})\n\n        form = Form({'field': 'c'})\n        self.assertFormErrors(['invalid choice'], form['field'].errors)\n", "    def test_empty_labels(self):\n        # Test that ModelChoiceField and ModelMultipleChoiceField\n        # render empty labels when used with empty choice lists.\n        class MyForm(Form):\n            choice = ModelChoiceField(queryset=ChoiceModel.objects.none(),\n                                    widget=forms.HiddenInput(),\n                                    required=False)\n            choices = ModelMultipleChoiceField(queryset=ChoiceModel.objects.none(),\n                                              widget=forms.HiddenInput(),\n                                              required=False)\n\n        # Test ModelChoiceField\n        form = MyForm()\n        self.assertHTMLEqual(form['choice'].render(), '<input type=\"hidden\" name=\"choice\" value=\"\"')\n\n        # Test ModelMultipleChoiceField\n        form = MyForm()\n        self.assertHTMLEqual(form['choices'].render(), '<input type=\"hidden\" name=\"choices\" value=\"\"')\n\n\n\n", "    def test_invalid_pk_value(self):\n        e = {\n            'invalid_pk_value': '\u201c%(pk)s\u201d is not a valid value.',\n        }\n        f = ModelMultipleChoiceField(queryset=ChoiceModel.objects.all(), error_messages=e)\n        with self.assertRaisesMessage(ValidationError, '\u201ca\u201d is not a valid value.'):\n            f.clean(['a', 'invalid'])\n\n\n\n", "    def test_empty_label_in_modelchoicefield(self):\n        f = ModelChoiceField(\n            queryset=ChoiceModel.objects.all(),\n            empty_label=None,\n        )\n        with self.assertRaises(ValueError):\n            f.widget.choices\n\n\n\n"], "sample_339": ["    def test_modelformset_factory_form_template_context(self):\n        class CustomForm(forms.Form):\n            test_field = forms.CharField()\n\n            return {'my_custom_var': 'Hello World'}\n\n        BookFormSet = modelformset_factory(Book, fields=\"__all__\", form_template_context=custom_context)\n        formset = BookFormSet()\n\n        formset_context = formset.get_formset_context()\n        self.assertIn('my_custom_var', formset_context)\n        self.assertEqual(formset_context['my_custom_var'], 'Hello World')\n", "    def test_inlineformset_factory_default_renderer(self):\n        BookFormSet = inlineformset_factory(Author, Book, fields='__all__')\n        formset = BookFormSet()\n        self.assertIsInstance(formset.renderer, forms.templates.DjangoTemplates) \n\n", "    def test_modelformset_factory_template_name_overrides(self):\n        BookFormSet = modelformset_factory(Book, fields=\"__all__\", template_name=\"test_form.html\")\n        self.assertEqual(BookFormSet().template_name, \"test_form.html\")\n\n\n\n", "    def test_modelformset_factory_passes_formset_template(self):\n        from django.forms import formset_factory\n        BookFormSet = modelformset_factory(Author, fields='__all__', formset_template='my_formset.html')\n        formset = BookFormSet()\n        self.assertEqual(formset.formset_template, 'my_formset.html')\n\n\n\n", "    def test_modelformset_factory_passes_template_name(self):\n        BookFormSet = modelformset_factory(Author, fields='__all__', template_name='my_template.html')\n        formset = BookFormSet()\n        self.assertEqual(formset.template_name, 'my_template.html')\n"], "sample_925": ["def test_mock_submodule_attributes():\n    with mock(['sphinx.unknown.module']):\n        import sphinx.unknown.module\n\n        assert sphinx.unknown.module.some_attribute == 'mocked'\n        assert sphinx.unknown.module.some_method() == 'mocked_result'\n", "    def test_mock_nested_modules():\n        with mock(['module1.module2.module3']):\n            import module1.module2.module3\n            assert 'module1.module2.module3' in sys.modules\n", "def test_mock_inheritance():\n    class Base:\n            return \"Base\"\n\n    with mock(['sphinx.unknown.module']):\n        class SubClass(Base, _MockObject):\n                return \"SubClass\"\n\n    obj = SubClass()\n    assert isinstance(obj, Base)\n    assert isinstance(obj, _MockObject)\n    assert obj.method() == \"SubClass\"\n", "def test_mock_inherited_attributes():\n    with mock(['sphinx.unknown']):\n        class Parent:\n            attr = _MockObject()\n                return _MockObject()\n\n        class Child(Parent):\n            pass\n\n        obj = Child()\n        assert isinstance(obj.attr, _MockObject)\n        assert isinstance(obj.method(), _MockObject)\n", "    def test_mock_import_error_no_match("], "sample_793": ["def test_iforest_sparse_dataset():\n    # test a sparse dataset\n    from sklearn.datasets import make_sparse_toy\n\n    X, y = make_sparse_toy(n_samples=100, n_features=100, random_state=42)\n    # train the model\n    clf = IsolationForest().fit(X)\n    # predict the labels\n    y_pred = clf.predict(X)\n    # check that the predictions are correct\n    assert_array_equal(y_pred, y)\n\n\n", "def test_iforest_n_estimators(n_estimators):\n    X = iris.data\n\n    clf = IsolationForest(n_estimators=n_estimators).fit(X)\n    assert clf.estimators_.shape[0] == n_estimators\n\n\n\n", "def test_iforest_sparse_threshold():\n    \"\"\"\n    Ensure threshold computation and predict works correctly on sparse input\n    \"\"\"\n    X_train = np.array([[0, 1], [1, 2]])\n    X_test = np.array([[2, 1], [1, 1]])\n    \n    # Test with sparse formats\n    X_train_sparse = csc_matrix(X_train)\n    X_test_sparse = csc_matrix(X_test)\n    \n    clf = IsolationForest().fit(X_train_sparse)\n    \n    # Predict using the trained sparse model\n    predictions = clf.predict(X_test_sparse)\n    \n    # Ensure predictions match dense input expectations\n    dense_clf = IsolationForest().fit(X_train)\n    dense_predictions = dense_clf.predict(X_test)\n    assert_array_equal(predictions, dense_predictions) \n\n    # Test predict_proba for consistency across dense and sparse\n    dense_proba = clf.predict_proba(X_test)\n    sparse_proba = clf.predict_proba(X_test_sparse)\n    assert_array_almost_equal(dense_proba, sparse_proba)\n\n\n\n", "def test_iforest_sparse_behaviour():\n    X_train_dense = iris.data\n    X_train_sparse = csc_matrix(X_train_dense)\n    X_test_dense = iris.data[:5]\n    X_test_sparse = csc_matrix(X_test_dense)\n\n    clf_dense = IsolationForest().fit(X_train_dense)\n    clf_sparse = IsolationForest().fit(X_train_sparse)\n\n    assert_array_equal(clf_dense.predict(X_test_dense), clf_sparse.predict(X_test_sparse))\n\n\n\n", "def test_iforest_sparse_max_samples_in_fit():\n    \"\"\"Check IsolationForest's behavior with sparse matrices and max_samples.\"\"\"\n    X_train = csc_matrix([[1, 2], [3, 4], [5, 6]])\n    X_test = csc_matrix([[7, 8], [9, 10]])\n\n    # Test max_samples is respected when fitting on sparse data\n    clf = IsolationForest(max_samples=0.5).fit(X_train)\n    assert_equal(clf._max_samples, 0.5 * X_train.shape[0])\n\n    # Test that predictions on sparse data use the fitted max_samples\n    clf.predict(X_test)\n\n\n\n"], "sample_47": ["    def test_cleanse_setting_preserves_non_sensitive_nested_keys(self):\n        initial = {'user': {'name': 'John Doe', 'email': 'john@example.com'}, 'password': 'secret'}\n        expected = {'user': {'name': 'John Doe', 'email': 'john@example.com'}, 'password': CLEANSED_SUBSTITUTE}\n        self.assertEqual(cleanse_setting('SETTING_NAME', initial), expected)\n", "    def test_cleanse_setting_with_nested_dictionaries(self):\n        initial = {\n            'user_data': {\n                'name': 'Alice',\n                'password': 'secret',\n                'address': {\n                    'street': '123 Main St',\n                    'city': 'Anytown'\n                }\n            }\n        }\n        expected = {\n            'user_data': {\n                'name': 'Alice',\n                'password': CLEANSED_SUBSTITUTE,\n                'address': {\n                    'street': '123 Main St',\n                    'city': 'Anytown'\n                }\n            }\n        }\n        self.assertEqual(cleanse_setting('SETTING_NAME', initial), expected) \n", "    def test_cleanse_setting_in_nested_objects(self):\n        initial = {'setting_name': {'sensitive_data': 'secret'}}\n        expected = {'setting_name': {'sensitive_data': CLEANSED_SUBSTITUTE}}\n        self.assertEqual(cleanse_setting('SETTING_NAME', initial), expected)\n", "    def test_cleanse_setting_handles_nested_dictionaries(self):\n        initial = {'user': {'name': 'jared', 'password': 'secret'}}\n        expected = {'user': {'name': 'jared', 'password': CLEANSED_SUBSTITUTE}}\n        self.assertEqual(cleanse_setting('SETTING_NAME', initial), expected)\n", "    def test_cleanse_setting_handles_nested_dictionaries(self):\n        initial = {\n            'level1': {\n                'level2': {\n                    'password': 'really_secret'\n                }\n            }\n        }\n        expected = {\n            'level1': {\n                'level2': {\n                    'password': CLEANSED_SUBSTITUTE\n                }\n            }\n        }\n        self.assertEqual(cleanse_setting('SETTING_NAME', initial), expected)\n"], "sample_1125": ["compilation error", "def test_operator_application():\n    from sympy.physics.quantum.operator import Operator\n    from sympy.physics.quantum.state import Ket\n\n    # Define a basic operator\n    A = Operator('A')\n\n    # Define a ket\n    psi = Ket('\u03c8')\n\n    # Apply the operator to the ket\n    result = A * psi\n\n    # Assert that the result is a valid ket\n    assert isinstance(result, Ket)\n\n", "def test_differential_operator():\n    from sympy.physics.quantum.state import Wavefunction\n    from sympy.physics.quantum.operator import DifferentialOperator\n    from sympy import Symbol, Function, Derivative\n\n    x = Symbol('x')\n    f = Function('f')\n    w = Wavefunction(x**2, x) \n\n    d = DifferentialOperator(Derivative(f(x), x), f(x))\n    assert qapply(d*w) == Wavefunction(2*x, x)\n\n", "compilation error", "def test_apply_operators():\n    from sympy.physics.quantum.state import Wavefunction\n\n    x = symbols('x')\n    f = Function('f')\n    w = Wavefunction(x**2, x)\n\n    d = DifferentialOperator(Derivative(f(x), x), f(x))\n\n    qapply(d*w)\n"], "sample_986": ["def test_issue_11013():\n    from sympy import asin, pi\n    assert asin(1).evalf() == pi/2\n    assert asin(-1).evalf() == -pi/2\n    assert asin(0).evalf() == 0\n", "def test_issue_11897():\n    from sympy import sqrt, integrate, Symbol\n    x = Symbol('x')\n    assert integrate(sqrt(x**2 + 1), (x, 0, 1)).evalf() == 0.8862269254527566\n", "def test_issue_10877():\n    a = sin(x)**2\n    assert a.evalf(subs={x: 1}) == 0.8414709848078966\n    b = sin(x)**2 + cos(x)**2\n    assert b.evalf(subs={x: 1}) == 1.0\n\n\n\n", "compilation error", "def test_issue_11070():\n    a = sin(pi*I)/2 + cos(pi*I)/3\n    assert a.evalf() == (1.j/2 + 1/3) \n    assert abs(a.evalf() - (1.j/2 + 1/3)) < 1e-15\n"], "sample_557": ["def test_gridspec_kwarg_propagation():\n    fig = plt.figure()\n\n    # Test that gridspec_kwarg is propagated correctly\n    gs = gridspec.GridSpec(3, 3, wspace=0.1, hspace=0.2)\n    ax1 = fig.add_subplot(gs[0, 0])\n    ax2 = fig.add_subplot(gs[1, :], gridspec_kw={'wspace': 0.3})\n    ax3 = fig.add_subplot(gs[:, 2], gridspec_kw={'hspace': 0.4})\n\n    assert ax1.axes.get_subplotspec().get_gridspec().wspace == 0.1\n    assert ax2.axes.get_subplotspec().get_gridspec().wspace == 0.3\n    assert ax3.axes.get_subplotspec().get_gridspec().hspace == 0.4\n\n", "def test_subplot_mosaic_with_subplots():\n    fig, axes = plt.subplots(2, 2)\n\n    with pytest.raises(ValueError, match=\"Subplots can not be mixed with subplot_mosaic\"):\n        plt.subplot_mosaic(\"ABCD\", ax=axes)\n\n\n\n\n", "def test_get_constrained_layout_pads_empty():\n    fig = plt.figure(layout=mpl.layout_engine.ConstrainedLayoutEngine())\n    with pytest.warns(PendingDeprecationWarning, match=\"will be deprecated\"):\n        assert fig.get_constrained_layout_pads() == (0.0, 0.0, 0.0, 0.0) \n", "def test_unpickle_with_subplot_idx():\n    fig = Figure()\n    ax = fig.add_subplot(1, 2, 1)\n    ax.plot([0, 1], [2, 3])\n    fig2 = pickle.loads(pickle.dumps(fig))\n    assert fig2.axes[0] is not ax\n    fig2.axes[0].set_yscale('linear')\n    assert fig2.axes[0].get_yscale() == 'linear'\n\n\n\n", "def test_subplots_index_with_shared_axis():\n    fig, axes = plt.subplots(2, 2, sharex=True, sharey=True)\n    assert len(axes) == 4\n    assert axes[0, 0].index == (0, 0)\n    assert axes[0, 1].index == (0, 1)\n    assert axes[1, 0].index == (1, 0)\n    assert axes[1, 1].index == (1, 1)\n\n    axes[0, 0].set_title('Top Left')\n    axes[0, 1].set_title('Top Right')\n    axes[1, 0].set_title('Bottom Left')\n    axes[1, 1].set_title('Bottom Right')\n    plt.show()\n"]}, "raw_scored_generations": {"sample_2": [false, false, false, false, false], "sample_4": [false, false, false, false, false], "sample_3": [false, true, false, false, false], "sample_1": [false, false, false, false, false], "sample_0": [false, false, false, false, false], "sample_5": [false, false, false, false, false], "sample_6": [false, false, false, false, false], "sample_7": [false, false, false, false, false], "sample_8": [false, false, false, false, false], "sample_9": [false, false, false, false, false], "sample_10": [false, false, false, false, false], "sample_11": [false, false, false, false, false], "sample_12": [false, false, false, true, false], "sample_13": [false, false, true, true, false], "sample_14": [false, false, false, false, false], "sample_15": [false, true, true, true, true], "sample_16": [false, true, false, false, false], "sample_17": [false, false, true, false, false], "sample_18": [false, false, true, false, false], "sample_19": [true, false, false, false, false], "sample_20": [false, false, false, true, false], "sample_21": [false, false, false, false, true], "sample_22": [false, false, false, false, false], "sample_23": [true, false, false, false, false], "sample_24": [false, false, false, false, false], "sample_25": [true, true, true, true, true], "sample_26": [true, false, false, false, false], "sample_27": [true, true, true, true, true], "sample_28": [true, true, true, true, true], "sample_29": [false, false, false, false, true], "sample_30": [false, false, true, false, false], "sample_31": [false, false, false, false, false], "sample_32": [true, true, true, true, false], "sample_33": [false, false, false, false, false], "sample_34": [false, false, false, false, false], "sample_35": [false, true, true, true, true], "sample_36": [true, false, true, false, false], "sample_37": [false, false, true, false, true], "sample_38": [false, false, false, false, false], "sample_39": [false, false, false, false, false], "sample_40": [false, false, false, false, false], "sample_41": [true, false, false, false, false], "sample_42": [false, false, false, false, false], "sample_43": [false, true, false, false, false], "sample_44": [false, false, false, false, false], "sample_45": [false, false, false, false, false], "sample_46": [false, false, false, false, false], "sample_47": [true, true, false, true, true], "sample_48": [false, false, true, false, false], "sample_49": [true, true, false, false, true], "sample_50": [true, false, false, false, false], "sample_51": [true, false, false, true, true], "sample_52": [false, false, false, true, false], "sample_54": [false, false, true, true, true], "sample_53": [false, true, false, false, false], "sample_58": [false, false, false, false, false], "sample_55": [false, false, false, false, false], "sample_56": [true, false, true, false, false], "sample_57": [false, false, false, false, false], "sample_59": [false, false, false, false, false], "sample_60": [false, false, false, false, false], "sample_61": [false, false, false, false, false], "sample_62": [true, false, false, false, true], "sample_63": [false, false, false, false, false], "sample_64": [false, false, false, false, false], "sample_65": [true, true, true, true, true], "sample_67": [false, false, true, true, false], "sample_66": [true, false, false, true, false], "sample_68": [false, false, true, false, false], "sample_69": [false, false, false, false, false], "sample_70": [false, false, false, false, false], "sample_71": [false, false, false, false, false], "sample_72": [false, false, false, false, false], "sample_73": [false, false, false, false, false], "sample_75": [false, false, false, false, false], "sample_74": [false, false, false, false, false], "sample_77": [false, false, false, false, false], "sample_76": [true, true, true, true, true], "sample_78": [false, true, false, false, false], "sample_79": [true, false, false, false, false], "sample_80": [false, false, false, false, true], "sample_82": [false, false, false, false, false], "sample_81": [false, false, false, false, false], "sample_83": [false, false, false, false, false], "sample_85": [false, false, false, false, false], "sample_84": [true, false, true, true, true], "sample_86": [false, false, false, false, false], "sample_88": [false, false, true, true, false], "sample_87": [false, false, true, false, false], "sample_89": [false, true, false, true, false], "sample_90": [false, false, false, true, false], "sample_91": [false, false, false, true, false], "sample_92": [true, false, false, false, false], "sample_93": [false, false, false, false, false], "sample_94": [false, false, false, false, false], "sample_95": [false, false, false, false, false], "sample_98": [false, false, false, false, false], "sample_96": [false, false, false, false, false], "sample_99": [false, false, false, false, false], "sample_97": [true, false, false, false, false], "sample_100": [false, false, false, false, false], "sample_102": [false, false, false, false, true], "sample_101": [false, false, false, false, false], "sample_103": [false, false, false, false, false], "sample_104": [false, false, false, false, false], "sample_107": [false, true, false, true, false], "sample_105": [false, false, false, false, false], "sample_106": [false, false, false, false, false], "sample_108": [false, false, false, false, false], "sample_109": [false, true, false, false, false], "sample_111": [true, true, true, true, true], "sample_110": [false, false, false, true, false], "sample_112": [false, true, false, false, false], "sample_113": [false, false, false, false, false], "sample_114": [false, false, false, false, false], "sample_115": [false, false, false, false, false], "sample_116": [true, false, false, false, false], "sample_117": [false, false, false, false, false], "sample_118": [false, false, false, false, true], "sample_119": [false, false, false, false, false], "sample_120": [false, false, false, false, false], "sample_121": [true, false, false, false, false], "sample_122": [false, false, true, false, false], "sample_123": [true, false, false, true, true], "sample_124": [false, true, false, true, false], "sample_125": [true, false, true, true, true], "sample_126": [true, false, false, false, false], "sample_127": [false, false, false, false, false], "sample_128": [false, false, false, false, false], "sample_129": [true, false, false, false, true], "sample_130": [false, false, false, false, false], "sample_131": [false, true, false, false, false], "sample_132": [false, false, false, false, false], "sample_133": [true, true, true, true, true], "sample_135": [false, false, false, false, false], "sample_134": [false, false, false, false, false], "sample_136": [true, false, false, false, false], "sample_139": [true, true, true, true, true], "sample_137": [false, false, false, false, false], "sample_138": [false, false, false, false, false], "sample_140": [false, false, false, false, false], "sample_141": [false, true, true, false, true], "sample_142": [false, false, false, false, false], "sample_143": [false, false, false, false, false], "sample_144": [false, false, false, false, false], "sample_145": [false, false, false, false, false], "sample_146": [false, false, false, false, false], "sample_147": [false, false, false, false, false], "sample_148": [false, false, false, false, false], "sample_151": [false, false, true, false, false], "sample_149": [false, false, false, true, false], "sample_152": [false, true, false, false, false], "sample_150": [false, false, false, true, false], "sample_153": [false, false, false, false, true], "sample_154": [true, true, true, false, false], "sample_155": [true, true, true, true, false], "sample_156": [false, true, false, false, false], "sample_157": [false, false, false, true, false], "sample_158": [false, false, false, false, true], "sample_159": [false, false, false, false, true], "sample_160": [false, false, false, false, false], "sample_161": [false, false, false, false, true], "sample_162": [true, true, true, true, true], "sample_163": [false, false, false, false, false], "sample_164": [false, false, false, false, true], "sample_165": [true, false, false, true, false], "sample_166": [true, true, true, true, false], "sample_167": [false, false, true, false, false], "sample_168": [false, false, false, false, false], "sample_169": [false, false, false, false, false], "sample_171": [false, false, true, false, false], "sample_170": [false, false, false, false, false], "sample_172": [true, false, false, false, true], "sample_173": [true, false, false, false, true], "sample_174": [false, false, false, false, false], "sample_175": [false, false, false, false, false], "sample_176": [false, false, false, false, false], "sample_177": [true, true, false, true, false], "sample_178": [true, false, false, false, false], "sample_180": [false, false, false, false, false], "sample_179": [false, false, false, false, false], "sample_182": [false, true, false, false, false], "sample_181": [false, false, false, false, false], "sample_183": [false, false, false, false, false], "sample_184": [false, false, false, false, false], "sample_185": [false, false, false, false, false], "sample_186": [false, false, false, false, false], "sample_187": [false, false, false, false, false], "sample_188": [false, false, false, false, false], "sample_189": [false, false, false, false, false], "sample_190": [true, false, false, false, false], "sample_191": [false, false, false, true, false], "sample_192": [false, false, false, true, false], "sample_193": [true, false, false, false, false], "sample_194": [false, false, false, true, false], "sample_195": [false, false, false, false, false], "sample_196": [false, false, true, false, false], "sample_198": [false, false, false, true, false], "sample_197": [false, false, false, false, false], "sample_199": [false, false, false, false, false], "sample_200": [false, false, false, false, false], "sample_201": [false, false, false, false, false], "sample_202": [false, false, false, false, false], "sample_203": [false, false, false, false, false], "sample_204": [false, false, false, false, false], "sample_205": [false, false, false, false, false], "sample_206": [false, false, false, true, false], "sample_207": [false, false, true, false, false], "sample_208": [false, false, false, false, false], "sample_209": [false, false, false, false, false], "sample_210": [false, false, false, false, false], "sample_211": [false, false, false, false, false], "sample_213": [false, true, false, false, true], "sample_212": [false, false, true, false, false], "sample_214": [false, false, false, false, false], "sample_215": [false, false, false, false, false], "sample_216": [false, false, false, false, false], "sample_217": [false, false, false, false, true], "sample_218": [false, false, false, false, false], "sample_219": [false, false, false, false, false], "sample_220": [false, true, false, true, false], "sample_221": [false, false, false, true, false], "sample_222": [false, true, true, true, false], "sample_223": [false, false, false, false, false], "sample_224": [false, false, false, false, false], "sample_225": [false, false, false, false, false], "sample_226": [false, false, false, true, false], "sample_227": [false, false, true, false, false], "sample_228": [false, false, false, false, false], "sample_229": [false, false, false, false, false], "sample_230": [true, false, false, false, true], "sample_231": [false, false, false, false, false], "sample_232": [false, false, false, false, false], "sample_233": [true, false, false, false, true], "sample_234": [false, false, true, false, false], "sample_235": [false, true, false, false, true], "sample_236": [false, false, false, false, false], "sample_237": [false, false, true, true, false], "sample_238": [false, false, false, true, false], "sample_239": [false, false, false, false, false], "sample_240": [true, false, false, false, true], "sample_241": [false, false, false, false, false], "sample_242": [false, false, false, false, false], "sample_243": [false, false, false, false, false], "sample_244": [false, false, false, false, false], "sample_245": [true, true, true, true, true], "sample_246": [true, true, true, true, true], "sample_247": [false, false, true, false, false], "sample_248": [true, false, true, false, true], "sample_249": [false, false, false, true, false], "sample_250": [false, false, true, false, true], "sample_251": [false, false, false, false, false], "sample_252": [false, false, false, false, false], "sample_253": [false, false, false, false, true], "sample_254": [true, true, true, true, false], "sample_256": [false, false, false, false, false], "sample_255": [false, false, false, false, false], "sample_257": [true, false, false, false, false], "sample_258": [false, false, false, false, false], "sample_259": [false, false, false, false, false], "sample_260": [false, false, true, true, false], "sample_261": [true, false, false, true, true], "sample_262": [false, false, false, false, false], "sample_263": [false, false, false, false, false], "sample_264": [false, false, false, false, false], "sample_265": [false, false, false, true, false], "sample_266": [true, false, false, false, false], "sample_267": [false, true, false, false, true], "sample_268": [false, false, false, false, false], "sample_269": [true, true, true, true, true], "sample_270": [false, false, false, false, false], "sample_271": [true, true, true, false, false], "sample_272": [true, false, false, false, false], "sample_273": [false, false, false, false, false], "sample_274": [false, false, true, false, false], "sample_275": [false, false, false, false, false], "sample_276": [false, false, true, false, true], "sample_277": [false, false, false, false, false], "sample_278": [false, true, false, true, false], "sample_279": [false, true, true, true, false], "sample_280": [false, false, false, false, false], "sample_281": [true, true, true, true, true], "sample_282": [false, false, false, false, true], "sample_283": [false, true, true, false, false], "sample_284": [false, false, true, false, false], "sample_285": [false, false, false, false, false], "sample_286": [false, false, false, true, false], "sample_287": [false, false, false, true, false], "sample_288": [false, false, false, false, false], "sample_289": [false, false, false, false, false], "sample_290": [false, false, false, false, false], "sample_291": [false, true, false, false, false], "sample_292": [false, false, false, false, true], "sample_293": [false, false, false, false, false], "sample_294": [false, false, false, false, true], "sample_295": [false, false, false, false, false], "sample_296": [false, false, false, true, false], "sample_297": [false, false, false, false, false], "sample_298": [true, false, false, true, true], "sample_299": [true, true, false, false, false], "sample_300": [false, false, false, true, false], "sample_301": [false, false, false, false, false], "sample_302": [false, false, false, false, false], "sample_303": [false, false, false, false, false], "sample_304": [true, false, false, false, true], "sample_305": [true, false, true, false, true], "sample_306": [true, false, false, false, false], "sample_307": [false, false, true, true, true], "sample_308": [false, false, false, true, false], "sample_309": [true, true, true, false, true], "sample_310": [false, false, false, false, false], "sample_312": [false, false, true, false, false], "sample_311": [false, true, false, false, true], "sample_313": [false, false, false, false, false], "sample_314": [true, false, false, false, false], "sample_315": [true, false, false, false, false], "sample_316": [true, false, true, false, false], "sample_317": [false, false, false, false, false], "sample_318": [false, false, false, false, false], "sample_319": [false, false, false, false, false], "sample_320": [true, false, false, false, false], "sample_321": [true, false, true, false, false], "sample_322": [false, false, false, false, false], "sample_323": [false, false, false, false, false], "sample_324": [false, false, false, false, true], "sample_325": [false, false, false, false, false], "sample_326": [false, false, false, false, false], "sample_327": [false, false, false, false, false], "sample_328": [false, false, true, false, false], "sample_329": [false, false, false, false, false], "sample_330": [true, true, true, false, true], "sample_331": [true, true, true, true, false], "sample_332": [false, false, false, false, false], "sample_333": [false, false, false, false, false], "sample_334": [false, true, false, false, false], "sample_335": [true, false, false, false, false], "sample_336": [false, false, false, false, false], "sample_337": [false, true, false, false, false], "sample_338": [false, true, true, false, true], "sample_339": [false, false, false, false, false], "sample_340": [false, false, false, false, false], "sample_341": [false, false, false, true, true], "sample_342": [true, true, true, true, false], "sample_343": [false, false, false, false, false], "sample_344": [true, false, false, true, false], "sample_345": [true, false, false, false, true], "sample_346": [false, false, false, false, false], "sample_347": [true, false, true, true, false], "sample_348": [false, false, false, false, false], "sample_349": [false, true, false, false, false], "sample_350": [false, false, false, false, false], "sample_351": [false, false, false, false, false], "sample_352": [false, false, false, false, false], "sample_353": [false, true, false, false, false], "sample_354": [false, false, false, false, false], "sample_355": [true, false, false, false, false], "sample_356": [false, false, true, false, false], "sample_357": [false, true, false, true, false], "sample_358": [false, true, false, false, false], "sample_359": [false, false, true, false, true], "sample_360": [false, false, false, false, false], "sample_361": [false, false, false, false, false], "sample_362": [false, false, false, true, false], "sample_363": [false, true, false, false, false], "sample_364": [false, false, false, false, false], "sample_365": [false, false, false, false, false], "sample_366": [false, false, true, false, false], "sample_367": [false, false, false, false, false], "sample_368": [false, false, true, false, false], "sample_369": [false, false, false, true, false], "sample_370": [false, false, false, false, false], "sample_371": [false, false, false, false, false], "sample_372": [false, false, false, false, false], "sample_373": [true, false, false, false, false], "sample_374": [false, false, false, false, false], "sample_375": [false, false, false, false, false], "sample_376": [false, false, false, false, false], "sample_377": [false, false, false, false, false], "sample_378": [false, false, true, false, false], "sample_379": [true, true, true, false, false], "sample_380": [false, false, false, false, false], "sample_381": [true, false, false, false, false], "sample_382": [false, true, false, false, false], "sample_383": [false, false, false, false, false], "sample_384": [false, true, false, false, true], "sample_385": [false, false, false, true, false], "sample_386": [false, true, false, false, false], "sample_387": [false, false, true, true, true], "sample_388": [false, false, false, false, true], "sample_389": [true, false, false, false, false], "sample_390": [true, false, false, false, false], "sample_391": [false, false, false, false, false], "sample_392": [true, false, false, false, false], "sample_393": [false, true, true, true, true], "sample_394": [true, false, true, true, false], "sample_395": [false, false, false, false, false], "sample_396": [false, false, false, false, false], "sample_397": [false, false, false, true, false], "sample_398": [false, false, false, false, false], "sample_399": [false, false, false, false, false], "sample_400": [false, false, false, false, false], "sample_401": [false, false, false, false, false], "sample_403": [true, false, true, false, false], "sample_402": [false, false, false, false, false], "sample_404": [false, true, true, false, false], "sample_405": [false, false, false, true, false], "sample_406": [false, false, false, false, false], "sample_407": [false, true, true, false, false], "sample_408": [false, false, false, false, false], "sample_409": [true, false, false, false, false], "sample_410": [false, false, false, false, true], "sample_411": [false, false, false, false, true], "sample_412": [false, false, false, false, false], "sample_413": [false, false, false, false, false], "sample_414": [false, false, false, false, true], "sample_415": [false, false, false, false, true], "sample_416": [true, false, false, true, false], "sample_417": [true, true, false, true, true], "sample_418": [false, false, false, false, false], "sample_419": [false, false, false, true, false], "sample_420": [false, false, false, false, false], "sample_421": [false, false, false, false, false], "sample_422": [false, false, false, false, false], "sample_423": [false, false, false, false, false], "sample_424": [true, false, true, false, false], "sample_425": [false, false, false, false, false], "sample_426": [false, false, false, true, true], "sample_427": [false, false, true, true, false], "sample_428": [false, false, false, false, false], "sample_429": [false, false, false, false, false], "sample_430": [false, false, false, false, false], "sample_431": [false, false, false, false, false], "sample_432": [true, true, true, true, true], "sample_433": [false, false, false, true, false], "sample_434": [false, false, false, true, false], "sample_435": [false, false, false, false, false], "sample_436": [false, false, false, false, false], "sample_437": [false, false, false, false, false], "sample_438": [false, false, false, false, false], "sample_439": [false, false, false, false, false], "sample_440": [false, true, false, true, false], "sample_441": [false, false, false, false, false], "sample_442": [true, true, false, false, false], "sample_443": [true, false, false, false, false], "sample_444": [false, true, false, false, false], "sample_445": [false, false, false, false, false], "sample_446": [true, true, true, false, true], "sample_447": [false, false, false, false, true], "sample_448": [false, false, false, true, false], "sample_449": [false, false, false, false, false], "sample_450": [false, false, false, false, false], "sample_451": [false, false, false, false, false], "sample_453": [false, false, false, false, false], "sample_452": [false, false, false, true, false], "sample_454": [true, false, false, false, false], "sample_455": [false, false, false, true, false], "sample_456": [false, true, false, false, false], "sample_457": [false, false, false, false, false], "sample_458": [true, false, true, false, true], "sample_459": [true, true, true, false, true], "sample_460": [true, true, true, true, true], "sample_461": [false, false, false, false, false], "sample_462": [true, false, true, false, false], "sample_463": [true, false, false, false, false], "sample_464": [true, false, false, true, false], "sample_465": [false, false, false, false, false], "sample_466": [false, false, false, false, false], "sample_467": [false, false, false, false, false], "sample_469": [true, false, false, false, false], "sample_468": [false, false, false, false, false], "sample_470": [false, false, false, false, false], "sample_471": [false, false, false, false, false], "sample_472": [false, false, true, false, true], "sample_473": [false, false, false, false, false], "sample_474": [false, false, false, false, false], "sample_475": [false, false, false, false, false], "sample_476": [false, false, false, false, false], "sample_477": [false, false, false, false, false], "sample_478": [false, false, false, false, false], "sample_479": [false, false, false, false, false], "sample_480": [false, true, false, false, false], "sample_481": [false, false, false, false, false], "sample_482": [false, false, false, false, false], "sample_483": [false, false, false, false, true], "sample_484": [true, true, true, true, false], "sample_485": [false, false, false, false, true], "sample_486": [false, false, false, false, false], "sample_487": [false, false, false, false, false], "sample_488": [true, false, false, false, false], "sample_489": [false, false, false, true, false], "sample_490": [false, false, false, false, false], "sample_491": [false, true, false, false, false], "sample_492": [false, false, false, false, false], "sample_493": [false, false, false, true, false], "sample_494": [false, false, false, false, false], "sample_495": [false, true, false, false, false], "sample_496": [true, true, false, false, false], "sample_497": [false, false, false, true, true], "sample_498": [false, false, false, false, false], "sample_499": [true, false, true, false, false], "sample_500": [false, true, false, false, false], "sample_501": [false, false, true, false, false], "sample_502": [true, false, false, false, false], "sample_503": [false, false, false, false, false], "sample_504": [false, false, false, false, false], "sample_505": [false, false, false, false, false], "sample_506": [false, false, false, false, false], "sample_507": [false, false, false, true, true], "sample_508": [false, false, false, false, false], "sample_509": [false, false, false, false, false], "sample_510": [true, false, false, true, false], "sample_511": [false, false, true, false, false], "sample_512": [false, true, false, false, false], "sample_513": [false, false, false, false, false], "sample_514": [false, false, false, false, false], "sample_515": [false, false, false, false, false], "sample_516": [false, false, false, false, false], "sample_517": [false, false, false, false, false], "sample_518": [true, false, true, false, true], "sample_519": [false, false, false, false, false], "sample_520": [false, false, false, false, false], "sample_521": [false, false, false, false, false], "sample_522": [false, false, false, false, false], "sample_523": [false, false, false, false, false], "sample_524": [false, false, false, false, false], "sample_525": [false, false, false, false, true], "sample_526": [false, false, false, false, false], "sample_527": [false, false, false, false, false], "sample_528": [true, true, false, false, false], "sample_529": [true, false, false, false, true], "sample_530": [false, false, false, false, false], "sample_531": [false, false, false, false, false], "sample_532": [false, false, false, false, true], "sample_533": [false, true, false, false, false], "sample_534": [false, false, false, false, false], "sample_535": [false, false, false, false, false], "sample_536": [false, false, false, false, false], "sample_537": [false, false, false, true, false], "sample_538": [false, false, false, true, false], "sample_539": [false, false, false, false, false], "sample_540": [false, false, false, false, false], "sample_541": [false, false, false, false, false], "sample_542": [false, false, true, false, false], "sample_543": [false, false, false, false, false], "sample_544": [false, false, false, false, false], "sample_545": [false, false, false, false, false], "sample_546": [false, false, false, false, false], "sample_547": [false, false, false, false, false], "sample_548": [false, false, false, false, false], "sample_549": [false, false, false, false, false], "sample_550": [false, false, false, false, false], "sample_551": [false, false, false, false, false], "sample_552": [false, false, false, false, false], "sample_553": [false, false, false, false, false], "sample_554": [false, false, false, false, false], "sample_555": [false, false, false, false, false], "sample_556": [false, false, false, false, false], "sample_557": [false, false, false, true, false], "sample_558": [false, false, false, false, false], "sample_559": [false, false, false, false, false], "sample_560": [true, false, false, false, false], "sample_561": [false, false, false, false, false], "sample_562": [false, false, false, true, false], "sample_563": [true, false, false, false, false], "sample_564": [true, false, true, false, false], "sample_565": [false, false, false, false, false], "sample_566": [false, false, false, false, false], "sample_567": [false, false, false, false, false], "sample_568": [false, false, true, false, false], "sample_569": [false, false, false, true, false], "sample_570": [false, true, false, false, false], "sample_571": [false, false, false, false, false], "sample_572": [false, false, false, false, false], "sample_573": [true, true, false, false, false], "sample_574": [false, false, false, false, false], "sample_575": [false, false, false, false, false], "sample_576": [false, false, false, false, false], "sample_577": [false, false, false, false, false], "sample_578": [false, false, false, false, false], "sample_579": [true, true, false, true, true], "sample_580": [true, true, true, false, false], "sample_581": [false, false, false, false, false], "sample_582": [false, false, false, false, false], "sample_583": [false, false, false, false, false], "sample_584": [false, false, false, false, true], "sample_585": [false, true, false, false, false], "sample_586": [false, false, false, false, false], "sample_587": [false, false, false, false, false], "sample_588": [false, false, false, false, false], "sample_589": [false, false, false, false, false], "sample_590": [false, false, false, false, false], "sample_591": [true, false, false, false, true], "sample_592": [false, false, true, false, true], "sample_593": [false, false, false, false, true], "sample_594": [false, true, true, false, true], "sample_595": [true, false, false, true, true], "sample_596": [false, true, false, true, true], "sample_597": [false, false, false, false, true], "sample_598": [false, true, true, false, true], "sample_599": [true, false, false, false, false], "sample_600": [false, false, true, false, false], "sample_601": [false, false, false, false, true], "sample_602": [false, false, false, false, false], "sample_603": [false, false, false, false, false], "sample_604": [true, true, true, true, false], "sample_605": [false, false, true, false, false], "sample_606": [false, true, true, false, true], "sample_607": [false, true, false, false, false], "sample_608": [false, true, false, true, true], "sample_609": [false, false, false, false, false], "sample_610": [false, false, false, false, false], "sample_611": [false, false, true, false, false], "sample_612": [false, true, false, false, false], "sample_613": [false, false, false, false, false], "sample_614": [true, true, true, false, false], "sample_615": [true, false, true, false, false], "sample_616": [false, false, false, false, false], "sample_617": [false, false, true, false, false], "sample_618": [false, false, false, true, false], "sample_619": [false, false, false, false, false], "sample_620": [true, false, false, false, false], "sample_621": [false, false, false, false, false], "sample_622": [true, false, false, true, true], "sample_623": [false, false, false, false, false], "sample_624": [false, false, false, false, false], "sample_625": [false, false, false, true, false], "sample_626": [false, false, true, false, false], "sample_627": [false, false, false, false, false], "sample_628": [true, false, false, false, false], "sample_629": [false, true, true, false, false], "sample_630": [false, false, false, false, false], "sample_631": [false, false, false, true, false], "sample_632": [false, false, false, false, false], "sample_633": [false, false, false, false, false], "sample_634": [false, false, false, false, false], "sample_635": [false, false, false, false, false], "sample_636": [false, false, false, false, false], "sample_637": [true, true, true, false, false], "sample_638": [false, false, false, false, false], "sample_639": [false, false, true, false, false], "sample_640": [false, false, true, false, false], "sample_641": [false, false, false, false, false], "sample_642": [false, false, false, false, false], "sample_643": [false, false, false, false, false], "sample_644": [false, false, false, false, false], "sample_645": [false, false, true, false, false], "sample_646": [true, false, false, false, false], "sample_647": [false, false, true, false, false], "sample_648": [false, false, false, false, false], "sample_649": [false, false, false, false, false], "sample_650": [false, false, false, false, false], "sample_651": [false, false, false, false, false], "sample_652": [false, false, false, false, false], "sample_653": [false, false, false, false, true], "sample_654": [false, true, false, false, true], "sample_655": [false, false, false, true, true], "sample_656": [false, false, false, false, false], "sample_657": [false, true, false, false, false], "sample_658": [true, false, true, true, true], "sample_659": [false, false, false, false, false], "sample_660": [false, false, false, false, false], "sample_661": [false, false, false, false, false], "sample_662": [false, false, false, false, false], "sample_663": [false, false, false, false, false], "sample_664": [false, false, false, false, false], "sample_665": [false, false, false, false, false], "sample_666": [false, false, false, false, true], "sample_667": [false, true, true, true, false], "sample_668": [false, false, false, false, false], "sample_669": [false, false, false, false, true], "sample_670": [false, false, false, false, true], "sample_671": [false, false, true, false, false], "sample_672": [false, false, false, false, false], "sample_673": [true, true, true, true, true], "sample_674": [false, false, false, false, false], "sample_675": [false, false, false, false, false], "sample_676": [false, false, false, false, false], "sample_677": [false, false, false, false, false], "sample_678": [false, false, false, false, false], "sample_679": [false, false, false, false, true], "sample_680": [true, false, false, false, false], "sample_681": [true, false, false, true, true], "sample_682": [false, false, false, false, false], "sample_683": [false, false, false, false, false], "sample_684": [true, true, true, true, true], "sample_685": [false, false, true, false, false], "sample_686": [false, false, false, false, false], "sample_687": [false, true, false, true, true], "sample_688": [false, true, true, true, true], "sample_689": [false, false, false, false, true], "sample_690": [false, false, false, false, false], "sample_691": [true, false, true, false, false], "sample_692": [false, false, false, false, true], "sample_693": [false, false, true, true, true], "sample_694": [false, false, false, false, false], "sample_695": [false, false, false, false, false], "sample_696": [false, false, false, false, false], "sample_697": [true, false, false, false, false], "sample_698": [false, false, false, false, false], "sample_699": [false, false, true, true, false], "sample_700": [false, false, false, false, false], "sample_701": [false, false, false, false, false], "sample_702": [false, false, false, false, false], "sample_703": [false, false, false, true, false], "sample_704": [false, false, false, false, false], "sample_705": [false, false, false, false, false], "sample_706": [false, false, false, false, false], "sample_707": [false, false, false, false, false], "sample_708": [false, true, false, false, false], "sample_709": [false, false, false, false, false], "sample_710": [false, true, false, true, true], "sample_711": [false, false, false, false, false], "sample_712": [true, false, false, false, false], "sample_713": [false, false, true, false, false], "sample_714": [false, false, false, false, false], "sample_715": [false, true, false, false, false], "sample_716": [false, false, true, false, false], "sample_717": [true, true, true, true, true], "sample_718": [false, false, false, false, false], "sample_719": [false, true, false, false, true], "sample_720": [false, false, false, false, false], "sample_721": [false, true, false, false, true], "sample_722": [false, false, false, false, false], "sample_723": [false, false, false, false, true], "sample_724": [false, false, true, false, false], "sample_725": [false, false, false, false, false], "sample_726": [false, false, false, false, true], "sample_727": [true, false, false, false, false], "sample_728": [true, true, false, true, true], "sample_729": [false, false, false, false, false], "sample_730": [false, false, false, false, false], "sample_731": [false, false, false, false, false], "sample_732": [false, false, false, false, true], "sample_733": [true, false, true, false, false], "sample_734": [false, false, false, false, false], "sample_735": [false, false, false, false, false], "sample_736": [false, true, false, false, false], "sample_737": [false, false, false, true, false], "sample_738": [false, false, false, false, false], "sample_739": [false, false, false, false, true], "sample_740": [false, false, false, false, false], "sample_741": [false, false, false, false, false], "sample_742": [true, false, false, false, false], "sample_743": [false, false, false, false, false], "sample_744": [false, false, false, false, false], "sample_745": [false, true, false, true, false], "sample_746": [false, false, false, false, false], "sample_747": [true, false, false, false, false], "sample_748": [false, false, false, false, false], "sample_749": [false, false, false, false, false], "sample_750": [false, false, false, false, false], "sample_751": [false, false, false, false, false], "sample_752": [false, false, false, false, false], "sample_753": [false, true, false, false, true], "sample_754": [false, false, false, false, false], "sample_755": [false, false, false, false, false], "sample_756": [true, false, false, true, false], "sample_757": [false, false, false, true, true], "sample_758": [false, false, false, false, false], "sample_759": [true, true, false, false, false], "sample_760": [false, false, false, false, true], "sample_761": [false, false, false, false, false], "sample_762": [false, true, false, false, true], "sample_763": [false, false, false, false, false], "sample_764": [false, false, false, false, false], "sample_765": [false, true, false, false, false], "sample_766": [false, false, false, false, false], "sample_767": [false, false, false, false, false], "sample_768": [false, false, false, false, false], "sample_769": [false, false, true, false, false], "sample_770": [false, false, true, false, false], "sample_771": [false, false, false, false, false], "sample_772": [true, true, false, false, false], "sample_773": [false, false, false, false, false], "sample_774": [false, false, false, false, false], "sample_775": [false, false, false, false, false], "sample_776": [false, false, false, false, false], "sample_777": [false, true, false, false, false], "sample_778": [false, true, false, false, false], "sample_779": [false, false, false, false, false], "sample_780": [false, false, true, false, true], "sample_781": [false, false, true, false, false], "sample_782": [false, false, false, false, false], "sample_783": [false, false, false, false, false], "sample_784": [false, false, false, false, false], "sample_785": [false, false, false, false, false], "sample_786": [false, false, false, false, false], "sample_787": [false, true, false, false, false], "sample_788": [false, false, false, false, false], "sample_789": [false, false, false, false, false], "sample_790": [false, false, false, false, false], "sample_791": [false, false, false, false, false], "sample_792": [false, false, true, false, false], "sample_793": [false, false, false, false, false], "sample_794": [false, false, false, true, true], "sample_795": [false, false, false, false, false], "sample_796": [true, true, true, false, false], "sample_797": [false, false, false, false, false], "sample_798": [true, false, false, false, false], "sample_799": [false, false, false, false, false], "sample_800": [false, false, false, false, false], "sample_801": [false, false, false, false, false], "sample_802": [false, false, false, false, false], "sample_803": [false, false, false, false, false], "sample_804": [false, false, false, false, true], "sample_805": [false, false, false, false, false], "sample_806": [false, false, false, false, false], "sample_807": [true, false, false, true, false], "sample_808": [true, true, false, false, false], "sample_809": [false, false, false, false, false], "sample_810": [false, false, false, false, false], "sample_811": [false, true, true, true, false], "sample_812": [false, false, false, false, false], "sample_813": [false, false, false, false, false], "sample_814": [false, false, false, false, false], "sample_815": [false, false, true, false, false], "sample_816": [false, false, false, false, false], "sample_817": [false, false, true, false, false], "sample_818": [true, false, false, false, false], "sample_819": [false, false, false, false, false], "sample_820": [false, false, false, false, false], "sample_821": [false, true, true, false, false], "sample_822": [false, false, false, true, true], "sample_823": [false, false, false, false, true], "sample_824": [true, true, false, false, false], "sample_825": [false, false, false, false, false], "sample_826": [false, false, false, false, false], "sample_827": [false, false, false, false, false], "sample_828": [false, true, false, true, true], "sample_829": [true, false, true, false, false], "sample_830": [true, true, false, false, true], "sample_831": [false, false, false, false, false], "sample_832": [false, false, true, false, true], "sample_833": [false, false, false, false, false], "sample_834": [false, false, true, true, false], "sample_835": [false, false, false, true, false], "sample_836": [false, false, false, false, false], "sample_837": [true, false, false, true, false], "sample_838": [false, false, false, false, false], "sample_839": [false, false, false, false, false], "sample_840": [false, false, false, false, false], "sample_841": [true, false, false, true, false], "sample_842": [false, false, false, false, false], "sample_843": [false, false, false, false, false], "sample_844": [false, true, false, false, false], "sample_845": [false, false, false, false, false], "sample_846": [true, false, true, false, false], "sample_847": [false, true, true, true, true], "sample_848": [false, false, false, false, false], "sample_849": [false, false, false, false, false], "sample_850": [false, true, true, true, true], "sample_851": [false, false, false, false, true], "sample_852": [false, true, false, false, false], "sample_853": [false, true, false, false, false], "sample_854": [false, false, true, false, false], "sample_855": [false, false, false, false, false], "sample_856": [false, false, false, false, true], "sample_857": [false, false, false, true, false], "sample_858": [false, false, false, false, false], "sample_859": [false, false, true, false, false], "sample_860": [false, false, false, false, false], "sample_861": [false, false, false, false, false], "sample_862": [false, false, false, false, false], "sample_863": [false, false, false, false, false], "sample_864": [false, false, false, true, false], "sample_865": [false, false, false, false, false], "sample_866": [false, false, false, false, false], "sample_867": [false, false, false, true, true], "sample_868": [false, false, false, false, false], "sample_869": [false, false, false, true, true], "sample_870": [true, false, false, false, false], "sample_871": [false, true, false, true, false], "sample_872": [false, false, true, false, false], "sample_873": [true, true, true, true, true], "sample_874": [false, false, false, false, false], "sample_875": [false, false, true, false, false], "sample_876": [false, false, false, false, false], "sample_877": [false, true, false, false, false], "sample_878": [false, false, false, false, false], "sample_879": [false, false, false, false, false], "sample_880": [false, false, false, false, false], "sample_881": [false, false, false, false, false], "sample_882": [false, false, false, false, false], "sample_883": [false, false, false, false, false], "sample_884": [true, true, false, true, true], "sample_885": [false, false, false, false, false], "sample_886": [false, false, true, true, false], "sample_887": [false, false, false, false, false], "sample_888": [false, false, false, false, true], "sample_889": [false, false, false, true, false], "sample_890": [false, false, false, false, false], "sample_891": [false, false, false, false, false], "sample_892": [false, true, false, false, false], "sample_893": [false, false, false, false, false], "sample_894": [false, false, false, false, false], "sample_895": [false, false, false, false, false], "sample_896": [false, false, false, true, false], "sample_897": [false, false, false, false, false], "sample_898": [false, false, false, false, false], "sample_899": [false, false, false, false, false], "sample_900": [false, false, false, false, false], "sample_901": [false, false, true, true, true], "sample_902": [false, false, false, false, false], "sample_903": [false, false, true, true, false], "sample_904": [false, true, false, false, false], "sample_905": [false, false, false, false, false], "sample_906": [false, false, true, false, false], "sample_907": [false, false, false, false, false], "sample_908": [false, false, true, false, false], "sample_909": [false, false, false, false, false], "sample_910": [true, true, false, true, true], "sample_911": [false, false, false, false, false], "sample_912": [true, false, false, false, false], "sample_913": [true, true, false, false, true], "sample_914": [true, false, false, true, false], "sample_915": [true, false, false, false, false], "sample_916": [false, false, false, false, false], "sample_917": [false, false, false, false, false], "sample_918": [false, false, false, true, false], "sample_919": [false, false, false, false, false], "sample_920": [false, false, false, false, false], "sample_921": [false, false, false, false, false], "sample_922": [true, false, false, true, true], "sample_923": [false, false, false, false, false], "sample_924": [false, false, false, false, false], "sample_925": [false, true, false, false, false], "sample_926": [false, false, false, false, false], "sample_927": [false, false, false, false, false], "sample_928": [false, false, false, false, false], "sample_929": [false, false, false, false, false], "sample_930": [false, false, false, false, false], "sample_931": [false, false, false, false, false], "sample_932": [false, false, false, false, false], "sample_933": [false, false, false, false, false], "sample_934": [false, false, false, false, false], "sample_935": [false, false, false, false, false], "sample_936": [true, true, false, true, false], "sample_937": [false, false, false, false, true], "sample_938": [false, false, false, false, false], "sample_939": [true, true, true, true, false], "sample_940": [false, false, false, false, false], "sample_941": [false, false, true, false, false], "sample_942": [false, false, false, false, false], "sample_943": [false, false, true, true, false], "sample_944": [false, true, true, false, true], "sample_945": [false, false, false, false, false], "sample_946": [false, false, false, false, false], "sample_947": [false, false, false, false, false], "sample_948": [false, false, false, false, false], "sample_949": [false, false, false, false, false], "sample_950": [false, false, false, false, false], "sample_951": [false, false, false, false, false], "sample_952": [false, false, false, false, false], "sample_953": [true, false, false, false, true], "sample_954": [false, false, false, false, false], "sample_955": [true, true, true, true, true], "sample_956": [true, false, true, true, false], "sample_957": [false, true, false, false, false], "sample_958": [false, false, false, false, false], "sample_959": [true, true, false, false, false], "sample_960": [false, false, false, false, true], "sample_961": [false, true, false, false, false], "sample_962": [false, false, false, true, false], "sample_963": [false, true, false, false, false], "sample_964": [false, false, false, false, false], "sample_965": [false, false, false, false, false], "sample_966": [false, false, false, false, false], "sample_967": [false, false, false, false, false], "sample_968": [false, true, false, false, false], "sample_969": [false, false, true, false, false], "sample_970": [false, false, false, false, false], "sample_971": [true, true, false, false, true], "sample_972": [false, true, false, false, false], "sample_973": [false, false, false, false, false], "sample_974": [false, false, false, true, false], "sample_975": [true, false, false, false, true], "sample_976": [false, true, false, false, false], "sample_977": [false, false, true, false, false], "sample_978": [false, false, false, true, false], "sample_979": [false, true, false, true, false], "sample_980": [false, false, false, false, false], "sample_981": [false, false, false, true, false], "sample_982": [false, false, false, false, false], "sample_983": [false, false, false, false, false], "sample_984": [false, false, false, true, true], "sample_985": [false, false, false, false, true], "sample_986": [false, false, false, false, false], "sample_987": [false, true, false, true, false], "sample_988": [true, false, true, false, false], "sample_989": [false, false, false, false, false], "sample_990": [false, false, false, false, false], "sample_991": [false, false, false, false, false], "sample_992": [false, false, false, false, false], "sample_993": [false, false, true, true, false], "sample_994": [false, false, false, true, true], "sample_995": [true, false, true, false, false], "sample_996": [false, false, false, false, false], "sample_997": [false, false, false, false, false], "sample_998": [false, false, false, false, false], "sample_999": [false, false, false, false, false], "sample_1000": [false, false, false, false, false], "sample_1001": [true, false, true, false, true], "sample_1002": [false, false, false, false, true], "sample_1003": [false, false, false, false, false], "sample_1004": [false, false, false, false, true], "sample_1005": [false, false, false, false, false], "sample_1006": [true, false, false, false, false], "sample_1007": [false, false, false, false, false], "sample_1008": [false, false, false, false, false], "sample_1009": [false, false, false, false, false], "sample_1010": [true, false, false, false, false], "sample_1011": [false, false, false, false, false], "sample_1012": [false, false, false, false, false], "sample_1013": [false, false, false, false, false], "sample_1014": [true, false, false, false, false], "sample_1015": [false, false, false, false, false], "sample_1016": [false, false, false, true, false], "sample_1017": [false, false, false, false, false], "sample_1018": [false, false, false, false, false], "sample_1019": [true, true, false, false, false], "sample_1020": [false, false, false, false, false], "sample_1021": [false, false, false, false, false], "sample_1022": [false, false, false, false, false], "sample_1023": [false, false, false, false, false], "sample_1024": [false, false, false, false, false], "sample_1025": [false, false, false, false, false], "sample_1026": [false, false, false, false, false], "sample_1027": [false, false, false, false, false], "sample_1028": [true, true, true, false, true], "sample_1029": [false, false, false, false, false], "sample_1030": [false, false, false, false, false], "sample_1031": [false, true, false, false, false], "sample_1032": [false, true, true, false, true], "sample_1033": [false, true, false, false, true], "sample_1034": [true, false, false, false, false], "sample_1035": [false, false, false, false, false], "sample_1036": [true, true, false, false, false], "sample_1037": [false, false, true, false, false], "sample_1038": [false, false, false, false, false], "sample_1039": [false, false, false, false, false], "sample_1040": [false, false, false, false, false], "sample_1041": [false, false, false, true, false], "sample_1042": [false, true, false, false, false], "sample_1043": [false, false, false, false, false], "sample_1044": [false, true, false, false, true], "sample_1045": [false, false, false, false, false], "sample_1046": [false, false, false, false, false], "sample_1047": [false, true, true, true, false], "sample_1048": [false, false, false, false, false], "sample_1049": [false, false, false, false, false], "sample_1050": [false, false, false, false, false], "sample_1051": [false, false, false, false, false], "sample_1052": [false, false, false, false, false], "sample_1053": [false, false, false, false, false], "sample_1054": [false, false, false, true, false], "sample_1055": [false, false, true, false, false], "sample_1056": [false, false, false, false, false], "sample_1057": [false, false, false, false, false], "sample_1058": [false, false, false, false, false], "sample_1059": [false, false, false, false, false], "sample_1060": [false, false, false, true, false], "sample_1061": [false, false, false, false, false], "sample_1062": [false, false, false, false, false], "sample_1063": [true, false, true, true, true], "sample_1064": [true, false, true, false, true], "sample_1065": [false, false, false, false, false], "sample_1066": [false, false, false, false, false], "sample_1067": [false, false, false, true, false], "sample_1068": [true, false, false, false, false], "sample_1069": [false, false, false, false, true], "sample_1070": [true, false, false, false, true], "sample_1071": [false, false, false, false, false], "sample_1072": [false, false, false, false, false], "sample_1073": [false, false, false, false, true], "sample_1074": [false, false, false, false, false], "sample_1075": [false, false, false, false, false], "sample_1076": [false, true, true, false, false], "sample_1077": [false, false, false, false, false], "sample_1078": [false, true, false, true, true], "sample_1079": [false, false, false, false, false], "sample_1080": [false, true, false, false, false], "sample_1081": [false, false, false, false, false], "sample_1082": [false, false, true, false, true], "sample_1083": [false, true, false, false, false], "sample_1084": [false, false, false, false, false], "sample_1085": [false, false, false, false, true], "sample_1086": [false, false, true, false, false], "sample_1087": [false, false, false, false, false], "sample_1088": [false, false, false, false, false], "sample_1089": [false, false, false, false, false], "sample_1090": [false, false, true, false, false], "sample_1091": [false, false, false, true, false], "sample_1092": [true, true, false, true, false], "sample_1093": [false, false, false, false, false], "sample_1094": [false, false, false, false, false], "sample_1095": [false, false, false, false, false], "sample_1096": [false, false, false, false, false], "sample_1097": [true, true, false, false, true], "sample_1098": [false, false, false, false, false], "sample_1099": [false, false, false, false, false], "sample_1100": [false, true, false, false, false], "sample_1101": [false, false, false, false, false], "sample_1102": [true, false, true, true, false], "sample_1103": [false, false, false, false, false], "sample_1104": [false, true, false, false, true], "sample_1105": [true, false, false, true, false], "sample_1106": [false, false, false, true, false], "sample_1107": [false, false, false, true, false], "sample_1108": [false, false, false, false, false], "sample_1109": [false, false, false, false, true], "sample_1110": [true, false, false, false, false], "sample_1111": [false, false, false, false, false], "sample_1112": [false, false, false, false, false], "sample_1113": [false, false, false, false, false], "sample_1114": [false, true, false, false, true], "sample_1115": [false, false, false, false, false], "sample_1116": [false, false, false, false, false], "sample_1117": [true, true, false, true, true], "sample_1118": [false, false, true, true, true], "sample_1119": [false, false, true, false, false], "sample_1120": [false, false, false, false, false], "sample_1121": [true, true, false, false, false], "sample_1122": [false, false, false, false, false], "sample_1123": [false, false, false, false, false], "sample_1124": [false, false, false, false, false], "sample_1125": [false, false, false, false, false], "sample_1126": [true, true, false, false, false], "sample_1127": [false, false, false, false, false], "sample_1128": [true, false, false, false, false], "sample_1129": [false, false, false, false, false], "sample_1130": [false, true, true, false, true], "sample_1131": [false, false, true, true, false], "sample_1132": [false, false, false, false, false], "sample_1133": [false, false, false, false, false], "sample_1134": [false, false, false, false, false], "sample_1135": [false, false, false, false, false], "sample_1136": [true, false, true, false, false], "sample_1137": [false, false, false, false, false], "sample_1138": [false, false, false, false, false], "sample_1139": [false, false, false, false, false], "sample_1140": [false, false, false, false, false], "sample_1141": [false, false, false, false, false], "sample_1142": [false, false, false, false, true], "sample_1143": [false, false, false, false, false], "sample_1144": [false, true, false, false, true], "sample_1145": [false, false, false, false, false], "sample_1146": [false, false, false, false, false], "sample_1147": [false, false, false, false, false], "sample_1148": [false, false, false, true, false], "sample_1149": [false, false, true, false, false], "sample_1150": [false, false, false, false, false], "sample_1151": [false, false, true, false, false], "sample_1152": [false, true, false, true, false], "sample_1153": [true, false, false, false, false], "sample_1154": [true, false, false, true, false], "sample_1155": [false, false, false, false, false], "sample_1156": [false, true, false, true, true], "sample_1157": [false, false, false, false, true], "sample_1158": [false, false, false, true, false], "sample_1159": [false, false, true, false, false], "sample_1160": [false, false, false, false, false], "sample_1161": [false, true, false, false, false], "sample_1162": [false, false, false, false, false], "sample_1163": [false, true, false, false, true], "sample_1164": [false, false, false, false, false], "sample_1165": [false, false, false, true, false], "sample_1166": [false, false, false, false, false], "sample_1167": [false, false, false, false, false], "sample_1168": [false, false, false, true, false], "sample_1169": [true, true, false, false, false], "sample_1170": [false, false, false, false, false], "sample_1171": [true, false, false, false, false], "sample_1172": [false, false, false, false, false], "sample_1173": [false, false, true, true, true], "sample_1174": [false, false, false, false, false], "sample_1175": [false, false, false, false, false], "sample_1176": [false, false, false, false, false], "sample_1177": [false, false, false, false, false], "sample_1178": [false, false, false, false, false], "sample_1179": [false, false, true, false, false], "sample_1180": [false, false, false, false, false], "sample_1181": [true, true, true, true, true], "sample_1182": [false, false, false, false, false], "sample_1183": [false, false, false, false, false], "sample_1184": [false, false, false, false, false], "sample_1185": [false, false, false, false, true], "sample_1186": [false, false, false, true, false], "sample_1187": [false, false, false, false, false], "sample_1188": [false, false, false, false, false], "sample_1189": [true, false, true, true, true], "sample_1190": [false, false, true, true, false], "sample_1191": [false, false, false, false, false], "sample_1192": [false, false, false, false, false], "sample_1193": [false, false, true, false, false], "sample_1194": [false, false, false, false, true], "sample_1195": [false, false, false, false, false], "sample_1196": [false, true, true, true, true], "sample_1197": [false, false, false, true, false], "sample_1198": [false, false, false, false, false], "sample_1199": [false, false, false, false, false], "sample_1200": [false, false, false, false, false], "sample_1201": [false, false, false, false, false], "sample_1202": [false, false, false, true, false], "sample_1203": [false, false, false, false, false], "sample_1204": [false, false, false, false, true], "sample_1205": [false, false, false, false, false], "sample_1206": [false, false, false, false, false], "sample_1207": [true, true, false, false, false], "sample_1208": [false, false, false, false, false], "sample_1209": [false, true, true, false, true]}}