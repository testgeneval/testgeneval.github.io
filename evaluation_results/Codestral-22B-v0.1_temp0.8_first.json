{"raw_generations": {"sample_200": ["    def test_attach_file(self):\n        with tempfile.NamedTemporaryFile(delete=False) as temp:\n            temp.write(b'attachment content')\n            temp_path = temp.name\n\n        msg = EmailMessage('Subject', 'Body', 'from@example.com', ['to@example.com'])\n        msg.attach_file(temp_path)\n\n        attachments = self.get_decoded_attachments(msg)\n        self.assertEqual(len(attachments), 1)\n        filename, content, mimetype = attachments[0]\n        self.assertEqual(filename, os.path.basename(temp_path))\n        self.assertEqual(content, b'attachment content')\n        self.assertEqual(mimetype, mimetypes.guess_type(temp_path)[0])\n\n        os.remove(temp_path)\n", "    def test_attach_file(self):\n        tmp_dir = tempfile.mkdtemp()\n        try:\n            filename = os.path.join(tmp_dir, 'test.txt')\n            with open(filename, 'w') as f:\n                f.write('Test content')\n\n            msg = EmailMessage(subject='Test email', body='Test body', from_email='from@example.com', to=['to@example.com'])\n            msg.attach_file(filename)\n            attachments = self.get_decoded_attachments(msg)\n\n            self.assertEqual(len(attachments), 1)\n            self.assertEqual(attachments[0][0], 'test.txt')\n            self.assertEqual(attachments[0][1], b'Test content')\n            self.assertEqual(attachments[0][2], 'text/plain')\n        finally:\n            shutil.rmtree(tmp_dir)\n", "def test_sanitize_address(self):\n    self.assertEqual(\n        sanitize_address('Test <test@example.com>', 'utf-8'),\n        '=?utf-8?q?Test?= <test@example.com>',\n    )\n    self.assertEqual(\n        sanitize_address('=?utf-8?q?Test?= <test@example.com>', 'utf-8'),\n        '=?utf-8?q?=3D?=utf-8?q?Test?= =?utf-8?q?<test@example.com>?=',\n    )\n    self.assertEqual(\n        sanitize_address(('Test', 'test@example.com'), 'utf-8'),\n        '=?utf-8?q?Test?= <test@example.com>',\n    )\n    self.assertRaisesMessage(\n        ValueError,\n        'Invalid address; address parts cannot contain newlines.',\n        sanitize_address,\n        'Test <test@ex\\rample.com>',\n        'utf-8',\n    )\n", "    def test_email_message_attach_file(self):\n        # Create a temporary file with known content\n        temp_dir = tempfile.mkdtemp()\n        test_file_path = os.path.join(temp_dir, 'testfile.txt')\n        with open(test_file_path, 'w') as f:\n            f.write('Test content')\n\n        # Create an EmailMessage and attach the file\n        email = EmailMessage()\n        email.attach_file(test_file_path)\n\n        # Check the attachment\n        attachments = self.get_decoded_attachments(email)\n        self.assertEqual(len(attachments), 1)\n        filename, content, mimetype = attachments[0]\n        self.assertEqual(filename, 'testfile.txt')\n        self.assertEqual(content, b'Test content')\n        self.assertEqual(mimetype, 'text/plain')\n\n        # Clean up\n        shutil.rmtree(temp_dir)\n", "    def test_sanitize_address(self):\n        # Test with a simple email address\n        self.assertEqual(sanitize_address('test@example.com', 'utf-8'), 'test@example.com')\n\n        # Test with a display name\n        self.assertEqual(sanitize_address(('Test User', 'test@example.com'), 'utf-8'), 'Test User <test@example.com>')\n\n        # Test with unicode in the display name\n        self.assertEqual(sanitize_address(('T\u00ebst \u00dcs\u00e9r', 'test@example.com'), 'utf-8'), '=?utf-8?b?VMO8dGVzdCBVwrxtZXI?= <test@example.com>')\n\n        # Test with a non-ASCII domain name\n        self.assertEqual(sanitize_address('test@ex\u00e4mple.com', 'utf-8'), 'test@xn--exmple-cua.com')\n\n        # Test with newline in the address parts\n        with self.assertRaises(ValueError):\n            sanitize_address('test@ex\\rample.com', 'utf-8')\n\n        # Test with invalid email address\n        with self.assertRaises(ValueError):\n            sanitize_address('invalid_email', 'utf-8')\n"], "sample_1007": ["def test_binomial_with_poly():\n    x = symbols('x')\n    p = Poly(x**3, x)\n    assert binomial(p, 2) == Poly(x**6 + 3*x**5 + 3*x**4 + x**3, x, domain='ZZ')\n", "def test_binomial_properties():\n    n, k = symbols('n k', integer=True)\n\n    # Test is_even property\n    assert binomial(n, 2).is_even == (n > 1)\n\n    # Test is_odd property\n    assert binomial(n, 2).is_odd == (n > 0)\n\n    # Test is_positive property\n    assert binomial(n, k).is_positive == ((n >= k) & (k >= 0))\n\n    # Test is_negative property\n    assert binomial(n, k).is_negative == False\n\n    # Test is_real property\n    assert binomial(n, k).is_real == True\n\n    # Test is_complex property\n    assert binomial(n, k).is_complex == False\n\n    # Test is_integer property\n    assert binomial(n, k).is_integer == ((n >= k) & (k >= 0))\n", "def test_binomial_is_positive():\n    n, k = symbols('n k', integer=True)\n    assert binomial(n, k).is_positive == Piecewise((True, And(n.is_integer, n.is_nonnegative, k.is_integer, k.is_nonnegative, k <= n)), (None, True))\n", "def test_binomial_expand_func():\n    n, k = symbols('n k', integer=True, positive=True)\n    assert expand_func(binomial(n, k)) == ff(n, k) / factorial(k)\n", "def test_subfactorial():\n    assert subfactorial(0) == 1\n    assert subfactorial(1) == 0\n    assert subfactorial(5) == 44\n    assert subfactorial(Symbol('n', integer=True)) == subfactorial(Symbol('n', integer=True))\n    assert subfactorial(Symbol('n', integer=True, positive=True)) is None  # Not computed\n    assert subfactorial(-1) is None  # Not computed\n    assert subfactorial(nan) == nan\n    assert subfactorial(oo) == oo\n\n    assert subfactorial(5).is_even() is True\n    assert subfactorial(6).is_odd() is True\n    assert subfactorial(Symbol('n', integer=True, positive=True)).is_integer() is True\n    assert subfactorial(Symbol('n', integer=True, positive=True)).is_nonnegative() is True\n"], "sample_55": ["    def test_action_checkbox_rendering(self):\n        response = self.client.get('/admin/admin_views/article/')\n        self.assertInHTML(f'<input type=\"checkbox\" name=\"{ACTION_CHECKBOX_NAME}\" class=\"action-select\" />', response.content.decode('utf-8'))\n", "    def test_admin_readonly_fields(self):\n        response = self.client.get(reverse('admin:admin_views_book_change', args=(self.b1.pk,)))\n        readonly_field = self.get_admin_readonly_field(response, 'name')\n        self.assertIsNotNone(readonly_field)\n        self.assertEqual(readonly_field.field['name'], 'name')\n", "    def test_inline_formset_data(self):\n        response = self.client.get(reverse('admin:admin_views_section_change', args=[self.s1.pk]))\n        self.assertContains(response, 'inline_admin_formset_data')\n        formset_data = response.context['inline_admin_formset_data']\n        self.assertIn('article_set', formset_data)\n        self.assertEqual(formset_data['article_set']['options']['prefix'], 'article_set')\n", "    def test_admin_form_fields(self):\n        response = self.client.get(reverse('admin:admin_views_section_change', args=(self.s1.pk,)))\n        self.assertEqual(response.status_code, 200)\n        admin_form_fields = self.get_admin_form_fields(response)\n        # Check if all expected fields are present in the admin form\n        expected_fields = ['name']  # Add more fields as needed\n        for field in admin_form_fields:\n            self.assertIn(field.field['name'], expected_fields)\n", "    def test_inline_admin_formset(self):\n        # Testing that the inline formset for Article in Section works correctly\n        response = self.client.get(reverse('admin:admin_views_section_change', args=[self.s1.id]))\n        self.assertEqual(response.status_code, 200)\n        self.assertIn('article_set-0-id', response.content.decode())\n        self.assertIn('article_set-1-id', response.content.decode())\n        self.assertIn('article_set-2-id', response.content.decode())\n        response = self.client.post(reverse('admin:admin_views_section_change', args=[self.s1.id]), self.inline_post_data)\n        self.assertRedirects(response, reverse('admin:admin_views_section_changelist'))\n        self.s1.refresh_from_db()\n        self.assertEqual(self.s1.article_set.count(), 3)  # Ensure no additional articles were created\n"], "sample_744": ["def test_power_transformer_transform_shape():\n    pt = PowerTransformer(method='box-cox')\n    pt.fit(X_2d)\n    X_transformed = pt.transform(X_2d)\n    assert_equal(X_transformed.shape, X_2d.shape)\n", "def test_power_transformer_non_positive():\n    pt = PowerTransformer(method='box-cox')\n    X = np.array([[1, 2], [3, -2], [4, 5]])\n    with assert_raises(ValueError, match=\"The Box-Cox transformation can only be applied to strictly positive data\"):\n        pt.fit(X)\n", "def test_power_transformer_transform_inverse_consistency():\n    pt = PowerTransformer(method='box-cox', standardize=True)\n    Xt = pt.fit_transform(X_2d)\n    X_roundtrip = pt.inverse_transform(Xt)\n    assert_array_almost_equal(X_2d, X_roundtrip, decimal=2)\n", "def test_add_dummy_feature():\n    X = np.array([[0, 1], [1, 0]])\n    expected_output = np.array([[1., 0., 1.],\n                                [1., 1., 0.]])\n    assert_array_equal(add_dummy_feature(X), expected_output)\n", "def test_quantile_transform_invalid_n_quantiles():\n    # Test with n_quantiles <= 0\n    X = np.array([[1], [2], [3]])\n    with pytest.raises(ValueError, match=\"Invalid value for 'n_quantiles': .* The number of quantiles must be at least one.\"):\n        quantile_transform(X, n_quantiles=0)\n\n    # Test with n_quantiles > subsample\n    with pytest.raises(ValueError, match=\"The number of quantiles cannot be greater than the number of samples used.\"):\n        quantile_transform(X, n_quantiles=10, subsample=3)\n"], "sample_908": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_1060": ["def test_print_ImaginaryUnit():\n    printer = PythonCodePrinter()\n    assert printer._print_ImaginaryUnit(1j) == '1j'\n", "def test_print_SparseMatrix():\n    sm = SparseMatrix(2, 2, {(0, 0): 1, (1, 1): 2})\n    assert NumPyPrinter().doprint(sm) == \"numpy.array([[1, 0], [0, 2]])\"\n    assert SciPyPrinter().doprint(sm) == \"scipy.sparse.coo_matrix([1, 2], ([0, 1], [0, 1]), shape=(2, 2))\"\n", "def test_print_rational():\n    expr = Rational(3, 4)\n    printer = MpmathPrinter()\n    assert printer._print_Rational(expr) == 'mpmath.mpf(3)/mpmath.mpf(4)'\n", "def test_print_CodegenArrayTensorProduct():\n    from sympy.codegen.array_utils import CodegenArrayTensorProduct\n    expr = CodegenArrayTensorProduct(p[x, y], p[x, y])\n    printer = NumPyPrinter()\n    result = printer._print_CodegenArrayTensorProduct(expr)\n    assert result == \"numpy.einsum(p[x, y], [0, 1], p[x, y], [0, 1])\"\n", "def test_next_pythoncode():\n    expr = sign(x)\n    expected = \"(0.0 if x == 0 else math.copysign(1, x))\"\n    assert pycode(expr) == expected\n"], "sample_693": ["def test_pytest_pycollect_makeitem(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        class MyTestCase(unittest.TestCase):\n                self.assertEqual(1, 1)\n        \"\"\"\n    )\n\n    result = pytester.runpytest()\n\n    result.assert_outcomes(passed=1)\n", "def test_unittest_pycollect_makeitem(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        class TestClass(unittest.TestCase):\n                self.assertEqual(1, 1)\n        \"\"\"\n    )\n    result = pytester.runpytest(\"-v\")\n    result.assert_outcomes(passed=1)\n", "    def test_collect(self, pytester: Pytester):\n        pytester.makepyfile(\n            \"\"\"\n            import unittest\n\n            class MyTest(unittest.TestCase):\n                    self.assertEqual(1, 1)\n\n            class MySkippedTest(unittest.TestCase):\n                @unittest.skip(\"Skipping this test\")\n                    self.assertEqual(1, 1)\n            \"\"\"\n        )\n        result = pytester.runpytest(\"-q\")\n        result.assert_outcomes(passed=1, skipped=1)\n", "def test_testcasefunction_runtest_with_async_function(pytester: Pytester, monkeypatch: MonkeyPatch):\n    pytester.makepyfile(\n        \"\"\"\n        import asyncio\n        import unittest\n\n        class TestAsyncFunction(unittest.TestCase):\n            async def test_async_function(self):\n                await asyncio.sleep(0.1)\n                self.assertEqual(1, 1)\n        \"\"\"\n    )\n\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)\n", "def test_pytest_pycollect_makeitem_with_non_testcase_class(pytester: Pytester, monkeypatch: MonkeyPatch) -> None:\n    \"\"\"Test pytest_pycollect_makeitem with a class that is not a subclass of unittest.TestCase.\"\"\"\n    monkeypatch.setitem(sys.modules, \"unittest\", None)\n\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        class NonTestCase:\n            pass\n\n            pytester.makepyfile(\n                \"\"\"\n                from plugin import pytest_pycollect_makeitem\n\n                    pass\n\n                class NonTestCase:\n                    pass\n\n                assert pytest_pycollect_makeitem(None, 'NonTestCase', NonTestCase) is None\n                \"\"\"\n            )\n            result = pytester.runpytest()\n            result.assert_outcomes(passed=1)\n        \"\"\"\n    )\n\n    pytester.makepyfile(\n        \"\"\"\n        from plugin import pytest_pycollect_makeitem\n\n            pass\n\n        class NonTestCase:\n            pass\n\n        assert pytest_pycollect_makeitem(None, 'NonTestCase', NonTestCase) is None\n        \"\"\",\n        plugin=True,\n    )\n\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)\n"], "sample_488": ["    def test_set_installed_apps(self):\n        # Store the current apps for later restoration\n        original_apps = apps.app_configs.copy()\n        try:\n            # Set a new set of installed apps\n            apps.set_installed_apps(SOME_INSTALLED_APPS)\n            # Check if the apps have been set correctly\n            self.assertEqual(sorted(apps.app_configs.keys()), sorted([app.split('.')[-1] for app in SOME_INSTALLED_APPS]))\n        finally:\n            # Restore the original apps\n            apps.set_installed_apps(original_apps)\n", "    def test_register_model(self):\n        model = TotallyNormal()\n        apps.register_model('test_app', model)\n        self.assertEqual(apps.all_models['test_app']['totallynormal'], model)\n        self.assertEqual(apps.get_model('test_app', 'TotallyNormal'), model)\n", "    def test_get_model(self):\n        \"\"\"Test get_model() with a model that has not been imported yet.\"\"\"\n        model = apps.get_model(\"apps.TotallyNormal\", require_ready=False)\n        self.assertIs(model, TotallyNormal)\n", "    def test_set_available_apps(self):\n        with self.settings(INSTALLED_APPS=SOME_INSTALLED_APPS):\n            self.apps = new_apps(SOME_INSTALLED_APPS)\n            available_apps = [\n                \"django.contrib.auth\",\n                \"django.contrib.contenttypes\",\n            ]\n            self.apps.set_available_apps(available_apps)\n            self.assertEqual(len(self.apps.app_configs), 2)\n            self.assertIn(\"django.contrib.auth\", self.apps.app_configs)\n            self.assertIn(\"django.contrib.contenttypes\", self.apps.app_configs)\n            self.assertNotIn(\"django.contrib.sessions\", self.apps.app_configs)\n            self.assertNotIn(\"django.contrib.messages\", self.apps.app_configs)\n            self.assertNotIn(\"django.contrib.staticfiles\", self.apps.app_configs)\n            self.apps.unset_available_apps()\n            self.assertEqual(len(self.apps.app_configs), 5)\n", "    def test_set_available_apps(self):\n        # Test that set_available_apps() restricts the set of installed apps.\n        with isolate_apps(SOME_INSTALLED_APPS):\n            # Check the initial state.\n            self.assertEqual(\n                [ac.name for ac in apps.get_app_configs()],\n                SOME_INSTALLED_APPS_NAMES,\n            )\n\n            # Set available apps to a subset of installed apps.\n            available = SOME_INSTALLED_APPS_NAMES[2:]\n            apps.set_available_apps(available)\n\n            # Check that the set of installed apps is restricted.\n            self.assertEqual(\n                [ac.name for ac in apps.get_app_configs()],\n                available,\n            )\n\n            # Unset available apps.\n            apps.unset_available_apps()\n\n            # Check that the initial state is restored.\n            self.assertEqual(\n                [ac.name for ac in apps.get_app_configs()],\n                SOME_INSTALLED_APPS_NAMES,\n            )\n"], "sample_572": ["    def test_kde_univariate(self, x):\n        kde = KDE()\n        density, support = kde(x)\n        assert len(density) == kde.gridsize\n        assert len(support) == kde.gridsize\n        assert np.allclose(self.integrate(density, support), 1)\n", "    def test_histogram_univariate(self, x):\n        hist = Histogram()\n        counts, edges = hist(x)\n        assert np.isclose(np.sum(counts), len(x))\n        assert np.isclose(np.diff(edges).sum(), np.ptp(x))\n", "    def test_univariate_cumulative(self, x):\n        kde = KDE(cumulative=True)\n        density, support = kde(x)\n        cdf = np.cumsum(density * np.diff(support))\n        assert np.all(np.diff(cdf) >= 0), \"CDF should be non-decreasing\"\n", "    def test_univariate_density(self, x):\n        kde = KDE()\n        density, support = kde(x)\n\n        # Check that the density integrates to 1\n        assert np.isclose(self.integrate(density, support), 1)\n\n        # Check that the density is non-negative\n        assert np.all(density >= 0)\n", "    def test_kde_univariate(self, x):\n        kde = KDE()\n        density, support = kde(x)\n        assert density.ndim == 1\n        assert_array_almost_equal(density.sum(), 1)\n"], "sample_416": ["    def test_runshell(self):\n        with mock.patch(\"subprocess.run\") as mock_run, mock.patch(\"signal.signal\") as mock_signal:\n            client = DatabaseClient(connection)\n            client.runshell([\"-c\", \"SELECT 1;\"])\n            mock_run.assert_called_once()\n            mock_signal.assert_any_call(signal.SIGINT, signal.SIG_IGN)\n            mock_signal.assert_any_call(signal.SIGINT, signal.SIG_DFL)\n", "    def test_runshell(self):\n        sigint_handler = signal.getsignal(signal.SIGINT)\n        with mock.patch.object(subprocess, 'call') as mock_call:\n            with mock.patch.object(signal, 'signal') as mock_signal:\n                client = DatabaseClient(connection)\n                client.runshell([])\n\n        mock_signal.assert_called_with(signal.SIGINT, signal.SIG_IGN)\n        mock_signal.assert_called_with(signal.SIGINT, sigint_handler)\n        mock_call.assert_called_once()\n", "def test_settings_to_cmd_args_env_with_options(self):\n    settings_dict = {\n        \"HOST\": \"localhost\",\n        \"PORT\": 5432,\n        \"NAME\": \"testdb\",\n        \"USER\": \"testuser\",\n        \"PASSWORD\": \"testpass\",\n        \"OPTIONS\": {\n            \"passfile\": \"/path/to/passfile\",\n            \"service\": \"testservice\",\n            \"sslmode\": \"verify-ca\",\n            \"sslrootcert\": \"/path/to/sslrootcert\",\n            \"sslcert\": \"/path/to/sslcert\",\n            \"sslkey\": \"/path/to/sslkey\",\n        }\n    }\n    parameters = [\"-c\", \"SELECT * FROM test_table\"]\n    args, env = self.settings_to_cmd_args_env(settings_dict, parameters)\n\n    self.assertEqual(args, [\n        \"psql\",\n        \"-U\", \"testuser\",\n        \"-h\", \"localhost\",\n        \"-p\", \"5432\",\n        \"-c\", \"SELECT * FROM test_table\",\n        \"testdb\"\n    ])\n    self.assertEqual(env, {\n        \"PGPASSWORD\": \"testpass\",\n        \"PGPASSFILE\": \"/path/to/passfile\",\n        \"PGSERVICE\": \"testservice\",\n        \"PGSSLMODE\": \"verify-ca\",\n        \"PGSSLROOTCERT\": \"/path/to/sslrootcert\",\n        \"PGSSLCERT\": \"/path/to/sslcert\",\n        \"PGSSLKEY\": \"/path/to/sslkey\",\n    })\n", "    def test_runshell(self):\n        with mock.patch('subprocess.Popen') as mock_popen:\n            mock_popen.return_value = subprocess.Popen([\"echo\", \"Test\"])\n            sigint_handler = signal.getsignal(signal.SIGINT)\n            try:\n                DatabaseClient().runshell([\"test_parameter\"])\n                # Assert that the correct commands were run\n                mock_popen.assert_called_once_with([\"psql\", \"test_parameter\"], env=os.environ)\n                # Assert that the signal was ignored and then restored\n                self.assertEqual(signal.getsignal(signal.SIGINT), signal.SIG_IGN)\n            finally:\n                signal.signal(signal.SIGINT, sigint_handler)\n", "    def test_signal_handling(self):\n        with mock.patch('subprocess.Popen') as mock_popen:\n            with mock.patch('signal.signal') as mock_signal:\n                client = DatabaseClient({})\n                client.runshell([])\n\n                # Check that the signal handler was ignored during the runshell command\n                mock_signal.assert_any_call(signal.SIGINT, signal.SIG_IGN)\n\n                # Check that the original signal handler was restored after runshell command\n                mock_signal.assert_called_with(signal.SIGINT, signal.SIG_DFL)\n\n        # Check that Popen was called with the expected arguments\n        mock_popen.assert_called_once()\n"], "sample_1114": ["def test_complex_region_contains():\n    region = ComplexRegion(Interval(1, 3) * Interval(4, 6))\n    assert 2 + 5*I in region\n    assert 5*I not in region\n", "def test_complex_region_from_real():\n    unit = Interval(0, 1)\n    complex_region = ComplexRegion.from_real(unit)\n    assert complex_region == CartesianComplexRegion(ProductSet(unit, FiniteSet(0)))\n", "def test_normalize_theta_set():\n    theta = Interval(-3*pi/2, -pi/2)\n    expected = Interval(pi/2, 3*pi/2)\n    assert normalize_theta_set(theta) == expected\n", "def test_range_slicing():\n    r = Range(10)\n    assert r[2:8:2] == Range(2, 8, 2)\n    assert list(r[2:8:2]) == [2, 4, 6]\n", "def test_complex_region_from_real():\n    unit = Interval(0,1)\n    region = ComplexRegion.from_real(unit)\n    assert region == CartesianComplexRegion(ProductSet(Interval(0, 1), FiniteSet(0)))\n"], "sample_5": ["def test_bounding_box(model):\n    if model['bounding_box'] is not False:\n        m = model['class'](**model['parameters'])\n        bb = m.bounding_box\n        assert bb is not None\n        if isinstance(bb, ModelBoundingBox):\n            assert_quantity_allclose(bb.lower_bounds, model['bounding_box'][0])\n            assert_quantity_allclose(bb.upper_bounds, model['bounding_box'][1])\n        else:\n            assert_quantity_allclose(bb, model['bounding_box'])\n", "def test_model_bounding_box(model):\n    model_cls = model['class']\n    parameters = model['parameters']\n    bbox = model['bounding_box']\n    if bbox is False:\n        # No bounding box was provided, so we expect the model to have None\n        assert model_cls(**parameters).bounding_box is None\n    else:\n        assert model_cls(**parameters).bounding_box == ModelBoundingBox(bbox)\n", "def test_parameter_quantity(model):\n    cls = model['class']\n    params = model['parameters']\n    evaluation = model['evaluation']\n\n    m = cls(**params)\n    for pname, pval in params.items():\n        assert_quantity_allclose(getattr(m, pname).quantity, pval)\n\n    for eval_args, expected in evaluation:\n        result = m(*fix_inputs(eval_args, cls))\n        assert_quantity_allclose(result, expected)\n", "def test_parameter_defaults(model):\n    Model = model['class']\n    parameters = model['parameters']\n\n    # Create a model instance with default parameters\n    instance = Model()\n\n    # Verify that the default parameters are set correctly\n    for param_name, param_value in parameters.items():\n        assert instance[param_name] == param_value\n", "def test_bounding_box(model):\n    \"\"\"\n    Test that the bounding box is correctly calculated for the model.\n    \"\"\"\n    if model['bounding_box'] is False:\n        with pytest.raises(ModelBoundingBox.MissingBoundingBoxError):\n            model['class'](**model['parameters']).bounding_box\n    else:\n        assert_quantity_allclose(model['class'](**model['parameters']).bounding_box,\n                                 model['bounding_box'])\n"], "sample_1029": ["def test_print_Matrix():\n    m = Matrix([[1, 2], [3, 4]])\n    sT(m, \"Matrix([[1, 2], [3, 4]])\")\n", "def test_srepr_Function():\n    f = Function('f')\n    sT(f(x, y), \"Function('f')(x, y)\")\n", "def test_print_Cycle():\n    x, y, z = symbols('x y z')\n    c = Cycle(x, y, z)\n    sT(c, \"Cycle(x, y, z)\")\n", "def test_srepr_AlgebraicNumber():\n    alpha = sqrt(2) + sqrt(3)\n    assert srepr(alpha) == \"AlgebraicNumber(x**2 - 5, [1, 0, 1])\"\n    assert eval(srepr(alpha), ENV) == alpha\n", "def test_srepr_Abs():\n    expr = Abs(x)\n    string = \"Abs(x)\"\n    sT(expr, string)\n"], "sample_738": ["def test_strip_tags():\n    text = \"<html><body><h1>Hello, World!</h1></body></html>\"\n    expected = \"Hello, World!\"\n    assert strip_tags(text) == expected\n", "def test_tfidf_vectorizer_with_stop_words():\n    # Test the TF-IDF Vectorizer with stop words\n    vectorizer = TfidfVectorizer(stop_words='english')\n    X = vectorizer.fit_transform(ALL_FOOD_DOCS)\n    assert 'the' not in vectorizer.vocabulary_\n    assert 'copyright' not in vectorizer.vocabulary_\n", "def test_custom_analyzer():\n    vectorizer = CountVectorizer(analyzer=split_tokenize)\n    X = vectorizer.fit_transform(ALL_FOOD_DOCS)\n    assert_equal(X.shape[0], len(ALL_FOOD_DOCS))\n\n    expected_vocabulary = {\n        'the': 0, 'pizza': 1, 'beer': 2, 'copyright': 3, 'burger': 4,\n        'coke': 5, 'salad': 6, 'celeri': 7, 'sparkling': 8, 'water': 9,\n        'tomato': 10\n    }\n    assert_equal(vectorizer.vocabulary_, expected_vocabulary)\n\n    vectorizer = CountVectorizer(analyzer=lazy_analyze)\n    X = vectorizer.fit_transform(ALL_FOOD_DOCS)\n    assert_equal(X.shape[0], len(ALL_FOOD_DOCS))\n    expected_vocabulary = {'the_ultimate_feature': 0}\n    assert_equal(vectorizer.vocabulary_, expected_vocabulary)\n", "def test_tfidf_vectorizer_max_df_smoke():\n    # Test TfidfVectorizer with max_df set\n    vectorizer = TfidfVectorizer(max_df=0.5)\n    X = vectorizer.fit_transform(JUNK_FOOD_DOCS + NOTJUNK_FOOD_DOCS)\n    assert_equal(vectorizer.get_feature_names(), ['beer', 'burger', 'pizza'])\n", "def test_count_vectorizer_with_lowercase_and_strip_accents():\n    vectorizer = CountVectorizer(lowercase=True, strip_accents='ascii')\n    X = vectorizer.fit_transform([\"r\u00e9sum\u00e9\", \"resume\", \"Resume\", \"RESUME\"])\n    feature_names = vectorizer.get_feature_names()\n    assert_array_equal(X.toarray(), np.array([[1, 1], [1, 0], [0, 1], [0, 1]]))\n    assert_array_equal(feature_names, np.array(['resume', 'r']))\n"], "sample_272": ["def test_migration_plan_clean_start(self):\n    executor = MigrationExecutor(connection)\n    plan = executor.migration_plan([('migrations', None)], clean_start=True)\n    self.assertEqual(len(plan), len(executor.loader.graph.root_nodes()))\n    for migration, backwards in plan:\n        self.assertFalse(backwards)\n        self.assertIsInstance(migration, migrations.Migration)\n        self.assertEqual(migration.app_label, 'migrations')\n", "def test_migrate_all_backwards(self):\n    \"\"\"\n    Test the _migrate_all_backwards method of the MigrationExecutor class.\n    \"\"\"\n    plan = [\n        (migrations.Migration('migrations.0001_initial', 'migrations'), True),\n    ]\n    full_plan = [\n        (migrations.Migration('migrations.0001_initial', 'migrations'), True),\n        (migrations.Migration('migrations2.0001_initial', 'migrations2'), True),\n    ]\n    executor = MigrationExecutor(connection)\n    with mock.patch.object(executor, 'unapply_migration', return_value=None) as mock_unapply:\n        executor._migrate_all_backwards(plan, full_plan, fake=False)\n        mock_unapply.assert_called_once_with(mock.ANY, migrations.Migration('migrations.0001_initial', 'migrations'), fake=False)\n", "def test_migrate_all_backwards(self, mock_record_migration):\n    executor = MigrationExecutor(connection)\n    plan = [\n        (executor.loader.graph.nodes[('migrations', '0002_apply_field_default')], True),\n        (executor.loader.graph.nodes[('migrations', '0001_initial')], True),\n    ]\n    full_plan = executor.migration_plan(executor.loader.graph.leaf_nodes(), clean_start=True)\n    state = executor._migrate_all_backwards(plan, full_plan, fake=False)\n    # Assert that the record_migration function was called for each migration in the plan\n    self.assertEqual(mock_record_migration.call_count, len(plan))\n", "def test_apply_migration(self):\n    # Set up a fake migration\n    class FakeMigration(migrations.Migration):\n        operations = [\n            migrations.CreateModel(\n                name='FakeModel',\n                fields=[\n                    ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                    ('name', models.CharField(max_length=200)),\n                ],\n            ),\n        ]\n\n    # Mock the recorder to check if the migration is recorded\n    with mock.patch.object(MigrationRecorder, 'record_applied') as mock_record_applied:\n        executor = MigrationExecutor(connection)\n        state = ProjectState()\n        new_state = executor.apply_migration(state, FakeMigration())\n\n        # Check if the migration was recorded\n        mock_record_applied.assert_called_once_with('migrations', FakeMigration.name)\n\n        # Check if the state was updated\n        self.assertIsNot(state, new_state)\n        self.assertEqual(new_state.models, {('migrations', 'fakemodel')})\n", "def test_detect_soft_applied(self):\n    migration = migrations.Migration(\"migrations\", \"0001_initial\")\n    migration.operations = [\n        migrations.CreateModel(\n            name=\"Model1\",\n            fields=[(\"id\", models.AutoField(primary_key=True))],\n        ),\n        migrations.AddField(\n            model_name=\"Model1\",\n            name=\"field1\",\n            field=models.CharField(max_length=100),\n        ),\n    ]\n    migration.initial = True\n\n    project_state = ProjectState(real_apps=list(self.executor.loader.unmigrated_apps))\n\n    # Test when the model and field both exist\n    with connection.cursor() as cursor:\n        cursor.execute(\"CREATE TABLE migrations_model1 (id SERIAL PRIMARY KEY, field1 VARCHAR(100))\")\n\n    applied, state = self.executor.detect_soft_applied(project_state, migration)\n    self.assertTrue(applied)\n    self.assertEqual(state.apps.get_model(\"migrations\", \"Model1\")._meta.get_field(\"field1\").name, \"field1\")\n\n    # Test when the model exists but the field does not\n    with connection.cursor() as cursor:\n        cursor.execute(\"DROP COLUMN migrations_model1.field1\")\n\n    applied, state = self.executor.detect_soft_applied(project_state, migration)\n    self.assertFalse(applied)\n\n    # Test when the model does not exist\n    with connection.cursor() as cursor:\n        cursor.execute(\"DROP TABLE migrations_model1\")\n\n    applied, state = self.executor.detect_soft_applied(project_state, migration)\n    self.assertFalse(applied)\n"], "sample_234": ["def test_union(self):\n    qs1 = Number.objects.filter(num__lt=5)\n    qs2 = Number.objects.filter(num__gte=5)\n    combined_qs = qs1.union(qs2)\n    self.assertNumbersEqual(combined_qs, Number.objects.all(), ordered=False)\n", "def test_union(self):\n    qs1 = Number.objects.filter(num__lt=5)\n    qs2 = Number.objects.filter(num__gt=5)\n    union_qs = qs1.union(qs2)\n    self.assertNumbersEqual(union_qs, Number.objects.all())\n", "def test_union_with_different_values_select(self):\n    qs1 = Number.objects.filter(num__gt=5).values('num')\n    qs2 = Number.objects.filter(num__lt=5).values('other_num')\n    union_qs = qs1.union(qs2)\n    with self.assertRaisesMessage(TypeError, \"Merging 'QuerySet' classes must involve the same values in each case.\"):\n        list(union_qs)\n", "def test_union_with_empty_queryset(self):\n    qs1 = Number.objects.filter(num__gte=5)\n    qs2 = Number.objects.none()\n    self.assertNumbersEqual(qs1.union(qs2), qs1)\n", "def test_union_distinct(self):\n    queryset = Number.objects.filter(num__gte=5).union(\n        Number.objects.filter(num__lte=5)\n    ).distinct()\n    expected_numbers = Number.objects.filter(num__in=range(6))\n    self.assertNumbersEqual(queryset, expected_numbers)\n"], "sample_312": ["    def test_add_method(self):\n        data = ('c', 3)\n        conn_type = Node.default\n        self.node1.add(data, conn_type)\n        self.assertIn(data, self.node1.children)\n        self.assertEqual(self.node1.connector, conn_type)\n        self.assertEqual(len(self.node1.children), len(self.node1_children) + 1)\n", "def test_add_method(self):\n    # Test adding a child to the node\n    self.node2.add(('c', 3), 'DEFAULT')\n    self.assertEqual(self.node2.children, [('c', 3)])\n\n    # Test adding another child with the same connector\n    self.node2.add(('d', 4), 'DEFAULT')\n    self.assertEqual(self.node2.children, [('c', 3), ('d', 4)])\n\n    # Test adding a child with a different connector\n    self.node2.add(('e', 5), 'AND')\n    self.assertEqual(self.node2.children, [Node([('c', 3), ('d', 4)], 'DEFAULT'), ('e', 5)])\n    self.assertEqual(self.node2.connector, 'AND')\n", "def test_add_nodes(self):\n    # Test adding a node with a different connector type\n    self.node2.add(('c', 3), 'OR')\n    self.assertEqual(self.node2.connector, 'OR')\n    self.assertEqual(self.node2.children, [('c', 3)])\n\n    # Test adding a node with the same connector type\n    self.node2.add(('d', 4), 'OR')\n    self.assertEqual(self.node2.connector, 'OR')\n    self.assertEqual(self.node2.children, [('c', 3), ('d', 4)])\n\n    # Test adding a node with the same connector type that already exists\n    self.assertEqual(self.node2.add(('d', 4), 'OR'), ('d', 4))\n\n    # Test adding a node with a different connector type that already exists\n    self.node2.add(('c', 3), 'AND')\n    self.assertEqual(self.node2.connector, 'OR')\n    self.assertEqual(self.node2.children, [('c', 3), ('d', 4), Node([('c', 3)], 'AND')])\n", "    def test_add_method(self):\n        # Test adding a child node with the same connector type\n        new_node = Node([('c', 3)], connector=Node.default)\n        result = self.node1.add(new_node, Node.default)\n        self.assertEqual(result, new_node)\n        self.assertEqual(self.node1.children, [('a', 1), ('b', 2), ('c', 3)])\n\n        # Test adding a child node with a different connector type\n        self.node1.children = self.node1_children.copy()\n        new_node = Node([('d', 4)], connector='AND')\n        result = self.node1.add(new_node, 'AND')\n        self.assertEqual(result, new_node)\n        self.assertEqual(self.node1.children, [self.node1, new_node])\n\n        # Test adding a child node that already exists in the children\n        existing_node = self.node1.children[0]\n        result = self.node1.add(existing_node, Node.default)\n        self.assertEqual(result, existing_node)\n        self.assertEqual(self.node1.children, [('a', 1), ('b', 2)])\n\n        # Test adding a child node with squash=False\n        new_node = Node([('e', 5)], connector='OR')\n        result = self.node1.add(new_node, 'OR', squash=False)\n        self.assertEqual(result, new_node)\n        self.assertEqual(self.node1.children, [('a', 1), ('b', 2), new_node])\n", "    def test_add_method(self):\n        self.node1.add(('c', 3), 'AND')\n        self.assertEqual(self.node1.children, [('a', 1), ('b', 2), ('c', 3)])\n        self.assertEqual(self.node1.connector, 'AND')\n\n        self.node2.add(self.node1, 'OR')\n        self.assertEqual(self.node2.children, [self.node1])\n        self.assertEqual(self.node2.connector, 'OR')\n\n        self.node2.add(('d', 4), 'OR')\n        self.assertEqual(self.node2.children, [self.node1, ('d', 4)])\n        self.assertEqual(self.node2.connector, 'OR')\n\n        # Test squash\n        node3 = Node([('e', 5)], 'AND')\n        self.node2.add(node3, 'OR')\n        self.assertEqual(self.node2.children, [self.node1, ('d', 4), ('e', 5)])\n        self.assertEqual(self.node2.connector, 'OR')\n\n        # Test no squash\n        node4 = Node([('f', 6)], 'OR')\n        self.node2.add(node4, 'AND', squash=False)\n        self.assertEqual(self.node2.children, [self.node1, ('d', 4), ('e', 5), node4])\n        self.assertEqual(self.node2.connector, 'OR')\n"], "sample_584": ["    def test_empty_list(self):\n        result = _infer_tile_ids_from_nested_list([], ())\n        assert list(result) == []\n", "    def test_infer_tile_ids_from_nested_list(self):\n        entry = [[1, 2], [3, 4]]\n        expected_result = {(0, 0): 1, (0, 1): 2, (1, 0): 3, (1, 1): 4}\n        assert dict(_infer_tile_ids_from_nested_list(entry, ())) == expected_result\n", "def test_infer_concat_order_from_coords(self):\n    ds1 = Dataset({'a': DataArray(np.random.randn(3), dims='x'),\n                   'b': DataArray(np.random.randn(3), dims='x'),\n                   'x': DataArray(['a', 'b', 'c'], dims='x')})\n    ds2 = Dataset({'a': DataArray(np.random.randn(3), dims='x'),\n                   'b': DataArray(np.random.randn(3), dims='x'),\n                   'x': DataArray(['d', 'e', 'f'], dims='x')})\n    datasets = [ds1, ds2]\n    combined_ids, concat_dims = _infer_concat_order_from_coords(datasets)\n    expected_ids = OrderedDict([((0,), ds1), ((1,), ds2)])\n    expected_dims = ['x']\n    assert_combined_tile_ids_equal(combined_ids, expected_ids)\n    assert concat_dims == expected_dims\n", "def test_combine_all_along_first_dim(self):\n    combined_ids = OrderedDict([\n        ((1, 0, 0), Dataset({'a': 1})),\n        ((2, 0, 0), Dataset({'a': 2})),\n        ((1, 1, 0), Dataset({'a': 3})),\n        ((2, 1, 0), Dataset({'a': 4})),\n    ])\n\n    expected_combined_ids = OrderedDict([\n        ((0, 0), Dataset({'a': [1, 2]})),\n        ((1, 0), Dataset({'a': [3, 4]})),\n    ])\n\n    result = _combine_all_along_first_dim(combined_ids, dim='first_dim', data_vars='all', coords='different', compat='no_conflicts')\n    assert_combined_tile_ids_equal(result, expected_combined_ids)\n", "    def test_simple_list(self):\n        entry = [1, 2, 3]\n        expected = {(0,): 1, (1,): 2, (2,): 3}\n        assert dict(_infer_tile_ids_from_nested_list(entry, ())) == expected\n"], "sample_1138": ["def test_TR11():\n    assert TR11(sin(2*x)) == 2*sin(x)*cos(x)\n    assert TR11(cos(2*x)) == -sin(x)**2 + cos(x)**2\n    assert TR11(sin(4*x)) == 4*(-sin(x)**2 + cos(x)**2)*sin(x)*cos(x)\n    assert TR11(sin(4*x/3)) == 4*(-sin(x/3)**2 + cos(x/3)**2)*sin(x/3)*cos(x/3)\n    assert TR11(cos(2)) == cos(2)\n    assert TR11(cos(4), 2) == -sin(2)**2 + cos(2)**2\n", "def test_TR22():\n    assert TR22(1 + tan(x)**2) == sec(x)**2\n    assert TR22(1 + cot(x)**2) == csc(x)**2\n    assert TR22(1 + tan(x)**4) == sec(x)**4 - 2*sec(x)**2 + 1\n    assert TR22(1 + cot(x)**4) == csc(x)**4 - 2*csc(x)**2 + 1\n", "def test_TR14():\n    expr = (cos(x) - 1)*(cos(x) + 1)\n    assert TR14(expr) == -sin(x)**2\n    expr = (sin(x) - 1)*(sin(x) + 1)\n    assert TR14(expr) == -cos(x)**2\n    expr = (cos(x) + 1)*(cos(x) - 1)\n    p2 = (cos(y) - 1)*2*(cos(y) + 1)\n    p3 = (3*(cos(y) - 1))*(3*(cos(y) + 1))\n    assert TR14(expr*p2*p3*(x - 1)) == -18*(x - 1)*sin(x)**2*sin(y)**4\n", "def test_TR22():\n    assert TR22(1 + tan(x)**2) == sec(x)**2\n    assert TR22(1 + cot(x)**2) == csc(x)**2\n", "def test_TR11():\n    # Test the TR11 function with arguments that trigger both the base and auto-simplification conditions\n    # This should return a simplified version of the input\n    result = TR11(sin(4 * x / 3))\n    expected = 4 * (-sin(x / 3)**2 + cos(x / 3)**2) * sin(x / 3) * cos(x / 3)\n    assert result == expected\n\n    # Test the TR11 function with arguments that trigger only the base condition\n    result = TR11(cos(6), 3)\n    expected = -sin(3)**2 + cos(3)**2\n    assert result == expected\n\n    # Test the TR11 function with arguments that trigger only the auto-simplification condition\n    result = TR11(cos(2 * x))\n    expected = -sin(x)**2 + cos(x)**2\n    assert result == expected\n\n    # Test the TR11 function with arguments that trigger neither condition\n    result = TR11(cos(3 * x))\n    expected = cos(3 * x)\n    assert result == expected\n"], "sample_329": ["    def test_settings_reference_serializer(self):\n        value = SettingsReference(\"DEBUG\")\n        serializer = serializer_factory(value)\n        self.assertEqual(serializer.serialize(), (\"settings.DEBUG\", {\"from django.conf import settings\"}))\n", "    def test_frozenset_serialization(self):\n        value = frozenset([1, 2, 3])\n        serializer = serializer_factory(value)\n        result, imports = serializer.serialize()\n        self.assertEqual(result, \"frozenset([1, 2, 3])\")\n        self.assertEqual(imports, set())\n", "    def test_deconstructible_serializer(self):\n        serializer = serializer_factory(DeconstructibleInstances())\n        result, imports = serializer.serialize()\n        self.assertEqual(result, 'DeconstructibleInstances()')\n        self.assertEqual(imports, set())\n", "    def test_serializer_factory(self):\n        tests = [\n            (datetime.datetime(2022, 1, 1, 12, 0, 0), DatetimeDatetimeSerializer),\n            (datetime.date(2022, 1, 1), DateTimeSerializer),\n            (datetime.timedelta(days=1), DateTimeSerializer),\n            (datetime.time(12, 0, 0), DateTimeSerializer),\n            (SettingsReference('TIME_ZONE'), SettingsReferenceSerializer),\n            (float('nan'), FloatSerializer),\n            (float('inf'), FloatSerializer),\n            (bool(True), BaseSimpleSerializer),\n            (int(10), BaseSimpleSerializer),\n            (type(None), BaseSimpleSerializer),\n            (bytes(b'hello'), BaseSimpleSerializer),\n            (str('hello'), BaseSimpleSerializer),\n            (range(1, 5), BaseSimpleSerializer),\n            (decimal.Decimal('10.5'), DecimalSerializer),\n            (functools.partial(int, base=16), FunctoolsPartialSerializer),\n            (functools.partialmethod(int, base=16), FunctoolsPartialSerializer),\n            (lambda x: x**2, FunctionTypeSerializer),\n            (math.sin, FunctionTypeSerializer),\n            ([1, 2, 3], SequenceSerializer),\n            ({1, 2, 3}, SetSerializer),\n            ((1, 2, 3), TupleSerializer),\n            ({'a': 1, 'b': 2}, DictionarySerializer),\n            (models.Choices('A', 'B'), ChoicesSerializer),\n            (TextEnum.A, EnumSerializer),\n            (TextTranslatedEnum.A, EnumSerializer),\n            (BinaryEnum.A, EnumSerializer),\n            (IntEnum.A, EnumSerializer),\n            (uuid.UUID('123e4567-e89b-12d3-a456-426614174000'), UUIDSerializer),\n            (pathlib.PurePath('/home/user'), PathSerializer),\n            (os.PathLike(), PathLikeSerializer),\n            (re", "    def test_settings_reference_serialization(self):\n        value = SettingsReference('DEFAULT_FROM_EMAIL')\n        serializer = serializer_factory(value)\n        serialized_value, imports = serializer.serialize()\n        self.assertEqual(serialized_value, 'settings.DEFAULT_FROM_EMAIL')\n        self.assertEqual(imports, {'from django.conf import settings'})\n"], "sample_1170": ["def test_print_MatPow():\n    A = Matrix([[1, 2], [3, 4]])\n    B = A ** 2\n    assert sstr(B) == 'Matrix([[7, 10], [15, 22]])'\n", "def test_StrPrinter_Xor():\n    a, b = symbols('a b')\n    expr = Xor(a, b)\n    assert sstr(expr) == 'a ^ b'\n", "def test_print_euler_gamma():\n    assert sstr(EulerGamma) == \"EulerGamma\"\n", "def test_print_Tr():\n    A = Matrix([[1, 2], [3, 4]])\n    expr = Tr(A)\n    assert sstr(expr) == \"Tr(Matrix([[1, 2], [3, 4]]))\"\n", "def test_print_Integral():\n    expr = Integral(cos(x), (x, 0, pi))\n    assert sstr(expr) == \"Integral(cos(x), (x, 0, pi))\"\n"], "sample_18": ["def test_quantity_operation(self):\n    q2 = self.q * 2\n    assert_info_equal(q2, self.q, ignore={\"value\"})\n    assert (q2.value == self.q.value * 2).all()\n\n    q3 = self.q + 3 * u.m / u.s\n    assert_info_equal(q3, self.q, ignore={\"value\"})\n    assert (q3.value == self.q.value + 3).all()\n", "def test_binary_operation(self):\n    q2 = self.q * 2.0\n    assert_info_equal(q2, self.q, ignore={'shape'})\n    assert q2.info.name == 'v'\n    assert q2.info.description == 'air speed of a african swallow'\n    assert q2.value.tolist() == [2.0, 4.0, 6.0, 8.0]\n", "def test_info_round_trip(self):\n    # Test round trip through Quantity constructor\n    q2 = u.Quantity(self.q)\n    assert_info_equal(q2, self.q)\n", "def test_quantity_slice(self):\n    sliced_q = self.q[1:3]\n    assert_info_equal(sliced_q, self.q)\n", "def test_quantity_arithmetic(self):\n    q2 = u.Quantity(np.arange(1.0, 5.0), \"km/s\")\n    q2.info.name = \"v2\"\n    q2.info.description = \"air speed of a european swallow\"\n\n    q_add = self.q + q2\n    assert_info_equal(self.q, q_add, ignore={\"name\", \"description\"})\n    assert q_add.info.name is None\n    assert q_add.info.description is None\n\n    q_sub = self.q - q2\n    assert_info_equal(self.q, q_sub, ignore={\"name\", \"description\"})\n    assert q_sub.info.name is None\n    assert q_sub.info.description is None\n\n    q_mul = self.q * q2\n    assert_info_equal(self.q, q_mul, ignore={\"name\", \"description\"})\n    assert q_mul.info.name is None\n    assert q_mul.info.description is None\n\n    q_div = self.q / q2\n    assert_info_equal(self.q, q_div, ignore={\"name\", \"description\"})\n    assert q_div.info.name is None\n    assert q_div.info.description is None\n"], "sample_184": ["    def test_index_condition_checks(self):\n        class MyModel(models.Model):\n            name = models.CharField(max_length=50)\n            is_active = models.BooleanField(default=True)\n\n            class Meta:\n                indexes = [\n                    models.Index(fields=['name'], condition=models.Q(is_active=True)),\n                ]\n\n        errors = MyModel.check(databases=['default'])\n        self.assertEqual(errors, [])\n", "    def test_check_related_field_accessor_clashes(self):\n        class Parent(models.Model):\n            name = models.CharField(max_length=10)\n\n        class Child(models.Model):\n            parent = models.ForeignKey(Parent, on_delete=models.CASCADE)\n\n        errors = Child._check_property_name_related_field_accessor_clashes()\n        self.assertEqual(errors, [Error(\"The property 'parent' clashes with a related field accessor.\", obj=Child, id='models.E025')])\n", "    def test_check_index_together_with_invalid_field(self):\n        class InvalidIndexTogether(models.Model):\n            f1 = models.CharField(max_length=10)\n            f2 = models.CharField(max_length=10)\n\n            class Meta:\n                index_together = [('f1', 'invalid_field')]\n\n        errors = InvalidIndexTogether.check()\n        self.assertEqual(errors, [\n            Error(\n                \"'index_together' refers to the nonexistent field 'invalid_field'.\",\n                obj=InvalidIndexTogether,\n                id='models.E012',\n            )\n        ])\n", "    def test_long_column_names(self):\n        class LongColumnName(models.Model):\n            # Create a field with a long name that exceeds the maximum length for a column name.\n            long_field_name_that_exceeds_max_column_length = models.CharField(max_length=100)\n\n        allowed_len, db_alias = get_max_column_name_length()\n        if allowed_len is not None:\n            errors = LongColumnName._check_long_column_names(['default'])\n            self.assertEqual(len(errors), 1)\n            self.assertIsInstance(errors[0], Error)\n", "    def test_unique_constraints_with_conditions(self):\n        class MyModel(models.Model):\n            name = models.CharField(max_length=50)\n            group = models.CharField(max_length=50)\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(fields=['name'], condition=models.Q(group='group1')),\n                ]\n\n        errors = MyModel.check(databases=['default'])\n        self.assertEqual(len(errors), 1)\n        self.assertEqual(errors[0].id, 'models.W036')\n"], "sample_39": ["def test_fix_ctype_log_message(self):\n    \"\"\"\n    Test the _fix_ctype method with log_message=True.\n    \"\"\"\n    w = wcs.WCS(naxis=2)\n    w.wcs.ctype = ['RA---TAN', 'DEC--TAN']\n    header = fits.Header()\n    header['CTYPE1'] = 'RA---TAN'\n    header['CTYPE2'] = 'DEC--TAN'\n    w._fix_ctype(header, add_sip=True, log_message=True)\n    assert header['CTYPE1'] == 'RA---TAN-SIP'\n    assert header['CTYPE2'] == 'DEC--TAN-SIP'\n", "def test_maps_coverage(self):\n    for filename in self._file_list:\n        header_lines = get_pkg_data_contents(filename).split(\"\\n\")\n        header = fits.Header.fromstring(\"\\n\".join(header_lines))\n        w = wcs.WCS(header)\n        assert w.naxis == 2\n", "def test_wcs_pix2world_origin(self):\n    with fits.open(self._file_list[0]) as hdulist:\n        w = wcs.WCS(hdulist[0].header)\n        sky = w.pix2world([[1, 2], [3, 4]], 1)\n        assert_array_equal(sky, w.pix2world([[1, 2], [3, 4]], 0))\n        sky = w.pix2world([[1, 2], [3, 4]], 2)\n        assert_array_equal(sky, w.pix2world([[0, 1], [2, 3]], 1))\n", "def test_map_roundtrip(self):\n    for filename in self._file_list:\n        header_string = get_pkg_data_contents(filename)\n        header = fits.Header.fromstring(header_string, sep='\\n')\n        wcsobj = wcs.WCS(header)\n        assert str(wcsobj.to_header(relax=True)) == header_string\n", "def test_maps_content(self):\n    \"\"\"\n    Test that the contents of the files in the maps data directory are\n    unchanged.\n    \"\"\"\n    content_list = [get_pkg_data_contents(filename) for filename in self._file_list]\n    assert content_list == [\n        'SIMPLE  =                    T / conforms to FITS standard\\n'\n        'BITPIX  =                    8 / array data type\\n'\n        'NAXIS   =                    2 / number of array dimensions\\n'\n        'NAXIS1  =                  360\\n'\n        'NAXIS2  =                  180\\n'\n        'CRPIX1  =                  180.\\n'\n        'CRPIX2  =                   90.\\n'\n        'CDELT1  =                   -1.\\n'\n        'CDELT2  =                    1.\\n'\n        'CRVAL1  =                    0.\\n'\n        'CRVAL2  =                   90.\\n'\n        'CTYPE1  = \\'RA---AIT\\'\\n'\n        'CTYPE2  = \\'DEC--AIT\\'\\n'\n        'END\\n'\n    ]\n"], "sample_45": ["    def test_extract_year(self):\n        obj = self.create_model(datetime(2022, 6, 15), datetime(2022, 6, 16))\n        self.assertEqual(DTModel.objects.filter(start_datetime__year=ExtractYear(2022)).count(), 1)\n", "    def test_extract_datetime(self):\n        dt_model = self.create_model(datetime(2022, 3, 15, 12, 30, 45), None)\n        self.assertEqual(dt_model.start_datetime, ExtractYear(dt_model.start_datetime))\n        self.assertEqual(dt_model.start_datetime.year, ExtractYear(dt_model.start_datetime))\n        self.assertEqual(dt_model.start_datetime.month, ExtractMonth(dt_model.start_datetime))\n        self.assertEqual(dt_model.start_datetime.day, ExtractDay(dt_model.start_datetime))\n        self.assertEqual(dt_model.start_datetime.hour, ExtractHour(dt_model.start_datetime))\n        self.assertEqual(dt_model.start_datetime.minute, ExtractMinute(dt_model.start_datetime))\n        self.assertEqual(dt_model.start_datetime.second, ExtractSecond(dt_model.start_datetime))\n", "def test_trunc_second(self):\n    obj = self.create_model(datetime(2015, 1, 2, 3, 4, 5, 6789), datetime(2015, 1, 2, 3, 4, 5, 9876))\n    result = DTModel.objects.annotate(trunc_sec=TruncSecond('start_datetime')).get(id=obj.id).trunc_sec\n    expected = truncate_to(datetime(2015, 1, 2, 3, 4, 5), 'second')\n    self.assertEqual(result, expected)\n", "    def test_extract_datetime_fields(self):\n        dt = datetime(2022, 3, 14, 12, 34, 56)\n        obj = self.create_model(dt, dt + timedelta(hours=2))\n        self.assertEqual(DTModel.objects.filter(start_datetime__year=ExtractYear(dt)).count(), 1)\n        self.assertEqual(DTModel.objects.filter(start_datetime__iso_year=ExtractIsoYear(dt)).count(), 1)\n        self.assertEqual(DTModel.objects.filter(start_datetime__month=ExtractMonth(dt)).count(), 1)\n        self.assertEqual(DTModel.objects.filter(start_datetime__day=ExtractDay(dt)).count(), 1)\n        self.assertEqual(DTModel.objects.filter(start_datetime__week=ExtractWeek(dt)).count(), 1)\n        self.assertEqual(DTModel.objects.filter(start_datetime__week_day=ExtractWeekDay(dt)).count(), 1)\n        self.assertEqual(DTModel.objects.filter(start_datetime__quarter=ExtractQuarter(dt)).count(), 1)\n        self.assertEqual(DTModel.objects.filter(start_datetime__hour=ExtractHour(dt)).count(), 1)\n        self.assertEqual(DTModel.objects.filter(start_datetime__minute=ExtractMinute(dt)).count(), 1)\n        self.assertEqual(DTModel.objects.filter(start_datetime__second=ExtractSecond(dt)).count(), 1)\n", "    def test_trunc(self):\n        start_datetime = datetime(2022, 3, 15, 12, 30, 15)\n        end_datetime = start_datetime + timedelta(hours=2)\n        model = self.create_model(start_datetime, end_datetime)\n        truncated_date = DTModel.objects.annotate(trunc_date=Trunc('start_datetime', 'day')).get(id=model.id).trunc_date\n        self.assertEqual(truncated_date, truncate_to(start_datetime, 'day'))\n"], "sample_686": ["def test_deprecated_warnings(deprecated_warning):\n    with pytest.warns(PytestDeprecationWarning) as record:\n        # Trigger the deprecated warning\n        # This will depend on the specific warning and how it is triggered\n        # For example, it could be a function call, attribute access, etc.\n\n        # Check that the correct warning was raised\n        assert len(record) == 1\n        assert record[0].message.args[0] == str(deprecated_warning)\n", "def test_node_use_from_parent():\n    with warnings.catch_warnings(record=True) as w:\n        node = nodes.Item()\n        warnings.simplefilter(\"always\")\n        node = node.from_parent(None, name=\"test\")\n        assert len(w) == 1\n        assert issubclass(w[-1].category, deprecated.PytestDeprecationWarning)\n        assert \"Direct construction of Item has been deprecated\" in str(w[-1].message)\n", "def test_node_use_from_parent():\n    config = Config.fromdict({})\n    with pytest.warns(deprecated.NODE_USE_FROM_parent, match=\"direct construction of\"):\n        nodes.Item(name=\"test\", parent=None, config=config)\n", "def test_deprecated_warnings(testdir):\n    pytest_code = \"\"\"\n    import pytest\n    from _pytest import deprecated\n\n        class MyTest:\n            funcargnames = [\"foo\"]\n                pass\n\n        with pytest.warns(deprecated.FUNCARGNAMES):\n            pytest.Function(name=\"test_method\", parent=None, callobj=MyTest().test_method)\n\n        with pytest.warns(deprecated.FILLFUNCARGS):\n            pytest._fillfuncargs([\"foo\"], {\"foo\": \"bar\"}, {})\n\n        with pytest.warns(deprecated.RESULT_LOG):\n            testdir.parseconfig(\"--result-log=mylog.txt\")\n\n        with pytest.warns(deprecated.FIXTURE_POSITIONAL_ARGUMENTS):\n            @pytest.fixture(\"foo\")\n                pass\n\n        with pytest.warns(deprecated.NODE_USE_FROM_PARENT):\n            nodes.Item(name=\"test_item\", parent=None)\n\n        with pytest.warns(deprecated.JUNIT_XML_DEFAULT_FAMILY):\n            testdir.parseconfig(\"--junitxml=junit.xml\")\n\n        with pytest.warns(deprecated.COLLECT_DIRECTORY_HOOK):\n            testdir.makepyfile(\"def pytest_collect_directory(path, parent): pass\")\n            testdir.runpytest()\n\n        with pytest.warns(deprecated.PYTEST_COLLECT_MODULE):\n            from _pytest import collect\n            collect.Module(name=\"test_module\", parent=None, path", "def test_node_use_from_parent_warning():\n    with pytest.warns(deprecated.NODE_USE_FROM_PARENT):\n        nodes.Item(\"test_item\")\n"], "sample_391": ["def test_create_model_with_managers(self):\n    operations = [\n        migrations.CreateModel(\n            name='TestModel',\n            fields=[\n                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n            ],\n            managers=[\n                ('custom_objects', EmptyManager()),\n            ],\n        ),\n    ]\n    expected = [\n        migrations.CreateModel(\n            name='TestModel',\n            fields=[\n                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n            ],\n            managers=[\n                ('custom_objects', EmptyManager()),\n            ],\n        ),\n    ]\n    self.assertOptimizesTo(operations, expected, exact=1)\n", "def test_create_model_with_fields(self):\n    operations = [\n        operations.CreateModel(\n            \"UnicodeModel\",\n            fields=[\n                (\"name\", models.CharField(max_length=255)),\n                (\"description\", models.TextField()),\n            ],\n        ),\n    ]\n    self.assertOptimizesTo(operations, operations, exact=1)\n", "def test_remove_field_optimization(self):\n    operations = [\n        migrations.CreateModel(\n            \"MyModel\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"field1\", models.CharField(max_length=50)),\n                (\"field2\", models.IntegerField()),\n            ],\n        ),\n        migrations.RemoveField(\"MyModel\", \"field1\"),\n        migrations.RemoveField(\"MyModel\", \"field2\"),\n    ]\n    expected = [\n        migrations.CreateModel(\"MyModel\", [(\"id\", models.AutoField(primary_key=True))]),\n    ]\n    self.assertOptimizesTo(operations, expected, exact=2)\n", "def test_alter_model_managers(self):\n    operations = [\n        migrations.CreateModel(\n            name='UnicodeModel',\n            fields=[\n                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n            ],\n            managers=[('objects', models.Manager())],\n        ),\n        migrations.AlterModelManagers(\n            name='UnicodeModel',\n            managers=[('custom_objects', EmptyManager())],\n        ),\n    ]\n    expected = [\n        migrations.CreateModel(\n            name='UnicodeModel',\n            fields=[\n                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n            ],\n            managers=[('custom_objects', EmptyManager())],\n        ),\n    ]\n    self.assertOptimizesTo(operations, expected)\n", "def test_reduce_create_model_with_add_field(self):\n    operations = [\n        operations.CreateModel(\n            name='TestModel',\n            fields=[\n                ('id', models.AutoField(primary_key=True)),\n            ],\n        ),\n        operations.AddField(\n            model_name='TestModel',\n            name='new_field',\n            field=models.CharField(max_length=200),\n        ),\n    ]\n    expected = [\n        operations.CreateModel(\n            name='TestModel',\n            fields=[\n                ('id', models.AutoField(primary_key=True)),\n                ('new_field', models.CharField(max_length=200)),\n            ],\n        ),\n    ]\n    self.assertOptimizesTo(operations, expected, exact=1)\n"], "sample_688": ["def test_bestrelpath(self, tmp_path):\n    directory = tmp_path / \"directory\"\n    dest = directory / \"file.txt\"\n    directory.mkdir()\n    dest.touch()\n\n    relpath = bestrelpath(directory, dest)\n    assert relpath == \"file.txt\"\n\n    relpath = bestrelpath(directory / \"subdir\", dest)\n    assert relpath == \"..\\\\file.txt\" if sys.platform.startswith(\"win\") else \"../file.txt\"\n\n    with pytest.raises(ValueError):\n        bestrelpath(directory, Path(\"/another/directory\"))\n\n    with pytest.raises(ValueError):\n        bestrelpath(Path(\"/another/directory\"), dest)\n", "def test_bestrelpath(self, pytester: Pytester):\n    pytester.makepyfile(\n        textwrap.dedent(\n            \"\"\"\n            import pytest\n            from _pytest.pathlib import bestrelpath\n\n                directory = Path(\"/home/user/project\")\n                dest = Path(\"/home/user/project/subdir/file.py\")\n                assert bestrelpath(directory, dest) == \"subdir/file.py\"\n            \"\"\"\n        )\n    )\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)\n", "def test_make_numbered_dir_with_cleanup(self, tmpdir):\n    root = Path(tmpdir)\n    prefix = \"test_\"\n    keep = 2\n    lock_timeout = 60\n    new_path = make_numbered_dir_with_cleanup(root, prefix, keep, lock_timeout)\n    assert new_path.exists()\n    assert new_path.is_dir()\n    assert new_path.name.startswith(prefix)\n    # Check if the lock file is created\n    lock_path = get_lock_path(new_path)\n    assert lock_path.exists()\n    assert lock_path.is_file()\n    # Check if cleanup works\n    for i in range(3):\n        make_numbered_dir_with_cleanup(root, prefix, keep, lock_timeout)\n    # Check that only the last `keep` numbered directories are preserved\n    assert len(list(root.glob(prefix + \"*\"))) == keep\n", "def test_import_path_importlib_mode(self, pytester):\n    pytester.makepyfile(\n        test_module=\"\"\"\n            assert True\n        \"\"\"\n    )\n    pytester.syspathinsert(str(pytester.path))\n    import _pytest._pathlib as pathlib\n    mod = pathlib.import_path(pytester.path / \"test_module.py\", mode=pathlib.ImportMode.importlib)\n    assert mod.test_function() is None\n", "def test_import_path(self, tmp_path: Path) -> None:\n    module_name = \"test_module\"\n    module_code = \"def test_func():\\n    return 'test'\"\n    module_path = tmp_path / f\"{module_name}.py\"\n    module_path.write_text(module_code)\n\n    sys.path.insert(0, str(tmp_path))\n    try:\n        mod = import_path(module_path)\n        assert mod.test_func() == 'test'\n    finally:\n        sys.path.remove(str(tmp_path))\n"], "sample_888": ["def test_isolation_forest_sparse_matrix():\n    \"\"\"Test IsolationForest with sparse matrix input.\"\"\"\n    X = csc_matrix(iris.data)\n    clf = IsolationForest(random_state=0).fit(X)\n    y_pred = clf.predict(X)\n    assert_array_equal(y_pred, np.ones(X.shape[0]))\n", "def test_isolation_forest_with_sparse_input():\n    # Test that IsolationForest works with sparse input\n    X = csc_matrix([[0, 1], [2, 0], [0, 3], [4, 0]])\n    clf = IsolationForest(random_state=42)\n    clf.fit(X)\n    y_pred = clf.predict(X)\n    assert_array_equal(y_pred, np.array([1, 1, 1, 1]))\n", "def test_isolation_forest_predict_proba():\n    rng = check_random_state(0)\n    X = rng.uniform(size=(100, 2))\n    y = np.ones(100)\n    y[0] = -1  # Add one outlier\n    clf = IsolationForest(random_state=0)\n    clf.fit(X, y)\n    proba = clf.predict_proba(X)\n    assert proba.shape == (100, 2)\n    assert_array_almost_equal(proba.sum(axis=1), np.ones(100))\n    assert proba[0, 1] < proba[1:, 1].mean()  # The outlier has a lower outlier probability\n", "def test_isolation_forest_fit_predict():\n    X = [[-1.1], [0.3], [0.5], [100]]\n    y = [1, 1, 1, -1]  # Anomalies are labeled as -1\n    clf = IsolationForest(random_state=0).fit(X)\n    predictions = clf.predict(X)\n    assert_array_equal(predictions, y)\n", "def test_isolation_forest_sample_weight():\n    # Test IsolationForest with sample weights\n    X = np.random.RandomState(42).uniform(low=0, high=1, size=(100, 2))\n    y = np.random.RandomState(42).randint(0, 2, size=100)\n    sample_weight = np.random.RandomState(42).uniform(low=0.5, high=1.5, size=100)\n\n    clf = IsolationForest(random_state=42)\n    clf.fit(X, y, sample_weight=sample_weight)\n\n    # Check if sample weights are used in the decision function\n    weights_score = clf.decision_function(X)\n    clf.fit(X, y)\n    no_weights_score = clf.decision_function(X)\n\n    assert not np.allclose(weights_score, no_weights_score)\n"], "sample_1148": ["def test_matrix_element_derivative():\n    M = MatrixSymbol('M', n, n)\n    v = MatrixSymbol('v', n, 1)\n    i, j = symbols('i j', integer=True)\n    elem = MatrixElement(M, i, j)\n    dv = MatrixElement(v, i, 0)\n    assert elem._eval_derivative(dv) == KroneckerDelta(i, i, (0, n-1)) * KroneckerDelta(j, 0, (0, n-1))\n", "def test_from_index_summation():\n    i, j, N = symbols('i j N', integer=True)\n    A = MatrixSymbol(\"A\", N, N)\n    B = MatrixSymbol(\"B\", N, N)\n    expr = Sum(A[i, j]*B[j, i], (j, 0, N-1))\n    result = MatrixExpr.from_index_summation(expr)\n    assert result == A*B.T\n", "def test_matrix_element_derivative():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', m, l)\n    i, j, k = symbols('i j k', integer=True)\n    element = MatrixElement(A*B, i, j)\n    derivative = diff(element, MatrixElement(A, k, j))\n    assert derivative == KroneckerDelta(i, k) * B[j, :]\n", "def test_matrix_element_derivative():\n    M = MatrixSymbol('M', 2, 2)\n    N = MatrixSymbol('N', 2, 2)\n    i, j, x = symbols('i j x', integer=True)\n    ME = MatrixElement(M, i, j)\n    NE = MatrixElement(N, i, j)\n    assert diff(ME, x) == S.Zero\n    assert diff(ME, ME) == KroneckerDelta(i, j, (0, 1)) * KroneckerDelta(i, j, (0, 1))\n    assert diff(ME, NE) == S.Zero\n", "def test_matrix_derivative():\n    A_deriv = diff(A, A)\n    assert A_deriv.equals(Identity(n))\n"], "sample_802": ["def test_feature_union_get_feature_names():\n    union = FeatureUnion([(\"pca\", PCA(n_components=2)),\n                          (\"svd\", TruncatedSVD(n_components=2))])\n    X = [[0., 1., 3], [2., 2., 5]]\n    union.fit_transform(X)\n    feature_names = union.get_feature_names()\n    expected_names = ['pca__0', 'pca__1', 'svd__0', 'svd__1']\n    assert_equal(feature_names, expected_names)\n", "def test_pipeline_with_transform_fit_params():\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([0, 1, 0])\n    pipe = make_pipeline(TransfFitParams(a=1), LogisticRegression())\n    pipe.fit(X, y, logisticregression__C=0.1)\n    assert 'C' in pipe.named_steps['logisticregression'].get_params()\n    assert pipe.named_steps['transffitparams'].fit_params == {'C': 0.1}\n", "def test_pipeline_transformer_weights():\n    pipeline = make_pipeline(Mult(mult=2), Mult(mult=3))\n    X = np.array([[1, 2, 3], [4, 5, 6]])\n    expected_result = np.array([[6, 12, 18], [24, 30, 36]])\n    assert_array_equal(pipeline.transform(X), expected_result)\n\n    pipeline = make_pipeline(Mult(mult=2), Mult(mult=3), transformer_weights={\"mult-1\": 0.5})\n    expected_result = np.array([[3, 6, 9], [12, 15, 18]])\n    assert_array_equal(pipeline.transform(X), expected_result)\n", "def test_pipeline_getitem():\n    # Test that indexing a pipeline returns the correct estimator\n    transformer = StandardScaler()\n    estimator = LogisticRegression()\n    pipeline = Pipeline([('transformer', transformer), ('estimator', estimator)])\n\n    assert pipeline[0] is transformer\n    assert pipeline['transformer'] is transformer\n    assert pipeline[-1] is estimator\n    assert pipeline[:1][0] is transformer\n    assert pipeline[:-1][-1] is transformer\n", "def test_pipeline_fit_transform_params():\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([1, 2, 3])\n    pipeline = make_pipeline(TransfFitParams(), TransfFitParams())\n    pipeline.fit(X, y, svc__param1=1, svc__param2=2)\n    assert pipeline.named_steps['svc'].fit_params == {'param1': 1, 'param2': 2}\n"], "sample_1089": ["def test_decompose_power():\n    base, exp = decompose_power(x**(2*y))\n    assert base == x**y\n    assert exp == 2\n\n    base, exp = decompose_power(x**(2*y/3))\n    assert base == x**(y/3)\n    assert exp == 2\n", "def test_monotonic_sign():\n    p = Dummy(integer=True, positive=True)\n    assert _monotonic_sign(p - 1) == Dummy('nneg', nonnegative=True)\n    p2 = Dummy(integer=True, positive=True)\n    assert _monotonic_sign(p2*p + 1) == 2\n    assert _monotonic_sign(p**2 - 1) is None\n", "def test_factor_terms_fraction():\n    expr = x/2 + y/3\n    result = factor_terms(expr, fraction=True)\n    expected = (3*x + 2*y) / 6\n    assert result == expected\n", "def test_monotonic_sign():\n    p = Symbol('p', positive=True)\n    p2 = Symbol('p2', positive=True)\n    assert _monotonic_sign(p - 1) == Dummy('nneg', nonnegative=True)\n    assert _monotonic_sign(p2*p + 1) == 2\n    assert _monotonic_sign(p2**(1/2) - 1) is None\n", "def test_decompose_power():\n    base, exp = decompose_power(x**2)\n    assert base == x and exp == 2\n\n    base, exp = decompose_power(x**(2*y))\n    assert base == x**y and exp == 2\n\n    base, exp = decompose_power(x**(2*y/3))\n    assert base == x**(y/3) and exp == 2\n\n    base, exp = decompose_power(x)\n    assert base == x and exp == 1\n\n    base, exp = decompose_power(x**0)\n    assert base == x and exp == 1\n"], "sample_647": ["def test_unformatted_warning_format(warning_class):\n    if issubclass(warning_class, warning_types.UnformattedWarning):\n        template = \"Test {test_name} failed\"\n        kwargs = {\"test_name\": \"example_test\"}\n        warning = warning_class(template=template)\n        formatted_warning = warning.format(**kwargs)\n        assert str(formatted_warning) == template.format(**kwargs)\n", "def test_warn_explicit_for(pytester):\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n        from _pytest import warning_types\n\n            pass\n\n            warning_types.warn_explicit_for(some_function, warning_types.PytestWarning(\"test warning\"))\n        \"\"\"\n    )\n\n    result = pytester.runpytest()\n\n    # Check that the warning is emitted with the correct filename and lineno\n    result.stdout.fnmatch_lines([\"_pytest/warning_types.py:10: UserWarning: test warning\"])\n", "def test_warn_explicit_for(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n            pass\n\n        pytest.warn_explicit_for(test_function, pytest.PytestWarning(\"Test Warning\"))\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines([\"*::test_function:Test Warning\"])\n", "def test_warn_explicit_for(pytester):\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n        from _pytest import warning_types\n\n            pytest.warn_explicit_for(test_warning_explicit, warning_types.PytestWarning(\"Test warning\"))\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines([\"*test_warning_explicit.py:4: UserWarning: Test warning*\"])\n", "def test_warn_explicit_for(pytester: Pytester, warning_class):\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n        from _pytest import warning_types\n\n            pass\n\n        warning_types.warn_explicit_for(test_func, warning_class('Test warning'))\n        \"\"\"\n    )\n\n    result = pytester.runpytest()\n\n    result.stdout.fnmatch_lines(\n        [\"*test_func:1: UserWarning: Test warning\"]\n    )\n"], "sample_359": ["    def test_add_index(self):\n        operation = AddIndex('UnicodeModel', models.Index(fields=['name'], name='name_idx'))\n        project_state = ProjectState()\n        new_state = project_state.clone()\n        new_state.add_model(ModelState('app_label', 'UnicodeModel', [('name', models.CharField(max_length=255))], {}))\n        operation.state_forwards('app_label', new_state)\n        self.assertEqual(len(new_state.models['app_label', 'unicodemodel'].options['indexes']), 1)\n        self.assertEqual(new_state.models['app_label', 'unicodemodel'].options['indexes'][0].name, 'name_idx')\n        with connection.schema_editor() as schema_editor:\n            operation.database_forwards('app_label', schema_editor, project_state, new_state)\n            self.assertIn('name_idx', connection.introspection.get_constraints(connection.cursor(), 'app_label_unicodemodel'))\n            operation.database_backwards('app_label', schema_editor, project_state, new_state)\n            self.assertNotIn('name_idx', connection.introspection.get_constraints(connection.cursor(), 'app_label_unicodemodel'))\n", "def test_remove_field_from_model(self):\n    # Create a model with a field\n    operation = migrations.CreateModel('TestModel', [('field1', models.CharField(max_length=20))])\n    operation.state_forwards('test', ProjectState())\n\n    # Remove the field from the model\n    remove_field_operation = migrations.RemoveField('TestModel', 'field1')\n    new_state = remove_field_operation.reduce(operation, 'test')[0].state_forwards('test', ProjectState())\n\n    # Check that the field is removed from the state\n    self.assertNotIn('field1', [field[0] for field in new_state.models['test', 'testmodel'].fields])\n\n    # Check that the field is removed from the database\n    with connection.schema_editor() as schema_editor:\n        schema_editor.create_model(self.apps.get_model('test', 'TestModel'))\n        remove_field_operation.database_forwards('test', schema_editor, ProjectState(), new_state)\n        with self.assertRaises(FieldDoesNotExist):\n            self.apps.get_model('test', 'TestModel')._meta.get_field('field1')\n\n    # Check that the field is added back to the state and database when we go backwards\n    new_state_backwards = remove_field_operation.reduce(operation, 'test')[0].state_forwards('test', new_state)\n    self.assertIn('field1', [field[0] for field in new_state_backwards.models['test', 'testmodel'].fields])\n    with connection.schema_editor() as schema_editor:\n        remove_field_operation.database_backwards('test', schema_editor, new_state, new_state_backwards)\n        self.apps.get_model('test', 'TestModel')._meta.get_field('field1')\n", "    def test_create_model_add_field(self):\n        old_state = ProjectState()\n        old_state.add_model(ModelState(\"app_label\", \"ModelName\", [], {}, (models.Model,), []))\n\n        new_state = ProjectState()\n        new_state.add_model(ModelState(\"app_label\", \"ModelName\", [(\"new_field\", models.CharField(max_length=100))], {}, (models.Model,), []))\n\n        operation = AddField(\"ModelName\", \"new_field\", models.CharField(max_length=100))\n\n        operation.state_forwards(\"app_label\", old_state)\n        self.assertEqual(len(old_state.models[\"app_label\", \"modelname\"].fields), 1)\n        self.assertEqual(old_state.models[\"app_label\", \"modelname\"].fields[0].name, \"new_field\")\n\n        with self.assertRaisesMessage(FieldDoesNotExist, 'ModelName.new_field'):\n            old_state.apps.get_model(\"app_label\", \"ModelName\")._meta.get_field(\"new_field\")\n\n        with self.assertRaisesMessage(FieldDoesNotExist, 'ModelName.new_field'):\n            new_state.apps.get_model(\"app_label\", \"ModelName\")._meta.get_field(\"new_field\")\n\n        with connection.schema_editor() as editor:\n            operation.database_forwards(\"app_label\", editor, old_state, new_state)\n            self.assertTableExists(\"app_label_modelname\")\n            self.assertColumnExists(\"app_label_modelname\", \"new_field\")\n\n        with connection.schema_editor() as editor:\n            operation.database_backwards(\"app_label\", editor, old_state, new_state)\n            self.assertTableExists(\"app_label_modelname\")\n            self.assertColumnNotExists(\"app_label_modelname\", \"new_field\")\n", "    def test_add_index(self):\n        state = ProjectState()\n        state.add_model(ModelState(\n            'app',\n            'Model',\n            [\n                ('field1', models.IntegerField()),\n                ('field2', models.CharField(max_length=20)),\n            ],\n            {'managed': True},\n        ))\n        operation = migrations.AddIndex('Model', models.Index(fields=['field1'], name='field1_idx'))\n\n        with self.assertRaisesMessage(models.Index.DoesNotExist, \"Model.field1_idx\"):\n            state.apps.get_model('app', 'Model')._meta.indexes\n\n        new_state = operation.state_forwards('app', state)\n        self.assertEqual(new_state.models['app', 'model'].indexes, [models.Index(fields=['field1'], name='field1_idx')])\n\n        with connection.schema_editor() as editor:\n            operation.database_forwards('app', editor, state, new_state)\n\n        with self.assertNumQueries(1):\n            self.assertEqual(state.apps.get_model('app', 'Model')._meta.indexes, [models.Index(fields=['field1'], name='field1_idx')])\n\n        with connection.schema_editor() as editor:\n            operation.database_backwards('app', editor, new_state, state)\n\n        with self.assertRaisesMessage(models.Index.DoesNotExist, \"Model.field1_idx\"):\n            state.apps.get_model('app', 'Model')._meta.indexes\n", "    def test_alter_model_options(self):\n        \"\"\"\n        Test that AlterModelOptions operation changes model options correctly.\n        \"\"\"\n        operation = migrations.AlterModelOptions(\n            name='UnicodeModel',\n            options={\n                'verbose_name': 'Test Model',\n                'verbose_name_plural': 'Test Models',\n            },\n        )\n\n        # Forwards operation\n        new_state = operation.state_forwards('app_label', self.state)\n        self.assertEqual(new_state.models[('app_label', 'unicodemodel')].options['verbose_name'], 'Test Model')\n        self.assertEqual(new_state.models[('app_label', 'unicodemodel')].options['verbose_name_plural'], 'Test Models')\n\n        # Backwards operation\n        old_state = operation.state_forwards('app_label', new_state)\n        self.assertEqual(old_state.models[('app_label', 'unicodemodel')].options['verbose_name'], 'unicode model')\n        self.assertEqual(old_state.models[('app_label', 'unicodemodel')].options['verbose_name_plural'], 'unicode models')\n\n        # Database operations do not affect model options, so they should be no-ops\n        with connection.schema_editor() as editor:\n            operation.database_forwards('app_label', editor, self.state, new_state)\n            operation.database_backwards('app_label', editor, new_state, self.state)\n"], "sample_14": ["def test_wrap_at():\n    a = Angle([-20.0, 150.0, 350.0] * u.deg)\n    wrapped_a = a.wrap_at(360 * u.deg)\n    assert_allclose(wrapped_a.degree, [340.0, 150.0, 350.0])\n\n    a.wrap_at('180d', inplace=True)\n    assert_allclose(a.degree, [-20.0, 150.0, -10.0])\n", "def test_longitude_wrap_angle_setter():\n    lon = Longitude(90, unit=u.deg)\n    assert lon.wrap_angle == 360 * u.deg\n    lon.wrap_angle = 180 * u.deg\n    assert lon.wrap_angle == 180 * u.deg\n    assert lon == 90 * u.deg\n    lon.wrap_angle = 0 * u.deg\n    assert lon.wrap_angle == 0 * u.deg\n    assert lon == -270 * u.deg\n", "def test_longitude_wrap_angle():\n    l = Longitude([360, 180, -180, -360], u.deg, wrap_angle=180*u.deg)\n    assert_array_equal(l.value, [180, 180, -180, -180])\n\n    l.wrap_angle = 360*u.deg\n    assert_array_equal(l.value, [0, 180, 180, 0])\n\n    l.wrap_angle = 0*u.deg\n    assert_array_equal(l.value, [-180, -180, 180, 180])\n", "def test_longitude_wrap_angle_setter():\n    # Test setting the wrap_angle of a Longitude object\n    lon = Longitude([180, 270, 360], unit=u.deg)\n    assert_array_equal(lon.degree, [180, 270, 0])  # Default wrap_angle is 360 deg\n\n    lon.wrap_angle = 180 * u.deg\n    assert_array_equal(lon.degree, [-180, -90, 0])  # Now wrapped at 180 deg\n", "def test_longitude_wrap_angle_setter():\n    # Test that setting the wrap_angle property of a Longitude object wraps the angle values correctly\n    lon = Longitude([361.0, 362.0, 363.0], unit=u.deg)\n    lon.wrap_angle = 180.0 * u.deg\n    assert_allclose(lon.degree, [-179.0, -178.0, -177.0])\n"], "sample_465": ["def test_get_object(self):\n    ma = ModelAdmin(Band, self.site)\n    obj = ma.get_object(request, str(self.band.id))\n    self.assertEqual(obj, self.band)\n", "def test_model_admin_has_change_permission(self):\n    ma = ModelAdmin(Band, self.site)\n    self.assertTrue(ma.has_change_permission(request, self.band))\n", "    def test_get_form(self):\n        ma = ModelAdmin(Band, self.site)\n        Form = ma.get_form(request)\n        self.assertIsInstance(Form(), forms.ModelForm)\n", "def test_formfield_for_foreignkey(self):\n    ma = ModelAdmin(Band, self.site)\n    db_field = models.ForeignKey(Concert, on_delete=models.CASCADE)\n    widget = ma.formfield_for_foreignkey(db_field, request).widget\n    self.assertIsInstance(widget, Select)\n\n    ma.raw_id_fields = ['concert']\n    widget = ma.formfield_for_foreignkey(db_field, request).widget\n    self.assertIsInstance(widget, widgets.ForeignKeyRawIdWidget)\n\n    ma.raw_id_fields = []\n    ma.radio_fields = {'concert': HORIZONTAL}\n    widget = ma.formfield_for_foreignkey(db_field, request).widget\n    self.assertIsInstance(widget, AdminRadioSelect)\n    self.assertEqual(widget.attrs['class'], 'radiolist inline')\n", "    def test_get_queryset(self):\n        ma = ModelAdmin(Band, self.site)\n        # Test default queryset\n        self.assertQuerysetEqual(ma.get_queryset(request), Band.objects.all())\n\n        # Test ordering\n        ma.ordering = (\"name\",)\n        self.assertQuerysetEqual(ma.get_queryset(request), Band.objects.order_by(\"name\"))\n"], "sample_273": ["def test_check_indexes(self):\n    class ModelWithInvalidIndex(models.Model):\n        field1 = models.CharField(max_length=10)\n\n        class Meta:\n            app_label = 'check_framework'\n            indexes = [\n                models.Index(fields=['nonexistent_field']),\n            ]\n\n    expected_errors = [\n        Error(\n            \"'indexes' refers to the nonexistent field 'nonexistent_field'.\",\n            obj=ModelWithInvalidIndex,\n            id='models.E012',\n        )\n    ]\n    self.assertEqual(ModelWithInvalidIndex.check(), expected_errors)\n", "    def test_check_ordering(self):\n        class ValidOrderingModel(models.Model):\n            field1 = models.CharField(max_length=20)\n            field2 = models.IntegerField()\n\n            class Meta:\n                ordering = ['field1', '-field2']\n\n        class InvalidOrderingModel(models.Model):\n            class Meta:\n                ordering = 'invalid_field'\n\n        self.assertEqual(ValidOrderingModel.check(), [])\n        self.assertEqual(InvalidOrderingModel.check(), [\n            Error(\n                \"'ordering' must be a tuple or list (even if you want to order by only one field).\",\n                obj=InvalidOrderingModel,\n                id='models.E014',\n            )\n        ])\n", "    def test_check_constraints_with_partial_indexes_support(self):\n        class ConstraintsModel(models.Model):\n            name = models.CharField(max_length=50)\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(\n                        fields=['name'],\n                        condition=models.Q(name__startswith='test'),\n                    ),\n                ]\n\n        with self.settings(DATABASES={'default': {'ENGINE': 'django.db.backends.postgresql'}}):\n            errors = ConstraintsModel.check(databases=['default'])\n            self.assertNotIn(\n                Warning(\n                    'psycopg2 does not support unique constraints with conditions.',\n                    hint=\"A constraint won't be created. Silence this warning if you don't care about it.\",\n                    obj=ConstraintsModel,\n                    id='models.W036',\n                ),\n                errors,\n            )\n", "    def test_check_constraints_supported(self):\n        class CheckSupportedModel(models.Model):\n            char_field = models.CharField(max_length=255)\n            constraints = [\n                models.CheckConstraint(check=models.Q(char_field__length__gt=0), name='char_field_not_empty'),\n            ]\n\n            class Meta:\n                app_label = 'check_framework'\n\n        self.assertEqual(CheckSupportedModel.check(), [])\n", "    def test_check_indexes_with_condition_unsupported(self):\n        class MyModel(models.Model):\n            name = models.CharField(max_length=10)\n\n            class Meta:\n                app_label = 'check_framework'\n                indexes = [\n                    models.Index(fields=['name'], condition=models.Q(name__startswith='A')),\n                ]\n\n        errors = MyModel.check()\n        self.assertEqual(\n            errors,\n            [\n                Warning(\n                    'sqlite does not support indexes with conditions.',\n                    hint=\"Conditions will be ignored. Silence this warning if you don't care about it.\",\n                    obj=MyModel,\n                    id='models.W037',\n                )\n            ]\n        )\n"], "sample_1050": ["def test_print_none():\n    assert pycode(none) == \"None\"\n", "def test_numpy_print_assignment():\n    a = Assignment(x, y + z)\n    printer = NumPyPrinter()\n    result = printer.doprint(a)\n    assert result == \"x = y + z\"\n", "def test_print_SparseMatrix():\n    sm = SparseMatrix(3, 3, {(0, 0): 1, (1, 1): 2, (2, 2): 3})\n    printer = SciPyPrinter()\n    result = printer._print_SparseMatrix(sm)\n    expected = \"scipy.sparse.coo_matrix([1, 2, 3], ([0, 1, 2], [0, 1, 2]), shape=(3, 3))\"\n    assert result == expected\n", "def test_print_sign():\n    expr = sign(x)\n    code = PythonCodePrinter().doprint(expr)\n    assert code == '(0.0 if x == 0 else math.copysign(1, x))'\n", "def test_print_code_gen_array_elementwise_add():\n    expr = Assignment(p(x, y), sign(x) + sign(y))\n    result = PythonCodePrinter().doprint(expr)\n    assert \"numpy.add(sign(x), sign(y))\" in result\n"], "sample_793": ["def test_isolation_forest_contamination_auto():\n    X = iris.data\n    clf = IsolationForest(contamination='auto', random_state=0)\n    clf.fit(X)\n    y_pred = clf.predict(X)\n    outlier_fraction = np.mean(y_pred == -1)\n    assert_allclose(outlier_fraction, clf.contamination, atol=0.05)\n", "def test_isolation_forest_sparse_input():\n    rng = check_random_state(0)\n    X = csc_matrix(rng.rand(100, 50))\n    y = rng.randint(0, 2, 100)\n\n    clf = IsolationForest(random_state=rng)\n    clf.fit(X, y)\n\n    assert_equal(clf.n_features_, 50)\n    assert_equal(len(clf.estimators_), clf.n_estimators)\n    assert_equal(len(clf.estimators_samples_), clf.n_estimators)\n\n    y_pred = clf.predict(X)\n    assert_array_equal(np.unique(y_pred), [-1, 1])\n", "def test_isolation_forest_decision_function_behaviour():\n    X = iris.data\n    clf = IsolationForest(behaviour='new', random_state=42)\n    clf.fit(X)\n    scores_new = clf.decision_function(X)\n\n    clf = IsolationForest(behaviour='old', random_state=42)\n    clf.fit(X)\n    scores_old = clf.decision_function(X)\n\n    assert_array_almost_equal(scores_new, scores_old, decimal=5)\n", "def test_contamination_parameter(contamination):\n    X = iris.data\n    iforest = IsolationForest(contamination=contamination, random_state=rng)\n    iforest.fit(X)\n    y_pred = iforest.predict(X)\n    outliers = X[y_pred == -1]\n    assert_greater(len(outliers), 0)\n    assert_greater(len(outliers), len(X) * contamination)\n", "def test_isolation_forest_predict_outliers():\n    # Test the prediction of outliers in IsolationForest\n    X = np.array([[1, 2], [2, 3], [3, 4], [10, 11], [11, 12]])\n    y = np.array([1, 1, 1, -1, -1])  # -1 indicates outlier\n    clf = IsolationForest(n_estimators=10, max_samples=\"auto\", random_state=rng)\n    clf.fit(X)\n    y_pred = clf.predict(X)\n    assert_array_equal(y_pred, y)\n"], "sample_52": ["def test_model_choice_iterator(self):\n    field = forms.ModelChoiceField(queryset=Category.objects.all())\n    iterator = ModelChoiceIterator(field)\n    choices = list(iterator)\n    self.assertEqual(len(choices), Category.objects.count() + 1)  # +1 for the empty label\n    self.assertEqual(choices[0], ('', field.empty_label))\n    self.assertEqual(choices[1], (self.c1.pk, str(self.c1)))\n    self.assertEqual(choices[2], (self.c2.pk, str(self.c2)))\n    self.assertEqual(choices[3], (self.c3.pk, str(self.c3)))\n", "    def setUpTestData(cls):\n        cls.c1 = Category.objects.create(name='First', slug='first', url='first')\n        cls.c2 = Category.objects.create(name='Second', slug='second', url='second')\n", "def test_model_choice_field_limit_choices_to(self):\n    queryset = Category.objects.all()\n    form_field = ModelChoiceField(queryset, limit_choices_to={'slug': 'entertainment'})\n    self.assertEqual(list(form_field.choices), [('', '---------'), (self.c1.pk, str(self.c1))])\n", "def test_label_from_instance(self):\n    # Test the label_from_instance method of the ModelChoiceField\n    field = forms.ModelChoiceField(queryset=Category.objects.all())\n    obj = Category.objects.first()\n    label = field.label_from_instance(obj)\n    self.assertEqual(label, str(obj))  # Check if the label is equal to the string representation of the object\n", "    def test_model_choice_field(self):\n        form_field = ModelChoiceField(queryset=Category.objects.all())\n        self.assertEqual(list(form_field.choices), [\n            (str(self.c1.pk), str(self.c1)),\n            (str(self.c2.pk), str(self.c2)),\n            (str(self.c3.pk), str(self.c3)),\n        ])\n"], "sample_726": ["def test_label_binarizer_sparse_output():\n    lb = LabelBinarizer(sparse_output=True)\n    lb.fit([1, 2, 6, 4, 2])\n    result = lb.transform([1, 6])\n    assert issparse(result)\n    assert_array_equal(toarray(result), np.array([[1, 0, 0, 0], [0, 0, 0, 1]]))\n", "def test_label_binarizer_sparse_input_output():\n    lb = LabelBinarizer(sparse_output=True)\n    y = csr_matrix([[1], [2], [3], [4]])\n    lb.fit(y)\n    y_bin = lb.transform(y)\n    assert_true(issparse(y_bin))\n    y_inv = lb.inverse_transform(y_bin)\n    assert_equal(type_of_target(y_inv), 'multiclass')\n    assert_array_equal(toarray(y_inv), toarray(y))\n", "def test_label_binarize_multilabel():\n    y = np.array([[0, 1, 1], [1, 0, 0]])\n    classes = [0, 1, 2]\n    expected = np.array([[1, 0, 0], [0, 1, 0]])\n    assert_array_equal(label_binarize(y, classes), expected)\n", "def test_label_binarizer_sparse_output():\n    lb = LabelBinarizer(sparse_output=True)\n    lb.fit([1, 2, 6, 4, 2])\n    transformed = lb.transform([1, 6])\n    assert_true(issparse(transformed))\n    assert_array_equal(toarray(transformed), np.array([[1, 0, 0, 0], [0, 0, 0, 1]]))\n", "def test_label_binarizer_sparse_input_output():\n    lb = LabelBinarizer(sparse_output=True)\n    y = csr_matrix([[1, 0], [0, 1], [1, 1]])\n    lb.fit(y)\n    y_bin = lb.transform(y)\n    assert_true(issparse(y_bin))\n    y_inv = lb.inverse_transform(y_bin)\n    assert_true(issparse(y_inv))\n    assert_array_equal(toarray(y_inv), y.toarray())\n"], "sample_1028": ["def test_mod_denest():\n    assert Mod(Mod(x, y), y) == Mod(x, y)\n    assert Mod(Mod(x, -y), y) == Mod(x, y)\n    assert Mod(Mod(x, y), -y) == Mod(x, y)\n    assert Mod(Mod(x, -y), -y) == Mod(x, y)\n", "def test_mod_with_symbolic_expressions():\n    expr1 = Mod(x**2, y)\n    assert expr1.subs({x: 5, y: 6}) == 1\n    expr2 = Mod(x**2, 6)\n    assert expr2.subs({x: 5}) == 1\n", "def test_modulo_with_rational():\n    p = Rational(5, 3)\n    q = Rational(2, 3)\n    assert Mod(p, q) == Rational(1, 3)\n", "def test_mod_integer():\n    assert (x % 3)._eval_is_integer() == True\n    assert (x % b)._eval_is_integer() is None\n    assert (4 % 2)._eval_is_integer() == True\n    assert (5 % 2)._eval_is_integer() == True\n    assert (4 % b)._eval_is_integer() is None\n", "def test_mod_eval():\n    assert Mod(x**2, y).subs({x: 5, y: 6}) == 1\n    assert Mod(x, 2).subs({x: 4}) == 0\n    assert Mod(x, 2).subs({x: 5}) == 1\n    assert Mod(x, -2).subs({x: 5}) == -1\n    assert Mod(x, -2).subs({x: -5}) == 1\n    assert Mod(x, y).subs({x: 0, y: 6}) == 0\n    assert Mod(x, y).subs({x: y, y: 6}) == 0\n    assert Mod(x, y).subs({x: -y, y: 6}) == 0\n    assert Mod(x, 1) == 0\n    assert Mod(x, oo) == x\n    assert Mod(x, zoo) == nan\n    assert Mod(oo, x) == nan\n    assert Mod(zoo, x) == nan\n    assert Mod(x, 0) == zoo\n    assert Mod(x, -1) == Mod(x, 1)\n    assert Mod(x, Rational(1, 2)) == Mod(2*x, 1)\n    assert Mod(2*x, 3*x) == Mod(2*x - 3*x, 3*x) + Mod(3*x, 3*x)\n"], "sample_441": ["    def test_password_field_help_text(self):\n        form = UserChangeForm(instance=self.u1)\n        self.assertIn(f\"../../{self.u1.pk}/password/\", form.fields['password'].help_text)\n", "    def test_read_only_password_hash_field_help_text(self):\n        form = UserChangeForm(instance=self.u1)\n        expected_help_text = _(\n            \"Raw passwords are not stored, so there is no way to see this \"\n            'user\u2019s password, but you can change the password using '\n            '<a href=\"{}\">this form</a>.'\n        ).format(f\"../../{self.u1.pk}/password/\")\n        self.assertEqual(form.fields[\"password\"].help_text, expected_help_text)\n", "    def test_send_mail(self, mock_send):\n        form = PasswordResetForm({'email': self.u1.email})\n        form.is_valid()\n        form.save(domain_override='example.com', from_email='test@example.com')\n        mock_send.assert_called_once()\n", "    def test_password_help_text(self):\n        form = UserChangeForm(instance=self.u1)\n        self.assertIn(\n            f\"../../{self.u1.pk}/password/\",\n            form.fields['password'].help_text\n        )\n", "    def test_get_context_no_password(self):\n        widget = ReadOnlyPasswordHashWidget()\n        context = widget.get_context('password', None, {})\n        self.assertEqual(context['summary'][0]['label'], 'No password set.')\n"], "sample_521": ["def test_line_collection_2d_to_3d(fig):\n    ax = fig.add_subplot(111, projection='3d')\n    lines = [[(0, 0, 0), (1, 1, 1)], [(0, 0, 1), (1, 1, 0)]]\n    col = LineCollection(lines)\n    ax.add_collection(col)\n    art3d.line_collection_2d_to_3d(col, zs=[0, 1])\n", "def test_patch_2d_to_3d():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    patch = Circle((0.5, 0.5), 0.25)\n    ax.add_patch(patch)\n    art3d.patch_2d_to_3d(patch, z=0.5, zdir='z')\n    plt.close(fig)\n", "def test_path_collection_2d_to_3d(fig):\n    ax = fig.add_subplot(111, projection='3d')\n    paths = [Path([(0, 0), (1, 1), (0, 1)])]\n    col = art3d.PathCollection(paths, facecolors='none', edgecolors='r')\n    ax.add_collection(col)\n    art3d.patch_collection_2d_to_3d(col, zs=1, zdir='z', depthshade=True)\n    assert isinstance(col, art3d.Path3DCollection)\n    assert col.get_depthshade() == True\n    assert np.array_equal(col._offsets3d, [[0, 1, 0], [1, 1, 1], [0, 1, 1]])\n", "def test_pathpatch_3d_properties():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n\n    path = Path([(0, 0), (1, 1), (0, 1)])\n    patch = PathPatch(path, zs=2, zdir='z', facecolor='b')\n    ax.add_patch(patch)\n\n    assert np.allclose(patch._segment3d, [(0, 0, 2), (1, 1, 2), (0, 1, 2)])\n    assert patch._code3d == [1, 2, 79]\n", "def test_line3d_collection_do_3d_projection():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    lines = [[(0, 0, 0), (1, 1, 1)], [(2, 2, 2), (3, 3, 3)]]\n    col = art3d.Line3DCollection(lines)\n    ax.add_collection3d(col)\n    minz = col.do_3d_projection()\n    assert isinstance(minz, float)\n"], "sample_490": ["    def test_check_constraint(self):\n        product = Product(name='Test', price=10)\n        product.full_clean()  # Should not raise an exception\n\n        product = Product(name='Test', price=-10)\n        with self.assertRaises(ValidationError):\n            product.full_clean()  # Should raise an exception\n", "    def test_validate_check_constraint(self):\n        constraint = CheckConstraint(check=Q(price__gt=0), name=\"positive_price\")\n        product = Product(name=\"Test\", price=-1)\n        with self.assertRaises(ValidationError) as cm:\n            constraint.validate(Product, product)\n        self.assertEqual(cm.exception.message, \"Constraint \u201cpositive_price\u201d is violated.\")\n", "    def test_check_constraint(self):\n        product = Product(price=-10)\n        with self.assertRaises(ValidationError) as context:\n            product.full_clean()\n        self.assertEqual(context.exception.message, \"Price must be greater than 0.\")\n\n        product.price = 10\n        product.full_clean()\n\n        # Test that the constraint is enforced in the database\n        with self.assertRaises(IntegrityError):\n            with connection.cursor() as cursor:\n                cursor.execute(\"INSERT INTO myapp_product (name, price) VALUES ('Test', -10)\")\n", "    def setUp(self):\n        self.product = Product.objects.create(price=50)\n", "    def test_check_constraint(self):\n        # Test CheckConstraint creation\n        check = CheckConstraint(check=Q(price__gt=0), name='price_check')\n        self.assertEqual(str(check), \"<CheckConstraint: check=Q(price__gt=0) name='price_check'>\")\n\n        # Test CheckConstraint validation\n        with self.assertRaisesMessage(ValidationError, \"Constraint \u201cprice_check\u201d is violated.\"):\n            Product.objects.create(name='Invalid product', price=-10)\n\n        # Test CheckConstraint creation with violation_error_message\n        check = CheckConstraint(check=Q(price__gt=0), name='price_check', violation_error_message=\"Price must be greater than zero.\")\n        with self.assertRaisesMessage(ValidationError, \"Price must be greater than zero.\"):\n            Product.objects.create(name='Invalid product', price=-10)\n"], "sample_141": ["def test_deserialization_error_with_data(self):\n    original_exc = ValueError(\"Test error\")\n    model = \"test_model\"\n    fk = \"test_fk\"\n    field_value = \"test_value\"\n    error = DeserializationError.WithData(original_exc, model, fk, field_value)\n    self.assertEqual(\n        str(error),\n        \"%s: (%s:pk=%s) field_value was '%s'\" % (original_exc, model, fk, field_value)\n    )\n", "def test_deserialize_fk_value(self):\n    field = models.ForeignKey(Category, on_delete=models.CASCADE)\n    using = None\n    handle_forward_references = False\n\n    # Test with None value\n    self.assertIsNone(serializers.deserialize_fk_value(field, None, using, handle_forward_references))\n\n    # Test with existing natural key\n    category = Category.objects.create(name=\"Test Category\")\n    field_value = [\"Test Category\"]\n    self.assertEqual(serializers.deserialize_fk_value(field, field_value, using, handle_forward_references), category.pk)\n\n    # Test with non-existing natural key and handle_forward_references = True\n    field_value = [\"Non-existing Category\"]\n    self.assertEqual(serializers.deserialize_fk_value(field, field_value, using, True), serializers.DEFER_FIELD)\n\n    # Test with non-existing natural key and handle_forward_references = False\n    with self.assertRaises(Category.DoesNotExist):\n        serializers.deserialize_fk_value(field, field_value, using, False)\n", "def test_deserialize_fk_value_with_forward_reference(self):\n    class RelatedModel(models.Model):\n        name = models.CharField(max_length=100)\n\n    class TestModel(models.Model):\n        related = models.ForeignKey(RelatedModel, on_delete=models.CASCADE)\n\n    field = TestModel._meta.get_field('related')\n    using = 'default'\n    handle_forward_references = True\n\n    # Test with an object that doesn't exist yet\n    field_value = [123]\n    with self.assertRaises(serializers.DeserializationError):\n        serializers.deserialize_fk_value(field, field_value, using, handle_forward_references)\n\n    # Create the object and test again\n    RelatedModel.objects.create(id=123, name='Test')\n    result = serializers.deserialize_fk_value(field, field_value, using, handle_forward_references)\n    self.assertEqual(result, serializers.DEFER_FIELD)\n", "def test_deserialize_m2m_values(self):\n    field = models.ManyToManyField(Score)\n    field.remote_field.model = Score\n    field.remote_field.model._default_manager = Score.objects\n    field.remote_field.field_name = 'id'\n\n    # Test with valid input\n    field_value = [1, 2, 3]\n    using = 'default'\n    handle_forward_references = False\n    expected_output = [1, 2, 3]\n    self.assertEqual(serializers.deserialize_m2m_values(field, field_value, using, handle_forward_references), expected_output)\n\n    # Test with TypeError input\n    field_value = 'invalid'\n    with self.assertRaises(serializers.M2MDeserializationError):\n        serializers.deserialize_m2m_values(field, field_value, using, handle_forward_references)\n\n    # Test with ObjectDoesNotExist input\n    field_value = [999]\n    with self.assertRaises(serializers.M2MDeserializationError):\n        serializers.deserialize_m2m_values(field, field_value, using, handle_forward_references)\n\n    # Test with ObjectDoesNotExist input and handle_forward_references=True\n    field_value = [999]\n    handle_forward_references = True\n    expected_output = serializers.DEFER_FIELD\n    self.assertEqual(serializers.deserialize_m2m_values(field, field_value, using, handle_forward_references), expected_output)\n", "def test_deserialization_error(self):\n    # Test that a DeserializationError is raised when the deserialized data is invalid\n    invalid_data = \"\"\"\n    [\n        {\n            \"model\": \"serializers.category\",\n            \"fields\": {\"invalid_field\": \"Invalid Value\"}\n        }\n    ]\n    \"\"\"\n    with self.assertRaises(DeserializationError):\n        list(serializers.deserialize(\"json\", invalid_data))\n"], "sample_626": ["def test_vectorized_indexer_with_masked_values():\n    array = np.array([1, 2, 3, 4, 5])\n    indexer = VectorizedIndexer((np.array([0, -1, 2, -1, 4]),))\n    mask = create_mask(indexer, array.shape)\n    assert np.array_equal(mask, np.array([False, True, False, True, False]))\n    indexer = posify_mask_indexer(indexer)\n    assert np.array_equal(indexer.tuple[0], np.array([0, 0, 2, 2, 4]))\n", "def test_pandas_multi_indexing_adapter_dtype(dtype):\n    data = np.arange(5)\n    index = pd.MultiIndex.from_arrays([data.astype(dtype), data[::-1].astype(dtype)], names=[\"level1\", \"level2\"])\n    adapter = PandasMultiIndexingAdapter(index)\n    assert adapter.dtype == dtype\n", "def test_explicit_indexing_adapter_vectorized_indexer():\n    # Test vectorized indexing with explicit indexing adapter\n    shape = (3, 4, 5)\n    array = np.arange(np.prod(shape)).reshape(shape)\n    key = VectorizedIndexer(([0, 1], [2, 3], [0, 1, 2]))\n    result = explicit_indexing_adapter(key, shape, IndexingSupport.VECTORIZED, array.__getitem__)\n    expected = array[(np.array([0, 1]), np.array([2, 3]), np.array([0, 1, 2]))]\n    np.testing.assert_array_equal(result, expected)\n", "def test_pandas_multi_indexing_adapter_with_level():\n    arrays = [['bar', 'bar', 'baz', 'baz', 'foo', 'foo', 'qux', 'qux'],\n              ['one', 'two', 'one', 'two', 'one', 'two', 'one', 'two']]\n    tuples = list(zip(*arrays))\n    index = pd.MultiIndex.from_tuples(tuples, names=['first', 'second'])\n    adapter = PandasMultiIndexingAdapter(index, level='first')\n\n    result = adapter[OuterIndexer(([0, 4],))]\n    expected = pd.Index(['bar', 'foo'], name='first')\n    assert_array_equal(result, expected)\n", "def test_pandas_multi_indexing_adapter():\n    levels = [['a', 'b', 'c'], ['one', 'two', 'three']]\n    codes = [[0, 0, 1], [0, 1, 2]]\n    array = pd.MultiIndex(levels=levels, codes=codes)\n    adapter = PandasMultiIndexingAdapter(array)\n\n    # Test __getitem__ with integer indexer\n    assert adapter[BasicIndexer((1,))] == ('b', 'one')\n\n    # Test __getitem__ with slice indexer\n    expected_result = pd.MultiIndex(levels=levels, codes=[[0, 0], [0, 1]])\n    assert adapter[BasicIndexer((slice(2),))].array.equals(expected_result)\n\n    # Test __getitem__ with vectorized indexer\n    indexer = VectorizedIndexer((np.array([0, 2]), np.array([1, 0])))\n    expected_result = pd.MultiIndex(levels=levels, codes=[[0, 1], [1, 0]])\n    assert adapter[indexer].array.equals(expected_result)\n\n    # Test _convert_scalar method\n    assert adapter._convert_scalar(('b', 'two')) == np.array(('b', 'two'), dtype=object)\n\n    # Test __repr__ method\n    assert repr(adapter) == f\"PandasMultiIndexingAdapter(array={array!r}, level=None, dtype={adapter.dtype!r})\"\n"], "sample_204": ["    def test_check_consistent_history(self):\n        \"\"\"\n        Test that check_consistent_history raises an error if any applied\n        migrations have unapplied dependencies.\n        \"\"\"\n        loader = MigrationLoader(connection)\n        # Apply a migration with an unapplied dependency.\n        recorder = MigrationRecorder(connection)\n        recorder.record_applied('app', '0001_initial')\n        # Check that check_consistent_history raises an error.\n        with self.assertRaises(InconsistentMigrationHistory):\n            loader.check_consistent_history(connection)\n", "    def test_load_disk_with_stale_pyc_file(self):\n        # Create a stale .pyc file in the migrations directory\n        stale_pyc_path = os.path.join(self.migrations_dir, '0001_initial.pyc')\n        with open(stale_pyc_path, 'w') as f:\n            f.write('stale pyc file content')\n\n        # Try to load the migration, it should raise an ImportError\n        with self.assertRaisesMessage(ImportError, \"Couldn't import 'migrations.0001_initial' as it appears to be a stale .pyc file.\"):\n            MigrationLoader(connection).load_disk()\n", "    def test_detect_conflicts(self):\n        loader = MigrationLoader(connection)\n        loader.build_graph()\n        # Add some conflicts\n        loader.graph.add_node(('app1', '0001_initial'), None)\n        loader.graph.add_node(('app1', '0002_add_field'), None)\n        conflicts = loader.detect_conflicts()\n        self.assertEqual(conflicts, {'app1': ['0001_initial', '0002_add_field']})\n", "def test_build_graph_with_replacements(self):\n    \"\"\"\n    Test that build_graph() handles replacing migrations correctly.\n    \"\"\"\n    loader = MigrationLoader(connection)\n    # Assume that 'replaced_app' has a migration '0001_initial' that has been replaced by '0002_squashed'\n    loader.disk_migrations = {\n        ('replaced_app', '0001_initial'): MockMigration(),\n        ('replaced_app', '0002_squashed'): MockMigration(replaces=['0001_initial']),\n    }\n    loader.applied_migrations = {\n        ('replaced_app', '0002_squashed'): MockMigration(),\n    }\n    loader.build_graph()\n    # The graph should contain the squashed migration and not the replaced one\n    self.assertIn(('replaced_app', '0002_squashed'), loader.graph.nodes)\n    self.assertNotIn(('replaced_app', '0001_initial'), loader.graph.nodes)\n", "    def test_build_graph_with_replacement(self):\n        \"\"\"\n        Test building the migration graph with a replacement migration.\n        \"\"\"\n        app_label = 'test_app'\n        migration_name = '0002_auto_replace'\n        replaced_migration_name = '0001_initial'\n\n        loader = MigrationLoader(connection, load=False)\n        loader.disk_migrations = {\n            (app_label, replaced_migration_name): self.create_migration(app_label, replaced_migration_name),\n            (app_label, migration_name): self.create_migration(app_label, migration_name, replaces=[replaced_migration_name])\n        }\n        loader.applied_migrations = {\n            (app_label, replaced_migration_name): loader.disk_migrations[(app_label, replaced_migration_name)]\n        }\n        loader.replace_migrations = True\n\n        loader.build_graph()\n\n        self.assertIn((app_label, migration_name), loader.graph.nodes)\n        self.assertNotIn((app_label, replaced_migration_name), loader.graph.nodes)\n"], "sample_984": ["def test_print_Tr():\n    A = Matrix([[1, 2], [3, 4]])\n    assert sstr(Tr(A)) == \"Tr(Matrix([[1, 2], [3, 4]]))\"\n", "def test_print_Tr():\n    A = Matrix([[1, 2], [3, 4]])\n    expr = Tr(A)\n    assert sstr(expr) == \"Tr(Matrix([[1, 2], [3, 4]]))\"\n", "def test_StrPrinter_print_Tr():\n    A = MatrixSymbol('A', 2, 2)\n    expr = Tr(A)\n    assert sstr(expr) == \"Tr(A)\"\n", "def test_limit_print():\n    expr = Limit(sin(x)/x, x, 0)\n    result = sstr(expr)\n    assert result == 'Limit(sin(x)/x, x, 0)'\n", "def test_print_StrPrinter():\n    printer = StrPrinter()\n\n    # Test _print_Tr\n    A = Matrix([[1, 2], [3, 4]])\n    assert printer._print(Tr(A)) == \"Tr(Matrix([[1, 2], [3, 4]]))\"\n"], "sample_422": ["def test_forward_many_to_many_descriptor_get_queryset(self):\n    # Test that the get_queryset method of the ForwardManyToManyDescriptor class\n    # returns the correct queryset of related objects.\n\n    # First, test that the queryset returned by get_queryset is correct when\n    # no objects have been prefetched.\n    authors = self.book1.authors.all()\n    self.assertIsInstance(authors, QuerySet)\n    self.assertEqual(authors.count(), 3)\n    self.assertIn(self.author1, authors)\n    self.assertIn(self.author2, authors)\n    self.assertIn(self.author3, authors)\n\n    # Next, test that the queryset returned by get_queryset is correct when\n    # objects have been prefetched using prefetch_related_objects.\n    prefetch_related_objects([self.book1], \"authors\")\n    authors = self.book1.authors.all()\n    self.assertIsInstance(authors, QuerySet)\n    self.assertEqual(authors.count(), 3)\n    self.assertIn(self.author1, authors)\n    self.assertIn(self.author2, authors)\n    self.assertIn(self.author3, authors)\n\n    # Finally, test that the queryset returned by get_queryset is correct when\n    # objects have been prefetched using prefetch_related.\n    book = Book.objects.prefetch_related(\"authors\").get(pk=self.book1.pk)\n    authors = book.authors.all()\n    self.assertIsInstance(authors, QuerySet)\n    self.assertEqual(authors.count(), 3)\n    self.assertIn(self.author1, authors)\n    self.assertIn(self.author2, authors)\n    self.assertIn(self.author3, authors)\n", "def test_forward_many_to_one_descriptor(self):\n    # Test __get__()\n    self.assertEqual(self.book1.first_author, self.author1)\n    with self.assertRaises(Book.first_author.RelatedObjectDoesNotExist):\n        Book.objects.create(title=\"Test Book\").first_author\n\n    # Test __set__()\n    book5 = Book.objects.create(title=\"Test Book\")\n    book5.first_author = self.author2\n    book5.refresh_from_db()\n    self.assertEqual(book5.first_author, self.author2)\n\n    # Test __set__() with None\n    book5.first_author = None\n    book5.refresh_from_db()\n    with self.assertRaises(Book.first_author.RelatedObjectDoesNotExist):\n        book5.first_author\n", "    def test_get_object(self):\n        book = Book.objects.get(id=self.book1.id)\n        author = book.first_author\n        self.assertEqual(author, self.author1)\n", "def test_reverse_many_to_one_prefetch_related(self):\n    authors = Author.objects.prefetch_related(Prefetch('book_set', queryset=Book.objects.order_by('title')))\n    for author in authors:\n        with self.subTest(author=author):\n            books = list(author.book_set.all())\n            self.assertEqual(books, list(Book.objects.filter(authors=author).order_by('title')))\n", "def test_prefetch_related_with_filters(self):\n    readers_with_prefetched_books = Reader.objects.prefetch_related(\n        Prefetch('books_read', queryset=Book.objects.filter(title__startswith='S'))\n    )\n    with self.assertNumQueries(2):  # One for Reader, one for Book\n        readers_books = {reader: [book.title for book in reader.books_read.all()] for reader in readers_with_prefetched_books}\n    self.assertEqual(readers_books[self.reader1], ['Sense and Sensibility'])\n    self.assertEqual(readers_books[self.reader2], ['Sense and Sensibility'])\n"], "sample_1100": ["def test_pow_eval_difference_delta():\n    assert (a**n)._eval_difference_delta(n, 1) == a**(n + 1) - a**n\n    assert (a**n)._eval_difference_delta(n, -1) == a**(n - 1) - a**n\n    assert (a**n)._eval_difference_delta(n, 2) == a**(n + 2) - a**n\n", "def test_eval_is_complex():\n    assert (a + b*I).is_complex\n    assert not (a + b).is_complex\n    assert (a + b*I).conjugate().is_complex\n", "def test_pow_is_extended_real():\n    assert Pow(2, 3).is_extended_real() == True\n    assert Pow(-1, 2).is_extended_real() == True\n    assert Pow(I, 2).is_extended_real() == True\n    assert Pow(-2, I).is_extended_real() == None\n", "def test_eval_is_finite():\n    assert Pow(3, 2)._eval_is_finite() is True\n    assert Pow(x, y)._eval_is_finite() is None\n    assert Pow(0, x)._eval_is_finite() is False\n    assert Pow(x, -1)._eval_is_finite() is True\n    assert Pow(oo, x)._eval_is_finite() is False\n", "def test_pow_exp_eval():\n    assert same_and_same_prec(Pow(2, x)._eval_evalf(15), Float(2**2.0, 15))\n"], "sample_226": ["    def test_test_db_signature(self):\n        test_connection = get_connection_copy()\n        creation = BaseDatabaseCreation(test_connection)\n        test_db_name = TEST_DATABASE_PREFIX + test_connection.settings_dict['NAME']\n        test_connection.settings_dict['NAME'] = test_db_name\n        expected_signature = (\n            test_connection.settings_dict['HOST'],\n            test_connection.settings_dict['PORT'],\n            test_connection.settings_dict['ENGINE'],\n            test_db_name,\n        )\n        self.assertEqual(creation.test_db_signature(), expected_signature)\n", "    def test_test_db_signature(self):\n        test_connection = get_connection_copy()\n        db_creation = BaseDatabaseCreation(test_connection)\n        test_database_name = 'test_database'\n        test_connection.settings_dict['NAME'] = test_database_name\n        expected_signature = (\n            test_connection.settings_dict['HOST'],\n            test_connection.settings_dict['PORT'],\n            test_connection.settings_dict['ENGINE'],\n            TEST_DATABASE_PREFIX + test_database_name,\n        )\n        self.assertEqual(db_creation.test_db_signature(), expected_signature)\n", "    def test_test_db_signature(self):\n        test_connection = get_connection_copy()\n        db_creation = BaseDatabaseCreation(test_connection)\n\n        # Set up some test settings\n        test_connection.settings_dict['HOST'] = 'test_host'\n        test_connection.settings_dict['PORT'] = '1234'\n        test_connection.settings_dict['ENGINE'] = 'test_engine'\n        test_connection.settings_dict['NAME'] = 'test_database'\n\n        # Calculate the expected signature\n        expected_signature = (\n            'test_host',\n            '1234',\n            'test_engine',\n            TEST_DATABASE_PREFIX + 'test_database',\n        )\n\n        # Test the test_db_signature method\n        self.assertEqual(db_creation.test_db_signature(), expected_signature)\n", "    def test_test_db_signature(self):\n        test_connection = get_connection_copy()\n        db_creation = BaseDatabaseCreation(test_connection)\n        test_connection.settings_dict['NAME'] = 'test_db'\n        expected_signature = ('localhost', '', 'django.db.backends.dummy', 'test_' + 'test_db')\n        self.assertEqual(db_creation.test_db_signature(), expected_signature)\n", "    def setUp(self):\n        self.test_connection = get_connection_copy()\n        self.database_creation = BaseDatabaseCreation(self.test_connection)\n"], "sample_727": ["def test_imputer_invalid_strategy():\n    # Test that an error is raised when an invalid strategy is used\n    X = np.array([[1, 2], [np.nan, 3], [7, 6]])\n    imputer = Imputer(missing_values=np.nan, strategy=\"invalid_strategy\")\n    assert_raises(ValueError, imputer.fit, X)\n", "def test_imputer_zero_strategy():\n    X = np.array([[1, 2, np.nan], [3, np.nan, 5], [6, 7, 8]])\n    X_true = np.array([[1, 2, 0], [3, 0, 5], [6, 7, 8]])\n    strategy = \"constant\"\n    missing_values = np.nan\n    statistics = [1, 0, 5]\n\n    _check_statistics(X, X_true, strategy, statistics, missing_values)\n", "compilation error", "def test_impute_with_custom_axis_strategy():\n    X = np.array([[1, 2, np.nan], [3, 4, np.nan], [np.nan, 6, 7]])\n    X_true = np.array([[1, 2, 3], [3, 4, 4], [5, 6, 7]])\n    strategy = \"mean\"\n    statistics = np.array([1, 2, 5])\n    missing_values = np.nan\n\n    _check_statistics(X, X_true, strategy, statistics, missing_values)\n", "def test_impute_with_axis_1():\n    \"\"\"Test impute with axis = 1\"\"\"\n    X = np.array([[1, 2, np.nan],\n                  [4, np.nan, 6],\n                  [7, np.nan, 9],\n                  [np.nan, np.nan, np.nan]])\n    X_true = np.array([[1, 2, 3],\n                       [4, 5, 6],\n                       [7, 8, 9],\n                       [np.nan, np.nan, np.nan]])\n    strategy = \"mean\"\n    missing_values = np.nan\n    statistics = np.array([1.5, 5, 8])\n    _check_statistics(X, X_true, strategy, statistics, missing_values)\n"], "sample_855": ["def test_dummy_classifier_constant_strategy():\n    X = np.array([[0], [1], [2], [3]])\n    y = np.array([0, 1, 2, 2])\n    clf = DummyClassifier(strategy='constant', constant=2)\n    clf.fit(X, y)\n    y_pred = clf.predict(X)\n    assert_array_equal(y_pred, np.array([2, 2, 2, 2]))\n", "def test_dummy_regressor_constant_strategy():\n    X = np.array([[0], [1], [2], [3]])\n    constant = np.array([[10, 20]])\n    y = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])  # ignored\n\n    est = DummyRegressor(strategy=\"constant\", constant=constant)\n    est.fit(X, y)\n    y_pred = est.predict(X)\n    assert_array_equal(constant, y_pred)\n", "def test_dummy_classifier_constant_strategy():\n    X = np.array([[0], [1], [2], [3]])\n    y = np.array([1, 2, 2, 1])\n    clf = DummyClassifier(strategy=\"constant\", constant=1)\n    clf.fit(X, y)\n    y_pred = clf.predict(X)\n    assert_array_equal(y_pred, np.full_like(y, 1))\n", "def test_dummy_regressor_constant_strategy():\n    X = np.array([[0], [1], [2], [3]])\n    y = np.array([5, 5, 5, 5])\n    est = DummyRegressor(strategy='constant', constant=5)\n    est.fit(X, y)\n    y_pred = est.predict(X)\n    assert_array_equal(y, y_pred)\n", "def test_dummy_classifier_stratified_strategy():\n    X = np.array([[0], [1], [2], [3]])\n    y = np.array([0, 1, 0, 1])\n    clf = DummyClassifier(strategy='stratified')\n    clf.fit(X, y)\n    y_pred = clf.predict(X)\n    assert y_pred.shape == y.shape\n    assert np.mean(y_pred == y) == pytest.approx(0.5, abs=0.1)  # Check if approximately 50% of predictions are correct\n"], "sample_953": ["def test_generate_function():\n    d = {\n        'path': '.',\n        'sep': False,\n        'dot': '_',\n        'project': 'Test Project',\n        'author': 'Test Author',\n        'version': '1.0',\n        'release': '1.0',\n        'language': 'en',\n        'suffix': '.rst',\n        'master': 'index',\n        'extensions': [],\n        'makefile': True,\n        'batchfile': True,\n    }\n\n    qs.generate(d, overwrite=False, silent=True)\n\n    assert path.exists('conf.py')\n    assert path.exists('index.rst')\n    assert path.exists('Makefile')\n    assert path.exists('make.bat')\n    assert path.exists('_templates')\n    assert path.exists('_static')\n", "def test_generate():\n    # Define the dictionary with the required values\n    d = {\n        'path': 'test_project',\n        'sep': False,\n        'dot': '_',\n        'project': 'Test Project',\n        'author': 'Test Author',\n        'version': '1.0',\n        'release': '1.0.0',\n        'language': 'en',\n        'suffix': '.rst',\n        'master': 'index',\n        'extensions': [],\n        'makefile': True,\n        'batchfile': True,\n        'quiet': True\n    }\n\n    # Call the generate function\n    qs.generate(d, overwrite=True, silent=True)\n\n    # Check that the expected files were created\n    assert path.isfile(path.join(d['path'], 'conf.py'))\n    assert path.isfile(path.join(d['path'], d['master'] + d['suffix']))\n    assert path.isfile(path.join(d['path'], 'Makefile'))\n    assert path.isfile(path.join(d['path'], 'make.bat'))\n    assert path.isdir(path.join(d['path'], d['dot'] + 'templates'))\n    assert path.isdir(path.join(d['path'], d['dot'] + 'static'))\n", "def test_valid_dir(tmp_path):\n    # Test when directory does not exist\n    d = {'path': str(tmp_path), 'dot': '_', 'master': 'index', 'suffix': '.rst', 'sep': False}\n    assert qs.valid_dir(d) is True\n\n    # Test when directory is a file\n    (tmp_path / 'test.txt').touch()\n    d = {'path': str(tmp_path / 'test.txt')}\n    assert qs.valid_dir(d) is False\n\n    # Test when directory contains reserved names\n    (tmp_path / 'conf.py').touch()\n    d = {'path': str(tmp_path), 'dot': '_', 'master': 'index', 'suffix': '.rst', 'sep': False}\n    assert qs.valid_dir(d) is False\n", "def test_generate_with_templatedir():\n    d = {\n        'path': '.',\n        'sep': False,\n        'dot': '_',\n        'project': 'Test Project',\n        'author': 'Test Author',\n        'version': '1.0',\n        'release': '1.0.0',\n        'language': 'en',\n        'suffix': '.rst',\n        'master': 'index',\n        'extensions': [],\n        'makefile': True,\n        'batchfile': True,\n    }\n    templatedir = 'test_template_dir'\n    qs.generate(d, overwrite=True, silent=True, templatedir=templatedir)\n    # Add assertions to verify the generated files based on the template directory\n", "def test_generate_function_with_custom_templates(tmp_path):\n    templates_path = tmp_path / 'templates'\n    templates_path.mkdir()\n    (templates_path / 'conf.py_t').write_text('Custom conf.py template')\n    (templates_path / 'quickstart' / 'root_doc.rst_t').write_text('Custom root doc template')\n\n    d = {\n        'path': str(tmp_path),\n        'sep': False,\n        'dot': '_',\n        'project': 'TestProject',\n        'author': 'TestAuthor',\n        'version': '1.0',\n        'release': '1.0',\n        'language': 'en',\n        'suffix': '.rst',\n        'master': 'index',\n        'extensions': [],\n        'makefile': True,\n        'batchfile': True,\n        'make_mode': True,\n    }\n\n    qs.generate(d, silent=True, templatedir=str(templates_path))\n\n    conf_file = tmp_path / 'conf.py'\n    master_file = tmp_path / 'index.rst'\n    makefile = tmp_path / 'Makefile'\n    batchfile = tmp_path / 'make.bat'\n\n    assert conf_file.read_text() == 'Custom conf.py template'\n    assert master_file.read_text() == 'Custom root doc template'\n    assert makefile.is_file()\n    assert batchfile.is_file()\n"], "sample_1062": ["def test_as_f_sign_1():\n    assert as_f_sign_1(x + 1) == (1, x, 1)\n    assert as_f_sign_1(x - 1) == (1, x, -1)\n    assert as_f_sign_1(-x + 1) == (-1, x, -1)\n    assert as_f_sign_1(-x - 1) == (-1, x, 1)\n    assert as_f_sign_1(2*x + 2) == (2, x, 1)\n", "def test_as_f_sign_1():\n    assert as_f_sign_1(x + 1) == (1, x, 1)\n    assert as_f_sign_1(x - 1) == (1, x, -1)\n    assert as_f_sign_1(-x + 1) == (-1, x, -1)\n    assert as_f_sign_1(-x - 1) == (-1, x, 1)\n    assert as_f_sign_1(2*x + 2) == (2, x, 1)\n", "def test_TRpower():\n    eq = sin(x)**6\n    result = TRpower(eq)\n    expected = -15*cos(2*x)/32 + 3*cos(4*x)/16 - cos(6*x)/32 + 5/16\n    assert result == expected\n", "def test_TRpower():\n    eq = sin(x)**6\n    result = TRpower(eq)\n    expected = -15*cos(2*x)/32 + 3*cos(4*x)/16 - cos(6*x)/32 + 5/16\n    assert result == expected\n", "def test_TR14():\n    assert TR14((cos(x) - 1)*(cos(x) + 1)) == -sin(x)**2\n    assert TR14((sin(x) - 1)*(sin(x) + 1)) == -cos(x)**2\n    assert TR14((cos(x) + 1)*(cos(x) - 1)) == -sin(x)**2\n    assert TR14((sin(x) + 1)*(sin(x) - 1)) == -cos(x)**2\n    assert TR14((cos(x) - 1)**2) == sin(x)**2\n    assert TR14((sin(x) - 1)**2) == cos(x)**2\n    assert TR14((cos(x) + 1)**2) == sin(x)**2\n    assert TR14((sin(x) + 1)**2) == cos(x)**2\n"], "sample_300": ["def test_add_filtered_relation(self):\n    query = Query(Author)\n    filtered_relation = RelatedIsNull('items')\n    query.add_filtered_relation(filtered_relation, 'items')\n    self.assertEqual(query._filtered_relations['items'], filtered_relation)\n", "def test_filter_with_related_isnull(self):\n    query = Query(Item)\n    query.add_filter(('author__isnull', True))\n    self.assertEqual(len(query.where.children), 1)\n    child = query.where.children[0]\n    self.assertIsInstance(child, RelatedIsNull)\n    self.assertEqual(child.lhs.target, Author._meta.pk)\n", "def test_build_filter_with_subquery(self):\n    query = Query(Author)\n    filter_expr = ('items__ranking__value__lt', 5)\n    clause, _ = query.build_filter(filter_expr, split_subq=False)\n    self.assertEqual(len(clause.children), 1)\n    subquery = clause.children[0]\n    self.assertIsInstance(subquery, Exists)\n    self.assertEqual(subquery.query.model, Ranking)\n    self.assertEqual(len(subquery.query.where.children), 1)\n    lookup = subquery.query.where.children[0]\n    self.assertIsInstance(lookup, LessThan)\n    self.assertEqual(lookup.lhs.target.name, 'value')\n    self.assertEqual(lookup.rhs, 5)\n", "    def test_build_lookup_transform(self):\n        query = Query(Author)\n        lookup_parts = ['lower']\n        lhs = Col('name', Author._meta.get_field('name'))\n        rhs = 'JOHN'\n        lookup = query.build_lookup(lookup_parts, lhs, rhs)\n        self.assertIsInstance(lookup, Exact)\n        self.assertIsInstance(lookup.lhs, Lower)\n        self.assertEqual(lookup.rhs, rhs.lower())\n", "    def test_build_filter_with_filtered_relation(self):\n        query = Query(Author)\n        filtered_relation = FilteredRelation('books', {'price__lt': 10})\n        query.add_filtered_relation(filtered_relation, 'book_alias')\n        filter_clause, _ = query.build_filter(('book_alias__title', 'Test'))\n        self.assertEqual(len(filter_clause.children), 1)\n        self.assertIsInstance(filter_clause.children[0], Exact)\n        self.assertEqual(filter_clause.children[0].rhs, 'Test')\n        self.assertEqual(filter_clause.children[0].lhs.target.name, 'title')\n        self.assertEqual(query._filtered_relations['book_alias'].relation_name, 'books')\n        self.assertEqual(query._filtered_relations['book_alias'].condition, Q(price__lt=10))\n"], "sample_1045": ["compilation error", "def test_sympify_mpq():\n    x = mpmath.rational.mpq(1, 2)\n    assert sympify_mpq(x) == Rational(1, 2)\n", "compilation error", "def test_integer_log():\n    assert integer_log(100, 10) == 2\n    assert integer_log(100, 2) == 6\n    assert integer_log(25, 5) == 2\n    assert integer_log(10, 3) is None\n    assert integer_log(1, 10) == 0\n    assert integer_log(0, 10) is None\n", "def test_rational_init():\n    # Test initializing a Rational with a string\n    r1 = Rational('3/4')\n    assert r1.p == 3 and r1.q == 4\n\n    # Test initializing a Rational with a float\n    r2 = Rational(0.75)\n    assert r2.p == 3 and r2.q == 4\n\n    # Test initializing a Rational with a fraction\n    r3 = Rational(fractions.Fraction(3, 4))\n    assert r3.p == 3 and r3.q == 4\n\n    # Test initializing a Rational with integers\n    r4 = Rational(3, 4)\n    assert r4.p == 3 and r4.q == 4\n\n    # Test initializing a Rational with a negative integer\n    r5 = Rational(-3, 4)\n    assert r5.p == -3 and r5.q == 4\n\n    # Test initializing a Rational with zero\n    r6 = Rational(0, 4)\n    assert r6.p == 0 and r6.q == 1\n\n    # Test initializing a Rational with an invalid input\n    with raises(TypeError):\n        Rational('abc')\n"], "sample_1071": ["def test_check_dimensions():\n    # Test adding a number to a dimensional quantity\n    with raises(ValueError):\n        check_dimensions(mile + 6)\n\n    # Test adding quantities with incompatible dimensions\n    with raises(ValueError):\n        check_dimensions(mile + hour)\n\n    # Test adding quantities with compatible dimensions\n    assert check_dimensions(mile + km) == 1.609344*mile + km\n\n    # Test multiplicative constants on Dimensions\n    assert check_dimensions(2 * meter) == 2 * meter\n    assert check_dimensions(2 * meter * kilogram / second) == 2 * meter * kilogram / second\n", "def test_quantity_simplify():\n    from sympy.physics.units import kilo, foot, inch\n    expr = kilo*foot*inch\n    simplified_expr = quantity_simplify(expr)\n    assert simplified_expr == 250*foot**2/3\n\n    expr = foot - 6*inch\n    simplified_expr = quantity_simplify(expr)\n    assert simplified_expr == foot/2\n", "def test_convert_to_multiple_units():\n    result = convert_to(newton, [meter, kilogram, second])\n    expected = kilogram * meter / second ** 2\n    assert result == expected\n", "def test_convert_to_multiple_units():\n    # Test conversion to multiple units\n    result = convert_to(newton, [meter, kilogram, second])\n    assert result == kilogram * meter / second**2\n", "def test_quantity_simplify():\n    from sympy.physics.units import quantity_simplify\n    from sympy.physics.units.prefixes import milli\n    from sympy.physics.units import meter, second\n\n    expr = milli*meter/second\n    simplified_expr = quantity_simplify(expr)\n    assert simplified_expr == 0.001*meter/second\n\n    expr = meter + 2*second\n    with raises(ValueError):\n        quantity_simplify(expr)\n"], "sample_467": ["def test_value_from_datadict(self):\n    data = {\n        \"my_date_year\": \"2010\",\n        \"my_date_month\": \"2\",\n        \"my_date_day\": \"15\",\n    }\n    result = self.widget.value_from_datadict(data, None, \"my_date\")\n    self.assertEqual(result, \"2010-02-15\")\n", "def test_format_value(self):\n    value = date(2015, 10, 31)\n    formatted_value = self.widget.format_value(value)\n    self.assertEqual(formatted_value, {\"year\": 2015, \"month\": 10, \"day\": 31})\n", "    def test_empty_label(self):\n        widget = SelectDateWidget(\n            years=(\n                \"2007\",\n                \"2008\",\n                \"2009\",\n                \"2010\",\n                \"2011\",\n                \"2012\",\n                \"2013\",\n                \"2014\",\n                \"2015\",\n                \"2016\",\n            ),\n            empty_label=(\"Choose year\", \"Choose month\", \"Choose day\"),\n        )\n        context = widget.get_context(\"date\", None, {})\n        self.assertIn((\"\", \"Choose year\"), context[\"widget\"][\"subwidgets\"][0][\"choices\"])\n        self.assertIn((\"\", \"Choose month\"), context[\"widget\"][\"subwidgets\"][1][\"choices\"])\n        self.assertIn((\"\", \"Choose day\"), context[\"widget\"][\"subwidgets\"][2][\"choices\"])\n", "    def test_format_value_with_date_object(self):\n        value = date(2015, 10, 21)\n        expected_output = {'year': 2015, 'month': 10, 'day': 21}\n        self.assertEqual(self.widget.format_value(value), expected_output)\n", "    def test_empty_label_is_tuple(self):\n        rendered = self.widget.render(\"date\", None, attrs={\"id\": \"test_id\"})\n        self.assertHTMLEqual(\n            rendered,\n            '<select name=\"date_year\" id=\"id_date_year\">'\n            '<option value=\"\">Year</option>'\n            '<option value=\"2007\">2007</option>'\n            '<option value=\"2008\">2008</option>'\n            '<option value=\"2009\">2009</option>'\n            '</select>'\n            '<select name=\"date_month\" id=\"id_date_month\">'\n            '<option value=\"\">Month</option>'\n            # ... rest of the months\n            '</select>'\n            '<select name=\"date_day\" id=\"id_date_day\">'\n            '<option value=\"\">Day</option>'\n            # ... rest of the days\n            '</select>',\n        )\n"], "sample_593": ["def test_array_repr(dataarray):\n    html_repr = fh.array_repr(dataarray)\n    assert isinstance(html_repr, str)\n    assert \"xarray.DataArray\" in html_repr\n    assert \"Dimensions\" in html_repr\n    assert \"Attributes\" in html_repr\n", "def test_array_repr(dataarray):\n    arr_repr = fh.array_repr(dataarray)\n    assert isinstance(arr_repr, str)\n    assert \"xarray.DataArray\" in arr_repr\n    assert \"Dimensions\" in arr_repr\n    assert \"Attributes\" in arr_repr\n", "def test_summarize_variable_multiindex(multiindex):\n    result = fh.summarize_coord(\"x\", multiindex.x)\n    assert \"level_1\" in result\n    assert \"level_2\" in result\n", "def test_array_repr(dataarray):\n    html_repr = fh.array_repr(dataarray)\n    assert isinstance(html_repr, str)\n    assert \"<pre class='xr-text-repr-fallback'>\" in html_repr\n    assert \"<div class='xr-wrap'\" in html_repr\n", "def test_array_repr(dataarray):\n    result = fh.array_repr(dataarray)\n    assert isinstance(result, str)\n    assert result.startswith(\"<div>\")\n    assert result.endswith(\"</div>\")\n    assert \"xr-array-name\" in result\n    assert \"xr-array-preview\" in result\n    assert \"xr-array-data\" in result\n"], "sample_712": ["def test_ordinal_encoder_with_unsorted_categories():\n    enc = OrdinalEncoder(categories=[['c', 'a', 'b'], [3, 1, 2]])\n    X = [['a', 2], ['b', 3]]\n    assert_raises_regex(ValueError, \"Unsorted categories are not supported for numerical categories\", enc.fit, X)\n", "def test_ordinal_encoder_inverse_transform():\n    enc = OrdinalEncoder()\n    X = [['Male', 1], ['Female', 3], ['Female', 2]]\n    enc.fit(X)\n    X_transformed = enc.transform(X)\n    X_inverse_transformed = enc.inverse_transform(X_transformed)\n    assert_array_equal(X, X_inverse_transformed)\n", "def test_ordinal_encoder_with_unseen_categories():\n    enc = OrdinalEncoder()\n    X = [['Male'], ['Female'], ['Unknown']]\n    enc.fit(X)\n    X_test = [['Female'], ['Unknown'], ['Male']]\n    X_transformed = enc.transform(X_test)\n    assert_array_equal(X_transformed, [[1], [2], [0]])\n    with pytest.raises(ValueError):\n        enc.transform([['NewCategory']])\n", "def test_ordinal_encoder_inverse_transform():\n    enc = OrdinalEncoder()\n    X = [['Male', 1], ['Female', 3], ['Female', 2]]\n    enc.fit(X)\n    X_trans = enc.transform(X)\n    X_inv = enc.inverse_transform(X_trans)\n    assert_array_equal(X, X_inv)\n", "def test_onehotencoder_unknown_categories():\n    X = np.array([[1, 'a'], [2, 'b'], [3, 'c']])\n    enc = OneHotEncoder(categories=[[1, 2], ['a', 'b']], handle_unknown='ignore')\n    enc.fit(X)\n    X_test = np.array([[4, 'd'], [2, 'b']])\n    X_trans = enc.transform(X_test)\n    assert_array_equal(X_trans.toarray(), [[0, 0, 0, 1], [0, 1, 0, 0]])\n"], "sample_108": ["    def test_converter_to_url(self):\n        for url, (url_name, namespace, kwargs) in converter_test_data:\n            match = resolve(url)\n            self.assertEqual(match.url_name, url_name)\n            self.assertEqual(match.app_name, namespace)\n            self.assertEqual(match.kwargs, kwargs)\n            reversed_url = reverse(url_name, kwargs=kwargs, urlconf='urlpatterns.path_urls')\n            self.assertEqual(url, reversed_url)\n", "    def to_python(self, value):\n        return value\n", "    def test_resolve_error_handler(self):\n        resolver = get_resolver()\n        handler, param_dict = resolver.resolve_error_handler(404)\n        self.assertEqual(handler, empty_view)\n        self.assertEqual(param_dict, {})\n", "def test_converter_to_url(self):\n    \"\"\"\n    Test that Converter.to_url() returns the correct URL representation for a value.\n    \"\"\"\n    for url, (url_name, _, expected_kwargs) in converter_test_data:\n        with self.subTest(url=url):\n            match = resolve(url)\n            for key, value in expected_kwargs.items():\n                self.assertEqual(match.kwargs[key], value)\n                # Test Converter.to_url() for the matched value.\n                converter = match.route.converters[key]\n                url_representation = converter.to_url(value)\n                self.assertEqual(reverse(url_name, kwargs={key: url_representation}), url)\n", "    def test_dynamic_converter(self):\n        DynamicConverter.regex = 'dynamic-(?P<value>\\w+)'\n        urlpatterns = [\n            path('test/<dynamic:value>/', empty_view, name='dynamic-test'),\n            path('test/<dynamic:value>/subpatterns/<dynamic:subvalue>/', empty_view, name='subpattern-dynamic'),\n            path('test/<dynamic:value>/namespaced/<dynamic:subvalue>/', empty_view, name='subpattern-dynamic-namespaced'),\n        ]\n        with self.settings(ROOT_URLCONF=urlpatterns):\n            for url, (url_name, app_name, kwargs) in converter_test_data:\n                resolved = resolve(url)\n                self.assertEqual(resolved.url_name, url_name)\n                self.assertEqual(resolved.app_name, app_name)\n                self.assertEqual(resolved.kwargs, kwargs)\n                reversed_url = reverse(url_name, kwargs=kwargs)\n                self.assertEqual(reversed_url, url)\n"], "sample_531": ["def test_figure_savefig_svg():\n    fig = Figure()\n    ax = fig.add_subplot(111)\n    ax.plot([1, 2, 3], [4, 5, 6])\n\n    buffer = io.BytesIO()\n    fig.savefig(buffer, format='svg')\n    buffer.seek(0)\n    data = buffer.read()\n    assert b'<svg' in data\n    assert b'<path' in data\n    assert b'M186.000' in data\n    assert b'L372.000' in data\n", "def test_figure_align_labels():\n    fig, axs = plt.subplots(2, 2)\n    for tick in axs[0, 0].get_xticklabels():\n        tick.set_rotation(55)\n    axs[0, 0].set_xlabel('XLabel 0')\n    axs[0, 1].set_xlabel('XLabel 1')\n    fig.align_xlabels()\n    axs[1, 0].plot(np.arange(0, 1000, 50))\n    axs[1, 0].set_ylabel('YLabel 0')\n    axs[1, 1].set_ylabel('YLabel 1')\n    fig.align_ylabels()\n", "def test_figure_align_labels():\n    fig, axs = plt.subplots(2, 2, figsize=(5, 4))\n    for i, ax in enumerate(axs.flat):\n        ax.plot([1, 2, 3], [i, i+1, i+2])\n        ax.set_xlabel('X Label')\n        ax.set_ylabel('Y Label')\n    fig.align_labels()\n    plt.close(fig)\n", "def test_subplots_adjust():\n    fig, axs = plt.subplots(2, 2)\n    fig.subplots_adjust(left=0.2, bottom=0.2, right=0.8, top=0.8, wspace=0.3, hspace=0.3)\n    assert fig.subplotpars.left == 0.2\n    assert fig.subplotpars.bottom == 0.2\n    assert fig.subplotpars.right == 0.8\n    assert fig.subplotpars.top == 0.8\n    assert fig.subplotpars.wspace == 0.3\n    assert fig.subplotpars.hspace == 0.3\n    fig.subplots_adjust(left=0.1, bottom=0.1, right=0.9, top=0.9, wspace=0.2, hspace=0.2)\n    assert fig.subplotpars.left == 0.1\n    assert fig.subplotpars.bottom == 0.1\n    assert fig.subplotpars.right == 0.9\n    assert fig.subplotpars.top == 0.9\n    assert fig.subplotpars.wspace == 0.2\n    assert fig.subplotpars.hspace == 0.2\n", "def test_align_labels():\n    fig, axs = plt.subplots(2, 2)\n    for ax in axs.flat:\n        ax.label_outer()\n    fig.align_labels()\n    for ax in axs.flat:\n        assert ax.get_xaxis().get_label_position() == ax.get_yaxis().get_label_position()\n"], "sample_928": ["def test_default_role():\n    from sphinx.util import docutils\n    from sphinx.util.rst import default_role\n\n    docname = \"dummy\"\n\n    with default_role(docname, \"em\"):\n        assert docutils.get_role(\"\") is not None\n\n    with default_role(docname, \"non_existent_role\"):\n        assert docutils.get_role(\"\") is None\n", "def test_default_role(mocker):\n    from sphinx.util import docutils\n\n    mock_unregister_role = mocker.patch.object(docutils, 'unregister_role')\n    mock_logger = mocker.patch('sphinx.util.rst.logger')\n\n    with default_role('test_doc', 'emphasis'):\n        pass\n\n    mock_unregister_role.assert_called_once_with('')\n    mock_logger.warning.assert_not_called()\n\n    with default_role('test_doc', 'nonexistent_role'):\n        pass\n\n    mock_logger.warning.assert_called_once()\n", "def test_default_role():\n    with default_role('test', 'literal'):\n        assert roles.role('', english, 0, None) is not None\n\n    with default_role('test', 'unknown_role'):\n        assert roles.role('', english, 0, None) is None\n", "def test_default_role():\n    with default_role('test', 'emphasis'):\n        assert 'emphasis' in docutils.get_all_roles()\n\n    assert 'emphasis' not in docutils.get_all_roles()\n\n    with default_role('test', 'nonexistent_role'):\n        pass\n\n    assert 'nonexistent_role' not in docutils.get_all_roles()\n", "def test_default_role(capsys):\n    from sphinx.util.rst import default_role\n    docname = \"test\"\n    name = \"code\"\n    with default_role(docname, name):\n        # Add a line of code to test the role function\n        pass\n    captured = capsys.readouterr()\n    assert \"default role code not found\" not in captured.err\n"], "sample_590": ["def test_concat_dataarray_different_names():\n    arr1 = DataArray(np.arange(10), name=\"a\")\n    arr2 = DataArray(np.arange(10, 20), name=\"b\")\n    with raises_regex(ValueError, \"array names not identical\"):\n        concat([arr1, arr2], dim=\"concat_dim\", compat=\"identical\")\n", "def test_concat_with_dim_coord():\n    data1 = DataArray(np.random.rand(3, 4), dims=(\"x\", \"y\"), name=\"A\")\n    data2 = DataArray(np.random.rand(3, 4), dims=(\"x\", \"y\"), name=\"A\")\n    dim_coord = pd.Index([\"a\", \"b\", \"c\"], name=\"dim\")\n    result = concat([data1, data2], dim=dim_coord)\n    expected = concat([data1, data2], dim=\"dim\")\n    assert_equal(result, expected)\n", "def test_concat_with_different_data_vars():\n    ds1 = Dataset({'x': Variable(('time'), np.arange(10)),\n                   'a': Variable(('time'), np.ones(10))})\n    ds2 = Dataset({'x': Variable(('time'), np.arange(10, 20)),\n                   'b': Variable(('time'), np.ones(10) * 2)})\n\n    result = concat([ds1, ds2], dim='time', data_vars='different')\n    expected = Dataset({'x': Variable(('time'), np.arange(20)),\n                        'a': Variable(('time'), np.concatenate([np.ones(10), np.full(10, np.nan)])),\n                        'b': Variable(('time'), np.concatenate([np.full(10, np.nan), np.ones(10) * 2]))})\n    assert_identical(result, expected)\n", "def test_concat_with_non_identical_global_attrs():\n    ds1 = Dataset({'x': [1, 2, 3]}, attrs={'global_attr': 'value1'})\n    ds2 = Dataset({'x': [4, 5, 6]}, attrs={'global_attr': 'value2'})\n\n    with pytest.raises(ValueError, match=\"Dataset global attributes not equal.\"):\n        concat([ds1, ds2], dim='y', compat='identical')\n", "def test_concat_dataarray_with_different_names():\n    a = DataArray(np.arange(10), dims=\"x\", name=\"A\")\n    b = DataArray(np.arange(10, 20), dims=\"x\", name=\"B\")\n\n    result = concat([a, b], dim=\"x\", compat=\"no_conflicts\")\n\n    expected = DataArray(np.concatenate([np.arange(10), np.arange(10, 20)]), dims=\"x\", name=\"A\")\n    assert_identical(result, expected)\n"], "sample_550": ["def test_set_navigate():\n    fig = Figure()\n    ax = fig.add_subplot(111)\n    assert ax.get_navigate() is True\n    ax.set_navigate(False)\n    assert ax.get_navigate() is False\n", "def test_minorticks_on_off(fig):\n    ax = fig.add_subplot(111)\n    ax.minorticks_on()\n    assert ax.xaxis.minor_locator is not None\n    assert ax.yaxis.minor_locator is not None\n    ax.minorticks_off()\n    assert ax.xaxis.minor_locator is None\n    assert ax.yaxis.minor_locator is None\n", "def test_get_tightbbox_for_layout_only():\n    fig, ax = plt.subplots()\n    ax.set_xlabel('x-label')\n    ax.set_ylabel('y-label')\n    ax.set_title('title')\n    bbox = ax.get_tightbbox(fig.canvas.get_renderer(), for_layout_only=True)\n    bbox_layout_only = ax.get_tightbbox(fig.canvas.get_renderer(), for_layout_only=True)\n    assert bbox_layout_only.width < bbox.width\n    assert bbox_layout_only.height < bbox.height\n", "def test_axes_contains():\n    fig, ax = plt.subplots()\n    canvas = FigureCanvasBase(fig)\n\n    # Test point outside the axes\n    mouseevent = MouseEvent(\"button_press_event\", canvas, 0, 0, button=MouseButton.LEFT)\n    assert not ax.contains(mouseevent)\n\n    # Test point inside the axes\n    mouseevent = MouseEvent(\"button_press_event\", canvas, 100, 100, button=MouseButton.LEFT)\n    assert ax.contains(mouseevent)\n\n    plt.close(fig)\n", "def test_navigate():\n    fig, ax = plt.subplots()\n    assert ax.get_navigate()\n    ax.set_navigate(False)\n    assert not ax.get_navigate()\n    ax.set_navigate(True)\n    assert ax.get_navigate()\n"], "sample_1151": ["def test_mod_with_add():\n    assert Mod(x + y + 2, x) == Mod(y + 2, x)\n", "def test_mod_simplification():\n    assert Mod(x + y + 2, x) == Mod(y + 2, x)\n    assert Mod(2*x, 3*y) == Mod(2*x, y)\n    assert Mod(0.6*x, 0.3*y) == 0.3*Mod(2*x, y)\n    assert Mod(1.0*x, 2.0*y) == 0.5*Mod(x, y)\n", "def test_mod_with_rational():\n    assert same_and_same_prec(Mod(Rational(7, 3), Rational(2, 3)), Rational(1, 3))\n", "def test_mod_eval():\n    # Test when p and q are both symbols\n    assert Mod(x, y) == Mod(x, y)\n\n    # Test when p and q are both numbers\n    assert Mod(5, 3) == 2\n\n    # Test when p and q are both expressions\n    assert Mod(x + 2, y - 1) == Mod(x + 2, y - 1)\n\n    # Test when p is a number and q is a symbol\n    assert Mod(7, y) == Mod(7, y)\n\n    # Test when p is a symbol and q is a number\n    assert Mod(x, 4) == Mod(x, 4)\n\n    # Test when p and q are both zero\n    assert Mod(0, 0) == nan\n\n    # Test when p is zero and q is a non-zero symbol\n    assert Mod(0, y) == 0\n\n    # Test when p is zero and q is a non-zero number\n    assert Mod(0, 3) == 0\n\n    # Test when p is a non-zero number and q is zero\n    with raises(ZeroDivisionError):\n        Mod(5, 0)\n\n    # Test when p and q are both complex numbers\n    assert Mod(2 + 3*I, 1 + 2*I) == 2*I - 1\n", "def test_mod_eval_with_add():\n    p = x + y\n    q = z\n    expected = Mod(x + y, z)\n    result = Mod.eval(p, q)\n    assert same_and_same_prec(result, expected)\n"], "sample_1099": ["def test_partial_derivative_evaluation():\n    expr = A(i)*B(j)\n    result = PartialDerivative(expr, i).doit()\n    assert result == B(j)*PartialDerivative(A(i), i)\n\n    result = PartialDerivative(expr, j).doit()\n    assert result == A(i)*PartialDerivative(B(j), j)\n", "def test_partial_derivative_eval():\n    x, y = symbols('x y')\n    f = A(x) * B(x, y)\n    pd = PartialDerivative(f, x)\n    expected = A(x).diff(x) * B(x, y) + A(x) * B(x, y).diff(x)\n    assert pd._perform_derivative() == expected\n", "def test_partial_derivative_evaluation():\n    expr = A(i) * B(j)\n    derivative = PartialDerivative(expr, A(k))\n    result = derivative._perform_derivative()\n    assert result == B(j) * A(i).diff(A(k))\n", "def test_partial_derivative_evaluation():\n    expr = PartialDerivative(A(i), A(j))\n    result = expr._perform_derivative()\n    # Assert the expected result here, based on the known behavior of the function.\n    # For instance, if the derivative of A with respect to A is known, you can use:\n    # assert result == expected_result\n", "def test_partial_derivative_expansion():\n    expr = A(i)*B(i) + A(j)*C(j)\n    var = D(k)\n    pd = PartialDerivative(expr, var)\n    expanded = pd._expand_partial_derivative()\n    assert expanded == PartialDerivative(A(i)*B(i), var) + PartialDerivative(A(j)*C(j), var)\n"], "sample_863": ["def test_feature_union_sparse_output():\n    # Test FeatureUnion with sparse output\n    union = FeatureUnion([(\"tsvd\", TruncatedSVD(n_components=2)),\n                          (\"pca\", PCA(n_components=1))])\n    X = sparse.csr_matrix([[0., 1., 3], [2., 2., 5]])\n    X_transformed = union.fit_transform(X)\n    assert sparse.issparse(X_transformed)\n    assert X_transformed.shape == (2, 3)\n", "def test_pipeline_final_estimator_passthrough():\n    X = iris.data\n    y = iris.target\n\n    pipeline = make_pipeline(StandardScaler(), 'passthrough')\n    pipeline.fit(X, y)\n    assert_array_equal(pipeline.predict(X), X)\n    assert_array_equal(pipeline.transform(X), StandardScaler().fit_transform(X))\n    assert_array_equal(pipeline.inverse_transform(X), StandardScaler().fit(X).inverse_transform(X))\n", "def test_pipeline_memory_cachedir():\n    # Test the pipeline with memory as a cachedir\n    cachedir = mkdtemp()\n    try:\n        pipe = Pipeline([('transform', DummyTransf()), ('estimator', Mult())],\n                        memory=cachedir)\n        pipe.fit(iris.data, iris.target)\n        transform_timestamp = pipe.named_steps['transform'].timestamp_\n\n        pipe.fit(iris.data, iris.target)\n        assert pipe.named_steps['transform'].timestamp_ == transform_timestamp\n    finally:\n        shutil.rmtree(cachedir)\n", "def test_feature_union_transformer_weights():\n    union = FeatureUnion([(\"pca\", PCA(n_components=1)),\n                          (\"svd\", TruncatedSVD(n_components=2))],\n                         transformer_weights={\"pca\": 0.3, \"svd\": 0.7})\n    X = np.array([[0., 1., 3], [2., 2., 5]])\n    X_transformed = union.fit_transform(X)\n    assert X_transformed.shape == (2, 3)  # One from PCA and two from TruncatedSVD\n    assert_allclose(X_transformed[:, 0], 0.3 * union.transformer_list[0][1].components_.ravel())\n    assert_allclose(X_transformed[:, 1:], 0.7 * union.transformer_list[1][1].components_.T)\n", "def test_cached_pipeline(func):\n    # This test checks that the pipeline caches the fitted transformers\n    # and that the fitted transformers are cloned\n    cachedir = mkdtemp()\n    try:\n        X = np.random.rand(10, 10)\n        pipeline = make_pipeline(DummyTransf(), SVC(), memory=cachedir)\n        getattr(pipeline, func)(X)\n        assert hasattr(pipeline.named_steps['dummytransf'], 'means_')\n        assert hasattr(pipeline.named_steps['dummytransf'], 'timestamp_')\n        timestamp = pipeline.named_steps['dummytransf'].timestamp_\n        time.sleep(1)  # make sure the timestamp changes\n        getattr(pipeline, func)(X)\n        assert pipeline.named_steps['dummytransf'].timestamp_ == timestamp\n    finally:\n        shutil.rmtree(cachedir)\n"], "sample_206": ["def test_image_field_update_dimension_fields(self):\n    class ImageModel(models.Model):\n        image = models.ImageField(upload_to='test_images/', width_field='width', height_field='height')\n        width = models.PositiveIntegerField(blank=True, null=True)\n        height = models.PositiveIntegerField(blank=True, null=True)\n\n    # Create a temporary image file\n    with tempfile.NamedTemporaryFile(suffix='.jpg', delete=False) as temp_image:\n        temp_image.write(b'Fake image data')\n        temp_image_name = temp_image.name\n\n    # Create an instance of ImageModel with the temporary image file\n    with open(temp_image_name, 'rb') as image_file:\n        instance = ImageModel.objects.create(image=File(image_file))\n\n    # Check if the width and height fields are updated correctly\n    self.assertEqual(instance.width, 1)  # Replace with the actual width of the image\n    self.assertEqual(instance.height, 1)  # Replace with the actual height of the image\n\n    # Clean up the temporary image file\n    os.unlink(temp_image_name)\n", "    def test_file_save_method(self):\n        instance = Document.objects.create()\n        content = ContentFile(b'A test content')\n        instance.file.save('test_file.txt', content)\n\n        # Check if the file is saved correctly\n        self.assertTrue(instance.file.storage.exists(instance.file.name))\n        saved_file = instance.file.storage.open(instance.file.name)\n        self.assertEqual(saved_file.read(), b'A test content')\n", "def test_file_field_save(self):\n    document = Document()\n    content = ContentFile(b'new content')\n    document.file.save('test.txt', content)\n    document.save()\n\n    document.refresh_from_db()\n    self.assertEqual(document.file.read(), b'new content')\n    self.assertTrue(document.file.storage.exists(document.file.name))\n\n    document.file.delete()\n    self.assertFalse(document.file.storage.exists(document.file.name))\n", "    def test_image_field_update_dimension_fields(self):\n        # Create a temporary image file\n        with tempfile.NamedTemporaryFile(suffix=\".jpg\", delete=False) as temp_image:\n            temp_image.write(b\"fake image data\")\n            temp_image_name = temp_image.name\n\n        # Create a document instance with the temporary image file\n        document = Document.objects.create(image=temp_image_name)\n\n        # Check if the width and height fields are updated correctly\n        self.assertEqual(document.width_field, 0)  # Replace 0 with the actual width of the image\n        self.assertEqual(document.height_field, 0)  # Replace 0 with the actual height of the image\n\n        # Remove the temporary image file\n        os.remove(temp_image_name)\n", "    def test_file_descriptor(self):\n        instance = Document.objects.create(file='test.txt')\n        file_descriptor = instance._meta.get_field('file')\n        file_object = file_descriptor.__get__(instance)\n        self.assertIsInstance(file_object, FieldFile)\n\n        new_file = ContentFile('new content')\n        file_descriptor.__set__(instance, new_file)\n        self.assertEqual(instance.file.read(), b'new content')\n"], "sample_532": ["def test_contour_with_custom_levels():\n    x = np.linspace(-5, 5, 100)\n    y = np.linspace(-5, 5, 100)\n    X, Y = np.meshgrid(x, y)\n    Z = np.sin(np.sqrt(X**2 + Y**2))\n\n    levels = np.linspace(-1, 1, 21)\n    CS = plt.contour(X, Y, Z, levels=levels)\n\n    assert len(CS.levels) == len(levels)\n    assert_array_almost_equal(CS.levels, levels)\n", "def test_quadcontourset_process_args():\n    fig, ax = plt.subplots()\n    x = np.arange(0, 5, 1)\n    y = np.arange(0, 5, 1)\n    z = np.array([[1, 2, 3, 4, 5],\n                  [6, 7, 8, 9, 10],\n                  [11, 12, 13, 14, 15],\n                  [16, 17, 18, 19, 20],\n                  [21, 22, 23, 24, 25]])\n    qcs = QuadContourSet(ax, x, y, z, levels=[10, 15, 20], corner_mask=True, algorithm='mpl2005')\n    assert qcs._corner_mask == True\n    assert qcs._algorithm == 'mpl2005'\n    assert isinstance(qcs._contour_generator, contourpy.contour_generator)\n    assert np.array_equal(qcs._mins, [0, 0])\n    assert np.array_equal(qcs._maxs, [4, 4])\n", "def test_contour_with_filled_and_line_contours():\n    # Create a test data\n    x = np.linspace(-3., 3., 256)\n    y = np.linspace(-3., 3., 256)\n    X, Y = np.meshgrid(x, y)\n    Z1 = np.exp(-X**2 - Y**2)\n    Z2 = np.exp(-(X - 1)**2 - (Y - 1)**2)\n    Z = (Z1 - Z2) * 2\n\n    # Create filled contours\n    contour_filled = plt.contourf(X, Y, Z, 8, alpha=.75, cmap='viridis')\n    plt.clabel(contour_filled, inline=True)\n\n    # Create line contours\n    contour_lines = plt.contour(X, Y, Z, 8, colors='black')\n    plt.clabel(contour_lines, inline=1, fontsize=10)\n\n    plt.close()\n", "def test_contourf_log_negative():\n    # Test contourf with log scale and negative values\n    x = np.linspace(1, 5, 50)\n    y = np.linspace(1, 5, 50)\n    X, Y = np.meshgrid(x, y)\n    Z = X * Y\n    Z[25:, 25:] = -Z[25:, 25:]  # Add negative values\n    fig, ax = plt.subplots()\n    cs = ax.contourf(X, Y, Z, levels=np.logspace(0, 2, 7),\n                     norm=LogNorm(), cmap='viridis')\n    fig.colorbar(cs)\n", "def test_contour_log():\n    # Test contour with log scale\n    np.random.seed(123456)\n    x = np.random.randn(200)\n    y = np.random.randn(200)\n    z = np.exp(x**2 + y**2)\n\n    fig, ax = plt.subplots()\n    ax.contour(x, y, z, levels=np.logspace(-1, 2, 12), norm=LogNorm())\n"], "sample_566": ["def test_figure_subfigures():\n    fig = plt.figure()\n    sfigs = fig.subfigures(1, 2)\n    axsL = sfigs[0].subplots(1, 2)\n    axsR = sfigs[1].subplots(2, 1)\n    plt.close(fig)\n", "def test_figure_align_labels():\n    fig, axs = plt.subplots(1, 2)\n    for tick in axs[0].get_xticklabels():\n        tick.set_rotation(55)\n    axs[0].set_xlabel('XLabel 0')\n    axs[1].set_xlabel('XLabel 1')\n    fig.align_xlabels()\n\n    fig, axs = plt.subplots(2, 1)\n    axs[0].plot(np.arange(0, 1000, 50))\n    axs[0].set_ylabel('YLabel 0')\n    axs[1].set_ylabel('YLabel 1')\n    fig.align_ylabels()\n\n    fig, axs = plt.subplots(2, 2)\n    axs[0, 0].set_xlabel('XLabel 0')\n    axs[0, 1].set_xlabel('XLabel 1')\n    axs[1, 0].set_ylabel('YLabel 0')\n    axs[1, 1].set_ylabel('YLabel 1')\n    fig.align_labels()\n", "def test_figure_align_labels_with_date_axis():\n    fig, ax = plt.subplots()\n    dates = mdates.drange(datetime(2022, 1, 1), datetime(2022, 1, 10), delta=datetime.timedelta(days=1))\n    y = np.random.rand(len(dates))\n    ax.plot(dates, y)\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Value')\n    fig.align_labels()\n", "def test_subplots_adjust(self):\n    fig, axs = plt.subplots(2, 2, sharex=True, sharey=True)\n    fig.subplots_adjust(hspace=0.1, wspace=0.1)\n", "def test_figimage():\n    fig = Figure()\n    data = np.random.rand(10, 10)\n    img = fig.figimage(data, xo=10, yo=10)\n    assert img.get_array() is data\n    assert img.get_offset() == (10, 10)\n    assert img.get_origin() == 'lower'\n\n    fig.clf()\n    img = fig.figimage(data, xo=10, yo=10, alpha=0.5, cmap='viridis')\n    assert img.get_alpha() == 0.5\n    assert img.get_cmap().name == 'viridis'\n"], "sample_990": ["def test_acsch_rewrite_as_log():\n    x = symbols('x')\n    assert acsch(x)._eval_rewrite_as_log(x) == log(1/x + sqrt(1/x**2 + 1))\n", "def test_atanh_taylor_series():\n    x = symbols('x')\n    assert atanh(x).series(x, 0, 6) == x + x**3/3 + x**5/5 + O(x**6)\n", "def test_asech_expansion_term():\n    x = symbols('x')\n    assert asech.expansion_term(0, x) == log(2 / x)\n    assert asech.expansion_term(2, x) == -x**2 / 4\n    assert asech.expansion_term(4, x) == x**4 / 32\n    assert asech.expansion_term(6, x) == -5*x**6 / 192\n", "def test_acsch_special_cases():\n    assert acsch(0) == zoo\n    assert acsch(1/sqrt(2)) == log((1+sqrt(2)))\n    assert acsch(I) == -I*pi/2\n    assert acsch(-2*I) == I*pi/6\n    assert acsch(I*(sqrt(6) - sqrt(2))) == -5*I*pi/12\n", "def test_atanh_rewrite_as_log():\n    x = symbols('x', real=True)\n    assert atanh(x).rewrite(log) == (log(1 + x) - log(1 - x)) / 2\n"], "sample_831": ["def test_plot_tree():\n    clf = DecisionTreeClassifier(random_state=0)\n    clf = clf.fit(X, y)\n    annotations = plot_tree(clf)\n    assert_equal(len(annotations), 3)\n", "def test_export_graphviz_not_fitted():\n    dt = DecisionTreeClassifier()\n    assert_raises(NotFittedError, export_graphviz, dt)\n", "def test_export_graphviz_invalid_feature_names_length():\n    clf = DecisionTreeClassifier(random_state=42)\n    clf.fit(X, y)\n    invalid_feature_names = [\"feature_1\"]\n    with assert_raises_regex(ValueError, \"Length of feature_names\"):\n        export_graphviz(clf, feature_names=invalid_feature_names)\n", "def test_export_text_max_depth():\n    # Test export_text function with max_depth parameter\n    decision_tree = DecisionTreeClassifier(random_state=0, max_depth=2)\n    decision_tree = decision_tree.fit(X, y)\n    report = export_text(decision_tree, feature_names=['feature_1', 'feature_2'], max_depth=1)\n    assert \"feature_1\" in report\n    assert \"truncated branch\" in report\n\n    # Test export_text function with max_depth=-1, which should raise ValueError\n    with assert_raises(ValueError):\n        report = export_text(decision_tree, max_depth=-1)\n", "def test_export_text_show_weights():\n    clf = DecisionTreeClassifier(random_state=0)\n    clf = clf.fit(X, y)\n    r = export_text(clf, feature_names=[\"feature1\", \"feature2\"], show_weights=True)\n    assert \"weights: [3.0, 3.0]\" in r, \"Classification weights not exported as expected\"\n"], "sample_8": ["def test_shape_property(self):\n    ma = Masked(self.a, mask=self.mask_a)\n    assert ma.shape == self.a.shape\n", "def test_masked_array_initialization_from_quantities(self):\n    ma = Masked(self.a)\n    assert isinstance(ma, MaskedNDArray)\n    assert ma.dtype == u.m\n", "    def test_masked_array_from_unmasked(self):\n        # Test initializing a Masked array from unmasked data\n        masked_a = Masked(self.a, mask=self.mask_a)\n        assert_masked_equal(masked_a, Masked(self.a, mask=self.mask_a))\n", "    def test_initialization_with_quantity(self):\n        masked_a = Masked(self.a, mask=self.mask_a)\n        assert_array_equal(masked_a.unmasked, self.a)\n        assert_array_equal(masked_a.mask, self.mask_a)\n        assert isinstance(masked_a, Quantity)\n", "    def test_initialization_from_nested_list_with_mask(self):\n        nested_list = [[1, 2], [3, np.ma.masked], [5, 6]]\n        masked_array = Masked(nested_list)\n        expected_unmasked = np.array([[1, 2], [3, 0], [5, 6]])\n        expected_mask = np.array([[False, False], [False, True], [False, False]])\n        assert_array_equal(masked_array.unmasked, expected_unmasked)\n        assert_array_equal(masked_array.mask, expected_mask)\n"], "sample_914": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_161": ["def test_save_form_data(self, mock_set):\n    class ParentModel(models.Model):\n        pass\n\n    class ChildModel(models.Model):\n        parents = models.ManyToManyField(ParentModel)\n\n    instance = ChildModel()\n    data = [ParentModel(id=1), ParentModel(id=2)]\n    instance.parents.save_form_data(instance, data)\n    mock_set.assert_called_once_with(data)\n", "    def test_m2m_field_unique(self):\n        class MyModel(models.Model):\n            m2m_field = models.ManyToManyField('self', unique=True)\n\n        errors = MyModel._meta.get_field('m2m_field').check()\n        self.assertEqual(len(errors), 1)\n        self.assertIsInstance(errors[0], Error)\n        self.assertEqual(errors[0].id, 'fields.E330')\n        self.assertEqual(errors[0].msg, 'ManyToManyFields cannot be unique.')\n", "def test_check_table_uniqueness_warning(self, m2m_db_table_mock):\n    m2m_db_table_mock.return_value = 'existing_table'\n\n    class ExistingModel(models.Model):\n        class Meta:\n            db_table = 'existing_table'\n\n    class NewModel(models.Model):\n        m2m_field = models.ManyToManyField(ExistingModel)\n\n    new_model_field = NewModel._meta.get_field('m2m_field')\n    errors = new_model_field._check_table_uniqueness()\n\n    self.assertEqual(len(errors), 1)\n    self.assertIsInstance(errors[0], DjangoWarning)\n    self.assertEqual(errors[0].id, 'fields.W344')\n", "def test_m2m_through_fields_validation(self, mock_apps):\n    class ChildModel(models.Model):\n        pass\n\n    class ParentModel(models.Model):\n        m2m = models.ManyToManyField(ChildModel, through='invalid_models_tests.ThroughModel')\n\n    class ThroughModel(models.Model):\n        child = models.ForeignKey(ChildModel, on_delete=models.CASCADE)\n\n    mock_apps.get_models.return_value = [ChildModel, ParentModel, ThroughModel]\n    field = ParentModel._meta.get_field('m2m')\n    field.remote_field.through = ThroughModel\n    field.remote_field.through_fields = ('invalid_field', 'another_invalid_field')\n\n    errors = field.check()\n    self.assertEqual(len(errors), 2)\n    self.assertIsInstance(errors[0], Error)\n    self.assertIsInstance(errors[1], Error)\n    self.assertEqual(errors[0].id, 'fields.E338')\n    self.assertEqual(errors[1].id, 'fields.E338')\n", "    def test_check_on_delete_with_set_null_and_null(self):\n        class Child(models.Model):\n            parent = models.ForeignKey('Parent', on_delete=models.SET_NULL, null=True)\n\n        with connection.schema_editor() as schema_editor:\n            schema_editor.create_model(Child)\n        errors = Child._meta.check()\n        self.assertEqual(errors, [])\n"], "sample_504": ["def test_colorbar_extension_length():\n    fig_uniform = _colorbar_extension_length('uniform')\n    fig_proportional = _colorbar_extension_length('proportional')\n    fig_uniform.savefig('colorbar_extensions_length_uniform.png')\n    fig_proportional.savefig('colorbar_extensions_length_proportional.png')\n    plt.close('all')\n", "def test_colorbar_extension_length():\n    uniform_fig = _colorbar_extension_length('uniform')\n    proportional_fig = _colorbar_extension_length('proportional')\n    return uniform_fig, proportional_fig\n", "def test_colorbar_extension_length():\n    # Test uniform spacing.\n    fig_uniform = _colorbar_extension_length(spacing='uniform')\n    # Test proportional spacing.\n    fig_proportional = _colorbar_extension_length(spacing='proportional')\n    # Return the figures to the caller.\n    return fig_uniform, fig_proportional\n", "def test_colorbar_extension_length():\n    fig_uniform = _colorbar_extension_length('uniform')\n    fig_proportional = _colorbar_extension_length('proportional')\n    check_figures_equal(fig_uniform, fig_proportional,\n                        err_msg=\"Uniform and proportional colorbars \"\n                                \"with variable extension lengths \"\n                                \"should be the same\")\n", "def test_colorbar_extension_orientation():\n    \"\"\"\n    Produce 2 colorbars with vertical and horizontal orientation.\n    \"\"\"\n    # Get a colormap and an appropriate norm.\n    cmap, norms = _get_cmap_norms()\n    # Create a figure and adjust whitespace for subplots.\n    fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(6, 12))\n    for ax, orientation in zip(axs, ('vertical', 'horizontal')):\n        # Generate the colorbar.\n        Colorbar(ax, cmap=cmap, norm=norms['both'],\n                 extend='both', extendrect=True,\n                 orientation=orientation)\n        # Turn off text and ticks.\n        ax.tick_params(left=False, labelleft=False,\n                        bottom=False, labelbottom=False)\n    # Return the figure to the caller.\n    return fig\n"], "sample_1171": ["def test_complex_region_from_real():\n    unit = Interval(0, 1)\n    complex_region = ComplexRegion.from_real(unit)\n    assert complex_region == CartesianComplexRegion(ProductSet(unit, FiniteSet(0)))\n", "def test_complex_region_intersection():\n    region1 = ComplexRegion(Interval(1, 3) * Interval(4, 6))\n    region2 = ComplexRegion(Interval(2, 4) * Interval(5, 7))\n    intersection = region1.intersect(region2)\n    assert intersection == ComplexRegion(Interval(2, 3) * Interval(5, 6))\n", "def test_complex_region_containment():\n    region = ComplexRegion(Interval(1, 3) * Interval(4, 6))\n    assert 2 + 5*I in region\n    assert 5*I not in region\n", "def test_ComplexRegion_contains():\n    a = Interval(2, 3)\n    b = Interval(4, 6)\n    c = Interval(1, 8)\n    c1 = ComplexRegion(a*b)\n    assert (2.5 + 4.5*I) in c1\n    assert (2.5 + 6.5*I) not in c1\n\n    c2 = ComplexRegion(Union(a*b, b*c))\n    assert (2.5 + 4.5*I) in c2\n    assert (5.5 + 6.5*I) in c2\n    assert (1.5 + 2.5*I) not in c2\n", "def test_complex_region_intersect():\n    a = Interval(1, 3)\n    b = Interval(2, 4)\n    c = Interval(3, 5)\n    d = Interval(4, 6)\n\n    region1 = ComplexRegion(a*b)\n    region2 = ComplexRegion(c*d)\n\n    intersection = region1.intersect(region2)\n    assert intersection == ComplexRegion(c*Intersection(b, d))\n"], "sample_472": ["def test_get_page_with_invalid_page(self):\n    \"\"\"\n    Test the get_page method with an invalid page number.\n    It should return the first page in this case.\n    \"\"\"\n    paginator = Paginator(range(100), 10)\n    page = paginator.get_page(\"invalid\")\n    self.assertEqual(page.number, 1)\n", "def test_paginator_iter(self):\n    \"\"\"\n    Test that paginator __iter__ method returns a page object for each page number.\n    \"\"\"\n    object_list = list(range(100))\n    paginator = Paginator(object_list, 10)\n    for i, page in enumerate(paginator, 1):\n        self.assertIsInstance(page, Page)\n        self.assertEqual(page.number, i)\n", "def test_elided_page_range(self):\n    \"\"\"\n    Test elided page range for a paginator with more than 10 pages.\n    \"\"\"\n    paginator = Paginator(range(1, 51), 10)\n    # Test elided page range for page 1\n    self.assertEqual(\n        list(paginator.get_elided_page_range(1)),\n        [1, 2, 3, 4, 5, Paginator.ELLIPSIS, 50]\n    )\n    # Test elided page range for page 43\n    self.assertEqual(\n        list(paginator.get_elided_page_range(43)),\n        [1, 2, Paginator.ELLIPSIS, 40, 41, 42, 43, 44, 45, Paginator.ELLIPSIS, 50]\n    )\n    # Test elided page range for last page\n    self.assertEqual(\n        list(paginator.get_elided_page_range(5)),\n        [1, 2, 3, 4, 5]\n    )\n", "    def test_page_attributes(self):\n        object_list = list(range(1, 11))\n        paginator = Paginator(object_list, 3)\n        page = paginator.page(2)\n\n        self.assertEqual(page.number, 2)\n        self.assertEqual(list(page), object_list[3:6])\n        self.assertEqual(page.has_next(), True)\n        self.assertEqual(page.has_previous(), True)\n        self.assertEqual(page.has_other_pages(), True)\n        self.assertEqual(page.next_page_number(), 3)\n        self.assertEqual(page.previous_page_number(), 1)\n        self.assertEqual(page.start_index(), 4)\n        self.assertEqual(page.end_index(), 6)\n\n        # Test last page\n        page = paginator.page(4)\n        self.assertEqual(page.has_next(), False)\n        self.assertEqual(page.has_previous(), True)\n        self.assertEqual(page.has_other_pages(), True)\n        self.assertEqual(page.end_index(), 10)\n\n        # Test first page\n        page = paginator.page(1)\n        self.assertEqual(page.has_next(), True)\n        self.assertEqual(page.has_previous(), False)\n        self.assertEqual(page.has_other_pages(), True)\n        self.assertEqual(page.start_index(), 1)\n\n        # Test single page\n        paginator = Paginator(object_list, 10)\n        page = paginator.page(1)\n        self.assertEqual(page.has_next(), False)\n        self.assertEqual(page.has_previous(), False)\n        self.assertEqual(page.has_other_pages(), False)\n        self.assertEqual(page.start_index(), 1)\n        self.assertEqual(page.end_index(), 10)\n", "    def setUp(self):\n        self.objects = list(range(1, 51))\n        self.paginator = Paginator(self.objects, 10)\n"], "sample_898": ["def test_roc_auc_score_binary():\n    # Test roc_auc_score with binary labels\n    y_true = np.array([0, 0, 1, 1])\n    y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n    expected_score = 0.75\n    assert_almost_equal(roc_auc_score(y_true, y_scores), expected_score)\n", "def test_roc_auc_score_single_class():\n    # Test case with only one class present in y_true\n    y_true = np.array([0, 0, 0])\n    y_score = np.array([0.1, 0.4, 0.35])\n    with assert_raises(ValueError):\n        roc_auc_score(y_true, y_score)\n", "def test_roc_auc_score_invalid_max_fpr():\n    y_true = np.array([0, 0, 1, 1])\n    y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n    with assert_raises(ValueError):\n        roc_auc_score(y_true, y_scores, max_fpr=0)\n    with assert_raises(ValueError):\n        roc_auc_score(y_true, y_scores, max_fpr=1.1)\n    with assert_raises(ValueError):\n        roc_auc_score(y_true, y_scores, max_fpr=-0.1)\n", "def test_roc_auc_score_invalid_average():\n    y_true = np.array([0, 0, 1, 1])\n    y_score = np.array([0.1, 0.4, 0.35, 0.8])\n    invalid_average = \"invalid\"\n    assert_raise_message(ValueError, \"Average parameter must be one of:\", roc_auc_score, y_true, y_score, invalid_average)\n", "def test_roc_auc_score_max_fpr():\n    y_true = np.array([0, 0, 1, 1])\n    y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n    max_fpr = 0.5\n    expected_auc = 0.75\n    expected_partial_auc = 0.72\n\n    auc = roc_auc_score(y_true, y_scores)\n    partial_auc = roc_auc_score(y_true, y_scores, max_fpr=max_fpr)\n\n    assert_almost_equal(auc, expected_auc)\n    assert_almost_equal(partial_auc, expected_partial_auc)\n"], "sample_985": ["def test_max_min_rewrite():\n    x, y = symbols('x y')\n    assert Max(x, y).rewrite(Piecewise) == Piecewise((x, x >= y), (y, True))\n    assert Min(x, y).rewrite(Piecewise) == Piecewise((x, x <= y), (y, True))\n", "def test_min_max_derivative():\n    x, y, z = symbols('x y z')\n    f = Max(x, y, z)\n    df_dx = f.diff(x)\n    assert df_dx == Piecewise((1, (x >= y) & (x >= z)), (0, True))\n", "def test_min_max_as_piecewise():\n    x, y = symbols('x y')\n\n    # Test Max as Piecewise\n    assert Max(x, y).rewrite(Piecewise) == Piecewise((x, x >= y), (y, True))\n\n    # Test Min as Piecewise\n    assert Min(x, y).rewrite(Piecewise) == Piecewise((y, x <= y), (x, True))\n", "def test_min_max_with_assumptions():\n    x, y = symbols('x y', positive=True)\n    z = symbols('z', negative=True)\n\n    assert Min(x, 2) == 2\n    assert Min(x, z) == z\n    assert Max(x, 2) == x\n    assert Max(x, z) == x\n\n    assert Min(x, -2) == -2\n    assert Min(z, 2) == z\n    assert Max(x, -2) == x\n    assert Max(z, 2) == 2\n\n    assert Min(x, -2, z) == -2\n    assert Max(x, -2, z) == x\n", "def test_min_function():\n    x, y = symbols('x y')\n\n    # Test with numerical inputs\n    assert Min(3, 5) == 3\n    assert Min(-3, 5) == -3\n    assert Min(3, -5) == -5\n    assert Min(-3, -5) == -5\n\n    # Test with symbolic inputs\n    assert Min(x, y) == Min(y, x)\n    assert Min(x, -2) == Min(-2, x)\n    assert Min(x, Max(y, 2)) == Min(Min(x, y), 2)\n\n    # Test with assumptions\n    p = symbols('p', positive=True)\n    n = symbols('n', negative=True)\n    assert Min(p, -3) == -3\n    assert Min(n, 8, p, -7, p, S.Infinity) == Min(n, -7)\n\n    # Test with Piecewise function\n    assert Min(Piecewise((x, x > 0), (0, True)), 1) == Piecewise((0, x > 1), (x, True))\n"], "sample_942": ["def test_parse_annotation(app):\n    env = app.builder.env\n    node_list = _parse_annotation(\"List[str]\", env)\n    assert len(node_list) == 3\n    assert isinstance(node_list[0], pending_xref)\n    assert node_list[0].get(\"reftarget\") == \"List\"\n    assert isinstance(node_list[1], addnodes.desc_sig_punctuation)\n    assert node_list[1].astext() == \"[\"\n    assert isinstance(node_list[2], pending_xref)\n    assert node_list[2].get(\"reftarget\") == \"str\"\n", "def test_parse_annotation(app, env):\n    annotations = [\n        ('List[int]', ['List', '[', 'int', ']']),\n        ('Tuple[str, int]', ['Tuple', '[', 'str', ', ', 'int', ']']),\n        ('Optional[List[str]]', ['Optional', '[', 'List', '[', 'str', ']', ']']),\n    ]\n    for annotation, expected in annotations:\n        nodes = _parse_annotation(annotation, env)\n        assert [str(node) for node in nodes] == expected\n", "def test_parse_annotation(app):\n    env = Mock()\n    env.config = app.config\n    anno = _parse_annotation(\"List[int]\", env)\n    assert len(anno) == 5\n    assert_node(anno[0], [pending_xref, nodes.Text], \"List\")\n    assert_node(anno[1], [desc_sig_punctuation], \"[\")\n    assert_node(anno[2], [nodes.Text], \"int\")\n    assert_node(anno[3], [desc_sig_punctuation], \"]\")\n    assert_node(anno[4], [nodes.Text], \"\")\n", "def test_parse_annotation(app, doctree):\n    env = app.builder.env\n    nodes = _parse_annotation(\"List[int]\", env)\n    assert len(nodes) == 3\n    assert isinstance(nodes[0], pending_xref)\n    assert nodes[0].astext() == \"List\"\n    assert nodes[1].astext() == \"[\"\n    assert isinstance(nodes[2], pending_xref)\n    assert nodes[2].astext() == \"int\"\n", "def test_parse_annotation():\n    env = Mock()\n    result = _parse_annotation(\"List[int]\", env)\n    expected_result = [\n        pending_xref('', nodes.Text('List'), refdomain='py', reftype='class', reftarget='List'),\n        addnodes.desc_sig_punctuation('', '['),\n        nodes.Text('int'),\n        addnodes.desc_sig_punctuation('', ']')\n    ]\n    assert len(result) == len(expected_result)\n    for r, e in zip(result, expected_result):\n        assert_node(r, e)\n"], "sample_818": ["def test_spectral_clustering_precomputed_affinity(eigen_solver, assign_labels):\n    X = np.array([[1, 1], [2, 1], [1, 0], [4, 7], [3, 5], [3, 6]])\n    affinity = pairwise_distances(X)\n    affinity = np.exp(-affinity ** 2 / (2. * 1. ** 2))  # RBF kernel\n\n    clustering = SpectralClustering(n_clusters=2, assign_labels=assign_labels,\n                                    affinity='precomputed', eigen_solver=eigen_solver,\n                                    random_state=0)\n    labels = clustering.fit_predict(affinity)\n\n    assert_array_equal(labels, np.array([1, 1, 1, 0, 0, 0]))\n", "def test_spectral_clustering_random_state(eigen_solver, assign_labels):\n    random_state = check_random_state(0)\n    X, y = make_blobs(n_samples=100, centers=2, random_state=random_state)\n    labels1 = spectral_clustering(X, n_clusters=2, random_state=random_state,\n                                  eigen_solver=eigen_solver,\n                                  assign_labels=assign_labels)\n    labels2 = spectral_clustering(X, n_clusters=2, random_state=random_state,\n                                  eigen_solver=eigen_solver,\n                                  assign_labels=assign_labels)\n    assert_array_equal(labels1, labels2)\n", "def test_spectral_clustering_warns_on_callable_kernel(eigen_solver, assign_labels):\n    \"\"\"Test if a warning is raised when a callable kernel is used with an invalid parameter.\"\"\"\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    with pytest.warns(UserWarning, match=\"The gamma parameter is ignored when using a callable.\"):\n        spectral_clustering(X, n_clusters=2, eigen_solver=eigen_solver, assign_labels=assign_labels, affinity=rbf_kernel, gamma=0.5)\n", "def test_spectral_clustering_with_precomputed_affinity(eigen_solver, assign_labels):\n    \"\"\"Test spectral clustering with precomputed affinity matrix\"\"\"\n    rng = check_random_state(42)\n    X = rng.random_sample(size=(100, 2))\n    affinity = rbf_kernel(X)\n\n    labels = spectral_clustering(affinity, n_clusters=2, eigen_solver=eigen_solver,\n                                 assign_labels=assign_labels, random_state=rng)\n\n    assert_equal(len(np.unique(labels)), 2)\n", "def test_spectral_clustering_prediction(eigen_solver, assign_labels, X, y):\n    # Test that spectral clustering returns correct number of clusters\n    sc = SpectralClustering(n_clusters=3, eigen_solver=eigen_solver,\n                            assign_labels=assign_labels, random_state=0)\n    labels = sc.fit_predict(X)\n    assert_equal(len(np.unique(labels)), 3)\n\n    # Test that the adjusted rand score is greater than 0.5\n    ari = adjusted_rand_score(y, labels)\n    assert ari > 0.5\n"], "sample_435": ["    def test_successful_authentication(self):\n        form = AuthenticationForm(data={\"username\": \"testclient\", \"password\": \"password\"})\n        self.assertTrue(form.is_valid())\n        self.assertEqual(form.get_user(), self.u1)\n", "    def test_password_help_text(self):\n        form = UserChangeForm(instance=self.u1)\n        self.assertIn(f\"../../{self.u1.pk}/password/\", form.fields[\"password\"].help_text)\n", "    def test_init(self):\n        form = UserChangeForm(instance=self.u1)\n        password_help_text = form.fields['password'].help_text.format(self.u1.pk)\n        self.assertIn(f'../../{self.u1.pk}/password/', password_help_text)\n", "    def test_password_help_text(self):\n        form = UserChangeForm(instance=self.u1)\n        self.assertIn(str(self.u1.pk), form.fields['password'].help_text)\n", "    def test_form_fields(self):\n        form = UserChangeForm(instance=self.u1)\n        self.assertIsInstance(form.fields[\"username\"], CharField)\n        self.assertIsInstance(form.fields[\"password\"], ReadOnlyPasswordHashField)\n        self.assertTrue(form.fields[\"password\"].disabled)\n"], "sample_1136": ["def test_expression_domain_from_sympy():\n    ED = EX()\n    sympy_expr = x + 2*y - 3*z\n    expr = ED.from_sympy(sympy_expr)\n    assert expr.ex == sympy_expr\n", "def test_expression_domain():\n    K = EX\n\n    a = K(x + y)\n    b = K(x - y)\n\n    assert a + b == K(2*x)\n    assert a - b == K(2*y)\n    assert a*b == K(x**2 - y**2)\n    assert a/b == K((x + y)/(x - y))\n    assert a**2 == K((x + y)**2)\n    assert K.is_positive(K(x)) is None\n    assert K.is_negative(K(-x))\n    assert K.is_nonpositive(K(0))\n    assert K.is_nonnegative(K(x**2))\n    assert K.numer(K((x + y)/(x - y))) == K(x + y)\n    assert K.denom(K((x + y)/(x - y))) == K(x - y)\n    assert K.gcd(K(x**2 - y**2), K(x**2 + 2*x*y + y**2)) == K(1)\n    assert K.lcm(K(x**2 - y**2), K(x**2 + 2*x*y + y**2)) == K((x + y)**2*(x - y)**2)\n", "def test_ExpressionDomain_addition():\n    K = EX\n    a = K(x + 2*y)\n    b = K(3*x - y)\n    c = a + b\n    assert c == K(4*x + y)\n", "def test_expression_domain():\n    K = EX\n\n    a = K('x + 2')\n    b = K('3*x - 1')\n\n    assert str(a + b) == 'EX(3*x + 1)'\n    assert str(a - b) == 'EX(-2*x - 3)'\n    assert str(a*b) == 'EX(3*x**2 + 5*x - 2)'\n    assert str(a/b) == 'EX((x + 2)/(3*x - 1))'\n    assert str(a**b) == 'EX((x + 2)**(3*x - 1))'\n\n    assert a.numer() == K('x + 2')\n    assert a.denom() == K('1')\n    assert a.simplify(a.ex) == a\n\n    assert K.is_positive(K('x')) is None\n    assert K.is_negative(K('-x')) is True\n    assert K.is_nonpositive(K('-x')) is True\n    assert K.is_nonnegative(K('x')) is True\n\n    assert str(K.numer(K('(x + 2)/(3*x - 1)'))) == 'EX(x + 2)'\n    assert str(K.denom(K('(x + 2)/(3*x - 1)'))) == 'EX(3*x - 1)'\n\n    assert str(K.gcd(a, b)) == 'EX(1)'\n    assert str(K.lcm(a, b)) == 'EX((x + 2)*(3*x - 1))'\n", "def test_expression_domain_sympy_conversion():\n    K = EX\n\n    # Test conversion from sympy object to EX\n    p = Poly(x**2 - 2, x)\n    ep = K.from_sympy(p)\n    assert K.to_sympy(ep) == p.as_expr()\n\n    # Test conversion from EX to sympy object\n    assert K.to_sympy(ep) == p.as_expr()\n\n    # Test conversion from sympy object to EX and back\n    p_new = K.to_sympy(K.from_sympy(p))\n    assert p_new == p.as_expr()\n\n    # Test conversion from EX to sympy object and back\n    ep_new = K.from_sympy(K.to_sympy(ep))\n    assert ep_new.ex == ep.ex\n"], "sample_705": ["def test_TimeoutExpired():\n    pytester = Pytester(None, None)\n    with pytest.raises(Pytester.TimeoutExpired):\n        pytester.run([\"sleep\", \"10\"], timeout=1)\n", "def test_sys_path_snapshot_restore(tmp_path):\n    sys.path.append(str(tmp_path))\n    snapshot = SysPathsSnapshot()\n    sys.path.append(\"new_path\")\n    assert \"new_path\" in sys.path\n    snapshot.restore()\n    assert \"new_path\" not in sys.path\n", "def test_SysPathsSnapshot_restore(monkeypatch):\n    # Modify sys.path and sys.meta_path\n    original_path = sys.path.copy()\n    original_meta_path = sys.meta_path.copy()\n    monkeypatch.syspath_prepend('/tmp/test')\n    monkeypatch.syspath_append('/tmp/test2')\n    sys.meta_path.append('fake_meta_path')\n\n    # Create a SysPathsSnapshot\n    snapshot = SysPathsSnapshot()\n\n    # Modify sys.path and sys.meta_path after creating the snapshot\n    monkeypatch.syspath_prepend('/tmp/test3')\n    sys.meta_path.append('another_fake_meta_path')\n\n    # Restore the original sys.path and sys.meta_path\n    snapshot.restore()\n\n    # Check if sys.path and sys.meta_path are restored to their original state\n    assert sys.path == original_path\n    assert sys.meta_path == original_meta_path\n", "def test_syspathinsert(pytester: Pytester):\n    initial_syspath = sys.path.copy()\n    pytester.syspathinsert()\n    assert sys.path[0] == str(pytester.path)\n    sys.path = initial_syspath\n", "def test_Pytester_syspathinsert_with_path(pytester: Pytester) -> None:\n    original_sys_path = sys.path.copy()\n    path_to_insert = Path(\"path/to/insert\")\n    pytester.syspathinsert(path_to_insert)\n    assert sys.path[0] == str(path_to_insert)\n    assert sys.path[1:] == original_sys_path\n"], "sample_1047": ["def test_assumptions_for_symbol():\n    x = Symbol('x', real=True)\n    assert x.is_real is True\n    assert x.is_complex is True\n    assert x.is_integer is None\n    assert x.is_odd is None\n    assert x.is_even is None\n    assert x.is_prime is None\n    assert x.is_composite is None\n    assert x.is_zero is None\n    assert x.is_nonzero is None\n    assert x.is_rational is None\n    assert x.is_algebraic is None\n    assert x.is_transcendental is None\n    assert x.is_irrational is None\n    assert x.is_finite is None\n    assert x.is_infinite is None\n    assert x.is_negative is None\n    assert x.is_nonnegative is None\n    assert x.is_positive is None\n    assert x.is_nonpositive is None\n    assert x.is_hermitian is True\n    assert x.is_antihermitian is None\n", "def test_transcendental_property():\n    x = Symbol('x')\n    assert exp(x).is_transcendental\n    assert sin(x).is_transcendental\n    assert log(x).is_transcendental\n    assert asin(x).is_transcendental\n    assert not sqrt(2).is_transcendental\n    assert not pi.is_transcendental\n", "def test_assumptions_complex():\n    x = Symbol('x')\n    assert x.is_complex == True\n    assert I.is_complex == True\n    assert sqrt(-1).is_complex == True\n    assert log(I).is_complex == True\n    assert exp(I).is_complex == True\n    assert sin(I).is_complex == True\n    assert asin(1).is_complex == False\n    assert factorial(5).is_complex == False\n    assert Mod(5, 2).is_complex == False\n    assert pi.is_complex == False\n", "def test_assumptions_infinite():\n    x = Symbol('x', real=True, positive=True)\n    assert x.is_finite is True\n    assert (1/x).is_infinite is True\n    assert exp(x).is_infinite is False\n    assert log(x).is_infinite is False\n    assert sin(x).is_infinite is False\n    assert asin(x).is_infinite is None\n    assert factorial(x).is_infinite is True\n    assert Mod(x, 2).is_infinite is False\n    assert (pi*x).is_infinite is False\n", "def test_odd_even_assumptions():\n    x = Symbol('x', integer=True)\n    assert x.is_integer == True\n    assert x.is_odd is None\n    assert x.is_even is None\n\n    y = Symbol('y', odd=True)\n    assert y.is_integer == True\n    assert y.is_odd == True\n    assert y.is_even == False\n\n    z = Symbol('z', even=True)\n    assert z.is_integer == True\n    assert z.is_odd == False\n    assert z.is_even == True\n"], "sample_1193": ["def test_are_coplanar():\n    a = Line3D(Point3D(5, 0, 0), Point3D(1, -1, 1))\n    b = Line3D(Point3D(0, -2, 0), Point3D(3, 1, 1))\n    c = Line3D(Point3D(0, -1, 0), Point3D(5, -1, 9))\n    assert not are_coplanar(a, b, c)\n\n    d = Line3D(Point3D(1, 2, 3), Point3D(4, 5, 6))\n    e = Line3D(Point3D(7, 8, 9), Point3D(10, 12, 15))\n    assert are_coplanar(d, e)\n", "def test_are_coplanar():\n    a = Line3D(Point3D(5, 0, 0), Point3D(1, -1, 1))\n    b = Line3D(Point3D(0, -2, 0), Point3D(3, 1, 1))\n    c = Line3D(Point3D(0, -1, 0), Point3D(5, -1, 9))\n    assert not are_coplanar(a, b, c)\n\n    d = Line3D(Point3D(1, 2, 3), Point3D(4, 5, 6))\n    e = Line3D(Point3D(2, 3, 4), Point3D(5, 7, 9))\n    assert are_coplanar(d, e)\n", "def test_are_coplanar():\n    a = Point3D(0, 0, 0)\n    b = Point3D(1, 0, 0)\n    c = Point3D(0, 1, 0)\n    d = Point3D(0, 0, 1)\n    e = Point3D(1, 1, 1)\n    assert are_coplanar(a, b, c, d)\n    assert not are_coplanar(a, b, c, e)\n", "def test_are_coplanar():\n    a = Line3D(Point3D(5, 0, 0), Point3D(1, -1, 1))\n    b = Line3D(Point3D(0, -2, 0), Point3D(3, 1, 1))\n    c = Line3D(Point3D(0, -1, 0), Point3D(5, -1, 9))\n    assert not are_coplanar(a, b, c)\n\n    p = Point3D(0, 0, 0)\n    q = Point3D(1, 1, 1)\n    r = Point3D(2, 2, 2)\n    assert are_coplanar(p, q, r)\n\n    s = Point3D(0, 1, 2)\n    assert not are_coplanar(p, q, r, s)\n", "def test_are_coplanar():\n    # Test with coplanar points\n    p1, p2, p3, p4 = Point3D(1, 2, 3), Point3D(4, 5, 6), Point3D(7, 8, 9), Point3D(10, 11, 12)\n    assert are_coplanar(p1, p2, p3, p4)\n\n    # Test with non-coplanar points\n    p5 = Point3D(1, 2, 4)\n    assert not are_coplanar(p1, p2, p3, p5)\n\n    # Test with a line and a point that are not coplanar\n    l1 = Line3D(p1, p2)\n    assert not are_coplanar(l1, p5)\n\n    # Test with coplanar lines\n    l2 = Line3D(p3, p4)\n    assert are_coplanar(l1, l2)\n\n    # Test with a line and a point that are coplanar\n    p6 = Point3D(1, 2, 3)\n    assert are_coplanar(l1, p6)\n"], "sample_666": ["def test_global_and_fixture_disabled(self, method):\n    cm = CaptureManager(StdCapture)\n    cm.method = method\n    with cm.global_and_fixture_disabled():\n        assert not cm.is_capturing()\n", "    def test_capture_exception_handling(self, method: str) -> None:\n        capture_manager = CaptureManager(self._create_capture_method(method))\n\n        with pytest.raises(RuntimeError):\n            with capture_manager.global_and_fixture_disabled():\n                raise RuntimeError(\"Test exception\")\n", "def test_pytest_runtestloop(self, method, monkeypatch):\n    # Arrange\n    class FakeSession:\n        class FakeConfig:\n            collectonly = False\n            option = type(\"\", (), {})()\n\n        config = FakeConfig()\n\n    class FakeTerminalReporter:\n            pass\n\n    session = FakeSession()\n    capture_manager = CaptureManager(method)\n    monkeypatch.setattr(capture_manager, \"_stream_captures\", StdCapture())\n\n    with contextlib.redirect_stdout(io.StringIO()):\n        # Act\n        with capture_manager.global_and_fixture_disabled():\n            # Assert\n            # You can add assertions here to verify the behavior of pytest_runtestloop\n            pass\n", "def test_disable_global_capture(capfd, caplog, method):\n    capmanager = CaptureManager(capfd)\n    capmanager.method = method\n\n    @contextlib.contextmanager\n        try:\n            yield\n        finally:\n            assert capfd.readouterr() == (\"\" if out else \"Captured\", \"\" if err else \"Captured\")\n            if in_:\n                with pytest.raises(UnsupportedOperation):\n                    capfd.readline()\n            else:\n                assert capfd.readline() == \"Captured\"\n\n    with assert_capture_state():\n        capmanager.global_capture_started = True\n\n    with capmanager.global_and_fixture_disabled():\n        with assert_capture_state(out=False, err=False, in_=False):\n            capmanager.global_capture_started = False\n", "    def test_capture_with_custom_streams(self, method):\n        \"\"\"Test capture with custom streams\"\"\"\n        class CustomStream(io.TextIOBase):\n                self.buffer = \"\"\n                self.buffer += s\n                return self.buffer\n\n        with CaptureManager(sys.stdin, sys.stdout, sys.stderr, method) as capman:\n            sys.stdout = CustomStream()\n            sys.stderr = CustomStream()\n            print(\"test output\")\n            sys.stderr.write(\"test error\\n\")\n\n        captured_out, captured_err = capman.readouterr()\n        assert \"test output\" in captured_out\n        assert \"test error\" in captured_err\n"], "sample_1115": ["def test_tensor_mul_and_contract_metric():\n    Lorentz = TensorIndexType('Lorentz', dummy_name='L')\n    m0, m1, m2 = tensor_indices('m0,m1,m2', Lorentz)\n    g = Lorentz.metric\n    p, q = tensor_heads('p,q', [Lorentz])\n    t = p(m0) * q(m1) * g(-m0, -m1)\n    assert _is_equal(t.canon_bp(), g(Lorentz(0), Lorentz(1)) * p(-Lorentz(0)) * q(-Lorentz(1)))\n    assert _is_equal(t.contract_metric(g).canon_bp(), p(Lorentz(0)) * q(-Lorentz(0)))\n", "def test_canon_bp_with_scalar():\n    Lorentz = TensorIndexType('Lorentz', dummy_name='L')\n    m0, m1 = tensor_indices('m0,m1', Lorentz)\n    A = TensorHead('A', [Lorentz]*2, TensorSymmetry.no_symmetry(2))\n    t = A(m0, m1) * 2\n    canon = t.canon_bp()\n    assert _is_equal(canon, A(m0, m1) * 2)\n", "def test_tensor_add_commutes_with_symbol():\n    Lorentz = TensorIndexType('Lorentz', dummy_name='L')\n    m0, m1, m2 = tensor_indices('m0,m1,m2', Lorentz)\n    g = Lorentz.metric\n    p, q = tensor_heads('p,q', [Lorentz])\n    a = Symbol('a')\n    t1 = p(m0) * g(m0, m1) + a * q(m1)\n    t2 = a * q(m1) + p(m0) * g(m0, m1)\n    assert _is_equal(t1, t2)\n", "def test_tensor_mul_with_tensadd():\n    Lorentz = TensorIndexType('Lorentz', dummy_name='L')\n    m0, m1 = tensor_indices('m0 m1', Lorentz)\n    p, q = tensor_heads('p,q', [Lorentz], [[2]])\n    t1 = p(m0) + p(m1)\n    t2 = q(-m0)\n    t3 = t1*t2\n    assert isinstance(t3, TensAdd)\n    assert len(t3.args) == 2\n    assert _is_equal(t3.args[0], p(m0)*q(-m0))\n    assert _is_equal(t3.args[1], p(m1)*q(-m0))\n", "def test_get_symmetric_group_sgs():\n    base, sgs = get_symmetric_group_sgs(2)\n    assert base == [(0, 1)]\n    assert sgs == [Permutation(0, 1), Permutation(1, 0)]\n\n    base, sgs = get_symmetric_group_sgs(2, True)\n    assert base == [(0, 1)]\n    assert sgs == [Permutation(0, 1)]\n\n    base, sgs = get_symmetric_group_sgs(4)\n    assert base == [(0, 1, 2, 3), (0, 2), (1, 3)]\n    assert sgs == [Permutation(0, 1), Permutation(2, 3), Permutation(0, 2), Permutation(1, 3), Permutation(0, 3), Permutation(1, 2)]\n"], "sample_466": ["    def test_custom_operations(self):\n        custom_operation = custom_migration_operations.operations.CustomOperation(\n            name='custom_operation'\n        )\n        operation_writer = OperationWriter(custom_operation)\n        operation_string, operation_imports = operation_writer.serialize()\n        self.assertIn(\"import custom_migration_operations.operations\", operation_imports)\n        self.assertIn(\"custom_migration_operations.operations.CustomOperation(name='custom_operation'),\", operation_string)\n", "    def test_migration_writer(self):\n        class TestMigration(migrations.Migration):\n            dependencies = [('test_app', '0001_initial')]\n            operations = [\n                migrations.CreateModel(\n                    name='TestModel',\n                    fields=[\n                        ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                        ('name', models.CharField(max_length=200)),\n                    ],\n                ),\n            ]\n\n        writer = MigrationWriter(TestMigration)\n        output = writer.as_string()\n\n        self.assertIn(\"from django.db import migrations, models\", output)\n        self.assertIn(\"class Migration(migrations.Migration):\", output)\n        self.assertIn(\"dependencies = [\", output)\n        self.assertIn(\"('test_app', '0001_initial'),\", output)\n        self.assertIn(\"operations = [\", output)\n        self.assertIn(\"migrations.CreateModel(\", output)\n        self.assertIn(\"name='TestModel',\", output)\n        self.assertIn(\"fields=[\", output)\n        self.assertIn(\"models.CharField(\", output)\n        self.assertIn(\"max_length=200,\", output)\n", "    def test_serialize_custom_types(self):\n        # Register custom serializers for various types\n        MigrationWriter.register_serializer(Money, BaseSerializer)\n        MigrationWriter.register_serializer(DeconstructibleInstances, BaseSerializer)\n        MigrationWriter.register_serializer(TextEnum, BaseSerializer)\n        MigrationWriter.register_serializer(TextTranslatedEnum, BaseSerializer)\n        MigrationWriter.register_serializer(BinaryEnum, BaseSerializer)\n        MigrationWriter.register_serializer(IntEnum, BaseSerializer)\n        MigrationWriter.register_serializer(IntFlagEnum, BaseSerializer)\n\n        # Create operations with custom types\n        operations = [\n            migrations.CreateModel(\n                name='CustomModel',\n                fields=[\n                    ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                    ('money_field', models.DecimalField(decimal_places=2, default=Money('10.00'), max_digits=5)),\n                    ('deconstructible_field', models.CharField(default=DeconstructibleInstances(), max_length=10)),\n                    ('text_enum_field', models.CharField(choices=[(tag.name, tag.value) for tag in TextEnum])),\n                    ('text_translated_enum_field', models.CharField(choices=[(tag.name, str(tag.value)) for tag in TextTranslatedEnum])),\n                    ('binary_enum_field', models.BinaryField(choices=[(tag.name, tag.value) for tag in BinaryEnum])),\n                    ('int_enum_field', models.IntegerField(choices=[(tag.name, tag.value) for tag in IntEnum])),\n                    ('int_flag_enum_field', models.IntegerField(choices=[(tag.name, tag.value) for tag in IntFlagEnum])),\n                ],\n            ),\n        ]\n\n        # Serialize the operations\n        serialized_operations = [OperationWriter(operation).serialize()[0] for operation in operations]", "    def test_operation_writer_with_custom_enums(self):\n        # Test Enum instances\n        operation = migrations.AddField(\n            model_name='testmodel',\n            name='text_enum',\n            field=models.CharField(\n                max_length=255,\n                choices=[(tag.value, tag.value) for tag in TextEnum],\n            ),\n        )\n        operation_string, operation_imports = OperationWriter(operation).serialize()\n        self.assertIn(\"migrations.AddField\", operation_string)\n        self.assertIn(\"TextEnum\", operation_string)\n\n        # Test translated Enum instances\n        operation = migrations.AlterField(\n            model_name='testmodel',\n            name='text_translated_enum',\n            field=models.CharField(\n                max_length=255,\n                choices=[(tag.value, tag.value) for tag in TextTranslatedEnum],\n            ),\n        )\n        operation_string, operation_imports = OperationWriter(operation).serialize()\n        self.assertIn(\"migrations.AlterField\", operation_string)\n        self.assertIn(\"TextTranslatedEnum\", operation_string)\n        self.assertIn(\"gettext_lazy\", operation_string)\n\n        # Test binary Enum instances\n        operation = migrations.RenameField(\n            model_name='testmodel',\n            old_name='binary_enum',\n            new_name='new_binary_enum',\n            field=models.BinaryField(\n                choices=[(tag.value, tag.value) for tag in BinaryEnum],\n            ),\n        )\n        operation_string, operation_imports = OperationWriter(operation).serialize()\n        self.assertIn(\"migrations.RenameField\", operation_string)\n        self.assertIn(\"BinaryEnum\", operation_string)\n\n        # Test IntEnum instances\n        operation = migrations.RemoveField(\n            model_name='testmodel',\n            name='int_enum',\n        )\n        operation.int_enum = IntEnum.A\n        operation_string, operation_imports = OperationWriter(operation).serialize()\n        self.assertIn", "    def test_serialize_enum(self):\n        # Test serialization of different types of Enum\n        self.assertEqual(\n            MigrationWriter.serialize(TextEnum.A)[0],\n            \"custom_migration_operations.more_operations.TextEnum.A\"\n        )\n        self.assertEqual(\n            MigrationWriter.serialize(TextTranslatedEnum.A)[0],\n            \"_('a-value')\"\n        )\n        self.assertEqual(\n            MigrationWriter.serialize(BinaryEnum.A)[0],\n            \"custom_migration_operations.more_operations.BinaryEnum.A\"\n        )\n        self.assertEqual(\n            MigrationWriter.serialize(IntEnum.A)[0],\n            \"custom_migration_operations.more_operations.IntEnum.A\"\n        )\n        self.assertEqual(\n            MigrationWriter.serialize(IntFlagEnum.A)[0],\n            \"custom_migration_operations.more_operations.IntFlagEnum.A\"\n        )\n"], "sample_486": ["def test_modelform_defines_fields(self):\n    class TestForm(forms.ModelForm):\n        class Meta:\n            model = UUIDPKParent\n            fields = ['field1', 'field2']\n\n    self.assertTrue(modelform_defines_fields(TestForm))\n\n    class TestForm2(forms.ModelForm):\n        class Meta:\n            model = UUIDPKParent\n\n    self.assertFalse(modelform_defines_fields(TestForm2))\n", "def test_modelform_defines_fields(self):\n    class CustomForm(ModelForm):\n        class Meta:\n            model = AutoPKParent\n            fields = [\"name\"]\n\n    self.assertTrue(modelform_defines_fields(CustomForm))\n", "    def test_inline_formset_with_to_field(self):\n        FormSet = inlineformset_factory(\n            ParentWithUUIDAlternateKey,\n            UUIDPKChildOfAutoPKParent,\n            fields=(\"id\", \"name\", \"parent\"),\n            fk_name=\"parent_alternate_key\",\n            can_delete=True,\n            extra=1,\n        )\n        parent = ParentWithUUIDAlternateKey.objects.create(alternate_key=\"key1\")\n        data = {\n            \"uuidpkchildofautopkparent_set-TOTAL_FORMS\": \"1\",\n            \"uuidpkchildofautopkparent_set-INITIAL_FORMS\": \"0\",\n            \"uuidpkchildofautopkparent_set-MAX_NUM_FORMS\": \"\",\n            \"uuidpkchildofautopkparent_set-0-id\": \"\",\n            \"uuidpkchildofautopkparent_set-0-name\": \"Child1\",\n            \"uuidpkchildofautopkparent_set-0-parent\": str(parent.alternate_key),\n        }\n        formset = FormSet(data, instance=parent)\n        self.assertTrue(formset.is_valid())\n        instances = formset.save()\n        self.assertEqual(len(instances), 1)\n        self.assertEqual(instances[0].name, \"Child1\")\n        self.assertEqual(instances[0].parent, parent)\n", "    def test_modelform_defines_fields(self):\n        FormSet = inlineformset_factory(\n            ParentWithUUIDAlternateKey,\n            UUIDPKChild,\n            fields=('child_key',),\n        )\n        self.assertTrue(modelform_defines_fields(FormSet.form))\n        self.assertEqual(FormSet.form._meta.fields, ('child_key',))\n", "def test_inlineformset_factory_with_fk_name(self):\n    \"\"\"\n    Test inlineformset_factory when the model has more than one ForeignKey to the parent model.\n    \"\"\"\n    ParentWithTwoFKs = ParentWithUUIDAlternateKey  # Using a model that has two ForeignKeys\n    ChildWithTwoFKs = AutoPKChildOfUUIDPKParent  # Using a child model that has two ForeignKeys to ParentWithTwoFKs\n\n    # Create a parent instance with two children\n    parent = ParentWithTwoFKs.objects.create()\n    child1 = ChildWithTwoFKs.objects.create(parent_fk1=parent)\n    child2 = ChildWithTwoFKs.objects.create(parent_fk2=parent)\n\n    # Test inlineformset_factory with fk_name specified\n    ChildWithTwoFKsFormSet = inlineformset_factory(\n        ParentWithTwoFKs, ChildWithTwoFKs, fk_name='parent_fk1', fields='__all__'\n    )\n    formset = ChildWithTwoFKsFormSet(instance=parent)\n    self.assertEqual(len(formset.forms), 1)  # Only one child is related via parent_fk1\n\n    ChildWithTwoFKsFormSet = inlineformset_factory(\n        ParentWithTwoFKs, ChildWithTwoFKs, fk_name='parent_fk2', fields='__all__'\n    )\n    formset = ChildWithTwoFKsFormSet(instance=parent)\n    self.assertEqual(len(formset.forms), 1)  # Only one child is related via parent_fk2\n"], "sample_403": ["    def test_add_index(self):\n        operation = migrations.AddIndex(\n            model_name=\"UnicodeModel\",\n            index=models.Index(fields=[\"title\"], name=\"title_idx\"),\n        )\n        project_state = ProjectState()\n        new_state = project_state.clone()\n        model_state = ModelState(\n            \"myapp\",\n            \"UnicodeModel\",\n            [\n                (\"title\", models.CharField(max_length=255)),\n                (\"description\", models.TextField()),\n            ],\n        )\n        new_state.add_model(model_state)\n        operation.state_forwards(\"myapp\", new_state)\n        self.assertEqual(\n            new_state.models[\"myapp\", \"unicodemodel\"].indexes,\n            [models.Index(fields=[\"title\"], name=\"title_idx\")],\n        )\n        with connection.schema_editor() as editor:\n            operation.database_forwards(\"myapp\", editor, project_state, new_state)\n            self.assertIndexExists(\"myapp_unicodemodel\", [\"title\"])\n            operation.database_backwards(\"myapp\", editor, project_state, new_state)\n            self.assertIndexNotExists(\"myapp_unicodemodel\", [\"title\"])\n", "    def test_add_constraint(self):\n        model_name = \"RenameModel\"\n        constraint_name = \"unique_name\"\n        constraint = models.UniqueConstraint(fields=[\"name\"], name=constraint_name)\n        operation = migrations.AddConstraint(model_name=model_name, constraint=constraint)\n\n        # State forward\n        state = ProjectState()\n        state.add_model(ModelState(name=model_name, fields=[(\"name\", models.CharField())]))\n        new_state = state.clone()\n        operation.state_forwards(\"app\", new_state)\n        self.assertEqual(new_state.models[(\"app\", model_name.lower())].constraints, {constraint_name: constraint})\n\n        # Database forward\n        with self.temporary_migration_table() as table:\n            operation.database_forwards(\"app\", self.schema_editor(table), state, new_state)\n            constraints = self.connection.introspection.get_constraints(table)\n            self.assertEqual(constraints[constraint_name], {\"columns\": [\"name\"], \"unique\": True, \"primary_key\": False, \"foreign_key\": None, \"check\": False, \"index\": False, \"type\": \"unique\"})\n\n        # Database backward\n        with self.temporary_migration_table() as table:\n            operation.database_backwards(\"app\", self.schema_editor(table), new_state, state)\n            constraints = self.connection.introspection.get_constraints(table)\n            self.assertNotIn(constraint_name, constraints)\n", "    def prepare(self):\n        \"\"\"Prepare the application models.\"\"\"\n        self.operations = [\n            migrations.CreateModel(\"OldModel\", [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=100)),\n            ]),\n            migrations.RenameModel(\"OldModel\", \"NewModel\"),\n        ]\n        return super().prepare()\n", "def test_rename_index_with_old_fields(self):\n    old_fields = (\"name\",)\n    new_name = \"new_index_name\"\n    old_index = models.Index(fields=old_fields, name=\"old_index_name\")\n    new_index = models.Index(fields=old_fields, name=new_name)\n\n    operation = migrations.RenameIndex(\"MyModel\", new_name=new_name, old_fields=old_fields)\n    state = ProjectState()\n    state.add_model(ModelState(\"app\", \"MyModel\", [(\"name\", models.CharField(max_length=100))]))\n    state.add_index(\"app\", \"mymodel\", old_index)\n\n    # Forward\n    operation.state_forwards(\"app\", state)\n    self.assertEqual(state.models[\"app\", \"mymodel\"].indexes, {new_index.name: new_index})\n\n    # Backward\n    operation.state_forwards(\"app\", state)  # Apply the reverse operation\n    self.assertEqual(state.models[\"app\", \"mymodel\"].indexes, {old_index.name: old_index})\n\n    # Database forward and backward are tested in TestMigrate, not here.\n", "    def test_add_constraint_db(self):\n        \"\"\"\n        Test adding a constraint to a model in the database.\n        \"\"\"\n        app_label = \"testapp\"\n        project_state = ProjectState()\n        model_state = ModelState(\n            app_label, \"Model\", [(\"field\", models.IntegerField())]\n        )\n        project_state.add_model(model_state)\n\n        constraint = models.CheckConstraint(check=models.Q(field__gte=0), name=\"positive\")\n        operation = migrations.AddConstraint(model_name=\"Model\", constraint=constraint)\n\n        # Forward operation\n        with self.assertRaises(IntegrityError):\n            with atomic():\n                with connection.schema_editor() as schema_editor:\n                    operation.database_forwards(app_label, schema_editor, project_state, project_state)\n\n        # Backward operation\n        with self.assertRaises(IntegrityError):\n            with atomic():\n                with connection.schema_editor() as schema_editor:\n                    operation.database_backwards(app_label, schema_editor, project_state, project_state)\n\n        # Check constraint was added and removed\n        with connection.cursor() as cursor:\n            cursor.execute(\"SELECT constraint_name FROM information_schema.table_constraints WHERE table_name = 'testapp_model';\")\n            constraints = cursor.fetchall()\n            self.assertIn((\"positive\",), constraints)\n"], "sample_1140": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_682": ["def test_evaluate_skip_marks(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.skipif(sys.version_info < (3, 7), reason=\"requires python3.7 or higher\")\n            pass\n\n        @pytest.mark.skip(reason=\"This test is skipped unconditionally\")\n            pass\n        \"\"\"\n    )\n\n    result = testdir.runpytest()\n\n    # Check if the tests were skipped as expected\n    result.stdout.fnmatch_lines([\"*test_skip_if_version.py::test_skip_if_version SKIPPED*\"])\n    result.stdout.fnmatch_lines([\"*test_skip_unconditionally SKIPPED*\"])\n", "def test_evaluate_xfail_marks(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.xfail(reason=\"bug 123\")\n            assert False\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*1 xfailed*\"])\n", "def test_evaluate_xfail_marks_with_conditions(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.xfail(sys.version_info >= (3, 10), reason=\"Not supported in this Python version\")\n            assert True\n        \"\"\"\n    )\n\n    result = testdir.runpytest()\n\n    if sys.version_info >= (3, 10):\n        result.stdout.fnmatch_lines([\"*::test_function XFAIL*\"])\n    else:\n        result.stdout.fnmatch_lines([\"*::test_function PASSED*\"])\n", "def test_evaluate_skip_marks(testdir):\n    pytest.mark.skipif(sys.version_info < (3, 7), reason=\"requires python3.7 or higher\")\n    pytest.mark.skip(reason=\"test skip mark\")\n\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.skipif(sys.version_info < (3, 7), reason=\"requires python3.7 or higher\")\n            pass\n\n        @pytest.mark.skip(reason=\"test skip mark\")\n            pass\n        \"\"\"\n    )\n\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*::test_version_skip SKIPPED*\",\n            \"*::test_unconditional_skip SKIPPED*\",\n        ]\n    )\n    result.assert_outcomes(skipped=2)\n\n    item, = testdir.getitems(\"test_version_skip\")\n    assert evaluate_skip_marks(item).reason == \"requires python3.7 or higher\"\n    item, = testdir.getitems(\"test_unconditional_skip\")\n    assert evaluate_skip_marks(item).reason == \"test skip mark\"\n", "def test_evaluate_xfail_marks_run_false(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.xfail(run=False, reason=\"test is expected to fail\")\n            assert False\n        \"\"\"\n    )\n\n    result = testdir.runpytest()\n    result.assert_outcomes(failed=0, skipped=1, xfailed=1)\n    result.stdout.fnmatch_lines([\"*test_function*SKIPPED*\"])\n"], "sample_679": ["def test_compiled_eval(self, modulename):\n    with mock.patch('_pytest._code.compile') as mock_compile:\n        mock_compile.return_value = '2 + 2'\n        d = {'__builtins__': __builtins__}\n        result = compiled_eval('2 + 2', d)\n        assert result == 4\n        mock_compile.assert_called_once_with('2 + 2', mode=\"eval\")\n", "    def mock_item(self, modulename):\n        item = mock.Mock()\n        item.config = mock.Mock()\n        item.config.option = {EMPTY_PARAMETERSET_OPTION: False}\n        item.iter_markers = mock.Mock()\n        item.iter_markers.return_value = []\n        item.obj = mock.Mock()\n        item.obj.__globals__ = {\"__name__\": modulename}\n        return item\n", "def test_istrue_with_invalid_syntax(self):\n    item = mock.Mock()\n    item.config = mock.Mock()\n    item.obj = mock.Mock()\n    item.obj.__globals__ = {}\n    item.iter_markers.return_value = [Mark(\"testmark\", kwargs={\"condition\": \"invalid syntax\"})]\n    evaluator = MarkEvaluator(item, \"testmark\")\n    with pytest.raises(pytest.fail.Exception) as excinfo:\n        evaluator.istrue()\n    assert \"SyntaxError: invalid syntax\" in str(excinfo.value)\n", "    def test_istrue(self, attr, modulename):\n        item = mock.Mock()\n        item.config = mock.Mock()\n        item.iter_markers.return_value = [Mark(name=attr, args=[\"True\"], kwargs={})]\n        evaluator = MarkEvaluator(item, attr)\n\n        with mock.patch.dict(sys.modules, {modulename: mock.Mock()}):\n            assert evaluator.istrue()\n", "    def test_istrue(self, attr, modulename):\n        mark = Mark(self.attr, \"sys.version_info >= (3, 0)\", reason=\"Requires Python 3\")\n        item = mock.Mock()\n        item.iter_markers.return_value = [mark]\n        item.obj.__globals__ = {\"sys\": sys}\n        evaluator = MarkEvaluator(item, self.attr)\n        assert evaluator.istrue() is True\n"], "sample_343": ["    def test_get_filter_kwargs_for_object(self):\n        post = Post(title=\"Test Post\")\n        question = Question(title=\"Test Question\", post=post)\n        gfk = GenericForeignKey('post', 'id')\n        filter_kwargs = gfk.get_filter_kwargs_for_object(question)\n        self.assertEqual(filter_kwargs, {'post': post, 'id': question.id})\n", "def test_get_filter_kwargs_for_object(self):\n    question = Question.objects.create(question_text=\"What's up?\")\n    answer = Answer.objects.create(question=question, answer_text=\"Not much.\")\n    gfk = GenericForeignKey()\n    filter_kwargs = gfk.get_filter_kwargs_for_object(answer)\n    self.assertEqual(filter_kwargs, {\n        'object_id': answer.id,\n        'content_type': answer.question.content_type.id,\n    })\n", "    def test_generic_foreign_key_check(self):\n        # Test the check method of the GenericForeignKey class\n        # This test should cover all possible scenarios that the check method can handle\n\n        # Scenario 1: Field name ends with an underscore\n        gfk_field_1 = GenericForeignKey(ct_field='content_type', fk_field='object_id_')\n        self.assertIn(\n            checks.Error(\n                'Field names must not end with an underscore.',\n                obj=gfk_field_1,\n                id='fields.E001',\n            ),\n            gfk_field_1.check()\n        )\n\n        # Scenario 2: Object ID field does not exist\n        gfk_field_2 = GenericForeignKey(ct_field='content_type', fk_field='nonexistent_field')\n        self.assertIn(\n            checks.Error(\n                \"The GenericForeignKey object ID references the nonexistent field 'nonexistent_field'.\",\n                obj=gfk_field_2,\n                id='contenttypes.E001',\n            ),\n            gfk_field_2.check()\n        )\n\n        # Scenario 3: Content type field does not exist\n        gfk_field_3 = GenericForeignKey(ct_field='nonexistent_field', fk_field='object_id')\n        self.assertIn(\n            checks.Error(\n                \"The GenericForeignKey content type references the nonexistent field 'Post.nonexistent_field'.\",\n                obj=gfk_field_3,\n                id='contenttypes.E002',\n            ),\n            gfk_field_3.check()\n        )\n\n        # Scenario 4: Content type field is not a ForeignKey\n        class NonForeignKeyModel(models.Model):\n            non_foreign_key = models.CharField(max_length=10)\n\n        gfk_field_4 = GenericForeignKey(ct_field='non_foreign_key', fk_field='object_id')\n       ", "    def test_set_and_get_generic_foreign_key(self):\n        # Create a question and an answer\n        question = Question.objects.create(question_text=\"What's up?\")\n        answer = Answer.objects.create(answer_text=\"Just testing.\")\n\n        # Set the generic foreign key on the post to the question\n        post = Post.objects.create()\n        post.content_object = question\n        post.save()\n\n        # Verify that the generic foreign key was set correctly\n        self.assertEqual(post.content_object, question)\n\n        # Set the generic foreign key to the answer and verify\n        post.content_object = answer\n        post.save()\n        self.assertEqual(post.content_object, answer)\n", "    def test_get_content_type(self):\n        question = Question.objects.create(question_text=\"What's up?\")\n        content_type = Post.question.field.get_content_type(question)\n        self.assertEqual(content_type.model, 'question')\n"], "sample_1059": ["def test_assoc_laguerre_fdiff():\n    n, alpha = Dummy('n'), Dummy('alpha')\n    assert assoc_laguerre(n, alpha, x).fdiff(2) == Sum(assoc_laguerre(Dummy('k'), alpha, x) / (n - alpha), (Dummy('k'), 0, n - 1))\n", "def test_hermite_derivative():\n    n = Symbol('n', integer=True)\n    assert diff(hermite(n, x), x) == 2 * n * hermite(n - 1, x)\n", "def test_hermite_polynomial_properties():\n    n = Dummy('n')\n    assert hermite(n, -x) == (-1)**n * hermite(n, x)\n    assert hermite(n, 0) == 2**n * sqrt(pi) / gamma((1 - n) / 2)\n    assert hermite(n, oo) == oo\n    assert diff(hermite(n, x), x) == 2 * n * hermite(n - 1, x)\n\n    assert hermite(n, x)._eval_conjugate() == hermite(n, x.conjugate())\n", "def test_jacobi_differentiation():\n    n = Symbol('n', integer=True, nonnegative=True)\n    a = Symbol('a')\n    b = Symbol('b')\n\n    assert diff(jacobi(n, a, b, x), x) == S.Half * (a + b + n + 1) * jacobi(n - 1, a + 1, b + 1, x)\n    assert diff(jacobi(n, a, b, x), a) == Sum(1 / (a + b + n + Dummy(\"k\") + 1) * (jacobi(n, a, b, x) + ((a + b + 2*Dummy(\"k\") + 1) * RisingFactorial(a + Dummy(\"k\") + 1, n - Dummy(\"k\")) / ((n - Dummy(\"k\")) * RisingFactorial(a + b + Dummy(\"k\") + 1, n - Dummy(\"k\")))) * jacobi(Dummy(\"k\"), a, b, x)), (Dummy(\"k\"), 0, n - 1))\n    assert diff(jacobi(n, a, b, x), b) == Sum(1 / (a + b + n + Dummy(\"k\") + 1) * (jacobi(n, a, b, x) + ((-1)**(n - Dummy(\"k\")) * (a + b + 2*Dummy(\"k\") + 1) * RisingFactorial(b + Dummy(\"k\") + 1, n - Dummy(\"k\")) / ((n - Dummy(\"k\")) * RisingFactorial(a + b + Dummy(\"k\") + 1, n - Dummy(\"k\")))) * jacobi(Dummy(\"k\"), a, b, x)), (Dummy(\"k\"), 0, n - 1))\n\n    # Test differentiation with respect to n, should raise ArgumentIndexError\n    with raises(ArgumentIndexError):\n        diff(jacobi(n, a, b, x), n)\n", "def test_jacobi_normalized_fdiff():\n    n = Dummy('n')\n    a = Dummy('a')\n    b = Dummy('b')\n    x = Symbol('x')\n\n    assert jacobi_normalized(n, a, b, x).fdiff(1) == 0  # Diff wrt n\n\n    # Test differentiation wrt a and b\n    # Here, we need to use assumptions to avoid division by zero errors\n    with raises(ZeroDivisionError):\n        jacobi_normalized(n, a, b, x).fdiff(2)\n    with raises(ZeroDivisionError):\n        jacobi_normalized(n, a, b, x).fdiff(3)\n\n    assert unchanged(jacobi_normalized(n, a, b, x).fdiff(4), Derivative(jacobi_normalized(n, a, b, x), x))\n\n    with raises(ArgumentIndexError):\n        jacobi_normalized(n, a, b, x).fdiff(5)\n"], "sample_142": ["compilation error", "    def test_label_from_instance(self):\n        queryset = Author.objects.all()\n        field = ModelChoiceField(queryset=queryset)\n        author = Author(name='Test Author')\n        label = field.label_from_instance(author)\n        self.assertEqual(label, 'Test Author')\n", "    def test_model_choice_field_to_python(self):\n        queryset = City.objects.filter(id__in=[1, 2, 3])\n        field = ModelChoiceField(queryset=queryset)\n        self.assertEqual(field.to_python(1), queryset.get(id=1))\n        with self.assertRaises(ValidationError):\n            field.to_python(4)\n", "    def test_modelformset_factory(self):\n        from .forms import modelformset_factory\n        formset = modelformset_factory(Song, fields=['title'])\n        self.assertIsInstance(formset, forms.BaseFormSet)\n", "    def get_form(self, request, obj=None, **kwargs):\n        class ExtraFieldForm(SongForm):\n            name = forms.CharField(max_length=50)\n        return ExtraFieldForm\n"], "sample_124": ["    def test_valid_uuid(self):\n        form_data = {'uuid_field': 'c9bf9e57-1685-4c89-bafb-ff5af830be8a'}\n        form = Form({'uuid_field': UUIDField()})\n        form.is_valid()\n        self.assertEqual(form.cleaned_data['uuid_field'], uuid.UUID('c9bf9e57-1685-4c89-bafb-ff5af830be8a'))\n", "def test_multiple_choice_field_clean_empty_value(self):\n    # Test that a MultipleChoiceField returns its empty_value when passed an empty value.\n    field = MultipleChoiceField(choices=[('1', 'One'), ('2', 'Two')], empty_value=[''])\n    self.assertEqual(field.clean([]), [''])\n", "def test_datetime_field(self):\n    form_data = {'date_field': '2022-01-01 12:34:56'}\n    form = DateTimeField()\n    form.clean(form_data['date_field'])\n    self.assertEqual(form.cleaned_data['date_field'], datetime.datetime(2022, 1, 1, 12, 34, 56))\n", "def test_choice_field_clean_with_invalid_choice(self):\n    class TestForm(Form):\n        choice = ChoiceField(choices=[('a', 'A'), ('b', 'B')])\n\n    form = TestForm({'choice': 'c'})\n    self.assertFalse(form.is_valid())\n    self.assertEqual(form.errors, {'choice': ['Select a valid choice. c is not one of the available choices.']})\n", "def test_multi_value_field(self):\n    class CustomMultiValueField(MultiValueField):\n            return '-'.join(data_list)\n\n    field = CustomMultiValueField(fields=(CharField(), IntegerField()))\n    self.assertEqual(field.clean(['a', '1']), 'a-1')\n    self.assertEqual(field.clean(['a', '']), 'a-')\n    self.assertRaises(ValidationError, field.clean, ['a', 'b'])\n    self.assertRaises(ValidationError, field.clean, 'a-1')\n"], "sample_1011": ["def test_lowergamma():\n    assert mcode(lowergamma(x, y)) == \"gammainc(y, x, 'lower')\"\n", "def test_octave_code_sinc():\n    assert mcode(sinc(x)) == \"sinc(x/pi)\"\n", "compilation error", "compilation error", "def test_print_hexadecimal_numbers():\n    hex_num = Integer(255)\n    assert mcode(hex_num) == '255'\n"], "sample_186": ["    def test_check_autocomplete_fields_item(self):\n        site = AdminSite()\n        admin_obj = ValidFields(model=Song, admin_site=site)\n        model_admin_checks = ModelAdminChecks()\n\n        # Test with a field that does not exist\n        errors = model_admin_checks._check_autocomplete_fields_item(admin_obj, 'invalid_field', 'autocomplete_fields[0]')\n        self.assertEqual(len(errors), 1)\n        self.assertEqual(errors[0].id, 'admin.E037')\n\n        # Test with a field that is not a ForeignKey or ManyToManyField\n        errors = model_admin_checks._check_autocomplete_fields_item(admin_obj, 'title', 'autocomplete_fields[0]')\n        self.assertEqual(len(errors), 1)\n        self.assertEqual(errors[0].id, 'admin.E038')\n\n        # Test with a ForeignKey field that does not have a related ModelAdmin with search_fields defined\n        admin_obj.autocomplete_fields = ['album']\n        errors = model_admin_checks._check_autocomplete_fields_item(admin_obj, 'album', 'autocomplete_fields[0]')\n        self.assertEqual(len(errors), 1)\n        self.assertEqual(errors[0].id, 'admin.E039')\n\n        # Register an admin with search_fields for the related model\n        site.register(Album, MyAdmin)\n        errors = model_admin_checks._check_autocomplete_fields_item(admin_obj, 'album', 'autocomplete_fields[0]')\n        self.assertEqual(len(errors), 1)\n        self.assertEqual(errors[0].id, 'admin.E040')\n", "    def test_check_dependencies_success(self):\n        errors = admin.site.check_dependencies()\n        self.assertEqual(errors, [])\n", "    def test_autocomplete_fields_item(self):\n        ma = ValidAutocompleteFields(Album, AdminSite())\n        errors = ma._check_autocomplete_fields_item(ma, 'album', 'autocomplete_fields[0]')\n        self.assertEqual(errors, [])\n", "    def test_check_dependencies(self):\n        errors = checks.run_checks()\n        self.assertEqual(len(errors), 0)\n", "    def test_raw_id_fields_validation(self):\n        class InvalidRawIdFields(admin.ModelAdmin):\n            raw_id_fields = 'invalid'\n\n        class ValidRawIdFields(admin.ModelAdmin):\n            raw_id_fields = ['author', 'album']\n\n        class MissingRawIdFields(admin.ModelAdmin):\n            raw_id_fields = ['nonexistent']\n\n        invalid_raw_id_fields_admin = InvalidRawIdFields(Song, AdminSite())\n        valid_raw_id_fields_admin = ValidRawIdFields(Song, AdminSite())\n        missing_raw_id_fields_admin = MissingRawIdFields(Song, AdminSite())\n\n        self.assertEqual(\n            InvalidRawIdFields.check(invalid_raw_id_fields_admin),\n            [checks.Error(\n                \"The value of 'raw_id_fields' must be a list or tuple.\",\n                obj=InvalidRawIdFields,\n                id='admin.E001',\n            )]\n        )\n\n        self.assertEqual(\n            ValidRawIdFields.check(valid_raw_id_fields_admin),\n            []\n        )\n\n        self.assertEqual(\n            MissingRawIdFields.check(missing_raw_id_fields_admin),\n            [checks.Error(\n                \"The value of 'raw_id_fields[0]' refers to 'nonexistent', which is not an attribute of 'admin_checks.Song'.\",\n                obj=MissingRawIdFields,\n                id='admin.E002',\n            )]\n        )\n"], "sample_409": ["def test_blocktranslate_with_plural(self):\n    output = self.engine.render_to_string(\"i18n03\", {\"anton\": 1})\n    self.assertEqual(output, \"Singular message\")\n\n    output = self.engine.render_to_string(\"i18n03\", {\"anton\": 2})\n    self.assertEqual(output, \"Plural message\")\n", "    def test_blocktranslate_tag_with_context(self):\n        with translation.override(\"de\"):\n            output = self.engine.render_to_string(\"i18n03\", {\"anton\": \"Anton\", \"message_context\": \"greeting\"})\n            self.assertEqual(output, \"Anton \u00fcbersetzt\")\n", "def test_i18n03_context_variable_with_single_variable(self, tag_name):\n    output = self.engine.render_to_string(\"i18n03\", {\"anton\": \"Hello\"})\n    self.assertEqual(output, \"Hello\")\n", "def test_blocktranslate_with_variable(self, tag_name):\n    self.assertInHTML(\n        '<p>Hello, &amp;Aacute;nt&ouml;n!</p>',\n        self.engine.render_to_string(f\"{{% load i18n %}}{{% {tag_name} 'i18n04' %}}\"),\n    )\n", "def test_noop_filter(self, tag_name):\n    \"\"\"\n    Test the 'noop' filter for blocktranslate and blocktrans tags.\n    \"\"\"\n    output = self.engine.render_to_string(\n        f\"{{% load i18n %}}{{% {tag_name} noop %}}Testing noop filter{{% end{tag_name} %}}\"\n    )\n    self.assertEqual(output, \"Testing noop filter\")\n"], "sample_709": ["def test_make_hook_recorder(pytester: Pytester):\n    pluginmanager = PytestPluginManager()\n    hook_recorder = pytester.make_hook_recorder(pluginmanager)\n    assert isinstance(hook_recorder, HookRecorder)\n", "def test_syspathinsert(pytester: Pytester) -> None:\n    # Test syspathinsert with a custom path\n    custom_path = pytester.path / 'custom_dir'\n    custom_path.mkdir()\n    pytester.syspathinsert(custom_path)\n    assert custom_path in sys.path\n\n    # Test syspathinsert without a custom path\n    initial_sys_path = list(sys.path)\n    pytester.syspathinsert()\n    assert sys.path[0] == str(pytester.path)\n    assert sys.path[1:] == initial_sys_path\n", "def test_pytester_inline_runsource_with_config_option(pytester):\n    source = \"def test_func(): pass\"\n    cmdlineargs = \"--version\"\n    reprec = pytester.inline_runsource(source, cmdlineargs)\n    assert \"pytest\" in reprec.stdout.lines[0]\n", "def test_syspathinsert(pytester: Pytester, monkeypatch: MonkeyPatch):\n    syspathinsert_mock = MonkeyPatch().syspath_prepend\n    monkeypatch.setattr(pytester._monkeypatch, 'syspath_prepend', syspathinsert_mock)\n\n    pytester.syspathinsert()\n    syspathinsert_mock.assert_called_once_with(str(pytester.path))\n\n    custom_path = \"/custom/path\"\n    pytester.syspathinsert(custom_path)\n    syspathinsert_mock.assert_called_with(custom_path)\n", "def test_get_public_names():\n    values = [\"_private\", \"public\", \"__dunder\"]\n    result = pytester_mod.get_public_names(values)\n    assert result == [\"public\"]\n"], "sample_362": ["def test_generate_created_models_with_indexes(self):\n    before_states = []\n    after_states = [self.book_indexes]\n\n    changes = self.get_changes(before_states, after_states)\n\n    self.assertNumberMigrations(changes, 'otherapp', 1)\n    self.assertOperationTypes(changes, 'otherapp', 0, ['CreateModel', 'AddIndex'])\n    self.assertOperationAttributes(changes, 'otherapp', 0, 1, name='book_title_author_idx')\n", "def test_model_deletion_with_foreign_key_dependencies(self):\n    before_states = [self.author_with_book, self.book]\n    after_states = [self.author_with_book]\n    changes = self.get_changes(before_states, after_states)\n    self.assertNumberMigrations(changes, 'otherapp', 1)\n    self.assertOperationTypes(changes, 'otherapp', 0, ['RemoveField'])\n", "    def test_detect_changes_unmanaged_model(self):\n        before_states = [self.author_unmanaged]\n        after_states = [self.author_unmanaged_managed]\n\n        changes = self.get_changes(before_states, after_states)\n\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, ['AlterModelOptions'])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name='AuthorUnmanaged', options={'managed': True})\n", "def test_altered_check_constraints(self):\n    # Test alteration of check constraints\n    changes = self.get_changes([self.author_name], [self.author_name_check_constraint])\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['AlterModelOptions'])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0,\n                                   options={'constraints': [models.CheckConstraint(check=models.Q(name__contains='Bob'), name='name_contains_bob')]})\n", "def test_swappable_custom_user(self):\n    \"\"\"\n    Tests the correct dependency is created when swappable custom user is\n    created.\n    \"\"\"\n    changes = self.get_changes(\n        [self.author_empty],\n        [self.author_with_custom_user],\n    )\n    self.assertMigrationDependencies(changes, 'testapp', 0, [('__setting__', 'AUTH_USER_MODEL', None, True)])\n"], "sample_659": ["    def test_raise_exception(self):\n        with pytest.raises(Exception):\n            raise Exception(\"Test exception\")\n", "    def test_exception_info_match(self, request):\n        with pytest.raises(AssertionError) as excinfo:\n            excinfo.match(\"Pattern not found\")\n        assert str(excinfo.value) == \"Pattern 'Pattern not found' not found in 'Pattern not found'\"\n", "def test_exception_info_from_current_with_assertion_error_message(self):\n    try:\n        assert False, \"custom assertion error message\"\n    except AssertionError:\n        excinfo = ExceptionInfo.from_current()\n        assert str(excinfo) == \"AssertionError: custom assertion error message\"\n", "def test_get_source_with_none_source(self):\n    entry = self.make_entry()\n    entry.getsource = lambda astcache=None: None\n    fmt = FormattedExcinfo()\n    source = fmt._getentrysource(entry)\n    assert source is not None\n    assert str(source) == \"???\"\n", "    def test_chain(self):\n        try:\n            try:\n                raise ValueError(\"inner\")\n            except ValueError:\n                raise TypeError(\"outer\") from None\n        except Exception as e:\n            excinfo = ExceptionInfo(sys.exc_info())\n            assert excinfo.chain is not None\n            assert len(excinfo.chain) == 2\n            assert isinstance(excinfo.chain[0], ExceptionInfo)\n            assert isinstance(excinfo.chain[1], ExceptionInfo)\n            assert excinfo.chain[0].errisinstance(TypeError)\n            assert excinfo.chain[1].errisinstance(ValueError)\n"], "sample_74": ["def test_runshell_without_optional_params(self):\n    dbinfo = {\n        'database': 'testdb',\n        'user': 'testuser',\n    }\n    args, env = self._run_it(dbinfo)\n    self.assertEqual(args, ['psql', '-U', 'testuser', 'testdb'])\n    self.assertEqual(env, {})\n", "    def test_runshell_with_ssl_parameters(self):\n        dbinfo = {\n            'host': 'localhost',\n            'port': 5432,\n            'database': 'test_db',\n            'user': 'test_user',\n            'password': 'test_password',\n            'sslmode': 'verify-ca',\n            'sslrootcert': '/path/to/rootcert.pem',\n            'sslcert': '/path/to/cert.pem',\n            'sslkey': '/path/to/key.pem',\n        }\n        subprocess_args, pg_env = self._run_it(dbinfo)\n        self.assertIn('-U', subprocess_args)\n        self.assertIn('test_user', subprocess_args)\n        self.assertIn('-h', subprocess_args)\n        self.assertIn('localhost', subprocess_args)\n        self.assertIn('-p', subprocess_args)\n        self.assertIn('5432', subprocess_args)\n        self.assertIn('test_db', subprocess_args)\n        self.assertEqual(pg_env['PGSSLMODE'], 'verify-ca')\n        self.assertEqual(pg_env['PGSSLROOTCERT'], '/path/to/rootcert.pem')\n        self.assertEqual(pg_env['PGSSLCERT'], '/path/to/cert.pem')\n        self.assertEqual(pg_env['PGSSLKEY'], '/path/to/key.pem')\n", "    def test_runshell_db_with_ssl_parameters(self):\n        dbinfo = {\n            'database': 'testdb',\n            'user': 'testuser',\n            'password': 'testpassword',\n            'host': 'localhost',\n            'port': '5432',\n            'sslmode': 'verify-ca',\n            'sslrootcert': 'root.crt',\n            'sslcert': 'client.crt',\n            'sslkey': 'client.key',\n        }\n        subprocess_args, pg_env = self._run_it(dbinfo)\n\n        self.assertEqual(subprocess_args, ['psql', '-U', 'testuser', '-h', 'localhost', '-p', '5432', 'testdb'])\n        self.assertEqual(pg_env, {\n            'PGPASSWORD': 'testpassword',\n            'PGSSLMODE': 'verify-ca',\n            'PGSSLROOTCERT': 'root.crt',\n            'PGSSLCERT': 'client.crt',\n            'PGSSLKEY': 'client.key',\n        })\n", "def test_runshell_with_all_parameters(self):\n    dbinfo = {\n        'host': 'localhost',\n        'port': 5432,\n        'database': 'testdb',\n        'user': 'testuser',\n        'password': 'testpassword',\n        'sslmode': 'verify-full',\n        'sslrootcert': '/path/to/rootcert.pem',\n        'sslcert': '/path/to/cert.pem',\n        'sslkey': '/path/to/key.pem',\n    }\n    subprocess_args, pg_env = self._run_it(dbinfo)\n\n    # Check if all parameters are correctly passed to subprocess.run\n    self.assertEqual(subprocess_args, ['psql', '-U', 'testuser', '-h', 'localhost', '-p', '5432', 'testdb'])\n    self.assertEqual(pg_env, {\n        'PGPASSWORD': 'testpassword',\n        'PGSSLMODE': 'verify-full',\n        'PGSSLROOTCERT': '/path/to/rootcert.pem',\n        'PGSSLCERT': '/path/to/cert.pem',\n        'PGSSLKEY': '/path/to/key.pem',\n    })\n\n    # Check if SIGINT is handled correctly\n    self.assertEqual(signal.getsignal(signal.SIGINT), signal.SIG_IGN)\n", "    def test_runshell_db_with_empty_params(self):\n        \"\"\"Test runshell_db with empty connection parameters.\"\"\"\n        args, env = self._run_it({})\n        self.assertEqual(args, ['psql', ''])\n        self.assertEqual(env, {})\n"], "sample_1180": ["def test_point3d_direction_cosine():\n    p1 = Point3D(1, 2, 3)\n    p2 = Point3D(2, 3, 5)\n    assert p1.direction_cosine(p2) == [sqrt(6)/6, sqrt(6)/6, sqrt(6)/3]\n", "def test_point_canberra_distance():\n    p1 = Point(0, 0)\n    p2 = Point(3, 3)\n    assert p1.canberra_distance(p2) == 2\n\n    p1 = Point(1, 1)\n    p2 = Point(3, 3)\n    assert p1.canberra_distance(p2) == 1\n\n    p1 = Point(0, 0)\n    p2 = Point(0, 0)\n    with raises(ValueError):\n        p1.canberra_distance(p2)\n", "def test_point_subtraction():\n    p1 = Point(1, 2, 3)\n    p2 = Point(4, 5, 6)\n    result = p1 - p2\n    assert result == Point(-3, -3, -3)\n", "def test_point_canberra_distance():\n    p1 = Point2D(1, 1)\n    p2 = Point2D(3, 3)\n    assert p1.canberra_distance(p2) == 1\n\n    p3 = Point2D(0, 0)\n    p4 = Point2D(3, 3)\n    assert p3.canberra_distance(p4) == 2\n\n    p5 = Point2D(0, 0)\n    p6 = Point2D(0, 0)\n    with raises(ValueError):\n        p5.canberra_distance(p6)\n", "def test_point_project():\n    a = Point(1, 2, 3)\n    b = Point(2, 5, 6)\n    p = Point.project(a, b)\n    assert Point3D.are_collinear(Point3D(0, 0, 0), p, b)\n    assert Line3D(p, a).is_perpendicular(Line3D(p, b))\n"], "sample_385": ["    def test_release_event_form(self):\n        form = ReleaseEventForm()\n        self.assertIsInstance(form.fields[\"band\"].widget, AutocompleteSelect)\n        self.assertEqual(form.fields[\"band\"].widget.field, ReleaseEvent._meta.get_field(\"band\"))\n", "    def test_autocomplete_select_multiple(self):\n        form = AlbumFormMultiple()\n        widget = form.fields[\"featuring\"].widget\n        self.assertIsInstance(widget, AutocompleteSelectMultiple)\n        self.assertEqual(widget.attrs[\"class\"], \"multiple-class\")\n", "    def setUpTestData(cls):\n        Band.objects.create(name=\"Test Band\")\n", "    def setUpTestData(cls):\n        Band.objects.create(name=\"The Beatles\")\n", "    def setUpTestData(cls):\n        Band.objects.create(name=\"The Beatles\")\n        Band.objects.create(name=\"Led Zeppelin\")\n        Band.objects.create(name=\"Queen\")\n\n        # Create an album with multiple featuring artists\n        album = Album.objects.create(name=\"Abbey Road\")\n        featuring_artists = Band.objects.filter(name__in=[\"The Beatles\", \"Queen\"])\n        album.featuring.set(featuring_artists)\n"], "sample_631": ["def test_undefined_loop_variable(self):\n    code = \"\"\"\n    for a in b:\n        pass\n    c = a\n    \"\"\"\n    with self.assertAddsMessages(Message('undefined-loop-variable', node=astroid.Name('a'))):\n        self.checker.process_tokens(code)\n", "def test_unused_import_from_typing(self):\n    node = astroid.extract_node(\"\"\"\n    from typing import Union, List\n\n        return a\n    \"\"\")\n    with self.assertNoMessages():\n        self.checker.process_tokens(node)\n", "def test_check_module_attrs_with_dict(self):\n    module = astroid.parse(\"\"\"\n    class A:\n        __dict__ = {}\n            return self.__dict__\n    \"\"\")\n    node = module.body[0].body[1].body[0].value\n    checker = variables.VariablesChecker(linter=self.linter)\n    result = checker._check_module_attrs(node, module.body[0], ['__dict__'])\n    self.assertIsNone(result)\n", "def test_check_unpacking_with_subscript(self):\n    node = astroid.parse(\"a, b = c[0], d[1]\")\n    inferred = astroid.parse(\"(1, 2)\")\n    targets = node.targets[0].elts\n    checker = variables.VariablesChecker(linter=self.linter)\n    with self.assertNoMessages():\n        checker._check_unpacking(inferred, node, targets)\n", "def test_check_metaclasses_with_unknown_metaclass(self):\n    node = astroid.parse(\"class MyClass(metaclass=UnknownMetaclass):\\n\"\n                         \"    pass\")\n    checker = variables.VariablesChecker(linter=self.linter)\n    checker._to_consume = [(node.locals, {}, 'module')]\n    checker._check_metaclasses(node)\n    self.assertAddsMessages(\n        Message('undefined-variable', line=1, node=node, args=('UnknownMetaclass',))\n    )\n"], "sample_919": ["compilation error", "compilation error", "def test_class_definition():\n    check('class', 'class {key}C {{}}',\n          {1: 'C'},\n          'class {key}C {{}}')\n", "def test_enum_parsing():\n    check('enum', '{key}E', {1: 'E'}, '{key}enum ::E', asTextOutput='enum ::E')\n", "def test_parse_template_declaration_prefix():\n    check(\"member\", \"template < class T > T {key}::member;\",\n          {1: \"T::member1\", 2: \"T::member2\"},\n          \"template<class T> T::member\")\n"], "sample_967": ["def test_mathjax_config(app, status, warning):\n    app.build()\n    assert 'mathjax' in app.registry.html_translators[app.builder.format].math_renderer_classes\n    content = (app.outdir / 'index.html').read_text()\n    assert MATHJAX_URL in content\n    assert 'MathJax.Hub.Config' in content\n    assert 'window.MathJax' in content\n    assert 'mathjax_path config value must be set' in warning.getvalue()\n\n    app.config.mathjax_path = 'custom_path'\n    app.config.mathjax2_config = {'tex2jax': {'inlineMath': [['$', '$']]}}\n    app.config.mathjax3_config = {'tex': {'inlineMath': [['$', '$']]}}\n    app.build()\n    content = (app.outdir / 'index.html').read_text()\n    assert 'custom_path' in content\n    assert 'tex2jax' in content\n    assert 'tex' in content\n", "def test_mathjax_config(app, status, warning):\n    app.build()\n    assert 'mathjax.js' in app.outdir.listdir()\n\n    mathjax_config = app.config.mathjax_config\n    assert mathjax_config is not None\n\n    mathjax_source = (app.outdir / '_static' / 'mathjax.js').read_text()\n    assert f'MathJax.Hub.Config({mathjax_config})' in mathjax_source\n\n    with warnings.catch_warnings(record=True) as w:\n        app.config.mathjax_path = MATHJAX_URL\n        app.build()\n        assert len(w) == 1\n        assert 'mathjax_config/mathjax2_config does not work' in str(w[0].message)\n", "def test_mathjax_config(app, status, warning):\n    app.build()\n    assert \"mathjax_config does not work for the current MathJax version, use mathjax3_config instead\" in warning.getvalue()\n\n    config = app.config.mathjax3_config\n    assert config is not None\n    assert 'loader' in config\n    assert 'load' in config['loader']\n    assert 'mathjax' in config['loader']['load']\n\n    mathjax_path = app.config.mathjax_path\n    assert mathjax_path == MATHJAX_URL\n", "def test_mathjax(app, status, warning):\n    app.build()\n    assert 'math.js' in app.outdir / '_static'\n    content = (app.outdir / 'index.html').read_text()\n    assert r'\\(E=mc^2\\)' in content\n    assert r'\\[E=mc^2\\]' in content\n    assert '<script src=\"_static/math.js\" defer=\"defer\"></script>' in content\n    # Test mathjax_options\n    app.config.mathjax_options = {'scale': 1.5}\n    app.build()\n    content = (app.outdir / 'index.html').read_text()\n    assert '<script src=\"_static/math.js\" defer=\"defer\" scale=\"1.5\"></script>' in content\n    # Test mathjax_inline\n    app.config.mathjax_inline = [r'\\(', r'\\)']\n    app.build()\n    content = (app.outdir / 'index.html').read_text()\n    assert r'<span class=\"math notranslate nohighlight\">\\(E=mc^2\\)</span>' in content\n    # Test mathjax_display\n    app.config.mathjax_display = [r'\\[', r'\\]']\n    app.build()\n    content = (app.outdir / 'index.html').read_text()\n    assert r'<div class=\"math notranslate nohighlight\">\\[[E=mc^2]\\]</div>' in content\n    # Test mathjax_config/mathjax2_config\n    app.config.mathjax_config = {'tex2jax': {'inlineMath': [['$', '$']], 'processEscapes': True}}\n    with pytest.warns(UserWarning, match='mathjax_config/mathjax2_config does not work for the current MathJax version'):\n        app.build()\n    # Test", "def test_mathjax_complex_equation(app, status, warning):\n    app.build()\n    assert 'build succeeded' in status.getvalue()  # build should succeed\n    html = (app.outdir / 'index.html').read_text()\n    assert r'\\[x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}\\]' in html  # check for a complex equation\n"], "sample_318": ["def test_extend_tried(self):\n    tried = []\n    pattern = 'pattern'\n    sub_tried = [['sub_pattern1'], ['sub_pattern2']]\n    URLResolver._extend_tried(tried, pattern, sub_tried)\n    self.assertEqual(tried, [['pattern', 'sub_pattern1'], ['pattern', 'sub_pattern2']])\n", "def test_pattern_name_contains_colon(self):\n    url_pattern = URLPattern(pattern=RegexPattern(regex=r'^test/$', name='test:view'), callback=views.empty_view)\n    warnings = url_pattern.check()\n    self.assertEqual(len(warnings), 1)\n    self.assertEqual(warnings[0].id, 'urls.W003')\n", "def test_resolve_error_handler(self):\n    # Test resolving error handlers\n    resolver = get_resolver()\n\n    # Test resolving a custom handler that exists\n    handler = resolver.resolve_error_handler(404)\n    self.assertEqual(handler.__name__, 'custom_handler404')\n\n    # Test resolving a custom handler that doesn't exist\n    with self.assertRaises(ViewDoesNotExist):\n        resolver.resolve_error_handler(999)\n\n    # Test resolving a custom handler that has a syntax error\n    with self.assertRaises(ImproperlyConfigured):\n        resolver.resolve_error_handler(500)\n", "def test_inner_nothing_namespace(self):\n    resolver = get_resolver()\n    url = resolver.reverse('inner-nothing', kwargs={'outer': '42'})\n    self.assertEqual(url, '/outer/42/')\n\n    match = resolve(url)\n    self.assertEqual(match.func, views.empty_view)\n    self.assertEqual(match.kwargs, {'outer': '42'})\n", "compilation error"], "sample_555": ["def test_annulus():\n    annulus = Annulus((0, 0), 2, 0.5)\n    assert_almost_equal(annulus.get_center(), (0, 0))\n    assert_almost_equal(annulus.get_radii(), (2, 2))\n    assert_almost_equal(annulus.get_width(), 3)\n    assert_almost_equal(annulus.get_height(), 3)\n    assert_almost_equal(annulus.get_angle(), 0)\n\n    annulus.set_center((1, 1))\n    annulus.set_width(4)\n    annulus.set_height(6)\n    annulus.set_angle(30)\n    assert_almost_equal(annulus.get_center(), (1, 1))\n    assert_almost_equal(annulus.get_radii(), (2, 3))\n    assert_almost_equal(annulus.get_width(), 4)\n    assert_almost_equal(annulus.get_height(), 6)\n    assert_almost_equal(annulus.get_angle(), 30)\n", "def test_rectangle_get_window_extent():\n    rect = Rectangle((0, 0), 1, 1)\n    transform = mpl.transforms.IdentityTransform()\n    bbox = rect.get_window_extent(transform)\n    assert bbox == Bbox.from_bounds(0, 0, 1, 1)\n", "def test_annulus_with_angle():\n    annulus = Annulus((0, 0), 2, 1, angle=45)\n    assert_almost_equal(annulus.get_path().vertices[0], [1.41421356, 0.0])\n", "def test_annulus_path():\n    annulus = Annulus((0, 0), 5, 3)\n    path = annulus.get_path()\n    assert len(path.vertices) == 12\n    assert path.vertices[0] == (0, -5)\n    assert path.vertices[6] == (0, 5)\n", "def test_fancyarrowpatch_mutation_scale(fig):\n    ax = fig.add_subplot(111, aspect='equal')\n    arrow = FancyArrowPatch((0, 0), (1, 1), mutation_scale=2, arrowstyle='->')\n    ax.add_patch(arrow)\n    ax.set_xlim(-2, 2)\n    ax.set_ylim(-2, 2)\n    fig.canvas.draw()\n\n    # Check that the mutation scale has been applied correctly\n    # by comparing the width of the arrow head with a reference value\n    # that was calculated manually\n    head_width = np.abs(arrow.get_path().vertices[1][0] - arrow.get_path().vertices[2][0])\n    assert_almost_equal(head_width, 0.4, decimal=1)\n"], "sample_975": ["def test_nsolve_with_one_dimensional_function():\n    x = Symbol('x')\n    result = nsolve(sin(x), x, 2)\n    assert mnorm(result - mpf(3.14159265358979)) < 1e-10\n", "def test_unrad_with_symbol_in_exponent():\n    x = Symbol('x')\n    eq = sqrt(x) + root(x, 3) - 2\n    result = unrad(eq)\n    assert result == (_p**3 + _p**2 - 2, [_p, _p**6 - x])\n", "def test_nsolve_with_one_dimension():\n    x = Symbol('x')\n    f = sin(x)\n    sol = nsolve(f, x, 2)\n    assert mnorm(sol - pi / 2) < 1e-10\n", "def test_nsolve_for_one_dimensional_function_without_specifying_variables():\n    x = Symbol('x')\n    result = nsolve(sin(x), 2)\n    assert mnorm(result - pi) < 1e-15\n", "def test_nsolve_with_precision():\n    x = Symbol('x')\n    eq = sin(x) - x\n    sol = nsolve(eq, 1, prec=100)\n    assert mnorm(eq.subs(x, sol)) < mpf(1e-99)\n"], "sample_194": ["    def test_check_constraint_sql(self):\n        check_constraint = CheckConstraint(check=models.Q(price__gte=0), name='check_price_non_negative')\n        product = Product(name='Test Product', price=10)\n        sql = check_constraint.constraint_sql(Product, connection.schema_editor())\n        self.assertIsInstance(sql, str)\n        self.assertIn('CHECK', sql)\n        self.assertIn('\"price\" >= 0', sql)\n", "    def test_unique_constraint_sql(self):\n        # Test constraint_sql method of UniqueConstraint\n        constraint = UniqueConstraint(fields=['name'], name='unique_name')\n        model = Product  # Assuming Product is a Django model with a 'name' field\n        schema_editor = connection.schema_editor()\n        expected_sql = schema_editor._unique_sql(\n            model, [model._meta.get_field('name').column], 'unique_name', condition=None,\n            deferrable=None, include=[], opclasses=()\n        )\n        self.assertEqual(constraint.constraint_sql(model, schema_editor), expected_sql)\n", "    def test_check_constraint(self):\n        # Test creation of CheckConstraint\n        check = models.CheckConstraint(check=models.Q(price__gte=0), name='price_non_negative')\n        self.assertEqual(check.constraint_sql(Product, connection.schema_editor),\n                         connection.ops.check_sql('price_non_negative', '((\"app_product\".\"price\" >= 0))'))\n\n        # Test creation of CheckConstraint with invalid check\n        with self.assertRaises(TypeError):\n            models.CheckConstraint(check=\"price >= 0\", name='invalid_check')\n\n        # Test removal of CheckConstraint\n        self.assertEqual(check.remove_sql(Product, connection.schema_editor),\n                         connection.ops.delete_check_sql(Product._meta.db_table, 'price_non_negative'))\n", "    def setUp(self):\n        self.check_constraint = CheckConstraint(check=models.Q(price__gte=0), name='check_price_non_negative')\n        self.model = Product\n", "    def test_check_constraint(self):\n        constraint = CheckConstraint(check=models.Q(price__gte=0), name='positive_price')\n        self.assertEqual(constraint.constraint_sql(Product, connection.schema_editor),\n                         connection.ops.check_sql('positive_price', '( \"myapp_product\".\"price\" >= 0 )'))\n        self.assertEqual(constraint.create_sql(Product, connection.schema_editor),\n                         connection.ops.create_check_sql(Product, 'positive_price', '( \"myapp_product\".\"price\" >= 0 )'))\n        self.assertEqual(constraint.remove_sql(Product, connection.schema_editor),\n                         connection.ops.delete_check_sql(Product, 'positive_price'))\n"], "sample_236": ["def test_collect_with_restricted_objects(self):\n    # Create a restricted object and try to collect it\n    r = R.objects.create()\n    m = M.objects.create(r=r)\n\n    collector = Collector(using=DEFAULT_DB_ALIAS)\n    with self.assertRaises(RestrictedError):\n        collector.collect([r])\n\n    # Check that the restricted object is correctly added to collector.restricted_objects\n    self.assertEqual(collector.restricted_objects[M][M.r], {m})\n", "    def test_restricted_error(self):\n        # Create instances of models with restricted foreign keys\n        b = B.objects.create()\n        m = M.objects.create(b=b)\n\n        # Delete the instance of B which has a restricted foreign key reference in M\n        with self.assertRaises(RestrictedError) as e:\n            b.delete()\n\n        # Check if the error message contains the restricted object\n        self.assertIn(str(m), str(e.exception))\n", "    def test_delete_cascade_with_keep_parents(self):\n        a1 = A.objects.create()\n        b1 = B1.objects.create(a=a1)\n        Collector(using='default').collect([b1], keep_parents=True)\n        # Assert that the parent instance was not collected for deletion\n        self.assertNotIn(a1, Collector(using='default').data[A])\n", "def test_restricted_error_raised(self):\n    # Create instances that cause RestrictedError\n    m = M.objects.create()\n    r = R.objects.create(m=m)\n\n    # Try to delete M instance, expecting RestrictedError\n    with self.assertRaises(RestrictedError):\n        m.delete()\n\n    # Verify that R instance still exists\n    self.assertIsNotNone(R.objects.filter(pk=r.pk).first())\n", "    def test_sort_method(self):\n        collector = Collector(using=connection.alias)\n        obj_a = A.objects.create(name=\"A\")\n        obj_b = B.objects.create(a=obj_a)\n        obj_c = Child.objects.create(b=obj_b)\n\n        collector.add([obj_a, obj_b, obj_c])\n        collector.sort()\n\n        # Since obj_c depends on obj_b, and obj_b depends on obj_a,\n        # obj_a should be deleted last, obj_b should be deleted second,\n        # and obj_c should be deleted first.\n        self.assertEqual(list(collector.data.keys()), [Child, B, A])\n"], "sample_443": ["    def setUp(self):\n        self.cache = caches[\"default\"]\n        self.cache.clear()\n", "    def setUp(self):\n        self.temp_dir = tempfile.mkdtemp()\n        self.cache = FileBasedCache(self.temp_dir, {})\n", "    def setUp(self):\n        self.cache = caches[\"default\"]\n        self.cache_dir = self.cache._dir\n        self.cache_suffix = self.cache.cache_suffix\n", "    def setUp(self):\n        self.cache = caches[\"default\"]\n", "    def setUp(self):\n        self.cache = caches[\"default\"]\n        self.cache.clear()\n"], "sample_212": ["def test_session_middleware(self):\n    middleware = SessionMiddleware(lambda request: HttpResponse())\n    request = HttpRequest()\n    response = middleware(request)\n    self.assertEqual(response.status_code, 200)\n    self.assertFalse(response.cookies.get(settings.SESSION_COOKIE_NAME))\n    request.COOKIES[settings.SESSION_COOKIE_NAME] = 'test_session_key'\n    request.session['test_key'] = 'test_value'\n    response = middleware(request)\n    self.assertEqual(response.status_code, 200)\n    self.assertEqual(response.cookies.get(settings.SESSION_COOKIE_NAME).value, 'test_session_key')\n    self.assertEqual(request.session.get('test_key'), 'test_value')\n    del request.session['test_key']\n    response = middleware(request)\n    self.assertEqual(response.status_code, 200)\n    self.assertFalse(response.cookies.get(settings.SESSION_COOKIE_NAME))\n", "    def setUp(self):\n        self.middleware = SessionMiddleware(lambda request: HttpResponse())\n        self.request = HttpRequest()\n        self.request.session = self.middleware.SessionStore()\n", "def test_session_middleware(self):\n    \"\"\"\n    Test that SessionMiddleware correctly sets and deletes session cookies.\n    \"\"\"\n    request = HttpRequest()\n    request.COOKIES['sessionid'] = 'testsession'\n    middleware = SessionMiddleware()\n    middleware.process_request(request)\n    response = HttpResponse()\n    middleware.process_response(request, response)\n    self.assertIn('sessionid', response.cookies)\n    request.session.flush()\n    response = HttpResponse()\n    middleware.process_response(request, response)\n    self.assertIn('sessionid', response.deleted_cookies)\n", "    def test_process_request(self):\n        \"\"\"\n        Test that the session is correctly set in the request.\n        \"\"\"\n        middleware = SessionMiddleware(lambda request: HttpResponse())\n        request = HttpRequest()\n        request.COOKIES[settings.SESSION_COOKIE_NAME] = 'test_session_key'\n        middleware.process_request(request)\n        self.assertIsInstance(request.session, middleware.SessionStore)\n        self.assertEqual(request.session.session_key, 'test_session_key')\n", "def test_session_middleware(self):\n    # Arrange\n    middleware = SessionMiddleware(get_response=lambda request: HttpResponse())\n    request = HttpRequest()\n    request.COOKIES[settings.SESSION_COOKIE_NAME] = 'test_session_key'\n\n    # Act\n    middleware.process_request(request)\n    response = middleware.process_response(request, HttpResponse())\n\n    # Assert\n    self.assertIsInstance(request.session, middleware.SessionStore)\n    self.assertEqual(response.cookies[settings.SESSION_COOKIE_NAME].value, 'test_session_key')\n"], "sample_297": ["    def test_annotation_filter(self):\n        self.assertSequenceEqual(\n            Annotation.objects.filter(tag=self.t1),\n            [self.ann1]\n        )\n", "    def test_add_filter(self):\n        query = Item.objects.all()\n        query.add_filter(('creator__name', 'a2'))\n        self.assertSequenceEqual(\n            query.values_list('name', flat=True),\n            ['two', 'three']\n        )\n", "def test_query_ordering(self):\n    # Test ordering by a single field\n    items = Item.objects.order_by('name')\n    self.assertEqual([i.name for i in items], ['four', 'one', 'three', 'two'])\n\n    # Test ordering by multiple fields\n    items = Item.objects.order_by('created', 'name')\n    self.assertEqual([i.name for i in items], ['two', 'one', 'three', 'four'])\n\n    # Test ordering by a related field\n    items = Item.objects.order_by('creator__name')\n    self.assertEqual([i.name for i in items], ['one', 'three', 'two', 'four'])\n\n    # Test ordering by a field with a transform\n    items = Item.objects.order_by('name__lower')\n    self.assertEqual([i.name for i in items], ['four', 'one', 'three', 'two'])\n\n    # Test ordering by a field with a transform and a related field\n    items = Item.objects.order_by('creator__name__lower')\n    self.assertEqual([i.name for i in items], ['one', 'three', 'two', 'four'])\n", "    def test_filter_created_range(self):\n        items = Item.objects.filter(created__range=(self.time1, self.time2))\n        self.assertEqual(set(items), {self.i1, self.i2})\n", "def test_raw_query(self):\n    query = 'SELECT * FROM myapp_author WHERE name = %s'\n    params = ('a1',)\n    authors = Author.objects.raw(query, params)\n    self.assertEqual(len(authors), 1)\n    self.assertEqual(authors[0].name, 'a1')\n"], "sample_156": ["def test_form_validation(self):\n    data = {'first_name': 'John', 'last_name': 'Doe', 'birthday': '1990-01-01'}\n    form = Person(data)\n    self.assertTrue(form.is_valid())\n    self.assertEqual(form.cleaned_data, data)\n", "def test_form_has_changed(self):\n    data = {'first_name': 'John', 'last_name': 'Doe', 'birthday': '1990-01-01'}\n    form = Person(data=data, initial={'first_name': 'John', 'last_name': 'Doe', 'birthday': '1990-01-01'})\n    self.assertFalse(form.has_changed())\n\n    data = {'first_name': 'Jane', 'last_name': 'Doe', 'birthday': '1990-01-01'}\n    form = Person(data=data, initial={'first_name': 'John', 'last_name': 'Doe', 'birthday': '1990-01-01'})\n    self.assertTrue(form.has_changed())\n", "    def test_form_as_p(self):\n        data = {'first_name': 'John', 'last_name': 'Doe', 'birthday': '1990-01-01'}\n        form = Person(data)\n        rendered_form = form.as_p()\n        self.assertIn('John', rendered_form)\n        self.assertIn('Doe', rendered_form)\n        self.assertIn('1990-01-01', rendered_form)\n        self.assertIn('first_name', rendered_form)\n        self.assertIn('last_name', rendered_form)\n        self.assertIn('birthday', rendered_form)\n", "def test_add_error(self):\n    form_data = {'first_name': 'John', 'last_name': 'Doe', 'birthday': '1990-01-01'}\n    form = Person(data=form_data)\n    form.full_clean()\n    form.add_error('first_name', 'This is a custom error message.')\n    self.assertIn('first_name', form.errors)\n    self.assertEqual(form.errors['first_name'][0], 'This is a custom error message.')\n", "    def test_form_as_p(self):\n        form = Person()\n        output = form.as_p()\n        self.assertIn('<p><label for=\"id_first_name\">First name:</label> <input type=\"text\" name=\"first_name\" required id=\"id_first_name\"></p>', output)\n        self.assertIn('<p><label for=\"id_last_name\">Last name:</label> <input type=\"text\" name=\"last_name\" required id=\"id_last_name\"></p>', output)\n        self.assertIn('<p><label for=\"id_birthday\">Birthday:</label> <input type=\"text\" name=\"birthday\" required id=\"id_birthday\"></p>', output)\n"], "sample_452": ["    def test_rename_index_operation(self):\n        # Forward operation\n        operation = RenameIndex(model_name='MyModel', new_name='new_index_name', old_name='old_index_name')\n        new_state = ProjectState()\n        operation.state_forwards('myapp', new_state)\n        self.assertIn(('myapp', 'mymodel'), new_state.models)\n        index = new_state.models['myapp', 'mymodel'].get_index_by_name('new_index_name')\n        self.assertEqual(index.name, 'new_index_name')\n\n        # Backward operation\n        old_state = ProjectState()\n        operation.state_forwards('myapp', old_state)  # Apply forward operation first\n        operation.state_forwards('myapp', new_state)  # Then apply backward operation\n        index = new_state.models['myapp', 'mymodel'].get_index_by_name('old_index_name')\n        self.assertEqual(index.name, 'old_index_name')\n", "    def test_rename_index(self):\n        operation = migrations.RenameIndex(\"Author\", \"new_name\", \"old_name\")\n        state = ProjectState()\n        state.add_model(ModelState(\"testapp\", \"Author\", [(\"name\", models.CharField(max_length=255))]))\n        operation.state_forwards(\"testapp\", state)\n        self.assertEqual(state.models[\"testapp\", \"author\"].indexes, [models.Index(fields=[\"name\"], name=\"new_name\")])\n        with self.assertRaisesMessage(ValueError, \"Found wrong number (0) of indexes for testapp_author(name).\"):\n            operation.database_forwards(\"testapp\", self.schema_editor, state, state)\n        state.add_index(\"testapp\", \"author\", models.Index(fields=[\"name\"], name=\"old_name\"))\n        with self.assertNoDatabaseQueries():\n            operation.database_forwards(\"testapp\", self.schema_editor, state, state)\n        self.assertEqual(state.models[\"testapp\", \"author\"].indexes, [models.Index(fields=[\"name\"], name=\"new_name\")])\n        with self.assertNoDatabaseQueries():\n            operation.database_backwards(\"testapp\", self.schema_editor, state, state)\n        self.assertEqual(state.models[\"testapp\", \"author\"].indexes, [models.Index(fields=[\"name\"], name=\"old_name\")])\n", "    def test_add_model_with_base_manager(self):\n        operation = migrations.CreateModel(\n            \"BaseManagerModel\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=255)),\n            ],\n            managers=[(\"objects\", models.Manager())],\n        )\n\n        new_state = self.set_up_test_state([], operation)\n        self.assertModelInState(new_state, \"BaseManagerModel\")\n        self.assertEqual(\n            new_state.apps.get_model(\"operations\", \"BaseManagerModel\")._default_manager.__class__,\n            models.Manager,\n        )\n\n        with self.temporary_migration_table():\n            operation.state_forwards(\"operations\", self.from_state)\n            with connection.schema_editor() as schema_editor:\n                operation.database_forwards(\"operations\", schema_editor, self.from_state, new_state)\n            self.assertTableExists(\"operations_basemanagermodel\")\n\n            operation.database_backwards(\"operations\", schema_editor, new_state, self.from_state)\n            self.assertTableNotExists(\"operations_basemanagermodel\")\n", "    def test_add_constraint(self):\n        old_state = ProjectState()\n        old_state.add_model(\n            ModelState(\n                \"app\",\n                \"Model\",\n                [(\"field\", models.CharField(max_length=20))],\n            )\n        )\n\n        operation = AddConstraint(\n            model_name=\"Model\",\n            constraint=models.CheckConstraint(check=models.Q(field__length__gt=0), name=\"check\"),\n        )\n\n        new_state = old_state.clone()\n        operation.state_forwards(\"app\", new_state)\n        self.assertEqual(new_state.models[(\"app\", \"model\")].constraints, {(\"check\", models.CheckConstraint(check=models.Q(field__length__gt=0), name=\"check\"))})\n\n        with connection.schema_editor() as editor:\n            operation.database_forwards(\"app\", editor, old_state, new_state)\n            # Test the constraint is added\n            with self.assertRaises(IntegrityError):\n                with transaction.atomic():\n                    UnicodeModel.objects.create(name=\"\")\n\n        # Test the constraint is removed when rolled back\n        with connection.schema_editor() as editor:\n            operation.database_backwards(\"app\", editor, new_state, old_state)\n            UnicodeModel.objects.create(name=\"\")  # No IntegrityError should be raised\n", "    def test_add_constraint(self):\n        # Create a model with a constraint\n        operation = AddConstraint(\n            model_name='UnicodeModel',\n            constraint=models.CheckConstraint(check=models.Q(name__startswith='test'), name='test_constraint')\n        )\n\n        # Test state forwards\n        state = ProjectState()\n        operation.state_forwards('app_label', state)\n        self.assertEqual(len(state.models['app_label', 'unicodemodel'].constraints), 1)\n\n        # Test database forwards\n        with connection.schema_editor() as schema_editor:\n            with atomic():\n                operation.database_forwards('app_label', schema_editor, None, state)\n                self.assertConstraintExists(schema_editor.connection, 'app_label', 'UnicodeModel', 'test_constraint')\n\n        # Test database backwards\n        with connection.schema_editor() as schema_editor:\n            with atomic():\n                operation.database_backwards('app_label', schema_editor, None, state)\n                self.assertConstraintNotExists(schema_editor.connection, 'app_label', 'UnicodeModel', 'test_constraint')\n"], "sample_1120": ["def test_matrix_symbol_entry():\n    A = MatrixSymbol('A', 3, 4)\n    assert A[0, 0] == MatrixElement(A, 0, 0)\n    assert A[1, 2] == MatrixElement(A, 1, 2)\n", "def test_matrix_element_derivative():\n    M = MatrixSymbol('M', n, n)\n    N = MatrixSymbol('N', n, n)\n    i, j = symbols('i j', integer=True)\n\n    # Test derivative of MatrixElement with respect to another MatrixElement\n    dM_dN = diff(MatrixElement(M, i, j), MatrixElement(N, i, j))\n    assert dM_dN == KroneckerDelta(i, i, (0, n-1)) * KroneckerDelta(j, j, (0, n-1))\n\n    # Test derivative of MatrixElement with respect to the same MatrixElement\n    dM_dM = diff(MatrixElement(M, i, j), MatrixElement(M, i, j))\n    assert dM_dM == S.Zero\n\n    # Test derivative of MatrixElement with respect to a scalar\n    dM_dx = diff(MatrixElement(M, i, j), x)\n    assert dM_dx == S.Zero\n", "def test_MatrixElement_derivative_matrix_lines():\n    M = MatrixSymbol('M', n, m)\n    v = MatrixSymbol('v', n, m)\n    x = MatrixElement(M, i, j)\n    result = x._eval_derivative_matrix_lines(v)\n    expected = [_LeftRightArgs(\n        [ZeroMatrix(v.shape[0], M.shape[0]) if M.shape[0] != 1 else S.Zero,\n         ZeroMatrix(v.shape[1], M.shape[1]) if M.shape[1] != 1 else S.Zero]\n    )]\n    assert result == expected\n", "def test_MatrixElement():\n    A = MatrixSymbol('A', 3, 3)\n    i, j = symbols('i j', integer=True)\n    elem = MatrixElement(A, i, j)\n    assert elem.parent == A\n    assert elem.i == i\n    assert elem.j == j\n    assert elem.is_symbol == True\n    assert elem.is_commutative == True\n    assert elem._diff_wrt == True\n\n    # Test doit method\n    B = Matrix([[1, 2], [3, 4]])\n    elem = MatrixElement(B, 0, 1)\n    assert elem.doit() == 2\n\n    # Test _eval_derivative method\n    X = MatrixSymbol('X', 2, 2)\n    Y = MatrixSymbol('Y', 2, 2)\n    elem = MatrixElement(X, i, j)\n    assert elem._eval_derivative(MatrixElement(Y, i, j)) == KroneckerDelta(i, i, (0, 2-1)) * KroneckerDelta(j, j, (0, 2-1))\n\n    # Test _eval_derivative method with Inverse\n    elem = MatrixElement(Inverse(X), i, j)\n    i1, i2 = symbols(\"z1, z2\", cls=Dummy)\n    assert elem._eval_derivative(MatrixElement(Y, i, j)) == -Sum(Inverse(X)[i, i1]*Y[i1, i2].diff(MatrixElement(Y, i, j))*Inverse(X)[i2, j], (i1, 0, 2-1), (i2, 0, 2-1))\n", "def test_matrix_element_derivative():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', m, n)\n    C = MatrixSymbol('C', n, n)\n    D = MatrixSymbol('D', m, m)\n\n    A_ij = MatrixElement(A, i, j)\n    B_ij = MatrixElement(B, i, j)\n    C_ij = MatrixElement(C, i, j)\n    D_ij = MatrixElement(D, i, j)\n\n    i, j = symbols('i j', integer=True)\n\n    # Test derivative of A_ij with respect to B_ij\n    assert A_ij.diff(B_ij) == KroneckerDelta(i, j, (0, n-1)) * KroneckerDelta(i, j, (0, m-1))\n\n    # Test derivative of A_ij with respect to A_ij\n    assert A_ij.diff(A_ij) == KroneckerDelta(i, i, (0, n-1)) * KroneckerDelta(j, j, (0, m-1))\n\n    # Test derivative of A_ij with respect to C_ij\n    assert A_ij.diff(C_ij) == 0\n\n    # Test derivative of A_ij*B_ij with respect to A_ij\n    assert (A_ij*B_ij).diff(A_ij) == B_ij\n\n    # Test derivative of A_ij*B_ji with respect to A_ij\n    assert (A_ij*B_ji).diff(A_ij) == B_ji\n\n    # Test derivative of A_ij*B_ij*C_ij with respect to A_ij\n    assert (A_ij*B_ij*C_ij).diff(A_ij) == B_ij*C_ij\n\n    # Test derivative of (A*B*C*D)*D_ij with respect to D_ij\n    assert ((A*B*C*D)*D_ij).diff(D_ij) == A*B*C\n"], "sample_34": ["def test_unit_constructor_with_quantity():\n    q = 10 * u.m\n    new_unit = u.Unit(q)\n    assert new_unit == q.unit\n", "def test_unit_compose():\n    # Test compose method for a unit with known equivalent units\n    unit = u.m / u.s\n    equivalencies = [(u.m, u.cm, lambda x: x / 100.0, lambda x: x * 100.0)]\n    units = [u.cm, u.s]\n    result = unit.compose(equivalencies=equivalencies, units=units)\n    assert len(result) == 1\n    assert result[0].scale == 100.0\n    assert result[0].bases == [u.cm, u.s]\n    assert result[0].powers == [1, -1]\n", "def test_unit_conversion_with_equivalencies():\n    # Test unit conversion with equivalencies\n    # Define some custom equivalencies\n    my_equivalencies = [(u.km, u.m, lambda x: x * 1000, lambda x: x / 1000)]\n\n    # Create some units\n    unit1 = u.km\n    unit2 = u.m\n\n    # Convert using the custom equivalencies\n    result = unit1.to(unit2, equivalencies=my_equivalencies)\n\n    # Assert the result is as expected\n    assert result == 1000\n", "def test_unit_equivalency():\n    # Test that the unit equivalency feature works as expected\n    u.set_enabled_equivalencies(u.dimensionless_angles())\n    phase = 0.5 * u.cycle\n    assert np.isclose(np.exp(1j * phase), -1.0 + 0j)\n", "def test_unit_format_parsing():\n    assert u.Unit(\"meter\") == u.m\n    assert u.Unit(\"m\") == u.m\n    assert u.Unit(\"m/s\") == u.m / u.s\n    assert u.Unit(\"m/s**-1\") == u.m / u.s\n    assert u.Unit(\"m/s**-1\", format=\"fits\") == u.m / u.s\n    assert u.Unit(\"m / s\") == u.m / u.s\n    assert u.Unit(\"m s^-1\") == u.m / u.s\n    assert u.Unit(\"m s^(-1)\") == u.m / u.s\n    assert u.Unit(\"m s**-1\") == u.m / u.s\n    assert u.Unit(\"meter / second\") == u.m / u.s\n    assert u.Unit(\"meter second^-1\") == u.m / u.s\n    assert u.Unit(\"meter second^(-1)\") == u.m / u.s\n    assert u.Unit(\"meter second**-1\") == u.m / u.s\n    assert u.Unit(\"meter/second\") == u.m / u.s\n    assert u.Unit(\"meter/ second\") == u.m / u.s\n    assert u.Unit(\"meter /second\") == u.m / u.s\n    assert u.Unit(\"meter/second \") == u.m / u.s\n    assert u.Unit(\"meter /second \") == u.m / u.s\n    assert u.Unit(\"meter / second \") == u.m / u.s\n    assert u.Unit(\"meter / second**-1\") == u.m / u.s\n    assert u.Unit(\"meter / second**(-1)\") == u.m / u.s\n    assert u.Unit(\"meter / second ** -1\") == u.m / u.s\n    assert u.Unit(\"meter / second ** (-1)\") == u.m / u.s\n    assert u.Unit(\"meter / second **-1\") == u.m / u."], "sample_368": ["def test_migrate_all_backwards(self):\n    executor = MigrationExecutor(connection)\n    with mock.patch('django.db.migrations.executor.MigrationExecutor.apply_migration') as apply_migration:\n        with mock.patch('django.db.migrations.executor.MigrationExecutor.unapply_migration') as unapply_migration:\n            executor.migrate([('migrations', None), ('migrations2', None)], fake=True)\n            # Check that apply_migration was not called\n            apply_migration.assert_not_called()\n            # Check that unapply_migration was called twice, once for each app\n            self.assertEqual(unapply_migration.call_count, 2)\n", "    def test_migration_plan_with_replacement_migrations(self):\n        executor = MigrationExecutor(connection)\n        # Add a replacement migration to the loader\n        executor.loader.replacements = {\n            ('migrations', '0001_initial'): migrations.Migration('migrations', '0002_squashed')\n        }\n        # Build the graph with the replacement migration\n        executor.loader.build_graph()\n        # Create a migration plan with the target as the replacement migration\n        targets = [('migrations', '0002_squashed')]\n        plan = executor.migration_plan(targets)\n        # The plan should include the replacement migration and not the replaced migration\n        self.assertEqual(len(plan), 1)\n        self.assertEqual(plan[0][0].name, '0002_squashed')\n", "def test_detect_soft_applied(self):\n    executor = MigrationExecutor(connection)\n    migration = migrations.Migration(\"0001_initial\", \"migrations\")\n    migration.operations = [\n        migrations.CreateModel(\n            name=\"InitialModel\",\n            fields=[(\"id\", models.AutoField(primary_key=True))],\n        ),\n    ]\n    migration.initial = True\n    project_state = ProjectState()\n    with self.assertRaises(DatabaseError):\n        executor.detect_soft_applied(project_state, migration)\n\n    # Simulate the table creation\n    with connection.schema_editor() as schema_editor:\n        schema_editor.create_model(migration.operations[0])\n\n    # The migration should be detected as soft-applied\n    applied, new_state = executor.detect_soft_applied(project_state, migration)\n    self.assertTrue(applied)\n    self.assertIsInstance(new_state, ProjectState)\n\n    # Clean up\n    with connection.schema_editor() as schema_editor:\n        schema_editor.delete_model(migration.operations[0])\n", "    def test_apply_migration(self):\n        \"\"\"\n        Test the apply_migration method.\n        \"\"\"\n        executor = MigrationExecutor(connection)\n        migration = migrations.Migration(\"0001_initial\", \"migrations\")\n        migration.operations = [\n            migrations.CreateModel(\n                name='TestModel',\n                fields=[\n                    ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                    ('name', models.CharField(max_length=200)),\n                ],\n            ),\n        ]\n        state = ProjectState()\n        with mock.patch.object(MigrationRecorder, 'record_applied') as mock_record_applied:\n            executor.apply_migration(state, migration)\n            mock_record_applied.assert_called_once_with(\"migrations\", \"0001_initial\")\n        self.assertEqual(len(connection.introspection.table_names()), 3)\n        self.assertIn('migrations_testmodel', connection.introspection.table_names())\n", "    def test_migration_plan_clean_start(self):\n        \"\"\"Test migration_plan with clean_start=True.\"\"\"\n        executor = MigrationExecutor(connection)\n        executor.loader.build_graph()  # Populate the graph.\n\n        # Apply all migrations to create a non-empty state.\n        executor.migrate(executor.loader.graph.leaf_nodes())\n\n        # Now, with clean_start=True, we should get a plan that unapplies all migrations.\n        plan = executor.migration_plan(executor.loader.graph.leaf_nodes(), clean_start=True)\n        self.assertEqual(len(plan), len(executor.loader.graph.nodes))\n        self.assertTrue(all(backwards for _, backwards in plan))\n"], "sample_994": ["compilation error", "def test_factorial():\n    assert factorial(0) == 1\n    assert factorial(1) == 1\n    assert factorial(2) == 2\n    assert factorial(3) == 6\n    assert factorial(4) == 24\n    assert factorial(5) == 120\n    assert factorial(-1) == S.ComplexInfinity\n", "def test_floating_point_numbers():\n    assert same_and_same_prec(Float(1e20), Float('1e20'))\n", "def test_igcd():\n    assert igcd(10, 0) == 10\n    assert igcd(0, 10) == 10\n    assert igcd(0, 0) == 0\n    assert igcd(12, 15) == 3\n    assert igcd(-12, 15) == 3\n    assert igcd(12, -15) == 3\n    assert igcd(-12, -15) == 3\n    assert igcd(12, -12) == 12\n    assert igcd(12, 12) == 12\n", "def test_integer_nthroot():\n    assert integer_nthroot(27, 3) == (3, True)\n    assert integer_nthroot(26, 3) == (3, False)\n    assert integer_nthroot(24, 3) == (2, True)\n    assert integer_nthroot(25, 3) == (2, False)\n    assert integer_nthroot(0, 3) == (0, True)\n    assert integer_nthroot(1, 3) == (1, True)\n    assert integer_nthroot(2, 3) == (1, False)\n    assert integer_nthroot(100, 1) == (100, True)\n    assert integer_nthroot(100, 2) == (10, True)\n    assert integer_nthroot(100, 3) == (4, False)\n    assert integer_nthroot(100, 4) == (2, False)\n"], "sample_339": ["    def test_model_form_save(self):\n        data = {'name': 'Test Book', 'author': 'Test Author'}\n        form = BookForm(data)\n        self.assertTrue(form.is_valid())\n        book = form.save()\n        self.assertEqual(book.name, 'Test Book')\n        self.assertEqual(book.author, 'Test Author')\n", "def test_save_existing_objects_with_deleted_form(self):\n    # Create a formset with an existing instance\n    AuthorFormSet = modelformset_factory(Author, fields=('name',))\n    formset = AuthorFormSet(initial=[{'id': 1, 'name': 'Author 1'}, {'id': 2, 'name': 'Author 2'}])\n\n    # Simulate deleting the second form\n    formset.deleted_forms = [formset.forms[1]]\n\n    # Call save_existing_objects\n    formset.save_existing_objects(commit=True)\n\n    # Assert that the first object is still in the database\n    self.assertEqual(Author.objects.count(), 1)\n    self.assertEqual(Author.objects.first().name, 'Author 1')\n\n    # Assert that the second object is deleted from the database\n    self.assertFalse(Author.objects.filter(id=2).exists())\n", "    def test_clean_unique_together(self):\n        class UniqueTogetherForm(forms.ModelForm):\n            class Meta:\n                model = Book\n                fields = ['title', 'publisher']\n\n        # Create a book with title \"Test Book\" and publisher \"Test Publisher\"\n        Book.objects.create(title=\"Test Book\", publisher=\"Test Publisher\")\n\n        # Try to create another book with the same title and publisher\n        form_data = {'title': \"Test Book\", 'publisher': \"Test Publisher\"}\n        form = UniqueTogetherForm(data=form_data)\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['__all__'], [\"Book with this Title and Publisher already exists.\"])\n", "    def test_save_m2m_with_custom_pk(self):\n        # Create a form instance with a custom primary key\n        book = BookWithCustomPK(title='Test Book', custom_pk='CUSTOM_PK')\n        form_data = {'title': 'Test Book', 'custom_pk': 'CUSTOM_PK'}\n        form = modelform_factory(BookWithCustomPK, fields=['title', 'custom_pk'])(form_data, instance=book)\n\n        # Save the form without committing to the database\n        form.save(commit=False)\n\n        # Assert that the save_m2m method is available\n        self.assertTrue(hasattr(form, 'save_m2m'))\n\n        # Call the save_m2m method to save many-to-many relationships\n        form.save_m2m()\n\n        # Assert that the many-to-many relationships are saved\n        # Add assertions to verify the saved relationships\n", "    def test_save_m2m(self):\n        # Test that the save_m2m method is called when commit=False\n        Author.objects.create(name=\"Author 1\")\n        Book.objects.create(title=\"Book 1\", author=Author.objects.get(name=\"Author 1\"))\n        formset = modelformset_factory(Book, fields=(\"title\", \"author\"))\n        data = {\n            \"form-TOTAL_FORMS\": \"1\",\n            \"form-INITIAL_FORMS\": \"1\",\n            \"form-0-title\": \"Book 1\",\n            \"form-0-author\": \"1\",\n        }\n        formset = formset(data=data, queryset=Book.objects.all())\n        instances = formset.save(commit=False)\n        self.assertEqual(len(instances), 1)\n        self.assertFalse(hasattr(formset, \"save_m2m\"))\n        for form in formset:\n            form.save_m2m()\n"], "sample_598": ["def test_inline_variable_array_repr(self):\n    data = np.array([1, 2, 3, 4, 5])\n    var = xr.Variable((\"x\"), data)\n    max_width = 20\n    result = formatting.inline_variable_array_repr(var, max_width)\n    expected = \"1 2 3 ... 4 5\"\n    assert result == expected\n", "def test_format_timedelta(self):\n    timedelta = pd.Timedelta(days=2, hours=3, minutes=4, seconds=5)\n    formatted = formatting.format_timedelta(timedelta)\n    assert formatted == \"2 days 03:04:05\"\n\n    formatted = formatting.format_timedelta(timedelta, timedelta_format=\"date\")\n    assert formatted == \"2 days\"\n\n    formatted = formatting.format_timedelta(timedelta, timedelta_format=\"time\")\n    assert formatted == \"03:04:05\"\n", "def test_format_timedelta_time_format(self):\n    timedelta = pd.Timedelta(days=2, hours=3, minutes=4, seconds=5)\n    result = formatting.format_timedelta(timedelta, timedelta_format=\"time\")\n    assert result == \"03:04:05\"\n", "def test_format_array_flat():\n    array = np.array([1, 2, 3, 4, 5])\n    max_width = 13\n    result = formatting.format_array_flat(array, max_width)\n    expected = '1 2 3 ... 4 5'\n    assert result == expected\n", "def test_format_items_timedelta_format(self):\n    x = np.array([np.timedelta64(1, 'D'), np.timedelta64(2, 'D'), np.timedelta64(3, 'D')])\n    assert formatting.format_items(x) == ['1 days', '2 days', '3 days']\n\n    x = np.array([np.timedelta64(1, 'h'), np.timedelta64(2, 'h'), np.timedelta64(3, 'h')])\n    assert formatting.format_items(x) == ['01:00:00', '02:00:00', '03:00:00']\n\n    x = np.array([np.timedelta64(3601, 's'), np.timedelta64(7202, 's'), np.timedelta64(10803, 's')])\n    assert formatting.format_items(x) == ['1 days 01:00:01', '2 days 02:00:02', '3 days 03:00:03']\n"], "sample_396": ["    def test_get_count(self):\n        qs = Item.objects.all()\n        self.assertEqual(qs.get_count(DEFAULT_DB_ALIAS), 4)\n", "def test_query_filtering_with_multi_join(self):\n    # Test filtering with multi-join\n    query = Item.objects.filter(creator__extra__note__extra__value__gt=41)\n    self.assertEqual(list(query), [self.i4])\n", "def test_query_filter_with_related_objects(self):\n    a1_items = Item.objects.filter(creator__name='a1')\n    self.assertEqual(a1_items.count(), 1)\n    self.assertEqual(a1_items[0].name, 'one')\n\n    a2_items = Item.objects.filter(creator__num=2002)\n    self.assertEqual(a2_items.count(), 2)\n    self.assertEqual(set(item.name for item in a2_items), {'two', 'three'})\n\n    a3_reports = Report.objects.filter(creator__extra__note__misc='bar')\n    self.assertEqual(a3_reports.count(), 1)\n    self.assertEqual(a3_reports[0].name, 'r2')\n", "    def test_queryset_order_by(self):\n        # Test ordering using a field\n        queryset = Item.objects.order_by('name')\n        self.assertEqual(queryset.query.order_by, ('name',))\n\n        # Test ordering using a field with a direction\n        queryset = Item.objects.order_by('-name')\n        self.assertEqual(queryset.query.order_by, ('-name',))\n\n        # Test ordering using multiple fields\n        queryset = Item.objects.order_by('name', 'created')\n        self.assertEqual(queryset.query.order_by, ('name', 'created'))\n\n        # Test ordering using a related field\n        queryset = Item.objects.order_by('creator__name')\n        self.assertEqual(queryset.query.order_by, ('creator__name',))\n\n        # Test ordering using a function of a field\n        queryset = Item.objects.order_by(F('name').upper())\n        self.assertEqual(str(queryset.query.order_by[0]), \"Upper(item.name)\")\n\n        # Test ordering using a function of a related field\n        queryset = Item.objects.order_by(F('creator__name').lower())\n        self.assertEqual(str(queryset.query.order_by[0]), \"Lower(author.name)\")\n", "def test_annotation_exclusion(self):\n    # Test that annotated fields are excluded correctly in values_list.\n    items = Item.objects.annotate(annotation=Count('tags')).values_list('annotation')\n    self.assertEqual(list(items), [(1,), (2,), (0,), (1,)])\n"], "sample_998": ["def test_Quaternion():\n    q = Quaternion(1, 2, 3, 4)\n    assert latex(q) == \"1 + 2\\\\mathbf{i} + 3\\\\mathbf{j} + 4\\\\mathbf{k}\"\n", "def test_translate_modifier():\n    assert translate('alphahatdotprime') == \"{\\\\dot{\\\\hat{\\\\alpha}}}'\"\n", "def test_latex_Quaternion():\n    i, j, k = symbols('i j k', commutative=False)\n    q = 2 + 3*i - 4*j + 5*k\n    assert latex(q) == \"2 + 3 i + -4 j + 5 k\"\n", "compilation error", "def test_next():\n    # Test Quaternion printing\n    q = Quaternion(1, 2, 3, 4)\n    assert latex(q) == \"1 + 2\\\\mathbf{i} + 3\\\\mathbf{j} + 4\\\\mathbf{k}\"\n"], "sample_1195": ["def test_kahane_simplify_mixed_indices():\n    i0, i1, i2 = tensor_indices('i0:3', LorentzIndex)\n    p, q = tensor_heads('p, q', [LorentzIndex])\n    ta = G(i0)*p(-i0)*G(i1)*q(-i1)\n    tb = kahane_simplify(ta)\n    assert _is_tensor_eq(tb, p(i0)*q(-i0) - q(i0)*p(-i0))\n", "def test_kahane_simplify_with_spinor_indices():\n    from sympy.physics.hep.gamma_matrices import DiracSpinorIndex\n    S1, S2 = tensor_indices('S1:3', DiracSpinorIndex)\n    sigma = TensorHead('sigma', [DiracSpinorIndex, DiracSpinorIndex])\n    t = sigma(S1, S2) * G(-S2) * G(S1)\n    result = kahane_simplify(t)\n    expected = 2 * eye(4)\n    assert _is_tensor_eq(result, expected)\n", "def test_kahane_simplify_with_odd_number_of_contractions():\n    i0, i1, i2, i3 = tensor_indices('i0:4', LorentzIndex)\n    tc = G(i0)*G(i1)*G(-i0)*G(i2)*G(-i1)*G(i3)*G(-i2)\n    result = kahane_simplify(tc)\n    expected = -2 * G(i3)\n    assert _is_tensor_eq(result, expected), f\"Expected {expected}, but got {result}\"\n", "def test_simplify_gamma_expression():\n    i0, i1 = tensor_indices('i0:2', LorentzIndex)\n    p = TensorHead('p', [LorentzIndex])\n    expression = p(i0)*G(i1)*G(-i0)*G(-i1)\n    expected = 4*p(i0)*p(-i0) - 4*eye(4)\n    assert _is_tensor_eq(simplify_gamma_expression(expression), expected)\n", "def test_kahane_simplify_multiple_contractions():\n    i0, i1, i2, i3 = tensor_indices('i0:4', LorentzIndex)\n    expression = G(i0)*G(-i0)*G(i1)*G(-i1)*G(i2)*G(-i2)*G(i3)*G(-i3)\n    result = kahane_simplify(expression)\n    expected_result = eye(4)**4\n    assert _is_tensor_eq(result, expected_result)\n"], "sample_49": ["    def test_merge_different_order(self):\n        # Test merge function with two lists in different order\n        list_1 = ['css/a.css', 'js/b.js']\n        list_2 = ['js/b.js', 'css/a.css']\n        with self.assertWarns(Media.MediaOrderConflictWarning):\n            result = Media.merge(list_1, list_2)\n        self.assertEqual(result, list_1)\n", "def test_multiwidget_media(self):\n    class MultiTestWidget(MultiWidget):\n            widgets = [TextInput(attrs={'class': 'foo'}), TextInput(attrs={'class': 'bar'})]\n            super().__init__(widgets, attrs)\n\n            return ['', ''] if value is None else value.split('-')\n\n    widget = MultiTestWidget()\n    expected_media = Media(css={'all': ['http://media.example.com/static/django/forms/widgets/text.css']},\n                           js=['http://media.example.com/static/django/forms/widgets/text.js'])\n    self.assertEqual(widget.media, expected_media)\n", "    def test_multiwidget_render(self):\n        class TestMultiWidget(MultiWidget):\n                return ['foo', 'bar']\n\n        widget = TestMultiWidget(widgets=(TextInput, TextInput))\n        rendered = widget.render('name', 'value')\n        self.assertIn('name_0', rendered)\n        self.assertIn('name_1', rendered)\n        self.assertIn('value_foo', rendered)\n        self.assertIn('value_bar', rendered)\n", "def test_media_merge_conflict_warning(self):\n    media1 = Media(css={'all': ['styles1.css']}, js=['scripts1.js'])\n    media2 = Media(css={'all': ['scripts1.js', 'styles1.css']}, js=['styles1.css', 'scripts1.js'])\n    merged_media = media1 + media2\n    with self.assertWarns(Media.MediaOrderConflictWarning):\n        merged_media.render()\n", "    def decompress(self, value):\n        return value.split(',') if value else [None, None]\n"], "sample_987": ["def test_get_integer_part():\n    expr = 3.14159\n    re_, im_, re_acc, im_acc = get_integer_part(expr, 1, {}, return_ints=True)\n    assert re_ == 4 and im_ == 0\n", "def test_get_integer_part():\n    assert get_integer_part(S(3.14159), 1, {}, return_ints=True) == (4, 0)\n    assert get_integer_part(S(3.14159), -1, {}, return_ints=True) == (3, 0)\n    assert get_integer_part(S(3.0), 1, {}, return_ints=True) == (3, 0)\n    assert get_integer_part(S(3.0), -1, {}, return_ints=True) == (3, 0)\n", "def test_complex_accuracy():\n    re, im = from_float(1.234), from_float(5.678)\n    re_acc, im_acc = 10, 12\n    result = re, im, re_acc, im_acc\n    assert complex_accuracy(result) == 10\n", "def test_evalf_abs():\n    assert NS(Abs(1 + I), 10) == '1.4142135624'\n", "def test_evalf_atan():\n    from sympy import atan, N, pi\n    x = atan(1)\n    assert NS(N(x), 2) == \"0.8 + 3.1e-17*I\"\n    x = atan(0)\n    assert NS(N(x), 2) == \"0\"\n    x = atan(oo)\n    assert NS(N(x), 2) == \"1.6*I\"\n    x = atan(ninf)\n    assert NS(N(x), 2) == \"-1.6*I\"\n    x = atan(I)\n    assert NS(N(x), 2) == \"1.6*I\"\n    x = atan(-I)\n    assert NS(N(x), 2) == \"-1.6*I\"\n    x = atan(1 + I)\n    assert NS(N(x), 2) == \"1.3 + 0.3*I\"\n    x = atan(pi)\n    assert NS(N(x), 2) == \"1.2 + 1.3e-16*I\"\n"], "sample_542": ["def test_annotation_contains(fig):\n    ax = fig.add_subplot(111)\n    annot = ax.annotate('Test', xy=(0.5, 0.5), xytext=(0.2, 0.2),\n                        arrowprops=dict(arrowstyle=\"->\"))\n    event = MouseEvent('button_press_event', fig.canvas, 100, 100, button=1)\n    contains, info = annot.contains(event)\n    assert contains\n    event = MouseEvent('button_press_event', fig.canvas, 1, 1, button=1)\n    contains, info = annot.contains(event)\n    assert not contains\n", "def test_text_get_rotation():\n    fig, ax = plt.subplots()\n    text = Text(0, 0, 'Test', rotation=45)\n    ax.add_artist(text)\n    assert text.get_rotation() == 45\n", "def test_text_rotation():\n    fig, ax = plt.subplots()\n    text = Text(0.5, 0.5, \"Test\", rotation=45)\n    ax.add_artist(text)\n    assert_almost_equal(text.get_rotation(), 45)\n\n    text.set_rotation(90)\n    assert_almost_equal(text.get_rotation(), 90)\n\n    text.set_rotation('vertical')\n    assert_almost_equal(text.get_rotation(), 90)\n\n    text.set_rotation('horizontal')\n    assert_almost_equal(text.get_rotation(), 0)\n\n    with pytest.raises(ValueError):\n        text.set_rotation('invalid')\n", "def test_font_weights(self):\n    fig, ax = plt.subplots()\n    weights = ['ultralight', 'light', 'normal', 'regular', 'book', 'medium',\n               'roman', 'semibold', 'demibold', 'demi', 'bold', 'heavy',\n               'extra bold', 'black']\n    y = np.linspace(0.95, 0.05, len(weights))\n    for i, weight in enumerate(weights):\n        ax.text(0.5, y[i], f'{weight} {i+1}', weight=weight,\n                va='center', ha='center', size=24)\n    ax.set_axis_off()\n", "def test_get_rotation(fig, caplog):\n    text = Text(0, 0, 'test')\n    text.set_rotation(None)\n    assert text.get_rotation() == 0.0\n    text.set_rotation('horizontal')\n    assert text.get_rotation() == 0.0\n    text.set_rotation('vertical')\n    assert text.get_rotation() == 90.0\n    text.set_rotation(45.0)\n    assert text.get_rotation() == 45.0\n    text.set_rotation(360.0)\n    assert text.get_rotation() == 0.0\n    text.set_rotation(-90.0)\n    assert text.get_rotation() == 270.0\n    with pytest.raises(ValueError):\n        text.set_rotation('invalid')\n\n    # Test logging warning when rotation is not a number, 'horizontal', 'vertical', or None\n    text.set_rotation('test')\n    assert \"rotation is 'test'; expected either 'horizontal', 'vertical', numeric value, or None\" in caplog.text\n"], "sample_334": ["    def test_form_prefix(self):\n        form = Person(prefix='person')\n        self.assertEqual(form.prefix, 'person')\n        self.assertEqual(form['first_name'].html_name, 'person-first_name')\n", "    def test_as_table(self):\n        form = Person({'first_name': 'John', 'last_name': 'Doe', 'birthday': '1990-01-01'})\n        self.assertIn('John', form.as_table())\n        self.assertIn('Doe', form.as_table())\n        self.assertIn('1990-01-01', form.as_table())\n", "def test_form_as_table(self):\n    form = PersonNew({\n        'first_name': 'John',\n        'last_name': 'Doe',\n        'birthday': '1990-01-01'\n    })\n    html = form.as_table()\n    self.assertInHTML('<input type=\"text\" name=\"first_name\" value=\"John\" id=\"first_name_id\" required>', html)\n    self.assertInHTML('<input type=\"text\" name=\"last_name\" value=\"Doe\" required>', html)\n    self.assertInHTML('<input type=\"text\" name=\"birthday\" value=\"1990-01-01\" required>', html)\n", "def test_has_error_method(self):\n    form = PersonNew({'first_name': 'John', 'last_name': 'Doe', 'birthday': '1990-01-01'})\n    form.add_error('first_name', 'Error in first name')\n    self.assertTrue(form.has_error('first_name'))\n    self.assertFalse(form.has_error('last_name'))\n    form.add_error('first_name', ValidationError('Another error in first name', code='invalid'))\n    self.assertTrue(form.has_error('first_name', code='invalid'))\n    self.assertFalse(form.has_error('first_name', code='required'))\n", "    def test_as_ul_rendering(self):\n        form = Person(data={'first_name': 'John', 'last_name': 'Doe', 'birthday': '1990-01-01'})\n        ul_rendering = form.as_ul()\n        self.assertIn('<li><label', ul_rendering)\n        self.assertIn('John', ul_rendering)\n        self.assertIn('Doe', ul_rendering)\n        self.assertIn('1990-01-01', ul_rendering)\n"], "sample_835": ["def test_adaboost_classifier_with_sample_weight():\n    X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=rng)\n    sample_weight = rng.rand(len(y_train))\n    clf = AdaBoostClassifier(n_estimators=10, random_state=rng)\n    clf.fit(X_train, y_train, sample_weight=sample_weight)\n    y_pred = clf.predict(X_test)\n    assert_array_equal(y_pred.shape, y_test.shape)\n", "def test_adaboost_classifier_staged_predict_proba():\n    clf = AdaBoostClassifier(n_estimators=2)\n    clf.fit(iris.data, iris.target)\n    probas = list(clf.staged_predict_proba(iris.data))\n    assert len(probas) == 2\n    for proba in probas:\n        assert_array_almost_equal(np.sum(proba, axis=1), np.ones(iris.target.size))\n", "def test_adaboost_classifier_with_custom_base_estimator():\n    # Test AdaBoostClassifier with a custom base estimator\n    class CustomEstimator(BaseEstimator):\n            self.param = param\n\n            self.classes_ = np.unique(y)\n            return self\n\n            return np.full(len(X), self.classes_[0])\n\n    clf = AdaBoostClassifier(base_estimator=CustomEstimator(param=\"test\"),\n                             n_estimators=50, random_state=rng)\n    clf.fit(X, y_class)\n    assert_array_equal(clf.predict(T), y_t_class)\n", "def test_adaboost_classifier_fit_predict():\n    X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=rng)\n    clf = AdaBoostClassifier(n_estimators=100, random_state=rng)\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    assert_array_equal(y_pred.shape, y_test.shape)\n", "def test_adaboost_classifier_with_sparse_input():\n    # Test AdaBoostClassifier with sparse input matrices\n    X_sparse = csc_matrix(X)\n    y_sparse = np.array(y_class)\n    clf = AdaBoostClassifier(n_estimators=10)\n    clf.fit(X_sparse, y_sparse)\n    assert clf.predict(X_sparse).tolist() == clf.predict(X).tolist()\n\n    # Test with other sparse matrix formats\n    for sparse_format in [csr_matrix, coo_matrix, dok_matrix, lil_matrix]:\n        X_sparse = sparse_format(X)\n        clf.fit(X_sparse, y_sparse)\n        assert clf.predict(X_sparse).tolist() == clf.predict(X).tolist()\n"], "sample_305": ["def test_aggregate_with_year_lookup(self):\n    books_published_after_2007 = Book.objects.filter(pubdate__year__gt=2007)\n    avg_rating = books_published_after_2007.aggregate(Avg('rating'))\n    self.assertAlmostEqual(avg_rating['rating__avg'], 4.142857, places=4)\n", "def test_aggregate_on_uuid_field(self):\n    uuid_value = uuid.uuid4()\n    self.b7 = Book.objects.create(\n        isbn='1234567890',\n        name='Test Book with UUID',\n        pages=200,\n        rating=3.5,\n        price=Decimal('15.99'),\n        contact=self.a1,\n        publisher=self.p1,\n        pubdate=datetime.date(2021, 1, 1),\n        uuid_field=uuid_value\n    )\n    aggregate_result = Book.objects.filter(uuid_field=uuid_value).aggregate(Avg('pages'))\n    self.assertEqual(aggregate_result['pages__avg'], 200)\n", "def test_uuid_lookups(self):\n    uuid = '123e4567-e89b-12d3-a456-426614174000'\n    self.a1.uuid = uuid\n    self.a1.save()\n\n    author = Author.objects.get(uuid__iexact=uuid)\n    self.assertEqual(author, self.a1)\n\n    author = Author.objects.get(uuid__contains=uuid[5:10])\n    self.assertEqual(author, self.a1)\n\n    author = Author.objects.get(uuid__icontains=uuid[5:10].upper())\n    self.assertEqual(author, self.a1)\n\n    author = Author.objects.get(uuid__startswith=uuid[:5])\n    self.assertEqual(author, self.a1)\n\n    author = Author.objects.get(uuid__istartswith=uuid[:5].upper())\n    self.assertEqual(author, self.a1)\n\n    author = Author.objects.get(uuid__endswith=uuid[-5:])\n    self.assertEqual(author, self.a1)\n\n    author = Author.objects.get(uuid__iendswith=uuid[-5:].upper())\n    self.assertEqual(author, self.a1)\n", "def test_year_lookup(self):\n    books_published_in_2008 = Book.objects.filter(pubdate__year=2008)\n    self.assertEqual(books_published_in_2008.count(), 4)\n    self.assertIn(self.b1, books_published_in_2008)\n    self.assertNotIn(self.b5, books_published_in_2008)\n\n    books_published_before_2008 = Book.objects.filter(pubdate__year__lt=2008)\n    self.assertEqual(books_published_before_2008.count(), 2)\n    self.assertIn(self.b5, books_published_before_2008)\n\n    books_published_after_2008 = Book.objects.filter(pubdate__year__gt=2008)\n    self.assertEqual(books_published_after_2008.count(), 1)\n    self.assertIn(self.b6, books_published_after_2008)\n", "def test_aggregate_with_when_then(self):\n    # Test aggregate with When and Then\n    aggregate_result = Book.objects.aggregate(\n        avg_pages=Avg(Case(\n            When(rating__gt=4.0, then='pages'),\n            default=0,\n            output_field=IntegerField(),\n        )),\n        total_books=Count('id'),\n    )\n    self.assertAlmostEqual(aggregate_result['avg_pages'], 447.0, places=1)\n    self.assertEqual(aggregate_result['total_books'], 6)\n"], "sample_964": ["def test_parse_annotation(app):\n    env = app.builder.env\n    result = _parse_annotation(\"List[int]\", env)\n    expected = [pending_xref('', 'List', refdomain='py', reftype='class', reftarget='List'),\n                nodes.Text('['),\n                pending_xref('', 'int', refdomain='py', reftype='class', reftarget='int'),\n                nodes.Text(']')]\n    assert result == expected\n", "def test_parse_annotation_with_env(app):\n    env = app.builder.env\n    annotation = \"List[Union[int, str]]\"\n    result = _parse_annotation(annotation, env)\n    assert isinstance(result, list)\n    assert len(result) == 8\n    assert isinstance(result[0], pending_xref)\n    assert isinstance(result[1], addnodes.desc_sig_punctuation)\n    assert isinstance(result[2], pending_xref)\n    assert isinstance(result[3], addnodes.desc_sig_punctuation)\n    assert isinstance(result[4], addnodes.desc_sig_punctuation)\n    assert isinstance(result[5], addnodes.desc_sig_punctuation)\n    assert isinstance(result[6], addnodes.desc_sig_punctuation)\n    assert isinstance(result[7], pending_xref)\n", "def test_parse_annotation():\n    # mock environment for _parse_annotation\n    env = Mock()\n    env.config.python_use_unqualified_type_names = False\n\n    # test parsing of a simple type annotation\n    annotation = \"int\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 1\n    assert isinstance(result[0], pending_xref)\n    assert result[0].astext() == \"int\"\n\n    # test parsing of a complex type annotation\n    annotation = \"List[Union[int, str]]\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 5\n    assert isinstance(result[0], pending_xref)\n    assert result[0].astext() == \"List\"\n    assert isinstance(result[1], nodes.Text)\n    assert result[1].astext() == \"[\"\n    assert isinstance(result[2], pending_xref)\n    assert result[2].astext() == \"Union\"\n    assert isinstance(result[3], nodes.Text)\n    assert result[3].astext() == \"]\"\n    assert isinstance(result[4], nodes.Text)\n    assert result[4].astext() == \"\"\n", "def test_parse_annotation(app):\n    env = app.builder.env\n    result = _parse_annotation(\"List[int]\", env)\n    assert isinstance(result, list)\n    assert len(result) == 5\n    assert isinstance(result[0], pending_xref)\n    assert result[0].astext() == \"List\"\n    assert isinstance(result[1], desc_sig_punctuation)\n    assert result[1].astext() == \"[\"\n    assert isinstance(result[2], pending_xref)\n    assert result[2].astext() == \"int\"\n    assert isinstance(result[3], desc_sig_punctuation)\n    assert result[3].astext() == \"]\"\n", "def test_parse_annotation_typevar(app, status, warning):\n    env = app.builder.env\n\n    nodes = _parse_annotation(\"typing.TypeVar('T')\", env)\n    assert_node(nodes, [\n        pending_xref(reftarget=\"typing.TypeVar\", reftype=\"class\",\n                     children=[nodes.Text(\"typing.TypeVar\")]),\n        nodes.Text(\"(\"),\n        nodes.literal(text=\"'T'\"),\n        nodes.Text(\")\"),\n    ])\n"], "sample_774": ["def test_ordinal_encoder_inverse_transform():\n    enc = OrdinalEncoder()\n    X = [['Male', 1], ['Female', 3], ['Female', 2]]\n    enc.fit(X)\n    X_trans = enc.transform(X)\n    X_inv = enc.inverse_transform(X_trans)\n    assert_array_equal(X_inv, np.array(X))\n", "def test_one_hot_encoder_drop_first():\n    enc = OneHotEncoder(drop='first')\n    X = [['Male', 1], ['Female', 3], ['Female', 2]]\n    enc.fit(X)\n    X_transformed = enc.transform([['Female', 1], ['Male', 2]]).toarray()\n    expected_transformed = np.array([[1., 0.], [0., 1.]])\n    assert_array_equal(X_transformed, expected_transformed)\n", "def test_handle_unknown_error(encoder):\n    X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    enc = encoder(handle_unknown='error')\n    enc.fit(X)\n    X_test = np.array([[1, 2, 10]])  # contains an unknown category\n    assert_raises_regex(ValueError, \"Found unknown categories\", enc.transform, X_test)\n", "def test_ordinal_encoder_with_drop():\n    enc = OrdinalEncoder(categories=[[\"cold\", \"mild\", \"hot\"]])\n    enc.fit([[\"cold\"], [\"hot\"], [\"mild\"], [\"hot\"]])\n    assert_array_equal(enc.transform([[\"mild\"]]), [[1]])\n    enc = OrdinalEncoder(categories=[[\"cold\", \"mild\", \"hot\"]], dtype=np.int)\n    enc.fit([[\"cold\"], [\"hot\"], [\"mild\"], [\"hot\"]])\n    assert_array_equal(enc.transform([[\"mild\"]]), [[1]])\n    enc = OrdinalEncoder(categories=[[\"cold\", \"mild\", \"hot\"]], dtype=np.float)\n    enc.fit([[\"cold\"], [\"hot\"], [\"mild\"], [\"hot\"]])\n    assert_array_equal(enc.transform([[\"mild\"]]), [[1.]])\n    enc = OrdinalEncoder(drop=\"first\", categories=[[\"cold\", \"mild\", \"hot\"]])\n    enc.fit([[\"cold\"], [\"hot\"], [\"mild\"], [\"hot\"]])\n    assert_array_equal(enc.transform([[\"mild\"]]), [[0]])\n    enc = OrdinalEncoder(drop=\"first\", categories=[[\"cold\", \"mild\", \"hot\"]], dtype=np.int)\n    enc.fit([[\"cold\"], [\"hot\"], [\"mild\"], [\"hot\"]])\n    assert_array_equal(enc.transform([[\"mild\"]]), [[0]])\n    enc = OrdinalEncoder(drop=\"first\", categories=[[\"cold\", \"mild\", \"hot\"]], dtype=np.float)\n    enc.fit([[\"cold\"], [\"hot\"], [\"mild\"], [\"hot\"]])\n    assert_array_equal(enc.transform([[\"mild\"]]), [[0.]])\n", "def test_onehotencoder_drop_and_unknown(sparse, drop, handle_unknown):\n    enc = OneHotEncoder(sparse=sparse, drop=drop, handle_unknown=handle_unknown)\n    X = [[0, 1, 2], [1, 2, 0], [0, 2, 1], [1, 1, 2]]\n    enc.fit(X)\n\n    if handle_unknown == 'error':\n        X_test = [[0, 3, 2], [1, 2, 0]]\n        if drop is None:\n            assert_raises(ValueError, enc.transform, X_test)\n        else:\n            # The unknown category is dropped, so it's equivalent to the first category\n            X_expected = [[0, 0, 1], [1, 0, 1]]\n            assert_array_equal(toarray(enc.transform(X_test)), X_expected)\n    elif handle_unknown == 'ignore':\n        X_test = [[0, 'a', 2], [1, 2, 0]]\n        if drop is None:\n            X_expected = [[1, 0, 0, 0, 1, 0], [0, 1, 0, 1, 0, 0]]\n        else:\n            X_expected = [[0, 0, 0, 1, 0], [0, 0, 1, 0, 0]]\n        assert_array_equal(toarray(enc.transform(X_test)), X_expected)\n"], "sample_946": ["def test_parse_annotation_with_env():\n    env = Mock()\n    env.config.python_use_unqualified_type_names = False\n    annotation = \"List[Union[str, int]]\"\n    result = _parse_annotation(annotation, env)\n    expected = [pending_xref('', 'List', refdomain='py', reftype='class', reftarget='List'),\n                nodes.Text('['),\n                pending_xref('', 'Union', refdomain='py', reftype='class', reftarget='Union'),\n                nodes.Text('['),\n                pending_xref('', 'str', refdomain='py', reftype='class', reftarget='str'),\n                nodes.Text(', '),\n                pending_xref('', 'int', refdomain='py', reftype='class', reftarget='int'),\n                nodes.Text(']'),\n                nodes.Text(']')]\n    assert_node(result, expected)\n", "def test_parse_annotation(app, env):\n    domain = PythonDomain(env)\n    annotation = \"List[Union[int, str]]\"\n    result = _parse_annotation(annotation, env)\n    expected = [\n        nodes.Text('List'),\n        addnodes.desc_sig_punctuation('', '['),\n        nodes.Text('Union'),\n        addnodes.desc_sig_punctuation('', '['),\n        pending_xref('', nodes.Text('int'), refdomain='py', reftype='class', reftarget='int'),\n        addnodes.desc_sig_punctuation('', ', '),\n        pending_xref('', nodes.Text('str'), refdomain='py', reftype='class', reftarget='str'),\n        addnodes.desc_sig_punctuation('', ']'),\n        addnodes.desc_sig_punctuation('', ']')\n    ]\n    assert_node(result, expected)\n", "def test_parse_annotation():\n    env = Mock()\n    text = \"List[int]\"\n    expected = [pending_xref('', nodes.Text('List'), refdomain='py', reftype='class', reftarget='List'),\n                nodes.Text('['),\n                nodes.Text('int'),\n                nodes.Text(']')]\n    assert _parse_annotation(text, env) == expected\n", "def test_parse_annotation(app):\n    env = Mock()\n    env.config.python_use_unqualified_type_names = False\n    result = _parse_annotation(\"List[str]\", env)\n    assert isinstance(result, list)\n    assert len(result) == 5\n    assert isinstance(result[0], nodes.Text) and str(result[0]) == \"List\"\n    assert isinstance(result[1], nodes.Text) and str(result[1]) == \"[\"\n    assert isinstance(result[2], nodes.Text) and str(result[2]) == \"str\"\n    assert isinstance(result[3], nodes.Text) and str(result[3]) == \"]\"\n    assert isinstance(result[4], pending_xref) and str(result[4]) == \"str\"\n", "def test_parse_annotation_with_type_name(app):\n    env = Mock()\n    env.config.python_use_unqualified_type_names = False\n    result = _parse_annotation(\"Optional[str]\", env)\n    assert isinstance(result[0], pending_xref)\n    assert result[0]['reftarget'] == \"Optional[str]\"\n"], "sample_962": ["def test_mock_ismock():\n    with mock(['mock.module.name']):\n        import mock.module.name as mocked_module\n        assert mock.ismock(mocked_module)\n        assert mock.ismock(mocked_module.submodule)\n        assert mock.ismock(mocked_module.submodule.attr)\n        assert not mock.ismock(sys)\n\n    @mock.mock(['decorator.module'])\n        pass\n\n    assert not mock.ismock(decorated_function)\n    assert mock.ismock(decorated_function())\n", "def test_mock_module():\n    with mock(['sphinx.ext.mockmodule']):\n        import sphinx.ext.mockmodule as mm\n        assert mm.__sphinx_mock__ is True\n        assert mm.submodule.__sphinx_mock__ is True\n        assert mm.submodule.SubClass.__sphinx_mock__ is True\n        assert mm.submodule.SubClass().method.__sphinx_mock__ is True\n", "def test_undecorate(obj, expected):\n    from sphinx.ext.autodoc.mock import undecorate\n    assert undecorate(obj) == expected\n", "def test_mock_ismock():\n    with mock(['mocked.module']):\n        import mocked.module\n        assert mock.ismock(mocked.module)\n        assert mock.ismock(mocked.module.submodule)\n        assert mock.ismock(mocked.module.submodule.MockedClass())\n        assert mock.ismock(mocked.module.submodule.MockedClass.mocked_method)\n        assert not mock.ismock(MyClass1())\n\n    assert not mock.ismock(mocked.module)\n", "def test_mock_functionality():\n    with mock(['target.module.name']):\n        import target.module.name\n        assert isinstance(target.module.name, mock._MockModule)\n        assert mock.ismock(target.module.name)\n\n        obj = target.module.name.some_function()\n        assert mock.ismock(obj)\n\n        undecorated_obj = mock.undecorate(obj)\n        assert undecorated_obj == obj  # since obj is not decorated\n"], "sample_1013": ["def test_lambdify_with_indexedbase():\n    i = IndexedBase('i')\n    expr = i[x] + i[y]\n    f = lambdify((x, y, i), expr)\n    assert f(1, 2, {1: 10, 2: 20}) == 30\n", "def test_lambdify_with_functions_in_dictionary():\n        return 'sin(%s) is cool' % x\n    myfuncs = {\"sin\": my_cool_function}\n    f = lambdify(x, sin(x), myfuncs)\n    assert f(1) == 'sin(1) is cool'\n", "def test_lambdify_with_tuple_arguments():\n    f = lambdify((x, (y, z)), x + y)\n    assert f(1, (2, 4)) == 3\n", "def test_lambdify_with_tensorflow():\n    from sympy import Max\n    import tensorflow as tf\n\n    f = Max(x, sin(x))\n    func = lambdify(x, f, 'tensorflow')\n    result = func(tf.constant(1.0))\n    sess = tf.Session()\n    assert sess.run(result) == 1.0\n\n    var = tf.Variable(1.0)\n    sess.run(tf.global_variables_initializer())\n    assert sess.run(func(var)) == 1.0\n\n    tensor = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n    assert sess.run(func(tensor)).all() == [1.0, 2.0, 3.0, 4.0]\n", "def test_lambdify_with_mpmath():\n    expr = sqrt(x) + sin(x) * cos(x) + tan(x)\n    func = lambdify(x, expr, \"mpmath\")\n    assert abs(func(pi) - mpmath.mpf(2.0)) < 1e-15\n"], "sample_459": ["def test_integer_field_exact(self):\n    obj = self.model.objects.create(value=123)\n    self.assertEqual(self.model.objects.get(value__exact=123), obj)\n    with self.assertRaises(self.model.DoesNotExist):\n        self.model.objects.get(value__exact=456)\n", "    def test_backend_range(self):\n        min_value, max_value = self.backend_range\n        self.assertEqual(min_value, self.documented_range[0])\n        self.assertEqual(max_value, self.documented_range[1])\n", "def test_year_exact(self):\n    if not connection.features.supports_date_lookup_using_year:\n        raise SkipTest(\"Database backend does not support year lookup.\")\n    model_instance = self.model.objects.create(value=2022)\n    result = self.model.objects.filter(value__year__exact=2022).exists()\n    self.assertTrue(result)\n", "    def test_integer_field_overflow(self):\n        min_value, max_value = self.backend_range\n        if max_value is not None:\n            obj = self.model.objects.create(value=max_value + 1)\n            with self.assertRaises(self.rel_db_type_class.overflow_exception):\n                obj.save()\n        if min_value is not None:\n            obj = self.model.objects.create(value=min_value - 1)\n            with self.assertRaises(self.rel_db_type_class.underflow_exception):\n                obj.save()\n", "def test_exact_lookup_with_float(self):\n    obj = self.model.objects.create(value=5.7)\n    result = self.model.objects.filter(value__exact=6)\n    self.assertEqual(list(result), [obj])\n"], "sample_527": ["def test_set_dpi():\n    fig = Figure()\n    old_dpi = fig.dpi\n    new_dpi = 200\n    fig.set_dpi(new_dpi)\n    assert fig.dpi == new_dpi\n    assert fig.bbox.size == old_dpi / new_dpi * fig.bbox_inches.size\n", "def test_figure_size_inches():\n    fig = Figure()\n    assert np.array_equal(fig.get_size_inches(), np.array(mpl.rcParams['figure.figsize']))\n    fig.set_size_inches(5, 7)\n    assert np.array_equal(fig.get_size_inches(), np.array([5, 7]))\n    with pytest.raises(ValueError):\n        fig.set_size_inches(np.nan, 7)\n    with pytest.raises(ValueError):\n        fig.set_size_inches(-1, 7)\n", "def test_figure_pick_event():\n    fig = Figure()\n    canvas = FigureCanvasBase(fig)\n    fig.canvas = canvas\n\n    event = MouseEvent('pick_event', canvas, 1, 1, button=MouseButton.LEFT)\n    fig.pick(event)\n    # Add assertions to verify the behavior of fig.pick() in response to the pick_event\n", "def test_tight_layout_h_pad_w_pad():\n    fig, ax = plt.subplots(2, 2)\n    fig.tight_layout(pad=1.0, h_pad=2.0, w_pad=3.0)\n    plt.close(fig)\n", "def test_canvas_manager():\n    fig = Figure()\n    canvas = FigureCanvasBase(fig)\n    manager = canvas.manager\n    assert manager is None, \"Canvas manager should be None initially\"\n\n    # Simulate a backend creating a manager\n    mock_manager = object()\n    canvas.manager = mock_manager\n    assert canvas.manager is mock_manager, \"Canvas manager should be set correctly\"\n\n    # Simulate a backend setting the manager to None\n    canvas.manager = None\n    assert canvas.manager is None, \"Canvas manager should be None after being set to None\"\n"], "sample_786": ["def test_KBinsDiscretizer_encode_n_bins(strategy, n_bins, encode, expected):\n    est = KBinsDiscretizer(n_bins=n_bins, encode=encode, strategy=strategy)\n    est.fit(X)\n    Xt = est.transform(X)\n", "def test_bin_edges_with_constant_feature():\n    X_constant = [[2, 2, 2, 2], [2, 2, 2, 2], [2, 2, 2, 2], [2, 2, 2, 2]]\n    est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n    with assert_warns_message(UserWarning, \"Feature 0 is constant and will be replaced with 0.\"):\n        est.fit(X_constant)\n    assert_array_equal(est.bin_edges_[0], np.array([-np.inf, np.inf]))\n    assert_array_equal(est.n_bins_, np.array([1, 1, 1, 1]))\n", "def test_encode(encode, expected):\n    est = KBinsDiscretizer(n_bins=3, encode=encode, strategy='uniform')\n    est.fit(X)\n    Xt = est.transform(X)\n    assert_array_equal(Xt.toarray(), expected)\n", "def test_kmeans_strategy_with_duplicates(strategy='kmeans'):\n    # Test the kmeans strategy when there are duplicates in the data\n    X_with_duplicates = [[-2, 1.5, -4, -1],\n                         [-1, 2.5, -3, -0.5],\n                         [0, 3.5, -2, 0.5],\n                         [1, 4.5, -1, 2],\n                         [1, 4.5, -1, 2]]  # duplicate\n    est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy=strategy)\n    est.fit(X_with_duplicates)\n    Xt = est.transform(X_with_duplicates)\n    expected = [[0, 0, 0, 0],\n                [0, 0, 0, 0],\n                [1, 1, 1, 1],\n                [2, 2, 2, 2],\n                [2, 2, 2, 2]]  # duplicate\n    assert_array_equal(Xt, expected)\n", "def test_inverse_transform(strategy, expected):\n    est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy=strategy)\n    est.fit(X)\n    Xt = est.transform(X)\n    Xinv = est.inverse_transform(Xt)\n    assert np.allclose(X, Xinv)\n"], "sample_387": ["def test_formfield_for_uuid_field(self):\n    \"\"\"\n    Test that formfield_for_dbfield returns UUIDField with a TextInput widget.\n    \"\"\"\n    self.assertFormfield(MyFileField, \"uuid_field\", forms.TextInput)\n", "def test_formfield_for_charfield_with_choices(self):\n    \"\"\"\n    Test formfield_for_dbfield for a CharField with choices.\n    \"\"\"\n    self.assertFormfield(\n        Car,\n        \"color\",\n        widgets.AdminRadioSelect,\n        radio_fields={\"color\": admin.VERTICAL},\n    )\n", "    def test_manytomany_field(self):\n        self.assertFormfield(Car, 'owners', widgets.ManyToManyRawIdWidget)\n", "def test_formfield_for_dbfield_manytomanyfield(self):\n    self.assertFormfield(\n        Band,\n        'members',\n        widgets.FilteredSelectMultiple,\n        filter_horizontal=['members'],\n    )\n", "def test_formfield_for_foreignkey(self):\n    \"\"\"\n    Test formfield_for_foreignkey with different combinations of widgets and querysets.\n    \"\"\"\n    self.assertFormfield(Car, 'owner', widgets.ForeignKeyRawIdWidget)\n\n    # Test with autocomplete_fields\n    self.assertFormfield(Car, 'owner', widgets.AutocompleteSelect, autocomplete_fields=('owner',))\n\n    # Test with raw_id_fields\n    self.assertFormfield(Car, 'owner', widgets.ForeignKeyRawIdWidget, raw_id_fields=('owner',))\n\n    # Test with radio_fields\n    self.assertFormfield(Car, 'owner', widgets.AdminRadioSelect, radio_fields={'owner': admin.HORIZONTAL})\n\n    # Test with custom queryset\n    queryset = User.objects.filter(username__startswith='test')\n    self.assertFormfield(Car, 'owner', widgets.AutocompleteSelect, get_field_queryset=lambda db, field, request: queryset)\n"], "sample_669": ["    def test_tee_sys_capture_method(self, method=\"tee-sys\"):\n        capman = CaptureManager(method)\n        cap = _get_multicapture(method)\n        assert isinstance(cap, MultiCapture)\n        assert isinstance(cap.out, capture.TeeSysCapture)\n        assert isinstance(cap.err, capture.TeeSysCapture)\n        assert cap.in_ is None\n", "def test_global_capture(method):\n    manager = CaptureManager(method)\n    manager.start_global_capturing()\n    assert manager.is_globally_capturing() == (method != \"no\")\n    assert manager.is_capturing() == (method != \"no\")\n    manager.stop_global_capturing()\n    assert not manager.is_globally_capturing()\n    assert not manager.is_capturing()\n", "def test_suspend_resume_global_capture(method):\n    capman = CaptureManager(method)\n    capman.start_global_capturing()\n    assert capman.is_globally_capturing()\n    capman.suspend_global_capture()\n    assert not capman.is_globally_capturing()\n    capman.resume_global_capture()\n    assert capman.is_globally_capturing()\n    capman.stop_global_capturing()\n", "def test_activate_deactivate_fixture(self, method):\n    capman = CaptureManager(method)\n    capman._capture_fixture = capture.CaptureFixture(capture.SysCapture, None)\n    capman.activate_fixture()\n    # Add assertions to verify that the fixture is activated\n    # For example, check that the capture object is created and started\n    assert capman._capture_fixture._capture is not None\n    assert capman._capture_fixture._capture._state == \"started\"\n    capman.deactivate_fixture()\n    # Add assertions to verify that the fixture is deactivated\n    # For example, check that the capture object is closed\n    assert capman._capture_fixture._capture._state == \"done\"\n", "def test_capture_manager_with_tee_sys(method=\"tee-sys\"):\n    capture_manager = CaptureManager(method)\n    capture_manager.start_global_capturing()\n    sys.stdout.write(\"stdout message\")\n    sys.stderr.write(\"stderr message\")\n    capture_manager.stop_global_capturing()\n    out, err = capture_manager.read_global_capture()\n    assert \"stdout message\" in out\n    assert \"stderr message\" in err\n"], "sample_27": ["    def test_diff_ignore_blanks(self):\n        a = Header()\n        a['KEY'] = 'value  '\n        b = Header()\n        b['KEY'] = 'value'\n\n        diff = HeaderDiff(a, b, ignore_blanks=True)\n        assert diff.identical\n\n        diff = HeaderDiff(a, b, ignore_blanks=False)\n        assert not diff.identical\n", "def test_header_diff_with_comment_differences(self):\n    header1 = Header([(\"KEY1\", \"value1\", \"comment1\"), (\"KEY2\", \"value2\", \"comment2\")])\n    header2 = Header([(\"KEY1\", \"value1\", \"comment3\"), (\"KEY2\", \"value2\", \"comment2\")])\n    diff = HeaderDiff(header1, header2)\n    assert not diff.identical\n    assert \"KEY1\" in diff.diff_keyword_comments\n", "def test_table_data_diff(self):\n    col1 = Column(name='A', format='I', array=np.array([1, 2, 3]))\n    col2 = Column(name='B', format='E', array=np.array([1.0, 2.0, 3.0]), unit='m')\n    col3 = Column(name='A', format='I', array=np.array([1, 2, 4]))\n    col4 = Column(name='B', format='E', array=np.array([1.0, 2.1, 3.0]), unit='m')\n\n    hdu1 = BinTableHDU.from_columns([col1, col2])\n    hdu2 = BinTableHDU.from_columns([col3, col4])\n\n    diff = TableDataDiff(hdu1.data, hdu2.data)\n    assert not diff.identical\n    assert diff.diff_total == 2\n    assert diff.diff_values == [(('A', 2), (3, 4)), (('B', 1), (2.0, 2.1))]\n    assert diff.diff_column_attributes == [(('B', 'units'), ('', 'm'))]\n", "def test_table_data_diff_with_ignore_fields(self):\n    # Create two tables with different values in a column to be ignored\n    col1 = Column(name='COL1', format='I', array=np.array([1, 2, 3]))\n    col2 = Column(name='COL2', format='I', array=np.array([4, 5, 6]))\n    col3 = Column(name='COL3', format='I', array=np.array([7, 8, 9]))\n\n    data1 = np.array([(1, 4, 7)], dtype=[('COL1', 'i4'), ('COL2', 'i4'), ('COL3', 'i4')])\n    data2 = np.array([(1, 10, 7)], dtype=[('COL1', 'i4'), ('COL2', 'i4'), ('COL3', 'i4')])\n\n    hdu1 = BinTableHDU(data=data1)\n    hdu2 = BinTableHDU(data=data2)\n\n    # Compare tables with COL2 in ignore_fields\n    diff = TableDataDiff(hdu1.data, hdu2.data, ignore_fields=['COL2'])\n\n    # The diff_values should be empty as COL2 is ignored\n    assert len(diff.diff_values) == 0\n", "    def test_diff_raw_data(self):\n        hdu1 = DummyNonstandardExtHDU(data=[1, 2, 3])\n        hdu2 = DummyNonstandardExtHDU(data=[1, 2, 4])\n        diff = HDUDiff(hdu1, hdu2)\n        assert diff.diff_data is not None\n        assert diff.diff_data.diff_total == 1\n        assert diff.diff_data.diff_bytes == [(2, (3, 4))]\n"], "sample_673": ["def test_doctest_namespace_fixture(doctest_namespace):\n    assert isinstance(doctest_namespace, dict)\n    assert len(doctest_namespace) == 0\n", "def test_is_setup_py(tmpdir):\n    setup_py = tmpdir.join(\"setup.py\")\n    setup_py.write(\"from setuptools import setup\")\n    assert _is_setup_py(setup_py) is True\n\n    setup_py = tmpdir.join(\"setup.py\")\n    setup_py.write(\"from distutils.core import setup\")\n    assert _is_setup_py(setup_py) is True\n\n    setup_py = tmpdir.join(\"setup.py\")\n    setup_py.write(\"print('This is not a setup.py')\")\n    assert _is_setup_py(setup_py) is False\n", "def test_get_checker(mocker):\n    mocker.patch('doctest.register_optionflag')\n    checker = _get_checker()\n    assert checker is not None\n", "def test__is_setup_py():\n    from py.path import local\n\n    # Test with a non-setup.py file\n    path = local(__file__).dirpath(\"../example.py\")\n    assert not _is_setup_py(path)\n\n    # Test with a setup.py file containing setuptools\n    path = local(__file__).dirpath(\"../setup_setuptools.py\")\n    assert _is_setup_py(path)\n\n    # Test with a setup.py file containing distutils\n    path = local(__file__).dirpath(\"../setup_distutils.py\")\n    assert _is_setup_py(path)\n\n    # Test with a setup.py file containing neither setuptools nor distutils\n    path = local(__file__).dirpath(\"../setup_empty.py\")\n    assert not _is_setup_py(path)\n", "def test_output_checker(input_value, expected_result):\n    checker = _get_checker()\n    assert checker.check_output(expected_result, input_value, _get_checker()._get_allow_unicode_flag() | _get_checker()._get_allow_bytes_flag() | _get_checker()._get_number_flag())\n"], "sample_710": ["    def test_collect_unittest_testcase(self, pytester: Pytester) -> None:\n        pytester.makepyfile(\n            \"\"\"\n            import unittest\n\n            class MyTest(unittest.TestCase):\n                    self.assertEqual(1, 1)\n\n                    self.assertEqual(2, 2)\n            \"\"\"\n        )\n        result = pytester.runpytest(\"-v\")\n        result.stdout.fnmatch_lines(\n            [\n                \"*::test_one PASSED*\",\n                \"*::test_two PASSED*\",\n            ]\n        )\n", "def test_unittest_collection(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        class TestClass(unittest.TestCase):\n                pass\n\n            @unittest.skip(\"Skipping this test\")\n                pass\n        \"\"\"\n    )\n\n    result = pytester.runpytest(\"-vv\")\n    result.stdout.fnmatch_lines(\n        [\n            \"test_method PASSED\",\n            \"test_skipped_method SKIPPED\",\n        ]\n    )\n    assert result.ret == ExitCode.OK\n", "def test_pytest_pycollect_makeitem(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        test_unittest_import=r\"\"\"\n        import unittest\n        import pytest\n\n        class TestMyTest(unittest.TestCase):\n                assert True\n\n            assert True\n        \"\"\"\n    )\n\n    result = pytester.runpytest(\"-v\", \"--collect-only\")\n\n    result.stdout.fnmatch_lines([\n        \"*::TestMyTest*\",\n        \"*::test_not_unittest_testcase*\",\n    ])\n    assert \"::test_something\" in result.stdout.str()\n    assert \"::test_not_unittest_testcase\" in result.stdout.str()\n    assert \"::TestMyTest\" not in result.stdout.str()\n", "def test_unit_test_case(pytester: Pytester, monkeypatch: MonkeyPatch):\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        class MyTestCase(unittest.TestCase):\n            @classmethod\n                cls.shared_resource = \"Resource for all tests\"\n\n            @classmethod\n                cls.shared_resource = None\n\n                self.resource = \"Resource for this test\"\n\n                self.resource = None\n\n                self.assertEqual(self.shared_resource, \"Resource for all tests\")\n                self.assertEqual(self.resource, \"Resource for this test\")\n\n                pass\n        \"\"\"\n    )\n\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=2)\n    assert \"setUpClass\" in result.stdout.str()\n    assert \"tearDownClass\" in result.stdout.str()\n    assert \"setUp\" in result.stdout.str()\n    assert \"tearDown\" in result.stdout.str()\n\n    # Check that resources are cleaned up\n    assert gc.collect() == 0\n\n    # Check that the test case is collected by pytest\n    assert \"MyTestCase\" in result.stdout.str()\n    assert \"test_something\" in result.stdout.str()\n    assert \"test_something_else\" in result.stdout.str()\n\n    # Monkeypatch sys.modules to simulate that unittest is not imported\n    with monkeypatch.context() as m:\n        m.delitem(\"unittest\")\n        result = pytester.runpytest()\n        result.assert_outcomes(skipped=2)\n", "def test_pytest_runtest_protocol(pytester: Pytester, monkeypatch: MonkeyPatch) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n        from unittest.mock import Mock\n        import pytest\n\n        class TestMyTestCase(unittest.TestCase):\n                self.fail('This test should fail')\n\n            if isinstance(item, TestCaseFunction) and 'twisted.trial.unittest' in sys.modules:\n                ut = sys.modules['twisted.python.failure']\n                ut.Failure = Mock()\n\n        pytest.main()\n        \"\"\"\n    )\n\n    result = pytester.runpytest()\n\n    result.stdout.fnmatch_lines(\n        [\n            \"FAILED test_pytest_runtest_protocol.py::TestMyTestCase::test_something - This test should fail\"\n        ]\n    )\n    assert result.ret == ExitCode.TESTS_FAILED\n\n    # Check that Failure.__init__ was mocked during the test\n    ut = sys.modules['twisted.python.failure']\n    assert ut.Failure.__init__.called\n"], "sample_834": ["def test_nca_initialization():\n    nca = NeighborhoodComponentsAnalysis(n_components=2, init='auto')\n    nca.fit(iris_data, iris_target)\n    assert nca.components_.shape == (2, iris_data.shape[1])\n\n    nca = NeighborhoodComponentsAnalysis(n_components=2, init='pca')\n    nca.fit(iris_data, iris_target)\n    assert nca.components_.shape == (2, iris_data.shape[1])\n\n    nca = NeighborhoodComponentsAnalysis(n_components=2, init='lda')\n    nca.fit(iris_data, iris_target)\n    assert nca.components_.shape == (2, iris_data.shape[1])\n\n    nca = NeighborhoodComponentsAnalysis(n_components=2, init='identity')\n    nca.fit(iris_data, iris_target)\n    assert nca.components_.shape == (2, iris_data.shape[1])\n\n    nca = NeighborhoodComponentsAnalysis(n_components=2, init='random')\n    nca.fit(iris_data, iris_target)\n    assert nca.components_.shape == (2, iris_data.shape[1])\n", "def test_nca_with_predefined_init():\n    nca = NeighborhoodComponentsAnalysis(n_components=2, init=np.eye(2, 4), max_iter=1)\n    nca.fit(iris_data, iris_target)\n    assert_array_equal(nca.components_, np.eye(2, 4))\n", "def test_nca_transform():\n    nca = NeighborhoodComponentsAnalysis(n_components=2, random_state=0)\n    nca.fit(iris_data, iris_target)\n    X_transformed = nca.transform(iris_data)\n    assert X_transformed.shape == (iris_data.shape[0], 2)\n", "def test_n_components(n_components):\n    nca = NeighborhoodComponentsAnalysis(n_components=n_components, random_state=0)\n    nca.fit(iris_data, iris_target)\n    if n_components is None:\n        assert nca.components_.shape == (iris_data.shape[1], iris_data.shape[1])\n    else:\n        assert nca.components_.shape == (n_components, iris_data.shape[1])\n", "def test_nca_warm_start():\n    nca1 = NeighborhoodComponentsAnalysis(n_components=2, warm_start=True)\n    nca2 = NeighborhoodComponentsAnalysis(n_components=2, warm_start=True)\n\n    nca1.fit(iris_data[:100], iris_target[:100])\n    nca2.fit(iris_data[100:], iris_target[100:])\n\n    assert_array_equal(nca1.components_, nca2.components_)\n"], "sample_678": ["def test_fnmatch_ex(self, match, pattern, path):\n    assert match(pattern, path)\n", "def test_ensure_deletable(self, tmp_path):\n    path = tmp_path / \"test_dir\"\n    path.mkdir()\n    lock_path = get_lock_path(path)\n    lock_path.touch()\n    assert not ensure_deletable(path, float(\"inf\"))  # lock exists and is not considered dead\n    os.utime(str(lock_path), (0, 0))  # set mtime to a past time\n    assert ensure_deletable(path, 1)  # lock exists but is considered dead\n    lock_path.unlink()\n    assert ensure_deletable(path, float(\"inf\"))  # lock does not exist\n", "def test_fnmatch_ex_double_star(self, match, pattern, path):\n    assert match(pattern, path)\n", "def test_fnmatch_ex(self, match, pattern, path):\n    assert match(pattern, path)\n", "def test_fnmatch_ex_with_double_star(self, match, pattern, path):\n    assert match(pattern, path)\n"], "sample_635": ["def test_numpy_docstring_param_docs_with_description_only(self):\n    node = astroid.extract_node(\n        \"\"\"\n            '''\n            Parameters\n            ----------\n            param1\n                Description for param1.\n            param2\n                Description for param2.\n            param3\n                Description for param3.\n            '''\n            pass\n        \"\"\"\n    )\n\n    with self.assertAddsMessages(\n        MessageTest(\"missing-param-doc\", node=node, args=(\"param1\",)),\n        MessageTest(\"missing-param-doc\", node=node, args=(\"param2\",)),\n        MessageTest(\"missing-param-doc\", node=node, args=(\"param3\",)),\n    ):\n        self.checker.process_tokens(node.body[0])\n", "def test_numpy_docstring(self):\n    \"\"\"Test match_param_docs for NumpyDocstring.\"\"\"\n    docstring = \"\"\"\n    Parameters\n    ----------\n    arg1 : int, optional\n        Description for arg1.\n    arg2 : str\n        Description for arg2.\n    arg3\n        Description for arg3 without type.\n\n    Returns\n    -------\n    str\n        Description for the return value.\n\n    Yields\n    ------\n    int\n        Description for the yielded value.\n    \"\"\"\n    checker = self.checker\n    checker.docstring = NumpyDocstring(docstring)\n    params_with_doc, params_with_type = checker.docstring.match_param_docs()\n    assert params_with_doc == {\"arg1\", \"arg2\", \"arg3\"}\n    assert params_with_type == {\"arg1\", \"arg2\"}\n", "def test_check_property_return_type(self):\n    \"\"\"Test the check for property return type in docstring.\"\"\"\n    node = astroid.extract_node(\n        \"\"\"\n        class MyClass:\n            @property\n                '''A property.\n\n                :rtype: int\n                '''\n                return 42\n        \"\"\"\n    )\n    with set_config_directly(**self.CONFIG):\n        self.checker.process_tokens(node.parent.parent)\n    self.assertNoMessages()\n", "def test_numpy_docstring_match_param_docs(self):\n    \"\"\"Test the match_param_docs method of the NumpyDocstring class\"\"\"\n    docstring = \"\"\"\n    Args:\n        arg1 (int): Description of arg1.\n        arg2 (str): Description of arg2.\n    \"\"\"\n    parsed = DocstringParameterChecker.docstringify(docstring, \"numpy\")\n    params_with_doc, params_with_type = parsed.match_param_docs()\n    assert params_with_doc == {\"arg1\", \"arg2\"}\n    assert params_with_type == {\"arg1\", \"arg2\"}\n", "def test_check_numpy_docstring_parameters(self):\n    \"\"\"Test that numpy docstring parameters are checked correctly.\"\"\"\n    node = astroid.extract_node(\n        \"\"\"\n            \"\"\"\n            Parameters\n            ----------\n            x : int\n                The first parameter.\n            y : float\n                The second parameter.\n            z\n                The third parameter.\n            \"\"\"\n            pass\n        \"\"\"\n    )\n    with self.assertAddsMessages(\n        MessageTest(\n            msg_id=\"W0231\",\n            line=7,\n            node=node.body[0].body[0].value,\n            args=(\"z\", \"z\"),\n        )\n    ):\n        self.checker.process_tokens(node.body[0].body[0].value)\n"], "sample_1156": ["def test_asech_inverse():\n    x = Symbol('x')\n    assert asech(sech(x)) == x\n", "def test_asech_inverse():\n    x = symbols('x')\n    assert asech(asech(x)) == x\n", "def test_asech_rewrite_as_log():\n    x = symbols('x', real=True)\n    assert asech(x)._eval_rewrite_as_log(x) == log(2 / x + sqrt(4 / x - 1) * sqrt(4 / x + 1))\n", "def test_tanh_as_leading_term():\n    x = symbols('x')\n    assert tanh(x)._eval_as_leading_term(x) == x\n    assert tanh(2*x)._eval_as_leading_term(x) == 2*x\n    assert tanh(1/x)._eval_as_leading_term(x) == O(1/x)\n", "def test_asech_eval():\n    assert asech(sech(pi/3)) == S(2)\n"], "sample_741": ["def test_randomized_search_cv_predict():\n    X, y = make_classification(n_samples=100, random_state=0)\n    param_distributions = {'C': expon(scale=100)}\n    clf = RandomizedSearchCV(LinearSVC(), param_distributions, n_iter=5, random_state=0)\n    clf.fit(X, y)\n    assert_array_equal(clf.predict(X), clf.best_estimator_.predict(X))\n", "def test_grid_search_cv_parameter_distribution_errors():\n    param_distributions = {'param1': 'string', 'param2': np.array([1, 2, 3, 4, 5, 6]).reshape(2, 3)}\n    assert_raise_message(ValueError, \"Parameter array should be one-dimensional.\", GridSearchCV, estimator=SVC(), param_grid=param_distributions)\n\n    param_distributions = {'param1': np.array([1, 2, 3]), 'param2': ['a', 'b', 'c', {}]}\n    assert_raise_message(ValueError, \"Parameter values for parameter \\\\(param2\\\\) need to be a sequence\\\\(but not a string\\\\) or np.ndarray.\", GridSearchCV, estimator=SVC(), param_grid=param_distributions)\n\n    param_distributions = {'param1': [1, 2, 3], 'param2': []}\n    assert_raise_message(ValueError, \"Parameter values for parameter \\\\(param2\\\\) need to be a non-empty sequence.\", GridSearchCV, estimator=SVC(), param_grid=param_distributions)\n", "def test_grid_search_cv_with_transform():\n    estimator = MockClassifier()\n    param_grid = {'foo_param': [1, 2, 3]}\n    grid_search = GridSearchCV(estimator, param_grid, cv=2)\n    grid_search.fit(X, y)\n\n    # Check if transform and inverse_transform are available\n    assert hasattr(grid_search, 'transform')\n    assert hasattr(grid_search, 'inverse_transform')\n\n    # Check if transform and inverse_transform work as expected\n    X_transformed = grid_search.transform(X)\n    X_inverse_transformed = grid_search.inverse_transform(X_transformed)\n    assert_array_equal(X, X_inverse_transformed)\n", "def test_randomizedsearchcv_refit_value():\n    X, y = make_classification(n_samples=20, n_features=10, random_state=42)\n    clf = RandomizedSearchCV(LinearSVC(), {'C': [1, 2]}, refit='invalid')\n    assert_raise_message(ValueError, \"For multi-metric scoring, the parameter\",\n                         clf.fit, X, y)\n", "def test_fit_grid_point_with_refit_false_estimator():\n    estimator = SVC(kernel='linear', C=1, random_state=0)\n    parameters = {'kernel': 'rbf', 'C': 10}\n    train, test = next(iter(KFold(n_splits=2).split(X, y)))\n    scores, parameters, n_samples_test = fit_grid_point(\n        X, y, estimator, parameters, train, test, None, verbose=0, error_score='raise')\n    assert_false(hasattr(estimator, \"best_estimator_\"))\n    assert_array_equal(scores, estimator.score(X[test], y[test]))\n"], "sample_434": ["    def setUp(self):\n        self.factory = RequestFactory()\n", "def test_template_view(self):\n    request = RequestFactory().get('/')\n\n    class TestTemplateView(TemplateView):\n        template_name = \"test_template.html\"\n\n            context = super().get_context_data(**kwargs)\n            context['test_var'] = 'test_value'\n            return context\n\n    view = TestTemplateView.as_view()\n    response = view(request)\n\n    self.assertEqual(response.status_code, 200)\n    self.assertIn('test_var', response.context_data)\n    self.assertEqual(response.context_data['test_var'], 'test_value')\n", "def test_redirect_view(self):\n    factory = RequestFactory()\n    request = factory.get('/test')\n    view = RedirectView.as_view(url='/redirected')\n\n    response = view(request)\n    self.assertEqual(response.status_code, 302)\n    self.assertEqual(response.url, '/redirected')\n\n    view = RedirectView.as_view(url='/redirected', permanent=True)\n    response = view(request)\n    self.assertEqual(response.status_code, 301)\n    self.assertEqual(response.url, '/redirected')\n\n    view = RedirectView.as_view(pattern_name='some_view')\n    with self.settings(ROOT_URLCONF='tests.urls'):\n        response = view(request, key='value')\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response.url, '/some/url/value/')\n\n    view = RedirectView.as_view()\n    response = view(request)\n    self.assertEqual(response.status_code, 410)\n", "def test_template_view(self):\n    factory = RequestFactory()\n    request = factory.get('/')\n\n    class TestTemplateView(TemplateView):\n        template_name = \"test_template.html\"\n\n            context = super().get_context_data(**kwargs)\n            context['test_key'] = 'test_value'\n            return context\n\n    response = TestTemplateView.as_view()(request)\n    self.assertEqual(response.status_code, 200)\n    self.assertContains(response, 'test_value')\n", "    def setUp(self):\n        self.factory = RequestFactory()\n"], "sample_529": ["def test_legend_bbox_transform():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3], [1, 2, 3], label='line')\n    trans = mtransforms.Affine2D().translate(0.5, 0.5)\n    legend = ax.legend(bbox_to_anchor=(0.5, 0.5), bbox_transform=trans)\n    assert isinstance(legend._bbox_to_anchor.transform, mtransforms.TransformedBbox)\n    plt.close(fig)\n", "def test_legend_default_loc():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3], label=\"Test line\")\n    ax.legend()\n", "def test_legend_set_alignment():\n    fig, ax = plt.subplots()\n    line, = ax.plot([1, 2, 3], label='Test line')\n    leg = ax.legend()\n\n    # Test setting alignment to 'center'\n    leg.set_alignment('center')\n    assert leg.get_alignment() == 'center'\n\n    # Test setting alignment to 'left'\n    leg.set_alignment('left')\n    assert leg.get_alignment() == 'left'\n\n    # Test setting alignment to 'right'\n    leg.set_alignment('right')\n    assert leg.get_alignment() == 'right'\n\n    # Test setting alignment to invalid value\n    with pytest.raises(ValueError):\n        leg.set_alignment('invalid')\n", "def test_draggable_legend():\n    fig, ax = plt.subplots()\n    lines = [ax.plot([1, 2, 3], label=f'Line {i+1}')[0] for i in range(3)]\n    legend = ax.legend(handles=lines, draggable=True)\n    fig.canvas.draw()\n    # Test if legend can be dragged\n    assert legend.get_draggable()\n", "def test_legend_get_legend_handler_map():\n    fig, ax = plt.subplots()\n    legend = ax.legend(handler_map={mpatches.Circle: 'custom_handler'})\n    custom_map = legend.get_legend_handler_map()\n    assert custom_map[mpatches.Circle] == 'custom_handler'\n"], "sample_1145": ["def test_refine_matrixelement_antisymmetric():\n    X = MatrixSymbol('X', 3, 3)\n    assert refine(X[0, 1], Q.antisymmetric(X)) == -X[1, 0]\n    assert refine(X[1, 0], Q.antisymmetric(X)) == -X[0, 1]\n", "def test_refine_matrixelement_antisymmetric():\n    X = MatrixSymbol('X', 3, 3)\n    assert refine(X[0, 1], Q.antisymmetric(X)) == -X[1, 0]\n    assert refine(X[1, 0], Q.antisymmetric(X)) == -X[0, 1]\n", "def test_refine_atan2():\n    assert refine(atan2(0, 0), Q.zero(y) & Q.zero(x)) == nan\n", "def test_refine_matrixelement_asymmetric():\n    X = MatrixSymbol('X', 3, 3)\n    assert refine(X[0, 1], Q.asymmetric(X)) == X[0, 1]\n    assert refine(X[1, 0], Q.asymmetric(X)) == X[1, 0]\n", "def test_refine_matrixelement_skewsymmetric():\n    X = MatrixSymbol('X', 3, 3)\n    assert refine(X[0, 1], Q.skew_symmetric(X)) == -X[1, 0]\n    assert refine(X[1, 0], Q.skew_symmetric(X)) == -X[0, 1]\n"], "sample_602": ["def test_default_engine_remote_uri():\n    engine = _get_default_engine(\"http://example.com/data.nc\", allow_remote=True)\n    assert engine in [\"netcdf4\", \"pydap\"]\n", "def test_get_default_engine():\n    assert _get_default_engine(\"/path/to/file.nc\") == \"netcdf4\"\n    assert _get_default_engine(\"/path/to/file.gz\") == \"scipy\"\n    assert _get_default_engine(\"http://example.com/file.nc\", allow_remote=True) in [\"netcdf4\", \"pydap\"]\n", "def test_open_dataset_default_engine():\n    # Test that the default engine is chosen based on available dependencies\n    assert _get_default_engine(\"test.nc\") == \"netcdf4\"\n\n    # Test that the default engine is chosen based on the file extension\n    assert _get_default_engine(\"test.gz\") == \"scipy\"\n", "def test_open_dataset_with_decode_timedelta():\n    ds = xr.Dataset(\n        {\"time\": (\"time\", np.arange(10, dtype=\"timedelta64[s]\"))}\n    )\n    store = ds.to_netcdf()\n    ds_decoded = xr.open_dataset(store, decode_timedelta=True)\n    assert np.issubdtype(ds_decoded[\"time\"].dtype, np.timedelta64)\n    ds_not_decoded = xr.open_dataset(store, decode_timedelta=False)\n    assert np.issubdtype(ds_not_decoded[\"time\"].dtype, np.int64)\n", "def test_open_dataset_decode_coords_all():\n    ds = xr.Dataset(\n        data_vars={\n            \"var\": (\"x\", np.arange(3)),\n        },\n        coords={\n            \"coord\": (\"x\", np.arange(3)),\n            \"grid_mapping\": (\"x\", np.arange(3)),\n            \"bounds\": (\"x\", np.arange(3)),\n            \"other_attr\": (\"x\", np.arange(3)),\n        },\n        attrs={\"global_attr\": \"value\"},\n    )\n    ds.to_netcdf(\"test.nc\")\n\n    ds_decoded = xr.open_dataset(\"test.nc\", decode_coords=\"all\")\n\n    assert \"coord\" in ds_decoded.coords\n    assert \"grid_mapping\" in ds_decoded.coords\n    assert \"bounds\" in ds_decoded.coords\n    assert \"other_attr\" not in ds_decoded.coords\n    assert \"global_attr\" in ds_decoded.attrs\n"], "sample_1161": ["def test_print_MatPow():\n    A = MatrixSymbol('A', 2, 2)\n    expr = MatPow(A, 3)\n    assert sstr(expr) == \"A**3\"\n", "def test_print_Differential():\n    from sympy import symbols, Differential\n    x, y = symbols('x y')\n    dx = Differential(x)\n    dy = Differential(y)\n    assert sstr(dx) == 'd(%s)' % x\n    assert sstr(dy) == 'd(%s)' % y\n", "def test_print_MatPow():\n    A = Matrix([[1, 2], [3, 4]])\n    B = A ** 3\n    assert sstr(B) == \"Matrix([[34, 54], [68, 98]])**3\"\n", "def test_next_unit_test():\n    # Test _print_Sum function with more than one limit\n    expr = Sum(x**y, (x, 1, 10), (y, 1, 5))\n    printer = StrPrinter()\n    assert printer._print_Sum(expr) == \"Sum(x**y, (x, 1, 10), (y, 1, 5))\"\n", "def test_StrPrinter_PolyElement():\n    p = Poly(x + y, x, y)\n    q = p.as_expr()\n    q_str = sstr(q)\n    assert q_str == \"x + y\"\n"], "sample_70": ["def test_sort_method(self):\n    \"\"\"\n    Test the sort method of the Collector class.\n    \"\"\"\n    collector = Collector(using='default')\n    collector.dependencies = {\n        Parent: {Child, RChild},\n        Child: {RChild},\n        RChild: set(),\n    }\n    collector.data = {\n        Parent: [Parent(id=1)],\n        Child: [Child(id=1)],\n        RChild: [RChild(id=1)],\n    }\n\n    collector.sort()\n\n    expected_order = [RChild, Child, Parent]\n    self.assertEqual(list(collector.data), expected_order)\n", "    def test_collector_sort(self):\n        \"\"\"\n        Test sorting of models in the collector.\n        \"\"\"\n        collector = Collector(using=DEFAULT_DB_ALIAS)\n        parent1 = Parent.objects.create()\n        child1 = Child.objects.create(parent=parent1)\n        parent2 = Parent.objects.create()\n        child2 = Child.objects.create(parent=parent2)\n        collector.collect([child1])\n        collector.collect([child2])\n        collector.sort()\n        self.assertEqual(list(collector.data), [Child, Parent])\n", "    def test_sort_method(self):\n        collector = Collector(using=DEFAULT_DB_ALIAS)\n        collector.data = {User: set(), Parent: set(), Child: set()}\n        collector.dependencies = {\n            Parent: {User},\n            Child: {Parent},\n        }\n\n        collector.sort()\n\n        self.assertEqual(list(collector.data.keys()), [User, Parent, Child])\n", "    def test_sort(self):\n        collector = Collector(using=connection.alias)\n        parent1 = Parent.objects.create()\n        child1 = Child.objects.create(parent=parent1)\n        child2 = Child.objects.create(parent=parent1)\n        parent2 = Parent.objects.create()\n        child3 = Child.objects.create(parent=parent2)\n\n        collector.add([child1, child2, parent1, child3, parent2])\n        collector.sort()\n\n        expected_order = [Parent, Child]\n        self.assertEqual(list(collector.data.keys()), expected_order)\n", "def test_delete_with_fast_delete(self):\n    # Create instances of the model\n    objs = [A.objects.create(name='A{}'.format(i)) for i in range(5)]\n\n    # Create a collector instance\n    collector = Collector(using='default')\n\n    # Test the can_fast_delete method\n    self.assertTrue(collector.can_fast_delete(objs))\n\n    # Test the collect method with fast_deletes\n    collector.collect(objs)\n    self.assertEqual(len(collector.fast_deletes), 1)\n    self.assertEqual(len(collector.fast_deletes[0]), 5)\n\n    # Delete the instances\n    deleted_count, deleted_counter = collector.delete()\n\n    # Check the deleted count and counter\n    self.assertEqual(deleted_count, 5)\n    self.assertEqual(deleted_counter, {'app_label.A': 5})\n\n    # Check that the instances have been deleted\n    self.assertEqual(A.objects.count(), 0)\n"], "sample_811": ["def test_pairwise_distances_with_invalid_metric(metric):\n    X = np.random.RandomState(0).rand(5, 3)\n    Y = np.random.RandomState(0).rand(4, 3)\n    assert_raises(ValueError, pairwise_distances, X, Y, metric=metric.upper())\n", "def test_pairwise_distances_chunked():\n    X = np.random.RandomState(0).rand(5, 3)\n    r = .2\n\n        neigh = [np.flatnonzero(d < r) for d in D_chunk]\n        avg_dist = (D_chunk * (D_chunk < r)).mean(axis=1)\n        return neigh, avg_dist\n\n    gen = pairwise_distances_chunked(X, reduce_func=reduce_func)\n    neigh, avg_dist = next(gen)\n    assert_equal(len(neigh), X.shape[0])\n    assert_equal(len(avg_dist), X.shape[0])\n\n    # Test with reduce_func returning a tuple\n        neigh = [np.flatnonzero(d < r) for d in D_chunk]\n        avg_dist = (D_chunk * (D_chunk < r)).mean(axis=1)\n        return (neigh, avg_dist)\n\n    gen = pairwise_distances_chunked(X, reduce_func=reduce_func_tuple)\n    neigh, avg_dist = next(gen)\n    assert_equal(len(neigh), X.shape[0])\n    assert_equal(len(avg_dist), X.shape[0])\n", "def test_pairwise_distances_chunked_reduce_func():\n    r = .2\n\n        neigh = [np.flatnonzero(d < r) for d in D_chunk]\n        avg_dist = (D_chunk * (D_chunk < r)).mean(axis=1)\n        return neigh, avg_dist\n\n    X = np.random.RandomState(0).rand(5, 3)\n    gen = pairwise_distances_chunked(X, reduce_func=reduce_func)\n    neigh, avg_dist = next(gen)\n    assert len(neigh) == len(avg_dist) == X.shape[0]\n", "def test_pairwise_distances_chunked_reduce_func_return_length():\n    X = np.random.RandomState(0).rand(10, 5)\n    Y = np.random.RandomState(1).rand(15, 5)\n\n        return np.zeros(D_chunk.shape[0] + 1)\n\n    with assert_raises_regexp(ValueError,\n                              'Expected same length as input: 3.'):\n        next(pairwise_distances_chunked(X, Y, reduce_func=reduce_func,\n                                        working_memory=0))\n", "def test_pairwise_distances_chunked_with_reduce_func():\n    X = np.random.RandomState(0).rand(5, 3)\n    r = .2\n\n        neigh = [np.flatnonzero(d < r) for d in D_chunk]\n        avg_dist = (D_chunk * (D_chunk < r)).mean(axis=1)\n        return neigh, avg_dist\n\n    gen = pairwise_distances_chunked(X, reduce_func=reduce_func)\n    neigh, avg_dist = next(gen)\n\n    assert_array_equal(neigh, [np.array([0, 3]), np.array([1]), np.array([2]), np.array([0, 3]), np.array([4])])\n    assert_array_almost_equal(avg_dist, np.array([0.039, 0.0, 0.0, 0.039, 0.0]))\n"], "sample_483": ["def test_valid_prepopulated_fields(self):\n    site = AdminSite()\n    ma = ValidPrepopulatedFields(Song, site)\n    errors = ma._check_prepopulated_fields(ma)\n    self.assertEqual(errors, [])\n", "compilation error", "    def test_check_dependencies(self):\n        site = AdminSite()\n        site.register(Song, MyAdmin)\n        errors = checks.run_checks(tags=[\"admin\"])\n        self.assertEqual(len(errors), 0)\n", "    def test_check_autocomplete_fields(self):\n        site = AdminSite()\n        model_admin = ValidAutocompleteFields(Album, site)\n        errors = model_admin.check()\n        self.assertEqual(errors, [])\n", "    def setUpClass(cls):\n        super().setUpClass()\n        site = AdminSite()\n        site.register(Song, ValidAutocompleteFields)\n        site.register(Author)\n        cls.site = site\n"], "sample_10": ["def test_set_table_column(self):\n    self._setup(self.table_types)\n    self.t['c'] = self.d\n    assert_array_equal(self.t['c'], self.d)\n", "def test_table_set_column_with_scalar_broadcasting(self, table_types):\n    self._setup(table_types)\n    self.t['a'] = 5\n    assert_array_equal(self.t['a'], [5, 5, 5])\n", "def test_set_table_column_with_index(self, table_class):\n    self._setup(table_class)\n    self.t[1] = self.d\n    assert np.all(self.t['b'] == self.d)\n    assert self.t.columns[1] is self.d\n", "def test_setting_column_with_different_data_type(self, column_type):\n    self._column_type = column_type\n    t = self._table_type()\n    t['a'] = self.a\n\n    # Test setting column with different data type\n    new_a = self._column_type([1.0, 2.0, 3.0], name='a')\n    t['a'] = new_a\n    assert t['a'].dtype == new_a.dtype\n    assert_array_equal(t['a'], new_a)\n\n    # Test setting column with different data type and different shape\n    with pytest.raises(ValueError):\n        t['a'] = self._column_type([1.0, 2.0], name='a')\n", "def test_setitem_with_broadcasting(self):\n    t = self._table_type()\n    t['a'] = self._column_type([1], name='a')\n    t['b'] = self._column_type([2, 3], name='b')\n\n    # Test broadcasting with a scalar value\n    t['a'] = 10\n    assert_array_equal(t['a'], [10, 10])\n\n    # Test broadcasting with an array\n    t['b'] = [5, 5]\n    assert_array_equal(t['b'], [5, 5])\n\n    # Test broadcasting with a non-matching length array\n    with pytest.raises(ValueError):\n        t['b'] = [5, 5, 5]\n\n    # Test broadcasting with a mismatched data type\n    with pytest.raises(TypeError):\n        t['a'] = 'string'\n"], "sample_717": ["def test_fetch_lfw_pairs_invalid_subset():\n    \"\"\"Test fetch_lfw_pairs with invalid subset\"\"\"\n    assert_raises(ValueError, fetch_lfw_pairs, subset='invalid', data_home=SCIKIT_LEARN_DATA)\n", "def test_fetch_lfw_people_min_faces_per_person():\n    # Test that an exception is raised when min_faces_per_person is too restrictive\n    assert_raises(ValueError, fetch_lfw_people, data_home=LFW_HOME, min_faces_per_person=10)\n", "def test_fetch_lfw_people():\n    # Test fetching the LFW people dataset\n    dataset = fetch_lfw_people(data_home=SCIKIT_LEARN_DATA, resize=1.0)\n\n    # Check the shape of the data\n    assert_equal(dataset.data.shape, (len(FAKE_NAMES) * 4, 250 * 250))\n    assert_equal(dataset.images.shape, (len(FAKE_NAMES) * 4, 250, 250))\n    assert_equal(dataset.target.shape, (len(FAKE_NAMES) * 4,))\n\n    # Check the target names\n    assert_array_equal(dataset.target_names, FAKE_NAMES)\n", "def test_fetch_lfw_people():\n    # Test that fetch_lfw_people returns the correct data shape and type\n    data = fetch_lfw_people(data_home=SCIKIT_LEARN_DATA, color=True)\n    assert_equal(data.data.shape, (13233, 62, 47, 3))\n    assert_equal(data.images.shape, (13233, 62, 47, 3))\n    assert_equal(data.target.shape, (13233,))\n    assert_equal(data.target_names.shape, (5749,))\n    assert isinstance(data.DESCR, str)\n\n    # Test that fetch_lfw_people returns correct data type\n    assert_equal(data.data.dtype, np.float32)\n    assert_equal(data.images.dtype, np.float32)\n    assert_equal(data.target.dtype, np.int64)\n", "def test_fetch_lfw_people_shape():\n    lfw_people = fetch_lfw_people(data_home=SCIKIT_LEARN_DATA, funneled=True, resize=0.5)\n    assert_equal(lfw_people.data.shape[1], 1658)  # 62 * 47\n    assert_equal(lfw_people.images.shape[1], 62)\n    assert_equal(lfw_people.images.shape[2], 47)\n"], "sample_140": ["    def test_sensitive_variables_with_specified_variables(self):\n        @sensitive_variables('password', 'credit_card')\n            pass\n\n        self.assertEqual(test_function.sensitive_variables, ('password', 'credit_card'))\n", "    def test_sensitive_variables_with_specified_variables(self):\n        @sensitive_variables('user', 'password')\n            pass\n\n        self.assertEqual(test_function.sensitive_variables, ('user', 'password'))\n", "    def test_decorator_with_specified_variables(self):\n        @sensitive_variables('user', 'password')\n            pass\n\n        self.assertEqual(test_function.sensitive_variables, ('user', 'password'))\n", "    def test_sensitive_variables(self):\n        @sensitive_variables('user', 'password')\n            pass\n\n        my_function(User(), 'secret')\n        self.assertEqual(my_function.sensitive_variables, ('user', 'password'))\n", "    def test_sensitive_variables(self):\n        @sensitive_variables('password', 'credit_card')\n            password = user.pass_word\n            credit_card = user.credit_card_number\n            return user, password, credit_card\n\n        user = User()\n        user.pass_word = 'secret'\n        user.credit_card_number = '1234-5678-9012-3456'\n        result = my_function(user)\n        self.assertEqual(result[0], user)\n        self.assertEqual(my_function.sensitive_variables, ('password', 'credit_card'))\n"], "sample_971": ["def test_warning_suppressor(app):\n    app.config.suppress_warnings = ['test_warning.subtest_warning']\n    logger = logging.getLogger('test')\n\n    logger.warning('test message', type='test_warning', subtype='subtest_warning')\n    assert app._warncount == 0\n\n    logger.warning('test message', type='other_warning', subtype='subtest_warning')\n    assert app._warncount == 1\n\n    logger.warning('test message', type='test_warning', subtype='other_subtest_warning')\n    assert app._warncount == 2\n", "def test_is_suppressed_warning():\n    assert is_suppressed_warning('ref.python', None, ['ref.python'])\n    assert is_suppressed_warning('ref.python', 'module', ['ref.python.module'])\n    assert is_suppressed_warning('ref.python', 'module', ['ref.python.module', 'ref.python.other'])\n    assert is_suppressed_warning('ref.python', 'module', ['ref.python.*'])\n    assert not is_suppressed_warning('ref.python', 'module', ['ref.python'])\n    assert not is_suppressed_warning('ref.python', 'module', ['ref.python.other'])\n    assert not is_suppressed_warning('ref.python', None, ['ref.python.module'])\n    assert not is_suppressed_warning(None, None, ['ref.python'])\n", "def test_is_suppressed_warning():\n    # Test when type is None\n    assert is_suppressed_warning(None, None, []) is False\n\n    # Test when type matches but subtype does not\n    assert is_suppressed_warning('type1', 'subtype2', ['type1.subtype1']) is False\n\n    # Test when type matches and subtype matches\n    assert is_suppressed_warning('type1', 'subtype1', ['type1.subtype1']) is True\n\n    # Test when type matches and subtype is wildcard\n    assert is_suppressed_warning('type1', 'subtype2', ['type1.*']) is True\n\n    # Test when type matches and there is no subtype\n    assert is_suppressed_warning('type1', None, ['type1']) is True\n\n    # Test when type does not match but subtype does\n    assert is_suppressed_warning('type2', 'subtype1', ['type1.subtype1']) is False\n\n    # Test when type matches but there is no suppression rule for subtype\n    assert is_suppressed_warning('type1', 'subtype2', ['type1.subtype1', 'type1.subtype3']) is False\n", "def test_is_suppressed_warning():\n    assert is_suppressed_warning('ref', 'python', ['ref.python']) is True\n    assert is_suppressed_warning('ref', 'python', ['ref.c']) is False\n    assert is_suppressed_warning('ref', None, ['ref']) is True\n    assert is_suppressed_warning('ref', None, ['ref.c']) is False\n    assert is_suppressed_warning('ref', 'python', ['ref.python', 'ref.c']) is True\n    assert is_suppressed_warning('ref', 'python', ['ref.*']) is True\n    assert is_suppressed_warning(None, None, ['ref.python']) is False\n", "def test_warning_suppressor(app):\n    logger = logging.getLogger('test_logger')\n    suppressor = logging.WarningSuppressor(app)\n\n    app.config.suppress_warnings = ['test_type.test_subtype', 'another_type']\n    record = logging.LogRecord('test_logger', logging.WARNING, 'pathname', 1, 'test_msg', (), None, 'test_type', 'test_subtype')\n    assert suppressor.filter(record) is False\n\n    record = logging.LogRecord('test_logger', logging.WARNING, 'pathname', 1, 'test_msg', (), None, 'another_type', None)\n    assert suppressor.filter(record) is False\n\n    record = logging.LogRecord('test_logger', logging.WARNING, 'pathname', 1, 'test_msg', (), None, 'other_type', None)\n    assert suppressor.filter(record) is True\n\n    app.config.suppress_warnings = ['test_type.*']\n    record = logging.LogRecord('test_logger', logging.WARNING, 'pathname', 1, 'test_msg', (), None, 'test_type', 'other_subtype')\n    assert suppressor.filter(record) is False\n\n    record = logging.LogRecord('test_logger', logging.WARNING, 'pathname', 1, 'test_msg', (), None, 'other_type', None)\n    assert suppressor.filter(record) is True\n"], "sample_382": ["    def test_watch_for_template_changes(self):\n        watch_for_template_changes(autoreload)\n        self.assertTrue(autoreload.watch_dir.called_once_with(EXTRA_TEMPLATES_DIR, '**/*'))\n", "def test_template_changed_with_non_py_file(self, mock_file_changed, mock_reset_loaders):\n    file_path = EXTRA_TEMPLATES_DIR / \"test_template.html\"\n    file_path.parent.mkdir(parents=True, exist_ok=True)\n    file_path.touch()\n\n    template_changed(None, file_path)\n\n    mock_reset_loaders.assert_called_once()\n    mock_file_changed.assert_called_once_with(file_path)\n\n    file_path.unlink()\n", "    def test_template_changed(self):\n        file_path = EXTRA_TEMPLATES_DIR / 'test_template.html'\n        with mock.patch('template_tests.template_loaders.get_template_directories') as mock_dirs:\n            mock_dirs.return_value = [EXTRA_TEMPLATES_DIR]\n            template_changed(None, file_path)\n            self.reset_loaders.assert_called_once()\n", "def test_template_changed(self, mock_file_changed, mock_reset_loaders):\n    file_path = EXTRA_TEMPLATES_DIR / 'test_template.html'\n    file_path.parent.mkdir(parents=True, exist_ok=True)\n    file_path.touch()\n\n    autoreload.template_changed(None, file_path)\n\n    mock_file_changed.assert_called_once_with(None, file_path)\n    mock_reset_loaders.assert_called_once()\n\n    file_path.unlink()\n", "    def test_watch_for_template_changes(self):\n        sender = mock.MagicMock()\n        autoreload.watch_for_template_changes(sender, **{})\n        sender.watch_dir.assert_any_call(EXTRA_TEMPLATES_DIR, '**/*')\n"], "sample_642": ["def test_convert_option_to_argument(optdict, expected_type):\n    result = _convert_option_to_argument(\"test_option\", optdict)\n    assert isinstance(result, expected_type)\n", "def test_preprocess_options(capsys: CaptureFixture[str], option: str, value: str | None, expected: str | None) -> None:\n    \"\"\"Test the _preprocess_options function.\"\"\"\n    run = Run([\"test.py\"])\n    args = [option]\n    if value is not None:\n        args.append(value)\n    processed_args = config._preprocess_options(run, args)\n    if \"--init-hook\" in option:\n        captured = capsys.readouterr()\n        assert captured.out == expected\n    elif \"--rcfile\" in option:\n        assert run._rcfile == expected\n    elif \"--output\" in option:\n        assert run._output == expected\n    elif \"--load-plugins\" in option:\n        assert run._plugins == expected\n    elif \"--verbose\" in option or \"-v\" in option:\n        assert run.verbose == expected\n    elif \"--enable-all-extensions\" in option:\n        assert all(item in run._plugins for item in expected)\n    else:\n        assert processed_args == [option]\n", "def test_preprocess_options(capsys: CaptureFixture[str]) -> None:\n    \"\"\"Test the _preprocess_options function.\"\"\"\n    with tempfile.TemporaryDirectory() as tmpdir:\n        init_hook_path = Path(tmpdir) / \"init_hook.py\"\n        init_hook_path.write_text(\"print('init_hook_called')\")\n        run = Run([\"--init-hook\", str(init_hook_path), \"--rcfile\", \"test.rc\", \"--output\", \"test.out\",\n                   \"--load-plugins\", \"test_plugin\", \"--enable-all-extensions\"])\n        processed_args = config._preprocess_options(run, [])\n        captured = capsys.readouterr()\n        assert run._rcfile == \"test.rc\"\n        assert run._output == \"test.out\"\n        assert \"test_plugin\" in run._plugins\n        assert len(processed_args) == 0\n        assert \"init_hook_called\" in captured.out\n        extensions_path = Path(importlib.util.find_spec(\"pylint.extensions\").origin).parent\n        for filename in extensions_path.iterdir():\n            if filename.suffix == \".py\" and not filename.stem.startswith(\"_\"):\n                extension_name = f\"pylint.extensions.{filename.stem}\"\n                assert extension_name in run._plugins\n", "def test_add_plugins(args, expected_plugins):\n    with tempdir():\n        run = Run(args, exit=False)\n        config.load_config_file()\n        assert run._plugins == expected_plugins\n", "def test_preprocess_options_enable_all_extensions(capsys: CaptureFixture[str]) -> None:\n    \"\"\"Test _preprocess_options with --enable-all-extensions\"\"\"\n    with tempfile.TemporaryDirectory() as tmpdirname:\n        shutil.copytree(str(Path(config.__file__).parent / 'extensions'), str(Path(tmpdirname) / 'extensions'))\n        sys.path.insert(0, tmpdirname)\n        try:\n            run = Run([\"--enable-all-extensions\"], exit=False)\n            config.load_configuration(run)\n            captured = capsys.readouterr()\n            assert \"--enable=all\" in captured.out\n        finally:\n            sys.path.remove(tmpdirname)\n"], "sample_420": ["def test_modelform_factory_with_exclude(self):\n    form_class = modelform_factory(\n        Writer,\n        form=forms.ModelForm,\n        exclude=[\"name\"],\n    )\n    form = form_class()\n    self.assertNotIn(\"name\", form.fields)\n    self.assertIn(\"writer_profile\", form.fields)\n", "    def test_custom_error_message_form_clean(self):\n        form = CustomErrorMessageForm(data={\"name1\": \"invalid value\"})\n        self.assertFalse(form.is_valid())\n        self.assertEqual(\n            form.errors[\"name1\"],\n            [\"Form custom error message.\"],\n            \"Form custom error message not used\",\n        )\n", "    def test_custom_ff_form(self):\n        form = CustomFFForm()\n        self.assertEqual(form.fields[\"custom\"].widget.__class__, forms.Textarea)\n", "def test_model_to_dict_with_exclude(self):\n    instance = Category(name=\"Test\", parent=None)\n    data = model_to_dict(instance, exclude=[\"parent\"])\n    self.assertEqual(data, {\"name\": \"Test\", \"slug\": \"\"})\n", "def test_modelform_defines_fields(self):\n    class MyModelForm(forms.ModelForm):\n        class Meta:\n            model = MyModel\n            fields = ['field1', 'field2']\n\n    class MyModelFormWithoutMeta:\n        pass\n\n    self.assertTrue(modelform_defines_fields(MyModelForm))\n    self.assertFalse(modelform_defines_fields(MyModelFormWithoutMeta))\n"], "sample_31": ["def test_write_latex_overwrite(cosmo, tmp_path, format):\n    file = tmp_path / \"test.tex\"\n    write_latex(cosmo, file, format=format)\n    assert file.exists()\n\n    # Attempt to write again without overwrite\n    with pytest.raises(OSError):\n        write_latex(cosmo, file, format=format)\n\n    # Write again with overwrite\n    write_latex(cosmo, file, format=format, overwrite=True)\n    assert file.exists()\n", "def test_write_latex_latex_names(self, cosmo, tmp_path):\n    \"\"\"\n    Test that the write_latex function correctly converts parameter names to LaTeX format\n    when latex_names is set to True.\n    \"\"\"\n    file = tmp_path / \"cosmology.tex\"\n    write_latex(cosmo, file, latex_names=True)\n\n    with open(file, \"r\") as f:\n        data = f.read()\n\n    for name in cosmo.__parameters__:\n        assert _FORMAT_TABLE.get(name, name) in data\n", "    def test_write_latex_overwrite(self, tmp_path, cosmo):\n        # Write cosmology to a file\n        f = tmp_path / \"cosmo.tex\"\n        write_latex(cosmo, f)\n\n        # Try to write again without overwriting\n        with pytest.raises(OSError):\n            write_latex(cosmo, f)\n\n        # Write again with overwrite=True\n        write_latex(cosmo, f, overwrite=True)\n", "def test_latex_names_false(self, cosmo, tmp_path):\n    filename = tmp_path / \"cosmology.tex\"\n    write_latex(cosmo, filename, latex_names=False)\n\n    with open(filename, \"r\") as f:\n        content = f.read()\n\n    # Check that no LaTeX formatted names are used\n    for latex_name in _FORMAT_TABLE.values():\n        assert latex_name not in content\n", "def test_latex_unit_conversion(cosmo):\n    # Create a temporary file to write to\n    with tempfile.NamedTemporaryFile(suffix=\".tex\", delete=False) as tmp:\n        # Write the cosmology to the temporary file\n        write_latex(cosmo, tmp.name)\n\n        # Read the file back into a Table\n        table = Table.read(tmp.name, format=\"ascii.latex\")\n\n        # Check that the units are correct\n        for name, col in table.columns.items():\n            param = getattr(type(cosmo), name, None)\n            if isinstance(param, Parameter) and param.unit not in (None, u.one):\n                assert col.unit == param.unit\n\n    # Remove the temporary file\n    os.remove(tmp.name)\n"], "sample_64": ["    def test_http_response_redirect(self):\n        response = HttpResponseRedirect('/redirect-url/')\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response['Location'], '/redirect-url/')\n", "    def test_http_response_content(self):\n        response = HttpResponse(\"test content\")\n        self.assertEqual(response.content, b\"test content\")\n\n        response.content = \"new content\"\n        self.assertEqual(response.content, b\"new content\")\n\n        response.write(\" appended\")\n        self.assertEqual(response.content, b\"new content appended\")\n", "    def test_streaming_http_response(self):\n        response = StreamingHttpResponse(iter([b'foo', b'bar']))\n        self.assertEqual(b'foobar', b''.join(response.streaming_content))\n", "    def test_set_cookie(self):\n        response = HttpResponse()\n        response.set_cookie('test_cookie', 'test_value', max_age=3600, path='/test_path')\n        self.assertEqual(response.cookies['test_cookie'].value, 'test_value')\n        self.assertEqual(response.cookies['test_cookie']['max-age'], 3600)\n        self.assertEqual(response.cookies['test_cookie']['path'], '/test_path')\n", "    def test_set_signed_cookie(self):\n        response = HttpResponse()\n        response.set_signed_cookie('name', 'value', salt='salt')\n        self.assertIn('name', response.cookies)\n        self.assertNotEqual('value', response.cookies['name'].value)\n"], "sample_694": ["def test_check_ispytest_warning(pytester):\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n        from _pytest import deprecated\n\n            deprecated.check_ispytest(_ispytest)\n\n            with pytest.warns(PytestDeprecationWarning, match=\"A private pytest class or function was used.\"):\n                private_function()\n        \"\"\"\n    )\n    result = pytester.runpytest(\"-v\")\n    result.stdout.fnmatch_lines([\"*test_private_function PASSED*\"])\n    result.assert_outcomes(passed=1)\n", "def test_hook_legacy_path_arg_warning(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.hookimpl(hookwrapper=True)\n            pass\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestRemovedIn8Warning: The (pylib_path_arg: py.path.local) argument is deprecated, \"\n            \"please use (pathlib_path_arg: pathlib.Path)*\"\n        ]\n    )\n", "def test_hook_legacy_path_arg(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.hookimpl(tryfirst=True)\n            pass\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines([\n        \"*DEPRECATION*pytest_collection_modifyitems*pylib_path_arg*pathlib.Path*\",\n    ])\n", "def test_fs_collector_gethookproxy_isinitpath(pytester):\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n            session = request.node.session\n            assert session.gethookproxy(\"pytest_sessionstart\") == pytest.hookspec.pytest_sessionstart\n            assert session.isinitpath(__file__) == pytest.config.isinitpath(__file__)\n\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)\n", "def test_check_ispytest_warning(pytester):\n    pytester.makepyfile(\n        \"\"\"\n        import warnings\n        from _pytest import deprecated\n\n            with warnings.catch_warnings(record=True) as w:\n                deprecated.check_ispytest(False)\n\n            assert len(w) == 1\n            assert issubclass(w[-1].category, PytestDeprecationWarning)\n            assert \"A private pytest class or function was used.\" in str(w[-1].message)\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)\n"], "sample_159": ["    def test_non_unique_username(self):\n        errors = check_user_model()\n        self.assertEqual(len(errors), 1)\n        self.assertEqual(errors[0].id, 'auth.W004')\n", "def test_non_unique_username(self):\n    errors = check_user_model()\n    self.assertEqual(len(errors), 1)\n    self.assertIsInstance(errors[0], checks.Warning)\n    self.assertEqual(errors[0].id, 'auth.W004')\n    self.assertEqual(errors[0].obj, CustomUserNonUniqueUsername)\n", "    def test_non_unique_username_field_warning(self):\n        errors = check_user_model(self.apps)\n        self.assertEqual(len(errors), 1)\n        self.assertEqual(errors[0].id, 'auth.W004')\n", "def test_check_user_model_non_unique_username(self):\n    # Test the case where the username field is not unique\n    errors = check_user_model()\n    self.assertEqual(len(errors), 1)\n    self.assertEqual(errors[0].id, 'auth.W004')\n    self.assertIn(\"'CustomUserNonUniqueUsername.username' is named as the 'USERNAME_FIELD', but it is not unique.\", str(errors[0]))\n    self.assertEqual(errors[0].hint, \"Ensure that your authentication backend(s) can handle non-unique usernames.\")\n", "    def test_non_unique_username(self):\n        errors = check_user_model()\n        self.assertEqual(len(errors), 1)\n        self.assertEqual(errors[0].id, 'auth.W004')\n"], "sample_1082": ["def test_asech_diff():\n    x = symbols('x')\n    assert asech(x).diff(x) == -1/(x*sqrt(1 - x**2))\n    assert asech(1).diff(x) == 0\n", "def test_acoth_fdiff():\n    x = Symbol('x')\n    assert acoth(x).fdiff() == 1 / (1 - x**2)\n    assert acoth(x).fdiff(argindex=2) == ArgumentIndexError\n", "def test_asech_inverse():\n    x = symbols('x')\n    assert asech(asech(x)) == x\n", "def test_acsch_fdiff():\n    x = Symbol('x')\n    assert acsch(x).fdiff() == -1/(x**2*sqrt(1 + 1/x**2))\n    assert raises(ArgumentIndexError, lambda: acsch(x).fdiff(2))\n", "def test_asech_expansion_term():\n    x = Symbol('x')\n    assert asech.expansion_term(0, x) == log(2 / x)\n    assert asech.expansion_term(2, x) == -x**2 / 4\n    assert asech.expansion_term(4, x) == x**4 / 32\n"], "sample_848": ["def test_classifier_chain_predict_proba():\n    X, Y = make_classification(n_samples=100, n_features=10, n_informative=5, n_classes=3, random_state=0)\n    chain = ClassifierChain(LogisticRegression())\n    chain.fit(X, Y)\n    Y_prob = chain.predict_proba(X)\n    assert Y_prob.shape == (100, 3)\n    assert np.all(np.logical_and(Y_prob >= 0, Y_prob <= 1))\n", "def test_regressor_chain_predict_proba():\n    X, y = datasets.make_regression(n_samples=10, n_features=5, n_informative=5, random_state=0)\n    base_estimator = Ridge()\n    chain = RegressorChain(base_estimator=base_estimator)\n    chain.fit(X, y)\n    with pytest.raises(AttributeError):\n        chain.predict_proba(X)\n", "def test_multioutput_regressor_partial_fit():\n    X, y = datasets.make_regression(n_samples=100, n_features=10, n_targets=3)\n    X_train, X_test = X[:80], X[80:]\n    y_train, y_test = y[:80], y[80:]\n\n    est = MultiOutputRegressor(estimator=SGDRegressor())\n    est.partial_fit(X_train, y_train)\n    y_pred = est.predict(X_test)\n    assert y_pred.shape == (20, 3)\n", "def test_multioutput_classifier_partial_fit():\n    X, y = make_classification(n_samples=100, n_features=20, n_informative=2,\n                               n_redundant=10, n_classes=2,\n                               n_clusters_per_class=1, random_state=0)\n    y = np.hstack([y, y])  # create two identical targets\n    est = MultiOutputClassifier(LogisticRegression())\n    est.partial_fit(X[:50], y[:50], classes=[np.unique(y[:, 0]), np.unique(y[:, 1])])\n    est.partial_fit(X[50:], y[50:])\n    pred = est.predict(X)\n    assert_array_equal(pred, y)\n", "def test_multioutput_classifier_predict_proba():\n    # Test that predict_proba raises ValueError if base estimator does not implement it\n    base_estimator = RandomForestClassifier()\n    multi_output_clf = MultiOutputClassifier(base_estimator)\n    X, y = make_classification(n_samples=10, n_features=4, n_classes=2, n_informative=2, random_state=42)\n    multi_output_clf.fit(X, y)\n    with assert_raises_regex(ValueError, \"The base estimator should implement predict_proba method\"):\n        multi_output_clf.predict_proba(X)\n"], "sample_473": ["def test_asgi_request_initialization(self):\n    scope = {\n        \"type\": \"http\",\n        \"http_version\": \"1.1\",\n        \"method\": \"GET\",\n        \"path\": \"/test/path/\",\n        \"query_string\": b\"key=value\",\n        \"headers\": [(b\"Content-Type\", b\"text/plain\")],\n        \"client\": (\"127.0.0.1\", 12345),\n        \"server\": (\"testserver\", 8000),\n        \"root_path\": \"/root/\",\n    }\n    body_file = tempfile.SpooledTemporaryFile(mode=\"w+b\")\n    request = ASGIRequest(scope, body_file)\n\n    self.assertEqual(request.method, \"GET\")\n    self.assertEqual(request.path_info, \"/path/\")\n    self.assertEqual(request.path, \"/root/path/\")\n    self.assertEqual(request.META[\"REQUEST_METHOD\"], \"GET\")\n    self.assertEqual(request.META[\"QUERY_STRING\"], \"key=value\")\n    self.assertEqual(request.META[\"SCRIPT_NAME\"], \"/root/\")\n    self.assertEqual(request.META[\"PATH_INFO\"], \"/path/\")\n    self.assertEqual(request.META[\"REMOTE_ADDR\"], \"127.0.0.1\")\n    self.assertEqual(request.META[\"REMOTE_PORT\"], 12345)\n    self.assertEqual(request.META[\"SERVER_NAME\"], \"testserver\")\n    self.assertEqual(request.META[\"SERVER_PORT\"], \"8000\")\n    self.assertEqual(request.META[\"CONTENT_TYPE\"], \"text/plain\")\n", "    def test_create_request_with_unicode_decode_error(self):\n        scope = {\n            \"type\": \"http\",\n            \"http_version\": \"1.1\",\n            \"method\": \"GET\",\n            \"path\": \"/test/\",\n            \"headers\": [(b\"content-length\", b\"10\"), (b\"content-type\", b\"text/plain; charset=utf-8\")],\n        }\n        body_file = io.BytesIO(b\"\\x80\")\n\n        request, error_response = WSGIHandler().create_request(scope, body_file)\n\n        self.assertIsNone(request)\n        self.assertEqual(error_response.status_code, 400)\n", "compilation error", "compilation error", "compilation error"], "sample_745": ["def test_function_transformer_inverse_func():\n        return X + 5\n\n        return X - 5\n\n    transformer = FunctionTransformer(func=add_five, inverse_func=subtract_five)\n    X = np.array([[1, 2], [3, 4]])\n    transformer.fit(X)\n\n    X_transformed = transformer.transform(X)\n    assert_array_equal(X_transformed, np.array([[6, 7], [8, 9]]))\n\n    X_inverse_transformed = transformer.inverse_transform(X_transformed)\n    assert_array_equal(X_inverse_transformed, X)\n", "def test_function_transformer_with_kw_args():\n    args_store = []\n    kwargs_store = {}\n\n    # Define a function that stores arguments\n    func = _make_func(args_store, kwargs_store)\n\n    # Define some keyword arguments\n    kw_args = {\"key1\": \"value1\", \"key2\": \"value2\"}\n\n    # Create a FunctionTransformer with the function and keyword arguments\n    ft = FunctionTransformer(func=func, kw_args=kw_args)\n\n    # Define some input data\n    X = np.array([[1, 2], [3, 4]])\n\n    # Call transform on the input data\n    ft.transform(X)\n\n    # Check that the keyword arguments were passed to the function\n    assert kwargs_store == kw_args\n\n    # Clear the argument stores\n    args_store.clear()\n    kwargs_store.clear()\n\n    # Define some input data for inverse transform\n    X_transformed = np.array([[5, 6], [7, 8]])\n\n    # Call inverse transform on the transformed input data\n    ft.inverse_transform(X_transformed)\n\n    # Check that the keyword arguments were passed to the inverse function\n    assert kwargs_store == kw_args\n", "def test_function_transformer_inverse_func():\n    X = np.array([[1, 2], [3, 4]])\n    func = lambda X: np.log(X)\n    inverse_func = np.exp\n\n    ft = FunctionTransformer(func=func, inverse_func=inverse_func)\n    ft.fit(X)\n\n    X_transformed = ft.transform(X)\n    assert_allclose_dense_sparse(X, ft.inverse_transform(X_transformed))\n", "def test_function_transformer_inverse_func_validation():\n        return X * 2\n\n        return X / 2\n\n    transformer = FunctionTransformer(func=func, inverse_func=inverse_func)\n    X = np.array([[1, 2], [3, 4]])\n    transformer.fit(X)\n    assert_no_warnings(transformer.transform, X)\n    assert_no_warnings(transformer.inverse_transform, transformer.transform(X))\n\n    # Test with non-inverse functions\n        return X + 1\n\n    transformer = FunctionTransformer(func=func, inverse_func=non_inverse_func)\n    transformer.fit(X)\n    assert_warns_message(UserWarning, \"The provided functions are not strictly\"\n                          \" inverse of each other.\", transformer.transform, X)\n    assert_warns_message(UserWarning, \"The provided functions are not strictly\"\n                          \" inverse of each other.\", transformer.inverse_transform, transformer.transform(X))\n", "def test_function_transformer_inverse_transform():\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([1, 0])\n    func = np.log\n    inverse_func = np.exp\n\n    ft = FunctionTransformer(func=func, inverse_func=inverse_func, validate=True)\n    ft.fit(X)\n\n    transformed_X = ft.transform(X)\n    inverse_transformed_X = ft.inverse_transform(transformed_X)\n\n    assert_allclose_dense_sparse(X, inverse_transformed_X)\n\n    with pytest.warns(DeprecationWarning):\n        transformed_X_with_y = ft.transform(X, y=y)\n        inverse_transformed_X_with_y = ft.inverse_transform(transformed_X_with_y, y=y)\n\n    assert_allclose_dense_sparse(X, inverse_transformed_X_with_y)\n\n    ft.pass_y = True\n    with pytest.warns(DeprecationWarning):\n        transformed_X_with_pass_y = ft.transform(X, y=y)\n        inverse_transformed_X_with_pass_y = ft.inverse_transform(transformed_X_with_pass_y, y=y)\n\n    assert_allclose_dense_sparse(X, inverse_transformed_X_with_pass_y)\n"], "sample_1184": ["def test_gaussian_conj():\n    s_in, z_r_in, f = symbols('s_in z_r_in f')\n    s_out, z_r_out, m = gaussian_conj(s_in, z_r_in, f)\n    assert s_out == 1 / ( -1/(s_in + z_r_in**2/(s_in - f)) + 1/f )\n    assert z_r_out == z_r_in / ((1 - (s_in/f)**2) + (z_r_in/f)**2)\n    assert m == 1/sqrt((1 - (s_in/f)**2) + (z_r_in/f)**2)\n", "def test_conjugate_gauss_beams():\n    l, w_i, w_o, f = symbols('l w_i w_o f')\n    result = conjugate_gauss_beams(l, w_i, w_o, f=f)\n    assert streq(result[0], f*(1 - sqrt(w_i**2/w_o**2 - pi**2*w_i**4/(f**2*l**2))))\n    assert streq(factor(result[1]), f*w_o**2*(w_i**2/w_o**2 - sqrt(w_i**2/w_o**2 - pi**2*w_i**4/(f**2*l**2)))/w_i**2)\n    assert streq(result[2], f)\n", "def test_conjugate_gauss_beams():\n    l, w_i, w_o, f = symbols('l w_i w_o f')\n    result = conjugate_gauss_beams(l, w_i, w_o, f=f)\n    assert isinstance(result, tuple)\n    assert len(result) == 3\n    s_in, s_out, f_result = result\n    assert f_result == f\n", "def test_beam_parameter_radius():\n    p = BeamParameter(530e-9, 1, w=1e-3)\n    expected_radius = 1 + (5.92753330865999/1)**2\n    assert N(p.radius) == expected_radius\n", "def test_beam_parameter_properties():\n    p = BeamParameter(530e-9, 1, w=1e-3)\n    assert N(p.q, 5) == 1 + 5.92753330866*I\n    assert N(p.radius, 5) == 1 + 3.55998576006*pi**2\n    assert N(p.w, 5) == 0.00100000000000001\n    assert N(p.w_0, 5) == 0.001\n    assert N(p.divergence, 5) == 0.000530844220360226/pi\n    assert N(p.gouy, 5) == atan2(1, 1.88679245283019)\n    assert N(p.waist_approximation_limit, 5) == 1.06066017177982e-06/pi\n"], "sample_360": ["    def setUp(self):\n        self.factory = RequestFactory()\n        self.cache_middleware = CacheMiddleware(empty_response)\n", "    def setUp(self):\n        self.factory = RequestFactory()\n        self.middleware = CacheMiddleware(empty_response)\n", "    def setUp(self):\n        self.factory = RequestFactory()\n        self.middleware = CacheMiddleware(empty_response)\n", "    def setUp(self):\n        self.factory = RequestFactory()\n        self.cache = cache\n        self.cache.clear()\n        self.middleware = CacheMiddleware(empty_response)\n", "    def setUp(self):\n        self.factory = RequestFactory()\n        self.middleware = FetchFromCacheMiddleware(empty_response)\n"], "sample_1143": ["def test_integer_nthroot():\n    assert integer_nthroot(8, 3) == (2, True)\n    assert integer_nthroot(27, 3) == (3, True)\n    assert integer_nthroot(26, 2) == (5, False)\n", "def test_rational_as_numer_denom():\n    r = Rational(3, 4)\n    n, d = r.as_numer_denom()\n    assert n == 3\n    assert d == 4\n", "def test_integer_is_prime():\n    assert Integer(2).is_prime\n    assert not Integer(4).is_prime\n    assert not Integer(1).is_prime\n    assert not Integer(0).is_prime\n    assert not Integer(-1).is_prime\n    assert not Integer(-2).is_prime\n    assert not Integer(-4).is_prime\n", "def test_comparison_with_other_types():\n    assert S.Zero < 1\n    assert S.One > 0\n    assert not S.Zero > 1\n    assert not S.One < 0\n    assert S.Zero <= 0\n    assert S.One >= 1\n    assert not S.Zero >= 1\n    assert not S.One <= 0\n    assert S.Zero != 1\n    assert S.One != 0\n    assert not S.Zero == 1\n    assert not S.One == 0\n", "def test_integer_pow():\n    assert Integer(2) ** Integer(3) == Integer(8)\n"], "sample_1009": ["def test_vector_dot_product():\n    v1 = Vector([(x, A.x), (y, A.y), (z, A.z)])\n    v2 = Vector([(2*x, A.x), (3*y, A.y), (4*z, A.z)])\n    assert v1.dot(v2) == 2*x**2 + 3*y**2 + 4*z**2\n", "def test_vector_subtraction():\n    vec1 = 2*A.x + 3*A.y\n    vec2 = A.x + 2*A.y\n    result = vec1 - vec2\n    assert result == A.x + A.y\n", "def test_vector_magnitude_and_normalize():\n    v = 3*A.x + 4*A.y\n    assert v.magnitude() == 5\n    assert v.normalize() == 3/5*A.x + 4/5*A.y\n", "def test_vector_addition():\n    vec1 = Vector(A.x + A.y)\n    vec2 = Vector(A.z)\n    result = vec1 + vec2\n    expected_result = Vector(A.x + A.y + A.z)\n    assert result == expected_result\n", "def test_vector_diff():\n    q1 = dynamicsymbols('q1')\n    N = ReferenceFrame('N')\n    B = N.orientnew('B', 'Axis', [q1, N.y])\n    v = A.x + B.x\n    assert v.diff(q1, N) == A.y\n"], "sample_250": ["def test_timezone_offset(self):\n    dt = make_aware(datetime(2022, 1, 1, 12, 0), get_fixed_timezone(180))\n    df = DateFormat(dt)\n    self.assertEqual(df.format('O'), '+0300')\n", "    def test_date_format_methods(self):\n        d = date(2022, 2, 3)\n        df = dateformat.DateFormat(d)\n\n        self.assertEqual(df.b(), 'Feb')\n        self.assertEqual(df.c(), '2022-02-03')\n        self.assertEqual(df.d(), '03')\n        self.assertEqual(df.D(), 'Thu')\n        self.assertEqual(df.E(), 'February')\n        self.assertEqual(df.F(), 'February')\n        self.assertEqual(df.I(), '0')\n        self.assertEqual(df.j(), 3)\n        self.assertEqual(df.l(), 'Thursday')\n        self.assertEqual(df.L(), False)\n        self.assertEqual(df.m(), '02')\n        self.assertEqual(df.M(), 'Feb')\n        self.assertEqual(df.n(), 2)\n        self.assertEqual(df.N(), 'Feb.')\n        self.assertEqual(df.o(), 2022)\n        self.assertEqual(df.S(), 'rd')\n        self.assertEqual(df.t(), '28')\n        self.assertEqual(df.U(), 1643692800)\n        self.assertEqual(df.w(), 4)\n        self.assertEqual(df.W(), 5)\n        self.assertEqual(df.y(), '22')\n        self.assertEqual(df.Y(), 2022)\n        self.assertEqual(df.z(), 34)\n", "    def test_format_date(self):\n        d = date(2003, 10, 7)\n        df = dateformat.DateFormat(d)\n        self.assertEqual(df.format('jS F Y'), '7th October 2003')\n", "    def test_format_with_timezone(self):\n        dt = datetime(2022, 1, 1, 12, 30, 0, tzinfo=utc)\n        df = dateformat.DateFormat(dt)\n        self.assertEqual(df.format('Y-m-d H:i e'), '2022-01-01 13:30 UTC')\n        self.assertEqual(df.format('Y-m-d H:i T'), '2022-01-01 13:30 UTC')\n", "    def test_dateformat_with_datetime(self):\n        dt = datetime(2022, 3, 28, 14, 30, 0)\n        df = dateformat.DateFormat(dt)\n        self.assertEqual(df.format('Y-m-d H:i'), '2022-03-28 14:30')\n        self.assertEqual(df.format('l, F j, Y'), 'Monday, March 28, 2022')\n"], "sample_3": ["def test_ecsv_read_multidim():\n    # Test reading a table with a multidimensional column\n    ecsv_content = '''# %ECSV 1.0\n    # ---\n    # datatype:\n    # - {name: a, datatype: int64}\n    # - {name: b, datatype: float64[2,3]}\n    # schema: astropy-2.0\n    a b\n    1 [1, 2, 3, 4, 5, 6]\n    2 [7, 8, 9, 10, 11, 12]\n    '''\n    t = Table.read(ecsv_content, format='ascii.ecsv')\n    assert np.array_equal(t['b'][0], np.array([[1, 2, 3], [4, 5, 6]]))\n    assert np.array_equal(t['b'][1], np.array([[7, 8, 9], [10, 11, 12]]))\n", "def test_ecsv_write_read_with_dtypes():\n    # Test writing and reading ECSV file with all supported dtypes\n    out = StringIO()\n    T_DTYPES.write(out, format='ascii.ecsv')\n    out.seek(0)\n    T_DTYPES_READ = Table.read(out, format='ascii.ecsv')\n\n    # Check that the read table matches the original table\n    assert T_DTYPES.colnames == T_DTYPES_READ.colnames\n    for colname in T_DTYPES.colnames:\n        assert np.all(T_DTYPES[colname] == T_DTYPES_READ[colname])\n        assert T_DTYPES[colname].unit == T_DTYPES_READ[colname].unit\n        assert T_DTYPES[colname].description == T_DTYPES_READ[colname].description\n        assert T_DTYPES[colname].meta == T_DTYPES_READ[colname].meta\n", "def test_ecsv_outputter_with_mixin_columns():\n    # Create a table with mixin columns\n    table = Table()\n    table['time'] = [1, 2, 3] * u.s\n    table['position'] = [(1, 2, 3), (4, 5, 6), (7, 8, 9)] * u.m\n    table['velocity'] = [(10, 20, 30), (40, 50, 60), (70, 80, 90)] * (u.m / u.s)\n\n    # Convert mixin columns to regular columns\n    table_with_columns = ascii.ecsv.Ecsv().update_table_data(table)\n\n    # Check that the table with regular columns has the expected columns\n    assert set(table_with_columns.colnames) == {'time', 'position_0', 'position_1', 'position_2',\n                                                 'velocity_0', 'velocity_1', 'velocity_2'}\n\n    # Check that the data in the table with regular columns is the same as in the original table\n    assert np.all(table_with_columns['time'] == table['time'])\n    assert np.all(table_with_columns['position_0'] == table['position'][:, 0])\n    assert np.all(table_with_columns['position_1'] == table['position'][:, 1])\n    assert np.all(table_with_columns['position_2'] == table['position'][:, 2])\n    assert np.all(table_with_columns['velocity_0'] == table['velocity'][:, 0])\n    assert np.all(table_with_columns['velocity_1'] == table['velocity'][:, 1])\n    assert np.all(table_with_columns['velocity_2'] == table['velocity'][:, 2])\n", "def test_ecsv_read_write():\n    # Test that a round-trip read/write of a table with various datatypes works\n    s = StringIO()\n    T_DTYPES.write(s, format='ascii.ecsv')\n    s.seek(0)\n    T_DTYPES_READ = Table.read(s, format='ascii.ecsv')\n    assert T_DTYPES_READ.colnames == T_DTYPES.colnames\n    for colname in T_DTYPES.colnames:\n        assert np.all(T_DTYPES_READ[colname] == T_DTYPES[colname])\n", "def test_write_ecsv():\n    # Test writing a table with all allowed datatypes\n    out = StringIO()\n    T_DTYPES.write(out, format='ascii.ecsv')\n    out.seek(0)\n    table = Table.read(out)\n\n    # Check that all columns have the correct datatype and unit\n    for dtype in DTYPES:\n        assert table[dtype].dtype.name == dtype\n        assert table[dtype].unit == 'm / s'\n        assert table[dtype].description == 'descr_' + dtype\n        assert table[dtype].meta == {'meta ' + dtype: 1}\n\n    # Check that the table metadata is preserved\n    assert table.meta['comments'] == ['comment1', 'comment2']\n"], "sample_570": ["    def test_univariate_kde(self, x):\n        kde = KDE()\n        density, support = kde(x)\n\n        # Check that the density integrates to 1\n        assert np.isclose(self.integrate(density, support), 1)\n\n        # Check that the density is monotonically decreasing\n        assert np.all(np.diff(density) <= 0)\n", "    def test_cumulative_kde(self, x):\n        kde = KDE(cumulative=True)\n        density, support = kde(x)\n        cumulative_density = np.array([kde._fit(x)(np.array([s]))[0] for s in support])\n        assert_array_almost_equal(density, cumulative_density)\n", "    def test_cumulative_kde(self, x):\n        kde = KDE(cumulative=True)\n        y, x_eval = kde(x)\n        assert_array_almost_equal(y[-1], 1, decimal=2)\n        assert self.integrate(y, x_eval) > 0.95\n", "    def test_kde_univariate_cumulative(self, x):\n        kde = KDE(cumulative=True)\n        density, support = kde(x)\n        assert_array_almost_equal(density, np.linspace(0, 1, kde.gridsize))\n", "def test_kde_cumulative(self, x):\n    kde = KDE(cumulative=True)\n    density, support = kde(x)\n    cdf = np.cumsum(density) * np.diff(support)\n    assert np.isclose(cdf[-1], 1)\n"], "sample_797": ["def test_PowerTransformer_inverse_transform():\n    X = np.array([[1, 2], [3, 2], [4, 5]])\n    pt = PowerTransformer()\n    pt.fit(X)\n    X_transformed = pt.transform(X)\n    X_inverse_transformed = pt.inverse_transform(X_transformed)\n    assert_array_almost_equal(X, X_inverse_transformed, decimal=2)\n", "def test_power_transformer_yeo_johnson_no_standardize():\n    pt = PowerTransformer(method='yeo-johnson', standardize=False)\n    X_transformed = pt.fit_transform(X_2d)\n    assert_almost_equal(np.mean(X_transformed, axis=0), 0, decimal=1)\n    assert_almost_equal(np.std(X_transformed, axis=0), 1, decimal=1)\n", "def test_power_transformer_yeo_johnson_standardize():\n    pt = PowerTransformer(method='yeo-johnson', standardize=True)\n    X_trans = pt.fit_transform(X_2d)\n    assert_array_almost_equal(X_trans.mean(axis=0), np.zeros(n_features))\n    assert_array_almost_equal(X_trans.std(axis=0), np.ones(n_features))\n", "def test_binarizer_sparse_output_type():\n    X = sparse_random(10, 10, format='csr', random_state=42)\n    binarizer = Binarizer(copy=False)\n    X_binarized = binarizer.fit_transform(X)\n    assert sparse.isspmatrix_csr(X_binarized)\n", "def test_polynomial_features_interaction_only():\n    X = np.array([[1, 2, 3], [4, 5, 6]])\n    pf = PolynomialFeatures(degree=2, interaction_only=True)\n    X_transformed = pf.fit_transform(X)\n    assert_array_equal(X_transformed, np.array([[1, 2, 3, 2*3], [4, 5, 6, 5*6]]))\n"], "sample_530": ["def test_offsetbox_clipping():\n    fig, ax = plt.subplots()\n    da = DrawingArea(20, 20, 0, 0, clip=True)\n    p = mpatches.Circle((5, 5), radius=5)\n    da.add_artist(p)\n    da.set_offset((10, 10))\n    ax.add_artist(da)\n    fig.savefig(io.BytesIO())\n", "def test_get_packed_offsets():\n    widths = [1.0, 2.0, 3.0]\n    total, offsets = _get_packed_offsets(widths, total=None, sep=0.5, mode=\"fixed\")\n    assert_allclose(total, 6.5)\n    assert_allclose(offsets, [0.0, 1.5, 3.5])\n\n    total, offsets = _get_packed_offsets(widths, total=10.0, sep=0.5, mode=\"expand\")\n    assert_allclose(total, 10.0)\n    assert_allclose(offsets, [0.0, 3.0, 6.0])\n\n    total, offsets = _get_packed_offsets(widths, total=None, sep=1.0, mode=\"equal\")\n    assert_allclose(total, 9.0)\n    assert_allclose(offsets, [0.0, 3.0, 6.0])\n", "def test_offsetbox_extent():\n    fig, ax = plt.subplots()\n    ob = OffsetBox()\n    ob.set_width(50)\n    ob.set_height(30)\n    ob.set_offset((10, 10))\n    ax.add_artist(ob)\n    assert_allclose(ob.get_window_extent().bounds, (10, 10, 50, 30))\n", "def test_DraggableAnnotation():\n    fig, ax = plt.subplots()\n    ann = AnnotationBbox(OffsetBox(), (0.5, 0.5), frameon=False)\n    ax.add_artist(ann)\n    draggable_ann = DraggableAnnotation(ann)\n\n    # Simulate a pick event\n    event = MouseEvent(\"pick_event\", fig.canvas, x=300, y=300, button=MouseButton.LEFT)\n    fig.canvas.callbacks.process(\"pick_event\", event)\n    draggable_ann.on_pick(event)\n\n    # Simulate a motion event\n    event = MouseEvent(\"motion_notify_event\", fig.canvas, x=310, y=310, button=MouseButton.LEFT)\n    fig.canvas.callbacks.process(\"motion_notify_event\", event)\n    draggable_ann.on_motion(event)\n\n    # Check if the annotation position is updated correctly\n    assert_allclose(ann.xyann, (0.51, 0.51), atol=0.01)\n\n    # Simulate a release event\n    event = MouseEvent(\"button_release_event\", fig.canvas, x=310, y=310, button=MouseButton.LEFT)\n    fig.canvas.callbacks.process(\"button_release_event\", event)\n    draggable_ann.on_release(event)\n\n    # Check if the final position is set correctly\n    assert_allclose(ann.xyann, (0.51, 0.51), atol=0.01)\n", "def test_offsetbox_draggable():\n    fig, ax = plt.subplots()\n    box = TextArea(\"Draggable text\", textprops=dict(size=20))\n    ab = AnnotationBbox(box, (0.5, 0.5), xycoords='data',\n                        boxcoords=\"offset points\", box_alignment=(0, 0.5))\n    ax.add_artist(ab)\n\n    # Simulate dragging the text\n    dragger = DraggableAnnotation(ab)\n    dragger.save_offset()\n    dragger.update_offset(50, 50)\n    dragger.finalize_offset()\n\n    assert_allclose(ab.xyann, (0.55, 0.55))\n"], "sample_996": ["def test_product_convergence():\n    assert product(1/n**2, (n, 1, oo)).is_convergent() == True\n    assert product(exp(-n**2), (n, 1, oo)).is_convergent() == False\n", "def test_product_convergence():\n    assert product(1/n**2, (n, 1, oo)).is_convergent() == S.true\n    assert product(exp(-n**2), (n, 1, oo)).is_convergent() == S.false\n", "def test_product_reverse_order():\n    P = Product(x, (x, a, b))\n    Pr = P.reverse_order(x)\n    Pr_doit = Pr.doit()\n    P_doit = P.doit()\n    assert simplify(Pr_doit) == gamma(b + 1)/gamma(a)\n    assert simplify(P_doit) == gamma(b + 1)/gamma(a)\n", "def test_product_reverse_order():\n    P = Product(x, (x, a, b))\n    Pr = P.reverse_order(x)\n    assert simplify(Pr.doit()) == gamma(b + 1) / gamma(a)\n    assert simplify(P.doit()) == gamma(b + 1) / gamma(a)\n\n    S = Sum(x*y, (x, a, b), (y, 2, 5))\n    assert S.reverse_order(x, 1) == Sum(x*y, (x, b + 1, a - 1), (y, 6, 1))\n", "def test_product_difference_of_squares():\n    assert product(1 - x**2/k**2, (k, 1, n)) == rf(1 - x/2, n) * rf(-x/2 + 1, n) / factorial(n)**2\n"], "sample_901": ["def test_kmeans_sparse_elkan(representation, dtype):\n    if representation == 'sparse':\n        X_test = sp.csr_matrix(X_csr, dtype=dtype)\n        algo = 'elkan'\n        with pytest.raises(TypeError, match=\"algorithm='elkan' not supported for sparse input X\"):\n            k_means(X_test, n_clusters=n_clusters, init=centers, algorithm=algo, random_state=42)\n", "def test_kmeans_centers_input(representation, algo, dtype):\n    if representation == 'sparse' and algo == 'elkan':\n        pytest.skip(\"Elkan's algorithm does not support sparse input.\")\n    X = X_csr if representation == 'sparse' else X\n    X = X.astype(dtype)\n    kmeans = KMeans(n_clusters=n_clusters, init=centers, algorithm=algo, random_state=42)\n    kmeans.fit(X)\n    assert_array_almost_equal(kmeans.cluster_centers_, centers, decimal=3)\n", "def test_kmeans_sparse_elkan(representation, algo, dtype):\n    if representation == 'sparse' and algo == 'elkan':\n        # Test if a TypeError is raised when trying to use 'elkan' algorithm with sparse data\n        with assert_raises(TypeError):\n            KMeans(n_clusters=n_clusters, algorithm=algo, random_state=42).fit(X_csr.astype(dtype))\n", "def test_k_means_consistent_labels_and_centers(representation, algo, dtype):\n    if representation == 'sparse' and algo == 'elkan':\n        pytest.skip(\"Elkan algorithm does not support sparse input.\")\n    X = X_csr if representation == 'sparse' else X\n    X = X.astype(dtype)\n    kmeans = KMeans(n_clusters=n_clusters, random_state=0, algorithm=algo)\n    kmeans.fit(X)\n    labels = kmeans.predict(X)\n    assert_array_equal(labels, kmeans.labels_)\n    assert_array_almost_equal(kmeans.cluster_centers_,\n                              X[np.concatenate([np.where(labels == i)[0] for i in range(n_clusters)]).ravel()].mean(axis=0),\n                              decimal=4)\n", "def test_kmeans_consistent_inertia_and_labels(representation, algo, dtype):\n    if representation == 'dense':\n        data = X.astype(dtype)\n    else:\n        data = X_csr.astype(dtype)\n\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42, algorithm=algo)\n    kmeans.fit(data)\n    labels = kmeans.predict(data)\n    inertia = kmeans.inertia_\n\n    # Check that labels and inertia are consistent with each other\n    _, calculated_inertia = _labels_inertia(data, None, row_norms(data, squared=True), kmeans.cluster_centers_)\n    assert_almost_equal(inertia, calculated_inertia)\n\n    # Check that labels are consistent with the cluster_centers_\n    new_labels = np.empty(n_samples, dtype=np.int32)\n    for i in range(n_samples):\n        new_labels[i] = np.argmin(np.linalg.norm(data[i] - kmeans.cluster_centers_, axis=1))\n    assert_array_equal(labels, new_labels)\n"], "sample_1137": ["def test_convert_to():\n    # Test conversion to Planck units\n    expr = gravitational_constant * speed_of_light ** 2\n    target_units = [gravitational_constant, speed_of_light, hbar]\n    result = convert_to(expr, target_units)\n    assert result == hbar ** 2 / speed_of_light\n", "def test_convert_to_planck_units():\n    expr = joule * speed_of_light * gravitational_constant\n    converted_expr = convert_to(expr, [joule, speed_of_light, gravitational_constant])\n    assert converted_expr == 1\n", "def test_convert_to_multiple_dimensions():\n    expr = speed_of_light * gravitational_constant\n    target_units = [meter, second, kg]\n    result = convert_to(expr, target_units)\n    expected = 299792458 * 6.67430e-11 * meter**3 * second**-2 * kg**-1\n    assert result == expected\n", "def test_convert_to_planck_units():\n    expr = 4 * joule\n    target_units = [gravitational_constant, speed_of_light, planck_constant]\n    result = convert_to(expr, target_units)\n    expected_result = 1.085604411134673e-27 * gravitational_constant**(1/2) * planck_constant**(1/2) * speed_of_light**(1/2)\n    assert abs(result - expected_result) < 1e-20\n", "def test_convert_to_planck_units():\n    # Test conversion to Planck units\n    planck_units = [gravitational_constant, speed_of_light, Planck_constant]\n    result = convert_to(atomic_mass_constant, planck_units)\n    expected = Rational(1, 25343074911434820127995151922798714280100070862069310271748921949850) * gravitational_constant ** Rational(-1, 2) * Planck_constant ** Rational(1, 2) * speed_of_light ** Rational(1, 2)\n    assert result == expected\n"], "sample_285": ["def test_finders_check_with_warning(self):\n    \"\"\"Test check_finders with a warning.\"\"\"\n    settings.STATICFILES_DIRS = ['/nonexistent_directory']\n    expected_output = [\n        Warning(\n            \"The directory '/nonexistent_directory' in the STATICFILES_DIRS setting \"\n            \"does not exist.\",\n            id='staticfiles.W004',\n        )\n    ]\n    self.assertEqual(check_finders(None), expected_output)\n", "    def test_find_in_app(self):\n        finder = AppDirectoriesFinder()\n        test_app = 'test_app'\n        test_path = 'test_file.txt'\n        app_storage = finder.storage_class(Path(TEST_ROOT) / test_app / finder.source_dir)\n        finder.storages[test_app] = app_storage\n        finder.apps = [test_app]\n\n        # Create the test file\n        test_file_path = app_storage.path(test_path)\n        test_file_path.parent.mkdir(parents=True, exist_ok=True)\n        test_file_path.touch()\n\n        # Test finding the file in the app\n        result = finder.find_in_app(test_app, test_path)\n        self.assertEqual(result, str(test_file_path))\n\n        # Test finding a file that doesn't exist\n        result = finder.find_in_app(test_app, 'non_existent_file.txt')\n        self.assertIsNone(result)\n", "def test_default_storage_finder(self):\n    finder = get_finder('django.contrib.staticfiles.finders.DefaultStorageFinder')\n    self.assertIsInstance(finder, BaseFinder)\n\n    # Test the find method\n    with mock.patch('django.core.files.storage.default_storage.exists', return_value=True):\n        with mock.patch('django.core.files.storage.default_storage.path', return_value='/path/to/file.txt'):\n            result = finder.find('file.txt')\n            self.assertEqual(result, '/path/to/file.txt')\n\n    # Test the list method\n    with mock.patch('django.contrib.staticfiles.utils.get_files', return_value=['file1.txt', 'file2.txt']):\n        result = list(finder.list([]))\n        self.assertEqual(result, [('file1.txt', settings.DEFAULT_FILE_STORAGE), ('file2.txt', settings.DEFAULT_FILE_STORAGE)])\n", "def test_app_directories_finder(self):\n    \"\"\"\n    Test AppDirectoriesFinder's behavior.\n    \"\"\"\n    # Mock apps to return a known app\n    mock_app_config = mock.Mock()\n    mock_app_config.name = 'test_app'\n    mock_app_config.path = str(TEST_ROOT / 'test_app')\n    with mock.patch('django.contrib.staticfiles.finders.apps.get_app_configs', return_value=[mock_app_config]):\n        finder = AppDirectoriesFinder()\n\n        # Test finding a file in the app directory\n        (TEST_ROOT / 'test_app/static/test.txt').write_text('test content')\n        self.assertEqual(finder.find('test.txt'), str(TEST_ROOT / 'test_app/static/test.txt'))\n\n        # Test ignoring a file in the app directory\n        (TEST_ROOT / 'test_app/static/ignored.txt').write_text('ignored content')\n        self.assertEqual(finder.find('ignored.txt', ignore_patterns=['ignored.txt']), [])\n", "    def test_find_in_app(self):\n        finder = AppDirectoriesFinder(app_names=['testapp'])\n        with mock.patch('os.path.isdir', return_value=True):\n            with mock.patch('os.path.exists', return_value=True):\n                result = finder.find_in_app('testapp', 'test.css')\n                self.assertIsNotNone(result)\n"], "sample_1150": ["def test_complex_region_contains():\n    region = ComplexRegion(Interval(1, 3) * Interval(4, 6))\n    assert 2 + 5*I in region\n    assert 5*I not in region\n", "def test_normalize_theta_set():\n    theta = Interval(9*pi/2, 5*pi)\n    normalized = normalize_theta_set(theta)\n    assert normalized == Interval(pi/2, pi)\n", "def test_ComplexRegion_contains():\n    # Test the _contains method for CartesianComplexRegion\n    region = ComplexRegion(Interval(1, 3) * Interval(4, 6))\n    assert region._contains(2 + 5*I) == S.true\n    assert region._contains(5*I) == S.false\n\n    # Test the _contains method for PolarComplexRegion\n    rset = Interval(0, oo)\n    thetaset = Interval(0, pi)\n    upper_half_plane = ComplexRegion(rset * thetaset, polar=True)\n    assert upper_half_plane._contains(1 + I) == S.true\n    assert upper_half_plane._contains(1 - I) == S.false\n", "def test_complex_region_polar_form():\n    r = Interval(0, 1)\n    theta = Interval(0, pi)\n    region = ComplexRegion(r*theta, polar=True)\n    assert region == ComplexRegion(Interval(0, 1)*Interval(0, pi), polar=True)\n    assert region.polar is True\n    assert region.a_interval == Interval(0, 1)\n    assert region.b_interval == Interval(0, pi)\n    assert region.expr == r*(cos(theta) + I*sin(theta))\n    assert region.contains(0.5 + 0.5*I)\n    assert not region.contains(1.5 + 0.5*I)\n", "def test_complex_region_polar():\n    region = ComplexRegion(Interval(0, 1) * Interval(0, 2*pi), polar=True)\n    assert region == ComplexRegion(Interval(0, 1) * Interval(0, 2*pi), polar=True)\n    assert region.polar is True\n    assert region.expr == region.variables[0] * (cos(region.variables[1]) + I*sin(region.variables[1]))\n    assert region.a_interval == Interval(0, 1)\n    assert region.b_interval == Interval(0, 2*pi)\n    assert region._measure == 2*pi\n    assert region.from_real(Interval(0, 1)) == CartesianComplexRegion(ProductSet(Interval(0, 1), FiniteSet(0)))\n    assert 0.5 + 0.5*I in region\n    assert 1 + I not in region\n"], "sample_492": ["    def test_serialize_enum(self):\n        tests = [\n            (TextEnum.A, \"models.Text('a-value')\", {}),\n            (TextTranslatedEnum.B, \"models.Text(_('value-b'))\", {'from django.utils.translation import gettext_lazy as _'}),\n            (BinaryEnum.A, \"models.Binary(b'a-value')\", {}),\n            (IntEnum.B, \"models.Integer(2)\", {}),\n            (IntFlagEnum.B, \"models.Integer(2)\", {}),\n        ]\n\n        for value, expected_output, expected_imports in tests:\n            output, imports = MigrationWriter.serialize(value)\n            self.assertEqual(output, expected_output)\n            self.assertEqual(imports, expected_imports)\n", "    def test_operation_writer_with_enum_types(self):\n        operation = migrations.AlterField(\n            model_name=\"model\",\n            name=\"text_enum_field\",\n            field=models.CharField(\n                max_length=255,\n                choices=[(tag.name, tag.value) for tag in TextEnum],\n            ),\n        )\n        operation_writer = OperationWriter(operation)\n        operation_string, imports = operation_writer.serialize()\n        self.assertIn(\"from .models import TextEnum\", imports)\n        self.assertIn(\"choices=[(TextEnum.A.name, TextEnum.A.value), (TextEnum.B.name, TextEnum.B.value)]\", operation_string)\n\n        operation = migrations.AlterField(\n            model_name=\"model\",\n            name=\"text_translated_enum_field\",\n            field=models.CharField(\n                max_length=255,\n                choices=[(tag.name, tag.value) for tag in TextTranslatedEnum],\n            ),\n        )\n        operation_writer = OperationWriter(operation)\n        operation_string, imports = operation_writer.serialize()\n        self.assertIn(\"from .models import TextTranslatedEnum\", imports)\n        self.assertIn(\"choices=[(TextTranslatedEnum.A.name, TextTranslatedEnum.A.value), (TextTranslatedEnum.B.name, TextTranslatedEnum.B.value)]\", operation_string)\n\n        operation = migrations.AlterField(\n            model_name=\"model\",\n            name=\"binary_enum_field\",\n            field=models.BinaryField(\n                max_length=255,\n                choices=[(tag.name, tag.value) for tag in BinaryEnum],\n            ),\n        )\n        operation_writer = OperationWriter(operation)\n        operation_string, imports = operation_writer.serialize()\n        self.assertIn(\"from .models import BinaryEnum\", imports)\n        self.assertIn(\"choices=[(BinaryEnum.A.name, BinaryEnum.A.value), (BinaryEnum.B.name", "    def test_serialize(self):\n        value = ('auth', '0001_initial')\n        serialized, imports = MigrationWriter.serialize(value)\n        self.assertEqual(serialized, \"('auth', '0001_initial')\")\n        self.assertEqual(imports, set())\n", "    def test_serialize_enum_values(self):\n        \"\"\"\n        Test that Enum values are serialized correctly.\n        \"\"\"\n        operation = custom_migration_operations.operations.CustomEnumOperation(\n            name=\"CustomEnumOperation\",\n            enum_class=TextEnum,\n            new_values=[TextEnum.A],\n            remove_values=[TextEnum.B],\n        )\n        operation_writer = OperationWriter(operation)\n        operation_string, _ = operation_writer.serialize()\n        self.assertIn(\"TextEnum.A\", operation_string)\n        self.assertIn(\"TextEnum.B\", operation_string)\n\n        operation = custom_migration_operations.operations.CustomEnumOperation(\n            name=\"CustomEnumOperation\",\n            enum_class=TextTranslatedEnum,\n            new_values=[TextTranslatedEnum.A],\n            remove_values=[TextTranslatedEnum.B],\n        )\n        operation_writer = OperationWriter(operation)\n        operation_string, _ = operation_writer.serialize()\n        self.assertIn(\"_('a-value')\", operation_string)\n        self.assertIn(\"_('value-b')\", operation_string)\n\n        operation = custom_migration_operations.operations.CustomEnumOperation(\n            name=\"CustomEnumOperation\",\n            enum_class=BinaryEnum,\n            new_values=[BinaryEnum.A],\n            remove_values=[BinaryEnum.B],\n        )\n        operation_writer = OperationWriter(operation)\n        operation_string, _ = operation_writer.serialize()\n        self.assertIn(\"b'a-value'\", operation_string)\n        self.assertIn(\"b'value-b'\", operation_string)\n\n        operation = custom_migration_operations.operations.CustomEnumOperation(\n            name=\"CustomEnumOperation\",\n            enum_class=IntEnum,\n            new_values=[IntEnum.A],\n            remove_values=[IntEnum.B],\n        )\n        operation_writer = OperationWriter(operation)\n        operation_string, _ = operation_writer.serialize()\n        self.assertIn(\"IntEnum.A\", operation_string)\n        self.assertIn(\"IntEnum.B\", operation_string", "    def test_serialize_enum(self):\n        # Test serialization of different types of Enums\n        test_cases = [\n            (TextEnum, \"'%s.A'\" % TextEnum.__name__, {'import %s' % TextEnum.__module__}),\n            (TextTranslatedEnum, \"'%s.A'\" % TextTranslatedEnum.__name__, {'import %s' % TextTranslatedEnum.__module__}),\n            (BinaryEnum, \"b'a-value'\", set()),\n            (IntEnum, \"'%s.A'\" % IntEnum.__name__, {'import %s' % IntEnum.__module__}),\n            (IntFlagEnum, \"'%s.A'\" % IntFlagEnum.__name__, {'import %s' % IntFlagEnum.__module__}),\n        ]\n        for enum_type, expected_value, expected_imports in test_cases:\n            operation = custom_migration_operations.operations.CustomOperation(\n                field=models.CharField(choices=[(tag.value, tag.name) for tag in enum_type]),\n                to_field=models.IntegerField(),\n            )\n            operation_string, operation_imports = OperationWriter(operation).serialize()\n            self.assertIn(\"field=models.CharField(choices=[(%s, '%s')]\", operation_string % (expected_value, enum_type.A.name),)\n            self.assertEqual(operation_imports, expected_imports)\n"], "sample_940": ["def test_signature_from_str():\n    sig = inspect.signature_from_str(\"(a: int, b: str = 'default') -> bool\")\n    assert len(sig.parameters) == 2\n    assert sig.parameters['a'].annotation == int\n    assert sig.parameters['b'].annotation == str\n    assert sig.parameters['b'].default == 'default'\n    assert sig.return_annotation == bool\n", "def test_signature():\n        pass\n\n    sig = inspect.signature(func)\n    assert str(sig) == '(a: int, b: str = \\'default\\', *args: float, **kwargs: str) -> bool'\n", "def test_signature_from_ast():\n    code = \"\"\"", "def test_signature_from_str():\n    signature = '(a: int, b: str = \"default\")'\n    sig = inspect.signature_from_str(signature)\n    assert len(sig.parameters) == 2\n    assert sig.parameters['a'].annotation == int\n    assert sig.parameters['b'].annotation == str\n    assert sig.parameters['b'].default == 'default'\n", "def test_is_builtin_class_method():\n    # Test if is_builtin_class_method correctly identifies built-in class methods\n    assert is_builtin_class_method(int, '__init__') is True\n    assert is_builtin_class_method(str, '__init__') is True\n    assert is_builtin_class_method(list, 'append') is True\n    assert is_builtin_class_method(dict, 'keys') is False  # keys is a dictview object\n    assert is_builtin_class_method(datetime.datetime, 'today') is False  # today is a classmethod\n"], "sample_1176": ["def test_integer_methods():\n    assert Integer(3).__int__() == 3\n    assert Integer(3).floor() == Integer(3)\n    assert Integer(3).ceiling() == Integer(3)\n    assert Integer(3).__neg__() == Integer(-3)\n    assert Integer(3).__abs__() == Integer(3)\n", "def test_integer_operations():\n    a = Integer(5)\n    b = Integer(3)\n    assert a + b == Integer(8)\n    assert a - b == Integer(2)\n    assert a * b == Integer(15)\n    assert a // b == Integer(1)\n    assert a % b == Integer(2)\n    assert a ** b == Integer(125)\n", "def test_mpf_norm():\n    a = mpf(1.23456789123456789)\n    b = mpf_norm(a, 5)\n    assert same_and_same_prec(b, mpf(1.2346))\n", "def test_mpmath_conversion():\n    a = mpmath.mpf(3.14)\n    b = Float(a)\n    assert same_and_same_prec(b, Float(3.14))\n", "def test_float_as_mpf():\n    f = Float(3.14159, precision=53)\n    mpf_f = f._as_mpf_val(53)\n    assert mpf_norm(mpf_f, 53) == mpf_f\n    assert mpf_f._mpf_ == (0, 314159000000000000000, -52, 53)\n"], "sample_254": ["def test_inline_formset_has_change_link(self):\n    url = reverse('admin:admin_inlines_inner_change', args=[self.holder.inner.pk])\n    response = self.client.get(url)\n    self.assertContains(response, INLINE_CHANGELINK_HTML, count=1)\n", "def test_inline_delete(self):\n    inner_instance = Inner.objects.get(holder=self.holder)\n    url = reverse('admin:admin_inlines_inner_delete', args=[inner_instance.id])\n    response = self.client.post(url, {'post': 'yes'})\n    self.assertEqual(response.status_code, 302)\n    self.assertEqual(Inner.objects.count(), 0)\n", "def test_inline_add_view(self):\n    url = reverse('admin:admin_inlines_holder_add')\n    data = {\n        'dummy': 13,\n        'inner_set-TOTAL_FORMS': '1',\n        'inner_set-INITIAL_FORMS': '0',\n        'inner_set-MAX_NUM_FORMS': '',\n        'inner_set-0-dummy': 43,\n    }\n    response = self.client.post(url, data)\n    self.assertEqual(response.status_code, 302)\n    self.assertEqual(Holder.objects.count(), 2)\n    self.assertEqual(Inner.objects.count(), 2)\n    self.assertEqual(Inner.objects.last().dummy, 43)\n", "def test_inline_formset_errors(self):\n    url = reverse('admin:admin_inlines_inner_change', args=(self.holder.inner.pk,))\n    response = self.client.get(url)\n    self.assertEqual(response.status_code, 200)\n\n    data = {\n        'dummy': '',  # invalid input to trigger form errors\n        'holder': self.holder.pk,\n        'inner4stacked_set-TOTAL_FORMS': '0',\n        'inner4stacked_set-INITIAL_FORMS': '0',\n        'inner4stacked_set-MIN_NUM_FORMS': '0',\n        'inner4stacked_set-MAX_NUM_FORMS': '1000',\n        'inner4tabular_set-TOTAL_FORMS': '0',\n        'inner4tabular_set-INITIAL_FORMS': '0',\n        'inner4tabular_set-MIN_NUM_FORMS': '0',\n        'inner4tabular_set-MAX_NUM_FORMS': '1000',\n    }\n    response = self.client.post(url, data)\n    self.assertEqual(response.status_code, 200)\n    self.assertContains(response, 'Please correct the error below.')\n", "def test_inline_change_permission(self):\n    url = reverse('admin:admin_inlines_holder_change', args=(self.holder.id,))\n    response = self.client.get(url)\n    self.assertContains(response, INLINE_CHANGELINK_HTML, count=1)\n\n    # Change permission for Inner model\n    content_type = ContentType.objects.get_for_model(Inner)\n    permission = Permission.objects.get(content_type=content_type, codename='change_inner')\n    self.superuser.user_permissions.remove(permission)\n\n    response = self.client.get(url)\n    self.assertNotContains(response, INLINE_CHANGELINK_HTML)\n\n    # Add permission back\n    self.superuser.user_permissions.add(permission)\n"], "sample_665": ["def test_fixturenames(self):\n    definition = FunctionDefinition(name=\"func\", parent=None, callobj=lambda: None)\n    fixtureinfo = MagicMock()\n    fixtureinfo.names_closure = [\"fixture1\", \"fixture2\"]\n    metafunc = Metafunc(definition, fixtureinfo, config=None)\n    assert metafunc.fixturenames == [\"fixture1\", \"fixture2\"]\n", "def test_collect_file_with_non_python_file(self, tmp_path):\n    non_python_file = tmp_path / \"test_non_python_file.txt\"\n    non_python_file.write_text(\"This is not a python file\")\n\n    config = pytest.config.Config.fromdictargs({})\n    parent = pytest.collect.Collector(parent=None, config=config)\n    collector = pytest.collect.collect_file(non_python_file, parent)\n\n    assert collector is None\n", "def test_function_definition_repr(self):\n    config = pytest.config.Config.fromdictargs({})\n    class TestClass:\n            pass\n    test_class = TestClass()\n    function_definition = FunctionDefinition(\"test_method\", parent=None, callobj=test_class.test_method)\n    assert repr(function_definition) == \"<FunctionDefinition 'test_method'>\", \"FunctionDefinition.__repr__ returns incorrect representation\"\n", "def test_Module_inject_setup_module_fixture(self):\n    module = pytest.Module(py.path.local(__file__))\n    module.obj = ModuleWithSetup(self)\n    module._inject_setup_module_fixture()\n    assert \"__pytest_setup_module\" in dir(module.obj)\n", "def test_pycollect_makeitem_yields_generator(self):\n    class CollectorMock:\n        parent = None\n        config = None\n\n        yield\n\n    name = \"test_func\"\n    obj = yield_generator\n    collector = CollectorMock()\n    collector.istestfunction = lambda obj, name: True\n    collector.session = Session()\n\n    res = collector._makeitem(name, obj)\n\n    assert isinstance(res, pytest.Generator)\n    assert res.name == \"test_func[0]\"\n    assert \"yield tests were removed in pytest 4.0\" in res.keywords[\"xfail\"]._markers[0].kwargs[\"reason\"]\n"], "sample_57": ["    def test_clean_valid_credentials(self):\n        form = AuthenticationForm(data={'username': 'testclient', 'password': 'password'})\n        self.assertTrue(form.is_valid())\n", "    def test_clean_password(self):\n        form = UserChangeForm(instance=self.u1)\n        # Test that the cleaned_data is the initial password\n        self.assertEqual(form.clean_password(), form.initial.get('password'))\n", "    def test_valid_authentication(self):\n        form = AuthenticationForm(data={\n            'username': 'testclient',\n            'password': 'password',\n        })\n        self.assertTrue(form.is_valid())\n", "    def test_clean_email_field_required(self):\n        form_data = {'username': 'testuser', 'password1': 'testpassword123', 'password2': 'testpassword123'}\n        form = UserCreationForm(data=form_data)\n        self.assertFalse(form.is_valid())\n        self.assertIn('email', form.errors)\n", "    def test_clean_valid_credentials(self):\n        data = {'username': 'testclient', 'password': 'password'}\n        form = AuthenticationForm(data=data)\n        self.assertTrue(form.is_valid())\n        self.assertEqual(form.get_user(), self.u1)\n"], "sample_569": ["def test_estimate_data(self):\n    plotter = lm._RegressionPlotter(x=\"d\", y=\"y\", data=self.df, x_bins=3)\n    vals, points, cis = plotter.estimate_data\n    assert len(vals) == 3\n    assert len(points) == 3\n    assert len(cis) == 3\n    for ci in cis:\n        assert len(ci) == 2\n", "def test_plot_method_not_implemented(self):\n    plotter = lm._LinearPlotter()\n    with pytest.raises(NotImplementedError):\n        plotter.plot(plt.gca())\n", "def test_estimate_data(self):\n    plotter = lm._RegressionPlotter(x='d', y='y', data=self.df, x_estimator=np.mean, x_ci=95)\n    xs, ys, cis = plotter.estimate_data\n    assert len(xs) == len(np.unique(self.df.d))\n    assert len(ys) == len(np.unique(self.df.d))\n    assert len(cis) == len(np.unique(self.df.d))\n    for ci in cis:\n        assert len(ci) == 2\n", "    def test_residplot_with_lowess(self):\n        fig, ax = plt.subplots()\n        lm.residplot(x=\"x\", y=\"y\", data=self.df, lowess=True, ax=ax)\n        plt.close(fig)\n", "def test_regplot_with_partial_variables(self):\n    partial_x = self.rs.normal(size=60)\n    partial_y = self.rs.normal(size=60)\n    x_partial = self.df['x'] + partial_x\n    y_partial = self.df['y'] + partial_y\n\n    ax = lm.regplot(x=self.df['x'], y=self.df['y'], x_partial=x_partial, y_partial=y_partial)\n    _, yhat, _ = lm._RegressionPlotter(self.df['x'], self.df['y'], x_partial=x_partial, y_partial=y_partial).fit_regression()\n    npt.assert_allclose(self.df['y'], yhat)\n"], "sample_482": ["    def test_escapeseq_safe(self):\n        output = self.engine.render_to_string(\n            \"escapeseq_safe\",\n            {\"safe_a\": [mark_safe(\"a<b\"), \"c\"], \"safe_b\": [\"d\", mark_safe(\"e>f\")]},\n        )\n        self.assertEqual(output, \"a<b, c -- d, e>f\")\n", "def test_escapeseq_safe(self):\n    self.assertEqual(self.output(\"escapeseq_safe\", {\"c\": [\"<b>bold</b>\", mark_safe(\"<i>italic</i>\")]}),\n                     \"<b>bold</b>, <i>italic</i>\")\n", "def test_escapeseq_safe(self):\n    self.assertTemplateUsed(\"escapeseq_safe\")\n    context = {\"c\": [\"<b>safe</b>\", \"<i>safe</i>\"]}\n    self.assertTemplateOutputContains(\n        \"escapeseq_safe\", \"<b>safe</b>, <i>safe</i>\", context\n    )\n", "def test_escapeseq_safe_input(self):\n    self.assertEqual(\n        self.output(\"escapeseq_safe_input\", safe_input=[\"<html>\", \"&amp;\"]),\n        mark_safe(\"<html>, &amp;\"),\n    )\n", "def test_escapeseq_safe_strings(self):\n    self.output_dict[\"c\"] = [mark_safe(\"&amp;\"), mark_safe(\"<strong>\")]\n    self.assertTemplateOutputContains(\n        \"escapeseq_safe_strings\", \"&amp;, <strong>\"\n    )\n"], "sample_852": ["def test_make_classification_random_state():\n    X1, y1 = make_classification(n_samples=10, n_features=5, random_state=0)\n    X2, y2 = make_classification(n_samples=10, n_features=5, random_state=0)\n    assert_array_equal(X1, X2)\n    assert_array_equal(y1, y2)\n", "def test_make_sparse_spd_matrix():\n    dim = 5\n    prec = make_sparse_spd_matrix(dim)\n    assert prec.shape == (dim, dim)\n    assert np.all(np.linalg.eigvals(prec) > 0)\n", "def test_make_biclusters_shape():\n    shape = (10, 20)\n    n_clusters = 3\n    X, rows, cols = make_biclusters(shape, n_clusters)\n    assert X.shape == shape\n    assert rows.shape == (n_clusters, shape[0])\n    assert cols.shape == (n_clusters, shape[1])\n", "def test_make_biclusters_shuffle():\n    n_clusters = 3\n    shape = (10, 10)\n    X, rows, cols = make_biclusters(shape, n_clusters, shuffle=False)\n    X_shuffled, _, _ = make_biclusters(shape, n_clusters, shuffle=True)\n    assert not np.array_equal(X, X_shuffled)\n", "def test_make_classification_n_features_consistency():\n    with pytest.raises(ValueError, match=\"Number of informative, redundant and repeated features\"):\n        make_classification(n_features=10, n_informative=2, n_redundant=9, n_repeated=2)\n"], "sample_436": ["def test_runserver_with_debug_false_and_no_allowed_hosts(self):\n    with self.assertRaisesMessage(CommandError, 'You must set settings.ALLOWED_HOSTS if DEBUG is False.'):\n        call_command('runserver')\n", "    def setUp(self):\n        self.write_settings(\"settings.py\")\n        self.old_settings = settings.ALLOWED_HOSTS\n        settings.ALLOWED_HOSTS = ['testserver']\n", "    def test_runserver_validation(self):\n        command = RunserverCommand()\n        # Test valid address:port pair\n        command.handle(addrport=\"127.0.0.1:8000\", use_ipv6=False, skip_checks=True)\n        self.assertEqual(command.addr, \"127.0.0.1\")\n        self.assertEqual(command.port, \"8000\")\n\n        # Test valid IPv6 address:port pair\n        if socket.has_ipv6:\n            command.handle(addrport=\"[::1]:8000\", use_ipv6=True, skip_checks=True)\n            self.assertEqual(command.addr, \"::1\")\n            self.assertEqual(command.port, \"8000\")\n\n        # Test invalid address:port pair\n        with self.assertRaises(CommandError) as cm:\n            command.handle(addrport=\"invalid:8000\", use_ipv6=False, skip_checks=True)\n        self.assertEqual(str(cm.exception), '\"invalid:8000\" is not a valid port number or address:port pair.')\n\n        # Test invalid port number\n        with self.assertRaises(CommandError) as cm:\n            command.handle(addrport=\"127.0.0.1:invalid\", use_ipv6=False, skip_checks=True)\n        self.assertEqual(str(cm.exception), \"'invalid' is not a valid port number.\")\n\n        # Test invalid IPv6 address with IPv4\n        if socket.has_ipv6:\n            with self.assertRaises(CommandError) as cm:\n                command.handle(addrport=\"[::1]:8000\", use_ipv6=False, skip_checks=True)\n            self.assertEqual(str(cm.exception), '\"[::1]\" is not a valid IPv6 address.')\n", "    def setUp(self):\n        self.old_sys_argv = sys.argv\n        sys.argv = ['manage.py', 'runserver']\n        self.old_stdout = sys.stdout\n        sys.stdout = StringIO()\n", "    def test_runserver_ipv6_address(self):\n        # Test that the runserver command can handle an IPv6 address\n        self.write_settings(\"settings.py\")\n        with mock.patch.dict(os.environ, {\"DJANGO_SETTINGS_MODULE\": \"settings\"}):\n            with self.assertRaises(CommandError):\n                # If socket.has_ipv6 is False, a CommandError should be raised\n                with mock.patch(\"socket.has_ipv6\", False):\n                    call_command(\"runserver\", \"[::1]:8000\")\n            # If socket.has_ipv6 is True, the server should start successfully\n            with mock.patch(\"socket.has_ipv6\", True):\n                with self.assertRaises(SystemExit):\n                    # The server should exit with a SystemExit\n                    call_command(\"runserver\", \"[::1]:8000\")\n"], "sample_15": ["def test_sqrt():\n    q_in = 4 * u.m ** 2\n    q_out = 2 * u.m\n    assert np.sqrt(q_in) == q_out\n", "def test_quantity_subtract():\n    # Test subtraction between Quantity objects\n    q1 = u.Quantity(10, u.m)\n    q2 = u.Quantity(5, u.m)\n    result = q1 - q2\n    assert isinstance(result, u.Quantity)\n    assert result.value == 5\n    assert result.unit == u.m\n", "def test_arithmetic_operations(tc):\n    result = tc.f(*tc.q_in)\n    assert_quantity_equal(result, tc.q_out)\n", "def test_sqrt_function():\n    q_in = 4.0 * u.m**2\n    q_out = 2.0 * u.m\n    tc = testcase(np.sqrt, q_in, q_out)\n    res = tc.f(tc.q_in)\n    assert_allclose(res, tc.q_out)\n", "def test_ufunc_atan2():\n    f = np.arctan2\n    q_in = [1 * u.m, 2 * u.m]\n    q_out = np.arctan2(1, 2) * u.rad\n    np.testing.assert_allclose(f(*q_in), q_out)\n"], "sample_534": ["def test_contour_set_alpha():\n    x = np.linspace(-3., 3., 256)\n    y = np.linspace(-3., 3., 256)\n    X, Y = np.meshgrid(x, y)\n    Z = np.sqrt(X**2 + Y**2)\n    fig, ax = plt.subplots()\n    cs = ax.contourf(X, Y, Z, alpha=0.5)\n    assert cs.get_alpha() == 0.5\n    cs.set_alpha(0.8)\n    assert cs.get_alpha() == 0.8\n", "def test_contour_zeros():\n    x = np.arange(10)\n    y = np.arange(10)\n    X, Y = np.meshgrid(x, y)\n    Z = np.zeros((10, 10))\n    plt.contourf(X, Y, Z)\n", "def test_contour_find_nearest_contour():\n    x = np.array([1, 2, 3])\n    y = np.array([4, 5, 6])\n    z = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    fig, ax = plt.subplots()\n    contours = ax.contour(x, y, z, levels=[3.5, 5.5])\n    x_nearest, y_nearest, _, _, _, d2 = contours.find_nearest_contour(2.5, 5.0)\n    assert x_nearest == 0\n    assert y_nearest == 1\n    assert d2 == 0.25\n", "def test_contour_label_rotations():\n    x, y = np.meshgrid(np.arange(0, 10, 0.25), np.arange(0, 10, 0.25))\n    z = np.sin(x) + np.cos(y)\n    fig, ax = plt.subplots()\n    cs = ax.contour(x, y, z, levels=np.arange(-1, 1, 0.2), extend='both')\n    cs.clabel(inline=True, fontsize=10, rightside_up=True, use_clabeltext=True)\n    plt.close()\n", "def test_contour_corner_mask():\n    # Test corner masking with contour.\n    x = np.array([0, 1, 1, 0])\n    y = np.array([0, 0, 1, 1])\n    z = np.array([[0, 0], [0, 1]])\n    fig, ax = plt.subplots()\n    ax.contour(x, y, z, [0.5], corner_mask=True, colors='k')\n"], "sample_271": ["    def test_watch_dir(self):\n        reloader = autoreload.get_reloader()\n        path = self.temporary_file('test_dir')\n        path.mkdir()\n        reloader.watch_dir(path, '*.py')\n        self.assertIn(path, reloader.directory_globs)\n        self.assertEqual(reloader.directory_globs[path], {'*.py'})\n", "def test_zipimport_module(self):\n    filename = self.temporary_file('zipimport_module.zip')\n    with zipfile.ZipFile(filename, 'w') as zf:\n        zf.writestr('zipimport_module/__init__.py', '')\n        zf.writestr('zipimport_module/module.py', 'def function(): pass')\n    sys.path_importer_cache.clear()\n    self.import_and_cleanup('zipimport_module.module')\n    self.assertFileFound(filename)\n", "    def test_temporary_module_import(self):\n        filename = self.temporary_file('temp_module.py')\n        filename.write_text(\"print('Hello, World!')\")\n        sys.path.insert(0, str(filename.parent))\n        self.addCleanup(sys.path.remove, str(filename.parent))\n        self.assertFileFound(filename)\n        self.import_and_cleanup('temp_module')\n", "def test_typing_modules(self):\n    # Test that typing modules are not included in the result\n    typing_modules = ['typing.io', 'typing.re']\n    for module_name in typing_modules:\n        if module_name in sys.modules:\n            continue\n        sys.modules[module_name] = types.ModuleType(module_name)\n        self.addCleanup(lambda: sys.modules.pop(module_name, None))\n    self.clear_autoreload_caches()\n    for module_name in typing_modules:\n        self.assertNotIn(module_name, list(autoreload.iter_all_python_module_files()))\n", "    def test_module_without_spec(self):\n        # Test a module without a spec\n        self.import_and_cleanup('django.test.test_module.test_module')\n        self.clear_autoreload_caches()\n        # Test uncached access\n        self.assertEqual(len(list(autoreload.iter_all_python_module_files())), len(sys.modules))\n        # Test cached access\n        self.assertEqual(len(list(autoreload.iter_all_python_module_files())), len(sys.modules))\n        self.assertEqual(autoreload.iter_modules_and_files.cache_info().hits, 1)\n"], "sample_427": ["def test_choiceformset_with_non_form_error(self):\n    \"\"\"\n    Test a formset with a non-form error.\n    \"\"\"\n    formset = self.make_choiceformset(\n        formset_class=ChoiceFormsetWithNonFormError,\n        formset_data=[(\"coffee\", 1), (\"tea\", 2)],\n    )\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(formset.non_form_errors(), [\"non-form error\"])\n", "    def test_custom_kwarg_passed_to_forms(self):\n        CustomKwargFormSet = formset_factory(CustomKwargForm)\n        formset = CustomKwargFormSet(form_kwargs={\"custom_kwarg\": \"test_value\"})\n        for form in formset.forms:\n            self.assertEqual(form.custom_kwarg, \"test_value\")\n", "    def test_custom_form_and_formset(self):\n        CustomFormSet = formset_factory(CustomKwargForm, formset=BaseFormSet, extra=2)\n        formset = CustomFormSet(custom_kwarg=\"value\")\n        for form in formset:\n            self.assertEqual(form.custom_kwarg, \"value\")\n", "    def test_formset_rendering_with_template(self):\n        formset = self.make_choiceformset([(\"coffee\", 1)])\n        renderer = get_default_renderer()\n        renderer.get_template = mock.Mock(return_value=TemplatesSetting.TEMPLATE_STRING)\n        output = str(formset.render(renderer))\n        # Test that the formset is rendered correctly with the given template.\n        # Add assertions based on the expected output.\n", "def test_formset_form_kwargs(self):\n    \"\"\"Formset passes form_kwargs to form construction.\"\"\"\n\n    class CustomKwargFormSet(formset_factory(CustomKwargForm)):\n        pass\n\n    formset = CustomKwargFormSet(form_kwargs={\"custom_kwarg\": \"test\"})\n    for form in formset:\n        self.assertEqual(form.custom_kwarg, \"test\")\n"], "sample_672": ["def test_saferepr_exception_handling():\n    class BadRepr:\n            raise ValueError(\"Failed repr\")\n\n    obj = BadRepr()\n    result = saferepr(obj)\n    assert \"ValueError raised in repr()\" in result\n", "def test_saferepr_with_maxsize():\n    class TestClass:\n            return \"TestClass\" * 100\n\n    obj = TestClass()\n    result = saferepr(obj, maxsize=20)\n    assert len(result) <= 20\n    assert \"TestClass\" in result\n    assert \"...\" in result\n", "def test_saferepr_maxsize():\n    class TestClass:\n            return \"x\" * 300\n\n    obj = TestClass()\n    result = saferepr(obj, maxsize=50)\n    assert len(result) <= 50\n", "def test_saferepr_exception_in_repr():\n    class BrokenRepr:\n            raise Exception(\"Broken repr\")\n\n    obj = BrokenRepr()\n    result = saferepr(obj)\n    assert \"<[Exception('Broken repr') raised in repr()] BrokenRepr object at 0x\" in result\n", "def test_saferepr_with_large_input():\n    class LargeObject:\n            return 'a' * 1000\n\n    result = saferepr(LargeObject(), maxsize=20)\n    assert len(result) == 20\n    assert result.startswith('a')\n    assert result.endswith('...')\n"], "sample_1066": ["def test_mathml_printing_of_Derivative():\n    expr = Derivative(sin(x), x)\n    assert mathml(expr) == \"<apply><diff/><bvar><ci>x</ci></bvar><sin><ci>x</ci></sin></apply>\"\n    assert mathml(expr, printer='presentation') == \"<mrow><mo>&dd;</mo><mi>sin</mi><mfenced><mi>x</mi></mfenced></mrow>\"\n", "compilation error", "def test_mathml_lambda():\n    expr = Lambda((x, y), x + y)\n    expected = \"<mfenced><mrow><tuple><ci>x</ci><ci>y</ci></tuple>&#x21A6;<apply><plus/><ci>x</ci><ci>y</ci></apply></mrow></mfenced>\"\n    assert mathml(expr) == expected\n", "def test_mathml_laplacian():\n    expr = Laplacian(x*sin(x))\n    result = mpp.doprint(expr)\n    expected = \"<mrow><mo>&#x2206;</mo><mrow><mi>x</mi><mo>&#xB7;</mo><mi>sin</mi><mo>&#x2061;</mo><mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow>\"\n    assert result == expected\n", "compilation error"], "sample_1042": ["def test_indexed_properties():\n    A = IndexedBase('A', shape=(m, n))\n    i, j = symbols('i j', cls=Idx)\n    A_ij = A[i, j]\n    assert A_ij.base == A\n    assert A_ij.indices == (i, j)\n    assert A_ij.rank == 2\n    assert A_ij.shape == (m, n)\n    assert A_ij.ranges == [(0, m - 1), (0, n - 1)]\n", "def test_idx_comparison():\n    i = Idx('i', 3)\n    j = Idx('j', 5)\n    k = Idx('k', 2)\n    assert (i <= j) == True\n    assert (j >= i) == True\n    assert (j <= k) == False\n    assert (k >= j) == False\n    assert (i < j) == True\n    assert (j > i) == True\n    assert (j < k) == False\n    assert (k > j) == False\n    assert (i <= i) == True\n    assert (i >= i) == True\n    assert (i < i) == False\n    assert (i > i) == False\n", "def test_idx_ordering():\n    i, j, k = symbols('i j k', integer=True)\n    assert Idx(i, 3) <= Idx(j, 3)\n    assert not (Idx(i, 3) >= Idx(j, 3))\n    assert Idx(i, 3) < Idx(j, 3)\n    assert not (Idx(i, 3) > Idx(j, 3))\n    assert Idx(i, 3) <= Idx(i, 4)\n    assert Idx(i, 3) < Idx(i, 4)\n    assert not (Idx(i, 3) >= Idx(i, 4))\n    assert not (Idx(i, 3) > Idx(i, 4))\n    assert Idx(i, (2, 5)) <= Idx(j, (3, 6))\n    assert not (Idx(i, (2, 5)) >= Idx(j, (3, 6)))\n    assert Idx(i, (2, 5)) < Idx(j, (3, 6))\n    assert not (Idx(i, (2, 5)) > Idx(j, (3, 6)))\n", "def test_idx_comparisons():\n    i = Idx('i', 3)\n    j = Idx('j', 4)\n    k = Idx('k', 5)\n\n    assert (i <= j) == True\n    assert (i < j) == True\n    assert (i >= k) == False\n    assert (i > k) == False\n", "def test_indexed_base_repr():\n    A = IndexedBase('A')\n    assert repr(A) == \"IndexedBase('A')\"\n    assert str(A) == \"A\"\n\n    B = IndexedBase('B', shape=(2, 3))\n    assert repr(B) == \"IndexedBase('B', (2, 3))\"\n    assert str(B) == \"B\"\n"], "sample_1073": ["def test_sqrt_match():\n    expr = r2 + r3 + r3*r2 + 2*r5\n    a, b, r = _sqrt_match(expr)\n    assert a == r2 + r3\n    assert b == 2\n    assert r == r5\n", "def test_sqrt_match():\n    expr = 1 + sqrt(2) + sqrt(2)*sqrt(3) + 2*sqrt(1+sqrt(5))\n    result = _sqrt_match(expr)\n    expected = [1 + sqrt(2) + sqrt(6), 2, 1 + sqrt(5)]\n    assert result == expected\n", "def test_sqrt_ratcomb():\n    z = sqrt(1+sqrt(3)) + sqrt(3+3*sqrt(3)) - sqrt(10+6*sqrt(3))\n    assert sqrtdenest(z) == 0\n", "compilation error", "def test_sqrtdenest():\n    w = r2 + r3 + r5 + (1 + r3)*sqrt(r2 + 5*r3)\n    expr = sqrt((w**2).expand())\n    result = sqrtdenest(expr)\n    expected = sqrt(6*sqrt(2) + 2*sqrt(3) + 5) + sqrt(r2 + r3 + 4)\n    assert result == expected\n"], "sample_1027": ["def test_poly():\n    expr = x*(x**2 + x - 1)**2\n    result = poly(expr)\n    expected = Poly(x**5 + 2*x**4 - x**3 - 2*x**2 + x, x, domain='ZZ')\n    assert result == expected\n", "def test_poly_from_expr():\n    expr = x*(x**2 + x - 1)**2\n    result = poly(expr)\n    assert result == Poly(x**5 + 2*x**4 - x**3 - 2*x**2 + x, x, domain='ZZ')\n", "def test_to_rational_coeffs():\n    p = Poly(((x**2-1)*(x-2)).subs({x:x*(1 + sqrt(2))}), x, domain='EX')\n    lc, r, _, g = to_rational_coeffs(p)\n    assert lc == 7 + 5*sqrt(2)\n    assert r == -2*sqrt(2) + 2\n    assert g == Poly(x**3 + x**2 - 1/4*x - 1/4, x, domain='QQ')\n    r1 = simplify(1/r)\n    assert Poly(lc*r**3*(g.as_expr()).subs({x:x*r1}), x, domain='EX') == p\n", "def test_to_rational_coeffs():\n    p = poly(((x**2 - 1)*(x - 2)).subs({x: x*(1 + sqrt(2))}), x, domain='EX')\n    lc, r, _, g = to_rational_coeffs(p)\n    assert lc == 7 + 5*sqrt(2)\n    assert r == -2*sqrt(2) + 2\n    assert g == Poly(x**3 + x**2 - 1/4*x - 1/4, x, domain='QQ')\n    r1 = simplify(1/r)\n    assert Poly(lc*r**3*(g.as_expr()).subs({x: x*r1}), x, domain='EX') == p\n\n    p = poly(((x**2 - 1)*(x - 2)).subs({x: x + sqrt(2)}), x, domain='EX')\n    lc, r, _, g = to_rational_coeffs(p)\n    assert lc == 1\n    assert r is None\n    assert g == Poly(x**3 + 3*x**2 - 4*x - 8, x, domain='QQ')\n    r1 = sqrt(2)\n    assert Poly(g.as_expr().subs({x: x - r1}), x, domain='EX') == p\n", "def test_to_rational_coeffs():\n    f = Poly(((x**2-1)*(x-2)).subs({x:x*(1 + sqrt(2))}), x, domain='EX')\n    res = to_rational_coeffs(f)\n    assert _epsilon_eq(res[0], 7 + 5*sqrt(2))\n    assert _epsilon_eq(res[1], -2*sqrt(2) + 2)\n    g = res[3]\n    assert g == Poly(x**3 + x**2 - 1/4*x - 1/4, x, domain='QQ')\n    r1 = simplify(1/res[1])\n    assert Poly(res[0]*res[1]**3*(g.as_expr()).subs({x:x*r1}), x, domain='EX') == f\n\n    f = Poly(((x**2-1)*(x-2)).subs({x:x + sqrt(2)}), x, domain='EX')\n    res = to_rational_coeffs(f)\n    assert res[0] is None\n    assert res[2] == sqrt(2)\n    g = res[3]\n    assert g == Poly(x**3 - 2*x**2 + x - 4, x, domain='QQ')\n    assert Poly(g.as_expr().subs({x:x - res[2]}), x, domain='EX') == f\n"], "sample_394": ["def test_edit_inline_model_with_datetime_field(self):\n    url = reverse(\"admin:admin_views_article_change\", args=(self.a1.pk,))\n    response = self.client.get(url)\n    self.assertEqual(response.status_code, 200)\n    post_data = self.inline_post_data.copy()\n    post_data[\"article_set-0-date_0\"] = \"2008-03-19\"\n    post_data[\"article_set-0-date_1\"] = \"12:00:00\"\n    response = self.client.post(url, post_data)\n    self.assertEqual(response.status_code, 302)\n    self.a1.refresh_from_db()\n    self.assertEqual(self.a1.date.date(), datetime.date(2008, 3, 19))\n    self.assertEqual(self.a1.date.time(), datetime.time(12, 0))\n", "def test_admin_form_readonly_fields(self):\n    # Test that readonly_fields are rendered as such in the admin form\n    url = reverse(\"admin:admin_views_article_change\", args=(self.a1.id,))\n    response = self.client.get(url)\n    readonly_field = self.get_admin_readonly_field(response, \"title\")\n    self.assertIsNotNone(readonly_field)\n    self.assertTrue(readonly_field.is_readonly)\n", "    def test_modeladmin_media(self):\n        modeladmin = ModelAdmin(User, AdminSite())\n        media = modeladmin.media\n        self.assertIn('core.js', media._js[0])\n        self.assertIn('actions.js', media._js[0])\n        self.assertIn('jquery.init.js', media._js[0])\n        # Add more assertions as needed to test other media files and properties\n", "def test_delete_view(self):\n    \"\"\"\n    Test that the delete view uses the correct template and displays\n    the correct context variables.\n    \"\"\"\n    url = reverse('admin:admin_views_article_delete', args=[self.a1.pk])\n    response = self.client.get(url)\n    self.assertTemplateUsed(response, 'admin/admin_views/article/delete_confirmation.html')\n    self.assertEqual(response.context['object'], self.a1)\n    self.assertEqual(response.context['deleted_objects'], [])\n    self.assertEqual(response.context['perms_lacking'], [])\n    self.assertEqual(response.context['protected'], [])\n", "def test_article_model_admin_actions(self):\n    # Login as a superuser\n    self.client.force_login(self.superuser)\n\n    # Get the changelist page for Article\n    response = self.client.get(reverse(\"admin:admin_views_article_changelist\"))\n    self.assertEqual(response.status_code, 200)\n\n    # Get the IDs of the articles\n    article_ids = Article.objects.values_list(\"id\", flat=True)\n\n    # Test the \"delete_selected\" action\n    data = {\n        ACTION_CHECKBOX_NAME: article_ids,\n        \"action\": \"delete_selected\",\n        \"post\": \"yes\",\n    }\n    response = self.client.post(reverse(\"admin:admin_views_article_changelist\"), data)\n    self.assertRedirects(response, reverse(\"admin:admin_views_article_changelist\"))\n    self.assertEqual(Article.objects.count(), 0)\n\n    # Create some articles for further tests\n    Article.objects.create(content=\"Test content 1\", title=\"Test article 1\")\n    Article.objects.create(content=\"Test content 2\", title=\"Test article 2\")\n\n    # Test the \"make_published\" action\n    article_ids = Article.objects.values_list(\"id\", flat=True)\n    data = {\n        ACTION_CHECKBOX_NAME: article_ids,\n        \"action\": \"make_published\",\n    }\n    response = self.client.post(reverse(\"admin:admin_views_article_changelist\"), data)\n    self.assertRedirects(response, reverse(\"admin:admin_views_article_changelist\"))\n    self.assertEqual(Article.objects.filter(published=True).count(), 2)\n"], "sample_84": ["    def test_urlencode_multivalue(self):\n        data = MultiValueDict({'name': ['John', 'Doe'], 'age': ['30']})\n        encoded = urlencode(data)\n        self.assertEqual(encoded, 'name=John&name=Doe&age=30')\n", "    def test_urlencode_with_multivaluedict(self):\n        data = MultiValueDict({'key1': ['value1', 'value2'], 'key2': ['value3']})\n        result = urlencode(data)\n        self.assertIn('key1=value1', result)\n        self.assertIn('key1=value2', result)\n        self.assertIn('key2=value3', result)\n", "    def test_urlquote(self):\n        self.assertEqual(urlquote('http://example.com/some path'), 'http%3A//example.com/some%20path')\n        self.assertEqual(urlquote('http://example.com/?q=some query'), 'http%3A//example.com/%3Fq%3Dsome%20query')\n", "    def test_urlencode_with_none_value(self):\n        with self.assertRaisesMessage(TypeError, self.cannot_encode_none_msg):\n            urlencode({'key': None})\n", "    def test_urlsafe_base64_decode(self):\n        self.assertEqual(urlsafe_base64_decode('SGVsbG8gd29ybGQ='), b'Hello world')\n        self.assertEqual(urlsafe_base64_decode('SGVsbG8gd29ybGQ'), b'Hello world')\n        self.assertEqual(urlsafe_base64_decode('SGVsbG8gd29ybGQ==='), b'Hello world')\n        with self.assertRaises(ValueError):\n            urlsafe_base64_decode('invalid')\n"], "sample_192": ["def test_formset_factory_max_num(self):\n    # Test formset_factory with max_num\n    TestForm = Form\n    TestFormSet = formset_factory(TestForm, max_num=2)\n    formset = TestFormSet()\n    self.assertEqual(formset.max_num, 2)\n", "def test_custom_kwarg_formset(self):\n    CustomKwargFormSet = formset_factory(\n        CustomKwargForm, formset=BaseFormSet, extra=2,\n        form_kwargs={'custom_kwarg': 'test_value'}\n    )\n    formset = CustomKwargFormSet()\n    for form in formset:\n        self.assertEqual(form.custom_kwarg, 'test_value')\n", "def test_custom_kwarg_formset(self):\n    class CustomKwargFormSet(formsets.BaseFormSet):\n            kwargs = super().get_form_kwargs(index)\n            kwargs['custom_kwarg'] = 'test_value'\n            return kwargs\n\n    CustomKwargFormSet = formset_factory(CustomKwargForm, formset=CustomKwargFormSet, extra=1)\n    formset = CustomKwargFormSet()\n    for form in formset:\n        self.assertEqual(form.custom_kwarg, 'test_value')\n", "    def test_clean_method(self):\n        formset_data = [('Coffee', '2'), ('Tea', '1'), ('Coffee', '3')]\n        formset = self.make_choiceformset(formset_data, formset_class=FavoriteDrinksFormSet)\n        with self.assertRaises(ValidationError) as cm:\n            formset.clean()\n        self.assertEqual(str(cm.exception), '[''You may only specify a drink once.'']')\n", "def test_formset_factory_arguments(self):\n    CustomKwargFormSet = formset_factory(\n        CustomKwargForm, extra=2, can_order=True, can_delete=True, max_num=5,\n        validate_max=True, min_num=1, validate_min=True, absolute_max=10,\n        can_delete_extra=True, formset=BaseFormSet, form_kwargs={'custom_kwarg': 'test'}\n    )\n    formset = CustomKwargFormSet()\n    self.assertEqual(formset.form.custom_kwarg, 'test')\n    self.assertTrue(formset.can_order)\n    self.assertTrue(formset.can_delete)\n    self.assertEqual(formset.max_num, 5)\n    self.assertTrue(formset.validate_max)\n    self.assertEqual(formset.min_num, 1)\n    self.assertTrue(formset.validate_min)\n    self.assertEqual(formset.absolute_max, 10)\n    self.assertTrue(formset.can_delete_extra)\n    self.assertEqual(len(formset.forms), 2)\n"], "sample_643": ["def test_colorize_ansi_with_style(reporter):\n    msg = \"Test message\"\n    style = \"bold\"\n    color = \"red\"\n    result = reporter.colorize_ansi(msg, style=style, color=color)\n    assert result == \"\\033[1;31mTest message\\033[0m\"\n", "def test_colorize_ansi_with_message_style(reporter):\n    message = \"Test message\"\n    style = (\"bold\", \"underline\")\n    color = \"green\"\n    msg_style = MessageStyle(color, style)\n    colored_message = reporter.colorize_ansi(message, msg_style)\n    assert colored_message == f\"\\033[1;4;32m{message}\\033[0m\"\n", "def test_handle_message(reporter, disable):\n    out = StringIO()\n    linter = PyLinter()\n    linter.load_default_plugins()\n    linter.load_plugin_modules(checkers.get_checker_names())\n    linter.set_reporter(reporter(out))\n    linter.config.disable = disable\n\n    msg = Message(\n        msg_id=\"W1234\",\n        line=1,\n        node=None,\n        msg=\"Test warning message\",\n        symbol=\"test-symbol\",\n        obj=\"test-object\",\n        module=\"test-module\",\n        C=\"W\",\n        category=\"Warning\",\n    )\n\n    linter.reporter.handle_message(msg)\n    output = out.getvalue().strip()\n\n    assert \"************* Module test-module\" in output\n    assert \"test-module:1: Test warning message (test-symbol) (W1234)\" in output\n", "def test_parseable_text_reporter(reporter):\n    linter = PyLinter()\n    reporter = ParseableTextReporter()\n    linter.set_reporter(reporter)\n    linter.config.reports = True\n\n    msg = Message('test_msg', 1, 'Test message', 'Test symbol', 'E', 0)\n    msg.module = 'test_module'\n    msg.path = 'test_file.py'\n    msg.line = 10\n    msg.obj = 'test_obj'\n\n    with StringIO() as out:\n        with redirect_stdout(out):\n            reporter.handle_message(msg)\n        output = out.getvalue().strip()\n\n    expected_output = \"test_file.py:10: [E(test_symbol), test_obj] Test message\"\n    assert output == expected_output\n", "def test_colorize_ansi(msg_style, expected_ansi):\n    from pylint.reporters.text import colorize_ansi, MessageStyle\n\n    msg = \"msg\"\n    assert colorize_ansi(msg, MessageStyle(*msg_style)) == expected_ansi\n"], "sample_1040": ["def test_print_AssocOp():\n    e = Basic(x, y)\n    assert mathml(e) == '<basic>\\n  <ci>x</ci>\\n  <ci>y</ci>\\n</basic>'\n", "def test_print_AssocOp():\n    class DummyAssocOp(Basic):\n        pass\n    dummy_expr = DummyAssocOp(x, y)\n    result = mp._print_AssocOp(dummy_expr)\n    assert str(result.toxml()) == \"<apply><DummyAssocOp/><ci>x</ci><ci>y</ci></apply>\"\n", "def test_print_Log():\n    expr = log(x, 2)\n    result = mp.doprint(expr)\n    assert result == '<apply><log/><ci>x</ci><cn>2</cn></apply>'\n    result = mpp.doprint(expr)\n    assert result == '<mrow><msub><mi>log</mi><mn>2</mn></msub><mo>&thinsp;</mo><mfenced><mi>x</mi></mfenced></mrow>'\n", "def test_print_Matrix():\n    A = Matrix([[1, 2], [3, 4]])\n    assert mathml(A) == '<matrix><matrixrow><cn>1</cn><cn>2</cn></matrixrow><matrixrow><cn>3</cn><cn>4</cn></matrixrow></matrix>'\n    assert mathml(A, printer='presentation') == '<mfenced><mtable><mtr><mtd><mn>1</mn></mtd><mtd><mn>2</mn></mtd></mtr><mtr><mtd><mn>3</mn></mtd><mtd><mn>4</mn></mtd></mtr></mtable></mfenced>'\n", "def test_mathml_content_printer():\n    expr = Integral(sin(x), (x, 0, pi))\n    expected_output = '<apply><int/><bvar><ci>x</ci></bvar><lowlimit><cn>0</cn></lowlimit><uplimit><pi/></uplimit><apply><sin/><ci>x</ci></apply></apply>'\n    assert mathml(expr) == expected_output\n"], "sample_581": ["def test_add_url_rule_with_invalid_endpoint(app, blueprint):\n    with pytest.raises(ValueError) as e:\n        blueprint.add_url_rule(\"/test\", endpoint=\"invalid.endpoint\", view_func=lambda: None)\n    assert str(e.value) == \"'endpoint' may not contain a dot '.' character.\"\n", "def test_blueprint_url_defaults(app):\n    bp = flask.Blueprint('test', __name__, url_defaults={'user': 'default'})\n\n    @bp.route('/user/<user>')\n        return user\n\n    app.register_blueprint(bp)\n    client = app.test_client()\n\n    response = client.get('/user/test_user')\n    assert response.data == b'test_user'\n\n    response = client.get('/user/')\n    assert response.data == b'default'\n", "def test_add_url_rule_valid_input(app):\n    bp = flask.Blueprint('test_bp', __name__)\n\n    @bp.add_url_rule('/test', endpoint='test_endpoint', view_func=lambda: None)\n        pass\n\n    app.register_blueprint(bp)\n\n    assert 'test_bp.test_endpoint' in app.view_functions\n", "def test_blueprint_url_prefix(app):\n    bp = flask.Blueprint(\"test_bp\", __name__)\n\n    @bp.route(\"/\")\n        return \"Test\"\n\n    app.register_blueprint(bp, url_prefix=\"/prefix\")\n\n    with app.test_client() as client:\n        response = client.get(\"/prefix/\")\n        assert response.status_code == 200\n        assert response.get_data(as_text=True) == \"Test\"\n", "def test_blueprint_url_defaults(app):\n    bp = flask.Blueprint('test', __name__, url_defaults={'lang': 'en'})\n\n    @bp.route('/<lang>/path')\n        return flask.request.path\n\n    app.register_blueprint(bp)\n\n    with app.test_request_context():\n        assert flask.url_for('test.test_route') == '/en/path'\n"], "sample_993": ["def test_free_group_contains():\n    assert x in F\n    assert y in F\n    assert z in F\n    assert x**2 not in F\n    assert x**2 * y in F\n    assert x * y**-1 in F\n    G, a, b = free_group(\"a, b\")\n    assert a not in G\n    assert b not in G\n    assert x not in G\n    assert y not in G\n", "def test_free_group_element_order():\n    # Test order of a free group element\n    f, a, b = free_group(\"a b\")\n    assert (a**2 * b * b**-1 * a**-2).order() == 1\n    assert (a**2 * b * b**-4 * a**2).order() == oo\n", "def test_free_group_elements():\n    # Test the creation of FreeGroup elements\n    assert (x * y ** 2 * y ** -4) == x * y ** -2\n    assert (z * y ** -2) == z * y ** -2\n    assert (x ** 2 * y * y ** -1 * x ** -2) == F.identity\n\n    # Test the order of FreeGroup elements\n    assert (x ** 2 * y * y ** -1 * x ** -2).order() == 1\n\n    # Test the commutator of FreeGroup elements\n    assert (x * y).commutator(y * x) == F.identity\n\n    # Test the elimination of words in FreeGroup elements\n    assert (x ** 5 * y * x ** 2 * y ** -4 * x).eliminate_word(x, x ** 2) == x ** 10 * y * x ** 4 * y ** -4 * x ** 2\n    assert (x ** 5 * y * x ** 2 * y ** -4 * x).eliminate_word(y, y ** -1) == x ** 5 * y ** -11\n\n    # Test the length of FreeGroup elements\n    assert len(x ** 5 * y * x ** 2 * y ** -4 * x) == 13\n\n    # Test the equality of FreeGroup elements\n    assert x * y ** 2 != y ** 2 * x\n    assert x ** 0 == F.identity\n\n    # Test the ordering of FreeGroup elements\n    assert y ** 2 > x ** 2\n    assert y * z > z * y\n    assert x > x.inverse()\n\n    # Test the exponent sum of FreeGroup elements\n    assert (x ** 2 * y ** 3).exponent_sum(x) == 2\n    assert (x ** 2 * y ** 4 * x ** -3).exponent_sum(x) == -1\n\n    # Test the generator count of FreeGroup elements\n    assert (x ** 2 * y ** 3).generator_count(x) == 2\n    assert (x ** 2 * y ** 4 * x", "def test_free_group_order():\n    F, x = free_group(\"x\")\n    assert F.order() == oo\n    assert F[0].order() == 1\n", "def test_free_group_init():\n    F, a, b = free_group(\"a, b\")\n    assert isinstance(F, FreeGroup)\n    assert a.group == F\n    assert b.group == F\n    assert F.symbols == (Symbol('a'), Symbol('b'))\n    assert F.rank == 2\n"], "sample_187": ["    def test_unescape_entities(self):\n        self.assertEqual(text.unescape_entities('&lt;tag&gt;'), '<tag>')\n        self.assertEqual(text.unescape_entities('&quot;Quoted&quot;'), '\"Quoted\"')\n        self.assertEqual(text.unescape_entities('&#34;Quoted&#34;'), '\"Quoted\"')\n        self.assertEqual(text.unescape_entities('&#x22;Quoted&#x22;'), '\"Quoted\"')\n        self.assertEqual(text.unescape_entities('&invalid;'), '&invalid;')\n", "def test_truncator_words(self):\n    # Test words truncation\n    t = text.Truncator(\"This is a test string with several words.\")\n    self.assertEqual(t.words(4), \"This is a test\u2026\")\n    self.assertEqual(t.words(4, truncate='...'), \"This is a test...\")\n    self.assertEqual(t.words(100), \"This is a test string with several words.\")\n    self.assertEqual(t.words(0), \"\")\n", "def test_unescape_entities(self):\n    self.assertEqual(text.unescape_entities('&amp;lt;'), '<')\n    self.assertEqual(text.unescape_entities('&lt;'), '<')\n    self.assertEqual(text.unescape_entities('&#60;'), '<')\n    self.assertEqual(text.unescape_entities('&quot;'), '\"')\n    self.assertEqual(text.unescape_entities('&unknown;'), '&unknown;')\n", "    def test_truncator_chars(self):\n        truncator = text.Truncator('This is a long text that needs to be truncated.')\n        result = truncator.chars(15)\n        self.assertEqual(result, 'This is a long\u2026')\n", "    def test_unescape_entities(self):\n        with ignore_warnings(category=RemovedInDjango40Warning):\n            self.assertEqual(text.unescape_entities('&amp;'), '&')\n            self.assertEqual(text.unescape_entities('&lt;'), '<')\n            self.assertEqual(text.unescape_entities('&gt;'), '>')\n            self.assertEqual(text.unescape_entities('&quot;'), '\"')\n            self.assertEqual(text.unescape_entities('&apos;'), \"'\")\n            self.assertEqual(text.unescape_entities('&#34;'), '\"')\n            self.assertEqual(text.unescape_entities('&#x22;'), '\"')\n            self.assertEqual(text.unescape_entities('Invalid entity &invalid;'), 'Invalid entity &invalid;')\n"], "sample_103": ["    def test_stddev(self):\n        result = Author.objects.aggregate(stddev_age=StdDev('age'))\n        self.assertIsInstance(result['stddev_age'], float)\n\n        result = Author.objects.aggregate(stddev_age_sample=StdDev('age', sample=True))\n        self.assertIsInstance(result['stddev_age_sample'], float)\n", "def test_aggregate_with_filter(self):\n    query = Book.objects.filter(rating__gt=4.0).aggregate(avg_price=Avg('price'))\n    self.assertEqual(query['avg_price'], Decimal('30.00'))\n", "    def test_variance(self):\n        # Test Variance aggregate with sample and population\n        avg_age = Author.objects.aggregate(avg_age=Avg('age'))['avg_age']\n        sample_variance = Author.objects.aggregate(variance=Variance('age', sample=True))['variance']\n        population_variance = Author.objects.aggregate(variance=Variance('age'))['variance']\n\n        # Calculate variance manually for comparison\n        squared_diff_sum = Author.objects.annotate(sq_diff=((F('age') - avg_age) ** 2)).aggregate(sum=Sum('sq_diff'))['sum']\n        sample_variance_manual = squared_diff_sum / (Author.objects.count() - 1)\n        population_variance_manual = squared_diff_sum / Author.objects.count()\n\n        self.assertEqual(sample_variance, sample_variance_manual)\n        self.assertEqual(population_variance, population_variance_manual)\n", "def test_aggregate_filter(self):\n    queryset = Book.objects.filter(pubdate__year=2008)\n    count = queryset.aggregate(count=Count('id', filter=Q(price__gt=Decimal('30.00'))))\n    self.assertEqual(count['count'], 1)\n", "    def test_variance(self):\n        result = Author.objects.aggregate(variance=Variance('age'))\n        self.assertIsInstance(result['variance'], float)\n        # Test with sample=True\n        result_sample = Author.objects.aggregate(variance_sample=Variance('age', sample=True))\n        self.assertIsInstance(result_sample['variance_sample'], float)\n        # Test with filter\n        result_filter = Author.objects.filter(age__gt=30).aggregate(variance_filter=Variance('age'))\n        self.assertIsInstance(result_filter['variance_filter'], float)\n        # Test with annotation\n        result_annotation = Author.objects.annotate(variance_annotation=Variance('age')).first()\n        self.assertIsInstance(result_annotation.variance_annotation, float)\n"], "sample_983": ["def test_solve_least_squares():\n    A = SparseMatrix([[1, 2], [2, 3], [3, 4]])\n    rhs = Matrix([8, 14, 18])\n    xy = A.solve_least_squares(rhs)\n    assert (A * xy - rhs).norm().n(2) < 1e-6\n", "def test_scalar_multiply():\n    M = SparseMatrix([[1, 0], [0, 2]])\n    assert M.scalar_multiply(2) == SparseMatrix([[2, 0], [0, 4]])\n    assert M.scalar_multiply(0) == SparseMatrix([[0, 0], [0, 0]])\n", "def test_solve_least_squares():\n    A = SparseMatrix([[1, 2], [2, 3], [3, 4]])\n    rhs = Matrix([8, 14, 18])\n    solution = A.solve_least_squares(rhs)\n    assert (A * solution - rhs).norm().n(2) < 1e-10\n", "def test_sparse_matrix_cholesky_solve():\n    A = SparseMatrix(((25, 15, -5), (15, 18, 0), (-5, 0, 11)))\n    b = SparseMatrix(((1), (2), (3)))\n    L = A.cholesky()\n    y = L._lower_triangular_solve(b)\n    x = L.T._upper_triangular_solve(y)\n    assert A * x == b\n", "def test_solve_least_squares():\n    A = SparseMatrix([[1, 0, 3, 2], [0, 0, 1, 0], [4, 0, 0, 5], [0, 6, 7, 0]])\n    b = SparseMatrix([2, 1, 3, 5])\n    x = A.solve_least_squares(b)\n    assert A * x == b\n"], "sample_60": ["def test_generic_admin_view(self):\n    response = self.client.get(reverse('admin:generic_inline_admin_episode_change', args=[self.episode_pk]))\n    self.assertEqual(response.status_code, 200)\n    self.assertContains(response, 'http://example.com/podcast.mp3')\n    self.assertContains(response, 'http://example.com/logo.png')\n", "    def test_media_inline_changelist(self):\n        url = reverse('admin:generic_inline_admin_episode_changelist')\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n        self.assertContains(response, 'This Week in Django')\n", "def test_media_inline_change_view(self):\n    url = reverse('admin:generic_inline_admin_episode_change', args=[self.episode_pk])\n    response = self.client.get(url)\n    self.assertEqual(response.status_code, 200)\n    self.assertContains(response, 'formset id=\"media_set-0\"')\n    self.assertContains(response, 'formset id=\"media_set-1\"')\n", "def test_change_view_with_generic_inline(self):\n    url = reverse('admin:generic_inline_admin_episode_change', args=[self.episode_pk])\n    response = self.client.get(url)\n\n    self.assertEqual(response.status_code, 200)\n    self.assertContains(response, 'generic_inline_admin/episode/change_form.html')\n    self.assertContains(response, 'contenttypes/generic_tabular_inline.html')\n    self.assertContains(response, 'id=\"media_set-0-url\"')\n    self.assertContains(response, 'id=\"media_set-1-url\"')\n", "    def setUp(self):\n        self.client.force_login(self.superuser)\n        self.category = Category.objects.create(name='Test Category')\n        self.episode = EpisodePermanent.objects.create(name='Test Episode', category=self.category)\n        self.media = MediaPermanent.objects.create(episode=self.episode, url='http://example.com/media.mp3')\n"], "sample_1204": ["def test_coset_init():\n    a = Permutation(1, 2)\n    b = Permutation(0, 1)\n    G = PermutationGroup([a, b])\n    cst = Coset(a, G, dir=\"+\")\n    assert cst.is_right_coset\n    assert not cst.is_left_coset\n", "def test_coset_as_list():\n    a = Permutation(1, 2)\n    b = Permutation(0, 1)\n    G = PermutationGroup([a, b])\n    cst = Coset(a, G, dir=\"-\")\n    assert cst.as_list() == [a*b**i for i in range(6)]\n", "def test_perm_group_elements():\n    a = Permutation(1, 2, 3)\n    b = Permutation(0, 1)\n    G = PermutationGroup([a, b])\n    elements = list(G.elements)\n    assert len(elements) == 6\n    assert Permutation(1, 2, 3) in elements\n    assert Permutation(1, 3, 2) in elements\n    assert Permutation(0, 2, 1) in elements\n    assert Permutation(0, 1, 2) in elements\n    assert Permutation(2, 0, 1) in elements\n    assert Permutation(2, 1, 0) in elements\n", "def test_coset_init():\n    a = Permutation(1, 2)\n    b = Permutation(0, 1)\n    G = PermutationGroup([a, b])\n    cst = Coset(a, G)\n    assert cst.is_right_coset\n    assert a in cst\n    assert b in cst\n    assert Permutation(0, 2) in cst\n", "def test_coset_contains():\n    a = Permutation(1, 2)\n    b = Permutation(0, 1)\n    G = PermutationGroup([a, b])\n    cst = Coset(a, G, dir=\"+\")\n    assert a * b in cst\n    assert b * a not in cst\n"], "sample_432": ["def test_admin_view_custom_pagination(self):\n    url = reverse(\"admin:admin_changelist_custompagination_changelist\")\n    request = self._mocked_authenticated_request(url, self.superuser)\n    response = CustomPaginationAdmin(CustomPagination, custom_site).changelist_view(request)\n    self.assertEqual(response.status_code, 200)\n    self.assertIsInstance(response.context_data['cl'].paginator, CustomPaginator)\n", "def test_unauthenticated_user(self):\n    request = self.factory.get('/admin/myapp/mymodel/')\n    response = custom_site.admin_view(self.modeladmin.changelist_view)(request)\n    self.assertEqual(response.status_code, 302)  # Expect a redirect to the login page\n", "def test_get_changelist_instance_with_invalid_lookup_parameters(self):\n    request = self._mocked_authenticated_request('/admin/changelist/child/?name__invalid=test', self.superuser)\n    with self.assertRaises(IncorrectLookupParameters):\n        ChildAdmin(model=Child, admin_site=admin.site).get_changelist_instance(request)\n", "def test_get_changelist_instance(self):\n    user = self._create_superuser('testuser')\n    request = self._mocked_authenticated_request('/admin/admin_changelist/child/', user)\n    model_admin = ChildAdmin(Child, admin.site)\n    changelist = model_admin.get_changelist_instance(request)\n    self.assertIsNotNone(changelist)\n    self.assertEqual(changelist.list_display, ('name',))\n    self.assertEqual(changelist.list_display_links, ('name',))\n    self.assertEqual(changelist.list_filter, ('parent__name',))\n    self.assertIsNone(changelist.date_hierarchy)\n    self.assertEqual(changelist.search_fields, ('parent__name',))\n    self.assertEqual(changelist.list_select_related, False)\n    self.assertEqual(changelist.list_per_page, 100)\n    self.assertEqual(changelist.list_max_show_all, 200)\n    self.assertEqual(changelist.list_editable, ())\n", "def test_result_list_with_mocked_authenticated_user(self):\n    url = '/admin/admin_changelist/band/'\n    user = self._create_superuser(\"user1\")\n    request = self._mocked_authenticated_request(url, user)\n    ma = BandAdmin(Band, custom_site)\n    cl = ma.get_changelist_instance(request)\n    cl.get_results(request)\n    self.assertEqual(len(cl.result_list), Band.objects.count())\n"], "sample_762": ["def test_first_and_last_element():\n    arr = np.array([1, 2, 3, 4, 5])\n    first, last = _first_and_last_element(arr)\n    assert_equal(first, 1)\n    assert_equal(last, 5)\n\n    sparse_arr = sp.csr_matrix(arr)\n    first, last = _first_and_last_element(sparse_arr)\n    assert_equal(first, 1)\n    assert_equal(last, 5)\n\n    # Test with a 2D array\n    arr_2d = np.array([[1, 2], [3, 4]])\n    first, last = _first_and_last_element(arr_2d)\n    assert_equal(first, 1)\n    assert_equal(last, 4)\n", "def test_base_estimator_get_params():\n    # Test get_params method of BaseEstimator\n    estimator = MyEstimator(l1=1, empty=None)\n    params = estimator.get_params()\n    assert_dict_equal(params, {'l1': 1, 'empty': None})\n\n    # Test get_params method with nested estimators\n    nested_estimator = Pipeline([('svc', SVC(C=1)), ('tree', DecisionTreeClassifier())])\n    params = nested_estimator.get_params()\n    assert_dict_equal(params, {'svc': SVC(C=1), 'svc__C': 1, 'svc__kernel': 'rbf', 'svc__degree': 3,\n                               'svc__gamma': 'scale', 'svc__coef0': 0.0, 'svc__shrinking': True,\n                               'svc__probability': False, 'svc__tol': 0.001, 'svc__cache_size': 200,\n                               'svc__class_weight': None, 'svc__verbose': False, 'svc__max_iter': -1,\n                               'svc__decision_function_shape': 'ovr', 'svc__random_state': None,\n                               'tree': DecisionTreeClassifier(), 'tree__criterion': 'gini',\n                               'tree__splitter': 'best', 'tree__max_depth': None, 'tree__min_samples_split': 2,\n                               'tree__min_samples_leaf': 1, 'tree__min_weight_fraction_leaf': 0.0,\n                               'tree__max_features': None, 'tree__random_state': None,\n                               'tree__max_leaf_nodes': None, 'tree__min_impurity_decrease': 0.0,\n                               'tree__class_weight': None})\n", "def test_set_params():\n    # Test the set_params method\n    est = MyEstimator(l1=1, empty=None)\n    est.set_params(l1=2)\n    assert_equal(est.l1, 2)\n    assert_equal(est.empty, None)\n\n    # Test nested objects\n    est = Pipeline([('svc', SVC())])\n    est.set_params(svc__C=10)\n    assert_equal(est.named_steps['svc'].C, 10)\n", "def test_clone_with_array_like_params():\n    estimator = MyEstimator(l1=np.array([1, 2, 3]), empty=np.array([]))\n    estimator_clone = clone(estimator)\n    assert_array_equal(estimator.l1, estimator_clone.l1)\n    assert_array_equal(estimator.empty, estimator_clone.empty)\n    assert_false(estimator.l1 is estimator_clone.l1)\n    assert_false(estimator.empty is estimator_clone.empty)\n", "def test_base_estimator_get_params_empty():\n    est = BaseEstimator()\n    assert_dict_equal(est.get_params(), {})\n"], "sample_536": ["def test_button_callback(ax):\n    clicked = False\n\n        nonlocal clicked\n        clicked = True\n\n    button = widgets.Button(ax, 'Test')\n    button.on_clicked(callback)\n    do_event(button, 'button_press_event')\n    do_event(button, 'button_release_event')\n    assert clicked\n", "def test_span_selector(ax):\n    x = np.arange(10)\n    y = np.sin(x)\n    line, = ax.plot(x, y)\n\n        selected_indices = np.where((x >= vmin) & (x <= vmax))[0]\n        assert_allclose(line.get_xdata()[selected_indices], x[selected_indices])\n        assert_allclose(line.get_ydata()[selected_indices], y[selected_indices])\n\n    span = widgets.SpanSelector(ax, onselect, 'horizontal', minspan=3)\n\n    # Simulate a selection event\n    press_event = mock_event('button_press_event', x=3, y=0, button=1)\n    release_event = mock_event('button_release_event', x=7, y=0, button=1)\n    do_event(span, press_event)\n    do_event(span, release_event)\n", "def test_button_constructor(ax):\n    callback = mock.Mock()\n    button = widgets.Button(ax, 'Click me', callback)\n    assert button.label.get_text() == 'Click me'\n    assert button.ax.texts[0] == button.label\n    assert button.active\n    assert button.hovercolor is None\n    button.disconnect(button.cid)\n    assert not button.active\n\n    with pytest.raises(ValueError):\n        widgets.Button(ax, 'Click me', callback, hovercolor='invalid')\n", "def test_lasso_callback(ax):\n    callback = mock.Mock()\n    lasso = widgets.Lasso(ax, (0.5, 0.5), callback)\n\n    # Simulate mouse drag events\n    events = [\n        mock_event('motion_notify_event', xdata=0.6, ydata=0.6),\n        mock_event('motion_notify_event', xdata=0.7, ydata=0.7),\n        mock_event('motion_notify_event', xdata=0.6, ydata=0.7),\n    ]\n    for event in events:\n        do_event(lasso, event)\n\n    # Simulate mouse release event\n    do_event(lasso, mock_event('button_release_event', xdata=0.6, ydata=0.6, button=1))\n\n    # Check that the callback was called with the correct vertices\n    expected_verts = [(0.5, 0.5), (0.6, 0.6), (0.7, 0.7), (0.6, 0.7), (0.6, 0.6)]\n    callback.assert_called_once_with(expected_verts)\n", "def test_rectangle_selector_release_outside(fig, ax):\n    # Test that the rectangle selector can be released outside the axes\n    # without clearing the selector.\n    onselect = mock.Mock()\n    rect_selector = widgets.RectangleSelector(ax, onselect, interactive=True)\n    event_press = mock_event('button_press_event', button=1, x=10, y=10)\n    event_release = mock_event('button_release_event', button=1, x=20, y=20)\n    event_release_outside = mock_event('button_release_event', button=1, x=-10, y=-10)\n    click_and_drag(fig, event_press, event_release, rect_selector)\n    onselect.assert_called_once()\n    click_and_drag(fig, event_press, event_release_outside, rect_selector)\n    assert len(onselect.mock_calls) == 2\n"], "sample_619": ["def test_decode_cf_datetime_invalid_calendar(num_dates, units, calendar):\n    if calendar in _NON_STANDARD_CALENDARS:\n        with pytest.raises(ValueError):\n            decode_cf_datetime(num_dates, units, calendar)\n", "def test_decode_cf_datetime_invalid_calendar(num_dates, units, calendar):\n    with pytest.raises(ValueError):\n        decode_cf_datetime(num_dates, units, \"invalid_calendar\")\n", "def test_decode_cf_datetime_with_time_range(num_dates, units, calendar):\n    decoded_dates = decode_cf_datetime(num_dates, units, calendar)\n    assert isinstance(decoded_dates, np.ndarray)\n    assert np.issubdtype(decoded_dates.dtype, np.datetime64)\n\n    expected_shape = num_dates.shape\n    assert decoded_dates.shape == expected_shape\n", "def test_decode_cf_datetime_with_non_standard_calendar_and_cftime_unavailable(\n    num_dates, units, calendar", "def test_cf_datetime_nonstandard_calendars(num_dates, units, calendar):\n    date_type = _all_cftime_date_types()[calendar]\n    expected_dates = cftime_range(units, calendar=calendar, periods=len(num_dates))\n    decoded_dates = decode_cf_datetime(num_dates, units, calendar=calendar)\n    assert_array_equal(decoded_dates, expected_dates)\n"], "sample_819": ["def test_voting_classifier_set_params():\n    # Test set_params method of VotingClassifier\n    clf1 = LogisticRegression()\n    clf2 = RandomForestClassifier()\n    eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2)])\n    eclf.set_params(rf=None)\n    assert len(eclf.estimators) == 1\n    assert eclf.estimators[0][0] == 'lr'\n", "def test_voting_classifier_multilabel():\n    X, Y = make_multilabel_classification(random_state=1)\n    eclf = VotingClassifier(estimators=[\n        ('lr', LogisticRegression(solver='lbfgs', multi_class='multinomial', random_state=1)),\n        ('rf', RandomForestClassifier(n_estimators=50, random_state=1)),\n        ('gnb', GaussianNB())\n    ], voting='hard')\n\n    assert_raise_message(NotImplementedError,\n                         \"Multilabel and multi-output classification is not supported.\",\n                         eclf.fit, X, Y)\n", "def test_voting_classifier_with_sample_weights():\n    \"\"\"Test VotingClassifier with sample weights\"\"\"\n    clf1 = LogisticRegression(solver='lbfgs', multi_class='multinomial', random_state=1)\n    clf2 = RandomForestClassifier(n_estimators=50, random_state=1)\n    clf3 = GaussianNB()\n\n    X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n    y = np.array([1, 1, 1, 2, 2, 2])\n    sample_weight = np.array([1, 1, 2, 1, 1, 2])\n\n    eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard')\n    eclf.fit(X, y, sample_weight=sample_weight)\n\n    # Test that sample weights are correctly used in fitting\n    assert_array_almost_equal(eclf.named_estimators_['lr'].coef_,\n                              clf1.fit(X, y, sample_weight=sample_weight).coef_)\n    assert_array_equal(eclf.named_estimators_['rf'].feature_importances_,\n                       clf2.fit(X, y, sample_weight=sample_weight).feature_importances_)\n    assert_array_equal(eclf.named_estimators_['gnb'].class_count_,\n                       clf3.fit(X, y, sample_weight=sample_weight).class_count_)\n", "def test_voting_classifier_with_none_estimator():\n    clf1 = LogisticRegression(solver='lbfgs', multi_class='multinomial', random_state=1)\n    clf2 = RandomForestClassifier(n_estimators=50, random_state=1)\n    clf3 = GaussianNB()\n    eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard')\n    eclf.set_params(rf=None)\n\n    assert len(eclf.estimators) == 2\n    assert eclf.estimators[0][0] == 'lr'\n    assert eclf.estimators[1][0] == 'gnb'\n", "def test_voting_classifier_weights():\n    clf1 = LogisticRegression(solver='lbfgs', multi_class='multinomial', random_state=1)\n    clf2 = RandomForestClassifier(n_estimators=50, random_state=1)\n    clf3 = GaussianNB()\n    X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n    y = np.array([1, 1, 1, 2, 2, 2])\n    eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n                            voting='soft', weights=[2, 1, 1], flatten_transform=True)\n    eclf = eclf.fit(X, y)\n    assert_array_equal(eclf.predict(X), [1, 1, 1, 2, 2, 2])\n    assert_equal(eclf.transform(X).shape, (6, 6))\n"], "sample_446": ["def test_floatformat_with_arg(self):\n    self.engine.assert_output(\n        self.engine.render_to_string(\"floatformat02\", {\"c\": 34.23234, \"d\": 34.00000, \"e\": 34.26000}),\n        \"34.232 34.000 34.260\",\n    )\n", "    def test_floatformat_with_positive_arg(self):\n        self.assertEqual(self.engine.render_to_string(\"floatformat01\", {\"a\": 34.23234, \"b\": 34.00000}), \"34.23 34.000\")\n", "def test_floatformat_with_precision(self):\n    with translation.override(\"en\"):\n        result = floatformat(Decimal(\"1234.5678\"), \"2\")\n        self.assertEqual(result, \"1,234.57\")\n", "    def test_floatformat(self):\n        output = self.engine.render_to_string(\n            \"floatformat01\", {\"a\": 34.23234, \"b\": 34.00000}\n        )\n        self.assertEqual(output, \"34.2 34\")\n\n        output = self.engine.render_to_string(\n            \"floatformat02\", {\"c\": 6666.6666, \"d\": 10000}\n        )\n        self.assertEqual(output, \"6666.7 10000.00\")\n", "def test_floatformat_with_arg(self):\n    context = {\"a\": Decimal(\"34.23234\"), \"b\": Decimal(\"34.00000\")}\n    rendered = self.engine.render_to_string(\"floatformat01\", context)\n    self.assertIn(\"34.2\", rendered)\n    self.assertIn(\"34\", rendered)\n"], "sample_350": ["def test_union_with_empty_queryset(self):\n    qs1 = Number.objects.filter(num__lt=5)\n    qs2 = Number.objects.none()\n    union_qs = qs1.union(qs2)\n    self.assertNumbersEqual(union_qs, [Number(num=i) for i in range(5)])\n", "def test_union(self):\n    even_numbers = Number.objects.filter(num__mod=2)\n    odd_numbers = Number.objects.filter(num__mod=1)\n    all_numbers = Number.objects.all()\n    self.assertNumbersEqual(even_numbers.union(odd_numbers), all_numbers)\n", "def test_union_order_by(self):\n    qs1 = Number.objects.filter(num__lt=5).order_by('num')\n    qs2 = Number.objects.filter(num__gte=5).order_by('-num')\n    union_qs = qs1.union(qs2)\n    self.assertNumbersEqual(union_qs, [0, 1, 2, 3, 4, 9, 8, 7, 6, 5], ordered=True)\n", "def test_union_with_different_model_raises_error(self):\n    with self.assertRaisesMessage(TypeError, \"Union queries with a different model are not supported.\"):\n        Number.objects.filter(num__gt=5).union(Celebrity.objects.all())\n", "def test_union_with_extra(self):\n    qs1 = Number.objects.filter(num__gte=5).extra(select={'double_num': 'num * 2'})\n    qs2 = Number.objects.filter(num__lt=5).extra(select={'double_num': 'num * 2'})\n    result = qs1.union(qs2)\n    expected_numbers = [\n        Number(num=0, other_num=10, double_num=0),\n        Number(num=1, other_num=9, double_num=2),\n        Number(num=2, other_num=8, double_num=4),\n        Number(num=3, other_num=7, double_num=6),\n        Number(num=4, other_num=6, double_num=8),\n        Number(num=5, other_num=5, double_num=10),\n        Number(num=6, other_num=4, double_num=12),\n        Number(num=7, other_num=3, double_num=14),\n        Number(num=8, other_num=2, double_num=16),\n        Number(num=9, other_num=1, double_num=18),\n    ]\n    self.assertNumbersEqual(result, expected_numbers, ordered=False)\n"], "sample_845": ["def test_tfidf_vectorizer_inverse_transform():\n    vectorizer = TfidfVectorizer()\n    X = vectorizer.fit_transform(ALL_FOOD_DOCS)\n    transformed_docs = vectorizer.inverse_transform(X)\n    for i in range(len(ALL_FOOD_DOCS)):\n        assert set(ALL_FOOD_DOCS[i].split()) == set(transformed_docs[i])\n", "def test_tfidf_vectorizer_inverse_transform():\n    vectorizer = TfidfVectorizer()\n    X = vectorizer.fit_transform(ALL_FOOD_DOCS)\n    transformed_texts = vectorizer.inverse_transform(X)\n    assert len(transformed_texts) == len(ALL_FOOD_DOCS)\n    for text, transformed_text in zip(ALL_FOOD_DOCS, transformed_texts):\n        for word in transformed_text:\n            assert word in text\n", "def test_tfidf_vectorizer_binary_output():\n    vectorizer = TfidfVectorizer(binary=True)\n    X = vectorizer.fit_transform(ALL_FOOD_DOCS)\n    assert (X != 0).sum() == X.shape[0]  # all output values should be 1\n", "def test_vectorizer_with_custom_analyzer():\n    vectorizer = CountVectorizer(analyzer=lazy_analyze)\n    X = vectorizer.fit_transform(ALL_FOOD_DOCS)\n    assert X.shape == (11, 1)\n    assert_array_equal(X.toarray().ravel(), [1] * 11)\n", "def test_hashing_vectorizer_with_custom_analyzer():\n    # Create a custom analyzer that returns a list of tokens\n        return text.split()\n\n    # Initialize a HashingVectorizer with the custom analyzer\n    vectorizer = HashingVectorizer(analyzer=custom_analyzer, n_features=10)\n\n    # Fit and transform the vectorizer with a sample corpus\n    corpus = [\n        'This is the first document.',\n        'This document is the second document.',\n        'And this is the third one.',\n        'Is this the first document?',\n    ]\n    X = vectorizer.fit_transform(corpus)\n\n    # Check the shape of the resulting matrix\n    assert X.shape == (4, 10)\n"], "sample_484": ["    def test_right_function(self):\n        john = Author.objects.get(name=\"John Smith\")\n        self.assertEqual(Right(Lower('name'), 5).resolve_expression(john), 'mith')\n        rhonda = Author.objects.get(name=\"Rhonda\")\n        self.assertEqual(Right(Lower('name'), 5).resolve_expression(rhonda), 'honda')\n        self.assertEqual(Right('name', 5).resolve_expression(john), 'Smith')\n", "    def test_right_function(self):\n        john = Author.objects.get(name=\"John Smith\")\n        self.assertEqual(Right(Lower(\"name\"), 4).resolve_expression(john), 'smith')\n        self.assertEqual(Right(\"alias\", 3).resolve_expression(john), 'ith')\n        self.assertEqual(Right(\"name\", 3).resolve_expression(john), 'ith')\n        self.assertEqual(Right(\"name\", 10).resolve_expression(john), 'John Smith')\n        self.assertEqual(Right(\"name\", Value(5)).resolve_expression(john), 'Smith')\n", "    def test_right_function(self):\n        john = Author.objects.get(name=\"John Smith\")\n        self.assertEqual(Right(Value(john.name), 5).as_sql(connection.ops.connection, connection.vendor), ('RIGHT(%%s, 5)', ['John Smith']))\n\n        rhonda = Author.objects.get(name=\"Rhonda\")\n        self.assertEqual(Right(Value(rhonda.name), 5).as_sql(connection.ops.connection, connection.vendor), ('RIGHT(%%s, 5)', ['Rhonda']))\n\n        self.assertEqual(Right(Lower(Value('ABCDEF')), 3).as_sql(connection.ops.connection, connection.vendor), ('RIGHT(LOWER(%%s), 3)', ['ABCDEF']))\n\n        self.assertEqual(Right(Length(Value('ABCDEF')), 1).as_sql(connection.ops.connection, connection.vendor), ('RIGHT(LENGTH(%%s), 1)', ['ABCDEF']))\n", "    def test_sha256(self):\n        test_value = Value(\"test_value\")\n        author = Author.objects.create(name=\"Test Author\", alias=\"testalias\")\n        author.refresh_from_db()\n        alias_sha256 = SHA256(Lower(Right(author.name, 4)) + test_value).resolve_expression(Author.objects.all()).as_sql(with_params=True)\n        if connection.vendor == 'postgresql':\n            self.assertEqual(alias_sha256[0], \"SELECT ENCODE(DIGEST(SUBSTRING(LOWER(RIGHT((authors.name)::text, 4)) || 'test_value', 'sha256'), 'hex') FROM authors\")\n        elif connection.vendor == 'mysql':\n            self.assertEqual(alias_sha256[0], \"SELECT SHA2(CONCAT_WS('', LOWER(RIGHT(authors.name, 4))), 'test_value', '256') FROM authors\")\n        elif connection.vendor == 'oracle':\n            self.assertEqual(alias_sha256[0], \"SELECT LOWER(RAWTOHEX(STANDARD_HASH(UTL_I18N.STRING_TO_RAW(LOWER(SUBSTR(authors.name, -4, 4)) || 'test_value', 'AL32UTF8'), 'SHA256'))) FROM authors\")\n", "    def test_right_function(self):\n        john_smith = Author.objects.get(name=\"John Smith\")\n        self.assertEqual(Right(Lower(john_smith.name), 5), \"smith\")\n\n        rhonda = Author.objects.get(name=\"Rhonda\")\n        self.assertEqual(Right(rhonda.name, 6), \"honda\")\n\n        # Test with a number greater than the length of the string\n        self.assertEqual(Right(john_smith.name, 15), \"John Smith\")\n\n        # Test with a negative number\n        with self.assertRaises(ValueError):\n            Right(john_smith.name, -5)\n\n        # Test with an expression as the length\n        self.assertEqual(Right(john_smith.name, Length(john_smith.alias)), \" Smith\")\n\n        # Test with a number as the length and a non-integer type field\n        self.assertEqual(Right(john_smith.alias, 3), \"ith\")\n"], "sample_81": ["def test_check_include_trailing_dollar(self):\n    pattern = RegexPattern(r'^foo/include/$')\n    warnings = pattern._check_include_trailing_dollar()\n    self.assertEqual(len(warnings), 1)\n    self.assertEqual(warnings[0].id, 'urls.W001')\n\n    pattern = RegexPattern(r'^foo/include')\n    warnings = pattern._check_include_trailing_dollar()\n    self.assertEqual(len(warnings), 0)\n\n    pattern = RegexPattern(r'^foo/include\\$$')\n    warnings = pattern._check_include_trailing_dollar()\n    self.assertEqual(len(warnings), 0)\n", "    def test_route_pattern_match(self):\n        pattern = RoutePattern(route='foo/<int:pk>')\n        match = pattern.match('foo/123')\n        self.assertEqual(match, ('', (), {'pk': 123}))\n", "    def test_match(self):\n        pattern = RoutePattern(r'foo/<int:pk>')\n        match = pattern.match('foo/123')\n        self.assertEqual(match, ('', (), {'pk': 123}))\n", "    def test_check_pattern_startswith_slash_with_append_slash_false(self):\n        pattern = RegexPattern(r'^/pattern$', name='test_pattern')\n        with self.settings(APPEND_SLASH=False):\n            warnings = pattern._check_pattern_startswith_slash()\n            self.assertEqual(warnings, [])\n", "def test_pattern_str_representation(self):\n    pattern = RegexPattern(r'^test/')\n    self.assertEqual(str(pattern), r'^test/')\n"], "sample_418": ["def test_length_is02(self):\n    output = self.engine.render_to_string(\"length_is02\", {\"some_list\": [1, 2, 3]})\n    self.assertEqual(output, \"Three\")\n    output = self.engine.render_to_string(\"length_is02\", {\"some_list\": [1, 2, 3, 4]})\n    self.assertEqual(output, \"Not Three\")\n", "    def test_length_is02(self):\n        output = self.engine.render_to_string(\"length_is02\", {\"some_list\": []})\n        self.assertEqual(output, \"Zero\")\n", "compilation error", "compilation error", "compilation error"], "sample_748": ["def test_parameter_grid_invalid_input(input, error_type, error_message):\n    with assert_raise_message(error_type, error_message):\n        ParameterGrid(input)\n", "    def test_param_grid_initialization_errors(self):\n        with pytest.raises(TypeError) as err_info:\n            ParameterGrid(self.input)\n        assert_equal(str(err_info.value), self.error_message)\n", "def test_grid_search_cv_with_linear_svc_no_score():\n    estimator = LinearSVCNoScore()\n    param_grid = {'C': [1, 10]}\n    grid_search = GridSearchCV(estimator, param_grid, cv=2, scoring='accuracy')\n    grid_search.fit(X, y)\n    assert hasattr(grid_search, 'best_estimator_')\n    assert hasattr(grid_search, 'best_params_')\n    assert hasattr(grid_search, 'best_score_')\n", "def test_parameter_grid_error_messages():\n    input_values = [\n        ({'foo': [0]}, ['foo']),\n        ({'foo': [[0]]}, ['foo']),\n        ({'foo': np.array([[0]])}, ['foo']),\n        ({'foo': [0], 'bar': [[0]]}, ['bar']),\n        ({'foo': [0], 'bar': np.array([[0]])}, ['bar']),\n        ({'foo': [[0]], 'bar': np.array([[0]])}, ['foo', 'bar']),\n    ]\n\n    for input_value, error_key in input_values:\n        with assert_raises(ValueError) as error:\n            ParameterGrid(input_value)\n        assert_in(f\"Parameter array should be one-dimensional.\", str(error.value))\n\n    input_values = [\n        ({'foo': 'bar'}, ['foo']),\n        ({'foo': 0}, ['foo']),\n        ({'foo': [0], 'bar': 'baz'}, ['bar']),\n        ({'foo': [0], 'bar': 1}, ['bar']),\n        ({'foo': 'bar', 'bar': 'baz'}, ['foo', 'bar']),\n        ({'foo': 0, 'bar': 1}, ['foo', 'bar']),\n    ]\n\n    for input_value, error_key in input_values:\n        with assert_raises(ValueError) as error:\n            ParameterGrid(input_value)\n        assert_in(f\"Parameter values for parameter ({error_key[0]}) need to be a sequence(but not a string) or np.ndarray.\", str(error.value))\n\n    input_values = [\n        ({'foo': []}, ['foo']),\n        ({'foo': [0], 'bar': []}, ['bar']),\n        ({'foo': [], 'bar': []}, ['foo', 'bar']),\n    ]\n\n    for input_value, error_key in input_values:\n        with assert_raises(ValueError) as error:\n            ParameterGrid(input_value)\n        assert_in(f\"", "def test_grid_search_cv_with_error_scoring(estimator=MockClassifier()):\n    X, y = make_classification(n_samples=10, n_features=10, n_informative=5, n_redundant=5, random_state=42)\n    param_grid = {\"foo_param\": [0, 1, 2]}\n\n    # Test when error_score is 'raise-deprecating'\n    gs = GridSearchCV(estimator, param_grid, error_score='raise-deprecating')\n    assert_raises(ValueError, gs.fit, X, y)  # Since MockClassifier does not implement a score method\n\n    # Test when error_score is 'raise'\n    gs = GridSearchCV(estimator, param_grid, error_score='raise')\n    assert_raises(ValueError, gs.fit, X, y)  # Since MockClassifier does not implement a score method\n\n    # Test when error_score is a numeric value\n    gs = GridSearchCV(estimator, param_grid, error_score=0)\n    assert_warns(FitFailedWarning, gs.fit, X, y)\n"], "sample_753": ["def test_logistic_regression_path_multinomial_lbfgs():\n    X, y = make_classification(n_samples=100, n_features=20, n_classes=3, n_informative=2, random_state=42)\n    coefs, Cs, n_iter = logistic_regression_path(X, y, solver='lbfgs', multi_class='multinomial')\n    assert_equal(coefs.shape, (10, 3, 20))\n    assert_equal(Cs.shape, (10,))\n    assert_equal(n_iter.shape, (10,))\n", "def test_logistic_regression_path_multinomial():\n    X, y = make_classification(n_samples=100, n_features=20, n_classes=3, n_informative=2, random_state=42)\n    coefs, Cs, n_iter = logistic_regression_path(X, y, solver='lbfgs', multi_class='multinomial')\n    assert_equal(coefs.shape, (10, 3, 20))\n    assert_equal(Cs.shape, (10,))\n    assert_equal(n_iter.shape, (10,))\n", "def test_logistic_regression_path_binary():\n    # Test logistic_regression_path function with binary labels\n    X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n    y = np.array([0, 0, 1, 1])\n    coefs, Cs, n_iter = logistic_regression_path(X, y)\n    assert len(coefs) == len(Cs) == len(n_iter)\n    assert coefs[0].shape == (2,)\n    assert Cs.shape == (10,)\n    assert n_iter.shape == (10,)\n", "def test_logistic_regression_path():\n    Cs = [1.0, 0.5]\n    coefs, Cs_ret, n_iter = logistic_regression_path(X, Y1, Cs=Cs)\n\n    assert_array_equal(Cs_ret, Cs)\n    assert_equal(len(coefs), len(Cs))\n    assert_equal(len(n_iter), len(Cs))\n    assert_equal(coefs[0].shape, coefs[1].shape)\n\n    # Check that smaller C leads to smaller coefficients\n    assert_greater(np.linalg.norm(coefs[0]), np.linalg.norm(coefs[1]))\n\n    # Check that coefs are not all zeros\n    assert_true(not np.allclose(coefs[0], 0))\n    assert_true(not np.allclose(coefs[1], 0))\n", "def test_logistic_regression_cv_with_refit_and_multinomial():\n    X, y = make_classification(n_samples=100, n_features=20, n_informative=2,\n                               n_redundant=10, n_classes=3, n_clusters_per_class=1,\n                               random_state=42)\n\n    clf = LogisticRegressionCV(multi_class='multinomial', refit=True, random_state=42,\n                               solver='lbfgs', cv=StratifiedKFold(n_splits=5))\n    clf.fit(X, y)\n\n    assert_equal(clf.coef_.shape, (3, 20))\n    assert_equal(clf.intercept_.shape, (3,))\n    assert_equal(clf.C_.shape, (3,))\n    assert_equal(clf.n_iter_.shape, (1, 5, 10))\n\n    y_pred = clf.predict(X)\n    assert_greater(np.mean(y_pred == y), 0.9)\n"], "sample_1207": ["def test_convert_xor():\n    result = convert_xor([(OP, '^')], {}, {})\n    assert result == [(OP, '**')]\n", "def test_convert_xor():\n    tokens = [(OP, '^')]\n    result = convert_xor(tokens, {}, {})\n    assert result == [(OP, '**')]\n", "def test_function_exponentiation():\n    transformations = (function_exponentiation,)\n    expr = parse_expr(\"sin**4(x)\", transformations=transformations)\n    assert expr == sin(x)**4\n", "def test_convert_xor():\n    # Testing convert_xor transformation\n    result = convert_xor([(OP, '^')], {}, {})\n    assert result == [(OP, '**')]\n", "def test_parse_expr_rationalize():\n    s = \"0.5\"\n    expr = parse_expr(s, transformations=(auto_number, rationalize))\n    assert expr == Rational(1, 2)\n"], "sample_761": ["def test_simple_imputer_strategies():\n    X = np.array([[1, 2, np.nan], [3, np.nan, 4], [5, 6, np.nan]])\n    X_true_mean = np.array([[1, 2, 3.5], [3, 3.5, 4], [5, 6, 3.5]])\n    statistics_mean = np.array([1, 3.5, 3.5])\n    _check_statistics(X, X_true_mean, \"mean\", statistics_mean, np.nan)\n\n    X_true_median = np.array([[1, 2, 4], [3, 4, 4], [5, 6, 4]])\n    statistics_median = np.array([1, 4, 4])\n    _check_statistics(X, X_true_median, \"median\", statistics_median, np.nan)\n\n    X_true_most_frequent = np.array([[1, 2, 4], [3, 2, 4], [5, 6, 4]])\n    statistics_most_frequent = np.array([1, 2, 4])\n    _check_statistics(X, X_true_most_frequent, \"most_frequent\", statistics_most_frequent, np.nan)\n\n    X_true_constant = np.array([[1, 2, 10], [3, 10, 4], [5, 6, 10]])\n    statistics_constant = np.array([10, 10, 10])\n    _check_statistics(X, X_true_constant, \"constant\", statistics_constant, np.nan)\n", "def test_imputer_different_strategies():\n    \"\"\"Test imputer for different strategies.\"\"\"\n    # Test mean strategy\n    X = np.array([[1, 2, np.nan], [3, 4, np.nan], [np.nan, 6, 7]])\n    X_true = np.array([[1, 2, 5.5], [3, 4, 5.5], [3, 6, 7]])\n    _check_statistics(X, X_true, strategy='mean', statistics=[1, 4, 7], missing_values=np.nan)\n\n    # Test median strategy\n    X = np.array([[1, 2, np.nan], [3, 4, np.nan], [np.nan, 6, 7]])\n    X_true = np.array([[1, 2, 6], [3, 4, 6], [3, 6, 7]])\n    _check_statistics(X, X_true, strategy='median', statistics=[1, 4, 7], missing_values=np.nan)\n\n    # Test most_frequent strategy\n    X = np.array([[1, 2, np.nan], [3, 4, 4], [np.nan, 6, 7]])\n    X_true = np.array([[1, 2, 4], [3, 4, 4], [1, 6, 7]])\n    _check_statistics(X, X_true, strategy='most_frequent', statistics=[1, 4, 7], missing_values=np.nan)\n\n    # Test constant strategy\n    X = np.array([[1, 2, np.nan], [3, 4, np.nan], [np.nan, 6, 7]])\n    X_true = np.array([[1, 2, 0], [3, 4, 0], [3, 6, 7]])\n    _check_statistics(X, X_true, strategy='constant', statistics=[0, 0, 0], missing_values=np", "def test_iterative_imputer_initial_strategies():\n    X = np.array([[np.nan, 1, 3],\n                  [4, 0, np.nan],\n                  [8, 1, 0]])\n\n    strategies = [\"mean\", \"median\", \"most_frequent\", \"constant\"]\n    for strategy in strategies:\n        imputer = IterativeImputer(initial_strategy=strategy)\n        X_imputed = imputer.fit_transform(X)\n        # Check that the imputed data has no missing values\n        assert np.isnan(X_imputed).sum() == 0\n", "def test_iterative_imputer_bayesian_ridge():\n    rng = np.random.RandomState(0)\n    X = rng.normal(0, 1, (50, 2))\n    X[::2, 1] = np.nan\n    est = IterativeImputer(BayesianRidge())\n    est.fit(X)\n    X_transformed = est.transform(X)\n    assert_array_almost_equal(X_transformed[1::2, 1], X[1::2, 1], decimal=2)\n\n    # check that we get a warning when early stopping criterion is not reached\n    with pytest.warns(ConvergenceWarning):\n        est = IterativeImputer(BayesianRidge(), max_iter=2, random_state=0)\n        est.fit(X)\n\n    # check that sample_posterior=True is sampled from Gaussian\n    est = IterativeImputer(BayesianRidge(), max_iter=20, random_state=0,\n                           sample_posterior=True)\n    X_samples = np.array([est.transform(X) for _ in range(1000)])\n    p_value = kstest(X_samples[::2, 1].ravel(), 'norm')[1]\n    assert p_value > 0.05\n", "def test_iterative_imputer_dense_std_estimator():\n    X = np.array([[1, 2, np.nan], [np.nan, 3, 4], [5, 6, np.nan]])\n    imputer = IterativeImputer(estimator=BayesianRidge(), sample_posterior=True, random_state=0)\n    X_imputed = imputer.fit_transform(X)\n    expected_imputed = np.array([[1.0, 2.0, 3.65836337], [4.38077231, 3.0, 4.0], [5.0, 6.0, 4.62105974]])\n    assert_allclose(X_imputed, expected_imputed, atol=1e-6)\n"], "sample_675": ["def test_log_capture_fixture(testdir):\n    testdir.makepyfile(\n        \"\"\"\n            import logging\n\n            logger = logging.getLogger(__name__)\n            logger.error(\"Error message\")\n            logger.info(\"Info message\")\n\n            assert caplog.messages == [\"Error message\", \"Info message\"]\n        \"\"\"\n    )\n\n    result = testdir.runpytest()\n    result.assert_outcomes(passed=1)\n", "def test_live_logging_stream_handler(pytester):\n    pytester.makepyfile(\n        \"\"\"\n            import logging\n            logger = logging.getLogger('test')\n            logger.info('Info message')\n            logger.warning('Warning message')\n            assert 'Info message' in caplog.text\n            assert 'Warning message' in caplog.text\n        \"\"\"\n    )\n    result = pytester.runpytest(\"-vv\", \"--log-cli-level=INFO\")\n    result.stdout.fnmatch_lines([\n        \"live log start --------------------------------------------------------------------------------\",\n        \"*Info message\",\n        \"*Warning message\",\n    ])\n", "def test_get_option_ini(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        from _pytest.config import Config\n\n            config.addini(\"testoption\", default=\"default_value\")\n            config.option.testoption = \"option_value\"\n            assert pytest.get_option_ini(config, \"testoption\") == \"option_value\"\n            assert pytest.get_option_ini(config, \"nonexistentoption\") == \"default_value\"\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.assert_outcomes(passed=1)\n", "def test_get_log_level_for_setting(testdir):\n    testdir.makeconftest(\n        \"\"\"\n        import logging\n        from _pytest.config import Config\n        from _pytest.logging import get_log_level_for_setting\n\n            config.addinivalue_line(\"log_level\", \"DEBUG\")\n            config.addinivalue_line(\"log_file_level\", \"INFO\")\n        \"\"\"\n    )\n\n    testdir.makepyfile(\n        \"\"\"\n            assert get_log_level_for_setting(pytestconfig, \"log_level\") == logging.DEBUG\n            assert get_log_level_for_setting(pytestconfig, \"log_file_level\") == logging.INFO\n        \"\"\"\n    )\n\n    result = testdir.runpytest()\n    result.assert_outcomes(passed=1)\n", "def test_get_option_ini(pytester):\n    pytester.makepyfile(\n        \"\"\"\n            pytester.makeini(\n                \"\"\"\n                [pytest]\n                log_format = %(message)s\n                log_date_format = %Y-%m-%d\n                \"\"\"\n            )\n            pytester.makeconftest(\n                \"\"\"\n                import pytest\n\n                @pytest.fixture\n                    return request.config\n                \"\"\"\n            )\n            pytester.makepyfile(\n                \"\"\"\n                import logging\n\n                from my_logging_module import get_option_ini\n\n                    assert get_option_ini(config, 'log_format') == '%(message)s'\n                    assert get_option_ini(config, 'log_date_format') == '%Y-%m-%d'\n                    assert get_option_ini(config, 'not_exist') is None\n                \"\"\"\n            )\n            result = pytester.runpytest()\n            result.assert_outcomes(passed=1)\n        \"\"\"\n    )\n    pytester.runpytest(\"test_get_option_ini.py\")\n"], "sample_701": ["def test_check_ispytest_warning():\n    with pytest.warns(PytestDeprecationWarning, match=\"A private pytest class or function was used.\"):\n        deprecated.check_ispytest(False)\n", "def test_keyword_msg_arg(func):\n    with pytest.warns(PytestDeprecationWarning, match=str(deprecated.KEYWORD_MSG_ARG).format(func=func)):\n        getattr(pytest, func)(msg=\"deprecated message\")\n", "def test_keywords_msg_arg(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        from _pytest import deprecated\n\n            with pytest.raises(PytestDeprecationWarning):\n                pytest.skip(msg=\"deprecated message\")\n\n            pytest.skip(reason=\"new message\")\n\n            pytest.skip()\n        \"\"\"\n    )\n\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*test_with_msg*::PytestDeprecationWarning: pytest.skip(msg=...) is now deprecated, use pytest.skip(reason=...) instead\",\n            \"*1 passed*, *1 skipped*, *1 deprecated*\",\n        ]\n    )\n", "def test_hook_legacy_path_arg(pytester):\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n        from _pytest import deprecated\n\n        @pytest.hookimpl(trylast=True)\n            raise ValueError(\"Using legacy path argument\")\n\n            pass\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines([\n        \"WARNING: *Using legacy path argument*\",\n    ])\n    assert \"ValueError: Using legacy path argument\" in result.stdout.str()\n", "def test_deprecated_warnings(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n        import warnings\n        from _pytest.deprecated import *\n\n            class Plugin:\n                    pass\n            with pytest.warns(PytestDeprecationWarning, match=\"pytest_warning_captured is deprecated\"):\n                pytest.main(plugins=[Plugin()])\n\n            class Plugin:\n                    pass\n            with pytest.warns(PytestDeprecationWarning, match=\"pytest_cmdline_preparse hook is deprecated\"):\n                pytest.main(plugins=[Plugin()])\n\n            class Plugin:\n                    pass\n            with pytest.warns(PytestDeprecationWarning, match=\"The \\\\(py_path_arg: py.path.local\\\\) argument is deprecated\"):\n                pytest.main(plugins=[Plugin()])\n        \"\"\"\n    )\n    result = pytester.runpytest(\"-vv\")\n    result.assert_outcomes(passed=3)\n"], "sample_1061": ["def test_power_integer_nthroot():\n    assert integer_nthroot(16, 2) == (4, True)\n    assert integer_nthroot(26, 2) == (5, False)\n", "def test_pow_integer_nthroot():\n    assert integer_nthroot(16, 2) == (4, True)\n    assert integer_nthroot(26, 2) == (5, False)\n    assert integer_nthroot(27, 3) == (3, True)\n    assert integer_nthroot(81, 4) == (3, False)\n", "def test_pow_integer_log():\n    assert integer_log(16, 2) == (4, True)\n    assert integer_log(26, 2) == (4, False)\n", "def test_pow_constant_folding():\n    assert Pow(S.Two, S.Zero) == S.One\n    assert Pow(S.Two, S.One) == S.Two\n    assert Pow(S.NegativeOne, S.NegativeOne) == S.NegativeOne\n    assert Pow(S.Zero, S.NegativeOne) == S.ComplexInfinity\n    assert Pow(S.One, S.NegativeOne) == S.One\n    assert Pow(S.Infinity, S.NegativeOne) == S.Zero\n    assert Pow(S.Zero, S.Infinity) == S.Zero\n    assert Pow(S.Zero, S.NegativeInfinity) == S.ComplexInfinity\n    assert Pow(S.One, S.Infinity) == S.NaN\n    assert Pow(S.One, S.NegativeInfinity) == S.NaN\n    assert Pow(S.Infinity, S.Infinity) == S.Infinity\n    assert Pow(S.Infinity, S.NegativeInfinity) == S.Zero\n    assert Pow(S.NegativeInfinity, S.Infinity) == S.ComplexInfinity\n    assert Pow(S.NegativeInfinity, S.NegativeInfinity) == S.Zero\n", "def test_Pow_as_base_exp():\n    p = Pow(S.Half, 2, evaluate=False)\n    assert p.as_base_exp() == (2, -2)\n    assert p.args == (S.Half, 2)\n"], "sample_1133": ["def test_transverse_magnification():\n    assert transverse_magnification(30, 15) == -2\n    assert transverse_magnification(0.2, 0.1) == -2\n    assert transverse_magnification(100, 50) == -2\n", "def test_hyperfocal_distance():\n    f, N, c = symbols('f N c')\n    assert hyperfocal_distance(f, N, c) == (f**2)/(N * c)\n", "def test_lens_makers_formula_with_thick_lens():\n    n_lens = 1.5\n    n_surr = 1.0\n    r1 = 20\n    r2 = -15\n    d = 5\n    expected_focal_length = 18.75  # Calculated manually for verification\n    assert ae(lens_makers_formula(n_lens, n_surr, r1, r2, d), expected_focal_length, 2)\n", "def test_lens_formula():\n    f, u, v = symbols('f u v')\n    assert lens_formula(focal_length=f, u=u) == f*u/(f + u)\n    assert lens_formula(focal_length=f, v=v) == f*v/(f - v)\n    assert lens_formula(u=u, v=v) == u*v/(u - v)\n\n    assert lens_formula(focal_length=1, u=2) == 0.666666666666667\n    assert lens_formula(focal_length=1, v=2) == 2.0\n    assert lens_formula(u=1, v=2) == -0.5\n\n    assert lens_formula(focal_length=oo, u=2) == 2\n    assert lens_formula(focal_length=1, u=oo) == 1\n    assert lens_formula(focal_length=1, v=oo) == 1\n    assert lens_formula(u=1, v=oo) == 0\n", "def test_refraction_angle_as_angle():\n    assert ae(refraction_angle(0.3, 1, 1.5), 0.22360, 5)\n    assert ae(refraction_angle(0.6, 2, 1), 0.35961, 5)\n    raises(ValueError, lambda: refraction_angle(pi/2, 1, 1.5))\n    raises(ValueError, lambda: refraction_angle(0.3, 1, 1.5, normal=Matrix([0, 0, 1])))\n"], "sample_252": ["def test_key_transform_numeric_lookup_mixin(self):\n    obj = JSONModel.objects.create(data={\"key\": 123})\n    self.assertTrue(JSONModel.objects.filter(data__key__gt=100).exists())\n    self.assertFalse(JSONModel.objects.filter(data__key__gt=500).exists())\n    self.assertTrue(JSONModel.objects.filter(data__key__lt=500).exists())\n    self.assertFalse(JSONModel.objects.filter(data__key__lt=100).exists())\n    self.assertTrue(JSONModel.objects.filter(data__key__gte=123).exists())\n    self.assertFalse(JSONModel.objects.filter(data__key__gte=500).exists())\n    self.assertTrue(JSONModel.objects.filter(data__key__lte=123).exists())\n    self.assertFalse(JSONModel.objects.filter(data__key__lte=100).exists())\n", "    def test_key_transform_in(self):\n        key_transform = KeyTransform('key', OutputField())\n        in_lookup = KeyTransformIn(key_transform, ['value1', 'value2'])\n        sql, params = in_lookup.as_sql(None, None)\n        self.assertEqual(sql, 'JSON_EXTRACT(%s, %s) IN (%s, %s)')\n        self.assertEqual(params, ['value1', 'value2'])\n", "    def test_call(self):\n        factory = KeyTransformFactory('key')\n        transform = factory()\n        self.assertIsInstance(transform, KeyTransform)\n        self.assertEqual(transform.key_name, 'key')\n", "def test_key_transform_factory(self):\n    factory = KeyTransformFactory('key')\n    transform = factory(F('field'))\n    self.assertIsInstance(transform, KeyTransform)\n    self.assertEqual(transform.key_name, 'key')\n    self.assertEqual(transform.lhs, F('field'))\n", "    def test_key_transform_numeric_lookup_mixin(self):\n        JSONModel.objects.create(json='{\"num\": 10}')\n        obj = JSONModel.objects.get(json__num__gt=5)\n        self.assertEqual(obj.json, {\"num\": 10})\n\n        obj = JSONModel.objects.get(json__num__gte=10)\n        self.assertEqual(obj.json, {\"num\": 10})\n\n        obj = JSONModel.objects.get(json__num__lt=20)\n        self.assertEqual(obj.json, {\"num\": 10})\n\n        obj = JSONModel.objects.get(json__num__lte=10)\n        self.assertEqual(obj.json, {\"num\": 10})\n"], "sample_357": ["def test_altered_managers_removal(self):\n    questioner = MigrationQuestioner()\n    changes = self.get_changes(\n        [self.author_with_biography_non_blank],\n        [self.author_with_biography_blank],\n        questioner,\n    )\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterModelManagers\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"Author\", managers=[])\n", "def test_autodetect_constraint_addition_and_removal(self):\n    before_states = [self.author_name]\n    after_states = [self.author_name_check_constraint]\n    changes = self.get_changes(before_states, after_states)\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['AddConstraint'])\n\n    before_states = [self.author_name_check_constraint]\n    after_states = [self.author_name]\n    changes = self.get_changes(before_states, after_states)\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['RemoveConstraint'])\n", "def test_generate_altered_fields_with_foreign_key_to_swappable_model(self):\n    before_states = [self.author_with_user]\n    after_states = [self.author_with_custom_user]\n    changes = self.get_changes(before_states, after_states)\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['AlterField'])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name='user')\n    self.assertOperationFieldAttributes(changes, 'testapp', 0, 0, swappable_setting='AUTH_USER_MODEL')\n", "def test_deconstructible_objects(self):\n    questioner = MigrationQuestioner(specified_apps=[], dry_run=True)\n\n    # Test with two identical deconstructible objects\n    before_states = [self.author_name_deconstructible_1]\n    after_states = [self.author_name_deconstructible_2]\n    changes = self.get_changes(before_states, after_states, questioner)\n    self.assertNumberMigrations(changes, 'testapp', 0)\n\n    # Test with two deconstructible objects with a changed argument\n    before_states = [self.author_name_deconstructible_1]\n    after_states = [self.author_name_nested_deconstructible_changed_arg]\n    changes = self.get_changes(before_states, after_states, questioner)\n    self.assertNumberMigrations(changes, 'testapp', 1)\n\n    # Test with two deconstructible objects with an extra argument\n    before_states = [self.author_name_deconstructible_1]\n    after_states = [self.author_name_nested_deconstructible_extra_arg]\n    changes = self.get_changes(before_states, after_states, questioner)\n    self.assertNumberMigrations(changes, 'testapp', 1)\n\n    # Test with two deconstructible objects with a changed keyword argument\n    before_states = [self.author_name_deconstructible_1]\n    after_states = [self.author_name_nested_deconstructible_changed_kwarg]\n    changes = self.get_changes(before_states, after_states, questioner)\n    self.assertNumberMigrations(changes, 'testapp', 1)\n\n    # Test with two deconstructible objects with an extra keyword argument\n    before_states = [self.author_name_deconstructible_1]\n    after_states = [self.author_name_nested_deconstructible_extra_kwarg]\n    changes = self.get_changes(before_states, after_states, questioner)\n    self.assertNumberMigrations(changes, '", "def test_alter_field_dependencies_foreign_key_on_model_with_swappable_setting(self):\n    before_states = [\n        self.author_with_user,\n        self.custom_user,\n    ]\n    after_states = [\n        ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n            (\"user\", models.ForeignKey(\"thirdapp.CustomUser\", models.CASCADE, swappable_setting=\"AUTH_USER_MODEL\")),\n        ]),\n        self.custom_user,\n    ]\n    changes = self.get_changes(before_states, after_states)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterField\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, model_name=\"Author\", name=\"user\")\n    self.assertEqual(\n        changes[\"testapp\"][0].operations[0].field.deconstruct(),\n        (\"django.db.models.fields.related.ForeignKey\", [], {\n            \"model\": \"__setting__.AUTH_USER_MODEL\",\n            \"on_delete\": models.CASCADE,\n        }),\n    )\n"], "sample_266": ["    def test_load_disk(self):\n        loader = MigrationLoader(connection, load=False)\n        loader.load_disk()\n        self.assertIn('migrations_test_app', loader.migrated_apps)\n        self.assertNotIn('migrations_test_app', loader.unmigrated_apps)\n        self.assertIn(('migrations_test_app', '0001_initial'), loader.disk_migrations)\n", "    def test_check_consistent_history(self):\n        \"\"\"\n        Test that check_consistent_history raises InconsistentMigrationHistory\n        when there are applied migrations with unapplied dependencies.\n        \"\"\"\n        # Set up a migration graph with some applied and unapplied migrations\n        loader = MigrationLoader(connection)\n        loader.graph.add_node(('app1', '0001_initial'), None)\n        loader.graph.add_node(('app1', '0002_add_field'), None)\n        loader.graph.add_dependency(('app1', '0002_add_field'), ('app1', '0001_initial'), None)\n\n        # Apply the first migration but not the second one\n        recorder = MigrationRecorder(connection)\n        recorder.record_applied('app1', '0001_initial')\n\n        # Check that check_consistent_history raises InconsistentMigrationHistory\n        with self.assertRaises(InconsistentMigrationHistory):\n            loader.check_consistent_history(connection)\n", "    def test_check_key(self):\n        loader = MigrationLoader(None)\n        loader.build_graph()\n        key = ('auth', '__first__')\n        self.assertEqual(loader.check_key(key, 'auth'), ('auth', '0001_initial'))\n", "    def test_build_graph(self):\n        \"\"\"\n        Test that the build_graph method correctly loads migrations from disk,\n        populates the graph with nodes, and handles replacements correctly.\n        \"\"\"\n        loader = MigrationLoader(connection)\n        loader.build_graph()\n\n        # Check that the graph contains nodes for all migrations on disk\n        for migration in loader.disk_migrations.values():\n            self.assertIn((migration.app_label, migration.name), loader.graph.nodes)\n\n        # Check that the graph contains all applied migrations\n        recorder = MigrationRecorder(connection)\n        applied_migrations = recorder.applied_migrations()\n        for migration in applied_migrations:\n            self.assertIn(migration, loader.graph.nodes)\n\n        # Check that replacing migrations are handled correctly\n        for key, migration in loader.replacements.items():\n            if all(target in applied_migrations for target in migration.replaces):\n                self.assertIn(key, applied_migrations)\n            else:\n                self.assertNotIn(key, applied_migrations)\n                if all(target in loader.graph.nodes for target in migration.replaces):\n                    self.assertNotIn(key, loader.graph.nodes)\n                else:\n                    self.assertIn(key, loader.graph.nodes)\n", "def test_replace_migrations(self):\n    # Create a replacement migration\n    app_label = 'test_app'\n    migration_name = '0002_replacing'\n    replace_migration_name = '0001_initial'\n    migration_module = import_module(f'{app_label}.migrations.{migration_name}')\n    replace_migration_module = import_module(f'{app_label}.migrations.{replace_migration_name}')\n\n    # Mock the disk_migrations and applied_migrations attributes\n    loader = MigrationLoader(connection, load=False)\n    loader.disk_migrations = {\n        (app_label, migration_name): migration_module.Migration(migration_name, app_label),\n        (app_label, replace_migration_name): replace_migration_module.Migration(replace_migration_name, app_label),\n    }\n    loader.applied_migrations = {(app_label, replace_migration_name): replace_migration_module.Migration(replace_migration_name, app_label)}\n\n    # Build the graph and check that the replacing migration is used\n    loader.build_graph()\n    self.assertIn((app_label, migration_name), loader.graph.nodes)\n    self.assertNotIn((app_label, replace_migration_name), loader.graph.nodes)\n\n    # Check that the replacing migration is marked as applied\n    self.assertIn((app_label, migration_name), loader.applied_migrations)\n\n    # Check that the replaced migration is not marked as applied\n    self.assertNotIn((app_label, replace_migration_name), loader.applied_migrations)\n"], "sample_687": ["def test_sublogger_propagation(caplog):\n    sublogger.setLevel(logging.DEBUG)\n    sublogger.debug(\"This is a debug message from sublogger\")\n    assert len(caplog.records) == 1\n    assert caplog.records[0].name == __name__ + \".baz\"\n    assert caplog.records[0].levelno == logging.DEBUG\n    assert caplog.records[0].getMessage() == \"This is a debug message from sublogger\"\n", "def test_log_capture_fixture(caplog):\n    logger.info(\"This is a test log message\")\n    assert \"This is a test log message\" in caplog.text\n", "def test_caplog_records(caplog):\n    logger.info(\"Test log message\")\n    sublogger.info(\"Sublogger test message\")\n    assert len(caplog.get_records(\"call\")) == 2\n    assert caplog.get_records(\"call\")[0].message == \"Test log message\"\n    assert caplog.get_records(\"call\")[1].message == \"Sublogger test message\"\n", "def test_log_capture_fixture(caplog, request):\n    # Access the LogCaptureFixture and perform some actions\n    fixture = caplog._item._store[caplog_records_key]\n    fixture['test'] = []\n\n    # Log some messages at different levels\n    logger.debug('Debug message')\n    logger.info('Info message')\n    sublogger.warning('Warning message')\n\n    # Verify the log records\n    assert len(fixture['test']) == 3\n    assert fixture['test'][0].levelno == logging.DEBUG\n    assert fixture['test'][0].message == 'Debug message'\n    assert fixture['test'][1].levelno == logging.INFO\n    assert fixture['test'][1].message == 'Info message'\n    assert fixture['test'][2].levelno == logging.WARNING\n    assert fixture['test'][2].message == 'Warning message'\n\n    # Verify the log text\n    assert 'DEBUG' in caplog.text\n    assert 'Info message' in caplog.text\n    assert 'WARNING' in caplog.text\n    assert 'Warning message' in caplog.text\n\n    # Verify the log records\n    assert len(caplog.records) == 3\n    assert caplog.record_tuples[0] == (__name__, logging.DEBUG, 'Debug message')\n    assert caplog.record_tuples[1] == (__name__, logging.INFO, 'Info message')\n    assert caplog.record_tuples[2] == (__name__ + '.baz', logging.WARNING, 'Warning message')\n\n    # Verify the log messages\n    assert len(caplog.messages) == 3\n    assert caplog.messages[0] == 'Debug message'\n    assert caplog.messages[1] == 'Info message'\n    assert caplog.messages[2] == 'Warning message'\n\n    # Clear the log records and text\n    caplog.clear()\n    assert len(caplog.records) == 0\n    assert caplog.text == ''\n", "def test_logging_output(caplog, testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n            logger.debug('This is a debug message')\n            sublogger.warning('This is a warning message')\n            assert len(caplog.records) == 2\n            assert caplog.records[0].levelname == 'DEBUG'\n            assert caplog.records[0].message == 'This is a debug message'\n            assert caplog.records[1].levelname == 'WARNING'\n            assert caplog.records[1].message == 'This is a warning message'\n        \"\"\"\n    )\n    result = testdir.runpytest('-v')\n    result.stdout.fnmatch_lines(\n        [\n            \"*::test_logging_output PASSED*\",\n            \"*This is a debug message*\",\n            \"*This is a warning message*\",\n        ]\n    )\n    assert result.ret == 0\n"], "sample_274": ["    def test_model_multiple_choice_field_invalid_list(self):\n        field = ModelMultipleChoiceField(queryset=ChoiceModel.objects.all())\n        self.assertFormErrors([\"Enter a list of values.\"], field.clean, \"invalid\")\n", "def test_modelform_defines_fields_function(self):\n    class TestForm(Form):\n        pass\n\n    self.assertFalse(modelform_defines_fields(TestForm))\n\n    class TestFormWithMeta(Form):\n        class Meta:\n            pass\n\n    self.assertFalse(modelform_defines_fields(TestFormWithMeta))\n\n    class TestFormWithFields(Form):\n        class Meta:\n            fields = '__all__'\n\n    self.assertTrue(modelform_defines_fields(TestFormWithFields))\n\n    class TestFormWithExclude(Form):\n        class Meta:\n            exclude = ['field1', 'field2']\n\n    self.assertTrue(modelform_defines_fields(TestFormWithExclude))\n", "    def test_model_form_validation(self):\n        class TestModelForm(forms.ModelForm):\n            class Meta:\n                model = ChoiceModel\n                fields = ['choice_field']\n\n        form = TestModelForm({'choice_field': 'invalid'})\n        self.assertFalse(form.is_valid())\n        self.assertIn('choice_field', form.errors)\n        self.assertEqual(form.errors['choice_field'], ['Select a valid choice. invalid is not one of the available choices.'])\n", "def test_model_multiple_choice_field_invalid_list(self):\n    field = ModelMultipleChoiceField(queryset=ChoiceModel.objects.all())\n    self.assertFormErrors(['Enter a list of values.'], field.clean, 'invalid_list')\n", "    def test_modelform_meta_validation(self):\n        class InvalidModelForm(ModelForm):\n            class Meta:\n                model = ChoiceModel\n                fields = 'choice'\n\n        with self.assertRaises(TypeError) as e:\n            InvalidModelForm()\n        self.assertIn(\"'fields' cannot be a string. Did you mean to type: ('choice',)?\", str(e.exception))\n"], "sample_616": ["def test_collect_dict_values():\n    objects = [{\"a\": 1, \"b\": 2, \"c\": 3}, {\"a\": 4, \"b\": 5, \"d\": 6}, {\"a\": 7, \"c\": 8}]\n    keys = [\"a\", \"b\"]\n    fill_value = 0\n    result = collect_dict_values(objects, keys, fill_value)\n    expected = [[1, 4, 7], [2, 5, 0]]\n    assert result == expected\n", "def test_apply_ufunc_unified_dim_sizes():\n    var1 = xr.Variable([\"x\", \"y\"], np.ones((3, 4)))\n    var2 = xr.Variable([\"y\", \"z\"], np.ones((4, 5)))\n    dim_sizes = unified_dim_sizes([var1, var2])\n    assert dim_sizes == {\"x\": 3, \"y\": 4, \"z\": 5}\n\n    var3 = xr.Variable([\"x\"], np.ones(3))\n    with pytest.raises(ValueError, match=\"operands cannot be broadcast together\"):\n        unified_dim_sizes([var1, var3])\n\n    var4 = xr.Variable([\"x\", \"x\"], np.ones((3, 3)))\n    with pytest.raises(ValueError, match=\"broadcasting cannot handle duplicate\"):\n        unified_dim_sizes([var1, var4])\n", "def test_apply_ufunc_broadcast_compat_data():\n    # Test case for broadcast_compat_data function\n    variable = xr.Variable((\"x\", \"y\"), np.array([[1, 2], [3, 4]]))\n    broadcast_dims = (\"z\",)\n    core_dims = (\"x\", \"y\")\n\n    result = broadcast_compat_data(variable, broadcast_dims, core_dims)\n    expected = np.array([[[1, 2], [3, 4]]])\n    assert_identical(result, expected)\n", "def test_unified_dim_sizes():\n    var1 = xr.Variable((\"x\", \"y\"), np.ones((3, 4)))\n    var2 = xr.Variable((\"y\", \"z\"), np.ones((4, 5)))\n    dim_sizes = unified_dim_sizes([var1, var2])\n    assert dim_sizes == {\"x\": 3, \"y\": 4, \"z\": 5}\n\n    var3 = xr.Variable((\"x\",), np.ones(3))\n    with pytest.raises(ValueError):\n        unified_dim_sizes([var1, var3])\n\n    var4 = xr.Variable((\"x\", \"y\"), np.ones((3, 5)))\n    with pytest.raises(ValueError):\n        unified_dim_sizes([var1, var4])\n\n    var5 = xr.Variable((\"x\", \"x\"), np.ones((3, 3)))\n    with pytest.raises(ValueError):\n        unified_dim_sizes([var1, var5])\n", "def test_unified_dim_sizes():\n    # Test case 1: No broadcasting\n    var1 = xr.Variable(dims=(\"x\", \"y\"), data=np.random.rand(3, 4))\n    var2 = xr.Variable(dims=(\"x\", \"y\"), data=np.random.rand(3, 4))\n    dim_sizes = unified_dim_sizes([var1, var2])\n    assert dim_sizes == {\"x\": 3, \"y\": 4}\n\n    # Test case 2: Broadcasting\n    var3 = xr.Variable(dims=(\"x\"), data=np.random.rand(3))\n    var4 = xr.Variable(dims=(\"y\"), data=np.random.rand(4))\n    dim_sizes = unified_dim_sizes([var3, var4])\n    assert dim_sizes == {\"x\": 3, \"y\": 4}\n\n    # Test case 3: Mismatched lengths\n    var5 = xr.Variable(dims=(\"x\"), data=np.random.rand(3))\n    var6 = xr.Variable(dims=(\"x\"), data=np.random.rand(2))\n    with pytest.raises(ValueError):\n        unified_dim_sizes([var5, var6])\n\n    # Test case 4: Duplicate dimensions on a variable\n    var7 = xr.Variable(dims=(\"x\", \"x\"), data=np.random.rand(3, 3))\n    with pytest.raises(ValueError):\n        unified_dim_sizes([var7])\n"], "sample_758": ["def test_check_non_negative():\n    # Test with a numpy array\n    X_positive = np.array([[1, 2], [3, 4]])\n    check_non_negative(X_positive, \"test\")  # Should pass\n\n    X_negative = np.array([[1, -2], [3, 4]])\n    assert_raises(ValueError, check_non_negative, X_negative, \"test\")\n\n    # Test with a sparse matrix\n    X_positive_sparse = sp.csr_matrix([[1, 2], [3, 4]])\n    check_non_negative(X_positive_sparse, \"test\")  # Should pass\n\n    X_negative_sparse = sp.csr_matrix([[1, -2], [3, 4]])\n    assert_raises(ValueError, check_non_negative, X_negative_sparse, \"test\")\n", "def test_check_memory():\n    # Test that check_memory returns a Memory object with the correct location\n    memory = check_memory('temp_location')\n    assert isinstance(memory, sklearn.utils.Memory)\n    assert memory.location == 'temp_location'\n\n    # Test that check_memory returns the input object if it already has a cache method\n    class MockMemory:\n            pass\n    memory = MockMemory()\n    assert check_memory(memory) is memory\n\n    # Test that check_memory raises a ValueError if the input is not joblib.Memory-like\n    assert_raises(ValueError, check_memory, 'invalid_memory')\n", "def test_check_non_negative():\n    X = np.array([[1, 2], [3, 4]])\n    check_non_negative(X, \"Test\")\n\n    X = np.array([[1, 2], [3, -4]])\n    with pytest.raises(ValueError, match=\"Negative values in data passed to Test\"):\n        check_non_negative(X, \"Test\")\n\n    X = sp.csr_matrix([[1, 2], [3, -4]])\n    with pytest.raises(ValueError, match=\"Negative values in data passed to Test\"):\n        check_non_negative(X, \"Test\")\n", "def test_check_is_fitted():\n    # Test with a fitted estimator\n    clf = KNeighborsClassifier()\n    X, y = make_blobs(n_samples=10, random_state=0)\n    clf.fit(X, y)\n    check_is_fitted(clf, 'classes_')\n\n    # Test with an unfitted estimator\n    clf = KNeighborsClassifier()\n    with pytest.raises(NotFittedError):\n        check_is_fitted(clf, 'classes_')\n", "def test_check_non_negative():\n    X = np.array([[1, 2], [3, 4]])\n    check_non_negative(X, 'test')\n\n    X = np.array([[1, -2], [3, 4]])\n    with pytest.raises(ValueError):\n        check_non_negative(X, 'test')\n"], "sample_122": ["    def test_get_max_age(self):\n        response = HttpResponse()\n        patch_cache_control(response, max_age=60)\n        self.assertEqual(get_max_age(response), 60)\n        patch_cache_control(response, max_age='invalid')\n        self.assertIsNone(get_max_age(response))\n", "    def test_patch_cache_control(self):\n        response = HttpResponse()\n        patch_cache_control(response, no_cache=True, no_store=True, must_revalidate=True, max_age=60, private=True)\n        self.assertEqual(response['Cache-Control'], 'no-cache, no-store, must-revalidate, max-age=60, private')\n\n        response = HttpResponse()\n        response['Cache-Control'] = 'no-cache, max-age=120'\n        patch_cache_control(response, max_age=60, public=True)\n        self.assertEqual(response['Cache-Control'], 'no-cache, max-age=60, public')\n", "    def test_patch_cache_control(self):\n        response = HttpResponse()\n        patch_cache_control(response, no_cache=True, no_store=False, max_age=3600)\n        self.assertEqual(response['Cache-Control'], 'no-cache, max-age=3600')\n\n        patch_cache_control(response, no_cache='field1, field2', no_store=True)\n        self.assertEqual(response['Cache-Control'], 'no-cache=field1, no-cache=field2, no-store, max-age=3600')\n\n        patch_cache_control(response, private=False, public=True)\n        self.assertEqual(response['Cache-Control'], 'no-cache=field1, no-cache=field2, no-store, max-age=3600, public')\n", "    def test_patch_cache_control_override(self):\n        response = HttpResponse()\n        patch_cache_control(response, private=True)\n        self.assertEqual(response['Cache-Control'], 'private')\n        patch_cache_control(response, public=True)\n        self.assertEqual(response['Cache-Control'], 'public')\n        patch_cache_control(response, private=True)\n        self.assertEqual(response['Cache-Control'], 'private')\n        patch_cache_control(response, public=True)\n        self.assertEqual(response['Cache-Control'], 'public')\n", "    def setUp(self):\n        self.factory = RequestFactory()\n        self.middleware = CacheMiddleware()\n        self.cache = caches['default']\n"], "sample_1012": ["def test_print_Piecewise():\n    expr = Piecewise((x, x > 0), (0, True))\n    code = pycode(expr)\n    assert code == \"((x) if (x > 0) else (0))\"\n", "def test_print_Pow_with_0_5_exponent():\n    expr = x ** 0.5\n    printer = NumPyPrinter()\n    result = printer._print_Pow(expr)\n    assert result == 'numpy.sqrt(x)'\n", "def test_print_SparseMatrix_SciPyPrinter():\n    sparse_matrix = SparseMatrix(3, 3, {(0, 0): 1, (1, 1): 2, (2, 2): 3})\n    scipy_printer = SciPyPrinter()\n    result = scipy_printer._print_SparseMatrix(sparse_matrix)\n    expected = \"scipy.sparse.coo_matrix([1, 2, 3], ([0, 1, 2], [0, 1, 2]), shape=(3, 3))\"\n    assert result == expected\n", "def test_SymPyPrinter():\n    printer = SymPyPrinter()\n    expr = acos(x) + sign(y)\n    assert printer.doprint(expr) == \"sympy.acos(x) + sympy.sign(y)\"\n", "def test_print_SparseMatrix():\n    A = SparseMatrix(3, 3, {(0, 1): 1, (1, 2): 2})\n    code = SciPyPrinter().doprint(A)\n    assert code == \"scipy.sparse.coo_matrix([1, 2], ([0, 1], [1, 2]), shape=(3, 3))\"\n"], "sample_696": ["def test_deprecated_warnings():\n    with pytest.warns(PytestDeprecationWarning, match=\"pytest.collect.{name} was moved\"):\n        deprecated.PYTEST_COLLECT_MODULE.warn(name=\"Module\")\n\n    with pytest.warns(PytestDeprecationWarning, match=\"@pytest.yield_fixture is deprecated\"):\n        warnings.warn(deprecated.YIELD_FIXTURE)\n\n    with pytest.warns(PytestDeprecationWarning, match=\"The `-k '-expr'` syntax to -k is deprecated\"):\n        warnings.warn(deprecated.MINUS_K_DASH)\n\n    with pytest.warns(PytestDeprecationWarning, match=\"The `-k 'expr:'` syntax to -k is deprecated\"):\n        warnings.warn(deprecated.MINUS_K_COLON)\n\n    with pytest.warns(PytestDeprecationWarning, match=\"The pytest_warning_captured is deprecated\"):\n        warnings.warn(deprecated.WARNING_CAPTURED_HOOK)\n\n    with pytest.warns(PytestDeprecationWarning, match=\"The gethookproxy() and isinitpath() methods\"):\n        warnings.warn(deprecated.FSCOLLECTOR_GETHOOKPROXY_ISINITPATH)\n\n    with pytest.warns(PytestDeprecationWarning, match=\"The --strict option is deprecated\"):\n        warnings.warn(deprecated.STRICT_OPTION)\n\n    with pytest.warns(PytestDeprecationWarning, match=\"A private pytest class or function was used\"):\n        deprecated.check_ispytest(False)\n\n    with pytest.warns(PytestDeprecationWarning, match=\"Raising unittest.SkipTest to skip tests during collection is deprecated\"):\n        warnings.warn(deprecated.UNITTEST_SKIP_DURING_COLLECTION)\n\n    with pytest.warns(PytestDeprecationWarning, match=\"pytest now uses argparse. \\\"%default\\\" should be changed to \\\"%\\\\(default\\\\)s\\\"\"):\n        warnings.warn(deprecated.ARGUMENT_PERCENT_", "def test_check_ispytest_warning():\n    with pytest.warns(PytestDeprecationWarning, match=re.escape(str(deprecated.PRIVATE.message))):\n        deprecated.check_ispytest(False)\n", "def test_check_ispytest_warning():\n    with pytest.warns(PytestDeprecationWarning, match=\"A private pytest class or function was used.\"):\n        deprecated.check_ispytest(False)\n", "def test_warning_types(warning_type, expected_warning):\n    assert isinstance(warning_type, expected_warning)\n", "def test_argument_type_str_choice(option, typ, names):\n    with pytest.warns(deprecated.ARGUMENT_TYPE_STR_CHOICE) as record:\n        class Plugin:\n                pass\n\n        plugin = Plugin()\n        parser = mock.Mock()\n        plugin.add_option(parser)\n        assert len(record) == 1\n        assert re.match(\n            f\"'type' argument to addoption\\\\(\\\\) is the string {repr(typ)}, but when supplied should be a type. \"\n            f\"\\\\(options: {repr(names)}\\\\)\",\n            str(record[0].message),\n        )\n"], "sample_689": ["def test_deprecated_warnings(pytester: Pytester):\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n        from _pytest import deprecated\n\n            with pytest.deprecated_call():\n                pytest.Function('func', lambda: None)._fillfuncargs()\n\n            with pytest.deprecated_call():\n                pytest.collect.Module('module')\n\n            result = testdir.runpytest('-k', '-expr')\n            result.stdout.fnmatch_lines(['*The `-k \\'-expr\\'` syntax to -k is deprecated.*'])\n\n            result = testdir.runpytest('-k', 'expr:')\n            result.stdout.fnmatch_lines(['*The `-k \\'expr:\\'` syntax to -k is deprecated.*'])\n\n            testdir.makepyfile(\"\"\"\n                import pytest\n                    pass\n            \"\"\")\n            result = testdir.runpytest()\n            result.stdout.fnmatch_lines(['*The pytest_warning_captured is deprecated.*'])\n\n            testdir.makepyfile(\"\"\"\n                import pytest\n                    collector = request.node.session.getitem('module')\n                    collector.gethookproxy('pytest_sessionfinish')\n                    request.node.session.isinitpath(collector.fspath)\n            \"\"\")\n            result = testdir.runpytest()\n            result.stdout.fnmatch_lines(['*The gethookproxy() and isinitpath() methods of FSCollector and Package", "def test_deprecated_warnings(deprecated_warning, pytester: Pytester):\n    test_code = f\"import warnings; from _pytest import deprecated; warnings.warn(deprecated.{deprecated_warning.name})\"\n    pytester.makepyfile(test_code)\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines([f\"*{deprecated_warning.name}*\"])\n", "def test_deprecated_warnings(testdir: Testdir, warning_type: warnings.WarningMessage):\n    with warnings.catch_warnings(record=True) as w:\n        testdir.makepyfile(\"\"\"\n            import pytest\n            from _pytest import deprecated\n            warning = deprecated.%s\n            pytest.warns(warning, lambda: None)\n        \"\"\" % warning_type.__class__.__name__)\n        result = testdir.runpytest()\n        result.assert_outcomes(passed=1)\n        assert len(w) == 1\n        assert issubclass(w[-1].category, warning_type.warning_type)\n        assert warning_type.msg in str(w[-1].message)\n", "def test_deprecated_warnings(testdir):\n    warnings = []\n\n        warnings.append((message, category))\n\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        from _pytest import deprecated\n\n            with pytest.deprecated_call():\n                deprecated.MINUS_K_DASH\n                deprecated.MINUS_K_COLON\n                deprecated.WARNING_CAPTURED_HOOK\n                deprecated.FSCOLLECTOR_GETHOOKPROXY_ISINITPATH\n                deprecated.STRICT_OPTION\n        \"\"\"\n    )\n\n    result = testdir.runpytest()\n    result.assert_outcomes(passed=1)\n\n    expected_warnings = [\n        (deprecated.MINUS_K_DASH, PytestDeprecationWarning),\n        (deprecated.MINUS_K_COLON, PytestDeprecationWarning),\n        (deprecated.WARNING_CAPTURED_HOOK, PytestDeprecationWarning),\n        (deprecated.FSCOLLECTOR_GETHOOKPROXY_ISINITPATH, PytestDeprecationWarning),\n        (deprecated.STRICT_OPTION, PytestDeprecationWarning),\n    ]\n\n    for message, category in expected_warnings:\n        assert any(str(w[0]) == str(message) and w[1] == category for w in warnings)\n", "def test_deprecated_warnings(pytester: Pytester, testdir: Testdir, attribute: str) -> None:\n    testdir.makepyfile(\n        f\"\"\"\n        import pytest\n        from _pytest import deprecated\n\n            with pytest.warns(deprecated.{attribute}):\n                getattr(pytest.collect, '{attribute}')\n        \"\"\"\n    )\n\n    result = pytester.runpytest()\n\n    result.stdout.fnmatch_lines(\n        [f\"*{attribute}*\", \"*DeprecationWarning*\"]\n    )\n    assert result.ret == 0\n"], "sample_311": ["def test_add_action_and_disable_action(self):\n        pass\n\n    custom_action_name = 'custom_action'\n    self.assertNotIn(custom_action_name, dict(self.client.admin.site.actions))\n\n    self.client.admin.site.add_action(custom_action)\n    self.assertIn(custom_action_name, dict(self.client.admin.site.actions))\n\n    self.client.admin.site.disable_action(custom_action_name)\n    self.assertNotIn(custom_action_name, dict(self.client.admin.site.actions))\n", "    def test_index_view(self):\n        response = self.client.get(reverse('admin:index'))\n        self.assertEqual(response.status_code, 200)\n        self.assertContains(response, 'Site administration')\n        self.assertContains(response, 'Sections')\n        self.assertContains(response, 'Articles')\n", "    def test_index_page(self):\n        response = self.client.get('/admin/')\n        self.assertEqual(response.status_code, 200)\n        self.assertContains(response, \"Site administration\")\n        self.assertContains(response, \"Sections\")\n        self.assertContains(response, \"Articles\")\n", "    def test_add_section_with_inline_article(self):\n        # Test adding a Section with inline Articles\n        url = reverse('admin:admin_views_section_add')\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n\n        data = {\n            \"name\": \"New Test Section\",\n            \"article_set-TOTAL_FORMS\": \"3\",\n            \"article_set-INITIAL_FORMS\": \"0\",\n            \"article_set-MAX_NUM_FORMS\": \"0\",\n            \"article_set-0-title\": \"Article 1\",\n            \"article_set-0-content\": \"Content 1\",\n            \"article_set-0-date_0\": \"2022-01-01\",\n            \"article_set-0-date_1\": \"12:00:00\",\n            \"article_set-1-title\": \"Article 2\",\n            \"article_set-1-content\": \"Content 2\",\n            \"article_set-1-date_0\": \"2022-01-02\",\n            \"article_set-1-date_1\": \"12:00:00\",\n        }\n        response = self.client.post(url, data)\n        self.assertEqual(response.status_code, 302)  # Redirect after successful POST\n        self.assertEqual(Section.objects.filter(name=\"New Test Section\").count(), 1)\n        self.assertEqual(Article.objects.filter(title__in=[\"Article 1\", \"Article 2\"]).count(), 2)\n", "    def test_custom_admin_config(self):\n        response = self.client.get(reverse('admin:admin_views_person_changelist'))\n        self.assertEqual(response.status_code, 200)\n        self.assertContains(response, 'Custom Person Admin')\n"], "sample_730": ["def test_elastic_net_cv_with_l1_ratio_as_list():\n    X, y = load_boston(return_X_y=True)\n    l1_ratio_list = [0.1, 0.5, 0.9]\n    enet_cv = ElasticNetCV(l1_ratio=l1_ratio_list, cv=5, random_state=42)\n    enet_cv.fit(X, y)\n    assert_true(enet_cv.l1_ratio_ in l1_ratio_list)\n", "def test_enet_path_with_l1_ratio_zero():\n    X, y = load_boston(return_X_y=True)\n    with assert_raises_regex(ValueError, \"Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument.\"):\n        enet_path(X, y, l1_ratio=0.0, eps=1e-3, n_alphas=100)\n", "def test_LassoCV_with_positive_constraint():\n    # Create a simple dataset\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([1, 2, 3])\n\n    # Test LassoCV with positive constraint\n    lasso_cv = LassoCV(positive=True)\n    lasso_cv.fit(X, y)\n\n    # Check that all coefficients are positive\n    assert_true(np.all(lasso_cv.coef_ >= 0))\n", "def test_multi_task_elastic_net_cv_fit():\n    X = np.array([[0, 0], [1, 1], [2, 2]])\n    y = np.array([[0, 0], [1, 1], [2, 2]])\n    clf = MultiTaskElasticNetCV(l1_ratio=[.1, .5, .7, .9, .95, .99, 1])\n    clf.fit(X, y)\n    assert_almost_equal(clf.coef_.sum(), 3.23997343, decimal=6)\n    assert_almost_equal(clf.intercept_.sum(), 0.16002657, decimal=6)\n    assert_almost_equal(clf.l1_ratio_, 0.7, decimal=6)\n", "def test_lasso_path_with_positive_constraint():\n    # Test that Lasso path with positive constraint throws a warning\n    X = np.array([[-1, 0], [0, 1], [1, 0]])\n    y = np.array([0, 1, 2])\n    with pytest.warns(UserWarning, match=\"positive=True leads to a non-convex\"):\n        lasso_path(X, y, positive=True)\n"], "sample_568": ["def test_pathpatch_3d(fig, ax):\n    path = Path([(0, 0), (1, 1), (1, 0)])\n    patch = PathPatch(path, facecolor='r', alpha=0.5)\n    ax.add_patch(patch)\n    art3d.pathpatch_2d_to_3d(patch, z=0.5, zdir='z')\n    fig.canvas.draw()\n", "def test_zorder(fig, ax):\n    # Test that zorder is applied correctly in 3D plots\n    line1 = ax.plot3D([0, 1], [0, 1], [0, 0], zorder=1)[0]\n    line2 = ax.plot3D([0, 1], [0, 1], [1, 1], zorder=2)[0]\n    assert ax.lines.index(line2) < ax.lines.index(line1)\n", "def test_patch_collection_3d(fig):\n    ax = fig.add_subplot(111, projection='3d')\n    zs = np.array([1, 2, 3, 4, 5])\n    patches = [mpatch.Circle((0, 0), radius=0.1) for _ in range(len(zs))]\n    pc = art3d.PatchCollection(patches, zs=zs, zdir='z', depthshade=True)\n    ax.add_collection3d(pc)\n    ax.set_xlim(-1, 1)\n    ax.set_ylim(-1, 1)\n    ax.set_zlim(0, 6)\n", "def test_3d_text(fig):\n    ax = fig.add_subplot(111, projection='3d')\n    text = art3d.Text3D(1, 1, 1, '3D Text', zdir='z')\n    ax.add_artist(text)\n", "def test_patch3d_collection_depthshade():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n\n    # create a collection of 3D patches with varying z-positions\n    verts = [\n        [(0, 0, 0), (1, 0, 0), (1, 1, 0)],\n        [(0, 0, 1), (1, 0, 1), (1, 1, 1)],\n        [(0, 0, 2), (1, 0, 2), (1, 1, 2)]\n    ]\n    zs = [0, 1, 2]\n    colors = ['red', 'green', 'blue']\n    pc = art3d.Patch3DCollection(verts, zs=zs, facecolors=colors)\n    ax.add_collection3d(pc)\n\n    # test that depthshade property is set correctly\n    assert pc.get_depthshade() is True\n\n    # test that facecolors are modified by depthshade\n    pc.set_depthshade(False)\n    colors_no_depthshade = pc.get_facecolor()\n    pc.set_depthshade(True)\n    colors_depthshade = pc.get_facecolor()\n    assert not np.array_equal(colors_no_depthshade, colors_depthshade)\n\n    plt.close(fig)\n"], "sample_398": ["    def test_login_view(self):\n        response = self.client.get(reverse(\"login\"))\n        self.assertEqual(response.status_code, 200)\n        self.assertTemplateUsed(response, \"registration/login.html\")\n", "    def test_login_view_get(self):\n        response = self.client.get(reverse('login'))\n        self.assertEqual(response.status_code, 200)\n        self.assertTemplateUsed(response, 'registration/login.html')\n        self.assertIsInstance(response.context['form'], AuthenticationForm)\n", "    def test_password_change_view(self):\n        self.login()\n        response = self.client.get(reverse(\"password_change\"))\n        self.assertEqual(response.status_code, 200)\n        self.assertIsInstance(response.context[\"form\"], PasswordChangeForm)\n        response = self.client.post(\n            reverse(\"password_change\"),\n            {\"old_password\": \"password\", \"new_password1\": \"new_password\", \"new_password2\": \"new_password\"},\n        )\n        self.assertRedirects(response, reverse(\"password_change_done\"))\n        self.u1.refresh_from_db()\n        self.assertTrue(self.u1.check_password(\"new_password\"))\n        self.u1.set_password(\"password\")  # Reset the password\n", "    def test_login_view(self):\n        response = self.client.get(reverse(\"login\"))\n        self.assertEqual(response.status_code, 200)\n        self.assertTemplateUsed(response, \"registration/login.html\")\n", "    def test_login_view_get(self):\n        response = self.client.get(reverse(\"login\"))\n        self.assertEqual(response.status_code, 200)\n        self.assertTemplateUsed(response, \"registration/login.html\")\n"], "sample_439": ["    def test_form_initialization_with_renderer(self):\n        form = Form(renderer=DjangoTemplates())\n        self.assertIsInstance(form.renderer, DjangoTemplates)\n", "    def test_widget_data_value(self):\n        form = PersonNew()\n        widget = TextInput(attrs={\"id\": \"first_name_id\"})\n        html_name = \"first_name\"\n        data = {\"first_name\": \"John\"}\n        result = form._widget_data_value(widget, html_name)\n        self.assertEqual(result, \"John\")\n", "    def test_add_error(self):\n        form = FrameworkForm(data={\"name\": \"Django\", \"language\": \"P\"})\n        form.add_error(\"name\", \"This is a custom error message.\")\n        self.assertEqual(form.errors[\"name\"], [\"This is a custom error message.\"])\n", "def test_split_date_time_field(self):\n    class SplitDateTimeForm(Form):\n        split_datetime = SplitDateTimeField(widget=SplitHiddenDateTimeWidget)\n\n    data = {\n        \"split_datetime_0\": \"2022-01-01\",\n        \"split_datetime_1\": \"12:34:56\",\n    }\n    form = SplitDateTimeForm(data)\n    self.assertTrue(form.is_valid())\n    self.assertEqual(form.cleaned_data[\"split_datetime\"], datetime.datetime(2022, 1, 1, 12, 34, 56))\n", "    def test_file_field_clean(self):\n        form = TestForm({}, {\"file_field\": SimpleUploadedFile(\"test.txt\", b\"file content\")})\n        form.full_clean()\n        self.assertIn(\"file_field\", form.cleaned_data)\n        self.assertEqual(form.cleaned_data[\"file_field\"].read(), b\"file content\")\n"], "sample_690": ["    def test_evaluate_xfail_marks(self, pytester):\n        pytester.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.mark.xfail(reason=\"always xfail\")\n                assert False\n            \"\"\"\n        )\n        result = pytester.runpytest()\n        result.assert_outcomes(xfailed=1)\n\n        item = result.getcalls(\"test_func\")[0].node\n        xfailed = evaluate_xfail_marks(item)\n        assert xfailed.reason == \"always xfail\"\n        assert xfailed.run is True\n        assert xfailed.strict is False\n        assert xfailed.raises is None\n", "    def test_evaluate_skip_marks(self, pytester):\n        pytester.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.mark.skip\n                assert False\n\n            @pytest.mark.skip(reason=\"specific reason\")\n                assert False\n\n                assert True\n\n                from _pytest.skipping import evaluate_skip_marks\n                item = pytest.Item.from_parent(None, function=test_skipped_no_reason)\n                assert evaluate_skip_marks(item).reason == \"unconditional skip\"\n                item = pytest.Item.from_parent(None, function=test_skipped_with_reason)\n                assert evaluate_skip_marks(item).reason == \"specific reason\"\n                item = pytest.Item.from_parent(None, function=test_not_skipped)\n                assert evaluate_skip_marks(item) is None\n            \"\"\"\n        )\n        result = pytester.runpytest()\n        result.assert_outcomes(skipped=2, passed=1)\n", "def test_evaluate_skip_marks(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.skip(reason=\"always skipped\")\n            assert False\n\n        @pytest.mark.skipif(sys.version_info < (3, 6), reason=\"requires Python 3.6 or higher\")\n            assert False\n        \"\"\"\n    )\n\n    result = pytester.runpytest()\n    result.assert_outcomes(skipped=2)\n\n    # Check if the skip marks are evaluated correctly\n    item, = pytester.getitems(test_skip_mark)\n    skip = evaluate_skip_marks(item)\n    assert skip is not None\n    assert skip.reason == \"always skipped\"\n\n    item, = pytester.getitems(test_skipif_mark)\n    skip = evaluate_skip_marks(item)\n    assert skip is not None\n    assert skip.reason == \"requires Python 3.6 or higher\"\n", "    def test_evaluate_condition(self, pytester: Pytester) -> None:\n        pytester.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.mark.xfail(condition=\"1 + 1 == 3\", reason=\"2 + 2 is not 3\")\n                assert 1 + 1 == 3\n\n            @pytest.mark.xfail(condition=1 == 2, reason=\"1 is not 2\")\n                assert 1 == 2\n            \"\"\"\n        )\n\n        result = pytester.runpytest()\n\n        result.assert_outcomes(xfailed=2)\n        result.stdout.fnmatch_lines(\n            [\n                \"*1 is not 2*\",\n                \"*2 + 2 is not 3*\",\n            ]\n        )\n", "def test_evaluate_xfail_marks(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        test_xfail=\"\"\"\n            import pytest\n\n            @pytest.mark.xfail(reason=\"known issue\", strict=True)\n                assert False\n\n            @pytest.mark.xfail(reason=\"known issue\", strict=False)\n                assert True\n\n            @pytest.mark.xfail(reason=\"known issue\", run=False)\n                assert False\n        \"\"\"\n    )\n\n    result = pytester.runpytest(\"-v\")\n\n    result.stdout.fnmatch_lines(\n        [\n            \"*test_fail XFAIL*\",\n            \"*test_pass XPASS*\",\n            \"SKIP [NOTRUN]*test_not_run*\",\n        ]\n    )\n\n    assert result.ret == 1\n"], "sample_96": ["    def test_exclude_not_a_list_or_tuple(self):\n        class MyModelAdmin(ModelAdmin):\n            exclude = \"name\"\n\n        self.assertIsInvalid(MyModelAdmin, ValidationTestModel,\n                             \"The value of 'exclude' must be a list or tuple.\",\n                             id='admin.E014')\n", "    def test_exclude_not_a_list_or_tuple(self):\n        class MyModelAdmin(ModelAdmin):\n            exclude = 'not_a_list_or_tuple'\n\n        self.assertIsInvalid(MyModelAdmin, ValidationTestModel,\n                             \"The value of 'exclude' must be a list or tuple.\",\n                             id='admin.E014')\n", "def test_raw_id_fields_foreign_key_or_many_to_many_field(self):\n    class TestAdmin(ModelAdmin):\n        raw_id_fields = ('not_a_field',)\n\n    self.assertIsInvalid(TestAdmin, ValidationTestModel,\n        \"The value of 'raw_id_fields[0]' refers to 'not_a_field', which is not an attribute of 'validation.ValidationTestModel'.\",\n        id='admin.E002')\n", "    def test_invalid_raw_id_fields_item(self):\n        class InvalidRawIdAdmin(ModelAdmin):\n            raw_id_fields = ('invalid_field',)\n\n        self.assertIsInvalid(\n            InvalidRawIdAdmin, ValidationTestModel,\n            \"The value of 'raw_id_fields[0]' refers to 'invalid_field', which is not an attribute of 'tests.ValidationTestModel'.\",\n            id='admin.E002',\n        )\n", "    def test_filter_horizontal_must_be_a_list_or_tuple(self):\n        class InvalidModelAdmin(ModelAdmin):\n            filter_horizontal = 'invalid_value'\n\n        self.assertIsInvalid(InvalidModelAdmin, ValidationTestModel,\n                             \"The value of 'filter_horizontal' must be a list or tuple.\",\n                             id='admin.E018')\n"], "sample_304": ["compilation error", "compilation error", "compilation error", "compilation error", "    def test_validate_image_file_extension_no_pillow(self):\n        with self.assertRaises(ValidationError):\n            validate_image_file_extension(ContentFile('contents', name='file.jpg'))\n"], "sample_1052": ["def test_make_routine_cse():\n    x, y, z = symbols('x y z')\n    r = make_routine('fcn', [x*y + z, Eq(y, x + z)], language='C', cse=True)\n    assert len(r.local_vars) == 1\n    assert r.local_vars[0].name == r.results[0].expr.args[0]\n", "def test_simple_c_codegen():\n    x, y, z = symbols(\"x y z\")\n    [(c_name, c_code), (h_name, c_header)] = codegen(\n        (\"simple\", x + y * z), \"C99\", \"simple\", header=False, empty=False)\n\n    assert c_name == \"simple.c\"\n    assert c_code == r\"\"\"#include \"simple.h\"", "def test_codegen_output_argument():\n    x, y = symbols('x y')\n    routines = [make_routine('fcn', [x*y, Eq(y, 1), Eq(x, x + y)])]\n    c_code = get_string(CCodeGen.dump_c, routines)\n    f_code = get_string(FCodeGen.dump_f95, routines)\n    assert \"void fcn(double x, double y, double *y, double *x)\" in c_code\n    assert \"void fcn(x, y, y, x)\" in f_code\n", "def test_make_routine():\n    x, y, f, g = symbols('x y f g')\n    r = make_routine('fcn', [x*y, Eq(f, 1), Eq(g, x + g), Matrix([[x, 2]])])\n    assert [arg.expr for arg in r.results] == [x*y]\n    assert [a.expr for a in r.arguments if isinstance(a, OutputArgument)] == [1, Matrix([[x, 2]])]\n    assert [a.expr for a in r.arguments if isinstance(a, InOutArgument)] == [g + x]\n", "def test_codegen_fortran_multiple_output():\n    x, y, z = symbols('x y z')\n\n    routines = [make_routine(\"function_1\", Eq(x, y + z)),\n                make_routine(\"function_2\", Eq(y, x + z))]\n\n    source = get_string(FCodeGen().dump_f95, routines)\n    assert \"function function_1\" in source\n    assert \"function function_2\" in source\n    assert \"intent(inout) :: x, y\" in source\n    assert \"intent(in) :: z\" in source\n    assert \"x = y + z\" in source\n    assert \"y = x + z\" in source\n"], "sample_197": ["    def test_time_strings_customization(self):\n        time_strings = {\n            'year': npgettext_lazy('%d ano', '%d anos'),\n            'month': npgettext_lazy('%d m\u00eas', '%d meses'),\n            'week': npgettext_lazy('%d semana', '%d semanas'),\n            'day': npgettext_lazy('%d dia', '%d dias'),\n            'hour': npgettext_lazy('%d hora', '%d horas'),\n            'minute': npgettext_lazy('%d minuto', '%d minutos'),\n        }\n\n        with translation.override('pt-br'):\n            self.assertEqual(\n                timesince(self.t - self.oneweek, self.t, time_strings=time_strings),\n                '1 semana'\n            )\n            self.assertEqual(\n                timeuntil(self.t + self.oneweek, self.t, time_strings=time_strings),\n                '1 semana'\n            )\n", "    def test_time_strings(self):\n        custom_time_strings = {\n            'year': npgettext_lazy('%d year', '%d years'),\n            'month': npgettext_lazy('%d month', '%d months'),\n            'week': npgettext_lazy('%d week', '%d weeks'),\n            'day': npgettext_lazy('%d day', '%d days'),\n            'hour': npgettext_lazy('%d hour', '%d hours'),\n            'minute': npgettext_lazy('%d minute', '%d minutes'),\n        }\n        self.assertEqual(timesince(self.t, self.t + self.oneminute, time_strings=custom_time_strings), '1 minute')\n        self.assertEqual(timesince(self.t, self.t + self.onehour, time_strings=custom_time_strings), '1 hour')\n        self.assertEqual(timesince(self.t, self.t + self.oneday, time_strings=custom_time_strings), '1 day')\n        self.assertEqual(timesince(self.t, self.t + self.oneweek, time_strings=custom_time_strings), '1 week')\n        self.assertEqual(timesince(self.t, self.t + self.onemonth, time_strings=custom_time_strings), '1 month')\n        self.assertEqual(timesince(self.t, self.t + self.oneyear, time_strings=custom_time_strings), '1 year')\n", "    def test_timesince_leap_years(self):\n        d = datetime.datetime(2000, 2, 29)\n        now = datetime.datetime(2001, 3, 1)\n        self.assertEqual(timesince(d, now), '1 year')\n", "    def test_time_difference_with_leap_years(self):\n        leap_year = datetime.datetime(2020, 2, 29, 12, 0, 0)\n        non_leap_year = datetime.datetime(2021, 2, 28, 12, 0, 0)\n        diff = timesince(leap_year, non_leap_year)\n        self.assertEqual(diff, '365 days')\n", "    def test_leap_years(self):\n        # Test timesince with leap years\n        leap_year = datetime.datetime(2020, 2, 29, 13, 46, 0)\n        not_leap_year = datetime.datetime(2021, 2, 28, 13, 46, 0)\n        self.assertEqual(timesince(leap_year, self.t), '1 year, 10 months')\n        self.assertEqual(timesince(not_leap_year, self.t), '1 year, 10 months, 1 day')\n"], "sample_365": ["def test_lazy_object_equality(self):\n    class MyClass:\n            return True\n\n        return MyClass()\n\n    obj1 = SimpleLazyObject(setup_func)\n    obj2 = SimpleLazyObject(setup_func)\n\n    self.assertEqual(obj1, obj2)\n", "    def test_lazy_function_call(self):\n            return a + b\n\n        lazy_func = lazy(test_func, int)\n        result = lazy_func(2, 3)\n\n        self.assertIsInstance(result, Promise)\n        self.assertEqual(result.__cast(), 5)\n", "    def test_lazy_object(self):\n        class MyLazyObject(LazyObject):\n                self._wrapped = 'test'\n\n        obj = MyLazyObject()\n        self.assertEqual(obj._wrapped, empty)\n        self.assertEqual(str(obj), 'test')\n        self.assertEqual(obj._wrapped, 'test')\n", "    def test_cached_property_with_name(self):\n            return 42\n\n        with self.assertWarns(RemovedInDjango50Warning):\n            prop = cached_property(get_value, name='test_property')\n            prop.__set_name__(FunctionalTests, 'test_property')\n\n        self.assertEqual(prop.name, 'test_property')\n        self.assertEqual(prop.func, get_value)\n        self.assertEqual(prop.__doc__, get_value.__doc__)\n\n        class Dummy:\n            test_property = prop\n\n        instance = Dummy()\n        self.assertEqual(instance.test_property, 42)\n        self.assertEqual(instance.test_property, instance.__dict__['test_property'])\n", "def test_cached_property(self):\n    class TestClass:\n        @cached_property\n            return 'cached result'\n\n    test_obj = TestClass()\n    self.assertEqual(test_obj.test_property, 'cached result')\n    self.assertEqual(test_obj.test_property, 'cached result')\n    with mock.patch.object(test_obj, 'test_property', return_value='new result') as mock_method:\n        self.assertEqual(test_obj.test_property, 'cached result')\n        mock_method.assert_not_called()\n\n    test_obj = TestClass()\n    with self.assertRaises(TypeError):\n        class TestClass2:\n            @cached_property\n                return 'result'\n            test_property = cached_property(lambda self: 'other result', name='test_property')\n"], "sample_183": ["def test_case_expression_with_when_clauses(self):\n    cases = [\n        When(Q(integer__gt=3), then=Value('Greater than 3')),\n        When(Q(integer__lt=2), then=Value('Less than 2')),\n    ]\n    expression = Case(*cases, default=Value('Between 2 and 3'), output_field=CharField())\n    results = CaseTestModel.objects.annotate(result=expression).values('integer', 'result').order_by('integer')\n    expected_results = [\n        {'integer': 1, 'result': 'Less than 2'},\n        {'integer': 2, 'result': 'Between 2 and 3'},\n        {'integer': 2, 'result': 'Between 2 and 3'},\n        {'integer': 3, 'result': 'Between 2 and 3'},\n        {'integer': 3, 'result': 'Between 2 and 3'},\n        {'integer': 3, 'result': 'Between 2 and 3'},\n        {'integer': 4, 'result': 'Greater than 3'},\n    ]\n    self.assertEqual(list(results), expected_results)\n", "    def test_case_with_default_value(self):\n        CaseTestModel.objects.create(integer=None, integer2=None, string=None)\n        results = CaseTestModel.objects.annotate(\n            result=Case(\n                When(Q(integer__isnull=True), then=Value(0)),\n                default=F('integer2'),\n                output_field=IntegerField(),\n            )\n        ).values_list('result', flat=True).order_by('pk')\n        self.assertEqual(list(results), [0, 1, 3, 2, 3, 3, 5])\n", "def test_case_expression_with_duration_field(self):\n    duration = timedelta(days=1, hours=1, minutes=1)\n    cases = Case(\n        When(integer=1, then=duration),\n        default=timedelta(),\n        output_field=DurationField()\n    )\n    result = CaseTestModel.objects.annotate(duration_case=cases).values('integer', 'duration_case')\n    self.assertEqual(\n        sorted(result, key=itemgetter('integer')),\n        [\n            {'integer': 1, 'duration_case': duration},\n            {'integer': 2, 'duration_case': timedelta()},\n            {'integer': 3, 'duration_case': timedelta()},\n            {'integer': 4, 'duration_case': timedelta()},\n        ]\n    )\n", "    def test_case_expression_with_lookup(self):\n        case = Case(\n            When(integer__gt=2, then=F('integer2')),\n            default=Value(0),\n            output_field=IntegerField(),\n        )\n        result = CaseTestModel.objects.annotate(case_val=case).values('pk', 'case_val')\n        expected_result = [\n            {'pk': 1, 'case_val': 0},\n            {'pk': 2, 'case_val': 3},\n            {'pk': 3, 'case_val': 4},\n            {'pk': 4, 'case_val': 3},\n            {'pk': 5, 'case_val': 4},\n            {'pk': 6, 'case_val': 4},\n            {'pk': 7, 'case_val': 5},\n        ]\n        self.assertSequenceEqual(result, expected_result)\n", "    def test_case_expression_with_default(self):\n        qs = CaseTestModel.objects.annotate(\n            case_test=Case(\n                When(integer=1, then=Value('one')),\n                When(integer=2, then=Value('two')),\n                default=Value('other'),\n                output_field=CharField(),\n            )\n        )\n        cases = list(qs.values('case_test').order_by('pk'))\n        self.assertEqual(\n            [case['case_test'] for case in cases],\n            ['one', 'two', 'two', 'two', 'three', 'three', 'three', 'other']\n        )\n"], "sample_857": ["def test_ccp_pruning_path():\n    X, y = DATASETS[\"clf_small\"][\"X\"], DATASETS[\"clf_small\"][\"y\"]\n    clf = DecisionTreeClassifier(random_state=42, ccp_alpha=0.0)\n    clf.fit(X, y)\n    path = clf.cost_complexity_pruning_path(X, y)\n    assert \"ccp_alphas\" in path\n    assert \"impurities\" in path\n    assert len(path[\"ccp_alphas\"]) == len(path[\"impurities\"])\n    assert np.all(np.diff(path[\"ccp_alphas\"]) < 0)\n    assert np.all(np.diff(path[\"impurities\"]) >= 0)\n", "def test_decision_tree_classifier_log2_max_features(criterion):\n    clf = DecisionTreeClassifier(max_features='log2', criterion=criterion, random_state=42)\n    clf.fit(iris.data, iris.target)\n    assert clf.n_features_ == iris.data.shape[1]\n    assert clf.max_features_ == int(np.log2(iris.data.shape[1]))\n", "def test_predict_proba_single_output_classification():\n    for name, Tree in CLF_TREES.items():\n        tree_instance = Tree(random_state=42)\n        tree_instance.fit(X_small, y_small)\n        proba = tree_instance.predict_proba(X_small)\n        # Add assertions to check the shape and values of proba\n        assert proba.shape == (len(X_small), tree_instance.n_classes_)\n        # Add more assertions based on the expected output\n", "def test_decision_tree_classifier_sparse_input():\n    X_sparse = csc_matrix(DATASETS[\"iris\"][\"X\"])\n    y = DATASETS[\"iris\"][\"y\"]\n    clf = DecisionTreeClassifier(random_state=0)\n    clf.fit(X_sparse, y)\n    pred = clf.predict(X_sparse)\n    assert accuracy_score(y, pred) > 0.95\n", "def test_sparse_csr(estimator_name):\n    X = csr_matrix(DATASETS[\"sparse-pos\"][\"X\"])\n    y = DATASETS[\"sparse-pos\"][\"y\"]\n\n    estimator = ALL_TREES[estimator_name]()\n    estimator.fit(X, y)\n    assert estimator.tree_.max_depth > 0\n"], "sample_1201": ["def test_cgs_gauss_unit_conversions():\n    # Test conversion from SI to CGS-gaussian units\n    assert convert_to(coulomb, cgs_gauss).evalf() == 10*speed_of_light*statcoulomb\n    assert convert_to(ampere, cgs_gauss).evalf() == 10*speed_of_light*statcoulomb/second\n    assert convert_to(volt, cgs_gauss).evalf() == 10**6/speed_of_light*statvolt\n    assert convert_to(weber, cgs_gauss).evalf() == 10**8*maxwell\n    assert convert_to(tesla, cgs_gauss).evalf() == 10**4*gauss\n    assert convert_to(debye, cgs_gauss).evalf() == One/10**18*statcoulomb*centimeter\n    assert convert_to(ohm, cgs_gauss).evalf() == 10**5/speed_of_light**2*second/centimeter\n    assert convert_to(farad, cgs_gauss).evalf() == One/10**5*speed_of_light**2*centimeter\n    assert convert_to(henry, cgs_gauss).evalf() == 10**5/speed_of_light**2/centimeter*second**2\n\n    # Test conversion from CGS-gaussian to SI units\n    assert convert_to(statcoulomb, SI).evalf() == coulomb/10/speed_of_light\n    assert convert_to(statvolt, SI).evalf() == volt*speed_of_light/10**6\n    assert convert_to(gauss, SI).evalf() == tesla/10**4\n    assert convert_to(maxwell, SI).evalf() == weber/10**8\n", "def test_unit_conversion():\n    # Test conversion between statcoulomb and coulomb\n    assert convert_to(statcoulomb, coulomb) == 10*speed_of_light*coulomb\n\n    # Test conversion between statvolt and volt\n    assert convert_to(statvolt, volt) == 10**6/speed_of_light*volt\n\n    # Test conversion between ohm and SI ohm\n    assert convert_to(ohm, SI.ohm) == 10**-11*SI.ohm\n\n    # Test conversion between farad and SI farad\n    assert convert_to(farad, SI.farad) == 10**-9*SI.farad\n\n    # Test conversion between henry and SI henry\n    assert convert_to(henry, SI.henry) == 10**-7*SI.henry\n\n    # Test Coulomb's constant conversion\n    assert convert_to(coulomb_constant, 1) == 1\n", "def test_cgs_gauss_unit_conversion():\n    # Test conversion of some units from CGS-Gaussian to SI and vice versa\n\n    # Test conversion from CGS-Gaussian to SI\n    assert convert_to(1*statcoulomb, coulomb, cgs_gauss, SI) == 10*speed_of_light*coulomb\n    assert convert_to(1*statvolt, volt, cgs_gauss, SI) == 10**6/speed_of_light*volt\n    assert convert_to(1*ohm, ohm, cgs_gauss, SI) == 10**5/speed_of_light**2*second/meter\n\n    # Test conversion from SI to CGS-Gaussian\n    assert convert_to(1*coulomb, statcoulomb, SI, cgs_gauss) == coulomb/(10*speed_of_light)\n    assert convert_to(1*volt, statvolt, SI, cgs_gauss) == speed_of_light*volt/(10**6)\n    assert convert_to(1*ohm, ohm, SI, cgs_gauss) == speed_of_light**2*meter/(10**5*second)\n\n    # Test conversion of derived units\n    assert convert_to(1*farad, farad, SI, cgs_gauss) == 10**5/speed_of_light**2*second/centimeter\n    assert convert_to(1*henry, henry, SI, cgs_gauss) == 10**5/speed_of_light**2/centimeter*second**2\n", "def test_cgs_gauss_conversions():\n    assert convert_to(coulomb, statcoulomb, cgs_gauss) == 10*speed_of_light*statcoulomb\n    assert convert_to(statcoulomb, coulomb, cgs_gauss) == 10*speed_of_light*coulomb\n    assert convert_to(volt, statvolt, cgs_gauss) == 10**6/speed_of_light*statvolt\n    assert convert_to(statvolt, volt, cgs_gauss) == 10**6*speed_of_light/statvolt\n    assert convert_to(farad, cgs_gauss.quantity_of(ohm**-1), cgs_gauss) == One/10**5*speed_of_light**2*centimeter\n    assert convert_to(cgs_gauss.quantity_of(ohm**-1), farad, cgs_gauss) == 10**5/speed_of_light**2/centimeter\n    assert convert_to(henry, cgs_gauss.quantity_of(ohm*second), cgs_gauss) == 10**5/speed_of_light**2/centimeter*second**2\n    assert convert_to(cgs_gauss.quantity_of(ohm*second), henry, cgs_gauss) == 10**5*speed_of_light**2*centimeter/second**2\n", "def test_cgs_gauss_conversion():\n    # Test conversion from statcoulomb to coulomb\n    assert convert_to(statcoulomb, coulomb, cgs_gauss) == 10*speed_of_light*coulomb\n\n    # Test conversion from statvolt to volt\n    assert convert_to(statvolt, volt, cgs_gauss) == 10**6/speed_of_light*volt\n\n    # Test conversion from ohm to SI ohm\n    assert convert_to(ohm, ohm, SI) == 10**-5*speed_of_light**2/centimeter*second\n\n    # Test conversion from farad to SI farad\n    assert convert_to(farad, farad, SI) == 10**-5/speed_of_light**2*centimeter\n\n    # Test conversion from henry to SI henry\n    assert convert_to(henry, henry, SI) == 10**-5*speed_of_light**2/centimeter*second**2\n"], "sample_614": ["def test_last_n_items(self):\n    array = np.array([1, 2, 3, 4, 5])\n    n_desired = 3\n    result = formatting.last_n_items(array, n_desired)\n    assert np.array_equal(result, np.array([3, 4, 5]))\n", "def test_format_items_timedelta(self):\n    x = np.array([np.timedelta64(1, \"D\"), np.timedelta64(2, \"h\")])\n    formatted = formatting.format_items(x)\n    assert formatted == [\"1 days\", \"02:00:00\"]\n", "def test_format_array_flat_with_timedelta(self):\n    array = np.array([np.timedelta64(i, \"D\") for i in range(10)])\n    max_width = 20\n    result = formatting.format_array_flat(array, max_width)\n    expected = \"0days 0days ... 7days 8days 9days\"\n    assert result == expected\n", "def test_format_timedelta():\n    # Test with a timedelta with days and time\n    td = pd.Timedelta(days=2, hours=12, minutes=30, seconds=15)\n    assert formatting.format_timedelta(td) == \"2 days 12:30:15\"\n    assert formatting.format_timedelta(td, timedelta_format=\"date\") == \"2 days\"\n    assert formatting.format_timedelta(td, timedelta_format=\"time\") == \"12:30:15\"\n\n    # Test with a timedelta with only time\n    td = pd.Timedelta(hours=12, minutes=30, seconds=15)\n    assert formatting.format_timedelta(td) == \"12:30:15\"\n    assert formatting.format_timedelta(td, timedelta_format=\"date\") == \"0 days\"\n    assert formatting.format_timedelta(td, timedelta_format=\"time\") == \"12:30:15\"\n\n    # Test with a timedelta with only days\n    td = pd.Timedelta(days=2)\n    assert formatting.format_timedelta(td) == \"2 days 00:00:00\"\n    assert formatting.format_timedelta(td, timedelta_format=\"date\") == \"2 days\"\n    assert formatting.format_timedelta(td, timedelta_format=\"time\") == \"00:00:00\"\n", "def test_format_timestamp_out_of_bounds(self):\n    t = pd.Timestamp(\"2000-01-01T00:00:00.000000000\")\n    assert formatting.format_timestamp(t) == \"2000-01-01T00:00:00.000000000\"\n"], "sample_630": ["def test_dot_writer_get_values(generated_file):\n    \"\"\"Test DotWriter.get_values method\"\"\"\n    config = Config()\n    writer = DotWriter(config)\n    obj = astroid.ClassDef(\"TestClass\", bases=())\n\n    # Test with only classnames\n    config.only_classnames = True\n    result = writer.get_values(obj)\n    assert result[\"label\"] == \"TestClass\"\n    assert result[\"shape\"] == \"record\"\n\n    # Test without only classnames\n    config.only_classnames = False\n    result = writer.get_values(obj)\n    assert result[\"label\"] == \"{TestClass}\"\n    assert result[\"shape\"] == \"record\"\n", "def test_get_annotation_label(setup):\n    \"\"\"Test the get_annotation_label function\"\"\"\n    annotation = astroid.parse(\"x: int\").body[0].value.annotation\n    assert get_annotation_label(annotation) == \"int\"\n", "def test_get_values(generated_file):\n    \"\"\"Test the get_values method of DotWriter class.\"\"\"\n    config = Config()\n    writer = DotWriter(config)\n    mock_obj = Mock()\n    mock_obj.title = \"TestClass\"\n    mock_obj.shape = \"class\"\n    mock_obj.attrs = [\"attr1\", \"attr2\"]\n    mock_obj.methods = [\n        Mock(name=\"method1\", returns=None, args=Mock(args=[Mock(name=\"self\")])),\n        Mock(name=\"method2\", returns=Mock(type=\"return_type\"), args=Mock(args=[Mock(name=\"self\"), Mock(name=\"arg1\")])),\n    ]\n    mock_obj.node = Mock()\n\n    with patch.object(writer, 'get_title', return_value=\"TestClass\"), \\\n         patch.object(writer, 'get_annotation_label', side_effect=lambda x: x.type if x else \"\"), \\\n         patch.object(writer, 'is_exception', return_value=False):\n        result = writer.get_values(mock_obj)\n        assert result == {\n            'label': '{TestClass|attr1\\\\lattr2\\\\lmethod1()\\\\lmethod2(arg1): return_type\\\\l}',\n            'shape': 'record'\n        }\n", "def test_dot_writer_get_values_exception(generated_file):\n    config = Config()\n    config.only_classnames = True\n    writer = DotWriter(config)\n    obj = type('Obj', (object,), {'node': astroid.Instance(astroid.ClassDef('ExceptionClass', (Exception,), {})), 'title': 'ExceptionClass', 'attrs': [], 'methods': [], 'shape': 'class'})\n    expected_values = {'fontcolor': 'red', 'label': 'ExceptionClass', 'shape': 'record'}\n    assert writer.get_values(obj) == expected_values\n", "def test_get_annotation_label(generated_file):\n    \"\"\"Test get_annotation_label function\"\"\"\n    # Mock the get_annotation function to return a specific value for testing\n    with patch(\"pylint.pyreverse.utils.get_annotation\", return_value=astroid.Uninferable):\n        # Assuming that the get_annotation_label function is used in the DotWriter class\n        # Create an instance of DotWriter with a mock config\n        config = Config()\n        dot_writer = DotWriter(config)\n        # Call the get_values function which uses get_annotation_label\n        values = dot_writer.get_values(mock.Mock())\n        # Assert that the return type is an empty string when get_annotation returns Uninferable\n        assert \"return_type\" not in values or values[\"return_type\"] == \"\"\n"], "sample_1113": ["def test_block_collapse_with_blockmatrix():\n    B = BlockMatrix([[X1, X2], [X3, X4]])\n    C = BlockMatrix([[Y, Z], [ZeroMatrix(m, n), Identity(m)]])\n    result = block_collapse(B * C)\n    expected = BlockMatrix([[X1*Y + X2*Z, X1*Identity(m) + X2*Identity(m)]])\n    assert result == expected\n", "def test_block_matrix_entry():\n    B = BlockMatrix([[X1, X2], [X3, X4]])\n    assert B[0, 0] == X1[0, 0]\n    assert B[0, 1] == X1[0, 1]\n    assert B[1, 0] == X3[0, 0]\n    assert B[1, 1] == X3[0, 1]\n    assert B[0, 2] == X2[0, 0]\n    assert B[0, 3] == X2[0, 1]\n    assert B[1, 2] == X4[0, 0]\n    assert B[1, 3] == X4[0, 1]\n", "def test_block_collapse_with_block_diag_matrix():\n    B = BlockDiagMatrix(X1, X2, X3)\n    C = BlockDiagMatrix(Y, Y, Y)\n    result = block_collapse(B * C)\n    expected = BlockDiagMatrix(X1*Y, X2*Y, X3*Y)\n    assert result == expected\n", "def test_block_matrix_transpose():\n    B = BlockMatrix([[X, Z], [ZeroMatrix(m, n), Y]])\n    B_transpose = B.transpose()\n    expected_B_transpose = BlockMatrix([[X.T, ZeroMatrix(n, l)], [Z.T, Y.T]])\n    assert B_transpose == expected_B_transpose\n", "def test_block_matrix_operations():\n    # Test the BlockMatrix operations\n    B1 = BlockMatrix([[X1, Y], [Z, X2]])\n    B2 = BlockMatrix([[X3, ZeroMatrix(m, m)], [ZeroMatrix(m, m), X4]])\n\n    # Test multiplication\n    assert B1 * B2 == BlockMatrix([[X1*X3 + Y*X4, Y*X4], [Z*X3, Z*X4 + X2*X4]])\n\n    # Test addition\n    assert B1 + B2 == BlockMatrix([[X1 + X3, Y], [Z, X2 + X4]])\n\n    # Test transpose\n    assert B1.transpose() == BlockMatrix([[X1.transpose(), Z.transpose()], [Y.transpose(), X2.transpose()]])\n"], "sample_175": ["    def test_delete_restricted_objects(self):\n        a = A.objects.create()\n        m = M.objects.create(a=a)\n        a.delete()\n        with self.assertRaises(RestrictedError) as cm:\n            m.delete()\n        self.assertEqual(set(cm.exception.restricted_objects), {m})\n", "    def test_sort_method_with_circular_dependencies(self):\n        a1 = A.objects.create()\n        b1 = B1.objects.create(a=a1)\n        b2 = B2.objects.create(b1=b1)\n        b3 = B3.objects.create(b2=b2)\n        b4 = B3.objects.create(b2=b2)\n\n        collector = Collector(using=connection.alias)\n        collector.add([a1, b1, b2, b3, b4])\n\n        collector.sort()\n        sorted_models = list(collector.data.keys())\n\n        self.assertIn(A, sorted_models)\n        self.assertIn(B1, sorted_models)\n        self.assertIn(B2, sorted_models)\n        self.assertIn(B3, sorted_models)\n        self.assertEqual(sorted_models.index(B1), sorted_models.index(A) + 1)\n        self.assertEqual(sorted_models.index(B2), sorted_models.index(B1) + 1)\n        self.assertEqual(sorted_models.index(B3), sorted_models.index(B2) + 1)\n", "    def test_collector_with_restricted_objects(self):\n        p = P.objects.create()\n        MRNull.objects.create(p=p)\n        collector = Collector(using='default')\n        collector.collect([p])\n        self.assertIn(MRNull, collector.restricted_objects)\n", "    def test_collect_restricted_objects(self):\n        m = M.objects.create()\n        restricted_object = R.objects.create(m=m)\n        collector = Collector(using='default')\n        collector.add([m])\n        collector.collect([m], fail_on_restricted=False)\n        self.assertIn(R, collector.restricted_objects)\n        self.assertIn(M._meta.get_field('r'), collector.restricted_objects[R])\n        self.assertIn(restricted_object, collector.restricted_objects[R][M._meta.get_field('r')])\n", "    def test_restricted_error_raised(self):\n        a1 = A.objects.create(r=get_default_r())\n        r1 = R.objects.create(name=\"R1\", a=a1)\n        rc1 = RChild.objects.create(name=\"RC1\", r=r1)\n        RChildChild.objects.create(name=\"RCC1\", rchild=rc1)\n\n        collector = Collector(using=default)\n        collector.collect([a1])\n        with self.assertRaises(RestrictedError) as cm:\n            collector.delete()\n        self.assertIn('Cannot delete some instances of model', str(cm.exception))\n        self.assertIn('because they are referenced through restricted foreign keys', str(cm.exception))\n"], "sample_864": ["def test_mean_shift():\n    clustering = MeanShift(bandwidth=2).fit(X)\n    assert_array_equal(clustering.predict(centers), np.array([0, 1, 0]))\n    assert_array_equal(np.unique(clustering.labels_), np.array([0, 1]))\n", "def test_mean_shift_clustering():\n    # Test MeanShift clustering algorithm\n    ms = MeanShift(bandwidth=2)\n    ms.fit(X)\n    labels = ms.labels_\n    cluster_centers = ms.cluster_centers_\n    n_iter = ms.n_iter_\n\n    # Assert the number of clusters found\n    assert len(np.unique(labels)) == n_clusters\n\n    # Assert the number of iterations\n    assert n_iter <= ms.max_iter\n\n    # Assert the shape of cluster centers\n    assert cluster_centers.shape == (n_clusters, 2)\n", "def test_mean_shift_or_cluster_all_false():\n    # Test that when cluster_all is False, points outside of clusters are labelled -1\n    ms = MeanShift(bandwidth=1, cluster_all=False)\n    ms.fit(X)\n    labels = ms.labels_\n    assert -1 in labels, \"Points outside of clusters are not labelled -1 when cluster_all is False\"\n", "def test_mean_shift_estimate_bandwidth():\n    bandwidth = estimate_bandwidth(X, quantile=0.3)\n    assert bandwidth > 0\n", "def test_estimate_bandwidth():\n    bandwidth = estimate_bandwidth(X, quantile=0.3, n_samples=100, random_state=0, n_jobs=1)\n    assert bandwidth > 0, \"Bandwidth should be a positive value\"\n"], "sample_82": ["def test_format_value(self):\n    # Test with a datetime.date object\n    value = date(2020, 12, 31)\n    result = self.widget.format_value(value)\n    self.assertEqual(result, {'year': 2020, 'month': 12, 'day': 31})\n\n    # Test with a string in a valid format\n    value = '2020-12-31'\n    result = self.widget.format_value(value)\n    self.assertEqual(result, {'year': 2020, 'month': 12, 'day': 31})\n\n    # Test with a string in a localized format\n    with override_settings(USE_L10N=True, DATE_INPUT_FORMATS=['%d.%m.%Y']):\n        value = '31.12.2020'\n        result = self.widget.format_value(value)\n        self.assertEqual(result, {'year': 2020, 'month': 12, 'day': 31})\n\n    # Test with an invalid string\n    value = 'invalid'\n    result = self.widget.format_value(value)\n    self.assertEqual(result, {'year': None, 'month': None, 'day': None})\n\n    # Test with None\n    value = None\n    result = self.widget.format_value(value)\n    self.assertEqual(result, {'year': None, 'month': None, 'day': None})\n", "def test_select_date_widget_months(self):\n    widget = SelectDateWidget(\n        years=('2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016'),\n        months=MONTHS_AP,\n    )\n    html = widget.render('date', date(2008, 7, 1))\n    self.assertHTMLEqual(\n        '<select name=\"date_month\" id=\"id_date_month\">'\n        '<option value=\"\">---</option>'\n        '<option value=\"1\">Jan.</option>'\n        '<option value=\"2\">Feb.</option>'\n        '<option value=\"3\">Mar.</option>'\n        '<option value=\"4\">Apr.</option>'\n        '<option value=\"5\">May</option>'\n        '<option value=\"6\">June</option>'\n        '<option value=\"7\" selected>July</option>'\n        '<option value=\"8\">Aug.</option>'\n        '<option value=\"9\">Sept.</option>'\n        '<option value=\"10\">Oct.</option>'\n        '<option value=\"11\">Nov.</option>'\n        '<option value=\"12\">Dec.</option>'\n        '</select>'\n        '<select name=\"date_day\" id=\"id_date_day\">'\n        '<option value=\"\">---</option>'\n        '<option value=\"1\" selected>1</option>'\n        '<option value=\"2\">2</option>'\n        # Remaining days omitted for brevity\n        '<option value=\"31\">31</option>'\n        '</select>'\n        '<select name=\"date_year\" id=\"id_date_year\">'\n        '<option value=\"\">---</option>'\n        '<option value=\"2007\">2007</option>'\n        '<option value=\"2008\" selected>2008</option>'\n        '<option value=\"2009\">2009</option>'\n        # Remaining years omitted for brevity\n        '<option", "    def test_select_date_widget_localized_labels(self):\n        with translation.override('de'):\n            form = Form({}, date_field=DateField(widget=SelectDateWidget))\n            output = form.as_p()\n            self.assertIn('<option value=\"\">---</option>', output)\n            self.assertIn('<option value=\"1\">Januar</option>', output)\n            self.assertIn('<option value=\"12\">Dezember</option>', output)\n            self.assertIn('<option value=\"2015\">2015</option>', output)\n", "def test_select_date_widget_format_value(self):\n    # Test formatting a date value\n    value = date(2010, 5, 15)\n    formatted_value = self.widget.format_value(value)\n    self.assertEqual(formatted_value, {'year': 2010, 'month': 5, 'day': 15})\n\n    # Test formatting a non-date value\n    value = '2010-05-15'\n    formatted_value = self.widget.format_value(value)\n    self.assertEqual(formatted_value, {'year': 2010, 'month': 5, 'day': 15})\n\n    # Test formatting an invalid date value\n    value = '2010-15-32'\n    formatted_value = self.widget.format_value(value)\n    self.assertEqual(formatted_value, {'year': None, 'month': None, 'day': None})\n", "def test_value_from_datadict_with_valid_data(self):\n    data = {\n        'my_date_year': '2010',\n        'my_date_month': '12',\n        'my_date_day': '31',\n    }\n    output = self.widget.value_from_datadict(data, {}, 'my_date')\n    self.assertEqual(output, '2010-12-31')\n"], "sample_270": ["    def test_invalid_unique_together_type(self):\n        class InvalidUniqueTogetherModel(models.Model):\n            field1 = models.CharField(max_length=10)\n            field2 = models.CharField(max_length=10)\n\n            class Meta:\n                unique_together = 'invalid_type'\n\n        errors = InvalidUniqueTogetherModel.check()\n        self.assertEqual(len(errors), 1)\n        self.assertEqual(\n            errors[0],\n            Error(\n                \"'unique_together' must be a list or tuple.\",\n                obj=InvalidUniqueTogetherModel,\n                id='models.E010',\n            )\n        )\n", "    def test_unique_together_valid_fields(self):\n        class TestModel(models.Model):\n            field1 = models.CharField(max_length=10)\n            field2 = models.IntegerField()\n\n            class Meta:\n                unique_together = (('field1', 'field2'),)\n\n        errors = TestModel.check()\n        self.assertEqual(errors, [])\n", "    def test_include_in_indexes(self):\n        class Model(models.Model):\n            field1 = models.CharField(max_length=20)\n            field2 = models.IntegerField()\n\n            class Meta:\n                indexes = [\n                    models.Index(fields=['field1'], name='test_include', include=['field2']),\n                ]\n\n        errors = Model.check(databases=['default'])\n        self.assertEqual(errors, [])\n", "    def test_inheritance(self):\n        class BaseModel(models.Model):\n            field = models.CharField(max_length=10)\n\n        class ChildModel(BaseModel):\n            field = models.IntegerField()\n\n        errors = ChildModel.check()\n        self.assertEqual(len(errors), 1)\n        self.assertEqual(\n            errors[0],\n            Error(\n                \"The field 'field' clashes with the field 'field' from model 'invalid_models_tests.basemodel'.\",\n                obj=ChildModel._meta.get_field('field'),\n                id='models.E006',\n            )\n        )\n", "    def setUpClass(cls):\n        class IndexModel(models.Model):\n            field1 = models.CharField(max_length=20)\n            field2 = models.IntegerField()\n            field3 = models.DateTimeField()\n\n            class Meta:\n                indexes = [\n                    models.Index(fields=['field1', 'field2']),\n                    models.Index(fields=['-field3'], name='custom_index'),\n                    models.Index(fields=['field1'], name='long_index_name' * 10),\n                ]\n\n        cls.Model = IndexModel\n"], "sample_77": ["def test_urlize(self):\n    \"\"\"Test urlize() function.\"\"\"\n    self.check_output(urlize, 'http://example.com', '<a href=\"http://example.com\">http://example.com</a>')\n    self.check_output(urlize, 'www.example.com', '<a href=\"http://www.example.com\">www.example.com</a>')\n    self.check_output(urlize, 'example@example.com', '<a href=\"mailto:example@example.com\">example@example.com</a>')\n    self.check_output(urlize, 'example@example.com.', 'example@example.com.')  # Ensure punctuation isn't trimmed incorrectly\n    self.check_output(urlize, 'example@example.com,', '<a href=\"mailto:example@example.com\">example@example.com</a>,')\n    self.check_output(urlize, '(http://example.com)', '(<a href=\"http://example.com\">http://example.com</a>)')\n    self.check_output(urlize, 'http://example.com)', '<a href=\"http://example.com\">http://example.com</a>)')\n", "def test_urlize(self):\n    self.check_output(urlize, 'Visit https://www.djangoproject.com for more info.',\n                      'Visit <a href=\"https://www.djangoproject.com\">https://www.djangoproject.com</a> for more info.')\n    self.check_output(urlize, 'Email me at john.doe@example.com for help.',\n                      'Email me at <a href=\"mailto:john.doe@example.com\">john.doe@example.com</a> for help.')\n", "def test_urlize(self):\n    self.check_output(urlize, 'http://example.com', '<a href=\"http://example.com\">http://example.com</a>')\n    self.check_output(urlize, 'www.example.com', '<a href=\"http://www.example.com\">www.example.com</a>')\n    self.check_output(urlize, 'user@example.com', '<a href=\"mailto:user@example.com\">user@example.com</a>')\n    self.check_output(urlize, 'http://example.com/path?query=string', '<a href=\"http://example.com/path?query=string\">http://example.com/path?query=string</a>')\n    self.check_output(urlize, 'http://example.com/path?query=string&another=value', '<a href=\"http://example.com/path?query=string&another=value\">http://example.com/path?query=string&another=value</a>')\n    self.check_output(urlize, '(http://example.com)', '(<a href=\"http://example.com\">http://example.com</a>)')\n    self.check_output(urlize, 'http://example.com.', '<a href=\"http://example.com\">http://example.com</a>.')\n", "def test_urlize(self):\n    self.check_output(urlize, 'http://example.com', '<a href=\"http://example.com\">http://example.com</a>')\n    self.check_output(urlize, 'https://example.com', '<a href=\"https://example.com\">https://example.com</a>')\n    self.check_output(urlize, 'www.example.com', '<a href=\"http://www.example.com\">www.example.com</a>')\n    self.check_output(urlize, 'user@example.com', '<a href=\"mailto:user@example.com\">user@example.com</a>')\n    self.check_output(urlize, 'Visit http://example.com for more.', 'Visit <a href=\"http://example.com\">http://example.com</a> for more.')\n    self.check_output(urlize, 'Visit www.example.com for more.', 'Visit <a href=\"http://www.example.com\">www.example.com</a> for more.')\n    self.check_output(urlize, 'Send email to user@example.com.', 'Send email to <a href=\"mailto:user@example.com\">user@example.com</a>.')\n", "    def test_urlize_complex_url(self):\n        complex_url = 'https://example.com/some/path?param1=value1&param2=value2'\n        escaped_complex_url = 'https://example.com/some/path?param1=value1&amp;param2=value2'\n        expected_output = '<a href=\"{}\" rel=\"nofollow\">{}</a>'.format(escaped_complex_url, complex_url)\n        self.check_output(urlize, complex_url, expected_output)\n"], "sample_352": ["    def test_split_having_with_aggregate(self):\n        items = Item.objects.annotate(num_tags=Count('tags'))\n        query = items.filter(num_tags__gt=1)\n        where, having = query.where.split_having()\n        self.assertIsNone(where)\n        self.assertIsNotNone(having)\n", "def test_where_node_split_having_with_aggregate(self):\n    # Create a WhereNode with an aggregate function\n    q = Q(creator__note__extra__value__avg__gt=42)\n    query = Item.objects.filter(q)\n    query.query.where.resolve_expression(query)\n    where_node, having_node = query.query.where.split_having()\n\n    # Check that the aggregate function is in the having clause\n    self.assertIsNone(where_node)\n    self.assertIsNotNone(having_node)\n    self.assertTrue(having_node.contains_aggregate)\n", "    def test_split_having_clause(self):\n        # Test the split_having method for WHERE nodes containing aggregates\n        i = Item.objects.annotate(num_tags=Count('tags'))\n        i = i.filter(num_tags__gt=1)\n        where_node, having_node = i.query.where.split_having()\n        self.assertIsNone(where_node)\n        self.assertIsInstance(having_node, WhereNode)\n\n        # Test the split_having method for WHERE nodes not containing aggregates\n        i = Item.objects.filter(created__gt=self.time1)\n        where_node, having_node = i.query.where.split_having()\n        self.assertIsInstance(where_node, WhereNode)\n        self.assertIsNone(having_node)\n", "def test_split_having(self):\n    # Test split_having method when node contains aggregate\n    i = Item.objects.aggregate(Max('created'))\n    q = Item.objects.filter(created__gt=i['created__max'])\n    where_node, having_node = q.query.where.split_having()\n    self.assertIsNone(where_node)\n    self.assertIsNotNone(having_node)\n\n    # Test split_having method when node does not contain aggregate\n    q = Item.objects.filter(name='one')\n    where_node, having_node = q.query.where.split_having()\n    self.assertIsNotNone(where_node)\n    self.assertIsNone(having_node)\n", "def test_where_node_split_having(self):\n    # Test split_having method when contains_aggregate is True and connector is AND\n    where_node = WhereNode([self.i1, self.i2], AND, False)\n    where_node.contains_aggregate = True\n    self.assertIsNone(where_node.split_having()[0])\n    self.assertEqual(where_node.split_having()[1], where_node)\n\n    # Test split_having method when contains_aggregate is True and connector is OR\n    where_node = WhereNode([self.i1, self.i2], OR, False)\n    where_node.contains_aggregate = True\n    self.assertIsNone(where_node.split_having()[0])\n    self.assertEqual(where_node.split_having()[1], where_node)\n\n    # Test split_having method when contains_aggregate is False\n    where_node = WhereNode([self.i1, self.i2], AND, False)\n    where_node.contains_aggregate = False\n    self.assertEqual(where_node.split_having()[0], where_node)\n    self.assertIsNone(where_node.split_having()[1])\n"], "sample_840": ["def test_pls_svd_algorithm():\n    X, Y = load_linnerud(return_X_y=True)\n    pls = pls_.PLSRegression(n_components=2, algorithm=\"svd\")\n    with assert_raise_message(ValueError, \"Incompatible configuration: mode B is not implemented with svd algorithm\"):\n        pls.fit(X, Y)\n", "def test_PLS_nipals_with_zero_column_in_Y():\n    # Test that PLS with NIPALS algorithm handles zero columns in Y\n    X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    Y = np.array([[0, 0], [0, 0], [0, 0]])\n\n    pls = pls_.PLSRegression(n_components=1, algorithm='nipals')\n    pls.fit(X, Y)\n\n    # Since Y is all zeros, the scores, weights, and loadings should be zero\n    assert_array_equal(pls.x_scores_, np.zeros((3, 1)))\n    assert_array_equal(pls.y_scores_, np.zeros((3, 1)))\n    assert_array_equal(pls.x_weights_, np.zeros((3, 1)))\n    assert_array_equal(pls.y_weights_, np.zeros((2, 1)))\n    assert_array_equal(pls.x_loadings_, np.zeros((3, 1)))\n    assert_array_equal(pls.y_loadings_, np.zeros((2, 1)))\n", "def test_pls_regression_n_iter():\n    # Test that PLSRegression.n_iter_ attribute is correctly set\n    rng = check_random_state(0)\n    X = rng.rand(10, 5)\n    Y = rng.rand(10, 3)\n    pls = PLSRegression(n_components=2, max_iter=1000)\n    pls.fit(X, Y)\n    assert len(pls.n_iter_) == 2\n    assert all(iter < 1000 for iter in pls.n_iter_)\n", "def test_pls_nipals_inner_loop_max_iter():\n    rng = check_random_state(0)\n    X = rng.random_sample((100, 5))\n    Y = rng.random_sample((100, 3))\n    max_iter = 10\n    with assert_warns(ConvergenceWarning):\n        pls_._nipals_twoblocks_inner_loop(X, Y, mode=\"A\", max_iter=max_iter, tol=1e-10)\n", "def test_pls_regression_with_zero_variance_column():\n    rng = check_random_state(42)\n    X = rng.randn(10, 5)\n    Y = rng.randn(10, 3)\n    # Add a column of zeros to X\n    X[:, 2] = 0\n    pls_regression = pls_.PLSRegression(n_components=3)\n    pls_regression.fit(X, Y)\n    assert np.allclose(pls_regression.x_weights_[:, 2], 0), \"PLSRegression did not handle column of zeros in X correctly\"\n"], "sample_968": ["def test_parse_annotation():\n    # Test parsing of a simple type annotation\n    env = Mock()\n    result = _parse_annotation(\"int\", env)\n    assert_node(result[0], pending_xref, reftarget=\"int\")\n\n    # Test parsing of a nested type annotation\n    result = _parse_annotation(\"List[int]\", env)\n    assert_node(result[0], desc_sig_punctuation, text=\"[\")\n    assert_node(result[1], pending_xref, reftarget=\"int\")\n    assert_node(result[2], desc_sig_punctuation, text=\"]\")\n", "def test_parse_annotation(app):\n    env = Mock()\n    env.config.python_use_unqualified_type_names = False\n\n    annotation = \"List[Union[str, int]]\"\n    expected_output = [\"List\", nodes.Text(\"[\", \"[\"), \"Union\", nodes.Text(\"[\", \"[\"), \"str\",\n                       nodes.Text(\",\"), nodes.Text(\" \"), \"int\", nodes.Text(\"]\", \"]\"),\n                       nodes.Text(\"]\", \"]\")]\n\n    result = _parse_annotation(annotation, env)\n    for node, expected in zip(result, expected_output):\n        if isinstance(expected, str):\n            assert isinstance(node, pending_xref)\n            assert node.astext() == expected\n        else:\n            assert isinstance(node, type(expected))\n            assert node.astext() == expected.astext()\n\n    assert len(result) == len(expected_output)\n", "def test_parse_annotation():\n    env = Mock()\n    env.config.python_use_unqualified_type_names = False\n    result = _parse_annotation(\"List[int]\", env)\n    assert isinstance(result[0], pending_xref)\n    assert result[0].get(\"reftarget\") == \"List\"\n    assert result[1] == nodes.Text(\"[\")\n    assert isinstance(result[2], pending_xref)\n    assert result[2].get(\"reftarget\") == \"int\"\n    assert result[3] == nodes.Text(\"]\")\n", "def test_parse_annotation():\n    env = Mock()\n    env.config = Mock(python_use_unqualified_type_names=False)\n    annotation = \"Dict[str, List[int]]\"\n    result = _parse_annotation(annotation, env)\n    expected = [\n        nodes.Text('Dict'),\n        nodes.Text('['),\n        nodes.Text('str'),\n        nodes.Text(','),\n        nodes.Text(' '),\n        nodes.Text('List'),\n        nodes.Text('['),\n        nodes.Text('int'),\n        nodes.Text(']'),\n        nodes.Text(']'),\n    ]\n    assert_node(result, expected)\n", "def test_parse_annotation():\n    env = Mock()\n    annotations = [\"Optional[int]\", \"List[str]\", \"Dict[str, int]\", \"Tuple[int, ...]\",\n                   \"Union[int, str]\", \"TypeVar('T')\", \"Callable[[int], str]\"]\n    for annotation in annotations:\n        result = _parse_annotation(annotation, env)\n        assert isinstance(result, list)\n        assert all(isinstance(node, nodes.Node) for node in result)\n"], "sample_791": ["def test_onehotencoder_drop_first():\n    enc = OneHotEncoder(drop='first', sparse=False)\n    X = [['Male', 1], ['Female', 3], ['Female', 2]]\n    X_transformed = enc.fit_transform(X)\n    expected_transformed = np.array([[0., 1.], [1., 0.], [1., 1.]])\n    assert_array_equal(X_transformed, expected_transformed)\n", "def test_one_hot_encoder_drop():\n    X = [['Male', 1], ['Female', 3], ['Female', 2]]\n    enc = OneHotEncoder(drop='first')\n    enc.fit(X)\n    assert_array_equal(enc.transform([['Female', 1], ['Male', 2]]).toarray(),\n                       [[0., 0.], [1., 1.]])\n\n    enc = OneHotEncoder(drop=[0, 1])\n    enc.fit(X)\n    assert_array_equal(enc.transform([['Female', 1], ['Male', 2]]).toarray(),\n                       [[0.], [0.]])\n\n    enc = OneHotEncoder(drop='invalid')\n    with assert_raises_regex(ValueError, \"Wrong input for parameter `drop`\"):\n        enc.fit(X)\n", "def test_transform_with_drop_first():\n    X = np.array([[0, 1], [1, 0], [2, 1], [0, 2], [1, 2]])\n    enc = OneHotEncoder(drop='first')\n    enc.fit(X)\n    result = enc.transform(X).toarray()\n    expected = np.array([[0, 1], [1, 0], [0, 1], [0, 1], [1, 1]])\n    assert_array_equal(result, expected)\n", "def test_OneHotEncoder_handle_unknown():\n    enc = OneHotEncoder(handle_unknown='ignore')\n    X = [['Male', 1], ['Female', 3], ['Female', 2]]\n    enc.fit(X)\n    X_test = [['Female', 1], ['Unknown', 4]]\n    X_transformed = enc.transform(X_test).toarray()\n    expected_transformed = np.array([[1., 0., 1., 0., 0.], [0., 0., 0., 0., 0.]])\n    assert_array_equal(X_transformed, expected_transformed)\n    X_inverse_transformed = enc.inverse_transform(X_transformed)\n    expected_inverse_transformed = np.array([['Male', 1], [None, None]], dtype=object)\n    assert_array_equal(X_inverse_transformed, expected_inverse_transformed)\n", "def test_inverse_transform_dense(enc):\n    X = np.array([['Male', 1], ['Female', 3], ['Female', 2]], dtype=object)\n    enc.fit(X)\n    X_transformed = enc.transform(X)\n    X_inverse_transformed = enc.inverse_transform(X_transformed)\n    assert_array_equal(X, X_inverse_transformed)\n"], "sample_597": ["def test_merge_compat_override(self):\n    ds1 = xr.Dataset({'x': ('time', [1, 2, 3])}, coords={'time': ('time', [10, 20, 30])})\n    ds2 = xr.Dataset({'x': ('time', [1, 2, 4])}, coords={'time': ('time', [10, 20, 30])})\n\n    merged = merge([ds1, ds2], compat='override')\n    expected = xr.Dataset({'x': ('time', [1, 2, 4])}, coords={'time': ('time', [10, 20, 30])})\n    assert_identical(merged, expected)\n", "def test_merge_internals_broadcast_equals_compat(self):\n    ds1 = xr.Dataset({\"a\": ([\"x\", \"y\"], np.arange(6).reshape(2, 3))})\n    ds2 = xr.Dataset({\"a\": ([\"x\"], np.arange(2))})\n    merged = merge([ds1, ds2], compat=\"broadcast_equals\")\n    expected = xr.Dataset({\"a\": ds1[\"a\"]})\n    assert_identical(merged, expected)\n", "def test_merge_coords(self):\n    coords1 = {\"x\": (\"x\", np.arange(3)), \"y\": (\"y\", np.arange(4))}\n    coords2 = {\"x\": (\"x\", np.arange(3)), \"z\": (\"z\", np.arange(2))}\n    variables, indexes = merge.merge_coords([coords1, coords2])\n    assert set(variables) == {\"x\", \"y\", \"z\"}\n    assert set(indexes) == {\"x\"}\n", "def test_merge_with_explicit_coords():\n    data = {\"time\": [1, 2, 3], \"x\": [1, 2, 3]}\n    coords = {\"time\": [1, 2, 3]}\n    ds = xr.Dataset(data, coords=coords)\n    result = merge([ds], explicit_coords=[\"time\"])\n    assert \"time\" in result.coords\n    assert \"time\" not in result.data_vars\n", "def test_merge_coords(self):\n    obj1 = xr.Dataset({\"a\": ([\"x\", \"y\"], np.arange(6).reshape(2, 3))}, coords={\"x\": [\"a\", \"b\"]})\n    obj2 = xr.Dataset({\"b\": ([\"x\", \"z\"], np.arange(6).reshape(2, 3))}, coords={\"x\": [\"a\", \"b\"], \"z\": [1, 2, 3]})\n\n    variables, indexes = merge.merge_coords([obj1, obj2])\n\n    expected_variables = {\n        \"x\": xr.Variable((\"x\",), [\"a\", \"b\"]),\n        \"z\": xr.Variable((\"z\",), [1, 2, 3]),\n    }\n    assert variables == expected_variables\n    assert indexes == {\"x\": variables[\"x\"].to_index()}\n"], "sample_1010": ["def test_latex_primenu():\n    expr = primenu(123)\n    assert latex(expr) == r'\\nu\\left(123\\right)'\n", "def test_Quaternion():\n    i, j, k = symbols('i j k')\n    q = Quaternion(1, i, j, k)\n    assert latex(q) == r'1 + i\\mathbf{i} + j\\mathbf{j} + k\\mathbf{k}'\n", "def test_next_unit_test():\n    expr = sin(x) + cos(x)\n    expected = r\"\\sin{\\left(x\\right)} + \\cos{\\left(x\\right)}\"\n    assert latex(expr) == expected\n", "def test_latex_primenu():\n    expr = primenu(123)\n    assert latex(expr) == \"\\\\nu\\\\left(123\\\\right)\"\n", "def test_matrix_latex_printing():\n    M = Matrix([[1, 0], [0, 1]])\n    assert latex(M) == r\"\\left[\\begin{matrix}1 & 0\\\\0 & 1\\end{matrix}\\right]\"\n"], "sample_812": ["def test_changed_params():\n    estimator = LogisticRegression(C=0.5)\n    params = _changed_params(estimator)\n    assert params == {'C': 0.5}\n", "def test_estimator_pretty_printer_compact_parameters():\n    estimator = LogisticRegression(C=2.0, penalty='l1', tol=1e-3)\n    printer = _EstimatorPrettyPrinter(compact=True)\n    output = printer.pformat(estimator)\n    assert output == \"LogisticRegression(C=2.0, penalty='l1', tol=0.001)\"\n", "def test_print_changed_only():\n    set_config(print_changed_only=True)\n\n    lr = LogisticRegression(penalty='l1', C=0.5, max_iter=200)\n    rfe = RFE(lr, n_features_to_select=5, step=2)\n\n    expected_output = \"RFE(estimator=LogisticRegression(C=0.5, max_iter=200, penalty='l1'), n_features_to_select=5, step=2)\"\n    assert repr(rfe) == expected_output\n\n    set_config(print_changed_only=False)\n", "def test_estimator_pretty_printer_changed_only():\n    lr = LogisticRegression(C=0.5, penalty='l1', max_iter=200)\n    pp = _EstimatorPrettyPrinter(compact=True, n_max_elements_to_show=None)\n    set_config(print_changed_only=True)\n    output = pp.pformat(lr)\n    assert output == \"LogisticRegression(C=0.5, max_iter=200, penalty='l1')\"\n", "def test_estimator_pretty_printer_with_long_params():\n    # Test that the _EstimatorPrettyPrinter correctly handles estimators with\n    # many parameters that don't fit on a single line\n    params = {\"param{}\".format(i): i for i in range(20)}\n    estimator = LogisticRegressionCV(**params)\n\n    pp = _EstimatorPrettyPrinter(n_max_elements_to_show=10)\n    output = pp.pformat(estimator)\n\n    # Check that the output is correctly truncated\n    assert \"param10=\" not in output\n    assert \"param11=\" not in output\n    assert \"..., \" in output\n\n    # Check that all parameters are present in the output\n    for i in range(10):\n        assert \"param{}={}\".format(i, i) in output\n"], "sample_770": ["def test_silhouette_samples_sparse():\n    X = csr_matrix([[0, 1, 0], [1, 0, 1]])\n    labels = [0, 1]\n    expected_result = np.array([-1., 1.])\n    np.testing.assert_array_almost_equal(silhouette_samples(X, labels), expected_result)\n", "def test_silhouette_score_with_sample_size():\n    # Test silhouette_score with sample_size parameter\n    X, y = datasets.make_blobs(n_samples=100, n_features=2, centers=3, random_state=42)\n    sample_size = 50\n    score = silhouette_score(X, y, sample_size=sample_size, random_state=42)\n    assert isinstance(score, float)\n    assert -1 <= score <= 1\n\n    # Check that the sample size is correctly used\n    score_full = silhouette_score(X, y)\n    score_subset = silhouette_score(X[:sample_size], y[:sample_size], random_state=42)\n    assert score_subset != score_full\n", "def test_silhouette_score_with_precomputed_distance():\n    X = np.array([[0, 1], [2, 3], [4, 5]])\n    labels = np.array([0, 1, 1])\n    precomputed_distance = pairwise_distances(X)\n\n    score = silhouette_score(precomputed_distance, labels, metric=\"precomputed\")\n    assert_greater(score, -1)\n    assert_greater(1, score)\n", "def test_silhouette_score_with_sparse_input():\n    # Create a sparse matrix with 4 samples and 3 features\n    X = csr_matrix([[1, 2, 0], [2, 0, 3], [0, 1, 4], [5, 0, 6]])\n    labels = np.array([0, 0, 1, 1])\n\n    # Calculate the silhouette score\n    score = silhouette_score(X, labels, metric=\"euclidean\")\n\n    # Check if the score is a float value\n    assert isinstance(score, float)\n\n    # Check if the score is within the valid range (-1 to 1)\n    assert -1 <= score <= 1\n", "def test_silhouette_score_with_sample_size():\n    X, y = datasets.make_blobs(n_samples=100, centers=2, random_state=42)\n    sample_size = 50\n    score = silhouette_score(X, y, sample_size=sample_size, random_state=42)\n    assert_greater(score, 0.5)\n\n    # Test with precomputed distance matrix\n    dist_matrix = pairwise_distances(X)\n    score_precomputed = silhouette_score(dist_matrix, y, metric=\"precomputed\", sample_size=sample_size, random_state=42)\n    assert_equal(score, score_precomputed)\n\n    # Test with sparse distance matrix\n    sparse_dist_matrix = csr_matrix(dist_matrix)\n    score_sparse = silhouette_score(sparse_dist_matrix, y, metric=\"precomputed\", sample_size=sample_size, random_state=42)\n    assert_equal(score, score_sparse)\n"], "sample_413": ["    def test_check_string_if_invalid_is_string(self):\n        errors = check_string_if_invalid_is_string(None)\n        self.assertEqual(len(errors), 1)\n        self.assertEqual(errors[0].id, E002.id)\n", "def test_check_setting_app_dirs_loaders(self):\n    errors = check_setting_app_dirs_loaders(None)\n    self.assertEqual(len(errors), 1)\n    self.assertEqual(errors[0], E001)\n", "def test_check_setting_app_dirs_loaders(self):\n    errors = check_setting_app_dirs_loaders(None)\n    self.assertEqual(len(errors), 1)\n    self.assertEqual(errors[0], E001)\n", "    def test_string_if_invalid_is_string(self):\n        TEMPLATES_INVALID_STRING = [\n            {\n                \"BACKEND\": \"django.template.backends.django.DjangoTemplates\",\n                \"OPTIONS\": {\n                    \"string_if_invalid\": 123,\n                },\n            },\n        ]\n\n        with override_settings(TEMPLATES=TEMPLATES_INVALID_STRING):\n            errors = check_string_if_invalid_is_string(None)\n            self.assertEqual(len(errors), 1)\n            self.assertEqual(errors[0], Error(E002.msg.format(123, \"int\"), id=E002.id))\n", "    def test_string_if_invalid_is_string(self):\n        self.assertEqual(\n            check_string_if_invalid_is_string(None),\n            [copy(E002).msg.format(123, 'int')],\n        )\n"], "sample_1203": ["def test_group_isomorphism():\n    D = DihedralGroup(8)\n    p = Permutation(0, 1, 2, 3, 4, 5, 6, 7)\n    P = PermutationGroup(p)\n    assert not group_isomorphism(D, P)\n\n    F, a, b = free_group(\"a, b\")\n    G = FpGroup(F, [a**3, b**3, (a*b)**2])\n    H = AlternatingGroup(4)\n    check, T = group_isomorphism(G, H)\n    assert check\n    assert T(b*a*b**-1*a**-1*b**-1) == Permutation(0, 2, 3)\n\n    G = CyclicGroup(4)\n    H = PermutationGroup([Permutation(0, 1, 2, 3)])\n    assert is_isomorphic(G, H)\n", "def test_block_homomorphism():\n    # Create a permutation group and a block system\n    p = Permutation(0, 1, 2, 3)\n    q = Permutation(1, 2)\n    G = PermutationGroup(p, q)\n    blocks = [0, 0, 1, 1]\n\n    # Compute the block homomorphism\n    H = block_homomorphism(G, blocks)\n\n    # Check that the homomorphism is correct\n    assert H(p) == Permutation(0, 1)\n    assert H(q) == Permutation(0, 1)\n\n    # Check that the kernel is correct\n    assert H.kernel() == PermutationGroup(p**2)\n", "def test_group_isomorphism_dihedral_group_is_isomorphic():\n    D1 = DihedralGroup(6)\n    D2 = DihedralGroup(6)\n    assert is_isomorphic(D1, D2)\n", "def test_group_isomorphism_dihedral_cyclic():\n    D = DihedralGroup(8)\n    C = CyclicGroup(8)\n    assert is_isomorphic(D, C)\n\n    # Check that the isomorphism is correct\n    check, T = group_isomorphism(D, C)\n    assert check\n    assert T(D.generators[0]) == C.generators[0]\n    assert T(D.generators[1]) == C.generators[0]**4\n", "def test_block_homomorphism():\n    p = Permutation(0, 1, 2, 3)\n    q = Permutation(4, 5)\n    G = PermutationGroup(p, q)\n    H = block_homomorphism(G, [0, 1, 2, 3, 4, 5])\n    assert H.domain == G\n    assert H.codomain == PermutationGroup([0, 1])\n    assert H(p) == Permutation(0, 1)\n    assert H(q) == Permutation(1, 0)\n\n    p = Permutation(0, 1, 2)\n    q = Permutation(3, 4)\n    G = PermutationGroup(p, q)\n    H = block_homomorphism(G, [0, 1, 2, 3, 3, 4])\n    assert H.domain == G\n    assert H.codomain == PermutationGroup([0, 1, 2])\n    assert H(p) == Permutation(0, 1, 2)\n    assert H(q) == Permutation(2, 1)\n"], "sample_407": ["def test_reporter_article_relationship(self):\n    self.assertEqual(self.r.article_set.count(), 1)\n    self.assertEqual(self.r2.article_set.count(), 0)\n    self.assertEqual(self.a.reporter, self.r)\n\n    self.r2.article_set.add(self.a)\n    self.assertEqual(self.r.article_set.count(), 0)\n    self.assertEqual(self.r2.article_set.count(), 1)\n    self.assertEqual(self.a.reporter, self.r2)\n", "def test_reporter_article_relationship(self):\n    # Test the reporter-article relationship\n    self.assertEqual(self.r.article_set.count(), 1)\n    self.assertEqual(self.r.article_set.first(), self.a)\n    self.assertEqual(self.r2.article_set.count(), 0)\n\n    # Test changing the reporter of an article\n    self.a.reporter = self.r2\n    self.a.save()\n    self.assertEqual(self.r.article_set.count(), 0)\n    self.assertEqual(self.r2.article_set.count(), 1)\n    self.assertEqual(self.r2.article_set.first(), self.a)\n\n    # Test deleting a reporter\n    with self.assertRaises(ProtectedError):\n        self.r2.delete()\n    self.r2.article_set.clear()\n    self.r2.delete()\n", "    def setUpTestData(cls):\n        # Create a Reporter and an Article.\n        cls.r = Reporter(first_name=\"John\", last_name=\"Doe\", email=\"john@example.com\")\n        cls.r.save()\n        cls.a = Article(\n            headline=\"Test Article\",\n            pub_date=datetime.date(2022, 1, 1),\n            reporter=cls.r,\n        )\n        cls.a.save()\n", "def test_create_article_with_valid_reporter(self):\n    new_article = Article(\n        headline=\"New Article\",\n        pub_date=datetime.date(2022, 1, 1),\n        reporter=self.r2\n    )\n    new_article.save()\n    self.assertEqual(Article.objects.count(), 2)\n    self.assertEqual(new_article.reporter, self.r2)\n", "    def test_related_object_cache_invalidation(self):\n        # Test that related object cache is invalidated when a related object changes\n        a = Article.objects.get(headline=\"This is a test\")\n        self.assertEqual(a.reporter, self.r)\n\n        # Update the reporter\n        self.r.first_name = \"UpdatedName\"\n        self.r.save()\n\n        # Refresh the article from the database\n        a.refresh_from_db()\n\n        # Check that the reporter has been updated in the cache\n        self.assertEqual(a.reporter.first_name, \"UpdatedName\")\n\n        # Change the reporter of the article\n        a.reporter = self.r2\n        a.save()\n\n        # Check that the reporter has been updated in the cache\n        self.assertEqual(a.reporter, self.r2)\n"], "sample_117": ["    def test_authentication_form_with_valid_credentials(self):\n        form = AuthenticationForm(data={'username': 'testclient', 'password': 'password'})\n        self.assertTrue(form.is_valid())\n        self.assertEqual(form.get_user(), self.u1)\n", "    def test_clean_password(self):\n        form = UserChangeForm(instance=self.u1)\n        form.cleaned_data = {'password': 'new_password'}\n        form.initial = {'password': self.u1.password}\n        self.assertEqual(form.clean_password(), self.u1.password)\n", "    def test_clean_password(self):\n        form = UserChangeForm(instance=self.u1)\n        cleaned_password = form.clean_password()\n        self.assertEqual(cleaned_password, form.initial.get('password'))\n", "    def test_clean_password(self):\n        form = UserChangeForm(instance=self.u1)\n        form.initial = {'password': self.u1.password}\n        self.assertEqual(form.clean_password(), self.u1.password)\n", "    def test_clean(self):\n        # Correct username and password\n        form_data = {'username': 'testclient', 'password': 'password'}\n        form = AuthenticationForm(data=form_data)\n        self.assertTrue(form.is_valid())\n        self.assertEqual(form.get_user(), self.u1)\n\n        # Inactive user\n        form_data = {'username': 'inactive', 'password': 'password'}\n        form = AuthenticationForm(data=form_data)\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['__all__'][0], form.error_messages['inactive'])\n\n        # Empty password\n        form_data = {'username': 'empty_password', 'password': ''}\n        form = AuthenticationForm(data=form_data)\n        self.assertFalse(form.is_valid())\n        self.assertIn(form.error_messages['invalid_login'], str(form.errors['__all__'][0]))\n\n        # Unmanageable password\n        form_data = {'username': 'unmanageable_password', 'password': '$'}\n        form = AuthenticationForm(data=form_data)\n        self.assertFalse(form.is_valid())\n        self.assertIn(form.error_messages['invalid_login'], str(form.errors['__all__'][0]))\n\n        # Unknown password\n        form_data = {'username': 'unknown_password', 'password': 'foo$bar'}\n        form = AuthenticationForm(data=form_data)\n        self.assertFalse(form.is_valid())\n        self.assertIn(form.error_messages['invalid_login'], str(form.errors['__all__'][0]))\n"], "sample_546": ["def test_subplots_adjust():\n    fig, ax = plt.subplots()\n    fig.subplots_adjust(left=0.2, bottom=0.2, right=0.8, top=0.8)\n    assert fig.subplotpars.left == 0.2\n    assert fig.subplotpars.bottom == 0.2\n    assert fig.subplotpars.right == 0.8\n    assert fig.subplotpars.top == 0.8\n", "def test_figimage_resize():\n    fig = plt.figure()\n    data = np.random.random((50, 50))\n    fig.figimage(data, resize=True)\n    assert fig.get_size_inches() == (50 / fig.dpi, 50 / fig.dpi)\n", "def test_figure_clear():\n    fig = Figure()\n    ax = fig.add_subplot(111)\n    line, = ax.plot([1, 2, 3], [4, 5, 6])\n    assert fig.axes == [ax]\n    assert fig.lines == [line]\n    fig.clear()\n    assert fig.axes == []\n    assert fig.lines == []\n", "def test_figaspect():\n    # Test figaspect function with a simple aspect ratio\n    w, h = figaspect(2.)\n    assert w == pytest.approx(2. / 3. * h)\n\n    # Test figaspect function with an array\n    A = np.random.rand(5, 3)\n    w, h = figaspect(A)\n    assert w == pytest.approx(3. / 5. * h)\n\n    # Test figaspect function with a large aspect ratio\n    w, h = figaspect(10.)\n    assert w == pytest.approx(0.1 * h)\n\n    # Test figaspect function with a small aspect ratio\n    w, h = figaspect(0.1)\n    assert w == pytest.approx(10. * h)\n", "def test_subplots_adjust():\n    fig, ax = plt.subplots()\n    fig.subplots_adjust(left=0.2, bottom=0.2, right=0.8, top=0.8)\n    assert fig.subplotpars.left == 0.2\n    assert fig.subplotpars.bottom == 0.2\n    assert fig.subplotpars.right == 0.8\n    assert fig.subplotpars.top == 0.8\n"], "sample_296": ["    def test_default(self):\n        encoder = MessageEncoder()\n        msg = Message(constants.INFO, 'test message')\n        encoded_msg = encoder.default(msg)\n        self.assertEqual(encoded_msg, [MessageEncoder.message_key, 0, constants.INFO, 'test message'])\n\n        safe_msg = Message(constants.INFO, mark_safe('test safe message'))\n        encoded_safe_msg = encoder.default(safe_msg)\n        self.assertEqual(encoded_safe_msg, [MessageEncoder.message_key, 1, constants.INFO, 'test safe message'])\n\n        msg_with_tags = Message(constants.INFO, 'test message', extra_tags='tags')\n        encoded_msg_with_tags = encoder.default(msg_with_tags)\n        self.assertEqual(encoded_msg_with_tags, [MessageEncoder.message_key, 0, constants.INFO, 'test message', 'tags'])\n\n        non_message_obj = \"not a message object\"\n        self.assertEqual(encoder.default(non_message_obj), non_message_obj)\n", "    def test_dumps_loads(self):\n        messages = [\n            Message(constants.INFO, 'Test message 1'),\n            Message(constants.ERROR, mark_safe('Test message 2')),\n        ]\n        serializer = MessageSerializer()\n        serialized = serializer.dumps(messages)\n        deserialized = serializer.loads(serialized)\n        self.assertEqual(messages, deserialized)\n", "def test_message_serialization_with_safedata(self):\n    message = Message(constants.INFO, 'Test message', extra_tags='safe')\n    message.message = mark_safe(message.message)\n    storage = CookieStorage(self.request)\n    response = self.get_response()\n    storage._store([message], response)\n\n    # Verify that the message was stored correctly\n    self.assertEqual(self.stored_messages_count(storage, response), 1)\n\n    # Retrieve the message\n    messages, _ = storage._get()\n    self.assertEqual(len(messages), 1)\n    self.assertEqual(messages[0].level, constants.INFO)\n    self.assertEqual(messages[0].message, 'Test message')\n    self.assertIsInstance(messages[0].message, SafeData)\n    self.assertEqual(messages[0].extra_tags, 'safe')\n", "    def test_decode_with_bad_data(self):\n        storage = self.storage_class(self.request)\n        bad_data = get_random_string(32)  # Create a random string that is not valid data\n        self.assertIsNone(storage._decode(bad_data))\n", "    def test_encode_decode_with_empty_messages(self):\n        storage = self.storage_class(self.request)\n        data = storage._encode([], encode_empty=True)\n        self.assertIsNotNone(data)\n        self.assertNotEqual(data, \"\")\n"], "sample_512": ["def test_set_title():\n    plt.figure()\n    plt.title(\"Test Title\")\n    assert plt.gca().get_title() == \"Test Title\"\n", "def test_axhspan():\n    fig, ax = plt.subplots()\n    ax.axhspan(0.25, 0.75)\n    plt.close(fig)\n", "def test_plot_with_data_parameter():\n    x = np.linspace(0, 10, 100)\n    y = np.sin(x)\n    plt.plot(x, y, data={'x': x, 'y': y})\n    assert plt.gca().lines[0].get_xdata() is plt.gca().lines[0].get_ydata()\n", "def test_sci_with_no_mappable():\n    ax = plt.gca()\n    with pytest.raises(AttributeError):\n        ax.sci(None)\n", "def test_clim():\n    data = np.random.rand(10, 10)\n    plt.imshow(data)\n    plt.clim(0.2, 0.8)\n    clim = plt.gci().get_clim()\n    assert clim[0] == 0.2\n    assert clim[1] == 0.8\n    plt.close()\n"], "sample_1": ["def test_arithmetic_operators():\n    # Test arithmetic operators with same n_inputs and n_outputs\n    model1 = sh1 & sh2\n    model2 = scl1 & scl2\n    assert_allclose(is_separable(model1 + model2), np.array([False, False]))\n    assert_allclose(separability_matrix(model1 + model2), np.array([[True, True], [True, True]]))\n\n    # Test arithmetic operators with different n_inputs or n_outputs\n    model3 = sh1 & sh2 & sh1\n    with pytest.raises(ModelDefinitionError):\n        model1 + model3\n", "def test_custom_model():\n    \"\"\"Test separability of a custom model.\"\"\"\n\n    @custom_model\n        return a * x + b\n\n    custom_model_instance = custom_func()\n    compound_model = custom_model_instance & custom_model_instance\n\n    expected_is_separable = np.array([True, True])\n    expected_separability_matrix = np.array([[True, False], [False, True]])\n\n    assert_allclose(is_separable(compound_model), expected_is_separable)\n    assert_allclose(separability_matrix(compound_model), expected_separability_matrix)\n", "def test_arith_oper():\n    # Test the _arith_oper function with models that have the same n_inputs and n_outputs\n    model1 = models.Scale(2) & models.Shift(1)\n    model2 = models.Scale(3) & models.Shift(2)\n    result = _arith_oper(model1, model2)\n    expected_result = np.ones((2, 2))\n    assert_allclose(result, expected_result)\n\n    # Test the _arith_oper function with models that have different n_inputs\n    model3 = models.Scale(2)\n    with pytest.raises(ModelDefinitionError):\n        _arith_oper(model1, model3)\n\n    # Test the _arith_oper function with models that have different n_outputs\n    model4 = models.Scale(2) & models.Shift(1) & models.Shift(3)\n    with pytest.raises(ModelDefinitionError):\n        _arith_oper(model1, model4)\n", "compilation error", "compilation error"], "sample_513": ["def test_legend_framealpha():\n    fig, ax = plt.subplots()\n    line, = ax.plot([1, 2, 3], label=\"Test line\")\n    legend = ax.legend(framealpha=0.5)\n    assert legend.legendPatch.get_alpha() == 0.5\n", "def test_legend_set_draggable():\n    fig, ax = plt.subplots()\n    line, = ax.plot([1, 2, 3], [1, 2, 3], label=\"Line\")\n    legend = ax.legend()\n\n    assert not legend.get_draggable()\n\n    draggable = legend.set_draggable(state=True, use_blit=False, update='loc')\n    assert legend.get_draggable()\n    assert isinstance(draggable, mlegend.DraggableLegend)\n\n    legend.set_draggable(state=False)\n    assert not legend.get_draggable()\n", "def test_legend_scatteryoffsets():\n    fig, ax = plt.subplots()\n    scatter = ax.scatter([1, 2, 3], [1, 2, 3], label='Scatter')\n    offsets = [0.1, 0.2, 0.3]\n    legend = ax.legend(scatterpoints=3, scatteryoffsets=offsets)\n    assert np.array_equal(legend._scatteryoffsets, np.array(offsets))\n", "def test_legend_alignment():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3], [1, 2, 3], label='Test')\n    legend = ax.legend(alignment='right')\n    assert legend.get_alignment() == 'right'\n    plt.close(fig)\n", "def test_legend_mode_expand():\n    fig, ax = plt.subplots()\n    lines = [ax.plot([1, 2, 3], [1, 2, 3], label='line')[0] for _ in range(10)]\n    fig.legend(lines, loc='upper right', mode='expand')\n"], "sample_725": ["def test_check_array_accept_sparse_deprecation_warning():\n    X = sp.csr_matrix([[1, 0], [0, 1]])\n    with pytest.warns(DeprecationWarning, match=\"Passing 'None' to parameter 'accept_sparse'\"):\n        check_array(X, accept_sparse=None)\n", "def test_check_consistent_length():\n    # Test that check_consistent_length raises a ValueError if inputs have inconsistent lengths\n    X1 = np.array([1, 2, 3])\n    X2 = np.array([1, 2])\n    with pytest.raises(ValueError):\n        check_consistent_length(X1, X2)\n", "def test_check_array_with_complex_input():\n    # Test that check_array raises a ValueError when complex input is provided\n    X_complex = np.array([1 + 1j, 2 + 2j, 3 + 3j])\n    with assert_raises_regex(ValueError, \"Complex data not supported\"):\n        check_array(X_complex)\n", "def test_check_non_negative():\n    X_positive = np.array([[1, 2], [3, 4]])\n    check_non_negative(X_positive, \"test_check_non_negative\")\n\n    X_negative = np.array([[1, -2], [3, 4]])\n    assert_raises(ValueError, check_non_negative, X_negative, \"test_check_non_negative\")\n\n    X_sparse_negative = sp.csr_matrix([[1, -2], [3, 4]])\n    assert_raises(ValueError, check_non_negative, X_sparse_negative, \"test_check_non_negative\")\n", "def test_check_symmetric_exception():\n    X = np.array([[1, 2], [2, 3]])  # not symmetric\n    with assert_raises(ValueError):\n        check_symmetric(X, raise_exception=True)\n"], "sample_181": ["def test_aggregate_with_filter(self):\n    result = Book.objects.aggregate(\n        average_pages=Avg('pages', filter=Q(rating__gt=4.0))\n    )\n    self.assertEqual(result['average_pages'], 567.5)\n", "def test_filtered_aggregate(self):\n    result = Book.objects.aggregate(\n        average_pages=Avg('pages', filter=Q(price__gt=25)),\n        max_rating=Max('rating', filter=Q(publisher__num_awards__gt=2)),\n        count_books=Count('id', filter=Q(authors=self.a1))\n    )\n    self.assertEqual(result['average_pages'], 528)\n    self.assertEqual(result['max_rating'], 4.5)\n    self.assertEqual(result['count_books'], 2)\n", "def test_filtered_aggregate(self):\n    # Test the use of filtered aggregates in an annotation\n    result = Book.objects.annotate(\n        avg_rating_recent=Avg('rating', filter=Q(pubdate__gt=datetime.date(2008, 1, 1)))\n    ).get(isbn='159059725')\n    self.assertEqual(result.avg_rating_recent, 4.5)\n", "def test_aggregate_with_filtered_annotation(self):\n    # Test the use of a filtered annotation in an aggregate function\n    annotated_books = Book.objects.annotate(young_authors=Sum('authors__age', filter=Q(authors__age__lt=50)))\n    avg_young_authors = annotated_books.aggregate(Avg('young_authors'))\n    self.assertEqual(avg_young_authors['young_authors__avg'], 20)\n", "    def test_filtered_aggregate(self):\n        result = Book.objects.aggregate(\n            avg_rating=Avg('rating', filter=Q(contact__age__gt=40)),\n            max_pages=Max('pages', filter=Q(publisher__num_awards__gt=2)),\n            total_pages=Sum('pages', filter=Q(rating__gt=4)),\n            num_books=Count('id', filter=Q(publisher=self.p1)),\n        )\n        self.assertEqual(result['avg_rating'], 4.0)\n        self.assertEqual(result['max_pages'], 600)\n        self.assertEqual(result['total_pages'], 1065)\n        self.assertEqual(result['num_books'], 3)\n"], "sample_936": ["def test_stringify_functions():\n    assert stringify(Integral) == 'numbers.Integral'\n    assert stringify(MyClass1) == 'test_util_typing.MyClass1'\n    assert stringify(MyClass2) == '<MyClass2>'\n    assert stringify(MyList[int]) == 'test_util_typing.MyList[int]'\n    assert stringify(BrokenType) == \"<class 'test_util_typing.BrokenType'>\"\n", "def test_stringify_py37():\n    # Test for Python 3.7+ specific features\n    assert stringify(Dict[str, int]) == 'Dict[str, int]'\n    assert stringify(Union[str, int]) == 'Union[str, int]'\n    assert stringify(Optional[str]) == 'Optional[str]'\n    assert stringify(Callable[[int], str]) == 'Callable[[int], str]'\n    assert stringify(Tuple[int, ...]) == 'Tuple[int, ...]'\n    assert stringify(MyList[int]) == 'MyList[int]'\n    assert stringify(BrokenType) == repr(BrokenType)  # should return repr since __args__ is not a list or tuple\n", "def test_stringify(annotation, expected):\n    assert stringify(annotation) == expected\n", "def test_stringify():\n    # Test with a simple type\n    assert stringify(int) == 'int'\n\n    # Test with a nested type\n    assert stringify(List[int]) == 'List[int]'\n\n    # Test with a custom class\n    assert stringify(MyClass1) == 'MyClass1'\n\n    # Test with a custom class that has __qualname__\n    assert stringify(MyClass2) == '<MyClass2>'\n\n    # Test with a TypeVar\n    assert stringify(T) == 'T'\n\n    # Test with a custom generic class\n    assert stringify(MyList[int]) == 'MyList[int]'\n\n    # Test with a broken type\n    assert stringify(BrokenType) == \"<class 'test_util_typing.BrokenType'>\"\n\n    # Test with None\n    assert stringify(None) == 'None'\n\n    # Test with a Union\n    assert stringify(Union[int, str]) == 'Union[int, str]'\n\n    # Test with an Optional\n    assert stringify(Optional[int]) == 'Optional[int]'\n\n    # Test with a Callable\n    assert stringify(Callable[[int], str]) == 'Callable[[int], str]'\n\n    # Test with a Tuple\n    assert stringify(Tuple[int, str]) == 'Tuple[int, str]'\n\n    # Test with an Integral\n    assert stringify(Integral) == 'Integral'\n\n    # Test with a Generator\n    assert stringify(Generator[int, None, None]) == 'Generator[int, None, None]'\n\n    # Test with a Dict\n    assert stringify(Dict[str, int]) == 'Dict[str, int]'\n\n    # Test with a complex type\n    assert stringify(Dict[str, List[Union[int, float]]]) == 'Dict[str, List[Union[int, float]]]'\n\n    # Test with a complex type with system defined TypeVars\n    assert stringify(Dict[T, T]) == 'Dict'\n\n    # Test with a Generic\n    assert stringify(Generic[T]) == 'Generic", "def test_stringify_forward_ref():\n    class ForwardRefTest:\n        pass\n\n    ForwardRefTestRef = ForwardRef('ForwardRefTest')\n    assert stringify(ForwardRefTestRef) == 'ForwardRefTest'\n"], "sample_617": ["def test_broadcast_compat_data():\n    var = xr.Variable((\"x\", \"y\"), np.arange(6).reshape(2, 3))\n    broadcast_dims = (\"z\",)\n    core_dims = (\"x\", \"y\")\n    result = broadcast_compat_data(var, broadcast_dims, core_dims)\n    expected = np.arange(6).reshape(1, 2, 3)\n    assert_array_equal(result, expected)\n", "def test_unified_dim_sizes():\n    var1 = xr.Variable((\"x\", \"y\"), np.arange(6).reshape(2, 3))\n    var2 = xr.Variable((\"y\", \"z\"), np.arange(6).reshape(3, 2))\n\n    dim_sizes = unified_dim_sizes([var1, var2])\n    assert dim_sizes == {\"x\": 2, \"y\": 3, \"z\": 2}\n", "def test_unified_dim_sizes():\n    da = xr.DataArray(np.random.rand(3, 4), dims=(\"x\", \"y\"))\n    variables = [da.variable]\n    dim_sizes = unified_dim_sizes(variables)\n    assert dim_sizes == {\"x\": 3, \"y\": 4}\n\n    da_broadcast = xr.DataArray(np.random.rand(3), dims=(\"x\"))\n    variables_broadcast = [da.variable, da_broadcast.variable]\n    dim_sizes_broadcast = unified_dim_sizes(variables_broadcast)\n    assert dim_sizes_broadcast == {\"x\": 3, \"y\": 4}\n\n    da_mismatch = xr.DataArray(np.random.rand(5), dims=(\"x\"))\n    variables_mismatch = [da.variable, da_mismatch.variable]\n    with pytest.raises(ValueError, match=\"operands cannot be broadcast together\"):\n        unified_dim_sizes(variables_mismatch)\n\n    da_duplicate = xr.DataArray(np.random.rand(3, 4), dims=(\"x\", \"x\"))\n    variables_duplicate = [da_duplicate.variable]\n    with pytest.raises(ValueError, match=\"broadcasting cannot handle duplicate\"):\n        unified_dim_sizes(variables_duplicate)\n", "def test_apply_ufunc_with_dask_array_and_parallelized():\n    # Test apply_ufunc with dask array input and dask='parallelized'\n    import dask.array as da\n\n    x = da.from_array(np.arange(10).reshape(2, 5), chunks=(1, 2))\n    y = da.from_array(np.arange(5).reshape(5, 1), chunks=(2, 1))\n\n    result = apply_ufunc(np.add, x, y, dask='parallelized')\n\n    assert isinstance(result, da.Array)\n    assert_array_equal(result.compute(), np.add(np.arange(10).reshape(2, 5), np.arange(5).reshape(5, 1)))\n", "def test_apply_ufunc_broadcast_compat_data():\n    # Test case where broadcast_compat_data should raise a ValueError for missing core dimensions\n    variable = xr.Variable((\"x\", \"y\"), np.random.rand(3, 4))\n    broadcast_dims = (\"z\",)\n    core_dims = (\"x\",)\n    with pytest.raises(ValueError):\n        broadcast_compat_data(variable, broadcast_dims, core_dims)\n"], "sample_425": ["    def test_serializer_factory_with_TextEnum(self):\n        value = TextEnum.A\n        serializer = serializer_factory(value)\n        self.assertIsInstance(serializer, EnumSerializer)\n        serialized_value, imports = serializer.serialize()\n        self.assertEqual(serialized_value, \"custom_migration_operations.operations.TextEnum[A]\")\n        self.assertEqual(imports, {\"import custom_migration_operations.operations\"})\n", "    def test_serializer_factory_with_TextEnum(self):\n        value = TextEnum.A\n        serialized = serializer_factory(value)\n        self.assertIsInstance(serialized, EnumSerializer)\n        serialized_string, imports = serialized.serialize()\n        self.assertEqual(serialized_string, \"TestModel1.TextEnum['A']\")\n        self.assertEqual(imports, {\"import TestModel1\"})\n", "    def test_serialize_regex(self):\n        pattern = \"abc.*\"\n        flags = re.MULTILINE | re.IGNORECASE\n        regex = re.compile(pattern, flags)\n        serializer = serializer_factory(regex)\n        string, imports = serializer.serialize()\n        self.assertEqual(string, \"re.compile('abc.*', re.MULTILINE | re.IGNORECASE)\")\n        self.assertEqual(imports, {\"import re\"})\n", "    def test_functools_partial_serializer(self):\n            return a + b + c + d\n\n        partial_func = functools.partial(my_func, 1, d=4)\n        serializer = serializer_factory(partial_func)\n        result, imports = serializer.serialize()\n\n        self.assertEqual(result, \"functools.partial(my_func, 1, d=4)\")\n        self.assertEqual(imports, {\"import functools\"})\n", "    def test_uuid_serialization(self):\n        uuid_obj = uuid.uuid4()\n        serializer = serializer_factory(uuid_obj)\n        serialized_string, imports = serializer.serialize()\n        self.assertEqual(serialized_string, f\"uuid.{repr(uuid_obj)}\")\n        self.assertEqual(imports, {\"import uuid\"})\n"], "sample_655": ["    def test_is_globally_capturing(self):\n        cm = CaptureManager(\"fd\")\n        assert cm.is_globally_capturing()\n        cm = CaptureManager(\"no\")\n        assert not cm.is_globally_capturing()\n", "def test_deactivate_fixture(self, request):\n    item = request.node\n    item._capture_fixture = capture.CaptureFixture(capture.SysCapture, request)\n    capmanager = request.config.pluginmanager.getplugin(\"capturemanager\")\n    capmanager.activate_fixture(item)\n    assert sys.stdout.getvalue() == \"\"\n    print(\"Test output\")\n    assert sys.stdout.getvalue() != \"\"\n    capmanager.deactivate_fixture(item)\n    sys.stdout.write(\"More test output\")\n    assert sys.stdout.getvalue() == \"Test output\"\n", "def test_global_capture_disabled_when_no_method_specified(capsys):\n    manager = CaptureManager(\"no\")\n    assert not manager.is_globally_capturing()\n\n    manager.start_global_capturing()\n    assert not manager.is_capturing()\n\n    manager.stop_global_capturing()\n", "def test_global_and_fixture_disabled(pytester):\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture\n            # Dummy fixture to simulate the use of a capture fixture\n            pass\n\n            with capmanager.global_and_fixture_disabled():\n                print(\"This should not be captured\")\n            out, err = capsys.readouterr()\n            assert out == \"\", \"Output was captured when it should have been disabled\"\n            assert err == \"\", \"Error output was captured when it should have been disabled\"\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)\n", "def capture_manager(request):\n    method = \"fd\" if hasattr(os, \"dup\") else \"sys\"\n    manager = CaptureManager(method)\n    manager.start_global_capturing()\n    yield manager\n    manager.stop_global_capturing()\n"], "sample_400": ["def test_generate_removed_indexes(self):\n    before_states = [self.book_indexes]\n    after_states = [self.book_unordered_indexes]\n    changes = self.get_changes(before_states, after_states)\n    self.assertOperationTypes(changes, \"otherapp\", 0, [\"RemoveIndex\", \"AddIndex\"])\n    self.assertOperationAttributes(\n        changes, \"otherapp\", 0, 0, name=\"book_title_author_idx\"\n    )\n    self.assertOperationAttributes(\n        changes, \"otherapp\", 0, 1, name=\"book_author_title_idx\"\n    )\n", "def test_generate_added_indexes(self):\n    before_states = [self.author_indexes]\n    after_states = [self.book_indexes]\n    changes = self.get_changes(before_states, after_states)\n    self.assertOperationTypes(changes, \"otherapp\", 0, [\"AddIndex\"])\n    self.assertOperationAttributes(\n        changes,\n        \"otherapp\",\n        0,\n        0,\n        model_name=\"book\",\n        index=models.Index(fields=[\"author\", \"title\"], name=\"book_title_author_idx\"),\n    )\n", "def test_detect_changes_added_m2m_through(self):\n    before_states = [self.author_with_m2m]\n    after_states = [self.author_with_m2m_through]\n    changes = self.get_changes(before_states, after_states)\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"RemoveField\", \"AddField\"])\n", "def test_create_model_with_index_and_constraint(self):\n    before_states = [\n        self.author_empty,\n    ]\n    after_states = [\n        ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n            ],\n            {\n                \"indexes\": [\n                    models.Index(fields=[\"name\"], name=\"author_name_idx\"),\n                ],\n                \"constraints\": [\n                    models.CheckConstraint(\n                        check=models.Q(name__contains=\"A\"), name=\"name_contains_a\"\n                    ),\n                ],\n            },\n        ),\n    ]\n    changes = self.get_changes(before_states, after_states)\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"CreateModel\", \"AddIndex\", \"AddConstraint\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"Author\")\n    self.assertOperationAttributes(changes, \"testapp\", 0, 1, name=\"Author\", index=models.Index(fields=[\"name\"], name=\"author_name_idx\"))\n    self.assertOperationAttributes(changes, \"testapp\", 0, 2, name=\"Author\", constraint=models.CheckConstraint(check=models.Q(name__contains=\"A\"), name=\"name_contains_a\"))\n", "    def test_swappable_m2m_through(self):\n        questioner = MigrationQuestioner(specified_apps=set([\"testapp\"]))\n        changes = self.get_changes([self.author_empty], [self.author_with_m2m_through], questioner)\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"CreateModel\", \"CreateModel\", \"AddField\"])\n        self.assertOperationAttributes(changes, \"testapp\", 0, 1, name=\"publishers\", field__to=\"__setting__\", field__through=\"testapp.Contract\")\n"], "sample_816": ["def test_tfidf_vectorizer_inverse_transform():\n    corpus = [\n        'This is the first document.',\n        'This document is the second document.',\n        'And this is the third one.',\n        'Is this the first document?',\n    ]\n    vectorizer = TfidfVectorizer()\n    X = vectorizer.fit_transform(corpus)\n    X_inv = vectorizer.inverse_transform(X)\n    assert len(X_inv) == len(corpus)\n    for doc, terms in zip(corpus, X_inv):\n        for term in terms:\n            assert term in doc.lower()\n", "def test_hashingvectorizer_binary():\n    hv = HashingVectorizer(binary=True)\n    X = hv.fit_transform(JUNK_FOOD_DOCS)\n    assert_array_equal(X.data, np.ones(X.data.shape))\n", "def test_hashing_vectorizer_transform_input_validation():\n    vectorizer = HashingVectorizer()\n    with assert_raises(ValueError):\n        vectorizer.transform(\"This is a single document\")\n", "def test_countvectorizer_custom_analyzer_input_file():\n    vectorizer = CountVectorizer(analyzer=lazy_analyze)\n    # simulate a file\n    X = [StringIO(doc) for doc in JUNK_FOOD_DOCS]\n\n    with ignore_warnings(category=ChangedBehaviorWarning):\n        vectorizer.fit(X)\n        assert vectorizer.vocabulary_ == {'the_ultimate_feature': 0}\n        X_transformed = vectorizer.transform(X)\n        assert_array_equal(X_transformed.toarray(), np.ones((6, 1)))\n", "def test_hashingvectorizer_input_consistency():\n    hv = HashingVectorizer(input='content')\n    hv.fit(ALL_FOOD_DOCS)\n    X = hv.transform(ALL_FOOD_DOCS)\n    assert_equal(X.shape[0], len(ALL_FOOD_DOCS))\n\n    # Test with different input types\n    hv.input = 'filename'\n    assert_raises(ValueError, hv.fit, ALL_FOOD_DOCS)\n    hv.input = 'file'\n    assert_raises(ValueError, hv.fit, ALL_FOOD_DOCS)\n\n    # Test with correct input type\n    hv.input = 'content'\n    hv.fit(ALL_FOOD_DOCS)\n    X = hv.transform(ALL_FOOD_DOCS)\n    assert_equal(X.shape[0], len(ALL_FOOD_DOCS))\n"], "sample_111": ["def test_get_queryset_with_search(self):\n    request = self._mocked_authenticated_request('/admin/testapp/band/', self.superuser)\n    request.GET = {SEARCH_VAR: 'Queen'}\n\n    changelist = BandAdmin(Band, admin.site).get_changelist_instance(request)\n    queryset = changelist.get_queryset(request)\n\n    self.assertIn(Exact, queryset.query.where.children[0].children)\n    self.assertEqual(queryset.query.where.children[0].children[1], 'Queen')\n", "def test_get_queryset_search(self):\n    request = self._mocked_authenticated_request('/admin/admin_changelist/band/', self.superuser)\n    request.GET = {SEARCH_VAR: 'Test Band'}\n    changelist = BandAdmin(Band, custom_site).get_changelist_instance(request)\n    queryset = changelist.get_queryset(request)\n    self.assertEqual(queryset.count(), Band.objects.filter(name__icontains='Test Band').count())\n", "def test_get_queryset_with_search_query(self):\n    request = self._mocked_authenticated_request('/admin/testapp/band/', self.superuser)\n    request.GET = {SEARCH_VAR: 'Test'}\n    model_admin = BandAdmin(model=Band, admin_site=admin.site)\n    changelist = model_admin.get_changelist_instance(request)\n    queryset = changelist.get_queryset(request)\n    self.assertEqual(queryset.count(), Band.objects.filter(name__icontains='Test').count())\n", "def test_get_ordering(self):\n    # Test the get_ordering() method of ChangeList class\n    request = self.factory.get('/admin/admin_changelist/parent/')\n    request.user = self.superuser\n\n    # Mock the model admin and its get_ordering() method\n    model_admin = ParentAdmin(Parent, admin.site)\n    model_admin.get_ordering = lambda request: ['-name']\n\n    # Create a ChangeList object\n    cl = admin.ChangeList(\n        request,\n        Parent,\n        list_display=['name'],\n        list_display_links=['name'],\n        list_filter=[],\n        date_hierarchy=None,\n        search_fields=['name'],\n        list_select_related=False,\n        list_per_page=100,\n        list_max_show_all=200,\n        list_editable=[],\n        model_admin=model_admin,\n        sortable_by=['name'],\n    )\n\n    # Test the get_ordering() method\n    queryset = Parent.objects.all()\n    ordering = cl.get_ordering(request, queryset)\n    self.assertEqual(ordering, ['-name'])\n", "def test_get_queryset_filters(self):\n    request = self._mocked_authenticated_request('/admin/custom_app/band/', self.superuser)\n    admin_instance = BandAdmin(Band, custom_site)\n    cl = ChangeList(request, Band, [], [], [Group], None, ['name'], [], 200, 0, [], admin_instance, [])\n    filter_specs, has_filters, lookup_params, use_distinct = cl.get_filters(request)\n    self.assertEqual(len(filter_specs), 1)\n    self.assertTrue(has_filters)\n    self.assertEqual(lookup_params, {})\n    self.assertFalse(use_distinct)\n"], "sample_952": ["def test_stringify_signature():\n    sig = inspect.signature(datetime.datetime.now)\n    assert stringify_signature(sig) == '()'\n\n    sig = inspect.signature(datetime.datetime.strptime)\n    assert stringify_signature(sig) == '(date_string: str, format: str) -> datetime.datetime'\n\n    sig = inspect.signature(datetime.datetime.strptime, show_annotation=False)\n    assert stringify_signature(sig) == '(date_string, format)'\n\n    sig = inspect.signature(datetime.datetime.strptime, show_return_annotation=False)\n    assert stringify_signature(sig) == '(date_string: str, format: str)'\n", "def test_signature_from_str():\n    sig = inspect.signature_from_str(\"(a: int, b: str = 'default')\")\n    assert list(sig.parameters.keys()) == ['a', 'b']\n    assert sig.parameters['a'].annotation == int\n    assert sig.parameters['b'].annotation == str\n    assert sig.parameters['b'].default == 'default'\n", "def test_getdoc_with_inherited_decorated_method():\n    class Base:\n        @classmethod\n            \"\"\"Docstring for decorated method\"\"\"\n            pass\n\n    class Derived(Base):\n        pass\n\n    doc = inspect.getdoc(Derived.decorated, allow_inherited=True, cls=Derived, name='decorated')\n    assert doc == \"Docstring for decorated method\"\n", "def test_getargspec():\n        pass\n\n    spec = inspect.getargspec(func)\n    assert spec.args == ['a', 'b']\n    assert spec.varargs == 'args'\n    assert spec.varkw == 'kwargs'\n    assert spec.defaults == (1,)\n    assert spec.kwonlyargs == ['c']\n    assert spec.kwonlydefaults == {'c': 2}\n    assert 'a' in spec.annotations\n    assert 'b' in spec.annotations\n    assert 'c' in spec.annotations\n    assert 'return' in spec.annotations\n", "def test_evaluate_signature():\n        pass\n\n    sig = inspect.signature(func)\n    globalns = {'datetime': datetime, 'List': list}\n    evaluated_sig = inspect.evaluate_signature(sig, globalns)\n\n    assert evaluated_sig.parameters['a'].annotation == int\n    assert evaluated_sig.parameters['b'].annotation == str\n    assert evaluated_sig.parameters['c'].annotation == datetime.date\n    assert evaluated_sig.return_annotation == list\n"], "sample_788": ["def test_encode(encode, expected):\n    est = KBinsDiscretizer(n_bins=3, encode=encode, strategy='uniform')\n    est.fit(X)\n    Xt = est.transform(X)\n    assert_array_almost_equal(Xt.toarray() if sp.issparse(Xt) else Xt, expected)\n", "def test_transform_with_dense_output(strategy, expected):\n    est = KBinsDiscretizer(n_bins=3, encode='onehot-dense', strategy=strategy)\n    est.fit(X)\n    Xt = est.transform(X)\n    assert isinstance(Xt, np.ndarray)\n    assert_array_equal(Xt, np.array(expected))\n", "def test_n_bins_array_like(strategy, expected):\n    n_bins = [3, 3, 3, 3]\n    est = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy=strategy)\n    est.fit(X)\n    Xt = est.transform(X)\n    assert_array_equal(Xt, expected)\n", "def test_invalid_n_bins(strategy):\n    invalid_n_bins = [1, 0, -1, [2, 3], np.array([2, 3])]\n    for n_bins in invalid_n_bins:\n        with pytest.raises(ValueError):\n            KBinsDiscretizer(n_bins=n_bins, strategy=strategy).fit(X)\n", "def test_discretizer_fit_transform_onehot_dense(strategy, expected):\n    n_bins = 3\n    encode = 'onehot-dense'\n    est = KBinsDiscretizer(n_bins=n_bins, encode=encode, strategy=strategy)\n    est.fit(X)\n    Xt = est.transform(X)\n    enc = OneHotEncoder(categories=[np.arange(n_bins) for _ in range(4)], sparse=False)\n    enc.fit(np.zeros((1, 4), dtype=int))\n    expected_transformed = enc.transform(expected)\n    assert_array_equal(Xt.toarray(), expected_transformed)\n"], "sample_1081": ["def test_core():\n    assert core(24, 2) == 6\n    assert core(9424, 3) == 1178\n    assert core(379238) == 379238\n    assert core(15**11, 10) == 15\n", "def test_smoothness():\n    assert smoothness(2**7*3**2) == (3, 128)\n    assert smoothness(2**4*13) == (13, 16)\n    assert smoothness(2) == (2, 2)\n    assert smoothness(1) == (1, 1)\n", "def test_mersenne_prime_exponent():\n    assert mersenne_prime_exponent(1) == 2\n    assert mersenne_prime_exponent(20) == 4423\n    assert mersenne_prime_exponent(51) == 82589933\n    with raises(ValueError):\n        mersenne_prime_exponent(0)\n        mersenne_prime_exponent(52)\n", "def test_mersenne_prime_exponent():\n    assert mersenne_prime_exponent(1) == 2\n    assert mersenne_prime_exponent(2) == 3\n    assert mersenne_prime_exponent(3) == 5\n    assert mersenne_prime_exponent(4) == 7\n    assert mersenne_prime_exponent(5) == 13\n    assert mersenne_prime_exponent(51) == 216091\n\n    with raises(ValueError):\n        mersenne_prime_exponent(0)\n        mersenne_prime_exponent(-1)\n        mersenne_prime_exponent(52)\n", "def test_core():\n    assert core(24, 2) == 6\n    assert core(9424, 3) == 1178\n    assert core(379238) == 379238\n    assert core(15**11, 10) == 15\n\n    # Test with invalid inputs\n    raises(ValueError, lambda: core(-3958, 27))\n    raises(ValueError, lambda: core(379238, -3))\n    raises(ValueError, lambda: core(15**11, 1))\n"], "sample_773": ["def test_logistic_regression_cv_fit_predict():\n    X, y = make_classification(n_samples=100, n_features=20, n_classes=2, random_state=42)\n    clf = LogisticRegressionCV(cv=5, random_state=0)\n    clf.fit(X, y)\n    y_pred = clf.predict(X)\n    assert y_pred.shape == (100,)\n    assert np.unique(y_pred).size == 2\n", "def test_logistic_regression_cv_refit_false():\n    X, y = make_classification(n_samples=100, n_features=20, n_classes=3, random_state=42)\n    clf = LogisticRegressionCV(cv=5, refit=False, random_state=42)\n    clf.fit(X, y)\n    assert_array_equal(clf.C_.shape, (3,))\n    assert_array_equal(clf.coef_.shape, (3, 20))\n    assert_array_equal(clf.intercept_.shape, (3,))\n    assert_array_equal(clf.scores_[0].shape, (5, 10))\n    assert_array_equal(clf.coefs_paths_[0].shape, (5, 10, 20))\n    assert_array_equal(clf.n_iter_.shape, (3, 5, 10))\n", "def test_logistic_regression_multinomial():\n    X, y = make_classification(n_samples=100, n_features=20, n_informative=2, n_redundant=10, n_classes=3, random_state=42)\n    clf = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000, random_state=42)\n    check_predictions(clf, X, y)\n", "def test_logistic_regression_predict_proba():\n    clf = LogisticRegression(solver='lbfgs', multi_class='multinomial')\n    X, y = make_classification(n_samples=100, n_features=20, n_informative=2,\n                               n_redundant=10, n_classes=3, random_state=42)\n    clf.fit(X, y)\n    proba = clf.predict_proba(X)\n    assert_array_almost_equal(proba.sum(axis=1), np.ones(len(y)))\n    assert_equal(proba.shape, (len(y), 3))\n", "def test_logistic_regression_path():\n    X = np.array([[0, 0], [1, 1], [2, 2], [3, 3], [4, 4]])\n    y = np.array([0, 0, 1, 1, 1])\n    Cs = [1.0, 0.1]\n    coefs, Cs_out, n_iter = logistic_regression_path(X, y, Cs=Cs)\n    assert_equal(coefs.shape, (2, 2))\n    assert_array_equal(Cs_out, Cs)\n    assert_array_equal(n_iter, np.array([4, 100]))\n"], "sample_823": ["def test_pairwise_kernels_with_chi2_kernel():\n    X = np.array([[1, 2], [3, 4]])\n    Y = np.array([[1, 2], [3, 4]])\n    gamma = 0.5\n\n    expected_result = np.exp(-gamma * additive_chi2_kernel(X, Y))\n    result = pairwise_kernels(X, Y, metric=\"chi2\", gamma=gamma)\n\n    assert_array_almost_equal(result, expected_result)\n", "def test_pairwise_distances_chunked_with_reduce_func():\n    X = np.random.RandomState(0).rand(5, 3)\n    r = .2\n\n        neigh = [np.flatnonzero(d < r) for d in D_chunk]\n        avg_dist = (D_chunk * (D_chunk < r)).mean(axis=1)\n        return neigh, avg_dist\n\n    gen = pairwise_distances_chunked(X, reduce_func=reduce_func)\n    neigh, avg_dist = next(gen)\n    assert isinstance(neigh, list)\n    assert isinstance(avg_dist, np.ndarray)\n", "def test_pairwise_distances_argmin_min():\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    Y = np.array([[1, 1], [3, 4], [7, 8]])\n\n    # Test pairwise_distances_argmin_min function\n    indices, values = pairwise_distances_argmin_min(X, Y)\n    assert_array_equal(indices, [0, 1, 1])\n    assert_array_almost_equal(values, [1.41421356, 0.0, 2.82842712])\n\n    # Test pairwise_distances_argmin function\n    indices = pairwise_distances_argmin(X, Y)\n    assert_array_equal(indices, [0, 1, 1])\n", "def test_pairwise_distances_chunked_reduce_func_output_types():\n    X = np.random.RandomState(0).rand(5, 3)\n\n        return D_chunk[0]  # single object\n\n    gen = pairwise_distances_chunked(X, reduce_func=reduce_func)\n    assert_raises_regexp(TypeError, \"expected sequence\", next, gen)\n\n        return D_chunk[0], D_chunk[0]  # tuple of objects\n\n    gen = pairwise_distances_chunked(X, reduce_func=reduce_func)\n    assert_raises_regexp(TypeError, \"expected sequence\", next, gen)\n\n        return [D_chunk[0]]  # list of objects\n\n    gen = pairwise_distances_chunked(X, reduce_func=reduce_func)\n    next(gen)  # should not raise\n\n        return ([D_chunk[0]], [D_chunk[0]])  # tuple of lists\n\n    gen = pairwise_distances_chunked(X, reduce_func=reduce_func)\n    next(gen)  # should not raise\n\n        return [D_chunk[0]] * 2  # list of objects of different length\n\n    gen = pairwise_distances_chunked(X, reduce_func=reduce_func)\n    assert_raises_regexp(ValueError, \"Expected same length as input\", next, gen)\n\n        return ([D_chunk[0]] * 2, [D_chunk[0]])  # tuple of lists of different length\n\n    gen = pairwise_distances_chunked(X, reduce_func=reduce_func)\n    assert_raises_regexp(ValueError, \"Expected same length as input\", next, gen)\n", "def test_haversine_distances():\n    # Test haversine_distances function\n    X = np.array([[45, 90], [0, 0], [30, 60]])\n    Y = np.array([[0, 0], [30, 60], [45, 90]])\n    expected_distances = np.array([[15707.96326795, 5745.58340543, 0.0],\n                                   [5745.58340543, 0.0, 15707.96326795],\n                                   [0.0, 15707.96326795, 15707.96326795]])\n    distances = haversine_distances(X, Y)\n    assert_array_almost_equal(distances, expected_distances)\n"], "sample_202": ["def test_cookie_samesite_lax(self):\n    storage = self.get_storage()\n    message = Message(constants.INFO, 'Test message')\n    storage._store([message], self.response)\n    self.assertEqual(self.response.cookies[storage.cookie_name]['samesite'], 'Lax')\n", "    def test_store_large_messages(self):\n        \"\"\"\n        Test that messages larger than max_cookie_size are handled correctly.\n        \"\"\"\n        storage = self.storage_class(self.request)\n        # Create a message larger than max_cookie_size\n        large_message = 'a' * (storage.max_cookie_size + 1)\n        messages = [Message(constants.INFO, large_message)]\n        response = self.get_response()\n        unstored_messages = storage._store(messages, response)\n        self.assertEqual(len(unstored_messages), 1)\n        self.assertEqual(unstored_messages[0].message, large_message)\n        self.assertEqual(self.stored_messages_count(storage, response), 0)\n", "def test_cookie_encoding_decoding(self):\n    storage = self.storage_class(self.request)\n    message = Message(constants.INFO, \"Test message\")\n    messages = [message]\n    set_cookie_data(storage, messages)\n    self.assertEqual(storage._get()[0], messages)\n\n    # Test with SafeData\n    safe_message = Message(constants.INFO, mark_safe(\"Safe data test message\"))\n    safe_messages = [safe_message]\n    set_cookie_data(storage, safe_messages)\n    self.assertEqual(storage._get()[0], safe_messages)\n\n    # Test with invalid data\n    set_cookie_data(storage, messages, invalid=True)\n    self.assertIsNone(storage._get()[0])\n\n    # Test with empty data\n    set_cookie_data(storage, [], encode_empty=True)\n    self.assertEqual(storage._get()[0], [])\n", "    def test_default_message(self):\n        message = Message(constants.INFO, \"Test message\")\n        encoder = MessageEncoder()\n        encoded_message = encoder.default(message)\n        self.assertEqual(encoded_message, [MessageEncoder.message_key, 0, constants.INFO, \"Test message\"])\n", "    def test_safe_data_encoding_decoding(self):\n        # Create a safe message\n        safe_message = Message(constants.INFO, mark_safe(\"<script>alert('Hello');</script>\"))\n\n        # Store the safe message\n        response = self.request_factory.get('/')\n        storage = self.storage_class(self.request_factory.get('/'))\n        storage._store([safe_message], response)\n\n        # Check that the message was stored as safe data\n        encoded_data = response.cookies[CookieStorage.cookie_name].value\n        decoded_data = storage._decode(encoded_data)\n        self.assertIsInstance(decoded_data[0].message, SafeData)\n        self.assertEqual(decoded_data[0].message, safe_message.message)\n"], "sample_815": ["def test_brier_score_loss():\n    y_true = np.array([0, 1, 1, 0])\n    y_prob = np.array([0.1, 0.9, 0.8, 0.3])\n    expected_score = 0.037\n    assert_almost_equal(brier_score_loss(y_true, y_prob), expected_score)\n", "def test_brier_score_loss():\n    y_true = np.array([0, 1, 1, 0])\n    y_prob = np.array([0.1, 0.9, 0.8, 0.3])\n    expected_score = 0.0375\n    assert_almost_equal(brier_score_loss(y_true, y_prob), expected_score)\n\n    y_true_categorical = np.array([\"spam\", \"ham\", \"ham\", \"spam\"])\n    assert_almost_equal(brier_score_loss(y_true_categorical, y_prob, pos_label=\"ham\"), expected_score)\n\n    with assert_raises(ValueError):\n        brier_score_loss(y_true, np.array([0.1, 0.9, 0.8, 1.1]))\n\n    with assert_raises(ValueError):\n        brier_score_loss(y_true, np.array([0.1, 0.9, 0.8, -0.1]))\n\n    with assert_raises(ValueError):\n        brier_score_loss(np.array([0, 1, 2, 0]), y_prob)\n", "def test_hinge_loss():\n    y_true = np.array([-1, 1, 1, -1])\n    pred_decision = np.array([0.1, 0.8, -0.2, 0.5])\n    expected_loss = 0.5\n    assert_almost_equal(hinge_loss(y_true, pred_decision), expected_loss)\n", "def test_cohen_kappa_score_with_weights():\n    y_true = [1, 2, 3, 4, 5]\n    y_pred = [1, 2, 3, 4, 5]\n    assert_almost_equal(cohen_kappa_score(y_true, y_pred, weights='linear'), 1.0)\n    assert_almost_equal(cohen_kappa_score(y_true, y_pred, weights='quadratic'), 1.0)\n\n    y_true = [1, 2, 3, 4, 5]\n    y_pred = [2, 3, 4, 5, 1]\n    assert_almost_equal(cohen_kappa_score(y_true, y_pred, weights='linear'), 0.6, decimal=1)\n    assert_almost_equal(cohen_kappa_score(y_true, y_pred, weights='quadratic'), 0.286, decimal=3)\n", "def test_zero_one_loss():\n    y_true = [0, 1, 2, 3]\n    y_pred = [1, 2, 2, 3]\n    assert_almost_equal(zero_one_loss(y_true, y_pred), 0.25)\n    assert_almost_equal(zero_one_loss(y_true, y_pred, normalize=False), 1)\n\n    y_true = [0, 1, 0, 1]\n    y_pred = [0, 1, 1, 1]\n    assert_almost_equal(zero_one_loss(y_true, y_pred), 0.25)\n    assert_almost_equal(zero_one_loss(y_true, y_pred, normalize=False), 1)\n\n    y_true = [[0, 1], [1, 1]]\n    y_pred = [[0, 1], [1, 0]]\n    assert_almost_equal(zero_one_loss(y_true, y_pred), 0.5)\n    assert_almost_equal(zero_one_loss(y_true, y_pred, normalize=False), 1)\n"], "sample_65": ["    def test_get_formats(self):\n        \"\"\"Test that get_formats returns the correct dictionary.\"\"\"\n        formats = get_formats()\n        self.assertIsInstance(formats, dict)\n        self.assertIn('DATE_FORMAT', formats)\n        self.assertIn('TIME_FORMAT', formats)\n        # add more assertions for other format settings\n", "    def test_get_paths(self):\n        \"\"\"Test the get_paths method of JavaScriptCatalog.\"\"\"\n        view = JavaScriptCatalog()\n        paths = view.get_paths(['django.conf'])\n        self.assertEqual(paths, [path.join(settings.BASE_DIR, 'django/conf/locale')])\n        with self.assertRaises(ValueError):\n            view.get_paths(['nonexistent_package'])\n", "    def setUp(self):\n        self.factory = RequestFactory()\n", "    def test_get_formats(self):\n        \"\"\"Test the get_formats function.\"\"\"\n        formats = get_formats()\n        self.assertIsInstance(formats, dict)\n        self.assertIn('DATE_FORMAT', formats)\n        # Add more assertions based on the expected formats\n", "    def test_get_catalog_with_fallback(self):\n        \"\"\"Test that the get_catalog method returns a catalog with fallback.\"\"\"\n        with self.settings(LANGUAGE_CODE='fr'):\n            # Create a translation in the 'fr' locale and a fallback in 'en'\n            fr_translation = gettext.translation('djangojs', locale_dir, languages=['fr'])\n            en_translation = gettext.translation('djangojs', locale_dir, languages=['en'])\n\n            # Mock the translation object returned by DjangoTranslation\n            view = JavaScriptCatalog()\n            view.translation = fr_translation\n            view.translation._fallback = en_translation\n\n            catalog = view.get_catalog()\n\n            # Check that the catalog contains the 'fr' translation\n            self.assertIn('Hello', catalog)\n            self.assertEqual(catalog['Hello'], 'Bonjour')\n\n            # Check that the catalog contains the 'en' fallback translation\n            self.assertIn('World', catalog)\n            self.assertEqual(catalog['World'], 'World')\n"], "sample_806": ["def test_classification_toy_presort(presort, loss):\n    check_classification_toy(presort, loss)\n", "def test_gradient_boosting_regressor_n_iter_no_change(presort):\n    # Check that early stopping with n_iter_no_change works for regressor\n    X, y = make_regression(n_samples=200, random_state=0)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\n    reg = GradientBoostingRegressor(n_estimators=100, random_state=0, n_iter_no_change=10, presort=presort)\n    reg.fit(X_train, y_train)\n\n    assert reg.n_estimators_ < 100\n    assert mean_squared_error(y_test, reg.predict(X_test)) < mean_squared_error(y_test, reg.init_.predict(X_test))\n", "def test_gradient_boosting_regressor_loss_quantile(presort):\n    # Test GradientBoostingRegressor with quantile loss\n    X, y = make_regression(n_samples=100, n_features=1, random_state=0)\n    reg = GradientBoostingRegressor(loss='quantile', n_estimators=100,\n                                    random_state=0, presort=presort)\n    reg.fit(X, y)\n    y_pred = reg.predict(X)\n    assert_equal(y_pred.shape, y.shape)\n    assert_greater(mean_squared_error(y, y_pred), 0.0)\n", "def test_gradient_boosting_with_custom_loss(presort, loss):\n    # Custom loss function for testing\n    class CustomLoss:\n            return mean_squared_error(y, pred, sample_weight=sample_weight)\n\n            return 2 * (pred - y)\n\n    # Check gradient boosting with custom loss function on a toy dataset.\n    reg = GradientBoostingRegressor(loss=CustomLoss(), n_estimators=10, random_state=1, presort=presort)\n    reg.fit(X, y)\n    assert_array_almost_equal(reg.predict(T), true_result)\n    assert_equal(10, len(reg.estimators_))\n\n    deviance_decrease = (reg.train_score_[:-1] - reg.train_score_[1:])\n    assert np.any(deviance_decrease >= 0.0)\n", "def test_classification_toy_with_sample_weight(presort, loss):\n    # Check classification on a toy dataset with sample weights.\n    sample_weight = np.ones(len(y))\n    sample_weight[0] = 2.0\n\n    clf = GradientBoostingClassifier(loss=loss, n_estimators=10,\n                                     random_state=1, presort=presort)\n\n    clf.fit(X, y, sample_weight=sample_weight)\n    assert_array_equal(clf.predict(T), true_result)\n    assert_equal(10, len(clf.estimators_))\n\n    deviance_decrease = (clf.train_score_[:-1] - clf.train_score_[1:])\n    assert np.any(deviance_decrease >= 0.0)\n\n    leaves = clf.apply(X)\n    assert_equal(leaves.shape, (6, 10, 1))\n"], "sample_547": ["def test_draggable_annotation():\n    fig, ax = plt.subplots()\n    ann = AnnotationBbox(TextArea(\"Test\"), (0.5, 0.5))\n    ax.add_artist(ann)\n\n    draggable_ann = DraggableAnnotation(ann)\n    event = MouseEvent('motion_notify_event', fig.canvas, 100, 100, button=MouseButton.LEFT)\n    draggable_ann.on_pick(namedtuple('Event', 'artist mouseevent')(ann, event))\n    draggable_ann.on_motion(event)\n    draggable_ann.on_release(event)\n\n    assert ann.xyann != (0.5, 0.5)\n", "def test_anchored_offsetbox():\n    fig, ax = plt.subplots()\n    box = mpatches.Circle((0, 0), 0.25)\n    ab = AnchoredOffsetbox(loc=2, child=box, pad=0.1, frameon=True,\n                           bbox_to_anchor=(1.05, 0.5),\n                           bbox_transform=ax.transAxes,\n                           borderpad=0.1)\n    ax.add_artist(ab)\n    assert_allclose(ab.get_bbox_to_anchor().bounds, (1.05, 0.5, 0, 0))\n    plt.close(fig)\n", "def test_clipping_in_drawing_area():\n    fig, ax = plt.subplots()\n    da = DrawingArea(width=10, height=10, clip=True)\n    line = mlines.Line2D([0, 10], [0, 10])\n    da.add_artist(line)\n    ax.add_artist(da)\n    fig.savefig(io.BytesIO())\n", "def test_offsetbox_get_bbox_and_child_offsets():\n    class DummyOffsetBox(OffsetBox):\n            return Bbox.from_bounds(0, 0, 10, 20), [(5, 10)]\n\n    renderer = MockRenderer()\n    ob = DummyOffsetBox()\n    bbox, offsets = ob._get_bbox_and_child_offsets(renderer)\n    assert bbox.bounds == (0, 0, 10, 20)\n    assert offsets == [(5, 10)]\n", "def test_offsetbox_draggable():\n    fig, ax = plt.subplots()\n    box = OffsetBox()\n    text = TextArea(\"Draggable Text\")\n    box.set_child(text)\n    draggable = DraggableOffsetBox(box, box)\n    ax.add_artist(box)\n    plt.close(fig)\n"], "sample_275": ["    def setUpTestData(cls):\n        # Create some sample data for testing\n        Book.objects.create(title='Book 1')\n        Book.objects.create(title='Book 2')\n        Book.objects.create(title='Book 3')\n", "    def test__merge_sanity_check(self):\n        qs1 = Book.objects.values('id', 'title')\n        qs2 = Book.objects.values('id', 'author')\n        with self.assertRaises(TypeError):\n            qs1._merge_sanity_check(qs2)\n", "    def test_populate(self):\n        person = Person.objects.create(name=\"John Doe\")\n        location = Location.objects.create(name=\"Home\", person=person)\n\n        klass_info = {\n            'model': Location,\n            'select_fields': [0, 1, 2],\n            'from_parent': False,\n            'local_setter': lambda obj, rel_obj: setattr(obj, 'location', rel_obj),\n            'remote_setter': lambda obj, from_obj: setattr(obj, 'person', from_obj),\n        }\n        select = [\n            (Location._meta.get_field('id'), None, None),\n            (Location._meta.get_field('name'), None, None),\n            (Person._meta.get_field('id'), 'person', None),\n        ]\n        db = 'default'\n\n        rel_populator = RelatedPopulator(klass_info, select, db)\n        row = [location.id, location.name, person.id]\n        obj = Person.objects.create(name=\"Jane Doe\")\n\n        rel_populator.populate(row, obj)\n\n        self.assertEqual(obj.location, location)\n        self.assertEqual(location.person, obj)\n", "def test_delete_with_select_related(self):\n    # Create a Book object with related Award objects\n    book = Book.objects.create(name='Test Book')\n    award1 = Award.objects.create(book=book, name='Award 1')\n    award2 = Award.objects.create(book=book, name='Award 2')\n\n    # Use select_related to fetch related Awards\n    book_with_awards = Book.objects.select_related('award_set').get(id=book.id)\n\n    # Start a new transaction on the second connection\n    with transaction.atomic(using='default'):\n        # Delete the Book object on the second connection\n        Book.objects.using(self.conn2).filter(id=book.id).delete()\n\n    # Check that the Book object and its related Awards have been deleted\n    with self.assertRaises(Book.DoesNotExist):\n        Book.objects.get(id=book.id)\n    with self.assertRaises(Award.DoesNotExist):\n        Award.objects.get(id=award1.id)\n    with self.assertRaises(Award.DoesNotExist):\n        Award.objects.get(id=award2.id)\n\n    # Check that the select_related queryset is empty\n    self.assertFalse(book_with_awards.award_set.exists())\n", "    def test_prefetch_related_objects(self):\n        # Creating some objects for the test\n        person1 = Person.objects.create(name='Person 1')\n        person2 = Person.objects.create(name='Person 2')\n        Book.objects.create(name='Book 1', author=person1)\n        Book.objects.create(name='Book 2', author=person2)\n\n        # Using _prefetch_related_objects()\n        qs = Person.objects.all()\n        qs._result_cache = list(qs)\n        qs._prefetch_related_lookups = ('book_set',)\n        qs._prefetch_related_objects()\n\n        # Checking if the prefetching was successful\n        for person in qs._result_cache:\n            self.assertTrue(hasattr(person, '_prefetched_objects_cache'))\n            self.assertIn('book_set', person._prefetched_objects_cache)\n"], "sample_1049": ["def test_arbitrary_point():\n    p = Plane((1, 1, 1), normal_vector=(1, 0, 0))\n    u, v = symbols('u v')\n    assert p.arbitrary_point(u, v) == Point3D(1, u + 1, v + 1)\n    t = symbols('t')\n    assert p.arbitrary_point(t) == Point3D(1, cos(t) + 1, sin(t) + 1)\n", "def test_distance_to_line():\n    a = Plane(Point3D(1, 1, 1), normal_vector=(1, 1, 1))\n    c = Line3D(Point3D(2, 3, 1), Point3D(1, 2, 2))\n    assert a.distance(c) == 0\n", "def test_plane_intersection_with_plane():\n    a = Plane(Point3D(1, 2, 3), normal_vector=(1, 1, 1))\n    b = Plane(Point3D(4, 5, 6), normal_vector=(2, 2, 2))\n    intersection = a.intersection(b)\n    assert len(intersection) == 1\n    assert isinstance(intersection[0], Line3D)\n    assert intersection[0].p1 == Point3D(78/23, -24/23, 0)\n    assert intersection[0].p2 == Point3D(147/23, 321/23, 23)\n", "def test_plane_distance_to_point():\n    a = Plane(Point3D(1, 1, 1), normal_vector=(1, 1, 1))\n    b = Point3D(1, 2, 3)\n    assert a.distance(b) == sqrt(3)\n", "def test_are_coplanar_3d():\n    # Test are_coplanar with 3D points\n    p1 = Point3D(1, 2, 3)\n    p2 = Point3D(4, 5, 6)\n    p3 = Point3D(7, 8, 9)\n    assert not are_coplanar(p1, p2, p3)\n\n    # Test are_coplanar with 3D points that are coplanar\n    p4 = Point3D(2, 4, 6)\n    assert are_coplanar(p1, p2, p4)\n\n    # Test are_coplanar with 3D lines that are coplanar\n    l1 = Line3D(p1, p2)\n    l2 = Line3D(p1, p4)\n    assert are_coplanar(l1, l2)\n\n    # Test are_coplanar with 3D lines that are not coplanar\n    l3 = Line3D(p1, p3)\n    assert not are_coplanar(l1, l3)\n\n    # Test are_coplanar with a 3D line and a 3D plane that are coplanar\n    plane = Plane(p1, p2, p4)\n    assert are_coplanar(l1, plane)\n\n    # Test are_coplanar with a 3D line and a 3D plane that are not coplanar\n    assert not are_coplanar(l3, plane)\n\n    # Test are_coplanar with 3D planes that are coplanar\n    plane2 = Plane(p1, p2, p3)\n    assert are_coplanar(plane, plane2)\n\n    # Test are_coplanar with 3D planes that are not coplanar\n    plane3 = Plane(p1, p3, p4)\n    assert not are_coplanar(plane, plane3)\n"], "sample_165": ["    def setUpTestData(cls):\n        for i in range(5):\n            ChoiceModel.objects.create(choice=\"choice %s\" % i)\n", "def test_modelform_defines_fields(self):\n    class ModelFormDefinesFields(Form):\n        class Meta:\n            fields = ['field1', 'field2']\n\n    class ModelFormDoesNotDefineFields(Form):\n        pass\n\n    self.assertTrue(modelform_defines_fields(ModelFormDefinesFields))\n    self.assertFalse(modelform_defines_fields(ModelFormDoesNotDefineFields))\n", "    def test_modelform_choicefield(self):\n        class ChoiceModelForm(forms.ModelForm):\n            class Meta:\n                model = ChoiceModel\n                fields = ('choice',)\n\n        choices = [(obj.pk, str(obj)) for obj in ChoiceModel.objects.all()]\n        form = ChoiceModelForm()\n        self.assertIsInstance(form.fields['choice'], ModelChoiceField)\n        self.assertEqual(list(form.fields['choice'].choices), choices)\n", "def test_model_multiple_choice_field_clean(self):\n    queryset = ChoiceModel.objects.filter(pk__in=[1, 2, 3])\n    field = ModelMultipleChoiceField(queryset=queryset)\n\n    # Test with valid values\n    self.assertEqual(list(field.clean([1, 2])), list(queryset.filter(pk__in=[1, 2])))\n\n    # Test with invalid values\n    expected_error = ['Select a valid choice. 4 is not one of the available choices.']\n    self.assertFormErrors(expected_error, field.clean, [1, 4])\n", "def test_model_choice_field_clean(self):\n    queryset = ChoiceModel.objects.all()\n    field = ModelChoiceField(queryset=queryset)\n\n    # Test cleaning a valid value\n    value = queryset.first().pk\n    cleaned_value = field.clean(value)\n    self.assertEqual(cleaned_value, queryset.get(pk=value))\n\n    # Test cleaning an invalid value\n    invalid_value = 999\n    expected_error = ['Select a valid choice. That choice is not one of the available choices.']\n    self.assertFormErrors(expected_error, field.clean, invalid_value)\n"], "sample_759": ["def test_onehotencoder_sparse_output():\n    enc = OneHotEncoder(sparse=True)\n    X = [['Male', 1], ['Female', 3], ['Female', 2]]\n    enc.fit(X)\n    X_transformed = enc.transform(X)\n    assert sparse.issparse(X_transformed)\n", "def test_ordinal_encoder_with_custom_categories():\n    enc = OrdinalEncoder(categories=[['Female', 'Male'], [1, 2, 3]])\n    X = [['Male', 1], ['Female', 3], ['Female', 2]]\n    enc.fit(X)\n    assert_array_equal(enc.categories_, [['Female', 'Male'], [1, 2, 3]])\n    X_transformed = enc.transform(X)\n    assert_array_equal(X_transformed, [[1., 0.], [0., 2.], [0., 1.]])\n    X_inverse = enc.inverse_transform(X_transformed)\n    assert_array_equal(X_inverse, np.array(X, dtype=object))\n", "def test_ordinal_encoder_inverse_transform_dtype():\n    # Test that the dtype of the output of inverse_transform is the same as the\n    # input dtype.\n    X = np.array([['Male', 1], ['Female', 3], ['Female', 2]], dtype=object)\n    enc = OrdinalEncoder()\n    enc.fit(X)\n    X_trans = enc.transform(X)\n    X_inv = enc.inverse_transform(X_trans)\n    assert_equal(X_inv.dtype, X.dtype)\n", "def test_ordinal_encoder_categories():\n    enc = OrdinalEncoder(categories=[[\"male\", \"female\"], [1, 2, 3]])\n    enc.fit([[\"female\", 3], [\"male\", 1]])\n    assert_array_equal(enc.transform([[\"female\", 3], [\"male\", 1]]), [[0., 2.], [1., 0.]])\n    assert_array_equal(enc.inverse_transform([[1., 0.], [0., 2.]]), [[\"male\", 1], [\"female\", 3]])\n", "def test_ordinal_encoder_with_unknown_values():\n    enc = OrdinalEncoder(categories=[[\"apple\", \"banana\", \"cherry\"], [\"red\", \"green\", \"blue\"]])\n    enc.fit([[\"apple\", \"red\"], [\"banana\", \"green\"], [\"cherry\", \"blue\"]])\n\n    X_test = [[\"banana\", \"green\"], [\"apple\", \"blue\"], [\"grape\", \"red\"]]\n    with pytest.raises(ValueError, match=\"Found unknown categories.*\"):\n        enc.transform(X_test)\n"], "sample_859": ["def test_dual_gap(est):\n    # Test that dual gap is smaller than tol\n    rng = np.random.RandomState(0)\n    X = rng.randn(100, 10)\n    y = rng.randn(100)\n    model = est(tol=1e-6, random_state=0)\n    model.fit(X, y)\n    assert model.dual_gap_ < 1e-6\n", "def test_elastic_net_cv_fit_mono_task():\n    X, y = load_boston(return_X_y=True)\n    X = np.c_[X, np.ones(X.shape[0])]  # add constant\n    enet_cv = ElasticNetCV(cv=5, random_state=0)\n    enet_cv.fit(X, y)\n    assert hasattr(enet_cv, \"coef_\")\n    assert hasattr(enet_cv, \"intercept_\")\n    assert hasattr(enet_cv, \"alpha_\")\n    assert hasattr(enet_cv, \"l1_ratio_\")\n    assert hasattr(enet_cv, \"mse_path_\")\n    assert hasattr(enet_cv, \"alphas_\")\n    assert hasattr(enet_cv, \"n_iter_\")\n    assert enet_cv.coef_.shape == (X.shape[1],)\n    assert enet_cv.intercept_.shape == ()\n    assert enet_cv.mse_path_.shape == (len(enet_cv.alphas_), 5)\n    assert enet_cv.alphas_.shape == (len(enet_cv.alphas_),)\n    assert isinstance(enet_cv.n_iter_, int)\n", "def test_elastic_net_fit_with_sparse_input():\n    X = sparse.csc_matrix([[1, 2], [3, 4], [5, 6]])\n    y = np.array([1, 2, 3])\n\n    model = ElasticNet(alpha=0.1, l1_ratio=0.5)\n    model.fit(X, y)\n\n    assert_array_almost_equal(model.coef_, [0.35714286, 0.64285714], decimal=4)\n    assert_almost_equal(model.intercept_, 0.78571429, decimal=4)\n", "def test_elasticnet_path_multi_task_input():\n    # Test elasticnet_path with multi-task input\n    n_samples, n_features, n_tasks = 50, 10, 3\n    rng = np.random.RandomState(0)\n    X = rng.randn(n_samples, n_features)\n    Y = rng.randn(n_samples, n_tasks)\n    l1_ratio = 0.5\n    eps = 1e-3\n    n_alphas = 10\n    alphas = np.logspace(-2, 2, n_alphas)\n\n    alphas_, coefs, _ = enet_path(X, Y, l1_ratio=l1_ratio, eps=eps, n_alphas=n_alphas, alphas=alphas)\n\n    assert coefs.shape == (n_tasks, n_features, n_alphas)\n    assert_array_equal(alphas_, alphas)\n", "def test_elasticnet_coef_init():\n    rng = np.random.RandomState(0)\n    X = rng.randn(10, 5)\n    y = rng.randn(10)\n    coef_init = rng.randn(5)\n\n    enet = ElasticNet(alpha=0.1, l1_ratio=0.5, fit_intercept=False, max_iter=1000,\n                      tol=1e-4, copy_X=True, random_state=0, coef_init=coef_init)\n    enet.fit(X, y)\n\n    enet_no_init = ElasticNet(alpha=0.1, l1_ratio=0.5, fit_intercept=False, max_iter=1000,\n                              tol=1e-4, copy_X=True, random_state=0)\n    enet_no_init.fit(X, y)\n\n    assert_array_almost_equal(enet.coef_, enet_no_init.coef_, decimal=3)\n"], "sample_522": ["def test_colorbar_extension_length():\n    fig_uniform = _colorbar_extension_length('uniform')\n    fig_proportional = _colorbar_extension_length('proportional')\n    return fig_uniform, fig_proportional\n", "def test_colorbar_extension_length():\n    fig_uniform = _colorbar_extension_length('uniform')\n    fig_proportional = _colorbar_extension_length('proportional')\n    return fig_uniform, fig_proportional\n", "def test_colorbar_extensions_length():\n    fig_uniform = _colorbar_extension_length(spacing='uniform')\n    fig_proportional = _colorbar_extension_length(spacing='proportional')\n    return fig_uniform, fig_proportional\n", "def test_colorbar_orientation_vertical():\n    \"\"\"\n    Test that the colorbar orientation is set correctly to vertical.\n    \"\"\"\n    # Get a colormap and appropriate norms for each extension type.\n    cmap, norms = _get_cmap_norms()\n    # Create a figure and adjust whitespace for subplots.\n    fig, ax = plt.subplots()\n    im = ax.imshow(np.random.random((10, 10)), cmap=cmap, norm=norms['neither'])\n    # Generate the colorbar.\n    cb = fig.colorbar(im, ax=ax, orientation='vertical')\n    # Check the orientation.\n    assert cb.orientation == 'vertical'\n", "def test_colorbar_extension_length():\n    # Test with uniform spacing\n    fig_uniform = _colorbar_extension_length(spacing='uniform')\n    # Test with proportional spacing\n    fig_proportional = _colorbar_extension_length(spacing='proportional')\n    # Return figures for comparison\n    return fig_uniform, fig_proportional\n"], "sample_814": ["def test_classification_toy_presort_and_loss(presort, loss):\n    check_classification_toy(presort, loss)\n", "def test_gradient_boosting_classifier_loss(presort, loss):\n    # Test the gradient boosting classifier with different loss functions.\n    clf = GradientBoostingClassifier(loss=loss, n_estimators=10,\n                                     random_state=1, presort=presort)\n    clf.fit(iris.data, iris.target)\n    assert_greater(clf.score(iris.data, iris.target), 0.9)\n", "def check_classification_boston(presort, loss):\n    # Check classification on the Boston housing dataset.\n    clf = GradientBoostingClassifier(loss=loss, n_estimators=10,\n                                     random_state=1, presort=presort)\n    clf.fit(boston.data, boston.target)\n    assert_greater(clf.score(boston.data, boston.target), 0.8)\n\n    leaves = clf.apply(boston.data)\n    assert_equal(leaves.shape, (boston.data.shape[0], 10, 1))\n", "def test_regression_toy_presort(presort):\n    # Check regression on a toy dataset with presort.\n    reg = GradientBoostingRegressor(n_estimators=10, random_state=1, presort=presort)\n\n    reg.fit(X, y)\n    pred = reg.predict(T)\n\n    # Check that the predicted results are close to the true results\n    assert_array_almost_equal(pred, true_result, decimal=1)\n\n    # Check that the number of estimators is equal to the number specified\n    assert_equal(10, len(reg.estimators_))\n\n    # Check that the train score decreases at each stage\n    deviance_decrease = (reg.train_score_[:-1] - reg.train_score_[1:])\n    assert np.all(deviance_decrease >= 0.0)\n\n    # Check the shape of the leave indices\n    leaves = reg.apply(X)\n    assert_equal(leaves.shape, (6, 10, 1))\n", "def check_regression_toy(presort, loss, alpha=0.9):\n    # Check regression on a toy dataset.\n    reg = GradientBoostingRegressor(loss=loss, n_estimators=10,\n                                    random_state=1, presort=presort,\n                                    alpha=alpha)\n\n    assert_raises(ValueError, reg.predict, T)\n\n    reg.fit(X, y)\n    assert_array_almost_equal(reg.predict(T), true_result, decimal=1)\n    assert_equal(10, len(reg.estimators_))\n\n    deviance_decrease = (reg.train_score_[:-1] - reg.train_score_[1:])\n    assert np.any(deviance_decrease <= 0.0)\n\n    leaves = reg.apply(X)\n    assert_equal(leaves.shape, (6, 10))\n"], "sample_903": ["def test_tsne_with_barnes_hut_method():\n    tsne = TSNE(n_components=2, method='barnes_hut', random_state=42)\n    X_transformed = tsne.fit_transform(X_2d_grid)\n    assert_equal(X_transformed.shape, (100, 2))\n", "def test_trustworthiness_with_precomputed():\n    # Test trustworthiness with precomputed metric\n    X = np.array([[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 1]])\n    X_embedded = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n    dist_X = pairwise_distances(X, metric='euclidean')\n    t = trustworthiness(dist_X, X_embedded, n_neighbors=2, precomputed=True)\n    assert_almost_equal(t, 1.0, decimal=2)\n", "def test_tsne_gradient_descent():\n    # Test the gradient descent function with a simple objective function\n        if compute_error:\n            error = np.sum(p ** 2)\n        else:\n            error = np.nan\n        grad = 2 * p\n        return error, grad\n\n    p0 = np.array([1.0, 2.0])\n    p, error, it = _gradient_descent(objective, p0, it=0, n_iter=100)\n    assert_array_almost_equal(p, np.zeros(2), decimal=2)\n    assert_less(error, 1e-4)\n    assert_equal(it, 99)\n", "def test_gradient_descent():\n    # Define a simple objective function for testing\n        error = np.sum(p ** 2)\n        grad = 2 * p\n        return error, grad\n\n    # Test gradient descent\n    p0 = np.array([1.0, 2.0])\n    p, error, it = _gradient_descent(simple_objective, p0, it=0, n_iter=100,\n                                     n_iter_check=1, learning_rate=0.1, verbose=0)\n    assert_array_almost_equal(p, np.zeros(2))\n", "def test_kl_divergence_bh_input_types():\n    # Test that _kl_divergence_bh accepts float32 and float64 input\n    rng = check_random_state(42)\n    X = rng.randn(10, 2).astype(np.float64)\n    P = _joint_probabilities_nn(*NearestNeighbors().fit(X).kneighbors())\n    degrees_of_freedom = 1\n    n_samples, n_components = X.shape\n    params = X.ravel().astype(np.float32)\n    _kl_divergence_bh(params, P, degrees_of_freedom, n_samples, n_components)\n    params = X.ravel().astype(np.float64)\n    _kl_divergence_bh(params, P, degrees_of_freedom, n_samples, n_components)\n"], "sample_1084": ["def test_intersection_sets_complexregion_interval():\n    a = ComplexRegion(Interval(0, 2), Interval(0, pi))\n    b = ComplexRegion(Interval(1, 3), Interval(0, pi))\n    expected = ComplexRegion(Interval(1, 2), Interval(0, pi))\n    assert intersection_sets(a, b) == expected\n", "def test_intersection_sets_interval_and_complexregion():\n    a = Interval(0, 2)\n    b = ComplexRegion(Interval(0, 1) * Interval(0, pi))\n    result = intersection_sets(a, b)\n    assert result == Interval(0, 1)\n", "def test_intersection_sets_rationals_integers():\n    a = S.Rationals\n    b = S.Integers\n    result = intersection_sets(a, b)\n    assert result == a\n", "def test_intersection_sets_range_interval():\n    a = Range(1, 5)\n    b = Interval(2, 6)\n    expected = Range(2, 5)\n    assert intersection_sets(a, b) == expected\n", "def test_intersection_sets_interval_complexregion():\n    a = ComplexRegion(Interval(1, 2), Interval(0, pi))\n    b = ComplexRegion(Interval(2, 3), Interval(0, pi))\n    c = intersection_sets(a, b)\n    expected_result = ComplexRegion(Interval(2, 2), Interval(0, pi))\n    assert c == expected_result\n"], "sample_1132": ["def test_is_palindromic():\n    assert is_palindromic([1, 0, 1])\n    assert not is_palindromic('abcbb')\n    assert not is_palindromic('abcbb', 1)\n    assert is_palindromic('abcbb', 1, -1)\n    assert is_palindromic('abcbb', -4, -1)\n", "def test_is_palindromic():\n    assert is_palindromic([1, 0, 1]) == True\n    assert is_palindromic('abcbb') == False\n    assert is_palindromic('abcbb', 1) == False\n    assert is_palindromic('abcbb', 1, -1) == True\n    assert is_palindromic('abcbb', -4, -1) == True\n", "def test_rotations():\n    seq = [1, 2, 3]\n    assert list(rotations(seq)) == [[1, 2, 3], [2, 3, 1], [3, 1, 2]]\n    assert list(rotations(seq, -1)) == [[1, 2, 3], [3, 1, 2], [2, 3, 1]]\n", "def test_is_palindromic():\n    assert is_palindromic('abcba')\n    assert not is_palindromic('abcab')\n    assert is_palindromic([1, 0, 1])\n    assert not is_palindromic('abcbb', 1)\n    assert is_palindromic('abcbb', 1, -1)\n    assert is_palindromic('abcbb', -4, -1)\n", "def test_is_palindromic():\n    assert is_palindromic([1, 0, 1]) is True\n    assert is_palindromic('abcbb') is False\n    assert is_palindromic('abcbb', 1) is False\n    assert is_palindromic('abcbb', 1, -1) is True\n    assert is_palindromic('abcbb', -4, -1) is True\n    assert is_palindromic('abcba', 1, -1) is True\n    assert is_palindromic('abcba', 1, -2) is False\n"], "sample_554": ["def test_annotation_with_arrow():\n    fig, ax = plt.subplots()\n    annotation = Annotation(\"Test Annotation\", xy=(0.5, 0.5), xytext=(0.2, 0.2),\n                            arrowprops=dict(facecolor='black', shrink=0.05))\n    ax.add_artist(annotation)\n    fig.canvas.draw()\n    plt.close(fig)\n", "def test_annotation_clip():\n    fig, ax = plt.subplots()\n    ann = ax.annotate(\"Test\", xy=(1.5, 1.5), annotation_clip=True)\n    assert not ann.contains(MouseEvent(\"test\", ax.figure.canvas, 0, 0))[0]\n    ann.set_annotation_clip(False)\n    assert ann.contains(MouseEvent(\"test\", ax.figure.canvas, 0, 0))[0]\n    ann.set_annotation_clip(None)\n    ann.set_xycoords(\"figure pixels\")\n    assert not ann.contains(MouseEvent(\"test\", ax.figure.canvas, 0, 0))[0]\n    ann.set_xycoords(\"data\")\n    assert ann.contains(MouseEvent(\"test\", ax.figure.canvas, 0, 0))[0]\n    plt.close(fig)\n", "def test_annotation_xycoords():\n    fig, ax = plt.subplots()\n    annotation = Annotation(\"Test\", xy=(0.5, 0.5), xycoords='axes fraction',\n                            xytext=(0, 0), textcoords='offset points',\n                            arrowprops=dict(arrowstyle='->'))\n    ax.add_artist(annotation)\n    plt.close(fig)\n", "def test_annotation_arrowprops_no_arrow():\n    fig, ax = plt.subplots()\n    annot = Annotation(\"Test\", xy=(0.5, 0.5), xytext=(0.2, 0.2),\n                       arrowprops=None)\n    ax.add_artist(annot)\n    fig.canvas.draw()\n    assert annot.arrow_patch is None\n", "def test_annotation_arrowprops_fancy():\n    fig, ax = plt.subplots()\n    ann = Annotation(\"Test\", xy=(0.5, 0.5), xytext=(0.2, 0.2),\n                     arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3\"))\n    ax.add_artist(ann)\n    plt.close(fig)\n"], "sample_188": ["def test_f_expression_get_group_by_cols(self):\n    f_expr = F('num_employees')\n    self.assertEqual(f_expr.get_group_by_cols(), [f_expr])\n", "def test_combinable_expression_methods(self):\n    # Test __neg__ method\n    expr = F('num_employees')\n    negated_expr = -expr\n    self.assertIsInstance(negated_expr, Combinable)\n    self.assertEqual(str(negated_expr), '0 - num_employees')\n\n    # Test __add__ method\n    expr = F('num_employees')\n    added_expr = expr + 5\n    self.assertIsInstance(added_expr, Combinable)\n    self.assertEqual(str(added_expr), 'num_employees + 5')\n\n    # Test __sub__ method\n    expr = F('num_employees')\n    subtracted_expr = expr - 10\n    self.assertIsInstance(subtracted_expr, Combinable)\n    self.assertEqual(str(subtracted_expr), 'num_employees - 10')\n\n    # Test __mul__ method\n    expr = F('num_employees')\n    multiplied_expr = expr * 2\n    self.assertIsInstance(multiplied_expr, Combinable)\n    self.assertEqual(str(multiplied_expr), 'num_employees * 2')\n\n    # Test __truediv__ method\n    expr = F('num_employees')\n    divided_expr = expr / 3\n    self.assertIsInstance(divided_expr, Combinable)\n    self.assertEqual(str(divided_expr), 'num_employees / 3')\n\n    # Test __mod__ method\n    expr = F('num_employees')\n    modulo_expr = expr % 4\n    self.assertIsInstance(modulo_expr, Combinable)\n    self.assertEqual(str(modulo_expr), 'num_employees %% 4')\n\n    # Test __pow__ method\n    expr = F('num_employees')\n    powered_expr = expr ** 2\n    self.assertIsInstance(powered_expr, Combinable)\n    self.assertEqual(str(powered_expr), 'num_employ", "    def test_combined_expression(self):\n        # Test for combined expressions\n        expr = F('num_employees') + 5\n        self.assertEqual(str(expr), \"Company.num_employees + 5\")\n\n        expr = F('num_employees') * 2\n        self.assertEqual(str(expr), \"Company.num_employees * 2\")\n\n        expr = F('num_employees') / 10\n        self.assertEqual(str(expr), \"Company.num_employees / 10\")\n\n        expr = F('num_employees') - F('num_chairs')\n        self.assertEqual(str(expr), \"Company.num_employees - Company.num_chairs\")\n\n        expr = F('num_employees') % 3\n        self.assertEqual(str(expr), \"Company.num_employees %% 3\")\n\n        expr = F('num_employees') ** 2\n        self.assertEqual(str(expr), \"Company.num_employees ^ 2\")\n", "    def test_subquery_annotation(self):\n        # Test subquery annotation\n        subquery = Company.objects.filter(name=OuterRef('name')).values('num_employees')\n        annotated_query = Company.objects.annotate(employee_count=Subquery(subquery))\n        result = annotated_query.get(name='Example Inc.')\n        self.assertEqual(result.employee_count, 2300)\n", "def test_expression_repr(self):\n    expr = F('name') + Value(1)\n    self.assertEqual(repr(expr), \"<CombinedExpression: name + 1>\")\n    expr = F('name') + F('salary')\n    self.assertEqual(repr(expr), \"<CombinedExpression: name + salary>\")\n    expr = F('name') * Value(2.5)\n    self.assertEqual(repr(expr), \"<CombinedExpression: name * 2.5>\")\n    expr = F('name') / F('num_employees')\n    self.assertEqual(repr(expr), \"<CombinedExpression: name / num_employees>\")\n    expr = F('num_chairs') - Value(3)\n    self.assertEqual(repr(expr), \"<CombinedExpression: num_chairs - 3>\")\n    expr = F('num_employees') ** Value(2)\n    self.assertEqual(repr(expr), \"<CombinedExpression: num_employees ^ 2>\")\n    expr = F('num_employees') % Value(5)\n    self.assertEqual(repr(expr), \"<CombinedExpression: num_employees %% 5>\")\n"], "sample_478": ["    def test_filter_vertical_invalid_field_type(self):\n        class TestModelAdmin(ModelAdmin):\n            filter_vertical = ['invalid_field']\n\n        msg = \"The value of 'filter_vertical[0]' must be a many-to-many field.\"\n        self.assertIsInvalid(TestModelAdmin, ValidationTestModel, msg, id='admin.E020')\n", "    def test_filter_horizontal_must_be_sequence(self):\n        class BandAdmin(ModelAdmin):\n            filter_horizontal = \"members\"\n\n        self.assertIsInvalid(\n            BandAdmin,\n            Band,\n            \"The value of 'filter_horizontal' must be a list or tuple.\",\n            id=\"admin.E018\",\n        )\n", "    def test_list_display_with_callable(self):\n        class TestModelAdmin(ModelAdmin):\n            list_display = [callable]\n\n        self.assertIsValid(TestModelAdmin, Band)\n", "    def test_save_as_invalid_type(self):\n        class MyModelAdmin(ModelAdmin):\n            save_as = \"invalid\"\n\n        self.assertIsInvalid(\n            MyModelAdmin,\n            ValidationTestModel,\n            \"The value of 'save_as' must be a boolean.\",\n            id=\"admin.E101\",\n        )\n", "    def test_raw_id_fields_attname(self):\n        class BandAdmin(ModelAdmin):\n            raw_id_fields = (\"user\",)\n\n        self.assertIsInvalidRegexp(\n            BandAdmin,\n            Band,\n            \"The value of 'raw_id_fields\\\\[0\\\\]' refers to 'user', which is not a field of 'app_label.Band'.\",\n            id=\"admin.E002\",\n        )\n"], "sample_1102": ["def test_poly_with_invalid_input():\n    expr = sin(x)\n    poly(expr)\n", "def test_to_rational_coeffs():\n    p = poly((x**2 - 1)*(x - 2), x, domain='EX').subs({x: x*(1 + sqrt(2))})\n    lc, r, _, g = to_rational_coeffs(p)\n    assert lc == 7 + 5*sqrt(2)\n    assert r == 2 - 2*sqrt(2)\n    assert g == Poly(x**3 + x**2 - 1/4*x - 1/4, x, domain='QQ')\n", "def test_poly_multiplication():\n    f = poly(x*(x+1), x)\n    g = poly(x*(x-1), x)\n    result = f * g\n    expected = poly(x**3 - x, x)\n    assert result == expected\n", "def test_poly_evaluation():\n    p = Poly(x**2 - 2*x + 1)\n    assert p(2) == -3\n    assert p(x) == x**2 - 2*x + 1\n    assert p(x + 1) == (x + 1)**2 - 2*(x + 1) + 1\n", "def test_to_rational_coeffs():\n    f = poly((x*(1 + sqrt(2))**2 - 1)*(x*(1 + sqrt(2)) - 2), x)\n    lc, r, _, g = to_rational_coeffs(f)\n    assert lc == 7 + 5*sqrt(2)\n    assert r == 2 - 2*sqrt(2)\n    assert g == poly(x**3 + x**2 - Rational(1, 4)*x - Rational(1, 4), x)\n\n    f = poly((x + sqrt(2))**2 - 1)*(x + sqrt(2) - 2), x)\n    _, _, t, g = to_rational_coeffs(f)\n    assert t == sqrt(2)\n    assert g == poly(x**3 + x**2 - 2*x - 1, x)\n"], "sample_462": ["def test_choicefield_to_python(self):\n    field = ChoiceField(choices=[('1', 'One'), ('2', 'Two')])\n    self.assertEqual(field.to_python('1'), '1')\n    self.assertEqual(field.to_python('2'), '2')\n    self.assertEqual(field.to_python(''), '')\n    self.assertEqual(field.to_python(None), '')\n", "    def test_uuid_field_valid(self):\n        field = UUIDField()\n        value = '123e4567-e89b-12d3-a456-426614174000'\n        python_value = field.to_python(value)\n        self.assertEqual(str(python_value), value)\n", "def test_choice_field_valid_value(self):\n    field = ChoiceField(choices=[(\"A\", \"Option A\"), (\"B\", \"Option B\")])\n    self.assertTrue(field.valid_value(\"A\"))\n    self.assertTrue(field.valid_value(\"B\"))\n    self.assertFalse(field.valid_value(\"C\"))\n", "    def test_uuid_field(self):\n        field = UUIDField()\n        valid_uuid = '123e4567-e89b-12d3-a456-426614174000'\n        invalid_uuid = 'not-a-uuid'\n\n        # Test valid UUID\n        self.assertEqual(field.to_python(valid_uuid), uuid.UUID(valid_uuid))\n\n        # Test invalid UUID\n        with self.assertRaises(ValidationError):\n            field.to_python(invalid_uuid)\n\n        # Test empty value\n        self.assertIsNone(field.to_python(''))\n", "def test_choice_field_to_python(self):\n    field = ChoiceField(choices=[('1', 'One'), ('2', 'Two')])\n    self.assertEqual(field.to_python('1'), '1')\n    self.assertEqual(field.to_python('2'), '2')\n    self.assertEqual(field.to_python(''), '')\n    self.assertEqual(field.to_python(None), '')\n"], "sample_633": ["def test_hide_code_with_imports():\n    sim = similar.Similar(min_lines=3, ignore_imports=True)\n    with open(HIDE_CODE_WITH_IMPORTS, encoding=\"utf-8\") as stream:\n        sim.append_stream(HIDE_CODE_WITH_IMPORTS, stream)\n    similarities = sim._compute_sims()\n    assert len(similarities) == 1\n    num, couples = similarities[0]\n    assert num == 3\n    assert len(couples) == 2\n", "def test_ignore_imports():\n    sim = similar.Similar(min_lines=3, ignore_imports=True)\n    with open(HIDE_CODE_WITH_IMPORTS, encoding=\"utf-8\") as stream:\n        sim.append_stream(HIDE_CODE_WITH_IMPORTS, stream)\n    sim.run()\n    # Check that the similarity report does not contain any imports\n    assert \"import\" not in sim._get_similarity_report(sim._compute_sims())\n", "def test_similar_multiline_import():\n    linter = PyLinter()\n    linter.set_reporter(Reporter())\n    linter.config.min_similarity_lines = 4\n    linter.config.ignore_comments = True\n    linter.config.ignore_docstrings = True\n    linter.config.ignore_imports = True\n    linter.config.ignore_signatures = True\n\n    linter.check([MULTILINE])\n    assert len(linter.reporter.messages) == 1\n    msg = linter.reporter.messages[0]\n    assert msg.symbol == \"R0801\"\n    assert msg.msg_id == \"R0801\"\n    assert msg.line == 1\n    assert msg.args[0] == 2\n    assert \"==multiline-import:[1:5]\" in msg.args[1]\n    assert \"==multiline-import:[6:10]\" in msg.args[1]\n", "def test_similar_checker_with_imports_and_signatures(lint_runner):\n    output = StringIO()\n    with redirect_stdout(output):\n        lint_runner(\n            [HIDE_CODE_WITH_IMPORTS, SIMILAR1],\n            \"--disable=all\",\n            \"--enable=similarities\",\n            \"--ignore-imports=no\",\n            \"--ignore-signatures=no\",\n        )\n    assert \"4 similar lines in 2 files\" in output.getvalue()\n", "def test_similar_imports():\n    sim = similar.Similar(ignore_imports=True)\n    sim.append_stream(\"file1\", open(MULTILINE))\n    sim.append_stream(\"file2\", open(HIDE_CODE_WITH_IMPORTS))\n    output = StringIO()\n    with redirect_stdout(output):\n        sim.run()\n    result = output.getvalue()\n    # Assert that no similarities are found due to imports\n    assert \"similar lines\" not in result\n"], "sample_930": ["def test_create_index(app, status, warning):\n    env = app.builder.env\n    idx_entries = IndexEntries(env)\n\n    # Add some index entries\n    domain = env.get_domain('index')\n    domain.entries = {\n        'file1': [('single', 'Keyword1 Subkeyword1', 'id1', 'Main1', None)],\n        'file2': [('pair', 'Keyword2 Subkeyword2', 'id2', 'Main2', None)],\n    }\n\n    # Create the index\n    result = idx_entries.create_index(app.builder)\n\n    # Check the result\n    assert len(result) == 2  # Two index entries\n    assert result[0][0] == 'Keyword1'  # First keyword\n    assert result[0][1][0][0] == ('Main1', 'genindex.html#id1')  # Main entry for first keyword\n    assert result[0][1][0][1][0][0] == 'Subkeyword1'  # Subentry for first keyword\n    assert result[1][0] == 'Keyword2'  # Second keyword\n    assert result[1][1][0][0] == ('Main2', 'genindex.html#id2')  # Main entry for second keyword\n    assert result[1][1][0][1][0][0] == 'Subkeyword2'  # Subentry for second keyword\n", "def test_create_index(app, status, warning):\n    env = app.builder.env\n    index_entries = IndexEntries(env)\n\n    # Add some test data\n    domain = env.get_domain('index')\n    domain.entries['testfile'] = [\n        ('single', 'TestEntry', 'test-id', 'Test Entry', None),\n        ('pair', 'TestPair1 TestPair2', 'test-id', 'Test Pair', None),\n        ('triple', 'TestTriple1 TestTriple2 TestTriple3', 'test-id', 'Test Triple', None),\n        ('see', 'TestSee TestSeen', 'test-id', None, None),\n        ('seealso', 'TestSeeAlso TestSeenAlso', 'test-id', None, None),\n    ]\n\n    # Call the function under test\n    index = index_entries.create_index(app.builder)\n\n    # Assert the expected results\n    assert len(index) > 0\n    for key, group in index:\n        for entry in group:\n            assert entry[0] is not None\n            assert len(entry[1]) > 0\n", "def test_create_index(app, status, warning):\n    env = app.builder.env\n    index_entries = IndexEntries(env)\n\n    # Add some dummy entries to the index domain\n    env.domains['index'].entries['dummy_file'] = [\n        ('single', 'Entry1 Subentry1', 'id1', 'Main1', None),\n        ('pair', 'Entry2 Entry3', 'id2', 'Main2', None),\n        ('triple', 'Entry4 Entry5 Entry6', 'id3', 'Main3', None),\n        ('see', 'Entry7 Entry8', 'id4', 'Main4', None),\n        ('seealso', 'Entry9 Entry10', 'id5', 'Main5', None),\n    ]\n\n    # Call the create_index method and check the results\n    index = index_entries.create_index(app.builder)\n    assert len(index) == 5  # 5 main entries\n    for group_key, group_items in index:\n        for key, (targets, subitems, _key) in group_items:\n            if key == 'Entry1':\n                assert targets == [('Main1', 'dummy_file#id1')]\n                assert subitems == {'Subentry1': [[], {}, None]}\n            elif key == 'Entry2':\n                assert targets == [('Main2', 'dummy_file#id2')]\n                assert subitems == {'Entry3': [('Main2', 'dummy_file#id2')], 'Entry10': [('Main5', None)]}\n            elif key == 'Entry3':\n                assert targets == [('Main2', 'dummy_file#id2')]\n                assert subitems == {'Entry2': [('Main2', 'dummy_file#id2')], 'Entry9': [('Main5', None)]}\n            elif key == 'Entry4':\n                assert targets == [('Main3', 'dummy_file#id3')]\n                assert subitems == {'Entry5 Entry6': [('Main3', 'dummy_file#id3')]}\n            elif key == 'Entry5':\n                assert targets == [('Main3", "def test_create_index(app, status, warning):\n    indexentries = IndexEntries(app.env)\n    builder = app.builder\n\n    # Add some sample data to the index\n    domain = app.env.get_domain('index')\n    domain.entries['file1'] = [\n        ('single', 'Keyword', 'id1', 'Main Entry', None),\n        ('pair', 'First Second', 'id2', 'Main Entry', None),\n        ('triple', 'First Second Third', 'id3', 'Main Entry', None),\n        ('see', 'Keyword See', 'See Entry', None, None),\n        ('seealso', 'Keyword See Also', 'See Also Entry', None, None),\n        ('unknown', 'Unknown Entry', 'id4', 'Main Entry', None),\n        ('single', 'Keyword Error', 'id5', 'Main Entry', None),\n    ]\n\n    # Call the create_index method\n    index = indexentries.create_index(builder)\n\n    # Assert the expected results\n    assert len(index) > 0\n    assert index[0][0] == 'K'\n    assert index[0][1][0][0] == 'Keyword'\n    assert index[0][1][1][0] == 'Keyword Error'\n    assert index[0][1][2][0] == 'Keyword See'\n    assert index[0][1][3][0] == 'Keyword See Also'\n    assert warning.getvalue() == 'expected error message\\n'\n", "def test_create_index(app, status, warning):\n    index_entries = IndexEntries(app.env)\n    builder = app.builder\n\n    # Add some test entries\n    indexentries = [\n        ('single', 'Python Libraries; Sphinx', 'index-0', 'Sphinx Documentation', None),\n        ('pair', 'Python; Language Reference', 'index-1', 'Python Language Documentation', None),\n        ('triple', 'Python; Libraries; requests', 'index-2', 'Requests: HTTP for Humans', None),\n        ('see', 'Python; PEP 8', 'index-3', None, None),\n        ('seealso', 'Python; PEP 20', 'index-4', None, None),\n    ]\n    app.env.domains['index'].entries['testfile'] = indexentries\n\n    index = index_entries.create_index(builder)\n\n    # Check if the index is correctly generated\n    assert index[0][0] == 'Python'  # Python should be the first entry\n    assert len(index[0][1]) == 3  # Python should have 3 sub-entries\n    assert index[1][0] == 'Symbols'  # Symbols should be the second entry\n    assert len(index[1][1]) == 1  # Symbols should have 1 sub-entry\n    assert index[1][1][0][0] == 'Sphinx Documentation'  # Sphinx Documentation should be under Symbols\n\n    # Check if the warning is raised for unknown index entry type\n    app.env.domains['index'].entries['testfile'].append(('unknown', 'Test; Entry', 'index-5', 'Test Entry', None))\n    with pytest.raises(UserWarning):\n        index_entries.create_index(builder)\n"], "sample_317": ["    def test_rss201rev2_feed(self):\n        response = self.client.get('/rss201rev2/')\n        self.assertEqual(response.status_code, 200)\n        feed = minidom.parseString(response.content)\n        channel = feed.getElementsByTagName('channel')[0]\n        self.assertChildNodes(channel, ['title', 'link', 'description', 'language', 'item', 'lastBuildDate'])\n        self.assertChildNodeContent(channel, {'title': 'Test Feed', 'description': 'This is a test feed.'})\n        items = channel.getElementsByTagName('item')\n        self.assertEqual(len(items), 5)\n        item_titles = [i.getElementsByTagName('title')[0].firstChild.wholeText for i in items]\n        self.assertEqual(set(item_titles), {'My first entry', 'My second entry', 'My third entry', 'A & B < C > D', 'My last entry'})\n", "def test_atom1_feed(self):\n    feed = Atom1Feed(\n        title=\"Test Feed\",\n        link=\"/feed/\",\n        description=\"This is a test feed\",\n        language=\"en\",\n        author_name=\"Test Author\",\n        author_email=\"test@example.com\",\n        author_link=\"/author/\",\n        subtitle=\"Subtitle for the test feed\",\n        categories=[\"Category 1\", \"Category 2\"],\n        feed_url=\"/feed-url/\",\n        feed_copyright=\"Copyright 2022, Test Author\",\n        feed_guid=\"/feed-guid/\",\n        ttl=60,\n    )\n\n    feed.add_item(\n        title=\"Test Item\",\n        link=\"/item/\",\n        description=\"This is a test item\",\n        author_email=\"item@example.com\",\n        author_name=\"Test Item Author\",\n        author_link=\"/item-author/\",\n        pubdate=datetime.datetime(2022, 1, 1, tzinfo=TZ),\n        comments=\"/item/comments/\",\n        unique_id=\"/item-guid/\",\n        categories=[\"Item Category 1\", \"Item Category 2\"],\n        item_copyright=\"Copyright 2022, Test Item Author\",\n        ttl=30,\n        updateddate=datetime.datetime(2022, 1, 2, tzinfo=TZ),\n        enclosures=[Enclosure(\"/enclosure/\", \"12345\", \"audio/mpeg\")],\n    )\n\n    feed_content = feed.writeString(\"utf-8\")\n    xml = minidom.parseString(feed_content)\n\n    self.assertChildNodes(xml.documentElement, [\"title\", \"link\", \"id\", \"updated\", \"author\", \"subtitle\", \"category\", \"rights\", \"entry\"])\n    self.assertChildNodeContent(xml.documentElement, {\"title\": \"Test Feed\", \"subtitle\": \"Subtitle for the test feed\", \"rights\": \"Copyright 2022, Test Author\"})\n    self.assertCategories(xml.documentElement, [\"Category 1\", \"", "    def test_feed_attributes(self):\n        feed = Atom1Feed(\n            title='Test Title',\n            link='http://example.com/',\n            description='Test Description',\n            language='en',\n        )\n        self.assertEqual(\n            feed.root_attributes(),\n            {'xmlns': 'http://www.w3.org/2005/Atom', 'xml:lang': 'en'},\n        )\n", "    def test_atom1_feed_generation(self):\n        feed = Atom1Feed(\n            title=\"Test Feed\",\n            link=\"http://test.com/feed\",\n            description=\"Test Description\",\n            language=\"en\",\n            feed_url=\"http://test.com/atom\",\n            author_name=\"Test Author\",\n            author_email=\"test@test.com\",\n            author_link=\"http://test.com/author\",\n            subtitle=\"Test Subtitle\",\n            categories=[\"Test\", \"Category\"],\n            feed_copyright=\"Copyright Test\",\n        )\n        feed.add_item(\n            title=\"Test Item\",\n            link=\"http://test.com/item\",\n            description=\"Test Description\",\n            author_name=\"Test Author\",\n            author_email=\"test@test.com\",\n            author_link=\"http://test.com/author\",\n            pubdate=datetime.datetime(2022, 1, 1, tzinfo=TZ),\n            updateddate=datetime.datetime(2022, 1, 2, tzinfo=TZ),\n            unique_id=\"http://test.com/item\",\n            categories=[\"Test\", \"Item\"],\n            item_copyright=\"Copyright Test\",\n        )\n        feed_str = feed.writeString('utf-8')\n        dom = minidom.parseString(feed_str)\n        feed_elem = dom.getElementsByTagName('feed')[0]\n        self.assertEqual(feed_elem.attributes['xmlns'].value, Atom1Feed.ns)\n        self.assertEqual(feed_elem.attributes['xml:lang'].value, 'en')\n        self.assertChildNodes(feed_elem, ['title', 'link', 'link', 'id', 'updated', 'author', 'subtitle', 'category', 'rights', 'entry'])\n        self.assertChildNodeContent(feed_elem, {'title': 'Test Feed', 'id': 'http://test.com/feed', 'subtitle': 'Test Subtitle', 'rights': 'Copyright", "def test_rss_feed_generation(self):\n    \"\"\"\n    Test the generation of RSS feed.\n    \"\"\"\n    feed = Rss201rev2Feed(\n        title=\"Test Feed\",\n        link=\"/feed/\",\n        description=\"A test feed.\",\n        language=\"en\",\n    )\n    feed.add_item(\n        title=self.e1.title,\n        link=\"/article/%d/\" % self.e1.id,\n        description=self.e1.content,\n        pubdate=self.e1.published,\n        updateddate=self.e1.updated,\n    )\n    rss = minidom.parseString(feed.writeString(\"utf-8\"))\n    self.assertChildNodes(rss.childNodes[0], ['channel'])\n    channel = rss.childNodes[0].childNodes[0]\n    self.assertChildNodes(\n        channel,\n        ['title', 'link', 'description', 'language', 'lastBuildDate', 'item'],\n    )\n    self.assertChildNodeContent(channel, {'title': \"Test Feed\", 'link': \"/feed/\"})\n    item = channel.childNodes[-1]\n    self.assertChildNodes(item, ['title', 'link', 'description', 'pubDate', 'guid'])\n    self.assertChildNodeContent(item, {'title': self.e1.title})\n    self.assertEqual(item.getElementsByTagName('guid')[0].firstChild.wholeText, str(self.e1.id))\n"], "sample_216": ["def test_field_is_referenced(self):\n    project_state = ProjectState()\n    model_states = [self.author_with_book, self.book]\n    for model_state in model_states:\n        project_state.add_model(model_state.clone())\n    self.assertTrue(field_is_referenced(project_state, (\"testapp\", \"author\"), (\"id\",)))\n    self.assertFalse(field_is_referenced(project_state, (\"testapp\", \"author\"), (\"name\",)))\n    self.assertTrue(field_is_referenced(project_state, (\"otherapp\", \"book\"), (\"author\",)))\n    self.assertFalse(field_is_referenced(project_state, (\"otherapp\", \"book\"), (\"title\",)))\n", "    def test_field_references_with_related_field(self):\n        before_state = [\n            self.author_with_book,\n            self.book,\n        ]\n        after_state = [\n            self.author_with_book_order_wrt,\n            self.book,\n        ]\n        changes = self.get_changes(before_state, after_state)\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, ['AlterModelOptions'])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name='Author', options={'order_with_respect_to': 'book'})\n", "    def test_field_references(self):\n        author_to_book_field = self.book.fields['author']\n        author_model_tuple = ('testapp', 'author')\n        book_model_tuple = ('otherapp', 'book')\n\n        # Test when field references to the provided context\n        reference = field_references(book_model_tuple, author_to_book_field, author_model_tuple)\n        self.assertIsInstance(reference, FieldReference)\n\n        # Test when field does not reference to the provided context\n        publisher_to_author_field = self.publisher_with_author.fields['author']\n        reference = field_references(book_model_tuple, publisher_to_author_field, author_model_tuple)\n        self.assertFalse(reference)\n", "    def test_field_is_referenced(self):\n        # Create a project state with a model referencing another model\n        project_state = ProjectState()\n        author_state = self.author_with_book.clone()\n        book_state = self.book.clone()\n        project_state.add_model(author_state)\n        project_state.add_model(book_state)\n\n        # Test that the 'author' field in 'Book' is referenced by the 'Author' model\n        self.assertTrue(field_is_referenced(project_state, ('testapp', 'author'), ('author',)))\n\n        # Test that a non-existent field is not referenced\n        self.assertFalse(field_is_referenced(project_state, ('testapp', 'author'), ('nonexistent',)))\n", "    def test_field_references_with_foreign_key(self):\n        before_states = [self.author_with_book]\n        after_states = [self.author_with_book]\n        changes = self.get_changes(before_states, after_states)\n        # Test if the field_references function correctly identifies foreign keys\n        self.assertTrue(field_is_referenced(changes.new_state, ('testapp', 'author'), ('book', None)))\n"], "sample_1110": ["def test_print_Mod():\n    expr = Mod(x, y)\n    assert pycode(expr) == \"x % y\"\n", "def test_print_KroneckerDelta():\n    expr = KroneckerDelta(x, y)\n    result = pycode(expr)\n    assert result == '(1 if x == y else 0)'\n", "def test_KroneckerDelta():\n    expr = KroneckerDelta(x, y)\n    printer = PythonCodePrinter()\n    assert printer._print_KroneckerDelta(expr) == '(1 if x == y else 0)'\n", "def test_SymPyPrinter():\n    # Test the _print_Function method\n    expr = acos(x)\n    printer = SymPyPrinter()\n    result = printer._print_Function(expr)\n    assert result == 'sympy.acos(x)'\n", "def test_print_MatrixSolve():\n    A = MatrixSymbol('A', 3, 3)\n    b = MatrixSymbol('b', 3, 1)\n    expr = MatrixSolve(A, b)\n    result = pycode(expr)\n    expected = \"numpy.linalg.solve(A, b)\"\n    assert result == expected\n"], "sample_1032": ["def test_minmax_assumptions():\n    x = Symbol('x', real=True)\n    y = Symbol('y', positive=True)\n    z = Symbol('z', negative=True)\n\n    assert Min(x, y).is_real is True\n    assert Min(x, y).is_positive is None\n    assert Min(x, z).is_negative is True\n    assert Max(x, y).is_real is True\n    assert Max(x, y).is_positive is True\n    assert Max(x, z).is_negative is False\n", "def test_sqrt_properties():\n    x = Symbol('x', positive=True)\n    assert sqrt(x**2) == x\n    assert sqrt(x**4) == x**2\n    y = Symbol('y', negative=True)\n    assert sqrt(y**2) == I*abs(y)\n\n    z = Symbol('z')\n    assert sqrt(z**2) == abs(z)\n", "def test_min_max_with_functions():\n    x = Symbol('x', real=True)\n    assert Min(sin(x), cos(x)) == Min(sin(x), cos(x))\n    assert Max(sin(x), cos(x)) == Max(sin(x), cos(x))\n", "def test_min_max_evaluation():\n    x, y = Symbol('x'), Symbol('y')\n    assert Min(x, -2).subs(x, 3) == -2\n    assert Max(x, -2).subs(x, 3) == 3\n", "def test_root_of_unity():\n    x = Symbol('x')\n    assert root(x**2 - 1, 2, 0) == 1\n    assert root(x**2 - 1, 2, 1) == -1\n    assert root(x**3 - 1, 3, 0) == 1\n    assert root(x**3 - 1, 3, 1) == -1/2 - sqrt(3)*I/2\n    assert root(x**3 - 1, 3, 2) == -1/2 + sqrt(3)*I/2\n    assert root(x**4 - 1, 4, 0) == 1\n    assert root(x**4 - 1, 4, 1) == -1\n    assert root(x**4 - 1, 4, 2) == -I\n    assert root(x**4 - 1, 4, 3) == I\n"], "sample_363": ["    def test_foreignkey_rawid_widget(self):\n        \"\"\"\n        Test that a ForeignKey field with raw_id_fields set returns a\n        ForeignKeyRawIdWidget.\n        \"\"\"\n        self.assertFormfield(\n            Car, 'owner',\n            widgets.ForeignKeyRawIdWidget,\n            raw_id_fields=('owner',),\n        )\n", "def test_formfield_for_uuidfield(self):\n    self.assertFormfield(ReleaseEvent, 'uuid', widgets.AdminUUIDInputWidget)\n", "def test_formfield_for_datetimefield(self):\n    \"\"\"Test formfield_for_dbfield() for a DateTimeField.\"\"\"\n    self.assertFormfield(ReleaseEvent, 'release_date', widgets.AdminSplitDateTime)\n", "    def test_many_to_many_field(self):\n        \"\"\"\n        Test that formfield_for_dbfield returns a MultipleChoiceField for ManyToMany fields\n        \"\"\"\n        self.assertFormfield(Band, 'members', forms.widgets.SelectMultiple)\n", "    def test_relatedfieldwidgetwrapper_with_can_add_related(self):\n        \"\"\"\n        Test that RelatedFieldWidgetWrapper is used with can_add_related set to True\n        \"\"\"\n        ff = self.assertFormfield(Car, 'owner', widgets.RelatedFieldWidgetWrapper, can_add_related=True)\n        self.assertTrue(ff.widget.can_add_related)\n"], "sample_979": ["def test_matrix_element_derivative():\n    A_elem = MatrixElement(A, n, m)\n    B_elem = MatrixElement(A, l, k)\n    assert diff(A_elem, A_elem) == KroneckerDelta(n, l)*KroneckerDelta(m, k)\n    assert diff(A_elem, B_elem) == S.Zero\n    assert diff(A_elem, x) == S.Zero\n", "def test_matrix_element_derivative():\n    i, j = symbols('i j', integer=True)\n    A_ij = MatrixElement(A, i, j)\n    B_ij = MatrixElement(B, i, j)\n\n    assert diff(A_ij, A_ij) == 1\n    assert diff(A_ij, B_ij) == 0\n    assert diff(A_ij, MatrixElement(A, i+1, j)) == 0\n    assert diff(A_ij, MatrixElement(A, i, j+1)) == 0\n    assert diff(A_ij, MatrixElement(A, i+1, j+1)) == 0\n", "def test_matrix_element_derivative():\n    A_ij = MatrixElement(A, i, j)\n    B_ij = MatrixElement(B, k, l)\n    v_ij = MatrixElement(A, k, l)\n\n    # Test derivative with respect to another matrix element\n    assert diff(A_ij, B_ij) == KroneckerDelta(i, k) * KroneckerDelta(j, l)\n\n    # Test derivative with respect to the same matrix element\n    assert diff(A_ij, v_ij) == KroneckerDelta(i, k) * KroneckerDelta(j, l)\n\n    # Test derivative with respect to a scalar\n    assert diff(A_ij, x) == S.Zero\n", "def test_matrix_symbol_subs():\n    A_new = MatrixSymbol('A_new', l, k)\n    B_subs = B._subs(MatrixSymbol('B', m, l), A_new)\n    assert B_subs == A_new\n", "def test_matrix_element_derivative():\n    i, j = symbols('i j')\n    A_ij = MatrixElement(A, i, j)\n    B_ij = MatrixElement(A, i, j)\n    v_ij = MatrixElement(A, k, l)\n\n    derivative = diff(A_ij, B_ij)\n    assert derivative == KroneckerDelta(i, k) * KroneckerDelta(j, l)\n\n    derivative = diff(A_ij, v_ij)\n    assert derivative == A.diff(v_ij)[i, j]\n"], "sample_263": ["    def test_dumpdata_with_pks(self):\n        category = Category.objects.create(name='Test')\n        article1 = Article.objects.create(title='Test1', category=category)\n        article2 = Article.objects.create(title='Test2', category=category)\n\n        out = StringIO()\n        management.call_command('dumpdata', 'myapp.Article', primary_keys=str(article1.pk), stdout=out)\n        self.assertIn(f'\"pk\": {article1.pk}', out.getvalue())\n        self.assertNotIn(f'\"pk\": {article2.pk}', out.getvalue())\n", "    def test_primary_keys_filter(self):\n        site1 = Site.objects.create(domain=\"example1.com\")\n        site2 = Site.objects.create(domain=\"example2.com\")\n\n        out = StringIO()\n        with self.settings(SITE_ID=site1.pk):\n            management.call_command(\"dumpdata\", \"sites.Site\", primary_keys=site1.pk, stdout=out)\n\n        self.assertIn(str(site1.pk), out.getvalue())\n        self.assertNotIn(str(site2.pk), out.getvalue())\n", "    def test_dumpdata_with_exclude(self):\n        out = StringIO()\n        management.call_command('dumpdata', 'sites', exclude=['sites.Site'], stdout=out)\n        self.assertEqual(out.getvalue(), '')\n", "    def test_exclude_option(self):\n        # Create some data\n        Category.objects.create(name='test_category')\n        Tag.objects.create(name='test_tag')\n\n        # Run the dumpdata command with the exclude option\n        out = StringIO()\n        management.call_command('dumpdata', 'sites', exclude=['sites.Site'], stdout=out)\n\n        # Check that the Site model was not included in the output\n        self.assertNotIn('\"model\": \"sites.site\"', out.getvalue())\n", "    def test_exclude_option(self):\n        # Test the --exclude option\n        out = StringIO()\n        management.call_command('dumpdata', 'auth', exclude=['User'], stdout=out)\n        self.assertNotIn('auth.user', out.getvalue())\n"], "sample_19": ["def test_maps_scale(self):\n    for filename in self._file_list:\n        header = fits.Header.fromtextfile(filename)\n        w = wcs.WCS(header)\n        pix_scale = w.proj_plane_pixel_scales()\n        assert_quantity_allclose(pix_scale[0], pix_scale[1], atol=1e-6 * u.arcsec)\n", "def test_wcs_maps(self):\n    for filename in self._file_list:\n        hdr = fits.Header.fromtextfile(filename)\n        w = wcs.WCS(hdr)\n        w.printwcs()\n", "def test_wcs_maps_with_skip_keywords(self):\n    with pytest.warns(wcs.FITSFixedWarning, match=\"Card with key 'SKIP' was ignored\"):\n        wcs.WCS(self._file_list[0])\n", "def test_maps_wcs(self):\n    for filename in self._file_list:\n        with fits.open(filename, mode=\"readonly\") as hdulist:\n            # Test if WCS can be created from the header\n            w = wcs.WCS(hdulist[0].header)\n            assert w.naxis == 2\n            assert w.wcs.naxis == 2\n            assert w.wcs.lng == 0\n            assert w.wcs.lat == 1\n            assert w.wcs.lngtyp == \"RA\"\n            assert w.wcs.lattyp == \"DEC\"\n\n            # Test if pixel coordinates can be converted to world coordinates\n            x, y = np.array([[0, 0], [1, 1], [2, 2]])\n            ra, dec = w.all_pix2world(x, y, 0)\n            assert_array_almost_equal(ra, [0, 0.000277777777778, 0.000555555555556])\n            assert_array_almost_equal(dec, [0, 0.000277777777778, 0.000555555555556])\n\n            # Test if world coordinates can be converted to pixel coordinates\n            x, y = w.all_world2pix(ra, dec, 0)\n            assert_array_almost_equal(x, [0, 1, 2])\n            assert_array_almost_equal(y, [0, 1, 2])\n", "def test_maps_header_only(self):\n    for filename in self._file_list:\n        with fits.open(filename, ignore_missing_simple=True) as hdulist:\n            header = hdulist[0].header\n            w = wcs.WCS(header)\n            assert w.naxis == 2\n            assert w.wcs.naxis == 2\n            assert w.wcs.lng >= 0\n            assert w.wcs.lat >= 0\n            assert w.wcs.lngtyp == \"RA\"\n            assert w.wcs.lattyp == \"DEC\"\n"], "sample_30": ["def test_table_element_to_xml():\n    votable = tree.VOTableFile()\n    table = tree.Table(votable, ID=\"test_table\")\n    table.format = \"tabledata\"\n    with io.StringIO() as f:\n        table.to_xml(tree.XMLWriter(f))\n        output = f.getvalue()\n    assert \"<TABLE ID=\\\"test_table\\\" nrows=\\\"0\\\"><DATA><TABLEDATA/></DATA></TABLE>\" in output\n", "def test_get_first_table():\n    filename = get_pkg_data_filename(\"data/regression.xml\")\n    votable = tree.parse(filename)\n    first_table = votable.get_first_table()\n    assert first_table is not None\n    assert len(first_table.array) > 0\n", "def test_large_array():\n    # Test that a table with a large array can be parsed without error\n    nrows = 10000\n    data = np.random.rand(nrows, 3)\n    table = tree.Table(tree.VOTableFile(), data=data)\n    votable = tree.VOTableFile()\n    resource = tree.Resource()\n    resource.tables.append(table)\n    votable.resources.append(resource)\n    fd = io.BytesIO()\n    votable.to_xml(fd)\n    fd.seek(0)\n    votable = parse(fd)\n    assert len(votable.resources[0].tables[0].array) == nrows\n", "def test_validate_votable_files():\n    for filename in get_pkg_data_filenames(\"data\", pattern=\"*.xml\"):\n        with open(filename, \"rb\") as f:\n            votable = tree.parse(f)\n        for table in votable.iter_tables():\n            assert table.array.dtype.names is not None\n", "def test_parse_single_table_invalid_row():\n    votable_str = \"\"\"<VOTABLE version=\"1.3\">\n    <RESOURCE>\n        <TABLE ID=\"table1\">\n            <FIELD ID=\"col1\" datatype=\"int\" name=\"col1\"/>\n            <DATA>\n                <TABLEDATA>\n                    <TR><TD>1</TD></TR>\n                    <TR><TD>invalid</TD></TR>\n                </TABLEDATA>\n            </DATA>\n        </TABLE>\n    </RESOURCE>\n    </VOTABLE>\n    \"\"\"\n    with pytest.raises(ValueError):\n        parse_single_table(votable_str)\n"], "sample_458": ["def test_floatformat_with_positive_arg(self):\n    with self.settings(USE_THOUSAND_SEPARATOR=True, USE_L10N=True):\n        translation.activate(\"en\")\n        self.assertEqual(floatformat(Decimal(\"1234.5678\"), 2), \"1,234.57\")\n        self.assertEqual(floatformat(Decimal(\"1234.0000\"), 3), \"1,234.000\")\n        self.assertEqual(floatformat(Decimal(\"1234.5600\"), 3), \"1,234.560\")\n        translation.deactivate()\n", "def test_floatformat_with_args(self):\n    self.context.update(\n        {\n            \"c\": Decimal(\"6666.6666\"),\n            \"d\": Decimal(\"66666.6666\"),\n            \"e\": Decimal(\"6666.6666\"),\n        }\n    )\n    self.assertEqual(self.engine.render(\"floatformat02\"), \"6,666.667 66666.667 6,666.666\")\n", "def test_floatformat_decimal_with_precision(self):\n    with translation.override(\"en\"):\n        self.assertHTMLEqual(\n            self.engine.render_to_string(\n                \"floatformat01\", {\"a\": Decimal(\"1.23456\"), \"b\": Decimal(\"1.00000\")}\n            ),\n            \"1.2 1\",\n        )\n\n    with translation.override(\"de\"):\n        self.assertHTMLEqual(\n            self.engine.render_to_string(\n                \"floatformat01\", {\"a\": Decimal(\"1.23456\"), \"b\": Decimal(\"1.00000\")}\n            ),\n            \"1,2 1\",\n        )\n", "def test_floatformat_with_arg(self):\n    self.assertEqual(\n        self.engine.render_to_string(\n            \"floatformat02\", {\"a\": 34.23234, \"b\": 34.00000}\n        ),\n        \"34.232 34\",\n    )\n", "def test_floatformat_with_arg(self):\n    context = {\"a\": Decimal(\"34.23234\"), \"b\": Decimal(\"34.00000\")}\n    result = self.engine.render_to_string(\"floatformat01\", context)\n    self.assertEqual(result, \"34.2 34\")\n"], "sample_925": ["def test_mock_module():\n    with mock(['target.module']):\n        m = import_module('target.module')\n        assert isinstance(m, _MockModule)\n        assert m.__name__ == 'target.module'\n        assert m.__file__ == os.devnull\n\n        attr = m.nonexistent_attribute\n        assert isinstance(attr, _MockObject)\n        assert attr.__display_name__ == 'target.module.nonexistent_attribute'\n\n        submodule = m.submodule\n        assert isinstance(submodule, _MockObject)\n        assert submodule.__display_name__ == 'target.module.submodule'\n", "def test_mock_module_creation(module_name):\n    with mock([module_name]):\n        imported_module = import_module(module_name)\n        assert isinstance(imported_module, _MockModule)\n        assert imported_module.__file__ == os.devnull\n        assert imported_module.__name__ == module_name\n        assert imported_module.__sphinx_mock__\n", "def test_mock_module_attributes():\n    \"\"\"Test the attributes of the mocked module.\"\"\"\n    modname = \"test.module.name\"\n    with mock([modname]):\n        mod = import_module(modname)\n        assert mod.__sphinx_mock__ is True\n        assert mod.__file__ == os.devnull\n        assert mod.__all__ == []\n        assert mod.__path__ == []\n", "def test_mock_module_getattr():\n    mock_module = _MockModule('test_module')\n    assert isinstance(mock_module.nonexistent_attribute, _MockObject)\n    assert mock_module.nonexistent_attribute.__display_name__ == 'test_module.nonexistent_attribute'\n", "def test_mock_module_creation(modname):\n    with mock([modname]):\n        mod = import_module(modname)\n        assert isinstance(mod, _MockModule)\n        assert mod.__name__ == modname\n        assert mod.__file__ == os.devnull\n        assert mod.__sphinx_mock__ is True\n"], "sample_506": ["def test_spines_proxy():\n    fig, ax = plt.subplots()\n    spines = Spines(left=ax.spines['left'], right=ax.spines['right'])\n    proxy = spines[['left', 'right']]\n    proxy.set_color('red')\n    assert ax.spines['left'].get_edgecolor() == 'red'\n    assert ax.spines['right'].get_edgecolor() == 'red'\n", "def test_spines_set_color():\n    fig, ax = plt.subplots()\n    ax.spines['top'].set_color('red')\n    ax.spines['bottom'].set_color('green')\n    ax.spines['left'].set_color('blue')\n    ax.spines['right'].set_color('yellow')\n", "def test_spine_bounds():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3], [4, 5, 6])\n    ax.spines['left'].set_bounds(2, 5)\n    ax.spines['bottom'].set_bounds(1.5, 2.5)\n", "def test_spines_proxy():\n    fig, ax = plt.subplots()\n    spines = Spines(left=ax.spines['left'], right=ax.spines['right'])\n    proxy = SpinesProxy(spines)\n    proxy.set_color('red')\n    assert ax.spines['left'].get_edgecolor() == 'red'\n    assert ax.spines['right'].get_edgecolor() == 'red'\n", "def test_spines_proxy_broadcasts_to_all_spines():\n    fig, ax = plt.subplots()\n    spines = Spines.from_dict({\n        'left': ax.spines['left'],\n        'right': ax.spines['right'],\n        'bottom': ax.spines['bottom'],\n        'top': ax.spines['top']\n    })\n    spines[:].set_color('red')\n    assert all(spine.get_edgecolor() == 'red' for spine in spines.values())\n"], "sample_255": ["def test_wsgi_request_handler(self):\n    request = self.request_factory.get('/')\n    environ = WSGIRequest(request).environ\n\n    stdin = BytesIO()\n    stdout = BytesIO()\n    stderr = BytesIO()\n\n    handler = WSGIRequestHandler(Stub(rfile=stdin, wfile=stdout, get_stderr=lambda: stderr), ('127.0.0.1', 8000), Stub())\n    handler.request = request\n    handler.request_version = '1.1'\n    handler.command = 'GET'\n    handler.path = '/'\n    handler.headers = {}\n\n    handler.handle_one_request()\n\n    self.assertIn(b'HTTP/1.1 200 OK', stdout.getvalue())\n    self.assertIn(b'Connection: close', stdout.getvalue())\n\n    stdout.seek(0)\n    stdout.truncate()\n\n    handler.headers['Content-Length'] = '10'\n    handler.handle_one_request()\n\n    self.assertIn(b'HTTP/1.1 200 OK', stdout.getvalue())\n    self.assertNotIn(b'Connection: close', stdout.getvalue())\n", "def test_handle_one_request(self):\n    request = self.request_factory.get('/')\n    environ = WSGIRequestHandler.environ(request)\n    environ['REQUEST_METHOD'] = 'GET'\n    environ['PATH_INFO'] = '/'\n    environ['wsgi.input'] = BytesIO()\n    environ['wsgi.errors'] = BytesIO()\n    environ['wsgi.version'] = (1, 0)\n    environ['wsgi.run_once'] = True\n\n    response_content = b'Test content'\n        status = '200 OK'\n        response_headers = [('Content-type', 'text/plain')]\n        start_response(status, response_headers)\n        return [response_content]\n\n    stdout = BytesIO()\n    stderr = BytesIO()\n\n    handler = WSGIRequestHandler(Stub(rfile=BytesIO(b'GET / HTTP/1.1\\r\\n\\r\\n'), wfile=stdout, get_stderr=lambda: stderr), ('127.0.0.1', 80), WSGIServer)\n    handler.server = Stub(get_app=lambda: wsgi_app)\n    handler.handle_one_request()\n\n    self.assertEqual(stdout.getvalue(), b'HTTP/1.1 200 OK\\r\\nContent-Type: text/plain\\r\\nConnection: close\\r\\n\\r\\nTest content')\n    self.assertEqual(stderr.getvalue(), b'')\n", "def test_handle_one_request(self):\n    request = self.request_factory.get('/')\n    environ = WSGIRequest(request).environ\n    stdout = BytesIO()\n    stderr = BytesIO()\n\n    handler = WSGIRequestHandler(Stub(getsockname=lambda: ('127.0.0.1', 80)), Stub(), stdout, environ)\n    handler.send_response = lambda code, message=None: None\n    handler.send_header = lambda keyword, value: None\n    handler.end_headers = lambda: None\n    handler.close_connection = True\n\n    handler.handle_one_request()\n    self.assertIn(b'HTTP/1.1 200 OK', stdout.getvalue())\n", "def test_handle_one_request(self):\n    request = self.request_factory.get('/')\n    environ = WSGIRequestHandler.get_environ(Stub(request=request))\n    environ['wsgi.input'] = BytesIO()\n    environ['SERVER_PROTOCOL'] = 'HTTP/1.1'\n    environ['CONTENT_LENGTH'] = '0'\n\n    handler = WSGIRequestHandler(request, '127.0.0.1', Stub(socket=Stub(makefile=lambda *args, **kwargs: BytesIO())))\n    handler.server = Stub(get_app=lambda: lambda env, start_response: start_response('200 OK', []) and [b'Hello, World!'])\n\n    with captured_stderr() as stderr:\n        handler.handle_one_request()\n\n    self.assertIn(b'200 OK', stderr.getvalue())\n", "def test_handle_one_request(self):\n    request = self.request_factory.get('/')\n    environ = WSGIRequestHandler.environ(request)\n\n    # Stub request and response objects\n    rfile = BytesIO(b'GET / HTTP/1.1\\r\\n\\r\\n')\n    wfile = BytesIO()\n\n    # Initialize the handler\n    handler = WSGIRequestHandler(rfile, wfile, None, environ, Stub(getsockname=lambda: ('127.0.0.1', 8000)))\n    handler.server = Stub(get_app=lambda: lambda environ, start_response: start_response('200 OK', []))\n    handler.handle_one_request()\n\n    # Verify the response\n    self.assertEqual(wfile.getvalue(), b'HTTP/1.1 200 OK\\r\\n\\r\\n')\n"], "sample_480": ["    def test_key_transform_lt(self):\n        JSONModel.objects.create(json_field={'key': 10})\n        JSONModel.objects.create(json_field={'key': 20})\n        JSONModel.objects.create(json_field={'key': 30})\n\n        result = JSONModel.objects.filter(json_field__key__lt=25)\n        self.assertEqual(result.count(), 2)\n        self.assertEqual(result.first().json_field['key'], 10)\n", "    def test_key_transform_in(self):\n        JSONModel.objects.create(json_field={'key1': 'value1', 'key2': 'value2'})\n        JSONModel.objects.create(json_field={'key1': 'value3', 'key2': 'value4'})\n\n        queryset = JSONModel.objects.filter(json_field__key1__in=['value1', 'value3'])\n        self.assertEqual(queryset.count(), 2)\n", "    def test_key_transform_in(self):\n        JSONModel.objects.create(json_field={'key1': 'value1', 'key2': 'value2'})\n        JSONModel.objects.create(json_field={'key1': 'value3', 'key2': 'value4'})\n\n        key_transform = KeyTransform('key1', Value('value1'))\n        key_transform_in = KeyTransformIn(key_transform, ['value1', 'value3'])\n\n        queryset = JSONModel.objects.filter(key_transform_in)\n        self.assertEqual(queryset.count(), 2)\n\n        key_transform_in = KeyTransformIn(key_transform, ['value5'])\n        queryset = JSONModel.objects.filter(key_transform_in)\n        self.assertEqual(queryset.count(), 0)\n", "    def test_key_transform_iendswith(self):\n        with self.assertRaisesMessage(TypeError, \"Transform should be an instance of KeyTransform\"):\n            JSONModel.objects.filter(json_field__key__iendswith='value')\n\n        transform = KeyTransform('key', F('json_field'))\n        qs = JSONModel.objects.annotate(key=KeyTransform('key', 'json_field')).filter(key__iendswith='value')\n        self.assertEqual(str(qs.query), 'SELECT ... FROM \"app_jsonmodel\" WHERE \"app_jsonmodel\".\"json_field\" ->> \\'key\\' ILIKE \\'%value\\'')\n", "    def test_custom_encoder_decoder(self):\n        model_instance = JSONModel.objects.create(json_field={\"key\": \"value\"})\n        self.assertEqual(JSONModel.objects.get().json_field, {\"key\": \"value\"})\n        JSONModel.objects.filter(json_field__key=\"value\").update(json_field={\"key\": \"new_value\"})\n        self.assertEqual(JSONModel.objects.get().json_field, {\"key\": \"new_value\"})\n        model_instance.refresh_from_db()\n        self.assertEqual(model_instance.json_field, {\"key\": \"new_value\"})\n\n        model_instance.json_field = {\"key\": \"another_value\"}\n        model_instance.save()\n        self.assertEqual(JSONModel.objects.get().json_field, {\"key\": \"another_value\"})\n"], "sample_661": ["def test_add_global_property(self):\n    xml = LogXML(\"junit.xml\", prefix=None)\n    xml.add_global_property(\"key\", \"value\")\n    properties_node = xml._get_global_properties_node()\n    assert properties_node.tag == \"properties\"\n    property_node = properties_node.get_unique_child\n    assert property_node.tag == \"property\"\n    property_node.assert_attr(name=\"key\", value=\"value\")\n", "    def test_add_global_property(self, testdir):\n        logxml = LogXML(testdir.tmpdir.join(\"junit.xml\"), prefix=None)\n        logxml.add_global_property(\"key\", \"value\")\n        properties_node = logxml._get_global_properties_node()\n        assert properties_node.tag == \"properties\"\n        property_node = properties_node.find_first_by_tag(\"property\")\n        assert property_node[\"name\"] == \"key\"\n        assert property_node[\"value\"] == \"value\"\n", "    def test_logxml_finalize(self, testdir):\n        logxml = LogXML(\"log.xml\", None)\n        report = BaseReport(None, \"test_nodeid\", \"test_location\", \"test_outcome\", \"test_longrepr\")\n        report.nodeid = \"test_nodeid\"\n        reporter = logxml.node_reporter(report)\n        logxml.finalize(report)\n        assert \"test_nodeid\" not in logxml.node_reporters\n", "def test_logging_no(testdir):\n    testdir.makepyfile(\n        \"\"\"\n            assert True\n        \"\"\"\n    )\n    result, xmldoc = runandparse(testdir)\n    assert result.ret == 0\n    testcase = xmldoc.find_first_by_tag(\"testcase\")\n    assert not testcase.find_by_tag(\"system-out\")\n    assert not testcase.find_by_tag(\"system-err\")\n", "def test_node_reporter_add_property(testdir):\n    result, dom = runandparse(testdir)\n    reporter = LogXML(str(testdir.tmpdir), None).node_reporter(\"test_node_reporter_add_property\")\n    reporter.add_property(\"key\", \"value\")\n    reporter.finalize()\n    prop_node = dom.find_first_by_tag(\"properties\").find_first_by_tag(\"property\")\n    assert prop_node[\"name\"] == \"key\"\n    assert prop_node[\"value\"] == \"value\"\n"], "sample_837": ["def test_get_blas_info():\n    blas_info = _get_blas_info()\n    assert isinstance(blas_info, dict)\n    assert 'macros' in blas_info\n    assert 'lib_dirs' in blas_info\n    assert 'cblas_libs' in blas_info\n", "def test_get_blas_info():\n    blas_info = _get_blas_info()\n    assert isinstance(blas_info, dict)\n    assert 'macros' in blas_info\n    assert 'lib_dirs' in blas_info\n    assert 'cblas_libs' in blas_info\n", "def test_get_blas_info():\n    blas_info = _get_blas_info()\n    assert isinstance(blas_info, dict)\n    assert 'macros' in blas_info\n    assert 'lib_dirs' in blas_info\n    assert 'cblas_libs' in blas_info\n", "def test_get_blas_info():\n    blas_info = _get_blas_info()\n    assert isinstance(blas_info, dict)\n    assert 'macros' in blas_info\n    assert 'lib_dirs' in blas_info\n    assert 'cblas_libs' in blas_info\n", "def test_get_blas_info():\n    blas_info = _get_blas_info()\n    assert isinstance(blas_info, dict)\n    assert 'macros' in blas_info\n    assert 'lib_dirs' in blas_info\n    assert 'cblas_libs' in blas_info\n"], "sample_469": ["    def test_aggregate_annotation(self):\n        authors = Author.objects.annotate(num_books=Count('book')).order_by('num_books')\n        self.assertEqual(authors[0].num_books, 1)\n        self.assertEqual(authors[1].num_books, 1)\n        self.assertEqual(authors[2].num_books, 1)\n        self.assertEqual(authors[3].num_books, 2)\n        self.assertEqual(authors[4].num_books, 2)\n        self.assertEqual(authors[5].num_books, 3)\n", "    def setUpTestData(cls):\n        cls.authors = NonAggregateAnnotationTestCase.authors\n        cls.books = NonAggregateAnnotationTestCase.books\n        cls.stores = NonAggregateAnnotationTestCase.stores\n", "    def test_annotate_with_filtered_relation(self):\n        Author.objects.annotate(\n            friends_count=Count('friends', filter=Q(friends__age__gt=30))\n        ).order_by('id')\n        # Add assertions to check the correctness of the annotate method\n", "    def setUpTestData(cls):\n        # Set up data for the test\n        # ...\n", "def test_annotations_with_filtered_relations(self):\n    # Create a filtered relation that includes books published by Apress\n    Author.objects.add_filtered_relation(\n        Q(book__publisher__name=\"Apress\"), \"apress_books\"\n    )\n\n    authors = Author.objects.annotate(\n        num_apress_books=Count(\"apress_books\")\n    ).order_by(\"num_apress_books\")\n\n    self.assertEqual(authors[0].num_apress_books, 2)\n    self.assertEqual(authors[1].num_apress_books, 1)\n    self.assertEqual(authors[2].num_apress_books, 0)\n"], "sample_267": ["    def test_sqlite_date_trunc_quarter(self):\n        dt = datetime.datetime(2022, 4, 15)\n        result = _sqlite_date_trunc('quarter', dt, None, None)\n        self.assertEqual(result, '2022-04-01')\n", "def test_sqlite_connection_params(self):\n    with self.settings(DATABASES={'default': {'ENGINE': 'django.db.backends.sqlite3', 'NAME': ':memory:', 'OPTIONS': {'timeout': 10}}}, DATABASE_ENGINE='sqlite3'):\n        conn_params = connection.get_connection_params()\n        self.assertEqual(conn_params['database'], ':memory:')\n        self.assertEqual(conn_params['detect_types'], dbapi2.PARSE_DECLTYPES | dbapi2.PARSE_COLNAMES)\n        self.assertEqual(conn_params['timeout'], 10)\n        self.assertEqual(conn_params['check_same_thread'], False)\n        self.assertEqual(conn_params['uri'], True)\n", "    def setUpTestData(cls):\n        Object.objects.create(char_field='test', float_field=3.14, int_field=42)\n", "    def test_disable_constraint_checking(self):\n        self.assertTrue(connection.disable_constraint_checking())\n", "def test_disable_constraint_checking(self):\n    with self.assertRaisesMessage(IntegrityError, 'FOREIGN KEY constraint failed'):\n        with transaction.atomic():\n            author = Author.objects.create(name='Test Author')\n            Item.objects.create(author=author, name='Test Item')\n            author.delete()\n\n    connection.disable_constraint_checking()\n    author = Author.objects.create(name='Test Author')\n    Item.objects.create(author=author, name='Test Item')\n    self.assertEqual(Author.objects.count(), 1)\n    self.assertEqual(Item.objects.count(), 1)\n\n    connection.check_constraints()\n    connection.enable_constraint_checking()\n\n    with self.assertRaisesMessage(IntegrityError, 'FOREIGN KEY constraint failed'):\n        Author.objects.get(pk=author.pk)\n"], "sample_364": ["    def test_view_must_be_callable(self):\n        with self.assertRaises(TypeError):\n            _path('/test/', View())\n", "    def test_include_with_improper_app_name(self):\n        with self.assertRaises(ImproperlyConfigured):\n            include(('urlpatterns.path_urls', None), namespace='test')\n", "    def test_include_with_namespace(self):\n        urlconf = ('included_urls', 'included')\n        view = include(urlconf, namespace='test')\n        pattern = Pattern('/include/', is_endpoint=False)\n        resolver = URLResolver(pattern, urlconf[0], kwargs=None, app_name='included', namespace='test')\n        self.assertEqual(view, resolver)\n", "    def test_path_function_view(self):\n        url = reverse('empty-view')\n        self.assertEqual(url, '/empty/')\n        resolver_match = resolve('/empty/')\n        self.assertEqual(resolver_match.view_name, 'empty-view')\n        self.assertEqual(resolver_match.func, empty_view)\n", "    def test_include_errors(self):\n        # Test for ImproperlyConfigured error when providing namespace for a dynamic module that provides a namespace\n            app_name = 'test'\n            urlpatterns = []\n        try:\n            include((urlconf,), namespace='custom_namespace')\n        except ImproperlyConfigured as e:\n            self.assertEqual(str(e), 'Cannot override the namespace for a dynamic module that provides a namespace.')\n\n        # Test for ImproperlyConfigured error when passing a tuple with more or less than 2 elements to include()\n        try:\n            include((urlconf, 'app_name', 'extra'))\n        except ImproperlyConfigured as e:\n            self.assertEqual(str(e), 'Passing a 3-tuple to include() is not supported. Pass a 2-tuple containing the list of patterns and app_name, and provide the namespace argument to include() instead.')\n\n        # Test for ImproperlyConfigured error when specifying a namespace in include() without providing an app_name\n            urlpatterns = []\n        try:\n            include(urlconf_no_app_name, namespace='custom_namespace')\n        except ImproperlyConfigured as e:\n            self.assertEqual(str(e), 'Specifying a namespace in include() without providing an app_name is not supported. Set the app_name attribute in the included module, or pass a 2-tuple containing the list of patterns and app_name instead.')\n\n        # Test for TypeError when providing a View instance instead of a callable or a tuple/list for include()\n        class TestView(View):\n                pass\n        try:\n            include(TestView.as_view())\n        except TypeError as e:\n            self.assertEqual(str(e), 'view must be a callable or a list/tuple in the case of include().')\n"], "sample_1091": ["def test_relational_canonical():\n    assert Eq(x, -y).canonical == Eq(x, -y)\n    assert Eq(-x, y).canonical == Eq(x, -y)\n    assert Eq(x, y**2).canonical == Eq(x, y**2)\n    assert Lt(x, -y).canonical == Gt(x, -y)\n    assert Lt(-x, y).canonical == Lt(x, -y)\n    assert Lt(x, y**2).canonical == Lt(x, y**2)\n    assert Gt(x, -y).canonical == Lt(x, -y)\n    assert Gt(-x, y).canonical == Gt(x, -y)\n    assert Gt(x, y**2).canonical == Gt(x, y**2)\n", "def test_relational_properties():\n    a, b = symbols('a b')\n\n    # Test canonical property\n    assert (a < b).canonical == (a < b)\n    assert (a > b).canonical == (b < a)\n    assert (-b < a).canonical == (a > -b)\n    assert (-b > a).canonical == (a < -b)\n\n    # Test reversed property\n    assert (a < b).reversed == (b > a)\n    assert (a > b).reversed == (b < a)\n\n    # Test reversedsign property\n    assert (a < b).reversedsign == (-a > -b)\n    assert (a > b).reversedsign == (-a < -b)\n\n    # Test negated property\n    assert (a < b).negated == (a >= b)\n    assert (a > b).negated == (a <= b)\n    assert (a == b).negated == (a != b)\n    assert (a != b).negated == (a == b)\n", "def test_relational_simplify():\n    e = Eq(x + y, 0)\n    simplified = e._eval_simplify()\n    assert simplified == Eq(x, -y)\n", "def test_relational_evaluation():\n    # Test evaluation of relational expressions\n    assert Eq(x, x).doit() == S.true\n    assert Eq(x, y).doit() == Eq(x, y)\n    assert Eq(x + y, x + y).doit() == S.true\n    assert Eq(x + y, x).doit() == Eq(x + y, x)\n    assert Ne(x, x).doit() == S.false\n    assert Ne(x, y).doit() == Ne(x, y)\n    assert Ne(x + y, x).doit() == Ne(x + y, x)\n    assert Lt(x, x).doit() == S.false\n    assert Lt(x, y).doit() == Lt(x, y)\n    assert Lt(x + y, x).doit() == S.false\n    assert Lt(x + y, x + 1).doit() == Lt(x + y, x + 1)\n    assert Le(x, x).doit() == S.true\n    assert Le(x, y).doit() == Le(x, y)\n    assert Le(x + y, x).doit() == S.false\n    assert Le(x + y, x + 1).doit() == Le(x + y, x + 1)\n    assert Gt(x, x).doit() == S.false\n    assert Gt(x, y).doit() == Gt(x, y)\n    assert Gt(x + y, x).doit() == S.true\n    assert Gt(x + y, x + 1).doit() == Gt(x + y, x + 1)\n    assert Ge(x, x).doit() == S.true\n    assert Ge(x, y).doit() == Ge(x, y)\n    assert Ge(x + y, x).doit() == S.true\n    assert Ge(x + y, x + 1).doit() == Ge(x + y, x + 1)\n", "def test_relational_simplify():\n    eq1 = Eq(x + y, x + y)\n    assert eq1.simplify() == True\n\n    eq2 = Eq(x**2 - y**2, (x + y)*(x - y))\n    assert eq2.simplify() == True\n\n    eq3 = Eq(sin(x) + cos(x), sqrt(2)*sin(x + pi/4))\n    assert eq3.simplify() == True\n"], "sample_102": ["def test_union(self):\n    qs1 = Number.objects.filter(num__gt=5)\n    qs2 = Number.objects.filter(num__lt=5)\n    self.assertNumbersEqual(qs1.union(qs2), [0, 1, 2, 3, 4, 6, 7, 8, 9])\n", "def test_union_with_other_queryset(self):\n    qs1 = Number.objects.filter(num__gt=5)\n    qs2 = Number.objects.filter(other_num__lt=5)\n    expected_numbers = set(qs1).union(set(qs2))\n    result = qs1.union(qs2)\n    self.assertNumbersEqual(result, expected_numbers, ordered=False)\n", "def test_union(self):\n    qs1 = Number.objects.filter(num__lt=5)\n    qs2 = Number.objects.filter(num__gt=5)\n    union_qs = qs1.union(qs2)\n    self.assertNumbersEqual(union_qs, [0, 1, 2, 3, 4, 6, 7, 8, 9])\n", "def test_queryset_union(self):\n    queryset1 = Number.objects.filter(num__lt=5)\n    queryset2 = Number.objects.filter(num__gte=5)\n    union_queryset = queryset1.union(queryset2)\n    self.assertNumbersEqual(union_queryset, [i for i in range(10)])\n", "def test_union_difference(self):\n    qs1 = Number.objects.filter(num__gt=4)\n    qs2 = Number.objects.filter(num__lt=6)\n    result = qs1.difference(qs2)\n    self.assertNumbersEqual(result, [5, 7, 8, 9])\n"], "sample_487": ["    def test_filter_horizontal_invalid_type(self):\n        class InvalidFilterHorizontalModelAdmin(admin.ModelAdmin):\n            filter_horizontal = \"invalid_type\"\n\n        self.assertIsInvalid(\n            InvalidFilterHorizontalModelAdmin,\n            ValidationTestModel,\n            \"The value of 'filter_horizontal' must be a list or tuple.\",\n            id=\"admin.E018\",\n        )\n", "    def test_radio_fields_not_dict(self):\n        class MyModelAdmin(admin.ModelAdmin):\n            radio_fields = \"not a dictionary\"\n\n        self.assertIsInvalid(\n            MyModelAdmin,\n            ValidationTestModel,\n            \"The value of 'radio_fields' must be a dictionary.\",\n            id=\"admin.E021\",\n        )\n", "    def test_date_hierarchy_valid(self):\n        class BandAdmin(admin.ModelAdmin):\n            date_hierarchy = \"founded_date\"\n        self.assertIsValid(BandAdmin, Band)\n", "    def test_filter_horizontal_invalid_type(self):\n        class TestModelAdmin(ModelAdmin):\n            filter_horizontal = \"invalid\"\n\n        self.assertIsInvalid(TestModelAdmin, ValidationTestModel, \"The value of 'filter_horizontal' must be a list or tuple.\", id='admin.E018')\n", "    def test_invalid_exclude_not_sequence(self):\n        class MyModelAdmin(ModelAdmin):\n            exclude = \"invalid_value\"\n\n        self.assertIsInvalid(\n            MyModelAdmin,\n            ValidationTestModel,\n            \"The value of 'exclude' must be a list or tuple.\",\n            id=\"admin.E014\",\n        )\n"], "sample_1183": ["def test_field_construction():\n    K, f = field((x*log(x) + 4*x**2)*exp(1/x + log(x)/3)/x**2)\n    assert isinstance(K, FracField)\n    assert K.symbols == (x, exp(1/x), log(x), x**(1/3))\n    assert K(f) == (4*x**2*(exp(1/x)) + x*(exp(1/x))*(log(x)))/((x**(1/3))**5)\n", "def test_field_evaluate():\n    _, f = field(\"x, y\", ZZ)\n    expr = (x**2 + y) / (x + 1)\n    f = f.from_expr(expr)\n    assert f(2, 3) == Rational(5, 3)\n    assert f.evaluate(x, 2) == Poly(y + 4, x, domain=ZZ) / Poly(x + 1, x, domain=ZZ)\n", "def test_rational_function_field():\n    K, f = field(\"x\", QQ, x=x)\n    assert K.domain == QQ\n    assert K.symbols == (x,)\n    assert f == x\n    assert f.as_expr() == x\n    assert f.diff(x) == 1\n\n    K, f = field((x*y + 4*x**2)*exp(1/x + y/3)/x**2, x, y)\n    assert K.domain == QQ\n    assert K.symbols == (x, y)\n    assert f == (4*x**2*exp(1/x + y/3) + x*exp(1/x + y/3)*y)/(x**(1/3))**5\n", "def test_field_operations():\n    K, x, y = field('x,y', QQ)\n    f = (x**2 + y)/(x - y)\n    g = (x + y)/(x**2 - y**2)\n\n    # Test addition\n    assert f + g == (x*(x + y) + y*(x - y))/((x - y)*(x + y))\n\n    # Test multiplication\n    assert f * g == (x**2 + y**2)/(x**2 - y**2)\n\n    # Test division\n    assert f / g == (x*(x - y) + y*(x + y))/((x - y)*(x + y))\n", "def test_evaluate():\n    K, f = field((x*sin(x) + 4*x**2)*exp(1/x + sin(x)/3)/x**2)\n    assert f(0) == 0\n    assert f(1) == 5*exp(4/3)\n"], "sample_316": ["    def test_get_image_dimensions_from_file(self):\n        img = Image.new('RGB', (100, 200))\n        temp_file = tempfile.NamedTemporaryFile(suffix='.png')\n        img.save(temp_file, 'PNG')\n\n        width, height = get_image_dimensions(temp_file.name)\n        self.assertEqual(width, 100)\n        self.assertEqual(height, 200)\n", "    def test_image_dimensions(self):\n        # Create a simple image in memory\n        image = Image.new('RGB', (100, 200))\n        data = BytesIO()\n        image.save(data, 'JPEG')\n        data.seek(0)\n\n        # Use Django's File class to wrap the image data\n        image_file = File(data)\n\n        # Use the get_image_dimensions function to get the dimensions of the image\n        width, height = images.get_image_dimensions(image_file)\n\n        # Assert that the dimensions are correct\n        self.assertEqual(width, 100)\n        self.assertEqual(height, 200)\n", "    def test_get_image_dimensions(self):\n        if Image is None:\n            self.skipTest(\"PIL not installed\")\n\n        # Create a simple image using Pillow\n        image = Image.new('RGB', (100, 200))\n        image_data = BytesIO()\n        image.save(image_data, format='PNG')\n        image_data.seek(0)\n\n        # Test with open file\n        file = File(image_data)\n        width, height = images.get_image_dimensions(file)\n        self.assertEqual(width, 100)\n        self.assertEqual(height, 200)\n\n        # Test with file path\n        temp_file = NamedTemporaryFile(delete=False, suffix='.png')\n        temp_file.write(image_data.getvalue())\n        temp_file.close()\n        width, height = images.get_image_dimensions(temp_file.name)\n        self.assertEqual(width, 100)\n        self.assertEqual(height, 200)\n\n        # Clean up\n        os.unlink(temp_file.name)\n", "def test_image_dimensions(self):\n    with mock.patch('PIL.ImageFile.Parser.feed') as mock_feed:\n        mock_feed.side_effect = [zlib.error(\"Error -5\"), None]\n        img = Image.new('RGB', (800, 600))\n        img.save('test.png')\n        with open('test.png', 'rb') as f:\n            width, height = get_image_dimensions(f)\n            self.assertEqual(width, 800)\n            self.assertEqual(height, 600)\n", "    def test_get_image_dimensions(self):\n        if not Image:\n            self.skipTest(\"Pillow not installed\")\n\n        with tempfile.NamedTemporaryFile(suffix=\".png\", delete=False) as f:\n            img = Image.new(\"RGB\", (600, 400))\n            img.save(f, \"PNG\")\n            f.seek(0)\n            self.assertEqual(images.get_image_dimensions(f), (600, 400))\n\n        with tempfile.NamedTemporaryFile(suffix=\".png\", delete=False) as f:\n            img = Image.new(\"RGB\", (800, 600))\n            img.save(f, \"PNG\")\n            f.seek(0)\n            self.assertEqual(images.get_image_dimensions(f.name), (800, 600))\n\n        self.assertEqual(images.get_image_dimensions(\"nonexistent_file.png\"), (None, None))\n"], "sample_524": ["def test_colorbar_extension_length():\n    fig_uniform = _colorbar_extension_length(spacing='uniform')\n    fig_proportional = _colorbar_extension_length(spacing='proportional')\n    return fig_uniform, fig_proportional\n", "def test_colorbar_extension_length():\n    fig_uniform = _colorbar_extension_length('uniform')\n    fig_proportional = _colorbar_extension_length('proportional')\n    fig_uniform.savefig('colorbar_extensions_length_uniform.png')\n    fig_proportional.savefig('colorbar_extensions_length_proportional.png')\n", "def test_colorbar_extension_length():\n    fig1 = _colorbar_extension_length('uniform')\n    fig2 = _colorbar_extension_length('proportional')\n    return fig1, fig2\n", "def test_colorbar_extend_kwarg():\n    \"\"\"Test the extend kwarg of colorbar.\"\"\"\n    # Create a figure and axis.\n    fig, ax = plt.subplots()\n    # Generate some data.\n    data = np.random.rand(10, 10)\n    # Create a colorbar.\n    im = ax.imshow(data, cmap='RdBu', vmin=-1, vmax=1)\n    cbar = fig.colorbar(im, extend='both')\n    # Check that the extend kwarg was set correctly.\n    assert cbar.extend == 'both'\n", "def test_colorbar_extension_length():\n    fig1 = _colorbar_extension_length('uniform')\n    fig2 = _colorbar_extension_length('proportional')\n    return fig1, fig2\n"], "sample_1074": ["def test_pointwise_stabilizer():\n    S = SymmetricGroup(7)\n    Stab = S.pointwise_stabilizer([2, 3, 5])\n    assert Stab.is_subgroup(S.stabilizer(2).stabilizer(3).stabilizer(5))\n", "def test_schreier_sims_incremental():\n    A = AlternatingGroup(7)\n    base = [2, 3]\n    seq = [2, 3]\n    base, strong_gens = A.schreier_sims_incremental(base=seq)\n    assert _verify_bsgs(A, base, strong_gens)\n    assert base[:2] == [2, 3]\n", "def test_PermutationGroup_random_pr():\n    # Test the random_pr method of PermutationGroup\n    S = SymmetricGroup(5)\n    g = S.random_pr()\n    assert isinstance(g, Permutation)\n    assert g in S\n", "def test_is_primitive():\n    G = PermutationGroup([Permutation(0, 1, 2, 3, 4), Permutation(1, 2)])\n    assert G.is_primitive(randomized=False) is True\n", "def test_stabilizer_chain():\n    G = DihedralGroup(6)\n    base = [2]\n    gens = G.generators\n    stab_gens = _stabilizer(G.degree, gens, base[0])\n    strong_gens_distr = _distribute_gens_by_base(base, gens)\n    assert _orbit(G.degree, strong_gens_distr[1], base[0]) == set([0, 2])\n    assert stab_gens == [Permutation((0, 2, 4), (1, 3, 5))]\n    base.append(3)\n    stab_gens = _stabilizer(G.degree, stab_gens, base[1])\n    strong_gens_distr.append(stab_gens)\n    assert _orbit(G.degree, strong_gens_distr[2], base[1]) == set([0, 1, 2, 3, 4, 5])\n    assert stab_gens == [Permutation((0, 3)(1, 4)(2, 5)), Permutation((0, 5)(1, 4), (2, 3))]\n"], "sample_854": ["def test_svc_decision_function_shape_ovo():\n    # Test decision_function with decision_function_shape='ovo'\n    svc = svm.SVC(kernel='linear', decision_function_shape='ovo')\n    svc.fit(X, Y)\n    dec = svc.decision_function(T)\n    assert dec.shape == (3, 3)\n", "def test_decision_function_shape_ovo():\n    # Test decision_function_shape='ovo'\n    clf = svm.SVC(kernel='linear', decision_function_shape='ovo')\n    clf.fit(iris.data, iris.target)\n    dec = clf.decision_function(iris.data)\n    assert dec.shape[1] == len(clf.classes_) * (len(clf.classes_) - 1) / 2\n", "def test_predict_proba_shape():\n    X, y = make_blobs(n_samples=100, centers=2, n_features=2, random_state=42)\n\n    svc = svm.SVC(probability=True)\n    svc.fit(X, y)\n    proba = svc.predict_proba(X)\n\n    assert proba.shape == (100, 2), \"Probability shape mismatch\"\n    assert np.allclose(np.sum(proba, axis=1), np.ones(100)), \"Probabilities do not sum to 1\"\n", "def test_decision_function_ovo():\n    clf = svm.SVC(kernel='linear', decision_function_shape='ovo')\n    clf.fit(X, Y)\n    decision_scores = clf.decision_function(T)\n    expected_scores = np.array([[-1.0, -1.0, -1.0], [3.0, 2.0, 3.0]])\n    assert_array_equal(decision_scores, expected_scores)\n", "def test_libsvm_precomputed_kernel_shape_validation():\n    X = np.random.rand(10, 20)\n    y = np.random.choice([0, 1], 10)\n    kernel_matrix = rbf_kernel(X)\n\n    svc = svm.SVC(kernel='precomputed')\n\n    # Test with square kernel matrix\n    svc.fit(kernel_matrix, y)\n    assert svc.shape_fit_ == (10, 10)\n\n    # Test with non-square kernel matrix\n    with pytest.raises(ValueError, match=\"Precomputed matrix must be a square matrix.\"):\n        svc.fit(kernel_matrix[:5], y)\n"], "sample_1101": ["def test_schur_number_evaluation():\n    assert SchurNumber(1) == 1\n    assert SchurNumber(2) == 4\n    assert SchurNumber(3) == 13\n    assert SchurNumber(4) == 44\n    assert SchurNumber(6).lower_bound() == 364\n", "def test_schur_partition_with_symbol():\n    x = symbols('x')\n    with raises(ValueError):\n        schur_partition(x)\n", "def test_schur_number_eval():\n    assert SchurNumber(0) == 0\n    assert SchurNumber(1) == 1\n    assert SchurNumber(2) == 4\n    assert SchurNumber(3) == 13\n    assert SchurNumber(4) == 44\n    assert SchurNumber(5).lower_bound() == 364\n\n    with raises(ValueError):\n        SchurNumber(-1)\n        SchurNumber(Rational(1, 2))\n        SchurNumber(symbols('x'))\n", "def test_schur_number_evaluation():\n    assert SchurNumber(1) == 1\n    assert SchurNumber(2) == 4\n    assert SchurNumber(3) == 13\n    assert SchurNumber(4) == 44\n    assert SchurNumber(0) == 0\n    assert SchurNumber(S.Infinity) == S.Infinity\n    with raises(ValueError):\n        SchurNumber(-1)\n    with raises(ValueError):\n        SchurNumber(2.5)\n    with raises(ValueError):\n        SchurNumber(S.Pi)\n", "def test_schur_partition_errors():\n    n = symbols('n')\n    raises(ValueError, lambda: schur_partition(n))  # Test with a non-number input\n    raises(ValueError, lambda: schur_partition(0))  # Test with zero\n    raises(ValueError, lambda: schur_partition(-5))  # Test with a negative number\n"], "sample_53": ["    def test_text_input(self):\n        widget = TextInput()\n        context = widget.get_context('test_input', 'test_value', {})\n        self.assertEqual(context['widget']['type'], 'text')\n        self.assertEqual(context['widget']['value'], 'test_value')\n", "    def test_band_widget_rendering(self):\n        form = AlbumForm()\n        widget = form.fields['band'].widget\n        output = widget.render('band', None)\n        self.assertIn('data-ajax--cache=\"true\"', output)\n        self.assertIn('data-ajax--delay=\"250\"', output)\n        self.assertIn('data-ajax--url=\"/admin/admin_widgets/band/autocomplete/\"', output)\n        self.assertIn('data-theme=\"admin-autocomplete\"', output)\n        self.assertIn('class=\"my-class\"', output)\n        self.assertIn(self.empty_option, output)\n", "    def test_hidden_input(self):\n        widget = HiddenInput()\n        html = widget.render('test', 'value')\n        self.assertInHTML('<input type=\"hidden\" name=\"test\" value=\"value\">', html)\n", "    def setUpTestData(cls):\n        cls.band = Band.objects.create(name='Test Band')\n", "    def test_band_model_choice_field_widget(self):\n        form = BandModelChoiceFieldForm()\n        widget = form.fields['band'].widget\n        url = widget.get_url()\n        self.assertEqual(url, '/admin/admin_widgets/band/autocomplete/')\n        self.assertEqual(widget.format_value(Band(pk=1)), '1')\n        self.assertHTMLEqual(\n            str(widget.render('band', Band(pk=1))),\n            '<input type=\"text\" name=\"band\" value=\"1\" class=\"admin-autocomplete\" '\n            'data-admin-autocomplete-url=\"/admin/admin_widgets/band/autocomplete/\" '\n            'data-app-label=\"admin_widgets\" data-model-name=\"band\" '\n            'data-field-name=\"name\" data-theme=\"admin-autocomplete\" '\n            'data-allow-clear=\"false\">'\n        )\n"], "sample_650": ["def test_set_level(pytester: Pytester, caplog: LogCaptureFixture) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n\n            caplog.set_level(logging.INFO, 'test_logger')\n            logger = logging.getLogger('test_logger')\n            logger.info('Test message')\n            assert 'Test message' in caplog.text\n        \"\"\"\n    )\n    result = pytester.runpytest(\"-q\")\n    result.assert_outcomes(passed=1)\n", "def test_log_cli_enabled(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n\n            caplog._config.option.log_cli_level = \"INFO\"\n            caplog._config.pluginmanager.get_plugin(\"terminalreporter\")._tw = object()\n            assert caplog._log_cli_enabled()\n        \"\"\"\n    )\n\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)\n", "def test_log_cli_enabled(pytester: Pytester):\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n\n        @pytest.hookimpl\n            config.option.log_cli_level = \"INFO\"\n            config.option.log_cli = True\n        \"\"\"\n    )\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n\n            logger = logging.getLogger(\"test_logger\")\n            logger.info(\"Test log message\")\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines([\"*::test_log_cli_enabled PASSED*\"])\n    assert \"Test log message\" in result.stdout.str()\n", "def test_logging_plugin_log_cli_enabled(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n\n            caplog.set_level(logging.INFO)\n            logging.getLogger(\"test_logger\").info(\"test message\")\n            assert \"test message\" in caplog.text\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)\n    result.stdout.fnmatch_lines([\"*test message*\"])\n", "def test_log_capture_fixture_force_enable_logging(caplog: LogCaptureFixture, monkeypatch):\n    import logging\n\n    # Mock the logger object to simulate the disabled logging level\n    class MockLogger(logging.Logger):\n            super().__init__(name)\n            self.manager = MockLoggerManager()\n\n    class MockLoggerManager:\n            self.disable = logging.CRITICAL + 1\n\n    monkeypatch.setattr(logging, \"getLogger\", MockLogger)\n\n    # Test the _force_enable_logging method with a valid level\n    initial_disabled_level = caplog._force_enable_logging(logging.DEBUG, logging.getLogger(\"test\"))\n    assert logging.root.disabled == logging.DEBUG - 10\n    logging.disable(initial_disabled_level)\n\n    # Test the _force_enable_logging method with an invalid level\n    initial_disabled_level = caplog._force_enable_logging(\"INVALID_LEVEL\", logging.getLogger(\"test\"))\n    assert logging.root.disabled == logging.NOTSET\n    logging.disable(initial_disabled_level)\n"], "sample_553": ["def test_null_movie_writer_grab_frame_kwargs():\n    \"\"\"Test that NullMovieWriter.grab_frame() validates its keyword arguments.\"\"\"\n    writer = NullMovieWriter()\n    writer.setup(None, None, None)\n    with pytest.raises(TypeError, match=\"got an unexpected keyword argument\"):\n        writer.grab_frame(dpi=200)\n    with pytest.raises(TypeError, match=\"got an unexpected keyword argument\"):\n        writer.grab_frame(bbox_inches='tight')\n    with pytest.raises(TypeError, match=\"got an unexpected keyword argument\"):\n        writer.grab_frame(format='png')\n", "def test_animation_save_writer_instance_with_args(anim, tmpdir):\n    outfile = str(tmpdir / \"test_animation.mp4\")\n    writer = NullMovieWriter()\n    with pytest.raises(RuntimeError):\n        anim.save(outfile, writer=writer, fps=30, codec='h264', bitrate=1000, extra_args=['-preset', 'fast'])\n", "def test_null_movie_writer(anim):\n    \"\"\"Test NullMovieWriter.\"\"\"\n    outfile = 'test_output.mp4'\n    dpi = 100\n    args = ('arg1', 'arg2')\n\n    writer = NullMovieWriter()\n    with writer.saving(anim._fig, outfile, dpi, *args):\n        for _ in range(5):\n            writer.grab_frame()\n\n    assert writer.fig is anim._fig\n    assert writer.outfile == outfile\n    assert writer.dpi == dpi\n    assert writer.args == args\n    assert writer._count == 5\n    assert 'dpi' not in writer.savefig_kwargs\n    assert 'bbox_inches' not in writer.savefig_kwargs\n    assert 'format' not in writer.savefig_kwargs\n", "def test_save_with_null_writer():\n    \"\"\"Test the save() method with a custom NullMovieWriter.\"\"\"\n    anim = anim.param.mark(klass=NullMovieWriter, save_count=10)\n    anim.save('test.mp4', writer=anim)\n    assert anim.outfile == 'test.mp4'\n    assert anim.dpi == anim.fig.dpi\n    assert anim._count == 10\n    assert 'dpi' not in anim.savefig_kwargs\n    assert 'bbox_inches' not in anim.savefig_kwargs\n    assert 'format' not in anim.savefig_kwargs\n", "def test_null_movie_writer():\n    # This test doesn't actually write a movie.  It just exercises the\n    # NullMovieWriter class, which doesn't actually write a movie.\n    # Instead, it saves the arguments that were given to the setup()\n    # and grab_frame() methods as attributes, and counts how many\n    # times grab_frame() is called.\n\n    fig, ax = plt.subplots()\n    line, = ax.plot([], [])\n\n    ax.set_xlim(0, 10)\n    ax.set_ylim(-1, 1)\n\n        line.set_data([], [])\n        return line,\n\n        x = np.linspace(0, 10, 100)\n        y = np.sin(x + i)\n        line.set_data(x, y)\n        return line,\n\n    writer = NullMovieWriter()\n    anim = animation.FuncAnimation(fig=fig, func=animate, init_func=init, frames=5,\n                                   writer=writer)\n    plt.close(fig)\n\n    # Check that the writer was setup with the correct arguments\n    assert writer.fig is fig\n    assert writer.outfile == 'test.mp4'\n    assert writer.dpi == fig.dpi\n    assert writer.args == ()\n\n    # Check that grab_frame was called the correct number of times\n    assert writer._count == 5\n\n    # Check that savefig_kwargs were passed correctly to grab_frame\n    assert writer.savefig_kwargs == {'dpi': fig.dpi}\n"], "sample_670": ["def test_not_expression():\n        return s == 'test'\n\n    assert evaluate('not test', matcher) is False\n    assert evaluate('not other', matcher) is True\n", "def test_evaluate_with_identifiers():\n        return s == \"test\"\n\n    assert evaluate(\"test\", matcher) is True\n    assert evaluate(\"not test\", matcher) is False\n    assert evaluate(\"not not test\", matcher) is True\n", "def test_evaluate_expression_with_parentheses():\n        return s == \"test\"\n\n    assert evaluate(\"(test)\", matcher) is True\n    assert evaluate(\"(not test)\", matcher) is False\n    assert evaluate(\"(test and test)\", matcher) is True\n    assert evaluate(\"(test and not test)\", matcher) is False\n    assert evaluate(\"(test or test)\", matcher) is True\n    assert evaluate(\"(test or not test)\", matcher) is True\n", "def test_evaluate_not_expression():\n        return s == \"test\"\n\n    assert evaluate(\"not test\", matcher) == False\n    assert evaluate(\"not another\", matcher) == True\n    assert evaluate(\"not (test)\", matcher) == False\n    assert evaluate(\"not (another)\", matcher) == True\n\n    with pytest.raises(ParseError):\n        evaluate(\"not\", matcher)\n", "def test_evaluate_with_complex_expression():\n        return ident in [\"apple\", \"banana\", \"cherry\"]\n\n    assert evaluate(\"apple or banana and not cherry\", matcher) is True\n    assert evaluate(\"apple and (banana or cherry)\", matcher) is True\n    assert evaluate(\"not (apple and banana) or cherry\", matcher) is True\n    assert evaluate(\"apple and (banana and cherry)\", matcher) is False\n    assert evaluate(\"apple or (banana and cherry)\", matcher) is True\n\n    with pytest.raises(ParseError):\n        evaluate(\"apple and banana or\", matcher)\n    with pytest.raises(ParseError):\n        evaluate(\"apple and (banana or cherry\", matcher)\n    with pytest.raises(ParseError):\n        evaluate(\"apple and banana @ cherry\", matcher)\n"], "sample_1096": ["def test_idx_comparison_operations():\n    i = Idx('i', 3)\n    j = Idx('j', 4)\n    k = Idx('k', 2)\n\n    assert (i < j) == True\n    assert (j <= k) == False\n    assert (k > i) == True\n    assert (i >= k) == False\n", "def test_idx_ordering():\n    i = Idx('i', 5)\n    j = Idx('j', 5)\n    assert i <= j\n    assert not i > j\n    assert i < j + 1\n    assert not i >= j + 1\n    assert not i <= j - 1\n    assert i > j - 1\n    assert not i >= 6\n    assert i < 6\n    assert i >= 0\n    assert not i > 5\n", "def test_indexed_base_assumptions():\n    A = IndexedBase('A', real=True)\n    assert A.is_real is True\n    assert A.is_complex is False\n    assert A.is_integer is False\n\n    B = IndexedBase('B', integer=True)\n    assert B.is_real is False\n    assert B.is_complex is False\n    assert B.is_integer is True\n", "def test_indexed_derivative():\n    A = IndexedBase('A')\n    i, j, k = symbols('i j k', cls=Idx)\n    A_ij = A[i, j]\n    dA_dj = A_ij.diff(j)\n    assert dA_dj == KroneckerDelta(j, k)\n", "def test_indexed_derivative():\n    i, j = symbols('i j', cls=Idx)\n    A = IndexedBase('A')\n    B = IndexedBase('B')\n\n    # Test derivative with respect to another Indexed object\n    result = Derivative(A[i, j], B[i, j]).doit()\n    assert result == KroneckerDelta(B[i, j], A[i, j])\n\n    # Test derivative with respect to the same IndexedBase\n    result = Derivative(A[i, j], A[i, j]).doit()\n    assert result == S.One\n\n    # Test derivative with respect to a different IndexedBase\n    result = Derivative(A[i, j], B[i, j]).doit()\n    assert result == S.Zero\n\n    # Test derivative with respect to a symbol present in the indices\n    x = Symbol('x')\n    k = Idx('k', x)\n    result = Derivative(A[i, k], x).doit()\n    assert result == S.NaN  # Since x is present in the indices\n"], "sample_871": ["def test_silhouette_score_sample_size():\n    X, y = datasets.make_blobs(n_samples=100, n_features=2, centers=2, random_state=42)\n    sample_size = 50\n    score_full = silhouette_score(X, y)\n    score_sample = silhouette_score(X, y, sample_size=sample_size, random_state=42)\n    assert score_full != score_sample\n    assert 0 < len(silhouette_samples(X, y, sample_size=sample_size, random_state=42)) <= sample_size\n", "def test_calinski_harabasz_score():\n    X, y = datasets.make_blobs(n_samples=100, n_features=2, centers=3, random_state=42)\n    score = calinski_harabasz_score(X, y)\n    assert isinstance(score, float)\n    assert score > 0\n", "def test_davies_bouldin_score():\n    X, y = datasets.make_blobs(n_samples=100, centers=3, random_state=42)\n    score = davies_bouldin_score(X, y)\n    assert isinstance(score, float)\n    assert 0 <= score <= np.inf\n\n    # Test with single cluster\n    y_single_cluster = np.zeros(100)\n    score_single_cluster = davies_bouldin_score(X, y_single_cluster)\n    assert score_single_cluster == 0.0\n", "def test_silhouette_score_sparse_input():\n    # Test silhouette_score function with sparse input\n    X = csr_matrix([[0, 1, 2], [1, 0, 3], [2, 3, 0]])\n    labels = np.array([0, 0, 1])\n    score = silhouette_score(X, labels, metric='precomputed')\n    assert isinstance(score, float)\n\n    # Test with other sparse formats\n    X_csc = csc_matrix([[0, 1, 2], [1, 0, 3], [2, 3, 0]])\n    score_csc = silhouette_score(X_csc, labels, metric='precomputed')\n    assert_allclose(score, score_csc)\n\n    X_dok = dok_matrix([[0, 1, 2], [1, 0, 3], [2, 3, 0]])\n    score_dok = silhouette_score(X_dok, labels, metric='precomputed')\n    assert_allclose(score, score_dok)\n\n    X_lil = lil_matrix([[0, 1, 2], [1, 0, 3], [2, 3, 0]])\n    score_lil = silhouette_score(X_lil, labels, metric='precomputed')\n    assert_allclose(score, score_lil)\n\n    # Test with invalid sparse format\n    with pytest.raises(TypeError):\n        X_invalid = dok_matrix([[0, 1, 2], [1, 0, 3], [2, 3, 0]])\n        X_invalid.setdiag(np.zeros(3))  # Convert to CSR format\n        silhouette_score(X_invalid, labels, metric='precomputed')\n", "def test_silhouette_score_with_sparse_matrix():\n    X = csr_matrix([[0, 1], [1, 0], [2, 2]])\n    labels = [0, 1, 0]\n    expected_score = -0.2857142857142857\n    score = silhouette_score(X, labels, metric=\"precomputed\")\n    assert_allclose(score, expected_score)\n"], "sample_493": ["def test_annotation_aggregate(self):\n    authors = Author.objects.annotate(num_books=Count('book')).filter(num_books__gt=1)\n    self.assertEqual(authors.count(), 5)\n", "def test_count_with_distinct(self):\n    # Test Count with distinct\n    self.assertEqual(Book.objects.distinct().count(), len(Book.objects.distinct()))\n", "def test_greatest_and_least_aggregations(self):\n    # Test Max and Min aggregations\n    self.assertEqual(Book.objects.aggregate(max_pages=Max('pages')), {'max_pages': 1132})\n    self.assertEqual(Book.objects.aggregate(min_pages=Min('pages')), {'min_pages': 300})\n\n    # Test Greatest and Least aggregations\n    self.assertEqual(Book.objects.aggregate(greatest_rating=Greatest('rating')), {'greatest_rating': 5.0})\n    self.assertEqual(Book.objects.aggregate(least_rating=Least('rating')), {'least_rating': 3.0})\n", "    def test_subquery_aggregate(self):\n        subquery = Book.objects.filter(name__contains=OuterRef('name')).aggregate(avg_pages=Avg('pages'))\n        result = Publisher.objects.annotate(avg_book_pages=Subquery(subquery['avg_pages'])).values('name', 'avg_book_pages')\n        expected_result = [\n            {'name': 'Apress', 'avg_book_pages': 400.0},\n            {'name': 'Sams', 'avg_book_pages': 528.0},\n            {'name': 'Prentice Hall', 'avg_book_pages': 639.0},\n            {'name': 'Morgan Kaufmann', 'avg_book_pages': 350.0},\n            {'name': 'Jonno\\'s House of Books', 'avg_book_pages': None},\n        ]\n        self.assertEqual(list(result), expected_result)\n", "    def test_avg(self):\n        self.assertAlmostEqual(Author.objects.aggregate(Avg(\"age\"))[\"age__avg\"], 35.8)\n"], "sample_893": ["def test_export_text_regressor():\n    regressor = DecisionTreeRegressor(random_state=0, max_depth=2)\n    regressor = regressor.fit(X, y)\n    r = export_text(regressor, feature_names=[\"feature_1\", \"feature_2\"])\n    expected_output = dedent(\n        \"\"\"\\\n        |--- feature_1 <= -0.50\n        |   |--- value: [-1.00]\n        |--- feature_1 >  -0.50\n        |   |--- feature_2 <= 1.25\n        |   |   |--- value: [1.00]\n        |   |--- feature_2 >  1.25\n        |   |   |--- value: [1.00]\n        \"\"\"\n    )\n    assert r == expected_output\n", "def test_plot_tree_with_custom_feature_names():\n    clf = DecisionTreeClassifier()\n    clf.fit(X, y)\n\n    feature_names = [\"feature1\", \"feature2\"]\n    plot_tree(clf, feature_names=feature_names)\n\n    # Add assertions to verify that the feature names are used in the plot\n", "def test_plot_tree_sample_weight(tree_type):\n    if tree_type == \"classifier\":\n        tree = DecisionTreeClassifier(random_state=0)\n    else:\n        tree = DecisionTreeRegressor(random_state=0)\n    tree = tree.fit(X, y, sample_weight=w)\n    plot_tree(tree)\n", "def test_export_text_decision_tree_classifier():\n    clf = DecisionTreeClassifier(random_state=0)\n    clf = clf.fit(X, y)\n    report = export_text(clf, feature_names=['feature_1', 'feature_2'])\n    assert \"feature_1 <= 0.50\" in report\n    assert \"class: -1\" in report\n", "def test_export_text_regressor():\n    rng = RandomState(42)\n    X_reg = rng.random((100, 4))\n    y_reg = rng.random(100)\n    dt_reg = DecisionTreeRegressor(max_depth=3, random_state=42)\n    dt_reg.fit(X_reg, y_reg)\n    report = export_text(dt_reg, feature_names=[\"feature_1\", \"feature_2\", \"feature_3\", \"feature_4\"])\n    assert \"feature_1\" in report\n    assert \"feature_2\" in report\n    assert \"feature_3\" in report\n    assert \"feature_4\" in report\n    assert \"value:\" in report\n"], "sample_444": ["def test_manifest_file_loading(self):\n    manifest_content = {\n        \"paths\": {\"example.css\": \"example.12345678.css\"},\n        \"version\": \"1.1\",\n        \"hash\": \"abcdef123456\",\n    }\n    with mock.patch.object(\n        storage.staticfiles_storage, \"read_manifest\", return_value=json.dumps(manifest_content)\n    ):\n        storage.staticfiles_storage.load_manifest()\n        self.assertEqual(storage.staticfiles_storage.hashed_files, {\"example.css\": \"example.12345678.css\"})\n        self.assertEqual(storage.staticfiles_storage.manifest_hash, \"abcdef123456\")\n    self.assertPostCondition()\n", "def test_hashed_name_with_fragment(self):\n    name = \"/path/to/file.css?#fragment\"\n    content = StringIO(\"body { background: url('/path/to/image.png?#fragment'); }\")\n    expected_hashed_name = \"/path/to/file.css?#fragment\"  # Fragment should be preserved\n\n    hashed_name = storage.staticfiles_storage.hashed_name(name, content)\n    self.assertEqual(hashed_name, expected_hashed_name)\n\n    self.assertPostCondition()\n", "def test_hashed_name(self):\n    test_file = \"test.css\"\n    test_content = \"body { background: url('image.jpg'); }\"\n\n    with tempfile.NamedTemporaryFile(mode='w+', delete=False) as f:\n        f.write(test_content)\n        f.seek(0)\n        hashed_name = storage.staticfiles_storage.hashed_name(test_file, f)\n\n    expected_hashed_name = \"test.%s.css\" % storage.staticfiles_storage.file_hash(test_file, StringIO(test_content))\n    self.assertEqual(hashed_name, expected_hashed_name)\n\n    os.remove(f.name)  # Clean up\n", "def test_manifest_strict_true_raises_error_for_missing_entry(self):\n    # Arrange\n    self.settings(STATICFILES_STORAGE='django.contrib.staticfiles.storage.ManifestStaticFilesStorage')\n    self.settings(MANIFEST_STRICT=True)\n    self.create_file('missing_file.css', 'body { background: red; }')\n    self.set_up_staticfiles_setting('missing_file.css')\n\n    # Act and Assert\n    with self.assertRaisesMessage(ValueError, \"Missing staticfiles manifest entry for 'missing_file.css'\"):\n        finders.find('missing_file.css')\n\n    # Assert post conditions\n    self.assertPostCondition()\n", "def test_url_converter(self):\n    # Test the url_converter method\n    name = \"app/static/style.css\"\n    hashed_files = {\"app/static/img.jpg\": \"app/static/img.c6578b962038.jpg\"}\n    template = \"url('%(url)s')\"\n    converter = storage.staticfiles_storage.url_converter(name, hashed_files, template)\n    matched_url = \"url('img.jpg')\"\n    result = converter(matched_url)\n    self.assertEqual(result, \"url('img.c6578b962038.jpg')\")\n    self.assertPostCondition()\n"], "sample_668": ["def test_node_use_from_parent():\n    with pytest.warns(deprecated.NODE_USE_FROM_PARENT) as record:\n        node = nodes.Item(\"name\")\n    assert str(record[0].message) == \"Direct construction of Item has been deprecated, please use Item.from_parent.\"\n", "def test_node_use_from_parent():\n    with pytest.deprecated_call(deprecated.NODE_USE_FROM_PARENT):\n        node = nodes.Node(\"name\")\n    assert isinstance(node, nodes.Node)\n    assert node.name == \"name\"\n", "def test_node_construction():\n    with pytest.warns(deprecated.NODE_USE_FROM_PARENT.get_warning_class(), match=r\"Direct construction of Node\"):\n        node = nodes.Node(\"name\", \"parent\", \"nodeid\")\n", "def test_node_use_from_parent(recwarn):\n    node = nodes.Item(\"test_name\")\n    with pytest.deprecated_call(match=\"Direct construction of Item has been deprecated\"):\n        nodes.Item.from_parent(node, \"new_test_name\")\n    assert len(recwarn) == 1\n    assert issubclass(recwarn[0].category, deprecated.PytestDeprecationWarning)\n    assert \"nodes.Item\" in str(recwarn[0].message)\n", "def test_node_use_from_parent():\n    with pytest.warns(deprecated.NODE_USE_FROM_PARENT):\n        node = nodes.Item(\"name\")\n    assert isinstance(node, nodes.Item)\n"], "sample_718": ["    def test_not_invariant_predict(self):\n        estimator = NotInvariantPredict()\n        X = np.array([[1], [2], [3]])\n        estimator.fit(X, X)\n        pred_full = estimator.predict(X)\n        pred_batch = np.array([estimator.predict(X[i:i+1]) for i in range(len(X))])\n        np.testing.assert_array_equal(pred_full, pred_batch, err_msg=\"predict is not invariant when applied to a subset.\")\n", "    def test_correct_not_fitted_error_classifier(self):\n        e = CorrectNotFittedErrorClassifier()\n        with self.assertRaises(CorrectNotFittedError):\n            e.predict([[1, 2]])\n", "    def test_check_estimator(self):\n        check_estimator(CorrectNotFittedErrorClassifier)\n", "    def test_check_estimator_sparse_data(self):\n        estimator = NMF()\n        check_estimator_sparse_data(\"NMF\", estimator)\n", "    def test_not_invariant_predict(self):\n        X = np.array([[1, 2], [3, 4], [5, 6]])\n        y = np.array([0, 1, 0])\n        estimator = NotInvariantPredict()\n        estimator.fit(X, y)\n        result_full = estimator.predict(X)\n        result_by_batch = [estimator.predict(batch.reshape(1, -1)) for batch in X]\n        np.testing.assert_allclose(result_full, np.array(result_by_batch), err_msg=\"predict is not invariant when applied to a subset.\")\n"], "sample_280": ["    def test_count_with_default(self):\n        result = Author.objects.aggregate(num_authors=Count('id', default=0))\n        self.assertEqual(result['num_authors'], Author.objects.count())\n", "def test_variance(self):\n    avg_age = Author.objects.aggregate(avg_age=Avg('age'))['avg_age']\n    variance = Author.objects.aggregate(variance=Variance('age'))['variance']\n    expected_variance = Author.objects.aggregate(sum_squares=Sum((F('age') - avg_age) ** 2))['sum_squares'] / Author.objects.count()\n    self.assertAlmostEqual(variance, expected_variance)\n\n    # Test variance with filter\n    young_authors_variance = Author.objects.filter(age__lt=40).aggregate(variance=Variance('age'))['variance']\n    self.assertTrue(young_authors_variance <= variance)\n", "def test_aggregate_with_default(self):\n    # Test Aggregate with default\n    qs = Author.objects.all()\n    qs = qs.annotate(num_books=Count('books', default=0))\n    for author in qs:\n        self.assertIsNotNone(author.num_books)\n    self.assertEqual(qs.filter(num_books=0).count(), 1)  # Jonno's House of Books doesn't have any books\n", "def test_stddev_and_variance(self):\n    # Test StdDev\n    avg_rating = Book.objects.aggregate(stddev_rating=StdDev('rating'))['stddev_rating']\n    self.assertIsNotNone(avg_rating)\n    self.assertIsInstance(avg_rating, float)\n\n    # Test Variance\n    avg_pages = Book.objects.aggregate(variance_pages=Variance('pages'))['variance_pages']\n    self.assertIsNotNone(avg_pages)\n    self.assertIsInstance(avg_pages, float)\n\n    # Test StdDev with sample\n    avg_rating_sample = Book.objects.aggregate(stddev_rating=StdDev('rating', sample=True))['stddev_rating']\n    self.assertIsNotNone(avg_rating_sample)\n    self.assertIsInstance(avg_rating_sample, float)\n\n    # Test Variance with sample\n    avg_pages_sample = Book.objects.aggregate(variance_pages=Variance('pages', sample=True))['variance_pages']\n    self.assertIsNotNone(avg_pages_sample)\n    self.assertIsInstance(avg_pages_sample, float)\n", "def test_aggregate_functions(self):\n    # Test Count function\n    self.assertEqual(Author.objects.count(), 9)\n    self.assertEqual(Author.objects.filter(age=29).count(), 2)\n    self.assertEqual(Book.objects.count(), 6)\n    self.assertEqual(Book.objects.filter(pages__gt=400).count(), 2)\n\n    # Test Avg function\n    self.assertEqual(Author.objects.aggregate(Avg('age'))['age__avg'], 37.22222222222222)\n\n    # Test Min and Max functions\n    self.assertEqual(Book.objects.aggregate(Min('price'))['price__min'], Decimal('23.09'))\n    self.assertEqual(Book.objects.aggregate(Max('rating'))['rating__max'], 4.5)\n\n    # Test Sum function\n    self.assertEqual(Publisher.objects.aggregate(Sum('num_awards'))['num_awards__sum'], 20)\n\n    # Test StdDev and Variance functions\n    self.assertEqual(Book.objects.aggregate(StdDev('pages'))['pages__stddev'], Approximate(149.074, 0.001))\n    self.assertEqual(Book.objects.aggregate(Variance('pages'))['pages__variance'], Approximate(222.229, 0.001))\n"], "sample_949": ["def test_default_man_pages(app):\n    config = Config({\n        'root_doc': 'index',\n        'project': 'Test Project',\n        'release': '1.0',\n        'author': 'Test Author',\n    })\n    result = default_man_pages(config)\n    expected_result = [('index', 'testproject', 'Test Project 1.0', ['Test Author'], 1)]\n    assert result == expected_result\n", "def test_default_man_pages(app):\n    config = Config({\n        'project': 'Test Project',\n        'release': '1.0',\n        'author': 'Test Author',\n        'root_doc': 'index'\n    })\n    result = default_man_pages(config)\n    assert result == [('index', 'testproject', 'Test Project 1.0', ['Test Author'], 1)]\n", "def test_default_man_pages(app):\n    config = Config({\n        'project': 'Test Project',\n        'release': '1.0',\n        'author': 'Test Author',\n        'root_doc': 'index'\n    })\n    result = default_man_pages(config)\n    assert len(result) == 1\n    docname, name, description, authors, section = result[0]\n    assert docname == 'index'\n    assert name == 'TestProject'\n    assert description == 'Test Project 1.0'\n    assert authors == ['Test Author']\n    assert section == 1\n", "def test_default_man_pages(app):\n    config = Config({\n        'project': 'Test Project',\n        'release': '1.0',\n        'root_doc': 'index',\n        'author': 'Test Author'\n    })\n    man_pages = default_man_pages(config)\n    assert len(man_pages) == 1\n    assert man_pages[0][0] == 'index'\n    assert man_pages[0][1] == 'test-project'\n    assert man_pages[0][2] == 'Test Project 1.0'\n    assert man_pages[0][3] == ['Test Author']\n    assert man_pages[0][4] == 1\n", "def test_default_man_pages(app, monkeypatch):\n    config = Config({\n        'project': 'TestProject',\n        'release': '1.0',\n        'root_doc': 'index',\n        'author': 'TestAuthor'\n    })\n    monkeypatch.setattr(\"sphinx.builders.manpage.make_filename_from_project\", lambda x: 'testproject')\n    result = default_man_pages(config)\n    assert result == [('index', 'testproject', 'TestProject 1.0', ['TestAuthor'], 1)]\n"], "sample_367": ["    def test_fully_decorated_metadata(self):\n        \"\"\"Test that decorators preserve metadata.\"\"\"\n        self.assertEqual(fully_decorated.__doc__, \"Expected __doc__\")\n        self.assertEqual(fully_decorated.anything, \"Expected __dict__\")\n", "def test_fully_decorated_function(self):\n    request = HttpRequest()\n    response = fully_decorated(request)\n    self.assertIsInstance(response, HttpResponse)\n    self.assertEqual(response.content, b'<html><body>dummy</body></html>')\n    self.assertEqual(response['Cache-Control'], 'private')\n    self.assertEqual(response['Expires'], '-1')\n    self.assertEqual(response['Pragma'], 'no-cache')\n    self.assertEqual(response['Vary'], 'Cookie, Accept-language')\n    self.assertFalse(hasattr(response, 'anything'))\n", "    def test_decorated_function_metadata(self):\n        \"\"\"\n        Test that the decorated function retains its docstring and other attributes.\n        \"\"\"\n        self.assertEqual(fully_decorated.__doc__, \"Expected __doc__\")\n        self.assertEqual(fully_decorated.anything, \"Expected __dict__\")\n", "    def test_cache_control(self):\n        request = HttpRequest()\n        response = cache_control(private=True)(fully_decorated)(request)\n        self.assertEqual(response['Cache-Control'], 'private')\n", "    def test_fully_decorated_docstring_and_attributes(self):\n        \"\"\"\n        Tests that the docstring and other attributes are preserved when decorating a function.\n        \"\"\"\n        self.assertEqual(fully_decorated.__doc__, \"Expected __doc__\")\n        self.assertEqual(fully_decorated.anything, \"Expected __dict__\")\n"], "sample_713": ["def test_ridge_regression_multitarget():\n    X, y = make_regression(n_samples=100, n_features=5, n_targets=3, random_state=0)\n    alpha = [1.0, 2.0, 3.0]\n    coef = ridge_regression(X, y, alpha)\n    assert_true(coef.shape == (3, 5))\n    assert_almost_equal(coef[0], ridge_regression(X, y[:, 0], alpha[0]))\n    assert_almost_equal(coef[1], ridge_regression(X, y[:, 1], alpha[1]))\n    assert_almost_equal(coef[2], ridge_regression(X, y[:, 2], alpha[2]))\n", "compilation error", "def test_ridge_regression_multi_target():\n    X = np.array([[1, 1], [2, 2], [3, 3]])\n    y = np.array([[1, 2], [3, 4], [5, 6]])\n    alpha = [1, 2]\n\n    coef = ridge_regression(X, y, alpha=alpha)\n    assert_array_almost_equal(coef, np.array([[0.40909091, 0.81818182],\n                                              [0.27272727, 0.54545455]]))\n\n    coef_svd = ridge_regression(X, y, alpha=alpha, solver='svd')\n    assert_array_almost_equal(coef_svd, coef)\n", "def test_ridge_regression_multidimensional_y():\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([[1, 2], [3, 4], [5, 6]])\n    alpha = 1.0\n    coef = ridge_regression(X, y, alpha)\n    assert_array_almost_equal(coef, np.array([[0.110651, 0.110651], [0.110651, 0.110651]]))\n", "def test_ridge_classifier_cv_with_class_weight():\n    X, y = make_multilabel_classification(random_state=0)\n    y = y[:, 0]  # reduce to binary classification\n    for class_weight in [None, 'balanced', {0: 0.5, 1: 1.5}]:\n        ridge_cv = RidgeClassifierCV(class_weight=class_weight)\n        ridge_cv.fit(X, y)\n        assert_true(hasattr(ridge_cv, 'coef_'))\n"], "sample_281": ["    def test_permission_denied_for_missing_params(self):\n        request = self.factory.get(self.url)\n        request.user = self.user\n        response = self.view(request)\n        self.assertEqual(response.status_code, 403)\n", "    def test_permission_denied_if_missing_parameters(self):\n        request = self.factory.get(self.url)\n        request.user = self.user\n        response = self.view(request)\n        self.assertEqual(response.status_code, 403)\n", "    def test_queryset_filtered_by_limit_choices_to(self):\n        question = Question.objects.create(question_text='Test question')\n        author = Author.objects.create(name='Test author')\n        Authorship.objects.create(question=question, author=author)\n\n        request = self.factory.get(self.url, data={'term': 'Test', **self.opts})\n        request.user = self.user\n        view = AutocompleteJsonView(admin_site=site)\n        view.process_request(request)\n        view.object_list = view.get_queryset()\n\n        self.assertEqual(view.object_list.count(), 1)\n        self.assertEqual(view.object_list.first(), question)\n", "def test_autocomplete_json_view_permission_denied(self):\n    \"\"\"Test the AutocompleteJsonView raises PermissionDenied if the user has no view permission.\"\"\"\n    request = self.factory.get(self.url, data=self.opts)\n    request.user = self.user\n\n    # Remove view permission for the user\n    view_permission = Permission.objects.get(\n        content_type=ContentType.objects.get_for_model(Answer),\n        codename='view_answer',\n    )\n    self.user.user_permissions.remove(view_permission)\n\n    with self.assertRaises(PermissionDenied):\n        self.view(request)\n", "def test_search_query(self):\n    question = Question.objects.create(question_text=\"Why?\")\n    for i in range(30):\n        Answer.objects.create(question=question, answer_text=\"Answer {}\".format(i))\n\n    request = self.factory.get(self.url, {**self.opts, 'term': \"Answer 1\"})\n    request.user = self.user\n    response = AutocompleteJsonView.as_view(**self.as_view_args)(request)\n\n    self.assertEqual(response.status_code, 200)\n    data = json.loads(response.content)\n    self.assertEqual(len(data['results']), PAGINATOR_SIZE)\n    self.assertEqual(data['results'][0]['text'], \"Answer 1\")\n    self.assertEqual(data['pagination']['more'], True)\n"], "sample_905": ["def test_getdoc_for_inherited_method():\n    class Base:\n            \"\"\"Base method docstring.\"\"\"\n            pass\n\n    class Derived(Base):\n        pass\n\n    assert inspect.getdoc(Derived.method, allow_inherited=True) == \"Base method docstring.\"\n", "def test_getdoc_for_partialmethod():\n    class TestClass:\n            \"\"\"Test method docstring.\"\"\"\n            pass\n\n    test_instance = TestClass()\n    partial_method = functools.partialmethod(test_instance.test_method, b='partial')\n    doc = inspect.getdoc(partial_method)\n    assert doc == \"Test method docstring.\"\n", "def test_unwrap_all():\n            return func(*args, **kwargs)\n        return wrapper\n\n    @wrapped\n        \"\"\"This is a test function.\"\"\"\n        pass\n\n    assert inspect.unwrap_all(test_func) is test_func.__wrapped__\n", "def test_getdoc_allow_inherited():\n    class Base:\n            \"\"\"Base class method docstring.\"\"\"\n            pass\n\n    class Derived(Base):\n        pass\n\n    assert inspect.getdoc(Derived.method, allow_inherited=True) == \"Base class method docstring.\"\n", "def test_getdoc_with_inherited_docstring():\n    class BaseClass:\n            \"\"\"Base method docstring.\"\"\"\n            pass\n\n    class SubClass(BaseClass):\n        pass\n\n    assert inspect.getdoc(SubClass.method, allow_inherited=True) == \"Base method docstring.\"\n"], "sample_683": ["def test_resume_global_capture(self, method):\n    capman = CaptureManager(method)\n    capman.start_global_capturing()\n    capman.suspend_global_capture()\n    capman.resume_global_capture()\n    assert capman.is_globally_capturing()\n    capman.stop_global_capturing()\n", "    def test_activate_deactivate_fixture(self, capsys):\n        capmanager = CaptureManager(\"fd\")\n        capmanager.set_fixture(capsys)\n\n        # Ensure fixture is activated\n        capmanager.activate_fixture()\n        assert capmanager._capture_fixture is not None\n\n        # Ensure fixture is deactivated\n        capmanager.deactivate_fixture()\n        assert capmanager._capture_fixture._capture is None\n", "def test_start_global_capturing(self, method):\n    capman = CaptureManager(method)\n    capman.start_global_capturing()\n    assert capman._global_capturing is not None\n    assert capman._global_capturing._state == \"started\"\n", "def capsysbinary_fixture(request):\n    return request.getfixturevalue(\"capsysbinary\")\n", "def test_capture_manager_resume_fixture(self, method):\n    capture_manager = CaptureManager(method)\n    capture_fixture = capture_manager._capture_fixture\n    # Add some setup for capture_fixture here\n\n    # Call the method being tested\n    capture_manager.resume_fixture()\n\n    # Add assertions to check the expected behavior\n    # For example, you might check that the fixture's _resume() method was called\n    assert capture_fixture._capture is not None, \"Fixture capture should be started\"\n"], "sample_1054": ["def test_complexregion_intersect():\n    a = Interval(2, 3)\n    b = Interval(4, 6)\n    c = Interval(1, 8)\n    c1 = ComplexRegion(a*b)\n    c2 = ComplexRegion(Union(a*b, b*c))\n    intersection = c1.intersect(c2)\n    assert intersection == c1\n", "def test_complexregion_contains():\n    c = ComplexRegion(Interval(2, 3) * Interval(4, 6))\n    assert (2.5 + 4.5*I in c) == True\n    assert (2.5 + 6.5*I in c) == False\n\n    r = Interval(0, 1)\n    theta = Interval(0, 2*S.Pi)\n    c2 = ComplexRegion(r*theta, polar=True)\n    assert (0.5 + 0.5*I in c2) == True\n    assert (1 + 2*I in c2) == False\n", "def test_complex_region_intersection():\n    a = Interval(2, 3)\n    b = Interval(4, 5)\n    c = Interval(1, 7)\n    C1 = ComplexRegion(a*b)\n    C2 = ComplexRegion(Union(a*b, b*c))\n    intersection = C1.intersect(C2)\n    assert intersection == C1\n", "def test_ComplexRegion_init():\n    a = Interval(2, 3)\n    b = Interval(4, 6)\n    c = ComplexRegion(a*b)\n    assert c.sets == a*b\n    assert c.polar is False\n\n    d = Interval(0, 1)\n    e = Interval(0, 2*pi)\n    f = ComplexRegion(d*e, polar=True)\n    assert f.sets == d*e\n    assert f.polar is True\n\n    # Test for invalid input\n    with raises(ValueError):\n        ComplexRegion(a*b, polar=\"invalid\")\n", "def test_complex_region_polar_intersection():\n    r1 = Interval(0, 1)\n    theta1 = Interval(0, 2 * pi)\n    r2 = Interval(0, 1)\n    theta2 = Interval(0, pi)\n\n    c1 = ComplexRegion(r1 * theta1, polar=True)\n    c2 = ComplexRegion(r2 * theta2, polar=True)\n    intersection = c1.intersect(c2)\n\n    assert intersection == c2\n    assert intersection.polar is True\n    assert intersection.a_interval == r2\n    assert intersection.b_interval == theta2\n"], "sample_1182": ["def test_MpmathPrinter_loggamma():\n    expr = loggamma(x)\n    result = MpmathPrinter().doprint(expr)\n    assert result == \"mpmath.loggamma(x)\"\n", "def test_loggamma():\n    expr = loggamma(x)\n    result = SymPyPrinter().doprint(expr)\n    assert result == \"sympy.loggamma(x)\"\n", "def test_print_loggamma():\n    p = PythonCodePrinter()\n    expr = loggamma(x)\n    result = p.doprint(expr)\n    assert result == \"lgamma(x)\"\n", "def test_print_loggamma():\n    assert pycode(loggamma(x)) == \"mpmath.loggamma(%s)\" % pycode(x)\n", "def test_loggamma_printing():\n    expr = loggamma(x)\n    printed = pycode(expr)\n    expected = \"loggamma(x)\"\n    assert printed == expected\n"], "sample_1160": ["def test_intersection_sets_range_range():\n    a = Range(1, 10)\n    b = Range(5, 15)\n    assert intersection_sets(a, b) == Range(5, 10)\n", "def test_intersection_sets_range_range():\n    a = Range(2, 8, 2)\n    b = Range(3, 10, 3)\n    expected = Range(4, 8, 4)\n    assert intersection_sets(a, b) == expected\n", "def test_intersection_sets_naturals_rationals():\n    a = S.Naturals\n    b = S.Rationals\n    result = intersection_sets(a, b)\n    assert result == a\n", "def test_intersection_sets_complex_region_rationals():\n    c1 = ComplexRegion(Interval(0, 1), Interval(0, pi))\n    c2 = S.Rationals\n    result = intersection_sets(c1, c2)\n    assert result == ComplexRegion(Interval(0, 1, left_open=True, right_open=True), Interval(0, pi, left_open=True, right_open=True))\n", "def test_intersection_sets_range_range():\n    a = Range(1, 10)\n    b = Range(5, 15)\n    expected_result = Range(5, 10)\n    assert intersection_sets(a, b) == expected_result\n"], "sample_1006": ["def test_binomial_with_polynomial():\n    x = symbols('x')\n    p = Poly(x**3, x)\n    assert binomial(p, 2) == Poly(x**6 + 3*x**4 + 3*x**2, x, domain='ZZ')\n", "def test_subfactorial_rewrite_as_uppergamma():\n    n = Symbol('n', integer=True, positive=True)\n    assert subfactorial(n).rewrite(uppergamma) == uppergamma(n + 1, -1)/S.Exp1\n", "def test_binomial_symmetry():\n    n, k = symbols('n k', integer=True, positive=True)\n    assert binomial(n, k) == binomial(n, n - k)\n", "def test_binomial_expand_func():\n    n = Symbol('n', integer=True)\n    assert expand_func(binomial(n, 3)) == n * (n - 2) * (n - 1) / 6\n", "def test_binomial_symbolic_n():\n    n = Symbol('n', integer=True, positive=True)\n    k = 3\n    result = binomial(n, k).expand(func=True)\n    expected = n**3/6 - n**2/2 + n/3\n    assert result == expected\n"], "sample_208": ["def test_author_name_deconstructible_nested_changes(self):\n    changes = self.get_changes([self.author_name_nested_deconstructible_1], [self.author_name_nested_deconstructible_2])\n    self.assertNumberMigrations(changes, \"testapp\", 0)\n\n    changes = self.get_changes([self.author_name_nested_deconstructible_1], [self.author_name_nested_deconstructible_changed_arg])\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterField\"])\n    self.assertOperationFieldAttributes(changes, \"testapp\", 0, 0, default=DeconstructibleObject(\n        DeconstructibleObject(1),\n        (DeconstructibleObject('t1'), DeconstructibleObject('t2-changed'),),\n        a=DeconstructibleObject('A'),\n        b=DeconstructibleObject(B=DeconstructibleObject('c')),\n    ))\n\n    changes = self.get_changes([self.author_name_nested_deconstructible_1], [self.author_name_nested_deconstructible_extra_arg])\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterField\"])\n    self.assertOperationFieldAttributes(changes, \"testapp\", 0, 0, default=DeconstructibleObject(\n        DeconstructibleObject(1),\n        (DeconstructibleObject('t1'), DeconstructibleObject('t2'),),\n        None,\n        a=DeconstructibleObject('A'),\n        b=DeconstructibleObject(B=DeconstructibleObject('c')),\n    ))\n\n    changes = self.get_changes([self.author_name_nested_deconstructible_1], [self.author_name_nested_deconstructible_changed_kwarg])\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"Alter", "def test_generate_altered_order_with_respect_to(self):\n    before_states = [self.author_with_book]\n    after_states = [self.author_with_book_order_wrt]\n    changes = self.get_changes(before_states, after_states)\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['AlterOrderWithRespectTo'])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name='Author', order_with_respect_to='book')\n", "def test_deconstructible_objects(self):\n    questioner = MigrationQuestioner(specified_apps=[\"testapp\"])\n    questioner.ask_not_null_addition = lambda field_name, model_name: models.CharField(max_length=200)\n    changes = self.get_changes(\n        [self.author_empty],\n        [self.author_name_deconstructible_1],\n        questioner=questioner,\n    )\n    self.assertOperationFieldAttributes(changes, \"testapp\", 0, 0, default='testapp.DeconstructibleObject()')\n\n    changes = self.get_changes(\n        [self.author_name_deconstructible_1],\n        [self.author_name_deconstructible_2],\n        questioner=questioner,\n    )\n    self.assertOperationFieldAttributes(changes, \"testapp\", 0, 0, preserve_default=True)\n\n    changes = self.get_changes(\n        [self.author_name_deconstructible_1],\n        [self.author_name_deconstructible_3],\n        questioner=questioner,\n    )\n    self.assertOperationFieldAttributes(changes, \"testapp\", 0, 0, default=models.IntegerField())\n\n    changes = self.get_changes(\n        [self.author_name_deconstructible_3],\n        [self.author_name_deconstructible_4],\n        questioner=questioner,\n    )\n    self.assertOperationFieldAttributes(changes, \"testapp\", 0, 0, preserve_default=True)\n\n    changes = self.get_changes(\n        [self.author_name_deconstructible_1],\n        [self.author_name_deconstructible_list_1],\n        questioner=questioner,\n    )\n    self.assertOperationFieldAttributes(changes, \"testapp\", 0, 0, default='[testapp.DeconstructibleObject(), 123]')\n\n    changes = self.get_changes(\n        [self.author_name_deconstructible_list_1],\n        [self.author_name", "def test_renamed_fields_with_swappable_relations(self):\n    before_states = [\n        self.author_with_book,\n        self.book,\n        self.custom_user,\n    ]\n    after_states = [\n        self.author_with_book_order_wrt,\n        self.book_with_field_and_author_renamed,\n        self.custom_user,\n    ]\n    questioner = MigrationQuestioner(specified_apps={\"otherapp\"})\n    changes = self.get_changes(before_states, after_states, questioner)\n    self.assertNumberMigrations(changes, \"otherapp\", 1)\n    self.assertOperationTypes(changes, \"otherapp\", 0, [\"RenameField\", \"AlterField\"])\n    self.assertOperationAttributes(changes, \"otherapp\", 0, 0, name=\"author\", new_name=\"writer\")\n    self.assertOperationFieldAttributes(changes, \"otherapp\", 0, 1, name=\"writer\", to=\"thirdapp.CustomUser\")\n", "def test_generate_altered_fields_with_deconstructible_objects(self):\n    questioner = MigrationQuestioner(specified_apps={'testapp'})\n    questioner.ask_not_null_alteration = lambda field_name, model_name: models.NOT_PROVIDED\n    changes = self.get_changes(\n        [self.author_name_deconstructible_1],\n        [self.author_name_deconstructible_2],\n        questioner,\n    )\n    self.assertOperationTypes(changes, 'testapp', 0, ['AlterField'])\n    self.assertOperationFieldAttributes(changes, 'testapp', 0, 0, default=DeconstructibleObject())\n"], "sample_233": ["    def test_check_token_with_different_algorithms(self):\n        user = User.objects.create_user(username='testuser', email='testuser@example.com', password='testpassword')\n        now = datetime.now()\n        token_generator = MockedPasswordResetTokenGenerator(now)\n        token_generator.algorithm = 'sha256'\n        token = token_generator.make_token(user)\n\n        # Check token with default algorithm (sha1)\n        self.assertFalse(token_generator.check_token(user, token))\n\n        # Check token with correct algorithm (sha256)\n        token_generator.algorithm = 'sha256'\n        self.assertTrue(token_generator.check_token(user, token))\n\n        # Check token with incorrect algorithm (sha512)\n        token_generator.algorithm = 'sha512'\n        self.assertFalse(token_generator.check_token(user, token))\n", "    def test_token_expiration(self):\n        user = User.objects.create_user(username='testuser', email='test@example.com', password='testpassword')\n        now = datetime.now()\n        token_generator = MockedPasswordResetTokenGenerator(now)\n\n        # Generate a token\n        token = token_generator.make_token(user)\n\n        # Check token is valid\n        self.assertTrue(token_generator.check_token(user, token))\n\n        # Move time forward beyond the expiration limit\n        future = now + timedelta(seconds=settings.PASSWORD_RESET_TIMEOUT + 1)\n        token_generator = MockedPasswordResetTokenGenerator(future)\n\n        # Check token is no longer valid\n        self.assertFalse(token_generator.check_token(user, token))\n", "    def test_check_token_with_legacy_algorithm(self):\n        user = User.objects.create_user(username='testuser', email='test@test.com', password='testpassword')\n        user.last_login = datetime.now() - timedelta(days=1)\n        user.save()\n\n        now = datetime.now()\n        token_generator = MockedPasswordResetTokenGenerator(now)\n        token_generator.algorithm = 'sha1'  # Legacy algorithm\n\n        token = token_generator.make_token(user)\n        self.assertTrue(token_generator.check_token(user, token))\n\n        # Check that the token is invalidated if the algorithm is changed\n        token_generator.algorithm = 'sha256'\n        self.assertFalse(token_generator.check_token(user, token))\n", "    def test_check_token_valid(self):\n        user = User.objects.create_user(username='testuser', password='testpassword', email='test@example.com')\n        now = datetime.now()\n        token_generator = MockedPasswordResetTokenGenerator(now)\n        token = token_generator.make_token(user)\n        self.assertTrue(token_generator.check_token(user, token))\n", "    def test_token_validity(self):\n        user = User.objects.create_user(username='testuser', email='test@example.com', password='password123')\n        now = datetime.now()\n        token_generator = MockedPasswordResetTokenGenerator(now)\n        token = token_generator.make_token(user)\n\n        # Test token validity\n        self.assertTrue(token_generator.check_token(user, token))\n\n        # Test token invalidity after timeout\n        later = now + timedelta(seconds=settings.PASSWORD_RESET_TIMEOUT + 1)\n        later_token_generator = MockedPasswordResetTokenGenerator(later)\n        self.assertFalse(later_token_generator.check_token(user, token))\n\n        # Test token invalidity for a different user\n        different_user = User.objects.create_user(username='otheruser', email='other@example.com', password='password456')\n        self.assertFalse(token_generator.check_token(different_user, token))\n\n        # Test token invalidity for a user with a different email\n        user.email = 'different@example.com'\n        user.save()\n        self.assertFalse(token_generator.check_token(user, token))\n\n        # Test token invalidity for a user with a different password\n        user.set_password('newpassword789')\n        user.save()\n        self.assertFalse(token_generator.check_token(user, token))\n\n        # Test token invalidity for a user with a different last_login\n        user.last_login = now - timedelta(days=1)\n        user.save()\n        self.assertFalse(token_generator.check_token(user, token))\n"], "sample_496": ["def test_command_error_handling(self):\n    class FailingCommand(BaseCommand):\n            raise CommandError(\"This command fails\")\n\n    with self.assertRaises(CommandError) as cm:\n        call_command(\"failingcommand\")\n    self.assertEqual(str(cm.exception), \"This command fails\")\n", "def test_django_admin_runserver_help(self):\n    out = StringIO()\n    call_command('runserver', '--help', stdout=out)\n    help_text = out.getvalue()\n    self.assertIn('Usage: manage.py runserver [options] [port or ipaddr:port]', help_text)\n    self.assertIn('Options:', help_text)\n    self.assertIn('--noreload', help_text)\n    self.assertIn('--nothreading', help_text)\n    self.assertIn('--adminmedia=<path>', help_text)\n", "def test_command_error_handling(self):\n    out = StringIO()\n    err = StringIO()\n    with mock.patch('sys.stdout', new=out):\n        with mock.patch('sys.stderr', new=err):\n            with self.assertRaises(SystemExit) as cm:\n                call_command('check', traceback=False, settings='nonexistent')\n            self.assertEqual(cm.exception.code, 1)\n            self.assertIn(\"Error: No module named 'nonexistent'\", err.getvalue())\n", "def test_command_error_handling(self):\n    class ErrorCommand(BaseCommand):\n            raise CommandError(\"Test error message\")\n\n    with self.assertRaises(SystemExit) as cm, self.assertRaises(CommandError):\n        call_command('error')\n    self.assertEqual(cm.exception.code, 1)\n\n    # Test that the error message is printed to stderr\n    err = StringIO()\n    with self.assertRaises(SystemExit) as cm, self.assertRaises(CommandError):\n        call_command('error', stderr=err)\n    self.assertIn(\"Test error message\", err.getvalue())\n\n    # Test that the traceback option re-raises the exception\n    with self.assertRaises(CommandError):\n        call_command('error', traceback=True)\n", "def test_handle_default_options(self):\n    # Test that handle_default_options correctly sets DJANGO_SETTINGS_MODULE and pythonpath\n    settings_module = 'test_settings'\n    python_path = '/tmp/test_pythonpath'\n    options = mock.Mock(settings=settings_module, pythonpath=python_path)\n\n    handle_default_options(options)\n\n    self.assertEqual(os.environ['DJANGO_SETTINGS_MODULE'], settings_module)\n    self.assertIn(python_path, sys.path)\n"], "sample_190": ["    def test_year_lookup(self):\n        self.assertEqual(\n            list(Article.objects.filter(pub_date__year=2005)),\n            [self.a1, self.a2, self.a3, self.a4, self.a5, self.a6, self.a7]\n        )\n        self.assertEqual(\n            list(Article.objects.filter(pub_date__year__gt=2004)),\n            [self.a1, self.a2, self.a3, self.a4, self.a5, self.a6, self.a7]\n        )\n        self.assertEqual(\n            list(Article.objects.filter(pub_date__year__gte=2005)),\n            [self.a1, self.a2, self.a3, self.a4, self.a5, self.a6, self.a7]\n        )\n        self.assertEqual(\n            list(Article.objects.filter(pub_date__year__lt=2006)),\n            [self.a1, self.a2, self.a3, self.a4, self.a5, self.a6, self.a7]\n        )\n        self.assertEqual(\n            list(Article.objects.filter(pub_date__year__lte=2005)),\n            [self.a1, self.a2, self.a3, self.a4, self.a5, self.a6, self.a7]\n        )\n", "    def test_year_exact_lookup(self):\n        articles = Article.objects.filter(pub_date__year__exact=2005)\n        self.assertEqual(articles.count(), 7)\n", "def test_endswith_lookup(self):\n    articles = Article.objects.filter(headline__endswith='3')\n    self.assertQuerysetEqual(articles, [repr(self.a3)], ordered=False)\n", "def test_year_lookup_with_direct_value(self):\n    # Test YearLookup with direct value\n    articles = Article.objects.filter(pub_date__year=2005)\n    self.assertQuerysetEqual(\n        articles,\n        ['<Article: Article 1>', '<Article: Article 2>', '<Article: Article 3>',\n         '<Article: Article 4>', '<Article: Article 7>'],\n        ordered=False\n    )\n", "def test_iexact_lookup_for_uuidfield(self):\n    uuid_value = self.a1.uuid\n    results = Article.objects.filter(uuid__iexact=str(uuid_value).replace('-', '').lower())\n    self.assertEqual(results.count(), 1)\n    self.assertEqual(results.first(), self.a1)\n"], "sample_841": ["def test_ridge_regression_solver(solver):\n    X, y = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=0)\n    alpha = 1.0\n    coef_ = ridge_regression(X, y, alpha, solver=solver)\n    assert coef_.shape == (5,)\n", "def test_ridge_regression_solver_saga(solver):\n    X, y = make_regression(n_samples=100, n_features=20, random_state=42)\n    clf = Ridge(alpha=1.0, solver=solver)\n    clf.fit(X, y)\n    assert clf.coef_.shape == (20,)\n    assert clf.intercept_ is not None\n    assert isinstance(clf.n_iter_, int)\n", "def test_ridge_regression_n_features_gt_n_samples(solver):\n    n_samples = 10\n    n_features = 15\n    X = rng.randn(n_samples, n_features)\n    y = rng.randn(n_samples)\n    alpha = 1.0\n    coef_ = ridge_regression(X, y, alpha, solver=solver)\n    # Check that the number of coefficients equals the number of features\n    assert coef_.shape[0] == n_features\n", "def test_ridge_regression_saga_sparse_intercept(solver):\n    X, y = make_regression(n_samples=100, n_features=50, n_targets=1, random_state=0)\n    X = sp.csr_matrix(X)\n\n    # Test with return_intercept=True for sparse input with sag or saga solver\n    coef, intercept, _ = ridge_regression(X, y, alpha=1.0, solver=solver,\n                                          return_n_iter=True, return_intercept=True)\n    assert isinstance(intercept, np.ndarray)\n    assert intercept.shape == (1,)\n", "def test_ridge_regression_solver_intercept(solver):\n    if solver not in [\"sparse_cg\", \"sag\"]:\n        pytest.skip(\"This solver does not support intercept for sparse data\")\n    X, y = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=0)\n    X = sp.csr_matrix(X)\n    ridge = Ridge(alpha=1.0, solver=solver, fit_intercept=True)\n    ridge.fit(X, y)\n    X_offset = np.mean(X, axis=0)\n    y_offset = np.mean(y)\n    y_pred = ridge.predict(X)\n    assert_almost_equal(y_pred[0], y_offset + np.dot(X[0].toarray(), ridge.coef_.T))\n"], "sample_876": ["def test_mlp_classifier_predict_proba(activation):\n    for X, y in classification_datasets:\n        clf = MLPClassifier(activation=activation, random_state=42, max_iter=100)\n        clf.fit(X, y)\n        y_pred_proba = clf.predict_proba(X)\n        assert y_pred_proba.shape == (X.shape[0], clf.n_classes_)\n        assert np.allclose(y_pred_proba.sum(axis=1), np.ones(X.shape[0]))\n", "def test_mlp_classifier_predict_proba_multilabel():\n    X, y = make_multilabel_classification(random_state=0)\n    clf = MLPClassifier(random_state=0)\n    clf.fit(X, y)\n    proba = clf.predict_proba(X)\n    assert proba.shape == (X.shape[0], 2 * len(clf.classes_))\n    assert_allclose(np.sum(proba, axis=1), 1)\n", "def test_mlp_classifier_multilabel():\n    # Test MLPClassifier with multilabel classification\n    X, y = make_multilabel_classification(random_state=0)\n    clf = MLPClassifier(random_state=0)\n    clf.fit(X, y)\n    y_pred = clf.predict(X)\n    assert y_pred.shape == y.shape\n    assert_array_equal(np.unique(y_pred), np.unique(y))\n\n    # Test MLPClassifier with multilabel classification and partial_fit\n    classes = np.array([0, 1, 2, 3, 4])\n    clf = MLPClassifier(random_state=0)\n    for i in range(5):\n        clf.partial_fit(X, y[:, i], classes=classes)\n    y_pred = clf.predict(X)\n    assert y_pred.shape == y.shape\n    assert_array_equal(np.unique(y_pred), np.unique(y))\n", "def test_mlp_classifier_activation(activation):\n    clf = MLPClassifier(\n        solver=\"lbfgs\",\n        activation=activation,\n        hidden_layer_sizes=(10,),\n        random_state=0,\n        max_iter=500,\n    )\n    clf.fit(X_iris, y_iris)\n    assert clf.score(X_iris, y_iris) > 0.8\n", "def test_mlp_classifier_activation_functions(activation):\n    X, y = X_iris, y_iris\n    clf = MLPClassifier(\n        solver=\"lbfgs\",\n        alpha=1e-5,\n        hidden_layer_sizes=(5, 2),\n        random_state=1,\n        activation=activation,\n    )\n    clf.fit(X, y)\n    assert clf.n_layers_ == 3  # input layer, one hidden layer, and output layer\n    assert clf.out_activation_ == \"softmax\"\n    y_pred = clf.predict(X)\n    assert_array_equal(y_pred.shape, y.shape)\n    assert set(y_pred) == set(y)\n"], "sample_145": ["    def test_exclude_is_not_a_sequence(self):\n        class MyModelAdmin(ModelAdmin):\n            exclude = 'not a sequence'\n\n        self.assertIsInvalid(MyModelAdmin, ValidationTestModel, \"The value of 'exclude' must be a list or tuple.\", id='admin.E014')\n", "    def test_filter_vertical_invalid_type(self):\n        class InvalidFilterVerticalAdmin(ModelAdmin):\n            filter_vertical = 'invalid_type'\n\n        self.assertIsInvalid(InvalidFilterVerticalAdmin, ValidationTestModel, \"The value of 'filter_vertical' must be a list or tuple.\", id='admin.E017')\n", "    def test_readonly_fields_valid(self):\n        class MyModelAdmin(ModelAdmin):\n            readonly_fields = ('field1', 'callable_field', 'model_field', 'model_method')\n\n        self.assertIsValid(MyModelAdmin, ValidationTestModel)\n", "    def test_radio_fields_value(self):\n        msg = \"The value of 'radio_fields[\\\"field2\\\"]' must be either admin.HORIZONTAL or admin.VERTICAL.\"\n        self.assertIsInvalid(self.RadioFieldsModelAdmin, ValidationTestModel, msg, id='admin.E024')\n", "    def test_list_filter_invalid(self):\n        class BandAdmin(ModelAdmin):\n            list_filter = 'invalid_field'\n\n        self.assertIsInvalidRegexp(\n            BandAdmin, Band,\n            \"The value of 'list_filter\\\\[0\\\\]' refers to 'invalid_field', which does not refer to a Field.\",\n            id='admin.E116',\n        )\n\n        class BandAdminWithCallableListFilter(ModelAdmin):\n            list_filter = (lambda _: True,)\n\n        self.assertIsInvalid(\n            BandAdminWithCallableListFilter, Band,\n            \"The value of 'list_filter[0]' must inherit from 'ListFilter'.\",\n            id='admin.E113',\n        )\n\n        class BandAdminWithFieldListFilterSubclass(ModelAdmin):\n            class MyFieldListFilter(BooleanFieldListFilter):\n                pass\n\n            list_filter = ('name', MyFieldListFilter)\n\n        self.assertIsInvalid(\n            BandAdminWithFieldListFilterSubclass, Band,\n            \"The value of 'list_filter[1][1]' must not inherit from 'FieldListFilter'.\",\n            id='admin.E115',\n        )\n"], "sample_479": ["def test_optimize_rename_model(self):\n    operations = [\n        migrations.CreateModel(\n            name='OldModel',\n            fields=[\n                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                ('name', models.CharField(max_length=100)),\n            ],\n        ),\n        migrations.RenameModel(\n            old_name='OldModel',\n            new_name='NewModel',\n        ),\n    ]\n    expected = [\n        migrations.CreateModel(\n            name='NewModel',\n            fields=[\n                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                ('name', models.CharField(max_length=100)),\n            ],\n        ),\n    ]\n    self.assertOptimizesTo(operations, expected, exact=1)\n", "def test_optimize_create_model_and_delete_model(self):\n    operations = [\n        migrations.CreateModel(\n            name='TestModel',\n            fields=[\n                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                ('name', models.CharField(max_length=200)),\n            ],\n        ),\n        migrations.DeleteModel(\n            name='TestModel',\n        ),\n    ]\n    expected = []\n    self.assertOptimizesTo(operations, expected)\n", "def test_alter_model_options_with_delete_model(self):\n    operations = [\n        operations.CreateModel(\n            name='MyModel',\n            fields=[('id', models.AutoField(primary_key=True))],\n        ),\n        operations.AlterModelOptions(\n            name='MyModel',\n            options={'managed': False},\n        ),\n        operations.DeleteModel(name='MyModel'),\n    ]\n    expected = [\n        operations.DeleteModel(name='MyModel'),\n    ]\n    self.assertOptimizesTo(operations, expected)\n", "def test_alter_model_options(self):\n    operations = [\n        operations.CreateModel('MyModel', [('id', models.AutoField(primary_key=True))]),\n        operations.AlterModelOptions('MyModel', {'verbose_name': 'My New Verbose Name'}),\n    ]\n    expected = [\n        operations.CreateModel(\n            'MyModel',\n            [('id', models.AutoField(primary_key=True))],\n            options={'verbose_name': 'My New Verbose Name'},\n        ),\n    ]\n    self.assertOptimizesTo(operations, expected)\n", "def test_add_index_remove_index_optimization(self):\n    add_index_op = operations.AddIndex('ModelName', models.Index(fields=['field'], name='index_name'))\n    remove_index_op = operations.RemoveIndex('ModelName', 'index_name')\n\n    self.assertOptimizesTo([add_index_op, remove_index_op], [])\n"], "sample_313": ["def test_watch_for_template_changes(self, mock_reset_loaders):\n    sender = mock.Mock()\n    autoreload.autoreload_started.send(sender=autoreload)\n    sender.watch_dir.assert_called_with(EXTRA_TEMPLATES_DIR, '**/*')\n", "    def test_watch_for_template_changes(self):\n        sender = mock.MagicMock()\n        autoreload.watch_for_template_changes(sender)\n        sender.watch_dir.assert_any_call(EXTRA_TEMPLATES_DIR, '**/*')\n", "    def test_watch_for_template_changes(self):\n        sender = mock.Mock()\n        autoreload.file_changed.send(sender=self, file_path=EXTRA_TEMPLATES_DIR / \"test.html\")\n        sender.watch_dir.assert_called_with(EXTRA_TEMPLATES_DIR, '**/*')\n        self.reset_loaders.assert_called_once()\n", "def test_template_changed(self, mock_reset_loaders):\n    sender = mock.Mock()\n    file_path = EXTRA_TEMPLATES_DIR / \"test_template.html\"\n    autoreload.template_changed(sender, file_path)\n    mock_reset_loaders.assert_called_once()\n", "    def test_template_changed(self):\n        sender = mock.MagicMock()\n        file_path = EXTRA_TEMPLATES_DIR / \"test_template.html\"\n        autoreload.file_changed.send(sender=self, file_path=file_path)\n        autoreload.reset_loaders.assert_called_once()\n"], "sample_258": ["def test_receiver_with_bound_method(self):\n    \"\"\"Test connecting and disconnecting a bound method.\"\"\"\n    callable_obj = Callable()\n    a_signal.connect(callable_obj.a)\n    self.assertTrue(a_signal.has_listeners())\n    self.assertEqual(len(a_signal.receivers), 1)\n    self.assertEqual(a_signal.send(self)[0][1], self)\n    a_signal.disconnect(callable_obj.a)\n    self.assertTestIsClean(a_signal)\n", "    def test_receiver_decorator(self):\n        \"\"\"Test that the receiver decorator connects signals correctly\"\"\"\n        @receiver(a_signal, sender=self)\n            return kwargs.get('val', 'default')\n\n        self.assertTrue(a_signal.has_listeners(sender=self))\n        self.assertEqual(a_signal.send(sender=self, val='test'), [(my_receiver, 'test')])\n\n        a_signal.disconnect(my_receiver, sender=self)\n        self.assertFalse(a_signal.has_listeners(sender=self))\n", "    def test_signal_connect_disconnect(self):\n        # Test connecting and disconnecting receivers\n        receiver_disconnect = receiver(a_signal, dispatch_uid=\"disconnect\")(receiver_1_arg)\n\n        # Assert the receiver is connected\n        self.assertTrue(a_signal.has_listeners())\n\n        # Disconnect the receiver\n        a_signal.disconnect(dispatch_uid=\"disconnect\")\n\n        # Assert the receiver is disconnected\n        self.assertFalse(a_signal.has_listeners())\n        self.assertTestIsClean(a_signal)\n", "    def test_send_robust_error_handling(self):\n        \"\"\"Test send_robust handles errors correctly\"\"\"\n            raise Exception(\"Test error\")\n\n        a_signal.connect(err_receiver)\n        responses = a_signal.send_robust(self)\n        self.assertEqual(len(responses), 1)\n        self.assertIsInstance(responses[0][1], Exception)\n        self.assertEqual(str(responses[0][1]), \"Test error\")\n\n        # Clean up\n        a_signal.disconnect(err_receiver)\n", "def test_connect_disconnect(self):\n    \"\"\"Test connecting and disconnecting receivers\"\"\"\n    @receiver(a_signal)\n        return sender\n\n    self.assertTrue(a_signal.connect(receiver_func))\n    self.assertTrue(a_signal.has_listeners())\n\n    self.assertEqual(len(a_signal.send(self)), 1)\n\n    self.assertTrue(a_signal.disconnect(receiver_func))\n    self.assertFalse(a_signal.has_listeners())\n    self.assertEqual(len(a_signal.send(self)), 0)\n\n    self.assertTestIsClean(a_signal)\n"], "sample_645": ["def test_log_capture_fixture(caplog):\n    logger.info(\"Test message\")\n    sublogger.debug(\"Sublogger message\")\n    assert len(caplog.records) == 2\n    assert caplog.record_tuples == [\n        (__name__, logging.INFO, \"Test message\"),\n        (__name__ + \".baz\", logging.DEBUG, \"Sublogger message\"),\n    ]\n", "def test_logging_level_during_test(pytester: Pytester, caplog: pytest.LogCaptureFixture) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n        import pytest\n\n        logger = logging.getLogger(__name__)\n\n        @pytest.fixture\n            logger.setLevel(logging.DEBUG)\n            yield\n            logger.setLevel(logging.NOTSET)\n\n            logger.debug(\"This is a debug message\")\n            assert logger.level == logging.DEBUG\n            assert \"This is a debug message\" in caplog.text\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)\n", "def test_live_logging(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n\n            caplog.set_level(logging.INFO, logger=\"__name__\")\n            logger.info(\"Test message\")\n            assert \"Test message\" in caplog.text\n        \"\"\"\n    )\n\n    result = pytester.runpytest(\"-s\", \"--log-cli-level=INFO\")\n\n    result.stdout.fnmatch_lines([\"live log call*\"])\n    result.stdout.fnmatch_lines([\"Test message\"])\n    result.assert_outcomes(passed=1)\n", "def test_logging_level_propagation(pytester):\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n        import pytest\n\n        @pytest.fixture\n            caplog.setLevel(logging.DEBUG)\n            yield\n            caplog.setLevel(logging.WARNING)\n\n            logger.debug(\"This is a debug message\")\n            logger.info(\"This is an info message\")\n            assert (\"__main__.baz\", logging.DEBUG, \"This is a debug message\") in caplog.record_tuples\n            assert (\"__main__\", logging.INFO, \"This is an info message\") in caplog.record_tuples\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)\n", "def test_caplog_records_per_phase(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n        import pytest\n\n        logger = logging.getLogger(__name__)\n\n        @pytest.fixture(autouse=True)\n            logger.info(\"Setting up test\")\n            yield\n            logger.info(\"Tearing down test\")\n\n            logger.info(\"Test is running\")\n    \"\"\"\n    )\n    result = pytester.runpytest(\"-s\")\n\n    # Get the captured log records for setup, call, and teardown phases\n    item = result.parseoutcomes()[0][1][0]\n    setup_records = item.stash[caplog_records_key].get(\"setup\", [])\n    call_records = item.stash[caplog_records_key].get(\"call\", [])\n    teardown_records = item.stash[caplog_records_key].get(\"teardown\", [])\n\n    # Check that the correct number of records were captured for each phase\n    assert len(setup_records) == 1\n    assert len(call_records) == 1\n    assert len(teardown_records) == 1\n\n    # Check that the correct log messages were captured for each phase\n    assert setup_records[0].getMessage() == \"Setting up test\"\n    assert call_records[0].getMessage() == \"Test is running\"\n    assert teardown_records[0].getMessage() == \"Tearing down test\"\n"], "sample_501": ["def test_legend_title_fontproperties():\n    fig, ax = plt.subplots()\n    title_fontproperties = FontProperties(family='monospace', size=12, weight='bold')\n    ax.plot([1, 2, 3], [1, 2, 3], label='Test')\n    ax.legend(title='Legend Title', title_fontproperties=title_fontproperties)\n    assert ax.get_legend().get_title().get_fontproperties() == title_fontproperties\n    plt.close(fig)\n", "def test_legend_text_color():\n    fig, ax = plt.subplots()\n    lines = [ax.plot([1, 2, 3], [4, 5, 6], label='Line 1'),\n             ax.plot([1, 2, 3], [7, 8, 9], label='Line 2', color='red')]\n    legend = ax.legend(labelcolor='linecolor')\n    assert all(text.get_color() == handle.get_color() for text, handle in zip(legend.texts, lines))\n", "def test_legend_markerfirst_false():\n    fig, ax = plt.subplots()\n    line, = ax.plot([1, 2, 3], [1, 2, 3], label='Line')\n    legend = ax.legend(handler_map={line: HandlerTuple(ndivide=None)}, markerfirst=False)\n    assert legend.texts[0].get_text() == 'Line'\n", "def test_legend_with_empty_handles_labels():\n    fig, ax = plt.subplots()\n    handles, labels = [], []\n    legend = ax.legend(handles, labels)\n    assert len(legend.get_texts()) == 0\n    assert len(legend.get_patches()) == 0\n    assert len(legend.get_lines()) == 0\n", "def test_legend_with_custom_handler():\n    class CustomArtist:\n            self.label = 'Custom Artist'\n\n    class CustomHandler(mlegend.HandlerBase):\n                           width, height, fontsize, trans):\n            line = mlines.Line2D([xdescent, xdescent + width], [0.5 * height, 0.5 * height])\n            line.update_from(orig_handle)\n            line.set_transform(trans)\n            return [line]\n\n    fig, ax = plt.subplots()\n    artist = CustomArtist()\n    ax.add_artist(artist)\n\n    ax.legend([artist], handler_map={CustomArtist: CustomHandler()})\n"], "sample_1144": ["def test_split_super_sub():\n    assert split_super_sub('a_x^1') == ('a', ['1'], ['x'])\n    assert split_super_sub('var_sub1__sup_sub2') == ('var', ['sup'], ['sub1', 'sub2'])\n    assert split_super_sub('abc123') == ('abc', [], ['123'])\n    assert split_super_sub('') == ('', [], [])\n", "def test_split_super_sub():\n    assert split_super_sub('a_x^1') == ('a', ['1'], ['x'])\n    assert split_super_sub('var_sub1__sup_sub2') == ('var', ['sup'], ['sub1', 'sub2'])\n    assert split_super_sub('abc123') == ('abc', [], ['123'])\n    assert split_super_sub('') == ('', [], [])\n", "def test_split_super_sub():\n    assert split_super_sub('a_x^1') == ('a', ['1'], ['x'])\n    assert split_super_sub('var_sub1__sup_sub2') == ('var', ['sup'], ['sub1', 'sub2'])\n    assert split_super_sub('name123') == ('name', [], ['123'])\n    assert split_super_sub('') == ('', [], [])\n", "def test_split_super_sub():\n    assert split_super_sub('a_x^1') == ('a', ['1'], ['x'])\n    assert split_super_sub('var_sub1__sup_sub2') == ('var', ['sup'], ['sub1', 'sub2'])\n    assert split_super_sub('var123_sub') == ('var', [], ['123', 'sub'])\n    assert split_super_sub('var') == ('var', [], [])\n    assert split_super_sub('') == ('', [], [])\n", "def test_split_super_sub():\n    assert split_super_sub('a_x^1') == ('a', ['1'], ['x'])\n    assert split_super_sub('var_sub1__sup_sub2') == ('var', ['sup'], ['sub1', 'sub2'])\n    assert split_super_sub('var123') == ('var', [], ['123'])\n"], "sample_991": ["def test_product_convergence():\n    assert product(1/n**2, (n, 1, oo)).is_convergent() == True\n", "def test_product_convergence():\n    assert product(1 / sqrt(n), (n, 2, oo)).is_convergent() == False\n", "def test_product_convergence():\n    assert product(1/n, (n, 1, oo)).is_convergent() is False\n    assert product(exp(-n), (n, 1, oo)).is_convergent() is True\n", "def test_product_convergence():\n    assert Product(1 / sqrt(1 - cos(pi / n) ** 2), (n, 2, oo)).is_convergent()\n    assert not Product(exp(-n**2), (n, 1, oo)).is_convergent()\n    assert Product(n/(n + 1), (n, 1, oo)).is_convergent() is False\n", "def test_product_commutative_property():\n    assert product(k, (k, 1, m)) == product(k, (k, 1, m)).doit()\n    assert product(k**2, (k, 1, m)) == product(k**2, (k, 1, m)).doit()\n    assert product(k, (k, 1, m)).doit() == factorial(m)\n    assert product(k**2, (k, 1, m)).doit() == factorial(m)**2\n"], "sample_144": ["def test_model_refresh_from_db(self):\n    user = User.objects.create(name='Test User')\n    user.name = 'Updated User'\n    user.refresh_from_db()\n    self.assertEqual(user.name, 'Test User')\n", "    def test_model_save_base(self):\n        user = User(name='Test User')\n        user.save_base()\n        self.assertIsNotNone(user.pk)\n        self.assertEqual(user._state.db, 'default')\n        self.assertFalse(user._state.adding)\n", "    def test_model_save_base(self):\n        user = User(username=\"testuser\")\n        user.save_base(raw=True)\n        self.assertIsNone(user.pk)\n        user.save_base()\n        self.assertIsNotNone(user.pk)\n", "    def setUp(self):\n        self.user1 = User.objects.create(username='user1', order=1)\n        self.user2 = User.objects.create(username='user2', order=2)\n        self.user3 = User.objects.create(username='user3', order=3)\n", "def test_model_ordering(self):\n    # Test ordering on a model with a specified ordering\n    ordered_objects = Place.objects.order_by('name')\n    self.assertEqual(ordered_objects.count(), 4)\n    self.assertEqual(ordered_objects[0].name, 'Bus Station')\n    self.assertEqual(ordered_objects[1].name, 'Parking Lot')\n    self.assertEqual(ordered_objects[2].name, 'Train Station')\n    self.assertEqual(ordered_objects[3].name, 'Wholesaler')\n\n    # Test ordering on a model without a specified ordering (should default to using the primary key)\n    unordered_objects = User.objects.all()\n    self.assertEqual(unordered_objects.count(), 2)\n    self.assertEqual(unordered_objects[0].id, 1)\n    self.assertEqual(unordered_objects[1].id, 2)\n"], "sample_749": ["def test_column_transformer_remainder_estimator():\n    ct = ColumnTransformer([(\"norm1\", Normalizer(norm='l1'), [0, 1])], remainder=DoubleTrans())\n    X = np.array([[0., 1., 2.], [1., 1., 0.]])\n    transformed = ct.fit_transform(X)\n    assert_array_equal(transformed, np.array([[0., 1., 4.], [0.70710678, 0.70710678, 0.]]))\n\n    X_test = np.array([[3., 2., 1.]])\n    transformed_test = ct.transform(X_test)\n    assert_array_equal(transformed_test, np.array([[0.6, 0.8, 2.]]))\n", "def test_column_transformer_remainder_estimator():\n    remainder = Trans()\n    ct = ColumnTransformer([(\"norm\", Normalizer(), [0, 1])], remainder=remainder)\n    X = np.array([[0., 1., 2., 2.], [1., 1., 0., 1.]])\n\n    ct.fit_transform(X)\n    assert_array_equal(ct.transformers_[-1][1].transform(X[:, 2:]), ct.transform(X)[:, 2:])\n", "def test_transformers_with_passthrough():\n    ct = ColumnTransformer(\n        [(\"norm1\", Normalizer(norm='l1'), [0, 1]),\n         (\"norm2\", Normalizer(norm='l1'), slice(2, 4))],\n        remainder='passthrough'\n    )\n    X = np.array([[0., 1., 2., 2., 3.],\n                  [1., 1., 0., 1., 4.]])\n    X_transformed = ct.fit_transform(X)\n    expected_transformed = np.array([[0., 1., 0.5, 0.5, 3.],\n                                     [0.5, 0.5, 0., 1., 4.]])\n    assert_array_equal(X_transformed, expected_transformed)\n", "def test_column_transformer_transform_with_non_fitted_transformer():\n    ct = ColumnTransformer(transformers=[(\"trans\", Trans(), [0])])\n    X = np.array([[0], [1], [2]])\n    with pytest.raises(NotFittedError, match=\"This ColumnTransformer instance is not fitted yet\"):\n        ct.transform(X)\n", "def test_column_transformer_transform_before_fit():\n    # Test that transform before fit raises an error\n    ct = ColumnTransformer([(\"trans\", Trans(), [0])])\n    X = np.array([[1, 2], [3, 4]])\n    assert_raises(NotFittedError, ct.transform, X)\n"], "sample_1016": ["compilation error", "def test_octave_code_constants():\n    assert mcode(pi) == 'pi'\n    assert mcode(S.ImaginaryUnit) == '1i'\n    assert mcode(S.Exp1) == 'exp(1)'\n    assert mcode(GoldenRatio) == '(1+sqrt(5))/2'\n    assert mcode(oo) == 'inf'\n    assert mcode(-oo) == '-inf'\n    assert mcode(S.NaN) == 'NaN'\n", "def test_elementary_functions():\n    assert mcode(sin(x)) == \"sin(x)\"\n    assert mcode(cos(x)) == \"cos(x)\"\n    assert mcode(tan(x)) == \"tan(x)\"\n    assert mcode(cot(x)) == \"cot(x)\"\n    assert mcode(sec(x)) == \"sec(x)\"\n    assert mcode(csc(x)) == \"csc(x)\"\n    assert mcode(asin(x)) == \"asin(x)\"\n    assert mcode(acos(x)) == \"acos(x)\"\n    assert mcode(acot(x)) == \"acot(x)\"\n    assert mcode(atan(x)) == \"atan(x)\"\n    assert mcode(atan2(y, x)) == \"atan2(y, x)\"\n    assert mcode(asec(x)) == \"asec(x)\"\n    assert mcode(acsc(x)) == \"acsc(x)\"\n    assert mcode(sinh(x)) == \"sinh(x)\"\n    assert mcode(cosh(x)) == \"cosh(x)\"\n    assert mcode(tanh(x)) == \"tanh(x)\"\n    assert mcode(coth(x)) == \"coth(x)\"\n    assert mcode(csch(x)) == \"csch(x)\"\n    assert mcode(sech(x)) == \"sech(x)\"\n    assert mcode(asinh(x)) == \"asinh(x)\"\n    assert mcode(acosh(x)) == \"acosh(x)\"\n    assert mcode(atanh(x)) == \"atanh(x)\"\n    assert mcode(acoth(x)) == \"acoth(x)\"\n    assert mcode(asech(x)) == \"asech(x)\"\n    assert mcode(acsch(x)) == \"acsch(x)\"\n", "def test_octave_code_with_zeros():\n    A = Matrix([[0, 0], [0, 0]])\n    expected_code = 'A = zeros(2, 2);'\n    assert mcode(A, assign_to='A') == expected_code\n", "compilation error"], "sample_131": ["    def setUp(self):\n        self.db_creation = BaseDatabaseCreation(connection)\n", "    def test_test_db_signature(self):\n        test_connection = self.get_connection_copy()\n        db_creation = BaseDatabaseCreation(test_connection)\n\n        # Modify the settings_dict to have different values for testing\n        test_settings = test_connection.settings_dict\n        test_settings['HOST'] = 'test_host'\n        test_settings['PORT'] = 'test_port'\n        test_settings['ENGINE'] = 'test_engine'\n        test_settings['NAME'] = 'test_db'\n\n        # Call the test_db_signature method\n        signature = db_creation.test_db_signature()\n\n        # Assert that the signature matches the expected values\n        self.assertEqual(signature, ('test_host', 'test_port', 'test_engine', TEST_DATABASE_PREFIX + 'test_db'))\n", "    def test_create_test_db(self):\n        test_connection = self.get_connection_copy()\n        with mock.patch('django.core.management.call_command') as mock_call_command:\n            test_db_name = TEST_DATABASE_PREFIX + test_connection.settings_dict['NAME']\n            creation = BaseDatabaseCreation(test_connection)\n            creation.create_test_db(verbosity=1, autoclobber=False, serialize=True, keepdb=False)\n            self.assertEqual(test_connection.settings_dict['NAME'], test_db_name)\n            self.assertEqual(connections[DEFAULT_DB_ALIAS].settings_dict['NAME'], test_db_name)\n            mock_call_command.assert_any_call('migrate', verbosity=0, interactive=False, database=DEFAULT_DB_ALIAS, run_syncdb=True)\n            mock_call_command.assert_any_call('createcachetable', database=DEFAULT_DB_ALIAS)\n", "    def test_create_test_db(self, mock_create_test_db, mock_call_command):\n        test_connection = self.get_connection_copy()\n        creation = BaseDatabaseCreation(test_connection)\n\n        # Mock return value of _create_test_db\n        mock_create_test_db.return_value = TEST_DATABASE_PREFIX + test_connection.settings_dict['NAME']\n\n        # Call create_test_db\n        test_database_name = creation.create_test_db()\n\n        # Assert _create_test_db was called\n        mock_create_test_db.assert_called_once_with(1, False, False)\n\n        # Assert call_command was called with 'migrate' and 'createcachetable'\n        mock_call_command.assert_any_call(\n            'migrate',\n            verbosity=0,\n            interactive=False,\n            database=test_connection.alias,\n            run_syncdb=True,\n        )\n        mock_call_command.assert_any_call('createcachetable', database=test_connection.alias)\n\n        # Assert the connection settings were updated\n        self.assertEqual(test_database_name, test_connection.settings_dict['NAME'])\n        self.assertEqual(test_database_name, connections[test_connection.alias].settings_dict['NAME'])\n\n        # Assert the connection was closed and re-opened\n        self.assertTrue(test_connection.connection is None)\n        test_connection.ensure_connection()\n        self.assertIsNotNone(test_connection.connection)\n\n        # Assert the test database was serialized\n        self.assertIn('_test_serialized_contents', dir(test_connection))\n", "    def test_create_test_db(self):\n        test_connection = self.get_connection_copy()\n        test_connection.settings_dict['TEST'] = {'MIGRATE': False, 'NAME': ''}\n        db_creation = BaseDatabaseCreation(test_connection)\n\n        with mock.patch('django.core.management.call_command') as mock_call_command:\n            with mock.patch.object(db_creation, '_create_test_db') as mock_create_test_db:\n                mock_create_test_db.return_value = 'test_db'\n                with mock.patch.object(db_creation.connection, 'close'):\n                    test_db_name = db_creation.create_test_db(serialize=False)\n\n        mock_create_test_db.assert_called_once()\n        mock_call_command.assert_called_once_with('createcachetable', database=DEFAULT_DB_ALIAS)\n        self.assertEqual(test_db_name, 'test_db')\n        self.assertEqual(test_connection.settings_dict['NAME'], 'test_db')\n"], "sample_256": ["    def test_username_field_attributes(self):\n        form = UserCreationForm()\n        username_field = form.fields['username']\n        self.assertEqual(username_field.widget.attrs['autofocus'], True)\n        self.assertEqual(username_field.widget.attrs['autocapitalize'], 'none')\n        self.assertEqual(username_field.widget.attrs['autocomplete'], 'username')\n", "    def test_unicode_ci_compare(self):\n        self.assertTrue(_unicode_ci_compare('test', 'TEST'))\n        self.assertTrue(_unicode_ci_compare('\u00e9', '\u00c9'))\n        self.assertFalse(_unicode_ci_compare('test', 'test1'))\n", "    def test_read_only_password_hash_field(self):\n        form = UserChangeForm(instance=self.u1)\n        self.assertIsInstance(form.fields['password'], ReadOnlyPasswordHashField)\n", "    def test_password_change(self):\n        new_password = 'new_password'\n        form = SetPasswordForm(user=self.u1, data={\n            'new_password1': new_password,\n            'new_password2': new_password,\n        })\n        self.assertTrue(form.is_valid())\n        form.save()\n        self.assertTrue(self.u1.check_password(new_password))\n", "    def test_username_field_to_python(self):\n        field = UsernameField()\n        self.assertEqual(field.to_python('T\u00ebst'), 'Test')\n"], "sample_331": ["    def test_parse_datetime_with_tzinfo(self):\n        dt = parse_datetime(\"2022-01-01T12:34:56+03:00\")\n        self.assertEqual(dt, datetime(2022, 1, 1, 12, 34, 56, tzinfo=get_fixed_timezone(180)))\n        dt = parse_datetime(\"2022-01-01T12:34:56-05:00\")\n        self.assertEqual(dt, datetime(2022, 1, 1, 12, 34, 56, tzinfo=get_fixed_timezone(-300)))\n", "    def test_parse_datetime_with_tzinfo(self):\n        dt = parse_datetime('2022-01-01T12:00:00+02:00')\n        self.assertEqual(dt, datetime(2022, 1, 1, 12, 0, 0, tzinfo=get_fixed_timezone(120)))\n", "    def test_parse_datetime_with_timezone(self):\n        value = \"2022-01-01T00:00:00+03:00\"\n        result = parse_datetime(value)\n        expected = datetime(2022, 1, 1, 0, 0, 0, tzinfo=get_fixed_timezone(180))\n        self.assertEqual(result, expected)\n\n        value = \"2022-01-01T00:00:00-05:00\"\n        result = parse_datetime(value)\n        expected = datetime(2022, 1, 1, 0, 0, 0, tzinfo=get_fixed_timezone(-300))\n        self.assertEqual(result, expected)\n", "    def test_parse_datetime_with_tzinfo(self):\n        dt_str = '2022-01-01T12:00:00+02:00'\n        dt = parse_datetime(dt_str)\n        self.assertEqual(dt, datetime(2022, 1, 1, 12, 0, 0, tzinfo=get_fixed_timezone(120)))\n\n        dt_str = '2022-01-01T12:00:00-03:30'\n        dt = parse_datetime(dt_str)\n        self.assertEqual(dt, datetime(2022, 1, 1, 12, 0, 0, tzinfo=get_fixed_timezone(-210)))\n", "    def test_parse_duration(self):\n        # Test standard duration format\n        self.assertEqual(parse_duration('3 04:05:06.789123'), timedelta(days=3, hours=4, minutes=5, seconds=6, microseconds=789123))\n        # Test ISO 8601 duration format\n        self.assertEqual(parse_duration('P3DT4H5M6.789S'), timedelta(days=3, hours=4, minutes=5, seconds=6, microseconds=789000))\n        # Test PostgreSQL's day-time interval format\n        self.assertEqual(parse_duration('3 days 04:05:06.789123'), timedelta(days=3, hours=4, minutes=5, seconds=6, microseconds=789123))\n"], "sample_217": ["def test_media_merge_with_duplicates(self):\n    m1 = Media(js=['a.js', 'b.js'])\n    m2 = Media(js=['b.js', 'c.js'])\n    merged = m1 + m2\n    self.assertEqual(merged._js, ['a.js', 'b.js', 'c.js'])\n", "    def test_multiwidget_media(self):\n        class MyMultiWidget(MultiWidget):\n                widgets = (TextInput(attrs={'class': 'text1'}), TextInput(attrs={'class': 'text2'}))\n                super().__init__(widgets, attrs)\n\n                if value:\n                    return value.split(' ')\n                return ['', '']\n\n        widget = MyMultiWidget()\n        expected_media = Media(css={'all': []}, js=['django/forms/widgets/text.js'])\n        self.assertEqual(widget.media, expected_media)\n", "    def test_media_property(self):\n        class SimpleWidget(Widget):\n            class Media:\n                css = {'all': ('path/to/simple.css',)}\n\n        widget = SimpleWidget()\n        expected_media = Media(css={'all': ('path/to/simple.css',)})\n        self.assertEqual(widget.media, expected_media)\n", "def test_widget_media_addition(self):\n    class TestWidget1(Widget):\n        class Media:\n            css = {'all': ('test1.css',)}\n            js = ('test1.js',)\n\n    class TestWidget2(Widget):\n        class Media:\n            css = {'all': ('test2.css',)}\n            js = ('test2.js',)\n\n    class TestWidget3(MultiWidget):\n        widgets = [TestWidget1, TestWidget2]\n\n    widget3 = TestWidget3()\n    expected_media = Media(css={'all': ['test1.css', 'test2.css']}, js=['test1.js', 'test2.js'])\n    self.assertEqual(widget3.media, expected_media)\n", "def test_media_property_with_extended_media(self):\n    class ExtendedMediaForm(Form):\n        class Media:\n            extend = True\n            css = {'screen': ('extend.css',)}\n            js = ('extend.js',)\n\n    class ExtendedMediaWidget(MultiWidget):\n        class Media:\n            extend = True\n            css = {'screen': ('widget_extend.css',)}\n            js = ('widget_extend.js',)\n\n            self.widgets = [TextInput()] if widgets is None else widgets\n            super().__init__(widgets=self.widgets)\n\n            return value or []\n\n    class TestForm(Form):\n        multi = CharField(widget=ExtendedMediaWidget())\n\n    form = TestForm()\n    media = form.media\n    expected_css = {'screen': {'extend.css', 'widget_extend.css'}}\n    expected_js = {'extend.js', 'widget_extend.js'}\n    self.assertEqual(media._css, expected_css)\n    self.assertEqual(set(media._js), expected_js)\n"], "sample_981": ["def test_transpositions():\n    p = Permutation([[1, 2, 3], [0, 4, 5, 6, 7]])\n    t = p.transpositions()\n    assert t == [(0, 7), (0, 6), (0, 5), (0, 4), (1, 3), (1, 2)]\n    assert Permutation.rmul(*[Permutation([ti], size=p.size) for ti in t]) == p\n", "def test_permutation_from_sequence():\n    p = Permutation.from_sequence('SymPy')\n    assert p(sorted(\"SymPy\")) == list(\"SymPy\")\n    assert Permutation.from_sequence('SymPy', key=lambda x: x.lower())(sorted(\"SymPy\", key=lambda x: x.lower())) == list(\"SymPy\")\n", "def test_rmul_with_af():\n    p = Permutation([1, 2, 0])\n    q = Permutation([2, 0, 1])\n    result = Permutation.rmul_with_af(p, q)\n    assert result == Permutation([2, 1, 0])\n", "def test_rmuln():\n    p = Permutation([0, 2, 1])\n    q = Permutation([1, 0, 2])\n    r = Permutation([2, 1, 0])\n    assert rmul(p, q, r) == Permutation([1, 0, 2])\n    assert rmul(p, r, q) == Permutation([2, 0, 1])\n    assert rmul(q, r, p) == Permutation([1, 2, 0])\n", "def test_Permutation_transpositions():\n    p = Permutation([[1, 2, 3], [0, 4, 5, 6, 7]])\n    t = p.transpositions()\n    assert t == [(0, 7), (0, 6), (0, 5), (0, 4), (1, 3), (1, 2)]\n    assert Permutation.rmul(*[Permutation([ti], size=p.size) for ti in t]) == p\n"], "sample_1003": ["def test_series_flag():\n    options = Options((), {'series': True})\n    assert options.series == True\n\n    options = Options((), {'series': False})\n    assert options.series == False\n\n    raises(OptionError, lambda: Options((), {'series': 'invalid'}))\n", "def test_composite_domain_and_gens_interference():\n    options = {'gens': (x, y), 'domain': ZZ['x', 'y'], 'composite': True}\n    raises(GeneratorsError, lambda: Options(None, options))\n", "def test_method_option():\n    opt = Options((), {'method': 'groebner'})\n    assert opt.method == 'groebner'\n\n    opt = Options((), {'method': 'solve'})\n    assert opt.method == 'solve'\n\n    raises(OptionError, lambda: Options((), {'method': 123}))\n", "def test_composite_option():\n    options = Options((), {'composite': True, 'domain': 'QQ'})\n    assert options['composite'] == True\n    assert options['domain'] == QQ\n\n    raises(GeneratorsError, lambda: Options((x,), {'composite': True, 'domain': 'EX'}))\n", "def test_composite_option_with_interfering_gens():\n    options = Options((), {'domain': ZZ.poly_ring(x), 'gens': (x,)})\n    with raises(GeneratorsError):\n        Composite.postprocess(options)\n"], "sample_997": ["def test_convert_equals_signs():\n    expr = parse_expr(\"1*2=x\", transformations=(standard_transformations + (convert_equals_signs,)))\n    assert expr == Eq(2, Symbol('x'))\n", "def test_convert_equals_signs():\n    expr = parse_expr(\"a=b=c\", transformations=(standard_transformations + (convert_equals_signs,)))\n    assert expr == Eq(Eq(a, b), c)\n", "def test_function_exponentiation():\n    from sympy.parsing.sympy_parser import function_exponentiation\n    transformations = standard_transformations + (function_exponentiation,)\n    result = parse_expr(\"sin**2(x)\", transformations=transformations)\n    assert result == sin(x)**2\n", "def test_implicit_application():\n    expr = parse_expr(\"sin x cos x\", transformations=standard_transformations + (implicit_multiplication,))\n    assert isinstance(expr, Mul)\n    assert isinstance(expr.args[0], sin)\n    assert expr.args[1] == Symbol('x') * Symbol('cos(x)')\n", "def test_convert_equals_signs():\n    expr = parse_expr(\"(1*2=x)=False\", transformations=(standard_transformations + (convert_equals_signs,)))\n    assert expr == Eq(Eq(2, Symbol('x')), False)\n"], "sample_558": ["def test_imagegrid_label_mode():\n    fig, ax = plt.subplots()\n    grid = ImageGrid(fig, 111, (2, 2), label_mode=\"all\")\n    for ax in grid:\n        assert not ax.xaxis.get_ticklabels()[0].get_visible()\n        assert not ax.yaxis.get_ticklabels()[0].get_visible()\n    plt.close(fig)\n\n    fig, ax = plt.subplots()\n    grid = ImageGrid(fig, 111, (2, 2), label_mode=\"L\")\n    for ax in grid[:-1]:\n        assert not ax.yaxis.get_ticklabels()[0].get_visible()\n    for ax in grid[::-1][:-1]:\n        assert not ax.xaxis.get_ticklabels()[0].get_visible()\n    plt.close(fig)\n\n    fig, ax = plt.subplots()\n    grid = ImageGrid(fig, 111, (2, 2), label_mode=\"1\")\n    for ax in grid[:-1]:\n        assert ax.xaxis.get_ticklabels()[0].get_visible()\n        assert ax.yaxis.get_ticklabels()[0].get_visible()\n    assert not grid[-1].xaxis.get_ticklabels()[0].get_visible()\n    assert not grid[-1].yaxis.get_ticklabels()[0].get_visible()\n    plt.close(fig)\n", "def test_imagegrid_colorbar_mode_edge():\n    fig = plt.figure(figsize=(6, 6))\n    grid = ImageGrid(fig, 111, (2, 2), cbar_mode='edge', cbar_location='right')\n    for ax, i in zip(grid, range(1, 5)):\n        ax.imshow(np.arange(100).reshape((10, 10)))\n    plt.close(fig)\n", "def test_image_grid_aspect():\n    fig = plt.figure(figsize=(5.5, 4))\n    grid = ImageGrid(fig, 111, nrows_ncols=(2, 2), axes_pad=(0.1, 0.1))\n\n    for i, ax in enumerate(grid):\n        ax.imshow(np.random.random((10, 10)), origin=\"lower\")\n        ax.cax.colorbar(ax.images[0])\n        ax.cax.toggle_label(i)\n\n    plt.draw()\n", "def test_image_grid_custom_axes_class():\n    fig = plt.figure(figsize=(4, 4))\n    custom_axes_class = (mpl_toolkits.axes_grid1.mpl_axes.Axes, {'facecolor': 'lightgray'})\n    grid = ImageGrid(fig, 111, (1, 2), axes_pad=0.1, axes_class=custom_axes_class)\n    for ax in grid:\n        ax.imshow([[1, 2], [3, 4]])\n", "def test_imagegrid_single_colorbar_bottom():\n    fig = plt.figure(figsize=(6, 6))\n    grid = ImageGrid(fig, 111, (2, 2), cbar_mode=\"single\", cbar_location=\"bottom\", cbar_size=\"7%\", cbar_pad=0.15)\n\n    for ax in grid:\n        im = ax.imshow(np.random.random((10, 10)))\n\n    cb = grid.cbar_axes[0].colorbar(im)\n    cb.ax.set_xlabel('Colorbar Label')\n\n    plt.close(fig)\n"], "sample_1098": ["def test_appellf1_derivative():\n    a, b1, b2, c, x, y = symbols('a b1 b2 c x y')\n    f = appellf1(a, b1, b2, c, x, y)\n    assert f.fdiff(5) == (a*b1/c)*appellf1(a + 1, b1 + 1, b2, c + 1, x, y)\n    assert f.fdiff(6) == (a*b2/c)*appellf1(a + 1, b1, b2 + 1, c + 1, x, y)\n    raises(ArgumentIndexError, lambda: f.fdiff(1))\n    raises(ArgumentIndexError, lambda: f.fdiff(2))\n    raises(ArgumentIndexError, lambda: f.fdiff(3))\n    raises(ArgumentIndexError, lambda: f.fdiff(4))\n    raises(ArgumentIndexError, lambda: f.fdiff(7))\n", "def test_appellf1():\n    a, b1, b2, c, x, y = symbols('a b1 b2 c x y')\n    f = appellf1(a, b1, b2, c, x, y)\n    assert f.fdiff(5) == (a*b1/c)*appellf1(a + 1, b1 + 1, b2, c + 1, x, y)\n    assert f.fdiff(6) == (a*b2/c)*appellf1(a + 1, b1, b2 + 1, c + 1, x, y)\n    assert appellf1(a, b1, b2, c, 0, 0) == S.One\n", "def test_appellf1_fdiff():\n    a, b1, b2, c = symbols('a b1 b2 c')\n    x, y = symbols('x y', positive=True)\n    f = appellf1(a, b1, b2, c, x, y)\n\n    assert f.fdiff(5) == (a*b1/c)*appellf1(a + 1, b1 + 1, b2, c + 1, x, y)\n    assert f.fdiff(6) == (a*b2/c)*appellf1(a + 1, b1, b2 + 1, c + 1, x, y)\n    assert f.fdiff(1) == Derivative(f, a)\n    assert f.fdiff(2) == Derivative(f, b1)\n    assert f.fdiff(3) == Derivative(f, b2)\n    assert f.fdiff(4) == Derivative(f, c)\n\n    raises(ArgumentIndexError, lambda: f.fdiff(7))\n", "def test_appellf1_fdiff():\n    a, b1, b2, c, x, y = symbols('a b1 b2 c x y', real=True)\n    f = appellf1(a, b1, b2, c, x, y)\n    assert f.fdiff(5) == (a*b1/c)*appellf1(a + 1, b1 + 1, b2, c + 1, x, y)\n    assert f.fdiff(6) == (a*b2/c)*appellf1(a + 1, b1, b2 + 1, c + 1, x, y)\n    raises(ArgumentIndexError, lambda: f.fdiff(7))\n", "def test_hyper_series_expansion():\n    f = hyper((1, 2, 3), [3, 4], x)\n    series = f.nseries(x, n=5)\n    assert str(series) == \"1 + x*(1/2 + 1/6*x + 1/24*x**2 + 1/120*x**3 + 1/720*x**4) + O(x**5)\"\n"], "sample_746": ["def test_balanced_accuracy_score():\n    y_true = np.array([0, 1, 0, 0, 1, 0])\n    y_pred = np.array([0, 1, 0, 0, 0, 1])\n    balanced_acc = balanced_accuracy_score(y_true, y_pred)\n    assert_almost_equal(balanced_acc, 0.625)\n", "def test_brier_score_loss():\n    y_true = np.array([0, 1, 1, 0])\n    y_prob = np.array([0.1, 0.9, 0.8, 0.3])\n    expected_score = 0.037\n    assert_almost_equal(brier_score_loss(y_true, y_prob), expected_score, decimal=3)\n\n    y_true_categorical = np.array([\"spam\", \"ham\", \"ham\", \"spam\"])\n    assert_almost_equal(brier_score_loss(y_true_categorical, y_prob, pos_label=\"ham\"), expected_score, decimal=3)\n", "def test_brier_score_loss():\n    y_true = np.array([0, 1, 1, 0])\n    y_prob = np.array([0.1, 0.9, 0.8, 0.3])\n    expected_score = 0.037\n    score = brier_score_loss(y_true, y_prob)\n    assert_almost_equal(score, expected_score)\n\n    y_true_categorical = np.array([\"spam\", \"ham\", \"ham\", \"spam\"])\n    score = brier_score_loss(y_true_categorical, y_prob, pos_label=\"ham\")\n    assert_almost_equal(score, expected_score)\n\n    y_true_binary = np.array([True, False, False, True])\n    score = brier_score_loss(y_true_binary, y_prob)\n    assert_almost_equal(score, expected_score)\n", "def test_brier_score_loss():\n    # Test brier_score_loss function with binary and categorical targets\n    y_true = np.array([0, 1, 1, 0])\n    y_true_categorical = np.array([\"spam\", \"ham\", \"ham\", \"spam\"])\n    y_prob = np.array([0.1, 0.9, 0.8, 0.3])\n\n    score = brier_score_loss(y_true, y_prob)\n    assert_almost_equal(score, 0.037, decimal=3)\n\n    score = brier_score_loss(y_true, 1-y_prob, pos_label=0)\n    assert_almost_equal(score, 0.037, decimal=3)\n\n    score = brier_score_loss(y_true_categorical, y_prob, pos_label=\"ham\")\n    assert_almost_equal(score, 0.037, decimal=3)\n\n    score = brier_score_loss(y_true, np.array(y_prob) > 0.5)\n    assert_almost_equal(score, 0.0)\n", "def test_brier_score_loss():\n    # Testing brier_score_loss function\n    y_true = np.array([0, 1, 1, 0])\n    y_prob = np.array([0.1, 0.9, 0.8, 0.3])\n    expected_score = 0.037\n    assert_almost_equal(brier_score_loss(y_true, y_prob), expected_score)\n\n    y_true_categorical = np.array([\"spam\", \"ham\", \"ham\", \"spam\"])\n    expected_score = 0.037\n    assert_almost_equal(brier_score_loss(y_true_categorical, y_prob, pos_label=\"ham\"), expected_score)\n\n    y_true = np.array([0, 1, 1, 0])\n    y_prob_boolean = np.array([0, 1, 1, 0])\n    expected_score = 0.0\n    assert_almost_equal(brier_score_loss(y_true, y_prob_boolean), expected_score)\n\n    # Testing with sample weights\n    sample_weight = np.array([0.5, 0.2, 0.3, 0.7])\n    expected_score = 0.033\n    assert_almost_equal(brier_score_loss(y_true, y_prob, sample_weight=sample_weight), expected_score)\n\n    # Testing with custom positive label\n    y_true = np.array([\"spam\", \"ham\", \"ham\", \"spam\"])\n    expected_score = 0.037\n    assert_almost_equal(brier_score_loss(y_true, y_prob, pos_label=\"ham\"), expected_score)\n"], "sample_244": ["def test_custom_form_kwargs_passed_to_form_constructor(self):\n    \"\"\"Test that custom form kwargs are passed to the form constructor.\"\"\"\n    CustomKwargFormSet = formset_factory(CustomKwargForm, extra=2, form_kwargs={'custom_kwarg': 'test'})\n    formset = CustomKwargFormSet()\n    for form in formset:\n        self.assertEqual(form.custom_kwarg, 'test')\n", "def test_custom_kwargs(self):\n    CustomKwargFormSet = formset_factory(CustomKwargForm, form_kwargs={'custom_kwarg': 'foo'})\n    formset = CustomKwargFormSet()\n    for form in formset:\n        self.assertEqual(form.custom_kwarg, 'foo')\n", "def test_custom_form_kwargs(self):\n    CustomKwargFormSet = formset_factory(\n        CustomKwargForm,\n        formset=BaseFormSet,\n        form_kwargs={'custom_kwarg': 'test_value'},\n    )\n    formset = CustomKwargFormSet()\n    for form in formset:\n        self.assertEqual(form.custom_kwarg, 'test_value')\n", "def test_cleaned_data_attribute_error(self):\n    formset = self.make_choiceformset()\n    with self.assertRaises(AttributeError) as cm:\n        formset.cleaned_data\n    self.assertEqual(str(cm.exception), \"'ChoiceFormSet' object has no attribute 'cleaned_data'\")\n", "def test_custom_kwargs_in_formset(self):\n    \"\"\"Test that custom kwargs are passed to the formset forms.\"\"\"\n    CustomKwargFormSet = formset_factory(CustomKwargForm, formset=BaseFormSet, extra=2, form_kwargs={'custom_kwarg': 'test'})\n    formset = CustomKwargFormSet()\n    for form in formset.forms:\n        self.assertEqual(form.custom_kwarg, 'test')\n"], "sample_884": ["def test_is_deprecated():\n    assert _is_deprecated(mock_function)\n    assert _is_deprecated(MockClass1)\n    assert _is_deprecated(MockClass2().method)\n    assert _is_deprecated(MockClass2().n_features_)\n    assert _is_deprecated(MockClass3().__init__)\n    assert _is_deprecated(MockClass6)\n    assert not _is_deprecated(MockClass4)\n    assert not _is_deprecated(MockClass5)\n", "def test_is_deprecated():\n    assert _is_deprecated(MockClass1) is True\n    assert _is_deprecated(MockClass2().method) is True\n    assert _is_deprecated(MockClass2().n_features_) is True\n    assert _is_deprecated(MockClass3.__init__) is True\n    assert _is_deprecated(MockClass4) is False\n    assert _is_deprecated(MockClass5) is False\n    assert _is_deprecated(MockClass6) is True\n    assert _is_deprecated(mock_function) is True\n", "def test_deprecated_function_coverage():\n    assert mock_function() == 10\n    with pytest.warns(FutureWarning, match=\"Function mock_function is deprecated; a message\"):\n        mock_function()\n", "def test_deprecated_functions_picklable():\n    with pytest.warns(FutureWarning, match=\"Function mock_function is deprecated; a message\"):\n        mock_function()\n\n    # Pickle the function\n    pickled_func = pickle.dumps(mock_function)\n    unpickled_func = pickle.loads(pickled_func)\n\n    # Check that the unpickled function is deprecated\n    assert _is_deprecated(unpickled_func)\n\n    # Check that the unpickled function still works\n    with pytest.warns(FutureWarning, match=\"Function mock_function is deprecated; a message\"):\n        result = unpickled_func()\n        assert result == 10\n", "def test_deprecated_decorator():\n    # Test for deprecated class\n    with pytest.warns(FutureWarning, match=\"Class MockClass1 is deprecated; qwerty\"):\n        MockClass1()\n\n    # Test for deprecated method\n    mock_class2 = MockClass2()\n    with pytest.warns(FutureWarning, match=\"Function method is deprecated; mockclass2_method\"):\n        mock_class2.method()\n\n    # Test for deprecated property\n    mock_class2 = MockClass2()\n    with pytest.warns(FutureWarning, match=\"n_features_ is deprecated\"):\n        _ = mock_class2.n_features_\n\n    # Test for deprecated __init__\n    with pytest.warns(FutureWarning, match=\"Function __init__ is deprecated\"):\n        MockClass3()\n\n    # Test for deprecated class with overridden __new__\n    with pytest.warns(FutureWarning, match=\"Class MockClass6 is deprecated; a message\"):\n        MockClass6(1)\n\n    # Test for deprecated function\n    with pytest.warns(FutureWarning, match=\"Function mock_function is deprecated; a message\"):\n        mock_function()\n\n    # Test for deprecated class that does not call super().__init__\n    mock_class5 = MockClass5(1)\n    assert mock_class5.a == 1\n\n    # Test for _is_deprecated function\n    assert _is_deprecated(mock_function)\n    assert not _is_deprecated(MockClass4)\n    assert not _is_deprecated(MockClass5)\n"], "sample_264": ["    def test_message_encoder_decoder(self):\n        message = Message(constants.INFO, 'test message', extra_tags='test_tag')\n        encoded_message = json.dumps(message, cls=MessageEncoder)\n        decoded_message = json.loads(encoded_message, cls=MessageDecoder)\n        self.assertEqual(message.level, decoded_message.level)\n        self.assertEqual(message.message, decoded_message.message)\n        self.assertEqual(message.extra_tags, decoded_message.extra_tags)\n", "    def test_samesite_attribute(self):\n        storage = self.make_storage()\n        response = self.make_response()\n        storage._update_cookie(\"test_data\", response)\n        self.assertEqual(response.cookies[storage.cookie_name]['samesite'], 'Lax')\n", "def test_legacy_decode(self):\n    storage = self.storage_class(self.request)\n    message = Message(\"Test message\", constants.INFO)\n    encoded_data = storage._encode([message])\n    hash_, value = encoded_data.split('$', 1)\n    # Test valid data\n    decoded_data = storage._legacy_decode(encoded_data)\n    self.assertEqual(decoded_data, json.dumps([message], cls=MessageEncoder))\n    # Test invalid hash\n    invalid_hash = '0' * len(hash_)\n    invalid_encoded_data = f'{invalid_hash}${value}'\n    decoded_data = storage._legacy_decode(invalid_encoded_data)\n    self.assertIsNone(decoded_data)\n    # Test invalid format\n    invalid_encoded_data = 'invalid_format'\n    decoded_data = storage._legacy_decode(invalid_encoded_data)\n    self.assertIsNone(decoded_data)\n", "    def test_encode_decode_safedata(self):\n        storage = self.create_storage()\n        message = Message(constants.INFO, \"test message\", extra_tags=\"safe\")\n        safe_message = Message(constants.INFO, mark_safe(\"test message\"), extra_tags=\"safe\")\n\n        encoded = storage._encode([message, safe_message])\n        decoded = storage._decode(encoded)\n\n        self.assertEqual(len(decoded), 2)\n        self.assertIsInstance(decoded[1].message, SafeData)\n        self.assertEqual(decoded[1].message, safe_message.message)\n", "def test_legacy_decode(self):\n    storage = self.storage_class(self.request)\n    message = Message('Test message', constants.INFO)\n    legacy_hash = storage._legacy_hash(json.dumps([message], cls=MessageEncoder))\n    data = f'{legacy_hash}${json.dumps([message], cls=MessageEncoder)}'\n    decoded = storage._legacy_decode(data)\n    self.assertEqual(decoded, json.dumps([message], cls=MessageEncoder))\n"], "sample_127": ["def test_bulk_create_empty_list(self):\n    result = Country.objects.bulk_create([])\n    self.assertEqual(result, [])\n", "def test_bulk_create_with_conflicts(self):\n    Country.objects.bulk_create(self.data)\n    with self.assertRaises(IntegrityError):\n        Country.objects.bulk_create(self.data, ignore_conflicts=True)\n", "    def test_bulk_create_with_returning_fields(self):\n        if connection.features.can_return_rows_from_bulk_insert:\n            with self.assertNumQueries(1):\n                countries = Country.objects.bulk_create(self.data, ignore_conflicts=False)\n                # Check if the returned countries have the expected IDs\n                self.assertEqual([c.id for c in countries], [c.id for c in self.data])\n", "def test_bulk_create_with_ignore_conflicts(self):\n    # Create initial data\n    Country.objects.bulk_create(self.data)\n\n    # Try to bulk create the same data again, ignoring conflicts\n    Country.objects.bulk_create(self.data, ignore_conflicts=True)\n\n    # Check that the data was not duplicated\n    self.assertEqual(Country.objects.count(), 4)\n", "    def test_bulk_create_with_ignore_conflicts(self):\n        # Create initial objects\n        Country.objects.bulk_create(self.data)\n\n        # Create a new object with the same iso_two_letter as an existing one\n        new_country = Country(name=\"United States\", iso_two_letter=\"US\")\n\n        # Test bulk_create with ignore_conflicts\n        with self.assertNumQueries(1):\n            Country.objects.bulk_create([new_country], ignore_conflicts=True)\n\n        # Check that no new object was created\n        self.assertEqual(Country.objects.count(), 4)\n"], "sample_951": ["def test_evaluate_signature():\n        pass\n\n    sig = inspect.signature(func)\n    evaluated_sig = inspect.evaluate_signature(sig, globals(), locals())\n\n    assert evaluated_sig.parameters['a'].annotation == list\n    assert evaluated_sig.parameters['b'].annotation == str\n    assert evaluated_sig.return_annotation == tuple\n", "def test_signature_from_str():\n    sig = inspect.signature_from_str('(a: int, b: str = \"default\", *args: Any, **kwargs: Any) -> None')\n    assert str(sig) == '(a: int, b: str = \"default\", *args: Any, **kwargs: Any) -> None'\n", "def test_signature_from_str():\n    sig = inspect.signature_from_str(\"(a: int, b: str = 'default') -> None\")\n    assert str(sig) == \"(a: int, b: str = 'default') -> None\"\n\n    params = list(sig.parameters.values())\n    assert len(params) == 2\n    assert params[0].name == 'a'\n    assert params[0].annotation == int\n    assert params[0].default == Parameter.empty\n    assert params[1].name == 'b'\n    assert params[1].annotation == str\n    assert params[1].default == 'default'\n\n    assert sig.return_annotation == None\n", "def test_stringify_signature():\n    sig = inspect.signature(lambda x: x)\n    assert stringify_signature(sig) == '(x)'\n\n    sig = inspect.signature(lambda x, y=1: x)\n    assert stringify_signature(sig) == '(x, y=1)'\n\n    sig = inspect.signature(lambda x, y=1, *, z=2: x)\n    assert stringify_signature(sig) == '(x, y=1, *, z=2)'\n\n    sig = inspect.signature(lambda x, *, y: x)\n    assert stringify_signature(sig) == '(x, *, y)'\n\n    sig = inspect.signature(lambda x, *args, y: x)\n    assert stringify_signature(sig) == '(x, *args, y)'\n\n    sig = inspect.signature(lambda x, *args, y, **kwargs: x)\n    assert stringify_signature(sig) == '(x, *args, y, **kwargs)'\n\n    if sys.version_info >= (3, 8):\n        sig = inspect.signature(lambda x, /, y=1: x)\n        assert stringify_signature(sig) == '(x, /, y=1)'\n\n    sig = inspect.signature(lambda x: x)\n    sig = sig.replace(return_annotation='int')\n    assert stringify_signature(sig) == '(x) -> int'\n", "def test_signature_from_ast():\n    code = \"\"\""], "sample_838": ["def test_column_transformer_transformer_weights():\n    ct = ColumnTransformer(\n        transformers=[(\"double\", DoubleTrans(), [0, 1])],\n        transformer_weights={\"double\": 2.0}\n    )\n    X = np.array([[1, 2], [3, 4]])\n    X_transformed = ct.fit_transform(X)\n    assert_array_equal(X_transformed, np.array([[2, 4], [6, 8]]))\n", "def test_column_transformer_empty_column_selection():\n    # Test that the transformer is correctly skipped when the column selection is empty\n    ct = ColumnTransformer([(\"trans\", Trans(), [0])])\n    X = np.array([[1], [2]])\n    X_trans = ct.fit_transform(X)\n    assert_array_equal(X_trans, [[1], [2]])\n\n    ct = ColumnTransformer([(\"trans\", Trans(), [])])\n    X_trans = ct.fit_transform(X)\n    assert_array_equal(X_trans, [])\n", "def test_column_transformer_with_mixed_data_types():\n    # Test ColumnTransformer with mixed data types\n    X = np.array([[0, 'a'], [1, 'b'], [2, 'c']], dtype=object)\n    transformer = ColumnTransformer(transformers=[('num', StandardScaler(), [0]), ('cat', OneHotEncoder(), [1])])\n    X_transformed = transformer.fit_transform(X)\n    assert_array_equal(X_transformed, np.array([[-1.22474487, -1., 0., 0.],\n                                                [-0.43643578, 0., 1., 0.],\n                                                [1.22474487, 0., 0., 1.]]))\n", "def test_transform_sparse_matrix():\n    ct = ColumnTransformer([(\"sparse\", SparseMatrixTrans(), slice(None))])\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    ct.fit(X)\n    X_trans = ct.transform(X)\n    assert sparse.issparse(X_trans)\n    assert_array_equal(X_trans.toarray(), np.eye(3))\n", "def test_transform_on_unseen_data():\n    # Create a column transformer with some transformers\n    ct = ColumnTransformer(\n        transformers=[(\"norm1\", Normalizer(norm='l1'), [0, 1]),\n                      (\"norm2\", Normalizer(norm='l2'), [2, 3])])\n    X = np.array([[0., 1., 2., 2.],\n                  [1., 1., 0., 1.]])\n    ct.fit(X)\n\n    # Transform a new dataset\n    X_new = np.array([[3., 1., 0., 2.]])\n    X_transformed = ct.transform(X_new)\n\n    # Check that the transformer works as expected\n    expected_transformed = np.array([[0.6, 0.4, 0.8, 0.4]])\n    assert_allclose_dense_sparse(X_transformed, expected_transformed)\n"], "sample_475": ["    def test_filter_vertical_invalid_type(self):\n        class InvalidFilterVerticalAdmin(ModelAdmin):\n            filter_vertical = \"invalid\"\n\n        self.assertIsInvalid(\n            InvalidFilterVerticalAdmin,\n            ValidationTestModel,\n            \"The value of 'filter_vertical' must be a list or tuple.\",\n            id=\"admin.E017\",\n        )\n", "    def test_list_filter_non_field(self):\n        class NonFieldFilter:\n            pass\n\n        class NonFieldListFilter(ListFilter):\n            pass\n\n        class NonFieldFilterAdmin(ModelAdmin):\n            list_filter = (NonFieldFilter, NonFieldListFilter)\n\n        self.assertIsInvalid(\n            NonFieldFilterAdmin,\n            ValidationTestModel,\n            \"The value of 'list_filter[0]' must inherit from 'ListFilter'.\",\n            id=\"admin.E113\",\n        )\n        self.assertIsInvalid(\n            NonFieldFilterAdmin,\n            ValidationTestModel,\n            \"The value of 'list_filter[1]' must not inherit from 'FieldListFilter'.\",\n            id=\"admin.E114\",\n        )\n", "    def test_autocomplete_fields_not_a_list_or_tuple(self):\n        class MyModelAdmin(admin.ModelAdmin):\n            autocomplete_fields = \"not_a_list_or_tuple\"\n\n        self.assertIsInvalid(\n            MyModelAdmin,\n            ValidationTestModel,\n            \"The value of 'autocomplete_fields' must be a list or tuple.\",\n            id=\"admin.E036\",\n        )\n", "    def test_autocomplete_fields_must_be_list_or_tuple(self):\n        class MyModelAdmin(ModelAdmin):\n            autocomplete_fields = \"non_list_or_tuple\"\n\n        self.assertIsInvalid(\n            MyModelAdmin,\n            ValidationTestModel,\n            \"The value of 'autocomplete_fields' must be a list or tuple.\",\n            id=\"admin.E036\",\n        )\n", "    def test_filter_vertical_invalid_type(self):\n        class InvalidFilterAdmin(admin.ModelAdmin):\n            list_filter = \"invalid\"\n\n        self.assertIsInvalid(\n            InvalidFilterAdmin,\n            ValidationTestModel,\n            \"The value of 'list_filter' must be a list or tuple.\",\n            id=\"admin.E112\",\n        )\n"], "sample_404": ["def test_variable_node_render_unicode_error(self):\n    class BadString:\n            raise UnicodeDecodeError(\"utf-8\", b\"\\x80\", 0, 1, \"invalid start byte\")\n\n    engine = self._engine()\n    template = engine.from_string(\"{{ bad_string }}\")\n    context = Context({\"bad_string\": BadString()})\n    self.assertEqual(template.render(context), \"\")\n", "def test_variable_node_rendering(self):\n    engine = self._engine()\n    template = engine.from_string(\"{{ var }}\")\n    context = Context({\"var\": \"Test Value\"})\n    rendered = template.render(context)\n    self.assertEqual(rendered, \"Test Value\")\n", "def test_variable_node_render_with_autoescape(self):\n    template = self._engine().from_string(\"{{ var }}\")\n    context = Context({\"var\": \"<script>alert('Hello, world!');</script>\"})\n    context.autoescape = True\n    rendered = template.render(context)\n    self.assertEqual(rendered, \"&lt;script&gt;alert(&#39;Hello, world!&#39;);&lt;/script&gt;\")\n", "def test_filter_expression_args_check(self):\n    # Test the args_check method of FilterExpression class\n        pass\n\n        pass\n\n        pass\n\n        pass\n\n    # Test when the number of provided arguments is less than the required arguments\n    with self.assertRaises(TemplateSyntaxError):\n        FilterExpression.args_check(\"func_one_arg\", func_one_arg, [])\n\n    # Test when the number of provided arguments is equal to the required arguments\n    self.assertTrue(FilterExpression.args_check(\"func_one_arg\", func_one_arg, [(True, \"var\")]))\n\n    # Test when the number of provided arguments is greater than the required arguments\n    with self.assertRaises(TemplateSyntaxError):\n        FilterExpression.args_check(\"func_one_arg\", func_one_arg, [(True, \"var1\"), (True, \"var2\")])\n\n    # Test when the function has default arguments\n    self.assertTrue(FilterExpression.args_check(\"func_default_args\", func_default_args, []))\n    self.assertTrue(FilterExpression.args_check(\"func_default_args\", func_default_args, [(True, \"var\")]))\n    self.assertTrue(FilterExpression.args_check(\"func_default_args\", func_default_args, [(True, \"var1\"), (True, \"var2\")]))\n", "def test_localize_rendering(self):\n    \"\"\"\n    Test that a template is correctly localized and rendered.\n    \"\"\"\n    engine = self._engine()\n    template = engine.from_string(\"{{ value|localize }}\")\n    context = Context({\"value\": 1234.56})\n    with translation.override(\"de\"):\n        self.assertEqual(template.render(context), \"1.234,56\")\n"], "sample_149": ["    def test_non_unique_username_field(self):\n        errors = check_user_model()\n        self.assertEqual(len(errors), 1)\n        self.assertIsInstance(errors[0], checks.Warning)\n        self.assertEqual(errors[0].id, 'auth.W004')\n", "def test_non_unique_username(self):\n    errors = check_user_model()\n    self.assertEqual(len(errors), 1)\n    self.assertEqual(errors[0].id, 'auth.W004')\n", "    def test_user_model_non_unique_username(self):\n        errors = check_user_model()\n        self.assertEqual(len(errors), 1)\n        self.assertEqual(errors[0].id, 'auth.W004')\n", "def test_non_unique_username_field(self):\n    errors = check_user_model(self.apps.get_app_configs())\n    self.assertEqual(len(errors), 1)\n    self.assertIsInstance(errors[0], checks.Warning)\n    self.assertEqual(errors[0].id, 'auth.W004')\n", "    def test_non_unique_username_warning(self):\n        errors = check_user_model(self.apps.get_models())\n        self.assertEqual(len(errors), 1)\n        self.assertIsInstance(errors[0], checks.Warning)\n        self.assertEqual(errors[0].id, 'auth.W004')\n"], "sample_830": ["def test_get_blas_info():\n    from sklearn.utils._show_versions import _get_blas_info\n\n    blas_info = _get_blas_info()\n    assert 'macros' in blas_info\n    assert 'lib_dirs' in blas_info\n    assert 'cblas_libs' in blas_info\n", "def test_get_blas_info():\n    blas_info = _get_blas_info()\n    assert 'macros' in blas_info\n    assert 'lib_dirs' in blas_info\n    assert 'cblas_libs' in blas_info\n", "def test_get_blas_info():\n    mock_blas_info = {'define_macros': [('HAVE_CBLAS', None)],\n                      'library_dirs': ['/usr/lib'],\n                      'libraries': ['cblas']}\n    mock_get_blas_info = MagicMock(return_value=(['cblas'], mock_blas_info))\n\n    with patch('sklearn._build_utils.get_blas_info', mock_get_blas_info):\n        blas_info = _get_blas_info()\n\n    assert blas_info['macros'] == 'HAVE_CBLAS=None'\n    assert blas_info['lib_dirs'] == '/usr/lib'\n    assert blas_info['cblas_libs'] == 'cblas'\n", "def test_get_blas_info():\n    mock_get_blas_info = MagicMock(return_value=(['cblas'], {'define_macros': [('MACRO', '1')], 'library_dirs': ['/path']}))\n\n    with patch('sklearn._build_utils.get_blas_info', mock_get_blas_info):\n        blas_info = _get_blas_info()\n        assert blas_info['macros'] == 'MACRO=1'\n        assert blas_info['lib_dirs'] == '/path'\n        assert blas_info['cblas_libs'] == 'cblas'\n", "def test_get_blas_info():\n    blas_info = _get_blas_info()\n    assert isinstance(blas_info, dict)\n    assert 'macros' in blas_info\n    assert 'lib_dirs' in blas_info\n    assert 'cblas_libs' in blas_info\n\n    # Check if values are not None\n    assert blas_info['macros'] is not None\n    assert blas_info['lib_dirs'] is not None\n    assert blas_info['cblas_libs'] is not None\n\n    # Check if values are of correct type\n    assert isinstance(blas_info['macros'], str)\n    assert isinstance(blas_info['lib_dirs'], str)\n    assert isinstance(blas_info['cblas_libs'], str)\n"], "sample_414": ["def test_formfield_for_datefield(self):\n    self.assertFormfield(Car, 'year', widgets.AdminDateWidget)\n", "def test_formfield_for_foreignkey(self):\n    # Test that the correct widget is used for ForeignKey fields\n    self.assertFormfield(Car, 'owner', widgets.AutocompleteSelect)\n", "def test_formfield_for_charfield(self):\n    # Test formfield_for_dbfield for CharField with default parameters\n    ff = self.assertFormfield(Car, \"make\", forms.TextInput)\n    self.assertEqual(ff.max_length, 255)\n\n    # Test formfield_for_dbfield for CharField with overridden widget\n    custom_widget = forms.Textarea\n    ff = self.assertFormfield(Car, \"make\", custom_widget, formfield_overrides={CharField: {\"widget\": custom_widget}})\n    self.assertIsInstance(ff.widget, custom_widget)\n", "def test_formfield_for_foreignkey(self):\n    \"\"\"\n    Test formfield_for_dbfield for ForeignKey fields.\n    \"\"\"\n    # Test default widget for ForeignKey\n    self.assertFormfield(Car, 'owner', widgets.AutoCompleteSelect)\n\n    # Test widget override for ForeignKey\n    self.assertFormfield(Car, 'owner', widgets.ForeignKeyRawIdWidget, raw_id_fields=['owner'])\n\n    # Test widget override for ForeignKey in autocomplete_fields\n    self.assertFormfield(Car, 'owner', widgets.AdminTextInputWidget, autocomplete_fields=['owner'])\n\n    # Test widget override for ForeignKey in radio_fields\n    self.assertFormfield(Car, 'owner', widgets.AdminRadioSelect, radio_fields={'owner': admin.VERTICAL})\n", "    def test_datefield_widget(self):\n        self.assertFormfield(Car, \"purchase_date\", widgets.AdminDateWidget)\n"], "sample_321": ["def test_post_request_without_token(self):\n    req = self._get_POST_csrf_cookie_request()\n    middleware = CsrfViewMiddleware()\n    response = middleware.process_view(req, post_form_view, (), {})\n    self.assertEqual(response.status_code, 403)\n    self.assertContains(response, REASON_CSRF_TOKEN_MISSING, status_code=403)\n", "def test_csrf_origin_check(self):\n    # Test that CSRF middleware checks the origin of the request\n    req = self._get_POST_csrf_cookie_request()\n    req.META['HTTP_ORIGIN'] = 'http://evil.com'\n    resp = CsrfViewMiddleware().process_view(req, post_form_view, (), {})\n    self.assertEqual(resp.status_code, 403)\n    self.assertEqual(resp.reason_phrase, REASON_BAD_ORIGIN % 'http://evil.com')\n", "def test_process_request_with_invalid_token(self):\n    req = self._get_GET_csrf_cookie_request(cookie='invalid_token')\n    middleware = CsrfViewMiddleware()\n    middleware.process_request(req)\n    self.assertNotEqual(req.META['CSRF_COOKIE'], 'invalid_token')\n", "def test_post_form_view_no_token(self):\n    req = self._get_POST_csrf_cookie_request()\n    resp = post_form_view(req)\n    self.assertEqual(resp.status_code, 403)\n    self.assertEqual(resp.reason_phrase, REASON_CSRF_TOKEN_MISSING)\n", "def test_process_view_invalid_csrf_token(self):\n    req = self._get_POST_request_with_token('invalid_token')\n    middleware = CsrfViewMiddleware()\n    response = middleware.process_view(req, None, (), {})\n    self.assertEqual(response.status_code, 403)\n    self.assertContains(response, REASON_CSRF_TOKEN_INCORRECT)\n"], "sample_714": ["def test_brier_score_loss():\n    y_true = np.array([0, 1, 1, 0])\n    y_prob = np.array([0.1, 0.9, 0.8, 0.3])\n    expected_score = 0.037\n    assert_almost_equal(brier_score_loss(y_true, y_prob), expected_score)\n", "def test_brier_score_loss():\n    y_true = np.array([0, 1, 1, 0])\n    y_prob = np.array([0.1, 0.9, 0.8, 0.3])\n    expected_score = 0.0375\n    assert_almost_equal(brier_score_loss(y_true, y_prob), expected_score)\n", "def test_hinge_loss():\n    y_true = np.array([0, 1, 2, 3])\n    pred_decision = np.array([[0.1, 0.2, 0.3, 0.4],\n                              [0.5, 0.6, 0.7, 0.8],\n                              [0.9, 0.1, 0.2, 0.3],\n                              [0.4, 0.3, 0.2, 0.1]])\n    labels = np.array([0, 1, 2, 3])\n    expected_loss = 0.75\n    assert_almost_equal(hinge_loss(y_true, pred_decision, labels), expected_loss)\n", "def test_brier_score_loss():\n    y_true = np.array([0, 1, 1, 0])\n    y_prob = np.array([0.1, 0.9, 0.8, 0.3])\n    score = brier_score_loss(y_true, y_prob)\n    assert_almost_equal(score, 0.037, decimal=3)\n\n    y_true_categorical = np.array([\"spam\", \"ham\", \"ham\", \"spam\"])\n    score_categorical = brier_score_loss(y_true_categorical, y_prob, pos_label=\"ham\")\n    assert_almost_equal(score_categorical, 0.037, decimal=3)\n\n    y_pred = np.array([0, 1, 1, 0])\n    score_binary = brier_score_loss(y_true, y_pred)\n    assert_almost_equal(score_binary, 0.0)\n", "def test_brier_score_loss():\n    # Test the brier_score_loss function\n    y_true = np.array([0, 1, 1, 0])\n    y_prob = np.array([0.1, 0.9, 0.8, 0.3])\n    expected_score = 0.037\n    assert_almost_equal(brier_score_loss(y_true, y_prob), expected_score)\n\n    y_true_categorical = np.array([\"spam\", \"ham\", \"ham\", \"spam\"])\n    assert_almost_equal(brier_score_loss(y_true_categorical, y_prob, pos_label=\"ham\"), expected_score)\n\n    # Test with sample_weight\n    sample_weight = np.array([0.1, 0.2, 0.3, 0.4])\n    weighted_score = np.average((y_true - y_prob) ** 2, weights=sample_weight)\n    assert_almost_equal(brier_score_loss(y_true, y_prob, sample_weight=sample_weight), weighted_score)\n\n    # Test with invalid inputs\n    y_true_invalid = np.array([0, 1, 2, 0])\n    assert_raise_message(ValueError, \"Only binary classification is supported\", brier_score_loss, y_true_invalid, y_prob)\n\n    y_prob_invalid = np.array([0.1, 1.1, 0.8, 0.3])\n    assert_raise_message(ValueError, \"y_prob contains values greater than 1.\", brier_score_loss, y_true, y_prob_invalid)\n\n    y_prob_invalid = np.array([0.1, -0.1, 0.8, 0.3])\n    assert_raise_message(ValueError, \"y_prob contains values less than 0.\", brier_score_loss, y_true, y_prob_invalid)\n"], "sample_622": ["def test_bool_type_array():\n    x = np.array([1, 0, 1, 1, 0], dtype=\"i1\")\n    bool_array = coding.BoolTypeArray(x)\n    assert bool_array.dtype == np.dtype(\"bool\")\n    assert np.array_equal(bool_array[slice(None)], x.astype(dtype=\"bool\"))\n", "def test_bool_type_array_getitem():\n    x = np.array([1, 0, 1, 1, 0], dtype=\"i1\")\n    bool_array = coding.BoolTypeArray(x)\n    indexer = coding.indexing.BasicIndexer((slice(None),))\n    result = bool_array[indexer]\n    assert result.dtype == np.bool_\n    assert_array_equal(result, np.array([True, False, True, True, False]))\n", "def test_decode_cf_variable_bool_type():\n    data = np.array([1, 0, 1, 1, 0], dtype=\"i1\")\n    var = Variable((\"x\",), data, attrs={\"dtype\": \"bool\"})\n\n    decoded_var = coding.decode_cf_variable(\"test\", var)\n\n    assert decoded_var.dtype == np.dtype(\"bool\")\n    assert_array_equal(decoded_var.data, np.array([True, False, True, True, False]))\n", "def test_decode_cf_bool_type_array():\n    data = np.array([1, 0, 1, 1, 0], dtype=\"i1\")\n    decoded_var = coding.decode_cf_variable(\"test\", Variable((\"x\",), data), decode_times=False)\n    expected_var = Variable((\"x\",), coding.BoolTypeArray(data))\n    assert_identical(decoded_var, expected_var)\n", "def test_bool_type_array(self):\n    x = np.array([1, 0, 1, 1, 0], dtype=\"i1\")\n    bool_array = coding.BoolTypeArray(x)\n    assert bool_array.dtype == np.dtype(\"bool\")\n    indexer = indexing.BasicIndexer((slice(None),))\n    assert bool_array[indexer].dtype == np.dtype(\"bool\")\n    assert_array_equal(bool_array[indexer], np.array([True, False, True, True, False]))\n"], "sample_1051": ["def test_dotprint_with_float():\n    expr = Float(3.14159)\n    result = dotprint(expr)\n    expected = 'digraph{\\n\\n# Graph style\\n\"ordering\"=\"out\"\\n\"rankdir\"=\"TD\"\\n\\n#########\\n# Nodes #\\n#########\\n\\n\"Float(3.14159)_()\" [\"color\"=\"black\", \"label\"=\"Float\", \"shape\"=\"ellipse\"];\\n\\n#########\\n# Edges #\\n#########\\n\\n}'\n    assert result == expected\n", "def test_dotprint_float():\n    f = Float(3.14)\n    result = dotprint(f)\n    expected = '\"Float(3.14,)' + ' [\"color\"=\"black\", \"label\"=\"3.14\", \"shape\"=\"ellipse\"];'\n    assert result.split('\\n')[6] == expected\n", "def test_dotprint():\n    # Test the dotprint function\n    expr = x + 2*x\n    result = dotprint(expr, repeat=True, labelfunc=srepr)\n    # Assert that the result is a string and contains the expected substrings\n    assert isinstance(result, str)\n    assert \"Add(Integer(2), Mul(Integer(2), Symbol('x')))_()\" in result\n    assert \"Integer(2)_(0,)\" in result\n    assert \"Integer(2)_(1, 0)\" in result\n    assert \"Symbol('x')_(1, 1)\" in result\n", "def test_dotprint_float():\n    expr = Float(3.14)\n    result = dotprint(expr)\n    expected_output = \\\n    \"\"\"digraph{\n\n    # Graph style\n    \"ordering\"=\"out\"\n    \"rankdir\"=\"TD\"\n\n    #########\n    # Nodes #\n    #########\n\n    \"Float(3.14, dps=15, prec=53)_()\" [\"color\"=\"black\", \"label\"=\"Float\", \"shape\"=\"ellipse\"];\n\n    #########\n    # Edges #\n    #########\n\n    }\"\"\"\n    assert result == expected_output\n", "def test_dotprint_with_custom_labelfunc():\n        return srepr(expr).replace(\"'\", \"\")\n\n    expected_output = 'digraph{\\n\\n# Graph style\\n\"ordering\"=\"out\"\\n\"rankdir\"=\"TD\"\\n\\n#########\\n# Nodes #\\n#########\\n\\n\"Add(Integer(2), Symbol(x))_()\" [\"color\"=\"black\", \"label\"=\"Add\", \"shape\"=\"ellipse\"];\\n\"Integer(2)_(0,)\" [\"color\"=\"black\", \"label\"=\"Integer(2)\", \"shape\"=\"ellipse\"];\\n\"Symbol(x)_(1,)\" [\"color\"=\"black\", \"label\"=\"Symbol(x)\", \"shape\"=\"ellipse\"];\\n\\n#########\\n# Edges #\\n#########\\n\\n\"Add(Integer(2), Symbol(x))_()\" -> \"Integer(2)_(0,)\";\\n\"Add(Integer(2), Symbol(x))_()\" -> \"Symbol(x)_(1,)\";\\n}'\n\n    assert dotprint(x+2, labelfunc=custom_labelfunc) == expected_output\n"], "sample_495": ["    def test_paginator_iter(self):\n        \"\"\"\n        Test the __iter__ method of the Paginator class.\n        \"\"\"\n        object_list = list(range(1, 101))\n        paginator = Paginator(object_list, 10)\n        for i, page in enumerate(paginator, start=1):\n            self.assertEqual(page.number, i)\n            self.assertEqual(list(page), object_list[(i-1)*10:i*10])\n", "def test_validate_number(self):\n    \"\"\"\n    Test the validate_number method of Paginator.\n    \"\"\"\n    paginator = Paginator(range(100), 10)\n\n    # Test valid page number\n    self.assertEqual(paginator.validate_number(1), 1)\n\n    # Test non-integer page number\n    with self.assertRaises(PageNotAnInteger):\n        paginator.validate_number('a')\n\n    # Test page number less than 1\n    with self.assertRaises(EmptyPage):\n        paginator.validate_number(0)\n\n    # Test page number greater than num_pages\n    with self.assertRaises(EmptyPage):\n        paginator.validate_number(11)\n\n    # Test allow_empty_first_page\n    paginator = Paginator(range(5), 10, allow_empty_first_page=False)\n    with self.assertRaises(EmptyPage):\n        paginator.validate_number(1)\n\n    # Test float page number\n    with self.assertRaises(PageNotAnInteger):\n        paginator.validate_number(1.5)\n", "    def test_validate_number(self):\n        \"\"\"\n        Test validate_number method of the Paginator class.\n        \"\"\"\n        paginator = Paginator(range(100), 10)\n\n        # Test valid page number\n        self.assertEqual(paginator.validate_number(1), 1)\n        self.assertEqual(paginator.validate_number(5), 5)\n        self.assertEqual(paginator.validate_number(10), 10)\n\n        # Test non-integer page number\n        with self.assertRaises(PageNotAnInteger):\n            paginator.validate_number(1.5)\n\n        # Test page number less than 1\n        with self.assertRaises(EmptyPage):\n            paginator.validate_number(0)\n\n        # Test page number greater than num_pages\n        with self.assertRaises(EmptyPage):\n            paginator.validate_number(11)\n\n        # Test empty first page\n        self.assertEqual(paginator.validate_number(1), 1)\n\n        # Test non-empty first page\n        paginator = Paginator(range(100), 10, allow_empty_first_page=False)\n        with self.assertRaises(EmptyPage):\n            paginator.validate_number(1)\n", "    def test_page_repr(self):\n        paginator = Paginator([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 2)\n        page = paginator.page(3)\n        self.assertEqual(repr(page), '<Page 3 of 5>')\n", "def test_validate_number(self):\n    \"\"\"\n    Test the validate_number method of the Paginator class.\n    \"\"\"\n    paginator = Paginator(range(100), 10)\n\n    # Test that valid integers are returned as is\n    self.assertEqual(paginator.validate_number(1), 1)\n    self.assertEqual(paginator.validate_number(5), 5)\n    self.assertEqual(paginator.validate_number(10), 10)\n\n    # Test that non-integers raise a PageNotAnInteger exception\n    with self.assertRaises(PageNotAnInteger):\n        paginator.validate_number(1.5)\n        paginator.validate_number('1')\n\n    # Test that numbers less than 1 raise an EmptyPage exception\n    with self.assertRaises(EmptyPage):\n        paginator.validate_number(0)\n        paginator.validate_number(-1)\n\n    # Test that numbers greater than the number of pages raise an EmptyPage exception\n    with self.assertRaises(EmptyPage):\n        paginator.validate_number(11)\n        paginator.validate_number(100)\n"], "sample_589": ["def test_interp_na_with_scipy_interpolator(ds):\n    interp_ds = ds.interp_na(dim=\"time\", method=\"cubic\")\n    assert isinstance(interp_ds.data_vars[\"var1\"].interpolator, ScipyInterpolator)\n", "def test_scipy_interpolator(da):\n    interpolator = ScipyInterpolator(da.time.values, da.values, method='linear')\n    result = interpolator(da.time.values)\n    expected = da.interpolate_na(dim='time', method='linear').values\n    np.testing.assert_allclose(result, expected, equal_nan=True)\n", "def test_get_clean_interp_index_with_datetime_index():\n    da, df = make_interpolate_example_data((10, 5), 0.2, non_uniform=True)\n    index = get_clean_interp_index(da, \"time\", use_coordinate=True)\n    assert_array_equal(index, da.time.values.astype(np.float64))\n", "def test_get_clean_interp_index():\n    da, _ = make_interpolate_example_data(shape=(10, 5), frac_nan=0.2)\n    index = get_clean_interp_index(da, \"time\", use_coordinate=True)\n    assert isinstance(index, pd.DatetimeIndex)\n    assert index.is_monotonic\n    assert index.is_unique\n    assert index.name == \"time\"\n\n    index = get_clean_interp_index(da, \"time\", use_coordinate=False)\n    assert np.array_equal(index, np.arange(10, dtype=np.float64))\n\n    da[\"time\"] = [1, 2, 3, 3, 4]\n    with pytest.raises(ValueError, match=\"Index 'time' has duplicate values\"):\n        get_clean_interp_index(da, \"time\", use_coordinate=True)\n\n    da[\"time\"] = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n    with pytest.raises(TypeError, match=\"Index 'time' must be castable to float64 to support interpolation, got str.\"):\n        get_clean_interp_index(da, \"time\", use_coordinate=True)\n\n    da[\"time\"] = pd.to_datetime(da[\"time\"])\n    da[\"x\"] = [1, 2, 3, 4, 5]\n    da = da.set_index(time=\"x\")\n    with pytest.raises(ValueError, match=\"Coordinates used for interpolation must be 1D, x is 2D.\"):\n        get_clean_interp_index(da, \"time\", use_coordinate=\"x\")\n", "def test_ScipyInterpolator_with_NaN(da):\n    xi = da.time.values\n    yi = da.values\n\n    interpolator = ScipyInterpolator(xi, yi, method=\"linear\", fill_value=None)\n    result = interpolator(da.time.values)\n\n    assert not np.isnan(result).any(), \"Interpolation should not result in NaN values\"\n"], "sample_353": ["def test_create_superuser_with_validation_bypass(self):\n    # Test creating a superuser with password validation bypassed\n    out = StringIO()\n    call_command('createsuperuser', username='alice', email='alice@example.com', stdin=MockTTY(), stdout=out)\n    self.assertIn('Superuser created successfully.', out.getvalue())\n    self.assertTrue(User.objects.filter(username='alice').exists())\n", "def test_create_superuser_with_email(self):\n    out = StringIO()\n    management.call_command('createsuperuser', username='alice', email='alice@example.com', stdin=MockTTY(), stdout=out)\n    self.assertIn('Superuser created successfully.', out.getvalue())\n    self.assertTrue(User.objects.filter(username='alice').exists())\n    self.assertEqual(User.objects.get(username='alice').email, 'alice@example.com')\n", "def test_create_superuser_with_password(self):\n    stdout, stderr = StringIO(), StringIO()\n    call_command('createsuperuser', username='bob', interactive=True, stdin=MockTTY(), stdout=stdout, stderr=stderr)\n    self.assertIn('Superuser created successfully.', stdout.getvalue())\n    self.assertFalse(stderr.getvalue())\n    self.assertTrue(User.objects.filter(username='bob', is_superuser=True).exists())\n", "def test_createsuperuser_with_invalid_password_and_bypass(self):\n    out = StringIO()\n    with mock.patch('sys.stdout', out):\n        management.call_command('createsuperuser', username='test', interactive=False, stdin=MockTTY())\n    self.assertIn('Bypass password validation and create user anyway? [y/N]:', out.getvalue())\n    self.assertTrue(User.objects.filter(username='test').exists())\n", "def test_password_validation_failure(self):\n    # Test case where password validation fails and the user decides not to bypass it\n    out = StringIO()\n    err = StringIO()\n    with self.assertRaises(SystemExit) as cm, mock.patch('sys.stdin', MockTTY()):\n        call_command('createsuperuser', stdout=out, stderr=err, interactive=True)\n    self.assertEqual(cm.exception.code, 1)\n    self.assertIn('Error: Your passwords didn\\'t match.', err.getvalue())\n    self.assertIn('Bypass password validation and create user anyway? [y/N]: ', out.getvalue())\n"], "sample_95": ["def test_fully_decorated(self):\n    request = HttpRequest()\n    response = fully_decorated(request)\n    self.assertEqual(response.status_code, 200)\n    self.assertIn(b'<html><body>dummy</body></html>', response.content)\n    self.assertEqual(fully_decorated.__doc__, 'Expected __doc__')\n    self.assertEqual(fully_decorated.anything, 'Expected __dict__')\n", "def test_fully_decorated_doc_and_dict(self):\n    # Test that the original function's docstring and __dict__ are preserved\n    self.assertEqual(fully_decorated.__doc__, \"Expected __doc__\")\n    self.assertEqual(fully_decorated.anything, \"Expected __dict__\")\n", "def test_patch_cache_control(self):\n    response = HttpResponse()\n    patch_cache_control(response, max_age=60, private=True, no_cache=False)\n    self.assertEqual(response['Cache-Control'], 'max-age=60, private')\n\n    patch_cache_control(response, public=True, max_age=120)\n    self.assertEqual(response['Cache-Control'], 'max-age=60, public')\n\n    patch_cache_control(response, no_store=True)\n    self.assertEqual(response['Cache-Control'], 'max-age=60, public, no-store')\n", "    def test_patch_cache_control(self):\n        response = HttpResponse()\n        patch_cache_control(response, max_age=120, private=True)\n        self.assertEqual(response['Cache-Control'], 'max-age=120, private')\n\n        patch_cache_control(response, public=True, max_age=90)\n        self.assertEqual(response['Cache-Control'], 'max-age=90, public')\n\n        patch_cache_control(response, no_cache=True)\n        self.assertEqual(response['Cache-Control'], 'max-age=90, public, no-cache')\n", "    def test_patch_response_headers(self):\n        response = HttpResponse()\n        patch_response_headers(response, cache_timeout=100)\n        self.assertIn('Expires', response)\n        self.assertIn('Cache-Control', response)\n        self.assertIn('max-age=100', response['Cache-Control'])\n\n        response = HttpResponse()\n        response['Expires'] = 'Some Date'\n        response['Cache-Control'] = 'no-cache'\n        patch_response_headers(response, cache_timeout=200)\n        self.assertEqual(response['Expires'], 'Some Date')\n        self.assertEqual(response['Cache-Control'], 'no-cache, max-age=200')\n"], "sample_113": ["    def test_simplify_regex(self):\n        self.assertEqual(simplify_regex(r'^(?P<a>\\w+)/b/(\\w+)$'), '^<a>/b/<var>')\n        self.assertEqual(simplify_regex(r'^(?P<a>\\w+)/b/(?P<c>\\w+)/$'), '^<a>/b/<c>/')\n        self.assertEqual(simplify_regex(r'^(?P<a>\\w+)/b/(\\w+)'), '^<a>/b/<var>')\n        self.assertEqual(simplify_regex(r'^(?P<a>\\w+)/b/(?P<c>\\w+)'), '^<a>/b/<c>')\n        self.assertEqual(simplify_regex(r'^(?P<a>\\w+)/b/((x|y)\\w+)$'), '^<a>/b/<var>')\n        self.assertEqual(simplify_regex(r'^(?P<a>\\w+)/b/((x|y)\\w+)'), '^<a>/b/<var>')\n", "    def test_replace_named_groups(self):\n        pattern = \"^(?P<a>\\w+)/b/(\\w+)$\"\n        expected_output = \"^<a>/b/(\\w+)$\"\n        self.assertEqual(utils.replace_named_groups(pattern), expected_output)\n\n        pattern = \"^(?P<a>\\w+)/b/(?P<c>\\w+)/$\"\n        expected_output = \"^<a>/b/<c>/$\"\n        self.assertEqual(utils.replace_named_groups(pattern), expected_output)\n\n        pattern = \"^(?P<a>\\w+)/b/(\\w+)\"\n        expected_output = \"^<a>/b/(\\w+)\"\n        self.assertEqual(utils.replace_named_groups(pattern), expected_output)\n\n        pattern = \"^(?P<a>\\w+)/b/(?P<c>\\w+)\"\n        expected_output = \"^<a>/b/<c>\"\n        self.assertEqual(utils.replace_named_groups(pattern), expected_output)\n", "    def test_simplify_regex(self):\n        self.assertEqual(simplify_regex(r'^(?P<a>\\w+)/b/(\\w+)$'), '^<a>/b/<var>$')\n        self.assertEqual(simplify_regex(r'^(?P<a>\\w+)/b/(?P<c>\\w+)/$'), '^<a>/b/<c>/$')\n        self.assertEqual(simplify_regex(r'^(?P<a>\\w+)/b/(\\w+)'), '^<a>/b/<var>')\n        self.assertEqual(simplify_regex(r'^(?P<a>\\w+)/b/(?P<c>\\w+)'), '^<a>/b/<c>')\n        self.assertEqual(simplify_regex(r'^b/((x|y)\\w+)$'), '^b/<var>$')\n        self.assertEqual(simplify_regex(r'^b/((x|y)\\w+)'), '^b/<var>')\n", "    def test_replace_named_groups(self):\n        pattern = \"^(?P<a>\\w+)/b/(\\w+)$\"\n        expected_output = \"^<a>/b/(\\w+)$\"\n        self.assertEqual(simplify_regex(pattern), expected_output)\n", "    def test_simplify_regex(self):\n        patterns = [\n            r'^(?P<year>[0-9]{4})/(?P<month>[0-9]{2})/(?P<day>[0-9]{2})/(?P<slug>[-\\w]+)/$',\n            r'^(?P<pk>\\d+)/$',\n            r'^(?P<year>\\d{4})/(?P<month>\\d{2})/(?P<day>\\d{2})/(?P<slug>[-\\w]+)/$',\n            r'^(?P<pk>\\d+)/$',\n            r'^(?P<pk>\\d+)/(?P<slug>[-\\w]+)/$',\n            r'^(?P<pk>\\d+)/(?P<operation>[-\\w]+)/$',\n        ]\n        expected_results = [\n            r'^<year>/<month>/<day>/<slug>/$',\n            r'^<pk>/$',\n            r'^<year>/<month>/<day>/<slug>/$',\n            r'^<pk>/$',\n            r'^<pk>/<slug>/$',\n            r'^<pk>/<operation>/$',\n        ]\n        for pattern, expected in zip(patterns, expected_results):\n            self.assertEqual(simplify_regex(pattern), expected)\n"], "sample_944": ["def test_stringify(annotation, expected):\n    assert stringify(annotation) == expected\n", "def test_stringify(annotation, expected):\n    assert stringify(annotation) == expected\n", "def test_restify():\n    # Test restify function with built-in classes\n    assert restify(Integral) == ':class:`numbers.Integral`'\n    assert restify(Struct) == ':class:`struct.Struct`'\n    assert restify(TracebackType) == ':class:`types.TracebackType`'\n\n    # Test restify function with custom classes\n    assert restify(MyClass1) == ':class:`test_util_typing.MyClass1`'\n    assert restify(MyClass2) == ':class:`<MyClass2>`'\n\n    # Test restify function with typing module classes\n    assert restify(TypeVar('T')) == ':class:`T`'\n    assert restify(MyInt) == ':class:`MyInt`'\n    assert restify(MyList) == ':class:`test_util_typing.MyList`'\n    assert restify(BrokenType) == ':obj:`test_util_typing.BrokenType`'\n\n    # Test restify function with Union\n    assert restify(Optional[int]) == 'Optional[int]'\n    assert restify(Union[int, str]) == 'int | str'\n\n    # Test restify function with Callable\n    assert restify(Callable[[int], str]) == ':obj:`Callable`\\\\ [[int], str]'\n\n    # Test restify function with NoneType\n    assert restify(None) == ':obj:`None`'\n    assert restify(type(None)) == ':obj:`None`'\n\n    # Test restify function with Ellipsis\n    assert restify(Ellipsis) == '...'\n", "def test_restify():\n    assert restify(None) == \":obj:`None`\"\n    assert restify(MyClass1) == \":class:`MyClass1`\"\n    assert restify(MyClass2) == \":class:`<MyClass2>`\"\n    assert restify(Integral) == \":class:`numbers.Integral`\"\n    assert restify(Struct) == \":class:`struct.Struct`\"\n    assert restify(TracebackType) == \":class:`types.TracebackType`\"\n    assert restify(MyInt) == \":class:`MyInt`\"\n    assert restify(MyList[int]) == \":class:`MyList`\\\\ [:class:`int`]\"\n    assert restify(Optional[int]) == \":obj:`Optional`\\\\ [:class:`int`]\"\n    assert restify(Optional[Optional[int]]) == \":obj:`Optional`\\\\ [:obj:`Optional`\\\\ [:class:`int`]]\"\n    assert restify(Union[int, str]) == \":obj:`Union`\\\\ [:class:`int`, :class:`str`]\"\n    assert restify(Union[int, str, None]) == \":obj:`Optional`\\\\ [:obj:`Union`\\\\ [:class:`int`, :class:`str`]]\"\n    assert restify(Tuple[int, str]) == \":class:`tuple`\\\\ [:class:`int`, :class:`str`]\"\n    assert restify(Callable[[int], str]) == \":class:`Callable`\\\\ [[:class:`int`], :class:`str`]\"\n    assert restify(Callable[[], None]) == \":class:`Callable`\\\\ [[], :obj:`None`]\"\n    assert restify(BrokenType) == \":class:`object`\"\n", "    def test_stringify(self):\n        assert stringify(T) == \"T\"\n        assert stringify(MyInt) == \"MyInt\"\n        assert stringify(MyList[int]) == \"MyList[int]\"\n        assert stringify(BrokenType) == \"<class '__main__.BrokenType'>\"\n        assert stringify(Optional[int]) == \"Optional[int]\"\n        assert stringify(Union[int, str]) == \"Union[int, str]\"\n        assert stringify(Callable[[int], str]) == \"Callable[[int], str]\"\n        assert stringify(TracebackType) == \"types.TracebackType\"\n        assert stringify(Struct('>i')) == \"struct.Struct\"\n"], "sample_37": ["def test_find_all_wcs(self):\n    for filename in self._file_list:\n        header = fits.Header.fromtextfile(filename)\n        wcses = wcs.find_all_wcs(header)\n        assert len(wcses) > 0\n", "def test_maps_sip(self):\n    \"\"\"\n    Test that we can parse SIP coefficients from the maps test data files.\n    \"\"\"\n    for filename in self._file_list:\n        with open(filename, 'rb') as f:\n            header = fits.Header.fromtextfile(f)\n        w = wcs.WCS(header)\n        assert w.sip is not None\n", "def test_maps_files(self):\n    for filename in self._file_list:\n        hdr = fits.Header.fromtextfile(filename)\n        with fits.open(filename) as hdulist:\n            w = wcs.WCS(hdr, hdulist)\n            # Perform some basic checks\n            assert w.naxis == 2\n            assert w.wcs.naxis == 2\n            assert w.wcs.lng == 0\n            assert w.wcs.lat == 1\n            assert w.wcs.lngtyp == 'RA'\n            assert w.wcs.lattyp == 'DEC'\n            assert w.wcs.alt == ' '\n            # Perform a round trip check\n            hdr2 = w.to_header()\n            w2 = wcs.WCS(hdr2)\n            assert_array_almost_equal(w.wcs.crval, w2.wcs.crval)\n            assert_array_almost_equal(w.wcs.crpix, w2.wcs.crpix)\n            assert_array_almost_equal(w.wcs.cd, w2.wcs.cd)\n", "def test_fits_single_hdu(self):\n    \"\"\"\n    Test that the WCS is initialized correctly from a single-HDU fits file.\n    \"\"\"\n    # Open a fits file with a single HDU containing a WCS\n    with fits.open(self._file_list[0]) as hdulist:\n        # Initialize WCS from the fits file\n        w = wcs.WCS(hdulist[0].header)\n        # Check that the WCS is not None\n        assert w is not None\n", "def test_validate(self):\n    results = wcs.validate(get_pkg_data_filename('data/3d-wcs.hdr'))\n    assert len(results) == 1\n    assert len(results[0]) == 1\n    assert 'Trying to replace CRVAL2 with' in results[0][0]\n    assert 'PC2_1 is not close to zero' in results[0][0]\n    assert 'CDELT1 is not close to CDELT2' in results[0][0]\n    assert 'PC1_2 is not close to zero' in results[0][0]\n    assert 'PC1_1 is not close to PC2_2' in results[0][0]\n"], "sample_878": ["def test_column_transformer_with_dataframe_input():\n    import pandas as pd\n\n    X = pd.DataFrame(\n        {\n            \"city\": [\"London\", \"London\", \"Paris\", \"Sallisaw\"],\n            \"rating\": [5, 3, 4, 5],\n            \"price\": [100, 200, 300, 400],\n        }\n    )\n    ct = make_column_transformer(\n        (StandardScaler(), make_column_selector(dtype_include=np.number)),\n        (OneHotEncoder(), make_column_selector(dtype_include=object)),\n    )\n    transformed_X = ct.fit_transform(X)\n    assert transformed_X.shape == (4, 7)\n", "def test_transformer_list():\n    # Test that the _get_transformer_list function correctly constructs\n    # the transformer list from the input estimators\n    transformers = [(StandardScaler(), [0, 1]), (OneHotEncoder(), ['city'])]\n    transformer_list = _get_transformer_list(transformers)\n    assert transformer_list == [('standardscaler', StandardScaler(), [0, 1]),\n                                ('onehotencoder', OneHotEncoder(), ['city'])]\n", "def test_column_transformer_with_pandas():\n    import pandas as pd\n\n    X = pd.DataFrame({\n        \"numerical\": [1, 2, 3],\n        \"categorical\": [\"a\", \"b\", \"c\"],\n    })\n\n    ct = ColumnTransformer(\n        transformers=[\n            (\"num\", StandardScaler(), [\"numerical\"]),\n            (\"cat\", OneHotEncoder(), [\"categorical\"]),\n        ],\n        remainder=\"passthrough\",\n    )\n\n    Xt = ct.fit_transform(X)\n    assert Xt.shape == (3, 5)\n    assert_allclose_dense_sparse(Xt[:, 0], [-1.22474487, 0., 1.22474487])\n", "def test_column_transformer_get_feature_names_out():\n    # Create a ColumnTransformer\n    ct = make_column_transformer(\n        (StandardScaler(), ['numeric1', 'numeric2']),\n        (OneHotEncoder(), ['categorical1', 'categorical2'])\n    )\n\n    # Fit the ColumnTransformer on some data\n    X = np.array([[1, 2, 'a', 'b'], [3, 4, 'c', 'd']])\n    ct.fit(X)\n\n    # Test get_feature_names_out method\n    feature_names_out = ct.get_feature_names_out()\n    expected_feature_names = ['standardscaler__numeric1', 'standardscaler__numeric2', 'onehotencoder__x0_a', 'onehotencoder__x0_b', 'onehotencoder__x1_c', 'onehotencoder__x1_d']\n    assert feature_names_out.tolist() == expected_feature_names\n", "def test_get_feature_names_out_with_dataframe():\n    import pandas as pd\n\n    X = pd.DataFrame({\"city\": [\"London\", \"London\", \"Paris\", \"Sallisaw\"],\n                      \"rating\": [5, 3, 4, 5]})\n\n    ct = make_column_transformer(\n        (StandardScaler(), make_column_selector(dtype_include=np.number)),  # rating\n        (OneHotEncoder(), make_column_selector(dtype_include=object)))  # city\n\n    ct.fit(X)\n\n    feature_names_out = ct.get_feature_names_out()\n    expected_feature_names = np.array(['standardscaler__rating', 'onehotencoder__city_London',\n                                      'onehotencoder__city_Paris', 'onehotencoder__city_Sallisaw'])\n\n    assert_array_equal(feature_names_out, expected_feature_names)\n"], "sample_143": ["    def test_unescape_entities(self):\n        self.assertEqual(text.unescape_entities('&amp;'), '&')\n        self.assertEqual(text.unescape_entities('&lt;'), '<')\n        self.assertEqual(text.unescape_entities('&gt;'), '>')\n        self.assertEqual(text.unescape_entities('&quot;'), '\"')\n        self.assertEqual(text.unescape_entities('&#34;'), '\"')\n        self.assertEqual(text.unescape_entities('&#x22;'), '\"')\n        self.assertEqual(text.unescape_entities('Invalid entity &invalid;'), 'Invalid entity &invalid;')\n", "    def test_unescape_entities(self):\n        self.assertEqual(text.unescape_entities('&quot;'), '\"')\n        self.assertEqual(text.unescape_entities('&lt;'), '<')\n        self.assertEqual(text.unescape_entities('&gt;'), '>')\n        self.assertEqual(text.unescape_entities('&amp;'), '&')\n        self.assertEqual(text.unescape_entities('&nbsp;'), '\\xa0')\n        self.assertEqual(text.unescape_entities('&#39;'), \"'\")\n        self.assertEqual(text.unescape_entities('&#x27;'), \"'\")\n        self.assertEqual(text.unescape_entities('&invalid;'), '&invalid;')\n", "    def test_capfirst(self):\n        self.assertEqual(text.capfirst('hello'), 'Hello')\n        self.assertEqual(text.capfirst('hello world'), 'Hello world')\n        self.assertEqual(text.capfirst(''), '')\n        self.assertEqual(text.capfirst(None), None)\n        self.assertEqual(text.capfirst(123), '123')\n        self.assertEqual(text.capfirst('hello'), 'Hello')\n        self.assertEqual(text.capfirst('HELLO'), 'HELLO')\n", "    def test_truncator_chars(self):\n        truncator = text.Truncator(\"This is a long string to be truncated.\")\n        result = truncator.chars(10)\n        self.assertEqual(result, \"This is a\u2026\")\n", "    def test_capfirst(self):\n        self.assertEqual(text.capfirst('hello'), 'Hello')\n        self.assertEqual(text.capfirst(''), '')\n        self.assertEqual(text.capfirst(None), None)\n        self.assertEqual(text.capfirst('123'), '123')\n        self.assertEqual(text.capfirst(lazystr('hello')), 'Hello')\n"], "sample_502": ["def test_sci():\n    im = np.random.rand(10, 10)\n    plt.sci(im)\n    assert plt.gci() == im\n", "def test_ticklabel_format():\n    fig, ax = plt.subplots()\n    ax.plot(np.arange(1e6))\n    ax.ticklabel_format(style='sci', scilimits=(0, 0))\n    plt.close(fig)\n", "def test_colorbar_with_no_mappable():\n    with pytest.raises(RuntimeError, match='No mappable was found to use for colorbar creation.'):\n        plt.colorbar()\n", "def test_clf():\n    plt.figure()  # Create a new figure\n    plt.clf()  # Clear the current figure\n    assert plt.gcf().axes == []  # Verify that there are no axes\n", "def test_savefig_with_transparent_true():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3], [4, 5, 6])\n    fig.savefig('test.png', transparent=True)\n    plt.close(fig)\n    assert Path('test.png').exists()\n"], "sample_158": ["def test_contribute_to_class(self, mock_super):\n    field = models.ManyToManyField('AnotherModel')\n    field.contribute_to_class(mock.Mock(spec=models.Model), 'test_field')\n    mock_super.assert_called_once()\n    self.assertTrue(hasattr(field, 'm2m_db_table'))\n", "def test_foreign_key_to_field(self):\n    class CustomPKModel(models.Model):\n        custom_pk = models.CharField(primary_key=True, max_length=10)\n\n        class Meta:\n            app_label = 'invalid_models_tests'\n\n    class TestModel(models.Model):\n        fk = models.ForeignKey(CustomPKModel, to_field='custom_pk', on_delete=models.CASCADE)\n\n        class Meta:\n            app_label = 'invalid_models_tests'\n\n    errors = TestModel.check()\n    self.assertEqual(errors, [])\n", "    def test_m2m_field_to_invalid_model(self):\n        class InvalidModel(models.Model):\n            m2m_field = models.ManyToManyField('invalid_models_tests.InvalidRelatedModel')\n\n        errors = InvalidModel.check()\n        self.assertEqual(errors, [\n            Error(\n                \"Field specifies a many-to-many relation through model \"\n                \"'invalid_models_tests.invalidmodel_m2m_field', which has not been installed.\",\n                obj=InvalidModel._meta.get_field('m2m_field'),\n                id='fields.E331',\n            )\n        ])\n", "def test_m2m_through_fields_none(self, *args):\n    class MyModel(models.Model):\n        m2m_field = models.ManyToManyField('AnotherModel', through='ThroughModel', through_fields=None)\n\n    errors = MyModel.check()\n    self.assertEqual(len(errors), 1)\n    self.assertIsInstance(errors[0], Error)\n    self.assertEqual(errors[0].id, 'fields.E337')\n", "    def test_foreign_key_related_name_with_app_label(self):\n        class Child(models.Model):\n            parent = models.ForeignKey(\n                'Parent', on_delete=models.CASCADE,\n                related_name='invalid_models_tests.child_set',\n            )\n\n        self.assertEqual(Child._meta.get_field('parent').remote_field.related_name, 'invalid_models_tests.child_set')\n"], "sample_1111": ["def test_textplot_str_with_complex_values():\n    x = Symbol('x')\n    expr = sqrt(x)\n    a = -1\n    b = 1\n    W = 55\n    H = 21\n    result = list(textplot_str(expr, a, b, W, H))\n    assert all(x in line for line in result for x in ['/', '\\\\', '.', '_', '|'])\n", "def test_textplot_str_complex_numbers():\n    x = Symbol('x')\n    expr = sqrt(x**2 - 1)\n    result = list(textplot_str(expr, -5, 5))\n    # Check that the output doesn't contain any complex numbers\n    assert all('complex' not in line for line in result)\n", "def test_textplot_str_single_variable_function():\n    x = Symbol('x')\n    expr = sqrt(x)\n    a = 0\n    b = 16\n    W = 55\n    H = 21\n\n    plot = list(textplot_str(expr, a, b, W, H))\n\n    # Check that the plot is of the correct length\n    assert len(plot) == H + 1\n\n    # Check that the first and last lines contain the correct x values\n    assert plot[-1].startswith(' ' * (H // 2 + 2) + '0')\n    assert plot[-1].endswith('16')\n\n    # Check that the plot contains the correct number of y values\n    y_values = [line[H // 2 + 2:H // 2 + 3] for line in plot[:-1]]\n    assert len(set(y_values)) == H\n", "def test_textplot_str_constant_function():\n    x = Symbol('x')\n    expr = S(5)\n    expected_output = [\n        \"   5 |_______________________________________________________\",\n        \"   0                          7.5                        15\",\n    ]\n    assert list(textplot_str(expr, 0, 15)) == expected_output\n", "def test_textplot_str_single_point():\n    x = Symbol('x')\n    expr = S(1)\n    result = list(textplot_str(expr, 0, 1))\n    assert result[-1] == \" 0                                        1\"\n    assert all(row[7] == '_' for row in result[:-1])\n"], "sample_40": ["def test_with_H0():\n    H0 = 70 * u.km / u.s / u.Mpc\n    equiv = u.with_H0(H0)\n    assert_quantity_allclose(1 * u.Mpc / u.littleh, 100 * u.Mpc / H0, rtol=0)\n", "def test_with_H0():\n    H0 = 70 * (u.km / u.s / u.Mpc)\n    q = 1 * u.Mpc / u.littleh\n    q_physical = q.to(u.Mpc, u.with_H0(H0))\n    assert_quantity_allclose(q_physical, 1 * u.Mpc / (H0.value / 70))\n", "def test_parallax_converter_negative_distance():\n    # Test parallax converter with a negative distance\n    parallax_equiv = u.parallax()\n    distance = u.Quantity(-1, u.pc)\n    result = u.Quantity(distance).to(u.arcsecond, equivalencies=parallax_equiv)\n    assert np.isnan(result.value)\n", "def test_with_H0():\n    H0 = 70 * u.km / u.s / u.Mpc\n    equiv = u.with_H0(H0)\n\n    # Test conversion from little-h to physical units\n    q_littleh = 1 * u.Mpc / u.littleh\n    q_physical = q_littleh.to(u.Mpc)\n    assert_quantity_allclose(q_physical, 100 / 70 * u.Mpc)\n\n    # Test conversion from physical units to little-h\n    q_physical = 1 * u.Mpc\n    q_littleh = q_physical.to(u.Mpc / u.littleh, equivalencies=equiv)\n    assert_quantity_allclose(q_littleh, 70 / 100 * u.Mpc / u.littleh)\n\n    # Test conversion with default H0\n    equiv = u.with_H0()\n    q_littleh = 1 * u.Mpc / u.littleh\n    q_physical = q_littleh.to(u.Mpc, equivalencies=equiv)\n    assert_quantity_allclose(q_physical, 100 / cosmology.default_cosmology.get().H0.value * u.Mpc)\n", "def test_brightness_temperature_conversion():\n    freq = 5 * u.GHz\n    beam_area = (50 * u.arcsec)**2\n    equiv = u.brightness_temperature(freq, beam_area)\n\n    # Test conversion from Jy/beam to K\n    jy_per_beam = 1 * u.Jy / u.beam\n    expected_K = 3.526295144567176 * u.K\n    assert_quantity_allclose(jy_per_beam.to(u.K, equivalencies=equiv), expected_K)\n\n    # Test conversion from K to Jy/beam\n    K = 3.526295144567176 * u.K\n    expected_jy_per_beam = 1 * u.Jy / u.beam\n    assert_quantity_allclose(K.to(u.Jy / u.beam, equivalencies=equiv), expected_jy_per_beam)\n"], "sample_580": ["def test_categorical_order():\n    # Test with a categorical Series\n    s = pd.Series(pd.Categorical([\"b\", \"a\", \"b\", \"c\", \"a\"], categories=[\"c\", \"b\", \"a\"]))\n    assert categorical_order(s) == [\"c\", \"b\", \"a\"]\n\n    # Test with a non-categorical Series\n    s = pd.Series([\"b\", \"a\", \"b\", \"c\", \"a\"])\n    assert categorical_order(s) == [\"a\", \"b\", \"c\"]\n\n    # Test with a non-categorical Series containing numeric values\n    s = pd.Series([2, 1, 2, 3, 1])\n    assert categorical_order(s) == [1, 2, 3]\n\n    # Test with an override order\n    s = pd.Series([\"b\", \"a\", \"b\", \"c\", \"a\"])\n    assert categorical_order(s, order=[\"c\", \"a\", \"b\"]) == [\"c\", \"a\", \"b\"]\n", "def test_categorical_order():\n    # Test with a pandas Categorical Series\n    data = pd.Categorical([\"b\", \"c\", \"a\", \"d\"], ordered=True)\n    assert categorical_order(data) == [\"b\", \"c\", \"a\", \"d\"]\n\n    # Test with a pandas Series of strings\n    data = pd.Series([\"b\", \"c\", \"a\", \"d\", np.nan])\n    assert categorical_order(data) == [\"a\", \"b\", \"c\", \"d\"]\n\n    # Test with a pandas Series of numbers\n    data = pd.Series([2, 1, 3, 4, np.nan])\n    assert categorical_order(data) == [1, 2, 3, 4]\n\n    # Test with a custom order\n    data = pd.Series([\"b\", \"c\", \"a\", \"d\", np.nan])\n    custom_order = [\"d\", \"a\", \"b\", \"c\"]\n    assert categorical_order(data, custom_order) == custom_order\n", "def test_categorical_order_with_order_provided():\n    vector = pd.Series([\"b\", \"a\", \"c\", \"a\", np.nan])\n    order = [\"a\", \"b\", \"c\"]\n    assert categorical_order(vector, order) == order\n", "def test_categorical_order_with_explicit_order():\n    vector = pd.Series([\"c\", \"b\", \"a\", \"b\", np.nan])\n    order = [\"a\", \"b\", \"c\"]\n    assert categorical_order(vector, order=order) == order\n", "def test_variable_type_with_extension_boolean_dtypes():\n    # Test variable_type function with boolean extension dtypes\n    vector = pd.Series(pd.arrays.BooleanArray([True, False, np.nan]))\n    assert variable_type(vector, boolean_type=\"boolean\", strict_boolean=True) == VarType(\"boolean\")\n    assert variable_type(vector, boolean_type=\"numeric\", strict_boolean=False) == VarType(\"numeric\")\n"], "sample_639": ["    def test_process_tokens_not_implemented(self):\n        checker = BaseTokenChecker()\n        with self.assertRaises(NotImplementedError):\n            checker.process_tokens([])\n", "    def test_check_consistency(self):\n        consistent_checker = OtherBasicChecker()\n        consistent_checker.check_consistency()  # No error should be raised\n\n        inconsistent_checker = DifferentBasicChecker()\n        with self.assertRaises(InvalidMessageError):\n            inconsistent_checker.check_consistency()  # Error should be raised due to inconsistent checker id\n", "    def test_get_message_definition(self):\n        checker = OtherBasicChecker()\n        msg_def = checker.get_message_definition(\"W0001\")\n        self.assertEqual(msg_def.msgid, \"W0001\")\n        self.assertEqual(msg_def.msg, \"Basic checker has an example.\")\n        self.assertEqual(msg_def.symbol, \"basic-checker-example\")\n        self.assertEqual(msg_def.description, \"Used nowhere and serves no purpose.\")\n\n        with self.assertRaises(InvalidMessageError):\n            checker.get_message_definition(\"W0002\")\n", "def test_create_message_definition_from_tuple():\n    checker = BaseChecker()\n    checker.name = \"test\"\n    msgid = \"W1234\"\n    msg_tuple = (\n        \"message\",\n        \"message-symbol\",\n        \"Message description with detail.\",\n        {\"scope\": WarningScope.LINE, \"used_for_option\": \"some-option\"}\n    )\n    msg_def = checker.create_message_definition_from_tuple(msgid, msg_tuple)\n    assert msg_def.msgid == msgid\n    assert msg_def.msg == \"message\"\n    assert msg_def.symbol == \"message-symbol\"\n    assert msg_def.description == \"Message description with detail.\"\n    assert msg_def.scope == WarningScope.LINE\n    assert msg_def.used_for_option == \"some-option\"\n", "def test_checker_equality():\n    checker1 = OtherBasicChecker()\n    checker2 = OtherBasicChecker()\n    checker3 = DifferentBasicChecker()\n\n    assert checker1 == checker2, \"Equal checkers with the same name and messages should be equal\"\n    assert checker1 != checker3, \"Different checkers with different names or messages should not be equal\"\n"], "sample_704": ["def test_iterparentnodeids(nodeid: str, expected: List[str]) -> None:\n    assert list(nodes.iterparentnodeids(nodeid)) == expected\n", "def test_iterparentnodeids(nodeid: str, expected: List[str]):\n    result = list(nodes.iterparentnodeids(nodeid))\n    assert result == expected\n", "def test_imply_path(pytester: Pytester):\n    path = Path(\"/test/path\")\n    fspath = legacy_path(path)\n\n    result_path, result_fspath = nodes._imply_path(path, fspath)\n    assert result_path == path\n    assert result_fspath == fspath\n\n    result_path, result_fspath = nodes._imply_path(path, None)\n    assert result_path == path\n    assert result_fspath == fspath\n\n    result_path, result_fspath = nodes._imply_path(None, fspath)\n    assert result_path == path\n    assert result_fspath == fspath\n\n    with pytest.raises(ValueError):\n        nodes._imply_path(Path(\"/different/path\"), fspath)\n", "def test_iterparentnodeids_multiple_colons_in_test_name(nodeid: str, expected: List[str]) -> None:\n    result = list(nodes.iterparentnodeids(nodeid))\n    assert result == expected\n", "def test_iterparentnodeids_edge_cases(nodeid, expected):\n    result = list(nodes.iterparentnodeids(nodeid))\n    assert result == expected\n"], "sample_752": ["def test_iforest_threshold():\n    # Test that the threshold_ attribute is deprecated and warns\n    X = iris.data\n    clf = IsolationForest(random_state=rng)\n    clf.fit(X)\n    with pytest.warns(DeprecationWarning) as record:\n        thresh = clf.threshold_\n    assert len(record) == 1\n    assert \"threshold_ attribute is deprecated\" in str(record[0].message)\n", "def test_sparse_matrix_input():\n    rng = np.random.RandomState(42)\n    n_samples, n_features = 100, 10\n    X = rng.rand(n_samples, n_features)\n    X[X < 0.95] = 0\n    X = csc_matrix(X)\n\n    clf = IsolationForest(random_state=42)\n    clf.fit(X)\n    y_pred = clf.predict(X)\n\n    # Check if the model can handle sparse matrix input\n    assert_equal(len(y_pred), n_samples)\n", "def test_isolation_forest_sparse_matrix():\n    X = csc_matrix(iris.data)\n    y = iris.target\n    clf = IsolationForest(random_state=rng)\n    clf.fit(X)\n    assert_equal(clf.predict(X).shape, y.shape)\n    assert_equal(clf.decision_function(X).shape, y.shape)\n    assert_equal(clf.score_samples(X).shape, y.shape)\n", "def test_isolation_forest_random_state():\n    X = iris.data\n    y = iris.target\n\n    # Set a random state\n    rng = np.random.RandomState(42)\n    clf = IsolationForest(random_state=rng)\n    scores1 = clf.fit(X).decision_function(X)\n\n    # Set the same random state\n    clf = IsolationForest(random_state=rng)\n    scores2 = clf.fit(X).decision_function(X)\n\n    # Check that the scores are the same\n    assert_array_almost_equal(scores1, scores2)\n\n    # Set a different random state\n    clf = IsolationForest(random_state=43)\n    scores3 = clf.fit(X).decision_function(X)\n\n    # Check that the scores are different\n    assert not np.array_equal(scores1, scores3)\n", "def test_isolation_forest_contamination(contamination):\n    X = iris.data\n    clf = IsolationForest(contamination=contamination, random_state=rng)\n    clf.fit(X)\n    y_pred = clf.predict(X)\n\n    if contamination == \"auto\":\n        expected_outliers = int(X.shape[0] * 0.1)\n    else:\n        expected_outliers = int(X.shape[0] * contamination)\n\n    assert_equal(np.sum(y_pred == -1), expected_outliers)\n"], "sample_1024": ["compilation error", "def test_fibonacci():\n    assert fibonacci(10) == 55\n    assert fibonacci(20) == 6765\n    assert fibonacci(-10) == 0\n", "def test_integer_cache():\n    assert Integer(1000) is _intcache[1000]\n    assert Integer(-1000) is _intcache[-1000]\n    assert Integer(0) is _intcache[0]\n    assert Integer(1) is _intcache[1]\n    assert Integer(-1) is _intcache[-1]\n", "def test_Rational_eval_power():\n    assert Rational(3, 4)**2 == 9/16\n    assert Rational(3, 4)**Rational(1, 2) == sqrt(3)/2\n    assert Rational(3, 4)**(-2) == 16/9\n    assert Rational(3, 4)**Rational(-1, 2) == 2/sqrt(3)\n    assert Rational(3, 4)**0 == 1\n    assert Rational(3, 4)**(-1) == 4/3\n    assert Rational(3, 4)**oo == oo\n    assert Rational(3, 4)**-oo == 0\n    assert Rational(3, 4)**Float(2) == 0.234375\n", "def test_integer_eval_power():\n    assert Integer(2)**Rational(1, 2) == sqrt(2)\n    assert Integer(2)**Rational(3, 2) == 2*sqrt(2)\n    assert Integer(2)**Rational(-1, 2) == 1/sqrt(2)\n    assert Integer(2)**Rational(-3, 2) == 1/(2*sqrt(2))\n    assert Integer(2)**Rational(1, 3) == cbrt(2)\n    assert Integer(2)**Rational(2, 3) == 2**Rational(1, 3)*2**Rational(1, 3)\n    assert Integer(2)**Rational(-1, 3) == 1/cbrt(2)\n    assert Integer(2)**Rational(-2, 3) == 1/(2**Rational(1, 3)*2**Rational(1, 3))\n    assert Integer(2)**Integer(3) == 8\n    assert Integer(2)**Integer(-3) == Rational(1, 8)\n    assert Integer(2)**0 == 1\n    assert Integer(2)**1 == 2\n    assert Integer(2)**(-1) == Rational(1, 2)\n    assert Integer(2)**Float(0.5) == sqrt(2)\n    assert Integer(2)**Float(-0.5) == 1/sqrt(2)\n    assert Integer(2)**Float(0) == 1\n    assert Integer(2)**Float(1) == 2\n    assert Integer(2)**Float(-1) == Rational(1, 2)\n    assert Integer(2)**t == Pow(2, t)\n"], "sample_239": ["def test_custom_kwargs(self):\n    form_data = {\n        'form-TOTAL_FORMS': '1',\n        'form-INITIAL_FORMS': '0',\n        'form-MAX_NUM_FORMS': '',\n        'form-0-custom_kwarg': 'test_value',\n    }\n    CustomKwargFormSet = formset_factory(CustomKwargForm, extra=1, formset=BaseFormSet)\n    formset = CustomKwargFormSet(form_data, form_kwargs={'custom_kwarg': 'initial_value'})\n    form = formset.forms[0]\n    self.assertEqual(form.custom_kwarg, 'test_value')\n", "def test_custom_kwarg_formset_factory(self):\n    CustomKwargFormSet = formset_factory(CustomKwargForm, extra=2, form_kwargs={'custom_kwarg': 'test'})\n    formset = CustomKwargFormSet()\n    for form in formset:\n        self.assertEqual(form.custom_kwarg, 'test')\n", "def test_custom_kwarg_formset(self):\n    CustomKwargFormSet = formset_factory(CustomKwargForm, formset=BaseFormSet, extra=2, form_kwargs={'custom_kwarg': 'foo'})\n    formset = CustomKwargFormSet()\n    for form in formset:\n        self.assertEqual(form.custom_kwarg, 'foo')\n", "def test_custom_kwargs(self):\n    CustomKwargFormSet = formset_factory(CustomKwargForm, extra=2, formset=BaseFormSet, form_kwargs={'custom_kwarg': 'value'})\n    formset = CustomKwargFormSet()\n    for form in formset:\n        self.assertEqual(form.custom_kwarg, 'value')\n", "    def test_custom_form_kwargs(self):\n        CustomKwargFormSet = formset_factory(\n            CustomKwargForm, formset=BaseFormSet, extra=2,\n            form_kwargs={'custom_kwarg': 'test'}\n        )\n        formset = CustomKwargFormSet()\n        for form in formset.forms:\n            self.assertEqual(form.custom_kwarg, 'test')\n"], "sample_92": ["def test_get_all_permissions(self):\n    backend = SimpleBackend()\n    permissions = backend.get_all_permissions(self.user)\n    self.assertEqual(permissions, {'user_perm', 'group_perm'})\n", "    def test_get_all_permissions(self):\n        backend = SimpleBackend()\n        permissions = backend.get_all_permissions(self.user)\n        self.assertIn('user_perm', permissions)\n        self.assertIn('group_perm', permissions)\n", "    def setUpTestData(cls):\n        cls.user = User.objects.create_user('test', 'test@example.com', 'test')\n        cls.group = Group.objects.create(name='test_group')\n        cls.user.groups.add(cls.group)\n        cls.permission = Permission.objects.create(\n            name='test_permission',\n            content_type=ContentType.objects.get_for_model(User),\n            codename='test_codename',\n        )\n        cls.group.permissions.add(cls.permission)\n", "    def test_get_all_permissions(self):\n        backend = SimpleBackend()\n        self.assertEqual(\n            backend.get_all_permissions(self.user),\n            {'user_perm', 'group_perm'}\n        )\n", "def test_get_all_permissions(self):\n    backend = SimpleBackend()\n    permissions = backend.get_all_permissions(self.user)\n    self.assertEqual(permissions, {'user_perm', 'group_perm'})\n"], "sample_224": ["    def test_aggregate_with_annotation(self):\n        annotated = Book.objects.annotate(num_authors=Count('authors'))\n        result = annotated.aggregate(max_authors=Max('num_authors'), min_authors=Min('num_authors'))\n        self.assertEqual(result['max_authors'], 3)\n        self.assertEqual(result['min_authors'], 1)\n", "def test_aggregate_with_filter(self):\n    avg_pages = Book.objects.filter(price__gt=25).aggregate(Avg('pages'))\n    self.assertAlmostEqual(avg_pages['pages__avg'], 527.5)\n", "def test_aggregate_with_annotation_expression(self):\n    books_with_publisher_awards = Book.objects.annotate(publisher_awards=F('publisher__num_awards'))\n    result = books_with_publisher_awards.aggregate(Avg('publisher_awards'))\n    self.assertEqual(result['publisher_awards__avg'], (3 + 1 + 7 + 9 + 0) / 5)\n", "def test_aggregate_with_custom_field(self):\n    # Test aggregate with a custom field\n    class RoundedAge(Func):\n        function = 'ROUND'\n        template = '%(function)s(%(expressions)s, 1)'\n\n    aggregate_result = Author.objects.aggregate(avg_age=Avg(RoundedAge('age')))\n    self.assertEqual(aggregate_result['avg_age'], 36.0)\n", "    def test_aggregate_with_filter(self):\n        books = Book.objects.filter(publisher=self.p1)\n        avg_price = books.aggregate(Avg('price'))\n        self.assertEqual(avg_price['price__avg'], Decimal('26.845'))\n"], "sample_1072": ["compilation error", "def test_frac_comparison():\n    assert frac(Rational(4, 3)) == Rational(1, 3)\n    assert frac(-Rational(4, 3)) == Rational(2, 3)\n    assert frac(n) == 0\n    assert frac(x).rewrite(floor) == x - floor(x)\n    assert frac(t + I*r) == I*frac(r) + frac(t)\n    assert frac(E) > 0\n    assert frac(E) < 1\n    assert frac(E) >= 0\n    assert frac(E) <= 1\n    assert frac(E) != 1\n    assert frac(E) != 0\n", "def test_frac_eval():\n    assert frac(Rational(4, 3)) == Rational(1, 3)\n    assert frac(-Rational(4, 3)) == Rational(2, 3)\n    assert frac(n) == 0\n    assert frac(x).rewrite(floor) == x - floor(x)\n    assert frac(y + I*n) == I*frac(n) + frac(y)\n", "def test_frac_comparisons():\n    x = Symbol('x')\n    assert frac(x) >= 0\n    assert frac(x) < 1\n    assert frac(x) <= 0 is False\n    assert frac(x) > 1 is False\n    assert frac(x) > -1\n    assert frac(x) <= 2\n    assert frac(x) < 3\n    assert frac(x) >= 3 is False\n    assert frac(x) > -3 is False\n    assert frac(x) >= -3\n", "def test_frac_comparisons():\n    assert frac(pi) < 1\n    assert frac(pi) > 0\n    assert frac(pi) <= 1\n    assert frac(pi) >= 0\n    assert frac(pi) != 1\n    assert frac(pi) != 0\n    assert frac(pi) != pi\n    assert frac(2) == 0\n    assert frac(-3.5) == -0.5\n"], "sample_609": ["def test_broadcast_compat_data():\n    var = xr.Variable((\"x\", \"y\"), np.arange(6).reshape(2, 3))\n    broadcast_dims = (\"z\",)\n    core_dims = (\"x\", \"y\")\n    expected_data = np.arange(6).reshape(1, 2, 3)\n    result_data = broadcast_compat_data(var, broadcast_dims, core_dims)\n    assert_identical(result_data, expected_data)\n", "def test_apply_ufunc_with_empty_inputs():\n    # Test apply_ufunc with empty inputs\n    empty_array = np.array([])\n    empty_da = xr.DataArray(empty_array)\n\n        return x * 2\n\n    result = apply_ufunc(func, empty_da)\n    assert result.equals(empty_da)\n", "def test_unified_dim_sizes():\n    var1 = xr.Variable((\"x\", \"y\"), np.ones((3, 4)))\n    var2 = xr.Variable((\"y\", \"z\"), np.ones((4, 5)))\n    dim_sizes = unified_dim_sizes([var1, var2])\n    assert dim_sizes == {\"x\": 3, \"y\": 4, \"z\": 5}\n", "def test_apply_ufunc_with_different_output_core_dims():\n        return x + y, x - y\n\n    x = xr.DataArray(np.arange(3 * 2).reshape(3, 2), dims=[\"a\", \"b\"])\n    y = xr.DataArray(np.arange(3 * 2 * 2).reshape(3, 2, 2), dims=[\"a\", \"b\", \"c\"])\n\n    result = apply_ufunc(\n        func,\n        x,\n        y,\n        input_core_dims=[[\"a\", \"b\"], [\"a\", \"b\", \"c\"]],\n        output_core_dims=[[\"a\", \"c\"], [\"a\", \"b\"]],\n    )\n\n    expected_sum = x + y\n    expected_diff = x - y\n\n    assert_identical(result[0], expected_sum)\n    assert_identical(result[1], expected_diff)\n", "def test_apply_ufunc_with_exclude_dims():\n    a = xr.DataArray(np.arange(6).reshape(2, 3), dims=(\"x\", \"y\"), coords={\"x\": [\"a\", \"b\"], \"y\": np.arange(3)})\n    b = xr.DataArray(np.arange(4).reshape(2, 2), dims=(\"x\", \"z\"), coords={\"x\": [\"a\", \"b\"], \"z\": [\"c\", \"d\"]})\n\n        return x + y\n\n    result = apply_ufunc(func, a, b, input_core_dims=[[\"y\"], [\"z\"]], exclude_dims={\"x\"})\n\n    expected_result = xr.DataArray(np.array([[3, 5], [7, 9]]), dims=(\"x\", \"z\"), coords={\"x\": [\"a\", \"b\"], \"z\": [\"c\", \"d\"]})\n\n    assert_identical(result, expected_result)\n"], "sample_1202": ["def test_ilcm():\n    assert ilcm(3, 5, 7) == 105\n", "def test_igcd_lehmer():\n    assert igcd_lehmer(4, 6) == 2\n    assert igcd_lehmer(0, 0) == 0\n    assert igcd_lehmer(0, 1) == 1\n    assert igcd_lehmer(1, 0) == 1\n    assert igcd_lehmer(1, 1) == 1\n", "def test_integer_gcd_lehmer():\n    assert igcd_lehmer(12, 18) == 6\n    assert igcd_lehmer(12, 0) == 12\n    assert igcd_lehmer(0, 18) == 18\n    assert igcd_lehmer(0, 0) == 0\n", "compilation error", "def test_float_abs():\n    a = Float(3.14, 10)\n    assert abs(a) == Float(3.14, 10)\n    assert abs(-a) == Float(3.14, 10)\n    assert abs(Float(0.0, 10)) == Float(0.0, 10)\n    assert abs(Float(-0.0, 10)) == Float(0.0, 10)\n"], "sample_653": ["def test_logging_plugin_set_log_path(tmpdir, caplog):\n    log_file = str(tmpdir.join(\"test.log\"))\n    caplog.set_log_path(log_file)\n    logger = logging.getLogger(\"test_logger\")\n    logger.info(\"Test log message\")\n    with open(log_file, \"r\", encoding=\"utf-8\") as f:\n        log_content = f.read()\n    assert \"Test log message\" in log_content\n", "def test_log_capture_fixture(caplog):\n    import logging\n\n    logger = logging.getLogger(__name__)\n    logger.warning('This is a warning message')\n    logger.error('This is an error message')\n\n    assert len(caplog.record_tuples) == 2\n    assert caplog.record_tuples[0] == (__name__, logging.WARNING, 'This is a warning message')\n    assert caplog.record_tuples[1] == (__name__, logging.ERROR, 'This is an error message')\n", "def test_colored_level_formatter(caplog, log_level, expected_color_opts):\n    logger = logging.getLogger('test_logger')\n    handler = caplog.handler\n    formatter = ColoredLevelFormatter(handler.terminalwriter, \"%(levelname)s: %(message)s\")\n    handler.setFormatter(formatter)\n    logger.setLevel(log_level)\n    logger.addHandler(handler)\n\n    logger.log(log_level, 'test message')\n\n    assert len(caplog.records) == 1\n    assert caplog.records[0].levelno == log_level\n    assert caplog.messages[0] == 'test message'\n    formatted_levelname = caplog.text.split(':')[0].strip()\n    assert formatted_levelname == handler.terminalwriter.markup(\n        logging.getLevelName(log_level), **expected_color_opts\n    )\n", "def test_LogCaptureFixture(caplog):\n    logger = logging.getLogger(\"test_logger\")\n    logger.setLevel(logging.INFO)\n\n    with caplog.at_level(logging.INFO, logger=\"test_logger\"):\n        logger.debug(\"This is a debug message\")\n        logger.info(\"This is an info message\")\n        logger.warning(\"This is a warning message\")\n\n    assert len(caplog.records) == 2\n    assert caplog.record_tuples == [\n        (\"test_logger\", logging.INFO, \"This is an info message\"),\n        (\"test_logger\", logging.WARNING, \"This is a warning message\"),\n    ]\n", "def test_get_actual_log_level_with_invalid_level():\n    config = _DummyConfig()\n    config.setoption(\"log_level\", \"INVALID\")\n    config.setini(\"log_level\", \"INVALID\")\n    with pytest.raises(pytest.UsageError, match=\"'INVALID' is not recognized\"):\n        get_actual_log_level(config, \"log_level\")\n"], "sample_739": ["def test_label_binarizer_sparse_output():\n    lb = LabelBinarizer(sparse_output=True)\n    y = [1, 2, 6, 4, 2]\n    lb.fit(y)\n    Y = lb.transform(y)\n    assert_true(issparse(Y))\n    assert_array_equal(Y.toarray(), label_binarize(y, classes=lb.classes_))\n", "def test_label_encoder_inverse_transform_error():\n    le = LabelEncoder()\n    le.fit([1, 2, 3, 4])\n    with assert_raises(ValueError):\n        le.inverse_transform([0, 1, 5])\n", "def test_label_encoder_with_non_numeric_labels():\n    le = LabelEncoder()\n    labels = [\"paris\", \"paris\", \"tokyo\", \"amsterdam\"]\n    le.fit(labels)\n    assert_array_equal(le.classes_, np.array(['amsterdam', 'paris', 'tokyo']))\n    assert_array_equal(le.transform([\"tokyo\", \"tokyo\", \"paris\"]), np.array([2, 2, 1]))\n    assert_array_equal(le.inverse_transform([2, 2, 1]), np.array(labels))\n", "def test_label_binarizer_sparse_output_pos_label_neg_label():\n    y = csr_matrix([[0, 1], [1, 0]])\n    lb = LabelBinarizer(sparse_output=True, pos_label=1, neg_label=0)\n    lb.fit(y)\n    y_bin = lb.transform(y)\n    assert_true(issparse(y_bin))\n    assert_array_equal(y_bin.toarray(), y.toarray())\n", "def test_label_binarize_sparse_input():\n    y = [0, 1, 2, 1]\n    classes = [0, 1, 2]\n    Y = label_binarize(y, classes, sparse_output=True)\n    assert issparse(Y)\n    assert_array_equal(Y.toarray(), np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1], [0, 1, 0]]))\n"], "sample_579": ["def test_heatmap_with_custom_labels(self):\n    custom_xticklabels = [\"X1\", \"X2\", \"X3\", \"X4\", \"X5\", \"X6\", \"X7\", \"X8\"]\n    custom_yticklabels = [\"Y1\", \"Y2\", \"Y3\", \"Y4\"]\n    ax = mat.heatmap(self.x_norm, xticklabels=custom_xticklabels, yticklabels=custom_yticklabels)\n    assert ax.get_xticklabels() == custom_xticklabels\n    assert ax.get_yticklabels() == custom_yticklabels\n", "def test_heatmap_with_custom_colors(self):\n    # Test heatmap with custom colors\n    custom_colors = [\"red\", \"green\", \"blue\", \"yellow\"]\n    ax = mat.heatmap(self.df_norm, cmap=custom_colors)\n    assert isinstance(ax, plt.Axes)\n", "def test_clustermap_different_metrics(self):\n    metric = \"cityblock\"  # Using Manhattan distance metric for this test\n    cg = mat.clustermap(self.x_norm, metric=metric)\n    assert cg.dendrogram_row.metric == metric\n    assert cg.dendrogram_col.metric == metric\n", "def test_heatmap_center(self):\n    heatmap_kws = copy.copy(self.default_kws)\n    heatmap_kws['center'] = 0\n    heatmap_kws['cmap'] = 'coolwarm'\n    ax = mat.heatmap(self.x_norm, **heatmap_kws)\n    assert ax.get_xlabel() == \"\"\n    assert ax.get_ylabel() == \"Index\"\n    npt.assert_equal(ax.get_xticks(), [0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5])\n    npt.assert_equal(ax.get_yticks(), [0.5, 1.5, 2.5, 3.5])\n    npt.assert_equal(ax.get_xticklabels(), ['0', '1', '2', '3', '4', '5', '6', '7'])\n    npt.assert_equal(ax.get_yticklabels(), ['0', '1', '2', '3'])\n    assert ax.collections[0].get_cmap().name == 'coolwarm'\n", "def test_heatmap_with_divergent_colormap_and_center(self):\n    data = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    center = 5\n    cmap = \"coolwarm\"\n    ax = mat.heatmap(data, center=center, cmap=cmap)\n\n    # Assert that the color of the center cell is white\n    center_cell_color = ax.collections[0].get_facecolor()[4]  # Index 4 is the center cell\n    npt.assert_allclose(center_cell_color, [1.0, 1.0, 1.0, 1.0])\n\n    # Assert that the colormap is divergent and centered correctly\n    assert ax.collections[0].colorbar.orientation == \"vertical\"\n    assert_colors_equal(ax.collections[0].colorbar.cmap(0), get_colormap(cmap)(0))\n    assert_colors_equal(ax.collections[0].colorbar.cmap(127), [1.0, 1.0, 1.0, 1.0])\n    assert_colors_equal(ax.collections[0].colorbar.cmap(255), get_colormap(cmap)(1))\n"], "sample_47": ["    def test_get_traceback_data(self):\n        request = RequestFactory().get('/')\n        try:\n            raise ValueError('Test exception')\n        except ValueError:\n            exc_type, exc_value, tb = sys.exc_info()\n\n        reporter = ExceptionReporter(request, exc_type, exc_value, tb)\n        data = reporter.get_traceback_data()\n\n        self.assertEqual(data['exception_type'], 'ValueError')\n        self.assertEqual(data['exception_value'], 'Test exception')\n", "    def test_get_traceback_data(self):\n        request = RequestFactory().get('/')\n        exc_type, exc_value, tb = sys.exc_info()\n        reporter = ExceptionReporter(request, exc_type, exc_value, tb)\n        data = reporter.get_traceback_data()\n        self.assertIsInstance(data, dict)\n        # Add more assertions based on the expected output of get_traceback_data\n", "    def setUp(self):\n        self.factory = RequestFactory()\n", "    def setUp(self):\n        self.factory = RequestFactory()\n", "    def test_get_traceback_frames_with_traceback_hide(self):\n            import traceback\n            # Hide frames above __traceback_hide__\n            traceback.format_stack()\n            raise Exception(\"Exception with hidden frames\")\n\n        request = RequestFactory().get('/')\n        try:\n            view_with_traceback_hide(request)\n        except Exception as e:\n            exc_type, exc_value, tb = sys.exc_info()\n            reporter = ExceptionReporter(request, exc_type, exc_value, tb)\n            frames = reporter.get_traceback_frames()\n            self.assertEqual(len(frames), 1)\n            self.assertEqual(frames[0]['function'], 'view_with_traceback_hide')\n"], "sample_507": ["    def test_update(self, data, locs):\n        \"\"\"Test the update method of UnitData.\"\"\"\n        ud = cat.UnitData(data)\n        assert list(ud._mapping.values()) == locs\n        new_data = [data[0], \"new_category\"]\n        ud.update(new_data)\n        assert list(ud._mapping.values()) == locs + [max(locs) + 1]\n", "def test_duplicate_values(self):\n    data = ['A', 'B', 'A', 'C', 'B']\n    locs = [0, 1, 0, 2, 1]\n    unit = cat.UnitData(data)\n    assert list(unit._mapping.values()) == locs\n", "def test_numeric_strings(self, data, locs):\n    unit_data = cat.UnitData(data)\n    assert list(unit_data._mapping.values()) == locs\n", "def test_numeric_strings(self, data, locs):\n    unit_data = cat.UnitData(data)\n    assert list(unit_data._mapping.values()) == locs\n    assert all(isinstance(val, str) for val in unit_data._mapping.keys())\n", "def test_update_with_duplicates(self):\n    data = ['A', 'B', 'A', 'C']\n    unit_data = cat.UnitData(data)\n    assert list(unit_data._mapping.keys()) == ['A', 'B', 'C']\n    assert list(unit_data._mapping.values()) == [0, 1, 2]\n"], "sample_62": ["    def test_register_model_twice(self):\n        self.site.register(Location, NameAdmin)\n        with self.assertRaisesMessage(admin.sites.AlreadyRegistered,\n            'The model Location is already registered with admin.NameAdmin.'):\n            self.site.register(Location, NameAdmin)\n", "    def test_register_model(self):\n        self.site.register(Person, NameAdmin)\n        self.assertIn(Person, self.site._registry)\n        self.assertIsInstance(self.site._registry[Person], NameAdmin)\n", "    def test_register_model(self):\n        self.site.register(Location, NameAdmin)\n        self.assertTrue(self.site.is_registered(Location))\n        self.assertEqual(self.site._registry[Location].save_on_top, True)\n", "def test_register(self):\n    # Test registering a model with a custom admin class\n    self.site.register(Person, NameAdmin)\n    self.assertTrue(self.site.is_registered(Person))\n    self.assertIsInstance(self.site._registry[Person], NameAdmin)\n\n    # Test registering an abstract model raises ImproperlyConfigured\n    with self.assertRaises(ImproperlyConfigured):\n        self.site.register(Place)\n\n    # Test registering a model twice raises AlreadyRegistered\n    with self.assertRaises(admin.AlreadyRegistered):\n        self.site.register(Person)\n\n    # Test unregistering a model\n    self.site.unregister(Person)\n    self.assertFalse(self.site.is_registered(Person))\n\n    # Test unregistering a model that's not registered raises NotRegistered\n    with self.assertRaises(admin.NotRegistered):\n        self.site.unregister(Person)\n", "    def test_register_model_admin_subclass(self):\n        self.site.register(Location, NameAdmin)\n        self.assertIn(Location, self.site._registry)\n        self.assertTrue(isinstance(self.site._registry[Location], NameAdmin))\n"], "sample_79": ["    def test_pluralize_with_custom_suffix(self):\n        tests = [\n            (0, 'classes'),\n            (1, 'class'),\n            (2, 'classes'),\n        ]\n        self.engine.filters['pluralize'] = pluralize\n        self.engine.templates['t'] = 'class{{ value|pluralize:\"es\" }}'\n        self.check_values(*tests)\n", "def test_pluralize_with_custom_suffix(self):\n    self.check_values(\n        (0, 'votes'),\n        (1, 'vote'),\n        (2, 'votes'),\n    )\n", "def test_pluralize(self):\n    self.check_values(\n        (0, 'votes'),\n        (1, 'vote'),\n        (2, 'votes'),\n        ('0', 'votes'),\n        ('1', 'vote'),\n        ('2', 'votes'),\n        ('', 's'),\n        (None, 's'),\n        ([], 's'),\n        ([1], ''),\n        ([1, 2], 's'),\n        ('a', 's'),\n        ('', 'es'),\n        (1, ''),\n        (2, 'es'),\n        ('y,ies', 'ies'),\n        ('y,', 'y'),\n        (Decimal('1.0'), 'vote'),\n        (Decimal('1.1'), 'votes'),\n    )\n", "def test_pluralize_default_suffix(self):\n    self.check_values(\n        (0, 'votes'),\n        (1, 'vote'),\n        (2, 'votes'),\n        (1.0, 'vote'),\n        (2.5, 'votes'),\n        (0.0, 'votes'),\n        ('0', 'votes'),\n        ('1', 'vote'),\n        ('2', 'votes'),\n        ([1], 'vote'),\n        ([1, 2], 'votes'),\n        ('1 item', '1 items'),\n        (Decimal(0), 'votes'),\n        (Decimal(1), 'vote'),\n        (Decimal(2), 'votes'),\n    )\n", "def test_pluralize_with_singular_suffix(self):\n    self.check_values(\n        (0, 'votes'),\n        (1, 'vote'),\n        (2, 'votes'),\n        ('', 'votes'),\n        (None, 'votes'),\n        ([], 'votes'),\n        ({}, 'votes'),\n    )\n"], "sample_301": ["    def test_zip_imported_module(self):\n        zip_filename = self.temporary_file('test_module.zip')\n        with zipfile.ZipFile(zip_filename, 'w') as zf:\n            zf.writestr('test_module/__init__.py', '')\n            zf.writestr('test_module/module.py', 'test_variable = \"test_value\"')\n\n        sys.path.insert(0, str(zip_filename))\n        self.addCleanup(sys.path.remove, str(zip_filename))\n\n        self.import_and_cleanup('test_module.module')\n        self.assertFileFound(zip_filename)\n", "    def test_import_error_file(self):\n        # Test that a file with a syntax error is handled correctly\n        filename = self.temporary_file('bad_syntax.py')\n        with filename.open('w') as f:\n            f.write('this is bad syntax')\n        with self.assertRaises(SyntaxError):\n            self.import_and_cleanup('bad_syntax')\n        self.assertFileFound(filename)\n", "    def test_zip_imported_modules(self):\n        zip_file = self.temporary_file('test.zip')\n        with zipfile.ZipFile(zip_file, 'w') as zf:\n            zf.writestr('test_module.py', 'print(\"Hello, World!\")')\n        sys.path.insert(0, str(zip_file))\n        self.addCleanup(lambda: sys.path.pop(0))\n\n        self.import_and_cleanup('test_module')\n        self.assertFileFound(zip_file)\n", "    def test_import_error_files(self):\n        filename = self.temporary_file('error_module.py')\n        filename.write_text('syntax error')\n        try:\n            import_module(filename.stem)\n        except SyntaxError:\n            pass\n        self.assertFileFound(filename)\n", "    def test_iter_modules_and_files_with_weakref(self):\n        name = 'test_module'\n        self.import_and_cleanup(name)\n        module = sys.modules[name]\n        weak_ref = weakref.ref(module)\n        del sys.modules[name]\n\n        # Test that weakref'd modules are not included in the result.\n        self.clear_autoreload_caches()\n        self.assertNotIn(weak_ref(), list(autoreload.iter_all_python_module_files()))\n"], "sample_193": ["def test_m2m_field_deconstruct(self):\n    field = models.ManyToManyField('RelatedModel')\n    name, path, args, kwargs = field.deconstruct()\n    self.assertEqual(name, 'ManyToManyField')\n    self.assertEqual(path, 'django.db.models.fields.related.ManyToManyField')\n    self.assertEqual(args, ['RelatedModel'])\n    self.assertEqual(kwargs, {})\n", "def test_related_field_deconstruct(self):\n    apps = Apps()\n    state = ProjectState(apps)\n    related_field = models.ForeignKey('otherapp.OtherModel', on_delete=models.CASCADE)\n    name, path, args, kwargs = related_field.deconstruct()\n    self.assertEqual(name, 'ForeignKey')\n    self.assertEqual(path, 'django.db.models.fields.related')\n    self.assertEqual(args, [])\n    self.assertEqual(kwargs, {'to': 'otherapp.othermodel', 'on_delete': models.CASCADE})\n", "def test_many_to_many_field_contribute_to_related_class(self):\n    \"\"\"\n    Test the contribute_to_related_class method of ManyToManyField.\n    \"\"\"\n    apps = Apps()\n    state = ProjectState()\n\n    # Create a simple model for the test\n    model_state = ModelState(\n        name='TestModel',\n        fields=[\n            ('id', models.AutoField(primary_key=True)),\n            ('m2m_field', models.ManyToManyField('self', related_name='test_rel')),\n        ],\n        options={\n            'app_label': 'test',\n        },\n        apps=apps,\n    )\n    state.add_model(model_state)\n\n    # Get the model and field\n    model = apps.get_model('test', 'TestModel')\n    field = model._meta.get_field('m2m_field')\n\n    # Call contribute_to_related_class\n    field.contribute_to_related_class(model, field.remote_field)\n\n    # Check if the related descriptor is set\n    self.assertTrue(hasattr(model, 'test_rel'))\n\n    # Check if the accessors for the column names on the m2m table are set\n    self.assertTrue(callable(field.m2m_column_name))\n    self.assertTrue(callable(field.m2m_reverse_name))\n\n    # Check if the accessors for the field names on the m2m table are set\n    self.assertTrue(callable(field.m2m_field_name))\n    self.assertTrue(callable(field.m2m_reverse_field_name))\n\n    # Check if the accessors for the target field names on the m2m table are set\n    self.assertTrue(callable(field.m2m_target_field_name))\n    self.assertTrue(callable(field.m2m_reverse_target_field_name))\n", "    def setUp(self):\n        self.apps = Apps()\n        self.state = ProjectState(self.apps)\n", "def test_contribute_to_related_class_with_swapped_model(self):\n    apps = Apps()\n    project_state = ProjectState()\n\n    class SwappedModel(models.Model):\n        class Meta:\n            swappable = 'TEST_SWAPPABLE_MODEL'\n\n    apps.register_model('myapp', SwappedModel)\n\n    class RelatedModel(models.Model):\n        pass\n\n    apps.register_model('myapp', RelatedModel)\n\n    class MyModel(models.Model):\n        m2m_field = models.ManyToManyField(RelatedModel)\n\n    apps.register_model('myapp', MyModel)\n\n    project_state.add_model(ModelState.from_model(MyModel))\n    project_state.add_model(ModelState.from_model(RelatedModel))\n    project_state.add_model(ModelState.from_model(SwappedModel))\n\n    with self.settings(SWAPPED_MODEL='myapp.SwappedModel'):\n        project_state.apps = apps\n        with override_settings(SWAPPED_MODEL='myapp.SwappedModel'):\n            MyModel.m2m_field.contribute_to_related_class(RelatedModel, MyModel.m2m_field.remote_field)\n\n    self.assertFalse(hasattr(RelatedModel, 'm2m_field'))\n"], "sample_238": ["    def test_absolute_value(self):\n        self.assertEqual(Book.objects.aggregate(abs_price=Abs('price'))['abs_price'], Decimal('30.00'))\n", "    def test_abs_function(self):\n        author = Author.objects.create(age=-10)\n        result = Author.objects.annotate(abs_age=Abs('age')).first().abs_age\n        self.assertEqual(result, 10)\n", "    def setUpTestData(cls):\n        cls.book_pages = [cls.b1.pages, cls.b2.pages, cls.b3.pages, cls.b4.pages, cls.b5.pages, cls.b6.pages]\n        cls.publisher_awards = [cls.p1.num_awards, cls.p2.num_awards, cls.p3.num_awards, cls.p4.num_awards, cls.p5.num_awards]\n", "def test_math_functions(self):\n    # Testing Abs lookup\n    self.assertEqual(Book.objects.filter(rating=Abs(Value(-4.5))).count(), 1)\n\n    # Testing ATan2 function\n    self.assertTrue(ATan2(Value(1), Value(1)) in Book.objects.annotate(atan2=ATan2('pages', 'rating')))\n\n    # Testing Ceil function\n    self.assertTrue(Ceil('price') in Book.objects.annotate(ceil_price=Ceil('price')))\n\n    # Testing Cos lookup\n    self.assertEqual(Book.objects.filter(rating=Cos(Value(0))).count(), 1)\n\n    # Testing Degrees function\n    self.assertTrue(Degrees(Value(1)) in Book.objects.annotate(degrees_rating=Degrees('rating')))\n\n    # Testing Exp function\n    self.assertTrue(Exp('rating') in Book.objects.annotate(exp_rating=Exp('rating')))\n\n    # Testing Floor function\n    self.assertTrue(Floor('price') in Book.objects.annotate(floor_price=Floor('price')))\n\n    # Testing Ln function\n    self.assertTrue(Ln('price') in Book.objects.annotate(ln_price=Ln('price')))\n\n    # Testing Log function\n    self.assertTrue(Log('pages', 'rating') in Book.objects.annotate(log_pages_rating=Log('pages', 'rating')))\n\n    # Testing Mod function\n    self.assertTrue(Mod('pages', Value(2)) in Book.objects.annotate(mod_pages=Mod('pages', Value(2))))\n\n    # Testing Pi function\n    self.assertTrue(Pi() in Book.objects.annotate(pi=Pi()))\n\n    # Testing Power function\n    self.assertTrue(Power('pages', Value(2)) in Book.objects.annotate(squared_pages=Power('pages', Value(2))))\n\n    # Testing Radians function\n    self.assertTrue(Radians('rating') in Book.objects.annotate(radians_", "    def test_mathematical_functions(self):\n        # Test ABS function\n        self.assertEqual(Book.objects.annotate(abs_pages=Abs('pages')).get(name='The Definitive Guide to Django: Web Development Done Right').abs_pages, 447)\n\n        # Test ACOS function\n        self.assertAlmostEqual(Book.objects.annotate(acos_rating=ACos('rating')).get(name='The Definitive Guide to Django: Web Development Done Right').acos_rating, 1.0471975511965976, places=5)\n\n        # Test ASIN function\n        self.assertAlmostEqual(Book.objects.annotate(asin_rating=ASin('rating')).get(name='The Definitive Guide to Django: Web Development Done Right').asin_rating, 0.451091834679953, places=5)\n\n        # Test ATAN function\n        self.assertAlmostEqual(Book.objects.annotate(atan_rating=ATan('rating')).get(name='The Definitive Guide to Django: Web Development Done Right').atan_rating, 0.40560550261263154, places=5)\n\n        # Test CEIL function\n        self.assertEqual(Book.objects.annotate(ceil_rating=Ceil('rating')).get(name='The Definitive Guide to Django: Web Development Done Right').ceil_rating, 5)\n\n        # Test COS function\n        self.assertAlmostEqual(Book.objects.annotate(cos_rating=Cos('rating')).get(name='The Definitive Guide to Django: Web Development Done Right').cos_rating, 0.7073882641809137, places=5)\n\n        # Test SIN function\n        self.assertAlmostEqual(Book.objects.annotate(sin_rating=Sin('rating')).get(name='The Definitive Guide to Django: Web Development Done Right').sin_rating, 0.7"], "sample_182": ["def test_union_with_exists(self):\n    subquery = Number.objects.filter(num=OuterRef('num'))\n    queryset = Number.objects.filter(Exists(subquery)).union(Number.objects.filter(num__gt=5))\n    expected_numbers = [i for i in range(6, 10)]\n    self.assertNumbersEqual(queryset, expected_numbers, ordered=False)\n", "def test_union(self):\n    # Test the union of two querysets\n    qs1 = Number.objects.filter(num__lt=5)\n    qs2 = Number.objects.filter(num__gt=5)\n    union_qs = qs1.union(qs2)\n    expected_numbers = [0, 1, 2, 3, 4, 6, 7, 8, 9]\n    self.assertNumbersEqual(union_qs, expected_numbers)\n", "def test_union(self):\n    qs1 = Number.objects.filter(num__gt=3)\n    qs2 = Number.objects.filter(other_num__lt=7)\n    union_qs = qs1.union(qs2)\n    self.assertNumbersEqual(union_qs, set(range(4, 10)), ordered=False)\n", "def test_union(self):\n    qs1 = Number.objects.filter(num__gt=5)\n    qs2 = Number.objects.filter(num__lt=5)\n    union_qs = qs1.union(qs2)\n    self.assertNumbersEqual(union_qs, set(range(10)))\n", "def test_union_with_annotate(self):\n    numbers = Number.objects.filter(num__gt=5).annotate(doubled_num=F('num') * 2)\n    all_numbers = Number.objects.annotate(doubled_num=F('num') * 2)\n    union = numbers.union(all_numbers)\n    self.assertNumbersEqual(union, [6, 8, 10, 12, 14, 16, 18, 0, 2, 4, 6, 8, 10])\n"], "sample_743": ["def test_kneighbors_with_sparse_input():\n    for sparse_type in SPARSE_TYPES:\n        X_sparse = sparse_type(iris.data)\n        nn = neighbors.NearestNeighbors(n_neighbors=3, algorithm='brute')\n        nn.fit(X_sparse)\n        distances, indices = nn.kneighbors(X_sparse)\n        assert_equal(distances.shape, (iris.data.shape[0], 3))\n        assert_equal(indices.shape, (iris.data.shape[0], 3))\n\n        # Test with different metrics\n        for metric in VALID_METRICS_SPARSE['brute']:\n            nn = neighbors.NearestNeighbors(n_neighbors=3, algorithm='brute', metric=metric)\n            nn.fit(X_sparse)\n            distances, indices = nn.kneighbors(X_sparse)\n            assert_equal(distances.shape, (iris.data.shape[0], 3))\n            assert_equal(indices.shape, (iris.data.shape[0], 3))\n", "def test_supervised_integer_mixin():\n    X = iris.data\n    y = iris.target\n\n    clf = neighbors.KNeighborsClassifier()\n    clf.fit(X, y)\n\n    # check if classes_ attribute is correctly set\n    assert_array_equal(clf.classes_, np.unique(y))\n\n    # check if _y attribute is correctly set\n    assert_array_equal(clf._y, np.searchsorted(clf.classes_, y))\n\n    # check if fit method returns self\n    assert_equal(clf.fit(X, y), clf)\n\n    # check if fit method raises error for multi-output y\n    y_multi_output = np.column_stack((y, y))\n    assert_raises(ValueError, clf.fit, X, y_multi_output)\n", "def test_get_weights():\n    dist = np.array([1, 2, 3, 4, 5])\n\n    # Test 'uniform' weights\n    weights = _get_weights(dist, 'uniform')\n    assert_equal(weights, None)\n\n    # Test 'distance' weights\n    expected_weights = np.array([1, 0.5, 0.33333333, 0.25, 0.2])\n    weights = _get_weights(dist, 'distance')\n    assert_array_almost_equal(weights, expected_weights)\n\n    # Test callable weights\n    weights = _get_weights(dist, _weight_func)\n    expected_weights = np.array([1, 0.25, 0.11111111, 0.0625, 0.04])\n    assert_array_almost_equal(weights, expected_weights)\n\n    # Test invalid weights\n    assert_raises(ValueError, _get_weights, dist, 'invalid_option')\n", "def test_kneighbors_with_callable_metric():\n    X = [[0], [1], [2]]\n        return np.sum(np.abs(x - y))\n    n_neighbors = 2\n    knn = neighbors.NearestNeighbors(n_neighbors=n_neighbors, metric=custom_metric)\n    knn.fit(X)\n    distances, indices = knn.kneighbors(X)\n    assert_array_equal(indices, [[0, 1], [0, 1], [1, 2]])\n    assert_array_equal(distances, [[0, 1], [1, 1], [1, 1]])\n", "def test_radius_neighbors_sparse_input():\n    for sparse_type, algorithm in product(SPARSE_TYPES, ALGORITHMS):\n        if algorithm not in ('auto', 'brute'):\n            with assert_warns(UserWarning, match=\"cannot use tree with sparse input: using brute force\"):\n                neigh = neighbors.NearestNeighbors(algorithm=algorithm)\n                neigh.fit(sparse_type(iris.data[:100]))\n        else:\n            for metric in VALID_METRICS_SPARSE['brute']:\n                neigh = neighbors.NearestNeighbors(algorithm=algorithm, metric=metric)\n                neigh.fit(sparse_type(iris.data[:100]))\n\n    with assert_raises_regex(ValueError, \"metric '.*' not valid for sparse input\"):\n        neigh = neighbors.NearestNeighbors(metric='euclidean')\n        neigh.fit(coo_matrix(iris.data[:100]))\n\n    with assert_raises_regex(ValueError, \"%s does not work with sparse matrices. Densify the data, or set algorithm='brute'\"):\n        neigh = neighbors.NearestNeighbors(algorithm='ball_tree')\n        neigh.fit(coo_matrix(iris.data[:100]))\n"], "sample_623": ["def test_to_netcdf_with_none_path():\n    # Test to_netcdf when path_or_file is None\n    ds = xr.Dataset({'x': xr.DataArray([1, 2, 3], dims='a')})\n    result = xr.to_netcdf(ds, path_or_file=None)\n    assert isinstance(result, bytes)\n", "def test_to_netcdf_valid_engines(engine):\n    ds = xr.Dataset({\"x\": (\"time\", np.arange(5))}, coords={\"time\": np.arange(5)})\n    with assert_no_warnings():\n        ds.to_netcdf(\"test.nc\", engine=engine)\n        ds_roundtrip = xr.open_dataset(\"test.nc\", engine=engine)\n    assert_identical(ds, ds_roundtrip)\n", "def test_validate_dataset_names():\n    dataset = xr.Dataset({'': ('dim', np.arange(10))})\n    with pytest.raises(ValueError):\n        xr.io._validate_dataset_names(dataset)\n\n    dataset = xr.Dataset({'valid_name': ('dim', np.arange(10))})\n    xr.io._validate_dataset_names(dataset)\n\n    dataset = xr.Dataset({None: ('dim', np.arange(10))})\n    with pytest.raises(TypeError):\n        xr.io._validate_dataset_names(dataset)\n", "def test_validate_dataset_names(engine):\n    # Test case where all Dataset keys and DataArray names are valid\n    ds = xr.Dataset({\"var1\": (\"dim1\", np.random.rand(10))}, {\"dim1\": np.arange(10)})\n    xr.backends.api._validate_dataset_names(ds)\n\n    # Test case where a Dataset key is an empty string\n    ds = xr.Dataset({\"\": (\"dim1\", np.random.rand(10))}, {\"dim1\": np.arange(10)})\n    with pytest.raises(ValueError):\n        xr.backends.api._validate_dataset_names(ds)\n\n    # Test case where a Dataset key is not a string or None\n    ds = xr.Dataset({123: (\"dim1\", np.random.rand(10))}, {\"dim1\": np.arange(10)})\n    with pytest.raises(TypeError):\n        xr.backends.api._validate_dataset_names(ds)\n", "def test_to_netcdf_with_chunks(chunks):\n    ds = xr.Dataset({\"x\": (\"dim\", np.arange(10))})\n    with assert_no_warnings():\n        with ds.chunk(chunks).to_netcdf(\"test.nc\") as nc:\n            assert_identical(ds, xr.open_dataset(nc))\n"], "sample_956": ["def test_fetch_inventory_with_basic_auth(mock_read_from_url, mock_inventory_file, app):\n    url = \"https://user:pass@example.com/inv\"\n    mock_inventory_file.load.return_value = inventory_v2\n    mock_read_from_url.return_value.url = url\n    result = fetch_inventory(app, \"https://example.com\", url)\n    mock_read_from_url.assert_called_once_with(\"https://example.com/inv\", config=app.config)\n    mock_inventory_file.load.assert_called_once()\n    assert result == inventory_v2\n", "def test_missing_reference_with_named_inventory(app, status, warning):\n    app.builder.build_all()\n    set_config(app, {\n        'python': ('https://docs.python.org/3', [None]),\n    })\n    normalize_intersphinx_mapping(app, app.config)\n    load_mappings(app)\n    assert 'python' in app.env.intersphinx_named_inventory\n\n    node, contnode = fake_node('py', 'class', 'python:str', 'str')\n    result = missing_reference(app, app.env, node, contnode)\n    assert result is not None\n    assert result['refuri'] == 'https://docs.python.org/3/library/stdtypes.html#str'\n", "def test_resolve_reference_detect_inventory(app, mock_read_from_url, mock_inventory_file):\n    mock_read_from_url.return_value = mock.MagicMock(url='http://example.com/new_inv')\n    mock_inventory_file.load.return_value = inventory_v2\n    set_config(app, {('inv_name', ('http://example.com', 'inv'))})\n    load_mappings(app)\n    app.env.config.intersphinx_disabled_reftypes = []\n    node, contnode = fake_node('py', 'class', 'inv_name:Sphinx', 'Sphinx')\n    res = missing_reference(app, app.env, node, contnode)\n    assert res is not None\n    assert res['refuri'] == 'http://example.com/new_inv'\n", "def test_fetch_inventory_without_url_in_inv(mock_read_from_url, mock_inventory_file):\n    mock_app = mock.Mock()\n    mock_app.srcdir = '.'\n    mock_app.config.intersphinx_timeout = None\n    mock_inv_file = mock.Mock()\n    mock_inv_file.url = 'http://example.com/new_inv'\n    mock_read_from_url.return_value = mock_inv_file\n    mock_inventory_file.load.return_value = inventory_v2\n\n    invdata = fetch_inventory(mock_app, 'http://example.com', 'inv')\n\n    mock_read_from_url.assert_called_once_with('http://example.com/inv', config=mock_app.config)\n    mock_inventory_file.load.assert_called_once_with(mock_inv_file, 'http://example.com', os.path.join)\n    assert invdata == inventory_v2\n", "def test_inventory_loaded(app, mock_read_from_url, mock_inventory_file):\n    mock_inventory_file.load.return_value = inventory_v2\n    set_config(app, {'python': ('https://docs.python.org/3', None)})\n    load_mappings(app)\n    mock_read_from_url.assert_called_once_with(\n        'https://docs.python.org/3/{}'.format(INVENTORY_FILENAME),\n        config=app.config\n    )\n    assert 'py:class' in app.env.intersphinx_inventory\n"], "sample_9": ["def test_html_custom_parser():\n    data = \"<table><tr><th>col1</th><th>col2</th></tr><tr><td>1</td><td>2</td></tr></table>\"\n    htmldict = {'parser': 'html.parser'}  # Use the standard library parser\n    table = ascii.read(StringIO(data), format='html', reader=html.HTML(htmldict))\n    assert len(table) == 1\n    assert table['col1'][0] == 1\n    assert table['col2'][0] == 2\n\n    htmldict = {'parser': 'lxml'}  # Use the lxml parser\n    table = ascii.read(StringIO(data), format='html', reader=html.HTML(htmldict))\n    assert len(table) == 1\n    assert table['col1'][0] == 1\n    assert table['col2'][0] == 2\n\n    htmldict = {'parser': 'xml'}  # Use the lxml XML parser\n    table = ascii.read(StringIO(data), format='html', reader=html.HTML(htmldict))\n    assert len(table) == 1\n    assert table['col1'][0] == 1\n    assert table['col2'][0] == 2\n\n    htmldict = {'parser': 'html5lib'}  # Use the html5lib parser\n    table = ascii.read(StringIO(data), format='html', reader=html.HTML(htmldict))\n    assert len(table) == 1\n    assert table['col1'][0] == 1\n    assert table['col2'][0] == 2\n\n    htmldict = {'parser': 'invalid_parser'}  # Use an invalid parser\n    with pytest.raises(FeatureNotFound):\n        ascii.read(StringIO(data), format='html', reader=html.HTML(htmldict))\n", "def test_custom_parser(parser):\n    html_data = \"<table><tr><th>a</th><th>b</th></tr><tr><td>1</td><td>2</td></tr></table>\"\n    table = Table.read(StringIO(html_data), format='html', htmldict={'parser': parser})\n    assert len(table) == 1\n    assert table['a'][0] == 1\n    assert table['b'][0] == 2\n", "def test_html_identify_table():\n    table_html = \"<table id='test_table'>...</table>\"\n    soup = BeautifulSoup(table_html, 'html.parser')\n    htmldict = {'table_id': 'test_table'}\n    assert html.identify_table(soup, htmldict, 1) is True\n    htmldict = {'table_id': 'wrong_id'}\n    assert html.identify_table(soup, htmldict, 1) is False\n    htmldict = {'table_id': 2}\n    assert html.identify_table(soup, htmldict, 2) is True\n    htmldict = {'table_id': 1}\n    assert html.identify_table(soup, htmldict, 2) is False\n", "def test_identify_table(table_id, expected_index):\n    tables = [BeautifulSoup('<table id=\"table1\"></table>'), BeautifulSoup('<table id=\"my_table\"></table>'), BeautifulSoup('<table id=\"table3\"></table>')]\n    htmldict = {'table_id': table_id}\n    for i, table in enumerate(tables):\n        if html.identify_table(table, htmldict, i + 1):\n            assert i == expected_index\n", "def test_html_output_multidim_cols():\n    \"\"\"\n    Test output of HTML table with multi-dimensional columns.\n    \"\"\"\n    t = Table([[1, 2, 3], [(1, 2), (3, 4), (5, 6)]], names=('a', 'b'))\n    h = html.HTML(htmldict={'multicol': True})\n    output = h.write(t)\n    soup = BeautifulSoup('\\n'.join(output), 'html.parser')\n    table = soup.find('table')\n    thead = table.find('thead')\n    th_elements = thead.find_all('th')\n    assert len(th_elements) == 3\n    assert th_elements[1].has_attr('colspan')\n    assert th_elements[1]['colspan'] == '2'\n"], "sample_591": ["def test_merge_internals_with_conflicts():\n    ds1 = xr.Dataset({'x': ('a', [1, 2, 3])})\n    ds2 = xr.Dataset({'x': ('a', [4, 5, 6])})\n\n    with pytest.raises(merge.MergeError):\n        merge._merge_internals(ds1, ds2, compat='equals')\n", "def test_merge_identical():\n    ds1, ds2, fill_value = create_test_data()\n    merged = merge.dataset_merge_method(ds1, ds2, compat='identical')\n    assert_identical(merged, ds1)\n", "def test_merge_coord_with_same_name_as_var(self):\n    ds1 = xr.Dataset({'x': ('a', [1, 2, 3])}, coords={'y': ('a', [4, 5, 6])})\n    ds2 = xr.Dataset(coords={'x': ('a', [7, 8, 9])})\n    merged = merge(ds1, ds2)\n    expected = xr.Dataset({'x': ('a', [1, 2, 3]), 'x_1': ('a', [7, 8, 9])}, coords={'y': ('a', [4, 5, 6])})\n    assert_identical(merged, expected)\n", "def test_merge_with_different_dimension_order():\n    ds1 = xr.Dataset({'A': ('x', np.random.rand(3))}, {'x': np.array([1, 2, 3])})\n    ds2 = xr.Dataset({'B': ('y', np.random.rand(4))}, {'y': np.array([4, 5, 6, 7])})\n    merged = merge.dataset_merge_method(ds1, ds2)\n    expected_coords = {'x': ('x', np.array([1, 2, 3])), 'y': ('y', np.array([4, 5, 6, 7]))}\n    assert merged.coords == expected_coords\n", "def test_merge_internals_no_conflicts_with_dtype_change():\n    ds1 = xr.Dataset({'A': ('x', [1, 2], np.int32)}, coords={'x': [0, 1]})\n    ds2 = xr.Dataset({'A': ('x', [3, 4], np.float64)}, coords={'x': [1, 2]})\n    expected = xr.Dataset({'A': ('x', [1, 2, 3, 4], np.float64)}, coords={'x': [0, 1, 2]})\n    result = merge._merge_internals([ds1, ds2], compat='no_conflicts')\n    assert_identical(result, expected)\n"], "sample_582": ["def test_locate_app_with_invalid_module(monkeypatch):\n    monkeypatch.setitem(sys.modules, \"nonexistent_module\", None)\n    with pytest.raises(NoAppException):\n        locate_app(\"nonexistent_module\", None)\n", "def test_prepare_import():\n    path = test_path / \"simple_app.py\"\n    module_name = prepare_import(str(path))\n    assert module_name == \"simple_app\"\n\n    path = test_path / \"subdir\" / \"sub_app.py\"\n    module_name = prepare_import(str(path))\n    assert module_name == \"subdir.sub_app\"\n", "def test_find_best_app_with_factory_without_args(monkeypatch):\n    module = types.ModuleType(\"module\")\n    module.create_app = lambda: Flask(__name__)\n    monkeypatch.setitem(sys.modules, \"module\", module)\n\n    app = find_best_app(module)\n    assert isinstance(app, Flask)\n", "def test_load_dotenv_no_python_dotenv(runner, monkeypatch):\n    monkeypatch.setitem(sys.modules, \"dotenv\", None)\n\n    assert not load_dotenv()\n    assert \"python-dotenv must be installed to load an env file.\" in runner.invoke(FlaskGroup().make_context(\"\", [\"--env-file\", \"test.env\"]), catch_exceptions=False).output\n", "def test_with_appcontext(runner, monkeypatch):\n    @click.command()\n    @with_appcontext\n        click.echo(\"Hello, \" + current_app.name)\n\n    @click.group(cls=AppGroup)\n        pass\n\n    cli.add_command(hello)\n\n    @cli.command()\n        click.echo(\"Initialized the database\")\n\n    app = Flask(__name__)\n    app.name = \"TestApp\"\n    app.cli.add_command(initdb)\n\n    script_info = ScriptInfo(create_app=lambda: app)\n\n    result = runner.invoke(cli, [\"hello\"], obj=script_info)\n    assert result.exit_code == 0\n    assert \"Hello, TestApp\" in result.output\n"], "sample_794": ["def test_ridge_regression_multi_target(solver):\n    n_samples, n_features = 10, 5\n    n_targets = 3\n    rng = np.random.RandomState(0)\n    X = rng.randn(n_samples, n_features)\n    y = rng.randn(n_samples, n_targets)\n    alpha = [1.0, 2.0, 3.0]\n    coef = ridge_regression(X, y, alpha, solver=solver)\n    assert coef.shape == (n_targets, n_features)\n", "def test_fit_intercept_sparse_sag(solver):\n    ridge = Ridge(alpha=1.0, fit_intercept=True, solver=solver)\n    ridge.fit(X_iris, y_iris)\n    assert_greater(ridge.intercept_, 0.0)\n", "def test_ridge_fit_intercept(solver, fit_intercept):\n    # Test Ridge.fit() with fit_intercept parameter\n    ridge = Ridge(solver=solver, fit_intercept=fit_intercept)\n    ridge.fit(X_diabetes, y_diabetes)\n    if fit_intercept:\n        assert_greater(ridge.intercept_, 0)\n    else:\n        assert_equal(ridge.intercept_, 0)\n", "def test_ridge_regression_return_n_iter(solver):\n    X, y = make_regression(n_samples=100, n_features=2, random_state=0)\n    alpha = 1.0\n    coef, n_iter = ridge_regression(X, y, alpha, solver=solver, return_n_iter=True)\n    assert isinstance(n_iter, np.int32)\n", "def test_ridge_regression_solver_consistency(solver):\n    X, y = make_regression(n_samples=10, n_features=5, random_state=0)\n    alpha = 1.0\n    coef1 = ridge_regression(X, y, alpha, solver=solver)\n    model = Ridge(alpha=alpha, solver=solver)\n    model.fit(X, y)\n    coef2 = model.coef_\n    assert_array_almost_equal(coef1, coef2)\n"], "sample_829": ["def test_incremental_pca_with_sparse_input():\n    X_sparse = sparse.csr_matrix(iris.data)\n    ipca = IncrementalPCA(n_components=2)\n    X_transformed = ipca.fit_transform(X_sparse)\n    assert X_transformed.shape == (150, 2)\n", "def test_incremental_pca_partial_fit_sparse_input():\n    X_sparse = sparse.csr_matrix(iris.data)\n    ipca = IncrementalPCA(n_components=2)\n\n    with pytest.raises(TypeError):\n        ipca.partial_fit(X_sparse)\n", "def test_incremental_pca_sparse_input():\n    X_sparse = sparse.csr_matrix(iris.data)\n    ipca = IncrementalPCA(n_components=2, batch_size=3)\n    ipca.fit(X_sparse)\n    X_transformed = ipca.transform(X_sparse)\n    assert X_transformed.shape == (150, 2)\n", "def test_incremental_pca_sparse_input():\n    X_sparse = sparse.csr_matrix(iris.data)\n    ipca = IncrementalPCA(n_components=2)\n    X_transformed = ipca.fit_transform(X_sparse)\n    pca = PCA(n_components=2)\n    X_transformed_pca = pca.fit_transform(iris.data)\n    assert_allclose_dense_sparse(X_transformed, X_transformed_pca, atol=1e-5)\n", "def test_incremental_pca_with_sparse_input():\n    X = sparse.csr_matrix(iris.data)\n    pca = PCA(n_components=2)\n    ipca = IncrementalPCA(n_components=2, batch_size=50)\n\n    X_pca = pca.fit_transform(X.toarray())\n    X_ipca = ipca.fit_transform(X)\n\n    assert_allclose_dense_sparse(X_pca, X_ipca)\n"], "sample_514": ["def test_colorbar_orientation():\n    # Create a figure and axes for each orientation\n    fig, axs = plt.subplots(2, 1, figsize=(6, 10))\n\n    # Create a colormap and norm for the colorbar\n    cmap = cm.get_cmap('viridis')\n    norm = Normalize(vmin=0, vmax=1)\n\n    # Generate data for the colorbar\n    data = np.linspace(0, 1, 256).reshape(16, 16)\n\n    # Create colorbars with different orientations\n    cbar1 = axs[0].imshow(data, cmap=cmap, norm=norm)\n    fig.colorbar(cbar1, ax=axs[0], orientation='vertical')\n\n    cbar2 = axs[1].imshow(data, cmap=cmap, norm=norm)\n    fig.colorbar(cbar2, ax=axs[1], orientation='horizontal')\n\n    # Turn off text and ticks\n    for ax in axs:\n        ax.tick_params(left=False, labelleft=False,\n                       bottom=False, labelbottom=False)\n\n    # Return the figure to the caller\n    return fig\n", "def test_colorbar_orientation():\n    \"\"\"\n    Test colorbar orientation.\n\n    This test checks that the colorbar is correctly oriented when the\n    orientation is set to 'horizontal' or 'vertical'.\n    \"\"\"\n    # Create a figure and a subplot.\n    fig, ax = plt.subplots()\n\n    # Generate some data.\n    data = np.random.rand(10, 10)\n\n    # Create a vertical colorbar.\n    cax_vertical = fig.add_axes([0.85, 0.1, 0.05, 0.8])\n    Colorbar(cax_vertical, ax.imshow(data), orientation='vertical')\n\n    # Create a horizontal colorbar.\n    cax_horizontal = fig.add_axes([0.1, 0.05, 0.8, 0.05])\n    Colorbar(cax_horizontal, ax.imshow(data), orientation='horizontal')\n\n    # Check that the colorbar lengths are correct.\n    assert cax_vertical.get_position().height > cax_vertical.get_position().width\n    assert cax_horizontal.get_position().width > cax_horizontal.get_position().height\n\n    # Close the figure.\n    plt.close(fig)\n", "def test_colorbar_extension_length():\n    fig1 = _colorbar_extension_length('uniform')\n    fig2 = _colorbar_extension_length('proportional')\n    # Compare the two figures\n    check_figures_equal(fig1, fig2)\n", "def test_colorbar_orientation():\n    \"\"\"\n    Test the orientation of the colorbar.\n\n    This test creates two colorbars, one with the default vertical orientation\n    and another with the horizontal orientation. It checks that the orientation\n    of the colorbar is set correctly and that the label is placed on the\n    correct axis.\n    \"\"\"\n    fig, axs = plt.subplots(1, 2, figsize=(8, 3))\n\n    # Vertical orientation (default)\n    cax1 = fig.add_axes([0.05, 0.1, 0.02, 0.8])\n    Colorbar(cax1, cmap='viridis', label='Vertical colorbar')\n\n    # Horizontal orientation\n    cax2 = fig.add_axes([0.15, 0.05, 0.8, 0.02])\n    Colorbar(cax2, cmap='viridis', orientation='horizontal', label='Horizontal colorbar')\n\n    assert cax1.yaxis.get_label().get_text() == 'Vertical colorbar'\n    assert cax2.xaxis.get_label().get_text() == 'Horizontal colorbar'\n\n    plt.close(fig)\n", "def test_colorbar_extension_alpha():\n    # Get a colormap and appropriate norms for each extension type.\n    cmap, norms = _get_cmap_norms()\n    # Create a figure and adjust whitespace for subplots.\n    fig, axs = plt.subplots(4, 1, sharex=True)\n    fig.subplots_adjust(hspace=4)\n    for i, extension_type in enumerate(('neither', 'min', 'max', 'both')):\n        # Get the appropriate norm and use it to get colorbar boundaries.\n        norm = norms[extension_type]\n        boundaries = values = norm.boundaries\n        # note that the last value was silently dropped pre 3.3:\n        values = values[:-1]\n        # Generate the colorbar with alpha.\n        Colorbar(axs[i], cmap=cmap, norm=norm,\n                 boundaries=boundaries, values=values,\n                 extend=extension_type, extendrect=True,\n                 orientation='horizontal', alpha=0.5)\n        # Turn off text and ticks.\n        axs[i].tick_params(left=False, labelleft=False,\n                            bottom=False, labelbottom=False)\n    # Return the figure to the caller.\n    return fig\n"], "sample_383": ["def test_related_in_lookup(self):\n    items = Item.objects.filter(creator__in=[self.a1, self.a2])\n    self.assertCountEqual(items, [self.i1, self.i2, self.i3])\n\n    items = Item.objects.filter(creator__in=Author.objects.filter(pk__in=[self.a1.pk, self.a2.pk]))\n    self.assertCountEqual(items, [self.i1, self.i2, self.i3])\n", "    def test_related_in(self):\n        items = Item.objects.filter(creator__in=[self.a1, self.a2])\n        self.assertCountEqual(items, [self.i1, self.i2, self.i3])\n", "    def test_related_in(self):\n        items = Item.objects.filter(creator__in=[self.a1, self.a2])\n        self.assertCountEqual(items, [self.i1, self.i2, self.i3])\n", "    def setUpTestData(cls):\n        # Additional setup for RelatedLookup tests\n        # ...\n        pass\n", "    def test_related_lookup(self):\n        # Test RelatedExact lookup\n        items = Item.objects.filter(creator__name__exact='a2')\n        self.assertCountEqual([self.i2, self.i3], list(items))\n\n        # Test RelatedLessThan lookup\n        items = Item.objects.filter(creator__num__lt=3003)\n        self.assertCountEqual([self.i1, self.i2], list(items))\n\n        # Test RelatedGreaterThan lookup\n        items = Item.objects.filter(creator__num__gt=2002)\n        self.assertCountEqual([self.i3, self.i4], list(items))\n\n        # Test RelatedGreaterThanOrEqual lookup\n        items = Item.objects.filter(creator__num__gte=3003)\n        self.assertCountEqual([self.i3, self.i4], list(items))\n\n        # Test RelatedLessThanOrEqual lookup\n        items = Item.objects.filter(creator__num__lte=2002)\n        self.assertCountEqual([self.i1, self.i2], list(items))\n\n        # Test RelatedIsNull lookup\n        items = Item.objects.filter(creator__extra__isnull=True)\n        self.assertCountEqual([self.i4], list(items))\n"], "sample_961": ["def test_parse_annotation():\n    # Test parsing of type annotation\n    env = Mock()\n    annotation = \"List[Union[str, int]]\"\n    result = _parse_annotation(annotation, env)\n    expected = [pending_xref('', 'List', refdomain='py', reftype='class', reftarget='typing.List'),\n                nodes.Text('['),\n                pending_xref('', 'Union', refdomain='py', reftype='class', reftarget='typing.Union'),\n                nodes.Text('['),\n                pending_xref('', 'str', refdomain='py', reftype='class', reftarget='str'),\n                nodes.Text(','),\n                nodes.Text(' '),\n                pending_xref('', 'int', refdomain='py', reftype='class', reftarget='int'),\n                nodes.Text(']'),\n                nodes.Text(']')]\n    assert_node(result, expected)\n", "def test_parse_annotation():\n    env = Mock()\n    env.config.python_use_unqualified_type_names = False\n\n    assert _parse_annotation(\"int\", env) == [pending_xref('', 'int', refdomain='py', reftype='class', reftarget='int')]\n\n    annotation = \"Union[int, str]\"\n    expected = [pending_xref('', 'Union', refdomain='py', reftype='class', reftarget='typing.Union'),\n                nodes.Text('['),\n                pending_xref('', 'int', refdomain='py', reftype='class', reftarget='int'),\n                desc_sig_punctuation('', ','),\n                nodes.Text(' '),\n                pending_xref('', 'str', refdomain='py', reftype='class', reftarget='str'),\n                nodes.Text(']')]\n    assert _parse_annotation(annotation, env) == expected\n", "def test_parse_annotation_with_complex_expression(app):\n    # Test parsing a complex expression with a literal and a class\n    annotation = \"Literal['a', 'b'] | MyClass[Literal['c', 'd']]\"\n    env = app.builder.env\n    result = _parse_annotation(annotation, env)\n    expected = [pending_xref('', 'typing.Literal', refdomain='py', reftype='class', reftarget='typing.Literal'),\n                nodes.Text('['),\n                nodes.literal('', '', nodes.Text('a')),\n                nodes.Text(', '),\n                nodes.literal('', '', nodes.Text('b')),\n                nodes.Text(']'),\n                nodes.Text(' | '),\n                pending_xref('', 'MyClass', refdomain='py', reftype='class', reftarget='MyClass'),\n                nodes.Text('['),\n                pending_xref('', 'typing.Literal', refdomain='py', reftype='class', reftarget='typing.Literal'),\n                nodes.Text('['),\n                nodes.literal('', '', nodes.Text('c')),\n                nodes.Text(', '),\n                nodes.literal('', '', nodes.Text('d')),\n                nodes.Text(']'),\n                nodes.Text(']')]\n    assert_node(result, expected)\n", "def test_parse_annotation(app):\n    env = app.builder.env\n    annotations = [\n        ('List[str]', ['List[', pending_xref('', 'str', refdomain='py', reftype='class', reftarget='str'), ']']),\n        ('Optional[int]', ['Optional[', pending_xref('', 'int', refdomain='py', reftype='class', reftarget='int'), ']']),\n        ('Tuple[int, ...]', ['Tuple[', pending_xref('', 'int', refdomain='py', reftype='class', reftarget='int'), ', ', addnodes.desc_sig_punctuation('', '...'), ']']),\n        ('Union[int, str]', ['Union[', pending_xref('', 'int', refdomain='py', reftype='class', reftarget='int'), ', ', pending_xref('', 'str', refdomain='py', reftype='class', reftarget='str'), ']']),\n    ]\n\n    for annotation, expected in annotations:\n        result = _parse_annotation(annotation, env)\n        assert_node(result, expected)\n", "def test_parse_annotation_with_subscript(app):\n    env = app.builder.env\n    env.config.python_use_unqualified_type_names = True\n    result = _parse_annotation(\"List[int]\", env)\n    assert len(result) == 3\n    assert isinstance(result[0], pending_xref)\n    assert result[0].astext() == \"List\"\n    assert isinstance(result[1], nodes.Text)\n    assert result[1].astext() == \"[\"\n    assert isinstance(result[2], pending_xref)\n    assert result[2].astext() == \"int\"\n"], "sample_332": ["def test_formset_factory_with_custom_kwargs(self):\n    CustomKwargFormSet = formset_factory(CustomKwargForm, form_kwargs={'custom_kwarg': 'test_value'})\n    formset = CustomKwargFormSet()\n    for form in formset:\n        self.assertEqual(form.custom_kwarg, 'test_value')\n", "    def test_formset_factory_with_custom_kwargs(self):\n        CustomKwargFormSet = formset_factory(CustomKwargForm, extra=1, form_kwargs={'custom_kwarg': 'test'})\n        formset = CustomKwargFormSet()\n        self.assertEqual(formset.forms[0].custom_kwarg, 'test')\n", "    def test_formset_validation(self):\n        data = [('Coffee', '5'), ('Tea', '3'), ('Coffee', '2')]\n        formset = self.make_choiceformset(formset_data=data)\n        self.assertFalse(formset.is_valid())\n        self.assertEqual(formset.total_error_count(), 1)\n        self.assertEqual(formset.non_form_errors(), [])\n        self.assertEqual(len(formset.errors), 3)\n        self.assertEqual(formset.errors[0], {})\n        self.assertEqual(formset.errors[1], {})\n        self.assertEqual(formset.errors[2], {'__all__': ['You may only specify a drink once.']})\n", "def test_formset_get_form_kwargs(self):\n    class TestForm(CustomKwargForm):\n        test_field = CharField()\n\n    TestFormSet = formset_factory(TestForm, formset=BaseFormSet, form_kwargs={'custom_kwarg': 'test'})\n    formset = TestFormSet()\n    form_kwargs = formset.get_form_kwargs(0)\n    self.assertEqual(form_kwargs['custom_kwarg'], 'test')\n", "def test_choice_formset_validation(self):\n    formset_data = [('coffee', 3), ('tea', 2)]\n    formset = self.make_choiceformset(formset_data)\n    self.assertTrue(formset.is_valid())\n\n    # Test validation for too many forms\n    formset = self.make_choiceformset(formset_data, max_num_forms=1)\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(formset.non_form_errors(), ['Please submit at most 1 form.'])\n\n    # Test validation for too few forms\n    formset = self.make_choiceformset(formset_data, min_num_forms=3)\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(formset.non_form_errors(), ['Please submit at least 3 forms.'])\n\n    # Test validation for missing management form\n    formset_data = [('coffee', 3), ('tea', 2)]\n    del formset_data['choices-TOTAL_FORMS']\n    formset = self.make_choiceformset(formset_data)\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(formset.non_form_errors(), [\"ManagementForm data is missing or has been tampered with. Missing fields: choices-TOTAL_FORMS. You may need to file a bug report if the issue persists.\"])\n"], "sample_265": ["def test_template_rendering(self):\n    engine = self.engine_class({'OPTIONS': {'libraries': {}}})\n    template = engine.from_string(\"Hello, {{ name }}!\")\n    rendered = template.render(Context({\"name\": \"World\"}))\n    self.assertEqual(rendered, \"Hello, World!\")\n", "def test_templatetag_libraries(self):\n    custom_libraries = {\"custom_lib\": \"path.to.custom_lib\"}\n    engine = self.engine_class({\"OPTIONS\": {\"libraries\": custom_libraries}})\n    libraries = engine.get_templatetag_libraries(custom_libraries)\n    self.assertIn(\"custom_lib\", libraries)\n    self.assertIn(\"i18n\", libraries)  # Built-in library\n", "def test_get_templatetag_libraries(self):\n    # Test that the function correctly combines custom and installed libraries\n    custom_libraries = {'custom_lib': 'custom_lib_module'}\n    installed_libraries = {'installed_lib': 'installed_lib_module'}\n    with patch('your_module.get_installed_libraries', return_value=installed_libraries):\n        result = self.engine.get_templatetag_libraries(custom_libraries)\n        expected = {**installed_libraries, **custom_libraries}\n        self.assertEqual(result, expected)\n", "def test_get_templatetag_libraries(self):\n    custom_libraries = {'custom_tag': 'path.to.custom_tag'}\n    expected_libraries = {\n        'custom_tag': 'path.to.custom_tag',\n        # add more expected libraries from get_installed_libraries() if available\n    }\n    backend = self.engine_class({'DIRS': [], 'OPTIONS': {}})\n    libraries = backend.get_templatetag_libraries(custom_libraries)\n    self.assertEqual(libraries, expected_libraries)\n", "def test_get_installed_libraries(self):\n    # Test the function to get installed libraries\n    libraries = get_installed_libraries()\n    self.assertIn('i18n', libraries)  # Django's built-in library\n    self.assertIn('test_tags', libraries)  # Assume 'test_tags' is defined in 'template_tests.templatetags'\n    with self.assertRaises(InvalidTemplateLibrary):\n        # Assume 'invalid_tags' is defined in 'template_tests.templatetags' but doesn't have a 'register' attribute\n        import_module('template_tests.templatetags.invalid_tags')\n        get_installed_libraries()\n"], "sample_6": ["def test_angle_wrap_at():\n    a = Angle([-20.0, 150.0, 350.0] * u.deg)\n    wrapped = a.wrap_at(360 * u.deg)\n    assert_allclose(wrapped, Angle([340.0, 150.0, 350.0] * u.deg))\n\n    a.wrap_at('180d', inplace=True)\n    assert_allclose(a, Angle([-20.0, 150.0, -10.0] * u.deg))\n", "def test_angle_wrap_at():\n    a = Angle([-20.0, 150.0, 350.0] * u.deg)\n    wrapped = a.wrap_at(360 * u.deg)\n    npt.assert_allclose(wrapped.degree, [340.0, 150.0, 350.0])\n\n    a.wrap_at('180d', inplace=True)\n    npt.assert_allclose(a.degree, [-20.0, 150.0, -10.0])\n", "def test_longitude_wrap_at():\n    longitude = Longitude([270, 360, 90] * u.deg, wrap_angle=180 * u.deg)\n    wrapped_longitude = longitude.wrap_at(180 * u.deg)\n    assert_allclose(wrapped_longitude, [-90, 0, 90] * u.deg)\n", "def test_latitude_within_bounds():\n    lat = Latitude([-20, 45, 90] * u.deg)\n    assert lat.is_within_bounds(-90 * u.deg, 90 * u.deg)\n    assert not lat.is_within_bounds(0 * u.deg, 90 * u.deg)\n    assert not lat.is_within_bounds(-90 * u.deg, 0 * u.deg)\n", "def test_angle_to_string_format():\n    a = Angle(30, unit=u.degree)\n    assert a.to_string(format='unicode') == '30\u00b000\u203200\u2033'\n    assert a.to_string(format='latex') == '$30^{\\\\circ}00{}^{\\\\prime}00{}^{\\\\prime\\\\prime}$'\n    with pytest.raises(ValueError):\n        a.to_string(format='invalid')\n"], "sample_269": ["    def test_get_plural(self):\n        \"\"\"Test the get_plural method.\"\"\"\n        view = JavaScriptCatalog()\n        view.translation = gettext.translation('djangojs', localedir=locale_dir, languages=['fr'])\n        self.assertEqual(view.get_plural(), \"n > 1\")\n", "def test_set_language_post(self):\n    \"\"\"Test set_language view with POST request.\"\"\"\n    inactive_language_code = self._get_inactive_language_code()\n    url = reverse('set_language')\n    data = {'language': inactive_language_code, 'next': '/'}\n    response = self.client.post(url, data)\n\n    # Check if the response is a redirect to the translated URL\n    self.assertRedirects(response, '/' + inactive_language_code + '/')\n\n    # Check if the language cookie is set correctly\n    self.assertEqual(response.cookies[settings.LANGUAGE_COOKIE_NAME].value, inactive_language_code)\n", "    def test_get_paths(self):\n        view = JavaScriptCatalog()\n        packages = ['django.contrib.admin', 'django.contrib.auth']\n        paths = view.get_paths(packages)\n        self.assertEqual(len(paths), 2)\n        self.assertEqual(paths[0], path.join(path.dirname(gettext.__file__), '..', 'contrib', 'admin', 'locale'))\n        self.assertEqual(paths[1], path.join(path.dirname(gettext.__file__), '..', 'contrib', 'auth', 'locale'))\n", "    def test_get_formats(self):\n        formats = get_formats()\n        self.assertIsInstance(formats, dict)\n        self.assertTrue(all(key in formats for key in [\n            'DATE_FORMAT', 'DATETIME_FORMAT', 'TIME_FORMAT',\n            'YEAR_MONTH_FORMAT', 'MONTH_DAY_FORMAT', 'SHORT_DATE_FORMAT',\n            'SHORT_DATETIME_FORMAT', 'FIRST_DAY_OF_WEEK', 'DECIMAL_SEPARATOR',\n            'THOUSAND_SEPARATOR', 'NUMBER_GROUPING',\n            'DATE_INPUT_FORMATS', 'TIME_INPUT_FORMATS', 'DATETIME_INPUT_FORMATS'\n        ]))\n", "    def test_javascript_catalog_get(self):\n        \"\"\"Test the JavaScriptCatalog view's GET method.\"\"\"\n        url = reverse('javascript-catalog')\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response['Content-Type'], 'text/javascript; charset=\"utf-8\"')\n\n        # Check that the response contains the catalog, formats, and plural string\n        self.assertContains(response, 'django.catalog')\n        self.assertContains(response, 'django.formats')\n        self.assertContains(response, 'django.pluralidx')\n\n        # Check that the catalog and formats are valid JSON\n        start_index = response.content.index(b'{')\n        end_index = response.content.rindex(b'}') + 1\n        json_content = response.content[start_index:end_index]\n        json_dict = json.loads(json_content)\n        self.assertIn('catalog', json_dict)\n        self.assertIn('formats', json_dict)\n        self.assertIsInstance(json_dict['catalog'], dict)\n        self.assertIsInstance(json_dict['formats'], dict)\n\n        # Check that the plural string is a valid JavaScript function\n        plural_string = json_dict['plural']\n        if plural_string is not None:\n            plural_function = 'django.pluralidx = function(n) {%s}' % plural_string\n            compiled_code = compile(plural_function, '<string>', 'exec')\n            exec(compiled_code)\n"], "sample_38": ["    def test_maps_projection(self):\n        # Test projection attribute for each map in the list\n        for filename in self._file_list:\n            w = wcs.WCS(filename)\n            assert hasattr(w, 'projection')\n            assert isinstance(w.projection, str)\n            assert len(w.projection) > 0\n", "def test_wcs_projection_consistency(self):\n    for filename in self._file_list:\n        with fits.open(filename, memmap=False) as hdulist:\n            header = hdulist[0].header\n            wcsobj = wcs.WCS(header)\n            assert wcsobj.wcs.lngtyp == header.get('CTYPE1', '').split('-')[0]\n            assert wcsobj.wcs.lattyp == header.get('CTYPE2', '').split('-')[0]\n", "def test_no_distortion_keywords(self):\n    # Test a header with no distortion keywords\n    header = fits.Header()\n    header['NAXIS'] = 2\n    header['NAXIS1'] = 100\n    header['NAXIS2'] = 100\n    header['CTYPE1'] = 'RA---TAN'\n    header['CTYPE2'] = 'DEC--TAN'\n    header['CRVAL1'] = 0.0\n    header['CRVAL2'] = 0.0\n    header['CRPIX1'] = 50.0\n    header['CRPIX2'] = 50.0\n    header['CDELT1'] = 0.01\n    header['CDELT2'] = 0.01\n\n    with pytest.warns(wcs.FITSFixedWarning, match=\"Missing CD or PC\"):\n        w = wcs.WCS(header)\n\n    assert w.naxis == 2\n    assert w.wcs.lng == 0\n    assert w.wcs.lat == 1\n    assert w.wcs.lngtyp == 'RA'\n    assert w.wcs.lattyp == 'DEC'\n    assert_array_equal(w.wcs.crpix, [50.0, 50.0])\n    assert_array_equal(w.wcs.crval, [0.0, 0.0])\n    assert_array_equal(w.wcs.cd, [[0.01, 0.0], [0.0, 0.01]])\n", "def test_celestial_world_coordinates(self):\n    for filename in self._file_list:\n        hdr = fits.Header.fromtextfile(filename)\n        w = wcs.WCS(hdr)\n        if w.has_celestial:\n            # Test all_pix2world with celestial coordinates\n            pix = np.array([[100, 100], [200, 200]])\n            world = w.all_pix2world(pix, 1)\n            assert world.shape == (2, 2)\n\n            # Test all_world2pix with celestial coordinates\n            pix2 = w.all_world2pix(world, 1)\n            assert_allclose(pix2, pix, atol=1e-6)\n", "def test_maps(self):\n    # Iterate over all the files in the list\n    for filename in self._file_list:\n        # Read the header and the data\n        hdu = fits.open(filename)\n        header = hdu[0].header\n        data = hdu[0].data\n\n        # Create a WCS object from the header\n        w = wcs.WCS(header)\n\n        # Check that the number of WCS axes is equal to the number of data dimensions\n        assert w.naxis == data.ndim\n\n        # Check that the WCS object is not None\n        assert w is not None\n\n        # Check that the WCS object has a valid CD matrix\n        cd = w.wcs.get_cd()\n        assert np.all(np.isfinite(cd))\n        assert np.linalg.det(cd) > 0\n"], "sample_1172": ["def test_solve_poly_system():\n    x, y, z = symbols('x y z')\n    F = [x**2 + y + z - 1, x + y**2 + z - 1, x + y + z**2 - 1]\n    solutions = solve_poly_system(F, x, y, z)\n    assert solutions == [(0, 0, 1), (0, 1, 0), (1, 0, 0)]\n", "def test_solve_poly_system():\n    x, y = symbols('x y')\n    seq = [x**2 - y, x - y**2]\n    solutions = solve_poly_system(seq, x, y)\n    expected_solutions = [(0, 0), (1, 1)]\n    assert solutions == expected_solutions\n", "def test_solve_biquadratic_with_no_solution():\n    a = Poly(y + x**2, x, y, domain='ZZ')\n    b = Poly(-y + x, x, y, domain='ZZ')\n    assert solve_biquadratic(a, b, PolyOptions((x, y), {'domain': 'ZZ'})) == []\n", "def test_solve_poly_system_biquadratic_with_complex_roots():\n    a = Poly(y + x**2 - 3 + I, x, y, domain='ZZ')\n    b = Poly(-y + x - 4 - I, x, y, domain='ZZ')\n    solutions = solve_poly_system([a, b], x, y)\n    expected_solutions = [(2 - I, 1 + I), (2 + I, 1 - I)]\n    assert sorted(solutions) == sorted(expected_solutions)\n", "def test_solve_poly_system_integration():\n    x, y, z = symbols('x y z')\n    F = [x**2 + y + z - 1, x + y**2 + z - 1, x + y + z**2 - 1]\n    solution = solve_poly_system(F, x, y, z)\n    assert solution == [(0, 0, 1), (0, 1, 0), (1, 0, 0)]\n"], "sample_118": ["    def test_year_exact_lookup(self):\n        articles = Article.objects.filter(pub_date__year__exact=2005)\n        self.assertCountEqual(articles, [self.a1, self.a2, self.a3, self.a4, self.a5, self.a6, self.a7])\n", "def test_exact_lookup_with_subquery(self):\n    subquery = Article.objects.filter(headline__contains='Article').values('pk')\n    articles = Article.objects.filter(pk__exact=subquery)\n    self.assertEqual(list(articles), [self.a1, self.a2, self.a3, self.a4, self.a5, self.a6, self.a7])\n", "    def test_integer_greater_than_or_equal(self):\n        self.assertEqual(Article.objects.filter(id__gte=3).count(), 5)\n        self.assertEqual(Article.objects.filter(id__gte=3.5).count(), 5)  # Test float as rhs\n", "    def test_in_lookup_with_queryset(self):\n        articles = Article.objects.filter(headline__in=Article.objects.filter(author=self.au1))\n        self.assertEqual(list(articles), [self.a1, self.a2, self.a3, self.a4])\n", "def test_in_lookup(self):\n    articles = Article.objects.filter(headline__in=['Article 1', 'Article 3'])\n    self.assertCountEqual(articles, [self.a1, self.a3])\n"], "sample_549": ["def test_delete_masked_points_with_scalars():\n    with pytest.raises(ValueError) as excinfo:\n        delete_masked_points(1, 2, 3)\n    assert str(excinfo.value) == \"First argument must be a sequence\"\n", "def test_delete_masked_points_with_empty_array():\n    x = []\n    y = []\n    result = delete_masked_points(x, y)\n    assert result == ([], [])\n", "def test_delete_masked_points_with_ndarrays(self):\n    x = np.array([1, 2, 3, 4, 5])\n    y = np.array([1, np.nan, 3, np.inf, 5])\n    x_out, y_out = delete_masked_points(x, y)\n    np.testing.assert_array_equal(x_out, np.array([1, 3, 5]))\n    np.testing.assert_array_equal(y_out, np.array([1, 3, 5]))\n", "def test_delete_masked_points_mixed_types(self):\n    # Test that delete_masked_points handles a mix of masked arrays and other types\n    a = np.ma.masked_array([1, 2, np.nan, 4], mask=[0, 1, 0, 1])\n    b = [5, 6, 7, 8]\n    c = np.array([9, 10, 11, 12])\n    d = \"string\"\n    e = [13, np.inf, 15, 16]\n    result = delete_masked_points(a, b, c, d, e)\n    np.testing.assert_array_equal(result[0].data, [1, 4])\n    np.testing.assert_array_equal(result[1], [5, 7])\n    np.testing.assert_array_equal(result[2], [9, 11])\n    assert result[3] == \"string\"\n    np.testing.assert_array_equal(result[4], [13, np.inf, 15, 16])\n", "def test_delete_masked_points_with_masked_array():\n    a = np.ma.masked_array([1, 2, np.nan, 4], mask=[0, 0, 1, 0])\n    b = [2, 3, 5, 6]\n    c = delete_masked_points(a, b)\n    assert_array_equal(c[0].data, [1, 2, 4])\n    assert_array_equal(c[1], [2, 5, 6])\n"], "sample_107": ["    def test_get_traceback_data(self):\n        request = RequestFactory().get('/')\n        exception = ValueError(\"Test Exception\")\n        try:\n            raise exception\n        except ValueError:\n            exc_type, exc_value, tb = sys.exc_info()\n        reporter = ExceptionReporter(request, exc_type, exc_value, tb)\n        data = reporter.get_traceback_data()\n        self.assertIsInstance(data, dict)\n        self.assertIn('frames', data)\n        self.assertIsInstance(data['frames'], list)\n", "    def setUp(self):\n        self.request_factory = RequestFactory()\n", "    def test_get_traceback_frames(self):\n        try:\n            raise ValueError(\"Test exception\")\n        except ValueError as e:\n            exc_type, exc_value, tb = sys.exc_info()\n            reporter = ExceptionReporter(None, exc_type, exc_value, tb)\n            frames = reporter.get_traceback_frames()\n            self.assertIsNotNone(frames)\n            self.assertTrue(all('type' in frame for frame in frames))\n", "    def test_get_traceback_frames(self):\n        # Test that the get_traceback_frames method correctly extracts the traceback frames\n            raise Exception(\"Test Exception\")\n\n            func1()\n\n        try:\n            func2()\n        except Exception as e:\n            reporter = ExceptionReporter(None, type(e), e, e.__traceback__)\n            frames = reporter.get_traceback_frames()\n\n        # Check that the frames contain the expected functions\n        self.assertEqual(len(frames), 2)\n        self.assertEqual(frames[0]['function'], 'func1')\n        self.assertEqual(frames[1]['function'], 'func2')\n", "    def setUp(self):\n        self.factory = RequestFactory()\n"], "sample_776": ["def test_lars_path_residues():\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    alphas, active, coefs, residues = _lars_path_residues(X_train, y_train, X_test, y_test)\n    assert alphas.shape == coefs.shape[1]\n    assert residues.shape == (alphas.shape[0], X_test.shape[0])\n", "def test_lars_path_residues():\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    alphas, active, coefs, residues = _lars_path_residues(X_train, y_train, X_test, y_test)\n    assert_equal(residues.shape, (alphas.size, y_test.size))\n    assert_equal(coefs.shape, (X_train.shape[1], alphas.size))\n", "def test_lars_path_residues_positive_coef():\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n    alphas, active, coefs, residues = _lars_path_residues(\n        X_train, y_train, X_test, y_test, positive=True)\n    assert_greater(np.min(coefs), 0)\n", "def test_lars_path_residues():\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    alphas, active, coefs, residues = _lars_path_residues(X_train, y_train, X_test, y_test, method='lasso')\n    assert_greater(len(alphas), 0)\n    assert_greater(len(active), 0)\n    assert_equal(coefs.shape, (X_train.shape[1], len(alphas)))\n    assert_equal(residues.shape, (len(alphas), y_test.shape[0]))\n", "def test_lars_path_residues():\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    alphas, active, coefs, residues = _lars_path_residues(\n        X_train, y_train, X_test, y_test, method='lasso', verbose=False,\n        fit_intercept=True, normalize=True, max_iter=500, eps=np.finfo(np.float).eps, positive=False)\n    assert_equal(residues.shape, (alphas.shape[0], y_test.shape[0]))\n    assert_greater(alphas.shape[0], 0)\n    assert_greater(coefs.shape[0], 0)\n"], "sample_485": ["def test_urlize(self):\n    self.check_output(urlize, \"http://example.com\", '<a href=\"http://example.com\">http://example.com</a>')\n    self.check_output(urlize, \"www.example.com\", '<a href=\"http://www.example.com\">www.example.com</a>')\n    self.check_output(urlize, \"example@test.com\", '<a href=\"mailto:example@test.com\">example@test.com</a>')\n    self.check_output(urlize, \"Text with www.example.com and http://example.com\", 'Text with <a href=\"http://www.example.com\">www.example.com</a> and <a href=\"http://example.com\">http://example.com</a>')\n", "    def test_format_html_join(self):\n        names = [(\"Alice\", \"Smith\"), (\"Bob\", \"Johnson\")]\n        expected_output = mark_safe('<li>Alice Smith</li><li>Bob Johnson</li>')\n        self.check_output(format_html_join, ('<li>{} {}</li>', names), expected_output)\n", "def test_urlize(self):\n    text = \"Check out this website: http://example.com and this email: test@example.com\"\n    output = 'Check out this website: <a href=\"http://example.com\">http://example.com</a> and this email: <a href=\"mailto:test@example.com\">test@example.com</a>'\n    self.check_output(urlize, text, output)\n", "    def test_escapejs(self):\n        \"\"\"Test the escapejs function\"\"\"\n        self.check_output(escapejs, \"test\", \"test\")\n        self.check_output(escapejs, \"<script>alert('test');</script>\", \"\\\\u003Cscript\\\\u003Ealert(\\\\u0027test\\\\u0027);\\\\u003C/script\\\\u003E\")\n        self.check_output(escapejs, \"test\\x01test\", \"test\\\\u0001test\")\n        self.check_output(escapejs, 'test\\n\\rtest', \"test\\\\u000A\\\\u000Dtest\")\n", "    def test_escapejs(self):\n        self.check_output(escapejs, 'Test String', r'Test\\u0020String')\n        self.check_output(escapejs, '>', r'\\u003E')\n        self.check_output(escapejs, '<', r'\\u003C')\n        self.check_output(escapejs, '&', r'\\u0026')\n        self.check_output(escapejs, '=', r'\\u003D')\n        self.check_output(escapejs, '-', r'\\u002D')\n        self.check_output(escapejs, ';', r'\\u003B')\n        self.check_output(escapejs, '`', r'\\u0060')\n        self.check_output(escapejs, '\\u2028', r'\\u2028')\n        self.check_output(escapejs, '\\u2029', r'\\u2029')\n        self.check_output(escapejs, 'control char', r'control\\u0020char')\n"], "sample_1022": ["def test_convert_equals_signs():\n    expr = parse_expr(\"(1*2=x)=False\", transformations=(standard_transformations + (convert_equals_signs,)))\n    assert expr == sympy.Eq(sympy.Eq(2, sympy.symbols('x')), False)\n", "def test_split_symbols_custom_with_excluded_names():\n        if symbol not in ('list', 'of', 'unsplittable', 'names'):\n                return _token_splittable(symbol)\n        return False\n\n    transformation = split_symbols_custom(can_split)\n    result = parse_expr('unsplittable', transformations=standard_transformations + (transformation, implicit_multiplication))\n    assert str(result) == 'unsplittable'\n", "def test_function_exponentiation_nested():\n    transformations = standard_transformations + (function_exponentiation,)\n    expr = parse_expr(\"sin**(cos**2)(x)\", transformations=transformations)\n    expected = sympy.sin(x)**(sympy.cos(x)**2)\n    assert expr == expected\n", "def test_function_exponentiation():\n    # Test case for function_exponentiation transformation\n    transformations = standard_transformations + (function_exponentiation,)\n    expr = parse_expr(\"sin**4(x)\", transformations=transformations)\n    assert expr == sympy.sin(x)**4\n", "def test_convert_xor():\n    # Testing convert_xor function\n    tokens = [(sympy.parsing.sympy_parser.OP, '^'), (sympy.parsing.sympy_parser.OP, '+')]\n    result = convert_xor(tokens, {}, {})\n    assert result == [(sympy.parsing.sympy_parser.OP, '**'), (sympy.parsing.sympy_parser.OP, '+')]\n"], "sample_20": ["def test_single_table_roundtrip(self, tmpdir):\n    table = Table(self.data)\n    filename = str(tmpdir.join(\"test.fits\"))\n    table.write(filename, format=\"fits\")\n    new_table = Table.read(filename, format=\"fits\")\n    assert_array_equal(table.as_array(), new_table.as_array())\n", "def test_roundtrip_table(self):\n    t = Table(self.data)\n    hdu = table_to_hdu(t)\n    new_t = Table.read(hdu)\n    assert equal_data(t, new_t)\n", "    def test_write_table_fits(self, tmp_path):\n        t = Table(self.data)\n        output_file = str(tmp_path / \"test.fits\")\n        fits.write_table_fits(t, output_file)\n        with fits.open(output_file) as hdul:\n            assert equal_data(hdul[1].data, t)\n", "    def test_time_column_conversion(self):\n        times = Time(['2000-01-01T00:00:00', '2000-01-02T00:00:00'], format='isot')\n        table = QTable([times], names=['time'])\n        hdu = table_to_hdu(table)\n        assert hdu.columns[0].format == 'D'\n        new_table = Table.read(hdu)\n        assert_array_equal(new_table['time'], times.jd)\n        new_table = Table.read(hdu, format='fits', astropy_native=True)\n        assert_array_equal(new_table['time'], times)\n", "    def test_write_table_fits_with_mixin_columns(self, tmp_path):\n        # Create a table with mixin columns\n        table = QTable(mixin_cols)\n        table_path = str(tmp_path / \"test_table.fits\")\n\n        # Write the table to a FITS file\n        fits.write_table_fits(table, table_path)\n\n        # Read the table back from the FITS file\n        read_table = fits.read_table_fits(table_path)\n\n        # Check that the table data is equal\n        assert equal_data(table.as_array(), read_table.as_array())\n\n        # Check that the table meta is equal\n        assert table.meta == read_table.meta\n\n        # Check that the table column attributes are equal\n        for name in serialized_names:\n            assert_array_equal(getattr(table[name], compare_attrs[name]), getattr(read_table[name], compare_attrs[name]))\n"], "sample_245": ["    def test_msgid_plural_and_msgstr(self):\n        \"\"\"Test the assertMsgIdPlural and assertMsgStr methods.\"\"\"\n        with open(self.PO_FILE, 'w') as fp:\n            fp.write('msgid_plural \"ngettext test\\\\n\"')\n            fp.write('msgstr[0] \"German singular\\\\n\"')\n            fp.write('msgstr[1] \"German plural\\\\n\"')\n\n        self.assertMsgIdPlural('ngettext test', fp)\n        self.assertMsgStr('German singular', fp)\n        self.assertMsgStr('German plural', fp)\n", "def test_handle_add_location_option(self):\n    \"\"\"\n    Test the --add-location option.\n    \"\"\"\n    self.work_dir.joinpath('django_app', 'templates', 'django_app', 'template.html').write_text(\n        '{% load i18n %}{% blocktrans %}Translate this.{% endblocktrans %}',\n    )\n    self._run_makemessages(add_location='full')\n    self.assertLocationCommentPresent(self.PO_FILE, 'Translate this.', 'django_app', 'templates', 'django_app', 'template.html')\n    self._run_makemessages(add_location='file')\n    self.assertLocationCommentPresent(self.PO_FILE, None, 'django_app', 'templates', 'django_app', 'template.html')\n    self._run_makemessages(add_location='never')\n    self.assertLocationCommentNotPresent(self.PO_FILE, 'Translate this.', 'django_app', 'templates', 'django_app', 'template.html')\n", "def test_msgattrib(self):\n    self.copy_test_data('msgattrib')\n    with self.settings(USE_I18N=True):\n        output, po_contents = self._run_makemessages(no_obsolete=True)\n        self.assertMsgId('obsolete', po_contents)\n        self.assertMsgStr('Obsolete', po_contents)\n        self.assertMsgId('do not remove', po_contents)\n        self.assertMsgStr('Do not remove', po_contents)\n        self.assertNotMsgId('obsolete', output)\n        self.assertNotMsgId('Obsolete', output)\n", "    def test_domain_djangojs(self):\n        self.mkdir('app')\n        with self.write('app/js/test.js', \"\"\"\n            gettext_noop('Message 1');\n            gettext_lazy('Message 2');\n            ngettext_lazy('Singular', 'Plural', 1);\n            pgettext('Context', 'Message 3');\n            npgettext('Context', 'Singular', 'Plural', 2);\n        \"\"\"):\n            self._run_makemessages(domain='djangojs')\n            self.assertMsgId('Message 1', self.PO_FILE)\n            self.assertMsgId('Message 2', self.PO_FILE)\n            self.assertMsgId('Singular', self.PO_FILE)\n            self.assertMsgIdPlural('Plural', self.PO_FILE)\n            self.assertMsgId('Context\\\\004Message 3', self.PO_FILE)\n            self.assertMsgId('Context\\\\004Singular', self.PO_FILE)\n            self.assertMsgIdPlural('Context\\\\004Plural', self.PO_FILE)\n", "def test_build_potfiles(self):\n    # Arrange\n    self.work_dir.joinpath('file1.py').write_text('gettext_noop(\"Message 1\")')\n    self.work_dir.joinpath('file2.py').write_text('gettext_noop(\"Message 2\")')\n    self.work_dir.joinpath('locale', LOCALE, 'LC_MESSAGES').mkdir(parents=True)\n\n    # Act\n    command = MakeMessagesCommand()\n    command.locale_paths = [str(self.work_dir.joinpath('locale'))]\n    command.domain = 'django'\n    command.extensions = ['.py']\n    command.ignore_patterns = []\n    command.symlinks = False\n    command.build_potfiles()\n\n    # Assert\n    potfile = self.work_dir.joinpath('locale', LOCALE, 'LC_MESSAGES', 'django.pot')\n    self.assertTrue(potfile.exists())\n    with open(potfile, 'r') as file:\n        content = file.read()\n        self.assertIn('msgid \"Message 1\"', content)\n        self.assertIn('msgid \"Message 2\"', content)\n"], "sample_50": ["    def test_runshell_db_without_password(self):\n        dbinfo = {\n            'host': 'localhost',\n            'port': 5432,\n            'database': 'mydb',\n            'user': 'myuser',\n        }\n        args, pgpassword = self._run_it(dbinfo)\n        self.assertEqual(args, ['psql', '-U', 'myuser', '-h', 'localhost', '-p', '5432', 'mydb'])\n        self.assertIsNone(pgpassword)\n", "    def test_runshell_with_empty_parameters(self):\n        dbinfo = {}\n        args, pgpassword = self._run_it(dbinfo)\n        self.assertEqual(args, ['psql'])\n        self.assertIsNone(pgpassword)\n", "def test_runshell_command_without_password(self):\n    dbinfo = {\n        'host': 'localhost',\n        'port': '5432',\n        'database': 'testdb',\n        'user': 'testuser',\n    }\n    cmd_args, pgpassword = self._run_it(dbinfo)\n    self.assertEqual(cmd_args, ['psql', '-U', 'testuser', '-h', 'localhost', '-p', '5432', 'testdb'])\n    self.assertIsNone(pgpassword)\n", "    def test_runshell_with_empty_dbinfo(self):\n        \"\"\"\n        Test the runshell command with an empty dbinfo dictionary.\n        \"\"\"\n        args, pgpassword = self._run_it({})\n        expected_args = [DatabaseClient.executable_name]\n        self.assertEqual(args, expected_args)\n        self.assertIsNone(pgpassword)\n", "def test_runshell(self):\n    dbinfo = {\n        'host': 'localhost',\n        'port': 5432,\n        'database': 'testdb',\n        'user': 'testuser',\n        'password': 'testpassword'\n    }\n    expected_args = ['psql', '-U', 'testuser', '-h', 'localhost', '-p', '5432', 'testdb']\n    expected_pgpassword = 'testpassword'\n\n    args, pgpassword = self._run_it(dbinfo)\n    self.assertEqual(args, expected_args)\n    self.assertEqual(pgpassword, expected_pgpassword)\n"], "sample_162": ["    def test_keep_pot_option(self):\n        output, po_contents = self._run_makemessages(keep_pot=True)\n        self.assertTrue(os.path.exists('locale/%s.pot' % LOCALE))\n", "def test_keep_pot(self):\n    with self.modify_settings(USE_I18N=True):\n        with self.settings(LOCALE_PATHS=[os.path.join(self.work_dir, 'locale')]):\n            output, po_contents = self._run_makemessages(keep_pot=True)\n            self.assertTrue(os.path.exists('locale/%s/LC_MESSAGES/django.pot' % LOCALE))\n", "def test_location_comment_with_add_location(self):\n    \"\"\"\n    Test that the --add-location option produces correct location comments.\n    \"\"\"\n    self.add_python_file('django.py', '\"Test string\"')\n    self._run_makemessages(add_location='full')\n    self.assertLocationCommentPresent(self.PO_FILE, 1, 'django.py')\n    self._run_makemessages(add_location='file')\n    self.assertLocationCommentPresent(self.PO_FILE, None, 'django.py')\n    self._run_makemessages(add_location='never')\n    self.assertLocationCommentNotPresent(self.PO_FILE, 1, 'django.py')\n", "    def test_skip_media_static_files(self):\n        # Create test project structure\n        os.makedirs(os.path.join('test_project', 'media'))\n        os.makedirs(os.path.join('test_project', 'static'))\n        with open(os.path.join('test_project', 'templates', 'test.html'), 'w') as f:\n            f.write('<p>{% trans \"Test string\" %}</p>')\n\n        # Call makemessages command\n        self._run_makemessages(settings='test_project.settings')\n\n        # Check that media and static files are not processed\n        self.assertNotLocationCommentPresent(self.PO_FILE, None, 'media', 'test.html')\n        self.assertNotLocationCommentPresent(self.PO_FILE, None, 'static', 'test.html')\n", "    def test_command_with_django_domain(self):\n        with self.create_files('project', ['templates/template.html']) as project:\n            with open(project.path('templates', 'template.html'), 'w') as fp:\n                fp.write(\"{% trans 'Hello, world!' %}\")\n            options = {'domain': 'django'}\n            output, po_contents = self._run_makemessages(**options)\n            self.assertMsgId('Hello, world!', po_contents)\n            self.assertLocationCommentPresent(self.PO_FILE, 'Hello, world!', 'templates', 'template.html')\n"], "sample_1147": ["compilation error", "compilation error", "def test_print_lowergamma():\n    expr = lowergamma(x, y)\n    latex_expr = latex(expr)\n    assert latex_expr == r\"\\gamma\\left(x, y\\right)\"\n", "def test_latex_printing_of_tensorproduct():\n    A = ArraySymbol('A', (2, 2))\n    B = ArraySymbol('B', (2, 2))\n    expr = TensorProduct(A, B)\n    assert latex(expr) == r\"A \\otimes B\"\n", "def test_lowergamma():\n    expr = lowergamma(x, y)\n    latex_str = latex(expr)\n    assert latex_str == r\"\\gamma\\left(x, y\\right)\"\n"], "sample_734": ["def test_fowlkes_mallows_score():\n    labels_true = np.array([0, 0, 1, 1, 2, 2, 3, 3])\n    labels_pred = np.array([0, 0, 1, 2, 2, 3, 3, 3])\n    assert_almost_equal(fowlkes_mallows_score(labels_true, labels_pred), 0.8577638849908081)\n", "def test_fowlkes_mallows_score():\n    # Test case where clusters are identical\n    labels_true = np.array([0, 0, 1, 1])\n    labels_pred = np.array([0, 0, 1, 1])\n    assert_almost_equal(fowlkes_mallows_score(labels_true, labels_pred), 1.0)\n\n    # Test case where clusters are completely different\n    labels_true = np.array([0, 0, 0, 0])\n    labels_pred = np.array([1, 1, 1, 1])\n    assert_almost_equal(fowlkes_mallows_score(labels_true, labels_pred), 0.0)\n\n    # Test case with random clusters\n    labels_true = np.array([0, 0, 1, 1, 2, 2])\n    labels_pred = np.array([1, 1, 0, 0, 2, 2])\n    assert_almost_equal(fowlkes_mallows_score(labels_true, labels_pred), 1.0)\n\n    # Test case with partially overlapping clusters\n    labels_true = np.array([0, 0, 1, 1, 2, 2])\n    labels_pred = np.array([0, 0, 1, 3, 2, 2])\n    assert_almost_equal(fowlkes_mallows_score(labels_true, labels_pred), 0.816496580927726, decimal=10)\n", "def test_expected_mutual_information():\n    contingency = np.array([[2, 2], [2, 2]])\n    n_samples = 4\n    assert_almost_equal(expected_mutual_information(contingency, n_samples), 0.5)\n", "def test_mutual_info_score():\n    labels_true = np.array([0, 0, 1, 1])\n    labels_pred = np.array([0, 0, 1, 1])\n    assert_almost_equal(mutual_info_score(labels_true, labels_pred), 1.0)\n\n    labels_true = np.array([0, 0, 1, 1])\n    labels_pred = np.array([1, 1, 0, 0])\n    assert_almost_equal(mutual_info_score(labels_true, labels_pred), 1.0)\n\n    labels_true = np.array([0, 0, 0, 0])\n    labels_pred = np.array([0, 1, 2, 3])\n    assert_almost_equal(mutual_info_score(labels_true, labels_pred), 0.0)\n", "def test_fowlkes_mallows_score():\n    # Test with perfect labelings\n    assert_almost_equal(fowlkes_mallows_score([0, 0, 1, 1], [0, 0, 1, 1]), 1.0)\n    assert_almost_equal(fowlkes_mallows_score([0, 0, 1, 1], [1, 1, 0, 0]), 1.0)\n\n    # Test with completely split classes across different clusters\n    assert_almost_equal(fowlkes_mallows_score([0, 0, 0, 0], [0, 1, 2, 3]), 0.0)\n"], "sample_172": ["def test_formfield_for_charfield(self):\n    self.assertFormfield(Car, 'make', widgets.TextInput)\n", "def test_formfield_for_manytomanyfield(self):\n    \"\"\"\n    Test the formfield_for_dbfield method for a ManyToManyField.\n    \"\"\"\n    self.assertFormfield(Honeycomb, 'bees', widgets.ManyToManyRawIdWidget)\n", "def test_formfield_for_foreignkey(self):\n    \"\"\"\n    Tests that the correct widget is used for ForeignKey fields.\n    \"\"\"\n    # Test with ForeignKeyRawIdWidget for raw_id_fields\n    self.assertFormfield(Car, 'owner', widgets.ForeignKeyRawIdWidget, raw_id_fields=['owner'])\n\n    # Test with AutocompleteSelect for autocomplete_fields\n    self.assertFormfield(Car, 'owner', widgets.AutocompleteSelect, autocomplete_fields=['owner'])\n\n    # Test with AdminRadioSelect for radio_fields\n    self.assertFormfield(Car, 'owner', widgets.AdminRadioSelect, radio_fields={'owner': admin.VERTICAL})\n\n    # Test with default widget for other cases\n    self.assertFormfield(Car, 'owner', forms.Select)\n", "def test_formfield_for_datefield(self):\n    \"\"\"\n    Test formfield_for_dbfield for DateField.\n    \"\"\"\n    self.assertFormfield(MyModel, 'my_date_field', widgets.AdminDateWidget)\n", "def test_datefield_uses_admindatewidget(self):\n    \"\"\"\n    ModelAdmin.formfield_for_dbfield should use AdminDateWidget for DateField.\n    \"\"\"\n    self.assertFormfield(Member, 'date_joined', widgets.AdminDateWidget)\n"], "sample_767": ["def test_column_transformer_remainder_drop():\n    X = np.array([[0, 1, 2], [3, 4, 5]])\n    ct = ColumnTransformer(transformers=[(\"trans\", Trans(), [0])], remainder='drop')\n    ct.fit_transform(X)\n    expected_output = np.array([[0], [3]])\n    assert_array_equal(ct.transform(X), expected_output)\n", "def test_column_transformer_with_callable():\n    X = np.array([[0, 1, 2], [3, 4, 5]])\n    transformer = ColumnTransformer([\n        (\"transformer\", Trans(), lambda X: [0, 1]),\n    ])\n    expected_result = np.array([[0, 1], [3, 4]])\n    assert_array_equal(transformer.fit_transform(X), expected_result)\n", "def test_remainder_estimator():\n    # Test that remainder estimator is applied correctly\n    X = np.array([[0, 1, 2, 3], [1, 2, 3, 4]])\n    ct = ColumnTransformer([('norm', Normalizer(), [0, 1])], remainder=StandardScaler())\n    Xt = ct.fit_transform(X)\n    assert_almost_equal(Xt[:, 2:], StandardScaler().fit_transform(X[:, 2:]))\n", "def test_column_transformer_feature_names():\n    # Test get_feature_names with transformers that have get_feature_names\n    ct = ColumnTransformer([('scaler', StandardScaler(), ['a', 'b']),\n                             ('encoder', OneHotEncoder(), [0])])\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    ct.fit(X)\n    feature_names = ct.get_feature_names()\n    assert_equal(feature_names, ['scaler__a', 'scaler__b', 'encoder__x0_0', 'encoder__x0_1', 'encoder__x0_2'])\n\n    # Test get_feature_names with 'passthrough' transformer\n    ct = ColumnTransformer([('scaler', StandardScaler(), ['a', 'b']),\n                             ('passthrough', 'passthrough', [0])])\n    ct.fit(X)\n    assert_raise_message(NotImplementedError,\n                         \"get_feature_names is not yet supported when using a 'passthrough' transformer.\",\n                         ct.get_feature_names)\n\n    # Test get_feature_names with transformer that does not have get_feature_names\n    ct = ColumnTransformer([('trans', Trans(), [0])])\n    ct.fit(X)\n    assert_raise_message(AttributeError,\n                         \"Transformer trans (type Trans) does not provide get_feature_names.\",\n                         ct.get_feature_names)\n", "def test_remainder_passthrough():\n    X = np.array([[0., 1., 2., 2.],\n                  [1., 1., 0., 1.]])\n\n    ct = ColumnTransformer(\n        transformers=[(\"norm1\", Normalizer(norm='l1'), [0, 1])],\n        remainder='passthrough')\n\n    transformed = ct.fit_transform(X)\n    expected = np.array([[0., 1., 2., 2.],\n                         [0.5, 0.5, 0., 1.]])\n\n    assert_array_equal(transformed, expected)\n"], "sample_1044": ["def test_pow_as_base_exp():\n    p = Pow(S.Half, 2, evaluate=False)\n    assert p.as_base_exp() == (2, -2)\n    assert p.args == (S.Half, 2)\n", "def test_pow_as_base_exp():\n    p = Pow(S.Half, 2, evaluate=False)\n    assert p.as_base_exp() == (2, -2)\n", "def test_is_polynomial():\n    x, y = symbols('x y')\n    assert (x**2).is_polynomial(x)\n    assert not (x**y).is_polynomial(x)\n    assert not (x**2).is_polynomial(y)\n", "def test_pow_is_zero():\n    x = Symbol('x')\n    assert (0**x).is_zero is True\n    assert (0**x).is_zero is not False\n    assert (0**x).is_zero is not None\n    assert (0**(-1)).is_zero is False\n", "def test_pow_expansion_real_base_and_rational_exp():\n    x = Symbol('x')\n    assert (sqrt(x + 2))._eval_expand_multinomial() == x/2 + sqrt(2) + sqrt(x)/2 - 1/2\n"], "sample_620": ["def test_dataarray_concat_with_scalar_coord():\n    da1 = DataArray(np.arange(5), coords={\"scalar_coord\": 1}, name=\"data\")\n    da2 = DataArray(np.arange(5, 10), coords={\"scalar_coord\": 2}, name=\"data\")\n    concatenated = concat([da1, da2], dim=\"new_dim\")\n\n    expected_coords = {\n        \"scalar_coord\": (\"new_dim\", [1, 2]),\n        \"new_dim\": [0, 1],\n    }\n    assert concatenated.coords == expected_coords\n", "def test_concat_with_inaccessible_array():\n    x = DataArray(InaccessibleArray(np.arange(5)))\n    y = DataArray(InaccessibleArray(np.arange(5, 10)))\n    result = concat([x, y], dim='dim')\n    expected = DataArray(InaccessibleArray(np.arange(10)))\n    assert_identical(result, expected)\n", "def test_concat_with_positions():\n    da1 = DataArray(np.arange(6).reshape(2, 3), dims=(\"x\", \"y\"))\n    da2 = DataArray(np.arange(6, 12).reshape(2, 3), dims=(\"x\", \"y\"))\n\n    result = concat([da1, da2], dim=\"x\", positions=[[0], [2]])\n    expected = DataArray(np.concatenate([da1.values, da2.values], axis=0), dims=(\"x\", \"y\"))\n    assert_identical(result, expected)\n", "def test_concat_with_overlap():\n    da1 = DataArray(np.arange(6).reshape(2, 3), dims=(\"x\", \"y\"))\n    da2 = DataArray(np.arange(6, 12).reshape(2, 3), dims=(\"x\", \"y\"))\n    da3 = DataArray(np.arange(12, 18).reshape(2, 3), dims=(\"x\", \"y\"))\n    da4 = DataArray(np.arange(18, 24).reshape(2, 3), dims=(\"x\", \"y\"))\n\n    result = concat([da1, da2, da3, da4], dim=\"z\")\n    expected = DataArray(\n        np.concatenate([da1.values, da2.values, da3.values, da4.values]).reshape(4, 2, 3),\n        dims=(\"z\", \"x\", \"y\"),\n    )\n    assert_identical(result, expected)\n", "def test_dataarray_concat_fill_value(fill_value):\n    x = DataArray(np.random.randn(3, 4))\n    y = DataArray(np.random.randn(2, 4))\n    result = concat([x, y], dim=\"dim\", fill_value=fill_value)\n    expected = np.concatenate([x.values, np.full((1, 4), fill_value) if fill_value is not None else np.nan, y.values], axis=0)\n    assert_array_equal(result.values, expected)\n"], "sample_785": ["def test_time_series_split():\n    tscv = TimeSeriesSplit(n_splits=5)\n    for train_index, test_index in tscv.split(X):\n        assert_greater(test_index[0], train_index[-1])\n        assert_equal(len(train_index) + len(test_index), len(X))\n", "def test_train_test_split_errors():\n    # Test that train_test_split raises an error when no arrays are provided\n    with assert_raises(ValueError):\n        train_test_split()\n\n    # Test that train_test_split raises an error when shuffle is False and stratify is not None\n    with assert_raises(ValueError):\n        train_test_split(X, y, shuffle=False, stratify=y)\n\n    # Test that train_test_split raises an error when an invalid parameter is passed\n    with assert_raises(TypeError):\n        train_test_split(X, y, invalid_param=True)\n", "def test_time_series_split():\n    tscv = TimeSeriesSplit(n_splits=3)\n    train_indices, test_indices = list(tscv.split(X))\n    assert_equal(train_indices, [[0, 1, 2], [0, 1, 2, 3, 4], [0, 1, 2, 3, 4, 5, 6]])\n    assert_equal(test_indices, [[3, 4], [5, 6], [7, 8, 9]])\n", "def test_GroupShuffleSplit():\n    \"\"\"Test GroupShuffleSplit\"\"\"\n    groups = [1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4]\n    gss = GroupShuffleSplit(n_splits=2, test_size=2, random_state=0)\n    expected_splits = [\n        (np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]), np.array([14])),\n        (np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14]), np.array([12]))\n    ]\n    for i, (train, test) in enumerate(gss.split(X, y, groups)):\n        assert_array_equal(train, expected_splits[i][0])\n        assert_array_equal(test, expected_splits[i][1])\n", "def test_repeated_kfold():\n    # Test RepeatedKFold with default parameters\n    rkf = RepeatedKFold()\n    assert rkf.n_splits == 5\n    assert rkf.n_repeats == 10\n    assert rkf.random_state is None\n\n    # Test RepeatedKFold with custom parameters\n    rkf = RepeatedKFold(n_splits=3, n_repeats=2, random_state=42)\n    assert rkf.n_splits == 3\n    assert rkf.n_repeats == 2\n    assert rkf.random_state == 42\n\n    # Test RepeatedKFold split method\n    X = np.arange(10).reshape(5, 2)\n    y = np.array([0, 0, 1, 1, 2])\n    rkf = RepeatedKFold(n_splits=2, n_repeats=2, random_state=42)\n    splits = list(rkf.split(X, y))\n    assert len(splits) == 4  # 2 splits * 2 repeats\n"], "sample_213": ["    def test_save_form_data(self):\n        obj = ImageFieldTestModel()\n        data = 'path/to/file.jpg'\n        obj._meta.get_field('image').save_form_data(obj, data)\n        self.assertEqual(obj.image, data)\n", "    def test_fieldfile_init(self):\n        instance = Storage()\n        field = FileField()\n        name = 'test.txt'\n        fieldfile = FieldFile(instance, field, name)\n        self.assertEqual(fieldfile.name, name)\n        self.assertEqual(fieldfile.instance, instance)\n        self.assertEqual(fieldfile.field, field)\n        self.assertEqual(fieldfile.storage, field.storage)\n        self.assertTrue(fieldfile._committed)\n", "    def test_file_descriptor_get_with_string(self):\n        instance = Storage(text='test.txt')\n        file = instance.text\n        self.assertIsInstance(file, FileField.attr_class)\n        self.assertEqual(file.name, 'test.txt')\n", "    def test_update_dimension_fields(self):\n        # Create a new instance of a model with an ImageField\n        instance = ImageModel.objects.create()\n        # Assign a file to the ImageField\n        file = SimpleUploadedFile(\"test_image.jpg\", b\"file_content\", content_type=\"image/jpeg\")\n        instance.image_field.save(\"test_image.jpg\", file, save=True)\n\n        # Check that the width and height fields are updated\n        self.assertEqual(instance.width_field, file.width)\n        self.assertEqual(instance.height_field, file.height)\n", "    def test_save_form_data(self):\n        instance = Storage()\n        data = 'path/to/file.txt'\n        field = FileField()\n        field.name = 'file'\n        field.save_form_data(instance, data)\n        self.assertEqual(instance.file, data)\n\n        data = None\n        field.save_form_data(instance, data)\n        self.assertEqual(instance.file, '')\n"], "sample_345": ["    def test_extra_files(self):\n        extra_file = self.temporary_file('extra_file.py')\n        extra_file.touch()\n        self.clear_autoreload_caches()\n        # Test uncached access\n        self.assertNotIn(extra_file, list(autoreload.iter_all_python_module_files()))\n        autoreload._error_files.append(str(extra_file))\n        self.assertFileFound(extra_file)\n        autoreload._error_files.remove(str(extra_file))\n        self.assertFileNotFound(extra_file)\n", "    def test_import_error_file(self):\n        filename = self.temporary_file(\"error_module.py\")\n        with open(filename, 'w') as f:\n            f.write(\"syntax error: unmatched parenthesis\")\n        self.import_and_cleanup(\"error_module\")\n        self.assertFileFound(filename)\n        self.assertIn(filename, autoreload._error_files)\n", "    def test_iter_modules_and_files_with_typing_modules(self):\n        self.import_and_cleanup('typing.io')\n        self.import_and_cleanup('typing.re')\n\n        # Typing modules are not ModuleType instances, so they should be skipped.\n        self.clear_autoreload_caches()\n        files = list(autoreload.iter_all_python_module_files())\n        self.assertNotIn('typing.io', files)\n        self.assertNotIn('typing.re', files)\n", "    def test_zip_import(self):\n        zip_file = self.temporary_file('test.zip')\n        with zipfile.ZipFile(str(zip_file), 'w') as zf:\n            zf.writestr('test_module.py', '')\n        sys.path.insert(0, str(zip_file))\n        self.addCleanup(sys.path.remove, str(zip_file))\n        self.import_and_cleanup('test_module')\n        self.assertFileFound(zip_file)\n", "    def test_zip_imported_module(self):\n        with tempfile.TemporaryDirectory() as temp_dir:\n            zip_path = Path(temp_dir) / 'test_module.zip'\n            module_path = Path(temp_dir) / 'test_module'\n            module_path.mkdir()\n            (module_path / '__init__.py').touch()\n            (module_path / 'test.py').touch()\n            with zipfile.ZipFile(zip_path, 'w') as z:\n                z.write(module_path / '__init__.py', arcname='test_module/__init__.py')\n                z.write(module_path / 'test.py', arcname='test_module/test.py')\n            sys.path.insert(0, str(zip_path))\n            self.addCleanup(sys.path.remove, str(zip_path))\n            self.import_and_cleanup('test_module')\n            self.assertFileFound(zip_path)\n"], "sample_219": ["def test_expression_wrapper_get_group_by_cols(self):\n    expr = ExpressionWrapper(Sum('num_employees'), output_field=IntegerField())\n    group_by_cols = expr.get_group_by_cols()\n    self.assertEqual(len(group_by_cols), 1)\n    self.assertIsInstance(group_by_cols[0], Sum)\n    self.assertEqual(group_by_cols[0].field, 'num_employees')\n", "def test_combined_expression_resolution(self):\n    expr = CombinedExpression(F('num_employees'), Combinable.ADD, Value(100))\n    resolved_expr = expr.resolve_expression(query=self.company_query)\n    self.assertIsInstance(resolved_expr, CombinedExpression)\n    self.assertEqual(resolved_expr.connector, Combinable.ADD)\n    self.assertIsInstance(resolved_expr.lhs, Col)\n    self.assertIsInstance(resolved_expr.rhs, Value)\n", "def test_window_expression(self):\n    with self.assertRaisesMessage(ValueError, \"Expression 'F' isn't compatible with OVER clauses.\"):\n        self.company_query.annotate(avg_salary=Window(F('ceo__salary')))\n", "def test_combinable_operations(self):\n    # Test addition\n    expr = F('num_employees') + 10\n    self.assertEqual(str(expr), '`test_app_company`.`num_employees` + 10')\n\n    # Test subtraction\n    expr = F('num_employees') - 5\n    self.assertEqual(str(expr), '`test_app_company`.`num_employees` - 5')\n\n    # Test multiplication\n    expr = F('num_employees') * 2\n    self.assertEqual(str(expr), '`test_app_company`.`num_employees` * 2')\n\n    # Test division\n    expr = F('num_employees') / 3\n    self.assertEqual(str(expr), '`test_app_company`.`num_employees` / 3')\n\n    # Test modulus\n    expr = F('num_employees') % 4\n    self.assertEqual(str(expr), '`test_app_company`.`num_employees` %% 4')\n\n    # Test power\n    expr = F('num_employees') ** 2\n    self.assertEqual(str(expr), '`test_app_company`.`num_employees` ^ 2')\n", "def test_expression_wrapper_get_group_by_cols(self):\n    expr = ExpressionWrapper(F('num_employees') + 1, output_field=IntegerField())\n    self.assertEqual(expr.get_group_by_cols(), [F('num_employees')])\n"], "sample_1185": ["def test_compogen():\n    f = compogen([sin(x), cos(x)], x)\n    assert f == sin(cos(x))\n\n    f = compogen([x**2 + x + 1, sin(x)], x)\n    assert f == sin(x)**2 + sin(x) + 1\n\n    f = compogen([sqrt(x), 6*x**2 - 5], x)\n    assert f == sqrt(6*x**2 - 5)\n\n    f = compogen([sin(x), sqrt(x), cos(x), x**2 + 1], x)\n    assert f == sin(sqrt(cos(x**2 + 1)))\n\n    f = compogen([x**2 - x - 1, x**2 + x], x)\n    assert f == -x**2 - x + (x**2 + x)**2 - 1\n", "def test_decompogen_compogen_consistency():\n    functions = [sin(cos(x)), sqrt(6*x**2 - 5), exp(x**2 + 1), Max(x, y)]\n    for f in functions:\n        decomposition = decompogen(f, x)\n        composition = compogen(decomposition, x)\n        assert f.expand() == composition.expand(), \"Decomposition and composition are not consistent for function: {}\".format(f)\n", "def test_compogen_with_polynomial():\n    g_s = [x**2 - x - 1, x**2 + x]\n    result = compogen(g_s, x)\n    expected = -x**2 - x + (x**2 + x)**2 - 1\n    assert result == expected\n", "def test_decompogen_compogen():\n    # Testing decompogen and compogen functions together\n    f = sin(sqrt(cos(x**2 + 1)))\n    g_s = decompogen(f, x)\n    assert compogen(g_s, x) == f\n", "def test_compogen():\n    f = sin(cos(x))\n    g_s = decompogen(f, x)\n    assert compogen(g_s, x) == f\n\n    f = sqrt(6*x**2 - 5)\n    g_s = decompogen(f, x)\n    assert compogen(g_s, x) == f\n\n    f = sin(sqrt(cos(x**2 + 1)))\n    g_s = decompogen(f, x)\n    assert compogen(g_s, x) == f\n\n    f = x**4 + 2*x**3 - x - 1\n    g_s = decompogen(f, x)\n    assert compogen(g_s, x) == f\n\n    f = Max(x, y, x**2 - y**2)\n    g_s = decompogen(f, x)\n    assert compogen(g_s, x) == f\n"], "sample_1189": ["def test_lambdify_with_implemented_function():\n    f = implemented_function(Function('f'), lambda x: x + 1)\n    func = lambdify(x, f(x))\n    assert func(4) == 5\n", "def test_lambdify_with_deferred_vector():\n    from sympy.matrices import DeferredVector\n    A = DeferredVector('A')\n    f = lambdify(A, A[0] + A[1])\n    assert f([1, 2]) == 3\n", "def test_lambdify_with_matrix():\n    expr = Matrix([x, y])\n    f = lambdify((x, y), expr)\n    result = f(1, 2)\n    assert result.shape == (2, 1)\n    assert result[0, 0] == 1\n    assert result[1, 0] == 2\n", "compilation error", "def test_lambdify_with_tensorflow():\n    import tensorflow as tf\n    from sympy import Max, sin, lambdify\n    from sympy.abc import x\n\n    f = Max(x, sin(x))\n    func = lambdify(x, f, 'tensorflow')\n\n    tf.compat.v1.enable_eager_execution()\n\n    result = func(tf.constant(1.0))\n    assert result.numpy() == 1.0\n\n    var = tf.Variable(2.0)\n    result = func(var)\n    assert result.numpy() == 2.0\n\n    tensor = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n    result = func(tensor)\n    expected_result = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n    assert tf.reduce_all(tf.equal(result, expected_result))\n"], "sample_379": ["    def test_mark_safe_decorator(self):\n        @mark_safe\n            return '<html>Safe content</html>'\n\n        self.assertIsInstance(safe_func(), SafeString)\n        self.assertEqual(safe_func(), '<html>Safe content</html>')\n", "    def test_mark_safe_with_custom_safe_data(self):\n        safe_data = customescape(\"Test <customescape> string\")\n        safe_data = mark_safe(safe_data)\n        self.assertIsInstance(safe_data, SafeData)\n        self.assertEqual(safe_data.__html__(), \"Test <<customescape>> string\")\n", "    def test_mark_safe_with_custom_class(self):\n        custom_data = customescape('<tag>')\n        safe_custom_data = mark_safe(custom_data)\n        self.assertIsInstance(safe_custom_data, SafeData)\n        self.assertEqual(safe_custom_data, '<tag>')\n        self.assertRenderEqual('{{ data }}', '<tag>', data=safe_custom_data)\n", "def test_safe_string_operations(self):\n    safe1 = SafeString(\"<b>Bold text</b>\")\n    safe2 = SafeString(\"<i>Italic text</i>\")\n    not_safe = \"<u>Underlined text</u>\"\n\n    # Concatenation with another SafeString should result in a SafeString\n    self.assertEqual(safe1 + safe2, SafeString(\"<b>Bold text</b><i>Italic text</i>\"))\n\n    # Concatenation with a regular string should result in a regular string\n    self.assertEqual(safe1 + not_safe, \"<b>Bold text</b><u>Underlined text</u>\")\n\n    # mark_safe should return a SafeString\n    self.assertIsInstance(mark_safe(\"Safe text\"), SafeString)\n\n    # mark_safe should handle SafeData objects\n    safe_data = customescape(\"<custom>Escape this</custom>\")\n    self.assertIsInstance(mark_safe(safe_data), SafeString)\n\n    # mark_safe should handle functions\n    @mark_safe\n        return \"Safe from function\"\n    self.assertIsInstance(safe_func(), SafeString)\n", "    def test_add_safe_string(self):\n        s1 = SafeString(\"Hello, \")\n        s2 = SafeString(\"world!\")\n        s3 = s1 + s2\n        self.assertIsInstance(s3, SafeString)\n        self.assertEqual(s3, \"Hello, world!\")\n"], "sample_167": ["    def test_intcomma_with_decimal(self):\n        self.assertEqual(humanize.intcomma(Decimal('1234567890')), '1,234,567,890')\n        self.assertEqual(humanize.intcomma(Decimal('1234567890.123')), '1,234,567,890.123')\n        self.assertEqual(humanize.intcomma(Decimal('-1234567890')), '-1,234,567,890')\n", "    def test_ordinal(self):\n        self.assertEqual(humanize.ordinal(1), \"1st\")\n        self.assertEqual(humanize.ordinal(2), \"2nd\")\n        self.assertEqual(humanize.ordinal(3), \"3rd\")\n        self.assertEqual(humanize.ordinal(4), \"4th\")\n        self.assertEqual(humanize.ordinal(11), \"11th\")\n        self.assertEqual(humanize.ordinal(12), \"12th\")\n        self.assertEqual(humanize.ordinal(13), \"13th\")\n        self.assertEqual(humanize.ordinal(\"foo\"), \"foo\")\n        self.assertEqual(humanize.ordinal(None), None)\n", "    def test_intcomma_with_locale_and_decimal(self):\n        with translation.override('de'):\n            self.assertEqual(humanize.intcomma(Decimal('1234567.89')), '1.234.567,89')\n", "    def test_naturaltime(self):\n        with self.settings(USE_L10N=False):\n            # Test future time\n            value = now + datetime.timedelta(seconds=10)\n            self.assertEqual(humanize.naturaltime(value), 'a second from now')\n\n            value = now + datetime.timedelta(minutes=1)\n            self.assertEqual(humanize.naturaltime(value), 'a minute from now')\n\n            value = now + datetime.timedelta(hours=1)\n            self.assertEqual(humanize.naturaltime(value), 'an hour from now')\n\n            value = now + datetime.timedelta(days=1)\n            self.assertEqual(humanize.naturaltime(value), '1 day from now')\n\n            # Test past time\n            value = now - datetime.timedelta(seconds=10)\n            self.assertEqual(humanize.naturaltime(value), 'a second ago')\n\n            value = now - datetime.timedelta(minutes=1)\n            self.assertEqual(humanize.naturaltime(value), 'a minute ago')\n\n            value = now - datetime.timedelta(hours=1)\n            self.assertEqual(humanize.naturaltime(value), 'an hour ago')\n\n            value = now - datetime.timedelta(days=1)\n            self.assertEqual(humanize.naturaltime(value), '1 day ago')\n", "def test_apnumber_with_valid_input(self):\n    self.assertEqual(humanize.apnumber(1), _('one'))\n    self.assertEqual(humanize.apnumber(5), _('five'))\n    self.assertEqual(humanize.apnumber(9), _('nine'))\n    self.assertEqual(humanize.apnumber(10), 10)\n    self.assertEqual(humanize.apnumber('1'), _('one'))\n"], "sample_421": ["    def setUpTestData(cls):\n        CaseTestModel.objects.create(integer=None, integer2=1, string=\"1\")\n        CaseTestModel.objects.create(integer=2, integer2=None, string=\"2\")\n        CaseTestModel.objects.create(integer=3, integer2=3, string=None)\n", "def test_case_expression_with_complex_condition(self):\n    # Test the Case expression with a complex condition\n    qs = CaseTestModel.objects.annotate(\n        complex_case=Case(\n            When(Q(integer__gt=F('integer2')) | Q(string__endswith='3'), then=Value('GT or ends with 3')),\n            When(Q(integer__lt=F('integer2')) & Q(string__startswith='2'), then=Value('LT and starts with 2')),\n            default=Value('Other')\n        )\n    )\n    result = list(qs.values('id', 'complex_case').order_by('id'))\n    expected_result = [\n        {'id': 1, 'complex_case': 'Other'},\n        {'id': 2, 'complex_case': 'LT and starts with 2'},\n        {'id': 3, 'complex_case': 'GT or ends with 3'},\n        {'id': 4, 'complex_case': 'GT or ends with 3'},\n        {'id': 5, 'complex_case': 'GT or ends with 3'},\n        {'id': 6, 'complex_case': 'GT or ends with 3'},\n        {'id': 7, 'complex_case': 'Other'},\n    ]\n    self.assertEqual(result, expected_result)\n", "def test_case_expression_with_mixed_types(self):\n    cases = [\n        When(integer__gt=3, then=Value(\"high\")),\n        When(integer__lt=2, then=Value(\"low\")),\n        When(integer__gte=2, integer__lte=3, then=Value(\"medium\")),\n    ]\n    queryset = CaseTestModel.objects.annotate(\n        category=Case(*cases, default=Value(\"unknown\"))\n    ).values(\"category\")\n    result = sorted(queryset, key=itemgetter(\"category\"))\n    self.assertEqual(\n        result,\n        [\n            {'category': 'high'},\n            {'category': 'low'},\n            {'category': 'medium'},\n            {'category': 'medium'},\n            {'category': 'medium'},\n            {'category': 'medium'},\n            {'category': 'unknown'},\n        ],\n    )\n", "def test_case_expression_with_aggregates(self):\n    with connection.cursor() as cursor:\n        query = CaseTestModel.objects.values(*self.group_by_fields).annotate(\n            case_max=Max('integer2'),\n            case_min=Min('integer2'),\n            case_sum=Sum('integer2'),\n            case_count=Count('integer2'),\n        ).order_by(*self.group_by_fields)\n        cursor.execute(str(query.query))\n        results = cursor.fetchall()\n        self.assertEqual(\n            list(query),\n            [\n                {\n                    'integer': 1,\n                    'integer2': 1,\n                    'string': '1',\n                    'case_max': 1,\n                    'case_min': 1,\n                    'case_sum': 1,\n                    'case_count': 1,\n                },\n                # Add more expected results as needed\n            ]\n        )\n", "    def test_case_expression_with_aggregate_functions(self):\n        query = CaseTestModel.objects.annotate(\n            max_integer=Max('integer'),\n            min_integer=Min('integer'),\n            sum_integer=Sum('integer'),\n            count_integer=Count('integer')\n        ).annotate(\n            case_expression=Case(\n                When(Q(integer=F('max_integer')), then=Value('Maximum')),\n                When(Q(integer=F('min_integer')), then=Value('Minimum')),\n                default=Value('Other'),\n                output_field=TextField()\n            )\n        )\n        results = query.values('integer', 'case_expression', 'sum_integer', 'count_integer')\n        expected_results = [\n            {'integer': 1, 'case_expression': 'Minimum', 'sum_integer': 21, 'count_integer': 1},\n            {'integer': 2, 'case_expression': 'Other', 'sum_integer': 21, 'count_integer': 4},\n            {'integer': 3, 'case_expression': 'Other', 'sum_integer': 21, 'count_integer': 6},\n            {'integer': 4, 'case_expression': 'Maximum', 'sum_integer': 21, 'count_integer': 1},\n        ]\n        self.assertCountEqual(results, expected_results)\n"], "sample_849": ["def test_build_repr():\n    mock_classifier = MockClassifier(a=1, allow_nd=True)\n    assert _build_repr(mock_classifier) == \"MockClassifier(a=1, allow_nd=True)\"\n", "def test_leave_p_out():\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n    y = np.array([1, 2, 3, 4])\n    lpo = LeavePOut(p=2)\n    assert lpo.get_n_splits(X) == 6\n    splits = list(lpo.split(X, y))\n    assert len(splits) == 6\n    expected_splits = [(np.array([2, 3]), np.array([0, 1])),\n                       (np.array([1, 3]), np.array([0, 2])),\n                       (np.array([1, 2]), np.array([0, 3])),\n                       (np.array([0, 3]), np.array([1, 2])),\n                       (np.array([0, 2]), np.array([1, 3])),\n                       (np.array([0, 1]), np.array([2, 3]))]\n    for i, (train_index, test_index) in enumerate(splits):\n        assert_array_equal(train_index, expected_splits[i][0])\n        assert_array_equal(test_index, expected_splits[i][1])\n", "def test_build_repr():\n    \"\"\"Test the _build_repr function\"\"\"\n    classifier = MockClassifier(a=1, allow_nd=True)\n    expected_repr = \"MockClassifier(a=1, allow_nd=True)\"\n    assert _build_repr(classifier) == expected_repr\n", "def test_build_repr():\n    mock_classifier = MockClassifier(a=1, allow_nd=True)\n    representation = _build_repr(mock_classifier)\n    assert representation == \"MockClassifier(a=1, allow_nd=True)\"\n", "def test_leave_p_groups_out():\n    for groups in test_groups:\n        cv = LeavePGroupsOut(n_groups=2)\n        n_splits = int(comb(len(np.unique(groups)), 2, exact=True))\n        assert cv.get_n_splits(groups=groups) == n_splits\n        splits = list(cv.split(X, y, groups))\n        assert len(splits) == n_splits\n        for train_index, test_index in splits:\n            assert len(np.unique(groups[test_index])) == 2\n"], "sample_12": ["def test_longitude_wrap_angle_setter():\n    lon = Longitude(180, unit=u.deg)\n    assert lon.wrap_angle == 360 * u.deg\n    lon.wrap_angle = 180 * u.deg\n    assert lon.wrap_angle == 180 * u.deg\n    assert_allclose(lon.degree, -180)\n", "def test_angle_subtraction():\n    a = Angle(45, unit=u.deg)\n    b = Angle(30, unit=u.deg)\n    c = a - b\n    assert c == Angle(15, unit=u.deg)\n", "def test_angle_wrap_at():\n    # Test wrapping at a specific angle\n    a = Angle([-20.0, 150.0, 350.0] * u.deg)\n    wrapped_a = a.wrap_at(180 * u.deg)\n    assert_array_equal(wrapped_a.degree, np.array([-20.0, -30.0, -10.0]))\n\n    # Test wrapping at the same angle in-place\n    a.wrap_at(180 * u.deg, inplace=True)\n    assert_array_equal(a.degree, np.array([-20.0, -30.0, -10.0]))\n\n    # Test wrapping at a different angle\n    a = Angle([-20.0, 150.0, 350.0] * u.deg)\n    wrapped_a = a.wrap_at(270 * u.deg)\n    assert_array_equal(wrapped_a.degree, np.array([-20.0, 150.0, -120.0]))\n\n    # Test wrapping at a different angle in-place\n    a.wrap_at(270 * u.deg, inplace=True)\n    assert_array_equal(a.degree, np.array([-20.0, 150.0, -120.0]))\n", "def test_longitude_wrap_angle():\n    # Test setting and getting the wrap_angle property\n    lon = Longitude(180.0, unit=u.deg, wrap_angle=180.0*u.deg)\n    assert lon.wrap_angle == 180.0*u.deg\n\n    # Test changing the wrap_angle property\n    lon.wrap_angle = 360.0*u.deg\n    assert lon.wrap_angle == 360.0*u.deg\n    assert_allclose(lon.degree, 180.0)\n\n    # Test initializing Longitude with a wrap_angle\n    lon = Longitude(180.0, unit=u.deg, wrap_angle=360.0*u.deg)\n    assert lon.wrap_angle == 360.0*u.deg\n    assert_allclose(lon.degree, 180.0)\n\n    # Test wrapping angles at the wrap_angle\n    lon = Longitude([179.9, 180.0, 180.1], unit=u.deg, wrap_angle=180.0*u.deg)\n    assert_allclose(lon.degree, [-179.9, 180.0, -179.9])\n", "def test_wrap_at():\n    # Test wrapping at a specific angle\n    angle = Longitude([200, 370, -10], u.deg)\n    wrapped_angle = angle.wrap_at(180, inplace=False)\n    expected_angle = Longitude([-160, -10, -10], u.deg)\n    assert_allclose(wrapped_angle, expected_angle)\n\n    # Test wrapping inplace\n    angle = Longitude([200, 370, -10], u.deg)\n    angle.wrap_at(180, inplace=True)\n    expected_angle = Longitude([-160, -10, -10], u.deg)\n    assert_allclose(angle, expected_angle)\n"], "sample_523": ["def test_legend_with_draggable():\n    fig, ax = plt.subplots()\n    line, = ax.plot([1, 2, 3], [1, 2, 3], label='Test line')\n    legend = ax.legend(handles=[line], draggable=True)\n    assert isinstance(legend._draggable, mlegend.DraggableLegend)\n    legend.set_draggable(state=False)\n    assert legend._draggable is None\n", "def test_legend_set_loc():\n    fig, ax = plt.subplots()\n    legend = ax.legend()\n    legend._set_loc(2)  # Upper left\n    assert legend._loc == 2\n    legend._set_loc(\"lower right\")\n    assert legend._loc == 4\n    with pytest.raises(ValueError):\n        legend._set_loc(\"invalid_loc\")\n", "def test_legend_title_fontproperties():\n    fig, ax = plt.subplots()\n    title_fontproperties = FontProperties(size=12, weight='bold')\n    ax.plot([1, 2, 3], [4, 5, 6], label='Test')\n    legend = ax.legend(title='Title', title_fontproperties=title_fontproperties)\n    assert legend.get_title().get_fontproperties() == title_fontproperties\n", "def test_legend_set_bbox_to_anchor():\n    fig, ax = plt.subplots()\n    line, = ax.plot([1, 2, 3], [1, 2, 3], label='Test Line')\n    legend = ax.legend(bbox_to_anchor=(0.5, 0.5), loc='upper left')\n    assert legend.get_bbox_to_anchor().bounds == (0.5, 0.5, 0, 0)\n    legend.set_bbox_to_anchor((0.7, 0.7))\n    assert legend.get_bbox_to_anchor().bounds == (0.7, 0.7, 0, 0)\n", "def test_legend_markerfirst():\n    fig, ax = plt.subplots()\n    line1, = ax.plot([1, 2, 3], label='Line 1')\n    line2, = ax.plot([3, 2, 1], label='Line 2')\n    ax.legend(markerfirst=False)\n"], "sample_68": ["    def test_get_traceback_data(self):\n        request = RequestFactory().get('/')\n        exc_type = ValueError\n        exc_value = ValueError(\"Test exception\")\n        tb = exc_value.__traceback__\n        reporter = ExceptionReporter(request, exc_type, exc_value, tb)\n        data = reporter.get_traceback_data()\n\n        self.assertEqual(data['is_email'], False)\n        self.assertEqual(data['unicode_hint'], '')\n        self.assertEqual(len(data['frames']), 1)\n        self.assertEqual(data['request'], request)\n        self.assertEqual(data['user_str'], 'jacob')\n        self.assertEqual(data['exception_type'], 'ValueError')\n        self.assertEqual(data['exception_value'], 'Test exception')\n", "    def test_get_traceback_frames(self):\n        request = RequestFactory().get('/')\n        exception = Exception('test exception')\n        reporter = ExceptionReporter(request, type(exception), exception, exception.__traceback__)\n        frames = reporter.get_traceback_frames()\n        self.assertTrue(frames)  # Ensure some frames are returned\n", "    def test_get_traceback_frames(self):\n            pass\n\n        sensitive_function('sensitive', 'not_sensitive', c='sensitive')\n        _, _, tb = sys.exc_info()\n        reporter = ExceptionReporter(None, None, None, tb)\n        frames = reporter.get_traceback_frames()\n        self.assertEqual(len(frames), 1)\n        frame = frames[0]\n        self.assertEqual(frame['function'], 'sensitive_function')\n        vars = dict(frame['vars'])\n        self.assertEqual(vars['a'], CLEANSED_SUBSTITUTE)\n        self.assertEqual(vars['b'], 'not_sensitive')\n        self.assertEqual(vars['c'], CLEANSED_SUBSTITUTE)\n", "    def test_get_traceback_frames_with_context_hide(self):\n            __traceback_hide__ = True\n            raise Exception(\"This error should be hidden.\")\n\n        request = RequestFactory().get('/')\n        try:\n            func_with_context_hide(request)\n        except Exception as e:\n            reporter = ExceptionReporter(request, type(e), e, e.__traceback__)\n            frames = reporter.get_traceback_frames()\n            self.assertNotIn('func_with_context_hide', [frame['function'] for frame in frames])\n", "    def setUp(self):\n        self.rf = RequestFactory()\n"], "sample_90": ["def test_modelform_defines_fields(self):\n    class MyModelForm(forms.ModelForm):\n        class Meta:\n            model = Author\n            fields = ['name']\n\n    class MyOtherModelForm(forms.ModelForm):\n        class Meta:\n            model = Author\n            exclude = ['name']\n\n    class MyEmptyModelForm(forms.ModelForm):\n        pass\n\n    self.assertTrue(modelform_defines_fields(MyModelForm))\n    self.assertTrue(modelform_defines_fields(MyOtherModelForm))\n    self.assertFalse(modelform_defines_fields(MyEmptyModelForm))\n", "    def test_form_explicit_pk(self):\n        form_data = {'key': 'test_key', 'desc': 'test_desc'}\n        form = ExplicitPKForm(data=form_data)\n        self.assertTrue(form.is_valid())\n        self.assertEqual(form.cleaned_data['key'], 'test_key')\n        self.assertEqual(form.cleaned_data['desc'], 'test_desc')\n        obj = form.save()\n        self.assertEqual(obj.key, 'test_key')\n        self.assertEqual(obj.desc, 'test_desc')\n", "    def test_model_form_metaclass(self):\n        class MyModelForm(forms.ModelForm):\n            class Meta:\n                model = Book\n                fields = ['title', 'author']\n\n        self.assertIn('title', MyModelForm.base_fields)\n        self.assertIn('author', MyModelForm.base_fields)\n        self.assertNotIn('pub_date', MyModelForm.base_fields)\n", "    def test_improved_article_form_with_parent_link(self):\n        # Test that a ModelForm can be created for a model with a parent_link\n        class ImprovedArticleWithParentLinkForm(forms.ModelForm):\n            class Meta:\n                model = ImprovedArticleWithParentLink\n                fields = '__all__'\n\n        form = ImprovedArticleWithParentLinkForm()\n        self.assertIsInstance(form.instance, ImprovedArticleWithParentLink)\n", "def test_construct_instance(self):\n    form_data = {'title': 'Test Book', 'publisher': 'Test Publisher'}\n    form = BookForm(data=form_data)\n    self.assertTrue(form.is_valid())\n    instance = construct_instance(form, Book())\n    self.assertEqual(instance.title, 'Test Book')\n    self.assertEqual(instance.publisher, 'Test Publisher')\n"], "sample_381": ["def test_deconstructible_objects(self):\n    project_state_before = self.make_project_state([self.author_name_deconstructible_1])\n    project_state_after = self.make_project_state([self.author_name_deconstructible_2])\n    changes = self.get_changes(project_state_before, project_state_after)\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['AlterField'])\n    self.assertOperationFieldAttributes(changes, 'testapp', 0, 0, default=DeconstructibleObject())\n", "def test_altered_db_table_options_change(self):\n    \"\"\"\n    Tests that changing db_table options is detected and AlterModelTable\n    operations are generated.\n    \"\"\"\n    changes = self.get_changes([\n        self.author_with_db_table_options,\n    ], [\n        self.author_with_new_db_table_options,\n    ])\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['AlterModelTable'])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, table='author_two')\n", "def test_create_model_with_indexes_and_constraints(self):\n    before_states = []\n    after_states = [self.author_with_options]\n    changes = self.get_changes(before_states, after_states)\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['CreateModel', 'AlterModelOptions'])\n    self.assertOperationAttributes(changes, 'testapp', 0, 1, options={'permissions': [('can_hire', 'Can hire')], 'verbose_name': 'Authi'})\n", "def test_create_model_with_swappable_field(self):\n    class SwappableAuthor(models.Model):\n        user = models.OneToOneField(settings.AUTH_USER_MODEL, on_delete=models.CASCADE)\n\n    before_state = self.make_project_state([])\n    after_state = self.make_project_state([self.author_empty, self.custom_user, SwappableAuthor])\n\n    changes = self.get_changes(before_state, after_state)\n    self.assertNumberMigrations(changes, 'testapp', 2)\n    self.assertOperationTypes(changes, 'testapp', 0, ['CreateModel'])\n    self.assertOperationTypes(changes, 'testapp', 1, ['CreateModel'])\n    self.assertMigrationDependencies(changes, 'testapp', 1, [('testapp', '__first__')])\n    self.assertOperationAttributes(changes, 'testapp', 1, 0, name='SwappableAuthor')\n", "def test_remove_field_order_with_respect_to(self):\n    before_states = [self.author_with_book_order_wrt]\n    after_states = [self.author_with_book]\n    questioner = MigrationQuestioner(specified_apps={\"testapp\"}, dry_run=True)\n    changes = self.get_changes(before_states, after_states, questioner)\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"RemoveField\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"book\")\n"], "sample_373": ["    def test_get_return_data_type(self):\n        self.assertEqual(get_return_data_type('get_list'), 'List')\n        self.assertEqual(get_return_data_type('get_count'), 'Integer')\n        self.assertEqual(get_return_data_type('get_something'), '')\n", "    def test_get_return_data_type(self):\n        self.assertEqual(get_return_data_type('get_list'), 'List')\n        self.assertEqual(get_return_data_type('get_count'), 'Integer')\n        self.assertEqual(get_return_data_type('get_something_else'), '')\n", "    def test_model_detail_view(self):\n        url = reverse('django-admindocs-model-detail', kwargs={'app_label': 'tests', 'model_name': 'person'})\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n        self.assertContains(response, 'Person object')\n        self.assertContains(response, 'name')\n        self.assertContains(response, 'age')\n        self.assertContains(response, 'company')\n", "def test_get_return_data_type(self):\n    self.assertEqual(get_return_data_type('get_list'), 'List')\n    self.assertEqual(get_return_data_type('get_count'), 'Integer')\n    self.assertEqual(get_return_data_type('get_item'), '')\n", "def test_get_return_data_type(self):\n    self.assertEqual(get_return_data_type('get_list'), 'List')\n    self.assertEqual(get_return_data_type('get_count'), 'Integer')\n    self.assertEqual(get_return_data_type('get_name'), '')\n"], "sample_261": ["def test_parse_duration(self):\n    self.assertEqual(parse_duration('3 12:34:56.789'), timedelta(days=3, hours=12, minutes=34, seconds=56, microseconds=789000))\n    self.assertEqual(parse_duration('P3DT12H34M56.789S'), timedelta(days=3, hours=12, minutes=34, seconds=56, microseconds=789000))\n    self.assertEqual(parse_duration('3 days 12:34:56.789'), timedelta(days=3, hours=12, minutes=34, seconds=56, microseconds=789000))\n    self.assertEqual(parse_duration('-3 12:34:56.789'), -timedelta(days=3, hours=12, minutes=34, seconds=56, microseconds=789000))\n    self.assertEqual(parse_duration('-P3DT12H34M56.789S'), -timedelta(days=3, hours=12, minutes=34, seconds=56, microseconds=789000))\n    self.assertEqual(parse_duration('-3 days 12:34:56.789'), -timedelta(days=3, hours=12, minutes=34, seconds=56, microseconds=789000))\n    self.assertIsNone(parse_duration('Invalid duration'))\n", "    def test_parse_datetime_with_offset(self):\n        dt = parse_datetime('2022-01-01T12:34:56+02:00')\n        self.assertIsInstance(dt, datetime)\n        self.assertEqual(dt.year, 2022)\n        self.assertEqual(dt.month, 1)\n        self.assertEqual(dt.day, 1)\n        self.assertEqual(dt.hour, 12)\n        self.assertEqual(dt.minute, 34)\n        self.assertEqual(dt.second, 56)\n        self.assertEqual(dt.tzinfo, get_fixed_timezone(120))\n", "    def test_parse_duration_postgres_interval(self):\n        value = \"3 days 04:05:06.123456\"\n        expected = timedelta(days=3, hours=4, minutes=5, seconds=6, microseconds=123456)\n        self.assertEqual(parse_duration(value), expected)\n", "    def test_parse_datetime_with_timezone(self):\n        dt = parse_datetime('2022-03-04T12:34:56-05:00')\n        self.assertEqual(dt, datetime(2022, 3, 4, 12, 34, 56, tzinfo=get_fixed_timezone(-300)))\n", "    def test_parse_datetime_with_tzinfo(self):\n        value = \"2022-01-01T12:30:45+05:00\"\n        expected = datetime(2022, 1, 1, 12, 30, 45, tzinfo=get_fixed_timezone(300))\n        self.assertEqual(parse_datetime(value), expected)\n"], "sample_306": ["    def test_parse_datetime(self):\n        # Test parsing datetime with timezone\n        dt_str = '2022-01-01T12:34:56+03:00'\n        dt = parse_datetime(dt_str)\n        self.assertEqual(dt, datetime(2022, 1, 1, 12, 34, 56, tzinfo=get_fixed_timezone(180)))\n\n        # Test parsing datetime without timezone\n        dt_str = '2022-01-01T12:34:56'\n        dt = parse_datetime(dt_str)\n        self.assertEqual(dt, datetime(2022, 1, 1, 12, 34, 56))\n\n        # Test parsing datetime with microseconds\n        dt_str = '2022-01-01T12:34:56.123456'\n        dt = parse_datetime(dt_str)\n        self.assertEqual(dt, datetime(2022, 1, 1, 12, 34, 56, 123456))\n\n        # Test parsing invalid datetime\n        dt_str = 'invalid-datetime'\n        dt = parse_datetime(dt_str)\n        self.assertIsNone(dt)\n", "    def test_parse_datetime_with_timezone_offset(self):\n        # Test parsing datetime with timezone offset\n        dt_str = '2022-01-01T12:30:45+05:00'\n        expected_dt = datetime(2022, 1, 1, 12, 30, 45, tzinfo=get_fixed_timezone(300))\n        self.assertEqual(parse_datetime(dt_str), expected_dt)\n", "    def test_parse_datetime_with_offset(self):\n        dt = parse_datetime('2022-01-01T00:00:00-05:00')\n        self.assertEqual(dt, datetime(2022, 1, 1, 0, 0, 0, tzinfo=get_fixed_timezone(-300)))\n", "    def test_parse_duration_postgres_interval(self):\n        duration = parse_duration('3 days 04:05:06.123456')\n        self.assertEqual(duration, timedelta(days=3, hours=4, minutes=5, seconds=6, microseconds=123456))\n", "def test_parse_datetime(self):\n    \"\"\"Test the parse_datetime function.\"\"\"\n    dt = parse_datetime(\"2022-01-01T12:30:45.123456Z\")\n    self.assertEqual(dt, datetime(2022, 1, 1, 12, 30, 45, 123456, tzinfo=utc))\n\n    dt = parse_datetime(\"2022-01-01T12:30:45.123456-0500\")\n    self.assertEqual(dt, datetime(2022, 1, 1, 12, 30, 45, 123456, tzinfo=get_fixed_timezone(-300)))\n\n    dt = parse_datetime(\"2022-01-01T12:30:45.123456+0530\")\n    self.assertEqual(dt, datetime(2022, 1, 1, 12, 30, 45, 123456, tzinfo=get_fixed_timezone(330)))\n\n    dt = parse_datetime(\"2022-01-01T12:30:45\")\n    self.assertEqual(dt, datetime(2022, 1, 1, 12, 30, 45))\n\n    dt = parse_datetime(\"2022-01-01 12:30:45\")\n    self.assertEqual(dt, datetime(2022, 1, 1, 12, 30, 45))\n\n    with self.assertRaises(ValueError):\n        parse_datetime(\"2022-02-30T12:30:45\")  # Invalid date\n\n    with self.assertRaises(ValueError):\n        parse_datetime(\"2022-01-01T25:30:45\")  # Invalid time\n\n"], "sample_1103": ["def test_pow_as_content_primitive():\n    eq = (3**((5 + y)/2)).as_content_primitive()\n    assert eq == (9, 3**((y + 1)/2))\n", "def test_pow_with_oo():\n    assert Pow(oo, 2) == oo\n    assert Pow(-oo, 2) == oo\n    assert Pow(oo, -2) == 0\n    assert Pow(-oo, -2) == 0\n    assert Pow(0, oo) == 0\n    assert Pow(0, -oo) == zoo\n", "def test_integer_nthroot_python():\n    # Test _integer_nthroot_python function with some examples\n    assert integer_nthroot_python(16, 2) == (4, True)\n    assert integer_nthroot_python(26, 2) == (5, False)\n    assert integer_nthroot_python(0, 3) == (0, True)\n    assert integer_nthroot_python(1, 5) == (1, True)\n    assert integer_nthroot_python(27, 3) == (3, True)\n    assert integer_nthroot_python(81, 4) == (3, False)\n", "def test_pow_is_algebraic():\n    assert (a**(2*b)).is_algebraic == (a**b).is_algebraic\n    assert (a**b).is_algebraic == (a**(b/2)).is_algebraic\n    assert (a**b).is_algebraic == (a**(b/3)).is_algebraic\n    assert (a**b).is_algebraic == False  # Assuming a is not algebraic and b is not a rational number\n", "def test_Pow_eval_is_rational():\n    assert (x**2)._eval_is_rational() == True\n    assert (x**(-2))._eval_is_rational() == True\n    assert (x**(-1))._eval_is_rational() == True\n    assert (x**y)._eval_is_rational() == None\n    assert (2**x)._eval_is_rational() == False\n    assert (2**(-x))._eval_is_rational() == False\n    assert (0**x)._eval_is_rational() == True\n    assert (0**y)._eval_is_rational() == True\n    assert (0**0)._eval_is_rational() == True\n    assert ((-1)**x)._eval_is_rational() == False\n    assert ((-1)**y)._eval_is_rational() == False\n    assert ((-1)**0)._eval_is_rational() == True\n"], "sample_411": ["def test_execute_with_system_checks(self, mock_check):\n    out = StringIO()\n    cmd = BaseCommand()\n    cmd.requires_system_checks = [Tags.models]\n    cmd.execute(stdout=out)\n    mock_check.assert_called_once_with(tags=[Tags.models])\n", "    def test_command_error_handling(self):\n        command = BaseCommand()\n        command.stderr = StringIO()\n        test_error_message = \"Test error message\"\n        test_error_returncode = 42\n        try:\n            raise CommandError(test_error_message, returncode=test_error_returncode)\n        except CommandError as e:\n            command.handle_error(e)\n        self.assertIn(test_error_message, command.stderr.getvalue())\n        self.assertEqual(e.returncode, test_error_returncode)\n", "    def test_create_parser_help_formatter(self):\n        command = BaseCommand()\n        parser = command.create_parser('prog_name', 'subcommand')\n        self.assertIsInstance(parser.formatter_class, management.DjangoHelpFormatter)\n", "def test_handle_system_checks(self, mock_checks):\n    mock_checks.run_checks.return_value = []\n    cmd = dance.Command()\n    cmd.requires_system_checks = [Tags.models]\n    cmd.handle_system_checks = mock.Mock()\n    cmd.handle()\n    mock_checks.run_checks.assert_called_once_with(app_configs=None, tags=[Tags.models], include_deployment_checks=False, databases=None)\n", "def test_system_check_error(self, mock_check):\n    mock_check.side_effect = management.check.SystemCheckError(\"System check error\")\n    command = BaseCommand()\n    command.requires_system_checks = \"__all__\"\n    command.stderr = StringIO()\n    with self.assertRaises(management.check.SystemCheckError):\n        command.execute()\n    self.assertIn(\"System check error\", command.stderr.getvalue())\n"], "sample_1168": ["def test_is_palindromic():\n    assert is_palindromic([1, 0, 1])\n    assert not is_palindromic('abcbb')\n    assert not is_palindromic('abcbb', 1)\n    assert is_palindromic('abcbb', 1, -1)\n    assert is_palindromic('abcbb', -4, -1)\n", "def test_rotations():\n    assert list(rotations([1, 2, 3])) == [[1, 2, 3], [2, 3, 1], [3, 1, 2]]\n    assert list(rotations([1, 2, 3], -1)) == [[1, 2, 3], [3, 1, 2], [2, 3, 1]]\n", "def test_is_palindromic():\n    assert is_palindromic('abcba') == True\n    assert is_palindromic('abc') == False\n    assert is_palindromic('abcba', 1) == False\n    assert is_palindromic('abcba', 1, -1) == True\n    assert is_palindromic('abcba', -4, -1) == True\n", "def test_rotations():\n    seq = list(rotations([1, 2, 3]))\n    assert seq == [[1, 2, 3], [2, 3, 1], [3, 1, 2]]\n    seq = list(rotations([1, 2, 3], -1))\n    assert seq == [[1, 2, 3], [3, 1, 2], [2, 3, 1]]\n", "def test_is_palindromic():\n    assert is_palindromic([1, 0, 1]) is True\n    assert is_palindromic('abcbb') is False\n    assert is_palindromic('abcbb', 1) is False\n    assert is_palindromic('abcbb', 1, -1) is True\n    assert is_palindromic('abcbb', -4, -1) is True\n    assert is_palindromic(['a', 'b', 'c', 'b', 'a']) is True\n    assert is_palindromic(['a', 'b', 'c', 'b', 'a'], 1, -1) is True\n    assert is_palindromic(['a', 'b', 'c', 'b', 'a'], -4, -1) is True\n    assert is_palindromic(['a', 'b', 'c', 'b', 'd'], 1, -1) is False\n    assert is_palindromic(['a', 'b', 'c', 'b', 'd'], -4, -1) is False\n"], "sample_809": ["def test_sparse_continuous_features():\n    X = csr_matrix([[1, 0], [0, 1], [1, 1]])\n    y = np.array([0, 1, 2])\n    with assert_raises(ValueError):\n        mutual_info_regression(X, y)\n", "def test_sparse_matrix_continuous_features():\n    X = csr_matrix(np.random.rand(10, 5))\n    y = np.random.rand(10)\n    with assert_raises(ValueError):\n        mutual_info_regression(X, y)\n", "def test_sparse_matrix_with_continuous_features():\n    X = csr_matrix(np.random.rand(10, 5))\n    y = np.random.rand(10)\n    assert_raises(ValueError, mutual_info_regression, X, y)\n    assert_raises(ValueError, mutual_info_classif, X, y)\n", "def test_sparse_continuous_features():\n    X = csr_matrix([[1, 2], [3, 4]])\n    y = np.array([1, 2])\n    with assert_raises(ValueError, match=\"Sparse matrix `X` can't have continuous features.\"):\n        mutual_info_regression(X, y)\n", "def test_mutual_info_classif_sparse():\n    rng = check_random_state(42)\n    X = csr_matrix(rng.randint(0, 2, size=(100, 10)))\n    y = rng.randint(0, 3, size=100)\n    mi = mutual_info_classif(X, y, discrete_features=True)\n    assert_equal(mi.shape, (10,))\n    assert_greater(mi.min(), 0)\n"], "sample_42": ["def test_with_H0_equivalency():\n    # Test with default H0 value\n    q = 1 * u.Mpc * u.littleh\n    assert_quantity_allclose(q.to(u.Mpc, u.with_H0()), cosmology.default_cosmology.get().h * u.Mpc)\n\n    # Test with custom H0 value\n    custom_H0 = 70 * (u.km / u.s / u.Mpc)\n    q_custom = 1 * u.Mpc * u.littleh\n    assert_quantity_allclose(q_custom.to(u.Mpc, u.with_H0(custom_H0)), custom_H0.value / 100 * u.Mpc)\n", "def test_parallax_converter():\n    # Test parallax converter with positive value\n    distance = 1 / (1 * u.arcsecond)\n    assert_quantity_allclose(distance, 3.26156377721 * u.parsec)\n\n    # Test parallax converter with negative value\n    with pytest.raises(u.UnitsError):\n        distance = 1 / (-1 * u.arcsecond)\n", "def test_with_H0_equivalency():\n    H0 = 70 * u.km / u.s / u.Mpc\n    equiv = u.with_H0(H0)\n    distance = 1 * u.Mpc / u.littleh\n    assert_quantity_allclose(distance.to(u.Mpc, equivalencies=equiv), 1 * u.Mpc / (70 / 100))\n", "def test_spectral_density_with_factor_argument():\n    # Test the spectral_density equivalency with the deprecated syntax\n    # (where the factor was an argument)\n    wav = 5500 * u.AA\n    f_nu_val = 1e-12 * u.erg / u.cm**2 / u.s / u.Hz\n    f_la_val = f_nu_val * wav.to(u.Hz, u.spectral())\n\n    with warnings.catch_warnings():\n        warnings.simplefilter('ignore', DeprecationWarning)\n        equiv = u.spectral_density(factor=wav)\n\n    assert_quantity_allclose(f_nu_val, f_la_val.to(u.erg / u.cm**2 / u.s / u.Hz, equiv))\n", "def test_spectral_density():\n    wav = 5500 * u.AA\n    f_la = u.erg / u.s / u.cm**2 / u.AA\n    f_nu = u.erg / u.s / u.cm**2 / u.Hz\n    phot_f_la = u.photon / u.s / u.cm**2 / u.AA\n    phot_f_nu = u.photon / u.s / u.cm**2 / u.Hz\n\n    conv = u.spectral_density(wav)\n\n    # Test conversion between f_la and f_nu\n    assert_quantity_allclose(\n        u.Quantity(1, f_la).to(f_nu, equivalencies=conv),\n        3.343545001789156e-05 * u.erg / u.s / u.cm**2 / u.Hz,\n        rtol=1e-5\n    )\n\n    # Test conversion between phot_f_la and phot_f_nu\n    assert_quantity_allclose(\n        u.Quantity(1, phot_f_la).to(phot_f_nu, equivalencies=conv),\n        3.343545001789156e-05 * u.photon / u.s / u.cm**2 / u.Hz,\n        rtol=1e-5\n    )\n"], "sample_210": ["    def test_get_redirect_url_with_url(self):\n        view = RedirectView.as_view(url='/redirect-url/')\n        request = self.rf.get('/some-url/')\n        response = view(request)\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response.url, '/redirect-url/')\n", "    def test_dispatch_is_decorated(self):\n        view = DecoratedDispatchView.as_view()\n        request = self.client.get('/')\n        response = view(request)\n        self.assertTrue(hasattr(view, 'is_decorated'))\n", "    def test_get_redirect(self):\n        request = self.rf.get('/')\n        redirect_view = RedirectView.as_view(url='/redirect-url/')\n        response = redirect_view(request)\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response.url, '/redirect-url/')\n", "    def test_redirect_view_permanent(self):\n        view = RedirectView.as_view(url='/about/', permanent=True)\n        request = self.rf.get('/redirect/')\n        response = view(request)\n        self.assertEqual(response.status_code, 301)\n", "    def test_decorated_view(self):\n        view = DecoratedDispatchView.as_view()\n        self.assertTrue(hasattr(view, 'is_decorated'))\n        self.assertTrue(view.is_decorated)\n        request = self.rf.get('/')\n        response = view(request)\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.content, b'This is a simple view')\n"], "sample_800": ["    def test_check_outlier_corruption(self):\n        # Test with same number of outliers\n        num_outliers = 5\n        expected_outliers = 5\n        decision = np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\n        check_outlier_corruption(num_outliers, expected_outliers, decision)\n\n        # Test with less number of outliers\n        num_outliers = 3\n        expected_outliers = 5\n        decision = np.array([0.1, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.8])\n        check_outlier_corruption(num_outliers, expected_outliers, decision)\n\n        # Test with more number of outliers, should raise AssertionError\n        num_outliers = 7\n        expected_outliers = 5\n        decision = np.array([0.1, 0.1, 0.2, 0.3, 0.4, 0.5, 0.5, 0.6, 0.7, 0.8])\n        with self.assertRaises(AssertionError):\n            check_outlier_corruption(num_outliers, expected_outliers, decision)\n", "    def test_check_outlier_corruption(self):\n        # Test when num_outliers is less than expected_outliers\n        num_outliers = 25\n        expected_outliers = 30\n        decision = np.random.rand(100)\n        decision[np.argsort(decision)[25:30]] = np.mean(decision[np.argsort(decision)[25:30]])\n        self.assertRaises(AssertionError, check_outlier_corruption, num_outliers, expected_outliers, decision)\n\n        # Test when num_outliers is greater than expected_outliers\n        num_outliers = 35\n        expected_outliers = 30\n        decision = np.random.rand(100)\n        decision[np.argsort(decision)[30:35]] = np.mean(decision[np.argsort(decision)[30:35]])\n        self.assertRaises(AssertionError, check_outlier_corruption, num_outliers, expected_outliers, decision)\n\n        # Test when num_outliers is equal to expected_outliers\n        num_outliers = 30\n        expected_outliers = 30\n        decision = np.random.rand(100)\n        decision[np.argsort(decision)[25:30]] = np.mean(decision[np.argsort(decision)[25:30]])\n        check_outlier_corruption(num_outliers, expected_outliers, decision)\n", "def test_correct_not_fitted_error():\n    estimator = CorrectNotFittedErrorClassifier()\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([0, 1])\n    assert_raises(CorrectNotFittedError, estimator.predict, X)\n    estimator.fit(X, y)\n    estimator.predict(X)  # should not raise an error\n", "    def fit(self, X, y=None):\n        X = check_array(X)\n        return self\n", "    def test_check_outlier_corruption(self):\n        # Test the check_outlier_corruption function\n        # Numbers of outliers equal to the expected outliers\n        num_outliers = 10\n        expected_outliers = 10\n        decision = np.random.rand(20)\n        check_outlier_corruption(num_outliers, expected_outliers, decision)\n\n        # Numbers of outliers less than the expected outliers, but within the tolerance\n        num_outliers = 9\n        expected_outliers = 10\n        decision = np.hstack([np.random.rand(9), np.full(1, np.nan)])\n        check_outlier_corruption(num_outliers, expected_outliers, decision)\n\n        # Numbers of outliers more than the expected outliers\n        num_outliers = 12\n        expected_outliers = 10\n        decision = np.random.rand(20)\n        with self.assertRaises(AssertionError):\n            check_outlier_corruption(num_outliers, expected_outliers, decision)\n\n        # Numbers of outliers less than the expected outliers, outside the tolerance\n        num_outliers = 8\n        expected_outliers = 10\n        decision = np.random.rand(20)\n        with self.assertRaises(AssertionError):\n            check_outlier_corruption(num_outliers, expected_outliers, decision)\n"], "sample_652": ["def test_idmaker():\n    argnames = [\"arg1\", \"arg2\"]\n    parametersets = [\n        fixtures.ParameterSet(values=[\"val1\", \"val2\"], marks=[], id=None),\n        fixtures.ParameterSet(values=[\"val3\", \"val4\"], marks=[], id=None),\n    ]\n    result = idmaker(argnames, parametersets)\n    assert result == [\"val1-val2\", \"val3-val4\"]\n", "def test_show_fixtures_per_test(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture\n            '''Fixture 1 documentation'''\n            return 1\n\n        @pytest.fixture\n            '''Fixture 2 documentation'''\n            return 2\n\n            assert fixture1 == 1\n            assert fixture2 == 2\n        \"\"\"\n    )\n    result = testdir.runpytest(\"--show-fixtures-per-test\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*fixtures used by test_func*\",\n            \"fixture1\",\n            \"    Fixture 1 documentation\",\n            \"fixture2\",\n            \"    Fixture 2 documentation\",\n        ]\n    )\n", "def test_show_fixtures_per_test(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture\n            '''my_fixture docstring'''\n            return 42\n\n            assert my_fixture == 42\n        \"\"\"\n    )\n    result = testdir.runpytest('--show-fixtures-per-test')\n    result.stdout.fnmatch_lines(\n        [\n            \"fixtures used by test_function\",\n            \"*test_function.py*\",\n            \"my_fixture\",\n            \"    my_fixture docstring\",\n        ]\n    )\n", "def test_showfixtures_function_starts_with_underscore(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture\n            '''Private fixture docstring'''\n            pass\n\n            pass\n        \"\"\"\n    )\n\n    result = testdir.runpytest(\"--fixtures\", \"-v\")\n\n    # Assert that the private fixture is not shown in the output\n    assert \"_private_fixture\" not in result.stdout.str()\n\n    # Rerun with verbose flag and assert that the private fixture is shown\n    result = testdir.runpytest(\"--fixtures\", \"-vv\")\n    assert \"_private_fixture\" in result.stdout.str()\n", "def test_parametrize_with_invalid_ids(self):\n        pass\n\n    class TestClass:\n        @pytest.mark.parametrize(\"arg\", [1, 2], ids=[None, \"id2\", 3])\n            func(arg)\n\n    metafunc = Metafunc(\n        definition=FunctionDefinition(\"test_method\", TestClass),\n        fixtureinfo=fixtures.FixtureInfo.argnames(func, names=[\"arg\"]),\n        config=self.config,\n        cls=TestClass,\n    )\n\n    with pytest.raises(ValueError) as excinfo:\n        metafunc.parametrize(\"arg\", [1, 2], ids=[None, \"id2\", 3])\n\n    assert \"In test_method: ids must be list of strings, found: 3 (type: <class 'int'>)\" in str(excinfo.value)\n"], "sample_862": ["def test_hashing_vectorizer_alternate_sign():\n    # Test alternate_sign parameter of HashingVectorizer\n    vec = HashingVectorizer(n_features=2**3, alternate_sign=False)\n    X1 = vec.transform(ALL_FOOD_DOCS)\n\n    vec = HashingVectorizer(n_features=2**3, alternate_sign=True)\n    X2 = vec.transform(ALL_FOOD_DOCS)\n\n    assert not np.allclose(X1.toarray(), X2.toarray())\n", "def test_hashing_vectorizer_input_validation():\n    vectorizer = HashingVectorizer()\n\n    # Test with string input\n    with assert_raise_message(ValueError, \"Iterable over raw text documents expected, string object received.\"):\n        vectorizer.fit_transform(\"This is a single string input\")\n\n    # Test with list of integers\n    with assert_raise_message(TypeError, \"documents must be str or bytes, got <class 'int'>\"):\n        vectorizer.fit_transform([1, 2, 3])\n", "def test_tfidf_transformer_fit_transform_with_sublinear_tf():\n    X = np.array([[1, 2, 3],\n                  [1, 2, 0],\n                  [0, 1, 2]])\n    transformer = TfidfTransformer(sublinear_tf=True)\n    X_transformed = transformer.fit_transform(X)\n    X_expected = np.array([[0.2750, 0.6250, 0.7813],\n                           [0.2750, 0.6250, 0.0000],\n                           [0.0000, 0.3536, 0.8472]])\n    assert_array_almost_equal(X_transformed.toarray(), X_expected, decimal=4)\n", "def test_hashing_vectorizer_input():\n    # Test HashingVectorizer with different input types\n    vectorizer = HashingVectorizer()\n\n    # Test with list of strings\n    X_list = [\"This is a test document.\", \"Another test document.\"]\n    X_transformed = vectorizer.fit_transform(X_list)\n    assert X_transformed.shape == (2, 2 ** 20)  # 2 samples, 2**20 features\n\n    # Test with generator of strings\n    X_gen = (doc for doc in X_list)\n    X_transformed = vectorizer.fit_transform(X_gen)\n    assert X_transformed.shape == (2, 2 ** 20)  # 2 samples, 2**20 features\n\n    # Test with numpy array of strings\n    X_array = np.array(X_list)\n    X_transformed = vectorizer.fit_transform(X_array)\n    assert X_transformed.shape == (2, 2 ** 20)  # 2 samples, 2**20 features\n\n    # Test with invalid input type\n    with pytest.raises(ValueError) as excinfo:\n        X_invalid = \"This is an invalid input type.\"\n        vectorizer.fit_transform(X_invalid)\n    assert \"Iterable over raw text documents expected, string object received.\" in str(excinfo.value)\n", "def test_tfidf_vectorizer_custom_analyzer():\n    vectorizer = TfidfVectorizer(analyzer=split_tokenize)\n    X = vectorizer.fit_transform(JUNK_FOOD_DOCS)\n    assert X.shape == (6, 5)\n"], "sample_729": ["def test_elastic_net_path_multi_output():\n    # Test elastic_net_path with multi-output target\n    X = np.array([[1, 2, 3.1], [2.3, 5.4, 4.3]]).T\n    y = np.array([[1, 2], [3.1, 4.2]])\n    alphas, _, _ = enet_path(X, y, l1_ratio=0.5, eps=1e-3, n_alphas=100)\n    assert alphas.shape == (100,)\n    assert_array_equal(alphas, enet_path(X, y, l1_ratio=0.5, eps=1e-3, alphas=alphas)[0])\n", "def test_enet_path_multi_output():\n    # Test enet_path with multi-output target\n    X = np.array([[1, 2, 3.1], [2.3, 5.4, 4.3]]).T\n    y = np.array([[1, 2, 3.1], [4, 5, 6]]).T\n\n    alphas, coefs, dual_gaps = enet_path(X, y, l1_ratio=0.5, eps=1e-3, n_alphas=100)\n\n    assert_equal(coefs.shape, (2, 3, 100))\n    assert_equal(dual_gaps.shape, (100,))\n    assert_true(np.all(dual_gaps >= 0))\n", "def test_elastic_net_cv_multitask_output():\n    # Test ElasticNetCV with multi-task output\n    X, y = load_boston(return_X_y=True)\n    y = np.c_[y, y]  # Create multi-task output\n    model = ElasticNetCV(cv=3, random_state=0)\n    model.fit(X, y)\n    assert_equal(model.coef_.shape, (2, X.shape[1]))\n    assert_greater(model.alpha_, 0)\n    assert_greater(model.mse_path_.shape[0], 0)\n", "def test_enet_path_with_intercept():\n    X, y = load_boston(return_X_y=True)\n    alphas, coefs, dual_gaps = enet_path(X, y, l1_ratio=0.5, fit_intercept=True)\n    assert_equal(coefs.shape, (X.shape[1] + 1, len(alphas)))\n", "def test_enet_path_with_intercept():\n    rng = np.random.RandomState(0)\n    X = rng.randn(10, 5)\n    y = rng.randn(10)\n    alphas = np.logspace(-2, 2, 5)\n\n    _, coefs_no_intercept, _ = enet_path(X, y, alphas=alphas, fit_intercept=False)\n    _, coefs_with_intercept, _ = enet_path(X, y, alphas=alphas, fit_intercept=True)\n\n    assert_greater(np.abs(coefs_with_intercept[:, 0]).max(),\n                   np.abs(coefs_no_intercept[:, 0]).max())\n"], "sample_516": ["def test_pdf_no_use14corefonts():\n    rcParams['pdf.use14corefonts'] = False\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3], [1, 2, 3])\n", "def test_pdf_page_count():\n    with PdfPages(NamedTemporaryFile(suffix='.pdf').name) as pdf:\n        plt.figure()\n        pdf.savefig()\n        plt.close()\n        assert pdf.get_pagecount() == 1\n\n    with PdfPages(NamedTemporaryFile(suffix='.pdf').name) as pdf:\n        for _ in range(3):\n            plt.figure()\n            pdf.savefig()\n            plt.close()\n        assert pdf.get_pagecount() == 3\n", "def test_pdf_custom_font():\n    font_path = findfont(FontProperties(family='monospace'))\n    plt.figure()\n    plt.text(0.5, 0.5, 'Hello, World!', fontfamily='monospace', fontsize=20,\n             fontproperties=FontProperties(fname=font_path))\n", "def test_pdf_custom_font():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3], [4, 5, 6])\n    font = findfont(FontProperties(family='DejaVu Sans'))\n    ax.set_xlabel('X Label', fontproperties=FontProperties(fname=font))\n    ax.set_ylabel('Y Label', fontproperties=FontProperties(fname=font))\n    fig.savefig('pdf_custom_font.pdf', format='pdf')\n", "def test_pdf_use14corefonts_with_custom_font(self):\n    fig = plt.figure(figsize=(6, 4))\n    ax = fig.add_subplot(111)\n    ax.text(0.5, 0.5, \"Hello, world!\", fontname=\"Courier\", size=20)\n    with NamedTemporaryFile(suffix=\".pdf\") as f:\n        fig.savefig(f.name, format=\"pdf\")\n        with open(f.name, \"rb\") as fh:\n            data = fh.read()\n        # Check that the font is embedded in the PDF\n        assert b\"Courier\" in data\n    plt.close(fig)\n"], "sample_287": ["    def test_check_autocomplete_fields_item(self):\n        ma = ValidFields(Song, AdminSite())\n        errors = ma._check_autocomplete_fields_item(ma, 'title', 'autocomplete_fields[0]')\n        self.assertEqual(errors, [])\n\n        errors = ma._check_autocomplete_fields_item(ma, 'nonexistent_field', 'autocomplete_fields[0]')\n        self.assertEqual(len(errors), 1)\n        self.assertIsInstance(errors[0], checks.Error)\n        self.assertEqual(errors[0].id, 'admin.E037')\n\n        ma.autocomplete_fields = ['album']\n        errors = ma._check_autocomplete_fields_item(ma, 'album', 'autocomplete_fields[0]')\n        self.assertEqual(len(errors), 1)\n        self.assertIsInstance(errors[0], checks.Error)\n        self.assertEqual(errors[0].id, 'admin.E039')\n\n        admin.site.register(Album, admin.ModelAdmin)\n        errors = ma._check_autocomplete_fields_item(ma, 'album', 'autocomplete_fields[0]')\n        self.assertEqual(len(errors), 1)\n        self.assertIsInstance(errors[0], checks.Error)\n        self.assertEqual(errors[0].id, 'admin.E040')\n\n        admin.site.unregister(Album)\n", "    def test_check_readonly_fields(self):\n        class ReadOnlyFieldsTestAdmin(admin.ModelAdmin):\n            readonly_fields = ('title', 'non_existent_field')\n\n        site = AdminSite()\n        site.register(Song, ReadOnlyFieldsTestAdmin)\n\n        errors = site.check()\n        self.assertEqual(len(errors), 1)\n        self.assertEqual(errors[0].id, 'admin.E035')\n        self.assertIn(\"'non_existent_field'\", errors[0].msg)\n", "    def test_autocomplete_fields_validation(self):\n        site = AdminSite()\n        modeladmin = self.model_admin_cls(model=Song, admin_site=site)\n        errors = modeladmin.check()\n        self.assertEqual(errors, [])\n", "def test_check_admin_app_with_model_backend(self):\n    errors = checks.run_checks(tags=['admin'])\n    self.assertEqual(errors, [\n        checks.Error(\n            \"'django.contrib.auth.context_processors.auth' must be enabled in \"\n            \"DjangoTemplates (TEMPLATES) if using the default auth backend in \"\n            \"order to use the admin application.\",\n            id='admin.E402',\n        ),\n    ])\n", "    def test_check_fields_with_extra_form_fields(self):\n        site = AdminSite()\n        ma = ValidFormFieldsets(Song, site)\n        errors = ma.check()\n        # Check that no errors are raised for extra form fields in fieldsets\n        self.assertEqual(errors, [])\n"], "sample_880": ["def test_check_classification_targets_multiclass(y):\n    # Test that check_classification_targets does not raise an error for multiclass targets\n    check_classification_targets(y)\n", "def test_ovr_decision_function():\n    # Create sample data\n    predictions = np.array([[0, 1, 0], [1, 0, 1], [0, 1, 0]])\n    confidences = np.array([[0.9, 0.8, 0.7], [0.6, 0.7, 0.8], [0.7, 0.9, 0.6]])\n    n_classes = 3\n\n    # Call the function\n    result = _ovr_decision_function(predictions, confidences, n_classes)\n\n    # Expected result\n    expected_result = np.array([[-0.13333333, 1.23333333, -0.13333333],\n                                [0.13333333, -1.23333333, 0.13333333],\n                                [-0.13333333, 1.23333333, -0.13333333]])\n\n    # Check the result\n    assert_array_almost_equal(result, expected_result, decimal=6)\n", "def test_ovr_decision_function():\n    # Test with a simple example\n    predictions = np.array([[0, 1, 1], [1, 1, 0]])\n    confidences = np.array([[0.8, 0.7, 0.6], [0.9, 0.8, 0.7]])\n    n_classes = 3\n\n    expected_output = np.array([[-0.025, 0.04583333, 0.01916667],\n                               [0.05583333, 0.01916667, -0.025]])\n\n    output = _ovr_decision_function(predictions, confidences, n_classes)\n    assert_array_almost_equal(output, expected_output)\n", "def test_ovr_decision_function():\n    # Test the _ovr_decision_function utility function\n    predictions = np.array([[0, 1, 1], [0, 0, 1]])\n    confidences = np.array([[0.5, 0.6, 0.7], [0.4, 0.3, 0.9]])\n    n_classes = 3\n    expected_output = np.array([[1.5, -0.5, -0.16666667], [0.5, -0.66666667, 0.16666667]])\n    assert_array_almost_equal(_ovr_decision_function(predictions, confidences, n_classes), expected_output)\n", "def test_ovr_decision_function():\n    # Test case for _ovr_decision_function\n    predictions = np.array([[0, 1, 0], [1, 0, 0], [1, 1, 0]])\n    confidences = np.array([[0.8, 0.6, 0.9], [0.7, 0.9, 0.6], [0.85, 0.75, 0.9]])\n    n_classes = 3\n\n    result = _ovr_decision_function(predictions, confidences, n_classes)\n\n    expected_result = np.array([[0.26666667, 1.06666667, -0.33333333],\n                                [-0.06666667, 1.33333333, -0.26666667],\n                                [0.46666667, 1.13333333, -0.6]])\n\n    assert_array_almost_equal(result, expected_result, decimal=6)\n"], "sample_71": ["    def test_format_with_large_exponent(self):\n        number = Decimal('1.2345678901234567890123456789' + '0' * 180)\n        result = nformat(number, decimal_sep='.', decimal_pos=2, grouping=3)\n        self.assertEqual(result, '1.23e200')\n", "    def test_format_with_large_decimal(self):\n        number = Decimal('1234567890123456789012345678901234567890.1234567890')\n        result = nformat(number, decimal_sep='.', decimal_pos=10, grouping=3)\n        self.assertEqual(result, '1,234,567,890,123,456,789,012,345,678,90.1234567890')\n", "def test_scientific_notation(self):\n    number = Decimal('1' + '0' * 201)\n    result = nformat(number, decimal_sep='.', decimal_pos=None, grouping=0, thousand_sep='', force_grouping=False, use_l10n=None)\n    self.assertEqual(result, '1e201')\n", "    def test_format_with_large_decimal(self):\n        number = Decimal('1' + '0' * 210)\n        result = nformat(number, decimal_sep='.', decimal_pos=None, grouping=0, thousand_sep='')\n        self.assertEqual(result, '1e210')\n", "def test_format_decimal_with_large_exponent(self):\n    number = Decimal('1.23456789012345678901234567890E200')\n    formatted = nformat(number, decimal_sep='.', decimal_pos=10, grouping=3, thousand_sep=',', use_l10n=False)\n    self.assertEqual(formatted, '1.2345678901e200')\n"], "sample_562": ["def test_line2d_set_markevery_float():\n    fig, ax = plt.subplots()\n    x = np.linspace(0, 10, 100)\n    y = np.sin(x)\n    line, = ax.plot(x, y, marker='o', markevery=0.1)\n    assert len(line.get_markevery()) == 2\n    assert isinstance(line.get_markevery()[0], float)\n    assert isinstance(line.get_markevery()[1], float)\n", "def test_lines_gapcolor():\n    fig, ax = plt.subplots()\n    x = np.linspace(0, 2*np.pi, 100)\n    y = np.sin(x)\n    ax.plot(x, y, linestyle='--', gapcolor='grey')\n", "def test_line2d_set_antialiased():\n    line = mlines.Line2D([0, 1], [0, 1])\n    assert line.get_antialiased() == mpl.rcParams['lines.antialiased']\n\n    line.set_antialiased(True)\n    assert line.get_antialiased() is True\n    line.set_antialiased(False)\n    assert line.get_antialiased() is False\n", "def test_set_linestyle_with_custom_dash_pattern():\n    line = mlines.Line2D([0, 1], [0, 1])\n    custom_dash_pattern = (5, (10, 5))\n    line.set_linestyle(custom_dash_pattern)\n    assert line.get_linestyle() == custom_dash_pattern\n    assert line.is_dashed()\n", "def test_line2d_set_linestyle_custom_dash_pattern():\n    fig, ax = plt.subplots()\n    line = mlines.Line2D([0, 1], [0, 1], linestyle=(5, (10, 5)), color='black')\n    ax.add_line(line)\n    assert line.get_linestyle() == (5, (10, 5))\n    assert line.get_dashes() == (5, (10, 5))\n\n    line.set_linestyle('--')\n    assert line.get_linestyle() == '--'\n    assert line.get_dashes() == (None, None)\n"], "sample_180": ["    def test_unique_together_with_invalid_fields(self):\n        class InvalidUniqueTogetherModel(models.Model):\n            field1 = models.CharField(max_length=10)\n            field2 = models.CharField(max_length=10)\n\n            class Meta:\n                unique_together = (('field1', 'nonexistent_field'),)\n\n        errors = InvalidUniqueTogetherModel.check()\n        self.assertEqual(\n            errors,\n            [Error(\n                \"'unique_together' refers to the nonexistent field 'nonexistent_field'.\",\n                obj=InvalidUniqueTogetherModel,\n                id='models.E012',\n            )]\n        )\n", "    def test_check_unique_together(self):\n        class InvalidModel(models.Model):\n            field1 = models.CharField(max_length=255)\n            field2 = models.CharField(max_length=255)\n\n            class Meta:\n                unique_together = 'invalid_field'\n\n        errors = InvalidModel.check()\n        self.assertEqual(len(errors), 1)\n        self.assertEqual(errors[0], Error(\n            \"'unique_together' elements must be lists or tuples.\",\n            obj=InvalidModel,\n            id='models.E011',\n        ))\n", "    def test_partial_index_with_condition(self):\n        class PartialIndexModel(models.Model):\n            name = models.CharField(max_length=10)\n            value = models.IntegerField()\n\n            class Meta:\n                index_together = (('name', 'value'), ('name',), ('value', 'name'))\n                indexes = [\n                    models.Index(fields=['name'], condition=models.Q(value__gt=0)),\n                ]\n\n        errors = PartialIndexModel.check()\n        self.assertEqual(len(errors), 1)\n        self.assertIsInstance(errors[0], Warning)\n        self.assertEqual(errors[0].id, 'models.W037')\n", "    def test_unique_together_local_fields(self):\n        class ValidModel(models.Model):\n            field1 = models.CharField(max_length=10)\n            field2 = models.IntegerField()\n\n            class Meta:\n                unique_together = (('field1', 'field2'),)\n\n        self.assertEqual(ValidModel.check(), [])\n", "    def test_check_m2m_through_same_relationship(self):\n        class Author(models.Model):\n            name = models.CharField(max_length=50)\n\n        class Book(models.Model):\n            title = models.CharField(max_length=50)\n            authors = models.ManyToManyField(Author, through='AuthorBook')\n\n        class AuthorBook(models.Model):\n            author = models.ForeignKey(Author, on_delete=models.CASCADE)\n            book = models.ForeignKey(Book, on_delete=models.CASCADE)\n\n        class AnotherBook(models.Model):\n            title = models.CharField(max_length=50)\n            authors = models.ManyToManyField(Author, through='AuthorBook')\n\n        self.assertEqual(Book._check_m2m_through_same_relationship(), [\n            Error(\n                \"The model has two identical many-to-many relations through the intermediate model 'invalid_models_tests.authorbook'.\",\n                obj=AnotherBook,\n                id='models.E003',\n            ),\n        ])\n"], "sample_1209": ["def test_prefix_multiplication():\n    assert kilo * meter == 1000 * meter\n    assert kilo * kibi == 1024 * kibi\n    assert kilo / kibi == 1000 / 1024\n    assert kibi / kilo == 1024 / 1000\n", "def test_prefix_multiplication():\n    assert kilo * meter == 1000 * meter\n    assert kilo * kibi == 1024 * kibi\n    assert kilo / kibi == Rational(1024, 1000)\n", "def test_prefix_multiplication():\n    assert kilo * meter == 1000 * meter\n    assert kilo * kibi == 2**13\n    assert kilo / kibi == 2**-3\n", "def test_prefix_multiplication():\n    # Test multiplication of two prefixes\n    assert kilo * mega == mega * kilo\n    assert (kilo * mega).scale_factor == 1e6\n\n    # Test multiplication of prefix and quantity\n    quantity = Quantity('quantity', abbrev='q')\n    assert (kilo * quantity).scale_factor == 1e3\n\n    # Test multiplication of prefix and non-unit\n    assert (kilo * 2).scale_factor == 2e3\n\n    # Test multiplication with binary prefix\n    assert (kibi * 2).scale_factor == 2048\n", "def test_prefix_mul():\n    assert kilo * meter == Quantity('kilometer', abbrev='km', scale_factor=1000)\n    assert kilo * kibi == Quantity('kibikilo', abbrev='Kik', scale_factor=1024000)\n"], "sample_1130": ["def test_point_position_from():\n    N = ReferenceFrame('N')\n    p1 = Point('p1')\n    p2 = Point('p2')\n    p1.set_pos(p2, 10 * N.x)\n    assert p1.pos_from(p2) == -10 * N.x\n", "def test_set_vel_and_acc():\n    N = ReferenceFrame('N')\n    p1 = Point('p1')\n    p1.set_vel(N, 10 * N.x)\n    assert p1.vel(N) == 10 * N.x\n\n    p1.set_acc(N, 5 * N.y)\n    assert p1.acc(N) == 5 * N.y\n", "def test_point_velocity_calculation_with_cycle():\n    N = ReferenceFrame('N')\n    p1 = Point('p1')\n    p2 = Point('p2')\n    p3 = Point('p3')\n    p1.set_pos(p2, 10 * N.x)\n    p2.set_pos(p3, 20 * N.x)\n    p3.set_pos(p1, 30 * N.x)\n\n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter(\"always\")\n        p1.vel(N)\n        assert len(w) == 1\n        assert issubclass(w[-1].category, UserWarning)\n        assert \"Kinematic loops are defined among the positions of points.\" in str(w[-1].message)\n", "def test_set_vel():\n    N = ReferenceFrame('N')\n    p1 = Point('p1')\n    p1.set_vel(N, 10 * N.x)\n    assert p1.vel(N) == 10 * N.x\n\n    q = dynamicsymbols('q')\n    p2 = Point('p2')\n    p2.set_vel(N, q * N.x)\n    assert p2.vel(N) == q * N.x\n\n    with raises(TypeError):\n        p1.set_vel(N, 'invalid')\n", "def test_position_from():\n    N = ReferenceFrame('N')\n    p1 = Point('p1')\n    p2 = Point('p2')\n    p3 = Point('p3')\n    p1.set_pos(p2, 10 * N.x)\n    p2.set_pos(p3, 20 * N.x)\n    assert p1.pos_from(p3) == 30 * N.x\n"], "sample_494": ["def test_serialize_text_enum(self):\n    value = TextEnum.A\n    serializer = serializer_factory(value)\n    serialized, imports = serializer.serialize()\n    self.assertEqual(serialized, \"TestModel1.TextEnum['A']\")\n    self.assertEqual(imports, {\"import TestModel1\"})\n", "    def test_settings_reference_serializer(self):\n        value = SettingsReference(\"TIME_ZONE\")\n        serializer = serializer_factory(value)\n        self.assertEqual(serializer.serialize(), (\"settings.TIME_ZONE\", {\"from django.conf import settings\"}))\n", "    def test_settings_reference_serializer(self):\n        s = SettingsReference(\"LANGUAGE_CODE\", \"en-us\")\n        result, imports = serializer_factory(s).serialize()\n        self.assertEqual(result, \"settings.LANGUAGE_CODE\")\n        self.assertEqual(imports, {\"from django.conf import settings\"})\n", "    def test_float_serializer(self):\n        self.assertEqual(\n            serializer_factory(float(\"inf\")).serialize(),\n            ('float(\"inf\")', set())\n        )\n        self.assertEqual(\n            serializer_factory(float(\"nan\")).serialize(),\n            ('float(\"nan\")', set())\n        )\n        self.assertEqual(\n            serializer_factory(1.23).serialize(),\n            ('1.23', set())\n        )\n", "    def test_serializer_factory(self):\n        test_cases = [\n            (datetime.time(12, 0), \"datetime.time(12, 0)\", {\"import datetime\"}),\n            (datetime.timedelta(days=1), \"datetime.timedelta(1)\", {\"import datetime\"}),\n            (frozenset([1, 2, 3]), \"frozenset([1, 2, 3])\", set()),\n            ([1, 2, 3], \"[1, 2, 3]\", set()),\n            ({1, 2, 3}, \"{1, 2, 3}\", set()),\n            ((1, 2, 3), \"(1, 2, 3)\", set()),\n            ({'a': 1, 'b': 2}, \"{'a': 1, 'b': 2}\", set()),\n            (models.CharField(max_length=255), \"models.CharField(max_length=255)\", {\"from django.db import models\"}),\n            (TextEnum.A, \"TestModel.TextEnum['A']\", {\"import TestModel\"}),\n            (TextTranslatedEnum.A, \"TestModel.TextTranslatedEnum['A']\", {\"import TestModel\"}),\n            (BinaryEnum.A, \"TestModel.BinaryEnum['A']\", {\"import TestModel\"}),\n            (IntEnum.A, \"TestModel.IntEnum[1]\", {\"import TestModel\"}),\n            (IntFlagEnum.A, \"TestModel.IntFlagEnum[1]\", {\"import TestModel\"}),\n            (float(\"nan\"), \"float('nan')\", set()),\n            (float(\"inf\"), \"float('inf')\", set()),\n            (uuid.uuid4(), f\"uuid.UUID('{str(uuid.uuid4())}')\", {\"import uuid\"}),\n            (pathlib.PurePath('/some/path'), \"pathlib.PurePosixPath('/some/path')\", {\"import pathlib\"}),\n            (os.PathLike(), \"''\", {}),\n            (re.compile('test'), \"re.compile('test', re.UNICODE)\","], "sample_116": ["    def test_make_template_fragment_key(self):\n        fragment_name = 'test_fragment'\n        vary_on = ['arg1', 'arg2']\n        expected_key = 'template.cache.test_fragment.098f6bcd4621d373cade4e832627b4f6'\n        self.assertEqual(make_template_fragment_key(fragment_name, vary_on), expected_key)\n\n        vary_on = None\n        expected_key = 'template.cache.test_fragment.d41d8cd98f00b204e9800998ecf8427e'\n        self.assertEqual(make_template_fragment_key(fragment_name, vary_on), expected_key)\n", "    def test_make_template_fragment_key_with_vary_on(self):\n        fragment_name = 'test_fragment'\n        vary_on = ['arg1', 'arg2']\n        key = make_template_fragment_key(fragment_name, vary_on)\n        self.assertEqual(key, 'template.cache.test_fragment.1f40fc92da241694750979ee6cf582f2')\n", "    def test_make_template_fragment_key(self):\n        # Test with no vary_on\n        key = make_template_fragment_key('fragment_name')\n        self.assertEqual(key, 'template.cache.fragment_name.d41d8cd98f00b204e9800998ecf8427e')\n\n        # Test with vary_on\n        key = make_template_fragment_key('fragment_name', vary_on=['arg1', 'arg2'])\n        self.assertEqual(key, 'template.cache.fragment_name.3456f797fcb2239392424c315a9f9e54')\n", "    def test_make_template_fragment_key(self):\n        # Test with no vary_on\n        self.assertEqual(\n            make_template_fragment_key('test_fragment'),\n            'template.cache.test_fragment.d41d8cd98f00b204e9800998ecf8427e'\n        )\n\n        # Test with vary_on\n        self.assertEqual(\n            make_template_fragment_key('test_fragment', ['vary1', 'vary2']),\n            'template.cache.test_fragment.5552a6879a333943707395598f9f759d'\n        )\n\n        # Test with non-string vary_on\n        self.assertEqual(\n            make_template_fragment_key('test_fragment', [123, 456]),\n            'template.cache.test_fragment.e173116a7751971809275902d55d1346'\n        )\n", "    def test_make_template_fragment_key(self):\n        # Test make_template_fragment_key function with fragment_name only\n        key = make_template_fragment_key('fragment_name')\n        self.assertEqual(key, 'template.cache.fragment_name.d41d8cd98f00b204e9800998ecf8427e')\n\n        # Test make_template_fragment_key function with fragment_name and vary_on\n        key = make_template_fragment_key('fragment_name', vary_on=['arg1', 'arg2'])\n        self.assertEqual(key, 'template.cache.fragment_name.92b95d4528e825d740706c3e18d7b4f0')\n\n        # Test make_template_fragment_key function with fragment_name and complex vary_on\n        key = make_template_fragment_key('fragment_name', vary_on=[f, C, Unpicklable()])\n        # Since Unpicklable object raises PickleError, it should not be included in the hash\n        self.assertEqual(key, 'template.cache.fragment_name.1285764d2371741f7155a59b6b42095d')\n"], "sample_295": ["def test_col_repr(self):\n    col = Col(\"alias\", \"column\")\n    self.assertEqual(repr(col), \"Col(alias, column)\")\n", "def test_pickle_combined_expression(self):\n    expr = CombinedExpression(F('num_employees'), '+', F('num_chairs'))\n    pickled_expr = pickle.dumps(expr)\n    unpickled_expr = pickle.loads(pickled_expr)\n    self.assertEqual(str(expr), str(unpickled_expr))\n", "def test_combinable_operators(self):\n    # Test __neg__ operator\n    expr = F('num_employees')\n    neg_expr = -expr\n    self.assertIsInstance(neg_expr, CombinedExpression)\n    self.assertEqual(neg_expr.connector, Combinable.MUL)\n    self.assertEqual(neg_expr.rhs, -1)\n\n    # Test __add__ operator\n    expr1 = F('num_employees')\n    expr2 = F('num_chairs')\n    add_expr = expr1 + expr2\n    self.assertIsInstance(add_expr, CombinedExpression)\n    self.assertEqual(add_expr.connector, Combinable.ADD)\n    self.assertEqual(add_expr.lhs, expr1)\n    self.assertEqual(add_expr.rhs, expr2)\n\n    # Test __sub__ operator\n    sub_expr = expr1 - expr2\n    self.assertIsInstance(sub_expr, CombinedExpression)\n    self.assertEqual(sub_expr.connector, Combinable.SUB)\n    self.assertEqual(sub_expr.lhs, expr1)\n    self.assertEqual(sub_expr.rhs, expr2)\n\n    # Test __mul__ operator\n    mul_expr = expr1 * expr2\n    self.assertIsInstance(mul_expr, CombinedExpression)\n    self.assertEqual(mul_expr.connector, Combinable.MUL)\n    self.assertEqual(mul_expr.lhs, expr1)\n    self.assertEqual(mul_expr.rhs, expr2)\n\n    # Test __truediv__ operator\n    div_expr = expr1 / expr2\n    self.assertIsInstance(div_expr, CombinedExpression)\n    self.assertEqual(div_expr.connector, Combinable.DIV)\n    self.assertEqual(div_expr.lhs, expr1)\n    self.assertEqual(div_expr.rhs, expr2)\n\n    # Test __mod__ operator\n    mod_expr = expr1 % expr2\n    self.assertIsInstance(mod_expr, CombinedExpression)\n    self.assertEqual(mod_expr.connector, Combinable.", "def test_f_object_equality(self):\n    self.assertEqual(F('name'), F('name'))\n    self.assertNotEqual(F('name'), F('num_employees'))\n", "def test_expression_as_sql_with_numeric_field(self):\n    expr = Expression(F('num_employees'), output_field=DecimalField())\n    sql, params = expr.as_sql(self.company_query.query.get_compiler(connection.alias), connection)\n    self.assertEqual(sql, 'CAST(\"test_company\".\"num_employees\" AS NUMERIC)')\n    self.assertEqual(params, [])\n"], "sample_76": ["def test_check_setting_language_code(self):\n    with self.settings(LANGUAGE_CODE='en-US'):\n        self.assertEqual(check_setting_language_code(None), [])\n    with self.settings(LANGUAGE_CODE='invalid_code'):\n        self.assertEqual(check_setting_language_code(None), [Error('You have provided an invalid value for the LANGUAGE_CODE setting: invalid_code.', id='translation.E001')])\n", "    def test_setting_languages_bidi(self):\n        \"\"\"Test the check_setting_languages_bidi function.\"\"\"\n        with self.settings(LANGUAGES_BIDI=self.valid_tags):\n            errors = check_setting_languages_bidi(None)\n            self.assertEqual(errors, [])\n\n        with self.settings(LANGUAGES_BIDI=self.invalid_tags):\n            errors = check_setting_languages_bidi(None)\n            self.assertEqual(len(errors), len(self.invalid_tags))\n            for error in errors:\n                self.assertIsInstance(error, Error)\n", "    def test_consistent_language_settings(self):\n        with self.settings(LANGUAGES=(('en', 'English'), ('fr', 'French')), LANGUAGE_CODE='fr'):\n            self.assertEqual(check_language_settings_consistent(None), [])\n        with self.settings(LANGUAGES=(('en', 'English'), ('fr', 'French')), LANGUAGE_CODE='es'):\n            self.assertEqual(check_language_settings_consistent(None), [Error('You have provided a value for the LANGUAGE_CODE setting that is not in the LANGUAGES setting.', id='translation.E004')])\n", "    def test_language_settings_consistent(self):\n        \"\"\"Test that the language settings are consistent with each other.\"\"\"\n        with self.settings(LANGUAGES=self.valid_tags, LANGUAGE_CODE='en'):\n            self.assertEqual(check_language_settings_consistent(None), [])\n\n        with self.settings(LANGUAGES=self.valid_tags, LANGUAGE_CODE='de'):\n            self.assertEqual(check_language_settings_consistent(None), [Error('translation.E004')])\n", "    def test_check_language_settings_consistent(self):\n        \"\"\"Test the consistency check between LANGUAGE_CODE and LANGUAGES settings.\"\"\"\n        # Test with LANGUAGE_CODE not in LANGUAGES\n        with self.settings(LANGUAGE_CODE='en', LANGUAGES=[('fr', 'French'), ('de', 'German')]):\n            self.assertEqual(check_language_settings_consistent(None), [Error('translation.E004', msg='You have provided a value for the LANGUAGE_CODE setting that is not in the LANGUAGES setting.')])\n\n        # Test with LANGUAGE_CODE in LANGUAGES\n        with self.settings(LANGUAGE_CODE='en', LANGUAGES=[('en', 'English'), ('fr', 'French'), ('de', 'German')]):\n            self.assertEqual(check_language_settings_consistent(None), [])\n"], "sample_48": ["    def test_min_aggregate(self):\n        min_pages = Book.objects.aggregate(Min('pages'))\n        self.assertEqual(min_pages['pages__min'], 300)\n", "def test_count_with_filter(self):\n    # Test the Count aggregate function with a filter\n    count = Book.objects.filter(rating__gt=4.0).aggregate(book_count=Count('id', filter=F('price') < Decimal('30.00')))\n    self.assertEqual(count['book_count'], 1)\n", "def test_max_aggregate(self):\n    max_pages = Book.objects.aggregate(Max('pages'))['pages__max']\n    self.assertEqual(max_pages, 1132)\n\n    max_rating = Book.objects.aggregate(Max('rating'))['rating__max']\n    self.assertEqual(max_rating, 5.0)\n", "def test_count_filter(self):\n    # Test Count with filter\n    count = Author.objects.filter(age=29).aggregate(count=Count('id', filter=Q(name__startswith='J')))\n    self.assertEqual(count['count'], 2)  # James Bennett and Jeffrey Forcier\n", "def test_aggregate_with_filter(self):\n    authors_under_40 = Author.objects.filter(age__lt=40)\n    books_authors_under_40 = Book.objects.filter(authors__in=authors_under_40)\n    avg_rating_under_40 = books_authors_under_40.aggregate(Avg('rating'))['rating__avg']\n    self.assertAlmostEqual(avg_rating_under_40, 4.0)\n"], "sample_333": ["def test_has_changed(self):\n    data = {'first_name': 'John', 'last_name': 'Doe', 'birthday': '1990-01-01'}\n    form = Person(data, initial={'first_name': 'John', 'last_name': 'Smith', 'birthday': '1990-01-01'})\n    self.assertFalse(form.has_changed())\n\n    data = {'first_name': 'John', 'last_name': 'Doe', 'birthday': '1990-01-02'}\n    form = Person(data, initial={'first_name': 'John', 'last_name': 'Doe', 'birthday': '1990-01-01'})\n    self.assertTrue(form.has_changed())\n", "    def test_form_rendering(self):\n        data = {\n            'first_name': 'John',\n            'last_name': 'Doe',\n            'birthday': '1990-01-01',\n        }\n        form = Person(data)\n        self.assertTrue(form.is_valid())\n        self.assertEqual(str(form), form.as_table())\n        self.assertIn('<th><label for=\"id_first_name\">First name:</label></th>', str(form))\n        self.assertIn('<td><input type=\"text\" name=\"first_name\" value=\"John\" required id=\"id_first_name\">', str(form))\n", "def test_has_changed(self):\n    form = Person(MultiValueDictLike({'first_name': 'John', 'last_name': 'Doe', 'birthday': '1990-01-01'}))\n    form.full_clean()\n    self.assertTrue(form.has_changed())\n\n    form = Person(MultiValueDictLike({'first_name': 'John', 'last_name': 'Doe', 'birthday': '1990-01-01'}), initial={'first_name': 'John', 'last_name': 'Doe', 'birthday': '1990-01-01'})\n    form.full_clean()\n    self.assertFalse(form.has_changed())\n\n    form = Person(MultiValueDictLike({'first_name': 'John', 'last_name': 'Doe', 'birthday': '1990-02-02'}), initial={'first_name': 'John', 'last_name': 'Doe', 'birthday': '1990-01-01'})\n    form.full_clean()\n    self.assertTrue(form.has_changed())\n", "def test_form_rendering(self):\n    # Test the rendering of a form\n    form = PersonNew(data={'first_name': 'John', 'last_name': 'Doe', 'birthday': '1990-01-01'})\n    rendered = str(form)\n    self.assertIn('<input type=\"text\" name=\"first_name\" value=\"John\" id=\"first_name_id\" />', rendered)\n    self.assertIn('<input type=\"text\" name=\"last_name\" value=\"Doe\" />', rendered)\n    self.assertIn('<input type=\"text\" name=\"birthday\" value=\"1990-01-01\" />', rendered)\n", "def test_form_add_error(self):\n    person_form = Person({'first_name': 'John', 'last_name': 'Doe', 'birthday': '1990-01-01'})\n    person_form.add_error('first_name', 'This is a test error.')\n    self.assertEqual(person_form.errors['first_name'], ['This is a test error.'])\n"], "sample_577": ["def test_init_data_frame():\n    data = pd.DataFrame({\"x\": [1, 2, 3], \"y\": [4, 5, 6]})\n    p = Plot(data)\n    assert_frame_equal(p._data.frame, data)\n", "def test_plot_layout(self):\n    p = Plot(self.data)\n    p = p.layout(size=(8, 6), engine=\"constrained\")\n    plotter = p._plot()\n    fig = plotter._figure\n    assert fig.get_size_inches() == (8, 6)\n    assert fig.get_layout_engine().__class__.__name__ == \"ConstrainedLayout\"\n", "    def test_plot_initialization(self):\n        p = Plot()\n        assert p._data is not None\n        assert p._layers == []\n        assert p._scales == {}\n        assert p._shares == {}\n        assert p._limits == {}\n        assert p._labels == {}\n        assert p._theme == {}\n        assert p._facet_spec == {}\n        assert p._pair_spec == {}\n        assert p._figure_spec == {}\n        assert p._subplot_spec == {}\n        assert p._layout_spec == {}\n        assert p._target is None\n", "def test_scale_with_none():\n    p = Plot(x=[1, 2, 3], y=[4, 5, 6])\n    p = p.scale(x=None)\n    assert p._scales[\"x\"] is None\n", "def test_plot_limit(tips):\n\n    p = Plot(tips, x=\"time\", y=\"total_bill\").add(Dot()).limit(x=(1, 3))\n    assert p._limits == {\"x\": (1, 3)}\n\n    plotter = p.plot()\n    assert plotter._figure.axes[0].get_xlim() == (1, 3)\n"], "sample_565": ["def test_zoomed_inset_axes():\n    fig, ax = plt.subplots()\n    ax.imshow(np.arange(100).reshape((10, 10)))\n\n    axins = zoomed_inset_axes(ax, 2.5, loc='lower left')\n    axins.imshow(np.arange(100).reshape((10, 10)))\n\n    mark_inset(ax, axins, loc1=3, loc2=4, fc=\"none\", ec=\"0.5\")\n", "def test_inset_locator_manual_position():\n    fig, ax = plt.subplots()\n    ax.plot(np.arange(10))\n\n    ax_ins = plt.axes([0, 0, 1, 1])\n    ip = InsetPosition(ax, [0.5, 0.1, 0.4, 0.2])\n    ax_ins.set_axes_locator(ip)\n\n    ax_ins.plot(np.arange(5))\n", "def test_anchored_size_locator():\n    fig, ax = plt.subplots()\n    aloc = AnchoredSizeLocator(bbox_to_anchor=(0.5, 0.5), x_size='50%', y_size='30%', loc='center')\n    bbox = aloc(ax, fig.canvas.get_renderer())\n    assert_array_almost_equal(bbox.bounds, [0.25, 0.40, 0.5, 0.3])\n", "def test_inset_axes_basic():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3], [4, 5, 6])\n    axins = inset_axes(ax, width=\"30%\", height=\"30%\", loc='upper right')\n    axins.plot([1, 2, 3], [6, 5, 4])\n    plt.close(fig)\n", "def test_inset_locator_2():\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.plot([1, 2, 3, 4], [1, 2, 3, 4])\n\n    axins = inset_axes(ax, width=\"30%\", height=\"30%\", loc='upper right')\n    axins.plot([1, 2, 3, 4], [1, 2, 3, 4])\n    axins.set_xlim(2, 3)\n    axins.set_ylim(2, 3)\n\n    mark_inset(ax, axins, loc1=1, loc2=3, fc=\"none\", ec=\"0.5\")\n"], "sample_1083": ["def test_acsch_diff():\n    x = symbols('x')\n    assert acsch(x).diff(x) == -1/(x**2*sqrt(1 + 1/x**2))\n", "def test_acsch_fdiff():\n    x = symbols('x')\n    assert acsch(x).fdiff() == -1/(x**2*sqrt(1 + 1/x**2))\n    assert acsch(x).fdiff(2) == ArgumentIndexError\n", "def test_atanh_rewrite_as_log():\n    x = symbols('x')\n    assert atanh(x).rewrite(log) == (log(1 + x) - log(1 - x)) / 2\n", "def test_acsch_inverse():\n    x = Symbol('x')\n    assert acsch(csch(x)).doit() == x\n", "def test_asech_as_leading_term():\n    x = symbols('x')\n    assert asech(1 + x)._eval_as_leading_term(x) == 0\n    assert asech(2*x)._eval_as_leading_term(x) == 1/(2*x)\n"], "sample_662": ["def test_collect_report_serialization(tmp_path):\n    nodeid = \"node1\"\n    outcome = \"passed\"\n    longrepr = None\n    result = []\n    sections = []\n    fspath = tmp_path / \"test.py\"\n    fspath.touch()\n    report = CollectReport(nodeid, outcome, longrepr, result, sections)\n    data = report._to_json()\n    new_report = CollectReport._from_json(data)\n    assert new_report.nodeid == nodeid\n    assert new_report.outcome == outcome\n    assert new_report.longrepr == longrepr\n    assert new_report.result == result\n    assert new_report.sections == sections\n    assert new_report.fspath == str(fspath)\n", "def test_test_report_serialization(tmp_path):\n    # Test serialization and deserialization of TestReport\n    report = TestReport(\n        nodeid=\"test_node\",\n        location=(Path(tmp_path / \"test.py\"), 10, \"test_domain\"),\n        keywords={\"keyword1\": 1},\n        outcome=\"passed\",\n        longrepr=None,\n        when=\"call\",\n        sections=[],\n        duration=0.1,\n        user_properties=[(\"prop1\", \"value1\")],\n    )\n    serialized = report._to_json()\n    deserialized = TestReport._from_json(serialized)\n    assert report.nodeid == deserialized.nodeid\n    assert report.location == deserialized.location\n    assert report.keywords == deserialized.keywords\n    assert report.outcome == deserialized.outcome\n    assert report.longrepr == deserialized.longrepr\n    assert report.when == deserialized.when\n    assert report.sections == deserialized.sections\n    assert report.duration == deserialized.duration\n    assert report.user_properties == deserialized.user_properties\n", "def test_collect_report_serialization(self):\n    nodeid = \"nodeid_example\"\n    outcome = \"passed\"\n    longrepr = \"example longrepr\"\n    result = [\"result1\", \"result2\"]\n    sections = [(\"section1\", \"content1\"), (\"section2\", \"content2\")]\n    report = CollectReport(nodeid, outcome, longrepr, result, sections)\n    serialized = report._to_json()\n    deserialized = CollectReport._from_json(serialized)\n    assert report.nodeid == deserialized.nodeid\n    assert report.outcome == deserialized.outcome\n    assert report.longrepr == deserialized.longrepr\n    assert report.result == deserialized.result\n    assert report.sections == deserialized.sections\n", "def test_collect_report_serialization(self):\n    nodeid = \"node1\"\n    outcome = \"passed\"\n    longrepr = \"Long representation\"\n    result = [None]\n    sections = [(\"section1\", \"content1\")]\n    collect_report = CollectReport(nodeid, outcome, longrepr, result, sections)\n    serialized = collect_report._to_json()\n    deserialized = CollectReport._from_json(serialized)\n    assert deserialized.nodeid == nodeid\n    assert deserialized.outcome == outcome\n    assert deserialized.longrepr == longrepr\n    assert deserialized.result == result\n    assert deserialized.sections == sections\n", "def test_report_from_json(tmp_path):\n    # Create a TestReport object\n    report = TestReport(\n        nodeid=\"test_node\",\n        location=(str(tmp_path / \"test_file.py\"), 10, \"test_domain\"),\n        keywords={\"keyword1\": 1},\n        outcome=\"passed\",\n        longrepr=None,\n        when=\"call\",\n        sections=[(\"Captured stdout\", \"test output\")],\n        duration=1.5,\n        user_properties=[(\"user_key\", \"user_value\")],\n    )\n\n    # Serialize the TestReport object to JSON\n    report_dict = report._to_json()\n\n    # Deserialize the JSON data back to a TestReport object\n    deserialized_report = TestReport._from_json(report_dict)\n\n    # Assert that the deserialized object has the same attributes as the original object\n    assert deserialized_report.nodeid == report.nodeid\n    assert deserialized_report.location == report.location\n    assert deserialized_report.keywords == report.keywords\n    assert deserialized_report.outcome == report.outcome\n    assert deserialized_report.longrepr == report.longrepr\n    assert deserialized_report.when == report.when\n    assert deserialized_report.sections == report.sections\n    assert deserialized_report.duration == report.duration\n    assert deserialized_report.user_properties == report.user_properties\n"], "sample_410": ["def test_abstract_base_user_methods(self):\n    UserModel = get_user_model()\n    user = UserModel.objects.create_user(username='testuser', password='testpassword')\n\n    # Test __str__ method\n    self.assertEqual(str(user), 'testuser')\n\n    # Test get_username method\n    self.assertEqual(user.get_username(), 'testuser')\n\n    # Test is_anonymous property\n    self.assertFalse(user.is_anonymous)\n\n    # Test is_authenticated property\n    self.assertTrue(user.is_authenticated)\n\n    # Test check_password method\n    self.assertTrue(user.check_password('testpassword'))\n    self.assertFalse(user.check_password('wrongpassword'))\n\n    # Test set_unusable_password method and has_usable_password method\n    user.set_unusable_password()\n    self.assertFalse(user.has_usable_password())\n", "    def setUp(self):\n        self.user = get_user_model().objects.create_user(username='testuser', password='testpassword')\n", "    def setUp(self):\n        self.user = get_user_model()(username='testuser')\n        self.user.set_password('testpassword')\n", "    def setUp(self):\n        self.user = get_user_model().objects.create_user(username='testuser', password='testpassword')\n", "def test_abstract_base_user_methods(self):\n    User = get_user_model()\n    user = User(username='testuser')\n    user.set_password('testpassword')\n    self.assertTrue(user.check_password('testpassword'))\n    self.assertFalse(user.check_password('wrongpassword'))\n\n    user.set_unusable_password()\n    self.assertFalse(user.has_usable_password())\n\n    user.set_password('testpassword')\n    session_auth_hash = user.get_session_auth_hash()\n    self.assertIsInstance(session_auth_hash, str)\n\n    self.assertFalse(user.is_anonymous)\n    self.assertTrue(user.is_authenticated)\n\n    email_field_name = User.get_email_field_name()\n    self.assertIsInstance(email_field_name, str)\n\n    normalized_username = User.normalize_username('testUser')\n    self.assertEqual(normalized_username, 'testUser')\n"], "sample_290": ["    def test_deconstructible_objects_with_changes(self):\n        before_states = [\n            self.author_name_deconstructible_1,\n            self.author_name_deconstructible_2,\n            self.author_name_deconstructible_3,\n            self.author_name_deconstructible_4,\n            self.author_name_deconstructible_list_1,\n            self.author_name_deconstructible_list_2,\n            self.author_name_deconstructible_tuple_1,\n            self.author_name_deconstructible_tuple_2,\n            self.author_name_deconstructible_dict_1,\n            self.author_name_deconstructible_dict_2,\n            self.author_name_nested_deconstructible_1,\n            self.author_name_nested_deconstructible_2,\n        ]\n        after_states = [\n            self.author_name_deconstructible_2,\n            self.author_name_deconstructible_4,\n            self.author_name_deconstructible_list_2,\n            self.author_name_deconstructible_list_3,\n            self.author_name_deconstructible_tuple_2,\n            self.author_name_deconstructible_tuple_3,\n            self.author_name_deconstructible_dict_2,\n            self.author_name_deconstructible_dict_3,\n            self.author_name_nested_deconstructible_2,\n            self.author_name_nested_deconstructible_changed_arg,\n            self.author_name_nested_deconstructible_extra_arg,\n            self.author_name_nested_deconstructible_changed_kwarg,\n            self.author_name_nested_deconstructible_extra_kwarg,\n        ]\n        changes = self.get_changes(before_states, after_states)\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, ['AlterField'])\n        self.assertOperationFieldAttributes(changes, 'testapp', ", "def test_dates_of_birth_field_change(self):\n    changes = self.get_changes(\n        [self.author_dates_of_birth_auto_now],\n        [self.author_dates_of_birth_auto_now_add],\n    )\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterField\", \"AlterField\", \"AlterField\"])\n    self.assertOperationFieldAttributes(changes, \"testapp\", 0, 0, auto_now=False, auto_now_add=True)\n    self.assertOperationFieldAttributes(changes, \"testapp\", 0, 1, auto_now=False, auto_now_add=True)\n    self.assertOperationFieldAttributes(changes, \"testapp\", 0, 2, auto_now=False, auto_now_add=True)\n", "def test_deconstructible_object_default(self):\n    changes = self.get_changes(\n        [self.author_name_deconstructible_1],\n        [self.author_name_deconstructible_2],\n    )\n    self.assertOperationFieldAttributes(\n        changes,\n        \"testapp\",\n        0,\n        0,\n        default=DeconstructibleObject(),\n    )\n", "    def test_author_name_deconstructible_changed_arg_detects_correctly(self):\n        changes = self.get_changes(\n            [self.author_name_deconstructible_1],\n            [self.author_name_nested_deconstructible_changed_arg],\n        )\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, ['AlterField'])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name='name')\n        self.assertOperationFieldAttributes(changes, 'testapp', 0, 0, max_length=200)\n", "    def test_initial_migration_true(self):\n        autodetector = MigrationAutodetector(ProjectState(), ProjectState(), MigrationQuestioner())\n        autodetector.project_state_before = ProjectState()\n        autodetector.project_state_after = ProjectState()\n        autodetector.project_state_after.add_model(self.author_name)\n        changes = autodetector._detect_changes()\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        migration = changes['testapp'][0]\n        self.assertTrue(migration.initial)\n        self.assertEqual(migration.operations[0].__class__, migrations.CreateModel)\n"], "sample_525": ["def test_figure_align_labels():\n    fig, axs = plt.subplots(2, 2, sharex=True, sharey=True)\n    for ax in axs.flat:\n        ax.label_outer()\n    fig.align_labels()\n", "def test_figure_align_labels():\n    fig, axs = plt.subplots(2, 2)\n    for ax in axs.flat:\n        ax.plot([1, 2, 3], [4, 5, 6])\n        ax.set_xlabel('X Label')\n        ax.set_ylabel('Y Label')\n    fig.align_labels()\n    plt.close(fig)\n", "def test_figure_get_size_inches():\n    fig = Figure(figsize=(5, 7))\n    assert np.allclose(fig.get_size_inches(), [5, 7])\n", "def test_figure_align_labels_smoke():\n    fig, axs = plt.subplots(2, 2)\n    fig.align_labels()\n", "def test_figure_legend_location(fig_test):\n    fig = Figure()\n    ax = fig.add_subplot(111)\n    ax.plot([1, 2, 3], label='Test')\n    fig.legend(loc='upper right')\n    fig_test(fig)\n"], "sample_157": ["    def test_serialize_db_to_string(self):\n        test_connection = get_connection_copy()\n        database_creation = BaseDatabaseCreation(test_connection)\n\n        # Create some test data\n        Object.objects.create(name='Test Object')\n        ObjectReference.objects.create(referenced_object=Object.objects.get(name='Test Object'))\n\n        # Call the method to serialize the database to a string\n        serialized_data = database_creation.serialize_db_to_string()\n\n        # Assert that the serialized data is not empty\n        self.assertIsNotNone(serialized_data)\n\n        # Assert that the serialized data contains the test data\n        self.assertIn('Test Object', serialized_data)\n        self.assertIn('Test ObjectReference', serialized_data)\n", "    def test_serialize_db_to_string(self):\n        # Create a mock database creation object\n        test_connection = get_connection_copy()\n        db_creation = BaseDatabaseCreation(test_connection)\n\n        # Create some test data\n        Object.objects.create(name=\"Test Object\")\n        ObjectReference.objects.create(referenced_object=Object.objects.first())\n\n        # Serialize the database to a string\n        serialized_data = db_creation.serialize_db_to_string()\n\n        # Assert that the serialized data is not empty\n        self.assertIsNotNone(serialized_data)\n\n        # Optionally, you can also assert that the serialized data contains the test data\n        self.assertIn('Test Object', serialized_data)\n        self.assertIn('ObjectReference', serialized_data)\n", "    def test_get_test_db_name(self):\n        # Test when the 'TEST' database name is specified\n        test_connection = get_connection_copy()\n        test_connection.settings_dict['TEST']['NAME'] = 'custom_test_db'\n        creation = BaseDatabaseCreation(test_connection)\n        self.assertEqual(creation._get_test_db_name(), 'custom_test_db')\n\n        # Test when the 'TEST' database name is not specified\n        test_connection = get_connection_copy()\n        test_connection.settings_dict['NAME'] = 'test_db'\n        creation = BaseDatabaseCreation(test_connection)\n        self.assertEqual(creation._get_test_db_name(), TEST_DATABASE_PREFIX + 'test_db')\n", "    def test_test_db_signature(self, mock_nodb_cursor):\n        # Arrange\n        test_connection = get_connection_copy()\n        db_creation = BaseDatabaseCreation(test_connection)\n        test_database_name = 'test_database'\n        test_connection.settings_dict['NAME'] = test_database_name\n        expected_signature = (\n            test_connection.settings_dict['HOST'],\n            test_connection.settings_dict['PORT'],\n            test_connection.settings_dict['ENGINE'],\n            test_database_name,\n        )\n\n        # Act\n        result_signature = db_creation.test_db_signature()\n\n        # Assert\n        self.assertEqual(result_signature, expected_signature)\n        mock_nodb_cursor.assert_called_once()\n", "    def test_create_test_db(self):\n        # Setup\n        test_connection = get_connection_copy()\n        test_connection.settings_dict['TEST']['NAME'] = ''\n        test_database_name = TEST_DATABASE_PREFIX + test_connection.settings_dict['NAME']\n        db_creation = BaseDatabaseCreation(test_connection)\n\n        # Mock the call to _execute_create_test_db to avoid actual database creation\n        with mock.patch.object(db_creation, '_execute_create_test_db') as mock_execute_create_test_db:\n            # Call the method under test\n            returned_test_database_name = db_creation.create_test_db(verbosity=0, autoclobber=False, serialize=False)\n\n            # Assertions\n            mock_execute_create_test_db.assert_called_once()\n            self.assertEqual(returned_test_database_name, test_database_name)\n"], "sample_338": ["def test_generate_altered_db_table(self):\n    changes = self.get_changes(\n        [self.author_with_db_table_options],\n        [self.author_with_new_db_table_options],\n    )\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['AlterModelTable'])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name='Author', table='author_two')\n", "def test_altered_fields_between_m2m_and_concrete(self):\n    changes = self.get_changes(\n        [self.author_with_m2m],\n        [self.author_with_former_m2m],\n    )\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"RemoveField\", \"AddField\"])\n", "def test_altered_field_with_deconstructible_default(self):\n    before_states = [self.author_name_deconstructible_1]\n    after_states = [self.author_name_deconstructible_2]\n    changes = self.get_changes(before_states, after_states)\n\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['AlterField'])\n    self.assertOperationFieldAttributes(changes, 'testapp', 0, 0, default=DeconstructibleObject())\n", "def test_generate_altered_managers(self):\n    before_states = [self.author_with_options]\n    after_states = [self.other_pony_food]\n    changes = self.get_changes(before_states, after_states)\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['AlterModelManagers'])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0,\n                                    name='Author',\n                                    managers=[('food_qs', FoodQuerySet.as_manager()),\n                                              ('food_mgr', FoodManager('a', 'b')),\n                                              ('food_mgr_kwargs', FoodManager('x', 'y', 3, 4))])\n", "    def test_create_model_with_unique_callable_default(self):\n        questioner = MigrationQuestioner(specified_apps={'testapp'})\n        questioner.ask_not_null_addition = mock.Mock(return_value='default')\n        questioner.ask_unique_callable_default_addition = mock.Mock()\n        changes = self.get_changes([], [self.author_name_default], questioner)\n        questioner.ask_unique_callable_default_addition.assert_called_once_with('name', 'Author')\n"], "sample_497": ["def test_maxnlocator_basic(self, vmin, vmax, expected):\n    loc = mticker.MaxNLocator(5)\n    assert_array_equal(loc.tick_values(vmin, vmax), expected)\n", "def test_max_n_locator_basic(self, vmin, vmax, expected):\n    locator = mticker.MaxNLocator(nbins=5, min_n_ticks=3)\n    ticks = locator.tick_values(vmin, vmax)\n    assert_array_equal(ticks, expected)\n", "def test_max_n_locator_basic(self, vmin, vmax, expected):\n    locator = mticker.MaxNLocator(nbins=5)\n    locator.set_view_interval(vmin, vmax)\n    assert_array_equal(locator(), expected)\n", "def test_maxnlocator_basic(self, vmin, vmax, expected):\n    locator = mticker.MaxNLocator(nbins=5, min_n_ticks=5)\n    locator.set_view_interval(vmin, vmax)\n    assert_array_equal(locator(), expected)\n", "def test_integer_locator(self, vmin, vmax, steps, expected):\n    locator = mticker.MaxNLocator(integer=True, steps=steps)\n    locator.tick_values(vmin, vmax)\n    assert_array_equal(locator._tick_values, expected)\n"], "sample_46": ["    def test_lookup_rhs_is_direct_value(self):\n        lookup = Exact(UUIDModel.uuid_field, uuid.uuid4())\n        self.assertTrue(lookup.rhs_is_direct_value())\n\n        lookup = Exact(UUIDModel.uuid_field, UUIDModel.objects.first())\n        self.assertFalse(lookup.rhs_is_direct_value())\n", "    def setUp(self):\n        self.obj = UUIDModel.objects.create(uuid_field=uuid.uuid4())\n", "    def setUpTestData(cls):\n        UUIDModel.objects.create(uuid_field=uuid.uuid4())\n", "    def test_year_exact(self):\n        from datetime import date\n        uuid_obj = UUIDModel.objects.create(uuid=uuid.uuid4(), date=date(2022, 1, 1))\n        self.assertEqual(UUIDModel.objects.filter(date__year__exact=2022).count(), 1)\n        self.assertEqual(UUIDModel.objects.filter(date__year__exact=2023).count(), 0)\n", "    def setUpTestData(cls):\n        UUIDModel.objects.create(uuid=uuid.uuid4(), name='Test1', date_field='2020-01-01')\n        UUIDModel.objects.create(uuid=uuid.uuid4(), name='Test2', date_field='2021-01-01')\n"], "sample_977": ["def test_mathematica_code():\n    assert mcode(x**y) == 'x^y'\n    assert mcode(exp(x)) == 'Exp[x]'\n    assert mcode(sin(x)) == 'Sin[x]'\n    assert mcode(cos(x)) == 'Cos[x]'\n    assert mcode(pi) == 'Pi'\n    assert mcode(oo) == 'Infinity'\n    assert mcode(-oo) == '-Infinity'\n    assert mcode(Tuple(1, 2, 3)) == '{1, 2, 3}'\n    assert mcode(f(x)) == 'f[x]'\n    assert mcode(Integral(x**2, x)) == 'Hold[Integrate[x**2, x]]'\n    assert mcode(Sum(x, (x, 1, n))) == 'Hold[Sum[x, x, 1, n]]'\n    assert mcode(Derivative(f(x), x)) == 'Hold[D[f[x], x]]'\n", "def test_mcode_printer():\n    assert mcode(x**y) == 'x^y'\n    assert mcode(x*y) == 'x*y'\n    assert mcode(pi) == 'Pi'\n    assert mcode(oo) == 'Infinity'\n    assert mcode(sin(x)) == 'Sin[x]'\n    assert mcode(f(x)) == 'f[x]'\n    assert mcode(Integral(sin(x), x)) == 'Hold[Integrate[sin[x], x]]'\n    assert mcode(Sum(x**y, (y, 1, z))) == 'Hold[Sum[x^y, y, 1, z]]'\n    assert mcode(Derivative(f(x), x)) == 'Hold[D[f[x], x]]'\n", "def test_mathematica_code():\n    assert mcode(exp(x) + sin(x)) == \"E^x + Sin[x]\"\n    assert mcode(Integral(sin(x), (x, 0, pi))) == \"Hold[Integrate[Sin[x], x, 0, Pi]]\"\n    assert mcode(Sum(x**y, (y, 1, z))) == \"Hold[Sum[x^y, y, 1, z]]\"\n    assert mcode(Derivative(f(x), x, x, x)) == \"Hold[D[f[x], {x, 3}]]\"\n", "def test_mathematica_code():\n    assert mcode(pi) == \"Pi\"\n    assert mcode(oo) == \"Infinity\"\n    assert mcode(-oo) == \"-Infinity\"\n    assert mcode(Tuple(1, 2, 3)) == \"{1, 2, 3}\"\n    assert mcode(f(x)) == \"f[x]\"\n    assert mcode(Integral(sin(x), x)) == \"Hold[Integrate[sin[x], x]]\"\n    assert mcode(Sum(x**i, (i, 1, n))) == \"Hold[Sum[x**i, i, 1, n]]\"\n    assert mcode(Derivative(sin(x), x)) == \"Hold[D[sin[x], x]]\"\n    assert mcode(exp(x) ** Rational(1, 2)) == \"Exp[x/2]\"\n    assert mcode(sin(x) * cos(x)) == \"sin[x]*cos[x]\"\n    assert mcode(x ** y) == \"x^y\"\n", "def test_mathematica_code():\n    # Testing basic arithmetic operations\n    assert mcode(x + y) == 'x + y'\n    assert mcode(x - y) == 'x - y'\n    assert mcode(x * y) == 'x*y'\n    assert mcode(x / y) == 'x/y'\n    assert mcode(x ** y) == 'x^y'\n\n    # Testing mathematical functions\n    assert mcode(sin(x)) == 'Sin[x]'\n    assert mcode(cos(x)) == 'Cos[x]'\n    assert mcode(exp(x)) == 'Exp[x]'\n\n    # Testing constants\n    assert mcode(pi) == 'Pi'\n    assert mcode(oo) == 'Infinity'\n    assert mcode(-oo) == '-Infinity'\n\n    # Testing lists and tuples\n    assert mcode(Tuple(x, y, z)) == '{x, y, z}'\n\n    # Testing user-defined functions\n    assert mcode(f(x)) == 'f[x]'\n    assert mcode(f(x, y)) == 'f[x, y]'\n\n    # Testing integration and summation\n    assert mcode(Integral(sin(x), x)) == 'Hold[Integrate[sin[x], x]]'\n    assert mcode(Sum(x**y, (y, 1, z))) == 'Hold[Sum[x^y, y, 1, z]]'\n\n    # Testing derivatives\n    assert mcode(Derivative(f(x), x)) == 'Hold[D[f[x], x]]'\n"], "sample_463": ["def test_nested_deconstructible_objects(self):\n    changes = self.get_changes(\n        [self.author_name_nested_deconstructible_1],\n        [self.author_name_nested_deconstructible_2],\n    )\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterField\"])\n    self.assertOperationFieldAttributes(\n        changes, \"testapp\", 0, 0, default=self.author_name_nested_deconstructible_1.get_field(\"name\").default\n    )\n\n    changes = self.get_changes(\n        [self.author_name_nested_deconstructible_1],\n        [self.author_name_nested_deconstructible_changed_arg],\n    )\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterField\"])\n    self.assertOperationFieldAttributes(\n        changes, \"testapp\", 0, 0, default=self.author_name_nested_deconstructible_changed_arg.get_field(\"name\").default\n    )\n\n    changes = self.get_changes(\n        [self.author_name_nested_deconstructible_1],\n        [self.author_name_nested_deconstructible_extra_arg],\n    )\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterField\"])\n    self.assertOperationFieldAttributes(\n        changes, \"testapp\", 0, 0, default=self.author_name_nested_deconstructible_extra_arg.get_field(\"name\").default\n    )\n\n    changes = self.get_changes(\n        [self.author_name_nested_deconstructible_1],\n        [self.author_name_nested_deconstructible_changed_kwarg],\n    )\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assert", "def test_deconstructible_objects_equality(self):\n    questioner = MigrationQuestioner(specified_apps=[])\n    questioner.ask_not_null_addition = mock.Mock(return_value=models.IntegerField(default=0))\n\n    changes = self.get_changes(\n        [self.author_name_deconstructible_1],\n        [self.author_name_deconstructible_2],\n        questioner=questioner,\n    )\n    self.assertNumberMigrations(changes, \"testapp\", 0)\n\n    changes = self.get_changes(\n        [self.author_name_deconstructible_1],\n        [self.author_name_deconstructible_3],\n        questioner=questioner,\n    )\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n\n    changes = self.get_changes(\n        [self.author_name_deconstructible_3],\n        [self.author_name_deconstructible_4],\n        questioner=questioner,\n    )\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n\n    changes = self.get_changes(\n        [self.author_name_deconstructible_list_1],\n        [self.author_name_deconstructible_list_2],\n        questioner=questioner,\n    )\n    self.assertNumberMigrations(changes, \"testapp\", 0)\n\n    changes = self.get_changes(\n        [self.author_name_deconstructible_list_1],\n        [self.author_name_deconstructible_list_3],\n        questioner=questioner,\n    )\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n\n    changes = self.get_changes(\n        [self.author_name_deconstructible_tuple_1],\n        [self.author_name_deconstructible_tuple_2],\n        questioner=questioner,\n    )\n    self.assertNumberMigrations(changes, \"testapp\", 0)\n\n    changes = self.get_changes", "def test_autodetector_renamed_model_with_relations(self):\n    changes = self.get_changes(\n        [self.author_with_book],\n        [self.author_renamed_with_book],\n    )\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"RenameModel\"])\n    self.assertOperationTypes(changes, \"otherapp\", 0, [\"AlterField\"])\n    self.assertOperationFieldAttributes(changes, \"otherapp\", 0, 0, to=\"testapp.Writer\")\n", "    def test_author_name_deconstructible_changed(self):\n        # Test when deconstructible object changes between states\n        changes = self.get_changes(\n            [self.author_name_deconstructible_1],\n            [self.author_name_deconstructible_3],\n        )\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterField\"])\n        self.assertOperationAttributes(\n            changes,\n            \"testapp\",\n            0,\n            0,\n            name=\"name\",\n        )\n        self.assertOperationFieldAttributes(\n            changes,\n            \"testapp\",\n            0,\n            0,\n            default=models.IntegerField(),\n        )\n", "def test_m2m_changed(self):\n    changes = self.get_changes(\n        [self.author_with_m2m],\n        [self.author_with_former_m2m],\n    )\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"RemoveField\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"publishers\")\n\n    changes = self.get_changes(\n        [self.author_with_former_m2m],\n        [self.author_with_m2m],\n    )\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AddField\"])\n    self.assertOperationAttributes(\n        changes,\n        \"testapp\",\n        0,\n        0,\n        name=\"publishers\",\n        field=models.ManyToManyField(\"testapp.Publisher\"),\n    )\n"], "sample_440": ["def test_bulk_create_ignore_conflicts(self):\n    Country.objects.bulk_create(self.data, ignore_conflicts=True)\n    # Try to create the same objects again, should not raise an error\n    Country.objects.bulk_create(self.data, ignore_conflicts=True)\n    self.assertEqual(Country.objects.count(), 4)\n", "    def test_bulk_create_ignore_conflicts(self):\n        Country.objects.bulk_create(self.data, ignore_conflicts=True)\n        self.assertEqual(Country.objects.count(), len(self.data))\n", "    def test_bulk_create_with_ignore_conflicts(self):\n        Country.objects.bulk_create(self.data, ignore_conflicts=True)\n        self.assertEqual(Country.objects.count(), 4)\n        # Try to create the same objects again and ensure no error is raised.\n        Country.objects.bulk_create(self.data, ignore_conflicts=True)\n        self.assertEqual(Country.objects.count(), 4)\n", "def test_bulk_create_with_unique_fields(self):\n    Country.objects.bulk_create(self.data)\n    with self.assertRaises(IntegrityError):\n        Country.objects.bulk_create(self.data, ignore_conflicts=True)\n    with self.assertRaises(IntegrityError):\n        Country.objects.bulk_create(self.data, update_conflicts=True, update_fields=['name'])\n    with self.assertRaises(ValueError):\n        Country.objects.bulk_create(self.data, update_conflicts=True)\n    with self.assertRaises(ValueError):\n        Country.objects.bulk_create(self.data, ignore_conflicts=True, update_conflicts=True)\n", "    def test_bulk_create_single_object_no_autofield(self):\n        obj = Country(name=\"France\", iso_two_letter=\"FR\")\n        self.assertEqual(Country.objects.bulk_create([obj]), [obj])\n        self.assertIsNotNone(obj.pk)\n"], "sample_177": ["    def test_reload_model(self):\n        state = ProjectState()\n        state.add_model(ModelState('testapp', 'TestModel', {'name': models.CharField(max_length=100)}))\n        state.add_model(ModelState('testapp', 'RelatedModel', {'test_model': models.ForeignKey('testapp.TestModel', on_delete=models.CASCADE)}))\n        state.reload_model('testapp', 'testmodel')\n        self.assertIn(('testapp', 'testmodel'), state.apps.all_models)\n        self.assertIn(('testapp', 'relatedmodel'), state.apps.all_models)\n        state.remove_model('testapp', 'testmodel')\n        self.assertNotIn(('testapp', 'testmodel'), state.apps.all_models)\n        self.assertNotIn(('testapp', 'relatedmodel'), state.apps.all_models)\n", "    def test_add_field_operation(self):\n        project_state = ProjectState()\n        model_state = ModelState(\"test_app\", \"TestModel\", {})\n        project_state.add_model(model_state)\n\n        operation = AddField(\"test_app\", \"TestModel\", models.CharField(\"test_field\", max_length=255))\n        operation.state_forwards(\"test_app\", project_state)\n\n        new_model_state = project_state.models[\"test_app\", \"testmodel\"]\n        self.assertIn(\"test_field\", new_model_state.fields)\n", "def test_get_related_models_recursive(self):\n    project_state = ProjectState()\n    model_state = ModelState('testapp', 'TestModel', {'field1': models.ForeignKey('otherapp.OtherModel', on_delete=models.CASCADE)})\n    project_state.add_model(model_state)\n    related_models = get_related_models_recursive(project_state.apps.get_model('testapp', 'testmodel'))\n    self.assertEqual(related_models, {('otherapp', 'othermodel')})\n", "    def test_get_related_models_recursive(self):\n        class Author(models.Model):\n            name = models.CharField(max_length=100)\n\n        class Book(models.Model):\n            author = models.ForeignKey(Author, on_delete=models.CASCADE)\n\n        class Review(models.Model):\n            book = models.ForeignKey(Book, on_delete=models.CASCADE)\n            author = models.ForeignKey(Author, on_delete=models.CASCADE)\n\n        related_models = get_related_models_recursive(Author)\n        self.assertEqual(related_models, {('django.db.migrations', 'book'), ('django.db.migrations', 'review')})\n", "def test_model_state_managers(self):\n    # Test the construction and rendering of ModelState with custom managers\n    state = ProjectState()\n\n    # Create a model state with custom managers\n    custom_managers = [('objects', FoodManager()), ('custom', NoMigrationFoodManager())]\n    model_state = ModelState(\n        app_label='app_label',\n        name='CustomModel',\n        fields=[],\n        managers=custom_managers,\n    )\n\n    # Add the model state to the project state\n    state.add_model(model_state)\n\n    # Render the model\n    model = state.apps.get_model('app_label', 'custommodel')\n\n    # Check if the model has the correct managers\n    self.assertIsInstance(model._default_manager, models.Manager)\n    self.assertIsInstance(model.objects, FoodManager)\n    self.assertIsInstance(model.custom, NoMigrationFoodManager)\n\n    # Check if the managers have the correct model attribute\n    self.assertEqual(model._default_manager.model, model)\n    self.assertEqual(model.objects.model, model)\n    self.assertEqual(model.custom.model, model)\n"], "sample_853": ["def test_transformed_target_regressor_with_custom_transformer():\n    class CustomTransformer(BaseEstimator, TransformerMixin):\n            return self\n\n            return np.log(y)\n\n            return np.exp(y_trans)\n\n    X, y = friedman\n    reg = TransformedTargetRegressor(regressor=LinearRegression(),\n                                     transformer=CustomTransformer())\n    reg.fit(X, y)\n    y_pred = reg.predict(X)\n    assert_allclose(y_pred, np.exp(reg.regressor_.predict(X)), atol=1e-5)\n", "def test_transformed_target_regressor_with_custom_transformer():\n    class CustomTransformer(BaseEstimator, TransformerMixin):\n            return self\n\n            return X ** 2\n\n            return np.sqrt(X)\n\n    tt = TransformedTargetRegressor(regressor=LinearRegression(), transformer=CustomTransformer())\n    X, y = friedman\n    tt.fit(X, y)\n    y_pred = tt.predict(X)\n    assert_allclose(y_pred, np.sqrt(LinearRegression().fit(X, y**2).predict(X)))\n", "def test_transformed_target_regressor_with_sample_weight():\n    X, y = friedman\n    reg = TransformedTargetRegressor(regressor=LinearRegression(),\n                                     func=np.log, inverse_func=np.exp)\n    reg.fit(X, y, sample_weight=np.ones_like(y))\n    pred = reg.predict(X)\n    assert pred.shape == y.shape\n    assert np.all(pred > 0)\n", "def test_transformed_target_regressor_with_scaler():\n    # Create a TransformedTargetRegressor with StandardScaler\n    regr = TransformedTargetRegressor(regressor=LinearRegression(), transformer=StandardScaler())\n\n    # Fit the model\n    regr.fit(*friedman)\n\n    # Predict using the model\n    y_pred = regr.predict(friedman[0])\n\n    # Check that the predictions have the same shape as the target\n    assert y_pred.shape == friedman[1].shape\n\n    # Check that the transformed target and the predicted target have the same shape\n    assert regr.transformer_.transform(friedman[1].reshape(-1, 1)).shape == regr.regressor_.predict(friedman[0]).reshape(-1, 1).shape\n", "def test_transformed_target_regressor_with_different_regressors():\n    X, y = friedman\n\n    # Test with Linear Regression\n    reg_linear = LinearRegression()\n    transformer = FunctionTransformer(func=np.log1p, inverse_func=np.expm1)\n    regr_linear = TransformedTargetRegressor(regressor=reg_linear, transformer=transformer)\n    regr_linear.fit(X, y)\n    assert hasattr(regr_linear, \"regressor_\")\n    assert hasattr(regr_linear, \"transformer_\")\n\n    # Test with Lasso Regression\n    reg_lasso = Lasso()\n    transformer = StandardScaler()\n    regr_lasso = TransformedTargetRegressor(regressor=reg_lasso, transformer=transformer)\n    regr_lasso.fit(X, y)\n    assert hasattr(regr_lasso, \"regressor_\")\n    assert hasattr(regr_lasso, \"transformer_\")\n"], "sample_933": ["def test_message_catalog_builder_config(app, status, warning):\n    app.builder.build_all()\n\n    config_values = {\n        'gettext_compact': app.config.gettext_compact,\n        'gettext_location': app.config.gettext_location,\n        'gettext_uuid': app.config.gettext_uuid,\n        'gettext_auto_build': app.config.gettext_auto_build,\n        'gettext_additional_targets': app.config.gettext_additional_targets,\n        'gettext_last_translator': app.config.gettext_last_translator,\n        'gettext_language_team': app.config.gettext_language_team,\n    }\n\n    assert config_values == {\n        'gettext_compact': True,\n        'gettext_location': True,\n        'gettext_uuid': False,\n        'gettext_auto_build': True,\n        'gettext_additional_targets': [],\n        'gettext_last_translator': 'FULL NAME <EMAIL@ADDRESS>',\n        'gettext_language_team': 'LANGUAGE <LL@li.org>',\n    }\n", "def test_gettext_last_translator_configuration(app, status, warning):\n    app.config.gettext_last_translator = 'Test User <test@example.com>'\n    app.builder.build_all()\n    pot_file = os.path.join(app.outdir, 'index.pot')\n    with open(pot_file, 'r', encoding='utf-8') as f:\n        pot_content = f.read()\n    assert 'Test User <test@example.com>' in pot_content\n", "def test_build_gettext_additional_targets(app, status, warning):\n    app.builder.config.gettext_additional_targets = ['index']\n    app.builder.build_all()\n\n    content = (app.outdir / 'sphinx.pot').read_text(encoding='utf-8')\n    assert 'This is a test index entry.' in content\n    assert status(1)\n    assert not warning()\n", "def test_message_catalog_builder(app, status, warning):\n    app.builder.build_all()\n\n    # Test if the message catalog files are created\n    assert os.path.exists(os.path.join(app.outdir, 'sphinx.pot'))\n    assert os.path.exists(os.path.join(app.outdir, 'python.pot'))\n\n    # Test the content of the message catalog files\n    with open(os.path.join(app.outdir, 'sphinx.pot'), 'r', encoding='utf-8') as f:\n        sphinx_pot_content = f.read()\n        assert 'SOME DESCRIPTIVE TITLE.' in sphinx_pot_content\n        assert 'The message catalogs are in %(outdir)s.' in sphinx_pot_content\n\n    with open(os.path.join(app.outdir, 'python.pot'), 'r', encoding='utf-8') as f:\n        python_pot_content = f.read()\n        assert 'Python language domain' in python_pot_content\n        assert 'Python object' in python_pot_content\n\n    # Test if the gettext compilation process runs without errors\n    with cd(app.outdir):\n        try:\n            subprocess.check_output(['msgfmt', 'sphinx.pot'], stderr=subprocess.STDOUT)\n            subprocess.check_output(['msgfmt', 'python.pot'], stderr=subprocess.STDOUT)\n        except CalledProcessError as e:\n            pytest.fail(f\"Gettext compilation failed with error: {e.output.decode('utf-8')}\")\n\n    # Test if the gettext compilation creates the expected MO files\n    assert os.path.exists(os.path.join(app.outdir, 'sphinx.mo'))\n    assert os.path.exists(os.path.join(app.outdir, 'python.mo'))\n\n    # Test if the MO files can be loaded and used with gettext\n    sphinx_translations = gettext.translation('sphinx', localedir=app.outdir)\n    assert", "def test_message_catalog_builder(app, status, warning):\n    app.builder.build_all()\n\n    pot_file = os.path.join(app.outdir, 'sphinx.pot')\n    assert os.path.exists(pot_file), \"Message catalog file does not exist\"\n\n    with open(pot_file, 'r', encoding='utf-8') as f:\n        content = f.read()\n        # Check if the header is correctly generated\n        assert re.search(r'Project-Id-Version: %s' % app.config.project, content)\n        assert re.search(r'Report-Msgid-Bugs-To:', content)\n\n        # Check if messages are correctly extracted\n        assert '\"POT-Creation-Date:\"' in content\n        assert 'msgid \"The message catalogs are in %(outdir)s.\"' in content\n        assert 'msgid \"building [%s]: \"' % app.builder.name in content\n\n        # Check if the location and UUID are correctly handled\n        if app.config.gettext_location:\n            assert re.search(r'#: .*?:\\d+', content)\n        if app.config.gettext_uuid:\n            assert re.search(r'#, python-format', content)\n\n    # Check the output of sphinx-build command\n    assert \"build succeeded\" in status.getvalue()\n    assert \"warning\" not in warning.getvalue().lower()\n"], "sample_424": ["    def test_create_model_with_duplicated_field_names(self):\n        \"\"\"\n        Test that an error is raised when a model is created with duplicated field names.\n        \"\"\"\n        with self.assertRaisesMessage(ValueError, \"Found duplicate value field_name in CreateModel fields argument.\"):\n            class DuplicateFieldModel(migrations.CreateModel):\n                name = 'DuplicateField'\n                fields = [\n                    ('field_name', models.CharField(max_length=50)),\n                    ('field_name', models.IntegerField()),\n                ]\n        ", "    def test_add_field(self):\n        operation = migrations.AddField(\"TestModel\", \"new_field\", models.CharField(max_length=50))\n        state = ProjectState()\n        state.add_model(ModelState(\"app_label\", \"TestModel\", []))\n\n        # Test state forward\n        operation.state_forwards(\"app_label\", state)\n        self.assertIn((\"new_field\", models.CharField(max_length=50)), state.models[\"app_label\", \"testmodel\"].fields)\n\n        # Test database forward\n        with self.create_model((models.Model,), name=\"TestModel\") as (TestModel, _):\n            with connection.schema_editor() as editor:\n                operation.database_forwards(\"app_label\", editor, ProjectState(), state)\n                self.assertTrue(hasattr(TestModel, \"new_field\"))\n\n        # Test database backward\n        with self.create_model((models.Model,), name=\"TestModel\", attrs={\"new_field\": models.CharField(max_length=50)}) as (TestModel, _):\n            with connection.schema_editor() as editor:\n                operation.database_backwards(\"app_label\", editor, state, ProjectState())\n                self.assertFalse(hasattr(TestModel, \"new_field\"))\n", "    def test_add_index_forwards(self):\n        operation = migrations.AddIndex(\"TestModel\", models.Index(fields=[\"field1\", \"field2\"], name=\"test_index\"))\n        initial_state = ProjectState()\n        initial_state.add_model(ModelState(\"app_label\", \"TestModel\", [(\"field1\", models.IntegerField()), (\"field2\", models.IntegerField())]))\n        final_state = ProjectState()\n        operation.state_forwards(\"app_label\", final_state)\n        self.assertEqual(len(final_state.models[\"app_label\", \"testmodel\"].options[\"indexes\"]), 1)\n        self.assertEqual(final_state.models[\"app_label\", \"testmodel\"].options[\"indexes\"][0].name, \"test_index\")\n", "    def test_add_index(self):\n        operation = migrations.AddIndex(\"Food\", models.Index(fields=[\"name\"], name=\"food_name_idx\"))\n        state = ProjectState()\n        state.add_model(ModelState(\"test\", \"Food\", [(\"name\", models.CharField(max_length=50))]))\n\n        # Test state forwards\n        operation.state_forwards(\"test\", state)\n        self.assertIn(\"food_name_idx\", [index.name for index in state.models[\"test\", \"food\"].options[\"indexes\"]])\n\n        # Test database forwards\n        with connection.schema_editor() as schema_editor:\n            schema_editor.create_model(state.apps.get_model(\"test\", \"Food\"))\n            operation.database_forwards(\"test\", schema_editor, ProjectState(), state)\n            self.assertIn(\"food_name_idx\", connection.introspection.get_constraints(state.apps.get_model(\"test\", \"Food\")._meta.db_table))\n\n        # Test database backwards\n        with connection.schema_editor() as schema_editor:\n            operation.database_backwards(\"test\", schema_editor, ProjectState(), state)\n            self.assertNotIn(\"food_name_idx\", connection.introspection.get_constraints(state.apps.get_model(\"test\", \"Food\")._meta.db_table))\n", "    def initial_state(self):\n        return ProjectState(\n            {\n                (self.app, \"model\"): ModelState(\n                    self.app,\n                    \"model\",\n                    fields=[\n                        (\"id\", models.AutoField(primary_key=True)),\n                    ],\n                    options={},\n                ),\n            },\n        )\n"], "sample_326": ["    def test_urlize(self):\n        self.check_output(urlize, 'http://example.com', '<a href=\"http://example.com\">http://example.com</a>')\n        self.check_output(urlize, 'www.example.com', '<a href=\"http://www.example.com\">www.example.com</a>')\n        self.check_output(urlize, 'example@example.com', '<a href=\"mailto:example@example.com\">example@example.com</a>')\n        self.check_output(urlize, 'example@example.com.', 'example@example.com.')\n        self.check_output(urlize, 'Visit http://example.com for more.', 'Visit <a href=\"http://example.com\">http://example.com</a> for more.')\n        self.check_output(urlize, 'Visit example@example.com for more.', 'Visit <a href=\"mailto:example@example.com\">example@example.com</a> for more.')\n", "def test_urlize(self):\n    # Test simple URLs\n    self.check_output(urlize, 'http://example.com', '<a href=\"http://example.com\">http://example.com</a>')\n    self.check_output(urlize, 'https://example.com', '<a href=\"https://example.com\">https://example.com</a>')\n    self.check_output(urlize, 'www.example.com', '<a href=\"http://www.example.com\">www.example.com</a>')\n    self.check_output(urlize, 'example.com', '<a href=\"http://example.com\">example.com</a>')\n    self.check_output(urlize, 'example.net', '<a href=\"http://example.net\">example.net</a>')\n\n    # Test URLs with trailing punctuation\n    self.check_output(urlize, 'http://example.com.', '<a href=\"http://example.com\">http://example.com</a>.')\n    self.check_output(urlize, 'http://example.com,', '<a href=\"http://example.com\">http://example.com</a>,')\n    self.check_output(urlize, 'http://example.com)', '<a href=\"http://example.com\">http://example.com</a>)')\n\n    # Test URLs with leading punctuation\n    self.check_output(urlize, '(http://example.com)', '(<a href=\"http://example.com\">http://example.com</a>)')\n\n    # Test email addresses\n    self.check_output(urlize, 'email@example.com', '<a href=\"mailto:email@example.com\">email@example.com</a>')\n", "    def test_escape(self):\n        self.check_output(escape, '<script>alert(\"Hello, World!\");</script>', '&lt;script&gt;alert(&quot;Hello, World!&quot;);&lt;/script&gt;')\n        self.check_output(escape, \"A 'quote' is <b>bold</b>\", 'A &#x27;quote&#x27; is &lt;b&gt;bold&lt;/b&gt;')\n", "    def test_urlize_autoescape(self):\n        \"\"\"urlize() autoescapes output when input is not safe.\"\"\"\n        url = 'http://example.com/\"onmouseover=\"alert(1)'\n        expected = '<a href=\"http://example.com/%22onmouseover=%22alert(1)\">http://example.com/\"onmouseover=\"alert(1)</a>'\n        self.assertEqual(urlize(url, autoescape=True), expected)\n", "def test_urlize(self):\n    \"\"\"\n    Test the urlize function with various URLs and cases.\n    \"\"\"\n    self.check_output(\n        urlize,\n        'Check out http://example.com and www.example.net.',\n        'Check out <a href=\"http://example.com\">http://example.com</a> and '\n        '<a href=\"http://www.example.net\">www.example.net</a>.'\n    )\n    self.check_output(\n        urlize,\n        'Send email to someone@example.com.',\n        'Send email to <a href=\"mailto:someone@example.com\">someone@example.com</a>.'\n    )\n    self.check_output(\n        urlize,\n        'Check out example.org.',\n        'Check out <a href=\"http://example.org\">example.org</a>.'\n    )\n    self.check_output(\n        urlize,\n        'Invalid URL: http://[invalid',\n        'Invalid URL: <a href=\"http://[invalid\">http://[invalid</a>'\n    )\n"], "sample_351": ["def test_model_choice_field_custom_to_field_name(self):\n    class CategoryChoiceForm(forms.Form):\n        category = forms.ModelChoiceField(queryset=Category.objects.all(), to_field_name='slug')\n\n    form = CategoryChoiceForm(data={'category': 'test'})\n    self.assertTrue(form.is_valid())\n    self.assertEqual(form.cleaned_data['category'], self.c2)\n", "    def test_to_python_with_valid_value(self):\n        field = ModelChoiceField(queryset=Category.objects.all())\n        value = self.c1.pk\n        result = field.to_python(value)\n        self.assertEqual(result, self.c1)\n", "def test_model_choice_iterator(self):\n    field = forms.ModelChoiceField(Category.objects.all())\n    iterator = ModelChoiceIterator(field)\n    choices = list(iterator)\n    self.assertEqual(len(choices), 3)\n    self.assertEqual(choices[0][0], ModelChoiceIteratorValue(self.c1.pk, self.c1))\n    self.assertEqual(choices[0][1], 'Entertainment')\n", "    def test_model_choice_iterator(self):\n        field = ModelChoiceField(queryset=Category.objects.all())\n        iterator = ModelChoiceIterator(field)\n        choices = list(iterator)\n        self.assertEqual(len(choices), Category.objects.count() + 1)  # +1 for the empty label\n        self.assertEqual(choices[0], (\"\", field.empty_label))\n        for choice, obj in zip(choices[1:], Category.objects.all()):\n            self.assertEqual(choice, (ModelChoiceIteratorValue(obj.pk, obj), field.label_from_instance(obj)))\n", "def test_model_choice_iterator(self):\n    field = forms.ModelChoiceField(queryset=Category.objects.all())\n    iterator = ModelChoiceIterator(field)\n    choices = list(iterator)\n    self.assertEqual(len(choices), 3)  # Number of categories created\n    self.assertEqual(choices[0][1], 'Entertainment')  # First category name\n    self.assertEqual(choices[1][1], 'A test')  # Second category name\n    self.assertEqual(choices[2][1], 'Third')  # Third category name\n"], "sample_448": ["    def test_check_constraint_validate(self):\n        check_constraint = CheckConstraint(check=Q(price__gt=0), name='positive_price')\n        product = Product(name='Test', price=-1)\n        with self.assertRaises(ValidationError) as cm:\n            check_constraint.validate(Product, product)\n        self.assertEqual(str(cm.exception), 'Constraint \u201cpositive_price\u201d is violated.')\n", "    def test_check_constraint_violation_error_message(self):\n        check_constraint = CheckConstraint(\n            check=models.Q(price__gte=0),\n            name=\"positive_price\",\n            violation_error_message=\"Price must be a positive number.\"\n        )\n        product = Product(price=-5)\n        with self.assertRaisesMessage(ValidationError, \"Price must be a positive number.\"):\n            check_constraint.validate(Product, product)\n", "    def test_validate_unique_constraint_with_check_constraint(self):\n        with self.assertRaisesMessage(ValidationError, \"Constraint \u201cunique_check\u201d is violated.\"):\n            UniqueConstraintProduct.objects.create(name=\"DUPLICATE\", price=10)\n            UniqueConstraintProduct.objects.create(name=\"DUPLICATE\", price=20)\n\n        self.assertEqual(UniqueConstraintProduct.objects.count(), 1)\n\n        with self.assertRaisesMessage(ValidationError, \"Constraint \u201cunique_check\u201d is violated.\"):\n            UniqueConstraintProduct.objects.create(name=\"duplicate\", price=10)\n\n        self.assertEqual(UniqueConstraintProduct.objects.count(), 1)\n", "    def test_check_constraint_validates(self):\n        with self.assertRaises(ValidationError):\n            Product.objects.create(name=\"test\", price=-1)  # price cannot be negative\n\n        product = Product.objects.create(name=\"test\", price=1)\n        self.assertEqual(product.price, 1)\n", "    def test_unique_constraint_with_expressions(self):\n        class UniqueConstraintExpressionProduct(Product):\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(\n                        Lower('name'),\n                        name='unique_lower_name',\n                    ),\n                ]\n\n        # Create products with unique lowercase names\n        product1 = UniqueConstraintExpressionProduct.objects.create(name=\"Test\")\n        product2 = UniqueConstraintExpressionProduct.objects.create(name=\"TEST\")\n\n        # Check that a product with a duplicate lowercase name cannot be created\n        with self.assertRaises(IntegrityError):\n            product3 = UniqueConstraintExpressionProduct.objects.create(name=\"test\")\n\n        # Check that the constraint works with updates\n        product1.name = \"TEST\"\n        with self.assertRaises(IntegrityError):\n            product1.save()\n\n        # Check that the constraint is enforced during validation\n        product1.name = \"AnotherTest\"\n        product1.full_clean()\n        product1.name = \"anothertest\"\n        with self.assertRaises(ValidationError):\n            product1.full_clean()\n"], "sample_17": ["    def test_structured_to_unstructured(self):\n        dtype = np.dtype([(\"a\", float), (\"b\", float)])\n        q = np.array([(1.0, 2.0), (3.0, 4.0)], dtype=dtype) * u.m\n        result = rfn.structured_to_unstructured(q)\n        expected = np.array([1.0, 2.0, 3.0, 4.0]) * u.m\n        assert np.all(result == expected)\n", "    def test_shape(self):\n        self.check(np.shape, out=None)\n", "    def test_sin(self):\n        self.check(np.sin)\n", "    def test_shape(self):\n        assert np.shape(self.q) == self.q.shape\n        assert np.size(self.q) == self.q.size\n        assert np.ndim(self.q) == self.q.ndim\n", "    def test_gradient(self):\n        self.check(np.gradient, self.q)\n        self.check(np.gradient, self.q, 2)\n        self.check(np.gradient, self.q, 2, axis=0)\n"], "sample_760": ["def test_check_scoring_with_custom_scorer():\n    est = EstimatorWithFit()\n    scorer = make_scorer(fbeta_score, beta=2)\n    checked_scorer = check_scoring(est, scoring=scorer)\n    assert isinstance(checked_scorer, _PredictScorer)\n    assert checked_scorer._score_func == fbeta_score\n    assert checked_scorer._kwargs == {'beta': 2}\n", "def test_pickle_scorer():\n    # Test that scorer objects can be pickled\n    scorers = [v for v in SCORERS.values() if isinstance(v, _PredictScorer)]\n    for scorer in scorers:\n        scorer_pickled = pickle.loads(pickle.dumps(scorer))\n        assert_equal(repr(scorer), repr(scorer_pickled))\n", "def test_make_scorer_with_custom_score_func():\n        return np.mean(y_true == y_pred)\n\n    custom_scorer = make_scorer(custom_score_func)\n    assert_equal(custom_scorer._score_func, custom_score_func)\n    assert_equal(custom_scorer._sign, 1)\n    assert_equal(custom_scorer._kwargs, {})\n", "def test_pickle_scorer():\n    scorers = [SCORERS[name] for name in SCORERS]\n    for scorer in scorers:\n        scorer_pickled = pickle.loads(pickle.dumps(scorer))\n        assert str(scorer) == str(scorer_pickled)\n", "def test_make_scorer():\n        return np.mean(y_true == y_pred)\n\n    scorer = make_scorer(custom_score_func)\n    assert_equal(scorer.__class__, _PredictScorer)\n    assert_equal(scorer._score_func, custom_score_func)\n    assert_equal(scorer._sign, 1)\n    assert_equal(scorer._kwargs, {})\n\n    scorer = make_scorer(custom_score_func, greater_is_better=False)\n    assert_equal(scorer._sign, -1)\n\n    scorer = make_scorer(custom_score_func, needs_proba=True)\n    assert_equal(scorer.__class__, _ProbaScorer)\n\n    scorer = make_scorer(custom_score_func, needs_threshold=True)\n    assert_equal(scorer.__class__, _ThresholdScorer)\n\n    with assert_raises(ValueError):\n        make_scorer(custom_score_func, needs_proba=True, needs_threshold=True)\n\n    scorer = make_scorer(custom_score_func, arg1='foo', arg2=12)\n    assert_equal(scorer._kwargs, {'arg1': 'foo', 'arg2': 12})\n"], "sample_657": ["def test_parameter_set_with_unknown_kwargs(self, attr, modulename):\n    with pytest.warns(PytestDeprecationWarning) as record:\n        ParameterSet.param(1, 2, unknown_arg=3)\n    assert len(record) == 1\n    assert \"PYTEST_PARAM_UNKNOWN_KWARGS\" in str(record[0].message)\n", "def test_markdecorator_attributes(self, attr):\n    mark_decorator = Mark.MARK_GEN.test_mark\n    assert getattr(mark_decorator, attr) == getattr(mark_decorator.mark, attr)\n", "def test_mark_equality(self, modulename, attr):\n    mark_generator = Mark()\n    mark1 = getattr(mark_generator, \"test_mark\")\n    mark2 = getattr(mark_generator, \"test_mark\")\n    mark3 = getattr(mark_generator, \"different_mark\")\n\n    assert mark1 == mark2\n    assert mark1 != mark3\n", "    def test_get_empty_parameterset_mark(self, config, argnames, func):\n        config.setini(EMPTY_PARAMETERSET_OPTION, \"\")\n        mark = get_empty_parameterset_mark(config, argnames, func)\n        assert isinstance(mark, MARK_GEN.skip)\n\n        config.setini(EMPTY_PARAMETERSET_OPTION, \"skip\")\n        mark = get_empty_parameterset_mark(config, argnames, func)\n        assert isinstance(mark, MARK_GEN.skip)\n\n        config.setini(EMPTY_PARAMETERSET_OPTION, \"xfail\")\n        mark = get_empty_parameterset_mark(config, argnames, func)\n        assert isinstance(mark, MARK_GEN.xfail)\n\n        config.setini(EMPTY_PARAMETERSET_OPTION, \"fail_at_collect\")\n        with pytest.raises(Collector.CollectError):\n            get_empty_parameterset_mark(config, argnames, func)\n\n        config.setini(EMPTY_PARAMETERSET_OPTION, \"invalid_option\")\n        with pytest.raises(LookupError):\n            get_empty_parameterset_mark(config, argnames, func)\n", "    def test_import_module(self, attr, modulename):\n        with mock.patch.dict(sys.modules, {'py.test': None, 'pytest': None}):\n            with pytest.raises(ImportError):\n                __import__(modulename).mark.__getattr__(attr)\n"], "sample_346": ["def test_method_decorator(self):\n        @wraps(view_func)\n            return HttpResponse(\"Decorated response\")\n        return _wrapped_view\n\n    class TestView:\n        @method_decorator(test_decorator)\n            return HttpResponse(\"Original response\")\n\n    request = HttpRequest()\n    test_view = TestView()\n    response = test_view.test_method(request)\n    self.assertEqual(response.content, b\"Decorated response\")\n", "    def test_middleware_decorator(self):\n        middleware_class = XFrameOptionsMiddleware\n        decorator = decorator_from_middleware(middleware_class)\n\n        @decorator\n            return HttpResponse()\n\n        request = HttpRequest()\n        response = dummy_view(request)\n\n        self.assertEqual(response.get('X-Frame-Options'), 'DENY')\n", "    def test_middleware_decorator(self):\n        @decorator_from_middleware(XFrameOptionsMiddleware)\n            return HttpResponse()\n\n        request = HttpRequest()\n        response = view_func(request)\n        self.assertEqual(response.get('X-Frame-Options'), 'DENY')\n", "    def test_method_decorator(self):\n        @method_decorator(full_decorator)\n            return HttpResponse('<html><body>method</body></html>')\n\n        class View:\n            @method_decorator(full_decorator, name='method')\n                return HttpResponse('<html><body>class method</body></html>')\n\n        # Test method decorator\n        request = HttpRequest()\n        response = method(View(), request)\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.content, b'<html><body>method</body></html>')\n\n        # Test class method decorator\n        view = View()\n        response = view.method(request)\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.content, b'<html><body>class method</body></html>')\n", "    def test_middleware_decorator(self):\n        # Test the decorator_from_middleware function\n        @decorator_from_middleware(XFrameOptionsMiddleware)\n            return HttpResponse()\n\n        # Test the process_request method of the middleware\n        request = HttpRequest()\n        response = test_view(request)\n        self.assertEqual(response.get('X-Frame-Options'), 'DENY')\n\n        # Test the process_view method of the middleware\n        response = test_view(request)\n        self.assertEqual(response.get('X-Frame-Options'), 'DENY')\n\n        # Test the process_response method of the middleware\n        response = test_view(request)\n        self.assertEqual(response.get('X-Frame-Options'), 'DENY')\n"], "sample_922": ["def test_parse_annotation():\n    annotation = 'List[int]'\n    result = _parse_annotation(annotation)\n    expected_result = [pending_xref('', nodes.Text('List'),\n                                    refdomain='py', reftype='class', reftarget='List'),\n                       addnodes.desc_sig_punctuation('', '['),\n                       pending_xref('', nodes.Text('int'),\n                                    refdomain='py', reftype='class', reftarget='int'),\n                       addnodes.desc_sig_punctuation('', ']')]\n    assert result == expected_result\n", "def test_parse_annotation(app):\n    # Test parsing of type annotation\n    annotation = \"List[Union[str, int]]\"\n    result = _parse_annotation(annotation)\n    expected = [\n        pending_xref('', nodes.Text('List'), refdomain='py', reftype='class', reftarget='List'),\n        desc_sig_punctuation('', '['),\n        pending_xref('', nodes.Text('Union'), refdomain='py', reftype='class', reftarget='Union'),\n        desc_sig_punctuation('', '['),\n        pending_xref('', nodes.Text('str'), refdomain='py', reftype='class', reftarget='str'),\n        desc_sig_punctuation('', ', '),\n        pending_xref('', nodes.Text('int'), refdomain='py', reftype='class', reftarget='int'),\n        desc_sig_punctuation('', ']'),\n        desc_sig_punctuation('', ']'),\n    ]\n    assert result == expected\n", "def test_parse_annotation():\n    annotation = \"List[Union[int, float]]\"\n    result = _parse_annotation(annotation)\n    assert isinstance(result, list)\n    assert len(result) == 9\n    assert isinstance(result[0], pending_xref)\n    assert result[0]['reftype'] == 'class'\n    assert result[0]['reftarget'] == 'List'\n    assert isinstance(result[1], nodes.Text)\n    assert result[1] == nodes.Text('[')\n    assert isinstance(result[2], pending_xref)\n    assert result[2]['reftype'] == 'class'\n    assert result[2]['reftarget'] == 'Union'\n    assert isinstance(result[3], nodes.Text)\n    assert result[3] == nodes.Text('[')\n    assert isinstance(result[4], pending_xref)\n    assert result[4]['reftype'] == 'obj'\n    assert result[4]['reftarget'] == 'int'\n    assert isinstance(result[5], nodes.Text)\n    assert result[5] == nodes.Text(', ')\n    assert isinstance(result[6], pending_xref)\n    assert result[6]['reftype'] == 'obj'\n    assert result[6]['reftarget'] == 'float'\n    assert isinstance(result[7], nodes.Text)\n    assert result[7] == nodes.Text(']')\n    assert isinstance(result[8], nodes.Text)\n    assert result[8] == nodes.Text(']')\n", "def test_parse_annotation():\n    result = _parse_annotation('List[int]')\n    assert isinstance(result, list)\n    assert len(result) == 5\n    assert isinstance(result[0], pending_xref)\n    assert result[0]['reftarget'] == 'List'\n    assert isinstance(result[1], addnodes.desc_sig_punctuation)\n    assert result[1].astext() == '['\n    assert isinstance(result[2], pending_xref)\n    assert result[2]['reftarget'] == 'int'\n    assert isinstance(result[3], addnodes.desc_sig_punctuation)\n    assert result[3].astext() == ']'\n", "def test_parse_annotation():\n    annotation = \"List[Optional[int]]\"\n    result = _parse_annotation(annotation)\n    expected = [\n        pending_xref('', nodes.Text('List'), refdomain='py', reftype='class', reftarget='List'),\n        desc_sig_punctuation('', '['),\n        pending_xref('', nodes.Text('Optional'), refdomain='py', reftype='class', reftarget='Optional'),\n        desc_sig_punctuation('', '['),\n        nodes.Text('int'),\n        desc_sig_punctuation('', ']'),\n        desc_sig_punctuation('', ']')\n    ]\n    assert_node(result, expected)\n"], "sample_314": ["    def test_authentication_success(self):\n        form = AuthenticationForm(data={'username': 'testclient', 'password': 'password'})\n        self.assertTrue(form.is_valid())\n        self.assertEqual(form.get_user(), self.u1)\n", "    def test_password_help_text(self):\n        form = UserChangeForm(instance=self.u1)\n        self.assertIn('../password/', form.fields['password'].help_text)\n", "    def test_password_help_text_is_rendered(self):\n        form = UserChangeForm(instance=self.u1)\n        self.assertIn('../password/', form.as_p())\n", "    def test_password_help_text(self):\n        form = UserChangeForm(instance=self.u1)\n        self.assertIn('../password/', form.fields['password'].help_text)\n", "    def test_clean_password2(self):\n        form = UserCreationForm(data={'username': 'newuser', 'password1': 'testpassword', 'password2': 'testpassword'})\n        self.assertEqual(form.clean_password2(), 'testpassword')\n\n        form = UserCreationForm(data={'username': 'newuser', 'password1': 'testpassword', 'password2': 'differentpassword'})\n        with self.assertRaises(ValidationError):\n            form.clean_password2()\n"], "sample_656": ["def test_suspend_resume_global_capture_with_no_capture(self):\n    config = pytest.config._prepareconfig([])\n    config.option.capture = \"no\"\n    capman = CaptureManager(config.option.capture)\n    capman.suspend_global_capture()\n    capman.resume_global_capture()\n    assert capman._global_capturing is None\n", "    def test_resume_fixture(self, capsys):\n        item = pytest.Item(\"test_resume_fixture\", parent=None)\n        item._capture_fixture = capsys\n        capmanager = CaptureManager(\"fd\")\n        capmanager.resume_fixture(item)\n        # Add assertions to verify the expected behavior, such as checking if the fixture's capturing is resumed\n", "    def test_resume_global_capture_when_none(self, capmanager):\n        capmanager._global_capturing = None\n        capmanager.resume_global_capture()\n        assert capmanager._global_capturing is None\n", "    def test_suspend_resume(self, capmanager):\n        capmanager.start_global_capturing()\n        print(\"Before suspension\")\n        capmanager.suspend_global_capture()\n        print(\"During suspension\")\n        capmanager.resume_global_capture()\n        print(\"After resumption\")\n        out, err = capmanager.read_global_capture()\n        assert \"Before suspension\" in out\n        assert \"During suspension\" not in out\n        assert \"After resumption\" in out\n", "def test_activate_deactivate_fixture(testdir):\n    \"\"\"Test activating and deactivating the capture fixture.\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n            print('Hello, World!')\n            captured = capsys.readouterr()\n            assert captured.out == 'Hello, World!\\\\n'\n        \"\"\"\n    )\n    result = testdir.runpytest(\"-s\")\n    assert \"Hello, World!\" in result.stdout.str()\n    assert result.ret == 0\n"], "sample_453": ["def test_prepopulated_fields_js_tag(self):\n    request = self.request_factory.get('/admin/')\n    request.user = self.user\n\n    # Create a mock context\n    context = {\n        \"adminform\": ModelAdmin(Article, site).get_form(request)(instance=Article()),\n        \"inline_admin_formsets\": [],\n    }\n\n    # Add a prepopulated field to the context\n    context[\"adminform\"].prepopulated_fields = [\n        {\n            \"field\": context[\"adminform\"].base_fields[\"title\"],\n            \"dependencies\": [context[\"adminform\"].base_fields[\"slug\"]],\n        }\n    ]\n\n    # Create a mock parser and token\n    parser = None\n    token = None\n\n    # Call the prepopulated_fields_js_tag function\n    node = prepopulated_fields_js_tag(parser, token)\n\n    # Render the node with the context\n    rendered = node.render(context)\n\n    # Assert that the rendered output contains the prepopulated field data\n    self.assertIn('\"name\": \"title\"', rendered)\n    self.assertIn('\"dependency_list\": [\"slug\"]', rendered)\n", "def test_prepopulated_fields_js(self):\n    \"\"\"Test the prepopulated_fields_js function.\"\"\"\n    # Create a mock context\n    context = {\n        \"adminform\": ModelAdmin(Article, site).get_form(None)(instance=Article()),\n        \"inline_admin_formsets\": [\n            [\n                ModelAdmin(Question, site).get_formset(self.request)(\n                    instance=Article()\n                ).form(instance=Question())\n            ]\n        ],\n    }\n\n    # Call the prepopulated_fields_js function\n    result = prepopulated_fields_js(context)\n\n    # Assert that the prepopulated_fields_json key is in the result\n    self.assertIn(\"prepopulated_fields_json\", result)\n\n    # Assert that the prepopulated_fields key is in the result\n    self.assertIn(\"prepopulated_fields\", result)\n\n    # Assert that the prepopulated_fields_json value is a string\n    self.assertIsInstance(result[\"prepopulated_fields_json\"], str)\n\n    # Assert that the prepopulated_fields value is a list\n    self.assertIsInstance(result[\"prepopulated_fields\"], list)\n", "def test_prepopulated_fields_js(self):\n    \"\"\"Test the prepopulated_fields_js template tag.\"\"\"\n    modeladmin = ArticleAdmin(Article, site)\n    request = self.request_factory.get('/')\n    request.user = self.superuser\n    context = {\n        'adminform': modeladmin.get_form(request)(instance=self.article),\n    }\n    prepopulated_fields = prepopulated_fields_js(context)['prepopulated_fields_json']\n    self.assertIn('title', prepopulated_fields)\n    self.assertIn('slug', prepopulated_fields)\n", "def test_prepopulated_fields_js(self):\n    \"\"\"\n    Test that the prepopulated_fields_js template tag correctly generates the JSON data.\n    \"\"\"\n    admin = ArticleAdmin(Article, site)\n    request = self.request_factory.get('/admin/blog/article/add/')\n    request.user = self.user\n    context = {\n        'adminform': admin.get_form(request)(),\n        'inline_admin_formsets': [],\n    }\n    context = prepopulated_fields_js(context)\n    self.assertEqual(len(context['prepopulated_fields_json']), 0)  # No prepopulated fields in Article model\n\n    # Create an inline formset with a prepopulated field\n    inline = admin.get_inline_instances(request)[0]\n    inline_formset = inline.get_formset(request)(instance=self.article)\n    context['inline_admin_formsets'] = [inline_formset]\n    context = prepopulated_fields_js(context)\n    self.assertEqual(len(context['prepopulated_fields_json']), 1)  # One prepopulated field in Question model\n", "def test_prepopulated_fields_js(self):\n    \"\"\"Test prepopulated_fields_js template tag.\"\"\"\n    modeladmin = ArticleAdmin(Article, site)\n    prepopulated_fields = {\"field\": {\"auto_id\": \"field1\", \"name\": \"name\", \"field\": {\"max_length\": 20}},\n                           \"dependencies\": [{\"auto_id\": \"dep1\", \"name\": \"dep_name\"}]}\n    modeladmin.prepopulated_fields = {\"name\": (\"dep_name\",)}\n    request = self.request_factory.get(\"/\")\n    request.user = self.get_superuser()\n    obj = Article.objects.create(title=\"Test Article\", content=\"Test Content\")\n    context = modeladmin.admin_site.each_context(request)\n    context.update(modeladmin.admin_site.each_context(request))\n    context['adminform'] = modeladmin.get_form(request)()\n    context['adminform'].prepopulated_fields = [prepopulated_fields]\n    context = prepopulated_fields_js(context)\n    self.assertIn('prepopulated_fields', context)\n    self.assertIn('prepopulated_fields_json', context)\n    prepopulated_fields_json = json.loads(context['prepopulated_fields_json'])\n    self.assertEqual(prepopulated_fields_json[0]['id'], '#field1')\n    self.assertEqual(prepopulated_fields_json[0]['dependency_ids'], ['#dep1'])\n"], "sample_171": ["    def test_migration_plan(self):\n        \"\"\"\n        Tests that the migration plan is correctly generated and displayed\n        when the --plan option is used.\n        \"\"\"\n        out = io.StringIO()\n        call_command('migrate', '--plan', '--verbosity=0', stdout=out)\n        self.assertIn('Planned operations:', out.getvalue())\n        self.assertIn('[MIGRATE]  0001_initial', out.getvalue())\n        self.assertIn('    Create model UnicodeModel', out.getvalue())\n", "    def test_migration_progress_callback(self):\n        \"\"\"\n        Test the migration_progress_callback method.\n        \"\"\"\n        out = io.StringIO()\n        command = MigrateCommand()\n        command.stdout = out\n        command.verbosity = 1\n\n        command.migration_progress_callback(\"apply_start\", \"test_migration\", fake=False)\n        self.assertIn(\"Applying test_migration...\", out.getvalue())\n\n        out.seek(0)\n        out.truncate(0)\n\n        command.migration_progress_callback(\"apply_success\", \"test_migration\", fake=False)\n        self.assertIn(\"OK\", out.getvalue())\n\n        out.seek(0)\n        out.truncate(0)\n\n        command.migration_progress_callback(\"unapply_start\", \"test_migration\", fake=False)\n        self.assertIn(\"Unapplying test_migration...\", out.getvalue())\n\n        out.seek(0)\n        out.truncate(0)\n\n        command.migration_progress_callback(\"unapply_success\", \"test_migration\", fake=False)\n        self.assertIn(\"OK\", out.getvalue())\n", "def test_syncdb_with_unmigrated_apps(self, mock_migrations_check):\n    mock_migrations_check.return_value = None\n    with self.assertRaises(CommandError):\n        call_command('migrate', 'unmigrated', run_syncdb=True)\n    self.assertEqual(mock_migrations_check.call_count, 1)\n", "    def test_migration_plan_option(self, stdout):\n        out = io.StringIO()\n        call_command('migrate', 'migrations', plan=True, stdout=out)\n        command_output = out.getvalue()\n        self.assertIn('Planned operations:', command_output)\n        self.assertIn('0001_initial.py -> Create model UnicodeModel', command_output)\n        self.assertNotIn('Applying 0001_initial...', command_output)\n", "    def test_migrate_command_with_fake_initial(self, mock_check):\n        \"\"\"Test the migrate command with the --fake-initial option.\"\"\"\n        self.write_migration(\n            self.initial_migration_name(\"migrations\"),\n            \"\"\"\n            from django.db import migrations, models\n\n            class Migration(migrations.Migration):\n\n                operations = [\n                    migrations.CreateModel(\n                        name='UnicodeModel',\n                        fields=[\n                            ('id', models.AutoField(verbose_name='ID', primary_key=True, auto_created=True)),\n                            ('title', models.CharField(max_length=100)),\n                        ],\n                    ),\n                ]\n            \"\"\",\n        )\n        UnicodeModel.objects.create(title=\"Test\")\n        out = io.StringIO()\n        call_command('migrate', 'migrations', 'zero', fake_initial=True, verbosity=1, stdout=out)\n        self.assertEqual(UnicodeModel.objects.count(), 1)\n        self.assertIn(\"  Unapply all migrations: migrations\", out.getvalue())\n        self.assertIn(\"  Fake applying initial migrations\", out.getvalue())\n"], "sample_1208": ["def test_matrix_student_t_distribution_sample():\n    nu = symbols('nu', positive=True)\n    M = MatrixStudentT('M', nu, [[1, 2]], [[1, 0], [0, 1]], [1])\n    samples = sample(M, size=100, library='scipy')\n    assert samples.shape == (100, 1, 2)\n", "def test_matrix_student_t_sampling():\n    nu, n, p = symbols('nu n p', positive=True)\n    M = MatrixSymbol('M', n, p)\n    Omega = MatrixSymbol('Omega', p, p)\n    Sigma = MatrixSymbol('Sigma', n, n)\n    T = MatrixStudentT('T', nu, M, Omega, Sigma)\n    samples = sample(T)\n    assert isinstance(samples, dict)\n    assert T in samples\n    assert isinstance(samples[T], Matrix)\n    assert samples[T].shape == (n, p)\n", "def test_matrix_student_t_sample():\n    if not import_module('scipy'):\n        skip(\"SciPy is not installed.\")\n    v = symbols('v', positive=True)\n    M = MatrixStudentT('M', v, [[1, 2]], [[1, 0], [0, 1]], [1])\n    sample_size = (10,)\n    samples = sample(M, size=sample_size, library='scipy')\n    assert samples.shape == sample_size + M.pspace.dimension\n", "def test_MatrixStudentT_distribution():\n    v, n, p = symbols('v n p', positive=True)\n    M = MatrixStudentT('M', v, MatrixSymbol('A', n, p), MatrixSymbol('B', p, p), MatrixSymbol('C', n, n))\n    X = MatrixSymbol('X', n, p)\n    pdf_expr = density(M)(X)\n    assert isinstance(pdf_expr, exp)\n    assert isinstance(pdf_expr.args[0], gamma)\n    assert pdf_expr.args[0].args[0] == (v + n + p - 1)/2\n    assert isinstance(pdf_expr.args[1], Determinant)\n    assert pdf_expr.args[1].args[0].shape == (n, n)\n", "def test_matrix_student_t_distribution():\n    v, n, p = symbols('v n p', positive=True)\n    M = MatrixStudentT('M', v, MatrixSymbol('\u03bc', n, p), MatrixSymbol('\u03a31', p, p), MatrixSymbol('\u03a32', n, n))\n    X = MatrixSymbol('X', n, p)\n    assert isinstance(M.pspace, MatrixPSpace)\n    assert M.pspace.set == MatrixSet(n, p, S.Reals)\n    assert M.pspace.distribution == M.distribution\n    assert M.distribution.set == MatrixSet(n, p, S.Reals)\n    assert M.distribution.dimension == (n, p)\n\n    pdf = M.distribution.pdf(X)\n    assert pdf.has(Determinant) and pdf.has(Trace)\n    assert pdf.has(gamma((v + n + p - 1)/2, p)) and pdf.has(gamma((v + p - 1)/2, p))\n\n    # Test the pdf for specific values\n    M_specific = MatrixStudentT('M', 2, [[1, 2]], [[1, 0], [0, 1]], [1])\n    X_specific = [[3, 4]]\n    pdf_specific = M_specific.distribution.pdf(X_specific)\n    assert pdf_specific.is_real and pdf_specific > 0\n"], "sample_1164": ["def test_cg_simp():\n    a = CG(1, 1, 0, 0, 1, 1)\n    b = CG(1, 0, 0, 0, 1, 0)\n    c = CG(1, -1, 0, 0, 1, -1)\n    assert cg_simp(a + b + c) == 3\n", "def test_wigner3j_doit():\n    w3j = Wigner3j(6, 0, 4, 0, 2, 0)\n    assert w3j.doit() == sqrt(715)/143\n", "def test_Wigner3j_evaluation():\n    w3j = Wigner3j(6, 0, 4, 0, 2, 0)\n    assert w3j.doit() == sqrt(715) / 143\n", "def test_cg_simp():\n    a, b, c, alpha, beta = symbols('a b c alpha beta')\n    cg1 = CG(a, alpha, b, beta, c, 0)\n    cg2 = CG(a, alpha, b, beta, c, 0)\n    result = cg_simp(cg1 * cg2)\n    expected = sqrt(2*a + 1)*(2*b + 1)*(2*c + 1)*KroneckerDelta(a, b)*KroneckerDelta(b, c)\n    assert result == expected\n", "def test_wigner_3j():\n    w3j = Wigner3j(6,0,4,0,2,0)\n    assert w3j.doit() == sqrt(715)/143\n"], "sample_1122": ["def test_re_im_functions():\n    x, y = symbols('x y', real=True)\n    assert re(2*E) == 2*E\n    assert im(2*E) == 0\n    assert re(2*I + 17) == 17\n    assert im(2*I) == 0\n    assert re(im(x) + x*I + 2) == 2\n    assert im(re(x) + y) == im(y)\n", "def test_Abs_derivative():\n    x, y = symbols('x y', real=True)\n    z = x + I*y\n    assert Abs(z)._eval_derivative(x) == Derivative(x, x)*sign(x) + Derivative(y, x)*sign(y)\n", "def test_polar_lift_eval():\n    assert polar_lift(4) == 4*exp_polar(0)\n    assert polar_lift(-4) == 4*exp_polar(I*pi)\n    assert polar_lift(-I) == exp_polar(-I*pi/2)\n    assert polar_lift(I + 2) == polar_lift(2 + I)\n    x = Symbol('x')\n    assert polar_lift(4*x) == 4*polar_lift(x)\n    p = Symbol('p', polar=True)\n    assert polar_lift(4*p) == 4*p\n", "def test_arg_derivative():\n    x, y, t = symbols('x y t', real=True)\n    z = x + I*y\n    dz_dt = Derivative(z, t)\n    assert arg(z)._eval_derivative(t) == (im(dz_dt)*Derivative(x, t) - re(dz_dt)*Derivative(y, t)) / (x**2 + y**2)\n", "def test_Abs():\n    assert Abs(exp(I*pi)) == 1\n    assert Abs(exp(I*pi/2)) == 1\n    assert Abs(exp(I*pi/4)) == sqrt(2)/2\n"], "sample_78": ["def test_dance_command(self):\n    out = StringIO()\n    management.call_command('dance', stdout=out)\n    self.assertEqual(out.getvalue().strip(), \"Let's dance!\")\n", "def test_check_migrations(self):\n    with mock.patch('django.db.migrations.executor.MigrationExecutor') as mock_executor:\n        mock_executor.return_value.migration_plan.return_value = [('migration1', False), ('migration2', False)]\n        command = BaseCommand()\n        command.stdout = StringIO()\n        command.check_migrations()\n        self.assertIn(\"You have 2 unapplied migration(s).\", command.stdout.getvalue())\n", "def test_dance_command_handle(self):\n    with mock.patch.object(dance.Command, 'handle') as mock_handle:\n        call_command('dance')\n        mock_handle.assert_called_once()\n", "    def test_dance_command(self):\n        out = StringIO()\n        command = dance.Command(stdout=out)\n        command.handle()\n        self.assertIn('Starting the dance...', out.getvalue())\n", "def test_dance_command_run_from_argv(self, mock_execute):\n    command = dance.Command()\n    command.run_from_argv(['manage.py', 'dance'])\n    mock_execute.assert_called_once()\n"], "sample_882": ["def test_mlp_classifier_incremental_fit_classes_mismatch():\n    X, y = classification_datasets[0]\n    clf = MLPClassifier(warm_start=True)\n    clf.partial_fit(X[:100], y[:100], classes=[0, 1])\n    with pytest.raises(ValueError, match=\"warm_start can only be used where `y` has the same classes as in the previous call to fit.\"):\n        clf.partial_fit(X[100:], y[100:], classes=[0, 1, 2])\n", "def test_mlp_classifier_activation_functions(activation):\n    mlp = MLPClassifier(activation=activation, random_state=0)\n    mlp.fit(X_digits_multi, y_digits_multi)\n    y_pred = mlp.predict(X_digits_multi)\n    assert y_pred.shape == y_digits_multi.shape\n", "def test_mlp_classifier_activation_functions(activation):\n    clf = MLPClassifier(activation=activation, max_iter=500, random_state=42)\n    clf.fit(X_iris, y_iris)\n    assert clf.activation == activation\n    y_pred = clf.predict(X_iris)\n    assert len(y_pred) == len(y_iris)\n    assert set(y_pred) == set(y_iris)\n", "def test_mlp_classifier_activation_functions(activation):\n    clf = MLPClassifier(hidden_layer_sizes=(10,), activation=activation, max_iter=10, random_state=42)\n    clf.fit(X_digits_binary, y_digits_binary)\n    assert clf.score(X_digits_binary, y_digits_binary) >= 0.0, f\"Test failed for activation function: {activation}\"\n", "def test_mlp_classifier_multilabel_consistency(activation):\n    X, y = make_multilabel_classification(n_samples=100, n_features=20, n_labels=3, random_state=42)\n    clf = MLPClassifier(hidden_layer_sizes=(50,), activation=activation, random_state=42)\n    clf.fit(X, y)\n    y_pred_proba = clf.predict_proba(X)\n    y_pred = clf.predict(X)\n    # Check that predict_proba and predict are consistent\n    assert_array_equal(np.argmax(y_pred_proba, axis=1), y_pred)\n"], "sample_347": ["    def test_activate(self):\n        timezone.deactivate()\n        self.assertEqual(timezone.get_current_timezone(), timezone.get_default_timezone())\n        timezone.activate(\"Europe/Paris\")\n        self.assertEqual(timezone.get_current_timezone(), CET)\n        timezone.deactivate()\n        self.assertEqual(timezone.get_current_timezone(), timezone.get_default_timezone())\n", "    def test_localtime(self):\n        dt = datetime.datetime(2022, 1, 1, 12, 0, 0, tzinfo=UTC)\n        with timezone.override(CET):\n            local_dt = timezone.localtime(dt)\n            self.assertEqual(local_dt.tzinfo, CET)\n            self.assertEqual(local_dt.hour, 13)\n\n        with timezone.override(EAT):\n            local_dt = timezone.localtime(dt)\n            self.assertEqual(local_dt.tzinfo, EAT)\n            self.assertEqual(local_dt.hour, 15)\n\n        with timezone.override(ICT):\n            local_dt = timezone.localtime(dt)\n            self.assertEqual(local_dt.tzinfo, ICT)\n            self.assertEqual(local_dt.hour, 19)\n", "def test_make_naive(self):\n    dt = datetime.datetime(2022, 1, 1, 12, 0, 0, tzinfo=CET)\n    naive_dt = timezone.make_naive(dt)\n    self.assertEqual(naive_dt, datetime.datetime(2022, 1, 1, 11, 0, 0))\n", "def test_localtime_with_explicit_timezone(self):\n    dt = datetime.datetime(2022, 1, 1, 12, 0, 0, tzinfo=UTC)\n    local_dt = timezone.localtime(dt, ICT)\n    self.assertEqual(local_dt.hour, 19)\n", "    def test_activate_deactivate(self):\n        self.assertEqual(timezone.get_current_timezone(), timezone.utc)\n        timezone.activate('Europe/Paris')\n        self.assertEqual(timezone.get_current_timezone(), CET)\n        timezone.deactivate()\n        self.assertEqual(timezone.get_current_timezone(), timezone.utc)\n"], "sample_397": ["def test_template_context_processors(self):\n    engine = self.engine_class()\n    context_processors = engine.template_context_processors\n    self.assertIn(test_processor_name, [cp.__name__ for cp in context_processors])\n", "def test_get_template_loaders(self):\n    engine = self.engine_class()\n    loaders = [\"django.template.loaders.filesystem.Loader\"]\n    template_loaders = engine.get_template_loaders(loaders)\n    self.assertEqual(len(template_loaders), 1)\n    self.assertIsInstance(template_loaders[0], Loader)\n", "def test_get_template_builtins(self):\n    engine = self.engine_class(builtins=[\"django.template.defaulttags\"])\n    builtins = engine.get_template_builtins(engine.builtins)\n    self.assertEqual(len(builtins), 1)\n    self.assertTrue(\"register\" in dir(builtins[0]))\n\n    # Test invalid template library\n    with self.assertRaises(InvalidTemplateLibrary):\n        engine.get_template_builtins([\"invalid.library\"])\n", "    def test_engine_init_defaults(self):\n        engine = Engine()\n        self.assertEqual(engine.dirs, [])\n        self.assertFalse(engine.app_dirs)\n        self.assertTrue(engine.autoescape)\n        self.assertEqual(engine.context_processors, [])\n        self.assertFalse(engine.debug)\n        self.assertEqual(engine.loaders, [(\"django.template.loaders.cached.Loader\", [\"django.template.loaders.filesystem.Loader\"])])\n        self.assertEqual(engine.string_if_invalid, \"\")\n        self.assertEqual(engine.file_charset, \"utf-8\")\n        self.assertEqual(engine.libraries, {})\n        self.assertEqual(engine.template_libraries, {})\n        self.assertEqual(engine.builtins, Engine.default_builtins)\n        self.assertIsInstance(engine.template_builtins, list)\n        self.assertIsInstance(engine.template_loaders, list)\n", "def test_template_builtins(self):\n    \"\"\"\n    Test that template builtins are loaded correctly.\n    \"\"\"\n    custom_builtins = [\"template_tests.custom_tags\"]\n    engine = self.engine_class(builtins=custom_builtins)\n\n    # Check if custom_tag from custom_tags is in template_builtins\n    assert any(\"custom_tag\" in dir(library) for library in engine.template_builtins)\n"], "sample_390": ["    def test_serve_file(self):\n        url = f\"/{self.prefix}/example.txt\"\n        file_path = path.join(media_dir, \"example.txt\")\n        with open(file_path, \"w\") as f:\n            f.write(\"test content\")\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.content, b\"test content\")\n        self.assertEqual(response[\"Content-Type\"], \"text/plain\")\n", "    def test_serve_with_directory_index(self):\n        \"\"\"Test serving a directory index\"\"\"\n        response = self.client.get('/' + self.prefix + '/')\n        self.assertEqual(response.status_code, 200)\n        self.assertIn(b'Index of /site_media/', response.content)\n        self.assertIn(b'<a href=\"test_file.txt\">test_file.txt</a>', response.content)\n        self.assertIn(b'<a href=\"test_dir/\">test_dir/</a>', response.content)\n", "    def test_serve(self):\n        url = \"/site_media/test.txt\"\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.get('Content-Type'), 'text/plain')\n        self.assertEqual(response['Last-Modified'], http_date(path.getmtime(path.join(media_dir, 'test.txt'))))\n", "def test_serve_file(self):\n    file_path = 'test.txt'\n    url = '/'.join([self.prefix, file_path])\n    response = self.client.get(url)\n\n    # Check the response status code\n    self.assertEqual(response.status_code, 200)\n\n    # Check the response content type\n    content_type, encoding = mimetypes.guess_type(file_path)\n    self.assertEqual(response.get('Content-Type'), content_type)\n\n    # Check the response content\n    with open(path.join(media_dir, file_path), 'rb') as f:\n        self.assertEqual(response.content, f.read())\n\n    # Check the Last-Modified header\n    statobj = path.stat(path.join(media_dir, file_path))\n    self.assertEqual(response.get('Last-Modified'), http_date(statobj.st_mtime))\n", "    def test_serve_file(self):\n        url = f\"/{self.prefix}/example.txt\"\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response['Content-Type'], 'text/plain')\n        self.assertEqual(response.content, b'This is a test file.')\n"], "sample_386": ["    def test_mark_safe_with_custom_safe_data(self):\n        s = customescape(\"<script>alert('unsafe')</script>\")\n        safe_s = mark_safe(s)\n        self.assertIsInstance(safe_s, SafeString)\n        self.assertEqual(safe_s, \"<script>alert('unsafe')</script>\")\n        self.assertRenderEqual(\"{{ s|safe }}\", \"<script>alert('unsafe')</script>\", s=s)\n        self.assertRenderEqual(\"{{ safe_s|safe }}\", \"<script>alert('unsafe')</script>\", safe_s=safe_s)\n", "def test_safe_string_operations(self):\n    # Test safe string concatenation\n    safe_str1 = mark_safe(\"<b>Hello</b>\")\n    safe_str2 = mark_safe(\" <i>World</i>\")\n    result = safe_str1 + safe_str2\n    self.assertEqual(result, \"<b>Hello</b> <i>World</i>\")\n    self.assertIsInstance(result, SafeString)\n\n    # Test safe string with non-safe string\n    safe_str = mark_safe(\"<b>Hello</b>\")\n    non_safe_str = \" <i>World</i>\"\n    result = safe_str + non_safe_str\n    self.assertEqual(result, \"<b>Hello</b> <i>World</i>\")\n    self.assertNotIsInstance(result, SafeString)\n\n    # Test safe string with custom escape method\n    custom_safe_str = customescape(\"<b>Hello</b>\")\n    safe_str = mark_safe(\" <i>World</i>\")\n    result = custom_safe_str + safe_str\n    self.assertEqual(result, \"<<b>>Hello<</b>> <i>World</i>\")\n    self.assertNotIsInstance(result, SafeString)\n", "    def test_mark_safe_decorator(self):\n        @mark_safe\n            return \"<p>This is a safe string.</p>\"\n\n        self.assertIsInstance(decorated_safe_string(), SafeString)\n        self.assertEqual(str(decorated_safe_string()), \"<p>This is a safe string.</p>\")\n", "def test_mark_safe_with_safe_data(self):\n    safe_data = customescape(\"<test>\")\n    marked_safe = mark_safe(safe_data)\n    self.assertEqual(marked_safe, safe_data)\n    self.assertIsInstance(marked_safe, SafeData)\n    self.assertIsInstance(marked_safe, SafeString)\n", "    def test_mark_safe_with_customescape(self):\n        test_str = customescape(\"<script>alert('test');</script>\")\n        safe_str = mark_safe(test_str)\n        self.assertIsInstance(safe_str, SafeString)\n        self.assertEqual(safe_str, \"<script>alert('test');</script>\")\n        self.assertEqual(str(safe_str), \"<script>alert('test');</script>\")\n        self.assertEqual(html.escape(safe_str), \"&lt;script&gt;alert(&#x27;test&#x27;);&lt;/script&gt;\")\n        self.assertRenderEqual(\"{{ var|escape }}\", \"&lt;script&gt;alert(&#x27;test&#x27;);&lt;/script&gt;\", var=safe_str)\n"], "sample_119": ["    def test_add_filtered_relation(self):\n        query = Query(Author)\n        filtered_relation = Author.objects.filter(name__contains='Test')\n        query.add_filtered_relation(filtered_relation, 'filtered_authors')\n        self.assertEqual(query._filtered_relations['filtered_authors'].relation_name, 'filtered_authors')\n        self.assertEqual(str(query._filtered_relations['filtered_authors'].condition), \"name__contains = 'Test'\")\n", "def test_filter_with_related_isnull(self):\n    query = Query(Author)\n    query.add_filter(('books__isnull', True))\n    self.assertEqual(\n        str(query),\n        'SELECT \"myapp_author\".\"id\", \"myapp_author\".\"name\" FROM \"myapp_author\" '\n        'WHERE \"myapp_author\".\"id\" NOT IN (SELECT U0.\"author_id\" FROM \"myapp_book\" U0 '\n        'WHERE U0.\"author_id\" IS NOT NULL)'\n    )\n", "def test_add_filter_with_exact_lookup(self):\n    query = Query(Author)\n    query.add_filter(('name__exact', 'Author Name'))\n    self.assertEqual(query.where.children[0].children[0].__class__, Exact)\n    self.assertEqual(query.where.children[0].children[0].rhs, 'Author Name')\n", "def test_query_add_filter_with_lookups(self):\n    query = Query(Author)\n    query.add_filter(('name__startswith', 'John'))\n    self.assertEqual(len(query.where.children), 1)\n    self.assertEqual(query.where.children[0].lookup_name, 'startswith')\n    self.assertEqual(query.where.children[0].rhs, 'John')\n    self.assertIsInstance(query.where.children[0].lhs, SimpleCol)\n    self.assertEqual(query.where.children[0].lhs.alias, 'T0')\n    self.assertEqual(query.where.children[0].lhs.target.name, 'name')\n", "def test_lookup_type_solving(self):\n    query = Query(Author)\n    lookup_parts, field_parts, reffed_expression = query.solve_lookup_type('author__name__exact')\n    self.assertEqual(lookup_parts, ['exact'])\n    self.assertEqual(field_parts, ['author', 'name'])\n    self.assertFalse(reffed_expression)\n"], "sample_881": ["def test_roc_curve_drop_intermediate(drop):\n    y_true = np.array([0, 0, 1, 1])\n    y_score = np.array([0.1, 0.4, 0.35, 0.8])\n    fpr, tpr, thresholds = roc_curve(y_true, y_score, drop_intermediate=drop)\n\n    if drop:\n        assert_array_equal(fpr, np.array([0. , 0. , 0.5, 1. ]))\n        assert_array_equal(tpr, np.array([0. , 0.5, 1. , 1. ]))\n        assert_array_equal(thresholds, np.array([1.8 , 0.4 , 0.35, 0.1 ]))\n    else:\n        assert_array_equal(fpr, np.array([0. , 0. , 0.5, 0.5, 1. ]))\n        assert_array_equal(tpr, np.array([0. , 1. , 1. , 0.5, 0.5]))\n        assert_array_equal(thresholds, np.array([1.8 , 0.8 , 0.4 , 0.35, 0.1 ]))\n", "def test_average_precision_score_single_class():\n    y_true = np.array([0, 0, 0, 0])\n    y_score = np.array([0.1, 0.2, 0.3, 0.4])\n    with pytest.raises(ValueError, match=\"Only one class present in y_true. ROC AUC score is not defined in that case.\"):\n        average_precision_score(y_true, y_score)\n", "def test_det_curve_maximized(drop):\n    y_true = np.array([0, 0, 1, 1])\n    y_score = np.array([0.1, 0.4, 0.35, 0.8])\n    fpr, fnr, _ = det_curve(y_true, y_score, drop_intermediate=drop)\n    # Det curve reaches its maximum at point (0.5, 0.5)\n    assert np.max(fpr) == 0.5\n    assert np.max(fnr) == 0.5\n", "def test_det_curve_with_drop(drop):\n    y_true = np.array([0, 0, 1, 1])\n    y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n    fpr, fnr, thresholds = det_curve(y_true, y_scores)\n\n    # Check if fpr, fnr and thresholds are ndarrays\n    assert isinstance(fpr, np.ndarray)\n    assert isinstance(fnr, np.ndarray)\n    assert isinstance(thresholds, np.ndarray)\n\n    # Check if fpr, fnr and thresholds have the correct shape\n    assert fpr.shape == (3,)\n    assert fnr.shape == (3,)\n    assert thresholds.shape == (3,)\n\n    # Check if fpr is decreasing and fnr is increasing\n    assert np.all(np.diff(fpr) <= 0)\n    assert np.all(np.diff(fnr) >= 0)\n\n    # Check if fpr and fnr are within the range [0, 1]\n    assert np.all((fpr >= 0) & (fpr <= 1))\n    assert np.all((fnr >= 0) & (fnr <= 1))\n\n    # Check if the curves are correct when drop is set to True or False\n    if drop:\n        assert_array_equal(fpr, np.array([0.5, 0.5, 0.]))\n        assert_array_equal(fnr, np.array([0., 0.5, 0.5]))\n        assert_array_equal(thresholds, np.array([0.35, 0.4, 0.8]))\n    else:\n        assert_array_equal(fpr, np.array([0.5, 0.5, 0., 0.]))\n        assert_array_equal(fnr, np.array([0., 0.5, 0.5, 1.]))\n        assert_array_equal(thresholds,", "def test_det_curve_invariance(drop):\n    # Test that det_curve is invariant to a constant offset in y_score\n    y_true = np.array([0, 0, 1, 1])\n    y_score = np.array([0.1, 0.4, 0.35, 0.8])\n\n    fpr1, fnr1, thresholds1 = det_curve(y_true, y_score, drop_intermediate=drop)\n    fpr2, fnr2, thresholds2 = det_curve(y_true, y_score + 1.0, drop_intermediate=drop)\n\n    assert_array_equal(fpr1, fpr2)\n    assert_array_equal(fnr1, fnr2)\n    assert_array_equal(thresholds1, thresholds2 + 1.0)\n"], "sample_832": ["def test_BayesianRidge_predict_with_std():\n    X = diabetes.data\n    y = diabetes.target\n    br = BayesianRidge(compute_score=True)\n    br.fit(X, y)\n    y_mean, y_std = br.predict(X, return_std=True)\n    assert y_mean.shape == (len(y),)\n    assert y_std.shape == (len(y),)\n    assert_array_less(y_std, np.inf)\n", "def test_bayesian_ridge_predict_std():\n    X = diabetes.data\n    y = diabetes.target\n    br = BayesianRidge()\n    br.fit(X, y)\n    y_mean, y_std = br.predict(X, return_std=True)\n    assert y_mean.shape == (len(y),)\n    assert y_std.shape == (len(y),)\n    assert_array_less(y_std, np.inf)\n", "def test_bayesian_ridge_predict_std():\n    X = diabetes.data\n    y = diabetes.target\n    br = BayesianRidge()\n    br.fit(X, y)\n    y_mean, y_std = br.predict(X, return_std=True)\n    assert y_mean.shape == y.shape\n    assert y_std.shape == y.shape\n    assert_array_less(y_std, np.inf)\n", "def test_ard_regression_with_intercept():\n    X, y = diabetes.data, diabetes.target\n    n_samples, n_features = X.shape\n\n    # Add intercept\n    X = np.c_[np.ones(n_samples), X]\n    n_features += 1\n\n    ard = ARDRegression(fit_intercept=False)\n    ard.fit(X, y)\n\n    # Check that coefficients are non-zero for all features\n    assert_array_less(0, ard.coef_)\n\n    # Check that intercept is approximately zero\n    assert_almost_equal(ard.intercept_, 0, decimal=5)\n\n    # Check that lambda_ and sigma_ have the correct shape\n    assert ard.lambda_.shape == (n_features,)\n    assert ard.sigma_.shape == (n_features, n_features)\n\n    # Check that sigma_ is symmetric and positive semi-definite\n    assert_array_almost_equal(ard.sigma_, ard.sigma_.T)\n    assert_array_less(0, np.linalg.eigvals(ard.sigma_))\n", "def test_bayesian_ridge_scores():\n    X, y = diabetes.data, diabetes.target\n    n_samples, n_features = X.shape\n\n    # Test that scores are computed and monotonically increasing\n    br = BayesianRidge(compute_score=True, n_iter=50)\n    br.fit(X, y)\n    assert hasattr(br, \"scores_\")\n    assert_array_less(br.scores_[:-1], br.scores_[1:])\n\n    # Test that scores are correct\n    XT_y = np.dot(X.T, y)\n    U, S, Vh = np.linalg.svd(X, full_matrices=False)\n    eigen_vals_ = S ** 2\n\n    alpha_ = 1. / (np.var(y) + np.finfo(np.float64).eps)\n    lambda_ = 1.\n    coef_ = np.linalg.solve(X.T.dot(X) / n_samples + np.eye(n_features) / lambda_, XT_y / n_samples)\n    rmse_ = np.sum((y - np.dot(X, coef_)) ** 2)\n    score = br._log_marginal_likelihood(n_samples, n_features, eigen_vals_, alpha_, lambda_, coef_, rmse_)\n\n    assert_almost_equal(br.scores_[-1], score)\n"], "sample_231": ["    def test_html_response(self):\n        request = RequestFactory().get('/')\n        exc_type, exc_value, tb = sys.exc_info()\n        response = technical_500_response(request, exc_type, exc_value, tb)\n        self.assertEqual(response.status_code, 500)\n        self.assertEqual(response['content-type'], 'text/html')\n", "    def test_get_traceback_data(self):\n        request = RequestFactory().get('/')\n        exc_type = ValueError\n        exc_value = ValueError('Test error')\n        tb = exc_value.__traceback__\n        reporter = ExceptionReporter(request, exc_type, exc_value, tb)\n        data = reporter.get_traceback_data()\n        self.assertIsInstance(data, dict)\n", "    def test_get_traceback_frames(self):\n            raise ValueError(\"Cause exception\")\n\n            try:\n                cause_exception()\n            except ValueError as e:\n                raise RuntimeError(\"Main exception\") from e\n\n        try:\n            main_exception()\n        except RuntimeError as e:\n            reporter = ExceptionReporter(None, RuntimeError, e, e.__traceback__)\n            frames = reporter.get_traceback_frames()\n            self.assertEqual(len(frames), 3)  # main_exception, cause_exception, and the outermost frame\n", "    def setUp(self):\n        self.request = RequestFactory().get('/')\n        self.exc_type, self.exc_value, self.tb = sys.exc_info()\n", "    def test_get_traceback_frames(self):\n        # Create a simple exception with a cause\n        try:\n            raise ValueError(\"First error\")\n        except ValueError as e:\n            try:\n                raise TypeError(\"Second error\") from e\n            except TypeError as f:\n                reporter = ExceptionReporter(None, TypeError, f, f.__traceback__)\n                frames = reporter.get_traceback_frames()\n\n        # Check if both exceptions are present in the frames\n        self.assertEqual(len(frames), 2)\n        self.assertEqual(frames[0]['exc_value'], f)\n        self.assertEqual(frames[1]['exc_value'], e)\n"], "sample_1019": ["def test_monotonic_sign():\n    p = Dummy(positive=True)\n    assert _monotonic_sign(p) == Dummy('pos', positive=True)\n    n = Dummy(negative=True)\n    assert _monotonic_sign(n) == Dummy('neg', negative=True)\n    nn = Dummy(integer=True, nonnegative=True)\n    assert _monotonic_sign(nn) == Dummy('nneg', nonnegative=True)\n    np = Dummy(integer=True, nonpositive=True)\n    assert _monotonic_sign(np) == Dummy('npos', nonpositive=True)\n", "def test_gcd_terms_fraction():\n    expr = x/2 + y/3\n    cont, numer, denom = _gcd_terms([expr], fraction=True)\n    assert cont == 1\n    assert numer == (3*x + 2*y)\n    assert denom == 6\n", "def test_factor_terms_radical():\n    expr = sqrt(x) + sqrt(x*y) + sqrt(x*y**2)\n    factored = factor_terms(expr, radical=True)\n    expected = sqrt(x)*(1 + sqrt(y) + y)\n    assert factored == expected\n", "def test_decompose_power():\n    # Test decompose_power function with various expressions\n    assert decompose_power(x) == (x, 1)\n    assert decompose_power(x**2) == (x, 2)\n    assert decompose_power(x**(2*y)) == (x**y, 2)\n    assert decompose_power(x**(2*y/3)) == (x**(y/3), 2)\n    assert decompose_power((2*x)**(1/3)) == (2**(1/3)*x, 1/3)\n    assert decompose_power((2*x)**(1/2)) == (sqrt(2)*x, 1/2)\n", "def test_decompose_power():\n    assert decompose_power(x**2) == (x, 2)\n    assert decompose_power(x**(2*y)) == (x**y, 2)\n    assert decompose_power(x**(2*y/3)) == (x**(y/3), 2)\n"], "sample_21": ["def test_write_and_read_table_qdp():\n    # Test writing and reading a table with both symmetric and two-sided errors\n    data = Table()\n    data['a'] = [1, 2, 3]\n    data['b'] = [4, 5, 6]\n    data['a_err'] = [0.1, 0.2, 0.3]\n    data['b_perr'] = [0.4, 0.5, 0.6]\n    data['b_nerr'] = [0.7, 0.8, 0.9]\n\n    lines = _write_table_qdp(data)\n    read_data = _read_table_qdp(lines)\n\n    assert np.allclose(data['a'], read_data['a'])\n    assert np.allclose(data['b'], read_data['b'])\n    assert np.allclose(data['a_err'], read_data['a_err'])\n    assert np.allclose(data['b_perr'], read_data['b_perr'])\n    assert np.allclose(data['b_nerr'], read_data['b_nerr'])\n", "def test_write_table_qdp_with_err_specs():\n    table = Table([[1, 2, 3], [0.1, 0.2, 0.3], [0.05, 0.1, 0.15]], names=('a', 'b', 'c'))\n    lines = _write_table_qdp(table, err_specs={'serr': [2], 'terr': [3]})\n    expected_lines = [\n        \"READ SERR 2\",\n        \"READ TERR 3\",\n        \"!a a_err b b_perr b_nerr c c_perr c_nerr\",\n        \"1 0.1 NO 2 0.05 0.1 3 0.05 0.15\"\n    ]\n    assert lines == expected_lines\n", "def test_read_write_consistency():\n    # Create a sample table\n    t = Table()\n    t['a'] = [1, 2, 3]\n    t['b'] = [4.0, 5.0, 6.0]\n    t['c'] = [7, 8, 9]\n    t['c_err'] = [0.1, 0.2, 0.3]\n    t['d'] = [10.0, 11.0, 12.0]\n    t['d_perr'] = [0.4, 0.5, 0.6]\n    t['d_nerr'] = [0.7, 0.8, 0.9]\n\n    # Write the table to a QDP file\n    qdp_lines = _write_table_qdp(t, err_specs={'serr': [3], 'terr': [5]})\n\n    # Read the table back from the QDP file\n    t_read = _read_table_qdp(qdp_lines, delimiter=' ')\n\n    # Check that the original and read tables are equal\n    assert t == t_read\n", "def test_read_table_qdp():\n    qdp_content = \"! Initial comment\\nREAD SERR 3\\n!a a_err b c c_err\\n1 2 3 4 5\\nNO NO NO NO NO\"\n    table = _read_table_qdp(qdp_content)\n    assert table.colnames == ['a', 'a_err', 'b', 'c', 'c_err']\n    assert np.allclose(table['a'], [1])\n    assert np.allclose(table['b'], [3])\n    assert np.allclose(table['c'], [4])\n    assert np.allclose(table['a_err'], [2])\n    assert np.allclose(table['c_err'], [5])\n", "def test_write_table_qdp():\n    table = Table()\n    table['a'] = Column([1, 2, 3], dtype=int)\n    table['b'] = MaskedColumn([4.0, 5.0, None], dtype=float)\n    table['c'] = Column(['d', 'e', 'f'], dtype=str)\n    table.meta['initial_comments'] = ['Initial comment']\n    table.meta['comments'] = ['Table comment']\n\n    lines = _write_table_qdp(table, err_specs={'serr': [2], 'terr': [3]})\n\n    assert lines[0] == 'Initial comment'\n    assert lines[1] == 'READ SERR 2'\n    assert lines[2] == 'READ TERR 3'\n    assert lines[3] == 'Table comment'\n    assert lines[4] == '!a b c_perr c_nerr'\n    assert lines[5] == '1 4.0 NO NO'\n    assert lines[6] == '2 5.0 NO NO'\n    assert lines[7] == '3 NO NO d e f'\n"], "sample_765": ["def test_brier_score_loss():\n    y_true = np.array([0, 1, 1, 0])\n    y_prob = np.array([0.1, 0.9, 0.8, 0.3])\n    expected_score = 0.037\n    assert_almost_equal(brier_score_loss(y_true, y_prob), expected_score)\n", "def test_multilabel_confusion_matrix():\n    y_true = np.array([[1, 0, 1], [0, 1, 0]])\n    y_pred = np.array([[1, 0, 0], [0, 1, 1]])\n    result = multilabel_confusion_matrix(y_true, y_pred)\n    expected_result = np.array([[[1, 0], [0, 1]],\n                                [[1, 0], [0, 1]],\n                                [[0, 1], [1, 0]]])\n    assert_array_equal(result, expected_result)\n", "def test_zero_one_loss_binary():\n    y_true = np.array([0, 0, 1, 1])\n    y_pred = np.array([0, 1, 0, 1])\n    loss = zero_one_loss(y_true, y_pred)\n    assert_almost_equal(loss, 0.5)\n\n    loss = zero_one_loss(y_true, y_pred, normalize=False)\n    assert_equal(loss, 2)\n\n    loss = zero_one_loss(y_true, y_pred, sample_weight=[0.25, 0.25, 0.25, 0.25])\n    assert_almost_equal(loss, 0.5)\n\n    loss = zero_one_loss(y_true, y_pred, sample_weight=[0.5, 0.5, 0.25, 0.25])\n    assert_almost_equal(loss, 0.4)\n", "def test_brier_score_loss():\n    y_true = np.array([0, 1, 1, 0])\n    y_prob = np.array([0.1, 0.9, 0.8, 0.3])\n    score = brier_score_loss(y_true, y_prob)\n    assert_almost_equal(score, 0.0375)\n", "def test_brier_score_loss():\n    y_true = np.array([0, 1, 1, 0])\n    y_prob = np.array([0.1, 0.9, 0.8, 0.3])\n    expected_score = 0.0375\n    assert_almost_equal(brier_score_loss(y_true, y_prob), expected_score)\n\n    # Test with sample_weight\n    sample_weight = np.array([0.5, 0.5, 1, 2])\n    expected_score_weighted = 0.0675\n    assert_almost_equal(brier_score_loss(y_true, y_prob, sample_weight=sample_weight), expected_score_weighted)\n\n    # Test with non-default pos_label\n    y_true_categorical = np.array([\"spam\", \"ham\", \"ham\", \"spam\"])\n    expected_score_categorical = 0.0375\n    assert_almost_equal(brier_score_loss(y_true_categorical, y_prob, pos_label=\"ham\"), expected_score_categorical)\n\n    # Test with binary y_true and y_prob as integers\n    y_true_binary = np.array([0, 1, 1, 0], dtype=int)\n    y_prob_binary = np.array([0, 1, 0, 1], dtype=int)\n    expected_score_binary = 0.0\n    assert_almost_equal(brier_score_loss(y_true_binary, y_prob_binary), expected_score_binary)\n\n    # Test with invalid y_true and y_prob\n    y_true_invalid = np.array([0, 1, 2, 0])\n    y_prob_invalid = np.array([0.1, 0.9, 0.8, 1.1])\n    assert_raises(ValueError, brier_score_loss, y_true_invalid, y_prob)\n    assert_raises(Value"], "sample_253": ["    def test_zip_imported_module(self):\n        zip_file = self.temporary_file('test_zip.zip')\n        module_name = 'test_zip_module'\n        module_path = 'test_zip_module.py'\n        module_code = b'print(\"Hello from zip\")'\n\n        with zipfile.ZipFile(zip_file, mode='w') as zf:\n            zf.writestr(module_path, module_code)\n\n        with extend_sys_path(str(zip_file)):\n            self.import_and_cleanup(module_name)\n            self.assertFileFound(zip_file)\n\n        self.assertFileNotFound(zip_file)\n", "def test_zip_import(self):\n    zip_filename = self.temporary_file('test.zip')\n    with zipfile.ZipFile(zip_filename, 'w') as zf:\n        zf.writestr('test_module.py', 'import os')\n    sys.path.insert(0, str(zip_filename))\n    self.addCleanup(sys.path.remove, str(zip_filename))\n    self.import_and_cleanup('test_module')\n    self.assertFileFound(zip_filename)\n", "def test_iter_modules_and_files_with_weakref(self):\n    module = types.ModuleType('weakref_module')\n    weakref.ref(module)\n    sys.modules['weakref_module'] = module\n\n    self.clear_autoreload_caches()\n    # Test uncached access\n    self.assertNotIn(module, list(autoreload.iter_all_python_module_files()))\n    # Test cached access\n    self.assertNotIn(module, list(autoreload.iter_all_python_module_files()))\n    self.assertEqual(autoreload.iter_modules_and_files.cache_info().hits, 1)\n", "    def test_zip_import(self):\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            test_zip = os.path.join(tmpdirname, 'test.zip')\n            test_module = 'test_module'\n            test_code = b'print(\"test_module\")'\n\n            with zipfile.ZipFile(test_zip, 'w') as zipf:\n                zipf.writestr(f'{test_module}.py', test_code)\n\n            sys.path.insert(0, test_zip)\n            self.addCleanup(lambda: sys.path.remove(test_zip))\n\n            self.import_and_cleanup(test_module)\n            self.assertFileFound(Path(test_zip))\n", "    def test_weakref_in_sys_modules(self):\n        # Test that weakref modules in sys.modules are handled correctly\n        key = 'some_weakref_module'\n        weakref_module = types.ModuleType(key)\n        weakref_proxy = weakref.proxy(weakref_module)\n        sys.modules[key] = weakref_proxy\n        self.clear_autoreload_caches()\n        # Test uncached access\n        self.assertEqual(list(autoreload.iter_all_python_module_files()), [])\n        # Test cached access\n        self.assertEqual(list(autoreload.iter_all_python_module_files()), [])\n        self.assertEqual(autoreload.iter_modules_and_files.cache_info().hits, 1)\n        del sys.modules[key]\n"], "sample_246": ["def test_no_location(self):\n    output, po_contents = self._run_makemessages(no_location=True)\n    self.assertLocationCommentNotPresent(self.PO_FILE, 1, 'admin_scripts', 'tests', 'test_project', 'templates', 'test_no_location.html')\n", "def test_handle_no_obsolete(self, mock_write_po_file, mock_build_potfiles):\n    mock_build_potfiles.return_value = [self.PO_FILE]\n    self._run_makemessages(no_obsolete=True)\n    self.assertTrue(mock_write_po_file.called)\n    args, kwargs = mock_write_po_file.call_args\n    self.assertTrue(kwargs['no_obsolete'])\n", "def test_no_location(self):\n    # Run makemessages with --no-location option\n    self._run_makemessages(no_location=True)\n    # Check that the generated .po file does not contain any location comments\n    self.assertLocationCommentNotPresent(self.PO_FILE, None, 'test_no_location.py')\n", "    def test_process_locale_dir_djangojs(self):\n        \"\"\"\n        Test process_locale_dir method with domain 'djangojs'.\n        \"\"\"\n        locale_dir = tempfile.mkdtemp()\n        files = [\n            self.create_translatable_file('file1.js', 'console.log(gettext(\"Message 1\"));'),\n            self.create_translatable_file('file2.js', 'console.log(gettext(\"Message 2\"));'),\n        ]\n        command = MakeMessagesCommand()\n        command.domain = 'djangojs'\n        command.process_locale_dir(locale_dir, files)\n        potfile = os.path.join(locale_dir, 'djangojs.pot')\n        self.assertTrue(os.path.exists(potfile))\n        with open(potfile, 'r', encoding='utf-8') as f:\n            content = f.read()\n        self.assertIn('msgid \"Message 1\"', content)\n        self.assertIn('msgid \"Message 2\"', content)\n", "    def test_multiple_locales(self):\n        # Create a project with multiple locale directories\n        os.makedirs('locale1')\n        os.makedirs('locale2')\n        with open('file.py', 'w') as f:\n            f.write('from django.utils.translation import gettext_lazy as _\\nmsg = _(\"Hello\")')\n\n        # Run makemessages for two locales in different directories\n        options = {\n            'locale': ['locale1', 'locale2'],\n            'domain': 'django',\n        }\n        self._run_makemessages(**options)\n\n        # Check that both .po files were created\n        self.assertTrue(os.path.exists('locale1/locale1/LC_MESSAGES/django.po'))\n        self.assertTrue(os.path.exists('locale2/locale2/LC_MESSAGES/django.po'))\n"], "sample_796": ["def test_huber_loss_and_gradient_no_intercept():\n    X, y = make_regression_with_outliers()\n    w = np.zeros(X.shape[1] + 1)\n    w[-1] = 1.0  # scale factor\n    epsilon = 1.35\n    alpha = 0.0001\n    loss, gradient = _huber_loss_and_gradient(w, X, y, epsilon, alpha)\n    assert_greater(loss, 0)\n    assert_array_equal(gradient[:-1], np.zeros(X.shape[1]))\n    assert_greater(gradient[-1], 0)\n", "def test_huber_loss_and_gradient():\n    X, y = make_regression_with_outliers()\n    epsilon = 1.35\n    alpha = 0.0001\n    sample_weight = np.ones(len(y))\n\n    # Initialize w with random values\n    w = np.random.rand(X.shape[1] + 2)\n\n    # Calculate loss and gradient\n    loss, grad = _huber_loss_and_gradient(w, X, y, epsilon, alpha, sample_weight)\n\n    # Check if loss is a scalar\n    assert isinstance(loss, float)\n\n    # Check if gradient is a numpy array\n    assert isinstance(grad, np.ndarray)\n\n    # Check if gradient has the correct shape\n    assert grad.shape == (X.shape[1] + 2,)\n", "def test_huber_regressor_with_sparse_input():\n    rng = np.random.RandomState(0)\n    n_samples, n_features = 50, 20\n    X, y = make_regression_with_outliers(n_samples, n_features)\n    X_sparse = sparse.csr_matrix(X)\n\n    huber = HuberRegressor()\n    huber.fit(X_sparse, y)\n    huber_sparse_coef = huber.coef_\n\n    huber.fit(X, y)\n    huber_dense_coef = huber.coef_\n\n    assert_array_almost_equal(huber_sparse_coef, huber_dense_coef)\n", "def test_huber_regressor_with_sparse_input():\n    X, y = make_regression_with_outliers()\n    X_sparse = sparse.csr_matrix(X)\n\n    huber = HuberRegressor()\n    huber.fit(X_sparse, y)\n    huber_sparse = huber.coef_\n\n    huber.fit(X, y)\n    huber_dense = huber.coef_\n\n    assert_array_almost_equal(huber_sparse, huber_dense)\n", "def test_huber_regressor_with_sparse_input():\n    X, y = make_regression_with_outliers(n_samples=100, n_features=50)\n    X = sparse.csr_matrix(X)\n\n    huber = HuberRegressor()\n    huber.fit(X, y)\n\n    assert_array_almost_equal(huber.predict(X), huber.predict(X.toarray()))\n"], "sample_35": ["def test_resolve_name():\n    assert introspection.resolve_name('astropy.utils.introspection.resolve_name') == introspection.resolve_name\n    assert introspection.resolve_name('astropy', 'utils', 'introspection', 'resolve_name') == introspection.resolve_name\n    with pytest.raises(ImportError):\n        introspection.resolve_name('nonexistent.module')\n", "def test_resolve_name():\n    # Test resolve_name function with valid name\n    assert introspection.resolve_name('astropy.utils.introspection.resolve_name') == introspection.resolve_name\n\n    # Test resolve_name function with valid name and additional parts\n    assert introspection.resolve_name('astropy', 'utils', 'introspection', 'resolve_name') == introspection.resolve_name\n\n    # Test resolve_name function with invalid name\n    with pytest.raises(ImportError):\n        introspection.resolve_name('astropy.utils.introspection.nonexistent_function')\n", "def test_resolve_name():\n    # Test that resolve_name can handle additional_parts\n    obj = introspection.resolve_name('astropy.utils.introspection', 'resolve_name')\n    assert obj is introspection.resolve_name\n\n    # Test that resolve_name raises ImportError for non-existent module or object\n    with pytest.raises(ImportError):\n        introspection.resolve_name('nonexistent.module')\n    with pytest.raises(ImportError):\n        introspection.resolve_name('astropy.utils.introspection', 'nonexistent_function')\n", "def test_resolve_name():\n    # Test that resolve_name correctly resolves names with additional parts\n    assert introspection.resolve_name('astropy.utils.introspection', 'resolve_name') == introspection.resolve_name\n", "def test_resolve_name():\n    # Testing the resolve_name function with various inputs\n    assert introspection.resolve_name('math.sqrt') == math.sqrt\n    assert introspection.resolve_name('numpy', 'sin') == numpy.sin\n    with pytest.raises(ImportError):\n        introspection.resolve_name('nonexistent.module')\n    with pytest.raises(ImportError):\n        introspection.resolve_name('math.nonexistent_function')\n"], "sample_913": ["def test_parse_annotation():\n    annotation = \"List[Union[int, str]]\"\n    result = _parse_annotation(annotation)\n    assert isinstance(result, list)\n    assert len(result) == 5\n    assert isinstance(result[0], nodes.Text)\n    assert result[0].astext() == \"List\"\n    assert isinstance(result[1], addnodes.desc_sig_punctuation)\n    assert result[1].astext() == \"[\"\n    assert isinstance(result[2], nodes.Text)\n    assert result[2].astext() == \"Union\"\n    assert isinstance(result[3], nodes.Text)\n    assert result[3].astext() == \"int\"\n    assert isinstance(result[4], addnodes.desc_sig_punctuation)\n    assert result[4].astext() == \"]\"\n", "def test_parse_annotation():\n    result = _parse_annotation('List[str]')\n    assert len(result) == 3\n    assert isinstance(result[0], nodes.Text)\n    assert result[0].astext() == 'List'\n    assert isinstance(result[1], addnodes.desc_sig_punctuation)\n    assert result[1].astext() == '['\n    assert isinstance(result[2], addnodes.pending_xref)\n    assert result[2].astext() == 'str'\n", "def test_parse_annotation():\n    annotation = \"List[int]\"\n    result = _parse_annotation(annotation)\n    expected = [pending_xref(refdomain='py', reftype='class', reftarget='List', children=[nodes.Text('List')]),\n                addnodes.desc_sig_punctuation('', '['),\n                pending_xref(refdomain='py', reftype='class', reftarget='int', children=[nodes.Text('int')]),\n                addnodes.desc_sig_punctuation('', ']')]\n    assert_node(result, expected)\n", "def test_parse_annotation():\n    annotation = \"Optional[List[Union[int, str]]]\"\n    result = _parse_annotation(annotation)\n    assert len(result) == 5\n    assert isinstance(result[0], nodes.Text)\n    assert result[0].astext() == \"Optional\"\n    assert isinstance(result[1], pending_xref)\n    assert result[1]['reftarget'] == \"List\"\n    assert isinstance(result[2], addnodes.desc_sig_punctuation)\n    assert result[2].astext() == \"[\"\n    assert isinstance(result[3], pending_xref)\n    assert result[3]['reftarget'] == \"Union\"\n    assert isinstance(result[4], addnodes.desc_sig_punctuation)\n    assert result[4].astext() == \"]\"\n", "def test_parse_annotation():\n    # Test that _parse_annotation can handle a simple type annotation\n    annotation = \"int\"\n    result = _parse_annotation(annotation)\n    assert len(result) == 1\n    assert isinstance(result[0], pending_xref)\n    assert result[0].astext() == \"int\"\n    assert result[0]['refdomain'] == \"py\"\n    assert result[0]['reftype'] == \"class\"\n    assert result[0]['reftarget'] == \"int\"\n\n    # Test that _parse_annotation can handle a type annotation with a subscript\n    annotation = \"List[int]\"\n    result = _parse_annotation(annotation)\n    assert len(result) == 5\n    assert result[0].astext() == \"List\"\n    assert result[1].astext() == \"[\"\n    assert result[2].astext() == \"int\"\n    assert result[3].astext() == \"]\"\n    assert isinstance(result[4], nodes.Text)\n\n    # Test that _parse_annotation can handle a type annotation with multiple subscripts\n    annotation = \"Dict[str, List[int]]\"\n    result = _parse_annotation(annotation)\n    assert len(result) == 13\n    assert result[0].astext() == \"Dict\"\n    assert result[1].astext() == \"[\"\n    assert result[2].astext() == \"str\"\n    assert result[3].astext() == \",\"\n    assert result[4].astext() == \" \"\n    assert result[5].astext() == \"List\"\n    assert result[6].astext() == \"[\"\n    assert result[7].astext() == \"int\"\n    assert result[8].astext() == \"]\"\n    assert result[9].astext() == \"]\"\n    assert isinstance(result[10], nodes.Text)\n    assert result[10].astext() == \" \"\n    assert result[11].astext() == \"->\"\n    assert result[12].astext() == \" None\"\n"], "sample_508": ["def test_artist_set_zorder():\n    artist = martist.Artist()\n    artist.set_zorder(5)\n    assert artist.get_zorder() == 5\n", "def test_artist_set_zorder():\n    artist = martist.Artist()\n    assert artist.get_zorder() == martist.Artist.zorder\n    artist.set_zorder(10)\n    assert artist.get_zorder() == 10\n    artist.set_zorder(None)\n    assert artist.get_zorder() == martist.Artist.zorder\n", "def test_set_path_effects():\n    artist = martist.Artist()\n    pe = mtransforms.Stroke(linewidth=3, foreground='white')\n    artist.set_path_effects(pe)\n    assert artist.get_path_effects() == pe\n", "def test_set_path_effects():\n    fig, ax = plt.subplots()\n    path_effects = [mpath.PathPatchEffect(), mpatches.PathPatchEffect()]\n    for pe in path_effects:\n        line = mlines.Line2D([0, 1], [0, 1], path_effects=[pe])\n        ax.add_line(line)\n    plt.close(fig)\n", "def test_set_get_clip_path():\n    fig, ax = plt.subplots()\n    patch = mpatches.Circle((0, 0), radius=0.5)\n    ax.add_patch(patch)\n    ax.set_clip_path(patch)\n    assert ax.get_clip_path() == mtransforms.TransformedPatchPath(patch)\n\n    # Test that setting a Rectangle sets clip_box and not clip_path\n    rect = mpatches.Rectangle((0, 0), 1, 1)\n    ax.set_clip_path(rect)\n    assert ax.get_clip_box() == mtransforms.TransformedBbox(mpatches.Bbox.unit(), rect.get_transform())\n    assert ax.get_clip_path() is None\n\n    # Test that setting clip_path to None clears the clip_path\n    ax.set_clip_path(None)\n    assert ax.get_clip_path() is None\n"], "sample_865": ["def test_max_leaf_nodes(estimator):\n    X, y = DATASETS[\"iris\"][\"X\"], DATASETS[\"iris\"][\"y\"]\n    clf = ALL_TREES[estimator](max_leaf_nodes=3, random_state=0)\n    clf.fit(X, y)\n    assert clf.tree_.node_count <= 3\n", "def test_decision_tree_classifier_invalid_ccp_alpha(estimator_name):\n    est = CLF_TREES[estimator_name](random_state=42)\n    with pytest.raises(ValueError, match=\"ccp_alpha must be greater than or equal to 0\"):\n        est.fit(X_small, y_small, ccp_alpha=-1)\n", "def test_sparse_preserve_input(estimator_name):\n    estimator = ALL_TREES[estimator_name](random_state=0)\n    data = DATASETS[\"sparse-mix\"]\n    X_csc = data[\"X_sparse\"].tocsc()\n    X_csr = data[\"X_sparse\"].tocsr()\n    y = data[\"y\"]\n\n    estimator.fit(X_csc, y)\n    y_pred_csc = estimator.predict(X_csc)\n\n    estimator.fit(X_csr, y)\n    y_pred_csr = estimator.predict(X_csr)\n\n    assert_array_equal(y_pred_csc, y_pred_csr)\n", "def test_tree_regressor_with_sample_weight():\n    # Test decision tree regressor with sample weights\n    X, y = DATASETS[\"boston\"][\"X\"], DATASETS[\"boston\"][\"y\"]\n    sample_weight = np.ones(len(y))\n    regressor = DecisionTreeRegressor(random_state=0)\n    regressor.fit(X, y, sample_weight=sample_weight)\n\n    # Check if the regressor was fitted correctly\n    assert hasattr(regressor, \"tree_\")\n    assert regressor.tree_.node_count > 0\n\n    # Check if the prediction is correct\n    y_pred = regressor.predict(X)\n    assert len(y_pred) == len(y)\n\n    # Change the weight of one sample and check if the prediction changes\n    sample_weight[0] = 100\n    regressor.fit(X, y, sample_weight=sample_weight)\n    y_pred_new = regressor.predict(X)\n    assert not np.allclose(y_pred, y_pred_new)\n", "def test_classification_sample_weight(tree_class):\n    X = np.array([[0], [1], [2], [3]])\n    y = np.array([0, 0, 1, 1])\n    sample_weight = np.array([0.1, 0.2, 0.3, 0.4])\n\n    clf = tree_class(random_state=42)\n    clf.fit(X, y, sample_weight=sample_weight)\n    assert_array_equal(clf.tree_.weighted_n_node_samples[0],\n                       np.sum(sample_weight))\n    assert_array_almost_equal(clf.tree_.weighted_n_node_samples[1:],\n                              sample_weight[[0, 1, 2, 3]],\n                              decimal=5)\n\n    sample_weight_updated = np.array([0.1, 0.2, 0.6, 0.7])\n    clf.fit(X, y, sample_weight=sample_weight_updated)\n    assert_array_equal(clf.tree_.weighted_n_node_samples[0],\n                       np.sum(sample_weight_updated))\n    assert_array_almost_equal(clf.tree_.weighted_n_node_samples[1:],\n                              sample_weight_updated[[0, 1, 2, 3]],\n                              decimal=5)\n"], "sample_941": ["def test_restify():\n    assert restify(None) == ':obj:`None`'\n    assert restify(Ellipsis) == '...'\n    assert restify(Struct) == ':class:`struct.Struct`'\n    assert restify(MyInt) == ':class:`MyInt`'\n    assert restify(MyClass1) == ':class:`test_util_typing.MyClass1`'\n    assert restify(MyClass2) == ':class:`test_util_typing.<MyClass2>`'\n    assert restify(Optional[int]) == ':obj:`Optional`\\\\ [:obj:`int`]'\n    assert restify(Optional[Optional[int]]) == ':obj:`Optional`\\\\ [:obj:`Optional`\\\\ [:obj:`int`]]'\n    assert restify(Union[int, str]) == ':obj:`Union`\\\\ [:obj:`int`, :obj:`str`]'\n    assert restify(Union[int, str, None]) == ':obj:`Optional`\\\\ [:obj:`Union`\\\\ [:obj:`int`, :obj:`str`]]'\n    assert restify(Union[int, Optional[str]]) == ':obj:`Union`\\\\ [:obj:`int`, :obj:`Optional`\\\\ [:obj:`str`]]'\n    assert restify(List[int]) == ':class:`list`\\\\ [:obj:`int`]'\n    assert restify(MyList[int]) == ':class:`test_util_typing.MyList`\\\\ [:obj:`int`]'\n    assert restify(Dict[str, int]) == ':class:`dict`\\\\ [:obj:`str`, :obj:`int`]'\n    assert restify(Callable[[int], str]) == ':class:`Callable`\\\\ [[:obj:`int`], :obj:`str`]'\n    assert restify(Tuple[int, ...]) == ':class:`tuple`\\\\ [:obj:`int`, ...]'\n    assert restify(BrokenType) == ':obj:`test_util_typing.BrokenType`'\n", "def test_restify(annotation, expected):\n    assert restify(annotation) == expected\n", "def test_restify():\n    assert restify(MyClass1) == ':class:`test_util_typing.MyClass1`'\n    assert restify(MyClass2) == ':class:`test_util_typing.MyClass2`'\n    assert restify(None) == ':obj:`None`'\n    assert restify(Ellipsis) == '...'\n    assert restify(Struct) == ':class:`struct.Struct`'\n    assert restify(MyInt) == ':class:`MyInt`'\n    assert restify(MyList[int]) == ':class:`test_util_typing.MyList`\\\\ [:obj:`int`]'\n    assert restify(Optional[int]) == ':obj:`Optional`\\\\ [:obj:`int`]'\n    assert restify(Union[int, str]) == ':obj:`Union`\\\\ [:obj:`int`, :obj:`str`]'\n    assert restify(Callable[[int, str], bool]) == ':obj:`Callable`\\\\ [[:obj:`int`, :obj:`str`], :obj:`bool`]'\n", "def test_restify(annotation, expected):\n    assert restify(annotation) == expected\n", "def test_restify_for_builtin_types():\n    assert restify(int) == ':class:`int`'\n    assert restify(str) == ':class:`str`'\n    assert restify(dict) == ':class:`dict`'\n"], "sample_109": ["    def test_build_attrs(self):\n        widget = AutocompleteSelect(Album._meta.get_field('band').remote_field, admin.site)\n        attrs = widget.build_attrs({'class': 'custom-class'})\n        expected_attrs = {\n            'class': 'custom-class admin-autocomplete',\n            'data-ajax--cache': 'true',\n            'data-ajax--delay': 250,\n            'data-ajax--type': 'GET',\n            'data-ajax--url': '/admin/admin_widgets/band/autocomplete/',\n            'data-theme': 'admin-autocomplete',\n            'data-allow-clear': 'true',\n            'data-placeholder': '',\n        }\n        self.assertEqual(attrs, expected_attrs)\n", "    def test_autocomplete_media(self):\n        widget = AutocompleteSelect(\n            Album._meta.get_field('band').remote_field,\n            admin.site,\n        )\n        expected_media = forms.Media(\n            js=(\n                'admin/js/vendor/jquery/jquery.min.js',\n                'admin/js/vendor/select2/select2.full.min.js',\n                'admin/js/jquery.init.js',\n                'admin/js/autocomplete.js',\n            ),\n            css={\n                'screen': (\n                    'admin/css/vendor/select2/select2.min.css',\n                    'admin/css/autocomplete.css',\n                ),\n            },\n        )\n        self.assertEqual(widget.media, expected_media)\n", "    def setUpTestData(cls):\n        cls.band = Band.objects.create(name='Test Band')\n", "    def test_optgroups(self):\n        form = MultipleSelectedBandForm()\n        widget = form.fields['featuring'].widget\n        optgroups = widget.optgroups('featuring', [1, 2])\n        self.assertEqual(len(optgroups), 1)\n        self.assertEqual(len(optgroups[0][1]), 2)\n        self.assertEqual(optgroups[0][1][0]['value'], '1')\n        self.assertEqual(optgroups[0][1][1]['value'], '2')\n", "    def test_autocomplete_select_multiple(self):\n        form = FeaturedAlbumsForm()\n        self.assertInHTML(\n            '<select name=\"featured_albums\" class=\"my-class admin-autocomplete\" multiple data-ajax--url=\"/admin/admin_widgets/album/autocomplete/\" data-ajax--cache=\"true\" data-ajax--delay=\"250\" data-ajax--type=\"GET\" data-theme=\"admin-autocomplete\" data-allow-clear=\"true\" data-placeholder=\"\">',\n            str(form['featured_albums']),\n            html=True\n        )\n"], "sample_380": ["    def test_aggregate_with_filter(self):\n        result = Author.objects.aggregate(Avg('age', filter=Q(age__gt=30)))\n        self.assertAlmostEqual(result['age__avg'], 40.666666666666664)\n", "    def test_count_with_filter(self):\n        count = Author.objects.filter(age__gt=30).annotate(count=Count('id', filter=Q(name__startswith='J')))\n        self.assertEqual(count.first().count, 3)\n", "    def test_variance(self):\n        result = Author.objects.aggregate(variance=Variance('age'))\n        self.assertAlmostEqual(result['variance'], 76.47, places=2)\n", "    def test_default_argument(self):\n        # Testing Count aggregate with a default argument\n        result = Author.objects.aggregate(count_with_default=Count('id', default=10))\n        self.assertEqual(result['count_with_default'], 9)\n\n        # Testing Max aggregate with a default argument\n        result = Author.objects.aggregate(max_with_default=Max('age', default=0))\n        self.assertEqual(result['max_with_default'], 57)\n\n        # Testing Min aggregate with a default argument\n        result = Author.objects.filter(age=None).aggregate(min_with_default=Min('age', default=0))\n        self.assertEqual(result['min_with_default'], 0)\n\n        # Testing Avg aggregate with a default argument\n        result = Book.objects.aggregate(avg_with_default=Avg('price', default=Decimal('0.00')))\n        self.assertEqual(result['avg_with_default'], Decimal('33.54611111'))\n\n        # Testing Sum aggregate with a default argument\n        result = Book.objects.filter(rating=None).aggregate(sum_with_default=Sum('pages', default=0))\n        self.assertEqual(result['sum_with_default'], 0)\n", "    def test_filter_count(self):\n        qs = Book.objects.filter(publisher=self.p1).aggregate(\n            count_filter=Count('id', filter=Q(price__gt=Decimal('25.00'))))\n        self.assertEqual(qs['count_filter'], 1)\n"], "sample_615": ["def test_broadcast_compat_data():\n    var = xr.Variable((\"x\", \"y\"), np.random.randn(3, 4))\n    broadcast_dims = (\"z\",)\n    core_dims = (\"x\", \"y\")\n    result = broadcast_compat_data(var, broadcast_dims, core_dims)\n    assert result.shape == (1, 3, 4)\n", "def test_broadcast_compat_data():\n    var = xr.Variable(dims=(\"x\", \"y\"), data=np.ones((3, 4)))\n    data = broadcast_compat_data(var, broadcast_dims=(\"z\",), core_dims=(\"x\", \"y\"))\n    assert data.shape == (1, 3, 4)\n", "def test_apply_ufunc_with_multiple_outputs():\n        return x + y, x - y\n\n    x = xr.DataArray(np.arange(10), dims=\"a\")\n    y = xr.DataArray(np.arange(10, 20), dims=\"a\")\n\n    result = apply_ufunc(func, x, y, output_core_dims=[[\"a\"], [\"a\"]])\n\n    expected_add = xr.DataArray(np.arange(10) + np.arange(10, 20), dims=\"a\")\n    expected_sub = xr.DataArray(np.arange(10) - np.arange(10, 20), dims=\"a\")\n\n    assert_identical(result[0], expected_add)\n    assert_identical(result[1], expected_sub)\n", "def test_ordered_set_intersection():\n    all_keys = [[\"a\", \"b\", \"c\"], [\"b\", \"c\", \"d\"]]\n    result = ordered_set_intersection(all_keys)\n    assert result == [\"b\", \"c\"]\n", "def test_broadcast_compat_data():\n    var = xr.Variable((\"x\", \"y\"), np.arange(6).reshape(2, 3))\n    broadcast_dims = (\"z\",)\n    core_dims = (\"x\", \"y\")\n    expected_data = np.arange(6).reshape(1, 2, 3)\n    result_data = broadcast_compat_data(var, broadcast_dims, core_dims)\n    assert_array_equal(result_data, expected_data)\n\n    # Test unexpected dimensions\n    var = xr.Variable((\"x\", \"y\", \"z\"), np.arange(12).reshape(2, 2, 3))\n    with pytest.raises(ValueError):\n        broadcast_compat_data(var, broadcast_dims, core_dims)\n\n    # Test missing core dimensions\n    var = xr.Variable((\"x\", \"z\"), np.arange(6).reshape(2, 3))\n    with pytest.raises(ValueError):\n        broadcast_compat_data(var, broadcast_dims, core_dims)\n\n    # Test broadcast dimensions to the left\n    var = xr.Variable((\"y\", \"x\"), np.arange(6).reshape(3, 2))\n    expected_data = np.arange(6).reshape(1, 3, 2)\n    result_data = broadcast_compat_data(var, broadcast_dims, core_dims)\n    assert_array_equal(result_data, expected_data)\n\n    # Test reordered dimensions\n    var = xr.Variable((\"y\", \"x\"), np.arange(6).reshape(3, 2))\n    broadcast_dims = (\"z\", \"y\")\n    expected_data = np.arange(6).reshape(1, 3, 2)\n    result_data = broadcast_compat_data(var, broadcast_dims, core_dims)\n    assert_array_equal(result_data, expected_data)\n"], "sample_605": ["def test_consolidate_slices():\n    # Test case where slices are adjacent and can be consolidated\n    slices = [slice(0, 5), slice(5, 10), slice(10, 15)]\n    assert _consolidate_slices(slices) == [slice(0, 15)]\n\n    # Test case where slices are not adjacent and cannot be consolidated\n    slices = [slice(0, 5), slice(10, 15), slice(20, 25)]\n    assert _consolidate_slices(slices) == slices\n\n    # Test case where slices have step values\n    slices = [slice(0, 10, 2), slice(10, 20, 2)]\n    assert _consolidate_slices(slices) == [slice(0, 20, 2)]\n\n    # Test case where slices have None values\n    slices = [slice(None, 5), slice(5, None)]\n    assert _consolidate_slices(slices) == [slice(None)]\n\n    # Test case where input is not a list of slices\n    with pytest.raises(ValueError):\n        _consolidate_slices([0, 1, 2])\n", "def test_consolidate_slices():\n    slices = [slice(0, 3), slice(3, 6), slice(6, 9)]\n    expected_output = [slice(0, 9)]\n    assert _consolidate_slices(slices) == expected_output\n\n    slices = [slice(0, 3), slice(4, 6), slice(6, 9)]\n    expected_output = [slice(0, 3), slice(4, 9)]\n    assert _consolidate_slices(slices) == expected_output\n\n    slices = [slice(0, 3, 2), slice(3, 6, 2), slice(6, 9, 2)]\n    expected_output = [slice(0, 9, 2)]\n    assert _consolidate_slices(slices) == expected_output\n\n    slices = [slice(0, 3), 5]\n    with pytest.raises(ValueError, match=\"list element is not a slice: 5\"):\n        _consolidate_slices(slices)\n", "def test_consolidate_slices():\n    # Test consolidation of adjacent slices\n    slices = [slice(0, 10, 1), slice(10, 20, 1), slice(20, 30, 1)]\n    result = _consolidate_slices(slices)\n    assert result == [slice(0, 30, 1)]\n\n    # Test consolidation with non-adjacent slices\n    slices = [slice(0, 10, 1), slice(20, 30, 1), slice(40, 50, 1)]\n    result = _consolidate_slices(slices)\n    assert result == [slice(0, 10, 1), slice(20, 30, 1), slice(40, 50, 1)]\n\n    # Test consolidation with step other than 1\n    slices = [slice(0, 10, 2), slice(10, 20, 2)]\n    result = _consolidate_slices(slices)\n    assert result == [slice(0, 20, 2)]\n\n    # Test error when list element is not a slice\n    slices = [slice(0, 10, 1), 20]\n    with pytest.raises(ValueError):\n        _consolidate_slices(slices)\n", "def test_consolidate_slices():\n    slices = [slice(None, 5), slice(5, 10), slice(10, None)]\n    result = _consolidate_slices(slices)\n    assert result == [slice(None, None)]\n\n    slices = [slice(None, 5), slice(5, 10), slice(15, 20)]\n    result = _consolidate_slices(slices)\n    assert result == [slice(None, 5), slice(5, 10), slice(15, 20)]\n\n    slices = [slice(None, 5, 2), slice(5, 10, 3), slice(10, None, 4)]\n    result = _consolidate_slices(slices)\n    assert result == [slice(None, 5, 2), slice(5, 10, 3), slice(10, None, 4)]\n", "def test_groupby_with_bins(dataset):\n    bins = [0, 1, 2]\n    grouped = dataset.groupby(\"foo\", bins=bins)\n    assert len(grouped) == 2  # Two bins: (-inf, 0], (0, 1]\n    assert list(grouped.groups.keys()) == pd.IntervalIndex.from_breaks(bins).categories\n"], "sample_628": ["    def test_check_spelling_in_comment(self):\n        token_stream = _tokenize_str(\"# This is a comment with a speling error.\")\n        with self.assertAddsMessages(\n            Message(\n                \"wrong-spelling-in-comment\",\n                line=1,\n                args=(\"speling\", \"# This is a comment with a speling error.\", \"^\", self._get_msg_suggestions(\"speling\")),\n            )\n        ):\n            self.checker.process_tokens(token_stream)\n", "    def test_check_spelling_in_docstring(self):\n        node = astroid.parse(\n            \"\"\"\n                '''This is a docstring with a speeling error in it.\n                '''\n                pass\n            \"\"\"\n        )\n        with self.assertAddsMessages(\n            Message(\n                msg_id=\"wrong-spelling-in-docstring\",\n                line=3,\n                args=(\"speeling\", \"This is a docstring with a speeling error in it.\", \" \" * 32 + \"^^^^^\", self._get_msg_suggestions(\"speeling\")),\n            )\n        ):\n            self.checker.visit_functiondef(node.body[0])\n", "def test_check_spelling_in_comment(self):\n    tokens = _tokenize_str(\"# This is a test comment with a spelking errror\")\n    with self.assertAddsMessages(\n        Message('wrong-spelling-in-comment', line=1, args=('spelking', '# This is a test comment with a spelking errror', '                              ^^^^^^^', self._get_msg_suggestions('spelking')))) as msg:\n        self.checker.process_tokens(list(tokens))\n", "def test_spelling_in_comment(self):\n    # Test spelling in comments\n    code = '''\n    # This is a comment with a misspelled word: speeling\n    print(\"Hello, World!\")\n    '''\n    tokens = _tokenize_str(code)\n    self.checker.process_tokens(tokens)\n    msg = self.linter.release_messages()[0]\n    self.assertEqual(msg.msg_id, 'wrong-spelling-in-comment')\n    self.assertEqual(msg.symbol, 'speeling')\n    self.assertIn(self._get_msg_suggestions('speeling'), msg.msg)\n", "    def test_docstring_spelling(self):\n        node = astroid.extract_node(\n            \"\"\"\n                '''This is a docstring with a splling error in documnt.\n                Another line with a spelling error in trhe comment.\n                '''\n                pass\n            \"\"\"\n        )\n        with self.assertAddsMessages(\n            Message(\"wrong-spelling-in-docstring\", line=4, args=(\"documnt\", \"...\", \"...\", self._get_msg_suggestions(\"documnt\"))),\n            Message(\"wrong-spelling-in-docstring\", line=5, args=(\"trhe\", \"...\", \"...\", self._get_msg_suggestions(\"trhe\"))),\n        ):\n            self.checker.visit_functiondef(node.body[0])\n"], "sample_583": ["def test_vectorized_indexer(self):\n    x = np.arange(10)\n    i = indexing.VectorizedIndexer((np.array([1, 3, 5]), np.array([2, 4, 6])))\n    expected = np.array([x[1, 2], x[3, 4], x[5, 6]])\n    assert_array_equal(x[i], expected)\n", "def test_outer_indexer_with_dask_array(self):\n    import dask.array as da\n\n    data = da.from_array(np.random.rand(5, 5), chunks=(2, 2))\n    var = Variable(dims=('x', 'y'), data=data)\n    indexer = indexing.OuterIndexer((np.array([0, 2, 4]), np.array([1, 3])))\n    result = var[indexer]\n\n    assert isinstance(result.data, da.Array)\n    assert result.dims == ('x', 'y')\n    assert result.shape == (3, 2)\n", "def test_pandas_index_adapter_with_timedelta(self):\n    index = pd.Index([pd.Timedelta(days=i) for i in range(5)])\n    adapter = indexing.PandasIndexAdapter(index)\n    assert_array_equal(adapter[B((2,))], np.timedelta64(2, 'D'))\n", "def test_outer_indexer(self):\n    data = np.random.rand(3, 4)\n    var = Variable(('x', 'y'), data)\n\n    outer_indexer = indexing.OuterIndexer((np.array([0, 2]), np.array([1, 3])))\n    expected_result = data[[0, 2], [1, 3]]\n\n    assert_array_equal(var[outer_indexer], expected_result)\n", "def test_vectorized_indexing_with_mask(self):\n    x = DataArray(np.arange(10), dims='x')\n    i = ([0, 2, 4, -1],)\n    expected = DataArray([0, 2, 4, np.nan], dims='x')\n    result = x.isel(x=i)\n    assert_array_equal(result, expected)\n"], "sample_170": ["    def test_get_traceback_data(self):\n        request = RequestFactory().get('/')\n        exc_type = TemplateDoesNotExist\n        exc_value = TemplateDoesNotExist(\"Error message\")\n        tb = mock.Mock()\n        reporter = ExceptionReporter(request, exc_type, exc_value, tb)\n        data = reporter.get_traceback_data()\n        self.assertTrue(data['template_does_not_exist'])\n", "    def test_get_traceback_frames(self):\n        request = RequestFactory().get('/')\n        try:\n            raise ValueError(\"Test exception\")\n        except ValueError:\n            exc_type, exc_value, tb = sys.exc_info()\n        reporter = ExceptionReporter(request, exc_type, exc_value, tb)\n        frames = reporter.get_traceback_frames()\n        self.assertIsInstance(frames, list)\n        self.assertTrue(len(frames) > 0)\n        self.assertIsInstance(frames[0], dict)\n", "    def test_get_traceback_frames_with_cyclic_cause(self):\n        class CycleCauseException(Exception):\n            pass\n\n        exc_value = CycleCauseException()\n        exc_value.__cause__ = exc_value\n\n        reporter = ExceptionReporter(None, CycleCauseException, exc_value, None)\n        frames = reporter.get_traceback_frames()\n\n        self.assertEqual(len(frames), 1)\n", "    def test_get_traceback_data(self):\n        request = RequestFactory().get('/')\n        exc_type, exc_value, tb = sys.exc_info()\n        reporter = ExceptionReporter(request, exc_type, exc_value, tb)\n        data = reporter.get_traceback_data()\n        self.assertIsInstance(data, dict)\n        self.assertIn('request', data)\n", "    def setUp(self):\n        self.filter = SafeExceptionReporterFilter()\n"], "sample_241": ["def test_query_count_annotation(self):\n    query = Company.objects.annotate(num_employees_count=Count('employee'))\n    self.assertEqual(len(query), 3)\n    self.assertEqual(query[0].num_employees_count, 1)\n    self.assertEqual(query[1].num_employees_count, 1)\n    self.assertEqual(query[2].num_employees_count, 1)\n", "def test_filter_with_annotate(self):\n    # Test filtering with annotated values\n    annotated_query = Company.objects.annotate(total_salary=Sum('employee__salary'))\n    filtered_query = annotated_query.filter(total_salary__gt=50)\n    self.assertEqual(filtered_query.count(), 1)\n    self.assertEqual(filtered_query.first().name, 'Example Inc.')\n", "def test_annotate_with_aggregate_function_in_order_by(self):\n    query = Company.objects.annotate(avg_salary=Avg('employees__salary')).order_by('avg_salary')\n    self.assertIn('AVG(', str(query))\n", "def test_filter_with_transform(self):\n    # Test that transforms work correctly in filters.\n    company_query = Company.objects.filter(name__lower__startswith='example')\n    self.assertEqual(list(company_query), [self.example_inc])\n", "def test_query_join_promotion(self):\n    query = Company.objects.filter(\n        Q(num_employees__lt=1000) | Q(name__icontains='Inc')\n    )\n    query.add_filter(('num_chairs__gt', 2))\n    promoted_tables = query.query.joinpromoter.update_join_types(query.query)\n    self.assertIn('ceo_id', promoted_tables)\n"], "sample_772": ["def check_regression_toy(name):\n    \"\"\"Check regression on a toy dataset.\"\"\"\n    ForestRegressor = FOREST_REGRESSORS[name]\n\n    reg = ForestRegressor(n_estimators=10, random_state=1)\n    reg.fit(X, y)\n    assert_array_almost_equal(reg.predict(T), np.mean(y), decimal=1)\n    assert_equal(10, len(reg))\n\n    reg = ForestRegressor(n_estimators=10, max_features=1, random_state=1)\n    reg.fit(X, y)\n    assert_array_almost_equal(reg.predict(T), np.mean(y), decimal=1)\n    assert_equal(10, len(reg))\n", "def check_feature_importances(name):\n    \"\"\"Check feature importances on a larger dataset.\"\"\"\n    ForestClassifier = FOREST_CLASSIFIERS[name]\n\n    clf = ForestClassifier(n_estimators=10, random_state=0)\n    clf.fit(X_large, y_large)\n    importances = clf.feature_importances_\n    assert_equal(importances.shape, (10,))\n    assert_greater(importances.sum(), 0.9)\n    assert_less(importances.sum(), 1.1)\n", "def check_classification_iris(name):\n    \"\"\"Check classification on the iris dataset.\"\"\"\n    ForestClassifier = FOREST_CLASSIFIERS[name]\n\n    clf = ForestClassifier(n_estimators=10, random_state=1)\n    clf.fit(iris.data, iris.target)\n    assert_greater(clf.score(iris.data, iris.target), 0.9)\n    assert_equal(10, len(clf))\n\n    # also test apply\n    leaf_indices = clf.apply(iris.data)\n    assert_equal(leaf_indices.shape, (iris.data.shape[0], clf.n_estimators))\n\n    # also test feature importances\n    feature_importances = clf.feature_importances_\n    assert_equal(feature_importances.shape, (iris.data.shape[1],))\n    assert_greater(np.sum(feature_importances), 0)\n", "def check_regression_toy(name):\n    \"\"\"Check regression on a toy dataset.\"\"\"\n    ForestRegressor = FOREST_REGRESSORS[name]\n\n    regr = ForestRegressor(n_estimators=10, random_state=1)\n    regr.fit(X, y)\n    predicted_result = regr.predict(T)\n    assert_array_almost_equal(predicted_result, np.mean(y))  # For regressors, prediction should be close to mean of y\n    assert_equal(10, len(regr))\n\n    regr = ForestRegressor(n_estimators=10, max_features=1, random_state=1)\n    regr.fit(X, y)\n    predicted_result = regr.predict(T)\n    assert_array_almost_equal(predicted_result, np.mean(y))  # For regressors, prediction should be close to mean of y\n    assert_equal(10, len(regr))\n", "def check_classification_iris(name):\n    \"\"\"Check classification on the iris dataset.\"\"\"\n    ForestClassifier = FOREST_CLASSIFIERS[name]\n\n    clf = ForestClassifier(n_estimators=10, random_state=1)\n    clf.fit(iris.data, iris.target)\n    score = clf.score(iris.data, iris.target)\n    assert_greater(score, 0.9)\n\n    # Test feature importances\n    clf = ForestClassifier(n_estimators=100, random_state=0)\n    clf.fit(X_large, y_large)\n    importances = clf.feature_importances_\n    assert_equal(len(importances), 10)\n    assert_greater(importances.sum(), 0.5)\n"], "sample_1097": ["def test_blockcut():\n    M = ImmutableMatrix(4, 4, range(16))\n    B = blockcut(M, (1, 3), (1, 3))\n    assert isinstance(B, BlockMatrix)\n    assert ImmutableMatrix(B.blocks[0, 1]) == Matrix([[1, 2, 3]])\n", "def test_block_collapse_complex():\n    X = MatrixSymbol('X', n, n, complex=True)\n    Y = MatrixSymbol('Y', m, m, complex=True)\n    Z = MatrixSymbol('Z', n, m, complex=True)\n    B = BlockMatrix([[X, Z], [ZeroMatrix(m, n), Y]])\n    C = BlockMatrix([[Identity(n), Z]])\n    result = block_collapse(C * B)\n    expected = BlockMatrix([[X, Z + Z*Y]])\n    assert result == expected\n", "def test_block_collapse_transpose():\n    B = BlockMatrix([[A, B], [C, D]])\n    Bt = B.transpose()\n    B_collapsed = block_collapse(Bt)\n    expected_result = BlockMatrix([[A.transpose(), C.transpose()], [B.transpose(), D.transpose()]])\n    assert B_collapsed == expected_result\n", "def test_block_collapse():\n    expr = b1 * b2\n    collapsed_expr = block_collapse(expr)\n    expected_result = Matrix([[G*G + H*G, G*H]])\n    assert collapsed_expr == expected_result\n", "def test_bc_dist():\n    X = MatrixSymbol('X', m, l)\n    Y = MatrixSymbol('Y', n, p)\n    Z = MatrixSymbol('Z', m, p)\n    BM = BlockMatrix([[X, Z], [ZeroMatrix(n, l), Y]])\n    result = bc_dist(A*BM)\n    assert result == BlockMatrix([[A*X, A*Z], [ZeroMatrix(n, l), A*Y]])\n"], "sample_1187": ["def test_distance_to_side():\n    point = (0, 0, 0)\n    assert distance_to_side(point, [(0, 0, 1), (0, 1, 0)], (1, 0, 0)) == -sqrt(2)/2\n", "def test_best_origin():\n    l = Segment2D(Point(0, 3), Point(1, 1))\n    expr = x**3*y**7\n    assert best_origin((2, 1), 3, l, expr) == (0, 3.0)\n", "def test_decompose():\n    expr = x**2 + x*y + x + y + x**3*y**2 + y**5\n    result = decompose(expr)\n    assert result == {1: x + y, 2: x**2 + x*y, 5: x**3*y**2 + y**5}\n\n    expr = x**2 + x*y + x + y + x**3*y**2 + y**5\n    result = decompose(expr, separate=True)\n    assert result == {x, x**2, y, y**5, x*y, x**3*y**2}\n\n    expr = x\n    result = decompose(expr)\n    assert result == {1: x}\n\n    expr = 5\n    result = decompose(expr)\n    assert result == {0: 5}\n", "def test_integration_reduction_dynamic():\n    facets = [Segment2D(Point2D(0, 0), Point2D(1, 0)), Segment2D(Point2D(1, 0), Point2D(1, 1)), Segment2D(Point2D(1, 1), Point2D(0, 1)), Segment2D(Point2D(0, 1), Point2D(0, 0))]\n    index = 0\n    a = (0, 1)\n    b = 0\n    expr = x\n    degree = 1\n    dims = (x, y)\n    x_index = 1\n    y_index = 0\n    max_index = 1\n    x0 = (0, 0)\n    monomial_values = [[0, 0, 0, 0], [1, 1, 0, None]]\n    monom_index = 1\n    result = integration_reduction_dynamic(facets, index, a, b, expr, degree, dims, x_index, y_index, max_index, x0, monomial_values, monom_index)\n    assert result == Rational(1, 2)\n", "def test_best_origin():\n    l = Segment2D(Point(0, 3), Point(1, 1))\n    expr = x**3*y**7\n    assert best_origin((2, 1), 3, l, expr) == (0, 3.0)\n"], "sample_322": ["    def test_migration_plan_clean_start(self):\n        executor = MigrationExecutor(connection)\n        targets = [(\"migrations\", \"0001_initial\")]\n        plan = executor.migration_plan(targets, clean_start=True)\n        self.assertEqual(plan, [(executor.loader.graph.nodes[(\"migrations\", \"0001_initial\")], False)])\n", "    def test_migration_plan_unmigrate_all(self):\n        \"\"\"\n        Tests that the migration plan generates the correct set of migrations\n        when unmigrating all migrations in an app.\n        \"\"\"\n        executor = MigrationExecutor(connection)\n        targets = [('migrations', None)]\n        plan = executor.migration_plan(targets)\n        expected_plan = [\n            (executor.loader.graph.nodes[('migrations', '0002_add_m2m')], True),\n            (executor.loader.graph.nodes[('migrations', '0001_initial')], True),\n        ]\n        self.assertEqual(plan, expected_plan)\n", "    def test_migration_plan_clean_start(self):\n        executor = MigrationExecutor(connection)\n        targets = [('migrations', '0001_initial')]\n        plan = executor.migration_plan(targets, clean_start=True)\n        self.assertEqual(len(plan), 1)\n        self.assertEqual(plan[0][0].name, '0001_initial')\n        self.assertFalse(plan[0][1])\n", "    def test_migrate_all_backwards(self):\n        \"\"\"\n        Test the _migrate_all_backwards method of MigrationExecutor.\n        \"\"\"\n        # Mock the necessary components for the MigrationExecutor\n        with mock.patch('django.db.migrations.executor.MigrationLoader') as MockLoader, \\\n             mock.patch('django.db.migrations.executor.MigrationRecorder') as MockRecorder:\n            # Mock the graph and nodes for the MigrationLoader\n            mock_loader_instance = MockLoader.return_value\n            mock_loader_instance.graph = MigrationGraph()\n            mock_loader_instance.graph.nodes = {\n                ('migrations', '0001_initial'): mock.Mock(spec=migrations.Migration),\n                ('migrations2', '0001_initial'): mock.Mock(spec=migrations.Migration),\n            }\n            mock_loader_instance.applied_migrations = [('migrations', '0001_initial')]\n\n            # Create an instance of MigrationExecutor\n            executor = MigrationExecutor(connection)\n\n            # Define the plan for the migration\n            plan = [\n                (mock_loader_instance.graph.nodes[('migrations', '0001_initial')], True),\n            ]\n\n            # Define the full plan for the migration\n            full_plan = [\n                (mock_loader_instance.graph.nodes[('migrations', '0001_initial')], True),\n                (mock_loader_instance.graph.nodes[('migrations2', '0001_initial')], True),\n            ]\n\n            # Call the _migrate_all_backwards method\n            state = executor._migrate_all_backwards(plan, full_plan, fake=False)\n\n            # Assert that the migration was unapplied\n            mock_loader_instance.graph.nodes[('migrations', '0001_initial')].unapply.assert_called_once()\n\n            # Assert that the migration was recorded as unapplied\n            MockRecorder.return_value.record_unapplied.assert_called_once_", "    def test_migration_plan_with_replacement(self):\n        # Test the migration plan when a replacement migration is involved\n        targets = [('migrations2', '0002_auto_20220101_0000')]\n        executor = MigrationExecutor(connection)\n        with mock.patch.object(MigrationGraph, 'backwards_plan', return_value=[('migrations2', '0001_initial')]):\n            plan = executor.migration_plan(targets)\n        # Assert that the plan is correct, which should include the replacement migration\n        self.assertEqual(plan[0][0].name, '0001_auto_20220101_0000')\n        self.assertTrue(plan[0][1])\n"], "sample_912": ["def test__parse_annotation():\n    result = _parse_annotation('List[int]')\n    assert len(result) == 3\n    assert isinstance(result[0], pending_xref)\n    assert isinstance(result[1], addnodes.desc_sig_punctuation)\n    assert isinstance(result[2], nodes.Text)\n", "def test_parse_annotation():\n    annotation = \"Dict[str, List[int]]\"\n    result = _parse_annotation(annotation)\n    expected = [\n        pending_xref('', 'Dict', refdomain='py', reftype='class', reftarget='Dict'),\n        desc_sig_punctuation('', '['),\n        pending_xref('', 'str', refdomain='py', reftype='class', reftarget='str'),\n        desc_sig_punctuation('', ', '),\n        pending_xref('', 'List', refdomain='py', reftype='class', reftarget='List'),\n        desc_sig_punctuation('', '['),\n        pending_xref('', 'int', refdomain='py', reftype='class', reftarget='int'),\n        desc_sig_punctuation('', ']'),\n        desc_sig_punctuation('', ']'),\n    ]\n    assert len(result) == len(expected)\n    for node, exp_node in zip(result, expected):\n        assert isinstance(node, exp_node.__class__)\n        if isinstance(node, nodes.Text):\n            assert node.astext() == exp_node.astext()\n        elif isinstance(node, pending_xref):\n            assert node['reftarget'] == exp_node['reftarget']\n        else:\n            assert node.astext() == exp_node.astext()\n", "def test_parse_annotation(app):\n    app.setup_extension('sphinx.domains.python')\n\n    annotation = 'List[Optional[str]]'\n    result = _parse_annotation(annotation)\n\n    assert len(result) == 6\n    assert_node(result[0], [nodes.Text, pending_xref], text='List')\n    assert_node(result[1], [desc_sig_punctuation], text='[')\n    assert_node(result[2], [nodes.Text, pending_xref], text='Optional')\n    assert_node(result[3], [desc_sig_punctuation], text='[')\n    assert_node(result[4], [nodes.Text, pending_xref], text='str')\n    assert_node(result[5], [desc_sig_punctuation], text=']]')\n", "def test__parse_annotation():\n    annotation = \"List[Optional[str]]\"\n    result = _parse_annotation(annotation)\n    assert len(result) == 5\n    assert isinstance(result[0], pending_xref)\n    assert result[0]['reftarget'] == \"List\"\n    assert isinstance(result[1], addnodes.desc_sig_punctuation)\n    assert result[1].astext() == \"[\"\n    assert isinstance(result[2], pending_xref)\n    assert result[2]['reftarget'] == \"Optional\"\n    assert isinstance(result[3], addnodes.desc_sig_punctuation)\n    assert result[3].astext() == \"[\"\n    assert isinstance(result[4], pending_xref)\n    assert result[4]['reftarget'] == \"str\"\n", "def test_parse_annotation():\n    annotation = 'List[Union[str, int]]'\n    nodes = _parse_annotation(annotation)\n    assert len(nodes) == 1\n    assert isinstance(nodes[0], pending_xref)\n    assert nodes[0]['reftype'] == 'class'\n    assert nodes[0]['reftarget'] == 'List'\n    assert nodes[0].astext() == 'List'\n    assert len(nodes[0]) == 1\n    assert isinstance(nodes[0][0], nodes.Text)\n    assert nodes[0][0].astext() == 'List[Union[str, int]]'\n"], "sample_155": ["    def test_file_response_with_streaming_content(self):\n        streaming_content = iter(['Streaming content 1', 'Streaming content 2'])\n        response = FileResponse(streaming_content, as_attachment=True, filename='test.txt')\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response['Content-Disposition'], 'attachment; filename=\"test.txt\"')\n        self.assertEqual(response['Content-Type'], 'text/plain; charset=utf-8')\n        self.assertEqual(b''.join(response.streaming_content), b'Streaming content 1Streaming content 2')\n", "def test_file_response_as_attachment(self):\n    content = b\"This is a test content.\"\n    response = FileResponse(io.BytesIO(content), as_attachment=True, filename='test.txt')\n    self.assertEqual(response['Content-Disposition'], 'attachment; filename=\"test.txt\"')\n    self.assertEqual(response.content, content)\n", "def test_file_response_with_filelike_object(self):\n    content = b'File content'\n    filelike = io.BytesIO(content)\n    response = FileResponse(filelike)\n    self.assertEqual(response.getvalue(), content)\n", "    def test_file_response_headers(self):\n        content = b'File content'\n        with tempfile.NamedTemporaryFile(delete=False) as temp:\n            temp.write(content)\n            temp_name = temp.name\n\n        response = FileResponse(open(temp_name, 'rb'))\n        response.set_headers(open(temp_name, 'rb'))\n        self.assertEqual(response['Content-Length'], str(len(content)))\n        self.assertEqual(response['Content-Type'], 'application/octet-stream')\n        self.assertEqual(response['Content-Disposition'], f'inline; filename=\"{os.path.basename(temp_name)}\"')\n\n        os.remove(temp_name)\n", "    def test_file_response_streaming_content(self):\n        content = b'This is a test file content.'\n        filelike = io.BytesIO(content)\n        response = FileResponse(filelike)\n\n        self.assertEqual(b''.join(response.streaming_content), content)\n\n        # Check if the file was closed after streaming\n        with self.assertRaises(ValueError):\n            filelike.getvalue()\n"], "sample_625": ["def test_apply_ufunc_with_variable_input():\n    var1 = xr.Variable((\"x\", \"y\"), np.random.rand(3, 4))\n    var2 = xr.Variable((\"x\", \"y\"), np.random.rand(3, 4))\n    result = apply_ufunc(np.add, var1, var2)\n    assert_array_equal(result.data, var1.data + var2.data)\n", "def test_unified_dim_sizes_with_duplicate_dims():\n    data = np.array([1, 2, 3])\n    variable = xr.Variable(dims=(\"dim\", \"dim\"), data=data)\n    with pytest.raises(ValueError, match=\"broadcasting cannot handle duplicate dimensions\"):\n        unified_dim_sizes([variable])\n", "def test_apply_ufunc_with_variable_input():\n    from xarray import Variable, DataArray\n\n    a = Variable((\"x\", \"y\"), np.array([[1, 2], [3, 4]]))\n    b = Variable((\"x\", \"y\"), np.array([[5, 6], [7, 8]]))\n    func = np.add\n    signature = _UFuncSignature([[\"x\", \"y\"], [\"x\", \"y\"]], [[\"x\", \"y\"]])\n    exclude_dims = set()\n    dask = \"forbidden\"\n    vectorize = False\n    keep_attrs = \"override\"\n    dask_gufunc_kwargs = None\n    output_dtypes = None\n\n    result = apply_ufunc(\n        func,\n        a,\n        b,\n        signature=signature,\n        exclude_dims=exclude_dims,\n        dask=dask,\n        vectorize=vectorize,\n        keep_attrs=keep_attrs,\n        dask_gufunc_kwargs=dask_gufunc_kwargs,\n        output_dtypes=output_dtypes,\n    )\n\n    expected_result = Variable((\"x\", \"y\"), np.array([[6, 8], [10, 12]]))\n    assert_identical(result, expected_result)\n", "def test_unify_chunks():\n    # Test case with single DataArray\n    da = xr.DataArray(np.random.rand(10, 10), dims=('x', 'y'), chunks=(2, 3))\n    unified_da, = unify_chunks(da)\n    assert all(dim in unified_da.chunks for dim in da.chunks)\n\n    # Test case with Dataset\n    ds = xr.Dataset({'a': da, 'b': da})\n    unified_ds, = unify_chunks(ds)\n    assert all(dim in unified_ds['a'].chunks for dim in da.chunks)\n    assert all(dim in unified_ds['b'].chunks for dim in da.chunks)\n\n    # Test case with multiple DataArrays\n    da2 = xr.DataArray(np.random.rand(10, 10), dims=('x', 'y'), chunks=(3, 2))\n    unified_da1, unified_da2 = unify_chunks(da, da2)\n    assert unified_da1.chunks == unified_da2.chunks\n\n    # Test case with Dataset and DataArray\n    unified_ds, unified_da = unify_chunks(ds, da)\n    assert unified_ds['a'].chunks == unified_da.chunks\n", "def test_unified_dim_sizes():\n    var1 = xr.Variable((\"x\", \"y\"), np.ones((3, 4)))\n    var2 = xr.Variable((\"y\", \"z\"), np.ones((4, 5)))\n    dim_sizes = unified_dim_sizes([var1, var2])\n    assert dim_sizes == {\"y\": 4}\n\n    var3 = xr.Variable((\"x\", \"y\"), np.ones((3, 5)))\n    with pytest.raises(ValueError):\n        unified_dim_sizes([var1, var3])\n\n    var4 = xr.Variable((\"x\", \"x\"), np.ones((3, 3)))\n    with pytest.raises(ValueError):\n        unified_dim_sizes([var4])\n"], "sample_137": ["    def test_parse_docstring(self):\n        title, body, metadata = parse_docstring(self.docstring)\n        self.assertEqual(title, \"This __doc__ output is required for testing. I copied this example from `admindocs` documentation.\")\n        self.assertIn(\"Display an individual :model:`myapp.MyModel`.\", body)\n        self.assertIn(\"**Context**\", body)\n        self.assertIn(\"**Template:**\", body)\n        self.assertIn(\":template:`myapp/my_template.html`\", body)\n        self.assertIn(\"some_metadata\", metadata)\n        self.assertEqual(metadata[\"some_metadata\"], \"some data\")\n", "    def test_parse_docstring(self):\n        title, body, metadata = parse_docstring(self.docstring)\n        self.assertEqual(title, \"This __doc__ output is required for testing. I copied this example from `admindocs` documentation.\")\n        self.assertEqual(body, \"Display an individual :model:`myapp.MyModel`.\\n\\n**Context**\\n\\n``RequestContext``\\n\\n``mymodel``\\n    An instance of :model:`myapp.MyModel`.\\n\\n**Template:**\\n\\n:template:`myapp/my_template.html`\")\n        self.assertEqual(metadata, {'some_metadata': 'some data'})\n", "    def test_parse_docstring(self):\n        title, body, metadata = parse_docstring(self.docstring)\n        self.assertEqual(title, 'This __doc__ output is required for testing. I copied this example from `admindocs` documentation.')\n        self.assertIn('Display an individual :model:`myapp.MyModel`.', body)\n        self.assertIn('some_metadata: some data', metadata)\n", "    def test_parse_docstring(self):\n        title, body, metadata = parse_docstring(self.docstring)\n        self.assertEqual(title, \"This __doc__ output is required for testing. I copied this example from `admindocs` documentation.\")\n        self.assertIn(\"Display an individual :model:`myapp.MyModel`.\", body)\n        self.assertIn(\"**Context**\", body)\n        self.assertIn(\"**Template:**\", body)\n        self.assertIn(\":template:`myapp/my_template.html`\", body)\n        self.assertEqual(metadata, {'some_metadata': 'some data'})\n", "    def test_parse_docstring(self):\n        title, body, metadata = parse_docstring(self.docstring)\n        self.assertEqual(title, \"This __doc__ output is required for testing. I copied this example from `admindocs` documentation.\")\n        self.assertTrue(\"Display an individual :model:`myapp.MyModel`.\" in body)\n        self.assertTrue(\"some_metadata: some data\" in metadata)\n"], "sample_125": ["def test_set_cookie_max_age_and_expires(self):\n    response = HttpResponse()\n    response.set_cookie('name', 'value', max_age=10)\n    cookie = response.cookies['name']\n    self.assertEqual(cookie.value, 'value')\n    self.assertEqual(cookie['max-age'], '10')\n    self.assertEqual(cookie['expires'], http_date(time.time() + 10))\n\n    response = HttpResponse()\n    response.set_cookie('name', 'value', expires=datetime.now(utc) + timedelta(seconds=20))\n    cookie = response.cookies['name']\n    self.assertEqual(cookie.value, 'value')\n    self.assertEqual(cookie['max-age'], '20')\n    self.assertEqual(cookie['expires'], http_date(time.time() + 20))\n", "    def test_set_cookie_expires_datetime_naive(self):\n        response = HttpResponse()\n        expires = datetime.utcnow() + timedelta(days=1)\n        response.set_cookie('key', 'value', expires=expires)\n        self.assertIn('expires', response.cookies['key'])\n        self.assertEqual(response.cookies['key']['expires'], http_date(time.mktime(expires.timetuple())))\n", "    def test_set_cookie_samesite_invalid(self):\n        response = HttpResponse()\n        with self.assertRaisesMessage(ValueError, 'samesite must be \"lax\", \"none\", or \"strict\".'):\n            response.set_cookie('key', 'value', samesite='invalid')\n", "    def test_set_cookie_with_datetime_expires(self):\n        response = HttpResponse()\n        expires = datetime(2022, 1, 1, tzinfo=utc)\n        response.set_cookie('key', 'value', expires=expires)\n        self.assertEqual(response.cookies['key']['expires'], http_date(time.mktime(expires.timetuple())))\n        self.assertEqual(response.cookies['key']['max-age'], '31536001')  # 1 year from now\n", "    def test_http_response_redirect_allowed_schemes(self):\n        with self.assertRaises(DisallowedRedirect):\n            HttpResponseRedirect('ftp://example.com')\n        with self.assertRaises(DisallowedRedirect):\n            HttpResponseRedirect('file:///etc/passwd')\n        # Ensure allowed schemes are not raising exceptions\n        HttpResponseRedirect('http://example.com')\n        HttpResponseRedirect('https://example.com')\n"], "sample_457": ["    def test_check_constraint_validation(self):\n        product = Product(name='Test', price=-5)\n        with self.assertRaisesMessage(ValidationError, \"Check constraint 'price_non_negative' is violated.\"):\n            product.full_clean()\n", "    def test_check_constraint(self):\n        constraint = CheckConstraint(\n            check=models.Q(discount__gt=F(\"unit_price\")),\n            name=\"valid_discount\",\n        )\n        product = Product(name=\"Test\", unit_price=10, discount=5)\n        with self.assertRaises(ValidationError):\n            product.full_clean()\n\n        product.discount = 15\n        product.full_clean()\n", "    def test_unique_constraint_with_expressions(self):\n        constraint = UniqueConstraint(Lower('name'), name='unique_lower_name')\n        self.assertTrue(constraint.contains_expressions)\n\n        # Create a product with a unique lower name\n        product = Product.objects.create(name='Test')\n        self.assertIsNotNone(product.id)\n\n        # Try to create another product with the same lower name\n        with self.assertRaises(IntegrityError):\n            with atomic():\n                Product.objects.create(name='TEST')\n\n        # Try to create a product with the same lower name using a different case\n        # but different in Unicode\n        with self.assertRaises(IntegrityError):\n            with atomic():\n                Product.objects.create(name='T\u00ebst')\n", "    def setUp(self):\n        self.product = Product.objects.create(name='Test', price=10.0)\n", "    def test_unique_constraint_validation_with_expressions(self):\n        with self.assertRaises(ValidationError):\n            product = UniqueConstraintProduct(lower_name=\"Test\", value=1)\n            product.full_clean()\n            product.save()\n            product = UniqueConstraintProduct(lower_name=\"Test\", value=2)\n            product.full_clean()\n"], "sample_67": ["def test_model_to_dict(self):\n    author = Author.objects.create(name=\"Test Author\")\n    author_dict = model_to_dict(author)\n    self.assertEqual(author_dict, {'name': 'Test Author', 'age': None, 'birth_date': None})\n", "def test_validate_unique_with_custom_error_message(self):\n    CustomErrorMessage.objects.create(name1='foo')\n    form = CustomErrorMessageForm(data={'name1': 'foo'})\n    form.full_clean()\n    self.assertEqual(form.errors, {'name1': ['Form custom error message.']})\n", "def test_model_form_with_media(self):\n    \"\"\"\n    Test the rendering of media for a ModelForm with a custom Media class.\n    \"\"\"\n    form = ModelFormWithMedia()\n    rendered_media = form.media.render()\n    self.assertIn('/some/form/javascript', rendered_media)\n    self.assertIn('/some/form/css', rendered_media)\n", "def test_custom_error_message_form(self):\n    form = CustomErrorMessageForm({'name1': 'invalid_value'})\n    self.assertFalse(form.is_valid())\n    self.assertEqual(form.errors['name1'], ['Form custom error message.'])\n", "    def test_has_changed(self):\n        form = InventoryForm(data={'barcode': '123', 'item_name': 'Test Item'})\n        self.assertTrue(form.has_changed(Inventory(barcode='321', item_name='Test Item'), form.data))\n        self.assertFalse(form.has_changed(Inventory(barcode='123', item_name='Test Item'), form.data))\n"], "sample_627": ["def test_concat_with_different_coords():\n    ds1 = Dataset({\"x\": (\"a\", [1, 2, 3])}, coords={\"y\": (\"a\", [4, 5, 6])})\n    ds2 = Dataset({\"x\": (\"b\", [4, 5, 6])}, coords={\"y\": (\"b\", [7, 8, 9])})\n    result = concat([ds1, ds2], dim=\"c\")\n    expected_y = DataArray([4, 5, 6, 7, 8, 9], dims=(\"c\",), name=\"y\")\n    assert_equal(result.y, expected_y)\n", "def test_concat_with_fill_value():\n    arrays = [\n        DataArray([1, 2, 3], dims=\"x\"),\n        DataArray([4, 5, 6], dims=\"x\"),\n    ]\n    expected = DataArray([1, 2, 3, 4, 5, 6], dims=\"x\")\n    result = concat(arrays, \"x\", fill_value=-99)\n    assert_array_equal(result, expected)\n    assert result.dtype == np.int64\n\n    arrays = [\n        DataArray([1, 2, 3], dims=\"x\"),\n        DataArray([4, 5, np.nan], dims=\"x\"),\n    ]\n    expected = DataArray([1, 2, 3, 4, 5, -99], dims=\"x\")\n    result = concat(arrays, \"x\", fill_value=-99)\n    assert_array_equal(result, expected)\n    assert result.dtype == np.float64\n", "def test_concat_with_different_coords():\n    # Test concatenating datasets with different coordinates\n    ds1 = Dataset(\n        data_vars={\"var\": ([\"x\", \"y\"], np.random.rand(1, 4))},\n        coords={\"lat\": ([\"x\", \"y\"], np.random.rand(1, 4)), \"coord1\": (\"x\", np.arange(1))},\n    )\n    ds2 = Dataset(\n        data_vars={\"var\": ([\"x\", \"y\"], np.random.rand(1, 4))},\n        coords={\"lat\": ([\"x\", \"y\"], np.random.rand(1, 4)), \"coord2\": (\"x\", np.arange(1, 2))},\n    )\n    result = concat([ds1, ds2], dim=\"x\", coords=\"different\")\n    assert \"coord1\" in result.coords\n    assert \"coord2\" in result.coords\n    assert_equal(result.coords[\"coord1\"], ds1.coords[\"coord1\"])\n    assert_equal(result.coords[\"coord2\"], ds2.coords[\"coord2\"])\n", "def test_concat_different_index_coords():\n    # Create datasets with different index coordinates\n    ds1 = Dataset({\"x\": (\"index1\", [1, 2])})\n    ds2 = Dataset({\"x\": (\"index2\", [3, 4])})\n\n    # Concatenate datasets with coords='different'\n    result = concat([ds1, ds2], dim=\"dim\", coords=\"different\")\n\n    # Check that the index coordinates are preserved\n    assert \"index1\" in result.coords\n    assert \"index2\" in result.coords\n    assert_equal(result.coords[\"index1\"], ds1.coords[\"index1\"])\n    assert_equal(result.coords[\"index2\"], ds2.coords[\"index2\"])\n\n    # Check that the data variables are concatenated correctly\n    assert_equal(result[\"x\"], DataArray([1, 2, 3, 4], dims=\"dim\"))\n\n    # Check that the dimensions are concatenated correctly\n    assert_equal(result.dims[\"dim\"], 4)\n", "def test_concat_join(join: JoinOptions, expected_len: int) -> None:\n    datasets = create_concat_datasets()\n\n    if isinstance(expected_len, type) and issubclass(expected_len, Exception):\n        with expected_len:\n            result = concat(datasets, dim=\"new_dim\", join=join)\n    else:\n        result = concat(datasets, dim=\"new_dim\", join=join)\n        assert len(result[\"day\"]) == expected_len\n\n    if join in [\"outer\", \"left\"]:\n        assert_array_equal(result[\"lat\"], datasets[0][\"lat\"])\n        assert_array_equal(result[\"lon\"], datasets[0][\"lon\"])\n    elif join == \"inner\":\n        assert \"lat\" not in result\n        assert \"lon\" not in result\n    elif join == \"right\":\n        assert_array_equal(result[\"lat\"], datasets[1][\"lat\"])\n        assert_array_equal(result[\"lon\"], datasets[1][\"lon\"])\n"], "sample_606": ["def test_broadcast_compat_data():\n    variable = xr.Variable((\"x\", \"y\"), np.arange(6).reshape(2, 3))\n    broadcast_dims = (\"z\",)\n    core_dims = (\"x\", \"y\")\n\n    result = broadcast_compat_data(variable, broadcast_dims, core_dims)\n    expected = np.arange(6).reshape(1, 2, 3)\n\n    assert_identical(result, expected)\n", "def test_broadcast_compat_data():\n    var = xr.Variable((\"x\", \"y\"), np.arange(12).reshape(3, 4))\n    broadcast_dims = (\"z\",)\n    core_dims = (\"x\", \"y\")\n\n    data = broadcast_compat_data(var, broadcast_dims, core_dims)\n\n    assert data.shape == (1, 3, 4)\n    assert_array_equal(data, var.data[None, :, :])\n", "def test_apply_ufunc_with_signature():\n    signature = _UFuncSignature([[\"x\", \"y\"], []], [[\"z\"]])\n    data1 = np.array([[1, 2, 3], [4, 5, 6]])\n    data2 = np.array([7, 8, 9])\n    var1 = xr.Variable([\"x\", \"y\"], data1)\n    var2 = xr.Variable([\"y\"], data2)\n    result = broadcast_compat_data(var1, (\"x\",), (\"x\", \"y\"))\n    assert_identical(result, data1)\n    result = broadcast_compat_data(var2, (\"x\",), (\"y\",))\n    assert_identical(result, np.array([[7, 8, 9], [7, 8, 9]]))\n", "def test_unified_dim_sizes():\n    var1 = xr.Variable(dims=(\"x\", \"y\"), data=np.zeros((3, 4)))\n    var2 = xr.Variable(dims=(\"x\", \"z\"), data=np.zeros((3, 5)))\n    var3 = xr.Variable(dims=(\"x\",), data=np.zeros(3))\n\n    dim_sizes = unified_dim_sizes([var1, var2, var3])\n    assert dim_sizes == {\"x\": 3}\n\n    # Test with duplicate dimensions\n    var4 = xr.Variable(dims=(\"x\", \"x\"), data=np.zeros((3, 3)))\n    with pytest.raises(ValueError, match=\"broadcasting cannot handle duplicate dimensions\"):\n        unified_dim_sizes([var1, var4])\n\n    # Test with mismatched lengths\n    var5 = xr.Variable(dims=(\"x\",), data=np.zeros(4))\n    with pytest.raises(ValueError, match=\"operands cannot be broadcast together with mismatched lengths\"):\n        unified_dim_sizes([var1, var5])\n", "def test_broadcast_compat_data(signature):\n    shape = (3, 2)\n    dims = (\"x\", \"y\")\n    data = np.arange(np.product(shape)).reshape(shape)\n    var = xr.Variable(dims, data)\n\n    broadcast_dims = (\"z\",)\n    output_data = broadcast_compat_data(var, broadcast_dims, signature.input_core_dims[0])\n    expected_shape = (3, 2, 1) if \"x\" in signature.output_core_dims[0] else (1, 3, 2)\n    assert output_data.shape == expected_shape\n"], "sample_867": ["def test_parameter_grid_with_dicts_of_lists(klass):\n    param_grid = [{'foo': [1, 2]}, {'bar': ['one', 'two']}]\n    grid = klass(param_grid)\n    assert_grid_iter_equals_getitem(grid)\n    assert list(grid) == [{'foo': 1}, {'foo': 2}, {'bar': 'one'}, {'bar': 'two'}]\n", "compilation error", "def test_parameter_grid_with_ndarray_parameter(klass):\n    param_grid = {'arr_param': np.array([[1, 2], [3, 4]])}\n    with pytest.raises(ValueError, match=\"Parameter array should be one-dimensional.\"):\n        klass(param_grid)\n", "def test_parameter_grid_single_dict(klass):\n    param_grid = {'a': [1, 2], 'b': [True, False]}\n    grid = klass(param_grid)\n    expected = [{'a': 1, 'b': True}, {'a': 1, 'b': False}, {'a': 2, 'b': True}, {'a': 2, 'b': False}]\n    assert list(grid) == expected\n    assert_grid_iter_equals_getitem(grid)\n", "def test_parameter_grid_single_dict(klass):\n    param_grid = {'a': [1, 2], 'b': [True, False]}\n    grid = klass(param_grid)\n    expected_params = [{'a': 1, 'b': True}, {'a': 1, 'b': False},\n                      {'a': 2, 'b': True}, {'a': 2, 'b': False}]\n    assert list(grid) == expected_params\n    assert_grid_iter_equals_getitem(grid)\n    assert len(grid) == len(expected_params)\n"], "sample_178": ["    def test_duplicate_drinks(self):\n        formset_data = [{'name': 'Coffee'}, {'name': 'Tea'}, {'name': 'Coffee'}]\n        formset = FavoriteDrinksFormSet(data={'form-TOTAL_FORMS': '3', 'form-INITIAL_FORMS': '0',\n                                              'form-MAX_NUM_FORMS': '', 'form-MIN_NUM_FORMS': '',\n                                              'form-0-name': 'Coffee', 'form-1-name': 'Tea',\n                                              'form-2-name': 'Coffee'})\n        self.assertFalse(formset.is_valid())\n        self.assertEqual(formset.non_form_errors(), ['You may only specify a drink once.'])\n", "def test_formset_custom_kwargs(self):\n    CustomKwargFormSet = formset_factory(CustomKwargForm, formset=BaseFormSet, extra=2, form_kwargs={'custom_kwarg': 'test'})\n    formset = CustomKwargFormSet()\n    for form in formset:\n        self.assertEqual(form.custom_kwarg, 'test')\n", "    def test_formset_factory_with_custom_kwargs(self):\n        CustomKwargFormSet = formset_factory(CustomKwargForm, form_kwargs={'custom_kwarg': 'test'})\n        formset = CustomKwargFormSet()\n        for form in formset:\n            self.assertEqual(form.custom_kwarg, 'test')\n", "def test_cleaned_data_property(self):\n    formset_data = [('Drink1', 1), ('Drink2', 2)]\n    formset = self.make_choiceformset(formset_data=formset_data)\n    formset.is_valid()  # Trigger full_clean\n    cleaned_data = formset.cleaned_data\n    self.assertEqual(cleaned_data, [{'choice': 'Drink1', 'votes': 1}, {'choice': 'Drink2', 'votes': 2}])\n", "def test_custom_kwargs_in_formset(self):\n    custom_kwarg_value = \"custom_kwarg_value\"\n    CustomKwargFormSet = formset_factory(CustomKwargForm, formset=BaseFormSet, extra=2, form_kwargs={'custom_kwarg': custom_kwarg_value})\n    formset = CustomKwargFormSet()\n    for form in formset:\n        self.assertEqual(form.custom_kwarg, custom_kwarg_value)\n"], "sample_1124": ["def test_evaluate():\n    _, x, y, z = field(\"x,y,z\", QQ)\n    f = (x**2 + y) / (z + 1)\n    result = f.evaluate([(x, 2), (y, 3), (z, 4)])\n    assert result == Rational(7, 5)\n", "def test_sfield_rational_function():\n    x, y = symbols('x y')\n    K, f = sfield((x*log(x) + 4*x**2)*exp(1/x + log(x)/3)/x**2)\n    assert isinstance(K, FracField)\n    assert isinstance(f, FracElement)\n    assert f.as_expr(x, y) == (4*x**2*exp(1/x) + x*exp(1/x)*log(x))/(x**(1/3))**5\n", "def test_sfield_with_exp_log():\n    K, f = sfield((x*log(x) + 4*x**2)*exp(1/x + log(x)/3)/x**2)\n    assert K == FracField([x, exp(1/x), log(x), x**(Rational(1, 3))], QQ, lex)\n    assert f == FracElement((4*x**2*(exp(1/x)) + x*(exp(1/x))*(log(x))), (x**(Rational(1, 3)))**5, field=K)\n", "def test_fracfield_methods():\n    _, x, y = field(\"x, y\", ZZ)\n    f = (x**2 + y)/(x - 1)\n    assert f.diff(x) == 2*x/(x - 1) + (x**2 + y)/(x - 1)**2\n    assert f(x, 2) == 3\n    assert f.subs(x, 2) == (4 + y)/(1)\n    assert f.evaluate([(x, 2)]) == 3\n", "def test_evaluate_with_ground_field():\n    _, x, y = field(\"x,y\", QQ)\n    f = (x**2 + y)/(x - 2)\n    result = f.evaluate([(x, Rational(3, 2)), (y, Rational(1, 2))])\n    assert result == Rational(9, 5)\n"], "sample_100": ["    def test_zip_imported_modules(self):\n        zip_file = self.temporary_file('test.zip')\n        with zipfile.ZipFile(zip_file, 'w') as z:\n            z.writestr('test_module.py', 'test_content = \"test\"')\n        sys.path.insert(0, str(zip_file))\n        self.addCleanup(lambda: sys.path.remove(str(zip_file)))\n        self.import_and_cleanup('test_module')\n        self.assertFileFound(zip_file)\n", "def test_zip_imported_module(self):\n    zip_file = self.temporary_file('test.zip')\n    with zipfile.ZipFile(zip_file, 'w') as zf:\n        zf.writestr('test_module.py', 'print(\"Hello, World!\")')\n\n    sys.path.append(str(zip_file))\n    self.addCleanup(sys.path.remove, str(zip_file))\n\n    self.import_and_cleanup('test_module')\n    self.assertFileFound(zip_file)\n", "    def test_module_without_spec(self):\n        name = 'test_module_without_spec'\n        self.import_and_cleanup(name)\n        module = sys.modules[name]\n        module.__spec__ = None\n        self.assertFileNotFound(Path(module.__file__))\n", "def test_error_file(self):\n    filename = self.temporary_file('error_file.py')\n    with open(filename, 'w') as f:\n        f.write('invalid syntax:')\n\n    with self.assertRaises(SyntaxError):\n        self.import_and_cleanup('error_file')\n\n    self.assertFileNotFound(filename)\n\n    # After fixing the syntax error, the file should be found\n    with open(filename, 'w') as f:\n        f.write('')\n\n    self.import_and_cleanup('error_file')\n    self.assertFileFound(filename)\n", "    def test_import_error(self):\n        filename = self.temporary_file('error_module.py')\n        filename.write_text('syntax error')\n        sys.path.insert(0, str(filename.parent))\n        self.addCleanup(sys.path.remove, str(filename.parent))\n        # This will trigger an ImportError and add the file to _error_files\n        with self.assertRaises(ImportError):\n            self.import_and_cleanup('error_module')\n        # Check that the file is in _error_files\n        self.assertIn(str(filename), autoreload._error_files)\n        self.assertFileFound(filename)\n        # Remove the file from _error_files\n        autoreload._error_files.remove(str(filename))\n        # Check that the file is not found anymore\n        self.assertFileNotFound(filename)\n"], "sample_755": ["def test_silhouette_samples_precomputed_zero_diagonal():\n    X = np.array([[0., 1., 2.], [1., 0., 3.], [2., 3., 0.]])\n    labels = np.array([0, 1, 1])\n    metric = \"precomputed\"\n\n    # Adding non-zero values to the diagonal should raise an error\n    X_non_zero_diag = X.copy()\n    np.fill_diagonal(X_non_zero_diag, 1)\n    with pytest.raises(ValueError, match=\"The precomputed distance matrix contains non-zero elements on the diagonal.\"):\n        silhouette_samples(X_non_zero_diag, labels, metric=metric)\n\n    # Zero diagonal should not raise an error\n    silhouette_samples(X, labels, metric=metric)\n", "def test_davies_bouldin_score():\n    X = np.array([[1, 2], [1, 4], [1, 0], [4, 2], [4, 4], [4, 0]])\n    labels = np.array([0, 0, 0, 1, 1, 1])\n    score = davies_bouldin_score(X, labels)\n    expected_score = 0.6666666666666666\n    assert np.isclose(score, expected_score)\n", "def test_silhouette_score_sparse_input():\n    X = csr_matrix([[0, 1], [1, 0]])\n    labels = np.array([0, 1])\n    score = silhouette_score(X, labels, metric=\"cosine\")\n    assert np.isclose(score, -0.5)\n", "def test_silhouette_score_sparse_input():\n    # Test silhouette_score with sparse input\n    X = csr_matrix([[1, 0], [2, 0], [1, 1], [2, 2]])\n    labels = [0, 0, 1, 1]\n    score = silhouette_score(X, labels, metric='euclidean')\n    assert isinstance(score, float)\n    assert -1 <= score <= 1\n", "def test_silhouette_score_with_sample_size():\n    # Generate a synthetic dataset\n    X, y = datasets.make_blobs(n_samples=500, centers=4, random_state=42)\n\n    # Calculate silhouette score with and without sample size\n    score_full = silhouette_score(X, y)\n    score_sample = silhouette_score(X, y, sample_size=200)\n\n    # Check that the scores are close but not equal\n    assert np.isclose(score_full, score_sample, atol=0.1)\n    assert np.abs(score_full - score_sample) > 0.01\n"], "sample_879": ["def test_onehotencoder_infrequent_categories():\n    X = np.array([[\"a\"] * 5 + [\"b\"] * 20 + [\"c\"] * 10 + [\"d\"] * 3], dtype=object).T\n    ohe = OneHotEncoder(max_categories=3, sparse_output=False).fit(X)\n    assert_array_equal(ohe.infrequent_categories_, [[\"a\", \"d\"]])\n    assert_allclose(ohe.transform([[\"a\"], [\"b\"]]), [[0., 0., 1.], [1., 0., 0.]])\n", "def test_ordinal_encoder_encoded_missing_value():\n    X = [['Male', 1], ['Female', np.nan], ['Female', 2]]\n    enc = OrdinalEncoder(encoded_missing_value=-1)\n    enc.fit(X)\n    X_trans = enc.transform(X)\n    expected_trans = np.array([[1, 0], [-1, -1], [0, 1]])\n    assert_array_equal(X_trans, expected_trans)\n", "def test_ordinal_encoder_with_unknown_value():\n    enc = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n    X = [['Male', 1], ['Female', 3], ['Female', 2], ['Unknown', 4]]\n    enc.fit(X)\n    assert_array_equal(enc.transform([['Female', 3], ['Male', 1], ['Unknown', 4]]),\n                       [[0., 1.], [1., 0.], [-1., -1.]])\n    assert_array_equal(enc.inverse_transform([[1, 0], [0, 1], [-1, -1]]),\n                       [['Male', 1], ['Female', 2], [None, None]])\n", "def test_ordinal_encoder_encoded_missing_value():\n    enc = OrdinalEncoder(encoded_missing_value=-1)\n    X = [[np.nan], [1], [2], [np.nan]]\n    enc.fit(X)\n    assert_array_equal(enc.transform(X), [[-1], [0], [1], [-1]])\n\n    enc = OrdinalEncoder(encoded_missing_value=np.nan, dtype=float)\n    enc.fit(X)\n    assert_array_equal(enc.transform(X), [[np.nan], [0], [1], [np.nan]])\n\n    # Check that encoded_missing_value is used in inverse_transform\n    enc = OrdinalEncoder(encoded_missing_value=-1)\n    enc.fit(X)\n    X_trans = enc.transform(X)\n    assert_array_equal(enc.inverse_transform(X_trans), [[np.nan], [1], [2], [np.nan]])\n\n    enc = OrdinalEncoder(encoded_missing_value=np.nan, dtype=float)\n    enc.fit(X)\n    X_trans = enc.transform(X)\n    assert_array_equal(enc.inverse_transform(X_trans), [[np.nan], [1], [2], [np.nan]])\n\n    # Check that an error is raised when encoded_missing_value is already used to encode a known category\n    X = [[1], [2], [np.nan]]\n    enc = OrdinalEncoder(encoded_missing_value=1)\n    with pytest.raises(ValueError, match=\"encoded_missing_value .* is already used to encode a known category\"):\n        enc.fit(X)\n\n    # Check that an error is raised when dtype is not float and encoded_missing_value is np.nan\n    X = [[1], [np.nan]]\n    enc = OrdinalEncoder(encoded_missing_value=np.nan)\n    with pytest.raises(ValueError, match=\"There are missing values in features .* For OrdinalEncoder to encode missing values\"):\n        enc.fit(X)\n", "def test_ordinal_encoder_inverse_transform_unknown_values():\n    enc = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=99)\n    X = [['Male', 1], ['Female', 3], ['Female', 2]]\n    enc.fit(X)\n    X_transformed = enc.transform(X)\n    X_inverse = enc.inverse_transform(X_transformed)\n    assert_array_equal(X_inverse, X)\n\n    # Test with unknown values\n    X_with_unknown = [['Male', 1], ['Female', 3], ['Unknown', 2]]\n    X_with_unknown_transformed = enc.transform(X_with_unknown)\n    X_with_unknown_inverse = enc.inverse_transform(X_with_unknown_transformed)\n    expected_output = [['Male', 1], ['Female', 3], [None, 2]]\n    assert_array_equal(X_with_unknown_inverse, expected_output)\n"], "sample_335": ["def test_decimal_field_with_max_digits_and_decimal_places(self):\n    field = DecimalField(max_digits=5, decimal_places=2)\n    self.assertEqual(decimal.Decimal('123.45'), field.clean('123.45'))\n    with self.assertRaises(ValidationError):\n        field.clean('1234.56')  # More than 5 digits\n    with self.assertRaises(ValidationError):\n        field.clean('123.456')  # More than 2 decimal places\n", "    def test_decimal_field_to_python(self):\n        field = DecimalField(max_digits=5, decimal_places=2)\n        self.assertEqual(field.to_python('123.45'), decimal.Decimal('123.45'))\n        self.assertEqual(field.to_python('123.456'), decimal.Decimal('123.46'))\n        self.assertEqual(field.to_python('123.4'), decimal.Decimal('123.40'))\n        self.assertIsNone(field.to_python(''))\n        self.assertRaises(ValidationError, field.to_python, 'abc')\n", "    def test_decimal_field_with_decimal_places(self):\n        field = DecimalField(max_digits=5, decimal_places=2)\n        self.assertEqual(field.clean('12.34'), decimal.Decimal('12.34'))\n        self.assertEqual(field.clean('0.0000'), decimal.Decimal('0'))\n        with self.assertRaises(ValidationError):\n            field.clean('12.3456')\n", "    def test_invalid_decimal(self):\n        field = DecimalField()\n        with self.assertRaisesMessage(ValidationError, 'Enter a number.'):\n            field.clean('invalid')\n", "    def test_to_python(self):\n        field = DecimalField()\n        self.assertEqual(field.to_python('123.45'), decimal.Decimal('123.45'))\n        self.assertIsNone(field.to_python(''))\n        with self.assertRaises(ValidationError) as cm:\n            field.to_python('invalid')\n        self.assertEqual(cm.exception.code, 'invalid')\n"], "sample_222": ["def test_file_locking(self):\n    with tempfile.NamedTemporaryFile() as f:\n        self.assertTrue(locks.lock(f, locks.LOCK_EX))\n        self.assertFalse(locks.lock(f, locks.LOCK_EX | locks.LOCK_NB))\n        self.assertTrue(locks.unlock(f))\n        self.assertTrue(locks.lock(f, locks.LOCK_SH))\n        self.assertTrue(locks.unlock(f))\n", "    def test_lock_unlock(self):\n        # Create a temporary file\n        with tempfile.NamedTemporaryFile() as temp:\n            # Check that the file is initially unlocked\n            self.assertFalse(locks.lock(temp, locks.LOCK_EX))\n\n            # Lock the file\n            locks.lock(temp, locks.LOCK_EX)\n\n            # Try to lock the file again, it should fail\n            self.assertFalse(locks.lock(temp, locks.LOCK_EX))\n\n            # Unlock the file\n            locks.unlock(temp)\n\n            # Try to lock the file again, it should succeed\n            self.assertTrue(locks.lock(temp, locks.LOCK_EX))\n\n        # Check that the file is unlocked after it's closed\n        self.assertTrue(locks.unlock(temp))\n", "    def test_file_locking(self):\n        with tempfile.NamedTemporaryFile() as temp:\n            # Write some data to the file\n            temp.write(b'Test data')\n            temp.flush()\n\n            # Lock the file\n            self.assertTrue(locks.lock(temp, locks.LOCK_EX))\n\n            # Try to open the file in exclusive mode (should fail)\n            with self.assertRaises(BlockingIOError if os.name != 'nt' else PermissionError):\n                with open(temp.name, 'w') as f:\n                    pass\n\n            # Unlock the file\n            self.assertTrue(locks.unlock(temp))\n\n            # Try to open the file in exclusive mode (should succeed)\n            with open(temp.name, 'w') as f:\n                pass\n", "    def test_file_locking(self):\n        with tempfile.TemporaryFile() as f:\n            self.assertTrue(locks.lock(f, locks.LOCK_EX))\n            f.write(b'Locked content')\n            self.assertTrue(locks.unlock(f))\n            f.seek(0)\n            self.assertEqual(f.read(), b'Locked content')\n", "    def test_lock_unlock(self):\n        with NamedTemporaryFile() as temp:\n            # Test LOCK_EX\n            self.assertTrue(locks.lock(temp, locks.LOCK_EX))\n            with self.assertRaises(BlockingIOError):\n                locks.lock(temp, locks.LOCK_EX | locks.LOCK_NB)\n            self.assertTrue(locks.unlock(temp))\n\n            # Test LOCK_SH\n            self.assertTrue(locks.lock(temp, locks.LOCK_SH))\n            self.assertTrue(locks.lock(temp, locks.LOCK_SH))\n            self.assertTrue(locks.unlock(temp))\n            self.assertTrue(locks.unlock(temp))\n\n        # Test LOCK_NB\n        if os.name != 'nt':\n            with NamedTemporaryFile() as temp:\n                self.assertTrue(locks.lock(temp, locks.LOCK_EX))\n                self.assertFalse(locks.lock(temp, locks.LOCK_EX | locks.LOCK_NB))\n                self.assertTrue(locks.unlock(temp))\n"], "sample_392": ["    def test_key_transform_text_lookup_mixin(self):\n        key_transform = KeyTransform(\"key\", F(\"field\"))\n        mixin = KeyTransformTextLookupMixin(key_transform)\n        self.assertIsInstance(mixin.lhs, KeyTextTransform)\n        self.assertEqual(mixin.lhs.key_name, \"key\")\n        self.assertEqual(mixin.lhs.lhs, F(\"field\"))\n", "    def test_key_transform_text_lookup_mixin(self):\n        key_transform = KeyTransform(\"key\", models.ExpressionWrapper(models.Value(1), output_field=IntegerField()))\n        mixin = KeyTransformTextLookupMixin(key_transform, \"lookup\")\n        self.assertIsInstance(mixin.lhs, KeyTextTransform)\n        self.assertEqual(mixin.lhs.key_name, \"key\")\n", "    def test_key_transform_factory(self):\n        key_transform = KeyTransformFactory(\"key\")(F(\"field\"))\n        self.assertIsInstance(key_transform, KeyTransform)\n        self.assertEqual(key_transform.key_name, \"key\")\n", "    def test_key_transform_lookup_exact(self):\n        JSONModel.objects.create(json=\"{'key': 'value'}\")\n        JSONModel.objects.create(json=\"{'key': 'other'}\")\n\n        self.assertEqual(JSONModel.objects.filter(json__key__exact='value').count(), 1)\n        self.assertEqual(JSONModel.objects.filter(json__key__exact='other').count(), 1)\n", "    def setUp(self):\n        JSONModel.objects.create(json_field={\"key1\": \"value1\", \"key2\": 2, \"key3\": [1, 2, 3]})\n"], "sample_1109": ["def test_frac_rewrite():\n    assert frac(x).rewrite(floor) == x - floor(x)\n    assert frac(x).rewrite(ceiling) == x + ceiling(-x)\n", "def test_frac_eval_Eq():\n    assert frac(pi) == pi - floor(pi)\n    assert frac(2.5) == 2.5 - floor(2.5)\n    assert frac(n) == Eq(n, 0)\n    assert frac(x) != x - ceiling(x)\n", "def test_frac_relational():\n    assert frac(Rational(4, 3)) == Rational(1, 3)\n    assert frac(-Rational(4, 3)) == Rational(2, 3)\n    assert frac(n) == 0\n    assert frac(x).rewrite(floor) == x - floor(x)\n    assert frac(y + I*x) == I*frac(x) + frac(y)\n    assert frac(oo) == AccumBounds(0, 1)\n    assert frac(-oo) == AccumBounds(0, 1)\n    assert frac(nan) is nan\n    assert frac(zoo) is nan\n    assert frac(S.ComplexInfinity) is S.NaN\n    assert frac(x) >= 0\n    assert frac(x) < 1\n    assert frac(x) <= 0 or frac(x) < 1\n    assert frac(x) > -1\n    assert not (frac(x) >= 1)\n    assert not (frac(x) > 0)\n    assert not (frac(x) <= -1)\n    assert not (frac(x) < 0)\n", "compilation error", "def test_frac_eval_and_relations():\n    assert frac(Rational(4, 3)) == Rational(1, 3)\n    assert frac(-Rational(4, 3)) == Rational(2, 3)\n    assert frac(n) == 0\n    assert frac(x).rewrite(floor) == x - floor(x)\n    assert frac(y + I*x) == I*frac(x) + frac(y)\n    assert frac(oo) == AccumBounds(0, 1)\n    assert frac(nan) is nan\n    assert frac(zoo) is nan\n    assert frac(E) == E - 2\n    assert frac(pi) == pi - 3\n    assert frac(i) == i\n    assert frac(k) == 0\n    assert frac(k + 1) == 0\n    assert frac(sin(x)) == sin(x) - floor(sin(x))\n    assert frac(exp(x)) == exp(x) - floor(exp(x))\n    assert frac(log(x)) == log(x) - floor(log(x))\n    assert frac(factorial(n)) == factorial(n) - floor(factorial(n))\n    assert frac(sqrt(2)) == sqrt(2) - 1\n    assert frac(S.Zero) == 0\n    assert frac(S.One) == 0\n    assert frac(S.NegativeOne) == 0\n    assert frac(S.Half) == S.Half\n    assert frac(S.ImaginaryUnit) == S.ImaginaryUnit\n    assert frac(S.Pi) == S.Pi - 3\n    assert frac(S.Exp1) == S.Exp1 - 2\n    assert frac(S.Infinity) == AccumBounds(0, 1)\n    assert frac(S.ComplexInfinity) is nan\n    assert frac(S.NaN) is nan\n    assert frac(x).is_real is True\n    assert frac(I*x).is_"], "sample_310": ["def test_get_return_data_type(self):\n    self.assertEqual(get_return_data_type('get_list'), 'List')\n    self.assertEqual(get_return_data_type('get_count'), 'Integer')\n    self.assertEqual(get_return_data_type('get_object'), '')\n", "def test_get_return_data_type(self):\n    self.assertEqual(get_return_data_type('get_list'), 'List')\n    self.assertEqual(get_return_data_type('get_count'), 'Integer')\n    self.assertEqual(get_return_data_type('other_function'), '')\n", "def test_get_return_data_type(self):\n    self.assertEqual(get_return_data_type('get_list'), 'List')\n    self.assertEqual(get_return_data_type('get_count'), 'Integer')\n    self.assertEqual(get_return_data_type('other_func'), '')\n", "def test_get_return_data_type(self):\n    self.assertEqual(get_return_data_type('get_list'), 'List')\n    self.assertEqual(get_return_data_type('get_count'), 'Integer')\n    self.assertEqual(get_return_data_type('some_method'), '')\n", "    def test_model_detail_view(self):\n        response = self.client.get(reverse('django-admindocs-model-detail', kwargs={'app_label': 'tests', 'model_name': 'Person'}))\n        self.assertEqual(response.status_code, 200)\n        self.assertContains(response, \"Person object\")\n        self.assertContains(response, \"company\")\n        self.assertContains(response, \"name\")\n        self.assertContains(response, \"get_full_name\")\n"], "sample_1053": ["def test_integer_nthroot():\n    assert integer_nthroot(8, 3) == (2, True)\n", "def test_isqrt():\n    assert isqrt(4) == 2\n    assert isqrt(5) == 2\n    assert isqrt(0) == 0\n", "compilation error", "def test_rational_comparison():\n    a = Rational(1, 2)\n    b = Rational(2, 3)\n    assert a < b\n    assert b > a\n    assert a != b\n", "def test_comparison_with_complex_infinity():\n    assert not (zoo < oo)\n    assert not (zoo <= oo)\n    assert not (zoo > oo)\n    assert not (zoo >= oo)\n    assert not (zoo < -oo)\n    assert not (zoo <= -oo)\n    assert not (zoo > -oo)\n    assert not (zoo >= -oo)\n    assert zoo == zoo\n    assert not (zoo != zoo)\n    assert not (zoo < zoo)\n    assert zoo <= zoo\n    assert not (zoo > zoo)\n    assert zoo >= zoo\n"], "sample_1129": ["def test_print_MatrixSolve():\n    A = MatrixSymbol('A', 3, 3)\n    b = MatrixSymbol('b', 3, 1)\n    expr = MatrixSolve(A, b)\n    printer = PythonCodePrinter()\n    result = printer._print(expr)\n    assert result == \"numpy.linalg.solve(A, b)\"\n", "def test_loggamma():\n    expr = loggamma(x)\n    assert pycode(expr, fully_qualified_modules=False) == \"loggamma(x)\"\n", "def test_print_MatrixSolve():\n    A = MatrixSymbol('A', 3, 3)\n    b = MatrixSymbol('b', 3, 1)\n    expr = MatrixSolve(A, b)\n    result = pycode(expr, standard='python3')\n    assert result == \"numpy.linalg.solve(A, b)\"\n", "def test_MatrixSolve():\n    A = MatrixSymbol('A', 3, 3)\n    b = MatrixSymbol('b', 3, 1)\n    expr = MatrixSolve(A, b)\n    assert pycode(expr, language='numpy') == \"numpy.linalg.solve(A, b)\"\n", "def test_sympy_printer():\n    # Test SymPyPrinter\n    expr = acos(x) + KroneckerDelta(y, z) + Piecewise((x, y > 0), (z, True)) + sign(x) + sqrt(x) + loggamma(x)\n    result = SymPyPrinter().doprint(expr)\n    assert result == \"acos(x) + KroneckerDelta(y, z) + Piecewise((x, y > 0), (z, True)) + sign(x) + sympy.sqrt(x) + loggamma(x)\"\n"], "sample_528": ["def test_context_manager():\n    \"\"\"Test the context manager functionality.\"\"\"\n    with style.context('default'):\n        assert mpl.rcParams[PARAM] != VALUE\n    with style.context('seaborn-colorblind'):\n        assert mpl.rcParams[PARAM] != VALUE\n    with style.context(DUMMY_SETTINGS):\n        assert mpl.rcParams[PARAM] == VALUE\n    with temp_style('temp_style'):\n        with style.context('temp_style'):\n            assert mpl.rcParams[PARAM] == VALUE\n", "def test_use_with_dict():\n    original_cmap = mpl.rcParams[PARAM]\n    assert VALUE != original_cmap\n    with style.context(DUMMY_SETTINGS):\n        assert mpl.rcParams[PARAM] == VALUE\n    assert mpl.rcParams[PARAM] == original_cmap\n", "def test_context_manager():\n    with style.context(DUMMY_SETTINGS):\n        assert mpl.rcParams[PARAM] == VALUE\n    assert mpl.rcParams[PARAM] != VALUE\n", "def test_use_style_file_with_absolute_path():\n    with temp_style(\"temp_style\", settings=DUMMY_SETTINGS) as temp_file_path:\n        style.use(temp_file_path)\n        assert mpl.rcParams[PARAM] == VALUE\n", "def test_use_with_temporary_style():\n    style_name = \"temp_style\"\n    with temp_style(style_name):\n        style.use(style_name)\n        assert mpl.rcParams[PARAM] == VALUE\n\n    # Check that the style was unloaded correctly.\n    with pytest.raises(KeyError):\n        mpl.rcParams[PARAM]\n"], "sample_700": ["def test_call_with_optional_argument_no_args(self, mocker):\n    func = mocker.Mock()\n    _call_with_optional_argument(func, None)\n    func.assert_called_once_with()\n", "def test_function_definition_setup(self):\n    item = FunctionDefinition(name=\"test_func\", parent=None, config=None)\n    item._initrequest()\n    assert item.funcargs == {}\n", "def test_skip_marks_evaluation(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.skip(reason=\"always skip\")\n            assert False\n\n            assert True\n\n            if True:\n                pytest.skip(\"skipping due to condition\")\n            assert False\n\n            pytest.skip(reason=\"skipping due to mark\", if=True)\n            assert False\n        \"\"\"\n    )\n\n    result = pytester.runpytest(\"-v\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*test_always_skip.py::test_always_skip SKIPPED*\",\n            \"*test_no_skip_marks.py::test_no_skip_marks PASSED*\",\n            \"*test_skip_if_condition.py::test_skip_if_condition SKIPPED*\",\n            \"*test_skip_if_mark.py::test_skip_if_mark SKIPPED*\",\n        ]\n    )\n    assert result.ret == 0\n", "def test_xfail_marks(self, pytester: Pytester):\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.xfail(reason=\"This test is expected to fail\")\n            assert False\n        \"\"\"\n    )\n\n    result = pytester.runpytest()\n\n    # Verify the test result and xfail reason\n    result.stdout.fnmatch_lines([\n        \"*1 failed, 1 xfailed*\",\n        \"*test_func.py::test_func XFAIL*This test is expected to fail*\",\n    ])\n\n    # Verify the xfail reason is in the report\n    assert \"This test is expected to fail\" in result.stdout.str()\n", "def test_callspec_copy(pytester):\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture\n            return 'value1'\n\n        @pytest.fixture\n            return 'value2'\n\n        @pytest.mark.parametrize('arg1, arg2', [('value1', 'value2')])\n            pass\n\n            metafunc = FunctionDefinition.from_parent(parent=None, name='test_function')._fixtureinfo.metafunc\n            metafunc._calls = [CallSpec2(metafunc)]\n            metafunc.parametrize('arg1', ['value3'], indirect=True)\n\n            new_callspec = metafunc._calls[0].copy()\n            assert new_callspec.funcargs == {'arg1': 'value3'}\n            assert new_callspec.params == {'arg2': 'value2'}\n            assert new_callspec._arg2scopenum == {'arg1': 2, 'arg2': 1}\n            assert new_callspec.marks == []\n            assert new_callspec.indices == {'arg1': 0, 'arg2': 0}\n        \"\"\"\n    )\n    pytester.runpytest().assert_outcomes(passed=1)\n"], "sample_248": ["    def test_stdin_execution_with_inline_function(self):\n        with captured_stdout() as stdout:\n            call_command('shell')\n        self.assertEqual(stdout.getvalue().strip(), __version__)\n", "    def test_ipython(self, mock_start_ipython):\n        call_command('shell', interface='ipython')\n        mock_start_ipython.assert_called_once()\n", "def test_stdin_execution(self):\n    stdin = mock.Mock()\n    stdin.isatty.return_value = False\n    stdin.read.return_value = self.script_with_inline_function\n\n    with mock.patch('sys.stdin', stdin), captured_stdout() as stdout:\n        call_command('shell')\n\n    self.assertIn(__version__, stdout.getvalue())\n", "    def test_stdin_execution(self, mock_stdin):\n        mock_stdin.write(self.script_with_inline_function)\n        mock_stdin.seek(0)\n        with captured_stdout() as stdout:\n            call_command('shell')\n        self.assertEqual(stdout.getvalue().strip(), __version__)\n", "    def test_python_shell_with_command_option(self, mocked_input):\n        out = io.StringIO()\n        sys.stdout = out\n        call_command('shell', command='print(\"Test command executed\")')\n        sys.stdout = sys.__stdout__\n        self.assertEqual(out.getvalue(), 'Test command executed\\n')\n"], "sample_519": ["def test_align_labels():\n    fig, axs = plt.subplots(2, 2)\n    axs[0, 0].set_xlabel('XLabel 0')\n    axs[0, 1].set_xlabel('XLabel 1')\n    axs[1, 0].set_ylabel('YLabel 0')\n    axs[1, 1].set_ylabel('YLabel 1')\n    fig.align_labels()\n", "def test_figure_subplots_adjust():\n    fig, ax = plt.subplots()\n    fig.subplots_adjust(left=0.2, right=0.8, bottom=0.2, top=0.8)\n    assert fig.subplotpars.left == 0.2\n    assert fig.subplotpars.right == 0.8\n    assert fig.subplotpars.bottom == 0.2\n    assert fig.subplotpars.top == 0.8\n", "def test_figure_align_labels():\n    fig, axs = plt.subplots(1, 2)\n    axs[0].set_xlabel('XLabel 0')\n    axs[1].set_xlabel('XLabel 1')\n    fig.align_xlabels()\n", "def test_figure_align_labels():\n    fig, axs = plt.subplots(2, 2)\n    for ax in axs.flat:\n        ax.set(xlabel='x-label', ylabel='y-label')\n    fig.align_labels()\n", "def test_figure_add_axes():\n    fig = Figure()\n    rect = [0.1, 0.2, 0.6, 0.7]\n    ax = fig.add_axes(rect)\n    assert isinstance(ax, Axes)\n    assert ax.get_position().bounds == rect\n\n    # Test adding an axes with a different projection\n    ax = fig.add_axes(rect, projection='polar')\n    assert isinstance(ax, Axes)\n    assert ax.name == 'polar'\n\n    # Test adding an axes with polar=True\n    ax = fig.add_axes(rect, polar=True)\n    assert isinstance(ax, Axes)\n    assert ax.name == 'polar'\n\n    # Test adding an axes with sharex and sharey\n    ax1 = fig.add_axes(rect)\n    ax2 = fig.add_axes(rect, sharex=ax1, sharey=ax1)\n    assert ax1.get_shared_x_axes().joined(ax1, ax2)\n    assert ax1.get_shared_y_axes().joined(ax1, ax2)\n\n    # Test adding an axes with a label\n    ax = fig.add_axes(rect, label='test')\n    assert ax.get_label() == 'test'\n"], "sample_1163": ["def test_Abs_rewrite_as_Heaviside():\n    x = Symbol('x', real=True)\n    assert Abs(x).rewrite(Heaviside) == x*(Heaviside(x) - Heaviside(-x))\n", "def test_re_im_functions():\n    x, y = symbols('x y', real=True)\n\n    assert re(2*E) == 2*E\n    assert re(2*I + 17) == 17\n    assert re(2*I) == 0\n    assert re(im(x) + x*I + 2) == 2\n    assert re(5 + I + 2) == 7\n\n    assert im(2*E) == 0\n    assert im(2*I + 17) == 2\n    assert im(x*I) == re(x)\n    assert im(re(x) + y) == im(y)\n    assert im(2 + 3*I) == 3\n", "def test_unpolarify():\n    from sympy import unpolarify, polar_lift, sin, I\n    assert unpolarify(sin(polar_lift(I + 7))) == sin(7 + I)\n", "def test_arg_derivative():\n    x, y, t = symbols('x y t', real=True)\n    z = x + I*y\n    dz = Derivative(z, t)\n    assert arg(z)._eval_derivative(t) == (y*dz.diff(x) - x*dz.diff(y)) / (x**2 + y**2)\n", "def test_principal_branch_with_non_infinity_period():\n    z = exp_polar(7*I*pi)\n    result = principal_branch(z, 2*pi)\n    expected = exp_polar(-pi)\n    assert result == expected\n"], "sample_747": ["def test_power_transform_method_validation():\n    X = [[1, 2], [3, 2], [4, 5]]\n    pt = PowerTransformer(method='invalid_method')\n    assert_raises_regex(ValueError, \"method' must be one of\", pt.fit, X)\n", "def test_power_transformer_box_cox_no_standardize():\n    pt = PowerTransformer(method='box-cox', standardize=False)\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    X_transformed = pt.fit_transform(X)\n    X_inverse = pt.inverse_transform(X_transformed)\n    assert_array_almost_equal(X, X_inverse, decimal=2)\n", "def test_quantile_transform_n_quantiles_greater_than_subsample():\n    n_quantiles = 101\n    subsample = 100\n    with pytest.raises(ValueError, match=\"The number of quantiles cannot be greater than the number of samples used.\"):\n        quantile_transform(X_2d, n_quantiles=n_quantiles, subsample=subsample)\n", "def test_binarizer():\n    X_bin = Binarizer().fit_transform(X_2d)\n    assert_equal(X_bin.dtype, np.int)\n    assert_no_warnings(lambda: Binarizer().fit_transform(X_2d, y=0))\n\n    X_bin = Binarizer(copy=False).fit_transform(X_2d)\n    assert_equal(X_bin.dtype, np.int)\n\n    X_bin = Binarizer(threshold=0.5).fit_transform(X_2d)\n    assert_equal(X_bin.dtype, np.int)\n", "def test_transform_selected():\n    transform = lambda X: X + 1\n    dtype = np.float64\n    selected = [1, 3, 5]\n    X = np.array([[1, 2, 3], [4, 5, 6]])\n    result = _transform_selected(X, transform, dtype, selected)\n    expected = np.array([[1, 3, 3], [4, 6, 6]])\n    assert_array_equal(result, expected)\n"], "sample_1021": ["def test_quaternion_rotation_matrix():\n    q = Quaternion(sqrt(2)/2, sqrt(2)/2, 0, 0)\n    M = q.to_rotation_matrix()\n    expected_M = Matrix([[0, -1, 0], [1, 0, 0], [0, 0, 1]])\n    assert M == expected_M\n", "def test_rotation_matrix_conversion():\n    # Test conversion from rotation matrix to quaternion and back\n    R = Matrix([[cos(x), -sin(x), 0], [sin(x), cos(x), 0], [0, 0, 1]])\n    q = trigsimp(Quaternion.from_rotation_matrix(R))\n    R_back = q.to_rotation_matrix()\n    assert trigsimp(R - R_back) == Matrix.zeros(3)\n", "def test_quaternion_from_rotation_matrix():\n    M = Matrix([[cos(x), -sin(x), 0], [sin(x), cos(x), 0], [0, 0, 1]])\n    q = trigsimp(Quaternion.from_rotation_matrix(M))\n    assert q == Quaternion(cos(x/2), 0, 0, sin(x/2))\n", "def test_rotate_point_with_vector_angle():\n    q = Quaternion.from_axis_angle((sqrt(3)/3, sqrt(3)/3, sqrt(3)/3), 2*pi/3)\n    point = (1, 1, 1)\n    result = Quaternion.rotate_point(point, q.to_axis_angle())\n    assert result == (0, -1, 1)\n", "def test_quaternion_multiplication_over_complex_field():\n    q1 = Quaternion(3 + 4*I, 2 + 5*I, 0, 7 + 8*I, real_field=False)\n    q2 = Quaternion(1 - I, 2 + 3*I, 4 - 5*I, 6 + 7*I, real_field=False)\n    result = q1.mul(q2)\n    expected = Quaternion(-19 - 31*I, 32 + 59*I, -46 + 4*I, -38 + 12*I)\n    assert result == expected\n"], "sample_641": ["def test_save_and_load_results(tmp_path):\n    base = \"test_base\"\n    pylint_home = tmp_path\n    results = LinterStats(BadNames())\n\n    save_results(results, base, pylint_home)\n    loaded_results = load_results(base, pylint_home)\n\n    assert loaded_results == results\n", "def test_save_load_results(tmp_path):\n    # Create a dummy LinterStats object\n    dummy_stats = LinterStats()\n    dummy_stats.bad_names = BadNames()\n\n    # Save the results\n    base_path = tmp_path / \"test_project\"\n    save_results(dummy_stats, base_path, tmp_path)\n\n    # Load the results\n    loaded_stats = load_results(base_path, tmp_path)\n\n    # Check that the loaded results are equal to the saved results\n    assert loaded_stats == dummy_stats\n", "def test_save_and_load_results(tmp_path):\n    base = \"test_base\"\n    results = LinterStats()\n    results.bad_names = BadNames()\n    results.bad_names.add(\"test_name\")\n    save_results(results, base, tmp_path)\n    loaded_results = load_results(base, tmp_path)\n    assert loaded_results is not None\n    assert loaded_results.bad_names.names == results.bad_names.names\n", "def test_get_pdata_path_special_characters(base_name, recur, pylint_home, expected):\n    assert _get_pdata_path(base_name, recur, pylint_home) == expected\n", "def test_get_pdata_path_invalid_chars(base, pylint_home, expected):\n    assert _get_pdata_path(Path(base), 1, Path(pylint_home)) == expected\n"], "sample_104": ["def test_hashed_name_with_query_fragment(self):\n    test_file = 'test.css?#iefix'\n    self.write_file(test_file, '')\n    hashed_name = storage.staticfiles_storage.hashed_name(test_file)\n    self.assertTrue(hashed_name.endswith('.css?#iefix'))\n    self.assertPostCondition()\n", "    def test_cached_hashed_name(self):\n        # Test the hashed_name method of CachedStaticFilesStorage\n        name = 'test.css'\n        content = StringIO(\"url('test.png')\")\n        hashed_name = storage.staticfiles_storage.hashed_name(name, content)\n        self.assertIn('.', hashed_name)\n        self.assertPostCondition()\n", "def test_post_process_max_passes_exceeded(self):\n    \"\"\"Test that post_process raises an error when max passes are exceeded.\"\"\"\n    storage.staticfiles_storage.max_post_process_passes = 1\n    with self.assertRaises(RuntimeError):\n        storage.staticfiles_storage.post_process({\n            'test.css': (storage.staticfiles_storage, self.path('test.css')),\n            'test2.css': (storage.staticfiles_storage, self.path('test2.css')),\n        }, dry_run=False)\n    self.assertPostCondition()\n", "def test_max_post_process_passes_exceeded(self):\n    storage.staticfiles_storage.max_post_process_passes = 1\n    with self.modify_settings(STATICFILES_STORAGE='django.contrib.staticfiles.storage.ManifestStaticFilesStorage'):\n        with self.settings(STATIC_ROOT=self.temp_dir):\n            self.write_file(\n                'test.css',\n                '.test { background: url(\"other.png\"); }',\n                subdir='sub',\n            )\n            self.write_file('other.png', 'other file content')\n\n            # Make post_process loop more than max_post_process_passes times\n            storage.staticfiles_storage.patterns = (\n                (\"*.css\", (\n                    (r\"\"\"(url\\(['\"]{0,1}\\s*(.*?)[\"']{0,1}\\))\"\"\", \"\"\"url(\"%s?version=1\")\"\"\"),\n                )),\n            )\n\n            with self.assertRaisesMessage(RuntimeError, 'Max post-process passes exceeded.'):\n                call_command('collectstatic', interactive=False, verbosity=0)\n\n    self.assertPostCondition()\n", "    def setUpClass(cls):\n        super().setUpClass()\n        cls.cache = CachedStaticFilesStorage().hashed_files.cache\n"], "sample_894": ["def test_feature_importances_with_max_features_none(name):\n    \"\"\"Test feature importances with max_features=None.\"\"\"\n    ForestClassifier = FOREST_CLASSIFIERS[name]\n\n    clf = ForestClassifier(n_estimators=10, max_features=None, random_state=1)\n    clf.fit(X_large, y_large)\n\n    importances = clf.feature_importances_\n    assert len(importances) == X_large.shape[1]\n    assert np.all(importances >= 0)\n    assert np.sum(importances) > 0\n", "def test_feature_importances(name):\n    \"\"\"Check feature importances on a larger dataset.\"\"\"\n    ForestClassifier = FOREST_CLASSIFIERS[name]\n\n    clf = ForestClassifier(n_estimators=10, random_state=1)\n    clf.fit(X_large, y_large)\n\n    importances = clf.feature_importances_\n    assert importances.shape == (10,)\n    assert np.all(importances >= 0)\n    assert np.all(importances <= 1)\n    assert np.all(np.isfinite(importances))\n", "def check_classification_large(name):\n    \"\"\"Check classification on a larger dataset.\"\"\"\n    ForestClassifier = FOREST_CLASSIFIERS[name]\n\n    clf = ForestClassifier(n_estimators=10, random_state=0)\n    clf.fit(X_large, y_large)\n    assert clf.n_features_in_ == X_large.shape[1]\n    assert clf.feature_importances_.shape == (X_large.shape[1],)\n    assert np.all(clf.feature_importances_ >= 0)\n    assert_almost_equal(clf.feature_importances_.sum(), 1)\n", "def check_feature_importances_large(name):\n    \"\"\"Check feature importances on a larger dataset.\"\"\"\n    ForestClassifier = FOREST_CLASSIFIERS[name]\n\n    clf = ForestClassifier(n_estimators=100, random_state=0)\n    clf.fit(X_large, y_large)\n    importances = clf.feature_importances_\n    assert importances.shape == (10,)\n    # Check that importances are ordered correctly\n    assert np.all(importances[::-1] <= importances[1:][::-1])\n", "def check_regression_toy(name):\n    \"\"\"Check regression on a toy dataset.\"\"\"\n    ForestRegressor = FOREST_REGRESSORS[name]\n\n    reg = ForestRegressor(n_estimators=10, random_state=1)\n    reg.fit(X_reg, y_reg)\n    assert len(reg.predict(X_reg)) == len(y_reg)\n\n    reg = ForestRegressor(n_estimators=10, max_features=1, random_state=1)\n    reg.fit(X_reg, y_reg)\n    assert len(reg.predict(X_reg)) == len(y_reg)\n\n    # also test apply\n    leaf_indices = reg.apply(X_reg)\n    assert leaf_indices.shape == (len(X_reg), reg.n_estimators)\n"], "sample_51": ["def test_parse_datetime_with_tzinfo(self):\n    dt = parse_datetime(\"2022-01-01T12:30:45+03:00\")\n    self.assertIsInstance(dt, datetime)\n    self.assertEqual(dt.year, 2022)\n    self.assertEqual(dt.month, 1)\n    self.assertEqual(dt.day, 1)\n    self.assertEqual(dt.hour, 12)\n    self.assertEqual(dt.minute, 30)\n    self.assertEqual(dt.second, 45)\n    self.assertEqual(dt.tzinfo, get_fixed_timezone(180))\n", "    def test_parse_duration(self):\n        self.assertEqual(parse_duration('1 02:03:04.000005'), timedelta(days=1, hours=2, minutes=3, seconds=4, microseconds=5))\n        self.assertEqual(parse_duration('P3DT6H8M'), timedelta(days=3, hours=6, minutes=8))\n        self.assertEqual(parse_duration('3 days 04:05:06'), timedelta(days=3, hours=4, minutes=5, seconds=6))\n        self.assertIsNone(parse_duration('1 year'))\n", "    def test_parse_duration(self):\n        self.assertEqual(parse_duration('1 day 1:00:00'), timedelta(days=1))\n        self.assertEqual(parse_duration('1 1:00:00'), timedelta(days=1, hours=1))\n        self.assertEqual(parse_duration('1:00:00'), timedelta(hours=1))\n        self.assertEqual(parse_duration('01:00:00.123456'), timedelta(hours=1, microseconds=123456))\n        self.assertEqual(parse_duration('P1D'), timedelta(days=1))\n        self.assertEqual(parse_duration('P1DT1H'), timedelta(days=1, hours=1))\n        self.assertEqual(parse_duration('P1DT1H1M1S'), timedelta(days=1, hours=1, minutes=1, seconds=1))\n        self.assertEqual(parse_duration('1 day 01:00:00.123456'), timedelta(days=1, microseconds=123456))\n        self.assertEqual(parse_duration('-1 day -1:00:00.123456'), timedelta(days=-1, hours=-1, microseconds=123456))\n        self.assertEqual(parse_duration('3 04:05:06.789123'), timedelta(days=3, hours=4, minutes=5, seconds=6, microseconds=789123))\n        self.assertIsNone(parse_duration('invalid duration string'))\n", "def test_parse_datetime_with_tzinfo(self):\n    dt_str = '2021-12-31T23:59:59+05:00'\n    dt = parse_datetime(dt_str)\n    self.assertEqual(dt, datetime(2021, 12, 31, 23, 59, 59, tzinfo=get_fixed_timezone(300)))\n\n    dt_str = '2021-12-31T23:59:59-06:00'\n    dt = parse_datetime(dt_str)\n    self.assertEqual(dt, datetime(2021, 12, 31, 23, 59, 59, tzinfo=get_fixed_timezone(-360)))\n\n    dt_str = '2021-12-31T23:59:59Z'\n    dt = parse_datetime(dt_str)\n    self.assertEqual(dt, datetime(2021, 12, 31, 23, 59, 59, tzinfo=utc))\n", "def test_parse_datetime(self):\n    \"\"\"Test the parse_datetime function.\"\"\"\n    self.assertEqual(\n        parse_datetime('2019-03-28T12:30:45Z'),\n        datetime(2019, 3, 28, 12, 30, 45, tzinfo=get_fixed_timezone(0))\n    )\n    self.assertEqual(\n        parse_datetime('2019-03-28T12:30:45+05:00'),\n        datetime(2019, 3, 28, 12, 30, 45, tzinfo=get_fixed_timezone(300))\n    )\n    self.assertEqual(\n        parse_datetime('2019-03-28T12:30:45-03:30'),\n        datetime(2019, 3, 28, 12, 30, 45, tzinfo=get_fixed_timezone(-210))\n    )\n    self.assertIsNone(parse_datetime('2019-03-28T12:30:45.1234567'))  # microsecond part is too long\n    self.assertIsNone(parse_datetime('2019-03-28T12:30:45.123456Z12'))  # invalid timezone\n"], "sample_355": ["    def test_create_user(self):\n        User = apps.get_model(User._meta.app_label, User._meta.object_name)\n        user = User.objects._create_user('testuser', 'test@example.com', 'password123')\n        self.assertEqual(user.username, 'testuser')\n        self.assertEqual(user.email, 'test@example.com')\n        self.assertTrue(user.check_password('password123'))\n        self.assertFalse(user.is_staff)\n        self.assertFalse(user.is_superuser)\n", "    def test_create_user(self):\n        User.objects.create_user(username=\"testuser\", email=\"testuser@example.com\", password=\"testpassword\")\n        user = User.objects.get(username=\"testuser\")\n        self.assertEqual(user.email, \"testuser@example.com\")\n        self.assertTrue(user.check_password(\"testpassword\"))\n        self.assertFalse(user.is_staff)\n        self.assertFalse(user.is_superuser)\n", "def test_has_perm(self):\n    self.assertIn('user_perm', self.user.get_user_permissions())\n    self.assertIn('group_perm', self.user.get_group_permissions())\n    self.assertIn('user_perm', self.user.get_all_permissions())\n    self.assertIn('group_perm', self.user.get_all_permissions())\n    self.assertTrue(self.user.has_perm('user_perm'))\n    self.assertTrue(self.user.has_perm('group_perm'))\n", "    def test_user_has_perm(self):\n        user = User.objects.get(username='test')\n        self.assertTrue(user.has_perm('user_perm'))\n", "    def setUpTestData(cls):\n        cls.user = User.objects.create_user('test', 'test@example.com', 'test')\n"], "sample_461": ["    def test_to_python_with_valid_url(self):\n        url_field = URLField()\n        valid_url = \"http://example.com\"\n        self.assertEqual(url_field.to_python(valid_url), valid_url)\n", "    def test_url_field_with_custom_assume_scheme(self):\n        field = URLField(assume_scheme='https')\n        self.assertEqual(field.clean('example.com'), 'https://example.com')\n        self.assertEqual(field.clean('http://example.com'), 'http://example.com')\n        self.assertEqual(field.clean('https://example.com'), 'https://example.com')\n        self.assertEqual(field.clean('//example.com'), 'https://example.com')\n        self.assertEqual(field.clean('ftp://example.com'), 'ftp://example.com')\n        with self.assertRaises(ValidationError):\n            field.clean('not a url')\n", "    def test_url_field_assume_scheme(self):\n        url_field = URLField(assume_scheme=\"https\")\n        self.assertEqual(url_field.clean(\"example.com\"), \"https://example.com\")\n        self.assertEqual(url_field.clean(\"http://example.com\"), \"http://example.com\")\n", "def test_url_field_to_python(self):\n    field = URLField()\n    self.assertEqual(field.to_python(\"https://www.example.com\"), \"https://www.example.com\")\n    self.assertEqual(field.to_python(\"www.example.com\"), \"http://www.example.com\")\n    self.assertEqual(field.to_python(\"example.com\"), \"http://example.com\")\n    self.assertEqual(field.to_python(\"example.com/path\"), \"http://example.com/path\")\n    self.assertEqual(field.to_python(\"http://example.com\"), \"http://example.com\")\n    self.assertEqual(field.to_python(\"http://example.com/path\"), \"http://example.com/path\")\n    self.assertEqual(field.to_python(\"https://example.com\"), \"https://example.com\")\n    self.assertEqual(field.to_python(\"https://example.com/path\"), \"https://example.com/path\")\n    self.assertEqual(field.to_python(\"ftp://example.com\"), \"ftp://example.com\")\n    self.assertEqual(field.to_python(\"ftp://example.com/path\"), \"ftp://example.com/path\")\n    self.assertEqual(field.to_python(\"\"), \"\")\n    self.assertEqual(field.to_python(None), None)\n    with self.assertRaises(ValidationError):\n        field.to_python(\"not a url\")\n", "    def test_url_field_with_assume_scheme(self):\n        url_field = URLField(assume_scheme=\"https\")\n        value = \"example.com\"\n        result = url_field.clean(value)\n        self.assertEqual(result, \"https://example.com\")\n"], "sample_22": ["def test_matrix_product():\n    # Test matrix_product function with three matrices\n    m1 = np.array([[1, 2], [3, 4]])\n    m2 = np.array([[5, 6], [7, 8]])\n    m3 = np.array([[9, 10], [11, 12]])\n    expected_result = np.matmul(np.matmul(m1, m2), m3)\n    assert_array_equal(matrix_product(m1, m2, m3), expected_result)\n\n    # Test matrix_product function with more than two dimensions\n    m1 = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n    m2 = np.array([[[9, 10], [11, 12]], [[13, 14], [15, 16]]])\n    expected_result = np.matmul(m1, m2)\n    assert_array_equal(matrix_product(m1, m2), expected_result)\n\n    # Test matrix_product function with deprecated warning\n    with pytest.warns(AstropyDeprecationWarning):\n        matrix_product(m1, m2)\n", "def test_matrix_product():\n    m1 = np.array([[1, 2], [3, 4]])\n    m2 = np.array([[5, 6], [7, 8]])\n    expected = np.array([[19, 22], [43, 50]])\n    result = matrix_product(m1, m2)\n    assert_array_equal(result, expected)\n", "def test_rotation_matrix_with_quantity_angle():\n    angle = 45 * u.deg\n    rot_matrix = rotation_matrix(angle)\n    expected_rot_matrix = rotation_matrix(angle.to_value(u.rad))\n    assert_allclose(rot_matrix, expected_rot_matrix)\n", "def test_matrix_product_dimensions():\n    # Test that matrix_product handles matrices with dimensions other than 2\n    mat1 = np.array([[1, 2, 3], [4, 5, 6]])\n    mat2 = np.array([[7, 8], [9, 10], [11, 12]])\n    result = matrix_product(mat1, mat2)\n    expected = np.dot(mat1, mat2)\n    assert_array_equal(result, expected)\n", "def test_matrix_product():\n    m1 = np.array([[1, 2], [3, 4]])\n    m2 = np.array([[5, 6], [7, 8]])\n    m3 = np.array([[9, 10], [11, 12]])\n\n    result = matrix_product(m1, m2, m3)\n    expected = np.dot(m1, np.dot(m2, m3))\n\n    assert_array_equal(result, expected)\n\n    with pytest.deprecated_call():\n        deprecated_result = matrix_product(m1, m2, m3)\n    assert_array_equal(deprecated_result, expected)\n"], "sample_375": ["def test_rename_field(self):\n    project_state = ProjectState()\n    model_state = ModelState('test_app', 'TestModel', {\n        'old_field': models.CharField(max_length=100),\n        'related_field': models.ForeignKey('other_app.OtherModel', on_delete=models.CASCADE),\n    })\n    project_state.add_model(model_state)\n\n    operation = RenameField('test_app', 'TestModel', 'old_field', 'new_field')\n    operation.state_forwards('test_app', project_state)\n\n    new_model_state = project_state.models[('test_app', 'testmodel')]\n    self.assertNotIn('old_field', new_model_state.fields)\n    self.assertIn('new_field', new_model_state.fields)\n    self.assertEqual(new_model_state.fields['new_field'], model_state.fields['old_field'])\n    self.assertEqual(new_model_state.fields['related_field'].to_fields, ('new_field',))\n", "def test_alter_model_managers(self):\n    old_state = ProjectState(models={\n        ('app_label', 'model_name'): ModelState(\n            app_label='app_label',\n            name='ModelName',\n            fields=[],\n            options={'managed': True},\n            managers=[('objects', models.Manager())]\n        )\n    })\n    new_state = old_state.clone()\n    new_managers = [('custom_objects', FoodManager())]\n    new_state.alter_model_managers('app_label', 'model_name', new_managers)\n    self.assertEqual(new_state.models[('app_label', 'model_name')].managers, new_managers)\n", "    def test_add_field(self):\n        initial_state = ProjectState()\n        initial_state.add_model(ModelState('myapp', 'MyModel', {'id': models.AutoField(primary_key=True)}))\n\n        operation = AddField('myapp', 'MyModel', 'new_field', models.CharField(max_length=200))\n        operation.state_forwards('myapp', initial_state)\n\n        new_state = initial_state.clone()\n        operation.database_forwards('myapp', new_state, None, None)\n\n        self.assertIn('new_field', new_state.models['myapp', 'mymodel'].fields)\n        self.assertEqual(new_state.models['myapp', 'mymodel'].fields['new_field'].max_length, 200)\n", "    def test_alter_model_options(self):\n        app_label = \"myapp\"\n        model_name = \"MyModel\"\n        models = {\n            (app_label, model_name.lower()): ModelState(app_label, model_name, {}),\n        }\n        state = ProjectState(models)\n\n        # Test alter_model_options() with an existing option\n        options = {\"managed\": False}\n        state.alter_model_options(app_label, model_name.lower(), options)\n        self.assertEqual(state.models[app_label, model_name.lower()].options, {\"managed\": False})\n\n        # Test alter_model_options() with a new option\n        options = {\"verbose_name\": \"My New Model\"}\n        state.alter_model_options(app_label, model_name.lower(), options)\n        self.assertEqual(state.models[app_label, model_name.lower()].options, {\n            \"managed\": False,\n            \"verbose_name\": \"My New Model\",\n        })\n\n        # Test alter_model_options() with option_keys\n        options = {\"managed\": True, \"verbose_name\": \"My Updated Model\"}\n        option_keys = [\"verbose_name\"]\n        state.alter_model_options(app_label, model_name.lower(), options, option_keys)\n        self.assertEqual(state.models[app_label, model_name.lower()].options, {\n            \"managed\": False,\n            \"verbose_name\": \"My Updated Model\",\n        })\n", "    def test_rename_field(self):\n        state = ProjectState()\n        model_state = ModelState('tests', 'UnicodeModel', {\n            'old_name': models.CharField(max_length=20),\n        })\n        state.add_model(model_state)\n        state.rename_field('tests', 'unicodemodel', 'old_name', 'new_name')\n        self.assertNotIn('old_name', state.models[('tests', 'unicodemodel')].fields)\n        self.assertIn('new_name', state.models[('tests', 'unicodemodel')].fields)\n"], "sample_539": ["def test_rectangle_selector(ax, kwargs):\n    callback = mock.Mock()\n    rs = widgets.RectangleSelector(ax, callback, **kwargs)\n    click_and_drag(rs.ax, 0, 0, 100, 100)\n    assert callback.called\n", "def test_rectangle_selector_with_kwargs(ax, kwargs):\n    onselect = mock.MagicMock()\n    rect_selector = widgets.RectangleSelector(ax, onselect, **kwargs)\n    click_and_drag(rect_selector, (1, 1), (2, 2))\n    onselect.assert_called_once()\n", "def test_rectangle_selector(ax, kwargs):\n    onselect = mock.Mock()\n    rs = widgets.RectangleSelector(ax, onselect, **kwargs)\n    click_and_drag(rs, ax, 0, 0, 100, 100)\n    assert onselect.call_count == 1\n    assert_allclose(onselect.call_args[0][0].xdata, 0)\n    assert_allclose(onselect.call_args[0][0].ydata, 0)\n    assert_allclose(onselect.call_args[0][1].xdata, 100)\n    assert_allclose(onselect.call_args[0][1].ydata, 100)\n", "def test_rectangleselector_init(ax, kwargs):\n    with mock.patch('matplotlib.widgets.RectangleSelector.update') as mock_update:\n        rs = widgets.RectangleSelector(ax, noop, **kwargs)\n        assert isinstance(rs._selection_artist, Rectangle)\n        assert isinstance(rs._corner_handles, widgets.ToolHandles)\n        assert isinstance(rs._edge_handles, widgets.ToolHandles)\n        assert isinstance(rs._center_handle, widgets.ToolHandles)\n        assert rs.minspanx == kwargs.get('minspanx', 0)\n        assert rs.minspany == kwargs.get('minspany', 0)\n        assert rs.spancoords == kwargs.get('spancoords', 'data')\n        mock_update.assert_called_once()\n", "def test_rectangle_selector(fig, ax, kwargs):\n    callback = mock.Mock()\n    rs = widgets.RectangleSelector(ax, callback, **kwargs)\n\n    click_and_drag(fig.canvas, 10, 10, 100, 100, button=kwargs.get('button', 1))\n    assert callback.called\n\n    # Test that the rectangle is removed when the selection is cancelled\n    callback.reset_mock()\n    click_and_drag(fig.canvas, 10, 10, 5, 5, button=kwargs.get('button', 1))\n    assert not callback.called\n\n    plt.close(fig)\n"], "sample_220": ["def test_set_cookie_max_age_with_aware_datetime_expires(self):\n    response = HttpResponse()\n    expires = datetime.utcnow().replace(tzinfo=utc) + timedelta(days=1)\n    response.set_cookie('key', 'value', expires=expires)\n    self.assertEqual(response.cookies['key']['max-age'], 86400)\n    self.assertEqual(response.cookies['key']['expires'], http_date(time.time() + 86400))\n", "    def test_set_cookie_with_expires_datetime(self):\n        response = HttpResponse()\n        expires = datetime.utcnow() + timedelta(days=1)\n        response.set_cookie('test_cookie', 'test_value', expires=expires)\n        cookie = response.cookies['test_cookie']\n        self.assertEqual(cookie['expires'], http_date(time.mktime(expires.timetuple())))\n", "    def test_max_age_set_cookie(self):\n        response = HttpResponse()\n        expires_date = datetime.now(utc) + timedelta(days=1)\n        expires_str = http_date(time.mktime(expires_date.timetuple()))\n        response.set_cookie('name', 'value', max_age=86400)\n        self.assertEqual(response.cookies['name']['max-age'], '86400')\n        self.assertEqual(response.cookies['name']['expires'], expires_str)\n", "    def test_set_cookie_with_samesite(self):\n        response = HttpResponse()\n        response.set_cookie('name', 'value', samesite='Lax')\n        cookie = response.cookies['name']\n        self.assertEqual(cookie['samesite'], 'Lax')\n        with self.assertRaisesMessage(ValueError, 'samesite must be \"lax\", \"none\", or \"strict\".'):\n            response.set_cookie('name', 'value', samesite='InvalidValue')\n", "    def test_set_cookie_samesite(self):\n        response = HttpResponse()\n        response.set_cookie('name', 'value', samesite='Lax')\n        self.assertEqual(response.cookies['name']['samesite'], 'Lax')\n\n        response.set_cookie('name', 'value', samesite='None')\n        self.assertEqual(response.cookies['name']['samesite'], 'None')\n\n        response.set_cookie('name', 'value', samesite='Strict')\n        self.assertEqual(response.cookies['name']['samesite'], 'Strict')\n\n        with self.assertRaises(ValueError):\n            response.set_cookie('name', 'value', samesite='invalid')\n"], "sample_1128": ["def test_locatenew():\n    N = ReferenceFrame('N')\n    P1 = Point('P1')\n    P2 = P1.locatenew('P2', 10 * N.x)\n    assert P2.pos_from(P1) == 10 * N.x\n    assert P1.pos_from(P2) == -10 * N.x\n", "def test_set_pos():\n    N = ReferenceFrame('N')\n    p1 = Point('p1')\n    p2 = Point('p2')\n    p1.set_pos(p2, 10 * N.x)\n    assert p1.pos_from(p2) == 10 * N.x\n    assert p2.pos_from(p1) == -10 * N.x\n", "def test_point_set_pos():\n    N = ReferenceFrame('N')\n    p1 = Point('p1')\n    p2 = Point('p2')\n    p1.set_pos(p2, 10 * N.x)\n    assert p1.pos_from(p2) == 10 * N.x\n    assert p2.pos_from(p1) == -10 * N.x\n", "def test_set_pos_and_pos_from():\n    N = ReferenceFrame('N')\n    p1 = Point('p1')\n    p2 = Point('p2')\n    pos_vector = 10 * N.x\n    p1.set_pos(p2, pos_vector)\n    assert p1.pos_from(p2) == pos_vector\n    assert p2.pos_from(p1) == -pos_vector\n", "def test_point_position():\n    N = ReferenceFrame('N')\n    P1 = Point('P1')\n    P2 = Point('P2')\n    P1.set_pos(P2, 10 * N.x)\n    assert P1.pos_from(P2) == 10 * N.x\n    assert P2.pos_from(P1) == -10 * N.x\n"], "sample_763": ["def test_check_non_negative():\n    X = np.array([[1, 2], [3, -4]])\n    with pytest.raises(ValueError, match=\"Negative values in data passed to check_non_negative\"):\n        check_non_negative(X, \"test\")\n\n    X = np.array([[1, 2], [3, 4]])\n    check_non_negative(X, \"test\")  # No exception should be raised\n", "def test_check_non_negative():\n    # Test with a numpy array\n    X = np.array([[1, 2], [3, 4]])\n    check_non_negative(X, \"test\")  # No error expected\n\n    # Test with a scipy sparse matrix\n    X = sp.csr_matrix([[1, 2], [3, 4]])\n    check_non_negative(X, \"test\")  # No error expected\n\n    # Test with a negative value in numpy array\n    X = np.array([[1, 2], [-3, 4]])\n    assert_raises(ValueError, check_non_negative, X, \"test\")\n\n    # Test with a negative value in scipy sparse matrix\n    X = sp.csr_matrix([[1, 2], [-3, 4]])\n    assert_raises(ValueError, check_non_negative, X, \"test\")\n", "def test_check_non_negative_with_negative_values():\n    X = np.array([[1, 2, -3], [4, 5, 6]])\n    whom = \"test\"\n    with pytest.raises(ValueError, match=\"Negative values in data passed to test\"):\n        check_non_negative(X, whom)\n", "def test_check_non_negative():\n    X = np.array([[1, 2], [3, 4]])\n    check_non_negative(X, \"test function\")\n\n    X_neg = np.array([[1, -2], [3, 4]])\n    with pytest.raises(ValueError, match=\"Negative values in data passed to test function\"):\n        check_non_negative(X_neg, \"test function\")\n", "def test_check_is_fitted():\n    # Test case when estimator is not fitted\n    estimator = SVR()\n    with pytest.raises(NotFittedError):\n        check_is_fitted(estimator, 'support_vectors_')\n\n    # Test case when estimator is fitted\n    X, y = make_blobs(n_samples=50, centers=2, random_state=0)\n    estimator.fit(X, y)\n    check_is_fitted(estimator, 'support_vectors_')\n"], "sample_1067": ["def test_unevaluated_mul():\n    x, y = symbols('x y')\n    a = _unevaluated_Mul(S(3.0), x, S(2))\n    assert a.args[0] == S(6.0)\n    assert a.args[1] == x\n", "def test_eval_expand_mul_with_fractions():\n    x, y = symbols('x y')\n    expr = (3/x) * (x/4) * y\n    result = expr._eval_expand_mul()\n    assert result == 3/4 * y\n", "def test_eval_is_rational():\n    x, y = symbols('x y')\n    assert (2*x*y)._eval_is_rational() == True\n    assert (2*x*sin(y))._eval_is_rational() == None\n    assert (2*x*sqrt(y))._eval_is_rational() == False\n", "def test_eval_expand_mul():\n    x, y, z = symbols('x y z')\n    expr = (x + y)*(x + z)\n    assert expr.expand() == x**2 + x*y + x*z + y*z\n", "def test_mul_evaluation():\n    x, y, z = symbols('x y z')\n    assert Mul(x, y, z)._eval_evalf(2) == x*y*z  # Evaluate Mul with evalf\n"], "sample_207": ["    def test_json_field_get_transform(self):\n        json_field = JSONModel._meta.get_field('json')\n        transform = json_field.get_transform('key')\n        self.assertIsInstance(transform, KeyTransformFactory)\n        self.assertEqual(transform.key_name, 'key')\n", "    def test_key_transform_factory(self):\n        factory = KeyTransformFactory('test_key')\n        transform = factory(models.F('field_name'))\n        self.assertIsInstance(transform, KeyTransform)\n        self.assertEqual(transform.key_name, 'test_key')\n", "    def test_key_transform_lt(self):\n        obj = JSONModel.objects.create(json='{\"key\": 5}')\n        self.assertTrue(JSONModel.objects.filter(json__key__lt=6).exists())\n        self.assertFalse(JSONModel.objects.filter(json__key__lt=5).exists())\n", "    def test_key_transform_numeric_lookup(self):\n        JSONModel.objects.create(data={'key': 5})\n        self.assertEqual(JSONModel.objects.filter(**{'data__key__gt': 3}).count(), 1)\n        self.assertEqual(JSONModel.objects.filter(**{'data__key__lt': 7}).count(), 1)\n        self.assertEqual(JSONModel.objects.filter(**{'data__key__gte': 5}).count(), 1)\n        self.assertEqual(JSONModel.objects.filter(**{'data__key__lte': 5}).count(), 1)\n        self.assertEqual(JSONModel.objects.filter(**{'data__key__gt': 5}).count(), 0)\n        self.assertEqual(JSONModel.objects.filter(**{'data__key__lt': 5}).count(), 0)\n", "    def test_key_transform_exact(self):\n        obj = JSONModel.objects.create(data={'key': 'value'})\n        self.assertQuerysetEqual(\n            JSONModel.objects.filter(data__key__exact='value'),\n            [obj.pk]\n        )\n        self.assertQuerysetEqual(\n            JSONModel.objects.filter(data__key__exact='wrong'),\n            []\n        )\n"], "sample_943": ["def test_no_toc(apidoc):\n    assert not (apidoc.outdir / 'modules.rst').exists()\n", "def test_includeprivate(apidoc):\n    assert (apidoc.outdir / 'test_module._private.rst').exists()\n", "def test_dryrun_and_no_toc(apidoc):\n    # check that no files were created in the output directory\n    assert not any(apidoc.outdir.listdir())\n", "def test_apidoc_separate(apidoc):\n    assert (apidoc.outdir / 'test_module.rst').exists()\n    assert (apidoc.outdir / 'test_package.rst').exists()\n    assert (apidoc.outdir / 'test_package.test_module.rst').exists()\n", "def test_apidoc_with_options(apidoc):\n    # Check if the generated Markdown files exist\n    assert (apidoc.outdir / 'test_module.md').exists()\n    assert (apidoc.outdir / 'test_package.md').exists()\n\n    # Check if the exclude pattern is working\n    assert not (apidoc.outdir / '_build').exists()\n"], "sample_498": ["def test_legend_set_alignment():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3], [1, 2, 3], label='Line 1')\n    legend = ax.legend()\n    legend.set_alignment('left')\n    assert legend.get_alignment() == 'left'\n    legend.set_alignment('right')\n    assert legend.get_alignment() == 'right'\n    with pytest.raises(ValueError):\n        legend.set_alignment('invalid')\n", "def test_legend_set_get_title():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3], [4, 5, 6], label='Line')\n    legend = ax.legend()\n    assert legend.get_title().get_text() == ''\n    legend.set_title('New Title')\n    assert legend.get_title().get_text() == 'New Title'\n    plt.close(fig)\n", "def test_legend_numpoints():\n    fig, ax = plt.subplots()\n    line, = ax.plot([0, 1], [0, 1], label='test')\n    legend = ax.legend(numpoints=3)\n    assert legend.numpoints == 3\n    assert legend.legend_handles[0].get_xdata().size == 3\n", "def test_legend_markerfirst_false():\n    fig, ax = plt.subplots()\n    line = ax.plot([1, 2, 3], [4, 5, 6], marker='o', label='Test')\n    ax.legend(handles=line, labels=['Test'], markerfirst=False)\n", "def test_legend_scatter():\n    np.random.seed(0)\n    x = np.random.randn(30)\n    y = np.random.randn(30)\n    colors = np.random.randn(30)\n    sizes = np.abs(np.random.randn(30)) * 1000\n\n    fig, ax = plt.subplots()\n    sc = ax.scatter(x, y, c=colors, s=sizes, cmap='viridis')\n\n    handles, labels = sc.legend_elements(prop=\"sizes\", alpha=0.6)\n    legend1 = ax.legend(handles, labels, loc=\"upper right\", title=\"Sizes\")\n\n    handles, labels = sc.legend_elements(prop=\"colors\", alpha=0.6)\n    legend2 = ax.legend(handles, labels, loc=\"center right\", title=\"Colors\")\n\n    ax.add_artist(legend1)\n"], "sample_517": ["def test_annotation_contains():\n    fig, ax = plt.subplots()\n    ann = ax.annotate('Test', xy=(0.5, 0.5), xytext=(0.6, 0.6),\n                      arrowprops={'arrowstyle': '->'})\n    event = MouseEvent('button_press_event', fig.canvas, 300, 300, 1,\n                       button=1, dblclick=False)\n    contains, info = ann.contains(event)\n    assert contains\n", "def test_annotation_visibility():\n    fig, ax = plt.subplots()\n    ann = ax.annotate(\"Test\", xy=(0.5, 0.5), xycoords='axes fraction')\n    assert ann.get_visible()\n    ann.set_visible(False)\n    assert not ann.get_visible()\n", "def test_annotation_contains(fig, mouse_event):\n    ax = fig.add_subplot(111)\n    annot = ax.annotate(\"Test\", xy=(0.5, 0.5), xytext=(0.2, 0.2),\n                        arrowprops=dict(arrowstyle=\"->\"))\n    contains, _ = annot.contains(mouse_event)\n    assert contains, \"Mouse event should be contained within the annotation\"\n", "def test_annotation_update_positions(fig):\n    annot = mpl.text.Annotation(\"Text\", xy=(0, 0), xytext=(1, 1),\n                                xycoords='data', textcoords='data',\n                                arrowprops={'arrowstyle': '->'})\n    fig.add_subplot(111).add_artist(annot)\n    renderer = fig.canvas.get_renderer()\n    annot.update_positions(renderer)\n    assert_almost_equal(annot.arrow_patch.get_positions(), (0, 0), (1, 1))\n", "def test_annotation_arrowstyle():\n    fig, ax = plt.subplots()\n    annot = ax.annotate(\"Test\", xy=(0.5, 0.5), xytext=(0.2, 0.8),\n                        arrowprops=dict(arrowstyle=\"->\"))\n    assert isinstance(annot.arrow_patch, mpatches.FancyArrowPatch)\n"], "sample_703": ["def test_evaluate_empty_expression():\n    assert evaluate(\"\", lambda x: True) == False\n", "def test_evaluate_with_true_matcher():\n        return True\n\n    assert evaluate(\"foo\", true_matcher) is True\n    assert evaluate(\"foo and bar\", true_matcher) is True\n    assert evaluate(\"foo or bar\", true_matcher) is True\n    assert evaluate(\"not foo\", true_matcher) is False\n    assert evaluate(\"(foo)\", true_matcher) is True\n    assert evaluate(\"foo and not bar\", true_matcher) is False\n    assert evaluate(\"foo or not bar\", true_matcher) is True\n", "def test_expression_with_invalid_token():\n    with pytest.raises(ParseError):\n        evaluate(\"test$invalid\", lambda x: True)\n", "def test_not_expression():\n    assert evaluate(\"not test1\", lambda x: x == \"test2\")\n    assert not evaluate(\"not test1\", lambda x: x == \"test1\")\n", "def test_expression_evaluate():\n        if ident == 'test1':\n            return True\n        elif ident == 'test2':\n            return False\n        else:\n            return False\n\n    assert evaluate('test1', matcher) is True\n    assert evaluate('test2', matcher) is False\n    assert evaluate('test3', matcher) is False\n    assert evaluate('test1 and test2', matcher) is False\n    assert evaluate('test1 or test2', matcher) is True\n    assert evaluate('test1 and not test2', matcher) is True\n    assert evaluate('not test1', matcher) is False\n    assert evaluate('(test1 and test2) or test1', matcher) is True\n    assert evaluate('not (test1 and test2)', matcher) is True\n"], "sample_677": ["def test_and_expression():\n        return s in {\"test1\", \"test2\"}\n\n    assert evaluate(\"test1 and test2\", matcher) is True\n    assert evaluate(\"test1 and test3\", matcher) is False\n    assert evaluate(\"test3 and test2\", matcher) is False\n    assert evaluate(\"test3 and test4\", matcher) is False\n", "def test_parse_error():\n    with pytest.raises(ParseError):\n        evaluate(\"test$invalid\", lambda x: True)\n", "def test_evaluate_complex_expression(mocker):\n    matcher = mocker.MagicMock()\n    matcher.side_effect = lambda x: True if x in ['test1', 'test3', 'test5'] else False\n\n    assert evaluate(\"(test1 or test2) and not test3 and (test4 or test5)\", matcher) is True\n    assert evaluate(\"(test6 or test7) and not (test1 or test2)\", matcher) is False\n", "def test_evaluate_not_expression():\n    assert evaluate(\"not foo\", lambda x: x == \"foo\") is True\n    assert evaluate(\"not bar\", lambda x: x == \"foo\") is False\n", "def test_evaluate_expression_with_empty_matcher():\n        return False\n\n    assert not evaluate(\"test\", empty_matcher)\n"], "sample_376": ["    def test_update_cookie(self):\n        storage = CookieStorage()\n        response = self.client.get('/')\n\n        # Test setting the cookie\n        storage._update_cookie('test_data', response)\n        self.assertEqual(response.cookies[CookieStorage.cookie_name].value, 'test_data')\n        self.assertEqual(response.cookies[CookieStorage.cookie_name]['domain'], '.example.com')\n        self.assertEqual(response.cookies[CookieStorage.cookie_name]['secure'], True)\n        self.assertEqual(response.cookies[CookieStorage.cookie_name]['httponly'], True)\n\n        # Test deleting the cookie\n        response = self.client.get('/')\n        storage._update_cookie('', response)\n        self.assertNotIn(CookieStorage.cookie_name, response.cookies)\n", "    def test_large_messages_not_stored(self):\n        storage = self.get_storage()\n        response = self.get_response()\n        large_message = get_random_string(storage.max_cookie_size + 100)\n        message = Message(constants.DEBUG, large_message)\n        unstored_messages = storage._store([message], response)\n        self.assertEqual(len(unstored_messages), 1)\n        self.assertEqual(unstored_messages[0], message)\n        self.assertEqual(self.stored_messages_count(storage, response), 0)\n", "    def setUp(self):\n        self.storage = CookieStorage(request=None)\n", "    def test_store_retrieve_messages(self):\n        storage = CookieStorage(None)\n        messages = [\n            Message(constants.INFO, 'info message'),\n            Message(constants.ERROR, mark_safe('error message')),\n        ]\n        response = SimpleCookie()\n        unstored_messages = storage._store(messages, response)\n        self.assertEqual(unstored_messages, [])\n\n        retrieved_messages, all_retrieved = storage._get()\n        self.assertEqual(retrieved_messages, messages)\n        self.assertTrue(all_retrieved)\n\n        # Check that the messages are stored in a cookie\n        self.assertIn(storage.cookie_name, response)\n        cookie = response[storage.cookie_name]\n        self.assertEqual(cookie['domain'], settings.SESSION_COOKIE_DOMAIN)\n        self.assertEqual(cookie['secure'], 'True')\n        self.assertEqual(cookie['httponly'], 'True')\n\n        # Check that the messages can be retrieved from the cookie\n        set_cookie_data(storage, messages)\n        retrieved_messages, all_retrieved = storage._get()\n        self.assertEqual(retrieved_messages, messages)\n        self.assertTrue(all_retrieved)\n\n        # Check that a bad signature is handled correctly\n        set_cookie_data(storage, messages, invalid=True)\n        retrieved_messages, all_retrieved = storage._get()\n        self.assertIsNone(retrieved_messages)\n        self.assertTrue(all_retrieved)\n", "    def test_large_message_storage(self):\n        storage = self.get_storage()\n        message_text = get_random_string(self.max_cookie_size)\n        messages = [Message(constants.INFO, message_text) for _ in range(10)]\n        set_cookie_data(storage, messages)\n        self.assertEqual(self.stored_messages_count(storage, storage.response), 10)\n\n        # Reduce the max_cookie_size to simulate data that doesn't fit\n        storage.max_cookie_size = self.max_cookie_size - 1\n        unstored_messages = storage._store(messages, storage.response, remove_oldest=True)\n        self.assertEqual(self.stored_messages_count(storage, storage.response), 0)\n        self.assertEqual(len(unstored_messages), 10)\n"], "sample_185": ["def test_time_format_with_locale(self):\n    time_obj = datetime.time(14, 30, 45)\n    formatted_time = time_format(time_obj)\n    self.assertEqual(formatted_time, '14:30')\n", "def test_number_format(self):\n    value = 1234567.89\n    formatted_value = number_format(value, use_l10n=True)\n    self.assertEqual(formatted_value, '1.234.567,89')\n", "def test_localize_input_datetime(self):\n    value = datetime.datetime(2022, 1, 1, 12, 34, 56)\n    default = '%Y-%m-%d %H:%M:%S'\n    with patch_formats('en', DATETIME_INPUT_FORMATS=[default]):\n        result = localize_input(value)\n        self.assertEqual(result, '2022-01-01 12:34:56')\n\n    default = '%Y/%m/%d %H:%M'\n    with patch_formats('en', DATETIME_INPUT_FORMATS=[default]):\n        result = localize_input(value, default=default)\n        self.assertEqual(result, '2022/01/01 12:34')\n", "    def test_get_format_with_l10n(self):\n        with patch_formats('de', DATE_FORMAT='%d.%m.%Y'):\n            self.assertEqual(get_format('DATE_FORMAT', lang='de'), '%d.%m.%Y')\n            # Test that the result is cached\n            self.assertEqual(get_format('DATE_FORMAT', lang='de'), '%d.%m.%Y')\n", "def test_get_format_with_l10n_and_no_format_module(self):\n    with mock.patch.dict(settings.LANGUAGE_CODE, {'en'}):\n        with patch_formats('en', DATE_FORMAT='%d.%m.%Y'):\n            format = get_format('DATE_FORMAT', lang='en', use_l10n=True)\n            self.assertEqual(format, '%d.%m.%Y')\n"], "sample_405": ["    def test_create_model_operation_state_forwards(self):\n        operation = migrations.CreateModel(\n            name=\"TestModel\",\n            fields=[\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=50)),\n            ],\n            options={\"managed\": True},\n            bases=(models.Model,),\n            managers=[(\"objects\", models.Manager())],\n        )\n        state = ProjectState()\n        operation.state_forwards(\"test_app\", state)\n        self.assertIn((\"test_app\", \"testmodel\"), state.models)\n", "def test_rename_index(self):\n    operations = [\n        migrations.CreateModel(\n            \"RenameIndexModel\",\n            [\n                (\"name\", models.CharField(max_length=100)),\n                (\"age\", models.IntegerField()),\n            ],\n        ),\n        migrations.AddIndex(\n            \"RenameIndexModel\",\n            models.Index(fields=[\"name\"], name=\"old_name_index\"),\n        ),\n        migrations.RenameIndex(\n            \"RenameIndexModel\",\n            old_name=\"old_name_index\",\n            new_name=\"new_name_index\",\n        ),\n    ]\n\n    # Forwards\n    new_state = self.apply_operations(operations)\n    new_model = new_state.apps.get_model(\"app_label\", \"RenameIndexModel\")\n    self.assertEqual(\n        new_model._meta.indexes,\n        [\n            models.Index(fields=[\"name\"], name=\"new_name_index\"),\n        ],\n    )\n\n    with connection.schema_editor() as editor:\n        self.assertIndexName(\n            \"old_name_index\",\n            editor.connection.introspection.index_name_fixer(\"old_name_index\"),\n        )\n        self.assertIndexName(\n            \"new_name_index\",\n            editor.connection.introspection.index_name_fixer(\"new_name_index\"),\n        )\n        editor.create_model(new_model)\n        self.assertIndexExists(new_model, \"new_name_index\")\n        self.assertIndexNotExists(new_model, \"old_name_index\")\n\n    # Backwards\n    old_state = self.apply_operations(operations, reverse=True)\n    old_model = old_state.apps.get_model(\"app_label\", \"RenameIndexModel\")\n    self.assertEqual(\n        old_model._meta.indexes,\n        [\n            models.Index(fields=[\"name\"], name=\"old_name_index\"),\n        ],\n    )\n\n    with connection.schema_editor() as editor:\n        self.assertIndexExists(old", "    def test_add_index_and_rename(self):\n        operation = migrations.AddIndex(\"Author\", models.Index(fields=[\"name\"], name=\"author_name_idx\"))\n        new_state = operation.state_forwards(\"operations\", self.state)\n        self.assertIsNone(new_state.models[\"operations\", \"author\"].get_index_by_name(\"author_name_idx\"))\n        with self.assertRaisesMessage(FieldDoesNotExist, 'Field name not found.'):\n            new_state.models[\"operations\", \"author\"]._meta.get_field(\"author_name_idx\")\n\n        operation.database_forwards(\"operations\", self.schema_editor, self.state, new_state)\n        self.assertIndexExists(new_state.apps.get_model(\"operations\", \"Author\"), [\"name\"], name=\"author_name_idx\")\n\n        rename_operation = migrations.RenameIndex(\n            model_name=\"Author\",\n            new_name=\"author_name_idx_renamed\",\n            old_name=\"author_name_idx\",\n        )\n        renamed_state = rename_operation.state_forwards(\"operations\", new_state)\n        self.assertIsNone(renamed_state.models[\"operations\", \"author\"].get_index_by_name(\"author_name_idx\"))\n        self.assertIsNotNone(renamed_state.models[\"operations\", \"author\"].get_index_by_name(\"author_name_idx_renamed\"))\n\n        rename_operation.database_forwards(\"operations\", self.schema_editor, new_state, renamed_state)\n        self.assertIndexExists(renamed_state.apps.get_model(\"operations\", \"Author\"), [\"name\"], name=\"author_name_idx_renamed\")\n        self.assertIndexNotExists(renamed_state.apps.get_model(\"operations\", \"Author\"), [\"name\"], name=\"author_name_idx\")\n", "    def test_add_field_with_charfield_and_choices(self):\n        # Set up the initial state with a model and a field\n        initial_state = ProjectState()\n        initial_state.add_model(\n            ModelState(\n                \"test_app\",\n                \"TestModel\",\n                [(\"id\", models.AutoField(primary_key=True))],\n            )\n        )\n\n        # Define the operation to add a CharField with choices\n        operation = AddField(\"TestModel\", \"test_field\", models.CharField(max_length=10, choices=[(\"A\", \"Option A\"), (\"B\", \"Option B\")]))\n\n        # Forward operation - apply the operation to the initial state\n        new_state = initial_state.clone()\n        operation.state_forwards(\"test_app\", new_state)\n        self.assertIn(\"test_field\", dict(new_state.models[\"test_app\", \"testmodel\"].fields))\n\n        # Database operation - apply the operation to the database\n        with connection.schema_editor() as schema_editor:\n            operation.database_forwards(\"test_app\", schema_editor, initial_state, new_state)\n            self.assertTrue(new_state.apps.get_model(\"test_app\", \"TestModel\")._meta.get_field(\"test_field\"))\n\n        # Backward operation - reverse the operation on the database\n        with connection.schema_editor() as schema_editor:\n            operation.database_backwards(\"test_app\", schema_editor, new_state, initial_state)\n            with self.assertRaises(FieldDoesNotExist):\n                initial_state.apps.get_model(\"test_app\", \"TestModel\")._meta.get_field(\"test_field\")\n", "    def test_add_constraint(self):\n        operations = [\n            migrations.CreateModel(\"Food\", [(\"name\", models.CharField(max_length=50))]),\n            migrations.AddConstraint(\"Food\", models.CheckConstraint(check=models.Q(name__length__gt=0), name=\"positive_length\")),\n        ]\n        with self.assertNoDatabaseQueries():\n            with self.apply_operations(operations) as project_state:\n                food_model = project_state.apps.get_model(\"testapp\", \"Food\")\n                constraint = food_model._meta.constraints[0]\n                self.assertEqual(constraint.name, \"positive_length\")\n        # Test the forwards and backwards operations\n        with self.assertRaisesMessage(IntegrityError, \"positive_length\"):\n            with self.assertDatabaseQueries([\"CREATE CONSTRAINT\", \"ALTER TABLE\"]):\n                with transaction.atomic(using=self.connection.alias):\n                    self.apply_operations(operations)\n            with self.assertDatabaseQueries([\"ALTER TABLE\", \"DROP CONSTRAINT\"]):\n                with transaction.atomic(using=self.connection.alias):\n                    self.unapply_operations(operations)\n"], "sample_707": ["def test_iterparentnodeids(nodeid, expected):\n    parent_nodeids = list(nodes.iterparentnodeids(nodeid))\n    assert parent_nodeids == expected\n", "def test_iterparentnodeids(nodeid: str, expected: List[str]):\n    result = list(nodes.iterparentnodeids(nodeid))\n    assert result == expected\n", "def test_iterparentnodeids(nodeid: str, expected: List[str]) -> None:\n    result = list(nodes.iterparentnodeids(nodeid))\n    assert result == expected\n", "compilation error", "compilation error"], "sample_1014": ["def test_mutable_dense_ndim_array_getitem():\n    from sympy import MutableDenseNDimArray, symbols\n\n    i, j = symbols('i j')\n    a = MutableDenseNDimArray([0, 1, 2, 3], (2, 2))\n    assert a[0, 0] == 0\n    assert a[1, 1] == 3\n    assert a[i, j] == [[0, 1], [2, 3]][i, j]\n    assert a[i, j].subs({i: 1, j: 1}) == 3\n", "def test_mutable_dense_ndim_array_setitem():\n    from sympy import MutableDenseNDimArray, Symbol\n    a = MutableDenseNDimArray.zeros(2, 2)\n    a[0, 0] = 1\n    a[1, 1] = 2\n    assert a[0, 0] == 1\n    assert a[1, 1] == 2\n    assert a[0, 1] == 0\n    assert a[1, 0] == 0\n\n    a[0, 0] = Symbol('x')\n    assert a[0, 0] == Symbol('x')\n\n    raises(TypeError, lambda: a[0, 0] = [[1, 2], [3, 4]])\n", "def test_mutable_dense_ndim_array():\n    from sympy.tensor.array.dense_ndim_array import MutableDenseNDimArray\n    from sympy import symbols\n\n    a = MutableDenseNDimArray.zeros(2, 2)\n    a[0, 0] = 1\n    a[1, 1] = 1\n    assert a[0, 0] == 1\n    assert a[1, 1] == 1\n    assert a[0, 1] == 0\n    assert a[1, 0] == 0\n\n    b = a.as_immutable()\n    with raises(TypeError):\n        b[0, 0] = 2\n\n    i, j = symbols('i j')\n    c = MutableDenseNDimArray([[1, 2], [3, 4]], (2, 2))\n    assert c[i, j] == [[1, 2], [3, 4]][i, j]\n    assert c[i, j].subs({i: 1, j: 1}) == 4\n", "def test_mutable_dense_ndim_array_setitem():\n    from sympy import MutableDenseNDimArray, Symbol\n\n    a = MutableDenseNDimArray.zeros(2, 2)\n    a[0, 0] = 1\n    a[1, 1] = 2\n    assert a[0, 0] == 1\n    assert a[1, 1] == 2\n\n    x = Symbol('x')\n    a[0, 1] = x\n    assert a[0, 1] == x\n\n    with raises(TypeError):\n        a[0, 0] = None\n", "def test_dense_ndim_array_getitem():\n    a = MutableDenseNDimArray([0, 1, 2, 3], (2, 2))\n    assert a[0, 0] == 0\n    assert a[1, 1] == 3\n\n    i, j = symbols('i j')\n    assert a[i, j] == IndexedBase('[[0, 1], [2, 3]]')[i, j]\n    assert a[i, j].subs({i: 1, j: 1}) == 3\n\n    assert a[0, :] == [0, 1]\n    assert a[:, 0] == [0, 2]\n    assert a[:] == [[0, 1], [2, 3]]\n\n    b = a[0:2, 0:2]\n    assert isinstance(b, MutableDenseNDimArray)\n    assert b.shape == (2, 2)\n    assert b[1, 1] == 3\n"], "sample_402": ["    def test_should_redirect_with_slash(self):\n        \"\"\"\n        Test that should_redirect_with_slash method returns True when APPEND_SLASH is True,\n        the request path doesn't end with a slash, and appending a slash to the request path\n        turns an invalid path into a valid one.\n        \"\"\"\n        request = self.rf.get('/invalid_path')\n        request.urlconf = 'middleware.urls'\n        request.path_info = '/invalid_path'\n        middleware = CommonMiddleware()\n        self.assertTrue(middleware.should_redirect_with_slash(request))\n", "def test_denied_user_agent(self):\n    \"\"\"\n    Test that a request with a denied user agent is denied.\n    \"\"\"\n    denied_agent = \"DeniedAgent/1.0\"\n    with self.settings(DISALLOWED_USER_AGENTS=[re.compile(denied_agent)]):\n        request = self.rf.get(\"/\", HTTP_USER_AGENT=denied_agent)\n        middleware = CommonMiddleware()\n        with self.assertRaises(PermissionDenied):\n            middleware.process_request(request)\n", "def test_disallowed_user_agent(self):\n    user_agent = 'BadBot'\n    request = self.rf.get('/')\n    request.META['HTTP_USER_AGENT'] = user_agent\n    with self.settings(DISALLOWED_USER_AGENTS=[re.compile(user_agent)]):\n        middleware = CommonMiddleware()\n        with self.assertRaises(PermissionDenied):\n            middleware.process_request(request)\n", "    def test_user_agent_forbidden(self):\n        user_agent = \"TestBot\"\n        with self.settings(DISALLOWED_USER_AGENTS=[re.compile(user_agent)]):\n            request = self.rf.get('/', HTTP_USER_AGENT=user_agent)\n            middleware = CommonMiddleware()\n            with self.assertRaises(PermissionDenied):\n                middleware.process_request(request)\n", "    def test_should_redirect_with_slash(self):\n        request = self.rf.get('/some-url')\n        request.urlconf = 'middleware.urls'  # Replace this with your actual urlconf\n        self.assertTrue(CommonMiddleware().should_redirect_with_slash(request))\n\n        request = self.rf.get('/some-url/')\n        self.assertFalse(CommonMiddleware().should_redirect_with_slash(request))\n"], "sample_742": ["def test_logistic_regression_path():\n    \"\"\"Test logistic_regression_path function\"\"\"\n    coefs, Cs, n_iter = logistic_regression_path(X, Y1)\n    assert_equal(coefs.shape, (10, 2))\n    assert_equal(Cs.shape, (10,))\n    assert_equal(n_iter.shape, (10,))\n    assert_greater(np.all(n_iter), 0)\n", "def test_logistic_regression_path():\n    coefs, Cs, n_iter = logistic_regression_path(X, Y1, pos_class=1, Cs=10)\n    assert_equal(coefs.shape, (10, 3))\n    assert_equal(Cs.shape, (10,))\n    assert_equal(n_iter.shape, (10,))\n", "def test_logistic_regression_path():\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([0, 1, 0])\n    Cs = [0.1, 1.0]\n    coefs, Cs_out, n_iter = logistic_regression_path(X, y, Cs=Cs, fit_intercept=False)\n    assert_equal(coefs.shape, (2, 2))\n    assert_array_equal(Cs_out, np.array([0.1, 1.0]))\n    assert_equal(n_iter.shape, (2,))\n", "def test_logistic_regression_path():\n    \"\"\"Test logistic_regression_path function\"\"\"\n    coefs, Cs, n_iter = logistic_regression_path(X, Y1, Cs=[1.0], solver='lbfgs')\n    assert_equal(coefs.shape, (1, 3))\n    assert_equal(Cs.shape, (1,))\n    assert_equal(n_iter.shape, (1,))\n\n    coefs, Cs, n_iter = logistic_regression_path(X_sp, Y1, Cs=[1.0], solver='lbfgs')\n    assert_equal(coefs.shape, (1, 3))\n    assert_equal(Cs.shape, (1,))\n    assert_equal(n_iter.shape, (1,))\n\n    coefs, Cs, n_iter = logistic_regression_path(X, Y2, Cs=[1.0], multi_class='multinomial', solver='lbfgs')\n    assert_equal(coefs.shape, (1, 3, 3))\n    assert_equal(Cs.shape, (1,))\n    assert_equal(n_iter.shape, (1,))\n\n    coefs, Cs, n_iter = logistic_regression_path(X_sp, Y2, Cs=[1.0], multi_class='multinomial', solver='lbfgs')\n    assert_equal(coefs.shape, (1, 3, 3))\n    assert_equal(Cs.shape, (1,))\n    assert_equal(n_iter.shape, (1,))\n\n    # Test with sample_weight\n    sample_weight = np.array([1.0, 2.0, 3.0])\n    coefs, Cs, n_iter = logistic_regression_path(X, Y1, Cs=[1.0], solver='lbfgs', sample_weight=sample_weight)\n    assert_equal(coefs.shape, (1, 3", "def test_logistic_regression_path_with_sample_weight():\n    # Test logistic_regression_path function with sample_weight\n    X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n    y = np.array([0, 0, 1, 1])\n    sample_weight = np.array([0.5, 1, 0.5, 1])\n\n    coefs, _, _ = logistic_regression_path(X, y, sample_weight=sample_weight)\n\n    # The function should return a list of coefficients for each C value\n    assert isinstance(coefs, list)\n\n    # Check the shape of the coefficients\n    assert all(coef.shape == (2,) for coef in coefs)\n"], "sample_442": ["    def test_sign_and_unsign_object(self):\n        obj = {\"key\": \"value\"}\n        signed_obj = signing.dumps(obj)\n        unsigned_obj = signing.loads(signed_obj)\n        self.assertEqual(obj, unsigned_obj)\n", "    def test_unsign_object_compression(self):\n        obj = {'key': 'value'}\n        signed_obj = signing.dumps(obj, compress=True)\n        unsigned_obj = signing.loads(signed_obj)\n        self.assertEqual(obj, unsigned_obj)\n", "    def test_b64_encode_decode(self):\n        original_data = b'Hello, World!'\n        encoded_data = signing.b64_encode(original_data)\n        decoded_data = signing.b64_decode(encoded_data)\n        self.assertEqual(original_data, decoded_data)\n", "    def test_timestamp_signer_with_max_age(self):\n        signer = signing.TimestampSigner()\n        value = \"test-value\"\n        signed_value = signer.sign(value)\n\n        # Test unsign with max_age\n        unsigned_value = signer.unsign(signed_value, max_age=10)\n        self.assertEqual(unsigned_value, value)\n\n        # Test unsign with expired max_age\n        with self.assertRaises(signing.SignatureExpired):\n            signer.unsign(signed_value, max_age=0)\n\n        # Test unsign with max_age as timedelta\n        unsigned_value = signer.unsign(signed_value, max_age=datetime.timedelta(seconds=10))\n        self.assertEqual(unsigned_value, value)\n", "    def test_loads_invalid_signature(self):\n        value = 'test value'\n        signed_value = signing.dumps(value)\n        signed_value = signed_value.replace(':', ';')  # invalidate the signature\n        with self.assertRaises(signing.BadSignature):\n            signing.loads(signed_value)\n"], "sample_173": ["    def test_distinct_sql(self):\n        ops = BaseDatabaseOperations(connection=connection)\n        fields = ['field1', 'field2']\n        with self.assertRaises(NotSupportedError):\n            ops.distinct_sql(fields, None)\n        no_fields = []\n        self.assertEqual(ops.distinct_sql(no_fields, None), (['DISTINCT'], []))\n", "    def test_conditional_expression_supported_in_where_clause(self):\n        self.assertEqual(self.ops.conditional_expression_supported_in_where_clause(None), True)\n", "def test_savepoint_create_sql(self):\n    savepoint_id = 'test_savepoint'\n    sql = self.ops.savepoint_create_sql(savepoint_id)\n    expected_sql = 'SAVEPOINT \"{}\"'.format(savepoint_id)\n    self.assertEqual(sql, expected_sql)\n", "    def test_last_executed_query(self):\n        sql = \"SELECT * FROM my_table WHERE id = %s\"\n        params = (1,)\n        result = self.ops.last_executed_query(None, sql, params)\n        expected = \"QUERY = 'SELECT * FROM my_table WHERE id = %s' - PARAMS = (1,)\"\n        self.assertEqual(result, expected)\n", "    def test_date_trunc_sql(self):\n        lookup_type = 'day'\n        field_name = 'date_field'\n        with self.assertRaises(NotImplementedError) as cm:\n            self.ops.date_trunc_sql(lookup_type, field_name)\n        self.assertEqual(str(cm.exception), self.may_require_msg % 'date_trunc_sql')\n"], "sample_691": ["def test_faulthandler_timeout_option(pytester: Pytester) -> None:\n    \"\"\"Test that the faulthandler_timeout option works correctly.\"\"\"\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture(autouse=True)\n            import faulthandler\n            faulthandler.enable()\n        \"\"\"\n    )\n\n    pytester.makepyfile(\n        \"\"\"\n        import time\n\n            time.sleep(1)\n        \"\"\"\n    )\n\n    result = pytester.runpytest(\"-o\", \"faulthandler_timeout=0.5\", \"--capture=no\")\n    result.stdout.fnmatch_lines([\"*Dumping the traceback*\"])\n    assert \"test_timeout FAILED\" in result.stdout.str()\n", "def test_faulthandler_timeout_option(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import time\n            time.sleep(2)\n        \"\"\"\n    )\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        faulthandler_timeout = 1.0\n        \"\"\"\n    )\n    result = pytester.runpytest_inprocess()\n    result.stdout.fnmatch_lines([\"*Dumping the traceback for all threads*\"])\n    assert result.ret == 1\n", "def test_faulthandler_timeout_option(pytester):\n    pytester.makepyfile(\n        \"\"\"\n        import time\n            time.sleep(1)\n        \"\"\"\n    )\n    result = pytester.runpytest(\"-o\", \"faulthandler_timeout=0.5\")\n    assert result.ret == 0  # Test should pass even if it runs longer than the timeout\n", "def test_faulthandler_timeout_option(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import time\n        import pytest\n        import faulthandler\n\n        @pytest.mark.timeout(5)\n            faulthandler.disable()  # Disable faulthandler to prevent it from interfering with the test\n            time.sleep(10)  # Simulate a test that takes longer than the timeout\n        \"\"\"\n    )\n\n    result = pytester.runpytest(\"--faulthandler-timeout=2\", \"-vv\")\n\n    result.stdout.fnmatch_lines(\n        [\n            \"*test_timeout FAILED*\",\n            \"*FaultHandlerTimeoutError: Timeout occurred while executing test*\",\n        ]\n    )\n    assert \"faulthandler module enabled before pytest configuration step\" not in result.stderr.str()\n", "def test_faulthandler_timeout_option(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n\n            config.addinivalue_line(\"markers\", \"timeout: mark test as using faulthandler_timeout\")\n\n        @pytest.hookimpl(tryfirst=True)\n            if 'timeout' in item.keywords:\n                item.config._store[fault_handler_stderr_key] = io.StringIO()\n        \"\"\"\n    )\n\n    pytester.makepyfile(\n        \"\"\"\n        import time\n\n        import pytest\n\n        @pytest.mark.timeout\n            time.sleep(1)\n            assert \"test_fault_handler_timeout\" in sys._getframe().f_code.co_name\n        \"\"\"\n    )\n\n    result = pytester.runpytest(\"--faulthandler-timeout=0.5\")\n    result.assert_outcomes(passed=1)\n    assert \"Dumping the tracebacks of all threads\" not in result.stdout.str()\n\n    result = pytester.runpytest(\"--faulthandler-timeout=2\")\n    result.assert_outcomes(passed=1)\n    assert \"Dumping the tracebacks of all threads\" in result.stdout.str()\n"], "sample_428": ["    def test_large_floats(self):\n        \"\"\"Test that very large or small floats are treated as Decimals.\"\"\"\n        large_float = 1.23456789123456789e100\n        formatted_large_float = nformat(large_float, decimal_sep='.', decimal_pos=2)\n        self.assertEqual(formatted_large_float, '1.23e100')\n", "def test_scientific_notation(self):\n    # Test numbers with more than 200 digits\n    number = Decimal(\"1\" + \"0\" * 201)\n    result = nformat(number, decimal_sep=\".\", decimal_pos=None, grouping=0, thousand_sep=\",\", use_l10n=False)\n    self.assertEqual(result, \"1e201\")\n\n    # Test numbers with less than 200 digits\n    number = Decimal(\"12345678901234567890.12345678901234567890\")\n    result = nformat(number, decimal_sep=\".\", decimal_pos=None, grouping=0, thousand_sep=\",\", use_l10n=False)\n    self.assertEqual(result, \"12345678901234567890.12345678901234567890\")\n\n    # Test numbers with decimal_pos\n    number = Decimal(\"1234567890.1234567890\")\n    result = nformat(number, decimal_sep=\".\", decimal_pos=4, grouping=0, thousand_sep=\",\", use_l10n=False)\n    self.assertEqual(result, \"1234567890.1234\")\n", "def test_large_exponential_numbers(self):\n    number = Decimal(\"1.23456789\" * 100)  # This creates a number with more than 200 digits\n    result = nformat(number, decimal_sep='.', decimal_pos=3, use_l10n=False)\n    self.assertEqual(result, '1.235e273')  # The result should be in scientific notation\n", "    def test_float_with_exponent(self):\n        # Test formatting of a float with exponent\n        number = 12345678901234567890.123456789\n        expected_output = \"1.234567890123456789e+19\"\n        self.assertEqual(nformat(number, decimal_sep='.', decimal_pos=17, grouping=0, thousand_sep='', force_grouping=False), expected_output)\n", "def test_scientific_notation(self):\n    # Test number formatting with scientific notation for large/small numbers\n    self.assertEqual(nformat(Decimal(\"1.2345e20\"), decimal_sep=\".\", decimal_pos=2), \"1.23e20\")\n    self.assertEqual(nformat(Decimal(\"1.2345e-20\"), decimal_sep=\".\", decimal_pos=2), \"1.23e-20\")\n    self.assertEqual(nformat(float_info.max, decimal_sep=\".\", decimal_pos=2), f\"{float_info.max:e}\".split(\"e\")[0] + \"e308\")\n"], "sample_1134": ["compilation error", "def test_print_lowergamma():\n    assert latex(lowergamma(3, 4)) == r\"\\gamma\\left(3, 4\\right)\"\n", "def test_partial_derivative_printing():\n    expr = PartialDerivative(f(x, y, z), x, y)\n    assert latex(expr) == r\"\\frac{\\partial^{2}}{\\partial x \\partial y}{f\\left(x, y, z\\right)}\"\n", "compilation error", "def test_latex_printer_DiracDelta():\n    expr = DiracDelta(x - y, 0)\n    latex_str = latex(expr)\n    assert latex_str == r\"\\delta\\left(x - y\\right)\"\n\n    expr = DiracDelta(x - y, 1)\n    latex_str = latex(expr)\n    assert latex_str == r\"\\delta^{\\left( 1 \\right)}\\left( x - y \\right)\"\n"], "sample_1190": ["def test_get_units_non_prefixed():\n    system = SI()\n    non_prefixed_units = system.get_units_non_prefixed()\n    assert m in non_prefixed_units\n    assert kilogram in non_prefixed_units\n    assert second not in non_prefixed_units  # kilo is a prefix\n    assert volt not in non_prefixed_units  # volt is a derived unit\n", "def test_unit_system_extension():\n    base_units = (m, s)\n    units = (meter, second)\n    name = \"Custom\"\n    description = \"A custom unit system\"\n    dimension_system = None\n    derived_units = {energy: joule}\n\n    us = UnitSystem(base_units, units, name, description, dimension_system, derived_units)\n\n    extended_base_units = (km, hour)\n    extended_units = (kilometer, minute)\n    extended_name = \"Extended Custom\"\n    extended_description = \"An extended custom unit system\"\n    extended_derived_units = {length: meter, time: second}\n\n    extended_us = us.extend(extended_base_units, extended_units, extended_name, extended_description, dimension_system, extended_derived_units)\n\n    assert extended_us.name == extended_name\n    assert extended_us.descr == extended_description\n    assert set(extended_us._base_units) == set(base_units + extended_base_units)\n    assert set(extended_us._units) == set(units + extended_units)\n    assert extended_us._derived_units == {energy: joule, length: meter, time: second}\n", "def test_get_units_non_prefixed():\n    units = SI.get_units_non_prefixed()\n    assert meter in units\n    assert kilogram not in units\n    assert km not in units\n    assert PhysicalConstant(\"c\", \"speed of light\", SI.speed_of_light) not in units\n", "def test_unit_system_extend():\n    new_unit_system = SI.extend([Quantity(\"new_base_unit\")], [Quantity(\"new_unit\")], \"NewSystem\", \"Description of the new unit system\")\n    assert new_unit_system.name == \"NewSystem\"\n    assert new_unit_system.descr == \"Description of the new unit system\"\n    assert Quantity(\"new_base_unit\") in new_unit_system._base_units\n    assert Quantity(\"new_unit\") in new_unit_system._units\n    assert new_unit_system.get_dimension_system() is SI.get_dimension_system()\n", "def test_extend_unit_system():\n    # Extend the SI unit system with new base units\n    new_base_units = (Quantity('minute', abbrev='min'), Quantity('inch', abbrev='in'))\n    new_units = (Quantity('foot', abbrev='ft'), Quantity('quart', abbrev='qt'))\n    extended_SI = SI.extend(new_base_units, new_units, name='ExtendedSI', description='SI with additional units')\n\n    # Check if the base units and units are correctly extended\n    assert len(extended_SI._base_units) == len(SI._base_units) + len(new_base_units)\n    assert len(extended_SI._units) == len(SI._units) + len(new_units)\n\n    # Check if the name and description are correctly set\n    assert extended_SI.name == 'ExtendedSI'\n    assert extended_SI.descr == 'SI with additional units'\n"], "sample_719": ["def test_TfidfVectorizer_empty_vocabulary():\n    vectorizer = TfidfVectorizer()\n    assert_raise_message(ValueError, \"empty vocabulary; perhaps the documents only contain stop words\",\n                         vectorizer.fit, [\"this is a test\"])\n", "def test_tfidf_vectorizer_with_custom_analyzer():\n    vectorizer = TfidfVectorizer(analyzer=split_tokenize)\n    X = vectorizer.fit_transform(ALL_FOOD_DOCS)\n    assert_true(sparse.issparse(X))\n    assert_equal(X.shape, (11, 14))\n", "def test_tfidf_vectorizer_with_custom_analyzer():\n    vect = TfidfVectorizer(analyzer=split_tokenize)\n    X = vect.fit_transform(ALL_FOOD_DOCS)\n    assert_equal(X.shape[0], len(ALL_FOOD_DOCS))\n    assert_equal(X.shape[1], 12)\n", "def test_hashing_vectorizer_transform_and_fit_transform():\n    corpus = [\n        \"This is the first document.\",\n        \"This is the second second document.\"\n    ]\n    vectorizer = HashingVectorizer(n_features=20)\n    X = vectorizer.transform(corpus)\n    assert sparse.isspmatrix_csr(X)\n    assert X.shape == (2, 20)\n\n    X_fit = vectorizer.fit_transform(corpus)\n    assert_array_equal(X.toarray(), X_fit.toarray())\n", "def test_hashingvectorizer_transform_binary():\n    vectorizer = HashingVectorizer(binary=True)\n    X = vectorizer.fit_transform(ALL_FOOD_DOCS)\n    X_transform = vectorizer.transform(ALL_FOOD_DOCS)\n    assert_array_equal(X.toarray(), X_transform.toarray())\n    assert_array_equal(X_transform.toarray(), np.ones_like(X.toarray()))\n"], "sample_1181": ["def test_print_DiagonalMatrix():\n    expr = Array(Identity(3)*x)\n    code = SciPyPrinter().doprint(expr)\n    expected_code = \"numpy.multiply(numpy.eye(3, 3), x)\"\n    assert str(code) == expected_code\n", "def test_print_array():\n    A = Array(x**2, (i, j), (3, 3))\n    np_code = NumPyPrinter().doprint(A)\n    assert np_code == \"numpy.array([[x**2, x**2, x**2], [x**2, x**2, x**2], [x**2, x**2, x**2]])\"\n", "def test_logaddexp_and_logaddexp2():\n    expr = logaddexp(x, y) + logaddexp2(a, b)\n    printer = NumPyPrinter()\n    result = printer.doprint(expr)\n    assert result == \"numpy.logaddexp(x, y) + numpy.logaddexp2(a, b)\"\n", "def test_SciPyPrinter_betainc():\n    from sympy import beta, symbols\n    from sympy.functions.special.beta_functions import betainc\n\n    a, b, x, y = symbols('a b x y')\n    e = betainc(a, b, x, y)\n    f = SciPyPrinter().doprint(e)\n    assert eval(f) == beta(a, b).evalf(3) * (betainc(a, b, y) - betainc(a, b, x))\n", "def test_MatrixSolve_printing():\n    A = MatrixSymbol('A', 3, 3)\n    b = MatrixSymbol('b', 3, 1)\n    expr = MatrixSolve(A, b)\n    code = NumPyPrinter().doprint(expr)\n    assert code == \"numpy.linalg.solve(A, b)\"\n"], "sample_98": ["    def test_server_threaded(self):\n        response = self.urlopen('/threaded/')\n        self.assertContains(response, 'Threaded WSGI server')\n", "    def test_request_handler(self):\n        # Create a socket and send a GET request to the live server\n        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        s.connect((self.live_server_url_test[0].split('://')[1].split(':')[0], self.live_server_url_test[0].split(':')[2]))\n        s.sendall(b'GET / HTTP/1.1\\r\\nHost: localhost\\r\\n\\r\\n')\n        response = s.recv(1024)\n        s.close()\n\n        # Check that the response starts with \"HTTP/1.1 200 OK\"\n        self.assertTrue(response.startswith(b'HTTP/1.1 200 OK'))\n\n        # Check that the response contains the expected content\n        self.assertIn(b'<title>LiveServerTestCase</title>', response)\n", "    def test_server_thread_creation(self):\n        server_thread = LiveServerThread(WSGIServer, QuietWSGIRequestHandler, ('localhost', 8081))\n        server_thread.daemon = True\n        server_thread.start()\n        self.assertTrue(server_thread.is_alive())\n        server_thread.stop()\n        server_thread.join()\n        self.assertFalse(server_thread.is_alive())\n", "    def setUpClass(cls):\n        super().setUpClass()\n        cls.live_server_thread = LiveServerThread(\n            port=cls.server_thread.port,\n            host=cls.server_thread.host,\n            wsgi_handler=cls.server_thread.wsgi_handler,\n            request_handler=QuietWSGIRequestHandler,\n            server_cls=WSGIServer,\n        )\n        cls.live_server_thread.daemon = True\n        cls.live_server_thread.start()\n", "    def setUp(self):\n        super().setUp()\n        self.wsgi_app = get_internal_wsgi_application()\n"], "sample_868": ["def test_mutual_info_score_consistency():\n    # Test that mutual_info_score is consistent with the contingency matrix\n    contingency = contingency_matrix(y1, y2)\n    score1 = mutual_info_score(y1, y2)\n    score2 = mutual_info_score(None, None, contingency=contingency)\n    assert_allclose(score1, score2)\n", "def test_silhouette_score():\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n    labels = np.array([0, 0, 1, 1])\n\n    score = silhouette_score(X, labels)\n    assert_allclose(score, 0.5, atol=1e-2)\n", "def test_silhouette_score_input_layout():\n    # Test the silhouette_score function with different input layouts\n    X = rng.random_sample(size=(30, 2))\n    labels = rng.randint(3, size=30)\n\n    # Test with default sample_size and input as a dense array\n    score_dense = silhouette_score(X, labels)\n\n    # Test with sample_size=20 and input as a CSR sparse matrix\n    from scipy.sparse import csr_matrix\n    X_sparse = csr_matrix(X)\n    score_sparse = silhouette_score(X_sparse, labels, sample_size=20)\n\n    assert_allclose(score_dense, score_sparse)\n", "def test_silhouette_score_error_on_single_cluster():\n    y = np.zeros(10)\n    X = rng.rand(10, 2)\n    with pytest.raises(ValueError, match=\"Number of labels is 1. Valid values are 2 to n_samples - 1 (inclusive)\"):\n        silhouette_score(X, y)\n", "def test_adjusted_mutual_info_score_average_method():\n    ami_arithmetic = adjusted_mutual_info_score(y1, y2, average_method='arithmetic')\n    ami_geometric = adjusted_mutual_info_score(y1, y2, average_method='geometric')\n    ami_min = adjusted_mutual_info_score(y1, y2, average_method='min')\n    ami_max = adjusted_mutual_info_score(y1, y2, average_method='max')\n    assert_allclose(ami_arithmetic, ami_geometric, rtol=1e-5)\n    assert ami_min <= ami_arithmetic <= ami_max\n"], "sample_636": ["def test_ignore_signatures(capsys) -> None:\n    with _patch_streams(capsys.out):\n        Run([join(DATA, \"func_signatures.py\"), \"--disable=all\", \"--enable=R0801\", \"--ignore-signatures\"])\n    out, _ = capsys.readouterr()\n    assert \"Similar lines in 2 files\" not in out\n", "def test_duplicate_code_in_single_file(capsys):\n    args = [\"--disable=all\", \"--enable=similarities\", join(DATA, \"duplicate_code_single_file.py\")]\n    Run(args)\n    out, err = capsys.readouterr()\n    assert \"4 similar lines in 2 files\" in out\n", "def test_duplicate_code_with_signatures(capfd):\n    with _patch_streams(capfd.out):\n        Run([f\"--ignore-signatures={DATA}/code_with_signatures.py\"])\n        out, _ = capfd.readouterr()\n        assert \"4 similar lines in 2 files\" in out\n        assert \"code_with_signatures.py:[2:6]\" in out\n        assert \"code_with_signatures.py:[8:12]\" in out\n        assert \"def func1(a, b):\" not in out\n        assert \"def func2(c, d):\" not in out\n", "def test_ignore_signatures(capfd: pytest.CaptureFixture[str]) -> None:\n    args = [\n        \"--min-similarity-lines=2\",\n        \"--ignore-signatures\",\n        join(DATA, \"similar_functions_signatures.py\"),\n    ]\n    with _patch_streams(StringIO()):\n        Run(args)\n    out, _ = capfd.readouterr()\n    assert \"similar_functions_signatures.py:13:0: R0801: Similar lines in 2 files\" not in out\n", "def test_similar_code_checker_duplicates_in_single_file(capsys):\n    args = [\n        \"--disable=all\",\n        \"--enable=similarities\",\n        \"--min-similarity-lines=3\",\n        \"--reports=n\",\n        join(DATA, \"duplicate_code.py\"),\n    ]\n    Run(args)\n    out, _ = capsys.readouterr()\n    assert \"3 similar lines in 2 files\" in out\n"], "sample_500": ["def test_colorbar_alpha():\n    \"\"\"\n    Test the alpha parameter for the colorbar.\n    \"\"\"\n    fig, ax = plt.subplots()\n    im = ax.imshow(np.random.rand(10, 10), cmap='viridis', alpha=0.5)\n    fig.colorbar(im, ax=ax, alpha=0.8)\n    return fig\n", "def test_colorbar_spacing():\n    \"\"\"\n    Test the spacing parameter for colorbar.\n    \"\"\"\n    # Get a colormap and appropriate norms for each extension type.\n    cmap, norms = _get_cmap_norms()\n    # Create a figure and adjust whitespace for subplots.\n    fig, axs = plt.subplots(2, 1, figsize=(6, 10))\n    for i, spacing in enumerate(('uniform', 'proportional')):\n        # Get the appropriate norm and use it to get colorbar boundaries.\n        norm = norms['both']\n        boundaries = values = norm.boundaries\n        values = values[:-1]\n        # Generate the colorbar.\n        Colorbar(axs[i], cmap=cmap, norm=norm,\n                 boundaries=boundaries, values=values,\n                 extend='both', extendrect=True,\n                 orientation='vertical', spacing=spacing)\n        # Turn off text and ticks.\n        axs[i].tick_params(left=False, labelleft=False,\n                           bottom=False, labelbottom=False)\n    # Return the figure to the caller.\n    return fig\n", "def test_colorbar_ticks():\n    \"\"\"Test custom ticks on colorbar.\"\"\"\n    fig, ax = plt.subplots()\n    cmap = cm.get_cmap('viridis')\n    norm = Normalize(vmin=0, vmax=10)\n    mappable = cm.ScalarMappable(cmap=cmap, norm=norm)\n    mappable.set_array([])\n    cbar = fig.colorbar(mappable, ticks=[2, 4, 6, 8], orientation='horizontal')\n    assert np.array_equal(cbar.get_ticks(), [2, 4, 6, 8])\n    plt.close(fig)\n", "def test_colorbar_ticks_labels():\n    \"\"\"Test setting ticks and labels on colorbar.\"\"\"\n    fig, ax = plt.subplots()\n    im = ax.imshow(np.arange(100).reshape((10, 10)), cmap='viridis')\n    cbar = fig.colorbar(im, ticks=[20, 50, 80], orientation='horizontal')\n    cbar.ax.set_xticklabels(['Low', 'Medium', 'High'])\n    return fig\n", "def test_colorbar_ticks_and_labels():\n    \"\"\"\n    Test setting custom ticks and labels on the colorbar.\n    \"\"\"\n    # Create a figure and axis\n    fig, ax = plt.subplots()\n\n    # Define a colormap and norm\n    cmap = cm.get_cmap('viridis', 5)\n    norm = Normalize(vmin=0, vmax=1)\n\n    # Create a mappable and colorbar\n    mappable = cm.ScalarMappable(norm=norm, cmap=cmap)\n    cbar = fig.colorbar(mappable, ax=ax)\n\n    # Set custom ticks and labels\n    ticks = [0.2, 0.4, 0.6, 0.8]\n    labels = ['Low', 'Medium', 'High', 'Very High']\n    cbar.set_ticks(ticks)\n    cbar.set_ticklabels(labels)\n\n    # Check if ticks and labels are set correctly\n    assert np.allclose(cbar.get_ticks(), ticks)\n    assert [t.get_text() for t in cbar.ax.yaxis.get_ticklabels()] == labels\n\n    plt.close(fig)\n"], "sample_75": ["def test_prefetch_related_on_m2m_field(self):\n    with CaptureQueriesContext(connection) as ctx:\n        authors = Author.objects.prefetch_related('books_written')\n        for author in authors:\n            # Accessing the prefetch_related field doesn't trigger additional queries\n            _ = author.books_written.all()\n    # Check the number of queries\n    self.assertEqual(len(ctx), 2)\n", "def test_save_form_data(self):\n    book = Book.objects.create(title='Test Book')\n    data = [self.author1.pk, self.author2.pk]\n    book.authors.save_form_data(book, data)\n    self.assertCountEqual(book.authors.all(), [self.author1, self.author2])\n", "    def test_related_name_with_multiple_many_to_many_relations(self):\n        author1_books = Author.objects.filter(name='Charlotte').prefetch_related('books')\n        author3_books = Author.objects.filter(name='Emily').prefetch_related('books')\n\n        self.assertEqual(len(author1_books), 1)\n        self.assertEqual(len(author3_books), 1)\n\n        self.assertEqual(len(author1_books[0].books.all()), 2)\n        self.assertEqual(len(author3_books[0].books.all()), 2)\n\n        self.assertEqual({book.title for book in author1_books[0].books.all()}, {'Poems', 'Jane Eyre'})\n        self.assertEqual({book.title for book in author3_books[0].books.all()}, {'Poems', 'Wuthering Heights'})\n", "def test_prefetch_related_query_count(self):\n    with self.assertNumQueries(3):\n        readers = Reader.objects.prefetch_related('books_read').all()\n        for reader in readers:\n            for book in reader.books_read.all():\n                pass\n", "def test_m2m_query_optimization(self):\n    with self.assertNumQueries(1):\n        list(Book.objects.prefetch_related('authors'))\n\n    with self.assertNumQueries(1):\n        list(Book.objects.prefetch_related('readers'))\n"], "sample_89": ["    def test_zip_imported_module(self):\n        filename = self.temporary_file('test_module.zip')\n        with zipfile.ZipFile(filename, 'w') as z:\n            z.writestr('test_module.py', '')\n        sys.path.insert(0, str(filename))\n        self.addCleanup(sys.path.remove, str(filename))\n        self.import_and_cleanup('test_module')\n        self.assertFileFound(filename)\n", "    def test_zip_imported_module(self):\n        with extend_sys_path(tempfile.mkdtemp()):\n            zip_path = Path(sys.path[-1]) / 'test_zip.zip'\n            zip_path.touch()\n            self.addCleanup(zip_path.unlink)\n\n            # Create a simple module in the zip file\n            with zipfile.ZipFile(zip_path, 'w') as zipf:\n                zipf.writestr('test_module.py', 'print(\"Hello, World!\")')\n\n            # Import the module from the zip file\n            sys.path_importer_cache.clear()\n            sys.modules.pop('test_module', None)\n            import_module('test_module')\n\n            # Check if the module file is found\n            self.assertFileFound(zip_path)\n", "    def test_iter_modules_and_files_with_zip_import(self):\n        zip_filename = self.temporary_file('test.zip')\n        with zipfile.ZipFile(zip_filename, 'w') as zf:\n            zf.writestr('test_module.py', '')\n        sys.path.insert(0, str(zip_filename))\n        self.addCleanup(sys.path.remove, str(zip_filename))\n        self.import_and_cleanup('test_module')\n        self.assertFileFound(zip_filename)\n", "    def test_error_file_handling(self):\n        error_file = self.temporary_file('error_module.py')\n        error_file.write_text('syntax error')\n\n        with self.assertRaises(SyntaxError):\n            self.import_and_cleanup('error_module')\n\n        self.assertFileNotFound(error_file)\n\n        # Fix the syntax error\n        error_file.write_text('print(\"Fixed\")')\n        self.import_and_cleanup('error_module')\n        self.assertFileFound(error_file)\n", "def test_zip_import(self):\n    zip_filename = self.temporary_file('test.zip')\n    zip_file_path = str(zip_filename)\n    with zipfile.ZipFile(zip_filename, 'w') as zf:\n        zf.writestr('test_module.py', 'test_content = 1')\n    sys.path.append(zip_file_path)\n    self.addCleanup(sys.path.remove, zip_file_path)\n    self.import_and_cleanup('test_module')\n    self.assertFileFound(zip_filename)\n"], "sample_847": ["def test_enet_path_positive_coef():\n    X = np.array([[1, 2, 3.1], [2.3, 5.4, 4.3]]).T\n    y = np.array([1, 2, 3.1])\n    with pytest.raises(ValueError, match=\"positive=True is not allowed for multi-output\"):\n        enet_path(X, y[:, np.newaxis], l1_ratio=0.5, positive=True)\n", "def test_elastic_net_cv_with_sparse_input():\n    X, y = load_boston(return_X_y=True)\n    X = sparse.csr_matrix(X)\n    elastic_net_cv = ElasticNetCV(cv=5, l1_ratio=[.1, .5, .7, .9, .95, .99, 1])\n    elastic_net_cv.fit(X, y)\n    assert hasattr(elastic_net_cv, 'alpha_')\n    assert hasattr(elastic_net_cv, 'l1_ratio_')\n    assert hasattr(elastic_net_cv, 'coef_')\n    assert hasattr(elastic_net_cv, 'intercept_')\n    assert hasattr(elastic_net_cv, 'mse_path_')\n    assert hasattr(elastic_net_cv, 'alphas_')\n    assert hasattr(elastic_net_cv, 'n_iter_')\n", "def test_enet_path_l1_ratio_zero():\n    X, y = load_boston(return_X_y=True)\n    with pytest.raises(ValueError):\n        enet_path(X, y, l1_ratio=0.0)\n", "def test_enet_path_positive_constraint_multitask():\n    # Test the positive constraint in enet_path for multitask\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([[1, 2], [3, 4], [5, 6]])\n    with pytest.raises(ValueError, match=\"positive=True is not allowed for multi-output\"):\n        enet_path(X, y, positive=True)\n", "def test_elasticnet_path():\n    X, y = load_boston(return_X_y=True)\n    n_samples, n_features = X.shape\n    _, n_alphas = lasso_path(X, y, eps=1e-3, n_alphas=10)\n    _, coefs, dual_gaps = enet_path(X, y, l1_ratio=0.5, eps=1e-3, n_alphas=n_alphas)\n    assert coefs.shape == (n_features, n_alphas)\n    assert dual_gaps.shape == (n_alphas,)\n\n    _, coefs, dual_gaps = enet_path(X, y, l1_ratio=1.0, eps=1e-3, n_alphas=n_alphas)\n    assert coefs.shape == (n_features, n_alphas)\n    assert dual_gaps.shape == (n_alphas,)\n\n    _, coefs, dual_gaps = enet_path(X, y, l1_ratio=0.0, eps=1e-3, n_alphas=n_alphas)\n    assert coefs.shape == (n_features, n_alphas)\n    assert dual_gaps.shape == (n_alphas,)\n"], "sample_692": ["def test_mktemp_unnumbered(tmp_path_factory: TempPathFactory) -> None:\n    basename = \"unnumbered_dir\"\n    p = tmp_path_factory.mktemp(basename, numbered=False)\n    assert p.is_dir()\n    assert p.name == basename\n    assert p.parent == tmp_path_factory.getbasetemp()\n", "def test_mktemp_with_relative_path(tmp_path_factory: TempPathFactory):\n    basename = \"test_dir\"\n    temp_dir = tmp_path_factory.mktemp(basename)\n    assert temp_dir.name == basename\n    assert temp_dir.parent == tmp_path_factory.getbasetemp()\n", "def mock_getuser(monkeypatch: MonkeyPatch) -> Callable[[str], None]:\n        monkeypatch.setattr(get_user, \"__returns__\", value)\n\n    return mock_return_value\n", "def test_temp_path_factory_getbasetemp_with_given_basetemp(tmp_path: Path):\n    given_basetemp = tmp_path / \"test_basetemp\"\n    given_basetemp.mkdir()\n    factory = TempPathFactory(given_basetemp=given_basetemp, trace=lambda *args: None)\n    basetemp = factory.getbasetemp()\n    assert basetemp == given_basetemp\n    assert basetemp.is_dir()\n", "def test_TempPathFactory_mktemp_existing_directory():\n    factory = TempPathFactory(given_basetemp=None, trace=None)\n    factory.getbasetemp().joinpath(\"foo-0\").mkdir()\n    with pytest.raises(ValueError, match=\"is not a normalized and relative path\"):\n        factory.mktemp(\"../foo-0\", numbered=True)\n"], "sample_795": ["    def test_check_outlier_corruption(self):\n        num_outliers = 35\n        expected_outliers = 30\n        decision = np.array([1] * (num_outliers - expected_outliers) + [0] * expected_outliers)\n        with self.assertRaises(AssertionError):\n            check_outlier_corruption(num_outliers, expected_outliers, decision)\n", "    def fit(self, X, y=None):\n        X = check_array(X, accept_sparse='csr')\n        return self\n", "    def __init__(self, param=None):\n        self.param = param\n", "    def test_check_outlier_corruption(self):\n        # Test with equal number of outliers\n        num_outliers = 10\n        expected_outliers = 10\n        decision = np.concatenate([np.ones(num_outliers), np.zeros(90)])\n        check_outlier_corruption(num_outliers, expected_outliers, decision)\n\n        # Test with fewer outliers than expected\n        num_outliers = 5\n        expected_outliers = 10\n        decision = np.concatenate([np.ones(num_outliers), np.zeros(95)])\n        check_outlier_corruption(num_outliers, expected_outliers, decision)\n\n        # Test with more outliers than expected\n        num_outliers = 20\n        expected_outliers = 10\n        decision = np.concatenate([np.ones(num_outliers), np.zeros(80)])\n        check_outlier_corruption(num_outliers, expected_outliers, decision)\n\n        # Test with no outliers\n        num_outliers = 0\n        expected_outliers = 0\n        decision = np.zeros(100)\n        check_outlier_corruption(num_outliers, expected_outliers, decision)\n\n        # Test with all outliers\n        num_outliers = 100\n        expected_outliers = 100\n        decision = np.ones(100)\n        check_outlier_corruption(num_outliers, expected_outliers, decision)\n\n        # Test with error: num_outliers not within expected range\n        num_outliers = 5\n        expected_outliers = 10\n        decision = np.concatenate([np.ones(6), np.zeros(94)])\n        with self.assertRaises(AssertionError):\n            check_", "    def test_check_outlier_corruption(self):\n        num_outliers = 25\n        expected_outliers = 30\n        decision = np.array([0.5, 0.4, 0.3, 0.2, 0.1] * 5)\n\n        # This should not raise an assertion error\n        check_outlier_corruption(num_outliers, expected_outliers, decision)\n\n        num_outliers = 20\n        expected_outliers = 30\n        decision = np.array([0.5, 0.4, 0.3, 0.2, 0.1] * 5)\n\n        # This should raise an assertion error\n        with self.assertRaises(AssertionError):\n            check_outlier_corruption(num_outliers, expected_outliers, decision)\n"], "sample_0": ["def test_pickle_uncert(UncertClass):\n    uncert = UncertClass(np.array([1, 2, 3]), unit='m')\n    pickled_uncert = pickle.dumps(uncert)\n    unpickled_uncert = pickle.loads(pickled_uncert)\n    assert_array_equal(uncert.array, unpickled_uncert.array)\n    assert uncert.unit == unpickled_uncert.unit\n", "def test_uncertainty_quantity_property(UncertClass):\n    uncert = UncertClass(array=[1, 2, 3], unit='m')\n    assert_allclose(uncert.quantity.value, [1, 2, 3])\n    assert uncert.quantity.unit == u.m\n", "def test_array_setter(UncertClass):\n    uncert = UncertClass()\n    uncert.array = [1, 2, 3]\n    assert_array_equal(uncert.array, np.array([1, 2, 3]))\n", "def test_unit_setting(UncertClass):\n    # Test setting unit on initialization\n    uncert = UncertClass(array=[1, 2, 3], unit=\"m\")\n    assert uncert.unit == u.m\n\n    # Test setting unit via property\n    uncert.unit = \"km\"\n    assert uncert.unit == u.km\n\n    # Test setting unit to None\n    uncert.unit = None\n    assert uncert.unit is None\n", "def test_uncertainty_convert_from_variance(UncertClass):\n    if UncertClass in uncertainty_types_with_conversion_support:\n        var_uncert = VarianceUncertainty([1, 4, 9], unit='m^2')\n        converted_uncert = UncertClass._convert_from_variance(var_uncert)\n        assert_allclose(converted_uncert.array, [1, 2, 3])\n        assert converted_uncert.unit == u.m\n    else:\n        var_uncert = VarianceUncertainty([1, 4, 9], unit='m^2')\n        with pytest.raises(TypeError):\n            UncertClass._convert_from_variance(var_uncert)\n"], "sample_559": ["def test_axes_divider_locatable_axes():\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    divider = make_axes_locatable(ax)\n    assert divider.locatable_axes == ax\n", "def test_stackplot_empty():\n    fig, ax = plt.subplots()\n    ax.stackplot([], [])\n", "def test_streamplot_log_scale(scale):\n    Y, X = np.mgrid[-3:3:100j, -3:3:100j]\n    U = -1 - X**2 + Y\n    V = 1 + X - Y**2\n    fig, ax = plt.subplots()\n    ax.streamplot(X, Y, U, V, density=[0.5, 1])\n    if scale == \"log\":\n        ax.set_xscale('log')\n        ax.set_yscale('log')\n    fig.canvas.draw()\n", "def test_axes_grid1_demo():\n        fig = plt.figure(figsize=(3.5, 3.5))\n        grid = ImageGrid(fig, 111,  # similar to subplot(111)\n                         nrows_ncols=(2, 2),  # creates 2x2 grid of axes\n                         axes_pad=0.1,  # pad between axes in inch.\n                         )\n\n        for ax, i in zip(grid, range(1, 5)):\n            ax.text(0.5, 0.5, f\"ax{i}\", va=\"center\", ha=\"center\", size=20,\n                    bbox=dict(facecolor=\"yellow\", alpha=0.5),\n                    transform=ax.transAxes)\n\n        return fig\n\n    fig = get_demo_image()\n", "def test_divider_with_inset():\n    fig = plt.figure()\n    ax = host_subplot(111)\n\n    ax.plot([1, 2, 3], [3, 4, 5], \"r-\", label=\"100 %\")\n    ax.plot([1, 2, 3], [1, 2, 3], \"g-\", label=\"50 %\")\n    ax.plot([1, 2, 3], [1, 0.5, 1], \"b-\", label=\"20 %\")\n    ax.set_title(\"Host axes\")\n\n    divider = make_axes_locatable(ax)\n    ax_top = divider.new_vertical(size=\"50%\", pad=0.1)\n    fig.add_axes(ax_top)\n\n    ax_top.plot([1, 2, 3], [2, 3, 4], \"r-\", label=\"100 %\")\n    ax_top.plot([1, 2, 3], [1, 2, 3], \"g-\", label=\"50 %\")\n    ax_top.plot([1, 2, 3], [1, 0.5, 1], \"b-\", label=\"20 %\")\n    ax_top.set_title(\"Top axes\")\n\n    divider = make_axes_locatable(ax_top)\n    ax_inset = divider.new_horizontal(size=\"30%\", pad=0.1)\n    fig.add_axes(ax_inset)\n\n    ax_inset.plot([1, 2, 3], [2, 3, 4], \"r-\", label=\"100 %\")\n    ax_inset.plot([1, 2, 3], [1, 2, 3], \"g-\", label=\"50 %\")\n    ax_inset.plot([1, 2, 3], [1, 0.5, 1],"], "sample_684": ["def test_repr_func_args():\n    args = [(\"arg1\", 1), (\"arg2\", \"two\")]\n    repr_args = ReprFuncArgs(args)\n    tw = mock.Mock()\n    repr_args.toterminal(tw)\n    tw.line.assert_called_with(\"arg1 = 1, arg2 = two\")\n", "def test_exception_info_match():\n    class MyException(Exception):\n        pass\n\n    try:\n        raise MyException(\"Hello, World!\")\n    except MyException:\n        excinfo = ExceptionInfo.from_current()\n        assert excinfo.match(\"Hello, World!\")\n        assert excinfo.match(re.compile(\"Hello, World!\"))\n        with pytest.raises(AssertionError, match=\"Regex pattern 'Goodbye, World!' does not match 'Hello, World!'.\"):\n            excinfo.match(\"Goodbye, World!\")\n", "def test_exception_info_from_current():\n    # Mock sys.exc_info to return a known exception\n    mock_exc_info = (Exception, Exception(\"Test exception\"), None)\n    with mock.patch(\"sys.exc_info\", return_value=mock_exc_info):\n        # Call the function under test\n        exc_info = ExceptionInfo.from_current()\n\n        # Check that the exception info was created correctly\n        assert exc_info.type is Exception\n        assert str(exc_info.value) == \"Test exception\"\n        assert exc_info.tb is None\n", "def test_filter_traceback():\n    # Test with a traceback entry from a generated code\n    generated_code = compile(\"x = 1\", \"<generated>\", \"exec\")\n    frame = FrameType(generated_code, globals(), globals(), [], 1, {}, None, None)\n    entry = TracebackEntry(mock.Mock(tb_frame=frame, tb_lineno=1))\n    assert filter_traceback(entry) is False\n\n    # Test with a traceback entry from pytest directory\n    pytest_code = compile(\"x = 1\", str(_PYTEST_DIR / \"file.py\"), \"exec\")\n    frame = FrameType(pytest_code, globals(), globals(), [], 1, {}, None, None)\n    entry = TracebackEntry(mock.Mock(tb_frame=frame, tb_lineno=1))\n    assert filter_traceback(entry) is False\n\n    # Test with a traceback entry from a non-generated and non-pytest code\n    code = compile(\"x = 1\", \"/path/to/file.py\", \"exec\")\n    frame = FrameType(code, globals(), globals(), [], 1, {}, None, None)\n    entry = TracebackEntry(mock.Mock(tb_frame=frame, tb_lineno=1))\n    assert filter_traceback(entry) is True\n", "def test_getargs():\n        pass\n\n    code = Code(func.__code__)\n    assert code.getargs() == ('a', 'b', 'c')\n    assert code.getargs(var=True) == ('a', 'b', 'c', 'args', 'kwargs')\n"], "sample_393": ["def test_python_files(self):\n    self.write_files({\n        'foo.py': '# coding: utf-8\\nfrom django.utils.translation import gettext_lazy as _\\nmsg = _(\"Message\")',\n    })\n    self._run_makemessages()\n    self.assertMsgId('Message')\n", "def test_msgid_plural(self):\n    self.write_file(\"path/to/file.html\", \"<p>{% blocktrans count counter=number %}Yesterday, {{ counter }} person came here.{% plural %}Yesterday, {{ counter }} people came here.{% endblocktrans %}</p>\")\n    self._run_makemessages()\n    self.assertMsgIdPlural(\"Yesterday, %(counter)s person came here.\", self.PO_FILE)\n    self.assertMsgStr(\"Gestern kamen %(counter)s Personen hier.\", self.PO_FILE)\n", "def test_build_potfiles_with_no_obsolete_option(self):\n    self.tmp_path.joinpath('locale').mkdir()\n    self.tmp_path.joinpath('templates').mkdir()\n    self.tmp_path.joinpath('templates', 'test.html').write_text('<p>{% trans \"Hello\" %}</p>')\n\n    with mock.patch.object(MakeMessagesCommand, 'no_obsolete', True):\n        command = MakeMessagesCommand()\n        command.build_potfiles()\n        potfile = self.tmp_path.joinpath('locale', 'django.pot')\n        self.assertTrue(potfile.exists())\n        self.assertNotIn('#, fuzzy', potfile.read_text())\n", "    def test_plural_forms_header_copied(self):\n        # Create a Django catalog with custom plural forms\n        django_po = os.path.join(\n            os.path.dirname(os.path.dirname(django.__file__)),\n            \"conf\", \"locale\", LOCALE, \"LC_MESSAGES\", \"django.po\",\n        )\n        with open(django_po, \"w\", encoding=\"utf-8\") as fp:\n            fp.write('msgid \"\"\\nmsgstr \"\"\\n\"Plural-Forms: nplurals=2; plural=(n != 1);\\\\n\"\\n')\n\n        # Create a simple Python file\n        with open(\"test.py\", \"w\", encoding=\"utf-8\") as fp:\n            fp.write('from django.utils.translation import gettext_lazy as _\\n_(\"singular\")\\n_(\"plural\", 2)\\n')\n\n        # Run makemessages\n        self._run_makemessages()\n\n        # Check that plural forms header was copied\n        with open(self.PO_FILE) as fp:\n            po_contents = fp.read()\n        self.assertIn('\"Plural-Forms: nplurals=2; plural=(n != 1);\\\\n\"', po_contents)\n", "    def test_django_project_with_js_files(self):\n        self.setup_django_project()\n        os.makedirs(os.path.join(\"locale\", LOCALE, \"LC_MESSAGES\"))\n        Path(\"djangojs.po\").write_text(\"\")\n        Path(\"project.js\").write_text('gettext_noop(\"test message\");')\n\n        output, _ = self._run_makemessages(domain=\"djangojs\")\n\n        self.assertIn(\"processing locale %s\" % LOCALE, output)\n        self.assertIn(\"processing file project.js\", output)\n        self.assertMsgId('gettext_noop(\"test message\")', self.PO_FILE)\n"], "sample_477": ["def test_random_with_empty_list(self):\n    output = self.engine.render_to_string(\"random02\", {\"c\": []})\n    self.assertEqual(output, \"\")\n", "    def test_random_filter_with_different_input(self):\n        a = [1, 2, 3]\n        c = ['x', 'y', 'z']\n        output = self.engine.render_to_string('random02', {'a': a, 'c': c})\n        self.assertIn(str(self.engine.render_to_string('random02', {'a': a, 'c': c})),\n                      [\"1 x\", \"1 y\", \"1 z\", \"2 x\", \"2 y\", \"2 z\", \"3 x\", \"3 y\", \"3 z\"])\n", "def test_random_empty_list(self):\n    output = self.engine.render_to_string(\"random02\", {\"empty\": []})\n    self.assertEqual(output, \"\")\n", "def test_random_list_element(self):\n    output = self.engine.render_to_string(\"random02\")\n    self.assertIn(output.strip(), [\"apple\", \"banana\", \"cherry\"])\n", "def test_random_filter_with_list(self):\n    result = self.engine.render_to_string(\n        self.template,\n        {\"a\": \"abc\", \"b\": \"def\"},\n    )\n    self.assertIn(self.engine.render_to_string(\"{{ [1, 2, 3]|random }}\"), \"123\")\n"], "sample_1139": ["def test_complex_region_contains():\n    region = ComplexRegion(Interval(1, 3) * Interval(4, 6))\n    assert 2 + 5*I in region\n    assert 5*I not in region\n", "def test_complex_region_from_real():\n    unit = Interval(0, 1)\n    region = ComplexRegion.from_real(unit)\n    assert region == CartesianComplexRegion(ProductSet(unit, FiniteSet(0)))\n", "def test_complex_region_from_real():\n    unit = Interval(0,1)\n    complex_region = ComplexRegion.from_real(unit)\n    assert complex_region == CartesianComplexRegion(ProductSet(unit, FiniteSet(0)))\n", "def test_normalize_theta_set():\n    theta = Interval(3*pi/2, -pi/2)\n    normalized_theta = normalize_theta_set(theta)\n    expected_theta = Union(Interval(pi/2, 3*pi/2), Interval.Ropen(0, pi/2))\n    assert normalized_theta == expected_theta\n", "def test_complex_region_contains():\n    region = ComplexRegion(Interval(1, 3) * Interval(4, 6))\n    assert 2 + 5*I in region\n    assert 5*I not in region\n"], "sample_520": ["def test_line3d_collection_do_3d_projection(fig):\n    ax = fig.add_subplot(111, projection='3d')\n    segments = [[(0, 0, 0), (1, 1, 1)], [(2, 2, 2), (3, 3, 3)]]\n    collection = art3d.Line3DCollection(segments)\n    ax.add_collection(collection)\n    minz = collection.do_3d_projection()\n    assert isinstance(minz, float)\n    assert minz == 0\n", "def test_line3d_collection_set_segments():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    segments = [[(1, 1, 1), (2, 2, 2)], [(3, 3, 3), (4, 4, 4)]]\n    collection = art3d.Line3DCollection(segments)\n    ax.add_collection3d(collection)\n\n    new_segments = [[(5, 5, 5), (6, 6, 6)], [(7, 7, 7), (8, 8, 8)]]\n    collection.set_segments(new_segments)\n\n    assert collection._segments3d == new_segments\n", "def test_patch_collection_2d_to_3d(fig):\n    ax = fig.add_subplot(111, projection='3d')\n    patches = [Circle((0, 0), 0.5), Circle((1, 1), 0.5)]\n    collection = mpl.collections.PatchCollection(patches)\n    art3d.patch_collection_2d_to_3d(collection, zs=[0, 1], zdir='z', depthshade=True)\n    ax.add_collection(collection)\n", "def test_text3d_draw_method():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    text3d = art3d.Text3D(0, 0, 0, text='Test', zdir='z')\n    ax.add_artist(text3d)\n    text3d.draw(fig.canvas.get_renderer())\n", "def test_line3d_collection_draw():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n\n    x = np.linspace(0, 1, 100)\n    y = np.sin(x * 2 * np.pi) / 2 + 0.5\n    z = np.linspace(0, 1, 100)\n\n    segments = np.array([np.column_stack([x, y, z])])\n\n    lc = art3d.Line3DCollection(segments)\n    ax.add_collection3d(lc)\n\n    fig.canvas.draw()  # This will trigger the draw method of Line3DCollection\n"], "sample_105": ["    def test_redirect_view_get(self):\n        request = self.rf.get('/some-url/')\n        response = RedirectView.as_view(url='/redirected-url/')(request)\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response.url, '/redirected-url/')\n", "    def test_get_redirect_url(self):\n        request = self.rf.get('/')\n        view = RedirectView(url='/target/')\n        view.setup(request)\n        self.assertEqual(view.get_redirect_url(), '/target/')\n", "    def test_get_redirect_url(self):\n        view = RedirectView.as_view(url='/about/', permanent=True)\n        request = self.rf.get('/')\n        response = view(request)\n        self.assertEqual(response.status_code, 301)\n        self.assertEqual(response.url, '/about/')\n", "    def test_redirect_view(self):\n        request = self.rf.get('/redirect')\n        response = RedirectView.as_view(url='/new-url')(request)\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response.url, '/new-url')\n", "    def test_redirect_view_permanent(self):\n        request = self.rf.get('/redirect-permanent')\n        response = RedirectView.as_view(url='/target', permanent=True)(request)\n        self.assertEqual(response.status_code, 301)\n        self.assertEqual(response.url, '/target')\n"], "sample_988": ["compilation error", "def test_number_symbol_comparisons():\n    assert S.Exp1 != S.Pi\n    assert S.Pi != S.GoldenRatio\n    assert S.GoldenRatio != S.EulerGamma\n    assert S.EulerGamma != S.Catalan\n    assert S.Catalan != S.ImaginaryUnit\n    assert S.ImaginaryUnit != S.Exp1\n", "compilation error", "def test_number_symbol_comparisons():\n    assert S.Pi > 3\n    assert S.Pi != 3\n    assert S.EulerGamma != 0\n    assert S.EulerGamma < 1\n", "def test_number_subtraction():\n    assert S.One - S.One == S.Zero\n    assert S.One - S.Zero == S.One\n    assert S.Zero - S.One == -S.One\n    assert S.Zero - S.Zero == S.Zero\n    assert oo - oo is nan\n    assert oo - S.One == oo\n    assert S.One - oo == -oo\n    assert oo - S.Zero == oo\n    assert S.Zero - oo == -oo\n    assert zoo - zoo is nan\n    assert zoo - S.One == zoo\n    assert S.One - zoo == -zoo\n    assert zoo - S.Zero == zoo\n    assert S.Zero - zoo == -zoo\n    assert nan - nan is nan\n    assert nan - S.One is nan\n    assert S.One - nan is nan\n    assert nan - S.Zero is nan\n    assert S.Zero - nan is nan\n"], "sample_1008": ["def test_variable_map():\n    A = ReferenceFrame('A')\n    q = dynamicsymbols('q')\n    B = A.orientnew('B', 'Axis', [q, A.z])\n    mapping = A.variable_map(B)\n    assert mapping[A.x] == B.x*cos(q) - B.y*sin(q)\n    assert mapping[A.y] == B.x*sin(q) + B.y*cos(q)\n    assert mapping[A.z] == B.z\n", "def test_variable_map():\n    A = ReferenceFrame('A')\n    q = dynamicsymbols('q')\n    B = A.orientnew('B', 'Axis', [q, A.z])\n    mapping = A.variable_map(B)\n    assert mapping[A.x] == B.x * cos(q) - B.y * sin(q)\n    assert mapping[A.y] == B.x * sin(q) + B.y * cos(q)\n    assert mapping[A.z] == B.z\n", "def test_variable_map():\n    N = ReferenceFrame('N')\n    q = dynamicsymbols('q')\n    A = N.orientnew('A', 'Axis', [q, N.z])\n    mapping = A.variable_map(N)\n    assert mapping[A[0]] == N[0]*cos(q) - N[1]*sin(q)\n    assert mapping[A[1]] == N[0]*sin(q) + N[1]*cos(q)\n    assert mapping[A[2]] == N[2]\n", "def test_orientnew_variables():\n    N = ReferenceFrame('N', variables=('n_x', 'n_y', 'n_z'))\n    A = N.orientnew('A', 'Axis', [pi/2, N.x], variables=('a_x', 'a_y', 'a_z'))\n    assert A.varlist == (CoordinateSym('a_x', A, 0), CoordinateSym('a_y', A, 1), CoordinateSym('a_z', A, 2))\n", "def test_reference_frame_dcm_cache():\n    N = ReferenceFrame('N')\n    A = ReferenceFrame('A')\n    B = ReferenceFrame('B')\n\n    # Define orientation of A with respect to N\n    q1 = dynamicsymbols('q1')\n    A.orient(N, 'Axis', [q1, N.x])\n\n    # Calculate DCM from A to N\n    dcm_AN = A.dcm(N)\n\n    # Check if DCM from A to N is in A's DCM cache\n    assert N in A._dcm_cache\n\n    # Calculate DCM from N to A\n    dcm_NA = N.dcm(A)\n\n    # Check if DCM from N to A is in N's DCM cache\n    assert A in N._dcm_cache\n\n    # Check if DCM from A to N is the transpose of DCM from N to A\n    assert dcm_AN.T == dcm_NA\n"], "sample_937": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_225": ["    def test_add_action(self):\n            pass\n\n        site.add_action(custom_action, name=\"custom_action\")\n        self.assertIn(\"custom_action\", site._actions)\n        self.assertIn(\"custom_action\", site._global_actions)\n        self.assertIs(site.get_action(\"custom_action\"), custom_action)\n", "    def setUpTestData(cls):\n        cls.u1 = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n        cls.u2 = User.objects.create_user(username='user', password='secret', email='user@example.com')\n", "    def setUpTestData(cls):\n        cls.u1 = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n", "    def test_add_and_disable_action(self):\n            pass\n\n        site.add_action(test_action, 'test_action')\n        self.assertIn('test_action', dict(site.actions))\n\n        site.disable_action('test_action')\n        self.assertNotIn('test_action', dict(site.actions))\n\n        with self.assertRaises(KeyError):\n            site.disable_action('nonexistent_action')\n", "    def test_add_action(self):\n            pass\n        site.add_action(custom_action, 'custom_action')\n        self.assertIn('custom_action', dict(site.actions).keys())\n"], "sample_896": ["def test_zero_input(Estimator, solver):\n    X = np.zeros((10, 5))\n    nmf_model = Estimator(n_components=3, **solver)\n    with pytest.raises(ValueError, match=\"NMF (input X)\"):\n        nmf_model.fit_transform(X)\n", "def test_nmf_convergence_warning(Estimator, solver):\n    # Test that the ConvergenceWarning is raised when max_iter is reached\n    X = np.array([[1, 1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])\n    estimator = Estimator(n_components=2, init='random', random_state=0, max_iter=2, **solver)\n\n    with pytest.warns(ConvergenceWarning, match=\"Maximum number of iterations 2 reached\"):\n        estimator.fit(X)\n", "def test_dense_input_cd_solver(Estimator, solver):\n    rng = np.random.RandomState(0)\n    X = rng.rand(10, 20)\n\n    estimator = Estimator(n_components=5, random_state=0, **solver)\n    W = estimator.fit_transform(X)\n    H = estimator.components_\n\n    assert W.shape == (10, 5)\n    assert H.shape == (5, 20)\n    assert np.allclose(X, np.dot(W, H))\n", "def test_fit_transform_consistency(Estimator, solver):\n    \"\"\"Check that fit_transform is equivalent to fit followed by transform.\"\"\"\n    rng = np.random.RandomState(0)\n    X = np.abs(rng.randn(10, 10))\n\n    est1 = Estimator(n_components=3, random_state=0, **solver)\n    W1 = est1.fit_transform(X)\n    H1 = est1.components_\n\n    est2 = Estimator(n_components=3, random_state=0, **solver)\n    W2 = est2.fit(X).transform(X)\n    H2 = est2.components_\n\n    assert_array_almost_equal(W1, W2)\n    assert_array_almost_equal(H1, H2)\n", "def test_nmf_zero_elements_beta_loss(Estimator, solver):\n    # Test case where input data has zero elements and beta_loss is less than or equal to 0\n    # Should raise a ValueError indicating that beta_loss <= 0 and X contains zeros, the solver may diverge\n\n    X = np.array([[1, 0], [0, 1]])\n    estimator = Estimator(n_components=2, beta_loss=\"itakura-saito\", **solver)\n\n    with pytest.raises(ValueError, match=\"When beta_loss <= 0 and X contains zeros, the solver may diverge\"):\n        estimator.fit(X)\n"], "sample_328": ["def test_bulk_update_with_invalid_fields(self):\n    fields = ['invalid_field']\n    with self.assertRaises(FieldDoesNotExist):\n        Note.objects.bulk_update(self.notes, fields)\n", "def test_bulk_update_with_f_expression(self):\n    self.create_tags()\n    notes = Note.objects.all()\n    notes.update(tag=F('id'))\n    for note in notes:\n        note.refresh_from_db()\n        self.assertEqual(note.tag_id, note.id)\n", "def test_bulk_update_with_expression(self):\n    self.create_tags()\n    notes = Note.objects.all()\n    tag_ids = [tag.id for tag in self.tags]\n\n    # Bulk update with an expression\n    updated = notes.bulk_update(notes, {'tag_id': F('id') + 1})\n    self.assertEqual(updated, len(notes))\n\n    # Verify the update\n    for note in notes:\n        self.assertEqual(note.tag_id, note.id + 1)\n", "def test_bulk_update_with_f_expressions(self):\n    Note.objects.bulk_update(self.notes, ['note'], batch_size=5)\n    for note in self.notes:\n        note.refresh_from_db()\n        self.assertEqual(note.note, note.misc)\n\n    # Update using F expressions\n    Note.objects.bulk_update(self.notes, [F('note')], batch_size=3)\n    for note in self.notes:\n        note.refresh_from_db()\n        self.assertEqual(note.note, note.misc)\n", "def test_bulk_update_with_expression(self):\n    self.create_tags()\n    notes = Note.objects.all()\n    notes.update(misc=F('note') + ' updated')\n    notes = notes.order_by('pk')\n    for i, note in enumerate(notes):\n        self.assertEqual(note.misc, str(i) + ' updated')\n"], "sample_101": ["    def test_wsgi_request(self):\n        environ = {\n            'REQUEST_METHOD': 'GET',\n            'PATH_INFO': '/test/',\n            'SERVER_NAME': 'localhost',\n            'SERVER_PORT': '80',\n            'wsgi.input': b'',\n            'wsgi.version': (1, 0),\n            'wsgi.url_scheme': 'http',\n            'wsgi.multithread': True,\n            'wsgi.multiprocess': False,\n            'wsgi.run_once': False,\n        }\n        request = WSGIRequest(environ)\n        self.assertEqual(request.method, 'GET')\n        self.assertEqual(request.path_info, '/test/')\n        self.assertEqual(request.path, '/test/')\n        self.assertIsInstance(request.GET, QueryDict)\n        self.assertIsInstance(request.COOKIES, dict)\n", "    def test_limited_stream(self):\n        stream = LimitedStream(BytesIO(b'1234567890'), 5)\n        self.assertEqual(stream.read(3), b'123')\n        self.assertEqual(stream.read(3), b'45')\n        self.assertEqual(stream.read(3), b'')\n\n        stream = LimitedStream(BytesIO(b'1234567890'), 5)\n        self.assertEqual(stream.readline(3), b'123')\n        self.assertEqual(stream.readline(3), b'4')\n        self.assertEqual(stream.readline(3), b'567890')\n        self.assertEqual(stream.readline(3), b'')\n", "    def test_limited_stream(self):\n        class FakeStream:\n                self.data = data\n\n                if size is None:\n                    result = self.data\n                    self.data = b''\n                else:\n                    result = self.data[:size]\n                    self.data = self.data[size:]\n                return result\n\n        data = b'abcdefghijklmnopqrstuvwxyz'\n        stream = LimitedStream(FakeStream(data), limit=10)\n\n        # Read less than limit\n        self.assertEqual(stream.read(5), b'abcde')\n\n        # Read exactly limit\n        self.assertEqual(stream.read(5), b'fghij')\n\n        # Read more than limit\n        self.assertEqual(stream.read(5), b'')\n\n        # Readline without size\n        stream = LimitedStream(FakeStream(data), limit=10)\n        self.assertEqual(stream.readline(), b'abcdefghij')\n\n        # Readline with size\n        stream = LimitedStream(FakeStream(data), limit=10)\n        self.assertEqual(stream.readline(5), b'abcde')\n", "    def test_limited_stream_readline(self):\n        data = b'line1\\nline2\\nline3'\n        limited_stream = LimitedStream(BytesIO(data), len(data))\n\n        self.assertEqual(limited_stream.readline(), b'line1\\n')\n        self.assertEqual(limited_stream.readline(5), b'line2\\n')\n        self.assertEqual(limited_stream.readline(), b'line3')\n        self.assertEqual(limited_stream.readline(), b'')\n", "    def test_get_response(self):\n        environ = {\n            'REQUEST_METHOD': 'GET',\n            'PATH_INFO': '/test/',\n            'wsgi.input': b'',\n            'wsgi.url_scheme': 'http',\n        }\n        handler = WSGIHandler()\n        request = handler.request_class(environ)\n        response = handler.get_response(request)\n        self.assertEqual(response.status_code, 200)\n"], "sample_982": ["def test_udivisor_sigma():\n    assert udivisor_sigma(18, 0) == 4\n    assert udivisor_sigma(74, 1) == 114\n    assert udivisor_sigma(36, 3) == 47450\n    assert udivisor_sigma(111) == 152\n", "def test_reduced_totient():\n    assert reduced_totient(1) == 1\n    assert reduced_totient(2) == 1\n    assert reduced_totient(3) == 2\n    assert reduced_totient(4) == 2\n    assert reduced_totient(5) == 4\n    assert reduced_totient(6) == 2\n    assert reduced_totient(7) == 6\n    assert reduced_totient(8) == 2\n    assert reduced_totient(9) == 6\n    assert reduced_totient(10) == 4\n", "def test_smoothness_p():\n    assert smoothness_p(10431, m=1) == (1, [(3, (2, 2, 4)), (19, (1, 5, 5)), (61, (1, 31, 31))])\n    assert smoothness_p(10431) == (-1, [(3, (2, 2, 2)), (19, (1, 3, 9)), (61, (1, 5, 5))])\n    assert smoothness_p(10431, power=1) == (-1, [(3, (2, 2, 2)), (61, (1, 5, 5)), (19, (1, 3, 9))])\n    assert smoothness_p('p**i=3**2 has p-1 B=2, B-pow=2\\np**i=17**1 has p-1 B=2, B-pow=16') == {3: 2, 17: 1}\n    assert smoothness_p({3: 2, 17: 1}) == 'p**i=3**2 has p-1 B=2, B-pow=2\\np**i=17**1 has p-1 B=2, B-pow=16'\n", "def test_smoothness():\n    assert smoothness(2**7*3**2) == (3, 128)\n    assert smoothness(2**4*13) == (13, 16)\n    assert smoothness(2) == (2, 2)\n    assert smoothness(1) == (1, 1)\n", "def test_core_function():\n    assert core(24, 2) == 6\n    assert core(9424, 3) == 1178\n    assert core(379238) == 379238\n    assert core(15**11, 10) == 15\n"], "sample_621": ["def test_asarray_tuplesafe():\n    # Test with a tuple\n    result = _asarray_tuplesafe((1, 2, 3))\n    assert np.array_equal(result, np.array([(1, 2, 3)], dtype=object))\n\n    # Test with a list\n    result = _asarray_tuplesafe([1, 2, 3])\n    assert np.array_equal(result, np.array([1, 2, 3]))\n\n    # Test with a numpy array\n    result = _asarray_tuplesafe(np.array([1, 2, 3]))\n    assert np.array_equal(result, np.array([1, 2, 3]))\n\n    # Test with a 2D numpy array\n    result = _asarray_tuplesafe(np.array([[1, 2], [3, 4]]))\n    assert np.array_equal(result, np.array([[1, 2], [3, 4]], dtype=object))\n", "def test_asarray_tuplesafe():\n    # Test with a tuple\n    assert np.array_equal(_asarray_tuplesafe((1, 2, 3)), np.array([(1, 2, 3)]))\n\n    # Test with a 2-dimensional array\n    assert np.array_equal(_asarray_tuplesafe([[1, 2], [3, 4]]), np.array([[1, 2], [3, 4]]))\n\n    # Test with a scalar\n    assert np.array_equal(_asarray_tuplesafe(5), np.array(5))\n", "def test_asarray_tuplesafe():\n    # Test with a single tuple\n    single_tuple = (1, 2)\n    result = _asarray_tuplesafe(single_tuple)\n    assert isinstance(result, np.ndarray)\n    assert result.dtype.kind == 'O'\n    assert np.array_equal(result, np.array(single_tuple, dtype=object))\n\n    # Test with a list of tuples\n    list_of_tuples = [(1, 2), (3, 4)]\n    result = _asarray_tuplesafe(list_of_tuples)\n    assert isinstance(result, np.ndarray)\n    assert result.dtype.kind == 'O'\n    assert np.array_equal(result, np.array(list_of_tuples, dtype=object))\n", "def test_normalize_label():\n    values = [1, 2, 3]\n    dtype = np.int32\n    expected = np.array(values, dtype=dtype)\n    assert np.array_equal(_asarray_tuplesafe(values), expected)\n\n    values = [(1, 2), (3, 4)]\n    expected = np.array(values, dtype=object)\n    assert np.array_equal(_asarray_tuplesafe(values), expected)\n\n    values = np.array([1, 2, 3], dtype=np.int64)\n    expected = np.array([1, 2, 3], dtype=np.int32)\n    assert np.array_equal(_asarray_tuplesafe(values), expected)\n", "def test_normalize_label():\n    # Test with a scalar value\n    value = 5\n    expected_output = np.array(5)\n    assert np.array_equal(_asarray_tuplesafe(value), expected_output)\n\n    # Test with a list of values\n    value = [1, 2, 3]\n    expected_output = np.array([1, 2, 3])\n    assert np.array_equal(_asarray_tuplesafe(value), expected_output)\n\n    # Test with a tuple of values\n    value = (4, 5, 6)\n    expected_output = np.array(object, dtype=object)\n    expected_output[:] = value\n    assert np.array_equal(_asarray_tuplesafe(value), expected_output)\n\n    # Test with a numpy array\n    value = np.array([7, 8, 9])\n    expected_output = np.array([7, 8, 9])\n    assert np.array_equal(_asarray_tuplesafe(value), expected_output)\n\n    # Test with a 2-dimensional numpy array\n    value = np.array([[1, 2], [3, 4]])\n    expected_output = np.empty(2, dtype=object)\n    expected_output[:] = [np.array([1, 2]), np.array([3, 4])]\n    assert np.array_equal(_asarray_tuplesafe(value), expected_output)\n"], "sample_476": ["def test_file_assignment(self):\n    \"\"\"\n    Test assigning a File object to an ImageField instance.\n    \"\"\"\n    instance = self.PersonModel()\n    instance.mugshot = self.file1\n    instance.save()\n    self.assertEqual(instance.mugshot.read(), self.file1.read())\n    self.check_dimensions(instance, 4, 8)\n\n    instance.mugshot = self.file2\n    instance.save()\n    self.assertEqual(instance.mugshot.read(), self.file2.read())\n    self.check_dimensions(instance, 8, 4)\n", "    def test_delete_method(self):\n        \"\"\"\n        Test the delete method of ImageFieldFile.\n        \"\"\"\n        instance = self.PersonModel.objects.create()\n        instance.mugshot.save(\"test_delete.png\", self.file1, save=True)\n\n        # Check the file exists\n        self.assertTrue(os.path.exists(os.path.join(temp_storage_dir, \"test_delete.png\")))\n\n        instance.mugshot.delete(save=True)\n\n        # Check the file is deleted\n        self.assertFalse(os.path.exists(os.path.join(temp_storage_dir, \"test_delete.png\")))\n\n        # Check the dimensions are cleared\n        self.check_dimensions(instance, None, None)\n", "def test_save_form_data(self):\n    \"\"\"\n    Test the save_form_data method of the ImageField.\n    \"\"\"\n    instance = self.PersonModel()\n    data = self.File(open(os.path.join(os.path.dirname(__file__), \"4x8.png\"), \"rb\"), name=\"4x8.png\")\n    instance.mugshot.save_form_data(instance, data)\n    self.assertEqual(instance.mugshot.name, \"4x8.png\")\n    data = None\n    instance.mugshot.save_form_data(instance, data)\n    self.assertEqual(instance.mugshot.name, \"\")\n", "    def test_update_dimension_fields(self):\n        person = self.PersonModel.objects.create(mugshot=self.file1)\n        self.check_dimensions(person, 4, 8)\n\n        # Change the image\n        person.mugshot = self.file2\n        person.save()\n        self.check_dimensions(person, 8, 4)\n\n        # Clear the image\n        person.mugshot.delete(save=True)\n        self.check_dimensions(person, None, None)\n", "def test_dimensions_on_assignment(self):\n    \"\"\"\n    Tests that the dimensions are updated when an image is assigned to\n    an ImageField.\n    \"\"\"\n    instance = self.PersonModel.objects.create()\n    instance.mugshot = self.file1\n    instance.save()\n    self.check_dimensions(instance, 4, 8)\n\n    instance.mugshot = self.file2\n    instance.save()\n    self.check_dimensions(instance, 8, 4)\n"], "sample_215": ["    def test_get_traceback_data(self):\n        request = RequestFactory().get('/')\n        exc_type = Exception\n        exc_value = Exception(\"Test exception\")\n        tb = exc_value.__traceback__\n        reporter = ExceptionReporter(request, exc_type, exc_value, tb)\n        data = reporter.get_traceback_data()\n        self.assertIsInstance(data, dict)\n        self.assertEqual(data['exception_type'], 'Exception')\n        self.assertEqual(data['exception_value'], 'Test exception')\n", "    def test_get_traceback_frames(self):\n        try:\n            raise ValueError(\"Test error\")\n        except ValueError:\n            exc_type, exc_value, tb = sys.exc_info()\n\n        request = RequestFactory().get('/')\n        reporter = ExceptionReporter(request, exc_type, exc_value, tb)\n        frames = reporter.get_traceback_frames()\n\n        self.assertIsInstance(frames, list)\n        for frame in frames:\n            self.assertIn('tb', frame)\n            self.assertIn('type', frame)\n            self.assertIn('filename', frame)\n            self.assertIn('function', frame)\n            self.assertIn('lineno', frame)\n            self.assertIn('vars', frame)\n            self.assertIn('id', frame)\n            self.assertIn('pre_context', frame)\n            self.assertIn('context_line', frame)\n            self.assertIn('post_context', frame)\n            self.assertIn('pre_context_lineno', frame)\n", "    def setUp(self):\n        self.rf = RequestFactory()\n", "    def setUp(self):\n        self.factory = RequestFactory()\n        self.request = self.factory.get('/')\n        self.request.user = User()\n        self.exc_type, self.exc_value, self.tb = sys.exc_info()\n", "    def test_get_safe_settings(self):\n        # Test that sensitive settings are replaced with stars\n        with self.settings(API_KEY='sensitive_key'):\n            filter = SafeExceptionReporterFilter()\n            settings_dict = filter.get_safe_settings()\n            self.assertEqual(settings_dict['API_KEY'], filter.cleansed_substitute)\n"], "sample_708": ["def test_getstatementrange_ast():\n    source = Source([\"def func():\", \"    if True:\", \"        pass\", \"    return 42\"])\n    _, start, end = getstatementrange_ast(2, source)\n    assert start == 1\n    assert end == 4\n", "def test_source_getitem_slice():\n    source = Source([\"line1\", \"line2\", \"line3\"])\n    assert source[1:3] == Source([\"line2\", \"line3\"])\n    with pytest.raises(IndexError):\n        source[::2]\n", "def test_getstatementrange():\n    source_code = \"\"\"\n        result = a + b\n        if result > 10:\n            result *= 2\n        return result\n    \"\"\"\n    source = Source(source_code)\n    start, end = source.getstatementrange(3)\n    assert start == 2\n    assert end == 5\n", "def test_getstatementrange():\n    source = Source((\"def foo():\", \"    if True:\", \"        pass\", \"    return 42\"))\n    start, end = source.getstatementrange(1)\n    assert start == 0\n    assert end == 1\n    start, end = source.getstatementrange(2)\n    assert start == 1\n    assert end == 3\n    start, end = source.getstatementrange(3)\n    assert start == 2\n    assert end == 3\n", "def test_source_equality():\n    source1 = Source([\"def func():\", \"    pass\"])\n    source2 = Source(\"def func():\\n    pass\")\n    source3 = Source(func)\n    source4 = Source([\"def func2():\", \"    pass\"])\n\n    assert source1 == source2\n    assert source1 != source4\n    assert source1 != source3  # assuming `func` is not a simple function with just a pass statement\n"], "sample_134": ["    def test_uuid_serializer(self):\n        value = uuid.uuid4()\n        serializer = UUIDSerializer(value)\n        expected_string = \"uuid.%s\" % repr(value)\n        expected_imports = {\"import uuid\"}\n        self.assertEqual(serializer.serialize(), (expected_string, expected_imports))\n", "    def test_settings_reference_serializer(self):\n        value = SettingsReference('TIME_ZONE')\n        serializer = serializer_factory(value)\n        self.assertEqual(serializer.serialize(), (\"settings.TIME_ZONE\", {\"from django.conf import settings\"}))\n", "    def test_serialize_text_enum(self):\n        value = TextEnum.A\n        serializer = serializer_factory(value)\n        result, imports = serializer.serialize()\n        self.assertEqual(result, 'TestSerializers.TextEnum[A]')\n        self.assertEqual(imports, {'import TestSerializers'})\n", "    def test_uuid_serializer(self):\n        value = uuid.uuid4()\n        serializer = serializer_factory(value)\n        serialized_value, imports = serializer.serialize()\n        expected_value = f\"uuid.{repr(value)}\"\n        expected_imports = {\"import uuid\"}\n        self.assertEqual(serialized_value, expected_value)\n        self.assertEqual(imports, expected_imports)\n", "    def test_enum_serializer(self):\n        for enum_cls in [TextEnum, TextTranslatedEnum, BinaryEnum, IntEnum]:\n            serializer = serializer_factory(enum_cls.A)\n            serialized, imports = serializer.serialize()\n            expected_serialized = f'{enum_cls.__module__}.{enum_cls.__qualname__}[{enum_cls.A.name!r}]'\n            expected_imports = {f'import {enum_cls.__module__}'}\n            self.assertEqual(serialized, expected_serialized)\n            self.assertEqual(imports, expected_imports)\n"], "sample_249": ["    def test_create_test_db(self, mock_call_command):\n        test_connection = get_connection_copy()\n        db_creation = BaseDatabaseCreation(test_connection)\n\n        old_name = test_connection.settings_dict['NAME']\n        test_database_name = db_creation.create_test_db()\n\n        self.assertNotEqual(test_database_name, old_name)\n        self.assertTrue(test_database_name.startswith(TEST_DATABASE_PREFIX))\n        mock_call_command.assert_called_with(\n            'migrate',\n            verbosity=0,\n            interactive=False,\n            database=DEFAULT_DB_ALIAS,\n            run_syncdb=True,\n        )\n        mock_call_command.assert_called_with('createcachetable', database=DEFAULT_DB_ALIAS)\n", "    def test_test_db_signature(self):\n        test_connection = get_connection_copy()\n        test_database_creation = BaseDatabaseCreation(test_connection)\n\n        test_connection.settings_dict['NAME'] = 'test_db_name'\n        test_connection.settings_dict['HOST'] = 'localhost'\n        test_connection.settings_dict['PORT'] = 1234\n        test_connection.settings_dict['ENGINE'] = 'django.db.backends.postgresql'\n\n        expected_signature = (\n            'localhost',\n            1234,\n            'django.db.backends.postgresql',\n            TEST_DATABASE_PREFIX + 'test_db_name',\n        )\n\n        self.assertEqual(test_database_creation.test_db_signature(), expected_signature)\n", "def test_create_test_db_with_error(self, input_mock):\n    test_connection = get_connection_copy()\n    test_database_name = test_connection.settings_dict['NAME']\n    test_database_params = {\n        'dbname': test_connection.ops.quote_name(TEST_DATABASE_PREFIX + test_database_name),\n        'suffix': '',\n    }\n    with mock.patch('django.db.backends.base.creation.BaseDatabaseCreation._nodb_cursor') as cursor_mock:\n        cursor_mock.return_value.__enter__.return_value.execute.side_effect = [\n            Exception('Test exception'),\n            None,\n            None,\n        ]\n        creation = BaseDatabaseCreation(test_connection)\n        with self.assertRaises(SystemExit):\n            creation._create_test_db(verbosity=2, autoclobber=False)\n        cursor_mock.return_value.__enter__.return_value.execute.assert_has_calls([\n            mock.call('CREATE DATABASE %(dbname)s %(suffix)s' % test_database_params),\n            mock.call('DROP DATABASE %(dbname)s' % test_database_params),\n            mock.call('CREATE DATABASE %(dbname)s %(suffix)s' % test_database_params),\n        ])\n", "    def setUp(self):\n        self.test_connection = get_connection_copy()\n        self.test_connection.settings_dict['TEST'] = {\n            'NAME': None,\n            'MIGRATE': True,\n        }\n        self.creator = BaseDatabaseCreation(self.test_connection)\n", "    def test_test_db_signature(self):\n        test_connection = get_connection_copy()\n        db_creation = BaseDatabaseCreation(test_connection)\n\n        # Test for the default database settings\n        expected_signature = (\n            '',  # default host\n            '',  # default port\n            test_connection.settings_dict['ENGINE'],\n            TEST_DATABASE_PREFIX + test_connection.settings_dict['NAME'],\n        )\n        self.assertEqual(db_creation.test_db_signature(), expected_signature)\n\n        # Test for non-default database settings\n        test_connection.settings_dict['HOST'] = 'example.com'\n        test_connection.settings_dict['PORT'] = '1234'\n        expected_signature = (\n            'example.com',\n            '1234',\n            test_connection.settings_dict['ENGINE'],\n            TEST_DATABASE_PREFIX + test_connection.settings_dict['NAME'],\n        )\n        self.assertEqual(db_creation.test_db_signature(), expected_signature)\n"], "sample_1192": ["def test_disambiguate():\n    x, y = symbols('x y')\n    ix = Symbol('x', integer=True)\n    vx = Symbol('x')\n    assert disambiguate(vx + ix) == (Symbol('x') + Symbol('x_1'),)\n", "def test_disambiguate_with_different_assumptions():\n    ix = Symbol('x', integer=True)\n    vx = Symbol('x')\n    assert disambiguate(vx + ix) == (Symbol('x') + Symbol('x_1'),)\n", "def test_symbol_as_dummy():\n    x = Symbol('x')\n    x_dummy = x.as_dummy()\n    assert isinstance(x_dummy, Dummy)\n    assert x_dummy.name == 'x'\n    assert x_dummy.is_commutative is None\n\n    y = Symbol('y', commutative=False)\n    y_dummy = y.as_dummy()\n    assert isinstance(y_dummy, Dummy)\n    assert y_dummy.name == 'y'\n    assert y_dummy.is_commutative is False\n", "def test_uniquely_named_symbol():\n    x = Symbol('x')\n    y = Symbol('y')\n    z = uniquely_named_symbol('x', (x, y))\n    assert z.name == 'x1'\n    assert z.is_real is None\n\n    z = uniquely_named_symbol('x', (x, y), real=True)\n    assert z.name == 'x1'\n    assert z.is_real is True\n\n    z = uniquely_named_symbol('x1', (x, y))\n    assert z.name == 'x2'\n    assert z.is_real is None\n", "def test_disambiguate_with_different_assumptions():\n    ix = Symbol('x', integer=True)\n    vx = Symbol('x')\n    assert disambiguate(vx + ix) == (Symbol('x') + Symbol('x_1'),)\n"], "sample_1112": ["def test_is_palindromic():\n    assert is_palindromic(12321) == True\n    assert is_palindromic(12345) == False\n    assert is_palindromic(123321, 10) == True\n    assert is_palindromic(123321, 8) == False\n    assert is_palindromic(0o121) == False\n    assert is_palindromic(0o121, 8) == True\n", "def test_is_palindromic():\n    assert is_palindromic(12321)\n    assert not is_palindromic(123210)\n    assert is_palindromic(123321)\n    assert is_palindromic(1234321)\n    assert not is_palindromic(123456789)\n", "def test_is_palindromic():\n    assert is_palindromic(12321)\n    assert not is_palindromic(12345)\n    assert is_palindromic(123321)\n    assert is_palindromic(1221, 10)\n    assert not is_palindromic(1221, 8)\n    assert is_palindromic(0b10101010, 2)\n    assert not is_palindromic(0b10101011, 2)\n", "def test_is_palindromic():\n    assert is_palindromic(12321)\n    assert not is_palindromic(12345)\n    assert is_palindromic(123321, 10)\n    assert not is_palindromic(123321, 8)\n    assert is_palindromic(-12321, 10)\n    assert is_palindromic(0)\n    raises(ValueError, lambda: is_palindromic(123, 1))\n", "def test_count_digits():\n    assert count_digits(123321) == {1: 2, 2: 2, 3: 2}\n    assert count_digits(0) == {0: 1}\n    assert count_digits(-12321) == {1: 2, 2: 2, 3: 1}\n    assert count_digits(1010101, 2) == {1: 4, 0: 3}\n"], "sample_1000": ["def test_octave_code_HadamardProduct():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    H = HadamardProduct(A, B)\n    assert mcode(H) == 'A.*B'\n", "def test_octave_code_special_functions():\n    assert mcode(lowergamma(x, y)) == \"gammainc(y, x, 'lower')\"\n    assert mcode(uppergamma(x, y)) == \"gammainc(y, x, 'upper')\"\n    assert mcode(sinc(x)) == \"sinc(x/pi)\"\n    assert mcode(hankel1(n, x)) == \"besselh(n, 1, x)\"\n    assert mcode(hankel2(n, x)) == \"besselh(n, 2, x)\"\n    assert mcode(jn(n, x)) == \"sqrt(pi/(2*x))*besselj(n + 1/2, x)\"\n    assert mcode(yn(n, x)) == \"sqrt(pi/(2*x))*bessely(n + 1/2, x)\"\n    assert mcode(airyai(x)) == \"airy(0, x)\"\n    assert mcode(airyaiprime(x)) == \"airy(1, x)\"\n    assert mcode(airybi(x)) == \"airy(2, x)\"\n    assert mcode(airybiprime(x)) == \"airy(3, x)\"\n", "def test_OctaveCodePrinter_print_log():\n    assert mcode(log(x)) == \"log(x)\"\n", "def test_print_OctaveCodePrinter():\n    printer = OctaveCodePrinter()\n    assert printer._print_Assignment(Assignment(x, y)) == 'x = y;'\n", "def test_octave_code_hadamard_product():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    HP = HadamardProduct(A, B)\n    assert mcode(HP) == 'A.*B'\n"], "sample_1001": ["def test_latex_Tr():\n    A = Matrix(2, 2, [1, 2, 3, 4])\n    assert latex(Tr(A)) == r'\\mbox{Tr}\\left(\\left[\\begin{matrix}1 & 2\\\\3 & 4\\end{matrix}\\right]\\right)'\n", "def test_latex_log():\n    expr = log(10)\n    assert latex(expr) == r\"\\log{\\left (10 \\right )}\"\n    assert latex(expr, ln_notation=True) == r\"\\ln{\\left (10 \\right )}\"\n", "def test_latex_Quaternion():\n    q = Quaternion(1, 2, 3, 4)\n    expected = \"1 + 2\\\\;\\\\mathbf{i} + 3\\\\;\\\\mathbf{j} + 4\\\\;\\\\mathbf{k}\"\n    assert latex(q) == expected\n", "def test_print_Quaternion():\n    q = Quaternion(1, 2, 3, 4)\n    assert latex(q) == \"1 + 2\\\\mathbf{i} + 3\\\\mathbf{j} + 4\\\\mathbf{k}\"\n", "def test_latex_PolynomialRingBase():\n    R, x, y = ring(\"x,y\", ZZ)\n    assert latex(R) == r\"ZZ[x, y]\"\n    assert latex(R.to_domain()) == r\"ZZ[x, y]\"\n    assert latex(R.to_field()) == r\"QQS_<^{-1}ZZ[x, y]\"\n"], "sample_870": ["def test_predict_std(kernel):\n    gpr = GaussianProcessRegressor(kernel=kernel, alpha=0.0)\n    gpr.fit(X, y)\n    _, y_std = gpr.predict(X2, return_std=True)\n    # Check that std is non-negative\n    assert_array_less(y_std, 0.0)\n", "def test_predict_unfitted_model(kernel):\n    gpr = GaussianProcessRegressor(kernel=kernel)\n    y_mean = gpr.predict(X)\n    assert y_mean.shape == (X.shape[0],)\n", "def test_fit_with_restarts(kernel):\n    gpr = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=5)\n    gpr.fit(X, y)\n    assert gpr.log_marginal_likelihood_value_ >= -np.inf\n", "def test_n_restarts_optimizer(kernel, n_restarts_optimizer):\n    gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=n_restarts_optimizer)\n    gp.fit(X, y)\n    assert gp.kernel_ is not None, \"Kernel should be optimized\"\n", "def test_multioutput_fit_predict_prior(n_targets):\n    gpr = GaussianProcessRegressor(kernel=None, n_targets=n_targets)\n    y_mean_prior = gpr.predict(X2)\n    y_mean_prior_cov, y_cov_prior = gpr.predict(X2, return_cov=True)\n    y_mean_prior_std, y_std_prior = gpr.predict(X2, return_std=True)\n\n    assert y_mean_prior.shape == (X2.shape[0], n_targets)\n    assert y_mean_prior_cov.shape == (X2.shape[0], n_targets)\n    assert y_cov_prior.shape == (X2.shape[0], X2.shape[0], n_targets)\n    assert y_mean_prior_std.shape == (X2.shape[0], n_targets)\n    assert y_std_prior.shape == (X2.shape[0], n_targets)\n\n    assert_array_almost_equal(y_mean_prior, y_mean_prior_cov)\n    assert_array_almost_equal(y_std_prior, np.sqrt(np.diagonal(y_cov_prior, axis1=1, axis2=2)))\n"], "sample_1186": ["def test_array_derivative():\n    a = ImmutableDenseNDimArray([x**2, y**2], (2,))\n    da = a.diff(x)\n    assert da[0] == 2*x\n    assert da[1] == 0\n", "def test_array_derivative():\n    a = ImmutableDenseNDimArray([x**2, y**2], (2,))\n    da_dx = a.diff(x)\n    assert da_dx == ImmutableDenseNDimArray([2*x, 0], (2,))\n    da_dy = a.diff(y)\n    assert da_dy == ImmutableDenseNDimArray([0, 2*y], (2,))\n", "def test_array_operations():\n    a = ImmutableDenseNDimArray([1, 2, 3, 4], (2, 2))\n    b = ImmutableDenseNDimArray([5, 6, 7, 8], (2, 2))\n\n    # Test addition\n    assert a + b == ImmutableDenseNDimArray([6, 8, 10, 12], (2, 2))\n\n    # Test subtraction\n    assert a - b == ImmutableDenseNDimArray([-4, -4, -4, -4], (2, 2))\n\n    # Test multiplication by scalar\n    assert a * 3 == ImmutableDenseNDimArray([3, 6, 9, 12], (2, 2))\n\n    # Test division by scalar\n    assert a / 2 == ImmutableDenseNDimArray([0.5, 1, 1.5, 2], (2, 2))\n\n    # Test negation\n    assert -a == ImmutableDenseNDimArray([-1, -2, -3, -4], (2, 2))\n", "def test_array_different_shape_addition():\n    a = MutableDenseNDimArray([1, 2, 3], (1, 3))\n    b = MutableDenseNDimArray([1, 2], (2, 1))\n    with raises(ValueError) as err:\n        c = a + b\n    assert str(err.value) == \"array shape mismatch\"\n", "def test_array_operations():\n    for array_type in array_types:\n        a = array_type([1, 2, 3, 4], (2, 2))\n        b = array_type([4, 3, 2, 1], (2, 2))\n\n        assert a + b == array_type([5, 5, 5, 5], (2, 2))\n        assert a - b == array_type([-3, -1, 1, 3], (2, 2))\n        assert a * 2 == array_type([2, 4, 6, 8], (2, 2))\n        assert 2 * a == array_type([2, 4, 6, 8], (2, 2))\n        assert a / 2 == array_type([0.5, 1.0, 1.5, 2.0], (2, 2))\n"], "sample_366": ["    def test_parse_datetime_with_offset(self):\n        # Test parsing datetime with timezone offset\n        dt = parse_datetime('2022-01-01T12:34:56+03:00')\n        self.assertEqual(dt, datetime(2022, 1, 1, 12, 34, 56, tzinfo=get_fixed_timezone(180)))\n\n        # Test parsing datetime with negative timezone offset\n        dt = parse_datetime('2022-01-01T12:34:56-05:00')\n        self.assertEqual(dt, datetime(2022, 1, 1, 12, 34, 56, tzinfo=get_fixed_timezone(-300)))\n", "    def test_parse_duration(self):\n        self.assertEqual(parse_duration('1 00:00:00.000001'), timedelta(days=1, microseconds=1))\n        self.assertEqual(parse_duration('1 00:00:00'), timedelta(days=1))\n        self.assertEqual(parse_duration('00:00:01'), timedelta(seconds=1))\n        self.assertEqual(parse_duration('-1 00:00:01'), timedelta(days=-1, seconds=1))\n        self.assertEqual(parse_duration('P1DT1S'), timedelta(days=1, seconds=1))\n        self.assertEqual(parse_duration('-P1DT1S'), timedelta(days=-1, seconds=1))\n        self.assertEqual(parse_duration('3 days 04:05:06'), timedelta(days=3, hours=4, minutes=5, seconds=6))\n        self.assertEqual(parse_duration('-3 days 04:05:06'), timedelta(days=-3, hours=4, minutes=5, seconds=6))\n", "    def test_parse_duration(self):\n        self.assertEqual(parse_duration('1 00:00:00'), timedelta(days=1))\n        self.assertEqual(parse_duration('-1 00:00:00'), timedelta(days=-1))\n        self.assertEqual(parse_duration('00:01:00'), timedelta(minutes=1))\n        self.assertEqual(parse_duration('00:00:01'), timedelta(seconds=1))\n        self.assertEqual(parse_duration('00:00:01.123456'), timedelta(seconds=1, microseconds=123456))\n        self.assertEqual(parse_duration('P1D'), timedelta(days=1))\n        self.assertEqual(parse_duration('P1DT1H'), timedelta(days=1, hours=1))\n        self.assertEqual(parse_duration('P1DT1H1M1S'), timedelta(days=1, hours=1, minutes=1, seconds=1))\n        self.assertEqual(parse_duration('P1DT1H1M1.123456S'), timedelta(days=1, hours=1, minutes=1, seconds=1, microseconds=123456))\n        self.assertEqual(parse_duration('-1 01:01:01'), timedelta(days=-1, hours=1, minutes=1, seconds=1))\n        self.assertEqual(parse_duration('-1 01:01:01.123456'), timedelta(days=-1, hours=1, minutes=1, seconds=1, microseconds=123456))\n        self.assertEqual(parse_duration('3 days 04:05:06'), timedelta(days=3, hours=4, minutes=5, seconds=6))\n        self.assertEqual(parse_duration('-3 days 04:05:06'), timedelta(days=-3, hours=4, minutes=5, seconds=6))\n        self.assertEqual(parse_", "    def test_parse_datetime(self):\n        # Test a datetime string without timezone information\n        self.assertEqual(parse_datetime('2022-01-01T12:34:56'), datetime(2022, 1, 1, 12, 34, 56))\n        # Test a datetime string with 'Z' timezone information\n        self.assertEqual(parse_datetime('2022-01-01T12:34:56Z'), datetime(2022, 1, 1, 12, 34, 56, tzinfo=utc))\n        # Test a datetime string with positive timezone information\n        self.assertEqual(parse_datetime('2022-01-01T12:34:56+03:00'), datetime(2022, 1, 1, 12, 34, 56, tzinfo=get_fixed_timezone(180)))\n        # Test a datetime string with negative timezone information\n        self.assertEqual(parse_datetime('2022-01-01T12:34:56-05:00'), datetime(2022, 1, 1, 12, 34, 56, tzinfo=get_fixed_timezone(-300)))\n", "def test_parse_duration(self):\n    self.assertEqual(parse_duration('3 days 04:05:06.789123'), timedelta(days=3, hours=4, minutes=5, seconds=6, microseconds=789123))\n    self.assertEqual(parse_duration('P3DT4H5M6.789123S'), timedelta(days=3, hours=4, minutes=5, seconds=6, microseconds=789123))\n    self.assertEqual(parse_duration('3 04:05:06.789123'), timedelta(days=3, hours=4, minutes=5, seconds=6, microseconds=789123))\n    self.assertEqual(parse_duration('-3 04:05:06.789123'), -timedelta(days=3, hours=4, minutes=5, seconds=6, microseconds=789123))\n    self.assertEqual(parse_duration('+3 04:05:06.789123'), timedelta(days=3, hours=4, minutes=5, seconds=6, microseconds=789123))\n"], "sample_251": ["def test_value_expression(self):\n    authors = Author.objects.annotate(\n        value_age=Value(F('age') + 1)\n    ).values('name', 'value_age')\n    authors_list = list(authors)\n    self.assertEqual(authors_list[0]['value_age'], self.a1.age + 1)\n    self.assertEqual(authors_list[1]['value_age'], self.a2.age + 1)\n", "    def test_case_when_annotation(self):\n        case_when = Case(\n            When(name__startswith='The', then=Value('The_')),\n            When(name__startswith='Python', then=Value('Python_')),\n            default=Value('Other_'),\n            output_field=CharField(),\n        )\n        books = Book.objects.annotate(custom_name=case_when)\n        self.assertEqual(books[0].custom_name, 'The_')\n        self.assertEqual(books[3].custom_name, 'Python_')\n        self.assertEqual(books[5].custom_name, 'Other_')\n", "    def test_raw_sql_with_foreign_key_field(self):\n        # Test that RawSQL expressions can reference foreign key fields.\n        books = Book.objects.annotate(\n            publisher_name=RawSQL('SELECT name FROM myapp_publisher WHERE id=%s', (F('publisher_id'),))\n        )\n        self.assertEqual(books[0].publisher_name, self.p1.name)\n", "    def test_combined_expression(self):\n        expr = F('pages') + 5\n        self.assertIsInstance(expr, CombinedExpression)\n        self.assertEqual(expr.connector, Combinable.ADD)\n        self.assertEqual(str(expr), \"pages + 5\")\n", "    def test_subquery_expression_wrapper(self):\n        subquery = Subquery(Book.objects.filter(authors=OuterRef('pk')).values('price'))\n        annotated_books = Book.objects.annotate(\n            cheapest_price=ExpressionWrapper(subquery, output_field=FloatField())\n        )\n        cheapest_price = annotated_books.get(isbn='159059725').cheapest_price\n        self.assertEqual(cheapest_price, Decimal('29.69'))\n"], "sample_737": ["def test_strip_tags():\n    text = \"<html><body><p>This is a test.</p></body></html>\"\n    expected = \"This is a test.\"\n    assert strip_tags(text) == expected\n", "def test_strip_tags():\n    text = \"<html><body><p>This is a <b>test</b> paragraph.</p></body></html>\"\n    assert strip_tags(text) == \"This is a test paragraph.\"\n", "def test_strip_tags():\n    html_doc = \"<html><body><p>This is a test.</p></body></html>\"\n    expected_output = \"This is a test.\"\n    assert strip_tags(html_doc) == expected_output\n", "def test_hashingvectorizer_input_validation():\n    vectorizer = HashingVectorizer()\n    with assert_raises(ValueError):\n        vectorizer.fit(\"string object\")\n", "def test_tfidf_transformer_with_custom_idf():\n    # Create a simple CountVectorizer and fit it to the documents\n    vectorizer = CountVectorizer()\n    X = vectorizer.fit_transform(ALL_FOOD_DOCS)\n\n    # Create a custom IDF vector\n    custom_idf = np.ones(X.shape[1])\n    custom_idf[vectorizer.vocabulary_['pizza']] = 1.5\n\n    # Create a TfidfTransformer with custom idf vector\n    tfidf_transformer = TfidfTransformer(use_idf=True)\n    tfidf_transformer.idf_ = custom_idf\n\n    # Transform the count matrix with the custom transformer\n    X_tfidf = tfidf_transformer.transform(X)\n\n    # Check if the pizza feature's IDF is equal to the custom value\n    pizza_idx = vectorizer.vocabulary_['pizza']\n    assert_almost_equal(X_tfidf[:, pizza_idx].toarray(), np.full((X.shape[0], 1), custom_idf[pizza_idx]))\n"], "sample_26": ["def test_update_header(self):\n    hdu = fits.ImageHDU(np.ones((10, 10)))\n    hdu._axes = [20, 20]\n    hdu.update_header()\n    assert hdu._header['NAXIS1'] == 20\n    assert hdu._header['NAXIS2'] == 20\n", "def test_section_slicing(self):\n    # Create a simple ImageHDU with known data\n    data = np.arange(24).reshape(4, 6)\n    hdu = fits.ImageHDU(data)\n\n    # Test slicing the section\n    section = hdu.section\n    sliced_data = section[1:3, 2:4]\n    expected_data = np.array([[8, 9], [14, 15]])\n    assert_equal(sliced_data, expected_data)\n", "def test_scale_internal(self):\n    \"\"\"\n    Test the _scale_internal method of the _ImageBaseHDU class.\n    \"\"\"\n    hdu = fits.ImageHDU(data=np.array([1, 2, 3, 4], dtype=np.uint8))\n    hdu._scale_internal(type=\"int16\", bscale=2, bzero=1)\n    assert_equal(hdu.data, np.array([1, 3, 5, 7], dtype=np.int16))\n    assert hdu._header[\"BSCALE\"] == 2\n    assert hdu._header[\"BZERO\"] == 1\n", "def test_section_slicing(self):\n    data = np.arange(60).reshape(3, 4, 5)\n    hdu = fits.PrimaryHDU(data)\n    section = hdu.section\n\n    # Test slicing with integers\n    assert_equal(section[0, 1, 2], 12)\n    assert_equal(section[-1, -2, -3], 52)\n\n    # Test slicing with slices\n    assert_equal(section[1:, 0:2, :], data[1:, 0:2, :])\n    assert_equal(section[:, :, 2:4], data[:, :, 2:4])\n\n    # Test slicing with a combination of integers and slices\n    assert_equal(section[0, 1:, 2:4], data[0, 1:, 2:4])\n    assert_equal(section[1:, :, -1], data[1:, :, -1])\n\n    # Test slicing with ellipsis\n    assert_equal(section[1, ..., 2], data[1, :, :, 2])\n    assert_equal(section[..., 2, 3], data[:, :, 2, 3])\n\n    # Test slicing with a combination of integers, slices, and ellipsis\n    assert_equal(section[0, 1:, ..., 2], data[0, 1:, :, :, 2])\n    assert_equal(section[..., 2, :], data[:, :, :, 2, :])\n", "def test_section_slicing(self):\n    data = np.arange(1, 26).reshape(5, 5)\n    hdu = fits.PrimaryHDU(data)\n    section = hdu.section\n\n    # Test slicing with int\n    sliced_data = section[1, 1]\n    assert_equal(sliced_data, data[1, 1])\n\n    # Test slicing with slice\n    sliced_data = section[1:3, 1:3]\n    assert_equal(sliced_data, data[1:3, 1:3])\n\n    # Test slicing with step\n    sliced_data = section[::2, ::2]\n    assert_equal(sliced_data, data[::2, ::2])\n\n    # Test slicing with mixed int and slice\n    sliced_data = section[1, :]\n    assert_equal(sliced_data, data[1, :])\n"], "sample_902": ["def test_pipeline_with_fit_params():\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([0, 1, 0])\n\n    pipeline = make_pipeline(Mult(mult=2), TransfFitParams())\n    pipeline.fit(X, y, transffitparams__should_succeed=True)\n\n    assert_dict_equal(pipeline.named_steps['transffitparams'].fit_params, {'should_succeed': True})\n", "def test_pipeline_transform():\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    transformer1 = Mult(2)\n    transformer2 = Mult(3)\n    estimator = Lasso()\n    pipeline = Pipeline([('transformer1', transformer1), ('transformer2', transformer2), ('estimator', estimator)])\n    Xt = pipeline.transform(X)\n    assert_array_equal(Xt, np.array([[6, 12], [18, 24], [30, 36]]))\n", "def test_pipeline_with_fit_params():\n    \"\"\"Test that the pipeline supports fit_params.\"\"\"\n    X = np.array([[0, 1], [2, 3], [4, 5]])\n    y = np.array([0, 1, 0])\n    p = Pipeline([('mult1', Mult(mult=10)), ('mult2', Mult(mult=10))])\n    p.set_params(mult1__mult=20, mult2__mult=30).fit(X, y, mult1__should_succeed=True)\n    assert_array_equal(p.predict(X), np.array([0, 180, 0]))\n    assert_array_equal(p.named_steps['mult1'].mult, 20)\n    assert_array_equal(p.named_steps['mult2'].mult, 30)\n", "def test_pipeline_fit_transform_params():\n    # Test that fit_params are passed to fit of each estimator\n    pipe = make_pipeline(TransfFitParams(), TransfFitParams())\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([0, 1])\n    fit_params = {'est1__should_succeed': True, 'est2__should_succeed': False}\n    pipe.fit(X, y, **fit_params)\n    assert_true(pipe.steps[0][1].fit_params == {'should_succeed': True})\n    assert_false(pipe.steps[1][1].fit_params == {'should_succeed': True})\n", "def test_pipeline_transform_and_inverse_transform():\n    # Creating a simple pipeline with a single transformation\n    pipeline = Pipeline(steps=[('mult', Mult(mult=2))])\n    X = np.array([[1, 2], [3, 4]])\n\n    # Testing the 'transform' method\n    X_transformed = pipeline.transform(X)\n    assert_array_equal(X_transformed, np.array([[2, 4], [6, 8]]))\n\n    # Testing the 'inverse_transform' method\n    X_inverse_transformed = pipeline.inverse_transform(X_transformed)\n    assert_array_equal(X_inverse_transformed, X)\n"], "sample_417": ["def test_floatformat02(self):\n    with self.settings(USE_THOUSAND_SEPARATOR=True, THOUSAND_SEPARATOR=\",\"):\n        self.assertHTMLEqual(\n            self.engine.render_to_string(\n                \"floatformat01\", {\"a\": Decimal(\"6666.6666\"), \"b\": 10000}\n            ),\n            \"6,666.7 10,000\",\n        )\n", "def test_floatformat_with_args(self):\n    self.engine.assert_output(\n        self.templates[\"floatformat02\"],\n        {\"a\": 34.23234, \"b\": 34.00000, \"c\": 6666.6666, \"d\": 10000, \"e\": 66666.6666},\n        \"34.232 34 6,666.7 10,000 66666.67\",\n    )\n", "    def test_floatformat_with_positive_arg(self):\n        self.context[\"c\"] = Decimal(\"34.23234\")\n        self.execute_template(\"floatformat02\")\n        self.assertHTMLEqual(self.rendered_content, \"34.23\")\n", "    def test_floatformat_positive_and_negative_arguments(self):\n        \"\"\"\n        Test the floatformat filter with positive and negative arguments.\n        \"\"\"\n        self.engine.context[\"c\"] = Decimal(\"6666.6666\")\n        self.engine.context[\"d\"] = Decimal(\"34.23234\")\n        rendered = self.engine.render_to_string(\"floatformat02\")\n        self.assertHTMLEqual(rendered, \"6666.667 34.232\")\n", "def test_floatformat_with_arg(self):\n    context = self.get_context(\n        {\n            \"a\": Decimal(\"34.23234\"),\n            \"b\": Decimal(\"34.00000\"),\n        }\n    )\n    output = self.engine.render_to_string(\"floatformat01\", context)\n    self.assertEqual(output, \"34.2 34\")\n"], "sample_790": ["def test_kernel_pca_precomputed_kernel():\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    K = rbf_kernel(X)\n\n    kpca = KernelPCA(n_components=2, kernel='precomputed')\n    X_transformed = kpca.fit_transform(K)\n\n    assert X_transformed.shape == (3, 2)\n    assert np.allclose(np.dot(X_transformed.T, X_transformed), np.eye(2))\n", "def test_kernel_pca_precomputed():\n    # Generate sample data\n    X, _ = make_circles(n_samples=100, noise=0.05, factor=0.2, random_state=42)\n\n    # Precompute the RBF kernel matrix\n    K = rbf_kernel(X, gamma=15)\n\n    # Fit KernelPCA with precomputed kernel\n    kpca = KernelPCA(n_components=2, kernel='precomputed')\n    X_kpca = kpca.fit_transform(K)\n\n    # Assert that the transformed data has the correct shape\n    assert X_kpca.shape == (100, 2)\n\n    # Assert that the eigenvalues are not zero\n    assert np.all(kpca.lambdas_ > 0)\n\n    # Assert that the eigenvectors are not zero\n    assert np.all(np.abs(kpca.alphas_) > 0)\n", "def test_kernel_pca_precomputed():\n    X, _ = make_circles(n_samples=100, random_state=0, noise=0.1)\n    K = rbf_kernel(X)\n    kpca = KernelPCA(n_components=2, kernel='precomputed', fit_inverse_transform=True)\n    X_transformed = kpca.fit_transform(K)\n    X_inverse = kpca.inverse_transform(X_transformed)\n    assert_allclose(X, X_inverse, atol=1e-2)\n", "def test_kernel_pca_precomputed_kernel():\n    # Generate a simple dataset\n    X, _ = make_circles(n_samples=100, noise=0.05, random_state=42)\n    # Compute the RBF kernel matrix\n    gamma = 1.0 / X.shape[1]\n    K = rbf_kernel(X, gamma=gamma)\n\n    # Fit KernelPCA with the precomputed kernel\n    kpca = KernelPCA(n_components=2, kernel=\"precomputed\", fit_inverse_transform=True)\n    X_transformed = kpca.fit_transform(K)\n    X_inverse_transformed = kpca.inverse_transform(X_transformed)\n\n    # Assert that the shape of the transformed data is correct\n    assert X_transformed.shape == (100, 2)\n\n    # Assert that the inverse transform brings the data back to the original space\n    assert_allclose(X_inverse_transformed, X, atol=1e-2)\n", "def test_kernel_pca_with_precomputed_kernel():\n    \"\"\"Test KernelPCA with precomputed kernel\"\"\"\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n    K = rbf_kernel(X, gamma=0.5)\n\n    kpca = KernelPCA(n_components=2, kernel=\"precomputed\", fit_inverse_transform=True)\n    X_transformed = kpca.fit_transform(K)\n    X_inverse = kpca.inverse_transform(X_transformed)\n\n    assert_array_almost_equal(X, X_inverse)\n"], "sample_681": ["def test_logging_plugin_set_log_path(testdir: Testdir) -> None:\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n        import pytest\n\n        @pytest.fixture\n            return str(tmp_path / \"test.log\")\n\n            logging_plugin = pytestconfig.pluginmanager.get_plugin(\"logging-plugin\")\n            logging_plugin.set_log_path(log_file)\n            logger = logging.getLogger(__name__)\n            logger.warning(\"Test log message\")\n\n            with open(log_file, \"r\") as f:\n                assert \"Test log message\" in f.read()\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.assert_outcomes(passed=1)\n", "def test_live_logging_stream_handler(testdir, capsys):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n        import pytest\n\n        @pytest.fixture\n            class MockTerminalReporter:\n                    pass\n                    pass\n            return MockTerminalReporter()\n\n        @pytest.fixture\n            class MockCaptureManager:\n                    return nullcontext()\n            return MockCaptureManager()\n\n            handler = _LiveLoggingStreamHandler(terminal_reporter, capture_manager)\n            record = logging.LogRecord('name', logging.INFO, 'pathname', 1, 'msg', None, None)\n            handler.emit(record)\n            # Add assertions to check if the emit method is working correctly\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    assert result.ret == 0\n", "def test_set_log_path(pytester: Testdir, monkeypatch):\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n        import pytest\n\n            fname = 'test.log'\n            logging_plugin.set_log_path(fname)\n            assert logging_plugin.log_file_handler.stream.name == fname\n            os.remove(fname)\n\n        @pytest.fixture\n            return request.config.pluginmanager.get_plugin('logging-plugin')\n        \"\"\"\n    )\n    result = pytester.runpytest('-vv')\n    result.assert_outcomes(passed=1)\n", "def test_set_log_path_creates_parent_directory(testdir: Testdir) -> None:\n    testdir.makeconftest(\n        \"\"\"\n        import pytest\n        from _pytest.logging import LoggingPlugin\n\n        @pytest.fixture\n            config = request.config\n            plugin = LoggingPlugin(config)\n            return plugin\n        \"\"\"\n    )\n\n    testdir.makepyfile(\n        \"\"\"\n        import os\n            fname = 'test/test.log'\n            logging_plugin.set_log_path(fname)\n            assert os.path.isdir('test')\n            os.rmdir('test')\n        \"\"\"\n    )\n\n    result = testdir.runpytest()\n    result.assert_outcomes(passed=1)\n", "def test_log_cli_enabled(pytester: Testdir) -> None:\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n        @pytest.hookimpl\n            terminal_reporter = config.pluginmanager.get_plugin(\"terminalreporter\")\n            if terminal_reporter:\n                config.option.log_cli = True\n        \"\"\"\n    )\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n            logging.info('Test log message')\n            assert 'Test log message' in caplog.text\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines(['*Test log message*'])\n"], "sample_904": ["def test_cmdoption_handle_signature(app):\n    env = app.builder.env\n    std_domain = StandardDomain(env)\n    cmdoption = std_domain.directives['cmdoption'](\n        name='cmdoption', arguments=['/opt args'], options={}, content=None, lineno=1,\n        content_offset=0, block_text='', state=mock.Mock(), state_machine=mock.Mock()\n    )\n    cmdoption.env = env\n    cmdoption.state = mock.Mock()\n    cmdoption.state.document = mock.Mock()\n\n    signode = desc_signature()\n    firstname = cmdoption.handle_signature('/opt args', signode)\n\n    assert firstname == '/opt'\n    assert isinstance(signode[0], desc_name)\n    assert signode[0].astext() == '/opt'\n    assert isinstance(signode[1], desc_addname)\n    assert signode[1].astext() == 'args'\n", "def test_make_glossary_term(app):\n    env = app.builder.env\n    document = env.get_doctree('dummy')\n    textnodes = [nodes.Text(\"Test Term\")]\n    index_key = \"Test Key\"\n    source = \"test.rst\"\n    lineno = 1\n    node_id = \"test-node-id\"\n\n    term_node = StandardDomain.make_glossary_term(env, textnodes, index_key, source, lineno, node_id, document)\n\n    assert isinstance(term_node, nodes.term)\n    assert term_node.rawsource == \"Test Term\"\n    assert term_node.astext() == \"Test Term\"\n    assert term_node['ids'] == [node_id]\n    assert isinstance(term_node.children[0], addnodes.index)\n    assert term_node.children[0]['entries'] == [('single', \"Test Term\", node_id, 'main', index_key)]\n", "def test_envvar_role(app):\n    app.builder.build_all()\n\n    doctree = app.env.get_doctree('test-envvar')\n    ref_nodes = list(doctree.traverse(nodes.reference))\n\n    assert len(ref_nodes) == 1\n    assert ref_nodes[0]['refdomain'] == 'std'\n    assert ref_nodes[0]['reftype'] == 'envvar'\n    assert ref_nodes[0]['reftarget'] == 'PLUGIN_PATH'\n", "def test_standard_domain_note_hyperlink_target(app):\n    env = app.builder.env\n    std = StandardDomain(env)\n\n    # Test adding a new hyperlink target\n    std.note_hyperlink_target('new_target', 'docname', 'labelid', 'title')\n    assert ('docname', 'labelid') == std.anonlabels['new_target']\n    assert ('docname', 'labelid', 'title') == std.labels['new_target']\n\n    # Test warning when adding a duplicate label\n    with mock.patch('sphinx.util.logging.warning') as warning_mock:\n        std.note_hyperlink_target('new_target', 'other_docname', 'other_labelid', 'other_title')\n        warning_mock.assert_called_once()\n", "def test_cmdoption_directive(app, warning):\n    with app.test_env():\n        env = app.builder.env\n        std = StandardDomain(env)\n        env.domains['std'] = std\n\n        node = nodes.document()\n        option = mock.Mock(env=env, state=mock.Mock(document=node))\n\n        option.arguments = ['--opt args']\n        option.handle_signature = mock.Mock(return_value='opt')\n\n        option.run()\n\n        assert 'std:program' not in env.ref_context\n        assert ('cmdoption', '--opt') in std.objects\n        assert ('arg', '--opt') in std.objects\n        assert ('cmdoption', 'opt') in std.progoptions\n        assert 'cmdoption' in node[0]['classes']\n        assert 'opt' in node[0][0][0]['classes']\n        assert 'arg' in node[0][0][0]['classes']\n        assert node[0][0][0].astext() == '--opt'\n        assert node[0][0][1].astext() == 'args'\n        assert node[0][0]['ids'] == [\n            'cmdoption-opt',\n            'cmdoption---opt',\n            'cmdoption-arg-opt',\n            'cmdoption-arg---opt',\n            'cmdoption-program-opt',\n            'cmdoption-program---opt',\n        ]\n"], "sample_2": ["def test_uncertainty_unit_equivalence():\n    # Test the _uncertainty_unit_equivalent_to_parent function\n    ccd = create_ccd_data()\n\n    # Test with StdDevUncertainty\n    assert _uncertainty_unit_equivalent_to_parent(StdDevUncertainty, u.adu, u.adu)\n    assert not _uncertainty_unit_equivalent_to_parent(StdDevUncertainty, u.Jy, u.adu)\n\n    # Test with VarianceUncertainty\n    assert _uncertainty_unit_equivalent_to_parent(VarianceUncertainty, u.adu**2, u.adu)\n    assert not _uncertainty_unit_equivalent_to_parent(VarianceUncertainty, u.Jy**2, u.adu)\n\n    # Test with InverseVariance\n    assert _uncertainty_unit_equivalent_to_parent(InverseVariance, 1 / (u.adu**2), u.adu)\n    assert not _uncertainty_unit_equivalent_to_parent(InverseVariance, 1 / (u.Jy**2), u.adu)\n\n    # Test with unsupported uncertainty type\n    with pytest.raises(ValueError):\n        _uncertainty_unit_equivalent_to_parent(object, u.adu, u.adu)\n", "def test_uncertainty_variance():\n    data = _random_array.copy()\n    uncertainty = np.abs(_random_array.copy())\n    variance_uncertainty = VarianceUncertainty(uncertainty ** 2)\n    ccd = CCDData(data, unit=u.adu, uncertainty=variance_uncertainty)\n    assert isinstance(ccd.uncertainty, VarianceUncertainty)\n    assert np.allclose(ccd.uncertainty.array, uncertainty ** 2)\n", "def test_uncertainty_unit_equivalent_to_parent():\n    data = np.array([1, 2, 3])\n    ccd = CCDData(data, unit=u.adu)\n\n    # Test StdDevUncertainty\n    uncertainty = StdDevUncertainty(np.array([0.1, 0.2, 0.3]), unit=u.adu)\n    assert _uncertainty_unit_equivalent_to_parent(StdDevUncertainty, uncertainty.unit, ccd.unit)\n\n    # Test VarianceUncertainty\n    uncertainty = VarianceUncertainty(np.array([0.01, 0.04, 0.09]), unit=u.adu**2)\n    assert _uncertainty_unit_equivalent_to_parent(VarianceUncertainty, uncertainty.unit, ccd.unit)\n\n    # Test InverseVariance\n    uncertainty = InverseVariance(np.array([100, 25, 11.11]), unit=1/(u.adu**2))\n    assert _uncertainty_unit_equivalent_to_parent(InverseVariance, uncertainty.unit, ccd.unit)\n\n    # Test unsupported uncertainty type\n    with pytest.raises(ValueError):\n        _uncertainty_unit_equivalent_to_parent(float, u.adu, ccd.unit)\n", "def test_ccd_data_uncertainty_variance():\n    data = _random_array.copy()\n    uncertainty = np.sqrt(data)\n    ccd = CCDData(data, uncertainty=VarianceUncertainty(uncertainty**2), unit=u.adu)\n\n    assert np.array_equal(ccd.uncertainty.array, uncertainty**2)\n    assert isinstance(ccd.uncertainty, VarianceUncertainty)\n\n    hdulist = ccd.to_hdu(hdu_uncertainty='UNCERT')\n    assert 'UNCERT' in hdulist\n    assert hdulist['UNCERT'].header['UTYPE'] == 'VarianceUncertainty'\n    assert np.array_equal(hdulist['UNCERT'].data, uncertainty**2)\n", "def test_wcs_keyword_removal_for_wcs_test_files():\n    \"\"\"\n    Test that WCS keywords are removed from the header when generating a WCS object.\n    This test uses the WCS test files from astropy/wcs.\n    \"\"\"\n    wcs_test_files = [\n        'idctab.fits', 'sip.fits', 'tpv.fits', 'wcs.fits', '1904-66_TAN.fits',\n        'tpv.hdr', 'wcs.hdr', 'idctab.hdr', 'sip.hdr', '1904-66_TAN.hdr']\n\n    for filename in wcs_test_files:\n        with fits.open(get_pkg_data_filename(f'data/wcstest/{filename}')) as hdul:\n            hdr = hdul[0].header\n            hdr_orig = hdr.copy()\n            hdr, wcs = _generate_wcs_and_update_header(hdr)\n            assert wcs is not None, f\"WCS object should not be None for {filename}\"\n\n            for key in hdr:\n                assert key not in hdr_orig or key in _KEEP_THESE_KEYWORDS_IN_HEADER, \\\n                       f\"Key {key} should have been removed from the header for {filename}\"\n\n            # Check that the WCS object can be used to convert pixel coordinates\n            # to world coordinates and back again.\n            x, y = np.meshgrid(np.arange(10), np.arange(10))\n            ra, dec = wcs.all_pix2world(x, y, 0)\n            x2, y2 = wcs.all_world2pix(ra, dec, 0)\n            assert np.allclose(x, x2, atol=1e-6), \\\n                   f\"WCS object should be able to convert pixel coordinates to world coordinates and back for {filename}\"\n            assert np.allclose(y, y2, atol=1e-6), \\\n                   f\"WCS object should be able"], "sample_135": ["    def test_format_with_timezone(self):\n        dt = datetime(2022, 1, 1, 12, 34, 56, tzinfo=get_fixed_timezone(120))\n        df = dateformat.DateFormat(dt)\n        self.assertEqual(df.format('Y-m-d H:i:s O'), '2022-01-01 12:34:56 +0200')\n", "    def test_date_formatting(self):\n        d = date(2003, 10, 7)\n        df = DateFormat(d)\n        self.assertEqual(df.format('jS F Y'), '7th October 2003')\n        self.assertEqual(df.format('l, d F Y'), 'Tuesday, 07 October 2003')\n        self.assertEqual(df.format('Y-m-d'), '2003-10-07')\n", "    def test_date_format_with_time(self):\n        d = datetime(2022, 3, 14, 15, 30, 0)\n        df = dateformat.DateFormat(d)\n        self.assertEqual(df.format('Y-m-d H:i'), '2022-03-14 15:30')\n", "    def test_date_format_functions(self):\n        dt = datetime(2022, 10, 15, 13, 30, 45, 123456)\n        df = DateFormat(dt)\n        self.assertEqual(df.b(), 'oct')\n        self.assertEqual(df.c(), '2022-10-15T13:30:45.123456')\n        self.assertEqual(df.d(), '15')\n        self.assertEqual(df.D(), 'Sat')\n        self.assertEqual(df.E(), 'Oct')\n        self.assertEqual(df.F(), 'October')\n        self.assertEqual(df.I(), '0')  # Assuming Copenhagen is not in DST on Oct 15\n        self.assertEqual(df.j(), 15)\n        self.assertEqual(df.l(), 'Saturday')\n        self.assertEqual(df.L(), False)\n        self.assertEqual(df.m(), '10')\n        self.assertEqual(df.M(), 'Oct')\n        self.assertEqual(df.n(), 10)\n        self.assertEqual(df.N(), 'Oct.')\n        self.assertEqual(df.o(), 40)  # 40th week of the year\n        self.assertEqual(df.S(), 'th')\n        self.assertEqual(df.t(), '31')\n        self.assertEqual(df.U(), 1665805845)  # Seconds since the Unix epoch\n        self.assertEqual(df.w(), 6)  # Saturday is 6th day of the week\n        self.assertEqual(df.W(), 40)  # 40th week of the year\n        self.assertEqual(df.y(), '22')\n        self.assertEqual(df.Y(), 2022)\n        self.assertEqual(df.z(), 288)  # 288th day of the year\n", "    def test_time_format(self):\n        # Test time format function with various time formats\n        dt = datetime(2022, 1, 1, 15, 30, 45)\n        self.assertEqual(dateformat.time_format(dt, \"h:i:s a\"), \"03:30:45 pm\")\n        self.assertEqual(dateformat.time_format(dt, \"H:i:s\"), \"15:30:45\")\n        self.assertEqual(dateformat.time_format(dt, \"H:i:s e\"), \"15:30:45 +0100\")\n        self.assertEqual(dateformat.time_format(dt, \"f\"), \"3:30\")\n        self.assertEqual(dateformat.time_format(dt, \"G:i:s u\"), \"15:30:45 000000\")\n        self.assertEqual(dateformat.time_format(dt, \"P\"), \"3:30 pm\")\n"], "sample_356": ["def test_deconstructible_objects(self):\n    questioner = MigrationQuestioner(specified_apps=[\"testapp\"], dry_run=True)\n    changes = self.get_changes([self.author_name_deconstructible_1], [self.author_name_deconstructible_2], questioner)\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, preserve_default=True)\n\n    changes = self.get_changes([self.author_name_deconstructible_1], [self.author_name_deconstructible_3], questioner)\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, preserve_default=False)\n\n    changes = self.get_changes([self.author_name_deconstructible_1], [self.author_name_deconstructible_4], questioner)\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, preserve_default=True)\n\n    changes = self.get_changes([self.author_name_deconstructible_list_1], [self.author_name_deconstructible_list_2], questioner)\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, preserve_default=True)\n\n    changes = self.get_changes([self.author_name_deconstructible_list_1], [self.author_name_deconstructible_list_3], questioner)\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, preserve_default=False)\n\n    changes = self.get_changes([self.author_name_deconstructible_tuple_1], [self.author_name_deconstructible_tuple_2], questioner)\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, preserve_default=True)\n\n    changes = self.get_changes([self.author_name_deconstructible_tuple_1], [self.author_name_deconstructible_tuple_3], questioner)\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, preserve_default=False)\n\n   ", "def test_deconstructible_objects(self):\n    questioner = MigrationQuestioner(specified_apps=[\"testapp\"], dry_run=True)\n    changes = self.get_changes(\n        [self.author_name_deconstructible_1],\n        [self.author_name_deconstructible_2],\n        questioner,\n    )\n    self.assertNumberMigrations(changes, \"testapp\", 0)\n", "def test_detect_changes_add_not_null_field(self, mock_ask_not_null_addition):\n    mock_ask_not_null_addition.return_value = 'default value'\n    before_states = [self.author_empty]\n    after_states = [self.author_name]\n    changes = self.get_changes(before_states, after_states)\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['AddField'])\n    self.assertOperationFieldAttributes(changes, 'testapp', 0, 0, null=False)\n", "def test_generate_added_constraints(self):\n    before_states = [self.author_empty]\n    after_states = [self.author_name_check_constraint]\n    changes = self.get_changes(before_states, after_states)\n    self.assertOperationTypes(changes, 'testapp', 0, ['AddConstraint'])\n", "def test_generate_altered_fields_with_deconstructible_objects(self):\n    before_states = [\n        self.author_name_deconstructible_1,\n    ]\n    after_states = [\n        self.author_name_deconstructible_2,\n    ]\n    changes = self.get_changes(before_states, after_states)\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['AlterField'])\n    self.assertOperationFieldAttributes(changes, 'testapp', 0, 0, default=DeconstructibleObject())\n"], "sample_294": ["    def test_POST_request_with_invalid_token(self):\n        req = self._get_POST_request_with_token()\n        req.POST['csrfmiddlewaretoken'] = 'invalid_token'\n        response = self.middleware.process_view(req, token_view, [], {})\n        self.assertEqual(response.status_code, 403)\n        self.assertEqual(response.reason_phrase, REASON_BAD_TOKEN)\n", "def test_origin_verified(self):\n    middleware = CsrfViewMiddleware()\n    request = TestingHttpRequest()\n    request.META['HTTP_ORIGIN'] = 'https://example.com'\n    request.get_host = lambda: 'example.com'\n    self.assertTrue(middleware._origin_verified(request))\n\n    request.META['HTTP_ORIGIN'] = 'http://example.com'\n    request._is_secure_override = True\n    self.assertFalse(middleware._origin_verified(request))\n\n    with self.settings(CSRF_TRUSTED_ORIGINS=['https://*.example.com']):\n        request.META['HTTP_ORIGIN'] = 'https://sub.example.com'\n        self.assertTrue(middleware._origin_verified(request))\n\n    with self.settings(CSRF_TRUSTED_ORIGINS=['https://*.example.com']):\n        request.META['HTTP_ORIGIN'] = 'https://example.org'\n        self.assertFalse(middleware._origin_verified(request))\n", "    def _get_GET_csrf_cookie_request(self):\n        req = self._get_GET_no_csrf_cookie_request()\n        req.COOKIES[settings.CSRF_COOKIE_NAME] = self._csrf_id_cookie\n        return req\n", "def test_session_based_csrf_token_rotation(self):\n    req = TestingHttpRequest()\n    req.session[CSRF_SESSION_KEY] = self._csrf_id\n\n    middleware = CsrfViewMiddleware()\n    response = middleware.process_view(req, token_view, (), {})\n\n    old_token = self._csrf_id\n    new_token = get_token(req)\n\n    self.assertNotEqual(old_token, new_token)\n    self.assertEqual(req.session[CSRF_SESSION_KEY], new_token)\n", "    def test_origin_check_valid_origin(self):\n        middleware = CsrfViewMiddleware()\n        request = self._get_GET_csrf_cookie_request()\n        request.META['HTTP_ORIGIN'] = 'http://testserver'\n        response = middleware.process_view(request, token_view, (), {})\n        self.assertIsNone(response)\n"], "sample_699": ["def test_patch_unwrap_mock_aware(mocker):\n    mock_obj = mocker.Mock()\n    mock_obj.pytest_mock_example_attribute_that_shouldnt_exist = True\n\n    with _patch_unwrap_mock_aware():\n        unwrapped_obj = inspect.unwrap(mock_obj)\n        assert unwrapped_obj is mock_obj\n", "def test_get_checker(pytester: Pytester) -> None:\n    pytester.makepyfile(_GET_CHECKER_CODE)\n    pytester.runpytest().assert_outcomes(passed=1)\n", "def test_get_checker(pytester):\n    checker = _get_checker()\n    assert checker.check_output(\"1.23\", \"1.230000\", checker.NUMBER)\n    assert not checker.check_output(\"1.23\", \"1.24\", checker.NUMBER)\n", "def test_patch_unwrap_mock_aware(self):\n    class DummyClass:\n        pass\n\n    obj = DummyClass()\n    obj.pytest_mock_example_attribute_that_shouldnt_exist = \"mock\"\n\n    with _patch_unwrap_mock_aware():\n        result = inspect.unwrap(obj)\n\n    assert result == obj\n", "def test_doctest_item_repr_failure_no_failures(self, pytester):\n    pytester.makepyfile(\n        \"\"\"\n            pass\n\n            '''\n            '''\n            pass\n        \"\"\"\n    )\n    pytester.import_plugin(\"pytest_doctestplus.plugin\")\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=2)\n"], "sample_1048": ["def test_parabola_intersection_with_line():\n    p1 = Parabola(Point(0, 0), Line(Point(5, 8), Point(7, 8)))\n    l1 = Line(Point(1, -2), Point(-1, -2))\n    assert p1.intersection(l1) == [Point2D(-4, -2), Point2D(4, -2)]\n", "def test_parabola_intersection_with_parabola():\n    p1 = Parabola(Point(0, 0), Line(Point(5, 8), Point(7, 8)))\n    p2 = Parabola(Point(0, 1), Line(Point(3, 8), Point(5, 8)))\n    assert p1.intersection(p2) == [Point(2, 4), Point(-2, 4)]\n", "def test_parabola_intersection_with_line():\n    p1 = Parabola(Point(0, 0), Line(Point(5, 8), Point(7, 8)))\n    l1 = Line(Point(-4, 3), Point(4, 3))\n    assert p1.intersection(l1) == [Point2D(-4, 3), Point2D(4, 3)]\n", "def test_parabola_intersection():\n    p1 = Parabola(Point(0, 0), Line(Point(5, 8), Point(7, 8)))\n    p2 = Parabola(Point(2, 0), Line(Point(5, -2), Point(7, -2)))\n    assert p1.intersection(p2) == [Point2D(0, 4)]\n", "def test_parabola_intersection_with_line():\n    p1 = Parabola(Point(0, 0), Line(Point(5, 8), Point(7, 8)))\n    l1 = Line(Point(1, -2), Point(-1, -2))\n    assert p1.intersection(l1) == [Point2D(-4, -2), Point2D(4, -2)]\n"], "sample_1165": ["def test_quaternion_from_rotation_matrix():\n    M = Matrix([[cos(phi), -sin(phi), 0], [sin(phi), cos(phi), 0], [0, 0, 1]])\n    q = trigsimp(Quaternion.from_rotation_matrix(M))\n    assert q == Quaternion(cos(phi / 2), 0, 0, sin(phi / 2))\n", "def test_quaternion_subtraction():\n    q1 = Quaternion(1, 2, 3, 4)\n    q2 = Quaternion(5, 6, 7, 8)\n    result = q1 - q2\n    expected = Quaternion(-4, -4, -4, -4)\n    assert result == expected\n", "def test_quaternion_from_rotation_matrix_special_orthogonal():\n    M = Matrix([[0, 0, 1], [0, 1, 0], [-1, 0, 0]])\n    q = Quaternion.from_rotation_matrix(M)\n    assert q.norm() == 1\n", "def test_quaternion_rotation_matrix():\n    q = Quaternion(cos(phi/2), 0, 0, sin(phi/2))\n    expected_matrix = Matrix([[cos(phi), -sin(phi), 0], [sin(phi), cos(phi), 0], [0, 0, 1]])\n    assert trigsimp(q.to_rotation_matrix()) == expected_matrix\n", "def test_quaternion_rotation():\n    q = Quaternion(cos(phi/2), 0, 0, sin(phi/2))\n    p = (1, 0, 0)\n    p_rotated = Quaternion.rotate_point(p, q)\n    assert p_rotated == (1, 0, 0)  # Point should not change along z-axis\n    assert abs(p_rotated[0] - cos(phi)) < 1e-10\n    assert abs(p_rotated[1] - sin(phi)) < 1e-10\n"], "sample_784": ["def test_calibrated_classifier_cv_with_sample_weight():\n    # Test CalibratedClassifierCV with sample weight\n    X, y = make_classification(n_samples=100, n_features=20, n_informative=2,\n                               n_redundant=10, n_classes=2, random_state=42)\n    sample_weight = np.random.RandomState(42).rand(100)\n\n    base_estimator = LinearSVC()\n    calibrator = CalibratedClassifierCV(base_estimator=base_estimator, cv=2, method='sigmoid')\n    calibrator.fit(X, y, sample_weight=sample_weight)\n    proba = calibrator.predict_proba(X)\n\n    # Check if the shape of the output is correct\n    assert proba.shape == (100, 2)\n\n    # Check if the probabilities are in the range [0, 1]\n    assert_greater_equal(proba.min(), 0.0)\n    assert_greater_equal(1.0, proba.max())\n", "def test_calibration_curve_normalize():\n    np.random.seed(0)\n    y_true = np.random.randint(2, size=100)\n    y_prob = np.random.rand(100) * 2\n\n    prob_true, prob_pred = calibration_curve(y_true, y_prob, normalize=True)\n\n    assert_almost_equal(y_prob.min(), 0)\n    assert_almost_equal(y_prob.max(), 1)\n", "def test_calibration_curve_errors():\n    y_true = np.array([0, 1, 0, 1])\n    y_prob = np.array([0.2, 0.9, 0.3, 1.1])\n\n    with pytest.raises(ValueError, match=\"y_prob has values outside\"):\n        calibration_curve(y_true, y_prob, normalize=False)\n\n    with pytest.raises(ValueError, match=\"Invalid entry to 'strategy' input\"):\n        calibration_curve(y_true, y_prob, strategy='invalid')\n", "def test_calibration_curve():\n    np.random.seed(0)\n    y_true = np.array([0, 0, 1, 1])\n    y_prob = np.array([0.1, 0.4, 0.35, 0.8])\n\n    prob_true, prob_pred = calibration_curve(y_true, y_prob, n_bins=2)\n\n    assert_array_almost_equal(prob_true, [0.5, 0.5])\n    assert_array_almost_equal(prob_pred, [0.225, 0.675])\n", "def test_calibrated_classifier_cv_sample_weight():\n    # Test that sample_weight is passed correctly to the base estimator\n    # and the calibration step.\n    X, y = make_classification(n_samples=100, n_features=20, n_informative=2,\n                               n_redundant=10, random_state=42)\n    sample_weight = np.random.RandomState(42).rand(100)\n    base_estimator = LinearSVC()\n    calibrator = CalibratedClassifierCV(base_estimator, cv=2)\n    calibrator.fit(X, y, sample_weight=sample_weight)\n\n    assert hasattr(calibrator.calibrated_classifiers_[0].base_estimator, \"coef_\")\n    assert_array_almost_equal(calibrator.calibrated_classifiers_[0].calibrators_[0].sample_weight_,\n                              sample_weight[calibrator.cv.split(X, y).__next__()[0]])\n"], "sample_230": ["def test_json_field_to_python(self):\n    field = JSONField()\n    valid_json = '{\"key\": \"value\"}'\n    self.assertEqual(field.to_python(valid_json), {\"key\": \"value\"})\n    self.assertEqual(field.to_python(''), None)\n    self.assertEqual(field.to_python(None), None)\n    invalid_json = '{\"key\": \"value\"'  # missing closing brace\n    with self.assertRaises(ValidationError):\n        field.to_python(invalid_json)\n", "    def test_uuid_serialization(self):\n        uuid_value = uuid.uuid4()\n        class TestForm(Form):\n            json_field = JSONField()\n\n        form_data = {'json_field': json.dumps(str(uuid_value))}\n        form = TestForm(data=form_data)\n        self.assertTrue(form.is_valid())\n        self.assertEqual(form.cleaned_data['json_field'], str(uuid_value))\n", "def test_bound_data(self):\n    field = JSONField()\n    data = '{\"key\": \"value\"}'\n    result = field.bound_data(data, None)\n    self.assertEqual(result, {\"key\": \"value\"})\n\n    data = 'invalid json'\n    result = field.bound_data(data, None)\n    self.assertIsInstance(result, InvalidJSONInput)\n", "def test_jsonfield_encoder_decoder(self):\n    class TestForm(Form):\n        json_field = JSONField(encoder=DjangoJSONEncoder, decoder=json.JSONDecoder)\n\n    test_data = {\"key1\": \"value1\", \"key2\": 2, \"key3\": [\"item1\", \"item2\"]}\n    json_data = json.dumps(test_data, cls=DjangoJSONEncoder)\n\n    form = TestForm({\"json_field\": json_data})\n    self.assertTrue(form.is_valid())\n\n    cleaned_data = form.cleaned_data[\"json_field\"]\n    self.assertEqual(cleaned_data, test_data)\n", "    def test_json_field_to_python_with_valid_json(self):\n        field = JSONField()\n        json_data = '{\"key\": \"value\"}'\n        python_data = field.to_python(json_data)\n        self.assertEqual(python_data, json.loads(json_data))\n"], "sample_644": ["def test_check_same_line_imports(self):\n    node = astroid.extract_node(\"from module import a, a\")\n    with self.assertAddsMessages(MessageTest(\"reimported\", node=node, args=(\"a\", node.fromlineno))):\n        self.checker._check_same_line_imports(node)\n", "def test_check_wildcard_imports(self):\n    node = astroid.parse(\"from module import *\").body[0]\n    imported_module = astroid.parse(\"__all__ = ['a', 'b']\").locals\n    with self.assertAddsMessages(MessageTest('wildcard-import', args=('module',), node=node)):\n        self.checker._check_wildcard_imports(node, imported_module)\n", "def test_reimported(self) -> None:\n    code = \"\"\"\n    import os\n    import os\n    from os import path\n    from os import path\n    \"\"\"\n    with self.assertAddsMessages(\n        MessageTest(msg_id=\"reimported\", line=3, args=(\"os\", 1)),\n        MessageTest(msg_id=\"reimported\", line=5, args=(\"path\", 3)),\n    ):\n        self.checker.process_tokens(code)\n", "    def test_check_toplevel_imports_outside_module(self):\n        with self.assertAddsMessages(\n            MessageTest(msg_id=\"import-outside-toplevel\", line=4, args=(\"os\",))\n        ):\n            node = astroid.parse(\n                \"\"\"\n                    import os\n                \"\"\"\n            )\n            self.checker.visit_import(node.body[0].body[0].value)\n", "def test_reimport_shadowed(self):\n    msg_id = 'shadowed-import'\n    node = astroid.extract_node(\n        '''\n        from foo import bar\n        from foo import baz as bar\n        '''\n    )\n    with self.assertAddsMessages(MessageTest(msg_id, line=2, args=('bar', 1))):\n        self.checker.visit_importfrom(node.body[1])\n"], "sample_227": ["compilation error", "compilation error", "    def test_generic_relation_filter(self):\n        book_admin = BookAdminWithGenericRelation(self.alfred, Book, site)\n        request = self.request_factory.get('/')\n        request.user = self.alfred\n        changelist = book_admin.get_changelist_instance(request)\n        queryset = book_admin.get_queryset(request)\n        filter_spec = changelist.get_filters(request)[0][0]\n        choices = list(filter_spec.choices(changelist))\n        self.assertEqual(len(choices), 2)  # Two tags are present in the data\n        self.assertEqual(choices[0]['query_string'], '?book__tags__tag__exact=django')\n        self.assertEqual(choices[1]['query_string'], '?book__tags__tag__exact=music')\n", "    def test_employee_filter_book_admin(self):\n        request = self.request_factory.get('/')\n        request.user = self.alfred\n        ma = EmployeeFilterBookAdmin(model=Employee, admin_site=site)\n        changelist = ma.get_changelist_instance(request)\n        changelist.get_results(request)\n        self.assertEqual(len(changelist.result_list), 2)\n        self.assertEqual(changelist.result_list[0].name, 'John Blue')\n        self.assertEqual(changelist.result_list[1].name, 'Jack Red')\n", "    def test_list_filter_options(self):\n        book_admin = BookAdminWithListFilter(Book, site)\n        request = self.request_factory.get('/')\n        request.user = self.alfred\n\n        changelist = book_admin.get_changelist_instance(request)\n        filters_specs = changelist.get_filters(request)\n\n        self.assertEqual(len(filters_specs), 8)\n\n        # Test options for each list filter\n        self.assertEqual(filters_specs[0].title, 'Year')\n        self.assertEqual(filters_specs[1].title, 'Author')\n        self.assertEqual(filters_specs[2].title, 'Contributors')\n        self.assertEqual(filters_specs[3].title, 'Is best seller')\n        self.assertEqual(filters_specs[4].title, 'Date registered')\n        self.assertEqual(filters_specs[5].title, 'No')\n        self.assertEqual(filters_specs[6].title, 'Availability')\n        self.assertEqual(filters_specs[7].title, 'Employee')\n"], "sample_228": ["def test_formset_with_custom_kwarg(self):\n    CustomKwargFormSet = formset_factory(\n        CustomKwargForm, formset=BaseFormSet, form_kwargs={'custom_kwarg': 'test'}, extra=2\n    )\n    formset = CustomKwargFormSet()\n    for form in formset:\n        self.assertEqual(form.custom_kwarg, 'test')\n", "def test_custom_form_kwargs_passed_to_forms(self):\n    CustomKwargFormSet = formset_factory(CustomKwargForm, extra=2, form_kwargs={'custom_kwarg': 'test'})\n    formset = CustomKwargFormSet()\n    for form in formset:\n        self.assertEqual(form.custom_kwarg, 'test')\n", "def test_custom_form_kwargs(self):\n    CustomKwargFormSet = formset_factory(CustomKwargForm, formset=BaseFormSet, form_kwargs={'custom_kwarg': 'value'})\n    formset = CustomKwargFormSet()\n    for form in formset.forms:\n        self.assertEqual(form.custom_kwarg, 'value')\n", "    def test_duplicate_drinks_not_allowed(self):\n        form_data = {\n            'form-TOTAL_FORMS': '4',\n            'form-INITIAL_FORMS': '0',\n            'form-0-name': 'Coffee',\n            'form-1-name': 'Tea',\n            'form-2-name': 'Coffee',  # This is a duplicate\n            'form-3-name': 'Milk',\n        }\n        formset = FavoriteDrinksFormSet(form_data)\n        self.assertFalse(formset.is_valid())\n        self.assertEqual(formset.non_form_errors(), ['You may only specify a drink once.'])\n", "def test_formset_get_form_kwargs(self):\n    # Create a CustomKwargFormSet with custom_kwarg set to a specific value\n    CustomKwargFormSet = formset_factory(CustomKwargForm, extra=1, formset=BaseFormSet, form_kwargs={'custom_kwarg': 'test_value'})\n    formset = CustomKwargFormSet()\n\n    # Check that get_form_kwargs returns the correct value for custom_kwarg\n    form_kwargs = formset.get_form_kwargs(0)\n    self.assertEqual(form_kwargs['custom_kwarg'], 'test_value')\n\n    # Check that _construct_form passes custom_kwarg to the form constructor\n    form = formset._construct_form(0)\n    self.assertEqual(form.custom_kwarg, 'test_value')\n"], "sample_370": ["def test_forward_many_to_one_descriptor_get_object(self):\n    # Test getting the related object through the forward relation\n    author = Author.objects.get(name='Charlotte')\n    book = author.first_book\n    self.assertEqual(book.title, 'Poems')\n\n    # Test getting the related object when it doesn't exist\n    author = Author.objects.get(name='Emily')\n    with self.assertRaises(Author.first_book.RelatedObjectDoesNotExist):\n        author.first_book\n\n    # Test getting the related object when the value is cached\n    author = Author.objects.get(name='Jane')\n    book = author.first_book\n    self.assertEqual(book.title, 'Sense and Sensibility')\n    with self.assertNumQueries(0):\n        self.assertEqual(author.first_book, book)\n", "def test_reverse_accessor_does_not_exist(self):\n    with self.assertRaises(ObjectDoesNotExist):\n        self.author1.reader_set.get(name='Non-existent Reader')\n", "def test_forward_many_to_many_prefetch(self):\n    with CaptureQueriesContext(connection) as captured_queries:\n        authors_with_books = Author.objects.prefetch_related('book_set')\n        for author in authors_with_books:\n            # Accessing the pre-fetched books should not trigger additional queries\n            _ = author.book_set.all()\n    # Check that the number of queries executed is equal to the number of authors plus one\n    # (the initial query to get all authors)\n    self.assertEqual(len(captured_queries), Author.objects.count() + 1)\n", "def test_prefetch_related_one_to_one_field(self):\n    # Test prefetch_related_objects with a OneToOneField\n    author_with_age = AuthorWithAge.objects.create(author=self.author1, age=40)\n    with CaptureQueriesContext(connection) as ctx:\n        authors_with_age = Author.objects.all()\n        prefetch_related_objects(authors_with_age, 'authorwithage')\n\n    self.assertNumQueries(0, ctx)  # No additional queries should be made\n    for author in authors_with_age:\n        self.assertTrue(hasattr(author, 'authorwithage'))  # The related object should be cached\n", "def test_forward_many_to_many_descriptor_add(self):\n    author = Author.objects.create(name='New Author')\n    book = Book.objects.create(title='New Book')\n\n    # Add a book to the author's list of books\n    author.books.add(book)\n\n    # Verify that the book is in the author's list of books\n    self.assertIn(book, author.books.all())\n\n    # Verify that the author is in the book's list of authors\n    self.assertIn(author, book.authors.all())\n\n    # Verify that the correct through instances have been created\n    through_instance = author.books.through.objects.get(author=author, book=book)\n    self.assertEqual(through_instance.author, author)\n    self.assertEqual(through_instance.book, book)\n"], "sample_954": ["def test_manpage_writer(app, status, warning):\n    app.builder.config.man_pages = default_man_pages\n    app.builder.write(app.config.master_doc, \"manpage\")\n    assert \"manpage\" in app.builder.outdir\n", "def test_man_page_writer(app, status, warning):\n    app.builder.build_all()\n    man_pages = app.config.man_pages\n    for pagename, title, description, authors, section in man_pages:\n        man_path = app.outdir / f'{pagename}.{section}'\n        assert man_path.exists()\n        content = man_path.read_text()\n        assert title in content\n        assert description in content\n        assert authors in content\n        assert section in content\n", "def test_nested_inline_transform(app, status, warning):\n    config = Config({})\n    app.builder.config = config\n    app.builder.env.temp_data['docname'] = 'index'\n\n    document = app.env.get_doctree('index')\n    transform = NestedInlineTransform(document)\n    transform.apply()\n\n    # Add assertions here to check if the transform worked correctly\n", "def test_nested_inline_transform(app, status, warning):\n    document = nodes.document()\n    text = 'foo=<emphasis>1</emphasis>&bar=<emphasis>2</emphasis>'\n    document += nodes.literal(text=text)\n\n    transform = NestedInlineTransform(document)\n    transform.apply()\n\n    assert len(document) == 3\n    assert isinstance(document[0], nodes.literal)\n    assert document[0].astext() == 'foo='\n    assert isinstance(document[1], nodes.emphasis)\n    assert document[1].astext() == '1'\n    assert isinstance(document[2], nodes.literal)\n    assert document[2].astext() == '&bar='\n\n    document += nodes.strong('baz=<emphasis>3</emphasis>', text='baz=3')\n    transform.apply()\n\n    assert len(document) == 5\n    assert isinstance(document[3], nodes.strong)\n    assert document[3].astext() == 'baz='\n    assert isinstance(document[4], nodes.emphasis)\n    assert document[4].astext() == '3'\n", "def test_manual_page_writer_nested_inline_transform(app):\n    from sphinx.writers.manpage import NestedInlineTransform\n    from docutils.parsers.rst import Parser\n    from docutils import nodes\n\n    source = \"**foo=<emphasis>1</emphasis> &bar=<emphasis>2</emphasis>**\"\n    document = Parser().parse(source)\n\n    transform = NestedInlineTransform(document)\n    transform.apply()\n\n    # Check the transformed document structure\n    assert len(document.children) == 1  # Only one top-level element\n    assert isinstance(document.children[0], nodes.paragraph)\n    assert len(document.children[0].children) == 5  # 5 child nodes after transformation\n    assert [isinstance(node, node_type) for node, node_type in zip(document.children[0].children,\n                                                                    [nodes.strong, nodes.Text, nodes.emphasis,\n                                                                     nodes.strong, nodes.emphasis])]\n"], "sample_340": ["    def test_get_migration_by_prefix(self):\n        loader = MigrationLoader(connection)\n        loader.build_graph()\n\n        # Test getting a migration by prefix\n        migration = loader.get_migration_by_prefix('app_label', 'prefix')\n        self.assertEqual(migration.__class__.__name__, 'Migration')\n\n        # Test getting a migration by prefix with multiple matches\n        with self.assertRaises(AmbiguityError):\n            loader.get_migration_by_prefix('app_label', 'ambi')\n\n        # Test getting a migration by prefix with no matches\n        with self.assertRaises(KeyError):\n            loader.get_migration_by_prefix('app_label', 'nonexistent')\n", "    def test_get_migration_by_prefix(self):\n        \"\"\"\n        Test the get_migration_by_prefix method.\n        \"\"\"\n        loader = MigrationLoader(connection)\n        # Test with a valid prefix\n        migration = loader.get_migration_by_prefix('test_app', '0001_')\n        self.assertEqual(migration.name, '0001_initial')\n\n        # Test with a prefix that matches multiple migrations\n        with self.assertRaises(AmbiguityError):\n            loader.get_migration_by_prefix('test_app', '00')\n\n        # Test with a prefix that matches no migrations\n        with self.assertRaises(KeyError):\n            loader.get_migration_by_prefix('test_app', '0005_')\n", "    def test_detect_conflicts(self):\n        \"\"\"\n        Test the detect_conflicts method.\n        \"\"\"\n        loader = MigrationLoader(None)\n        loader.graph = {\n            'nodes': {\n                ('app1', '0001_initial'): {'parents': []},\n                ('app1', '0002_add_field'): {'parents': [('app1', '0001_initial')]},\n                ('app2', '0001_initial'): {'parents': []},\n                ('app2', '0002_add_field'): {'parents': [('app2', '0001_initial')]},\n                ('app3', '0001_initial'): {'parents': []},\n                ('app3', '0002_add_field'): {'parents': [('app3', '0001_initial')]},\n                ('app3', '0003_add_another_field'): {'parents': [('app3', '0002_add_field')]},\n            },\n            'leaf_nodes': lambda: [('app1', '0002_add_field'), ('app2', '0002_add_field'), ('app3', '0003_add_another_field')],\n        }\n        conflicts = loader.detect_conflicts()\n        self.assertEqual(conflicts, {'app3': ['0002_add_field', '0003_add_another_field']})\n", "    def test_get_migration_by_prefix(self):\n        loader = MigrationLoader(connection)\n        loader.build_graph()\n\n        # Test getting a migration by prefix\n        migration = loader.get_migration_by_prefix('auth', '0001')\n        self.assertEqual(migration.__class__.__name__, 'Migration')\n        self.assertEqual(migration.name, '0001_initial')\n\n        # Test getting a migration by prefix that doesn't exist\n        with self.assertRaises(KeyError):\n            loader.get_migration_by_prefix('auth', '9999')\n\n        # Test getting a migration by prefix that has multiple matches\n        with self.assertRaises(AmbiguityError):\n            loader.get_migration_by_prefix('auth', '00')\n", "def test_build_graph_with_no_migrations(self):\n    with override_settings(MIGRATION_MODULES={'test_app': None}):\n        loader = MigrationLoader(connection, ignore_no_migrations=True)\n        loader.build_graph()\n        self.assertEqual(set(loader.disk_migrations.keys()), set())\n        self.assertEqual(set(loader.applied_migrations.keys()), set())\n        self.assertEqual(set(loader.graph.nodes.keys()), set())\n"], "sample_419": ["def test_custom_kwarg_formset(self):\n    CustomKwargFormSet = formset_factory(\n        CustomKwargForm, formset=BaseFormSet, extra=2, form_kwargs={'custom_kwarg': 'test'}\n    )\n    formset = CustomKwargFormSet()\n    for form in formset:\n        self.assertEqual(form.custom_kwarg, 'test')\n", "def test_get_form_kwargs(self):\n    formset = self.make_choiceformset()\n    kwargs = formset.get_form_kwargs(0)\n    self.assertEqual(kwargs, {})\n    formset.form_kwargs = {\"custom_kwarg\": \"test\"}\n    kwargs = formset.get_form_kwargs(0)\n    self.assertEqual(kwargs, {\"custom_kwarg\": \"test\"})\n", "def test_custom_form_kwargs(self):\n    CustomKwargFormSet = formset_factory(CustomKwargForm, form_kwargs={'custom_kwarg': 'foo'})\n    formset = CustomKwargFormSet()\n    for form in formset:\n        self.assertEqual(form.custom_kwarg, 'foo')\n", "def test_custom_kwarg_formset(self):\n    formset = formset_factory(CustomKwargForm, formset=BaseFormSet, extra=2)(\n        form_kwargs={'custom_kwarg': 'test'}\n    )\n    for form in formset:\n        self.assertEqual(form.custom_kwarg, 'test')\n", "def test_management_form_with_missing_fields(self):\n    formset = self.make_choiceformset(\n        formset_data=[(\"choice1\", 1), (\"choice2\", 2)],\n        total_forms=3,\n        initial_forms=0,\n        max_num_forms=5,\n        min_num_forms=1,\n    )\n    formset.management_form.data = {}\n    formset.management_form.full_clean()\n    self.assertEqual(formset.management_form.cleaned_data[TOTAL_FORM_COUNT], 0)\n    self.assertEqual(formset.management_form.cleaned_data[INITIAL_FORM_COUNT], 0)\n"], "sample_963": ["def test_restify():\n    assert restify(MyClass1) == ':py:class:`test_util_typing.MyClass1`'\n    assert restify(MyClass2) == ':py:class:`<MyClass2>`'\n    assert restify(MyInt) == ':py:class:`test_util_typing.MyInt`'\n    assert restify(MyList) == ':py:class:`test_util_typing.MyList`'\n    assert restify(BrokenType) == \"<class 'test_util_typing.BrokenType'>\"\n    assert restify(Union[int, None]) == 'Optional[int]'\n    assert restify(Union[int, str]) == 'int | str'\n    assert restify(Struct('h')) == ':py:class:`struct.Struct`'\n    assert restify(TracebackType) == ':py:class:`types.TracebackType`'\n", "def test_restify(annotation, expected):\n    assert restify(annotation) == expected\n", "def test_restify():\n    assert restify(MyClass1) == ':py:class:`test_util_typing.MyClass1`'\n    assert restify(MyClass2) == ':py:class:`test_util_typing.<MyClass2>`'\n    assert restify(MyInt) == ':py:class:`test_util_typing.MyInt`'\n    assert restify(MyList) == ':py:class:`test_util_typing.MyList`\\\\ [T]'\n    assert restify(Struct) == ':py:class:`struct.Struct`'\n    assert restify(TracebackType) == ':py:class:`types.TracebackType`'\n    assert restify(BrokenType) == \"int\"\n    assert restify(None) == ':py:obj:`None`'\n    assert restify(Ellipsis) == '...'\n    assert restify('str') == 'str'\n\n    if sys.version_info >= (3, 7):  # py37+\n        assert restify(Union[int, str]) == ':py:obj:`~typing.Union`\\\\ [int, str]'\n        assert restify(Optional[int]) == ':py:obj:`~typing.Optional`\\\\ [int]'\n    else:\n        assert restify(Union[int, str]) == ':py:obj:`~typing.Union`\\\\ [int, str]'\n        assert restify(Optional[int]) == ':py:obj:`~typing.Optional`\\\\ [int]'\n\n    # Additional test for _restify_py36\n    if sys.version_info < (3, 7):  # py36\n        assert restify(Tuple[int, str]) == ':py:class:`typing.Tuple`\\\\ [int, str]'\n        assert restify(Dict[str, int]) == ':py:class:`typing.Dict`\\\\ [str, int]'\n        assert restify(Callable[[int], str]) == ':py:class:`typing.Callable`\\\\ [[int], str]'\n", "def test_restify(cls, expected):\n    assert restify(cls) == expected\n", "def test_restify(annotation, expected):\n    assert restify(annotation) == expected\n"], "sample_1090": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_268": ["def test_import_error_file(self):\n    # Test that a file that raises an import error is added to _error_files\n    filename = self.temporary_file('broken_module.py')\n    filename.write_text('invalid syntax')\n    with self.assertRaises(SyntaxError):\n        self.import_and_cleanup(filename.stem)\n    self.assertIn(str(filename), autoreload._error_files)\n", "def test_import_from_zip(self):\n    filename = self.temporary_file('temp_module.py')\n    filename.write_text(\"print('Hello, world!')\")\n    zip_filename = self.temporary_file('temp_module.zip')\n    with zipfile.ZipFile(zip_filename, 'w') as zipf:\n        zipf.write(filename, arcname='temp_module.py')\n    sys.path.append(str(zip_filename))\n    self.addCleanup(sys.path.remove, str(zip_filename))\n    self.import_and_cleanup('temp_module')\n    self.assertFileFound(zip_filename)\n", "    def test_extra_files(self):\n        filename = self.temporary_file('extra_file.py')\n        with filename.open('w') as f:\n            f.write('# Test file')\n        self.assertFileNotFound(filename)\n        self.clear_autoreload_caches()\n        autoreload._error_files.append(str(filename))\n        self.assertFileFound(filename)\n        autoreload._error_files.clear()\n", "    def test_module_loading_error(self):\n        # Test the case when there is an error while importing a module.\n        # Create a temporary file with a syntax error.\n        filename = self.temporary_file('error_module.py')\n        with filename.open('w') as f:\n            f.write('syntax error')\n        # Ensure the module is not in sys.modules.\n        self.assertNotIn('error_module', sys.modules)\n        # Try importing the module.\n        try:\n            self.import_and_cleanup('error_module')\n        except SyntaxError:\n            pass\n        # Check that the file is in _error_files.\n        self.assertIn(str(filename), autoreload._error_files)\n        # Check that the file is returned by iter_all_python_module_files.\n        self.assertFileFound(filename)\n", "    def test_weakref_modules(self):\n        # Create a weak reference to a temporary file\n        temp_file = self.temporary_file(\"weakref_module.py\")\n        temp_file.write_text(\"test_weakref = 'test'\")\n        py_compile.compile(temp_file)\n\n        # Import the module and add a weak reference\n        self.import_and_cleanup(\"weakref_module\")\n        weakref.ref(sys.modules[\"weakref_module\"])\n\n        # The module is still loaded even if it's only a weak reference\n        self.assertFileFound(temp_file)\n\n        # The module is not loaded after it's removed from sys.modules\n        del sys.modules[\"weakref_module\"]\n        self.assertFileNotFound(temp_file)\n"], "sample_63": ["def test_template_context_processors(self):\n    self.engine.context_processors = ['django.template.context_processors.request']\n    context = Context()\n    context_processors = self.engine.template_context_processors\n    self.assertEqual(len(context_processors), len(self.engine.context_processors) + 1)\n    for processor in context_processors:\n        processor(context)\n    self.assertIn('request', context)\n", "def test_template_rendering(self):\n    template = self.engine.get_template('template.html')\n    context = Context({'variable': 'value'})\n    rendered = template.render(context)\n    self.assertEqual(rendered, 'Expected output')\n", "def test_render_template_context_processors(self):\n        return {\"custom_var\": \"custom_value\"}\n\n    self.engine = Engine(\n        dirs=[TEMPLATE_DIR],\n        context_processors=['tests.test_engine.custom_processor']\n    )\n    template = self.engine.get_template('template_with_context_processor.html')\n    rendered = template.render(Context())\n    self.assertIn('custom_value', rendered)\n", "    def test_engine_init(self):\n        # Test the initialization of the Engine class with default arguments\n        engine = Engine()\n        self.assertEqual(engine.dirs, [])\n        self.assertEqual(engine.app_dirs, False)\n        self.assertEqual(engine.context_processors, [])\n        self.assertEqual(engine.debug, False)\n        self.assertEqual(engine.loaders, ['django.template.loaders.filesystem.Loader'])\n        self.assertEqual(engine.string_if_invalid, '')\n        self.assertEqual(engine.file_charset, 'utf-8')\n        self.assertEqual(engine.libraries, {})\n        self.assertEqual(engine.builtins, Engine.default_builtins)\n        self.assertEqual(engine.autoescape, True)\n\n        # Test the initialization of the Engine class with custom arguments\n        custom_args = {\n            'dirs': [TEMPLATE_DIR, OTHER_DIR],\n            'app_dirs': True,\n            'context_processors': ['myapp.context_processors.custom_processor'],\n            'debug': True,\n            'loaders': ['myapp.loaders.CustomLoader'],\n            'string_if_invalid': 'Invalid template',\n            'file_charset': 'latin1',\n            'libraries': {'custom_tags': 'myapp.templatetags.custom_tags'},\n            'builtins': ['myapp.templatetags.extra_builtins'],\n            'autoescape': False,\n        }\n        engine = Engine(**custom_args)\n        self.assertEqual(engine.dirs, custom_args['dirs'])\n        self.assertEqual(engine.app_dirs, custom_args['app_dirs'])\n        self.assertEqual(engine.context_processors, custom_args['context_processors'])\n        self.assertEqual(engine.debug, custom_args['debug'])\n        self.assertEqual(engine.loaders, [custom_args['loaders']])\n        self.assertEqual(engine.string_if_invalid, custom_args['string_if_invalid'])\n        self.assertEqual(engine", "def test_render_to_string_with_context(self):\n    template_name = 'test_template.html'\n    context = {'key': 'value'}\n    rendered = self.engine.render_to_string(template_name, context)\n    self.assertIn('value', rendered)\n"], "sample_1039": ["def test_print_Integral():\n    expr = Integral(sin(x), (x, 0, pi))\n    result = mathml(expr)\n    expected = '<apply><int/><bvar><ci>x</ci></bvar><lowlimit><cn>0</cn></lowlimit><uplimit><pi/></uplimit><apply><sin/><ci>x</ci></apply></apply>'\n    assert result == expected\n", "def test_print_MatrixSymbol():\n    M = MatrixSymbol('M', 3, 3)\n    xml = mp._print(M)\n    assert str(xml) == '<ci>M</ci>'\n    xml = mpp._print(M)\n    assert str(xml) == '<mi>M</mi>'\n", "def test_mathml_relational():\n    expr = Eq(x, y)\n    result = mathml(expr)\n    assert result == \"<apply><eq/><ci>x</ci><ci>y</ci></apply>\"\n", "def test_mathml_Derivative():\n    expr = diff(sin(x), x)\n    assert mathml(expr) == '<apply><diff/><bvar><ci>x</ci></bvar><sin/><ci>x</ci></apply>'\n    assert mathml(expr, printer='presentation') == '<mrow><mo>&dd;</mo><mi>sin</mi><mfenced><mi>x</mi></mfenced><mfrac><mrow><mo>&dd;</mo><mi>x</mi></mrow><mi>x</mi></mfrac></mrow>'\n", "def test_mathml_printer():\n    expr = diff(sin(x), x)\n    result = mp.doprint(expr)\n    expected = \"<apply><diff/><bvar><ci>x</ci></bvar><apply><sin/><ci>x</ci></apply></apply>\"\n    assert result == expected\n"], "sample_361": ["def test_urlize(self):\n    self.check_output(\n        urlize,\n        \"Check out https://www.example.com and www.another-example.com. Also, email me at test@example.com.\",\n        'Check out <a href=\"https://www.example.com\">https://www.example.com</a> and '\n        '<a href=\"http://www.another-example.com\">www.another-example.com</a>. '\n        'Also, email me at <a href=\"mailto:test@example.com\">test@example.com</a>.'\n    )\n", "def test_urlize_with_nofollow(self):\n    text = \"Visit https://www.example.com for more info.\"\n    expected_output = 'Visit <a href=\"https://www.example.com\" rel=\"nofollow\">https://www.example.com</a> for more info.'\n    self.check_output(lambda t: urlize(t, nofollow=True), text, expected_output)\n", "    def test_urlize(self):\n        text = \"Check out www.example.com and https://test.com!\"\n        output = 'Check out <a href=\"http://www.example.com\">www.example.com</a> and <a href=\"https://test.com\">https://test.com</a>!'\n        self.check_output(urlize, text, output)\n", "def test_urlize_complex_cases(self):\n    # Test URLize with complex cases that include wrapping punctuation, trailing punctuation, and emails.\n    tests = [\n        ('(www.example.com)', '<a href=\"http://www.example.com\">www.example.com</a>'),\n        ('(www.example.com,)', '<a href=\"http://www.example.com\">www.example.com</a>,'),\n        ('www.example.com.', '<a href=\"http://www.example.com\">www.example.com</a>.'),\n        ('test@example.com', '<a href=\"mailto:test@example.com\">test@example.com</a>'),\n        ('(test@example.com)', '<a href=\"mailto:test@example.com\">test@example.com</a>'),\n        ('(test@example.com,)', '<a href=\"mailto:test@example.com\">test@example.com</a>,'),\n        ('test@example.com.', '<a href=\"mailto:test@example.com\">test@example.com</a>.'),\n    ]\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.check_output(urlize, value, output)\n", "def test_urlize(self):\n    self.check_output(urlize, 'http://example.com', '<a href=\"http://example.com\">http://example.com</a>')\n    self.check_output(urlize, 'https://example.com', '<a href=\"https://example.com\">https://example.com</a>')\n    self.check_output(urlize, 'www.example.com', '<a href=\"http://www.example.com\">www.example.com</a>')\n    self.check_output(urlize, 'example.com', 'example.com')\n    self.check_output(urlize, 'example@test.com', '<a href=\"mailto:example@test.com\">example@test.com</a>')\n    self.check_output(urlize, 'text http://example.com more text', 'text <a href=\"http://example.com\">http://example.com</a> more text')\n"], "sample_240": ["    def test_check_token_valid_token(self):\n        user = User.objects.create_user(username=\"testuser\", email=\"testuser@test.com\", password=\"testpassword\")\n        token_generator = MockedPasswordResetTokenGenerator(datetime.now())\n        token = token_generator.make_token(user)\n        self.assertTrue(token_generator.check_token(user, token))\n", "    def test_check_token(self):\n        user = User.objects.create_user(username='testuser', email='testuser@example.com', password='testpassword')\n        token_generator = MockedPasswordResetTokenGenerator(now=datetime.now())\n        token = token_generator.make_token(user)\n\n        self.assertTrue(token_generator.check_token(user, token))\n\n        # Test with invalid token\n        self.assertFalse(token_generator.check_token(user, 'invalidtoken'))\n\n        # Test with expired token\n        expired_token_generator = MockedPasswordResetTokenGenerator(now=datetime.now() + timedelta(days=settings.PASSWORD_RESET_TIMEOUT + 1))\n        expired_token = expired_token_generator.make_token(user)\n        self.assertFalse(token_generator.check_token(user, expired_token))\n", "    def test_check_token_validity(self):\n        user = User.objects.create_user(username='testuser', password='12345')\n        now = datetime.now()\n        token_generator = MockedPasswordResetTokenGenerator(now)\n        token = token_generator.make_token(user)\n\n        # Token should be valid immediately after creation\n        self.assertTrue(token_generator.check_token(user, token))\n\n        # Token should be invalid after timeout period\n        future = now + timedelta(seconds=settings.PASSWORD_RESET_TIMEOUT + 1)\n        token_generator = MockedPasswordResetTokenGenerator(future)\n        self.assertFalse(token_generator.check_token(user, token))\n\n        # Token should be invalid if user changes their password\n        user.set_password('newpassword')\n        user.save()\n        token_generator = MockedPasswordResetTokenGenerator(now)\n        self.assertFalse(token_generator.check_token(user, token))\n\n        # Token should be invalid if user's last login changes\n        user = User.objects.create_user(username='testuser2', password='12345')\n        token = token_generator.make_token(user)\n        user.last_login = now\n        user.save()\n        token_generator = MockedPasswordResetTokenGenerator(now)\n        self.assertFalse(token_generator.check_token(user, token))\n\n        # Token should be invalid if user's email changes\n        user = User.objects.create_user(username='testuser3', password='12345')\n        setattr(user, CustomEmailField.EMAIL_FIELD, 'test@example.com')\n        user.save()\n        token = token_generator.make_token(user)\n        setattr(user, CustomEmailField.EMAIL_FIELD, 'new@example.com')\n        user.save()\n        token_generator = MockedPasswordResetTokenGenerator(now)\n        self.assertFalse(token_generator.check_token(user, token))\n", "    def test_check_token_expiry(self):\n        user = User.objects.create_user(username='testuser', password='12345')\n        token_generator = MockedPasswordResetTokenGenerator(now=datetime.now())\n        token = token_generator.make_token(user)\n        self.assertTrue(token_generator.check_token(user, token))\n\n        # Fast-forward to the token expiry time\n        expiry_time = datetime.now() + timedelta(seconds=settings.PASSWORD_RESET_TIMEOUT + 1)\n        token_generator = MockedPasswordResetTokenGenerator(now=expiry_time)\n        self.assertFalse(token_generator.check_token(user, token))\n", "    def test_check_token_expired(self):\n        user = User.objects.create_user(username='testuser', password='12345')\n        now = datetime.now()\n        generator = MockedPasswordResetTokenGenerator(now)\n        token = generator.make_token(user)\n\n        # Timeout is set to 1 day for this test\n        settings.PASSWORD_RESET_TIMEOUT = 86400\n\n        # Simulate the token has been expired\n        expired_date = now + timedelta(seconds=settings.PASSWORD_RESET_TIMEOUT + 1)\n        generator._now_val = expired_date\n\n        self.assertFalse(generator.check_token(user, token))\n"], "sample_651": ["def test_warnings_recorder_pop(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n        import warnings\n\n            warnings.warn(\"first warning\", UserWarning)\n            warnings.warn(\"second warning\", DeprecationWarning)\n\n            pop_warning = recwarn.pop(UserWarning)\n            assert str(pop_warning.message) == \"first warning\"\n            assert len(recwarn) == 1\n\n            pop_warning = recwarn.pop(DeprecationWarning)\n            assert str(pop_warning.message) == \"second warning\"\n            assert len(recwarn) == 0\n\n            with pytest.raises(AssertionError):\n                recwarn.pop(UserWarning)\n        \"\"\"\n    )\n\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)\n", "def test_warnings_recorder_pop(pytester: Pytester) -> None:\n    \"\"\"Test the pop method of the WarningsRecorder class.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n        import warnings\n\n            warnings.warn(\"Warning 1\", UserWarning)\n            warnings.warn(\"Warning 2\", DeprecationWarning)\n\n            assert len(recwarn) == 2\n\n            # Pop the UserWarning\n            popped_warning = recwarn.pop(UserWarning)\n            assert popped_warning.message.args[0] == \"Warning 1\"\n            assert len(recwarn) == 1\n\n            # Pop the DeprecationWarning\n            popped_warning = recwarn.pop(DeprecationWarning)\n            assert popped_warning.message.args[0] == \"Warning 2\"\n            assert len(recwarn) == 0\n\n            # Try to pop a warning of a non-existent type\n            with pytest.raises(AssertionError):\n                recwarn.pop(SyntaxWarning)\n        \"\"\"\n    )\n\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)\n", "def test_warnings_recorder_pop(pytester: Pytester):\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n        import warnings\n\n            with pytest.warns(UserWarning) as record:\n                warnings.warn(\"foo\", UserWarning)\n                warnings.warn(\"bar\", DeprecationWarning)\n\n            assert len(record) == 2\n            assert str(record.pop(UserWarning).message) == \"foo\"\n            assert len(record) == 1\n            assert str(record.pop(DeprecationWarning).message) == \"bar\"\n            assert len(record) == 0\n\n            with pytest.raises(AssertionError):\n                record.pop(UserWarning)\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)\n", "def test_warnings_recorder_pop(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n        import warnings\n\n            with pytest.warns(UserWarning) as record:\n                warnings.warn(\"User Warning\", UserWarning)\n                warnings.warn(\"Deprecation Warning\", DeprecationWarning)\n            assert len(record) == 2\n            assert record.pop(UserWarning).message.args[0] == \"User Warning\"\n            assert len(record) == 1\n            assert record.pop(DeprecationWarning).message.args[0] == \"Deprecation Warning\"\n            assert len(record) == 0\n            with pytest.raises(AssertionError):\n                record.pop(RuntimeWarning)\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)\n", "def test_warnings_recorder_pop(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n        import warnings\n\n            with pytest.warns(UserWarning) as record:\n                warnings.warn(\"User Warning 1\", UserWarning)\n                warnings.warn(\"User Warning 2\", UserWarning)\n                warnings.warn(\"Deprecation Warning\", DeprecationWarning)\n\n            assert len(record) == 3\n            assert str(record.pop(UserWarning).message) == \"User Warning 1\"\n            assert str(record.pop(UserWarning).message) == \"User Warning 2\"\n            with pytest.raises(AssertionError):\n                record.pop(UserWarning)\n            assert str(record.pop(DeprecationWarning).message) == \"Deprecation Warning\"\n            with pytest.raises(AssertionError):\n                record.pop(DeprecationWarning)\n\n        if __name__ == \"__main__\":\n            pytest.main()\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)\n"], "sample_112": ["def test_prepopulated_fields_js(self):\n    # Create a mock admin form with prepopulated fields\n    class MockAdminForm:\n        class MockField:\n            name = \"mock_field\"\n            auto_id = \"id_mock_field\"\n            max_length = 100\n            allow_unicode = True\n\n        class MockDependency:\n            name = \"mock_dependency\"\n            auto_id = \"id_mock_dependency\"\n\n        prepopulated_fields = [\n            {\"field\": MockField(), \"dependencies\": [MockDependency()]}\n        ]\n\n    # Create a mock context with the admin form\n    context = {\"adminform\": MockAdminForm()}\n\n    # Call the prepopulated_fields_js function\n    prepopulated_fields_js(context)\n\n    # Check that the context was updated correctly\n    self.assertIn(\"prepopulated_fields\", context)\n    self.assertIn(\"prepopulated_fields_json\", context)\n    self.assertEqual(len(context[\"prepopulated_fields\"]), 1)\n    self.assertEqual(\n        context[\"prepopulated_fields_json\"],\n        '[{\"id\": \"#id_mock_field\", \"name\": \"mock_field\", \"dependency_ids\": [\"#id_mock_dependency\"], \"dependency_list\": [\"mock_dependency\"], \"maxLength\": 100, \"allowUnicode\": true}]'\n    )\n", "def test_prepopulated_fields_js(self):\n    \"\"\"Test the prepopulated_fields_js function.\"\"\"\n    model_admin = ArticleAdmin(Article, site)\n    request = self.request_factory.get('/admin/')\n    request.user = User.objects.get(username='admin')\n\n    article = Article.objects.create(title='Test Article', slug='test-article')\n    prepopulated_fields = [{'field': article._meta.get_field('slug'), 'dependencies': [article._meta.get_field('title')]}]\n\n    model_admin.get_prepopulated_fields = lambda request: prepopulated_fields\n\n    context = {'adminform': model_admin.get_form(request)(instance=article)}\n    context = prepopulated_fields_js(context)\n\n    self.assertEqual(context['prepopulated_fields'], prepopulated_fields)\n    self.assertEqual(context['prepopulated_fields_json'], '[{\"id\": \"#id_slug\", \"name\": \"slug\", \"dependency_ids\": [\"#id_title\"], \"dependency_list\": [\"title\"], \"maxLength\": 50, \"allowUnicode\": false}]')\n", "    def test_prepopulated_fields_js_tag(self):\n        context = {\n            'adminform': ModelAdmin(User, site).get_form(self.request)(instance=self.user),\n        }\n        prepopulated_fields = prepopulated_fields_js(context)\n        self.assertIn('prepopulated_fields', prepopulated_fields)\n        self.assertIn('prepopulated_fields_json', prepopulated_fields)\n", "    def setUp(self):\n        self.site = AdminSite()\n        self.model_admin = ModelAdmin(User, self.site)\n        self.request = self.request_factory.get('/admin/auth/user/')\n        self.request.user = User.objects.create_superuser('super', 'super@example.com', 'password')\n", "    def test_submit_row_tag(self):\n        \"\"\"Test the submit_row template tag.\"\"\"\n        request = self.request_factory.get('/admin')\n        request.user = self.superuser\n        context = {\n            'add': True,\n            'change': False,\n            'is_popup': False,\n            'save_as': False,\n            'show_save': True,\n            'show_save_and_add_another': True,\n            'show_save_and_continue': True,\n            'has_add_permission': True,\n            'has_change_permission': False,\n            'has_view_permission': True,\n            'has_editable_inline_admin_formsets': False,\n        }\n        template = '{% load admin_modify %}{% submit_row %}'\n        rendered = submit_row(context).render()\n        self.assertIn('Save', rendered)\n        self.assertIn('Save and add another', rendered)\n        self.assertIn('Save and continue editing', rendered)\n        self.assertNotIn('Save as new', rendered)\n        self.assertNotIn('Delete', rendered)\n"], "sample_900": ["def test_mlp_classifier_sparse_input():\n    # Test MLPClassifier with sparse input\n    X_sparse = csr_matrix(X_digits_binary)\n    mlp = MLPClassifier(random_state=0)\n    mlp.fit(X_sparse, y_digits_binary)\n    assert_array_equal(mlp.predict(X_sparse), mlp.predict(X_digits_binary))\n", "compilation error", "def test_mlp_classifier_early_stopping(activation):\n    X, y = X_digits_multi, y_digits_multi\n    clf = MLPClassifier(solver='sgd', activation=activation,\n                        early_stopping=True, validation_fraction=0.1,\n                        random_state=0, max_iter=1000, verbose=0)\n    clf.fit(X, y)\n\n    # Check if early stopping worked\n    assert clf.n_iter_ < 1000\n    assert len(clf.loss_curve_) <= 1000\n    assert len(clf.validation_scores_) == clf.n_iter_\n\n    # Check if best_validation_score_ is the maximum validation score\n    assert clf.best_validation_score_ == max(clf.validation_scores_)\n", "def test_multilayer_perceptron_regressor_lbfgs(activation):\n    mlp = MLPRegressor(hidden_layer_sizes=(10,), activation=activation,\n                       solver='lbfgs', max_iter=200, random_state=1)\n    mlp.fit(Xboston, yboston)\n    y_pred = mlp.predict(Xboston)\n    assert y_pred.shape == yboston.shape\n    assert np.issubdtype(y_pred.dtype, np.floating)\n", "def test_mlp_classifier_with_different_activations(activation):\n    X, y = X_digits_multi, y_digits_multi\n    mlp = MLPClassifier(hidden_layer_sizes=(10,), activation=activation, max_iter=10, random_state=0)\n    mlp.fit(X, y)\n    assert mlp.n_iter_ == 10\n    assert mlp.loss_ < mlp.init_loss_\n    assert mlp.predict(X).shape == (200,)\n"], "sample_533": ["def test_contour_filled_with_extend():\n    # Test contourf with extend option\n    x = np.arange(-5, 5, 0.1)\n    y = np.arange(-5, 5, 0.1)\n    X, Y = np.meshgrid(x, y)\n    Z = np.sin(X) * np.cos(Y)\n\n    levels = np.linspace(-1, 1, 10)\n\n    fig, ax = plt.subplots()\n    cs = ax.contourf(X, Y, Z, levels=levels, extend='both')\n\n    assert len(cs.collections) == len(levels) - 1\n    assert cs.extend == 'both'\n    assert cs.norm.clip is False\n\n    plt.close(fig)\n", "def test_contour_with_extent():\n    x = np.linspace(-1, 1, 100)\n    y = np.linspace(-1, 1, 100)\n    X, Y = np.meshgrid(x, y)\n    Z = np.sin(X) + np.cos(Y)\n\n    fig, ax = plt.subplots()\n    cs = ax.contour(X, Y, Z, levels=10, extent=[-2, 2, -2, 2])\n\n    # Assert that the contour levels are within the specified extent\n    assert np.min(cs.levels) >= -2\n    assert np.max(cs.levels) <= 2\n\n    # Assert that the contour lines are within the specified extent\n    for collection in cs.collections:\n        paths = collection.get_paths()\n        for path in paths:\n            verts = path.vertices\n            assert np.min(verts[:, 0]) >= -2\n            assert np.max(verts[:, 0]) <= 2\n            assert np.min(verts[:, 1]) >= -2\n            assert np.max(verts[:, 1]) <= 2\n\n    plt.close(fig)\n", "def test_contour_quadcontourset_clabel():\n    X, Y = np.meshgrid(np.arange(5), np.arange(5))\n    Z = X + Y\n\n    fig, ax = plt.subplots()\n    cs = ax.contour(X, Y, Z, levels=[2, 4, 6])\n    labels = cs.clabel(inline=True, fontsize=10)\n\n    assert len(labels) == 3\n    assert labels[0].get_text() == '2'\n    assert labels[1].get_text() == '4'\n    assert labels[2].get_text() == '6'\n\n    plt.close(fig)\n", "def test_contour_zmin_zmax_consistency():\n    x = np.linspace(0, 10, 101)\n    y = np.linspace(0, 10, 101)\n    X, Y = np.meshgrid(x, y)\n    Z = X**2 + Y**2\n\n    levels = np.array([10, 20, 30, 40, 50])\n    contour_set = QuadContourSet(plt.gca(), levels, (X, Y, Z))\n\n    assert contour_set.zmin == np.min(Z)\n    assert contour_set.zmax == np.max(Z)\n", "def test_contour_extend():\n    x = y = np.arange(-5.0, 5.0, 0.25)\n    X, Y = np.meshgrid(x, y)\n    Z = np.sin(X) + np.cos(Y)\n\n    fig, ax = plt.subplots()\n    CS = ax.contourf(X, Y, Z, levels=[-1, -0.5, 0, 0.5, 1], extend='both')\n    fig.colorbar(CS)\n"], "sample_578": ["def test_bar_color_resolution(self):\n    data = pd.DataFrame({\n        \"x\": [1, 2, 3],\n        \"y\": [4, 5, 6],\n        \"color\": [\"red\", \"green\", \"blue\"]\n    })\n    bars = self.plot_bars(data, {}, {\"hue\": \"color\"})\n    assert_array_equal(bars[0].get_facecolor(), to_rgba(\"red\"))\n    assert_array_equal(bars[1].get_facecolor(), to_rgba(\"green\"))\n    assert_array_equal(bars[2].get_facecolor(), to_rgba(\"blue\"))\n", "def test_bar_colors(self):\n    data = pd.DataFrame({'x': ['A', 'B', 'C'], 'y': [1, 2, 3], 'color': ['red', 'green', 'blue']})\n    bars = self.plot_bars(variables={'data': data, 'x': 'x', 'y': 'y'},\n                          mark_kws={'color': 'color'},\n                          layer_kws={'data': data})\n\n    assert bars[0].get_facecolor() == to_rgba('red')\n    assert bars[1].get_facecolor() == to_rgba('green')\n    assert bars[2].get_facecolor() == to_rgba('blue')\n", "def test_bar_color(self):\n    variables = {\"x\": [0, 1, 2], \"y\": [3, 4, 5]}\n    mark_kws = {\"color\": \"red\"}\n    layer_kws = {}\n\n    bars = self.plot_bars(variables, mark_kws, layer_kws)\n\n    for bar in bars:\n        assert_array_equal(bar.get_facecolor(), to_rgba(\"red\"))\n", "def test_bars_with_different_orientation(self):\n    variables = {\"data\": pd.DataFrame({\"x\": [1, 2, 3], \"y\": [4, 5, 6]})}\n    mark_kws = {\"orient\": \"y\"}\n    layer_kws = {\"vars\": {\"orient\": \"y\", \"x\": \"y\", \"y\": \"x\"}}\n    bars = self.plot_bars(variables, mark_kws, layer_kws)\n    self.check_bar(bars[0], 4, 0, 1, 1)\n    self.check_bar(bars[1], 5, 1, 1, 1)\n    self.check_bar(bars[2], 6, 2, 1, 1)\n", "    def test_bar_colors(self):\n        variables = {\"x\": [1, 2, 3], \"y\": [4, 5, 6]}\n        mark_kws = {\"color\": [\"red\", \"green\", \"blue\"]}\n        layer_kws = {}\n\n        bars = self.plot_bars(variables, mark_kws, layer_kws)\n        assert_array_equal(\n            [to_rgba(bar.get_facecolor()) for bar in bars],\n            to_rgba_array(mark_kws[\"color\"])\n        )\n"], "sample_126": ["def test_generate_altered_managers_with_arguments(self):\n    questioner = MigrationQuestioner()\n    questioner.ask_not_null_alteration = mock.Mock(return_value=models.NOT_PROVIDED)\n\n    changes = self.get_changes(\n        [self.other_pony],\n        [self.other_pony_food],\n        questioner,\n    )\n\n    self.assertNumberMigrations(changes, 'otherapp', 1)\n    self.assertOperationTypes(changes, 'otherapp', 0, ['AlterModelManagers'])\n    self.assertOperationAttributes(changes, 'otherapp', 0, 0, name='Pony', managers=[\n        ('food_qs', FoodQuerySet.as_manager()),\n        ('food_mgr', FoodManager('a', 'b')),\n        ('food_mgr_kwargs', FoodManager('x', 'y', 3, 4)),\n    ])\n", "def test_generate_altered_db_table(self):\n    changes = self.get_changes([self.author_with_db_table_options], [self.author_with_new_db_table_options])\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterModelTable\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, table=\"author_two\")\n", "def test_generate_altered_managers_with_args(self):\n    before_states = [self.author_with_biography_non_blank]\n    after_states = [self.other_pony_food]\n\n    changes = self.get_changes(before_states, after_states)\n\n    self.assertNumberMigrations(changes, 'otherapp', 1)\n    self.assertOperationTypes(changes, 'otherapp', 0, ['AlterModelManagers'])\n    self.assertOperationAttributes(changes, 'otherapp', 0, 0, name='Pony', managers=['food_qs', 'food_mgr', 'food_mgr_kwargs'])\n", "def test_generate_altered_order_with_respect_to(self):\n    changes = self.get_changes([self.author_with_book], [self.author_with_book_order_wrt])\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterOrderWithRespectTo\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"author\", order_with_respect_to=\"book\")\n", "def test_generate_altered_fields_deconstructible_default(self):\n    \"\"\"\n    Test that deconstructible objects are compared by deconstructed\n    representation and not by object equality.\n    \"\"\"\n    before_states = [self.author_name_deconstructible_1, self.author_name_deconstructible_2]\n    after_states = [self.author_name_deconstructible_3, self.author_name_deconstructible_4]\n    changes = self.get_changes(before_states, after_states)\n\n    self.assertOperationTypes(changes, 'testapp', 0, ['AlterField'])\n    self.assertOperationFieldAttributes(changes, 'testapp', 0, 0, default=models.IntegerField())\n"], "sample_262": ["    def test_lazy_object_init(self):\n        obj = SimpleLazyObject(lambda: 'test')\n        self.assertEqual(obj._wrapped, empty)\n", "def test_lazy_object_with_callable(self):\n        return \"Test Object\"\n\n    lazy_obj = SimpleLazyObject(create_obj)\n    self.assertEqual(lazy_obj._wrapped, empty)\n    self.assertEqual(str(lazy_obj), \"Test Object\")\n    self.assertIsNot(lazy_obj._wrapped, empty)\n\n    # Test __copy__ and __deepcopy__\n    copied_obj = copy.copy(lazy_obj)\n    self.assertIs(copied_obj._wrapped, lazy_obj._wrapped)\n    deep_copied_obj = copy.deepcopy(lazy_obj)\n    self.assertEqual(deep_copied_obj._wrapped, lazy_obj._wrapped)\n    self.assertIsNot(deep_copied_obj._wrapped, lazy_obj._wrapped)\n", "def test_lazy_object(self):\n    func = mock.Mock(return_value='test')\n    lazy_obj = SimpleLazyObject(func)\n    self.assertEqual(str(lazy_obj), 'test')\n    func.assert_called_once()\n", "    def test_lazy_object(self):\n            return 'test'\n\n        lazy_obj = SimpleLazyObject(setup_func)\n        self.assertEqual(lazy_obj._wrapped, empty)\n        self.assertEqual(str(lazy_obj), 'test')\n        self.assertEqual(lazy_obj._wrapped, 'test')\n        lazy_obj_copy = copy(lazy_obj)\n        self.assertEqual(lazy_obj_copy._wrapped, 'test')\n        lazy_obj_deepcopy = deepcopy(lazy_obj)\n        self.assertEqual(lazy_obj_deepcopy._wrapped, 'test')\n", "def test_lazy_object(self):\n        return 'lazy object'\n\n    lazy_obj = SimpleLazyObject(setup_func)\n    self.assertEqual(lazy_obj._wrapped, empty)\n    self.assertEqual(str(lazy_obj), 'lazy object')\n    self.assertNotEqual(lazy_obj._wrapped, empty)\n"], "sample_148": ["def test_nested_objects(self):\n    self._connect(1, 0)\n    self._connect(2, 0)\n    self._connect(3, 1)\n    self._connect(4, 1)\n    self._collect(0)\n    self._check([0, [1, [3, 4], 2]])\n", "def test_collect_with_related_fields(self):\n    # Create a parent-child relationship between the first two objects\n    self._connect(0, 1)\n\n    # Collect the first object and its related fields\n    self._collect(0)\n\n    # Check if the parent-child relationship is correctly represented in the nested structure\n    self._check([1, [0]])\n", "    def test_collect_nested_objects(self):\n        self._connect(1, 0)\n        self._connect(2, 1)\n        self._connect(3, 2)\n        self._collect(0)\n        self._check([0, [1, [2, [3]]]])\n", "    def test_multiple_collect_calls(self):\n        self._connect(1, 0)\n        self._connect(2, 0)\n        self._collect(1)\n        self._collect(2)\n        self._check([0, [1, 2]])\n", "def test_related_objects_with_select_related(self):\n    # Create some objects with relationships\n    car1 = Car.objects.create(name=\"Car1\")\n    car2 = Car.objects.create(name=\"Car2\")\n    event1 = Event.objects.create(name=\"Event1\", location=car1)\n    event2 = Event.objects.create(name=\"Event2\", location=car2)\n\n    # Collect related objects\n    self.n.collect([event1, event2], source=None, source_attr=None)\n\n    # Verify that related objects are fetched with select_related\n    related_fields = [\n        f for f in Event._meta.get_fields() if f.is_relation and f.name == \"location\"\n    ]\n    related_objects = self.n.related_objects(Car, related_fields, [event1, event2])\n    self.assertEqual(related_objects.count(), 2)\n    self.assertIsNotNone(related_objects[0].location)\n    self.assertIsNotNone(related_objects[1].location)\n"], "sample_722": ["def test_k_means_with_sparse_input():\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n    kmeans.fit(X_csr)\n    assert_array_equal(kmeans.labels_, true_labels)\n", "def test_k_means_single_elkan():\n    centers, labels, inertia, n_iter = k_means(X, n_clusters, init=centers, algorithm='elkan')\n    # Check that labels are correct\n    assert_array_equal(labels, true_labels)\n    # Check that inertia is correct\n    assert_almost_equal(inertia, 0.0, decimal=2)\n", "def test_k_means_elkan():\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42, algorithm='elkan')\n    labels = kmeans.fit_predict(X)\n    assert_equal(labels.shape, (n_samples,))\n    assert_array_equal(np.unique(labels), np.arange(n_clusters))\n    assert_less(kmeans.inertia_, 100)\n    assert_greater(v_measure_score(true_labels, labels), 0.9)\n", "def test_kmeans_with_elkan_algorithm():\n    kmeans = KMeans(n_clusters=n_clusters, algorithm='elkan', random_state=42)\n    kmeans.fit(X)\n    labels = kmeans.labels_\n    inertia = kmeans.inertia_\n    assert_array_equal(labels, np.zeros(n_samples))\n    assert_almost_equal(inertia, 0.0, decimal=1)\n", "def test_mini_batch_kmeans_centers_update():\n    # Test the centers update in the MiniBatchKMeans\n    mbk = MiniBatchKMeans(n_clusters=n_clusters, random_state=42)\n    mbk.cluster_centers_ = centers.copy()\n    mbk.counts_ = np.ones(n_clusters, dtype=np.int32)\n\n    # Perform a single update step\n    x_squared_norms = row_norms(X, squared=True)\n    batch_inertia, centers_squared_diff = _mini_batch_step(\n        X, x_squared_norms, mbk.cluster_centers_, mbk.counts_,\n        np.zeros(n_features, dtype=X.dtype), True, distances=None)\n\n    # Check that the centers have been updated\n    assert_array_almost_equal(mbk.cluster_centers_, np.array([\n        [0.12534909, 4.65411155, 0.19667618, 0.05714784, 0.01028584],\n        [0.85848845, 0.67580419, 4.08390659, 0.05136613, 0.01232602],\n        [0.96789297, 0.36935583, 0.10564304, 4.96201176, 0.98933079],\n    ]))\n"], "sample_989": ["def test_Rational_str():\n    assert str(Rational(1, 2)) == '1/2'\n", "def test_Number_sympify():\n    assert sympify(1) == Integer(1)\n", "def test_rational_comparisons():\n    assert Rational(3, 4) > Rational(1, 2)\n    assert Rational(3, 4) >= Rational(1, 2)\n    assert Rational(1, 2) < Rational(3, 4)\n    assert Rational(1, 2) <= Rational(3, 4)\n", "def test_integer_cache_info():\n    ints = sorted(_intcache.keys())\n    nhit = _intcache_hits\n    nmiss = _intcache_misses\n\n    assert nhit == 3 and nmiss == 0, \"Integer cache statistic is incorrect\"\n", "def test_sympy_numbers_Rational():\n    r = Rational(3, 4)\n    assert r.p == 3\n    assert r.q == 4\n    assert Rational(3, -4) == Rational(-3, 4)\n    assert Rational(3, 4) == 3/4\n    assert Rational(3, 4) + Rational(1, 2) == Rational(5, 4)\n    assert Rational(3, 4) - Rational(1, 2) == Rational(1, 4)\n    assert Rational(3, 4) * Rational(1, 2) == Rational(3, 8)\n    assert Rational(3, 4) / Rational(1, 2) == Rational(3, 2)\n    assert Rational(3, 4) ** Rational(1, 2) == Rational(sqrt(3), 2)\n    assert Rational(3, 4) == Rational(Float(0.75))\n    assert Rational(3, 4) == Rational(Decimal('0.75'))\n    assert Rational(3, 4) == Rational('0.75')\n    assert Rational(3, 4) == Rational(str(0.75))\n    assert Rational(3, 4) == Rational('3/4')\n    assert Rational(3, 4) == Rational(str(3) + '/' + str(4))\n    assert Rational(3, 4) == Rational('3' + '/' + '4')\n    assert Rational(3, 4) == Rational(Rational(3, 4))\n    assert Rational(3, 4) == Rational(Integer(3), Integer(4))\n    assert Rational(3, 4) != Rational(5, 6)\n    assert Rational(3, 4) != 0.75\n    assert Rational(3, 4) != Float(0.75)\n    assert Rational(3, 4) != Decimal('0.75')\n    assert Rational("], "sample_695": ["def test_iterparentnodeids(nodeid: str, expected: List[str]) -> None:\n    assert list(nodes.iterparentnodeids(nodeid)) == expected\n", "def test_iterparentnodeids(nodeid: str, expected: List[str]):\n    assert list(nodes.iterparentnodeids(nodeid)) == expected\n", "def test_iterparentnodeids(nodeid: str, expected: List[str]) -> None:\n    result = list(nodes.iterparentnodeids(nodeid))\n    assert result == expected\n", "compilation error", "def test_imply_path(pytester: Pytester) -> None:\n    path = Path(\"/path/to/test\")\n    fspath = legacy_path(\"/path/to/test\")\n    assert nodes._imply_path(path, fspath) == (path, fspath)\n\n    with pytest.raises(ValueError):\n        nodes._imply_path(path, legacy_path(\"/different/path\"))\n\n    assert nodes._imply_path(None, fspath) == (path, fspath)\n\n    with pytest.raises(AssertionError):\n        nodes._imply_path(path, legacy_path(\"/different/path\"))\n"], "sample_139": ["def test_has_change_permission(self):\n    user = self._create_superuser('has_change_permission')\n    request = self._mocked_authenticated_request('/admin/admin_changelist/band/', user)\n    band = Band.objects.create(name='Test Band')\n    admin = BandAdmin(Band, custom_site)\n    self.assertTrue(admin.has_change_permission(request, obj=band))\n", "def test_get_results(self):\n    \"\"\"\n    Test the get_results method of the ChangeList class.\n    \"\"\"\n    site = admin.AdminSite()\n    admin_class = ParentAdmin(Parent, site)\n    request = self._mocked_authenticated_request('/admin/parent/', self.superuser)\n    cl = admin_class.get_changelist_instance(request)\n    # Test the get_results method with no search query\n    results = cl.get_results(request)\n    self.assertEqual(len(results), Parent.objects.count())\n    # Test the get_results method with a search query\n    request.GET[SEARCH_VAR] = 'test'\n    results = cl.get_results(request)\n    self.assertEqual(len(results), Parent.objects.filter(name__icontains='test').count())\n", "def test_queryset_filtering_with_actions(self):\n    url = reverse('admin:admin_changelist_event_changelist')\n    request = self._mocked_authenticated_request(url, self.superuser)\n    admin = EventAdmin(Event, custom_site)\n    queryset = admin.get_queryset(request)\n    queryset = queryset.filter(name='Test event')\n    admin.response_action(request, queryset=queryset)\n    # Add assertions to verify that the queryset is filtered correctly and the action is performed\n", "def test_pagination_all_results_displayed(self):\n    url = reverse('admin:admin_changelist_quartet_changelist')\n    request = self._mocked_authenticated_request(url, self.superuser)\n    cl = QuartetAdmin(Quartet, custom_site).get_changelist_instance(request)\n    self.assertEqual(cl.show_all, '0')\n    self.assertEqual(cl.result_count, 0)\n    self.assertEqual(cl.page_num, 1)\n    self.assertFalse(cl.show_all_url)\n", "def test_list_editable_handling(self):\n    parent_admin = ParentAdmin(Parent, admin.site)\n    request = self._mocked_authenticated_request('/admin/admin_changelist/parent/', self.superuser)\n    request.method = 'POST'\n    request._messages = CookieStorage(request)\n    request.POST = {'_save': 'Save', 'parent_set-0-id': '1', 'parent_set-0-name': 'New Name'}\n\n    with self.assertNumQueries(6):\n        response = parent_admin.changelist_view(request)\n\n    self.assertEqual(response.status_code, 302)\n    self.assertEqual(Parent.objects.get(id=1).name, 'New Name')\n"], "sample_938": ["def test_default_man_pages(app, monkeypatch):\n    config = Config({\n        'project': 'TestProject',\n        'release': '1.0',\n        'author': 'TestAuthor',\n        'master_doc': 'index'\n    })\n    monkeypatch.setattr(\"sphinx.util.osutil.make_filename_from_project\", lambda x: 'testproject')\n    result = default_man_pages(config)\n    assert result == [('index', 'testproject', 'TestProject 1.0', ['TestAuthor'], 1)]\n", "def test_default_man_pages(config: Config) -> None:\n    \"\"\"Test the default man_pages function.\"\"\"\n    config.project = 'TestProject'\n    config.release = '1.0'\n    config.master_doc = 'index'\n    config.author = 'TestAuthor'\n\n    result = default_man_pages(config)\n    expected_result = [('index', 'TestProject', 'TestProject 1.0', ['TestAuthor'], 1)]\n\n    assert result == expected_result\n", "def test_default_man_pages(app):\n    config = Config({\n        'project': 'TestProject',\n        'release': '1.0',\n        'author': 'TestAuthor',\n        'master_doc': 'index'\n    })\n    result = default_man_pages(config)\n    assert result == [('index', 'testproject', 'TestProject 1.0', ['TestAuthor'], 1)]\n", "def test_default_man_pages(app):\n    config = Config({'project': 'TestProject', 'release': '1.0', 'author': 'TestAuthor', 'master_doc': 'index'})\n    result = default_man_pages(config)\n    assert result == [('index', 'testproject', 'TestProject 1.0', ['TestAuthor'], 1)]\n", "def test_default_man_pages(app, config):\n    # Setup a minimal configuration\n    config.project = 'Test Project'\n    config.release = '1.0'\n    config.author = 'Test Author'\n    config.master_doc = 'index'\n\n    # Call the default_man_pages function\n    result = default_man_pages(config)\n\n    # Check the result\n    assert len(result) == 1\n    assert result[0][0] == 'index'\n    assert result[0][1] == 'testproject'\n    assert result[0][2] == 'Test Project 1.0'\n    assert result[0][3] == ['Test Author']\n    assert result[0][4] == 1\n"], "sample_7": ["def test_column_with_quantity_data():\n    data = [1, 2, 3] * u.m\n    col = table.Column(data=data, name='distance')\n    assert col.unit == u.m\n    assert_array_equal(col.data, [1, 2, 3])\n\n    col.unit = u.cm\n    assert_array_equal(col.data, [100, 200, 300])\n", "def test_copy(self):\n    col1 = table.Column(name='a', data=[1, 2, 3], unit='m')\n    col2 = col1.copy()\n    assert col1.attrs_equal(col2)\n    assert col1.info.attrs_equal(col2.info)\n    assert np.all(col1 == col2)\n    assert col1 is not col2\n", "def test_column_with_quantity_and_unit(self):\n    q = np.array([1, 2, 3]) * u.m\n    col = table.Column(data=q, name='col', unit=u.km)\n    assert_array_equal(col, [1000, 2000, 3000])\n    assert col.unit == u.km\n", "def test_column_name_property(self):\n    col = table.Column(name='old_name', data=[1, 2, 3])\n    assert col.name == 'old_name'\n    col.name = 'new_name'\n    assert col.name == 'new_name'\n", "def test_column_with_quantity_data():\n    data = [1, 2, 3] * u.m\n    col = table.Column(data=data, name='distance')\n    assert_array_equal(col.data, [1, 2, 3])\n    assert col.unit == u.m\n    assert col.dtype == np.float64\n"], "sample_503": ["def test_set_markersize():\n    line = mlines.Line2D([0, 1], [0, 1])\n    assert line.get_markersize() == plt.rcParams['lines.markersize']\n    line.set_markersize(10)\n    assert line.get_markersize() == 10\n    line.set_markersize(15.5)\n    assert line.get_markersize() == 15.5\n    with pytest.raises(TypeError):\n        line.set_markersize(\"invalid\")\n", "def test_line2d_marker_with_clipping():\n    fig, ax = plt.subplots()\n    y = np.arange(10)\n    line, = ax.plot(y, 'o', clip_on=False)\n    ax.set_xlim(2, 8)\n    fig.savefig(\"line2d_marker_with_clipping\")\n", "def test_line2d_markevery_slice():\n    fig, ax = plt.subplots()\n    x = np.linspace(0, 10, 100)\n    y = np.sin(x)\n    ax.plot(x, y, markevery=slice(10, 90, 10))\n", "def test_line2d_alpha():\n    fig, ax = plt.subplots()\n    line = mlines.Line2D([0, 1], [0, 1], alpha=0.5)\n    ax.add_line(line)\n    fig.savefig(__file__.replace('.py', '_output.png'))\n", "def test_line2d_markevery_float_list():\n    fig, ax = plt.subplots()\n    x = np.linspace(0, 10, 1000)\n    y = np.sin(x)\n    line, = ax.plot(x, y, marker='o', markevery=[0.1, 0.5, 0.9])\n    assert len(line.get_markevery()) == 3\n    assert np.allclose(line.get_markevery(), [100, 500, 900])\n    plt.close(fig)\n"], "sample_632": ["def test_similar_with_docstrings():\n    sim = similar.Similar(min_lines=2, ignore_docstrings=False)\n    with open(SIMILAR1) as stream:\n        sim.append_stream(SIMILAR1, stream)\n    with open(SIMILAR2) as stream:\n        sim.append_stream(SIMILAR2, stream)\n    similarities = sim._compute_sims()\n    assert len(similarities) == 1\n    num, files = similarities[0]\n    assert num == 2\n    assert len(files) == 2\n    assert {(SIMILAR1, 1), (SIMILAR2, 1)} in files\n", "def test_similar_min_lines():\n    sim = similar.Similar(min_lines=3)\n    with open(SIMILAR1) as stream:\n        sim.append_stream(SIMILAR1, stream)\n    with open(SIMILAR2) as stream:\n        sim.append_stream(SIMILAR2, stream)\n    sim.run()\n    # Add assertions here to validate the output\n", "def test_ignore_signatures():\n    linter = PyLinter()\n    linter.config.ignore_signatures = True\n    checker = similar.SimilarChecker(linter)\n    checker.open()\n    checker.process_module(linter.ast_manager.parse(EMPTY_FUNCTION_1))\n    checker.process_module(linter.ast_manager.parse(EMPTY_FUNCTION_2))\n    checker.close()\n    assert linter.stats[\"nb_duplicated_lines\"] == 0\n    assert linter.stats[\"percent_duplicated_lines\"] == 0\n", "def test_ignore_signatures():\n    linter = PyLinter()\n    linter.set_option(\"ignore-signatures\", True)\n    linter.read_config_file(SIMILAR1)\n    reporter = Reporter()\n    linter.add_reporter(reporter)\n    linter.check([SIMILAR2])\n    msgs = reporter.messages\n    assert len(msgs) == 1\n    assert msgs[0].msg_id == \"R0801\"\n    assert \"4 similar lines in 2 files\" in msgs[0].msg\n", "def test_similar_ignore_signatures():\n    linter = PyLinter()\n    linter.config.ignore_signatures = True\n    similar_checker = similar.SimilarChecker(linter)\n    similar_checker.set_option(\"ignore-signatures\", True)\n    with open(SIMILAR5, \"r\") as stream:\n        similar_checker.process_module(stream, SIMILAR5)\n    with open(SIMILAR6, \"r\") as stream:\n        similar_checker.process_module(stream, SIMILAR6)\n    similar_checker.close()\n    assert linter.msg_status == 0  # No similar lines detected\n"], "sample_372": ["    def test_resolve_error_handler(self):\n        resolver = URLResolver(RegexPattern(r'^/'), 'urlpatterns_reverse.error_urls')\n        handler400 = resolver.resolve_error_handler(400)\n        handler403 = resolver.resolve_error_handler(403)\n        handler404 = resolver.resolve_error_handler(404)\n        handler500 = resolver.resolve_error_handler(500)\n        self.assertEqual(handler400.__name__, 'handler400')\n        self.assertEqual(handler403.__name__, 'handler403')\n        self.assertEqual(handler404.__name__, 'handler404')\n        self.assertEqual(handler500.__name__, 'handler500')\n\n        with self.assertRaises(ViewDoesNotExist):\n            resolver.resolve_error_handler(405)\n\n        with self.settings(ROOT_URLCONF='urlpatterns_reverse.no_handler_urls'):\n            with self.assertRaises(ImproperlyConfigured):\n                resolver.resolve_error_handler(400)\n", "    def test_no_url_patterns(self):\n        with self.assertRaisesMessage(ImproperlyConfigured, \"The included URLconf 'urlpatterns_reverse.no_urls' does not appear to have any patterns in it. If you see the 'urlpatterns' variable with valid patterns in the file then the issue is probably caused by a circular import.\"):\n            get_resolver('urlpatterns_reverse.no_urls')\n", "    def test_pattern_startswith_slash(self):\n        pattern = RegexPattern(r'^/pattern/')\n        warning = pattern._check_pattern_startswith_slash()\n        self.assertEqual(len(warning), 1)\n        self.assertEqual(warning[0].id, 'urls.W002')\n\n        pattern = RegexPattern(r'pattern/')\n        warning = pattern._check_pattern_startswith_slash()\n        self.assertEqual(len(warning), 0)\n", "def test_check_include_trailing_dollar(self):\n    pattern = RoutePattern('test/$')\n    warnings = pattern._check_include_trailing_dollar()\n    self.assertEqual(len(warnings), 1)\n    self.assertEqual(warnings[0].id, 'urls.W001')\n", "    def test_resolve_error_handler(self):\n        resolver = URLResolver(RegexPattern(r'^/'), 'urlpatterns_reverse.error_handler_urls')\n        handler400 = resolver.resolve_error_handler(400)\n        self.assertEqual(handler400.__name__, 'custom_handler400')\n        handler403 = resolver.resolve_error_handler(403)\n        self.assertEqual(handler403.__name__, 'custom_handler403')\n        handler404 = resolver.resolve_error_handler(404)\n        self.assertEqual(handler404.__name__, 'custom_handler404')\n        handler500 = resolver.resolve_error_handler(500)\n        self.assertEqual(handler500.__name__, 'custom_handler500')\n"], "sample_1046": ["def test_tensorsymmetry():\n    Lorentz = TensorIndexType('Lorentz', dummy_fmt='L')\n    sym2 = tensorsymmetry([1, 1])\n    S2 = TensorType([Lorentz]*2, sym2)\n    V = S2('V')\n    assert isinstance(V, TensorHead)\n", "def test_tensor_symmetry():\n    Lorentz = TensorIndexType('Lorentz', dummy_fmt='L')\n    sym2 = TensorSymmetry(get_symmetric_group_sgs(2))\n    S2 = TensorType([Lorentz]*2, sym2)\n    V = S2('V')\n    i, j = tensor_indices('i,j', Lorentz)\n    assert V(i, j) == V(j, i)\n    assert V(i, i) == V(i, i)\n", "def test_canon_bp_with_symbols():\n    Lorentz = TensorIndexType('Lorentz', dummy_fmt='L')\n    m0, m1, m2 = tensor_indices('m0,m1,m2', Lorentz)\n    A = tensorhead('A', [Lorentz]*2, [[2]])\n    t = A(m0,-m1)*A(m1,-m0)\n    assert t.canon_bp() == -A(Lorentz(0), Lorentz(1))*A(-Lorentz(0), -Lorentz(1))\n", "def test_tensor_sum():\n    Lorentz = TensorIndexType('Lorentz', dummy_fmt='L')\n    a, b = tensor_indices('a,b', Lorentz)\n    p, q = tensorhead('p,q', [Lorentz], [[1]])\n    t = p(a) + q(a)\n    assert t(b) == p(b) + q(b)\n", "def test_tensor_index_type():\n    Lorentz = TensorIndexType('Lorentz', dummy_fmt='L')\n    assert Lorentz.dim is None\n    assert Lorentz.metric_antisym is False\n    assert Lorentz.metric_name == 'metric'\n    assert Lorentz.name == 'Lorentz'\n    assert Lorentz.dummy_fmt == 'L'\n    assert Lorentz.metric is None\n    assert Lorentz.epsilon_top == (-1)**Rational(1, 2)\n    assert Lorentz.epsilon_bottom == (-1)**Rational(1, 2)\n"], "sample_412": ["def test_escapejs(self):\n    self.check_output(escapejs, '<script>alert(\"XSS\");</script>', '\\\\u003Cscript\\\\u003Ealert(\\\\u0022XSS\\\\u0022);\\\\u003C/script\\\\u003E')\n", "def test_escapejs(self):\n    self.check_output(escapejs, '<script>alert(\"Hello, world!\");</script>', '\\\\u003Cscript\\\\u003Ealert(\\\\u0022Hello, world!\\\\u0022);\\\\u003C/script\\\\u003E')\n", "def test_urlize(self):\n    self.check_output(\n        urlize,\n        \"http://example.com/ this is a simple URL.\",\n        '<a href=\"http://example.com/\">http://example.com/</a> this is a simple URL.',\n    )\n", "def test_format_html(self):\n    self.assertEqual(format_html(\"Hello, {}!\", \"World\"), mark_safe(\"Hello, World!\"))\n    self.assertEqual(format_html(\"{} + {} = {}\", 1, 2, 3), mark_safe(\"1 + 2 = 3\"))\n", "def test_urlize(self):\n    self.check_output(urlize, 'http://example.com', '<a href=\"http://example.com\">http://example.com</a>')\n    self.check_output(urlize, 'www.example.com', '<a href=\"http://www.example.com\">www.example.com</a>')\n    self.check_output(urlize, 'example@example.com', '<a href=\"mailto:example@example.com\">example@example.com</a>')\n    self.check_output(urlize, '(http://example.com)', '(<a href=\"http://example.com\">http://example.com</a>)')\n    self.check_output(urlize, 'http://example.com.', '<a href=\"http://example.com\">http://example.com</a>.')\n    self.check_output(urlize, 'http://example.com,', '<a href=\"http://example.com\">http://example.com</a>,')\n    self.check_output(urlize, 'http://example.com)', '<a href=\"http://example.com\">http://example.com</a>)')\n    self.check_output(urlize, 'Check out http://example.com and www.example.org.', 'Check out <a href=\"http://example.com\">http://example.com</a> and <a href=\"http://www.example.org\">www.example.org</a>.')\n"], "sample_408": ["def test_remove_constraints(self):\n    before_states = [self.author_name_check_constraint]\n    after_states = [self.author_name]\n    changes = self.get_changes(before_states, after_states)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"RemoveConstraint\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"name_contains_bob\")\n", "def test_generate_altered_db_table(self, mock_questioner):\n    changes = self.get_changes([self.author_with_db_table_options], [self.author_with_new_db_table_options], mock_questioner())\n    self.assertOperationTypes(changes, 'testapp', 0, ['AlterModelTable'])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, table='author_two')\n", "def test_model_renaming_with_related_fields(self):\n    before_states = [\n        self.author_with_book,\n        self.book,\n    ]\n    after_states = [\n        self.author_renamed_with_book,\n        self.book_with_author_renamed,\n    ]\n    changes = self.get_changes(before_states, after_states)\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"RenameModel\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"author\", new_name=\"Writer\")\n    self.assertNumberMigrations(changes, \"otherapp\", 1)\n    self.assertOperationTypes(changes, \"otherapp\", 0, [\"AlterField\"])\n    self.assertOperationFieldAttributes(changes, \"otherapp\", 0, 0, model_name=\"Book\", name=\"author\", to=\"testapp.Writer\")\n", "def test_m2m_renamed_through(self, mock_questioner):\n    mock_questioner.return_value = True\n    changes = self.get_changes(\n        [self.author_with_m2m_through], [self.author_with_renamed_m2m_through]\n    )\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"RemoveField\", \"AddField\"])\n    self.assertOperationAttributes(\n        changes, \"testapp\", 0, 0, name=\"publishers\", field_name=\"publishers\"\n    )\n    self.assertOperationAttributes(\n        changes, \"testapp\", 0, 1, name=\"publishers\", field_name=\"publishers\"\n    )\n    self.assertOperationFieldAttributes(\n        changes, \"testapp\", 0, 1, through=\"testapp.Deal\"\n    )\n", "def test_deconstructible_objects_are_treated_as_equal(self):\n    questioner = MigrationQuestioner(specified_apps=[\"testapp\"], dry_run=True)\n    changes = self.get_changes([self.author_name_deconstructible_1], [self.author_name_deconstructible_2], questioner)\n    self.assertNumberMigrations(changes, 'testapp', 0)\n"], "sample_1178": ["def test_augmented_assignment_classes():\n    assert aug_assign(x, '+', y) == AddAugmentedAssignment(x, y)\n    assert aug_assign(x, '-', y) == SubAugmentedAssignment(x, y)\n    assert aug_assign(x, '*', y) == MulAugmentedAssignment(x, y)\n    assert aug_assign(x, '/', y) == DivAugmentedAssignment(x, y)\n    assert aug_assign(x, '%', y) == ModAugmentedAssignment(x, y)\n    with raises(ValueError):\n        aug_assign(x, '^', y)\n", "def test_CodeBlock_free_symbols():\n    c = CodeBlock(Assignment(x, y + z), Assignment(y, z + 1))\n    assert c.free_symbols == {z}\n", "def test_augmented_assignment():\n    assign = aug_assign(x, '+', y)\n    assert isinstance(assign, AddAugmentedAssignment)\n    assert assign.lhs == x\n    assert assign.rhs == y\n\n    assign = aug_assign(x, '-', y)\n    assert isinstance(assign, SubAugmentedAssignment)\n    assert assign.lhs == x\n    assert assign.rhs == y\n\n    assign = aug_assign(x, '*', y)\n    assert isinstance(assign, MulAugmentedAssignment)\n    assert assign.lhs == x\n    assert assign.rhs == y\n\n    assign = aug_assign(x, '/', y)\n    assert isinstance(assign, DivAugmentedAssignment)\n    assert assign.lhs == x\n    assert assign.rhs == y\n\n    assign = aug_assign(x, '%', y)\n    assert isinstance(assign, ModAugmentedAssignment)\n    assert assign.lhs == x\n    assert assign.rhs == y\n\n    with raises(ValueError):\n        aug_assign(x, '^', y)\n", "def test_variable_as_declaration():\n    v = Variable('x', type=float32)\n    decl = v.as_Declaration(value=42.0)\n    assert decl.variable.value == 42.0\n", "def test_augmented_assignments():\n    assert AugmentedAssignment(x, '+=', y).op == '+='\n    assert AddAugmentedAssignment(x, y).op == '+='\n    assert SubAugmentedAssignment(x, y).op == '-='\n    assert MulAugmentedAssignment(x, y).op == '*='\n    assert DivAugmentedAssignment(x, y).op == '/='\n    assert ModAugmentedAssignment(x, y).op == '%='\n"], "sample_354": ["    def test_create_superuser_non_interactive(self):\n        with mock.patch('sys.stdin', MockTTY()):\n            call_command('createsuperuser', username='alice', email='alice@example.com', interactive=False, stdin=StringIO())\n        self.assertEqual(CustomUser.objects.count(), 1)\n        user = CustomUser.objects.get(username='alice')\n        self.assertTrue(user.is_superuser)\n        self.assertEqual(user.email, 'alice@example.com')\n        self.assertTrue(user.check_password('secret'))\n", "    def test_create_superuser_interactive(self):\n        out = StringIO()\n        call_command('createsuperuser', interactive=True, stdin=MockTTY(), stdout=out)\n        self.assertIn('Superuser created successfully.', out.getvalue())\n        self.assertTrue(User.objects.filter(username='alice', is_superuser=True).exists())\n", "def test_createsuperuser_non_interactive(self):\n    out = StringIO()\n    call_command('createsuperuser', '--noinput', '--username=alice', '--email=alice@example.com', stdout=out)\n    self.assertEqual(out.getvalue(), \"Superuser created successfully.\\n\")\n    self.assertTrue(User.objects.filter(username='alice').exists())\n    self.assertTrue(User.objects.get(username='alice').is_superuser)\n", "def test_create_superuser_valid_password_with_input(self):\n    out = StringIO()\n    call_command('createsuperuser', stdout=out, interactive=True, stdin=MockTTY())\n    self.assertIn('Superuser created successfully.', out.getvalue())\n    self.assertTrue(User.objects.filter(username='alice').exists())\n", "def test_createsuperuser_non_interactive(self):\n    out = StringIO()\n    options = {'username': 'alice', 'email': 'alice@example.com', 'noinput': False, 'database': 'default'}\n    with mock.patch('os.environ', {'DJANGO_SUPERUSER_PASSWORD': 'testpassword'}):\n        management.call_command('createsuperuser', **options, stdout=out)\n    self.assertIn('Superuser created successfully.', out.getvalue())\n    self.assertTrue(User.objects.filter(username='alice', is_superuser=True).exists())\n"], "sample_789": ["def test_adaboost_classifier_feature_importances_():\n    X, y = iris.data, iris.target\n    clf = AdaBoostClassifier(n_estimators=10, random_state=rng)\n    clf.fit(X, y)\n    feature_importances = clf.feature_importances_\n    assert_array_equal(feature_importances.shape, (X.shape[1],))\n    assert_greater(feature_importances.sum(), 0)\n", "def test_adaboost_classifier_sparse_input():\n    # Test AdaBoostClassifier with sparse input\n    X_csc = csc_matrix(X)\n    clf = AdaBoostClassifier()\n    clf.fit(X_csc, y_class)\n    assert_array_equal(clf.predict(X_csc), clf.predict(X))\n\n    X_csr = csr_matrix(X)\n    clf.fit(X_csr, y_class)\n    assert_array_equal(clf.predict(X_csr), clf.predict(X))\n\n    X_coo = coo_matrix(X)\n    clf.fit(X_coo, y_class)\n    assert_array_equal(clf.predict(X_coo), clf.predict(X))\n\n    X_dok = dok_matrix(X)\n    clf.fit(X_dok, y_class)\n    assert_array_equal(clf.predict(X_dok), clf.predict(X))\n\n    X_lil = lil_matrix(X)\n    clf.fit(X_lil, y_class)\n    assert_array_equal(clf.predict(X_lil), clf.predict(X))\n", "def test_adaboost_classifier_custom_base_estimator():\n    clf = AdaBoostClassifier(base_estimator=SVC(probability=True))\n    clf.fit(iris.data, iris.target)\n    assert_greater(clf.score(iris.data, iris.target), 0.9)\n", "def test_adaboost_classifier_with_sample_weights():\n    X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=rng)\n    sample_weights = rng.rand(y_train.shape[0])\n\n    clf = AdaBoostClassifier(random_state=rng)\n    clf.fit(X_train, y_train, sample_weight=sample_weights)\n\n    assert_greater(clf.score(X_test, y_test), 0.9)\n", "def test_boost_real():\n    clf = AdaBoostClassifier(algorithm='SAMME.R', random_state=rng)\n    clf.n_classes_ = 2\n    clf.classes_ = np.array([0, 1])\n    y = np.array([0, 1, 0, 1, 0, 1])\n    y_predict_proba = np.array([[0.9, 0.1], [0.4, 0.6], [0.8, 0.2], [0.3, 0.7], [0.6, 0.4], [0.7, 0.3]])\n    sample_weight = np.ones(6) / 6.\n    estimator = BaseEstimator()\n    estimator.predict_proba = lambda X: y_predict_proba\n    clf.estimators_ = [estimator]\n    clf.estimator_weights_ = np.array([1.])\n    sample_weight_new, _, _ = clf._boost_real(0, X, y, sample_weight, rng)\n    assert_array_almost_equal(sample_weight_new, np.array([0.06719571, 0.23688272, 0.06719571, 0.23688272, 0.16840378, 0.16840378]))\n"], "sample_567": ["def test_annotation_clip(self):\n    fig, ax = plt.subplots()\n    ann = Annotation(\"Text\", (0.5, 0.5), xycoords='axes fraction',\n                     annotation_clip=True)\n    ax.add_artist(ann)\n    fig.savefig(self.figname())\n", "def test_text_update():\n    text = Text(0, 0, \"Test\")\n    text.update(fontsize=12, color=\"red\", text=\"Updated\")\n    assert text.get_text() == \"Updated\"\n    assert text.get_color() == \"red\"\n    assert text.get_fontsize() == 12\n", "def test_annotation_arrow(fig_test, fig_ref):\n    fig, ax = fig_test.subplots()\n    annotation = Annotation(\"Test Annotation\", xy=(0.5, 0.5), xytext=(0.2, 0.8),\n                            arrowprops=dict(facecolor='black', shrink=0.05))\n    ax.add_artist(annotation)\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n\n    fig_ref.subplots().text(0.5, 0.5, \"Test Annotation\",\n                            bbox=dict(facecolor='white', edgecolor='black'),\n                            arrowprops=dict(facecolor='black', shrink=0.05))\n", "def test_annotation_setters_and_getters():\n    fig, ax = plt.subplots()\n    ann = Annotation(\"Test Annotation\", xy=(0.5, 0.5), xycoords='axes fraction',\n                     textcoords='offset points', xytext=(10, 10),\n                     arrowprops=dict(arrowstyle=\"->\"))\n    ax.add_artist(ann)\n\n    # Test getters\n    assert ann.xy == (0.5, 0.5)\n    assert ann.xycoords == 'axes fraction'\n    assert ann.anncoords == 'offset points'\n    assert ann.xyann == (10, 10)\n\n    # Test setters\n    ann.xy = (0.2, 0.2)\n    ann.xycoords = 'data'\n    ann.anncoords = 'figure pixels'\n    ann.xyann = (200, 200)\n\n    assert ann.xy == (0.2, 0.2)\n    assert ann.xycoords == 'data'\n    assert ann.anncoords == 'figure pixels'\n    assert ann.xyann == (200, 200)\n", "def test_text_rotation_mode():\n    fig, ax = plt.subplots()\n    text = Text(0, 0, \"Test\", rotation=45, rotation_mode=\"anchor\")\n    ax.add_artist(text)\n    fig.canvas.draw()\n    # add assertions to test the rotation mode\n"], "sample_543": ["def test_slider_update(fig):\n    ax = fig.subplots()\n    slider = widgets.Slider(ax, \"Test\", 0, 100, valinit=50)\n    assert slider.val == 50\n    slider.set_val(75)\n    assert slider.val == 75\n", "def test_button_release_event(ax):\n        pass\n\n    lasso = widgets.Lasso(ax, (0.5, 0.5), callback)\n    event = mock_event(ax, 'button_release_event', x=100, y=100, button=1)\n    lasso.onrelease(event)\n\n    assert lasso.verts is None\n    assert lasso.line not in ax.lines\n    assert lasso.callback_cid is None\n    assert lasso.motion_cid is None\n", "def test_textbox_creation(ax):\n    initial = \"test\"\n    tb = widgets.TextBox(ax, \"label\", initial=initial)\n    assert tb.text == initial\n", "def test_textbox_blit(fig):\n    ax = fig.add_subplot(111)\n    ax.plot(np.random.rand(10))\n    textbox = widgets.TextBox(ax, 'Test', initial='test')\n    textbox.on_submit(noop)\n    do_event(textbox.textbox, 'button_press_event', 50, 50)\n    do_event(textbox.textbox, 'button_release_event', 50, 50)\n    do_event(textbox.textbox, 'key_press_event', x=50, y=50, key='t')\n    do_event(textbox.textbox, 'key_press_event', x=50, y=50, key='e')\n    do_event(textbox.textbox, 'key_press_event', x=50, y=50, key='s')\n    do_event(textbox.textbox, 'key_press_event', x=50, y=50, key='t')\n    do_event(textbox.textbox, 'key_press_event', x=50, y=50, key='enter')\n", "def test_rectangle_selector_on_release_no_span(ax):\n    # Test on_release when span is less than or equal to minspan\n    callback = mock.Mock()\n    rect_selector = widgets.RectangleSelector(ax, callback)\n    rect_selector.minspanx = 1\n    rect_selector.minspany = 1\n    rect_selector.extents = (0, 0.5, 0, 0.5)  # spanx and spany are both less than minspan\n    eventpress = mock_event(ax, 'button_press_event', 0, 0)\n    eventrelease = mock_event(ax, 'button_release_event', 0.5, 0.5)\n    rect_selector._eventpress = eventpress\n    rect_selector._eventrelease = eventrelease\n    rect_selector._release(eventrelease)\n    callback.assert_not_called()\n    assert rect_selector._selection_completed is False\n"], "sample_344": ["def test_get_concrete_model_key(self):\n    project_state = ProjectState()\n    model_state = ModelState('test_app', 'TestModel', {})\n    project_state.models[('test_app', 'testmodel')] = model_state\n    concrete_model_key = project_state.get_concrete_model_key(model_state)\n    self.assertEqual(concrete_model_key, ('test_app', 'testmodel'))\n", "    def test_alter_field_updates_related_model_when_delay_is_false(self):\n        # Create a project state with two related models\n        initial_state = ProjectState()\n        initial_state.add_model(ModelState('test', 'Author', fields={'id': models.AutoField(primary_key=True)}))\n        initial_state.add_model(ModelState('test', 'Book', fields={'id': models.AutoField(primary_key=True), 'author': models.ForeignKey('test.Author', on_delete=models.CASCADE)}))\n\n        # Alter a field on the related model with delay=False\n        operation = AlterField('test', 'Book', 'author', models.ForeignKey('test.Author', on_delete=models.SET_NULL))\n        operation.state_forwards('test', initial_state)\n\n        # Check that the related model is updated\n        self.assertEqual(initial_state.models[('test', 'book')].fields['author'].remote_field.on_delete, models.SET_NULL)\n", "def test_rename_model(self):\n    old_state = ProjectState()\n    old_state.add_model(ModelState('app', 'OldModel', {'name': models.CharField(max_length=255)}))\n    old_state.add_field('app', 'anothermodel', 'old_model', models.ForeignKey('app.OldModel', on_delete=models.CASCADE), True)\n    new_state = old_state.clone()\n    new_state.rename_model('app', 'oldmodel', 'NewModel')\n    self.assertEqual(new_state.models, {\n        ('app', 'newmodel'): ModelState('app', 'NewModel', {'name': models.CharField(max_length=255)}),\n        ('app', 'anothermodel'): ModelState('app', 'AnotherModel', {\n            'old_model': models.ForeignKey('app.NewModel', on_delete=models.CASCADE)\n        }),\n    })\n", "    def test_alter_field_preserve_default(self):\n        # Test that altering a field preserves its default value if preserve_default is True\n        state = ProjectState()\n        state.add_model(\n            ModelState(\n                app_label='test',\n                name='TestModel',\n                fields=[\n                    ('id', models.AutoField(primary_key=True)),\n                    ('char_field', models.CharField(max_length=10, default='test')),\n                ],\n            )\n        )\n        state.alter_field(\n            app_label='test',\n            model_name='testmodel',\n            name='char_field',\n            field=models.CharField(max_length=20),\n            preserve_default=True,\n        )\n        altered_field = state.models['test', 'testmodel'].fields['char_field']\n        self.assertEqual(altered_field.default, 'test')\n", "def test_remove_field_renders_only_related_models(self):\n    class Country(models.Model):\n        name = models.CharField(max_length=50)\n\n    class City(models.Model):\n        name = models.CharField(max_length=50)\n        country = models.ForeignKey(Country, on_delete=models.CASCADE)\n\n    project_state = ProjectState.from_apps(Apps())\n    operation = RemoveField('migrations', 'City', 'country')\n    operation.state_forwards('migrations', project_state)\n    # Only City and Country should be rendered, not UnicodeModel.\n    self.assertEqual(project_state.apps.all_models['migrations'].keys(), {'country', 'city'})\n"], "sample_307": ["    def test_time_format(self):\n        dt = datetime(2022, 1, 1, 13, 30, 0)\n        tf = dateformat.TimeFormat(dt)\n        self.assertEqual(tf.format('g:i a'), '1:30 p.m.')\n        self.assertEqual(tf.format('H:i:s'), '13:30:00')\n        self.assertEqual(tf.format('P'), '1:30 p.m.')\n        self.assertEqual(tf.format('T'), 'CET')\n        dt = make_aware(dt, utc)\n        tf = dateformat.TimeFormat(dt)\n        self.assertEqual(tf.format('e'), 'UTC')\n        self.assertEqual(tf.format('O'), '+0000')\n        self.assertEqual(tf.format('Z'), 0)\n", "    def test_aware_datetime_format(self):\n        d = make_aware(datetime(2023, 4, 25, 12, 30, 45), utc)\n        self.assertEqual(format(d, 'Y-m-d H:i:s T'), '2023-04-25 12:30:45 UTC')\n        self.assertEqual(format(d, 'Y-m-d H:i:s Z'), '2023-04-25 12:30:45 +0000')\n        self.assertEqual(format(d, 'Y-m-d H:i:s e'), '2023-04-25 12:30:45 UTC')\n", "    def test_date_format_jS_F_Y_H_i(self):\n        d = datetime(2003, 10, 7, 11, 39)\n        df = dateformat.DateFormat(d)\n        self.assertEqual(df.format('jS F Y H:i'), '7th October 2003 11:39')\n", "    def test_format_with_timezone(self):\n        dt = datetime(2022, 1, 1, 12, 34, 56, tzinfo=get_fixed_timezone(180))\n        self.assertEqual(format(dt, 'e'), '+0300')\n        self.assertEqual(format(dt, 'O'), '+0300')\n        self.assertEqual(format(dt, 'T'), '+0300')\n", "    def test_time_format_with_timezone(self):\n        # Test time format with timezone information\n        dt = datetime(2022, 1, 1, 12, 0, 0)\n        dt_aware = make_aware(dt, timezone=get_fixed_timezone(120))\n        tf = dateformat.TimeFormat(dt_aware)\n        self.assertEqual(tf.format('e'), '+0200')\n        self.assertEqual(tf.format('T'), '+0200')\n"], "sample_613": ["def test_consolidate_slices():\n    slices = [slice(0, 5), slice(5, 10), slice(10, 20, 2)]\n    result = _consolidate_slices(slices)\n    assert result == [slice(0, 10), slice(10, 20, 2)]\n", "def test_consolidate_slices():\n    slices = [slice(0, 1), slice(1, 2), slice(3, 4), slice(4, 5)]\n    result = _consolidate_slices(slices)\n    assert result == [slice(0, 2), slice(3, 5)]\n", "def test_consolidate_slices():\n    slices = [slice(0, 5), slice(5, 10), slice(10, 20, 2)]\n    consolidated = _consolidate_slices(slices)\n    expected = [slice(0, 10), slice(10, 20, 2)]\n    assert consolidated == expected\n\n    slices = [slice(0, 5), slice(6, 10), slice(10, 20, 2)]\n    consolidated = _consolidate_slices(slices)\n    expected = slices  # No consolidation possible\n    assert consolidated == expected\n\n    slices = [slice(None), slice(5, 10), slice(10, 20, 2)]\n    consolidated = _consolidate_slices(slices)\n    expected = [slice(None)]  # Only the first slice is None\n    assert consolidated == expected\n\n    with pytest.raises(ValueError):\n        _consolidate_slices([1, 2, 3])  # Non-slice elements should raise ValueError\n", "def test_groupby_consolidate_slices():\n    slices = [slice(0, 5), slice(5, 10), slice(10, 15, 2)]\n    consolidated = _consolidate_slices(slices)\n    expected = [slice(0, 10), slice(10, 15, 2)]\n    assert consolidated == expected\n", "def test_groupby_quantile(array):\n    quantile_values = [0, 0.5, 1]\n    result = array.groupby(\"x\").quantile(quantile_values)\n\n    assert isinstance(result, DataArray)\n    assert result.dims == (\"x\", \"y\", \"z\", \"quantile\")\n    assert_array_equal(result.quantile, quantile_values)\n\n    # Add more specific assertions based on your expected outcome\n"], "sample_966": ["def test_parse_annotation():\n    env = Mock()\n    env.config.python_use_unqualified_type_names = False\n\n    annotation = \"List[Union[int, str]]\"\n    result = _parse_annotation(annotation, env)\n    assert isinstance(result, list)\n    assert len(result) == 5\n    assert isinstance(result[0], nodes.Text)\n    assert result[0].astext() == \"List\"\n    assert isinstance(result[1], addnodes.desc_sig_punctuation)\n    assert result[1].astext() == \"[\"\n    assert isinstance(result[2], pending_xref)\n    assert result[2].astext() == \"Union\"\n    assert isinstance(result[3], addnodes.desc_sig_punctuation)\n    assert result[3].astext() == \"[\"\n    assert isinstance(result[4], addnodes.desc_sig_punctuation)\n    assert result[4].astext() == \"]\"\n", "def test_parse_annotation():\n    # Test _parse_annotation function with a simple annotation\n    env = Mock()\n    env.config.python_use_unqualified_type_names = False\n    result = _parse_annotation(\"int\", env)\n    assert isinstance(result[0], pending_xref)\n    assert result[0].get('reftarget') == 'int'\n\n    # Test _parse_annotation function with a complex annotation\n    result = _parse_annotation(\"List[Union[int, str]]\", env)\n    assert len(result) == 9\n    assert isinstance(result[0], pending_xref)\n    assert result[0].get('reftarget') == 'typing.List'\n    assert isinstance(result[2], pending_xref)\n    assert result[2].get('reftarget') == 'typing.Union'\n    assert isinstance(result[4], pending_xref)\n    assert result[4].get('reftarget') == 'int'\n    assert isinstance(result[6], pending_xref)\n    assert result[6].get('reftarget') == 'str'\n", "def test_parse_annotation(app, status, warning):\n    env = app.builder.env\n\n        children = _parse_annotation(annotation, env)\n        assert_node(expected, children)\n\n    assert_annotation_parsing('int', [pending_xref('', 'int', refdomain='py', reftype='class', reftarget='int')])\n    assert_annotation_parsing('List[int]',\n                              [pending_xref('', 'List', refdomain='py', reftype='class', reftarget='List'),\n                               desc_sig_punctuation('', '['),\n                               pending_xref('', 'int', refdomain='py', reftype='class', reftarget='int'),\n                               desc_sig_punctuation('', ']')])\n    assert_annotation_parsing('Optional[int]',\n                              [pending_xref('', 'Optional', refdomain='py', reftype='class', reftarget='Optional'),\n                               desc_sig_punctuation('', '['),\n                               pending_xref('', 'int', refdomain='py', reftype='class', reftarget='int'),\n                               desc_sig_punctuation('', ']')])\n    assert_annotation_parsing('List[Optional[int]]',\n                              [pending_xref('', 'List', refdomain='py', reftype='class', reftarget='List'),\n                               desc_sig_punctuation('', '['),\n                               pending_xref('', 'Optional', refdomain='py', reftype='class', reftarget='Optional'),\n                               desc_sig_punctuation('', '['),\n                               pending_xref('', 'int', refdomain='py', reftype='class', reftarget='int'),\n                               desc_sig_punctuation('', ']'),\n                               desc_sig_punctuation('', ']')])\n    assert_annotation_parsing('Tuple[int, int]',", "def test_parse_annotation():\n    env = Mock()\n    env.config.python_use_unqualified_type_names = False\n\n    annotation = 'List[str]'\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 5\n    assert isinstance(result[0], nodes.Text)\n    assert result[0].astext() == 'List'\n    assert isinstance(result[1], nodes.Text)\n    assert result[1].astext() == '['\n    assert isinstance(result[2], pending_xref)\n    assert result[2].astext() == 'str'\n    assert isinstance(result[3], nodes.Text)\n    assert result[3].astext() == ']'\n    assert isinstance(result[4], nodes.Text)\n    assert result[4].astext() == ''\n", "def test_parse_annotation():\n    app = Mock()\n    env = Mock()\n    env.config.python_use_unqualified_type_names = False\n\n    # Test parsing a simple type annotation\n    annotation = \"int\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 1\n    assert isinstance(result[0], pending_xref)\n    assert result[0]['reftarget'] == \"int\"\n\n    # Test parsing a complex type annotation\n    annotation = \"List[Union[str, int]]\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 6\n    assert isinstance(result[0], pending_xref)\n    assert result[0]['reftarget'] == \"List\"\n    assert isinstance(result[2], pending_xref)\n    assert result[2]['reftarget'] == \"Union\"\n    assert isinstance(result[4], pending_xref)\n    assert result[4]['reftarget'] == \"int\"\n\n    # Test parsing a Literal type annotation\n    annotation = \"Literal['a', 'b', 'c']\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 7\n    assert isinstance(result[0], pending_xref)\n    assert result[0]['reftarget'] == \"Literal\"\n    assert isinstance(result[2], nodes.literal)\n    assert result[2].astext() == \"'a'\"\n    assert isinstance(result[4], nodes.literal)\n    assert result[4].astext() == \"'b'\"\n    assert isinstance(result[6], nodes.literal)\n    assert result[6].astext() == \"'c'\"\n"], "sample_282": ["    def test_bound_field_initial_value(self):\n        form = ComplexFieldForm(initial={'field1': ['John', ['J'], '2022-01-01 12:00:00']})\n        bound_field = form['field1']\n        self.assertEqual(bound_field.initial, ['John', ['J'], datetime(2022, 1, 1, 12, 0, 0)])\n", "def test_bound_field_with_initial_data(self):\n    form = ComplexFieldForm(initial={'field1': ['John Doe', ['J', 'P'], datetime(2022, 1, 1, 12, 0, 0)]})\n    bound_field = form['field1']\n    self.assertEqual(bound_field.initial, ['John Doe', ['J', 'P'], datetime(2022, 1, 1, 12, 0, 0)])\n", "    def test_bound_field_with_initial(self):\n        form = ComplexFieldForm(initial={'field1': ['John', ['J'], datetime(1940, 10, 9, 12, 0, 0)]})\n        bound_field = form['field1']\n        self.assertEqual(bound_field.initial, ['John', ['J'], datetime(1940, 10, 9, 12, 0, 0)])\n", "def test_partially_required_field(self):\n    form = PartiallyRequiredForm(data={'f_0': 'John'})\n    self.assertTrue(form.is_valid())\n    self.assertEqual(form.cleaned_data['f'], 'John,')\n\n    form = PartiallyRequiredForm(data={})\n    self.assertFalse(form.is_valid())\n    self.assertEqual(form.errors['f'], ['This field is required.'])\n", "    def test_boundfield_str(self):\n        form = ComplexFieldForm(data={'field1_0': 'John', 'field1_1': ['J', 'R'], 'field1_2_0': '2022-01-01', 'field1_2_1': '12:00:00'})\n        bound_field = BoundField(form, form.fields['field1'], 'field1')\n        self.assertEqual(str(bound_field), bound_field.as_widget())\n"], "sample_1077": ["def test_complex_region_intersection():\n    a = Interval(2, 3)\n    b = Interval(4, 5)\n    c = Interval(1, 7)\n    c1 = ComplexRegion(a*b)\n    c2 = ComplexRegion(b*c)\n    intersection = c1.intersect(c2)\n    assert intersection == ComplexRegion(b*Intersection(a, c))\n", "def test_complex_region_intersection():\n    a = Interval(2, 3)\n    b = Interval(4, 6)\n    c = Interval(5, 7)\n    c1 = ComplexRegion(a * b)\n    c2 = ComplexRegion(b * c)\n    intersection = c1.intersect(c2)\n    assert intersection == ComplexRegion(b * Intersection(b, c))\n", "def test_range_symbolic():\n    n = Symbol('n', integer=True)\n    r = Range(n, n + 20, 3)\n    assert r.inf == n\n    assert list(r) == [n + i*3 for i in range(7)]\n", "def test_complex_region_intersection():\n    a = Interval(2, 3)\n    b = Interval(4, 6)\n    c = Interval(5, 7)\n    c1 = ComplexRegion(a*b)\n    c2 = ComplexRegion(b*c)\n    intersection = c1.intersect(c2)\n    expected = ComplexRegion(ProductSet(Interval(5, 6), Interval(4, 6)), False)\n    assert intersection == expected\n", "def test_complex_region_containment():\n    a = Interval(2, 3)\n    b = Interval(4, 5)\n    c1 = ComplexRegion(a*b)\n    assert (2.5 + 4.5*I in c1) == True\n    assert (2.5 + 6.5*I in c1) == False\n"], "sample_58": ["def test_has_changed(self):\n    data = {\n        'first_name': 'John',\n        'last_name': 'Doe',\n        'birthday': '2000-01-01',\n    }\n    form = Person(initial=data, data=data)\n    self.assertFalse(form.has_changed())\n\n    data['first_name'] = 'Jane'\n    form = Person(initial=data, data=data)\n    self.assertTrue(form.has_changed())\n", "def test_form_initial_values(self):\n    form_data = {'first_name': 'John', 'last_name': 'Doe', 'birthday': '1990-01-01'}\n    form = Person(initial=form_data)\n    for field_name, value in form_data.items():\n        self.assertEqual(form.fields[field_name].initial, value)\n", "    def test_form_rendering(self):\n        data = {'first_name': 'John', 'last_name': 'Doe', 'birthday': '1990-01-01'}\n        form = Person(data)\n        self.assertTrue(form.is_valid())\n        self.assertIn('<input type=\"text\" name=\"first_name\" value=\"John\" required id=\"id_first_name\">', str(form))\n        self.assertIn('<input type=\"text\" name=\"last_name\" value=\"Doe\" required id=\"id_last_name\">', str(form))\n        self.assertIn('<input type=\"text\" name=\"birthday\" value=\"1990-01-01\" required id=\"id_birthday\">', str(form))\n", "    def test_add_prefix(self):\n        f = Person(prefix='person')\n        self.assertEqual(f.add_prefix('first_name'), 'person-first_name')\n", "    def test_form_is_valid(self):\n        form_data = {\n            'first_name': 'John',\n            'last_name': 'Doe',\n            'birthday': '1990-01-01',\n        }\n        form = Person(data=form_data)\n        self.assertTrue(form.is_valid())\n"], "sample_401": ["def test_formset_with_initial_data(self):\n    initial_data = [{\"choice\": \"option1\", \"votes\": 10}, {\"choice\": \"option2\", \"votes\": 20}]\n    formset = self.make_choiceformset(initial=initial_data)\n    self.assertEqual(formset.total_form_count(), len(initial_data))\n    for i, form in enumerate(formset):\n        self.assertEqual(form.initial, initial_data[i])\n", "def test_formset_with_non_form_error(self):\n    formset = ChoiceFormsetWithNonFormError({})\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(\n        formset.non_form_errors(),\n        [\"non-form error\"],\n    )\n", "    def test_custom_kwarg_in_form(self):\n        CustomKwargFormSet = formset_factory(CustomKwargForm, formset=BaseFormSet, extra=2)\n        formset = CustomKwargFormSet(form_kwargs={'custom_kwarg': 'test'})\n        for form in formset:\n            self.assertEqual(form.custom_kwarg, 'test')\n", "def test_custom_form_kwargs(self):\n    \"\"\"\n    Formset factory passes custom kwargs to forms.\n    \"\"\"\n    CustomKwargFormSet = formset_factory(CustomKwargForm, extra=1, form_kwargs={\"custom_kwarg\": \"value\"})\n    formset = CustomKwargFormSet()\n    for form in formset:\n        self.assertEqual(form.custom_kwarg, \"value\")\n", "def test_formset_factory_extra_kwargs(self):\n    CustomKwargFormSet = formset_factory(CustomKwargForm, formset=BaseFormSet, form_kwargs={\"custom_kwarg\": \"foo\"})\n    formset = CustomKwargFormSet()\n    for form in formset:\n        self.assertEqual(form.custom_kwarg, \"foo\")\n"], "sample_573": ["def test_polyfit_gridsize(self, df):\n    stat = PolyFit(gridsize=50)\n    result = stat(df, GroupBy(df, \"group\"), None, None)\n    assert result[\"x\"].nunique() == 50\n", "def test_polyfit_single_group(self, df):\n    groupby = GroupBy(df, \"group\")\n    stat = PolyFit(order=2)\n    result = stat(df, groupby, None, None)\n\n    assert isinstance(result, pd.DataFrame)\n    assert \"x\" in result.columns\n    assert \"y\" in result.columns\n    assert \"group\" in result.columns\n    assert result[\"group\"].nunique() == 2\n    assert result[\"x\"].nunique() == stat.gridsize\n    assert result[\"y\"].nunique() > 2\n", "def test_polyfit_with_data(self, df):\n    groupby = GroupBy(df, group=\"color\")\n    scales = {\"x\": None, \"y\": None}\n    poly_fit = PolyFit(order=2, gridsize=100)\n    result = poly_fit(df, groupby, orient=None, scales=scales)\n\n    # Check that the result is a DataFrame\n    assert isinstance(result, pd.DataFrame)\n\n    # Check that the result contains 'x' and 'y' columns\n    assert 'x' in result.columns\n    assert 'y' in result.columns\n\n    # Check that the 'x' column has 'gridsize' number of unique values\n    assert result['x'].nunique() == poly_fit.gridsize\n\n    # Check that the 'y' column does not contain NaN values\n    assert not result['y'].isnull().values.any()\n", "def test_polyfit_order(self, df):\n    pf = PolyFit(order=3)\n    result = pf(df, GroupBy(df, \"color\"), orient=None, scales=None)\n    assert result.shape[0] == 3 * pf.gridsize\n    for group in df[\"color\"].unique():\n        group_data = df[df[\"color\"] == group]\n        if group_data[\"x\"].nunique() > pf.order:\n            p = np.polyfit(group_data[\"x\"], group_data[\"y\"], pf.order)\n            xx = np.linspace(group_data[\"x\"].min(), group_data[\"x\"].max(), pf.gridsize)\n            yy = np.polyval(p, xx)\n            expected = pd.DataFrame(dict(x=xx, y=yy))\n            assert_frame_equal(result[result[\"color\"] == group].reset_index(drop=True), expected)\n", "def test_poly_fit_groupby(self, df):\n    groupby = GroupBy(df, \"group\")\n    stat = PolyFit(order=2, gridsize=50)\n    result = stat(df, groupby, None, None)\n\n    assert isinstance(result, pd.DataFrame)\n    assert \"x\" in result.columns\n    assert \"y\" in result.columns\n    assert \"group\" in result.columns\n    assert result.group.nunique() == 2\n    assert len(result) == 2 * stat.gridsize\n\n    # Check that the fitted curves are not empty for each group\n    assert not result.loc[result.group == \"x\", \"y\"].isnull().all()\n    assert not result.loc[result.group == \"y\", \"y\"].isnull().all()\n"], "sample_474": ["    def setUpTestData(cls):\n        cls.author = Author.objects.create(name=\"John Smith\", alias=\"smithj\")\n", "    def setUpTestData(cls):\n        cls.john = Author.objects.create(name=\"John Smith\", alias=\"smithj\")\n        cls.elena = Author.objects.create(name=\"\u00c9lena Jordan\", alias=\"elena\")\n        cls.rhonda = Author.objects.create(name=\"Rhonda\")\n", "    def setUpTestData(cls):\n        cls.john = Author.objects.create(name=\"John Smith\", alias=\"smithj\")\n        cls.elena = Author.objects.create(name=\"\u00c9lena Jordan\", alias=\"elena\")\n        cls.rhonda = Author.objects.create(name=\"Rhonda\")\n", "    def setUpTestData(cls):\n        cls.john = Author.objects.create(name=\"John Smith\", alias=\"smithj\")\n        cls.elena = Author.objects.create(name=\"\u00c9lena Jordan\", alias=\"elena\")\n        cls.rhonda = Author.objects.create(name=\"Rhonda\")\n", "    def setUpTestData(cls):\n        cls.john = Author.objects.create(name=\"John Smith\", alias=\"smithj\")\n        cls.elena = Author.objects.create(name=\"\u00c9lena Jordan\", alias=\"elena\")\n        cls.rhonda = Author.objects.create(name=\"Rhonda\")\n"], "sample_468": ["def test_render_context(self):\n    render_context = RenderContext()\n    render_context.push({'key1': 'value1'})\n    self.assertEqual(render_context['key1'], 'value1')\n\n    with render_context.push_state(None):\n        render_context.push({'key2': 'value2'})\n        self.assertEqual(render_context['key2'], 'value2')\n        self.assertEqual(render_context['key1'], 'value1')\n\n    self.assertNotIn('key2', render_context)\n    self.assertEqual(render_context['key1'], 'value1')\n", "def test_make_context_with_request(self):\n    request = HttpRequest()\n    data = {\"key\": \"value\"}\n    context = make_context(data, request=request)\n\n    self.assertIsInstance(context, RequestContext)\n    self.assertEqual(context[\"key\"], \"value\")\n    self.assertEqual(context.request, request)\n", "    def test_render_context_push_state(self):\n        template = Template(\"\")\n        render_context = RenderContext()\n\n        with render_context.push_state(template, isolated_context=True):\n            render_context[\"test_key\"] = \"test_value\"\n\n        self.assertEqual(render_context[\"test_key\"], \"test_value\")\n\n        with render_context.push_state(template, isolated_context=False):\n            render_context[\"test_key\"] = \"new_value\"\n\n        self.assertEqual(render_context[\"test_key\"], \"new_value\")\n", "def test_request_context_template_binding(self):\n    template = Template(\"Hello, {{ user.username }}!\")\n    request = HttpRequest()\n    request.user = mock.Mock(username='TestUser')\n\n    context = RequestContext(request)\n    with context.bind_template(template):\n        rendered = template.render(context)\n        self.assertEqual(rendered, \"Hello, TestUser!\")\n\n    with self.assertRaises(RuntimeError):\n        with context.bind_template(template):\n            pass\n", "    def test_request_context(self):\n        request = HttpRequest()\n        engine = Engine()\n        template = Template(\"{{ variable }}\", engine=engine)\n        context = RequestContext(request, {\"variable\": \"value\"})\n\n        # Ensure context processors are not initialized\n        self.assertNotIn(\"user\", context)\n        self.assertNotIn(\"csrf_token\", context)\n\n        # Render template with context and check output\n        with context.bind_template(template):\n            output = template.render(context)\n            self.assertEqual(output, \"value\")\n\n            # Ensure context processors are initialized now\n            self.assertIn(\"user\", context)\n            self.assertIn(\"csrf_token\", context)\n"], "sample_939": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_1023": ["def test_cycle_length():\n    func = lambda i: (i**2 + 1) % 51\n    lam, mu = next(cycle_length(func, 4))\n    assert lam == 6 and mu == 2\n", "def test_cycle_length():\n    func = lambda i: (i**2 + 1) % 51\n    mu, lam = next(cycle_length(func, 4))\n    assert mu == 6 and lam == 2\n", "def test_sieve_totientrange():\n    s = Sieve()\n    assert list(s.totientrange(1, 11)) == [1, 1, 2, 2, 4, 2, 6, 4, 6, 4]\n", "def test_nextprime_with_ith():\n    assert nextprime(2, ith=3) == 11\n    assert nextprime(10, ith=2) == 13\n    assert nextprime(20, ith=1) == 23\n", "def test_compositepi():\n    assert compositepi(1) == 0\n    assert compositepi(3) == 0\n    assert compositepi(4) == 1\n    assert compositepi(10) == 8\n    assert compositepi(20) == 17\n    assert compositepi(100) == 78\n"], "sample_777": ["def check_regression_toy(presort, loss):\n    # Check regression on a toy dataset.\n    reg = GradientBoostingRegressor(loss=loss, n_estimators=10,\n                                    random_state=1, presort=presort)\n\n    assert_raises(ValueError, reg.predict, T)\n\n    reg.fit(X, y)\n    assert_array_almost_equal(reg.predict(T), true_result)\n    assert_equal(10, len(reg.estimators_))\n\n    deviance_decrease = (reg.train_score_[:-1] - reg.train_score_[1:])\n    assert np.any(deviance_decrease >= 0.0)\n\n    leaves = reg.apply(X)\n    assert_equal(leaves.shape, (6, 10))\n", "def test_regression_toy(presort, loss):\n    # Check regression on a toy dataset.\n    reg = GradientBoostingRegressor(loss=loss, n_estimators=10,\n                                    random_state=1, presort=presort)\n\n    assert_raises(ValueError, reg.predict, T)\n\n    reg.fit(X, y)\n    assert_array_almost_equal(reg.predict(T), true_result, decimal=1)\n    assert_equal(10, len(reg.estimators_))\n\n    deviance_decrease = (reg.train_score_[:-1] - reg.train_score_[1:])\n    assert np.any(deviance_decrease >= 0.0)\n\n    leaves = reg.apply(X)\n    assert_equal(leaves.shape, (6, 10))\n", "def check_classification_iris(presort, loss):\n    # Check classification on the iris dataset.\n    clf = GradientBoostingClassifier(loss=loss, n_estimators=10,\n                                     random_state=1, presort=presort)\n\n    clf.fit(iris.data, iris.target)\n    score = clf.score(iris.data, iris.target)\n    assert_greater(score, 0.95)\n\n    leaves = clf.apply(iris.data)\n    assert_equal(leaves.shape, (iris.data.shape[0], 10, 1))\n", "def check_regression_toy(presort):\n    # Check regression on a toy dataset.\n    reg = GradientBoostingRegressor(n_estimators=10, random_state=1, presort=presort)\n\n    assert_raises(ValueError, reg.predict, T)\n\n    reg.fit(X, y)\n    assert_array_almost_equal(reg.predict(T), true_result, decimal=1)\n    assert_equal(10, len(reg.estimators_))\n\n    mse_decrease = (reg.train_score_[:-1] - reg.train_score_[1:])\n    assert np.any(mse_decrease >= 0.0)\n\n    leaves = reg.apply(X)\n    assert_equal(leaves.shape, (6, 10))\n", "def test_gradient_boosting_regressor_sparse_input(presort):\n    # Check regression with sparse input\n    X, y = make_regression(n_samples=100, n_features=100, random_state=0)\n    X_csc = csc_matrix(X)\n    X_csr = csr_matrix(X)\n    X_coo = coo_matrix(X)\n\n    reg = GradientBoostingRegressor(random_state=0, presort=presort)\n    reg.fit(X_csc, y)\n    pred_csc = reg.predict(X_csc)\n\n    reg.fit(X_csr, y)\n    pred_csr = reg.predict(X_csr)\n\n    reg.fit(X_coo, y)\n    pred_coo = reg.predict(X_coo)\n\n    assert_array_almost_equal(pred_csc, pred_csr)\n    assert_array_almost_equal(pred_csc, pred_coo)\n"], "sample_505": ["def test_auto_date_locator_interval_multiples():\n    locator = mdates.AutoDateLocator(interval_multiples=True)\n    locator.intervald[mdates.DAILY] = [1, 2, 4, 7, 14]\n    dmin = datetime.datetime(2022, 1, 1)\n    dmax = datetime.datetime(2022, 1, 31)\n    locator.get_locator(dmin, dmax)\n    assert locator._byranges[mdates.DAILY] == [1, 8, 15, 22]\n", "def test_rrulewrapper():\n    tz = dateutil.tz.gettz('America/Los_Angeles')\n    dtstart = datetime.datetime(2022, 1, 1, 0, 0, 0, tzinfo=tz)\n    until = datetime.datetime(2022, 1, 31, 0, 0, 0, tzinfo=tz)\n    rrule = mdates.rrulewrapper(mdates.DAILY, dtstart=dtstart, until=until, byweekday=mdates.MO, tzinfo=tz)\n    assert rrule.between(dtstart, until) == [datetime.datetime(2022, 1, 3, 0, 0, tzinfo=tz), datetime.datetime(2022, 1, 10, 0, 0, tzinfo=tz), datetime.datetime(2022, 1, 17, 0, 0, tzinfo=tz), datetime.datetime(2022, 1, 24, 0, 0, tzinfo=tz), datetime.datetime(2022, 1, 31, 0, 0, tzinfo=tz)]\n", "def test_AutoDateLocator_interval_multiples():\n    # Test AutoDateLocator with interval_multiples=True\n    locator = mdates.AutoDateLocator(interval_multiples=True)\n    dmin = datetime.datetime(2022, 1, 1)\n    dmax = datetime.datetime(2022, 1, 31)\n    locator.set_view_interval(mdates.date2num(dmin), mdates.date2num(dmax))\n    ticks = locator()\n    # Check that ticks are multiples of the interval\n    interval = locator._get_interval()\n    assert all(tick % interval == 0 for tick in ticks)\n", "def test_rrulewrapper_timezone_aware_datetimes():\n    rrule = mdates.rrulewrapper(mdates.HOURLY, dtstart=datetime.datetime(2022, 1, 1, 0, 0, 0, tzinfo=dateutil.tz.gettz('UTC')),\n                                until=datetime.datetime(2022, 1, 1, 12, 0, 0, tzinfo=dateutil.tz.gettz('UTC')),\n                                byminute=0, bysecond=0)\n    assert rrule.between(datetime.datetime(2022, 1, 1, 6, 0, 0, tzinfo=dateutil.tz.gettz('UTC')),\n                         datetime.datetime(2022, 1, 1, 18, 0, 0, tzinfo=dateutil.tz.gettz('UTC'))) == [\n        datetime.datetime(2022, 1, 1, 6, 0, 0, tzinfo=dateutil.tz.tzutc()),\n        datetime.datetime(2022, 1, 1, 7, 0, 0, tzinfo=dateutil.tz.tzutc()),\n        # ... continue with the rest of the hours\n        datetime.datetime(2022, 1, 1, 18, 0, 0, tzinfo=dateutil.tz.tzutc())\n    ]\n", "def test_rrulewrapper():\n    # Test initialization of rrulewrapper with timezone\n    tzinfo = dateutil.tz.gettz('America/New_York')\n    rrule = mdates.rrulewrapper(mdates.HOURLY, tzinfo=tzinfo)\n    assert rrule._tzinfo == tzinfo\n\n    # Test setting attributes of rrulewrapper\n    rrule.set(byminute=30)\n    assert rrule._rrule._byminute == [30]\n\n    # Test that rrulewrapper returns datetime objects with the correct timezone\n    dt = rrule.after(datetime.datetime(2022, 1, 1, 0, 0, tzinfo=tzinfo))\n    assert dt.tzinfo == tzinfo\n    assert dt.hour == 0 and dt.minute == 30\n"], "sample_899": ["    def __init__(self, alpha=0.1):\n        self.alpha = alpha\n", "    def set_params(self, old_parameter=None, **kwargs):\n        if old_parameter is not None:\n            kwargs['new_parameter'] = old_parameter\n        return super(CheckSetParamsWithDeprecatedParameter, self).set_params(**kwargs)\n", "def test_check_fit_score_takes_y():\n    # test for an estimator that does not implement score\n    estimator = KNeighborsRegressor()\n    check_fit_score_takes_y('KNeighborsRegressor', estimator)\n", "    def test_check_fit_score_takes_y(self):\n        check_fit_score_takes_y(\"LinearRegression\", LinearRegression())\n", "    def test_not_invariant_transform(self):\n        # Test that the transformer has invariant output size when applied on\n        # mini batches or the whole set\n        estimator = NotInvariantPredict()\n        X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n        y = np.array([1, 2, 3])\n\n        # Fit the estimator\n        estimator.fit(X, y)\n\n        # Transform the whole set\n        result_full = estimator.transform(X)\n\n        # Transform mini batches\n        result_by_batch = [estimator.transform(batch.reshape(1, -1))\n                           for batch in X]\n\n        # The output should be the same size whether applied on the whole set or mini batches\n        np.testing.assert_array_equal(result_full.shape, result_by_batch[0].shape)\n"], "sample_721": ["def test_check_memory():\n    # Test check_memory function with various inputs\n    memory = check_memory(None)\n    assert isinstance(memory, sklearn.externals.joblib.Memory)\n\n    memory = check_memory(\"./cache_dir\")\n    assert isinstance(memory, sklearn.externals.joblib.Memory)\n    assert memory.location == \"./cache_dir\"\n\n    memory = check_memory(sklearn.externals.joblib.Memory(cachedir=\"./cache_dir\"))\n    assert isinstance(memory, sklearn.externals.joblib.Memory)\n    assert memory.location == \"./cache_dir\"\n\n    with pytest.raises(ValueError):\n        check_memory(\"invalid_input\")\n", "def test_check_array_accept_sparse_none():\n    with warnings.catch_warnings(record=True) as w:\n        X = sp.csr_matrix([[1, 2], [3, 4]])\n        X_checked = check_array(X, accept_sparse=None)\n        assert_array_equal(X.toarray(), X_checked)\n        assert len(w) == 1\n        assert \"Passing 'None' to parameter 'accept_sparse' in methods\" in str(w[0].message)\n", "def test_check_array_sparse_matrix_complex_data():\n    X_complex = sp.csr_matrix([[1+1j, 2], [3, 4]])\n    with assert_raises(ValueError, match=\"Complex data not supported\"):\n        check_array(X_complex)\n", "def test_check_consistent_length():\n    # Test with arrays of consistent length\n    arr1 = np.array([1, 2, 3])\n    arr2 = np.array([4, 5, 6])\n    check_consistent_length(arr1, arr2)\n\n    # Test with arrays of inconsistent length\n    arr3 = np.array([1, 2, 3, 4])\n    with pytest.raises(ValueError):\n        check_consistent_length(arr1, arr3)\n\n    # Test with None values\n    check_consistent_length(arr1, None)\n", "def test_check_memory():\n    # Testing with None\n    memory = check_memory(None)\n    assert isinstance(memory, sklearn.externals.joblib.Memory)\n\n    # Testing with a string\n    temp_dir = NamedTemporaryFile().name\n    memory = check_memory(temp_dir)\n    assert isinstance(memory, sklearn.externals.joblib.Memory)\n    assert memory.location == temp_dir\n\n    # Testing with an object with the joblib.Memory interface\n    class MockMemory:\n            self.cache = {}\n\n    memory = check_memory(MockMemory())\n    assert isinstance(memory, MockMemory)\n\n    # Testing with an object without the joblib.Memory interface\n    with pytest.raises(ValueError):\n        check_memory(\"invalid_memory\")\n"], "sample_915": ["def test_stringify_signature():\n    sig = inspect.signature_from_str(\"(a: int, b: str = 'default') -> bool\")\n    assert stringify_signature(sig) == \"(a: int, b: str = 'default') -> bool\"\n    assert stringify_signature(sig, show_annotation=False) == \"(a, b = 'default')\"\n    assert stringify_signature(sig, show_return_annotation=False) == \"(a: int, b: str = 'default')\"\n", "def test_signature_from_str():\n    sig = inspect.signature_from_str(\"(a: int, b: str = 'default', *args, **kwargs) -> None\")\n    assert str(sig) == \"(a: int, b: str = 'default', *args, **kwargs) -> None\"\n    assert list(sig.parameters) == ['a', 'b', 'args', 'kwargs']\n    assert sig.parameters['a'].annotation == int\n    assert sig.parameters['b'].default == 'default'\n    assert sig.return_annotation == None\n", "def test_stringify_signature():\n    sig = stringify_signature(inspect.signature(datetime.datetime.now))\n    assert sig == '()'\n\n    sig = stringify_signature(inspect.signature(functools.partial(lambda x: x + 1)))\n    assert sig == '(x)'\n\n    sig = stringify_signature(inspect.signature(datetime.datetime.strptime), show_annotation=False)\n    assert sig == '(date_string, format)'\n\n    sig = stringify_signature(inspect.signature(datetime.datetime.strptime), show_return_annotation=False)\n    assert sig == '(date_string: str, format: str) -> datetime.datetime'\n\n    sig = stringify_signature(inspect.signature(datetime.datetime.strptime), show_annotation=False, show_return_annotation=False)\n    assert sig == '(date_string, format)'\n", "def test_stringify_signature():\n    sig = inspect.signature_from_str(\"(a: int, b: str = 'default', *args: Any, **kwargs: Any) -> None\")\n    assert stringify_signature(sig) == \"(a: int, b: str = 'default', *args: Any, **kwargs: Any) -> None\"\n    assert stringify_signature(sig, show_annotation=False) == \"(a, b = 'default', *args, **kwargs)\"\n    assert stringify_signature(sig, show_return_annotation=False) == \"(a: int, b: str = 'default', *args: Any, **kwargs: Any)\"\n    assert stringify_signature(sig, show_annotation=False, show_return_annotation=False) == \"(a, b = 'default', *args, **kwargs)\"\n", "def test_signature():\n    # Test stringify_signature function\n    sig = inspect.signature_from_str(\"(a: int, b: str = 'default')\")\n    assert stringify_signature(sig) == \"(a: int, b: str = 'default')\"\n    assert stringify_signature(sig, show_annotation=False) == \"(a, b='default')\"\n    assert stringify_signature(sig, show_return_annotation=False) == \"(a: int, b: str = 'default')\"\n    assert stringify_signature(sig, show_annotation=False, show_return_annotation=False) == \"(a, b='default')\"\n\n    # Test stringify_signature function with return annotation\n    sig = inspect.signature_from_str(\"(a: int, b: str = 'default') -> bool\")\n    assert stringify_signature(sig) == \"(a: int, b: str = 'default') -> bool\"\n    assert stringify_signature(sig, show_annotation=False) == \"(a, b='default') -> bool\"\n    assert stringify_signature(sig, show_return_annotation=False) == \"(a: int, b: str = 'default')\"\n    assert stringify_signature(sig, show_annotation=False, show_return_annotation=False) == \"(a, b='default')\"\n"], "sample_649": ["def test_logging_plugin_create_formatter(pytester: Pytester, caplog: LogCaptureFixture) -> None:\n    pytester.makeconftest(\n        \"\"\"\n        import logging\n        from _pytest.logging import LoggingPlugin\n\n            config = caplog._config\n            plugin = LoggingPlugin(config)\n            formatter = plugin._create_formatter(\"%(levelname)s:%(message)s\", \"%H:%M:%S\", None)\n            assert isinstance(formatter, logging.Formatter)\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)\n", "def test_log_capture_fixture_clear(request: FixtureRequest):\n    result = LogCaptureFixture(request.node, _ispytest=True)\n    result.handler.records.append(\"test_record\")  # Adding a dummy record for testing\n    result.handler.stream.write(\"test_text\")\n\n    result.clear()\n\n    assert len(result.handler.records) == 0\n    assert result.handler.stream.getvalue() == \"\"\n", "def test_log_capture_fixture_text(caplog: LogCaptureFixture, capfd):\n    import logging\n\n    logger = logging.getLogger(\"test_logger\")\n    logger.setLevel(logging.DEBUG)\n    logger.debug(\"Debug message\")\n    logger.info(\"Info message\")\n    logger.warning(\"Warning message\")\n    logger.error(\"Error message\")\n    logger.critical(\"Critical message\")\n\n    out, err = capfd.readouterr()\n    assert \"Debug message\" in caplog.text\n    assert \"Info message\" in caplog.text\n    assert \"Warning message\" in caplog.text\n    assert \"Error message\" in caplog.text\n    assert \"Critical message\" in caplog.text\n    assert out == \"\"\n    assert err == \"\"\n", "def test_set_log_path(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n\n            caplog.set_log_path(\"test.log\")\n            assert os.path.exists(\"test.log\")\n            logger = logging.getLogger(__name__)\n            logger.warning(\"Test warning message\")\n            with open(\"test.log\", \"r\") as f:\n                log_content = f.read()\n            assert \"Test warning message\" in log_content\n        \"\"\"\n    )\n\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)\n", "def test_pytest_runtestloop_verbose(pytester: Pytester, monkeypatch, log_cli_enabled, verbose):\n    # Setup the pytester environment\n    pytester.makepyfile(\n        \"\"\"\n            pass\n        \"\"\"\n    )\n\n    config = pytester.parseconfig()\n    config.option.verbose = verbose\n    plugin = LoggingPlugin(config)\n    session = pytester._getsession(config)\n\n    # Mock _log_cli_enabled method\n    monkeypatch.setattr(plugin, \"_log_cli_enabled\", lambda: log_cli_enabled)\n\n    # Run the pytest_runtestloop hook\n    with plugin.pytest_runtestloop(session):\n        pass\n\n    # Check if verbose option is updated correctly\n    assert config.option.verbose == (1 if log_cli_enabled and verbose < 1 else verbose)\n"], "sample_680": ["def test_evaluate_xfail_marks_without_conditions(self, pytestconfig):\n    @pytest.mark.xfail(reason=\"expected failure\")\n        pass\n\n    item = pytestconfig.getitemcolslected(test_func)\n    xfailed = evaluate_xfail_marks(item)\n    assert xfailed is not None\n    assert xfailed.reason == \"expected failure\"\n    assert xfailed.run is True\n    assert xfailed.strict is False\n    assert xfailed.raises is None\n", "def test_evaluate_skip_marks_with_reason(self):\n    @pytest.mark.skip(reason=\"test reason\")\n        pass\n\n    item = pytest.Item.from_parent(None, name=\"test_func\")\n    item.add_marker(pytest.mark.skip(reason=\"test reason\"))\n    skip = evaluate_skip_marks(item)\n    assert skip is not None\n    assert skip.reason == \"test reason\"\n", "def test_evaluate_xfail_marks(self, pytestconfig):\n    @pytest.mark.xfail(condition=sys.platform == \"win32\", reason=\"xfail on win32\")\n        assert False\n\n    item = pytestconfig.getitemcollector().collect(test_func)\n    result = evaluate_xfail_marks(item)\n\n    if sys.platform == \"win32\":\n        assert result is not None\n        assert result.reason == \"xfail on win32\"\n        assert result.run == True\n        assert result.strict == False\n        assert result.raises is None\n    else:\n        assert result is None\n", "def test_evaluate_xfail_marks(pytestconfig):\n    @pytest.mark.xfail(sys.platform == \"win32\", reason=\"does not run on windows\")\n        assert False, \"this test should be marked as xfail\"\n\n    item, _ = pytestconfig._makeitem(test_function, \"test_function\")\n    xfail = evaluate_xfail_marks(item)\n    assert xfail is not None\n    assert xfail.reason == \"does not run on windows\"\n    assert xfail.run is True\n", "def test_evaluate_xfail_marks(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.xfail(reason=\"known issue\")\n            assert False\n\n        @pytest.mark.xfail(condition=sys.platform == \"win32\", reason=\"windows issue\")\n            assert False\n        \"\"\"\n    )\n\n    result = testdir.runpytest()\n\n    result.stdout.fnmatch_lines(\n        [\n            \"*.test_xfailed XFAIL*known issue\",\n            \"*.test_xfailed_on_windows XFAIL*windows issue\",\n        ]\n    )\n"], "sample_551": ["def test_line_collection_2d_to_3d():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n\n    x = np.linspace(0, 1, 100)\n    y = np.sin(x * 2 * np.pi) / 2 + 0.5\n    segments = [[[i, y[i], 0], [i, y[i], 0.5]] for i in range(len(x))]\n\n    lc = art3d.LineCollection(segments, zsort='average')\n    art3d.line_collection_2d_to_3d(lc)\n    ax.add_collection3d(lc)\n\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    ax.set_zlim(0, 1)\n    plt.close(fig)\n", "def test_patch_collection_2d_to_3d():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    patches = [plt.Circle((0, 0), radius=0.5), plt.Circle((1, 1), radius=0.5)]\n    collection = art3d.PatchCollection(patches, zs=[0, 1], zdir='z', depthshade=True)\n    ax.add_collection3d(collection)\n    ax.set_xlim(-1, 2)\n    ax.set_ylim(-1, 2)\n    ax.set_zlim(-1, 2)\n    plt.close(fig)\n", "def test_patch_collection_2d_to_3d():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    x = np.array([[0, 0], [1, 1]])\n    y = np.array([[0, 1], [0, 1]])\n    z = np.array([[0, 0], [0, 0]])\n    col = ax.scatter(x, y, z, c='b')\n    art3d.patch_collection_2d_to_3d(col, zs=0, zdir='z')\n    assert same_color(col._facecolors3d, [0, 0, 1, 1])\n", "def test_pathpatch_2d_to_3d():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n\n    p = plt.Path.circle((0, 0), radius=1)\n    pp = plt.PathPatch(p, facecolor='none', edgecolor='b')\n    ax.add_patch(pp)\n\n    art3d.pathpatch_2d_to_3d(pp, z=0.5, zdir='z')\n\n    ax.set_xlim(-2, 2)\n    ax.set_ylim(-2, 2)\n    ax.set_zlim(-2, 2)\n\n    plt.close(fig)\n", "def test_line3d_demo():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n\n    z = np.linspace(0, 1, 100)\n    x = z * np.sin(20 * z)\n    y = z * np.cos(20 * z)\n\n    ax.plot3D(x, y, z, 'gray')\n\n    z = np.linspace(0, 1, 100)\n    x = z * np.sin(24 * z)\n    y = z * np.cos(24 * z)\n\n    ax.plot3D(x, y, z, 'r')\n\n    plt.show()\n"], "sample_839": ["def test_hashing_vectorizer_fit_transform():\n    vec = HashingVectorizer(n_features=10)\n    X = vec.fit_transform(ALL_FOOD_DOCS)\n    assert X.shape == (11, 10)\n", "def test_tfidf_vectorizer_stop_words():\n    vectorizer = TfidfVectorizer(stop_words=\"english\")\n    X = vectorizer.fit_transform(ALL_FOOD_DOCS)\n    assert \"the\" not in vectorizer.get_feature_names()\n", "def test_hashing_vectorizer_parameters():\n    docs = [\"the quick brown fox\", \"jumps over the lazy dog\"]\n    vectorizer = HashingVectorizer(n_features=10, alternate_sign=False)\n    X = vectorizer.fit_transform(docs)\n    assert X.shape == (2, 10)\n    assert X.dtype == np.float64\n    assert sparse.issparse(X)\n\n    vectorizer = HashingVectorizer(n_features=10, alternate_sign=True)\n    X_alt = vectorizer.fit_transform(docs)\n    assert_array_almost_equal(X.toarray(), np.abs(X_alt.toarray()), decimal=10)\n", "def test_vectorizer_input_validation():\n    # Test that the vectorizer raises an error when input is invalid\n    vectorizer = CountVectorizer()\n    with assert_raises(ValueError, match=\"Iterable over raw text documents expected, string object received.\"):\n        vectorizer.fit_transform(\"This is a single string, not a collection of documents.\")\n\n    with assert_raises(ValueError, match=\"Iterable over raw text documents expected, string object received.\"):\n        vectorizer.transform(\"This is a single string, not a collection of documents.\")\n", "def test_hashing_vectorizer_alternate_sign():\n    X = [\n        \"This is the first document.\",\n        \"This document is the second document.\",\n        \"And this is the third one.\",\n        \"Is this the first document?\",\n    ]\n    vectorizer = HashingVectorizer(n_features=2**4, alternate_sign=True)\n    X_transformed = vectorizer.fit_transform(X)\n    assert sparse.isspmatrix_csr(X_transformed)\n    assert X_transformed.shape == (4, 16)\n    assert np.all(np.abs(X_transformed.data) == 1)\n\n    vectorizer_no_alternate_sign = HashingVectorizer(n_features=2**4, alternate_sign=False)\n    X_transformed_no_alternate_sign = vectorizer_no_alternate_sign.fit_transform(X)\n    assert not np.all(np.abs(X_transformed_no_alternate_sign.data) == 1)\n"], "sample_782": ["def test_column_transformer_transform_1d_array():\n    transformer = ColumnTransformer(transformers=[('trans', Trans(), [0])])\n    X = np.array([1, 2, 3])\n    X_transformed = transformer.fit_transform(X)\n    assert_array_equal(X_transformed, np.atleast_2d(X).T)\n", "def test_column_transformer_get_params():\n    ct = ColumnTransformer(\n        [(\"norm1\", Normalizer(norm='l1'), [0, 1]),\n         (\"norm2\", Normalizer(norm='l2'), slice(2, 4))]\n    )\n    expected_params = {\n        \"norm1__norm\": \"l1\",\n        \"norm1__copy\": True,\n        \"norm2__norm\": \"l2\",\n        \"norm2__copy\": True,\n        \"remainder\": \"drop\",\n        \"sparse_threshold\": 0.3,\n        \"n_jobs\": None,\n        \"transformer_weights\": None\n    }\n    assert_dict_equal(ct.get_params(), expected_params)\n", "def test_column_transformer_weight():\n    X = np.array([[0., 1., 2., 2.],\n                  [1., 1., 0., 1.]])\n    ct = ColumnTransformer(\n        transformers=[(\"norm\", Normalizer(norm='l1'), [0, 1]),\n                      (\"double\", DoubleTrans(), slice(2, 4))],\n        transformer_weights={'norm': 0.5, 'double': 2.0})\n    X_transformed = ct.fit_transform(X)\n    expected_transformed = np.array([[0., 0.5, 1., 4.],\n                                     [0.5, 0.5, 0., 2.]])\n    assert_array_equal(X_transformed, expected_transformed)\n", "def test_column_transformer_weighted_transform():\n    ct = ColumnTransformer(transformers=[('double', DoubleTrans(), [0])],\n                           transformer_weights={'double': 2.0})\n    X = np.array([[1], [2]])\n    X_trans = ct.fit_transform(X)\n    assert_array_equal(X_trans, np.array([[2], [4]]))\n", "def test_column_transformer_with_passthrough():\n    transformer = ColumnTransformer(\n        transformers=[\n            ('trans', Trans(), [0]),\n            ('passthrough', 'passthrough', [1])\n        ]\n    )\n\n    X = np.array([[1, 2], [3, 4]])\n    X_transformed = transformer.fit_transform(X)\n\n    expected_transformed = np.array([[1], [3], [2], [4]])\n    assert_array_equal(X_transformed, expected_transformed)\n"], "sample_945": ["def test_parse_annotation():\n    env = Mock()\n    env.config.python_use_unqualified_type_names = False\n    result = _parse_annotation('List[str]', env)\n    assert len(result) == 5\n    assert isinstance(result[0], nodes.Text)\n    assert result[0].astext() == 'List'\n    assert isinstance(result[1], pending_xref)\n    assert result[1]['reftarget'] == 'List'\n    assert isinstance(result[2], addnodes.desc_sig_punctuation)\n    assert result[2].astext() == '['\n    assert isinstance(result[3], nodes.Text)\n    assert result[3].astext() == 'str'\n    assert isinstance(result[4], addnodes.desc_sig_punctuation)\n    assert result[4].astext() == ']'\n", "def test_parse_annotation_with_ellipsis():\n    annotation = \"int ...\"\n    env = Mock()\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 3\n    assert isinstance(result[0], pending_xref)\n    assert result[0].astext() == \"int\"\n    assert isinstance(result[1], nodes.Text)\n    assert result[1].astext() == \" \"\n    assert isinstance(result[2], addnodes.desc_sig_punctuation)\n    assert result[2].astext() == \"...\"\n", "def test_parse_annotation(app, env):\n    annotation = \"List[Union[int, str]]\"\n    result = _parse_annotation(annotation, env)\n    assert_node(result, [\n        pending_xref,\n        desc_sig_punctuation,\n        pending_xref,\n        desc_sig_punctuation,\n        desc_sig_punctuation,\n        desc_sig_punctuation,\n        pending_xref,\n        desc_sig_punctuation,\n    ])\n    assert result[0]['reftarget'] == 'List'\n    assert result[2]['reftarget'] == 'Union'\n    assert result[6]['reftarget'] == 'int'\n    assert result[8]['reftarget'] == 'str'\n", "def test_parse_annotation(app, status, warning):\n    env = Mock()\n    env.config.python_use_unqualified_type_names = False\n\n    # Test a simple type annotation\n    result = _parse_annotation(\"int\", env)\n    assert len(result) == 1\n    assert isinstance(result[0], pending_xref)\n    assert result[0].get('reftarget') == \"int\"\n\n    # Test a nested type annotation\n    result = _parse_annotation(\"List[Tuple[int, str]]\", env)\n    assert len(result) == 7\n    assert result[0].get('reftarget') == \"List\"\n    assert result[2].get('reftarget') == \"Tuple\"\n    assert result[4].get('reftarget') == \"int\"\n    assert result[6].get('reftarget') == \"str\"\n", "def test_parse_annotation(app, env):\n    annotation = \"List[Dict[str, Union[int, str]]]\"\n    result = _parse_annotation(annotation, env)\n    assert isinstance(result, list)\n    assert len(result) == 15\n    assert isinstance(result[0], pending_xref)\n    assert result[0]['reftarget'] == 'List'\n    assert isinstance(result[2], addnodes.desc_sig_punctuation)\n    assert result[2].astext() == '['\n    assert isinstance(result[4], pending_xref)\n    assert result[4]['reftarget'] == 'Dict'\n    assert isinstance(result[10], pending_xref)\n    assert result[10]['reftarget'] == 'Union'\n    assert isinstance(result[14], addnodes.desc_sig_punctuation)\n    assert result[14].astext() == ']'\n"], "sample_571": ["def test_estimate_data_with_x_estimator(self):\n    plotter = lm._RegressionPlotter(x='x', y='y', data=self.df, x_estimator=np.mean)\n    vals, points, cis = plotter.estimate_data\n    assert len(vals) == len(points) == len(cis)\n", "def test_regplot_with_ci(self):\n    ax = lm.regplot(x=\"x\", y=\"y\", data=self.df, ci=95)\n    plt.close()\n", "def test_regplot_with_dataframe_and_column_names():\n    ax = lm.regplot(x='x', y='y', data=TestLinearPlotter.df)\n    assert isinstance(ax, mpl.axes.Axes)\n", "def test_regplot_with_categorical_x(self):\n    ax = lm.regplot(x=\"s\", y=\"y\", data=self.df)\n    assert isinstance(ax, mpl.axes.Axes)\n    plt.close()\n", "def test_residplot_with_partial_variables(self):\n    ax = lm.residplot(x=\"x\", y=\"z\", data=self.df,\n                      x_partial=\"d\", y_partial=\"y\")\n    assert isinstance(ax, mpl.axes.Axes)\n"], "sample_163": ["    def test_password_reset_view(self):\n        response = self.client.get(reverse(\"password_reset\"))\n        self.assertEqual(response.status_code, 200)\n        self.assertTemplateUsed(response, \"registration/password_reset_form.html\")\n\n        response = self.client.post(reverse(\"password_reset\"), {\"email\": self.u1.email})\n        self.assertEqual(response.status_code, 302)\n        self.assertRedirects(response, reverse(\"password_reset_done\"))\n\n        self.assertEqual(len(mail.outbox), 1)\n        email = mail.outbox[0]\n        self.assertEqual(email.subject, \"Password reset on example.com\")\n        self.assertEqual(email.to, [self.u1.email])\n        uid = re.search(r\"/reset/(?P<uidb64>[0-9A-Za-z_\\-]+)/\", email.body).group(\"uidb64\")\n        token = re.search(r\"/reset/\\S+/(?P<token>[0-9A-Za-z]{1,13}-[0-9A-Za-z]{1,20})/\", email.body).group(\"token\")\n\n        response = self.client.get(reverse(\"password_reset_confirm\", args=[uid, token]))\n        self.assertEqual(response.status_code, 200)\n        self.assertTemplateUsed(response, \"registration/password_reset_confirm.html\")\n\n        new_password = \"new_password\"\n        response = self.client.post(reverse(\"password_reset_confirm\", args=[uid, token]), {\"new_password1\": new_password, \"new_password2\": new_password})\n        self.assertEqual(response.status_code, 302)\n        self.assertRedirects(response, reverse(\"password_reset_complete\"))\n\n        self.u1.refresh_from_db()\n        self.assertTrue(self.u1.check_password(new_password))\n", "    def test_login_view_redirect_authenticated_user(self):\n        self.client.login(username=\"testclient\", password=\"password\")\n        response = self.client.get(reverse(\"login\"))\n        self.assertRedirects(response, settings.LOGIN_REDIRECT_URL)\n", "    def test_password_change(self):\n        self.login()\n        response = self.client.get(reverse('password_change'))\n        self.assertEqual(response.status_code, 200)\n        self.assertTemplateUsed(response, 'registration/password_change_form.html')\n\n        response = self.client.post(reverse('password_change'), {\n            'old_password': 'password',\n            'new_password1': 'newpassword123',\n            'new_password2': 'newpassword123',\n        })\n        self.assertEqual(response.status_code, 302)\n        self.assertRedirects(response, reverse('password_change_done'))\n\n        self.assertTrue(self.u1.check_password('newpassword123'))\n", "    def test_logout_view_get(self):\n        self.login()\n        with ignore_warnings(category=RemovedInDjango50Warning):\n            response = self.client.get(\"/logout/\")\n        self.assertTemplateUsed(response, \"registration/logged_out.html\")\n", "    def test_password_reset_view(self):\n        response = self.client.get(reverse('password_reset'))\n        self.assertEqual(response.status_code, 200)\n        self.assertIsInstance(response.context['form'], PasswordResetForm)\n\n        response = self.client.post(reverse('password_reset'), {'email': 'testclient@example.com'})\n        self.assertEqual(response.status_code, 302)\n        self.assertRedirects(response, reverse('password_reset_done'))\n        self.assertEqual(len(mail.outbox), 1)\n\n        # Extract the reset URL from the email body\n        email_lines = mail.outbox[0].body.splitlines()\n        reset_url_line = [line for line in email_lines if '/reset/' in line][0]\n        reset_url = re.search(r'(https?://\\S+)', reset_url_line).group(0)\n\n        response = self.client.get(reset_url)\n        self.assertEqual(response.status_code, 200)\n        self.assertIsInstance(response.context['form'], SetPasswordForm)\n\n        new_password = 'new_password'\n        response = self.client.post(reset_url, {'new_password1': new_password, 'new_password2': new_password})\n        self.assertEqual(response.status_code, 302)\n        self.assertRedirects(response, reverse('password_reset_complete'))\n\n        # Login with the new password\n        self.client.login(username='testclient', password=new_password)\n        response = self.client.get('/admin/')\n        self.assertEqual(response.status_code, 200)\n"], "sample_637": ["    def test_use_symbolic_message_instead(self):\n        config = {'enable': ('I0023',)}\n        with set_config(config):\n            node = self.checker.linter.astroid_cache.build_file('file.py', '''", "    def test_use_symbolic_message_instead(self):\n        with set_config(disable=[\"W0511\"]):\n            self.checker.linter._by_id_managed_msgs.append(\n                (\"file.py\", \"W0511\", \"fixme\", 1, True)\n            )\n            node = self.checker.linter.get_ast(\"file.py\", \"content\")\n            self.checker.process_module(node)\n            msg = self.linter.release_messages()\n            self.assertEqual(len(msg), 1)\n            self.assertEqual(msg[0].msg_id, \"use-symbolic-message-instead\")\n            self.assertEqual(msg[0].symbol, \"file.py\")\n            self.assertEqual(msg[0].line, 1)\n", "    def test_note_pattern_match(self):\n        checker = self.create_checker()\n        checker.config.notes = ['NOTE']\n        checker.config.notes_rgx = None\n        checker.open()\n        with self.assertAddsMessages(\n            MessageTest(msg_id='fixme', line=1, args='# NOTE: This is a note'),\n        ):\n            checker.process_tokens([_tokenize_str('# NOTE: This is a note')])\n", "    def test_use_symbolic_message_instead(self):\n        with set_config(self.checker.config, notes=(\"FIXME\",)):\n            module_content = \"\"\"\n            # pylint: disable=C0103\n                x = 1  # FIXME: This is a fixme comment\n            \"\"\"\n            tokens = _tokenize_str(module_content)\n            self.checker.process_tokens(tokens)\n            self.checker.process_module(self.walker.ast)\n            self.assertEqual(len(self.linter.release_messages()), 0)\n\n            self.linter._by_id_managed_msgs.append(('__init__', 'I0023', 'use-symbolic-message-instead', 4, True))\n            self.checker.process_module(self.walker.ast)\n            self.assertEqual(len(self.linter.release_messages()), 1)\n            msg = self.linter.release_messages()[0]\n            self.assertEqual(msg.msg_id, 'I0023')\n            self.assertEqual(msg.line, 4)\n", "    def test_encoding_issue(self):\n        module = self.checker.process_tokens(_tokenize_str(u\"\"\"\n        # -*- coding: utf-8 -*-\n        print(u\"Hello, world!\")  # FIXME: Non-ASCII characters in the comment\n        print(\"Hello, world!\")  # This comment is fine\n        \"\"\"))\n        with self.assertAddsMessages(MessageTest(msg_id='fixme', line=3)):\n            self.checker.process_module(module)\n"], "sample_667": ["def test_ensure_relative_to_basetemp(tmp_path_factory):\n    # Test that _ensure_relative_to_basetemp raises ValueError for an absolute path\n    with pytest.raises(ValueError):\n        tmp_path_factory._ensure_relative_to_basetemp(str(Path(\"/absolute/path\")))\n\n    # Test that _ensure_relative_to_basetemp returns the basename for a relative path\n    assert tmp_path_factory._ensure_relative_to_basetemp(\"relative/path\") == \"relative/path\"\n\n    # Test that _ensure_relative_to_basetemp raises ValueError for a path that escapes the base temp directory\n    with pytest.raises(ValueError):\n        tmp_path_factory._ensure_relative_to_basetemp(\"../escaped/path\")\n", "def test_tmp_path_exists_and_is_dir(tmp_path_factory, tmp_path):\n    assert tmp_path.exists()\n    assert tmp_path.is_dir()\n    assert tmp_path.parent == tmp_path_factory.getbasetemp()\n", "def test_getbasetemp(basetemp, given_basetemp, expected_basetemp, monkeypatch):\n    monkeypatch.setattr(pathlib, \"get_user\", lambda: \"test_user\" if given_basetemp is None else None)\n    factory = TempPathFactory(given_basetemp=given_basetemp, trace=None, _basetemp=basetemp)\n    assert str(factory.getbasetemp()) == expected_basetemp\n", "def test_mktemp_with_numbered_false(tmp_path_factory):\n    basename = \"test_dir\"\n    p = tmp_path_factory.mktemp(basename, numbered=False)\n    assert p.exists()\n    assert p.is_dir()\n    assert p.name == basename\n\n    # Test that calling mktemp with the same basename and numbered=False raises an error\n    with pytest.raises(FileExistsError):\n        tmp_path_factory.mktemp(basename, numbered=False)\n", "def test_temp_path_factory_given_basetemp(given_basetemp, expected_basetemp):\n    class Config:\n        class option:\n            basetemp = given_basetemp\n\n        class trace:\n            @staticmethod\n                pass\n\n    tf = TempPathFactory.from_config(Config())\n    assert str(tf._given_basetemp) == expected_basetemp\n"], "sample_337": ["    def test_get_token_with_use_sessions(self):\n        request = HttpRequest()\n        request.session = SessionStore()\n        request.session.save()\n        settings.CSRF_USE_SESSIONS = True\n        self.assertIsNone(request.session.get(CSRF_SESSION_KEY))\n        token = get_token(request)\n        self.assertIsInstance(token, str)\n        self.assertEqual(len(token), CSRF_TOKEN_LENGTH)\n        self.assertEqual(request.session.get(CSRF_SESSION_KEY), token)\n        settings.CSRF_USE_SESSIONS = False\n", "    def test_token_rotation(self):\n        request = HttpRequest()\n        token1 = get_token(request)\n        rotate_token(request)\n        token2 = get_token(request)\n        self.assertNotEqual(token1, token2)\n", "    def test_get_token_with_session(self):\n        request = HttpRequest()\n        request.session = SessionStore()\n        request.session[CSRF_SESSION_KEY] = MASKED_TEST_SECRET1\n        settings.CSRF_USE_SESSIONS = True\n        token = get_token(request)\n        self.assertEqual(token, MASKED_TEST_SECRET1)\n", "    def test_bad_referer_check(self):\n        request = HttpRequest()\n        request.META = {'HTTP_REFERER': 'http://bad.example.com'}\n        request.is_secure = lambda: True\n        request.get_host = lambda: 'example.com'\n\n        middleware = CsrfViewMiddleware()\n        middleware.allowed_origin_subdomains = {'https': ['example.com']}\n\n        with self.assertRaises(RejectRequest) as context:\n            middleware._check_referer(request)\n\n        self.assertEqual(context.exception.reason, REASON_BAD_REFERER % request.META['HTTP_REFERER'])\n", "    def test_token_rotation(self):\n        request = HttpRequest()\n        request.META['CSRF_COOKIE'] = MASKED_TEST_SECRET1\n        csrf_token1 = get_token(request)\n        self.assertIn(request.META['CSRF_COOKIE'], [MASKED_TEST_SECRET1, MASKED_TEST_SECRET2])\n\n        # Rotate the token\n        request.session = SessionStore()  # Session is required for token rotation\n        request.session.save()\n        CsrfViewMiddleware().rotate_token(request)\n        csrf_token2 = get_token(request)\n        self.assertNotEqual(csrf_token1, csrf_token2)\n        self.assertNotIn(request.META['CSRF_COOKIE'], [MASKED_TEST_SECRET1, MASKED_TEST_SECRET2])\n"], "sample_59": ["def test_model_save_base_with_update_fields(self):\n    obj = Model1(field1='initial', field2='initial')\n    obj.save()\n\n    obj.field1 = 'updated'\n    obj.save_base(update_fields=['field1'])\n\n    obj.refresh_from_db()\n    self.assertEqual(obj.field1, 'updated')\n    self.assertEqual(obj.field2, 'initial')\n", "def test_model_save_base_force_update_no_pk(self):\n    instance = Model1()\n    with self.assertRaises(ValueError):\n        instance.save_base(force_update=True)\n", "def test_model_check_managers(self):\n    class CustomManager(models.Manager):\n            return [models.checks.Error(\"Custom manager check failed.\", obj=self, id='models.E999')]\n\n    class CheckModel(models.Model):\n        objects = CustomManager()\n\n    errors = CheckModel.check()\n    self.assertEqual(len(errors), 1)\n    self.assertEqual(errors[0].id, 'models.E999')\n", "def test_model_save_update_fields(self):\n    article = Article.objects.create(title='Test Article', body='Initial body')\n    self.assertEqual(article.body, 'Initial body')\n\n    article.body = 'Updated body'\n    article.save(update_fields=['body'])\n\n    article.refresh_from_db()\n    self.assertEqual(article.body, 'Updated body')\n    self.assertEqual(article.title, 'Test Article')  # Ensure title didn't change\n", "    def test_validate_unique_with_date_fields(self):\n        # Test unique_for_date, unique_for_year, and unique_for_month validation\n        date = datetime.date(2022, 1, 1)\n        Event.objects.create(name='Event 1', date=date, unique_for_date_field='Unique Date 1')\n        Event.objects.create(name='Event 2', date=date, unique_for_year_field='Unique Year 1')\n        Event.objects.create(name='Event 3', date=date, unique_for_month_field='Unique Month 1')\n\n        # Test unique_for_date validation error\n        with self.assertRaisesMessage(ValidationError, \"Event with this Date and Unique for date field already exists.\"):\n            Event.objects.create(name='Event 4', date=date, unique_for_date_field='Unique Date 1')\n\n        # Test unique_for_year validation error\n        with self.assertRaisesMessage(ValidationError, \"Event with this Unique for year field and Date year already exists.\"):\n            Event.objects.create(name='Event 5', date=date, unique_for_year_field='Unique Year 1')\n\n        # Test unique_for_month validation error\n        with self.assertRaisesMessage(ValidationError, \"Event with this Unique for month field and Date month already exists.\"):\n            Event.objects.create(name='Event 6', date=date, unique_for_month_field='Unique Month 1')\n"], "sample_1085": ["def test_rational_as_content_primitive():\n    r = Rational(3, 4)\n    c, p = r.as_content_primitive()\n    assert c == Rational(3, 4)\n    assert p == 1\n\n    r = Rational(-3, 4)\n    c, p = r.as_content_primitive()\n    assert c == Rational(3, 4)\n    assert p == -1\n\n    r = Rational(0, 4)\n    c, p = r.as_content_primitive()\n    assert c == 1\n    assert p == 0\n", "def test_sympify_rational():\n    from fractions import Fraction\n    f = Fraction(1, 2)\n    r = sympify(f)\n    assert r == Rational(1, 2)\n    assert isinstance(r, Rational)\n", "def test_imaginary_unit_properties():\n    assert I * I == -1\n    assert I * I * I == -I\n    assert I ** 4 == 1\n    assert I ** 0 == 1\n    assert I ** (1 / 2) == I / sqrt(2)\n    assert I ** (3 / 2) == -I / sqrt(2)\n    assert I ** (2 / 3) == exp(2 * I * pi / 3)\n    assert I ** (4 / 3) == exp(4 * I * pi / 3)\n    assert I ** (1 / 3) == exp(I * pi / 3)\n    assert I ** (2 / 3) == exp(2 * I * pi / 3)\n", "def test_algebraic_number_as_expr():\n    a = AlgebraicNumber(sqrt(2))\n    assert a.as_expr() == sqrt(2)\n    assert a.as_expr(t) == t\n", "def test_number_properties():\n    assert S.One.is_positive\n    assert not S.One.is_negative\n    assert S.Zero.is_nonpositive\n    assert not S.Zero.is_negative\n    assert S.Zero.is_nonnegative\n    assert not S.Zero.is_positive\n    assert S.NegativeOne.is_negative\n    assert not S.NegativeOne.is_positive\n    assert not S.NegativeOne.is_nonnegative\n    assert S.NegativeOne.is_nonpositive\n    assert S.Half.is_positive\n    assert not S.Half.is_negative\n    assert not S.Half.is_nonpositive\n    assert S.Half.is_nonnegative\n    assert S.Infinity.is_infinite\n    assert not S.Infinity.is_finite\n    assert S.Infinity.is_positive\n    assert not S.Infinity.is_negative\n    assert not S.Infinity.is_nonpositive\n    assert S.Infinity.is_nonnegative\n    assert S.Infinity.is_extended_real\n    assert not S.Infinity.is_extended_positive\n    assert not S.Infinity.is_extended_negative\n    assert S.NegativeInfinity.is_infinite\n    assert not S.NegativeInfinity.is_finite\n    assert not S.NegativeInfinity.is_positive\n    assert S.NegativeInfinity.is_negative\n    assert S.NegativeInfinity.is_nonpositive\n    assert not S.NegativeInfinity.is_nonnegative\n    assert S.NegativeInfinity.is_extended_real\n    assert not S.NegativeInfinity.is_extended_positive\n    assert not S.NegativeInfinity.is_extended_negative\n    assert S.ComplexInfinity.is_infinite\n    assert not S.ComplexInfinity.is_finite\n    assert not S.ComplexInfinity.is_positive\n    assert not S.ComplexInfinity.is_negative\n    assert not S.ComplexInfinity.is_nonpositive\n    assert not S."], "sample_54": ["    def test_file_response_set_headers(self):\n        content = b'File content'\n        with tempfile.NamedTemporaryFile(delete=False) as temp:\n            temp.write(content)\n            temp_name = temp.name\n\n        with open(temp_name, 'rb') as file:\n            response = FileResponse(file)\n            response.set_headers(file)\n\n            self.assertEqual(response['Content-Length'], str(len(content)))\n            self.assertEqual(response['Content-Type'], 'application/octet-stream')\n            self.assertEqual(response['Content-Disposition'], 'inline; filename=\"{}\"'.format(os.path.basename(temp_name)))\n\n        os.remove(temp_name)\n", "    def test_file_response_headers(self):\n        file_content = b\"This is some file content.\"\n        with tempfile.NamedTemporaryFile(delete=False) as temp:\n            temp.write(file_content)\n            temp_path = temp.name\n\n        response = FileResponse(open(temp_path, 'rb'))\n        response.set_headers(open(temp_path, 'rb'))\n\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response['Content-Type'], 'text/plain')\n        self.assertEqual(response['Content-Length'], str(len(file_content)))\n        self.assertIn('filename=\"', response['Content-Disposition'])\n\n        os.remove(temp_path)\n", "def test_file_response_headers(self):\n    test_file = ContentFile(b\"Test content\", name=\"test_file.txt\")\n    response = FileResponse(test_file, as_attachment=True)\n    self.assertEqual(response['Content-Length'], str(len(test_file.read())))\n    self.assertEqual(response['Content-Type'], 'text/plain')\n    self.assertEqual(response['Content-Disposition'], 'attachment; filename=\"test_file.txt\"')\n\n    test_file.seek(0)  # Reset the file pointer\n    response = FileResponse(test_file, as_attachment=False, filename=\"another_test_file.txt\")\n    self.assertEqual(response['Content-Disposition'], 'inline; filename=\"another_test_file.txt\"')\n", "    def test_file_response_with_streaming_content(self):\n        streaming_content = (chunk for chunk in [b'Hello, ', b'World!'])\n        response = FileResponse(streaming_content, content_type='text/plain')\n        self.assertTrue(response.streaming)\n        self.assertEqual(b'Hello, World!', b''.join(response.streaming_content))\n", "def test_file_response_with_contentfile(self):\n    content = b\"This is a test content\"\n    file = ContentFile(content)\n    response = FileResponse(file)\n    self.assertEqual(response.getvalue(), content)\n    self.assertEqual(response['Content-Length'], str(len(content)))\n    self.assertEqual(response['Content-Disposition'], 'inline')\n    file.close()\n"], "sample_491": ["    def test_boundfield_with_custom_attrs(self):\n        form = FrameworkForm(data={\"name\": \"Django\", \"language\": \"P\"})\n        bound_field = form[\"name\"]\n        widget_attrs = bound_field.build_widget_attrs({\"class\": \"custom-class\", \"placeholder\": \"Enter name\"})\n        self.assertEqual(widget_attrs, {\"class\": \"custom-class\", \"placeholder\": \"Enter name\", \"required\": True})\n", "def test_bound_field_rendering(self):\n    # Test rendering of a BoundField with various widgets\n    form = FrameworkForm(data={\"name\": \"Django\", \"language\": \"P\"})\n    field = form[\"name\"]\n\n    # Test rendering as a text input\n    rendered_text = field.as_text()\n    self.assertIn('type=\"text\"', rendered_text)\n    self.assertIn('name=\"name\"', rendered_text)\n    self.assertIn('value=\"Django\"', rendered_text)\n\n    # Test rendering as a textarea\n    rendered_textarea = field.as_textarea()\n    self.assertIn('<textarea', rendered_textarea)\n    self.assertIn('name=\"name\"', rendered_textarea)\n    self.assertIn('>Django</textarea>', rendered_textarea)\n\n    # Test rendering as a hidden input\n    rendered_hidden = field.as_hidden()\n    self.assertIn('type=\"hidden\"', rendered_hidden)\n    self.assertIn('name=\"name\"', rendered_hidden)\n    self.assertIn('value=\"Django\"', rendered_hidden)\n", "    def test_legend_tag(self):\n        form = FrameworkForm()\n        bound_field = form[\"name\"]\n        contents = \"Legend Content\"\n        tag = bound_field.legend_tag(contents)\n        self.assertIn(contents, tag)\n        self.assertIn(\"legend\", tag)\n", "    def test_bound_widget_choice_label(self):\n        form = SongForm()\n        bound_field = form[\"composers\"]\n        bound_widget = bound_field[0]\n        self.assertEqual(bound_widget.choice_label, \"John Lennon\")\n", "def test_bound_field_build_widget_attrs(self):\n    # Test the build_widget_attrs method of BoundField\n    form = FrameworkForm(data={'name': 'Django', 'language': 'P'})\n    bound_field = form['language']\n\n    # Test case 1: Widget uses required attribute and field is required\n    attrs = bound_field.build_widget_attrs({})\n    self.assertEqual(attrs['required'], True)\n\n    # Test case 2: Widget does not use required attribute\n    bound_field.field.required = False\n    attrs = bound_field.build_widget_attrs({})\n    self.assertNotIn('required', attrs)\n\n    # Test case 3: Field is disabled\n    bound_field.field.disabled = True\n    attrs = bound_field.build_widget_attrs({})\n    self.assertEqual(attrs['disabled'], True)\n\n    # Test case 4: Custom aria-describedby attribute and help_text is used\n    bound_field.field.disabled = False\n    bound_field.field.help_text = 'Help text'\n    attrs = bound_field.build_widget_attrs({'aria-describedby': 'custom_id'})\n    self.assertEqual(attrs['aria-describedby'], 'custom_id')\n"], "sample_327": ["    def test_encoder_decoder(self):\n        class MyEncoder(DjangoJSONEncoder):\n                if isinstance(o, uuid.UUID):\n                    return str(o)\n                return super().default(o)\n\n        class MyForm(Form):\n            json_field = JSONField(encoder=MyEncoder)\n\n        data = {'json_field': json.dumps({'uuid': uuid.uuid4()}, cls=MyEncoder)}\n        form = MyForm(data)\n        self.assertTrue(form.is_valid())\n        self.assertIsInstance(form.cleaned_data['json_field'], dict)\n        self.assertIsInstance(form.cleaned_data['json_field']['uuid'], uuid.UUID)\n", "def test_jsonfield_decoder(self):\n    class Decoder(json.JSONDecoder):\n            obj = super().decode(s, **kwargs)\n            if 'custom_key' in obj:\n                obj['custom_key'] = obj['custom_key'].upper()\n            return obj\n\n    class TestForm(Form):\n        json_field = JSONField(decoder=Decoder)\n\n    form = TestForm(data={'json_field': '{\"custom_key\": \"test\"}'})\n    self.assertTrue(form.is_valid())\n    self.assertEqual(form.cleaned_data['json_field'], {\"custom_key\": \"TEST\"})\n", "    def test_jsonfield_to_python_with_invalid_json(self):\n        field = JSONField()\n        invalid_json = \"{'key': 'value'\"  # missing closing single quote\n        with self.assertRaises(ValidationError):\n            field.to_python(invalid_json)\n", "def test_json_field_serialization(self):\n    class TestForm(Form):\n        json_field = JSONField()\n\n    data = {'json_field': '{\"key\": \"value\"}'}\n    form = TestForm(data)\n    self.assertTrue(form.is_valid())\n    self.assertEqual(form.cleaned_data['json_field'], {'key': 'value'})\n\n    # Test custom encoder\n    class CustomEncoder(DjangoJSONEncoder):\n            if isinstance(o, uuid.UUID):\n                return str(o)\n            return super().default(o)\n\n    data = {'json_field': '{\"key\": \"123e4567-e89b-12d3-a456-426614174000\"}'}\n    form = TestForm(data, fields={'json_field': JSONField(encoder=CustomEncoder)})\n    self.assertTrue(form.is_valid())\n    self.assertEqual(form.cleaned_data['json_field'], {'key': uuid.UUID('123e4567-e89b-12d3-a456-426614174000')})\n\n    # Test invalid JSON\n    data = {'json_field': 'invalid_json'}\n    form = TestForm(data)\n    self.assertFalse(form.is_valid())\n    self.assertEqual(form.errors['json_field'], ['Enter a valid JSON.'])\n", "    def test_to_python(self):\n        field = JSONField()\n        self.assertIsNone(field.to_python(''))\n        self.assertEqual(field.to_python('[]'), [])\n        self.assertEqual(field.to_python('{\"key\": \"value\"}'), {'key': 'value'})\n        self.assertEqual(field.to_python('1'), 1)\n        self.assertEqual(field.to_python('1.5'), 1.5)\n        self.assertEqual(field.to_python(str(uuid.uuid4())), str(uuid.uuid4()))\n        with self.assertRaises(ValidationError):\n            field.to_python('invalid_json')\n"], "sample_450": ["    def test_admin_log_node(\n        self,", "def test_get_admin_log_tag(self):\n    # Test the get_admin_log template tag with a user\n    response = self.client.get(reverse('admin:index'))\n    self.assertContains(response, '<h1>Site administration</h1>')\n    self.assertContains(response, 'get_admin_log 1 as single_log for_user {}'.format(self.user.pk))\n    self.assertContains(response, 'get_admin_log 10 as admin_log for_user user')\n\n    # Test the get_admin_log template tag without a user\n    response = self.client.get(reverse('admin:index'))\n    self.assertContains(response, 'get_admin_log 5 as limited_log')\n\n    # Test the get_admin_log template tag with an invalid limit\n    with self.assertRaises(template.TemplateSyntaxError):\n        template.Template('{% load admin_log %}{% get_admin_log not_a_number as invalid_log %}')\n\n    # Test the get_admin_log template tag with missing arguments\n    with self.assertRaises(template.TemplateSyntaxError):\n        template.Template('{% load admin_log %}{% get_admin_log %}')\n", "    def test_get_admin_log_template_tag(self):\n        response = self.client.get(reverse('admin_utils:test_template'))\n        self.assertContains(response, '<li>Changed something</li>')\n        self.assertContains(response, escape(str(self.user)))\n        self.assertContains(response, escape(str(self.a1)))\n\n        # Test for_user filter\n        response = self.client.get(reverse('admin_utils:test_template'), {'user_id': self.user.pk})\n        self.assertContains(response, '<li>Changed something</li>')\n\n        # Test limit filter\n        response = self.client.get(reverse('admin_utils:test_template'), {'limit': 0})\n        self.assertNotContains(response, '<li>Changed something</li>')\n", "    def test_admin_log_with_user(self):\n        response = self.client.get(reverse('admin:index'))\n        self.assertContains(response, '{% get_admin_log 10 as admin_log for_user super %}')\n        self.assertContains(response, '<li>Changed something</li>')\n", "    def test_get_admin_log_for_user(self):\n        url = reverse('admin:log_entries') + f'?user={self.user.pk}'\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n        self.assertContains(response, f'<td class=\"action-flag\">{escape(self.user.get_username())}</td>')\n"], "sample_856": ["def test_group_shuffle_split():\n    X, y, groups = np.ones(8), np.ones(8), np.array([1, 1, 2, 2, 2, 3, 3, 3])\n    gss = GroupShuffleSplit(n_splits=2, train_size=.7, random_state=42)\n    assert gss.get_n_splits() == 2\n    train_idx, test_idx = next(gss.split(X, y, groups))\n    assert_array_equal(train_idx, [2, 3, 4, 5, 6, 7])\n    assert_array_equal(test_idx, [0, 1])\n", "def test_shuffle_split_consistency():\n    rng = np.random.RandomState(42)\n    X, y = make_classification(n_samples=100, n_features=20, n_informative=2,\n                               n_redundant=10, n_classes=2, random_state=rng)\n    cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=rng)\n    cv_bis = ShuffleSplit(n_splits=10, test_size=0.2, random_state=rng)\n    for train, test in zip(cv.split(X), cv_bis.split(X)):\n        assert_array_equal(train, test)\n", "def test_group_shuffle_split():\n    X, y = make_classification(n_samples=100, n_features=10, n_informative=5, n_redundant=5, n_classes=3, random_state=42)\n    groups = np.repeat(np.arange(10), 10)\n\n    gss = GroupShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n\n    for train_index, test_index in gss.split(X, y, groups):\n        assert len(set(groups[train_index]).intersection(set(groups[test_index]))) == 0\n        assert len(np.unique(groups[train_index])) == 8\n        assert len(np.unique(groups[test_index])) == 2\n", "def test_train_test_split_sparse_csr():\n    X = csr_matrix([[1, 2], [3, 4], [5, 6]])\n    y = np.array([1, 2, 3])\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n\n    assert isinstance(X_train, csr_matrix)\n    assert isinstance(X_test, csr_matrix)\n    assert_array_equal(y_train, np.array([2, 1]))\n    assert_array_equal(y_test, np.array([3]))\n", "def test_validate_shuffle_split():\n    # Test with valid inputs\n    assert _validate_shuffle_split(100, 0.2, None) == (80, 20)\n    assert _validate_shuffle_split(100, None, 0.8) == (80, 20)\n    assert _validate_shuffle_split(100, 20, None) == (80, 20)\n    assert _validate_shuffle_split(100, None, 80) == (80, 20)\n\n    # Test with invalid inputs\n    assert_raises(ValueError, _validate_shuffle_split, 100, 1.2, None)\n    assert_raises(ValueError, _validate_shuffle_split, 100, -0.2, None)\n    assert_raises(ValueError, _validate_shuffle_split, 100, 0.8, 0.8)\n    assert_raises(ValueError, _validate_shuffle_split, 100, 80, 80)\n    assert_raises(ValueError, _validate_shuffle_split, 100, 100, None)\n    assert_raises(ValueError, _validate_shuffle_split, 100, None, 101)\n    assert_raises(ValueError, _validate_shuffle_split, 100, 0.8, 0.3)\n"], "sample_875": ["def test_class_likelihood_ratios():\n    y_true = np.array([0, 1, 0, 1, 0])\n    y_pred = np.array([1, 1, 0, 0, 0])\n    positive_ratio, negative_ratio = class_likelihood_ratios(y_true, y_pred)\n    assert_almost_equal(positive_ratio, 1.5)\n    assert_almost_equal(negative_ratio, 0.75)\n", "def test_hinge_loss_multilabel():\n    # Test hinge loss for multilabel classification\n    X, y = make_multilabel_classification(random_state=0)\n    clf = svm.LinearSVC(random_state=0)\n    clf.fit(X, y)\n    pred_decision = clf.decision_function(X)\n    labels = np.arange(y.shape[1])\n    loss = hinge_loss(y, pred_decision, labels=labels)\n    assert loss >= 0\n", "def test_balanced_accuracy_score():\n    # Test binary case\n    y_true = [0, 0, 1, 1]\n    y_pred = [0, 1, 0, 1]\n    assert_almost_equal(balanced_accuracy_score(y_true, y_pred), 0.5)\n\n    # Test multiclass case\n    y_true = [0, 1, 2, 0, 1, 2]\n    y_pred = [0, 2, 1, 0, 0, 1]\n    assert_almost_equal(balanced_accuracy_score(y_true, y_pred), 0.444, decimal=3)\n", "def test_cohen_kappa_score(average, weights, normalize, sample_weight, expected):\n    y1 = np.array([1, 1, 1, 2, 2, 2])\n    y2 = np.array([1, 1, 2, 2, 2, 2])\n    score = cohen_kappa_score(y1, y2, weights=weights, sample_weight=sample_weight)\n    assert_almost_equal(score, expected)\n", "def test_hinge_loss_multiclass():\n    X = np.array([[0], [1], [2], [3]])\n    Y = np.array([0, 1, 2, 3])\n    labels = np.array([0, 1, 2, 3])\n    est = svm.LinearSVC()\n    est.fit(X, Y)\n    pred_decision = est.decision_function([[-1], [2], [3]])\n    y_true = [0, 2, 3]\n    assert_almost_equal(hinge_loss(y_true, pred_decision, labels=labels), 0.56, decimal=2)\n"], "sample_980": ["def test_commutes_with():\n    p = Permutation([1, 4, 3, 0, 2, 5])\n    q = Permutation([0, 1, 2, 3, 4, 5])\n    assert p.commutes_with(q) == True\n    q = Permutation([2, 3, 5, 4, 1, 0])\n    assert p.commutes_with(q) == False\n", "def test_rmul_with_af():\n    p = Permutation([1, 0, 2, 3])\n    q = Permutation([2, 3, 1, 0])\n    expected = Permutation([3, 2, 0, 1])\n    assert Permutation.rmul_with_af(p, q) == expected\n", "def test_rmul_with_af():\n    a = Permutation([1, 0, 2])\n    b = Permutation([0, 2, 1])\n    expected = Permutation([1, 2, 0])\n    result = Permutation.rmul_with_af(a, b)\n    assert result == expected\n", "def test_unrank_lex_identity():\n    p = Permutation.unrank_lex(4, 0)\n    assert p == Permutation([0, 1, 2, 3])\n", "def test_unrank_nonlex():\n    # Test unrank_nonlex method\n    p = Permutation.unrank_nonlex(4, 5)\n    assert p == Permutation([2, 0, 3, 1])\n\n    p = Permutation.unrank_nonlex(4, -1)\n    assert p == Permutation([0, 1, 2, 3])\n\n    p = Permutation.unrank_nonlex(3, 0)\n    assert p == Permutation([0, 1, 2])\n\n    p = Permutation.unrank_nonlex(3, 5)\n    assert p == Permutation([2, 1, 0])\n"], "sample_824": ["def test_pairwise_distances_chunked_with_reduce_func():\n    X = np.random.RandomState(0).rand(5, 3)\n    r = .2\n\n        neigh = [np.flatnonzero(d < r) for d in D_chunk]\n        avg_dist = (D_chunk * (D_chunk < r)).mean(axis=1)\n        return neigh, avg_dist\n\n    gen = pairwise_distances_chunked(X, reduce_func=reduce_func)\n    neigh, avg_dist = next(gen)\n\n    expected_neigh = [np.array([0, 3]), np.array([1]), np.array([2]), np.array([0, 3]), np.array([4])]\n    expected_avg_dist = np.array([0.039, 0.000, 0.000, 0.039, 0.000])\n\n    assert_array_equal(neigh, expected_neigh)\n    assert_array_almost_equal(avg_dist, expected_avg_dist)\n", "def test_euclidean_distances_with_sparse_input():\n    X = csr_matrix([[1, 2, 0], [4, 0, 6]])\n    Y = csr_matrix([[1, 2, 3], [4, 5, 6]])\n    expected_distance = np.array([[0.0, 3.0], [5.0, 0.0]])\n    actual_distance = euclidean_distances(X, Y)\n    assert_array_almost_equal(actual_distance, expected_distance)\n", "def test_pairwise_distances_argmin():\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    Y = np.array([[2, 3], [4, 5], [6, 7]])\n    expected_indices = np.array([0, 1, 2])\n    indices = pairwise_distances_argmin(X, Y)\n    assert_array_equal(indices, expected_indices)\n", "def test_check_pairwise_arrays_sparse_dtype_preservation():\n    X_dense = np.array([[1., 2.], [3., 4.]])\n    X_sparse = csr_matrix(X_dense)\n    Y_sparse = csr_matrix(X_dense)\n\n    X_dense_out, Y_dense_out = check_pairwise_arrays(X_dense, Y_sparse)\n    assert issparse(X_dense_out)\n    assert issparse(Y_dense_out)\n\n    X_sparse_out, Y_sparse_out = check_pairwise_arrays(X_sparse, Y_sparse)\n    assert issparse(X_sparse_out)\n    assert issparse(Y_sparse_out)\n", "def test_pairwise_distances_chunked_with_reduce_func():\n    X = np.random.RandomState(0).rand(5, 3)\n    r = .2\n\n        neigh = [np.flatnonzero(d < r) for d in D_chunk]\n        avg_dist = (D_chunk * (D_chunk < r)).mean(axis=1)\n        return neigh, avg_dist\n\n    gen = pairwise_distances_chunked(X, reduce_func=reduce_func)\n    neigh, avg_dist = next(gen)\n    assert isinstance(neigh, list)\n    assert isinstance(avg_dist, np.ndarray)\n    assert len(neigh) == len(avg_dist)\n"], "sample_438": ["    def test_model_save_load(self):\n        question = Question.objects.create(question_text=\"What's up?\")\n        answer = Answer.objects.create(question=question, answer_text=\"Not much.\")\n        pk = answer.pk\n        answer = Answer.objects.get(pk=pk)\n        self.assertEqual(answer.question, question)\n        self.assertEqual(answer.answer_text, \"Not much.\")\n", "def test_model_save(self):\n    # Create a new Question instance\n    question = Question(question_text=\"What is your favorite color?\")\n    question.save()\n\n    # Create a new Answer instance for the Question\n    answer = Answer(question=question, answer_text=\"Blue\")\n    answer.save()\n\n    # Retrieve the Answer instance and check its attributes\n    retrieved_answer = Answer.objects.get(pk=answer.pk)\n    self.assertEqual(retrieved_answer.question, question)\n    self.assertEqual(retrieved_answer.answer_text, \"Blue\")\n", "def test_model_deferred_fields(self):\n    post = Post.objects.create(title='Test Post')\n    post._defer = True\n    post._deferred_fields = {'title'}\n\n    self.assertIn('title', post.get_deferred_fields())\n    self.assertNotIn(Post._meta.pk.attname, post.get_deferred_fields())\n", "def test_prepare_database_save(self):\n    question = Question.objects.create(question_text=\"Test Question\")\n    answer = Answer.objects.create(question=question, answer_text=\"Test Answer\")\n    content_type_id = ContentType.objects.get_for_model(Question).id\n    generic_fk = GenericForeignKey(\"content_type_id\", \"object_id\")\n    generic_fk.instance = answer\n    saved_id = generic_fk.prepare_database_save(generic_fk)\n    self.assertEqual(saved_id, question.id)\n", "def test_model_get_constraints(self):\n    # Test the get_constraints() method of the Model class\n    constraints = Question.get_constraints()\n    self.assertEqual(len(constraints), 1)\n    self.assertEqual(constraints[0][0], Question)\n    self.assertEqual(len(constraints[0][1]), 2)\n    self.assertIsInstance(constraints[0][1][0], models.UniqueConstraint)\n    self.assertIsInstance(constraints[0][1][1], models.CheckConstraint)\n"], "sample_671": ["    def test_xfail_no_run(self, pytestconfig):\n        class FakeItem:\n                self.config = pytestconfig\n                self._store = {}\n                self._store[evalxfail_key] = MarkEvaluator(self, \"xfail\", run=False)\n\n        item = FakeItem()\n        check_xfail_no_run(item)\n\n        # assert that xfail was called with the correct argument\n        assert pytestconfig._store[\"xfail\"].args[0] == \"[NOTRUN] \" + item._store[evalxfail_key].getexplanation()\n", "def test_xfail_no_run(mocker):\n    mock_item = mocker.Mock()\n    mock_item.config.option.runxfail = False\n    mock_item._store = {evalxfail_key: MarkEvaluator(mock_item, \"xfail\", {\"run\": False, \"reason\": \"test reason\"})}\n\n    with pytest.raises(pytest.xfail.Exception) as exc_info:\n        check_xfail_no_run(mock_item)\n    assert str(exc_info.value) == \"[NOTRUN] test reason\"\n", "def test_xfail_no_run(pytestconfig, testdir):\n    pytestconfig.option.runxfail = False\n    testdir.makepyfile(\"\"\"\n        import pytest\n        @pytest.mark.xfail(run=False)\n            assert False\n    \"\"\")\n    result = testdir.runpytest()\n    result.assert_outcomes(skipped=1)\n    result.stdout.fnmatch_lines(['*test_func*: XFAIL*'])\n", "    def test_runtest_setup_skipif_mark(self, request):\n        # Prepare a test item with skipif marker\n        item = request.node\n        item._store = {}\n        item.add_marker(pytest.mark.skipif(\"True\", reason=\"Test skipif marker\"))\n\n        # Run the pytest_runtest_setup hook\n        pytest_runtest_setup(item)\n\n        # Assert that the test was skipped and the correct reason was provided\n        assert item._store[skipped_by_mark_key] is True\n        with pytest.raises(pytest.skip.Exception) as excinfo:\n            runtestprotocol(item, lambda: None)\n        assert excinfo.value.msg == \"Test skipif marker\"\n", "def test_check_strict_xfail_passing(mocker, pytestconfig):\n    class MockItem:\n            self.config = pytestconfig\n            self._store = {}\n            self._store[evalxfail_key] = MarkEvaluator(self, \"xfail\", marks)\n\n    mock_fail = mocker.patch('_pytest.outcomes.fail')\n    pytestconfig.addinivalue_line(\"xfail_strict\", True)\n\n    # Testing with xfail(strict=True) and the test passing\n    mock_item = MockItem(marks=[\"xfail(reason='strict failure', strict=True)\"])\n    check_strict_xfail(mock_item)\n    mock_fail.assert_called_once_with(\"[XPASS(strict)] strict failure\", pytrace=False)\n"], "sample_564": ["def test_wireframe_plot(fig):\n    ax = fig.add_subplot(111, projection='3d')\n    X, Y, Z = axes3d.get_test_data(0.05)\n    ax.plot_wireframe(X, Y, Z, rstride=10, cstride=10)\n", "def test_set_view():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n\n    ax.set_view(elev=30, azim=45)\n\n    assert ax.elev == 30\n    assert ax.azim == 45\n", "def test_scatter3d_random():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    x = np.random.rand(100)\n    y = np.random.rand(100)\n    z = np.random.rand(100)\n    ax.scatter(x, y, z, c='r', marker='o')\n", "def test_wireframe_3d_plot():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    X, Y, Z = axes3d.get_test_data(0.05)\n    ax.plot_wireframe(X, Y, Z, rstride=10, cstride=10)\n", "def test_format_coord_2d():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    ax.plot([0, 1], [0, 1], [0, 1])\n    ax.format_coord = lambda x, y: f\"x={x:.2f}, y={y:.2f}\"\n    ax.format_coord(0.5, 0.5)\n"], "sample_1078": ["def test_indexed_broadcasting():\n    A = IndexedBase('A', shape=(2, 3))\n    B = IndexedBase('B', shape=(3,))\n    i, j = symbols('i j', cls=Idx)\n\n    result = A[i, j] * B[j]\n    expected = A[i, j] * B[Tuple(j)]\n    assert result == expected\n", "def test_indexed_base_properties():\n    A = IndexedBase('A', shape=(x, y))\n    assert A.name == 'A'\n    assert A.assumptions0 == {}\n    assert A.shape == (x, y)\n    assert A.strides is None\n    assert A.offset == S.Zero\n    assert A.label == Symbol('A')\n", "def test_indexed_base_init():\n    A = IndexedBase('A', shape=(x, y))\n    assert A.shape == (x, y)\n    assert A.label == Symbol('A')\n\n    B = IndexedBase(Symbol('B'), shape=(o, p))\n    assert B.shape == (o, p)\n    assert B.label == Symbol('B')\n", "def test_indexed_eval_derivative():\n    A = IndexedBase('A')\n    i, j = symbols('i j', cls=Idx)\n    A_ij = A[i, j]\n\n    # Test derivative with respect to an Indexed object\n    wrt = A[i, j + 1]\n    result = A_ij._eval_derivative(wrt)\n    expected = KroneckerDelta(i, i) * KroneckerDelta(j, j + 1)\n    assert result == expected\n\n    # Test derivative with respect to a non-Indexed object\n    x = symbols('x')\n    result = A_ij._eval_derivative(x)\n    assert result == 0\n\n    # Test derivative with different number of indices\n    B = IndexedBase('B')\n    B_ij = B[i, j]\n    B_ijk = B[i, j, j + 1]\n    with raises(IndexException):\n        B_ij._eval_derivative(B_ijk)\n", "def test_idx_comparison_operators():\n    i = Idx('i', 5)\n    j = Idx('j', 6)\n\n    assert (i <= j) is True\n    assert (i < j) is True\n    assert (j >= i) is True\n    assert (j > i) is True\n\n    assert (i <= i) is True\n    assert (i >= i) is True\n    assert (i == i) is True\n    assert (i != i) is False\n\n    assert (i < i) is False\n    assert (i > i) is False\n\n    assert (i <= 4) is False\n    assert (i < 5) is True\n    assert (i >= 0) is True\n    assert (i > 0) is True\n\n    assert (i <= 5) is True\n    assert (i < 6) is True\n    assert (i >= 5) is True\n    assert (i > 5) is False\n"], "sample_1131": ["def test_loggamma():\n    expr = loggamma(x)\n    result = pycode(expr, printer=SciPyPrinter)\n    expected = \"scipy.special.gammaln(x)\"\n    assert result == expected\n", "def test_print_loggamma():\n    e = loggamma(x)\n    assert pycode(e, standard='python3') == \"sympy.loggamma(x)\"\n", "def test_SymPyPrinter_Print_function():\n    from sympy import print_function, Symbol\n    a = Symbol('a')\n    printer = SymPyPrinter()\n    assert printer._print(print_function(a)) == \"print(a)\"\n", "def test_matrix_solve():\n    A = MatrixSymbol('A', 3, 3)\n    b = MatrixSymbol('b', 3, 1)\n    expr = MatrixSolve(A, b)\n    assert str(NumPyPrinter().doprint(expr)) == \"numpy.linalg.solve(A, b)\"\n", "def test_sympy_printer_KroneckerDelta():\n    printer = SymPyPrinter()\n    expr = KroneckerDelta(x, y)\n    result = printer.doprint(expr)\n    expected = \"KroneckerDelta(x, y)\"\n    assert result == expected\n"], "sample_787": ["def test_brier_score_loss():\n    y_true = np.array([0, 1, 1, 0])\n    y_prob = np.array([0.1, 0.9, 0.8, 0.3])\n    expected_score = 0.037\n    assert_almost_equal(brier_score_loss(y_true, y_prob), expected_score)\n", "def test_brier_score_loss():\n    # Test brier_score_loss with binary labels\n    y_true = np.array([0, 1, 1, 0])\n    y_prob = np.array([0.1, 0.9, 0.8, 0.3])\n    expected_score = 0.037\n    assert_almost_equal(brier_score_loss(y_true, y_prob), expected_score)\n\n    # Test brier_score_loss with custom positive label\n    y_true = np.array([\"spam\", \"ham\", \"ham\", \"spam\"])\n    y_prob = np.array([0.1, 0.9, 0.8, 0.3])\n    expected_score = 0.037\n    assert_almost_equal(brier_score_loss(y_true, y_prob, pos_label=\"ham\"), expected_score)\n\n    # Test brier_score_loss with binary predictions\n    y_true = np.array([0, 1, 1, 0])\n    y_prob = np.array([0, 1, 1, 0])\n    expected_score = 0.0\n    assert_almost_equal(brier_score_loss(y_true, y_prob), expected_score)\n\n    # Test brier_score_loss with sample weights\n    y_true = np.array([0, 1, 1, 0])\n    y_prob = np.array([0.1, 0.9, 0.8, 0.3])\n    sample_weight = np.array([0.5, 1, 0.8, 1.2])\n    expected_score = 0.041\n    assert_almost_equal(brier_score_loss(y_true, y_prob, sample_weight=sample_weight), expected_score)\n\n    # Test brier_score_loss with invalid probabilities\n    y_true = np.array([0, 1, 1, 0])\n    y_prob = np.array([1.1", "def test_multilabel_confusion_matrix():\n    y_true = np.array([[0, 1], [1, 1], [0, 0]])\n    y_pred = np.array([[1, 1], [1, 0], [0, 1]])\n    sample_weight = np.array([1, 1, 2])\n    expected_output = np.array([[[1, 1], [1, 0]], [[0, 0], [1, 1]], [[1, 0], [1, 1]]])\n\n    result = multilabel_confusion_matrix(y_true, y_pred, sample_weight=sample_weight)\n    assert_array_equal(result, expected_output)\n", "def test_cohen_kappa_score():\n    y1 = [1, 2, 3, 1, 2, 3]\n    y2 = [1, 2, 3, 3, 2, 1]\n    assert_almost_equal(cohen_kappa_score(y1, y2), 0.4166666666666667)\n    assert_almost_equal(cohen_kappa_score(y1, y2, weights='linear'), 0.25)\n    assert_almost_equal(cohen_kappa_score(y1, y2, weights='quadratic'), 0.1111111111111111)\n", "def test_log_loss_with_labels():\n    y_true = np.array([0, 1, 2])\n    y_pred = np.array([[0.1, 0.2, 0.7], [0.3, 0.6, 0.1], [0.8, 0.1, 0.1]])\n    labels = np.array([0, 1, 2])\n    expected_loss = 1.6762685784995223\n    assert_almost_equal(log_loss(y_true, y_pred, labels=labels), expected_loss)\n\n    # Test with non-default labels\n    labels = np.array([2, 1, 0])\n    assert_almost_equal(log_loss(y_true, y_pred, labels=labels), expected_loss)\n\n    # Test with incomplete labels\n    labels = np.array([0, 1])\n    with pytest.raises(ValueError):\n        log_loss(y_true, y_pred, labels=labels)\n\n    # Test with invalid labels\n    labels = np.array([0, 1, 3])\n    with pytest.raises(ValueError):\n        log_loss(y_true, y_pred, labels=labels)\n"], "sample_1108": ["def test_is_palindromic():\n    assert is_palindromic([1, 0, 1])\n    assert not is_palindromic('abcbb')\n    assert not is_palindromic('abcbb', 1)\n    assert is_palindromic('abcbb', 1, -1)\n    assert is_palindromic('abcbb', -4, -1)\n", "def test_is_palindromic():\n    assert is_palindromic([1, 0, 1])\n    assert not is_palindromic('abcbb')\n    assert not is_palindromic('abcbb', 1)\n    assert is_palindromic('abcbb', 1, -1)\n    assert is_palindromic('abcbb', -4, -1)\n    assert is_palindromic([1, 2, 3, 2, 1])\n    assert not is_palindromic([1, 2, 3, 4, 1])\n", "def test_is_palindromic():\n    assert is_palindromic([1, 0, 1])\n    assert not is_palindromic('abcbb')\n    assert not is_palindromic('abcbb', 1)\n    assert is_palindromic('abcbb', 1, -1)\n    assert is_palindromic('abcbb', -4, -1)\n    assert is_palindromic('abcbb', 2, -2)\n", "def test_is_palindromic():\n    assert is_palindromic([1, 0, 1]) is True\n    assert is_palindromic('abcbb') is False\n    assert is_palindromic('abcbb', 1) is False\n    assert is_palindromic('abcbb', 1, -1) is True\n    assert is_palindromic('abcbb', -4, -1) is True\n    assert is_palindromic([1, 2, 3, 2, 1]) is True\n    assert is_palindromic([1, 2, 3, 4, 5]) is False\n", "def test_is_palindromic():\n    assert is_palindromic('abcba')\n    assert not is_palindromic('abcab')\n    assert is_palindromic([1, 2, 2, 1])\n    assert not is_palindromic([1, 2, 3, 2, 1])\n    assert is_palindromic('abcba', 1, -1)\n    assert not is_palindromic('abcba', 1, -2)\n"], "sample_820": ["def test_voting_classifier_fit_predict():\n    clf1 = LogisticRegression(solver='lbfgs', multi_class='multinomial', random_state=1)\n    clf2 = RandomForestClassifier(n_estimators=50, random_state=1)\n    clf3 = GaussianNB()\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n\n    eclf1 = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard')\n    eclf1 = eclf1.fit(X_train, y_train)\n    predictions = eclf1.predict(X_test)\n\n    assert len(predictions) == len(y_test)\n", "def test_voting_classifier_multilabel():\n    X, y = make_multilabel_classification(random_state=1)\n    clf1 = SVC(kernel='linear', probability=True)\n    clf2 = KNeighborsClassifier()\n    eclf = VotingClassifier(estimators=[('svc', clf1), ('knn', clf2)], voting='soft')\n    with pytest.raises(NotImplementedError, match='Multilabel and multi-output classification is not supported.'):\n        eclf.fit(X, y)\n", "def test_voting_regressor_with_sample_weights():\n    r1 = LinearRegression()\n    r2 = RandomForestRegressor(n_estimators=10, random_state=1)\n    X_train, X_test, y_train, y_test = train_test_split(X_r, y_r, random_state=1)\n    sample_weight = np.ones(len(y_train))\n    er = VotingRegressor([('lr', r1), ('rf', r2)])\n    er.fit(X_train, y_train, sample_weight=sample_weight)\n    y_pred = er.predict(X_test)\n    assert len(y_pred) == len(y_test)\n", "def test_voting_classifier_weights():\n    clf1 = LogisticRegression(solver='lbfgs', multi_class='multinomial', random_state=1)\n    clf2 = RandomForestClassifier(n_estimators=50, random_state=1)\n    clf3 = GaussianNB()\n    X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n    y = np.array([1, 1, 1, 2, 2, 2])\n    weights = [2, 1, 1]\n    eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n                            voting='soft', weights=weights)\n    eclf = eclf.fit(X, y)\n    pred = eclf.predict(X)\n    expected_pred = np.array([1, 1, 1, 2, 2, 2])\n    assert_array_equal(pred, expected_pred)\n", "def test_voting_classifier_sample_weight():\n    \"\"\"Test that VotingClassifier handles sample_weight correctly.\"\"\"\n    X, y = make_classification(n_samples=100, n_features=2, n_informative=2,\n                               n_redundant=0, random_state=0)\n\n    clf1 = LogisticRegression(solver='lbfgs', multi_class='multinomial', random_state=1)\n    clf2 = RandomForestClassifier(n_estimators=50, random_state=1)\n    eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2)], voting='hard')\n\n    sample_weight = np.random.rand(100)\n    eclf.fit(X, y, sample_weight=sample_weight)\n\n    # Check that the estimators have been fitted correctly\n    assert hasattr(eclf.named_estimators_.lr, 'coef_')\n    assert hasattr(eclf.named_estimators_.rf, 'feature_importances_')\n\n    # Check that predict works with sample_weight\n    assert_array_equal(eclf.predict(X), eclf.predict(X, sample_weight=sample_weight))\n"], "sample_221": ["def test_pickleability_with_values(self):\n    qs = Event.objects.values('id', 'name')\n    self.assert_pickles(qs)\n", "def test_pickle_queryset_with_filter(self):\n    qs = Happening.objects.filter(id=self.happening.id)\n    self.assert_pickles(qs)\n", "def test_pickle_values_queryset(self):\n    qs = Event.objects.values('name', 'start')\n    self.assert_pickles(qs)\n", "def test_queryset_pickleability(self):\n    # Test if a QuerySet instance can be pickled and unpickled correctly\n    qs = Group.objects.all()\n    self.assert_pickles(qs)\n", "def test_queryset_pickling_with_different_versions(self):\n    qs = Happening.objects.all()\n    old_version = django.__version__\n    try:\n        django.__version__ = '2.0.0'  # Simulate a different Django version\n        pickled_qs = pickle.dumps(qs)\n        django.__version__ = old_version  # Restore the original version\n        unpickled_qs = pickle.loads(pickled_qs)\n        self.assertEqual(list(unpickled_qs), list(qs))\n    finally:\n        django.__version__ = old_version  # Ensure the version is restored even if the test fails\n"], "sample_999": ["def test_latex_Quaternion():\n    q = Quaternion(1, 2, 3, 4)\n    assert latex(q) == \"1 + 2\\\\,i + 3\\\\,j + 4\\\\,k\"\n", "compilation error", "compilation error", "def test_latex_trigonometric_functions():\n    assert latex(sin(x)) == \"\\\\sin{\\\\left(x\\\\right)}\"\n    assert latex(cos(x)) == \"\\\\cos{\\\\left(x\\\\right)}\"\n    assert latex(tan(x)) == \"\\\\tan{\\\\left(x\\\\right)}\"\n    assert latex(cot(x)) == \"\\\\cot{\\\\left(x\\\\right)}\"\n    assert latex(sec(x)) == \"\\\\sec{\\\\left(x\\\\right)}\"\n    assert latex(csc(x)) == \"\\\\csc{\\\\left(x\\\\right)}\"\n", "compilation error"], "sample_821": ["def test_affinity_propagation_warnings():\n    # Test that AffinityPropagation raises a warning when all similarities\n    # and preferences are equal\n    X = np.array([[1, 2], [1, 4], [1, 0]])\n    S = -euclidean_distances(X, squared=True)\n    preference = np.median(S)\n    assert_warns(UserWarning, affinity_propagation, S, preference)\n\n    # Test that AffinityPropagation raises a warning when it does not converge\n    S = np.array([[1, 2, 3], [2, 1, 4], [3, 4, 1]])\n    preference = np.median(S)\n    assert_warns(ConvergenceWarning, affinity_propagation, S, preference)\n", "def test_affinity_propagation_with_empty_input():\n    X = np.array([])\n    with pytest.raises(ValueError, match=\"need at least one sample\"):\n        AffinityPropagation().fit(X)\n", "def test_affinity_propagation_single_cluster():\n    X = np.array([[1, 1], [1, 1], [1, 1]])\n    preference = 1.5\n    with assert_warns(ConvergenceWarning):\n        cluster_centers_indices, labels = affinity_propagation(X, preference)\n    assert_array_equal(cluster_centers_indices, [])\n    assert_array_equal(labels, [-1, -1, -1])\n", "def test_affinity_propagation_convergence_warning():\n    # Create a scenario where affinity propagation does not converge\n    S = np.zeros((10, 10))\n    preference = np.median(S)\n\n    # Run affinity propagation\n    with pytest.warns(ConvergenceWarning):\n        affinity_propagation(S, preference)\n", "def test_affinity_propagation():\n    # Test affinity_propagation function\n    S = -euclidean_distances(X, squared=True)\n    preference = np.median(S)\n    cluster_centers_indices, labels, n_iter = affinity_propagation(S, preference, return_n_iter=True)\n    assert len(cluster_centers_indices) == n_clusters\n    assert len(labels) == len(X)\n    assert n_iter > 0\n"], "sample_284": ["def test_max_post_process_passes(self):\n    storage.staticfiles_storage.max_post_process_passes = 1\n    self.assertNotIn('test.css', storage.staticfiles_storage.hashed_files)\n    call_command('collectstatic', verbosity=0, interactive=False)\n    self.assertIn('test.css', storage.staticfiles_storage.hashed_files)\n    self.assertPostCondition()\n", "    def test_hashed_files_storage(self):\n        # Create a test file and collect static\n        with self.create_test_file('test.css', 'body { background: red; }') as css_file:\n            call_command('collectstatic', interactive=False, verbosity=0)\n\n            # Check that the hashed file is created\n            hashed_file = hashed_file_path(self, 'test.css')\n            self.assertTrue(self.storage.exists(hashed_file))\n\n            # Check that the hashed file's content is the same as the original\n            with self.storage.open(hashed_file) as f:\n                self.assertEqual(f.read(), b'body { background: red; }')\n\n            # Check that the manifest file is created\n            manifest_file = self.storage.path(self.storage.manifest_name)\n            self.assertTrue(os.path.exists(manifest_file))\n\n            # Check that the manifest file contains the hashed file name\n            with open(manifest_file) as f:\n                manifest = json.load(f)\n                self.assertEqual(manifest['paths']['test.css'], hashed_file)\n\n        self.assertPostCondition()\n", "    def test_missing_manifest(self):\n        self.create_file('static_files/missing.txt', content='missing')\n        with self.settings(DEBUG=True):\n            self.assertEqual(\n                self.render_template(self.static_template_snippet('static_files/missing.txt')),\n                '/static/static_files/missing.txt',\n            )\n        with self.settings(DEBUG=False):\n            with self.assertRaisesMessage(ValueError, \"Missing staticfiles manifest entry for 'static_files/missing.txt'\"):\n                self.render_template(self.static_template_snippet('static_files/missing.txt'))\n        self.assertPostCondition()\n", "def test_hash_change(self):\n    # Change the content of a file and check if the hash changes\n    original_content = \"Original content\"\n    new_content = \"New content\"\n\n    # Create a temporary file with original content\n    with tempfile.NamedTemporaryFile(mode='w+', delete=False, suffix='.css') as f:\n        f.write(original_content)\n        original_file = f.name\n\n    # Save the file to static storage\n    saved_file = storage.staticfiles_storage.save(original_file, StringIO(original_content))\n\n    # Get the hashed name of the original file\n    hashed_name = storage.staticfiles_storage.stored_name(saved_file)\n\n    # Change the content of the file\n    with open(original_file, 'w') as f:\n        f.write(new_content)\n\n    # Get the new hashed name of the file\n    new_hashed_name = storage.staticfiles_storage.stored_name(saved_file)\n\n    # Assert that the hashed name has changed\n    self.assertNotEqual(hashed_name, new_hashed_name)\n\n    # Clean up the temporary files\n    os.remove(original_file)\n    storage.staticfiles_storage.delete(saved_file)\n    storage.staticfiles_storage.delete(hashed_name)\n    storage.staticfiles_storage.delete(new_hashed_name)\n", "def test_post_process_max_passes_exceeded(self):\n    storage.staticfiles_storage.max_post_process_passes = 1\n    paths = {'app1/style.css': ('style1.css', self.get_file_content('app1/style.css'))}\n\n    # Force a loop where post_process never reaches stability\n        return name + '.invalid'\n\n    storage.staticfiles_storage.hashed_name = mock_hashed_name\n\n    with self.settings(DEBUG=True):\n        with self.assertRaisesMessage(RuntimeError, 'Max post-process passes exceeded.'):\n            list(storage.staticfiles_storage.post_process(paths))\n\n    self.assertPostCondition()\n"], "sample_349": ["    def setUp(self):\n        self.band1 = Band.objects.create(name='Band 1')\n        self.band2 = Band.objects.create(name='Band 2')\n        self.album = Album.objects.create(band=self.band1)\n        self.field = Album._meta.get_field('band')\n        self.widget = AutocompleteSelect(self.field, admin.site)\n", "    def test_autocomplete_select_widget_renders_correctly(self):\n        form = AlbumForm()\n        band_field = form.fields['band']\n        widget = band_field.widget\n\n        self.assertIsInstance(widget, AutocompleteSelect)\n        self.assertEqual(widget.field, Album._meta.get_field('band'))\n        self.assertEqual(widget.attrs['class'], 'my-class')\n        self.assertIn('admin-autocomplete', widget.attrs['class'])\n        self.assertEqual(widget.attrs['data-ajax--url'], '/admin/autocomplete/')\n        self.assertEqual(widget.attrs['data-app-label'], 'admin_widgets')\n        self.assertEqual(widget.attrs['data-model-name'], 'band')\n        self.assertEqual(widget.attrs['data-field-name'], 'band')\n", "    def setUpTestData(cls):\n        band1 = Band.objects.create(name=\"Band 1\")\n        band2 = Band.objects.create(name=\"Band 2\")\n        Album.objects.create(name=\"Album 1\", band=band1)\n        Album.objects.create(name=\"Album 2\", band=band2)\n", "    def setUpTestData(cls):\n        band = Band.objects.create(name='Test Band')\n        album1 = Album.objects.create(band=band, name='Album 1')\n        album2 = Album.objects.create(band=band, name='Album 2')\n", "    def setUpTestData(cls):\n        # Set up non-modified objects used by all test methods\n        Band.objects.create(name='The Beatles')\n        Band.objects.create(name='Led Zeppelin')\n        Band.objects.create(name='Queen')\n"], "sample_960": ["def test_parse_annotation(app):\n    env = app.builder.env\n    annotation = \"List[Union[int, str]]\"\n    result = _parse_annotation(annotation, env)\n    assert isinstance(result, list)\n    assert len(result) == 6\n    assert isinstance(result[0], pending_xref)\n    assert result[0]['reftarget'] == 'List'\n    assert isinstance(result[1], nodes.Text)\n    assert result[1].astext() == '['\n    assert isinstance(result[2], pending_xref)\n    assert result[2]['reftarget'] == 'Union'\n    assert isinstance(result[3], nodes.Text)\n    assert result[3].astext() == '['\n    assert isinstance(result[4], nodes.Text)\n    assert result[4].astext() == 'int'\n    assert isinstance(result[5], nodes.Text)\n    assert result[5].astext() == ', str]'\n", "def test_parse_annotation():\n    env = Mock()\n    env.config.python_use_unqualified_type_names = False\n    result = _parse_annotation(\"List[str]\", env)\n    assert len(result) == 5\n    assert_node(result[0], [pending_xref, desc_sig_name], attributes={'reftarget': 'List'})\n    assert_node(result[1], [desc_sig_punctuation], text='[')\n    assert_node(result[2], [desc_sig_name, pending_xref], attributes={'reftarget': 'str'})\n    assert_node(result[3], [desc_sig_punctuation], text=']')\n", "def test_parse_annotation_with_ellipsis(app):\n    with app.test_env():\n        env = app.builder.env\n        result = _parse_annotation(\"Tuple[..., ...]\", env)\n        assert len(result) == 6\n        assert isinstance(result[0], nodes.Text)\n        assert result[0].astext() == \"Tuple\"\n        assert isinstance(result[1], pending_xref)\n        assert isinstance(result[2], addnodes.desc_sig_punctuation)\n        assert result[2].astext() == \"[\"\n        assert isinstance(result[3], addnodes.desc_sig_punctuation)\n        assert result[3].astext() == \"...\"\n        assert isinstance(result[4], addnodes.desc_sig_punctuation)\n        assert result[4].astext() == \"]\"\n", "def test_parse_annotation(app):\n    env = app.builder.env\n\n    result = _parse_annotation(\"List[int]\", env)\n    assert len(result) == 4\n    assert isinstance(result[0], nodes.Text)\n    assert isinstance(result[1], addnodes.pending_xref)\n    assert result[0].astext() == \"List[\"\n    assert result[1]['reftarget'] == \"List\"\n    assert result[2].astext() == \",\"\n    assert isinstance(result[3], addnodes.pending_xref)\n    assert result[3]['reftarget'] == \"int\"\n", "def test_parse_annotation_simple(app):\n    env = Mock()\n    env.config.python_use_unqualified_type_names = False\n    result = _parse_annotation(\"int\", env)\n    assert len(result) == 1\n    assert isinstance(result[0], pending_xref)\n    assert result[0].get('reftarget') == \"int\"\n"], "sample_4": ["def test_latex_names(self, cosmo, tmp_path):\n    # Create a cosmology object\n    cosmology = cosmo\n\n    # Write the cosmology object to a temporary HTML file with latex_names=True\n    file = tmp_path / \"cosmology.html\"\n    write_html_table(cosmology, file, latex_names=True)\n\n    # Read the cosmology object from the temporary HTML file with latex_names=True\n    read_cosmology = read_html_table(file, latex_names=True)\n\n    # Check that the read cosmology object is equal to the original cosmology object\n    assert read_cosmology == cosmology\n", "def test_read_write_html_latex_names(self, tmp_path, cosmo, latex_names):\n    \"\"\"Test that reading/writing HTML with latex_names works.\"\"\"\n    filename = str(tmp_path / \"cosmology.html\")\n    write_html_table(cosmo, filename, latex_names=latex_names)\n    cosmo_read = read_html_table(filename, latex_names=latex_names)\n    assert cosmo_read == cosmo\n", "def test_latex_names(cosmo, tmpdir):\n    \"\"\"\n    Test that the latex names for parameters are correctly handled when writing\n    and reading a cosmology to/from an HTML table.\n    \"\"\"\n    filename = tmpdir.join(\"cosmo.html\")\n    write_html_table(cosmo, filename, latex_names=True)\n    cosmo_read = read_html_table(filename, latex_names=True)\n\n    # Check that all parameters are equal\n    for param in cosmo.__parameters__:\n        assert getattr(cosmo, param) == getattr(cosmo_read, param)\n", "def test_write_html_table_latex_names(self, cosmo, tmp_path):\n    # Write cosmology to a temporary file with latex_names=True\n    file = tmp_path / \"test_cosmo.html\"\n    write_html_table(cosmo, file, latex_names=True)\n\n    # Read the cosmology back from the file\n    cosmo_read = read_html_table(file, latex_names=True)\n\n    # Check that the parameters match\n    for name in cosmo.__parameters__:\n        assert getattr(cosmo, name) == getattr(cosmo_read, name)\n\n    # Check that the latex names are used in the file\n    with open(file, 'r') as f:\n        content = f.read()\n        for latex in _FORMAT_TABLE.values():\n            assert latex in content\n", "def test_write_html_table_latex_names(self, cosmo, tmp_path):\n    \"\"\"\n    Test writing a Cosmology to an HTML table with latex_names=True.\n    \"\"\"\n    # Write the Cosmology to an HTML table with latex_names=True\n    file = tmp_path / \"cosmology.html\"\n    write_html_table(cosmo, file, latex_names=True)\n\n    # Check that the file exists\n    assert file.exists()\n\n    # Read the Cosmology back from the HTML table\n    cosmo_read = read_html_table(file)\n\n    # Check that the Cosmology is the same\n    assert cosmo_read == cosmo\n"], "sample_464": ["def test_file_response_with_filename(self):\n    # Create a temporary file for testing\n    with tempfile.NamedTemporaryFile(delete=False) as temp:\n        temp.write(b\"Test content\")\n        temp_name = temp.name\n\n    # Open the temporary file\n    with open(temp_name, \"rb\") as file:\n        # Create a FileResponse with the file and a custom filename\n        response = FileResponse(file, as_attachment=True, filename=\"custom_filename.txt\")\n\n        # Assert the response headers are set correctly\n        self.assertEqual(response.headers[\"Content-Disposition\"], 'attachment; filename=\"custom_filename.txt\"')\n        self.assertEqual(response.headers[\"Content-Type\"], \"text/plain\")\n\n    # Remove the temporary file\n    os.remove(temp_name)\n", "    def test_content_length_with_unseekable_file(self):\n        content = b\"test content\"\n        unseekable_file = UnseekableBytesIO(content)\n        response = FileResponse(unseekable_file)\n        self.assertIn(\"Content-Length\", response.headers)\n        self.assertEqual(response.headers[\"Content-Length\"], str(len(content)))\n", "def test_file_response_with_unseekable_file(self):\n    unseekable_file = UnseekableBytesIO(b\"test content\")\n    response = FileResponse(unseekable_file)\n    self.assertIn(\"Content-Length\", response.headers)\n    self.assertEqual(response.getvalue(), b\"test content\")\n", "def test_file_response_with_unseekable_file_like_object(self):\n    data = b\"test data\"\n    filelike = UnseekableBytesIO(data)\n    response = FileResponse(filelike, as_attachment=True, filename=\"testfile.txt\")\n    content = b\"\".join(response.streaming_content)\n    self.assertEqual(content, data)\n    self.assertIn(\"Content-Length\", response.headers)\n    self.assertEqual(int(response.headers[\"Content-Length\"]), len(data))\n    self.assertEqual(response.headers[\"Content-Type\"], \"text/plain\")\n    self.assertEqual(response.headers[\"Content-Disposition\"], \"attachment; filename=testfile.txt\")\n", "def test_file_response_with_tell(self):\n    content = b\"some content\"\n    file = ContentFile(content)\n    response = FileResponse(file)\n\n    # Assert that the content length is set correctly\n    self.assertEqual(response.headers[\"Content-Length\"], str(len(content)))\n\n    # Assert that the content is not read before iteration\n    self.assertEqual(file.tell(), 0)\n\n    # Iterate through the response\n    response_content = b\"\".join(response.streaming_content)\n\n    # Assert that the content is read completely\n    self.assertEqual(file.tell(), len(content))\n\n    # Assert that the content is correct\n    self.assertEqual(response_content, content)\n"], "sample_1055": ["def test_encipher_atbash():\n    msg = 'GONAVYBEATARMY'\n    assert encipher_atbash(msg) == 'TLMZEBYVZGZINB'\n", "def test_encipher_atbash():\n    msg = \"GONAVYBEATARMY\"\n    assert encipher_atbash(msg) == \"TLMZEBYVZGZINB\"\n", "def test_encipher_decipher_shift():\n    msg = \"GONAVYBEATARMY\"\n    key = 1\n    encrypted = encipher_shift(msg, key)\n    decrypted = decipher_shift(encrypted, key)\n    assert msg == decrypted\n", "def test_decipher_bifid():\n    key = \"gold bug\"\n    msg = \"hello world!\"\n    encrypted = encipher_bifid(msg, key)\n    decrypted = decipher_bifid(encrypted, key)\n    assert decrypted == msg\n", "def test_encipher_rot13():\n    msg = \"GONAVYBEATARMY\"\n    ct = encipher_rot13(msg)\n    assert decipher_rot13(ct) == msg\n\n    # Test with symbols other than uppercase letters\n    symbols = \"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\"\n    msg = \"G0n4vyB3at4rmy\"\n    ct = encipher_rot13(msg, symbols)\n    assert decipher_rot13(ct, symbols) == msg\n"], "sample_1070": ["def test_lambertw_fdiff():\n    x = symbols('x')\n    k = symbols('k')\n    assert LambertW(x).fdiff() == LambertW(x)/(x*(1 + LambertW(x)))\n    assert LambertW(x, k).fdiff() == LambertW(x, k)/(x*(1 + LambertW(x, k)))\n    raises(ArgumentIndexError, lambda: LambertW(x).fdiff(2))\n", "def test_lambertw_fdiff():\n    x = symbols('x')\n    k = symbols('k')\n    assert LambertW(x).fdiff() == LambertW(x)/(x*(1 + LambertW(x)))\n    assert LambertW(x, k).fdiff() == LambertW(x, k)/(x*(1 + LambertW(x, k)))\n    raises(ArgumentIndexError, lambda: LambertW(x).fdiff(2))\n", "def test_log_expand():\n    assert expand_log(log(x*y)) == log(x) + log(y)\n    assert expand_log(log(x/y)) == log(x) - log(y)\n", "def test_LambertW_fdiff():\n    x = symbols('x')\n    k = symbols('k')\n\n    # Test the first derivative of LambertW function\n    assert LambertW(x).fdiff(1) == LambertW(x) / (x * (1 + LambertW(x)))\n    assert LambertW(x, k).fdiff(1) == LambertW(x, k) / (x * (1 + LambertW(x, k)))\n\n    # Test ArgumentIndexError for invalid argindex\n    raises(ArgumentIndexError, lambda: LambertW(x).fdiff(2))\n    raises(ArgumentIndexError, lambda: LambertW(x, k).fdiff(3))\n", "def test_LambertW_derivative():\n    k = Symbol('k')\n    assert LambertW(x, k).diff(x) == LambertW(x, k)/(x*(1 + LambertW(x, k)))\n"], "sample_1127": ["def test_symmetric_permutation_group_contains():\n    G = SymmetricPermutationGroup(4)\n    assert Permutation(1, 2, 3) in G\n    assert Permutation(1, 2, 3, 4) not in G\n", "def test_coset():\n    a = Permutation(1, 2)\n    b = Permutation(0, 1)\n    G = PermutationGroup([a, b])\n    cst = Coset(a, G, dir=\"-\")\n    assert cst.is_left_coset\n    assert not cst.is_right_coset\n    assert a*b in cst.as_list()\n", "def test_coset_init():\n    a = Permutation(1, 2)\n    b = Permutation(0, 1)\n    G = PermutationGroup([a, b])\n    cst = Coset(a, G)\n    assert cst.args[0] == a\n    assert cst.args[1] == G\n    assert cst._dir == '+'\n", "def test_perm_group_isomorphic():\n    a = Permutation(1, 2)\n    b = Permutation(0, 1)\n    G = PermutationGroup([a, b])\n    H = PermutationGroup([b, a])\n    assert is_isomorphic(G, H)\n", "def test_coset_properties():\n    a = Permutation(1, 2)\n    b = Permutation(0, 1)\n    G = PermutationGroup([a, b])\n    cst_left = Coset(a, G, dir=\"-\")\n    cst_right = Coset(a, G, dir=\"+\")\n    assert cst_left.is_left_coset\n    assert not cst_left.is_right_coset\n    assert not cst_right.is_left_coset\n    assert cst_right.is_right_coset\n"], "sample_518": ["def test_fancyarrowpatch_set_positions():\n    arrow = FancyArrowPatch((0, 0), (1, 1))\n    assert arrow._posA_posB == [[0, 0], [1, 1]]\n    arrow.set_positions((2, 2), (3, 3))\n    assert arrow._posA_posB == [[2, 2], [3, 3]]\n", "def test_fancyarrowpatch_set_connectionstyle():\n    arrow = FancyArrowPatch((0, 0), (1, 1), connectionstyle=\"arc3\")\n    assert isinstance(arrow.get_connectionstyle(), mpl.patches.ConnectionStyle)\n\n    arrow.set_connectionstyle(\"angle3\", angleA=30, angleB=60)\n    assert isinstance(arrow.get_connectionstyle(), mpl.patches.ConnectionStyle)\n", "def test_annulus_radii_property():\n    annulus = Annulus((0, 0), 3, 4)\n    assert annulus.radii == (3, 4)\n\n    annulus.radii = 5\n    assert annulus.radii == (5, 5)\n\n    annulus.radii = (6, 7)\n    assert annulus.radii == (6, 7)\n\n    with pytest.raises(ValueError):\n        annulus.radii = (1, 2, 3)\n", "def test_annulus_radii():\n    annulus = Annulus((0, 0), 5, 10)\n    assert_almost_equal(annulus.get_radii(), (5, 10))\n    annulus.set_radii(8, 12)\n    assert_almost_equal(annulus.get_radii(), (8, 12))\n", "def test_annulus_area():\n    # Test the area calculation of Annulus\n    annulus = Annulus((0, 0), 3, 4)\n    expected_area = np.pi * (16 - 9)\n    assert_almost_equal(annulus.get_area(), expected_area)\n"], "sample_654": ["def test_getfixturedefs_no_matching_fixturedefs(fixturemanager):\n    argname = \"nonexistent_fixture\"\n    nodeid = \"test_module::test_function\"\n    fixturemanager._arg2fixturedefs = {argname: []}  # No matching fixturedefs\n\n    result = fixturemanager.getfixturedefs(argname, nodeid)\n\n    assert result is None\n", "def test_getfixturevalue_with_invalid_argname():\n    request = FixtureRequest(pytest.Function(\"test_func\"))\n    with pytest.raises(FixtureLookupError):\n        request.getfixturevalue(\"invalid_fixture\")\n", "def test_fixture_request_addfinalizer(pytestconfig):\n    class DummyItem:\n        session = pytestconfig._session\n        parent = pytestconfig._session.fspath\n        nodeid = \"dummy\"\n        config = pytestconfig\n\n    request = FixtureRequest(DummyItem)\n    finalizer = lambda: None\n    request._addfinalizer(finalizer, \"session\")\n    assert len(request._pyfuncitem.session._setupstate.finalizers) == 1\n", "def test_fixture_request_getfixturevalue_not_found(pytester):\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture\n            pass\n\n            pass\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.assert_outcomes(errors=1)\n    assert \"FixtureLookupError: fixture 'non_existent_fixture' not found\" in result.outlines\n", "def test_subrequest_scope(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture(scope='module')\n            pass\n\n        @pytest.fixture(scope='class')\n            pass\n\n        @pytest.fixture(scope='function')\n            pass\n\n            assert request.scope == 'function'\n            assert request._parent_request.scope == 'class'\n            assert request._parent_request._parent_request.scope == 'module'\n            assert request._parent_request._parent_request._parent_request is None\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.assert_outcomes(passed=1)\n"], "sample_574": ["def test_continuous_tick_locator(self, x):\n    locator = mpl.ticker.FixedLocator([0.2, 0.5, 0.8])\n    a = self.setup_ticks(x, locator=locator)\n    assert_array_equal(a.major.locator(), [0.2, 0.5, 0.8])\n", "def test_continuous_tick_upto(self, x):\n    a = self.setup_ticks(x, upto=5)\n    assert len(a.major.locator()) <= 5\n", "def test_continuous_tick_at(self, x):\n    at = [2, 4, 6]\n    a = self.setup_ticks(x, at=at)\n    assert_array_equal(a.get_tick_locs(), at)\n", "def test_temporal_label_concise_formatter(self, x):\n\n    a, locs = self.setup_labels(x, formatter=Temporal().label(concise=True))\n    labels = a.major.formatter.format_ticks(locs)\n    assert all(re.match(r'^\\d+[smhd]$', label) for label in labels)\n", "def test_tick_locator(self, x):\n    locator = mpl.ticker.FixedLocator([0.2, 0.4, 0.6, 0.8])\n    a = self.setup_ticks(x, locator=locator)\n    assert_array_equal(a.get_tick_space(), [0.2, 0.4, 0.6, 0.8])\n"], "sample_648": ["    def test_extract_from_with_force_tuple(self, attr: str):\n        obj = (1, 2, 3)\n        parameter_set = getattr(ParameterSet, attr)(*obj)\n        extracted_set = ParameterSet.extract_from(parameter_set, force_tuple=True)\n        assert extracted_set == parameter_set\n", "    def test_extract_from(self, attr):\n        values = (1, 2, 3)\n        marks = [pytest.mark.foo]\n        id = \"test_id\"\n        param = getattr(pytest.mark, attr).param(*values, marks=marks, id=id)\n        extracted = pytest.mark.ParameterSet.extract_from(param)\n        assert extracted.values == values\n        assert extracted.marks == marks\n        assert extracted.id == id\n", "def test_has_param_ids(\n    self,\n    name: str,\n    args: tuple,\n    kwargs: dict,\n    param_ids_from: Optional[Mark],\n    param_ids_generated: Optional[List[str]],\n    expected_has_param_ids: bool,", "def test_get_empty_parameterset_mark(pytester, tmpdir):\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture\n            return request.function\n\n            config = pytest.config\n            argnames = (\"x\", \"y\")\n            mark = pytest.mark.get_empty_parameterset_mark(config, argnames, func)\n            assert mark.name == config.getini(EMPTY_PARAMETERSET_OPTION)\n        \"\"\"\n    )\n    pytester.runpytest().assert_outcomes(passed=1)\n", "def test_parametrize_with_invalid_number_of_values(self, pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.parametrize(\"a,b\", [(1,)])\n            pass\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*In 'test_func', the number of names (2):*\",\n            \"*  ['a', 'b']*\",\n            \"*must be equal to the number of values (1):*\",\n            \"*  (1,)*\",\n        ]\n    )\n    assert result.ret == ExitCode.TESTS_FAILED\n"], "sample_596": ["def test_concat_dataarray_with_coord():\n    data = np.random.randn(3, 4)\n    da1 = DataArray(data, dims=(\"x\", \"y\"), coords={\"z\": (\"x\", np.arange(3))})\n    da2 = DataArray(data, dims=(\"x\", \"y\"), coords={\"z\": (\"x\", np.arange(3, 6))})\n    result = concat([da1, da2], dim=\"x\")\n    expected_coords = {\"z\": (\"x\", np.concatenate([np.arange(3), np.arange(3, 6)]))}\n    assert result.coords == expected_coords\n", "def test_concat_with_overlapping_coords():\n    ds1 = Dataset({\n        'x': ('time', [1, 2, 3]),\n        'y': ('time', [4, 5, 6]),\n        'z': ('space', [7, 8, 9])\n    })\n\n    ds2 = Dataset({\n        'x': ('time', [2, 3, 4]),\n        'y': ('time', [5, 6, 7]),\n        'z': ('space', [10, 11, 12])\n    })\n\n    with pytest.raises(ValueError):\n        concat([ds1, ds2], dim='time')\n\n    with pytest.raises(ValueError):\n        concat([ds1, ds2], dim='space')\n", "def test_concat_with_inaccessible_array():\n    data1 = DataArray(np.random.randn(3, 4), dims=('x', 'y'))\n    data2 = DataArray(InaccessibleArray(np.random.randn(3, 4)), dims=('x', 'y'))\n    with raises_regex(ValueError, \"cannot concatenate a dask array with a non-dask array\"):\n        concat([data1, data2], dim='x')\n", "def test_concat_with_dim_as_coord():\n    \"\"\"Test concat when dim is a coordinate.\"\"\"\n    da = DataArray(np.random.rand(10), coords={'dim': np.arange(10)}, dims='dim')\n    da2 = DataArray(np.random.rand(10), coords={'dim': np.arange(10, 20)}, dims='dim')\n\n    result = concat([da, da2], dim='dim')\n    expected = DataArray(np.concatenate([da.data, da2.data]), coords={'dim': np.arange(20)}, dims='dim')\n\n    assert_identical(result, expected)\n", "def test_concat_dataarray_dimension_coords():\n    \"\"\"Test concatenating DataArray objects with dimensions as coordinates.\"\"\"\n    # create dataarrays with dimensions as coordinates\n    da1 = DataArray(np.random.randn(3, 4), dims=('dim1', 'dim2'), name='var')\n    da2 = DataArray(np.random.randn(3, 4), dims=('dim1', 'dim2'), name='var')\n\n    # concatenate along a new dimension\n    result = concat([da1, da2], dim='new_dim')\n\n    # check if the dimensions are now coordinates\n    assert 'dim1' in result.coords and 'dim2' in result.coords\n    assert 'new_dim' in result.dims\n\n    # check if the data is concatenated correctly\n    expected = np.concatenate([da1.data, da2.data], axis=0)\n    assert_array_equal(result.data, expected)\n"], "sample_891": ["def test_auc_exceptions():\n    # Test that auc raises an error if x and y have different lengths\n    x = [1, 2, 3]\n    y = [4, 5]\n    with pytest.raises(ValueError):\n        auc(x, y)\n\n    # Test that auc raises an error if x and y have less than 2 points\n    x = [1]\n    y = [1]\n    with pytest.raises(ValueError):\n        auc(x, y)\n\n    # Test that auc raises an error if x is neither increasing nor decreasing\n    x = [1, 2, 1, 3]\n    y = [1, 2, 3, 4]\n    with pytest.raises(ValueError):\n        auc(x, y)\n", "def test_roc_curve_drop_intermediate(drop):\n    \"\"\"Test roc_curve function with different drop_intermediate parameters.\"\"\"\n    y_true = np.array([0, 0, 1, 1])\n    y_score = np.array([0.1, 0.4, 0.35, 0.8])\n    fpr, tpr, thresholds = roc_curve(y_true, y_score, drop_intermediate=drop)\n\n    if drop:\n        assert_array_equal(fpr, np.array([0. , 0.5, 1. ]))\n        assert_array_equal(tpr, np.array([0. , 1. , 1. ]))\n    else:\n        assert_array_equal(fpr, np.array([0. , 0. , 0.5, 0.5, 1. ]))\n        assert_array_equal(tpr, np.array([0. , 1. , 1. , 0. , 0. ]))\n", "def test_top_k_accuracy_score(drop):\n    y_true = np.array([0, 1, 2, 2])\n    y_score = np.array([[0.5, 0.2, 0.2],\n                       [0.3, 0.4, 0.2],\n                       [0.2, 0.4, 0.3],\n                       [0.7, 0.2, 0.1]])\n    k = 2\n    normalize = True\n    sample_weight = None\n    labels = None\n\n    # Test the top_k_accuracy_score function with the given parameters\n    score = top_k_accuracy_score(y_true, y_score, k=k, normalize=normalize, sample_weight=sample_weight, labels=labels)\n\n    # Check if the score is correct\n    assert_almost_equal(score, 0.75)\n", "def test_top_k_accuracy_score(normalize):\n    y_true = np.array([0, 1, 2, 2])\n    y_score = np.array([[0.5, 0.2, 0.2],\n                        [0.3, 0.4, 0.2],\n                        [0.2, 0.4, 0.3],\n                        [0.7, 0.2, 0.1]])\n    expected_score = 0.75 if normalize else 3\n    assert top_k_accuracy_score(y_true, y_score, k=2, normalize=normalize) == expected_score\n", "def test_ndcg_score_ignore_ties(drop):\n    \"\"\"Test that ndcg_score is equal to the average of several ndcg_scores\n    without ties when there are ties.\"\"\"\n    y_true = np.array([[3, 2, 3, 0, 0, 1, 2, 2, 3, 0]])\n    y_score = np.array([[0.7, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]])\n    scores = []\n    for i in range(10):\n        y_score_tie_broken = np.copy(y_score)\n        y_score_tie_broken[0, 2] += i / 1000  # break tie\n        scores.append(ndcg_score(y_true, y_score_tie_broken, ignore_ties=True))\n    assert_almost_equal(ndcg_score(y_true, y_score, ignore_ties=False), np.mean(scores))\n"], "sample_229": ["def test_union_with_values(self):\n    qs1 = Number.objects.filter(num__gt=5).values('num')\n    qs2 = Number.objects.filter(num__lt=5).values('num')\n    union_qs = qs1.union(qs2)\n    self.assertQuerysetEqual(union_qs, ['6', '7', '8', '9', '0', '1', '2', '3', '4'], ordered=False)\n", "def test_union(self):\n    qs1 = Number.objects.filter(num__lt=5)\n    qs2 = Number.objects.filter(num__gt=5)\n    combined_qs = qs1.union(qs2)\n    self.assertNumbersEqual(combined_qs, Number.objects.all(), ordered=False)\n", "def test_union(self):\n    union_queryset = Number.objects.filter(num__gte=5).union(Number.objects.filter(other_num__lte=5))\n    self.assertNumbersEqual(union_queryset, Number.objects.filter(num__gte=5) | Number.objects.filter(other_num__lte=5))\n", "def test_union(self):\n    qs1 = Number.objects.filter(num__lt=5)\n    qs2 = Number.objects.filter(num__gt=4)\n    expected_numbers = set(Number.objects.filter(num__lt=10))\n    self.assertNumbersEqual(qs1.union(qs2), expected_numbers)\n", "def test_union(self):\n    qs1 = Number.objects.filter(num__lt=5)\n    qs2 = Number.objects.filter(num__gt=5)\n    union_qs = qs1.union(qs2)\n    self.assertNumbersEqual(union_qs, Number.objects.all(), ordered=False)\n"], "sample_535": ["def test_table_edges():\n    fig, ax = plt.subplots()\n    table = Table(ax, bbox=[0.1, 0.2, 0.8, 0.7])\n\n    # Test 'open' edge setting\n    table.edges = 'open'\n    cell = table.add_cell(0, 0, text='Open Edges')\n    assert cell.visible_edges == ''\n    assert cell.get_path().codes == [Path.MOVETO] * 4\n\n    # Test 'closed' edge setting\n    table.edges = 'closed'\n    cell = table.add_cell(1, 0, text='Closed Edges')\n    assert cell.visible_edges == 'BRTL'\n    assert cell.get_path().codes == [Path.MOVETO, Path.LINETO, Path.LINETO, Path.LINETO, Path.CLOSEPOLY]\n\n    # Test 'horizontal' edge setting\n    table.edges = 'horizontal'\n    cell = table.add_cell(2, 0, text='Horizontal Edges')\n    assert cell.visible_edges == 'BT'\n    assert cell.get_path().codes == [Path.MOVETO, Path.LINETO, Path.MOVETO, Path.LINETO, Path.CLOSEPOLY]\n\n    # Test 'vertical' edge setting\n    table.edges = 'vertical'\n    cell = table.add_cell(3, 0, text='Vertical Edges')\n    assert cell.visible_edges == 'RL'\n    assert cell.get_path().codes == [Path.MOVETO, Path.MOVETO, Path.LINETO, Path.LINETO, Path.CLOSEPOLY]\n\n    plt.axis('off')\n", "def test_table_edges():\n    fig, ax = plt.subplots()\n    data = [['Visible', 'Closed'],\n            ['Open', 'Horizontal'],\n            ['Vertical', 'Custom']]\n    edges = ['closed', 'open', 'horizontal', 'vertical', 'BRTL', '']\n\n    table = ax.table(cellText=data, edges='closed', loc='center')\n\n    # Test each edge type\n    for i, edge in enumerate(edges):\n        table.edges = edge\n        ax.clear()\n        ax.table(cellText=data, edges=edge, loc='center')\n        fig.savefig(f'table_edges_{i}.png')\n\n    plt.close(fig)\n", "def test_custom_cell_edges():\n    fig, ax = plt.subplots()\n    cell = CustomCell((0, 0), width=0.5, height=0.5, text='Test',\n                      visible_edges='horizontal')\n    table = Table(ax, bbox=[0.25, 0.25, 0.5, 0.5])\n    table[0, 0] = cell\n    ax.add_table(table)\n    fig.savefig(\"test_custom_cell_edges.png\")\n", "def test_custom_edges():\n    fig, ax = plt.subplots()\n    data = [[1, 2], [3, 4]]\n    table = ax.table(cellText=data, loc='center')\n    for key, cell in table.get_celld().items():\n        cell.visible_edges = 'horizontal' if key[0] == 0 else 'vertical'\n    plt.close(fig)\n", "def test_table_edges():\n    fig, ax = plt.subplots()\n    data = np.array([[1, 2], [3, 4]])\n    table = ax.table(cellText=data, edges='vertical')\n\n    # Check that visible_edges property is set correctly\n    for i in range(2):\n        for j in range(2):\n            assert table._cells[i, j].visible_edges == 'RL'\n\n    # Check that get_path returns the correct path\n    path = table._cells[0, 0].get_path()\n    expected_path = Path([[0.0, 0.0], [1.0, 0.0], [1.0, 1.0], [0.0, 1.0], [0.0, 0.0]],\n                         [Path.MOVETO, Path.LINETO, Path.LINETO, Path.LINETO, Path.CLOSEPOLY],\n                         readonly=True)\n    assert path.vertices.tolist() == expected_path.vertices.tolist()\n    assert path.codes.tolist() == expected_path.codes.tolist()\n"], "sample_286": ["def test_save_base_update_fields(self):\n    article = Article.objects.create(headline='Test Article')\n    article.pub_date = datetime.now()\n    article.save_base(update_fields=['pub_date'])\n    article.refresh_from_db()\n    self.assertIsNotNone(article.pub_date)\n", "    def test_model_save(self):\n        # Create a new instance of the Article model\n        article = Article(title='Test Article', content='This is a test article.')\n\n        # Save the instance to the database\n        article.save()\n\n        # Retrieve the instance from the database\n        saved_article = Article.objects.get(title='Test Article')\n\n        # Assert that the saved instance matches the original instance\n        self.assertEqual(article.title, saved_article.title)\n        self.assertEqual(article.content, saved_article.content)\n", "    def test_deferred_fields(self):\n        # Create a model instance with a deferred field\n        article = Article.objects.create(headline='Test Headline', pub_date=datetime.now(), content='Test Content')\n        article.refresh_from_db(fields=['headline'])\n        self.assertEqual(article.content, 'Test Content')\n        self.assertNotIn('content', article.__dict__)\n", "def test_model_instance_delete_without_pk(self):\n    article = Article(title='Test Article')\n    with self.assertRaisesMessage(\n        ValueError,\n        f\"{article._meta.object_name} object can't be deleted because its {article._meta.pk.attname} \"\n        f\"attribute is set to None.\"\n    ):\n        article.delete()\n", "    def test_save_with_unique_constraint(self):\n        # Create two instances of Article with the same title\n        article1 = Article(title=\"Test Article\")\n        article1.save()\n        article2 = Article(title=\"Test Article\")\n        # Attempt to save the second article, which should raise a DatabaseError\n        with self.assertRaises(DatabaseError):\n            article2.save()\n"], "sample_471": ["def test_integer_field_widget_attrs(self):\n    field = IntegerField(min_value=0, max_value=10, step_size=2)\n    widget_attrs = field.widget_attrs(Textarea())\n    self.assertEqual(widget_attrs['min'], 0)\n    self.assertEqual(widget_attrs['max'], 10)\n    self.assertEqual(widget_attrs['step'], 2)\n", "    def test_widget_attrs(self):\n        field = IntegerField()\n        widget_attrs = field.widget_attrs(Textarea())\n        self.assertEqual(widget_attrs, {})\n\n        field = IntegerField(min_value=0, max_value=100, step_size=5)\n        widget_attrs = field.widget_attrs(Textarea())\n        self.assertEqual(widget_attrs, {'min': 0, 'max': 100, 'step': 5})\n", "    def test_valid_decimal(self):\n        field = DecimalField(max_digits=5, decimal_places=2)\n        self.assertEqual(field.clean('123.45'), Decimal('123.45'))\n", "    def test_max_length(self):\n        field = CharField(max_length=5)\n        self.assertFieldOutput(field, {\"value\": \"Hello\", \"html\": \"input\"}, value=\"Hello\")\n        with self.assertRaises(ValidationError):\n            field.clean(\"Hello World\")\n", "    def test_duration_field_to_python(self):\n        field = DurationField()\n        self.assertEqual(field.to_python('1 day'), datetime.timedelta(days=1))\n        self.assertEqual(field.to_python('2 hours'), datetime.timedelta(hours=2))\n        self.assertEqual(field.to_python('30 minutes'), datetime.timedelta(minutes=30))\n"], "sample_426": ["def test_time_strings(self):\n    time_strings = {\n        \"year\": npgettext_lazy(\"%(num)d yr\", \"%(num)d yrs\", \"num\"),\n        \"month\": npgettext_lazy(\"%(num)d mo\", \"%(num)d mos\", \"num\"),\n        \"week\": npgettext_lazy(\"%(num)d wk\", \"%(num)d wks\", \"num\"),\n        \"day\": npgettext_lazy(\"%(num)d day\", \"%(num)d days\", \"num\"),\n        \"hour\": npgettext_lazy(\"%(num)d hr\", \"%(num)d hrs\", \"num\"),\n        \"minute\": npgettext_lazy(\"%(num)d min\", \"%(num)d mins\", \"num\"),\n    }\n    self.assertEqual(timesince(self.t, self.t + self.oneyear, time_strings=time_strings), \"1 yr\")\n    self.assertEqual(timesince(self.t, self.t + self.onemonth, time_strings=time_strings), \"1 mo\")\n    self.assertEqual(timesince(self.t, self.t + self.oneweek, time_strings=time_strings), \"1 wk\")\n    self.assertEqual(timesince(self.t, self.t + self.oneday, time_strings=time_strings), \"1 day\")\n    self.assertEqual(timesince(self.t, self.t + self.onehour, time_strings=time_strings), \"1 hr\")\n    self.assertEqual(timesince(self.t, self.t + self.oneminute, time_strings=time_strings), \"1 min\")\n    self.assertEqual(timesince(self.t, self.t + self.onesecond, time_strings=time_strings), \"0 mins\")\n    self.assertEqual(timesince(self.t, self.t + self.onemicrosecond, time_strings=time_strings), \"0 mins\")\n    self.assertEqual(timesince(self.t, self.t - self.onemicrosecond, time_strings=time_strings), \"0", "    def test_depth_parameter(self):\n        # Test that the depth parameter controls the number of adjacent time units returned\n        time_strings = {\n            \"year\": npgettext_lazy(\"%d year\", \"%d years\"),\n            \"month\": npgettext_lazy(\"%d month\", \"%d months\"),\n            \"week\": npgettext_lazy(\"%d week\", \"%d weeks\"),\n            \"day\": npgettext_lazy(\"%d day\", \"%d days\"),\n            \"hour\": npgettext_lazy(\"%d hour\", \"%d hours\"),\n            \"minute\": npgettext_lazy(\"%d minute\", \"%d minutes\"),\n        }\n        now = self.t + self.oneyear + self.onemonth + self.oneday + self.onehour + self.oneminute\n\n        # Depth 1 should return only the largest unit\n        self.assertEqual(timesince(self.t, now, time_strings=time_strings, depth=1), \"1 year\")\n\n        # Depth 2 should return the largest and second largest units\n        self.assertEqual(timesince(self.t, now, time_strings=time_strings, depth=2), \"1 year, 1 month\")\n\n        # Depth 3 should return the largest, second largest, and third largest units\n        self.assertEqual(timesince(self.t, now, time_strings=time_strings, depth=3), \"1 year, 1 month, 1 day\")\n\n        # Depth 0 should raise a ValueError\n        with self.assertRaises(ValueError):\n            timesince(self.t, now, depth=0)\n", "    def test_time_strings_with_npgettext_lazy(self):\n        custom_time_strings = {\n            \"year\": npgettext_lazy(\"singular=%(num)d year\", \"plural=%(num)d years\", \"num\"),\n            \"month\": npgettext_lazy(\"singular=%(num)d month\", \"plural=%(num)d months\", \"num\"),\n            \"week\": npgettext_lazy(\"singular=%(num)d week\", \"plural=%(num)d weeks\", \"num\"),\n            \"day\": npgettext_lazy(\"singular=%(num)d day\", \"plural=%(num)d days\", \"num\"),\n            \"hour\": npgettext_lazy(\"singular=%(num)d hour\", \"plural=%(num)d hours\", \"num\"),\n            \"minute\": npgettext_lazy(\"singular=%(num)d minute\", \"plural=%(num)d minutes\", \"num\"),\n        }\n        result = timesince(self.t - self.oneyear, self.t, time_strings=custom_time_strings)\n        self.assertEqual(result, \"1 year\")\n        result = timesince(self.t - self.oneyear * 2, self.t, time_strings=custom_time_strings)\n        self.assertEqual(result, \"2 years\")\n", "    def test_time_with_timezone(self):\n        tz = timezone.get_fixed_timezone(60)\n        t_aware = timezone.make_aware(self.t, tz)\n        now = timezone.now()\n        now_aware = timezone.make_aware(now, tz)\n        self.assertEqual(timesince(t_aware, now_aware), timesince(self.t, now))\n", "def test_timesince_aware_naive_datetimes(self):\n    now_aware = timezone.make_aware(datetime.datetime.now(), timezone.utc)\n    now_naive = datetime.datetime.now()\n    # Ensure the result doesn't raise a TypeError when now is an aware datetime and d is a naive datetime\n    self.assertIsInstance(timesince(now_naive, now=now_aware), str)\n\n    # Ensure the result doesn't raise a TypeError when now is a naive datetime and d is an aware datetime\n    self.assertIsInstance(timesince(now_aware, now=now_naive), str)\n"], "sample_801": ["def test_pipeline_repr():\n    # Test __repr__ of a Pipeline\n    pipe = make_pipeline(StandardScaler(), LogisticRegression())\n    repr_ = repr(pipe)\n    assert \"StandardScaler\" in repr_\n    assert \"LogisticRegression\" in repr_\n    assert \"memory=None\" in repr_\n    assert \"verbose=False\" in repr_\n", "def test_set_params_on_nested_objects():\n    # Test set_params on nested objects\n    estimator = RFE(LogisticRegression(C=1), n_features_to_select=3)\n    estimator.set_params(estimator__C=10)\n    assert estimator.estimator.C == 10\n", "def test_repr_complex_estimators():\n    # Test the repr of complex estimators\n    estimators = [\n        make_pipeline(SelectKBest(chi2), LogisticRegression()),\n        LogisticRegressionCV(cv=3),\n        RFE(estimator=LogisticRegression(), n_features_to_select=10, step=1),\n        GridSearchCV(estimator=SVC(), param_grid={'C': [1, 10]}, cv=3),\n        Pipeline(steps=[('count', CountVectorizer()), ('pca', PCA(n_components=2))]),\n        Pipeline(steps=[('imputer', SimpleImputer(missing_values=np.nan, strategy='mean')), ('nmf', NMF(n_components=2))])\n    ]\n\n    for estimator in estimators:\n        repr_str = repr(estimator)\n        assert isinstance(repr_str, str)\n        assert len(repr_str) > 0\n        assert estimator.__class__.__name__ in repr_str\n", "def test_estimator_repr():\n    # Test the __repr__ method of BaseEstimator\n    log_reg = LogisticRegression(C=0.1)\n    log_reg_cv = LogisticRegressionCV(estimator=log_reg)\n    pipe = make_pipeline(StandardScaler(), log_reg_cv)\n\n    # Check that the repr method works\n    assert repr(pipe).startswith('Pipeline')\n    assert 'StandardScaler' in repr(pipe)\n    assert 'LogisticRegressionCV' in repr(pipe)\n    assert 'C=0.1' in repr(pipe)\n\n    # Check that the repr method handles long parameters\n    long_param = 'a' * 501\n    long_estimator = LogisticRegression(tol=long_param)\n    assert long_param[:300] in repr(long_estimator)\n    assert long_param[-100:] in repr(long_estimator)\n", "    def test_repr_long(self):\n        # Test the long repr with a complex estimator\n        clf = make_pipeline(\n            SelectKBest(chi2, k=2),\n            LogisticRegressionCV(Cs=np.logspace(-3, 3, 7),\n                                 fit_intercept=False, solver='liblinear',\n                                 multi_class='multinomial')\n        )\n\n        expected_repr = (\"Pipeline(steps=[('selectkbest', SelectKBest(k=2, score_func=<function chi2 at 0x...>)), ('logisticregressioncv', LogisticRegressionCV(Cs=array([1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03]), cv=None, fit_intercept=False, multi_class='multinomial', solver='liblinear'))])\")\n\n        with set_config(print_changed_only=False):\n            assert re.match(expected_repr, repr(clf)) is not None\n"], "sample_283": ["    def test_runshell(self):\n        sigint_handler = signal.getsignal(signal.SIGINT)\n        with mock.patch.object(subprocess, 'call') as mock_call:\n            with mock.patch.object(signal, 'signal') as mock_signal:\n                db_client = DatabaseClient(connection)\n                db_client.runshell(['-c', 'SELECT 1'])\n                mock_signal.assert_called_with(signal.SIGINT, signal.SIG_IGN)\n                mock_call.assert_called_with([db_client.executable_name, '-c', 'SELECT 1'], env=mock.ANY)\n                mock_signal.assert_called_with(signal.SIGINT, sigint_handler)\n", "    def test_runshell(self):\n        with mock.patch('subprocess.Popen') as mock_popen, \\\n                mock.patch('signal.signal') as mock_signal:\n            mock_sigint_handler = mock.Mock()\n            mock_signal.side_effect = [mock_sigint_handler, None]\n            client = DatabaseClient()\n            client.runshell(parameters=['-c', 'SELECT 1'])\n            mock_popen.assert_called_once_with(['psql', '-c', 'SELECT 1'], env=os.environ)\n            mock_signal.assert_has_calls([\n                mock.call(signal.SIGINT, signal.SIG_IGN),\n                mock.call(signal.SIGINT, mock_sigint_handler)\n            ])\n", "def test_runshell_sigint_handler(self):\n    # Mock subprocess.run to avoid actual command execution\n    with mock.patch('subprocess.run') as mock_run:\n        # Mock signal.getsignal and signal.signal to test signal handling\n        with mock.patch('signal.getsignal') as mock_getsignal, mock.patch('signal.signal') as mock_signal:\n            mock_getsignal.return_value = signal.SIG_DFL\n            mock_signal.return_value = None\n\n            # Call runshell with some parameters\n            db_client = DatabaseClient(connection)\n            db_client.runshell(['-c', 'SELECT 1;'])\n\n            # Check that signal.signal was called with SIGINT and SIG_IGN to ignore it\n            mock_signal.assert_any_call(signal.SIGINT, signal.SIG_IGN)\n\n            # Check that signal.signal was called with SIGINT and the original handler to restore it\n            mock_signal.assert_any_call(signal.SIGINT, signal.SIG_DFL)\n\n            # Check that subprocess.run was called with the expected command\n            mock_run.assert_called_once_with(['psql'] + ['-c', 'SELECT 1;'], env=mock.ANY, check=True)\n", "def test_runshell(self):\n    # Mock the super().runshell method\n    with mock.patch.object(DatabaseClient, 'runshell', return_value=None) as mock_runshell:\n        # Create a dummy SIGINT handler\n        dummy_handler = lambda signum, frame: None\n\n        # Set the original SIGINT handler to the dummy handler\n        original_handler = signal.getsignal(signal.SIGINT)\n        signal.signal(signal.SIGINT, dummy_handler)\n\n        try:\n            # Call the runshell method\n            DatabaseClient().runshell(parameters=[])\n\n            # Assert that super().runshell was called\n            mock_runshell.assert_called_once_with([])\n        finally:\n            # Restore the original SIGINT handler\n            signal.signal(signal.SIGINT, original_handler)\n", "def test_runshell(self, mock_popen):\n    mock_popen.return_value.returncode = 0\n    mock_sigint_handler = signal.SIG_DFL\n    with mock.patch('django.db.backends.postgresql.client.signal.getsignal') as mock_getsignal, \\\n         mock.patch('django.db.backends.postgresql.client.signal.signal') as mock_signal:\n        mock_getsignal.return_value = mock_sigint_handler\n        client = DatabaseClient(connection.settings_dict)\n        client.runshell(['-c', 'SELECT 1'])\n        mock_getsignal.assert_called_once_with(signal.SIGINT)\n        mock_signal.assert_has_calls([\n            mock.call(signal.SIGINT, signal.SIG_IGN),\n            mock.call(signal.SIGINT, mock_sigint_handler),\n        ])\n        mock_popen.assert_called_once_with(\n            [client.executable_name, '-U', 'test', '-h', 'localhost', '-p', '5432', 'testdb', '-c', 'SELECT 1'],\n            env={'PGPASSWORD': 'password', 'PGSERVICE': 'service', 'PGSSLMODE': 'verify-full',\n                 'PGSSLROOTCERT': 'rootcert', 'PGSSLCERT': 'cert', 'PGSSLKEY': 'key', 'PGPASSFILE': 'passfile'}\n        )\n"], "sample_733": ["def test_tfidf_vectorizer_with_custom_preprocessor():\n    vectorizer = TfidfVectorizer(preprocessor=uppercase)\n    X = vectorizer.fit_transform(JUNK_FOOD_DOCS)\n    assert_greater(X.shape[1], 0)\n    assert_greater(X.shape[0], 0)\n", "def test_tfidf_vectorizer_with_custom_analyzer():\n    vectorizer = TfidfVectorizer(analyzer=lazy_analyze)\n    X = vectorizer.fit_transform(ALL_FOOD_DOCS)\n    assert_array_equal(X.toarray(), np.ones((len(ALL_FOOD_DOCS), 1)))\n", "def test_count_vectorizer_vocabulary_validation():\n    vectorizer = CountVectorizer()\n    vectorizer.fit(ALL_FOOD_DOCS)\n\n    # Test that the vocabulary is validated properly\n    with assert_raises(ValueError):\n        vectorizer.vocabulary = {'pizza': 1, 'burger': 1}  # Duplicate indices\n        vectorizer._validate_vocabulary()\n\n    with assert_raises(ValueError):\n        vectorizer.vocabulary = {'pizza': 1, 'burger': 3}  # Missing index\n        vectorizer._validate_vocabulary()\n\n    with assert_raises(ValueError):\n        vectorizer.vocabulary = {}  # Empty vocabulary\n        vectorizer._validate_vocabulary()\n", "def test_strip_accents_unicode():\n    # Test that strip_accents_unicode correctly handles accented characters\n    accented_string = \"r\u00e9sum\u00e9\"\n    expected_output = \"resume\"\n    assert_equal(strip_accents_unicode(accented_string), expected_output)\n", "def test_hashing_vectorizer_transform_with_preprocessor():\n    vectorizer = HashingVectorizer(preprocessor=uppercase)\n    X = vectorizer.transform(JUNK_FOOD_DOCS)\n    assert_equal(X.shape, (len(JUNK_FOOD_DOCS), vectorizer.n_features))\n\n    vectorizer = HashingVectorizer(preprocessor=strip_eacute)\n    X = vectorizer.transform([\"r\u00e9sum\u00e9\", \"espa\u00f1ol\"])\n    assert_array_equal(X.toarray(), [[0, 1, 0], [1, 0, 0]])\n"], "sample_716": ["def test_sparse_ridge_regression():\n    X, y = make_regression(n_samples=50, n_features=10, noise=0.1, random_state=0)\n    X_sparse = sp.csr_matrix(X)\n    coef_dense = ridge_regression(X, y, alpha=1.0)\n    coef_sparse = ridge_regression(X_sparse, y, alpha=1.0)\n    assert_array_almost_equal(coef_dense, coef_sparse)\n", "def test_ridge_regression_with_sample_weight():\n    # Test ridge_regression with sample_weight\n    X = np.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n    y = np.array([1.0, 2.0, 3.0])\n    sample_weight = np.array([0.5, 1.0, 1.5])\n    alpha = 0.1\n\n    coef = ridge_regression(X, y, alpha, sample_weight=sample_weight)\n    assert_array_almost_equal(coef, np.array([0.11764706, 0.11764706]))\n", "def test_ridge_regression_sag():\n    # Test the sag solver of ridge_regression\n    X, y = make_regression(n_samples=50, n_features=10, noise=0.1, random_state=0)\n    alpha = 1.0\n    coef, n_iter, intercept = ridge_regression(X, y, alpha=alpha, solver='sag', return_n_iter=True, return_intercept=True)\n    assert_greater(n_iter, 0)\n    assert_equal(coef.shape, (10,))\n    assert_equal(intercept.shape, ())\n", "def custom_scorer(estimator, X, y):\n    y_pred = estimator.predict(X)\n    return -mean_squared_error(y, y_pred)\n", "def test_ridge_classifier_cv_multiclass_warning():\n    X, y = make_multilabel_classification(n_classes=3, random_state=0)\n    ridge_cv = RidgeClassifierCV()\n    with assert_raises_regex(ValueError, \"doesn't support multi-label classification\"):\n        ridge_cv.fit(X, y)\n"], "sample_833": ["def test_logistic_regression_with_l1_penalty():\n    clf = LogisticRegression(penalty='l1', solver='liblinear', random_state=0)\n    check_predictions(clf, X, Y1)\n", "def test_logistic_regression_path_binary():\n    coefs, Cs, n_iter = logistic_regression_path(X, Y1, Cs=np.logspace(-4, 4, 10))\n    assert_equal(coefs.shape, (10, 2))\n    assert_equal(Cs.shape, (10,))\n    assert_equal(n_iter.shape, (10,))\n", "def test_logistic_regression_path():\n    # Test logistic_regression_path function\n    coefs, Cs, n_iter = logistic_regression_path(X, Y1)\n    assert coefs.shape == (10, 2, 3)\n    assert Cs.shape == (10,)\n    assert n_iter.shape == (10,)\n\n    coefs, Cs, n_iter = logistic_regression_path(X, Y2, multi_class='multinomial')\n    assert coefs.shape == (3, 10, 2, 3)\n    assert Cs.shape == (10,)\n    assert n_iter.shape == (10,)\n\n    coefs, Cs, n_iter = logistic_regression_path(X, Y1, penalty='elasticnet', l1_ratio=0.5)\n    assert coefs.shape == (10, 2, 3)\n    assert Cs.shape == (10,)\n    assert n_iter.shape == (10,)\n", "def test_logistic_regression_cv_refit_false():\n    X, y = make_classification(n_samples=100, n_features=20, n_classes=3, random_state=42)\n    clf = LogisticRegressionCV(Cs=np.logspace(-4, 4, 3), cv=5, refit=False, random_state=42)\n    clf.fit(X, y)\n    assert clf.C_.shape == (3,)\n    assert clf.l1_ratio_.shape == (3,)\n    assert_array_equal(clf.coef_.shape, (3, 20))\n    assert_array_equal(clf.intercept_.shape, (3,))\n", "def test_logistic_regression_with_l1_penalty():\n    # Test LogisticRegression with l1 penalty\n    X, y = make_classification(n_samples=100, n_features=20, n_informative=2, n_redundant=10, random_state=42)\n    clf = LogisticRegression(penalty='l1', solver='liblinear', random_state=42)\n    check_predictions(clf, X, y)\n    assert_greater(np.sum(clf.coef_ != 0), 0)  # Check that some coefficients are non-zero\n"], "sample_986": ["def test_get_integer_part():\n    expr = 3.141592653589793\n    re_, im_, re_acc, im_acc = get_integer_part(expr, 1, {}, return_ints=True)\n    assert (re_, im_) == (4, 0)\n", "def test_get_integer_part():\n    assert get_integer_part(S(3.14159), 1, {}) == (from_float(4), None, 3, None)\n    assert get_integer_part(S(3.14159), -1, {}) == (from_float(3), None, 3, None)\n", "def test_evalf_sum_hypergeometric():\n    expr = Sum(1/factorial(n), (n, 0, oo))\n    re, im, re_acc, im_acc = evalf(expr, 15, {})\n    assert N(exp(1)) == N(re)\n", "def test_complex_accuracy():\n    re = (0, 123456789, 0, 8)\n    im = (0, 100000000, 0, 8)\n    re_acc = 6\n    im_acc = 6\n    assert complex_accuracy((re, im, re_acc, im_acc)) == 7\n", "def test_complex_accuracy():\n    # Test complex_accuracy function\n    re = (0, 123456789, 0, 9)\n    im = (0, 987654321, 0, 9)\n    re_acc = 6\n    im_acc = 7\n    assert complex_accuracy((re, im, re_acc, im_acc)) == 7\n"], "sample_120": ["    def test_uuid_serializer(self):\n        test_uuid = uuid.uuid4()\n        serializer = serializer_factory(test_uuid)\n        result, imports = serializer.serialize()\n        self.assertEqual(result, \"uuid.%s\" % repr(test_uuid))\n        self.assertEqual(imports, {\"import uuid\"})\n", "    def test_serialize(self):\n        value = decimal.Decimal('123.456')\n        serializer = DecimalSerializer(value)\n        serialized_value, imports = serializer.serialize()\n        self.assertEqual(serialized_value, \"Decimal('123.456')\")\n        self.assertEqual(imports, {\"from decimal import Decimal\"})\n", "    def test_serializer_factory_with_custom_decimal_subclass(self):\n        value = Money('123.45')\n        serializer = serializer_factory(value)\n        self.assertIsInstance(serializer, DeconstructableSerializer)\n        result, imports = serializer.serialize()\n        self.assertEqual(result, \"Money('123.45')\")\n        self.assertEqual(imports, set())\n", "    def test_decimal_serializer(self):\n        value = decimal.Decimal('123.456')\n        serializer = DecimalSerializer(value)\n        serialized_value, imports = serializer.serialize()\n        self.assertEqual(serialized_value, 'Decimal(\\'123.456\\')')\n        self.assertEqual(imports, {'from decimal import Decimal'})\n", "    def test_decimal_serializer(self):\n        value = decimal.Decimal('123.45')\n        serializer = DecimalSerializer(value)\n        result, imports = serializer.serialize()\n        self.assertEqual(result, \"Decimal('123.45')\")\n        self.assertEqual(imports, {\"from decimal import Decimal\"})\n"], "sample_594": ["def test_last_n_items_with_dask_array(self):\n    import dask.array as da\n\n    # Test with a dask array\n    dask_array = da.from_array(np.arange(10), chunks=(5,))\n    result = formatting.last_n_items(dask_array, 3)\n    np.testing.assert_array_equal(result, np.array([7, 8, 9]))\n", "def test_format_timedelta(self):\n    td = pd.Timedelta('3 days 12:05:03.123456789')\n    assert formatting.format_timedelta(td) == '3 days 12:05:03.123456789'\n    assert formatting.format_timedelta(td, timedelta_format='date') == '3 days'\n    assert formatting.format_timedelta(td, timedelta_format='time') == '12:05:03.123456789'\n", "def test_format_items(self):\n    # Test format_items with different types of data\n    assert formatting.format_items([1, 2, 3]) == ['1', '2', '3']\n    assert formatting.format_items([1.1, 2.2, 3.3]) == ['1.1000', '2.2000', '3.3000']\n    assert formatting.format_items(['a', 'b', 'c']) == [\"'a'\", \"'b'\", \"'c'\"]\n    assert formatting.format_items([np.datetime64('2022-01-01'), np.datetime64('2022-01-02')]) == ['2022-01-01', '2022-01-02']\n    assert formatting.format_items([np.timedelta64(1, 'D'), np.timedelta64(2, 'D')]) == ['1 days', '2 days']\n", "def test_format_array_flat():\n    # Create a 1D array\n    array = np.array([1, 2, 3, 4, 5])\n    max_width = 10\n    expected_output = \"1 2 ... 4 5\"\n    assert formatting.format_array_flat(array, max_width) == expected_output\n\n    # Create a 2D array\n    array = np.array([[1, 2], [3, 4]])\n    max_width = 10\n    expected_output = \"1 2 3 4\"\n    assert formatting.format_array_flat(array, max_width) == expected_output\n\n    # Create an array with large values\n    array = np.array([1000, 2000, 3000, 4000, 5000])\n    max_width = 15\n    expected_output = \"1000 2000 ... 4000 5000\"\n    assert formatting.format_array_flat(array, max_width) == expected_output\n\n    # Create an empty array\n    array = np.array([])\n    max_width = 10\n    expected_output = \"\"\n    assert formatting.format_array_flat(array, max_width) == expected_output\n", "def test_format_items(self):\n    # Test formatting of items with different types\n    input_array = [\n        np.datetime64(\"2022-01-01\"),\n        np.timedelta64(1, \"D\"),\n        \"test_string\",\n        b\"test_bytes\",\n        123.456789,\n        123,\n        True,\n    ]\n    expected_output = [\n        \"2022-01-01\",\n        \"1 days 00:00:00\",\n        \"'test_string'\",\n        \"b'test_bytes'\",\n        \"123.4568\",\n        \"123\",\n        \"True\",\n    ]\n    assert formatting.format_items(input_array) == expected_output\n\n    # Test formatting of timedelta64 with days and time parts\n    input_array = [\n        np.timedelta64(2, \"D\"),\n        np.timedelta64(12 * 60 * 60 + 30 * 60, \"s\"),\n    ]\n    expected_output = [\n        \"2 days\",\n        \"1 days 12:30:00\",\n    ]\n    assert formatting.format_items(input_array) == expected_output\n\n    # Test formatting of timedelta64 with only time part\n    input_array = [\n        np.timedelta64(12 * 60 * 60, \"s\"),\n        np.timedelta64(30 * 60, \"s\"),\n    ]\n    expected_output = [\n        \"12:00:00\",\n        \"00:30:00\",\n    ]\n    assert formatting.format_items(input_array, timedelta_format=\"time\") == expected_output\n\n    # Test formatting of timedelta64 with only date part\n    input_array = [\n        np.timedelta64(2, \"D\"),\n        np.timedelta64(3, \"D\"),\n"], "sample_1200": ["def test_extend_unit_system():\n    base_units = (m, s)\n    units = (kilogram, joule)\n    name = \"TestSystem\"\n    description = \"A test unit system\"\n    dimension_system = SI.get_dimension_system()\n    derived_units = {length*time: speed_of_light}\n\n    test_system = SI.extend(base_units, units, name, description, dimension_system, derived_units)\n\n    assert test_system.name == name\n    assert test_system.descr == description\n    assert test_system._base_units == base_units\n    assert test_system._units == base_units + units\n    assert test_system._derived_units == derived_units\n", "def test_extend():\n    # Test the extend method of UnitSystem\n    new_unit_system = SI.extend(base=[kilogram], units=[grams], name=\"SI_extended\")\n    assert new_unit_system.name == \"SI_extended\"\n    assert kilogram in new_unit_system._base_units\n    assert grams in new_unit_system._units\n", "def test_extend_method():\n    us = SI.extend([Quantity('new_unit', abbrev='nu')], name='NewSystem', description='A new unit system')\n    assert us.name == 'NewSystem'\n    assert us.descr == 'A new unit system'\n    assert len(us._base_units) == len(SI._base_units) + 1\n    assert Quantity('new_unit', abbrev='nu') in us._base_units\n", "def test_extend():\n    base_units = (m, s)\n    units = (kg, amu)\n    name = \"Custom System\"\n    description = \"A custom unit system\"\n    derived_units = {length / time: km / hour}\n    new_system = SI.extend(base_units, units, name, description, derived_units=derived_units)\n    assert new_system.name == name\n    assert new_system.descr == description\n    assert set(new_system._base_units) == set(base_units) | set(units)\n    assert new_system._derived_units == derived_units\n    assert new_system.derived_units == derived_units\n", "def test_unit_system_extended():\n    extended_system = SI.extend(base=[Quantity(\"N\", \"Newton\")], units=[Quantity(\"kgf\", \"kilogram-force\")], name=\"Extended SI\", description=\"SI with Newton and kilogram-force units\")\n    assert extended_system.name == \"Extended SI\"\n    assert extended_system.descr == \"SI with Newton and kilogram-force units\"\n    assert len(extended_system._base_units) == len(SI._base_units) + 1\n    assert len(extended_system._units) == len(SI._units) + 1\n    assert extended_system.get_quantity_dimension(Quantity(\"N\")) == Dimension(mass * length / time**2)\n    assert extended_system.get_quantity_dimension(Quantity(\"kgf\")) == Dimension(mass * length / time**2)\n"], "sample_13": ["def test_latitude_invalid_angles():\n    # Test creating Latitude objects with invalid angles\n    with pytest.raises(ValueError):\n        Latitude(-91 * u.deg)\n    with pytest.raises(ValueError):\n        Latitude(91 * u.deg)\n", "def test_angle_initialization():\n    # Test initialization of Angle with various input formats\n    assert Angle('10.2345d').degree == 10.2345\n    assert Angle(['10.2345d', '-20d']).degree.tolist() == [10.2345, -20.0]\n    assert Angle('1:2:30.43 degrees').degree == pytest.approx(1.04178611, rel=1e-6)\n    assert Angle('1 2 0 hours').hourangle == pytest.approx(1.03333333, rel=1e-6)\n    assert Angle(np.arange(1, 8), unit=u.deg).degree.tolist() == [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0]\n    assert Angle('1\u00b02\u20323\u2033').degree == pytest.approx(1.03416667, rel=1e-6)\n    assert Angle('1\u00b02\u20323\u2033N').degree == pytest.approx(1.03416667, rel=1e-6)\n    assert Angle('1d2m3.4s').degree == pytest.approx(1.03427778, rel=1e-6)\n    assert Angle('1d2m3.4sS').degree == pytest.approx(-1.03427778, rel=1e-6)\n    assert Angle('-1h2m3s').hourangle == pytest.approx(-1.03416667, rel=1e-6)\n    assert Angle('-1h2m3sE').hourangle == pytest.approx(1.03416667, rel=1e-6)\n    assert Angle('-1h2.5m').hourangle == pytest.approx(-1.04166667, rel=1e-6", "def test_latitude_out_of_bounds():\n    with pytest.raises(ValueError):\n        Latitude(95 * u.deg)\n    with pytest.raises(ValueError):\n        Latitude(-95 * u.deg)\n", "def test_angle_init_with_large_degrees():\n    # Test initializing Angle with large degrees\n    a = Angle(1000, unit=u.deg)\n    assert a.value == 1000\n    assert a.unit == u.deg\n    assert a.degree == 1000\n    assert a.hour == 27.77777777777778\n    assert a.radian == 17.453292519943295\n\n    a = Angle([1000, 2000], unit=u.deg)\n    assert_array_equal(a.degree, [1000, 2000])\n    assert_array_equal(a.hour, [27.77777777777778, 55.55555555555556])\n    assert_array_equal(a.radian, [17.453292519943295, 34.90658503988659])\n", "def test_longitude_wrap_angle():\n    # Test that the wrap_angle property of Longitude works correctly\n    lon = Longitude([360.0, 720.0, -360.0], unit=u.deg)\n    assert_allclose(lon.degree, [0.0, 0.0, 0.0])\n\n    lon.wrap_angle = 180.0 * u.deg\n    assert_allclose(lon.degree, [0.0, 0.0, 0.0])\n\n    lon[1] = 180.0 * u.deg\n    assert_allclose(lon.degree, [0.0, 0.0, -180.0])\n"], "sample_921": ["def test_signature_with_bound_method():\n    class Foo:\n            pass\n\n    foo = Foo()\n    sig = inspect.signature(foo.method, bound_method=True)\n    assert str(sig) == '(b, c=3)'\n", "def test_signature_from_str():\n    sig = inspect.signature_from_str('(a: int, b: str = \"default\") -> bool')\n    assert str(sig) == '(a: int, b: str = \\'default\\') -> bool'\n    assert [str(param) for param in sig.parameters.values()] == ['a: int', 'b: str = \\'default\\'']\n    assert str(sig.return_annotation) == 'bool'\n", "def test_stringify_signature():\n    sig = inspect.signature_from_str(\"(a: int, b: str = 'default', *args, **kwargs) -> None\")\n    result = stringify_signature(sig)\n    assert result == \"(a: int, b: str = 'default', *args, **kwargs) -> None\"\n", "def test_getdoc_inherited_decorated_method():\n    class Base:\n        @classmethod\n            \"\"\"Docstring for the decorated method.\"\"\"\n            pass\n\n    class Derived(Base):\n        pass\n\n    doc = inspect.getdoc(Derived.decorated_method, cls=Derived, name='decorated_method')\n    assert doc == \"Docstring for the decorated method.\"\n", "def test_is_builtin_class_method():\n    assert is_builtin_class_method(int, '__init__')\n    assert not is_builtin_class_method(int, 'bit_length')\n    assert is_builtin_class_method(str, '__init__')\n    assert not is_builtin_class_method(str, 'capitalize')\n\n    # check non-existing method\n    assert not is_builtin_class_method(int, '__non_existing__')\n\n    # check non-class object\n    assert not is_builtin_class_method('not a class', '__init__')\n"], "sample_456": ["    def test_formset_with_non_form_error(self):\n        formset = self.make_choiceformset(\n            formset_data=[(\"choice1\", 1)],\n            formset_class=ChoiceFormsetWithNonFormError,\n        )\n        self.assertFalse(formset.is_valid())\n        self.assertEqual(len(formset.non_form_errors()), 1)\n        self.assertEqual(formset.non_form_errors()[0], \"non-form error\")\n", "def test_custom_kwarg_form(self):\n    CustomKwargFormSet = formset_factory(CustomKwargForm, formset=BaseFormSet, extra=2)\n    formset = CustomKwargFormSet(form_kwargs={'custom_kwarg': 'test'})\n    for form in formset:\n        self.assertEqual(form.custom_kwarg, 'test')\n", "def test_formset_validation_with_error_list(self):\n    \"\"\"\n    Test formset validation with an error_class that is a subclass of ErrorList.\n    \"\"\"\n    class CustomErrorList(ErrorList):\n            return self.as_divs()\n\n    formset = self.make_choiceformset(\n        formset_data=[(\"coffee\", \"3\"), (\"tea\", \"5\")],\n        error_class=CustomErrorList,\n    )\n\n    self.assertTrue(formset.is_valid())\n    self.assertIsInstance(formset.non_form_errors(), CustomErrorList)\n    self.assertIsInstance(formset.errors[0], CustomErrorList)\n", "def test_custom_form_kwargs(self):\n    class CustomKwargFormSet(formset_factory(CustomKwargForm, extra=2)):\n            return {\"custom_kwarg\": \"custom_value\"}\n\n    formset = CustomKwargFormSet()\n    for form in formset:\n        self.assertEqual(form.custom_kwarg, \"custom_value\")\n", "def test_management_form_defaults(self):\n    \"\"\"\n    Test that the management form defaults are correctly populated.\n    \"\"\"\n    formset = self.make_choiceformset(\n        formset_data=[(\"choice1\", 1), (\"choice2\", 2)],\n        formset_class=ChoiceFormSet,\n        initial_forms=0,\n        max_num_forms=3,\n        min_num_forms=1,\n    )\n\n    management_form = formset.management_form\n    self.assertEqual(management_form.initial[TOTAL_FORM_COUNT], 2)\n    self.assertEqual(management_form.initial[INITIAL_FORM_COUNT], 0)\n    self.assertEqual(management_form.initial[MIN_NUM_FORM_COUNT], 1)\n    self.assertEqual(management_form.initial[MAX_NUM_FORM_COUNT], 3)\n"], "sample_585": ["def test_consolidate_slices():\n    slices = [slice(0, 5), slice(5, 10), slice(10, 15, 2)]\n    result = _consolidate_slices(slices)\n    assert result == [slice(0, 10), slice(10, 15, 2)]\n", "def test_consolidate_slices():\n    slices = [slice(0, 5), slice(5, 10), slice(10, 20, 2)]\n    consolidated = _consolidate_slices(slices)\n    assert consolidated == [slice(0, 10), slice(10, 20, 2)]\n\n    slices = [slice(0, 5), slice(5, 10, 2), slice(10, 20)]\n    consolidated = _consolidate_slices(slices)\n    assert consolidated == slices\n", "def test_consolidate_slices():\n    slices = [slice(0, 5), slice(5, 10), slice(10, 15, 2)]\n    result = _consolidate_slices(slices)\n    expected = [slice(0, 10), slice(10, 15, 2)]\n    assert result == expected\n", "def test_consolidate_slices():\n    # Test with a list of slices that can be consolidated\n    slices = [slice(0, 5), slice(5, 10), slice(10, 15)]\n    assert _consolidate_slices(slices) == [slice(0, 15, None)]\n\n    # Test with a list of slices that cannot be consolidated\n    slices = [slice(0, 5), slice(6, 10), slice(10, 15)]\n    assert _consolidate_slices(slices) == [slice(0, 5, None), slice(6, 10, None), slice(10, 15, None)]\n\n    # Test with a list of slices with step\n    slices = [slice(0, 5, 2), slice(6, 10, 3), slice(10, 15, 1)]\n    assert _consolidate_slices(slices) == [slice(0, 5, 2), slice(6, 10, 3), slice(10, 15, 1)]\n", "def test_consolidate_slices():\n    slices = [slice(0, 3), slice(3, 5), slice(5, 10, 2)]\n    result = _consolidate_slices(slices)\n    assert result == [slice(0, 5), slice(5, 10, 2)]\n"], "sample_147": ["def test_queryset_union(self):\n    qs1 = Number.objects.filter(num__gt=4)\n    qs2 = Number.objects.filter(num__lt=6)\n    union_qs = qs1.union(qs2)\n    self.assertNumbersEqual(union_qs, [5, 6, 7, 8, 9])\n", "def test_union_with_empty_queryset(self):\n    qs1 = Number.objects.filter(num__gte=5)\n    qs2 = Number.objects.none()\n    qs_union = qs1.union(qs2)\n    self.assertNumbersEqual(qs_union, [5, 6, 7, 8, 9])\n", "def test_union_all(self):\n    qs1 = Number.objects.filter(num__gte=5)\n    qs2 = Number.objects.filter(num__lt=5)\n    union_qs = qs1.union(qs2, all=True)\n    self.assertNumbersEqual(union_qs, range(10), ordered=False)\n", "    def test_union_different_values_select(self):\n        qs1 = Number.objects.values('num')\n        qs2 = Number.objects.values('other_num')\n        with self.assertRaisesMessage(TypeError, \"Merging 'QuerySet' classes must involve the same values in each case.\"):\n            qs1.union(qs2)\n", "def test_union_with_empty_queryset(self):\n    qs1 = Number.objects.filter(num__gt=5)\n    qs2 = Number.objects.none()\n    union_qs = qs1.union(qs2)\n    self.assertNumbersEqual(union_qs, [6, 7, 8, 9])\n"], "sample_160": ["    def test_large_float_number(self):\n        number = 1.23456789e100\n        formatted_number = nformat(number, decimal_sep='.', decimal_pos=2, grouping=3)\n        self.assertEqual(formatted_number, '123,456,789,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000.00e100')\n", "def test_format_scientific_notation(self):\n    # Test the format function with a large number that should be formatted in scientific notation\n    number = 123456789123456789123456789.123456789\n    result = nformat(number, decimal_sep='.', decimal_pos=3, use_l10n=False)\n    self.assertEqual(result, '1.235e+26')\n", "    def test_scientific_notation(self):\n        # Test that very large or small numbers are formatted using scientific notation\n        self.assertEqual(nformat(123456789123456789123456789, decimal_sep='.', decimal_pos=2), '1.23e+29')\n        self.assertEqual(nformat(0.0000000000000000000000000123, decimal_sep='.', decimal_pos=2), '1.23e-25')\n", "    def test_float_with_exponent(self):\n        number = float(123456789012345678901234567890.12345678901234567890)\n        formatted = nformat(number, decimal_sep='.', decimal_pos=6, grouping=3, thousand_sep=',')\n        self.assertEqual(formatted, '123,456,789,012,345,678,901,234,567,890.123457')\n", "    def test_format_scientific_notation(self):\n        # Test a number that should be formatted in scientific notation\n        number = Decimal('1.23456789123456789e200')\n        result = nformat(number, decimal_sep='.', decimal_pos=4)\n        self.assertEqual(result, '1.2346e+200')\n"], "sample_1197": ["def test_get_dimensional_expr():\n    # Testing get_dimensional_expr method with a simple expression\n    expr = 2 * meter / second\n    dim = SI.get_dimensional_expr(expr)\n    assert dim == length / time\n\n    # Testing with a derived quantity\n    expr = joule / (coulomb * volt)\n    dim = SI.get_dimensional_expr(expr)\n    assert dim == Dimension(1)\n\n    # Testing with a function\n    expr = sin(2 * meter / second)\n    dim = SI.get_dimensional_expr(expr)\n    assert dim == Dimension(1)\n\n    # Testing with a derivative\n    t, x = symbols('t x')\n    expr = diff(sin(x), t)\n    dim = SI.get_dimensional_expr(expr)\n    assert dim == Dimension(1)\n", "def test_get_dimensional_expr():\n    # Test get_dimensional_expr method with a simple quantity\n    assert SI.get_dimensional_expr(Quantity('meter')) == 'length'\n\n    # Test get_dimensional_expr method with a complex expression\n    x, y, z = symbols('x y z')\n    expr = 2*Quantity('meter')*x + Quantity('second')*y**2/Quantity('kilogram')\n    assert SI.get_dimensional_expr(expr) == 'length/mass'\n\n    # Test get_dimensional_expr method with a derivative\n    expr = diff(sin(x), x)\n    assert SI.get_dimensional_expr(expr) == S.One\n", "def test_get_dimensional_expr():\n    ds = SI.get_dimension_system()\n\n    # Test with a simple quantity\n    assert SI.get_dimensional_expr(Quantity('length', 5, 'meter')) == ds.length.name\n\n    # Test with a product of quantities\n    assert SI.get_dimensional_expr(Quantity('area', 9, 'meter**2')) == ds.area.name\n\n    # Test with a derivative\n    x = Symbol('x')\n    assert SI.get_dimensional_expr(diff(sin(x), x)) == ds.length**(-1)\n\n    # Test with a function\n    f = Function('f')\n    assert SI.get_dimensional_expr(f(Quantity('time', 2, 'second'))) == S.One\n\n    # Test with a dimensionless quantity\n    assert SI.get_dimensional_expr(Quantity('dimensionless', 3)) == S.One\n", "def test_unit_system_get_quantity_scale_factor():\n    # Test getting the scale factor of a quantity\n    assert SI.get_quantity_scale_factor(meter) == 1  # base unit\n    assert SI.get_quantity_scale_factor(second) == 1  # base unit\n    assert SI.get_quantity_scale_factor(kilogram) == 1  # base unit\n    assert SI.get_quantity_scale_factor(ampere) == 1  # base unit\n    assert SI.get_quantity_scale_factor(kelvin) == 1  # base unit\n    assert SI.get_quantity_scale_factor(mole) == 1  # base unit\n    assert SI.get_quantity_scale_factor(candela) == 1  # base unit\n    assert SI.get_quantity_scale_factor(kilometer) == 1000  # derived unit\n    assert SI.get_quantity_scale_factor(joule) == 1  # derived unit\n    assert SI.get_quantity_scale_factor(volt) == 1  # derived unit\n    assert SI.get_quantity_scale_factor(ohm) == 1  # derived unit\n", "def test_extend_unit_system():\n    base_units = (meter, second)\n    units = (centimeter, kilogram)\n    name = \"CGS\"\n    description = \"Centimeter-gram-second system\"\n    dimension_system = SI.get_dimension_system()\n    derived_units = {length: centimeter}\n\n    cgs = SI.extend(base_units, units, name, description, dimension_system, derived_units)\n\n    assert cgs.dim == 2\n    assert cgs.is_consistent == SI.is_consistent\n    assert cgs.derived_units == {length: centimeter}\n"], "sample_751": ["def test_adaboost_classifier_sparse_input():\n    X_csr = csr_matrix(X)\n    X_csc = csc_matrix(X)\n    X_coo = coo_matrix(X)\n    X_dok = dok_matrix(X)\n    X_lil = lil_matrix(X)\n\n    ada_csr = AdaBoostClassifier(n_estimators=3, random_state=rng)\n    ada_csc = AdaBoostClassifier(n_estimators=3, random_state=rng)\n    ada_coo = AdaBoostClassifier(n_estimators=3, random_state=rng)\n    ada_dok = AdaBoostClassifier(n_estimators=3, random_state=rng)\n    ada_lil = AdaBoostClassifier(n_estimators=3, random_state=rng)\n\n    ada_csr.fit(X_csr, y_class)\n    ada_csc.fit(X_csc, y_class)\n    ada_coo.fit(X_coo, y_class)\n    ada_dok.fit(X_dok, y_class)\n    ada_lil.fit(X_lil, y_class)\n\n    assert_array_equal(ada_csr.predict(T), ada_csc.predict(T))\n    assert_array_equal(ada_csr.predict(T), ada_coo.predict(T))\n    assert_array_equal(ada_csr.predict(T), ada_dok.predict(T))\n    assert_array_equal(ada_csr.predict(T), ada_lil.predict(T))\n\n    assert_array_almost_equal(ada_csr.predict_proba(T), ada_csc.predict_proba(T))\n    assert_array_almost_equal(ada_csr.predict_proba(T), ada_coo.predict_", "def test_ada_boost_classifier_with_weighted_samples():\n    # Test AdaBoostClassifier with weighted samples\n    X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n    sample_weight = rng.rand(y_train.size)\n    clf = AdaBoostClassifier(n_estimators=100, random_state=rng)\n    clf.fit(X_train, y_train, sample_weight=sample_weight)\n    score = clf.score(X_test, y_test)\n    assert_greater(score, 0.9)\n", "def test_adaboost_with_decision_stump():\n    X, y = datasets.make_classification(n_samples=100, n_features=20,\n                                        n_informative=2, n_redundant=10,\n                                        n_classes=2, random_state=0,\n                                        shuffle=False)\n    clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),\n                             n_estimators=100, random_state=rng)\n    clf.fit(X, y)\n    assert_greater(clf.score(X, y), 0.9)\n", "def test_adaboost_regressor_sparse_input():\n    X_sparse = csr_matrix(X)\n    clf = AdaBoostRegressor(n_estimators=10, random_state=rng)\n    clf.fit(X_sparse, y_regr)\n    assert_array_almost_equal(clf.predict(T), y_t_regr)\n    assert_array_almost_equal(clf.predict(X_sparse), y_regr)\n", "def test_adaboost_classifier_predict_proba():\n    clf = AdaBoostClassifier(n_estimators=10, random_state=rng)\n    clf.fit(X, y_class)\n    proba = clf.predict_proba(T)\n    assert proba.shape == (3, 2)\n    assert_array_almost_equal(proba, np.array([[0.6002448, 0.3997552],\n                                               [0.28295856, 0.71704144],\n                                               [0.14361473, 0.85638527]]))\n"], "sample_892": ["def test_baseweightboosting_feature_importances_not_fitted():\n    clf = AdaBoostClassifier()\n    with pytest.raises(ValueError, match=\"Estimator not fitted\"):\n        clf.feature_importances_\n", "def test_adaboost_classifier_with_decision_tree_and_iris_dataset():\n    # Test AdaBoostClassifier with DecisionTreeClassifier and iris dataset\n    X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=rng)\n    clf = AdaBoostClassifier(\n        DecisionTreeClassifier(max_depth=1),\n        n_estimators=300,\n        random_state=rng\n    )\n    clf.fit(X_train, y_train)\n    score = clf.score(X_test, y_test)\n    assert score > 0.9, \"AdaBoostClassifier with DecisionTreeClassifier and iris dataset should have accuracy > 0.9\"\n", "def test_adaboost_classifier_discrete():\n    X, y = iris.data, iris.target\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=rng)\n\n    clf = AdaBoostClassifier(n_estimators=100, algorithm=\"SAMME\", random_state=rng)\n    clf.fit(X_train, y_train)\n\n    y_pred = clf.predict(X_test)\n    accuracy = np.mean(y_pred == y_test)\n    assert accuracy > 0.85, \"Accuracy is too low for discrete SAMME algorithm\"\n", "def test_samme_proba(estimator, expected_shape):\n    estimator.fit(X, y_class)\n    proba = _samme_proba(estimator, 2, X)\n    assert proba.shape == expected_shape\n    assert_array_less(np.abs(proba.sum(axis=1) - 0), 1e-15)\n", "def test_adaboost_classifier_stumps():\n    clf = AdaBoostClassifier(\n        DecisionTreeClassifier(max_depth=1),\n        n_estimators=3,\n        random_state=rng,\n    )\n    clf.fit(iris.data, iris.target)\n    assert len(clf.estimators_) == 3\n    assert_array_equal(clf.estimator_errors_, [0.33333333, 0.11111111, 0.11111111])\n    assert_array_almost_equal(\n        clf.estimator_weights_, [0.91824105, 0.26130248, 0.47687022]\n    )\n    assert clf.score(iris.data, iris.target) >= 0.9\n\n    # Test predict\n    y_pred = clf.predict(iris.data)\n    assert len(y_pred) == len(iris.target)\n\n    # Test staged_predict\n    y_pred_staged = list(clf.staged_predict(iris.data))\n    assert len(y_pred_staged) == 3\n    assert_array_equal(y_pred_staged[0], y_pred)\n\n    # Test decision_function\n    decision_function = clf.decision_function(iris.data)\n    assert decision_function.shape == (len(iris.target), 3)\n\n    # Test staged_decision_function\n    decision_function_staged = list(clf.staged_decision_function(iris.data))\n    assert len(decision_function_staged) == 3\n    assert_array_equal(decision_function_staged[0], decision_function)\n\n    # Test predict_proba\n    proba = clf.predict_proba(iris.data)\n    assert proba."], "sample_860": ["def test_check_sample_weight():\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    sample_weight = np.array([0.5, 0.3, 0.2])\n    result = _check_sample_weight(sample_weight, X)\n    assert_array_equal(result, sample_weight)\n\n    sample_weight = 2\n    result = _check_sample_weight(sample_weight, X)\n    assert_array_equal(result, np.full(X.shape[0], sample_weight))\n\n    sample_weight = None\n    result = _check_sample_weight(sample_weight, X)\n    assert_array_equal(result, np.ones(X.shape[0]))\n\n    sample_weight = np.array([0.5, 0.3])\n    assert_raises(ValueError, _check_sample_weight, sample_weight, X)\n\n    sample_weight = np.array([[0.5, 0.3], [0.2, 0.1]])\n    assert_raises(ValueError, _check_sample_weight, sample_weight, X)\n", "def test_check_scalar():\n    # Test valid inputs\n    assert check_scalar(5, 'test_param', int) is None\n    assert check_scalar(5.0, 'test_param', float) is None\n    assert check_scalar(5.0, 'test_param', (float, int)) is None\n    assert check_scalar(5, 'test_param', (float, int)) is None\n    assert check_scalar(5, 'test_param', int, min_val=3) is None\n    assert check_scalar(5, 'test_param', int, max_val=7) is None\n    assert check_scalar(5, 'test_param', int, min_val=3, max_val=7) is None\n\n    # Test invalid inputs\n    assert_raises(TypeError, check_scalar, '5', 'test_param', int)\n    assert_raises(ValueError, check_scalar, 2, 'test_param', int, min_val=3)\n    assert_raises(ValueError, check_scalar, 8, 'test_param', int, max_val=7)\n    assert_raises(ValueError, check_scalar, 9, 'test_param', int, min_val=3, max_val=7)\n", "def test_check_X_y_warn_on_dtype():\n    # Test that check_X_y warns when the dtype of the input data structure does not match the requested dtype\n    X = np.array([[1, 2], [3, 4]], dtype=np.float64)\n    y = np.array([1, 0], dtype=np.int64)\n\n    with pytest.warns(DataConversionWarning):\n        X_converted, y_converted = check_X_y(X, y, dtype=np.float32, warn_on_dtype=True)\n\n    # Check that the data was converted to the requested dtype\n    assert X_converted.dtype == np.float32\n    assert y_converted.dtype == np.float32\n", "def test_check_scalar():\n    # Test check_scalar with valid input\n    x = 10\n    name = \"test_param\"\n    target_type = int\n    min_val = 5\n    max_val = 20\n    check_scalar(x, name, target_type, min_val, max_val)\n\n    # Test check_scalar with invalid type\n    x = \"invalid_type\"\n    with pytest.raises(TypeError):\n        check_scalar(x, name, target_type, min_val, max_val)\n\n    # Test check_scalar with value below minimum\n    x = 4\n    with pytest.raises(ValueError):\n        check_scalar(x, name, target_type, min_val, max_val)\n\n    # Test check_scalar with value above maximum\n    x = 21\n    with pytest.raises(ValueError):\n        check_scalar(x, name, target_type, min_val, max_val)\n", "def test_check_non_negative_sparse():\n    X = sparse_random_matrix(10, 10, density=0.5, random_state=42)\n    X.data[:] = np.abs(X.data)\n    check_non_negative(X, \"test\")\n\n    X.data[0] = -1\n    assert_raises(ValueError, check_non_negative, X, \"test\")\n"], "sample_995": ["def test_integer_as_content_primitive():\n    assert S(3).as_content_primitive() == (3, 1)\n    assert S(-3).as_content_primitive() == (3, -1)\n    assert S(0).as_content_primitive() == (1, 0)\n", "def test_integer_nthroot():\n    assert integer_nthroot(125, 3) == (5, True)\n    assert integer_nthroot(124, 3) == (4, False)\n", "compilation error", "compilation error", "def test_real_field():\n    RF = RealField(20)\n    assert RF.is_real(2)\n    assert RF.is_real(2.0)\n    assert RF.is_real(RF(2))\n    assert not RF.is_real(2 + I)\n"], "sample_1205": ["def test_poly_evaluate():\n    R, x, y = ring(\"x,y\", ZZ)\n    p = x**2 + y**2\n    assert p.evaluate(x, 3) == 9 + y**2\n    assert p.evaluate(y, 4) == x**2 + 16\n    assert p.evaluate([(x, 3), (y, 4)]) == 25\n", "def test_subtraction():\n    _, x, y = ring(\"x, y\", ZZ)\n    p1 = x + y**2\n    p2 = x*y + y**2\n    assert p1 - p2 == -x*y + x\n", "def test_poly_evaluate():\n    R, x, y = ring('x, y', ZZ)\n    p = x + 2*x**2*y**3\n    assert p.evaluate(x, 3) == 54\n    assert p.evaluate(y, 2) == x + 16*x**2\n", "def test_poly_ring_drop_to_ground():\n    R, x, y, z = ring(\"x,y,z\", ZZ, lex)\n    f = x**2 + y**2 + z**2\n    g = f.drop_to_ground(x)\n    assert g == y**2 + z**2 + sqrt(2)*x*sqrt(x**2 + y**2 + z**2)\n", "def test_ring_constructors():\n    R, x, y, z = ring(\"x,y,z\", ZZ, lex)\n    assert R == PolyRing(symbols(\"x,y,z\"), ZZ, lex)\n    assert isinstance(R(x), PolyElement)\n    assert R.symbols == (x, y, z)\n    assert R.ngens == 3\n    assert R.domain == ZZ\n    assert R.order == lex\n    assert R.zero == PolyElement(R, [])\n    assert R.one == PolyElement(R, [((0, 0, 0), 1)])\n"], "sample_198": ["def test_expression_wrapper(self):\n    expression = F('num_employees') + 1\n    wrapped_expression = ExpressionWrapper(expression, IntegerField())\n    self.assertEqual(str(wrapped_expression), str(expression))\n    self.assertEqual(wrapped_expression.output_field, IntegerField())\n", "def test_expression_wrapper(self):\n    expr = ExpressionWrapper(Value(1), output_field=IntegerField())\n    self.assertEqual(expr.as_sql(self.company_query.query.get_compiler(connection.alias), connection)[0], '1')\n", "def test_expression_list(self):\n    expr_list = ExpressionList(F('num_employees'), F('num_chairs'))\n    query = self.company_query.annotate(expr_list=expr_list)\n    self.assertNumQueries(1)\n    self.assertEqual(\n        str(query.query),\n        'SELECT \"tests_company\".\"name\", \"tests_company\".\"num_employees\", \"tests_company\".\"num_chairs\", '\n        '\"tests_company\".\"num_employees\", \"tests_company\".\"num_chairs\" FROM \"tests_company\" '\n        'ORDER BY \"tests_company\".\"name\", \"tests_company\".\"num_employees\", \"tests_company\".\"num_chairs\"'\n    )\n", "def test_integer_field_expression(self):\n    expr = F('num_employees') + 5\n    self.assertIsInstance(expr, Combinable)\n    self.assertIsInstance(expr, Expression)\n    self.assertEqual(str(expr), \"num_employees + 5\")\n    self.assertIn(str(expr), str(self.company_query.annotate(extra_employees=expr).query))\n", "def test_expression_wrapper_as_sql(self):\n    expression = F('num_employees') + 1\n    wrapped_expression = ExpressionWrapper(expression, output_field=IntegerField())\n    sql, params = wrapped_expression.as_sql(self.company_query.query.compiler, connection)\n    expected_sql = '(\"example_company\".\"num_employees\" + %s)'\n    self.assertEqual(sql, expected_sql)\n    self.assertEqual(params, [1])\n"], "sample_191": ["    def test_import_error_files(self):\n        filename = self.temporary_file('error_module.py')\n        filename.write_text('syntax error')\n        try:\n            py_compile.compile(filename, doraise=True)\n        except SyntaxError:\n            pass\n        self.assertFileNotFound(filename)\n        with self.assertRaises(SyntaxError):\n            import_module(filename.stem)\n        self.assertFileFound(filename)\n", "    def test_zip_imported_modules(self):\n        zip_path = self.temporary_file('test.zip')\n        with zipfile.ZipFile(zip_path, 'w') as zipf:\n            zipf.writestr('test_module.py', 'print(\"Hello, world!\")')\n        sys.path_importer_cache.clear()\n        self.import_and_cleanup('test_module')\n        self.assertFileFound(zip_path)\n", "    def test_zip_imported_modules(self):\n        zip_filename = self.temporary_file('test.zip')\n        with zipfile.ZipFile(zip_filename, 'w') as zf:\n            zf.writestr('zip_module.py', 'print(\"Hello from zip\")')\n        sys.path_importer_cache.clear()\n        with extend_sys_path(str(zip_filename)):\n            import_module('zip_module')\n            self.assertFileFound(zip_filename)\n            self.addCleanup(lambda: sys.modules.pop('zip_module', None))\n            self.addCleanup(lambda: sys.path_importer_cache.clear())\n", "    def test_import_error(self):\n        # Test that an import error is handled correctly.\n        filename = self.temporary_file('error_module.py')\n        filename.write_text('raise ImportError()')\n        autoreload._error_files.clear()\n        with self.assertRaises(ImportError):\n            self.import_and_cleanup('error_module')\n        self.assertFileFound(filename)\n        self.assertIn(filename, autoreload._error_files)\n", "    def test_iter_modules_and_files_with_zipped_module(self):\n        # Create a temporary directory and add a zip file containing a module\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            zip_path = Path(tmpdirname) / 'test_module.zip'\n            with zipfile.ZipFile(zip_path, 'w') as zf:\n                zf.writestr('test_module.py', 'print(\"Hello, world!\")')\n            sys.path.insert(0, str(zip_path))\n            self.addCleanup(sys.path.remove, str(zip_path))\n            self.import_and_cleanup('test_module')\n            self.assertFileFound(zip_path)\n"], "sample_629": ["def test_expand_modules():\n    files_or_modules = ['test_module', 'tests/test_file.py', 'ignore_module']\n    ignore_list = ['ignore_module']\n    ignore_list_re = [re.compile('ignore_pattern')]\n    ignore_list_paths_re = [re.compile('ignore_path_pattern')]\n\n    result, errors = expand_modules(files_or_modules, ignore_list, ignore_list_re, ignore_list_paths_re)\n\n    assert len(result) > 0\n    assert len(errors) == 0\n    assert 'test_module' in [item['name'] for item in result]\n    assert 'ignore_module' not in [item['name'] for item in result]\n", "def test_expand_modules():\n    files_or_modules = ['path/to/test_module.py', 'test_package']\n    ignore_list = ['__init__.py']\n    ignore_list_re = [re.compile(r'.*ignore_this_.*')]\n    ignore_list_paths_re = [re.compile(r'path/to/ignore')]\n\n    result, errors = expand_modules(files_or_modules, ignore_list, ignore_list_re, ignore_list_paths_re)\n\n    assert len(result) > 0\n    assert len(errors) == 0\n    for item in result:\n        assert 'path' in item\n        assert 'name' in item\n        assert 'isarg' in item\n        assert 'basepath' in item\n        assert 'basename' in item\n", "def test_expand_modules():\n    files_or_modules = ['test_module.py', 'another_module', './ignore_module.py']\n    ignore_list = ['ignore_module.py']\n    ignore_list_re = [re.compile('.*_ignore_.*')]\n    ignore_list_paths_re = [re.compile('.*ignore_path.*')]\n\n    result, errors = expand_modules(files_or_modules, ignore_list, ignore_list_re, ignore_list_paths_re)\n\n    assert len(result) == 2\n    assert len(errors) == 0\n    assert result[0]['name'] == 'test_module'\n    assert result[1]['name'] == 'another_module'\n", "def test_expand_modules():\n    files_or_modules = [\"test_module.py\", \"test_package\"]\n    ignore_list = []\n    ignore_list_re = [re.compile(r'__pycache__')]\n    ignore_list_paths_re = []\n\n    result, errors = expand_modules(files_or_modules, ignore_list, ignore_list_re, ignore_list_paths_re)\n\n    # Add assertions here based on the expected behavior\n    assert len(result) > 0  # At least one module should be expanded\n    assert len(errors) == 0  # No errors should be present\n", "def test_expand_modules():\n    files_or_modules = [\"module1\", \"path/to/module2\", \"path/to/ignore_module\"]\n    ignore_list = [\"ignore_module\"]\n    ignore_list_re = [re.compile(\"ignore_.*\")]\n    ignore_list_paths_re = [re.compile(\"path/to/ignore_.*\")]\n\n    result, errors = expand_modules(files_or_modules, ignore_list, ignore_list_re, ignore_list_paths_re)\n\n    assert len(result) > 0\n    assert len(errors) == 0\n    assert not any(_is_in_ignore_list_re(item[\"name\"], ignore_list_re) for item in result)\n    assert not any(_is_in_ignore_list_re(item[\"path\"], ignore_list_paths_re) for item in result)\n    assert \"ignore_module\" not in [item[\"name\"] for item in result]\n"], "sample_537": ["    def test_stride_windows_same_data(self, shape):\n        x = np.arange(20)\n        x.shape = shape\n        NFFT = 10\n        noverlap = 2\n        target = self.calc_window_target(x, NFFT, noverlap)\n        result = mlab.stride_windows(x, NFFT, noverlap)\n        assert_array_equal(result, target)\n", "def test_stride_windows_noverlap_greater_than_n(self, shape):\n    x = np.random.rand(*shape)\n    NFFT = 5\n    noverlap = NFFT + 1\n    with pytest.raises(ValueError):\n        mlab.stride_windows(x, NFFT, noverlap)\n", "    def test_stride_windows_shape(self, shape):\n        x = np.arange(100).reshape(shape)\n        NFFT = 10\n        noverlap = 5\n        result = mlab.stride_windows(x, NFFT, noverlap)\n        target = self.calc_window_target(x.ravel(), NFFT, noverlap)\n        assert_array_equal(result, target)\n", "def test_stride_windows_invalid_input(self, shape):\n    x = np.zeros(shape)\n    with pytest.raises(ValueError):\n        mlab.stride_windows(x, 5, noverlap=6)\n", "def test_stride_windows_axis(self, shape):\n    x = np.arange(20).reshape(shape)\n    NFFT = 5\n    noverlap = 2\n    axis = 1\n    target = self.calc_window_target(x, NFFT, noverlap, axis)\n    result = mlab.stride_windows(x, NFFT, noverlap, axis)\n    assert_array_equal(result, target)\n"], "sample_607": ["def test_detect_parameters():\n    assert plugins.detect_parameters(DummyBackendEntrypoint1.open_dataset) == (\"decoder\",)\n\n    with pytest.raises(TypeError):\n        plugins.detect_parameters(DummyBackendEntrypointArgs.open_dataset)\n\n    with pytest.raises(TypeError):\n        plugins.detect_parameters(DummyBackendEntrypointKwargs.open_dataset)\n", "def test_detect_parameters():\n    # Test detect_parameters with a backend that accepts positional args\n    with pytest.raises(TypeError):\n        plugins.detect_parameters(DummyBackendEntrypointArgs.open_dataset)\n\n    # Test detect_parameters with a backend that accepts kwargs\n    with pytest.raises(TypeError):\n        plugins.detect_parameters(DummyBackendEntrypointKwargs.open_dataset)\n\n    # Test detect_parameters with a backend that accepts a keyword argument\n    assert plugins.detect_parameters(DummyBackendEntrypoint1.open_dataset) == (\"decoder\",)\n\n    # Test detect_parameters with a backend that accepts the same keyword argument as another\n    assert plugins.detect_parameters(DummyBackendEntrypoint2.open_dataset) == (\"decoder\",)\n", "def test_remove_duplicates_warning(dummy_duplicated_entrypoints, caplog):\n    unique_eps = plugins.remove_duplicates(dummy_duplicated_entrypoints)\n    assert len(unique_eps) == 2\n    assert \"Found 2 entrypoints for the engine name engine1\" in caplog.text\n    assert \"Found 2 entrypoints for the engine name engine2\" in caplog.text\n", "def test_detect_parameters():\n    params = plugins.detect_parameters(DummyBackendEntrypoint1.open_dataset)\n    assert params == (\"filename_or_obj\", \"decoder\")\n\n    with pytest.raises(TypeError):\n        plugins.detect_parameters(DummyBackendEntrypointArgs.open_dataset)\n\n    with pytest.raises(TypeError):\n        plugins.detect_parameters(DummyBackendEntrypointKwargs.open_dataset)\n", "def test_detect_parameters():\n    # Test with *args\n    with pytest.raises(TypeError):\n        plugins.detect_parameters(DummyBackendEntrypointArgs.open_dataset)\n\n    # Test with **kwargs\n    with pytest.raises(TypeError):\n        plugins.detect_parameters(DummyBackendEntrypointKwargs.open_dataset)\n\n    # Test with explicit parameters\n    assert plugins.detect_parameters(DummyBackendEntrypoint1.open_dataset) == (\"decoder\",)\n\n    # Test with same parameters\n    assert plugins.detect_parameters(DummyBackendEntrypoint2.open_dataset) == (\"decoder\",)\n"], "sample_164": ["def test_require_debug_true_filter(self):\n    record = logging.LogRecord('test', logging.INFO, '', 0, '', (), None)\n    f = RequireDebugTrue()\n    self.assertEqual(f.filter(record), True)\n", "def test_require_debug_true_filter(self):\n    f = RequireDebugTrue()\n    record = logging.LogRecord('name', logging.INFO, 'pathname', 'lineno', 'msg', None, None)\n    self.assertEqual(f.filter(record), True)\n", "def test_require_debug_true_filter(self):\n    f = RequireDebugTrue()\n    record = logging.LogRecord('name', logging.INFO, 'pathname', 1, 'msg', args=None, exc_info=None)\n    self.assertEqual(f.filter(record), True)\n", "def test_require_debug_true_filter(self):\n    record = logging.LogRecord('django.request', logging.INFO, 'pathname', 0, 'msg', None, None)\n    filter = RequireDebugTrue()\n    self.assertEqual(filter.filter(record), True)\n", "    def test_admin_email_handler(self):\n        logger = logging.getLogger('django.request')\n        request = RequestFactory().get('/')\n        try:\n            raise PermissionDenied\n        except PermissionDenied:\n            logger.error('Permission denied', exc_info=True, extra={'request': request})\n\n        self.assertEqual(len(mail.outbox), 1)\n        email = mail.outbox[0]\n        self.assertEqual(email.subject, 'ERROR (EXTERNAL IP): Permission denied')\n        self.assertEqual(email.to, ['admin@example.com'])\n        self.assertIn('PermissionDenied at /', email.body)\n        self.assertIn('Exception Type: PermissionDenied', email.body)\n"], "sample_106": ["    def test_patch_cache_control(self):\n        response = HttpResponse()\n        patch_cache_control(response, private=True, max_age=60, no_cache=True)\n        self.assertEqual(response['Cache-Control'], 'private, max-age=60, no-cache')\n", "    def test_has_vary_header(self):\n        response = HttpResponse()\n        patch_vary_headers(response, ['Accept-Language'])\n        self.assertTrue(has_vary_header(response, 'Accept-Language'))\n        self.assertFalse(has_vary_header(response, 'Accept-Encoding'))\n", "    def test_patch_cache_control(self):\n        response = HttpResponse()\n        patch_cache_control(response, max_age=60, no_cache=True, public=True)\n        self.assertEqual(response['Cache-Control'], 'max-age=60, no-cache, public')\n\n        response = HttpResponse()\n        response['Cache-Control'] = 'max-age=120, private'\n        patch_cache_control(response, max_age=60, public=True)\n        self.assertEqual(response['Cache-Control'], 'max-age=60, public')\n", "    def test_patch_vary_headers(self):\n        response = HttpResponse()\n        newheaders = ['Accept-Language', 'Cookie']\n        patch_vary_headers(response, newheaders)\n        self.assertEqual(response['Vary'], 'Accept-Language, Cookie')\n\n        newheaders = ['*']\n        patch_vary_headers(response, newheaders)\n        self.assertEqual(response['Vary'], '*')\n\n        newheaders = ['Accept-Encoding']\n        patch_vary_headers(response, newheaders)\n        self.assertEqual(response['Vary'], '*')\n\n        response = HttpResponse()\n        response['Vary'] = 'Accept-Language'\n        newheaders = ['Cookie', 'Accept-Encoding']\n        patch_vary_headers(response, newheaders)\n        self.assertEqual(response['Vary'], 'Accept-Language, Cookie, Accept-Encoding')\n", "    def setUp(self):\n        self.factory = RequestFactory()\n"], "sample_16": ["    def test_gradient(self):\n        self.check(np.gradient, axis=0)\n        self.check(np.gradient, axis=1)\n", "    def test_array_function(self):\n        assert np.copy(self.q).__array_function__(np.ndarray.__array_function__) is self.q\n", "    def test_sinc(self):\n        self.check(np.sinc, self.q)\n", "    def test_covered(self):\n        all_covered = (\n            SUBCLASS_SAFE_FUNCTIONS\n            | FUNCTION_HELPERS.keys()\n            | DISPATCHED_FUNCTIONS.keys()\n            | UNSUPPORTED_FUNCTIONS\n            | TBD_FUNCTIONS\n            | IGNORED_FUNCTIONS\n        )\n        missing = all_covered.difference(self.covered)\n        assert not missing, f\"Missing tests for: {missing}\"\n", "    def test_function_helpers(self):\n        for func in FUNCTION_HELPERS.values():\n            self.check(func, self.q)\n"], "sample_897": ["def test_PartialDependenceDisplay_from_estimator_subsample(clf_diabetes, diabetes, grid_resolution):\n    display = PartialDependenceDisplay.from_estimator(clf_diabetes, diabetes.data, [0, 1], subsample=0.5)\n    assert display.lines_.shape[2] == int(np.ceil(0.5 * diabetes.data.shape[0]))\n", "def test_PartialDependenceDisplay_from_estimator_categorical_features(diabetes, clf_diabetes, grid_resolution):\n    # Test PartialDependenceDisplay with categorical features\n    X_categorical = np.random.choice([\"a\", \"b\", \"c\"], size=(len(diabetes.data), 1))\n    X_combined = np.hstack([diabetes.data, X_categorical])\n    categorical_features = [6]\n\n    display = PartialDependenceDisplay.from_estimator(\n        clf_diabetes,\n        X_combined,\n        features=[0, (0, 6)],\n        categorical_features=categorical_features,\n        grid_resolution=grid_resolution,\n    )\n\n    # Check if categorical features are correctly identified\n    assert display.is_categorical == [(False,), (False, True)]\n\n    # Check if deciles are correctly calculated for continuous features\n    deciles = mquantiles(diabetes.data[:, 0], prob=np.arange(0.1, 1.0, 0.1))\n    assert_allclose(display.deciles[0], deciles)\n\n    # Check if categorical feature values are correctly stored\n    assert np.array_equal(display.pd_results[1][\"grid_values\"][1], [\"a\", \"b\", \"c\"])\n", "def test_partial_dependence_with_custom_limits(clf_diabetes, diabetes, grid_resolution):\n    pdp_lim = {1: (-50, 100), 2: (-200, 200)}\n    display = PartialDependenceDisplay.from_estimator(\n        clf_diabetes,\n        diabetes.data,\n        [0, (0, 1)],\n        grid_resolution=grid_resolution,\n        pdp_lim=pdp_lim,\n    )\n    assert display.axes_[0].get_ylim() == pdp_lim[1]\n    assert display.axes_[1].get_clim() == pdp_lim[2]\n", "def test_PartialDependenceDisplay_subsample(diabetes, clf_diabetes, grid_resolution):\n    subsample = 0.5\n    display = PartialDependenceDisplay.from_estimator(\n        clf_diabetes,\n        diabetes.data,\n        [0],\n        grid_resolution=grid_resolution,\n        subsample=subsample,\n        kind=\"individual\",\n    )\n    n_samples = len(diabetes.data)\n    expected_samples = int(n_samples * subsample)\n    assert len(display.lines_[0]) == expected_samples\n", "def test_partial_dependence_display_from_estimator(clf_diabetes, diabetes, grid_resolution):\n    # Test the from_estimator method of PartialDependenceDisplay\n    features = [0, 1]\n    target_idx = 0\n    display = PartialDependenceDisplay.from_estimator(\n        clf_diabetes, diabetes.data, features, grid_resolution=grid_resolution\n    )\n\n    # Check that the display object contains the expected attributes\n    assert hasattr(display, \"pd_results\")\n    assert hasattr(display, \"features\")\n    assert hasattr(display, \"feature_names\")\n    assert hasattr(display, \"target_idx\")\n    assert hasattr(display, \"deciles\")\n    assert hasattr(display, \"kind\")\n    assert hasattr(display, \"subsample\")\n    assert hasattr(display, \"random_state\")\n    assert hasattr(display, \"is_categorical\")\n\n    # Check that the attributes have the expected values\n    assert display.features == features\n    assert display.feature_names == diabetes.feature_names\n    assert display.target_idx == target_idx\n    assert len(display.deciles) == len(features)\n    assert display.kind == 'average'\n    assert display.subsample == 1000\n    assert display.random_state is None\n    assert display.is_categorical == [(False,) if len(fxs) == 1 else (False, False) for fxs in features]\n\n    # Check that the pd_results attribute contains the expected values\n    assert len(display.pd_results) == len(features)\n    for pd_result in display.pd_results:\n        assert hasattr(pd_result, \"grid_values\")\n        assert hasattr(pd_result, \"average\")\n        assert pd_result.average.shape[0] == 1  # Only one target\n        assert pd_result.average.shape[1] == grid_resolution\n"], "sample_618": ["def test_broadcast_compat_data():\n    variable = xr.Variable(dims=(\"x\", \"y\"), data=np.arange(6).reshape(2, 3))\n    broadcast_dims = (\"z\",)\n    core_dims = (\"x\", \"y\")\n    result = broadcast_compat_data(variable, broadcast_dims, core_dims)\n    expected_result = np.arange(6).reshape(1, 2, 3)\n    assert_array_equal(result, expected_result)\n", "def test_apply_ufunc_unified_dim_sizes():\n    # Test unified_dim_sizes function with Variable inputs\n    var1 = xr.Variable((\"x\", \"y\"), np.random.rand(3, 4))\n    var2 = xr.Variable((\"y\", \"z\"), np.random.rand(4, 5))\n    var3 = xr.Variable((\"x\", \"z\"), np.random.rand(3, 5))\n    vars = [var1, var2, var3]\n    dim_sizes = unified_dim_sizes(vars)\n    assert dim_sizes == {\"x\": 3, \"y\": 4, \"z\": 5}\n\n    # Test with duplicate dimensions on a variable\n    var_dup = xr.Variable((\"x\", \"x\"), np.random.rand(3, 3))\n    with pytest.raises(ValueError):\n        unified_dim_sizes([var_dup])\n\n    # Test with mismatched lengths for dimension\n    var4 = xr.Variable((\"x\",), np.random.rand(4))\n    with pytest.raises(ValueError):\n        unified_dim_sizes([var1, var4])\n", "def test_broadcast_compat_data():\n    var = xr.Variable([\"x\", \"y\"], np.arange(6).reshape(2, 3))\n    broadcast_dims = (\"z\",)\n    core_dims = (\"x\", \"y\")\n    expected_data = np.arange(6).reshape(1, 2, 3)\n    result = broadcast_compat_data(var, broadcast_dims, core_dims)\n    assert_identical(result, expected_data)\n", "def test_result_name():\n    a = xr.DataArray(name=\"a\")\n    b = xr.DataArray(name=\"b\")\n    assert result_name([a, b]) is None\n    assert result_name([a, a]) == \"a\"\n    assert result_name([b, b]) == \"b\"\n    assert result_name([a]) == \"a\"\n    assert result_name([b]) == \"b\"\n    assert result_name([]) is None\n", "def test_apply_ufunc_with_keep_attrs(func):\n    da = xr.DataArray([1, 2, 3], name=\"test\", attrs={\"unit\": \"m\"})\n    result = apply_ufunc(func, da, keep_attrs=True)\n    assert result.attrs == {\"unit\": \"m\"}\n"], "sample_992": ["def test_print_SparseMatrix():\n    expr = SparseMatrix(2, 3, {(0, 1): 2, (1, 2): 3})\n    printer = SciPyPrinter()\n    result = printer._print_SparseMatrix(expr)\n    assert result == \"scipy.sparse.coo_matrix([2, 3], ([0, 1], [1, 2]), shape=(2, 3))\"\n", "def test_sympy_printer():\n    expr = acos(x)\n    result = SymPyPrinter(settings={}).doprint(expr)\n    assert result == \"sympy.acos(x)\"\n", "def test_print_SparseMatrix_SciPyPrinter():\n    expr = SparseMatrix(2, 2, {(0, 1): 1, (1, 0): 2})\n    printer = SciPyPrinter()\n    result = printer._print_SparseMatrix(expr)\n    assert result == \"scipy.sparse.coo_matrix([1, 2], ([0, 1], [1, 0]), shape=(2, 2))\"\n", "def test_next_unit_test():\n    # Test the _print_Mod method for handling Mod expressions\n    expr = Mod(x, y)\n    printer = PythonCodePrinter()\n    result = printer._print_Mod(expr)\n    assert result == \"x % y\"\n", "def test_numpy_mod_operator():\n    expr = Mod(x, y)\n    result = NumPyPrinter().doprint(expr)\n    assert result == \"numpy.mod(x, y)\"\n"], "sample_541": ["def test_textbox_set_val(ax):\n    tb = widgets.TextBox(ax, \"Test\", initial=\"Initial\")\n    tb.set_val(\"Updated\")\n    assert tb.text == \"Updated\"\n", "def test_rectangleselector_props(ax):\n    callback = mock.Mock()\n    rect = widgets.RectangleSelector(ax, callback)\n    assert rect._props == {'facecolor': 'red', 'edgecolor': 'black', 'alpha': 0.2, 'fill': True, 'animated': False}\n\n    props = {'facecolor': 'green', 'edgecolor': 'blue', 'alpha': 0.5, 'fill': False}\n    rect = widgets.RectangleSelector(ax, callback, props=props)\n    assert rect._props == {**props, 'animated': False}\n", "def test_span_selector_props_change(ax):\n        pass\n\n    span = widgets.SpanSelector(ax, onselect, 'horizontal', props={'facecolor': 'blue'})\n    assert span._selection_artist.get_facecolor() == (0, 0, 1, 0.5)\n    span.set_props(facecolor='red')\n    assert span._selection_artist.get_facecolor() == (1, 0, 0, 0.5)\n", "def test_Button_label_is_text_with_correct_properties(ax):\n    button = widgets.Button(ax, 'Test')\n    text = button.label\n    assert isinstance(text, mtext.Text)\n    assert text.get_text() == 'Test'\n    assert text.get_fontsize() == mpl.rcParams['button.fontsize']\n    assert text.get_color() == mpl.rcParams['button.color']\n", "def test_button_clicked(ax):\n        clicked_button.append(label)\n\n    clicked_button = []\n    labels = ['Button 1', 'Button 2', 'Button 3']\n    buttons = widgets.RadioButtons(ax, labels, active=0)\n    buttons.on_clicked(on_clicked)\n\n    # Trigger the button click event\n    event = mock_event(buttons.ax.button('click_event'))\n    buttons.onclick(event)\n\n    assert clicked_button == ['Button 1']\n"], "sample_330": ["    def test_typecast_time(self):\n        # Test with seconds having a fractional part\n        self.assertEqual(\n            typecast_time('12:34:56.789'),\n            datetime.time(12, 34, 56, 789000)\n        )\n        # Test without seconds having a fractional part\n        self.assertEqual(\n            typecast_time('12:34:56'),\n            datetime.time(12, 34, 56, 0)\n        )\n        # Test with null input\n        self.assertIsNone(typecast_time(None))\n", "    def test_format_number(self):\n        from django.db.backends.base.base import format_number\n        self.assertIsNone(format_number(None, 10, 2))\n        self.assertEqual(format_number(123456.789, 10, 2), '123456.79')\n        self.assertEqual(format_number(123456.789, 5, 2), '123456.79')\n        self.assertEqual(format_number(123456.789, 10, None), '123456')\n        self.assertEqual(format_number(123456.789, 5, None), '123460')\n", "    def setUp(self):\n        self.connection = connection\n        self.cursor = self.connection.cursor()\n        self.wrapper = CursorWrapper(self.cursor, self.connection)\n", "    def setUp(self):\n        self.db = connection\n        self.cursor = self.db.cursor()\n        self.wrapped_cursor = CursorWrapper(self.cursor, self.db)\n", "    def setUp(self):\n        self.connection = connection\n        self.cursor = CursorDebugWrapper(self.connection.cursor(), self.connection)\n"], "sample_481": ["def test_join_with_space(self):\n    output = self.engine.render_to_string(\"join02\", {\"a\": [\"foo\", \"bar\", \"baz\"]})\n    self.assertEqual(output, \"foo bar baz\")\n", "def test_join_with_different_separator(self):\n    output = self.engine.render_to_string(\"join02\", {\"a\": [\"foo\", \"bar\", \"baz\"]})\n    self.assertEqual(output, \"foo-bar-baz\")\n", "def test_join02(self):\n    self.assertEqual(self.engine.render_to_string(\"join02\", {\"a\": [\"foo\", \"bar\", \"baz\"]}), \"foo-bar-baz\")\n", "def test_join_with_semicolon(self):\n    self.assertEqual(self.engine.render_to_string(\"join02\", {\"a\": [\"apples\", \"oranges\", \"bananas\"]}), \"apples;oranges;bananas\")\n", "    def test_join_with_pipe(self):\n        context = {\"b\": [\"apple\", \"banana\", \"cherry\"]}\n        self.assertEqual(self.engine.render_to_string(\"join02\", context), \"apple|banana|cherry\")\n"], "sample_499": ["def test_legend_title_fontsize():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3], [1, 2, 3], label='line')\n    legend = ax.legend(title='Title', title_fontsize=14)\n    assert legend.get_title().get_fontsize() == 14\n", "def test_legend_set_bbox_to_anchor():\n    fig, ax = plt.subplots()\n    line, = ax.plot([1, 2, 3], [4, 5, 6], label='Test Line')\n    legend = ax.legend()\n\n    # Test setting bbox_to_anchor using a BboxBase object\n    bbox = mtransforms.Bbox.from_bounds(0.5, 0.5, 0.5, 0.5)\n    legend.set_bbox_to_anchor(bbox)\n    assert legend.get_bbox_to_anchor() == bbox\n\n    # Test setting bbox_to_anchor using a tuple\n    bbox_tuple = (0.2, 0.2, 0.3, 0.3)\n    legend.set_bbox_to_anchor(bbox_tuple)\n    assert legend.get_bbox_to_anchor().bounds == bbox_tuple\n\n    # Test setting bbox_to_anchor to None\n    legend.set_bbox_to_anchor(None)\n    assert legend.get_bbox_to_anchor() == ax.bbox\n\n    plt.close(fig)\n", "def test_legend_with_custom_handler():\n    class CustomHandler(mlegend.HandlerTuple):\n                           xdescent, ydescent, width, height, fontsize,\n                           trans):\n            l1, l2 = orig_handle\n            return [l1, l2]\n\n    fig, ax = plt.subplots()\n    line1, = ax.plot([1, 2, 3], [4, 5, 6], label='Line 1')\n    line2, = ax.plot([1, 2, 3], [2, 3, 4], label='Line 2')\n    custom_handler_map = {tuple: CustomHandler()}\n    legend = ax.legend(handler_map=custom_handler_map)\n    assert len(legend.legendHandles) == 2\n    assert isinstance(legend.legendHandles[0], mlines.Line2D)\n    assert isinstance(legend.legendHandles[1], mlines.Line2D)\n", "def test_legend_handler_map_priority():\n    class CustomHandler:\n        pass\n\n    custom_map = {\n        mlines.Line2D: CustomHandler()\n    }\n\n    fig, ax = plt.subplots()\n    line, = ax.plot([1, 2, 3], [1, 2, 3], label='Test Line')\n    ax.legend(handler_map=custom_map)\n    handler = mlegend.Legend.get_legend_handler(ax.get_legend().get_legend_handler_map(), line)\n\n    assert isinstance(handler, CustomHandler)\n", "def test_legend_with_custom_handler_map():\n    fig, ax = plt.subplots()\n\n    class CustomArtist:\n            self.label = label\n\n    class CustomHandler:\n            return mlines.Line2D([0, 1], [0, 1], label=orig_handle.label)\n\n    custom_artist = CustomArtist(\"Custom Artist\")\n    custom_handler_map = {CustomArtist: CustomHandler()}\n\n    ax.add_artist(custom_artist)\n    ax.legend(handler_map=custom_handler_map)\n\n    plt.close(fig)\n"], "sample_858": ["def test_voting_classifier_transform():\n    clf1 = LogisticRegression(multi_class='multinomial', random_state=1)\n    clf2 = RandomForestClassifier(n_estimators=50, random_state=1)\n    X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n    y = np.array([1, 1, 1, 2, 2, 2])\n    eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2)], voting='hard')\n    eclf = eclf.fit(X, y)\n    transformed = eclf.transform(X)\n    assert transformed.shape == (6, 2)  # 6 samples and 2 classifiers\n    assert_array_equal(transformed[:, 0], eclf.named_estimators_.lr.predict(X))\n    assert_array_equal(transformed[:, 1], eclf.named_estimators_.rf.predict(X))\n", "def test_voting_regressor_transform():\n    r1 = LinearRegression()\n    r2 = RandomForestRegressor(n_estimators=10, random_state=1)\n    X = np.array([[1, 1], [2, 4], [3, 9], [4, 16], [5, 25], [6, 36]])\n    y = np.array([2, 6, 12, 20, 30, 42])\n    er = VotingRegressor([('lr', r1), ('rf', r2)])\n    er.fit(X, y)\n    predictions = er.transform(X)\n    assert predictions.shape == (6, 2)\n", "def test_voting_classifier_soft_voting():\n    clf1 = LogisticRegression(multi_class='multinomial', random_state=1)\n    clf2 = RandomForestClassifier(n_estimators=50, random_state=1)\n    clf3 = GaussianNB()\n    X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n    y = np.array([1, 1, 1, 2, 2, 2])\n    eclf = VotingClassifier(estimators=[\n            ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n            voting='soft')\n    eclf = eclf.fit(X, y)\n    assert_array_equal(eclf.predict(X), [1, 1, 1, 2, 2, 2])\n    assert_array_almost_equal(eclf.predict_proba(X),\n                              [[0.372, 0.628], [0.372, 0.628], [0.372, 0.628],\n                               [0.837, 0.163], [0.837, 0.163], [0.837, 0.163]])\n", "def test_voting_classifier():\n    clf1 = LogisticRegression(multi_class='multinomial', random_state=1)\n    clf2 = RandomForestClassifier(n_estimators=50, random_state=1)\n    clf3 = GaussianNB()\n    X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n    y = np.array([1, 1, 1, 2, 2, 2])\n    eclf1 = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard')\n    eclf1 = eclf1.fit(X, y)\n    assert_array_equal(eclf1.predict(X), np.array([1, 1, 1, 2, 2, 2]))\n", "def test_voting_regressor_with_dummy_estimator():\n    X, y = datasets.load_boston(return_X_y=True)\n    dummy_regressor = DummyRegressor(strategy=\"mean\")\n    voting_regressor = VotingRegressor([(\"dummy\", dummy_regressor)])\n    voting_regressor.fit(X, y)\n    y_pred = voting_regressor.predict(X)\n    assert y_pred.shape == (len(X),)\n    assert_array_almost_equal(y_pred, np.full(len(X), np.mean(y)))\n"], "sample_1121": ["def test_unevaluated_Mul():\n    m = Mul(sqrt(2), sqrt(3), evaluate=False)\n    assert m == Mul(sqrt(3), sqrt(2), evaluate=False)\n    u = Mul(sqrt(3), sqrt(2), evaluate=False)\n    assert m == uMul(u)\n    assert m == Mul(*m.args)\n", "def test_eval_subs():\n    expr = Mul(2*x, 3*y, evaluate=False)\n    old = Mul(2, x, evaluate=False)\n    new = Mul(4, evaluate=False)\n    assert expr._eval_subs(old, new) == 3*y*new\n", "def test_as_powers_dict():\n    expr = Mul(a**2, a, b**2, c**3)\n    d = expr.as_powers_dict()\n    assert d == {a: 3, b: 2, c: 3}\n", "def test_unevaluated_Mul():\n    # Test _unevaluated_Mul function\n    from sympy.core.mul import _unevaluated_Mul\n\n    # Test with integer arguments\n    result = _unevaluated_Mul(3, x, 2)\n    assert result == Mul(6, x, evaluate=False)\n\n    # Test with float arguments\n    result = _unevaluated_Mul(3.0, x, 2.0)\n    assert result == Mul(6.0, x, evaluate=False)\n\n    # Test with Pow argument\n    result = _unevaluated_Mul(Pow(2, x), Pow(3, y))\n    assert result == Mul(Pow(2, x), Pow(3, y), evaluate=False)\n\n    # Test with Mul argument\n    result = _unevaluated_Mul(Mul(2, x), Mul(3, y))\n    assert result == Mul(6, x, y, evaluate=False)\n\n    # Test with non-commutative argument\n    A, B = symbols('A B', commutative=False)\n    result = _unevaluated_Mul(A, B)\n    assert result == Mul(A, B, evaluate=False)\n", "def test_eval_is_composite():\n    assert (3*5).is_composite is True\n    assert (2*3*5).is_composite is True\n    assert (2*3).is_composite is False\n    assert (2*3*x).is_composite is None\n    assert (2*x*y).is_composite is None\n"], "sample_406": ["    def test_manager_contribute_to_class(self):\n        class TestModel(models.Model):\n            objects = Manager()\n\n        self.assertEqual(TestModel.objects.name, 'objects')\n        self.assertEqual(TestModel.objects.model, TestModel)\n        self.assertIn('objects', TestModel._meta.managers_map)\n        self.assertIsInstance(getattr(TestModel, 'objects'), ManagerDescriptor)\n", "    def test_from_queryset(self):\n        class CustomQuerySet(QuerySet):\n                return \"Custom QuerySet method\"\n\n        CustomManager = BaseManager.from_queryset(CustomQuerySet)\n        obj = CustomManager()\n        obj.model = Article  # Assuming Article is a model defined in the models module\n        obj.contribute_to_class(Article, 'custom_manager')\n\n        self.assertTrue(hasattr(Article, 'custom_manager'))\n        self.assertTrue(hasattr(Article.custom_manager, 'custom_method'))\n        self.assertEqual(Article.custom_manager.custom_method(), \"Custom QuerySet method\")\n", "    def test_manager_str_representation(self):\n        manager = Article.objects\n        self.assertEqual(str(manager), \"tests.Article.objects\")\n", "    def test_manager_db_property(self):\n        manager = Article.objects\n        self.assertEqual(manager.db, DEFAULT_DB_ALIAS)\n\n        using = 'other'\n        manager = manager.db_manager(using=using)\n        self.assertEqual(manager.db, using)\n", "    def test_contribute_to_class(self):\n        class TestModel(models.Model):\n            objects = Manager()\n\n        self.assertEqual(TestModel._meta.managers_map[\"objects\"].name, \"objects\")\n        self.assertEqual(TestModel._meta.managers_map[\"objects\"].model, TestModel)\n        self.assertEqual(TestModel.objects.model, TestModel)\n"], "sample_1196": ["def test_contains_with_symbol():\n    x = Symbol('x')\n    s = FiniteSet(1, 2, 3, x)\n    assert Contains(x, s) == S.true\n    assert Contains(4, s) == S.false\n", "def test_contains_with_interval():\n    x = Symbol('x')\n    assert Contains(x, Interval(0, 1)) == And(0 <= x, x <= 1)\n    assert Contains(2, Interval(0, 1)) == False\n", "def test_contains_non_set():\n    x = Symbol('x')\n    with raises(TypeError, match='expecting Set, not Symbol'):\n        Contains(x, x)\n", "def test_contains_with_interval():\n    x = Symbol('x')\n    assert Contains(x, Interval(1, 5)) == Ne(x < 1, x > 5)\n    assert Contains(2, Interval(1, 5)) == True\n    assert Contains(6, Interval(1, 5)) == False\n", "def test_contains_with_interval():\n    x = Symbol('x')\n    assert Contains(x, Interval(0, 1)) == Ne(x, oo) & (x >= 0) & (x <= 1)\n    assert Contains(2, Interval(0, 1)) == False\n"], "sample_93": ["def test_expression_wrapper(self):\n    expr = ExpressionWrapper(Value(1), output_field=IntegerField())\n    query = Author.objects.annotate(wrapped_value=expr)\n    self.assertSequenceEqual(query.values_list('wrapped_value', flat=True), Author.objects.values_list('id', flat=True))\n", "def test_duration_expression(self):\n    duration = DurationField()\n    test_cases = [\n        (F('publisher__duration') + datetime.timedelta(days=1), 'INTERVAL \"publisher__duration\" DAY + INTERVAL \\'1 day\\' DAY'),\n        (F('publisher__duration') - datetime.timedelta(days=1), 'INTERVAL \"publisher__duration\" DAY - INTERVAL \\'1 day\\' DAY'),\n        (F('publisher__duration') * 2, 'INTERVAL \"publisher__duration\" DAY * 2'),\n        (F('publisher__duration') / 2, 'INTERVAL \"publisher__duration\" DAY / 2'),\n        (F('publisher__duration') % datetime.timedelta(days=1), 'MOD(INTERVAL \"publisher__duration\" DAY, INTERVAL \\'1 day\\' DAY)'),\n    ]\n    for expression, expected_sql in test_cases:\n        with self.subTest(expression=expression):\n            sql, params = expression.as_sql(compiler=self.compiler, connection=connection)\n            self.assertEqual(sql, expected_sql)\n            self.assertEqual(params, [])\n", "def test_window_frame(self):\n    # Testing the WindowFrame class\n    query = Book.objects.annotate(\n        avg_rating=Window(\n            expression=Avg('rating'),\n            order_by=F('id').asc(),\n            frame=RowRange(start=-1, end=1)\n        )\n    )\n    # Replace 'query.query' with actual query if 'query' is a QuerySet\n    sql, params = query.query.get_compiler(using=connection).as_sql()\n    self.assertIn('ROW BETWEEN 1 PRECEDING AND 1 FOLLOWING', sql)\n", "    def setUpTestData(cls):\n        # The same data setup as in AggregateTestCase\n", "def test_complex_expression_combination(self):\n    # Test complex combination of expressions with different operators\n    expr = (F('rating') + 2) * F('pages') / Value(10) - F('price')\n    books = Book.objects.annotate(result=expr)\n    self.assertEqual(books[0].result, (self.b1.rating + 2) * self.b1.pages / 10 - self.b1.price)\n"], "sample_810": ["def test_pipeline_fit_transform_with_fit_params():\n    # Create a pipeline with a transformer that accepts fit parameters\n    pipe = make_pipeline(TransfFitParams(), Mult())\n\n    # Fit the pipeline with some data and fit parameters\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([0, 1])\n    fit_params = {'transffitparams__should_succeed': True}\n    pipe.fit(X, y, **fit_params)\n\n    # Check that the fit parameters were passed to the transformer\n    assert pipe.named_steps['transffitparams'].fit_params == {'should_succeed': True}\n\n    # Transform the data and check that it was transformed correctly\n    X_transformed = pipe.transform(X)\n    assert_array_equal(X_transformed, np.array([[1, 2], [3, 4]]) * pipe.named_steps['mult'].mult)\n", "def test_pipeline_transform_with_none_estimator():\n    # Test that transform works even when final estimator is None\n    p = make_pipeline(Mult(2), Mult(3), None)\n    X = np.array([[1, 2, 3], [4, 5, 6]])\n    X_transformed = p.transform(X)\n    assert_array_equal(X_transformed, np.array([[6, 12, 18], [12, 15, 18]]))\n\n    # Test that inverse_transform works even when final estimator is None\n    X_inverse_transformed = p.inverse_transform(X_transformed)\n    assert_array_equal(X_inverse_transformed, X)\n", "def test_pipeline_fit_transform_parameters():\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([0, 1, 0])\n    pipe = make_pipeline(StandardScaler(), LogisticRegression())\n\n    # Test that fit_transform raises an error when passing invalid parameters\n    with pytest.raises(ValueError):\n        pipe.fit_transform(X, y, invalid_parameter=True)\n\n    # Test that fit_transform passes parameters to the correct estimators\n    pipe.fit_transform(X, y, logisticregression__C=0.1)\n    assert pipe.named_steps['logisticregression'].C == 0.1\n", "def test_pipeline_with_fit_transform_params():\n    # Test that fit_transform parameters are passed to the final estimator\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([1, 2])\n    pipeline = make_pipeline(TransfFitParams(), LinearRegression())\n    pipeline.fit(X, y, linearregression__fit_intercept=False)\n    assert not pipeline.named_steps[\"linearregression\"].fit_intercept\n", "def test_pipeline_with_fit_transform_params():\n    # Test that fit_transform parameters are correctly dispatched to\n    # estimators in Pipeline\n    transf = TransfFitParams()\n    est = \"passthrough\"\n    pipe = Pipeline([('transformer', transf), ('estimator', est)])\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([0, 1])\n    pipe.fit_transform(X, y, transformer__mult=2)\n    assert transf.fit_params == {'mult': 2}\n"], "sample_433": ["    def test_deconstructible_objects_are_not_equal(self):\n        changes = self.get_changes(\n            [self.author_name_deconstructible_1],\n            [self.author_name_deconstructible_2],\n        )\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterField\"])\n        self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"name\")\n", "def test_deconstructible_object_changes(self):\n    # Test changes with deconstructible objects in CharField default\n    before_states = [self.author_name_deconstructible_1]\n    after_states = [self.author_name_deconstructible_2]\n    changes = self.get_changes(before_states, after_states)\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterField\"])\n    self.assertOperationFieldAttributes(changes, \"testapp\", 0, 0, default=DeconstructibleObject())\n\n    # Test changes with different deconstructible objects in CharField default\n    before_states = [self.author_name_deconstructible_1]\n    after_states = [self.author_name_deconstructible_3]\n    changes = self.get_changes(before_states, after_states)\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterField\"])\n    self.assertOperationFieldAttributes(changes, \"testapp\", 0, 0, default=models.IntegerField())\n\n    # Test no changes with same deconstructible objects in CharField default\n    before_states = [self.author_name_deconstructible_3]\n    after_states = [self.author_name_deconstructible_4]\n    changes = self.get_changes(before_states, after_states)\n    self.assertNumberMigrations(changes, \"testapp\", 0)\n\n    # Test changes with deconstructible objects in CharField default list\n    before_states = [self.author_name_deconstructible_list_1]\n    after_states = [self.author_name_deconstructible_list_2]\n    changes = self.get_changes(before_states, after_states)\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterField\"])\n    self.assertOperationField", "def test_deconstructible_object_equality(self):\n    \"\"\"\n    Test the equality of two DeconstructibleObject instances.\n    \"\"\"\n    obj1 = DeconstructibleObject(1, 2, a=3, b='test')\n    obj2 = DeconstructibleObject(1, 2, a=3, b='test')\n    obj3 = DeconstructibleObject(4, 5, a=6, b='another')\n\n    self.assertEqual(obj1, obj2)\n    self.assertNotEqual(obj1, obj3)\n", "def test_rename_model_with_db_table_option(self):\n    changes = self.get_changes(self.author_with_db_table_options, self.author_renamed_with_new_db_table_options)\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['RenameModel'])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, old_name='Author', new_name='NewAuthor')\n", "def test_author_name_deconstructible_changed_args(self):\n    changes = self.get_changes(\n        [self.author_name_nested_deconstructible_1],\n        [self.author_name_nested_deconstructible_changed_arg],\n    )\n    self.assertOperationAttributes(\n        changes,\n        \"testapp\",\n        0,\n        0,\n        field_name=\"name\",\n        old_default=DeconstructibleObject(\n            DeconstructibleObject(1),\n            (DeconstructibleObject(\"t1\"), DeconstructibleObject(\"t2\")),\n            a=DeconstructibleObject(\"A\"),\n            b=DeconstructibleObject(B=DeconstructibleObject(\"c\")),\n        ),\n        new_default=DeconstructibleObject(\n            DeconstructibleObject(1),\n            (DeconstructibleObject(\"t1\"), DeconstructibleObject(\"t2-changed\")),\n            a=DeconstructibleObject(\"A\"),\n            b=DeconstructibleObject(B=DeconstructibleObject(\"c\")),\n        ),\n    )\n"], "sample_83": ["    def test_filter_decorator(self):\n        @self.library.filter\n            return value + ' filtered'\n\n        self.assertIn('test_filter', self.library.filters)\n        self.assertEqual(self.library.filters['test_filter']('test'), 'test filtered')\n", "def test_register_filter(self):\n    @self.library.filter\n        return value.lower()\n\n    self.assertIn('test_filter', self.library.filters)\n    self.assertEqual(self.library.filters['test_filter'], test_filter)\n", "def test_simple_tag_registration(self):\n    @self.library.simple_tag\n        return f\"{arg1} {arg2}\"\n\n    self.assertTrue('my_simple_tag' in self.library.tags)\n    compile_func = self.library.tags['my_simple_tag']\n    self.assertIsInstance(compile_func, functools.partial)\n    self.assertEqual(compile_func.func.__name__, 'compile_func')\n    self.assertEqual(compile_func.func.__wrapped__, my_simple_tag)\n", "def test_simple_tag_registration(self):\n    @self.library.simple_tag\n        return f\"{arg1} {arg2} {context}\"\n\n    self.assertIn(\"test_tag\", self.library.tags)\n    self.assertTrue(callable(self.library.tags[\"test_tag\"]))\n    self.assertTrue(hasattr(test_tag, \"_decorated_function\"))\n    self.assertEqual(test_tag._decorated_function.__name__, \"test_tag\")\n", "    def test_simple_tag_registration(self):\n        @self.library.simple_tag\n            return f'{arg1} - {arg2}'\n\n        self.assertEqual(self.library.tags['test_tag'].__name__, 'compile_func')\n        self.assertTrue(callable(self.library.tags['test_tag']))\n\n        parsed_node = self.library.tags['test_tag'](None, None)\n        self.assertIsInstance(parsed_node, Node)\n        self.assertEqual(parsed_node.func.__name__, 'test_tag')\n"], "sample_43": ["def test_regular_events():\n    dt = 0.05\n    t = dt * np.arange(1000)\n    x = np.zeros(len(t))\n    x[np.random.randint(0, len(t), len(t) // 10)] = 1\n    edges = bayesian_blocks(t, x, fitness='regular_events', dt=dt)\n    assert len(edges) > 1\n    assert_allclose(t[0], edges[0])\n    assert_allclose(t[-1], edges[-1])\n\n    # Test that the custom fitness function works\n    fitness = RegularEvents(dt=dt)\n    edges_custom = fitness.fit(t, x)\n    assert_allclose(edges, edges_custom)\n", "def test_regular_events_fitness():\n    dt = 0.05\n    t = dt * np.arange(1000)\n    x = np.zeros(len(t))\n    x[np.random.randint(0, len(t), len(t) // 10)] = 1\n\n    # Initialize RegularEvents fitness function\n    fitness_func = RegularEvents(dt=dt)\n\n    # Compute the fitness\n    N_k = np.cumsum(x[::-1])[::-1]\n    T_k = dt * np.arange(1, len(x) + 1)\n    fitness = fitness_func.fitness(T_k, N_k)\n\n    # Check that the fitness is a numpy array with the correct shape\n    assert isinstance(fitness, np.ndarray)\n    assert fitness.shape == (len(x),)\n\n    # Check that the fitness is non-negative\n    assert np.all(fitness >= 0)\n", "def test_regular_events():\n    # Test RegularEvents fitness function with a simple case\n    dt = 0.1\n    t = np.arange(0, 10, dt)\n    x = np.zeros_like(t)\n    x[::2] = 1  # Set every second event\n    edges = bayesian_blocks(t, x, fitness='regular_events', dt=dt)\n\n    # Check if the edges are correctly identified as every second tick\n    expected_edges = np.arange(0, 10.1, 2 * dt)\n    assert_allclose(edges, expected_edges)\n", "def test_regular_events_fitness():\n    dt = 0.05\n    t = dt * np.arange(1000)\n    x = np.zeros(len(t))\n    x[np.random.randint(0, len(t), len(t) // 10)] = 1\n    fitfunc = RegularEvents(dt)\n    fitness = fitfunc.fitness(T_k=np.ones(10), N_k=np.random.randint(0, 11, 10))\n    assert len(fitness) == 10\n", "def test_regular_events_fitness():\n    dt = 0.05\n    t = dt * np.arange(1000)\n    x = np.zeros(len(t))\n    x[np.random.randint(0, len(t), len(t) // 10)] = 1\n\n    # Expected fitness values computed manually or using known results\n    expected_fitness = np.array([...])  # replace with actual values\n\n    fitfunc = RegularEvents(dt=dt)\n    T_k = np.diff(np.concatenate([t[:1], 0.5 * (t[1:] + t[:-1]), t[-1:]]))\n    N_k = np.cumsum(x[::-1])[::-1]\n\n    fitness = fitfunc.fitness(T_k, N_k)\n    assert_allclose(fitness, expected_fitness)\n"], "sample_861": ["def test_parameter_grid_empty_dict(klass):\n    pg = klass({})\n    assert list(pg) == [{}]\n    assert len(pg) == 1\n    assert_grid_iter_equals_getitem(pg)\n", "def test_invalid_input(klass, input, error_type, error_message):\n    with pytest.raises(error_type, match=error_message):\n        klass(input)\n", "def test_non_empty_dictionary(klass):\n    param_grid = {'a': [1, 2], 'b': ['x', 'y']}\n    grid = klass(param_grid)\n    assert len(grid) == 4\n    assert_grid_iter_equals_getitem(grid)\n", "def test_grid_parameter_iter_with_dict_input(klass):\n    param_grid = {'a': [1, 2], 'b': [True, False]}\n    grid = klass(param_grid)\n    expected_output = [{'a': 1, 'b': True}, {'a': 1, 'b': False}, {'a': 2, 'b': True}, {'a': 2, 'b': False}]\n    assert list(grid) == expected_output\n    assert_grid_iter_equals_getitem(grid)\n", "def test_parameter_grid_empty_grid(klass):\n    assert_grid_iter_equals_getitem(klass({}))\n    assert_grid_iter_equals_getitem(klass([]))\n    assert_grid_iter_equals_getitem(klass([{}]))\n"], "sample_711": ["def test_iterparentnodeids(nodeid, expected):\n    result = list(nodes.iterparentnodeids(nodeid))\n    assert result == expected\n", "def test_iterparentnodeids(nodeid: str, expected: List[str]) -> None:\n    result = list(nodes.iterparentnodeids(nodeid))\n    assert result == expected\n", "def test_iterparentnodeids(nodeid: str, expected: List[str]) -> None:\n    result = list(nodes.iterparentnodeids(nodeid))\n    assert result == expected\n", "def test_iterparentnodeids(nodeid: str, expected: List[str]) -> None:\n    result = list(nodes.iterparentnodeids(nodeid))\n    assert result == expected\n", "def test_iterparentnodeids_with_slashes_in_test_name(nodeid, expected):\n    result = list(nodes.iterparentnodeids(nodeid))\n    assert result == expected\n"], "sample_658": ["def test_is_mocked(mocker):\n    mocked_obj = mocker.MagicMock()\n    assert _is_mocked(mocked_obj) is True\n\n    not_mocked_obj = object()\n    assert _is_mocked(not_mocked_obj) is False\n", "def test__is_mocked(mocker):\n    mocked_obj = mocker.Mock()\n    assert _is_mocked(mocked_obj) is True\n\n    class NormalObj:\n        pass\n\n    normal_obj = NormalObj()\n    assert _is_mocked(normal_obj) is False\n", "def test__is_mocked(mocker):\n    mock_obj = mocker.MagicMock()\n    assert _is_mocked(mock_obj) is True\n\n    class NonMockObj:\n        pass\n\n    assert _is_mocked(NonMockObj()) is False\n", "def test__patch_unwrap_mock_aware(mocker):\n    real_unwrap = inspect.unwrap\n    mocked_unwrap = mocker.patch('inspect.unwrap')\n\n    with _patch_unwrap_mock_aware():\n        assert inspect.unwrap == mocked_unwrap\n\n    assert inspect.unwrap == real_unwrap\n", "def test_check_all_skipped(self):\n    import doctest\n\n    class DummyTest(object):\n            self.examples = examples\n\n    # Test case where all examples are skipped\n    examples = [doctest.Example(\"\", options={doctest.SKIP: True})]\n    test = DummyTest(examples)\n    with pytest.raises(pytest.skip.Exception, match=\"all tests skipped by \\\\+SKIP option\"):\n        DoctestItem._check_all_skipped(test)\n\n    # Test case where not all examples are skipped\n    examples = [\n        doctest.Example(\"\"),\n        doctest.Example(\"\", options={doctest.SKIP: True}),\n    ]\n    test = DummyTest(examples)\n    try:\n        DoctestItem._check_all_skipped(test)\n    except pytest.skip.Exception:\n        pytest.fail(\"pytest.skip.Exception raised unexpectedly\")\n\n    # Test case where no examples are skipped\n    examples = [doctest.Example(\"\")]\n    test = DummyTest(examples)\n    try:\n        DoctestItem._check_all_skipped(test)\n    except pytest.skip.Exception:\n        pytest.fail(\"pytest.skip.Exception raised unexpectedly\")\n"], "sample_663": ["def test_in_venv(tmpdir):\n    # Create a virtual environment\n    venv_path = tmpdir.mkdir(\"venv\")\n    if sys.platform.startswith(\"win\"):\n        venv_path.join(\"Scripts\").mkdir()\n        venv_path.join(\"Scripts\", \"activate\").write(\"\")\n    else:\n        venv_path.join(\"bin\").mkdir()\n        venv_path.join(\"bin\", \"activate\").write(\"\")\n\n    # Test _in_venv function with the virtual environment path\n    assert _in_venv(venv_path) is True\n\n    # Test _in_venv function with a non-virtual environment path\n    assert _in_venv(tmpdir) is False\n", "def test_in_venv(tmpdir):\n    # Test when path is not a directory\n    assert not _in_venv(tmpdir.join(\"not_a_directory\"))\n\n    # Test when path is a directory but doesn't contain activate scripts\n    empty_dir = tmpdir.mkdir(\"empty_directory\")\n    assert not _in_venv(empty_dir)\n\n    # Test when path is a directory and contains activate scripts\n    if sys.platform.startswith(\"win\"):\n        activate_script = empty_dir.mkdir(\"Scripts\").join(\"activate\")\n    else:\n        activate_script = empty_dir.mkdir(\"bin\").join(\"activate\")\n    activate_script.write(\"\")\n    assert _in_venv(empty_dir)\n", "def test_in_venv(tmp_path):\n    # Test when path is not a directory\n    assert not _in_venv(tmp_path / \"not_a_dir\")\n\n    # Test when path is a directory but does not contain virtualenv activate scripts\n    venv_dir = tmp_path / \"venv\"\n    venv_dir.mkdir()\n    assert not _in_venv(venv_dir)\n\n    # Test when path is a directory and contains virtualenv activate scripts\n    activates = [\"activate\", \"activate.csh\", \"activate.fish\", \"Activate.bat\", \"Activate.ps1\"]\n    if sys.platform.startswith(\"win\"):\n        bindir = venv_dir / \"Scripts\"\n        bindir.mkdir()\n        for activate in activates:\n            (bindir / activate).touch()\n    else:\n        bindir = venv_dir / \"bin\"\n        bindir.mkdir()\n        for activate in activates:\n            (bindir / activate).touch()\n    assert _in_venv(venv_dir)\n", "def test_in_venv(self, tmpdir):\n    # create a mock venv directory\n    venv_path = tmpdir.mkdir(\"venv\")\n    if sys.platform.startswith(\"win\"):\n        bindir = venv_path.mkdir(\"Scripts\")\n        activate_file = bindir.join(\"activate.bat\")\n    else:\n        bindir = venv_path.mkdir(\"bin\")\n        activate_file = bindir.join(\"activate\")\n    activate_file.write(\"\")\n\n    # test that _in_venv returns True for the mock venv directory\n    assert _in_venv(py.path.local(venv_path)) is True\n\n    # test that _in_venv returns False for a non-venv directory\n    non_venv_path = tmpdir.mkdir(\"non_venv\")\n    assert _in_venv(py.path.local(non_venv_path)) is False\n", "def test_in_venv(tmpdir):\n    # Test when path is not a directory\n    assert not _in_venv(tmpdir.join(\"not_a_dir\"))\n\n    # Test when path is a directory but doesn't contain a virtual environment\n    assert not _in_venv(tmpdir)\n\n    # Test when path is a directory containing a virtual environment\n    venv_dir = tmpdir.mkdir(\"venv\")\n    bin_dir = venv_dir.mkdir(\"bin\" if sys.platform != \"win32\" else \"Scripts\")\n    activate_file = bin_dir.join(\"activate\")\n    activate_file.write(\"\")\n    assert _in_venv(venv_dir)\n"], "sample_1198": ["def test_airy_functions():\n    assert parse_mathematica(\"AiryAi[x]\") == airyai(x)\n    assert parse_mathematica(\"AiryAiPrime[x]\") == airyaiprime(x)\n    assert parse_mathematica(\"AiryBi[x]\") == airybi(x)\n    assert parse_mathematica(\"AiryBiPrime[x]\") == airybiprime(x)\n", "def test_parse_function_with_multiple_arguments():\n    expr = parse_mathematica(\"f[x, y, z]\")\n    assert expr.func == Function(\"f\")\n    assert expr.args == (x, y, z)\n", "def test_parse_fullformlist_to_sympy():\n    parser = MathematicaParser()\n    full_form_list = [\"Plus\", [\"Times\", \"2\", \"x\"], [\"Times\", \"3\", \"y\"]]\n    result = parser._from_fullformlist_to_sympy(full_form_list)\n    assert result == 2*x + 3*y\n", "def test_parse_function():\n    e = parse_mathematica(\"f[x_, 3] := x^3 /; x > 0\")\n    assert e == Lambda((x,), Piecewise((x**3, x > 0)))\n", "def test_parse_mathematica_airyai_function():\n    result = parse_mathematica(\"AiryAi[x]\")\n    assert result == airyai(x)\n"], "sample_1017": ["def test_as_Boolean():\n    assert as_Boolean(True) is S.true\n    assert as_Boolean(False) is S.false\n    assert as_Boolean(A) == A\n    assert as_Boolean(Boolean(True)) == S.true\n    raises(TypeError, lambda: as_Boolean(2))\n", "def test_as_boolean():\n    assert as_Boolean(True) is S.true\n    assert as_Boolean(False) is S.false\n    assert as_Boolean(A) is A\n    assert as_Boolean(BooleanTrue()) is S.true\n    assert as_Boolean(BooleanFalse()) is S.false\n    with raises(TypeError):\n        as_Boolean(2)\n", "def test_as_Boolean():\n    assert as_Boolean(1) is true\n    assert as_Boolean(0) is false\n    x = symbols('x')\n    assert as_Boolean(x) == x\n    with raises(TypeError):\n        as_Boolean('string')\n", "def test_as_Boolean():\n    assert as_Boolean(True) is true\n    assert as_Boolean(False) is false\n    assert as_Boolean(x) == x\n    raises(TypeError, lambda: as_Boolean(2))\n\n    assert isinstance(BooleanAtom(), Boolean)\n    assert as_Boolean(BooleanAtom()) == BooleanAtom()\n    raises(TypeError, lambda: as_Boolean(\"not a boolean\"))\n\n    assert as_Boolean(1) is true\n    assert as_Boolean(0) is false\n", "def test_is_literal():\n    assert is_literal(A)\n    assert is_literal(~A)\n    assert is_literal(Q.zero(A))\n    assert is_literal(A + B)\n    assert not is_literal(Or(A, B))\n"], "sample_1015": ["def test_c_code_printer():\n    expr = Piecewise((x + 1, x > 0), (x, True))\n    assign_to = Dummy('tau')\n    result = ccode(expr, assign_to, standard='C89')\n    expected = \"if (x > 0) {\\n    tau = x + 1;\\n} else {\\n    tau = x;\\n}\"\n    assert result == expected\n", "def test_ccode_matrix():\n    mat = Matrix([x**2, Piecewise((x + 1, x > 0), (x, True)), sin(x)])\n    A = MatrixSymbol('A', 3, 1)\n    code = ccode(mat, A, standard='C89')\n    expected_code = 'A[0] = pow(x, 2);\\nif (x > 0) {\\n   A[1] = x + 1;\\n} else {\\n   A[1] = x;\\n}\\nA[2] = sin(x);'\n    assert code == expected_code\n", "compilation error", "def test_print_piecewise_with_assignment():\n    expr = Piecewise((x + 1, x > 0), (x, True))\n    assign_to = \"result\"\n    printer = CCodePrinter()\n    result = printer.doprint(expr, assign_to)\n    expected = \"if (x > 0) {\\n    result = x + 1;\\n} else {\\n    result = x;\\n}\"\n    assert result == expected\n", "def test_CCodePrinter_print_FunctionCall():\n    func = FunctionPrototype(name='test_func', return_type=float64, parameters=[Variable('a', type=intc), Variable('b', type=float64)])\n    call = FunctionCall(func, [Integer(1), Float(2.0)])\n    printer = CCodePrinter()\n    result = printer._print_FunctionCall(call)\n    assert result == 'test_func(1, 2.0)'\n"], "sample_1149": ["def test_singleton_registry():\n    class TestSingleton(Basic, metaclass=Singleton):\n        pass\n\n    assert TestSingleton() is TestSingleton()\n    assert S.TestSingleton is TestSingleton()\n    assert isinstance(S.TestSingleton, TestSingleton)\n", "def test_singleton_rational():\n    a = S(1)/2\n    b = Rational(1, 2)\n    assert a is b\n", "def test_singleton_registry():\n    class MySingleton(Basic, metaclass=Singleton):\n        pass\n\n    assert MySingleton() is MySingleton()\n    assert S.MySingleton is MySingleton()\n\n    # Test that a duplicate class overwrites the old one\n    class MySingleton(Basic, metaclass=Singleton):\n        pass\n\n    assert MySingleton() is not S.MySingleton\n", "def test_sympify():\n    assert S(1) == 1\n    assert S(\"1/2\") == Rational(1, 2)\n    assert S(\"x\") == symbols(\"x\")\n", "def test_singleton_registry_call():\n    assert S(1) == S.One\n    assert S(\"1/2\") == Rational(1, 2)\n"], "sample_323": ["def test_mixed_migration_plan(self, mock_unapply_migration, mock_apply_migration):\n    executor = MigrationExecutor(connection)\n    plan = [(migrations.Migration('migrations', '0001_initial'), False),\n            (migrations.Migration('migrations2', '0001_initial'), True)]\n    with self.assertRaises(InvalidMigrationPlan):\n        executor.migrate([], plan=plan)\n", "    def test_migrate_all_forwards(self):\n        \"\"\"Test _migrate_all_forwards method.\"\"\"\n        executor = MigrationExecutor(connection)\n        state = ProjectState()\n        plan = [\n            (migrations.Migration(\"migrations.0001_initial\"), False),\n            (migrations.Migration(\"migrations2.0001_initial\"), False),\n        ]\n        full_plan = [\n            (migrations.Migration(\"migrations.0001_initial\"), False),\n            (migrations.Migration(\"migrations2.0001_initial\"), False),\n        ]\n        with mock.patch.object(executor, \"apply_migration\") as mock_apply_migration:\n            mock_apply_migration.side_effect = lambda state, migration, fake, fake_initial: state\n            result = executor._migrate_all_forwards(state, plan, full_plan, fake=False, fake_initial=False)\n            self.assertEqual(mock_apply_migration.call_count, 2)\n            self.assertIsInstance(result, ProjectState)\n", "def test_migrate_all_backwards(self):\n    executor = MigrationExecutor(connection)\n    plan = [(executor.loader.graph.nodes[('migrations', '0002_second')], True)]\n    full_plan = executor.migration_plan(executor.loader.graph.leaf_nodes(), clean_start=True)\n    state = executor._migrate_all_backwards(plan, full_plan, fake=False)\n    self.assertEqual(state.apps, executor._create_project_state(with_applied_migrations=False).apps)\n", "def test_migrate_with_mixed_plan(self):\n    \"\"\"Test that a mixed plan raises an error.\"\"\"\n    executor = MigrationExecutor(connection)\n    plan = [\n        (migrations.Migration(\"migrations.0001_initial\", \"migrations\"), False),\n        (migrations.Migration(\"migrations.0002_add_field\", \"migrations\"), True),\n    ]\n    with self.assertRaises(InvalidMigrationPlan):\n        executor.migrate([], plan=plan)\n", "    def test_migrate_all_backwards(self):\n        executor = MigrationExecutor(connection)\n\n        # Mock the full plan and plan for all backwards migrations\n        full_plan = [(migrations.Migration(\"migrations.0001_initial\"), True)]\n        plan = [(migrations.Migration(\"migrations.0001_initial\"), True)]\n\n        # Mock the states before migrations\n        states = {migrations.Migration(\"migrations.0001_initial\"): ProjectState()}\n\n        # Call the _migrate_all_backwards method\n        state = executor._migrate_all_backwards(plan, full_plan, fake=False)\n\n        # Assert the state is as expected\n        self.assertEqual(state, ProjectState())\n\n        # Assert the migrations were unapplied\n        self.assertNotIn(\"migrations.0001_initial\", executor.loader.applied_migrations)\n"], "sample_766": ["def test_sparse_encode_positive_constraint():\n    # Test that an error is raised when the positive constraint is used with\n    # an algorithm that doesn't support it.\n    with assert_raises(ValueError):\n        sparse_encode(X, X, algorithm='lars', positive=True)\n", "def test_sparse_encode_algorithms(algorithm):\n    n_components = 2\n    dictionary = rng_global.randn(n_components, n_features)\n    code = sparse_encode(X, dictionary, algorithm=algorithm)\n    assert_equal(code.shape, (n_samples, n_components))\n", "def test_dict_learning_positive():\n    n_components = 5\n    alpha = 0.1\n    max_iter = 10\n\n    # Test positive_dict and positive_code parameters\n    code, dictionary, _ = dict_learning(X, n_components, alpha, max_iter=max_iter, positive_dict=True, positive_code=True)\n    assert np.all(dictionary >= 0)\n    assert np.all(code >= 0)\n", "def test_dict_learning_positive_code_and_dict(method, positive_code, positive_dict):\n    n_components = 2\n    alpha = 1.0\n    max_iter = 10\n    tol = 1e-8\n    n_jobs = 1\n    dict_init = rng_global.randn(n_components, n_features)\n    code_init = rng_global.randn(n_samples, n_components)\n    verbose = False\n    random_state = 0\n\n    if positive_code and method == \"omp\":\n        # Positive constraint not supported for 'omp' coding method.\n        with pytest.raises(ValueError):\n            dict_learning(X, n_components, alpha, max_iter, tol, method,\n                          n_jobs, dict_init, code_init, verbose, random_state,\n                          positive_code=positive_code, positive_dict=positive_dict)\n    else:\n        code, dictionary, errors = dict_learning(X, n_components, alpha, max_iter, tol, method,\n                                                 n_jobs, dict_init, code_init, verbose, random_state,\n                                                 positive_code=positive_code, positive_dict=positive_dict)\n\n        if positive_code:\n            assert np.all(code >= 0)\n        if positive_dict:\n            assert np.all(dictionary >= 0)\n", "def test_sparse_encode_positive_constraint():\n    n_components = 3\n    dictionary = np.abs(rng_global.randn(n_components, n_features))\n    X_positive = np.abs(rng_global.randn(n_samples, n_features))\n    code_positive = sparse_encode(X_positive, dictionary, positive=True)\n    assert np.all(code_positive >= 0)\n\n    X_negative = -np.abs(rng_global.randn(n_samples, n_features))\n    with pytest.raises(ValueError, match=\"Positive constraint not supported for 'lasso_lars' coding method.\"):\n        sparse_encode(X_negative, dictionary, algorithm='lasso_lars', positive=True)\n"], "sample_886": ["def test_wrap_in_pandas_container_with_sparse_data():\n    data_to_wrap = csr_matrix((3, 3))\n    with pytest.raises(ValueError, match=\"Pandas output does not support sparse data.\"):\n        _wrap_in_pandas_container(data_to_wrap, columns=None)\n", "def test_wrap_in_pandas_container_sparse_input():\n    with pytest.raises(ValueError, match=\"Pandas output does not support sparse data.\"):\n        _wrap_in_pandas_container(csr_matrix([[1, 0], [0, 1]]), columns=[\"a\", \"b\"])\n", "def test_get_output_config_with_estimator():\n    class DummyEstimator:\n        _sklearn_output_config = {\"transform\": \"pandas\"}\n\n    estimator = DummyEstimator()\n    config = _get_output_config(\"transform\", estimator)\n    assert config[\"dense\"] == \"pandas\"\n\n    # Test with method not present in _sklearn_output_config\n    config = _get_output_config(\"fit\", estimator)\n    assert config[\"dense\"] == get_config()[\"fit_output\"]\n\n    # Test with invalid config value\n    estimator._sklearn_output_config = {\"transform\": \"invalid\"}\n    with pytest.raises(ValueError):\n        _get_output_config(\"transform\", estimator)\n", "def test_wrap_in_pandas_container():\n    # Test wrapping a ndarray in a pandas DataFrame\n    data = np.array([[1, 2], [3, 4]])\n    columns = [\"feature1\", \"feature2\"]\n    index = [\"sample1\", \"sample2\"]\n    df = _wrap_in_pandas_container(data, columns=columns, index=index)\n    assert df.equals(pd.DataFrame(data, columns=columns, index=index))\n\n    # Test wrapping a DataFrame with new columns and index\n    data = pd.DataFrame(data, columns=[\"old1\", \"old2\"])\n    df = _wrap_in_pandas_container(data, columns=columns, index=index)\n    assert df.equals(pd.DataFrame(data, columns=columns, index=index))\n\n    # Test wrapping a DataFrame with None columns (should keep original columns)\n    df = _wrap_in_pandas_container(data, columns=None, index=index)\n    assert df.equals(pd.DataFrame(data, index=index))\n\n    # Test wrapping a sparse matrix (should raise ValueError)\n    with pytest.raises(ValueError):\n        _wrap_in_pandas_container(csr_matrix(data), columns=columns, index=index)\n", "def test_wrap_in_pandas_container():\n    data = np.array([[1, 2, 3], [4, 5, 6]])\n    columns = [\"A\", \"B\", \"C\"]\n    index = [\"X\", \"Y\"]\n\n    df = _wrap_in_pandas_container(data, columns=columns, index=index)\n\n    assert_array_equal(df.columns.values, columns)\n    assert_array_equal(df.index.values, index)\n    assert_array_equal(df.values, data)\n"], "sample_557": ["def test_figure_subplots_adjust():\n    fig, axs = plt.subplots(2, 2)\n    fig.subplots_adjust(left=0.2, right=0.8, bottom=0.2, top=0.8, wspace=0.3, hspace=0.3)\n    assert fig.subplotpars.left == 0.2\n    assert fig.subplotpars.right == 0.8\n    assert fig.subplotpars.bottom == 0.2\n    assert fig.subplotpars.top == 0.8\n    assert fig.subplotpars.wspace == 0.3\n    assert fig.subplotpars.hspace == 0.3\n", "def test_align_labels():\n    fig, axs = plt.subplots(2, 2)\n    for tick in axs[0, 0].get_xticklabels():\n        tick.set_rotation(55)\n    axs[0, 0].set_xlabel('XLabel 0')\n    axs[0, 1].set_xlabel('XLabel 1')\n    fig.align_xlabels()\n\n    axs[1, 0].plot(np.arange(0, 1000, 50))\n    axs[1, 0].set_ylabel('YLabel 0')\n    axs[1, 1].set_ylabel('YLabel 1')\n    fig.align_ylabels()\n\n    fig.align_labels()\n", "def test_figure_subplots_adjust():\n    fig = Figure()\n    fig.subplots_adjust(left=0.2, bottom=0.2, right=0.8, top=0.8, wspace=0.2, hspace=0.2)\n    assert fig.subplotpars.left == 0.2\n    assert fig.subplotpars.bottom == 0.2\n    assert fig.subplotpars.right == 0.8\n    assert fig.subplotpars.top == 0.8\n    assert fig.subplotpars.wspace == 0.2\n    assert fig.subplotpars.hspace == 0.2\n", "def test_figure_gca():\n    fig = plt.figure()\n    ax = fig.gca()\n    assert isinstance(ax, Axes)\n    assert ax.figure is fig\n\n    ax2 = fig.add_subplot(121)\n    ax3 = fig.gca()\n    assert ax3 is ax2\n", "def test_figure_get_dpi():\n    fig = Figure()\n    fig.set_dpi(100)\n    assert fig.get_dpi() == 100\n"], "sample_1146": ["def test_invalid_decimal_separator():\n    latex(S.One, decimal_separator='invalid')\n", "def test_ImmutableDenseNDimArray_latex():\n    A = ImmutableDenseNDimArray([[1, 2], [3, 4]])\n    assert latex(A) == r\"\\left[\\begin{smallmatrix}1 & 2\\\\3 & 4\\end{smallmatrix}\\right]\"\n", "def test_latex_printing_of_imaginary_unit():\n    expr = I\n    assert latex(expr) == r'i'\n    assert latex(expr, imaginary_unit='j') == r'j'\n    assert latex(expr, imaginary_unit='ri') == r'\\mathrm{i}'\n    assert latex(expr, imaginary_unit='rj') == r'\\mathrm{j}'\n    assert latex(expr, imaginary_unit='ti') == r'\\text{i}'\n    assert latex(expr, imaginary_unit='tj') == r'\\text{j}'\n", "compilation error", "compilation error"], "sample_358": ["    def test_references_table(self):\n        self.assertTrue(self.reference.references_table('table'))\n        self.assertFalse(self.reference.references_table('other_table'))\n", "def test_references_table(self):\n    self.assertTrue(self.reference.references_table('table'))\n    self.assertFalse(self.reference.references_table('other_table'))\n", "    def test_rename_table_references(self):\n        reference = Table('old_table', lambda table: table.upper())\n        reference.rename_table_references('old_table', 'new_table')\n        self.assertEqual(reference.table, 'new_table')\n        self.assertEqual(str(reference), 'NEW_TABLE')\n", "    def test_references_table(self):\n        self.assertTrue(self.reference.references_table('table'))\n        self.assertFalse(self.reference.references_table('other_table'))\n", "    def test_references_table(self):\n        self.assertTrue(self.reference.references_table('table'))\n        self.assertFalse(self.reference.references_table('other_table'))\n"], "sample_69": ["    def test_zip_imported_module(self):\n        # Create a temporary zip file with a Python module in it.\n        zip_filename = self.temporary_file('test_module.zip')\n        with zipfile.ZipFile(zip_filename, 'w') as zf:\n            zf.writestr('test_module.py', 'print(\"Hello, world!\")')\n\n        # Add the zip file to sys.path and import the module.\n        with extend_sys_path(str(zip_filename)):\n            self.import_and_cleanup('test_module')\n\n        # Check that the zip file is in the list of files to watch.\n        self.assertFileFound(zip_filename)\n", "    def test_weakref_modules_not_included(self):\n        # Test that weakref modules are not included in the list of files to watch\n        class MockModule:\n                self.__spec__ = types.ModuleSpec('mock_module', None)\n\n        with mock.patch.dict(sys.modules, {'mock_module': weakref.ref(MockModule())}):\n            self.clear_autoreload_caches()\n            self.assertEqual(len(list(autoreload.iter_all_python_module_files())), 0)\n", "    def test_zip_imported_modules(self):\n        zip_filename = self.temporary_file('test_package.zip')\n        zip_file = zipfile.ZipFile(zip_filename, 'w')\n        zip_file.writestr('test_package/__init__.py', '')\n        zip_file.writestr('test_package/module.py', 'value = 1')\n        zip_file.close()\n        sys.path.insert(0, str(zip_filename))\n        self.addCleanup(sys.path.remove, str(zip_filename))\n        self.import_and_cleanup('test_package.module')\n        self.assertFileFound(zip_filename)\n", "    def test_import_from_zip(self):\n        with tempfile.NamedTemporaryFile(suffix='.zip') as zip_file:\n            with zipfile.ZipFile(zip_file.name, 'w') as z:\n                z.writestr('test_module.py', b'# test module')\n            sys.path_importer_cache.clear()\n            sys.path.append(zip_file.name)\n            self.import_and_cleanup('test_module')\n            self.assertFileFound(Path(zip_file.name) / 'test_module.py')\n", "    def test_zip_import(self):\n        # Test modules loaded from a zip file\n        zip_filename = self.temporary_file('test.zip')\n        mod_filename = 'mymodule.py'\n        mod_content = b'def myfunc():\\n    pass'\n\n        with zipfile.ZipFile(zip_filename, 'w') as z:\n            z.writestr(mod_filename, mod_content)\n\n        with extend_sys_path(str(zip_filename)):\n            self.import_and_cleanup('mymodule')\n            self.assertFileFound(zip_filename)\n\n        # Test modules with no location\n        mod_without_location = types.ModuleType('mod_without_location')\n        mod_without_location.__spec__ = types.ModuleSpec('mod_without_location', None)\n        sys.modules['mod_without_location'] = mod_without_location\n        self.clear_autoreload_caches()\n        self.assertEqual(list(autoreload.iter_all_python_module_files()), [])\n"], "sample_515": ["def test_colorbar_extension_length():\n    uniform_fig = _colorbar_extension_length('uniform')\n    proportional_fig = _colorbar_extension_length('proportional')\n    check_figures_equal(uniform_fig, proportional_fig, check_colors=True)\n", "def test_colorbar_ticks_position():\n    fig, ax = plt.subplots()\n    im = ax.imshow(np.arange(100).reshape((10, 10)))\n    cbar = fig.colorbar(im, ticks=[20, 40, 60, 80], orientation='horizontal', ticklocation='left')\n    return fig\n", "def test_colorbar_orientation():\n    fig, ax = plt.subplots(2, 1, figsize=(6, 8))\n    data = np.random.rand(10, 10)\n    cmap = cm.get_cmap('viridis')\n\n    im = ax[0].imshow(data, cmap=cmap)\n    fig.colorbar(im, ax=ax[0], orientation='vertical')\n\n    im = ax[1].imshow(data, cmap=cmap)\n    fig.colorbar(im, ax=ax[1], orientation='horizontal')\n\n    return fig\n", "def test_colorbar_ticks_custom_locator():\n    \"\"\"Test colorbar with custom tick locator.\"\"\"\n    fig, ax = plt.subplots()\n    cmap = plt.get_cmap('viridis')\n    norm = Normalize(vmin=0, vmax=1)\n    cb = Colorbar(ax, cmap=cmap, norm=norm, orientation='vertical')\n    cb.set_ticks([0.2, 0.4, 0.6, 0.8])\n    return fig\n", "def test_colorbar_ticks_locator():\n    \"\"\"Test the ticks locator on a colorbar.\"\"\"\n    # Create a figure and an axes.\n    fig, ax = plt.subplots()\n\n    # Create a colorbar with a fixed locator.\n    cmap = cm.get_cmap('viridis', 5)\n    norm = Normalize(vmin=0, vmax=1)\n    cbar = fig.colorbar(cm.ScalarMappable(norm=norm, cmap=cmap),\n                        ax=ax, ticks=FixedLocator([0.25, 0.5, 0.75]))\n\n    # Check the ticks locator.\n    assert isinstance(cbar.locator, FixedLocator)\n    assert np.allclose(cbar.locator.locs, [0.25, 0.5, 0.75])\n\n    # Check the ticks labels.\n    labels = [label.get_text() for label in cbar.ax.yaxis.get_ticklabels()]\n    assert labels == ['0.25', '0.5', '0.75']\n\n    plt.close(fig)\n"], "sample_510": ["def test_sca():\n    fig1 = plt.figure()\n    ax1 = fig1.add_subplot(111)\n    plt.sca(ax1)\n    assert plt.gca() is ax1\n", "def test_figure_size():\n    fig = plt.figure(figsize=(4, 3))\n    assert fig.get_size_inches() == (4, 3)\n", "def test_set_cmap():\n    plt.set_cmap('viridis')\n    assert plt.rcParams['image.cmap'] == 'viridis'\n    im = plt.imshow(np.random.rand(10, 10))\n    assert im.get_cmap().name == 'viridis'\n", "def test_tick_params():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3], [4, 5, 6])\n    ax.tick_params(axis='x', colors='red')\n    plt.close(fig)\n", "def test_figlegend():\n    fig, ax = plt.subplots()\n    lines = ax.plot([1, 2, 3], [4, 5, 6])\n    fig.legend(lines, ['Line 1', 'Line 2'])\n    plt.close(fig)\n"], "sample_341": ["    def test_custom_kwarg_passed_to_forms(self):\n        CustomKwargFormSet = formset_factory(CustomKwargForm, extra=2, form_kwargs={'custom_kwarg': 'test'})\n        formset = CustomKwargFormSet()\n        for form in formset:\n            self.assertEqual(form.custom_kwarg, 'test')\n", "    def test_duplicate_drinks_raises_error(self):\n        formset_data = [\n            {'name': 'Coffee'},\n            {'name': 'Tea'},\n            {'name': 'Coffee'},\n        ]\n        formset = FavoriteDrinksFormSet({\n            'form-TOTAL_FORMS': '3',\n            'form-INITIAL_FORMS': '0',\n            'form-MAX_NUM_FORMS': '',\n            'form-0-name': 'Coffee',\n            'form-1-name': 'Tea',\n            'form-2-name': 'Coffee',\n        })\n        self.assertFalse(formset.is_valid())\n        self.assertEqual(formset.non_form_errors(), ['You may only specify a drink once.'])\n", "def test_clean_method_in_custom_formset(self):\n    # Create a list of favorite drinks with a duplicate\n    formset_data = [{'name': 'Coffee'}, {'name': 'Tea'}, {'name': 'Coffee'}]\n    # Create a FavoriteDrinksFormSet with the data\n    formset = FavoriteDrinksFormSet({'form-TOTAL_FORMS': '3', 'form-INITIAL_FORMS': '0',\n                                     'form-0-name': 'Coffee', 'form-1-name': 'Tea', 'form-2-name': 'Coffee'})\n    # Call the clean method of the formset\n    with self.assertRaisesMessage(ValidationError, 'You may only specify a drink once.'):\n        formset.clean()\n", "def test_formset_custom_kwargs(self):\n    CustomKwargFormSet = formset_factory(CustomKwargForm, formset=BaseFormSet, extra=2)\n    formset = CustomKwargFormSet(form_kwargs={'custom_kwarg': 'test'})\n    for form in formset:\n        self.assertEqual(form.custom_kwarg, 'test')\n", "def test_initial_form_count_bound(self):\n    formset = self.make_choiceformset(formset_data=[('coffee', 3), ('tea', 2)], initial_forms=1)\n    self.assertEqual(formset.initial_form_count(), 1)\n"], "sample_223": ["    def test_select_related_and_prefetch_related(self):\n        # Test select_related with prefetch_related\n        items = Item.objects.select_related('creator__extra__note').prefetch_related('tags')\n        for item in items:\n            # Access related objects to trigger query execution\n            _ = item.creator.extra.note\n            _ = item.tags.all()\n\n        # Test prefetch_related with select_related\n        authors = Author.objects.prefetch_related('item_set__tags').select_related('extra__note')\n        for author in authors:\n            # Access related objects to trigger query execution\n            _ = author.item_set.all()\n            _ = author.extra.note\n", "    def test_item_filter_with_in_lookup_and_subquery(self):\n        subquery = Author.objects.filter(num__in=[2002, 3003]).values('id')\n        items = Item.objects.filter(creator__id__in=subquery)\n        self.assertQuerySetEqual(items, [self.i2, self.i3], ordered=False)\n", "def test_annotate_with_aggregate(self):\n    annotations = Annotation.objects.annotate(num_notes=Count('notes'))\n    self.assertEqual(annotations.get(name='a1').num_notes, 1)\n    self.assertEqual(annotations.get(name='a2').num_notes, 2)\n", "    def setUpTestData(cls):\n        Author.objects.create(name='Author1', num=1)\n        Author.objects.create(name='Author2', num=2)\n", "    def test_item_filter_with_subquery(self):\n        # Test filtering items based on a subquery in the filter expression\n        # This should improve coverage of the filter() method and Exists() expression\n        subquery = Report.objects.filter(name__startswith='r')\n        items = Item.objects.filter(report__in=subquery)\n        self.assertEqual(len(items), 2)\n        self.assertIn(self.i1, items)\n        self.assertIn(self.i2, items)\n"], "sample_1162": ["def test_matrix_is_commutative():\n    M = Matrix([[1, 2], [3, 4]])\n    assert M.is_commutative == True\n\n    M = Matrix([[comm_x, 2], [3, 4]])\n    assert M.is_commutative == True\n\n    M = Matrix([[noncomm_x, 2], [3, 4]])\n    assert M.is_commutative == False\n", "def test_derivative():\n    f = Function('f')\n    x = Symbol('x')\n    y = Symbol('y')\n\n    # Test differentiation with respect to a symbol\n    assert f(x).diff(x) == Derivative(f(x), x)\n\n    # Test differentiation with respect to a function\n    assert f(x).diff(f(x)) == Derivative(f(x), f(x))\n\n    # Test differentiation with multiple variables\n    assert f(x, y).diff(x, y) == Derivative(f(x, y), x, y)\n\n    # Test differentiation with repeated variables\n    assert f(x).diff(x, 2) == Derivative(f(x), x, x)\n", "def test_Derivative_init():\n    # Test the __new__ method of the Derivative class\n    expr = Symbol('x')**2\n    var = Symbol('x')\n    count = 2\n    derivative = Derivative(expr, (var, count))\n    assert derivative.expr == expr\n    assert derivative.variable_count == [(var, count)]\n    assert derivative.variables == (var, var)\n    assert derivative.free_symbols == {var}\n", "def test_AlgebraicNumber():\n    a = AlgebraicNumber(pi, 5)\n    assert a.as_expr() == pi\n    assert a.minpoly(x) == x - pi\n    assert a.is_algebraic\n    assert not a.is_transcendental\n", "compilation error"], "sample_146": ["    def test_language_code_setting(self):\n        \"\"\"Test the LANGUAGE_CODE setting.\"\"\"\n        with override_settings(LANGUAGE_CODE='en'):\n            self.assertEqual(check_setting_language_code(None), [])\n\n        for tag in self.invalid_tags:\n            with override_settings(LANGUAGE_CODE=tag):\n                self.assertEqual(check_setting_language_code(None), [Error('You have provided an invalid value for the LANGUAGE_CODE setting: {!r}.'.format(tag), id='translation.E001')])\n", "    def test_check_language_settings_consistent(self):\n        with self.settings(LANGUAGE_CODE='en', LANGUAGES=[('en', 'English')]):\n            errors = check_language_settings_consistent(None)\n            self.assertEqual(errors, [])\n\n        with self.settings(LANGUAGE_CODE='fr', LANGUAGES=[('en', 'English')]):\n            errors = check_language_settings_consistent(None)\n            self.assertEqual(errors, [Error('You have provided a value for the LANGUAGE_CODE setting that is not in '\n                                           'the LANGUAGES setting.', id='translation.E004')])\n", "    def test_language_settings_consistent(self):\n        with self.settings(LANGUAGE_CODE='en', LANGUAGES=[('en', 'English'), ('fr', 'French')]):\n            self.assertEqual(check_language_settings_consistent(None), [])\n        with self.settings(LANGUAGE_CODE='en-US', LANGUAGES=[('en', 'English'), ('fr', 'French')]):\n            self.assertEqual(check_language_settings_consistent(None), [Error('You have provided a value for the LANGUAGE_CODE setting that is not in the LANGUAGES setting.', id='translation.E004')])\n", "    def test_languages_bidi_setting(self):\n        \"\"\"Test check_setting_languages_bidi function with invalid values.\"\"\"\n        with override_settings(LANGUAGES_BIDI=self.invalid_tags):\n            errors = check_setting_languages_bidi(None)\n            self.assertEqual(len(errors), len(self.invalid_tags))\n            for i, error in enumerate(errors):\n                self.assertEqual(error, Error(f'You have provided an invalid language code in the LANGUAGES_BIDI setting: {self.invalid_tags[i]!r}.', id='translation.E003'))\n\n        with override_settings(LANGUAGES_BIDI=self.valid_tags):\n            errors = check_setting_languages_bidi(None)\n            self.assertEqual(len(errors), 0)\n", "def test_setting_languages_consistent(self):\n    \"\"\"Test that LANGUAGE_CODE setting is consistent with LANGUAGES.\"\"\"\n    with self.settings(LANGUAGE_CODE='en'):\n        errors = check_language_settings_consistent(None)\n        self.assertEqual(errors, [])\n\n    with self.settings(LANGUAGE_CODE='fr'):\n        errors = check_language_settings_consistent(None)\n        self.assertEqual(errors, [Error('translation.E004')])\n"], "sample_291": ["    def test_get_redirect_url(self):\n        view = RedirectView.as_view(url='/test-url/%(pk)s/')\n        request = self.rf.get('/redirect-test/123/')\n        response = view(request, pk=123)\n        self.assertEqual(response.url, '/test-url/123/')\n", "    def test_redirect_url(self):\n        request = self.rf.get('/redirect_url')\n        response = RedirectView.as_view(url='/about')(request)\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response.url, '/about')\n", "    def test_get_redirect_url(self):\n        view = RedirectView(pattern_name='some_pattern_name')\n        request = self.rf.get('/')\n        view.setup(request)\n        url = view.get_redirect_url(arg='value')\n        self.assertEqual(url, '/some_url/value')\n", "    def test_permanent_redirect(self):\n        url = '/redirect-url/'\n        view = RedirectView.as_view(url=url, permanent=True)\n        request = self.rf.get('/')\n        response = view(request)\n        self.assertEqual(response.status_code, 301)\n        self.assertEqual(response.url, url)\n", "    def test_custom_template_view(self):\n        view = AboutTemplateView.as_view()\n        request = self.rf.get('/about/')\n        response = view(request)\n        self.assertEqual(response.status_code, 200)\n        self.assertTemplateUsed(response, 'generic_views/about.html')\n"], "sample_754": ["def test_sparse_pca_components_orthogonal(norm_comp):\n    \"\"\"Test that SparsePCA components are orthogonal.\"\"\"\n    n_components = 3\n    n_samples = 20\n    image_size = (10, 10)\n    Y, _, _ = generate_toy_data(n_components, n_samples, image_size, random_state=0)\n\n    spca = SparsePCA(n_components=n_components, normalize_components=norm_comp)\n    spca.fit(Y)\n\n    components = spca.components_\n    dot_product = np.abs(np.dot(components, components.T))\n    np.fill_diagonal(dot_product, 0)  # fill diagonal with zeros\n    assert_allclose(dot_product, np.zeros_like(dot_product), atol=1e-7)\n", "def test_sparse_pca_with_custom_init(norm_comp):\n    np.random.seed(0)\n    Y, U, V = generate_toy_data(n_components=3, n_samples=5, image_size=(10, 10))\n    sparse_pca = SparsePCA(n_components=3, U_init=0.1 * np.random.randn(5, 3), V_init=0.1 * np.random.randn(3, 100), normalize_components=norm_comp)\n    sparse_pca.fit(Y)\n    assert_true(np.allclose(sparse_pca.components_, V, atol=1e-2))\n", "def test_sparse_pca_transform(norm_comp):\n    n_components = 3\n    n_samples = 100\n    image_size = (10, 10)\n    random_state = 42\n\n    Y, U, V = generate_toy_data(n_components, n_samples, image_size, random_state)\n\n    sparse_pca = SparsePCA(n_components=n_components, random_state=random_state,\n                           normalize_components=norm_comp)\n    sparse_pca.fit(Y)\n\n    U_transform = sparse_pca.transform(Y)\n\n    assert_allclose(U_transform, U, atol=1e-2)\n", "def test_sparse_pca_transform(norm_comp):\n    Y, U, V = generate_toy_data(3, 20, (10, 10), random_state=0)\n    sparse_pca = SparsePCA(n_components=3, random_state=0, normalize_components=norm_comp)\n    sparse_pca.fit(Y)\n    U_transform = sparse_pca.transform(Y)\n    # Test that the transformed data has the correct shape\n    assert U_transform.shape == (20, 3)\n    # Test that the transformed data is close to the original U, given the sparsity and noise in the data\n    assert_allclose(U_transform, U, atol=0.5)\n", "def test_sparse_pca_decomposition(norm_comp):\n    n_components = 3\n    n_samples = 10\n    image_size = (10, 10)\n    random_state = 42\n\n    Y, U, V = generate_toy_data(n_components, n_samples, image_size, random_state)\n\n    sparse_pca = SparsePCA(n_components=n_components, random_state=random_state, normalize_components=norm_comp)\n    sparse_pca.fit(Y)\n\n    # Check that the components are correct\n    assert_array_almost_equal(sparse_pca.components_, V, decimal=1)\n\n    # Check that the transformation is correct\n    U_transform = sparse_pca.transform(Y)\n    assert_array_almost_equal(U_transform, U, decimal=1)\n\n    # Check that the transformation with ridge_alpha parameter is deprecated\n    with pytest.warns(DeprecationWarning, match=\"The ridge_alpha parameter on transform() is deprecated\"):\n        U_transform_ridge = sparse_pca.transform(Y, ridge_alpha=0.1)\n\n    # Check that the number of iterations is not None\n    assert sparse_pca.n_iter_ is not None\n"], "sample_29": ["def test_write_latex_with_custom_table_class(tmp_path, cosmo):\n    \"\"\"Test that write_latex works with a custom table class.\"\"\"\n\n    class CustomTable(QTable):\n        pass\n\n    file = tmp_path / \"custom_table.tex\"\n    write_latex(cosmo, file, cls=CustomTable)\n\n    with open(file, \"r\") as f:\n        content = f.read()\n        assert \"CustomTable\" in content\n", "def test_write_latex_latex_names(self, tmp_path, cosmo, latex_names):\n    file = tmp_path / \"cosmo.tex\"\n    write_latex(cosmo, file, latex_names=latex_names)\n\n    with open(file) as f:\n        contents = f.read()\n\n    if latex_names:\n        for latex_name in _FORMAT_TABLE.values():\n            assert latex_name in contents\n    else:\n        for param_name in cosmo.__parameters__:\n            assert param_name in contents\n", "def test_write_latex_with_cls(cosmo, tmp_path):\n    \"\"\"Test write_latex with a different cls\"\"\"\n    file = tmp_path / \"cosmology.tex\"\n    write_latex(cosmo, file, cls=Table)\n    assert file.exists()\n\n    # Check that the file contains the correct data\n    with open(file, \"r\") as f:\n        lines = f.readlines()\n\n    for param in cosmo.__parameters__:\n        assert _FORMAT_TABLE.get(param, param) in \"\".join(lines)\n", "def test_latex_names(cosmo):\n    # Create a QTable with latex names\n    table = QTable()\n    for name in cosmo.__parameters__:\n        table[name] = [getattr(cosmo, name)]\n\n    # Call the function with latex_names=True\n    write_latex(cosmo, \"test.tex\", latex_names=True)\n\n    # Read the table back from the file\n    table_read = Table.read(\"test.tex\", format=\"ascii.latex\")\n\n    # Check that the column names are in LaTeX format\n    for name in cosmo.__parameters__:\n        assert _FORMAT_TABLE.get(name, name) in table_read.colnames\n", "def test_write_latex_different_table_class(cosmo, tmp_path):\n    \"\"\"\n    Test writing a Cosmology to a LaTeX file using a different table class.\n    \"\"\"\n    file = tmp_path / \"test.tex\"\n\n    # Test with Table class instead of QTable\n    write_latex(cosmo, file, cls=Table)\n\n    # Check if the file was created\n    assert file.exists()\n\n    # Read the file and check its contents\n    with open(file, \"r\") as f:\n        content = f.read()\n        assert \"\\\\documentclass\" in content\n        assert \"\\\\begin{document}\" in content\n        assert \"\\\\end{document}\" in content\n        assert \"\\\\begin{tabular}\" in content\n        assert \"\\\\end{tabular}\" in content\n\n    # Clean up\n    file.unlink()\n"], "sample_799": ["def test_cross_validate_multimetric():\n    # Test cross_validate with multiple metrics\n    estimator = MockClassifier()\n    scoring = {'accuracy': make_scorer(accuracy_score),\n               'precision': make_scorer(precision_score, average='macro')}\n    cv_results = cross_validate(estimator, X, y, scoring=scoring, cv=2,\n                                return_train_score=True)\n    assert_equal(sorted(cv_results.keys()),\n                 ['estimator', 'fit_time', 'score_time',\n                  'test_accuracy', 'test_precision',\n                  'train_accuracy', 'train_precision'])\n", "def test_cross_validate_with_fit_params():\n    X, y = load_iris(return_X_y=True)\n    estimator = MockClassifier()\n    cv = KFold(n_splits=3)\n    fit_params = {'dummy_int': 1, 'dummy_str': 'test', 'dummy_obj': object(),\n                  'sample_weight': np.ones(X.shape[0]),\n                  'class_prior': np.ones(len(np.unique(y))),\n                  'sparse_sample_weight': coo_matrix(np.ones(X.shape[0])),\n                  'sparse_param': P_sparse}\n    scores = cross_validate(estimator, X, y, cv=cv, fit_params=fit_params)\n    assert_equal(estimator.dummy_int, fit_params['dummy_int'])\n    assert_equal(estimator.dummy_str, fit_params['dummy_str'])\n    assert_equal(estimator.dummy_obj, fit_params['dummy_obj'])\n", "def test_learning_curve_incremental_learning():\n    n_samples = 100\n    X, y = make_classification(n_samples=n_samples, n_features=20,\n                               n_informative=2, n_redundant=10,\n                               n_classes=2, random_state=0)\n    estimator = MockIncrementalImprovingEstimator(n_samples)\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=ShuffleSplit(n_splits=10, test_size=0.2,\n                                         random_state=0),\n        train_sizes=np.linspace(0.1, 1.0, 5),\n        exploit_incremental_learning=True)\n    assert_greater(train_scores[:, 0], train_scores[:, -1])\n    assert_less(test_scores[:, 0], test_scores[:, -1])\n", "def test_cross_validate_multimetric():\n    estimator = MockClassifier()\n    scorers = {'score1': make_scorer(accuracy_score), 'score2': make_scorer(precision_score, average='micro')}\n    cv_results = cross_validate(estimator, X, y, cv=3, scoring=scorers)\n    assert 'test_score1' in cv_results\n    assert 'test_score2' in cv_results\n", "def test_cross_val_score_multimetric_single_string():\n    scoring = 'accuracy'\n    scores = cross_val_score(MockClassifier(), X, y, scoring=scoring)\n    assert_array_almost_equal(scores, np.array([0.8, 0.8, 0.8, 0.8, 0.8]))\n"], "sample_114": ["def test_generate_altered_order_with_respect_to(self):\n    before_states = [self.author_with_book]\n    after_states = [self.author_with_book_order_wrt]\n    changes = self.get_changes(before_states, after_states)\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['AlterOrderWithRespectTo'])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name='Author', order_with_respect_to='book')\n", "def test_deconstructible_objects_equivalent(self):\n    questioner = MigrationQuestioner(specified_apps={'testapp'})\n    changes = self.get_changes([self.author_name_deconstructible_1], [self.author_name_deconstructible_2], questioner)\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['AlterField'])\n    self.assertOperationFieldAttributes(changes, 'testapp', 0, 0, default=self.author_name_deconstructible_1.fields[1][1]['default'])\n\n    changes = self.get_changes([self.author_name_deconstructible_1], [self.author_name_deconstructible_3], questioner)\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['AlterField'])\n    self.assertOperationFieldAttributes(changes, 'testapp', 0, 0, default=self.author_name_deconstructible_3.fields[1][1]['default'])\n", "def test_altered_options(self):\n    changes = self.get_changes(\n        [self.author_with_options],\n        [self.author_with_db_table_options],\n    )\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterModelOptions\"])\n", "def test_deep_deconstruct(self):\n    \"\"\"\n    Test deep deconstruction of fields and their arguments.\n    \"\"\"\n    autodetector = MigrationAutodetector(None, None)\n    obj1 = DeconstructibleObject(1, 2, 3)\n    obj2 = DeconstructibleObject(4, 5, 6)\n    field = models.CharField(max_length=200, default=[obj1, obj2])\n    deconstructed = autodetector.deep_deconstruct(field)\n    expected = ('django.db.models.fields.CharField', [], {\n        'default': [\n            ('__main__.DeconstructibleObject', (1, 2, 3), {}),\n            ('__main__.DeconstructibleObject', (4, 5, 6), {})\n        ],\n        'max_length': 200,\n    })\n    self.assertEqual(deconstructed, expected)\n", "def test_deconstructible_objects(self):\n    questioner = MigrationQuestioner(specified_apps=['testapp'])\n    changes = self.get_changes([self.author_empty], [self.author_name_deconstructible_1], questioner)\n    self.assertOperationFieldAttributes(changes, 'testapp', 0, 0, max_length=200, default='<__main__.DeconstructibleObject object>')\n    changes = self.get_changes([self.author_name_deconstructible_1], [self.author_name_deconstructible_2], questioner)\n    self.assertOperationFieldAttributes(changes, 'testapp', 0, 0, max_length=200, default='<__main__.DeconstructibleObject object>')\n    changes = self.get_changes([self.author_name_deconstructible_1], [self.author_name_deconstructible_3], questioner)\n    self.assertOperationFieldAttributes(changes, 'testapp', 0, 0, max_length=200, default='<django.db.models.fields.IntegerField>')\n    changes = self.get_changes([self.author_name_deconstructible_3], [self.author_name_deconstructible_4], questioner)\n    self.assertOperationFieldAttributes(changes, 'testapp', 0, 0, max_length=200, default='<django.db.models.fields.IntegerField>')\n"], "sample_804": ["def test_ordinal_encoder_inverse_transform():\n    enc = OrdinalEncoder()\n    X = [['Male', 1], ['Female', 3], ['Female', 2]]\n    enc.fit(X)\n    transformed = enc.transform(X)\n    inverse_transformed = enc.inverse_transform(transformed)\n    assert_array_equal(inverse_transformed, X)\n", "def test_ordinal_encoder_with_drop():\n    enc = OrdinalEncoder(categories=[[\"cold\", \"hot\"], [\"apple\", \"banana\", \"cherry\"]], dtype=np.int)\n    enc.fit([[\"hot\", \"banana\"], [\"cold\", \"apple\"]])\n    assert_array_equal(enc.transform([[\"hot\", \"cherry\"]]), [[1, 2]])\n    enc_drop = OrdinalEncoder(categories=[[\"cold\", \"hot\"], [\"apple\", \"banana\", \"cherry\"]], dtype=np.int, drop=\"first\")\n    enc_drop.fit([[\"hot\", \"banana\"], [\"cold\", \"apple\"]])\n    assert_array_equal(enc_drop.transform([[\"hot\", \"cherry\"]]), [[0, 1]])\n", "def test_ordinal_encoder_inverse_transform():\n    X = [['Male', 1], ['Female', 3], ['Female', 2]]\n    enc = OrdinalEncoder()\n    enc.fit(X)\n    X_transformed = enc.transform(X)\n    X_inverse = enc.inverse_transform(X_transformed)\n    assert_array_equal(X, X_inverse)\n", "def test_onehotencoder_handle_unknown_error():\n    enc = OneHotEncoder(handle_unknown='error')\n    X = [['Male', 1], ['Female', 3], ['Female', 2]]\n    enc.fit(X)\n    assert_raises(ValueError, enc.transform, [['Unknown', 1]])\n", "def test_ordinal_encoder(encoder, expected_warnings, expected_error):\n    X = [['Male', 1], ['Female', 3], ['Female', 2]]\n    if expected_error is not None:\n        with pytest.raises(expected_error) as exc_info:\n            encoder.fit(X)\n        assert expected_warnings[0] in str(exc_info.value)\n    else:\n        with pytest.warns(UserWarning) as record:\n            encoder.fit(X)\n        assert [str(w.message) for w in record] == expected_warnings\n        assert_array_equal(encoder.transform([['F"], "sample_509": ["def test_datestr2num():\n    date_str = '2022-01-01 12:00:00'\n    expected_output = mdates.date2num(datetime.datetime.strptime(date_str, '%Y-%m-%d %H:%M:%S'))\n    assert mdates.datestr2num(date_str) == expected_output\n", "def test_year_locator_with_interval():\n    dmin = datetime.datetime(2000, 1, 1)\n    dmax = datetime.datetime(2010, 1, 1)\n    locator = mdates.YearLocator(base=2)\n    result = locator._create_rrule(dmin, dmax)\n    assert result == (datetime.datetime(2000, 1, 1), datetime.datetime(2010, 1, 1))\n", "def test_num2date():\n    # Test with a single float\n    dt = mdates.num2date(737760.0)\n    assert dt == datetime.datetime(2020, 1, 2, 0, 0, tzinfo=datetime.timezone.utc)\n\n    # Test with a sequence of floats\n    dts = mdates.num2date([737760.0, 737761.0])\n    assert dts == [datetime.datetime(2020, 1, 2, 0, 0, tzinfo=datetime.timezone.utc),\n                   datetime.datetime(2020, 1, 3, 0, 0, tzinfo=datetime.timezone.utc)]\n\n    # Test with a single float and a timezone\n    tz = dateutil.tz.gettz('America/New_York')\n    dt = mdates.num2date(737760.0, tz=tz)\n    assert dt == datetime.datetime(2019, 12, 31, 19, 0, tzinfo=tz)\n", "def test_auto_date_locator():\n    dmin = datetime.datetime(2020, 1, 1)\n    dmax = datetime.datetime(2020, 1, 31)\n    locator = mdates.AutoDateLocator()\n    values = locator.tick_values(mdates.date2num(dmin), mdates.date2num(dmax))\n    expected_values = [mdates.date2num(datetime.datetime(2020, 1, i)) for i in range(1, 32, 5)]\n    np.testing.assert_array_equal(values, expected_values)\n", "def test_auto_date_formatter_mixed_dates():\n    fig, ax = plt.subplots(figsize=(6, 4), constrained_layout=True)\n\n    # create a list of datetime objects\n    dates = [datetime.datetime(2021, 1, 1),\n             datetime.datetime(2021, 1, 2),\n             datetime.datetime(2021, 1, 3, 12),\n             datetime.datetime(2021, 1, 3, 18),\n             datetime.datetime(2021, 1, 4, 6),\n             datetime.datetime(2021, 1, 5)]\n\n    # convert the list of datetime objects to matplotlib dates\n    mdates = mdates.date2num(dates)\n\n    # plot the data\n    ax.plot(mdates, range(len(dates)))\n\n    # set the x-axis locator and formatter\n    locator = mdates.AutoDateLocator()\n    formatter = mdates.AutoDateFormatter(locator)\n    ax.xaxis.set_major_locator(locator)\n    ax.xaxis.set_major_formatter(formatter)\n\n    # set the title\n    ax.set_title('AutoDateFormatter with Mixed Dates')\n"], "sample_872": ["def test_top_k_accuracy_score_multiclass_without_labels(self):\n    y_true = np.array([0, 1, 2, 2])\n    y_score = np.array([[0.5, 0.2, 0.2],\n                        [0.3, 0.4, 0.2],\n                        [0.2, 0.4, 0.3],\n                        [0.7, 0.2, 0.1]])\n    k = 2\n    normalize = True\n    expected_score = 0.75\n    assert_almost_equal(top_k_accuracy_score(y_true, y_score, k=k, normalize=normalize), expected_score)\n", "def test_top_k_accuracy_score(binary=False):\n    y_true, y_pred, y_score = make_prediction(binary=binary)\n\n    for k in range(1, 5):\n        score = top_k_accuracy_score(y_true, y_score, k=k)\n        assert 0 <= score <= 1\n\n    # Test normalize=False\n    for k in range(1, 5):\n        score = top_k_accuracy_score(y_true, y_score, k=k, normalize=False)\n        assert 0 <= score <= len(y_true)\n\n    # Test sample_weight\n    sample_weight = np.random.rand(len(y_true))\n    for k in range(1, 5):\n        score = top_k_accuracy_score(y_true, y_score, k=k, sample_weight=sample_weight)\n        assert 0 <= score <= 1\n\n    # Test labels\n    if not binary:\n        labels = np.unique(y_true)\n        for k in range(1, 5):\n            score = top_k_accuracy_score(y_true, y_score, k=k, labels=labels)\n            assert 0 <= score <= 1\n", "def test_ndcg_score_ignore_ties(drop):\n    y_true = np.array([[10, 0, 0, 1, 5]])\n    scores = np.array([[1, 0, 0, 0, 1]])\n    gain = _ndcg_sample_scores(y_true, scores, ignore_ties=drop)\n    if drop:\n        assert_almost_equal(gain, 0.5)\n    else:\n        assert_almost_equal(gain, 0.75)\n", "def test_ndcg_score_ignore_ties(n_samples=30, n_labels=10, k=3, ignore_ties=True):\n    \"\"\"Test NDCG with ignore_ties=True.\"\"\"\n    rng = np.random.RandomState(0)\n    y_true = rng.randint(0, 2, size=(n_samples, n_labels))\n    y_score = rng.randint(0, 2, size=(n_samples, n_labels))\n    ndcg_slow = ndcg_score(y_true, y_score, k=k, ignore_ties=False)\n    ndcg_fast = ndcg_score(y_true, y_score, k=k, ignore_ties=ignore_ties)\n    assert_almost_equal(ndcg_slow, ndcg_fast)\n", "def test_ndcg_score_tie_averaging(ignore_ties):\n    y_true = np.asarray([[10, 0, 0, 1, 5]])\n    y_score = np.asarray([[1, 0, 0, 0, 1]])\n    expected_score = 0.75  # The average true relevance of our top prediction: (10 / 10 + 5 / 10) / 2 = 0.75\n    if ignore_ties:\n        expected_score = 0.5  # If we ignore ties, we get a different result: 5 / 10 = 0.5\n    assert_almost_equal(ndcg_score(y_true, y_score, k=1, ignore_ties=ignore_ties), expected_score)\n"], "sample_1079": ["def test_point_canberra_distance():\n    p1 = Point(1, 1)\n    p2 = Point(3, 3)\n    assert p1.canberra_distance(p2) == 1\n\n    p1 = Point(0, 0)\n    p2 = Point(3, 3)\n    assert p1.canberra_distance(p2) == 2\n\n    p1 = Point(0, 0)\n    p2 = Point(0, 0)\n    with raises(ValueError):\n        p1.canberra_distance(p2)\n", "def test_are_coplanar():\n    p1 = Point3D(1, 2, 2)\n    p2 = Point3D(2, 7, 2)\n    p3 = Point3D(0, 0, 2)\n    p4 = Point3D(1, 1, 2)\n    assert Point3D.are_coplanar(p1, p2, p3, p4) == True\n    p5 = Point3D(0, 1, 3)\n    assert Point3D.are_coplanar(p1, p2, p3, p5) == False\n", "def test_point_subtraction():\n    p1 = Point(1, 2, 3)\n    p2 = Point(4, 5, 6)\n    result = p1 - p2\n    expected_result = Point(-3, -3, -3)\n    assert result == expected_result\n", "def test_intersection_with_plane():\n    p = Point3D(1, 2, 3)\n    plane = Plane(Point3D(0, 0, 0), Point3D(1, 0, 0), Point3D(0, 1, 0))\n    intersection = p.intersection(plane)\n    assert intersection == [p]\n", "def test_point_equality():\n    p1 = Point(1, 2)\n    p2 = Point(1, 2)\n    p3 = Point(2, 3)\n    assert p1 == p2\n    assert p1 != p3\n    assert p1 != \"not a point\"\n"], "sample_1194": ["def test_print_ImaginaryUnit():\n    assert julia_code(S.ImaginaryUnit) == \"im\"\n", "def test_bessel_functions():\n    assert julia_code(besselj(n, x)) == 'besselj(n, x)'\n    assert julia_code(bessely(n, x)) == 'bessely(n, x)'\n    assert julia_code(besseli(n, x)) == 'besseli(n, x)'\n    assert julia_code(besselk(n, x)) == 'besselk(n, x)'\n    assert julia_code(hankel1(n, x)) == 'hankelh1(n, x)'\n    assert julia_code(hankel2(n, x)) == 'hankelh2(n, x)'\n    assert julia_code(airyai(x)) == 'airyai(x)'\n    assert julia_code(airybi(x)) == 'airybi(x)'\n    assert julia_code(airyaiprime(x)) == 'airyaiprime(x)'\n    assert julia_code(airybiprime(x)) == 'airybiprime(x)'\n", "def test_julia_code_printing_hankel1():\n    expr = hankel1(2, x)\n    expected = 'hankelh1(2, x)'\n    assert julia_code(expr) == expected\n", "def test_julia_code_with_assign_to_and_piecewise():\n    pw = Piecewise((x + 1, x > 0), (x, True))\n    result = julia_code(pw, assign_to=z)\n    assert result == 'z = ((x > 0) ? (x + 1) : (x))'\n", "def test_julia_code_HadamardPower():\n    A, B = MatrixSymbol('A', 3, 3), MatrixSymbol('B', 3, 3)\n    expr = HadamardProduct(A, B)\n    expected_code = 'A .* B'\n    assert julia_code(expr) == expected_code\n"], "sample_176": ["def test_m2m_through_removed(self):\n    changes = self.get_changes([\n        self.author_with_m2m_through,\n        self.contract,\n        self.publisher,\n    ], [\n        self.author_with_former_m2m,\n        self.contract,\n        self.publisher,\n    ])\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"RemoveField\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"publishers\")\n", "def test_deconstructible_objects(self):\n    \"\"\"\n    Test that deconstructible objects are compared correctly.\n    \"\"\"\n    before_states = [\n        self.author_name_deconstructible_1,\n        self.author_name_deconstructible_list_1,\n        self.author_name_deconstructible_tuple_1,\n        self.author_name_deconstructible_dict_1,\n        self.author_name_nested_deconstructible_1,\n    ]\n    after_states = [\n        self.author_name_deconstructible_2,\n        self.author_name_deconstructible_list_2,\n        self.author_name_deconstructible_tuple_2,\n        self.author_name_deconstructible_dict_2,\n        self.author_name_nested_deconstructible_2,\n    ]\n    changes = self.get_changes(before_states, after_states)\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterField\", \"AlterField\", \"AlterField\", \"AlterField\", \"AlterField\"])\n\n    after_states = [\n        self.author_name_deconstructible_3,\n        self.author_name_deconstructible_list_3,\n        self.author_name_deconstructible_tuple_3,\n        self.author_name_deconstructible_dict_3,\n        self.author_name_nested_deconstructible_changed_arg,\n    ]\n    changes = self.get_changes(before_states, after_states)\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterField\", \"AlterField\", \"AlterField\", \"AlterField\", \"AlterField\"])\n\n    after_states = [\n        self.author_name_nested_deconstructible_extra_arg,\n    ]\n    changes = self.get_changes(before_states, after_states)\n    self.assertNumberMigr", "def test_unmigrated_models(self):\n    changes = self.get_changes(\n        [self.author_empty, self.book_migrations_fk],\n        [self.author_empty, self.book_migrations_fk],\n    )\n    self.assertNumberMigrations(changes, \"otherapp\", 1)\n    self.assertMigrationDependencies(changes, \"otherapp\", 0, [])\n    self.assertOperationTypes(changes, \"otherapp\", 0, [\"AddField\", \"RemoveField\"])\n", "def test_generate_altered_managers_with_deconstructible_objects(self):\n    author_state_1 = ModelState(\"testapp\", \"Author\", [], managers=[('food_mgr', FoodManager('a', 'b'))])\n    author_state_2 = ModelState(\"testapp\", \"Author\", [], managers=[('food_mgr', FoodManager('x', 'y'))])\n    changes = self.get_changes([author_state_1], [author_state_2])\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['AlterModelManagers'])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name='Author', managers=[('food_mgr', FoodManager('x', 'y'))])\n", "def test_added_check_constraint(self):\n    before_states = [self.author_empty]\n    after_states = [self.author_name_check_constraint]\n    changes = self.get_changes(before_states, after_states)\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['AddField'])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name='name')\n    self.assertOperationFieldAttributes(changes, 'testapp', 0, 0, max_length=200)\n    self.assertOperationTypes(changes, 'testapp', 0, ['AddConstraint'])\n    self.assertOperationAttributes(changes, 'testapp', 0, 1, name='name_contains_bob')\n"], "sample_805": ["def test_mean_tweedie_deviance():\n    y_true = np.array([2, 0, 1, 4])\n    y_pred = np.array([0.5, 0.5, 2., 2.])\n    assert_almost_equal(mean_tweedie_deviance(y_true, y_pred, p=1), 1.4260, decimal=4)\n", "def test_mean_tweedie_deviance_errors():\n    y_true = [2, 0, 1, 4]\n    y_pred = [0.5, -0.5, 2., 2.]\n\n    with pytest.raises(ValueError, match=\"Mean Tweedie deviance error with p=0 can only be used on strictly positive y_pred.\"):\n        mean_tweedie_deviance(y_true, y_pred, p=0)\n\n    y_true = [-2, 0, 1, 4]\n    y_pred = [0.5, 0.5, 2., 2.]\n\n    with pytest.raises(ValueError, match=\"Mean Tweedie deviance error with p=1 can only be used on non-negative y_true and strictly positive y_pred.\"):\n        mean_tweedie_deviance(y_true, y_pred, p=1)\n\n    y_true = [2, 0, 1, 4]\n    y_pred = [0.5, 0.5, 0., 2.]\n\n    with pytest.raises(ValueError, match=\"Mean Tweedie deviance error with p=1 can only be used on non-negative y_true and strictly positive y_pred.\"):\n        mean_tweedie_deviance(y_true, y_pred, p=1)\n\n    y_true = [2, 0, 1, 4]\n    y_pred = [0.5, 0.5, 2., 2.]\n\n    with pytest.raises(ValueError, match=\"Mean Tweedie deviance error with p=2 can only be used on strictly positive y_true and y_pred.\"):\n        mean_tweedie_deviance(y_true, y_pred, p=2)\n\n    y_true = [2, 0, 1, 4]\n    y_pred = [0.5, 0.5, 2., -2.]\n\n    with pytest.raises(ValueError, match=\"Me", "def test_mean_tweedie_deviance_errors():\n    y_true = [1, 2, 3, 4]\n    y_pred = [0.5, 1.5, 2.5, 3.5]\n\n    # Test with p < 0\n    with assert_raises_regex(ValueError, \"strictly positive y_pred\"):\n        mean_tweedie_deviance(y_true, y_pred, p=-1)\n\n    # Test with p > 2 and y_true <= 0\n    y_true = [1, 2, 0, 4]\n    with assert_raises_regex(ValueError, \"strictly positive y_true and y_pred\"):\n        mean_tweedie_deviance(y_true, y_pred, p=3)\n\n    # Test with 1 < p < 2 and y_true < 0\n    y_true = [1, 2, -1, 4]\n    with assert_raises_regex(ValueError, \"non-negative y_true and strictly positive y_pred\"):\n        mean_tweedie_deviance(y_true, y_pred, p=1.5)\n", "def test_mean_tweedie_deviance_errors():\n    y_true = np.array([2, 0, 1, 4])\n    y_pred = np.array([0, 0.5, 2., 2.])\n    assert_raises(ValueError, mean_tweedie_deviance, y_true, y_pred, p=-0.5)\n    assert_raises(ValueError, mean_tweedie_deviance, y_true, y_pred, p=1.5)\n    assert_raises(ValueError, mean_tweedie_deviance, y_true, y_pred, p=3)\n    y_pred = np.array([0.5, 0.5, 2., 2.])\n    assert_raises(ValueError, mean_tweedie_deviance, y_true, y_pred, p=1)\n    y_true = np.array([2, 0.5, 1, 4])\n    assert_raises(ValueError, mean_tweedie_deviance, y_true, y_pred, p=2)\n", "def test_mean_tweedie_deviance_p_values():\n    y_true = np.array([2, 0, 1, 4])\n    y_pred = np.array([0.5, 0.5, 2., 2.])\n\n    # Test different values of p\n    assert_allclose(mean_tweedie_deviance(y_true, y_pred, p=-1), 1.4260, rtol=1e-4)\n    assert_allclose(mean_tweedie_deviance(y_true, y_pred, p=0), 1.375, rtol=1e-4)\n    assert_allclose(mean_tweedie_deviance(y_true, y_pred, p=1), 1.4260, rtol=1e-4)\n    assert_allclose(mean_tweedie_deviance(y_true, y_pred, p=2), 1.0568, rtol=1e-4)\n    assert_allclose(mean_tweedie_deviance(y_true, y_pred, p=3), 1.0284, rtol=1e-4)\n\n    # Test error messages for invalid p values\n    assert_raises(ValueError, mean_tweedie_deviance, y_true, y_pred, p=-2)\n    assert_raises(ValueError, mean_tweedie_deviance, y_true, y_pred, p=0.5)\n"], "sample_885": ["def test_generate_invalid_param_val_interval(interval, constraints, expected):\n    result = _generate_invalid_param_val_interval(interval, constraints)\n    assert result == expected\n", "def test_interval_contains(interval_type, interval_closed):\n    interval = Interval(interval_type, 0, 10, closed=interval_closed)\n    if interval_closed in (\"left\", \"both\"):\n        assert 0 in interval\n    else:\n        assert 0 not in interval\n    if interval_closed in (\"right\", \"both\"):\n        assert 10 in interval\n    else:\n        assert 10 not in interval\n    if interval_type is Integral:\n        assert 5.5 not in interval\n    else:\n        assert 5.5 in interval\n", "def test_interval_right_closed_invalid():\n    interval = Interval(interval_type, 0, 1, closed=\"right\")\n    assert not interval.is_satisfied_by(1)\n    assert not interval.is_satisfied_by(1.0)\n", "def test_generate_invalid_param_val_for_interval_with_real_not_int_constraint(interval_type):\n    # Given\n    constraint = Interval(\"real_not_int\", None, None, closed=\"both\")\n\n    # When\n    invalid_value = generate_invalid_param_val(constraint)\n\n    # Then\n    assert isinstance(invalid_value, Integral)\n", "def test_interval_constraint_with_closed_right():\n    interval = Interval(Real, 0, 1, closed=\"right\")\n    assert 0 not in interval\n    assert 1 in interval\n    assert 2 not in interval\n    assert -1 not in interval\n    assert 0.5 in interval\n    assert 1.0 in interval\n    assert np.nan not in interval\n    assert np.inf not in interval\n    assert -np.inf not in interval\n"], "sample_168": ["def test_remove_stale_content_types_interactive(self, mock_input):\n    mock_input.return_value = 'yes'\n    with captured_stdout() as stdout:\n        call_command('remove_stale_contenttypes')\n    output = stdout.getvalue()\n    self.assertIn(\"Some content types in your database are stale and can be deleted.\", output)\n    self.assertIn(\"Type 'yes' to continue, or 'no' to cancel:\", output)\n    self.assertEqual(ContentType.objects.count(), self.before_count)\n", "    def test_remove_stale_content_types_with_confirmation(self):\n        with mock.patch('builtins.input', return_value='yes'):\n            with captured_stdout() as stdout:\n                call_command('remove_stale_contenttypes', '--noinput', '--include-stale-apps')\n                output = stdout.getvalue()\n                self.assertIn(\"Are you sure you want to delete these content types?\", output)\n                self.assertIn(\"Type 'yes' to continue, or 'no' to cancel: \", output)\n                self.assertNotIn(\"Stale content types remain.\", output)\n        self.assertEqual(ContentType.objects.count(), self.before_count)\n", "    def test_stale_content_types_with_include_stale_apps(self):\n        with captured_stdout() as stdout:\n            call_command('remove_stale_contenttypes', include_stale_apps=True, interactive=False)\n        self.assertIn('Deleting stale content type', stdout.getvalue())\n        self.assertEqual(ContentType.objects.count(), self.before_count)\n", "    def test_no_input_option(self):\n        \"\"\"\n        Test the --noinput option which should not prompt for confirmation.\n        \"\"\"\n        with mock.patch('builtins.input', return_value=\"yes\"):\n            with captured_stdout() as stdout:\n                call_command('remove_stale_contenttypes', '--noinput')\n        self.assertIn(\"Deleting stale content type\", stdout.getvalue())\n        self.assertNotEqual(self.before_count, ContentType.objects.count())\n", "    def test_stale_content_types_deletion(self):\n        # Create a stale content type\n        stale_content_type = ContentType.objects.create(app_label='no_models', model='StaleModel')\n\n        # Create a dependent object for the stale content type\n        dependent_object = ModelWithNullFKToSite.objects.create()\n        dependent_object.content_type = stale_content_type\n        dependent_object.save()\n\n        # Run the remove_stale_contenttypes command with interactive mode off\n        with mock.patch('builtins.input', return_value='yes'):\n            call_command('remove_stale_contenttypes', interactive=False, verbosity=2)\n\n        # Check if the stale content type and dependent object are deleted\n        with self.assertRaises(ContentType.DoesNotExist):\n            ContentType.objects.get(pk=stale_content_type.pk)\n\n        with self.assertRaises(ModelWithNullFKToSite.DoesNotExist):\n            ModelWithNullFKToSite.objects.get(pk=dependent_object.pk)\n\n        # Check if the number of content types is reduced by 1\n        self.assertEqual(ContentType.objects.count(), self.before_count)\n"], "sample_348": ["    def test_model_form_meta_fields_invalid_string(self):\n        class InvalidModelAdmin(ModelAdmin):\n            model = Band\n            fields = \"name\"\n\n        self.assertIsInvalid(\n            InvalidModelAdmin, Band,\n            \"BandAdmin.Meta.fields cannot be a string. Did you mean to type: ('name',)?\"\n        )\n", "def test_modelform_defines_fields_check(self):\n    class TestModelAdmin(ModelAdmin):\n        modelform_defines_fields = True\n\n    class TestModel(Model):\n        pass\n\n    self.assertIsInvalid(\n        TestModelAdmin,\n        TestModel,\n        msg=\"The value of 'modelform_defines_fields' must not be True.\",\n        id='admin.E008',\n        hint='Set it to False or remove the attribute altogether.',\n    )\n\n    # Test valid case\n    class ValidTestModelAdmin(ModelAdmin):\n        pass\n\n    self.assertIsValid(ValidTestModelAdmin, TestModel)\n", "    def test_exclude_fields(self):\n        class ExcludeTestModel(Model):\n            field1 = Field()\n            field2 = Field()\n            field3 = Field()\n\n        class ExcludeTestForm(forms.ModelForm):\n            class Meta:\n                model = ExcludeTestModel\n                exclude = ['field2']\n\n        form = ExcludeTestForm()\n        self.assertNotIn('field2', form.fields)\n        self.assertIn('field1', form.fields)\n        self.assertIn('field3', form.fields)\n", "    def test_model_formset_with_undefined_fields(self):\n        class InvalidModelAdmin(ModelAdmin):\n            model = Band\n            fields = ['name']\n\n                class CustomModelFormSet(BaseModelFormSet):\n                    fields = ('undefined_field',)\n                return CustomModelFormSet\n\n        msg = \"The model form 'admin.forms.CustomModelFormSet' used in BandAdmin \" \\\n              \"has fields that are not defined in its model 'forms.Band'.\"\n        self.assertIsInvalidRegexp(InvalidModelAdmin, Band, msg, id='admin.E210')\n", "    def test_raw_id_field_without_admin_in_admin_readonly_fields(self):\n        class BandAdmin(admin.ModelAdmin):\n            raw_id_fields = ('singer',)\n            readonly_fields = ('singer',)\n\n        self.assertIsInvalid(\n            BandAdmin, Band,\n            \"The value of 'singer' field in raw_id_fields is present in readonly_fields.\",\n            id='admin.E136',\n            hint=(\"remove 'singer' from 'readonly_fields' or 'raw_id_fields' \"\n                  \"in class BandAdmin.\"),\n        )\n"], "sample_377": ["    def test_get_traceback_frames(self):\n        \"\"\"Test the get_traceback_frames method of ExceptionReporter\"\"\"\n        try:\n            raise Exception(\"Test exception\")\n        except Exception as e:\n            request = HttpRequest()\n            reporter = ExceptionReporter(request, type(e), e, e.__traceback__)\n            frames = reporter.get_traceback_frames()\n            self.assertIsInstance(frames, list)\n            self.assertTrue(all(isinstance(frame, dict) for frame in frames))\n            # Add more assertions based on the expected output\n", "    def test_get_traceback_data(self):\n        # Create a mock request\n        request = HttpRequest()\n        request.user = User()\n\n        # Create a mock exception\n        exc_type, exc_value, tb = sys.exc_info()\n\n        # Create an ExceptionReporter instance\n        reporter = ExceptionReporter(request, exc_type, exc_value, tb)\n\n        # Call the get_traceback_data method\n        data = reporter.get_traceback_data()\n\n        # Assert that the data dictionary contains expected keys\n        self.assertIn('is_email', data)\n        self.assertIn('unicode_hint', data)\n        self.assertIn('frames', data)\n        self.assertIn('request', data)\n        self.assertIn('request_meta', data)\n        self.assertIn('request_COOKIES_items', data)\n        self.assertIn('user_str', data)\n        self.assertIn('filtered_POST_items', data)\n        self.assertIn('settings', data)\n        self.assertIn('sys_executable', data)\n        self.assertIn('sys_version_info', data)\n        self.assertIn('server_time', data)\n        self.assertIn('django_version_info', data)\n        self.assertIn('sys_path', data)\n        self.assertIn('template_info', data)\n        self.assertIn('template_does_not_exist', data)\n        self.assertIn('postmortem', data)\n        self.assertIn('request_GET_items', data)\n        self.assertIn('request_FILES_items', data)\n        self.assertIn('request_insecure_uri', data)\n        self.assertIn('raising_view_name', data)\n        self.assertIn('exception_type', data)\n        self.assertIn('exception_value', data)\n        self.assertIn('lastframe', data)\n", "    def test_get_traceback_data(self):\n        request = HttpRequest()\n        request.user = User()\n        exception = Exception(\"Test Exception\")\n        type_, value, traceback = sys.exc_info()\n        reporter = ExceptionReporter(request, type_, value, traceback)\n        data = reporter.get_traceback_data()\n        self.assertEqual(data['exception_type'], 'Exception')\n        self.assertEqual(data['exception_value'], 'Test Exception')\n        self.assertEqual(data['user_str'], 'jacob')\n", "    def test_get_traceback_data(self):\n        request = HttpRequest()\n        request.user = User()\n        request.GET = {\"get_key\": \"get_value\"}\n        request.FILES = {\"file_key\": SimpleUploadedFile(\"file.txt\", b\"file_content\")}\n        request.POST = {\"post_key\": \"post_value\"}\n        request.META = {\"META_key\": \"META_value\"}\n        request.COOKIES = {\"cookie_key\": \"cookie_value\"}\n        request.path = \"/test_path/\"\n        request.resolver_match = mock.Mock()\n        request.resolver_match._func_path = \"test_func_path\"\n        request.sensitive_post_parameters = [\"post_key\"]\n\n        exc_type = ValueError\n        exc_value = ValueError(\"Test error\")\n        tb = sys.exc_info()[2]\n\n        reporter = ExceptionReporter(request, exc_type, exc_value, tb)\n        data = reporter.get_traceback_data()\n\n        # Verify that the data contains the expected information\n        self.assertEqual(data[\"request\"], request)\n        self.assertEqual(data[\"user_str\"], str(request.user))\n        self.assertEqual(data[\"filtered_POST_items\"], [(\"post_key\", \"********************\")])\n        self.assertEqual(data[\"request_GET_items\"], request.GET.items())\n        self.assertEqual(data[\"request_FILES_items\"], request.FILES.items())\n        self.assertEqual(data[\"raising_view_name\"], \"test_func_path\")\n        self.assertEqual(data[\"exception_type\"], exc_type.__name__)\n        self.assertEqual(data[\"exception_value\"], str(exc_value))\n", "    def test_get_traceback_frames_handles_cycles(self):\n        # Create a cycle in the exception chain\n        exc1 = Exception(\"Exception 1\")\n        exc2 = Exception(\"Exception 2\")\n        exc1.__cause__ = exc2\n        exc2.__cause__ = exc1\n\n        request = HttpRequest()\n        reporter = ExceptionReporter(request, type(exc1), exc1, exc1.__traceback__)\n\n        with self.assertWarns(ExceptionCycleWarning):\n            frames = reporter.get_traceback_frames()\n\n        # Check that the cycle is detected and only one instance of the exception is included in the frames\n        self.assertEqual(len(frames), 2)  # Each exception should have one frame\n        self.assertIs(frames[0]['exc_cause'], exc2)\n        self.assertIsNone(frames[1]['exc_cause'])\n"], "sample_1043": ["def test_mcode_print_derivative():\n    expr = Derivative(sin(x), x)\n    assert mcode(expr) == \"Hold[D[Sin[x], x]]\"\n", "def test_mathematica_code():\n    assert mcode(Integral(sin(x), (x, 0, pi))) == \"Hold[Integrate[sin[x], {x, 0, Pi}]]\"\n    assert mcode(Sum(x**2, (x, 1, 10))) == \"Hold[Sum[x^2, {x, 1, 10}]]\"\n    assert mcode(Derivative(f(x, y), x, y)) == \"Hold[D[f[x, y], x, y]]\"\n    assert mcode(Max(x, y, z)) == \"Max[x, y, z]\"\n    assert mcode(Min(x, y, z)) == \"Min[x, y, z]\"\n", "def test_mathematica_code():\n    assert mcode(exp(x)) == \"E^x\"\n    assert mcode(sin(x)) == \"Sin[x]\"\n    assert mcode(cos(x)) == \"Cos[x]\"\n    assert mcode(conjugate(x)) == \"Conjugate[x]\"\n    assert mcode(Max(x, y)) == \"Max[x, y]\"\n    assert mcode(Min(x, y)) == \"Min[x, y]\"\n    assert mcode(Integral(f(x), x)) == \"Hold[Integrate[f[x], x]]\"\n    assert mcode(Sum(x**y, (y, 0, z))) == \"Hold[Sum[x**y, y, 0, z]]\"\n    assert mcode(Derivative(f(x, y), x, y)) == \"Hold[D[f[x, y], x, y]]\"\n", "def test_derivative():\n    assert mcode(Derivative(sin(x), x)) == \"Hold[D[Sin[x], x]]\"\n    assert mcode(Derivative(f(x), x, 2)) == \"Hold[D[f[x], {x, 2}]]\"\n", "def test_print_derivative():\n    expr = Derivative(sin(x), x, 2) + Derivative(cos(x), x, 3)\n    assert mcode(expr) == \"Hold[D[sin[x], x, 2] + D[cos[x], x, 3]]\"\n"], "sample_1123": ["def test_condition_set_subs():\n    c = ConditionSet(x, x < 1, {x, z})\n    assert c.subs(x, y) == ConditionSet(x, x < 1, FiniteSet(y, z))\n    assert c.subs(x, y).subs(x, y) == ConditionSet(y, y < 1, FiniteSet(y, z))\n    raises(ValueError, lambda: c.subs(x, 1))\n    assert c.subs(x, 1) == ConditionSet(x, x < 1, FiniteSet(z))\n", "def test_condition_set_subs():\n    c = ConditionSet(x, x < 1, {x, z})\n    assert c.subs(x, y) == ConditionSet(x, x < 1, FiniteSet(y, z))\n    assert c.subs(x, y).subs(x, y) == ConditionSet(y, y < 1, FiniteSet(y, z))\n    raises(ValueError, lambda: c.subs(y, 1))\n    assert c.subs(y, 1) == c\n", "def test_condition_set_subs():\n    c = ConditionSet(x, x < 1, {x, z})\n    assert c.subs(x, y) == ConditionSet(x, x < 1, FiniteSet(y, z))\n    assert c.subs(x, y).subs(x, w) == ConditionSet(w, w < 1, FiniteSet(y, z))\n    assert ConditionSet(y, y < 1, {y, z}).subs(y, 1) == ConditionSet(y, y < 1, FiniteSet(z))\n", "def test_condition_set_union():\n    c1 = ConditionSet(x, x > 0, Interval(-1, 1))\n    c2 = ConditionSet(x, x < 0, Interval(-1, 1))\n    union_set = Union(c1, c2)\n    assert union_set == Interval(-1, 1)\n", "def test_subs_method():\n    c = ConditionSet(x, x < 1, {x, z})\n    assert c.subs(x, y) == ConditionSet(x, x < 1, FiniteSet(y, z))\n    assert c.subs(x, y).subs(x, y) == ConditionSet(y, y < 1, FiniteSet(y, z))\n"], "sample_1107": ["def test_rotations():\n    assert list(rotations([1, 2, 3])) == [[1, 2, 3], [2, 3, 1], [3, 1, 2]]\n    assert list(rotations([1, 2, 3], -1)) == [[1, 2, 3], [3, 1, 2], [2, 3, 1]]\n", "def test_permute_signs():\n    assert list(permute_signs((0, 1, 2))) == [(0, 1, 2), (0, -1, 2), (0, 1, -2), (0, -1, -2)]\n", "def test_rotations():\n    seq = [1, 2, 3]\n    result = list(rotations(seq))\n    expected = [[1, 2, 3], [2, 3, 1], [3, 1, 2]]\n    assert result == expected\n\n    result = list(rotations(seq, -1))\n    expected = [[1, 2, 3], [3, 1, 2], [2, 3, 1]]\n    assert result == expected\n", "def test_rotate_left():\n    assert rotate_left([0, 1, 2], 1) == [1, 2, 0]\n", "def test_rotate_right():\n    a = [0, 1, 2]\n    assert rotate_right(a, 1) == [2, 0, 1]\n    assert rotate_right(a, 0) == a\n    assert rotate_right(a, 2) == [1, 2, 0]\n    assert rotate_right(a, 3) == a\n    assert rotate_right(a, 4) == [2, 0, 1]\n    assert rotate_right([]) == []\n"], "sample_129": ["def test_floatformat_with_args(self):\n    self.engine.assert_output(self.template.render({'a': Decimal('34.23234'), 'b': Decimal('34.00000')}), '34.232 34')\n    self.engine.assert_output(self.template.render({'a': Decimal('34.26000'), 'b': Decimal('34.12345')}), '34.260 34.123')\n", "def test_floatformat02(self):\n    self.output = '34.232'\n    self.context = {'c': Decimal('34.23234')}\n", "    def test_floatformat02(self):\n        self.engine.render_to_string('floatformat02', {'c': Decimal('34.26000')})\n", "def test_floatformat02(self):\n    self.assertEqual(self.engine.render_to_string('floatformat02', {'c': Decimal('34.23234')}), '34.232 ')\n    self.assertEqual(self.engine.render_to_string('floatformat02', {'c': Decimal('34.00000')}), '34 ')\n    self.assertEqual(self.engine.render_to_string('floatformat02', {'c': Decimal('34.26000')}), '34.26 ')\n", "def test_floatformat_with_args(self):\n    self.engine.assert_output(\n        self.t('floatformat02'),\n        context={\n            'c': Decimal('34.23234'),\n            'd': Decimal('34.00000'),\n            'e': Decimal('34.26000'),\n            'f': 'non-numeric',\n            'arg': '3',\n        },\n        expected_output='34.232 34.000 non-numeric non-numeric'\n    )\n"], "sample_189": ["    def setUp(self):\n        self.cache = BaseCache({'TIMEOUT': 60})\n", "    def setUp(self):\n        self.cache = BaseCache({})\n", "    def setUp(self):\n        self.cache = caches['default']\n", "    def setUp(self):\n        self.cache = caches[DEFAULT_CACHE_ALIAS]\n", "    def test_add_get_set_delete(self):\n        # Test adding, getting, setting, and deleting a key-value pair in the cache\n        cache.set('test_key', 'test_value')\n        self.assertEqual(cache.get('test_key'), 'test_value')\n\n        cache.set('test_key', 'new_value')\n        self.assertEqual(cache.get('test_key'), 'new_value')\n\n        cache.delete('test_key')\n        self.assertIsNone(cache.get('test_key'))\n\n        # Test adding a key-value pair that doesn't exist in the cache\n        self.assertTrue(cache.add('test_key', 'test_value'))\n        self.assertEqual(cache.get('test_key'), 'test_value')\n\n        # Test adding a key-value pair that already exists in the cache\n        self.assertFalse(cache.add('test_key', 'new_value'))\n        self.assertEqual(cache.get('test_key'), 'test_value')\n"], "sample_136": ["def test_get_signed_cookie_with_valid_salt(self):\n    factory = RequestFactory()\n    request = factory.get('/')\n    request.COOKIES['test_cookie'] = signing.get_cookie_signer(salt='testsalt').sign('test_value')\n    self.assertEqual(request.get_signed_cookie('test_cookie', salt='testsalt'), 'test_value')\n", "def test_get_host_with_allowed_hosts(self):\n    r = WSGIRequest({\n        'HTTP_HOST': 'example.com',\n        'SERVER_NAME': 'testserver',\n        'SERVER_PORT': '80',\n    })\n    self.assertEqual(r.get_host(), 'example.com')\n", "    def test_get_raw_uri(self):\n        request = HttpRequest()\n        request.META = {'SERVER_NAME': 'testserver', 'SERVER_PORT': '80'}\n        request.path = '/test/'\n        self.assertEqual(request.get_raw_uri(), 'http://testserver/test/')\n\n        request.META['SERVER_PORT'] = '8000'\n        self.assertEqual(request.get_raw_uri(), 'http://testserver:8000/test/')\n\n        request.META['HTTP_X_FORWARDED_HOST'] = 'testforwardedserver'\n        self.assertEqual(request.get_raw_uri(), 'http://testforwardedserver/test/')\n", "def test_build_absolute_uri(self):\n    # Test building absolute URIs\n    request = HttpRequest()\n    request.method = \"GET\"\n    request.META = {\n        'SERVER_NAME': 'testserver',\n        'SERVER_PORT': '80',\n        'QUERY_STRING': urlencode({'foo': 'bar'}),\n    }\n    request.path = '/test/'\n\n    # Test with default scheme\n    self.assertEqual(request.build_absolute_uri(), 'http://testserver/test/?foo=bar')\n\n    # Test with specified location\n    self.assertEqual(request.build_absolute_uri('/new/'), 'http://testserver/new/')\n    self.assertEqual(request.build_absolute_uri('http://example.com/'), 'http://example.com/')\n\n    # Test with secure scheme\n    request.META['SERVER_PORT'] = '443'\n    self.assertEqual(request.build_absolute_uri(), 'https://testserver/test/?foo=bar')\n", "    def test_build_absolute_uri(self):\n        # Test building absolute URI from the request variables\n        request = HttpRequest()\n        request.META = {'SERVER_NAME': 'testserver', 'SERVER_PORT': '80', 'REQUEST_METHOD': 'GET'}\n        request.path = '/path/'\n        request.GET = {'key': 'value'}\n\n        # Test with no location\n        self.assertEqual(request.build_absolute_uri(), 'http://testserver/path/?key=value')\n\n        # Test with absolute location\n        self.assertEqual(request.build_absolute_uri('https://example.com/location/'), 'https://example.com/location/')\n\n        # Test with relative location\n        self.assertEqual(request.build_absolute_uri('/location/'), 'http://testserver/location/')\n\n        # Test with scheme-relative location\n        self.assertEqual(request.build_absolute_uri('//example.com/location/'), 'http://example.com/location/')\n"], "sample_447": ["    def test_expression_wrapper_with_output_field(self):\n        books = Book.objects.annotate(\n            upper_name=ExpressionWrapper(Upper(\"name\"), output_field=CharField())\n        )\n        self.assertEqual(books[0].upper_name, \"THE DEFINITIVE GUIDE TO DJANGO: WEB DEVELOPMENT DONE RIGHT\")\n", "    def test_raw_sql_annotation(self):\n        query = Author.objects.annotate(num_books=RawSQL(\"SELECT COUNT(*) FROM books_book WHERE books_book.contact_id = authors_author.id\", ()))\n        self.assertEqual(query.get(name=\"Adrian Holovaty\").num_books, 1)\n", "def test_expression_wrapper(self):\n    author = Author.objects.annotate(lower_name=Lower(F(\"name\"))).get(id=self.a1.id)\n    self.assertEqual(author.lower_name, \"adrian holovaty\")\n", "    def setUpTestData(cls):\n        cls.book = Book.objects.create(\n            name=\"Python Web Development with Django\",\n            pages=350,\n        )\n", "    def test_expression_wrapper_annotation(self):\n        authors = Author.objects.annotate(\n            lower_name=ExpressionWrapper(Lower('name'), output_field=CharField())\n        )\n        self.assertEqual(authors.first().lower_name, 'adrian holovaty')\n"], "sample_80": ["def test_add_filter_with_related_objects(self):\n    query = Query(Author)\n    query.add_filter(('items__name', 'Test Item'))\n    self.assertEqual(len(query.where.children), 1)\n    condition = query.where.children[0]\n    self.assertIsInstance(condition, Exact)\n    self.assertIsInstance(condition.lhs, SimpleCol)\n    self.assertEqual(condition.lhs.field.name, 'name')\n    self.assertEqual(condition.rhs, 'Test Item')\n", "def test_add_filter_with_related_isnull(self):\n    query = Query(Author)\n    query.add_filter(('items__isnull', True))\n    self.assertEqual(len(query.where.children), 1)\n    condition = query.where.children[0]\n    self.assertIsInstance(condition, RelatedIsNull)\n    self.assertEqual(condition.lhs, SimpleCol('book_set', 'author_id'))\n    self.assertEqual(condition.rhs, True)\n", "def test_build_filter_with_related_isnull(self):\n    query = Query(Author)\n    filter_expr = ('friend__isnull', True)\n    clause, _ = query.build_filter(filter_expr)\n    self.assertEqual(clause.children[0].__class__, RelatedIsNull)\n    self.assertEqual(clause.children[0].lhs.field.name, 'friend')\n    self.assertEqual(clause.children[0].rhs, True)\n", "def test_join_promotion_with_multiple_filters(self):\n    query = Query(Author)\n    query.add_q(Q(items__name__icontains='Test') | Q(items__price__gt=10))\n    query.add_q(Q(items__name__gte='A') & Q(items__price__lt=20))\n    query.join_promoter.update_join_types(query)\n    # Verify that the join to items has been promoted to INNER\n    self.assertEqual(query.alias_map['items'].join_type, 'INNER')\n", "def test_build_filter_with_complex_query(self):\n    query = Query(Item)\n    filter_expr = (Q(author__name__icontains='test') | Q(ranking__rank__gt=5), 'foo')\n    clause, used_joins = query.build_filter(filter_expr)\n    self.assertIsInstance(clause.children[0], Lower)\n    self.assertIsInstance(clause.children[0].lhs, SimpleCol)\n    self.assertIsInstance(clause.children[0].rhs, str)\n    self.assertIsInstance(clause.children[1], GreaterThan)\n    self.assertIsInstance(clause.children[1].lhs, SimpleCol)\n    self.assertIsInstance(clause.children[1].rhs, int)\n    self.assertIn('author', used_joins)\n    self.assertIn('ranking', used_joins)\n"], "sample_257": ["    def test_key_transform_in(self):\n        obj = JSONModel.objects.create(json_field={'key': 'value'})\n        self.assertEqual(JSONModel.objects.filter(json_field__key__in=['value']).count(), 1)\n        self.assertEqual(JSONModel.objects.filter(json_field__key__in=['other']).count(), 0)\n", "    def test_key_transform_exact_lookup(self):\n        instance = JSONModel.objects.create(json=\"{\\\"key\\\": \\\"value\\\"}\")\n        self.assertEqual(JSONModel.objects.filter(json__key__exact=\"value\").count(), 1)\n        self.assertEqual(JSONModel.objects.filter(json__key__exact=\"invalid\").count(), 0)\n", "    def test_key_transform_lookup_mixin(self):\n        # Test KeyTransformTextLookupMixin\n        obj = JSONModel.objects.create(json='{\"key\": \"value\"}')\n        key_transform = KeyTransform('key', obj._meta.get_field('json'))\n        lookup_mixin = KeyTransformTextLookupMixin(key_transform, 'value')\n        self.assertEqual(lookup_mixin.process_lhs(connection.cursor(), connection), ('LOWER((json #>> ARRAY[\\'key\\']))', ()))\n        self.assertEqual(lookup_mixin.process_rhs(connection.cursor(), connection), ('LOWER(%s)', ('value',)))\n\n        # Test CaseInsensitiveMixin\n        key_transform = KeyTransform('key', obj._meta.get_field('json'))\n        case_insensitive_mixin = CaseInsensitiveMixin(key_transform, 'VALUE')\n        self.assertEqual(case_insensitive_mixin.process_lhs(connection.cursor(), connection), ('LOWER((json #>> ARRAY[\\'key\\']))', ()))\n        self.assertEqual(case_insensitive_mixin.process_rhs(connection.cursor(), connection), ('LOWER(%s)', ('VALUE',)))\n", "def test_key_transform_lookup_mixin(self):\n    key_transform = KeyTransform('key', OutputField())\n    lookup = KeyTransformTextLookupMixin('lookup', key_transform, 'value')\n    self.assertIsInstance(lookup.lhs, KeyTextTransform)\n    self.assertEqual(lookup.lhs.key_name, 'key')\n", "def test_key_transform_numeric_lookups(self):\n    JSONModel.objects.create(json_field={'int_key': 42, 'float_key': 3.14})\n    self.assertEqual(JSONModel.objects.filter(json_field__int_key__gt=40).count(), 1)\n    self.assertEqual(JSONModel.objects.filter(json_field__int_key__lt=45).count(), 1)\n    self.assertEqual(JSONModel.objects.filter(json_field__float_key__gt=3.10).count(), 1)\n    self.assertEqual(JSONModel.objects.filter(json_field__float_key__lt=3.20).count(), 1)\n"], "sample_1030": ["def test_idiff():\n    x, y, a = symbols('x y a')\n    eq = x**2 + a*y - 4\n    assert idiff(eq, y, x) == -x/(a*y)\n", "def test_idiff():\n    x = Symbol('x')\n    y = Symbol('y')\n    a = Symbol('a')\n    eq = x**2 + y**2 - 4\n    assert idiff(eq, y, x) == -x/y\n    assert idiff(eq, y, x, 2).simplify() == -(x**2 + y**2)/y**3\n    eq = x + a + y\n    assert idiff(eq, y, x) == -1\n    assert idiff(eq, [y, a], x) == -Derivative(a, x) - 1\n", "def test_are_coplanar():\n    a = Point3D(0, 0, 0)\n    b = Point3D(1, 1, 1)\n    c = Point3D(2, 2, 2)\n    d = Point3D(3, 3, 3)\n    e = Point3D(4, 4, 0)\n    assert are_coplanar(a, b, c, d)\n    assert not are_coplanar(a, b, c, e)\n", "def test_idiff():\n    x = Symbol('x')\n    y = Symbol('y')\n    a = Symbol('a')\n    eq = x**2 + y**2 - 4\n    assert idiff(eq, y, x) == -x/y\n    assert idiff(eq, y, x, 2).simplify() == -(x**2 + y**2)/y**3\n    eq = x + a + y\n    assert idiff(eq, y, x) == -1\n    assert idiff(eq, [y, a], x) == -Derivative(a, x) - 1\n", "def test_idiff():\n    x, y, a = symbols('x y a')\n    eq = x + a + y\n    assert idiff(eq, [y, a], x) == -Derivative(a, x) - 1\n"], "sample_895": ["def test_column_transformer_get_feature_names_out_with_transformers_without_get_feature_names_out():\n    class TransNoGetFeatureNamesOut(BaseEstimator, TransformerMixin):\n            return self\n\n            return 2 * X\n\n    transformer = TransNoGetFeatureNamesOut()\n    column_transformer = ColumnTransformer(transformers=[(\"transformer\", transformer, [0])])\n    X = np.array([[1], [2]])\n    column_transformer.fit(X)\n    with pytest.raises(AttributeError, match=\"Transformer transformer .* does not provide get_feature_names_out.\"):\n        column_transformer.get_feature_names_out()\n", "def test_column_transformer_feature_names_out():\n    # Test get_feature_names_out with verbose_feature_names_out=True and False\n    X = np.array([[0, 1, 2, 3], [4, 5, 6, 7]])\n    ct = ColumnTransformer(\n        [(\"trans1\", StandardScaler(), [0, 1]), (\"trans2\", Normalizer(), [2, 3])],\n        verbose_feature_names_out=True,\n    )\n    ct.fit(X)\n    feature_names_out = ct.get_feature_names_out()\n    assert_array_equal(\n        feature_names_out,\n        np.array([\"trans1__0\", \"trans1__1\", \"trans2__0\", \"trans2__1\"]),\n    )\n\n    ct = ColumnTransformer(\n        [(\"trans1\", StandardScaler(), [0, 1]), (\"trans2\", Normalizer(), [2, 3])],\n        verbose_feature_names_out=False,\n    )\n    ct.fit(X)\n    feature_names_out = ct.get_feature_names_out()\n    assert_array_equal(feature_names_out, np.array([0, 1, 2, 3]))\n\n    # Test get_feature_names_out with overlapping feature names\n    ct = ColumnTransformer(\n        [(\"trans1\", OneHotEncoder(), [0]), (\"trans2\", OneHotEncoder(), [1])],\n        verbose_feature_names_out=False,\n    )\n    X = np.array([[1, 2], [3, 4]])\n    with pytest.raises(ValueError, match=\"Output feature names\"):\n        ct.fit(X)\n        ct.get_feature_names_out()\n", "def test_column_transformer_remainder_with_dataframe_inputs():\n    # Test that the remainder works as expected with dataframe inputs.\n    transformer = make_column_transformer(\n        (StandardScaler(), make_column_selector(dtype_include=np.number)),\n        remainder=\"passthrough\",\n    )\n\n    X = pd.DataFrame({\n        'numerical_column': [1, 2, 3],\n        'categorical_column': ['a', 'b', 'c'],\n    })\n\n    transformed = transformer.fit_transform(X)\n    assert transformed.shape == (3, 3)\n    assert_almost_equal(transformed.iloc[:, 0], [-1.22474487, 0.0, 1.22474487])\n    assert_array_equal(transformed.iloc[:, 1:], X.iloc[:, 1:])\n", "def test_transformer_names():\n    \"\"\"Test that transformer names are unique\"\"\"\n    ct = ColumnTransformer(\n        [\n            (\"trans1\", Trans(), [0, 1]),\n            (\"trans2\", Trans(), [2, 3]),\n        ]\n    )\n    with pytest.raises(ValueError, match=\"Transformer names must be unique\"):\n        ct.fit_transform(np.array([[1, 2, 3, 4]]))\n", "def test_column_transformer_remainder_estimator():\n    ct = ColumnTransformer(\n        transformers=[\n            (\"trans\", Trans(), [0]),\n            (\"double\", DoubleTrans(), [1]),\n        ],\n        remainder=StandardScaler(),\n    )\n    X = np.array([[0.0, 1.0, 2.0], [1.0, 2.0, 3.0]])\n    ct.fit(X)\n    X_trans = ct.transform(X)\n    assert_allclose(X_trans[:, 0], [0.0, 1.0])\n    assert_allclose(X_trans[:, 1], [2.0, 4.0])\n    assert_allclose(X_trans[:, 2], [0.0, 1.0])  # remainder transformed\n"], "sample_199": ["    def test_expression_wrapper(self):\n        annotated_query = Author.objects.annotate(lower_name=Lower('name'))\n        annotated_author = annotated_query.get(id=self.a1.id)\n        wrapped_expression = ExpressionWrapper(F('lower_name'), output_field=CharField())\n        wrapped_query = annotated_query.annotate(wrapped_name=wrapped_expression)\n        wrapped_author = wrapped_query.get(id=self.a1.id)\n        self.assertEqual(wrapped_author.wrapped_name, annotated_author.lower_name)\n", "    def test_expression_wrapper(self):\n        # Test ExpressionWrapper with a simple expression\n        expr = ExpressionWrapper(F('age') + 1, output_field=IntegerField())\n        author = Author.objects.annotate(next_age=expr).first()\n        self.assertEqual(author.next_age, author.age + 1)\n\n        # Test ExpressionWrapper with a complex expression\n        expr = ExpressionWrapper(F('price') * F('rating'), output_field=FloatField())\n        book = Book.objects.annotate(score=expr).first()\n        self.assertEqual(book.score, book.price * book.rating)\n", "    def test_expression_wrapper_with_integer_field(self):\n        expression = ExpressionWrapper(F('age') + 1, output_field=IntegerField())\n        author = Author.objects.annotate(age_plus_one=expression).get(name='Adrian Holovaty')\n        self.assertEqual(author.age_plus_one, 35)\n", "    def test_expression_wrapper_query(self):\n        # Test ExpressionWrapper in a QuerySet\n        expression = ExpressionWrapper(F('age') + Value(5), output_field=IntegerField())\n        authors = Author.objects.annotate(future_age=expression)\n        self.assertEqual(authors.get(id=self.a1.id).future_age, self.a1.age + 5)\n", "    def test_rawsql_annotation(self):\n        qs = Book.objects.annotate(doubled_pages=RawSQL(\"2 * %s\", (F('pages'),)))\n        self.assertEqual(qs.first().doubled_pages, self.b1.pages * 2)\n"], "sample_292": ["    def test_origin_verified(self):\n        middleware = CsrfViewMiddleware()\n        request = TestingHttpRequest()\n\n        # Test with matching origin\n        request.META['HTTP_ORIGIN'] = 'http://testserver'\n        self.assertTrue(middleware._origin_verified(request))\n\n        # Test with matching origin for secure connection\n        request._is_secure_override = True\n        request.META['HTTP_ORIGIN'] = 'https://testserver'\n        self.assertTrue(middleware._origin_verified(request))\n\n        # Test with non-matching origin\n        request.META['HTTP_ORIGIN'] = 'http://example.com'\n        self.assertFalse(middleware._origin_verified(request))\n\n        # Test with non-matching origin for secure connection\n        request.META['HTTP_ORIGIN'] = 'http://testserver'\n        self.assertFalse(middleware._origin_verified(request))\n\n        # Test with invalid origin\n        request.META['HTTP_ORIGIN'] = 'invalid_origin'\n        self.assertFalse(middleware._origin_verified(request))\n", "    def _get_GET_csrf_cookie_request(self):\n        req = self._get_GET_no_csrf_cookie_request()\n        req.COOKIES['csrftoken'] = self._csrf_id_cookie\n        return req\n", "    def test_origin_verification(self):\n        req = self._get_POST_request_with_token()\n        req.META['HTTP_ORIGIN'] = 'http://evil.com'\n\n        response = CsrfViewMiddleware().process_view(req, token_view, (), {})\n        self.assertEqual(response.status_code, 403)\n        self.assertEqual(response.reason_phrase, REASON_BAD_ORIGIN % 'http://evil.com')\n", "def test_csrf_protection_for_invalid_origin(self):\n    request = self._get_GET_csrf_cookie_request()\n    request.META['HTTP_ORIGIN'] = 'https://evil.example.com'\n    response = CsrfViewMiddleware().process_view(request, None, (), {})\n    self.assertEqual(response.status_code, 403)\n    self.assertEqual(response.reason_phrase, REASON_BAD_ORIGIN % 'https://evil.example.com')\n", "    def test_csrf_origin_check(self):\n        csrf_middleware = CsrfViewMiddleware()\n        good_origin = 'http://testserver'\n        bad_origin = 'http://evil.com'\n\n        # Test that the middleware accepts a request with a good origin\n        request = self._get_POST_request_with_token()\n        request.META['HTTP_ORIGIN'] = good_origin\n        response = csrf_middleware.process_view(request, None, (), {})\n        self.assertIsNone(response)\n\n        # Test that the middleware rejects a request with a bad origin\n        request = self._get_POST_request_with_token()\n        request.META['HTTP_ORIGIN'] = bad_origin\n        response = csrf_middleware.process_view(request, None, (), {})\n        self.assertEqual(response.reason_phrase, REASON_BAD_ORIGIN % bad_origin)\n\n        # Test that the middleware accepts a request with no origin header when not secure\n        request = self._get_POST_request_with_token()\n        response = csrf_middleware.process_view(request, None, (), {})\n        self.assertIsNone(response)\n\n        # Test that the middleware rejects a request with no origin header when secure\n        request = self._get_POST_request_with_token()\n        request._is_secure_override = True\n        response = csrf_middleware.process_view(request, None, (), {})\n        self.assertEqual(response.reason_phrase, REASON_NO_REFERER)\n"], "sample_460": ["    def test_autocomplete_view(self):\n        url = reverse(\"admin:autocomplete\")\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n        self.assertJSONEqual(response.content, {})\n", "def test_admin_site_check(self):\n    admin_site = AdminSite()\n    admin_site.register(Person)\n    admin_site.register(Book, admin_class=ModelAdmin)\n\n    errors = admin_site.check(None)\n\n    # Assuming there are no custom checks on Person and Book\n    self.assertEqual(len(errors), 0)\n", "    def test_autocomplete_view(self):\n        url = reverse('admin:autocomplete')\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n        self.assertIn(\"results\", response.json())\n        # You can add more specific assertions based on the expected results\n", "    def test_book_promo_inline(self):\n        url = reverse('admin:admin_views_book_change', args=(self.b1.id,))\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n        self.assertContains(response, 'Promo 1')\n\n        post_data = {\n            'name': 'Book 1',\n            'promo_set-TOTAL_FORMS': '1',\n            'promo_set-INITIAL_FORMS': '1',\n            'promo_set-MAX_NUM_FORMS': '',\n            'promo_set-0-id': self.pro1.id,\n            'promo_set-0-name': 'Updated Promo 1',\n            'promo_set-0-book': self.b1.id,\n        }\n        response = self.client.post(url, post_data)\n        self.assertEqual(response.status_code, 302)\n        self.pro1.refresh_from_db()\n        self.assertEqual(self.pro1.name, 'Updated Promo 1')\n", "    def test_add_get(self):\n        \"\"\"\n        Test the add view for Article model.\n        \"\"\"\n        url = reverse('admin:admin_views_article_add')\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n        self.assertContains(response, '<form')\n        self.assertContains(response, '<input type=\"text\" name=\"title\"')\n        self.assertContains(response, '<textarea name=\"content\"')\n        self.assertContains(response, '<input type=\"text\" name=\"date_0\"')\n        self.assertContains(response, '<input type=\"text\" name=\"date_1\"')\n        self.assertContains(response, '<select name=\"section\"')\n"], "sample_769": ["def test_brier_score_loss():\n    y_true = np.array([0, 1, 1, 0])\n    y_prob = np.array([0.1, 0.9, 0.8, 0.3])\n    expected_score = 0.037\n    assert_almost_equal(brier_score_loss(y_true, y_prob), expected_score)\n", "def test_brier_score_loss():\n    y_true = np.array([0, 1, 1, 0])\n    y_prob = np.array([0.1, 0.9, 0.8, 0.3])\n    expected_score = 0.0375\n    assert_almost_equal(brier_score_loss(y_true, y_prob), expected_score)\n", "def test_hinge_loss_multiclass():\n    X = np.array([[0], [1], [2], [3]])\n    Y = np.array([0, 1, 2, 3])\n    labels = np.array([0, 1, 2, 3])\n    est = svm.LinearSVC()\n    est.fit(X, Y)\n    pred_decision = est.decision_function([[-1], [2], [3]])\n    y_true = [0, 2, 3]\n    loss = hinge_loss(y_true, pred_decision, labels)\n    assert_almost_equal(loss, 0.5625)\n", "def test_brier_score_loss():\n    y_true = np.array([0, 1, 1, 0])\n    y_prob = np.array([0.1, 0.9, 0.8, 0.3])\n    expected_score = 0.037\n    assert_almost_equal(brier_score_loss(y_true, y_prob), expected_score)\n", "def test_multilabel_confusion_matrix():\n    y_true = np.array([[0, 1, 1], [1, 0, 1]])\n    y_pred = np.array([[1, 0, 1], [1, 0, 0]])\n    mcm = multilabel_confusion_matrix(y_true, y_pred)\n    expected_mcm = np.array([[[1, 0], [0, 1]], [[1, 1], [1, 0]], [[2, 0], [0, 1]]])\n    assert_array_equal(mcm, expected_mcm)\n\n    y_true = np.array([[0, 1, 1], [1, 0, 1], [0, 1, 0]])\n    y_pred = np.array([[1, 0, 1], [1, 0, 0], [0, 0, 1]])\n    mcm = multilabel_confusion_matrix(y_true, y_pred, samplewise=True)\n    expected_mcm = np.array([[[1, 0], [0, 1]], [[1, 1], [1, 0]], [[1, 0], [0, 1]]])\n    assert_array_equal(mcm, expected_mcm)\n\n    y_true = np.array([[0, 1, 1], [1, 0, 1], [0, 1, 0]])\n    y_pred = np.array([[1, 0, 1], [1, 0, 0], [0, 0, 1]])\n    sample_weight = np.array([1, 2, 3])\n    mcm = multilabel_confusion_matrix(y_true, y_pred, samplewise=True, sample_weight=sample_weight)\n    expected_mcm = np.array([[[1, 0], [0, 1]], [[2, 2], [2, 0]], [[3, 0], [0, 3]]])\n    assert_array_equal(mcm, expected_"], "sample_36": ["def test_biweight_midcovariance_constant_input():\n    # Test biweight_midcovariance with constant input\n    data = np.ones(100)\n    result = biweight_midcovariance(data)\n    assert_allclose(result, np.zeros((1, 1)), atol=1e-10)\n", "def test_biweight_midvariance_modify_sample_size():\n    with NumpyRNGContext(12345):\n        data = randn(1000)\n        bivar_no_modify = biweight_midvariance(data, modify_sample_size=False)\n        bivar_modify = biweight_midvariance(data, modify_sample_size=True)\n        assert bivar_modify <= bivar_no_modify\n", "def test_biweight_scale_constant_array():\n    data = np.ones(10)\n    biscl = biweight_scale(data)\n    assert_allclose(biscl, 0.0)\n", "def test_biweight_midcovariance_modify_sample_size():\n    # Test biweight_midcovariance with modify_sample_size=True\n    # Generate two random variables x and y\n    rng = np.random.RandomState(1)\n    x = rng.normal(0, 1, 200)\n    y = rng.normal(0, 3, 200)\n    # Introduce an obvious outlier\n    x[0] = 30.0\n    # Calculate the biweight midcovariances between x and y with modify_sample_size=True\n    bicov = biweight_midcovariance([x, y], modify_sample_size=True)\n    # Calculate the expected biweight midcovariances\n    expected_bicov = np.cov(x, y, ddof=0)\n    # Adjust the expected biweight midcovariances for the outlier\n    expected_bicov[0, 0] *= 199 / 198\n    expected_bicov[1, 1] *= 199 / 198\n    expected_bicov[0, 1] *= 199 / 198\n    expected_bicov[1, 0] *= 199 / 198\n    # Check that the biweight midcovariances are close to the expected values\n    assert_allclose(bicov, expected_bicov, atol=0.01)\n", "def test_biweight_functions_with_constant_input():\n    # Test biweight functions with constant input\n    data = np.ones(10)\n    assert_allclose(biweight_location(data), 1.0)\n    assert_allclose(biweight_scale(data), 0.0)\n    assert_allclose(biweight_midvariance(data), 0.0)\n    with catch_warnings(RuntimeWarning) as warning_lines:\n        assert_allclose(biweight_midcovariance(data), 0.0)\n        assert \"invalid value encountered in divide\" in warning_lines[0].message.args[0]\n    data_2D = np.ones((2, 10))\n    assert_allclose(biweight_midcovariance(data_2D), np.zeros((2, 2)))\n    x, y = np.ones(10), np.ones(10)\n    assert_allclose(biweight_midcorrelation(x, y), 1.0)\n"], "sample_768": ["def test_build_repr():\n    # Test _build_repr function\n    clf = MockClassifier(a=1, allow_nd=True)\n    clf_repr = _build_repr(clf)\n    assert clf_repr == \"MockClassifier(a=1, allow_nd=True)\"\n", "def test_validate_shuffle_split():\n    n_samples = 10\n    test_size = 2\n    train_size = None\n    n_train, n_test = _validate_shuffle_split(n_samples, test_size, train_size)\n    assert n_train == 8\n    assert n_test == 2\n\n    test_size = 0.2\n    train_size = None\n    n_train, n_test = _validate_shuffle_split(n_samples, test_size, train_size)\n    assert n_train == 8\n    assert n_test == 2\n\n    test_size = None\n    train_size = 8\n    n_train, n_test = _validate_shuffle_split(n_samples, test_size, train_size)\n    assert n_train == 8\n    assert n_test == 2\n\n    test_size = 0.8\n    train_size = None\n    n_train, n_test = _validate_shuffle_split(n_samples, test_size, train_size)\n    assert n_train == 2\n    assert n_test == 8\n\n    test_size = None\n    train_size = 0.8\n    n_train, n_test = _validate_shuffle_split(n_samples, test_size, train_size)\n    assert n_train == 8\n    assert n_test == 2\n", "def test_train_test_split_validation():\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([0, 1, 0])\n\n    with assert_raises(ValueError, match=\"At least one array required as input\"):\n        train_test_split()\n\n    with assert_raises(TypeError, match=\"Invalid parameters passed: {'invalid_param': True}\"):\n        train_test_split(X, invalid_param=True)\n\n    with assert_raises(ValueError, match=\"Stratified train/test split is not implemented for shuffle=False\"):\n        train_test_split(X, y, shuffle=False, stratify=y)\n\n    with assert_raises(ValueError, match=\"Expected cv as an integer, cross-validation object or an iterable. Got 'invalid'.\"):\n        check_cv('invalid')\n", "def test_time_series_split():\n    X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])\n    y = np.array([1, 2, 3, 4, 5, 6])\n    tscv = TimeSeriesSplit(n_splits=3)\n    expected_splits = [(np.array([0, 1]), np.array([2])),\n                       (np.array([0, 1, 2]), np.array([3])),\n                       (np.array([0, 1, 2, 3]), np.array([4])),\n                       (np.array([0, 1, 2, 3, 4]), np.array([5]))]\n    for split, expected in zip(tscv.split(X), expected_splits):\n        assert_array_equal(split[0], expected[0])\n        assert_array_equal(split[1], expected[1])\n    assert tscv.get_n_splits() == 3\n", "def test_shuffle_split_train_test_split_compatibility():\n    X = np.random.RandomState(0).rand(10, 2)\n    y = np.array([0, 0, 1, 1, 2, 2, 3, 3, 4, 4])\n    sss = StratifiedShuffleSplit(n_splits=5, test_size=0.5, random_state=0)\n    for train_index, test_index in sss.split(X, y):\n        X_train, X_test, y_train, y_test = train_test_split(X[train_index], y[train_index], test_size=0.5, random_state=0)\n        assert_equal(len(X_train), len(X_test))\n        assert_equal(len(y_train), len(y_test))\n"], "sample_235": ["def test_on_commit_with_savepoint(self):\n    \"\"\"\n    Test that transaction.on_commit() works with savepoints.\n    \"\"\"\n    # Start a transaction.\n    with transaction.atomic():\n        # Create a savepoint.\n        with transaction.atomic():\n            self.do(1)\n        # The callback should not be called yet.\n        self.assertNotified([])\n        # Create another object and a callback.\n        self.do(2)\n    # The callbacks should be called now.\n    self.assertDone([1, 2])\n", "def test_savepoint_rollback(self):\n    \"\"\"\n    Test that on_commit() callbacks registered during a savepoint are not\n    executed when the savepoint is rolled back.\n    \"\"\"\n    self.do(1)\n    with transaction.atomic(savepoint=True):\n        self.do(2)\n        sid = transaction.savepoint()\n        self.do(3)\n        transaction.savepoint_rollback(sid)\n    self.do(4)\n    self.assertDone([1, 4])\n    self.assertNotified([1, 4])\n", "def test_on_commit_rollback_on_error(self):\n    \"\"\"Test on_commit() hooks are rolled back if an error occurs.\"\"\"\n    try:\n        with transaction.atomic():\n            self.do(1)\n            self.do('error')\n    except ForcedError:\n        pass\n    self.assertDone([])\n", "    def test_nested_on_commit(self):\n        \"\"\"\n        Test that on_commit hooks are only called once per transaction,\n        even when nested.\n        \"\"\"\n        with transaction.atomic():\n            self.do(1)\n            with transaction.atomic():\n                self.do(2)\n        self.assertDone([1, 2])\n", "def test_commit_hooks_with_savepoint(self):\n    with transaction.atomic():\n        self.do(1)\n        with transaction.atomic():\n            self.do(2)\n            savepoint_id = transaction.savepoint()\n            self.do(3)\n            transaction.on_commit(lambda: self.notify(4))\n            transaction.savepoint_rollback(savepoint_id)\n        self.do(5)\n    self.assertDone([1, 5])\n    self.assertNotified([5, 1])\n"], "sample_646": ["def test_pytest_pycollect_makeitem(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        class TestClass(unittest.TestCase):\n                pass\n\n            pass\n        \"\"\"\n    )\n\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*::TestClass::test_method PASSED*\",\n            \"*test_function PASSED*\",\n        ]\n    )\n    assert result.ret == ExitCode.OK\n", "def test_pytest_pycollect_makeitem(pytester: Pytester, monkeypatch: MonkeyPatch) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        class TestClass(unittest.TestCase):\n                self.assertTrue(True)\n        \"\"\"\n    )\n\n    # Mock sys.modules to return our test module\n    monkeypatch.setitem(sys.modules, \"test_class\", sys.modules[\"test_class\"])\n\n    # Run pytest and ensure it collects our test case\n    result = pytester.runpytest(\"-q\", \"--collect-only\")\n    result.stdout.fnmatch_lines([\n        \"*::TestClass::test_method*\",\n    ])\n    assert result.ret == ExitCode.OK\n", "def test_is_skipped(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        class TestClass(unittest.TestCase):\n            @unittest.skip(\"test skipped\")\n                pass\n\n            pass\n        \"\"\"\n    )\n    pytester.importorskip(\"unittest\")\n    result = pytester.runpytest(\"-v\")\n    result.stdout.fnmatch_lines([\"*::test_method SKIPPED*\"])\n    assert \"test_function\" in result.stdout.str()\n", "def test_pytest_pycollect_makeitem(pytester: Pytester, monkeypatch: MonkeyPatch):\n    \"\"\"Test the pytest_pycollect_makeitem hook.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n        import unittest\n\n        class TestMyClass(unittest.TestCase):\n            pass\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)\n\n    monkeypatch.delitem(sys.modules, \"unittest\", raising=False)\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=0)\n", "def test_pytest_pycollect_makeitem(pytester: Pytester):\n    pytester.makepyfile(\n        test_module=\"\"\"\n        import unittest\n\n        class TestClass(unittest.TestCase):\n                self.assertEqual(1, 1)\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)\n"], "sample_33": ["def test_set_locale():\n    saved_locale = locale.setlocale(locale.LC_ALL)\n    try:\n        with misc.set_locale(\"C\"):\n            assert locale.localeconv()['decimal_point'] == '.'\n    finally:\n        locale.setlocale(locale.LC_ALL, saved_locale)\n", "def test_set_locale():\n    saved = locale.setlocale(locale.LC_ALL)\n    try:\n        with misc.set_locale(\"C\"):\n            assert locale.localeconv()['decimal_point'] == '.'\n        assert locale.setlocale(locale.LC_ALL) == saved\n    finally:\n        locale.setlocale(locale.LC_ALL, saved)\n", "def test_did_you_mean():\n    candidates = [\"apple\", \"banana\", \"cherry\", \"date\", \"elderberry\"]\n    assert misc.did_you_mean(\"aple\", candidates) == \"Did you mean apple?\"\n    assert misc.did_you_mean(\"banan\", candidates) == \"Did you mean banana?\"\n    assert misc.did_you_mean(\"non_existent\", candidates) == \"\"\n", "def test_check_broadcast():\n    # Test shapes that are mutually broadcastable\n    shape_a = (3, 1)\n    shape_b = (1, 4)\n    assert misc.check_broadcast(shape_a, shape_b) == (3, 4)\n\n    # Test shapes that are not mutually broadcastable\n    shape_a = (3, 4)\n    shape_b = (2, 4)\n    with pytest.raises(misc.IncompatibleShapeError):\n        misc.check_broadcast(shape_a, shape_b)\n", "def test_json_custom_encoder():\n    from astropy import units as u\n    obj = u.Quantity(10, u.m)\n    encoded = json.dumps(obj, cls=misc.JsonCustomEncoder)\n    decoded = json.loads(encoded)\n    assert decoded == {'value': 10, 'unit': 'm'}\n"], "sample_87": ["    def test_weakref_modules(self):\n        # Test that weakref modules are not included in the list of files to watch\n        weakref_module = weakref.ref(types.ModuleType('weakref_test'))\n        self.addCleanup(lambda: sys.modules.pop('weakref_test', None))\n        sys.modules['weakref_test'] = weakref_module()\n        self.assertFileNotFound(self.temporary_file('weakref_test.py'))\n", "    def test_import_error_files(self):\n        filename = self.temporary_file('import_error.py')\n        filename.write_text(\"syntax error: '\")\n        with self.assertRaises(SyntaxError):\n            self.import_and_cleanup('import_error')\n        self.assertFileFound(filename)\n        autoreload._error_files.clear()\n        self.assertFileNotFound(filename)\n", "    def test_zip_import(self):\n        # Test importing a module from a zip file\n        with tempfile.TemporaryDirectory() as temp_dir:\n            zip_path = os.path.join(temp_dir, 'test_module.zip')\n            with zipfile.ZipFile(zip_path, 'w') as zf:\n                zf.writestr('test_module.py', 'def test_function():\\n    pass')\n            sys.path.insert(0, zip_path)\n            self.import_and_cleanup('test_module')\n            self.assertFileFound(Path(zip_path))\n            sys.path.pop(0)\n", "    def test_zip_imported_modules(self):\n        zip_filename = self.temporary_file('test.zip')\n        module_filename = 'zip_module.py'\n        module_code = 'print(\"Hello from zip module\")'\n        with zipfile.ZipFile(zip_filename, 'w') as zipf:\n            zipf.writestr(module_filename, module_code)\n\n        sys.path.insert(0, str(zip_filename))\n        self.addCleanup(sys.path.remove, str(zip_filename))\n\n        self.assertFileFound(zip_filename)\n        self.import_and_cleanup('zip_module')\n        self.assertFileFound(zip_filename)\n", "    def test_import_error_file(self):\n        filename = self.temporary_file(\"broken_module.py\")\n        filename.write_text(\"syntax error: '\")\n        sys.path.append(str(filename.parent))\n        self.import_and_cleanup(\"broken_module\")\n        self.assertFileFound(filename)\n        self.assertRaises(SyntaxError, import_module, \"broken_module\")\n        self.assertIn(filename, autoreload._error_files)\n        self.assertFileFound(filename)\n        del sys.path[-1]\n        self.assertFileNotFound(filename)\n"], "sample_931": ["def test_parse_annotation_with_env(app, env):\n    annotation = \"List[int]\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 5\n    assert isinstance(result[0], pending_xref)\n    assert result[0].astext() == \"List\"\n    assert isinstance(result[1], addnodes.desc_sig_punctuation)\n    assert result[1].astext() == \"[\"\n    assert isinstance(result[2], nodes.Text)\n    assert result[2].astext() == \"int\"\n    assert isinstance(result[3], addnodes.desc_sig_punctuation)\n    assert result[3].astext() == \"]\"\n", "def test_parse_annotation(app, env):\n    app.builder.env = env\n    nodes = _parse_annotation(\"typing.List[str]\", env)\n    assert len(nodes) == 3\n    assert isinstance(nodes[0], pending_xref)\n    assert nodes[0].astext() == 'typing.List'\n    assert isinstance(nodes[1], desc_sig_punctuation)\n    assert nodes[1].astext() == '['\n    assert isinstance(nodes[2], nodes.Text)\n    assert nodes[2].astext() == 'str'\n", "def test_parse_annotation(app):\n    env = app.builder.env\n    result = _parse_annotation(\"int\", env)\n    assert_node(result[0], (pending_xref, \"\", nodes.Text, [(nodes.Text, \"int\")]))\n\n    result = _parse_annotation(\"List[int]\", env)\n    expected = [\n        addnodes.desc_sig_punctuation('', '['),\n        nodes.Text(\"int\"),\n        addnodes.desc_sig_punctuation('', ']')\n    ]\n    assert [type(node) for node in result] == [type(node) for node in expected]\n", "def test_parse_annotation_syntax_error(app):\n    env = app.builder.env\n    with pytest.raises(SyntaxError):\n        _parse_annotation(\"foo: bar)\", env)\n", "def test_parse_annotation(app, status, warning):\n    env = Mock()\n    result = _parse_annotation(\"List[int]\", env)\n    assert len(result) == 5\n    assert isinstance(result[0], nodes.Text) and result[0].astext() == \"List\"\n    assert isinstance(result[1], nodes.Text) and result[1].astext() == \"[\"\n    assert isinstance(result[2], pending_xref) and result[2]['reftarget'] == \"int\"\n    assert isinstance(result[3], nodes.Text) and result[3].astext() == \"]\"\n"], "sample_1167": ["def test_latex_Quaternion():\n    q = Quaternion(1, 2, 3, 4)\n    assert latex(q) == \"1 + 2 i + 3 j + 4 k\"\n", "compilation error", "compilation error", "compilation error", "def test_latex_prime_notation():\n    expr = Derivative(sin(x), x)\n    latex_expr = latex(expr)\n    assert latex_expr == \"\\\\frac{d}{d x} \\\\sin{\\\\left(x \\\\right)}\"\n"], "sample_44": ["def test_function_unit_creation():\n    for pu in pu_sample:\n        for lu in lu_units:\n            func_unit = lu(pu)\n            assert isinstance(func_unit, u.FunctionUnitBase)\n            assert func_unit.physical_unit == pu\n            assert func_unit.function_unit == lu\n", "def test_logunit_comparisons():\n    # Test comparisons between logarithmic units\n    assert u.dex(u.m) == u.dex(u.m)\n    assert u.mag(u.m) != u.mag(u.s)\n    assert u.decibel(u.m) > u.decibel(u.cm)\n    assert u.decibel(u.m) < u.decibel(u.km)\n    assert u.decibel(u.m) <= u.decibel(u.km)\n    assert u.decibel(u.m) >= u.decibel(u.cm)\n", "def test_function_quantity_creation():\n    for lu in lu_units:\n        for pu in pu_sample:\n            fq = lu(pu)\n            assert fq.physical_unit == pu\n            assert fq.function_unit == lu\n", "def test_log_unit_conversion():\n    for lu1, lu2 in itertools.product(lu_units, repeat=2):\n        if lu1 != lu2:\n            assert lu1.is_equivalent(lu2)\n            assert not lu1 == lu2\n            assert_quantity_allclose(1 * lu1, 1 * lu2, atol=1e-10)\n", "def test_function_unit_equivalencies():\n    for unit1, unit2 in itertools.product(lu_units, repeat=2):\n        assert unit1.is_equivalent(unit2)\n        assert u.Quantity(1, unit1).is_equivalent(u.Quantity(1, unit2))\n\n    # Test equivalencies with physical units\n    assert u.Quantity(1, u.mag).is_equivalent(u.Quantity(1, u.Jy), equivalencies=u.spectral_density(3e18 * u.Hz))\n    assert u.Quantity(1, u.dex).is_equivalent(u.Quantity(2, u.dimensionless_unscaled))\n"], "sample_866": ["def test_affinity_propagation_equal_similarities_and_preferences():\n    # Test when all similarities and preferences are equal\n    S = np.full((5, 5), 0.5)\n    preference = np.full(5, 0.5)\n    assert _equal_similarities_and_preferences(S, preference)\n\n    # Test when similarities and preferences are not equal\n    S = np.array([[0.5, 0.6], [0.6, 0.5]])\n    preference = np.array([0.5, 0.6])\n    assert not _equal_similarities_and_preferences(S, preference)\n", "def test_affinity_propagation_degenerate_cases():\n    # Test when all samples have equal similarities and equal preferences\n    n_samples = 10\n    S = np.ones((n_samples, n_samples))\n    preference = np.ones(n_samples)\n\n    # Preference smaller than similarities\n    with assert_warns(UserWarning, match=\"All samples have mutually equal similarities\"):\n        centers, labels = affinity_propagation(S, preference)\n        assert_array_equal(centers, np.arange(n_samples))\n        assert_array_equal(labels, np.arange(n_samples))\n\n    # Preference larger than similarities\n    preference = np.full(n_samples, 2)\n    with assert_warns(UserWarning, match=\"All samples have mutually equal similarities\"):\n        centers, labels = affinity_propagation(S, preference)\n        assert_array_equal(centers, np.array([0]))\n        assert_array_equal(labels, np.zeros(n_samples))\n\n    # Test when there is only one sample\n    S = np.array([[0]])\n    preference = np.array([0])\n    with assert_warns(UserWarning, match=\"All samples have mutually equal similarities\"):\n        centers, labels = affinity_propagation(S, preference)\n        assert_array_equal(centers, np.array([0]))\n        assert_array_equal(labels, np.array([0]))\n", "def test_equal_similarities_and_preferences():\n    # Test case where all similarities and preferences are equal\n    S = np.ones((5, 5))\n    preference = np.full(5, 1)\n    assert _equal_similarities_and_preferences(S, preference)\n\n    # Test case where similarities and preferences are not equal\n    S = np.array([[1, 2], [3, 4]])\n    preference = np.array([1, 2])\n    assert not _equal_similarities_and_preferences(S, preference)\n", "def test_affinity_propagation_convergence_warning():\n    # Test that a warning is raised when affinity propagation does not converge\n    S = -euclidean_distances(X, squared=True)\n    with assert_warns(ConvergenceWarning):\n        affinity_propagation(S, max_iter=1)\n", "def test_affinity_propagation_degeneracies():\n    # Test the degenerate case when all similarities are equal\n    S = np.ones((10, 10))\n    preference = 0.5\n    assert _equal_similarities_and_preferences(S, preference)\n\n    # Test when preference is smaller than similarities\n    with pytest.warns(UserWarning, match=\"All samples have mutually equal similarities.\"):\n        cluster_centers_indices, labels = affinity_propagation(S, preference)\n        assert_array_equal(cluster_centers_indices, np.arange(10))\n        assert_array_equal(labels, np.arange(10))\n\n    # Test when preference is larger than similarities\n    preference = 2.0\n    with pytest.warns(UserWarning, match=\"All samples have mutually equal similarities.\"):\n        cluster_centers_indices, labels = affinity_propagation(S, preference)\n        assert_array_equal(cluster_centers_indices, np.array([0]))\n        assert_array_equal(labels, np.array([0] * 10))\n"], "sample_342": ["    def setUp(self):\n        self.request = self.factory.get(self.url, self.opts)\n        self.request.user = self.user\n", "    def test_missing_params(self):\n        request = self.factory.get(self.url)\n        request.user = self.user\n        view = AutocompleteJsonView()\n        view.admin_site = site\n        with self.assertRaises(PermissionDenied):\n            view.process_request(request)\n", "    def test_get_queryset(self):\n        request = self.factory.get(self.url, data={**self.opts, 'term': 'test'})\n        request.user = self.user\n        view = AutocompleteJsonView(**self.as_view_args)\n        view.setup(request, **self.opts)\n        view.process_request(request)\n        qs = view.get_queryset()\n        self.assertEqual(qs.model, Answer)\n        self.assertEqual(qs.query.where[0].rhs, 'test')\n", "def test_permission_denied_for_invalid_to_field(self):\n    \"\"\"Test that a PermissionDenied exception is raised for invalid to_field.\"\"\"\n    request = self.factory.get(self.url, data={'term': 'test', 'app_label': 'admin', 'model_name': 'logentry', 'field_name': 'user'})\n    request.user = self.user\n    with self.assertRaises(PermissionDenied):\n        AutocompleteJsonView.as_view(admin_site=site)(request)\n", "def test_autocomplete_view_permission_denied(self):\n    \"\"\"\n    Test that a permission denied error is raised when the user doesn't have\n    view permission on the related model.\n    \"\"\"\n    request = self.factory.get(self.url, self.opts)\n    request.user = self.user\n\n    # Remove view permission for the user\n    model = apps.get_model(self.opts['app_label'], self.opts['model_name'])\n    view_permission = Permission.objects.get(\n        content_type=ContentType.objects.get_for_model(model),\n        codename='view_%s' % model._meta.model_name\n    )\n    self.user.user_permissions.remove(view_permission)\n\n    with self.assertRaises(PermissionDenied):\n        response = self.view(request)\n        response.render()  # Force the view to execute\n"], "sample_61": ["    def test_ascii_username_validator(self):\n        validator = validators.ASCIIUsernameValidator()\n        # Test valid usernames\n        self.assertIsNone(validator('username'))\n        self.assertIsNone(validator('user.name'))\n        self.assertIsNone(validator('user@name'))\n        self.assertIsNone(validator('user+name'))\n        self.assertIsNone(validator('user-name'))\n        self.assertIsNone(validator('user_name'))\n        self.assertIsNone(validator('user123'))\n        # Test invalid usernames\n        with self.assertRaises(ValidationError):\n            validator('user name')\n        with self.assertRaises(ValidationError):\n            validator('user$name')\n        with self.assertRaises(ValidationError):\n            validator('user\u00f1ame')\n", "    def test_ascii_username_validator(self):\n        validator = validators.ASCIIUsernameValidator()\n        self.assertIsNone(validator('valid.username+123@example.com'))\n        with self.assertRaises(ValidationError):\n            validator('inv@lid_username')\n", "    def test_valid_ascii_username(self):\n        validator = ASCIIUsernameValidator()\n        valid_usernames = ['test.user', 'user@example.com', 'user+test', 'user-test', 'user_test', 'user123']\n        for username in valid_usernames:\n            self.assertIsNone(validator(username))\n", "    def test_ascii_username_validator(self):\n        validator = ASCIIUsernameValidator()\n        self.assertIsNone(validator('username'))\n        self.assertIsNone(validator('username123'))\n        self.assertIsNone(validator('username.username'))\n        self.assertIsNone(validator('username+username'))\n        self.assertIsNone(validator('username-username'))\n        self.assertIsNone(validator('username@domain.com'))\n\n        with self.assertRaises(ValidationError):\n            validator('username with space')\n            validator('username_with_underscore')\n            validator('username$invalid')\n            validator('username\\x80')\n", "    def test_ascii_username_validator(self):\n        validator = validators.ASCIIUsernameValidator()\n        valid_usernames = ['user123', 'user.name', 'user+name', 'user-name', 'user@name', 'user_name']\n        invalid_usernames = ['user_name\u00a9', 'user name', 'user\\nname', 'user\\tname']\n\n        for username in valid_usernames:\n            try:\n                validator(username)\n            except ValidationError:\n                self.fail(f'ASCIIUsernameValidator incorrectly rejected username: {username}')\n\n        for username in invalid_usernames:\n            with self.assertRaises(ValidationError):\n                validator(username)\n"], "sample_1179": ["def test_print_Quaternion():\n    q = Quaternion(1, 2, 3, 4)\n    assert sstr(q) == \"1 + 2*i + 3*j + 4*k\"\n", "def test_print_Quaternion():\n    q = Quaternion(1, 2, 3, 4)\n    assert sstr(q) == \"1 + 2*i + 3*j + 4*k\"\n", "def test_print_Quaternion():\n    q = Quaternion(1, 2, 3, 4)\n    assert sstr(q) == \"1 + 2*i + 3*j + 4*k\"\n", "def test_print_Quaternion():\n    q = Quaternion(1, 2, 3, 4)\n    assert sstr(q) == '1 + 2*i + 3*j + 4*k'\n", "def test_str_printer_Quaternion():\n    q = Quaternion(1, 2, 3, 4)\n    assert sstr(q) == \"1 + 2*i + 3*j + 4*k\"\n"], "sample_201": ["def test_encode_decode_with_safe_data(self):\n    safe_message = mark_safe(\"This is a safe message\")\n    message = Message(constants.INFO, safe_message)\n    storage = CookieStorage(self.request)\n    encoded_data = storage._encode([message])\n    decoded_data = storage._decode(encoded_data)\n    self.assertEqual(decoded_data, [message])\n    self.assertIsInstance(decoded_data[0].message, SafeData)\n", "    def test_message_encoder_decoder(self):\n        message = Message(constants.INFO, \"This is a test message\")\n        encoder = MessageEncoder()\n        decoder = MessageDecoder()\n\n        encoded_message = encoder.encode(message)\n        decoded_message = decoder.decode(encoded_message)\n\n        self.assertEqual(message.level, decoded_message.level)\n        self.assertEqual(message.message, decoded_message.message)\n        self.assertEqual(message.extra_tags, decoded_message.extra_tags)\n", "    def test_process_messages(self):\n        decoder = MessageDecoder()\n\n        # Test with a simple message\n        message = Message(constants.INFO, 'Test message')\n        encoded_message = [MessageEncoder.message_key, 0, constants.INFO, 'Test message']\n        self.assertEqual(decoder.process_messages(encoded_message), message)\n\n        # Test with a SafeData message\n        safe_message = Message(constants.INFO, mark_safe('Test safe message'))\n        encoded_safe_message = [MessageEncoder.message_key, 1, constants.INFO, 'Test safe message']\n        self.assertEqual(decoder.process_messages(encoded_safe_message), safe_message)\n\n        # Test with a message containing extra tags\n        message_with_tags = Message(constants.INFO, 'Test message', extra_tags='test_tag')\n        encoded_message_with_tags = [MessageEncoder.message_key, 0, constants.INFO, 'Test message', 'test_tag']\n        self.assertEqual(decoder.process_messages(encoded_message_with_tags), message_with_tags)\n\n        # Test with a list of messages\n        messages = [Message(constants.INFO, 'Message 1'), Message(constants.ERROR, 'Message 2')]\n        encoded_messages = [[MessageEncoder.message_key, 0, constants.INFO, 'Message 1'], [MessageEncoder.message_key, 0, constants.ERROR, 'Message 2']]\n        self.assertEqual(decoder.process_messages(encoded_messages), messages)\n\n        # Test with a dictionary containing messages\n        messages_dict = {'key1': Message(constants.INFO, 'Message 1'), 'key2': Message(constants.ERROR, 'Message 2')}\n        encoded_messages_dict = {'key1': [MessageEncoder.message_key, 0, constants.INFO, 'Message 1'], 'key2': [MessageEncoder.", "    def test_message_encoder_decoder(self):\n        message = Message(constants.INFO, \"test message\")\n        encoded_message = json.dumps(message, cls=MessageEncoder)\n        decoded_message = json.loads(encoded_message, cls=MessageDecoder)\n        self.assertEqual(message.level, decoded_message.level)\n        self.assertEqual(message.message, decoded_message.message)\n", "    def test_encode_decode_messages(self):\n        messages = [\n            Message(constants.INFO, 'Info message'),\n            Message(constants.WARNING, mark_safe('Safe warning message')),\n            Message(constants.ERROR, 'Error message'),\n        ]\n        storage = CookieStorage(None)\n        encoded_data = storage._encode(messages)\n        decoded_data = storage._decode(encoded_data)\n        self.assertEqual(messages, decoded_data)\n"], "sample_429": ["    def test_prohibit_null_characters_validator_message(self):\n        validator = ProhibitNullCharactersValidator(message=\"Custom error message\")\n        with self.assertRaisesMessage(ValidationError, \"Custom error message\"):\n            validator(\"\\x00something\")\n", "    def test_url_validator_extended_schemes(self):\n        for url in VALID_URLS:\n            with self.subTest(url=url):\n                self.assertIsNone(URLValidator(EXTENDED_SCHEMES)(url))\n        for url in INVALID_URLS:\n            with self.subTest(url=url):\n                with self.assertRaises(ValidationError):\n                    URLValidator(EXTENDED_SCHEMES)(url)\n", "compilation error", "    def test_url_validator_with_extended_schemes(self):\n        url_validator = URLValidator(schemes=EXTENDED_SCHEMES)\n        for url in VALID_URLS:\n            with self.subTest(url=url):\n                self.assertIsNone(url_validator(url))\n        for url in INVALID_URLS:\n            with self.subTest(url=url):\n                with self.assertRaises(ValidationError):\n                    url_validator(url)\n", "    def test_url_validator_with_extended_schemes(self):\n        url_validator = URLValidator(schemes=EXTENDED_SCHEMES)\n        for url in VALID_URLS:\n            with self.subTest(url=url):\n                url_validator(url)  # This should not raise any exception\n        for url in INVALID_URLS:\n            with self.subTest(url=url):\n                with self.assertRaises(ValidationError):\n                    url_validator(url)\n"], "sample_540": ["def test_null_movie_writer(anim):\n    \"\"\"Test NullMovieWriter with a simple animation.\"\"\"\n    writer = NullMovieWriter()\n    with writer.saving(anim._fig, \"test.mp4\", dpi=100, extra_arg=1):\n        for frame_data in anim.new_frame_seq():\n            anim._draw_frame(frame_data)\n            writer.grab_frame()\n\n    assert writer.fig is anim._fig\n    assert writer.outfile == \"test.mp4\"\n    assert writer.dpi == 100\n    assert writer.args == (1,)\n    assert writer._count == len(anim.new_frame_seq())\n    assert writer.savefig_kwargs == {}\n", "def test_abstract_movie_writer_saving(anim):\n    \"\"\"Test the saving() context manager of AbstractMovieWriter.\"\"\"\n    writer = NullMovieWriter()\n    with writer.saving(anim._fig, \"outfile.mp4\", 100, \"arg1\", \"arg2\"):\n        writer.grab_frame(format=\"png\", dpi=200)\n        assert writer.fig is anim._fig\n        assert writer.outfile == \"outfile.mp4\"\n        assert writer.dpi == 100\n        assert writer.args == (\"arg1\", \"arg2\")\n        assert writer.savefig_kwargs == {\"format\": \"png\", \"dpi\": 200}\n        assert writer._count == 1\n    # Check that finish() was called.\n    assert not hasattr(writer, \"_proc\")\n", "def test_FuncAnimation_garbage_collection(anim):\n    # Test that FuncAnimation is garbage collected when it has not been run\n    anim_weakref = weakref.ref(anim)\n    del anim\n    assert anim_weakref() is None\n", "def test_null_movie_writer(anim):\n    \"\"\"Test NullMovieWriter with an animation.\"\"\"\n    with anim.saving(anim.fig, 'test.mp4', 200, 'arg1', 'arg2'):\n        assert anim.outfile == 'test.mp4'\n        assert anim.dpi == 200\n        assert anim.args == ('arg1', 'arg2')\n        anim.grab_frame(dpi=300, format='png')\n        assert anim.savefig_kwargs == {'dpi': 300, 'format': 'png'}\n        assert anim._count == 1\n", "def test_anim_pause_resume():\n    \"\"\"Test the pause and resume methods of the Animation class.\"\"\"\n    anim = animation.FuncAnimation(fig=plt.figure(), func=lambda x: plt.plot([], []), frames=range(5))\n\n    # Initial state should be running\n    assert anim.event_source.is_running()\n\n    # Pausing should stop the event source\n    anim.pause()\n    assert not anim.event_source.is_running()\n\n    # Resuming should start the event source\n    anim.resume()\n    assert anim.event_source.is_running()\n"], "sample_395": ["    def test_watch_for_template_changes(self):\n        sender = mock.Mock()\n        autoreload.watch_for_template_changes(sender)\n        sender.watch_dir.assert_any_call(EXTRA_TEMPLATES_DIR, \"**/*\")\n", "def test_file_changed(self, mock_to_path, mock_reset_loaders):\n    mock_to_path.side_effect = lambda x: x\n    file_path = EXTRA_TEMPLATES_DIR / \"subdir\" / \"test_template.html\"\n    file_path.parent.mkdir(parents=True, exist_ok=True)\n    file_path.touch()\n    self.assertTrue(autoreload.template_changed(sender=None, file_path=file_path))\n    mock_reset_loaders.assert_called_once()\n", "    def test_watch_for_template_changes(self):\n        sender = mock.MagicMock()\n        autoreload.watch_for_template_changes(sender)\n        sender.watch_dir.assert_called_with(EXTRA_TEMPLATES_DIR, \"**/*\")\n", "    def test_get_template_directories(self):\n        directories = autoreload.get_template_directories()\n        self.assertIn(EXTRA_TEMPLATES_DIR, directories)\n        self.assertNotIn(str(Path(__file__).parent / \"django\" / \"templates\"), directories)\n", "    def test_watch_for_template_changes(self):\n        autoreload.file_changed(EXTRA_TEMPLATES_DIR / \"test.html\")\n        self.assertTrue(autoreload.file_changed.sent)\n        self.assertEqual(autoreload.reset_loaders.call_count, 1)\n"], "sample_1191": ["def test_hermite_normal_form_modulo_D():\n    m = DomainMatrix([[ZZ(12), ZZ(6), ZZ(4)],\n                      [ZZ(3), ZZ(9), ZZ(6)],\n                      [ZZ(2), ZZ(16), ZZ(14)]], (3, 3), ZZ)\n    D = ZZ(30)\n    result = _hermite_normal_form_modulo_D(m, D)\n    expected = DomainMatrix([[ZZ(10), ZZ(0), ZZ(2)],\n                             [ZZ(0), ZZ(15), ZZ(3)],\n                             [ZZ(0), ZZ(0), ZZ(2)]], (3, 3), ZZ)\n    assert result == expected\n", "def test_hermite_normal_form_modulo_D_with_small_D():\n    A = DomainMatrix([[ZZ(12), ZZ(6), ZZ(4)],\n                      [ZZ(3), ZZ(9), ZZ(6)],\n                      [ZZ(2), ZZ(16), ZZ(14)]], (3, 3), ZZ)\n    D = ZZ(1)\n    with raises(DMDomainError):\n        _hermite_normal_form_modulo_D(A, D)\n", "def test_hermite_normal_form_modulo_D_errors():\n    A = DomainMatrix([[1, 2, 3], [4, 5, 6]], (2, 3), QQ)\n    D = 7\n    raises(DMDomainError, lambda: _hermite_normal_form_modulo_D(A, D))\n\n    A = DomainMatrix([[1, 2, 3], [4, 5, 6]], (2, 3), ZZ)\n    D = -7\n    raises(DMDomainError, lambda: _hermite_normal_form_modulo_D(A, D))\n\n    A = DomainMatrix([[1, 2, 3], [4, 5, 6], [7, 8, 9]], (3, 3), ZZ)\n    D = 7\n    raises(DMShapeError, lambda: _hermite_normal_form_modulo_D(A, D))\n", "def test_hermite_normal_form():\n    A = DomainMatrix([[ZZ(12), ZZ(6), ZZ(4)],\n                      [ZZ(3), ZZ(9), ZZ(6)],\n                      [ZZ(2), ZZ(16), ZZ(14)]], (3, 3), ZZ)\n    expected = DomainMatrix([[ZZ(10), ZZ(0), ZZ(2)],\n                             [ZZ(0), ZZ(15), ZZ(3)],\n                             [ZZ(0), ZZ(0), ZZ(2)]], (3, 3), ZZ)\n    assert hermite_normal_form(A) == expected\n\n    # Test with modulo D\n    D = ZZ(900)\n    assert hermite_normal_form(A, D=D) == expected\n\n    # Test with check_rank\n    assert hermite_normal_form(A, D=D, check_rank=True) == expected\n\n    # Test with non-square matrix and check_rank\n    A = DomainMatrix([[ZZ(12), ZZ(6), ZZ(4)],\n                      [ZZ(3), ZZ(9), ZZ(6)]], (2, 3), ZZ)\n    expected = DomainMatrix([[ZZ(3), ZZ(0), ZZ(1)],\n                             [ZZ(0), ZZ(3), ZZ(0)]], (2, 3), ZZ)\n    assert hermite_normal_form(A, check_rank=True) == expected\n\n    # Test with non-integer matrix\n    A = DomainMatrix([[Symbol('a'), ZZ(6), ZZ(4)],\n                      [ZZ(3), ZZ(9), ZZ(6)]], (2, 3), QQ)\n    with raises(DMDomainError):\n        hermite_normal_form(A)\n\n    # Test with non-positive D\n    A = DomainMatrix([[ZZ(12), ZZ(6), ZZ(4)],\n                ", "def test_hermite_normal_form():\n    A = DomainMatrix([[ZZ(12), ZZ(6), ZZ(4)],\n                      [ZZ(3), ZZ(9), ZZ(6)],\n                      [ZZ(2), ZZ(16), ZZ(14)]], (3, 3), ZZ)\n    H = _hermite_normal_form(A)\n    assert H == DomainMatrix([[10, 0, 2], [0, 15, 3], [0, 0, 2]], (3, 3), ZZ)\n\n    D = ZZ(180)\n    H_mod = _hermite_normal_form_modulo_D(A, D)\n    assert H_mod == H\n"], "sample_877": ["def test_isotonic_regression_increasing_decreasing(increasing):\n    \"\"\"Test isotonic_regression function with increasing and decreasing data.\"\"\"\n    rng = np.random.RandomState(0)\n    y = rng.rand(100)\n    y_iso = isotonic_regression(y, increasing=increasing)\n\n    if increasing:\n        assert np.all(np.diff(y_iso) >= 0)\n    else:\n        assert np.all(np.diff(y_iso) <= 0)\n", "def test_isotonic_regression_with_duplicates():\n    # Test isotonic regression with duplicate values in y\n    X = np.array([1, 2, 3, 4, 5])\n    y = np.array([1, 2, 2, 3, 4])\n    y_expected = np.array([1, 2, 2, 3, 4])\n    y_pred = isotonic_regression(y, increasing=True)\n    assert_array_equal(y_pred, y_expected)\n\n    y_expected = np.array([4, 3, 3, 2, 1])\n    y_pred = isotonic_regression(y, increasing=False)\n    assert_array_equal(y_pred, y_expected)\n\n    # Test isotonic regression with duplicate values in y and sample_weight\n    sample_weight = np.array([1, 1, 0, 1, 1])\n    y_expected = np.array([1, 2, 2, 3, 4])\n    y_pred = isotonic_regression(y, sample_weight=sample_weight, increasing=True)\n    assert_array_equal(y_pred, y_expected)\n\n    y_expected = np.array([4, 3, 3, 2, 1])\n    y_pred = isotonic_regression(y, sample_weight=sample_weight, increasing=False)\n    assert_array_equal(y_pred, y_expected)\n", "def test_isotonic_regression_with_zero_weights():\n    X = np.array([1, 2, 3, 4, 5])\n    y = np.array([1, 2, 3, 4, 5])\n    sample_weight = np.zeros_like(X)\n\n    with pytest.raises(ValueError, match=\"Sample weights sum to zero.\"):\n        isotonic_regression(y, sample_weight=sample_weight)\n", "def test_isotonic_regression_duplicates():\n    # Test that removing duplicates in y does not have any impact on the resulting\n    # interpolation function.\n    n_samples = 10\n    X, y = make_regression(n_samples=n_samples, n_features=1, random_state=41)\n\n    # Add some duplicates in y\n    y_duplicates = np.concatenate((y, y[:5]))\n    X_duplicates = np.concatenate((X, X[:5]))\n\n    # Shuffle to ensure that duplicates are not in the same order\n    X_duplicates, y_duplicates = shuffle(X_duplicates, y_duplicates, random_state=42)\n\n    iso_reg = IsotonicRegression()\n    iso_reg_duplicates = IsotonicRegression()\n\n    iso_reg.fit(X, y)\n    iso_reg_duplicates.fit(X_duplicates, y_duplicates)\n\n    # Check that the interpolation functions are the same\n    X_test = np.linspace(np.min(X), np.max(X), n_samples * 10)\n    assert_array_almost_equal(iso_reg.predict(X_test), iso_reg_duplicates.predict(X_test))\n", "def test_isotonic_regression_y_min_y_max():\n    \"\"\"Test isotonic_regression with y_min and y_max\"\"\"\n    y = np.array([2.0, 1.0, 3.0, 4.0, 5.0])\n    y_min = 2.5\n    y_max = 4.5\n    expected_y = np.array([2.5, 2.5, 3.0, 4.0, 4.5])\n    result = isotonic_regression(y, y_min=y_min, y_max=y_max)\n    assert_array_equal(result, expected_y)\n"], "sample_920": ["    def test_parse_numpydoc_see_also_section(self):\n        docstring = '''\n        See Also\n        --------\n        func_name : Descriptive text\n            continued text\n        another_func_name : Descriptive text\n        func_name1, func_name2, :meth:`func_name`, func_name3\n        '''\n        expected = dedent('''\n        .. seealso::\n\n           :obj:`func_name`\n               Descriptive text\n               continued text\n\n           :obj:`another_func_name`\n               Descriptive text\n\n           :obj:`func_name1`, :obj:`func_name2`, :meth:`func_name`, :obj:`func_name3`\n        ''').strip()\n        self.assertEqual(str(NumpyDocstring(docstring)), expected)\n", "    def test_attributes_section(self):\n        docstring = GoogleDocstring(NamedtupleSubclass.__doc__, config=Config())\n        attributes_section = docstring._parse_attributes_section('Attributes')\n        expected_output = [\n            '.. attribute:: attr1',\n            '   :type: Arbitrary type',\n            '',\n            '   Quick description of attr1',\n            '',\n            '.. attribute:: attr2',\n            '   :type: Another arbitrary type',\n            '',\n            '   Quick description of attr2',\n            '',\n            '.. attribute:: attr3',\n            '',\n            '   Type',\n            '',\n            '   Adds a newline after the type',\n            '',\n        ]\n        self.assertEqual(attributes_section, expected_output)\n", "    def test_parsing_attributes(self):\n        docstring = NamedtupleSubclass.__doc__\n        config = Config(napoleon_use_ivar=True)\n        parsed = GoogleDocstring(docstring, config, what='class', obj=NamedtupleSubclass)\n        expected = dedent(\"\"\"\n            Sample namedtuple subclass\n\n            :ivar attr1: Quick description of attr1\n            :vartype attr1: Arbitrary type\n            :ivar attr2: Quick description of attr2\n            :vartype attr2: Another arbitrary type\n            :ivar attr3:\n                Adds a newline after the type\n            :vartype attr3: Type\n        \"\"\")\n        self.assertEqual(str(parsed), cleandoc(expected))\n", "    def test_napoleon_docstring(self):\n        docstring = NamedtupleSubclass.__doc__\n        config = Config()\n        parsed_docstring = NumpyDocstring(docstring, config)\n        expected_output = cleandoc(dedent(\"\"\"\n            Sample namedtuple subclass\n\n            :param attr1: Quick description of attr1\n            :type attr1: Arbitrary type\n            :param attr2: Quick description of attr2\n            :type attr2: Another arbitrary type\n            :param attr3:\n\n                Adds a newline after the type\n            :type attr3: Type\n        \"\"\"))\n        self.assertEqual(str(parsed_docstring), expected_output)\n", "    def test_numpy_docstring_with_sections(self):\n        config = Config(napoleon_use_param=True, napoleon_use_rtype=True)\n        docstring = '''One line summary.\n\n        Extended description.\n\n        Parameters\n        ----------\n        arg1 : int\n            Description of `arg1`\n        arg2 : str\n            Description of `arg2`\n\n        Returns\n        -------\n        str\n            Description of return value.\n        '''\n        expected = '''One line summary.\n\n        Extended description.\n\n        :param arg1: Description of `arg1`\n        :type arg1: int\n        :param arg2: Description of `arg2`\n        :type arg2: str\n\n        :returns: Description of return value.\n        :rtype: str\n        '''\n        self.assertEqual(dedent(str(NumpyDocstring(docstring, config))), dedent(expected))\n"], "sample_299": ["def test_default_cache_is_not_configured(self):\n    errors = check_default_cache_is_configured(None)\n    self.assertEqual(len(errors), 1)\n    self.assertEqual(errors[0], E001)\n", "    def test_check_default_cache_is_configured(self):\n        errors = check_default_cache_is_configured(None)\n        self.assertEqual(errors, [])\n\n        with self.settings(CACHES=self.INVALID_CACHES_CONFIGURATION):\n            errors = check_default_cache_is_configured(None)\n            self.assertEqual(errors, [E001])\n", "def test_default_cache_is_not_configured(self):\n    errors = check_default_cache_is_configured(None)\n    self.assertEqual(len(errors), 1)\n    self.assertEqual(errors[0], E001)\n", "    def test_check_default_cache_is_configured(self):\n        self.assertEqual(check_default_cache_is_configured(None), [])\n\n        with self.settings(CACHES=self.INVALID_CACHES_CONFIGURATION):\n            self.assertEqual(check_default_cache_is_configured(None), [E001])\n", "    def test_file_based_cache_absolute_path(self):\n        absolute_path_config = {\n            'filebased': {\n                'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache',\n                'LOCATION': '/absolute/path',\n            },\n        }\n        relative_path_config = {\n            'filebased': {\n                'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache',\n                'LOCATION': 'relative/path',\n            },\n        }\n\n        with override_settings(CACHES=absolute_path_config):\n            errors = check_file_based_cache_is_absolute(None)\n            self.assertEqual(errors, [])\n\n        with override_settings(CACHES=relative_path_config):\n            errors = check_file_based_cache_is_absolute(None)\n            self.assertEqual(errors, [\n                Warning(\n                    \"Your 'filebased' cache LOCATION path is relative. Use an \"\n                    \"absolute path instead.\",\n                    id='caches.W003',\n                ),\n            ])\n"], "sample_73": ["def test_max_post_process_passes_exceeded(self):\n    # Arrange\n    storage.staticfiles_storage.max_post_process_passes = 1\n    self.write_file('css/test.css', 'body { background: url(\"/media/test.png\"); }')\n    self.write_file('media/test.png', 'test')\n\n    # Act\n    with self.assertRaises(RuntimeError) as context:\n        self.collect()\n\n    # Assert\n    self.assertEqual(str(context.exception), 'Max post-process passes exceeded.')\n    self.assertPostCondition()\n", "    def test_stored_name(self):\n        # Prepare the test data\n        test_name = 'test.css'\n        hashed_name = 'test.abc123.css'\n        storage.staticfiles_storage.hashed_files = {test_name: hashed_name}\n\n        # Call the method under test\n        result = storage.staticfiles_storage.stored_name(test_name)\n\n        # Assert the result\n        self.assertEqual(result, hashed_name)\n\n        # Assert post conditions\n        self.assertPostCondition()\n", "    def test_manifest_strict(self):\n        self.write_file('foo.txt', 'foo')\n        self.write_file('bar.txt', 'bar')\n        call_command('collectstatic', '--noinput')\n        self.assertEqual(storage.staticfiles_storage.hashed_files, {\n            'foo.txt': self.hashed_file_path('foo.txt'),\n            'bar.txt': self.hashed_file_path('bar.txt'),\n            'staticfiles.json': self.hashed_file_path('staticfiles.json'),\n        })\n        # Test missing staticfiles manifest entry raises ValueError when strict\n        with self.assertRaises(ValueError):\n            storage.staticfiles_storage.stored_name('missing.txt')\n", "def test_max_post_process_passes(self):\n    # Set max_post_process_passes to a very low number to force the test to trigger the error.\n    storage.staticfiles_storage.max_post_process_passes = 1\n\n    # Create a circular reference between two CSS files.\n    self.write_file('css/a.css', \"@import url('b.css');\")\n    self.write_file('css/b.css', \"@import url('a.css');\")\n\n    # Run the collectstatic command.\n    with self.settings(STATICFILES_DIRS=[self.tmpdir]):\n        with self.assertRaises(RuntimeError) as cm:\n            call_command('collectstatic', interactive=False, verbosity=0)\n\n    # Check the error message.\n    self.assertEqual(str(cm.exception), 'Max post-process passes exceeded.')\n\n    self.assertPostCondition()\n", "    def test_max_post_process_passes(self):\n        \"\"\"\n        Test that max_post_process_passes is respected.\n        \"\"\"\n        storage.staticfiles_storage.max_post_process_passes = 1\n        with self.settings(STATICFILES_DIRS=[os.path.join(TEST_ROOT, 'test_project', 'sub_project')]):\n            call_command('collectstatic', interactive=False, verbosity=0)\n            # Check that the files were collected correctly\n            self.assertIn('sub_project/sub.css', storage.staticfiles_storage.hashed_files)\n            # Check that the maximum number of post-process passes was respected\n            self.assertEqual(storage.staticfiles_storage.hashed_files['sub_project/sub.css'],\n                             'sub_project/sub.774c6c442243.css')\n        self.assertPostCondition()\n"], "sample_808": ["def test_isolation_forest_sparse_matrix():\n    # Test IsolationForest with a sparse matrix\n    X = csc_matrix(iris.data)\n    clf = IsolationForest(random_state=rng)\n    clf.fit(X)\n    y_pred = clf.predict(X)\n    assert_array_equal(np.unique(y_pred), [-1, 1])\n", "def test_iforest_with_sparse_matrix():\n    X = csc_matrix(iris.data)\n    clf = IsolationForest(random_state=rng)\n    clf.fit(X)\n    y_pred = clf.predict(X)\n    assert y_pred.shape == (X.shape[0],)\n    assert_array_equal(np.unique(y_pred), [-1, 1])\n", "def test_isolation_forest_sparse_matrix():\n    # Test the IsolationForest algorithm with a sparse matrix input\n    X = csr_matrix([[1, 2, 3], [4, 0, 6], [7, 8, 0]])\n    clf = IsolationForest(random_state=rng)\n    clf.fit(X)\n    y_pred = clf.predict(X)\n    assert y_pred.shape == (3,)\n    assert set(y_pred) == {-1, 1}\n", "def test_isolation_forest_sparse_matrix():\n    rng = check_random_state(0)\n    X = csc_matrix(rng.randn(100, 5))\n    clf = IsolationForest(random_state=rng)\n    clf.fit(X)\n    y_pred = clf.predict(X)\n    assert y_pred.shape == (100,)\n    assert np.unique(y_pred).shape == (2,)  # Check if both -1 and 1 are present\n\n    # Test with csr_matrix\n    X = csr_matrix(rng.randn(100, 5))\n    clf = IsolationForest(random_state=rng)\n    clf.fit(X)\n    y_pred = clf.predict(X)\n    assert y_pred.shape == (100,)\n    assert np.unique(y_pred).shape == (2,)  # Check if both -1 and 1 are present\n", "def test_iforest_predict_contamination():\n    X = iris.data\n    y = iris.target\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n    # Test contamination parameter\n    iforest = IsolationForest(contamination='auto', random_state=42)\n    iforest.fit(X_train)\n    y_pred = iforest.predict(X_test)\n    outlier_fraction = np.mean(y_pred == -1)\n    assert_greater(outlier_fraction, 0)\n    assert_greater(outlier_fraction, 0.1)\n"], "sample_548": ["def test_colorbar_extension_length():\n    fig_uniform = _colorbar_extension_length(spacing='uniform')\n    fig_proportional = _colorbar_extension_length(spacing='proportional')\n    return fig_uniform, fig_proportional\n", "def test_colorbar_orientation():\n    \"\"\"Test the orientation of the colorbar.\"\"\"\n    fig, ax = plt.subplots()\n    data = np.random.rand(10, 10)\n    im = ax.imshow(data)\n    cbar_vertical = fig.colorbar(im, orientation='vertical')\n    fig, ax = plt.subplots()\n    im = ax.imshow(data)\n    cbar_horizontal = fig.colorbar(im, orientation='horizontal')\n    return fig\n", "def test_colorbar_ticks_labels():\n    \"\"\"\n    Produce a colorbar with custom ticks and labels.\n    \"\"\"\n    cmap = cm.get_cmap('viridis', 5)\n    norm = Normalize(vmin=0, vmax=1)\n    fig, ax = plt.subplots()\n    cb = fig.colorbar(cm.ScalarMappable(norm=norm, cmap=cmap),\n                      ax=ax, orientation='horizontal',\n                      ticks=[0, 0.25, 0.5, 0.75, 1],\n                      label='Custom Colorbar')\n    cb.ax.set_xticklabels(['Low', 'Q1', 'Medium', 'Q3', 'High'])\n    return fig\n", "def test_colorbar_orientation():\n    fig, ax = plt.subplots(2, 1, sharex=True)\n    cmap = mpl.colormaps[\"viridis\"]\n    norm = Normalize(vmin=0, vmax=1)\n\n    im1 = ax[0].imshow(np.random.random((10, 20)), cmap=cmap, norm=norm)\n    cbar1 = fig.colorbar(im1, ax=ax[0], orientation='horizontal')\n    cbar1.set_label('Orientation = horizontal')\n\n    im2 = ax[1].imshow(np.random.random((10, 20)), cmap=cmap, norm=norm)\n    cbar2 = fig.colorbar(im2, ax=ax[1], orientation='vertical')\n    cbar2.set_label('Orientation = vertical')\n\n    return fig\n", "def test_colorbar_lines_orientation():\n    \"\"\"\n    Test that lines are drawn correctly with both vertical and horizontal\n    orientations.\n    \"\"\"\n    fig, ax = plt.subplots(2, 1, figsize=(6, 10))\n\n    # Test data\n    levels = [-1.0, 0.0, 1.0]\n    colors = ['r', 'g', 'b']\n    linewidths = 2.0\n\n    # Vertical orientation\n    cb = fig.colorbar(cm.ScalarMappable(norm=Normalize(-1, 1), cmap=cm.RdBu_r),\n                      cax=ax[0], orientation='vertical', ticks=levels)\n    cb.add_lines(levels, colors, linewidths)\n\n    # Horizontal orientation\n    cb = fig.colorbar(cm.ScalarMappable(norm=Normalize(-1, 1), cmap=cm.RdBu_r),\n                      cax=ax[1], orientation='horizontal', ticks=levels)\n    cb.add_lines(levels, colors, linewidths)\n\n    plt.tight_layout()\n    return fig\n"], "sample_950": ["def test_parse_annotation_with_env(app):\n    annotation = \"Dict[str, List[int]]\"\n    env = app.builder.env\n    result = _parse_annotation(annotation, env)\n    assert isinstance(result, list)\n    assert len(result) == 7\n    assert isinstance(result[0], pending_xref)\n    assert result[0].astext() == \"Dict\"\n    assert isinstance(result[1], addnodes.desc_sig_punctuation)\n    assert result[1].astext() == \"[\"\n    assert isinstance(result[2], nodes.Text)\n    assert result[2].astext() == \"str\"\n    assert isinstance(result[3], addnodes.desc_sig_punctuation)\n    assert result[3].astext() == \",\"\n    assert isinstance(result[4], nodes.Text)\n    assert result[4].astext() == \" \"\n    assert isinstance(result[5], pending_xref)\n    assert result[5].astext() == \"List\"\n    assert isinstance(result[6], addnodes.desc_sig_punctuation)\n    assert result[6].astext() == \"]\"\n", "def test_parse_annotation(app, env):\n    env.config.python_use_unqualified_type_names = True\n\n    annotation = \"List[Union[int, str]]\"\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 5\n    assert isinstance(result[0], nodes.Text)\n    assert result[0].astext() == \"List\"\n    assert isinstance(result[1], nodes.Text)\n    assert result[1].astext() == \"[\"\n    assert isinstance(result[2], pending_xref)\n    assert result[2].get(\"reftarget\") == \"typing.Union\"\n    assert isinstance(result[3], nodes.Text)\n    assert result[3].astext() == \"]\"\n", "def test_parse_annotation(app):\n    env = app.builder.env\n    test_cases = [\n        ('List[int]', 'List[int]'),\n        ('Tuple[str, int]', 'Tuple[str, int]'),\n        ('Union[int, str]', 'Union[int, str]'),\n        ('Optional[List[int]]', 'Optional[List[int]]'),\n        ('Callable[[int], str]', 'Callable[[int], str]'),\n    ]\n    for annotation, expected in test_cases:\n        result = _parse_annotation(annotation, env)\n        assert nodes.Text(expected) in result\n", "def test_parse_annotation(app, status, warning):\n    env = app.builder.env\n    env.ref_context['py:module'] = 'sphinx.testing'\n    result = _parse_annotation('List[int]', env)\n    assert_node(result, [\n        pending_xref(reftarget='List'),\n        addnodes.desc_sig_punctuation('['),\n        pending_xref(reftarget='int'),\n        addnodes.desc_sig_punctuation(']')\n    ])\n", "def test_parse_annotation():\n    env = Mock()\n    env.config.python_use_unqualified_type_names = False\n    result = _parse_annotation(\"List[int]\", env)\n    assert len(result) == 4\n    assert isinstance(result[0], nodes.Text) and result[0].astext() == \"List[\"\n    assert isinstance(result[1], addnodes.pending_xref) and result[1]['reftarget'] == \"int\"\n    assert isinstance(result[2], nodes.Text) and result[2].astext() == \"]\"\n"], "sample_1094": ["def test_basic_subs():\n    x, y = symbols('x y')\n    b = Basic(x, y)\n    b_subs = b.subs(x, 2)\n    assert b_subs == Basic(2, y)\n", "def test_as_Basic():\n    assert as_Basic(1) == S(1)\n    assert as_Basic(S(2)) == S(2)\n    raises(TypeError, lambda: as_Basic('a'))\n", "def test_basic_eq_ne():\n    assert b1 == b1\n    assert b1 != b2\n    assert b2 == b2\n    assert b2 != b3\n    assert b2 == b21.args[0]\n    assert b1 != b21.args[1]\n", "def test_Basic_sort_key():\n    x, y = symbols('x y')\n    expr = S(\"[x, 1/x, 1/x**2, x**2, x**(1/2), x**(1/4), x**(3/2)]\")\n    sorted_expr = sorted(expr, key=lambda x: x.sort_key())\n    assert sorted_expr == [x**(-2), 1/x, x**(1/4), S.One/S.Two, S.One/S.Sqrt(x), x, x**(3/2)]\n", "def test_aresame():\n    assert _aresame(S(2.0), S(2)) is False\n    assert _aresame(S(2), S(2)) is True\n    assert _aresame(S(2), S(2.0)) is False\n    assert _aresame(S(2), S(3)) is False\n    assert _aresame(sin(x), sin(x)) is True\n    assert _aresame(sin(x), cos(x)) is False\n    assert _aresame(gamma(Q(1, 2)), gamma(Q(1, 2))) is True\n    assert _aresame(gamma(Q(1, 2)), gamma(Q(1, 3))) is False\n"], "sample_822": ["def test_pairwise_distances_chunked_reduce_func():\n    X = np.random.RandomState(0).rand(5, 3)\n    r = .2\n\n        neigh = [np.flatnonzero(d < r) for d in D_chunk]\n        avg_dist = (D_chunk * (D_chunk < r)).mean(axis=1)\n        return neigh, avg_dist\n\n    gen = pairwise_distances_chunked(X, reduce_func=reduce_func)\n    neigh, avg_dist = next(gen)\n\n    assert_array_equal(neigh, [np.array([0, 3]), np.array([1]), np.array([2]), np.array([0, 3]), np.array([4])])\n    assert_array_almost_equal(avg_dist, [0.039, 0.0, 0.0, 0.039, 0.0], decimal=2)\n", "def test_pairwise_distances_argmin_min():\n    # Test pairwise_distances_argmin_min function\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    Y = np.array([[2, 3], [4, 5], [6, 7]])\n    indices, distances = pairwise_distances_argmin_min(X, Y)\n    expected_indices = np.array([0, 1, 2])\n    expected_distances = np.array([1.41421356, 1.41421356, 1.41421356])\n    assert_array_almost_equal(indices, expected_indices)\n    assert_array_almost_equal(distances, expected_distances)\n", "def test_pairwise_distances_chunked_with_reduce_func():\n    r = 0.2\n        neigh = [np.flatnonzero(d < r) for d in D_chunk]\n        avg_dist = (D_chunk * (D_chunk < r)).mean(axis=1)\n        return neigh, avg_dist\n\n    X = np.random.RandomState(0).rand(5, 3)\n    gen = pairwise_distances_chunked(X, reduce_func=reduce_func)\n    neigh, avg_dist = next(gen)\n\n    assert_array_equal(neigh, [np.array([0, 3]), np.array([1]), np.array([2]), np.array([0, 3]), np.array([4])])\n    assert_array_almost_equal(avg_dist, np.array([0.03923977, 0.0, 0.0, 0.03923977, 0.0]))\n", "def test_paired_cosine_distances():\n    X = np.array([[1, 2, 3], [4, 5, 6]])\n    Y = np.array([[7, 8, 9], [10, 11, 12]])\n    expected_result = 0.5 * np.array([linalg.norm(normalize(X[0] - normalize(Y[0]))),\n                                     linalg.norm(normalize(X[1] - normalize(Y[1])))])\n    result = paired_cosine_distances(X, Y)\n    assert_array_almost_equal(result, expected_result)\n", "def test_cosine_similarity():\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    Y = np.array([[7, 8], [9, 10]])\n    expected_output = np.array([[0.02581989, 0.04690416],\n                                [0.06696764, 0.09301077],\n                                [0.1081154, 0.14301791]])\n    assert_allclose(cosine_similarity(X, Y), expected_output)\n\n    # Test cosine similarity with dense_output=False for sparse matrices\n    X_sparse = csr_matrix(X)\n    Y_sparse = csr_matrix(Y)\n    output_sparse = cosine_similarity(X_sparse, Y_sparse, dense_output=False)\n    assert issparse(output_sparse)\n    assert_allclose(output_sparse.toarray(), expected_output)\n"], "sample_664": ["def test_fixture_positional_arguments():\n    with pytest.deprecated_call():\n        @pytest.fixture()\n            pass\n", "def test_fixture_positional_arguments(recwarn):\n    @pytest.fixture\n        pass\n\n    with pytest.warns(deprecated.FIXTURE_POSITIONAL_ARGUMENTS):\n        my_fixture(\"value1\", \"value2\")\n\n    assert len(recwarn) == 1\n    assert issubclass(recwarn[0].category, PytestDeprecationWarning)\n    assert \"Passing arguments to pytest.fixture() as positional arguments is deprecated\" in str(recwarn[0].message)\n", "def test_deprecated_warnings():\n    with pytest.warns(deprecated.FUNCARGNAMES):\n        class TestClass:\n            funcargnames = [\"fixture1\", \"fixture2\"]\n\n    with pytest.warns(deprecated.RESULT_LOG):\n        pytest.main([\"--result-log\", \"path/to/log\"])\n\n    with pytest.warns(deprecated.FIXTURE_POSITIONAL_ARGUMENTS):\n        @pytest.fixture(\"positional\", \"argument\")\n            pass\n\n    with pytest.warns(deprecated.JUNIT_XML_DEFAULT_FAMILY):\n        pytest.main([\"--junit-xml\", \"path/to/report.xml\"])\n", "def test_deprecated_fixture_positional_arguments():\n    with pytest.warns(deprecated.FIXTURE_POSITIONAL_ARGUMENTS):\n        @pytest.fixture(args)\n            pass\n", "def test_deprecated_messages():\n    with pytest.warns(deprecated.FIXTURE_POSITIONAL_ARGUMENTS):\n        @pytest.fixture('function', 10)\n            pass\n\n    with pytest.warns(deprecated.JUNIT_XML_DEFAULT_FAMILY):\n        @pytest.hookimpl(hookwrapper=True)\n            pass\n"], "sample_1086": ["def test_print_Tr():\n    M = Matrix([[1, 2], [3, 4]])\n    assert sstr(Tr(M)) == \"Tr(Matrix([[1, 2], [3, 4]]))\"\n", "def test_print_Tr():\n    A = Matrix([[1, 2], [3, 4]])\n    expr = Tr(A)\n    assert sstr(expr) == \"Tr(Matrix([[1, 2], [3, 4]]))\"\n", "def test_Quaternion_printing():\n    q = Quaternion(1, 2, 3, 4)\n    assert sstr(q) == \"1 + 2*i + 3*j + 4*k\"\n", "def test_sstr_tr():\n    A = Matrix([[1, 2], [3, 4]])\n    assert sstr(Tr(A)) == \"Tr(Matrix([[1, 2], [3, 4]]))\"\n", "def test_next():\n    # Testing the _print_Quaternion method\n    q = Quaternion(x, y, z, w)\n    assert sstr(q) == \"x + i*y + j*z + k*w\"\n"], "sample_624": ["def test_format_timedelta_date(self):\n    t = np.timedelta64(3, 'D')\n    assert formatting.format_timedelta(t, timedelta_format='date') == '3 days'\n", "def test_last_n_items():\n    array = np.array([1, 2, 3, 4, 5])\n    n_desired = 2\n    result = formatting.last_n_items(array, n_desired)\n    assert np.array_equal(result, np.array([4, 5]))\n", "def test_last_item(self):\n    array = np.array([1, 2, 3, 4, 5])\n    assert formatting.last_item(array) == [5]\n\n    array = np.array([[1, 2], [3, 4]])\n    assert formatting.last_item(array) == [4]\n\n    array = np.array([])\n    assert formatting.last_item(array) == []\n", "def test_summarize_index(self):\n    index = pd.Index(['a', 'b', 'c'], name='test_index')\n    result = formatting.summarize_index('test_index', index, 20, 50)\n    expected = '    test_index Index(['a', 'b', 'c'], dtype='object')'\n    assert result == expected\n", "def test_format_array_flat():\n    array = np.array([1, 2, 3, 4, 5])\n    max_width = 10\n    result = formatting.format_array_flat(array, max_width)\n    assert result == \"1 2 3 4 5\"\n\n    array = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n    max_width = 15\n    result = formatting.format_array_flat(array, max_width)\n    assert result == \"1 2 3 ... 9 10\"\n\n    array = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n    max_width = 5\n    result = formatting.format_array_flat(array, max_width)\n    assert result == \"1 ... 10\"\n"], "sample_214": ["    def test_key_transform_in(self):\n        obj = JSONModel.objects.create(json='{\"key\": \"value\"}')\n        self.assertIn(obj, JSONModel.objects.filter(json__key__in=['value']))\n        self.assertNotIn(obj, JSONModel.objects.filter(json__key__in=['other']))\n", "def test_key_transform_factory(self):\n    factory = KeyTransformFactory('test_key')\n    transform = factory(Value('{\"test_key\": \"test_value\"}'))\n    self.assertIsInstance(transform, KeyTransform)\n    self.assertEqual(transform.key_name, 'test_key')\n", "def test_key_transform_in(self):\n    obj = JSONModel.objects.create(json='{\"key\": \"value\"}')\n    self.assertTrue(JSONModel.objects.filter(json__key__in=['value']).exists())\n    self.assertFalse(JSONModel.objects.filter(json__key__in=['other']).exists())\n", "    def test_value_from_object_with_encoder(self):\n        obj = JSONModel(json_field={'key': 'value'})\n        field = JSONModel._meta.get_field('json_field')\n        field.encoder = DjangoJSONEncoder\n        self.assertEqual(field.value_from_object(obj), '{\"key\": \"value\"}')\n", "    def test_key_transform_lookup_with_none_value(self):\n        JSONModel.objects.create(data={'key': None})\n        self.assertEqual(JSONModel.objects.filter(data__key__exact=None).count(), 1)\n        self.assertEqual(JSONModel.objects.filter(data__key__isnull=True).count(), 1)\n        self.assertEqual(JSONModel.objects.filter(data__key__isnull=False).count(), 0)\n"], "sample_1033": ["def test_extract_leading_order():\n    expr = (x + 1 + 1/x**5)\n    result = expr.extract_leading_order(x)\n    assert result == ((x**(-5), S.O(x**(-5))),)\n", "def test_as_two_terms():\n    expr = 3*x - 2*y + 5\n    coeff, rest = expr.as_two_terms()\n    assert coeff == 5\n    assert rest == 3*x - 2*y\n", "def test_eval_as_leading_term():\n    assert same_and_same_prec(Add(x, 1/x)._eval_as_leading_term(x), x)\n", "def test_eval_power():\n    assert same_and_same_prec((7 + 3*I)**Rational(2, 1), 5 + 6*I*sqrt(2))\n", "def test_as_coefficients_dict():\n    expr = 3*x + a*x + 4\n    expected = {1: 4, x: 3, a*x: 1}\n    assert expr.as_coefficients_dict() == expected\n\n    expr = 3*a*x\n    expected = {a*x: 3}\n    assert expr.as_coefficients_dict() == expected\n"], "sample_1093": ["def test_print_MatrixSolve():\n    A = MatrixSymbol('A', 3, 3)\n    b = MatrixSymbol('b', 3, 1)\n    expr = MatrixSolve(A, b)\n    result = NumPyPrinter().doprint(expr)\n    assert result == \"numpy.linalg.solve(A, b)\"\n", "def test_assignment_printing():\n    a = Assignment(x, y + z)\n    printer = PythonCodePrinter()\n    assert printer._print(a) == \"x = y + z\"\n", "def test_print_Mod():\n    expr = Mod(x, y)\n    printer = PythonCodePrinter()\n    result = printer._print_Mod(expr)\n    assert result == '(x % y)'\n", "def test_print_Piecewise():\n    expr = Piecewise((x, x > 0), (0, True))\n    printer = PythonCodePrinter({'standard': 'python3'})\n    assert printer._print_Piecewise(expr) == '(x if (x > 0) else 0)'\n", "def test_print_MatrixSolve():\n    A = MatrixSymbol('A', 2, 2)\n    b = MatrixSymbol('b', 2, 1)\n    expr = MatrixSolve(A, b)\n    printer = PythonCodePrinter()\n    assert printer._print(expr) == \"sympy.MatrixSolve(A, b)\"\n"], "sample_728": ["def test_make_biclusters_shape():\n    shape = (10, 15)\n    n_clusters = 3\n    X, rows, cols = make_biclusters(shape, n_clusters)\n    assert_equal(X.shape, shape)\n    assert_equal(rows.shape, (n_clusters, shape[0]))\n    assert_equal(cols.shape, (n_clusters, shape[1]))\n", "def test_make_classification_fixed_seed():\n    X1, y1 = make_classification(n_samples=10, random_state=42)\n    X2, y2 = make_classification(n_samples=10, random_state=42)\n    assert_array_equal(X1, X2)\n    assert_array_equal(y1, y2)\n", "def test_make_regression_coef():\n    n_samples = 50\n    n_features = 10\n    n_informative = 5\n    n_targets = 2\n    bias = 2.0\n    noise = 1.0\n    X, y, coef = make_regression(n_samples=n_samples, n_features=n_features,\n                                 n_informative=n_informative, n_targets=n_targets,\n                                 bias=bias, noise=noise, coef=True, random_state=0)\n\n    assert_equal(X.shape, (n_samples, n_features))\n    assert_equal(y.shape, (n_samples, n_targets))\n    assert_equal(coef.shape, (n_features, n_targets))\n\n    assert_all_finite(X)\n    assert_all_finite(y)\n    assert_all_finite(coef)\n", "def test_make_classification_random_state():\n    X1, y1 = make_classification(n_samples=10, random_state=0)\n    X2, y2 = make_classification(n_samples=10, random_state=0)\n    assert_array_equal(X1, X2)\n    assert_array_equal(y1, y2)\n", "def test_make_regression_coef():\n    n_samples = 200\n    n_features = 10\n    n_informative = 5\n    X, y, coef = make_regression(n_samples=n_samples, n_features=n_features,\n                                 n_informative=n_informative, coef=True,\n                                 random_state=0)\n\n    assert_equal(X.shape, (n_samples, n_features))\n    assert_equal(y.shape, (n_samples,))\n    assert_equal(coef.shape, (n_features,))\n    assert_equal(np.count_nonzero(coef), n_informative)\n"], "sample_702": ["def test_syspathinsert(pytester: Pytester):\n    original_sys_path = sys.path.copy()\n    pytester.syspathinsert()\n    assert sys.path[0] == str(pytester.path)\n    sys.path = original_sys_path\n", "def test_pytester_chdir(pytester: Pytester, request: pytest.FixtureRequest):\n    original_cwd = os.getcwd()\n    pytester.chdir()\n    assert os.getcwd() == str(pytester.path)\n    pytester._finalize()\n    assert os.getcwd() == original_cwd\n", "def test_syspathinsert(pytester: Pytester):\n    # Test that syspathinsert adds the given path to sys.path\n    pytester.syspathinsert()\n    assert str(pytester.path) in sys.path\n", "def test_syspathinsert(pytester: Pytester) -> None:\n    pytester.syspathinsert()\n    assert str(pytester.path) in sys.path\n", "def test_runpytest_inprocess_exception(pytester: Pytester) -> None:\n    pytester.makepyfile(test_exception=\"def test_exception(): raise Exception('test exception')\")\n    result = pytester.runpytest_inprocess()\n    assert result.ret == ExitCode.TESTS_FAILED\n    assert \"test exception\" in \"\\n\".join(result.errlines)\n"], "sample_1020": ["def test_mathematica_code():\n    assert mcode(exp(x)) == \"Exp[x]\"\n    assert mcode(sin(x)) == \"Sin[x]\"\n    assert mcode(cos(x)) == \"Cos[x]\"\n    assert mcode(pi) == \"Pi\"\n    assert mcode(oo) == \"Infinity\"\n    assert mcode(-oo) == \"-Infinity\"\n    assert mcode(Tuple(x, y, z)) == \"{x, y, z}\"\n    assert mcode(f(x, y)) == \"f[x, y]\"\n    assert mcode(Integral(f(x), x)) == \"Hold[Integrate[f[x], x]]\"\n    assert mcode(Sum(x**2, (x, 1, n))) == \"Hold[Sum[x^2, (x, 1, n)]]\"\n    assert mcode(Derivative(f(x), x)) == \"Hold[D[f[x], x]]\"\n    assert mcode(Max(x, y)) == \"Max[x, y]\"\n    assert mcode(Min(x, y)) == \"Min[x, y]\"\n    assert mcode(conjugate(x)) == \"Conjugate[x]\"\n", "def test_mathematica_code():\n    assert mcode(exp(x)) == 'Exp[x]'\n    assert mcode(sin(x)) == 'Sin[x]'\n    assert mcode(cos(x)) == 'Cos[x]'\n    assert mcode(pi) == 'Pi'\n    assert mcode(oo) == 'Infinity'\n    assert mcode(-oo) == '-Infinity'\n    assert mcode(Tuple(x, y, z)) == '{x, y, z}'\n    assert mcode(f(x, y)) == 'f[x, y]'\n    assert mcode(Integral(f(x), (x, 0, 1))) == 'Hold[Integrate[f[x], x, 0, 1]]'\n    assert mcode(Sum(f(n), (n, 1, 10))) == 'Hold[Sum[f[n], {n, 1, 10}]]'\n    assert mcode(Derivative(f(x, y), x, y)) == 'Hold[D[f[x, y], x, y]]'\n    assert mcode(Max(x, y)) == 'Max[x, y]'\n    assert mcode(Min(x, y)) == 'Min[x, y]'\n", "def test_mcode():\n    assert mcode(exp(x)) == \"Exp[x]\"\n    assert mcode(sin(x)) == \"Sin[x]\"\n    assert mcode(cos(x)) == \"Cos[x]\"\n    assert mcode(pi) == \"Pi\"\n    assert mcode(oo) == \"Infinity\"\n    assert mcode(-oo) == \"-Infinity\"\n    assert mcode((x, y, z)) == \"{x, y, z}\"\n    assert mcode(f(x)) == \"f[x]\"\n    assert mcode(Integral(x, (x, 0, 1))) == \"Hold[Integrate[x, x]]\"\n    assert mcode(Sum(x, (x, 1, n))) == \"Hold[Sum[x, x, 1, n]]\"\n    assert mcode(Derivative(f(x), x)) == \"Hold[D[f[x], x, 1]]\"\n    assert mcode(Max(x, y)) == \"Max[x, y]\"\n    assert mcode(Min(x, y)) == \"Min[x, y]\"\n    assert mcode(conjugate(x)) == \"Conjugate[x]\"\n", "def test_mcode_printer():\n    # Test the _print_Function method\n    assert mcode(f(x)) == \"f[x]\"\n\n    # Test the _print_Integral method\n    assert mcode(Integral(sin(x), x)) == \"Hold[Integrate[sin[x], x]]\"\n\n    # Test the _print_Sum method\n    assert mcode(Sum(x, (x, 1, n))) == \"Hold[Sum[x, (x, 1, n)]]\"\n\n    # Test the _print_Derivative method\n    assert mcode(Derivative(sin(x), x)) == \"Hold[D[sin[x], x]]\"\n\n    # Test the _print_Mul method\n    assert mcode(x*y*z) == \"x*y*z\"\n\n    # Test the _print_Pow method\n    assert mcode(x**y) == \"x^y\"\n\n    # Test the _print_Pi method\n    assert mcode(pi) == \"Pi\"\n\n    # Test the _print_Infinity method\n    assert mcode(oo) == \"Infinity\"\n\n    # Test the _print_NegativeInfinity method\n    assert mcode(-oo) == \"-Infinity\"\n\n    # Test the _print_list method\n    assert mcode(Tuple(x, y, z)) == \"{x, y, z}\"\n\n    # Test the _print_MinMaxBase method\n    assert mcode(Max(x, y, z)) == \"Max[x, y, z]\"\n    assert mcode(Min(x, y, z)) == \"Min[x, y, z]\"\n\n    # Test the _print_Function method with known functions\n    assert mcode(exp(x)) == \"Exp[x]\"\n    assert mcode(sin(x)) == \"Sin[x]\"\n    assert mcode(cos(x)) == \"Cos[x]\"\n    assert mcode(conjugate(x)) == \"Conjugate[x]\"\n\n    # Test the _default_settings\n    assert mcode(Rational(", "def test_mathematica_code():\n    assert mcode(f(x) + f(y)) == 'f[x] + f[y]'\n    assert mcode(exp(x)) == 'Exp[x]'\n    assert mcode(sin(x)) == 'Sin[x]'\n    assert mcode(cos(x)) == 'Cos[x]'\n    assert mcode(conjugate(x)) == 'Conjugate[x]'\n    assert mcode(Max(x, y)) == 'Max[x, y]'\n    assert mcode(Min(x, y)) == 'Min[x, y]'\n    assert mcode(x**y) == 'x^y'\n    assert mcode(pi) == 'Pi'\n    assert mcode(oo) == 'Infinity'\n    assert mcode(-oo) == '-Infinity'\n    assert mcode((1, 2, 3)) == '{1, 2, 3}'\n    assert mcode(Integral(sin(x), x)) == 'Hold[Integrate[sin[x], x]]'\n    assert mcode(Sum(x, (x, 1, n))) == 'Hold[Sum[x, {x, 1, n}]]'\n    assert mcode(Derivative(f(x), x)) == 'Hold[D[f[x], x]]'\n"], "sample_237": ["    def test_non_unique_username(self):\n        errors = check_user_model()\n        self.assertEqual(len(errors), 1)\n        self.assertIsInstance(errors[0], checks.Warning)\n        self.assertEqual(errors[0].id, 'auth.W004')\n        self.assertIn('is not unique', str(errors[0]))\n", "    def test_non_unique_username_field_warning(self):\n        errors = check_user_model(apps.get_app_configs())\n        self.assertEqual(len(errors), 1)\n        self.assertIsInstance(errors[0], checks.Warning)\n        self.assertEqual(errors[0].id, 'auth.W004')\n", "    def test_non_unique_username_field(self):\n        errors = check_user_model()\n        self.assertEqual(len(errors), 1)\n        self.assertIsInstance(errors[0], checks.Warning)\n        self.assertEqual(errors[0].id, 'auth.W004')\n", "    def test_non_unique_username_field(self):\n        errors = check_user_model(self.apps.get_app_configs())\n        self.assertEqual(len(errors), 1)\n        self.assertIsInstance(errors[0], checks.Warning)\n        self.assertEqual(errors[0].id, 'auth.W004')\n", "def test_username_field_not_unique(self):\n    errors = check_user_model(self.apps)\n    self.assertEqual(len(errors), 1)\n    self.assertEqual(errors[0].id, 'auth.W004')\n"], "sample_978": ["def test_bspline_basis_set_with_repeated_knots():\n    d = 1\n    knots = [0, 0, 2, 3, 4]\n    splines = bspline_basis_set(d, knots, x)\n    assert splines[0] == Piecewise((-x/2 + 1, And((x >= 0), (x <= 2))), (0, True))\n", "def test_bspline_basis_set_degree_zero():\n    d = 0\n    knots = range(5)\n    splines = bspline_basis_set(d, knots, x)\n    expected_splines = [\n        Piecewise((1, Interval(knots[n], knots[n + 1]).contains(x)), (0, True))\n        for n in range(len(knots) - 1)\n    ]\n    assert splines == expected_splines\n", "def test_bspline_basis_set():\n    d = 3\n    knots = [0, 1, 2, 3, 4, 5]\n    splines = bspline_basis_set(d, knots, x)\n    assert len(splines) == len(knots) - d - 1\n    assert splines[0] == Piecewise((x**3/6, (x >= 0) & (x <= 1)),\n                                  (-x**3/2 + 2*x**2 - 2*x + Rational(2, 3),\n                                   (x >= 1) & (x <= 2)),\n                                  (x**3/2 - 4*x**2 + 10*x - Rational(22, 3),\n                                   (x >= 2) & (x <= 3)),\n                                  (-x**3/6 + 2*x**2 - 8*x + Rational(32, 3),\n                                   (x >= 3) & (x <= 4)),\n                                  (0, True))\n    assert splines[1] == Piecewise((-x**3/6 + x**2/2 + x/3 + Rational(1, 6),\n                                   (x >= 1) & (x <= 2)),\n                                  (x**3/2 - 2*x**2 + Rational(7, 3),\n                                   (x >= 2) & (x <= 3)),\n                                  (-x**3/6 + 4*x**2 - 14*x + Rational(32, 3),\n                                   (x >= 3) & (x <= 4)),\n                                  (x**3/6 - 2*x**2 + 8*x - Rational(32, 3),\n                                   (x >= 4) & (x <= 5)),\n                                  (0, True))\n", "def test_bspline_basis_set_extended_knots():\n    d = 3\n    knots = [0, 0, 0, 1, 2, 3, 4, 4, 4]\n    splines = bspline_basis_set(d, knots, x)\n    assert len(splines) == len(knots) - d - 1\n    assert splines[0] == Piecewise((x**3 / 6, And(x >= 0, x <= 1)), (0, True))\n    assert splines[1] == Piecewise((-x**3 / 2 + 3*x**2 / 2, And(x >= 1, x <= 2)),\n                                   (x**3 / 2 - 6*x**2 + 12*x - 10, And(x >= 2, x <= 3)),\n                                   (-x**3 / 6 + 2*x**2 - 8*x + 32/3, And(x >= 3, x <= 4)),\n                                   (0, True))\n", "def test_bspline_basis_set():\n    d = 3\n    knots = range(10)\n    splines = bspline_basis_set(d, knots, x)\n    assert isinstance(splines, list)\n    assert len(splines) == len(knots) - d - 1\n    for spline in splines:\n        assert isinstance(spline, Piecewise)\n        for arg in spline.args:\n            assert isinstance(arg.cond, And)\n            assert isinstance(arg.cond.args[0], Interval)\n            assert isinstance(arg.cond.args[1], Interval)\n"], "sample_66": ["def test_get_signed_cookie(self):\n    request = HttpRequest()\n    request.COOKIES['my_cookie'] = signing.get_cookie_signer().sign('my_value')\n\n    self.assertEqual(request.get_signed_cookie('my_cookie'), 'my_value')\n\n    with self.assertRaises(KeyError):\n        request.get_signed_cookie('nonexistent_cookie')\n\n    request.COOKIES['bad_cookie'] = 'bad_value'\n    with self.assertRaises(signing.BadSignature):\n        request.get_signed_cookie('bad_cookie')\n\n    self.assertEqual(request.get_signed_cookie('bad_cookie', default='default_value'), 'default_value')\n", "def test_get_signed_cookie(self):\n    factory = RequestFactory()\n    request = factory.get('/', COOKIES={'mycookie': 'signed_value:signature'})\n\n    # Test a valid signed cookie\n    self.assertEqual(request.get_signed_cookie('mycookie', salt='salt'), 'signed_value')\n\n    # Test an invalid signed cookie\n    with self.assertRaises(BadSignature):\n        request.get_signed_cookie('mycookie', salt='wrong_salt')\n\n    # Test a missing signed cookie\n    with self.assertRaises(KeyError):\n        request.get_signed_cookie('missingcookie')\n\n    # Test a missing signed cookie with a default value\n    self.assertEqual(request.get_signed_cookie('missingcookie', default='default_value'), 'default_value')\n", "def test_get_full_path(self):\n    request = HttpRequest()\n    request.path = '/test/'\n    request.META = {'QUERY_STRING': 'param=value'}\n\n    full_path = request.get_full_path()\n    self.assertEqual(full_path, '/test/?param=value')\n\n    full_path_with_slash = request.get_full_path(force_append_slash=True)\n    self.assertEqual(full_path_with_slash, '/test/?param=value')\n\n    request.path = '/test'\n    full_path_with_slash = request.get_full_path(force_append_slash=True)\n    self.assertEqual(full_path_with_slash, '/test/?param=value')\n", "    def test_get_signed_cookie_default_value(self):\n        request = HttpRequest()\n        request.COOKIES['test_cookie'] = 'signed_value'\n        with self.settings(SECRET_KEY='secret-key'):\n            value = request.get_signed_cookie('test_cookie', default='default_value')\n            self.assertEqual(value, 'default_value')\n", "def test_get_full_path_with_query_string(self):\n    request = HttpRequest()\n    request.path = \"/test/\"\n    request.META = {\"QUERY_STRING\": \"param1=value1&param2=value2\"}\n\n    full_path = request.get_full_path()\n    self.assertEqual(full_path, \"/test/?param1=value1&param2=value2\")\n"], "sample_24": ["    def test_concatenate(self):\n        self.check2(np.concatenate, axis=0)\n        self.check2(np.concatenate, axis=1)\n", "    def test_flip(self):\n        self.check(np.flip)\n", "    def test_apply_over_axes(self):\n        self.check(np.apply_over_axes, np.sum, 0)\n", "    def test_shape_information(self):\n        funcs = {\n            np.shape, np.size, np.ndim,\n            np.reshape, np.ravel, np.moveaxis, np.rollaxis, np.swapaxes,\n            np.transpose, np.atleast_1d, np.atleast_2d, np.atleast_3d,\n            np.expand_dims, np.squeeze, np.broadcast_to, np.broadcast_arrays,\n            np.flip, np.fliplr, np.flipud, np.rot90,\n        }\n        for func in funcs:\n            self.check(func)\n", "    def test_stack(self):\n        self.check(np.stack, axis=1)\n        self.check2(np.stack, axis=1)\n"], "sample_218": ["    def test_trunc_second(self):\n        obj = self.create_model(datetime(2022, 1, 1, 12, 34, 56), None)\n        result = DTModel.objects.annotate(trunc_sec=TruncSecond('start_datetime')).get(id=obj.id).trunc_sec\n        self.assertEqual(result, truncate_to(obj.start_datetime, 'second'))\n", "    def test_extract_with_duration_field(self):\n        duration = timedelta(days=2, hours=3, minutes=4, seconds=5)\n        obj = self.create_model(datetime(2022, 1, 1, 0, 0, 0), datetime(2022, 1, 3, 3, 4, 5))\n        self.assertEqual(DTModel.objects.filter(duration=ExtractSecond('duration') + 3600 * 4 + 60 * 60).count(), 1)\n", "def test_trunc_with_timezone(self):\n    # Create a datetime object with a timezone\n    dt = datetime(2022, 1, 1, 12, 30, 0, tzinfo=datetime_timezone.utc)\n    model = self.create_model(dt, dt + timedelta(days=1))\n\n    # Test TruncYear with a timezone\n    result = DTModel.objects.filter(start_datetime__trunc_year__year=2022).first()\n    self.assertEqual(result, model)\n\n    # Test TruncMonth with a timezone\n    result = DTModel.objects.filter(start_datetime__trunc_month__month=1).first()\n    self.assertEqual(result, model)\n\n    # Test TruncWeek with a timezone\n    result = DTModel.objects.filter(start_datetime__trunc_week__week=1).first()\n    self.assertEqual(result, model)\n\n    # Test TruncDay with a timezone\n    result = DTModel.objects.filter(start_datetime__trunc_day__day=1).first()\n    self.assertEqual(result, model)\n\n    # Test TruncHour with a timezone\n    result = DTModel.objects.filter(start_datetime__trunc_hour__hour=12).first()\n    self.assertEqual(result, model)\n\n    # Test TruncMinute with a timezone\n    result = DTModel.objects.filter(start_datetime__trunc_minute__minute=30).first()\n    self.assertEqual(result, model)\n\n    # Test TruncSecond with a timezone\n    result = DTModel.objects.filter(start_datetime__trunc_second__second=0).first()\n    self.assertEqual(result, model)\n", "    def test_trunc(self):\n        dt = datetime(2022, 3, 15, 12, 30, 45)\n        obj = self.create_model(dt, dt + timedelta(hours=2))\n        trunc_hour = Trunc('start_datetime', 'hour')\n        self.assertEqual(\n            DTModel.objects.filter(pk=obj.pk).annotate(trunc_hour=trunc_hour).get().trunc_hour,\n            truncate_to(dt, 'hour')\n        )\n", "    def test_trunc_with_timezone(self):\n        dt = timezone.datetime(2016, 1, 1, 12, 0, 0, tzinfo=pytz.timezone('America/New_York'))\n        obj = self.create_model(dt, dt + timedelta(days=1))\n\n        result = DTModel.objects.annotate(truncated_datetime=Trunc('start_datetime', 'day', tzinfo=pytz.timezone('UTC'))).first()\n        self.assertEqual(result.truncated_datetime, truncate_to(dt, 'day', tzinfo=pytz.timezone('UTC')))\n\n        result = DTModel.objects.annotate(truncated_date=TruncDate('start_datetime', tzinfo=pytz.timezone('UTC'))).first()\n        self.assertEqual(result.truncated_date, truncate_to(dt, 'day', tzinfo=pytz.timezone('UTC')).date())\n\n        result = DTModel.objects.annotate(truncated_time=TruncTime('start_datetime', tzinfo=pytz.timezone('UTC'))).first()\n        self.assertEqual(result.truncated_time, truncate_to(dt, 'day', tzinfo=pytz.timezone('UTC')).time())\n"], "sample_813": ["def test_bayesian_ridge_scores():\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([1, 2, 3])\n    clf = BayesianRidge(n_iter=300, compute_score=True)\n    clf.fit(X, y)\n    assert len(clf.scores_) == clf.n_iter_ + 1\n    assert_array_less(np.diff(clf.scores_), 0)\n", "def test_bayesian_ridge_with_intercept():\n    X = diabetes.data[:10]\n    y = diabetes.target[:10]\n    reg = BayesianRidge(fit_intercept=True)\n    reg.fit(X, y)\n    assert reg.intercept_ is not None\n    assert_array_almost_equal(reg.coef_, reg.coef_, decimal=2)\n    assert_almost_equal(reg.alpha_, reg.alpha_, decimal=2)\n    assert_almost_equal(reg.lambda_, reg.lambda_, decimal=2)\n    assert_array_almost_equal(reg.sigma_, reg.sigma_, decimal=2)\n    assert_array_almost_equal(reg.predict(X), reg.predict(X), decimal=2)\n", "def test_ard_regression_convergence():\n    X, y = diabetes.data, diabetes.target\n    n_samples, n_features = X.shape\n\n    # Test convergence of ARDRegression\n    ard = ARDRegression(n_iter=1000, tol=1e-4)\n    ard.fit(X, y)\n\n    assert_equal(ard.n_iter_, 1000)  # Test that the maximum number of iterations was reached\n    assert_array_less(np.abs(ard.coef_ - np.zeros(n_features)), 1e-4)  # Test that the coefficients converged to zero\n", "def test_bayesian_ridge_with_sample_weights():\n    X = np.array([[1], [2], [3]])\n    y = np.array([1, 2, 3])\n    sample_weight = np.array([1, 0.5, 2])\n\n    br = BayesianRidge().fit(X, y, sample_weight=sample_weight)\n    br_without_weights = BayesianRidge().fit(X, y)\n\n    assert_array_almost_equal(br.coef_, br_without_weights.coef_)\n", "def test_bayesian_ridge_with_sample_weight():\n    X = np.array([[0], [1], [2]])\n    y = np.array([0, 1, 2])\n    sample_weight = np.array([1, 2, 3])\n\n    br = BayesianRidge(fit_intercept=False)\n    br_sw = BayesianRidge(fit_intercept=False)\n\n    br.fit(X, y)\n    br_sw.fit(X, y, sample_weight=sample_weight)\n\n    # Check that results are close to each other\n    assert_array_almost_equal(br.coef_, br_sw.coef_, decimal=2)\n"], "sample_736": ["def test_logistic_regression_path():\n    \"\"\"Test the logistic_regression_path function\"\"\"\n    coefs, Cs, n_iter = logistic_regression_path(X, Y1, Cs=np.logspace(-4, 4, 10))\n    assert_equal(coefs.shape, (10, 2))\n    assert_equal(Cs.shape, (10,))\n    assert_equal(n_iter.shape, (10,))\n\n    coefs, Cs, n_iter = logistic_regression_path(X, Y2, Cs=np.logspace(-4, 4, 10), multi_class='multinomial')\n    assert_equal(coefs.shape, (10, 3, 2))\n    assert_equal(Cs.shape, (10,))\n    assert_equal(n_iter.shape, (10,))\n", "def test_logistic_regression_path_multinomial():\n    X, y = make_classification(n_samples=100, n_features=20, n_informative=2,\n                               n_redundant=10, n_classes=3, random_state=42)\n    coefs, Cs, n_iter = logistic_regression_path(X, y, solver='lbfgs',\n                                                 multi_class='multinomial')\n    assert_equal(coefs.shape, (len(Cs), 3, 20))\n    assert_equal(n_iter.shape, (len(Cs),))\n    assert_greater(np.min(n_iter), 0)\n    assert_greater(np.max(n_iter), 0)\n", "def test_logistic_regression_path():\n    X = np.array([[0, 0], [1, 1], [2, 2], [3, 3]])\n    y = np.array([0, 0, 1, 1])\n    coefs, Cs, n_iter = logistic_regression_path(X, y, Cs=10)\n    assert_equal(len(coefs), len(Cs))\n    assert_equal(len(coefs), len(n_iter))\n    assert_true(np.all(Cs > 0))\n    assert_true(np.all(n_iter >= 0))\n    for coef in coefs:\n        assert_equal(coef.shape, (2,))\n", "def test_logistic_regression_path():\n    \"\"\"Test logistic_regression_path function\"\"\"\n    coefs, Cs, n_iter = logistic_regression_path(X, Y1, Cs=10, fit_intercept=True, solver='lbfgs')\n    assert_equal(len(coefs), 10)\n    assert_equal(len(Cs), 10)\n    assert_equal(len(n_iter), 10)\n    assert_greater(coefs[-1][1], 0)\n    assert_greater(coefs[-1][2], 0)\n\n    coefs, Cs, n_iter = logistic_regression_path(X, Y2, Cs=10, fit_intercept=True, solver='lbfgs', multi_class='multinomial')\n    assert_equal(len(coefs), 10)\n    assert_equal(len(Cs), 10)\n    assert_equal(len(n_iter), 10)\n    assert_greater(coefs[-1][0][1], 0)\n    assert_greater(coefs[-1][0][2], 0)\n    assert_greater(coefs[-1][1][0], 0)\n    assert_greater(coefs[-1][1][2], 0)\n    assert_greater(coefs[-1][2][0], 0)\n    assert_greater(coefs[-1][2][1], 0)\n", "def test_logistic_regression_path_multinomial():\n    X, y = make_classification(n_samples=100, n_features=20, n_informative=2,\n                               n_redundant=10, n_classes=3, random_state=0)\n    coefs, Cs, n_iter = logistic_regression_path(X, y, multi_class='multinomial')\n    assert_equal(coefs.shape, (len(Cs), 3, 21))  # 21 coefficients for intercept\n    assert_equal(n_iter.shape, (len(Cs),))\n\n    # Check that the last coefficient corresponds to the intercept\n    for coef, c in zip(coefs, Cs):\n        clf = LogisticRegression(C=c, fit_intercept=True, multi_class='multinomial',\n                                 solver='lbfgs', max_iter=100)\n        clf.fit(X, y)\n        assert_array_almost_equal(clf.coef_, coef[:, :-1], decimal=4)\n        assert_array_almost_equal(clf.intercept_, coef[:, -1], decimal=4)\n"], "sample_110": ["def test_pickleability_with_exclude(self):\n    qs = Happening.objects.exclude(happened_on__gt=datetime.date(2022, 1, 1))\n    self.assert_pickles(qs)\n", "def test_window_expression_pickleability(self):\n    qs = Happening.objects.annotate(\n        window=Window(\n            expression=F('id'),\n            partition_by=F('name'),\n            order_by=F('created_at'),\n            frame=RowRange(start=1, end=3)\n        )\n    )\n    self.assert_pickles(qs)\n", "def test_pickleability_of_queryset_with_annotate_and_window(self):\n    qs = Happening.objects.annotate(\n        num_events=models.Window(\n            expression=models.Count('events'),\n            partition_by=[models.F('date')]\n        )\n    )\n    self.assert_pickles(qs)\n", "def test_expression_pickleability(self):\n    expression = models.ExpressionWrapper(models.F('field'), output_field=models.IntegerField())\n    self.assert_pickles(expression)\n", "def test_pickleability_with_filters(self):\n    qs = Happening.objects.filter(name='Test')\n    self.assert_pickles(qs)\n"], "sample_166": ["    def test_salted_hmac(self):\n        key_salt = \"salt\"\n        value = \"value\"\n        secret = \"secret\"\n        algorithm = \"sha256\"\n        result = salted_hmac(key_salt, value, secret, algorithm=algorithm)\n        correct_result = hmac.new(hashlib.sha256(b\"saltsecret\").digest(), msg=b\"value\", digestmod=hashlib.sha256)\n        self.assertEqual(result.hexdigest(), correct_result.hexdigest())\n", "    def test_salted_hmac(self):\n        key_salt = \"test_salt\"\n        value = \"test_value\"\n        secret = \"test_secret\"\n        algorithm = \"sha256\"\n        result = salted_hmac(key_salt, value, secret, algorithm=algorithm)\n        expected_result = hashlib.pbkdf2_hmac(algorithm, value.encode(), (key_salt + secret).encode(), 1)\n        self.assertEqual(result.hexdigest(), hashlib.sha256(expected_result).hexdigest())\n", "    def test_salted_hmac(self):\n        key_salt = 'test_salt'\n        value = 'test_value'\n        secret = 'test_secret'\n        algorithm = 'sha1'\n        result = salted_hmac(key_salt, value, secret, algorithm=algorithm)\n        expected = hashlib.sha1(key_salt.encode() + secret.encode()).digest()\n        expected = hmac.new(expected, msg=value.encode(), digestmod=hashlib.sha1)\n        self.assertEqual(result.hexdigest(), expected.hexdigest())\n", "    def test_salted_hmac(self):\n        key_salt = 'test_salt'\n        value = 'test_value'\n        secret = 'test_secret'\n        algorithm = 'sha256'\n\n        result = salted_hmac(key_salt, value, secret, algorithm=algorithm)\n        expected_result = hmac.new(\n            hashlib.sha256(b'test_salt' + b'test_secret').digest(),\n            msg=b'test_value',\n            digestmod=hashlib.sha256\n        )\n\n        self.assertEqual(result.hexdigest(), expected_result.hexdigest())\n", "    def test_salted_hmac(self):\n        key_salt = \"testsalt\"\n        value = \"testvalue\"\n        secret = \"testsecret\"\n        algorithm = \"sha256\"\n        expected_output = b'\\xb0W\\xb1\\x91\\x96\\x07\\x82\\x15\\x8e\\x94\\x97\\x10\\x8b\\x76\\x04\\xdc\\xd9\\x87\\x78\\x85\\x4b\\x74\\x7d\\x82\\x6b\\xf0\\x37\\x7f\\x7f\\x8c\\x06\\x42\\xbb'\n        self.assertEqual(salted_hmac(key_salt, value, secret, algorithm=algorithm).digest(), expected_output)\n"], "sample_209": ["def test_save_base_with_update_fields(self):\n    article = Article.objects.create(headline='Original Headline')\n    original_id = article.id\n    article.headline = 'Updated Headline'\n    article.save_base(update_fields=['headline'])\n    updated_article = Article.objects.get(id=original_id)\n    self.assertEqual(updated_article.headline, 'Updated Headline')\n", "    def test_model_check_constraints(self):\n        errors = Model1._check_constraints(databases=['default'])\n        self.assertEqual(errors, [])\n\n        # Test with a database that doesn't support check constraints\n        connection = router.connections['default']\n        connection.features.supports_table_check_constraints = False\n        errors = Model1._check_constraints(databases=['default'])\n        self.assertEqual(len(errors), 1)\n        self.assertEqual(errors[0].id, 'models.W027')\n        connection.features.supports_table_check_constraints = True\n", "    def test_save_with_update_fields(self):\n        # Create a new instance of a model\n        model_instance = Model1(field1='original', field2='original')\n        model_instance.save()\n\n        # Update the instance with new values\n        model_instance.field1 = 'updated'\n        model_instance.save(update_fields=['field1'])\n\n        # Retrieve the instance from the database\n        updated_instance = Model1.objects.get(pk=model_instance.pk)\n\n        # Assert that only the specified field was updated\n        self.assertEqual(updated_instance.field1, 'updated')\n        self.assertEqual(updated_instance.field2, 'original')\n", "    def test_get_next_or_previous_by_FIELD(self):\n        obj1 = Worker.objects.create(name='John', order=1)\n        obj2 = Worker.objects.create(name='Jane', order=2)\n        obj3 = Worker.objects.create(name='Doe', order=3)\n\n        self.assertEqual(obj1.get_next_or_previous_by_FIELD('order', is_next=True), obj2)\n        self.assertEqual(obj2.get_next_or_previous_by_FIELD('order', is_next=True), obj3)\n        self.assertEqual(obj3.get_next_or_previous_by_FIELD('order', is_next=True), obj2)\n        self.assertEqual(obj3.get_next_or_previous_by_FIELD('order', is_next=False), obj2)\n        self.assertEqual(obj2.get_next_or_previous_by_FIELD('order', is_next=False), obj1)\n        self.assertEqual(obj1.get_next_or_previous_by_FIELD('order', is_next=False), obj3)\n\n        with self.assertRaises(Worker.DoesNotExist):\n            obj1.get_next_or_previous_by_FIELD('order', is_next=False, name='Jane')\n", "def test_save_method_with_update_fields(self):\n    # Create an instance of Model1\n    instance = Model1.objects.create(f1='old_value', f2='old_value')\n\n    # Update the instance with new values, specifying update_fields\n    instance.f1 = 'new_value'\n    instance.save(update_fields=['f1'])\n\n    # Refresh the instance from the database\n    instance.refresh_from_db()\n\n    # Assert that only the specified field was updated\n    self.assertEqual(instance.f1, 'new_value')\n    self.assertEqual(instance.f2, 'old_value')\n"], "sample_277": ["    def test_combine_with_empty_q(self):\n        q1 = Q(field1='value1')\n        q2 = Q()\n\n        combined_q = q1 & q2\n        self.assertEqual(combined_q, q1)\n\n        combined_q = q2 & q1\n        self.assertEqual(combined_q, q1)\n\n        combined_q = q1 | q2\n        self.assertEqual(combined_q, q1)\n\n        combined_q = q2 | q1\n        self.assertEqual(combined_q, q1)\n", "    def test_resolve_expression(self):\n        q = Q(field__lt=F('other_field'))\n        query = Mock()\n        query._add_q.return_value = (Mock(), Mock())\n        query.promote_joins.return_value = None\n        result = q.resolve_expression(query)\n        query._add_q.assert_called_once()\n        query.promote_joins.assert_called_once()\n        self.assertEqual(result, query._add_q.return_value[0])\n", "def test_q_combine(self):\n    q1 = Q(name='test1')\n    q2 = Q(name='test2')\n\n    # Test OR combination\n    combined_q = q1 | q2\n    self.assertEqual(str(combined_q), \"(OR: ('name', 'test1'), ('name', 'test2'))\")\n\n    # Test AND combination\n    combined_q = q1 & q2\n    self.assertEqual(str(combined_q), \"(AND: ('name', 'test1'), ('name', 'test2'))\")\n\n    # Test inversion\n    inverted_q = ~q1\n    self.assertEqual(str(inverted_q), \"(NOT (AND: ('name', 'test1')))\")\n", "def test_q_combine_with_different_connectors(self):\n    q1 = Q(name='foo')\n    q2 = Q(name='bar')\n\n    combined_q_and = q1 & q2\n    self.assertEqual(combined_q_and.connector, Q.AND)\n    self.assertEqual(len(combined_q_and.children), 2)\n    self.assertEqual(combined_q_and.children[0], ('name', 'foo'))\n    self.assertEqual(combined_q_and.children[1], ('name', 'bar'))\n\n    combined_q_or = q1 | q2\n    self.assertEqual(combined_q_or.connector, Q.OR)\n    self.assertEqual(len(combined_q_or.children), 2)\n    self.assertEqual(combined_q_or.children[0], ('name', 'foo'))\n    self.assertEqual(combined_q_or.children[1], ('name', 'bar'))\n", "def test_resolve_expression(self):\n    q = Q(field1='value1') | Q(field2='value2')\n    query = ...  # Assume query is a mocked or created Query instance\n    clause, joins = q.resolve_expression(query=query)\n    self.assertEqual(str(clause), '(\"test_app\".\"app_model\".\"field1\" = value1 OR \"test_app\".\"app_model\".\"field2\" = value2)')\n    self.assertEqual(len(joins), 0)  # No additional joins expected for this simple Q object\n"], "sample_41": ["def test_to_string_with_format():\n    # Test the to_string method with a specific format\n    unit = u.m / u.s\n    assert unit.to_string(format='latex') == r'\\frac{\\mathrm{m}}{\\mathrm{s}}'\n", "def test_unit_conversion_with_equivalencies():\n    # Test conversion with equivalencies\n    with u.add_enabled_equivalencies(u.dimensionless_angles()):\n        assert u.Quantity(1, u.cycle) == u.Quantity(2*np.pi, u.rad)\n", "def test_unit_decompose_bases():\n    # Test decomposing a unit into a specific set of bases\n    km = u.km\n    m = u.m\n    s = u.s\n    kmps = km / s\n    decomposed = kmps.decompose(bases=[m, s])\n    assert decomposed.scale == 1000\n    assert decomposed.bases == [m, s]\n    assert decomposed.powers == [1, -1]\n", "def test_unit_conversion():\n    # Test conversion between units\n    assert u.m.to(u.cm) == 100\n    assert u.cm.to(u.m) == 0.01\n    assert u.m.to(u.km) == 0.001\n    assert u.km.to(u.m) == 1000\n    assert u.s.to(u.ms) == 1000\n    assert u.ms.to(u.s) == 0.001\n", "def test_unit_decompose_with_bases():\n    # Test the decompose method with bases parameter\n    # Define a composite unit\n    my_unit = u.m / u.s\n\n    # Decompose with bases parameter set to meters and seconds\n    decomposed_unit = my_unit.decompose(bases=set([u.m, u.s]))\n\n    # Check if the decomposed unit is equal to the original unit\n    assert decomposed_unit == my_unit\n\n    # Decompose with bases parameter set to kilometers and hours\n    decomposed_unit = my_unit.decompose(bases=set([u.km, u.hour]))\n\n    # Check if the decomposed unit is equal to the expected unit\n    expected_unit = u.km / u.hour\n    assert decomposed_unit == expected_unit\n"], "sample_592": ["    def test_inline_variable_array_repr(self):\n        data = np.array([1, 2, 3, 4, 5])\n        var = xr.Variable('var', data)\n        max_width = 20\n        result = formatting.inline_variable_array_repr(var, max_width)\n        assert result == '1 2 ... 4 5'\n", "    def test_format_items(self):\n        # Test format_items function with different data types\n        # Test with numpy array of timestamps\n        timestamps = np.array(\n            [\"2022-01-01T00:00:00\", \"2022-01-02T12:34:56\", \"2022-01-03T23:59:59\"],\n            dtype=\"datetime64[ns]\",\n        )\n        expected_output = [\"2022-01-01\", \"2022-01-02T12:34:56\", \"2022-01-03T23:59:59\"]\n        assert formatting.format_items(timestamps) == expected_output\n\n        # Test with numpy array of timedeltas\n        timedeltas = np.array(\n            [\"1 days\", \"12:34:56\", \"3 days 23:59:59\"], dtype=\"timedelta64[ns]\"\n        )\n        expected_output = [\"1 days\", \"12:34:56\", \"3 days 23:59:59\"]\n        assert formatting.format_items(timedeltas) == expected_output\n\n        # Test with numpy array of strings\n        strings = np.array([\"apple\", \"banana\", \"cherry\"])\n        expected_output = [\"'apple'\", \"'banana'\", \"'cherry'\"]\n        assert formatting.format_items(strings) == expected_output\n\n        # Test with numpy array of floats\n        floats = np.array([1.23456789, 2.34567890, 3.45678901])\n        expected_output = [\"1.2346\", \"2.3457\", \"3.4568\"]\n        assert formatting.format_items(floats) == expected_output\n\n        # Test with numpy array of integers\n        integers = np.array([1, 2", "def test_last_item():\n    # Test with 1D array\n    array = np.array([1, 2, 3])\n    assert formatting.last_item(array) == [3]\n\n    # Test with 2D array\n    array = np.array([[1, 2], [3, 4]])\n    assert formatting.last_item(array) == [4]\n\n    # Test with empty array\n    array = np.array([])\n    assert formatting.last_item(array) == []\n", "    def test_summarize_coord(self):\n        ds = xr.Dataset({'a': ('x', [1, 2, 3])}, coords={'x': ('x', [4, 5, 6])})\n        summary = formatting.summarize_coord('x', ds['x'], 10)\n        assert summary == \"  * x        (x) int64 4 5 6\"\n", "def test_short_numpy_repr(self):\n    arr = np.arange(100).reshape(10, 10)\n    repr_str = formatting.short_numpy_repr(arr)\n    assert \"array([[ 0,  1,  2, ...,  7,  8,  9],\" in repr_str\n    assert \"[...,  70,  71,  72],\" in repr_str\n    assert \"[...,  90,  91,  92]])\" in repr_str\n\n    arr = np.arange(200).reshape(20, 10)\n    repr_str = formatting.short_numpy_repr(arr)\n    assert \"array([[  0,   1,   2, ...,   7,   8,   9],\\n\" in repr_str\n    assert \"       [..., 170, 171, 172],\\n\" in repr_str\n    assert \"       [..., 190, 191, 192]])\" in repr_str\n"], "sample_526": ["def test_date_converter_with_interval_multiples():\n    converter = mdates.DateConverter(interval_multiples=True)\n    unit = dateutil.tz.gettz('UTC')\n    axis = plt.gca()\n\n    axisinfo = converter.axisinfo(unit, axis)\n    assert isinstance(axisinfo.majloc, mdates.AutoDateLocator)\n    assert axisinfo.majloc._interval_multiples is True\n    assert isinstance(axisinfo.majfmt, mdates.AutoDateFormatter)\n    assert axisinfo.majfmt._locator is axisinfo.majloc\n", "def test_auto_date_formatter_with_custom_callables():\n        x = mdates.num2date(x)\n        if pos == 0:\n            fmt = '%D %H:%M:%S.%f'\n        else:\n            fmt = '%H:%M:%S.%f'\n        label = x.strftime(fmt)\n        label = label.rstrip(\"0\")\n        label = label.rstrip(\".\")\n        return label\n\n    locator = mdates.AutoDateLocator()\n    formatter = mdates.AutoDateFormatter(locator)\n    formatter.scaled[1/(24*60)] = my_format_function\n\n    # Test with some sample data\n    dates = mdates.drange(datetime.datetime(2022, 1, 1), datetime.datetime(2022, 1, 2), datetime.timedelta(minutes=15))\n    labels = formatter.format_ticks(dates)\n\n    assert len(labels) == len(dates)\n    # Add more assertions based on the expected output of my_format_function\n", "def test_auto_date_locator_maxticks():\n    with rc_context(rc={\"date.converter\": \"auto\"}):\n        locator = mdates.AutoDateLocator(maxticks={mdates.YEARLY: 10, mdates.MONTHLY: 5})\n        assert locator.maxticks[mdates.YEARLY] == 10\n        assert locator.maxticks[mdates.MONTHLY] == 5\n", "def test_datestr2num():\n    # Test converting date string to number\n    d = \"2022-01-01 12:00:00\"\n    num = mdates.datestr2num(d)\n    assert isinstance(num, float)\n    assert num == 738216.5\n\n    # Test converting list of date strings to numbers\n    d_list = [\"2022-01-01 12:00:00\", \"2022-01-02 12:00:00\"]\n    num_list = mdates.datestr2num(d_list)\n    assert isinstance(num_list, np.ndarray)\n    assert np.array_equal(num_list, np.array([738216.5, 738217.5]))\n", "def test_auto_date_formatter_custom_format_function():\n        x = mdates.num2date(x)\n        if pos == 0:\n            fmt = '%D %H:%M:%S.%f'\n        else:\n            fmt = '%H:%M:%S.%f'\n        label = x.strftime(fmt)\n        label = label.rstrip(\"0\")\n        label = label.rstrip(\".\")\n        return label\n\n    locator = mdates.AutoDateLocator()\n    formatter = mdates.AutoDateFormatter(locator)\n    formatter.scaled[1/(24*60)] = my_format_function\n\n    # Test the custom format function with some sample data\n    dates = mdates.date2num(np.array(['2022-01-01T00:00:00', '2022-01-01T00:00:05', '2022-01-01T00:00:10'], dtype='datetime64'))\n    formatted_dates = [formatter(date) for date in dates]\n    assert formatted_dates == ['Jan 01 00:00:00', '00:00:05', '00:00:10']\n"], "sample_289": ["    def test_init(self):\n        d = MultiValueDict({'name': ['Adrian', 'Simon'], 'position': ['Developer']})\n        self.assertEqual(d['name'], 'Simon')\n        self.assertEqual(d.getlist('name'), ['Adrian', 'Simon'])\n        self.assertEqual(d.getlist('doesnotexist'), [])\n        self.assertEqual(d.getlist('doesnotexist', ['Adrian', 'Simon']), ['Adrian', 'Simon'])\n        self.assertEqual(d.get('lastname', 'nonexistent'), 'nonexistent')\n", "    def setUp(self):\n        self.md = MultiValueDict({'name': ['Adrian', 'Simon'], 'position': ['Developer']})\n", "    def test_ordered_set_operations(self):\n        s = OrderedSet(['a', 'b', 'c', 'd'])\n\n        self.assertIn('a', s)\n        self.assertNotIn('z', s)\n\n        s.add('e')\n        self.assertIn('e', s)\n\n        s.remove('b')\n        self.assertNotIn('b', s)\n\n        s.discard('b')  # No error if the item is not in the set\n\n        s.discard('c')\n        self.assertNotIn('c', s)\n\n        self.assertEqual(len(s), 3)\n\n        self.assertEqual(list(s), ['a', 'd', 'e'])\n        self.assertEqual(list(reversed(s)), ['e', 'd', 'a'])\n\n        s.clear()\n        self.assertFalse(s)\n\n        s.add('x')\n        self.assertTrue(s)\n", "    def test_ordered_set(self):\n        s = OrderedSet(['a', 'b', 'c', 'd', 'e', 'b', 'c'])\n        self.assertEqual(len(s), 5)\n        self.assertListEqual(list(s), ['a', 'b', 'c', 'd', 'e'])\n        self.assertIn('b', s)\n        self.assertNotIn('z', s)\n        s.remove('b')\n        self.assertListEqual(list(s), ['a', 'c', 'd', 'e'])\n        s.discard('z')  # should not raise\n        s.add('f')\n        self.assertListEqual(list(reversed(s)), ['f', 'e', 'd', 'c', 'a'])\n        s.discard('c')\n        self.assertListEqual(list(s), ['a', 'd', 'e', 'f'])\n        self.assertTrue(s)\n        s.remove('a')\n        s.remove('d')\n        s.remove('e')\n        s.remove('f')\n        self.assertFalse(s)\n", "    def test_ordered_set(self):\n        s = OrderedSet(['a', 'b', 'c', 'd', 'e'])\n        self.assertIn('a', s)\n        self.assertNotIn('z', s)\n        self.assertEqual(len(s), 5)\n        s.remove('b')\n        self.assertNotIn('b', s)\n        self.assertEqual(len(s), 4)\n        s.discard('z')  # should not raise\n        s.add('f')\n        self.assertIn('f', s)\n        self.assertEqual(len(s), 5)\n        s.discard('a')\n        s.discard('c')\n        s.discard('e')\n        self.assertEqual(len(s), 2)\n        self.assertEqual(list(s), ['d', 'f'])\n        self.assertEqual(list(reversed(s)), ['f', 'd'])\n        s.clear()\n        self.assertFalse(s)\n        self.assertEqual(len(s), 0)\n"], "sample_470": ["def test_cached_property(self):\n    class TestClass:\n        @cached_property\n            return \"cached value\"\n\n    obj = TestClass()\n    self.assertEqual(obj.test_property, \"cached value\")\n    with mock.patch.object(TestClass, \"test_property\") as mock_method:\n        self.assertEqual(obj.test_property, \"cached value\")\n        mock_method.assert_not_called()\n", "def test_lazy_object(self):\n        return 'test'\n\n    lazy_object = SimpleLazyObject(func)\n    self.assertEqual(lazy_object._wrapped, empty)\n    self.assertEqual(str(lazy_object), 'test')\n    self.assertIs(lazy_object._wrapped, 'test')\n\n    # Test __copy__ and __deepcopy__\n    lazy_object_copy = copy.copy(lazy_object)\n    self.assertIs(lazy_object_copy._wrapped, 'test')\n    lazy_object_deepcopy = copy.deepcopy(lazy_object)\n    self.assertIs(lazy_object_deepcopy._wrapped, 'test')\n\n    # Test uninitialized __copy__ and __deepcopy__\n    lazy_object_uninitialized = SimpleLazyObject(func)\n    lazy_object_copy_uninitialized = copy.copy(lazy_object_uninitialized)\n    self.assertEqual(lazy_object_copy_uninitialized._wrapped, empty)\n    lazy_object_deepcopy_uninitialized = copy.deepcopy(lazy_object_uninitialized)\n    self.assertEqual(lazy_object_deepcopy_uninitialized._wrapped, empty)\n", "    def test_lazy_evaluation(self):\n        func = mock.Mock(return_value='result')\n        lazy_func = lazy(func, str)\n        result = lazy_func('arg')\n        func.assert_not_called()\n        self.assertEqual(str(result), 'result')\n        func.assert_called_once_with('arg')\n", "def test_lazy_object_setup(self):\n        return \"test\"\n\n    lazy_obj = SimpleLazyObject(test_func)\n    self.assertEqual(lazy_obj._wrapped, empty)\n    self.assertEqual(str(lazy_obj), \"test\")\n    self.assertEqual(lazy_obj._wrapped, \"test\")\n", "def test_lazy_function_with_multiple_arguments(self):\n        return a * b\n\n    lazy_func = lazy(func, int)\n    result = lazy_func(5, 6)\n    self.assertIsInstance(result, lazy._lazy_proxy__)\n    self.assertEqual(result, 30)\n"], "sample_121": ["    def test_valid_unique_together(self):\n        class ValidModel(models.Model):\n            field1 = models.CharField(max_length=10)\n            field2 = models.IntegerField()\n\n            class Meta:\n                unique_together = (('field1', 'field2'),)\n\n        self.assertEqual(ValidModel.check(), [])\n", "    def setUpClass(cls):\n        super().setUpClass()\n        cls.msg = \"'unique_together' refers to the nonexistent field\"\n\n        # Model with unique_together referring to nonexistent fields.\n        class UniqueTogetherNonexistentFields(models.Model):\n            field1 = models.CharField(max_length=10)\n            field2 = models.CharField(max_length=10)\n\n            class Meta:\n                unique_together = [('field1', 'nonexistent')]\n\n        # Model with unique_together referring to ManyToManyField.\n        class UniqueTogetherManyToManyField(models.Model):\n            field1 = models.CharField(max_length=10)\n            field2 = models.ManyToManyField('self')\n\n            class Meta:\n                unique_together = [('field1', 'field2')]\n\n        cls.UniqueTogetherNonexistentFields = UniqueTogetherNonexistentFields\n        cls.UniqueTogetherManyToManyField = UniqueTogetherManyToManyField\n", "    def test_unique_together(self):\n        class UniqueTogetherModel(models.Model):\n            field1 = models.CharField(max_length=20)\n            field2 = models.CharField(max_length=20)\n\n            class Meta:\n                unique_together = [('field1', 'field2')]\n\n        errors = UniqueTogetherModel.check()\n        self.assertEqual(len(errors), 0)\n\n        UniqueTogetherModel.objects.create(field1='test', field2='test')\n        with self.assertRaises(models.IntegrityError):\n            UniqueTogetherModel.objects.create(field1='test', field2='test')\n", "    def test_index_together_local_fields(self):\n        class InvalidIndexTogetherLocalField(models.Model):\n            field1 = models.CharField(max_length=10)\n            field2 = models.CharField(max_length=10)\n\n            class Meta:\n                index_together = ['field3', 'field2']\n\n        errors = InvalidIndexTogetherLocalField.check()\n        expected_error = Error(\n            \"'index_together' refers to the nonexistent field 'field3'.\",\n            obj=InvalidIndexTogetherLocalField,\n            id='models.E012',\n        )\n        self.assertEqual(errors, [expected_error])\n", "    def test_unique_together_check(self):\n        class BadUniqueTogether(models.Model):\n            a = models.IntegerField()\n            b = models.IntegerField()\n            c = models.IntegerField()\n\n            class Meta:\n                unique_together = (\n                    ('a', 'b'),\n                    'c',\n                )\n\n        errors = BadUniqueTogether.check()\n        self.assertEqual(len(errors), 1)\n        self.assertEqual(errors[0].id, 'models.E012')\n        self.assertEqual(\n            str(errors[0]),\n            \"'unique_together' refers to the nonexistent field 'c'.\"\n        )\n"], "sample_1206": ["def test_float_conversion():\n    assert Float(3.14) == Float(314e-2)\n", "def test_igcd():\n    assert igcd(0, 0) == 0\n    assert igcd(1, 0) == 1\n    assert igcd(0, 1) == 1\n    assert igcd(1, 1) == 1\n    assert igcd(2, 3) == 1\n    assert igcd(6, 8) == 2\n    assert igcd(12, 18) == 6\n    assert igcd(12345678901234567890, 98765432109876543210) == 10\n", "def test_int_as_mpf_val():\n    prec = 100\n    i = Integer(42)\n    mpf_val = i._as_mpf_val(prec)\n    assert mpf_val == mpf(42)\n", "def test_as_content_primitive():\n    r = Rational(6, 9)\n    content, primitive = r.as_content_primitive()\n    assert content == 2\n    assert primitive == Rational(1, 3)\n", "compilation error"], "sample_929": ["def test_parse_annotation_with_env(app):\n    env = Mock()\n    result = _parse_annotation(\"List[int]\", env)\n    assert len(result) == 5\n    assert isinstance(result[0], pending_xref)\n    assert isinstance(result[1], nodes.Text)\n    assert isinstance(result[2], desc_sig_punctuation)\n    assert isinstance(result[3], pending_xref)\n    assert isinstance(result[4], desc_sig_punctuation)\n", "def test_parse_annotation_with_env(app, env):\n    annotation = \"Optional[List[str]]\"\n    result = _parse_annotation(annotation, env)\n    expected = [pending_xref('', nodes.Text('Optional'), refdomain='py', reftype='class', reftarget='Optional'),\n                addnodes.desc_sig_punctuation('', '['),\n                pending_xref('', nodes.Text('List'), refdomain='py', reftype='class', reftarget='List'),\n                addnodes.desc_sig_punctuation('', '['),\n                nodes.Text('str'),\n                addnodes.desc_sig_punctuation('', ']'),\n                addnodes.desc_sig_punctuation('', ']')]\n    assert_node(result, expected)\n", "def test_parse_annotation(app):\n    env = app.builder.env\n\n    # Test parsing a simple type annotation\n    annotation = 'int'\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 1\n    assert_node(result[0], [nodes.Text(annotation)])\n\n    # Test parsing a type annotation with a subscript\n    annotation = 'List[int]'\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 5\n    assert_node(result[0], [nodes.Text('List')])\n    assert_node(result[1], [nodes.Text('[')])\n    assert_node(result[2], [nodes.Text('int')])\n    assert_node(result[3], [nodes.Text(']')])\n    assert_node(result[4], [])\n\n    # Test parsing a type annotation with a nested subscript\n    annotation = 'Dict[str, List[int]]'\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 10\n    assert_node(result[0], [nodes.Text('Dict')])\n    assert_node(result[1], [nodes.Text('[')])\n    assert_node(result[2], [nodes.Text('str')])\n    assert_node(result[3], [nodes.Text(',')])\n    assert_node(result[4], [nodes.Text('List')])\n    assert_node(result[5], [nodes.Text('[')])\n    assert_node(result[6], [nodes.Text('int')])\n    assert_node(result[7], [nodes.Text(']')])\n    assert_node(result[8], [nodes.Text(']')])\n    assert_node(result[9], [])\n", "def test_parse_annotation():\n    env = Mock()\n    result = _parse_annotation(\"List[int]\", env)\n    expected = [pending_xref(reftype='class', reftarget='List', refdomain='py'),\n                addnodes.desc_sig_punctuation('', '['),\n                pending_xref(reftype='class', reftarget='int', refdomain='py'),\n                addnodes.desc_sig_punctuation('', ']')]\n    assert_node(result, expected)\n", "def test_parse_annotation(app):\n    env = app.builder.env\n    result = _parse_annotation(\"List[int]\", env)\n    assert_node(result[0], [\n        pending_xref(reftype=\"class\", reftarget=\"List\", refdomain=\"py\"),\n        nodes.Text(\"[\")\n    ])\n    assert_node(result[2], [\n        pending_xref(reftype=\"class\", reftarget=\"int\", refdomain=\"py\"),\n        nodes.Text(\"]\")\n    ])\n\n    result = _parse_annotation(\"Tuple[str, ...]\", env)\n    assert_node(result[0], [\n        pending_xref(reftype=\"class\", reftarget=\"Tuple\", refdomain=\"py\"),\n        nodes.Text(\"[\")\n    ])\n    assert_node(result[2], [\n        pending_xref(reftype=\"class\", reftarget=\"str\", refdomain=\"py\"),\n        nodes.Text(\",\")\n    ])\n    assert_node(result[4], [\n        nodes.Text(\"...\"),\n        nodes.Text(\"]\")\n    ])\n"], "sample_674": ["def test_ischildnode(baseid, nodeid, expected):\n    assert nodes.ischildnode(baseid, nodeid) == expected\n", "def test_ischildnode(baseid, nodeid, expected):\n    assert nodes.ischildnode(baseid, nodeid) == expected\n", "def test_ischildnode(baseid, nodeid, expected):\n    result = nodes.ischildnode(baseid, nodeid)\n    assert result == expected\n", "def test_ischildnode(baseid, nodeid, expected):\n    assert nodes.ischildnode(baseid, nodeid) == expected\n", "def test_ischildnode(baseid, nodeid, expected):\n    assert nodes.ischildnode(baseid, nodeid) == expected\n"], "sample_887": ["def test_calibrated_classifier_cv_with_sample_weights(data, method, ensemble):\n    X, y = data\n    estimator = LogisticRegression()\n    calibrator = CalibratedClassifierCV(estimator, method=method, cv=5, ensemble=ensemble)\n\n    sample_weight = np.random.rand(N_SAMPLES)\n    calibrator.fit(X, y, sample_weight=sample_weight)\n\n    proba = calibrator.predict_proba(X)\n    assert proba.shape == (N_SAMPLES, 2)\n    assert np.all(proba >= 0) and np.all(proba <= 1)\n", "def test_calibrated_classifier_cv_decision_function(data, method, ensemble):\n    X, y = data\n    estimator = DecisionTreeClassifier(random_state=42)\n    calibrator = CalibratedClassifierCV(estimator=estimator, cv=3, method=method, ensemble=ensemble)\n    calibrator.fit(X, y)\n\n    assert hasattr(calibrator, \"calibrated_classifiers_\")\n    for calibrated_classifier in calibrator.calibrated_classifiers_:\n        assert hasattr(calibrated_classifier, \"estimator\")\n        assert hasattr(calibrated_classifier, \"calibrators\")\n\n    y_pred_proba = calibrator.predict_proba(X)\n    assert y_pred_proba.shape == (N_SAMPLES, 2)\n    assert np.all(np.isfinite(y_pred_proba))\n", "def test_calibrated_classifier_cv_sample_weights(data, method, ensemble):\n    X, y = data\n    sample_weight = np.random.rand(N_SAMPLES)\n    estimator = LogisticRegression(solver=\"lbfgs\", multi_class=\"auto\")\n    calibrator = CalibratedClassifierCV(estimator, method=method, cv=2, ensemble=ensemble)\n    calibrator.fit(X, y, sample_weight=sample_weight)\n\n    assert hasattr(calibrator, \"calibrated_classifiers_\")\n    assert len(calibrator.calibrated_classifiers_) == (1 if ensemble else 2)\n    for calibrated_classifier in calibrator.calibrated_classifiers_:\n        assert hasattr(calibrated_classifier, \"estimator\")\n        assert hasattr(calibrated_classifier, \"calibrators\")\n", "def test_calibrated_classifier_cv_with_binary_estimator_and_custom_sample_weights(data, method, ensemble):\n    X, y = data\n    y = y > 0  # Binary problem\n\n    sample_weights = np.random.RandomState(42).rand(N_SAMPLES)\n\n    estimator = LogisticRegression(random_state=42)\n    calibrator = CalibratedClassifierCV(estimator=estimator, method=method, cv=\"prefit\", ensemble=ensemble)\n    calibrator.fit(X, y, sample_weight=sample_weights)\n\n    assert len(calibrator.calibrated_classifiers_) == 1\n    assert isinstance(calibrator.calibrated_classifiers_[0].calibrators[0], _SigmoidCalibration if method == \"sigmoid\" else IsotonicRegression)\n    assert_array_almost_equal(calibrator.predict_proba(X), calibrator.predict_proba(X[::-1]))  # Test symmetry\n", "def test_calibrated_classifier_cv_sample_weights_invariance(data):\n    X, y = data\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.5, random_state=42\n    )\n\n    # create a classifier with sample weights support\n    clf = LogisticRegression()\n    calibrator = CalibratedClassifierCV(clf, cv=2, method=\"sigmoid\", ensemble=True)\n\n    # fit with sample weights\n    sample_weight = np.random.rand(len(y_train))\n    calibrator.fit(X_train, y_train, sample_weight=sample_weight)\n    proba_with_weights = calibrator.predict_proba(X_test)\n\n    # fit without sample weights\n    calibrator.fit(X_train, y_train)\n    proba_without_weights = calibrator.predict_proba(X_test)\n\n    # check that removing a sample is not strictly equal to putting its weight to zero\n    assert not np.allclose(proba_with_weights, proba_without_weights)\n\n    # check that sample weights are taken into account when fitting the calibrator\n    sample_weight_test = np.random.rand(len(y_test))\n    calibrator_with_weights_test = CalibratedClassifierCV(\n        clf, cv=2, method=\"sigmoid\", ensemble=False\n    )\n    calibrator_with_weights_test.fit(X_train, y_train)\n    proba_with_weights_test = calibrator_with_weights_test.predict_proba(X_test)\n    calibrator_with_weights_test.calibrated_classifiers_[0].calibrators[0].fit(\n        proba_with_weights_test[:, 1], y_test, sample_weight=sample_weight_test\n    )\n    proba_with_weights_test_calibrated = calibrator_with_weights_test.predict_proba(\n       "], "sample_957": ["def test_restify():\n    assert restify(MyClass1) == ':class:`test_util_typing.MyClass1`'\n    assert restify(MyClass2) == ':class:`test_util_typing.MyClass2`'\n    assert restify(T) == ':obj:`~typing.T`'\n    assert restify(MyInt) == ':class:`MyInt`'\n    assert restify(MyList[int]) == ':class:`test_util_typing.MyList`\\\\ [:class:`int`]'\n    assert restify(Optional[int]) == ':obj:`~typing.Optional`\\\\ [:class:`int`]'\n    assert restify(Union[int, str]) == ':obj:`~typing.Union`\\\\ [:class:`int`, :class:`str`]'\n    assert restify(Union[int, None]) == ':obj:`~typing.Optional`\\\\ [:class:`int`]'\n    assert restify(Union[None, int]) == ':obj:`~typing.Optional`\\\\ [:class:`int`]'\n    assert restify(Union[int, str, None]) == ':obj:`~typing.Optional`\\\\ [:obj:`~typing.Union`\\\\ [:class:`int`, :class:`str`]]'\n    assert restify(Callable[[int], str]) == ':obj:`~typing.Callable`\\\\ [[:class:`int`], :class:`str`]'\n    assert restify(Struct('I')) == ':class:`struct.Struct`'\n    assert restify(TracebackType) == ':class:`types.TracebackType`'\n", "def test_restify():\n    assert restify(MyClass1) == ':class:`test_util_typing.MyClass1`'\n    assert restify(MyClass2) == ':class:`test_util_typing.MyClass2`'\n    assert restify(T) == ':obj:`~typing.T`'\n    assert restify(MyInt) == ':class:`MyInt`'\n    assert restify(MyList[int]) == ':class:`test_util_typing.MyList`\\\\ [:obj:`int`]'\n    assert restify(Union[int, str]) == ':obj:`~typing.Union`\\\\ [:obj:`int`, :obj:`str`]'\n    assert restify(Optional[int]) == ':obj:`~typing.Optional`\\\\ [:obj:`int`]\n    assert restify(Callable[[int], str]) == ':obj:`~typing.Callable`\\\\ [[int], str]'\n    assert restify(BrokenType) == ':obj:`<class \\'test_util_typing.BrokenType\\'>`'\n    assert restify(Struct) == ':class:`struct.Struct`'\n    assert restify(TracebackType) == ':class:`types.TracebackType`'\n    assert restify(None) == ':obj:`None`'\n    assert restify(Ellipsis) == '...'\n", "def test_restify_and_stringify():\n    assert restify(MyClass1) == ':class:`~sphinx.util.typing.MyClass1`'\n    assert stringify(MyClass1) == 'sphinx.util.typing.MyClass1'\n\n    assert restify(MyClass2) == ':class:`MyClass2`'\n    assert stringify(MyClass2) == '<MyClass2>'\n\n    assert restify(MyInt) == ':class:`MyInt`'\n    assert stringify(MyInt) == 'MyInt'\n\n    assert restify(MyList) == ':class:`~sphinx.util.typing.MyList`'\n    assert stringify(MyList) == 'sphinx.util.typing.MyList'\n\n    assert restify(BrokenType) == ':class:`int`'\n    assert stringify(BrokenType) == '<class \\'sphinx.util.typing.BrokenType\\'>'\n\n    assert restify(Union[int, str]) == ':obj:`~typing.Union`\\\\ [:class:`int`, :class:`str`]'\n    assert stringify(Union[int, str]) == 'Union[int, str]'\n\n    assert restify(Optional[int]) == ':obj:`~typing.Optional`\\\\ [:class:`int`]'\n    assert stringify(Optional[int]) == 'Optional[int]'\n\n    assert restify(Tuple[int, ...]) == ':class:`~typing.Tuple`\\\\ [:class:`int`, :obj:`Ellipsis`]'\n    assert stringify(Tuple[int, ...]) == 'Tuple[int, ...]'\n\n    assert restify(Callable[[int], str]) == ':obj:`~typing.Callable`\\\\ [[ :class:`int`], :class:`str`]'\n    assert stringify(Callable[[int], str]) == 'Callable[[int], str]'\n\n    assert restify(Integral) == ':class:`numbers.Integral`'\n    assert stringify(Integral) == 'numbers.Integral'\n\n", "def test_restify():\n    assert restify(None) == ':obj:`None`'\n    assert restify(Ellipsis) == '...'\n    assert restify(TracebackType) == ':class:`types.TracebackType`'\n    assert restify(Struct) == ':class:`struct.Struct`'\n    assert restify(MyInt) == ':class:`MyInt`'\n    assert restify(Union[int, None]) == 'Optional[int]'\n    assert restify(Union[int, str]) == 'int | str'\n    assert restify(Union[MyClass1, MyClass2]) == ':class:`%s.MyClass1` | :class:`%s.<MyClass2>`' % (__name__, __name__)\n    assert restify(MyList[int]) == ':class:`%s.MyList`\\\\ [int]' % __name__\n    assert restify(BrokenType) == '<BrokenType>'  # restify() should not raise an exception for broken types\n", "def test_stringify():\n    assert stringify(MyClass1) == 'MyClass1'\n    assert stringify(MyClass2) == 'MyClass2'\n    assert stringify(MyInt) == 'MyInt'\n    assert stringify(MyList[T]) == 'MyList'\n    assert stringify(Union[int, str]) == 'Union[int, str]'\n    assert stringify(Optional[int]) == 'Optional[int]'\n    assert stringify(Callable[[int], str]) == 'Callable[[int], str]'\n    assert stringify(Any) == 'Any'\n    assert stringify(Integral) == 'numbers.Integral'\n    assert stringify(Struct('i')) == 'struct.Struct'\n    assert stringify(TracebackType) == 'types.TracebackType'\n    assert stringify(BrokenType) == \"<class 'test_util_typing.BrokenType'>\"\n\n    if sys.version_info >= (3, 8):\n        from typing import Literal, TypedDict\n        assert stringify(Literal[1, 'a']) == 'Literal[1, \\'a\\']'\n        class MyTypedDict(TypedDict):\n            a: int\n            b: str\n        assert stringify(MyTypedDict) == 'MyTypedDict'\n\n    if sys.version_info >= (3, 10):\n        assert stringify(int | str) == 'int | str'\n        assert stringify(int | None) == 'int | None'\n"], "sample_25": ["def test_card_fromstring(self):\n    image = \"KEYWORD  = 'This is a test' / Test comment\"\n    card = fits.Card.fromstring(image)\n    assert card.keyword == \"KEYWORD\"\n    assert card.value == \"This is a test\"\n    assert card.comment == \"Test comment\"\n", "def test_card_with_rvkc(self):\n    card = fits.Card(\"DP1.AXIS.1\", 2)\n    assert card.keyword == \"DP1.AXIS.1\"\n    assert card.value == 2\n    assert card.rawkeyword == \"DP1\"\n    assert card.rawvalue == \"AXIS.1: 2\"\n    assert card.field_specifier == \"AXIS.1\"\n    assert card.image == \"DP1     = 'AXIS.1: 2'                \"\n", "def test_card_value_type(self):\n    card = fits.Card(\"TESTKEY\", value=10.5)\n    assert isinstance(card.value, float)\n\n    card = fits.Card(\"TESTKEY\", value=10)\n    assert isinstance(card.value, int)\n\n    card = fits.Card(\"TESTKEY\", value=\"test\")\n    assert isinstance(card.value, str)\n\n    card = fits.Card(\"TESTKEY\", value=True)\n    assert isinstance(card.value, bool)\n\n    card = fits.Card(\"TESTKEY\", value=10 + 5j)\n    assert isinstance(card.value, complex)\n", "def test_card_image(self):\n    card = fits.Card(\"TEST\", 42, \"A test card\")\n    assert card.image == \"TEST                 =                    42 / A test card\"\n", "def test_card_with_non_ascii_characters(self):\n    # Test that a card with non-ASCII characters raises a ValueError\n    with pytest.raises(ValueError):\n        fits.Card('NONASCII', 'caf\u00e9')\n"], "sample_151": ["def test_altered_order_with_respect_to(self):\n    changes = self.get_changes(\n        [self.author_empty, self.book_with_author_renamed],\n        [self.author_renamed_with_book, self.book_with_author_renamed],\n    )\n    self.assertOperationTypes(changes, 'testapp', 0, ['AlterOrderWithRespectTo'])\n", "def test_deep_deconstruct(self):\n    ad = self.get_changes([], [self.author_name_deconstructible_1])\n    ad_deconstructed = ad['testapp'][0].operations[0].field.default.deconstruct()\n    self.assertEqual(ad_deconstructed, ('testapp.models.DeconstructibleObject', (), {}))\n\n    ad = self.get_changes([], [self.author_name_deconstructible_list_1])\n    ad_deconstructed = ad['testapp'][0].operations[0].field.default.deconstruct()\n    self.assertEqual(ad_deconstructed, (\n        '__main__.DeconstructibleObject',\n        (),\n        {}\n    ), [ad_deconstructed[0]])\n    self.assertEqual(ad_deconstructed[1], [123])\n\n    ad = self.get_changes([], [self.author_name_deconstructible_tuple_1])\n    ad_deconstructed = ad['testapp'][0].operations[0].field.default.deconstruct()\n    self.assertEqual(ad_deconstructed, (\n        '__main__.DeconstructibleObject',\n        (),\n        {}\n    ), [ad_deconstructed[0]])\n    self.assertEqual(ad_deconstructed[1], (123,))\n\n    ad = self.get_changes([], [self.author_name_deconstructible_dict_1])\n    ad_deconstructed = ad['testapp'][0].operations[0].field.default.deconstruct()\n    self.assertEqual(ad_deconstructed, (\n        '__main__.DeconstructibleObject',\n        (),\n        {}\n    ), [ad_deconstructed[0]])\n    self.assertEqual(ad_deconstructed[2], {'otheritem': 123})\n\n    ad = self.get_changes([], [self.author_name_nested_deconstructible_1])\n    ad_deconstructed = ad['testapp'][0].operations[0].field.default.deconstruct()\n    self.assertEqual(ad_deconstructed, (\n        '__main__.", "def test_author_proxy_with_user_foreign_key_is_not_swappable(self):\n    questioner = MigrationQuestioner(specified_apps=['testapp', 'otherapp'])\n    changes = self.get_changes([self.author_with_user], [self.author_proxy], questioner)\n    self.assertNumberMigrations(changes, 'testapp', 2)\n    self.assertOperationTypes(changes, 'testapp', 0, ['DeleteModel'])\n    self.assertOperationTypes(changes, 'testapp', 1, ['CreateModel'])\n    self.assertOperationFieldAttributes(changes, 'testapp', 1, 1, name='user', null=False, swappable_setting='AUTH_USER_MODEL')\n", "    def test_generate_altered_db_table(self):\n        questioner = MigrationQuestioner(specified_apps={\"testapp\"})\n        changes = self.get_changes([self.author_with_db_table_options], [self.author_with_new_db_table_options], questioner)\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterModelTable\"])\n        self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"author\", table=\"author_two\")\n", "    def test_add_field_swappable_fk(self):\n        # Check that a FK to a swappable model (AUTH_USER_MODEL) gets\n        # added correctly with its dependencies.\n        before_states = [self.author_empty]\n        after_states = [self.author_with_user]\n        changes = self.get_changes(before_states, after_states)\n        self.assertEqual(len(changes[\"testapp\"]), 1)\n        migration = changes[\"testapp\"][0]\n        self.assertEqual(len(migration.operations), 2)\n        self.assertEqual(migration.operations[0].name, \"user\")\n        self.assertEqual(migration.operations[1].name, \"user\")\n        self.assertEqual(migration.operations[1].field.remote_field.model, \"thirdapp.CustomUser\")\n"], "sample_697": ["def test_mktemp_with_basename(tmp_path_factory: TempPathFactory) -> None:\n    basename = \"test_directory\"\n    tmp_path = tmp_path_factory.mktemp(basename)\n    assert tmp_path.name == basename\n    assert tmp_path.is_dir()\n", "def test_ensure_relative_to_basetemp(tmp_path_factory):\n    # Test that _ensure_relative_to_basetemp raises ValueError for absolute paths\n    with pytest.raises(ValueError):\n        tmp_path_factory._ensure_relative_to_basetemp(os.path.abspath(\"test\"))\n\n    # Test that _ensure_relative_to_basetemp raises ValueError for paths outside basetemp\n    with pytest.raises(ValueError):\n        tmp_path_factory._ensure_relative_to_basetemp(\"../test\")\n\n    # Test that _ensure_relative_to_basetemp returns the normalized path for valid inputs\n    assert tmp_path_factory._ensure_relative_to_basetemp(\"test\") == \"test\"\n    assert tmp_path_factory._ensure_relative_to_basetemp(\"test/subdir\") == \"test/subdir\"\n", "def test_temp_path_factory_from_config(tmpdir_factory):\n    config = Config()\n    config.option.basetemp = None\n    config.trace = {}\n\n    factory = TempPathFactory.from_config(config, _ispytest=True)\n    assert factory._given_basetemp is None\n    assert factory._trace == {}\n    assert factory._basetemp is None\n", "def test_temp_path_factory_from_config(monkeypatch):\n    class MockConfig:\n        class MockOption:\n            basetemp = \"/path/to/basetemp\"\n\n        class MockTrace:\n                return lambda *args: None\n\n        option = MockOption()\n        trace = MockTrace()\n\n    config = MockConfig()\n    factory = TempPathFactory.from_config(config, _ispytest=True)\n    assert factory._given_basetemp == Path(\"/path/to/basetemp\")\n", "def test_getbasetemp_with_given_basetemp_exists(monkeypatch, tmp_path):\n    monkeypatch.setattr(os.path, \"abspath\", lambda x: str(tmp_path))\n    tmp_path.mkdir()\n\n    factory = TempPathFactory(given_basetemp=tmp_path, trace=lambda *args: None)\n    basetemp = factory.getbasetemp()\n\n    assert basetemp.exists()\n    assert str(basetemp) == str(tmp_path.resolve())\n"], "sample_844": ["def test_extract_xi_labels(predecessor_plot, ordering, expected_labels):\n    clusters = np.array([[0, 4], [5, 11], [12, 18]])\n    labels = _extract_xi_labels(ordering, clusters)\n    assert_array_equal(labels, expected_labels)\n", "def test_extend_region():\n    steep_point = np.array([True, False, False, True, False, False, True])\n    xward_point = np.array([False, True, True, False, True, True, False])\n    start = 1\n    min_samples = 2\n    index, end = _extend_region(steep_point, xward_point, start, min_samples)\n    assert index == 3\n    assert end == 2\n", "def test_extend_region_long_non_xward_points():\n    steep_point = [True, False, False, False, False, False, True]\n    xward_point = [False, True, True, True, True, True, False]\n    start = 1\n    min_samples = 2\n    expected_end = 4\n    result_end = _extend_region(steep_point, xward_point, start, min_samples)\n    assert result_end == expected_end\n", "def test_extract_xi_labels(predecessor_plot, ordering, expected_labels):\n    labels = _extract_xi_labels(ordering, [(0, 5)])\n    assert_array_equal(labels, expected_labels)\n", "def test_extend_region(r_plot, end):\n    steep_point = np.diff(r_plot) > 0\n    xward_point = np.diff(r_plot) < 0\n    result = _extend_region(steep_point, xward_point, start=0, min_samples=2)\n    assert result == end\n"], "sample_890": ["def test_sequential_feature_selector_with_custom_scorer():\n    X, y = make_classification(n_samples=100, n_features=20, random_state=42)\n    knn = KNeighborsClassifier(n_neighbors=3)\n    sfs = SequentialFeatureSelector(knn, n_features_to_select=3, scoring=\"f1\")\n    sfs.fit(X, y)\n    assert sfs.n_features_to_select_ == 3\n    assert sfs.get_support().sum() == 3\n", "def test_sequential_feature_selector_with_custom_scorer():\n    X, y = make_classification(n_samples=100, n_features=20, n_informative=5, random_state=42)\n    estimator = KNeighborsClassifier(n_neighbors=3)\n    sfs = SequentialFeatureSelector(estimator, n_features_to_select=3, scoring='recall')\n    sfs.fit(X, y)\n    assert sfs.n_features_to_select_ <= 3\n    assert np.sum(sfs.support_) == sfs.n_features_to_select_\n", "def test_sequential_feature_selector_direction(direction):\n    X, y = make_classification(n_samples=100, n_features=20, random_state=42)\n    knn = KNeighborsClassifier(n_neighbors=3)\n    sfs = SequentialFeatureSelector(knn, n_features_to_select=5, direction=direction)\n    sfs.fit(X, y)\n    if direction == \"forward\":\n        assert_array_equal(sfs.support_, np.array([True, True, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]))\n    else:\n        assert_array_equal(sfs.support_, np.array([False, False, False, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]))\n", "def test_sequential_feature_selector_directions(direction):\n    X, y = make_classification(n_samples=100, n_features=20, n_informative=10, random_state=42)\n    knn = KNeighborsClassifier(n_neighbors=3)\n    sfs = SequentialFeatureSelector(knn, n_features_to_select=5, direction=direction)\n    sfs.fit(X, y)\n    assert sfs.get_support().sum() == 5\n    if direction == \"forward\":\n        assert np.all(np.diff(np.where(sfs.get_support())) == 1)\n    else:\n        assert np.all(np.diff(np.where(~sfs.get_support())) == 1)\n", "def test_sequential_feature_selector_with_knn_classifier():\n    X, y = make_classification(n_samples=100, n_features=20, n_informative=5, random_state=42)\n    knn = KNeighborsClassifier(n_neighbors=3)\n    sfs = SequentialFeatureSelector(knn, n_features_to_select=5, direction=\"backward\")\n    sfs.fit(X, y)\n    assert sfs.n_features_to_select_ == 5\n    assert_array_equal(sfs.get_support(), np.array([True] * 5 + [False] * 15))\n"], "sample_599": ["def test_unsigned_integer_coder(dtype):\n    data = np.arange(10, dtype=dtype)\n    var = xr.Variable(\"x\", data, attrs={\"_Unsigned\": \"true\"})\n    encoded = variables.UnsignedIntegerCoder().encode(var)\n    assert encoded.data.dtype.kind == \"i\"\n    decoded = variables.UnsignedIntegerCoder().decode(encoded)\n    assert_equal(decoded, var)\n", "def test_cf_scale_offset_coder():\n    data = np.array([100, 200, 300], dtype=np.float32)\n    attrs = {\"scale_factor\": 0.1, \"add_offset\": 50}\n    var = xr.Variable(\"x\", data, attrs=attrs)\n\n    encoded_var = variables.CFScaleOffsetCoder().encode(var)\n    assert_allclose(encoded_var.data, np.array([5, 10, 15]))\n    assert \"scale_factor\" not in encoded_var.attrs\n    assert \"add_offset\" not in encoded_var.attrs\n\n    decoded_var = variables.CFScaleOffsetCoder().decode(encoded_var)\n    assert_allclose(decoded_var.data, data)\n    assert decoded_var.attrs == attrs\n", "def test_CFMaskCoder_encode_decode():\n    variable = xr.DataArray(np.arange(10), dims=\"x\", attrs={\"_FillValue\": 9})\n    coder = variables.CFMaskCoder()\n\n    encoded_variable = coder.encode(variable)\n    assert encoded_variable.data[-1] == 9\n\n    decoded_variable = coder.decode(encoded_variable)\n    assert_equal(variable, decoded_variable)\n", "def test_scale_offset_coder(scale_factor, add_offset):\n    data = np.arange(10, dtype=float)\n    attrs = {}\n    encoding = {}\n    if scale_factor is not None:\n        encoding[\"scale_factor\"] = scale_factor\n    if add_offset is not None:\n        encoding[\"add_offset\"] = add_offset\n\n    variable = variables.Variable((\"x\",), data, attrs, encoding)\n    encoded_variable = variables.CFScaleOffsetCoder().encode(variable)\n\n    if scale_factor is not None:\n        data /= scale_factor\n    if add_offset is not None:\n        data -= add_offset\n\n    assert_allclose(encoded_variable.data, data)\n\n    decoded_variable = variables.CFScaleOffsetCoder().decode(encoded_variable)\n    assert_allclose(decoded_variable.data, variable.data)\n", "def test_cfmaskcoder():\n    variable = xr.Variable(dims=(\"x\",), data=np.array([1, 2, np.nan]), attrs={\"_FillValue\": 999}, encoding={})\n    coder = variables.CFMaskCoder()\n\n    encoded_variable = coder.encode(variable, name=\"test_variable\")\n    assert np.all(encoded_variable.data == np.array([1, 2, 999]))\n\n    decoded_variable = coder.decode(encoded_variable, name=\"test_variable\")\n    assert np.all(decoded_variable.data == np.array([1, 2, np.nan]))\n"], "sample_1018": ["def test_print_Assignment():\n    x, y = symbols('x y')\n    expr = Assignment(x, y)\n    assert fcode(expr) == 'x = y'\n", "def test_modulo_function():\n    x, y = symbols('x y', integer=True)\n    expr = Mod(x, y)\n    assert fcode(expr) == \"      modulo(x, y)\"\n", "def test_fcode_printer():\n    x, y = symbols('x y')\n    expr = x + y\n    result = fcode(expr)\n    expected = 'x + y'\n    assert result == expected\n", "def test_logical_operations():\n    x, y = symbols('x y')\n    expr = And(x > 0, y < 1)\n    assert fcode(expr) == '(x > 0) .and. (y < 1)'\n", "def test_user_functions():\n    x, y = symbols('x y')\n    custom_functions = {\n        \"custom_func\": \"CUSTOM_FUNC\",\n        \"another_func\": [(lambda x: x > 0, \"POSITIVE_FUNC\"), (lambda x: x <= 0, \"NEGATIVE_FUNC\")]\n    }\n    expr = custom_func(x) + another_func(y)\n    expected = '      CUSTOM_FUNC(x) + POSITIVE_FUNC(y)'\n    assert fcode(expr, user_functions=custom_functions) == expected\n"], "sample_138": ["    def test_manifest_file_is_saved(self):\n        self.collectstatic()\n        manifest_path = os.path.join(self.tmp_dir, 'staticfiles.json')\n        self.assertTrue(os.path.exists(manifest_path))\n        with open(manifest_path) as manifest_file:\n            manifest_data = json.load(manifest_file)\n            self.assertIn('paths', manifest_data)\n        self.assertPostCondition()\n", "def test_manifest_files_mixin(self):\n    \"\"\"\n    Test the ManifestFilesMixin class.\n    \"\"\"\n    self.storage = storage.ManifestFilesMixin(location=self.location)\n    self.storage.manifest_storage = self.storage\n    self.storage.save('test.txt', StringIO('test content'))\n    self.storage.post_process({'test.txt': (self.storage, 'test.txt')})\n    manifest = self.storage.read_manifest()\n    data = json.loads(manifest)\n    self.assertEqual(data['version'], '1.0')\n    self.assertIn('test.txt', data['paths'])\n\n    # Test loading the manifest\n    self.storage.hashed_files = {}\n    self.storage.load_manifest()\n    self.assertIn('test.txt', self.storage.hashed_files)\n\n    # Test getting the stored name\n    stored_name = self.storage.stored_name('test.txt')\n    self.assertEqual(stored_name, data['paths']['test.txt'])\n\n    self.assertPostCondition()\n", "    def test_post_process_missing_manifest(self):\n        # Test post_process with a missing manifest file\n        # This should raise a ValueError\n        with self.assertRaises(ValueError):\n            storage.staticfiles_storage.post_process(paths={}, dry_run=False)\n\n        self.assertPostCondition()\n", "def test_manifest_files_mixin(self):\n    # Setup a temporary directory for testing\n    temp_dir = tempfile.mkdtemp()\n    try:\n        # Create a dummy storage with the ManifestFilesMixin\n        class TestStorage(storage.ManifestFilesMixin, storage.FileSystemStorage):\n            pass\n\n        test_storage = TestStorage(location=temp_dir)\n\n        # Create a sample file with content\n        sample_file_path = os.path.join(temp_dir, 'sample.txt')\n        with open(sample_file_path, 'w') as f:\n            f.write('This is a sample file.')\n\n        # Call the post_process method to generate the manifest\n        test_storage.post_process({sample_file_path: (test_storage, sample_file_path)}, dry_run=False)\n\n        # Check if the manifest file was created\n        manifest_path = os.path.join(temp_dir, test_storage.manifest_name)\n        self.assertTrue(os.path.exists(manifest_path))\n\n        # Load the manifest and check if the sample file is present\n        with open(manifest_path, 'r') as f:\n            manifest = json.load(f)\n        self.assertIn('sample.txt', manifest['paths'])\n\n    finally:\n        # Clean up the temporary directory\n        shutil.rmtree(temp_dir)\n\n    # Assert post conditions\n    self.assertPostCondition()\n", "def test_hashed_name(self):\n    test_content = \"test content\"\n    test_name = \"test.css\"\n    expected_hash = \"633b68f21ac8\"\n    expected_name = f\"test.{expected_hash}.css\"\n\n    with self.settings(DEBUG=True):\n        hashed_name = storage.staticfiles_storage.hashed_name(test_name, StringIO(test_content))\n        self.assertEqual(hashed_name, expected_name)\n\n    self.assertPostCondition()\n"], "sample_750": ["def test_omp_with_n_nonzero_coefs():\n    coef = orthogonal_mp(X, y, n_nonzero_coefs=n_nonzero_coefs)\n    assert_equal(len(np.nonzero(coef)[0]), n_nonzero_coefs)\n", "def test_orthogonal_mp_gram_tol():\n    coef = orthogonal_mp_gram(G, Xy, tol=0.1, norms_squared=np.sum(y**2, axis=0))\n    assert_true(coef.shape == (n_features, n_targets))\n    assert_true(np.all(np.sum(coef != 0, axis=0) <= n_nonzero_coefs))\n    assert_array_almost_equal(np.dot(X, coef) - y, 0, decimal=1)\n", "def test_orthogonal_mp_tol():\n    coef = orthogonal_mp(X, y, tol=1.0)\n    lin_reg = LinearRegression()\n    lin_reg.fit(X[:, np.abs(coef).sum(0) > 0], y)\n    y_pred = lin_reg.predict(X[:, np.abs(coef).sum(0) > 0])\n    assert_array_almost_equal(y_pred, y, decimal=0)\n", "def test_omp_invalid_parameters():\n    # Test that OrthogonalMatchingPursuit raises a ValueError when given invalid parameters\n    with assert_raises(ValueError):\n        OrthogonalMatchingPursuit(n_nonzero_coefs=0)\n    with assert_raises(ValueError):\n        OrthogonalMatchingPursuit(tol=-1)\n    with assert_raises(ValueError):\n        OrthogonalMatchingPursuit(tol=None, n_nonzero_coefs=n_features + 1)\n    with assert_raises(ValueError):\n        orthogonal_mp(X, y, tol=None, n_nonzero_coefs=0)\n    with assert_raises(ValueError):\n        orthogonal_mp(X, y, tol=-1)\n    with assert_raises(ValueError):\n        orthogonal_mp(X, y, tol=None, n_nonzero_coefs=n_features + 1)\n    with assert_raises(ValueError):\n        orthogonal_mp_gram(G, Xy, tol=None, n_nonzero_coefs=0)\n    with assert_raises(ValueError):\n        orthogonal_mp_gram(G, Xy, tol=-1)\n    with assert_raises(ValueError):\n        orthogonal_mp_gram(G, Xy, tol=None, n_nonzero_coefs=n_features + 1)\n    with assert_raises(ValueError):\n        orthogonal_mp_gram(G, Xy, tol=None, norms_squared=None)\n", "def test_omp_path_return_type():\n    coef, _ = orthogonal_mp(X, y, n_nonzero_coefs=n_nonzero_coefs, return_path=True)\n    assert_equal(coef.shape, (n_features, n_targets, n_nonzero_coefs))\n"], "sample_1002": ["def test_integer_log():\n    assert integer_log(1000, 10) == 3\n", "compilation error", "def test_Catalan():\n    assert isinstance(Catalan, NumberSymbol)\n    assert Catalan.is_real is True\n    assert Catalan.is_positive is True\n    assert Catalan.is_negative is False\n    assert Catalan.is_irrational is None\n    assert Catalan.is_number is True\n    assert Catalan._as_mpf_val(10) == mpmath.mpf(0.91596559417721901505460351493238411077414937428167)\n    assert Catalan.approximation_interval(Integer) == (S.Zero, S.One)\n    assert Catalan.approximation_interval(Rational) == (Rational(9, 10), S.One)\n", "compilation error", "def test_integer_divmod():\n    x = Integer(10)\n    y = Integer(3)\n    q, r = divmod(x, y)\n    assert q == Integer(3)\n    assert r == Integer(1)\n    q, r = x.__divmod__(y)\n    assert q == Integer(3)\n    assert r == Integer(1)\n    q, r = divmod(10, y)\n    assert q == Integer(3)\n    assert r == Integer(1)\n    q, r = divmod(x, 3)\n    assert q == Integer(3)\n    assert r == Integer(1)\n    q, r = x.__divmod__(3)\n    assert q == Integer(3)\n    assert r == Integer(1)\n    q, r = divmod(10, 3)\n    assert q == Integer(3)\n    assert r == Integer(1)\n    q, r = divmod(x, S.NegativeOne)\n    assert q == Integer(-10)\n    assert r == Integer(0)\n    q, r = x.__rdivmod__(3)\n    assert q == Integer(3)\n    assert r == Integer(1)\n    q, r = divmod(3, x)\n    assert q == Integer(0)\n    assert r == Integer(3)\n    q, r = Integer(3).__rdivmod__(x)\n    assert q == Integer(0)\n    assert r == Integer(3)\n    q, r = divmod(3, 10)\n    assert q == Integer(0)\n    assert r == Integer(3)\n    q, r = divmod(S.NegativeOne, x)\n    assert q == Integer(0)\n    assert r == Integer(-1)\n    q, r = Integer(-1).__rdivmod__(x)\n    assert q == Integer(0)\n    assert r == Integer(-1)\n    q, r = divmod(-1, 10)\n    assert q == Integer(0)\n    assert r == Integer(-1)\n"], "sample_324": ["def test_check_token_without_post_token(self):\n    with self.assertRaisesMessage(RejectRequest, REASON_CSRF_TOKEN_MISSING):\n        middleware = CsrfViewMiddleware()\n        request = self._get_POST_csrf_cookie_request()\n        middleware._check_token(request)\n", "def test_referer_check_for_secure_request(self):\n    middleware = CsrfViewMiddleware()\n    secure_request = self._get_POST_csrf_cookie_request()\n    secure_request._is_secure_override = True\n    secure_request.META['HTTP_REFERER'] = 'http://example.com/'\n\n    with self.assertRaises(RejectRequest) as context:\n        middleware._check_referer(secure_request)\n\n    self.assertEqual(str(context.exception), REASON_INSECURE_REFERER)\n", "def test_session_based_token_rotation(self):\n    middleware = CsrfViewMiddleware()\n    req = self._get_GET_csrf_cookie_request()\n\n    # First request should set the session-based CSRF token\n    response = middleware.process_view(req, token_view, [], {})\n    self.assertEqual(response, None)\n    self.assertEqual(req.session[CSRF_SESSION_KEY], req.META['CSRF_COOKIE'])\n    old_token = req.session[CSRF_SESSION_KEY]\n\n    # Rotate the token\n    rotate_token(req)\n    new_token = req.session[CSRF_SESSION_KEY]\n    self.assertNotEqual(old_token, new_token)\n\n    # Verify that the new token is used for subsequent requests\n    response = middleware.process_view(req, token_view, [], {})\n    self.assertEqual(response, None)\n    self.assertEqual(req.session[CSRF_SESSION_KEY], new_token)\n    self.assertEqual(req.META['CSRF_COOKIE'], new_token)\n", "    def test_process_view_with_bad_origin(self):\n        middleware = CsrfViewMiddleware()\n        request = self._get_POST_request_with_token()\n        request.META['HTTP_ORIGIN'] = 'http://malicious.example.com'\n        response = middleware.process_view(request, lambda r: None, (), {})\n        self.assertEqual(response.status_code, 403)\n        self.assertEqual(response.reason_phrase, REASON_BAD_ORIGIN % 'http://malicious.example.com')\n", "    def test_insecure_referer_https_request(self):\n        req = self._get_POST_csrf_cookie_request()\n        req._is_secure_override = True\n        req.META['HTTP_REFERER'] = 'http://testserver/'\n        response = self.middleware(req)\n        self.assertEqual(response.status_code, 403)\n        self.assertContains(response, REASON_INSECURE_REFERER)\n"], "sample_179": ["    def test_unique_together_with_invalid_fields(self):\n        class InvalidUniqueTogetherModel(models.Model):\n            field1 = models.CharField(max_length=255)\n            field2 = models.CharField(max_length=255)\n\n            class Meta:\n                unique_together = (('field1', 'invalid_field'),)\n\n        errors = InvalidUniqueTogetherModel.check()\n        self.assertEqual(len(errors), 1)\n        self.assertEqual(errors[0].id, 'models.E012')\n        self.assertEqual(errors[0].msg, \"'unique_together' refers to the nonexistent field 'invalid_field'.\")\n", "    def test_unique_together_with_invalid_fields(self):\n        class BadUniqueTogether(models.Model):\n            name = models.CharField(max_length=20)\n\n            class Meta:\n                unique_together = (('name', 'invalid_field'),)\n\n        errors = BadUniqueTogether.check()\n        self.assertEqual(len(errors), 1)\n        self.assertEqual(\n            errors[0],\n            Error(\n                \"'unique_together' refers to the nonexistent field 'invalid_field'.\",\n                id='models.E012',\n                obj=BadUniqueTogether,\n            )\n        )\n", "    def test_invalid_unique_together(self):\n        class InvalidUniqueTogetherModel(models.Model):\n            field1 = models.CharField(max_length=10)\n            field2 = models.CharField(max_length=10)\n\n            class Meta:\n                unique_together = 'invalid'\n\n        errors = InvalidUniqueTogetherModel.check()\n        self.assertEqual(\n            errors,\n            [Error(\n                \"'unique_together' must be a list or tuple.\",\n                obj=InvalidUniqueTogetherModel,\n                id='models.E010',\n            )]\n        )\n", "    def test_unique_together_check(self):\n        class ValidModel(models.Model):\n            field1 = models.CharField(max_length=10)\n            field2 = models.IntegerField()\n\n            class Meta:\n                unique_together = (('field1', 'field2'),)\n\n        self.assertEqual(ValidModel.check(), [])\n\n        class InvalidModel(models.Model):\n            field1 = models.CharField(max_length=10)\n            field2 = models.IntegerField()\n\n            class Meta:\n                unique_together = ('invalid_field',)\n\n        self.assertEqual(\n            InvalidModel.check(),\n            [\n                Error(\n                    \"'unique_together' refers to the nonexistent field 'invalid_field'.\",\n                    obj=InvalidModel,\n                    id='models.E012',\n                ),\n            ],\n        )\n", "    def test_unique_together_check_with_invalid_fields(self):\n        class InvalidUniqueTogetherModel(models.Model):\n            field1 = models.CharField(max_length=10)\n            field2 = models.CharField(max_length=10)\n\n            class Meta:\n                unique_together = (\n                    ('field1', 'invalid_field'),\n                    ('another_invalid_field',),\n                    'field2',\n                )\n\n        errors = InvalidUniqueTogetherModel.check()\n        self.assertEqual(len(errors), 3)\n        self.assertEqual(\n            errors,\n            [\n                Error(\n                    \"'unique_together' refers to the nonexistent field 'invalid_field'.\",\n                    obj=InvalidUniqueTogetherModel,\n                    id='models.E012',\n                ),\n                Error(\n                    \"'unique_together' refers to the nonexistent field 'another_invalid_field'.\",\n                    obj=InvalidUniqueTogetherModel,\n                    id='models.E012',\n                ),\n                Error(\n                    \"'unique_together' must be a list or tuple.\",\n                    obj=InvalidUniqueTogetherModel,\n                    id='models.E011',\n                ),\n            ]\n        )\n"], "sample_451": ["def test_parse_docstring(self):\n    title, body, metadata = parse_docstring(self.docstring)\n    self.assertEqual(title, \"This __doc__ output is required for testing.\")\n    self.assertEqual(metadata[\"some_metadata\"], \"some data\")\n    self.assertIn(\"Display an individual\", body)\n    self.assertIn(\"**Context**\", body)\n    self.assertIn(\"**Template:**\", body)\n", "    def test_parse_docstring(self):\n        title, body, metadata = parse_docstring(self.docstring)\n        self.assertEqual(title, \"This __doc__ output is required for testing. I copied this example from `admindocs` documentation.\")\n        self.assertIn(\"Display an individual :model:`myapp.MyModel`.\", body)\n        self.assertIn(\"**Template:**\", body)\n        self.assertIn(\":template:`myapp/my_template.html`\", body)\n        self.assertEqual(metadata[\"some_metadata\"], \"some data\")\n", "    def test_parse_docstring(self):\n        title, body, metadata = parse_docstring(self.docstring)\n        self.assertEqual(title, \"This __doc__ output is required for testing. I copied this example from `admindocs` documentation.\")\n        self.assertIn(\"Display an individual :model:`myapp.MyModel`.\", body)\n        self.assertIn(\":template:`myapp/my_template.html`\", body)\n        self.assertEqual(metadata, {\"some_metadata\": \"some data\"})\n", "    def test_parse_docstring(self):\n        title, body, metadata = parse_docstring(self.docstring)\n        self.assertEqual(title, \"This __doc__ output is required for testing. I copied this example from `admindocs` documentation.\")\n        self.assertTrue(\"Display an individual :model:`myapp.MyModel`.\" in body)\n        self.assertEqual(metadata[\"some_metadata\"], \"some data\")\n", "    def test_parse_docstring(self):\n        title, body, metadata = parse_docstring(self.docstring)\n        self.assertEqual(title, \"This __doc__ output is required for testing. I copied this example from `admindocs` documentation. (TITLE)\")\n        self.assertTrue(\"Display an individual :model:`myapp.MyModel`.\" in body)\n        self.assertEqual(metadata, {'some_metadata': 'some data'})\n"], "sample_608": ["def test_last_item_empty_array(self):\n    empty_array = np.array([])\n    result = formatting.last_item(empty_array)\n    assert result == []\n", "def test_inline_variable_array_repr(self):\n    data = np.arange(10)\n    var = xr.Variable(dims=\"x\", data=data)\n    result = formatting.inline_variable_array_repr(var, max_width=30)\n    assert result == \"0 1 2 ... 7 8 9\"\n", "    def test_format_timedelta_date(self):\n        # Test that format_timedelta correctly formats a timedelta as a date string\n        timedelta_obj = pd.Timedelta(days=10)\n        formatted_str = formatting.format_timedelta(timedelta_obj, timedelta_format=\"date\")\n        assert formatted_str == \"10 days\"\n", "def test_short_data_repr_large_array(self):\n    arr = xr.DataArray(np.random.rand(1000000), dims='x')\n    expected_repr = \"[1000000 values with dtype=float64]\"\n    assert formatting.short_data_repr(arr) == expected_repr\n", "def test_short_data_repr_with_large_duck_array(self):\n    class LargeDuckArray:\n            self.size = size\n\n        @property\n            return np.arange(self.size)\n\n    large_duck_array = LargeDuckArray(10**6)\n    expected_output = \"[1000000 values with dtype=int64]\"\n    assert formatting.short_data_repr(large_duck_array) == expected_output\n"], "sample_1041": ["def test_matrix_element_derivative():\n    i, j = symbols('i j', integer=True)\n    M = MatrixSymbol('M', n, n)\n    v = MatrixElement(M, i, j)\n    u = MatrixElement(M, k, l)\n    assert v._eval_derivative(u) == KroneckerDelta(i, k) * KroneckerDelta(j, l)\n", "def test_matrix_derivatives():\n    X = MatrixSymbol('X', 2, 2)\n    Y = MatrixSymbol('Y', 2, 2)\n    Z = MatrixSymbol('Z', 2, 2)\n\n    # Derivative of a matrix symbol with respect to itself\n    assert diff(X, X) == Identity(2)\n\n    # Derivative of a matrix symbol with respect to another matrix symbol\n    assert diff(X, Y) == ZeroMatrix(2, 2)\n\n    # Derivative of a matrix expression with respect to a matrix symbol\n    expr = MatMul(X, Y, Z)\n    assert diff(expr, X) == MatMul(Identity(2), Y, Z)\n\n    # Derivative of a matrix expression with respect to another matrix symbol\n    assert diff(expr, Y) == MatMul(X, Identity(2), Z)\n\n    # Derivative of a matrix expression with respect to a scalar symbol\n    assert diff(expr, x) == ZeroMatrix(2, 2)\n", "def test_matrix_element_derivative():\n    A_deriv = MatrixElement(A, k, l)\n    B_deriv = MatrixElement(B, p, k)\n    result = diff(A_deriv, B_deriv)\n    expected = KroneckerDelta(p, l)\n    assert result == expected\n", "def test_MatrixElement_eval_derivative():\n    v = MatrixElement('v', n, m)\n    result = v._eval_derivative(v)\n    assert result == KroneckerDelta(n, n) * KroneckerDelta(m, m)\n", "def test_matrix_element_derivative():\n    M = MatrixSymbol('M', n, n)\n    v = MatrixSymbol('v', n, 1)\n    A = MatrixElement(M, i, j)\n    B = MatrixElement(v, i, 0)\n    derivative = diff(A, B)\n    assert derivative == KroneckerDelta(j, 0) * KroneckerDelta(i, i)\n"], "sample_298": ["    def test_check_token_with_valid_token(self):\n        user = User.objects.create_user(username='testuser', email='test@test.com', password='testpassword')\n        now = datetime.now()\n        token_generator = MockedPasswordResetTokenGenerator(now)\n        token = token_generator.make_token(user)\n        self.assertTrue(token_generator.check_token(user, token))\n", "    def test_make_token(self):\n        user = User.objects.create_user(username='testuser', password='12345', email='testuser@example.com')\n        now = datetime.now()\n        token_generator = MockedPasswordResetTokenGenerator(now)\n        token = token_generator.make_token(user)\n        self.assertIsNotNone(token)\n        self.assertTrue(token_generator.check_token(user, token))\n", "def test_check_token_validity(self):\n    user = User.objects.create_user(username='testuser', password='12345')\n    now = datetime.now()\n    token_generator = MockedPasswordResetTokenGenerator(now)\n    token = token_generator.make_token(user)\n\n    # Check the token is valid\n    self.assertTrue(token_generator.check_token(user, token))\n\n    # Check the token is invalid after the timeout\n    future = now + timedelta(seconds=settings.PASSWORD_RESET_TIMEOUT + 1)\n    future_token_generator = MockedPasswordResetTokenGenerator(future)\n    self.assertFalse(future_token_generator.check_token(user, token))\n", "    def test_token_expiry(self):\n        \"\"\"\n        Test that the token expires after PASSWORD_RESET_TIMEOUT seconds.\n        \"\"\"\n        user = User.objects.create_user(username='testuser', password='testpassword')\n        now = datetime.now()\n        token_gen = MockedPasswordResetTokenGenerator(now)\n        token = token_gen.make_token(user)\n\n        # Check that the token is valid now\n        self.assertTrue(token_gen.check_token(user, token))\n\n        # Move time forward by PASSWORD_RESET_TIMEOUT + 1 seconds\n        future = now + timedelta(seconds=settings.PASSWORD_RESET_TIMEOUT + 1)\n        token_gen = MockedPasswordResetTokenGenerator(future)\n\n        # Check that the token is now invalid\n        self.assertFalse(token_gen.check_token(user, token))\n", "    def test_make_token(self):\n        user = User.objects.create_user(username='testuser', password='testpassword', email='test@test.com')\n        now = datetime.now()\n        token_generator = MockedPasswordResetTokenGenerator(now)\n\n        token = token_generator.make_token(user)\n        self.assertIsInstance(token, str)\n        self.assertEqual(len(token.split('-')), 2)\n\n        # Check that the token is valid\n        self.assertTrue(token_generator.check_token(user, token))\n\n        # Check that the token is invalid if the user changes their password\n        user.set_password('newpassword')\n        self.assertFalse(token_generator.check_token(user, token))\n\n        # Check that the token is invalid if it's used after the timeout period\n        future = now + timedelta(seconds=settings.PASSWORD_RESET_TIMEOUT + 1)\n        future_token_generator = MockedPasswordResetTokenGenerator(future)\n        self.assertFalse(future_token_generator.check_token(user, token))\n"], "sample_94": ["def test_create_superuser_with_additional_fields(self):\n    out = StringIO()\n    call_command('createsuperuser', interactive=True, stdout=out, stdin=MockTTY())\n    self.assertIn('Superuser created successfully.', out.getvalue())\n    self.assertTrue(User.objects.filter(username='bob').exists())\n    user = User.objects.get(username='bob')\n    self.assertEqual(user.email, 'bob@example.com')\n    self.assertEqual(user.date_of_birth, date(2000, 1, 1))\n", "def test_create_superuser_interactive(self):\n    out = StringIO()\n    call_command('createsuperuser', stdout=out, stdin=MockTTY())\n    self.assertIn('Superuser created successfully.', out.getvalue())\n    self.assertTrue(User.objects.get(username='alice').is_superuser)\n", "def test_createsuperuser_with_input(self):\n    out = StringIO()\n    call_command('createsuperuser', stdout=out, stdin=MockTTY(), username='alice', email='alice@example.com', password='testpassword')\n    self.assertIn('Superuser created successfully.', out.getvalue())\n", "def test_valid_input_creates_superuser(self):\n    out = StringIO()\n    call_command(createsuperuser.Command(), username='bob', stdin=MockTTY(), stdout=out)\n    self.assertIn('Superuser created successfully.', out.getvalue())\n    self.assertTrue(User.objects.filter(username='bob').exists())\n", "def test_create_superuser_non_interactive(self):\n    out = StringIO()\n    call_command('createsuperuser', username='alice', email='alice@example.com', stdin=MockTTY(), stdout=out)\n    self.assertIn('Superuser created successfully.', out.getvalue())\n"], "sample_1095": ["def test_AppliedPermutation():\n    p = Permutation(0, 1, 2)\n    x = Symbol('x')\n    ap = AppliedPermutation(p, x)\n    assert ap.subs(x, 1) == 2\n", "def test_apply_permutation_to_symbolic_variable():\n    x = Symbol('x')\n    p = Permutation(0, 1, 2)\n    ap = p.apply(x)\n    assert isinstance(ap, AppliedPermutation)\n    assert ap.subs(x, 1) == 2\n", "def test_commutes_with():\n    p = Permutation([1, 4, 3, 0, 2, 5])\n    q = Permutation([0, 1, 2, 3, 4, 5])\n    assert p.commutes_with(q) == True\n    q = Permutation([2, 3, 5, 4, 1, 0])\n    assert p.commutes_with(q) == False\n", "def test_apply_permutation_to_symbolic_variable():\n    x = Symbol('x')\n    p = Permutation(0, 1, 2)\n    result = p.apply(x)\n    expected = AppliedPermutation(p, x)\n    assert result == expected\n", "def test_perm_get_precedence_matrix():\n    p = Permutation.josephus(3, 6, 1)\n    expected_matrix = Matrix([\n        [0, 0, 0, 0, 0, 0],\n        [1, 0, 0, 0, 1, 0],\n        [1, 1, 0, 1, 1, 1],\n        [1, 1, 0, 0, 1, 0],\n        [1, 0, 0, 0, 0, 0],\n        [1, 1, 0, 1, 1, 0]\n    ])\n    assert p.get_precedence_matrix() == expected_matrix\n"], "sample_638": ["def test_run_without_arguments(capsys):\n    with pytest.raises(SystemExit) as exc_info:\n        main.Run([])\n    assert exc_info.value.code == 1\n    captured = capsys.readouterr()\n    assert \"Usage:\" in captured.out\n", "def test_run_with_unsupported_format(capsys, mock_graphviz):\n    with mock.patch.object(sys, \"argv\", [\"pyreverse\", \"-o\", \"XYZ\", \"pylint\"]):\n        with pytest.raises(SystemExit):\n            main.Run([])\n        captured = capsys.readouterr()\n        assert \"Format XYZ is not supported natively.\" in captured.out\n        assert \"Pyreverse will try to generate it using Graphviz...\" in captured.out\n        assert \"Format 'XYZ' is not recognized by 'dot'.\" in captured.out\n", "def test_run_with_unsupported_format(mock_graphviz):\n    with mock.patch(\"pylint.pyreverse.main.Run\") as mock_run:\n        mock_run.return_value = mock.Mock(config=mock.Mock(output_format=\"XYZ\"))\n        with pytest.raises(SystemExit) as e:\n            with fix_import_path([PROJECT_ROOT_DIR]):\n                main.Run([])\n        assert e.value.code == 1\n", "def test_run_with_invalid_format(mock_graphviz):\n    with pytest.raises(SystemExit) as pytest_wrapped_e:\n        with fix_import_path([\"data\"]):\n            main.Run([\"--output\", \"XYZ\", \"data\"])\n    assert pytest_wrapped_e.type == SystemExit\n    assert pytest_wrapped_e.value.code == 2\n", "def test_main_with_unsupported_format(mock_subprocess, mock_graphviz, capfd):\n    with mock.patch.object(sys, \"argv\", [\"pyreverse\", \"-o\", \"xyz\", \"data\"]):\n        with pytest.raises(SystemExit) as pytest_wrapped_e:\n            main.Run([])\n        assert pytest_wrapped_e.type == SystemExit\n        assert pytest_wrapped_e.value.code == 1\n        out, err = capfd.readouterr()\n        assert \"Format xyz is not supported natively.\" in out\n        mock_subprocess.run.assert_called_once_with([\"dot\", \"-Txyz\", \"-o\", \"-\", mock.ANY], check=True, stderr=mock.PIPE, stdout=mock.PIPE)\n"], "sample_288": ["    def test_key_transform_factory(self):\n        factory = KeyTransformFactory('test_key')\n        transform = factory(Value('test_value'))\n        self.assertIsInstance(transform, KeyTransform)\n        self.assertEqual(transform.key_name, 'test_key')\n        self.assertEqual(transform.lhs.value, 'test_value')\n", "    def test_key_transform_factory(self):\n        key_name = 'test_key'\n        factory = KeyTransformFactory(key_name)\n        transform = factory('test_value')\n        self.assertIsInstance(transform, KeyTransform)\n        self.assertEqual(transform.key_name, key_name)\n        self.assertEqual(transform.lhs, 'test_value')\n", "    def test_key_transform_factory(self):\n        factory = KeyTransformFactory('test_key')\n        key_transform = factory(Value('{\"test_key\": \"value\"}'))\n        self.assertEqual(key_transform.key_name, 'test_key')\n", "    def setUpTestData(cls):\n        JSONModel.objects.create(\n            name='Test object',\n            data={'key1': 'value1', 'key2': {'nested_key': 'nested_value'}}\n        )\n", "    def test_encoders_and_decoders(self):\n        model = JSONModel(json_field={'test': 'value'})\n        model.save()\n        # Test default encoder and decoder\n        self.assertEqual(model.json_field, {'test': 'value'})\n        # Test custom encoder and decoder\n        model.json_field = {'custom': uuid.uuid4()}\n        model.save()\n        self.assertIsInstance(JSONModel.objects.get(id=model.id).json_field['custom'], str)\n"], "sample_489": ["def test_bulk_create_ignore_conflicts(self):\n    Country.objects.bulk_create(self.data, ignore_conflicts=True)\n    self.assertEqual(Country.objects.count(), 4)\n    Country.objects.bulk_create(self.data, ignore_conflicts=True)\n    self.assertEqual(Country.objects.count(), 4)\n", "def test_bulk_create_ignore_conflicts(self):\n    Country.objects.bulk_create(self.data, ignore_conflicts=True)\n    self.assertEqual(Country.objects.count(), 4)\n    Country.objects.bulk_create(self.data, ignore_conflicts=True)\n    self.assertEqual(Country.objects.count(), 4)\n", "def test_bulk_create_ignore_conflicts(self):\n    \"\"\"\n    Test the bulk_create() method with ignore_conflicts=True.\n    \"\"\"\n    Country.objects.bulk_create(self.data, ignore_conflicts=True)\n    self.assertEqual(Country.objects.count(), 4)\n\n    # Try to insert the same data again, it should be ignored.\n    Country.objects.bulk_create(self.data, ignore_conflicts=True)\n    self.assertEqual(Country.objects.count(), 4)\n", "    def test_bulk_create_with_ignore_conflicts(self):\n        if connection.features.supports_ignore_conflicts:\n            # Create initial data\n            Country.objects.bulk_create(self.data)\n            # Try to create the same data again with ignore_conflicts=True\n            Country.objects.bulk_create(self.data, ignore_conflicts=True)\n            # Check that the data is still the same\n            self.assertEqual(Country.objects.count(), 4)\n        else:\n            with self.assertRaises(NotSupportedError):\n                Country.objects.bulk_create(self.data, ignore_conflicts=True)\n", "def test_bulk_create_with_large_batch_size(self):\n    batch_size = 100\n    Country.objects.bulk_create(self.data, batch_size=batch_size)\n    self.assertEqual(Country.objects.count(), len(self.data))\n"], "sample_445": ["    def test_time_strings(self):\n        custom_time_strings = {\n            \"year\": npgettext_lazy(\"%(num)d a\u00f1o\", \"%(num)d a\u00f1os\", \"num\"),\n            \"month\": npgettext_lazy(\"%(num)d mes\", \"%(num)d meses\", \"num\"),\n            \"week\": npgettext_lazy(\"%(num)d semana\", \"%(num)d semanas\", \"num\"),\n            \"day\": npgettext_lazy(\"%(num)d d\u00eda\", \"%(num)d d\u00edas\", \"num\"),\n            \"hour\": npgettext_lazy(\"%(num)d hora\", \"%(num)d horas\", \"num\"),\n            \"minute\": npgettext_lazy(\"%(num)d minuto\", \"%(num)d minutos\", \"num\"),\n        }\n        with translation.override(\"es\"):  # Spanish\n            self.assertEqual(timesince(self.t - self.oneweek, self.t, time_strings=custom_time_strings), \"1 semana\")\n            self.assertEqual(timesince(self.t - self.oneday, self.t, time_strings=custom_time_strings), \"1 d\u00eda\")\n            self.assertEqual(timesince(self.t - self.onehour, self.t, time_strings=custom_time_strings), \"1 hora\")\n            self.assertEqual(timesince(self.t - self.oneminute, self.t, time_strings=custom_time_strings), \"1 minuto\")\n            self.assertEqual(timeuntil(self.t + self.oneweek, self.t, time_strings=custom_time_strings), \"1 semana\")\n            self.assertEqual(timeuntil(self.t + self.oneday, self.t, time_strings=custom_time_strings), \"1 d\u00eda\")\n            self.assertEqual(timeuntil(self.t + self.onehour, self.t, time_strings=custom_time_strings), \"1 hora\")\n            self.assertEqual(timeuntil(self.t + self.oneminute, self.t, time_strings=custom_time_strings), \"1 minuto\")\n", "    def test_time_strings(self):\n        custom_time_strings = {\n            \"year\": npgettext_lazy(\"%(num)d yr\", \"%(num)d yrs\", \"num\"),\n            \"month\": npgettext_lazy(\"%(num)d mo\", \"%(num)d mos\", \"num\"),\n            \"week\": npgettext_lazy(\"%(num)d wk\", \"%(num)d wks\", \"num\"),\n            \"day\": npgettext_lazy(\"%(num)d d\", \"%(num)d ds\", \"num\"),\n            \"hour\": npgettext_lazy(\"%(num)d hr\", \"%(num)d hrs\", \"num\"),\n            \"minute\": npgettext_lazy(\"%(num)d min\", \"%(num)d mins\", \"num\"),\n        }\n        result = timesince(self.t, now=self.t + self.oneyear + self.onemonth + self.oneweek + self.oneday + self.onehour + self.oneminute, time_strings=custom_time_strings)\n        self.assertEqual(result, \"1 yr, 1 mo, 1 wk, 1 d, 1 hr, 1 min\")\n", "def test_timeuntil_with_timezone_aware_datetime(self):\n    t_aware = timezone.make_aware(self.t, timezone.get_default_timezone())\n    now_aware = timezone.now()\n    result = timeuntil(t_aware, now_aware)\n    self.assertIsInstance(result, str)\n", "    def test_timezone_aware(self):\n        now = timezone.now()\n        future = now + self.onehour\n        future = timezone.localtime(future)\n        self.assertEqual(timesince(now, future), \"1 hour\")\n        self.assertEqual(timeuntil(future, now), \"1 hour\")\n", "def test_time_with_timezone(self):\n    with override_settings(TIME_ZONE='America/New_York'):\n        t_ny = timezone.make_aware(self.t, timezone.get_current_timezone())\n        t_utc = t_ny.astimezone(timezone.utc)\n        self.assertEqual(timesince(t_utc, now=t_ny), \"0 minutes\")\n        self.assertEqual(timeuntil(t_ny, now=t_utc), \"0 minutes\")\n"], "sample_278": ["    def test_filtered_relation_equality(self):\n        fr1 = FilteredRelation('test_relation', condition=Q(field__gt=10))\n        fr2 = FilteredRelation('test_relation', condition=Q(field__gt=10))\n        fr3 = FilteredRelation('different_relation', condition=Q(field__gt=10))\n        fr4 = FilteredRelation('test_relation', condition=Q(field__gt=20))\n        self.assertEqual(fr1, fr2)\n        self.assertNotEqual(fr1, fr3)\n        self.assertNotEqual(fr1, fr4)\n", "    def test_filtered_relation_equality(self):\n        fr1 = FilteredRelation('example', condition=Q(id=1))\n        fr2 = FilteredRelation('example', condition=Q(id=1))\n        fr3 = FilteredRelation('different', condition=Q(id=1))\n        fr4 = FilteredRelation('example', condition=Q(id=2))\n        self.assertEqual(fr1, fr2)\n        self.assertNotEqual(fr1, fr3)\n        self.assertNotEqual(fr1, fr4)\n", "def test_select_related_descend(self):\n    # Test select_related_descend function with a foreign key field\n    field = Company._meta.get_field('ceo')\n    restricted = False\n    requested = {}\n    load_fields = None\n    reverse = False\n    self.assertTrue(select_related_descend(field, restricted, requested, load_fields, reverse))\n\n    # Test select_related_descend function with a null foreign key field\n    field = Company._meta.get_field('logo')\n    self.assertFalse(select_related_descend(field, restricted, requested, load_fields, reverse))\n\n    # Test select_related_descend function with a restricted field list\n    field = Company._meta.get_field('ceo')\n    restricted = True\n    requested = {}\n    self.assertFalse(select_related_descend(field, restricted, requested, load_fields, reverse))\n\n    # Test select_related_descend function with a restricted field list and the field is in the requested list\n    field = Company._meta.get_field('ceo')\n    requested = {'ceo': '+'}\n    self.assertTrue(select_related_descend(field, restricted, requested, load_fields, reverse))\n\n    # Test select_related_descend function with a load_fields set and the field is not in the load_fields set\n    field = Company._meta.get_field('ceo')\n    restricted = False\n    requested = {}\n    load_fields = {'num_employees', 'num_chairs'}\n    with self.assertRaises(FieldError):\n        select_related_descend(field, restricted, requested, load_fields, reverse)\n", "def test_filtered_relation(self):\n    # Test the FilteredRelation class\n    filtered_relation = FilteredRelation('employee', condition=Q(employee__salary__gt=15))\n    company_query = Company.objects.annotate(\n        high_salary_employees=Exists(Employee.objects.filter(company=OuterRef('pk'), salary__gt=15))\n    ).filter(high_salary_employees=True)\n    self.assertEqual(list(company_query), [self.gmbh])\n    self.assertEqual(filtered_relation.relation_name, 'employee')\n    self.assertEqual(filtered_relation.condition, Q(employee__salary__gt=15))\n", "def test_filtered_relation_as_sql(self):\n    # Test the as_sql method of the FilteredRelation class\n    fr = FilteredRelation('employee', condition=Q(salary__gt=20))\n    compiler = connection.ops.compiler(self.company_query.query)\n    sql, params = fr.as_sql(compiler, connection)\n    expected_sql = 'WHERE \"employee\".\"salary\" > 20'\n    self.assertEqual(sql, expected_sql)\n"], "sample_807": ["def test_calibration_curve_with_nan():\n    y_true = np.array([0, 1, 1, 0, 1])\n    y_prob = np.array([0.1, 0.9, np.nan, 0.2, 0.8])\n\n    with pytest.raises(ValueError, match=\"contains NaN\"):\n        calibration_curve(y_true, y_prob)\n", "def test_calibrated_classifier_cv_predict_proba():\n    # Test the predict_proba method of CalibratedClassifierCV\n    X, y = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, random_state=42)\n    base_estimator = LinearSVC(random_state=42)\n    calibrator = CalibratedClassifierCV(base_estimator=base_estimator, cv=3, method='sigmoid')\n    calibrator.fit(X, y)\n    proba = calibrator.predict_proba(X)\n    assert proba.shape == (100, 2)\n    assert np.all(np.isfinite(proba))\n    assert_greater(proba.min(), 0)\n    assert_greater_equal(proba.max(), 0)\n    assert_array_almost_equal(proba.sum(axis=1), np.ones(100))\n", "def test_calibrated_classifier_cv_with_sample_weight():\n    # Test CalibratedClassifierCV with sample_weight\n    X, y = make_classification(n_samples=200, n_features=2, n_informative=2,\n                               n_redundant=0, n_classes=2, random_state=42)\n    sample_weight = np.random.rand(200)\n\n    base_estimator = LinearSVC()\n    calibrated_cv = CalibratedClassifierCV(base_estimator=base_estimator, method='sigmoid', cv=3)\n    calibrated_cv.fit(X, y, sample_weight=sample_weight)\n    proba_with_weights = calibrated_cv.predict_proba(X)\n\n    calibrated_cv_no_weights = CalibratedClassifierCV(base_estimator=base_estimator, method='sigmoid', cv=3)\n    calibrated_cv_no_weights.fit(X, y)\n    proba_no_weights = calibrated_cv_no_weights.predict_proba(X)\n\n    # Check that probabilities are different when using sample_weight\n    assert not np.allclose(proba_with_weights, proba_no_weights)\n", "def test_calibration_curve_normalize():\n    # Test calibration_curve with normalize=True\n    y_true = np.array([0, 0, 1, 1])\n    y_prob = np.array([0.2, 0.6, 0.7, 1.2])\n    prob_true, prob_pred = calibration_curve(y_true, y_prob, normalize=True)\n    assert_array_almost_equal(prob_true, np.array([0., 1.]))\n    assert_array_almost_equal(prob_pred, np.array([0.2, 0.85]))\n", "def test_calibration_with_sparse_input():\n    # Test CalibratedClassifierCV with sparse input\n    X, y = make_classification(n_samples=100, n_features=10, n_informative=5, random_state=42)\n    X_sparse = sparse.csr_matrix(X)\n\n    clf = CalibratedClassifierCV(base_estimator=LinearSVC(), cv=3)\n    clf.fit(X_sparse, y)\n    y_pred_sparse = clf.predict_proba(X_sparse)\n\n    clf.fit(X, y)\n    y_pred_dense = clf.predict_proba(X)\n\n    assert_array_almost_equal(y_pred_sparse, y_pred_dense)\n"], "sample_32": ["def test_de_density_scale(self):\n    \"\"\"Test the redshift dependence of the dark energy density.\"\"\"\n    cosmo = Flatw0wzCDM(H0=70, Om0=0.3, w0=-0.9, wz=0.2)\n    z = np.linspace(0, 1, 10)\n    I = cosmo.de_density_scale(z)\n    expected_I = (z + 1.0) ** (3.0 * (1.0 + cosmo._w0 - cosmo._wz)) * np.exp(3.0 * cosmo._wz * z)\n    np.testing.assert_allclose(I, expected_I)\n", "    def test_w(self):\n        cosmo = self._cls(*self._cls_args, **self._cls_kwargs)\n        z = np.array([0.0, 0.5, 1.0])\n        w_expected = -0.9 + 0.1 * z\n        np.testing.assert_allclose(cosmo.w(z), w_expected)\n", "    def test_de_density_scale(self):\n        cosmo = self._cls(*self._cls_args, **self._cls_kwargs)\n        z = np.linspace(0, 1, 10)\n        I = cosmo.de_density_scale(z)\n        assert np.allclose(I, (z + 1.0) ** (3.0 * (1.0 + cosmo.w0 - cosmo.wz)) * np.exp(3.0 * cosmo.wz * z))\n", "    def test_de_density_scale(self):\n        cosmo = self._cls(**self._cls_args, **self._cls_kwargs)\n        z = np.array([0.0, 0.5, 1.0])\n        expected = (z + 1.0) ** (3.0 * (1.0 + cosmo.w0 - cosmo.wz)) * np.exp(3.0 * cosmo.wz * z)\n        np.testing.assert_allclose(cosmo.de_density_scale(z), expected)\n", "    def test_comoving_distance(self, cosmo):\n        z = 0.5\n        dc = cosmo.comoving_distance(z)\n        assert u.isclose(dc, 2006.04 * u.Mpc, rtol=1e-3)\n"], "sample_771": ["def test_PowerTransformer_inverse_transform():\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    pt = PowerTransformer()\n    pt.fit(X)\n    X_trans = pt.transform(X)\n    X_inv = pt.inverse_transform(X_trans)\n    assert_array_almost_equal(X, X_inv)\n", "def test_power_transform():\n    # Test the power_transform function\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    X_transformed = power_transform(X, method='yeo-johnson')\n    assert X_transformed.shape == X.shape\n    assert np.isfinite(X_transformed).all()\n", "def test_power_transform_boxcox_transform_and_inverse():\n    pt = PowerTransformer(method='box-cox')\n    pt.fit(X_2d)\n    X_transformed = pt.transform(X_2d)\n    X_inverse = pt.inverse_transform(X_transformed)\n    assert_array_almost_equal(X_2d, X_inverse, decimal=3)\n", "def test_power_transform():\n    X = np.array([[1, 2], [3, 2], [4, 5]])\n    pt = PowerTransformer(method='box-cox')\n    pt.fit(X)\n    X_trans = pt.transform(X)\n    expected_trans = np.array([[-1.33222754, -0.70710678],\n                               [0.25646957, -0.70710678],\n                               [1.07696409, 1.41421356]])\n    assert_array_almost_equal(X_trans, expected_trans)\n\n    X_inv = pt.inverse_transform(X_trans)\n    assert_array_almost_equal(X_inv, X)\n", "def test_quantile_transform_out_of_range():\n    rng = np.random.RandomState(0)\n    X = rng.uniform(size=(100, 10))\n    X_test = np.vstack([X.min(0) - 0.1, X.max(0) + 0.1, rng.uniform(size=(10, 10))])\n    X_transformed = quantile_transform(X, n_quantiles=10, random_state=0)\n    X_test_transformed = quantile_transform(X_test, n_quantiles=10, random_state=0)\n    assert_array_less(0, X_test_transformed)\n    assert_array_less(X_test_transformed, 1)\n    assert_array_less(np.min(X_transformed, 0), np.min(X_test_transformed, 0))\n    assert_array_less(np.max(X_transformed, 0), np.max(X_test_transformed, 0))\n"], "sample_11": ["def test_sliced_wcs_pixel_bounds():\n    wcs = SlicedLowLevelWCS(WCS_SPECTRAL_CUBE, (slice(1, 9), slice(3, 15), slice(6, 12)))\n    assert_equal(wcs.pixel_bounds, ((-0, 8), (0, 12), (-3, 6)))\n", "def test_sliced_low_level_wcs_axis_correlation_matrix():\n    # Test the axis_correlation_matrix property of SlicedLowLevelWCS\n    sliced_wcs = SlicedLowLevelWCS(WCS_SPECTRAL_CUBE, (slice(2, 8), slice(None), slice(10, 15)))\n    expected_matrix = np.array([[0., 1., 0.], [1., 0., 0.]])\n    assert_equal(sliced_wcs.axis_correlation_matrix, expected_matrix)\n", "def test_sanitize_slices():\n    # Test with a single int\n    assert sanitize_slices(5, 3) == [5, slice(None), slice(None)]\n\n    # Test with a slice\n    assert sanitize_slices(slice(1, 10, 2), 3) == [slice(1, 10, 2), slice(None), slice(None)]\n\n    # Test with Ellipsis\n    assert sanitize_slices((Ellipsis, 5), 3) == [slice(None), slice(None), 5]\n\n    # Test with invalid input\n    with pytest.raises(ValueError):\n        sanitize_slices((1, 2, 3, 4), 3)\n    with pytest.raises(IndexError):\n        sanitize_slices((1, 2, slice(None, None, 2)), 3)\n    with pytest.raises(IndexError):\n        sanitize_slices((1, 2, [3, 4]), 3)\n", "def test_sliced_wcs_slice_combination():\n    # Test the combination of slices in SlicedLowLevelWCS\n    wcs = WCS_SPECTRAL_CUBE\n    slice1 = (slice(1, 5), slice(None), slice(2, 7))\n    slice2 = (slice(2, 4), slice(10, 20), slice(None))\n    sliced_wcs = SlicedLowLevelWCS(wcs, slice1)\n    sliced_wcs = SlicedLowLevelWCS(sliced_wcs, slice2)\n    assert sliced_wcs._slices_array == [slice(3, 4), slice(10, 15), slice(2, 7)]\n\n    # Test the combination of slices with an integer\n    slice1 = (slice(1, 5), slice(None), slice(2, 7))\n    slice2 = 3\n    sliced_wcs = SlicedLowLevelWCS(wcs, slice1)\n    sliced_wcs = SlicedLowLevelWCS(sliced_wcs, slice2)\n    assert sliced_wcs._slices_array == [3, slice(None), slice(2, 7)]\n", "def test_sliced_low_level_wcs():\n    wcs = WCS_SPECTRAL_CUBE\n    sliced_wcs = SlicedLowLevelWCS(wcs, (slice(2, 8), slice(10, 20), slice(5, 10)))\n    assert sliced_wcs.pixel_n_dim == 3\n    assert sliced_wcs.world_n_dim == 3\n    assert sliced_wcs.world_axis_physical_types == ['custom:latitude', 'em.freq', 'custom:longitude']\n    assert sliced_wcs.world_axis_units == [u.deg, u.Hz, u.deg]\n    assert sliced_wcs.pixel_axis_names == ['Longitude', 'Frequency', 'Latitude']\n    assert sliced_wcs.world_axis_names == ['Longitude', 'Frequency', 'Latitude']\n\n    pixel_coords = [5.5, 15.0, 6.0]\n    world_coords = sliced_wcs.pixel_to_world_values(*pixel_coords)\n    assert_allclose(world_coords, [25.5, 20.0, 9.9])\n\n    sliced_pixel_coords = sliced_wcs.world_to_pixel_values(*world_coords)\n    assert_allclose(sliced_pixel_coords, pixel_coords)\n\n    assert sliced_wcs.world_axis_object_components == [[0, 0], [1, 0], [2, 0]]\n    assert sliced_wcs.world_axis_object_classes == {0: ICRS, 1: SpectralCoord, 2: ICRS}\n    assert sliced_wcs.array_shape == (10, 10, 5)\n    assert sliced_wcs.pixel_shape == (5, 10, 10)\n    assert sliced_wcs.pixel_bounds == ((-5, 5), (-10, 8), (0, 5"], "sample_1065": ["def test_binomial_rewrite_as_gamma():\n    n, k = symbols('n k', integer=True)\n    assert binomial(n, k).rewrite(gamma) == gamma(n + 1) / (gamma(k + 1) * gamma(n - k + 1))\n", "def test_rf_rewrite():\n    x, k = symbols('x k', integer=True)\n    assert rf(x, k).rewrite(ff) == FallingFactorial(k + x - 1, k)\n    assert rf(x, k).rewrite(binomial) == binomial(x + k - 1, k) * factorial(k)\n    assert rf(k, k).rewrite(factorial) == factorial(2 * k - 1)\n", "def test_binomial_fdiff():\n    n, k = symbols('n k')\n    assert binomial(n, k).fdiff(1) == binomial(n, k)*(polygamma(0, n + 1) - polygamma(0, n - k + 1))\n    assert binomial(n, k).fdiff(2) == binomial(n, k)*(polygamma(0, n - k + 1) - polygamma(0, k + 1))\n    raises(ArgumentIndexError, lambda: binomial(n, k).fdiff(3))\n", "def test_subfactorial_rewrite():\n    n = Symbol('n', integer=True, nonnegative=True)\n    assert subfactorial(n).rewrite(uppergamma) == uppergamma(n + 1, -1)/S.Exp1\n", "def test_subfactorial():\n    n = Symbol('n', integer=True)\n    assert subfactorial(n + 1) == subfactorial(n + 1)  # n + 1 is not an integer\n    assert subfactorial(5) == 44\n    assert subfactorial(0) == S.One\n    assert subfactorial(1) == S.Zero\n    assert subfactorial(-1) == S.Zero  # subfactorial is undefined for negative integers\n\n    assert subfactorial(n).is_even == (n > 1)\n    assert subfactorial(n).is_integer == n.is_integer and n >= 0\n    assert subfactorial(n).rewrite(uppergamma) == uppergamma(n + 1, -1)/S.Exp1\n    assert subfactorial(n).is_nonnegative == (n.is_integer and n >= 0)\n    assert subfactorial(n).is_odd == (n > 1)\n"], "sample_86": ["def test_lazy_object(self):\n        return \"Test String\"\n\n    lazy_obj = SimpleLazyObject(setup_func)\n    self.assertEqual(str(lazy_obj), \"Test String\")\n    self.assertEqual(lazy_obj._wrapped, \"Test String\")\n", "def test_cached_property(self):\n    class TestClass:\n        @cached_property\n            return 'test_value'\n\n    obj = TestClass()\n    self.assertEqual(obj.test_property, 'test_value')\n    # Check if the value is cached\n    with mock.patch.object(TestClass, 'test_property', return_value='new_value') as mock_method:\n        self.assertEqual(obj.test_property, 'test_value')\n        mock_method.assert_not_called()\n\n    # Check assignment and deletion\n    obj.test_property = 'assigned_value'\n    self.assertEqual(obj.test_property, 'assigned_value')\n    del obj.test_property\n    self.assertEqual(obj.test_property, 'test_value')\n", "    def test_lazy_with_str(self):\n        lazy_str = lazy(str, str)\n        result = lazy_str(\"Test\")\n        self.assertEqual(str(result), \"Test\")\n        self.assertIsInstance(result, Promise)\n        self.assertEqual(result.__cast__(), \"Test\")\n", "def test_cached_property(self):\n    class TestClass:\n        @cached_property\n            return 'test'\n\n    obj = TestClass()\n    # Call the method the first time\n    result1 = obj.test_method\n    # Call the method the second time, should return the cached value\n    result2 = obj.test_method\n\n    self.assertEqual(result1, result2)\n    self.assertEqual(result1, 'test')\n", "    def test_lazy_function(self):\n            return arg * 2\n\n        lazy_func = lazy(mock_func, int)(5)\n        self.assertIsInstance(lazy_func, Promise)\n        self.assertEqual(lazy_func, 10)\n\n        with mock.patch('django.utils.functional.mock_func') as mocked_func:\n            mocked_func.return_value = 15\n            self.assertEqual(lazy_func, 15)\n            mocked_func.assert_called_once_with(5)\n"], "sample_1199": ["def test_tensor_product_simp():\n    e = TensorProduct(A, B) * TensorProduct(C, D)\n    assert tensor_product_simp(e) == TensorProduct(A*C, B*D)\n", "compilation error", "def test_tensor_product_simp_with_mixed_expressions():\n    e = TensorProduct(A, B)*C*TensorProduct(D, x)\n    result = tensor_product_simp(e)\n    expected = (A*C)x(B*x*D)\n    assert result == expected\n", "def test_tensor_product_simp_complex():\n    e = TensorProduct(A, B)**2 * TensorProduct(C, D) + TensorProduct(A*C, B*D)\n    assert tensor_product_simp(e) == (A*C)**2 * TensorProduct(B, D) + TensorProduct(A*C, B*D)\n", "def test_tensor_product_simp_Pow():\n    e = TensorProduct(A, B)**2\n    expected = TensorProduct(A**2, B**2)\n    assert tensor_product_simp(e) == expected\n"], "sample_1080": ["def test_refine_sign_imaginary():\n    y = Symbol('y', imaginary=True)\n    expr = sign(y)\n    assert refine_sign(expr, Q.positive(im(y))) == I\n    assert refine_sign(expr, Q.negative(im(y))) == -I\n", "def test_refine_sign():\n    y = Symbol('y', imaginary=True)\n    expr = sign(y + I)\n    refined = refine(expr, Q.real(y))\n    assert refined == I * sign(y)\n", "def test_refine_abs():\n    # Test refine_abs function with a symbolic expression and assumptions\n    x = Symbol('x')\n    assert refine(Abs(x), Q.real(x)) == Abs(x)\n    assert refine(Abs(x), Q.positive(x)) == x\n    assert refine(Abs(x), Q.negative(x)) == -x\n", "def test_refine_symbol_assumptions():\n    x = Symbol('x', real=True)\n    expr = Abs(x)\n    assert refine(expr, Q.positive(x)) == x\n\n    y = Symbol('y', negative=True)\n    expr = Abs(y)\n    assert refine(expr, Q.negative(y)) == -y\n\n    z = Symbol('z', imaginary=True)\n    expr = Abs(z)\n    assert refine(expr, Q.imaginary(z)) == Abs(z)\n\n    w = Symbol('w')\n    expr = sign(w)\n    assert refine(expr, Q.positive(w) & Q.nonzero(w)) == 1\n    assert refine(expr, Q.negative(w) & Q.nonzero(w)) == -1\n    assert refine(expr, Q.zero(w)) == 0\n\n    expr = atan2(y, x)\n    assert refine(expr, Q.real(y) & Q.positive(x)) == atan(y / x)\n    assert refine(expr, Q.negative(y) & Q.negative(x)) == atan(y / x) - pi\n    assert refine(expr, Q.positive(y) & Q.negative(x)) == atan(y / x) + pi\n    assert refine(expr, Q.zero(y) & Q.negative(x)) == pi\n    assert refine(expr, Q.positive(y) & Q.zero(x)) == pi/2\n    assert refine(expr, Q.negative(y) & Q.zero(x)) == -pi/2\n    assert refine(expr, Q.zero(y) & Q.zero(x)) == nan\n\n    expr = re(w)\n    assert refine(expr, Q.real(w)) == w\n    assert refine(expr, Q.imaginary(w)) == 0\n\n    expr = im(z)\n    assert refine(expr, Q.real(z)) == 0\n    assert refine(expr, Q.imaginary(z)) == -I * z\n\n   ", "def test_refine_atan():\n    # Test refine_Pow with atan function\n    assert refine(atan(0), Q.zero(0)) == 0\n    assert refine(atan(1), Q.real(1)) == pi/4\n    assert refine(atan(-1), Q.real(1)) == -pi/4\n    assert refine(atan(1/sqrt(3)), Q.real(1)) == pi/6\n    assert refine(atan(-1/sqrt(3)), Q.real(1)) == -pi/6\n"], "sample_783": ["def test_missing_indicator():\n    X1 = np.array([[np.nan, 1, 3],\n                   [4, 0, np.nan],\n                   [8, 1, 0]])\n    X2 = np.array([[5, 1, np.nan],\n                   [np.nan, 2, 3],\n                   [2, 4, 0]])\n    indicator = MissingIndicator()\n    indicator.fit(X1)\n    X2_tr = indicator.transform(X2)\n    expected_result = np.array([[False,  True],\n                               [ True, False],\n                               [False, False]])\n    assert_array_equal(X2_tr, expected_result)\n", "def test_simple_imputer_with_constant_strategy():\n    X = np.array([[1, 2, np.nan], [3, np.nan, 6], [7, 8, 9]])\n    imputer = SimpleImputer(missing_values=np.nan, strategy=\"constant\", fill_value=0)\n    X_trans = imputer.fit_transform(X)\n    X_true = np.array([[1, 2, 0], [3, 0, 6], [7, 8, 9]])\n    assert_array_equal(X_trans, X_true)\n", "def test_simple_imputer_invalid_strategy():\n    X = np.array([[1, 2], [np.nan, 3], [7, 6]])\n    imputer = SimpleImputer(strategy=\"invalid_strategy\")\n    with pytest.raises(ValueError) as excinfo:\n        imputer.fit(X)\n    assert \"Can only use these strategies: ['mean', 'median', 'most_frequent', 'constant'] got strategy=invalid_strategy\" in str(excinfo.value)\n", "def test_simple_imputer_constant_strategy():\n    \"\"\"Test the imputer with constant strategy.\"\"\"\n    X = np.array([[np.nan, 2, 3], [4, np.nan, 6], [10, np.nan, 9]], dtype=np.float64)\n    X_true = np.array([[5, 2, 3], [4, 5, 6], [10, 5, 9]], dtype=np.float64)\n\n    # Constant strategy\n    _check_statistics(X, X_true, \"constant\", [5], np.nan)\n", "def test_simple_imputer_with_different_data_types():\n    # Test with different data types\n    X = np.array([[1, np.nan, 'text'],\n                  [2, 3.5, 'other'],\n                  [np.nan, 6.7, 'text']], dtype=object)\n    X_true = np.array([[1, 4.0, 'text'],\n                       [2, 3.5, 'other'],\n                       [3, 6.7, 'text']], dtype=object)\n    imputer = SimpleImputer(strategy='mean')\n    X_trans = imputer.fit(X).transform(X.copy())\n    assert_array_equal(X_trans, X_true)\n\n    # Test with integer data\n    X = np.array([[1, np.nan], [2, 3], [np.nan, 6]])\n    X_true = np.array([[1, 4], [2, 3], [3, 6]])\n    _check_statistics(X, X_true, strategy='mean',\n                      statistics=np.array([1, 4]),\n                      missing_values=np.nan)\n"], "sample_563": ["def test_offsetbox_visibility():\n    fig, ax = plt.subplots()\n    box = OffsetBox()\n    box.set_visible(False)\n    ax.add_artist(box)\n    plt.close(fig)\n", "def test_offsetbox_visibility():\n    fig, ax = plt.subplots()\n    box = OffsetBox()\n    box.set_visible(False)\n    ax.add_artist(box)\n    plt.close(fig)\n", "def test_anchored_offsetbox_get_children():\n    child = OffsetBox()\n    aob = AnchoredOffsetbox(\"upper left\", child=child)\n    assert aob.get_children() == [child]\n", "def test_offsetbox_get_visible_children():\n    box = OffsetBox()\n    child1, child2, child3 = mlines.Line2D([0, 1], [0, 1]), mlines.Line2D([0, 1], [1, 0]), mlines.Line2D([0, 1], [2, 3])\n    child2.set_visible(False)\n    box._children = [child1, child2, child3]\n\n    visible_children = box.get_visible_children()\n    assert len(visible_children) == 2\n    assert child1 in visible_children\n    assert child3 in visible_children\n", "def test_drawingarea_clipping():\n    fig, ax = plt.subplots()\n    da = DrawingArea(100, 100, clip=True)\n    p = mpatches.Rectangle((0, 0), 50, 50, fc=\"C0\")\n    da.add_artist(p)\n    ax.add_artist(da)\n    ax.set_xlim(-100, 200)\n    ax.set_ylim(-100, 200)\n    return fig\n"], "sample_211": ["    def test_get_redirect_url(self):\n        view = RedirectView.as_view(url=\"/redirect-url/\")\n        request = self.rf.get('/test/')\n        response = view(request)\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response.url, \"/redirect-url/\")\n", "    def test_get_redirect_url_with_url(self):\n        view = RedirectView.as_view(url='/someurl/')\n        request = self.rf.get('/')\n        response = view(request)\n        self.assertEqual(response.url, '/someurl/')\n", "    def test_get_redirect_url(self):\n        view = RedirectView.as_view(url='/redirect-url/')\n        request = self.rf.get('/some-url/')\n        response = view(request)\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response.url, '/redirect-url/')\n\n        view = RedirectView.as_view(pattern_name='some-view')\n        with self.settings(ROOT_URLCONF='myproject.urls'):\n            response = view(request)\n            self.assertEqual(response.url, '/some-view/')\n", "    def test_get_redirect_url(self):\n        url = '/redirect-url/'\n        view = RedirectView.as_view(url=url)\n        request = self.rf.get('/some-url/')\n        response = view(request)\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response.url, url)\n", "    def test_redirect_view(self):\n        request = self.rf.get('/some-path/')\n        redirect_view = RedirectView.as_view(url='/new-url/')\n        response = redirect_view(request)\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response.url, '/new-url/')\n"], "sample_740": ["def test_check_non_negative():\n    X = np.array([1, 2, 3])\n    check_non_negative(X, \"test\")  # Should pass\n\n    X = np.array([1, -2, 3])\n    with pytest.raises(ValueError):\n        check_non_negative(X, \"test\")  # Should raise ValueError\n", "def test_check_array_with_large_sparse_matrix():\n    # Test that check_array raises an error when accept_large_sparse is False\n    # and a large sparse matrix is provided\n    X_large = sp.csr_matrix(([1, 2, 3], ([0, 1, 2], [0, 1, 2])), shape=(3, 3), dtype=np.int64)\n    with pytest.raises(ValueError, match=\"Only sparse matrices with 32-bit integer indices are accepted.\"):\n        check_array(X_large, accept_large_sparse=False)\n", "def test_check_consistent_length_error():\n    X1 = np.array([[1, 2], [3, 4]])\n    X2 = np.array([[5, 6]])\n    X3 = np.array([[7, 8], [9, 10], [11, 12]])\n\n    assert_raises(ValueError, check_consistent_length, X1, X2)\n    assert_raises(ValueError, check_consistent_length, X1, X3)\n    assert_raises(ValueError, check_consistent_length, X2, X3)\n", "def test_check_array_with_complex_input():\n    X_complex = np.array([1 + 1j, 2 + 2j], dtype=complex)\n    with pytest.raises(ValueError, match=\"Complex data not supported\"):\n        check_array(X_complex)\n", "def test_check_is_fitted_attributes():\n    class DummyEstimator:\n            self.fitted_ = True\n\n    estimator = DummyEstimator()\n    check_is_fitted(estimator, 'fitted_')  # No error, attribute is present\n\n    estimator = DummyEstimator()\n    with pytest.raises(NotFittedError):\n        check_is_fitted(estimator, 'not_fitted_')  # Raises error, attribute is not present\n\n    estimator = DummyEstimator()\n    with pytest.raises(NotFittedError):\n        check_is_fitted(estimator, ['fitted_', 'not_fitted_'])  # Raises error, not all attributes are present\n\n    estimator = DummyEstimator()\n    estimator.fit()\n    check_is_fitted(estimator, ['fitted_', 'not_fitted_'], all_or_any=any)  # No error, any of the attributes is present\n"], "sample_595": ["def test_str_accessor_capitalize(dtype):\n    data = xr.DataArray([dtype(\"hello\"), dtype(\"world\")])\n    result = data.str.capitalize()\n    expected = xr.DataArray([dtype(\"Hello\"), dtype(\"World\")])\n    assert_equal(result, expected)\n", "def test_str_encode_decode(dtype):\n    arr = xr.DataArray([b\"hello\", b\"world\"], dtype=dtype)\n    decoded = arr.str.decode(\"utf-8\")\n    encoded = decoded.str.encode(\"utf-8\")\n    assert_equal(arr, encoded)\n", "def test_string_accessor_strip(dtype):\n    da = xr.DataArray([\"  text  \", \"  trim  \", \"  end \"], dtype=dtype)\n    expected = xr.DataArray([\"text\", \"trim\", \"end \"], dtype=dtype)\n    assert_equal(da.str.strip(), expected)\n", "def test_str_slice(dtype):\n    da = xr.DataArray([\"some\", \"text\", \"in\", \"an\", \"array\"], dtype=dtype)\n    result = da.str.slice(start=1, stop=3)\n    expected = xr.DataArray([\"om\", \"ex\", \"n\", \"n\", \"ra\"], dtype=dtype)\n    assert_equal(result, expected)\n", "def test_replace_regex_with_callable(dtype):\n    da = xr.DataArray([\"Hello, World!\", \"Python, Xarray!\", \"Test, Replace!\"], dtype=dtype)\n    replaced = da.str.replace(pat=r\"(\\w+), (\\w+)!\", repl=lambda m: f\"{m.group(2)}, {m.group(1)}!\", regex=True)\n    expected = xr.DataArray([\"World, Hello!\", \"Xarray, Python!\", \"Replace, Test!\"], dtype=dtype)\n    assert_equal(replaced, expected)\n"], "sample_123": ["    def test_urlencode_none_value(self):\n        with self.assertRaisesMessage(TypeError, self.cannot_encode_none_msg):\n            urlencode({'a': None})\n", "    def test_urlquote(self):\n        self.assertEqual(urlquote('https://example.com/?q=text'), 'https%3A//example.com/%3Fq%3Dtext')\n        self.assertEqual(urlquote('https://example.com/?q=t\u00e9xt'), 'https%3A//example.com/%3Fq%3Dt%C3%A9xt')\n", "    def test_parse_http_date(self):\n        # Test a valid RFC1123 date\n        date_str = \"Fri, 09 Nov 2018 23:12:40 GMT\"\n        expected_timestamp = 1541798320\n        self.assertEqual(parse_http_date(date_str), expected_timestamp)\n\n        # Test a valid RFC850 date\n        date_str = \"Friday, 09-Nov-18 23:12:40 GMT\"\n        self.assertEqual(parse_http_date(date_str), expected_timestamp)\n\n        # Test a valid ASCTIME date\n        date_str = \"Fri Nov  9 23:12:40 2018\"\n        self.assertEqual(parse_http_date(date_str), expected_timestamp)\n\n        # Test an invalid date format\n        date_str = \"Invalid date format\"\n        with self.assertRaises(ValueError):\n            parse_http_date(date_str)\n", "    def test_urlsafe_base64_decode(self):\n        encoded = urlsafe_base64_encode(b'test')\n        decoded = urlsafe_base64_decode(encoded)\n        self.assertEqual(decoded, b'test')\n\n        # Test with invalid input\n        with self.assertRaises(ValueError):\n            urlsafe_base64_decode('invalid')\n", "    def test_urlencode_with_multivalue_dict(self):\n        query = MultiValueDict({'a': ['1', '2'], 'b': ['3']})\n        self.assertEqual(urlencode(query), 'a=1&a=2&b=3')\n        self.assertEqual(urlencode(query, doseq=True), 'a=1&a=2&b=3')\n        query = MultiValueDict({'a': ['1', None]})\n        with self.assertRaisesMessage(TypeError, self.cannot_encode_none_msg):\n            urlencode(query)\n        with self.assertRaisesMessage(TypeError, self.cannot_encode_none_msg):\n            urlencode(query, doseq=True)\n"], "sample_735": ["def test_estimate_gaussian_covariances_full():\n    rng = np.random.RandomState(0)\n    data = RandomData(rng)\n\n    resp = np.abs(rng.randn(data.n_samples, data.n_components))\n    resp /= resp.sum(axis=1)[:, np.newaxis]\n    covariances = _estimate_gaussian_covariances_full(resp, data.X['full'], resp.sum(axis=0), data.means, 1e-6)\n\n    for i in range(data.n_components):\n        diff = data.X['full'] - data.means[i]\n        expected_cov = np.dot(resp[:, i] * diff.T, diff) / resp.sum(axis=0)[i]\n        expected_cov.flat[::data.n_features + 1] += 1e-6\n        assert_array_almost_equal(covariances[i], expected_cov)\n", "def test_estimate_gaussian_covariances_spherical():\n    rng = np.random.RandomState(0)\n    data = RandomData(rng)\n\n    covariances = _estimate_gaussian_covariances_spherical(\n        np.eye(data.n_components), data.X['spherical'],\n        np.ones(data.n_components), data.means, 0.)\n\n    assert_array_almost_equal(covariances, np.var(data.X['spherical'], axis=0).mean(axis=1))\n", "def test_gaussian_mixture_score_samples(self):\n    rng = np.random.RandomState(0)\n    data = RandomData(rng)\n\n    for covariance_type in COVARIANCE_TYPE:\n        gmm = GaussianMixture(n_components=data.n_components,\n                              covariance_type=covariance_type,\n                              random_state=0)\n        gmm.fit(data.X[covariance_type])\n        log_prob = gmm.score_samples(data.X[covariance_type])\n        assert_almost_equal(np.exp(log_prob).sum(), data.n_samples, decimal=2)\n", "def test_gaussian_mixture_bic_aic():\n    # Testing Bayesian Information Criterion (BIC) and\n    # Akaike Information Criterion (AIC) for Gaussian Mixture Model\n    rng = np.random.RandomState(0)\n    data = RandomData(rng)\n\n    for covar_type in COVARIANCE_TYPE:\n        gmm = GaussianMixture(n_components=2, covariance_type=covar_type,\n                              random_state=0)\n        gmm.fit(data.X[covar_type])\n\n        bic = gmm.bic(data.X[covar_type])\n        aic = gmm.aic(data.X[covar_type])\n\n        assert_greater(bic, 0, \"BIC should be positive\")\n        assert_greater(aic, 0, \"AIC should be positive\")\n        assert_greater(bic, aic, \"BIC should be greater than AIC\")\n", "def test_estimate_gaussian_covariances_spherical():\n    random_data = RandomData(np.random.RandomState(0))\n    resp = np.zeros((random_data.n_samples, random_data.n_components))\n    resp[np.arange(random_data.n_samples), random_data.Y.astype(int)] = 1\n    nk = resp.sum(axis=0) + 10 * np.finfo(resp.dtype).eps\n    means = random_data.means\n    reg_covar = 1e-6\n\n    covariances = _estimate_gaussian_covariances_spherical(resp, random_data.X['full'], nk, means, reg_covar)\n    assert_array_almost_equal(covariances, random_data.covariances['spherical'], decimal=2)\n"], "sample_932": ["compilation error", "def test_enum_declaration():\n    check('enum',\n          '{key}Enum {{ e1, e2, e3 }}',\n          {1: 'Enum1', 2: 'Enum2', 3: 'Enum3'},\n          '{key}enum {key}Enum {{ e1, e2, e3 }}')\n", "def test_parse_cast_expression():\n    check(\"member\",\n          \"void foo(int a) const id_attr paren_attr\",\n          {1: \"foo\"},\n          \"void foo(int a) const id_attr paren_attr\")\n", "def test_function_pointer():\n    check('function',\n          \"void (*f)(int) const\",\n          {1: \"void-f-int-const\"},\n          \"void ({f})(int) const\",\n          key=\"f\")\n", "def test_cpp_domain_template_alias():\n    check(\"type\", \"using T = int\", {1: \"T\"}, \"using <code class=\\\"descname\\\">T</code> = int\", key=\"T\")\n    check(\"type\", \"using T = int;\", {1: \"T\"}, \"using <code class=\\\"descname\\\">T</code> = int;\", key=\"T\")\n"], "sample_99": ["    def test_extract_second(self):\n        # Create a model instance with a known datetime\n        dt = datetime(2022, 1, 1, 12, 34, 56)\n        instance = self.create_model(dt, dt)\n\n        # Test ExtractSecond on DateTimeField\n        second = DTModel.objects.annotate(sec=ExtractSecond('start_datetime')).values_list('sec', flat=True).first()\n        self.assertEqual(second, 56)\n\n        # Test ExtractSecond on TimeField\n        time_instance = self.create_model(None, None)\n        time_instance.start_time = dt.time()\n        time_instance.save()\n        second = DTModel.objects.annotate(sec=ExtractSecond('start_time')).values_list('sec', flat=True).last()\n        self.assertEqual(second, 56)\n", "    def test_extract_week(self):\n        obj = self.create_model(datetime(2022, 1, 3), datetime(2022, 1, 9))\n        self.assertEqual(DTModel.objects.filter(start_datetime__week=1).count(), 1)\n", "    def test_extract_year(self):\n        dt_model = self.create_model(datetime(2022, 3, 15), datetime(2022, 12, 31))\n        self.assertEqual(DTModel.objects.filter(start_datetime=ExtractYear(F('start_datetime'), 'year')).first().start_datetime.year, 2022)\n", "    def test_trunc_time(self):\n        start_datetime = datetime(2022, 1, 1, 12, 30, 45)\n        end_datetime = datetime(2022, 1, 2, 13, 31, 46)\n        model = self.create_model(start_datetime, end_datetime)\n        truncated_time = DTModel.objects.annotate(truncated=TruncTime('end_datetime')).values('truncated')[0]['truncated']\n        self.assertEqual(truncated_time, end_datetime.replace(microsecond=0).time())\n", "    def test_trunc_with_timezone(self):\n        dt = datetime(2022, 3, 14, 12, 30, 45, tzinfo=datetime_timezone.utc)\n        tz = pytz.timezone('America/New_York')\n        obj = self.create_model(dt, dt + timedelta(days=1))\n\n        trunc_hour = TruncHour(F('start_datetime'), tzinfo=tz)\n        self.assertEqual(\n            DTModel.objects.filter(id=obj.id).annotate(trunc_hour=trunc_hour).get().trunc_hour,\n            truncate_to(dt, 'hour', tz)\n        )\n"], "sample_378": ["def test_bulk_update_with_expression(self):\n    Note.objects.bulk_update(self.notes, fields=['note'], note=F('misc'))\n    for note in self.notes:\n        note.refresh_from_db()\n        self.assertEqual(note.note, note.misc)\n", "def test_bulk_update_with_f_expressions(self):\n    # Create some notes\n    notes = [\n        Note.objects.create(note=str(i), misc=str(i))\n        for i in range(10)\n    ]\n\n    # Update notes using F expressions\n    Note.objects.bulk_update(notes, fields=['note'], note=F('misc'))\n\n    # Verify the updates\n    for note in notes:\n        note.refresh_from_db()\n        self.assertEqual(note.note, note.misc)\n", "def test_bulk_update_with_annotation(self):\n    self.create_tags()\n    Note.objects.bulk_update(self.notes, ['note'], batch_size=5)\n    annotated_notes = Note.objects.annotate(lower_note=Lower('note'))\n    self.assertEqual(len(annotated_notes), 10)\n    for i, note in enumerate(annotated_notes):\n        self.assertEqual(note.lower_note, str(i))\n", "    def test_bulk_update_with_invalid_fields(self):\n        with self.assertRaises(ValueError):\n            Note.objects.bulk_update(self.notes, fields=['invalid_field'])\n", "def test_bulk_update_exclude_fields(self):\n    notes_to_update = Note.objects.filter(id__lt=5)\n    update_fields = ['note']\n    new_note = 'updated note'\n\n    # Ensure that the misc field is not updated\n    original_misc_values = {note.id: note.misc for note in notes_to_update}\n    notes_to_update.update(note=new_note)\n\n    for note in notes_to_update:\n        note.refresh_from_db()\n        self.assertEqual(note.note, new_note)\n        self.assertEqual(note.misc, original_misc_values[note.id])\n"], "sample_130": ["def test_join_promotion_with_f_expression(self):\n    query = Query(Author)\n    query.add_q(Q(items__name__exact=F('name')))\n    query.promote_joins(query._lookup_joins)\n    self.assertEqual(query.alias_map[query._lookup_joins[1]].join_type, INNER)\n", "    def test_promote_joins_in_complex_query(self):\n        query = Query(Author)\n        filter_expr = (\n            'author_ptr__id',\n            Q(Q(author_ptr__rankings__rank__gt=50) | Q(author_ptr__rankings__rank__lt=30)) &\n            Q(author_ptr__rankings__title__icontains='test')\n        )\n        clause, _ = query.build_filter(filter_expr)\n        query.where.add(clause, AND)\n        joinpromoter = JoinPromoter(OR, 2, False)\n        joinpromoter.add_votes(['author_ptr'])\n        joinpromoter.update_join_types(query)\n        # Check that the join type for author_ptr is LOUTER\n        self.assertEqual(query.alias_map['author_ptr'].join_type, LOUTER)\n", "    def test_resolve_ref(self):\n        query = Query(Author)\n        query.add_annotation(Lower('name'), 'lower_name', is_summary=False)\n        result = query.resolve_ref('lower_name')\n        self.assertIsInstance(result, SimpleCol)\n        self.assertEqual(result.alias, 'lower_name')\n        self.assertIsInstance(result.target, Lower)\n", "def test_filter_query_with_transform(self):\n    qs = Item.objects.filter(name__lower='foo')\n    query = qs.query\n    self.assertEqual(query.where.children[0].children[0].__class__, Exact)\n    self.assertEqual(query.where.children[0].children[0].lhs.__class__, Lower)\n", "    def test_build_filter_with_join_promotion(self):\n        query = Query(Author)\n        query.add_filter(('name__item__value', 5))\n        query.add_filter(('name__item__objectc__value', 10))\n        query.promote_joins(set())\n        self.assertEqual(query.alias_map['item'].join_type, 'INNER')\n        self.assertEqual(query.alias_map['objectc'].join_type, 'LOUTER')\n"], "sample_23": ["def test_longitude_wrap_at():\n    lon = Longitude([-20.0, 150.0, 350.0] * u.deg)\n    wrapped_lon = lon.wrap_at(180 * u.deg)\n    assert_allclose(wrapped_lon.degree, [-20.0, -170.0, -10.0])\n    lon.wrap_at(180 * u.deg, inplace=True)\n    assert_allclose(lon.degree, [-20.0, -170.0, -10.0])\n", "def test_angle_wrap_at_scalar():\n    a = Angle(361.5, u.deg)\n    wrapped = a.wrap_at(360 * u.deg)\n    assert_allclose(wrapped.degree, 1.5)\n\n    a = Angle(-0.5, u.deg)\n    wrapped = a.wrap_at(360 * u.deg)\n    assert_allclose(wrapped.degree, 359.5)\n\n    a = Angle(181, u.deg)\n    wrapped = a.wrap_at('180d')\n    assert_allclose(wrapped.degree, -179)\n\n    a = Angle(-179, u.deg)\n    wrapped = a.wrap_at('180d')\n    assert_allclose(wrapped.degree, 181)\n", "def test_angle_to_string():\n    a = Angle(30, unit=u.degree)\n    assert a.to_string() == \"30.0 deg\"\n    assert a.to_string(unit=u.radian, decimal=True) == \"0.5235987755982988\"\n    assert a.to_string(decimal=True, precision=2) == \"30.00 deg\"\n    assert a.to_string(sep=\":\") == \"30:00:00.0 deg\"\n    assert a.to_string(sep=\":\", decimal=True) == \"30.0\"\n    assert a.to_string(sep=\":\", decimal=True, precision=2) == \"30.00\"\n    assert a.to_string(sep=\":\", fields=2) == \"30:00 deg\"\n    assert a.to_string(format=\"latex\") == \"$30.0^{\\circ}$\"\n    assert a.to_string(format=\"unicode\") == \"30.0\u00b0\"\n", "def test_angle_to_string_format():\n    a = Angle([10.2345, -20.6789], unit=u.deg)\n\n    # Test unicode format\n    expected_unicode = [\"10.2345\u00b0\", \"-20.6789\u00b0\"]\n    assert a.to_string(format='unicode') == expected_unicode\n\n    # Test latex format\n    expected_latex = [\"$10.2345^{\\\\circ}$\", \"$-20.6789^{\\\\circ}$\"]\n    assert a.to_string(format='latex') == expected_latex\n\n    # Test latex_inline format\n    expected_latex_inline = expected_latex\n    assert a.to_string(format='latex_inline') == expected_latex_inline\n", "def test_latitude_within_bounds():\n    # Test the is_within_bounds method for Latitude\n    lat = Latitude([-90, 0, 90], unit=u.deg)\n    assert lat.is_within_bounds(-90*u.deg, 90*u.deg)\n    assert not lat.is_within_bounds(-89.999*u.deg, 90*u.deg)\n    assert not lat.is_within_bounds(-90*u.deg, 89.999*u.deg)\n\n    # Test Latitude outside the range\n    with pytest.raises(ValueError):\n        Latitude([-90.001, 0, 90], unit=u.deg)\n    with pytest.raises(ValueError):\n        Latitude([-90, 0, 90.001], unit=u.deg)\n\n    # Test assignment of Latitude outside the range\n    lat = Latitude(0, unit=u.deg)\n    with pytest.raises(ValueError):\n        lat[0] = 90.001\n"], "sample_1135": ["def test_unevaluated_Mul():\n    from sympy import _unevaluated_Mul as uMul\n    m = uMul(sqrt(2), sqrt(3))\n    assert m == uMul(sqrt(3), sqrt(2))\n    assert m == Mul(sqrt(3), sqrt(2), evaluate=False)\n    assert m != Mul(*m.args)\n", "def test_unevaluated_Mul():\n    assert same_and_same_prec(_unevaluated_Mul(S(3.0), x, S(2)), Mul(6.0, x, evaluate=False))\n    assert _unevaluated_Mul(sqrt(2), sqrt(3)) == _unevaluated_Mul(sqrt(3), sqrt(2))\n    assert _unevaluated_Mul(Mul(sqrt(3), sqrt(2), evaluate=False)) == _unevaluated_Mul(sqrt(3), sqrt(2))\n", "def test_mul_nseries():\n    expr = Mul(x, sin(x))\n    series = expr.nseries(x, n=2)\n    expected = x**2/2 - x**4/24 + O(x**6)\n    assert same_and_same_prec(series, expected)\n", "def test_eval_expand_mul_with_add_and_deep():\n    expr = (x + y)*(a + b)\n    result = expr.expand(deep=True)\n    assert result == a*x + a*y + b*x + b*y\n", "def test_eval_is_composite():\n    assert (2*3).is_composite == True\n    assert (2*3*4).is_composite == True\n    assert (2*3*5).is_composite == True\n    assert (2*3*7).is_composite == False\n    assert (2*3*9).is_composite == True\n    assert (2*3*10).is_composite == False\n"], "sample_556": ["def test_figure_set_layout_engine():\n    fig = Figure()\n    fig.set_layout_engine('tight')\n    assert isinstance(fig.get_layout_engine(), TightLayoutEngine)\n    fig.set_layout_engine('constrained')\n    assert isinstance(fig.get_layout_engine(), ConstrainedLayoutEngine)\n    fig.set_layout_engine(None)\n    assert fig.get_layout_engine() is None\n", "def test_figure_align_labels():\n    fig, axs = plt.subplots(2, 2)\n    for ax in axs.flat:\n        ax.plot(np.arange(10), np.random.rand(10))\n    axs[0, 0].set_xlabel('X Label 0')\n    axs[0, 1].set_xlabel('X Label 1')\n    axs[1, 0].set_xlabel('X Label 2')\n    axs[1, 1].set_xlabel('X Label 3')\n    fig.align_labels()\n    plt.close(fig)\n", "def test_figure_align_labels():\n    fig, axs = plt.subplots(2, 2)\n    for i, ax in enumerate(axs.flat):\n        ax.set_xlabel(f'XLabel {i}')\n        ax.set_ylabel(f'YLabel {i}')\n    fig.align_labels()\n", "def test_figure_align_labels():\n    fig, axs = plt.subplots(2, 2)\n    for ax in axs.flat:\n        ax.set(xlabel='XLabel', ylabel='YLabel')\n    fig.align_labels()\n", "def test_figure_align_labels():\n    fig, axs = plt.subplots(2, 2, figsize=(8, 6))\n    for ax in axs.flat:\n        ax.plot(np.arange(0, 10, 0.1))\n    axs[0, 0].set_ylabel('YLabel 0')\n    axs[0, 1].set_ylabel('YLabel 1')\n    axs[1, 0].set_xlabel('XLabel 0')\n    axs[1, 1].set_xlabel('XLabel 1')\n    fig.align_labels()\n"], "sample_371": ["    def test_get_traceback_data(self):\n        request = HttpRequest()\n        request.user = User()\n        exc_type = Exception\n        exc_value = Exception(\"Test Exception\")\n        tb = exc_value.__traceback__\n        reporter = ExceptionReporter(request, exc_type, exc_value, tb)\n        data = reporter.get_traceback_data()\n        self.assertIn('request', data)\n        self.assertIn('user_str', data)\n        self.assertIn('exception_type', data)\n        self.assertIn('exception_value', data)\n        self.assertIn('frames', data)\n", "    def test_cleanse_setting(self):\n        filter = SafeExceptionReporterFilter()\n        self.assertEqual(filter.cleanse_setting('SECRET_KEY', 'secret'), filter.cleansed_substitute)\n        self.assertEqual(filter.cleanse_setting('NOT_SECRET', 'value'), 'value')\n        self.assertIsInstance(filter.cleanse_setting('CALLABLE', lambda: 'value'), CallableSettingWrapper)\n", "    def test_get_traceback_frames(self):\n        exc_value = Exception(\"Test exception\")\n        exc_type = type(exc_value)\n        tb = exc_value.__traceback__\n        reporter = ExceptionReporter(None, exc_type, exc_value, tb)\n        frames = reporter.get_traceback_frames()\n        self.assertIsInstance(frames, list)\n        self.assertTrue(all(isinstance(frame, dict) for frame in frames))\n", "    def setUp(self):\n        self.request_factory = RequestFactory()\n", "    def setUp(self):\n        self.request_factory = RequestFactory()\n        self.exc_type = ValueError\n        self.exc_value = ValueError('Test error')\n        self.tb = self.exc_value.__traceback__\n        self.reporter = ExceptionReporter(None, self.exc_type, self.exc_value, self.tb)\n"], "sample_384": ["def test_bulk_update_with_pk_fields_raises_error(self):\n    with self.assertRaisesMessage(ValueError, \"bulk_update() cannot be used with primary key fields.\"):\n        Note.objects.bulk_update(self.notes, fields=[\"id\"])\n", "def test_bulk_update_with_f_expressions(self):\n    self.create_tags()\n    notes = Note.objects.all()\n    tags = Tag.objects.all()\n    notes.update(tag=F(\"tag\") + 1)\n    notes = Note.objects.all()\n    for i, note in enumerate(notes):\n        self.assertEqual(note.tag, tags[i + 1])\n", "def test_bulk_update_with_expression(self):\n    self.create_tags()\n    updated_count = Note.objects.bulk_update(self.notes, {'note': F('misc')})\n    self.assertEqual(updated_count, len(self.notes))\n    for note in self.notes:\n        note.refresh_from_db()\n        self.assertEqual(note.note, note.misc)\n", "def test_bulk_update_with_F_expressions(self):\n    self.create_tags()\n    updated_count = Note.objects.bulk_update(self.notes, {'note': F('misc')})\n    self.assertEqual(updated_count, 10)\n    for note in Note.objects.all():\n        self.assertEqual(note.note, note.misc)\n", "def test_bulk_update_exclude_fields(self):\n    Note.objects.bulk_update(self.notes, {'note': 'updated'})\n    self.notes = Note.objects.all()\n    for note in self.notes:\n        self.assertEqual(note.note, 'updated')\n        # Misc field should remain unchanged\n        self.assertEqual(note.misc, str(note.id))\n"], "sample_203": ["    def test_allowed_extension(self):\n        validator = validators.FileExtensionValidator(allowed_extensions=['txt'])\n        valid_file = SimpleUploadedFile('file.txt', b'file_content')\n        validator(valid_file)\n", "    def test_file_extension_validator(self):\n        validator = validators.FileExtensionValidator(allowed_extensions=['pdf', 'jpg'])\n\n        # Test valid file extensions\n        validator(SimpleUploadedFile(\"file.pdf\", b\"file_content\"))\n        validator(SimpleUploadedFile(\"file.jpg\", b\"file_content\"))\n\n        # Test invalid file extensions\n        with self.assertRaises(ValidationError):\n            validator(SimpleUploadedFile(\"file.txt\", b\"file_content\"))\n        with self.assertRaises(ValidationError):\n            validator(SimpleUploadedFile(\"file.exe\", b\"file_content\"))\n", "def test_validate_ipv6_address(self):\n    # Test valid IPv6 addresses\n    valid_ipv6_addresses = [\"2001:0db8:85a3:0000:0000:8a2e:0370:7334\", \"::1\", \"fe80::1ff:fe23:4567:890a\"]\n    for ip in valid_ipv6_addresses:\n        self.assertIsNone(validators.validate_ipv6_address(ip))\n\n    # Test invalid IPv6 addresses\n    invalid_ipv6_addresses = [\"2001:0db8:85a3::8a2e:0370:7334\", \"2001:db8::1:1\", \":::1\"]\n    for ip in invalid_ipv6_addresses:\n        with self.assertRaises(ValidationError):\n            validators.validate_ipv6_address(ip)\n", "def test_file_extension_validator(self):\n    validator = validators.FileExtensionValidator(allowed_extensions=['txt', 'pdf'])\n    validator(SimpleUploadedFile(\"test.txt\", b\"file_content\"))\n    validator(SimpleUploadedFile(\"test.pdf\", b\"file_content\"))\n\n    with self.assertRaises(ValidationError) as context:\n        validator(SimpleUploadedFile(\"test.doc\", b\"file_content\"))\n    self.assertEqual(context.exception.code, 'invalid_extension')\n", "def test_validate_email(self):\n    # Test that valid emails are accepted\n    self.assertIsNone(validators.validate_email('test@example.com'))\n    self.assertIsNone(validators.validate_email('test.test@example.com'))\n    self.assertIsNone(validators.validate_email('test-test@example.com'))\n    self.assertIsNone(validators.validate_email('test_test@example.com'))\n    self.assertIsNone(validators.validate_email('test+test@example.com'))\n    self.assertIsNone(validators.validate_email('test@[127.0.0.1]'))\n    self.assertIsNone(validators.validate_email('test@[::1]'))\n    self.assertIsNone(validators.validate_email('test@localhost'))\n\n    # Test that invalid emails raise ValidationError\n    with self.assertRaises(ValidationError):\n        validators.validate_email('test@example')\n    with self.assertRaises(ValidationError):\n        validators.validate_email('test@example..com')\n    with self.assertRaises(ValidationError):\n        validators.validate_email('test@example.com.')\n    with self.assertRaises(ValidationError):\n        validators.validate_email('test@[127.0.0.0.1]')\n    with self.assertRaises(ValidationError):\n        validators.validate_email('test@[::1:1]')\n    with self.assertRaises(ValidationError):\n        validators.validate_email('test@')\n    with self.assertRaises(ValidationError):\n        validators.validate_email('@example.com')\n    with self.assertRaises(ValidationError):\n        validators.validate_email('test@example com')\n"], "sample_918": ["def test_parse_annotation():\n    annotation = \"List[Union[int, str]]\"\n    nodes = _parse_annotation(annotation)\n    expected_nodes = [\n        pending_xref('', nodes.Text('List'), refdomain='py', reftype='class', reftarget='List'),\n        nodes.Text('['),\n        pending_xref('', nodes.Text('Union'), refdomain='py', reftype='class', reftarget='Union'),\n        nodes.Text('['),\n        pending_xref('', nodes.Text('int'), refdomain='py', reftype='class', reftarget='int'),\n        nodes.Text(', '),\n        pending_xref('', nodes.Text('str'), refdomain='py', reftype='class', reftarget='str'),\n        nodes.Text(']'),\n        nodes.Text(']'),\n    ]\n    assert nodes == expected_nodes\n", "def test_parse_annotation():\n    annotation = \"List[Union[str, int]]\"\n    result = _parse_annotation(annotation)\n    assert isinstance(result, list)\n    assert len(result) == 5\n    assert isinstance(result[0], pending_xref)\n    assert result[0]['reftarget'] == 'List'\n    assert isinstance(result[1], nodes.Text)\n    assert result[1].astext() == '['\n    assert isinstance(result[2], pending_xref)\n    assert result[2]['reftarget'] == 'Union'\n    assert isinstance(result[3], nodes.Text)\n    assert result[3].astext() == '['\n    assert isinstance(result[4], nodes.Text)\n    assert result[4].astext() == 'str'\n", "def test_parse_annotation_with_subscript():\n    annotation = \"List[Tuple[int, str]]\"\n    result = _parse_annotation(annotation)\n    assert isinstance(result, list)\n    assert len(result) == 7\n    assert isinstance(result[0], nodes.Text)\n    assert result[0].astext() == \"List\"\n    assert isinstance(result[1], pending_xref)\n    assert result[1].astext() == \"[\"\n    assert isinstance(result[2], nodes.Text)\n    assert result[2].astext() == \"Tuple\"\n    assert isinstance(result[3], pending_xref)\n    assert result[3].astext() == \"[\"\n    assert isinstance(result[4], nodes.Text)\n    assert result[4].astext() == \"int\"\n    assert isinstance(result[5], pending_xref)\n    assert result[5].astext() == \",\"\n    assert isinstance(result[6], nodes.Text)\n    assert result[6].astext() == \"str\"\n", "def test_parse_annotation():\n    annotation = \"List[Dict[str, int]]\"\n    expected_output = [\n        pending_xref('', nodes.Text('List'), refdomain='py', reftype='class', reftarget='List'),\n        nodes.Text('['),\n        pending_xref('', nodes.Text('Dict'), refdomain='py', reftype='class', reftarget='Dict'),\n        nodes.Text('['),\n        nodes.Text('str'),\n        nodes.Text(', '),\n        nodes.Text('int'),\n        nodes.Text(']'),\n        nodes.Text(']')\n    ]\n    result = _parse_annotation(annotation)\n    assert len(result) == len(expected_output)\n    for i in range(len(result)):\n        assert_node(result[i], expected_output[i])\n", "def test_parse_annotation():\n    annotation = 'List[Union[int, str]]'\n    result = _parse_annotation(annotation)\n    assert len(result) == 6\n    assert_node(result[0], [pending_xref, nodes.Text])\n    assert_node(result[1], nodes.Text, text='[')\n    assert_node(result[2], [pending_xref, nodes.Text])\n    assert_node(result[3], nodes.Text, text=', ')\n    assert_node(result[4], [pending_xref, nodes.Text])\n    assert_node(result[5], nodes.Text, text=']')\n"], "sample_369": ["def test_detect_rename_model_with_constraints(self):\n    before_states = [\n        self.author_name_check_constraint,\n        self.book_with_author_renamed,\n    ]\n    after_states = [\n        self.author_name,\n        self.book_with_field_and_author_renamed,\n    ]\n    changes = self.get_changes(before_states, after_states)\n    self.assertNumberMigrations(changes, 'testapp', 2)\n    self.assertOperationTypes(changes, 'testapp', 0, ['RemoveConstraint'])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name='name_contains_bob')\n    self.assertOperationTypes(changes, 'testapp', 1, ['RenameModel'])\n    self.assertOperationAttributes(changes, 'testapp', 1, 0, old_name='Author', new_name='Writer')\n    self.assertOperationTypes(changes, 'otherapp', 0, ['RenameField'])\n    self.assertOperationAttributes(changes, 'otherapp', 0, 0, model_name='Book', old_name='author', new_name='writer')\n", "def test_deconstructible_object_equality(self):\n    self.assertEqual(\n        DeconstructibleObject(1, 2, a=3, b=4),\n        DeconstructibleObject(1, 2, a=3, b=4),\n    )\n    self.assertNotEqual(\n        DeconstructibleObject(1, 2, a=3, b=4),\n        DeconstructibleObject(1, 2, a=3, b=5),\n    )\n    self.assertNotEqual(\n        DeconstructibleObject(1, 2, a=3, b=4),\n        DeconstructibleObject(1, 2, a=3),\n    )\n", "def test_alter_field_default_deconstructible_object_changed(self):\n    changes = self.get_changes([self.author_name_deconstructible_1], [self.author_name_deconstructible_2])\n    self.assertOperationFieldAttributes(\n        changes,\n        \"testapp\",\n        0,\n        0,\n        default=DeconstructibleObject()  # The default has not changed\n    )\n", "def test_deconstructible_default_with_no_changes(self):\n    changes = self.get_changes(\n        [self.author_name_deconstructible_1],\n        [self.author_name_deconstructible_2]\n    )\n    self.assertNumberMigrations(changes, \"testapp\", 0)\n", "def test_auto_now_and_auto_now_add_detection(self):\n    questioner = MigrationQuestioner(specified_apps=None, dry_run=True)\n    changes = self.get_changes([self.author_empty], [self.author_dates_of_birth_auto_now], questioner)\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['AddField'])\n    self.assertOperationFieldAttributes(changes, 'testapp', 0, 0, auto_now=True)\n    self.assertOperationFieldAttributes(changes, 'testapp', 0, 1, auto_now=True)\n    self.assertOperationFieldAttributes(changes, 'testapp', 0, 2, auto_now=True)\n\n    changes = self.get_changes([self.author_dates_of_birth_auto_now], [self.author_dates_of_birth_auto_now_add], questioner)\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, ['AlterField'])\n    self.assertOperationFieldAttributes(changes, 'testapp', 0, 0, auto_now_add=True)\n    self.assertOperationFieldAttributes(changes, 'testapp', 0, 1, auto_now_add=True)\n    self.assertOperationFieldAttributes(changes, 'testapp', 0, 2, auto_now_add=True)\n"], "sample_974": ["def test_sign_function():\n    assert ccode(sign(x)) == '((({0}) > 0) - (({0}) < 0))'.format(ccode(x))\n", "def test_print_For():\n    i = symbols('i')\n    expr = For(i, Range(0, 10), Assignment(y, y + i))\n    result = ccode(expr)\n    expected = \"for (i = 0; i < 10; i += 1) {\\n   y = y + i;\\n}\"\n    assert result == expected\n", "def test_print_Matrix():\n    mat = Matrix([x**2, Piecewise((x + 1, x > 0), (x, True)), sin(x)])\n    A = MatrixSymbol('A', 3, 1)\n    expected_output = \"A[0] = pow(x, 2);\\nif (x > 0) {\\n   A[1] = x + 1;\\n}\\nelse {\\n   A[1] = x;\\n}\\nA[2] = sin(x);\"\n    assert ccode(mat, A) == expected_output\n", "def test_c_code_printer_augmented_assignment():\n    expr = aug_assign(x, \"+\", y)\n    result = ccode(expr)\n    assert result == \"x += y;\"\n", "def test_CCodePrinter_contract():\n    len_y = 5\n    y = IndexedBase('y', shape=(len_y,))\n    t = IndexedBase('t', shape=(len_y,))\n    Dy = IndexedBase('Dy', shape=(len_y-1,))\n    i = Idx('i', len_y-1)\n    e = Eq(Dy[i], (y[i+1]-y[i])/(t[i+1]-t[i]))\n    assert ccode(e.rhs, assign_to=e.lhs, contract=True) == \"for (int i=0; i<4; i++) {\\n   Dy[i] = (y[i + 1] - y[i])/(t[i + 1] - t[i]);\\n}\"\n"], "sample_388": ["def test_unknown_user(self):\n    response = self.client.get(\"/\", **{self.header: \"unknownuser\"})\n    self.assertEqual(response.status_code, 200)\n    self.assertTrue(User.objects.filter(username=\"unknownuser\").exists())\n", "    def test_user_can_authenticate(self):\n        user = User.objects.create_user(username='testuser', password='testpassword', is_active=False)\n        backend = AllowAllUsersModelBackend()\n        self.assertTrue(backend.user_can_authenticate(user))\n", "    def setUp(self):\n        self.patched_settings = modify_settings(\n            AUTHENTICATION_BACKENDS={\"append\": self.backend},\n            MIDDLEWARE={\"append\": self.middleware},\n            AUTHENTICATION_BACKENDS={\"remove\": \"django.contrib.auth.backends.AllowAllUsersRemoteUserBackend\"},\n        )\n        self.patched_settings.enable()\n", "    def setUp(self):\n        self.patched_settings = modify_settings(\n            AUTHENTICATION_BACKENDS={\"append\": self.backend},\n            MIDDLEWARE={\"append\": self.middleware},\n        )\n        self.patched_settings.enable()\n        self.user = User.objects.create_user(username='testuser', password='testpassword')\n", "    def setUp(self):\n        self.patched_settings = modify_settings(\n            AUTHENTICATION_BACKENDS={\"append\": self.backend},\n            MIDDLEWARE={\"append\": self.middleware},\n            REMOTE_USER_CREATE_UNKNOWN_USER=True,\n            REMOTE_USER_HEADER=self.header,\n            REMOTE_USER_EMAIL_HEADER=self.email_header,\n        )\n        self.patched_settings.enable()\n"], "sample_817": ["def test_variance_threshold_sparse_matrix():\n    X = csr_matrix(data)\n    selector = VarianceThreshold(threshold=0.5)\n    selector.fit(X)\n    support_mask = selector._get_support_mask()\n    expected_mask = np.array([True, True, True, True, True])\n    assert_array_equal(support_mask, expected_mask)\n", "def test_fit_transform_with_sparse_matrix():\n    X = csr_matrix(data)\n    selector = VarianceThreshold(threshold=1.0)\n    selector.fit(X)\n    X_new = selector.transform(X)\n    assert_array_equal(X_new.toarray(), [[1, 2, 3, 5], [1, 0, 3, 5], [1, 0, 4, 0]])\n", "def test_variance_threshold():\n    X = np.array(data)\n    selector = VarianceThreshold(threshold=1.0)\n    selector.fit(X)\n    expected_variances = np.array([1.0, 0.66666667, 0.0, 0.22222222, 4.0])\n    assert_array_equal(selector.variances_, expected_variances)\n    expected_support = np.array([True, True, False, False, True])\n    assert_array_equal(selector._get_support_mask(), expected_support)\n    expected_transformed_data = np.array([[1, 2, 5], [2, 2, 5], [1, 2, 0]])\n    assert_array_equal(selector.transform(X), expected_transformed_data)\n", "def test_variance_threshold():\n    X = np.array(data)\n    selector = VarianceThreshold(threshold=1.0)\n    selector.fit(X)\n    expected_variances = np.var(X, axis=0)\n    assert_array_equal(selector.variances_, expected_variances)\n    expected_support = [True, True, True, True, False]\n    assert_array_equal(selector._get_support_mask(), expected_support)\n", "def test_variance_threshold():\n    X = np.array(data)\n    selector = VarianceThreshold()\n    selector.fit(X)\n    X_new = selector.transform(X)\n    expected_variances = np.var(X, axis=0)\n    expected_X_new = np.delete(X, np.where(expected_variances <= selector.threshold), axis=1)\n    assert_array_equal(selector.variances_, expected_variances)\n    assert_array_equal(X_new, expected_X_new)\n"], "sample_259": ["def test_prefetch_related_objects_with_prefetch(self):\n    authors = Author.objects.all()\n    prefetch = Prefetch('books_written', queryset=Book.objects.filter(title__startswith='W'))\n    prefetch_related_objects(authors, prefetch)\n    self.assertIn('books_written', authors[0]._prefetched_objects_cache)\n    self.assertEqual(authors[0].books_written.count(), 1)\n    self.assertEqual(authors[0].books_written.first().title, 'Wuthering Heights')\n", "def test_prefetch_related_objects_basic(self):\n    authors = Author.objects.all()\n    prefetch_related_objects(authors, 'books_written')\n    for author in authors:\n        with self.assertNumQueries(0):\n            self.assertEqual(author.books_written.count(), 3 if author.name == 'Charlotte' else 1)\n", "def test_prefetch_related_objects_with_multiple_lookups(self):\n    readers = list(Reader.objects.all())\n    prefetch_related_objects(\n        readers,\n        Prefetch('books_read'),\n        Prefetch('books_read__authors'),\n    )\n    for reader in readers:\n        self.assertTrue(hasattr(reader, '_prefetched_objects_cache'))\n        self.assertIn('books_read', reader._prefetched_objects_cache)\n        for book in reader.books_read.all():\n            self.assertTrue(hasattr(book, '_prefetched_objects_cache'))\n            self.assertIn('authors', book._prefetched_objects_cache)\n", "def test_prefetch_related_objects(self):\n    authors = Author.objects.all()\n    prefetch_related_objects(authors, 'book_set', 'reader_set')\n    for author in authors:\n        self.assertTrue(hasattr(author, '_prefetched_objects_cache'))\n        self.assertIn('book_set', author._prefetched_objects_cache)\n        self.assertIn('reader_set', author._prefetched_objects_cache)\n        for book in author.book_set.all():\n            self.assertEqual(book.authors.all()[0], author)\n        for reader in author.reader_set.all():\n            self.assertIn(reader, [r for r in Reader.objects.all() if author in r.books_read.all()])\n", "def test_prefetch_related_objects_with_multiple_lookups(self):\n    \"\"\"\n    Test prefetch_related_objects with multiple lookups.\n    \"\"\"\n    authors = [self.author1, self.author2, self.author3, self.author4]\n    prefetch_related_objects(authors, 'books_written', 'books_written__readers')\n\n    # Verify that the prefetching was successful\n    for author in authors:\n        self.assertTrue(hasattr(author, '_prefetched_objects_cache'))\n        self.assertIn('books_written', author._prefetched_objects_cache)\n        self.assertIsInstance(author._prefetched_objects_cache['books_written'], list)\n        for book in author._prefetched_objects_cache['books_written']:\n            self.assertTrue(hasattr(book, '_prefetched_objects_cache'))\n            self.assertIn('readers', book._prefetched_objects_cache)\n            self.assertIsInstance(book._prefetched_objects_cache['readers'], list)\n"], "sample_169": ["def test_xml_serialization(self):\n    obj = JSONModel.objects.create(\n        name='Test',\n        json_field={'key': 'value'}\n    )\n    serialized = serializers.serialize('xml', [obj])\n    deserialized = list(serializers.deserialize('xml', serialized))\n    self.assertEqual(deserialized[0].object, obj)\n", "    def test_serialization_deserialization(self):\n        obj = JSONModel.objects.create(json_field={'key': 'value'})\n        serialized_obj = serializers.serialize('xml', [obj])\n        deserialized_obj = list(serializers.deserialize('xml', serialized_obj))[0].object\n        self.assertEqual(obj.json_field, deserialized_obj.json_field)\n", "    def test_deserialization_of_json_field(self):\n        json_obj = {\"key\": \"value\"}\n        model_obj = JSONModel(json_field=json_obj)\n        model_obj.save()\n\n        data = serializers.serialize('xml', JSONModel.objects.all())\n        deserialized_data = list(serializers.deserialize('xml', data))\n\n        self.assertEqual(len(deserialized_data), 1)\n        self.assertEqual(deserialized_data[0].object.json_field, json_obj)\n", "    def test_serializer_indent(self):\n        ser = serializers.Serializer()\n        ser.options = {'indent': 2}\n        ser.xml = mock.MagicMock()\n        ser.indent(1)\n        ser.xml.ignorableWhitespace.assert_called_with('\\n  ')\n", "    def test_serializer_deserializer_roundtrip(self):\n        original_data = {\"key\": \"value\", \"nested\": {\"key\": 123}}\n        obj = JSONModel.objects.create(json_field=original_data)\n\n        serialized_data = serializers.serialize('xml', JSONModel.objects.all())\n        deserialized_obj = list(serializers.deserialize('xml', serialized_data))[0].object\n\n        self.assertEqual(obj.json_field, deserialized_obj.json_field)\n        self.assertEqual(obj.json_field, original_data)\n"], "sample_561": ["def test_marker_style_transformed():\n    marker = markers.MarkerStyle('o')\n    transformed_marker = marker.transformed(Affine2D().scale(2))\n    assert transformed_marker.get_transform() != marker.get_transform()\n", "def test_marker_transform():\n    marker = markers.MarkerStyle(\"o\")\n    transformed_marker = marker.transformed(Affine2D().rotate_deg(45))\n    assert transformed_marker.get_transform() != marker.get_transform()\n", "def test_marker_style_transformed():\n    marker = markers.MarkerStyle('o')\n    transform = Affine2D().scale(2)\n    transformed_marker = marker.transformed(transform)\n    assert np.allclose(transformed_marker.get_transform().get_matrix(), transform.get_matrix())\n", "def test_marker_scale():\n    marker = markers.MarkerStyle('o')\n    scaled_marker = marker.scaled(2)\n    assert np.all(scaled_marker.get_path().vertices == 2 * marker.get_path().vertices)\n", "def test_marker_transformed():\n    ms = markers.MarkerStyle('o')\n    new_ms = ms.transformed(Affine2D().scale(2))\n    assert np.allclose(new_ms.get_path().vertices, ms.get_path().vertices * 2)\n"], "sample_374": ["def test_get_prefetcher_with_cached_property(self):\n    author = AuthorWithAge.objects.create(name='John', age=30)\n    prefetcher, _, attr_found, is_fetched = get_prefetcher(author, 'author_age', 'author_age')\n    self.assertIsNone(prefetcher)\n    self.assertTrue(attr_found)\n    self.assertFalse(is_fetched(author))\n    self.assertEqual(author.author_age, 30)\n    self.assertTrue(is_fetched(author))\n", "def test_prefetch_related_with_custom_queryset(self):\n    Book.objects.all().delete()\n    book1 = Book.objects.create(title='Book 1')\n    book2 = Book.objects.create(title='Book 2')\n    author1 = Author.objects.create(name='Author 1', first_book=book1)\n    author2 = Author.objects.create(name='Author 2', first_book=book2)\n    book1.authors.add(author1)\n    book2.authors.add(author2)\n\n    authors_queryset = Author.objects.filter(name__startswith='Author')\n    with self.assertNumQueries(2):\n        books = Book.objects.all().prefetch_related(Prefetch('authors', queryset=authors_queryset))\n        self.assertEqual(books[0].authors.all()[0].name, 'Author 1')\n        self.assertEqual(books[1].authors.all()[0].name, 'Author 2')\n", "def test_prefetch_related_with_multiple_lookups(self):\n    with CaptureQueriesContext(connection) as queries:\n        authors = Author.objects.all().prefetch_related('books', 'reader')\n        prefetch_related_objects(authors, 'books__bookreview_set', 'reader__books_read')\n    self.assertEqual(len(queries), 3)\n    self.assertWhereContains(queries[1].sql, '`queries_author`.`id`')\n    self.assertWhereContains(queries[2].sql, '`queries_reader`.`id`')\n", "def test_get_prefetcher(self):\n    # Test that get_prefetcher() correctly identifies the prefetcher for a given attribute.\n    prefetcher, descriptor, attr_found, is_fetched = get_prefetcher(self.reader1, 'books_read', 'books_read')\n    self.assertIsNotNone(prefetcher)\n    self.assertIsNotNone(descriptor)\n    self.assertTrue(attr_found)\n    self.assertTrue(callable(is_fetched))\n\n    # Test that get_prefetcher() correctly identifies when an attribute is not found.\n    prefetcher, descriptor, attr_found, is_fetched = get_prefetcher(self.reader1, 'nonexistent_attr', 'nonexistent_attr')\n    self.assertIsNone(prefetcher)\n    self.assertIsNone(descriptor)\n    self.assertFalse(attr_found)\n    self.assertTrue(callable(is_fetched))\n\n    # Test that get_prefetcher() correctly identifies when an attribute is a cached property.\n    prefetcher, descriptor, attr_found, is_fetched = get_prefetcher(self.author1, 'first_book', 'first_book')\n    self.assertIsNone(prefetcher)\n    self.assertIsInstance(descriptor, property)\n    self.assertTrue(attr_found)\n    self.assertTrue(callable(is_fetched))\n", "def test_prefetch_related_with_cached_property(self):\n    AuthorWithAge.objects.create(name='Author1', age=30)\n    AuthorWithAge.objects.create(name='Author2', age=35)\n    authors = AuthorWithAge.objects.all()\n\n    # Access the cached property to populate the cache.\n    for author in authors:\n        author.double_age\n\n    # Reset the query count and prefetch the related objects.\n    with self.assertNumQueries(1):\n        prefetch_related_objects(authors, 'bio')\n\n    # Access the cached property again, this time it should be fetched from the cache.\n    for author in authors:\n        _ = author.double_age\n"], "sample_910": ["def test_is_suppressed_warning():\n    # Test when type is None\n    assert is_suppressed_warning(None, None, []) is False\n\n    # Test when type is not None but suppress_warnings is empty\n    assert is_suppressed_warning('some_type', None, []) is False\n\n    # Test when type matches a suppress_warnings entry\n    assert is_suppressed_warning('some_type', None, ['some_type']) is True\n\n    # Test when type matches a suppress_warnings entry with subtype\n    assert is_suppressed_warning('some_type', 'sub_type', ['some_type.sub_type']) is True\n\n    # Test when type matches a suppress_warnings entry with wildcard subtype\n    assert is_suppressed_warning('some_type', 'sub_type', ['some_type.*']) is True\n\n    # Test when type does not match a suppress_warnings entry\n    assert is_suppressed_warning('some_type', None, ['other_type']) is False\n\n    # Test when type matches a suppress_warnings entry but subtype does not\n    assert is_suppressed_warning('some_type', 'sub_type', ['some_type.other_sub_type']) is False\n", "def test_is_suppressed_warning():\n    assert is_suppressed_warning('ref', None, ['ref']) is True\n    assert is_suppressed_warning('ref', None, ['ref.python']) is False\n    assert is_suppressed_warning('ref', 'python', ['ref.python']) is True\n    assert is_suppressed_warning('ref', 'c', ['ref.python']) is False\n    assert is_suppressed_warning('ref', 'python', ['ref.*']) is True\n    assert is_suppressed_warning('ref', 'c', ['ref.*']) is True\n    assert is_suppressed_warning('ref', None, ['ref.*']) is False\n    assert is_suppressed_warning('ref', 'python', ['*']) is False\n    assert is_suppressed_warning(None, None, ['ref.python']) is False\n", "def test_warning_suppressor(app):\n    # Test the WarningSuppressor filter\n    logger = logging.getLogger('test')\n    handler = logging.StreamHandler(sys.stdout)\n    suppressor = logging.WarningSuppressor(app)\n    handler.addFilter(suppressor)\n    logger.addHandler(handler)\n\n    app.config.suppress_warnings = ['test_type.test_subtype']\n\n    # Test a warning that should be suppressed\n    record = logging.LogRecord('test', logging.WARNING, __file__, 0, 'Test message', (), None, 'test_type', 'test_subtype')\n    assert suppressor.filter(record) is False\n\n    # Test a warning that should not be suppressed\n    record = logging.LogRecord('test', logging.WARNING, __file__, 0, 'Test message', (), None, 'other_type', 'other_subtype')\n    assert suppressor.filter(record) is True\n\n    # Test a non-warning message\n    record = logging.LogRecord('test', logging.INFO, __file__, 0, 'Test message', (), None)\n    assert suppressor.filter(record) is True\n", "def test_is_suppressed_warning():\n    assert is_suppressed_warning('ref', 'unknown', ['ref.unknown'])\n    assert is_suppressed_warning('ref', 'unknown', ['ref.unknown', 'ref.py:module'])\n    assert is_suppressed_warning('ref', 'py:module', ['ref.unknown', 'ref.py:module'])\n    assert is_suppressed_warning('ref', 'py:module', ['ref.py:module', 'ref.unknown'])\n    assert not is_suppressed_warning('ref', 'unknown', ['ref.py:module'])\n    assert not is_suppressed_warning('ref', 'py:module', ['ref.unknown'])\n    assert is_suppressed_warning('ref', 'unknown', ['ref.*'])\n    assert is_suppressed_warning('ref', 'py:module', ['ref.*'])\n    assert not is_suppressed_warning('ref', 'unknown', ['ref'])\n    assert not is_suppressed_warning('ref', None, ['ref.unknown'])\n    assert not is_suppressed_warning(None, 'unknown', ['ref.unknown'])\n    assert not is_suppressed_warning(None, None, ['ref.unknown'])\n", "def test_is_suppressed_warning():\n    assert is_suppressed_warning(\"ref.python\", \"unknown\", [\"ref.python\"])\n    assert is_suppressed_warning(\"ref.python\", \"unknown\", [\"ref.python.unknown\"])\n    assert not is_suppressed_warning(\"ref.python\", \"unknown\", [\"ref.cpp\"])\n    assert not is_suppressed_warning(\"ref.python\", \"unknown\", [\"ref.python.another\"])\n    assert is_suppressed_warning(\"ref.python\", \"unknown\", [\"ref.python\", \"ref.cpp\"])\n    assert is_suppressed_warning(\"ref.python\", \"unknown\", [\"ref.python.*\"])\n    assert not is_suppressed_warning(\"ref.python\", \"unknown\", [\"ref.python.*.another\"])\n    assert is_suppressed_warning(\"ref.python\", \"unknown\", [\"ref.python.unknown\", \"ref.cpp\"])\n"], "sample_720": ["compilation error", "def test_handle_zeros_in_scale():\n    # Test _handle_zeros_in_scale function\n    x = np.array([1, 2, 0])\n    expected_output = np.array([1, 2, 1])\n    assert_array_equal(_handle_zeros_in_scale(x), expected_output)\n\n    x = np.array([0, 0, 0])\n    expected_output = np.array([1, 1, 1])\n    assert_array_equal(_handle_zeros_in_scale(x), expected_output)\n", "def test_quantile_transformer_with_sparse_matrix():\n    # Test QuantileTransformer with sparse matrix\n    X_sparse = sparse.csr_matrix(X_2d)\n    n_quantiles = 10\n    qt = QuantileTransformer(n_quantiles=n_quantiles)\n    X_transformed = qt.fit_transform(X_sparse)\n    assert_true(sparse.issparse(X_transformed))\n    assert_equal(X_transformed.shape, X_sparse.shape)\n\n    # Test inverse transform\n    X_inverse = qt.inverse_transform(X_transformed)\n    assert_true(sparse.issparse(X_inverse))\n    assert_array_almost_equal(X_inverse.toarray(), X_2d)\n", "def test_power_transform_errors():\n    X = np.array([[1, 2], [3, 2], [4, 5]])\n\n    # Test error when method is not valid\n    pt = PowerTransformer(method='invalid')\n    with assert_raises_regex(ValueError, \"method' must be one of .*\"):\n        pt.fit(X)\n\n    # Test error when data contains non-positive values for Box-Cox transformation\n    X_nonpositive = np.array([[1, 2], [3, 0], [4, 5]])\n    pt = PowerTransformer(method='box-cox')\n    with assert_raises_regex(ValueError, \"The Box-Cox transformation can only be applied to strictly positive data\"):\n        pt.fit(X_nonpositive)\n\n    # Test error when transforming data with different number of features than fitting data\n    X_extra_feature = np.array([[1, 2, 3], [3, 2, 4], [4, 5, 6]])\n    pt = PowerTransformer(method='box-cox')\n    pt.fit(X)\n    with assert_raises_regex(ValueError, \"Input data has a different number of features than fitting data\"):\n        pt.transform(X_extra_feature)\n", "def test_quantile_transformer_inverse_transform():\n    n_quantiles = 1000\n    n_samples = 100\n    n_features = 2\n    random_state = 0\n\n    X = rng.randn(n_samples, n_features)\n    X_test = rng.randn(n_samples, n_features)\n\n    quantile_transformer = QuantileTransformer(n_quantiles=n_quantiles, random_state=random_state)\n    X_transformed = quantile_transformer.fit_transform(X)\n    X_inverse_transformed = quantile_transformer.inverse_transform(X_transformed)\n\n    # Check that the inverse transform brings the data back to the original scale\n    assert_array_almost_equal(X, X_inverse_transformed, decimal=2)\n\n    # Check that the inverse transform works on unseen data\n    X_test_transformed = quantile_transformer.transform(X_test)\n    X_test_inverse_transformed = quantile_transformer.inverse_transform(X_test_transformed)\n\n    # The inverse transform on unseen data should not exactly match the original data\n    # but it should be close enough\n    assert_array_less(np.abs(X_test - X_test_inverse_transformed), 0.5)\n"], "sample_792": ["def test_gaussian_nb_partial_fit():\n    clf = GaussianNB()\n    clf.partial_fit(X1[:5], y1[:5], np.unique(y1[:5]))\n    assert_array_almost_equal(clf.predict(X1[5:]), y1[5:])\n", "def test_multinomial_nb():\n    clf = MultinomialNB()\n    clf.fit(X2, y2)\n    y_pred = clf.predict(X2)\n    accuracy = np.mean(y_pred == y2)\n    assert_greater(accuracy, 0.5)\n", "def test_gaussian_nb_partial_fit():\n    # Test GaussianNB partial_fit method\n    gnb = GaussianNB()\n    gnb.partial_fit(X1[:5], y1[:5], classes=np.unique(y1))\n    gnb.partial_fit(X1[5:], y1[5:], classes=np.unique(y1))\n    assert_array_almost_equal(gnb.predict(X1), y1)\n", "def test_multinomialnb_fit_predict():\n    clf = MultinomialNB()\n    clf.fit(X2, y2)\n    y_pred = clf.predict(X2)\n    assert_array_equal(y_pred, y2)\n", "def test_gaussian_nb_partial_fit():\n    clf = GaussianNB()\n    clf.partial_fit(X1[:5], y1[:5], np.unique(y1[:5]))\n    assert_array_almost_equal(clf.predict(X1[5:]), y1[5:])\n"], "sample_955": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_869": ["def test_brier_score_loss():\n    y_true = np.array([0, 1, 1, 0])\n    y_prob = np.array([0.1, 0.9, 0.8, 0.3])\n    expected_score = 0.037\n    assert_almost_equal(brier_score_loss(y_true, y_prob), expected_score)\n", "def test_multilabel_confusion_matrix():\n    y_true = np.array([[1, 0, 1], [0, 1, 0]])\n    y_pred = np.array([[1, 0, 0], [0, 1, 1]])\n    sample_weight = [0.5, 1]\n    expected_result = np.array([[[0.5, 0.], [0., 1.]],\n                               [[0., 0.], [1., 0.]],\n                               [[0., 1.], [0., 0.]]])\n    result = multilabel_confusion_matrix(y_true, y_pred, sample_weight=sample_weight)\n    assert_array_equal(result, expected_result)\n", "def test_multilabel_confusion_matrix():\n    # Test multilabel_confusion_matrix with multilabel data\n    y_true = np.array([[1, 0, 1], [0, 1, 0], [1, 1, 0]])\n    y_pred = np.array([[1, 1, 0], [0, 0, 1], [1, 0, 1]])\n    mcm = multilabel_confusion_matrix(y_true, y_pred)\n    expected_mcm = np.array([[[2, 0], [0, 1]],\n                             [[1, 0], [0, 1]],\n                             [[1, 1], [1, 1]]])\n    assert_array_equal(mcm, expected_mcm)\n", "def test_precision_score_binary_pos_label_none():\n    y_true = [0, 1, 0, 1]\n    y_pred = [0, 1, 1, 0]\n    expected_precision = 0.5\n    with ignore_warnings(category=UndefinedMetricWarning):\n        precision = precision_score(y_true, y_pred, pos_label=None, average='binary')\n    assert_almost_equal(precision, expected_precision)\n", "def test_log_loss():\n    y_true = [0, 1, 2, 0]\n    y_pred = [[0.1, 0.8, 0.1], [0.2, 0.6, 0.2], [0.3, 0.4, 0.3], [0.9, 0.05, 0.05]]\n    expected_loss = 1.4656083264270156\n    assert_almost_equal(log_loss(y_true, y_pred), expected_loss)\n\n    y_true = [\"spam\", \"ham\", \"ham\", \"spam\"]\n    y_pred = [[.1, .9], [.9, .1], [.8, .2], [.35, .65]]\n    expected_loss = 0.21616499811813514\n    assert_almost_equal(log_loss(y_true, y_pred), expected_loss)\n"], "sample_1177": ["def test_Abs_derivative():\n    x = Symbol('x')\n    assert Abs(x)._eval_derivative(x) == Piecewise((1, x > 0), (-1, x < 0), (0, True))\n", "def test_sign_evaluation():\n    x = Symbol('x', real=True)\n    assert sign(-1) == -1\n    assert sign(0) == 0\n    assert sign(-3*I) == -I\n    assert sign(x) == sign(x)\n    assert sign(1 + I).evalf() == 0.707106781186548 + 0.707106781186548*I\n", "def test_abs_derivative():\n    x, y = symbols('x y')\n    expr = Abs(x + I*y)\n    derivative = expr._eval_derivative(x)\n    assert derivative == (re(expr) * Derivative(re(expr), x) + im(expr) * Derivative(im(expr), x)) / Abs(expr)\n", "def test_arg():\n    # Test arg function with various input values\n    assert arg(2.0) == 0\n    assert arg(I) == pi/2\n    assert arg(sqrt(2) + I*sqrt(2)) == pi/4\n    assert arg(sqrt(3)/2 + I/2) == pi/6\n    assert arg(4 + 3*I) == atan(3/4)\n    assert N_equals(arg(0.8 + 0.6*I), 0.643501108793284)\n    assert arg(arg(arg(arg(Symbol('x'))))) is S.NaN\n", "def test_sign_derivative():\n    x = Symbol('x', real=True)\n    expr = sign(x)\n    deriv = expr.diff(x)\n    assert isinstance(deriv, DiracDelta)\n    assert deriv.args[1] == x\n"], "sample_965": ["def test_signature_from_ast():\n    code = \"\"\"", "def test_stringify_signature():\n    sig = inspect.signature(datetime.datetime.now)\n    assert stringify_signature(sig) == '()'\n    assert stringify_signature(sig, show_annotation=False) == '()'\n    assert stringify_signature(sig, show_return_annotation=False) == '()'\n    assert stringify_signature(sig, show_annotation=False, show_return_annotation=False) == '()'\n\n    sig = inspect.signature(str)\n    assert stringify_signature(sig) == '(object) -> str'\n    assert stringify_signature(sig, show_annotation=False) == '(object) -> str'\n    assert stringify_signature(sig, show_return_annotation=False) == '(object)'\n    assert stringify_signature(sig, show_annotation=False, show_return_annotation=False) == '(object)'\n\n    sig = inspect.signature(_testcapi.func)\n    assert stringify_signature(sig) == '(i: int, s: str = None, b: bool = True) -> None'\n", "def test_signature_from_str_with_return_annotation():\n    signature = \"() -> int\"\n    sig = inspect.signature_from_str(signature)\n    assert str(sig) == \"(()) -> int\"\n", "def test_stringify_signature_positional_only():\n    sig = inspect.signature_from_str(\"(a, b, /, c=10, *d, e, f=20, **g) -> int\")\n    result = stringify_signature(sig)\n    expected = \"(a, b, /, c=10, *d, e, f=20, **g) -> int\"\n    assert result == expected\n", "def test_isbuiltin(func, expected):\n    assert inspect.isbuiltin(func) == expected\n"], "sample_775": ["def test_EstimatorPrettyPrinter_with_changed_only():\n    set_config(print_changed_only=True)\n    pipeline = Pipeline(steps=[('s', StandardScaler()), ('lr', LogisticRegression())])\n    pipeline.set_params(lr__C=10)\n    printer = _EstimatorPrettyPrinter()\n    output = printer.pformat(pipeline)\n    expected_output = \"Pipeline(steps=[('s', StandardScaler()),\\n               ('lr', LogisticRegression(C=10))])\"\n    assert output == expected_output\n", "def test_simple_pipeline():\n    pipeline = make_pipeline(StandardScaler(), LogisticRegression())\n    pp = _EstimatorPrettyPrinter()\n    output = pp.pformat(pipeline)\n    assert \"Pipeline(steps=[('standardscaler', StandardScaler()), ('logisticregression', LogisticRegression())])\" in output\n", "def test_estimator_pretty_printer():\n    # Test that the custom pprint for estimators is used\n    est = make_pipeline(StandardScaler(), LogisticRegression())\n    printer = _EstimatorPrettyPrinter()\n    est_str = printer.pformat(est)\n    assert est_str == \"Pipeline(steps=[('standardscaler', StandardScaler()), ('logisticregression', LogisticRegression())])\"\n\n    # Test that the compact parameter is respected\n    printer = _EstimatorPrettyPrinter(compact=True)\n    est_str = printer.pformat(est)\n    assert est_str == \"Pipeline(steps=[('standardscaler', StandardScaler()), ('logisticregression', LogisticRegression())])\"\n\n    # Test that only changed parameters are printed when changed_only is True\n    set_config(print_changed_only=True)\n    est = LogisticRegression(C=1.0, penalty='l2', solver='liblinear')\n    est.set_params(C=0.5)\n    est_str = printer.pformat(est)\n    assert est_str == \"LogisticRegression(C=0.5)\"\n\n    set_config(print_changed_only=False)\n", "def test_pprint_changed_params():\n    clf = LogisticRegression(C=1.0, max_iter=100, penalty='l2', solver='lbfgs', random_state=42)\n    clf.set_params(C=2.0, max_iter=200)\n    pp = _EstimatorPrettyPrinter(indent=1, width=80, compact=False, indent_at_name=True, n_max_elements_to_show=10)\n    expected_output = \"LogisticRegression(C=2.0, max_iter=200, random_state=42)\"\n    assert pp.pformat(clf) == expected_output\n", "def test_estimator_pretty_printer_with_complex_objects():\n    # Testing _EstimatorPrettyPrinter with complex objects in a pipeline\n    pipe = make_pipeline(\n        SelectKBest(chi2, k=5),\n        StandardScaler(),\n        LogisticRegression(solver='lbfgs', multi_class='auto', max_iter=5000)\n    )\n\n    grid_search = GridSearchCV(\n        pipe,\n        param_grid={'logisticregression__C': [0.1, 1.0, 10.0]},\n        cv=5\n    )\n\n    pretty_printer = _EstimatorPrettyPrinter(indent=2, n_max_elements_to_show=3)\n    output = pretty_printer.pformat(grid_search)\n\n    # Checking if the output contains the correct representation of the pipeline and grid search\n    assert \"SelectKBest(k=5, score_func=<function chi2 at\" in output\n    assert \"StandardScaler()\" in output\n    assert \"LogisticRegression(C=1.0, max_iter=5000, multi_class='auto', solver='lbfgs')\" in output\n    assert \"GridSearchCV(cv=5, estimator=Pipeline(steps=[('selectkbest', SelectKBest(k=5, score_func=<function chi2 at\" in output\n    assert \"param_grid={'logisticregression__C': [0.1, 1.0, 10.0]}\" in output\n"], "sample_325": ["    def test_as_widget(self):\n        form_data = {'first_name': 'John', 'last_name': 'Doe', 'birthday': '2000-01-01'}\n        form = Person(data=form_data)\n        bound_field = form['first_name']\n        self.assertEqual(bound_field.as_widget(), '<input type=\"text\" name=\"first_name\" value=\"John\" id=\"id_first_name\" required>')\n", "    def test_boundfield_initial(self):\n        form_data = {'first_name': 'John', 'last_name': 'Doe', 'birthday': '1990-01-01'}\n        form = Person(initial=form_data)\n        bound_field = BoundField(form, form.fields['first_name'], 'first_name')\n        self.assertEqual(bound_field.initial, 'John')\n", "    def test_boundfield_label_tag(self):\n        form = Person()\n        field = form.fields['first_name']\n        bf = BoundField(form, field, 'first_name')\n        label_tag = bf.label_tag()\n        self.assertEqual(label_tag, '<label for=\"id_first_name\">First name</label>')\n\n        # Test with contents\n        contents = 'Given name'\n        label_tag = bf.label_tag(contents)\n        self.assertEqual(label_tag, '<label for=\"id_first_name\">Given name</label>')\n\n        # Test with attrs\n        attrs = {'class': 'form-control'}\n        label_tag = bf.label_tag(attrs=attrs)\n        self.assertEqual(label_tag, '<label for=\"id_first_name\" class=\"form-control\">First name</label>')\n\n        # Test with label_suffix\n        form = PersonNew()\n        field = form.fields['first_name']\n        bf = BoundField(form, field, 'first_name')\n        label_tag = bf.label_tag(label_suffix='*')\n        self.assertEqual(label_tag, '<label for=\"first_name_id\" class=\"required\">First name*</label>')\n", "    def test_bound_field_as_text(self):\n        form = PersonNew({'first_name': 'John', 'last_name': 'Doe', 'birthday': '1990-01-01'})\n        bound_field = form['first_name']\n        self.assertEqual(bound_field.as_text(), '<input type=\"text\" name=\"first_name\" value=\"John\" id=\"first_name_id\" required>')\n", "    def test_bound_field_widget_attrs(self):\n        form = Person()\n        bound_field = form[\"first_name\"]\n        widget_attrs = bound_field.build_widget_attrs({})\n        self.assertNotIn('required', widget_attrs)\n\n        form = Person(data={'first_name': 'John'})\n        bound_field = form[\"first_name\"]\n        widget_attrs = bound_field.build_widget_attrs({})\n        self.assertIn('required', widget_attrs)\n        self.assertTrue(widget_attrs['required'])\n\n        form.fields['first_name'].required = False\n        bound_field = form[\"first_name\"]\n        widget_attrs = bound_field.build_widget_attrs({})\n        self.assertNotIn('required', widget_attrs)\n\n        form = PersonNew()\n        bound_field = form[\"first_name\"]\n        widget_attrs = bound_field.build_widget_attrs({})\n        self.assertNotIn('required', widget_attrs)\n\n        form = PersonNew(data={'first_name': 'John'})\n        bound_field = form[\"first_name\"]\n        widget_attrs = bound_field.build_widget_attrs({})\n        self.assertIn('required', widget_attrs)\n        self.assertTrue(widget_attrs['required'])\n"], "sample_205": ["    def test_validation_error_message_dict(self):\n        validation_error = ValidationError({'field1': ['Error 1', 'Error 2'], 'field2': ['Error 3']})\n        expected_dict = {'field1': ['Error 1', 'Error 2'], 'field2': ['Error 3']}\n        self.assertEqual(validation_error.message_dict, expected_dict)\n", "def test_validation_error_message_dict(self):\n    error = ValidationError({'field1': ['error1'], 'field2': ['error2']})\n    self.assertEqual(error.message_dict, {'field1': ['error1'], 'field2': ['error2']})\n", "def test_validation_error_initialization(self):\n    # Test with a single error message\n    error = ValidationError(\"This is a single error message\")\n    self.assertEqual(error.message, \"This is a single error message\")\n    self.assertIsNone(error.code)\n    self.assertIsNone(error.params)\n\n    # Test with a single error message and code\n    error = ValidationError(\"This is a single error message\", code=\"invalid\")\n    self.assertEqual(error.message, \"This is a single error message\")\n    self.assertEqual(error.code, \"invalid\")\n    self.assertIsNone(error.params)\n\n    # Test with a single error message, code, and params\n    error = ValidationError(\"This is a %(value)s error message\", code=\"invalid\", params={\"value\": \"single\"})\n    self.assertEqual(error.message, \"This is a single error message\")\n    self.assertEqual(error.code, \"invalid\")\n    self.assertEqual(error.params, {\"value\": \"single\"})\n\n    # Test with a list of error messages\n    error = ValidationError([\"Error 1\", \"Error 2\"])\n    self.assertEqual(error.error_list[0].message, \"Error 1\")\n    self.assertEqual(error.error_list[1].message, \"Error 2\")\n\n    # Test with a dictionary of field errors\n    error = ValidationError({\"field1\": [\"Error 1\"], \"field2\": [\"Error 2\", \"Error 3\"]})\n    self.assertEqual(error.error_dict[\"field1\"][0].message, \"Error 1\")\n    self.assertEqual(error.error_dict[\"field2\"][0].message, \"Error 2\")\n    self.assertEqual(error.error_dict[\"field2\"][1].message, \"Error 3\")\n", "def test_validation_error_init(self):\n    # Test initialization of ValidationError with a simple string message\n    err = ValidationError(\"This is a test error\")\n    self.assertEqual(err.message, \"This is a test error\")\n    self.assertIsNone(err.code)\n    self.assertIsNone(err.params)\n\n    # Test initialization of ValidationError with a string message and a code\n    err = ValidationError(\"This is a test error\", code=\"test_code\")\n    self.assertEqual(err.message, \"This is a test error\")\n    self.assertEqual(err.code, \"test_code\")\n    self.assertIsNone(err.params)\n\n    # Test initialization of ValidationError with a string message, code, and params\n    err = ValidationError(\"This is a test error with params: %(value)s\", code=\"test_code\", params={'value': 'test'})\n    self.assertEqual(err.message, \"This is a test error with params: test\")\n    self.assertEqual(err.code, \"test_code\")\n    self.assertEqual(err.params, {'value': 'test'})\n", "    def test_error_list_initialization(self):\n        error = ValidationError('Error message')\n        self.assertEqual(error.message, 'Error message')\n        self.assertEqual(len(error.error_list), 1)\n        self.assertEqual(error.error_list[0].message, 'Error message')\n"], "sample_85": ["def test_on_delete_set_default(self):\n    r = R.objects.create(name=\"Referrer\")\n    a_default = A.objects.create(r=self.DEFAULT)\n    a = A.objects.create(r=r)\n    r.delete()\n    a.refresh_from_db()\n    self.assertEqual(a.r, self.DEFAULT)\n", "def test_set_attributes_from_rel(self):\n    m2m_field = M2MFrom._meta.get_field('m2m_to')\n    m2m_field.set_attributes_from_rel()\n    self.assertEqual(m2m_field.name, 'm2m_to_id')\n    self.assertEqual(m2m_field.verbose_name, 'M2M to')\n    self.assertEqual(m2m_field.remote_field.field_name, 'id')\n", "def test_on_delete_set_null(self):\n    r = R.objects.create(name=\"test\")\n    a = A.objects.create(r=r)\n    r.delete()\n    a.refresh_from_db()\n    self.assertIsNone(a.r)\n", "    def test_contribute_to_class_creates_descriptor(self):\n        class MyModel(models.Model):\n            m2m_field = models.ManyToManyField(M2MTo)\n\n        self.assertTrue(hasattr(MyModel, 'm2m_field'))\n        self.assertIsInstance(MyModel.m2m_field, models.fields.related_descriptors.ManyToManyDescriptor)\n", "def test_set_null_on_delete(self):\n    # Create a referrer with an associated referree\n    referrer = Referrer.objects.create(referree=self.DEFAULT)\n\n    # Set the referree to None and save\n    referrer.referree = None\n    referrer.save()\n\n    # Retrieve the referrer from the database and check if referree is None\n    updated_referrer = Referrer.objects.get(pk=referrer.pk)\n    self.assertIsNone(updated_referrer.referree)\n"], "sample_634": ["def test_expand_modules_with_ignore_list_re(checker):\n    \"\"\"Test expand_modules function with ignore_list_re parameter\"\"\"\n    files_or_modules = [\"module1.py\", \"module2.py\", \"module3.py\"]\n    ignore_list = []\n    ignore_list_re = [re.compile(r\"module2\\.py\")]\n    ignore_list_paths_re = []\n\n    result, errors = expand_modules(\n        files_or_modules,\n        ignore_list,\n        ignore_list_re,\n        ignore_list_paths_re,\n    )\n\n    # module2.py should be ignored\n    assert \"module2.py\" not in [module[\"path\"] for module in result]\n", "def test_expand_modules_ignores_files_in_ignore_list_re(self):\n    ignore_list_re = [re.compile(r'test_.*\\.py')]\n    result, errors = expand_modules(['test_file.py'], [], ignore_list_re, [])\n    assert result == []\n", "def test_is_in_ignore_list_re(filename, expected_result):\n    ignore_list_re = [re.compile(r\"ignore_this\\.py$\")]\n    result = _is_in_ignore_list_re(filename, ignore_list_re)\n    assert result == expected_result\n", "def test_expand_modules_with_ignore_list(self):\n    files_or_modules = [\"test_file1.py\", \"test_file2.py\", \"test_file3.py\"]\n    ignore_list = [\"test_file2.py\"]\n    ignore_list_re = [re.compile(r\"test_file\\d.py\")]\n    ignore_list_paths_re = []\n\n    result, errors = expand_modules(\n        files_or_modules, ignore_list, ignore_list_re, ignore_list_paths_re\n    )\n\n    assert len(result) == 1\n    assert result[0][\"name\"] == \"test_file1\"\n    assert len(errors) == 0\n", "    def test_expand_modules_ignore_list(self):\n        files_or_modules = ['ignore_this_module', 'keep_this_module']\n        ignore_list = ['ignore_this_module']\n        ignore_list_re = []\n        ignore_list_paths_re = []\n        result, errors = expand_modules(files_or_modules, ignore_list, ignore_list_re, ignore_list_paths_re)\n        assert len(result) == 1\n        assert result[0]['name'] == 'keep_this_module'\n"], "sample_909": ["    def test_attributes_section(self):\n        docstring = NamedtupleSubclass.__doc__\n        config = Config()\n        doc = GoogleDocstring(docstring, config, what='class', name='NamedtupleSubclass', obj=NamedtupleSubclass)\n        attributes_section = doc._parse_attributes_section('Attributes')\n        expected_output = [\n            '.. attribute:: attr1',\n            '   :noindex:',\n            '',\n            '   Quick description of attr1',\n            '',\n            '.. attribute:: attr2',\n            '   :noindex:',\n            '',\n            '   Quick description of attr2',\n            '',\n            '.. attribute:: attr3',\n            '   :noindex:',\n            '',\n            '   Adds a newline after the type',\n            '',\n            ''\n        ]\n        self.assertEqual(attributes_section, expected_output)\n", "    def test_new_method(self):\n        docstring = GoogleDocstring(NamedtupleSubclass.__new__.__doc__, Config())\n        lines = docstring.lines()\n        self.assertIn(\":param cls:\", ' '.join(lines))\n        self.assertIn(\":param attr1:\", ' '.join(lines))\n        self.assertIn(\":param attr2:\", ' '.join(lines))\n        self.assertIn(\":returns:\", ' '.join(lines))\n", "    def test_namedtuple_subclass_init(self):\n        docstring = GoogleDocstring(NamedtupleSubclass.__init__.__doc__, Config())\n        parsed_docstring = '\\n'.join(docstring.lines())\n        expected_docstring = dedent(\"\"\"\n        Sample namedtuple subclass\n\n        :param attr1: Arbitrary type\n            Quick description of attr1\n        :param attr2: Another arbitrary type, optional\n            Quick description of attr2\n        :type attr2: Another arbitrary type, optional\n        :param attr3: Type\n\n            Adds a newline after the type\n        :type attr3: Type\n        \"\"\")\n        self.assertEqual(cleandoc(parsed_docstring), cleandoc(expected_docstring))\n", "    def test_docstring_parsing(self):\n        docstring = NamedtupleSubclass.__doc__\n        config = Config(napoleon_use_param=True, napoleon_use_rtype=True)\n        parsed_docstring = GoogleDocstring(docstring, config)\n        self.assertIn(':param attr1: Quick description of attr1', parsed_docstring.lines())\n        self.assertIn(':type attr1: Arbitrary type', parsed_docstring.lines())\n        self.assertIn(':param attr2: Quick description of attr2', parsed_docstring.lines())\n        self.assertIn(':type attr2: Another arbitrary type', parsed_docstring.lines())\n        self.assertIn(':param attr3: Adds a newline after the type', parsed_docstring.lines())\n        self.assertIn(':type attr3: Type', parsed_docstring.lines())\n", "    def test_attribute_ivar(self, mock_config):\n        mock_config.napoleon_use_ivar = True\n        docstring = GoogleDocstring(dedent(NamedtupleSubclass.__doc__),\n                                    config=mock_config,\n                                    what='attribute', name='attr1',\n                                    obj=NamedtupleSubclass)\n        expected_output = dedent(\"\"\"\n            Quick description of attr1\n\n            :type: Arbitrary type\n        \"\"\")\n        self.assertEqual(str(docstring), cleandoc(expected_output))\n"], "sample_612": ["def test_consolidate_slices():\n    # Test with no slices\n    assert _consolidate_slices([]) == []\n\n    # Test with consolidable slices\n    assert _consolidate_slices([slice(0, 5), slice(5, 10)]) == [slice(0, 10)]\n\n    # Test with non-consolidable slices\n    assert _consolidate_slices([slice(0, 5), slice(10, 15)]) == [slice(0, 5), slice(10, 15)]\n\n    # Test with step argument\n    assert _consolidate_slices([slice(0, 5, 2), slice(2, 10, 2)]) == [slice(0, 10, 2)]\n\n    # Test with non-slice object\n    with pytest.raises(ValueError):\n        _consolidate_slices([slice(0, 5), 10])\n", "def test_dataset_groupby_consolidate_slices(dataset):\n    slices = [slice(0, 2), slice(2, 5, 2)]\n    consolidated = _consolidate_slices(slices)\n    expected = [slice(0, 2), slice(2, 5, 2)]\n    assert consolidated == expected\n", "def test_consolidate_slices():\n    slices = [slice(0, 5), slice(5, 10), slice(10, 15, 2)]\n    result = _consolidate_slices(slices)\n    expected = [slice(0, 10), slice(10, 15, 2)]\n    assert result == expected\n\n    slices = [slice(0, 5), slice(6, 10), slice(10, 15, 2)]\n    result = _consolidate_slices(slices)\n    expected = [slice(0, 5), slice(6, 10), slice(10, 15, 2)]\n    assert result == expected\n", "def test_groupby_consolidate_slices():\n    # Test consolidating adjacent slices\n    slices = [slice(0, 5), slice(5, 10), slice(10, 15, 2), slice(17, 20)]\n    consolidated = _consolidate_slices(slices)\n    expected = [slice(0, 10), slice(10, 15, 2), slice(17, 20)]\n    assert consolidated == expected\n", "def test_consolidate_slices():\n    # Test case where slices are adjacent and can be consolidated\n    slices = [slice(0, 10, 1), slice(10, 20, 1)]\n    expected_output = [slice(0, 20, 1)]\n    assert _consolidate_slices(slices) == expected_output\n\n    # Test case where slices are not adjacent and cannot be consolidated\n    slices = [slice(0, 10, 1), slice(20, 30, 1)]\n    expected_output = [slice(0, 10, 1), slice(20, 30, 1)]\n    assert _consolidate_slices(slices) == expected_output\n\n    # Test case where slices have step size greater than 1 and can be consolidated\n    slices = [slice(0, 10, 2), slice(10, 20, 2)]\n    expected_output = [slice(0, 20, 2)]\n    assert _consolidate_slices(slices) == expected_output\n\n    # Test case where input is not a list of slices\n    slices = [slice(0, 10, 1), \"invalid_slice\"]\n    with pytest.raises(ValueError):\n        _consolidate_slices(slices)\n"], "sample_399": ["def test_aggregate_with_filter(self):\n    qs = Book.objects.filter(publisher=self.p1)\n    result = qs.aggregate(average_price=Avg('price'), total_pages=Sum('pages'))\n    self.assertEqual(result['average_price'], 31.23)\n    self.assertEqual(result['total_pages'], 797)\n", "    def test_age_distribution(self):\n        result = Author.objects.annotate(\n            young=Case(When(age__lt=30, then=1), default=0),\n            middle=Case(When(age__gte=30, age__lt=40, then=1), default=0),\n            old=Case(When(age__gte=40, then=1), default=0)\n        ).aggregate(young=Sum('young'), middle=Sum('middle'), old=Sum('old'))\n        self.assertEqual(result, {'young': 3, 'middle': 3, 'old': 4})\n", "def test_aggregate_with_annotation(self):\n    result = Book.objects.annotate(num_authors=Count('authors')).aggregate(avg_authors=Avg('num_authors'))\n    self.assertEqual(result['avg_authors'], 2.6)\n", "    def test_aggregate_with_subquery(self):\n        subquery = Book.objects.filter(id=OuterRef('id')).values('price').annotate(max_price=Max('price')).values('max_price')\n        result = Book.objects.annotate(max_price_subquery=subquery).aggregate(avg_max_price=Avg('max_price_subquery'))\n        self.assertEqual(result['avg_max_price'], 46.45)\n", "    def test_count_without_joins(self):\n        with self.assertNumQueries(1):\n            self.assertEqual(Book.objects.count(), 6)\n"], "sample_91": ["def test_page_not_found(self):\n    request = self.request_factory.get('/nonexistent_url/')\n    exception = Http404()\n    response = page_not_found(request, exception)\n    self.assertEqual(response.status_code, 404)\n    self.assertContains(response, 'Not Found')\n    self.assertContains(response, 'The requested resource was not found on this server.')\n", "    def test_bad_request(self):\n        request = self.request_factory.get('/')\n        exception = Exception('Bad request exception')\n        response = bad_request(request, exception)\n        self.assertEqual(response.status_code, 400)\n        self.assertContains(response, 'Bad Request (400)')\n", "def test_page_not_found(self):\n    request = self.request_factory.get('/nonexistent_url/')\n    response = page_not_found(request, Http404())\n    self.assertEqual(response.status_code, 404)\n    self.assertIn(b'Not Found', response.content)\n    self.assertIn(b'The requested resource was not found on this server.', response.content)\n", "    def test_page_not_found(self):\n        request = self.request_factory.get('/nonexistent_url/')\n        response = page_not_found(request, Http404())\n        self.assertEqual(response.status_code, 404)\n        self.assertIn(b'Not Found', response.content)\n", "def test_bad_request_handler(self):\n    \"\"\"Test the bad_request handler.\"\"\"\n    request = self.request_factory.get('/')\n    exception = Exception('Bad Request')\n    response = bad_request(request, exception)\n    self.assertEqual(response.status_code, 400)\n    self.assertIn(b'Bad Request (400)', response.content)\n\n    # Test with custom template\n    with self.settings(ERROR_400_TEMPLATE_NAME='custom_400.html'):\n        with self.assertRaises(TemplateDoesNotExist):\n            bad_request(request, exception, template_name='custom_400.html')\n"], "sample_1104": ["def test_print_Tr():\n    A = Matrix([[1, 2], [3, 4]])\n    assert sstr(Tr(A)) == \"Tr(Matrix([[1, 2], [3, 4]]))\"\n", "def test_print_Tr():\n    A = Matrix([[1, 2], [3, 4]])\n    assert sstr(Tr(A)) == \"Tr(Matrix([[1, 2], [3, 4]]))\"\n", "def test_print_Tr():\n    A = Matrix([[1, 2], [3, 4]])\n    assert sstr(Tr(A)) == \"Tr(Matrix([[1, 2], [3, 4]]))\"\n", "def test_print_Tr():\n    M = Matrix([[1, 2], [3, 4]])\n    assert sstr(Tr(M)) == \"Tr(Matrix([[1, 2], [3, 4]]))\"\n", "def test_StrPrinter_Derivative():\n    f = Function('f')\n    expr = Derivative(f(x), x)\n    assert sstr(expr) == 'Derivative(f(x), x)'\n"], "sample_293": ["def test_is_callback(self):\n    urlconf = URLResolver(RegexPattern(r'^/'), 'urlpatterns_reverse.urlconf')\n    self.assertTrue(urlconf._is_callback('urlpatterns_reverse.views.empty_view'))\n    self.assertFalse(urlconf._is_callback('non_existent_view'))\n", "    def test_no_url_patterns(self):\n        with self.assertRaises(ImproperlyConfigured):\n            get_resolver()\n", "    def test_populate(self):\n        resolver = URLResolver(RegexPattern(r'^/'), 'urlpatterns_reverse.urls')\n        resolver._populate()\n        self.assertTrue(resolver._populated)\n", "    def test_resolve_error_handler(self):\n        resolver = get_resolver('urlpatterns_reverse.urls')\n        handler400 = resolver.resolve_error_handler(400)\n        self.assertEqual(handler400, middleware.handler400)\n", "    def test_include_with_url_name(self):\n        resolver = get_resolver('urlpatterns_reverse.urls_include_with_name')\n        match = resolver.resolve('/included_namespace_urls/normal/42/37/')\n        self.assertEqual(match.view_name, 'included_namespace_urls:inc-normal-view')\n"], "sample_56": ["    def test_check_relation(self):\n        class InvalidInline(admin.TabularInline):\n            model = Song\n            fk_name = 'invalid_fk'\n\n        site = AdminSite()\n        inline = InvalidInline(parent_model=Album, admin_site=site)\n        errors = admin.checks.InlineModelAdminChecks()._check_relation(inline, Album)\n        self.assertEqual(len(errors), 1)\n        self.assertEqual(errors[0].id, 'admin.E202')\n", "    def test_check_raw_id_fields_valid(self):\n        ma = MyAdmin(Song, AdminSite())\n        ma.raw_id_fields = ['album']\n        self.assertEqual(ma.check(), [])\n", "    def test_check_autocomplete_fields(self):\n        site = AdminSite()\n        modeladmin = ValidFields(Album, site)\n        modeladmin.autocomplete_fields = ('artist',)\n        self.assertEqual(modeladmin.check(), [])\n\n        modeladmin.autocomplete_fields = 'artist'\n        self.assertEqual(modeladmin.check(), [checks.Error(\"The value of 'autocomplete_fields' must be a list or tuple.\", obj=ValidFields, id='admin.E036')])\n\n        modeladmin.autocomplete_fields = ('non_existent_field',)\n        self.assertEqual(modeladmin.check(), [checks.Error(\"The value of 'autocomplete_fields[0]' refers to 'non_existent_field', which is not an attribute of 'admin_checks.album'.\", obj=ValidFields, id='admin.E037')])\n\n        modeladmin.autocomplete_fields = ('title',)\n        self.assertEqual(modeladmin.check(), [checks.Error(\"The value of 'autocomplete_fields[0]' must be a foreign key or a many-to-many field.\", obj=ValidFields, id='admin.E038')])\n\n        modeladmin.autocomplete_fields = ('artist',)\n        site.register(Author)\n        self.assertEqual(modeladmin.check(), [checks.Error(\"AuthorModelAdmin must define 'search_fields', because it's referenced by ValidFields.autocomplete_fields.\", obj=ValidFields, id='admin.E040')])\n", "    def test_check_exclude_of_parent_model(self):\n        site = AdminSite()\n        parent_model = Author\n        inline_model = Book\n        fk = 'author'\n\n        class TestInline(admin.TabularInline):\n            model = inline_model\n            fk_name = fk\n            exclude = [fk]\n\n        inline = TestInline(parent_model, site)\n        errors = InlineModelAdminChecks()._check_exclude_of_parent_model(inline, parent_model)\n        self.assertEqual(len(errors), 1)\n        self.assertEqual(errors[0].id, 'admin.E201')\n", "    def delete_selected(self, request, queryset):\n        queryset.delete()\n"], "sample_260": ["def test_add_index_and_remove_index_optimization(self):\n    operations = [\n        operations.AddIndex('UnicodeModel', models.Index(fields=['title'], name='title_idx')),\n        operations.RemoveIndex('UnicodeModel', 'title_idx'),\n    ]\n    expected = []\n    self.assertOptimizesTo(operations, expected, exact=1)\n", "def test_alter_model_managers(self):\n    operations = [\n        operations.CreateModel('UnicodeModel', [\n            ('name', models.CharField(max_length=20)),\n        ], managers=[('objects', models.Manager())]),\n        operations.AlterModelManagers('UnicodeModel', [('custom_manager', EmptyManager())]),\n    ]\n    expected = [\n        operations.CreateModel('UnicodeModel', [\n            ('name', models.CharField(max_length=20)),\n        ], managers=[('custom_manager', EmptyManager())]),\n    ]\n    self.assertOptimizesTo(operations, expected, exact=1)\n", "def test_create_model_with_options(self):\n    operations = [\n        operations.CreateModel(\n            name='UnicodeModel',\n            fields=[\n                ('char_field', models.CharField(max_length=255)),\n                ('text_field', models.TextField()),\n            ],\n            options={\n                'verbose_name': 'Verbose Name',\n                'verbose_name_plural': 'Verbose Name Plurals',\n                'ordering': ['char_field'],\n                'unique_together': {('char_field', 'text_field')},\n                'indexes': [\n                    migrations.Index(fields=['char_field'], name='char_field_idx'),\n                ],\n                'constraints': [\n                    migrations.CheckConstraint(check=models.Q(char_field__startswith='prefix_'), name='char_field_prefix_check'),\n                ],\n            },\n        ),\n    ]\n    expected = operations\n    self.assertOptimizesTo(operations, expected, exact=1)\n", "def test_optimize_remove_index(self):\n    operations = [\n        migrations.CreateModel(\n            name='TestModel',\n            fields=[\n                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                ('name', models.CharField(max_length=50)),\n            ],\n        ),\n        migrations.AddIndex(\n            model_name='TestModel',\n            index=models.Index(fields=['name'], name='name_index'),\n        ),\n        migrations.RemoveIndex(\n            model_name='TestModel',\n            name='name_index',\n        ),\n    ]\n    expected = [\n        migrations.CreateModel(\n            name='TestModel',\n            fields=[\n                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                ('name', models.CharField(max_length=50)),\n            ],\n        ),\n    ]\n    self.assertOptimizesTo(operations, expected)\n", "def test_optimize_model_operations(self):\n    # Test that CreateModel and DeleteModel operations cancel each other out\n    operations = [\n        operations.CreateModel('TestModel', [('name', models.CharField(max_length=100))]),\n        operations.DeleteModel('TestModel'),\n    ]\n    expected = []\n    self.assertOptimizesTo(operations, expected, exact=1)\n\n    # Test that RenameModel operations are correctly combined\n    operations = [\n        operations.CreateModel('OldModel', [('name', models.CharField(max_length=100))]),\n        operations.RenameModel('OldModel', 'NewModel'),\n    ]\n    expected = [\n        operations.CreateModel('NewModel', [('name', models.CharField(max_length=100))]),\n    ]\n    self.assertOptimizesTo(operations, expected, exact=1)\n\n    # Test that AlterModelOptions operations are correctly combined\n    operations = [\n        operations.CreateModel('TestModel', [('name', models.CharField(max_length=100))]),\n        operations.AlterModelOptions('TestModel', {'verbose_name': 'New Verbose Name'}),\n    ]\n    expected = [\n        operations.CreateModel('TestModel', [('name', models.CharField(max_length=100))], options={'verbose_name': 'New Verbose Name'}),\n    ]\n    self.assertOptimizesTo(operations, expected, exact=1)\n"], "sample_889": ["def test_calibrated_classifier_cv_decision_function_binary(data, method, ensemble):\n    X, y = data\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\n    class DecisionFunctionClassifier(BaseEstimator):\n            self.estimator = estimator\n\n            self.estimator.fit(X, y, **fit_params)\n            return self\n\n            return self.estimator.predict_proba(X)\n\n            return self.estimator.decision_function(X)\n\n    base_clf = DecisionFunctionClassifier(LogisticRegression(random_state=42))\n    calibrated_clf = CalibratedClassifierCV(base_clf, cv=3, method=method, ensemble=ensemble)\n    calibrated_clf.fit(X_train, y_train)\n\n    assert hasattr(calibrated_clf, \"calibrated_classifiers_\")\n    assert all(hasattr(clf, \"estimator\") for clf in calibrated_clf.calibrated_classifiers_)\n    assert all(hasattr(clf, \"calibrators\") for clf in calibrated_clf.calibrated_classifiers_)\n    assert all(len(clf.calibrators) == 1 for clf in calibrated_clf.calibrated_classifiers_)\n", "def test_calibrated_classifier_cv_sample_weight_invariance(data):\n    X, y = data\n    estimator = LogisticRegression(solver=\"lbfgs\", multi_class=\"multinomial\", random_state=42)\n    calibrator = CalibratedClassifierCV(estimator, cv=5, method=\"sigmoid\")\n\n    # Fit the calibrator with all samples\n    calibrator.fit(X, y)\n    proba_all = calibrator.predict_proba(X)\n\n    # Fit the calibrator with all but one sample\n    calibrator.fit(X[:-1], y[:-1])\n    proba_subset = calibrator.predict_proba(X[:-1])\n\n    # Check that the probabilities for the samples in the subset are the same\n    assert_array_almost_equal(proba_all[:-1], proba_subset, decimal=6)\n", "def test_calibrated_classifier_cv_sample_weights_invariance(data):\n    X, y = data\n    estimator = LinearSVC()\n    calibrated_clf = CalibratedClassifierCV(estimator=estimator, cv=3)\n    calibrated_clf.fit(X, y)\n    proba_original = calibrated_clf.predict_proba(X)\n\n    X_modified = np.concatenate([X, X[:2]])\n    y_modified = np.concatenate([y, y[:2]])\n    sample_weight = np.ones(y_modified.shape[0])\n    sample_weight[-2:] = 0\n    calibrated_clf_modified = CalibratedClassifierCV(estimator=estimator, cv=3)\n    calibrated_clf_modified.fit(X_modified, y_modified, sample_weight=sample_weight)\n    proba_modified = calibrated_clf_modified.predict_proba(X)\n\n    assert_array_almost_equal(proba_original, proba_modified)\n", "def test_calibrated_classifier_cv_with_sample_weight(data, method, ensemble):\n    X, y = data\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n    sample_weight = np.random.rand(len(y_train))\n\n    estimator = LogisticRegression(solver=\"liblinear\", random_state=42)\n    calibrated_clf = CalibratedClassifierCV(\n        estimator, method=method, cv=3, ensemble=ensemble\n    )\n\n    calibrated_clf.fit(X_train, y_train, sample_weight=sample_weight)\n    proba = calibrated_clf.predict_proba(X_test)\n\n    assert proba.shape == (len(y_test), 2)\n    assert np.all(proba >= 0) and np.all(proba <= 1)\n", "def test_calibration_with_custom_estimator(method, ensemble, data):\n    X, y = data\n    class CustomEstimator(BaseEstimator):\n            self.classes_ = np.unique(y)\n            return self\n            return np.random.rand(len(X), len(self.classes_))\n    estimator = CustomEstimator()\n    calibrator = CalibratedClassifierCV(estimator=estimator, method=method, ensemble=ensemble, cv=2)\n    calibrator.fit(X, y)\n    assert hasattr(calibrator, \"calibrated_classifiers_\")\n    assert len(calibrator.calibrated_classifiers_) == (2 if ensemble else 1)\n    proba = calibrator.predict_proba(X)\n    assert proba.shape == (N_SAMPLES, len(np.unique(y)))\n"], "sample_1175": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_389": ["    def setUp(self):\n        self.request = HttpRequest()\n        self.request.META = {\n            'SERVER_NAME': 'testserver',\n            'SERVER_PORT': '80',\n            'HTTP_HOST': 'testserver',\n        }\n", "def test_http_request_get_host(self):\n    request = HttpRequest()\n    request.META = {\"HTTP_HOST\": \"example.com\"}\n\n    # Test with ALLOWED_HOSTS empty and DEBUG=True\n    with self.settings(ALLOWED_HOSTS=[], DEBUG=True):\n        self.assertEqual(request.get_host(), \"example.com\")\n\n    # Test with ALLOWED_HOSTS set and DEBUG=False\n    with self.settings(ALLOWED_HOSTS=[\"example.com\"], DEBUG=False):\n        self.assertEqual(request.get_host(), \"example.com\")\n\n    # Test with ALLOWED_HOSTS set and DEBUG=True\n    with self.settings(ALLOWED_HOSTS=[\"example.com\"], DEBUG=True):\n        self.assertEqual(request.get_host(), \"example.com\")\n\n    # Test with disallowed host\n    with self.settings(ALLOWED_HOSTS=[\"another-example.com\"], DEBUG=False):\n        with self.assertRaises(DisallowedHost):\n            request.get_host()\n\n    # Test with invalid host\n    request.META = {\"HTTP_HOST\": \"invalid_host\"}\n    with self.assertRaises(DisallowedHost):\n        request.get_host()\n", "    def test_body_within_size_limit(self):\n        request = HttpRequest()\n        request.META['CONTENT_LENGTH'] = '400000'\n        request._stream = BytesIO(b'a' * 400000)\n        self.assertEqual(len(request.body), 400000)\n", "    def setUp(self):\n        self.request = HttpRequest()\n", "    def setUp(self):\n        self.request = HttpRequest()\n"], "sample_336": ["compilation error", "    def test_resolve_error_handler(self):\n        resolver = URLResolver(RegexPattern(r'^/'), 'urlpatterns_reverse.urlconf_outer')\n        self.assertEqual(resolver.resolve_error_handler(404), middleware.error404)\n        self.assertEqual(resolver.resolve_error_handler(500), middleware.error500)\n        with self.assertRaises(ViewDoesNotExist):\n            resolver.resolve_error_handler(403)\n", "    def test_no_url_patterns(self):\n        with self.assertRaises(ImproperlyConfigured):\n            get_resolver()\n", "compilation error", "    def test_resolver_match_repr(self):\n        match = ResolverMatch(views.empty_view, ('arg1', 'arg2'), {'kwarg': 'value'}, 'view-name', ['app'], ['ns'], 'route')\n        expected_repr = \"ResolverMatch(func=urlpatterns_reverse.views.empty_view, args=('arg1', 'arg2'), kwargs={'kwarg': 'value'}, url_name='view-name', app_names=['app'], namespaces=['ns'], route='route')\"\n        self.assertEqual(repr(match), expected_repr)\n"], "sample_276": ["    def test_get_return_data_type(self):\n        self.assertEqual(get_return_data_type('get_list'), 'List')\n        self.assertEqual(get_return_data_type('get_count'), 'Integer')\n        self.assertEqual(get_return_data_type('some_method'), '')\n", "def test_get_return_data_type(self):\n    self.assertEqual(get_return_data_type('get_list'), 'List')\n    self.assertEqual(get_return_data_type('get_count'), 'Integer')\n    self.assertEqual(get_return_data_type('some_method'), '')\n", "def test_get_return_data_type(self):\n    self.assertEqual(get_return_data_type('get_list'), 'List')\n    self.assertEqual(get_return_data_type('get_count'), 'Integer')\n    self.assertEqual(get_return_data_type('get_'), '')\n    self.assertEqual(get_return_data_type('other_func'), '')\n", "    def test_simple_regex(self):\n        pattern = \"/^(?P<sport_slug>\\\\w+)/athletes/(?P<athlete_slug>\\\\w+)/$/\"\n        simplified = simplify_regex(pattern)\n        self.assertEqual(simplified, \"/<sport_slug>/athletes/<athlete_slug>/\")\n", "    def test_simplify_regex(self):\n        test_cases = [\n            ('^(?P<sport_slug>\\\\w+)/athletes/(?P<athlete_slug>\\\\w+)/$', '/<sport_slug>/athletes/<athlete_slug>/'),\n            ('^articles/(?P<year>[0-9]{4})/$', '/articles/<year>/'),\n            ('^(?P<page_slug>\\\\w+)/$', '/<page_slug>/'),\n        ]\n        for pattern, expected in test_cases:\n            with self.subTest(pattern=pattern):\n                self.assertEqual(simplify_regex(pattern), expected)\n"], "sample_757": ["def test_ordinal_encoder_inverse_transform_output_shape():\n    enc = OrdinalEncoder()\n    X = [['Male', 1], ['Female', 3], ['Female', 2]]\n    enc.fit(X)\n    X_trans = enc.transform(X)\n    X_inv_trans = enc.inverse_transform(X_trans)\n    assert X_inv_trans.shape == np.array(X).shape\n", "def test_ordinal_encoder_inverse_transform():\n    enc = OrdinalEncoder()\n    X = [['Male', 1], ['Female', 3], ['Female', 2]]\n    enc.fit(X)\n    X_trans = enc.transform(X)\n    X_inv_trans = enc.inverse_transform(X_trans)\n    assert_array_equal(X_inv_trans, X)\n", "def test_ordinal_encoder_handle_unknown(handle_unknown):\n    enc = OrdinalEncoder(handle_unknown=handle_unknown)\n    X = [['Male', 1], ['Female', 3], ['Female', 2]]\n    enc.fit(X)\n    if handle_unknown == 'ignore':\n        X_test = [['Male', 4], ['Unknown', 2]]\n        X_transformed = enc.transform(X_test)\n        assert_array_equal(X_transformed, [[1, 2], [np.nan, 1]])\n    else:  # handle_unknown == 'error'\n        X_test = [['Male', 4], ['Unknown', 2]]\n        with pytest.raises(ValueError, match=\"Found unknown categories\"):\n            enc.transform(X_test)\n", "def test_ordinal_encoder_transform_inverse_transform():\n    enc = OrdinalEncoder()\n    X = [['Male', 1], ['Female', 3], ['Female', 2]]\n    enc.fit(X)\n    X_t = enc.transform(X)\n    X_inv = enc.inverse_transform(X_t)\n    assert_array_equal(X, X_inv)\n", "def test_ordinal_encoder_inverse_transform():\n    enc = OrdinalEncoder()\n    X = [['Male', 1], ['Female', 3], ['Female', 2]]\n    enc.fit(X)\n    X_transformed = enc.transform(X)\n    X_inverse = enc.inverse_transform(X_transformed)\n    assert_array_equal(X, X_inverse)\n"], "sample_1166": ["def test_monomial_ldiv():\n    assert monomial_ldiv((3, 4, 1), (1, 2, 0)) == (2, 2, 1)\n    assert monomial_ldiv((3, 4, 1), (1, 2, 2)) == (2, 2, -1)\n", "def test_monomial_operations():\n    # Test Monomial multiplication\n    assert Monomial((2, 3, 4)) * Monomial((1, 2, 3)) == Monomial((3, 5, 7))\n\n    # Test Monomial division\n    assert Monomial((3, 5, 7)) / Monomial((1, 2, 3)) == Monomial((2, 3, 4))\n\n    # Test Monomial power\n    assert Monomial((2, 3, 4)) ** 2 == Monomial((4, 6, 8))\n\n    # Test Monomial gcd\n    assert Monomial((2, 4, 6)).gcd(Monomial((3, 6, 9))) == Monomial((2, 4, 6))\n\n    # Test Monomial lcm\n    assert Monomial((2, 4, 6)).lcm(Monomial((3, 6, 9))) == Monomial((3, 6, 9))\n\n    # Test Monomial division with ExactQuotientFailed\n    with raises(ExactQuotientFailed):\n        Monomial((2, 3, 4)) / Monomial((5, 6, 7))\n", "def test_monomial_ldiv():\n    assert monomial_ldiv((3, 4, 1), (1, 2, 0)) == (2, 2, 1)\n    assert monomial_ldiv((3, 4, 1), (1, 2, 2)) == (2, 2, -1)\n", "def test_monomial_ldiv():\n    assert monomial_ldiv((3, 4, 1), (1, 2, 0)) == (2, 2, 1)\n    assert monomial_ldiv((3, 4, 1), (1, 2, 2)) == (2, 2, -1)\n", "def test_monomial_class_as_expr():\n    gens = symbols('x, y, z')\n    m = Monomial((2, 3, 1), gens=gens)\n    assert m.as_expr() == x**2 * y**3 * z\n"], "sample_128": ["    def test_index_creation_and_deletion(self):\n        index = Index(fields=['title', 'author'])\n        index.set_name_with_model(Article)\n        create_sql = index.create_sql(Article, connection.schema_editor())\n        remove_sql = index.remove_sql(Article, connection.schema_editor())\n\n        self.assertIn('CREATE INDEX', create_sql)\n        self.assertIn('DROP INDEX', remove_sql)\n        self.assertIn(index.name, create_sql)\n        self.assertIn(index.name, remove_sql)\n", "    def test_create_index_with_opclasses(self):\n        class TestModel(models.Model):\n            field1 = models.CharField(max_length=255)\n            field2 = models.CharField(max_length=255)\n\n        index = Index(fields=['field1', 'field2'], opclasses=['varchar_pattern_ops', 'text_pattern_ops'], name='test_index')\n        sql = index.create_sql(TestModel, connection.schema_editor())\n        self.assertIn('CREATE INDEX test_index ON testmodel USING btree (field1 varchar_pattern_ops, field2 text_pattern_ops)', sql)\n", "    def test_index_creation_and_deletion(self):\n        index = Index(fields=['title'], name='test_idx')\n        sql = index.create_sql(Article, connection.schema_editor())\n        self.assertIn('CREATE INDEX \"test_idx\" ON \"app_article\" (\"title\")', sql)\n        sql = index.remove_sql(Article, connection.schema_editor())\n        self.assertIn('DROP INDEX \"test_idx\"', sql)\n", "    def test_index_creation_and_deletion(self):\n        index = Index(fields=['title'], name='test_index')\n        sql_create = index.create_sql(Article, connection.schema_editor(), using='')\n        sql_delete = index.remove_sql(Article, connection.schema_editor())\n\n        self.assertIsNotNone(sql_create)\n        self.assertIsNotNone(sql_delete)\n        self.assertIn('CREATE INDEX test_index ON', sql_create)\n        self.assertIn('DROP INDEX test_index', sql_delete)\n", "    def test_index_operations(self):\n        # Test index creation with fields, name, and db_tablespace\n        index = Index(fields=['title'], name='title_idx', db_tablespace='test_space')\n        self.assertEqual(index.fields, ['title'])\n        self.assertEqual(index.name, 'title_idx')\n        self.assertEqual(index.db_tablespace, 'test_space')\n\n        # Test index creation with opclasses and condition\n        index = Index(fields=['name'], opclasses=['varchar_pattern_ops'], condition=Q(age__gt=18))\n        self.assertEqual(index.opclasses, ['varchar_pattern_ops'])\n        self.assertEqual(str(index.condition), \"(AND: ('age__gt', 18))\")\n\n        # Test index creation with include\n        index = Index(fields=['city'], include=['state', 'zip_code'])\n        self.assertEqual(index.include, ('state', 'zip_code'))\n\n        # Test index creation without fields\n        with self.assertRaises(ValueError):\n            Index(fields=[])\n\n        # Test index creation without name for opclasses or condition\n        with self.assertRaises(ValueError):\n            Index(fields=['name'], opclasses=['varchar_pattern_ops'])\n        with self.assertRaises(ValueError):\n            Index(fields=['name'], condition=Q(age__gt=18))\n\n        # Test index creation without name for covering index\n        with self.assertRaises(ValueError):\n            Index(fields=['city'], include=['state'])\n\n        # Test index creation with invalid condition\n        with self.assertRaises(ValueError):\n            Index(fields=['name'], condition=\"invalid_condition\")\n\n        # Test index creation with invalid include\n        with self.assertRaises(ValueError):\n            Index(fields=['city'], include=\"invalid_include\")\n\n        # Test index creation with invalid fields\n        with self.assertRaises(ValueError):\n            Index(fields=\"invalid_fields\")\n\n        # Test index creation with invalid opclasses\n        with self.assertRaises(ValueError):\n"], "sample_803": ["def test_label_ranking_loss(drop):\n    \"\"\"Test label_ranking_loss function.\"\"\"\n    y_true = np.array([[1, 0, 0], [0, 0, 1]])\n    y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]])\n    expected_loss = 0.5833333333333333\n    loss = label_ranking_loss(y_true, y_score)\n    assert_almost_equal(loss, expected_loss)\n", "def test_auc_with_drop_intermediate(drop):\n    y_true = np.array([0, 0, 1, 1])\n    y_score = np.array([0.1, 0.4, 0.35, 0.8])\n    fpr, tpr, _ = roc_curve(y_true, y_score, drop_intermediate=drop)\n    expected_fpr = np.array([0., 0., 0.5, 0.5, 1.])\n    expected_tpr = np.array([0., 0.5, 0.5, 1., 1.])\n    if drop:\n        expected_fpr = expected_fpr[[0, 2, 4]]\n        expected_tpr = expected_tpr[[0, 2, 4]]\n    assert_array_equal(fpr, expected_fpr)\n    assert_array_equal(tpr, expected_tpr)\n", "def test_label_ranking_loss():\n    y_true = np.array([[1, 0, 0], [0, 0, 1]])\n    y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]])\n    expected_loss = 0.6\n    assert_almost_equal(label_ranking_loss(y_true, y_score), expected_loss)\n", "def test_average_precision_score_ties(drop):\n    y_true = np.array([1, 0, 0, 1])\n    y_score = np.array([0.5, 0.5, 0.5, 0.5])\n\n    # When all predictions are the same, average precision should be 0.5\n    expected_ap = 0.5\n    ap = average_precision_score(y_true, y_score)\n    assert_almost_equal(ap, expected_ap)\n", "def test_roc_curve_drop_intermediate(drop):\n    y_true = np.array([0, 0, 1, 1])\n    y_score = np.array([0.1, 0.4, 0.35, 0.8])\n    fpr, tpr, thresholds = roc_curve(y_true, y_score, drop_intermediate=drop)\n    if drop:\n        assert_array_equal(fpr, np.array([0., 0.5, 0.5, 1.]))\n        assert_array_equal(tpr, np.array([0., 0.5, 1., 1.]))\n        assert_array_equal(thresholds, np.array([0.8, 0.4, 0.35, 0.1]))\n    else:\n        assert_array_equal(fpr, np.array([0., 0., 0.5, 0.5, 1.]))\n        assert_array_equal(tpr, np.array([0., 0.5, 0.5, 1., 1.]))\n        assert_array_equal(thresholds, np.array([1.8, 0.8, 0.4, 0.35, 0.1]))\n"], "sample_28": ["def test_card_fromstring_with_bytes(self):\n    card = fits.Card.fromstring(b\"KEYWORD   = 'Test Value'\")\n    assert card.keyword == 'KEYWORD'\n    assert card.value == 'Test Value'\n", "def test_card_value_deletion(self):\n    card = fits.Card(\"KEYWORD\", \"VALUE\")\n    del card.value\n    assert card.value == \"\", \"Deleting the value of a Card should set it to an empty string.\"\n", "def test_verify_unparsable_card(self):\n    card = fits.Card.fromstring(\"UNPARSABLE = invalid\")\n    with pytest.raises(VerifyError, match=\"Unparsable card\"):\n        card.verify(\"fix\")\n", "def test_format_long_image(self):\n    card = fits.Card(\"LONGKEY\", \"A\" * 68, \"B\" * 65)\n    expected = \"LONGKEY  = 'A\" * 67 + \"'&'\\n\" + \"CONTINUE  'A\" * 2 + \"' / \" + \"B\" * 64\n    assert card._format_long_image() == expected\n", "def test_card_value_modification(self):\n    card = fits.Card(\"TESTKEY\", 123, \"Test comment\")\n    assert card.value == 123\n    assert card.comment == \"Test comment\"\n    assert not card._modified\n\n    card.value = 456\n    assert card.value == 456\n    assert card._modified\n    assert card._valuemodified\n\n    card._modified = False\n    card._valuemodified = False\n    card.comment = \"New comment\"\n    assert card.comment == \"New comment\"\n    assert card._modified\n    assert not card._valuemodified\n"], "sample_883": ["def test_ard_regression_convergence():\n    X, y = diabetes.data, diabetes.target\n    clf = ARDRegression(max_iter=1000, tol=1e-6)\n    clf.fit(X, y)\n    assert clf.n_iter_ <= 1000, \"ARDRegression did not converge within the maximum number of iterations\"\n", "def test_bayesian_ridge_predict_with_std():\n    X = diabetes.data\n    y = diabetes.target\n    br = BayesianRidge()\n    br.fit(X, y)\n    y_mean, y_std = br.predict(X, return_std=True)\n    assert y_mean.shape == (len(y),)\n    assert y_std.shape == (len(y),)\n    assert_array_less(y_std, np.abs(y_mean))  # std should be less than the absolute mean\n", "def test_bayesian_ridge_compute_score():\n    X = diabetes.data[:10]\n    y = diabetes.target[:10]\n    br = BayesianRidge(compute_score=True)\n    br.fit(X, y)\n    assert len(br.scores_) == br.n_iter_\n", "def test_bayesian_ridge_fit_intercept():\n    X, y = diabetes.data, diabetes.target\n    X_offset = X.mean(axis=0)\n    X_centered = X - X_offset\n    y_offset = y.mean()\n    y_centered = y - y_offset\n\n    br_with_intercept = BayesianRidge(fit_intercept=True, max_iter=100)\n    br_with_intercept.fit(X, y)\n\n    br_no_intercept = BayesianRidge(fit_intercept=False, max_iter=100)\n    br_no_intercept.fit(X_centered, y_centered)\n\n    assert_array_almost_equal(br_with_intercept.coef_, br_no_intercept.coef_)\n    assert_almost_equal(br_with_intercept.intercept_, y_offset - np.dot(X_offset, br_with_intercept.coef_))\n", "def test_intercept_consistency(BayesRegression):\n    X = diabetes.data\n    y = diabetes.target\n\n    br = BayesRegression(fit_intercept=True)\n    br.fit(X, y)\n    intercept_with_intercept = br.intercept_\n\n    br_no_intercept = BayesRegression(fit_intercept=False)\n    br_no_intercept.fit(X - X.mean(axis=0), y - y.mean())\n    intercept_no_intercept = br_no_intercept.intercept_ + y.mean() - np.dot(br_no_intercept.coef_, X.mean(axis=0))\n\n    assert_almost_equal(intercept_with_intercept, intercept_no_intercept)\n"], "sample_511": ["def test_margins():\n    fig, ax = plt.subplots()\n    ax.margins(x=0.2, y=0.3)\n    assert ax.margins() == (0.2, 0.3)\n", "def test_autoscale():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3], [4, 5, 6])\n    ax.autoscale(enable=True)\n    assert ax.get_xlim() == (1, 3)\n    assert ax.get_ylim() == (4, 6)\n", "def test_plot_with_custom_colors():\n    x = np.linspace(0, 2 * np.pi, 400)\n    y1 = np.sin(x)\n    y2 = np.cos(x)\n\n    fig, ax = plt.subplots()\n    ax.plot(x, y1, color='red')\n    ax.plot(x, y2, color='blue')\n\n    plt.close(fig)\n", "def test_new_unit_test():\n    # Test case for the next unit test\n    fig, ax = plt.subplots()\n    x = np.linspace(0, 10, 100)\n    y = np.sin(x)\n    ax.plot(x, y)\n    plt.close(fig)\n", "def test_plot_date():\n    dates = np.array('2005-02-25', '2005-03-01', '2005-03-02', dtype=np.datetime64)\n    y = np.arange(len(dates))\n    plt.plot_date(dates, y, '-')\n    plt.show()\n"], "sample_676": ["def test_plugin_nameversions(input, expected):\n    assert _plugin_nameversions(input) == expected\n", "def test_plugin_nameversions(input, expected):\n    result = _plugin_nameversions(input)\n    assert result == expected\n", "def test_plugin_nameversions(input, expected):\n    assert _plugin_nameversions(zip(range(len(input)), input)) == expected\n", "def test_plugin_nameversions(input, expected):\n    assert _plugin_nameversions(input) == expected\n", "def test_plugin_nameversions(input, expected):\n    assert _plugin_nameversions(input) == expected\n"], "sample_152": ["def test_collect_restricted_objects(self):\n    # Test the case where an object cannot be deleted due to RESTRICT\n    r = R.objects.create()\n    MR.objects.create(r=r)\n    collector = Collector(using=DEFAULT_DB_ALIAS)\n    with self.assertRaises(RestrictedError) as context:\n        collector.collect([r])\n    self.assertIn(r, context.exception.restricted_objects)\n", "    def test_collect_with_restricted_objects(self):\n        origin = Origin.objects.create()\n        referrer = Referrer.objects.create(origin=origin)\n        self.collector = Collector(using='default')\n        self.collector.collect([origin])\n        self.assertIn(referrer, self.collector.restricted_objects[Referrer.objects.model][Referrer.origin.field])\n        with self.assertRaises(RestrictedError):\n            self.collector.delete()\n", "    def test_collect_restricted_objects(self):\n        a = A.objects.create()\n        r = R.objects.create(a=a)\n        r.delete()\n        with self.assertRaises(RestrictedError) as context:\n            a.delete()\n        self.assertIn(r, context.exception.restricted_objects)\n", "    def test_clear_restricted_objects_from_set(self):\n        collector = Collector(using='default')\n        m = M.objects.create()\n        r = R.objects.create(m=m)\n        collector.restricted_objects[M.objects.model] = {M.m_set: {r}}\n        collector.clear_restricted_objects_from_set(M.objects.model, {m})\n        self.assertEqual(collector.restricted_objects[M.objects.model], {})\n", "def test_collect_restricted_objects(self):\n    # Create objects for testing RESTRICT\n    a = A.objects.create(r=self.DEFAULT)\n    b1 = B1.objects.create(a=a)\n    b2 = B2.objects.create(b=b1)\n\n    # Collect objects for deletion\n    collector = Collector(using='default')\n    collector.collect([a])\n\n    # Check if restricted objects are collected\n    self.assertIn(B1, collector.restricted_objects)\n    self.assertIn(B2, collector.restricted_objects)\n    self.assertIn(B1.objects.get(pk=b1.pk), collector.restricted_objects[B1][B1.b1_set.field])\n    self.assertIn(B2.objects.get(pk=b2.pk), collector.restricted_objects[B2][B2.b2_set.field])\n\n    # Try to delete, should raise RestrictedError\n    with self.assertRaises(RestrictedError):\n        collector.delete()\n"], "sample_247": ["    def test_annotation_filtering(self):\n        # Test filtering on annotations.\n        filtered_books = Book.objects.annotate(num_authors=Count('authors')).filter(num_authors=3)\n        self.assertSequenceEqual(filtered_books, [self.b4])\n", "def test_nonaggregate_annotation(self):\n    # Test annotating with a non-aggregate function\n    annotated_books = Book.objects.annotate(\n        name_length=Length('name'),\n        year_published=ExtractYear('pubdate'),\n        price_as_float=Cast('price', FloatField()),\n    )\n    book = annotated_books.get(name='The Definitive Guide to Django: Web Development Done Right')\n    self.assertEqual(book.name_length, 57)\n    self.assertEqual(book.year_published, 2007)\n    self.assertIsInstance(book.price_as_float, float)\n    self.assertEqual(book.price_as_float, 30.0)\n", "    def test_annotation(self):\n        # Test annotations with aggregates\n        queryset = Book.objects.annotate(\n            num_authors=Count('authors'),\n            price_plus_tax=ExpressionWrapper(F('price') * 1.05, output_field=FloatField()),\n            discounted_price=Case(\n                When(price__gt=50, then=ExpressionWrapper(F('price') * 0.9, output_field=FloatField())),\n                default=F('price')\n            )\n        )\n\n        annotated_book = queryset.get(name='The Definitive Guide to Django: Web Development Done Right')\n        self.assertEqual(annotated_book.num_authors, 2)\n        self.assertEqual(annotated_book.price_plus_tax, 31.53)\n        self.assertEqual(annotated_book.discounted_price, 30.00)\n", "    def test_annotation(self):\n        qs = Author.objects.annotate(num_books=Count('book'))\n        self.assertEqual(qs[0].num_books, 2)\n        self.assertEqual(qs[1].num_books, 1)\n        self.assertEqual(qs[2].num_books, 1)\n        self.assertEqual(qs[3].num_books, 2)\n        self.assertEqual(qs[4].num_books, 1)\n        self.assertEqual(qs[5].num_books, 1)\n        self.assertEqual(qs[6].num_books, 2)\n        self.assertEqual(qs[7].num_books, 3)\n        self.assertEqual(qs[8].num_books, 1)\n", "    def setUpTestData(cls):\n        # ... (existing setup)\n"], "sample_715": ["def test_cross_validate_multi_metric():\n    \"\"\"Test cross_validate with multiple metrics\"\"\"\n    estimator = MockClassifier()\n    scoring = {'score1': make_scorer(accuracy_score),\n               'score2': make_scorer(precision_score, average='macro')}\n    cv_results = cross_validate(estimator, X, y, scoring=scoring,\n                                return_train_score=True)\n    assert_equal(set(cv_results.keys()),\n                 {'fit_time', 'score_time', 'test_score1', 'test_score2',\n                  'train_score1', 'train_score2'})\n    assert_equal(cv_results['test_score1'].shape, (3,))\n    assert_equal(cv_results['test_score2'].shape, (3,))\n    assert_equal(cv_results['train_score1'].shape, (3,))\n    assert_equal(cv_results['train_score2'].shape, (3,))\n", "def test_cross_validate_return_train_score_warning():\n    # Test that a warning is raised when returning train scores\n    # and the return_train_score parameter is 'warn'\n    estimator = MockClassifier()\n    with assert_warns(FutureWarning, match=\"You are accessing a training score\"):\n        cv_results = cross_validate(estimator, X, y, return_train_score='warn')\n        cv_results['train_score']\n", "def test_cross_validate_with_multiple_metrics():\n    clf = SVC(random_state=42)\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n\n    scorers = {'accuracy': make_scorer(accuracy_score),\n               'precision': make_scorer(precision_score, average='macro')}\n    cv_results = cross_validate(clf, X, y, cv=5, scoring=scorers,\n                                return_train_score=True)\n\n    assert_true('fit_time' in cv_results)\n    assert_true('score_time' in cv_results)\n    assert_true('test_accuracy' in cv_results)\n    assert_true('test_precision' in cv_results)\n    assert_true('train_accuracy' in cv_results)\n    assert_true('train_precision' in cv_results)\n", "def test_cross_validate_with_multiple_metrics():\n    estimator = MockClassifier()\n    scoring = {'accuracy': make_scorer(accuracy_score), 'precision': make_scorer(precision_score, average='macro')}\n    cv_results = cross_validate(estimator=estimator, X=X, y=y, scoring=scoring, return_train_score=True)\n    assert_equal(len(cv_results), 6)  # check the number of keys in the result dictionary\n    assert_equal(len(cv_results['test_accuracy']), 5)  # default cv is 5 folds\n    assert_equal(len(cv_results['train_accuracy']), 5)\n    assert_equal(len(cv_results['test_precision']), 5)\n    assert_equal(len(cv_results['train_precision']), 5)\n    assert_equal(len(cv_results['fit_time']), 5)\n    assert_equal(len(cv_results['score_time']), 5)\n", "def test_cross_val_score_with_fit_params():\n    # Test cross_val_score with fit_params\n    clf = MockClassifier()\n    cv = KFold(n_splits=2)\n    sample_weight = np.ones(X.shape[0])\n    class_prior = np.ones(len(np.unique(y)))\n    sparse_sample_weight = csr_matrix(sample_weight)\n    sparse_param = P_sparse\n    dummy_int = 1\n    dummy_str = 'test'\n    dummy_obj = object()\n    fit_params = {'sample_weight': sample_weight,\n                  'class_prior': class_prior,\n                  'sparse_sample_weight': sparse_sample_weight,\n                  'sparse_param': sparse_param,\n                  'dummy_int': dummy_int,\n                  'dummy_str': dummy_str,\n                  'dummy_obj': dummy_obj}\n    scores = cross_val_score(clf, X, y, cv=cv, fit_params=fit_params)\n    assert_equal(scores.shape, (2,))\n"], "sample_544": ["def test_pcolorimage_cursor_data():\n    fig, ax = plt.subplots()\n    x = np.array([0, 1, 2])\n    y = np.array([0, 1, 2])\n    A = np.array([[1, 2], [3, 4]])\n    img = PcolorImage(ax, x=x, y=y, A=A)\n\n    # Test cursor data inside the image\n    event = type(\"Event\", (), {\"xdata\": 0.5, \"ydata\": 0.5})\n    assert img.get_cursor_data(event) == 1\n\n    # Test cursor data outside the image\n    event = type(\"Event\", (), {\"xdata\": 3, \"ydata\": 3})\n    assert img.get_cursor_data(event) is None\n", "def test_cmap():\n    data = np.random.random((10, 10))\n    fig, ax = plt.subplots()\n    im = ax.imshow(data, cmap='viridis')\n    assert im.get_cmap().name == 'viridis'\n", "def test_image_interpolation_none():\n    fig, ax = plt.subplots()\n    data = np.random.rand(10, 10)\n    im = ax.imshow(data, interpolation='none')\n    assert im.get_interpolation() == 'none'\n", "def test_image_resample(fig_test, fig_ref):\n    x = np.linspace(0, 10, 100)\n    y = np.sin(x)\n    data = np.tile(y[:, np.newaxis], (1, 100))\n    fig_test.subplots_adjust(wspace=0.4, hspace=0.4)\n    for i, interp in enumerate(['nearest', 'bilinear', 'bicubic', 'sinc']):\n        ax = fig_test.add_subplot(2, 2, i + 1)\n        ax.imshow(data, interpolation=interp)\n        ax.set_title(f'Interpolation: {interp}')\n    fig_ref.savefig('test_image_resample_ref.png')\n", "def test_interpolation_stages():\n    fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(12, 12))\n    x = np.linspace(-5, 5, 256)\n    y = np.linspace(-5, 5, 256)\n    X, Y = np.meshgrid(x, y)\n    Z = np.sqrt(X**2 + Y**2)\n\n    stages = ['data', 'rgba']\n    for i, ax in enumerate(axes.flat):\n        interpolation = 'bicubic'\n        if i < 3:\n            interpolation_stage = stages[i // 3]\n        else:\n            interpolation_stage = 'rgba'\n            interpolation = stages[(i - 3) // 3]\n\n        im = ax.imshow(Z, extent=[-5, 5, -5, 5], interpolation=interpolation,\n                       interpolation_stage=interpolation_stage)\n        ax.set_title(f'interpolation={interpolation}, interpolation_stage={interpolation_stage}')\n        ax.axis('off')\n\n    plt.tight_layout()\n"], "sample_545": ["def test_figure_subplots_adjust():\n    fig = Figure()\n    fig.subplots_adjust(left=0.1, bottom=0.1, right=0.9, top=0.9, wspace=0.2, hspace=0.2)\n    assert fig.subplotpars.left == 0.1\n    assert fig.subplotpars.bottom == 0.1\n    assert fig.subplotpars.right == 0.9\n    assert fig.subplotpars.top == 0.9\n    assert fig.subplotpars.wspace == 0.2\n    assert fig.subplotpars.hspace == 0.2\n", "def test_figure_align_labels_yaxis():\n    fig, axs = plt.subplots(2, 1, sharex=True)\n    axs[0].plot(np.arange(0, 1000, 50))\n    axs[0].set_ylabel('YLabel 0')\n    axs[1].set_ylabel('YLabel 1')\n    fig.align_ylabels()\n    plt.close(fig)\n", "def test_figure_align_labels_with_artists():\n    fig, axs = plt.subplots(2, 2)\n    for tick in axs[0, 0].get_xticklabels():\n        tick.set_rotation(55)\n    axs[0, 0].set_xlabel('XLabel 0')\n    axs[1, 1].set_xlabel('XLabel 1')\n    axs[0, 0].set_ylabel('YLabel 0')\n    axs[1, 1].set_ylabel('YLabel 1')\n    fig.align_labels()\n", "def test_figure_align_labels_with_datetime_xaxis():\n    fig, axs = plt.subplots(2, 1, figsize=(8, 6), sharex=True)\n\n    dates = mdates.date2num([datetime(2022, i, 1) for i in range(1, 13)])\n    y = np.random.rand(12)\n\n    axs[0].plot(dates, y)\n    axs[0].set_xlabel('Date')\n    axs[0].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n    axs[0].xaxis.set_tick_params(rotation=45)\n\n    axs[1].plot(dates, y)\n    axs[1].set_xlabel('Date')\n    axs[1].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n    axs[1].xaxis.set_tick_params(rotation=45)\n\n    fig.align_labels()\n    plt.tight_layout()\n", "def test_figaspect():\n    # Test figaspect with a float\n    w, h = figaspect(2.)\n    assert np.isclose(w / h, 2.)\n    assert 4. <= w <= 16.\n    assert 2. <= h <= 16.\n\n    # Test figaspect with an array\n    A = np.random.rand(5, 3)\n    w, h = figaspect(A)\n    assert np.isclose(w / h, A.shape[0] / A.shape[1])\n    assert 4. <= w <= 16.\n    assert 2. <= h <= 16.\n"], "sample_640": ["def test_is_builtin_object(node, expected):\n    assert utils.is_builtin_object(node) == expected\n", "compilation error", "compilation error", "def test_is_builtin_object_with_builtin_module(monkeypatch):\n    builtins_module = astroid.parse(\"\"\"from math import sqrt\"\"\").locals[\"sqrt\"][0]\n    assert utils.is_builtin_object(builtins_module)\n\n    non_builtin_module = astroid.parse(\"\"\"import os\"\"\").locals[\"os\"][0]\n    assert not utils.is_builtin_object(non_builtin_module)\n", "compilation error"], "sample_698": ["def test_colored_level_formatter_format():\n    terminalwriter = TerminalWriter(sys.stdout)\n    formatter = ColoredLevelFormatter(terminalwriter, \"%(levelname)s: %(message)s\")\n\n    # Create a mock log record\n    record = logging.LogRecord(\"test_logger\", logging.WARNING, \"test_module.py\", 42, \"Test warning message\", None, None)\n\n    # Format the log record\n    formatted_message = formatter.format(record)\n\n    # Assert that the formatted message contains the colorized level name\n    assert \"\\x1b[33mWARNING\\x1b[0m: Test warning message\" in formatted_message\n", "def test_colored_level_formatter():\n    terminal_writer = TerminalWriter(sys.stdout)\n    formatter = ColoredLevelFormatter(terminal_writer, '%(levelname)s: %(message)s')\n\n    record = logging.LogRecord('test_logger', logging.WARNING, 'test_logger.py', 42, 'Test warning message', None, None)\n    formatted_message = formatter.format(record)\n\n    # The formatted message should include the ANSI escape sequences for yellow color\n    assert '\\x1b[33m' in formatted_message\n    assert '\\x1b[0m' in formatted_message\n\n    # The formatted message should include the original log message\n    assert 'WARNING' in formatted_message\n    assert 'Test warning message' in formatted_message\n", "def test_colored_level_formatter():\n    class MockTerminalWriter(TerminalWriter):\n            return text\n\n    formatter = ColoredLevelFormatter(MockTerminalWriter(), \"%(levelname)s: %(message)s\")\n    record = logging.LogRecord(\"test_logger\", logging.INFO, \"test_module\", 123, \"Test message\", (), None)\n    formatted_message = formatter.format(record)\n    assert \"INFO\" in formatted_message\n    assert \"Test message\" in formatted_message\n", "def test_colored_level_formatter_format():\n    writer = TerminalWriter()\n    formatter = ColoredLevelFormatter(writer, \"%(levelname)s: %(message)s\")\n\n    record = logging.LogRecord(\n        name=\"test_logger\",\n        level=logging.WARNING,\n        pathname=\"/test/pathname\",\n        lineno=123,\n        msg=\"Test message\",\n        args=(),\n        exc_info=None,\n    )\n\n    output = formatter.format(record)\n    assert \"WARNING\" in output\n    assert \"Test message\" in output\n    assert writer.markup(\"WARNING\", yellow=True) in output\n", "def test_colored_level_formatter():\n    terminalwriter = TerminalWriter()\n    formatter = ColoredLevelFormatter(terminalwriter, \"%(levelname)s %(message)s\")\n    record = logging.LogRecord(\"test_logger\", logging.INFO, \"pathname\", 0, \"test message\", None, None)\n    formatted_message = formatter.format(record)\n    # Add assertions to validate the formatted message based on the expected output\n"], "sample_423": ["def test_altered_db_table_options(self):\n    changes = self.get_changes(\n        [self.author_with_db_table_options],\n        [self.author_with_new_db_table_options],\n    )\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterModelTable\"])\n    self.assertOperationAttributes(\n        changes, \"testapp\", 0, 0, name=\"Author\", table=\"author_two\"\n    )\n\n    changes = self.get_changes(\n        [self.author_with_db_table_options],\n        [self.author_renamed_with_db_table_options],\n    )\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"RenameModel\", \"AlterModelTable\"])\n    self.assertOperationAttributes(\n        changes, \"testapp\", 0, 0, old_name=\"Author\", new_name=\"NewAuthor\"\n    )\n    self.assertOperationAttributes(\n        changes, \"testapp\", 0, 1, name=\"NewAuthor\", table=\"author_one\"\n    )\n\n    changes = self.get_changes(\n        [self.author_with_db_table_options],\n        [self.author_renamed_with_new_db_table_options],\n    )\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"RenameModel\", \"AlterModelTable\"])\n    self.assertOperationAttributes(\n        changes, \"testapp\", 0, 0, old_name=\"Author\", new_name=\"NewAuthor\"\n    )\n    self.assertOperationAttributes(\n        changes, \"testapp\", 0, 1, name=\"NewAuthor\", table=\"author_three\"\n    )\n", "def test_author_renamed_with_db_table_options(self):\n    changes = self.get_changes(\n        [self.author_with_db_table_options],\n        [self.author_renamed_with_db_table_options],\n    )\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"RenameModel\", \"AlterModelTable\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"Author\", new_name=\"NewAuthor\")\n    self.assertOperationAttributes(changes, \"testapp\", 0, 1, name=\"NewAuthor\", table=\"author_one\")\n", "def test_removed_altered_unique_together(self):\n    changes = self.get_changes(\n        [self.book_unique_together],\n        [self.book_unique_together_2],\n    )\n    self.assertOperationTypes(changes, \"otherapp\", 0, [\"AlterUniqueTogether\"])\n    self.assertOperationAttributes(\n        changes,\n        \"otherapp\",\n        0,\n        0,\n        unique_together={(\"title\", \"author\")},\n    )\n", "    def test_altered_indexes_with_renamed_index_together(self):\n        changes = self.get_changes(\n            [\n                self.author_indexes,\n                self.book_with_multiple_authors_through_attribution,\n                self.attribution,\n            ],\n            [\n                self.author_indexes,\n                self.book_indexes,\n                self.attribution,\n            ],\n        )\n        self.assertNumberMigrations(changes, \"otherapp\", 1)\n        self.assertOperationTypes(changes, \"otherapp\", 0, [\"RemoveIndex\", \"AddIndex\"])\n        self.assertEqual(\n            changes[\"otherapp\"][0].operations[0].name,\n            \"book_title_author_idx\",\n        )\n        self.assertEqual(\n            changes[\"otherapp\"][0].operations[1].name,\n            \"book_author_title_idx\",\n        )\n", "    def test_create_operations_for_m2m_through_model_with_renamed_parent_models(self):\n        changes = self.get_changes(\n            [self.author_with_m2m_through, self.publisher, self.contract],\n            [self.author_with_renamed_m2m_through, self.publisher, self.contract_renamed],\n        )\n        self.assertEqual(changes, {\n            'testapp': [\n                migrations.Migration('auto_1', 'testapp', dependencies=[]),\n            ],\n        })\n        migrations = changes['testapp']\n        self.assertEqual(len(migrations[0].operations), 2)\n        self.assertIsInstance(migrations[0].operations[0], migrations.RemoveField)\n        self.assertIsInstance(migrations[0].operations[1], migrations.AddField)\n        self.assertEqual(migrations[0].operations[0].name, 'publishers')\n        self.assertEqual(migrations[0].operations[1].name, 'publishers')\n"], "sample_911": ["compilation error", "compilation error", "compilation error", "def test_cpp_namespace_object():\n    # Test the parsing of a namespace object\n    parser = DefinitionParser(\"::ns1::ns2\", location=None, config=None)\n    ast = parser.parse_namespace_object()\n    assert ast.objectType == \"namespace\"\n    assert str(ast.nestedName) == \"::ns1::ns2\"\n    assert ast.templatePrefix is None\n", "compilation error"], "sample_1169": ["def test_PermutationOperator():\n    p, q = symbols('p q')\n    f = Function('f')\n    assert PermutationOperator(p, q).get_permuted(f(p, q)) == -f(q, p)\n", "def test_contraction():\n    p, q = symbols('p,q')\n    a, b = symbols('a,b', above_fermi=True)\n    i, j = symbols('i,j', below_fermi=True)\n\n    assert contraction(F(a), Fd(b)) == KroneckerDelta(a, b)\n    assert contraction(Fd(i), F(j)) == KroneckerDelta(i, j)\n    assert contraction(Fd(p), F(q)) == KroneckerDelta(Dummy('i', below_fermi=True), q)*KroneckerDelta(p, q)\n    assert contraction(F(p), Fd(q)) == KroneckerDelta(Dummy('a', above_fermi=True), q)*KroneckerDelta(p, q)\n    assert contraction(F(p), F(q)) == 0\n    assert contraction(Fd(p), Fd(q)) == 0\n\n    with raises(ContractionAppliesOnlyToFermions):\n        contraction(Bd(p), F(q))\n", "def test_create_boson_apply_operator():\n    state = FockStateBosonKet([1, 2, 3])\n    op = CreateBoson(1)\n    result = op.apply_operator(state)\n    expected = sqrt(2) * FockStateBosonKet([1, 3, 3])\n    assert result == expected\n", "def test_antisymmetric_tensor_sorting():\n    i, j = symbols('i j', below_fermi=True)\n    a, b = symbols('a b', above_fermi=True)\n    assert str(AntiSymmetricTensor('v', (a, i), (b, j))) == 'v^ab_ij'\n    assert str(AntiSymmetricTensor('v', (i, a), (b, j))) == '-v^ab_ij'\n", "def test_bosonic_operators():\n    a = Symbol('a')\n    b = Bd(a)\n    assert b.state == a\n    assert b.is_symbolic is True\n    assert b.apply_operator(S.One) == b\n    assert b.apply_operator(BKet([1])) == sqrt(2)*BKet([2])\n\n    c = CreateBoson(a)\n    assert c.state == a\n    assert c.is_symbolic is True\n    assert c.apply_operator(S.One) == c\n    assert c.apply_operator(BKet([1])) == sqrt(2)*BKet([0])\n"], "sample_660": ["def test_add_global_property(self):\n    xml = LogXML(\"logfile.xml\", \"prefix\")\n    xml.add_global_property(\"key1\", \"value1\")\n    xml.add_global_property(\"key2\", \"value2\")\n    global_properties_node = xml._get_global_properties_node()\n    properties = global_properties_node.find_by_tag(\"property\")\n    assert len(properties) == 2\n    properties[0].assert_attr(name=\"key1\", value=\"value1\")\n    properties[1].assert_attr(name=\"key2\", value=\"value2\")\n", "def test_handle_unicode_characters_in_output(self, testdir):\n    testdir.makepyfile(\n        \"\"\"\n            record_property(\"unicode_key\", \"uni\u00e7\u00f8d\u00e9\")\n        \"\"\"\n    )\n    result, dom = runandparse(testdir)\n    testcase = dom.find_first_by_tag(\"testcase\")\n    properties = testcase.find_first_by_tag(\"properties\")\n    property = properties.find_first_by_tag(\"property\")\n    property.assert_attr(name=\"unicode_key\", value=\"uni\u00e7\u00f8d\u00e9\")\n", "def test_update_testcase_duration(testdir):\n    logxml = LogXML(\"junit.xml\", None)\n    report = BaseReport(\"test_nodeid\", \"call\", \"location\")\n    report.duration = 1.5\n\n    logxml.update_testcase_duration(report)\n    reporter = logxml.node_reporter(report)\n\n    assert reporter.duration == 1.5\n", "def test_junitxml_duration_report(self, testdir):\n    result, dom = runandparse(testdir, '--duration=1', '-o', 'junit_duration_report=call', '--junitxml=junit.xml', '-')\n    testcase = dom.find_first_by_tag(\"testcase\")\n    assert testcase[\"time\"] == \"1.000\"\n", "def test_record_testsuite_property(testdir):\n    testdir.makepyfile(\n        \"\"\"\n            record_testsuite_property(\"FOO\", \"BAR\")\n        \"\"\"\n    )\n    result, dom = runandparse(testdir)\n    result.assert_outcomes(passed=1)\n    properties = dom.find_nth_by_tag(\"properties\", 0)\n    assert properties is not None\n    property_nodes = properties.find_by_tag(\"property\")\n    assert len(property_nodes) == 1\n    property_nodes[0].assert_attr(name=\"FOO\", value=\"BAR\")\n"], "sample_798": ["def test_ridge_regression_with_sample_weight(solver):\n    # Test ridge_regression function with sample_weight parameter\n    X, y = make_regression(n_samples=100, n_features=10, random_state=0)\n    sample_weight = np.random.rand(100)\n    coef_with_weights = ridge_regression(X, y, alpha=1.0, sample_weight=sample_weight, solver=solver)\n    coef_without_weights = ridge_regression(X, y, alpha=1.0, solver=solver)\n    assert_raises(AssertionError, assert_array_almost_equal, coef_with_weights, coef_without_weights)\n", "def test_ridge_regression_multi_target(solver):\n    n_samples, n_features = 50, 10\n    n_targets = 3\n    X, y, coef = make_regression(n_samples=n_samples, n_features=n_features,\n                                 n_targets=n_targets, noise=0, coef=True)\n    alpha = np.array([1.0, 2.0, 3.0])\n    coef_pred = ridge_regression(X, y, alpha, solver=solver)\n    assert_array_almost_equal(coef_pred, coef)\n", "def test_ridge_regression_with_different_solvers(solver):\n    rng = np.random.RandomState(0)\n    X = rng.randn(50, 10)\n    y = rng.randn(50)\n    alpha = 0.5\n\n    coef = ridge_regression(X, y, alpha, solver=solver)\n\n    assert_equal(coef.shape, (10,))\n", "def test_ridge_regression_sparse_cg_with_fit_intercept(solver):\n    X, y = make_regression(n_samples=10, n_features=5, random_state=0, noise=0)\n    X = SPARSE_FILTER(X)\n\n    # Add a constant term to the data and the target\n    X = sp.hstack([X, np.ones((X.shape[0], 1))])\n    y += np.ones(y.shape[0])\n\n    coef, intercept = ridge_regression(X, y, alpha=1.0, solver=solver, return_intercept=True)\n\n    assert_array_almost_equal(coef, np.ones(X.shape[1] - 1))\n    assert_almost_equal(intercept, 1.0)\n", "def test_ridge_regression_solver_saga(solver):\n    X, y = make_regression(n_samples=10, n_features=5, random_state=0)\n    coef = ridge_regression(X, y, alpha=1.0, solver=solver, random_state=0)\n    expected_coef = np.array([-5.18933756, 3.13245864, -0.02871755, 0.52152086, -1.16275617])\n    assert_array_almost_equal(coef, expected_coef)\n"], "sample_1188": ["def test_pretty_print_cos():\n    expr = cos(x)\n    assert upretty(expr) == \"cos(x)\"\n    assert pretty(expr) == \"cos(x)\"\n", "compilation error", "def test_pretty_print_Differential():\n    from sympy import symbols, CoordSys3D, Differential\n\n    x, y, z = symbols('x y z')\n    N = CoordSys3D('N')\n\n    dx = Differential(x)\n    dy = Differential(y)\n    dz = Differential(z)\n    dNx = Differential(N.x)\n    dNy = Differential(N.y)\n    dNz = Differential(N.z)\n\n    assert pretty(dx) == \"dx\"\n    assert upretty(dx) == \"dx\"\n    assert pretty(dy) == \"dy\"\n    assert upretty(dy) == \"dy\"\n    assert pretty(dz) == \"dz\"\n    assert upretty(dz) == \"dz\"\n    assert pretty(dNx) == \"d(x_N)\"\n    assert upretty(dNx) == \"dx_N\"\n    assert pretty(dNy) == \"d(y_N)\"\n    assert upretty(dNy) == \"dy_N\"\n    assert pretty(dNz) == \"d(z_N)\"\n    assert upretty(dNz) == \"dz_N\"\n", "def test_pretty_printing_of_Sum():\n    expr = Sum(a**2, (a, 1, n))\n    assert upretty(expr) == \"\u2211 \u23a7n\u23ab  \u23a8 2\u23ac\\n\u23a8\u23aa\u23aa\\n\u23a9a \u23ad  a\\n\"\n    assert pretty(expr) == \"Sum(a**2, (a, 1, n))\"\n", "compilation error"], "sample_97": ["    def test_module_without_spec(self):\n        # Create a temporary module without a spec attribute\n        filename = self.temporary_file('temp_module.py')\n        filename.write_text('')\n        sys.path.append(str(filename.parent))\n        self.addCleanup(sys.path.remove, str(filename.parent))\n\n        self.import_and_cleanup('temp_module')\n        # Since the module doesn't have a spec attribute, it shouldn't be found\n        self.assertFileNotFound(filename)\n", "def test_import_error_file(self):\n    filename = self.temporary_file('broken_module.py')\n    filename.write_text('this is not valid python')\n    self.import_and_cleanup('broken_module')\n    autoreload._error_files = []  # clear the error files list before the test\n    with self.assertRaises(SyntaxError):\n        import_module('broken_module')\n    self.assertFileFound(filename)\n    self.assertIn(str(filename), autoreload._error_files)\n", "    def test_iter_modules_and_files_with_weakref_proxy_types(self):\n        # Create a mock weakref proxy type object and add it to sys.modules\n        weakref_proxy_type = mock.MagicMock(spec=weakref.ProxyType)\n        sys.modules['weakref_proxy_module'] = weakref_proxy_type\n\n        # Clear autoreload caches and check that the weakref proxy type module is not included\n        self.clear_autoreload_caches()\n        self.assertNotIn(weakref_proxy_type, list(autoreload.iter_all_python_module_files()))\n\n        # Clean up the added weakref proxy type module\n        self.addCleanup(lambda: sys.modules.pop('weakref_proxy_module', None))\n", "def test_import_from_zip(self):\n    zip_file_path = self.temporary_file('test.zip')\n    module_name = 'test_module'\n    module_path = 'test_module.py'\n    code = 'def func():\\n    pass\\n'\n\n    with zipfile.ZipFile(zip_file_path, 'w') as zipf:\n        zipf.writestr(module_path, code)\n\n    sys.path.insert(0, str(zip_file_path))\n    self.addCleanup(sys.path.remove, str(zip_file_path))\n\n    self.import_and_cleanup(module_name)\n    self.assertFileFound(zip_file_path)\n", "    def test_import_error_file(self):\n        filename = self.temporary_file('bad_syntax.py')\n        filename.write_text('invalid syntax')\n        with extend_sys_path(filename.parent):\n            with self.assertRaises(SyntaxError):\n                self.import_and_cleanup('bad_syntax')\n            self.assertFileFound(filename)\n        # Importing the same file again should not raise an exception.\n        with extend_sys_path(filename.parent):\n            self.import_and_cleanup('bad_syntax')\n"], "sample_851": ["def test_mean_tweedie_deviance_multioutput():\n    y_true = [[2, 0], [0.5, 1], [1, 4]]\n    y_pred = [[0.5, 0.5], [0.5, 2.0], [2.0, 2.0]]\n    with pytest.raises(ValueError, match=\"Multioutput not supported\"):\n        mean_tweedie_deviance(y_true, y_pred, power=1)\n", "def test_mean_tweedie_deviance_power_0():\n    y_true = [2, 0, 1, 4]\n    y_pred = [0.5, 0.5, 2., 2.]\n    result = mean_tweedie_deviance(y_true, y_pred, power=0)\n    expected_result = mean_squared_error(y_true, y_pred)\n    assert_almost_equal(result, expected_result)\n", "def test_mean_tweedie_deviance_power_0():\n    y_true = np.array([2, 0, 1, 4])\n    y_pred = np.array([2, 0.5, 1, 4])\n    deviance = mean_tweedie_deviance(y_true, y_pred, power=0)\n    expected_deviance = mean_squared_error(y_true, y_pred)\n    assert_almost_equal(deviance, expected_deviance)\n", "def test_mean_tweedie_deviance_errors():\n    y_true = np.array([2, 0, 1, 4])\n    y_pred = np.array([0.5, 0.5, 2., 2.])\n    with pytest.raises(ValueError, match=\"strictly positive y_pred\"):\n        mean_tweedie_deviance(y_true, -y_pred, power=-1)\n    with pytest.raises(ValueError, match=\"strictly positive y_pred\"):\n        mean_tweedie_deviance(y_true, -y_pred, power=0)\n    with pytest.raises(ValueError, match=\"non-negative y_true and strictly positive y_pred\"):\n        mean_tweedie_deviance(-y_true, y_pred, power=1)\n    with pytest.raises(ValueError, match=\"non-negative y_true and strictly positive y_pred\"):\n        mean_tweedie_deviance(-y_true, y_pred, power=0.5)\n    with pytest.raises(ValueError, match=\"strictly positive y_true and y_pred\"):\n        mean_tweedie_deviance(-y_true, y_pred, power=2)\n    with pytest.raises(ValueError, match=\"strictly positive y_true and y_pred\"):\n        mean_tweedie_deviance(-y_true, y_pred, power=3)\n", "def test_mean_tweedie_deviance_power_0():\n    y_true = np.array([2, 0, 1, 4])\n    y_pred = np.array([1, 0, 2, 3])\n    expected_loss = 2.75\n    loss = mean_tweedie_deviance(y_true, y_pred, power=0)\n    assert_almost_equal(loss, expected_loss)\n"], "sample_449": ["def test_wsgi_request_handler_get_environ(self):\n    request = self.request_factory.get('/', HTTP_USER_AGENT='test_user_agent')\n    environ = WSGIRequestHandler.get_environ(Stub(rfile=request.META['wsgi.input'], headers=request.META))\n    self.assertEqual(environ['HTTP_USER_AGENT'], 'test_user_agent')\n    self.assertNotIn('HTTP_USER_AGENT_WITH_UNDERSCORES', environ)\n", "    def test_address_string(self):\n        class FakeSocket:\n                self.addr = addr\n\n                return self.addr\n\n        class FakeRequestHandler(WSGIRequestHandler):\n                super().__init__(*args, **kwargs)\n                self.rfile = UnclosableBytesIO(b'GET / HTTP/1.1\\r\\n\\r\\n')\n                self.wfile = UnclosableBytesIO()\n                self.connection = FakeSocket((\"127.0.0.1\", 8000))\n                self.client_address = self.connection.getsockname()\n\n        request_handler = FakeRequestHandler(None, None, None)\n        self.assertEqual(request_handler.address_string(), \"127.0.0.1\")\n", "    def test_log_message(self):\n        request = WSGIRequest(self.request_factory.get('/').environ)\n        handler = WSGIRequestHandler(Stub(getsockname=lambda: ('127.0.0.1', 8000)), ('127.0.0.1', 8000), None)\n        handler.request = request\n        with captured_stderr() as stderr:\n            handler.log_message('%s %s %s', '127.0.0.1', '200', '-')\n        self.assertIn('\"status_code\": 200', stderr.getvalue())\n", "def test_handle_one_request(self):\n    wsgi_handler = lambda env, start_response: [b'Hello, world!']\n    request = self.request_factory.get('/')\n    request_bytes = request.get_raw_request()\n    client = Stub(rfile=BytesIO(request_bytes), wfile=UnclosableBytesIO(), get_stderr=lambda: BytesIO())\n    handler = WSGIRequestHandler(client, ('localhost', 8000), WSGIServer)\n    handler.set_app(wsgi_handler)\n\n    with captured_stderr() as err:\n        handler.handle_one_request()\n\n    self.assertIn('\"GET / HTTP/1.1\" 200', err.getvalue())\n    self.assertEqual(client.wfile.getvalue(), b'Hello, world!')\n", "    def test_close_connection(self):\n        request = self.request_factory.get('/')\n        request_handler = WSGIRequestHandler(request, '127.0.0.1', Stub())\n        request_handler.wfile = UnclosableBytesIO()\n\n        # Set close_connection attribute to True\n        request_handler.close_connection = True\n        request_handler.handle_one_request()\n\n        # Check if connection was closed\n        self.assertTrue(request_handler.wfile.closed)\n"], "sample_756": ["def test_extract_dbscan_stability_warning():\n    optics = OPTICS(min_samples=5, max_eps=2.0)\n    optics.fit(X)\n\n    # Test with eps close to max_eps, should raise a warning\n    with assert_warns(RuntimeWarning):\n        optics.extract_dbscan(eps=1.8)\n", "def test_extract_dbscan_eps_warning():\n    clust = OPTICS(min_samples=5, max_eps=5, metric='euclidean')\n    clust.fit(X)\n    with assert_warns(RuntimeWarning, match=\"Warning, max_eps\"):\n        clust.extract_dbscan(eps=4)\n", "def test_dbscan_extraction():\n    optics = OPTICS(min_samples=5, max_eps=3, metric='euclidean')\n    optics.fit(X)\n    core_sample_indices, labels = optics.extract_dbscan(eps=2)\n\n    # Test if the number of core samples is correct\n    assert_equal(len(core_sample_indices), 50)\n\n    # Test if the labels are correct\n    dbscan = DBSCAN(eps=2, min_samples=5)\n    dbscan_labels = dbscan.fit_predict(X)\n    assert_array_equal(labels, dbscan_labels)\n\n    # Test if the extraction is stable\n    eps_close_to_max_eps = optics.max_eps - 0.1\n    assert_warns(RuntimeWarning, optics.extract_dbscan, eps=eps_close_to_max_eps)\n", "def test_optics_min_cluster_size_too_large():\n    with assert_raise_message(ValueError, \"min_cluster_size must be no greater than the number of samples\"):\n        optics = OPTICS(min_cluster_size=len(X) + 1)\n        optics.fit(X)\n", "def test_optics_cluster_hierarchy():\n    # Test the extraction of cluster hierarchy from the OPTICS algorithm\n    clust = OPTICS(min_samples=5, max_eps=np.inf, metric='euclidean')\n    clust.fit(X)\n\n    # Define a simple hierarchy for the test data\n    true_labels = np.concatenate((np.zeros(50), np.ones(50)))\n    true_children = [np.arange(100), np.arange(50, 100)]\n\n    # Create a mock root node and its children\n    root_node = _TreeNode(clust.ordering_, 0, 100, None)\n    child_nodes = [_TreeNode(clust.ordering_[start:end], start, end, root_node)\n                   for start, end in zip(true_children[:-1], true_children[1:])]\n    root_node.children = child_nodes\n\n    # Test the _cluster_tree function\n    _cluster_tree(root_node, None, [], clust.reachability_[clust.ordering_],\n                  clust.ordering_, min_cluster_size=5, maxima_ratio=0.75,\n                  rejection_ratio=0.7, similarity_threshold=0.4,\n                  significant_min=0.003)\n\n    # Check the cluster hierarchy\n    extracted_labels = np.zeros(100)\n    for i, child in enumerate(root_node.children):\n        extracted_labels[child.start:child.end] = i\n\n    assert_array_equal(extracted_labels, true_labels)\n"], "sample_115": ["    def test_get_traceback_data(self):\n        request = RequestFactory().get('/')\n        exc_type = Exception\n        exc_value = Exception(\"Test exception\")\n        tb = exc_value.__traceback__\n        reporter = ExceptionReporter(request, exc_type, exc_value, tb)\n        data = reporter.get_traceback_data()\n        self.assertEqual(data['exception_type'], exc_type.__name__)\n        self.assertEqual(data['exception_value'], str(exc_value))\n", "    def setUp(self):\n        self.rf = RequestFactory()\n", "    def test_get_traceback_data(self):\n        request = RequestFactory().get('/')\n        try:\n            1 / 0\n        except Exception as e:\n            exc_type, exc_value, tb = sys.exc_info()\n            reporter = ExceptionReporter(request, exc_type, exc_value, tb)\n            data = reporter.get_traceback_data()\n            self.assertIsNotNone(data)\n            self.assertIn('is_email', data)\n            self.assertIn('frames', data)\n            # Add more assertions to validate other data fields as needed\n", "    def test_get_traceback_frames(self):\n        try:\n            raise ValueError(\"Test exception\")\n        except ValueError:\n            type_, value, tb = sys.exc_info()\n            reporter = ExceptionReporter(None, type_, value, tb)\n            frames = reporter.get_traceback_frames()\n            self.assertIsNotNone(frames)\n            self.assertIsInstance(frames, list)\n            self.assertTrue(all(isinstance(frame, dict) for frame in frames))\n", "    def setUp(self):\n        self.rf = RequestFactory()\n"], "sample_778": ["def test_NMF_with_dense_input_and_multiplicative_update():\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    model = NMF(n_components=2, init='random', solver='mu', random_state=0)\n    W = model.fit_transform(X)\n    H = model.components_\n\n    # Test the shape of the output\n    assert W.shape == (3, 2)\n    assert H.shape == (2, 2)\n\n    # Test that the output is non-negative\n    assert np.all(W >= 0)\n    assert np.all(H >= 0)\n\n    # Test that the product of W and H approximates X\n    assert_array_almost_equal(np.dot(W, H), X, decimal=2)\n", "def test_nmf_sparse_input_with_zero_elements():\n    X = sp.csc_matrix([[1, 0, 2], [0, 0, 3]])\n    nmf = NMF(n_components=2, init='random', random_state=0)\n    assert_raise_message(ValueError,\n                         \"When beta_loss <= 0 and X contains zeros, the solver may diverge.\",\n                         nmf.fit, X, y=None)\n", "def test_nmf_convergence_warning():\n    X = np.array([[1, 2], [3, 4]])\n    nmf = NMF(n_components=2, max_iter=1, tol=1e-4, random_state=0)\n    with pytest.warns(ConvergenceWarning):\n        nmf.fit(X)\n", "def test_non_negative_factorization_invalid_input():\n    # Test invalid input to non_negative_factorization function\n    X = np.array([[1, 2], [3, 4]])\n\n    # Test with invalid solver\n    with pytest.raises(ValueError, match=\"Invalid solver parameter: got 'invalid' instead of one of\"):\n        non_negative_factorization(X, solver='invalid')\n\n    # Test with invalid regularization\n    with pytest.raises(ValueError, match=\"Invalid regularization parameter: got 'invalid' instead of one of\"):\n        non_negative_factorization(X, regularization='invalid')\n\n    # Test with invalid beta_loss\n    with pytest.raises(ValueError, match=\"Invalid beta_loss parameter: got 'invalid' instead of one of\"):\n        non_negative_factorization(X, solver='mu', beta_loss='invalid')\n\n    # Test with invalid init\n    with pytest.raises(ValueError, match=\"Invalid init parameter: got 'invalid' instead of one of\"):\n        non_negative_factorization(X, init='invalid')\n\n    # Test with non-positive number of components\n    with pytest.raises(ValueError, match=\"Number of components must be a positive integer\"):\n        non_negative_factorization(X, n_components=0)\n\n    # Test with non-positive maximum number of iterations\n    with pytest.raises(ValueError, match=\"Maximum number of iterations must be a positive integer\"):\n        non_negative_factorization(X, max_iter=-1)\n\n    # Test with negative tolerance\n    with pytest.raises(ValueError, match=\"Tolerance for stopping criteria must be positive\"):\n        non_negative_factorization(X, tol=-1)\n\n    # Test with zero in input matrix when beta_loss <= 0\n    X_zero = np.array([[1, 0], [3, 4]])\n    with pytest.raises(ValueError, match=\"When beta_loss <= 0 and X contains zeros\"):\n        non_negative_factorization(X_zero, solver='mu', beta", "def test_n_components_types():\n    X = np.random.RandomState(0).rand(10, 10)\n    nmf_model = NMF(n_components=5)\n    with pytest.raises(ValueError, match=\"Number of components must be a positive integer; got \\\\(n_components='abc'\\\\)\"):\n        nmf_model.set_params(n_components='abc')\n"], "sample_72": ["    def test_money_serializer(self):\n        value = Money('10.50')\n        serializer = serializer_factory(value)\n        self.assertEqual(serializer.serialize(), ('Money(\\'10.50\\')', {'import decimal'}))\n", "    def test_money_serializer(self):\n        value = Money('123.45')\n        serializer = serializer_factory(value)\n        self.assertEqual(serializer.serialize(), (\"'__main__.Money'(['123.45'])\", {\"import __main__\"}))\n", "    def test_uuid_serializer(self):\n        value = uuid.uuid4()\n        serializer = UUIDSerializer(value)\n        expected_output = \"uuid.%s\" % repr(value)\n        expected_imports = {\"import uuid\"}\n        output, imports = serializer.serialize()\n        self.assertEqual(output, expected_output)\n        self.assertEqual(imports, expected_imports)\n", "    def test_custom_serializer_registration(self):\n        class CustomSerializer(BaseSerializer):\n                return 'custom_value', {'import custom_module'}\n\n        Serializer.register(Money, CustomSerializer)\n        serializer = serializer_factory(Money('12.34'))\n        self.assertIsInstance(serializer, CustomSerializer)\n        result, imports = serializer.serialize()\n        self.assertEqual(result, 'custom_value')\n        self.assertEqual(imports, {'import custom_module'})\n        Serializer.unregister(Money)\n", "    def test_serialize_enum(self):\n        class Color(enum.Enum):\n            RED = 1\n            GREEN = 2\n            BLUE = 3\n\n        serializer = serializer_factory(Color.RED)\n        result, imports = serializer.serialize()\n        expected_result = \"test_serialize_enum.<locals>.Color(1)\"\n        expected_imports = {\"import enum\"}\n        self.assertEqual(result, expected_result)\n        self.assertEqual(imports, expected_imports)\n"], "sample_846": ["def test_column_transformer_get_feature_names():\n    # Test get_feature_names method of ColumnTransformer\n    ct = ColumnTransformer([(\"trans1\", Trans(), [0, 1]),\n                             (\"trans2\", Trans(), [2])])\n    X = np.array([[0., 1., 2.], [1., 1., 0.]])\n    ct.fit(X)\n    feature_names = ct.get_feature_names()\n    assert feature_names == ['trans1__0', 'trans1__1', 'trans2__2']\n", "def test_column_transformer_with_slice():\n    # Test ColumnTransformer with slice column specification\n    ct = ColumnTransformer(transformers=[(\"trans\", Trans(), slice(0, 2))])\n    X = np.array([[0., 1., 2., 2.],\n                  [1., 1., 0., 1.]])\n    Xt = ct.fit_transform(X)\n    expected = np.array([[0., 0.],\n                         [1., 1.]])\n    assert_array_equal(Xt, expected)\n", "def test_column_transformer_with_empty_column_selection():\n    # Test that the ColumnTransformer handles the case where a transformer is\n    # specified with an empty column selection\n    ct = ColumnTransformer(\n        transformers=[\n            (\"trans\", Trans(), [0, 1]),\n            (\"empty\", Trans(), []),\n            (\"double\", DoubleTrans(), [2, 3]),\n        ],\n        remainder=\"passthrough\",\n    )\n\n    X = np.array([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\n    X_transformed = ct.fit_transform(X)\n\n    # The transformed output should only include the output of the \"trans\" and\n    # \"double\" transformers, since the \"empty\" transformer has an empty column\n    # selection\n    assert_array_equal(X_transformed, np.array([[1, 2, 6, 8, 5], [6, 7, 16, 18, 10]]))\n\n    X_test = np.array([[11, 12, 13, 14, 15], [16, 17, 18, 19, 20]])\n    X_test_transformed = ct.transform(X_test)\n\n    assert_array_equal(X_test_transformed, np.array([[11, 12, 26, 32, 15], [16, 17, 32, 36, 20]]))\n", "def test_column_transformer_callable_column_selection():\n    # Test when column selection is a callable function.\n    X = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8]])\n    ct = ColumnTransformer([(\"trans1\", Trans(), lambda X: [0])])\n    ct.fit_transform(X)\n    assert_array_equal(ct.transformers_[0][2], [0])\n", "def test_column_transformer_with_callable_column_selector():\n    # Test ColumnTransformer with callable column selector\n    transformer = make_column_transformer(\n        (StandardScaler(), lambda X: [0, 1]),\n        (Normalizer(), lambda X: [2, 3])\n    )\n    X = np.array([[1, 2, 3, 4], [5, 6, 7, 8]])\n    result = transformer.fit_transform(X)\n    expected_result = np.array([[-1.34164079, -0.4472136, 0.6081816, 0.79596059],\n                                [1.34164079, 0.4472136, 0.81649658, 0.99497475]])\n    assert_allclose_dense_sparse(result, expected_result)\n"], "sample_538": ["def test_transform_node_invalidate():\n    node = mtransforms.TransformNode()\n    parent = mtransforms.TransformNode()\n    parent.set_children(node)\n    assert parent._invalid == 0\n    node.invalidate()\n    assert parent._invalid == mtransforms.TransformNode.INVALID\n", "def test_transform_path():\n    trans = mtransforms.Affine2D().scale(2, 3)\n    path = Path([(1, 1), (2, 2), (3, 3)], [Path.MOVETO, Path.LINETO, Path.LINETO])\n    transformed_path = trans.transform_path(path)\n    expected_path = Path([(2, 3), (4, 6), (6, 9)], [Path.MOVETO, Path.LINETO, Path.LINETO])\n    assert_array_equal(transformed_path.vertices, expected_path.vertices)\n    assert_array_equal(transformed_path.codes, expected_path.codes)\n", "def test_TransformedPatchPath():\n    fig, ax = plt.subplots()\n    patch = mpatches.Circle((0.5, 0.5), 0.25, transform=ax.transData)\n    transformed_patch = mtransforms.TransformedPatchPath(patch)\n    assert isinstance(transformed_patch, mtransforms.TransformedPath)\n    assert transformed_patch._patch is patch\n    assert transformed_patch._path is patch.get_path()\n    assert transformed_patch._transform is patch.get_transform()\n    plt.close(fig)\n", "def test_transform_path():\n    path = Path([(0, 0), (1, 1), (2, 0)], [1, 2, 79])\n    transform = mtransforms.Affine2D().rotate_deg(45)\n    expected_vertices = np.array([(0, 0), (np.sqrt(2)/2, np.sqrt(2)/2), (1, 0)])\n    transformed_path = transform.transform_path(path)\n    assert_array_almost_equal(transformed_path.vertices, expected_vertices)\n", "def test_TransformWrapper_set_invalid_dimensions():\n    # Create a TransformWrapper with a child transform of input_dims=2 and output_dims=2\n    wrapper = mtransforms.TransformWrapper(mtransforms.Affine2D())\n    # Try to set a child transform with input_dims=1 and output_dims=1\n    with pytest.raises(ValueError, match=\"The input and output dims of the new child\"):\n        wrapper.set(mtransforms.IdentityTransform())\n"], "sample_850": ["def test_skewed_chi2_sampler():\n    skewedness = 0.01\n    n_components = 10\n    random_state = 0\n    chi2_sampler = SkewedChi2Sampler(skewedness=skewedness,\n                                     n_components=n_components,\n                                     random_state=random_state)\n    X_transformed = chi2_sampler.fit_transform(X)\n    assert X_transformed.shape == (300, n_components)\n    assert np.all(X_transformed >= -1) and np.all(X_transformed <= 1)\n\n    # Test transform with negative skewedness\n    X_neg = X - 2 * skewedness\n    with pytest.raises(ValueError):\n        chi2_sampler.transform(X_neg)\n", "def test_skewed_chi2_sampler():\n    chi2_sampler = SkewedChi2Sampler(skewedness=0.1, n_components=10, random_state=0)\n    X_transformed = chi2_sampler.fit_transform(X)\n    assert X_transformed.shape == (300, 10)\n    # Add more assertions to validate the transformed data if needed\n", "def test_skewed_chi2_sampler():\n    skewedness = 0.01\n    n_components = 10\n    chi2_feature = SkewedChi2Sampler(skewedness=skewedness, n_components=n_components, random_state=0)\n    X_features = chi2_feature.fit_transform(X)\n    assert X_features.shape == (X.shape[0], n_components)\n    assert np.all(X_features >= -1) and np.all(X_features <= 1)\n", "def test_nystroem_transform():\n    nystroem = Nystroem(kernel='rbf', gamma=0.5, n_components=100, random_state=0)\n    nystroem.fit(X)\n    transformed_X = nystroem.transform(X)\n    assert transformed_X.shape == (300, 100)\n\n    # Check that the transform is deterministic\n    transformed_X_again = nystroem.transform(X)\n    assert_array_equal(transformed_X, transformed_X_again)\n\n    # Check that the transform is correct by comparing with the exact kernel\n    K_exact = rbf_kernel(X, nystroem.components_, gamma=0.5)\n    K_approx = np.dot(K_exact, nystroem.normalization_.T)\n    assert_array_almost_equal(transformed_X, K_approx)\n", "def test_additive_chi2_sampler():\n    chi2_sampler = AdditiveChi2Sampler(sample_steps=2)\n    chi2_sampler.fit(X)\n    X_transformed = chi2_sampler.transform(X)\n    assert X_transformed.shape == (300, 150)  # 50 features * (2*2 + 1)\n    # Test for non-negativity\n    assert (X_transformed >= 0).all()\n\n    # Test with zero values\n    X_with_zeros = np.concatenate([X, np.zeros_like(X)], axis=1)\n    X_transformed_with_zeros = chi2_sampler.transform(X_with_zeros)\n    assert X_transformed_with_zeros.shape == (300, 300)  # 100 features * (2*2 + 1)\n    # Test for non-negativity\n    assert (X_transformed_with_zeros >= 0).all()\n\n    # Test with negative values\n    X_with_negatives = np.concatenate([X, -X], axis=1)\n    with pytest.raises(ValueError):\n        chi2_sampler.transform(X_with_negatives)\n"], "sample_174": ["def test_adapt_decimalfield_value(self):\n    value = decimal.Decimal('123.456')\n    max_digits = 10\n    decimal_places = 3\n    result = self.ops.adapt_decimalfield_value(value, max_digits, decimal_places)\n    self.assertEqual(result, '123.456')\n", "    def test_date_trunc_sql(self):\n        with self.assertRaisesMessage(NotImplementedError, self.may_require_msg % 'date_trunc_sql'):\n            self.ops.date_trunc_sql('year', 'field_name')\n", "    def test_adapt_decimalfield_value(self):\n        value = decimal.Decimal('123456789.123456789')\n        max_digits = 18\n        decimal_places = 9\n        result = self.ops.adapt_decimalfield_value(value, max_digits, decimal_places)\n        self.assertEqual(result, '123456789.123456789')\n", "def test_last_insert_id(self):\n    class TestCursor:\n        lastrowid = 12345\n\n    result = self.ops.last_insert_id(cursor=TestCursor(), table_name='test_table', pk_name='id')\n    self.assertEqual(result, 12345)\n", "    def test_datetime_cast_date_sql(self):\n        field_name = 'test_field'\n        tzname = 'UTC'\n        result = self.ops.datetime_cast_date_sql(field_name, tzname)\n        self.assertIsNotNone(result)\n"], "sample_88": ["    def test_attach_file(self):\n        temp_dir = tempfile.mkdtemp()\n        try:\n            test_file_path = os.path.join(temp_dir, 'test.txt')\n            with open(test_file_path, 'w') as f:\n                f.write('Test content')\n\n            msg = EmailMessage('Subject', 'Body', 'from@example.com', ['to@example.com'])\n            msg.attach_file(test_file_path)\n\n            attachments = self.get_decoded_attachments(msg)\n            self.assertEqual(len(attachments), 1)\n            filename, content, mimetype = attachments[0]\n            self.assertEqual(filename, 'test.txt')\n            self.assertEqual(content.decode(), 'Test content')\n            self.assertEqual(mimetype, 'text/plain')\n        finally:\n            shutil.rmtree(temp_dir)\n", "    def test_email_message_attachments(self):\n        email = EmailMessage(\n            subject='Test subject',\n            body='Test body',\n            from_email='from@example.com',\n            to=['to@example.com'],\n        )\n        email.attach(filename='test.txt', content='Test attachment content', mimetype='text/plain')\n        attachments = self.get_decoded_attachments(email)\n        self.assertEqual(len(attachments), 1)\n        self.assertEqual(attachments[0], ('test.txt', b'Test attachment content', 'text/plain'))\n", "    def test_email_message_attachments(self):\n        \"\"\"Test EmailMessage with attachments.\"\"\"\n        subject = 'Test email with attachments'\n        message = 'This is a test email with attachments.'\n        from_email = 'from@example.com'\n        to = ['to@example.com']\n\n        # Create an email message with an attachment\n        email = EmailMessage(subject, message, from_email, to)\n        attachment = b'This is the content of the attachment.'\n        email.attach('attachment.txt', attachment, 'text/plain')\n\n        # Verify the email message has the correct headers\n        headers = {('Subject', subject), ('From', from_email), ('To', to[0])}\n        self.assertMessageHasHeaders(email.message(), headers)\n\n        # Verify the email message has the correct attachment\n        attachments = self.get_decoded_attachments(email)\n        self.assertEqual(len(attachments), 1)\n        filename, content, mimetype = attachments[0]\n        self.assertEqual(filename, 'attachment.txt')\n        self.assertEqual(content, attachment)\n        self.assertEqual(mimetype, 'text/plain')\n", "def test_sanitize_address(self):\n    # Test sanitize_address function with ASCII characters\n    addr = 'test@example.com'\n    sanitized = sanitize_address(addr, 'ascii')\n    self.assertEqual(sanitized, addr)\n\n    # Test sanitize_address function with Unicode characters\n    addr = 'tester@exampl\u00e9.com'\n    sanitized = sanitize_address(addr, 'utf-8')\n    self.assertEqual(sanitized, 'tester@xn--exampl-gva.com')\n\n    # Test sanitize_address function with name and address\n    addr = ('John Doe', 'johndoe@example.com')\n    sanitized = sanitize_address(addr, 'ascii')\n    self.assertEqual(sanitized, '=?ascii?q?John_Doe?= <johndoe@example.com>')\n\n    # Test sanitize_address function with invalid address\n    addr = 'invalid-email'\n    with self.assertRaises(ValueError):\n        sanitize_address(addr, 'ascii')\n", "    def test_email_message_attachments(self):\n        test_file_path = self.create_test_file()\n        subject = 'Test subject'\n        message = 'Test message'\n        from_email = 'from@example.com'\n        to_email = ['to@example.com']\n\n        email = EmailMessage(subject, message, from_email, to_email)\n        email.attach_file(test_file_path)\n        email.send()\n\n        # Test attachments\n        attachments = self.get_decoded_attachments(email)\n        self.assertEqual(len(attachments), 1)\n        filename, content, mimetype = attachments[0]\n        self.assertEqual(filename, os.path.basename(test_file_path))\n        with open(test_file_path, 'rb') as file:\n            self.assertEqual(content, file.read())\n        self.assertEqual(mimetype, mimetypes.guess_type(test_file_path)[0])\n"], "sample_552": ["def test_figure_align_labels():\n    fig, axs = plt.subplots(2, 2)\n    axs[0, 0].set_xlabel('XLabel 0')\n    axs[0, 1].set_xlabel('XLabel 1')\n    axs[1, 0].set_ylabel('YLabel 0')\n    axs[1, 1].set_ylabel('YLabel 1')\n    fig.align_labels()\n", "def test_align_labels():\n    fig, axs = plt.subplots(2, 2, sharex=True, sharey=True)\n    for ax in axs.flat:\n        ax.label_outer()\n    fig.align_labels()\n", "def test_subplot_mosaic():\n    fig = Figure()\n    ax_dict = fig.subplot_mosaic([['A', 'B'], ['C', 'D']])\n    assert isinstance(ax_dict['A'], Axes)\n    assert isinstance(ax_dict['B'], Axes)\n    assert isinstance(ax_dict['C'], Axes)\n    assert isinstance(ax_dict['D'], Axes)\n", "def test_figure_align_labels():\n    fig, axs = plt.subplots(2, 2)\n    axs[0, 0].set_xlabel('XLabel 0')\n    axs[1, 0].set_xlabel('XLabel 1')\n    fig.align_xlabels()\n    axs[0, 0].set_ylabel('YLabel 0')\n    axs[0, 1].set_ylabel('YLabel 1')\n    fig.align_ylabels()\n    fig.canvas.draw()\n", "def test_align_labels():\n    fig, axs = plt.subplots(2, 2)\n    for i, ax in enumerate(axs.flat):\n        ax.set_title(f'Axes {i+1}')\n        ax.set_xlabel(f'X Label {i+1}')\n        ax.set_ylabel(f'Y Label {i+1}')\n\n    fig.align_labels()\n\n    # Check that labels are aligned\n    xlabels = [ax.xaxis.get_label().get_position() for ax in axs.flat]\n    ylabels = [ax.yaxis.get_label().get_position() for ax in axs.flat]\n    assert np.allclose(xlabels, xlabels[0])\n    assert np.allclose(ylabels, ylabels[0])\n"], "sample_706": ["def test_not_expression():\n    assert evaluate(\"not test\", lambda x: x == \"test\") is True\n    assert evaluate(\"not another\", lambda x: x == \"test\") is False\n", "def test_not_expression():\n        return id != 'foo'\n\n    assert evaluate('not foo', matcher) is True\n    assert evaluate('not bar', matcher) is False\n", "def test_evaluate_complex_expression(mocker):\n    matcher = mocker.Mock(side_effect=lambda x: x in ['foo', 'bar', 'baz'])\n\n    # Test a complex expression with parentheses and not operator\n    assert evaluate(\"(foo or bar) and not baz\", matcher) is True\n    matcher.assert_has_calls([mocker.call('foo'), mocker.call('bar'), mocker.call('baz')])\n\n    # Reset the mock and test a different complex expression\n    matcher.reset_mock()\n    assert evaluate(\"foo and (bar or not baz)\", matcher) is True\n    matcher.assert_has_calls([mocker.call('foo'), mocker.call('bar')])\n", "def test_or_expression():\n        return s in (\"foo\", \"bar\")\n\n    assert evaluate(\"foo or bar\", matcher) is True\n    assert evaluate(\"foo or baz\", matcher) is True\n    assert evaluate(\"baz or bar\", matcher) is True\n    assert evaluate(\"baz or qux\", matcher) is False\n", "def test_expression_compile_and_evaluate():\n        return s in {\"test1\", \"test2\"}\n\n    assert evaluate(\"test1\", matcher) is True\n    assert evaluate(\"test2\", matcher) is True\n    assert evaluate(\"test3\", matcher) is False\n    assert evaluate(\"(test1 and not test2) or test3\", matcher) is False\n    assert evaluate(\"(test1 and not test2) or (test3 and test1)\", matcher) is True\n"], "sample_315": ["    def test_process_request(self):\n        request = RequestFactory().get('/nl/test/')\n        middleware = LocaleMiddleware()\n        middleware.process_request(request)\n        self.assertEqual(request.LANGUAGE_CODE, 'nl')\n", "    def test_process_request(self):\n        request = RequestFactory().get('/nl/test/')\n        middleware = LocaleMiddleware()\n        middleware.process_request(request)\n        self.assertEqual(request.LANGUAGE_CODE, 'nl')\n", "    def setUp(self):\n        super().setUp()\n        self.factory = RequestFactory()\n        self.middleware = LocaleMiddleware()\n        self.request = self.factory.get('/')\n", "    def setUp(self):\n        super().setUp()\n        self.factory = RequestFactory()\n        self.middleware = LocaleMiddleware()\n", "    def setUp(self):\n        super().setUp()\n        self.factory = RequestFactory()\n        self.middleware = LocaleMiddleware()\n        self.request = self.factory.get('/')\n"], "sample_601": ["    def test_season(self):\n        expected_seasons = np.array(\n            [\"DJF\", \"DJF\", \"DJF\", \"MAM\", \"MAM\", \"MAM\", \"JJA\", \"JJA\", \"JJA\", \"SON\", \"SON\", \"SON\"]\n        )\n        expected_seasons = np.tile(expected_seasons, 8)  # Repeat the pattern to match the length of self.times\n        result_seasons = self.times_data.dt.season.values\n        assert_array_equal(result_seasons, expected_seasons)\n", "def test_round_accessors(self, freq, expected_name):\n    accessor = getattr(self.times_data.dt, expected_name)\n    result = accessor(freq)\n    assert isinstance(result, xr.DataArray)\n    assert result.name == expected_name\n", "def test_round_methods(self, freq):\n    result_ceil = self.times_data.dt.ceil(freq)\n    result_floor = self.times_data.dt.floor(freq)\n    result_round = self.times_data.dt.round(freq)\n\n    for res in [result_ceil, result_floor, result_round]:\n        assert_identical(res.time, self.times_data)\n        assert isinstance(res.data, type(self.times_data.data))\n        assert res.data.dtype == self.times_data.data.dtype\n", "def test_rounding(self, freq):\n    actual = self.data.time.dt.round(freq)\n    expected = self.data.time.dt.floor(freq) + pd.Timedelta(0.5, unit=freq)\n    expected = expected.where(self.data.time < expected, expected - pd.Timedelta(1, unit=freq))\n    assert_equal(actual, expected)\n", "def test_season_accessor(self, field):\n    result = self.times_data.dt[field]\n    expected_seasons = np.array(\n        [\"DJF\", \"DJF\", \"DJF\", \"MAM\", \"MAM\", \"MAM\", \"JJA\", \"JJA\", \"JJA\", \"SON\", \"SON\", \"SON\"]\n    )\n    expected = xr.DataArray(\n        expected_seasons[(self.times_data.dt.month - 1) // 3],\n        coords=[self.times_data.lon, self.times_data.lat, self.times_data.time],\n        dims=[\"lon\", \"lat\", \"time\"],\n        name=\"season\",\n    )\n    assert_identical(result, expected)\n"], "sample_1092": ["def test_cse_with_custom_optimizations():\n    expr = (w + x + y + z)**2\n    optimizations = [(sub_pre, sub_post)]\n    replacements, reduced_exprs = cse([expr], optimizations=optimizations)\n    assert replacements == [(x0, w + x + y + z)]\n    assert reduced_exprs == [x0**2]\n", "def test_cse_with_complex_expressions():\n    exprs = [sqrt(x**2 + y**2), exp(x + y), sin(x) + cos(y)]\n    replacements, reduced_exprs = cse(exprs)\n    assert len(replacements) == 1\n    assert len(reduced_exprs) == 3\n    assert replacements[0][0] == x0\n    assert replacements[0][1] == sqrt(x**2 + y**2)\n    assert reduced_exprs[0] == x0\n    assert reduced_exprs[1] == exp(x + y)\n    assert reduced_exprs[2] == sin(x) + cos(y)\n", "def test_cse_with_matrices():\n    m = Matrix([x + y, x + y + z])\n    replacements, reduced_exprs = cse([(x+y)**2, x + y + z, y + z, x + z + y, m])\n    assert replacements == [(x0, x + y), (x1, x0 + z)]\n    assert reduced_exprs == [x0**2, x1, y + z, x1, Matrix([[x0], [x1]])]\n", "def test_cse_with_matrices():\n    A = Matrix([[x + y, z], [w, x + z]])\n    B = Matrix([[x + y, z + w], [w, x + z + w]])\n    replacements, reduced_exprs = cse([A, B])\n    assert len(replacements) == 2\n    assert reduced_exprs == [Matrix([[x0, z], [w, x + z]]), Matrix([[x0, z + w], [w, x1]])]\n", "def test_cse_separate():\n    eq = (x + 1 + exp((x + 1)/(y + 1)) + cos(y + 1))\n    r, e = cse([eq, Eq(x, z + 1), z - 2], postprocess=cse_main.cse_separate)\n    assert sorted(r) == sorted([(x0, y + 1), (x, z + 1), (x1, x + 1)])\n    assert sorted(e) == sorted([x1 + exp(x1/x0) + cos(x0), z - 2])\n"], "sample_600": ["def test_CFMaskCoder():\n    data = np.array([1, 2, np.nan, 4])\n    fill_value = -9999\n    variable = xr.Variable(dims=\"x\", data=data, attrs={\"_FillValue\": fill_value})\n\n    coder = variables.CFMaskCoder()\n    encoded_variable = coder.encode(variable)\n    decoded_variable = coder.decode(encoded_variable)\n\n    assert_equal(decoded_variable.data, np.array([1, 2, fill_value, 4]))\n", "def test_CFScaleOffsetCoder_encode_decode():\n    variable = xr.DataArray([1, 2, 3], attrs={'scale_factor': 2, 'add_offset': 1}, dims='x')\n    coder = variables.CFScaleOffsetCoder()\n    encoded = coder.encode(variable)\n    decoded = coder.decode(encoded)\n    assert_allclose(decoded.data, variable.data)\n", "def test_cfmaskcoder():\n    data = np.array([1, 2, np.nan, 4], dtype=np.float32)\n    attrs = {\"_FillValue\": np.nan}\n    var = xr.Variable([\"x\"], data, attrs=attrs)\n\n    coder = variables.CFMaskCoder()\n    encoded_var = coder.encode(var)\n    decoded_var = coder.decode(encoded_var)\n\n    assert_equal(var, decoded_var)\n", "def test_CFMaskCoder():\n    data = np.array([1, 2, np.nan, 4])\n    attrs = {}\n    encoding = {'_FillValue': np.nan}\n    var = xr.Variable(['x'], data, attrs, encoding)\n\n    coder = variables.CFMaskCoder()\n    encoded_var = coder.encode(var)\n    assert_equal(encoded_var.data, np.array([1, 2, np.nan, 4]))\n\n    decoded_var = coder.decode(encoded_var)\n    assert_equal(decoded_var.data, np.array([1, 2, np.nan, 4]))\n", "def test_cf_mask_coder():\n    # Test encoding and decoding with a simple case\n    data = np.array([1, 2, np.nan, 4])\n    attrs = {\"missing_value\": np.nan}\n    variable = xr.Variable((\"x\",), data, attrs=attrs)\n\n    coder = variables.CFMaskCoder()\n    encoded_variable = coder.encode(variable)\n    decoded_variable = coder.decode(encoded_variable)\n\n    assert_equal(decoded_variable, variable)\n\n    # Test encoding and decoding with a variable having both _FillValue and missing_value\n    data = np.array([1, 2, 3, 4])\n    attrs = {\"_FillValue\": 3, \"missing_value\": 4}\n    variable = xr.Variable((\"x\",), data, attrs=attrs)\n\n    with pytest.raises(ValueError):\n        encoded_variable = coder.encode(variable)\n"], "sample_1031": ["def test_gravitational_constant():\n    assert gravitational_constant.dimension == Dimension(length**3 * mass**-1 * time**-2)\n    assert gravitational_constant.scale_factor == S(6.67408e-11) * m**3 / (kg * s**2)\n", "def test_planck_units():\n    from sympy.physics.units.definitions import (\n        planck_mass, planck_time, planck_temperature, planck_length, planck_charge,\n        planck_area, planck_volume, planck_momentum, planck_energy, planck_force,\n        planck_power, planck_density, planck_energy_density, planck_intensity,\n        planck_angular_frequency, planck_pressure, planck_current, planck_voltage,\n        planck_impedance, planck_acceleration\n    )\n    from sympy.physics.units.definitions import (\n        hbar, speed_of_light, G, boltzmann, electric_constant\n    )\n    from sympy.core.numbers import pi\n    from sympy.core.core import sqrt\n\n    assert planck_mass == sqrt(hbar*speed_of_light/G)\n    assert planck_time == sqrt(hbar*G/speed_of_light**5)\n    assert planck_temperature == sqrt(hbar*speed_of_light**5/G/boltzmann**2)\n    assert planck_length == sqrt(hbar*G/speed_of_light**3)\n    assert planck_charge == sqrt(4*pi*electric_constant*hbar*speed_of_light)\n\n    assert planck_area == planck_length**2\n    assert planck_volume == planck_length**3\n    assert planck_momentum == planck_mass * speed_of_light\n    assert planck_energy == planck_mass * speed_of_light**2\n    assert planck_force == planck_energy / planck_length\n    assert planck_power == planck_energy / planck_time\n    assert planck_density == planck_mass / planck_length**3\n    assert planck_energy_density == planck_energy / planck_length**3\n    assert planck_intensity == planck_energy_density * speed_of_light\n    assert planck_angular_frequency", "def test_units_and_constants():\n    assert c.dimension == velocity\n    assert kg.dimension == mass\n    assert m.dimension == length\n    assert s.dimension == time\n    assert hbar.dimension == action\n    assert planck.dimension == action\n    assert electronvolt.dimension == energy\n    assert avogadro_number.dimension == S.One\n    assert boltzmann_constant.dimension == energy/temperature\n    assert stefan_boltzmann_constant.dimension == energy*time**-1*length**-2*temperature**-4\n    assert atomic_mass_unit.dimension == mass\n    assert molar_gas_constant.dimension == energy/(temperature * amount_of_substance)\n    assert faraday_constant.dimension == charge/amount_of_substance\n    assert josephson_constant.dimension == frequency/voltage\n    assert von_klitzing_constant.dimension == voltage/current\n    assert acceleration_due_to_gravity.dimension == acceleration\n    assert magnetic_constant.dimension == force/current**2\n    assert vacuum_permittivity.dimension == capacitance/length\n    assert vacuum_impedance.dimension == impedance\n    assert coulomb_constant.dimension == force*length**2/charge**2\n    assert atmosphere.dimension == pressure\n    assert kilopascal.dimension == pressure\n    assert bar.dimension == pressure\n    assert pound.dimension == mass\n    assert psi.dimension == pressure\n    assert mmHg.dimension == pressure\n    assert milli_mass_unit.dimension == mass\n    assert quart.dimension == length**3\n    assert lightyear.dimension == length\n    assert astronomical_unit.dimension == length\n    assert planck_mass.dimension == mass\n    assert planck_time.dimension == time\n    assert planck_temperature.dimension == temperature\n    assert planck_length.dimension == length\n    assert planck_charge.dimension == charge\n    assert planck_area.dimension == length**2\n    assert planck_volume.dimension", "def test_dimensional_expr():\n    # Test the dimensional_expr function with various quantities\n    assert Quantity.dimensional_expr(m) == length\n    assert Quantity.dimensional_expr(kg) == mass\n    assert Quantity.dimensional_expr(s) == time\n    assert Quantity.dimensional_expr(c) == velocity\n    assert Quantity.dimensional_expr(Quantity(\"energy\")) == mass*length**2/time**2\n", "def test_unit_conversion():\n    # Test unit conversion from meters to kilometers\n    assert Quantity(5000, m).convert_to(km) == Quantity(5, km)\n    # Test unit conversion from kilometers to meters\n    assert Quantity(5, km).convert_to(m) == Quantity(5000, m)\n"], "sample_764": ["def test_column_transformer_with_callable_column_specification():\n    ct = ColumnTransformer(\n        [(\"trans1\", Trans(), lambda X: [0, 1]),\n         (\"trans2\", DoubleTrans(), lambda X: [2, 3])])\n    X = np.array([[0., 1., 2., 2.],\n                  [1., 1., 0., 1.]])\n    result = ct.fit_transform(X)\n    expected = np.array([[0., 2., 4., 8.],\n                         [1., 2., 0., 2.]])\n    assert_array_equal(result, expected)\n", "def test_column_transformer_with_sparse_threshold():\n    # Create a ColumnTransformer with a SparseMatrixTrans that returns a sparse matrix\n    transformer = ColumnTransformer(\n        transformers=[\n            (\"sparse\", SparseMatrixTrans(), [0])\n        ],\n        sparse_threshold=0.5\n    )\n\n    # Fit and transform the data\n    X = np.array([[1], [2], [3]])\n    X_transformed = transformer.fit_transform(X)\n\n    # Check that the output is sparse because the density is lower than the threshold\n    assert sparse.issparse(X_transformed)\n\n    # Create a ColumnTransformer with a Trans that returns a dense matrix\n    transformer = ColumnTransformer(\n        transformers=[\n            (\"dense\", Trans(), [0])\n        ],\n        sparse_threshold=0.5\n    )\n\n    # Fit and transform the data\n    X_transformed = transformer.fit_transform(X)\n\n    # Check that the output is dense because the density is not lower than the threshold\n    assert not sparse.issparse(X_transformed)\n", "def test_remainder_transform(remainder):\n    X = np.array([[0., 1., 2., 2.], [1., 1., 0., 1.]])\n    ct = ColumnTransformer(\n        [(\"norm1\", Normalizer(norm='l1'), [0, 1])],\n        remainder=remainder)\n    ct.fit(X)\n    if remainder == 'drop':\n        expected = np.array([[0., 1.], [0.5, 0.5]])\n    elif remainder == 'passthrough':\n        expected = np.array([[0., 1., 2., 2.], [0.5, 0.5, 0., 1.]])\n    assert_array_equal(ct.transform(X), expected)\n", "def test_column_transformer_with_sparse_matrix_transformer():\n    X = np.array([[0., 1.], [2., 3.]])\n    ct = ColumnTransformer(\n        [(\"sparse_trans\", SparseMatrixTrans(), slice(0, 2))])\n    X_transformed = ct.fit_transform(X)\n    assert sparse.issparse(X_transformed)\n    assert_array_equal(X_transformed.toarray(), sparse.eye(2).toarray())\n", "def test_column_transformer_with_sparse_output():\n    sparse_transformer = SparseMatrixTrans()\n    column_transformer = make_column_transformer(\n        (sparse_transformer, [0, 1]),\n        sparse_threshold=1.0\n    )\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    transformed_X = column_transformer.fit_transform(X)\n    assert sparse.issparse(transformed_X)\n    assert transformed_X.shape == (3, 3)\n"], "sample_836": ["def test_check_classification_targets(y):\n    # This should not raise an exception\n    check_classification_targets(y)\n", "def test_check_classification_targets():\n    # Test with valid input types\n    check_classification_targets([0, 1])  # binary\n    check_classification_targets([0, 1, 2])  # multiclass\n    check_classification_targets(np.array([[0, 1], [1, 0]]))  # multilabel-indicator\n    check_classification_targets(np.array([[0], [1]]))  # multiclass-multioutput\n    check_classification_targets([[0, 1], [1, 0]])  # multilabel-sequences\n\n    # Test with invalid input type\n    with pytest.raises(ValueError):\n        check_classification_targets([0.1, 0.6])  # continuous\n", "def test_ovr_decision_function():\n    predictions = np.array([[0, 1, 1], [0, 0, 1]])\n    confidences = np.array([[0.5, 0.6, 0.7], [0.4, 0.3, 0.8]])\n    n_classes = 3\n\n    result = _ovr_decision_function(predictions, confidences, n_classes)\n    expected_result = np.array([[0.33333333, 0.66666667, 1.33333333],\n                               [1.33333333, -0.33333333, 0.33333333]])\n\n    assert_array_almost_equal(result, expected_result)\n", "def test_check_classification_targets():\n    # Test case with binary target\n    y = [0, 1, 0, 1]\n    check_classification_targets(y)  # should not raise any error\n\n    # Test case with multiclass target\n    y = [1, 2, 3, 1, 2]\n    check_classification_targets(y)  # should not raise any error\n\n    # Test case with multiclass-multioutput target\n    y = np.array([[1, 2], [3, 1], [2, 3]])\n    check_classification_targets(y)  # should not raise any error\n\n    # Test case with multilabel-indicator target\n    y = np.array([[0, 1, 0], [1, 0, 1], [0, 0, 1]])\n    check_classification_targets(y)  # should not raise any error\n\n    # Test case with multilabel-sequences target (should raise ValueError)\n    y = MULTILABEL_SEQUENCES[0]\n    with pytest.raises(ValueError):\n        check_classification_targets(y)\n\n    # Test case with unknown target type (should raise ValueError)\n    y = EXAMPLES['unknown'][0]\n    with pytest.raises(ValueError):\n        check_classification_targets(y)\n", "def test_check_classification_targets():\n    # Test with binary target\n    y_binary = [0, 1, 0, 1]\n    check_classification_targets(y_binary)  # should not raise an error\n\n    # Test with multiclass target\n    y_multiclass = [0, 1, 2, 1, 0]\n    check_classification_targets(y_multiclass)  # should not raise an error\n\n    # Test with multilabel-indicator target\n    y_multilabel = [[0, 1, 0], [1, 0, 1]]\n    check_classification_targets(y_multilabel)  # should not raise an error\n\n    # Test with continuous target, should raise an error\n    y_continuous = [0.1, 0.6, 0.2, 0.9]\n    with pytest.raises(ValueError):\n        check_classification_targets(y_continuous)\n\n    # Test with unknown target type, should raise an error\n    y_unknown = [[[1, 2]]]\n    with pytest.raises(ValueError):\n        check_classification_targets(y_unknown)\n"], "sample_560": ["def test_legend_set_loc_outside():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3], [1, 2, 3], label='Test')\n    leg = ax.legend(loc='outside right upper')\n    assert leg._outside_loc == 'right'\n", "def test_legend_with_empty_labels():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3], [4, 5, 6], label='_nolegend_')\n    ax.plot([1, 2, 3], [4, 6, 5], label='line2')\n    handles, labels = ax.get_legend_handles_labels()\n    assert len(handles) == 1\n    assert len(labels) == 1\n    assert labels[0] == 'line2'\n", "def test_legend_with_draggable_option():\n    fig, ax = plt.subplots()\n    line1, = ax.plot([1, 2, 3], label='Line 1')\n    line2, = ax.plot([3, 2, 1], label='Line 2')\n    legend = ax.legend(draggable=True)\n    assert isinstance(legend._draggable, mlegend.DraggableLegend)\n", "def test_legend_set_loc_with_outside_option():\n    fig, ax = plt.subplots()\n    handles = [plt.Line2D([0], [0], color='r', label='red line')]\n    legend = ax.legend(handles, loc='outside right upper')\n    assert legend._outside_loc == 'right'\n    fig, ax = plt.subplots()\n    legend = ax.legend(handles, loc='outside left lower')\n    assert legend._outside_loc == 'left'\n", "def test_legend_set_loc():\n    fig, ax = plt.subplots()\n    line, = ax.plot([1, 2, 3], [1, 2, 3])\n    legend = ax.legend(handles=[line], labels=['Test line'], loc='upper right')\n    assert legend._loc == 1\n\n    legend.set_loc('lower left')\n    assert legend._loc == 3\n\n    legend.set_loc((0.5, 0.5))\n    assert legend._loc == (0.5, 0.5)\n\n    with pytest.raises(ValueError, match=\"loc must be string, coordinate tuple, or an integer 0-10, not 'invalid'\"):\n        legend.set_loc('invalid')\n\n    with pytest.raises(ValueError, match=\"loc must be string, coordinate tuple, or an integer 0-10, not 11\"):\n        legend.set_loc(11)\n\n    with pytest.raises(ValueError, match=\"loc must be string, coordinate tuple, or an integer 0-10, not \\\\(0.5, 1.5\\\\)\"):\n        legend.set_loc((0.5, 1.5))\n"], "sample_685": ["def test_log_capture_fixture(caplog):\n    logger.info(\"Test message\")\n    sublogger.debug(\"Sublogger message\")\n\n    assert len(caplog.records) == 2\n    assert caplog.records[0].message == \"Test message\"\n    assert caplog.records[1].message == \"Sublogger message\"\n", "def test_sublogger_records(caplog):\n    caplog.clear()\n    sublogger.info(\"sublogger info message\")\n    assert len(caplog.get_records(\"call\")) == 1\n    record = caplog.get_records(\"call\")[0]\n    assert record.name == __name__ + \".baz\"\n    assert record.levelno == logging.INFO\n    assert record.getMessage() == \"sublogger info message\"\n", "def test_sublogger_records(testdir: Testdir) -> None:\n    \"\"\"Test that subloggers correctly record messages.\"\"\"\n\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n        sublogger = logging.getLogger(__name__ + \".baz\")\n\n            logger.info(\"Info message\")\n            sublogger.warning(\"Warning message\")\n            assert len(caplog.records) == 2\n            assert caplog.records[0].name == __name__\n            assert caplog.records[0].levelno == logging.INFO\n            assert caplog.records[1].name == __name__ + \".baz\"\n            assert caplog.records[1].levelno == logging.WARNING\n        \"\"\"\n    )\n\n    result = testdir.runpytest()\n    result.assert_outcomes(passed=1)\n", "def test_logging_context_manager(caplog):\n    with caplog.at_level(logging.INFO, __name__):\n        logger.info(\"This should be captured\")\n        logger.debug(\"This should not be captured\")\n        sublogger.info(\"This should also be captured\")\n\n    assert len(caplog.get_records(\"setup\")) == 2\n    assert len(caplog.get_records(\"call\")) == 0\n    assert len(caplog.get_records(\"teardown\")) == 0\n    assert \"This should be captured\" in caplog.text\n    assert \"This should not be captured\" not in caplog.text\n    assert \"This should also be captured\" in caplog.text\n", "def test_logging_plugin_capture(pytester):\n    pytester.makepyfile(\n        test_logging_capture=\"\"\"\n            import logging\n\n                logger.info(\"Info message\")\n                sublogger.warning(\"Warning message\")\n                assert len(caplog.records) == 2\n                assert caplog.record_tuples == [\n                    (\"__main__.test_logging_capture\", logging.INFO, \"Info message\"),\n                    (\"__main__.test_logging_capture.baz\", logging.WARNING, \"Warning message\"),\n                ]\n        \"\"\"\n    )\n\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines([\"*::test_logger_capture PASSED*\"])\n    assert result.ret == 0\n"], "sample_843": ["def test_kernel_repr(kernel):\n    repr(kernel)\n", "def test_kernel_diagonal(kernel):\n    K_diag = kernel.diag(X)\n    K = kernel(X)\n    assert_array_almost_equal(np.diag(K), K_diag)\n", "def test_kernel_diagonal_consistency(kernel):\n    K = kernel(X)\n    K_diag = kernel.diag(X)\n    assert_array_almost_equal(np.diag(K), K_diag)\n", "def test_clone_with_theta(kernel):\n    theta = kernel.theta\n    new_kernel = kernel.clone_with_theta(theta)\n    assert_array_equal(new_kernel.theta, theta)\n", "def test_kernel_repr(kernel):\n    \"\"\"Test that the kernel repr is a valid Python expression.\"\"\"\n    kernel_repr = repr(kernel)\n    eval_kernel = eval(kernel_repr)\n    assert repr(kernel) == repr(eval_kernel)\n"], "sample_1158": ["def test_kernS_with_add_and_mul():\n    assert kernS('2*(x + y)') == 2*(x + y)\n    assert kernS('-(x + 1)') == -(x + 1)\n", "def test_sympify_numpy_types():\n    a = numpy.int64(5)\n    assert sympify(a) == Integer(5)\n\n    a = numpy.float64(3.14)\n    assert sympify(a) == Float(3.14)\n\n    a = numpy.complex128(1 + 2j)\n    assert sympify(a) == 1 + 2*I\n", "def test_kernS():\n    # Test kernS function\n    assert kernS('2*(x + y)') == 2*(x + y)\n    assert kernS('-(x + 1)') == -(x + 1)\n    assert kernS('x**2 + 2*x') == x**2 + 2*x\n    assert kernS('x**2 - -2*x') == x**2 + 2*x\n", "def test_kernS():\n    assert kernS('2*(x + y)') == 2*(x + y)\n    assert kernS('-(x + 1)') == -(x + 1)\n", "def test_kernS():\n    assert kernS('2*(x + y)**2 - 3*(x - y)') == 2*(x + y)**2 - 3*(x - y)\n    assert kernS('-(x + 1)**2') == -(x + 1)**2\n    assert kernS('-2*(x + y)**2 + 3*(x - y)') == -2*(x + y)**2 + 3*(x - y)\n"], "sample_587": ["def test_merge_data_and_coords(self):\n    data = xr.Dataset({'A': ('x', [1, 2, 3])})\n    coords = xr.Dataset({'x': ('x', [1, 2, 3])})\n    merged = merge.merge_data_and_coords(data, coords)\n    assert set(merged.keys()) == {'A', 'x'}\n    assert merged['x'].dims == ('x',)\n    assert merged['A'].dims == ('x',)\n", "def test_merge_coords(self):\n    coords1 = {'x': ('y', [1, 2, 3]), 'z': ('y', [4, 5, 6])}\n    coords2 = {'x': ('y', [1, 2, 3]), 'z': ('y', [7, 8, 9])}\n\n    with pytest.raises(merge.MergeError):\n        merge.merge_coords([coords1, coords2])\n\n    merged = merge.merge_coords([coords1, coords2], compat=\"override\")\n    expected = {'x': ('y', [1, 2, 3]), 'z': ('y', [7, 8, 9])}\n    assert merged == expected\n", "def test_merge_variables(self):\n    vars_dicts = [\n        {\"x\": xr.Variable((\"y\"), np.array([1, 2, 3]))},\n        {\"x\": xr.Variable((\"y\"), np.array([1, 2, 3]))},\n    ]\n    result = merge.merge_variables(vars_dicts)\n    assert isinstance(result, dict)\n    assert list(result.keys()) == [\"x\"]\n    xr.testing.assert_equal(result[\"x\"], vars_dicts[0][\"x\"])\n\n    vars_dicts = [\n        {\"x\": xr.Variable((\"y\"), np.array([1, 2, 3]))},\n        {\"x\": xr.Variable((\"y\"), np.array([4, 5, 6]))},\n    ]\n    with pytest.raises(merge.MergeError):\n        merge.merge_variables(vars_dicts)\n\n    vars_dicts = [\n        {\"x\": xr.Variable((\"y\"), np.array([1, 2, 3]))},\n        {\"x\": xr.Variable((\"y\"), np.array([4, 5, 6]))},\n    ]\n    result = merge.merge_variables(vars_dicts, compat=\"no_conflicts\")\n    xr.testing.assert_equal(result[\"x\"], xr.Variable((\"y\"), np.array([1, 2, 3])))\n", "def test_merge_pandas_values(self):\n    objects = [\n        {\"a\": np.arange(3), \"b\": np.array([1, 2, 3])},\n        {\"a\": pd.Series([4, 5, 6]), \"b\": pd.Series([7, 8, 9])},\n    ]\n    merged = merge(objects)\n    expected = xr.Dataset(\n        {\"a\": ((\"dim_0\",), np.array([4, 5, 6])), \"b\": ((\"dim_0\",), np.array([7, 8, 9]))}\n    )\n    xr.testing.assert_equal(merged, expected)\n", "def test_merge_internals_coerce_pandas_values(self):\n    import pandas as pd\n    objs = [\n        {\"a\": pd.Series([1, 2, 3]), \"b\": pd.DataFrame({\"c\": [4, 5, 6], \"d\": [7, 8, 9]})},\n        {\"e\": pd.Series([10, 11, 12])},\n    ]\n    coerced = merge.coerce_pandas_values(objs)\n    assert isinstance(coerced[0][\"a\"], xr.DataArray)\n    assert isinstance(coerced[0][\"b\"], xr.DataArray)\n    assert isinstance(coerced[1][\"e\"], xr.DataArray)\n"], "sample_970": ["def test_is_builtin_class_method():\n    class CustomClass:\n            pass\n\n    assert inspect.is_builtin_class_method(CustomClass, 'custom_method') is False\n    assert inspect.is_builtin_class_method(int, '__init__') is True\n", "def test_signature_from_str():\n    sig = inspect.signature_from_str('(a: int, b: str = \"default\", *args, **kwargs) -> None')\n    assert str(sig) == '(a: int, b: str = \\'default\\', *args, **kwargs)'\n    assert sig.parameters['a'].annotation == int\n    assert sig.parameters['b'].default == 'default'\n    assert sig.parameters['args'].kind == Parameter.VAR_POSITIONAL\n    assert sig.parameters['kwargs'].kind == Parameter.VAR_KEYWORD\n    assert sig.return_annotation == None\n", "def test_is_singledispatch_method():\n    class Example:\n        @functools.singledispatchmethod\n            pass\n\n        @dispatch_method.register(int)\n            pass\n\n    assert inspect.is_singledispatch_method(Example.dispatch_method)\n    assert not inspect.is_singledispatch_method(Example().dispatch_method)\n", "def test_signature_from_ast():\n    # Test signature_from_ast function with a simple AST node\n    code = \"def func(a: int, b: str = 'default') -> bool:\\n    pass\"\n    node = ast.parse(code).body[0]\n    sig = inspect.signature_from_ast(node)\n    assert str(sig) == \"(a: int, b: str = 'default') -> bool\"\n", "def test_stringify_signature_unqualified_typehints():\n    sig = inspect.signature(datetime.date.fromisoformat, follow_wrapped=False)\n    assert stringify_signature(sig, unqualified_typehints=True) == \"(date_string: str) -> datetime.date\"\n"], "sample_150": ["def test_database_checks_with_tags(self, mock_check):\n    mock_check.return_value = []\n    self.assertIsNone(self.command.check(tags=['database']))\n    mock_check.assert_called_once_with(databases=None, **{'tags': ['database']})\n", "def test_check_migrations(self):\n    from django.core.management.base import BaseCommand, CommandError\n    from unittest.mock import patch\n\n    class MigrationCommand(BaseCommand):\n        requires_migrations_checks = True\n\n            pass\n\n    command = MigrationCommand()\n    with patch('django.db.migrations.executor.MigrationExecutor.migration_plan', return_value=[('mock_migration', False)]):\n        with patch('django.core.management.base.BaseCommand.stdout') as mock_stdout:\n            command.execute()\n            mock_stdout.write.assert_called_once()\n", "def test_check_database_backends(self, mock_check):\n    mock_check.return_value = []\n    # Call the check_database_backends function\n    issues = check_database_backends(databases=['default', 'other'])\n    # Assert that the check function was called for each database\n    mock_check.assert_has_calls([mock.call(databases=['default']), mock.call(databases=['other'])])\n    # Assert that the function returned an empty list of issues\n    self.assertEqual(issues, [])\n", "def test_check_database_backends(self, mock_check):\n    errors = [mock.Mock(id='database.E001')]\n    mock_check.return_value = errors\n\n    with self.settings(DATABASES={\n        'default': {\n            'ENGINE': 'django.db.backends.dummy',\n        },\n        'other': {\n            'ENGINE': 'django.db.backends.dummy',\n        },\n    }):\n        issues = check_database_backends(connection.databases.values())\n\n    self.assertEqual(len(issues), 2)\n    self.assertIn(errors[0], issues)\n    self.assertIn(errors[0].id, [issue.id for issue in issues])\n", "def test_database_check_with_warnings(self, mock_check):\n    mock_check.return_value = [\n        check_database_backends.DatabaseCheckMessage(\n            id='database.E001',\n            level=check_database_backends.WARNING,\n            msg=\"This is a warning.\",\n            hint=None,\n            obj=connection,\n        ),\n    ]\n\n    with self.assertNoExceptionRaised():\n        with mock.patch('sys.stdout', new_callable=mock.StringIO) as mock_stdout:\n            self.command.check(databases=['default'])\n            self.assertIn('WARNINGS:\\nThis is a warning.', mock_stdout.getvalue())\n"], "sample_972": ["def test_restify():\n    assert restify(None) == ':py:obj:`None`'\n    assert restify(Ellipsis) == '...'\n    assert restify(str) == ':py:class:`str`'\n    assert restify(Struct) == ':py:class:`struct.Struct`'\n    assert restify(TracebackType) == ':py:class:`types.TracebackType`'\n    assert restify(MyInt) == ':py:class:`%s.MyInt`' % __name__\n    assert restify(MyList[T]) == ':py:class:`%s.MyList`\\\\ [:py:obj:`~typing.T`]' % __name__\n    assert restify(BrokenType) == 'BrokenType'\n    assert restify(Callable[[int], str]) == ':py:obj:`~typing.Callable`\\\\ [[:py:class:`int`], :py:class:`str`]'\n", "def test_restify():\n    assert restify(MyClass1) == ':py:class:`~sphinx.tests.test_util_typing.MyClass1`'\n    assert restify(MyClass2) == ':py:class:`<MyClass2>`'\n    assert restify(MyInt) == ':py:class:`~sphinx.tests.test_util_typing.MyInt`'\n    assert restify(MyList[int]) == ':py:class:`~sphinx.tests.test_util_typing.MyList`\\\\ [:py:class:`int`]'\n    assert restify(Optional[int]) == ':py:obj:`~typing.Optional`\\\\ [:py:class:`int`]'\n    assert restify(Union[int, str]) == ':py:obj:`~typing.Union`\\\\ [:py:class:`int`, :py:class:`str`]'\n    assert restify(Union[int, None]) == ':py:obj:`~typing.Optional`\\\\ [:py:class:`int`]'\n    assert restify(Union[None, str]) == ':py:obj:`~typing.Optional`\\\\ [:py:class:`str`]'\n    assert restify(Union[int, str, None]) == ':py:obj:`~typing.Optional`\\\\ [:py:obj:`~typing.Union`\\\\ [:py:class:`int`, :py:class:`str`]]'\n    assert restify(Callable[[int], str]) == ':py:obj:`~typing.Callable`\\\\ [[ :py:class:`int` ], :py:class:`str`]'\n    assert restify(BrokenType) == 'BrokenType'\n", "def test_restify(annotation, expected):\n    assert restify(annotation) == expected\n", "def test_restify_py37():\n    assert restify(MyClass1) == ':py:class:`~sphinx.util.typing.MyClass1`'\n    assert restify(MyClass2) == ':py:class:`sphinx.util.typing.<MyClass2>`'\n    assert restify(MyInt) == ':py:class:`sphinx.util.typing.MyInt`'\n    assert restify(MyList[int]) == ':py:class:`~sphinx.util.typing.MyList`\\\\ [:py:class:`int`]'\n    assert restify(Optional[int]) == ':py:obj:`~typing.Optional`\\\\ [:py:class:`int`]'\n    assert restify(Union[int, str]) == ':py:obj:`~typing.Union`\\\\ [:py:class:`int`, :py:class:`str`]'\n    assert restify(Callable[[int], str]) == ':py:obj:`~typing.Callable`\\\\ [[ :py:class:`int`], :py:class:`str`]'\n    assert restify(Tuple[int, str]) == ':py:obj:`~typing.Tuple`\\\\ [:py:class:`int`, :py:class:`str`]'\n    assert restify(Dict[str, int]) == ':py:obj:`~typing.Dict`\\\\ [:py:class:`str`, :py:class:`int`]'\n    assert restify(BrokenType) == ':py:obj:`BrokenType`'\n", "def test_restify(annotation, expected):\n    assert restify(annotation) == expected\n"], "sample_1105": ["def test_merge_explicit():\n    B = Matrix([[1, 1], [1, 1]])\n    C = Matrix([[1, 2], [3, 4]])\n    X = MatMul(A, B, C)\n    assert merge_explicit(X) == MatMul(A, Matrix([[4, 6], [4, 6]]))\n", "def test_matmul_with_number():\n    A = MatrixSymbol('A', 2, 2)\n    x = Symbol('x')\n    result = MatMul(x, A)\n    assert result == x * A\n", "def test_matmul_doit():\n    A = MatrixSymbol('A', 2, 3)\n    B = MatrixSymbol('B', 3, 2)\n    X = MatMul(A, B, evaluate=True)\n    assert X.shape == (2, 2)\n    assert isinstance(X, ImmutableMatrix)\n", "def test_matmul_transpose():\n    expr = MatMul(A, B)\n    transposed_expr = expr.transpose()\n    assert transposed_expr == MatMul(B.transpose(), A.transpose())\n", "def test_matmul_adjoint():\n    adj_AB = MatMul(A, B)._eval_adjoint()\n    expected_adj_AB = MatMul(B.T, A.T)\n    assert adj_AB == expected_adj_AB\n"], "sample_916": ["compilation error", "def test_function_pointer_declaration():\n    input = \"void (*func)(int, double);\"\n    idDict = {1: \"func\", 2: \"func\"}\n    output = \"void (*func)(int, double)\"\n    check(\"function\", input, idDict, output)\n", "def test_function_parsing():\n    input = \"void func(int a, char b, double c)\"\n    idDict = {1: \"func\"}\n    output = \"void func(int a, char b, double c)\"\n    check(\"function\", input, idDict, output)\n", "def test_enum_declaration():\n    input = \"enum ExampleEnum { A = 0, B = 1, C = 2 };\"\n    idDict = {1: \"ExampleEnum\"}\n    output = \"enum ExampleEnum { A = 0, B = 1, C = 2 };\"\n    check(\"enum\", input, idDict, output)\n", "def test_parse_function_pointer():\n    input = \"int (*func)(int, char)\"\n    idDict = {1: \"func\"}\n    output = \"int ( * func)(int, char)\"\n    check(\"function\", input, idDict, output)\n"], "sample_320": ["    def test_alter_model_managers(self):\n        new_managers = [(\"custom_objects\", FoodManager())]\n        operation = migrations.AlterModelManagers(\"Food\", new_managers)\n\n        # Forward\n        project_state = ProjectState()\n        project_state.add_model(ModelState(\"app\", \"Food\", [], {}, (models.Model,), []))\n        new_state = project_state.clone()\n        operation.state_forwards(\"app\", new_state)\n        self.assertEqual(new_state.models[\"app\", \"food\"].managers, new_managers)\n\n        # Backward\n        operation.state_forwards(\"app\", project_state)\n        self.assertEqual(project_state.models[\"app\", \"food\"].managers, [])\n", "    def test_create_model_with_field_operations(self):\n        # Create a new model with an initial field\n        operation = migrations.CreateModel(\n            \"TestModel\",\n            [\n                (\"id\", models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name=\"ID\")),\n                (\"name\", models.CharField(max_length=100)),\n            ],\n        )\n        new_state = self.apply_operation(operation)\n        self.assertIn((\"test\", \"testmodel\"), new_state.models)\n        self.assertEqual(len(new_state.models[\"test\", \"testmodel\"].fields), 2)\n\n        # Add a field to the model\n        operation = migrations.AddField(\"TestModel\", \"description\", models.TextField(default=\"\"))\n        new_state = self.apply_operation(operation)\n        self.assertEqual(len(new_state.models[\"test\", \"testmodel\"].fields), 3)\n\n        # Alter the field in the model\n        operation = migrations.AlterField(\"TestModel\", \"name\", models.CharField(max_length=200))\n        new_state = self.apply_operation(operation)\n        self.assertEqual(new_state.models[\"test\", \"testmodel\"].fields[1][1].max_length, 200)\n\n        # Remove a field from the model\n        operation = migrations.RemoveField(\"TestModel\", \"description\")\n        new_state = self.apply_operation(operation)\n        self.assertEqual(len(new_state.models[\"test\", \"testmodel\"].fields), 2)\n\n        # Rename a field in the model\n        operation = migrations.RenameField(\"TestModel\", \"name\", \"new_name\")\n        new_state = self.apply_operation(operation)\n        self.assertEqual(new_state.models[\"test\", \"testmodel\"].fields[1][0], \"new_name\")\n\n        # Test reducing operations\n        reduced_operations = migrations.CreateModel(\"TestModel\", []).reduce(\n            migrations.AlterModelOptions(\"TestModel\", {\"managed\": True}), \"test\"\n       ", "    def test_alter_model_managers(self):\n        # Initial state\n        state = ProjectState()\n        state.add_model(\n            ModelState(\n                \"app\",\n                \"Model\",\n                [],\n                options={},\n                bases=(models.Model,),\n                managers=[(\"objects\", models.Manager())],\n            )\n        )\n\n        # Operation\n        operation = AlterModelManagers(\n            name=\"Model\", managers=[(\"custom_manager\", FoodManager())]\n        )\n\n        # Forward operation\n        new_state = state.clone()\n        operation.state_forwards(\"app\", new_state)\n        self.assertEqual(\n            new_state.apps.get_model(\"app\", \"Model\")._meta.managers_map,\n            {\"objects\": models.Manager(), \"custom_manager\": FoodManager()},\n        )\n\n        # Backward operation\n        back_state = new_state.clone()\n        operation.state_backwards(\"app\", back_state)\n        self.assertEqual(\n            back_state.apps.get_model(\"app\", \"Model\")._meta.managers_map,\n            {\"objects\": models.Manager()},\n        )\n\n        # No database changes expected\n        with self.assertNoDatabaseChanges():\n            with connection.schema_editor() as schema_editor:\n                operation.database_forwards(\"app\", schema_editor, state, new_state)\n                operation.database_backwards(\"app\", schema_editor, new_state, state)\n", "    def test_create_model(self):\n        operation = migrations.CreateModel(\n            name='TestModel',\n            fields=[\n                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                ('name', models.CharField(max_length=100)),\n            ],\n        )\n\n        new_state = self.apply_operation(operation)\n\n        self.assertIn(('app', 'testmodel'), new_state.models)\n        self.assertEqual(new_state.models[('app', 'testmodel')].name, 'TestModel')\n        self.assertEqual(len(new_state.models[('app', 'testmodel')].fields), 2)\n\n        with self.temporary_migration_connection():\n            with transaction.atomic(using=self.connection):\n                operation.database_forwards(self.app_label, self.schema_editor, self.from_state, new_state)\n\n            self.assertTableExists('app_testmodel')\n\n            operation.database_backwards(self.app_label, self.schema_editor, self.from_state, new_state)\n\n            self.assertTableNotExists('app_testmodel')\n", "    def test_create_model_with_custom_manager(self):\n        operation = migrations.CreateModel(\n            name=\"CustomManagerModel\",\n            fields=[(\"name\", models.CharField(max_length=255))],\n            managers=[(\"objects\", FoodManager())],\n        )\n        new_state = operation.state_forwards(\"app\", ProjectState())\n        self.assertEqual(\n            new_state.models[\"app\", \"custommanagermodel\"].managers,\n            [(\"objects\", FoodManager())],\n        )\n\n        # Test database operation\n        with self.assertPreservesTableCommentAndNull(\n            \"app_custommanagermodel\", \"CustomManagerModel table\"\n        ), self.assertPreservesTableConstraints(\n            \"app_custommanagermodel\", [\"name\"]\n        ), self.assertPreservesTableIndexes(\n            \"app_custommanagermodel\", []\n        ), self.assertPreservesDefault(\n            \"app_custommanagermodel\", \"name\", \"\"\n        ), self.assertPreservesUnique(\n            \"app_custommanagermodel\", []\n        ):\n            operation.database_forwards(\n                \"app\", self.schema_editor(), ProjectState(), new_state\n            )\n            self.assertEqual(\n                new_state.apps.get_model(\"app\", \"CustomManagerModel\").objects.__class__,\n                FoodManager,\n            )\n            operation.database_backwards(\n                \"app\", self.schema_editor(), new_state, ProjectState()\n            )\n"], "sample_1157": ["def test_convert_xor():\n    result = convert_xor([(OP, '^')], {}, {})\n    assert result == [(OP, '**')]\n", "def test_function_exponentiation():\n    transformations = standard_transformations + (function_exponentiation,)\n    assert parse_expr('sin**4(x)', transformations=transformations) == sin(x)**4\n", "def test_convert_xor():\n    result = parse_expr(\"1 ^ 2\", transformations=(convert_xor,))\n    assert result == Pow(1, 2)\n", "def test_function_exponentiation():\n    expr = parse_expr(\"sin**4(x)\", transformations=(standard_transformations + (function_exponentiation,)))\n    assert expr == sin(x)**4\n", "def test_parse_expr_with_explicit_multiplication():\n    s = \"2 * x\"\n    result = parse_expr(s, transformations=(standard_transformations + (implicit_multiplication,)))\n    assert result == Mul(Integer(2), Symbol('x'))\n"], "sample_947": ["def test_function_with_varargs():\n    check('function',\n          'void func(int a, ...);',\n          {1: 'func', 2: 'func'},\n          'void func(int a, ...)')\n", "def test_parse_function_pointer():\n    input_str = \"int (*f)(int arg)\"\n    id_dict = {1: \"c.f\", 2: \"c.f\", 3: \"c.f\"}\n    output_str = \"int (*{key}f)(int arg)\"\n    check(\"function\", input_str, id_dict, output_str)\n", "def test_member_pointers():\n    input = \"int *p1, *p2, *p3;\"\n    idDict = {1: 'p1', 2: 'c.p1', 3: 'cpp:p1'}\n    output = \"int * p1, * p2, * p3\"\n    check('member', input, idDict, output)\n", "def test_parse_expression_in_declarator():\n    check('member',\n          'int (*x)(int (*y)(int, int), int z);',\n          {1: 'm.0.x', 2: 'm.x', 3: 'm.x'},\n          'int ( * x )( int ( * y )( int, int ) z )')\n", "def test_parse_function_pointer_type():\n    input = \"int (*func)(int)\"\n    idDict = {1: \"id-func\"}\n    output = \"int (*func)(int)\"\n    asTextOutput = \"<em>int</em> (<span class=\\\"paren_attr\\\">*</span><span id=\\\"id-func\\\">func</span>)<span class=\\\"paren_attr\\\">(</span><em>int</em><span class=\\\"paren_attr\\\">)</span>\"\n    check('function', input, idDict, output, asTextOutput=asTextOutput)\n"], "sample_874": ["def test_step_selector_transform():\n    selector = StepSelector(step=2)\n    selector.fit(X, y)\n    X_transformed = selector.transform(X)\n    assert_array_equal(X_transformed, Xt)\n\n    selector.fit(X)\n    X_transformed = selector.transform(X)\n    assert_array_equal(X_transformed, Xt)\n\n    selector.fit(X, y)\n    X_transformed = selector.inverse_transform(X_transformed)\n    assert_array_equal(X_transformed, Xinv)\n\n    selector.fit(X)\n    X_transformed = selector.inverse_transform(X_transformed)\n    assert_array_equal(X_transformed, Xinv)\n", "def test_step_selector_transform():\n    selector = StepSelector(step=2)\n    selector.fit(X)\n    assert_array_equal(selector.transform(X), Xt)\n", "def test_step_selector():\n    selector = StepSelector(step=2)\n    X_transformed = selector.fit_transform(X, y)\n    assert_array_equal(X_transformed, Xt)\n    assert selector.get_support().tolist() == support\n    assert selector.get_support(indices=True).tolist() == support_inds\n    assert selector.inverse_transform(X_transformed).tolist() == Xinv.tolist()\n    assert selector.get_feature_names_out().tolist() == feature_names_t\n    assert selector.inverse_transform(X_transformed, feature_names=feature_names).tolist() == feature_names_inv.tolist()\n", "def test_step_selector_with_feature_names():\n    selector = StepSelector(step=2)\n    selector.fit(X, y=y, feature_names=feature_names)\n    assert_array_equal(selector.get_support(), support)\n    assert_array_equal(selector.get_support(indices=True), support_inds)\n    assert_array_equal(selector.transform(X), Xt)\n    assert_array_equal(selector.inverse_transform(Xt), Xinv)\n    assert_array_equal(selector.get_feature_names_out(feature_names), feature_names_t)\n", "def test_step_selector(X, y, feature_names, expected_support, expected_support_inds, expected_Xt, expected_feature_names_t):\n    step_selector = StepSelector()\n    if feature_names is not None:\n        X = sp.csc_matrix(X) if sp.issparse(X) else X\n        X.columns = feature_names\n    if y is None:\n        Xt = step_selector.fit_transform(X)\n    else:\n        Xt = step_selector.fit_transform(X, y)\n    assert_array_equal(step_selector.get_support(), expected_support)\n    assert_array_equal(step_selector.get_support(indices=True), expected_support_inds)\n    assert_array_equal(Xt, expected_Xt)\n    if hasattr(Xt, 'columns'):\n        assert_array_equal(Xt.columns, expected_feature_names_t)\n"], "sample_1005": ["def test_latex_Quaternion():\n    q = Quaternion(1, 2, 3, 4)\n    assert latex(q) == \"1 + 2\\\\mathbf{i} + 3\\\\mathbf{j} + 4\\\\mathbf{k}\"\n", "def test_latex_Quaternion():\n    q = Quaternion(1, 2, 3, 4)\n    assert latex(q) == \"1 + 2\\\\, i + 3\\\\, j + 4\\\\, k\"\n", "def test_latex_Tr():\n    A = Matrix([[1, 2], [3, 4]])\n    assert latex(Tr(A)) == r'\\mbox{Tr}\\left(\\left[\\begin{matrix}1 & 2\\\\3 & 4\\end{matrix}\\right]\\right)'\n", "def test_Quaternion():\n    i, j, k = symbols('i j k', commutative=False)\n    q = Quaternion(1, 2, 3, 4)\n    assert latex(q) == '1 + 2 i + 3 j + 4 k'\n", "compilation error"], "sample_1153": ["def test_re():\n    x, y = symbols('x y', real=True)\n    assert re(2*E) == 2*E\n    assert re(2*I + 17) == 17\n    assert re(2*I) == 0\n    assert re(im(x) + x*I + 2) == 2\n    assert re(5 + I + 2) == 7\n    x = Symbol('x')\n    assert re(x) == x\n", "def test_conjugate_derivative():\n    x, y = symbols('x y', real=True)\n    z = conjugate(x + I*y)\n    dz_dx = z.diff(x)\n    assert dz_dx == conjugate(1)\n    dz_dy = z.diff(y)\n    assert dz_dy == -I*conjugate(1)\n", "def test_Abs_with_imaginary_number():\n    assert Abs(3*I) == 3\n", "def test_unpolarify_exponents_only():\n    x, y = symbols('x y', real=True)\n    eq = exp_polar(x * I) * y\n    res = unpolarify(eq, exponents_only=True)\n    expected = exp(x * I) * y\n    assert res == expected\n", "def test_Abs_derivative():\n    x = Symbol('x', real=True)\n    y = Symbol('y', imaginary=True)\n    assert unchanged(Abs, Abs(x)._eval_derivative(x))\n    assert Abs(x)._eval_derivative(x) == sign(x)\n    assert unchanged(Abs, Abs(y)._eval_derivative(x))\n    assert Abs(y)._eval_derivative(x) == 0\n"], "sample_924": ["def test_namespace_object():\n    parser = DefinitionParser(\"namespace ns\", location=None, config=None)\n    ast = parser.parse_namespace_object()\n    assert ast.nestedName.names[0].identOrOp.name == \"ns\"\n    assert ast.templatePrefix is None\n", "def test_nested_name():\n    check('type', 'A::B::C', {1: 'A::B::C'}, 'A::B::C', 'A::B::C')\n", "compilation error", "compilation error", "compilation error"], "sample_308": ["    def test_date_format_M(self):\n        d = date(2022, 5, 15)\n        df = dateformat.DateFormat(d)\n        self.assertEqual(df.format('M'), 'May')\n", "    def test_date_format(self):\n        dt = date(2022, 12, 31)\n        result = format(dt, 'Y-m-d')\n        self.assertEqual(result, '2022-12-31')\n\n        result = format(dt, 'D, jS F Y')\n        self.assertEqual(result, 'Sat, 31st December 2022')\n", "    def test_time_format_Z(self):\n        # Test that timezone offset in seconds is correct.\n        dt = datetime(2022, 1, 1, 0, 0, 0)\n        # Timezone is UTC+2\n        tz = get_fixed_timezone(120)\n        dt = make_aware(dt, timezone=tz)\n        tf = TimeFormat(dt)\n        self.assertEqual(tf.Z(), 7200)\n        # Timezone is UTC-5\n        tz = get_fixed_timezone(-300)\n        dt = make_aware(dt, timezone=tz)\n        tf = TimeFormat(dt)\n        self.assertEqual(tf.Z(), -18000)\n", "    def test_date_format_with_timezone(self):\n        dt = make_aware(datetime(2022, 3, 14, 12, 30, 0), timezone=get_fixed_timezone(120))\n        df = dateformat.DateFormat(dt)\n        self.assertEqual(df.format('Y-m-d\\TH:i:sP\\Z'), '2022-03-14T12:30:00PM+0200')\n", "    def test_time_format(self):\n        dt = datetime(2022, 1, 1, 15, 30, 45)\n        tf = dateformat.TimeFormat(dt)\n        self.assertEqual(tf.format('g:i A'), '3:30 PM')\n        self.assertEqual(tf.format('H:i:s'), '15:30:45')\n        self.assertEqual(tf.format('T'), 'EST')\n"], "sample_232": ["    def test_call(self):\n        key_name = 'test_key'\n        factory = KeyTransformFactory(key_name)\n        transform = factory()\n        self.assertIsInstance(transform, KeyTransform)\n        self.assertEqual(transform.key_name, key_name)\n", "    def test_key_transform_factory(self):\n        factory = KeyTransformFactory('test_key')\n        transform = factory()\n        self.assertIsInstance(transform, KeyTransform)\n        self.assertEqual(transform.key_name, 'test_key')\n", "    def test_key_transform_factory(self):\n        key_name = 'test_key'\n        factory = KeyTransformFactory(key_name)\n        transform = factory()\n        self.assertIsInstance(transform, KeyTransform)\n        self.assertEqual(transform.key_name, key_name)\n", "    def test_json_field_formfield(self):\n        field = JSONModel._meta.get_field('json_field')\n        form_field = field.formfield()\n        self.assertIsInstance(form_field, forms.JSONField)\n        self.assertEqual(form_field.encoder, field.encoder)\n        self.assertEqual(form_field.decoder, field.decoder)\n", "    def test_key_transform_iexact(self):\n        obj = JSONModel.objects.create(json_field={'key': 'Test'})\n        self.assertEqual(\n            JSONModel.objects.filter(json_field__key__iexact='test').get(),\n            obj,\n        )\n"], "sample_610": ["def test_parse_iso8601_with_reso(date_type):\n    datetime_str = \"1999-01-01T12:34:56\"\n    expected_resolution = \"second\"\n    expected_date = date_type(1999, 1, 1, 12, 34, 56)\n    result_date, result_resolution = _parse_iso8601_with_reso(date_type, datetime_str)\n    assert result_resolution == expected_resolution\n    assert result_date == expected_date\n", "def test_parse_iso8601_with_reso():\n    for date_type in _all_cftime_date_types():\n        for string, expected_dict in ISO8601_LIKE_STRING_TESTS.values():\n            date, resolution = _parse_iso8601_with_reso(date_type, string)\n            expected = date_type(**{k: int(v) for k, v in expected_dict.items() if v is not None})\n            assert date == expected\n            assert resolution == list(expected_dict.keys())[-1]\n", "def test_parsed_string_to_bounds_year(date_type=cftime.datetime):\n    parsed = date_dict(year=\"1999\")\n    start, end = _parsed_string_to_bounds(date_type, \"year\", parsed)\n    assert start == date_type(1999, 1, 1)\n    assert end == date_type(2000, 1, 1) - timedelta(microseconds=1)\n", "def test_parsed_string_to_bounds(date_type):\n    datetime_string = \"1999-01-01T12:34:56\"\n    parsed, resolution = _parse_iso8601_with_reso(date_type, datetime_string)\n    start, end = _parsed_string_to_bounds(date_type, resolution, parsed)\n\n    assert start == date_type(1999, 1, 1, 12, 34, 56)\n    assert end == date_type(1999, 1, 1, 12, 34, 57) - timedelta(microseconds=1)\n", "def test_parsed_string_to_bounds():\n    for date_type in _all_cftime_date_types():\n        parsed = parse_iso8601_like(\"1999-01-01T12:34:56\")\n        start, end = _parsed_string_to_bounds(date_type, \"second\", parsed)\n        assert start == date_type(1999, 1, 1, 12, 34, 56)\n        assert end == date_type(1999, 1, 1, 12, 34, 57) - timedelta(microseconds=1)\n"], "sample_455": ["    def setUpTestData(cls):\n        class CheckConstraintProduct(models.Model):\n            positive_integer = models.PositiveIntegerField()\n            positive_integer_check = models.CheckConstraint(\n                check=models.Q(positive_integer__gt=0), name=\"positive_integer_check\"\n            )\n", "    def test_check_constraint_validation(self):\n        # Create a product with a valid price\n        product = Product(name=\"Test Product\", price=10.0)\n        product.full_clean()\n        product.save()\n\n        # Try to create a product with an invalid price\n        with self.assertRaises(ValidationError):\n            invalid_product = Product(name=\"Invalid Product\", price=-5.0)\n            invalid_product.full_clean()\n            invalid_product.save()\n", "    def test_check_constraint_validate(self):\n        constraint = CheckConstraint(\n            check=Q(price__gte=0), name=\"positive_price\", violation_error_code=\"invalid_price\"\n        )\n        product = Product(name=\"Test Product\", price=-1)\n        with self.assertRaises(ValidationError) as context:\n            constraint.validate(Product, product)\n        self.assertEqual(context.exception.code, \"invalid_price\")\n", "    def test_validate(self):\n        constraint = CheckConstraint(check=Q(price__gte=0), name=\"positive_price\")\n        product = Product(name=\"Test\", price=-1)\n        with self.assertRaises(ValidationError) as cm:\n            constraint.validate(Product, product)\n        self.assertEqual(cm.exception.message, 'Constraint \u201cpositive_price\u201d is violated.')\n", "    def test_check_constraint_validation(self):\n        with self.assertRaises(ValidationError):\n            Product.objects.create(name=\"TEST\", price=-1)\n"], "sample_576": ["def test_plot_scales(self):\n    data = pd.DataFrame({\"x\": [1, 2, 3], \"y\": [4, 5, 6]})\n    p = Plot(data)\n\n    p.scale(x=Continuous(), y=Nominal())\n\n    assert isinstance(p._scales[\"x\"], Continuous)\n    assert isinstance(p._scales[\"y\"], Nominal)\n", "    def setup_method(self):\n        self.plotter = Plotter(pyplot=False, theme={})\n", "    def test_add_method_valid_mark_and_transforms(self):\n        plot = Plot(data=pd.DataFrame({\"x\": [1, 2, 3], \"y\": [4, 5, 6]}))\n        mark = MockMark()\n        stat = Agg(\"mean\")\n        move = Dodge()\n        new_plot = plot.add(mark, stat, move)\n        assert new_plot._layers[-1][\"mark\"] == mark\n        assert new_plot._layers[-1][\"stat\"] == stat\n        assert new_plot._layers[-1][\"move\"] == [move]\n", "def test_scale_magic_args(self):\n    p = Plot(self.mpg)\n\n    p.add(MockMark(), x=\"horsepower\", y=\"mpg\")\n\n    # Test passing a transform name\n    p.scale(x=\"log\").plot()\n    assert isinstance(p._scales[\"x\"], Continuous)\n    assert p._scales[\"x\"].transform == \"log\"\n\n    # Test passing a palette name\n    p.scale(color=\"viridis\").plot()\n    assert isinstance(p._scales[\"color\"], Nominal)\n    assert p._scales[\"color\"].palette == \"viridis\"\n\n    # Test passing a range tuple\n    p.scale(y=(0, 10)).plot()\n    assert isinstance(p._scales[\"y\"], Continuous)\n    assert p._scales[\"y\"].limits == (0, 10)\n\n    # Test passing a dict\n    p.scale(size={\"small\": 1, \"large\": 2}).plot()\n    assert isinstance(p._scales[\"size\"], Nominal)\n    assert p._scales[\"size\"].values == {\"small\": 1, \"large\": 2}\n\n    # Test passing a list\n    p.scale(hue=[\"blue\", \"red\"]).plot()\n    assert isinstance(p._scales[\"hue\"], Nominal)\n    assert p._scales[\"hue\"].values == [\"blue\", \"red\"]\n\n    # Test passing None\n    p.scale(x=None).plot()\n    assert p._scales[\"x\"] is None\n", "def test_add_method_transforms_order(flights_long):\n    p = Plot(flights_long)\n    mark = MockMark()\n    stat = Agg()\n    move = Move()\n\n    p.add(mark, stat, move, orient=\"x\")\n\n    assert p._layers[0][\"orient\"] == \"x\"\n    assert p._layers[0][\"mark\"] == mark\n    assert p._layers[0][\"stat\"] == stat\n    assert p._layers[0][\"move\"] == [move]\n"], "sample_724": ["def test_imputer_with_zero_missing_values():\n    \"\"\"Test the Imputer with missing values encoded as 0.\"\"\"\n    X = np.array([[1, 0, 3], [4, 0, 6], [7, 8, 9]])\n    X_true = np.array([[1, 5.5, 3], [4, 5.5, 6], [7, 8, 9]])\n    statistics = np.array([1, 5.5, 3])\n    _check_statistics(X, X_true, \"mean\", statistics, 0)\n", "def test_imputer_most_frequent():\n    X = np.array([[1, 2, 3],\n                  [4, 2, np.nan],\n                  [7, 6, 5],\n                  [4, np.nan, 5]])\n    X_true = np.array([[1, 2, 3],\n                       [4, 2, 3],\n                       [7, 6, 5],\n                       [4, 2, 5]])\n    statistics = np.array([1, 2, 3])\n    _check_statistics(X, X_true, 'most_frequent', statistics, np.nan)\n", "def test_imputer_sparse_matrix_with_missing_values_equal_to_zero():\n    X = sparse_random_matrix(10, 5, density=0.5, random_state=0)\n    X[0, 0] = 0\n    X_true = X.toarray()\n    X_true[0, 0] = np.mean(X_true[:, 0][np.nonzero(X_true[:, 0])])\n\n    imputer = Imputer(missing_values=0, strategy=\"mean\", axis=0)\n    X_trans = imputer.fit_transform(X.copy())\n\n    assert_array_almost_equal(X_trans.toarray(), X_true)\n", "def test_imputer_with_zeros():\n    \"\"\"Test imputation with missing values encoded as zeros.\"\"\"\n    X = np.array([[1, 2, 0],\n                  [3, 0, 5],\n                  [6, 7, 8]])\n    X_true = np.array([[1, 2, 3.5],\n                       [3, 3.5, 5],\n                       [6, 7, 8]])\n\n    _check_statistics(X, X_true, \"mean\", np.array([1, 3.5, 5]), 0)\n", "def test_imputer_with_strategies_and_missing_values():\n    \"\"\"Test Imputer with various strategies and missing values.\"\"\"\n\n    # Test case 1: Mean strategy with NaN as missing values\n    X = np.array([[1, 2, np.nan], [4, np.nan, 6], [7, 8, 9]])\n    X_true = np.array([[1, 2, 5.0], [4, 5.5, 6], [7, 8, 9]])\n    _check_statistics(X, X_true, \"mean\", np.array([1.0, 5.5, 9.0]), np.nan)\n\n    # Test case 2: Median strategy with -1 as missing values\n    X = np.array([[1, -1, 3], [4, -1, 6], [7, 8, -1]])\n    X_true = np.array([[1, 4.0, 3], [4, 4.0, 6], [7, 8, 6.0]])\n    _check_statistics(X, X_true, \"median\", np.array([1.0, 4.0, 6.0]), -1)\n\n    # Test case 3: Most frequent strategy with 'NA' as missing values\n    X = np.array([['A', 'B', 'NA'], ['A', 'NA', 'C'], ['A', 'B', 'NA']])\n    X_true = np.array([['A', 'B', 'A'], ['A', 'B', 'C'], ['A', 'B', 'A']])\n    _check_statistics(X, X_true, \"most_frequent\", np.array(['A', 'B', 'A']), 'NA')\n"], "sample_242": ["def test_year_lookup_bounds(self):\n    lookup = YearLookup()\n    lookup.lhs = mock.Mock()\n    lookup.lhs.lhs = mock.Mock()\n    lookup.lhs.lhs.output_field = DateTimeField()\n\n    connection = mock.Mock()\n    year = 2022\n\n    connection.ops.year_lookup_bounds_for_datetime_field.return_value = (\n        datetime(2022, 1, 1),\n        datetime(2022, 12, 31, 23, 59, 59, 999999),\n    )\n\n    bounds = lookup.year_lookup_bounds(connection, year)\n\n    connection.ops.year_lookup_bounds_for_datetime_field.assert_called_once_with(year)\n    self.assertEqual(bounds, (datetime(2022, 1, 1), datetime(2022, 12, 31, 23, 59, 59, 999999)))\n", "def test_batch_process_rhs_with_transform(self):\n    # Test that batch_process_rhs() works correctly when self.bilateral_transforms is not empty.\n    lookup = CustomLookup(DateTimeField(), datetime(2022, 1, 1))\n    lookup.bilateral_transforms = [Value]  # A simple transform that just wraps the value in a Value() expression.\n    compiler = mock.Mock()\n    connection = mock.Mock()\n    compiler.compile.return_value = ('%s', [datetime(2022, 1, 1)])\n    sqls, sqls_params = lookup.batch_process_rhs(compiler, connection)\n    self.assertEqual(sqls, ['%s'])\n    self.assertEqual(sqls_params, (datetime(2022, 1, 1),))\n", "    def test_year_lookup_bounds(self):\n        connection = mock.MagicMock()\n        year = 2022\n\n        lookup = YearLookup(DateTimeField(), Value(year))\n        lookup.year_lookup_bounds(connection, year)\n\n        connection.ops.year_lookup_bounds_for_datetime_field.assert_called_once_with(year)\n\n        connection.ops.year_lookup_bounds_for_datetime_field.reset_mock()\n        connection.ops.year_lookup_bounds_for_date_field.reset_mock()\n\n        lookup = YearLookup(mock.MagicMock(), Value(year))\n        lookup.year_lookup_bounds(connection, year)\n\n        connection.ops.year_lookup_bounds_for_date_field.assert_called_once_with(year)\n", "def test_year_lookup_bounds(self):\n    lookup = YearLookup()\n    lookup.lhs = mock.Mock()\n    lookup.lhs.lhs = mock.Mock()\n    lookup.lhs.lhs.output_field = DateTimeField()\n    connection = mock.Mock()\n    year = 2022\n\n    bounds = lookup.year_lookup_bounds(connection, year)\n\n    connection.ops.year_lookup_bounds_for_datetime_field.assert_called_once_with(year)\n    self.assertEqual(bounds, connection.ops.year_lookup_bounds_for_datetime_field.return_value)\n\n    lookup.lhs.lhs.output_field = mock.Mock()\n    bounds = lookup.year_lookup_bounds(connection, year)\n\n    connection.ops.year_lookup_bounds_for_date_field.assert_called_once_with(year)\n    self.assertEqual(bounds, connection.ops.year_lookup_bounds_for_date_field.return_value)\n", "def test_as_sql_with_direct_value(self):\n    lookup = CustomLookup(DateTimeField(\"created\"), datetime(2021, 1, 1))\n    with mock.patch.object(lookup, \"process_lhs\", return_value=(\"created\", [])) as mock_process_lhs, \\\n         mock.patch.object(lookup, \"process_rhs\", return_value=(\"'2021-01-01'\", [])) as mock_process_rhs, \\\n         mock.patch.object(lookup, \"get_rhs_op\", return_value=\"=\"):\n        sql, params = lookup.as_sql(mock.Mock(), mock.Mock())\n        mock_process_lhs.assert_called_once()\n        mock_process_rhs.assert_called_once()\n        self.assertEqual(sql, \"created = '2021-01-01'\")\n        self.assertEqual(params, [])\n"], "sample_842": ["def test_clone(kernel):\n    \"\"\"Test the clone function on kernels.\"\"\"\n    kernel_clone = clone(kernel)\n    assert kernel.get_params() == kernel_clone.get_params()\n", "def test_kernel_clone(kernel):\n    # Test that the clone of a kernel is equal to the original kernel\n    cloned_kernel = clone(kernel)\n    assert_array_equal(kernel(X, Y), cloned_kernel(X, Y))\n", "def test_kernel_operator_length(kernel):\n    # Test the length property of the KernelOperator class\n    if isinstance(kernel, KernelOperator):\n        # Calculate the expected length based on the kernels involved in the operation\n        expected_length = sum(k.length for k in kernel.kernels)\n        assert kernel.length == expected_length\n", "def test_kernel_clone(kernel):\n    \"\"\"Check that cloned kernels work correctly.\"\"\"\n    cloned_kernel = clone(kernel)\n    assert isinstance(cloned_kernel, type(kernel))\n    assert cloned_kernel.get_params() == kernel.get_params()\n    assert np.array_equal(cloned_kernel(X, Y), kernel(X, Y))\n", "def test_approx_fprime_no_params(kernel):\n    X_test = np.random.RandomState(0).normal(0, 1, (5, 2))\n    y_test = np.random.RandomState(0).normal(0, 1, 5)\n    params = kernel.get_params()\n    grad = _approx_fprime(X_test, y_test, kernel, params, epsilon=1e-6)\n    assert isinstance(grad, dict)\n    assert len(grad) == 0\n"], "sample_1026": ["def test_lambdastr_with_functions():\n    f = implemented_function(Function('f'), lambda x: x + 1)\n    g = implemented_function(Function('g'), lambda x: x * 10)\n    result = lambdastr(x, f(g(x)))\n    expected = \"lambda x: (f(g(x)))\"\n    assert result == expected, f\"Expected {expected}, but got {result}\"\n", "compilation error", "def test_tensorflow_printer():\n    from sympy.printing.tensorflow import TensorflowPrinter\n    printer = TensorflowPrinter()\n    expr = sin(x) + cos(y)\n    result = printer.doprint(expr)\n    assert \"tf.sin\" in result\n    assert \"tf.cos\" in result\n", "def test_lambdastr_nested_args():\n    expr = x + y + z\n    args = (w, (x, (y, z)))\n    result = lambdastr(args, expr)\n    assert result == 'lambda w,(_0,(_1,_2)): (w + _1 + _2)'\n", "def test_matrix_operations():\n    A = MatrixSymbol('A', 2, 2)\n    B = Matrix([[1, 2], [3, 4]])\n    expr = A * B\n    f = lambdify((A,), expr, modules=[\"numpy\"])\n    result = f(numpy.array([[5, 6], [7, 8]]))\n    expected_result = numpy.array([[19, 22], [43, 50]])\n    assert numpy.array_equal(result, expected_result)\n"], "sample_153": ["def test_database_checks(self, mocked_check):\n    mocked_check.return_value = []\n    errors = check_database_backends(databases=['default', 'other'])\n    self.assertEqual(errors, [])\n    mocked_check.assert_called()\n", "def test_check_database_backends_with_databases(self, mock_check):\n    # Arrange\n    mock_check.return_value = []\n\n    # Act\n    errors = check_database_backends(databases=['default', 'other'])\n\n    # Assert\n    self.assertEqual(len(errors), 0)\n    self.assertEqual(mock_check.call_count, 2)\n    mock_check.assert_any_call(databases=['default'])\n    mock_check.assert_any_call(databases=['other'])\n", "def test_check_database_backends_with_database_errors(self, mock_check):\n    mock_check.return_value = [(1, 'Error 1'), (2, 'Error 2')]\n    expected_output = [(1, 'Error 1', 'database'), (2, 'Error 2', 'database')]\n    self.assertEqual(check_database_backends(databases=['default']), expected_output)\n", "def test_check_database_backends_invalid_backend(self, mock_check):\n    mock_check.return_value = [(\"invalid_backend\", \"Invalid backend error\")]\n    with self.settings(DATABASES={'default': {'ENGINE': 'invalid_backend'}}):\n        errors = check_database_backends()\n        self.assertEqual(errors, [(\"invalid_backend\", \"Invalid backend error\")])\n", "def test_check_database_backends_with_error(self, mock_check):\n    mock_check.return_value = ['Error message']\n\n    errors = check_database_backends(databases=['default', 'other'])\n\n    mock_check.assert_called_once_with(databases=['default', 'other'])\n    self.assertEqual(errors, ['Error message', 'Error message'])\n"], "sample_1056": ["def test_print_Not():\n    expr = ~(x > y)\n    printer = LambdaPrinter({})\n    result = printer.doprint(expr)\n    assert result == '(not ((x) > (y)))'\n", "def test_lambdarepr_Piecewise():\n    expr = Piecewise((a, x < b), (c, True))\n    result = lambdarepr(expr, {\"x\": x, \"a\": a, \"b\": b, \"c\": c})\n    assert result == \"((a) if ((x) < (b)) else (c))\"\n", "def test_lambdarepr_boolean_expressions():\n    expr = (x > 0) & (y < 0) | ~(z == 0)\n    lambdarepr_str = lambdarepr(expr)\n    expected_str = \"(x > 0) and (y < 0) or (not (z == 0))\"\n    assert lambdarepr_str == expected_str\n", "def test_NumExprPrinter():\n    expr = sin(x) + cos(y) + sqrt(x**2 + y**2)\n    np = NumExprPrinter()\n    result = np.doprint(expr)\n    assert result == \"evaluate('sin(x) + cos(y) + sqrt(x**2 + y**2)', truediv=True)\"\n", "def test_lambdarepr():\n    expr = Piecewise((a, x > 0), (b, True))\n    result = lambdarepr(expr)\n    expected = \"((b) if ((x) > (0)) else (a))\"\n    assert result == expected\n"], "sample_1076": ["def test_print_MatrixSolve():\n    A = MatrixSymbol('A', 3, 3)\n    b = MatrixSymbol('b', 3, 1)\n    expr = MatrixSolve(A, b)\n    result = pycode(expr, printer=NumPyPrinter)\n    assert result == \"numpy.linalg.solve(A, b)\"\n", "def test_assignment():\n    expr = Assignment(x, y + z)\n    assert pycode(expr) == \"x = y + z\"\n", "def test_print_MatrixSolve():\n    A = MatrixSymbol('A', 3, 3)\n    b = MatrixSymbol('b', 3, 1)\n    expr = MatrixSolve(A, b)\n    result = pycode(expr, standard='python3', fully_qualified_modules=False)\n    expected = \"numpy.linalg.solve(A, b)\"\n    assert result == expected\n", "def test_SymPyPrinter_pow():\n    expr = sqrt(x)\n    assert SymPyPrinter().doprint(expr) == 'sympy.sqrt(x)'\n\n    expr = 1/sqrt(x)\n    assert SymPyPrinter().doprint(expr) == '1/sympy.sqrt(x)'\n", "def test_sympy_printer_mod():\n    expr = Mod(x, y)\n    result = SymPyPrinter().doprint(expr)\n    expected = \"sympy.Mod(x, y)\"\n    assert result == expected\n"], "sample_1057": ["def test_render_as_module():\n    from sympy import symbols\n\n    x, y = symbols('x y')\n    content = Print(x + y)\n\n    result = render_as_module(content)\n\n    expected = \"from sympy import symbols\\n\\nx, y = symbols('x y')\\nprint(x + y)\"\n\n    assert result == expected, f\"Expected {expected}, but got {result}\"\n", "def test_render_as_module_with_standard():\n    from sympy import symbols\n    x, y = symbols('x y')\n    content = Print(x + y)\n    result = render_as_module(content, standard='python3')\n    assert 'from sympy import symbols' in result\n    assert 'x, y = symbols(\\'x y\\')' in result\n    assert 'print(x + y)' in result\n", "def test_render_as_module():\n    from sympy import symbols\n\n    x, y = symbols('x y')\n    expr = x + y\n\n    result = render_as_module(expr)\n    expected_result = \"from sympy import symbols, Add\\n\\nx, y = symbols('x y')\\nexpr = Add(x, y)\\n\\n\"\n\n    assert result == expected_result\n", "def test_render_as_module():\n    from sympy import symbols\n\n    x, y = symbols('x y')\n    expr = x + y\n    expected_output = \"from sympy import symbols\\n\\nx, y = symbols('x y')\\nexpr = x + y\"\n\n    assert render_as_module(expr) == expected_output\n", "def test_render_as_module_standard_python2():\n    content = Print(1 + 2)\n    result = render_as_module(content, standard='python2')\n    assert \"from __future__ import print_function\" in result\n    assert \"print(3)\" in result\n"], "sample_196": ["    def test_adapt_decimalfield_value(self):\n        value = decimal.Decimal('123456789.123456789')\n        max_digits = 10\n        decimal_places = 2\n        result = self.ops.adapt_decimalfield_value(value, max_digits, decimal_places)\n        self.assertEqual(result, '123456789.12')\n", "def test_subtract_temporals(self):\n    if connection.features.supports_temporal_subtraction:\n        lhs = ('%s', [timezone.now()])\n        rhs = ('%s', [timezone.timedelta(days=1)])\n        result_sql, result_params = self.ops.subtract_temporals('DateTimeField', lhs, rhs)\n        self.assertEqual(result_sql, '(%s - %s)')\n        self.assertEqual(len(result_params), 2)\n    else:\n        with self.assertRaises(NotSupportedError):\n            self.ops.subtract_temporals('DateTimeField', ('%s', []), ('%s', []))\n", "    def test_integer_field_range(self):\n        for internal_type, expected_range in self.ops.integer_field_ranges.items():\n            with self.subTest(internal_type=internal_type):\n                self.assertEqual(self.ops.integer_field_range(internal_type), expected_range)\n", "def test_date_extract_sql(self):\n    with self.assertRaises(NotImplementedError) as cm:\n        self.ops.date_extract_sql('year', 'field_name')\n    self.assertEqual(str(cm.exception), self.may_require_msg % 'date_extract_sql')\n", "def test_no_limit_value(self):\n    # Make sure no_limit_value() is implemented.\n    self.assertIsNotNone(self.ops.no_limit_value())\n"], "sample_1106": ["def test_matadd_doit():\n    X = MatAdd(A, B, C)\n    Y = X.doit()\n    assert isinstance(Y, MatAdd)\n    assert A in Y.args\n    assert B in Y.args\n    assert C in Y.args\n", "def test_MatAdd_entry():\n    A = MatrixSymbol('A', 2, 2)\n    B = Matrix([[1, 2], [3, 4]])\n    X = MatAdd(A, B)\n    assert X._entry(0, 0) == A[0, 0] + 1\n    assert X._entry(1, 1) == A[1, 1] + 4\n", "def test_merge_explicit():\n    A = MatrixSymbol('A', 2, 2)\n    B = eye(2)\n    C = Matrix([[1, 2], [3, 4]])\n    X = MatAdd(A, B, C)\n    Y = merge_explicit(X)\n    assert Y == MatAdd(A, Matrix([[2, 2], [3, 5]]))\n", "def test_matadd_doit():\n    from sympy import MatAdd, Matrix\n    A = MatrixSymbol('A', 2, 2)\n    B = Matrix([[1, 2], [3, 4]])\n    C = Matrix([[5, 6], [7, 8]])\n    X = MatAdd(A, B, C)\n    Y = X.doit()\n    assert isinstance(Y, Matrix)\n    assert Y == Matrix([[A[0, 0] + 1, A[0, 1] + 2], [A[1, 0] + 3, A[1, 1] + 4]])\n", "def test_matadd_doit():\n    A = MatrixSymbol('A', 2, 2)\n    B = eye(2)\n    C = Matrix([[1, 2], [3, 4]])\n    X = MatAdd(A, B, C)\n    Y = X.doit()\n    assert Y == Matrix([[A[0, 0] + 1, A[0, 1] + 2], [A[1, 0] + 3, A[1, 1] + 4]])\n"], "sample_1088": ["def test_rational_interpolate():\n    data = [(1, -210), (2, -35), (3, 105), (4, 231), (5, 350), (6, 465)]\n    result = rational_interpolate(data, 2)\n    assert result == (105*x**2 - 525)/(x + 1)\n", "def test_viete():\n    f = a*x**2 + b*x + c\n    roots = [r1, r2]\n    assert viete(f, roots, x) == [(r1 + r2, -b/a), (r1*r2, c/a)]\n", "def test_viete():\n    roots = symbols('r1:4')\n    f = a*x**3 + b*x**2 + c*x + d\n    result = viete(f, roots, x)\n    expected = [(r1 + r2 + r3, -b/a), (-r1*r2 - r1*r3 - r2*r3, c/a), (r1*r2*r3, -d/a)]\n    assert result == expected\n", "def test_viete():\n    f = a*x**3 + b*x**2 + c*x + d\n    roots = [r1, r2, r3]\n    expected = [(r1 + r2 + r3, -b/a), (r1*r2 + r1*r3 + r2*r3, c/a), (-r1*r2*r3, -d/a)]\n    assert viete(f, roots, x) == expected\n", "def test_rational_interpolate():\n    data = [(1, -210), (2, -35), (3, 105), (4, 231), (5, 350), (6, 465)]\n    assert rational_interpolate(data, 2) == (105*x**2 - 525)/(x + 1)\n"], "sample_1068": ["def test_octave_code_MatrixSolve():\n    A = Matrix([[1, 2], [3, 4]])\n    b = Matrix([5, 6])\n    result = mcode(MatrixSolve(A, b))\n    assert result == 'A \\\\ b'\n", "def test_next_unit_test():\n    expr = RisingFactorial(x, y)\n    expected = \"pochhammer(x, y)\"\n    assert octave_code(expr) == expected\n", "def test_octave_code_chebyshevt():\n    expr = chebyshevt(2, x)\n    code = octave_code(expr)\n    assert code == \"chebyshevT(2, x)\"\n", "def test_octave_code_Min():\n    expr = Min(x, y, z)\n    result = mcode(expr)\n    assert result == \"min(min(x, y), z)\"\n", "def test_octave_code_relational():\n    # Test relational operators\n    expr = Eq(x, y)\n    assert mcode(expr) == 'x == y'\n"], "sample_973": ["def test_getargspec():\n        pass\n\n    argspec = inspect.getargspec(func)\n    assert argspec.args == ['a', 'b', 'c', 'd']\n    assert argspec.varargs == 'args'\n    assert argspec.varkw == 'kwargs'\n    assert argspec.defaults == (1, 2)\n    assert argspec.kwonlyargs == []\n    assert argspec.kwonlydefaults is None\n    assert argspec.annotations == {}\n", "def test_evaluate_signature():\n        return a + float(b)\n\n    sig = inspect.signature(func)\n    globalns = {'__name__': '__main__'}\n    evaluated_sig = inspect.evaluate_signature(sig, globalns)\n\n    assert evaluated_sig.parameters['a'].annotation == int\n    assert evaluated_sig.parameters['b'].annotation == str\n    assert evaluated_sig.return_annotation == float\n", "def test_isclassmethod():\n    class TestClass:\n        @classmethod\n            pass\n\n    assert inspect.isclassmethod(TestClass.class_method, TestClass, 'class_method')\n    assert not inspect.isclassmethod(TestClass.class_method)\n    assert not inspect.isclassmethod(TestClass.class_method, object, 'class_method')\n    assert not inspect.isclassmethod(lambda x: x)\n", "def test_stringify_signature():\n    # Test case 1: Simple function with annotations\n    sig = inspect.signature(lambda x: x)\n    assert stringify_signature(sig) == \"(x: Any) -> Any\"\n\n    # Test case 2: Function with default arguments\n    sig = inspect.signature(lambda x=1: x)\n    assert stringify_signature(sig) == \"(x: Any = 1) -> Any\"\n\n    # Test case 3: Function with positional-only arguments\n    if sys.version_info >= (3, 8):\n        sig = inspect.signature(lambda x, /: x)\n        assert stringify_signature(sig) == \"(x) -> Any\"\n\n    # Test case 4: Function with keyword-only arguments\n    sig = inspect.signature(lambda *, x: x)\n    assert stringify_signature(sig) == \"(*x) -> Any\"\n\n    # Test case 5: Function with variadic arguments\n    sig = inspect.signature(lambda *args, **kwargs: None)\n    assert stringify_signature(sig) == \"(*args, **kwargs) -> Any\"\n\n    # Test case 6: Function with annotations and default arguments\n    sig = inspect.signature(lambda x: int(x))\n    assert stringify_signature(sig) == \"(x: Any) -> int\"\n\n    # Test case 7: Function with unqualified typehints\n    sig = inspect.signature(lambda x: datetime.datetime.now())\n    assert stringify_signature(sig, unqualified_typehints=True) == \"(x) -> datetime\"\n", "def test_is_singledispatch_method():\n    class Dispatcher(object):\n        @functools.singledispatchmethod\n            return \"default\"\n\n        @dispatch.register(int)\n            return \"int\"\n\n    assert inspect.is_singledispatch_method(Dispatcher().dispatch) is True\n    assert inspect.is_singledispatch_method(Dispatcher().dispatch.registry[object]) is False\n"], "sample_1154": ["def test_linsolve_complex_coefficients():\n    eqs = [Eq(x + I*y, 1 + 2*I), Eq(x - I*y, 2 - I)]\n    sol = _linsolve(eqs, [x, y])\n    assert sol == {x: 2, y: 1}\n", "def test_linsolve_complex_coefficients():\n    eqs = [Eq(x + I*y, 1 + 2*I), Eq(x - I*y, 3 + 4*I)]\n    sol = _linsolve(eqs, [x, y])\n    assert sol == {x: S(2), y: S(3)}\n", "def test_linear_system_complex_coefficients():\n    eqs = [Eq(x + I*y, 1 + I), Eq(x - I*y, 2 - I)]\n    sol = _linsolve(eqs, [x, y])\n    assert sol == {x: 3/2 - I/2, y: 1/2 + I/2}\n", "def test_linsolve_complex_coefficients():\n    eqs = [Eq(2*I*x + y, 3 + 2*I), Eq(x - I*y, 1 - I)]\n    sol = _linsolve(eqs, [x, y])\n    assert sol == {x: 2 + I, y: 1 + 2*I}\n", "def test_linsolve_float_coefficients():\n    eqs = [Eq(2.0*x + 3.0*y, 5.0), Eq(4.0*x - 1.0*y, 7.0)]\n    sol = _linsolve(eqs, [x, y])\n    assert sol == {x: S(11)/10, y: S(3)/5}\n"], "sample_1119": ["def test_inverse_non_square():\n    with raises(NonSquareMatrixError):\n        A.inv()\n", "compilation error", "def test_non_square_inv_mod():\n    with raises(NonSquareMatrixError):\n        C.inv_mod(2)\n", "compilation error", "compilation error"], "sample_1036": ["def test_mul_with_matrices():\n    A = Matrix([[1, 2], [3, 4]])\n    B = Matrix([[5, 6], [7, 8]])\n    assert A * B == Matrix([[19, 22], [43, 50]])\n", "def test_mul_subs():\n    expr = A * B * C * D\n    old = A * B\n    new = E\n    result = expr.subs(old, new)\n    expected = E * C * D\n    assert result == expected\n", "def test_eval_transpose():\n    expr1 = Mul(A, B)\n    assert expr1.transpose() == Mul(B.transpose(), A.transpose())\n\n    expr2 = Mul(C, D)\n    assert expr2.transpose() == Mul(D.transpose(), C.transpose())\n\n    expr3 = Mul(A, C)\n    assert expr3.transpose() == Mul(C.transpose(), A.transpose())\n\n    expr4 = Mul(3, A)\n    assert expr4.transpose() == Mul(3, A.transpose())\n\n    expr5 = Mul(x, A)\n    assert expr5.transpose() == Mul(x, A.transpose())\n", "def test_adjoint_transpose_property():\n    assert adjoint(A) == transpose(A).conjugate()\n", "def test_mul_adjoint():\n    M = MatMul(A, B)\n    assert M.adjoint() == MatMul(B.adjoint(), A.adjoint())\n"], "sample_927": ["compilation error", "def test_namespace_object():\n    input = \"my_namespace\"\n    output = \"namespace my_namespace\"\n    idDict = {1: \"my_namespace\"}\n    _check(\"namespace\", input, idDict, output)\n", "def test_parse_function_pointer():\n    input = \"int (*func)(int)\"\n    idDict = {1: \"func\"}\n    output = \"int (<id_attr>{key}func</id_attr>)<paren_attr>(</paren_attr>int<paren_attr>)</paren_attr>\"\n    check('function', input, idDict, output)\n", "def test_parse_class():\n    input_string = \"class MyClass : public Base1, private Base2 {};\"\n    ast = parse(\"class\", input_string)\n    assert str(ast) == \"MyClass : public Base1 , private Base2 {}\"\n\n    parentNode = addnodes.desc()\n    signode = addnodes.desc_signature(input_string, '')\n    parentNode += signode\n    ast.describe_signature(signode, 'lastIsName', None, options={})\n    resAsText = parentNode.astext()\n    expected_output = \"class MyClass : public Base1, private Base2\"\n    assert resAsText == expected_output\n", "def test_parse_function_with_trailing_return_type():\n    input = \"void {key}name(int) -> int\"\n    id_dict = {1: \"name-1\"}\n    output = \"void {key}name(int) -> int\"\n    key = \"function\"\n    _check(\"function\", input, id_dict, output, key, None)\n"], "sample_588": ["def test_tile_ids_from_nested_list_with_nested_datasets():\n    nested_list = [[Dataset({'x': 1}), Dataset({'x': 2})], [Dataset({'x': 3}), Dataset({'x': 4})]]\n    expected_result = {(0, 0): Dataset({'x': 1}), (0, 1): Dataset({'x': 2}), (1, 0): Dataset({'x': 3}), (1, 1): Dataset({'x': 4})}\n    assert dict(_infer_tile_ids_from_nested_list(nested_list, ())) == expected_result\n", "def test_combine_all_along_first_dim(self):\n    datasets = [\n        Dataset({'x': ('time', [1, 2, 3])}),\n        Dataset({'x': ('time', [4, 5, 6])}),\n        Dataset({'x': ('time', [7, 8, 9])})\n    ]\n    combined_ids = OrderedDict(zip([(0,), (1,), (2,)], datasets))\n    expected_combined_ids = {\n        (): Dataset({'x': ('time', [1, 2, 3, 4, 5, 6, 7, 8, 9])})\n    }\n    result = _combine_all_along_first_dim(\n        combined_ids, dim='time', data_vars='all', coords='different', compat='no_conflicts'\n    )\n    assert_combined_tile_ids_equal(result, expected_combined_ids)\n", "def test_infer_concat_order_from_positions():\n    datasets = [create_test_data(), create_test_data()]\n    combined_ids = _infer_concat_order_from_positions(datasets)\n    expected_ids = OrderedDict([((0,), datasets[0]), ((1,), datasets[1])])\n    assert_combined_tile_ids_equal(combined_ids, expected_ids)\n", "def test_infer_concat_order_from_coords_monotonic_decreasing():\n    datasets = [\n        Dataset({'time': ('time', np.arange(10, 0, -1)), 'var': ('time', np.arange(10, 0, -1))}),\n        Dataset({'time': ('time', np.arange(20, 10, -1)), 'var': ('time', np.arange(20, 10, -1))}),\n    ]\n    combined_ids, concat_dims = _infer_concat_order_from_coords(datasets)\n    assert combined_ids == OrderedDict([((0,), datasets[1]), ((1,), datasets[0])])\n    assert concat_dims == ['time']\n", "def test_infer_concat_order_from_coords_different_variables():\n    ds1 = Dataset({'x': [1, 2, 3], 'var1': ('x', [1, 2, 3])})\n    ds2 = Dataset({'x': [4, 5, 6], 'var2': ('x', [4, 5, 6])})\n    ds3 = Dataset({'x': [7, 8, 9], 'var3': ('x', [7, 8, 9])})\n\n    combined_ids, concat_dims = _infer_concat_order_from_coords([ds1, ds2, ds3])\n\n    expected_combined_ids = OrderedDict([((0,), ds1), ((1,), ds2), ((2,), ds3)])\n    expected_concat_dims = ['x']\n\n    assert_combined_tile_ids_equal(combined_ids, expected_combined_ids)\n    assert concat_dims == expected_concat_dims\n"], "sample_430": ["def test_renamed_model_with_db_table_options(self):\n    changes = self.get_changes([self.author_with_db_table_options], [self.author_renamed_with_db_table_options])\n    self.assertOperationTypes(changes, 'testapp', 0, ['RenameModel', 'AlterModelTable'])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, old_name='Author', new_name='NewAuthor')\n    self.assertOperationAttributes(changes, 'testapp', 0, 1, name='NewAuthor', table='author_one')\n", "def test_author_name_deconstructible_objects(self):\n    questioner = MigrationQuestioner(specified_apps=[])\n    changes = self.get_changes(\n        [self.author_empty],\n        [self.author_name_deconstructible_1],\n        questioner,\n    )\n    self.assertOperationAttributes(\n        changes,\n        \"testapp\",\n        0,\n        0,\n        preserve_default=True,\n    )\n    changes = self.get_changes(\n        [self.author_name_deconstructible_1],\n        [self.author_name_deconstructible_2],\n        questioner,\n    )\n    self.assertOperationAttributes(\n        changes,\n        \"testapp\",\n        0,\n        0,\n        preserve_default=True,\n    )\n    changes = self.get_changes(\n        [self.author_name_deconstructible_1],\n        [self.author_name_deconstructible_3],\n        questioner,\n    )\n    self.assertOperationAttributes(\n        changes,\n        \"testapp\",\n        0,\n        0,\n        preserve_default=False,\n    )\n    changes = self.get_changes(\n        [self.author_name_deconstructible_3],\n        [self.author_name_deconstructible_4],\n        questioner,\n    )\n    self.assertOperationAttributes(\n        changes,\n        \"testapp\",\n        0,\n        0,\n        preserve_default=True,\n    )\n    changes = self.get_changes(\n        [self.author_name_deconstructible_list_1],\n        [self.author_name_deconstructible_list_2],\n        questioner,\n    )\n    self.assertOperationAttributes(\n        changes,\n        \"testapp\",\n        0,\n        0,\n        preserve_default=True,\n    )\n    changes = self.get_changes(\n        [self.author_name_deconstructible_list_1],\n        [self.author_name_deconstructible_list_3],\n        questioner,\n    )\n    self", "def test_custom_user(self):\n    changes = self.get_changes(\n        [self.author_empty],\n        [self.author_with_custom_user],\n    )\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AddField\"])\n    self.assertOperationAttributes(\n        changes,\n        \"testapp\",\n        0,\n        0,\n        field_name=\"user\",\n        field__to=(\"thirdapp\", \"CustomUser\"),\n    )\n", "def test_create_altered_indexes_index_together(self):\n    before_states = [\n        self.author_name,\n        self.book_indexes,\n    ]\n    after_states = [\n        self.author_name,\n        self.book_unique_together,\n    ]\n    changes = self.get_changes(before_states, after_states)\n    self.assertEqual(changes, {'otherapp': [mock.ANY]})\n    migration = changes['otherapp'][0]\n    self.assertEqual(len(migration.operations), 2)\n    self.assertIsInstance(migration.operations[0], migrations.RemoveIndex)\n    self.assertEqual(migration.operations[0].name, 'book_title_author_idx')\n    self.assertIsInstance(migration.operations[1], migrations.AlterUniqueTogether)\n    self.assertEqual(migration.operations[1].name, 'Book')\n    self.assertEqual(migration.operations[1].unique_together, {('author', 'title')})\n", "def test_swappable_model_creation(self):\n    before_states = []\n    after_states = [self.custom_user]\n    changes = self.get_changes(before_states, after_states)\n    self.assertNumberMigrations(changes, \"thirdapp\", 1)\n    self.assertOperationTypes(changes, \"thirdapp\", 0, [\"CreateModel\"])\n    self.assertOperationAttributes(\n        changes, \"thirdapp\", 0, 0, name=\"CustomUser\", swappable=\"AUTH_USER_MODEL\"\n    )\n"], "sample_959": ["def test_function_with_const_qualifier():\n    input = \"void f() const\"\n    idDict = {1: \"f\"}\n    output = \"void {key}f() const\"\n    check(\"function\", input, idDict, output)\n", "def test_member_object_parsing():\n    check(\"member\", \"int {key}foo\", {1: \"foo\"}, \"int {key}foo\")\n", "def test_parse_nested_name():\n    parser = DefinitionParser(\"std::vector<int>::iterator\", location=None, config=None)\n    nested_name = parser.parse_nested_name()\n    assert str(nested_name) == \"std::vector<int>::iterator\"\n", "def test_function_return_type_parsing():\n    check(\"function\",\n          \"void Func(int a, double b)\",\n          {1: \"Func(int,double)\"},\n          \"void Func(int a, double b)\")\n\n    check(\"function\",\n          \"void Func(int a = 42, double b = 1.5)\",\n          {1: \"Func(int,double)\"},\n          \"void Func(int a = 42, double b = 1.5)\")\n\n    check(\"function\",\n          \"void Func(int a, double b) noexcept\",\n          {1: \"Func(int,double)noexcept\"},\n          \"void Func(int a, double b) noexcept\")\n\n    check(\"function\",\n          \"void Func(int a, double b) noexcept(true)\",\n          {1: \"Func(int,double)noexcept\"},\n          \"void Func(int a, double b) noexcept(true)\")\n\n    check(\"function\",\n          \"void Func(int a, double b) const\",\n          {1: \"Func(int,double)const\"},\n          \"void Func(int a, double b) const\")\n\n    check(\"function\",\n          \"void Func(int a, double b) const volatile\",\n          {1: \"Func(int,double)constvolatile\"},\n          \"void Func(int a, double b) const volatile\")\n\n    check(\"function\",\n          \"void Func(int a, double b) const volatile noexcept\",\n          {1: \"Func(int,double)constvolatilenoexcept\"},\n          \"void Func(int a, double b) const volatile noexcept\")\n\n    check(\"function\",\n          \"void Func(int a, double b) const volatile noexcept(true)\",\n          {1: \"Func(int,double)constvolatilenoexcept\"},\n          \"void Func(int a, double b) const volatile noexcept(true)\")\n\n    check(\"function\",\n          \"void Func(int a, double b) &\",\n          {1: \"Func(int,double)&\"},\n          \"void Fun", "def test_template_introduction():\n    input = \"typename std::__detail::__comp_ref_type<T>::type\"\n    idDict = {1: \"N4429std10__detail26__comp_ref_typeE\"}\n    output = \"typename <span class=\\\"n\\\">std</span>::<span class=\\\"n\\\">__detail</span>::<span class=\\\"n\\\">__comp_ref_type</span><span class=\\\"p\\\">&lt;</span><span class=\\\"n\\\">T</span><span class=\\\"p\\\">&gt;</span>::<span class=\\\"n\\\">type</span>\"\n    _check(\"type\", input, idDict, output)\n"], "sample_1118": ["def test_MatPow_non_square_matrix():\n    F = MatrixSymbol('F', n, m)\n    with raises(NonSquareMatrixError):\n        MatPow(F, 2)\n", "compilation error", "def test_MatPow_derivative_matrix_lines():\n    x = symbols('x')\n    M = MatrixSymbol('M', 1, 1)\n    MP = MatPow(M, x)\n    dMP = MP._eval_derivative_matrix_lines(x)\n    assert len(dMP) == 1\n    assert dMP[0].args[0].args[2] == x*MatPow(M, x-1)\n", "def test_MatPow_doit():\n    # Test MatPow.doit() for MatrixSymbol\n    assert MatPow(A, 0) == Identity(n)\n    assert MatPow(A, 1) == A\n    assert MatPow(A, n) == A**n\n    assert MatPow(A, -1) == Inverse(A)\n\n    # Test MatPow.doit() for ZeroMatrix\n    assert MatPow(ZeroMatrix(n, n), 0) == Identity(n)\n    raises(NonInvertibleMatrixError, lambda: MatPow(ZeroMatrix(n, n), -1))\n\n    # Test MatPow.doit() for Power of Power\n    assert MatPow(MatPow(A, 2), 3) == A**6\n\n    # Test MatPow.doit() for OneMatrix\n    assert MatPow(OneMatrix(n, n), n) == OneMatrix(n, n)\n", "def test_MatPow_doit_non_square_matrix_positive_exp():\n    F = MatrixSymbol('F', n, m)\n    G = MatPow(F, 2)\n    with raises(NonSquareMatrixError):\n        G.doit()\n"], "sample_969": ["def test_restify(annotation, expected):\n    assert restify(annotation) == expected\n", "def test_restify_struct():\n    assert restify(Struct('i')) == ':py:class:`struct.Struct`'\n", "def test_restify():\n    assert restify(MyClass1) == ':py:class:`test_util_typing.MyClass1`'\n    assert restify(MyClass2) == ':py:class:`<MyClass2>`'\n    assert restify(MyInt) == ':py:class:`test_util_typing.MyInt`'\n    assert restify(MyList[T]) == ':py:class:`test_util_typing.MyList`\\\\ [T]'\n    assert restify(Optional[int]) == ':py:obj:`~typing.Optional`\\\\ [int]'\n    assert restify(Union[int, str]) == ':py:obj:`~typing.Union`\\\\ [int, str]'\n    assert restify(Callable[[int], str]) == ':py:obj:`~typing.Callable`\\\\ [[int], str]'\n    assert restify(Tuple[int, ...]) == ':py:obj:`~typing.Tuple`\\\\ [int, ...]'\n    assert restify(BrokenType) == '<BrokenType object>'  # Assuming inspect.object_description is used\n    assert restify(Struct) == ':py:class:`struct.Struct`'\n    assert restify(TracebackType) == ':py:class:`types.TracebackType`'\n", "def test_stringify(annotation, expected):\n    assert stringify(annotation) == expected\n    assert stringify(annotation, smartref=True) == expected.replace('typing.', '~typing.')\n", "def test_restify():\n    assert restify(MyClass1) == \":py:class:`test_util_typing.MyClass1`\"\n    assert restify(MyClass2) == \":py:class:`test_util_typing.MyClass2`\"\n    assert restify(MyInt) == \":py:class:`test_util_typing.MyInt`\"\n    assert restify(MyList) == \":py:class:`test_util_typing.MyList`\"\n    assert restify(BrokenType) == \"BrokenType\"\n    assert restify(Integral) == \":py:class:`numbers.Integral`\"\n    assert restify(Struct) == \":py:class:`struct.Struct`\"\n    assert restify(TracebackType) == \":py:class:`types.TracebackType`\"\n"], "sample_1141": ["compilation error", "compilation error", "def test_diff_matrix_element():\n    M = Matrix([[x**2, sin(x)], [exp(x), cos(x)]])\n    i, j = symbols('i j', integer=True)\n    ME = MatrixElement(M, i, j)\n    diff_ME = diff(ME, x)\n    assert isinstance(diff_ME, MatrixElement)\n    assert diff_ME.parent == M\n    assert diff_ME.i == i\n    assert diff_ME.j == j\n    assert diff_ME.doit() == diff(M[i, j], x)\n", "compilation error", "compilation error"], "sample_1174": ["def test_abs_derivative():\n    x, y = symbols('x y', real=True)\n    z = x + I*y\n    assert Abs(z)._eval_derivative(x) == Derivative(x, x, evaluate=True) * sign(conjugate(z))\n    assert Abs(z)._eval_derivative(y) == Derivative(y, y, evaluate=True) * sign(conjugate(z))\n", "def test_abs_derivative():\n    x, y = symbols('x y', real=True)\n    z = x + I*y\n    assert Abs(z)._eval_derivative(x) == Derivative(x, x, evaluate=True) * sign(conjugate(z)) + Derivative(y, x, evaluate=True) * I * sign(conjugate(z))\n    assert Abs(z)._eval_derivative(y) == Derivative(x, y, evaluate=True) * sign(conjugate(z)) + Derivative(y, y, evaluate=True) * I * sign(conjugate(z))\n", "def test_abs():\n    x = Symbol('x', real=True)\n    assert Abs(x)._eval_is_real() is True\n    assert Abs(x)._eval_is_integer() is None\n    assert Abs(x)._eval_is_extended_nonzero() is None\n    assert Abs(x)._eval_is_zero() is None\n    assert Abs(x)._eval_is_extended_positive() is None\n    assert Abs(x)._eval_is_rational() is None\n    assert Abs(x)._eval_is_even() is None\n    assert Abs(x)._eval_is_odd() is None\n    assert Abs(x)._eval_is_algebraic() is None\n    y = Symbol('y', real=True, integer=True)\n    assert Abs(y)._eval_is_integer() is True\n    assert Abs(0)._eval_is_zero() is True\n    assert Abs(1)._eval_is_extended_positive() is True\n    assert Abs(Rational(1, 2))._eval_is_rational() is True\n    assert Abs(2)._eval_is_even() is True\n    assert Abs(3)._eval_is_odd() is True\n    assert Abs(sqrt(2))._eval_is_algebraic() is True\n", "def test_principal_branch():\n    x, y = symbols('x y', polar=True)\n    assert principal_branch(exp_polar(2*pi*I)*3, 2*pi) == 3*exp_polar(0)\n    assert principal_branch(exp_polar(2*pi*I)*3*x, 2*pi) == 3*principal_branch(x, 2*pi)\n", "def test_unpolarify():\n    expr = exp_polar(I*pi/4)*2\n    unpolarized_expr = unpolarify(expr)\n    assert N_equals(unpolarized_expr, sqrt(2) + I*sqrt(2))\n"], "sample_133": ["def test_set_language_post_with_valid_language(self):\n    \"\"\"Test set_language view with valid language code in POST request.\"\"\"\n    language_code = self._get_inactive_language_code()\n    data = {'language': language_code, 'next': '/'}\n    request = RequestFactory().post('/i18n/setlang/', data)\n    request.session = {}\n    response = set_language(request)\n    self.assertEqual(response.status_code, 302)\n    self.assertRedirects(response, '/')\n    self.assertEqual(response.cookies[settings.LANGUAGE_COOKIE_NAME].value, language_code)\n    self.assertEqual(request.session[LANGUAGE_SESSION_KEY], language_code)\n", "    def test_set_language_with_invalid_next_url(self):\n        \"\"\"Test set_language view with an invalid next URL.\"\"\"\n        url = reverse('set_language')\n        invalid_url = 'http://example.com'\n        data = {LANGUAGE_QUERY_PARAMETER: self._get_inactive_language_code(), 'next': invalid_url}\n        response = self.client.post(url, data)\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response.url, '/')\n", "    def test_get_javascript_catalog(self):\n        url = reverse('javascript-catalog')\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response['Content-Type'], 'text/javascript; charset=\"utf-8\"')\n        # Test that the response is a valid JavaScript file\n        # This is a simple check, more comprehensive testing could be done\n        self.assertIn('function(globals) {', response.content.decode())\n", "    def test_set_language_with_valid_language(self):\n        \"\"\"Test set_language view with valid language code.\"\"\"\n        lang_code = self._get_inactive_language_code()\n        request = RequestFactory().post('/i18n/setlang/', {'language': lang_code})\n        request.session = {}\n        response = set_language(request)\n        self.assertEqual(response.status_code, 302)  # Redirect to a success page\n        self.assertEqual(request.session[LANGUAGE_SESSION_KEY], lang_code)\n        self.assertEqual(response.cookies[settings.LANGUAGE_COOKIE_NAME].value, lang_code)\n", "def test_language_redirection(self):\n    \"\"\"Test if the set_language view redirects to the next URL with the new language.\"\"\"\n    url = reverse('set_language')\n    next_url = '/test/'\n    lang_code = self._get_inactive_language_code()\n\n    response = self.client.post(url, data={'next': next_url, 'language': lang_code})\n\n    self.assertRedirects(response, '/' + lang_code + next_url)\n    self.assertEqual(self.client.session[LANGUAGE_SESSION_KEY], lang_code)\n    self.assertEqual(response.cookies[settings.LANGUAGE_COOKIE_NAME].value, lang_code)\n"], "sample_1058": ["def test_print_Mod():\n    expr = Mod(x, y)\n    code = PythonCodePrinter().doprint(expr)\n    assert code == 'x % y'\n", "def test_print_piecewise_with_none_condition():\n    expr = Piecewise((x, Eq(y, 0)), (none, True))\n    result = pycode(expr)\n    expected = \"x if (y == 0) else None\"\n    assert result == expected\n", "def test_print_MatMul():\n    expr = MatrixSymbol('A', 2, 3) * MatrixSymbol('B', 3, 2)\n    result = pycode(expr, printer='numpy')\n    expected = \"numpy.array([[numpy.dot(A[0, :], B[:, 0]), numpy.dot(A[0, :], B[:, 1])], [numpy.dot(A[1, :], B[:, 0]), numpy.dot(A[1, :], B[:, 1])]])\"\n    assert result == expected\n", "def test_print_Mod():\n    e = Mod(x, y)\n    assert pycode(e) == 'x % y'\n", "def test_print_Mod():\n    expr = Mod(x, y)\n    printer = PythonCodePrinter()\n    result = printer._print_Mod(expr)\n    assert result == \"x % y\"\n"], "sample_828": ["def test_pairwise_distances_chunked():\n    X = np.random.RandomState(0).rand(5, 3)\n    r = .2\n\n        neigh = [np.flatnonzero(d < r) for d in D_chunk]\n        avg_dist = (D_chunk * (D_chunk < r)).mean(axis=1)\n        return neigh, avg_dist\n\n    gen = pairwise_distances_chunked(X, reduce_func=reduce_func)\n    neigh, avg_dist = next(gen)\n    expected_neigh = [np.array([0, 3]), np.array([1]), np.array([2]), np.array([0, 3]), np.array([4])]\n    assert_array_equal(neigh, expected_neigh)\n    expected_avg_dist = np.array([0.03935185, 0.0, 0.0, 0.03935185, 0.0])\n    assert_array_almost_equal(avg_dist, expected_avg_dist)\n", "def test_pairwise_distances_with_callable_metric():\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    Y = np.array([[7, 8], [9, 10], [11, 12]])\n    metric = lambda x, y: np.sum(np.abs(x - y))\n    expected_output = np.array([[6, 12, 18], [10, 16, 22], [14, 20, 26]])\n    output = pairwise_distances(X, Y, metric=metric)\n    assert_array_equal(output, expected_output)\n", "def test_pairwise_kernels_with_callable_metric():\n    X = np.array([[1, 2], [3, 4]])\n    Y = np.array([[5, 6], [7, 8]])\n\n        return np.sum(x * y)\n\n    K = pairwise_kernels(X, Y, metric=custom_metric)\n    expected_K = np.array([[17, 39], [31, 71]])\n    assert_array_equal(K, expected_K)\n", "def test_pairwise_distances_chunked_with_reduce_func():\n    X = np.random.RandomState(0).rand(5, 3)\n    r = .2\n\n        neigh = [np.flatnonzero(d < r) for d in D_chunk]\n        avg_dist = (D_chunk * (D_chunk < r)).mean(axis=1)\n        return neigh, avg_dist\n\n    gen = pairwise_distances_chunked(X, reduce_func=reduce_func)\n    neigh, avg_dist = next(gen)\n    expected_neigh = [np.array([0, 3]), np.array([1]), np.array([2]), np.array([0, 3]), np.array([4])]\n    expected_avg_dist = np.array([0.03935185, 0.0, 0.0, 0.03935185, 0.0])\n    assert_array_equal(neigh, expected_neigh)\n    assert_array_almost_equal(avg_dist, expected_avg_dist)\n", "def test_pairwise_distances_chunked_reduce_func_shape():\n    X = np.random.RandomState(0).rand(5, 3)\n\n        return D_chunk, start\n\n    gen = pairwise_distances_chunked(X, reduce_func=reduce_func, working_memory=0)\n    D_chunk, start = next(gen)\n    assert D_chunk.shape[0] == 1  # one row per chunk\n    assert isinstance(start, int)\n"], "sample_827": ["def test_inplace_column_scale():\n    X = sp.csc_matrix([[1, 2], [3, 4]])\n    scale = np.array([2, 3])\n    inplace_column_scale(X, scale)\n    assert_array_equal(X.toarray(), np.array([[2, 6], [6, 12]]))\n\n    X = sp.csr_matrix([[1, 2], [3, 4]])\n    inplace_column_scale(X, scale)\n    assert_array_equal(X.toarray(), np.array([[2, 6], [6, 12]]))\n\n    X = np.array([[1, 2], [3, 4]])\n    with pytest.raises(TypeError):\n        inplace_column_scale(X, scale)\n", "def test_csc_median_axis_0():\n    # Test case 1: Test with a simple CSC matrix\n    X = sp.csc_matrix([[1, 2, 0], [3, 0, 4], [5, 6, 0]])\n    expected_median = np.array([3, 2, 4])\n    assert_array_equal(csc_median_axis_0(X), expected_median)\n\n    # Test case 2: Test with a CSC matrix containing all zeros\n    X = sp.csc_matrix([[0, 0, 0], [0, 0, 0], [0, 0, 0]])\n    expected_median = np.array([0, 0, 0])\n    assert_array_equal(csc_median_axis_0(X), expected_median)\n\n    # Test case 3: Test with a CSC matrix containing all the same value\n    X = sp.csc_matrix([[2, 2, 2], [2, 2, 2], [2, 2, 2]])\n    expected_median = np.array([2, 2, 2])\n    assert_array_equal(csc_median_axis_0(X), expected_median)\n\n    # Test case 4: Test with a CSC matrix containing NaN values\n    X = sp.csc_matrix([[np.nan, 2, np.nan], [3, np.nan, 4], [5, 6, np.nan]])\n    expected_median = np.array([3, 2, 4])\n    assert_array_equal(csc_median_axis_0(X), expected_median)\n\n    # Test case 5: Test with a CSC matrix containing negative values\n    X = sp.csc_matrix([[-1, 2, 0], [3, 0, -4], [5, 6, 0]])\n    expected_median = np.array([3, 2, 0])\n    assert_array_equal(csc_median_axis", "def test_csc_median_axis_0():\n    rng = RandomState(0)\n    X = sp.csc_matrix(rng.randn(10, 5))\n    expected_median = np.median(X.toarray(), axis=0)\n    calculated_median = csc_median_axis_0(X)\n    assert_array_almost_equal(calculated_median, expected_median)\n", "def test_mean_variance_axis_dense_matrix():\n    X = np.array([[1, 2, 3], [4, 5, 6]])\n    with assert_raises(TypeError) as err:\n        mean_variance_axis(X, axis=0)\n    assert \"Expected a CSR or CSC sparse matrix, got <class 'numpy.ndarray'>\" in str(err.value)\n", "def test_csc_median_axis_0():\n    # Test csc_median_axis_0 function with random CSC matrix\n    rng = np.random.RandomState(0)\n    data = rng.rand(100)\n    indices = np.random.randint(0, 10, 100)\n    indptr = np.sort(np.random.randint(0, 100, 11))\n    X = sp.csc_matrix((data, indices, indptr), shape=(100, 10))\n    medians = csc_median_axis_0(X)\n    assert_array_almost_equal(medians, np.median(X.toarray(), axis=0), decimal=6)\n\n    # Test csc_median_axis_0 function with CSC matrix containing zeros\n    data = np.array([0, 2, 4, 0, 6, 8, 0])\n    indices = np.array([0, 1, 2, 3, 4, 5, 6])\n    indptr = np.array([0, 2, 5, 7])\n    X = sp.csc_matrix((data, indices, indptr), shape=(3, 7))\n    medians = csc_median_axis_0(X)\n    assert_array_almost_equal(medians, np.array([1, 4, 4, 0, 6, 0, 0]), decimal=6)\n\n    # Test csc_median_axis_0 function with CSC matrix containing NaNs\n    data = np.array([0, np.nan, 4, 0, 6, np.nan, 0])\n    indices = np.array([0, 1, 2, 3, 4, 5, 6])\n    indptr = np.array([0, 2, 5, 7])\n    X = sp.csc_matrix((data, indices, indptr), shape=(3, 7))\n    medians = csc_median_axis_0(X)"], "sample_154": ["def test_check_database_backends(self, mock_check):\n    mock_check.return_value = []\n    issues = check_database_backends(databases=self.databases)\n    self.assertEqual(len(issues), 0)\n    mock_check.assert_called()\n", "def test_check_database_backends(self, mock_check):\n    mock_check.return_value = []\n    issues = check_database_backends(databases=self.databases)\n    self.assertEqual(len(issues), 0)\n    mock_check.assert_called()\n", "def test_check_database_backends(self, mock_check):\n    mock_issues = ['Test issue 1', 'Test issue 2']\n    mock_check.return_value = mock_issues\n    issues = check_database_backends(self.databases)\n    self.assertEqual(issues, mock_issues * len(self.databases))\n", "def test_check_database_backends(self, mock_check):\n    mock_check.side_effect = [[\"mock_issue1\"], [\"mock_issue2\"]]\n    issues = check_database_backends(databases=self.databases)\n    self.assertEqual(issues, [\"mock_issue1\", \"mock_issue2\"])\n", "def test_check_database_backends(self, mock_check):\n    mock_check.return_value = []\n    issues = check_database_backends(databases=self.databases)\n    self.assertEqual(len(issues), 0)\n    mock_check.assert_called_with()\n"], "sample_319": ["def test_model_renamed_with_db_table_options(self):\n    changes = self.get_changes(self.author_with_db_table_options, self.author_renamed_with_db_table_options)\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"RenameModel\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, old_name=\"Author\", new_name=\"NewAuthor\")\n", "    def test_identical_deconstructible_objects(self):\n        changes = self.get_changes(\n            [self.author_name_deconstructible_1],\n            [self.author_name_deconstructible_2],\n        )\n        self.assertNumberMigrations(changes, \"testapp\", 0)\n", "def test_check_constraint_added(self):\n    changes = self.get_changes([self.author_empty], [self.author_name_check_constraint])\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AddField\", \"AddConstraint\"])\n", "    def test_renamed_indexes(self):\n        \"\"\"\n        Test that indexes are renamed when old and new indexes are the same except\n        for the names.\n        \"\"\"\n        changes = self.get_changes(\n            [self.book_indexes],\n            [self.book_unordered_indexes],\n        )\n        self.assertOperationTypes(changes, \"otherapp\", 0, [\"RemoveIndex\", \"AddIndex\"])\n        self.assertEqual(\n            changes[\"otherapp\"][0].operations[0].name,\n            \"book_title_author_idx\",\n        )\n        self.assertEqual(\n            changes[\"otherapp\"][0].operations[1].name,\n            \"book_author_title_idx\",\n        )\n", "def test_alter_db_table(self):\n    \"\"\"\n    Tests that alterations to the db_table option are detected.\n    \"\"\"\n    before_states = [self.author_with_db_table_options]\n    after_states = [self.author_with_new_db_table_options]\n\n    changes = self.get_changes(before_states, after_states)\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterModelTable\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, table=\"author_two\")\n"], "sample_415": ["    def setUp(self):\n        self.constraint = CheckConstraint(check=Q(price__gt=0), name=\"positive_price\")\n", "    def test_check_constraint_validate(self):\n        product = Product(name='Test', price=10)\n        product.full_clean()  # Should not raise\n\n        product = Product(name='Test', price=-1)\n        with self.assertRaises(ValidationError):\n            product.full_clean()  # Should raise\n", "    def test_validate_with_expressions(self):\n        constraint = UniqueConstraint(\n            Lower(\"name\"),\n            name=\"unique_lower_name\",\n        )\n        Product.objects.create(name=\"Test\")\n        with self.assertRaises(ValidationError) as e:\n            Product(name=\"TEST\").full_clean()\n        self.assertEqual(str(e.exception), \"Constraint \u201cunique_lower_name\u201d is violated.\")\n", "    def test_check_constraint(self):\n        with self.assertRaisesMessage(ValidationError, \"Constraint \u201ccheck_discount\u201d is violated.\"):\n            Product.objects.create(name=\"test\", price=10, discount=20)\n\n        # Test that the constraint is enforced in the database\n        with self.assertRaises(IntegrityError):\n            with connection.cursor() as cursor:\n                cursor.execute(\n                    \"INSERT INTO {table} (name, price, discount) VALUES ('test', 10, 20)\".format(\n                        table=Product._meta.db_table,\n                    )\n                )\n", "    def test_check_constraint_validation(self):\n        with self.assertRaises(ValidationError):\n            Product.objects.create(price=-100)\n\n        product = Product.objects.create(price=100)\n        product.price = -50\n        with self.assertRaises(ValidationError):\n            product.full_clean()\n\n        product.price = 200\n        product.full_clean()\n"], "sample_826": ["def test_ordinal_encoder_inverse_transform():\n    X = [['Male', 1], ['Female', 3], ['Female', 2]]\n    enc = OrdinalEncoder()\n    enc.fit(X)\n    X_trans = enc.transform(X)\n    X_inv = enc.inverse_transform(X_trans)\n    assert_array_equal(X, X_inv)\n", "def test_one_hot_encoder_invalid_handle_unknown():\n    X = [['Male', 1], ['Female', 3], ['Female', 2]]\n    enc = OneHotEncoder(handle_unknown='invalid_value')\n    assert_raises_regex(ValueError, \"handle_unknown should be either 'error' or 'ignore', got invalid_value.\", enc.fit, X)\n", "def test_dtype_preservation(encoder, expected_dtype):\n    X = np.array([['Male', 1], ['Female', 3], ['Female', 2]])\n    X_encoded = encoder.fit_transform(X)\n    assert X_encoded.dtype == expected_dtype\n", "def test_OneHotEncoder_unknown_categories():\n    enc = OneHotEncoder(handle_unknown='ignore')\n    enc.fit([['Male', 1], ['Female', 3], ['Female', 2]])\n\n    X_test = [['Male', 1], ['Unknown', 4]]\n    X_transformed = enc.transform(X_test)\n\n    assert_array_equal(toarray(X_transformed.toarray()),\n                       [[1., 0., 1., 0., 0.],\n                        [0., 0., 0., 0., 0.]])\n\n    X_inverse = enc.inverse_transform(X_transformed)\n\n    assert_array_equal(X_inverse,\n                       [['Male', 1],\n                        [None, None]])\n", "def test_ordinal_encoder_with_drop():\n    enc = OrdinalEncoder(categories=[['Female', 'Male'], [1, 2, 3]], dtype=np.float64)\n    X = [['Male', 1], ['Female', 3], ['Female', 2]]\n    enc.fit(X)\n\n    enc_drop = OrdinalEncoder(categories=[['Female', 'Male'], [1, 2, 3]], dtype=np.float64, drop='first')\n    enc_drop.fit(X)\n\n    assert_array_equal(enc.transform(X), enc_drop.transform(X) - 1)\n"], "sample_781": ["def test_classification_iris(name):\n    \"\"\"Check classification on the iris dataset.\"\"\"\n    ForestClassifier = FOREST_CLASSIFIERS[name]\n\n    clf = ForestClassifier(n_estimators=10, random_state=0)\n    clf.fit(iris.data, iris.target)\n    assert_greater(clf.score(iris.data, iris.target), 0.9)\n    assert_equal(10, len(clf))\n\n    # also test apply\n    leaf_indices = clf.apply(iris.data)\n    assert_equal(leaf_indices.shape, (iris.data.shape[0], clf.n_estimators))\n", "def check_classification_toy_warm_start(name):\n    \"\"\"Check classification on a toy dataset with warm_start.\"\"\"\n    ForestClassifier = FOREST_CLASSIFIERS[name]\n\n    clf = ForestClassifier(n_estimators=5, random_state=1, warm_start=True)\n    clf.fit(X, y)\n    assert_array_equal(clf.predict(T), true_result)\n    assert_equal(5, len(clf))\n\n    clf.set_params(n_estimators=10)\n    clf.fit(X, y)\n    assert_array_equal(clf.predict(T), true_result)\n    assert_equal(10, len(clf))\n", "def check_feature_importances(name):\n    \"\"\"Check feature importances on a larger dataset.\"\"\"\n    ForestClassifier = FOREST_CLASSIFIERS[name]\n\n    clf = ForestClassifier(n_estimators=10, random_state=1)\n    clf.fit(X_large, y_large)\n    importances = clf.feature_importances_\n    assert_greater(importances.sum(), 0)\n    assert_equal(importances.shape, (X_large.shape[1],))\n", "def test_iris_classification(name):\n    \"\"\"Check classification on the iris dataset.\"\"\"\n    ForestClassifier = FOREST_CLASSIFIERS[name]\n\n    clf = ForestClassifier(n_estimators=10, random_state=0)\n    clf.fit(iris.data, iris.target)\n    assert_greater(clf.score(iris.data, iris.target), 0.9)\n\n    # also test apply\n    leaf_indices = clf.apply(iris.data)\n    assert_equal(leaf_indices.shape, (iris.data.shape[0], clf.n_estimators))\n", "def check_feature_importances(name):\n    \"\"\"Check feature importances on a larger dataset.\"\"\"\n    ForestClassifier = FOREST_CLASSIFIERS[name]\n\n    clf = ForestClassifier(n_estimators=10, random_state=0)\n    clf.fit(X_large, y_large)\n    importances = clf.feature_importances_\n    assert_greater(importances.shape[0], 0)\n    assert_equal(importances.ndim, 1)\n    assert_equal(importances.shape, (X_large.shape[1],))\n    assert_almost_equal(np.sum(importances), 1.0, decimal=2)\n"], "sample_195": ["    def test_adapt_datetimefield_value(self):\n        # Test when value is None\n        self.assertIsNone(connection.ops.adapt_datetimefield_value(None))\n\n        # Test when value is a datetime object\n        dt = timezone.now()\n        self.assertEqual(connection.ops.adapt_datetimefield_value(dt), str(dt))\n\n        # Test when value is a timezone-aware datetime object and USE_TZ is True\n        with override_settings(USE_TZ=True):\n            tz_dt = timezone.now()\n            self.assertEqual(connection.ops.adapt_datetimefield_value(tz_dt), str(timezone.make_naive(tz_dt, connection.timezone)))\n\n        # Test when value is a timezone-aware datetime object and USE_TZ is False\n        with override_settings(USE_TZ=False):\n            with self.assertRaises(ValueError):\n                connection.ops.adapt_datetimefield_value(tz_dt)\n", "    def test_datetime_cast_date_sql(self):\n        ops = connection.ops\n        field_name = '\"my_date_field\"'\n        tzname = 'America/New_York'\n        sql = ops.datetime_cast_date_sql(field_name, tzname)\n        if timezone.is_aware(timezone.now()):\n            expected_sql = \"django_datetime_cast_date(%s, '%s', '%s')\" % (field_name, tzname, connection.timezone_name)\n        else:\n            expected_sql = \"django_datetime_cast_date(%s, NULL, NULL)\" % field_name\n        self.assertEqual(sql, expected_sql)\n", "    def test_date_trunc_sql(self):\n        self.assertEqual(\n            self.ops.date_trunc_sql('year', 'date_column'),\n            \"django_date_trunc('year', date_column)\"\n        )\n", "    def test_date_trunc_sql(self):\n        sql = self.ops.date_trunc_sql('month', 'date_field')\n        self.assertEqual(sql, \"django_date_trunc('month', date_field)\")\n", "    def test_adapt_datetimefield_value(self):\n        naive_dt = timezone.make_naive(timezone.now())\n        self.assertEqual(connection.ops.adapt_datetimefield_value(naive_dt), str(naive_dt))\n\n        with self.assertRaises(ValueError):\n            connection.ops.adapt_datetimefield_value(timezone.now())\n"], "sample_1152": ["def test_powdenest_with_symbols():\n    i, j = symbols('i j', integer=True)\n    eq = ((x**(2*i))**(3*y))**x\n    result = powdenest(eq, force=True)\n    expected = x**(6*i*x*y)\n    assert result == expected\n", "def test_powdenest_with_symbols():\n    x, y, z = symbols('x y z')\n    assert powdenest((x**y)**z, force=True) == x**(y*z)\n", "def test_powdenest_with_addition_in_exponent():\n    x, y, z = symbols('x y z', positive=True)\n    assert powdenest(exp(3*(log(x) + log(y)))) == x**3*y**3\n", "def test_powdenest_with_sum_of_logs():\n    a, b, y = symbols('a b y', positive=True)\n    result = powdenest(exp(3*(log(a) + log(b))*y))\n    expected = (a*b)**(3*y)\n    assert result == expected\n", "def test_powdenest_with_symbols():\n    x, y, z = symbols('x y z', positive=True)\n    i = Symbol('i', integer=True)\n    assert powdenest((x**(2*i)*y**(4*i))**z, force=True) == (x*y**2)**(2*i*z)\n"], "sample_934": ["def test_enum_output():\n    check(\"enum\", \"enum {{ key }}\", {1: \"enum-key\"}, \"enum key\", \"key\")\n", "def test_function_pointer_declaration():\n    input = \"int (*func)(int, char);\"\n    idDict = {1: \"TestDoc_0func\"}\n    output = \"int ( * func )(int, char)\"\n    check(\"function\", input, idDict, output)\n", "def test_enum_object():\n    check('enum', '{{key}}enum E {{ {{a, 1}}, {{b, 2}}, {{c, 3}} }}',\n          {1: 'E', 2: 'E', 3: 'E'},\n          'enum {{key}}E {{ a = 1, b = 2, c = 3 }}')\n", "def test_parse_expression():\n    string = \"(int) i * 2 + 3 * j / (k + 1)\"\n    ast = parse('expression', string)\n    assert str(ast) == \"((int) i * 2 + 3 * j / (k + 1))\"\n", "def test_parse_member_with_template():\n    check('member',\n          '{key}std::vector<std::string>::iterator i',\n          {1: 'm_std_1a_string_0_iterator_i'},\n          'std::vector<std::string>::iterator {key}i',\n          asTextOutput='std::vector<std::string>::iterator i')\n"], "sample_132": ["    def test_get_traceback_data(self):\n        request = RequestFactory().get('/')\n        exc_type = Exception\n        exc_value = Exception('Test exception')\n        tb = exc_value.__traceback__\n        reporter = ExceptionReporter(request, exc_type, exc_value, tb)\n        data = reporter.get_traceback_data()\n        self.assertIsInstance(data, dict)\n        self.assertEqual(data['exception_type'], 'Exception')\n        self.assertEqual(data['exception_value'], 'Test exception')\n", "    def test_get_traceback_frames(self):\n        request = RequestFactory().get('/')\n        try:\n            raise Exception('Test exception')\n        except Exception:\n            exc_type, exc_value, tb = sys.exc_info()\n\n        reporter = ExceptionReporter(request, exc_type, exc_value, tb)\n        frames = reporter.get_traceback_frames()\n\n        self.assertIsInstance(frames, list)\n        self.assertGreater(len(frames), 0)\n        self.assertIn('function', frames[0])\n", "    def test_get_traceback_data(self):\n        request = RequestFactory().get('/')\n        exc_type = ValueError\n        exc_value = ValueError('Test error')\n        tb = exc_value.__traceback__\n        reporter = ExceptionReporter(request, exc_type, exc_value, tb)\n        data = reporter.get_traceback_data()\n        self.assertIn('is_email', data)\n        self.assertIn('frames', data)\n        # Add more assertions based on the expected output of get_traceback_data()\n", "    def test_get_traceback_frames(self):\n        try:\n            raise ValueError(\"Test Exception\")\n        except ValueError:\n            etype, evalue, tb = sys.exc_info()\n        reporter = ExceptionReporter(None, etype, evalue, tb)\n        frames = reporter.get_traceback_frames()\n        self.assertIsInstance(frames, list)\n        self.assertTrue(all(isinstance(frame, dict) for frame in frames))\n        for frame in frames:\n            self.assertIn('tb', frame)\n            self.assertIn('type', frame)\n            self.assertIn('filename', frame)\n            self.assertIn('function', frame)\n            self.assertIn('lineno', frame)\n            self.assertIn('vars', frame)\n            self.assertIn('id', frame)\n            self.assertIn('pre_context', frame)\n            self.assertIn('context_line', frame)\n            self.assertIn('post_context', frame)\n            self.assertIn('pre_context_lineno', frame)\n", "    def test_get_traceback_data(self):\n        request = RequestFactory().get('/')\n        exception = ValueError(\"Test exception\")\n        tb = exception.__traceback__\n        reporter = ExceptionReporter(request, ValueError, exception, tb)\n        data = reporter.get_traceback_data()\n        self.assertIn('is_email', data)\n        self.assertIn('frames', data)\n        self.assertIn('request', data)\n        # Add more assertions based on the expected data structure\n"], "sample_731": ["def test_fetch_california_housing_return_X_y():\n    check_return_X_y(partial(fetch_california_housing, return_X_y=True))\n", "def test_fetch_california_housing_return_X_y():\n    \"\"\"Test return_X_y option in fetch_california_housing.\"\"\"\n    check_return_X_y(partial(fetch, return_X_y=True))\n", "def test_fetch_california_housing_return_X_y():\n    \"\"\"Test fetch_california_housing with return_X_y=True\"\"\"\n    check_return_X_y(partial(fetch, return_X_y=True))\n", "def test_fetch_california_housing_return_X_y():\n    \"\"\"Test that fetch_california_housing returns (data, target) when return_X_y is True.\"\"\"\n    X, y = fetch(return_X_y=True)\n    assert X.shape[0] == y.shape[0]\n    assert X.shape[1] == 8\n", "def test_california_housing_return_X_y():\n    # Test the return_X_y option\n    X, y = fetch_california_housing(return_X_y=True)\n    assert isinstance(X, np.ndarray)\n    assert isinstance(y, np.ndarray)\n    assert X.shape[0] == y.shape[0]\n    assert X.shape[1] == 8  # number of features\n"], "sample_603": ["def test_array_repr(dataarray):\n    repr_html = fh.array_repr(dataarray)\n    assert isinstance(repr_html, str)\n    assert \"<div class='xr-wrap' hidden>\" in repr_html\n    assert \"<pre class='xr-text-repr-fallback'>\" in repr_html\n", "def test_array_repr(dataarray):\n    html_repr = fh.array_repr(dataarray)\n    assert isinstance(html_repr, str)\n    assert \"<div class='xr-wrap' hidden>\" in html_repr\n    assert f\"<div class='xr-obj-type'>xarray.{type(dataarray).__name__}</div>\" in html_repr\n", "def test_dataarray_repr_html(dataarray):\n    html_repr = fh.array_repr(dataarray)\n    assert isinstance(html_repr, str)\n    assert html_repr.startswith(\"<div>\")\n    assert html_repr.endswith(\"</div>\")\n    assert \"xr-obj-type\" in html_repr\n    assert \"xr-array-name\" in html_repr\n    assert \"xr-dim-list\" in html_repr\n    assert \"xr-section-summary\" in html_repr\n    assert \"xr-section-details\" in html_repr\n", "def test_dataset_repr(dataset):\n    ds_repr = fh.dataset_repr(dataset)\n    assert \"xarray.Dataset\" in ds_repr\n    assert \"Coordinates\" in ds_repr\n    assert \"Data variables\" in ds_repr\n    assert \"Attributes\" in ds_repr\n    assert \"description\" in ds_repr\n    assert \"time\" in ds_repr\n    assert \"location\" in ds_repr\n    assert \"tmin\" in ds_repr\n    assert \"tmax\" in ds_repr\n", "def test_dataset_repr_with_multiindex(dataset, multiindex):\n    dataset = dataset.assign_coords(multiindex=multiindex.multiindex)\n    repr_html = fh.dataset_repr(dataset)\n    assert 'MultiIndex' in repr_html\n    assert 'level_1' in repr_html\n    assert 'level_2' in repr_html\n"], "sample_935": ["compilation error", "def test_function_definition():\n    check('function', 'void {key}function(int a)',\n          {1: 'function-int'},\n          'void {key}function(int a)')\n", "def test_parse_declaration_enum_class():\n    input = \"enum class {key}Color {{ Red, Green, Blue }};\"\n    idDict = {1: \"Color\"}\n    output = \"enum class {key}Color {{ Red, Green, Blue }};\"\n    key = \"My\"\n    asTextOutput = \"enum class MyColor {Red, Green, Blue};\"\n    check(\"enum\", input, idDict, output, key, asTextOutput)\n", "def test_nested_name():\n    check(\"function\", \"void f(int A::* const &p)\", {}, \"void f(int A:: * const & p)\")\n", "def test_template_introduction():\n    input = \"std::ranges::sort<std::ranges::greater{}>(vec);\"\n    idDict = {1: \"sort_greater\"}\n    output = \"template <class T> std::ranges::sort<std::ranges::greater>(T &)\"\n    check(\"function\", input, idDict, output)\n"], "sample_923": ["compilation error", "def test_parse_function_declaration():\n    input = \"void func(int a, char b)\"\n    idDict = {1: \"func\", 2: \"func\", 3: \"func\"}\n    output = \"void func (int a, char b)\"\n    check(\"function\", input, idDict, output)\n", "def test_member_with_function_type():\n    input = \"void (TestClass::*func)()\"\n    idDict = {\n        1: \"TestClass::func\",\n        2: \"TestClass_1_1::func\",\n        3: \"TestClass_1_1::func_0_1\",\n        4: \"TestClass_1_1::func_0_1_1\"\n    }\n    output = \"void (TestClass::{key}func)()\"\n    key = \"TestClass::\"\n    check(\"member\", input, idDict, output, key)\n", "def test_namespace_scope():\n    root = Symbol(None, None, None, None, None, None)\n\n    ast = parse(\"namespace\", \"N::X\")\n    symbol = root.add_declaration(ast, docname=\"TestDoc\")\n    assert len(root._children) == 1\n    assert root._children[0].ident.name == \"N\"\n    assert len(root._children[0]._children) == 1\n    assert root._children[0]._children[0].ident.name == \"X\"\n\n    ast = parse(\"namespace\", \"N\")\n    symbol = root.add_declaration(ast, docname=\"TestDoc\")\n    assert len(root._children) == 1\n    assert root._children[0].ident.name == \"N\"\n    assert len(root._children[0]._children) == 1\n    assert root._children[0]._children[0].ident.name == \"X\"\n\n    ast = parse(\"member\", \"void f();\")\n    symbol = root._children[0].add_declaration(ast, docname=\"TestDoc\")\n    assert len(root._children) == 1\n    assert root._children[0].ident.name == \"N\"\n    assert len(root._children[0]._children) == 2\n    assert root._children[0]._children[1].ident.name == \"f\"\n\n    ast = parse(\"namespace\", \"N::Y\")\n    symbol = root.add_declaration(ast, docname=\"TestDoc\")\n    assert len(root._children) == 1\n    assert root._children[0].ident.name == \"N\"\n    assert len(root._children[0]._children) == 3\n    assert root._children[0]._children[2].ident.name == \"Y\"\n\n    ast = parse(\"member\", \"void g();\")\n    symbol = root._children[0]._children[2].add_declaration(ast, docname=\"TestDoc\")\n    assert len(root._children) == 1\n    assert root._children[0].ident.name == \"N\"\n    assert len(root._children[0]._children) == ", "def test_define_macro_with_parameters():\n    input = \"DEFINE_MACRO(MACRO_NAME, arg1, arg2)\"\n    output = \"DEFINE_MACRO(MACRO_NAME, arg1, arg2)\"\n    check('macro', input, {}, output)\n"], "sample_302": ["    def test_settings_to_cmd_args_env(self):\n        settings_dict = {\n            'HOST': 'localhost',\n            'PORT': 5432,\n            'NAME': 'test_db',\n            'USER': 'test_user',\n            'PASSWORD': 'test_password',\n            'OPTIONS': {\n                'passfile': 'test_passfile',\n                'service': 'test_service',\n                'sslmode': 'test_sslmode',\n                'sslrootcert': 'test_sslrootcert',\n                'sslcert': 'test_sslcert',\n                'sslkey': 'test_sslkey',\n            }\n        }\n        parameters = ['-c', 'SELECT 1']\n        expected_args = [\n            'psql',\n            '-U', 'test_user',\n            '-h', 'localhost',\n            '-p', '5432',\n            'test_db',\n            '-c', 'SELECT 1'\n        ]\n        expected_env = {\n            'PGPASSWORD': 'test_password',\n            'PGSERVICE': 'test_service',\n            'PGSSLMODE': 'test_sslmode',\n            'PGSSLROOTCERT': 'test_sslrootcert',\n            'PGSSLCERT': 'test_sslcert',\n            'PGSSLKEY': 'test_sslkey',\n            'PGPASSFILE': 'test_passfile',\n        }\n\n        args, env = self.client.settings_to_cmd_args_env(settings_dict, parameters)\n\n        self.assertEqual(args, expected_args)\n        self.assertEqual(env, expected_env)\n", "    def test_settings_to_cmd_args_env(self):\n        settings_dict = {\n            'HOST': 'localhost',\n            'PORT': 5432,\n            'NAME': 'mydb',\n            'USER': 'myuser',\n            'PASSWORD': 'mypassword',\n            'OPTIONS': {\n                'passfile': '/path/to/passfile',\n                'service': 'myservice',\n                'sslmode': 'require',\n                'sslrootcert': '/path/to/sslrootcert',\n                'sslcert': '/path/to/sslcert',\n                'sslkey': '/path/to/sslkey',\n            }\n        }\n        parameters = ['-c', 'SELECT 1']\n        expected_args = [\n            'psql',\n            '-U', 'myuser',\n            '-h', 'localhost',\n            '-p', '5432',\n            'mydb',\n            '-c', 'SELECT 1'\n        ]\n        expected_env = {\n            'PGPASSWORD': 'mypassword',\n            'PGSERVICE': 'myservice',\n            'PGSSLMODE': 'require',\n            'PGSSLROOTCERT': '/path/to/sslrootcert',\n            'PGSSLCERT': '/path/to/sslcert',\n            'PGSSLKEY': '/path/to/sslkey',\n            'PGPASSFILE': '/path/to/passfile'\n        }\n        args, env = self.client.settings_to_cmd_args_env(settings_dict, parameters)\n        self.assertEqual(args, expected_args)\n        self.assertEqual(env, expected_env)\n", "def test_runshell(self, mock_signal, mock_getsignal, mock_popen):\n    mock_getsignal.return_value = 'original_handler'\n    parameters = ['-c', 'SELECT 1;']\n\n    self.client.runshell(parameters)\n\n    mock_popen.assert_called_once_with(\n        ['psql'] + parameters,\n        env={\n            'PGPASSWORD': 'password',\n            'PGSERVICE': 'service',\n            'PGSSLMODE': 'require',\n            'PGSSLROOTCERT': 'sslrootcert',\n            'PGSSLCERT': 'sslcert',\n            'PGSSLKEY': 'sslkey',\n            'PGPASSFILE': 'passfile',\n        },\n        universal_newlines=True,\n    )\n    mock_signal.assert_has_calls([\n        mock.call(signal.SIGINT, signal.SIG_IGN),\n        mock.call(signal.SIGINT, 'original_handler'),\n    ])\n", "def test_runshell_sigint_handler(self, mock_settings_to_cmd_args_env):\n    mock_settings_to_cmd_args_env.return_value = ([], {})\n    with mock.patch('subprocess.Popen') as mock_popen:\n        self.client.runshell([])\n        mock_popen.assert_called_once()\n        self.assertEqual(mock_popen.call_args[0][1], 'ignore')\n", "    def test_runshell_calls_super_with_parameters(self, mock_settings_to_cmd_args_env):\n        mock_settings_to_cmd_args_env.return_value = ([], {})\n        parameters = ['-c', 'SELECT 1;']\n\n        with mock.patch.object(self.client, 'runshell_db') as mock_runshell_db:\n            self.client.runshell(parameters)\n\n        mock_runshell_db.assert_called_once_with(parameters)\n"], "sample_732": ["def test_fetch_kddcup99_subset_SF():\n    try:\n        data_sf = fetch_kddcup99(subset='SF', percent10=True, download_if_missing=False)\n        assert_equal(data_sf.data.shape[1], 4)\n        assert_equal(set(data_sf.target), {b'normal.', b'buffer_overflow.', b'loadmodule.', b'perl.', b'neptune.', b'smurf.', b'guess_passwd.', b'pod.', b'teardrop.', b'portsweep.', b'ipsweep.', b'land.', b'ftp_write.', b'back.', b'imap.', b'satan.', b'phf.', b'nmap.', b'multihop.', b'warezclient.', b'warezmaster.', b'spy.', b'rootkit.'})\n    except IOError:\n        raise SkipTest(\"Data not found and `download_if_missing` is False\")\n", "def test_fetch_kddcup99_subset_SA():\n    try:\n        data = fetch_kddcup99(subset='SA', percent10=True, download_if_missing=False)\n        assert_equal(data.data.shape, (195231, 41))\n        assert_equal(data.target.shape, (195231,))\n    except IOError:\n        raise SkipTest(\"Data not found\")\n", "def test_fetch_kddcup99_sf_http():\n    \"\"\"Test fetch_kddcup99 with SF subset and http filter.\"\"\"\n    try:\n        kddcup99 = fetch_kddcup99(subset='SF', percent10=True, return_X_y=True)\n    except IOError as e:\n        raise SkipTest(\"kddcup99 not downloaded: %s\" % e)\n\n    data, target = kddcup99\n    http_data = fetch_kddcup99(subset='http', percent10=True, return_X_y=True)[0]\n\n    assert_equal(data[data[:, 1] == b'http'], http_data)\n", "def test_kddcup99_subset_SF():\n    try:\n        kddcup99_sf = fetch_kddcup99(subset='SF', percent10=True)\n    except IOError:\n        raise SkipTest(\"Test data not found\")\n\n    assert_equal(kddcup99_sf.data.shape, (60000, 4))\n    assert_equal(kddcup99_sf.target.shape, (60000,))\n", "def test_fetch_kddcup99_subset_http():\n    try:\n        kddcup99_http = fetch_kddcup99(subset='http', percent10=True, download_if_missing=False)\n    except IOError:\n        raise SkipTest(\"kddcup99_10_data not found\")\n\n    assert_equal(kddcup99_http.data.shape, (619052, 3))\n    assert_equal(len(kddcup99_http.target), 619052)\n    check_return_X_y(partial(fetch_kddcup99, subset='http', percent10=True, download_if_missing=False))\n"], "sample_575": ["def test_continuous_tick_at(self, x):\n    a = self.setup_ticks(x, at=[2, 5])\n    assert_array_equal(a.get_majorticklocs(), [2, 5])\n", "def test_tick_locator(self, x):\n    locator = mpl.ticker.FixedLocator([0.2, 0.5, 0.8])\n    a = self.setup_ticks(x, locator=locator)\n    assert_array_equal(a.get_majorticklocs(), [0.2, 0.5, 0.8])\n", "def test_continuous_tick_at(self, x):\n    a = self.setup_ticks(x, at=[2, 4, 6])\n    assert_array_equal(a.get_majorticklocs(), [2, 4, 6])\n", "def test_ticks_locator(self, x):\n    locator = mpl.ticker.FixedLocator([0.2, 0.4, 0.6, 0.8])\n    a = self.setup_ticks(x, locator=locator)\n    assert_array_equal(a.get_majorticklocs(), [0.2, 0.4, 0.6, 0.8])\n", "    def test_tick_locator(self, x):\n        locator = mpl.ticker.MultipleLocator(0.5)\n        a = self.setup_ticks(x, locator=locator)\n        assert_array_equal(a.get_majorticklocs(), [0, 0.5, 1])\n"], "sample_926": ["def test_macro_with_parameters():\n    input = 'MACRO(a, b, ...)'\n    idDict = {}\n    output = 'MACRO(a, b, ...)'\n    check('macro', input, idDict, output)\n", "def test_function_pointer():\n    input = 'void (*func)(int, char)'\n    output = 'void (*func)(int, char)'\n    check('function', input, {1: 'func', 2: 'func'}, output)\n", "def test_parse_enumerator():\n    check(\"enumerator\", \"VALUE1\", {1: \"3VALUE1\", 2: \"2VALUE1\"}, \"VALUE1\")\n", "def test_parse_member_with_template_parameters():\n    input = \"template<typename T> T member;\"\n    idDict = {1: \"member\"}\n    output = \"member\"\n    _check(\"member\", input, idDict, output)\n", "def test_parse_var():\n    check('var', 'int x', {1: 'x'}, 'int x', 'x')\n    check('var', 'int x[3]', {1: 'x'}, 'int x[3]', 'x')\n    check('var', 'int *x', {1: 'x'}, 'int *x', 'x')\n    check('var', 'int **x', {1: 'x'}, 'int **x', 'x')\n    check('var', 'int x[2][3]', {1: 'x'}, 'int x[2][3]', 'x')\n    check('var', 'int x[2] = {1, 2}', {1: 'x'}, 'int x[2] = {1, 2}', 'x')\n    check('var', 'int x[2] = {[0] = 1, [1] = 2}', {1: 'x'}, 'int x[2] = {[0] = 1, [1] = 2}', 'x')\n    check('var', 'int x[2] = {[1] = 2}', {1: 'x'}, 'int x[2] = {[1] = 2}', 'x')\n    check('var', 'int x[2] = {1}', {1: 'x'}, 'int x[2] = {1}', 'x')\n    check('var', 'int x[] = {1, 2, 3}', {1: 'x'}, 'int x[] = {1, 2, 3}', 'x')\n    check('var', 'int x[][3] = {{1, 2, 3}, {4, 5, 6}}', {1: 'x'}, 'int x[][3] = {{1, 2, 3}, {4, 5, 6}}', 'x')\n    check('var', 'int x[][3] = {{1, 2, 3}, {4, 5}}', {1: 'x'}, 'int x[][3] = {{1, 2, 3}, {4, 5}}', 'x')\n    check('var', 'int x[2]["], "sample_279": ["    def test_check_constraint(self):\n        check_constraint = CheckConstraint(check=models.Q(age__gte=18), name='age_check')\n        self.assertEqual(check_constraint.name, 'age_check')\n        self.assertEqual(check_constraint.check, models.Q(age__gte=18))\n\n        # Test deconstruct method\n        path, args, kwargs = check_constraint.deconstruct()\n        self.assertEqual(path, 'django.db.models.CheckConstraint')\n        self.assertEqual(args, ())\n        self.assertEqual(kwargs, {'name': 'age_check', 'check': models.Q(age__gte=18)})\n\n        # Test clone method\n        cloned_constraint = check_constraint.clone()\n        self.assertEqual(cloned_constraint.name, 'age_check')\n        self.assertEqual(cloned_constraint.check, models.Q(age__gte=18))\n\n        # Test __eq__ method\n        self.assertEqual(check_constraint, cloned_constraint)\n\n        # Test __repr__ method\n        self.assertEqual(repr(check_constraint), \"<CheckConstraint: check=Q(age__gte=18) name='age_check'>\")\n", "    def setUp(self):\n        self.unique_constraint = UniqueConstraint(\n            fields=['name'],\n            name='unique_name',\n        )\n        self.model = UniqueConstraintProduct\n", "    def test_check_constraint(self):\n        with self.assertRaises(IntegrityError):\n            Product.objects.create(name='Invalid', price=-10)\n        product = Product.objects.create(name='Valid', price=10)\n        self.assertEqual(product.name, 'Valid')\n        self.assertEqual(product.price, 10)\n", "    def test_create_and_delete_check_constraint(self):\n        table_name = Product._meta.db_table\n        constraints_before = get_constraints(table_name)\n        self.assertNotIn('test_check_constraint', [c['name'] for c in constraints_before])\n\n        constraint = CheckConstraint(check=models.Q(price__gte=0), name='test_check_constraint')\n        with connection.schema_editor() as schema_editor:\n            schema_editor.add_constraint(Product, constraint)\n\n        constraints_after = get_constraints(table_name)\n        self.assertIn('test_check_constraint', [c['name'] for c in constraints_after])\n\n        with connection.schema_editor() as schema_editor:\n            schema_editor.remove_constraint(Product, constraint)\n\n        constraints_final = get_constraints(table_name)\n        self.assertNotIn('test_check_constraint', [c['name'] for c in constraints_final])\n", "    def test_check_constraint_sql(self):\n        check_constraint = CheckConstraint(check=Q(price__gte=0), name='check_price')\n        model = Product\n        with mock.patch('django.db.backends.utils.CursorWrapper') as mock_cursor_wrapper:\n            mock_schema_editor = mock.Mock()\n            mock_schema_editor.connection = connection\n            mock_schema_editor.quote_value.return_value = '0'\n            mock_schema_editor._check_sql.return_value = 'CHECK (price >= 0)'\n\n            sql = check_constraint.constraint_sql(model, mock_schema_editor)\n\n            self.assertEqual(sql, 'CHECK (price >= 0)')\n            mock_schema_editor._check_sql.assert_called_once_with('check_price', 'price >= 0')\n            mock_schema_editor.quote_value.assert_called_once_with(0)\n"], "sample_611": ["def test_offset_month(offset, expected_month):\n    assert offset.month == expected_month\n", "def test_apply_method_of_BaseCFTimeOffset(offset, expected_n):\n    other = cftime.DatetimeGregorian(2022, 1, 15)\n    with pytest.raises(NotImplementedError):\n        offset.__apply__(other)\n", "def test_subtraction(offset1, offset2, expected_n):\n    result = offset1 - offset2\n    assert result.n == expected_n\n", "def test_base_cftime_offset_add(offset, expected_n):\n    result = offset + offset\n    assert result.n == expected_n * 2\n", "def test_days_in_month(date_str, expected_day):\n    assert _days_in_month(date_str) == expected_day\n"], "sample_1064": ["def test_tensorflow_matrix_elementwise_operations():\n    expr = HadamardProduct(M, N) + Trace(P) - Determinant(Q)\n    _compare_tensorflow_matrix([M, N, P, Q], expr)\n", "def test_tensorflow_derivative():\n    expr = Derivative(sin(x) + cos(y), x)\n    _compare_tensorflow_scalar([x, y], expr)\n", "def test_tensorflow_einsum():\n    A = MatrixSymbol(\"A\", 2, 3)\n    B = MatrixSymbol(\"B\", 3, 4)\n    C = CodegenArrayTensorProduct(A, B)\n    expr = CodegenArrayContraction(C, (0, 1))\n    _compare_tensorflow_matrix([A, B], expr)\n", "def test_tensorflow_code_codegenarraytensorproduct():\n    a = MatrixSymbol(\"A\", 2, 3)\n    b = MatrixSymbol(\"B\", 3, 2)\n    expr = CodegenArrayTensorProduct(a, b)\n    code = tensorflow_code(expr)\n    expected_code = \"tensorflow.linalg.einsum(\\\"ij,jk->ik\\\", A, B)\"\n    assert code == expected_code\n", "def test_tensorflow_einsum_diagonal():\n    A = MatrixSymbol(\"A\", 3, 3)\n    expr = CodegenArrayDiagonal(A, diagonal_indices=[[0, 1]])\n    _compare_tensorflow_matrix([A], expr)\n"], "sample_948": ["def test_type_pointer():\n    check('type', 'int*', {1: 'int*'}, 'int *', key='int*')\n", "compilation error", "def test_function_with_default_parameter():\n    input = 'void func(int i = 10);'\n    idDict = {1: 'func', 2: 'func-int-i-10'}\n    output = 'void func(int i = 10)'\n    check('function', input, idDict, output)\n", "def test_function_declaration_with_requirements_clause():\n    input_str = \"void foo(int) requires (sizeof(int) > 2)\"\n    output_str = \"void foo(int) requires (sizeof(int) > 2)\"\n    check(\"function\", input_str, {1: \"foo\"}, output_str)\n", "def test_function_pointer_to_member():\n    input = \"void (Foo::*funcPtr)(int)\"\n    output = \"void (Foo::*funcPtr) (int)\"\n    idDict = {1: \"3Foo6funcPtr\"}\n    check(\"function\", input, idDict, output)\n"], "sample_1069": ["compilation error", "compilation error", "def test_print_Abs():\n    expr = Abs(x)\n    result = glsl_code(expr)\n    assert result == \"abs(x)\"\n", "def test_glsl_MatrixSolve():\n    A = Matrix([[1, 2], [3, 4]])\n    b = Matrix([5, 6])\n    x = Matrix([x, y])\n    result = MatrixSolve(A, b).doit()\n    expected = Matrix([-2, 2.5])\n    assert result == expected\n", "compilation error"], "sample_1125": ["def test_operator_multiplication():\n    A = Operator('A')\n    I = IdentityOperator()\n    assert A * I == A\n", "def test_differential_operator():\n    x = symbols('x')\n    f = Function('f')\n    d = DifferentialOperator(Derivative(f(x), x), f(x))\n    w = Wavefunction(x**2, x)\n    assert qapply(d*w) == Wavefunction(2, x)\n", "def test_differential_operator_wavefunction():\n    from sympy.physics.quantum.operator import DifferentialOperator\n    from sympy.physics.quantum.state import Wavefunction\n    from sympy import Symbol, Function, Derivative\n    x = Symbol('x')\n    f = Function('f')\n    d = DifferentialOperator(Derivative(f(x), x), f(x))\n    w = Wavefunction(x**2, x)\n    result = d._apply_operator_Wavefunction(w)\n    assert result == Wavefunction(2*x, x)\n", "def test_operator_multiplication():\n    A = Operator('A')\n    I = IdentityOperator()\n    assert A * I == A\n", "def test_operator_multiplication():\n    A = Operator('A')\n    I = IdentityOperator()\n    assert A * I == A\n"], "sample_723": ["compilation error", "compilation error", "compilation error", "compilation error", "def test_simple_imputer_strategy_most_frequent():\n    # Test strategy = \"most_frequent\"\n    X = np.array([[np.nan, 2, 3],\n                  [4, np.nan, 6],\n                  [7, 8, np.nan],\n                  [10, 11, 12]])\n    X_true = np.array([[2, 2, 3],\n                       [4, 6, 6],\n                       [7, 8, 8],\n                       [10, 11, 12]])\n    statistics = np.array([2, 6, 8])\n    _check_statistics(X, X_true, \"most_frequent\", statistics, np.nan)\n\n    # Test strategy = \"most_frequent\" with missing_values = 0\n    X = np.array([[0, 2, 3],\n                  [4, 0, 6],\n                  [7, 8, 0],\n                  [10, 11, 12]])\n    X_true = np.array([[2, 2, 3],\n                       [4, 6, 6],\n                       [7, 8, 8],\n                       [10, 11, 12]])\n    statistics = np.array([2, 6, 8])\n    _check_statistics(X, X_true, \"most_frequent\", statistics, 0)\n"], "sample_1142": ["def test_matrix_element_derivative():\n    M = MatrixSymbol('M', n, n)\n    v = MatrixSymbol('v', n, n)\n    i, j = symbols('i j', integer=True)\n    expr = MatrixElement(M, i, j)\n    derivative = expr.diff(v)\n    assert derivative == KroneckerDelta(i, v.args[1], (0, n-1)) * KroneckerDelta(j, v.args[2], (0, n-1))\n", "compilation error", "def test_matrix_element_derivative():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', m, l)\n    X = MatrixSymbol('X', n, l)\n    i, j = symbols('i j', integer=True)\n\n    M = A * B\n    dM = diff(M[i, j], X)\n    assert dM == KroneckerDelta(i, X[0], (0, n-1)) * KroneckerDelta(j, X[1], (0, l-1))\n", "def test_matrix_element_derivative():\n    M = MatrixSymbol('M', n, n)\n    N = MatrixSymbol('N', n, n)\n    v = MatrixSymbol('v', n, 1)\n    i, j = symbols('i j', cls=Dummy)\n    ME = MatrixElement(M, i, j)\n    vE = MatrixElement(v, i, 0)\n    assert diff(ME, vE) == KroneckerDelta(i, j, (0, n-1))\n    assert diff(ME, N) == ZeroMatrix(n, n)\n    assert diff(MatrixElement(Inverse(M), i, j), vE) == -Sum(Inverse(M)[i, k]*M[k, l].diff(vE)*Inverse(M)[l, j], (k, 0, n-1), (l, 0, n-1))\n", "def test_matrix_element_derivative():\n    i, j, h = symbols('i j h')\n    M = MatrixSymbol('M', n, m)\n    N = MatrixSymbol('N', n, m)\n    V = MatrixSymbol('V', n, 1)\n\n    ME = MatrixElement(M, i, j)\n    NE = MatrixElement(N, i, j)\n    VE = MatrixElement(V, i, 0)\n\n    # Test derivative with respect to another MatrixElement\n    assert ME.diff(NE) == KroneckerDelta(i, i, (0, n-1)) * KroneckerDelta(j, j, (0, m-1))\n\n    # Test derivative with respect to a MatrixSymbol\n    assert ME.diff(V) == M.diff(V)[i, j]\n\n    # Test derivative with respect to a scalar\n    assert ME.diff(x) == S.Zero\n\n    # Test derivative with respect to an inverse matrix\n    i1, i2 = symbols(\"z1, z2\", cls=Dummy)\n    assert ME.diff(Inverse(M)[i1, i2]).equals(-Sum(M[i, i1]*M[i1, i2].diff(ME)*M[i2, j], (i1, 0, n-1), (i2, 0, n-1)))\n"], "sample_309": ["    def test_none_in_sequence(self):\n        query = MultiValueDict({'a': ['1', None, '3']})\n        with self.assertRaisesMessage(TypeError, self.cannot_encode_none_msg):\n            urlencode(query, doseq=True)\n", "    def test_url_has_allowed_host_and_scheme(self):\n        # Test that the function returns True for a URL with an allowed host and scheme\n        self.assertTrue(url_has_allowed_host_and_scheme(\"https://example.com\", {\"example.com\"}))\n        # Test that the function returns False for a URL with a disallowed host\n        self.assertFalse(url_has_allowed_host_and_scheme(\"https://badexample.com\", {\"example.com\"}))\n        # Test that the function returns False for a URL with a disallowed scheme\n        self.assertFalse(url_has_allowed_host_and_scheme(\"http://example.com\", {\"example.com\"}, require_https=True))\n        # Test that the function returns False for a URL with a disallowed host and scheme\n        self.assertFalse(url_has_allowed_host_and_scheme(\"http://badexample.com\", {\"example.com\"}, require_https=True))\n        # Test that the function returns False for an empty URL\n        self.assertFalse(url_has_allowed_host_and_scheme(\"\", {\"example.com\"}))\n", "    def test_urlsafe_base64_decode(self):\n        encoded = urlsafe_base64_encode(b'test')\n        decoded = urlsafe_base64_decode(encoded)\n        self.assertEqual(decoded, b'test')\n\n        with self.assertRaises(ValueError):\n            urlsafe_base64_decode('invalid_base64_string')\n", "    def test_urlencode_multivaluedict(self):\n        mvd = MultiValueDict({'a': ['1', '2'], 'b': ['3']})\n        self.assertEqual(urlencode(mvd), 'a=1&a=2&b=3')\n", "    def test_urlsafe_base64_encode_decode(self):\n        original_string = b\"Hello, World!\"\n        encoded_string = urlsafe_base64_encode(original_string)\n        decoded_string = urlsafe_base64_decode(encoded_string)\n        self.assertEqual(decoded_string, original_string)\n"], "sample_1038": ["def test_matrix_symbol_entry():\n    i, j = symbols('i j', integer=True)\n    entry = A._entry(i, j)\n    assert isinstance(entry, MatrixElement)\n    assert entry.parent == A\n    assert entry.i == i\n    assert entry.j == j\n", "def test_matrix_derivative():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    expr = A * B\n    dA = MatrixSymbol('dA', 2, 2)\n    dB = MatrixSymbol('dB', 2, 2)\n    derivative = diff(expr, A)\n    assert derivative == dA * B\n    derivative = diff(expr, B)\n    assert derivative == A * dB\n", "def test_matrix_derivative():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', m, n)\n    X = MatrixSymbol('X', n, n)\n    Y = MatrixSymbol('Y', m, m)\n    expr = MatMul(A, X, B, Y)\n    result = diff(expr, X)\n    expected = MatMul(A, Identity(n), B, Y)\n    assert result == expected\n", "def test_matrix_element_derivative():\n    M = MatrixSymbol('M', n, n)\n    N = MatrixSymbol('N', n, n)\n    V = MatrixSymbol('V', n, 1)\n    dM = MatrixElement(M, i, j)\n    dV = MatrixElement(V, i, 0)\n    dN = MatrixElement(N, i, j)\n    assert dM.diff(dV) == KroneckerDelta(i, j)\n    assert dM.diff(dN) == S.Zero\n    assert (M*V).diff(dM) == KroneckerDelta(i, j)*V[j]\n", "def test_matrix_element_derivative():\n    X = MatrixSymbol('X', n, n)\n    dX = MatrixSymbol('dX', n, n)\n    ME = MatrixElement(X, i, j)\n    dME = diff(ME, dX)\n    assert dME == KroneckerDelta(i, dX.args[1]) * KroneckerDelta(j, dX.args[2])\n"], "sample_431": ["def test_model_instance_with_explicit_pk_and_deferred_fields(self):\n    article = Article(id=1, headline='Test Headline')\n    self.assertEqual(article.headline, 'Test Headline')\n    self.assertEqual(article.pk, 1)\n    self.assertIn('body', article.get_deferred_fields())\n", "    def test_validate_unique_with_exclude(self):\n        # Create an Article instance with a unique title\n        article = Article(title=\"Test Article\")\n        article.save()\n\n        # Try to create another Article instance with the same title, but exclude the title field\n        article2 = Article(title=\"Test Article\")\n        # This should not raise a ValidationError because the title field is excluded\n        article2.full_clean(exclude=[\"title\"])\n", "def test_custom_manager_used_by_default(self):\n    class ArticleWithCustomManager(models.Model):\n        title = models.CharField(max_length=200)\n        articles = models.Manager()\n\n    obj = ArticleWithCustomManager(title=\"Test Article\")\n    self.assertIsInstance(obj.articles, models.Manager)\n", "def test_save_with_update_fields(self):\n    article = Article.objects.create(headline='Original headline', pub_date=datetime.now())\n    original_id = article.id\n\n    article.headline = 'Updated headline'\n    article.save(update_fields=['headline'])\n\n    updated_article = Article.objects.get(id=original_id)\n    self.assertEqual(updated_article.headline, 'Updated headline')\n    # Ensure other fields remain unchanged\n    self.assertEqual(updated_article.pub_date, article.pub_date)\n", "    def test_create_instance_with_defaults(self):\n        # Test creating a new instance of Article with default values\n        article = Article(title=\"Test Article\", content=\"This is a test article.\")\n        article.save()\n\n        # Verify that the instance was created with the default values\n        self.assertIsNotNone(article.id)\n        self.assertEqual(article.title, \"Test Article\")\n        self.assertEqual(article.content, \"This is a test article.\")\n        self.assertEqual(article.pub_date, datetime.now().date())\n        self.assertEqual(article.status, \"draft\")\n"], "sample_604": ["def test_format_timedelta(self):\n    # Test formatting of timedelta objects\n    td = pd.Timedelta(days=2, hours=3, minutes=4, seconds=5)\n    assert formatting.format_timedelta(td) == \"2 days 03:04:05\"\n    assert formatting.format_timedelta(td, timedelta_format=\"date\") == \"2 days\"\n    assert formatting.format_timedelta(td, timedelta_format=\"time\") == \"03:04:05\"\n", "def test_last_n_items():\n    array = np.array([1, 2, 3, 4, 5])\n    assert np.array_equal(formatting.last_n_items(array, 3), np.array([3, 4, 5]))\n    assert np.array_equal(formatting.last_n_items(array, 0), np.array([]))\n    assert np.array_equal(formatting.last_n_items(array, 6), np.array([1, 2, 3, 4, 5]))\n", "def test_format_timedelta(self):\n    timedelta = pd.Timedelta(days=1, hours=2, minutes=3, seconds=4)\n    assert formatting.format_timedelta(timedelta, \"date\") == \"1 days\"\n    assert formatting.format_timedelta(timedelta, \"time\") == \"02:03:04\"\n    assert formatting.format_timedelta(timedelta) == \"1 days 02:03:04\"\n", "def test_format_timestamp(self):\n    dt = pd.Timestamp(\"2022-01-01T12:34:56\")\n    assert formatting.format_timestamp(dt) == \"2022-01-01T12:34:56\"\n\n    # Test for Timestamp out of bounds\n    dt_out_of_bounds = pd.Timestamp(\"1000-01-01T12:34:56\")\n    assert formatting.format_timestamp(dt_out_of_bounds) == str(dt_out_of_bounds)\n\n    # Test for non-Timestamp input\n    assert formatting.format_timestamp(\"2022-01-01T12:34:56\") == \"2022-01-01T12:34:56\"\n", "def test_short_data_repr(self):\n    # Test short_data_repr function for numpy array\n    arr = np.random.rand(10, 10)\n    data_repr = formatting.short_data_repr(arr)\n    assert isinstance(data_repr, str)\n\n    # Test short_data_repr function for xarray DataArray\n    da = xr.DataArray(arr, dims=('x', 'y'))\n    data_repr = formatting.short_data_repr(da)\n    assert isinstance(data_repr, str)\n\n    # Test short_data_repr function for large xarray DataArray\n    large_arr = np.random.rand(1000000,)\n    large_da = xr.DataArray(large_arr, dims=('x',))\n    data_repr = formatting.short_data_repr(large_da)\n    assert \"[1000000 values with dtype=float64]\" in data_repr\n"], "sample_917": ["def test_namespace_object_parsing():\n    parser = DefinitionParser(\"namespace ns::ns1\", location=None, config=None)\n    ast = parser.parse_namespace_object()\n    assert str(ast.nestedName) == \"ns::ns1\"\n    assert ast.templatePrefix is None\n", "compilation error", "def test_nested_namespace_declaration():\n    check(\"namespace\", \"namespace ns1 { namespace ns2 { } }\", {1: \"ns1::ns2\"}, \"namespace ns1::ns2\")\n", "def test_cpp_domain_typedef_alias():\n    check(\"type\", \"using foo = int;\", {1: \"foo\"})\n", "def test_template_template_parameter():\n    check(\"function\",\n          \"template<template<typename T> class Container> void fn(const Container<int>&);\",\n          {1: \"fn-1\"})\n"], "sample_1159": ["def test_failing_assumptions():\n    x = Symbol('x', real=True, positive=True)\n    y = Symbol('y')\n    assert failing_assumptions(6*x + y, real=True, positive=True) == {'positive': None, 'real': None}\n", "def test_common_assumptions():\n    exprs = [sqrt(2), 2, pi, oo, I]\n    common = common_assumptions(exprs)\n    assert common == {'commutative': True, 'composite': False, 'odd': False, 'imaginary': False}\n", "def test_common_assumptions():\n    exprs = [sqrt(2), 2, pi, oo, I]\n    common = common_assumptions(exprs)\n    assert common == {'commutative': True, 'composite': False, 'even': False, 'imaginary': False, 'odd': False}\n", "def test_common_assumptions():\n    exprs = [sqrt(2), 2, pi, oo, I, -1, 0]\n    assert common_assumptions(exprs) == {'commutative': True, 'composite': False, 'finite': True, 'imaginary': False, 'integer': False, 'odd': False}\n", "def test_symbol_assumptions():\n    x = Symbol('x', real=True, positive=True)\n    assert assumptions(x) == {'real': True, 'positive': True, 'complex': True, 'commutative': True, 'finite': True, 'extended_real': True, 'extended_positive': True, 'extended_nonnegative': True, 'extended_nonzero': True, 'nonnegative': True, 'nonzero': True, 'nonpositive': False, 'negative': False, 'extended_negative': False, 'integer': False, 'even': False, 'odd': False, 'zero': False, 'rational': False, 'algebraic': False, 'irrational': False, 'transcendental': False, 'composite': False, 'prime': False, 'hermitian': True, 'antihermitian': False, 'imaginary': False, 'infinite': False}\n"], "sample_1173": ["def test_function_exponentiation():\n    transformations = standard_transformations + (function_exponentiation,)\n    assert parse_expr('sin**4(x)', transformations=transformations) == sin(x)**4\n", "def test_convert_xor():\n    tokens = [(OP, '^')]\n    result = convert_xor(tokens, {}, {})\n    assert result == [(OP, '**')]\n", "def test_parse_expr_function_exponentiation():\n    transformations = standard_transformations + (function_exponentiation,)\n    result = parse_expr('sin**4(x)', transformations=transformations)\n    assert result == sin(x)**4\n", "def test_convert_equals_signs():\n    expr = parse_expr(\"a=b=c\", transformations=(standard_transformations + (convert_equals_signs,)))\n    assert expr == Eq(Eq(a, b), c)\n", "def test_convert_equals_signs():\n    expr = parse_expr(\"(1*2=x)=False\", transformations=(standard_transformations + (convert_equals_signs,)))\n    assert expr == Eq(Eq(Mul(Integer(1), Integer(2)), Symbol('x')), Boolean(False))\n"], "sample_1034": ["def test_oracle_gate_representation():\n    numqubits = 2\n    f = return_one_on_two\n    v = OracleGate(numqubits, f)\n    matrix = represent(v, basis='z')\n    expected_matrix = Matrix([[1, 0, 0, 0],\n                             [0, 1, 0, 0],\n                             [0, 0, -1, 0],\n                             [0, 0, 0, 1]])\n    assert matrix == expected_matrix\n", "def test_wgate_representation():\n    numqubits = 2\n    wgate = WGate(numqubits)\n    computed_matrix = represent(wgate, basis=IntQubit)\n    expected_matrix = Matrix([\n        [1/sqrt(2), 1/sqrt(2), 1/sqrt(2), 1/sqrt(2)],\n        [1/sqrt(2), -1/sqrt(2), 1/sqrt(2), -1/sqrt(2)],\n        [1/sqrt(2), 1/sqrt(2), -1/sqrt(2), -1/sqrt(2)],\n        [1/sqrt(2), -1/sqrt(2), -1/sqrt(2), 1/sqrt(2)]\n    ])\n    assert computed_matrix == expected_matrix\n", "def test_WGate():\n    wgate = WGate(2)\n    qubits = IntQubit(0)\n    result = qapply(wgate*qubits)\n    expected_result = (2/sqrt(4))*superposition_basis(2) - IntQubit(0)\n    assert result == expected_result\n", "def test_wgate_representation():\n    nqubits = 2\n    wgate = WGate(nqubits)\n    basis_states = [IntQubit(i, nqubits=nqubits) for i in range(2**nqubits)]\n\n    # Calculate the expected result manually\n    expected_result = Matrix([[2/(sqrt(2**nqubits))] * 2**nqubits for _ in range(2**nqubits)])\n    expected_result -= Matrix.eye(2**nqubits)\n\n    # Compare with the result from the WGate\n    assert represent(wgate, basis_states) == expected_result\n", "def test_oracle_gate():\n    numqubits = 2\n    f = return_one_on_two\n    v = OracleGate(numqubits, f)\n    assert qapply(v*IntQubit(2)) == -IntQubit(2)\n    assert qapply(v*IntQubit(3)) == IntQubit(3)\n"], "sample_437": ["def test_set_autocommit(self):\n    wrapper = BaseDatabaseWrapper({\n        'NAME': 'test',\n        'USER': 'test',\n        'PASSWORD': 'test',\n        'HOST': 'localhost',\n        'PORT': '',\n        'AUTOCOMMIT': True,\n        'CONN_MAX_AGE': 0,\n        'CONN_HEALTH_CHECKS': False,\n        'TIME_ZONE': None,\n    })\n    wrapper.connect()\n\n    # Verify initial autocommit state\n    self.assertTrue(wrapper.get_autocommit())\n\n    # Disable autocommit\n    wrapper.set_autocommit(False)\n    self.assertFalse(wrapper.get_autocommit())\n\n    # Enable autocommit\n    wrapper.set_autocommit(True)\n    self.assertTrue(wrapper.get_autocommit())\n\n    wrapper.close()\n", "def test_ensure_timezone(self):\n    wrapper = BaseDatabaseWrapper({\n        'NAME': ':memory:',\n        'ENGINE': 'django.db.backends.sqlite3',\n        'TIME_ZONE': 'UTC',\n    })\n    with patch.object(wrapper, 'connection') as mock_connection:\n        mock_connection.tzname = 'Europe/London'\n        self.assertTrue(wrapper.ensure_timezone())\n        mock_connection.execute.assert_called_once_with(\"PRAGMA timezone = 'UTC'\")\n\n    wrapper = BaseDatabaseWrapper({\n        'NAME': ':memory:',\n        'ENGINE': 'django.db.backends.sqlite3',\n        'TIME_ZONE': 'UTC',\n    })\n    with patch.object(wrapper, 'connection') as mock_connection:\n        mock_connection.tzname = 'UTC'\n        self.assertFalse(wrapper.ensure_timezone())\n        mock_connection.execute.assert_not_called()\n", "def test_on_commit(self):\n    mock_func = MagicMock()\n    with self.settings(DATABASES={\"default\": {\"ENGINE\": \"django.db.backends.dummy\"}}):\n        conn = connections[\"default\"]\n        with transaction.atomic():\n            conn.on_commit(mock_func)\n            self.assertEqual(mock_func.call_count, 0)\n        mock_func.assert_called_once()\n", "def test_close_if_unusable_or_obsolete(self, mock_is_usable):\n    # Create a mock connection\n    connection = MagicMock()\n\n    # Create a BaseDatabaseWrapper instance\n    wrapper = BaseDatabaseWrapper({})\n    wrapper.connection = connection\n\n    # Test when the connection is unusable\n    mock_is_usable.return_value = False\n    wrapper.close_if_unusable_or_obsolete()\n    connection.close.assert_called_once()\n\n    # Reset the mock and test when the connection is usable\n    connection.close.reset_mock()\n    mock_is_usable.return_value = True\n    wrapper.errors_occurred = True\n    wrapper.close_if_unusable_or_obsolete()\n    connection.close.assert_not_called()\n    self.assertFalse(wrapper.errors_occurred)\n", "    def test_thread_safety_validation(self):\n        wrapper = BaseDatabaseWrapper({}, DEFAULT_DB_ALIAS)\n        with patch(\"django.db.backends.base.base._thread.get_ident\", return_value=\"original\"):\n            wrapper.validate_thread_sharing()  # No error, same thread\n\n        with patch(\"django.db.backends.base.base._thread.get_ident\", return_value=\"different\"):\n            with self.assertRaises(DatabaseError):\n                wrapper.validate_thread_sharing()  # Error, different thread\n\n        wrapper.inc_thread_sharing()\n        with patch(\"django.db.backends.base.base._thread.get_ident\", return_value=\"different\"):\n            wrapper.validate_thread_sharing()  # No error, thread sharing allowed\n        wrapper.dec_thread_sharing()\n"], "sample_1155": ["def test_construct_domain_complex_numbers():\n    domain, result = construct_domain([2 + 3*I, 4 - I])\n    assert domain == QQ_I\n    assert result == [2 + 3*I, 4 - I]\n", "def test_construct_domain_with_golden_ratio():\n    domain, result = construct_domain([GoldenRatio])\n    assert domain == EX\n    assert result == [EX(GoldenRatio)]\n", "def test_rational_complex_numbers():\n    domain, coeffs = construct_domain([Rational(1, 2) + I*Rational(3, 4), Rational(5, 6) - I*Rational(7, 8)])\n    assert domain == QQ_I\n    assert coeffs == [Rational(1, 2) + I*Rational(3, 4), Rational(5, 6) - I*Rational(7, 8)]\n", "def test_complex_numbers():\n    coeffs = [2 + 3*I, 4 - I, 5*I]\n    domain, elements = construct_domain(coeffs)\n    assert domain == CC\n    assert elements == [2 + 3*I, 4 - I, 5*I]\n", "def test_construct_domain_complex_numbers():\n    domain, elements = construct_domain([2 + 3*I, 1 - I])\n    assert domain == CC\n    assert elements == [2 + 3*I, 1 - I]\n"], "sample_1037": ["def test_matmul_doit():\n    A = MatrixSymbol('A', 2, 2)\n    B = Matrix([[1, 2], [3, 4]])\n    C = MatrixSymbol('C', 2, 2)\n    X = MatMul(A, B, C)\n    Y = X.doit()\n    assert Y == MatMul(A*B, C)\n", "def test_matmul_derivative():\n    X = MatrixSymbol('X', n, n)\n    Y = MatrixSymbol('Y', n, n)\n    expr = MatMul(X, Y)\n    derivative = diff(expr, X)\n    assert derivative == MatMul(Y.T, X)\n", "def test_matmul_determinant():\n    \"\"\"Test the _eval_determinant method of MatMul class\"\"\"\n    assert A.determinant() * B.determinant() * C.determinant() == MatMul(A, B, C).determinant()\n", "def test_matmul_doit():\n    A = MatrixSymbol('A', 2, 2)\n    B = Matrix([[1, 2], [3, 4]])\n    X = MatMul(A, B)\n    result = X.doit()\n    assert result == Matrix([[A[0, 0] + 2*A[0, 1], A[0, 0] + 2*A[0, 1]], [3*A[1, 0] + 4*A[1, 1], 3*A[1, 0] + 4*A[1, 1]]))\n", "def test_matmul_eval_derivative_matrix_lines():\n    from sympy.matrices.expressions.derivative import Derivative\n    from sympy.matrices.expressions.linearmatrix import LinearMatrix\n    X = MatrixSymbol('X', 2, 2)\n    Y = MatrixSymbol('Y', 2, 2)\n    expr = MatMul(X, Y)\n    result = expr._eval_derivative_matrix_lines(x)\n    expected = [\n        LinearMatrix(MatMul(Transpose(Y), Derivative(X, x)), MatMul(Transpose(Y), Transpose(X))),\n        LinearMatrix(MatMul(Transpose(X), Derivative(Y, x)), MatMul(Transpose(X), Transpose(Y)))\n    ]\n    assert result == expected\n"], "sample_1063": ["def test_matrix_expression():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    expr = A * B\n    f = lambdify((A, B), expr, 'numpy')\n    A_np = numpy.array([[1, 2], [3, 4]])\n    B_np = numpy.array([[5, 6], [7, 8]])\n    result = f(A_np, B_np)\n    expected = numpy.dot(A_np, B_np)\n    assert numpy.array_equal(result, expected)\n", "def test_lambdify_custom_printer():\n    # Test lambdify with a custom printer\n    custom_printer = NumPyPrinter({'fully_qualified_modules': False, 'inline': True})\n    f = lambdify(x, sqrt(x), printer=custom_printer)\n    assert f(4) == 2\n", "def test_lambdify_different_arguments():\n    expr = sin(x) + cos(x)\n    f = lambdify(x, expr, 'numpy')\n    a = numpy.array([1, 2])\n    assert numpy.allclose(f(a), [1.38177329, 0.49315059])\n\n    f = lambdify([x, (y, z)], x + y + z)\n    assert f(1, (2, 3)) == 6\n\n    f = lambdify(x, [x, [x + 1, x + 2]])\n    assert f(1) == [1, [2, 3]]\n\n    f = lambdify(x, Matrix([x, x + 1]))\n    assert numpy.array_equal(f(1), numpy.array([[1], [2]]))\n", "compilation error", "def test_lambdify_tensorflow_immutabledensematrix():\n    A = MatrixSymbol('A', 2, 2)\n    f = lambdify((A,), A, 'tensorflow')\n    result = f(tensorflow.constant([[1.0, 2.0], [3.0, 4.0]]))\n    assert result.numpy().tolist() == [[1.0, 2.0], [3.0, 4.0]]\n"], "sample_586": ["def test_concat_dataarray_different_dims():\n    arr1 = DataArray(np.random.rand(3, 4), dims=(\"x\", \"y\"))\n    arr2 = DataArray(np.random.rand(3, 5), dims=(\"x\", \"z\"))\n    with raises_regex(ValueError, \"dimensions or coordinates of inputs are different\"):\n        concat([arr1, arr2], dim=\"w\")\n", "def test_dataset_concat_with_identical_compat():\n    ds1 = Dataset({'x': ('a', [1, 2, 3])})\n    ds2 = Dataset({'x': ('a', [4, 5, 6])})\n    result = concat([ds1, ds2], dim='a', compat='identical')\n    expected = Dataset({'x': ('a', [1, 2, 3, 4, 5, 6])})\n    assert_equal(result, expected)\n\n    ds3 = Dataset({'x': ('a', [1, 2, 3]), 'y': ('b', [7, 8])})\n    with pytest.raises(ValueError):\n        concat([ds1, ds3], dim='a', compat='identical')\n", "def test_dataset_concat_with_dim_coord_not_in_all_datasets():\n    ds1 = Dataset({'x': ('time', [1, 2, 3])}, coords={'time': ('time', [10, 20, 30])})\n    ds2 = Dataset({'x': ('time', [4, 5, 6])})\n\n    with pytest.raises(ValueError):\n        concat([ds1, ds2], dim='time')\n", "def test_concat_with_identical_compat():\n    # Create test data\n    data1 = Dataset({'var1': ('dim1', np.arange(5))}, attrs={'attr1': 'value1'})\n    data2 = Dataset({'var1': ('dim1', np.arange(5, 10))}, attrs={'attr1': 'value1'})\n    datasets = [data1, data2]\n\n    # Concatenate datasets with identical compatibility\n    result = concat(datasets, dim='dim1', compat='identical')\n\n    # Check the result\n    expected = Dataset({'var1': ('dim1', np.arange(10))}, attrs={'attr1': 'value1'})\n    assert_identical(result, expected)\n", "def test_dataarray_concat_fill_value():\n    x = DataArray([1, 2, 3], dims=\"x\")\n    y = DataArray([4, 5], dims=\"x\")\n    expected = DataArray([1, 2, 3, dtypes.NA, 4, 5], dims=\"x\")\n    result = concat([x, y], dim=\"x\", fill_value=dtypes.NA)\n    assert_identical(result, expected)\n"], "sample_780": ["def test_latent_dirichlet_allocation_fit_transform():\n    n_components, X = _build_sparse_mtx()\n    lda = LatentDirichletAllocation(n_components=n_components, random_state=0)\n    lda.fit(X)\n    transformed_X = lda.transform(X)\n    assert transformed_X.shape == (n_components * 3, n_components)\n    assert_almost_equal(np.sum(transformed_X), n_components * 3)\n", "def test_transform_before_fit():\n    \"\"\"Test transform before fit raises an error.\"\"\"\n    X = np.random.rand(10, 20)\n    lda = LatentDirichletAllocation()\n    with assert_raises_regexp(NotFittedError, 'This LatentDirichletAllocation instance is not fitted yet'):\n        lda.transform(X)\n", "def test_lda_fit_transform_with_sparse_input():\n    n_components, X = _build_sparse_mtx()\n    lda = LatentDirichletAllocation(n_components=n_components,\n                                    random_state=0)\n    doc_topic_distr = lda.fit_transform(X)\n    assert_equal(doc_topic_distr.shape, (9, n_components))\n    assert_allclose(doc_topic_distr.sum(axis=1), np.ones(9))\n", "def test_latent_dirichlet_allocation_partial_fit():\n    n_components, X = _build_sparse_mtx()\n    lda = LatentDirichletAllocation(n_components=n_components, random_state=0)\n    lda.partial_fit(X[:5], total_samples=X.shape[0])\n    assert_equal(lda.n_iter_, 1)\n    assert_equal(lda.n_batch_iter_, 2)\n    assert_equal(lda.components_.shape, (n_components, X.shape[1]))\n", "def test_lda_partial_fit_with_sparse_input():\n    n_components, X = _build_sparse_mtx()\n    lda = LatentDirichletAllocation(n_components=n_components, random_state=0)\n    lda.partial_fit(X, total_samples=X.shape[0])\n    assert_equal(lda.components_.shape, (n_components, X.shape[1]))\n    assert_equal(lda.n_batch_iter_, 2)\n    assert_almost_equal(lda.components_.sum(axis=1),\n                        np.full(n_components, n_components))\n"], "sample_1075": ["def test_beta_function_diff():\n    x = Symbol('x')\n    y = Symbol('y')\n\n    # Test differentiation with respect to x\n    assert diff(beta(x, y), x) == beta(x, y)*(digamma(x) - digamma(x + y))\n\n    # Test differentiation with respect to y\n    assert diff(beta(x, y), y) == beta(x, y)*(digamma(y) - digamma(x + y))\n\n    # Test differentiation with respect to an invalid argument index\n    raises(ArgumentIndexError, lambda: diff(beta(x, y), 3))\n", "def test_beta_differentiation():\n    x, y = Symbol('x'), Symbol('y')\n    assert diff(beta(x, y), x) == beta(x, y) * (digamma(x) - digamma(x + y))\n    assert diff(beta(x, y), y) == beta(x, y) * (digamma(y) - digamma(x + y))\n", "def test_beta_argument_index_error():\n    x, y = Symbol('x'), Symbol('y')\n    with raises(ArgumentIndexError):\n        beta(x, y).fdiff(3)\n", "def test_beta_function():\n    x, y = Symbol('x'), Symbol('y')\n\n    # Test the mirror symmetry property\n    assert conjugate(beta(x, y)) == beta(conjugate(x), conjugate(y))\n\n    # Test differentiation with respect to x\n    assert diff(beta(x, y), x) == beta(x, y) * (digamma(x) - digamma(x + y))\n\n    # Test differentiation with respect to y\n    assert diff(beta(x, y), y) == beta(x, y) * (digamma(y) - digamma(x + y))\n\n    # Test the relation between beta and gamma functions\n    assert expand_func(beta(x, y)) == gamma(x) * gamma(y) / gamma(x + y)\n\n    # Test the evaluation of beta function for integral values\n    assert beta(3, 4) == gamma(3) * gamma(4) / gamma(7)\n\n    # Test ArgumentIndexError for invalid argument index\n    with raises(ArgumentIndexError):\n        beta(x, y).fdiff(3)\n", "def test_beta_function_properties():\n    x, y = Symbol('x'), Symbol('y')\n\n    # Test the property B(a, 1) = 1/a\n    assert beta(x, 1) == 1/x\n\n    # Test the symmetry property B(a, b) = B(b, a)\n    assert beta(x, y) == beta(y, x)\n\n    # Test the property B(a, b) = Gamma(a)*Gamma(b)/Gamma(a+b)\n    assert beta(x, y) == expand_func(beta(x, y), func=gamma)\n"], "sample_906": ["def test_parse_declaration_function_pointer():\n    check(\"function\", \"void (*func)(int)\",\n          {1: \"func\", 2: \"func\", 3: \"func\"},\n          \"void (*func)(int)\")\n", "def test_parse_macro():\n    check(\"macro\", \"MACRO\", {1: \"m_MACRO\"}, \"macro MACRO()\", \"MACRO\")\n", "def test_function_pointer_without_name():\n    input = \"void (*)()\"\n    output = \"void ()\"\n    idDict = {1: \"void()\"}\n    key = None\n    asTextOutput = None\n    _check(\"function\", input, idDict, output, key, asTextOutput)\n", "def test_function_pointer_typedef():\n    check('function',\n          'void (*funcPtr)(int, double)',\n          {2: 'funcPtr', 3: 'funcPtr'},\n          'void ( *funcPtr )(int, double)')\n", "def test_complex_enum():\n    check(\"enum\", \"E{{ e1, e2 = 3, e3 = e1 + e2 }}\", {1: \"E\"})\n"], "sample_825": ["def test_nipals_twoblocks_inner_loop():\n    # Test the _nipals_twoblocks_inner_loop function\n    rng = check_random_state(0)\n    X = rng.randn(100, 3)\n    Y = rng.randn(100, 2)\n    x_weights, y_weights, n_iter = pls_._nipals_twoblocks_inner_loop(X, Y)\n    assert x_weights.shape == (3, 1)\n    assert y_weights.shape == (2, 1)\n    assert isinstance(n_iter, int)\n", "def test_pls_n_components_greater_than_X_shape():\n    X = np.array([[0., 0., 1.],\n                 [1.,0.,0.],\n                 [2.,2.,2.],\n                 [2.,5.,4.]])\n    Y = np.array([[0.1, -0.2],\n                 [0.9, 1.1],\n                 [6.2, 5.9],\n                 [11.9, 12.3]])\n    pls = PLSRegression(n_components=5)\n    with pytest.raises(ValueError) as excinfo:\n        pls.fit(X, Y)\n    assert \"Invalid number of components\" in str(excinfo.value)\n", "def test_pls_nipals_inner_loop_y_score_normalization():\n    rng = check_random_state(0)\n    X = rng.rand(100, 2)\n    Y = rng.rand(100, 1)\n\n    _, y_weights, _ = pls._nipals_twoblocks_inner_loop(X, Y, mode=\"A\", max_iter=500, tol=1e-06, norm_y_weights=True)\n\n    # Ensure y_weights are normalized to one\n    assert_array_almost_equal(np.linalg.norm(y_weights), 1.0)\n", "def test_pls_regression_deflation():\n    X = np.array([[0., 0., 1.], [1., 0., 0.], [2., 2., 2.], [2., 5., 4.]])\n    Y = np.array([[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]])\n    pls2 = PLSRegression(n_components=2, deflation_mode=\"regression\")\n    pls2.fit(X, Y)\n    assert_array_almost_equal(np.dot(pls2.x_scores_, pls2.x_loadings_.T), X - np.dot(pls2.y_scores_, pls2.y_loadings_.T), decimal=6)\n", "def test_pls_regression_multivariate():\n    rng = check_random_state(0)\n    X = rng.rand(100, 10)\n    Y = rng.rand(100, 3)\n\n    pls = PLSRegression(n_components=2)\n    pls.fit(X, Y)\n\n    assert_equal(pls.x_scores_.shape, (100, 2))\n    assert_equal(pls.y_scores_.shape, (100, 2))\n    assert_equal(pls.x_weights_.shape, (10, 2))\n    assert_equal(pls.y_weights_.shape, (3, 2))\n    assert_equal(pls.x_loadings_.shape, (10, 2))\n    assert_equal(pls.y_loadings_.shape, (3, 2))\n    assert_equal(pls.coef_.shape, (10, 3))\n    assert_equal(pls.x_rotations_.shape, (10, 2))\n    assert_equal(pls.y_rotations_.shape, (3, 2))\n"], "sample_1004": ["def test_condition_set_subs():\n    c = ConditionSet(x, x < 1, {x, z})\n    assert c.subs(x, y) == ConditionSet(x, x < 1, {y, z})\n    assert c.subs(x, y).subs(x, y) == ConditionSet(y, y < 1, {y, z})\n    assert ConditionSet(y, y < 1, {y, z}).subs(y, 1) == ConditionSet(y, y < 1, {z})\n    raises(ValueError, lambda: c.subs(x, f(y)))\n", "def test_condition_set_subs():\n    c = ConditionSet(x, x < 1, {x, z})\n    assert c.subs(x, y) == ConditionSet(x, x < 1, {y, z})\n    assert c.subs(x, y).subs(x, w) == ConditionSet(w, w < 1, {w, z})\n", "def test_condition_set_subs():\n    c = ConditionSet(x, x < 1, {x, z})\n    assert c.subs(x, y) == ConditionSet(x, x < 1, {y, z})\n    assert c.subs(x, y).subs(x, w) == ConditionSet(w, w < 1, {y, z})\n    raises(ValueError, lambda: c.subs(x, 1))\n", "def test_condition_set_with_expression():\n    expr = x + 1\n    cond = expr < 1\n    base_set = S.Integers\n    cs = ConditionSet(expr, cond, base_set)\n    assert expr in cs\n    assert x + 1 not in cs\n", "def test_ConditionSet_substitution():\n    cond = ConditionSet(x, x < 1, {x, y, z})\n    result = cond.subs(x, y)\n    assert result == ConditionSet(x, x < 1, {y, z})\n    result = result.subs(x, y)\n    assert result == ConditionSet(y, y < 1, {y, z})\n    result = ConditionSet(y, y < 1, {y, z}).subs(y, 1)\n    assert result == ConditionSet(y, y < 1, {z})\n    result = result.subs(y, 1)\n    assert result == ConditionSet(y, y < 1, {z})\n"], "sample_958": ["def test_parse_namespace_object():\n    parser = DefinitionParser(\"namespace::subnamespace::Name\", location=None, config=None)\n    name = parser.parse_namespace_object()\n    parser.assert_end()\n    assert str(name) == \"namespace::subnamespace::Name\"\n", "compilation error", "def test_function_member_parsing():\n    check('function', 'void f()', {1: 'f', 2: 'f'}, 'void f( )')\n    check('member', 'void f', {1: 'f', 2: 'f'}, 'void f')\n", "def test_function_pointer_declaration():\n    check('function', 'void (*f)()',\n          {1: 'f', 2: 'f', 3: 'f'},\n          'void (*f)()', key='f', asTextOutput='void (*f)()')\n", "compilation error"], "sample_303": ["    def setUp(self):\n        self.client = BaseDatabaseClient(connection=connection)\n", "    def test_runshell(self, mock_settings_to_cmd_args_env, mock_subprocess_run):\n        mock_settings_to_cmd_args_env.return_value = (['mock_command'], {'MOCK_ENV_VAR': 'mock_value'})\n        self.client.runshell(['mock_parameter'])\n        mock_settings_to_cmd_args_env.assert_called_once_with(self.client.connection.settings_dict, ['mock_parameter'])\n        mock_subprocess_run.assert_called_once_with(['mock_command'], env={'**os.environ**': None, 'MOCK_ENV_VAR': 'mock_value'}, check=True)\n", "    def test_runshell(self, mock_settings_to_cmd_args_env, mock_subprocess_run):\n        mock_settings_to_cmd_args_env.return_value = (['mock_arg'], {'mock_env': 'value'})\n        mock_subprocess_run.return_value = None\n\n        self.client.runshell(parameters={})\n\n        mock_settings_to_cmd_args_env.assert_called_once_with(self.client.connection.settings_dict, {})\n        mock_subprocess_run.assert_called_once_with(['mock_arg'], env={'mock_env': 'value'})\n", "    def test_runshell(self, mock_subprocess_run):\n        parameters = {'param1': 'value1'}\n        settings_dict = {'setting1': 'value1'}\n        self.client.connection.settings_dict = settings_dict\n\n        with mock.patch.object(self.client, 'settings_to_cmd_args_env', return_value=([], {})):\n            self.client.runshell(parameters)\n            mock_subprocess_run.assert_called_once_with([], env=None, check=True)\n\n        with mock.patch.object(self.client, 'settings_to_cmd_args_env', return_value=(['cmd'], {'env1': 'value1'})):\n            self.client.runshell(parameters)\n            mock_subprocess_run.assert_called_with(['cmd'], env={'env1': 'value1', **os.environ}, check=True)\n", "def test_runshell(self, mock_subprocess_run):\n    parameters = {'-p': '5432', '-U': 'user'}\n    expected_args = ['psql', '-p', '5432', '-U', 'user']\n    expected_env = {'PGHOST': 'localhost', 'PGUSER': 'user'}\n\n    with mock.patch.object(self.client, 'settings_to_cmd_args_env', return_value=(expected_args, expected_env)):\n        self.client.runshell(parameters)\n\n        mock_subprocess_run.assert_called_once_with(expected_args, env={**os.environ, **expected_env}, check=True)\n"], "sample_1126": ["def test_dagger_power():\n    A = Operator('A')\n    assert Dagger(A**2) == Dagger(A)**2\n", "def test_dagger_powers_sums_products():\n    A = Operator('A')\n    B = Operator('B')\n    assert Dagger(A*B) == Dagger(B)*Dagger(A)\n    assert Dagger(A+B) == Dagger(A) + Dagger(B)\n    assert Dagger(A**2) == Dagger(A)**2\n", "def test_dagger_properties():\n    A = Operator('A')\n    B = Operator('B')\n\n    # Test distributivity\n    assert Dagger(A + B) == Dagger(A) + Dagger(B)\n\n    # Test conjugation of a complex number\n    z = 1 + 2*I\n    assert Dagger(z) == conjugate(z)\n\n    # Test conjugation of a matrix\n    m = Matrix([[1, I], [2, I]])\n    assert Dagger(m) == m.conjugate().transpose()\n\n    # Test conjugation of a sum of matrices\n    n = Matrix([[3, 1], [1, 0]])\n    assert Dagger(m + n) == m.conjugate().transpose() + n.conjugate().transpose()\n\n    # Test conjugation of a product of matrices\n    assert Dagger(m*n) == n.conjugate().transpose() * m.conjugate().transpose()\n\n    # Test multiplication with IdentityOperator\n    assert Dagger(A) * IdentityOperator() == Dagger(A)\n", "def test_dagger_properties():\n    A = Operator('A')\n    B = Operator('B')\n    assert Dagger(A * B) == Dagger(B) * Dagger(A)\n    assert Dagger(A + B) == Dagger(A) + Dagger(B)\n    assert Dagger(A**2) == Dagger(A)**2\n", "def test_dagger_matrix():\n    m = Matrix([[1, 2*I], [3, 4*I]])\n    expected_result = Matrix([[1, 3], [-2*I, -4*I]])\n    assert Dagger(m) == expected_result\n"], "sample_1117": ["def test_ask_square_handler():\n    assert ask(Q.square(X), assumptions=None) == True\n    assert ask(Q.square(Y), assumptions=None) == False\n    assert ask(Q.square(Identity(2)), assumptions=None) == True\n    assert ask(Q.square(ZeroMatrix(2, 2)), assumptions=None) == True\n    assert ask(Q.square(OneMatrix(2, 2)), assumptions=None) == True\n    assert ask(Q.square(DiagonalMatrix([1, 2])), assumptions=None) == True\n    assert ask(Q.square(DiagMatrix([1, 2])), assumptions=None) == True\n", "def test_ask_square_handler():\n    assert ask(Q.square(X), Q.square(X))\n    assert not ask(Q.square(Y))\n    assert ask(Q.square(Z))\n    assert ask(Q.square(A1x1))\n    assert ask(Q.square(B1x1))\n    assert ask(Q.square(C0x0))\n    assert ask(Q.square(Identity(2)))\n    assert not ask(Q.square(ZeroMatrix(2, 3)))\n    assert ask(Q.square(OneMatrix(2)))\n    assert not ask(Q.square(V1))\n    assert not ask(Q.square(V2))\n", "def test_MatMul_elements_with_integer_matrices():\n    assert ask(Q.integer_elements(MatMul(A1x1, X)), Q.integer_elements(A1x1) & Q.integer_elements(X))\n", "def test_AskDiagonalHandler():\n    assert ask(Q.diagonal(X), Q.diagonal(X))\n    assert ask(Q.diagonal(A1x1), Q.diagonal(A1x1))\n    assert ask(Q.diagonal(C0x0), Q.diagonal(C0x0))\n    assert ask(Q.diagonal(DiagonalMatrix([1, 2])), Q.diagonal(DiagonalMatrix([1, 2])))\n    assert ask(Q.diagonal(DiagMatrix([1, 2])), Q.diagonal(DiagMatrix([1, 2])))\n    assert ask(Q.diagonal(Identity(2)), Q.diagonal(Identity(2)))\n    assert ask(Q.diagonal(ZeroMatrix(2, 2)), Q.diagonal(ZeroMatrix(2, 2)))\n    assert ask(Q.diagonal(OneMatrix(1, 1)), Q.diagonal(OneMatrix(1, 1)))\n    assert ask(Q.diagonal(MatrixSlice(X, 0, 1, 0, 1)), Q.diagonal(MatrixSlice(X, 0, 1, 0, 1)))\n    assert not ask(Q.diagonal(MatrixSlice(X, 0, 1, 0, 2)), Q.diagonal(MatrixSlice(X, 0, 1, 0, 2)))\n    assert not ask(Q.diagonal(MatrixSlice(X, 0, 2, 0, 2)), Q.diagonal(MatrixSlice(X, 0, 2, 0, 2)))\n", "def test_AskDiagonalHandler():\n    assert ask(Q.diagonal(DiagMatrix([1, 2, 3]))) == True\n    assert ask(Q.diagonal(DiagonalMatrix([1, 2, 3]))) == True\n    assert ask(Q.diagonal(ZeroMatrix(2, 2))) == True\n    assert ask(Q.diagonal(OneMatrix(1, 1))) == True\n    assert ask(Q.diagonal(Identity(2))) == True\n    assert ask(Q.diagonal(MatrixSlice(X, 0, 0))) == True\n    assert ask(Q.diagonal(X.T)) == ask(Q.diagonal(X))\n    assert ask(Q.diagonal(X.I)) == ask(Q.diagonal(X))\n    assert ask(Q.diagonal(X ** 2), Q.diagonal(X)) == True\n    assert ask(Q.diagonal(X + Y), Q.diagonal(X), Q.diagonal(Y)) == True\n"], "sample_1035": ["def test_measure_partial():\n    from sympy.physics.quantum.qubit import Qubit, measure_partial\n    from sympy.physics.quantum.gate import H\n\n    c = H(0)*H(1)*Qubit('00')\n    q = qapply(c)\n    result = measure_partial(q, (0,))\n    assert len(result) == 2\n    for state, prob in result:\n        assert prob >= 0 and prob <= 1\n        assert isinstance(state, Qubit)\n", "def test_measure_partial():\n    from sympy.physics.quantum.qubit import Qubit, measure_partial\n    from sympy.physics.quantum.gate import H\n    q = H(0)*H(1)*Qubit('00')\n    result = measure_partial(qapply(q), (0,))\n    # add assertions to check the expected outcome\n", "def test_measure_partial():\n    from sympy.physics.quantum.qubit import Qubit, measure_partial\n    from sympy.physics.quantum.gate import H\n\n    # Create a bell state\n    c = H(0) * Qubit('01')\n    q = qapply(c)\n\n    # Perform a partial measurement on the first qubit\n    result = measure_partial(q, (0,))\n\n    # The result should be a list of two tuples, each containing a qubit state and its probability\n    # Since we measured the first qubit, the second qubit should be the same in both states\n    assert len(result) == 2\n    assert all(isinstance(state, Qubit) for state, _ in result)\n    assert all(isinstance(prob, float) for _, prob in result)\n    assert result[0][0] != result[1][0]\n    assert result[0][0][1] == result[1][0][1]\n", "def test_measure_partial():\n    from sympy.physics.quantum.qubit import Qubit, measure_partial\n    from sympy.physics.quantum.gate import H, X, Y, Z\n    from sympy.physics.quantum.qapply import qapply\n\n    c = H(0)*H(1)*Qubit('00')\n    q = qapply(c)\n    result = measure_partial(q, (0,))\n\n    # Assert that the sum of probabilities is 1\n    prob_sum = sum(prob for _, prob in result)\n    assert prob_sum == 1\n\n    # Assert that the result is a list of tuples, each containing a Qubit and a probability\n    assert all(isinstance(res, tuple) and len(res) == 2 and isinstance(res[0], Qubit) for res in result)\n", "def test_measure_partial():\n    from sympy.physics.quantum.qubit import Qubit, measure_partial\n    from sympy.physics.quantum.gate import H, X\n    from sympy.physics.quantum.qapply import qapply\n\n    # Create a superposition of two qubits\n    q = H(0)*H(1)*Qubit('00')\n    q = qapply(q)\n\n    # Perform a partial measurement on the first qubit\n    result = measure_partial(q, (0,))\n\n    # Check that the result is a list of tuples with two elements each\n    assert isinstance(result, list)\n    assert all(isinstance(item, tuple) and len(item) == 2 for item in result)\n\n    # Check that the probabilities sum up to 1\n    total_probability = sum(prob for _, prob in result)\n    assert abs(total_probability - 1) < 1e-6\n\n    # Check that the measured states are correct\n    expected_states = [Qubit('00'), Qubit('10')]\n    measured_states = [state for state, _ in result]\n    assert all(state in expected_states for state in measured_states)\n"], "sample_1116": ["def test_Inverse_properties():\n    assert Inverse(A) * A == Identity(n)\n    assert A * Inverse(A) == Identity(n)\n    assert Inverse(Inverse(C)) == C\n    assert Inverse(C * D) == Inverse(D) * Inverse(C)\n", "def test_inverse_properties():\n    assert Inverse(A*B) == Inverse(B)*Inverse(A)\n    assert Inverse(C*D) == Inverse(D)*Inverse(C)\n    assert Inverse(C).inverse() == C\n    assert Inverse(C*Inverse(C)) == C\n", "def test_inverse_of_product():\n    assert (A*B).inverse() == B.inverse()*A.inverse()\n    assert Inverse(A*B) == Inverse(B)*Inverse(A)\n", "def test_Inverse_properties():\n    assert Inverse(C * D) == D.I * C.I\n    assert Inverse(C * E * D) == D.I * E.I * C.I\n    assert Inverse(C * D).shape == C.shape\n    assert Inverse(C).doit(deep=False) == C.inverse()\n    assert Inverse(MatPow(C, 2)).doit(inv_expand=False) == MatPow(C, -2)\n    with raises(NonSquareMatrixError):\n        Inverse(A)\n    with raises(TypeError):\n        Inverse(n)\n    assert Inverse(C * D)._eval_determinant() == 1 / (C * D).det()\n    assert Inverse(C * D)._eval_derivative_matrix_lines(n) == [-C.I * D.diff(n), C * D.I * D.diff(n)]\n", "def test_inverse_properties():\n    assert Inverse(C) == MatPow(C, S.NegativeOne)\n    assert Inverse(C*D) == MatPow(C*D, S.NegativeOne)\n    assert Inverse(Inverse(C)) == C\n    raises(TypeError, lambda: Inverse(n))\n    raises(NonSquareMatrixError, lambda: Inverse(A))\n"], "sample_779": ["    def fit(self, X, y=None):\n        X = check_array(X)\n        return self\n", "    def test_check_outlier_corruption(self):\n        # Test the check_outlier_corruption function\n        num_outliers = 35\n        expected_outliers = 30\n        decision = np.concatenate((np.full(num_outliers, -1), np.full(expected_outliers, 1)))\n        np.random.shuffle(decision)\n        with self.assertRaises(AssertionError) as context:\n            check_outlier_corruption(num_outliers, expected_outliers, decision)\n        self.assertTrue('The number of predicted outliers is not equal to the expected number of outliers and this difference is not explained by the number of ties in the decision_function values' in str(context.exception))\n", "    def test_check_outlier_corruption(self):\n        # Test case 1: num_outliers < expected_outliers\n        num_outliers = 10\n        expected_outliers = 15\n        decision = np.array([1, 2, 2, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 5])\n        with self.assertRaises(AssertionError) as cm:\n            check_outlier_corruption(num_outliers, expected_outliers, decision)\n        self.assertEqual(str(cm.exception), 'The number of predicted outliers is not equal to the expected number of outliers and this difference is not explained by the number of ties in the decision_function values')\n\n        # Test case 2: num_outliers > expected_outliers\n        num_outliers = 20\n        expected_outliers = 15\n        decision = np.array([1, 2, 2, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])\n        with self.assertRaises(AssertionError) as cm:\n            check_outlier_corruption(num_outliers, expected_outliers, decision)\n        self.assertEqual(str(cm.exception), 'The number of predicted outliers is not equal to the expected number of outliers and this difference is not explained by the number of ties in the decision_function values')\n\n        # Test case 3: num_outliers = expected_outliers with ties\n        num_outliers = 15\n        expected_outliers = 15\n        decision = np.array([1, 2, 2, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 5])\n", "    def fit(self, X, y):\n        X, y = check_X_y(X, y, dtype=None)\n        if isinstance(X, np.ndarray) and not X.dtype.names:\n            raise ValueError(\"Estimator requires feature names\")\n        return self\n", "    def fit(self, X, y=None):\n        X = check_array(X)\n        return self\n"], "sample_454": ["    def test_exclusion_constraint_validation(self):\n        product1 = Product.objects.create(name=\"Test\", range_start=1, range_end=5)\n        with self.assertRaises(ValidationError):\n            Product.objects.create(name=\"Test\", range_start=4, range_end=10)\n", "    def setUp(self):\n        self.product1 = Product.objects.create(name=\"Test1\", interval_start=1, interval_end=10)\n        self.product2 = Product.objects.create(name=\"Test2\", interval_start=5, interval_end=15)\n", "    def test_exclusion_constraint(self):\n        # Create a Product with ExclusionConstraint\n        product1 = Product.objects.create(\n            name=\"Test\", start_date=\"2022-01-01\", end_date=\"2022-01-05\"\n        )\n\n        # Try to create another Product with overlapping dates, which should raise a ValidationError\n        with self.assertRaises(ValidationError):\n            Product.objects.create(\n                name=\"Test\", start_date=\"2022-01-03\", end_date=\"2022-01-10\"\n            )\n\n        # Clean up\n        product1.delete()\n", "    def test_exclusion_constraint(self):\n        # Test the creation of ExclusionConstraint\n        Product.add_constraint(\n            ExclusionConstraint(\n                name=\"exclusion_constraint\",\n                expressions=[(\"date_from\", \"&&\"), (\"date_to\", \"&&\")],\n                index_type=\"GIST\",\n            )\n        )\n        constraints = get_constraints(Product._meta.db_table)\n        self.assertIn(\"exclusion_constraint\", constraints)\n\n        # Test the validation of ExclusionConstraint\n        with self.assertRaises(ValidationError):\n            Product.objects.create(date_from=\"2022-01-01\", date_to=\"2022-01-15\")\n            Product.objects.create(date_from=\"2022-01-10\", date_to=\"2022-01-20\")\n\n        # Test the removal of ExclusionConstraint\n        Product.remove_constraint(ExclusionConstraint(name=\"exclusion_constraint\"))\n        constraints = get_constraints(Product._meta.db_table)\n        self.assertNotIn(\"exclusion_constraint\", constraints)\n", "    def test_exclusion_constraint_creation(self):\n        constraints = get_constraints(ExclusionConstraintProduct._meta.db_table)\n        self.assertIn('exclusion_constraint_test', constraints)\n        constraint = constraints['exclusion_constraint_test']\n        self.assertEqual(constraint['columns'], ['timestamp'])\n        self.assertEqual(constraint['type'], 'exclusion')\n        self.assertEqual(constraint['index_type'], 'gist')\n        self.assertEqual(constraint['condition'], None)\n        self.assertEqual(constraint['deferrable'], None)\n        self.assertEqual(constraint['include'], ())\n"], "sample_1087": ["def test_swinnerton_dyer_poly():\n    assert swinnerton_dyer_poly(1, x) == x**2 - 2\n    assert swinnerton_dyer_poly(2, x) == x**4 - 10*x**2 + 1\n    assert swinnerton_dyer_poly(4, x).degree() == 4\n\n    # Test for non-symbolic input\n    assert swinnerton_dyer_poly(1, 2) == 2**2 - 2\n\n    # Test for negative order\n    raises(ValueError, lambda: swinnerton_dyer_poly(-1, x))\n\n    # Test for polys=True\n    assert swinnerton_dyer_poly(1, x, polys=True) == Poly(x**2 - 2, x)\n", "def test_swinnerton_dyer_poly():\n    assert swinnerton_dyer_poly(1) == x**2 - 2\n    assert swinnerton_dyer_poly(2) == x**4 - 10*x**2 + 1\n    assert swinnerton_dyer_poly(4) == (x**4 - 5*x**2 + 5)**2\n\n    with raises(ValueError):\n        swinnerton_dyer_poly(0)\n", "def test_swinnerton_dyer_poly():\n    assert swinnerton_dyer_poly(1, x) == x**2 - 2\n    assert swinnerton_dyer_poly(2, x) == x**4 - 10*x**2 + 1\n    assert swinnerton_dyer_poly(3, x) == x**8 - 40*x**6 + 352*x**4 - 960*x**2 + 576\n    assert swinnerton_dyer_poly(4, x) == x**16 - 104*x**12 + 3584*x**8 - 46080*x**4 + 245760\n\n    raises(ValueError, lambda: swinnerton_dyer_poly(0, x))\n    raises(ValueError, lambda: swinnerton_dyer_poly(-1, x))\n\n    # Testing with default x\n    assert swinnerton_dyer_poly(1).is_polynomial()\n    assert swinnerton_dyer_poly(2).is_polynomial()\n    assert swinnerton_dyer_poly(3).is_polynomial()\n\n    # Testing with polys=True\n    assert swinnerton_dyer_poly(1, x, polys=True) == Poly(x**2 - 2, x)\n    assert swinnerton_dyer_poly(2, x, polys=True) == Poly(x**4 - 10*x**2 + 1, x)\n    assert swinnerton_dyer_poly(3, x, polys=True) == Poly(x**8 - 40*x**6 + 352*x**4 - 960*x**2 + 576, x)\n", "def test_swinnerton_dyer_poly():\n    assert swinnerton_dyer_poly(1, x) == x**2 - 2\n    assert swinnerton_dyer_poly(2, x) == x**4 - 10*x**2 + 1\n    assert swinnerton_dyer_poly(4, x) == x**8 - 40*x**6 + 352*x**4 - 960*x**2 + 576\n    assert swinnerton_dyer_poly(1, x, polys=True) == Poly(x**2 - 2, x)\n\n    # Test for higher orders\n    for i in range(5, 10):\n        poly = swinnerton_dyer_poly(i, x)\n        assert poly.is_polynomial(x)\n        assert poly.degree(x) == 2**i\n\n    # Test for negative order and non-integer order\n    raises(ValueError, lambda: swinnerton_dyer_poly(0, x))\n    raises(ValueError, lambda: swinnerton_dyer_poly(-1, x))\n    raises(ValueError, lambda: swinnerton_dyer_poly(1.5, x))\n\n    # Test for symbolic base\n    base = symbols('a')\n    assert swinnerton_dyer_poly(2, base) == base**4 - 10*base**2 + 1\n", "def test_swinnerton_dyer_poly():\n    assert swinnerton_dyer_poly(1) == x**2 - 2\n    assert swinnerton_dyer_poly(2) == x**4 - 10*x**2 + 1\n    assert swinnerton_dyer_poly(4) == (x**8 - 10*x**6 + 35*x**4 - 40*x**2 + 1)\n\n    with raises(ValueError):\n        swinnerton_dyer_poly(0)\n    with raises(ValueError):\n        swinnerton_dyer_poly(-2)\n"], "sample_243": ["def test_build_lookup_with_transform(self):\n    query = Query(Author)\n    lhs = Author._meta.get_field('name').get_col('alias')\n    lookup_parts = ['lower', 'exact']\n    rhs = 'test'\n    lookup = query.build_lookup(lookup_parts, lhs, rhs)\n    self.assertIsInstance(lookup, Exact)\n    self.assertIsInstance(lookup.lhs, Lower)\n    self.assertEqual(lookup.rhs, 'test')\n", "def test_add_filter_with_lookup_expression(self):\n    query = Query(Author)\n    query.add_filter(('name__lower__exact', 'john'))\n    self.assertEqual(len(query.where.children), 1)\n    lookup = query.where.children[0]\n    self.assertIsInstance(lookup, Func)\n    self.assertEqual(lookup.function, Lower)\n    self.assertIsInstance(lookup.source_expressions[0], Col)\n    self.assertEqual(lookup.source_expressions[0].target.name, 'name')\n    self.assertIsInstance(lookup.source_expressions[0].target.output_field, CharField)\n    self.assertEqual(lookup.lookup_name, 'exact')\n    self.assertEqual(lookup.rhs, 'john')\n", "def test_build_filter_transform_exact(self):\n    q = Query(Author)\n    filter_expr = (('name__lower', 'test author'),)\n    clause, _ = q.build_filter(filter_expr)\n    self.assertEqual(len(clause.children), 1)\n    lookup = clause.children[0]\n    self.assertIsInstance(lookup, Exact)\n    self.assertIsInstance(lookup.lhs, Func)\n    self.assertIsInstance(lookup.lhs.source, Col)\n    self.assertEqual(lookup.lhs.source.target.name, 'name')\n    self.assertEqual(lookup.rhs, 'test author')\n", "def test_add_filter_with_lookup(self):\n    query = Query(Author)\n    query.add_filter(('first_name__startswith', 'J'))\n    self.assertEqual(len(query.where.children), 1)\n    filter_expr = query.where.children[0]\n    self.assertIsInstance(filter_expr, Q)\n    self.assertEqual(filter_expr.children, [(Col('first_name'), 'J')])\n    self.assertEqual(filter_expr.connector, AND)\n    self.assertEqual(filter_expr.negated, False)\n    lookup = filter_expr.children[0][1]\n    self.assertIsInstance(lookup, Func)\n    self.assertEqual(lookup.function, 'startswith')\n    self.assertEqual(lookup.source_expressions, [Col('first_name'), 'J'])\n", "def test_query_filter_transform(self):\n    query = Query(Author)\n    query.add_filter(('first_name__lower', 'john'))\n    self.assertEqual(len(query.where.children), 1)\n    lookup = query.where.children[0]\n    self.assertIsInstance(lookup, Exact)\n    self.assertIsInstance(lookup.lhs, Func)\n    self.assertEqual(lookup.lhs.function, Lower)\n    self.assertIsInstance(lookup.lhs.source, Col)\n    self.assertEqual(lookup.lhs.source.target.name, 'first_name')\n    self.assertEqual(lookup.rhs, 'john')\n"], "sample_1025": ["def test_print_Mod():\n    expr = Mod(x, y)\n    printer = PythonCodePrinter()\n    assert printer.doprint(expr) == \"x % y\"\n", "def test_print_Sign():\n    expr = sign(x)\n    code = pycode(expr)\n    assert code == \"math.copysign(1, x)\"\n", "def test_print_sympy_function():\n    expr = acos(x)\n    printer = SymPyPrinter()\n    printed_expr = printer.doprint(expr)\n    expected_expr = \"sympy.acos(x)\"\n    assert str(printed_expr) == expected_expr\n", "def test_print_Pow():\n    assert pycode(x**0.5) == \"sqrt(x)\"\n", "def test_print_Mod():\n    expr = Mod(x, y)\n    result = pycode(expr)\n    assert result == 'x % y'\n"], "sample_976": ["def test_symbol_call():\n    x = Symbol('x')\n    f = x()\n    assert f.func == Symbol('x')\n    assert f.args == ()\n", "def test_symbol_assumptions():\n    x = Symbol('x', real=True, positive=True)\n    assert x.is_real\n    assert x.is_positive\n\n    y = Symbol('y', real=False, positive=False)\n    assert not y.is_real\n    assert not y.is_positive\n\n    z = Symbol('z')\n    assert not z.is_real\n    assert not z.is_positive\n", "def test_symbol_matching():\n    x, y = symbols('x y')\n    a = Wild('a')\n    b = Wild('b', exclude=[x])\n\n    assert (3*x**2).match(a*x) == {a: 3*x}\n    assert (3*x**2).match(b*x) is None\n    assert a.match(b) == {b: a}\n\n    E = 2*x**3*y\n    assert E.match(a*b) == {a: 2*y, b: x**3}\n    assert E.match(Wild('a', exclude=[x, y])*b) == {a: y, b: 2*x**3}\n    assert E.match(Wild('a', exclude=[x, y, z])*b) == {a: 2, b: x**3*y}\n", "def test_symbol_assumptions():\n    x = Symbol('x', real=True, positive=True)\n    assert x.is_real\n    assert x.is_positive\n    y = Symbol('y', integer=True, nonnegative=True)\n    assert y.is_integer\n    assert y.is_nonnegative\n", "def test_symbol_properties():\n    x, y = symbols('x y')\n    assert x.is_commutative is True\n    assert y.is_commutative is True\n\n    A, B = symbols('A B', commutative=False)\n    assert A.is_commutative is False\n    assert B.is_commutative is False\n"], "sample_907": ["compilation error", "def test_complex_declaration():\n    input = \"int (*func(const char*))[10]\"\n    idDict = {1: \"func\"}\n    output = \"int (*{key}func(const char *))[10]\"\n    check(\"function\", input, idDict, output)\n", "def test_cpp_alias_object():\n    input_text = \".. cpp:alias:: AliasType\"\n    expected_output = \"<desc desctype='alias' domain='cpp' noindex='True'>\"\n\n    with restructuredtext.parse(input_text) as result:\n        assert_node(result, (desc, [addnodes.desc_signature, addnodes.desc_content]),\n                    desc_signature=expected_output)\n", "def test_function_parsing_overloaded_functions():\n    # Test parsing of overloaded functions\n    check('function',\n          'void func();',\n          {1: 'func'},\n          'void func()')\n    check('function',\n          'void func(int);',\n          {1: 'func_int'},\n          'void func(int)')\n    check('function',\n          'void func(double);',\n          {1: 'func_double'},\n          'void func(double)')\n    check('function',\n          'void func(int, double);',\n          {1: 'func_int_double'},\n          'void func(int, double)')\n    check('function',\n          'void func(double, int);',\n          {1: 'func_double_int'},\n          'void func(double, int)')\n    check('function',\n          'void func(...);',\n          {1: 'func'},\n          'void func(...)')\n    check('function',\n          'void func(int, ...);',\n          {1: 'func_int'},\n          'void func(int, ...)')\n    check('function',\n          'void func(const int&, ...);',\n          {1: 'func_const_int_'},\n          'void func(const int &, ...)')\n    check('function',\n          'void func(int *, ...);',\n          {1: 'func_int__ptr'},\n          'void func(int *, ...)')\n    check('function',\n          'void func(int &, ...);',\n          {1: 'func_int__'},\n          'void func(int &, ...)')\n    check('function',\n          'void func(const int *, ...);',\n          {1: 'func_const_int__ptr'},\n          'void func(const int *, ...)')\n    check('function',\n          'void func(const int &, ...);',\n          {1: 'func_const_int__'},\n          'void func(const int &, ...)')\n    check('function',\n          'void func(int (*)(...), ...);',\n          {1: '", "def test_cpp_domain_parse_type_function_pointer():\n    check('type',\n          '{key}void (*)(int)',\n          {1: 'void-void-int-'},\n          '{key}void(* )(int)')\n"], "sample_873": ["def test_step_selector():\n    selector = StepSelector(step=2)\n    selector.fit(X, y)\n\n    assert_array_equal(selector.get_support(), support)\n    assert_array_equal(selector.get_support(indices=True), support_inds)\n    assert_array_equal(selector.transform(X), Xt)\n    assert_array_equal(selector.inverse_transform(Xt), Xinv)\n    assert_array_equal(selector.get_feature_names_out(feature_names), feature_names_t)\n", "def test_inverse_transform_sparse():\n    selector = StepSelector(step=2)\n    selector.fit(X)\n    X_sparse = sp.csc_matrix(Xt)\n    X_reconstructed = selector.inverse_transform(X_sparse)\n    assert_array_equal(X_reconstructed.toarray(), Xinv)\n", "def test_transform():\n    sel = StepSelector(step=2).fit(X, y)\n    assert_array_equal(sel.transform(X), Xt)\n", "def test_step_selector_inverse_transform():\n    selector = StepSelector(step=2)\n    selector.fit(X, y)\n    Xt = selector.transform(X)\n    Xinv_test = selector.inverse_transform(Xt)\n    assert_array_equal(Xinv_test, Xinv)\n", "def test_step_selector(X, y, expected_support_mask, expected_transformed_X, expected_inverse_transformed_X, expected_feature_names_out):\n    selector = StepSelector(step=2)\n    selector.fit(X, y)\n\n    assert_array_equal(selector.get_support(), expected_support_mask)\n    assert_array_equal(selector.transform(X), expected_transformed_X)\n    assert_array_equal(selector.inverse_transform(expected_transformed_X), expected_inverse_transformed_X)\n    assert_array_equal(selector.get_feature_names_out(input_features=feature_names), expected_feature_names_out)\n"]}, "raw_scored_generations": {"sample_0": [true, true, true, true, false], "sample_3": [false, false, false, false, false], "sample_1": [true, false, true, false, false], "sample_5": [false, false, false, false, false], "sample_4": [false, false, false, false, false], "sample_2": [false, true, false, false, false], "sample_6": [true, true, false, false, false], "sample_7": [false, false, false, false, false], "sample_8": [false, false, true, false, false], "sample_9": [true, true, true, true, true], "sample_10": [false, false, false, false, false], "sample_11": [false, false, false, false, false], "sample_12": [true, true, false, false, false], "sample_13": [true, false, true, false, false], "sample_14": [true, true, false, true, false], "sample_15": [true, true, true, true, true], "sample_16": [false, false, false, false, false], "sample_17": [false, false, false, true, false], "sample_18": [false, false, false, false, false], "sample_19": [false, false, false, false, false], "sample_20": [false, false, false, false, false], "sample_21": [false, false, false, false, false], "sample_22": [false, false, false, false, false], "sample_23": [false, false, false, false, false], "sample_24": [false, false, false, false, false], "sample_25": [false, false, false, false, false], "sample_26": [false, false, false, false, false], "sample_27": [false, false, false, false, false], "sample_28": [false, false, false, false, false], "sample_29": [false, false, false, false, false], "sample_30": [false, false, false, false, false], "sample_31": [false, false, false, false, false], "sample_32": [false, true, true, true, true], "sample_33": [true, true, true, true, true], "sample_34": [false, true, true, true, false], "sample_35": [true, true, true, true, false], "sample_36": [true, true, true, false, false], "sample_37": [false, false, false, false, false], "sample_38": [false, false, false, false, false], "sample_39": [false, false, false, false, false], "sample_40": [false, false, true, false, false], "sample_41": [false, true, true, true, false], "sample_42": [true, false, true, false, false], "sample_43": [true, false, false, true, false], "sample_44": [false, false, false, false, false], "sample_45": [false, false, true, false, true], "sample_46": [false, false, false, false, false], "sample_47": [true, true, false, false, false], "sample_48": [true, true, true, true, true], "sample_49": [false, false, false, true, false], "sample_50": [true, false, true, false, true], "sample_51": [false, true, false, false, false], "sample_52": [true, false, true, true, false], "sample_54": [true, false, false, true, false], "sample_53": [false, false, false, false, false], "sample_55": [false, false, false, false, true], "sample_58": [false, false, true, true, true], "sample_56": [true, false, false, false, false], "sample_57": [true, true, true, false, true], "sample_59": [false, false, false, false, false], "sample_60": [true, false, true, true, false], "sample_61": [true, false, false, false, true], "sample_62": [false, true, false, true, false], "sample_63": [true, true, false, false, true], "sample_64": [true, true, true, true, true], "sample_65": [true, false, false, true, false], "sample_67": [false, false, false, false, false], "sample_66": [false, false, false, true, false], "sample_68": [false, false, false, false, false], "sample_69": [true], "sample_70": [true, false, false, false, true], "sample_71": [false, false, false, false, false], "sample_72": [false, false, false, false, false], "sample_73": [true, true, true, true, true], "sample_75": [true, true, true, true, true], "sample_74": [true, true, true, true, true], "sample_76": [true, true, true, false, false], "sample_77": [false, false, false, false, false], "sample_78": [false, false, false, false, false], "sample_79": [false, false, false, false, false], "sample_80": [false, false, false, false, false], "sample_82": [true, false, false, true, true], "sample_81": [false, true, true, true, false], "sample_83": [false, false, false, false, false], "sample_85": [true, true, true, true, true], "sample_84": [false, false, false, true, false], "sample_86": [false, false, false, false, false], "sample_88": [true, true, true, true, false], "sample_87": [true, false, true], "sample_89": [true], "sample_90": [false, true, true, true, false], "sample_91": [true, false, true, true, true], "sample_92": [true, true, false, true, true], "sample_93": [true, true, true, false, true], "sample_94": [false, false, false, false, false], "sample_95": [false, false, false, false, false], "sample_98": [false, false, false, false, false], "sample_96": [true, true, false, false, true], "sample_99": [true, true, false, true, false], "sample_97": [false, true, true, true, false], "sample_100": [true], "sample_102": [true, true, true, true, true], "sample_101": [false, false, false, false, false], "sample_103": [false, true, false, true, false], "sample_104": [true, true, true, true, false], "sample_107": [true, false, true, false, false], "sample_106": [true, false, true, true, false], "sample_105": [true, true, true, true, true], "sample_108": [false, false, false, false, false], "sample_109": [false, false, false, false, false], "sample_111": [true, true, true, true, true], "sample_110": [true, true, true, true, true], "sample_112": [true, true, false, false, false], "sample_113": [false, true, false, false, false], "sample_114": [true, true, true, true, true], "sample_115": [true, false, true, true, false], "sample_116": [false, false, false, false, false], "sample_117": [true, true, true, true, false], "sample_118": [true, true, false, false, true], "sample_119": [false, false, false, false, false], "sample_120": [false, false, false, false, false], "sample_121": [true, false, false, true, false], "sample_122": [false, true, false, true, false], "sample_123": [true, false, false, false, false], "sample_124": [false, false, false, false, false], "sample_125": [false, false, true, false, false], "sample_126": [true, true, true, true, true], "sample_127": [true, true, true, true, false], "sample_128": [false, false, false, false, false], "sample_129": [false, false, false, false, false], "sample_130": [false, false, false, false, false], "sample_131": [false, true, false, false, false], "sample_132": [true, true, true, true, true], "sample_133": [true, false, false, false, true], "sample_135": [true, false, true, false, false], "sample_134": [false, false, false, false, false], "sample_136": [false, false, false, false, false], "sample_139": [true, true, true, true, true], "sample_137": [false, false, false, false, false], "sample_138": [true, true, true, true, true], "sample_140": [false, false, false, false, false], "sample_141": [true, true, true, true, true], "sample_142": [false, false, false, false, false], "sample_143": [false, false, true, true, true], "sample_144": [false, false, false, false, false], "sample_145": [true, true, false, false, false], "sample_146": [true, true, false, true, true], "sample_147": [true, true, true, false, true], "sample_148": [false, false, false, false, true], "sample_151": [true, false, true, true, false], "sample_149": [false, false, false, false, false], "sample_152": [true, false, false, false, true], "sample_150": [false, false, false, false, false], "sample_153": [false, false, false, false, false], "sample_154": [false, false, false, false, false], "sample_155": [false, false, false, true, false], "sample_156": [false, false, true, false, true], "sample_157": [false, false, true, false, false], "sample_158": [false, false, false, false, true], "sample_159": [false, false, false, false, false], "sample_160": [false, false, false, false, false], "sample_161": [false, false, false, false, false], "sample_162": [true, false, false, true, true], "sample_163": [false, false, false, true, false], "sample_164": [false, false, false, false, false], "sample_165": [false, false, false, false, false], "sample_166": [false, false, false, false, false], "sample_167": [true, true, false, false, false], "sample_168": [true, false, false, false, false], "sample_169": [false, false, false, false, false], "sample_171": [false, false, false, false, false], "sample_170": [false, true, false, true, false], "sample_172": [true, true, true, true, true], "sample_173": [true, true, true, true, false], "sample_174": [true, true, true, true, false], "sample_175": [false, false, false, false, false], "sample_176": [true, true, true, true, true], "sample_177": [false, false, true, false, true], "sample_178": [true, true, false, true, true], "sample_180": [true, false, false, true, false], "sample_179": [true, true, true, true, false], "sample_182": [true, true, true, true, true], "sample_181": [true, true, true, true, false], "sample_183": [true, false, true, false, false], "sample_184": [false, false, true, true, false], "sample_185": [false, false, false, true, false], "sample_186": [false, false, false, true, false], "sample_187": [false, false, false, true, false], "sample_188": [true, false, false, true, true], "sample_189": [false, false, false, false, false], "sample_190": [false, true, true, true, true], "sample_191": [false, false, true], "sample_192": [true, true, false, false, true], "sample_193": [true, true, true, false, true], "sample_194": [false, false, false, false, false], "sample_195": [true, true, false, false, false], "sample_196": [false, true, true, true, true], "sample_198": [true, true, true, true, true], "sample_197": [false, false, false, false, false], "sample_199": [true, false, true, true, false], "sample_200": [false, true, true, true, false], "sample_201": [false, false, false, false, false], "sample_202": [false, false, false, false, false], "sample_203": [false, true, false, false, false], "sample_204": [false, false, true, true, false], "sample_205": [true, false, false, false, true], "sample_206": [false, false, false, false, false], "sample_207": [false, true, false, false, false], "sample_208": [false, true, false, true, true], "sample_209": [false, false, false, false, false], "sample_210": [true, false, true, true, true], "sample_211": [true, true, false, true, true], "sample_213": [false, false, false, false, false], "sample_212": [true, false, true, false, true], "sample_214": [false, false, false, false, false], "sample_215": [true, true, false, false, true], "sample_216": [true, false, false, false, false], "sample_217": [true, false, false, true, false], "sample_218": [true, false, true, true, false], "sample_219": [true, true, true, true, true], "sample_220": [false, false, false, true, true], "sample_221": [true, true, true, true, true], "sample_222": [false, false, false, true, false], "sample_223": [false, false, true, false, false], "sample_224": [true, true, true, true, false], "sample_225": [false, false, false, false, false], "sample_226": [false, true, true, false, false], "sample_227": [false, false, false, false, false], "sample_228": [true, true, true, true, true], "sample_229": [true, true, true, true, true], "sample_230": [false, true, false, false, true], "sample_231": [true, true, false, false, false], "sample_232": [false, false, false, false, false], "sample_233": [false, true, false, true, true], "sample_234": [true, true, true, true, true], "sample_235": [true, true, true, true, true], "sample_236": [true, false, false, true, false], "sample_237": [false, false, false, false, false], "sample_238": [false, false, false, false, false], "sample_239": [true, true, true, true, false], "sample_240": [true, false, false, true, true], "sample_241": [true, true, true, true, true], "sample_242": [false, false, false, false, false], "sample_243": [false, false, false, false, false], "sample_244": [true, true, true, true, true], "sample_245": [true, false, false, true, false], "sample_246": [false, false, false, true, true], "sample_247": [true, true, false, false, false], "sample_248": [false, false, true, false, false], "sample_249": [false, true, false, false, true], "sample_250": [true, false, true, false, true], "sample_251": [true, false, false, false, false], "sample_252": [false, false, false, false, false], "sample_253": [false, true, true, true], "sample_254": [true, true, true, true, true], "sample_256": [true, false, true, true, false], "sample_255": [true, false, true, true, true], "sample_257": [false, false, false, false, false], "sample_258": [false, false, true, false, false], "sample_259": [true, true, true, true, true], "sample_260": [true, true, true, true, true], "sample_261": [false, true, true, true, true], "sample_262": [false, false, false, false, false], "sample_263": [false, false, false, false, false], "sample_264": [false, false, false, false, false], "sample_265": [true, true, true, true, true], "sample_266": [false, false, false, true, true], "sample_267": [false, true, false, false, true], "sample_268": [true, true, true, false, false], "sample_269": [false, true, false, true, false], "sample_270": [true, true, false, false, false], "sample_271": [true, true, false, true, false], "sample_272": [false, false, false, false, false], "sample_273": [false, true, false, true, false], "sample_274": [true, false, false, false, false], "sample_275": [false, true, true, true, true], "sample_276": [true, true, true, false, true], "sample_277": [true, false, false, false, false], "sample_278": [false, false, true, true, true], "sample_279": [false, false, false, false, false], "sample_280": [false, true, true, true, true], "sample_281": [false, false, false, true, true], "sample_282": [false, true, true, true, false], "sample_283": [false, false, true, true, true], "sample_284": [true, true, true, true, false], "sample_285": [true, false, true, true, false], "sample_286": [false, false, false, false, false], "sample_287": [false, false, false, false, true], "sample_288": [true, false, true, false, false], "sample_289": [true, false, false, true, false], "sample_290": [false, true, true, true, false], "sample_291": [true, true, false, true, false], "sample_292": [true, false, true, true, true], "sample_293": [false, false, true, false, false], "sample_294": [true, true, false, true, true], "sample_295": [true, true, false, true, true], "sample_296": [false, false, false, false, false], "sample_297": [true, false, true, false, true], "sample_298": [true, true, false, true, true], "sample_299": [false, true, false, true, true], "sample_300": [false, false, false, false, false], "sample_301": [true], "sample_302": [false, false, true, true, false], "sample_303": [false, false, false, false, true], "sample_304": [false, false, false, false, false], "sample_305": [true, true, true, true, true], "sample_306": [true, true, true, true, false], "sample_307": [false, false, true, true, true], "sample_308": [true, true, false, false, false], "sample_309": [true, true, true, false, true], "sample_310": [true, true, true, true, false], "sample_312": [true, true, true, false, false], "sample_311": [false, true, false, false, false], "sample_313": [false, false, false, false, false], "sample_314": [true, true, true, true, false], "sample_315": [false, false, false, false, false], "sample_316": [false, true, true, false, true], "sample_317": [false, false, true, false, true], "sample_318": [false, false, false, false, false], "sample_319": [true, true, true, false, true], "sample_320": [false, false, false, false, false], "sample_321": [true, true, true, true, true], "sample_322": [false, false, true, false, false], "sample_323": [false, false, false, false, false], "sample_324": [true, true, true, true, true], "sample_325": [false, false, false, false, false], "sample_326": [false, false, false, false, false], "sample_327": [false, false, true, false, false], "sample_328": [true, true, true, true, true], "sample_329": [false, false, false, false, false], "sample_330": [false, false, false, false, false], "sample_331": [true, true, true, true, true], "sample_332": [true, false, false, true, true], "sample_333": [false, true, false, false, false], "sample_334": [true, true, false, false, true], "sample_335": [false, false, false, true, true], "sample_336": [false, false, false, false, true], "sample_337": [false, false, false, false, false], "sample_338": [true, true, true, true, false], "sample_339": [false, false, false, false, false], "sample_340": [false, false, false, false, true], "sample_341": [false, true, true, true, true], "sample_342": [false, true, false, true, true], "sample_343": [false, false, false, false, false], "sample_344": [true, false, true, false, true], "sample_345": [true, false, true, true], "sample_346": [false, false, false, false, false], "sample_347": [true, false, false, false, false], "sample_348": [false, false, true, false, false], "sample_349": [false, false, false, false, false], "sample_350": [true, true, true, true, true], "sample_351": [true, false, true, false, true], "sample_352": [false, true, true, true, true], "sample_353": [false, false, false, false, false], "sample_354": [false, false, false, false, false], "sample_355": [false, true, true, true, false], "sample_356": [true, true, true, true, true], "sample_357": [true, true, true, false, true], "sample_358": [true, true, true, true, true], "sample_359": [false, true, false, false, false], "sample_360": [false, false, false, false, false], "sample_361": [false, false, false, false, false], "sample_362": [true, true, false, true, true], "sample_363": [true, true, true, true, false], "sample_364": [false, false, false, false, false], "sample_365": [false, false, false, false, false], "sample_366": [true, false, false, false, false], "sample_367": [true, false, true, true, true], "sample_368": [false, false, false, false, false], "sample_369": [true, true, true, true, true], "sample_370": [true, true, true, true, true], "sample_371": [false, false, true, false, true], "sample_372": [false, false, false, false, false], "sample_373": [true, true, false, true, true], "sample_374": [true, true, true, true, true], "sample_375": [true, true, false, false, true], "sample_376": [false, false, false, false, false], "sample_377": [true, false, false, false, false], "sample_378": [true, true, true, false, true], "sample_379": [false, false, false, false, true], "sample_380": [false, false, false, false, false], "sample_381": [true, true, true, true, true], "sample_382": [false, false, false, false, false], "sample_383": [true, true, true, false, false], "sample_384": [true, true, true, true, true], "sample_385": [false, false, false, false, false], "sample_386": [false, true, false, true, false], "sample_387": [true, true, false, true, true], "sample_388": [true, false, false, false, false], "sample_389": [false, true, false, false, false], "sample_390": [false, false, false, true, false], "sample_391": [true, true, true, true, true], "sample_392": [false, false, true, false, false], "sample_393": [false, false, false, true, true], "sample_394": [false, false, false, false, false], "sample_395": [false, false, false, false, false], "sample_396": [false, true, true, false, true], "sample_397": [true, true, true, false, true], "sample_398": [true, true, false, true, true], "sample_399": [true, false, true, false, true], "sample_400": [true, true, true, true, false], "sample_401": [true, true, true, true, true], "sample_402": [false, false, false, false, false], "sample_403": [false, false, false, true, false], "sample_404": [true, true, true, false, true], "sample_405": [true, false, false, false, false], "sample_406": [false, false, false, true, false], "sample_407": [true, true, false, true, true], "sample_408": [true, true, true, true, true], "sample_409": [false, false, false, false, false], "sample_410": [false, false, false, false, false], "sample_411": [false, false, false, false, false], "sample_412": [false, false, false, true, false], "sample_413": [false, false, false, true, false], "sample_414": [true, true, true, true, false], "sample_415": [false, false, false, false, false], "sample_416": [false, false, true, false, false], "sample_417": [false, false, false, false, false], "sample_418": [false, false, false, false, false], "sample_419": [true, true, true, true, true], "sample_420": [false, true, false, false, false], "sample_421": [false, true, true, true, false], "sample_422": [true, true, false, true, true], "sample_423": [true, true, true, false, false], "sample_424": [false, false, false, false, false], "sample_425": [false, false, false, false, false], "sample_426": [false, false, false, false, true], "sample_427": [true, true, false, false, true], "sample_428": [false, false, false, false, false], "sample_429": [true, false, false, false, false], "sample_430": [true, true, true, true, true], "sample_431": [false, false, false, false, false], "sample_432": [true, true, true, true, true], "sample_433": [false, true, true, true, true], "sample_434": [false, false, false, false, false], "sample_435": [true, true, true, true, true], "sample_436": [true, false, false, false, false], "sample_437": [false, false, false, false, false], "sample_438": [false, false, false, false, false], "sample_439": [true, false, true, false, false], "sample_440": [true, true, true, true, true], "sample_441": [true, true, false, true, true], "sample_442": [true, true, true, true, true], "sample_443": [false, false, false, false, false], "sample_444": [true, true, true, true, true], "sample_445": [false, false, true, false, true], "sample_446": [false, false, false, false, false], "sample_447": [false, false, true, false, true], "sample_448": [false, false, false, false, false], "sample_449": [true, false, false, true, false], "sample_450": [false, true, false, false, false], "sample_451": [true, false, false, false, false], "sample_453": [true, true, true, true, true], "sample_452": [false, false, false, false, false], "sample_454": [false, false, false, false, false], "sample_455": [false, false, false, false, false], "sample_456": [true, true, false, false, true], "sample_457": [false, false, false, false, false], "sample_458": [false, false, false, false, false], "sample_459": [true, false, true, false, true], "sample_460": [false, false, false, false, true], "sample_461": [true, true, true, false, true], "sample_462": [false, false, false, false, false], "sample_463": [false, true, true, false, true], "sample_464": [false, true, false, false, false], "sample_465": [true, true, true, true, false], "sample_466": [false, false, false, false, false], "sample_467": [true, true, false, true, false], "sample_469": [false, false, true, false, true], "sample_468": [false, false, false, false, false], "sample_470": [false, false, true, false, false], "sample_471": [false, false, false, false, false], "sample_472": [true, true, true, true, false], "sample_473": [true, false, false, false, false], "sample_474": [false, false, false, false, false], "sample_475": [true, false, true, true, true], "sample_476": [true, false, true, false, false], "sample_477": [false, false, false, false, false], "sample_478": [false, true, true, true, false], "sample_479": [true, true, true, true, true], "sample_480": [false, false, false, false, false], "sample_481": [false, false, false, false, false], "sample_482": [false, false, false, false, false], "sample_483": [true, false, false, false, false], "sample_484": [false, false, false, false, false], "sample_485": [false, false, false, false, false], "sample_486": [false, false, false, false, false], "sample_487": [true, true, false, true, true], "sample_488": [false, false, true, false, false], "sample_489": [true, true, true, true, true], "sample_490": [false, false, false, false, false], "sample_491": [true, false, true, true, false], "sample_492": [false, false, true, false, false], "sample_493": [true, true, true, false, false], "sample_494": [false, false, false, false, false], "sample_495": [true, true, false, true, true], "sample_496": [true, true, true, true, true], "sample_497": [false, false, false, false, false], "sample_498": [true, true, true, false, false], "sample_499": [true, false, false, false, false], "sample_500": [false, false, false, false, false], "sample_501": [true, false, false, true, false], "sample_502": [false, false, true, true, true], "sample_503": [false, false, false, false, false], "sample_504": [false, false, false, false, false], "sample_505": [false, false, false, false, false], "sample_506": [false, false, false, false, false], "sample_507": [true, false, false, false, false], "sample_508": [true, true, false, false, false], "sample_509": [true, true, false, false, false], "sample_510": [true, false, true, false, false], "sample_511": [true, true, false, false, false], "sample_512": [true, false, false, true, true], "sample_513": [true, true, true, true, false], "sample_514": [false, false, false, false, false], "sample_515": [false, false, false, false, false], "sample_516": [false, false, false, false, false], "sample_517": [false, false, false, false, false], "sample_518": [false, false, false, false, false], "sample_519": [false, false, false, false, false], "sample_520": [false, false, false, false, false], "sample_521": [false, false, false, false, false], "sample_522": [false, false, false, false, false], "sample_523": [true, false, true, false, false], "sample_524": [false, false, false, false, false], "sample_525": [false, false, false, false, false], "sample_526": [false, false, true, false, false], "sample_527": [false, false, true, false, true], "sample_528": [false, true, true, false, false], "sample_529": [false, false, true, true, true], "sample_530": [false, false, false, false, false], "sample_531": [false, false, false, false, false], "sample_532": [true, false, false, false, false], "sample_533": [false, true, true, false, false], "sample_534": [true, false, false, false, false], "sample_535": [false, false, false, false, false], "sample_536": [false, false, false, false, false], "sample_537": [false, false, false, false, false], "sample_538": [false, true, false, false, false], "sample_539": [false, false, false, false, false], "sample_540": [false, true, false, false, false], "sample_541": [true, true, false, false, false], "sample_542": [false, false, false, false, false], "sample_543": [false, false, true, false, false], "sample_544": [false, false, false, false, false], "sample_545": [false, false, false, false, false], "sample_546": [true, false, false, false, true], "sample_547": [false, false, false, false, false], "sample_548": [false, false, false, false, false], "sample_549": [false, false, false, false, false], "sample_550": [true, false, false, false, true], "sample_551": [false, false, false, false, false], "sample_552": [false, false, false, false, false], "sample_553": [true, true, true, false, false], "sample_554": [false, false, false, false, false], "sample_555": [false, false, false, false, false], "sample_556": [false, false, false, false, false], "sample_557": [false, false, false, false, false], "sample_558": [false, false, false, false, false], "sample_559": [false, false, false, false, false], "sample_560": [false, true, true, false, false], "sample_561": [true, true, false, false, false], "sample_562": [false, false, true, false, false], "sample_563": [false, false, false, false, false], "sample_564": [false, false, false, false, false], "sample_565": [false, false, false, false, false], "sample_566": [false, false, false, false, false], "sample_567": [false, false, false, false, false], "sample_568": [false, false, false, false, false], "sample_569": [false, false, false, false, false], "sample_570": [false, false, false, false, false], "sample_571": [false, false, true, false, false], "sample_572": [false, false, false, false, false], "sample_573": [false, false, false, false, false], "sample_574": [false, false, false, false, false], "sample_575": [false, false, false, false, false], "sample_576": [false, false, true, false, false], "sample_577": [false, false, true, false, false], "sample_578": [false, false, false, false, false], "sample_579": [false, false, false, false, false], "sample_580": [false, false, true, true, false], "sample_581": [false, false, false, false, false], "sample_582": [true, false, true, false, false], "sample_583": [false, false, false, false, false], "sample_584": [false, false, false, false, false], "sample_585": [true, true, true, false, true], "sample_586": [false, true, false, true, false], "sample_587": [false, false, false, false, false], "sample_588": [false, false, false, false, false], "sample_589": [false, false, false, false, false], "sample_590": [true, false, false, true, true], "sample_591": [false, false, false, false, false], "sample_592": [false, false, false, false, false], "sample_593": [true, true, true, true, true], "sample_594": [false, false, false, false, false], "sample_595": [true, false, false, false, false], "sample_596": [false, false, false, true, false], "sample_597": [false, false, false, false, false], "sample_598": [false, false, false, false, false], "sample_599": [false, false, false, false, false], "sample_600": [false, false, true, false, false], "sample_601": [false, false, false, false, false], "sample_602": [true, true, true, false, false], "sample_603": [true, true, true, true, false], "sample_604": [false, false, false, false, false], "sample_605": [false, false, false, false, false], "sample_606": [false, false, false, false, false], "sample_607": [false, false, false, true, false], "sample_608": [false, false, true, false, false], "sample_609": [false, false, true, false, false], "sample_610": [false, false, false, false, false], "sample_611": [false, false, false, false, false], "sample_612": [false, true, true, true, false], "sample_613": [true, true, false, true, false], "sample_614": [false, false, false, false, false], "sample_615": [false, false, false, true, false], "sample_616": [true, false, false, false, true], "sample_617": [false, true, true, false, true], "sample_618": [false, true, false, true, false], "sample_619": [true, true, false, false, false], "sample_620": [false, false, false, true, false], "sample_621": [false, false, false, false, false], "sample_622": [false, false, false, false, false], "sample_623": [false, false, false, false, false], "sample_624": [false, false, false, false, false], "sample_625": [true, false, false, false, false], "sample_626": [false, false, false, false, false], "sample_627": [false, false, false, false, false], "sample_628": [true, true, false, false, true], "sample_629": [false, false, false, false, false], "sample_630": [false, false, false, false, false], "sample_631": [false, false, false, false, false], "sample_632": [false, true, false, false, false], "sample_633": [false, true, false, false, true], "sample_634": [true, false, false, false, false], "sample_635": [false, false, false, false, false], "sample_636": [false, false, false, false, false], "sample_637": [false, false, false, false, false], "sample_638": [true, false, false, false, false], "sample_639": [false, false, false, false, true], "sample_640": [false, false, false, false, false], "sample_641": [false, false, false, false, false], "sample_642": [false, false, false, false, false], "sample_643": [false, false, false, false, false], "sample_644": [false, false, false, false, false], "sample_645": [false, false, false, false, false], "sample_646": [false, false, false, false, false], "sample_647": [true, false, false, false, false], "sample_648": [false, false, false, false, false], "sample_649": [false, false, false, false, false], "sample_650": [false, false, false, false, false], "sample_651": [false, false, false, false, false], "sample_652": [false, false, false, false, false], "sample_653": [false, true, false, false, false], "sample_654": [false, false, false, false, false], "sample_655": [true, false, false, false, false], "sample_656": [false, false, false, false, false], "sample_657": [false, false, false, false, true], "sample_658": [false, false, false, false, false], "sample_659": [true, false, false, false, false], "sample_660": [false, false, false, false, false], "sample_661": [false, false, false, false, false], "sample_662": [false, false, false, false, false], "sample_663": [false, false, false, false, false], "sample_664": [false, false, false, false, false], "sample_665": [false, false, false, false, false], "sample_666": [false, false, false, false, false], "sample_667": [false, true, false, true, false], "sample_668": [false, false, false, false, false], "sample_669": [false, false, false, false, false], "sample_670": [false, false, false, false, false], "sample_671": [false, false, false, true, false], "sample_672": [false, false, false, false, false], "sample_673": [false, false, false, false, false], "sample_674": [true, true, true, true, true], "sample_675": [false, false, false, false, false], "sample_676": [false, false, true, false, false], "sample_677": [false, true, false, false, false], "sample_678": [false, false, false, false, false], "sample_679": [false, false, false, false, false], "sample_680": [false, false, false, false, false], "sample_681": [false, false, false, false, false], "sample_682": [false, false, false, false, false], "sample_683": [false, false, false, false, false], "sample_684": [false, true, false, false, false], "sample_685": [false, false, false, false, false], "sample_686": [false, false, false, false, false], "sample_687": [true, false, false, false, false], "sample_688": [false, false, false, false, false], "sample_689": [false, false, false, false, false], "sample_690": [false, false, false, false, false], "sample_691": [false, false, false, false, false], "sample_692": [true, false, false, false, false], "sample_693": [false, false, false, false, false], "sample_694": [false, false, false, false, false], "sample_695": [true, true, true, false, false], "sample_696": [false, false, false, false, false], "sample_697": [false, false, false, false, false], "sample_698": [false, false, false, true, true], "sample_699": [false, false, false, false, false], "sample_700": [false, false, false, false, false], "sample_701": [false, false, false, false, false], "sample_702": [true, false, true, true, false], "sample_703": [true, false, true, true, false], "sample_704": [true, true, false, true, true], "sample_705": [false, true, false, true, true], "sample_706": [false, false, false, false, false], "sample_707": [true, true, true, false, false], "sample_708": [false, true, false, false, false], "sample_709": [true, false, false, false, true], "sample_710": [false, false, false, false, false], "sample_711": [true, true, true, true, true], "sample_712": [false, false, false, false, false], "sample_713": [true, false, false, false, true], "sample_714": [false, true, false, true, false], "sample_715": [true, false, true, false, false], "sample_716": [false, false, true, false, false], "sample_717": [true, true, true, true, true], "sample_718": [false, false, false, false, false], "sample_719": [false, false, false, true, false], "sample_720": [false, true, false, true, false], "sample_721": [false, false, true, true, false], "sample_722": [false, false, false, false, false], "sample_723": [false, false, false, false, false], "sample_724": [false, false, false, false, false], "sample_725": [false, true, true, false, false], "sample_726": [true, false, false, true, true], "sample_727": [true, false, false, false, false], "sample_728": [true, true, true, true, true], "sample_729": [false, false, false, false, false], "sample_730": [true, true, true, false, false], "sample_731": [false, false, false, false, false], "sample_732": [true, true, false, false, false], "sample_733": [true, true, true, true, false], "sample_734": [false, false, false, false, true], "sample_735": [true, false, false, true, false], "sample_736": [false, false, false, false, false], "sample_737": [false, false, false, true, false], "sample_738": [false, false, false, false, false], "sample_739": [true, true, false, true, true], "sample_740": [false, false, true, true, false], "sample_741": [true, false, true, false, false], "sample_742": [false, false, false, false, false], "sample_743": [false, false, false, false, false], "sample_744": [false, true, false, true, true], "sample_745": [false, false, true, false, false], "sample_746": [false, true, false, true, false], "sample_747": [true, true, true, false, false], "sample_748": [false, false, false, false, false], "sample_749": [false, true, true, true, true], "sample_750": [false, true, false, false, false], "sample_751": [false, true, true, true, false], "sample_752": [true, true, true, false, false], "sample_753": [false, false, false, true, true], "sample_754": [true, false, false, false, false], "sample_755": [true, false, false, true, false], "sample_756": [false, false, false, false, false], "sample_757": [true, false, false, false, false], "sample_758": [true, false, true, true, true], "sample_759": [true, false, true, false, true], "sample_760": [true, true, false, true, false], "sample_761": [false, false, true, false, false], "sample_762": [false, false, true, true, true], "sample_763": [false, true, true, true, true], "sample_764": [false, true, false, false, true], "sample_765": [false, true, false, true, false], "sample_766": [true, false, false, false, false], "sample_767": [true, true, true, false, true], "sample_768": [true, true, true, false, false], "sample_769": [false, true, false, false, false], "sample_770": [false, true, true, true, false], "sample_771": [true, true, false, false, false], "sample_772": [false, false, false, false, false], "sample_773": [true, false, false, false, false], "sample_774": [false, false, false, false, false], "sample_775": [false, false, false, false, false], "sample_776": [false, true, false, true, true], "sample_777": [true, false, true, true, false], "sample_778": [true, false, false, false, false], "sample_779": [false, true, true, true, false], "sample_780": [true, true, true, false, false], "sample_781": [true, true, true, true, true], "sample_782": [false, false, false, false, false], "sample_783": [true, true, false, false, false], "sample_784": [true, false, false, false, false], "sample_785": [false, true, false, false, false], "sample_786": [false, false, false, false, false], "sample_787": [false, false, false, false, false], "sample_788": [false, false, true, false, false], "sample_789": [true, true, true, true, false], "sample_790": [false, true, false, false, false], "sample_791": [false, false, false, false, false], "sample_792": [false, true, false, true, false], "sample_793": [false, true, false, false, false], "sample_794": [true, false, false, false, false], "sample_795": [false, false, false, false, false], "sample_796": [false, true, true, true, true], "sample_797": [true, false, true, true, false], "sample_798": [true, false, true, false, false], "sample_799": [false, false, false, true, false], "sample_800": [true, true, false, false, true], "sample_801": [false, true, true, false, false], "sample_802": [false, false, false, true, false], "sample_803": [false, false, false, true, false], "sample_804": [false, false, false, true, false], "sample_805": [true, false, false, false, false], "sample_806": [false, false, false, false, true], "sample_807": [false, true, true, false, false], "sample_808": [true, true, true, true, false], "sample_809": [false, true, true, true, true], "sample_810": [true, false, false, false, true], "sample_811": [false, false, false, false, false], "sample_812": [false, false, false, false, false], "sample_813": [false, true, false, true, true], "sample_814": [false, false, false, false, false], "sample_815": [false, true, false, false, false], "sample_816": [true, false, true, true, false], "sample_817": [false, false, false, false, true], "sample_818": [true, false, false, true, false], "sample_819": [false, true, true, false, true], "sample_820": [true, true, true, true, false], "sample_821": [false, false, false, false, true], "sample_822": [false, false, false, false, false], "sample_823": [false, false, false, false, false], "sample_824": [false, false, false, false, false], "sample_825": [true, false, false, false, false], "sample_826": [false, true, false, false, false], "sample_827": [true, false, true, false, false], "sample_828": [false, false, false, false, false], "sample_829": [true, true, true, false, false], "sample_830": [true, false, false, false, false], "sample_831": [true, true, true, false, false], "sample_832": [true, true, true, false, false], "sample_833": [false, false, false, false, false], "sample_834": [true, false, true, false, false], "sample_835": [true, true, false, true, true], "sample_836": [false, true, false, true, true], "sample_837": [false, false, false, false, false], "sample_838": [false, false, false, false, false], "sample_839": [true, true, false, true, false], "sample_840": [false, false, false, false, false], "sample_841": [true, false, true, false, false], "sample_842": [true, true, false, true, false], "sample_843": [false, true, true, true, false], "sample_844": [false, false, false, false, false], "sample_845": [true, true, false, true, false], "sample_846": [false, false, false, true, false], "sample_847": [true, true, true, true, false], "sample_848": [false, false, true, false, false], "sample_849": [true, true, true, true, false], "sample_850": [true, true, true, true, false], "sample_851": [true, true, true, false, false], "sample_852": [true, false, true, true, true], "sample_853": [false, false, true, true, true], "sample_854": [false, true, true, false, false], "sample_855": [true, false, true, true, true], "sample_856": [true, false, true, false, true], "sample_857": [false, false, true, true, false], "sample_858": [true, true, false, true, true], "sample_859": [false, true, false, false, false], "sample_860": [true, true, false, true, true], "sample_861": [false, true, false, false, false], "sample_862": [true, false, false, true, false], "sample_863": [false, false, true, false, false], "sample_864": [false, false, true, true, true], "sample_865": [false, false, false, false, false], "sample_866": [true, false, true, false, false], "sample_867": [false, false, false, false, false], "sample_868": [false, false, false, false, false], "sample_869": [false, false, false, false, false], "sample_870": [false, true, true, false, false], "sample_871": [false, true, false, false, false], "sample_872": [false, false, false, false, false], "sample_873": [true, true, true, true, false], "sample_874": [false, true, false, false, false], "sample_875": [true, false, false, false, true], "sample_876": [false, false, false, false, false], "sample_877": [false, false, false, true, true], "sample_878": [false, false, false, false, true], "sample_879": [true, false, false, false, false], "sample_880": [false, false, false, false, false], "sample_881": [false, false, false, false, false], "sample_882": [false, false, false, false, false], "sample_883": [true, false, false, true, false], "sample_884": [false, false, false, false, false], "sample_885": [false, false, false, false, false], "sample_886": [true, true, false, false, true], "sample_887": [true, true, false, false, false], "sample_888": [false, false, false, true, false], "sample_889": [false, false, false, true, false], "sample_890": [true, true, false, false, false], "sample_891": [false, false, true, false, false], "sample_892": [false, true, true, false, false], "sample_893": [false, true, false, false, false], "sample_894": [true, true, false, false, false], "sample_895": [false, false, false, false, false], "sample_896": [false, false, false, false, false], "sample_897": [false, false, false, false, false], "sample_898": [true, true, true, false, false], "sample_899": [false, false, false, false, false], "sample_900": [true, false, false, false, false], "sample_901": [false, false, true, false, false], "sample_902": [true, false, false, false, true], "sample_903": [true, true, false, false, false], "sample_904": [false, false, false, false, false], "sample_905": [false, false, false, false, false], "sample_906": [false, false, false, false, false], "sample_907": [false, false, false, false, false], "sample_908": [false, false, false, false, false], "sample_909": [false, false, false, false, false], "sample_910": [true, false, false, false, false], "sample_911": [false, false, false, true, false], "sample_912": [false, false, false, false, false], "sample_913": [false, false, false, false, false], "sample_914": [false, false, false, false, false], "sample_915": [false, false, false, false, false], "sample_916": [false, false, false, false, false], "sample_917": [false, false, false, false, false], "sample_918": [false, false, false, false, false], "sample_919": [false, false, false, false, false], "sample_920": [false, false, false, false, false], "sample_921": [false, false, false, false, false], "sample_922": [false, false, false, false, false], "sample_923": [false, false, false, false, false], "sample_924": [false, false, false, false, false], "sample_925": [false, false, false, true, false], "sample_926": [false, false, false, false, false], "sample_927": [false, false, false, false, false], "sample_928": [false, false, false, false, true], "sample_929": [false, false, false, false, false], "sample_930": [false, true, false, false, false], "sample_931": [false, false, false, false, false], "sample_932": [false, false, false, false, false], "sample_933": [true, true, false, false, false], "sample_934": [false, false, false, false, false], "sample_935": [false, false, false, false, false], "sample_936": [false, false, false, false, false], "sample_937": [false, false, false, false, false], "sample_938": [false, false, false, false, false], "sample_939": [false, false, false, false, false], "sample_940": [false, false, false, false, false], "sample_941": [false, false, false, false, true], "sample_942": [false, false, false, false, false], "sample_943": [true, false, false, false, false], "sample_944": [false, false, false, false, false], "sample_945": [false, false, false, false, false], "sample_946": [false, false, false, false, false], "sample_947": [false, false, false, false, false], "sample_948": [false, false, false, false, false], "sample_949": [false, false, false, false, false], "sample_950": [false, false, false, false, false], "sample_951": [false, false, false, true, false], "sample_952": [false, false, false, false, false], "sample_953": [true, true, true, true, false], "sample_954": [false, false, false, false, false], "sample_955": [false, false, false, false, false], "sample_956": [false, false, false, false, false], "sample_957": [false, false, false, false, false], "sample_958": [false, false, false, false, false], "sample_959": [false, false, false, false, false], "sample_960": [false, false, false, false, true], "sample_961": [false, false, false, false, false], "sample_962": [false, true, false, false, false], "sample_963": [false, false, false, false, false], "sample_964": [false, false, false, false, false], "sample_965": [false, false, false, false, false], "sample_966": [false, false, false, false, false], "sample_967": [true, true, true, true, true], "sample_968": [false, false, false, false, true], "sample_969": [false, false, false, false, false], "sample_970": [true, false, false, false, false], "sample_971": [true, false, true, true, false], "sample_972": [false, false, false, false, false], "sample_973": [false, false, false, false, false], "sample_974": [true, false, true, true, false], "sample_975": [true, true, true, true, true], "sample_976": [false, true, false, false, false], "sample_977": [false, false, false, false, false], "sample_978": [true, true, false, false, false], "sample_979": [false, true, false, true, false], "sample_980": [true, false, true, true, false], "sample_981": [true, true, false, false, true], "sample_982": [true, true, true, true, true], "sample_983": [false, true, false, true, true], "sample_984": [false, false, true, true, false], "sample_985": [true, false, false, false, false], "sample_986": [false, false, false, false, false], "sample_987": [false, true, false, false, false], "sample_988": [false, true, false, true, true], "sample_989": [true, true, true, false, false], "sample_990": [true, true, false, false, true], "sample_991": [false, true, false, false, true], "sample_992": [true, false, true, true, true], "sample_993": [false, true, false, false, true], "sample_994": [false, true, false, true, false], "sample_995": [true, true, false, false, false], "sample_996": [false, false, false, false, false], "sample_997": [true, false, false, false, true], "sample_998": [false, true, false, false, false], "sample_999": [false, false, false, false, false], "sample_1000": [true, false, false, false, true], "sample_1001": [false, true, false, false, false], "sample_1002": [false, false, false, false, false], "sample_1003": [true, false, true, false, false], "sample_1004": [false, false, false, false, true], "sample_1005": [false, false, false, true, false], "sample_1006": [false, true, false, true, true], "sample_1007": [false, false, false, false, false], "sample_1008": [false, false, false, true, true], "sample_1009": [false, true, true, false, false], "sample_1010": [false, false, false, false, true], "sample_1011": [true, true, false, false, true], "sample_1012": [true, true, true, false, true], "sample_1013": [true, false, true, false, false], "sample_1014": [false, false, false, false, false], "sample_1015": [false, false, false, false, false], "sample_1016": [false, true, true, false, false], "sample_1017": [false, false, true, true, true], "sample_1018": [false, false, false, false, false], "sample_1019": [false, false, false, false, true], "sample_1020": [false, false, false, false, false], "sample_1021": [false, false, false, false, false], "sample_1022": [false, false, false, false, true], "sample_1023": [true, true, true, false, false], "sample_1024": [false, false, true, false, false], "sample_1025": [true, false, false, false, true], "sample_1026": [false, false, false, false, false], "sample_1027": [true, true, true, false, false], "sample_1028": [false, true, true, false, false], "sample_1029": [false, false, false, false, false], "sample_1030": [false, false, false, false, false], "sample_1031": [false, false, false, false, false], "sample_1032": [false, false, true, true, false], "sample_1033": [false, true, false, false, true], "sample_1034": [false, false, false, false, true], "sample_1035": [false, true, false, false, false], "sample_1036": [true, false, false, true, true], "sample_1037": [false, false, false, false, false], "sample_1038": [true, false, false, false, false], "sample_1039": [true, false, true, false, false], "sample_1040": [false, false, false, true, true], "sample_1041": [true, false, false, true, false], "sample_1042": [false, false, false, false, false], "sample_1043": [true, false, false, true, false], "sample_1044": [true, true, false, false, false], "sample_1045": [false, false, false, false, false], "sample_1046": [true, false, false, true, false], "sample_1047": [true, false, false, false, true], "sample_1048": [false, false, true, false, false], "sample_1049": [true, true, false, true, false], "sample_1050": [true, true, true, true, false], "sample_1051": [false, false, false, false, false], "sample_1052": [false, false, false, true, false], "sample_1053": [true, true, false, true, false], "sample_1054": [true, true, true, false, false], "sample_1055": [true, true, true, true, true], "sample_1056": [false, false, false, false, false], "sample_1057": [false, false, false, false, false], "sample_1058": [true, false, false, true, true], "sample_1059": [false, true, true, false, false], "sample_1060": [true, true, true, false, true], "sample_1061": [true, false, true, false, true], "sample_1062": [true, true, false, false, false], "sample_1063": [false, true, false, false, false], "sample_1064": [false, false, false, false, false], "sample_1065": [true, false, true, true, false], "sample_1066": [false, false, false, false, false], "sample_1067": [false, false, false, true, true], "sample_1068": [false, true, false, false, true], "sample_1069": [false, false, false, false, false], "sample_1070": [true, true, false, true, true], "sample_1071": [false, false, true, true, false], "sample_1072": [false, false, true, false, false], "sample_1073": [false, true, true, false, false], "sample_1074": [true, true, true, true, false], "sample_1075": [false, true, true, false, false], "sample_1076": [false, true, false, true, false], "sample_1077": [false, false, true, false, true], "sample_1078": [true, false, false, true, false], "sample_1079": [true, true, true, false, true], "sample_1080": [false, false, true, false, true], "sample_1081": [true, true, true, false, true], "sample_1082": [true, false, false, false, false], "sample_1083": [true, false, true, false, false], "sample_1084": [false, false, false, false, false], "sample_1085": [true, true, false, true, false], "sample_1086": [false, false, true, false, false], "sample_1087": [false, false, false, false, false], "sample_1088": [true, false, false, false, true], "sample_1089": [true, false, false, false, false], "sample_1090": [false, false, false, false, false], "sample_1091": [false, false, false, false, true], "sample_1092": [false, false, true, false, false], "sample_1093": [true, true, false, false, false], "sample_1094": [true, true, false, false, false], "sample_1095": [true, true, true, true, false], "sample_1096": [false, false, false, false, false], "sample_1097": [true, false, false, false, false], "sample_1098": [false, true, false, false, false], "sample_1099": [false, false, false, true, false], "sample_1100": [false, false, false, false, false], "sample_1101": [true, true, false, false, true], "sample_1102": [false, false, false, false, false], "sample_1103": [false, false, false, false, false], "sample_1104": [false, false, false, false, true], "sample_1105": [false, true, false, true, false], "sample_1106": [false, false, false, false, false], "sample_1107": [true, false, true, true, false], "sample_1108": [true, true, true, true, false], "sample_1109": [true, false, false, false, false], "sample_1110": [true, true, true, false, false], "sample_1111": [false, true, false, false, false], "sample_1112": [true, true, false, true, false], "sample_1113": [false, false, false, false, false], "sample_1114": [true, false, true, true, false], "sample_1115": [false, true, false, false, false], "sample_1116": [false, false, false, false, false], "sample_1117": [false, false, false, false, false], "sample_1118": [false, false, false, false, false], "sample_1119": [true, false, false, false, false], "sample_1120": [true, false, false, false, false], "sample_1121": [false, false, true, false, false], "sample_1122": [false, false, false, false, false], "sample_1123": [false, false, true, false, true], "sample_1124": [false, false, false, false, false], "sample_1125": [true, false, true, true, true], "sample_1126": [true, true, true, true, true], "sample_1127": [true, true, false, false, true], "sample_1128": [true, true, true, true, true], "sample_1129": [false, false, false, false, false], "sample_1130": [false, true, false, true, true], "sample_1131": [false, false, false, true, false], "sample_1132": [true, true, true, true, true], "sample_1133": [true, true, false, false, false], "sample_1134": [false, false, false, false, true], "sample_1135": [false, false, false, true, false], "sample_1136": [false, false, true, false, true], "sample_1137": [false, false, false, false, false], "sample_1138": [false, false, false, false, false], "sample_1139": [true, false, false, false, true], "sample_1140": [false, false, false, false, false], "sample_1141": [false, false, false, false, false], "sample_1142": [false, false, false, false, false], "sample_1143": [true, true, true, true, true], "sample_1144": [true, true, true, true, true], "sample_1145": [false, false, true, false, false], "sample_1146": [false, false, true, false, false], "sample_1147": [false, false, true, false, true], "sample_1148": [false, false, false, false, false], "sample_1149": [true, true, false, false, true], "sample_1150": [true, true, true, false, false], "sample_1151": [true, false, true, false, false], "sample_1152": [true, true, true, false, true], "sample_1153": [false, true, true, false, false], "sample_1154": [false, false, false, false, false], "sample_1155": [false, true, false, false, false], "sample_1156": [false, false, false, false, false], "sample_1157": [false, false, true, false, true], "sample_1158": [false, false, false, false, false], "sample_1159": [true, false, false, false, false], "sample_1160": [false, false, false, false, false], "sample_1161": [false, false, false, true, true], "sample_1162": [false, false, false, false, false], "sample_1163": [true, true, true, true, false], "sample_1164": [false, false, false, false, false], "sample_1165": [false, true, true, true, false], "sample_1166": [false, true, false, false, true], "sample_1167": [true, false, false, false, true], "sample_1168": [true, true, true, true, false], "sample_1169": [true, false, false, false, false], "sample_1170": [false, true, true, false, true], "sample_1171": [false, true, true, true, false], "sample_1172": [false, false, false, false, false], "sample_1173": [false, false, false, false, false], "sample_1174": [false, false, false, false, false], "sample_1175": [false, false, false, false, false], "sample_1176": [true, true, false, true, false], "sample_1177": [false, false, false, false, false], "sample_1178": [true, true, true, false, false], "sample_1179": [true, true, true, true, true], "sample_1180": [true, true, true, true, true], "sample_1181": [false, false, false, false, true], "sample_1182": [true, false, false, false, false], "sample_1183": [false, false, false, false, false], "sample_1184": [true, true, true, false, false], "sample_1185": [true, true, true, true, false], "sample_1186": [true, true, true, false, true], "sample_1187": [true, true, true, false, true], "sample_1188": [false, false, false, false, false], "sample_1189": [true, true, false, false, false], "sample_1190": [false, false, false, false, false], "sample_1191": [false, false, false, false, false], "sample_1192": [false, false, false, false, false], "sample_1193": [true, true, false, false, false], "sample_1194": [true, false, true, true, true], "sample_1195": [false, false, false, false, false], "sample_1196": [false, false, false, false, false], "sample_1197": [false, false, false, false, false], "sample_1198": [false, true, true, false, false], "sample_1199": [true, false, false, false, true], "sample_1200": [false, true, true, false, false], "sample_1201": [false, false, false, false, false], "sample_1202": [true, true, true, false, true], "sample_1203": [false, false, true, false, false], "sample_1204": [true, false, false, false, false], "sample_1205": [false, true, false, false, false], "sample_1206": [true, false, false, false, false], "sample_1207": [false, false, false, false, true], "sample_1208": [false, false, true, false, false], "sample_1209": [false, false, false, false, false]}}