{"raw_generations": {"sample_33": ["def test_send_robust_multiple_receivers(self):\n        return val * 2\n\n        return val * 3\n\n    a_signal.connect(receiver_1)\n    a_signal.connect(receiver_2)\n    result = a_signal.send_robust(sender=self, val=2)\n    self.assertEqual(result, [(receiver_1, 4), (receiver_2, 6)])\n    a_signal.disconnect(receiver_1)\n    a_signal.disconnect(receiver_2)\n    self.assertTestIsClean(a_signal)\n", "def test_send_robust_multiple_signals(self):\n        return val * 2\n\n        raise ValueError('this')\n\n    a_signal.connect(receiver_1_arg)\n    a_signal.connect(fails)\n    b_signal.connect(receiver_2_arg)\n\n    try:\n        with self.assertLogs('django.dispatch', 'ERROR') as cm:\n            result_a = a_signal.send_robust(sender=self, val=5)\n            result_b = b_signal.send_robust(sender=self, val=10)\n        err = result_a[1][1]\n        self.assertIsInstance(err, ValueError)\n        self.assertEqual(err.args, ('this',))\n        self.assertIs(hasattr(err, '__traceback__'), True)\n        self.assertIsInstance(err.__traceback__, TracebackType)\n\n        log_record = cm.records[0]\n        self.assertEqual(\n            log_record.getMessage(),\n            'Error calling '\n            'DispatcherTests.test_send_robust_multiple_signals.<locals>.fails in '\n            'Signal.send_robust() (this)',\n        )\n        self.assertIsNotNone(log_record.exc_info)\n        _, exc_value, _ = log_record.exc_info\n        self.assertIsInstance(exc_value, ValueError)\n        self.assertEqual(str(exc_value), 'this')\n\n        self.assertEqual(result_a[0][1], 5)\n        self.assertEqual(result_b[0][1], 20)\n    finally:\n        a_signal.disconnect(receiver_1_arg)\n        a_signal.disconnect(fails)\n        b_signal.disconnect(receiver_2_arg)\n    self.assertTestIsClean(a_signal)\n    self.assertTestIsClean(b_signal)\n", "    def test_receiver_signal_kwargs(self):\n        @receiver(a_signal, sender=self)\n            self.state.append(val)\n        self.state = []\n        a_signal.send(sender=self, val='a')\n        self.assertEqual(self.state, ['a'])\n        a_signal.disconnect(f, sender=self)\n        self.assertTestIsClean(a_signal)\n", "def test_cannot_connect_unhashable_receiver(self):\n    unhashable_receiver = []\n    msg = 'Signal receivers must be hashable.'\n    with self.assertRaisesMessage(TypeError, msg):\n        a_signal.connect(unhashable_receiver)\n    self.assertTestIsClean(a_signal)\n", "def test_caching_with_different_senders(self):\n    \"\"\"\n    Test that signal caching correctly handles different senders.\n    \"\"\"\n    d_signal.connect(receiver_1_arg, sender=object)\n    d_signal.send(sender=object(), val='object')\n    d_signal.send(sender=self, val='self')\n    result = d_signal.send(sender=object(), val='object')\n    self.assertEqual(result, [(receiver_1_arg, 'object')])\n    result = d_signal.send(sender=self, val='self')\n    self.assertEqual(result, [(receiver_1_arg, 'self')])\n    d_signal.disconnect(receiver_1_arg)\n    self.assertTestIsClean(d_signal)\n"], "sample_42": ["def test_serialize_type_type(self):\n    self.assertSerializedEqual(type)\n    self.assertSerializedResultEqual(\n        MigrationWriter.serialize(type),\n        (\"('type', {'import builtins'})\", set()),\n    )\n", "def test_serialize_datetime_with_microseconds(self):\n    dt = datetime.datetime(2022, 1, 1, 12, 0, 0, 123456)\n    self.assertSerializedEqual(dt)\n    self.assertSerializedResultEqual(\n        dt,\n        (\"datetime.datetime(2022, 1, 1, 12, 0, 0, 123456)\", {'import datetime'})\n    )\n", "def test_serialize_method_type(self):\n    class TestClass:\n        @staticmethod\n            return x + 1\n\n    self.assertSerializedEqual(TestClass.test_method)\n    self.assertSerializedResultEqual(\n        TestClass.test_method,\n        (\"migrations.test_writer.TestClass.test_method\", {'import migrations.test_writer'})\n    )\n", "def test_serialize_custom_serializer(self):\n    class CustomClass:\n            self.value = value\n\n    class CustomClassSerializer(BaseSerializer):\n            return \"CustomClass(%r)\" % self.value, {\"import migrations.test_writer\"}\n\n    MigrationWriter.register_serializer(CustomClass, CustomClassSerializer)\n    self.assertSerializedEqual(CustomClass(\"test\"))\n    MigrationWriter.unregister_serializer(CustomClass)\n    with self.assertRaisesMessage(ValueError, 'Cannot serialize: CustomClass(\\'test\\')'):\n        self.assertSerializedEqual(CustomClass(\"test\"))\n", "def test_serialize_custom_field(self):\n    class CustomField(models.Field):\n            return ('CustomField', [], {})\n\n    field = CustomField()\n    string = MigrationWriter.serialize(field)[0]\n    self.assertEqual(string, \"migrations.test_writer.CustomField()\")\n    self.serialize_round_trip(field)\n"], "sample_112": ["def test_isotonic_regression_output_transform():\n    \"\"\"Check that `transform` does return the expected output type.\n\n    We need to check that `transform` will output a DataFrame and a NumPy array\n    when we set `transform_output` to `pandas`.\n\n    Non-regression test for:\n    https://github.com/scikit-learn/scikit-learn/issues/25499\n    \"\"\"\n    pd = pytest.importorskip(\"pandas\")\n    X, y = make_regression(n_samples=10, n_features=1, random_state=42)\n    regressor = IsotonicRegression()\n    with sklearn.config_context(transform_output=\"pandas\"):\n        regressor.fit(X, y)\n        X_trans = regressor.transform(X)\n\n    assert isinstance(X_trans, pd.DataFrame)\n", "def test_isotonic_regression_with_sample_weight_zero():\n    # Test case when all sample weights are zero\n    X = np.array([1, 2, 3, 4, 5])\n    y = np.array([1, 2, 3, 4, 5])\n    sample_weight = np.zeros_like(X)\n\n    ir = IsotonicRegression()\n    ir.fit(X, y, sample_weight=sample_weight)\n    y_pred = ir.predict(X)\n\n    assert np.all(np.isnan(y_pred))\n", "def test_isotonic_regression_with_sample_weight_zero():\n    # Set y and x\n    y = np.array([3, 7, 5, 9, 8, 7, 10])\n    x = np.arange(len(y))\n\n    # Create sample weight with a zero value\n    sample_weight = np.ones(len(y))\n    sample_weight[2] = 0\n\n    # Create model and fit\n    ir = IsotonicRegression(increasing=\"auto\")\n    ir.fit(x, y, sample_weight=sample_weight)\n\n    # Predict from x and check that the resulting array has no NaN or inf values\n    y_pred = ir.predict(x)\n    assert np.all(np.isfinite(y_pred))\n", "def test_isotonic_regression_with_sample_weights_and_out_of_bounds():\n    # Test from #18000\n    # Check that IsotonicRegression handles out-of-bounds predictions correctly\n    # when sample_weights are used.\n    X = np.array([1, 2, 3, 4, 5])\n    y = np.array([1, 2, 3, 4, 5])\n    sample_weight = np.array([1, 0, 1, 0, 1])  # Zero weight for middle samples\n\n    ir = IsotonicRegression(out_of_bounds=\"raise\")\n    ir.fit(X, y, sample_weight=sample_weight)\n\n    # Prediction for in-bounds values should be fine\n    assert_array_equal(ir.predict([1, 5]), [1, 5])\n\n    # Prediction for out-of-bounds values should raise an error\n    with pytest.raises(ValueError, match=\"in x_new is below the interpolation range\"):\n        ir.predict([0])\n\n    with pytest.raises(ValueError, match=\"in x_new is above the interpolation range\"):\n        ir.predict([6])\n\n    # Check that the same behavior is true for out_of_bounds=\"clip\"\n    ir = IsotonicRegression(out_of_bounds=\"clip\")\n    ir.fit(X, y, sample_weight=sample_weight)\n\n    # Prediction for in-bounds values should be fine\n    assert_array_equal(ir.predict([1, 5]), [1, 5])\n\n    # Prediction for out-of-bounds values should be clipped\n    assert_array_equal(ir.predict([0, 6]), [1, 5])\n\n    # Check that the same behavior is true for out_of_bounds=\"nan\"\n    ir = IsotonicRegression(out_of_bounds=\"nan\")\n    ir.fit(X, y, sample_weight=sample_weight)\n\n    # Prediction for in-bounds values should be fine\n    assert_array_equal(ir.", "def test_isotonic_regression_2darray_input():\n    # Test from #15012\n    # Check that IsotonicRegression can handle 2darray with only 1 feature\n    X = np.arange(10)\n    X_2d = X.reshape(-1, 1)\n    y = np.arange(10)\n\n    ir = IsotonicRegression()\n    ir.fit(X_2d, y)\n    ir.transform(X_2d)\n    ir.predict(X_2d)\n\n    # Check that the output is the same as using 1d array\n    ir_1d = IsotonicRegression()\n    ir_1d.fit(X, y)\n    assert_array_almost_equal(ir.predict(X_2d), ir_1d.predict(X))\n    assert_array_almost_equal(ir.transform(X_2d), ir_1d.transform(X))\n"], "sample_84": ["def test_call_fixture_function_error_message(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture\n            return 1\n\n            with pytest.raises(pytest.fail.Exception) as e:\n                fix()\n            assert \"Fixture 'fix' called directly. Fixtures are not meant to be called directly, but are created automatically when test functions request them as parameters.\" in str(e.value)\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*1 passed*\"])\n", "def test_call_fixture_function_outside_context():\n    \"\"\"Check if an error is raised if a fixture function is called outside a pytest context (#4545)\"\"\"\n\n    @pytest.fixture\n        return 1\n\n    with pytest.raises(RuntimeError, match=\"Fixture function 'fix' cannot be called directly\"):\n        fix()\n", "def test_request_funcargvalue(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.fixture\n            return 1\n            val = request.getfuncargvalue(\"fix1\")\n            assert val == 1\n    \"\"\"\n    )\n    reprec = testdir.inline_run()\n    reprec.assertoutcome(passed=1)\n", "def test_getfuncargvalue_direct_call_error():\n    \"\"\"Check if a CallFixtureFuncError is raised if a fixture function is called directly (#4545)\"\"\"\n    @pytest.fixture\n        return 1\n\n    with pytest.raises(CallFixtureFuncError):\n        fix()\n", "def test_fixture_finalizer_function_not_collected(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.fixture\n            return 1\n\n            pass\n\n            assert test_app == 1\n        \"\"\"\n    )\n    reprec = testdir.inline_run()\n    reprec.assertoutcome(passed=1)\n"], "sample_85": ["def test_caplog_fixture(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n            logger = logging.getLogger(__name__)\n            logger.info('info message')\n            logger.warning('warning message')\n            logger.error('error message')\n\n            assert caplog.messages == ['info message', 'warning message', 'error message']\n            assert caplog.record_tuples == [\n                (__name__, logging.INFO, 'info message'),\n                (__name__, logging.WARNING, 'warning message'),\n                (__name__, logging.ERROR, 'error message'),\n            ]\n    \"\"\"\n    )\n    result = testdir.runpytest(\"--log-level=INFO\")\n    assert result.ret == 0\n", "def test_log_file_format_and_date_format(testdir):\n    log_file = testdir.tmpdir.join(\"pytest.log\").strpath\n    custom_format = '%(asctime)s %(levelname)s %(message)s'\n    custom_date_format = '%Y-%m-%d %H:%M:%S'\n\n    testdir.makeini(\n        f\"\"\"\n        [pytest]\n        log_file={log_file}\n        log_file_level = INFO\n        log_file_format = {custom_format}\n        log_file_date_format = {custom_date_format}\n        \"\"\"\n    )\n\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n            logging.getLogger().info(\"Custom format message\")\n    \"\"\"\n    )\n\n    result = testdir.runpytest()\n\n    assert result.ret == 0\n    assert os.path.isfile(log_file)\n    with open(log_file, encoding=\"utf-8\") as rfh:\n        contents = rfh.read()\n        assert \"Custom format message\" in contents\n        assert re.match(r'\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2} INFO Custom format message', contents)\n", "def test_set_level_method(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n        import pytest\n\n            caplog.set_level(logging.CRITICAL, logger='testcase-logger')\n            logger = logging.getLogger('testcase-logger')\n            logger.info('info message')\n            logger.critical('critical message')\n            assert len(caplog.messages) == 1\n            assert 'critical message' in caplog.messages\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*1 passed*\"])\n", "def test_log_cli_format(testdir):\n    # Custom log format\n    custom_format = '%(levelname)s:%(message)s'\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n            plugin = request.config.pluginmanager.getplugin('logging-plugin')\n            assert plugin.log_cli_handler.formatter._fmt == '{}'\n            logging.getLogger('catchlog').warning(\"This log message will be shown\")\n    \"\"\".format(custom_format)\n    )\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_cli=true\n        log_cli_format = {}\n        \"\"\".format(custom_format)\n    )\n\n    result = testdir.runpytest(\"-s\", \"--log-cli-level=WARNING\")\n\n    # fnmatch_lines does an assertion internally\n    result.stdout.fnmatch_lines(\n        [\"*test_log_cli_format.py*WARNING:This log message will be shown\"]\n    )\n    assert \"WARNING: This log message will be shown\" not in result.stdout.str()\n\n    # make sure that that we get a '0' exit code for the testsuite\n    assert result.ret == 0\n", "def test_log_file_format(testdir):\n    log_file = testdir.tmpdir.join(\"pytest.log\").strpath\n\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_file={}\n        log_file_level = DEBUG\n        log_file_format = %(asctime)s %(levelname)s %(message)s\n        log_file_date_format = %Y-%m-%d %H:%M:%S\n        \"\"\".format(\n            log_file\n        )\n    )\n    testdir.makepyfile(\n        \"\"\"\n            import logging\n            logger = logging.getLogger(\"testcase-logger\")\n                logger.info(\"message from test 1\")\n                assert True\n        \"\"\"\n    )\n    testdir.runpytest()\n    with open(log_file) as rfh:\n        contents = rfh.read()\n        assert re.search(r\"\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2} INFO message from test 1\", contents)\n"], "sample_4": ["    def test_file_to_stream(self):\n        with open(__file__, 'rb') as file:\n            r = StreamingHttpResponse(file)\n            self.assertFalse(file.closed)\n            self.assertEqual(r.getvalue(), file.read())\n            r.close()\n            self.assertTrue(file.closed)\n", "    def test_iter_content_with_file(self):\n        with open('test_file.txt', 'w') as f:\n            f.write('test content')\n        with open('test_file.txt', 'rb') as f:\n            r = HttpResponse(f)\n            self.assertEqual(r.content, b'test content')\n        os.remove('test_file.txt')\n", "    def test_content_length(self):\n        r = HttpResponse('content')\n        self.assertEqual(r['Content-Length'], '7')\n        r = StreamingHttpResponse(['streaming', 'content'])\n        with self.assertRaises(AttributeError):\n            r['Content-Length']\n", "def test_charset(self):\n    \"\"\"Test HttpResponseBase charset property.\"\"\"\n    r = HttpResponse(content_type='text/html; charset=iso-8859-1')\n    self.assertEqual(r.charset, 'iso-8859-1')\n    r.charset = 'utf-8'\n    self.assertEqual(r.charset, 'utf-8')\n    self.assertEqual(r['Content-Type'], 'text/html; charset=utf-8')\n", "def test_set_cookie_samesite_invalid_value(self):\n    response = HttpResponse()\n    with self.assertRaises(ValueError):\n        response.set_cookie('name', 'value', samesite='invalid')\n"], "sample_66": ["def test_exclude_of_parent_model(self):\n    class ValidationTestInline(TabularInline):\n        model = ValidationTestInlineModel\n        exclude = ['parent']\n\n    class TestModelAdmin(ModelAdmin):\n        inlines = [ValidationTestInline]\n\n    self.assertIsInvalid(\n        TestModelAdmin,\n        ValidationTestModel,\n        \"Cannot exclude the field 'parent', because it is the foreign key \"\n        \"to the parent model 'modeladmin.ValidationTestModel'.\",\n        \"admin.E201\",\n        invalid_obj=ValidationTestInline,\n    )\n", "    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            exclude = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'exclude' must be a list or tuple.\",\n            \"admin.E014\",\n        )\n", "    def test_readonly_fields_e034(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = \"invalid\"\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            \"admin.E034\",\n        )\n", "    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            exclude = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'exclude' must be a list or tuple.\",\n            \"admin.E014\",\n        )\n", "    def test_invalid_type(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            \"admin.E034\",\n        )\n"], "sample_116": ["def test_create_index_with_duplicates(app):\n    text = (\".. index:: docutils\\n\"\n            \".. index:: Python\\n\"\n            \".. index:: docutils\\n\"\n            \".. index:: Python\\n\")\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder)\n    assert len(index) == 2\n    assert index[0] == ('D', [('docutils', [[('', '#index-0'), ('', '#index-2')], [], None])])\n    assert index[1] == ('P', [('Python', [[('', '#index-1'), ('', '#index-3')], [], None])])\n", "def test_create_index_with_category(app):\n    text = (\".. index:: single: docutils; Category 1\\n\"\n            \".. index:: single: Python; Category 2\\n\"\n            \".. index:: Sphinx; Category 2\\n\"\n            \".. index:: single: Another; Category 1\\n\")\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder)\n\n    # check index is created correctly\n    assert len(index) == 3\n    assert index[0] == ('Category 1', [('Another', [[('', '#index-0')], [], 'Category 1']),\n                                       ('docutils', [[('', '#index-1')], [], 'Category 1'])])\n    assert index[1] == ('Category 2', [('Python', [[('', '#index-2')], [], 'Category 2']),\n                                       ('Sphinx', [[('', '#index-3')], [], 'Category 2'])])\n", "def test_create_index_with_duplicates(app):\n    text = (\".. index:: docutils\\n\"\n            \".. index:: docutils\\n\"\n            \".. index:: Python\\n\"\n            \".. index:: Python\\n\")\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder)\n    assert len(index) == 2\n    assert index[0] == ('D', [('docutils', [[('', '#index-0'), ('', '#index-1')], [], None])])\n    assert index[1] == ('P', [('Python', [[('', '#index-2'), ('', '#index-3')], [], None])])\n", "def test_create_index_with_error(app, caplog):\n    text = (\".. index:: single: docutils\\n\"\n            \".. index:: single: Python interpreter\\n\")\n    restructuredtext.parse(app, text)\n    IndexEntries(app.env).create_index(app.builder)\n    assert \"invalid value 'Python interpreter' for index entry 'single'\" in caplog.text\n", "def test_create_index_unicode_normalization(app):\n    text = (\".. index:: e\\N{COMBINING DIAERESIS}; Esperanto\\n\"\n            \".. index:: e\\N{COMBINING ACUTE ACCENT}; Esperanto\\n\")\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder)\n    assert len(index) == 1\n    assert index[0] == ('E', [('e\\N{COMBINING ACUTE ACCENT}', [[], [('Esperanto', [('', '#index-1')])], None]),\n                              ('e\\N{COMBINING DIAERESIS}', [[], [('Esperanto', [('', '#index-0')])], None])])\n"], "sample_52": ["def test_reference_field_by_limit_choices_to(self):\n    operation = FieldOperation(\n        \"Model\",\n        \"field\",\n        models.ForeignKey(\n            \"Other\",\n            models.CASCADE,\n            limit_choices_to={\"field__isnull\": False},\n        ),\n    )\n    self.assertIs(\n        operation.references_field(\"Other\", \"field\", \"migrations\"), True\n    )\n    self.assertIs(\n        operation.references_field(\"Other\", \"whatever\", \"migrations\"), False\n    )\n    self.assertIs(\n        operation.references_field(\"Missing\", \"whatever\", \"migrations\"), False\n    )\n", "def test_references_field_by_limit_choices_to(self):\n    operation = FieldOperation(\n        \"Model\",\n        \"field\",\n        models.ForeignKey(\n            \"Other\", models.CASCADE, limit_choices_to={'other_field': 'value'}\n        ),\n    )\n    self.assertIs(operation.references_field(\"Other\", \"other_field\", \"migrations\"), True)\n    self.assertIs(operation.references_field(\"Other\", \"whatever\", \"migrations\"), False)\n", "def test_references_field_by_related_name(self):\n    operation = FieldOperation(\n        \"Model\",\n        \"field\",\n        models.ForeignKey(\n            \"Other\",\n            models.CASCADE,\n            related_name=\"related_name\",\n            related_query_name=\"related_query_name\",\n        ),\n    )\n    self.assertIs(\n        operation.references_field(\"Model\", \"related_name\", \"migrations\"), True\n    )\n    self.assertIs(\n        operation.references_field(\"Model\", \"related_query_name\", \"migrations\"), True\n    )\n    self.assertIs(\n        operation.references_field(\"Other\", \"related_name\", \"migrations\"), False\n    )\n    self.assertIs(\n        operation.references_field(\"Other\", \"related_query_name\", \"migrations\"), False\n    )\n    self.assertIs(\n        operation.references_field(\"Missing\", \"whatever\", \"migrations\"), False\n    )\n", "def test_references_field_by_m2m_through_fields(self):\n    operation = FieldOperation(\n        \"Model\",\n        \"field\",\n        models.ManyToManyField(\n            \"Other\", m2m_through_fields=(\"first\", \"second\")\n        ),\n    )\n    self.assertIs(\n        operation.references_field(\"Other\", \"whatever\", \"migrations\"), True\n    )\n    self.assertIs(\n        operation.references_field(\"Model\", \"first\", \"migrations\"), True\n    )\n    self.assertIs(\n        operation.references_field(\"Model\", \"second\", \"migrations\"), True\n    )\n    self.assertIs(\n        operation.references_field(\"Missing\", \"whatever\", \"migrations\"), False\n    )\n", "    def test_add_field(self):\n        operation = migrations.AddField(\"Model\", \"new_field\", models.CharField(max_length=50))\n        self.assertIs(operation.references_model(\"Model\", \"migrations\"), True)\n        self.assertIs(operation.references_model(\"Other\", \"migrations\"), False)\n        self.assertIs(operation.references_field(\"Model\", \"new_field\", \"migrations\"), True)\n        self.assertIs(operation.references_field(\"Model\", \"old_field\", \"migrations\"), False)\n"], "sample_69": ["def test_sticky_edges():\n    fig, ax = plt.subplots()\n    ax.plot([0, 1], [0, 1], sticky_edges=[(0, 0), (1, 1)])\n    ax.set_xlim(-0.5, 1.5)\n    ax.set_ylim(-0.5, 1.5)\n", "def test_set_clip_path():\n    fig, ax = plt.subplots()\n    art = martist.Artist()\n    art.set_clip_path(None)\n    assert art._clippath is None\n\n    rect = mpatches.Rectangle((0, 0), 1, 1)\n    art.set_clip_path(rect)\n    assert isinstance(art._clippath, mtransforms.TransformedPatchPath)\n\n    path = mpath.Path([(0, 0), (1, 1), (2, 0)])\n    transform = mtransforms.IdentityTransform()\n    art.set_clip_path(path, transform)\n    assert isinstance(art._clippath, mtransforms.TransformedPath)\n\n    with pytest.raises(TypeError, match=\"Invalid arguments to set_clip_path\"):\n        art.set_clip_path(\"invalid\", \"arguments\")\n", "def test_mouseover():\n    art = martist.Artist()\n    assert art.mouseover is False\n    art.mouseover = True\n    assert art.mouseover is True\n    art.mouseover = False\n    assert art.mouseover is False\n    # Test the setter with a non-bool value\n    art.set_mouseover(1)\n    assert art.mouseover is True\n", "def test_set_clip_box():\n    # Create an artist and a test clip box\n    artist = martist.Artist()\n    test_clipbox = mtransforms.Bbox([[0, 0], [5, 5]])\n\n    # Set the clip box for the artist\n    artist.set_clip_box(test_clipbox)\n\n    # Check if the clip box is correctly set\n    assert artist.get_clip_box() == test_clipbox\n\n    # Check if the stale flag is correctly set\n    assert artist.stale is True\n\n    # Check if the pchanged method is called\n    artist.pchanged = MagicMock()\n    artist.set_clip_box(test_clipbox)\n    artist.pchanged.assert_called_once()\n", "def test_clip_path_fully_clipped_to_axes():\n    fig, ax = plt.subplots()\n    artist = mlines.Line2D([], [])\n    artist.set_clip_on(False)\n    assert not artist._fully_clipped_to_axes()\n\n    artist.set_clip_on(True)\n    assert not artist._fully_clipped_to_axes()\n\n    artist.set_clip_box(ax.bbox)\n    assert artist._fully_clipped_to_axes()\n\n    artist.set_clip_path(mpath.Path.unit_rectangle(), ax.transData)\n    assert not artist._fully_clipped_to_axes()\n\n    artist.set_clip_path(None)\n    assert not artist._fully_clipped_to_axes()\n\n    patch = mpatches.Rectangle((0, 0), 1, 1)\n    patch.set_transform(ax.transData)\n    clip_path = mpatches.PathPatch(mpath.Path.unit_circle(), transform=ax.transData)\n    artist.set_clip_path(clip_path)\n    assert not artist._fully_clipped_to_axes()\n\n    clip_path = mpatches.PathPatch(mpath.Path.unit_rectangle(), transform=ax.transData)\n    artist.set_clip_path(clip_path)\n    assert artist._fully_clipped_to_axes()\n\n    clip_path = mpatches.PathPatch(mpath.Path.unit_circle(), transform=ax.transAxes)\n    artist.set_clip_path(clip_path)\n    assert not artist._fully_clipped_to_axes()\n"], "sample_127": ["def test_latex_radian():\n    expr1 = pi*radian\n    assert latex(expr1) == r\"\\pi \\detokenize {radian}\"\n    expr2 = x*radian\n    assert latex(expr2) == r\"x \\detokenize {radian}\"\n    expr3 = sin(x*radian + pi*radian)\n    assert latex(expr3) == r'\\sin{\\left (x \\detokenize {radian} + \\pi \\detokenize {radian} \\right )}'\n", "def test_latex_radian():\n    expr1 = pi*radian\n    assert latex(expr1) == r\"\\pi \\cdot \\detokenize {radian}\"\n    expr2 = x*radian\n    assert latex(expr2) == r\"x \\cdot \\detokenize {radian}\"\n    expr3 = sin(x*radian + pi*radian)\n    assert latex(expr3) == r'\\sin{\\left (x \\cdot \\detokenize {radian} + \\pi \\cdot \\detokenize {radian} \\right )}'\n", "def test_latex_radian():\n    expr1 = pi*radian\n    assert latex(expr1) == r\"\\pi \\operatorname{radian}\"\n    expr2 = x*radian\n    assert latex(expr2) == r\"x \\operatorname{radian}\"\n    expr3 = sin(x*radian + pi*radian)\n    assert latex(expr3) == r'\\sin{\\left (x \\operatorname{radian} + \\pi \\operatorname{radian} \\right )}'\n", "def test_latex_radian():\n    expr1 = 2*radian\n    assert latex(expr1) == r\"2 \\operatorname{radian}\"\n    expr2 = x*radian\n    assert latex(expr2) == r\"x \\operatorname{radian}\"\n    expr3 = sin(x*radian + 2*radian)\n    assert latex(expr3) == r'\\sin{\\left (x \\operatorname{radian} + 2 \\operatorname{radian} \\right )}'\n", "def test_latex_radian():\n    expr1 = pi*radian\n    assert latex(expr1) == r\"\\pi \\mathrm{radian}\"\n    expr2 = x*radian\n    assert latex(expr2) == r\"x \\mathrm{radian}\"\n    expr3 = sin(x*radian)\n    assert latex(expr3) == r'\\sin{\\left (x \\mathrm{radian} \\right )}'\n"], "sample_65": ["def test_join09(self):\n    output = self.engine.render_to_string(\n        \"join09\", {\"a\": [\"Alpha\", \"Beta & me\"], \"var\": mark_safe(\" & \")}\n    )\n    self.assertEqual(output, \"Alpha & Beta & me\")\n", "def test_join09(self):\n    output = self.engine.render_to_string(\n        \"join09\", {\"a\": [\"Alpha\", \"Beta <me>\"], \"var\": \" & \"}\n    )\n    self.assertEqual(output, \"Alpha &amp; Beta &lt;me&gt;\")\n", "def test_join09(self):\n    output = self.engine.render_to_string(\"join09\", {\"a\": [\"apple\", \"banana\", \"cherry\"]})\n    self.assertEqual(output, \"apple | banana | cherry\")\n", "def test_join09(self):\n    output = self.engine.render_to_string(\n        \"join09\", {\"a\": [\"Alpha\", \"Beta & me\"], \"var\": mark_safe(\" - \")}\n    )\n    self.assertEqual(output, \"ALPHA - BETA &AMP; ME\")\n", "def test_join09(self):\n    output = self.engine.render_to_string(\"join09\", {\"a\": [\"alpha\", \"beta\", \"gamma\"]})\n    self.assertEqual(output, \"alpha & beta & gamma\")\n"], "sample_28": ["    def setUp(self):\n        self.user_active_staff = User.objects.create_user(\n            username='active_staff', password='secret', is_active=True, is_staff=True)\n        self.user_inactive_staff = User.objects.create_user(\n            username='inactive_staff', password='secret', is_active=False, is_staff=True)\n        self.user_active_not_staff = User.objects.create_user(\n            username='active_not_staff', password='secret', is_active=True, is_staff=False)\n        self.user_inactive_not_staff = User.objects.create_user(\n            username='inactive_not_staff', password='secret', is_active=False, is_staff=False)\n", "    def setUpTestData(cls):\n        cls.u1 = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n        cls.u2 = User.objects.create_user(username='user', password='secret', email='user@example.com')\n", "    def test_actions(self):\n        actions = dict(self.site.actions)\n        self.assertEqual(actions['delete_selected'], delete_selected)\n        self.assertIn('test_action', actions)\n        self.assertNotEqual(actions['test_action'], delete_selected)\n", "    def test_actions(self):\n        self.assertEqual(\n            set(self.site.actions),\n            set([('delete_selected', delete_selected)]),\n        )\n", "    def setUpTestData(cls):\n        cls.u1 = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n        cls.u2 = User.objects.create_user(username='user', password='secret', email='user@example.com')\n"], "sample_89": ["def test_node_location(testdir):\n    p = testdir.makepyfile(\n        \"\"\"\n            pass\n    \"\"\"\n    )\n    items = testdir.getitems(str(p))\n    item = items[0]\n    assert item.location == (p, None, \"test_func\")\n", "def test_node_location(testdir):\n    testdir.makepyfile(\"def test_func():\\n    assert True\")\n    result = testdir.runpytest()\n    result.assert_outcomes(passed=1)\n    item = testdir.getitems()[0]\n    assert item.location[0].endswith(\"test_func.py\")\n    assert item.location[1] is None\n    assert item.location[2] == \"test_func\"\n", "def test_node_location_to_relpath(testdir):\n    \"\"\"Test that _node_location_to_relpath handles relative paths correctly.\"\"\"\n    p = testdir.makepyfile(\"def test(): pass\")\n    class FakeSession:\n            return fspath.relto(self.config.rootdir)\n    node = nodes.Item.from_parent(FakeSession(), fspath=p)\n    assert node.location[0] == p.relto(testdir.tmpdir)\n", "def test_node_initialization_with_session_and_config():\n    \"\"\"Test that Node initialization works correctly with session and config.\"\"\"\n    config = Config()\n    session = object()  # Replace with a real Session instance if necessary\n\n    node = nodes.Node(\"test_node\", config=config, session=session)\n    assert node.config is config\n    assert node.session is session\n", "def test_FSCollector_collectfile(testdir):\n    p = testdir.makepyfile(\"\"\"\n        import pytest\n        @pytest.fixture\n            pass\n            assert True\n            assert False\n    \"\"\")\n\n    collector = nodes.FSCollector.from_parent(parent=None, fspath=p, session=None)\n    items = collector._collectfile(p)\n    assert len(items) == 2\n    assert isinstance(items[0], nodes.Item)\n    assert isinstance(items[1], nodes.Item)\n    assert items[0].name == \"test_1\"\n    assert items[1].name == \"test_2[fix]\"\n"], "sample_80": ["def test_format_timedelta():\n    cases = [\n        (pd.Timedelta(\"10 days 1 hour\"), \"10 days 01:00:00\", \"10 days\", \"01:00:00\"),\n        (pd.Timedelta(\"-3 days\"), \"-3 days +00:00:00\", \"-3 days\", \"00:00:00\"),\n        (pd.Timedelta(\"3 hours\"), \"0 days 03:00:00\", \"0 days\", \"03:00:00\"),\n        (pd.Timedelta(\"NaT\"), \"NaT\", \"NaT\", \"NaT\"),\n    ]\n    for item, expected, date_expected, time_expected in cases:\n        actual = formatting.format_timedelta(item)\n        assert expected == actual\n        actual_date = formatting.format_timedelta(item, timedelta_format=\"date\")\n        assert date_expected == actual_date\n        actual_time = formatting.format_timedelta(item, timedelta_format=\"time\")\n        assert time_expected == actual_time\n", "def test_format_timedelta():\n    cases = [\n        (np.timedelta64(10, 'D'), '10 days', '00:00:00'),\n        (np.timedelta64(-3, 'h'), '-1 days +21:00:00', '03:00:00'),\n        (np.timedelta64(500, 'ms'), '0 days 00:00:00.500000', '00:00:00.500000'),\n        (pd.Timedelta('10 days 1 hour'), '10 days', '01:00:00'),\n        (pd.Timedelta('-3 days'), '-3 days +00:00:00', '00:00:00'),\n        (pd.Timedelta('3 hours'), '0 days', '03:00:00'),\n        (pd.Timedelta('NaT'), 'NaT', 'NaT'),\n    ]\n    for item, expected_date, expected_time in cases:\n        actual_date = formatting.format_timedelta(item, timedelta_format='date')\n        actual_time = formatting.format_timedelta(item, timedelta_format='time')\n        assert expected_date == actual_date\n        assert expected_time == actual_time\n", "def test_format_timedelta_large_values():\n    cases = [\n        (pd.Timedelta(\"100000 days\"), \"100000 days\"),\n        (pd.Timedelta(\"100000 days 1 hour\"), \"100000 days 01:00:00\"),\n        (pd.Timedelta(\"100000 days 1 hour 30 minutes\"), \"100000 days 01:30:00\"),\n        (pd.Timedelta(\"100000 days 1 hour 30 minutes 20 seconds\"), \"100000 days 01:30:20\"),\n        (pd.Timedelta(\"100000 days 1 hour 30 minutes 20 seconds 500 milliseconds\"), \"100000 days 01:30:20.500000\"),\n    ]\n    for item, expected in cases:\n        actual = formatting.format_timedelta(item)\n        assert expected == actual\n", "def test_last_item_for_different_shapes():\n    array = np.arange(120)\n\n    shapes = [(12, 10), (1, 120), (2, 2, 3, 5)]\n    expected = np.array([119])\n\n    for shape in shapes:\n        result = formatting.last_item(array.reshape(shape))\n        assert result == expected\n", "def test_format_array_flat_large_input():\n    array = np.arange(100000)\n    max_width = 100\n    actual = formatting.format_array_flat(array, max_width)\n    expected = \"0 ... 99999\"\n    assert expected == actual\n"], "sample_124": ["def test_tanh_expansion():\n    x, y = symbols('x,y')\n    assert tanh(x+y).expand(trig=True) == (tanh(x) + tanh(y)) / (1 + tanh(x)*tanh(y))\n    assert tanh(2*x).expand(trig=True).expand() == 2*tanh(x) / (1 + tanh(x)**2)\n    assert tanh(3*x).expand(trig=True).expand() == (3*tanh(x) - 3*tanh(x)**3) / (1 + 3*tanh(x)**2 - tanh(x)**4)\n", "def test_tanh_expansion():\n    x, y = symbols('x,y')\n    assert tanh(x+y).expand(trig=True) == (tanh(x) + tanh(y)) / (1 + tanh(x)*tanh(y))\n    assert tanh(2*x).expand(trig=True).expand() == 2*tanh(x) / (1 + tanh(x)**2)\n    assert tanh(3*x).expand(trig=True).expand() == (3*tanh(x) - tanh(x)**3) / (1 - tanh(x)**2)\n", "def test_tanh_expansion():\n    x, y = symbols('x,y')\n    assert tanh(x+y).expand(trig=True) == (tanh(x) + tanh(y))/(1 + tanh(x)*tanh(y))\n    assert tanh(2*x).expand(trig=True) == 2*tanh(x)/(1 + tanh(x)**2)\n    assert tanh(3*x).expand(trig=True).expand() == (3*tanh(x) - 3*tanh(x)**3)/(1 + 3*tanh(x)**2 - tanh(x)**4)\n", "def test_tanh_expansion():\n    x, y = symbols('x,y')\n    assert tanh(x+y).expand(trig=True) == (tanh(x) + tanh(y)) / (1 + tanh(x)*tanh(y))\n    assert tanh(2*x).expand(trig=True).expand() == 2*tanh(x) / (1 + tanh(x)**2)\n    assert tanh(3*x).expand(trig=True).expand() == (3*tanh(x) - 3*tanh(x)**3) / (1 - 3*tanh(x)**2)\n", "def test_tanh_expansion():\n    x, y = symbols('x,y')\n    assert tanh(x+y).expand(trig=True) == (tanh(x) + tanh(y)) / (1 + tanh(x)*tanh(y))\n    assert tanh(2*x).expand(trig=True) == 2*tanh(x) / (1 + tanh(x)**2)\n    assert tanh(3*x).expand(trig=True) == (3*tanh(x) - 3*tanh(x)**3) / (1 - 3*tanh(x)**2)\n"], "sample_64": ["def test_submit_row_save_permission_required(self):\n    change_user = User.objects.create_user(\n        username=\"change_user\", password=\"secret\", is_staff=True\n    )\n    request = self.request_factory.get(\n        reverse(\"admin:auth_user_change\", args=[self.superuser.pk])\n    )\n    request.user = change_user\n    admin = UserAdmin(User, site)\n    response = admin.change_view(request, str(self.superuser.pk))\n    template_context = submit_row(response.context_data)\n    self.assertIs(template_context[\"show_save\"], False)\n\n    change_user.user_permissions.add(\n        get_perm(User, get_permission_codename(\"change\", User._meta)),\n    )\n    response = admin.change_view(request, str(self.superuser.pk))\n    template_context = submit_row(response.context_data)\n    self.assertIs(template_context[\"show_save\"], True)\n", "def test_submit_row_without_change_permission(self):\n    \"\"\"\n    submit_row template tag should hide save buttons if user doesn't have change permission.\n    \"\"\"\n    change_user = User.objects.create_user(\n        username=\"change_user\", password=\"secret\", is_staff=True\n    )\n    request = self.request_factory.get(\n        reverse(\"admin:auth_user_change\", args=[self.superuser.pk])\n    )\n    request.user = change_user\n    admin = UserAdmin(User, site)\n    response = admin.change_view(request, str(self.superuser.pk))\n    template_context = submit_row(response.context_data)\n    self.assertIs(template_context[\"show_save\"], False)\n    self.assertIs(template_context[\"show_save_and_add_another\"], False)\n    self.assertIs(template_context[\"show_save_and_continue\"], False)\n", "def test_submit_row_save_as_new_change_permission_required(self):\n    add_user = User.objects.create_user(\n        username=\"add_user\", password=\"secret\", is_staff=True\n    )\n    add_user.user_permissions.add(\n        get_perm(User, get_permission_codename(\"add\", User._meta)),\n    )\n    request = self.request_factory.get(\n        reverse(\"admin:auth_user_change\", args=[self.superuser.pk])\n    )\n    request.user = add_user\n    admin = UserAdmin(User, site)\n    admin.save_as = True\n    response = admin.change_view(request, str(self.superuser.pk))\n    template_context = submit_row(response.context_data)\n    self.assertIs(template_context[\"show_save_as_new\"], False)\n\n    change_user = User.objects.create_user(\n        username=\"change_user\", password=\"secret\", is_staff=True\n    )\n    change_user.user_permissions.add(\n        get_perm(User, get_permission_codename(\"add\", User._meta)),\n        get_perm(User, get_permission_codename(\"change\", User._meta)),\n    )\n    request = self.request_factory.get(\n        reverse(\"admin:auth_user_change\", args=[self.superuser.pk])\n    )\n    request.user = change_user\n    response = admin.change_view(request, str(self.superuser.pk))\n    template_context = submit_row(response.context_data)\n    self.assertIs(template_context[\"show_save_as_new\"], True)\n", "def test_override_submit_row_template_tags(self):\n    \"\"\"\n    admin_modify submit_row template tag follows the standard search pattern\n    admin/app_label/model/template.html.\n    \"\"\"\n    article = Article.objects.all()[0]\n    request = self.request_factory.get(\n        reverse(\"admin:admin_views_article_change\", args=[article.pk])\n    )\n    request.user = self.superuser\n    admin = ArticleAdmin(Article, site)\n    extra_context = {\"show_custom_button\": True}\n    response = admin.change_view(\n        request, str(article.pk), extra_context=extra_context\n    )\n    response.render()\n    self.assertContains(response, 'name=\"_custom_button\"')\n    self.assertContains(response, \"override-submit_row\")\n", "def test_override_submit_row_template_tag(self):\n    \"\"\"\n    submit_row template tag should allow overriding.\n    \"\"\"\n    request = self.request_factory.get(\n        reverse(\"admin:auth_user_change\", args=[self.superuser.pk])\n    )\n    request.user = self.superuser\n    admin = UserAdmin(User, site)\n    extra_context = {\"show_save\": False, \"extra\": True}\n    response = admin.change_view(\n        request, str(self.superuser.pk), extra_context=extra_context\n    )\n    template_context = submit_row(response.context_data)\n    self.assertIs(template_context[\"extra\"], True)\n    self.assertIs(template_context[\"show_save\"], False)\n    self.assertContains(response, 'name=\"override-save\"')\n"], "sample_15": ["def test_valid_script_consistent_language_settings(self):\n    tests = [\n        'sr-Latn',  # language + script\n    ]\n    for tag in tests:\n        with self.subTest(tag), self.settings(LANGUAGE_CODE=tag):\n            self.assertEqual(check_language_settings_consistent(None), [])\n", "def test_valid_script_consistent_language_settings(self):\n    # Test for a valid language code with a script that is not in the LANGUAGES setting.\n    # The 'sr-Latn' code is valid, but 'Latn' is not listed in LANGUAGES.\n    tag = 'sr-Latn'\n    with self.settings(LANGUAGE_CODE=tag):\n        self.assertEqual(check_language_settings_consistent(None), [\n            Error('You have provided a value for the LANGUAGE_CODE setting that is not in the LANGUAGES setting.', id='translation.E004'),\n        ])\n", "    def test_valid_script_consistent_language_settings(self):\n        tests = [\n            # language + script.\n            'zh-Hans',\n            'sgn-BE-FR',\n        ]\n        for tag in tests:\n            with self.subTest(tag), self.settings(LANGUAGE_CODE=tag):\n                self.assertEqual(check_language_settings_consistent(None), [])\n", "def test_valid_script_consistent_language_settings(self):\n    # language + script.\n    tests = ['zh-Hans', 'sr-Latn']\n    for tag in tests:\n        with self.subTest(tag), self.settings(LANGUAGE_CODE=tag):\n            self.assertEqual(check_language_settings_consistent(None), [])\n", "def test_valid_script_consistent_language_settings(self):\n    for tag in ['sr-Latn', 'sr-Cyrl', 'sr-Latn-RS']:\n        with self.subTest(tag), self.settings(LANGUAGE_CODE=tag):\n            self.assertEqual(check_language_settings_consistent(None), [])\n"], "sample_2": ["def test_to_fits_2():\n    \"\"\"\n    Test to_fits() with SIP distortion.\n    \"\"\"\n    fits_name = get_pkg_data_filename('data/sip.fits')\n    w = wcs.WCS(fits_name)\n    wfits = w.to_fits()\n    assert isinstance(wfits, fits.HDUList)\n    assert isinstance(wfits[0], fits.PrimaryHDU)\n    assert isinstance(wfits[1], fits.ImageHDU)\n", "def test_to_fits_2():\n    \"\"\"\n    Test to_fits() with SIP distortion.\n    \"\"\"\n    fits_name = get_pkg_data_filename('data/sip.fits')\n    w = wcs.WCS(fits_name)\n    wfits = w.to_fits()\n    assert isinstance(wfits, fits.HDUList)\n    assert isinstance(wfits[0], fits.PrimaryHDU)\n    assert isinstance(wfits[1], fits.ImageHDU)\n", "def test_axis_type_names():\n    w = wcs.WCS(naxis=3)\n    w.wcs.ctype = ['RA---TAN', 'DEC--TAN', 'FREQ']\n    w.wcs.cname = ['Right Ascension', 'Declination', 'Frequency']\n    assert w.axis_type_names == ['Right Ascension', 'Declination', 'FREQ']\n", "def test_sip_with_altkey_no_sip():\n    \"\"\"\n    Test that when creating a WCS object using a key, if no SIP is\n    found for that key, the primary SIP is not used.\n    \"\"\"\n    with fits.open(get_pkg_data_filename('data/sip.fits')) as f:\n        w = wcs.WCS(f[0].header)\n    # create a header with two WCSs.\n    h1 = w.to_header(relax=True, key='A')\n    h2 = w.to_header(relax=False)\n    h1['CTYPE1A'] = \"RA---SIN\"\n    h1['CTYPE2A'] = \"DEC--SIN\"\n    h1.update(h2)\n    w = wcs.WCS(h1, key='A')\n    assert w.sip is None\n", "def test_slice_method():\n    w = wcs.WCS(naxis=3)\n    w._naxis = [10, 20, 30]\n    w.wcs.crpix = [5, 10, 15]\n    w.wcs.cdelt = [0.1, 0.2, 0.3]\n    w.wcs.crval = [1, 2, 3]\n\n    sliced_w = w[::2, 5:15, 10:20]\n    assert sliced_w._naxis == [5, 10, 10]\n    assert_array_almost_equal(sliced_w.wcs.crpix, [2.5, 7.5, 8.0])\n    assert_array_almost_equal(sliced_w.wcs.cdelt, [0.2, 0.2, 0.3])\n    assert_array_almost_equal(sliced_w.wcs.crval, [1.05, 2.6, 3.9])\n"], "sample_41": ["def test_formset_without_data_is_not_valid(self):\n    formset = self.make_choiceformset()\n    self.assertFalse(formset.is_valid())\n", "def test_formset_with_deletion_and_order(self):\n    \"\"\"FormSets with ordering and deletion should handle deleted forms correctly.\"\"\"\n    ChoiceFormSet = formset_factory(Choice, can_order=True, can_delete=True)\n    initial = [\n        {'choice': 'Calexico', 'votes': 100},\n        {'choice': 'Fergie', 'votes': 900},\n        {'choice': 'The Decemberists', 'votes': 500},\n    ]\n    formset = ChoiceFormSet(initial=initial, auto_id=False, prefix='choices')\n    data = {\n        'choices-TOTAL_FORMS': '4',  # the number of forms rendered\n        'choices-INITIAL_FORMS': '3',  # the number of forms with initial data\n        'choices-MIN_NUM_FORMS': '0',  # min number of forms\n        'choices-MAX_NUM_FORMS': '0',  # max number of forms\n        'choices-0-choice': 'Calexico',\n        'choices-0-votes': '100',\n        'choices-0-ORDER': '1',\n        'choices-0-DELETE': '',\n        'choices-1-choice': 'Fergie',\n        'choices-1-votes': '900',\n        'choices-1-ORDER': '2',\n        'choices-1-DELETE': 'on',\n        'choices-2-choice': 'The Decemberists',\n        'choices-2-votes': '500',\n        'choices-2-ORDER': '0',\n        'choices-2-DELETE': '',\n        'choices-3-choice': '',\n        'choices-3-votes': '',\n        'choices-3-ORDER': '',\n        'choices-3-DELETE': '',\n    }\n    formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n    self.assertTrue(formset.is_valid())\n    self.assertEqual(\n        [form.cleaned_data", "    def test_add_fields_with_order_and_delete(self):\n        \"\"\"FormSets with ordering and deletion fields are added correctly.\"\"\"\n        ChoiceFormSet = formset_factory(Choice, can_order=True, can_delete=True)\n        formset = ChoiceFormSet()\n        self.assertIn('ORDER', formset.forms[0].fields)\n        self.assertIn('DELETE', formset.forms[0].fields)\n", "    def test_absolute_max_with_data(self):\n        data = {\n            'form-TOTAL_FORMS': '1001',\n            'form-INITIAL_FORMS': '0',\n            'form-MAX_NUM_FORMS': '1001',\n        }\n        AbsoluteMaxFavoriteDrinksFormSet = formset_factory(\n            FavoriteDrinkForm,\n            absolute_max=3000,\n        )\n        formset = AbsoluteMaxFavoriteDrinksFormSet(data=data)\n        self.assertIs(formset.is_valid(), True)\n        self.assertEqual(len(formset.forms), 1001)\n        # absolute_max provides a hard limit.\n        data['form-TOTAL_FORMS'] = '3001'\n        formset = AbsoluteMaxFavoriteDrinksFormSet(data=data)\n        self.assertIs(formset.is_valid(), False)\n        self.assertEqual(len(formset.forms), 3000)\n        self.assertEqual(\n            formset.non_form_errors(),\n            ['Please submit at most 1000 forms.'],\n        )\n", "def test_custom_error_messages(self):\n    \"\"\"Custom error messages are used in a formset.\"\"\"\n    custom_errors = {'required': 'This field is custom required.'}\n    ChoiceFormSet = formset_factory(Choice, error_messages={'required': custom_errors['required']})\n    data = {\n        'choices-TOTAL_FORMS': '1',\n        'choices-INITIAL_FORMS': '0',\n        'choices-MIN_NUM_FORMS': '0',\n        'choices-MAX_NUM_FORMS': '0',\n        'choices-0-choice': '',\n        'choices-0-votes': '',\n    }\n    formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(formset.errors, [{\n        'choice': [custom_errors['required']],\n        'votes': [custom_errors['required']],\n    }])\n"], "sample_132": ["def test_ordered_points():\n    p = Point2D(1, 2)\n    q = Point2D(3, 4)\n    r = Point2D(2, 3)\n    assert _ordered_points((p, q)) == (p, q)\n    assert _ordered_points((q, p)) == (p, q)\n    assert _ordered_points((p, r, q)) == (p, q, r)\n", "def test_are_coplanar():\n    from sympy import Point3D, Line3D, Plane\n    from sympy.geometry.util import are_coplanar\n\n    # Test coplanar points\n    p1, p2, p3, p4 = Point3D(1, 2, 3), Point3D(4, 5, 6), Point3D(7, 8, 9), Point3D(10, 11, 12)\n    assert are_coplanar(p1, p2, p3, p4) == True\n\n    # Test non-coplanar points\n    p5 = Point3D(13, 14, 15)\n    assert are_coplanar(p1, p2, p3, p5) == False\n\n    # Test coplanar lines\n    l1, l2 = Line3D(p1, p2), Line3D(p3, p4)\n    assert are_coplanar(l1, l2) == True\n\n    # Test coplanar line and point\n    assert are_coplanar(l1, p5) == True\n\n    # Test non-coplanar line and point\n    p6 = Point3D(16, 17, 18)\n    assert are_coplanar(l1, p6) == False\n\n    # Test coplanar plane and points\n    plane = Plane(p1, p2, p3)\n    assert are_coplanar(plane, p4) == True\n    assert are_coplanar(plane, p6) == False\n\n    # Test coplanar plane and line\n    assert are_coplanar(plane, l2) == True\n\n    # Test non-coplanar plane and line\n    l3 = Line3D(p5, p6)\n    assert are_coplanar(plane, l3) == False\n", "def test_are_coplanar():\n    from sympy import Line3D, Point3D\n    from sympy.geometry.util import are_coplanar\n\n    # Test with coplanar entities\n    a = Line3D(Point3D(5, 0, 0), Point3D(1, -1, 1))\n    b = Line3D(Point3D(0, -2, 0), Point3D(3, 1, 1))\n    c = Line3D(Point3D(0, -1, 0), Point3D(5, -1, 9))\n    assert not are_coplanar(a, b, c)\n\n    # Test with non-coplanar entities\n    d = Line3D(Point3D(0, 0, 1), Point3D(0, 1, 0))\n    assert not are_coplanar(a, b, d)\n\n    # Test with collinear entities\n    e = Line3D(Point3D(0, 0, 0), Point3D(1, 1, 1))\n    f = Line3D(Point3D(2, 2, 2), Point3D(3, 3, 3))\n    assert are_coplanar(e, f)\n\n    # Test with coplanar points\n    g = Point3D(0, 0, 0)\n    h = Point3D(1, 0, 0)\n    i = Point3D(0, 1, 0)\n    assert are_coplanar(g, h, i)\n\n    # Test with non-coplanar points\n    j = Point3D(0, 0, 1)\n    assert not are_coplanar(g, h, i, j)\n", "def test_are_coplanar():\n    from sympy import Point3D, Line3D, Plane\n    from sympy.geometry.util import are_coplanar\n\n    # Test with Plane\n    p = Plane(Point3D(0, 0, 0), normal_vector=(1, 0, 0))\n    l = Line3D(Point3D(1, 1, 1), Point3D(2, 2, 2))\n    assert are_coplanar(p, l) == False\n\n    # Test with all Point3D\n    p1 = Point3D(0, 0, 0)\n    p2 = Point3D(1, 1, 1)\n    p3 = Point3D(2, 2, 2)\n    assert are_coplanar(p1, p2, p3) == True\n\n    # Test with LinearEntity3D and Point3D\n    l1 = Line3D(Point3D(0, 0, 0), Point3D(1, 1, 1))\n    p = Point3D(2, 2, 2)\n    assert are_coplanar(l1, p) == True\n\n    # Test with 2D objects\n    # Note: This test is commented out because the current implementation of are_coplanar\n    # does not support 2D objects. This test should be uncommented once the support is added.\n    # from sympy.geometry import Point, Line\n    # p1 = Point(0, 0)\n    # p2 = Point(1, 1)\n    # l = Line(p1, p2)\n    # assert are_coplanar(p1, l) == True\n", "def test_ordered_points():\n    # Test _ordered_points function\n    p1 = Point(1, 2)\n    p2 = Point(2, 1)\n    p3 = Point(1, 2)  # same as p1\n    assert _ordered_points((p1, p2, p3)) == (p1, p3, p2)\n    assert _ordered_points((p2, p1, p3)) == (p1, p3, p2)\n"], "sample_152": ["def test_array_iteration():\n    for ArrayType in array_types:\n        test_array = ArrayType([[1, 2], [3, 4]])\n        iterator = iter(test_array)\n        assert next(iterator) == Array([1, 2])\n        assert next(iterator) == Array([3, 4])\n        with raises(StopIteration):\n            next(iterator)\n", "def test_array_addition():\n    for ArrayType in array_types:\n        a = ArrayType([1, 2, 3])\n        b = ArrayType([4, 5, 6])\n        assert a + b == ArrayType([5, 7, 9])\n        assert a - b == ArrayType([-3, -3, -3])\n        assert a * 2 == ArrayType([2, 4, 6])\n        assert 2 * a == ArrayType([2, 4, 6])\n        assert a / 2 == ArrayType([0.5, 1, 1.5])\n", "def test_array_multiplication():\n    for ArrayType in array_types:\n        A = ArrayType([[1, 2], [3, 4]])\n        B = ArrayType([[5, 6], [7, 8]])\n        C = A * B\n        assert C == ArrayType([[5, 12], [21, 32]])\n        assert C.shape == A.shape == B.shape\n        assert A * 2 == ArrayType([[2, 4], [6, 8]])\n        assert 2 * A == ArrayType([[2, 4], [6, 8]])\n", "def test_array_iteration():\n    for ArrayType in array_types:\n        test_array = ArrayType([1, 2, 3, 4, 5])\n        iterator = iter(test_array)\n        assert next(iterator) == 1\n        assert next(iterator) == 2\n        assert next(iterator) == 3\n        assert next(iterator) == 4\n        assert next(iterator) == 5\n        raises(StopIteration, lambda: next(iterator))\n", "def test_issue_19530():\n    for array_type in array_types:\n        A = array_type([[1, 2], [3, 4]])\n        assert A.transpose() == array_type([[1, 3], [2, 4]])\n"], "sample_51": ["def test_index_hidden_directory(self):\n    response = self.client.get(\"/%s/.hidden_dir/\" % self.prefix)\n    self.assertEqual(404, response.status_code)\n", "def test_directory_index_with_dotfiles(self):\n    response = self.client.get(\"/%s/\" % self.prefix)\n    self.assertNotIn(\".hidden/\", response.context[\"file_list\"])\n    self.assertNotIn(\".gitignore\", response.context[\"file_list\"])\n", "    def test_directory_index_template(self):\n        \"\"\"\n        The directory index template is correctly rendered with the correct context.\n        \"\"\"\n        response = self.client.get(\"/%s/subdir/\" % self.prefix)\n        self.assertTemplateUsed(response, \"static/directory_index.html\")\n        self.assertEqual(response.context[\"directory\"], \"subdir/\")\n        self.assertEqual(response.context[\"file_list\"], [\"visible\"])\n", "def test_index_with_special_characters(self):\n    file_name = \"file with spaces.txt\"\n    response = self.client.get(\"/%s/%s\" % (self.prefix, quote(file_name)))\n    self.assertEqual(200, response.status_code)\n", "def test_not_modified_since_different_size(self):\n    file_name = \"file.txt\"\n    response = self.client.get(\n        \"/%s/%s\" % (self.prefix, file_name),\n        HTTP_IF_MODIFIED_SINCE=\"Thu, 1 Jan 1970 00:00:00 GMT; length=0\"\n    )\n    response_content = b\"\".join(response)\n    with open(path.join(media_dir, file_name), \"rb\") as fp:\n        self.assertEqual(fp.read(), response_content)\n    self.assertEqual(len(response_content), int(response.headers[\"Content-Length\"]))\n"], "sample_134": ["def test_cbrt():\n    if not np:\n        skip(\"NumPy not installed\")\n\n    f = lambdify((a,), Cbrt(a), 'numpy')\n    assert abs(f(8) - 2) < 1e-16\n", "def test_cbrt():\n    if not np:\n        skip(\"NumPy not installed\")\n\n    f = lambdify((a,), Cbrt(a), 'numpy')\n    assert abs(f(27) - 3) < 1e-16\n", "def test_Cbrt():\n    if not np:\n        skip(\"NumPy not installed\")\n    assert abs(lambdify((a,), Cbrt(a), 'numpy')(8) - 2) < 1e-16\n", "def test_cbrt():\n    if not np:\n        skip(\"NumPy not installed\")\n    assert abs(lambdify((a,), cbrt(a), 'numpy')(27) - 3) < 1e-16\n", "def test_cbrt():\n    if not np:\n        skip(\"NumPy not installed\")\n    assert abs(lambdify((a,), Cbrt(a), 'numpy')(8) - 2) < 1e-16\n"], "sample_55": ["def test_command_force_color_no_color_together(self):\n    msg = \"'no_color' and 'force_color' can't be used together.\"\n    with self.assertRaisesMessage(CommandError, msg):\n        management.call_command(\"dance\", no_color=True, force_color=True)\n", "def test_call_command_with_extra_parameters_should_raise_exception(self):\n    msg = \"Error: unrecognized arguments: extra\"\n    with self.assertRaisesMessage(SystemExit, msg):\n        management.call_command(\"dance\", \"extra\")\n", "def test_call_command_with_positional_args(self):\n    out = StringIO()\n    management.call_command(\"positional_args\", \"one\", \"two\", three=\"three\", stdout=out)\n    self.assertIn(\"one\", out.getvalue())\n    self.assertIn(\"two\", out.getvalue())\n    self.assertIn(\"three\", out.getvalue())\n", "def test_call_command_with_app_labels_and_parameters_and_help(self):\n    out = StringIO()\n    management.call_command(\"hal\", \"myapp\", \"--help\", stdout=out)\n    self.assertIn(\"usage: manage.py hal\", out.getvalue())\n    self.assertIn(\"positional arguments:\", out.getvalue())\n    self.assertIn(\"optional arguments:\", out.getvalue())\n", "def test_command_with_no_arguments_should_not_raise_error(self):\n    out = StringIO()\n    management.call_command(\"hal\", stdout=out)\n    self.assertEqual(out.getvalue(), \"\\nDave, I can't do that.\\n\")\n"], "sample_49": ["def test_get_template_directories_with_nonexistent_directory(self):\n    self.assertSetEqual(\n        autoreload.get_template_directories(),\n        set()\n    )\n", "def test_django_admin_directories_excluded(self, mock_reset):\n    self.assertIsNone(autoreload.template_changed(None, Path(ROOT / 'admin' / 'index.html')))\n    mock_reset.assert_not_called()\n", "def test_get_template_directories_non_django_path(self, mock_to_path):\n    mock_to_path.return_value = '/non/django/path'\n    self.assertSetEqual(\n        autoreload.get_template_directories(),\n        set()\n    )\n", "def test_non_django_path_in_template_directories(self, mock_is_django_path):\n    mock_is_django_path.return_value = False\n    self.assertSetEqual(\n        autoreload.get_template_directories(),\n        {\n            ROOT / 'non_django_path',\n        }\n    )\n", "def test_custom_loader_without_get_dirs(self, mock_reset):\n    custom_template_path = EXTRA_TEMPLATES_DIR / 'custom_template.html'\n    self.assertIsNone(autoreload.template_changed(None, custom_template_path))\n    mock_reset.assert_not_called()\n"], "sample_13": ["    def test_no_escaping_needed(self):\n        tests = (\n            ('http://example.com', 'http://example.com'),\n            ('/path', '/path'),\n        )\n        for url, expected in tests:\n            with self.subTest(url=url):\n                self.assertEqual(escape_leading_slashes(url), expected)\n", "    def test_edge_cases(self):\n        # Test empty string\n        self.assertEqual(urlsafe_base64_encode(b''), '')\n        self.assertEqual(urlsafe_base64_decode(''), b'')\n\n        # Test string with special characters\n        special_chars = b'\\x00\\x01\\xff'\n        encoded = urlsafe_base64_encode(special_chars)\n        decoded = urlsafe_base64_decode(encoded)\n        self.assertEqual(special_chars, decoded)\n\n        # Test string with length divisible by 3\n        divisible_by_three = b'abcdef'\n        encoded = urlsafe_base64_encode(divisible_by_three)\n        decoded = urlsafe_base64_decode(encoded)\n        self.assertEqual(divisible_by_three, decoded)\n", "    def test_url_has_allowed_host_and_scheme_with_multiple_slashes(self):\n        self.assertIs(\n            url_has_allowed_host_and_scheme('///example.com', allowed_hosts={'example.com'}),\n            False,\n        )\n        self.assertIs(\n            url_has_allowed_host_and_scheme('///example.com', allowed_hosts={'example.com'}, require_https=True),\n            False,\n        )\n", "    def test_ipv6_url(self):\n        # Test if IPv6 URLs are handled correctly\n        allowed_hosts = {'[2001:cdba:0000:0000:0000:0000:3257:9652]'}\n        url = 'http://[2001:cdba:0000:0000:0000:0000:3257:9652]/'\n        self.assertTrue(url_has_allowed_host_and_scheme(url, allowed_hosts))\n\n        # Test if IPv6 URLs with port are handled correctly\n        allowed_hosts = {'[2001:cdba:0000:0000:0000:0000:3257:9652]:8000'}\n        url = 'http://[2001:cdba:0000:0000:0000:0000:3257:9652]:8000/'\n        self.assertTrue(url_has_allowed_host_and_scheme(url, allowed_hosts))\n", "    def test_parsing_invalid_rfc850(self):\n        invalid_rfc850s = (\n            'Tuesday, 31-Feb-69 08:49:37 GMT',  # invalid day\n            'Tuesday, 32-Dec-69 08:49:37 GMT',  # invalid day\n            'Tuesday, 31-Dec-99 08:49:37 GMT',  # year 2099 is not a leap year\n            'Tuesday, 29-Feb-99 08:49:37 GMT',  # year 1999 is not a leap year\n        )\n        for rfc850str in invalid_rfc850s:\n            with self.subTest(rfc850str=rfc850str):\n                with self.assertRaises(ValueError):\n                    parse_http_date(rfc850str)\n"], "sample_48": ["def test_references_field_by_limit_choices_to(self):\n    operation = FieldOperation('Model', 'field', models.ForeignKey('Other', models.CASCADE, limit_choices_to={'field': 'value'}))\n    self.assertIs(operation.references_field('Other', 'field', 'migrations'), True)\n    self.assertIs(operation.references_field('Other', 'whatever', 'migrations'), False)\n    self.assertIs(operation.references_field('Missing', 'whatever', 'migrations'), False)\n", "def test_references_field_by_foreign_key(self):\n    operation = FieldOperation('Model', 'field', models.ForeignKey('Other', models.CASCADE))\n    self.assertIs(operation.references_field('Other', 'id', 'migrations'), True)\n    self.assertIs(operation.references_field('Other', 'whatever', 'migrations'), False)\n    self.assertIs(operation.references_field('Missing', 'whatever', 'migrations'), False)\n", "def test_alter_field_with_expression_index(self):\n    \"\"\"\n    Test AlterField operation with an expression index to ensure indexes created\n    via Meta.indexes don't get dropped with sqlite3 remake.\n    \"\"\"\n    project_state = self.set_up_test_model(\"test_alflin\", index=True, expression_index=True)\n    operation = migrations.AlterField(\"Pony\", \"pink\", models.IntegerField(null=True))\n    new_state = project_state.clone()\n    operation.state_forwards(\"test_alflin\", new_state)\n    # Test the database alteration\n    self.assertColumnNotNull(\"test_alflin_pony\", \"pink\")\n    with connection.schema_editor() as editor:\n        operation.database_forwards(\"test_alflin\", editor, project_state, new_state)\n    # Expression index hasn't been dropped\n    self.assertIndexExists(\"test_alflin_pony\", [models.F(\"pink\") + 1])\n    # And test reversal\n    with connection.schema_editor() as editor:\n        operation.database_backwards(\"test_alflin\", editor, new_state, project_state)\n    # Ensure the expression index is still there\n    self.assertIndexExists(\"test_alflin_pony\", [models.F(\"pink\") + 1])\n", "def test_alter_field_with_func_index_unique_constraint(self):\n    app_label = 'test_alfuncucidx'\n    constraint_name = f'{app_label}_pony_abs_uq'\n    table_name = f'{app_label}_pony'\n    project_state = self.set_up_test_model(\n        app_label,\n        constraints=[models.UniqueConstraint(Abs('weight'), name=constraint_name)],\n        indexes=[models.Index(Abs('pink'), name=constraint_name)],\n    )\n    operation = migrations.AlterField('Pony', 'pink', models.IntegerField(null=True))\n    new_state = project_state.clone()\n    operation.state_forwards(app_label, new_state)\n    with connection.schema_editor() as editor:\n        operation.database_forwards(app_label, editor, project_state, new_state)\n    if connection.features.supports_expression_indexes:\n        self.assertIndexNameExists(table_name, constraint_name)\n    with connection.schema_editor() as editor:\n        operation.database_backwards(app_label, editor, new_state, project_state)\n    if connection.features.supports_expression_indexes:\n        self.assertIndexNameExists(table_name, constraint_name)\n", "def test_references_field_by_limit_choices_to(self):\n    operation = FieldOperation('Model', 'field', models.ForeignKey('Other', models.CASCADE, limit_choices_to={'field': 'value'}))\n    self.assertIs(operation.references_field('Other', 'field', 'migrations'), True)\n    self.assertIs(operation.references_field('Other', 'whatever', 'migrations'), False)\n    self.assertIs(operation.references_field('Missing', 'whatever', 'migrations'), False)\n"], "sample_12": ["def test_mti_inheritance_model_creation(self):\n    Animal = ModelState('app', 'Animal', [\n        (\"id\", models.AutoField(primary_key=True)),\n    ])\n    Dog = ModelState('app', 'Dog', [], bases=('app.Animal',))\n    changes = self.get_changes([Animal], [Animal, Dog])\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['CreateModel'])\n    self.assertOperationAttributes(changes, 'app', 0, 0, name='Dog')\n", "def test_add_non_blank_fk(self, mocked_ask_method):\n    \"\"\"\n    #23405 - Adding a NOT NULL and non-blank `ForeignKey` without default\n    should prompt for a default.\n    \"\"\"\n    Publisher = ModelState('app', 'Publisher', [\n        (\"id\", models.AutoField(primary_key=True)),\n    ])\n    Book = ModelState('app', 'Book', [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"publisher\", models.ForeignKey('app.Publisher', models.CASCADE)),\n    ])\n    changes = self.get_changes([Publisher], [Book])\n    self.assertEqual(mocked_ask_method.call_count, 1)\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, [\"CreateModel\"])\n    self.assertOperationAttributes(changes, 'app', 0, 0, name='Book')\n", "def test_add_non_nullable_date_field(self, mocked_ask_method):\n    \"\"\"#23405 - Adding a NOT NULL and non-nullable `DateField` without default should not prompt for a default.\"\"\"\n    changes = self.get_changes([self.author_empty], [self.author_with_date_of_birth])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AddField\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"date_of_birth\")\n", "def test_alter_field_to_textfield_and_charfield(self):\n    \"\"\"\n    #23405 - Altering a NOT NULL `CharField` or `TextField` to allow blank\n    without a default should not prompt for a default.\n    \"\"\"\n    author_with_biography = ModelState('testapp', 'Author', [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"name\", models.CharField(max_length=100)),\n        (\"biography\", models.TextField(null=False, blank=False)),\n    ])\n    author_with_biography_blank = ModelState('testapp', 'Author', [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"name\", models.CharField(max_length=100)),\n        (\"biography\", models.TextField(null=False, blank=True)),\n    ])\n    changes = self.get_changes([author_with_biography], [author_with_biography_blank])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AlterField\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name='biography')\n", "def test_mti_inheritance_with_fk_removal(self):\n    Animal = ModelState('app', 'Animal', [\n        (\"id\", models.AutoField(primary_key=True)),\n    ])\n    Dog = ModelState('app', 'Dog', [\n        (\"animal_ptr\", models.OneToOneField('app.Animal', models.CASCADE, primary_key=True, parent_link=True)),\n    ], bases=('app.Animal',))\n    Owner = ModelState('app', 'Owner', [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"pet\", models.ForeignKey('app.Dog', models.CASCADE)),\n    ])\n    changes = self.get_changes([Animal, Dog, Owner], [Animal, Owner])\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['DeleteModel'])\n    self.assertOperationAttributes(changes, 'app', 0, 0, name='Dog')\n"], "sample_6": ["    def test_validate(self):\n        valid_usernames = ['joe', 'GLEnN', 'jean-marc', 'joe.smith', 'joe+smith', 'joe_smith', 'joe-smith', 'joe@smith']\n        invalid_usernames = [\"o'connell\", '\u00c9ric', 'jean marc', \"\u0623\u062d\u0645\u062f\", 'trailingnewline\\n', 'joe$smith', 'joe#smith']\n        v = ASCIIUsernameValidator()\n        for valid in valid_usernames:\n            with self.subTest(valid=valid):\n                v(valid)\n        for invalid in invalid_usernames:\n            with self.subTest(invalid=invalid):\n                with self.assertRaises(ValidationError):\n                    v(invalid)\n", "    def test_ascii_username_validator(self):\n        valid_usernames = ['glenn', 'GLEnN', 'jean-marc', 'user123', 'user.user', 'user+user', 'user_user', 'user@user']\n        invalid_usernames = [\"o'connell\", '\u00c9ric', 'jean marc', \"\u0623\u062d\u0645\u062f\", 'trailingnewline\\n', 'user#user', 'user$user']\n        v = ASCIIUsernameValidator()\n        for valid in valid_usernames:\n            with self.subTest(valid=valid):\n                v(valid)\n        for invalid in invalid_usernames:\n            with self.subTest(invalid=invalid):\n                with self.assertRaises(ValidationError):\n                    v(invalid)\n", "    def test_validate(self):\n        valid_usernames = ['joe', 'joe123', 'joe.smith', 'joe-smith', 'joe_smith', 'joe+smith', 'joe@smith', 'joe+smith-123']\n        invalid_usernames = ['joe smith', 'joe#smith', 'joe&smith', 'joe=smith', 'joe,smith', 'joe;smith', 'joe:smith']\n        v = ASCIIUsernameValidator()\n        for valid in valid_usernames:\n            with self.subTest(valid=valid):\n                v(valid)\n        for invalid in invalid_usernames:\n            with self.subTest(invalid=invalid):\n                with self.assertRaises(ValidationError):\n                    v(invalid)\n", "    def test_valid_username(self):\n        valid_usernames = ['joe', 'joe123', 'joe.joe', 'joe@joe', 'joe+joe', 'joe-joe', 'joe_joe']\n        v = ASCIIUsernameValidator()\n        for valid in valid_usernames:\n            with self.subTest(valid=valid):\n                v(valid)\n", "    def test_ascii_validator(self):\n        valid_usernames = ['joe', 'GLEnN', 'jean-marc', 'glenn.smith', 'john+doe']\n        invalid_usernames = [\"o'connell\", '\u00c9ric', 'jean marc', \"\u0623\u062d\u0645\u062f\", 'trailingnewline\\n']\n        v = validators.ASCIIUsernameValidator()\n        for valid in valid_usernames:\n            with self.subTest(valid=valid):\n                v(valid)\n        for invalid in invalid_usernames:\n            with self.subTest(invalid=invalid):\n                with self.assertRaises(ValidationError):\n                    v(invalid)\n"], "sample_153": ["def test_issue_23058_with_custom_names():\n    from sympy import symbols, sin, cos, pi, UnevaluatedExpr\n\n    delop = Del()\n    CC_   = CoordSys3D(\"C\", vector_names=['x', 'y', 'z'], variable_names=['i', 'j', 'k'])\n    y     = CC_.y\n    xhat  = CC_.i\n\n    t = symbols(\"t\")\n    ten = symbols(\"10\", positive=True)\n    eps, mu = 4*pi*ten**(-11), ten**(-5)\n\n    Bx = 2 * ten**(-4) * cos(ten**5 * t) * sin(ten**(-3) * y)\n    vecB = Bx * xhat\n    vecE = (1/eps) * Integral(delop.cross(vecB/mu).doit(), t)\n    vecE = vecE.doit()\n\n    vecB_str = \"\"\"\\", "def test_custom_names_with_different_bases():\n    A = CoordSys3D('A', vector_names=['x', 'y', 'z'],\n                   variable_names=['i', 'j', 'k'])\n    B = CoordSys3D('B', vector_names=['p', 'q', 'r'],\n                   variable_names=['i', 'j', 'k'], bases=[A.i, A.j, A.k])\n    assert B.i.__str__() == 'A.i'\n    assert B.p.__str__() == 'B.p'\n    assert B.i._pretty_form == 'i_B'\n    assert B.p._pretty_form == 'p_B'\n    assert B.i._latex_form == r'\\mathbf{{i}_{B}}'\n    assert B.p._latex_form == r\"\\mathbf{\\hat{p}_{B}}\"\n", "def test_custom_symbols():\n    from sympy import symbols\n    x, y, z = symbols('x y z')\n    A = CoordSys3D('A', symbols=['x', 'y', 'z'])\n    assert A.i == x\n    assert A.j == y\n    assert A.k == z\n    assert A.i._pretty_form == 'x_A'\n    assert A.j._pretty_form == 'y_A'\n    assert A.k._pretty_form == 'z_A'\n    assert A.i._latex_form == r'\\mathbf{{x}_{A}}'\n    assert A.j._latex_form == r'\\mathbf{{y}_{A}}'\n    assert A.k._latex_form == r'\\mathbf{{z}_{A}}'\n", "def test_custom_coord_systems():\n    N = CoordSys3D('N')\n    C = N.orient_new_axis('C', a, N.k, vector_names=['x', 'y', 'z'])\n    assert C.x.__str__() == 'C.x'\n    assert C.y._pretty_form == 'y_C'\n    assert C.z._latex_form == r\"\\mathbf{\\hat{z}_{C}}\"\n", "def test_issue_19715():\n    from sympy import symbols, diff, Function, sin, cos, pi\n\n    x, y, z, t, r, theta, phi, rho, eta = symbols('x y z t r theta phi rho eta')\n    f = Function('f')\n    g = Function('g')\n    h = Function('h')\n\n    expr = diff(sin(x*cos(y)) + f(x)*g(y) + h(x), x)\n    expected = \"cos(x) sin(y) g(y) + cos(x) cos(y) f(x) + Derivative(f(x), x) + Derivative(h(x), x)\"\n    assert str(expr) == expected\n"], "sample_140": ["def test_point_set_vel():\n    q = dynamicsymbols('q')\n    N = ReferenceFrame('N')\n    P = Point('P')\n    P.set_vel(N, q * N.x)\n    assert P.vel(N) == q * N.x\n    raises(TypeError, lambda: P.set_vel(q, N.x)) # frame argument must be a ReferenceFrame\n    raises(TypeError, lambda: P.set_vel(N, 'invalid')) # value argument must be a Vector\n", "def test_point_vel_exceptions():\n    q = dynamicsymbols('q')\n    N = ReferenceFrame('N')\n    O = Point('O')\n    P = O.locatenew('P', q * N.x)\n    raises(TypeError, lambda: P.set_vel(N, 'invalid_vector'))\n    raises(TypeError, lambda: P.set_pos(O, 'invalid_vector'))\n    raises(TypeError, lambda: P.set_acc(N, 'invalid_vector'))\n    raises(TypeError, lambda: P.locatenew('Q', 'invalid_vector'))\n", "def test_point_vel_inconsistent_position():\n    q1, q2 = dynamicsymbols('q1 q2')\n    N = ReferenceFrame('N')\n    O = Point('O')\n    P = Point('P')\n    P.set_pos(O, q1 * N.x)\n    O.set_pos(P, q2 * N.x)\n    raises(ValueError, lambda: P.vel(N))  # Inconsistent position definition between O and P\n", "def test_point_pos_frame_changes():\n    q = dynamicsymbols('q')\n    N = ReferenceFrame('N')\n    B = N.orientnew('B', 'Axis', [q, N.z])\n    O = Point('O')\n    P = O.locatenew('P', 10 * N.x + 5 * B.x)\n    assert P.pos_from(O) == 10 * N.x + 5 * B.x\n    N.set_ang_vel(B, q * B.x)\n    assert P.pos_from(O) == 10 * N.x + 5 * (B.x - q * B.y)\n", "def test_point_vel_from_connected_frames_with_different_intermediate_points():\n    t = dynamicsymbols._t\n    q1, q2, q3, u1, u2 = dynamicsymbols('q1 q2 q3 u1 u2')\n    N = ReferenceFrame('N')\n    B = ReferenceFrame('B')\n    A = ReferenceFrame('A')\n\n    O = Point('O')\n    O.set_vel(N, u1 * N.x)\n\n    P = Point('P')\n    P.set_pos(O, q1 * N.x + q2 * B.y)\n\n    Q = Point('Q')\n    Q.set_pos(O, q3 * N.x + q2 * A.y)\n\n    N.orient(B, 'Axis', (q1, B.x))\n    N.orient(A, 'Axis', (q3, A.x))\n\n    assert P.vel(N) == (u1 + q1.diff(t)) * N.x + q2.diff(t) * B.y - q2 * q1.diff(t) * B.z\n    assert Q.vel(N) == (u1 + q3.diff(t)) * N.x + q2.diff(t) * A.y - q2 * q3.diff(t) * A.z\n"], "sample_19": ["    def test_template_exceptions(self):\n        with self.assertRaises(Exception):\n            self.client.get(reverse('template_exception'))\n", "    def test_non_sensitive_request(self):\n        \"\"\"\n        Everything can bee seen in the email error reports for non-sensitive requests.\n        \"\"\"\n        with self.settings(DEBUG=True, ADMINS=[('Admin', 'admin@example.com')]):\n            self.verify_unsafe_email(non_sensitive_view)\n\n        with self.settings(DEBUG=False, ADMINS=[('Admin', 'admin@example.com')]):\n            self.verify_unsafe_email(non_sensitive_view)\n", "    def test_unicode_error_with_ascii_string(self):\n        try:\n            'This is a non-ASCII string: \u1f40'.encode('ascii')\n        except Exception:\n            exc_type, exc_value, tb = sys.exc_info()\n        reporter = ExceptionReporter(None, exc_type, exc_value, tb)\n        html = reporter.get_traceback_html()\n        self.assertIn('<h2>Unicode error hint</h2>', html)\n        self.assertIn('The string that could not be encoded/decoded was: ', html)\n        self.assertIn('<strong>This is a non-ASCII string: &#941;</strong>', html)\n", "def test_unicode_error(self):\n    \"\"\"\n    ExceptionReporter should be able to handle UnicodeErrors.\n    \"\"\"\n    try:\n        \"this is a unicode error: \\x80\".encode('ascii')\n    except UnicodeError:\n        exc_type, exc_value, tb = sys.exc_info()\n\n    reporter = ExceptionReporter(None, exc_type, exc_value, tb)\n    html = reporter.get_traceback_html()\n    self.assertIn('Unicode error hint', html)\n\n    text = reporter.get_traceback_text()\n    self.assertIn('Unicode error hint', text)\n", "    def test_unicode_decode_error(self):\n        try:\n            \"abcdefg\u1f40\".encode(\"ascii\")\n        except Exception:\n            exc_type, exc_value, tb = sys.exc_info()\n        reporter = ExceptionReporter(None, exc_type, exc_value, tb)\n        text = reporter.get_traceback_text()\n        self.assertIn(\"UnicodeDecodeError\", text)\n        self.assertIn(\"'ascii' codec can't decode byte\", text)\n        self.assertIn(\"ordinal not in range(128)\", text)\n        self.assertIn(\"abcdefg\u1f40\", text)\n"], "sample_119": ["def test_TrigonometricFunctions():\n    assert mcode(sin(x)) == \"Sin[x]\"\n    assert mcode(cos(x)) == \"Cos[x]\"\n    assert mcode(tan(x)) == \"Tan[x]\"\n    assert mcode(cot(x)) == \"Cot[x]\"\n    assert mcode(asin(x)) == \"ArcSin[x]\"\n    assert mcode(acos(x)) == \"ArcCos[x]\"\n    assert mcode(atan(x)) == \"ArcTan[x]\"\n    assert mcode(sinh(x)) == \"Sinh[x]\"\n    assert mcode(cosh(x)) == \"Cosh[x]\"\n    assert mcode(tanh(x)) == \"Tanh[x]\"\n    assert mcode(coth(x)) == \"Coth[x]\"\n    assert mcode(sech(x)) == \"Sech[x]\"\n    assert mcode(csch(x)) == \"Csch[x]\"\n    assert mcode(asinh(x)) == \"ArcSinh[x]\"\n    assert mcode(acosh(x)) == \"ArcCosh[x]\"\n    assert mcode(atanh(x)) == \"ArcTanh[x]\"\n    assert mcode(acoth(x)) == \"ArcCoth[x]\"\n    assert mcode(asech(x)) == \"ArcSech[x]\"\n    assert mcode(acsch(x)) == \"ArcCsch[x]\"\n", "def test_Add():\n    assert mcode(x + y + z) == \"x + y + z\"\n    assert mcode(x + 2) == \"x + 2\"\n    assert mcode(2 + x) == \"2 + x\"\n", "def test_TrigonometricFunctions():\n    assert mcode(sin(x)) == \"Sin[x]\"\n    assert mcode(cos(x)) == \"Cos[x]\"\n    assert mcode(sin(x) + cos(x)) == \"Cos[x] + Sin[x]\"\n    assert mcode(sin(x**2)) == \"Sin[x^2]\"\n    assert mcode(cos(x**2)) == \"Cos[x^2]\"\n    assert mcode(sin(x) * cos(x)) == \"Cos[x]*Sin[x]\"\n", "def test_Sqrt():\n    assert mcode(sqrt(4)) == \"2\"\n    assert mcode(sqrt(x**2)) == \"Abs[x]\"\n    assert mcode(sqrt(x)) == \"Sqrt[x]\"\n", "def test_Logarithm():\n    assert mcode(log(x)) == \"Log[x]\"\n    assert mcode(log(x, 10)) == \"Log[x, 10]\"\n    assert mcode(log(x, y)) == \"Log[x, y]\"\n    assert mcode(log(x**2)) == \"Log[x^2]\"\n    assert mcode(log(x**2, y)) == \"Log[x^2, y]\"\n"], "sample_133": ["def test_fcode_complex_matrix():\n    import sympy.utilities.codegen\n    sympy.utilities.codegen.COMPLEX_ALLOWED = True\n    x = MatrixSymbol('x', 2, 2)\n    y = MatrixSymbol('y', 2, 2)\n    result = codegen(('test', x + y), 'f95', 'test', header=False, empty=False)\n    source = (result[0][1])\n    expected = (\n        \"subroutine test(x, y, out_%(hash)s)\\n\"\n        \"implicit none\\n\"\n        \"COMPLEX*16, intent(in), dimension(1:2, 1:2) :: x\\n\"\n        \"COMPLEX*16, intent(in), dimension(1:2, 1:2) :: y\\n\"\n        \"COMPLEX*16, intent(out), dimension(1:2, 1:2) :: out_%(hash)s\\n\"\n        \"out_%(hash)s = x + y\\n\"\n        \"end subroutine\\n\"\n    )\n    # look for the magic number\n    a = source.splitlines()[4]\n    b = a.split('_')\n    out = b[1]\n    expected = expected % {'hash': out}\n    assert source == expected\n    sympy.utilities.codegen.COMPLEX_ALLOWED = False\n", "def test_custom_codegen_f95():\n    from sympy.printing.fcode import FCodePrinter\n    from sympy.functions.elementary.exponential import exp\n\n    printer = FCodePrinter(settings={'user_functions': {'exp': 'fastexp'}})\n\n    x, y = symbols('x y')\n    expr = exp(x + y)\n\n    # replace math.h with a different header\n    gen = FCodeGen(printer=printer)\n\n    expected = (\n        'REAL*8 function expr(x, y)\\n'\n        'implicit none\\n'\n        'REAL*8, intent(in) :: x\\n'\n        'REAL*8, intent(in) :: y\\n'\n        'expr = fastexp(x + y)\\n'\n        'end function\\n'\n    )\n\n    result = codegen(('expr', expr), header=False, empty=False, code_gen=gen)\n    source = result[0][1]\n    assert source == expected\n", "def test_fcode_complex_matrix_output():\n    x = Symbol('x', real=True)\n    y = Symbol('y', real=True)\n    z = Symbol('z', real=True)\n    e1 = x + y\n    e2 = Matrix([[x, y], [z, 16]])\n    name_expr = (\"test\", (e1, e2))\n    result = codegen(name_expr, \"f95\", \"test\", header=False, empty=False)\n    source = result[0][1]\n    expected = (\n        \"REAL*8 function test(x, y, z, out_%(hash)s)\\n\"\n        \"implicit none\\n\"\n        \"REAL*8, intent(in) :: x\\n\"\n        \"REAL*8, intent(in) :: y\\n\"\n        \"REAL*8, intent(in) :: z\\n\"\n        \"REAL*8, intent(out), dimension(1:2, 1:2) :: out_%(hash)s\\n\"\n        \"out_%(hash)s(1, 1) = x\\n\"\n        \"out_%(hash)s(2, 1) = z\\n\"\n        \"out_%(hash)s(1, 2) = y\\n\"\n        \"out_%(hash)s(2, 2) = 16\\n\"\n        \"test = x + y\\n\"\n        \"end function\\n\"\n    )\n    # look for the magic number\n    a = source.splitlines()[5]\n    b = a.split('_')\n    out = b[1]\n    expected = expected % {'hash': out}\n    assert source == expected\n\n    x = Symbol('x')\n    y = Symbol('y', real=True)\n    z = Symbol('z', real=True)\n    e1 = x + y\n    e2 = Matrix([[x, y], [z, 16]])\n    name_expr = (\"test\", (e1, e2))\n    result = codegen(name_expr, \"f95\", \"test", "def test_fcode_complex_matrix():\n    import sympy.utilities.codegen\n    sympy.utilities.codegen.COMPLEX_ALLOWED = True\n    x = Symbol('x')\n    y = Symbol('y')\n    mat = Matrix([[x, y], [x+y, x-y]])\n    result = codegen(('test', mat), 'f95', 'test', header=False, empty=False)\n    source = (result[0][1])\n    expected = (\n        \"COMPLEX*16 function test(x, y, out_%(hash)s)\\n\"\n        \"implicit none\\n\"\n        \"COMPLEX*16, intent(in) :: x\\n\"\n        \"COMPLEX*16, intent(in) :: y\\n\"\n        \"COMPLEX*16, intent(out), dimension(1:2, 1:2) :: out_%(hash)s\\n\"\n        \"out_%(hash)s(1, 1) = x\\n\"\n        \"out_%(hash)s(2, 1) = x + y\\n\"\n        \"out_%(hash)s(1, 2) = y\\n\"\n        \"out_%(hash)s(2, 2) = x - y\\n\"\n        \"end function\\n\"\n    )\n    # look for the magic number\n    a = source.splitlines()[4]\n    b = a.split('_')\n    out = b[1]\n    expected = expected % {'hash': out}\n    assert source == expected\n    sympy.utilities.codegen.COMPLEX_ALLOWED = False\n", "def test_multiple_expressions_same_result():\n    x, y, z = symbols('x,y,z')\n    expr1 = (x + y)*z\n    expr2 = (x - y)*z\n    result_var = Symbol('result')\n    routine = make_routine(\n        \"test\",\n        [Equality(result_var, expr1), Equality(result_var, expr2)]\n    )\n    code_gen = FCodeGen()\n    with raises(CodeGenError):\n        get_string(code_gen.dump_f95, [routine])\n"], "sample_148": ["def test_conjugate_properties():\n    x = Symbol('x')\n    assert conjugate(x).is_complex is True\n    assert conjugate(x).is_real is None\n    assert conjugate(x).is_imaginary is None\n\n    r = Symbol('r', real=True)\n    assert conjugate(r).is_complex is False\n    assert conjugate(r).is_real is True\n    assert conjugate(r).is_imaginary is False\n\n    i = Symbol('i', imaginary=True)\n    assert conjugate(i).is_complex is False\n    assert conjugate(i).is_real is False\n    assert conjugate(i).is_imaginary is True\n", "def test_issue_17188():\n    from sympy.functions.elementary.complexes import Abs\n    from sympy import sqrt\n    x = Symbol('x', real=True)\n    assert Abs(sqrt(x)) == sqrt(x)\n", "def test_issue_15893_complex():\n    f = Function('f', complex=True)\n    x = Symbol('x', complex=True)\n    eq = Derivative(Abs(f(x)), f(x))\n    assert eq.doit() == sign(f(x)) * conjugate(f(x)) / Abs(f(x))\n", "def test_issue_15893_symbols_not_real():\n    f = Function('f', real=True)\n    x = Symbol('x')  # x is not necessarily real\n    eq = Derivative(Abs(f(x)), f(x))\n    assert eq.doit() == Derivative(Abs(f(x)), f(x))  # Should not simplify if x is not real\n", "def test_issue_16041():\n    # Test for simplification of Abs(x**2 - 1)\n    x = Symbol('x', real=True)\n    assert Abs(x**2 - 1) == Abs(x - 1)*Abs(x + 1)\n\n    y = Symbol('y', complex=True)\n    assert Abs(y**2 - 1) == sqrt((y - 1)*(y + 1)*(y - conjugate(y))*(y + conjugate(y)))\n"], "sample_23": ["def test_count_difference_empty_result(self):\n    qs1 = Number.objects.filter(pk__in=[])\n    qs2 = Number.objects.filter(pk__in=[])\n    self.assertEqual(qs1.difference(qs2).count(), 0)\n", "def test_intersection_with_values_and_order_by(self):\n    ReservedName.objects.bulk_create([\n        ReservedName(name='rn1', order=7),\n        ReservedName(name='rn2', order=5),\n        ReservedName(name='rn0', order=6),\n        ReservedName(name='rn9', order=-1),\n    ])\n    qs1 = ReservedName.objects.filter(order__gte=6)\n    qs2 = ReservedName.objects.filter(order__lte=5)\n    intersection_qs = qs1.intersection(qs2)\n    for qs, expected_result in (\n        # Order by a single column.\n        (intersection_qs.order_by('-pk').values_list('order', flat=True), [6, 5]),\n        (intersection_qs.order_by('pk').values_list('order', flat=True), [6, 5]),\n        (intersection_qs.values_list('order', flat=True).order_by('-pk'), [5, 6]),\n        (intersection_qs.values_list('order', flat=True).order_by('pk'), [5, 6]),\n        # Order by multiple columns.\n        (intersection_qs.order_by('-name', 'pk').values_list('order', flat=True), [5, 6]),\n        (intersection_qs.values_list('order', flat=True).order_by('-name', 'pk'), [5, 6]),\n    ):\n        with self.subTest(qs=qs):\n            self.assertEqual(list(qs), expected_result)\n", "def test_count_union_with_values_list_and_distinct(self):\n    ReservedName.objects.bulk_create([\n        ReservedName(name='rn1', order=1),\n        ReservedName(name='rn2', order=1),\n        ReservedName(name='rn3', order=2),\n    ])\n    qs1 = ReservedName.objects.filter(order=1).values_list('name', flat=True)\n    qs2 = ReservedName.objects.filter(order=2).values_list('name', flat=True)\n    self.assertEqual(qs1.union(qs2).distinct().count(), 2)\n", "def test_difference_with_different_models(self):\n    ReservedName.objects.create(name='a', order=2)\n    qs1 = Number.objects.all()\n    qs2 = ReservedName.objects.none()\n    self.assertEqual(len(qs1.difference(qs2)), 10)\n", "def test_union_with_empty_qs_all(self):\n    qs1 = Number.objects.all()\n    qs2 = Number.objects.none()\n    qs3 = Number.objects.filter(pk__in=[])\n    self.assertEqual(len(qs1.union(qs2, all=True)), 10)\n    self.assertEqual(len(qs2.union(qs1, all=True)), 10)\n    self.assertEqual(len(qs1.union(qs3, all=True)), 10)\n    self.assertEqual(len(qs3.union(qs1, all=True)), 10)\n    self.assertEqual(len(qs2.union(qs1, qs1, all=True)), 20)\n    self.assertEqual(len(qs2.union(qs1, qs1)), 10)\n    self.assertEqual(len(qs2.union(qs2, all=True)), 0)\n    self.assertEqual(len(qs3.union(qs3, all=True)), 0)\n"], "sample_146": ["def test_issue_21537():\n    assert str(Mul(-1, 1, evaluate=False)*x) == '-1*1*x'\n    assert str(Mul(x, -1, evaluate=False)*1) == 'x*(-1)*1'\n", "def test_NDimArray_with_symbols():\n    x = Symbol('x')\n    assert sstr(NDimArray([x, 2.0])) == '[x, 2.00000000000000]'\n", "def test_AlgebraicNumber():\n    from sympy.abc import x\n    from sympy.solvers import solve\n    eq = x**2 - 2\n    sol = solve(eq)\n    assert str(sol[0]) == \"sqrt(2)\"\n    assert str(sol[1]) == \"-sqrt(2)\"\n    assert str(sol[0].as_poly().as_expr()) == \"sqrt(2)\"\n    assert str(sol[1].as_poly().as_expr()) == \"-sqrt(2)\"\n", "def test_Str_array_expressions():\n    from sympy.tensor.array.expressions.array_expressions import ArraySymbol, ArrayElement\n    A = ArraySymbol(\"A\", (2, 3, 4))\n    assert sstr(A) == \"A\"\n    A_elem = ArrayElement(\"A\", (2, 1/(1-x), 0))\n    assert sstr(A_elem) == \"A[2, 1/(1 - x), 0]\"\n", "def test_AlgebraicNumber():\n    from sympy import sqrt\n    assert str(sqrt(2)) == \"sqrt(2)\"\n    assert str(sqrt(3).as_poly().as_expr()) == \"sqrt(3)\"\n    assert str(sqrt(3).as_expr()) == \"0.5*sqrt(3)*(-1 + I*sqrt(3)) + 0.5*sqrt(3)*(-1 - I*sqrt(3))\"\n"], "sample_17": ["    def test_set_test_mirror(self):\n        # set_as_test_mirror() sets the database up to be used in testing as a mirror of a primary database.\n        primary_settings_dict = {\n            'NAME': 'primary_database',\n            'ENGINE': 'django.db.backends.postgresql',\n            'USER': 'myuser',\n            'PASSWORD': 'mypassword',\n            'HOST': 'localhost',\n            'PORT': '5432',\n        }\n        test_connection = get_connection_copy()\n        creation = test_connection.creation_class(test_connection)\n        creation.set_as_test_mirror(primary_settings_dict)\n        self.assertEqual(test_connection.settings_dict['NAME'], 'primary_database')\n", "    def test_serialize_db(self):\n        # serialize_db_to_string() serializes data correctly.\n        Object.objects.create(id=1)\n        ObjectReference.objects.create(id=1, obj_id=1)\n        data = connection.creation.serialize_db_to_string()\n        self.assertIn('\"model\": \"backends.object\"', data)\n        self.assertIn('\"model\": \"backends.objectreference\"', data)\n        self.assertIn('\"obj\": 1', data)\n", "    def test_set_as_test_mirror(self):\n        test_connection = get_connection_copy()\n        primary_settings_dict = {'NAME': 'primary_db'}\n        test_connection.settings_dict['NAME'] = 'test_db'\n        creation = test_connection.creation_class(test_connection)\n        creation.set_as_test_mirror(primary_settings_dict)\n        self.assertEqual(test_connection.settings_dict['NAME'], 'primary_db')\n", "    def test_keepdb_true(self):\n        # create_test_db() skips creating a new test database if keepdb is True.\n        test_connection = get_connection_copy()\n        test_connection.settings_dict['TEST']['KEEPDB'] = True\n        creation = test_connection.creation_class(test_connection)\n        old_database_name = test_connection.settings_dict['NAME']\n        try:\n            with mock.patch.object(creation, '_create_test_db') as mock_create:\n                creation.create_test_db(verbosity=0, autoclobber=True, serialize=False, keepdb=True)\n            mock_create.assert_not_called()\n        finally:\n            with mock.patch.object(creation, '_destroy_test_db'):\n                creation.destroy_test_db(old_database_name, verbosity=0, keepdb=True)\n", "    def test_serialize_db_to_string(self, mocked_migrate, mocked_ensure_connection):\n        # serialize_db_to_string() correctly serializes the data in the database\n        Object.objects.create(id=1, obj_ref=None)\n        ObjectReference.objects.create(id=1, obj_id=1)\n        data = connection.creation.serialize_db_to_string()\n        expected_data = '[{\"model\": \"backends.object\", \"pk\": 1, \"fields\": {\"obj_ref\": 1, \"related_objects\": []}}, {\"model\": \"backends.objectreference\", \"pk\": 1, \"fields\": {\"obj\": 1}}]'\n        self.assertJSONEqual(data, expected_data)\n"], "sample_99": ["def test_kneighbors_classifier_weights_callable():\n    # Test k-neighbors classification with callable weights\n    rng = np.random.RandomState(0)\n    X = 2 * rng.rand(40, 5) - 1\n    y = ((X ** 2).sum(axis=1) < .5).astype(np.int)\n\n    weight_func = _weight_func\n\n    for algorithm in ALGORITHMS:\n        knn = neighbors.KNeighborsClassifier(n_neighbors=5,\n                                             weights=weight_func,\n                                             algorithm=algorithm)\n        knn.fit(X, y)\n        epsilon = 1e-5 * (2 * rng.rand(1, 5) - 1)\n        y_pred = knn.predict(X[:10] + epsilon)\n        assert_array_equal(y_pred, y[:10])\n", "def test_sparse_metric_callable_different_input_types():\n        assert_true(issparse(x) and issparse(y))\n        return x.dot(y.T).A.item()\n\n    X = np.array([  # Population matrix\n        [1, 1, 1, 1, 1],\n        [1, 0, 1, 0, 1],\n        [0, 0, 1, 0, 0]\n    ])\n\n    Y = csr_matrix([  # Query matrix\n        [1, 1, 0, 1, 1],\n        [1, 0, 0, 0, 1]\n    ])\n\n    nn = neighbors.NearestNeighbors(algorithm='brute', n_neighbors=2,\n                                    metric=sparse_metric).fit(X)\n\n    # Test that the metric function still works when the input types are different\n    N = nn.kneighbors(Y, return_distance=False)\n\n    # GS indices of nearest neighbours in `X` for `sparse_metric`\n    gold_standard_nn = np.array([\n        [2, 1],\n        [2, 1]\n    ])\n\n    assert_array_equal(N, gold_standard_nn)\n", "def test_sparse_metric_callable_with_ball_tree():\n        assert_true(issparse(x) and issparse(y))\n        return x.dot(y.T).A.item()\n\n    X = csr_matrix([  # Population matrix\n        [1, 1, 1, 1, 1],\n        [1, 0, 1, 0, 1],\n        [0, 0, 1, 0, 0]\n    ])\n\n    Y = csr_matrix([  # Query matrix\n        [1, 1, 0, 1, 1],\n        [1, 0, 0, 0, 1]\n    ])\n\n    nn = neighbors.NearestNeighbors(algorithm='ball_tree', n_neighbors=2,\n                                    metric=sparse_metric).fit(X)\n    N = nn.kneighbors(Y, return_distance=False)\n\n    # GS indices of nearest neighbours in `X` for `sparse_metric`\n    gold_standard_nn = np.array([\n        [2, 1],\n        [2, 1]\n    ])\n\n    assert_array_equal(N, gold_standard_nn)\n", "def test_sparse_metric_callable_invalid_input():\n        assert_true(issparse(x) and issparse(y))\n        return x.dot(y.T).A.item()\n\n    X = np.array([  # Population matrix\n        [1, 1, 1, 1, 1],\n        [1, 0, 1, 0, 1],\n        [0, 0, 1, 0, 0]\n    ])\n\n    Y = np.array([  # Query matrix\n        [1, 1, 0, 1, 1],\n        [1, 0, 0, 0, 1]\n    ])\n\n    nn = neighbors.NearestNeighbors(algorithm='brute', n_neighbors=2,\n                                    metric=sparse_metric).fit(X)\n\n    # Testing with non-sparse input should raise ValueError\n    assert_raises(ValueError, nn.kneighbors, Y, return_distance=False)\n", "def test_sparse_metric_with_auto_algorithm():\n        assert_true(issparse(x) and issparse(y))\n        return x.dot(y.T).A.item()\n\n    X = csr_matrix([  # Population matrix\n        [1, 1, 1, 1, 1],\n        [1, 0, 1, 0, 1],\n        [0, 0, 1, 0, 0]\n    ])\n\n    Y = csr_matrix([  # Query matrix\n        [1, 1, 0, 1, 1],\n        [1, 0, 0, 0, 1]\n    ])\n\n    nn = neighbors.NearestNeighbors(algorithm='auto', n_neighbors=2,\n                                    metric=sparse_metric).fit(X)\n    N = nn.kneighbors(Y, return_distance=False)\n\n    # GS indices of nearest neighbours in `X` for `sparse_metric`\n    gold_standard_nn = np.array([\n        [2, 1],\n        [2, 1]\n    ])\n\n    assert_array_equal(N, gold_standard_nn)\n"], "sample_34": ["    def test_deferrable_unique_constraint(self):\n        class Model(models.Model):\n            name = models.CharField(max_length=20, unique=True, deferrable=models.Deferrable.DEFERRED)\n\n        self.assertEqual(checks.run_checks(app_configs=self.apps.get_app_configs()), [])\n", "    def test_custom_db_table(self):\n        class Model(models.Model):\n            class Meta:\n                db_table = 'custom_table'\n\n        self.assertEqual(Model._meta.db_table, 'custom_table')\n        self.assertEqual(checks.run_checks(app_configs=self.apps.get_app_configs()), [])\n", "    def test_expression_index(self):\n        class Model(models.Model):\n            value = models.IntegerField()\n\n            class Meta:\n                indexes = [models.Index(expressions=[models.F('value') + 1])]\n\n        self.assertEqual(checks.run_checks(app_configs=self.apps.get_app_configs()), [])\n", "    def test_model_name_contains_lookup_separator(self):\n        class ModelWithLookupSeparator(models.Model):\n            class Meta:\n                model_name = 'Model_With_Lookup__Separator'\n\n        self.assertEqual(checks.run_checks(app_configs=self.apps.get_app_configs()), [\n            Error(\n                \"The model name 'ModelWithLookupSeparator' cannot contain double underscores as \"\n                \"it collides with the query lookup syntax.\",\n                obj=ModelWithLookupSeparator,\n                id='models.E024',\n            )\n        ])\n", "    def test_unique_together_valid(self):\n        class Model(models.Model):\n            field1 = models.CharField(max_length=20)\n            field2 = models.IntegerField()\n\n            class Meta:\n                unique_together = [('field1', 'field2')]\n\n        self.assertEqual(checks.run_checks(app_configs=self.apps.get_app_configs()), [])\n"], "sample_123": ["def test_Float_srepr():\n    assert srepr(Float('1.0', precision=15)) == \"Float('1.0', precision=15)\"\n    assert srepr(Float('1.0', dps=15)) == \"Float('1.0', dps=15)\"\n", "def test_Float_comparison():\n    f1 = Float('1.0', precision=53)\n    f2 = Float('1.0', precision=10)\n    f3 = Float('1.0000000001', precision=15)\n\n    assert f1 == f2\n    assert f1 != f3\n    assert f1 < f3\n    assert f1 <= f2\n    assert f3 > f1\n    assert f3 >= f2\n\n    assert f1 == 1.0\n    assert f1 != 1.0000000001\n    assert f1 < 1.0000000001\n    assert f1 <= 1.0\n    assert f3 > 1.0\n    assert f3 >= 1.0\n", "def test_Float_logic_operations():\n    assert not (Float('0.0') == False)\n    assert (Float('1.0') == True)\n    assert (Float('0.0') != True)\n    assert (Float('1.0') != False)\n    assert not (Float('0.0') > True)\n    assert not (Float('0.0') < False)\n    assert (Float('1.0') > False)\n    assert (Float('1.0') < True)\n    assert not (Float('0.0') >= True)\n    assert (Float('0.0') <= False)\n    assert (Float('1.0') >= True)\n    assert (Float('1.0') <= True)\n", "def test_Float_to_Integer():\n    f = Float(12.0, 10)\n    i = Integer(f)\n    assert i == 12\n    assert isinstance(i, Integer)\n", "def test_Float_complex_comparisons():\n    a = Float(2 + 3j)\n    b = Float(2 - 3j)\n\n    assert (a == b) is False\n    assert (a != b) is True\n    assert (a < b) is False\n    assert (a <= b) is False\n    assert (a > b) is False\n    assert (a >= b) is False\n\n    assert (b == a) is False\n    assert (b != a) is True\n    assert (b < a) is False\n    assert (b <= a) is False\n    assert (b > a) is False\n    assert (b >= a) is False\n"], "sample_149": ["def test_monomial_ldiv():\n    assert monomial_ldiv((3, 4, 1), (1, 2, 0)) == (2, 2, 1)\n    assert monomial_ldiv((3, 4, 1), (1, 2, 2)) == (2, 2, -1)\n", "def test_monomial_ldiv():\n    assert monomial_ldiv((3, 4, 1), (1, 2, 0)) == (2, 2, 1)\n    assert monomial_ldiv((3, 4, 1), (1, 2, 2)) == (2, 2, -1)\n", "def test_monomial_ldiv():\n    assert monomial_ldiv((3, 4, 1), (1, 2, 0)) == (2, 2, 1)\n    assert monomial_ldiv((3, 4, 1), (1, 2, 2)) == (2, 2, -1)\n", "def test_monomial_ldiv():\n    assert monomial_ldiv((3, 4, 1), (1, 2, 0)) == (2, 2, 1)\n    assert monomial_ldiv((3, 4, 1), (1, 2, 2)) == (2, 2, -1)\n", "def test_monomial_ldiv():\n    assert monomial_ldiv((3, 4, 1), (1, 2, 0)) == (2, 2, 1)\n    assert monomial_ldiv((3, 4, 1), (1, 2, 2)) == (2, 2, -1)\n"], "sample_46": ["    def setUp(self):\n        self.reference = IndexColumns(\n            'table', ['first_column', 'second_column'],\n            lambda column: column.upper(),\n            col_suffixes=['ASC', 'DESC'],\n            opclasses=['varchar_pattern_ops', 'text_pattern_ops'],\n        )\n", "    def setUp(self):\n        compiler = Person.objects.all().query.get_compiler(connection.alias)\n        self.editor = connection.schema_editor()\n        self.expressions = Expressions(\n            table=Person._meta.db_table,\n            expressions=ExpressionList(\n                IndexExpression(F('first_name'), opclasses=['varchar_pattern_ops']),\n                IndexExpression(F('last_name').desc(), opclasses=['text_pattern_desc']),\n                IndexExpression(Upper('last_name'), opclasses=['text_pattern_asc']),\n            ).resolve_expression(compiler.query),\n            compiler=compiler,\n            quote_value=self.editor.quote_value,\n        )\n", "    def test_expressions_with_param(self):\n        compiler = Person.objects.all().query.get_compiler(connection.alias)\n        self.expressions = Expressions(\n            table=Person._meta.db_table,\n            expressions=ExpressionList(\n                IndexExpression(F('first_name')),\n                IndexExpression(F('last_name').desc()),\n                IndexExpression(Upper('last_name').concat(Value(' ')).concat(F('first_name'))),\n            ).resolve_expression(compiler.query),\n            compiler=compiler,\n            quote_value=self.editor.quote_value,\n        )\n        table = Person._meta.db_table\n        self.expressions.rename_column_references(table, 'first_name', 'other')\n        self.assertIs(self.expressions.references_column(table, 'other'), True)\n        self.assertIs(self.expressions.references_column(table, 'first_name'), False)\n        self.assertIn(\n            '(UPPER(%s.%s) || \\' \\' || %s.%s)' % (\n                self.editor.quote_name(table),\n                self.editor.quote_name('last_name'),\n                self.editor.quote_name(table),\n                self.editor.quote_name('other'),\n            ),\n            str(self.expressions),\n        )\n", "    def test_rename_column_references_without_alias(self):\n        compiler = Query(Person, alias_cols=False).get_compiler(connection=connection)\n        table = Person._meta.db_table\n        expressions = Expressions(\n            table=table,\n            expressions=ExpressionList(\n                IndexExpression(Upper('last_name')),\n                IndexExpression(F('first_name')),\n            ).resolve_expression(compiler.query),\n            compiler=compiler,\n            quote_value=self.editor.quote_value,\n        )\n        expressions.rename_column_references(table, 'first_name', 'other')\n        self.assertIs(expressions.references_column(table, 'other'), True)\n        self.assertIs(expressions.references_column(table, 'first_name'), False)\n        expected_str = '(UPPER(%s)), %s' % (\n            self.editor.quote_name('last_name'),\n            self.editor.quote_name('other'),\n        )\n        self.assertEqual(str(expressions), expected_str)\n", "    def setUp(self):\n        self.reference = IndexColumns(\n            'table', ['first_column', 'second_column'],\n            lambda column: column.upper(),\n            ['ASC', 'DESC'],\n            ['op1', 'op2'],\n        )\n"], "sample_93": ["def test_tmp_path_factory_from_config(tmp_path: Path) -> None:\n    config = Config.get_config()\n    config.option.basetemp = tmp_path\n    factory = TempPathFactory.from_config(config, _ispytest=True)\n    assert factory.getbasetemp() == tmp_path\n", "def test_tmp_path_factory_mktemp_with_relative_path(tmp_path_factory: TempPathFactory) -> None:\n    tmp_path = tmp_path_factory.mktemp(\"relative_path\")\n    assert tmp_path.is_relative_to(tmp_path_factory.getbasetemp())\n    assert tmp_path.exists()\n", "def test_tmp_path_factory_handles_invalid_basetemp_characters(\n    tmp_path_factory: TempPathFactory, monkeypatch: MonkeyPatch", "def test_tmp_path_factory_handles_existing_basetemp(\n    tmp_path_factory: TempPathFactory, monkeypatch: MonkeyPatch", "def test_tmpdir_factory_cleanup(pytester: Pytester, tmp_path_factory: TempPathFactory, monkeypatch: MonkeyPatch) -> None:\n    monkeypatch.setattr(tmp_path_factory, \"_basetemp\", None)\n    monkeypatch.setattr(tmp_path_factory, \"_given_basetemp\", None)\n    initial_basetemp = tmp_path_factory.getbasetemp()\n    tmp_path_factory.getbasetemp()  # Trigger cleanup\n    assert not initial_basetemp.exists()\n"], "sample_16": ["def test_display_for_value_boolean(self):\n    self.assertEqual(\n        display_for_value(True, self.empty_value),\n        'True'\n    )\n    self.assertEqual(\n        display_for_value(False, self.empty_value),\n        'False'\n    )\n", "def test_foreign_key_display_for_field(self):\n    site = Site.objects.create(domain='example.com')\n    article = Article.objects.create(site=site, title='Test Article')\n    display_value = display_for_field(article.site, models.ForeignKey(Site, on_delete=models.CASCADE), self.empty_value)\n    expected_value = '<a href=\"{}\">{}</a>'.format(site.get_absolute_url(), str(site))\n    self.assertHTMLEqual(display_value, expected_value)\n", "def test_decimal_display_for_field(self):\n    display_value = display_for_field(Decimal('12345.6789'), models.DecimalField(decimal_places=2), self.empty_value)\n    self.assertEqual(display_value, '12,345.68')\n", "def test_reverse_field_path(self):\n    \"\"\"\n    Tests for reverse_field_path function\n    \"\"\"\n    # Test with direct foreign key\n    model, path = reverse_field_path(Article, \"site\")\n    self.assertEqual(model, Site)\n    self.assertEqual(path, \"article\")\n\n    # Test with reverse foreign key\n    model, path = reverse_field_path(Site, \"article\")\n    self.assertEqual(model, Article)\n    self.assertEqual(path, \"site\")\n\n    # Test with chained foreign keys\n    model, path = reverse_field_path(Location, \"event__site\")\n    self.assertEqual(model, Site)\n    self.assertEqual(path, \"location__event\")\n\n    # Test with chained foreign keys and reverse foreign keys\n    model, path = reverse_field_path(Event, \"location__guest\")\n    self.assertEqual(model, Guest)\n    self.assertEqual(path, \"event__location\")\n\n    # Test with non-existent field\n    with self.assertRaises(FieldDoesNotExist):\n        reverse_field_path(Article, \"nonexistent\")\n\n    # Test with non-relation field\n    with self.assertRaises(NotRelationField):\n        reverse_field_path(Article, \"title\")\n", "def test_file_field_display_for_field(self):\n    file_field = models.FileField()\n    file_field.get_prep_value = lambda value: value.url\n\n    test_file = SimpleUploadedFile(\"test.txt\", b\"test content\")\n    display_value = display_for_field(test_file, file_field, self.empty_value)\n    expected = '<a href=\"{}\">{}</a>'.format(test_file.url, test_file.url)\n    self.assertHTMLEqual(display_value, expected)\n"], "sample_82": ["def test_groupby_fillna(array):\n    filled = array.groupby(\"x\").fillna(0)\n    expected = array.fillna(0)\n    assert_identical(expected, filled)\n", "def test_groupby_keep_attrs(dataset):\n    dataset.attrs = {\"test_attr\": \"test_value\"}\n    grouped = dataset.groupby(\"x\")\n\n    # Test keep_attrs=True\n    result_true = grouped.mean(keep_attrs=True)\n    assert result_true.attrs == dataset.attrs\n\n    # Test keep_attrs=False\n    result_false = grouped.mean(keep_attrs=False)\n    assert result_false.attrs == {}\n", "def test_groupby_apply_dataset(dataset):\n\n        return ds.sum()\n\n    expected = dataset.sum(dim=['x', 'y', 'z'])\n    actual = dataset.groupby('x').apply(func)\n    assert_equal(expected, actual)\n", "def test_groupby_fillna(dataset):\n    grouped = dataset.groupby(\"x\")\n    filled = grouped.fillna(0)\n    expected = dataset.fillna(0)\n    assert_identical(expected, filled)\n", "def test_groupby_unsorted_index():\n    # Test for issue #6079 where groupby raised an error when the group coordinate is unsorted\n    data = xr.DataArray([1, 2, 3, 4, 5, 6], coords={'x': [1, 2, 1, 2, 1, 2]}, dims='x')\n    expected = xr.DataArray([2, 4, 6], coords={'x': [1, 2]}, dims='x')\n    actual = data.groupby('x').sum()\n    assert_equal(expected, actual)\n"], "sample_20": ["def test_unique_constraint_pointing_to_fk_with_db_column(self):\n    class Target(models.Model):\n        pass\n\n    class Model(models.Model):\n        fk_1 = models.ForeignKey(Target, models.CASCADE, related_name='target_1', db_column='custom_fk_1')\n        fk_2 = models.ForeignKey(Target, models.CASCADE, related_name='target_2', db_column='custom_fk_2')\n\n        class Meta:\n            constraints = [\n                models.UniqueConstraint(fields=['custom_fk_1', 'fk_2'], name='name'),\n            ]\n\n    self.assertEqual(Model.check(databases=self.databases), [])\n", "def test_unique_constraint_pointing_to_fk_with_db_column(self):\n    class Target(models.Model):\n        pass\n\n    class Model(models.Model):\n        fk = models.ForeignKey(Target, models.CASCADE, db_column='custom_fk')\n\n        class Meta:\n            constraints = [\n                models.UniqueConstraint(fields=['fk'], name='name'),\n            ]\n\n    self.assertEqual(Model.check(databases=self.databases), [])\n", "    def test_check_constraints_with_function_expression(self):\n        from django.db.models.functions import Upper\n\n        class Model(models.Model):\n            name = models.CharField(max_length=10)\n\n            class Meta:\n                constraints = [\n                    models.CheckConstraint(\n                        check=models.Q(name=Upper('name')),\n                        name='name_is_uppercase'\n                    ),\n                ]\n\n        errors = Model.check(databases=self.databases)\n        warn = Warning(\n            '%s does not support check constraints.' % connection.display_name,\n            hint=(\n                \"A constraint won't be created. Silence this warning if you \"\n                \"don't care about it.\"\n            ),\n            obj=Model,\n            id='models.W027',\n        )\n        expected = [] if connection.features.supports_table_check_constraints else [warn]\n        self.assertCountEqual(errors, expected)\n", "    def test_swappable_valid_model(self):\n        class Model(models.Model):\n            class Meta:\n                swappable = 'TEST_SWAPPED_MODEL'\n\n        self.assertEqual(Model.check(), [])\n", "    def test_unique_constraint_field_not_in_model(self):\n        class Model(models.Model):\n            class Meta:\n                constraints = [models.UniqueConstraint(fields=['not_a_field'], name='name')]\n\n        self.assertEqual(Model.check(databases=self.databases), [\n            Error(\n                \"'constraints' refers to the nonexistent field 'not_a_field'.\",\n                obj=Model,\n                id='models.E012',\n            ),\n        ])\n"], "sample_136": ["def test_BlockMatrix_as_real_imag():\n    A = MatrixSymbol('A', n, m, complex=True)\n    B = MatrixSymbol('B', l, k, complex=True)\n    X = BlockMatrix([[A, B]])\n\n    real_part, imag_part = X.as_real_imag()\n    assert real_part == BlockMatrix([[re(A), re(B)]])\n    assert imag_part == BlockMatrix([[im(A), im(B)]])\n", "def test_BlockMatrix_as_real_imag():\n    A = Matrix([[1, 2], [3, 4]], dtype=complex)\n    B = Matrix([[5, 6], [7, 8]], dtype=complex)\n    X = BlockMatrix([[A, B], [B, A]])\n\n    real_X, imag_X = X.as_real_imag()\n\n    assert real_X == BlockMatrix([[A.real, B.real], [B.real, A.real]])\n    assert imag_X == BlockMatrix([[A.imag, B.imag], [B.imag, A.imag]])\n", "def test_block_collapse_non_square_matrices():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', p, q)\n    X = BlockMatrix([[A, B]])\n    Y = BlockMatrix([[A], [B]])\n\n    assert block_collapse(X * Y) == BlockMatrix([[A * A + B * B.T]])\n", "def test_BlockMatrix_as_real_imag():\n    A, B, C, D = [MatrixSymbol(s, 3, 3) for s in 'ABCD']\n    X = BlockMatrix([[A, B], [C, D]])\n\n    real_matrices, im_matrices = X.as_real_imag()\n    assert real_matrices == BlockMatrix([[re(A), re(B)], [re(C), re(D)]])\n    assert im_matrices == BlockMatrix([[im(A), im(B)], [im(C), im(D)]])\n", "def test_BlockMatrix_as_real_imag():\n    A, B = [MatrixSymbol(s, 2, 2) for s in 'AB']\n    X = BlockMatrix([[A, B], [B, A]])\n\n    real_part, imag_part = X.as_real_imag()\n    assert real_part == BlockMatrix([[re(A), re(B)], [re(B), re(A)]])\n    assert imag_part == BlockMatrix([[im(A), im(B)], [im(B), im(A)]])\n"], "sample_91": ["def test_xfail_imperative_in_class_setup(self, testdir):\n    p = testdir.makepyfile(\n        \"\"\"\n        import pytest\n        class TestClass:\n                pytest.xfail(\"hello\")\n\n                assert 0\n        \"\"\"\n    )\n    result = testdir.runpytest(p)\n    result.stdout.fnmatch_lines([\"*1 xfailed*\"])\n    result = testdir.runpytest(p, \"-rx\")\n    result.stdout.fnmatch_lines([\"*XFAIL*test_this*\", \"*reason:*hello*\"])\n    result = testdir.runpytest(p, \"--runxfail\")\n    result.stdout.fnmatch_lines(\n        \"\"\"\n        *def test_this*\n        *1 fail*\n    \"\"\"\n    )\n", "def test_dynamic_skip_set_during_setup(self, testdir):\n    p = testdir.makepyfile(\n        \"\"\"\n        import pytest\n            pytest.mark.skip(function)\n            assert 0\n            assert 1\n        \"\"\"\n    )\n    result = testdir.runpytest(p, \"-rxX\")\n    result.stdout.fnmatch_lines([\"*SKIP*test_this*\", \"*1 passed*\"])\n", "def test_xfail_condition_invalid_syntax(self, testdir):\n    item = testdir.getitem(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail(\"condition with invalid syntax\")\n            pass\n        \"\"\"\n    )\n    with pytest.raises(pytest.fail.Exception) as excinfo:\n        evaluate_xfail_marks(item)\n    assert excinfo.value.msg is not None\n    assert \"Error evaluating 'xfail' condition\" in excinfo.value.msg\n    assert \"invalid syntax\" in excinfo.value.msg\n", "def test_xfail_imperative_with_reason(self, testdir):\n    p = testdir.makepyfile(\n        \"\"\"\n        import pytest\n            pytest.xfail(\"reason for failure\")\n        \"\"\"\n    )\n    result = testdir.runpytest(p, \"-rx\")\n    result.stdout.fnmatch_lines([\"*XFAIL*test_this*\", \"*reason for failure*\"])\n    result = testdir.runpytest(p, \"--runxfail\")\n    result.stdout.fnmatch_lines([\"*1 pass*\"])\n", "def test_imperativeskip_on_skip_test(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n            assert 1\n\n        @pytest.mark.skipif(\"True\")\n            pass\n    \"\"\"\n    )\n    testdir.makeconftest(\n        \"\"\"\n        import pytest\n            pytest.skip(\"abc\")\n    \"\"\"\n    )\n    result = testdir.runpytest(\"-rs\")\n    result.stdout.fnmatch_lines_random(\n        \"\"\"\n        *SKIP*abc*\n        *1 skipped*\n    \"\"\"\n    )\n"], "sample_118": ["def test_ccode_For_with_Matrix():\n    A = MatrixSymbol('A', 3, 3)\n    f = For(x, Range(0, 3), [aug_assign(A[x, x], '+', x)])\n    sol = ccode(f)\n    assert sol == (\"for (x = 0; x < 3; x += 1) {\\n\"\n                   \"   A[x + 3*x] += x;\\n\"\n                   \"}\")\n", "def test_ccode_nested_For():\n    f = For(x, Range(0, 2), [aug_assign(y, '*', x), For(z, Range(0, 3), [aug_assign(y, '+', z)])])\n    sol = ccode(f)\n    assert sol == (\"for (x = 0; x < 2; x += 1) {\\n\"\n                   \"   y *= x;\\n\"\n                   \"   for (z = 0; z < 3; z += 1) {\\n\"\n                   \"      y += z;\\n\"\n                   \"   }\\n\"\n                   \"}\")\n", "def test_ccode_nested_For():\n    f = For(x, Range(0, 2), [For(y, Range(0, 3), [aug_assign(z, '+', x * y)])])\n    sol = ccode(f)\n    assert sol == (\"for (x = 0; x < 2; x += 1) {\\n\"\n                   \"   for (y = 0; y < 3; y += 1) {\\n\"\n                   \"      z += x * y;\\n\"\n                   \"   }\\n\"\n                   \"}\")\n", "def test_ccode_For_with_Assignment():\n    f = For(x, Range(0, 10, 2), [Assignment(y, x**2)])\n    sol = ccode(f)\n    assert sol == (\"for (x = 0; x < 10; x += 2) {\\n\"\n                   \"   y = pow(x, 2);\\n\"\n                   \"}\")\n", "compilation error"], "sample_62": ["def test_zero_timeout(self):\n    \"\"\"\n    Passing in zero into timeout results in a value that is not cached\n    \"\"\"\n    cache.set(\"key1\", \"eggs\", 0)\n    self.assertIsNone(cache.get(\"key1\"))\n\n    self.assertIs(cache.add(\"key2\", \"ham\", 0), True)\n    self.assertIsNone(cache.get(\"key2\"))\n\n    cache.set_many({\"key3\": \"sausage\", \"key4\": \"lobster bisque\"}, 0)\n    self.assertIsNone(cache.get(\"key3\"))\n    self.assertIsNone(cache.get(\"key4\"))\n\n    cache.set(\"key5\", \"belgian fries\", timeout=5)\n    self.assertIs(cache.touch(\"key5\", timeout=0), True)\n    self.assertIsNone(cache.get(\"key5\"))\n", "    def test_cache_versioning_add_with_version(self):\n        self.assertIs(cache.add(\"answer1\", 42, version=2), True)\n        self.assertIsNone(cache.get(\"answer1\", version=1))\n        self.assertEqual(cache.get(\"answer1\", version=2), 42)\n\n        self.assertIs(cache.add(\"answer1\", 37, version=2), False)\n        self.assertIsNone(cache.get(\"answer1\", version=1))\n        self.assertEqual(cache.get(\"answer1\", version=2), 42)\n\n        self.assertIs(cache.add(\"answer1\", 37, version=1), True)\n        self.assertEqual(cache.get(\"answer1\", version=1), 37)\n        self.assertEqual(cache.get(\"answer1\", version=2), 42)\n\n        self.assertIs(caches[\"v2\"].add(\"answer2\", 42), True)\n        self.assertIsNone(cache.get(\"answer2\", version=1))\n        self.assertEqual(cache.get(\"answer2\", version=2), 42)\n\n        self.assertIs(caches[\"v2\"].add(\"answer2\", 37), False)\n        self.assertIsNone(cache.get(\"answer2\", version=1))\n        self.assertEqual(cache.get(\"answer2\", version=2), 42)\n\n        self.assertIs(caches[\"v2\"].add(\"answer2\", 37, version=1), True)\n        self.assertEqual(cache.get(\"answer2\", version=1), 37)\n        self.assertEqual(cache.get(\"answer2\", version=2), 42)\n\n        self.assertIs(caches[\"v2\"].add(\"answer3\", 42, version=1), True)\n        self.assertEqual(cache.get(\"answer3\", version=1), 42)\n        self.assertIsNone(cache.get(\"answer3\", version=2))\n\n        self.assertIs(caches[\"v2\"].add(\"answer3\", 37, version=1", "def test_patch_cache_control_with_max_age(self):\n    response = HttpResponse()\n    patch_cache_control(response, max_age=60)\n    self.assertEqual(response.headers[\"Cache-Control\"], \"max-age=60\")\n\n    # Test that max_age is updated correctly.\n    patch_cache_control(response, max_age=120)\n    self.assertEqual(response.headers[\"Cache-Control\"], \"max-age=120\")\n\n    # Test that max_age is removed when set to None.\n    patch_cache_control(response, max_age=None)\n    self.assertNotIn(\"max-age\", response.headers[\"Cache-Control\"])\n", "def test_invalid_key_length(self):\n    # memcached limits key length to 250.\n    key = (\"a\" * 250) + \"\u6e05\"\n    expected_warning = (\n        \"Cache key will cause errors if used with memcached: \"\n        \"%r (longer than %s)\" % (key, 250)\n    )\n    self._perform_invalid_key_test(key, expected_warning)\n", "def test_cache_middleware_with_header_encoding(self):\n    \"\"\"\n    Test that the cache middleware respects the Accept-Encoding header.\n    \"\"\"\n    # Define a view that returns a response with content that can be compressed\n        return HttpResponse(\"This is a long response that can be compressed.\")\n\n    # Use the cache_page decorator to cache the view\n    cached_view = cache_page(60)(view)\n\n    # Create a request with the Accept-Encoding header set to gzip\n    request = self.factory.get(\"/view/\", HTTP_ACCEPT_ENCODING=\"gzip\")\n\n    # Call the view twice to ensure it's cached\n    response1 = cached_view(request)\n    response2 = cached_view(request)\n\n    # Check that the responses are compressed\n    self.assertEqual(response1.get(\"Content-Encoding\"), \"gzip\")\n    self.assertEqual(response2.get(\"Content-Encoding\"), \"gzip\")\n\n    # Check that the content is the same\n    self.assertEqual(response1.content, response2.content)\n"], "sample_8": ["def test_sensitive_settings_with_callable_values(self):\n    \"\"\"\n    The debug page should not evaluate callable values in settings.\n    \"\"\"\n        return \"This should not be displayed\"\n\n    with self.settings(DEBUG=True, FOOBAR=callable_setting):\n        response = self.client.get('/raises500/')\n        self.assertNotContains(response, \"This should not be displayed\", status_code=500)\n", "def test_sensitive_function_arguments_obfuscated_in_caller(self):\n    \"\"\"\n    Sensitive variables don't leak in the sensitive_variables decorator's\n    frame, when those variables are passed as arguments to the decorated\n    function, and the decorated function is called in a function that has\n    sensitive variables.\n    \"\"\"\n    self.verify_safe_response(sensitive_args_function_caller, check_for_POST_params=False)\n    self.verify_safe_email(sensitive_args_function_caller, check_for_POST_params=False)\n", "def test_get_traceback_frames_sensitive_variables(self):\n    try:\n        @sensitive_variables('secret_var')\n            secret_var = 'secret_value'\n            return secret_var\n\n        func_with_sensitive_variables()\n    except Exception:\n        exc_type, exc_value, tb = sys.exc_info()\n    request = self.rf.get('/test_view/')\n    reporter = ExceptionReporter(request, exc_type, exc_value, tb)\n    frames = reporter.get_traceback_frames()\n    last_frame = frames[-1]\n    self.assertIn('secret_var', last_frame['vars'])\n    self.assertEqual(last_frame['vars']['secret_var'], CLEANSED_SUBSTITUTE)\n", "    def test_sensitive_multivalue_dict(self):\n        \"\"\"\n        Sensitive values in MultiValueDicts cannot be seen in the default\n        error reports for sensitive requests.\n        \"\"\"\n        request = self.rf.post('/some_url/', self.breakfast_data)\n        request.sensitive_post_parameters = ['bacon-key', 'hash-brown-key']\n        with self.settings(DEBUG=False):\n            self.verify_safe_response(sensitive_view)\n            self.verify_safe_email(sensitive_view)\n", "    def test_sensitive_setting_does_not_break_on_uncleanseable_type(self):\n        \"\"\"\n        The debug page should not break if a setting is of an uncleanseable type.\n        \"\"\"\n        class Uncleanseable:\n                raise Exception('Uncleanseable')\n\n        with self.settings(DEBUG=True, UNCLEANSEABLE=Uncleanseable()):\n            response = self.client.get('/raises500/')\n            self.assertContains(response, 'UNCLEANSEABLE', status_code=500)\n            self.assertContains(response, 'Exception: Uncleanseable', status_code=500)\n"], "sample_101": ["def test_pipeline_fit_transform_with_fit_params():\n    # tests that Pipeline passes fit_params to transformers\n    # when fit_transform is invoked\n    pipe = Pipeline([('transf', TransfFitParams()), ('clf', FitParamT())])\n    pipe.fit_transform(X=None,\n                       y=None,\n                       transf__should_get_this=True)\n    assert pipe.named_steps['transf'].fit_params['should_get_this']\n", "def test_feature_union_empty_transformer_list():\n    # Test that FeatureUnion handles an empty transformer list correctly\n    ft = FeatureUnion(transformer_list=[])\n    X = np.array([[1, 2, 3], [4, 5, 6]])\n    X_transformed = ft.fit_transform(X)\n    assert_array_equal(X_transformed, np.empty((X.shape[0], 0)))\n", "def test_pipeline_with_no_fit_transform():\n    # Test that pipeline works with a transformer missing both fit_transform and transform methods\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    no_fit_transform = NoFit()\n    pipeline = Pipeline([('no_fit_transform', no_fit_transform), ('clf', FitParamT())])\n\n    # test fit_transform:\n    with pytest.raises(TypeError) as excinfo:\n        pipeline.fit_transform(X, y)\n    assert \"does not implement fit and transform\" in str(excinfo.value)\n", "def test_pipeline_transform_with_memory():\n    # Test whether pipeline works with a transformer at the end and memory.\n    # Also test pipeline.transform and pipeline.inverse_transform\n    iris = load_iris()\n    X = iris.data\n    pca = PCA(n_components=2, svd_solver='full')\n    cachedir = mkdtemp()\n    try:\n        if LooseVersion(joblib_version) < LooseVersion('0.12'):\n            # Deal with change of API in joblib\n            memory = Memory(cachedir=cachedir, verbose=10)\n        else:\n            memory = Memory(location=cachedir, verbose=10)\n        pipeline = Pipeline([('pca', pca)], memory=memory)\n\n        # test transform and fit_transform:\n        X_trans = pipeline.fit(X).transform(X)\n        X_trans2 = pipeline.fit_transform(X)\n        X_trans3 = pca.fit_transform(X)\n        assert_array_almost_equal(X_trans, X_trans2)\n        assert_array_almost_equal(X_trans, X_trans3)\n\n        X_back = pipeline.inverse_transform(X_trans)\n        X_back2 = pca.inverse_transform(X_trans)\n        assert_array_almost_equal(X_back, X_back2)\n    finally:\n        shutil.rmtree(cachedir)\n", "def test_pipeline_memory_transformer_without_fit_transform():\n    X = np.array([[1, 2]])\n    transf = NoTrans()\n    pipe = Pipeline([('transf', transf), ('clf', Mult())], memory=DummyMemory())\n    pipe.fit(X, y=None)\n    assert hasattr(transf, 'means_')\n    # Check that the transformer was not cloned and was fit\n    assert transf.means_ is not None\n"], "sample_11": ["def test_serialize_type(self):\n    self.assertSerializedEqual(models.Model)\n    self.assertSerializedResultEqual(\n        models.Model,\n        (\"models.Model\", {\"from django.db import models\"})\n    )\n", "def test_serialize_custom_functions(self):\n        return x * y\n\n    self.assertSerializedEqual(custom_function)\n    self.assertSerializedResultEqual(\n        custom_function,\n        (\"migrations.test_writer.custom_function\", {'import migrations.test_writer'})\n    )\n", "def test_serialize_custom_operations(self):\n    class CustomOperation(migrations.operations.Operation):\n            self.custom_arg = custom_arg\n\n            return (\n                'migrations.test_writer.CustomOperation',\n                [self.custom_arg],\n                {}\n            )\n\n    migration = type(\"Migration\", (migrations.Migration,), {\n        \"operations\": [\n            CustomOperation(\"test_arg\"),\n        ],\n        \"dependencies\": []\n    })\n    writer = MigrationWriter(migration)\n    output = writer.as_string()\n    result = self.safe_exec(output)\n    self.assertIn(\"migrations.test_writer.CustomOperation\", result)\n    self.assertEqual(result[\"Migration\"].operations[0].custom_arg, \"test_arg\")\n", "def test_serialize_deconstructible_class(self):\n    @deconstructible\n    class CustomDeconstructible:\n            self.arg1 = arg1\n            self.arg2 = arg2\n\n    instance = CustomDeconstructible('value1', 'value2')\n    string = MigrationWriter.serialize(instance)[0]\n    self.assertEqual(string, \"migrations.test_writer.CustomDeconstructible('value1', 'value2')\")\n", "def test_serialize_dict(self):\n    self.assertSerializedEqual({'apple': 1, 'banana': 2, 'orange': 3})\n    self.assertSerializedResultEqual({'apple': 1, 'banana': 2, 'orange': 3}, (\"{'apple': 1, 'banana': 2, 'orange': 3}\", set()))\n"], "sample_122": ["def test_scalar_multiply():\n    A = SparseMatrix([[1, 2], [3, 4]])\n    B = A.scalar_multiply(2)\n    assert B == SparseMatrix([[2, 4], [6, 8]])\n    B = A.scalar_multiply(0)\n    assert B == SparseMatrix([[0, 0], [0, 0]])\n    B = A.scalar_multiply(x)\n    assert B == SparseMatrix([[x, 2*x], [3*x, 4*x]])\n", "def test_scalar_multiply():\n    A = SparseMatrix([[1, 2], [3, 4]])\n    B = A.scalar_multiply(2)\n    assert B == SparseMatrix([[2, 4], [6, 8]])\n    B = A.scalar_multiply(0)\n    assert B == SparseMatrix(2, 2, {})\n", "def test_fill():\n    m = SparseMatrix(3, 3, {})\n    m.fill(1)\n    assert m == SparseMatrix([[1, 1, 1], [1, 1, 1], [1, 1, 1]])\n    m.fill(0)\n    assert m == SparseMatrix([[0, 0, 0], [0, 0, 0], [0, 0, 0]])\n    m.fill(2)\n    assert m == SparseMatrix([[2, 2, 2], [2, 2, 2], [2, 2, 2]])\n", "def test_sparse_scalar_multiply():\n    A = SparseMatrix(((1, 2), (3, 4)))\n    B = A.scalar_multiply(2)\n    assert B == SparseMatrix(((2, 4), (6, 8)))\n\n    A = SparseMatrix(((1, 0), (0, 1)))\n    B = A.scalar_multiply(0)\n    assert B == SparseMatrix(((0, 0), (0, 0)))\n", "def test_scalar_multiply():\n    A = SparseMatrix(((1, 2), (3, 4)))\n    B = A.scalar_multiply(2)\n    assert B == SparseMatrix(((2, 4), (6, 8)))\n    A = SparseMatrix(((1, 0), (0, 1)))\n    B = A.scalar_multiply(0)\n    assert B == SparseMatrix(((0, 0), (0, 0)))\n    A = SparseMatrix(((1, 2), (3, 4)))\n    B = A.scalar_multiply(Symbol('x'))\n    assert B == SparseMatrix(((Symbol('x'), 2*Symbol('x')), (3*Symbol('x'), 4*Symbol('x'))))\n"], "sample_54": ["def test_urlize_unicode_idn_email(self):\n    value = \"foo@ex\u00e4mple.com\"\n    output = '<a href=\"mailto:foo@xn--exmple-cua.com\">foo@ex\u00e4mple.com</a>'\n    self.assertEqual(urlize(value), output)\n", "def test_urlize_nofollow(self):\n    value = \"Check out this site: http://example.com.\"\n    output = 'Check out this site: <a href=\"http://example.com/\" rel=\"nofollow\">http://example.com</a>.'\n    self.assertEqual(urlize(value, nofollow=True), output)\n", "def test_urlize_with_email(self):\n    tests = (\n        (\"foo@example.com\", '<a href=\"mailto:foo@example.com\">foo@example.com</a>'),\n        (\"foo@example.co.uk\", '<a href=\"mailto:foo@example.co.uk\">foo@example.co.uk</a>'),\n        (\"foo@xn--80ak6aa92e.com\", '<a href=\"mailto:foo@xn--80ak6aa92e.com\">foo@xn--80ak6aa92e.com</a>'),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value), output)\n", "def test_urlize_with_trim_url_limit(self):\n    tests = (\n        (\"Search for google.com/?q=django and see.\", '<a href=\"http://google.com/?q=django\">google.com/?q=django...</a> and see.', 20),\n        (\"foo@example.com\", '<a href=\"mailto:foo@example.com\">foo@example...</a>', 10),\n    )\n    for value, output, trim_url_limit in tests:\n        with self.subTest(value=value, trim_url_limit=trim_url_limit):\n            self.assertEqual(urlize(value, trim_url_limit=trim_url_limit), output)\n", "def test_urlize_quoting(self):\n    tests = (\n        (\"www.example.com/?q=1<2\", '<a href=\"http://www.example.com/?q=1%3C2\">www.example.com/?q=1&lt;2</a>'),\n        (\"http://www.example.com/?q=1<2\", '<a href=\"http://www.example.com/?q=1%3C2\">www.example.com/?q=1&lt;2</a>'),\n        (\"example.com/?q=1<2\", '<a href=\"http://example.com/?q=1%3C2\">example.com/?q=1&lt;2</a>'),\n        (\"www.example.com/?q=1&2\", '<a href=\"http://www.example.com/?q=1&2\">www.example.com/?q=1&amp;2</a>'),\n        (\"example.com/?q=1&2\", '<a href=\"http://example.com/?q=1&2\">example.com/?q=1&amp;2</a>'),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value), output)\n"], "sample_29": ["def test_expression_wrapper_group_by(self):\n    value = Value('f')\n    value.output_field = None\n    expr = ExpressionWrapper(Upper(value), output_field=IntegerField())\n    group_by_cols = expr.get_group_by_cols(alias=None)\n    self.assertEqual(group_by_cols, [expr.expression])\n    self.assertEqual(group_by_cols[0].output_field, expr.output_field)\n", "def test_uuid_field_update(self):\n    uuid_obj = UUIDPK.objects.create()\n    new_uuid = uuid.uuid4()\n    UUIDPK.objects.filter(pk=uuid_obj.pk).update(pk=new_uuid)\n    self.assertEqual(UUIDPK.objects.get(pk=new_uuid), uuid_obj)\n", "def test_value_with_unresolved_output_field(self):\n    value = Value('name')\n    self.assertIsNone(value.output_field)\n    value.resolve_expression(Query(Employee))\n    self.assertIsInstance(value.output_field, CharField)\n", "def test_expression_wrapper_with_annotated_aggregate(self):\n    Employee.objects.create(firstname='John', lastname='Doe', salary=1000)\n    Employee.objects.create(firstname='Jane', lastname='Doe', salary=2000)\n    queryset = Employee.objects.annotate(\n        salary_avg=ExpressionWrapper(Avg('salary'), output_field=DecimalField())\n    )\n    self.assertEqual(queryset.first().salary_avg, Decimal('1500.00'))\n", "def test_annotation_with_outerref_subquery(self):\n    self.gmbh.point_of_contact = Employee.objects.get(lastname='Meyer')\n    self.gmbh.save()\n    inner = Employee.objects.annotate(\n        outer_lastname=OuterRef(OuterRef('lastname')),\n    ).filter(lastname__startswith=Left('outer_lastname', 1))\n    qs = Employee.objects.annotate(\n        ceo_company=Subquery(\n            Company.objects.filter(\n                point_of_contact__in=inner,\n                ceo__pk=OuterRef('pk'),\n            ).values('name'),\n        ),\n    ).filter(ceo_company__isnull=False)\n    self.assertEqual(qs.get().ceo_company, 'Test GmbH')\n"], "sample_37": ["def test_duration_expressions_with_timedelta(self):\n    delta = datetime.timedelta(days=5)\n    qs = Experiment.objects.annotate(duration=F('estimated_time') + delta)\n    for obj in qs:\n        self.assertEqual(obj.duration, obj.estimated_time + delta)\n", "def test_expression_wrapper_output_field(self):\n    class FuncA(Func):\n        output_field = CharField()\n\n    class FuncB(Func):\n        pass\n\n    expr = ExpressionWrapper(FuncA(), output_field=IntegerField())\n    self.assertIsInstance(expr.output_field, IntegerField)\n\n    expr = ExpressionWrapper(FuncB())\n    self.assertIsInstance(expr.output_field, CharField)\n", "def test_expression_wrapper_with_durationfield(self):\n    start = datetime.datetime(2010, 6, 25, 12, 15, 30, 747000)\n    end = start + datetime.timedelta(days=1)\n    expr = ExpressionWrapper(end - start, output_field=DurationField())\n    qs = Experiment.objects.annotate(duration=expr)\n    self.assertEqual(qs.first().duration, datetime.timedelta(days=1))\n", "    def test_output_field_without_conversion(self):\n        expr = ExpressionWrapper(Value(3), output_field=IntegerField())\n        self.assertEqual(expr.convert_value(), 3)\n", "def test_date_addition(self):\n    # Adding a timedelta to a DateField\n    tomorrow = datetime.date(2010, 6, 26)\n    Experiment.objects.filter(name='e0').update(assigned=F('assigned') + datetime.timedelta(days=1))\n    self.assertEqual(Experiment.objects.get(name='e0').assigned, tomorrow)\n"], "sample_56": ["def test_no_templates_configured(self):\n    self.assertEqual(check_for_template_tags_with_the_same_name(None), [])\n", "def test_template_tags_with_same_module_name(self):\n    with self.settings(\n        TEMPLATES=[\n            self.get_settings(\n                \"same_module_1\", \"same_tags_app_1.templatetags.same_tags\"\n            ),\n            self.get_settings(\n                \"same_module_2\", \"same_tags_app_1.templatetags.same_tags\"\n            ),\n        ]\n    ):\n        self.assertEqual(\n            check_for_template_tags_with_the_same_name(None),\n            [\n                Error(\n                    E003.msg.format(\n                        \"'same_tags'\",\n                        \"'check_framework.template_test_apps.same_tags_app_1.\"\n                        \"templatetags.same_tags'\",\n                    ),\n                    id=E003.id,\n                )\n            ],\n        )\n", "    def test_template_tags_with_same_name_and_different_modules(self):\n        with self.settings(\n            TEMPLATES=[\n                self.get_settings(\n                    \"same_tags\", \"different_module_1.templatetags.same_tags\"\n                ),\n                self.get_settings(\n                    \"same_tags\", \"different_module_2.templatetags.same_tags\"\n                ),\n            ]\n        ):\n            self.assertEqual(\n                check_for_template_tags_with_the_same_name(None),\n                [\n                    Error(\n                        E003.msg.format(\n                            \"'same_tags'\",\n                            \"'different_module_1.templatetags.same_tags', \"\n                            \"'different_module_2.templatetags.same_tags'\",\n                        ),\n                        id=E003.id,\n                    )\n                ],\n            )\n", "    def test_template_tags_with_different_name_and_library(self):\n        with self.settings(\n            TEMPLATES=[\n                self.get_settings(\n                    \"different_tags\", \"different_tags_app.templatetags.different_tags\"\n                ),\n                self.get_settings(\n                    \"not_same_tags\", \"same_tags_app_2.templatetags.same_tags\"\n                ),\n            ]\n        ):\n            self.assertEqual(check_for_template_tags_with_the_same_name(None), [])\n", "    def test_custom_template_tags(self):\n        self.assertEqual(check_for_template_tags_with_the_same_name(None), [])\n"], "sample_88": ["def test_saferepr_with_large_objects():\n    class LargeObject:\n            self.size = size\n            self.data = \"x\" * size\n\n            return f\"LargeObject(size={self.size}, data={self.data!r})\"\n\n    obj = LargeObject(1000)\n    s = saferepr(obj, maxsize=50)\n    assert len(s) == 50\n    assert s.startswith(\"<[... raised in repr()] LargeObject object at 0x\")\n", "def test_maxsize_error_in_ellipsize():\n    class A:\n            raise ValueError(\"...\")\n\n    s = saferepr(A(), maxsize=25)\n    assert len(s) == 25\n    assert s.startswith(\"<[ValueError('...') raised in repr()] A object at 0x\")\n", "def test_maxsize_error_on_function():\n        raise ValueError(\"...\")\n\n    s = saferepr((broken_repr, \"other\"), maxsize=25)\n    assert len(s) == 25\n    assert s[0] == \"(\" and s[-1] == \")\"\n    assert \"ValueError\" in s\n", "def test_large_maxsize():\n    \"\"\"Test saferepr() with a very large maxsize.\"\"\"\n\n    # Create a long string that will not be ellipsized\n    long_str = 'x' * 10000\n\n    # Call saferepr with maxsize larger than the length of the string\n    result = saferepr(long_str, maxsize=15000)\n\n    # Assert that the result is equal to the representation of the string\n    assert result == repr(long_str)\n", "def test_maxdepth():\n    class RecursiveObject:\n            return 'RecursiveObject({!r})'.format(self)\n\n    obj = RecursiveObject()\n    s = saferepr(obj, maxsize=25)\n    assert len(s) == 25\n    assert s == '<RecursiveObject(RecursiveObject(Rec...'\n"], "sample_74": ["def test_colorbar_ticklabels():\n    fig, ax = plt.subplots()\n    im = ax.imshow([[0, 1], [2, 3]])\n    cb = fig.colorbar(im)\n    cb.set_ticklabels(['Low', 'Medium', 'High'])\n    fig.draw_without_rendering()\n    assert [t.get_text() for t in cb.ax.get_yticklabels()] == ['Low', 'Medium', 'High']\n", "def test_colorbar_inverted_axis_ticks():\n    data = np.arange(12).reshape(3, 4)\n    fig, ax = plt.subplots()\n    cmap = mpl.colormaps[\"viridis\"]\n    im = ax.imshow(data, cmap=cmap)\n    cbar = fig.colorbar(im, orientation='vertical')\n    cbar.ax.invert_yaxis()\n    assert np.all(np.diff(cbar.ax.get_yticks()) < 0)\n", "def test_colorbar_ticks_length():\n    fig, ax = plt.subplots()\n    im = ax.imshow([[0, 1], [2, 3]])\n    fig.colorbar(im, ticks=[0, 1, 2, 3], ticklocation='left')\n    fig.colorbar(im, ticks=[0, 1, 2, 3], ticklocation='right')\n", "def test_colorbar_with_orientation_and_position():\n    fig, ax = plt.subplots()\n    data = np.arange(1200).reshape(30, 40)\n    levels = [0, 200, 400, 600, 800, 1000, 1200]\n\n    plt.contourf(data, levels=levels)\n    plt.colorbar(orientation='vertical', position=(0.1, 0.2, 0.8, 0.6))\n", "def test_colorbar_locations():\n    fig, axs = plt.subplots(4, 3, figsize=(9, 12))\n    fig.subplots_adjust(left=0.05, bottom=0.05, right=0.95, top=0.95, hspace=0.5)\n\n    x = np.linspace(-2, 3, 50)\n    y = np.linspace(-2, 3, 30)\n    z = np.cos(x[np.newaxis, :]) + np.sin(y[:, np.newaxis])\n\n    cmap = mpl.colormaps[\"viridis\"]\n    for i, (orientation, loc) in enumerate([('vertical', 'left'), ('vertical', 'right'), ('horizontal', 'top'), ('horizontal', 'bottom')]):\n        ax = axs[i, 0]\n        cs = ax.contourf(x, y, z, 10, cmap=cmap)\n        fig.colorbar(cs, ax=ax, orientation=orientation, fraction=0.4, extendfrac=0.2, aspect=5, label='Label', pad=0.1, shrink=1.0, location=loc)\n        ax.set_title(f'Location: {loc}')\n\n        ax = axs[i, 1]\n        cs = ax.contourf(x, y, z, 10, cmap=cmap)\n        fig.colorbar(cs, ax=ax, orientation=orientation, fraction=0.4, extendfrac=0.2, aspect=5, label='Label', pad=0.05, shrink=0.5, location=loc)\n        ax.set_title(f'Shrink: 0.5')\n\n        ax = axs[i, 2]\n        cs = ax.contourf(x, y, z, 10, cmap=cmap)\n        fig.colorbar(cs, ax=ax"], "sample_111": ["def test_empty_input(metric_name, metric_func):\n    if metric_name in SUPERVISED_METRICS:\n        empty_inputs = [([], []), ([], [0, 1]), ([0, 1], []), ([], [])]\n    else:\n        X = np.random.randint(10, size=(0, 10))\n        empty_inputs = [(X, [])]\n    with pytest.raises(ValueError, match='empty'):\n        for args in empty_inputs:\n            metric_func(*args)\n", "def test_symmetry_with_identical_labels(metric_name):\n    metric = SUPERVISED_METRICS[metric_name]\n    y = [0, 0, 1, 1, 2, 2]\n    assert metric(y, y) == pytest.approx(1.0)\n", "def test_input_consistency(metric_name, metric_func):\n    with pytest.raises(ValueError, match='Input arrays should have the same size.'):\n        metric_func([0, 1], [0, 1, 2])\n", "def test_empty_input(metric_name):\n    # All supervised clustering metrics should return 1.0 for empty inputs\n    metric = SUPERVISED_METRICS[metric_name]\n    assert metric([], []) == 1.0\n    assert metric([], [0, 0]) == 1.0\n    assert metric([0, 0], []) == 1.0\n", "def test_constant_labels(metric_name):\n    metric = SUPERVISED_METRICS[metric_name]\n    constant_labels = [0, 0, 0, 0, 0]\n    unique_labels = [0, 1, 2, 3, 4]\n    assert metric(constant_labels, unique_labels) == 0.0\n    assert metric(unique_labels, constant_labels) == 0.0\n"], "sample_47": ["def test_migrate_backward_to_replaced(self):\n    \"\"\"\n    Test migrating backward to a replaced migration.\n    \"\"\"\n    executor = MigrationExecutor(connection)\n    try:\n        self.assertTableNotExists('migrations_author')\n        self.assertTableNotExists('migrations_book')\n        executor.migrate([('migrations', '0001_squashed_0002')])\n        self.assertTableExists('migrations_author')\n        self.assertTableExists('migrations_book')\n        executor.loader.build_graph()\n        # Migrate backward to a replaced migration.\n        executor.migrate([('migrations', '0001_initial')])\n        self.assertTableExists('migrations_author')\n        self.assertTableNotExists('migrations_book')\n    finally:\n        # Unmigrate everything.\n        executor = MigrationExecutor(connection)\n        executor.migrate([('migrations', None)])\n        self.assertTableNotExists('migrations_author')\n        self.assertTableNotExists('migrations_book')\n", "def test_detect_soft_applied_add_field_column(self):\n    \"\"\"\n    executor.detect_soft_applied() detects columns from an AddField operation.\n    This checks the case of AddField in a migration with other operations (0001)\n    and the case of AddField in its own migration (0002).\n    \"\"\"\n    tables = [\n        \"migrations_project\",\n    ]\n    executor = MigrationExecutor(connection)\n    # Create the table for 0001 but make it look like the migration hasn't\n    # been applied.\n    executor.migrate([(\"migrations\", \"0001_initial\")])\n    executor.migrate([(\"migrations\", None)], fake=True)\n    self.assertTableExists(tables[0])\n    # Table detection sees 0001 is applied but not 0002.\n    migration = executor.loader.get_migration(\"migrations\", \"0001_initial\")\n    self.assertIs(executor.detect_soft_applied(None, migration)[0], True)\n    migration = executor.loader.get_migration(\"migrations\", \"0002_initial\")\n    self.assertIs(executor.detect_soft_applied(None, migration)[0], False)\n\n    # Create the table for both migrations but make it look like neither\n    # has been applied.\n    executor.loader.build_graph()\n    executor.migrate([(\"migrations\", \"0001_initial\")], fake=True)\n    executor.migrate([(\"migrations\", \"0002_initial\")])\n    executor.loader.build_graph()\n    executor.migrate([(\"migrations\", None)], fake=True)\n    # Table detection sees 0002 is applied.\n    migration = executor.loader.get_migration(\"migrations\", \"0002_initial\")\n    self.assertIs(executor.detect_soft_applied(None, migration)[0], True)\n\n    # Leave the table for 0001 except the column. That missing column should\n   ", "    def test_minimize_rollbacks_complex(self):\n        r\"\"\"\n        Minimize rollbacks when target has multiple in-app children and dependencies.\n\n        a: 1 <--- 2 <---\\\n              \\ \\- 3 <- 4\n               \\       \\\n        b:      \\- 1 <-- 2\n        c:         \\- 1\n        d:             \\- 1 <- 2\n\n        Target is a1.\n        \"\"\"\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        a2_impl = FakeMigration('a2')\n        a2 = ('a', '2')\n        a3_impl = FakeMigration('a3')\n        a3 = ('a', '3')\n        a4_impl = FakeMigration('a4')\n        a4 = ('a', '4')\n        b1_impl = FakeMigration('b1')\n        b1 = ('b', '1')\n        b2_impl = FakeMigration('b2')\n        b2 = ('b', '2')\n        c1_impl = FakeMigration('c1')\n        c1 = ('c', '1')\n        d1_impl = FakeMigration('d1')\n        d1 = ('d', '1')\n        d2_impl = FakeMigration('d2')\n        d2 = ('d', '2')\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(a2, a2_impl)\n        graph.add_node(a3, a3_impl)\n        graph.add_node(a4, a4_impl)\n        graph.add_node(b1, b1_impl)\n        graph.add_node(b2, b2_impl)\n        graph.add_node(c1, c1_impl)\n        graph.add_node(d1, d1_impl)\n        graph.add_node(d2, d2_impl)\n        graph.add_dependency(None, a2, a1)\n       ", "def test_migrate_marks_replacement_unapplied_after_migrating_backwards(self):\n    \"\"\"\n    A replacement migration will be marked as unapplied after migrating backwards (#24628).\n    \"\"\"\n    executor = MigrationExecutor(connection)\n    executor.migrate([('migrations', '0001_squashed_0002')])\n    self.assertIn(\n        ('migrations', '0001_squashed_0002'),\n        executor.recorder.applied_migrations(),\n    )\n    executor.loader.build_graph()\n    executor.migrate([('migrations', None)])\n    self.assertNotIn(\n        ('migrations', '0001_squashed_0002'),\n        executor.recorder.applied_migrations(),\n    )\n", "def test_migrate_to_different_squashed_migration(self):\n    \"\"\"\n    Migrating to a different squashed migration unapplies the old one (#24628).\n    \"\"\"\n    recorder = MigrationRecorder(connection)\n    # Apply the first squashed migration\n    executor = MigrationExecutor(connection)\n    executor.migrate([(\"migrations\", \"0001_squashed_0002\")])\n\n    # Record the replaced migrations as applied, to simulate a state where they\n    # were applied separately.\n    recorder.record_applied(\"migrations\", \"0001_initial\")\n    recorder.record_applied(\"migrations\", \"0002_second\")\n\n    # Apply a different squashed migration\n    executor.loader.build_graph()\n    executor.migrate([(\"migrations\", \"0001_squashed_0003\")])\n\n    # The old squashed migration should be marked as unapplied\n    self.assertNotIn(\n        (\"migrations\", \"0001_squashed_0002\"),\n        recorder.applied_migrations(),\n    )\n    # And the new one as applied\n    self.assertIn(\n        (\"migrations\", \"0001_squashed_0003\"),\n        recorder.applied_migrations(),\n    )\n"], "sample_75": ["def test_image_grid_single_top():\n    imdata = np.arange(100).reshape((10, 10))\n\n    fig = plt.figure(1, (2.5, 1.5))\n    grid = ImageGrid(fig, (0, 0, 1, 1), nrows_ncols=(1, 3),\n                     axes_pad=(0.2, 0.15), cbar_mode=\"single\",\n                     cbar_location=\"top\", cbar_size=\"10%\", label_mode=\"L\")\n    for i in range(3):\n        im = grid[i].imshow(imdata, interpolation='none')\n    grid.cbar_axes[0].colorbar(im)\n", "def test_image_grid_single_top():\n    imdata = np.arange(100).reshape((10, 10))\n\n    fig = plt.figure(1, (2.5, 1.5))\n    grid = ImageGrid(fig, (0, 0, 1, 1), nrows_ncols=(1, 3),\n                     axes_pad=(0.2, 0.15), cbar_mode=\"single\",\n                     cbar_location=\"top\", cbar_size=\"10%\", label_mode=\"1\")\n    for i in range(3):\n        im = grid[i].imshow(imdata, interpolation='none')\n    grid.cbar_axes[0].colorbar(im)\n", "def test_image_grid_aspect_ratio():\n    imdata = np.arange(100).reshape((10, 10))\n\n    fig = plt.figure(1, (2, 2))\n    grid = ImageGrid(fig, (0, 0, 1, 1), nrows_ncols=(1, 1), aspect=False)\n    assert not grid.get_aspect()\n    grid[0].imshow(imdata, interpolation='none')\n    grid.set_aspect(True)\n    assert grid.get_aspect()\n", "def test_imagegrid_label_mode_all():\n    imdata = np.arange(9).reshape((3, 3))\n\n    fig = plt.figure()\n    grid = ImageGrid(fig, (0, 0, 1, 1), (2, 1), label_mode=\"all\")\n    for ax in grid:\n        assert ax.xaxis.get_ticklabels()[0].get_visible()\n        assert ax.yaxis.get_ticklabels()[0].get_visible()\n", "def test_grid_aspect_ratio():\n    fig = plt.figure()\n    grid = Grid(fig, 111, (2, 2), aspect=True)\n    ax = grid[0]\n    ax.set(xlim=(0, 2), ylim=(0, 1))\n    fig.canvas.draw()\n    assert ax.get_aspect() == \"equal\"\n\n    grid = Grid(fig, 111, (2, 2), aspect=False)\n    ax = grid[0]\n    ax.set(xlim=(0, 2), ylim=(0, 1))\n    fig.canvas.draw()\n    assert ax.get_aspect() == \"auto\"\n"], "sample_147": ["def test_MatMul_kind_noncommutative():\n    M = Matrix([[1,2],[3,4]])\n    assert MatMul(noncomm_x, M).kind is UndefinedKind\n", "def test_MatMul_kind_noncommutative():\n    M = Matrix([[1,2],[3,4]])\n    assert MatMul(noncomm_x, M).kind is UndefinedKind\n", "def test_Matrix_as_coefficients_kind():\n    M = Matrix([[1, 2], [3, 4]])\n    assert (M * comm_x).kind is MatrixKind(NumberKind)\n", "def test_Matrix_is_number():\n    M = Matrix([[1,2],[3,4]])\n    assert not M.is_number\n", "def test_Derivative_eval_derivative():\n    f = Function('f')\n    assert f(comm_x).diff(comm_x) == Derivative(f(comm_x), comm_x)\n    assert f(comm_x).diff(noncomm_x) == Derivative(f(comm_x), noncomm_x)\n"], "sample_115": ["def test__wrap_in_pandas_container_sparse_error():\n    \"\"\"Check that _wrap_in_pandas_container raises error for sparse input.\"\"\"\n    X_csr = csr_matrix([[1, 0, 3], [0, 0, 1]])\n    match = \"Pandas output does not support sparse data\"\n    with pytest.raises(ValueError, match=match):\n        _wrap_in_pandas_container(X_csr, columns=[\"a\", \"b\", \"c\"])\n", "def test_set_output_pandas_with_index_and_columns():\n    \"\"\"Check that set_output works with custom index and columns.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n\n    X = pd.DataFrame([[1, 2, 3], [4, 5, 6]], index=['A', 'B'], columns=['f1', 'f2', 'f3'])\n    est = EstimatorWithSetOutputIndex().set_output(transform=\"pandas\")\n    est.fit(X)\n\n    new_columns = np.asarray([\"c1\", \"c2\", \"c3\"], dtype=object)\n    new_index = ['X', 'Y']\n\n    X_trans = est.transform(X, columns=new_columns, index=new_index)\n    assert_array_equal(X_trans.index, new_index)\n    assert_array_equal(X_trans.columns, new_columns)\n", "def test_set_output_mixin_no_wrap_methods():\n    \"\"\"Check that methods not specified in auto_wrap_output_keys are not wrapped.\"\"\"\n\n    class CustomEstimator(_SetOutputMixin, auto_wrap_output_keys=()):\n            return self\n\n            return X\n\n            return X\n\n            return X\n\n    est = CustomEstimator()\n    assert hasattr(est, \"set_output\")\n\n    X = np.asarray([[1, 0, 3], [0, 0, 1]])\n    assert np.array_equal(est.transform(X), X)\n    assert np.array_equal(est.fit_transform(X), X)\n    assert np.array_equal(est.predict(X), X)\n", "def test_set_output_mixin_subclassing():\n    \"\"\"Check that subclassing _SetOutputMixin works as expected.\"\"\"\n\n    class CustomEstimator(_SetOutputMixin):\n            self.n_features_in_ = X.shape[1]\n            return self\n\n            return X\n\n            return np.asarray([f\"X{i}\" for i in range(self.n_features_in_)], dtype=object)\n\n    est = CustomEstimator()\n    assert hasattr(est, \"set_output\")\n\n    X = np.asarray([[1, 0, 3], [0, 0, 1]])\n    est.fit(X)\n\n    # Default output is numpy array\n    X_trans = est.transform(X)\n    assert isinstance(X_trans, np.ndarray)\n\n    # Setting output to pandas\n    est.set_output(transform=\"pandas\")\n    X_trans = est.transform(X)\n    assert isinstance(X_trans, pd.DataFrame)\n", "def test_set_output_mixin_inheritance():\n    \"\"\"Check that a subclass of _SetOutputMixin can disable auto-wrapping.\"\"\"\n\n    class NoAutoWrapEstimator(_SetOutputMixin, auto_wrap_output_keys=None):\n            return X\n\n            return input_features\n\n    est = NoAutoWrapEstimator()\n    assert not hasattr(est, \"set_output\")\n\n    X = np.asarray([[1, 0, 3], [0, 0, 1]])\n    assert X is est.transform(X)\n"], "sample_126": ["def test_Float_from_mpmath():\n    import mpmath\n    mpmath.mp.dps = 20\n    x = mpmath.pi\n    f = Float(x, precision=20)\n    assert f._mpf_ == x._mpf_\n", "def test_Float_as_real_imag():\n    a = Float(3.2)\n    assert a.as_real_imag() == (a, 0)\n    b = Float(3.2 + 1.5j)\n    assert b.as_real_imag() == (3.2, 1.5)\n", "def test_Float_equality():\n    f1 = Float(3.14, precision=10)\n    f2 = Float(3.14, precision=15)\n    assert f1 == f2\n    assert not (f1 != f2)\n", "def test_Float_conversion():\n    assert Float(mpf('1.0')) == Float(1.0)\n    assert Float(mpf('1.0'), 20) == Float(1.0, 20)\n    assert Float(mpf('1.0'), 20)._mpf_ == (0, long(1048576), -20, 53)\n    assert Float(mpf('1.0'))._mpf_ == (0, long(1073741824), -20, 53)\n", "def test_NumberSymbol_repr():\n    assert repr(pi) == \"pi\"\n    assert repr(E) == \"E\"\n    assert repr(GoldenRatio) == \"GoldenRatio\"\n    assert repr(EulerGamma) == \"EulerGamma\"\n    assert repr(oo) == \"oo\"\n    assert repr(-oo) == \"-oo\"\n    assert repr(zoo) == \"zoo\"\n    assert repr(nan) == \"nan\"\n    assert repr(I) == \"I\"\n"], "sample_138": ["def test_BlockMatrix_as_real_imag():\n    A = Matrix([[1, 2], [3, 4]])\n    B = Matrix([[1j, 2j], [3j, 4j]])\n    X = BlockMatrix([[A, B], [B, A]])\n\n    real_part, imag_part = X.as_real_imag()\n    assert real_part == BlockMatrix([[A, ZeroMatrix(2, 2)], [ZeroMatrix(2, 2), A]])\n    assert imag_part == BlockMatrix([[ZeroMatrix(2, 2), B], [B, ZeroMatrix(2, 2)]])\n", "def test_BlockMatrix_as_real_imag():\n    A = MatrixSymbol('A', n, n)\n    B = MatrixSymbol('B', m, m)\n    X = BlockMatrix([[A, B]])\n\n    real_X, im_X = X.as_real_imag()\n    assert real_X == BlockMatrix([[re(A), re(B)]])\n    assert im_X == BlockMatrix([[im(A), im(B)]])\n", "def test_BlockMatrix_equals():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', n, m)\n    X = BlockMatrix([[A]])\n    Y = BlockMatrix([[B]])\n    Z = BlockMatrix([[A]])\n\n    assert X != Y\n    assert X == Z\n    assert X.equals(Y) == False\n    assert X.equals(Z) == True\n\n    assert BlockMatrix([[A, B]]).equals(BlockMatrix([[A, B]])) == True\n    assert BlockMatrix([[A, B]]).equals(BlockMatrix([[B, A]])) == False\n", "def test_BlockMatrix_real_imag():\n    A = Matrix([[1 + 2j, 3 - 4j], [5 + 6j, 7 - 8j]])\n    B = BlockMatrix([[A]])\n    real, imag = B.as_real_imag()\n    assert real == ImmutableMatrix([[1, 3], [5, 7]])\n    assert imag == ImmutableMatrix([[2, -4], [6, -8]])\n", "def test_BlockMatrix_transpose():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', n, k)\n    C = MatrixSymbol('C', l, m)\n    D = MatrixSymbol('D', l, k)\n    X = BlockMatrix([[A, B], [C, D]])\n\n    assert X.transpose() == BlockMatrix(Matrix([[A.T, C.T], [B.T, D.T]]))\n    assert block_collapse(X.transpose()) == block_collapse(Transpose(X))\n"], "sample_117": ["def test_restify_type_hints_NewType():\n    assert restify(MyInt) == \":class:`MyInt`\"\n", "def test_restify_type_hints_broken_typevars():\n    assert restify(Tuple[T]) == \":class:`Tuple`\\\\ [:obj:`tests.test_util_typing.T`]\"\n    assert restify(Dict[T, int]) == \":class:`Dict`\\\\ [:obj:`tests.test_util_typing.T`, :class:`int`]\"\n    assert restify(MyInt[T]) == \":class:`MyInt`\\\\ [:obj:`tests.test_util_typing.T`]\"\n", "def test_stringify_type_hints_NewType():\n    assert stringify(MyInt) == \"MyInt\"\n    assert stringify(MyInt.__supertype__) == \"int\"\n", "def test_restify_type_hints_NewType():\n    MyInt2 = NewType('MyInt2', int)\n    assert restify(MyInt2) == \":class:`MyInt2`\"\n", "def test_restify_type_hints_broken_union():\n    from typing import UnionMeta\n    class BrokenUnion(metaclass=UnionMeta):\n        __union_params__ = (int, str, 'invalid')\n\n    assert restify(BrokenUnion) == \":obj:`typing.Union`\\\\ [:class:`int`, :class:`str`]\"\n"], "sample_63": ["def test_password_reset_email_context(self):\n    extra_email_context = {\"extra_info\": \"Some extra information\"}\n    (user, username, email) = self.create_dummy_user()\n    form = PasswordResetForm({\"email\": email})\n    self.assertTrue(form.is_valid())\n    form.save(extra_email_context=extra_email_context)\n    self.assertEqual(len(mail.outbox), 1)\n    self.assertIn(\"extra_info\", mail.outbox[0].context)\n    self.assertEqual(mail.outbox[0].context[\"extra_info\"], \"Some extra information\")\n", "def test_integer_username_in_reset_form(self):\n    username = 12345\n    email = \"test@example.com\"\n    password = \"password\"\n    user = IntegerUsernameUser.objects.create_user(username=username, password=password, email=email)\n    data = {\"email\": email}\n    form = PasswordResetForm(data)\n    self.assertTrue(form.is_valid())\n    form.save()\n    self.assertEqual(len(mail.outbox), 1)\n    self.assertEqual(mail.outbox[0].to, [email])\n", "    def test_validates_password(self):\n        user = User.objects.get(username=\"testclient\")\n        data = {\n            \"password1\": \"testclient\",\n            \"password2\": \"testclient\",\n        }\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        self.assertEqual(len(form[\"password2\"].errors), 2)\n        self.assertIn(\n            \"The password is too similar to the username.\", form[\"password2\"].errors\n        )\n        self.assertIn(\n            \"This password is too short. It must contain at least 12 characters.\",\n            form[\"password2\"].errors,\n        )\n", "    def test_help_text_translation(self):\n        french_help_texts = [\n            \"Votre mot de passe ne peut pas trop ressembler \u00e0 vos autres informations personnelles.\",\n            \"Votre mot de passe doit contenir au minimum 8 caract\u00e8res.\",\n        ]\n        user = User.objects.get(username=\"testclient\")\n        form = AdminPasswordChangeForm(user)\n        with translation.override(\"fr\"):\n            html = form.as_p()\n            for french_text in french_help_texts:\n                self.assertIn(french_text, html)\n", "def test_unicode_email(self):\n    email = \"user@example.com\"\n    username = \"\u03b2\u03b1\u03c3\u03af\u03bb\u03b7\u03c2\"\n    User.objects.create_user(username=username, password=\"pwd\", email=email)\n    data = {\"email\": email}\n    form = PasswordResetForm(data)\n    self.assertTrue(form.is_valid())\n    form.save(domain_override=\"example.com\")\n    self.assertEqual(form.cleaned_data[\"email\"], email)\n    self.assertEqual(len(mail.outbox), 1)\n    self.assertEqual(mail.outbox[0].to, [email])\n"], "sample_31": ["    def test_shell_with_bpython_installed(self, select):\n        select.return_value = ([], [], [])\n        # No exception should be raised if bpython is installed\n        call_command('shell', interface='bpython')\n", "def test_shell_with_ipython_installed(self, select):\n    select.return_value = ([], [], [])\n    with mock.patch('IPython.start_ipython') as mock_start_ipython:\n        call_command('shell', interface='ipython')\n    mock_start_ipython.assert_called_once_with(argv=[])\n", "def test_shell_with_ipython_installed(self, select):\n    select.return_value = ([], [], [])\n    with self.assertLogs('IPython', 'INFO') as cm:\n        call_command('shell', interface='ipython')\n    self.assertTrue(cm.records)\n", "def test_python_shell_with_pythonrc(self, mock_print_exc, mock_open, mock_isfile):\n    mock_isfile.return_value = True\n    mock_open.return_value = mock.mock_open(read_data='print(\"Hello from pythonrc\")').return_value\n    with captured_stdout() as stdout:\n        call_command('shell', interface='python')\n    mock_open.assert_called_once()\n    mock_print_exc.assert_not_called()\n    self.assertEqual(stdout.getvalue().strip(), 'Hello from pythonrc')\n", "def test_python_startup_file_execution(self, isfile, select):\n    isfile.return_value = True\n    select.return_value = ([], [], [])\n    with mock.patch('django.core.management.commands.shell.open', mock.mock_open(read_data='print(\"Python startup file executed\")')) as mocked_file, \\\n         mock.patch.dict('os.environ', {'PYTHONSTARTUP': '/path/to/startupfile'}):\n        with captured_stdout() as stdout:\n            call_command('shell')\n        self.assertIn('Python startup file executed', stdout.getvalue())\n"], "sample_81": ["def test_regex_notes(self) -> None:\n    code = \"\"\"a = 1\n            # TICKET-123\n            # TODO\n            # TICKET-456\n            \"\"\"\n    with self.assertAddsMessages(\n        MessageTest(msg_id=\"fixme\", line=2, args=\"TICKET-123\", col_offset=17),\n        MessageTest(msg_id=\"fixme\", line=4, args=\"TICKET-456\", col_offset=17),\n    ):\n        self.checker.process_tokens(_tokenize_str(code))\n", "def test_encoding_error(self) -> None:\n    code = \"\"\"# -*- coding: utf-8 -*-\n                b'\\xe9'.decode('ascii')\n                \"\"\"\n    with self.assertAddsMessages(\n        MessageTest(msg_id=\"syntax-error\", line=2, args=\"Cannot decode using encoding 'ascii', bad encoding\")\n    ):\n        self.checker.process_module(self.linter.get_ast(code))\n", "def test_regex_codetag(self) -> None:\n    code = \"\"\"a = 1\n            # CODETAG\n            # FIXME\n            \"\"\"\n    with self.assertAddsMessages(\n        MessageTest(msg_id=\"fixme\", line=2, args=\"CODETAG\", col_offset=17)\n    ):\n        self.checker.process_tokens(_tokenize_str(code))\n", "def test_notes_rgx_option(self) -> None:\n    set_config(notes_rgx=r'CUSTOMTAG')\n    code = \"\"\"a = 1\n            # CUSTOMTAG\n            \"\"\"\n    with self.assertAddsMessages(\n        MessageTest(msg_id=\"fixme\", line=2, args=\"CUSTOMTAG\", col_offset=17)\n    ):\n        self.checker.process_tokens(_tokenize_str(code))\n", "def test_custom_regex_codetag(self) -> None:\n    code = \"\"\"a = 1\n            # MYTAG\n            # TODO\n            \"\"\"\n    with self.assertAddsMessages(\n        MessageTest(msg_id=\"fixme\", line=2, args=\"MYTAG\", col_offset=17)\n    ):\n        self.checker.process_tokens(_tokenize_str(code))\n"], "sample_114": ["def test_ovr_decision_function_n_classes_mismatch():\n    # test that an exception is raised when n_classes is not consistent with the shape of predictions\n\n    predictions = np.array([[0, 1, 1], [0, 1, 0]])\n\n    confidences = np.array([[-1e16, 0, -1e16], [1.0, 2.0, -3.0]])\n\n    n_classes = 4\n\n    with pytest.raises(ValueError):\n        _ovr_decision_function(predictions, confidences, n_classes)\n", "def test_ovr_decision_function_invalid_inputs():\n    predictions = np.array([[0, 1, 1], [0, 1, 0], [0, 1, 1], [0, 1, 1]])\n    confidences = np.array(\n        [[-1e16, 0, -1e16], [1.0, 2.0, -3.0], [-5.0, 2.0, 5.0], [-0.5, 0.2, 0.5]]\n    )\n    n_classes = 3\n\n    # Test with invalid number of classifiers\n    with pytest.raises(ValueError):\n        _ovr_decision_function(predictions, confidences, n_classes + 1)\n\n    # Test with invalid predictions shape\n    with pytest.raises(ValueError):\n        _ovr_decision_function(predictions[:, :2], confidences, n_classes)\n\n    # Test with invalid confidences shape\n    with pytest.raises(ValueError):\n        _ovr_decision_function(predictions, confidences[:, :2], n_classes)\n", "def test_ovr_decision_function_invalid_input():\n    # test ovr decision function with invalid input\n    predictions = np.array([[0, 1, 1], [0, 1, 0]])\n    confidences = np.array([[1.0, 2.0, -3.0], [1.0, 2.0, -3.0]])\n\n    # Test with invalid n_classes\n    with pytest.raises(ValueError):\n        _ovr_decision_function(predictions, confidences, n_classes=4)\n\n    # Test with invalid predictions shape\n    with pytest.raises(ValueError):\n        _ovr_decision_function(predictions[:, :2], confidences, n_classes=3)\n\n    # Test with invalid confidences shape\n    with pytest.raises(ValueError):\n        _ovr_decision_function(predictions, confidences[:, :2], n_classes=3)\n", "def test_ovr_decision_function_negative_confidences():\n    # test ovr decision function with negative confidences\n\n    predictions = np.array([[0, 1, 1], [0, 1, 0], [0, 1, 1], [0, 1, 1]])\n\n    confidences = np.array(\n        [[-1.0, -2.0, -3.0], [-5.0, -2.0, 5.0], [1e16, 0, -1e16], [-0.5, 0.2, 0.5]]\n    )\n\n    n_classes = 3\n\n    dec_values = _ovr_decision_function(predictions, confidences, n_classes)\n\n    # check that the decision values are within 0.5 range of the votes\n    votes = np.array([[1, 2, 0], [2, 1, 1], [0, 1, 2], [2, 0, 1]])\n\n    assert_allclose(votes, dec_values, atol=0.5)\n\n    # check that the prediction are what we expect\n    expected_prediction = np.array([1, 0, 2, 0])\n    assert_array_equal(np.argmax(dec_values, axis=1), expected_prediction)\n\n    # first and second sample have the same vote but first sample\n    # has higher confidence, this should reflect on the decision values\n    assert dec_values[0, 0] > dec_values[1, 0]\n\n    # third and fourth sample have the same vote but third sample\n    # has higher confidence, this should reflect on the decision values\n    assert dec_values[2, 2] > dec_values[3, 2]\n", "def test_ovr_decision_function_different_n_classes():\n    # test with different number of classes\n\n    predictions = np.array([[0, 1], [0, 1], [1, 0], [1, 0]])\n\n    confidences = np.array([[0, -1e16], [1.0, -3.0], [-5.0, 2.0], [-0.5, 0.2]])\n\n    n_classes = 4\n\n    with pytest.raises(ValueError, match=\"n_classifiers must be equal to\"):\n        _ovr_decision_function(predictions, confidences, n_classes)\n"], "sample_130": ["def test_tensorflow_complex_numbers():\n    if not tensorflow:\n        skip(\"tensorflow not installed.\")\n    expr = Abs(x + I*y)\n    func = lambdify((x, y), expr, modules=\"tensorflow\")\n    a = tensorflow.placeholder(dtype=tensorflow.complex64)\n    b = tensorflow.placeholder(dtype=tensorflow.complex64)\n    s = tensorflow.Session()\n    result = func(a, b).eval(session=s, feed_dict={a: 3+4j, b: 1-2j})\n    assert result == 5.0\n", "def test_lambdify_nested_symbolic_args():\n    x, y = Dummy(), Dummy()\n    f4 = lambdify([[x, y]], [y, x], 'sympy')\n    assert f4([2, 3]) == [3, 2]\n", "def test_tensorflow_broadcasting():\n    if not tensorflow:\n        skip(\"tensorflow not installed.\")\n\n    expr = x * y\n    func = lambdify((x, y), expr, modules=\"tensorflow\")\n    a = tensorflow.placeholder(dtype=tensorflow.float32)\n    b = tensorflow.constant([1.0, 2.0, 3.0], dtype=tensorflow.float32)\n    s = tensorflow.Session()\n    numpy.testing.assert_array_equal(func(a, b).eval(session=s, feed_dict={a: 2.0}), numpy.array([2.0, 4.0, 6.0]))\n", "def test_lambdify_matrix_operations():\n    if not numpy:\n        skip(\"numpy not installed\")\n\n    A = Matrix([[x, y], [z, 1+z]])\n    B = Matrix([[x**2], [Abs(x)]])\n    mat_func = lambdify((x, y, z), A*B, modules=\"numpy\")\n    numpy.testing.assert_array_equal(mat_func(2, 3, 4), numpy.array([[13], [6]]))\n\n    mat_func = lambdify((x, y, z), A + B, modules=\"numpy\")\n    numpy.testing.assert_array_equal(mat_func(2, 3, 4), numpy.array([[x+2, y+3], [z+5, Abs(x)+1]]))\n\n    mat_func = lambdify((x, y, z), A - B, modules=\"numpy\")\n    numpy.testing.assert_array_equal(mat_func(2, 3, 4), numpy.array([[x-2, y-3], [z-5, -Abs(x)+1]]))\n", "def test_lambdify_array_arg():\n    # Test for issue 14655 (plain Python part)\n    f = lambdify([[x, y]], x*x + y, 'math')\n\n    assert f([2.0, 1.0]) == 5\n"], "sample_131": ["def test_trigonometric_functions():\n    assert mcode(sin(x)) == \"Sin[x]\"\n    assert mcode(cos(x)) == \"Cos[x]\"\n    assert mcode(tan(x)) == \"Tan[x]\"\n    assert mcode(exp(x)) == \"Exp[x]\"\n    assert mcode(sinh(x)) == \"Sinh[x]\"\n    assert mcode(cosh(x)) == \"Cosh[x]\"\n    assert mcode(tanh(x)) == \"Tanh[x]\"\n", "def test_Trigonometric_functions():\n    assert mcode(sin(x)) == \"Sin[x]\"\n    assert mcode(cos(x)) == \"Cos[x]\"\n    assert mcode(exp(x)) == \"Exp[x]\"\n    assert mcode(sin(x)**2 + cos(x)**2) == \"Sin[x]^2 + Cos[x]^2\"\n", "def test_TrigFunctions():\n    assert mcode(sin(x)) == \"Sin[x]\"\n    assert mcode(cos(x)) == \"Cos[x]\"\n    assert mcode(exp(x)) == \"Exp[x]\"\n    assert mcode(sin(x) + cos(x)) == \"Sin[x] + Cos[x]\"\n    assert mcode(sin(x)**2 + cos(x)**2) == \"Sin[x]^2 + Cos[x]^2\"\n", "def test_Trigonometric_Functions():\n    assert mcode(sin(x)) == \"Sin[x]\"\n    assert mcode(cos(x)) == \"Cos[x]\"\n    assert mcode(exp(x)) == \"Exp[x]\"\n    assert mcode(sinh(x)) == \"Sinh[x]\"\n    assert mcode(cosh(x)) == \"Cosh[x]\"\n    assert mcode(tan(x)) == \"Tan[x]\"\n    assert mcode(cot(x)) == \"Cot[x]\"\n    assert mcode(tanh(x)) == \"Tanh[x]\"\n    assert mcode(coth(x)) == \"Coth[x]\"\n    assert mcode(sech(x)) == \"Sech[x]\"\n    assert mcode(csch(x)) == \"Csch[x]\"\n", "def test_TrigonometricFunctions():\n    assert mcode(sin(x)) == \"Sin[x]\"\n    assert mcode(cos(x)) == \"Cos[x]\"\n    assert mcode(exp(x)) == \"Exp[x]\"\n    assert mcode(sin(x) + cos(x)) == \"Cos[x] + Sin[x]\"\n    assert mcode(sin(x**2)) == \"Sin[x^2]\"\n    assert mcode(cos(sin(x))) == \"Cos[Sin[x]]\"\n    assert mcode(exp(sin(cos(x)))) == \"Exp[Sin[Cos[x]]]\"\n"], "sample_32": ["def test_key_transform_raw_expression_none(self):\n    expr = RawSQL(self.raw_sql, ['null'])\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__foo=KeyTransform('x', expr)),\n        [],\n    )\n", "def test_key_transform_exact_with_key_transform(self):\n    obj = NullableJSONModel.objects.create(value={'a': {'b': 'c'}})\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__a__exact=KeyTransform('b', 'value')),\n        [obj],\n    )\n", "def test_key_transform_exact(self):\n    obj = NullableJSONModel.objects.create(value={'d': 'e', 'f': {'g': 'h'}})\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__d__exact=KeyTransform('d', 'value')),\n        [obj],\n    )\n", "def test_key_exact_quoted_string(self):\n    obj = NullableJSONModel.objects.create(value={'o': '\"quoted\"'})\n    self.assertEqual(\n        NullableJSONModel.objects.filter(value__o__exact='\"quoted\"').get(),\n        obj,\n    )\n", "def test_key_transform_expression_wrapper(self):\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.annotate(\n            expr=ExpressionWrapper(\n                KeyTransform('c', 'value'),\n                output_field=models.JSONField(),\n            ),\n        ).filter(expr__gt=2),\n        [self.objs[3], self.objs[4]],\n    )\n"], "sample_128": ["def test_Domain_excludes():\n    opt = Options((x, y), {'domain': 'ZZ', 'field': True})\n    raises(OptionError, lambda: opt)\n\n    opt = Options((x, y), {'domain': 'ZZ', 'greedy': True})\n    raises(OptionError, lambda: opt)\n\n    opt = Options((x, y), {'domain': 'ZZ', 'split': True})\n    raises(OptionError, lambda: opt)\n\n    opt = Options((x, y), {'domain': 'ZZ', 'gaussian': True})\n    raises(OptionError, lambda: opt)\n\n    opt = Options((x, y), {'domain': 'ZZ', 'extension': {sqrt(2)}})\n    raises(OptionError, lambda: opt)\n", "def test_Auto_postprocess_with_domain_or_field():\n    opt = {'domain': ZZ}\n    Auto.postprocess(opt)\n\n    assert opt == {'domain': ZZ, 'auto': False}\n\n    opt = {'field': True}\n    Auto.postprocess(opt)\n\n    assert opt == {'field': True, 'auto': False}\n\n    opt = {'extension': {sqrt(2)}}\n    Auto.postprocess(opt)\n\n    assert opt == {'extension': {sqrt(2)}, 'domain': QQ.algebraic_field(sqrt(2)), 'auto': False}\n\n    opt = {'gaussian': True}\n    Auto.postprocess(opt)\n\n    assert opt == {'gaussian': True, 'extension': {I}, 'domain': QQ.algebraic_field(I), 'auto': False}\n", "def test_Domain_with_gens_interfering():\n    raises(GeneratorsError, lambda: Domain.postprocess({'gens': (x, y), 'domain': ZZ[x, z]}))\n", "def test_Options_init_strict():\n    # Test strict mode for options\n    raises(OptionError, lambda: Options((x, y, z), {'domain': 'ZZ', 'frac': True}, strict=True))\n", "def test_Gens_postprocess_with_domain_and_gens_interfere():\n    opt = {'gens': (x, y), 'domain': ZZ[x]}\n    raises(GeneratorsError, lambda: Gens.postprocess(opt))\n\n    opt = {'gens': (x, y), 'domain': QQ[x]}\n    raises(GeneratorsError, lambda: Gens.postprocess(opt))\n\n    opt = {'gens': (x, y), 'domain': RR[x]}\n    raises(GeneratorsError, lambda: Gens.postprocess(opt))\n\n    opt = {'gens': (x, y), 'domain': CC[x]}\n    raises(GeneratorsError, lambda: Gens.postprocess(opt))\n"], "sample_144": ["def test_refine_nan():\n    assert refine(nan, Q.real(nan)) is nan\n    assert refine(nan, Q.imaginary(nan)) is nan\n    assert refine(nan, Q.complex(nan)) is nan\n    assert refine(nan, Q.finite(nan)) is False\n    assert refine(nan, Q.infinite(nan)) is False\n", "def test_abs_with_complex_base():\n    x = Symbol('x', complex=True)\n    assert refine(Abs(x), Q.real(x)) == Abs(re(x))\n    assert refine(Abs(x), Q.imaginary(x)) == Abs(im(x))\n", "def test_refine_real_imaginary():\n    x = Symbol('x', complex=True)\n    assert refine(re(x), Q.complex(x) & Q.real(re(x)) & Q.imaginary(im(x))) == re(x)\n    assert refine(im(x), Q.complex(x) & Q.real(re(x)) & Q.imaginary(im(x))) == im(x)\n", "def test_refine_integral():\n    from sympy import symbols, Integral, exp, oo, Q\n    x = symbols('x', real=True)\n    assert refine(Integral(exp(-x**2), (x, -oo, oo)), Q.real(x)) == sqrt(pi)/2\n", "def test_sign_complex():\n    x = Symbol('x', complex=True)\n    assert refine(sign(x), Q.zero(x)) == 0\n    assert refine(sign(x), Q.nonzero(x)) == sign(arg(x)) / pi + S.ImaginaryUnit\n"], "sample_35": ["def test_modelchoicefield_to_field_name(self):\n    # Create choices for the model choice field tests below.\n    ChoiceModel.objects.create(pk=1, name='a')\n    ChoiceModel.objects.create(pk=2, name='b')\n    ChoiceModel.objects.create(pk=3, name='c')\n\n    # ModelChoiceField with to_field_name\n    e = {\n        'required': 'REQUIRED',\n        'invalid_choice': 'INVALID CHOICE',\n    }\n    f = ModelChoiceField(queryset=ChoiceModel.objects.all(), to_field_name='name', error_messages=e)\n    self.assertFormErrors(['REQUIRED'], f.clean, '')\n    self.assertFormErrors(['INVALID CHOICE'], f.clean, 'd')\n", "    def test_modelchoicefield_to_field(self):\n        # Create choices for the model choice field tests below.\n        ChoiceModel.objects.create(pk=1, name='a', description='desc_a')\n        ChoiceModel.objects.create(pk=2, name='b', description='desc_b')\n        ChoiceModel.objects.create(pk=3, name='c', description='desc_c')\n\n        # ModelChoiceField with to_field_name\n        e = {\n            'required': 'REQUIRED',\n            'invalid_choice': 'INVALID CHOICE',\n        }\n        f = ModelChoiceField(queryset=ChoiceModel.objects.all(), to_field_name='description', error_messages=e)\n        self.assertFormErrors(['REQUIRED'], f.clean, '')\n        self.assertFormErrors(['INVALID CHOICE'], f.clean, 'desc_d')\n\n        # ModelMultipleChoiceField with to_field_name\n        e = {\n            'required': 'REQUIRED',\n            'invalid_choice': '%(value)s IS INVALID CHOICE',\n            'invalid_list': 'NOT A LIST OF VALUES',\n        }\n        f = ModelMultipleChoiceField(queryset=ChoiceModel.objects.all(), to_field_name='description', error_messages=e)\n        self.assertFormErrors(['REQUIRED'], f.clean, '')\n        self.assertFormErrors(['NOT A LIST OF VALUES'], f.clean, 'desc_d')\n        self.assertFormErrors(['desc_d IS INVALID CHOICE'], f.clean, ['desc_d'])\n", "def test_modelchoicefield_to_field_name(self):\n    # Create choices for the model choice field tests below.\n    ChoiceModel.objects.create(id=1, name='a')\n    ChoiceModel.objects.create(id=2, name='b')\n    ChoiceModel.objects.create(id=3, name='c')\n\n    # ModelChoiceField with to_field_name\n    e = {\n        'required': 'REQUIRED',\n        'invalid_choice': 'INVALID CHOICE',\n    }\n    f = ModelChoiceField(queryset=ChoiceModel.objects.all(), to_field_name='name', error_messages=e)\n    self.assertFormErrors(['REQUIRED'], f.clean, '')\n    self.assertFormErrors(['INVALID CHOICE'], f.clean, 'd')\n", "def test_modelchoicefield_to_field_name(self):\n    # Create choices for the model choice field tests below.\n    ChoiceModel.objects.create(pk=1, name='a')\n    ChoiceModel.objects.create(pk=2, name='b')\n    ChoiceModel.objects.create(pk=3, name='c')\n\n    # ModelChoiceField with to_field_name\n    e = {\n        'required': 'REQUIRED',\n        'invalid_choice': 'INVALID CHOICE',\n    }\n    f = ModelChoiceField(queryset=ChoiceModel.objects.all(), to_field_name='name', error_messages=e)\n    self.assertFormErrors(['REQUIRED'], f.clean, '')\n    self.assertFormErrors(['INVALID CHOICE'], f.clean, 'd')\n\n    # ModelMultipleChoiceField with to_field_name\n    e = {\n        'required': 'REQUIRED',\n        'invalid_choice': '%(value)s IS INVALID CHOICE',\n        'invalid_list': 'NOT A LIST OF VALUES',\n    }\n    f = ModelMultipleChoiceField(queryset=ChoiceModel.objects.all(), to_field_name='name', error_messages=e)\n    self.assertFormErrors(['REQUIRED'], f.clean, '')\n    self.assertFormErrors(['NOT A LIST OF VALUES'], f.clean, 'c')\n    self.assertFormErrors(['d IS INVALID CHOICE'], f.clean, ['d'])\n", "    def test_modelchoicefield_to_field(self):\n        ChoiceModel.objects.create(pk=1, name='a')\n        ChoiceModel.objects.create(pk=2, name='b')\n\n        f = ModelChoiceField(queryset=ChoiceModel.objects.all(), to_field_name='name')\n        self.assertEqual(f.clean('a').pk, 1)\n        self.assertFormErrors(['Select a valid choice. a is not one of the available choices.'], f.clean, 'd')\n\n        f = ModelMultipleChoiceField(queryset=ChoiceModel.objects.all(), to_field_name='name')\n        self.assertEqual(f.clean(['a', 'b']).count(), 2)\n        self.assertFormErrors(['Select a valid choice. d is not one of the available choices.'], f.clean, ['d'])\n"], "sample_61": ["    def test_non_uniform_grouping(self):\n        self.assertEqual(\n            nformat(1234567, \".\", grouping=(3, 2, 0), thousand_sep=\",\", force_grouping=True),\n            \"12,34,567\"\n        )\n", "def test_non_uniform_grouping(self):\n    self.assertEqual(nformat(123456789, \".\", grouping=(3, 2, 0), thousand_sep=\",\", force_grouping=True), \"123,45,6789\")\n", "def test_non_uniform_digit_grouping(self):\n    self.assertEqual(\n        nformat(1234567890, \".\", grouping=(3, 2, 0), thousand_sep=\",\"),\n        \"12,34,56,7890\"\n    )\n", "def test_grouping_sequence(self):\n    self.assertEqual(\n        nformat(123456789, \".\", grouping=[3, 2, 0], thousand_sep=\",\"),\n        \"123,45,6789\"\n    )\n", "    def test_scientific_notation(self):\n        self.assertEqual(nformat(Decimal(\"1e10\"), \".\", grouping=3, thousand_sep=\",\"), \"10,000,000,000\")\n        self.assertEqual(nformat(Decimal(\"1e10\"), \".\", decimal_pos=2, grouping=3, thousand_sep=\",\"), \"10,000,000,000.00\")\n"], "sample_108": ["def test_decision_function_shape_non_binary():\n    X, y = make_blobs(centers=4, random_state=0)\n    for estimator in [svm.SVC, svm.NuSVC]:\n        clf = estimator(decision_function_shape=\"ovr\").fit(X, y)\n        dec = clf.decision_function(X)\n        assert dec.shape == (len(X), 4)\n        assert_array_equal(clf.predict(X), np.argmax(dec, axis=1))\n", "def test_decision_function_shape_binary():\n    # Test decision function shape for binary classification\n    X, y = make_classification(n_classes=2, random_state=0)\n\n    clf = svm.SVC(decision_function_shape='ovr')\n    clf.fit(X, y)\n    dec = clf.decision_function(X)\n    assert dec.shape == (len(X), 1)\n\n    clf = svm.SVC(decision_function_shape='ovo')\n    clf.fit(X, y)\n    dec = clf.decision_function(X)\n    assert dec.shape == (len(X),)\n", "def test_linear_svc_intercept_scaling_error():\n    # Test that an error is raised when intercept_scaling is not a float\n\n    lsvc = svm.LinearSVC(intercept_scaling='invalid')\n    msg = \"intercept_scaling must be a float, got <class 'str'>.\"\n    assert_raise_message(TypeError, msg, lsvc.fit, X, Y)\n", "def test_linearsvr_dual_false():\n    # Test LinearSVR with dual=False (no dual formulation)\n    X, y = make_regression(n_samples=10, n_features=5, noise=0.1, random_state=0)\n\n    clf = svm.LinearSVR(dual=False, random_state=0)\n    clf.fit(X, y)\n\n    pred = clf.predict(X)\n    assert_array_almost_equal(pred, y, decimal=2)\n\n    # Check that coef_ and intercept_ are not None\n    assert clf.coef_ is not None\n    assert clf.intercept_ is not None\n", "def test_svc_multiclass():\n    # Test SVC with multi-class formulations\n    X, y = make_classification(n_samples=100, n_features=20, n_informative=2,\n                               n_redundant=10, n_classes=3, n_clusters_per_class=1,\n                               random_state=0)\n\n    # Fit SVC with different multi-class strategies\n    ovr_clf = svm.SVC(decision_function_shape='ovr', random_state=0).fit(X, y)\n    cs_clf = svm.SVC(decision_function_shape='ovr', multi_class='crammer_singer', random_state=0).fit(X, y)\n\n    # Check that predictions are similar for ovr and crammer-singer\n    assert (ovr_clf.predict(X) == cs_clf.predict(X)).mean() > 0.9\n\n    # Check that classifiers are not the same\n    assert (ovr_clf.coef_ != cs_clf.coef_).all()\n\n    # Test decision function\n    assert_array_equal(cs_clf.predict(X), np.argmax(cs_clf.decision_function(X), axis=1))\n    dec_func = np.dot(X, cs_clf.coef_.T) + cs_clf.intercept_\n    assert_array_almost_equal(dec_func, cs_clf.decision_function(X))\n"], "sample_141": ["def test_issue_14933():\n    assert (log(inch) - log(2)).expand(log=True) == log(inch) - log(2)\n    assert (log(inch) - log(foot)).expand(log=True) == log(inch/foot)\n    p = symbols('p', positive=True)\n    assert (log(inch) - log(p)).expand(log=True) == log(inch/p)\n", "def test_quantity_subs():\n    x = Symbol('x')\n    q = Quantity('q')\n    q.set_global_relative_scale_factor(S(2), meter)\n\n    expr = x * q\n    subs_expr = expr.subs(x, 5)\n\n    assert subs_expr == 5 * q\n    assert subs_expr.convert_to(centimeter) == 100 * centimeter\n", "def test_quantity_conversion_with_multiple_units():\n    expr = joule*second\n    conv = convert_to(expr, [joule, second])\n    assert conv == joule*second\n\n    expr = joule/second\n    conv = convert_to(expr, [joule, second])\n    assert conv == joule/second\n\n    expr = joule*second**2\n    conv = convert_to(expr, [joule, second])\n    assert conv == joule*second**2\n", "def test_convert_to_multiple_target_units():\n    expr = speed_of_light * coulomb\n    conv = convert_to(expr, [meter, second, coulomb])\n    assert conv == speed_of_light * meter / second * coulomb\n", "def test_issue_16055():\n    # Test conversion of units that are not directly compatible but have a common reference unit\n    # in this case, speed_of_light and atomic_mass_constant are converted to Planck units\n    expr = speed_of_light / atomic_mass_constant\n    converted_expr = convert_to(expr, [gravitational_constant, hbar])\n    expected_expr = speed_of_light / atomic_mass_constant * (hbar / gravitational_constant)**(Rational(1, 2))\n    assert converted_expr.simplify() == expected_expr.simplify()\n"], "sample_142": ["def test_has_variety():\n    assert has_variety((1, 2, 1)) is True\n    assert has_variety((1, 1, 1)) is False\n", "def test_has_variety():\n    assert has_variety((1, 2, 1))\n    assert not has_variety((1, 1, 1))\n", "def test_permutations():\n    assert list(permutations([])) == [()]\n    assert list(permutations([1])) == [(1,)]\n    assert list(permutations([1, 2])) == [(1, 2), (2, 1)]\n    assert list(permutations([1, 2, 3])) == [(1, 2, 3), (1, 3, 2), (2, 1, 3), (2, 3, 1), (3, 1, 2), (3, 2, 1)]\n    assert len(list(permutations(range(4)))) == factorial(4)\n    assert list(permutations('abc')) == [('a', 'b', 'c'), ('a', 'c', 'b'), ('b', 'a', 'c'), ('b', 'c', 'a'), ('c', 'a', 'b'), ('c', 'b', 'a')]\n", "def test_has_variety():\n    assert has_variety((1, 2, 1)) is True\n    assert has_variety((1, 1, 1)) is False\n    assert has_variety((1, 2, 3)) is True\n    assert has_variety((1,)) is False\n", "def test_multiset():\n    assert multiset([1, 2, 2]) == {1: 1, 2: 2}\n    assert multiset('abab') == {'a': 2, 'b': 2}\n    assert multiset('') == {}\n    assert multiset([]) == {}\n    assert multiset([1]) == {1: 1}\n"], "sample_105": ["def test_transform_hard_voting():\n    \"\"\"Check transform method of VotingClassifier with hard voting on toy dataset.\"\"\"\n    clf1 = LogisticRegression(random_state=123)\n    clf2 = RandomForestClassifier(random_state=123)\n    clf3 = GaussianNB()\n    X = np.array([[-1.1, -1.5], [-1.2, -1.4], [-3.4, -2.2], [1.1, 1.2]])\n    y = np.array([1, 1, 2, 2])\n\n    eclf = VotingClassifier(estimators=[\n        ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n        voting='hard').fit(X, y)\n\n    assert_array_equal(eclf.transform(X), np.array([[0, 0, 0], [0, 0, 1], [1, 1, 1], [1, 1, 1]]))\n", "def test_no_estimators():\n    \"\"\"Check that an error is raised if no estimators are provided.\"\"\"\n    with pytest.raises(ValueError, match='All estimators are None. At least one is required!'):\n        eclf = VotingClassifier(estimators=[(None, None)], voting='soft')\n        eclf.fit(X, y)\n", "def test_multiclass_weights():\n    \"\"\"Check weighted average prediction on multiclass problem.\"\"\"\n    clf1 = LogisticRegression(random_state=123, multi_class='multinomial')\n    clf2 = RandomForestClassifier(random_state=123)\n    clf3 = GaussianNB()\n    eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n                            voting='soft', weights=[1, 2, 1])\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n\n    clf1_pred = clf1.fit(X_train, y_train).predict_proba(X_test)\n    clf2_pred = clf2.fit(X_train, y_train).predict_proba(X_test)\n    clf3_pred = clf3.fit(X_train, y_train).predict_proba(X_test)\n    eclf_pred = eclf.fit(X_train, y_train).predict_proba(X_test)\n\n    avg = np.average(np.asarray([clf1_pred, clf2_pred, clf3_pred]), axis=0, weights=[1, 2, 1])\n    assert_almost_equal(eclf_pred, avg, decimal=2)\n", "def test_parallel_fit_regressor():\n    \"\"\"Check parallel backend of VotingRegressor on toy dataset.\"\"\"\n    reg1 = LinearRegression()\n    reg2 = RandomForestRegressor(n_estimators=5, random_state=123)\n    X = np.array([[-1.1, -1.5], [-1.2, -1.4], [-3.4, -2.2], [1.1, 1.2]])\n    y = np.array([1.1, 2.2, 3.3, 4.4])\n\n    ereg1 = VotingRegressor(estimators=[('lr', reg1), ('rf', reg2)], n_jobs=1).fit(X, y)\n    ereg2 = VotingRegressor(estimators=[('lr', reg1), ('rf', reg2)], n_jobs=2).fit(X, y)\n\n    assert_array_almost_equal(ereg1.predict(X), ereg2.predict(X))\n", "def test_set_weights_none():\n    \"\"\"VotingClassifier set_params should be able to set weights as None\"\"\"\n    clf1 = LogisticRegression(random_state=123)\n    clf2 = RandomForestClassifier(random_state=123)\n    eclf1 = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2)], voting='soft', weights=[1, 2]).fit(X, y)\n\n    eclf2 = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2)], voting='soft', weights=None)\n    eclf2.fit(X, y)\n\n    assert_array_equal(eclf1.predict(X), eclf2.predict(X))\n    assert_array_almost_equal(eclf1.predict_proba(X), eclf2.predict_proba(X))\n"], "sample_53": ["def test_alter_fk_to_m2m(self):\n    changes = self.get_changes(\n        [self.author_name, self.book_with_author_fk],\n        [\n            self.author_name,\n            ModelState(\n                \"otherapp\",\n                \"Book\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"author\", models.ManyToManyField(\"testapp.Author\")),\n                    (\"title\", models.CharField(max_length=200)),\n                ],\n            ),\n        ],\n    )\n    self.assertNumberMigrations(changes, \"otherapp\", 1)\n    self.assertOperationTypes(\n        changes, \"otherapp\", 0, [\"RemoveField\", \"AddField\"]\n    )\n    self.assertOperationAttributes(\n        changes, \"otherapp\", 0, 0, model_name=\"book\", name=\"author\"\n    )\n    self.assertOperationAttributes(\n        changes, \"otherapp\", 0, 1, model_name=\"book\", name=\"author\"\n    )\n", "def test_add_custom_fk_with_hardcoded_to_and_related_name(self):\n    class HardcodedForeignKey(models.ForeignKey):\n            kwargs[\"to\"] = \"testapp.Author\"\n            kwargs[\"related_name\"] = \"books\"\n            super().__init__(*args, **kwargs)\n\n            name, path, args, kwargs = super().deconstruct()\n            del kwargs[\"to\"]\n            del kwargs[\"related_name\"]\n            return name, path, args, kwargs\n\n    book_hardcoded_fk_to_and_related_name = ModelState(\n        \"testapp\",\n        \"Book\",\n        [\n            (\"author\", HardcodedForeignKey(on_delete=models.CASCADE)),\n        ],\n    )\n    changes = self.get_changes(\n        [self.author_empty],\n        [self.author_empty, book_hardcoded_fk_to_and_related_name],\n    )\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"CreateModel\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"Book\")\n    self.assertOperationFieldAttributes(changes, \"testapp\", 0, 0, related_name=\"books\")\n", "def test_add_custom_fk_with_hardcoded_pk(self):\n    class HardcodedPrimaryKey(models.AutoField):\n            kwargs[\"primary_key\"] = True\n            super().__init__(*args, **kwargs)\n\n            name, path, args, kwargs = super().deconstruct()\n            del kwargs[\"primary_key\"]\n            return name, path, args, kwargs\n\n    author_hardcoded_pk = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", HardcodedPrimaryKey()),\n            (\"name\", models.CharField(max_length=100)),\n        ],\n    )\n    changes = self.get_changes(\n        [],\n        [author_hardcoded_pk],\n    )\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"CreateModel\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"Author\")\n", "def test_add_foreignkey_to_proxy_model(self):\n    author_proxy = ModelState(\n        \"testapp\",\n        \"AuthorProxy\",\n        [],\n        {\"proxy\": True},\n        (\"testapp.Author\",),\n    )\n    book_proxy_fk = ModelState(\n        \"otherapp\",\n        \"Book\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"title\", models.CharField(max_length=200)),\n            (\"author\", models.ForeignKey(\"testapp.AuthorProxy\", models.CASCADE)),\n        ],\n    )\n    changes = self.get_changes(\n        [self.author_empty, author_proxy], [self.author_empty, author_proxy, book_proxy_fk]\n    )\n    self.assertNumberMigrations(changes, \"otherapp\", 1)\n    self.assertOperationTypes(changes, \"otherapp\", 0, [\"CreateModel\"])\n    self.assertOperationAttributes(changes, \"otherapp\", 0, 0, name=\"Book\")\n    self.assertMigrationDependencies(\n        changes, \"otherapp\", 0, [(\"testapp\", \"auto_1\")]\n    )\n", "def test_add_custom_fk_with_softcoded_to(self):\n    class SoftcodedForeignKey(models.ForeignKey):\n            kwargs[\"to\"] = \"testapp.Author\"\n            super().__init__(*args, **kwargs)\n\n            name, path, args, kwargs = super().deconstruct()\n            kwargs[\"to\"] = \"testapp.%(to)s\"\n            return name, path, args, kwargs\n\n    book_softcoded_fk_to = ModelState(\n        \"testapp\",\n        \"Book\",\n        [\n            (\"author\", SoftcodedForeignKey(on_delete=models.CASCADE)),\n        ],\n    )\n    changes = self.get_changes(\n        [self.author_empty],\n        [self.author_empty, book_softcoded_fk_to],\n    )\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"CreateModel\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"Book\")\n"], "sample_137": ["def test_has_variety():\n    assert has_variety((1, 2, 1)) is True\n    assert has_variety((1, 1, 1)) is False\n", "def test_permute_signs():\n    assert list(permute_signs((0, 1, 2))) == [(0, 1, 2), (0, -1, 2), (0, 1, -2), (0, -1, -2)]\n    assert list(permute_signs((0, 0, 0))) == [(0, 0, 0)]\n    assert list(permute_signs((1, -1, 0))) == [(1, -1, 0), (1, 1, 0), (1, -1, -0), (1, 1, -0)]\n", "def test_permute_signs():\n    assert list(permute_signs((1, 2, 3))) == [(1, 2, 3), (1, 2, -3), (1, -2, 3), (1, -2, -3), (-1, 2, 3), (-1, 2, -3), (-1, -2, 3), (-1, -2, -3)]\n    assert list(permute_signs((0, 0, 0))) == [(0, 0, 0)]\n    assert list(permute_signs((1, 0, 2))) == [(1, 0, 2), (1, 0, -2), (-1, 0, 2), (-1, 0, -2)]\n", "def test_ibin_invalid_arguments():\n    # Test ibin with invalid arguments\n    raises(ValueError, lambda: ibin(-1))\n    raises(ValueError, lambda: ibin(3, -1))\n    raises(ValueError, lambda: ibin(3, 'invalid'))\n", "def test_multiset():\n    assert multiset([1, 2, 2, 3]) == {1: 1, 2: 2, 3: 1}\n    assert multiset('aab') == {'a': 2, 'b': 1}\n    assert multiset('aab', 2) == {'a': 2}\n"], "sample_86": ["def test_logging_passing_tests_enabled_logs_test_output(testdir):\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        junit_log_passing_tests=True\n        junit_logging=system-out\n    \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n        import sys\n\n            sys.stdout.write('This is stdout')\n            sys.stderr.write('This is stderr')\n            logging.warning('hello')\n    \"\"\"\n    )\n    result, dom = runandparse(testdir)\n    assert result.ret == 0\n    node = dom.find_first_by_tag(\"testcase\")\n    systemout = node.find_first_by_tag(\"system-out\")\n    assert \"This is stdout\" in systemout.text\n    assert \"hello\" in systemout.text\n    systemerr = node.find_first_by_tag(\"system-err\")\n    assert \"This is stderr\" in systemerr.text\n", "def test_logging_passing_tests_enabled_logs_test_output(testdir):\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        junit_log_passing_tests=True\n        junit_logging=system-out\n    \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n        import sys\n\n            sys.stdout.write('This is stdout')\n            sys.stderr.write('This is stderr')\n            logging.warning('hello')\n    \"\"\"\n    )\n    result, dom = runandparse(testdir)\n    assert result.ret == 0\n    node = dom.find_first_by_tag(\"testcase\")\n    system_out = node.find_first_by_tag(\"system-out\")\n    assert \"This is stdout\" in system_out.text\n    assert \"hello\" in system_out.text\n    system_err = node.find_first_by_tag(\"system-err\")\n    assert \"This is stderr\" in system_err.text\n", "def test_logging_passing_tests_enabled_logs_test_output(testdir):\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        junit_log_passing_tests=True\n        junit_logging=system-out\n    \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n        import sys\n\n            sys.stdout.write('This is stdout')\n            sys.stderr.write('This is stderr')\n            logging.warning('hello')\n    \"\"\"\n    )\n    result, dom = runandparse(testdir)\n    assert result.ret == 0\n    node = dom.find_first_by_tag(\"testcase\")\n    systemout = node.find_first_by_tag(\"system-out\")\n    assert \"This is stdout\" in systemout.toxml()\n    systemerr = node.find_first_by_tag(\"system-err\")\n    assert \"This is stderr\" in systemerr.toxml()\n    assert \"hello\" in systemout.toxml()\n", "def test_logging_passing_tests_enabled_logs_test_output(testdir):\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        junit_log_passing_tests=True\n        junit_logging=system-out\n    \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n        import sys\n\n            sys.stdout.write('This is stdout')\n            sys.stderr.write('This is stderr')\n            logging.warning('hello')\n    \"\"\"\n    )\n    result, dom = runandparse(testdir)\n    assert result.ret == 0\n    node = dom.find_first_by_tag(\"testcase\")\n    systemout = node.find_first_by_tag(\"system-out\")\n    assert \"This is stdout\" in systemout.toxml()\n    assert \"hello\" in systemout.toxml()\n    systemerr = node.find_first_by_tag(\"system-err\")\n    assert \"This is stderr\" in systemerr.toxml()\n", "def test_record_testsuite_property_multiple_calls(testdir):\n    testdir.makepyfile(\n        \"\"\"\n            record_testsuite_property(\"stats\", \"all good\")\n            record_testsuite_property(\"info\", \"some data\")\n\n            record_testsuite_property(\"stats\", 10)\n            record_testsuite_property(\"debug\", \"verbose output\")\n    \"\"\"\n    )\n    result, dom = runandparse(testdir)\n    assert result.ret == 0\n    node = dom.find_first_by_tag(\"testsuite\")\n    properties_node = node.find_first_by_tag(\"properties\")\n    p1_node = properties_node.find_nth_by_tag(\"property\", 0)\n    p2_node = properties_node.find_nth_by_tag(\"property\", 1)\n    p3_node = properties_node.find_nth_by_tag(\"property\", 2)\n    p4_node = properties_node.find_nth_by_tag(\"property\", 3)\n    p1_node.assert_attr(name=\"stats\", value=\"all good\")\n    p2_node.assert_attr(name=\"info\", value=\"some data\")\n    p3_node.assert_attr(name=\"stats\", value=\"10\")\n    p4_node.assert_attr(name=\"debug\", value=\"verbose output\")\n"], "sample_83": ["def test_template_option_with_symbol(linter: PyLinter) -> None:\n    output = StringIO()\n    linter.reporter.out = output\n    linter.config.msg_template = '{path}:{line}:{column}:{symbol}: {msg_id}: {msg}'\n    linter.open()\n    linter.set_current_module(\"my_mod\")\n\n    linter.add_message(\"C0301\", line=1, args=(1, 2))\n    linter.add_message(\"line-too-long\", line=2, end_lineno=2, end_col_offset=4, args=(3, 4))\n\n    out_lines = output.getvalue().split(\"\\n\")\n    assert out_lines[1] == 'my_mod:1:0:missing-module-docstring: C0114: Missing module docstring'\n    assert out_lines[2] == 'my_mod:1:0:line-too-long: C0301: Line too long (1/2)'\n    assert out_lines[3] == 'my_mod:2:0:line-too-long: C0301: Line too long (3/4)'\n", "def test_colorized_text_reporter(linter):\n    output = StringIO()\n    linter.reporter = ColorizedTextReporter(output)\n    linter.open()\n    linter.set_current_module(\"color_module\")\n    linter.add_message(\"C0301\", line=1, args=(1, 2))\n    linter.add_message(\"line-too-long\", line=2, args=(3, 4))\n\n    out_lines = output.getvalue().split(\"\\n\")\n    assert \"************* Module color_module\" in out_lines[1]\n    assert \"C0301: Line too long (1/2)\" in out_lines[2]\n    assert \"C0301: Line too long (3/4)\" in out_lines[3]\n", "def test_colorized_output(linter):\n    output = StringIO()\n    linter.reporter = ColorizedTextReporter(output)\n    linter.open()\n    linter.set_current_module(\"my_module\")\n    linter.add_message(\"C0301\", line=1, args=(1, 2))\n    linter.add_message(\"line-too-long\", line=2, args=(3, 4))\n\n    # Check if the output contains ANSI escape codes for colorization\n    assert \"\\033[\" in output.getvalue()\n    assert \"\\033[0m\" in output.getvalue()  # Reset color at the end of each message\n", "def test_template_option_with_escaped_characters(linter: PyLinter) -> None:\n    output = StringIO()\n    linter.reporter.out = output\n    linter.config.msg_template = '{{ \"Message\": \"{}\" }}'.format('\\\\\"escaped\\\\\"')\n    linter.open()\n    linter.set_current_module(\"my_mod\")\n\n    linter.add_message(\"C0301\", line=1, args=(1, 2))\n    linter.add_message(\n        \"line-too-long\", line=2, end_lineno=2, end_col_offset=4, args=(3, 4)\n    )\n\n    out_lines = output.getvalue().split(\"\\n\")\n    assert out_lines[1] == '{\"Message\": \"\\\\\"escaped\\\\\"\"}'\n    assert out_lines[2] == '{\"Message\": \"\\\\\"escaped\\\\\"\"}'\n", "def test_colorized_text_reporter_custom_color_mapping(linter: PyLinter) -> None:\n    \"\"\"Test the ColorizedTextReporter with a custom color mapping.\"\"\"\n    output = StringIO()\n    custom_color_mapping = {\n        \"I\": MessageStyle(\"blue\"),\n        \"C\": MessageStyle(\"yellow\", (\"bold\",)),\n        \"R\": MessageStyle(\"cyan\", (\"bold\", \"italic\")),\n        \"W\": MessageStyle(\"magenta\"),\n        \"E\": MessageStyle(\"green\", (\"bold\",)),\n        \"F\": MessageStyle(\"red\", (\"bold\", \"underline\")),\n        \"S\": MessageStyle(\"white\", (\"inverse\",)),\n    }\n    linter.reporter = ColorizedTextReporter(output, custom_color_mapping)\n    linter.open()\n    linter.set_current_module(\"my_module\")\n    linter.add_message(\"C0301\", line=1, args=(1, 2))\n    linter.add_message(\"line-too-long\", line=2, args=(3, 4))\n\n    out_lines = output.getvalue().split(\"\\n\")\n    assert out_lines[1].startswith(\"\\033[7;37m************* Module my_module\\033[0m\")\n    assert out_lines[2].startswith(\"\\033[32;1mC0301: Line too long (1/2) (line-too-long)\\033[0m\")\n    assert out_lines[3].startswith(\"\\033[32;1mC0301: Line too long (3/4) (line-too-long)\\033[0m\")\n"], "sample_7": ["def test_watched_files_contains_extra_files(self):\n    self.reloader.watch_file(self.existing_file)\n    watched_files = list(self.reloader.watched_files())\n    self.assertIn(self.existing_file, watched_files)\n", "    def setUp(self):\n        self._tempdir = tempfile.TemporaryDirectory()\n        self.tempdir = Path(self._tempdir.name).resolve().absolute()\n        self.reloader = autoreload.BaseReloader()\n", "def test_nonexistent_file(self, mocked_modules, notify_mock):\n    self.reloader.watch_file(self.nonexistent_file)\n    with self.tick_twice():\n        self.ensure_file(self.nonexistent_file)  # Create the file after watching it\n    self.assertEqual(notify_mock.call_count, 1)\n    self.assertCountEqual(notify_mock.call_args[0], [self.nonexistent_file])\n", "def test_watch_dir_with_non_existing(self):\n    with mock.patch.object(self.reloader, '_subscribe') as mocked_subscribe:\n        with self.assertLogs('django.utils.autoreload', level='WARNING') as cm:\n            self.reloader._watch_dir(self.tempdir / 'does_not_exist', ['*'])\n        self.assertFalse(mocked_subscribe.called)\n        self.assertEqual(cm.output, ['WARNING:django.utils.autoreload:Unable to watch directory {} as neither it or its parent exist.'.format(self.tempdir / 'does_not_exist')])\n", "    def test_snapshot_files_with_non_existing_file(self):\n        non_existing_file = self.tempdir / 'does_not_exist.py'\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file, non_existing_file]):\n            snapshot = list(self.reloader.snapshot_files())\n            self.assertEqual(len(snapshot), 1)\n            self.assertEqual(snapshot[0][0], self.existing_file)\n"], "sample_22": ["def test_camel_case_to_spaces(self):\n    self.assertEqual(text.camel_case_to_spaces('CamelCaseExample'), 'camel case example')\n    self.assertEqual(text.camel_case_to_spaces('AnotherExample'), 'another example')\n    self.assertEqual(text.camel_case_to_spaces('LongExampleName'), 'long example name')\n    self.assertEqual(text.camel_case_to_spaces('NoChange'), 'no change')\n    self.assertEqual(text.camel_case_to_spaces(''), '')\n    self.assertEqual(text.camel_case_to_spaces(lazystr('CamelCaseExample')), 'camel case example')\n", "def test_camel_case_to_spaces(self):\n    self.assertEqual(text.camel_case_to_spaces('CamelCaseExample'), 'camel case example')\n    self.assertEqual(text.camel_case_to_spaces('AnotherExample'), 'another example')\n    self.assertEqual(text.camel_case_to_spaces('LazyStringExample'), 'lazy string example')\n    self.assertEqual(text.camel_case_to_spaces(lazystr('LazyStringExample')), 'lazy string example')\n", "def test_camel_case_to_spaces(self):\n    items = [\n        ('CamelCase', 'camel case'),\n        ('CamelCaseExample', 'camel case example'),\n        ('HTML', 'html'),\n        ('XMLHTTPRequest', 'xml http request'),\n        ('AString', 'a string'),\n        ('SimpleString', 'simple string'),\n        ('ALongStringWithoutSpaces', 'a long string without spaces'),\n        ('StringWithNoCapitals', 'string with no capitals'),\n        ('StringWithCAPITALS', 'string with capital s'),\n    ]\n    for value, output in items:\n        self.assertEqual(text.camel_case_to_spaces(value), output)\n        self.assertEqual(text.camel_case_to_spaces(lazystr(value)), output)\n", "    def test_camel_case_to_spaces(self):\n        self.assertEqual('this is a test', text.camel_case_to_spaces('ThisIsATest'))\n        self.assertEqual('this is a test', text.camel_case_to_spaces('thisIsATest'))\n        self.assertEqual('this is a test', text.camel_case_to_spaces('THISIsATest'))\n        self.assertEqual('this is a test', text.camel_case_to_spaces('thisIsaTest'))\n        self.assertEqual('this is a test', text.camel_case_to_spaces('thisisatest'))\n        self.assertEqual('this is a test', text.camel_case_to_spaces('ThisIsATestCase'))\n        self.assertEqual('lazystr test', text.camel_case_to_spaces('LazyStrTest'))\n        self.assertEqual('lazy str test', text.camel_case_to_spaces('LazyStrTest'))\n", "    def test_camel_case_to_spaces(self):\n        self.assertEqual(text.camel_case_to_spaces('CamelCaseExample'), 'camel case example')\n        self.assertEqual(text.camel_case_to_spaces('AnotherExample'), 'another example')\n        self.assertEqual(text.camel_case_to_spaces('SingleWord'), 'single word')\n        self.assertEqual(text.camel_case_to_spaces(''), '')\n        self.assertEqual(text.camel_case_to_spaces('lowercaseexample'), 'lowercaseexample')\n        self.assertEqual(text.camel_case_to_spaces('123NumberExample'), '123 number example')\n        self.assertEqual(text.camel_case_to_spaces('    LeadingSpacesExample   '), 'leading spaces example')\n"], "sample_72": ["def test_toolmanager_remove_all():\n    with pytest.warns(UserWarning, match=_EXPECTED_WARNING_TOOLMANAGER):\n        plt.rcParams['toolbar'] = 'toolmanager'\n    fig = plt.gcf()\n    initial_len = len(fig.canvas.manager.toolmanager.tools)\n    fig.canvas.manager.toolmanager.remove_all()\n    assert len(fig.canvas.manager.toolmanager.tools) == 0\n", "def test_widgetlock_zoompan_toolbar():\n    fig, ax = plt.subplots()\n    ax.plot([0, 1], [0, 1])\n    fig.canvas.widgetlock(ax)\n    tb = NavigationToolbar2(fig.canvas)\n    tb.zoom()\n    assert ax.get_navigate_mode() is None\n    tb.pan()\n    assert ax.get_navigate_mode() is None\n\n    with pytest.warns(UserWarning, match=_EXPECTED_WARNING_TOOLMANAGER):\n        plt.rcParams['toolbar'] = 'toolmanager'\n    fig = plt.gcf()\n    fig.canvas.manager.toolmanager.trigger_tool('zoom')\n    assert ax.get_navigate_mode() is None\n    fig.canvas.manager.toolmanager.trigger_tool('pan')\n    assert ax.get_navigate_mode() is None\n", "def test_savefig(backend):\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.plot([1, 2, 3], [1, 2, 3])\n    test_backend = pytest.importorskip(\n        f'matplotlib.backends.backend_{backend}'\n    )\n    with tempfile.NamedTemporaryFile(suffix='.' + backend) as f:\n        fig.savefig(f.name)\n        assert os.path.getsize(f.name) > 0\n", "def test_figaspect():\n    # Test figaspect with a float\n    w, h = figaspect(2.)\n    fig = Figure(figsize=(w, h))\n    assert fig.get_figwidth() == w\n    assert fig.get_figheight() == h\n\n    # Test figaspect with an array\n    A = np.random.rand(5, 3)\n    w, h = figaspect(A)\n    fig = Figure(figsize=(w, h))\n    assert fig.get_figwidth() == w\n    assert fig.get_figheight() == h\n\n    # Test figaspect with an array that has the same number of rows and columns\n    A = np.random.rand(5, 5)\n    w, h = figaspect(A)\n    fig = Figure(figsize=(w, h))\n    assert fig.get_figwidth() == fig.get_figheight()\n", "def test_toolmanager_replace_tool():\n    with pytest.warns(UserWarning, match=_EXPECTED_WARNING_TOOLMANAGER):\n        plt.rcParams['toolbar'] = 'toolmanager'\n    fig = plt.gcf()\n    initial_len = len(fig.canvas.manager.toolmanager.tools)\n    assert 'pan' in fig.canvas.manager.toolmanager.tools\n    fig.canvas.manager.toolmanager.replace_tool('pan', None)\n    assert len(fig.canvas.manager.toolmanager.tools) == initial_len - 1\n    assert 'pan' not in fig.canvas.manager.toolmanager.tools\n    with pytest.raises(ValueError, match=\"Cannot replace tool 'foo' as it does not exist\"):\n        fig.canvas.manager.toolmanager.replace_tool('foo', 'pan')\n"], "sample_150": ["def test_solve_poly_system_mixed_fractions():\n    x, y = symbols('x y')\n    assert solve_poly_system([x - 1.5, y - 2/3], x, y) == [(1.5, 2/3)]\n", "def test_solve_poly_system_with_rational_coefficients():\n    x, y = symbols('x y')\n    system = [x + Rational(1, 2)*y - Rational(1, 3), Rational(3, 4)*x - y + Rational(2, 5)]\n    solution = solve_poly_system(system, x, y)\n    assert solution == [(Rational(11, 35), Rational(7, 35))]\n", "def test_solve_poly_system_rational_functions():\n    f_1 = x / y - 2\n    f_2 = y / x - 3\n\n    assert solve_poly_system([f_1, f_2], x, y) == [(2, 1)]\n\n    f_1 = x / y - 2\n    f_2 = x - 3 * y\n\n    assert solve_poly_system([f_1, f_2], x, y) == [(3, 1), (6, 2)]\n", "def test_solve_poly_system_with_rational_coefficients():\n    x, y = symbols('x y')\n\n    # Test system with rational coefficients\n    f_1 = 3*x - 2*y + Rational(1, 2)\n    f_2 = 2*x + Rational(3, 2)*y - 1\n\n    solution = solve_poly_system([f_1, f_2], x, y)\n    assert solution == [(Rational(7, 13), Rational(4, 13))]\n", "def test_solve_poly_system_numeric():\n    x0, y0 = symbols('x0 y0', real=True)\n    f_1 = (x - x0)**2 + (y - y0)**2 - r**2\n    f_2 = (x - x1)**2 + (y - y1)**2 - r**2\n    result = solve_poly_system([f_1, f_2], x, y, domain=QQ.algebraic_field(I))\n    assert all(all(s.is_real for s in sol) for sol in result)\n"], "sample_40": ["def test_field_deep_copy_widget(self):\n    class CustomWidget(TextInput):\n        pass\n\n    field = CharField(widget=CustomWidget())\n    field_copy = copy.deepcopy(field)\n    self.assertIsInstance(field_copy, CharField)\n    self.assertIsNot(field_copy.widget, field.widget)\n", "def test_field_deep_copy_validators(self):\n        if value % 2 != 0:\n            raise ValidationError(\"This value must be even.\")\n\n    class CustomCharField(CharField):\n            kwargs['validators'] = [validate_even]\n            super().__init__(**kwargs)\n\n    field = CustomCharField()\n    field_copy = copy.deepcopy(field)\n    self.assertIsInstance(field_copy, CustomCharField)\n    self.assertIsNot(field_copy.validators, field.validators)\n", "def test_default_renderer_override(self):\n    class CustomForm(Form):\n        default_renderer = CustomRenderer()\n        name = CharField(max_length=10)\n\n    form = CustomForm()\n    self.assertIsInstance(form.renderer, CustomRenderer)\n", "def test_field_deep_copy_validators(self):\n        if value % 2 != 1:\n            raise ValidationError('Value must be odd.')\n\n    field = CharField(validators=[validate_odd])\n    field_copy = copy.deepcopy(field)\n    self.assertIsInstance(field_copy, CharField)\n    self.assertIsNot(field_copy.validators, field.validators)\n", "def test_required_attribute_for_non_text_input_types(self):\n    class NonTextInputForm(Form):\n        checkbox = BooleanField(required=True)\n        radio = ChoiceField(choices=[('P', 'Python'), ('J', 'Java')], widget=RadioSelect, required=True)\n        checkbox_multiple = MultipleChoiceField(choices=[('P', 'Python'), ('J', 'Java')], required=True)\n\n    form = NonTextInputForm()\n    self.assertHTMLEqual(\n        form.as_p(),\n        '<p><label for=\"id_checkbox\">Checkbox:</label> '\n        '<input id=\"id_checkbox\" name=\"checkbox\" required type=\"checkbox\"></p>'\n        '<p><label for=\"id_radio_0\">Radio:</label> '\n        '<input id=\"id_radio_0\" name=\"radio\" required type=\"radio\" value=\"P\"> '\n        '<label for=\"id_radio_0\">Python</label> '\n        '<input id=\"id_radio_1\" name=\"radio\" required type=\"radio\" value=\"J\"> '\n        '<label for=\"id_radio_1\">Java</label></p>'\n        '<p><label>Checkbox multiple:</label> '\n        '<input id=\"id_checkbox_multiple_0\" name=\"checkbox_multiple\" required type=\"checkbox\" value=\"P\"> '\n        '<label for=\"id_checkbox_multiple_0\">Python</label> '\n        '<input id=\"id_checkbox_multiple_1\" name=\"checkbox_multiple\" required type=\"checkbox\" value=\"J\"> '\n        '<label for=\"id_checkbox_multiple_1\">Java</label></p>',\n    )\n"], "sample_155": ["def test_physical_constant_conversion():\n    from sympy.physics.units import meter, joule, second, newton, kilogram\n\n    assert gravitational_constant.convert_to(newton * meter**2 / kilogram**2) == 6.67430e-11 * newton * meter**2 / kilogram**2\n    assert speed_of_light.convert_to(meter / second) == 299792458 * meter / second\n", "def test_physical_constant_values():\n    from sympy import N\n\n    # Test that the values of physical constants are correct\n    assert N(gravitational_constant.scale_factor, 4) == 6.674e-11\n    assert N(molar_gas_constant.scale_factor, 4) == 8.3145\n    assert N(vacuum_permittivity.scale_factor, 4) == 8.854e-12\n    assert N(speed_of_light.scale_factor, 4) == 299792458\n    assert N(elementary_charge.scale_factor, 4) == 1.602e-19\n", "def test_physical_constant_units():\n    from sympy.physics.units.definitions import gravitational_constant, speed_of_light, elementary_charge\n\n    # Test unit consistency for physical constants\n    assert gravitational_constant.dimension == mass * length**-2 * time**-2\n    assert speed_of_light.dimension == length / time\n    assert elementary_charge.dimension == charge\n\n    # Test value consistency for physical constants\n    assert gravitational_constant.scale_factor == 6.67430e-11 * meter**3 * kilogram**-1 * second**-2\n    assert speed_of_light.scale_factor == 299792458 * meter / second\n    assert elementary_charge.scale_factor == 1.602176634e-19 * coulomb\n", "def test_units_with_physical_constants():\n    assert convert_to(elementary_charge/volt, coulomb) == 1.602176634e-19\n    assert convert_to(volt/elementary_charge, 1/coulomb) == 6.241509074460763e18\n    assert convert_to(ohm*coulomb, volt*second) == 6.283185307179586e18*second\n", "def test_prefixed_quantity_dimensions():\n    from sympy.physics.units.systems import SI\n\n    km_dim = SI.get_quantity_dimension(kilometer)\n    assert km_dim == length\n\n    kg_dim = SI.get_quantity_dimension(kilogram)\n    assert kg_dim == mass\n\n    s_dim = SI.get_quantity_dimension(second)\n    assert s_dim == time\n\n    kJ_dim = SI.get_quantity_dimension(kilo*joule)\n    assert kJ_dim == energy\n"], "sample_21": ["def test_fast_delete_cascade_relationships(self):\n    r = R.objects.create()\n    s = S.objects.create(r=r)\n    T.objects.create(s=s)\n    # 1 to delete r, 1 to fast-delete s, 1 to fast-delete T for s\n    self.assertNumQueries(3, r.delete)\n    self.assertFalse(R.objects.exists())\n    self.assertFalse(S.objects.exists())\n    self.assertFalse(T.objects.exists())\n", "def test_fast_delete_combined_relationships_with_gfk(self):\n    # The cascading fast-delete of GenericDeleteBottom should be combined\n    # in a single DELETE WHERE generic_b1_id OR generic_b2_id.\n    delete_top = DeleteTop.objects.create()\n    generic_b1 = GenericB1.objects.create(generic_delete_top=delete_top)\n    generic_b2 = GenericB2.objects.create(generic_delete_top=delete_top)\n    GenericDeleteBottom.objects.create(generic_b1=generic_b1, generic_b2=generic_b2)\n    with self.assertNumQueries(2):\n        generic_b1.delete()\n    self.assertFalse(DeleteTop.objects.exists())\n    self.assertFalse(GenericB1.objects.exists())\n    self.assertFalse(GenericB2.objects.exists())\n    self.assertFalse(GenericDeleteBottom.objects.exists())\n", "def test_fast_delete_combined_relationships_with_multiple_objects(self):\n    # The cascading fast-delete of SecondReferrer should be combined\n    # in a single DELETE WHERE referrer_id OR unique_field even if multiple\n    # objects are involved.\n    origin1 = Origin.objects.create()\n    origin2 = Origin.objects.create()\n    referer1 = Referrer.objects.create(origin=origin1, unique_field=42)\n    referer2 = Referrer.objects.create(origin=origin2, unique_field=43)\n    with self.assertNumQueries(2):\n        Referrer.objects.filter(unique_field__in=[42, 43]).delete()\n", "def test_fast_delete_gfk(self):\n    delete_top = DeleteTop.objects.create()\n    generic_b1 = GenericB1.objects.create(generic_delete_top=delete_top)\n    self.assertTrue(Collector(using='default').can_fast_delete(generic_b1))\n    generic_b1.delete()\n    self.assertFalse(GenericB1.objects.exists())\n    self.assertTrue(DeleteTop.objects.exists())\n", "def test_fast_delete_on_delete_signal(self):\n    # Test that a fast delete is not performed if a pre_delete signal is connected.\n    # This is to ensure that the signal is actually called.\n    calls = []\n\n        calls.append('')\n\n    models.signals.pre_delete.connect(noop, sender=Avatar)\n\n    u = User.objects.create(\n        avatar=Avatar.objects.create()\n    )\n    a = Avatar.objects.get(pk=u.avatar_id)\n    # 1 query to find the users for the avatar.\n    # 1 query to delete the user\n    # 1 query to delete the avatar\n    self.assertNumQueries(3, a.delete)\n    self.assertFalse(User.objects.exists())\n    self.assertFalse(Avatar.objects.exists())\n    self.assertEqual(len(calls), 1)\n\n    models.signals.pre_delete.disconnect(noop, sender=Avatar)\n"], "sample_71": ["def test_context_after_reset():\n    original_value = 'gray'\n    other_value = 'blue'\n    mpl.rcParams[PARAM] = original_value\n    with temp_style('test', DUMMY_SETTINGS):\n        with style.context('test', after_reset=True):\n            assert mpl.rcParams[PARAM] == VALUE\n    assert mpl.rcParams[PARAM] == original_value\n", "def test_context_with_after_reset():\n    original_value = 'gray'\n    other_value = 'blue'\n    mpl.rcParams[PARAM] = original_value\n    with temp_style('test', DUMMY_SETTINGS):\n        with style.context('test', after_reset=True):\n            assert mpl.rcParams[PARAM] == VALUE\n        assert mpl.rcParams[PARAM] == original_value\n", "def test_context_after_reset():\n    original_value = 'gray'\n    other_value = 'blue'\n    mpl.rcParams[PARAM] = original_value\n    with temp_style('test', DUMMY_SETTINGS):\n        with style.context('test', after_reset=True):\n            assert mpl.rcParams[PARAM] == VALUE\n        # Check that this value is reset after the exiting the context.\n        assert mpl.rcParams[PARAM] == original_value\n\n    mpl.rcParams[PARAM] = other_value\n    with style.context('test', after_reset=True):\n        assert mpl.rcParams[PARAM] == VALUE\n    assert mpl.rcParams[PARAM] == original_value\n", "def test_context_with_after_reset():\n    # Test dict after style name with after_reset=True.\n    original_value = 'gray'\n    other_value = 'blue'\n    mpl.rcParams[PARAM] = original_value\n    with temp_style('test', DUMMY_SETTINGS):\n        with style.context('test', after_reset=True):\n            assert mpl.rcParams[PARAM] == VALUE\n        assert mpl.rcParams[PARAM] == original_value\n", "def test_context_after_reset():\n    original_value = 'gray'\n    other_value = 'blue'\n    mpl.rcParams[PARAM] = original_value\n    with temp_style('test', DUMMY_SETTINGS):\n        with style.context('test', after_reset=True):\n            assert mpl.rcParams[PARAM] == VALUE\n    # Check that this value is not reset after the exiting the context.\n    assert mpl.rcParams[PARAM] == VALUE\n"], "sample_10": ["def test_exact_query_rhs_with_unselected_columns(self):\n    newest_author = Author.objects.create(name='Author 3')\n    authors_max_ids = Author.objects.filter(\n        name='Author 3',\n    ).values(\n        'name',\n    ).annotate(\n        max_id=Max('id'),\n    ).values('max_id')\n    authors = Author.objects.filter(id=authors_max_ids[:1].values('max_id'))\n    self.assertEqual(authors.get(), newest_author)\n", "def test_exact_query_rhs_with_selected_columns_and_ordering(self):\n    newest_author = Author.objects.create(name='Author 3')\n    authors_max_ids = Author.objects.filter(\n        name='Author 3',\n    ).values(\n        'name',\n    ).annotate(\n        max_id=Max('id'),\n    ).values('max_id').order_by('-max_id')\n    authors = Author.objects.filter(id=authors_max_ids[:1])\n    self.assertEqual(authors.get(), newest_author)\n", "def test_exact_query_rhs_with_transform(self):\n    season = Season.objects.create(year=2012, transformed_field='Test')\n    query = Season.objects.get_queryset().query\n    field = query.model._meta.get_field('transformed_field')\n    self.assertEqual(\n        str(query.build_lookup(['exact'], field, 'test')),\n        \"'season'.'transformed_field' = UPPER('test')\"\n    )\n    self.assertTrue(Season.objects.filter(pk=season.pk, transformed_field__exact='Test'))\n    self.assertTrue(Season.objects.filter(pk=season.pk, transformed_field='Test'))\n", "def test_exact_lookup_with_queryset_rhs(self):\n    # Create additional authors and articles for testing.\n    au3 = Author.objects.create(name='Author 3', alias='a3')\n    a8 = Article.objects.create(headline='Article 8', pub_date=datetime(2006, 1, 1), author=au3, slug='a8')\n\n    # Test exact lookup with QuerySet as RHS.\n    authors_with_article_8 = Author.objects.filter(article__headline='Article 8')\n    articles = Article.objects.filter(author__in=authors_with_article_8)\n    self.assertCountEqual(articles, [a8])\n", "def test_year_lookups(self):\n    # Test year lookups\n    self.assertQuerysetEqual(\n        Season.objects.filter(year__exact=2010),\n        ['<Season: 2010>']\n    )\n    self.assertQuerysetEqual(\n        Season.objects.filter(year__gt=2010),\n        ['<Season: 2011>']\n    )\n    self.assertQuerysetEqual(\n        Season.objects.filter(year__gte=2010),\n        ['<Season: 2010>', '<Season: 2011>']\n    )\n    self.assertQuerysetEqual(\n        Season.objects.filter(year__lt=2010),\n        ['<Season: 2009>']\n    )\n    self.assertQuerysetEqual(\n        Season.objects.filter(year__lte=2010),\n        ['<Season: 2009>', '<Season: 2010>']\n    )\n    self.assertQuerysetEqual(\n        Season.objects.filter(year__range=(2009, 2011)),\n        ['<Season: 2009>', '<Season: 2010>', '<Season: 2011>']\n    )\n"], "sample_25": ["def test_add_non_blank_textfield_without_default(self, mocked_ask_method):\n    \"\"\"\n    #23405 - Adding a NOT NULL and non-blank `CharField` or `TextField`\n    without default should prompt for a default.\n    \"\"\"\n    changes = self.get_changes([self.author_empty], [self.author_with_biography_non_blank_no_default])\n    self.assertEqual(mocked_ask_method.call_count, 2)\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AddField\", \"AddField\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0)\n", "    def test_add_model_with_field_added_to_base_model(self):\n        \"\"\"\n        Adding a new field to a base model takes place before adding a new\n        inherited model.\n        \"\"\"\n        before = [\n            ModelState('app', 'readable', [\n                ('id', models.AutoField(primary_key=True)),\n            ]),\n        ]\n        after = [\n            ModelState('app', 'readable', [\n                ('id', models.AutoField(primary_key=True)),\n                ('title', models.CharField(max_length=200)),\n            ]),\n            ModelState('app', 'book', [], bases=('app.readable',)),\n        ]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['AddField', 'CreateModel'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='title', model_name='readable')\n        self.assertOperationAttributes(changes, 'app', 0, 1, name='book')\n", "    def test_alter_field_with_custom_deconstruct(self):\n        class CustomDeconstruct(str):\n                return (CustomDeconstruct, [], {'arg': 'value'})\n\n        field_before = models.CharField(max_length=200, default=CustomDeconstruct('default'))\n        field_after = models.CharField(max_length=200, default=CustomDeconstruct('new_default'))\n        before = [ModelState('testapp', 'Model', [('id', models.AutoField(primary_key=True)), ('field', field_before)])]\n        after = [ModelState('testapp', 'Model', [('id', models.AutoField(primary_key=True)), ('field', field_after)])]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, ['AlterField'])\n        self.assertOperationFieldAttributes(changes, 'testapp', 0, 0, default=CustomDeconstruct('new_default'))\n", "def test_multiple_create_models(self):\n    class Migration(migrations.Migration):\n        operations = [\n            migrations.CreateModel('Person', fields=[('name', models.CharField(max_length=100))]),\n            migrations.CreateModel('Animal', fields=[('name', models.CharField(max_length=100))]),\n            migrations.CreateModel('Plant', fields=[('name', models.CharField(max_length=100))]),\n        ]\n\n    migration = Migration('0001_initial', 'test_app')\n    self.assertEqual(migration.suggest_name(), 'animal_person_plant')\n", "    def test_remove_model_with_field_used_by_inherited_model(self):\n        \"\"\"\n        Removing a model that has a field used by an inherited model takes place\n        after removing the inherited model.\n        \"\"\"\n        before = [\n            ModelState('app', 'readable', [\n                ('id', models.AutoField(primary_key=True)),\n                ('title', models.CharField(max_length=200)),\n            ]),\n            ModelState('app', 'book', [\n                ('title', models.CharField(max_length=200)),\n            ], bases=('app.readable',)),\n        ]\n        after = [\n            ModelState('app', 'readable', [\n                ('id', models.AutoField(primary_key=True)),\n            ]),\n        ]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['DeleteModel', 'RemoveField'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='book')\n        self.assertOperationAttributes(changes, 'app', 0, 1, name='title', model_name='readable')\n"], "sample_9": ["def test_file_deleted(self, mocked_modules, notify_mock):\n    self.reloader.watch_file(self.existing_file)\n    with self.tick_twice():\n        self.existing_file.unlink()\n    self.assertEqual(notify_mock.call_count, 1)\n    self.assertCountEqual(notify_mock.call_args[0], [self.existing_file])\n", "def test_watch_dir_without_absolute(self):\n    with self.assertRaisesMessage(ValueError, 'test must be absolute.'):\n        self.reloader.watch_dir('test', '*.py')\n", "def test_nonexistent_file_ignored(self, mocked_modules, notify_mock):\n    self.reloader.watch_file(self.nonexistent_file)\n    with self.tick_twice():\n        pass\n    self.assertEqual(notify_mock.call_count, 0)\n", "def test_file_does_not_exist(self, mocked_modules, notify_mock):\n    nonexistent_file = self.tempdir / 'does_not_exist.py'\n    self.reloader.watch_file(nonexistent_file)\n    with self.tick_twice():\n        pass\n    self.assertEqual(notify_mock.call_count, 0)\n", "def test_snapshot_files_with_exception(self, mock_notify_file_changed):\n        raise OSError\n\n    with mock.patch.object(Path, 'stat', side_effect=mock_stat):\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file]):\n            snapshot = list(self.reloader.snapshot_files())\n            self.assertEqual(len(snapshot), 0)\n"], "sample_96": ["def test_ridge_classifier_cv_no_support_multilabel():\n    X, y = make_multilabel_classification(n_samples=10, random_state=0)\n    assert_raises(ValueError, RidgeClassifierCV().fit, X, y)\n", "def test_ridge_classifier_cv_with_sample_weight():\n    X, y = make_classification(n_samples=100, n_features=20, n_classes=2, random_state=42)\n    sample_weight = np.random.RandomState(42).rand(100)\n\n    clf = RidgeClassifierCV(alphas=[0.1, 1.0, 10.0])\n    clf.fit(X, y, sample_weight=sample_weight)\n\n    assert_true(hasattr(clf, \"coef_\"))\n    assert_true(hasattr(clf, \"intercept_\"))\n    assert_true(hasattr(clf, \"alpha_\"))\n", "def test_ridge_predict_input_shape():\n    # Test that Ridge.predict handles different input shapes correctly\n    rng = np.random.RandomState(0)\n    alpha = 1.0\n\n    n_samples, n_features = 6, 5\n    X_train = rng.randn(n_samples, n_features)\n    y_train = rng.randn(n_samples)\n\n    ridge = Ridge(alpha=alpha, solver='cholesky')\n    ridge.fit(X_train, y_train)\n\n    # Test with 1D input\n    X_test_1d = rng.randn(n_features)\n    y_pred_1d = ridge.predict(X_test_1d)\n    assert_equal(y_pred_1d.shape, ())\n\n    # Test with 2D input\n    X_test_2d = rng.randn(2, n_features)\n    y_pred_2d = ridge.predict(X_test_2d)\n    assert_equal(y_pred_2d.shape, (2,))\n\n    # Test with 3D input\n    X_test_3d = rng.randn(2, 3, n_features)\n    assert_raises(ValueError, ridge.predict, X_test_3d)\n", "def test_ridge_svd_single_alpha():\n    # Test that the SVD solver works with a single alpha value\n    rng = np.random.RandomState(0)\n    alpha = 1.0\n    n_samples, n_features = 6, 5\n    X = rng.randn(n_samples, n_features)\n    y = rng.randn(n_samples)\n\n    ridge_svd = Ridge(alpha=alpha, solver='svd')\n    ridge_svd.fit(X, y)\n\n    ridge_cholesky = Ridge(alpha=alpha, solver='cholesky')\n    ridge_cholesky.fit(X, y)\n\n    assert_array_almost_equal(ridge_svd.coef_, ridge_cholesky.coef_, decimal=5)\n", "def test_ridge_classifier_cv_store_cv_values_with_cv_none():\n    X = np.array([[-1.0, -1.0], [-1.0, 0], [-.8, -1.0],\n                  [1.0, 1.0], [1.0, 0.0]])\n    y = np.array([1, 1, 1, -1, -1])\n\n    alphas = [1e-1, 1e0, 1e1]\n    n_alphas = len(alphas)\n\n    r = RidgeClassifierCV(alphas=alphas, store_cv_values=True, cv=None)\n\n    n_samples = X.shape[0]\n    n_targets = 1\n    r.fit(X, y)\n    assert r.cv_values_.shape == (n_samples, n_targets, n_alphas)\n"], "sample_94": ["def test_getstatementrange_with_continuation_character() -> None:\n    source = Source(\n        \"\"\"\\\n        x = (1 +\n             2)\n        \"\"\"\n    )\n    assert source.getstatementrange(1) == (0, 2)\n", "def test_getstatementrange_bug3():\n    source = Source(\n        \"\"\"\\\n        try:\n            x = (\n               y +\n               z)\n            y = x\n        except:\n            pass\n    \"\"\"\n    )\n    assert len(source) == 6\n    assert source.getstatementrange(2) == (1, 3)\n", "def test_getstatementrange_within_functions() -> None:\n    source = Source(\n        \"\"\"\\\n                raise ValueError\n\n            try:\n                inner_func()\n            except ValueError:\n                pass\n        \"\"\"\n    )\n    assert source.getstatementrange(3) == (2, 3)\n    assert source.getstatementrange(4) == (4, 6)\n", "def test_getstatementrange_with_comments_and_empty_lines():\n    source = Source(\n        \"\"\"\\\n        # Comment 1\n\n            # Comment 2\n            x = 3\n\n            # Comment 3\n            y = 4\n            # Comment 4\n\n        # Comment 5\n        \"\"\"\n    )\n    assert source.getstatementrange(4) == (2, 6)\n    assert source.getstatementrange(5) == (6, 7)\n", "def test_source_indent():\n    source = Source(\"hello\")\n    indented_source = source.indent()\n    assert str(indented_source) == \"    hello\"\n"], "sample_0": ["def test_value_from_datadict(self):\n    rel = Album._meta.get_field('band').remote_field\n    w = AutocompleteSelect(rel, admin.site)\n    data = {'name': '1'}\n    value = w.value_from_datadict(data, {}, 'name')\n    self.assertEqual(value, '1')\n", "def test_build_attrs_with_custom_attrs(self):\n    form = AlbumForm()\n    attrs = form['band'].field.widget.get_context(name='my_field', value=None, attrs={'custom-attr': 'custom-value'})['widget']['attrs']\n    self.assertEqual(attrs['custom-attr'], 'custom-value')\n    self.assertEqual(attrs['class'], 'my-class admin-autocomplete')\n", "def test_render_options_localized(self):\n    \"\"\"Test rendering options in a different language.\"\"\"\n    beatles = Band.objects.create(name='The Beatles', style='rock')\n    who = Band.objects.create(name='The Who', style='rock')\n    form = AlbumForm(initial={'band': beatles.pk})\n    with translation.override('de'):\n        output = form.as_table()\n    selected_option = '<option value=\"%s\" selected>The Beatles</option>' % beatles.pk\n    option = '<option value=\"%s\">The Who</option>' % who.pk\n    self.assertIn(selected_option, output)\n    self.assertNotIn(option, output)\n", "def test_build_attrs_additional_attributes(self):\n    form = AlbumForm()\n    attrs = form['band'].field.widget.build_attrs({'data-custom': 'value'})\n    self.assertEqual(attrs['data-custom'], 'value')\n    self.assertEqual(attrs['class'], 'my-class admin-autocomplete')\n", "    def test_build_attrs_custom_placeholder(self):\n        form = AlbumForm()\n        attrs = form['band'].field.widget.build_attrs({'data-placeholder': 'Select a band'})\n        self.assertEqual(attrs['data-placeholder'], 'Select a band')\n"], "sample_27": ["def test_token_with_changed_password(self):\n    \"\"\"Updating the user password invalidates the token.\"\"\"\n    user = User.objects.create_user('changepassworduser', 'test5@example.com', 'testpw')\n    p0 = PasswordResetTokenGenerator()\n    tk1 = p0.make_token(user)\n    self.assertIs(p0.check_token(user, tk1), True)\n    user.set_password('newtestpw')\n    user.save()\n    self.assertIs(p0.check_token(user, tk1), False)\n", "def test_token_with_different_password(self):\n    \"\"\"Updating the user password invalidates the token.\"\"\"\n    user = User.objects.create_user('changepassworduser', 'test5@example.com', 'testpw')\n    p0 = PasswordResetTokenGenerator()\n    tk1 = p0.make_token(user)\n    self.assertIs(p0.check_token(user, tk1), True)\n    user.set_password('newpassword')\n    user.save()\n    self.assertIs(p0.check_token(user, tk1), False)\n", "def test_token_with_changed_password(self):\n    \"\"\"Updating the user password invalidates the token.\"\"\"\n    user = User.objects.create_user('changepassworduser', 'test5@example.com', 'testpw')\n    p0 = PasswordResetTokenGenerator()\n    tk1 = p0.make_token(user)\n    self.assertIs(p0.check_token(user, tk1), True)\n    user.set_password('newtestpw')\n    user.save()\n    self.assertIs(p0.check_token(user, tk1), False)\n", "def test_token_with_changed_password(self):\n    \"\"\"Updating the user password invalidates the token.\"\"\"\n    user = User.objects.create_user('changepassworduser', 'test5@example.com', 'testpw')\n    p0 = PasswordResetTokenGenerator()\n    tk1 = p0.make_token(user)\n    self.assertIs(p0.check_token(user, tk1), True)\n    user.set_password('newpassword')\n    user.save()\n    self.assertIs(p0.check_token(user, tk1), False)\n", "def test_token_with_changed_password(self):\n    \"\"\"Updating the user password invalidates the token.\"\"\"\n    user = User.objects.create_user('changepassworduser', 'test5@example.com', 'testpw')\n    p0 = PasswordResetTokenGenerator()\n    tk1 = p0.make_token(user)\n    self.assertIs(p0.check_token(user, tk1), True)\n    user.set_password('newtestpw')\n    user.save()\n    self.assertIs(p0.check_token(user, tk1), False)\n"], "sample_145": ["def test_printing_latex_array_symbols():\n    A = ArraySymbol(\"A\", 2, 3, 4)\n    B = ArraySymbol(\"B\", (2, 3), (4, 5))\n    assert latex(A) == \"A\"\n    assert latex(B) == \"B\"\n", "def test_printing_latex_array_slices():\n    A = ArraySymbol(\"A\", 3, 3, 3)\n    assert latex(ArraySlice(A, (0, None, None), (0, None, None))) == \"A_{:, :, :}\"\n    assert latex(ArraySlice(A, (None, 0, None), (None, 0, None))) == \"A_{:, 0, :}\"\n    assert latex(ArraySlice(A, (None, None, 1), (None, None, 1))) == \"A_{:, :, 1}\"\n", "def test_latex_printing_with_settings():\n    settings = {\"fold_short_frac\": True, \"fold_long_frac\": True, \"fold_func_brackets\": True}\n    assert latex((x + y)/(z + w), settings=settings) == r\"\\frac{x + y}{z + w}\"\n    assert latex(sin(x)/y, settings=settings) == r\"\\frac{\\sin{x}}{y}\"\n    assert latex(sin(x/y), settings=settings) == r\"\\sin{\\left(\\frac{x}{y} \\right)}\"\n", "def test_latex_issue_19181():\n    from sympy.matrices import MatrixSymbol\n    from sympy.printing.latex import LatexPrinter\n\n    X = MatrixSymbol('X', 2, 2)\n    Y = MatrixSymbol('Y', 2, 2)\n    lp = LatexPrinter()\n    assert lp._print_MatMul(2 * X @ Y) == r'2 X Y'\n", "def test_latex_printing_ArrayElement():\n    A = ArraySymbol(\"A\", 2, 3, 4)\n    expr = ArrayElement(A, (2, 1/(1-x), 0))\n    assert latex(expr) == \"{{A}_{2, \\\\frac{1}{1 - x}, 0}}\"\n"], "sample_1": ["def test_read_write_simple_specify_err_specs(tmp_path):\n    test_file = tmp_path / \"test.qdp\"\n    t1 = Table()\n    t1.add_column(Column(name=\"a\", data=[1, 2, 3]))\n    t1.add_column(Column(name=\"b\", data=[4.0, 5.0, 6.0]))\n    t1.write(test_file, format=\"ascii.qdp\", err_specs={'serr': [2]})\n    t2 = Table.read(test_file, table_id=0, format=\"ascii.qdp\", names=[\"a\", \"b\"])\n    assert np.all(t2[\"a\"] == t1[\"a\"])\n    assert np.all(t2[\"b_err\"] == t1[\"b\"])\n", "def test_roundtrip_example_with_err_specs(tmp_path):\n    example_qdp = \"\"\"\n        ! Initial comment line 1\n        READ TERR 1\n        READ SERR 3\n        ! Table 0 comment\n        !a,a_perr,a_nerr,b,c,c_err,d\n        53000.5,0.25,-0.5,1,1.5,3.5,2\n        54000.5,1.25,-1.5,2,2.5,4.5,3\n        NO,NO,NO,NO,NO\n        ! Table 1 comment\n        !a,a_perr,a_nerr,b,c,c_err,d\n        54000.5,2.25,-2.5,NO,3.5,5.5,5\n        55000.5,3.25,-3.5,4,4.5,6.5,nan\n        \"\"\"\n    test_file = tmp_path / \"test.qdp\"\n\n    t = Table.read(example_qdp, format=\"ascii.qdp\", table_id=1, sep=\",\")\n    t.write(test_file, err_specs={\"terr\": [1, 4], \"serr\": [6]})\n    t2 = Table.read(test_file, table_id=0, sep=\",\")\n\n    for col1, col2 in zip(t.itercols(), t2.itercols()):\n        assert np.allclose(col1, col2, equal_nan=True)\n", "def test_read_write_with_err_specs(tmp_path):\n    test_file = tmp_path / \"test.qdp\"\n    t1 = Table()\n    t1.add_column(Column(name=\"a\", data=[1.0, 2.0, 3.0]))\n    t1.add_column(Column(name=\"b\", data=[0.2, 0.4, 0.6]))\n    t1.add_column(Column(name=\"c\", data=[3.0, 2.0, 1.0]))\n    t1.add_column(Column(name=\"d\", data=[0.5, 0.4, 0.3]))\n    t1.write(test_file, format=\"ascii.qdp\", err_specs={\"terr\": [1], \"serr\": [3]})\n    t2 = Table.read(test_file, table_id=0, format=\"ascii.qdp\", names=[\"a\", \"b\", \"c\"])\n    assert np.allclose(t2[\"a\"], t1[\"a\"])\n    assert np.allclose(t2[\"b_perr\"], t1[\"b\"])\n    assert np.allclose(t2[\"b_nerr\"], t1[\"b\"])\n    assert np.allclose(t2[\"c\"], t1[\"c\"])\n    assert np.allclose(t2[\"c_err\"], t1[\"d\"])\n", "def test_read_write_simple_with_errors(tmp_path):\n    test_file = tmp_path / \"test.qdp\"\n    t1 = Table()\n    t1.add_column(Column(name=\"a\", data=[1, 2, 3]))\n    t1.add_column(Column(name=\"a_err\", data=[0.1, 0.2, 0.3]))\n    t1.add_column(Column(name=\"b\", data=[4.0, 5.0, 6.0]))\n    t1.write(test_file, format=\"ascii.qdp\", err_specs={\"serr\": [2]})\n    t2 = Table.read(test_file, table_id=0, format=\"ascii.qdp\", names=[\"a\", \"b\"])\n    assert np.allclose(t2[\"a\"], t1[\"a\"])\n    assert np.allclose(t2[\"a_err\"], t1[\"a_err\"])\n    assert np.allclose(t2[\"b\"], t1[\"b\"])\n", "def test_read_write_simple_err_specs(tmp_path):\n    test_file = tmp_path / \"test.qdp\"\n    t1 = Table()\n    t1.add_column(Column(name=\"a\", data=[1, 2, 3]))\n    t1.add_column(Column(name=\"b\", data=[4.0, 5.0, 6.0]))\n    t1.write(test_file, format=\"ascii.qdp\", err_specs={'serr': [2]})\n    t2 = Table.read(test_file, table_id=0, format=\"ascii.qdp\", names=[\"a\", \"b\"])\n    assert np.all(t2[\"a\"] == t1[\"a\"])\n    assert np.all(t2[\"b\"] == t1[\"b\"])\n"], "sample_156": ["def test_parser_mathematica_function_arguments():\n    parser = MathematicaParser()\n\n    # Test function with no arguments\n    assert parser._from_mathematica_to_tokens(\"f[]\") == [\"f\"]\n\n    # Test function with one argument\n    assert parser._from_mathematica_to_tokens(\"f[x]\") == [\"f\", \"x\"]\n\n    # Test function with multiple arguments\n    assert parser._from_mathematica_to_tokens(\"f[x, y, z]\") == [\"f\", \"x\", \"y\", \"z\"]\n\n    # Test function with nested arguments\n    assert parser._from_mathematica_to_tokens(\"f[g[x], h[y, z]]\") == [\"f\", [\"g\", \"x\"], [\"h\", \"y\", \"z\"]]\n\n    # Test function with nested and mixed arguments\n    assert parser._from_mathematica_to_tokens(\"f[x, g[y], h[z, w]]\") == [\"f\", \"x\", [\"g\", \"y\"], [\"h\", \"z\", \"w\"]]\n", "def test_parser_mathematica_exp_alt_extended():\n    parser = MathematicaParser()\n\n    convert_chain2 = lambda expr: parser._from_fullformlist_to_fullformsympy(parser._from_fullform_to_fullformlist(expr))\n    convert_chain3 = lambda expr: parser._from_fullformsympy_to_sympy(convert_chain2(expr))\n\n    # Testing extended functionality\n    full_form4 = \"List[1, 2, 3, List[4, 5]]\"\n    full_form5 = \"Function[x, Power[x, 2]]\"\n\n    assert parser._from_fullform_to_fullformlist(full_form4) == [\"List\", \"1\", \"2\", \"3\", [\"List\", \"4\", \"5\"]]\n    assert parser._from_fullform_to_fullformlist(full_form5) == [\"Function\", \"x\", [\"Power\", \"x\", \"2\"]]\n\n    assert convert_chain2(full_form4) == Tuple(1, 2, 3, Tuple(4, 5))\n    assert convert_chain2(full_form5) == Lambda(x, Power(x, 2))\n\n    assert convert_chain3(full_form4) == (1, 2, 3, (4, 5))\n    assert convert_chain3(full_form5) == Lambda(x, x**2)\n", "def test_parser_mathematica_fullform_expressions():\n    # Test cases for handling full form expressions\n    full_form1 = \"Plus[1, 2, 3]\"\n    full_form2 = \"Times[Sin[x], Cos[y]]\"\n    full_form3 = \"Power[a, b]\"\n\n    assert parse_mathematica(full_form1) == 1 + 2 + 3\n    assert parse_mathematica(full_form2) == sin(x)*cos(y)\n    assert parse_mathematica(full_form3) == a**b\n", "def test_parser_mathematica_functions_with_variable_arguments():\n    parser = MathematicaParser()\n\n    # Functions with variable arguments\n    assert parser.parse(\"Apply[f, {x, y, z}]\") == Apply(f, (x, y, z))\n    assert parser.parse(\"Map[f, {x, y, z}]\") == Map(f, (x, y, z))\n    assert parser.parse(\"MapAll[f, {x, y, z}]\") == MapAll(f, (x, y, z))\n    assert parser.parse(\"f @@@ {x, y, z}\") == Apply(f, (x, y, z), (1,))\n\n    # Functions with variable arguments and custom translations\n    translations = {\"f[*x]\": Lambda((\"a\", \"b\"), a + b)}\n    parser_with_translations = MathematicaParser(translations)\n    assert parser_with_translations.parse(\"f[x, y, z]\") == Lambda((\"a\", \"b\"), a + b)(x, y, z)\n\n    # Invalid function definition\n    invalid_translations = {\"f[*x\": Lambda((\"a\", \"b\"), a + b)}\n    raises(ValueError, lambda: MathematicaParser(invalid_translations))\n\n    # Invalid function call\n    raises(ValueError, lambda: parser.parse(\"f[x, y, z\"))\n", "def test_parser_mathematica_exponents():\n    parser = MathematicaParser()\n    chain = lambda expr: parser._from_tokens_to_fullformlist(parser._from_mathematica_to_tokens(expr))\n\n    # Exponents\n    assert chain(\"a^3\") == [\"Power\", \"a\", \"3\"]\n    assert chain(\"a^(3+4)\") == [\"Power\", \"a\", [\"Plus\", \"3\", \"4\"]]\n    assert chain(\"a^b^c\") == [\"Power\", \"a\", [\"Power\", \"b\", \"c\"]]\n    assert chain(\"a^b^(c+d)\") == [\"Power\", \"a\", [\"Power\", \"b\", [\"Plus\", \"c\", \"d\"]]]\n    assert chain(\"a^(b+c)^d\") == [\"Power\", \"a\", [\"Power\", [\"Plus\", \"b\", \"c\"], \"d\"]]\n    assert chain(\"a^(b^c)^d\") == [\"Power\", \"a\", [\"Power\", [\"Power\", \"b\", \"c\"], \"d\"]]\n    assert chain(\"(a^b)^c\") == [\"Power\", [\"Power\", \"a\", \"b\"], \"c\"]\n    assert chain(\"(a^(b+c))^d\") == [\"Power\", [\"Power\", \"a\", [\"Plus\", \"b\", \"c\"]], \"d\"]\n    assert chain(\"(a^b)^(c+d)\") == [\"Power\", [\"Power\", \"a\", \"b\"], [\"Plus\", \"c\", \"d\"]]\n    assert chain(\"a^b^c^d\") == [\"Power\", \"a\", [\"Power\", \"b\", [\"Power\", \"c\", \"d\"]]]\n    assert chain(\"a^(b^(c^d))\") == [\"Power\", \"a\", [\"Power\", \"b\", [\"Power\", \"c\", \"d\"]]]\n    assert chain(\"(a^b)^c^d\") == [\"Power\", [\"Power\", \"a\", \"b\"], [\"Power\", \"c\", \"d\"]]\n"], "sample_143": ["def test_Str_printing():\n    from sympy import Str\n    s = Str('hello')\n    assert pretty(s) == 'hello'\n    assert upretty(s) == 'hello'\n", "def test_issue_18553():\n    from sympy.core.symbol import Str\n    assert upretty(Str('x')) == 'x'\n", "def test_issue_18575():\n    assert pretty(Integral(x**2, (x, 0, oo))) == \\\n    '  oo   \\n'\\\n    ' __    \\n'\\\n    ' \\\\ `   \\n'\\\n    '  \\\\    2\\n'\\\n    '  /   x \\n'\\\n    ' /__,   \\n'\\\n    'x = 0   '\n\n    assert upretty(Integral(x**2, (x, 0, oo))) == \\\n    '  \u221e    \\n'\\\n    ' ___   \\n'\\\n    ' \u2572     \\n'\\\n    '  \u2572    2\\n'\\\n    '  \u2571   x \\n'\\\n    ' \u2571     \\n'\\\n    ' \u203e\u203e\u203e   \\n'\\\n    'x = 0  '\n", "def test_issue_18625():\n    # Testing that MatrixSymbol is preserved\n    A = MatrixSymbol(\"A\", 3, 3)\n    B = MatrixSymbol(\"B\", 3, 3)\n    C = MatrixSymbol(\"C\", 3, 3)\n    expr = A*B + A*B*C\n    assert pretty(expr) == \"A*B + A*B*C\"\n", "def test_issue_18922():\n    assert pretty(Tuple(*[x, y])) == '(x, y)'\n"], "sample_106": ["def test_transform_before_fit():\n    # Assert that calling transform before fit raises an error\n    nca = NeighborhoodComponentsAnalysis()\n    assert_raises(NotFittedError, nca.transform, iris_data)\n", "def test_transform_before_fit():\n    # Test that calling transform before fit raises an error\n    nca = NeighborhoodComponentsAnalysis()\n    assert_raises(NotFittedError, nca.transform, iris_data)\n", "def test_transform_before_fit():\n    # Assert that transform method raises an error if fit has not been called\n    nca = NeighborhoodComponentsAnalysis()\n    assert_raises(NotFittedError, nca.transform, iris_data)\n", "def test_fit_transform_method():\n    \"\"\"Test fit and transform methods of NeighborhoodComponentsAnalysis\"\"\"\n    X = iris_data\n    y = iris_target\n\n    nca = NeighborhoodComponentsAnalysis(max_iter=10)\n    nca.fit(X, y)\n    X_transformed = nca.transform(X)\n\n    # Check that transform method returns a numpy array\n    assert isinstance(X_transformed, np.ndarray)\n\n    # Check that transform method returns an array with correct shape\n    assert X_transformed.shape == (X.shape[0], nca.n_components)\n", "def test_transform_before_fit():\n    \"\"\"Test transform method before fit method.\"\"\"\n    nca = NeighborhoodComponentsAnalysis()\n    X = iris_data\n    assert_raises(NotFittedError, nca.transform, X)\n"], "sample_103": ["def test_mutual_info_regression_with_sparse_matrix():\n    # Test that the function works correctly with sparse matrices.\n    X = csr_matrix([[0, 0, 0],\n                    [1, 1, 0],\n                    [2, 0, 1],\n                    [2, 0, 1],\n                    [2, 0, 1]])\n    y = np.array([0, 1, 2, 2, 1])\n\n    mi = mutual_info_regression(X, y, discrete_features=True, random_state=0)\n    assert_array_equal(mi, np.array([0., 0., 0.]))  # Since all features are discrete, MI should be zero.\n", "def test_mutual_info_regression_sparse():\n    # Test that sparse matrix with continuous features raises a ValueError.\n    X = csr_matrix([[0, 0, 0],\n                    [1, 1, 0],\n                    [2, 0, 1],\n                    [2, 0, 1],\n                    [2, 0, 1]], dtype=float)\n    y = np.array([0, 1, 2, 2, 1], dtype=float)\n\n    with assert_raises(ValueError):\n        mutual_info_regression(X, y, discrete_features=False)\n", "def test_mutual_info_regression_sparse():\n    # Test that mutual_info_regression works correctly with sparse matrices.\n    # Here the target is continuous and there are two continuous and one\n    # discrete feature. The idea of this test is clear from the code.\n    rng = check_random_state(0)\n    X = rng.rand(1000, 3)\n    X[:, 1] += X[:, 0]\n    y = 0.5 * X[:, 0] + X[:, 2]\n    X[:, 2] = X[:, 2] > 0.5\n    X_csr = csr_matrix(X)\n\n    mi_dense = mutual_info_regression(X, y, discrete_features=[2], n_neighbors=3,\n                                      random_state=0)\n    mi_sparse = mutual_info_regression(X_csr, y, discrete_features=[2], n_neighbors=3,\n                                       random_state=0)\n\n    # Check that the results are the same for dense and sparse matrices\n    assert_array_equal(mi_dense, mi_sparse)\n", "def test_mutual_info_classif_sparse():\n    # Here we test the function with sparse input and discrete target.\n    # We create a sparse matrix where each row has two non-zero elements,\n    # one in the first half of the features and one in the second half.\n    # The target is based on the first non-zero element of each row.\n    # This allows us to test the function with both discrete and continuous features.\n    X = csr_matrix(([1, 1, 2, 2, 3, 3, 4, 4, 5, 5],\n                    ([0, 1, 2, 3, 4], [0, 50, 1, 51, 2])), shape=(5, 100))\n    y = np.array([0, 0, 1, 1, 2])\n\n    mi = mutual_info_classif(X, y, discrete_features='auto', random_state=0)\n    assert_array_equal(np.argsort(-mi), [0, 1, 2])\n", "def test_mutual_info_regression_sparse():\n    # Test mutual_info_regression with a sparse input\n    rng = check_random_state(0)\n    X = rng.rand(1000, 3)\n    X[:, 1] += X[:, 0]\n    y = 0.5 * X[:, 0] + X[:, 2]\n\n    # Convert X to a sparse matrix\n    X_csr = csr_matrix(X)\n\n    # Expect a ValueError since sparse matrix cannot have continuous features\n    assert_raises(ValueError, mutual_info_regression, X_csr, y)\n"], "sample_113": ["def test_column_transformer_output_indices_pandas_output():\n    # Checks for the output_indices_ attribute with data frames and pandas output\n    pd = pytest.importorskip(\"pandas\")\n\n    X_df = pd.DataFrame(np.arange(6).reshape(3, 2), columns=[\"first\", \"second\"])\n\n    ct = ColumnTransformer(\n        [(\"trans1\", Trans(), [\"first\"]), (\"trans2\", Trans(), [\"second\"])]\n    )\n    ct.set_output(transform=\"pandas\")\n    X_trans = ct.fit_transform(X_df)\n    assert ct.output_indices_ == {\n        \"trans1\": slice(0, 1),\n        \"trans2\": slice(1, 2),\n        \"remainder\": slice(0, 0),\n    }\n    assert_array_equal(X_trans.iloc[:, [0]], X_trans.iloc[:, ct.output_indices_[\"trans1\"]])\n    assert_array_equal(X_trans.iloc[:, [1]], X_trans.iloc[:, ct.output_indices_[\"trans2\"]])\n    assert_array_equal(X_trans.iloc[:, []], X_trans.iloc[:, ct.output_indices_[\"remainder\"]])\n\n    ct = ColumnTransformer([(\"trans1\", Trans(), [0]), (\"trans2\", Trans(), [1])])\n    ct.set_output(transform=\"pandas\")\n    X_trans = ct.fit_transform(X_df)\n    assert ct.output_indices_ == {\n        \"trans1\": slice(0, 1),\n        \"trans2\": slice(1, 2),\n        \"remainder\": slice(0, 0),\n    }\n    assert_array_equal(X_trans.iloc[:, [0]], X_trans.iloc[:, ct.output_indices_[\"trans1\"]])\n    assert_array_equal(X_trans.iloc[:, [1]], X_trans.iloc[:, ct.output_indices_[\"trans", "def test_column_transformer_output_indices_pandas_output():\n    # Checks for the output_indices_ attribute with data frames and pandas output\n    pd = pytest.importorskip(\"pandas\")\n\n    X_df = pd.DataFrame(np.arange(6).reshape(3, 2), columns=[\"first\", \"second\"])\n\n    ct = ColumnTransformer(\n        [(\"trans1\", Trans(), [\"first\"]), (\"trans2\", Trans(), [\"second\"])]\n    )\n    ct.set_output(transform=\"pandas\")\n    X_trans = ct.fit_transform(X_df)\n    assert ct.output_indices_ == {\n        \"trans1\": slice(0, 1),\n        \"trans2\": slice(1, 2),\n        \"remainder\": slice(0, 0),\n    }\n    assert_array_equal(X_trans.iloc[:, [0]], X_trans.iloc[:, ct.output_indices_[\"trans1\"]])\n    assert_array_equal(X_trans.iloc[:, [1]], X_trans.iloc[:, ct.output_indices_[\"trans2\"]])\n    assert_array_equal(X_trans.iloc[:, []], X_trans.iloc[:, ct.output_indices_[\"remainder\"]])\n\n    ct = ColumnTransformer([(\"trans1\", Trans(), [0]), (\"trans2\", Trans(), [1])])\n    ct.set_output(transform=\"pandas\")\n    X_trans = ct.fit_transform(X_df)\n    assert ct.output_indices_ == {\n        \"trans1\": slice(0, 1),\n        \"trans2\": slice(1, 2),\n        \"remainder\": slice(0, 0),\n    }\n    assert_array_equal(X_trans.iloc[:, [0]], X_trans.iloc[:, ct.output_indices_[\"trans1\"]])\n    assert_array_equal(X_trans.iloc[:, [1]], X_trans.iloc[:, ct.output_indices_[\"trans", "def test_column_transformer_output_indices_pandas_dataframe():\n    # Checks for the output_indices_ attribute with data frames\n    pd = pytest.importorskip(\"pandas\")\n\n    X_df = pd.DataFrame(np.arange(6).reshape(3, 2), columns=[\"first\", \"second\"])\n\n    ct = ColumnTransformer(\n        [(\"trans1\", Trans(), [\"first\"]), (\"trans2\", Trans(), [\"second\"])]\n    )\n    ct.set_output(transform=\"pandas\")\n    X_trans = ct.fit_transform(X_df)\n    assert ct.output_indices_ == {\n        \"trans1\": slice(0, 1),\n        \"trans2\": slice(1, 2),\n        \"remainder\": slice(0, 0),\n    }\n    assert_array_equal(X_trans.iloc[:, [0]], X_trans.iloc[:, ct.output_indices_[\"trans1\"]])\n    assert_array_equal(X_trans.iloc[:, [1]], X_trans.iloc[:, ct.output_indices_[\"trans2\"]])\n    assert_array_equal(X_trans.iloc[:, []], X_trans.iloc[:, ct.output_indices_[\"remainder\"]])\n\n    ct = ColumnTransformer([(\"trans1\", Trans(), [0]), (\"trans2\", Trans(), [1])])\n    ct.set_output(transform=\"pandas\")\n    X_trans = ct.fit_transform(X_df)\n    assert ct.output_indices_ == {\n        \"trans1\": slice(0, 1),\n        \"trans2\": slice(1, 2),\n        \"remainder\": slice(0, 0),\n    }\n    assert_array_equal(X_trans.iloc[:, [0]], X_trans.iloc[:, ct.output_indices_[\"trans1\"]])\n    assert_array_equal(X_trans.iloc[:, [1]], X_trans.iloc[:, ct.output_indices_[\"trans2\"]])", "def test_column_transformer_verbose_feature_names_out_false():\n    X_array = np.array([[0.0, 1.0, 2.0], [2.0, 4.0, 6.0]])\n    ct = ColumnTransformer([(\"trans1\", Trans(), [0]), (\"trans2\", Trans(), [1])], verbose_feature_names_out=False)\n    # check that feature names are not prefixed when verbose_feature_names_out=False\n    ct.fit(X_array)\n    assert ct.get_feature_names_out() == np.array(['trans1', 'trans2'])\n", "def test_column_transformer_with_nan_values():\n    pd = pytest.importorskip(\"pandas\")\n    X_array = np.array([[0, 1, np.nan], [2, np.nan, 6]]).T\n    X_df = pd.DataFrame(X_array, columns=[\"first\", \"second\"])\n\n    X_res_first = np.array([0, 1, np.nan]).reshape(-1, 1)\n    X_res_both = X_array\n\n    ct = ColumnTransformer([(\"trans\", Trans(), [0, 1])], remainder=\"drop\")\n    assert_array_equal(ct.fit_transform(X_df), X_res_both)\n    assert_array_equal(ct.fit(X_df).transform(X_df), X_res_both)\n    assert len(ct.transformers_) == 1\n    assert ct.transformers_[-1][0] != \"remainder\"\n\n    ct = ColumnTransformer([(\"trans1\", Trans(), [0]), (\"trans2\", Trans(), [1])])\n    assert_array_equal(ct.fit_transform(X_df), X_res_both)\n    assert_array_equal(ct.fit(X_df).transform(X_df), X_res_both)\n    assert len(ct.transformers_) == 2\n    assert ct.transformers_[-1][0] != \"remainder\"\n\n    # test with transformer_weights\n    transformer_weights = {\"trans1\": 0.1, \"trans2\": 10}\n    both = ColumnTransformer(\n        [(\"trans1\", Trans(), [0]), (\"trans2\", Trans(), [1])],\n        transformer_weights=transformer_weights,\n    )\n    res = np.vstack(\n        [\n            transformer_weights[\"trans1\"] * X_df[\"first\"],\n            transformer_weights[\"trans2\"] * X_df[\"second\"],\n        ]\n    ).T\n    assert_array_equal(both.fit_transform(X_df), res)\n    assert_array_"], "sample_97": ["def test_label_binarize_multilabel_indicator():\n    y_ind = np.array([[0, 1, 0], [1, 1, 1], [0, 0, 0]])\n    classes = [0, 1, 2]\n    pos_label = 2\n    neg_label = 0\n    expected = pos_label * y_ind\n    y_sparse = [sparse_matrix(y_ind)\n                for sparse_matrix in [coo_matrix, csc_matrix, csr_matrix,\n                                      dok_matrix, lil_matrix]]\n\n    for y in [y_ind] + y_sparse:\n        yield (check_binarized_results, y, classes, pos_label, neg_label,\n               expected)\n\n    assert_raises(ValueError, label_binarize, y, classes, neg_label=-1,\n                  pos_label=pos_label, sparse_output=True)\n", "def test_label_binarize_multiclass_with_negative_labels():\n    y = [-1, 0, 1]\n    classes = [-1, 0, 1]\n    pos_label = 2\n    neg_label = -2\n    expected = np.array([[2, -2, -2], [-2, 2, -2], [-2, -2, 2]])\n\n    yield check_binarized_results, y, classes, pos_label, neg_label, expected\n", "def test_label_binarize_multilabel_indicator():\n    y_ind = np.array([[0, 1, 0], [1, 1, 1], [0, 0, 0]])\n    classes = [0, 1, 2]\n    pos_label = 2\n    neg_label = 0\n    expected = pos_label * y_ind\n    y_sparse = [sparse_matrix(y_ind)\n                for sparse_matrix in [coo_matrix, csc_matrix, csr_matrix,\n                                      dok_matrix, lil_matrix]]\n\n    for y in [y_ind] + y_sparse:\n        yield (check_binarized_results, y, classes, pos_label, neg_label,\n               expected)\n\n    assert_raises(ValueError, label_binarize, y, classes, neg_label=-1,\n                  pos_label=pos_label, sparse_output=True)\n", "def test_label_binarize_multilabel_pos_neg_labels():\n    y_ind = np.array([[0, 1, 0], [1, 1, 1], [0, 0, 0]])\n    classes = [0, 1, 2]\n    pos_label = 3\n    neg_label = -1\n    expected = pos_label * y_ind - (1 - y_ind) * neg_label\n    y_sparse = [sparse_matrix(y_ind)\n                for sparse_matrix in [coo_matrix, csc_matrix, csr_matrix,\n                                      dok_matrix, lil_matrix]]\n\n    for y in [y_ind] + y_sparse:\n        yield (check_binarized_results, y, classes, pos_label, neg_label,\n               expected)\n", "def test_label_binarize_multilabel_indicator():\n    y_ind = np.array([[0, 1, 0], [1, 1, 1], [0, 0, 0]])\n    classes = [0, 1, 2]\n    pos_label = 2\n    neg_label = 0\n    expected = pos_label * y_ind\n    y_sparse = [sparse_matrix(y_ind)\n                for sparse_matrix in [coo_matrix, csc_matrix, csr_matrix,\n                                      dok_matrix, lil_matrix]]\n\n    for y in [y_ind] + y_sparse:\n        yield (check_binarized_results, y, classes, pos_label, neg_label,\n               expected)\n\n    assert_raises(ValueError, label_binarize, y, classes, neg_label=-1,\n                  pos_label=pos_label, sparse_output=True)\n"], "sample_26": ["    def test_migrate_test_setting_default(self, mocked_sync_apps, mocked_migrate, *mocked_objects):\n        # A test db is created with the default 'MIGRATE' setting.\n        test_connection = get_connection_copy()\n        del test_connection.settings_dict['TEST']['MIGRATE']\n        creation = test_connection.creation_class(test_connection)\n        if connection.vendor == 'oracle':\n            # Don't close connection on Oracle.\n            creation.connection.close = mock.Mock()\n        old_database_name = test_connection.settings_dict['NAME']\n        try:\n            with mock.patch.object(creation, '_create_test_db'):\n                creation.create_test_db(verbosity=0, autoclobber=True, serialize=False)\n            # Migrations run by default.\n            mocked_migrate.assert_called()\n            args, kwargs = mocked_migrate.call_args\n            self.assertEqual(args, ([('app_unmigrated', '0001_initial')],))\n            self.assertEqual(len(kwargs['plan']), 1)\n            # App is not synced.\n            mocked_sync_apps.assert_not_called()\n        finally:\n            with mock.patch.object(creation, '_destroy_test_db'):\n                creation.destroy_test_db(old", "    def test_clone_settings(self):\n        # get_test_db_clone_settings() returns the correct clone settings.\n        suffix = 'clone'\n        orig_name = 'test_db'\n        test_connection = get_connection_copy()\n        test_connection.settings_dict['NAME'] = orig_name\n        clone_settings = BaseDatabaseCreation(test_connection).get_test_db_clone_settings(suffix)\n        self.assertEqual(clone_settings['NAME'], f'{orig_name}_{suffix}')\n        self.assertEqual(clone_settings['HOST'], test_connection.settings_dict['HOST'])\n        self.assertEqual(clone_settings['PORT'], test_connection.settings_dict['PORT'])\n        self.assertEqual(clone_settings['ENGINE'], test_connection.settings_dict['ENGINE'])\n", "    def test_serialize_db_to_string(self, mocked_sync_apps, mocked_migrate, *mocked_objects):\n        test_connection = get_connection_copy()\n        test_connection.settings_dict['TEST']['MIGRATE'] = True\n        creation = test_connection.creation_class(test_connection)\n        if connection.vendor == 'oracle':\n            # Don't close connection on Oracle.\n            creation.connection.close = mock.Mock()\n        old_database_name = test_connection.settings_dict['NAME']\n        try:\n            with mock.patch.object(creation, '_create_test_db'):\n                creation.create_test_db(verbosity=0, autoclobber=True, serialize=True)\n            # Verify serialize_db_to_string() is called and returns data.\n            self.assertIsNotNone(test_connection._test_serialized_contents)\n            self.assertTrue(len(test_connection._test_serialized_contents) > 0)\n        finally:\n            with mock.patch.object(creation, '_destroy_test_db'):\n                creation.destroy_test_db(old_database_name, verbosity=0)\n", "    def test_clone_test_db(self, mocked_sync_apps, mocked_migrate, *mocked_objects):\n        test_connection = get_connection_copy()\n        creation = test_connection.creation_class(test_connection)\n        if connection.vendor == 'oracle':\n            # Don't close connection on Oracle.\n            creation.connection.close = mock.Mock()\n        try:\n            with mock.patch.object(creation, '_clone_test_db'):\n                creation.clone_test_db(suffix='001', verbosity=0, autoclobber=True)\n            # _clone_test_db is called.\n            creation._clone_test_db.assert_called_with('001', 0, False)\n            # Migrations and sync_apps are not called.\n            mocked_migrate.assert_not_called()\n            mocked_sync_apps.assert_not_called()\n        finally:\n            # Clean up after test.\n            with mock.patch.object(creation, '_destroy_test_db'):\n                creation.destroy_test_db(suffix='001', verbosity=0)\n", "def test_migrate_test_setting_with_error(self, mocked_sync_apps, mocked_migrate, *mocked_objects):\n    test_connection = get_connection_copy()\n    test_connection.settings_dict['TEST']['MIGRATE'] = True\n    creation = test_connection.creation_class(test_connection)\n    if connection.vendor == 'oracle':\n        # Don't close connection on Oracle.\n        creation.connection.close = mock.Mock()\n    old_database_name = test_connection.settings_dict['NAME']\n    mocked_migrate.side_effect = Exception(\"Migration error\")\n    try:\n        with mock.patch.object(creation, '_create_test_db'):\n            with self.assertRaises(SystemExit):\n                creation.create_test_db(verbosity=0, autoclobber=True, serialize=False)\n        # Migrations run and raise an exception.\n        mocked_migrate.assert_called()\n        args, kwargs = mocked_migrate.call_args\n        self.assertEqual(args, ([('app_unmigrated', '0001_initial')],))\n        self.assertEqual(len(kwargs['plan']), 1)\n        # App is not synced.\n        mocked_sync_apps.assert_not_called()\n    finally:\n        with mock.patch.object(creation, '_destroy_test_db'):\n            creation.destroy_test_db(old_database_name, verbosity=0)\n"], "sample_50": ["def test_cookie_samesite_lax(self):\n    \"\"\"\n    CookieStorage honors SESSION_COOKIE_SAMESITE with 'Lax' value.\n    \"\"\"\n    with override_settings(SESSION_COOKIE_SAMESITE='Lax'):\n        storage = self.get_storage()\n        response = self.get_response()\n        storage.add(constants.INFO, 'test')\n        storage.update(response)\n        self.assertEqual(response.cookies['messages']['samesite'], 'Lax')\n", "def test_remove_newest(self):\n    \"\"\"\n    If the data exceeds what is allowed in a cookie, newest messages are removed\n    before saving if remove_oldest is False.\n    \"\"\"\n    storage = self.get_storage()\n    response = self.get_response()\n\n    # When storing as a cookie, the cookie has constant overhead of approx\n    # 54 chars, and each message has a constant overhead of about 37 chars\n    # and a variable overhead of zero in the best case. We aim for a message\n    # size which will fit 4 messages into the cookie, but not 5.\n    msg_size = int((CookieStorage.max_cookie_size - 54) / 4.5 - 37)\n    last_msg = None\n    # Generate the same (tested) content every time that does not get run\n    # through zlib compression.\n    random.seed(42)\n    for i in range(5):\n        msg = get_random_string(msg_size)\n        storage.add(constants.INFO, msg)\n        if i == 4:\n            last_msg = msg\n    unstored_messages = storage.update(response, remove_oldest=False)\n\n    cookie_storing = self.stored_messages_count(storage, response)\n    self.assertEqual(cookie_storing, 4)\n\n    self.assertEqual(len(unstored_messages), 1)\n    self.assertEqual(unstored_messages[0].message, last_msg)\n", "def test_message_decoder_error(self):\n    \"\"\"\n    The custom JSON decoder returns the original value if a JSONDecodeError is raised.\n    \"\"\"\n    original_value = {'message': 'invalid json'}\n    encoded_value = json.dumps(original_value)\n    decoder = MessageDecoder()\n    decoded_value = decoder.decode(encoded_value)\n    self.assertEqual(decoded_value, original_value)\n", "    def test_message_persistence(self):\n        \"\"\"\n        Messages are persisted across requests when stored in a cookie.\n        \"\"\"\n        # Set a message in the first request\n        storage = self.get_storage()\n        storage.add(constants.INFO, 'test')\n        response = self.get_response()\n        storage.update(response)\n        self.assertEqual(self.stored_messages_count(storage, response), 1)\n\n        # Retrieve the message in the second request\n        request = self.get_request()\n        request.COOKIES = response.cookies\n        storage = self.get_storage(request=request)\n        self.assertEqual(list(storage), ['test'])\n", "def test_store_remove_oldest(self):\n    \"\"\"\n    The oldest messages are removed first when storing if remove_oldest=True.\n    \"\"\"\n    storage = self.get_storage()\n    response = self.get_response()\n\n    messages = [f'message {i}' for i in range(5)]\n    for message in messages:\n        storage.add(constants.INFO, message)\n    storage._store(storage._loaded_data, response, remove_oldest=True)\n\n    cookie_storing = self.stored_messages_count(storage, response)\n    self.assertEqual(cookie_storing, 4)\n\n    stored_messages = [msg.message for msg in storage._loaded_data]\n    self.assertEqual(stored_messages, messages[1:])\n"], "sample_90": ["def test_mark_getattr_with_default(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.a(b=1)\n            pass\n    \"\"\"\n    )\n    items, reprec = testdir.inline_genitems()\n    assert items[0].get_closest_marker(\"a\").get(\"b\") == 1\n    assert items[0].get_closest_marker(\"a\").get(\"c\", 2) == 2\n", "def test_marker_expr_eval_outcomes(testdir, expr, expected_outcome):\n    foo = testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.internal_err\n            pass\n        \"\"\"\n    )\n    result = testdir.runpytest(foo, \"-m\", expr)\n    if expected_outcome == \"failed\":\n        result.assert_outcomes(failed=1)\n    elif expected_outcome == \"passed\":\n        result.assert_outcomes(passed=1)\n    elif expected_outcome == \"skipped\":\n        result.assert_outcomes(skipped=1)\n", "def test_mark_decorator_order(testdir):\n    p = testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.a\n        @pytest.mark.b\n            pass\n\n        @pytest.mark.c\n        @pytest.mark.a\n        @pytest.mark.b\n            pass\n        \"\"\"\n    )\n    items, rec = testdir.inline_genitems(p)\n    assert len(items) == 2\n    assert [m.name for m in items[0].iter_markers()] == [\"a\", \"b\"]\n    assert [m.name for m in items[1].iter_markers()] == [\"c\", \"a\", \"b\"]\n", "def test_markers_from_parametrize_inheritance(testdir):\n    \"\"\"Test markers from parametrize are inherited by subclasses #3605\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        custom_mark = pytest.mark.custom_mark\n\n        @pytest.fixture(autouse=True)\n            custom_mark = list(request.node.iter_markers('custom_mark'))\n            print(\"Custom mark %s\" % custom_mark)\n\n        @custom_mark(\"custom mark non parametrized\")\n        class TestBase:\n                print(\"Hey from test\")\n\n        @pytest.mark.parametrize(\n            \"obj_type\",\n            [\n                custom_mark(\"custom mark1\")(\"template\"),\n                pytest.param(\n                    \"vm\",\n                    marks=custom_mark('custom mark2')\n                ),\n            ]\n        )\n        class TestDerived(TestBase):\n            pass\n        \"\"\"\n    )\n\n    result = testdir.runpytest()\n    result.assert_outcomes(passed=4)\n", "def test_mark_expression_eval_failure_handling(testdir):\n    foo = testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.internal_err\n            pass\n        \"\"\"\n    )\n    expr = \"internal_err and (NOT bogus/)\"\n    expected = \"ERROR: Wrong expression passed to '-m': {}: *\".format(expr)\n    result = testdir.runpytest(foo, \"-m\", expr)\n    result.stderr.fnmatch_lines([expected])\n    assert result.ret == ExitCode.USAGE_ERROR\n"], "sample_125": ["def test_Float_mpf():\n    import mpmath\n    mpmath.mp.dps = 100\n    mp_pi = mpmath.pi()\n\n    assert Float(mp_pi, 100) == Float(mp_pi._mpf_, 100) == pi.evalf(100)\n\n    mpmath.mp.dps = 15\n\n    assert Float(mp_pi, 100) == Float(mp_pi._mpf_, 100) == pi.evalf(100)\n", "def test_Float_abs():\n    a = Float(2.3)\n    b = Float(-2.3)\n    assert abs(a) == Float(2.3)\n    assert abs(b) == Float(2.3)\n", "def test_NumberSymbol_sympify():\n    assert sympify(pi) is pi\n    assert sympify(EulerGamma) is EulerGamma\n    assert sympify(GoldenRatio) is GoldenRatio\n    assert sympify(Catalan) is Catalan\n    assert sympify(I) is I\n    assert sympify(oo) is oo\n    assert sympify(-oo) is S.NegativeInfinity\n    assert sympify(zoo) is zoo\n    assert sympify(nan) is nan\n", "def test_Float_integral_value():\n    assert Float(2.0).is_integer\n    assert not Float(2.5).is_integer\n    assert Float(2.0, precision=10).is_integer\n    assert not Float(2.5, precision=10).is_integer\n", "def test_issue_10410():\n    x = Symbol('x')\n    assert expand_trig(sin(x)**2 + cos(x)**2) == 1\n"], "sample_129": ["def test_Quaternion_latex_printing_zero():\n    q = Quaternion(0, 0, 0, 0)\n    assert latex(q) == \"0\"\n", "def test_TensorProduct_array_printing():\n    from sympy.tensor.array import TensorProduct\n    A = ImmutableDenseNDimArray([[1, 2], [3, 4]])\n    B = ImmutableDenseNDimArray([[5, 6], [7, 8]])\n    assert latex(TensorProduct(A, B)) == r\"\\left[\\begin{matrix}1 & 2\\\\3 & 4\\end{matrix}\\right] \\otimes \\left[\\begin{matrix}5 & 6\\\\7 & 8\\end{matrix}\\right]\"\n", "def test_KroneckerProduct_printing():\n    from sympy import MatrixSymbol\n    from sympy.matrices import KroneckerProduct\n    A = MatrixSymbol(\"A\", 3, 3)\n    B = MatrixSymbol(\"B\", 2, 2)\n    assert latex(KroneckerProduct(A, B)) == r\"A \\otimes B\"\n", "def test_RisingFactorial_printing():\n    assert latex(RisingFactorial(x, 3)) == r\"x^{\\overline{3}}\"\n    assert latex(RisingFactorial(x, -3)) == r\"x^{\\overline{-3}}\"\n", "def test_Quaternion_latex_printing_with_zero_coefficients():\n    q = Quaternion(0, 0, z, t)\n    assert latex(q) == \"z j + t k\"\n    q = Quaternion(x, 0, 0, t)\n    assert latex(q) == \"x + t k\"\n    q = Quaternion(0, y, z, 0)\n    assert latex(q) == \"y i + z j\"\n    q = Quaternion(0, 0, 0, 0)\n    assert latex(q) == \"0\"\n"], "sample_70": ["def test_legend_loc_string(loc):\n    fig, ax = plt.subplots()\n    ax.plot(range(10), label='test')\n    leg = ax.legend(loc=loc)\n    assert leg._loc == loc\n", "def test_legend_loc_string(loc):\n    fig, ax = plt.subplots()\n    ax.plot(range(10), label='test')\n    leg = ax.legend(loc=loc)\n    assert leg.get_loc() == mlegend.Legend.codes[loc]\n", "def test_legend_scatteryoffsets():\n    # Test that scatteryoffsets can be customized\n    fig, ax = plt.subplots()\n    ax.scatter([0, 1], [0, 1], label='scatter')\n    leg = ax.legend(scatteryoffsets=[0.2, 0.4, 0.6])\n    assert leg._scatteryoffsets.tolist() == [0.2, 0.4, 0.6]\n", "def test_legend_loc_string():\n    # Test that the legend location can be set using a string\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3], [1, 2, 3], label='Test')\n    ax.legend(loc='upper left')\n    assert ax.get_legend()._loc == 2\n", "def test_plot_multiple_input_single_label_nocol(label):\n    # test ax.plot() with multidimensional input\n    # and single label and ncol\n    x = [1, 2, 3]\n    y = [[1, 2],\n         [2, 5],\n         [4, 9]]\n    ncol = 2\n\n    fig, ax = plt.subplots()\n    ax.plot(x, y, label=label)\n    leg = ax.legend(ncol=ncol)\n    legend_texts = [entry.get_text() for entry in leg.get_texts()]\n    assert legend_texts == [str(label)] * 2\n    assert leg._ncols == ncol\n"], "sample_3": ["def test_separable_multiple_models(model, expected):\n    assert_allclose(is_separable(model), expected[0])\n    assert_allclose(separability_matrix(model), expected[1])\n", "def test_separable_matrix_custom_model():\n    @custom_model\n        return x * y\n\n    expected_separability_matrix = np.array([[True, False], [False, True]])\n    assert_allclose(separability_matrix(model_b()), expected_separability_matrix)\n", "def test_custom_model_separable_computed_separability_matrix():\n    @custom_model\n        return x * y + z\n\n    model_b.n_inputs = 3\n    model_b.n_outputs = 1\n    model_b.separable = False\n    model_b._calculate_separability_matrix = lambda: np.array([[1, 1, 1]])\n\n    assert not is_separable(model_b())\n    assert np.all(separability_matrix(model_b()) == [[True, True, True]])\n", "def test_custom_model_separable_matrix():\n    @custom_model\n        return x * y\n\n    assert np.all(separability_matrix(model_b()) == [[True, False], [False, True]])\n", "def test_separable_with_mapping():\n    result = separability_matrix(map2 | map2)\n    assert_allclose(result, np.array([[1, 0, 1], [0, 0, 0], [1, 0, 1]]))\n"], "sample_157": ["def test_tensor_product_matrix_multiplication():\n    m1 = Matrix([[1, 2], [3, 4]])\n    m2 = Matrix([[5, 6], [7, 8]])\n    assert TensorProduct(m1, m2) * TensorProduct(m2, m1) == TensorProduct(m1*m2, m2*m1)\n", "def test_tensor_product_doit():\n    A, B = symbols('A B', commutative=False)\n    x = symbols('x')\n    assert TP(x*A, B).doit() == x*TP(A, B)\n", "def test_tensor_product_matrix():\n    assert TensorProduct(mat1, mat2) == Matrix([[2*I, 6, 3*I, 0], [4*I, 2, 12, 0], [1, 2*I, 3, 6*I], [0, 4*I, 0, 6]])\n", "def test_tensor_product_matrices():\n    assert TensorProduct(mat1, mat2) == Matrix([\n        [2*I, 6*I, 3*I, 6],\n        [(4*I+1)*I, 6*(1+I), 3*(1+I), 6*(1+I)],\n        [2*I, 6*I, 6*I, 12],\n        [(4*I+1)*I, 6*(1+I), 6*(1+I), 12*(1+I)]\n    ])\n", "def test_tensor_product_matrices():\n    assert TensorProduct(mat1, mat2) == Matrix([[2*I, 6, 3*I, 0], [2*I, 3, 4*I, 6], [4*I, 6, 6*I, 0], [0, 12, 0, 6]])\n    assert TensorProduct(mat2, mat1) == Matrix([[2*I, 3*I, 6, 0], [4*I, 6*I, 9, 6], [6, 0, 6*I, 12], [0, 6, 12, 6]])\n"], "sample_139": ["def test_issue_15902():\n    from sympy import polar_lift, unpolarify, exp_polar, Symbol\n    x = Symbol('x')\n    p = exp_polar(x) + 1\n    u = unpolarify(p)\n    assert unpolarify(polar_lift(u)) == u\n", "def test_polarify_issue_15893():\n    from sympy import polarify, polar_lift, I, symbols\n    f = Function('f', real=True)\n    x = Symbol('x', real=True)\n    y = Symbol('y', imaginary=True)\n    assert polarify(f(x), subs=False) == f(x)\n    assert polarify(f(x) + I, subs=False) == f(x) + polar_lift(I)\n    assert polarify(f(x) + y, subs=False) == f(x) + polar_lift(y)\n    assert polarify(f(x) + y, lift=True) == polar_lift(f(x)) + polar_lift(y)\n", "def test_issue_16068():\n    from sympy import im, re, polar_lift, exp_polar, I\n    from sympy.abc import x\n\n    assert im(polar_lift(x + I)) == re(x)\n    assert re(polar_lift(x + I)) == -im(x)\n    assert im(exp_polar(x + I)) == re(exp(x))\n    assert re(exp_polar(x + I)) == -im(exp(x))\n", "def test_issue_15902():\n    # Test for abs(x) and x.is_zero for complex symbols\n    x = Symbol('x', complex=True, zero=False)\n    assert Abs(x).is_zero is False\n", "def test_issue_16100():\n    from sympy import polar_lift, Abs, log, I, pi, unpolarify, exp, sin\n    z = polar_lift(1 + I)\n    assert unpolarify(log(Abs(z)**2)) == 2*I*pi\n    assert unpolarify(log(z)**2) == log(z)**2\n    assert unpolarify(sin(z)**2) == sin(z)**2\n    assert unpolarify(exp(z)**2) == exp(2*I)*exp(2*cos(pi/4))\n"], "sample_95": ["def test_skipif_markeval_namespace_multiple_nested(self, pytester: Pytester) -> None:\n    \"\"\"Keys defined by ``pytest_markeval_namespace()`` in nested plugins override top-level ones.\"\"\"\n    root = pytester.mkdir(\"root\")\n    root.joinpath(\"__init__.py\").touch()\n    root.joinpath(\"conftest.py\").write_text(\n        textwrap.dedent(\n            \"\"\"\\\n        import pytest\n\n            return {\"arg\": \"root\"}\n        \"\"\"\n        )\n    )\n    root.joinpath(\"test_root.py\").write_text(\n        textwrap.dedent(\n            \"\"\"\\\n        import pytest\n\n        @pytest.mark.skipif(\"arg == 'root'\")\n            assert False\n        \"\"\"\n        )\n    )\n    foo = root.joinpath(\"foo\")\n    foo.mkdir()\n    foo.joinpath(\"__init__.py\").touch()\n    foo.joinpath(\"conftest.py\").write_text(\n        textwrap.dedent(\n            \"\"\"\\\n        import pytest\n\n            return {\"arg\": \"foo\"}\n        \"\"\"\n        )\n    )\n    foo.joinpath(\"test_foo.py\").write_text(\n        textwrap.dedent(\n            \"\"\"\\\n        import pytest\n\n        @pytest.mark.skipif(\"arg == 'foo'\")\n            assert False\n        \"\"\"\n        )\n    )\n    bar = foo.joinpath(\"bar\")\n    bar.mkdir()\n    bar.joinpath(\"__init__.py\").touch()\n    bar.joinpath(\"conftest.py\").write_text(\n        textwrap.dedent(\n            \"\"\"\\\n        import pytest\n\n            return {\"arg\": \"bar\"}\n        \"\"\"\n        )\n    )\n    bar.joinpath(\"test_bar.py\").write_text(\n        textwrap.dedent", "def test_skip_with_boolean_without_reason(pytester: Pytester) -> None:\n    item = pytester.getitem(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.skipif(False)\n            pass\n    \"\"\"\n    )\n    with pytest.raises(pytest.fail.Exception) as excinfo:\n        evaluate_skip_marks(item)\n    assert excinfo.value.msg is not None\n    assert (\n        \"\"\"Error evaluating 'skipif': you need to specify reason=STRING when using booleans as conditions.\"\"\"\n        in excinfo.value.msg\n    )\n", "def test_imperativeskip_in_setup_function(self, pytester: Pytester) -> None:\n    p = pytester.makepyfile(\n        \"\"\"\n        import pytest\n            pytest.skip(\"hello\")\n\n            assert 0\n        \"\"\"\n    )\n    result = pytester.runpytest(p)\n    result.stdout.fnmatch_lines([\"*1 skipped*\"])\n    result = pytester.runpytest(p, \"-rs\")\n    result.stdout.fnmatch_lines([\"*SKIP*test_this*\", \"*hello*\"])\n    result = pytester.runpytest(p, \"--runxfail\")\n    result.stdout.fnmatch_lines(\n        \"\"\"\n        *def test_this*\n        *1 fail*\n        \"\"\"\n    )\n", "def test_xfail_not_run_no_setup_pass(self, pytester: Pytester) -> None:\n    p = pytester.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail(run=False, reason=\"hello\")\n            assert 1\n            raise ValueError(42)\n        \"\"\"\n    )\n    result = pytester.runpytest(p, \"-rx\")\n    result.stdout.fnmatch_lines([\"*test_this*\", \"*NOTRUN*hello\", \"*1 xfailed*\"])\n", "def test_xfail_imperative_in_setup_function_pass(self, pytester: Pytester) -> None:\n    p = pytester.makepyfile(\n        \"\"\"\n        import pytest\n            pytest.xfail(\"setup function xfail\")\n\n            assert 1\n    \"\"\"\n    )\n    result = pytester.runpytest(p, \"--runxfail\")\n    result.stdout.fnmatch_lines([\"*1 xfailed*\", \"*1 passed*\"])\n"], "sample_44": ["def test_label_from_instance_method(self):\n    f = forms.ModelChoiceField(Category.objects.all())\n    f.label_from_instance = lambda obj: 'Category: ' + obj.name\n    self.assertEqual(list(f.choices), [\n        ('', '---------'),\n        (self.c1.pk, 'Category: Entertainment'),\n        (self.c2.pk, 'Category: A test'),\n        (self.c3.pk, 'Category: Third'),\n    ])\n", "def test_choices_bool_custom_empty_label(self):\n    f = forms.ModelChoiceField(Category.objects.all(), empty_label='--------')\n    Category.objects.all().delete()\n    self.assertIs(bool(f.choices), True)\n", "def test_queryset_filtering(self):\n    \"\"\"\n    Test that filtering the queryset with limit_choices_to works correctly.\n    \"\"\"\n    Category.objects.create(name='Filtered out', slug='filtered-out', url='filtered-out')\n    f = forms.ModelChoiceField(Category.objects.all(), limit_choices_to={'slug__contains': 'test'})\n    self.assertEqual(list(f.choices), [\n        ('', '---------'),\n        (self.c2.pk, 'A test'),\n        (self.c3.pk, 'Third'),\n    ])\n\n    # Test with a callable limit_choices_to\n    f = forms.ModelChoiceField(Category.objects.all(), limit_choices_to=lambda: {'slug__contains': 'test'})\n    self.assertEqual(list(f.choices), [\n        ('', '---------'),\n        (self.c2.pk, 'A test'),\n        (self.c3.pk, 'Third'),\n    ])\n", "def test_limit_choices_to(self):\n    # Test limit_choices_to as a dictionary.\n    limited_queryset = Category.objects.filter(slug__contains='test')\n    f = forms.ModelChoiceField(Category.objects.all(), limit_choices_to={'slug__contains': 'test'})\n    self.assertCountEqual(f.choices.queryset, limited_queryset)\n\n    # Test limit_choices_to as a callable.\n    f = forms.ModelChoiceField(Category.objects.all(), limit_choices_to=lambda: {'slug__contains': 'test'})\n    self.assertCountEqual(f.choices.queryset, limited_queryset)\n", "def test_choices_with_limit_choices_to(self):\n    # Create additional categories that don't match the limit_choices_to condition\n    Category.objects.create(name='Inactive', slug='inactive', url='inactive', is_active=False)\n    Category.objects.create(name='Hidden', slug='hidden', url='hidden', is_visible=False)\n\n    # Create a ModelChoiceField with limit_choices_to condition\n    f = forms.ModelChoiceField(Category.objects.all(), limit_choices_to={'is_active': True, 'is_visible': True})\n\n    # Verify that only the active and visible categories are included in the choices\n    self.assertEqual(list(f.choices), [\n        ('', '---------'),\n        (self.c1.pk, 'Entertainment'),\n        (self.c2.pk, 'A test'),\n        (self.c3.pk, 'Third'),\n    ])\n\n    # Verify that the choices update when the queryset changes\n    Category.objects.create(name='New', slug='new', url='new', is_active=True, is_visible=True)\n    self.assertEqual(list(f.choices), [\n        ('', '---------'),\n        (self.c1.pk, 'Entertainment'),\n        (self.c2.pk, 'A test'),\n        (self.c3.pk, 'Third'),\n        # Newly created category is included in the choices\n        (Category.objects.get(name='New').pk, 'New'),\n    ])\n"], "sample_76": ["def test_low_order(self, df):\n\n    groupby = GroupBy([\"group\"])\n    order = df[\"x\"].nunique() - 1\n    res = PolyFit(order=order, gridsize=100)(df[[\"x\", \"y\"]], groupby, \"x\", {})\n\n    assert_array_equal(res[\"x\"], [])\n    assert_array_equal(res[\"y\"], [])\n", "    def test_order_greater_than_unique_values(self, df):\n\n        groupby = GroupBy([\"group\"])\n        df[\"x\"] = df[\"x\"].round().astype(int)  # Make x have <= 2 unique values\n        order = 3\n        res = PolyFit(order=order)(df[[\"x\", \"y\"]], groupby, \"x\", {})\n\n        assert res.empty  # Since there are <= order unique values in x, result should be empty\n", "def test_low_order(self, df):\n\n    groupby = GroupBy([\"group\"])\n    res = PolyFit(order=0, gridsize=100)(df[[\"x\", \"y\"]], groupby, \"x\", {})\n\n    assert res[\"x\"].empty\n    assert res[\"y\"].empty\n", "def test_low_order_fit(self, df):\n\n    groupby = GroupBy([\"group\"])\n    order = 1\n    res = PolyFit(order=order, gridsize=100)(df[[\"x\", \"y\"]], groupby, \"x\", {})\n\n    assert_array_equal(res.columns, [\"x\", \"y\"])\n\n    grid = np.linspace(df[\"x\"].min(), df[\"x\"].max(), 100)\n    assert_array_equal(res[\"x\"], grid)\n    assert_array_almost_equal(\n        res[\"y\"].diff(), np.zeros(grid.size - 1)\n    )\n", "    def test_high_order_poly(self, df):\n\n        groupby = GroupBy([\"group\"])\n        order = 3\n        res = PolyFit(order=order, gridsize=100)(df[[\"x\", \"y\"]], groupby, \"x\", {})\n\n        assert_array_equal(res.columns, [\"x\", \"y\"])\n\n        grid = np.linspace(df[\"x\"].min(), df[\"x\"].max(), 100)\n        assert_array_equal(res[\"x\"], grid)\n        assert res[\"y\"].diff().diff().diff().dropna().abs().gt(0).all()\n"], "sample_24": ["def test_message_dict_property(self):\n    message_dict = {'field1': ['E1', 'E2'], 'field2': ['E3', 'E4']}\n    exception = ValidationError(message_dict)\n    self.assertEqual(exception.message_dict, message_dict)\n\n    with self.assertRaises(AttributeError):\n        exception.message_dict = {}\n", "def test_update_error_dict_with_non_field_errors(self):\n    error = ValidationError('message')\n    error_dict = {}\n    error.update_error_dict(error_dict)\n    self.assertEqual(error_dict, {'__all__': ['message']})\n", "def test_update_error_dict(self):\n    error1 = ValidationError({'field1': 'message1'})\n    error2 = ValidationError({'field2': 'message2'})\n    error3 = ValidationError('message3')\n\n    error_dict = {}\n    error_dict = error1.update_error_dict(error_dict)\n    error_dict = error2.update_error_dict(error_dict)\n    error_dict = error3.update_error_dict(error_dict)\n\n    self.assertEqual(error_dict, {'field1': ['message1'], 'field2': ['message2'], '__all__': ['message3']})\n", "def test_update_error_dict_nested(self):\n    error1 = ValidationError({'field1': 'message', 'field2': 'other'})\n    error2 = ValidationError({'field1': 'message2', 'field3': 'another'})\n    error_dict = {}\n    error_dict = error1.update_error_dict(error_dict)\n    error_dict = error2.update_error_dict(error_dict)\n    self.assertEqual(error_dict, {'field1': ['message', 'message2'], 'field2': ['other'], 'field3': ['another']})\n", "    def test_error_list_handling(self):\n        error_list = [\n            ValidationError('Error message 1'),\n            ValidationError('Error message 2')\n        ]\n        exception = ValidationError(error_list)\n        self.assertEqual(len(exception.error_list), 2)\n        self.assertEqual(exception.error_list[0].message, 'Error message 1')\n        self.assertEqual(exception.error_list[1].message, 'Error message 2')\n"], "sample_36": ["def test_invert(self):\n    q = Q(price__gt=F('discounted_price'))\n    self.assertEqual(~q, Q(price__gt=F('discounted_price'), _negated=True))\n", "def test_invert_q_object(self):\n    q = Q(price=F('discounted_price'))\n    inverted_q = ~q\n    self.assertEqual(inverted_q.connector, Q.AND)\n    self.assertEqual(inverted_q.negated, True)\n    self.assertEqual(inverted_q.children, [q])\n", "def test_combine_and_with_self(self):\n    q = Q(x=1)\n    self.assertEqual(q & q, q)\n", "def test_combine_and_or(self):\n    q1 = Q(x=1)\n    q2 = Q(y=2)\n    q3 = Q(z=3)\n    combined = (q1 & q2) | q3\n    expected = Q(Q(x=1) & Q(y=2)) | Q(z=3)\n    self.assertEqual(combined, expected)\n", "def test_combine_or_with_duplicates(self):\n    q1 = Q(price__gt=F('discounted_price'))\n    q2 = Q(price__gt=F('discounted_price'))\n    q = q1 | q2\n    path, args, kwargs = q.deconstruct()\n    self.assertEqual(args, (('price__gt', F('discounted_price')),))\n    self.assertEqual(kwargs, {})\n"], "sample_67": ["def test_serialize_deconstructible(self):\n    instance = DeconstructibleInstances()\n    string = MigrationWriter.serialize(instance)[0]\n    self.assertEqual(\n        string,\n        \"migrations.test_writer.DeconstructibleInstances()\",\n    )\n", "def test_serialize_complex(self):\n    \"\"\"\n    Make sure complex numbers can be serialized.\n    \"\"\"\n    self.assertSerializedEqual(complex(1, 2))\n", "def test_serialize_function_type(self):\n        return 42\n\n    self.assertSerializedEqual(test_function)\n    self.assertSerializedResultEqual(\n        test_function,\n        (\n            \"migrations.test_writer.test_function\",\n            {\"import migrations.test_writer\"},\n        ),\n    )\n", "def test_serialize_deconstructible(self):\n    deconstructible_instance = DeconstructibleInstances()\n    string, imports = MigrationWriter.serialize(deconstructible_instance)\n    self.assertEqual(\n        string,\n        \"migrations.test_writer.DeconstructibleInstances()\",\n    )\n    self.assertEqual(imports, {\"import migrations.test_writer\"})\n", "def test_serialize_custom_fields(self):\n    custom_field = CustomFieldWithCustomSerializer(max_length=255)\n    string, imports = MigrationWriter.serialize(custom_field)\n    self.assertEqual(\n        string,\n        \"migrations.models.CustomFieldWithCustomSerializer(max_length=255)\",\n    )\n    self.assertEqual(imports, {\"import migrations.models\"})\n"], "sample_5": ["def test_fast_delete_reverse_foreign_key(self):\n    a = create_a('reverse_fk')\n    r = a.reverse_fk\n    # 1 query to fast-delete the A instance\n    # 1 query to delete the R instance\n    self.assertNumQueries(2, r.delete)\n    self.assertFalse(A.objects.exists())\n    self.assertFalse(R.objects.exists())\n", "    def test_fast_delete_with_signal_listeners(self):\n        \"\"\"\n        Objects with signal listeners cannot be fast-deleted.\n        \"\"\"\n            pass\n\n        models.signals.post_delete.connect(noop, sender=User)\n\n        a = Avatar.objects.create()\n        User.objects.create(avatar=a)\n\n        collector = Collector(using='default')\n        self.assertFalse(collector.can_fast_delete(a))\n\n        # Even though Avatar can be fast-deleted, the User has a signal listener\n        # so we need to fetch the objects into memory and delete them.\n        self.assertNumQueries(2, a.delete)\n        self.assertFalse(User.objects.exists())\n        self.assertFalse(Avatar.objects.exists())\n\n        models.signals.post_delete.disconnect(noop, sender=User)\n", "def test_fast_delete_o2o(self):\n    a = create_a('o2o_fastdelete')\n    a.o2o_fastdelete.delete()\n    a = A.objects.get(pk=a.pk)\n    self.assertIsNone(a.o2o_fastdelete)\n", "def test_fast_delete_m2m_through(self):\n    t = M2MTo.objects.create()\n    f = M2MFrom.objects.create()\n    f.m2m.add(t)\n    # 1 to delete t, 1 to fast-delete m2m_through relationship\n    self.assertNumQueries(2, t.delete)\n", "def test_fast_delete_relational_signals(self):\n    user_ids = []\n\n        user_ids.append(kwargs['instance'].id)\n\n    a = Avatar.objects.create()\n    User.objects.create(avatar=a)\n\n    models.signals.pre_delete.connect(log_pre_delete, sender=User)\n\n    try:\n        a.delete()\n    finally:\n        models.signals.pre_delete.disconnect(log_pre_delete, sender=User)\n\n    self.assertEqual(len(user_ids), 1)\n"], "sample_98": ["def test_check_array_memmap_copy_on_write():\n    X = np.ones((4, 4))\n    with TempMemmap(X, mmap_mode='r+') as X_memmap:\n        X_checked = check_array(X_memmap, copy=True)\n        assert not np.may_share_memory(X_memmap, X_checked)\n        assert X_checked.flags['WRITEABLE']\n", "def test_check_array_accept_sparse_types(accept_sparse):\n    X = [[1, 2], [3, 4]]\n    X_csr = sp.csr_matrix(X)\n\n    X_checked = check_array(X_csr, accept_sparse=accept_sparse)\n    if accept_sparse is False:\n        assert isinstance(X_checked, np.ndarray)\n    else:\n        assert sp.issparse(X_checked)\n", "def test_check_array_allow_nd_message():\n    X = np.ones((1, 2, 3, 4))\n    y = np.ones(1)\n    msg = \"Found array with dim 4. Estimator expected <= 2.\"\n    assert_raise_message(ValueError, msg, check_X_y, X, y, allow_nd=False)\n", "def test_check_array_different_dtypes_warning():\n    # Test that DataConversionWarning is raised when input DataFrame has different dtypes\n    pd = importorskip(\"pandas\")\n\n    df_different_dtypes = pd.DataFrame([[1, '2', 3], [4, '5', 6]])\n    assert_warns(DataConversionWarning, check_array, df_different_dtypes,\n                 dtype='numeric', warn_on_dtype=True)\n", "def test_check_array_large_indices_no_exception():\n    X = sp.rand(20, 10, format='csr')\n    X.indices = X.indices.astype('int64')\n    if LARGE_SPARSE_SUPPORTED:\n        # Large indices should be allowed if they are accepted\n        X_checked = check_array(X, accept_large_sparse=True, accept_sparse=True)\n        assert_equal(X_checked.indices.dtype, np.int64)\n"], "sample_120": ["def test_MatrixElement_with_symbolic_indices():\n    i, j = symbols(\"i j\")\n    A = MatrixSymbol(\"A\", n, n)\n    Aij = A[i, j]\n    assert isinstance(Aij, MatrixElement)\n    assert Aij.parent == A\n    assert Aij.i == i\n    assert Aij.j == j\n    assert Aij.diff(A[i, j]) == 1\n    assert Aij.diff(A[k, l]) == KroneckerDelta(i, k)*KroneckerDelta(j, l)\n", "def test_MatrixElement_subs():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', n, m)\n    assert A[n, m].subs(A, B) == B[n, m]\n    assert A[n, m].subs(n, k) == A[k, m]\n    assert A[n, m].subs(m, l) == A[n, l]\n", "def test_MatrixSymbol_indexing():\n    A = MatrixSymbol('A', n, m)\n    assert isinstance(A[0, 0], MatrixElement)\n    assert A[0, 0].parent == A\n    assert A[0, 0].i == 0\n    assert A[0, 0].j == 0\n\n    B = MatrixSymbol('B', n, m)\n    assert A[0, 0] != B[0, 0]\n\n    assert isinstance(A[n, m], MatrixElement)\n    assert A[n, m].parent == A\n    assert A[n, m].i == n\n    assert A[n, m].j == m\n\n    raises(IndexError, lambda: A[n, m+1])\n    raises(IndexError, lambda: A[n+1, m])\n    raises(IndexError, lambda: A[-1, m])\n    raises(IndexError, lambda: A[n, -1])\n", "def test_matrix_symbols():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', m, l)\n    C = MatrixSymbol('C', n, n)\n\n    expr = A * B * C.T\n    assert matrix_symbols(expr) == [A, B, C]\n", "def test_matrix_conversion():\n    A = ImmutableMatrix([[1, 2], [3, 4]])\n    B = A.as_mutable()\n    assert A.equals(B)\n    assert A == B\n    assert B.as_immutable() == A\n    assert B.as_immutable() is not A\n\n    A = ImmutableMatrix([[1, 2], [3, 4]])\n    B = A.as_mutable()\n    B[0, 0] = 5\n    assert A != B\n    assert not A.equals(B)\n"], "sample_104": ["def test_simple_imputer():\n    # Render a SimpleImputer object\n    imputer = SimpleImputer(missing_values=0, strategy='mean', fill_value=None, verbose=1, copy=False)\n    expected = \"\"\"", "def test_simple_imputer():\n    # Test the representation of SimpleImputer\n    imputer = SimpleImputer()\n    expected = \"\"\"", "def test_no_indent_at_name():\n    # Test with indent_at_name=False\n    pp = _EstimatorPrettyPrinter(compact=True, indent=2, indent_at_name=False)\n    lr = LogisticRegression(C=999)\n    expected = \"\"\"\n  LogisticRegression(C=999, class_weight=None, dual=False, fit_intercept=True,\n                     intercept_scaling=1, l1_ratio=None, max_iter=100,\n                     multi_class='warn', n_jobs=None, penalty='l2',\n                     random_state=None, solver='warn', tol=0.0001, verbose=0,\n                     warm_start=False)\"\"\"\n\n    expected = expected[1:]  # remove first \\n\n    assert pp.pformat(lr) == expected\n", "def test_tuple_repr():\n    # Test that tuples are correctly repr'd\n    param_grid = {'C': (1, 10, 100, 1000)}\n    gs = GridSearchCV(SVC(), param_grid)\n    expected = \"\"\"", "def test_gridsearch_deep_pipeline():\n    # render a deeply nested pipeline inside a gridsearch\n    pp = _EstimatorPrettyPrinter(compact=True, indent=1, indent_at_name=True)\n\n    pipeline = Pipeline([\n        ('reduce_dim', Pipeline([\n            ('pca', PCA()),\n            ('nmf', NMF())\n        ])),\n        ('classify', SVC())\n    ])\n    N_FEATURES_OPTIONS = [2, 4, 8]\n    C_OPTIONS = [1, 10, 100, 1000]\n    param_grid = [\n        {\n            'reduce_dim__pca__iterated_power': [5, 7],\n            'reduce_dim__pca__n_components': N_FEATURES_OPTIONS,\n            'reduce_dim__nmf__n_components': N_FEATURES_OPTIONS,\n            'classify__C': C_OPTIONS\n        },\n        {\n            'reduce_dim': [SelectKBest(chi2)],\n            'reduce_dim__k': N_FEATURES_OPTIONS,\n            'classify__C': C_OPTIONS\n        }\n    ]\n    gspipline = GridSearchCV(pipeline, cv=3, n_jobs=1, param_grid=param_grid)\n    expected = \"\"\""], "sample_87": ["def test_collector_respects_tbstyle_short(testdir):\n    p1 = testdir.makepyfile(\"assert 0\")\n    result = testdir.runpytest(p1, \"--tb=short\")\n    assert result.ret == ExitCode.INTERRUPTED\n    result.stdout.fnmatch_lines(\n        [\n            \"*_ ERROR collecting test_collector_respects_tbstyle_short.py _*\",\n            '  File \"*/test_collector_respects_tbstyle_short.py\", line 1, in <module>',\n            \"    assert 0\",\n            \"AssertionError: assert 0\",\n            \"*! Interrupted: 1 error during collection !*\",\n            \"*= 1 error in *\",\n        ]\n    )\n", "def test_collect_only_with_markers(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.skip(reason=\"always skip\")\n            assert False\n\n        @pytest.mark.xfail(reason=\"always xfail\")\n            assert False\n\n            assert True\n        \"\"\"\n    )\n\n    result = testdir.runpytest(\"--collect-only\", \"-m\", \"passed\")\n    result.stdout.fnmatch_lines([\"collected 1 item\", \"*test_passed*\"])\n    result.stdout.no_fnmatch_line(\"*test_skipped*\")\n    result.stdout.no_fnmatch_line(\"*test_xfailed*\")\n\n    result = testdir.runpytest(\"--collect-only\", \"-m\", \"xfail\")\n    result.stdout.fnmatch_lines([\"collected 1 item\", \"*test_xfailed*\"])\n    result.stdout.no_fnmatch_line(\"*test_skipped*\")\n    result.stdout.no_fnmatch_line(\"*test_passed*\")\n\n    result = testdir.runpytest(\"--collect-only\", \"-m\", \"skip\")\n    result.stdout.fnmatch_lines([\"collected 1 item\", \"*test_skipped*\"])\n    result.stdout.no_fnmatch_line(\"*test_xfailed*\")\n    result.stdout.no_fnmatch_line(\"*test_passed*\")\n\n    result = testdir.runpytest(\"--collect-only\", \"-m\", \"not passed\")\n    result.stdout.fnmatch_lines([\"collected 2 items\", \"*test_skipped*\", \"*test_xfailed*\"])\n    result.stdout.no_fnmatch_line(\"*test_passed*\")\n", "def test_collect_with_conftest_error(testdir):\n    testdir.makeconftest(\n        \"\"\"\n        import pytest\n            raise ValueError(\"Error in conftest\")\n    \"\"\"\n    )\n    p = testdir.makepyfile(\"def test_func(): pass\")\n    result = testdir.runpytest()\n    assert result.ret == ExitCode.INTERNAL_ERROR\n    result.stdout.fnmatch_lines([\"*ValueError: Error in conftest*\"])\n", "def test_collect_subdir_event_ordering_with_duplicates(self, testdir):\n    p = testdir.makepyfile(\"def test_func(): pass\")\n    aaa = testdir.mkpydir(\"aaa\")\n    test_aaa = aaa.join(\"test_aaa.py\")\n    p.copy(test_aaa)\n    p.move(aaa.join(\"test_aaa2.py\"))\n\n    id = \".\"\n\n    items, hookrec = testdir.inline_genitems(id)\n    assert len(items) == 2\n    pprint.pprint(hookrec.calls)\n    hookrec.assert_contains(\n        [\n            (\"pytest_collectstart\", \"collector.fspath == test_aaa\"),\n            (\"pytest_pycollect_makeitem\", \"name == 'test_func'\"),\n            (\"pytest_collectreport\", \"report.nodeid.startswith('aaa/test_aaa.py')\"),\n            (\"pytest_collectstart\", \"collector.fspath == aaa.join('test_aaa2.py')\"),\n            (\"pytest_pycollect_makeitem\", \"name == 'test_func'\"),\n            (\"pytest_collectreport\", \"report.nodeid.startswith('aaa/test_aaa2.py')\"),\n        ]\n    )\n", "def test_collect_with_confcutdir(testdir):\n    \"\"\"Check that --confcutdir works and is considered when setting up conftests (#4614).\"\"\"\n    sub = testdir.mkdir(\"sub\")\n    sub.ensure(\"__init__.py\")\n    sub.ensure(\"conftest.py\").write(\"def pytest_configure(config): assert 0, 'should_not_be_called'\")\n    sub.ensure(\"test_file.py\").write(\"def test_file(): pass\")\n\n    result = testdir.runpytest(\"--confcutdir=%s\" % testdir.tmpdir, \"-v\", str(sub))\n    result.stdout.fnmatch_lines([\"sub/test_file.py::test_file PASSED*\", \"*1 passed in*\"])\n    assert result.ret == 0\n"], "sample_78": ["def test_run_cert_path_does_not_exist():\n    # non-existent cert file\n    with pytest.raises(click.BadParameter):\n        run_command.make_context(\"run\", [\"--cert\", \"non-existent-file\"])\n\n    # non-existent key file\n    with pytest.raises(click.BadParameter):\n        run_command.make_context(\"run\", [\"--cert\", __file__, \"--key\", \"non-existent-file\"])\n", "def test_run_cert_key(monkeypatch):\n    # key without cert\n    with pytest.raises(click.BadParameter):\n        run_command.make_context(\"run\", [\"--key\", __file__])\n\n    # key without cert\n    with pytest.raises(click.BadParameter):\n        run_command.make_context(\"run\", [\"--key\", __file__, \"--cert\", \"adhoc\"])\n\n    # key without cert\n    ssl_context = ssl.SSLContext(ssl.PROTOCOL_TLS_SERVER)\n    monkeypatch.setitem(sys.modules, \"ssl_context\", ssl_context)\n    with pytest.raises(click.BadParameter):\n        run_command.make_context(\"run\", [\"--key\", __file__, \"--cert\", \"ssl_context\"])\n", "def test_run_debug(monkeypatch, runner):\n        app = Flask(__name__)\n        app.debug = True\n        return app\n\n    @click.group(cls=FlaskGroup, create_app=create_app)\n        pass\n\n    @cli.command()\n        click.echo(str(current_app.debug))\n\n    result = runner.invoke(cli, [\"test\"])\n    assert result.exit_code == 0\n    assert result.output == \"False\\n\"\n\n    monkeypatch.setenv(\"FLASK_DEBUG\", \"1\")\n    result = runner.invoke(cli, [\"test\"])\n    assert result.exit_code == 0\n    assert result.output == \"True\\n\"\n", "def test_run_debug(runner):\n    @click.command()\n    @click.pass_context\n        app = ctx.obj.load_app()\n        return app.debug\n\n    cli = FlaskGroup(create_app=lambda: Flask(\"testapp\", debug=True))\n    result = runner.invoke(cli, [\"check\"])\n    assert result.exit_code == 0\n    assert result.return_value is True\n\n    cli = FlaskGroup(create_app=lambda: Flask(\"testapp\", debug=False))\n    result = runner.invoke(cli, [\"check\"])\n    assert result.exit_code == 0\n    assert result.return_value is False\n", "def test_flaskgroup_plugin_commands(runner):\n    class FakeEntryPoint:\n            self.name = name\n            self.module_name = module_name\n\n            module = types.ModuleType(self.module_name)\n            module.cli = click.Group(self.name)\n\n            @module.cli.command()\n                click.echo(f\"{self.name}_command_result\")\n\n            return module.cli\n\n    fake_entry_points = [\n        FakeEntryPoint(\"plugin1\", \"plugin1\"),\n        FakeEntryPoint(\"plugin2\", \"plugin2\"),\n    ]\n\n        if group == \"flask.commands\":\n            return fake_entry_points\n        return []\n\n    with mock.patch(\"flask.cli.metadata.entry_points\", side_effect=mock_entry_points):\n        cli = FlaskGroup()\n        result = runner.invoke(cli, [\"plugin1\", \"test\"])\n        assert result.exit_code == 0\n        assert result.output == \"plugin1_command_result\\n\"\n\n        result = runner.invoke(cli, [\"plugin2\", \"test\"])\n        assert result.exit_code == 0\n        assert result.output == \"plugin2_command_result\\n\"\n"], "sample_92": ["def test_skipif_class_with_reason(self, testdir):\n    (item,) = testdir.getitems(\n        \"\"\"\n        import pytest\n        class TestClass(object):\n            pytestmark = pytest.mark.skipif(\"config._hackxyz\", reason=\"test reason\")\n                pass\n    \"\"\"\n    )\n    item.config._hackxyz = 3\n    skipped = evaluate_skip_marks(item)\n    assert skipped\n    assert skipped.reason == \"test reason\"\n", "def test_skipif_with_invalid_syntax(self, testdir):\n    item = testdir.getitem(\n        \"\"\"\n        import pytest\n        @pytest.mark.skipif(\"syntax error\")\n            pass\n    \"\"\"\n    )\n    with pytest.raises(pytest.fail.Exception) as excinfo:\n        evaluate_skip_marks(item)\n    assert excinfo.value.msg is not None\n    assert \"Error evaluating 'skipif' condition\" in excinfo.value.msg\n    assert \"SyntaxError: invalid syntax\" in excinfo.value.msg\n", "def test_xfail_condition_invalid_boolean(self, testdir) -> None:\n    p = testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        class InvalidBool:\n                raise TypeError(\"INVALID\")\n\n        @pytest.mark.xfail(InvalidBool(), reason=\"xxx\")\n            pass\n    \"\"\"\n    )\n    with pytest.raises(pytest.fail.Exception) as excinfo:\n        testdir.runpytest(p)\n    assert excinfo.value.msg is not None\n    assert \"Error evaluating 'xfail' condition as a boolean\" in excinfo.value.msg\n    assert \"INVALID\" in excinfo.value.msg\n", "def test_xfail_with_class_marker(self, testdir):\n    item = testdir.getitem(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.xfail\n        class TestXFail:\n                assert 0\n        \"\"\"\n    )\n    reports = runtestprotocol(item, log=False)\n    assert len(reports) == 3\n    callreport = reports[1]\n    assert callreport.skipped\n    assert callreport.wasxfail == \"\"\n", "def test_skipif_with_invalid_syntax(self, testdir) -> None:\n    item = testdir.getitem(\n        \"\"\"\n        import pytest\n        @pytest.mark.skipif(\"this is invalid syntax\", reason=\"xxx\")\n            pass\n    \"\"\"\n    )\n    with pytest.raises(pytest.fail.Exception) as excinfo:\n        evaluate_skip_marks(item)\n    assert excinfo.value.msg is not None\n    assert \"Error evaluating 'skipif' condition\" in excinfo.value.msg\n    assert \"SyntaxError: invalid syntax\" in excinfo.value.msg\n"], "sample_107": ["def test_logistic_regression_path_coefs_ovr():\n    # Make sure that the returned coefs by logistic_regression_path when\n    # multi_class='ovr' don't override each other (used to be a bug).\n    X, y = make_classification(n_samples=200, n_classes=3, n_informative=2,\n                               n_redundant=0, n_clusters_per_class=1,\n                               random_state=0, n_features=2)\n    Cs = [.00001, 1, 10000]\n    coefs, _, _ = _logistic_regression_path(X, y, penalty='l1', Cs=Cs,\n                                            solver='saga', random_state=0,\n                                            multi_class='ovr')\n\n    with pytest.raises(AssertionError):\n        assert_array_almost_equal(coefs[0], coefs[1], decimal=1)\n    with pytest.raises(AssertionError):\n        assert_array_almost_equal(coefs[0], coefs[2], decimal=1)\n    with pytest.raises(AssertionError):\n        assert_array_almost_equal(coefs[1], coefs[2], decimal=1)\n", "def test_logistic_regression_predict_proba_binary():\n    X, y = make_classification(n_samples=10, n_features=20, random_state=0, n_classes=2, n_informative=10)\n    clf = LogisticRegression(solver=\"liblinear\")\n    clf.fit(X, y)\n    probabilities = clf.predict_proba(X)\n    assert_equal(probabilities.shape, (10, 2))\n    assert_array_almost_equal(probabilities.sum(axis=1), np.ones(10))\n", "def test_elastic_net_coeffs_multinomial():\n    # make sure elasticnet penalty gives different coefficients from l1 and l2\n    # with saga solver (l1_ratio different from 0 or 1) for multinomial\n    X, y = make_classification(random_state=0, n_samples=100, n_classes=3, n_features=20, n_informative=10)\n\n    C = 2.\n    l1_ratio = .5\n    coeffs = list()\n    for penalty in ('elasticnet', 'l1', 'l2'):\n        lr = LogisticRegression(penalty=penalty, C=C, solver='saga',\n                                random_state=0, l1_ratio=l1_ratio, multi_class='multinomial')\n        lr.fit(X, y)\n        coeffs.append(lr.coef_)\n\n    elastic_net_coeffs, l1_coeffs, l2_coeffs = coeffs\n    # make sure coeffs differ by at least .1\n    assert not np.allclose(elastic_net_coeffs, l1_coeffs, rtol=0, atol=.1)\n    assert not np.allclose(elastic_net_coeffs, l2_coeffs, rtol=0, atol=.1)\n    assert not np.allclose(l2_coeffs, l1_coeffs, rtol=0, atol=.1)\n", "def test_intercept_scaling_non_default(solver):\n    # Make sure intercept_scaling is ignored when it is set to the default value\n    X, y = make_classification(n_samples=1000, random_state=0)\n\n    lr_default = LogisticRegression(solver=solver, random_state=0)\n    lr_custom = LogisticRegression(solver=solver, intercept_scaling=2.0, random_state=0)\n    pred_default = lr_default.fit(X, y).predict(X)\n    pred_custom = lr_custom.fit(X, y).predict(X)\n    assert_array_equal(pred_default, pred_custom)\n", "def test_logistic_regression_path_binary_classification():\n    # Test that logistic_regression_path returns correct results for binary classification\n    X, y = make_classification(n_samples=100, n_features=20, random_state=0)\n    y = np.where(y == 0, -1, 1)\n    coefs, Cs, n_iters = logistic_regression_path(X, y, Cs=10)\n    assert coefs.shape == (21, 10)  # 20 features + 1 intercept\n    assert Cs.shape == (10,)\n    assert n_iters.shape == (10,)\n"], "sample_45": ["def test_cache_control_decorator(self):\n    @cache_control(max_age=3600, public=True)\n        return HttpResponse()\n    r = a_view(HttpRequest())\n    self.assertEqual(r.headers['Cache-Control'], 'max-age=3600, public')\n", "def test_classonlymethod(self):\n    \"\"\"\n    Test the classonlymethod decorator.\n    \"\"\"\n    class TestClass:\n        @classonlymethod\n            return \"Class method called\"\n\n    self.assertEqual(TestClass.class_method(), \"Class method called\")\n    with self.assertRaises(AttributeError):\n        TestClass().class_method()\n", "    def test_cache_page_decorator_http_request(self):\n        class MyClass:\n            @cache_page(123)\n                return HttpResponse()\n\n        msg = (\n            \"cache_page didn't receive an HttpRequest. If you are \"\n            \"decorating a classmethod, be sure to use @method_decorator.\"\n        )\n        with self.assertRaisesMessage(TypeError, msg):\n            MyClass().a_view(HttpRequest())\n", "def test_xframe_options_middleware(self):\n    \"\"\"\n    Test for XFrameOptionsMiddleware.\n    \"\"\"\n        return HttpResponse(\"OK\")\n    my_view_deny = xframe_options_deny(my_view)\n    my_view_sameorigin = xframe_options_sameorigin(my_view)\n    my_view_exempt = xframe_options_exempt(my_view)\n    request = HttpRequest()\n    response = my_view_deny(request)\n    self.assertEqual(response['X-Frame-Options'], 'DENY')\n    response = my_view_sameorigin(request)\n    self.assertEqual(response['X-Frame-Options'], 'SAMEORIGIN')\n    response = my_view_exempt(request)\n    self.assertNotIn('X-Frame-Options', response)\n", "def test_cache_page_cache_control(self):\n        return \"response\"\n    my_view_cached = cache_page(123, cache=\"private\")(my_view)\n    request = HttpRequest()\n    response = my_view_cached(request)\n    self.assertEqual(response['Cache-Control'], 'private, max-age=123')\n"], "sample_100": ["def test_one_hot_encoder_sparse_input():\n    X = sparse.csr_matrix([[0, 2], [1, 1], [2, 0]])\n    enc = OneHotEncoder(categories=[[0, 1, 2], [0, 1]])\n    enc.fit(X)\n    X_tr = enc.transform(X)\n    exp = np.array([[1, 0, 0, 0, 1], [0, 1, 0, 1, 0], [0, 0, 1, 0, 1]])\n    assert_array_equal(X_tr.toarray(), exp)\n", "def test_one_hot_encoder_categorical_features_error():\n    X = np.array([[3, 2, 1], [0, 1, 1]])\n    cat = [True, False, 'all']\n    with pytest.raises(ValueError, match=\"categorical_features should be all or an array of indices\"):\n        _check_one_hot(X, X, cat, 5)\n", "def test_one_hot_encoder_warning_on_deprecated_params():\n    # test warning when using deprecated parameters\n    X = [['Male', 1], ['Female', 3]]\n    enc = OneHotEncoder(n_values=3)\n    with pytest.warns(DeprecationWarning, match=\"Passing 'n_values' is deprecated\"):\n        enc.fit_transform(X)\n\n    enc = OneHotEncoder(categorical_features='all')\n    with pytest.warns(DeprecationWarning, match=\"The 'categorical_features' keyword is deprecated\"):\n        enc.fit_transform(X)\n", "def test_one_hot_encoder_unsorted_categories_numeric():\n    X = np.array([[1, 2]]).T\n\n    enc = OneHotEncoder(categories=[[2, 1, 3]])\n    exp = np.array([[0., 1., 0.],\n                    [1., 0., 0.]])\n    assert_array_equal(enc.fit(X).transform(X).toarray(), exp)\n    assert enc.categories_[0].tolist() == [2, 1, 3]\n    assert np.issubdtype(enc.categories_[0].dtype, np.integer)\n", "def test_one_hot_encoder_handle_unknown_error():\n    X = np.array([[0, 2, 1], [1, 0, 3], [1, 0, 2]])\n    X2 = np.array([[4, 1, 1]])\n\n    # Test that one hot encoder raises error for unknown features\n    # present during transform.\n    oh = OneHotEncoder(handle_unknown='error')\n    assert_warns(FutureWarning, oh.fit, X)\n    assert_raises(ValueError, oh.transform, X2)\n\n    # Test the ignore option, ignores unknown features (giving all 0's)\n    oh = OneHotEncoder(handle_unknown='ignore')\n    oh.fit(X)\n    X2_transformed = oh.transform(X2)\n    assert_array_equal(X2_transformed.toarray(), np.array([[0., 0., 0., 1., 0.]]))\n"], "sample_77": ["def test_color_tuple_values_temporal(self, t):\n\n    cmap = color_palette(\"blend:b,g\", as_cmap=True)\n    s = Temporal((\"b\", \"g\"))._setup(t, Color())\n    normed = (t - t.min()) / (t.max() - t.min())\n    assert_array_equal(s(t), cmap(normed)[:, :3])  # FIXME RGBA\n", "def test_color_with_transform(self, t):\n\n    t = pd.Series(pd.to_datetime([\"1972-09-27\", \"1975-06-24\", \"1980-12-14\"]), name=\"x\")\n    cmap = color_palette(\"ch:\", as_cmap=True)\n    s = Temporal(trans=\"log\")._setup(t, Color())\n    x = mpl.dates.date2num(t)\n    normed = (np.log10(x) - np.log10(x.min())) / (np.log10(x.max()) - np.log10(x.min()))\n    assert_array_equal(s(t), cmap(normed)[:, :3])  # FIXME RGBA\n", "def test_label_concise_format(self, t, x):\n\n    ax = mpl.figure.Figure().subplots()\n    Temporal().label(concise=True)._setup(t, Coordinate(), ax.xaxis)\n    formatter = ax.xaxis.get_major_formatter()\n    labels = formatter.format_ticks(x)\n    for label in labels:\n        assert len(label) > 0  # Ensure the labels are not empty\n", "def test_tick_locator_input_check(self, t):\n    err = \"Tick locator must be an instance of .*?, not <class 'tuple'>.\"\n    with pytest.raises(TypeError, match=err):\n        Temporal().tick((1, 2))\n", "def test_coordinate_numeric_data_with_order_out_of_range(self, y):\n\n    order = [5, 6, -1.5]\n    ax = mpl.figure.Figure().subplots()\n    s = Nominal(order=order)._setup(y, Coordinate(), ax.yaxis)\n    assert_array_equal(s(y), np.array([np.nan, 2, np.nan, 2], float))\n    f = ax.yaxis.get_major_formatter()\n    assert f.format_ticks([0, 1, 2]) == [\"5.0\", \"6.0\", \"-1.5\"]\n"], "sample_68": ["def test_set_pk_and_query_efficiency_with_db_column(self):\n    with self.assertNumQueries(1):\n        fields_with_db_columns = FieldsWithDbColumns.objects.bulk_create(\n            [FieldsWithDbColumns(rank=i, name=f\"name{i}\") for i in range(1, 5)]\n        )\n    self.assertEqual(len(fields_with_db_columns), 4)\n    self.assertEqual(\n        FieldsWithDbColumns.objects.get(pk=fields_with_db_columns[0].pk),\n        fields_with_db_columns[0],\n    )\n", "def test_update_conflicts_unique_fields_multiple(self):\n    Country.objects.bulk_create(self.data)\n    self.assertEqual(Country.objects.count(), 4)\n\n    new_data = [\n        Country(\n            name=\"United States of America\",\n            iso_two_letter=\"US\",\n            description=\"Updated description for USA\",\n        ),\n        Country(\n            name=\"The Netherlands\",\n            iso_two_letter=\"NL\",\n            description=\"Updated description for NL\",\n        ),\n        Country(name=\"France\", iso_two_letter=\"FR\", description=\"New country description\"),\n    ]\n    results = Country.objects.bulk_create(\n        new_data,\n        update_conflicts=True,\n        update_fields=[\"description\"],\n        unique_fields=[\"iso_two_letter\", \"name\"],\n    )\n    self.assertEqual(len(results), len(new_data))\n    if connection.features.can_return_rows_from_bulk_insert:\n        for instance in results:\n            self.assertIsNotNone(instance.pk)\n    self.assertEqual(Country.objects.count(), 5)\n    self.assertCountEqual(\n        Country.objects.values(\"iso_two_letter\", \"description\"),\n        [\n            {\"iso_two_letter\": \"US\", \"description\": \"Updated description for USA\"},\n            {\"iso_two_letter\": \"NL\", \"description\": \"Updated description for NL\"},\n            {\"iso_two_letter\": \"DE\", \"description\": \"\"},\n            {\"iso_two_letter\": \"CZ\", \"description\": \"\"},\n            {\"iso_two_letter\": \"FR\", \"description\": \"New country description\"},\n        ],\n    )\n", "def test_ignore_update_conflicts_no_conflicts(self):\n    data = [\n        TwoFields(f1=1, f2=1),\n        TwoFields(f1=2, f2=2),\n    ]\n    TwoFields.objects.bulk_create(data)\n    self.assertEqual(TwoFields.objects.count(), 2)\n\n    # With ignore_conflicts=True, but no conflicts, all objects are created.\n    new_objects = [\n        TwoFields(f1=3, f2=3),\n        TwoFields(f1=4, f2=4),\n    ]\n    created_objects = TwoFields.objects.bulk_create(new_objects, ignore_conflicts=True)\n    self.assertEqual(len(created_objects), len(new_objects))\n    self.assertEqual(TwoFields.objects.count(), 4)\n\n    # With update_conflicts=True, but no conflicts, all objects are created.\n    created_objects = TwoFields.objects.bulk_create(\n        new_objects, update_conflicts=True, update_fields=[\"f2\"]\n    )\n    self.assertEqual(len(created_objects), len(new_objects))\n    self.assertEqual(TwoFields.objects.count(), 6)\n", "def test_update_conflicts_with_auto_field(self):\n    BigAutoFieldModel.objects.bulk_create([BigAutoFieldModel() for _ in range(3)])\n    self.assertEqual(BigAutoFieldModel.objects.count(), 3)\n\n    obj1, obj2, obj3 = BigAutoFieldModel.objects.all()\n    conflicting_objects = [\n        BigAutoFieldModel(id=obj1.id, data=\"c1\"),\n        BigAutoFieldModel(id=obj2.id, data=\"c2\"),\n    ]\n    results = BigAutoFieldModel.objects.bulk_create(\n        conflicting_objects + [BigAutoFieldModel(data=\"new\")],\n        update_conflicts=True,\n        update_fields=[\"data\"],\n        unique_fields=[\"id\"],\n    )\n    self.assertEqual(len(results), 3)\n    if connection.features.can_return_rows_from_bulk_insert:\n        for instance in results:\n            self.assertIsNotNone(instance.pk)\n    self.assertEqual(BigAutoFieldModel.objects.count(), 4)\n    self.assertCountEqual(\n        BigAutoFieldModel.objects.values(\"id\", \"data\"),\n        [\n            {\"id\": obj1.id, \"data\": \"c1\"},\n            {\"id\": obj2.id, \"data\": \"c2\"},\n            {\"id\": obj3.id, \"data\": \"\"},\n            {\"id\": results[2].id, \"data\": \"new\"},\n        ],\n    )\n", "def test_update_conflicts_unique_fields_invalid(self):\n    msg = \"Unique fields that can trigger the upsert must be provided.\"\n    with self.assertRaisesMessage(ValueError, msg):\n        TwoFields.objects.bulk_create(\n            [TwoFields(f1=1, f2=1), TwoFields(f1=2, f2=2)],\n            update_conflicts=True,\n            update_fields=[\"f1\"],\n            unique_fields=[],\n        )\n"], "sample_14": ["def test_serialize_custom_object(self):\n    class CustomObject:\n            self.value = value\n\n    custom_obj = CustomObject(\"test_value\")\n    self.assertSerializedResultEqual(\n        custom_obj,\n        (\"migrations.test_writer.CustomObject('test_value')\", {'import migrations.test_writer'})\n    )\n", "def test_serialize_time(self):\n    self.assertSerializedEqual(datetime.time(10, 30, 45))\n    self.assertSerializedEqual(datetime.time(hour=15, minute=15, second=15))\n    self.assertSerializedResultEqual(\n        datetime.time(10, 30, 45),\n        (\"datetime.time(10, 30, 45)\", {'import datetime'})\n    )\n    self.assertSerializedResultEqual(\n        datetime.time(hour=15, minute=15, second=15),\n        (\"datetime.time(hour=15, minute=15, second=15)\", {'import datetime'})\n    )\n", "def test_serialize_custom_deconstructable(self):\n    \"\"\"\n    Test serialization of custom deconstructable objects.\n    \"\"\"\n    custom_deconstructable = deconstructible(path=\"migrations.test_writer.CustomDeconstructable\")(DeconstructibleInstances)()\n    string = MigrationWriter.serialize(custom_deconstructable)[0]\n    self.assertEqual(string, \"migrations.test_writer.CustomDeconstructable()\")\n    self.serialize_round_trip(custom_deconstructable)\n", "def test_serialize_type_model(self):\n    self.assertSerializedEqual(models.Model)\n", "def test_serialize_datetime_tzinfo_none(self):\n    self.assertSerializedEqual(datetime.datetime(2014, 1, 1, 1, 1, tzinfo=None))\n    self.assertSerializedResultEqual(\n        datetime.datetime(2014, 1, 1, 1, 1, tzinfo=None),\n        (\"datetime.datetime(2014, 1, 1, 1, 1)\", {'import datetime'})\n    )\n"], "sample_57": ["def test_custom_form_prefix(self):\n    CustomPrefixChoiceFormSet = formset_factory(Choice, formset=BaseFormSet)\n    formset = CustomPrefixChoiceFormSet(prefix='custom')\n    self.assertEqual(formset.management_form.prefix, 'custom')\n", "def test_formset_with_custom_deletion_label(self):\n    \"\"\"FormSet with custom deletion field label.\"\"\"\n    data = {\n        \"choices-TOTAL_FORMS\": \"1\",\n        \"choices-INITIAL_FORMS\": \"1\",\n        \"choices-MIN_NUM_FORMS\": \"0\",\n        \"choices-MAX_NUM_FORMS\": \"1\",\n        \"choices-0-choice\": \"Calexico\",\n        \"choices-0-votes\": \"100\",\n        \"choices-0-DELETE\": \"on\",\n    }\n    ChoiceFormSet = formset_factory(Choice, can_delete=True, delete_label=\"Remove\")\n    formset = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n    self.assertTrue(formset.is_valid())\n    self.assertIn(\"Remove\", str(formset.forms[0]))\n", "def test_validate_max_not_called_when_min_validates(self):\n    \"\"\"validate_max is not called when validate_min is True and min_num is not validated.\"\"\"\n    ChoiceFormSet = formset_factory(Choice, min_num=2, validate_min=True)\n    data = {\n        \"choices-TOTAL_FORMS\": \"1\",\n        \"choices-INITIAL_FORMS\": \"0\",\n        \"choices-MIN_NUM_FORMS\": \"2\",\n        \"choices-MAX_NUM_FORMS\": \"0\",\n        \"choices-0-choice\": \"Calexico\",\n        \"choices-0-votes\": \"100\",\n    }\n    with mock.patch.object(ChoiceFormSet, 'validate_max', return_value=False) as mocked_validate_max:\n        formset = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n        mocked_validate_max.assert_not_called()\n", "def test_custom_management_form_class(self):\n    \"\"\"A custom management form class can be used.\"\"\"\n\n    class CustomManagementForm(ManagementForm):\n        pass\n\n    class CustomManagementFormSet(BaseFormSet):\n        management_form_class = CustomManagementForm\n\n    CustomFormSet = formset_factory(Choice, formset=CustomManagementFormSet)\n    formset = CustomFormSet()\n    self.assertIsInstance(formset.management_form, CustomManagementForm)\n", "def test_formset_invalid_management_form(self):\n    data = {\n        \"form-TOTAL_FORMS\": \"abc\",\n        \"form-INITIAL_FORMS\": \"def\",\n    }\n    formset = ArticleFormSet(data)\n    self.assertIs(formset.is_valid(), False)\n    self.assertEqual(\n        formset.non_form_errors(),\n        [\n            \"ManagementForm data is missing or has been tampered with. \"\n            \"Missing fields: form-TOTAL_FORMS, form-INITIAL_FORMS. \"\n            \"You may need to file a bug report if the issue persists.\",\n        ],\n    )\n    self.assertEqual(formset.errors, [])\n"], "sample_151": ["def test_direction_ratio():\n    p1 = Point3D(0, 0, 0)\n    p2 = Point3D(1, 1, 1)\n\n    assert p1.direction_ratio(Point3D(1, 0, 0)) == [1, 0, 0]\n    assert p1.direction_ratio(Point3D(0, 1, 0)) == [0, 1, 0]\n    assert p1.direction_ratio(Point3D(0, 0, pi)) == [0, 0, pi]\n\n    assert p1.direction_ratio(Point3D(5, 0, 0)) == [5, 0, 0]\n    assert p1.direction_ratio(Point3D(0, sqrt(3), 0)) == [0, sqrt(3), 0]\n    assert p1.direction_ratio(Point3D(0, 0, 5)) == [0, 0, 5]\n\n    assert p1.direction_ratio(Point3D(2.4, 2.4, 0)) == [2.4, 2.4, 0]\n    assert p1.direction_ratio(Point3D(1, 1, 1)) == [1, 1, 1]\n    assert p1.direction_ratio(Point3D(-12, 0 -15)) == [-12, 0, -15]\n\n    assert p2.direction_ratio(Point3D(0, 0, 0)) == [-1, -1, -1]\n    assert p2.direction_ratio(Point3D(1, 1, 12)) == [0, 0, 11]\n    assert p2.direction_ratio(Point3D(12, 1, 12)) == [11, 0, 11]\n", "def test_direction_ratio():\n    p1 = Point3D(0, 0, 0)\n    p2 = Point3D(1, 1, 1)\n\n    assert p1.direction_ratio(Point3D(1, 0, 0)) == [1, 0, 0]\n    assert p1.direction_ratio(Point3D(0, 1, 0)) == [0, 1, 0]\n    assert p1.direction_ratio(Point3D(0, 0, pi)) == [0, 0, pi]\n\n    assert p1.direction_ratio(Point3D(5, 0, 0)) == [5, 0, 0]\n    assert p1.direction_ratio(Point3D(0, sqrt(3), 0)) == [0, sqrt(3), 0]\n    assert p1.direction_ratio(Point3D(0, 0, 5)) == [0, 0, 5]\n\n    assert p1.direction_ratio(Point3D(2.4, 2.4, 0)) == [2.4, 2.4, 0]\n    assert p1.direction_ratio(Point3D(1, 1, 1)) == [1, 1, 1]\n    assert p1.direction_ratio(Point3D(-12, 0 -15)) == [-12, 0, -15]\n\n    assert p2.direction_ratio(Point3D(0, 0, 0)) == [-1, -1, -1]\n    assert p2.direction_ratio(Point3D(1, 1, 12)) == [0, 0, 11]\n    assert p2.direction_ratio(Point3D(12, 1, 12)) == [11, 0, 11]\n", "def test_direction_ratio():\n    p1 = Point3D(0, 0, 0)\n    p2 = Point3D(1, 1, 1)\n\n    assert p1.direction_ratio(Point3D(1, 0, 0)) == [1, 0, 0]\n    assert p1.direction_ratio(Point3D(0, 1, 0)) == [0, 1, 0]\n    assert p1.direction_ratio(Point3D(0, 0, pi)) == [0, 0, pi]\n\n    assert p1.direction_ratio(Point3D(5, 0, 0)) == [5, 0, 0]\n    assert p1.direction_ratio(Point3D(0, sqrt(3), 0)) == [0, sqrt(3), 0]\n    assert p1.direction_ratio(Point3D(0, 0, 5)) == [0, 0, 5]\n\n    assert p1.direction_ratio(Point3D(2.4, 2.4, 0)) == [2.4, 2.4, 0]\n    assert p1.direction_ratio(Point3D(1, 1, 1)) == [1, 1, 1]\n    assert p1.direction_ratio(Point3D(-12, 0 -15)) == [-12, -15, 0]\n\n    assert p2.direction_ratio(Point3D(0, 0, 0)) == [-1, -1, -1]\n    assert p2.direction_ratio(Point3D(1, 1, 12)) == [-1, -1, 11]\n    assert p2.direction_ratio(Point3D(12, 1, 12)) == [11, 0, 11]\n", "def test_direction_ratio():\n    p1 = Point3D(0, 0, 0)\n    p2 = Point3D(1, 1, 1)\n\n    assert p1.direction_ratio(Point3D(1, 0, 0)) == [1, 0, 0]\n    assert p1.direction_ratio(Point3D(0, 1, 0)) == [0, 1, 0]\n    assert p1.direction_ratio(Point3D(0, 0, 3)) == [0, 0, 3]\n\n    assert p1.direction_ratio(Point3D(5, 0, 0)) == [5, 0, 0]\n    assert p1.direction_ratio(Point3D(0, 3, 0)) == [0, 3, 0]\n    assert p1.direction_ratio(Point3D(0, 0, 5)) == [0, 0, 5]\n\n    assert p1.direction_ratio(Point3D(2.4, 2.4, 0)) == [2.4, 2.4, 0]\n    assert p1.direction_ratio(Point3D(1, 1, 1)) == [1, 1, 1]\n    assert p1.direction_ratio(Point3D(-12, 0, -15)) == [-12, 0, -15]\n\n    assert p2.direction_ratio(Point3D(0, 0, 0)) == [-1, -1, -1]\n    assert p2.direction_ratio(Point3D(1, 1, 12)) == [-1, -1, 11]\n    assert p2.direction_ratio(Point3D(12, 1, 12)) == [11, 0, 11]\n", "def test_direction_ratio():\n    p1 = Point3D(0, 0, 0)\n    p2 = Point3D(1, 1, 1)\n\n    assert p1.direction_ratio(Point3D(1, 0, 0)) == [1, 0, 0]\n    assert p1.direction_ratio(Point3D(0, 1, 0)) == [0, 1, 0]\n    assert p1.direction_ratio(Point3D(0, 0, pi)) == [0, 0, pi]\n\n    assert p1.direction_ratio(Point3D(5, 0, 0)) == [5, 0, 0]\n    assert p1.direction_ratio(Point3D(0, sqrt(3), 0)) == [0, sqrt(3), 0]\n    assert p1.direction_ratio(Point3D(0, 0, 5)) == [0, 0, 5]\n\n    assert p1.direction_ratio(Point3D(2.4, 2.4, 0)) == [2.4, 2.4, 0]\n    assert p1.direction_ratio(Point3D(1, 1, 1)) == [1, 1, 1]\n    assert p1.direction_ratio(Point3D(-12, 0 -15)) == [-12, -15, -15]\n\n    assert p2.direction_ratio(Point3D(0, 0, 0)) == [-1, -1, -1]\n    assert p2.direction_ratio(Point3D(1, 1, 12)) == [-1, -1, 11]\n    assert p2.direction_ratio(Point3D(12, 1, 12)) == [11, 0, 11]\n"], "sample_43": ["def test_custom_search_fields(self):\n    q1 = Question.objects.create(question='Python question')\n    q2 = Question.objects.create(question='Django question')\n\n    class CustomSearchAdmin(QuestionAdmin):\n        search_fields = ['question', 'answer__answer']\n\n    with model_admin(Question, CustomSearchAdmin):\n        a1 = Answer.objects.create(question=q1, answer='Python answer')\n        a2 = Answer.objects.create(question=q2, answer='Django answer')\n\n        request = self.factory.get(self.url, {'term': 'Django', **self.opts})\n        request.user = self.superuser\n        response = AutocompleteJsonView.as_view(**self.as_view_args)(request)\n        self.assertEqual(response.status_code, 200)\n        data = json.loads(response.content.decode('utf-8'))\n        self.assertEqual(data, {\n            'results': [{'id': str(q2.pk), 'text': q2.question}],\n            'pagination': {'more': False},\n        })\n\n        request = self.factory.get(self.url, {'term': 'answer', **self.opts})\n        request.user = self.superuser\n        response = AutocompleteJsonView.as_view(**self.as_view_args)(request)\n        self.assertEqual(response.status_code, 200)\n        data = json.loads(response.content.decode('utf-8'))\n        self.assertEqual(data, {\n            'results': [{'id': str(q1.pk), 'text': q1.question}, {'id': str(q2.pk), 'text': q2.question}],\n            'pagination': {'more': False},\n        })\n", "def test_malformed_request(self):\n    # Test when the request does not contain all required parameters.\n    request = self.factory.get(self.url, {'term': 'is'})\n    request.user = self.superuser\n    with self.assertRaises(PermissionDenied):\n        AutocompleteJsonView.as_view(**self.as_view_args)(request)\n", "    def test_invalid_model(self):\n        request = self.factory.get(self.url, {'term': 'is', **self.opts, 'model_name': 'invalid_model'})\n        request.user = self.superuser\n        with self.assertRaises(PermissionDenied):\n            AutocompleteJsonView.as_view(**self.as_view_args)(request)\n", "    def test_custom_admin_site(self):\n        custom_site = admin.AdminSite(name='custom_admin')\n        custom_site.register(Answer, AnswerAdmin)\n        custom_site.register(Question, QuestionAdmin)\n        opts = {\n            'app_label': Answer._meta.app_label,\n            'model_name': Answer._meta.model_name,\n            'field_name': 'question'\n        }\n        q = Question.objects.create(question='Is this a custom question?')\n        request = self.factory.get(self.url, {'term': 'is', **self.opts})\n        request.user = self.superuser\n        custom_as_view_args = {'admin_site': custom_site}\n        response = AutocompleteJsonView.as_view(**custom_as_view_args)(request)\n        self.assertEqual(response.status_code, 200)\n        data = json.loads(response.content.decode('utf-8'))\n        self.assertEqual(data, {\n            'results': [{'id': str(q.pk), 'text': q.question}],\n            'pagination': {'more': False},\n        })\n", "def test_admin_site_override(self):\n    \"\"\"\n    Check that the admin_site can be overridden in the view.\n    \"\"\"\n    # Create a new admin site\n    custom_site = admin.AdminSite(name='custom_admin')\n    custom_site.register(Question, QuestionAdmin)\n    custom_site.register(Answer, AnswerAdmin)\n\n    # Create a custom AutocompleteJsonView using the new admin site\n    class CustomAutocompleteJsonView(AutocompleteJsonView):\n        admin_site = custom_site\n\n    q = Question.objects.create(question='Is this a question?')\n    request = self.factory.get(self.url, {'term': 'is', **self.opts})\n    request.user = self.superuser\n    response = CustomAutocompleteJsonView.as_view()(request)\n    self.assertEqual(response.status_code, 200)\n    data = json.loads(response.content.decode('utf-8'))\n    self.assertEqual(data, {\n        'results': [{'id': str(q.pk), 'text': q.question}],\n        'pagination': {'more': False},\n    })\n"], "sample_38": ["    def test_integer_username(self):\n        data = {\n            'username': 123456,\n            'password1': 'test123',\n            'password2': 'test123',\n        }\n        form = UserCreationForm(data)\n        self.assertTrue(form.is_valid())\n        u = form.save()\n        self.assertEqual(u.username, '123456')\n", "def test_password_change_form_with_invalid_old_password(self):\n    user = User.objects.get(username='testclient')\n    data = {\n        'old_password': 'wrongpassword',\n        'new_password1': 'newpassword',\n        'new_password2': 'newpassword',\n    }\n    form = PasswordChangeForm(user, data)\n    self.assertFalse(form.is_valid())\n    self.assertEqual(form[\"old_password\"].errors, [str(form.error_messages['password_incorrect'])])\n", "def test_password_reset_form_validates_password(self):\n    user = User.objects.get(username='testclient')\n    data = {'email': user.email}\n    form = PasswordResetForm(data)\n    self.assertTrue(form.is_valid())\n    form.save()\n    self.assertEqual(len(mail.outbox), 1)\n    self.assertEqual(mail.outbox[0].to, [user.email])\n    self.assertNotIn(user.password, mail.outbox[0].message())\n", "    def test_custom_user(self):\n        user = CustomUser.objects.create_user(email='test@custom.com', password='test123', date_of_birth='1990-01-01')\n        data = {\n            'password1': 'newpassword',\n            'password2': 'newpassword',\n        }\n        form = AdminPasswordChangeForm(user, data)\n        self.assertTrue(form.is_valid())\n        form.save()\n        self.assertTrue(user.check_password('newpassword'))\n", "def test_html_autocomplete_attributes_autofocus(self):\n    user = User.objects.get(username='testclient')\n    form = AdminPasswordChangeForm(user)\n    self.assertEqual(form.fields['password1'].widget.attrs['autofocus'], 'autofocus')\n"], "sample_79": ["def test_concat_different_variables(self):\n    # Test concatenation with different variables in datasets\n    ds1 = Dataset({\"a\": (\"x\", [1, 2]), \"b\": (\"x\", [3, 4])})\n    ds2 = Dataset({\"a\": (\"x\", [5, 6]), \"c\": (\"x\", [7, 8])})\n    expected = Dataset({\"a\": (\"x\", [1, 2, 5, 6]), \"b\": (\"x\", [3, 4, np.nan, np.nan]), \"c\": (\"x\", [np.nan, np.nan, 7, 8])})\n    actual = concat([ds1, ds2], dim=\"x\", data_vars=\"all\")\n    assert_identical(expected, actual)\n", "def test_concat_dataarray_attributes(self):\n    foo = DataArray([1, 2], coords=[(\"x\", [1, 2])], attrs={\"attr1\": \"value1\"})\n    bar = DataArray([1, 2], coords=[(\"x\", [1, 3])], attrs={\"attr1\": \"value2\"})\n    with pytest.raises(ValueError):\n        concat((foo, bar), dim=\"y\", compat=\"identical\")\n\n    actual = concat((foo, bar), dim=\"y\", compat=\"equals\")\n    expected = DataArray(\n        [[1, 2, np.nan], [1, np.nan, 2]],\n        dims=[\"y\", \"x\"],\n        coords={\"x\": [1, 2, 3]},\n        attrs={\"attr1\": \"value1\"},\n    )\n    assert_identical(actual, expected)\n", "def test_concat_different_data_vars(self):\n    # Test concat with 'different' data_vars option\n    data1 = Dataset({\"foo\": (\"x\", [1, 2]), \"bar\": (\"x\", [3, 4])})\n    data2 = Dataset({\"foo\": (\"x\", [5, 6]), \"baz\": (\"x\", [7, 8])})\n    expected = Dataset({\"foo\": (\"x\", [1, 2, 5, 6]), \"bar\": (\"x\", [3, 4, np.nan, np.nan]), \"baz\": (\"x\", [np.nan, np.nan, 7, 8])})\n    actual = concat([data1, data2], dim=\"x\", data_vars=\"different\")\n    assert_identical(expected, actual)\n", "def test_concat_unlabeled_dimension(self):\n    # Test case for concatenating along an unlabeled dimension\n    ds1 = Dataset({\"foo\": ((\"x\", \"y\"), np.random.rand(2, 3))})\n    ds2 = Dataset({\"foo\": ((\"x\", \"y\"), np.random.rand(2, 3))})\n    expected = Dataset({\"foo\": ((\"concat_dim\", \"x\", \"y\"), np.concatenate([ds1.foo.data, ds2.foo.data]))}, coords={\"x\": ds1.x, \"y\": ds1.y})\n    actual = concat([ds1, ds2], dim=\"concat_dim\")\n    assert_identical(expected, actual)\n", "    def test_concat_attrs(self):\n        data = Dataset({\"foo\": (\"x\", np.random.randn(10))}, attrs={\"attr1\": \"value1\"})\n        split_data = [data.isel(x=slice(5)), data.isel(x=slice(5, None))]\n        split_data[1].attrs = {\"attr1\": \"value2\"}\n        with pytest.raises(ValueError, match=\"Dataset global attributes not equal.\"):\n            concat(split_data, \"x\", compat=\"identical\")\n        concat(split_data, \"x\", compat=\"equals\")\n"], "sample_135": ["def test_replace_pattern_expr():\n    x, y = symbols('x y')\n    e = (x**2 + x*y)\n    assert e.replace(x**2, y) == y + x*y\n    assert e.replace(x*y, x) == x**2 + x\n    assert e.replace(x, 2) == 4 + 2*y\n    assert e.replace(x + y, 2) == x**2 + x*y\n", "def test_replace_pattern_expr():\n    from sympy import symbols, Wild\n    x, y = symbols('x y')\n    a, b = Wild('a'), Wild('b')\n    expr = x**2 + y\n    assert expr.replace(a**2 + b, a - b) == x - y\n    assert expr.replace(a*x + b, a - b) == -y\n    assert expr.replace(a*x + b, a - b, exact=False) == x - y\n", "def test_as_real_imag():\n    assert S.One.as_real_imag() == (S.One, S.Zero)\n    assert (1 + I).as_real_imag() == (S.One, I)\n    assert (1 - I).as_real_imag() == (S.One, -I)\n    assert (2*I).as_real_imag() == (S.Zero, 2*I)\n    assert (-2*I).as_real_imag() == (S.Zero, -2*I)\n", "def test_replace():\n    x, y = symbols('x y')\n    e = x + y\n\n    # Replace x with 2\n    assert e.replace(x, 2) == 2 + y\n\n    # Replace y with a function of x\n    assert e.replace(y, sin(x)) == x + sin(x)\n\n    # Replace x + y with a function of x\n    a = Wild('a')\n    assert e.replace(x + y, cos(a)) == cos(x + y)\n\n    # Replace x with a function of y\n    assert e.replace(x, lambda y: y**2) == y**2 + y\n\n    # Replace x with a function of a wildcard\n    assert e.replace(x, lambda a: a**3) == x**3 + y\n\n    # Replace an expression containing more than one Wild symbol\n    a, b = map(Wild, 'ab')\n    assert (x*y).replace(a*x + b, b - a) == y - x\n\n    # Replace using a function\n    assert (2*x).replace(lambda expr: expr.is_Number, lambda expr: expr**2) == 4*x\n", "def test_replace_pattern_to_expr():\n    from sympy import Wild\n    x, y = symbols('x y')\n    a = Wild('a')\n    f = x**(1 + y)\n    assert f.replace(x**(1 + a), lambda a: x**-a, exact=False) == x**(1 - y)\n    assert f.replace(x**(1 + a), lambda a: x**-a, exact=True) == x**(1 - y)\n    g = x**y\n    assert g.replace(x**(1 + a), lambda a: x**-a, exact=False) == 1\n    assert g.replace(x**(1 + a), lambda a: x**-a, exact=True) == x**(1 - y)\n"], "sample_159": ["def test_prefix_division():\n    m = PREFIXES['m']\n    k = PREFIXES['k']\n\n    assert m / m is S.One\n    assert k / k is S.One\n    assert m / k == Rational(1, 1000)\n    assert k / m == 1000\n", "def test_prefix_division():\n    m = PREFIXES['m']\n    k = PREFIXES['k']\n    assert m / k == milli\n    assert k / m == 1000\n    assert k / k is S.One\n", "def test_prefix_multiplication():\n    assert kilo * kibi == 1024\n    assert kibi * kilo == 1024\n", "def test_prefix_combination():\n    k = PREFIXES['k']\n    M = PREFIXES['M']\n\n    assert k * M == PREFIXES['G']\n    assert M / k == PREFIXES['h']\n    assert k / M == PREFIXES['m']\n\n    assert k * kibi == PREFIXES['Ki']\n    assert kibi / k == PREFIXES['c']\n    assert kibi * kibi == PREFIXES['Mi']\n", "def test_prefix_division():\n    m = PREFIXES['m']\n    k = PREFIXES['k']\n\n    assert m / k == PREFIXES['m'] / PREFIXES['k']\n    assert m / m is S.One\n    assert k / k is S.One\n    assert k / m == PREFIXES['k'] / PREFIXES['m']\n    assert m / kibi == m / PREFIXES['Ki']\n    assert k / kibi == k / PREFIXES['Ki']\n\n    expr5 = kilo / (kibi * kibi)\n    assert isinstance(expr5, Mul)\n    assert expr5.args == (1000 / (2 ** 2), kilo)\n    assert expr5.args == (1000 / 4, kilo)\n"], "sample_30": ["def test_inlines_verbose_name_plural(self):\n    \"\"\"\n    The item added by the \"Add another XXX\" link must use the correct\n    verbose_name_plural in the inline form.\n    \"\"\"\n    self.admin_login(username='super', password='secret')\n    # Hide sidebar.\n    self.selenium.get(self.live_server_url + reverse('admin:admin_inlines_course_add'))\n    toggle_button = self.selenium.find_element_by_css_selector('#toggle-nav-sidebar')\n    toggle_button.click()\n    # Each combination of horizontal/vertical filter with stacked/tabular\n    # inlines.\n    tests = [\n        'admin:admin_inlines_course_add',\n        'admin:admin_inlines_courseproxy_add',\n        'admin:admin_inlines_courseproxy1_add',\n        'admin:admin_inlines_courseproxy2_add',\n    ]\n    css_selector = '.dynamic-class_set#class_set-%s h2'\n\n    for url_name in tests:\n        with self.subTest(url=url_name):\n            self.selenium.get(self.live_server_url + reverse(url_name))\n            # First inline shows the verbose_name_plural.\n            available, chosen = self.selenium.find_elements_by_css_selector(css_selector % 0)\n            self.assertEqual(available.text, 'AVAILABLE ATTENDANTS')\n            self.assertEqual(chosen.text, 'CHOSEN ATTENDANTS')\n            # Added inline should also have the correct verbose_name_plural.\n            self.selenium.find_element_by_link_text('Add another Class').click()\n            available, chosen = self.selenium.find_elements_by_css_selector(css_selector % 1)\n            self.assertEqual(available.text, 'AVAILABLE ATTENDANTS')\n            self.assertEqual(chosen.text, 'CHOSEN ATTENDANTS')\n            # Third inline should also have the", "def test_inline_add_fk_view_only_perm(self):\n    permission = Permission.objects.get(codename='view_inner2', content_type=self.inner_ct)\n    self.user.user_permissions.add(permission)\n    response = self.client.get(reverse('admin:admin_inlines_holder2_add'))\n    # View-only inlines. (It could be nicer to hide the empty, non-editable\n    # inlines on the add page.)\n    self.assertIs(response.context['inline_admin_formset'].has_view_permission, True)\n    self.assertIs(response.context['inline_admin_formset'].has_add_permission, False)\n    self.assertIs(response.context['inline_admin_formset'].has_change_permission, False)\n    self.assertIs(response.context['inline_admin_formset'].has_delete_permission, False)\n    self.assertContains(response, '<h2>Inner2s</h2>')\n    self.assertContains(\n        response,\n        '<input type=\"hidden\" name=\"inner2_set-TOTAL_FORMS\" value=\"0\" '\n        'id=\"id_inner2_set-TOTAL_FORMS\">',\n        html=True,\n    )\n    self.assertNotContains(response, 'Add another Inner2')\n", "    def test_inline_readonly_field_after_add_another(self):\n        self.admin_login(username='super', password='secret')\n        self.selenium.get(self.live_server_url + reverse('admin:admin_inlines_author_add'))\n        self.selenium.find_element_by_link_text('Add another Non-autopk book').click()\n        self.wait_until_visible('#id_nonautopkbook_set-2-0-id')\n        self.assertEqual(self.selenium.find_element_by_id('id_nonautopkbook_set-2-0-id').is_enabled(), False)\n", "def test_inline_m2m_verbose_name_plural(self):\n    \"\"\"\n    The item added by the \"Add another XXX\" link must use the correct\n    verbose_name_plural in the inline form.\n    \"\"\"\n    self.admin_login(username='super', password='secret')\n    self.selenium.get(self.live_server_url + reverse('admin:admin_inlines_book_add'))\n    # First inline shows the verbose_name_plural.\n    author_select = self.selenium.find_element_by_css_selector('#id_authors')\n    self.assertEqual(author_select.text, 'AUTHORS')\n    # Add another inline.\n    add_button = self.selenium.find_element_by_link_text('Add another Author')\n    add_button.click()\n    # Second inline should also have the correct verbose_name_plural.\n    author_select = self.selenium.find_element_by_css_selector('#id_authors_2')\n    self.assertEqual(author_select.text, 'AUTHORS')\n", "    def test_inline_filter_choices(self):\n        class FilteredInline(TabularInline):\n            model = Book\n            filter_horizontal = ('authors',)\n\n        modeladmin = ModelAdmin(Author, admin_site)\n        modeladmin.inlines = [FilteredInline]\n        obj = Author.objects.create(name='Author 1')\n        url = reverse('admin:admin_inlines_author_change', args=(obj.pk,))\n        request = self.factory.get(url)\n        request.user = self.superuser\n        response = modeladmin.changeform_view(request)\n        self.assertContains(response, 'filtered_authors')\n        self.assertContains(response, '<option value=\"\">---------</option>')\n        self.assertContains(response, '<option value=\"1\">Author 1</option>')\n"], "sample_154": ["def test_cupy_numexpr():\n    if not numpy:\n        skip(\"numpy not installed.\")\n    if not numexpr:\n        skip(\"numexpr not installed.\")\n    if not cupy:\n        skip(\"CuPy not installed.\")\n\n    a, b, c = cupy.random.randn(3, 128, 128)\n    expr = sin(x) + cos(y) + tan(z)**2 + Abs(z-y)*acos(sin(y*z)) + Abs(y-z)*acosh(2+exp(y-x))- sqrt(x**2+I*y**2)\n    cpfunc = lambdify((x, y, z), expr, modules='cupy')\n    nefunc = lambdify((x, y, z), expr, modules='numexpr')\n    assert cupy.allclose(cpfunc(a, b, c), nefunc(a, b, c))\n", "def test_lambdify_with_rational_number():\n    f = lambdify(x, Rational(1, 2)*x)\n    assert f(4) == 2\n", "def test_issue_23352():\n    if not numpy:\n        skip(\"numpy not installed\")\n\n    # Create a function that returns a numpy array\n    f = lambdify(x, numpy.array([x, x**2]), modules='numpy')\n\n    # Call the function with a scalar input\n    result = f(2)\n\n    # Check that the result is a numpy array with the expected values\n    numpy.testing.assert_array_equal(result, numpy.array([2, 4]))\n", "def test_lambdify_custom_module():\n        return x**2\n\n    custom_module = {\"custom_func\": custom_function}\n    expr = custom_module[\"custom_func\"](x)\n    func = lambdify(x, expr, modules=(custom_module, \"sympy\"))\n    assert func(3) == 9\n", "def test_lambdify_with_mutable_matrix():\n    if not numpy:\n        skip(\"numpy not installed\")\n\n    A = MutableDenseMatrix([[x, y], [z, 1+z]])\n    sol = MutableDenseMatrix([[1, 2], [sin(3) + 4, 1]])\n    f = lambdify((x, y, z), A, modules=\"sympy\")\n    assert f(1, 2, 3) == sol\n    f = lambdify((x, y, z), (A, [A]), modules=\"sympy\")\n    assert f(1, 2, 3) == (sol, [sol])\n    J = MutableDenseMatrix((x, x + y)).jacobian((x, y))\n    v = Matrix((x, y))\n    sol = MutableDenseMatrix([[1, 0], [1, 1]])\n    assert lambdify(v, J, modules='sympy')(1, 2) == sol\n    assert lambdify(v.T, J, modules='sympy')(1, 2) == sol\n"], "sample_18": ["def test_valid_foreign_object_unique_subset(self):\n    class Parent(models.Model):\n        a = models.PositiveIntegerField()\n        b = models.PositiveIntegerField()\n        c = models.PositiveIntegerField()\n\n        class Meta:\n            unique_together = (('a', 'b'), ('b', 'c'))\n\n    class Child(models.Model):\n        a = models.PositiveIntegerField()\n        b = models.PositiveIntegerField()\n        value = models.CharField(max_length=255)\n        parent = models.ForeignObject(\n            Parent,\n            on_delete=models.SET_NULL,\n            from_fields=('a', 'b'),\n            to_fields=('a', 'b'),\n            related_name='children',\n        )\n\n    field = Child._meta.get_field('parent')\n    self.assertEqual(field.check(from_model=Child), [])\n", "def test_m2m_with_to_field(self):\n    class Person(models.Model):\n        username = models.CharField(max_length=50, unique=True)\n\n    class Group(models.Model):\n        members = models.ManyToManyField('Person', to_field='username')\n\n    field = Group._meta.get_field('members')\n    self.assertEqual(field.check(from_model=Group), [])\n", "def test_foreign_object_to_multiple_fields(self):\n    class Parent(models.Model):\n        a = models.PositiveIntegerField(unique=True)\n        b = models.PositiveIntegerField(unique=True)\n\n    class Child(models.Model):\n        a = models.PositiveIntegerField()\n        b = models.PositiveIntegerField()\n        parent = models.ForeignObject(\n            Parent,\n            on_delete=models.SET_NULL,\n            from_fields=('a', 'b'),\n            to_fields=('a', 'b'),\n        )\n\n    field = Child._meta.get_field('parent')\n    self.assertEqual(field.check(), [\n        Error(\n            \"The to_field 'a' is not unique on the related model 'Parent'.\",\n            hint=(\n                'Add unique=True to this field or add a UniqueConstraint '\n                '(without condition) in the model Meta.constraints.'\n            ),\n            obj=field,\n            id='fields.E311',\n        ),\n        Error(\n            \"The to_field 'b' is not unique on the related model 'Parent'.\",\n            hint=(\n                'Add unique=True to this field or add a UniqueConstraint '\n                '(without condition) in the model Meta.constraints.'\n            ),\n            obj=field,\n            id='fields.E311',\n        ),\n    ])\n", "def test_through_fields_not_foreign_keys(self):\n    class Fan(models.Model):\n        pass\n\n    class Event(models.Model):\n        invitees = models.ManyToManyField(\n            Fan,\n            through='Invitation',\n            through_fields=('event', 'invitee'),\n        )\n\n    class Invitation(models.Model):\n        event = models.IntegerField()  # Not a ForeignKey\n        invitee = models.ForeignKey(Fan, models.CASCADE)\n        inviter = models.ForeignKey(Fan, models.CASCADE, related_name='+')\n\n    field = Event._meta.get_field('invitees')\n    self.assertEqual(field.check(from_model=Event), [\n        Error(\n            \"'Invitation.event' is not a foreign key.\",\n            hint=\"Did you mean one of the following foreign keys to 'Event': event?\",\n            obj=field,\n            id='fields.E339',\n        ),\n    ])\n", "def test_foreign_object_to_unique_together_field(self):\n    class Person(models.Model):\n        country_id = models.IntegerField()\n        city_id = models.IntegerField()\n\n        class Meta:\n            unique_together = (('country_id', 'city_id'),)\n\n    class MMembership(models.Model):\n        person_country_id = models.IntegerField()\n        person_city_id = models.IntegerField()\n        person = models.ForeignObject(\n            Person,\n            on_delete=models.CASCADE,\n            from_fields=['person_country_id', 'person_city_id'],\n            to_fields=['country_id', 'city_id'],\n        )\n\n    field = MMembership._meta.get_field('person')\n    self.assertEqual(field.check(), [])\n"], "sample_58": ["def test_default_dbname(self):\n    self.assertEqual(\n        self.settings_to_cmd_args_env(\n            {\n                \"USER\": \"someuser\",\n                \"PASSWORD\": \"somepassword\",\n                \"HOST\": \"somehost\",\n                \"PORT\": \"444\",\n            }\n        ),\n        (\n            [\"psql\", \"-U\", \"someuser\", \"-h\", \"somehost\", \"-p\", \"444\", \"postgres\"],\n            {\"PGPASSWORD\": \"somepassword\"},\n        ),\n    )\n", "def test_no_name_service(self):\n    self.assertEqual(\n        self.settings_to_cmd_args_env(\n            {\n                \"USER\": \"someuser\",\n                \"PASSWORD\": \"somepassword\",\n                \"HOST\": \"somehost\",\n                \"PORT\": \"444\",\n            }\n        ),\n        (\n            [\"psql\", \"-U\", \"someuser\", \"-h\", \"somehost\", \"-p\", \"444\", \"postgres\"],\n            {\"PGPASSWORD\": \"somepassword\"},\n        ),\n    )\n", "    def test_no_dbname_no_service(self):\n        self.assertEqual(\n            self.settings_to_cmd_args_env(\n                {\n                    \"USER\": \"someuser\",\n                    \"PASSWORD\": \"somepassword\",\n                    \"HOST\": \"somehost\",\n                    \"PORT\": \"5432\",\n                }\n            ),\n            (\n                [\"psql\", \"-U\", \"someuser\", \"-h\", \"somehost\", \"-p\", \"5432\", \"postgres\"],\n                {\"PGPASSWORD\": \"somepassword\"},\n            ),\n        )\n", "def test_default_db_name(self):\n    self.assertEqual(\n        self.settings_to_cmd_args_env(\n            {\n                \"USER\": \"someuser\",\n                \"PASSWORD\": \"somepassword\",\n                \"HOST\": \"somehost\",\n                \"PORT\": \"5432\",\n            }\n        ),\n        (\n            [\"psql\", \"-U\", \"someuser\", \"-h\", \"somehost\", \"-p\", \"5432\", \"postgres\"],\n            {\"PGPASSWORD\": \"somepassword\"},\n        ),\n    )\n", "def test_no_dbname_no_service(self):\n    # Test the case where no dbname is provided, but a service is not specified either\n    self.assertEqual(\n        self.settings_to_cmd_args_env({\"USER\": \"someuser\", \"HOST\": \"somehost\", \"PORT\": \"444\"}),\n        ([\"psql\", \"-U\", \"someuser\", \"-h\", \"somehost\", \"-p\", \"444\", \"postgres\"], None),\n    )\n"], "sample_73": ["def test_offsetimage_zoom():\n    fig, ax = plt.subplots()\n\n    im = np.random.rand(10, 10)\n    oi = OffsetImage(im, zoom=2)\n    ab = AnnotationBbox(oi, (0.5, 0.5), xycoords='axes fraction')\n    ax.add_artist(ab)\n\n    im = np.random.rand(10, 10)\n    oi = OffsetImage(im, zoom=0.5)\n    ab = AnnotationBbox(oi, (0.7, 0.7), xycoords='axes fraction')\n    ax.add_artist(ab)\n", "def test_anchoredoffsetbox_vertical_alignment():\n    fig, ax = plt.subplots()\n\n    ta0 = TextArea(\"test\", textprops={'verticalalignment': 'baseline'})\n    ab0 = AnchoredOffsetbox(\"center\", child=ta0, pad=0.2)\n    ax.add_artist(ab0)\n\n    ta1 = TextArea(\"test\", textprops={'verticalalignment': 'center'})\n    ab1 = AnchoredOffsetbox(\"center\", child=ta1, pad=0.2)\n    ax.add_artist(ab1)\n\n    ta2 = TextArea(\"test\", textprops={'verticalalignment': 'top'})\n    ab2 = AnchoredOffsetbox(\"center\", child=ta2, pad=0.2)\n    ax.add_artist(ab2)\n", "def test_packers_width_height(mode):\n    # set the DPI to match points to make the math easier below\n    fig = plt.figure(dpi=72)\n    renderer = fig.canvas.get_renderer()\n\n    x1, y1 = 10, 30\n    x2, y2 = 20, 60\n    r1 = DrawingArea(x1, y1)\n    r2 = DrawingArea(x2, y2)\n\n    # HPacker\n    hpacker = HPacker(children=[r1, r2], align='center', mode=mode, width=50, height=75)\n    hpacker.draw(renderer)\n    bbox = hpacker.get_bbox(renderer)\n    assert_allclose(bbox.bounds, (0, -15, 50, 75))\n\n    # VPacker\n    vpacker = VPacker(children=[r1, r2], align='center', mode=mode, width=60, height=40)\n    vpacker.draw(renderer)\n    bbox = vpacker.get_bbox(renderer)\n    assert_allclose(bbox.bounds, (-5, -40, 60, 40))\n", "def test_anchoredtext_vertical_alignment():\n    fig, ax = plt.subplots()\n\n    text0 = AnchoredText(\"test long text\\ntest\", loc=\"lower center\",\n                         pad=0.2, prop={\"va\": \"bottom\"})\n    ax.add_artist(text0)\n    text1 = AnchoredText(\"test long text\\ntest\", loc=\"center\",\n                         pad=0.2, prop={\"va\": \"center\"})\n    ax.add_artist(text1)\n    text2 = AnchoredText(\"test long text\\ntest\", loc=\"upper center\",\n                         pad=0.2, prop={\"va\": \"top\"})\n    ax.add_artist(text2)\n", "def test_textarea_get_bbox(monkeypatch):\n    ta = TextArea('Foo')\n    renderer = ta.figure.canvas.get_renderer()\n\n    # Mock the method to check its parameters\n        return 10, 20, 5\n\n    monkeypatch.setattr(renderer, 'get_text_width_height_descent', mock_get_text_width_height_descent)\n\n    bbox = ta.get_bbox(renderer)\n    assert_allclose(bbox.bounds, (-0, -15, 10, 20), atol=1e-15)\n"], "sample_121": ["def test_commutes_with():\n    p = Permutation([1, 5, 2, 0, 3, 6, 4])\n    q = Permutation([[1, 2, 3, 5, 6], [0, 4]])\n    assert p.commutes_with(q) == False\n    r = Permutation([3, 2, 1, 0])\n    assert q.commutes_with(r) == False\n    assert p.commutes_with(p) == True\n", "def test_from_inversion_vector():\n    p = Permutation([2, 5, 1, 6, 3, 0, 4])\n    iv = p.inversion_vector()\n    assert Permutation.from_inversion_vector(iv) == p\n\n    iv = [3, 2, 1, 0, 0]\n    assert Permutation.from_inversion_vector(iv) == Permutation([3, 2, 1, 0, 4, 5])\n", "def test_commutator():\n    a = Permutation([0, 2, 1, 3])\n    b = Permutation([1, 0, 2, 3])\n    assert a.commutator(b) == Permutation([0, 1, 2, 3])\n    assert b.commutator(a) == Permutation([1, 0, 2, 3])\n    assert a.commutator(a) == Permutation([0, 1, 2, 3])\n    assert b.commutator(b) == Permutation([0, 1, 2, 3])\n", "def test_power_of_permutation():\n    p = Permutation([1, 2, 0])\n    assert p**2 == Permutation([2, 0, 1])\n    assert p**3 == Permutation([0, 1, 2])\n    assert p**4 == Permutation([1, 2, 0])\n\n    p = Permutation([0, 1, 3, 2])\n    assert p**2 == Permutation([0, 1, 2, 3])\n    assert p**3 == Permutation([0, 2, 3, 1])\n    assert p**4 == Permutation([0, 3, 1, 2])\n\n    p = Permutation([2, 3, 1, 0])\n    assert p**2 == Permutation([3, 2, 0, 1])\n    assert p**3 == Permutation([1, 0, 3, 2])\n    assert p**4 == Permutation([2, 3, 1, 0])\n", "def test_xor_operator():\n    p = Permutation([1, 2, 9])\n    q = Permutation([6, 9, 8])\n    c = p ^ q\n    assert c == ~q * p * q and p == q * c * ~q\n    assert p ** q == q * p\n    assert 2 ^ p == p(2)\n    assert 2 ^ (q * p) == q(p(2))\n    assert 2 ^ (p * q) == p(q(2))\n    raises(NotImplementedError, lambda: p ** p)\n    p = Permutation([1, 2, 9])(5, 6, 8)\n    q = Permutation([6, 9, 8])\n    c = p ^ q\n    assert c != q ^ p\n    assert ~q * p * q == p ^ ~q\n    p = Permutation([1, 2, 9])(5, 6)\n    q = Permutation([6, 9, 8])\n    c = p ^ q\n    assert c != q ^ p\n    assert p ^ ~p == q\n"], "sample_158": ["def test_derived_units():\n    from sympy.physics.units import meter, second\n    derived_unit = meter / second\n    SI.derived_units[derived_unit.dimension] = derived_unit\n    assert SI.derived_units[derived_unit.dimension] == derived_unit\n", "def test_derived_units():\n    derived_unit = Quantity('derived_unit')\n    SI.set_quantity_dimension(derived_unit, length*time)\n    SI.set_quantity_scale_factor(derived_unit, 1*meter*second)\n    assert SI.derived_units == {length*time: derived_unit}\n", "def test_derived_units():\n    derived_units = {\n        length / time: Quantity('speed', abbrev='sp'),\n        mass * length / time**2: Quantity('force', abbrev='F')\n    }\n    custom_system = SI.extend([], derived_units=derived_units)\n\n    assert custom_system.derived_units == derived_units\n    assert custom_system.get_quantity_dimension(Quantity('speed')) == length / time\n    assert custom_system.get_quantity_dimension(Quantity('force')) == mass * length / time**2\n", "def test_unit_consistency_with_functions():\n    x = symbols('x')\n    f = Function('f')\n\n    v_w1 = Quantity('v_w1')\n    v_w2 = Quantity('v_w2')\n    v_w1.set_global_relative_scale_factor(1, meter/second)\n    v_w2.set_global_relative_scale_factor(1, meter/second)\n\n    expr = f(v_w1/v_w2)\n    assert SI._collect_factor_and_dimension(expr) == (f(1), Dimension(1))\n", "def test_quantity_div():\n    u = Quantity(\"u\")\n    v = Quantity(\"v\")\n    t = Quantity(\"t\")\n\n    u.set_global_relative_scale_factor(S(10), meter)\n    v.set_global_relative_scale_factor(S(5), meter)\n    t.set_global_relative_scale_factor(S(2), second)\n\n    # Test division between quantities with same dimensions\n    assert (u / v) == 2\n\n    # Test division between quantities with different dimensions\n    assert (u / t) == 10*meter/second\n\n    # Test division by a number\n    assert (u / 5) == 2*meter\n\n    # Test division of a number by a quantity\n    assert (1 / t) == 0.5/second\n"], "sample_59": ["def test_formset_with_custom_prefix(self):\n    \"\"\"Formsets can be created with a custom prefix.\"\"\"\n    CustomPrefixFormSet = formset_factory(Choice, extra=1)\n    formset = CustomPrefixFormSet(prefix=\"custom_prefix\")\n    self.assertEqual(formset.management_form.prefix, \"custom_prefix\")\n    self.assertHTMLEqual(\n        \"\\n\".join(str(form) for form in formset.forms),\n        \"\"\"<div><label for=\"id_custom_prefix-0-choice\">Choice:</label>", "def test_deleted_forms_with_initial_data(self):\n    \"\"\"Deleted forms with initial data are included in deleted_forms.\"\"\"\n    initial = [{\"choice\": \"Calexico\", \"votes\": 100}]\n    data = {\n        \"choices-TOTAL_FORMS\": \"1\",\n        \"choices-INITIAL_FORMS\": \"1\",\n        \"choices-MIN_NUM_FORMS\": \"0\",\n        \"choices-MAX_NUM_FORMS\": \"1\",\n        \"choices-0-choice\": \"Calexico\",\n        \"choices-0-votes\": \"100\",\n        \"choices-0-DELETE\": \"on\",\n    }\n    ChoiceFormSet = formset_factory(Choice, can_delete=True)\n    formset = ChoiceFormSet(data, auto_id=False, prefix=\"choices\", initial=initial)\n    self.assertTrue(formset.is_valid())\n    self.assertEqual(len(formset.deleted_forms), 1)\n    self.assertEqual(\n        [form.cleaned_data for form in formset.deleted_forms],\n        [{\"votes\": 100, \"DELETE\": True, \"choice\": \"Calexico\"}],\n    )\n", "def test_empty_formset_rendering(self):\n    \"\"\"Rendering an empty formset works.\"\"\"\n    EmptyFavoriteDrinkFormSet = formset_factory(FavoriteDrinkForm, extra=0)\n    formset = EmptyFavoriteDrinkFormSet()\n    self.assertHTMLEqual(\n        str(formset),\n        '<input type=\"hidden\" name=\"form-TOTAL_FORMS\" value=\"0\" id=\"id_form-TOTAL_FORMS\">'\n        '<input type=\"hidden\" name=\"form-INITIAL_FORMS\" value=\"0\" id=\"id_form-INITIAL_FORMS\">'\n        '<input type=\"hidden\" name=\"form-MIN_NUM_FORMS\" value=\"0\" id=\"id_form-MIN_NUM_FORMS\">'\n        '<input type=\"hidden\" name=\"form-MAX_NUM_FORMS\" value=\"1000\" id=\"id_form-MAX_NUM_FORMS\">',\n    )\n", "def test_formset_with_custom_error_message(self):\n    \"\"\"\n    Custom error messages can be provided to override the default error messages.\n    \"\"\"\n    data = {\n        \"drinks-TOTAL_FORMS\": \"2\",\n        \"drinks-INITIAL_FORMS\": \"0\",\n        \"drinks-MIN_NUM_FORMS\": \"0\",\n        \"drinks-MAX_NUM_FORMS\": \"0\",\n        \"drinks-0-name\": \"Gin and Tonic\",\n        \"drinks-1-name\": \"Gin and Tonic\",\n    }\n    custom_error_messages = {\"duplicate_drink\": \"Custom duplicate drink error message\"}\n    formset = FavoriteDrinksFormSet(data, prefix=\"drinks\", error_messages=custom_error_messages)\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(formset.non_form_errors(), [\"Custom duplicate drink error message\"])\n    self.assertEqual(\n        str(formset.non_form_errors()),\n        '<ul class=\"errorlist nonform\"><li>'\n        \"Custom duplicate drink error message</li></ul>\",\n    )\n", "def test_custom_template_name(self):\n    \"\"\"\n    Formsets with custom template_name attributes use that template.\n    \"\"\"\n    class CustomTemplateFormSet(BaseFormSet):\n        template_name = \"custom/template/path.html\"\n\n    ChoiceFormSet = formset_factory(Choice, formset=CustomTemplateFormSet)\n    formset = ChoiceFormSet()\n    self.assertEqual(formset.template_name, \"custom/template/path.html\")\n"], "sample_60": ["def test_serialize_dict_with_tuple_keys(self):\n    dict_with_tuple_keys = {(1, 2): \"tuple key\", \"string key\": \"string value\"}\n    string, imports = MigrationWriter.serialize(dict_with_tuple_keys)\n    self.assertEqual(string, \"{('1', '2'): 'tuple key', 'string key': 'string value'}\")\n    self.assertEqual(imports, set())\n", "def test_serialize_custom_model_field(self):\n    class CustomModelField(models.Field):\n            return 'custom_db_type'\n\n    field = CustomModelField()\n    string = MigrationWriter.serialize(field)[0]\n    self.assertEqual(\n        string,\n        \"migrations.test_writer.CustomModelField()\",\n    )\n", "def test_serialize_deconstructable(self):\n    deconstructible_instance = DeconstructibleInstances()\n    self.assertSerializedEqual(deconstructible_instance)\n    self.assertSerializedResultEqual(\n        deconstructible_instance,\n        (\n            \"migrations.test_writer.DeconstructibleInstances()\",\n            {\"import migrations.test_writer\"},\n        ),\n    )\n", "    def test_serialize_new_serializer(self):\n        class CustomSerializer(BaseSerializer):\n                return f\"Custom({self.value})\", set()\n\n        BaseSerializer.register(complex, CustomSerializer)\n        self.assertSerializedEqual(complex(1, 2))\n        BaseSerializer.unregister(complex)\n        with self.assertRaisesMessage(ValueError, \"Cannot serialize: (1+2j)\"):\n            self.assertSerializedEqual(complex(1, 2))\n", "def test_serialize_custom_deconstruct(self):\n    class CustomDeconstructible:\n            self.a = a\n            self.b = b\n\n            return (self.a, self.b) == (other.a, other.b)\n\n            return (\n                \"migrations.test_writer.CustomDeconstructible\",\n                [self.a, self.b],\n                {},\n            )\n\n    value = CustomDeconstructible(1, 2)\n    result = self.serialize_round_trip(value)\n    self.assertEqual(result, value)\n"], "sample_102": ["def test_iforest_max_features():\n    \"\"\"Test Isolation Forest with different max_features settings.\"\"\"\n    X_train = np.array([[0, 1, 2], [1, 2, 3], [2, 3, 4]])\n    X_test = np.array([[3, 4, 5], [4, 5, 6]])\n\n    clf_auto = IsolationForest(random_state=rng, max_features='auto').fit(X_train)\n    clf_sqrt = IsolationForest(random_state=rng, max_features='sqrt').fit(X_train)\n    clf_log2 = IsolationForest(random_state=rng, max_features='log2').fit(X_train)\n\n    pred_auto = clf_auto.predict(X_test)\n    pred_sqrt = clf_sqrt.predict(X_test)\n    pred_log2 = clf_log2.predict(X_test)\n\n    # Ensure that the predictions are not all the same (i.e., the tests are not trivial)\n    assert not np.all(pred_auto == pred_sqrt) or not np.all(pred_auto == pred_log2) or not np.all(pred_sqrt == pred_log2)\n", "def test_iforest_max_features():\n    X = np.array([[0, 1], [1, 2], [2, 1], [1, 1]])\n    y = np.array([1, 1, -1, -1])\n    clf = IsolationForest(max_features=1, random_state=rng).fit(X, y)\n    assert_array_equal(clf.feature_importances_, [0, 0])\n    clf = IsolationForest(max_features=0.5, random_state=rng).fit(X, y)\n    assert_array_equal(clf.feature_importances_, [0, 0])\n    clf = IsolationForest(max_features='sqrt', random_state=rng).fit(X, y)\n    assert_array_equal(clf.feature_importances_, [0, 0])\n", "def test_iforest_invalid_contamination():\n    \"\"\"Test Isolation Forest with invalid contamination parameter\"\"\"\n    X = np.array([[0, 1], [1, 2]])\n\n    # Test contamination\n    assert_raises(ValueError, IsolationForest(contamination=-0.1).fit, X)\n    assert_raises(ValueError, IsolationForest(contamination=1.1).fit, X)\n    assert_raises(ValueError, IsolationForest(contamination=\"invalid\").fit, X)\n", "def test_iforest_max_features():\n    \"\"\"Check Isolation Forest for various max_features settings.\"\"\"\n    X_train = np.array([[0, 1], [1, 2]])\n    X_test = np.array([[2, 1], [1, 1]])\n\n    grid = ParameterGrid({\"max_features\": [1, 0.5, \"auto\", \"sqrt\", \"log2\"]})\n\n    with ignore_warnings():\n        for params in grid:\n            IsolationForest(random_state=rng,\n                            **params).fit(X_train).predict(X_test)\n", "def test_iforest_predict_proba():\n    \"\"\"Test that predict_proba returns probabilities.\"\"\"\n    X_train = np.array([[0, 1], [1, 2]])\n    X_test = np.array([[2, 1], [1, 1]])\n\n    clf = IsolationForest(random_state=rng).fit(X_train)\n    probabilities = clf.predict_proba(X_test)\n\n    # Check that probabilities are between 0 and 1\n    assert np.all(probabilities >= 0)\n    assert np.all(probabilities <= 1)\n\n    # Check that the sum of probabilities for each instance is approximately 1\n    assert_array_almost_equal(np.sum(probabilities, axis=1), np.ones(2))\n"], "sample_109": ["def test_leave_one_group_out_empty_trainset():\n    cv = LeaveOneGroupOut()\n    X, y = [[1], [2]], [0, 3]  # 2 samples, 1 unique group\n    with pytest.raises(\n            ValueError,\n            match='The groups parameter contains fewer than 2 unique groups'):\n        next(cv.split(X, y, groups=[1, 1]))\n", "def test_group_kfold_empty_trainset():\n    # GroupKFold expect at least 2 groups so no need to check\n    cv = GroupKFold(n_splits=3)\n    X, y = [[1], [2]], [0, 3]  # 2 samples\n    with pytest.raises(\n            ValueError,\n            match='Cannot have number of splits n_splits=3 greater than the number of samples: 2.'):\n        next(cv.split(X, y, groups=[1, 2]))\n", "def test_leave_pgroups_out_empty_trainset():\n    cv = LeavePGroupsOut(n_groups=2)\n    X, y = [[1], [2]], [0, 3]  # 2 samples\n    with pytest.raises(\n            ValueError,\n            match=\"The groups parameter contains fewer than \\\\(or equal to\\\\) \"\n                  \"n_groups \\\\(2\\\\) numbers of unique groups \\\\(\\[1, 2]\\\\). \"\n                  \"LeavePGroupsOut expects that at least n_groups \\\\+ 1 \\\\(3\\\\) \"\n                  \"unique groups be present\"):\n        next(cv.split(X, y, groups=[1, 2]))\n", "def test_shuffle_split_non_default_random_state():\n    # Check that setting random_state to an integer or a RandomState instance\n    # results in the same split\n    ss1 = ShuffleSplit(test_size=0.2, random_state=0).split(X)\n    ss2 = ShuffleSplit(test_size=0.2, random_state=np.random.RandomState(0)).split(X)\n    for t1, t2 in zip(ss1, ss2):\n        assert_array_equal(t1[0], t2[0])\n        assert_array_equal(t1[1], t2[1])\n", "def test_validate_shuffle_split_errors():\n    # Testing _validate_shuffle_split function with invalid inputs\n    n_samples = 10\n\n    # Test with test_size and train_size both as None\n    with pytest.raises(ValueError, match=\"Both test_size and train_size are None.\"):\n        _validate_shuffle_split(n_samples, None, None)\n\n    # Test with test_size and train_size both as 0\n    with pytest.raises(ValueError, match=\"Both test_size and train_size are 0.\"):\n        _validate_shuffle_split(n_samples, 0, 0)\n\n    # Test with test_size and train_size both greater than 1\n    with pytest.raises(ValueError, match=\"The sum of test_size and train_size is greater than 1.\"):\n        _validate_shuffle_split(n_samples, 0.6, 0.7)\n\n    # Test with test_size and train_size both greater than n_samples\n    with pytest.raises(ValueError, match=\"The sum of test_size and train_size is greater than the number of samples.\"):\n        _validate_shuffle_split(n_samples, 12, 1)\n\n    # Test with test_size as negative\n    with pytest.raises(ValueError, match=\"test_size should be positive.\"):\n        _validate_shuffle_split(n_samples, -0.2, None)\n\n    # Test with train_size as negative\n    with pytest.raises(ValueError, match=\"train_size should be positive.\"):\n        _validate_shuffle_split(n_samples, None, -0.3)\n\n    # Test with test_size and train_size both as non-numeric\n    with pytest.raises(TypeError, match=\"Invalid value for train_size or test_size.\"):\n        _validate_shuffle_split(n_samples, 'test', 'train')\n"], "sample_110": ["def test_affinity_propagation_sparse_input():\n    # Test AffinityPropagation with sparse input\n    X_sparse = csr_matrix(X)\n    af = AffinityPropagation(affinity=\"euclidean\")\n    labels_sparse = af.fit_predict(X_sparse)\n    labels_dense = af.fit_predict(X)\n    assert_array_equal(labels_sparse, labels_dense)\n", "def test_affinity_propagation_sparse_input():\n    # Test AffinityPropagation with sparse input\n    X_sparse = csr_matrix(X)\n    af = AffinityPropagation(affinity=\"euclidean\")\n    labels_sparse = af.fit_predict(X_sparse)\n    labels_dense = af.fit_predict(X)\n    assert_array_equal(labels_sparse, labels_dense)\n", "def test_affinity_propagation_single_sample():\n    # Test AffinityPropagation with a single sample\n    X = np.array([[0, 0]])\n    af = AffinityPropagation(affinity=\"euclidean\")\n    labels = af.fit_predict(X)\n    assert_array_equal(labels, np.array([0]))\n", "def test_affinity_propagation_damping():\n    # Test damping parameter validation\n    af = AffinityPropagation(damping=0)\n    with pytest.raises(ValueError):\n        af.fit(X)\n    af = AffinityPropagation(damping=1)\n    with pytest.raises(ValueError):\n        af.fit(X)\n", "def test_affinity_propagation_precomputed_affinity():\n    # Test AffinityPropagation with precomputed affinity matrix\n    S = -euclidean_distances(X, squared=True)\n    preference = np.median(S)\n\n    af = AffinityPropagation(preference=preference, affinity=\"precomputed\")\n    labels_precomputed = af.fit_predict(S)\n\n    af = AffinityPropagation(preference=preference, affinity=\"euclidean\")\n    labels_euclidean = af.fit_predict(X)\n\n    assert_array_equal(labels_precomputed, labels_euclidean)\n"], "sample_39": ["def test_default_app_namespace_object(self):\n    \"\"\"\n    A default application namespace can be used for lookup when included\n    as a dynamic URL object.\n    \"\"\"\n    test_urls = [\n        ('testapp:urlobject-view', [], {}, '/default/inner/'),\n        ('testapp:urlobject-view', [37, 42], {}, '/default/inner/37/42/'),\n        ('testapp:urlobject-view', [], {'arg1': 42, 'arg2': 37}, '/default/inner/42/37/'),\n        ('testapp:urlobject-special-view', [], {}, '/default/inner/+%5C$*/'),\n    ]\n    for name, args, kwargs, expected in test_urls:\n        with self.subTest(name=name, args=args, kwargs=kwargs):\n            self.assertEqual(reverse(name, args=args, kwargs=kwargs), expected)\n", "    def test_default_settings(self):\n        default_settings = {\n            'APPEND_SLASH': True,\n            'LANGUAGE_CODE': 'en-us',\n        }\n        for setting, default_value in default_settings.items():\n            with self.settings(**{setting: default_value}):\n                resolver = get_resolver('urlpatterns_reverse.urls')\n                self.assertEqual(resolver.check(), [])\n", "    def test_check_custom_error_handlers(self):\n        resolver = get_resolver('urlpatterns_reverse.urls_error_handlers')\n        warnings = resolver.check()\n        self.assertEqual(len(warnings), 0)  # No warnings for valid handlers\n\n        resolver = get_resolver('urlpatterns_reverse.urls_error_handlers_invalid')\n        warnings = resolver.check()\n        self.assertEqual(len(warnings), 4)\n        self.assertEqual(warnings[0].id, 'urls.E007')\n        self.assertEqual(warnings[1].id, 'urls.E007')\n        self.assertEqual(warnings[2].id, 'urls.E007')\n        self.assertEqual(warnings[3].id, 'urls.E008')\n", "    def test_check_pattern_startswith_slash(self):\n        pattern = RegexPattern(r'^/pattern')\n        warnings = pattern.check()\n        self.assertEqual(len(warnings), 1)\n        self.assertEqual(warnings[0].id, 'urls.W002')\n\n        pattern = RegexPattern(r'^pattern')\n        warnings = pattern.check()\n        self.assertEqual(len(warnings), 0)\n\n        pattern = RegexPattern(r'^/pattern/')\n        warnings = pattern.check()\n        self.assertEqual(len(warnings), 0)\n\n        pattern = RegexPattern(r'pattern/$')\n        warnings = pattern.check()\n        self.assertEqual(len(warnings), 0)\n", "    def test_check_pattern_startswith_slash(self):\n        urlpatterns = [\n            path('slash-start/', views.empty_view),\n            re_path(r'^slash-start-re/', views.empty_view),\n            path('slash-end/', views.empty_view, name='slash-end'),\n            re_path(r'^slash-end-re/$', views.empty_view, name='slash-end-re'),\n        ]\n        resolver = URLResolver(RegexPattern(r'^/'), urlpatterns)\n        warnings = resolver.check()\n        self.assertEqual(len(warnings), 2)\n        self.assertEqual(\n            str(warnings[0]),\n            \"Your URL pattern '^slash-start/' has a route beginning with a '/'. \"\n            \"Remove this slash as it is unnecessary. If this pattern is targeted in an \"\n            \"include(), ensure the include() pattern has a trailing '/'.\"\n        )\n        self.assertEqual(\n            str(warnings[1]),\n            \"Your URL pattern '^slash-end-re/$' has a route beginning with a '/'. \"\n            \"Remove this slash as it is unnecessary. If this pattern is targeted in an \"\n            \"include(), ensure the include() pattern has a trailing '/'.\"\n        )\n"]}, "raw_scored_generations": {"sample_3": [false, false, false, false, false], "sample_2": [false, false, false, false, false], "sample_1": [false, false, false, false, false], "sample_0": [true, true, true, true, false], "sample_6": [false, false, false, false, true], "sample_4": [false, true, false, true, true], "sample_5": [true, false, true, true, false], "sample_7": [true, false, true, true, true], "sample_9": [true, true, true, true, false], "sample_8": [false, false, false, false, false], "sample_10": [true, true, true, true, true], "sample_11": [true, false, false, true, true], "sample_13": [true, true, true, true, false], "sample_12": [true, true, true, true, true], "sample_14": [true, true, true, true, true], "sample_15": [true, true, false, true, true], "sample_16": [true, true, true, true, true], "sample_17": [true, false, true, false, false], "sample_18": [true, true, true, true, true], "sample_19": [true, false, false, true, false], "sample_21": [true, true, true, true, false], "sample_20": [true, true, true, true, true], "sample_23": [true, true, true, true, true], "sample_22": [true, true, true, false, true], "sample_24": [true, true, true, true, true], "sample_25": [true, false, false, true, false], "sample_28": [false, false, false, true, false], "sample_26": [false, true, false, false, true], "sample_27": [true, true, true, true, true], "sample_29": [true, true, true, true, true], "sample_31": [false, true, true, true, true], "sample_30": [true, true, true, true, true], "sample_32": [true, true, true, true, true], "sample_33": [false, false, false, true, true], "sample_34": [false, false, false, false, false], "sample_35": [true, false, true, true, false], "sample_36": [true, true, true, true, true], "sample_37": [true, true, true, false, true], "sample_39": [true, true, false, false, false], "sample_38": [true, true, true, false, true], "sample_40": [true, false, true, false, true], "sample_42": [true, true, false, false, false], "sample_41": [true, false, true, true, true], "sample_43": [true, true, true, true, true], "sample_45": [false, false, false, false, false], "sample_44": [true, true, true, true, true], "sample_46": [false, false, false, true, false], "sample_48": [true, true, true, true, true], "sample_47": [true, true, false, true, true], "sample_50": [true, true, true, false, true], "sample_49": [true, true, true, true, true], "sample_51": [true, true, false, true, true], "sample_52": [true, true, true, true, true], "sample_53": [true, false, false, true, false], "sample_55": [true, true, true, true, true], "sample_54": [true, true, true, true, true], "sample_56": [true, true, false, true, true], "sample_58": [true, true, true, true, true], "sample_57": [true, true, true, true, true], "sample_60": [true, false, true, false, false], "sample_59": [false, true, true, true, true], "sample_61": [true, true, true, true, false], "sample_63": [true, true, false, false, true], "sample_62": [true, false, true, false, false], "sample_64": [true, true, true, true, true], "sample_66": [true, true, true, true, true], "sample_65": [true, true, true, true, true], "sample_68": [true, true, true, true, true], "sample_67": [true, true, false, true, true], "sample_69": [false, false, true, false, false], "sample_70": [false, false, true, true, false], "sample_71": [true, true, false, true, false], "sample_72": [false, false, false, false, false], "sample_73": [false, false, false, false, false], "sample_74": [false, false, false, false, false], "sample_75": [false, false, true, true, false], "sample_76": [false, false, false, false, true], "sample_77": [false, false, false, false, false], "sample_78": [true, true, false, false, false], "sample_79": [false, false, false, false, true], "sample_80": [false, false, false, false, false], "sample_82": [true, false, false, false, false], "sample_81": [false, false, false, false, false], "sample_83": [false, false, false, false, false], "sample_84": [false, false, false, false, false], "sample_85": [false, false, false, false, false], "sample_86": [false, false, false, false, false], "sample_87": [false, false, false, false, false], "sample_88": [false, false, false, true, false], "sample_89": [false, false, false, false, false], "sample_90": [false, false, false, false, true], "sample_91": [false, false, false, false, false], "sample_92": [false, false, false, false, false], "sample_93": [false, true, false, false, false], "sample_95": [false, false, false, false, false], "sample_94": [true, false, false, false, true], "sample_96": [false, false, false, true, true], "sample_97": [true, false, true, false, true], "sample_98": [true, false, true, true, true], "sample_99": [true, false, false, false, false], "sample_100": [false, false, false, false, false], "sample_101": [false, false, false, true, false], "sample_102": [false, false, false, false, false], "sample_103": [false, true, false, false, true], "sample_104": [false, false, false, false, false], "sample_105": [false, false, false, true, false], "sample_107": [false, true, false, false, false], "sample_106": [false, false, false, false, false], "sample_108": [true, false, false, false, false], "sample_109": [true, false, false, true, false], "sample_110": [true, true, true, true, true], "sample_111": [false, false, false, false, false], "sample_112": [true, false, true, false, true], "sample_113": [false, false, false, false, false], "sample_114": [false, false, false, false, false], "sample_115": [true, false, false, false, false], "sample_116": [false, false, false, false, false], "sample_117": [true, false, false, false, false], "sample_118": [false, true, false, true, false], "sample_119": [false, false, false, false, false], "sample_120": [true, false, false, false, true], "sample_121": [false, true, false, false, false], "sample_122": [false, true, true, true, true], "sample_123": [false, false, false, true, false], "sample_124": [false, false, false, false, false], "sample_125": [true, true, true, false, false], "sample_126": [false, false, false, false, true], "sample_127": [true, false, false, false, false], "sample_128": [false, false, true, true, false], "sample_129": [false, false, true, false, false], "sample_130": [true, true, true, true, true], "sample_131": [false, true, true, false, false], "sample_132": [false, false, false, false, true], "sample_133": [false, true, false, false, false], "sample_134": [true, true, true, true, true], "sample_135": [true, false, false, false, false], "sample_136": [false, false, false, false, false], "sample_137": [false, false, false, false, false], "sample_138": [false, false, false, false, true], "sample_139": [true, false, false, true, false], "sample_140": [true, true, true, false, false], "sample_142": [false, false, true, false, true], "sample_141": [false, false, true, false, false], "sample_143": [false, true, false, true, true], "sample_144": [false, false, true, false, false], "sample_145": [true, false, false, true, false], "sample_146": [false, false, false, true, false], "sample_147": [false, false, true, false, false], "sample_148": [false, false, false, false, false], "sample_149": [false, false, false, false, false], "sample_150": [true, false, false, false, false], "sample_151": [false, false, false, false, false], "sample_152": [true, true, false, true, true], "sample_153": [false, false, false, true, false], "sample_154": [true, true, true, false, true], "sample_155": [true, false, false, false, false], "sample_156": [false, false, false, false, true], "sample_157": [true, true, false, false, false], "sample_158": [false, false, false, false, false], "sample_159": [false, false, false, false, false]}}